more information  - www.cambridge.org 9781107033856    Numerical Methods in Engineering with Python 3  This book is an introduction to numerical methods for students in engi- neering. It covers the usual topics found in an engineering course: solu- tion of equations, interpolation and data ﬁtting, solution of differential equations, eigenvalue problems, and optimization. The algorithms are implemented in Python 3, a high-level programming language that ri- vals MATLAB R cid:2  in readability and ease of use. All methods include pro- grams showing how the computer code is utilized in the solution of problems.  The book is based on Numerical Methods in Engineering with Python, which used Python 2. Apart from the migration from Python 2 to Python 3, the major change in this new text is the introduction of the Python plotting package Matplotlib.  Jaan Kiusalaas is a Professor Emeritus in the Department of Engi- neering Science and Mechanics at Pennsylvania State University. He has taught computer methods, including ﬁnite element and bound- ary element methods, for more than 30 years. He is also the co-author or author of four books – Engineering Mechanics: Statics; Engineering Mechanics: Dynamics; Mechanics of Materials; Numerical Methods in Engineering with MATLAB  2nd edition ; and two previous editions of Numerical Methods in Engineering with Python.    NUMERICAL METHODS IN ENGINEERING WITH PYTHON 3  Jaan Kiusalaas The Pennsylvania State University   cambridge university press Cambridge, New York, Melbourne, Madrid, Cape Town, Singapore, S˜ao Paulo, Delhi, Mexico City  Cambridge University Press 32 Avenue of the Americas, New York, NY 10013-2473, USA  www.cambridge.org Information on this title: www.cambridge.org 9781107033856 C cid:2  Jaan Kiusalaas 2013  This publication is in copyright. Subject to statutory exception and to the provisions of relevant collective licensing agreements, no reproduction of any part may take place without the written permission of Cambridge University Press.  First published 2013  Printed in the United States of America  A catalog record for this publication is available from the British Library.  Library of Congress Cataloging in Publication data  Kiusalaas, Jaan. Numerical methods in engineering with Python 3   Jaan Kiusalaas.  pages cm  Includes bibliographical references and index. ISBN 978-1-107-03385-6 1. Engineering mathematics – Data processing. 2. Python  Computer program language  TA345.K58 2013  cid:3  620.00285  2012036775  5133–dc23  I. Title.  ISBN 978-1-107-03385-6 Hardback  Additional resources for this publication at www.cambridge.org kiusalaaspython.  Cambridge University Press has no responsibility for the persistence or accuracy of URLs for external or third-party Internet websites referred to in this publication and does not guarantee that any content on such websites is, or will remain, accurate or appropriate.   Contents  Preface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .ix  1  2  3  Introduction to Python . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.1 General Information .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.2 Core Python. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4 1.3 Functions and Modules. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 1.4 Mathematics Modules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 1.5 numpy Module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 1.6 Plotting with matplotlib.pyplot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 1.7 Scoping of Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 1.8 Writing and Running Programs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29  Systems of Linear Algebraic Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 2.1 Introduction .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 2.2 Gauss Elimination Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 2.3 LU Decomposition Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 Problem Set 2.1. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .55 2.4 Symmetric and Banded Coefﬁcient Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 2.5 Pivoting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 Problem Set 2.2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .78 ∗2.6 Matrix Inversion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .84 ∗2.7 Iterative Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87 Problem Set 2.3. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .98 2.8 Other Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102  Interpolation and Curve Fitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 3.1 Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 3.2 Polynomial Interpolation .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 3.3 Interpolation with Cubic Spline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120 Problem Set 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126 3.4 Least-Squares Fit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129 Problem Set 3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141  4 Roots of Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145 4.1 Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145 4.2 Incremental Search Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146  v   vi  Contents  4.3 Method of Bisection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .148 4.4 Methods Based on Linear Interpolation.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .151 4.5 Newton-Raphson Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156 4.6 Systems of Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161 Problem Set 4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166 ∗4.7 Zeros of Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173 Problem Set 4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180 4.8 Other Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182  5 Numerical Differentiation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183 5.1 Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .183 5.2 Finite Difference Approximations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .183 5.3 Richardson Extrapolation .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188 5.4 Derivatives by Interpolation .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191 Problem Set 5.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195  6 Numerical Integration. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .199 6.1 Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .199 6.2 Newton-Cotes Formulas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200 6.3 Romberg Integration. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .207 Problem Set 6.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212 6.4 Gaussian Integration .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216 Problem Set 6.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230 ∗6.5 Multiple Integrals. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .232 Problem Set 6.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243  7  8  9  Initial Value Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246 7.1 Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .246 7.2 Euler’s Method. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .247 7.3 Runge-Kutta Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252 Problem Set 7.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263 7.4 Stability and Stiffness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268 7.5 Adaptive Runge-Kutta Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271 7.6 Bulirsch-Stoer Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 280 Problem Set 7.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287 7.7 Other Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292  Two-Point Boundary Value Problems. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .293 8.1 Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .293 8.2 Shooting Method. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .294 Problem Set 8.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304 8.3 Finite Difference Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307 Problem Set 8.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317  Symmetric Matrix Eigenvalue Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321 9.1 Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .321 9.2 Jacobi Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 324 9.3 Power and Inverse Power Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336 Problem Set 9.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345 9.4 Householder Reduction to Tridiagonal Form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351   vii  Contents  9.5 Eigenvalues of Symmetric Tridiagonal Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359 Problem Set 9.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368 9.6 Other Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373  10  Introduction to Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 374 10.1 Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .374 10.2 Minimization Along a Line . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376 10.3 Powell’s Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382 10.4 Downhill Simplex Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 392 Problem Set 10.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399  Appendices. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .407 A1 Taylor Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 407 A2 Matrix Algebra. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .410  List of Program Modules  by Chapter  . . . . . . . 417  Index. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .421    Preface  This book is targeted toward engineers and engineering students of advanced stand- ing  juniors, seniors, and graduate students . Familiarity with a computer language is required; knowledge of engineering mechanics  statics, dynamics, and mechanics of materials  is useful, but not essential.  The primary purpose of the text is to teach numerical methods. It is not a primer on Python programming. We introduce just enough Python to implement the nu- merical algorithms. That leaves the vast majority of the language unexplored.  Most engineers are not programmers, but problem solvers. They want to know what methods can be applied to a given problem, what their strengths and pitfalls are, and how to implement them. Engineers are not expected to write computer code for basic tasks from scratch; they are more likely to use functions and subroutines that have been already written and tested. Thus, programming by engineers is largely conﬁned to assembling existing bits of code into a coherent package that solves the problem at hand.  The “bit” of code is usually a function that implements a speciﬁc task. For the user the details of the code are unimportant. What matters are the interface  what goes in and what comes out  and an understanding of the method on which the al- gorithm is based. Because no numerical algorithm is infallible, the importance of understanding the underlying method cannot be overemphasized; it is, in fact, the rationale behind learning numerical methods.  This book attempts to conform to the views outlined earlier. Each numerical method is explained in detail and its shortcomings are pointed out. The examples that follow individual topics fall into two categories: hand computations that illus- trate the inner workings of the method, and small programs that show how the com- puter code is utilized in solving a problem. Problems that require programming are marked with  cid:2 .  The material consists of the usual topics covered in an engineering course on nu- merical methods: solution of equations, interpolation and data ﬁtting, numerical dif- ferentiation and integration, solution of ordinary differential equations, and eigen- value problems. The choice of methods within each topic is tilted toward relevance to engineering problems. For example, there is an extensive discussion of symmetric, sparsely populated coefﬁcient matrices in the solution of simultaneous equations.  ix   x  Preface  In the same vein, the solution of eigenvalue problems concentrates on methods that efﬁciently extract speciﬁc eigenvalues from banded matrices.  An important criterion used in the selection of methods was clarity. Algorithms requiring overly complex bookkeeping were rejected regardless of their efﬁciency and robustness. This decision, which was taken with great reluctance, is in keeping with the intent to avoid emphasis on programming.  The selection of algorithms was also inﬂuenced by current practice. This disqual- iﬁed several well-known historical methods that have been overtaken by more recent developments. For example, the secant method for ﬁnding roots of equations was omitted as having no advantages over Ridder’s method. For the same reason, the mul- tistep methods used to solve differential equations  e.g., Milne and Adams methods  were left out in favor of the adaptive Runge-Kutta and Bulirsch-Stoer methods.  Notably absent is a chapter on partial differential equations. It was felt that this topic is best treated by ﬁnite element or boundary element methods, which are out- side the scope of this book. The ﬁnite difference model, which is commonly intro- duced in numerical methods texts, is just too impractical in handling multidimen- sional boundary value problems.  As usual, the book contains more material than can be covered in a three-credit course. The topics that can be skipped without loss of continuity are tagged with an asterisk  * .  What Is New in This Edition  This book succeeds Numerical Methods in Engineering with Python, which was based on Python 2. As the title implies, the new edition migrates to Python 3. Because the two versions are not entirely compatible, almost all computer routines required some code changes.  We also took the opportunity to make a few changes in the material covered:    An introduction to the Python plotting package matplotlib.pyplot was added to Chapter 1. This package is used in numerous example problems, mak- ing the book more graphics oriented than before.   The function plotPoly, which plots data points and the corresponding polyno- mial interpolant, was added to Chapter 3. This program provides a convenient means of evaluating the ﬁt of the interpolant.   At the suggestion of reviewers, the Taylor series method of solving initial value   The Jacobi method for solving eigenvalue problems in Chapter 9 now uses the threshold method in choosing the matrix elements marked for elimination. This change increases the speed of the algorithm.   The adaptive Runge-Kutta method in Chapter 7 was recoded, and the Cash-Karp coefﬁcients replaced with the Dormand-Prince coefﬁcients. The result is a more efﬁcient algorithm with tighter error control.  problems in Chapter 7 was dropped. It was replaced by Euler’s method.   xi  Preface   Twenty-one new problems were introduced, most of them replacing old prob-   Some example problems in Chapters 4 and 7 were rearranged or replaced with new problems. The result of these changes is better coordination of examples with the text.  lems.  The programs listed in the book were tested with Python 3.2 under Windows 7.  The source codes are available at www.cambridge.org kiusalaaspython.    1  Introduction to Python  1.1  General Information  Quick Overview  This chapter is not a comprehensive manual of Python. Its sole aim is to provide suf- ﬁcient information to give you a good start if you are unfamiliar with Python. If you know another computer language, and we assume that you do, it is not difﬁcult to pick up the rest as you go.  Python is an object-oriented language that was developed in the late 1980s as a scripting language  the name is derived from the British television series, Monty Python’s Flying Circus . Although Python is not as well known in engineering circles as are some other languages, it has a considerable following in the programming com- munity. Python may be viewed as an emerging language, because it is still being de- veloped and reﬁned. In its current state, it is an excellent language for developing engineering applications.  Python programs are not compiled into machine code, but are run by an interpreter.1 The great advantage of an interpreted language is that programs can be tested and debugged quickly, allowing the user to concentrate more on the principles behind the program and less on the programming itself. Because there is no need to compile, link, and execute after each correction, Python programs can be developed in much shorter time than equivalent Fortran or C programs. On the negative side, interpreted programs do not produce stand-alone applications. Thus a Python pro- gram can be run only on computers that have the Python interpreter installed.  Python has other advantages over mainstream languages that are important in a  learning environment:   Python is an open-source software, which means that it is free; it is included in   Python is available for all major operating systems  Linux, Unix, Windows, Mac OS, and so on . A program written on one system runs without modiﬁcation on all systems.  most Linux distributions.  1 The Python interpreter also compiles byte code, which helps speed up execution somewhat.  1   2  Introduction to Python   Python is easier to learn and produces more readable code than most languages.   Python and its extensions are easy to install.  Development of Python has been clearly inﬂuenced by Java and C++, but there is also a remarkable similarity to MATLABR  another interpreted language, very popular in scientiﬁc computing . Python implements the usual concepts of object-oriented languages such as classes, methods, inheritance etc. We do not use object-oriented programming in this text. The only object that we need is the N-dimensional array available in the module numpy  this module is discussed later in this chapter .  To get an idea of the similarities and differences between MATLAB and Python, let us look at the codes written in the two languages for solution of simultaneous equations Ax = b by Gauss elimination. Do not worry about the algorithm itself  it is explained later in the text , but concentrate on the semantics. Here is the function written in MATLAB:  a i,k+1:n  = a i,k+1:n  - lam*a k,k+1:n ;  b i = b i  - lam*b k ;  b k  =  b k  - a k,k+1:n *b k+1:n   a k,k ;  function x = gaussElimin a,b   n = length b ;  for k = 1:n-1  for i= k+1:n  if a i,k  ˜= 0  lam = a i,k  a k,k ;  end  end  end  for k = n:-1:1  end  x = b;  The equivalent Python function is  from numpy import dot  def gaussElimin a,b :  n = len b   for k in range 0,n-1 :  for i in range k+1,n :  if a[i,k] != 0.0:  lam = a [i,k] a[k,k]  a[i,k+1:n] = a[i,k+1:n] - lam*a[k,k+1:n]  b[i] = b[i] - lam*b[k]  for k in range n-1,-1,-1 :  b[k] =  b[k] - dot a[k,k+1:n],b[k+1:n]   a[k,k]  return b   3  1.1 General Information  The command from numpy import dot instructs the interpreter to load the function dot  which computes the dot product of two vectors  from the module numpy. The colon  :  operator, known as the slicing operator in Python, works the same way as it does in MATLAB and Fortran90—it deﬁnes a slice of an array. The statement for k = 1:n-1 in MATLAB creates a loop that is executed with k = 1, 2, . . . , n − 1. The same loop appears in Python as for k in range n-1 . Here the function range n-1  creates the sequence [0, 1, . . . , n − 2]; k then loops over the elements of the sequence. The differences in the ranges of k reﬂect the native offsets used for arrays. In Python all sequences have zero offset, meaning that the index of the ﬁrst element of the sequence is always 0. In contrast, the native offset in MATLAB is 1.  Also note that Python has no end statements to terminate blocks of code  loops, subroutines, and so on . The body of a block is deﬁned by its indentation; hence in- dentation is an integral part of Python syntax.  Like MATLAB, Python is case sensitive. Thus the names n and N would represent  different objects.  Obtaining Python  The Python interpreter can be downloaded from  http :   www.python.org getit  It normally comes with a nice code editor called Idle that allows you to run programs directly from the editor. If you use Linux, it is very likely that Python is already in- stalled on your machine. The download includes two extension modules that we use in our programs: the numpy module that contains various tools for array operations, and the matplotlib graphics module utilized in plotting.  The Python language is well documented in numerous publications. A com- mendable teaching guide is Python by Chris Fehly  Peachpit Press, CA, 2nd ed. . As a reference, Python Essential Reference by David M. Beazley  Addison-Wesley, 4th ed.  is highly recommended. Printed documentation of the extension modules is scant. However, tutorials and examples can be found on various websites. Our favorite ref- erence for numpy is  http:  www.scipy.org Numpy Example List  For matplotlib we rely on  http:  matplotlib.sourceforge.net contents.html  If you intend to become a serious Python programmer, you may want to acquire A Primer on Scientiﬁc Programming with Python by Hans P. Langtangen  Springer- Verlag, 2009 .   4  Introduction to Python  1.2  Core Python  Variables  In most computer languages the name of a variable represents a value of a given type stored in a ﬁxed memory location. The value may be changed, but not the type. This is not so in Python, where variables are typed dynamically. The following interac- tive session with the Python interpreter illustrates this feature  >>> is the Python prompt :  >>> b = 2   b is integer type  >>> b = b*2.0   Now b is float type  >>> print b   2  4.0  >>> print b   The assignment b = 2 creates an association between the name b and the in- teger value 2. The next statement evaluates the expression b*2.0 and associates the result with b; the original association with the integer 2 is destroyed. Now b refers to the ﬂoating point value 4.0.  The pound sign    denotes the beginning of a comment—all characters between   and the end of the line are ignored by the interpreter.  Strings  A string is a sequence of characters enclosed in single or double quotes. Strings are concatenated with the plus  +  operator, whereas slicing  :  is used to extract a por- tion of the string. Here is an example:  >>> string1 = ’Press return to exit’  >>> string2 = ’the program’  >>> print string1 + ’ ’ + string2    Concatenation  Press return to exit the program  >>> print string1[0:12]    Slicing  Press return  A string can be split into its component parts using the split command. The  components appear as elements in a list. For example,  >>> s = ’3 9 81’  [’3’, ’9’, ’81’]  >>> print s.split      Delimiter is white space  A string is an immutable object—its individual characters cannot be modiﬁed with an assignment statement, and it has a ﬁxed length. An attempt to violate im- mutability will result in TypeError, as follows:   5  1.2 Core Python  >>> s = ’Press return to exit’  >>> s[0] = ’p’  Traceback  most recent call last :  File ’’ ’’, line 1, in ?  s[0] = ’p’  TypeError: object doesn’t support item assignment  Tuples  A tuple is a sequence of arbitrary objects separated by commas and enclosed in parentheses. If the tuple contains a single object, a ﬁnal comma is required; for example, x =  2, . Tuples support the same operations as strings; they are also immutable. Here is an example where the tuple rec contains another tuple  6,23,68 :  >>> rec =  ’Smith’,’John’, 6,23,68     This is a tuple  >>> lastName,firstName,birthdate = rec   Unpacking the tuple  >>> print firstName   >>> birthYear = birthdate[2]  >>> print birthYear   >>> name = rec[1] + ’ ’ + rec[0]  >>> print name   John Smith  >>> print rec[0:2]    ’Smith’, ’John’   John  68  Lists  A list is similar to a tuple, but it is mutable, so that its elements and length can be changed. A list is identiﬁed by enclosing it in brackets. Here is a sampling of opera- tions that can be performed on lists:  >>> a = [1.0, 2.0, 3.0]   Create a list  >>> a.append 4.0    Append 4.0 to list  >>> a.insert 0,0.0    Insert 0.0 in position 0  >>> print a   [1.0, 2.0, 3.0, 4.0]  >>> print a   [0.0, 1.0, 2.0, 3.0, 4.0]  5  >>> print len a     Determine length of list  >>> a[2:4] = [1.0, 1.0, 1.0]  Modify selected elements  >>> print a   [0.0, 1.0, 1.0, 1.0, 1.0, 4.0]   6  Introduction to Python  If a is a mutable object, such as a list, the assignment statement b = a does not result in a new object b, but simply creates a new reference to a. Thus any changes made to b will be reﬂected in a. To create an independent copy of a list a, use the statement c = a[:], as shown in the following example:   ’b’ is an alias of ’a’   Change ’b’  >>> a = [1.0, 2.0, 3.0]  >>> b = a  >>> b[0] = 5.0  >>> print a   >>> c = a[:]  >>> c[0] = 1.0  >>> print a   [5.0, 2.0, 3.0]   The change is reflected in ’a’   ’c’ is an independent copy of ’a’   Change ’c’  [5.0, 2.0, 3.0]   ’a’ is not affected by the change  Matrices can be represented as nested lists, with each row being an element of  the list. Here is a 3 × 3 matrix a in the form of a list:  >>> a = [[1, 2, 3], \  [4, 5, 6], \  [7, 8, 9]]  [4, 5, 6]  6  >>> print a[1]    Print second row  element 1   >>> print a[1][2]    Print third element of second row  The backslash  \  is Python’s continuation character. Recall that Python se- quences have zero offset, so that a[0] represents the ﬁrst row, a[1] the second row, etc. With very few exceptions we do not use lists for numerical arrays. It is much more convenient to employ array objects provided by the numpy module. Array objects are discussed later.  Arithmetic Operators  Python supports the usual arithmetic operators:  + Addition − Subtraction ∗ Multiplication   Division ∗∗ % Modular division  Exponentiation  Some of these operators are also deﬁned for strings and sequences as follows:  >>> s = ’Hello ’  >>> t = ’to you’   7  1.2 Core Python  >>> a = [1, 2, 3]  >>> print 3*s   Hello Hello Hello  >>> print 3*a   [1, 2, 3, 4, 5]  >>> print s + t   Hello to you  [1, 2, 3, 1, 2, 3, 1, 2, 3]  >>> print a + [4, 5]    Append elements   Repetition   Repetition   Concatenation  >>> print 3 + s    This addition makes no sense  Traceback  most recent call last :  File " ", line 1, in    print 3 + s   TypeError: unsupported operand type s  for +: ’int’ and ’str’  Python also has augmented assignment operators, such as a+ = b, that are famil- iar to the users of C. The augmented operators and the equivalent arithmetic expres- sions are shown in following table.  a += b  a -= b  a *= b  a  = b  a = a + b  a = a - b  a = a*b  a = a b  a **= b  a = a**b  a %= b  a = a%b  Less than or equal to  < Less than > Greater than <= >= Greater than or equal to == != Not equal to  Equal to  Comparison Operators  The comparison  relational  operators return True or False. These operators are  Numbers of different type  integer, ﬂoating point, and so on  are converted to a common type before the comparison is made. Otherwise, objects of different type are considered to be unequal. Here are a few examples:  >>> a = 2   Integer  >>> b = 1.99   Floating point  >>> c = ’2’   String  >>> print a > b   True   8  Introduction to Python  >>> print a == c   >>> print  a > b  and  a != c    >>> print  a > b  or  a == b    False  True  True  Conditionals  The if construct  if condition:  block  elif condition:  block  else:  block  executes a block of statements  which must be indented  if the condition returns True. If the condition returns False, the block is skipped. The if conditional can be followed by any number of elif  short for “else if”  constructs  that work in the same manner. The else clause  can be used to deﬁne the block of statements that are to be executed if none of the if-elif clauses are true. The function sign of a illustrates the use of the condi- tionals.  def sign_of_a a :  if a < 0.0:  sign = ’negative’  elif a > 0.0:  sign = ’positive’  else:  sign = ’zero’  return sign  a = 1.5  print ’a is ’ + sign_of_a a    Running the program results in the output  a is positive   9  1.2 Core Python  Loops  The while construct  executes a block of  indented  statements if the condition is True. After execution of the block, the condition is evaluated again. If it is still True, the block is exe- cuted again. This process is continued until the condition becomes False. The else clause  while condition:  block  else:  block  can be used to deﬁne the block of statements that are to be executed if the condition is false. Here is an example that creates the list [1, 1 2, 1 3, . . .]:  nMax = 5  n = 1  a = []  while n < nMax:  n = n + 1  print a    Create empty list  a.append 1.0 n    Append element to list  The output of the program is  [1.0, 0.5, 0.33333333333333331, 0.25]  We met the for statement in Section 1.1. This statement requires a target and a  sequence over which the target loops. The form of the construct is  for tar get in sequence:  block  You may add an else clause that is executed after the for loop has ﬁnished.  The previous program could be written with the for construct as  nMax = 5  a = []  for n in range 1,nMax :  a.append 1.0 n   print a   Here n is the target, and the range object [1, 2, . . . , nMax − 1]  created by calling  the range function  is the sequence.  Any loop can be terminated by the  break   10  Introduction to Python  statement. If there is an else cause associated with the loop, it is not executed. The following program, which searches for a name in a list, illustrates the use of break and else in conjunction with a for loop:  list = [’Jack’, ’Jill’, ’Tim’, ’Dave’]  name = eval input ’Type a name: ’     Python input prompt  print name,’is number’,i + 1,’on the list’   for i in range len list  :  if list[i] == name:  break  else:  print name,’is not on the list’   Here are the results of two searches:  Type a name: ’Tim’  Tim is number 3 on the list  Type a name: ’June’  June is not on the list  The  continue  statement allows us to skip a portion of an iterative loop. If the interpreter encounters the continue statement, it immediately returns to the beginning of the loop without executing the statements that follow continue. The following example compiles a list of all numbers between 1 and 99 that are divisible by 7.  x = []   Create an empty list  for i in range 1,100 :  if i%7 != 0: continue   If not divisible by 7, skip rest of loop  x.append i    Append i to the list  print x   The printout from the program is  [7, 14, 21, 28, 35, 42, 49, 56, 63, 70, 77, 84, 91, 98]  Type Conversion  If an arithmetic operation involves numbers of mixed types, the numbers are automatically converted to a common type before the operation is carried out.   11  1.2 Core Python  Type conversions can also achieved by the following functions:  int a   float a   Converts a to integer Converts a to ﬂoating point Converts to complex a + 0j complex a,b  Converts to complex a + bj  complex a   These functions also work for converting strings to numbers as long as the lit- eral in the string represents a valid number. Conversion from a ﬂoat to an integer is carried out by truncation, not by rounding off. Here are a few examples:  >>> a = 5  >>> b = -3.6  >>> d = ’4.0’  >>> print a + b   >>> print int b    1.4  -3  4.0  >>> print complex a,b     5-3.6j   >>> print float d    >>> print int d     This fails: d is a string  Traceback  most recent call last :  File " ", line 1, in    print int d    ValueError: invalid literal for int   with base 10: ’4.0’  Mathematical Functions  Core Python supports only the following mathematical functions:  abs a  max sequence  min sequence  round a,n   Absolute value of a Largest element of sequence Smallest element of sequence Round a to n decimal places  ⎧⎪⎨ ⎪⎩  −1 if a < b 0 if a = b 1 if a > b  cmp a,b   Returns  The majority of mathematical functions are available in the math module.  Reading Input  The intrinsic function for accepting user input is  input prompt    12  Introduction to Python  It displays the prompt and then reads a line of input that is converted to a string. To convert the string into a numerical value use the function  eval string   The following program illustrates the use of these functions:  a = input ’Input a: ’   print a, type a     Print a and its type  b = eval a   print b,type b     Print b and its type  The function type a  returns the type of the object a; it is a very useful tool in  debugging. The program was run twice with the following results:  Input a: 10.0  10.0    10.0    Input a: 11**2  11**2    121    A convenient way to input a number and assign it to the variable a is  a = eval input prompt    Printing Output  Output can be displayed with the print function  print object1, object2, . . .   that converts object1, object2, and so on, to strings and prints them on the same line, separated by spaces. The newline character ’\n’ can be used to force a new line. For example,  >>> a = 1234.56789  >>> b = [2, 4, 6, 8]  >>> print a,b   1234.56789 [2, 4, 6, 8]  >>> print ’a =’,a, ’\nb =’,b   a = 1234.56789  b = [2, 4, 6, 8]  The print function always appends the newline character to the end of a line. We can replace this character with something else by using the keyword argument end. For example,  replaces \n with a space.  print object1, object2, . . . ,end=’ ’    13  1.2 Core Python  version statement is  Output can be formatted with the format method. The simplest form of the con-  ’{:fmt1}{:fmt2}. . .’.format arg1,arg2,. . .    where fmt1, fmt2,. . . are the format speciﬁcations for arg1, arg2,. . ., respectively. Typ- ically used format speciﬁcations are  wd w.df w.de  Integer Floating point notation Exponential notation  where w is the width of the ﬁeld and d is the number of digits after the decimal point. The output is right justiﬁed in the speciﬁed ﬁeld and padded with blank spaces  there are provisions for changing the justiﬁcation and padding . Here are several examples:  >>> a = 1234.56789  >>> n = 9876  >>> print ’{:7.2f}’.format a    >>> print ’n = {:6d}’.format n     Pad with spaces  >>> print ’n = {:06d}’.format n     Pad with zeros  1234.57  n =  9876  n =009876  >>> print ’{:12.4e} {:6d}’.format a,n    1.2346e+03  9876  Opening and Closing a File  Before a data ﬁle on a storage device  e.g., a disk  can be accessed, you must create a ﬁle object with the command  ﬁle object = open ﬁlename, action   where ﬁlename is a string that speciﬁes the ﬁle to be opened  including its path if necessary  and action is one of the following strings:  Read from an existing ﬁle.  ’r’ ’w’ Write to a ﬁle. If ﬁlename does not exist, it is created. ’a’ ’r+’ Read to and write from an existing ﬁle. ’w+’  Append to the end of the ﬁle.  Same as ’r+’, but ﬁlename is created if it does not exist. Same as ’w+’, but data is appended to the end of the ﬁle.  ’a+’  It is good programming practice to close a ﬁle when access to it is no longer re-  quired. This can be done with the method  ﬁle object.close     14  Introduction to Python  Reading Data from a File  There are three methods for reading data from a ﬁle. The method  ﬁle object.read n   reads n characters and returns them as a string. If n is omitted, all the characters in the ﬁle are read.  If only the current line is to be read, use  ﬁle object.readline n   which reads n characters from the line. The characters are returned in a string that terminates in the newline character \n. Omission of n causes the entire line to be read.  All the lines in a ﬁle can be read using  ﬁle object.readlines    This returns a list of strings, each string being a line from the ﬁle ending with the newline character.  A convenient method of extracting all the lines one by one is to use the loop  As an example, let us assume that we have a ﬁle named sunspots.txt in the working directory. This ﬁle contains daily data of sunspot intensity, each line having the format  year month date intensity , as follows:  for line in ﬁle object:  do something with line  1896 05 26 40.94 1896 05 27 40.58 1896 05 28 40.20  etc.  Our task is to read the ﬁle and create a list x that contains only the intensity. Since each line in the ﬁle is a string, we ﬁrst split the line into its pieces using the split command. This produces a list of strings, such as[’1896’,’05’,’26’,’40.94’]. Then we extract the intensity  element [3] of the list , evaluate it, and append the result to x. Here is the algorithm:  x = [] data = open ’sunspots.txt’,’r’  for line in data:  x.append eval line.split  [3]    data.close     15  1.2 Core Python  Writing Data to a File  The method  ﬁle object.write string   writes a string to a ﬁle, whereas  ﬁle object.writelines list of strings   is used to write a list of strings. Neither method appends a newline character to the end of a line. As an example, let us write a formatted table of k and k2 from k = 101 to 110 to  the ﬁle testfile. Here is the program that does the writing:  f.write ’{:4d} {:6d}’.format k,k**2    f = open ’testfile’,’w’   for k in range 101,111 :  f.write ’\n’   f.close    The contents of testfile are  101  102  103  104  105  106  107  108  109  110  10201  10404  10609  10816  11025  11236  11449  11664  11881  12100  to a ﬁle object:  Error Control  The print function can also be used to write to a ﬁle by redirecting the output  print object1, object2, . . . ,file = ﬁle object   Apart from the redirection, this works just like the regular print function.  When an error occurs during execution of a program an exception is raised and the program stops. Exceptions can be caught with try and except statements:  try:  do something  except error:  do something else   16  Introduction to Python  where error is the name of a built-in Python exception. If the exception error is not raised, the try block is executed; otherwise the execution passes to the except block. All exceptions can be caught by omitting error from the except statement.  The following statement raises the exception ZeroDivisionError:  >>> c = 12.0 0.0  Traceback  most recent call last :  File " ", line 1, in    c=12.0 0.0  ZeroDivisionError: float division by zero  This error can be caught by  try:  c = 12.0 0.0  except ZeroDivisionError:  print ’Division by zero’   1.3  Functions and Modules  Functions  The structure of a Python function is  def func name param1, param2,. . . :  statements return return values  where param1, param2,. . . are the parameters. A parameter can be any Python ob- ject, including a function. Parameters may be given default values, in which case the parameter in the function call is optional. If the return statement or return values are omitted, the function returns the null object.  The following function computes the ﬁrst two derivatives of f  x  by ﬁnite  differences:  def derivatives f,x,h=0.0001 :   h has a default value  df = f x+h  - f x-h    2.0*h   ddf = f x+h  - 2.0*f x  + f x-h   h**2  return df,ddf  Let us now use this function to determine the two derivatives of arctan x  at  x = 0.5:  from math import atan  df,ddf = derivatives atan,0.5    Uses default value of h  print ’First derivative  =’,df   print ’Second derivative =’,ddf    Note that atan is passed to derivatives as a parameter. The output from the  17  1.3 Functions and Modules  program is  First derivative  = 0.799999999573  Second derivative = -0.639999991892  The number of input parameters in a function deﬁnition may be left arbitrary.  For example, in the following function deﬁnition  def func x1,x2,*x3   x1 and x2 are the usual parameters, also called positional parameters, whereas x3 is a tuple of arbitrary length containing the excess parameters. Calling this function with  func a,b,c,d,e   results in the following correspondence between the parameters: a ←→ x1, b ←→ x2,  c,d,e  ←→ x3  The positional parameters must always be listed before the excess parameters.  If a mutable object, such as a list, is passed to a function where it is modiﬁed, the  changes will also appear in the calling program. An example follows:  def squares a :  for i in range len a  :  a[i] = a[i]**2  a = [1, 2, 3, 4]  squares a   The output is  [1, 4, 9, 16]  Lambda Statement  print a    ’a’ now contains ’a**2’  Multiple statements are not allowed.  Here is an example:  >>> c = lambda x,y : x**2 + y**2  >>> print c 3,4    25  If the function has the form of an expression, it can be deﬁned with the lambda statement  func name = lambda param1, param2,...: expression   18  Introduction to Python  Modules  It is sound practice to store useful functions in modules. A module is simply a ﬁle where the functions reside; the name of the module is the name of the ﬁle. A module can be loaded into a program by the statement  from module name import *  Python comes with a large number of modules containing functions and meth- ods for various tasks. Some of the modules are described brieﬂy in the next two sec- tions. Additional modules, including graphics packages, are available for download- ing on the Web.  1.4 Mathematics Modules  math Module  Most mathematical functions are not built into core Python, but are available by load- ing the math module. There are three ways of accessing the functions in a module. The statement  from math import *  loads all the function deﬁnitions in the math module into the current function or module. The use of this method is discouraged because it is not only wasteful but can also lead to conﬂicts with deﬁnitions loaded from other modules. For example, there are three different deﬁnitions of the sine function in the Python modules math, cmath, and numpy. If you have loaded two or more of these modules, it is unclear which deﬁnition will be used in the function call sin x  it is the deﬁnition in the module that was loaded last .  A safer but by no means foolproof method is to load selected deﬁnitions with the  statement  from math import func1, func2, . . .  as illustrated as follows:  >>> from math import log,sin  >>> print log sin 0.5     -0.735166686385  Conﬂicts can be avoided altogether by ﬁrst making the module accessible with  the statement  import math  and then accessing the deﬁnitions in the module by using the module name as a preﬁx. Here is an example:  >>> print math.log math.sin 0.5     >>> import math  -0.735166686385   19  1.4 Mathematics Modules  A module can also be made accessible under an alias. For example, the math  module can be made available under the alias m with the command  import math as m  Now the preﬁx to be used is m rather than math:  >>> import math as m  >>> print m.log m.sin 0.5     -0.735166686385  The contents of a module can be printed by calling dir module . Here is how  to obtain a list of the functions in the math module:  >>> import math  >>> dir math   [’__doc__’, ’__name__’, ’acos’, ’asin’, ’atan’,  ’atan2’, ’ceil’, ’cos’, ’cosh’, ’e’, ’exp’, ’fabs’,  ’floor’, ’fmod’, ’frexp’, ’hypot’, ’ldexp’, ’log’,  ’log10’, ’modf’, ’pi’, ’pow’, sign’, sin’, ’sinh’,  ’sqrt’, ’tan’, ’tanh’]  Most of these functions are familiar to programmers. Note that the module in-  cludes two constants: π and e.  cmath Module  The cmath module provides many of the functions found in the math module, but these functions accept complex numbers. The functions in the module are  [’__doc__’, ’__name__’, ’acos’, ’acosh’, ’asin’, ’asinh’,  ’atan’, ’atanh’, ’cos’, ’cosh’, ’e’, ’exp’, ’log’,  ’log10’, ’pi’, ’sin’, ’sinh’, ’sqrt’, ’tan’, ’tanh’]  Here are examples of complex arithmetic:  >>> from cmath import sin  >>> x = 3.0 -4.5j  >>> y = 1.2 + 0.8j  >>> z = 0.8  >>> print x y    -2.56205313375e-016-3.75j   >>> print sin x     6.35239299817+44.5526433649j   >>> print sin z     0.7173560909+0j    20  Introduction to Python  1.5  numpy Module  General Information  The numpy module2 is not a part of the standard Python release. As pointed out ear- lier, it must be installed separately  the installation is very easy . The module intro- duces array objects that are similar to lists, but can be manipulated by numerous functions contained in the module. The size of an array is immutable, and no empty elements are allowed.  The complete set of functions in numpy is far too long to be printed in its entirety.  The following list is limited to the most commonly used functions.  [’complex’, ’float’, ’abs’, ’append’, arccos’,  ’arccosh’, ’arcsin’, ’arcsinh’, ’arctan’, ’arctan2’,  ’arctanh’, ’argmax’, ’argmin’, ’cos’, ’cosh’, ’diag’,  ’diagonal’, ’dot’, ’e’, ’exp’, ’floor’, ’identity’,  ’inner, ’inv’, ’log’, ’log10’, ’max’, ’min’,  ’ones’, ’outer’, ’pi’, ’prod’ ’sin’, ’sinh’, ’size’,  ’solve’, ’sqrt’, ’sum’, ’tan’, ’tanh’, ’trace’,  ’transpose’, ’vectorize’,’zeros’]  Creating an Array  Arrays can be created in several ways. One of them is to use the array function to turn a list into an array:  Following are two examples of creating a 2 × 2 array with ﬂoating-point elements:  array list,type   >>> from numpy import array  >>> a = array [[2.0, -1.0],[-1.0, 3.0]]   >>> print a   [[ 2. -1.]  [-1.  3.]]  >>> print b   [[ 2. -1.]  [-1.  3.]]  >>> b = array [[2, -1],[-1, 3]],float   Other available functions are  which creates a dim1 × dim2 array and ﬁlls it with zeroes, and  zeros  dim1,dim2 ,type   ones  dim1,dim2 ,type   which ﬁlls the array with ones. The default type in both cases is float.  2 NumPy is the successor of older Python modules called Numeric and NumArray. Their interfaces and capabilities are very similar. Although Numeric and NumArray are still available, they are no longer supported.   21  1.5 numpy Module  Finally, there is the function  arange from,to,increment   which works just like the range function, but returns an array rather than a se- quence. Here are examples of creating arrays:  >>> from numpy import *  >>> print arange 2,10,2    [2 4 6 8]  >>> print arange 2.0,10.0,2.0    [ 2.  4.  6.  8.]  >>> print zeros 3    [ 0.  0.  0.]  >>> print zeros  3 ,int    >>> print ones  2,2     [0 0 0]  [[ 1.  1.]  [ 1.  1.]]  Accessing and Changing Array Elements  If a is a rank-2 array, then a[i,j] accesses the element in row i and column j, whereas a[i] refers to row i. The elements of an array can be changed by assign- ment as follows:  >>> from numpy import *  >>> a = zeros  3,3 ,int   >>> a[0] = [2,3,2]   Change a row  >>> a[1,1] = 5   Change an element  >>> a[2,0:2] = [8,-3]   Change part of a row  >>> print a   [[0 0 0]  [0 0 0]  [0 0 0]]  >>> print a   [[ 2 3  [ 0 5  2]  0]  [ 8 -3  0]]  Operations on Arrays  Arithmetic operators work differently on arrays than they do on tuples and lists—the operation is broadcast to all the elements of the array; that is, the operation is applied to each element in the array. Here are examples:  >>> from numpy import array  >>> a = array [0.0, 4.0, 9.0, 16.0]    The mathematical functions available in numpy are also broadcast, as follows:  [ 0.84147098 -0.7568025  0.41211849 -0.28790332]  Functions imported from the math module will work on the individual elements,  of course, but not on the array itself. An example follows:  22  Introduction to Python  >>> print a 16.0   [ 0.  0.25  0.5625  1.  ]  >>> print a - 4.0   [ -4.  0.  5.  12.]  >>> from numpy import array,sqrt,sin  >>> a = array [1.0, 4.0, 9.0, 16.0]   >>> print sqrt a    [ 1.  2.  3.  4.]  >>> print sin a    >>> from numpy import array  >>> from math import sqrt  >>> a = array [1.0, 4.0, 9.0, 16.0]   >>> print sqrt a[1]    >>> print sqrt a    Traceback  most recent call last :  2.0  ...  TypeError: only length-1 arrays can be converted to Python scalars  Array Functions  There are numerous functions in numpy that perform array operations and other use- ful tasks. Here are a few examples:  >>> from numpy import *  >>> A = array [[4,-2,1],[-2,4,-2],[1,-2,3]],float   >>> b = array [1,4,3],float   >>> print diagonal A     Principal diagonal  >>> print diagonal A,1     First subdiagonal  >>> print trace A     Sum of diagonal elements  >>> print argmax b     Index of largest element  [ 4.  4.  3.]  [-2. -2.]  11.0  1  [1 0 1]  >>> print argmin A,axis=0     Indices of smallest col. elements   >>> print identity 3     Identity matrix  23  1.5 numpy Module  [[ 1.  [ 0.  [ 0.  0.  1.  0.  0.]  0.]  1.]]  There are three functions in numpy that compute array products. They are illus-  trated by the following program. For more details, see Appendix A2.  from numpy import *  x = array [7,3]   y = array [2,1]   A = array [[1,2],[3,2]]   B = array [[1,1],[2,2]]    Dot product  print "dot x,y  =\n",dot x,y     {x}.{y}  print "dot A,x  =\n",dot A,x    print "dot A,B  =\n",dot A,B     [A]{x}   [A][B]   Inner product  print "inner x,y  =\n",inner x,y     {x}.{y}  print "inner A,x  =\n",inner A,x     [A]{x}  print "inner A,B  =\n",inner A,B     [A][B_transpose]   Outer product  print "outer x,y  =\n",outer x,y    print "outer A,x  =\n",outer A,x    print "outer A,B  =\n",outer A,B    The output of the program is  dot x,y  =  17  dot A,x  =  [13 27]  dot A,B  =  [[5 5]  [7 7]]  inner x,y  =  17  inner A,x  =  [13 27]  inner A,B  =  [[ 3 6]  [ 5 10]]  outer x,y  =   24  Introduction to Python  [[14  7]  [ 6  3]]  outer A,x  =  [[ 7  [14  [21  [14  3]  6]  9]  6]]  Outer A,B  =  [[1 1 2 2]  [2 2 4 4]  [3 3 6 6]  [2 2 4 4]]  Linear Algebra Module  The numpy module comes with a linear algebra module called linalg that contains routine tasks such as matrix inversion and solution of simultaneous equations. For example,  >>> from numpy import array  >>> from numpy.linalg import inv,solve  >>> A = array [[ 4.0, -2.0,  1.0], \  [-2.0,  4.0, -2.0], \  [ 1.0, -2.0,  3.0]]   >>> b = array [1.0, 4.0, 2.0]   >>> print inv A     Matrix inverse  [[ 0.33333333  0.16666667  0.  [ 0.16666667  0.45833333  0.25  [ 0.  0.25  0.5  ]  ]  ]]   Solve [A]{x} = {b}  >>> print solve A,b    [ 1. ,  2.5,  2. ]  Copying Arrays  We explained earlier that if a is a mutable object, such as a list, the assignment state- ment b = a does not result in a new object b, but simply creates a new reference to a, called a deep copy. This also applies to arrays. To make an independent copy of an array a, use the copy method in the numpy module:  b = a.copy    Vectorizing Algorithms  Sometimes the broadcasting properties of the mathematical functions in the numpy module can be used to replace loops in the code. This procedure is known as   25  1.6 Plotting with matplotlib.pyplot  vectorization. Consider, for example, the expression   cid:7   s = 100 cid:6   i=0  iπ 100  sin  iπ 100  The direct approach is to evaluate the sum in a loop, resulting in the following  ”scalar” code:  from math import sqrt,sin,pi  x = 0.0; s = 0.0  for i in range 101 :  s = s + sqrt x *sin x   x = x + 0.01*pi  print s   The vectorized version of the algorithm is  from numpy import sqrt,sin,arange  from math import pi  x = arange 0.0, 1.001*pi, 0.01*pi   print sum sqrt x *sin x     Note that the ﬁrst algorithm uses the scalar versions of sqrt and sin functions in the math module, whereas the second algorithm imports these functions from numpy. The vectorized algorithm executes much faster, but uses more memory.  1.6  Plotting with matplotlib.pyplot  The module matplotlib.pyplot is a collection of 2D plotting functions that pro- vide Python with MATLAB-style functionality. Not being a part of core Python, it requires separate installation. The following program, which plots sine and cosine functions, illustrates the application of the module to simple xy plots.  import matplotlib.pyplot as plt  from numpy import arange,sin,cos  x = arange 0.0,6.2,0.2   plt.plot x,sin x ,’o-’,x,cos x ,’ˆ-’    Plot with specified  plt.xlabel ’x’   plt.legend  ’sine’,’cosine’ ,loc = 0    Add legend in loc. 3  plt.grid True    Add coordinate grid  plt.savefig ’testplot.png’,format=’png’   Save plot in png   line and marker style   Add label to x-axis  plt.show    input "\nPress return to exit"    format for future use   Show plot on screen   26  Introduction to Python  The line and marker styles are speciﬁed by the string characters shown in the  following table  only some of the available characters are shown .  Solid line  ’-’ ’--’ Dashed line ’-.’ Dash-dot line ’:’  Dotted line Circle marker Triangle marker Square marker Hexagon marker x marker  ’o’  ’ˆ’  ’s’  ’h’  ’x’  0  1  2  3  4  ”Best” location Upper right Upper left Lower left Lower right  Some of the location  loc  codes for placement of the legend are  Running the program produces the following screen:   27  1.6 Plotting with matplotlib.pyplot  It is possible to have more than one plot in a ﬁgure, as demonstrated by the fol-  lowing code:  import matplotlib.pyplot as plt  from numpy import arange,sin,cos  x = arange 0.0,6.2,0.2   plt.xlabel ’x’ ;plt.ylabel ’sin x ’   plt.subplot 2,1,1   plt.plot x,sin x ,’o-’   plt.grid True   plt.subplot 2,1,2   plt.plot x,cos x ,’ˆ-’   plt.xlabel ’x’ ;plt.ylabel ’cos x ’   plt.grid True   plt.show    input "\nPress return to exit"   The command subplot rows,cols,plot number establishes a subplot window within the current ﬁgure. The parameters row and col divide the ﬁgure into row × col grid of subplots  in this case, two rows and one column . The commas between the parameters may be omitted. The output from this above program is   28  Introduction to Python  1.7  Scoping of Variables  Namespace is a dictionary that contains the names of the variables and their values. Namespaces are automatically created and updated as a program runs. There are three levels of namespaces in Python:  1. Local namespace is created when a function is called. It contains the variables passed to the function as arguments and the variables created within the func- tion. The namespace is deleted when the function terminates. If a variable is cre- ated inside a function, its scope is the function’s local namespace. It is not visible outside the function.  2. A global namespace is created when a module is loaded. Each module has its own namespace. Variables assigned in a global namespace are visible to any function within the module.  3. A built-in namespace is created when the interpreter starts. It contains the func- tions that come with the Python interpreter. These functions can be accessed by any program unit.  When a name is encountered during execution of a function, the interpreter tries to resolve it by searching the following in the order shown:  1  local namespace,  2  global namespace, and  3  built-in namespace. If the name cannot be resolved, Python raises a NameError exception.  Because the variables residing in a global namespace are visible to functions within the module, it is not necessary to pass them to the functions as arguments  although it is good programming practice to do so , as the following program illustrates:  def divide  :  c = a b  print ’a b =’,c   a = 100.0  b = 5.0  divide    a b = 20.0  def divide  :  c = a b  a = 100.0  b = 5.0  divide    print ’a b =’,c   Note that the variable c is created inside the function divide and is thus not accessible to statements outside the function. Hence an attempt to move the print statement out of the function fails:   29  1.8 Writing and Running Programs  Traceback  most recent call last :  File "C:\Python32\test.py", line 6, in    print ’a b =’,c   NameError: name ’c’ is not defined  1.8 Writing and Running Programs  When the Python editor Idle is opened, the user is faced with the prompt >>>, in- dicating that the editor is in interactive mode. Any statement typed into the editor is immediately processed on pressing the enter key. The interactive mode is a good way both to learn the language by experimentation and to try out new programming ideas.  Opening a new window places Idle in the batch mode, which allows typing and saving of programs. One can also use a text editor to enter program lines, but Idle has Python-speciﬁc features, such as color coding of keywords and automatic inden- tation, which make work easier. Before a program can be run, it must be saved as a Python ﬁle with the .py extension  e.g., myprog.py . The program can then be exe- cuted by typing python myprog.py; in Windows; double-clicking on the program icon will also work. But beware: The program window closes immediately after exe- cution, before you get a chance to read the output. To prevent this from happening, conclude the program with the line  input ’press return’   Double-clicking the program icon also works in Unix and Linux if the ﬁrst line of the program speciﬁes the path to the Python interpreter  or a shell script that provides a link to Python . The path name must be preceded by the sym- bols !. On my computer the path is  usr bin python, so that all my programs start with the line ! usr bin python. On multi-user systems the path is usually  usr local bin python.  When a module is loaded into a program for the ﬁrst time with the import state- ment, it is compiled into bytecode and written in a ﬁle with the extension .pyc. The next time the program is run, the interpreter loads the bytecode rather than the origi- nal Python ﬁle. If in the meantime changes have been made to the module, the mod- ule is automatically recompiled. A program can also be run from Idle using Run Run Module menu.  It is a good idea to document your modules by adding a docstring at the begin- ning of each module. The docstring, which is enclosed in triple quotes, should ex- plain what the module does. Here is an example that documents the module error  we use this module in several of our programs :   module error  ’’’ err string .  Prints ’string’ and terminates program.   30  Introduction to Python  ’’’  import sys  def err string :  print string   input ’Press return to exit’   sys.exit    The docstring of a module can be printed with the statement  print module name. doc    For example, the docstring of error is displayed by  >>> import error  >>> print error.__doc__   err string .  Prints ’string’ and terminates program.  Avoid backslashes in the docstring because they confuse the Python 3 inter-  preter.   2  Systems of Linear Algebraic Equations  Solve the simultaneous equations Ax = b.  2.1  Introduction  In this chapter we look at the solution of n linear, algebraic equations in n unknowns. It is by far the longest and arguably the most important topic in the book. There is a good reason for its importance—it is almost impossible to carry out numerical anal- ysis of any sort without encountering simultaneous equations. Moreover, equation sets arising from physical problems are often very large, consuming a lot of com- putational resources. It usually possible to reduce the storage requirements and the run time by exploiting special properties of the coefﬁcient matrix, such as sparseness  most elements of a sparse matrix are zero . Hence there are many algorithms dedi- cated to the solution of large sets of equations, each one being tailored to a particular form of the coefﬁcient matrix  symmetric, banded, sparse, and so on . A well-known collection of these routines is LAPACK—Linear Algebra PACKage, originally written in Fortran77.1  We cannot possibly discuss all the special algorithms in the limited space avail- able. The best we can do is to present the basic methods of solution, supplemented by a few useful algorithms for banded coefﬁcient matrices.  Notation  A system of algebraic equations has the form  A 11x1 + A 12x2 + ··· + A 1nxn = b1 A 21x1 + A 22x2 + ··· + A 2nxn = b2 ... A n1x1 + A n2x2 + ··· + A nnxn = bn   2.1   1 LAPACK is the successor of LINPACK, a 1970s and 80s collection of Fortran subroutines.  31   32  Systems of Linear Algebraic Equations  where the coefﬁcients Aij and the constants b j are known, and xi represents the un- knowns. In matrix notation the equations are written as  ⎡ ⎢⎢⎢⎢⎣  A 11 A 12 A 21 A 22 ... ... A n1 A n2  ⎤ ⎥⎥⎥⎥⎦  ⎡ ⎢⎢⎢⎢⎣  ⎤ ⎥⎥⎥⎥⎦ =  ⎡ ⎢⎢⎢⎢⎣  ⎤ ⎥⎥⎥⎥⎦  b1 b2 ... bn  x1 x2 ... xn  ··· A 1n ··· A 2n ... ... ··· A nn  or simply  Ax = b.  A particularly useful representation of the equations for computational purposes is the augmented coefﬁcient matrix obtained by adjoining the constant vector b to the coefﬁcient matrix A in the following fashion:  ⎡ ⎢⎢⎢⎢⎣   cid:15   =   cid:14   A b  ⎤ ⎥⎥⎥⎥⎦  A 11 A 12 A 21 A 22 ... ... A n1 A n2  ··· A 1n b1 ··· A 2n b2 ... ... ··· A n3 bn  ...   2.2    2.3    2.4   Uniqueness of Solution  A system of n linear equations in n unknowns has a unique solution, provided that the determinant of the coefﬁcient matrix is nonsingular; that is, A  cid:7 = 0. The rows and columns of a nonsingular matrix are linearly independent in the sense that no row  or column  is a linear combination of other rows  or columns .  If the coefﬁcient matrix is singular, the equations may have an inﬁnite number of solutions or no solutions at all, depending on the constant vector. As an illustration, take the equations  2x + y = 3  4x + 2y = 6  Since the second equation can be obtained by multiplying the ﬁrst equation by two, any combination of x and y that satisﬁes the ﬁrst equation is also a solution of the second equation. The number of such combinations is inﬁnite. In contrast, the equa- tions  2x + y = 3  4x + 2y = 0  have no solution because the second equation, being equivalent to 2x + y = 0, contradicts the ﬁrst one. Therefore, any solution that satisﬁes one equation cannot satisfy the other one.   33  2.1 Introduction  Ill Conditioning  A <<  cid:8 A cid:8    cid:16  cid:17  cid:17  cid:18  n cid:6   n cid:6   i=1  j=1  A 2 ij   cid:8 A cid:8 e =   cid:19  cid:19   n cid:6    cid:8 A cid:8 ∞ = max 1≤i≤n   cid:19  cid:19 Aij cond A  =  cid:8 A cid:8   cid:20  cid:20 A  cid:20  cid:20   j=1  −1  The obvious question is, What happens when the coefﬁcient matrix is almost singu- lar  i.e., if A is very small . To determine whether the determinant of the coefﬁcient matrix is “small,” we need a reference against which the determinant can be mea- sured. This reference is called the norm of the matrix and is denoted by  cid:8 A cid:8 . We can then say that the determinant is small if  Several norms of a matrix have been deﬁned in existing literature, such as the  Euclidan norm   2.5a    2.5b    2.5c   and the row-sum norm, also called the inﬁnity norm  A formal measure of conditioning is the matrix condition number, deﬁned as  If this number is close to unity, the matrix is well conditioned. The condition number increases with the degree of ill conditioning, reaching inﬁnity for a singular matrix. Note that the condition number is not unique, but depends on the choice of the ma- trix norm. Unfortunately, the condition number is expensive to compute for large matrices. In most cases it is sufﬁcient to gauge conditioning by comparing the deter- minant with the magnitudes of the elements in the matrix.  If the equations are ill conditioned, small changes in the coefﬁcient matrix result  in large changes in the solution. As an illustration, take the equations  2x + y = 3  2x + 1.001y = 0  that have the solution x = 1501.5, y = −3000. Since A = 2 1.001  − 2 1  = 0.002 is much smaller than the coefﬁcients, the equations are ill conditioned. The effect of ill conditioning can veriﬁed by changing the second equation to 2x + 1.002y = 0 and re-solving the equations. The result is x = 751.5, y = −1500. Note that a 0.1% change in the coefﬁcient of y produced a 100% change in the solution!  Numerical solutions of ill-conditioned equations are not to be trusted. The rea- son is that the inevitable roundoff errors during the solution process are equivalent to introducing small changes into the coefﬁcient matrix. This in turn introduces large errors into the solution, the magnitude of which depends on the severity of ill con- ditioning. In suspect cases the determinant of the coefﬁcient matrix should be com- puted so that the degree of ill conditioning can be estimated. This can be done during or after the solution with only a small computational effort.   34  Systems of Linear Algebraic Equations  Linear Systems  Linear, algebraic equations occur in almost all branches of numerical analysis. But their most visible application in engineering is in the analysis of linear systems  any system whose response is proportional to the input is deemed to be linear . Linear systems include structures, elastic solids, heat ﬂow, seepage of ﬂuids, electromag- netic ﬁelds, and electric circuits  i.e., most topics taught in an engineering curri- culum .  If the system is discrete, such as a truss or an electric circuit, then its analysis leads directly to linear algebraic equations. In the case of a statically determinate truss, for example, the equations arise when the equilibrium conditions of the joints are written down. The unknowns x1, x2, . . . , xn represent the forces in the members and the support reactions, and the constants b1, b2, . . . , bn are the prescribed external loads.  The behavior of continuous systems is described by differential equations, rather than algebraic equations. However, because numerical analysis can deal only with discrete variables, it is ﬁrst necessary to approximate a differential equation with a system of algebraic equations. The well-known ﬁnite difference, ﬁnite element, and boundary element methods of analysis work in this manner. They use different ap- proximations to achieve the “discretization,” but in each case the ﬁnal task is the same: solve a system  often a very large system  of linear, algebraic equations. In summary, the modeling of linear systems invariably gives rise to equations of the form Ax = b, where b is the input and x represents the response of the sys- tem. The coefﬁcient matrix A, which reﬂects the characteristics of the system, is in- dependent of the input. In other words, if the input is changed, the equations have to be solved again with a different b, but the same A. Therefore, it is desirable to have an equation-solving algorithm that can handle any number of constant vectors with minimal computational effort.  Methods of Solution  There are two classes of methods for solving systems of linear, algebraic equations: direct and iterative methods. The common characteristic of direct methods is that they transform the original equations into equivalent equations  equations that have the same solution  that can be solved more easily. The transformation is carried out by applying the following three operations. These so-called elementary operations do not change the solution, but they may affect the determinant of the coefﬁcient matrix as indicated in parenthesis. 1. Exchanging two equations  changes sign of A  2. Multiplying an equation by a nonzero constant  multiplies A by the same  3. Multiplying an equation by a nonzero constant and then subtracting it from an-  constant  other equation  leaves A unchanged    35  2.1 Introduction  Iterative, or indirect methods, start with a guess of the solution x and then re- peatedly reﬁne the solution until a certain convergence criterion is reached. Itera- tive methods are generally less efﬁcient than their direct counterparts because of the large number of iterations required. Yet they do have signiﬁcant computational ad- vantages if the coefﬁcient matrix is very large and sparsely populated  most coefﬁ- cients are zero .  Overview of Direct Methods  Table 2.1 lists three popular direct methods, each of which uses elementary opera- tions to produce its own ﬁnal form of easy-to-solve equations.  Method  Gauss elimination LU decomposition  Gauss-Jordan elimination  Initial form Final form Ux = c LUx = b Ix = c  Ax = b Ax = b Ax = b  Table 2.1. Three Popular Direct Methods  In Table 2.1 U represents an upper triangular matrix, L is a lower triangular ma- trix, and I denotes the identity matrix. A square matrix is called triangular if it con- tains only zero elements on one side of the leading diagonal. Thus a 3 × 3 upper tri- angular matrix has the form  and a 3 × 3 lower triangular matrix appears as  Triangular matrices play an important role in linear algebra, because they sim-  plify many computations. For example, consider the equations Lx = c, or  ⎡ ⎢⎣ U11 U12 U13  0 U22 U23 0 0 U33  ⎤ ⎥⎦  U =  ⎡ ⎢⎣ L11  L =  0 0 L21 L22 0 L31 L32 L33  ⎤ ⎥⎦  L11x1 = c1 L21x1 + L22x2 = c2 L31x1 + L32x2 + L33x3 = c3  If we solve the equations forward, starting with the ﬁrst equation, the compu- tations are very easy, because each equation contains only one unknown at a time.   36  Systems of Linear Algebraic Equations  The solution would thus proceed as follows:  x1 = c1 L11 x2 =  c2 − L21x1  L22 x3 =  c3 − L31x1 − L32x2  L33  This procedure is known as forward substitution. In a similar way, Ux = c, en- countered in Gauss elimination, can easily be solved by back substitution, which starts with the last equation and proceeds backward through the equations. The equations LUx = b, which are associated with LU decomposition, can also be solved quickly if we replace them with two sets of equivalent equations: Ly = b and Ux = y. Now Ly = b can be solved for y by forward substitution, followed by the solution of Ux = y by means of back substitution. The equations Ix = c, which are produced by Gauss-Jordan elimination, are equivalent to x = c  recall the identity Ix = x , so that c is already the solution.  EXAMPLE 2.1 Determine whether the following matrix is singular:  ⎡ ⎢⎣ 2.1 −0.6 3.2 3.1 −6.5  1.1 4.7 −0.8 4.1  ⎤ ⎥⎦  A =  Solution. Laplace’s development of the determinant  see Appendix A2  about the ﬁrst row of A yields  A = 2.1   cid:19  cid:19  cid:19  cid:19  cid:19  4.7 −0.8  −6.5   cid:19  cid:19  cid:19  cid:19  cid:19  −  −0.6    cid:19  cid:19  cid:19  cid:19  cid:19  3.2 −0.8   cid:19  cid:19  cid:19  cid:19  cid:19  + 1.1   cid:19  cid:19  cid:19  cid:19  cid:19  3.2  4.1 = 2.1 14.07  + 0.6 15.60  + 1.1 35.37  = 0  3.1  4.1  4.7 3.1 −6.5   cid:19  cid:19  cid:19  cid:19  cid:19   Since the determinant is zero, the matrix is singular. It can be veriﬁed that the singu- larity is due to the following row dependency:  row 3  =  3 × row 1 −  row 2 .  EXAMPLE 2.2 Solve the equations Ax = b, where ⎡ ⎢⎣ 8 −6 −4 4 −7  A =  2 11 −7 6  ⎤ ⎥⎦  ⎡ ⎢⎣ 28−40  ⎤ ⎥⎦  33  b =  knowing that the LU decomposition of the coefﬁcient matrix is  you should verify this   ⎡ ⎢⎣ 2 0 0 −1 2 0 1 −1 1  ⎤ ⎥⎦  ⎡ ⎢⎣ 4 −3  0 0  1 4 −3 2 0  ⎤ ⎥⎦  A = LU =   37  2.2 Gauss Elimination Method Solution. We ﬁrst solve the equations Ly = b by forward substitution:  2y1 = 28 −y1 + 2y2 = −40 y1 − y2 + y3 = 33  y1 = 28 2 = 14 y2 =  −40 + y1  2 =  −40 + 14  2 = −13 y3 = 33 − y1 + y2 = 33 − 14 − 13 = 6  The solution x is then obtained from Ux = y by back substitution:  2x3 = y3 4x2 − 3x3 = y2 4x1 − 3x2 + x3 = y1  x3 = y3 2 = 6 2 = 3 x2 =  y2 + 3x3  4 = [−13 + 3 3 ]  4 = −1 x1 =  y1 + 3x2 − x3  4 = [14 + 3 −1  − 3]  4 = 2  Hence the solution is x =  2 −1 3  T  .   cid:14    cid:15   2.2  Gauss Elimination Method  Introduction  Gauss elimination is the most familiar method for solving simultaneous equations. It consists of two parts: the elimination phase and the back substitution phase. As indicated in Table 2.1, the function of the elimination phase is to transform the equa- tions into the form Ux = c. The equations are then solved by back substitution. To illustrate the procedure, let us solve the following equations:  4x1 − 2x2 + x3 = 11 −2x1 + 4x2 − 2x3 = −16 x1 − 2x2 + 4x3 = 17   a    b    c   Elimination phase. The elimination phase uses only one of the elementary opera- tions listed in Table 2.1—multiplying one equation  say, equation j   by a constant λ and subtracting it from another equation  equation i . The symbolic representation of this operation is  Eq.  i  ← Eq.  i  − λ × Eq.  j     2.6   The equation being subtracted, namely Eq.  j  , is called the pivot equation.  We start the elimination by taking Eq.  a  to be the pivot equation and choosing  the multipliers λ so as to eliminate x1 from Eqs.  b  and  c : Eq.  b  ← Eq.  b  −   − 0.5  × Eq.  a  Eq.  c  ← Eq.  c  − 0.25 × Eq.  a    38  Systems of Linear Algebraic Equations  After this transformation, the equations become   a    b    c    a    b    c   4x1 − 2x2 + x3 = 11  3x2 − 1.5x3 = −10.5 −1.5x2 + 3.75x3 = 14.25  4x1 − 2x2 + x3 = 11  3x2 − 1.5x3 = −10.5  3x3 = 9  ⎡ ⎢⎣ 4 −2 −2 1 −2  1  11 4 −2 −16 17  4  ⎤ ⎥⎦  This completes the ﬁrst pass. Now we pick  b  as the pivot equation and eliminate x2 from  c :  Eq.  c  ← Eq.  c  −   − 0.5  × Eq.  b   which yields the equations  The elimination phase is now complete. The original equations have been replaced by equivalent equations that can be easily solved by back substitution.  As pointed out earlier, the augmented coefﬁcient matrix is a more convenient instrument for performing the computations. Thus the original equations would be written as  and the equivalent equations produced by the ﬁrst and the second passes of Gauss  ⎤ ⎥⎦  elimination would appear as⎡ ⎢⎣ 4 −2 11.00 3 −1.5 −10.50 0 0 −1.5 14.25 ⎡ ⎤ ⎢⎣ 4 −2 ⎥⎦  3.75  1  1  11.0 3 −1.5 −10.5 0 9.0  3  0 0  It is important to note that the elementary row operation in Eq.  2.6  leaves the determinant of the coefﬁcient matrix unchanged. This is fortunate, because the de- terminant of a triangular matrix is very easy to compute—it is the product of the diagonal elements  you can verify this quite easily . In other words,  A = U = U11 × U22 × ··· × Unn   2.7    39  2.2 Gauss Elimination Method  Back substitution phase. The unknowns can now be computed by back substitu- tion in the manner described previously. Solving Eqs.  c ,  b , and  a  in that order, we get  x3 = 9 3 = 3 x2 =  −10.5 + 1.5x3  3 = [−10.5 + 1.5 3 ] 3 = −2 x1 =  11 + 2x2 − x3  4 = [11 + 2 −2  − 3] 4 = 1  Algorithm for Gauss Elimination Method  Elimination phase. Let us look at the equations at some instant during the elim- ination phase. Assume that the ﬁrst k rows of A have already been transformed to upper triangular form. Therefore, the current pivot equation is the kth equation, and all the equations below it are still to be transformed. This situation is depicted by the following augmented coefﬁcient matrix. Note that the components of A are not the coefﬁcients of the original equations  except for the ﬁrst row , because they have been altered by the elimination procedure. The same applies to the components of the constant vector b.  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  A 11 A 12 A 13 A 22 A 23 0 A 33 0 0 ... ... ... 0 0 0 ... ... ... 0 0 0 ... ... ... 0 0 0  ··· A 1k ··· A 2k ··· A 3k ... ··· A kk ... Aik ... ··· A nk  ···  ··· A 1j ··· A 2j ··· A 3j ... ··· A kj ... Aij ... ··· Anj  ···  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  ...  ··· A 1n b1 ··· A 2n b2 ··· A 3n b3 ... ··· A kn bk ... ... Ain bi ... ... ··· A nn bn  ···  ← pivot row ← row being  transformed  Let the ith row be a typical row below the pivot equation that is to be trans- formed, meaning that the element Aik is to be eliminated. We can achieve this by multiplying the pivot row by λ = Aik  A kk and subtracting it from the ith row. The corresponding changes in the ith row are Aij ← Aij − λA kj , bi ← bi − λbk  j = k, k + 1, . . . , n   2.8b    2.8a   To transform the entire coefﬁcient matrix to upper triangular form, k and i in Eqs.  2.8  must have the ranges k = 1, 2, . . . , n − 1  chooses the pivot row , i = k + 1, k + 2 . . . , n  chooses the row to be transformed . The algorithm for the elimination phase now almost writes itself:  for k in range 0,n-1 :  for i in range k+1,n :   40  Systems of Linear Algebraic Equations  if a[i,k] != 0.0:  lam = a[i,k] a[k,k]  a[i,k+1:n] = a[i,k+1:n] - lam*a[k,k+1:n]  b[i] = b[i] - lam*b[k]  To avoid unnecessary operations, the preceding algorithm departs slightly from  Eqs.  2.8  in the following ways:   If Aik happens to be zero, the transformation of row i is skipped.   The index j in Eq.  2.8a  starts with k + 1 rather than k. Therefore, Aik is not re- placed by zero, but retains its original value. As the solution phase never accesses the lower triangular portion of the coefﬁcient matrix anyway, its contents are ir- relevant.  Back substitution phase. After Gauss elimination the augmented coefﬁcient matrix has the form   cid:14    cid:15   =  A b  ⎡  ⎢⎢⎢⎢⎢⎢⎣  A 11 A 12 A 13 A 22 A 23 0 A 33 0 0 ... ... ... 0 0 0  ··· A 1n b1 ··· A 2n b2 ··· A 3n b3 ... ... ··· Ann bn  ...  ⎤  ⎥⎥⎥⎥⎥⎥⎦  The last equation, A nnxn = bn, is solved ﬁrst, yielding  xn = bn Ann   2.9   Consider now the stage of back substitution where xn, xn−1, . . . , xk+1 have been already been computed  in that order , and we are about to determine xk from the kth equation  A kk xk + A k,k+1xk+1 + ··· + A knxn = bk  The solution is  ⎛ ⎝bk − n cid:6   xk =  A kj xj  j=k+1  ⎞ ⎠ 1  A kk  , k = n − 1, n − 2, . . . , 1   2.10   The corresponding algorithm for back substitution is  for k in range n-1,-1,-1 :  x[k]= b[k] - dot a[k,k+1:n],x[k+1:n]   a[k,k]  Operation count. The execution time of an algorithm depends largely on the num- ber of long operations  multiplications and divisions  performed. It can be shown that Gauss elimination contains approximately n3 3 such operations  n is the num- ber of equations  in the elimination phase, and n2 2 operations in back substitution. These numbers show that most of the computation time goes into the elimination phase. Moreover, the time increases very rapidly with the number of equations.   41  2.2 Gauss Elimination Method   cid:2  gaussElimin  The function gaussElimin combines the elimination and the back substitution phases. During back substitution b is overwritten by the solution vector x, so that b contains the solution upon exit.   module gaussElimin  ’’’ x = gaussElimin a,b .  Solves [a]{b} = {x} by Gauss elimination.  ’’’  import numpy as np  def gaussElimin a,b :  n = len b    Elimination Phase  for k in range 0,n-1 :  for i in range k+1,n :  if a[i,k] != 0.0:  lam = a [i,k] a[k,k]  a[i,k+1:n] = a[i,k+1:n] - lam*a[k,k+1:n]  b[i] = b[i] - lam*b[k]   Back substitution  for k in range n-1,-1,-1 :  return b  b[k] =  b[k] - np.dot a[k,k+1:n],b[k+1:n]   a[k,k]  Multiple Sets of Equations As mentioned earlier, it is frequently necessary to solve the equations Ax = b for sev- eral constant vectors. Let there be m such constant vectors, denoted by b1, b2, . . . , bm, and let the corresponding solution vectors be x1, x2, . . . , xm. We denote multiple sets of equations by AX = B, where   cid:14    cid:15    cid:14   X =  x1  x2  ···  xm  B =  b1 b2  ··· bm   cid:15   are n × m matrices whose columns consist of solution vectors and constant vectors, respectively.  An economical way to handle such equations during the elimination phase is to include all m constant vectors in the augmented coefﬁcient matrix, so that they are transformed simultaneously with the coefﬁcient matrix. The solutions are then obtained by back substitution in the usual manner, one vector at a time. It would be quite easy to make the corresponding changes in gaussElimin. However, the LU decomposition method, described in the next section, is more versatile in handling multiple constant vectors.   42  Systems of Linear Algebraic Equations  A =  ⎤ ⎥⎦  EXAMPLE 2.3 ⎡ Use Gauss elimination to solve the equations AX = B, where ⎢⎣−14 22 36 −18 7 6 ⎤ ⎥⎦  Solution. The augmented coefﬁcient matrix is 1 −14  ⎡ ⎢⎣ 6 −4 −4 1 −4 ⎡ ⎢⎣ 6 −4 −4 1 −4  22 36 −18 7 6  1 6 −4 6  6 −4 6  B =  ⎤ ⎥⎦  The elimination phase consists of the following two passes:  and  row 2 ← row 2 +  2 3  × row 1 row 3 ← row 3 −  1 6  × row 1  ⎡ ⎢⎣ 6 −4 22 10 3 −10 3 80 3 −10 3 0 0 −10 3 10 3  1 −14  35 6 25 3  ⎤ ⎥⎦  row 3 ← row 3 + row 2 −4  ⎡ ⎢⎣ 6 22 0 10 3 −10 3 80 3 −10 3 0 0  1 −14  5 2  35  0  ⎤ ⎥⎦  In the solution phase, we ﬁrst compute x1 by back substitution:  Thus the ﬁrst solution vector is  The second solution vector is computed next, also using back substitution:  = 14  X31 = 35 5 2 X21 = 80 3 +  10 3 X31 X11 = −14 + 4X21 − X31  10 3  6   cid:14   x1 =  X11 X21 X31  10 3  = 22  = 80 3 +  10 3 14 = −14 + 4 22  − 14 = 10  cid:15 T =  cid:15 T  10 22 14   cid:14   6  X32 = 0 X22 = −10 3 +  10 3 X32 X12 = 22 + 4X22 − X32  10 3  6  = −10 3 + 0 = 22 + 4 −1  − 0  10 3  = −1  = 3  6   43  2.2 Gauss Elimination Method  Therefore,   cid:14   x2 =  X12 X22 X32   cid:14    cid:15 T =  3 −1 0   cid:15 T  EXAMPLE 2.4 An n × n Vandermode matrix A is deﬁned by  Aij = vn−j  ,  i  i = 1, 2, . . . , n,  j = 1, 2, . . . , n  where v is a vector. Use the function gaussElimin to compute the solution of Ax = b, where A is the 6 × 6 Vandermode matrix generated from the vector   cid:14   v =   cid:14   b =  1.0 1.2 1.4 1.6 1.8 2.0  0 1 0 1 0 1   cid:15 T   cid:15 T  Also evaluate the accuracy of the solution  Vandermode matrices tend to be ill  and  conditioned .  Solution  ! usr bin python   example2_4  import numpy as np  from gaussElimin import *  def vandermode v :  n = len v   a = np.zeros  n,n    for j in range n :  a[:,j] = v** n-j-1   return a  v = np.array [1.0, 1.2, 1.4, 1.6, 1.8, 2.0]   b = np.array [0.0, 1.0, 0.0, 1.0, 0.0, 1.0]   a = vandermode v   aOrig = a.copy     Save original matrix  bOrig = b.copy     and the constant vector  x = gaussElimin a,b   det = np.prod np.diagonal a    print ’x =\n’,x   print ’\ndet =’,det   print ’\nCheck result: [a]{x} - b =\n’,np.dot aOrig,x  - bOrig   input "\nPress return to exit"    44  Systems of Linear Algebraic Equations  The program produced the following results:  x =  [  416.66666667  -3125.00000004  9250.00000012 -13500.00000017  9709.33333345  -2751.00000003]  det = -1.13246207999e-006  Check result: [a]{x} - b =  [0.00000000e+00  3.63797881e-12  0.00000000e+00  1.45519152e-11  0.00000000e+00  5.82076609e-11]  As the determinant is quite small relative to the elements of A  you may want to print A to verify this , we expect a detectable roundoff error. Inspection of x leads us to suspect that the exact solution is   cid:14   x =  1250 3 −3125 9250 −13500 29128 3 −2751   cid:15 T  in which case the numerical solution would be accurate to about 10 decimal places. Another way to gauge the accuracy of the solution is to compute Ax − b  the result should be 0 . The printout indicates that the solution is indeed accurate to at least 10 decimal places.  2.3  LU Decomposition Methods  Introduction  It is possible to show that any square matrix A can be expressed as a product of a lower triangular matrix L and an upper triangular matrix U:  A = LU   2.11   The process of computing L and U for a given A is known as LU decomposition or LU factorization. LU decomposition is not unique  the combinations of L and U for a prescribed A are endless , unless certain constraints are placed on L or U. These constraints distinguish one type of decomposition from another. Three commonly used decompositions are listed in Table 2.2.  Name Doolittle’s decomposition Crout’s decomposition Choleski’s decomposition  Constraints Lii = 1, Uii = 1, L = UT  i = 1, 2, . . . , n i = 1, 2, . . . , n  Table 2.2. Three Commonly Used Decompositions  After decomposing A, it is easy to solve the equations Ax = b, as pointed out in Section 2.1. We ﬁrst rewrite the equations as LUx = b. After using the notation   45  2.3 LU Decomposition Methods Ux = y, the equations become  which can be solved for y by forward substitution. Then  Ly = b,  Ux = y  will yield x by the back substitution process. The advantage of LU decomposition over the Gauss elimination method is that once A is decomposed, we can solve Ax = b for as many constant vectors b as we please. The cost of each additional solution is relatively small, because the forward and back substitution operations are much less time consuming than the decompo- sition process.  Doolittle’s Decomposition Method  Decomposition phase. Doolittle’s decomposition is closely related to Gauss elimi- nation. To illustrate the relationship, consider a 3 × 3 matrix A and assume that there exist triangular matrices  ⎡ ⎢⎣ 1  L =  0 1  0 L21 0 L31 L32 1  ⎤ ⎥⎦  ⎡ ⎢⎣ U11 U12 U13  0 U22 U23 0 U33 0  ⎤ ⎥⎦  U =  such that A = LU. After completing the multiplication on the right-hand side, we get  ⎡ ⎢⎣ U11 U11L21 U12L21 + U22 U11L31 U12L31 + U22L32 U13L31 + U23L32 + U33  U13 U13L21 + U23  U12  ⎤ ⎥⎦  A =   2.12   Let us now apply Gauss elimination to Eq.  2.12 . The ﬁrst pass of the elimina- tion procedure consists of choosing the ﬁrst row as the pivot row and applying the elementary operations  The result is  row 2 ← row 2 − L21 × row 1  eliminatesA 21  row 3 ← row 3 − L31 × row 1  eliminatesA 31   ⎡ ⎢⎣ U11 0 0 U22L32 U23L32 + U33  U12 U22  U13 U23  ⎤ ⎥⎦   cid:3  =  A  In the next pass we take the second row as the pivot row, and use the operation  row 3 ← row 3 − L32 × row 2  eliminatesA 32    46  Systems of Linear Algebraic Equations  ending up with  ⎡ ⎢⎣ U11 U12 U13  0 U22 U23 0 0 U33  ⎤ ⎥⎦   cid:3  cid:3  = U = A  The foregoing illustration reveals two important features of Doolittle’s decompo-  sition:  elimination.  1. The matrix U is identical to the upper triangular matrix that results from Gauss  2. The off-diagonal elements of L are the pivot equation multipliers used during  Gauss elimination; that is, Lij is the multiplier that eliminated Aij .  It is usual practice to store the multipliers in the lower triangular portion of the coefﬁcient matrix, replacing the coefﬁcients as they are eliminated  Lij replacing Aij  . The diagonal elements of L do not have to be stored, because it is understood that each of them is unity. The ﬁnal form of the coefﬁcient matrix would thus be the fol- lowing mixture of L and U:  ⎡ ⎢⎣ U11 U12 U13  L21 U22 U23 L31 L32 U33  ⎤ ⎥⎦  [L\U] =   2.13   The algorithm for Doolittle’s decomposition is thus identical to the Gauss elimi- nation procedure in gaussElimin, except that each multiplier λ is now stored in the lower triangular portion of A :  for k in range 0,n-1 :  for i in range k+1,n :  if a[i,k] != 0.0:  lam = a[i,k] a[k,k]  a[i,k+1:n] = a[i,k+1:n] - lam*a[k,k+1:n]  a[i,k] = lam  Solution phase. Consider now the procedure for the solution of Ly = b by forward substitution. The scalar form of the equations is  recall that Lii = 1   y1 = b1 L21y1 + y2 = b2 ... Lk1y1 + Lk2y2 + ··· + Lk,k−1yk−1 + yk = bk ...   47  2.3 LU Decomposition Methods  Solving the kth equation for yk yields  yk = bk − k−1 cid:6   j=1  Therefore, the forward substitution algorithm is  y[0] = b[0]  for k in range 1,n :  Lkj y j , k = 2, 3, . . . , n   2.14   y[k] = b[k] - dot a[k,0:k],y[0:k]   The back substitution phase for solving Ux = y is identical to what was used in  the Gauss elimination method.   cid:2  LUdecomp  This module contains both the decomposition and solution phases. The decompo- sition phase returns the matrix [L\U] shown in Eq.  2.13 . In the solution phase, the contents of b are replaced by y during forward substitution Similarly, the back sub- stitution overwrites y with the solution x.   module LUdecomp  ’’’ a = LUdecomp a   LUdecomposition: [L][U] = [a]  x = LUsolve a,b   Solution phase: solves [L][U]{x} = {b}  ’’’  import numpy as np  def LUdecomp a :  n = len a   for k in range 0,n-1 :  for i in range k+1,n :  if a[i,k  != 0.0:  a[i,k] = lam  return a  def LUsolve a,b :  n = len a   for k in range 1,n :  lam = a [i,k] a[k,k]  a[i,k+1:n] = a[i,k+1:n] - lam*a[k,k+1:n]  b[k] = b[k] - np.dot a[k,0:k],b[0:k]   b[n-1] = b[n-1] a[n-1,n-1]  for k in range n-2,-1,-1 :  b[k] =  b[k] - np.dot a[k,k+1:n],b[k+1:n]   a[k,k]  return b   48  Systems of Linear Algebraic Equations  Choleski’s Decomposition Method Choleski’s decomposition A = LLT has two limitations: 1. Since LLT is always a symmetric matrix, Choleski’s decomposition requires A to  be symmetric.  2. The decomposition process involves taking square roots of certain combinations of the elements of A. It can be shown that to avoid square roots of negative num- bers A must be positive deﬁnite.  Choleski’s decomposition contains approximately n3 6 long operations plus n square root computations. This is about half the number of operations required in LU decomposition. The relative efﬁciency of Choleski’s decomposition is due to its exploitation of symmetry.  Let us start by looking at Choleski’s decomposition  A = LLT  of a 3 × 3 matrix:⎡  ⎤ ⎥⎦ =  ⎡ ⎢⎣ L11  A 21 A 22 A 23 A 31 A 32 A 33  0 0 L21 L22 0 L31 L32 L33  ⎢⎣ A 11 A 12 A 13 ⎡ ⎢⎣ L2 11 L11L21 L2 21 L11L31 L21L31 + L22L32 L2  ⎤ ⎥⎦ =  L11L21 + L2  ⎡ ⎢⎣ A 11 A 12 A 13  A 21 A 22 A 23 A 31 A 32 A 33  0 0  22  ⎤ ⎥⎦  ⎡ ⎢⎣ L11 L21 L31  ⎤ ⎥⎦  L22 L32 0 L33  After completing the matrix multiplication on the right-hand side, we get  L11L31 L21L31 + L22L32 + L2 + L2  31  33  32  ⎤ ⎥⎦   2.15    2.16   Note that the right-hand-side matrix is symmetric, as pointed out earlier. Equating the matrices A and LLT element by element, we obtain six equations  because of sym- metry only lower or upper triangular elements have to be considered  in the six un- known components of L. By solving these equations in a certain order, it is possible to have only one unknown in each equation.  Consider the lower triangular portion of each matrix in Eq.  2.16   the upper tri- angular portion would do as well . By equating the elements in the ﬁrst column, start- ing with the ﬁrst row and proceeding downward, we can compute L11, L21, and L31 in that order:  11  A 11 = L2 A 21 = L11L21 A 31 = L11L31  A 11  L11 = L21 = A 21 L11 L31 = A 31 L11   cid:25    cid:26   The second column, starting with second row, yields L22 and L32:  + L2  A 22 = L2 A 32 = L21L31 + L22L32  21  22  A 22 − L2  L22 = L32 =  A 32 − L21L31  L22  21   49  2.3 LU Decomposition Methods  Finally the third column, third row gives us L33: L33 =  A 33 = L2  + L2  + L2  31  32  33  A 33 − L2  31  − L2  32   cid:26   We can now extrapolate the results for an n × n matrix. We observe that a typical  element in the lower triangular portion of LLT is of the form   LLT  ij = Li1L j 1 + Li2L j 2 + ··· + Lij L j j = j cid:6  Aij = j cid:6   i = j, j + 1, . . . , n,  Lik L j k,  k=1  Equating this term to the corresponding element of A yields  Lik L j k,  i ≥ j  j = 1, 2, . . . , n   2.17   k=1   cid:25   The range of indices shown limits the elements to the lower triangular part. For the ﬁrst column  j = 1 , we obtain from Eq.  2.17   L11 =  A 11  Li1 = Ai1 L11,  i = 2, 3, . . . , n   2.18   Proceeding to other columns, we observe that the unknown in Eq.  2.17  is Lij  the other elements of L appearing in the equation have already been computed . Taking the term containing Lij outside the summation in Eq.  2.17 , we obtain  k=1 If i = j  a diagonal term , the solution is  Lik L j k + Lij L j j  Aij = j−1 cid:6   cid:16  cid:17  cid:17  cid:18   A j j − j−1 cid:6   j = 2, 3, . . . , n  L2  j k,  k=1  j = 2, 3, . . . , n − 1,  i = j + 1, j + 2, . . . , n.   2.19    2.20   L j j = ⎞ ⎠  L j j ,  Lik L j k  For a nondiagonal term we get  ⎛ ⎝Aij − j−1 cid:6   k=1  Lij =   cid:2  choleski  Before presenting the algorithm for Choleski’s decomposition, we make a useful ob- servation: Aij appears only in the formula for Lij . Therefore, once Lij has been com- puted, Aij is no longer needed. This makes it possible to write the elements of L over the lower triangular portion of A as they are computed. The elements above the leading diagonal of A will remain untouched. The function listed next implements Choleski’s decomposition. If a negative diagonal term is encountered during decom- position, an error message is printed and the program is terminated.   50  Systems of Linear Algebraic Equations  After the coefﬁcient matrix A has been decomposed, the solution of Ax = b can be obtained by the usual forward and back substitution operations. The function choleskiSol  given here without derivation  carries out the solution phase.   module choleski  ’’’ L = choleski a   Choleski decomposition: [L][L]transpose = [a]  x = choleskiSol L,b   Solution phase of Choleski’s decomposition method  ’’’  import numpy as np  import math  import error  def choleski a :  n = len a   for k in range n :  try:  a[k,k] = math.sqrt a[k,k]  \  - np.dot a[k,0:k],a[k,0:k]    except ValueError:  for i in range k+1,n :  error.err ’Matrix is not positive definite’   a[i,k] =  a[i,k] - np.dot a[i,0:k],a[k,0:k]   a[k,k]  for k in range 1,n : a[0:k,k] = 0.0  return a  def choleskiSol L,b :  n = len b    Solution of [L]{y} = {b}  for k in range n :  b[k] =  b[k] - np.dot L[k,0:k],b[0:k]   L[k,k]   Solution of [L_transpose]{x} = {y}  for k in range n-1,-1,-1 :  b[k] =  b[k] - np.dot L[k+1:n,k],b[k+1:n]   L[k,k]  return b  Other Methods Crout’s decomposition. Recall that the various decompositions A = LU are char- acterized by the constraints placed on the elements of L or U. In Doolittle’s decom- position the diagonal elements of L were set to 1. An equally viable method is Crout’s   51  2.3 LU Decomposition Methods  decomposition, where the 1’s lie on the diagonal of U. There is little difference in the performance of the two methods.  Gauss-Jordan Elimination. The Gauss-Jordan method is essentially Gauss elimi- nation taken to its limit. In the Gauss elimination method only the equations that lie below the pivot equation are transformed. In the Gauss-Jordan method the elimina- tion is also carried out on equations above the pivot equation, resulting in a diagonal coefﬁcient matrix. The main disadvantage of Gauss-Jordan elimination is that it in- volves about n3 2 long operations, which is 1.5 times the number required in Gauss elimination.  EXAMPLE 2.5 Use Doolittle’s decomposition method to solve the equations Ax = b, where  ⎡ ⎢⎣ 1 4 1 6 −1 1 2 −1 2  ⎤ ⎥⎦  A =  ⎤ ⎥⎦  ⎡ ⎢⎣ 7  13 5  b =  Solution. We ﬁrst decompose A by Gauss elimination. The ﬁrst pass consists of the elementary operations  row 2 ← row 2 − 1 × row 1  eliminates A 21  row 3 ← row 3 − 2 × row 1  eliminates A 31   Storing the multipliers L21 = 1 and L31 = 2 in place of the eliminated terms, we ob- tain  ⎡ ⎢⎣ 1 4 1 2 −2 1 2 −9 0  ⎤ ⎥⎦   cid:3  = A  The second pass of Gauss elimination uses the operation  row 3 ← row 3 −  −4.5  × row 2  eliminates A 32   Storing the multiplier L32 = −4.5 in place of A 32, we get  ⎡ ⎢⎣ 1 1 −2 1 2 −4.5 −9  4 2  ⎤ ⎥⎦   cid:3  cid:3  = [L\U] = A  The decomposition is now complete, with  ⎡ ⎢⎣ 1 0 1 0 2 −4.5 1  0 1  ⎤ ⎥⎦  L =  ⎡ ⎢⎣ 1 4 1 0 2 −2 0 0 −9  ⎤ ⎥⎦  U =   52  Systems of Linear Algebraic Equations  Solution of Ly = b by forward substitution comes next. The augmented coefﬁ-  cient form of the equations is   cid:14   L b   cid:15   =  ⎡ ⎢⎣ 1 0 7 0 13 1 2 −4.5 1 5  0 1  ⎤ ⎥⎦  The solution is  y1 = 7 y2 = 13 − y1 = 13 − 7 = 6 y3 = 5 − 2y1 + 4.5y2 = 5 − 2 7  + 4.5 6  = 18  Finally, the equations Ux = y, or   cid:14   U y   cid:15   =  ⎡ ⎢⎣ 1 4 1 7 0 2 −2 6 0 0 −9 18  ⎤ ⎥⎦  are solved by back substitution. This yields  = −2 x3 = 18−9 x2 = 6 + 2x3 x1 = 7 − 4x2 − x3 = 7 − 4 1  −  −2  = 5  = 6 + 2 −2   = 1  2  2  EXAMPLE 2.6 Compute Choleski’s decomposition of the matrix  ⎡ ⎢⎣ 4 −2 −2 2 −4  2 2 −4 11  ⎤ ⎥⎦  A =  Solution. First we note that A is symmetric. Therefore, Choleski’s decomposition is applicable, provided that the matrix is also positive deﬁnite. An a priori test for pos- itive deﬁniteness is not needed, because the decomposition algorithm contains its own test: If a square root of a negative number is encountered, the matrix is not pos- itive deﬁnite and the decomposition fails.  Substituting the given matrix for A in Eq.  2.16  we obtain  ⎡ ⎢⎣ 4 −2 −2 2 −4  2 2 −4 11  ⎤ ⎥⎦ =  ⎡ ⎢⎣ L2 11 L11L21 L2 21 L11L31 L21L31 + L22L32 L2  L11L21 + L2  22  L11L31 L21L31 + L22L32 + L2 + L2  33  31  32  ⎤ ⎥⎦   53  2.3 LU Decomposition Methods  Equating the elements in the lower  or upper  triangular portions yields  √  21  4 = 2   cid:26  2 − L2  cid:26  L22 11 − L2  L11 = L21 = −2 L11 = −2 2 = −1 L31 = 2 L11 = 2 2 = 1  cid:25  2 − 12 = 1 L22 = = L32 = −4 − L21L31 = −4 −  −1  1   cid:25  1 − L2 11 −  1 2 −  −3 2 = 1 L33 = ⎡ ⎢⎣ 2 0 0 −1 1 0 1 −3 1  = −3  ⎤ ⎥⎦  L =  =  32  31  Therefore,  The result can easily be veriﬁed by performing the multiplication LLT .  EXAMPLE 2.7 Write a program that solves AX = B with Doolittle’s decomposition method and com- putes A. Use the functions LUdecomp and LUsolve. Test the program with  ⎡ ⎢⎣ 3 −1 −2 7  4 0 5 2 −2  ⎤ ⎥⎦  A =  ⎤ ⎥⎦  ⎡ ⎢⎣ 6 −4 3 2 7 −5  B =  Solution  ! usr bin python   example2_7  import numpy as np  from LUdecomp import *  a = np.array [[ 3.0, -1.0,  4.0], \  [-2.0,  0.0,  5.0], \  [ 7.0,  2.0, -2.0]]   b = np.array [[ 6.0,  3.0,  7.0], \  [-4.0,  2.0, -5.0]]   a = LUdecomp a    Decompose [a]  det = np.prod np.diagonal a    print "\nDeterminant =",det   for i in range len b  :   Back-substitute one  x = LUsolve a,b[i]    constant vector at a time  print "x",i+1,"=",x   input "\nPress return to exit"    54  Systems of Linear Algebraic Equations  The output of the program is  Determinant = -77.0  x 1 = [ 1.  1.  1.]  x 2 = [ -1.00000000e+00  1.00000000e+00  2.30695693e-17]  EXAMPLE 2.8 Solve the equations Ax = b by Choleski’s decomposition, where ⎡ ⎢⎢⎢⎣  1.44 −0.36 −0.36 5.52 −7.78 0.00 0.00  0.00 5.52 10.33 −7.78 0.00 28.40 9.00 9.00 61.00  ⎡ ⎢⎢⎢⎣  ⎤ ⎥⎥⎥⎦  A =  b =  0.04−2.15  0 0.88  ⎤ ⎥⎥⎥⎦  Also check the solution.  Solution  ! usr bin python   example2_8  import numpy as np  from choleski import *  a = np.array [[ 1.44, -0.36,  5.52,  0.0], \  [-0.36, 10.33, -7.78,  0.0], \  [ 5.52, -7.78, 28.40,  9.0], \  [ 0.0,  0.0,  9.0,  61.0]]   b = np.array [0.04, -2.15, 0.0, 0.88]   aOrig = a.copy    L = choleski a   x = choleskiSol L,b   print "x =",x   print ’\nCheck: A*x =\n’,np.dot aOrig,x    input "\nPress return to exit"   The output is  Check: A*x =  x = [ 3.09212567 -0.73871706 -0.8475723  0.13947788]  [4.00000000e-02 -2.15000000e+00 -3.55271368e-15  8.80000000e-01]   55  2.3 LU Decomposition Methods  PROBLEM SET 2.1  1. By evaluating the determinant, classify the following matrices as singular, ill con-  ⎤ ⎥⎦  ⎤ ⎥⎦  ⎤ ⎥⎦  ⎤ ⎥⎦   c  A =   a  A =   b  A =  2 3 4 3 4 5  3.03 1.29 5.25 4.30  3 −1 3 13  ditioned, or well conditioned:  ⎡ ⎢⎣ 2.11 −0.80 1.72 −1.84 ⎡ −1.57 ⎢⎣ 4  d  A = 7 −2 5 −18 ⎤ ⎡ 2. Given the LU decomposition A = LU, determine A and A: ⎥⎦ U = ⎢⎣ 1 2 4 0 3 21 ⎤ ⎡ 0 0 0 ⎥⎦ U = ⎢⎣ 2 −1 ⎡ ⎤ ⎢⎣ 2 −3 ⎥⎦ −1 0 13 2 −7 2 32 13 0  ⎡ ⎢⎣ 1 2 3 ⎡ ⎢⎣ 2 −1 0 −1 2 −1 0 −1 2 ⎡ ⎢⎣ 1 ⎡ ⎢⎣ 2 0 0 −1 1 0 1 −3 1 ⎡ ⎢⎣ 1  cid:14   0 3 2 0 1 2 11 13 1  3. Use the results of LU decomposition  0 0 1 1 5 3 1  ⎤ ⎥⎦ ⎤ ⎥⎦  1 1 −3 1 0  A = LU =   b  L =   a  L =  ⎤ ⎥⎦  0 0  0 1  0 1  0   cid:15  ⎤ ⎥⎦  .  to solve Ax = b, where bT = 1 −1 2 ⎡ ⎢⎣ 2 −3 −1 2 −5 4 −1  ⎡ ⎤ 4. Use Gauss elimination to solve the equations Ax = b, where ⎢⎣ 3 ⎥⎦ −9−5 5. Solve the equations AX = B by Gauss elimination, where ⎡ ⎢⎢⎢⎣  ⎤ ⎥⎥⎥⎦  B =  A =  A =  b =  3 2  2 0 −1 0 2 0 1 0 −1 2 0 1 1 −2 0 0  1 0 0 0 0 1 0 0  6. Solve the equations Ax = b by Gauss elimination, where ⎡  ⎤  A =  2 1 2 2 −1 0 0 −2 0 0 −1 1 1 −1 Hint: Reorder the equations before solving.  0 0 0 1 1 2 0 0 0 1 −1  ⎥⎥⎥⎥⎥⎦  b =  ⎢⎢⎢⎢⎢⎣  1  1−4−2 −1  ⎡ ⎢⎢⎢⎣ ⎡  ⎢⎢⎢⎢⎢⎣  ⎤ ⎥⎥⎥⎦ ⎤  ⎥⎥⎥⎥⎥⎦   56  Systems of Linear Algebraic Equations  7. Find L and U so that  ⎡ ⎢⎣ 4 −1 −1 0 −1  0 4 −1 4  ⎤ ⎥⎦  A = LU =  9. Solve the equations AX = b by Doolittle’s decomposition method, where  using  a  Doolittle’s decomposition;  b  Choleski’s decomposition.  b =  A =  ⎤ ⎥⎦  9 −8  6 −4 24 24 −26  ⎡ ⎢⎣ −3 −12  ⎡ ⎤ 8. Use Doolittle’s decomposition method to solve Ax = b, where ⎢⎣ −3 ⎥⎦ 65−42 ⎡ ⎢⎣ 0.02 −0.73−6.63 ⎤ ⎥⎦  1.78 3.47 −2.22 6.18  ⎤ ⎥⎦  ⎤ ⎥⎦  A =  b =  B =  10. Solve the equations AX = B by Doolittle’s decomposition method, where  ⎤ ⎥⎦  11. Solve the equations Ax = b by Choleski’s decomposition method, where  ⎡ ⎢⎣ 2.34 −4.10 −1.98 2.36 −15.17 ⎡ ⎢⎣ 4 −3 8 −3 −4 ⎡ ⎢⎣ 1 1 1  A =  A =  1 2 2 1 2 3  6 10 12 −10 ⎤ ⎥⎦ ⎤ ⎥⎦  4 −2 −3 4 −10 12 −16 18 28  0 1 0 0  ⎡ ⎢⎣ 1 0 ⎤ ⎥⎦  3 2 3  ⎡ ⎢⎣ 1 ⎡ ⎢⎣ 1.1 0−2.3  ⎤ ⎥⎦  b =  ⎤ ⎥⎦ =  ⎡ ⎢⎣ x1  x2 x3  12. Solve the equations ⎡ ⎢⎣  by Doolittle’s decomposition method.  13. Determine L that results from Choleski’s decomposition of the diagonal matrix  ⎡ ⎢⎢⎢⎢⎣  A =  α1 0 0 0 ...  0 α2 0 0 α3 ... ...  ⎤ ⎥⎥⎥⎥⎦  ··· ··· ··· ...  14.  cid:2  Modify the function gaussElimin so that it will work with m constant vectors.  ⎤ Test the program by solving AX = B, where ⎥⎦  ⎡ ⎢⎣ 2 −1 −1 0 −1  0 2 −1 2  A =  ⎤ ⎥⎦  ⎡ ⎢⎣ 1 0 0  0 1 0 0 0 1  B =   57  2.3 LU Decomposition Methods  15.  cid:2  A well-known example of an ill-conditioned matrix is the Hilbert matrix:  ⎡ ⎢⎢⎢⎢⎣  A =  ⎤ ⎥⎥⎥⎥⎦  1 2 1 3 ··· 1 1 2 1 3 1 4 ··· 1 3 1 4 1 5 ··· ... ...  ...  ...  bi = n cid:6   j=1  Aij   cid:14   x =  1 1 1 ···   cid:15 T  Write a program that specializes in solving the equations Ax = b by Doolittle’s decomposition method, where A is the Hilbert matrix of arbitrary size n × n, and  The program should have no input apart from n. By running the program, deter- mine the largest n for which the solution is within six signiﬁcant ﬁgures of the exact solution  16. Derive the forward and back substitution algorithms for the solution phase of 17.  cid:2  Determine the coefﬁcients of the polynomial y = a0 + a1x + a2x2 + a3x3 that  Choleski’s method. Compare them with the function choleskiSol.  18.  cid:2  Determine the fourth-degree polynomial y x  that passes through the points  passes through the points  0, 10 ,  1, 35 ,  3, 31 , and  4, 2 .  0, −1 ,  1, 1 ,  3, 3 ,  5, 2 , and  6, −2 .  0.75, −0.25 , and  1, 1  and has zero curvature at  0, 1  and  1, 1 .  19.  cid:2  Find the fourth-degree polynomial y x  that passes through the points  0, 1 , 20.  cid:2  Solve the equations Ax = b, where  ⎡ ⎢⎢⎢⎣  A =  3.50 2.77 −0.76 −1.80 2.68 0.27 5.07 1.71 5.45  7.31 4.23 13.85 11.55 By computing A and Ax comment on the accuracy of the solution.  1.80 3.44 −0.09 1.61 6.90 1.71 2.68  b =  ⎤ ⎥⎥⎥⎦  ⎡ ⎢⎢⎢⎣  ⎤ ⎥⎥⎥⎦  21. Compute the condition number of the matrix  ⎡ ⎢⎣ 1 −1 −1 1 −2 1 0  0 0  ⎤ ⎥⎦  A =  based on  a  the euclidean norm and  b  the inﬁnity norm. You may use the func- tion inv A in numpy.linalg to determine the inverse of A.   58  Systems of Linear Algebraic Equations  22.  cid:2  Write a function that returns the condition number of a matrix based on the euclidean norm. Test the function by computing the condition number of the ill-conditioned matrix:  ⎡ ⎢⎢⎢⎣  A =  ⎤ ⎥⎥⎥⎦  4 9 16 1 4 9 16 25 9 16 25 36 16 25 36 49  Use the function inv A in numpy.linalg to determine the inverse of A.  dom matrix and bi = cid:27   23.  cid:2  Test the function gaussElimin by solving Ax = b, where A is a n × n ran- n j=1 Aij  sum of the elements in the ith row of A . A ran- dom matrix can be generated with the rand function in the numpy.random module:  from numpy.random import ran  a = rand n,n    cid:14    cid:15 T  1 1 ···  1  . Run the program with n = 200 or  24.  cid:2  The function gaussElimin also works with complex numbers. Use it to solve  The solution should be x = bigger. Ax = b, where  A =  5 + 2i −5 + 3i 5 + i 7 − 2i 8 − i 5 + 2i 8 − i −3 − 3i −5 + 3i 2 + 2i 6 − 3i −1 + 3i  6 − 3i −1 + 3i 2 + 2i 8 + 14i  ⎡ ⎢⎢⎢⎣  cid:14   ⎤ ⎥⎥⎥⎦  cid:15 T  b =  15 − 35i  2 + 10i −2 − 34i √−1.  8 + 14i  Note that Python uses j to denote  25.  cid:2   3T  μ 3  m3  m4  2T μ2  m2  a m1 θ  1T  μ1  The four blocks of different masses mi are connected by ropes of negligible mass. Three of the blocks lie on a inclined plane, the coefﬁcients of friction between   59  2.4 Symmetric and Banded Coefﬁcient Matrices  the blocks and the plane being μi. The equations of motion for the blocks can be shown to be  T1 + m1a = m1g sin θ − μ1 cos θ  −T1 + T2 + m2a = m2g sin θ − μ2 cos θ  −T2 + T3 + m3a = m3g sin θ − μ3 cos θ   −T3 + m4a = −m4g  where Ti denotes the tensile forces in the ropes and a is the acceleration of the system. Determine a and Ti if θ = 45  cid:14   cid:14   , g = 9.82 m s2 and  10 4 5 6  kg  ◦   cid:15 T  cid:15 T  m = μ =  0.25 0.3 0.2  2.4  Symmetric and Banded Coefﬁcient Matrices  Introduction  Engineering problems often lead to coefﬁcient matrices that are sparsely populated, meaning that most elements of the matrix are zero. If all the nonzero terms are clus- tered about the leading diagonal, then the matrix is said to be banded. An example of a banded matrix is  ⎡  ⎢⎢⎢⎢⎢⎣  A =  ⎤  ⎥⎥⎥⎥⎥⎦  0 X X 0 0 0 X X X 0 0 X X X 0 0 X X X 0 0 0 0 X X  where X’s denote the nonzero elements that form the populated band  some of these elements may be zero . All the elements lying outside the band are zero. This banded matrix has a bandwidth of three, because there are at most three nonzero elements in each row  or column . Such a matrix is called tridiagonal. If a banded matrix is decomposed in the form A = LU, both L and U retain the banded structure of A. For example, if we decomposed the matrix shown above, we would get  ⎡  ⎢⎢⎢⎢⎢⎣  L =  ⎤  ⎥⎥⎥⎥⎥⎦  ⎡  ⎢⎢⎢⎢⎢⎣  U =  ⎤  ⎥⎥⎥⎥⎥⎦  X X 0 0 0 X X 0 0 0 0  0 0 0 X X 0 0 0 X X 0 X 0 0  0 X 0 0 X X 0 0 0 X X 0 0 0  0 0 0 0 X X 0 0 X X 0  The banded structure of a coefﬁcient matrix can be exploited to save storage and computation time. If the coefﬁcient matrix is also symmetric, further economies are   60  Systems of Linear Algebraic Equations  possible. In this section I show how the methods of solution discussed previously can be adapted for banded and symmetric coefﬁcient matrices.  Tridiagonal Coefﬁcient Matrix Consider the solution of Ax = b by Doolittle’s decomposition, where A is the n × n tridiagonal matrix  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  ··· 0 ··· 0 ··· 0 ··· 0 ... ... cn−1 dn  0 0 e3 d4 ... 0  e1 d1 c1 d2 0 c2 0 0 ... ... 0 0  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  ⎤ ⎥⎥⎥⎥⎦  A =  ⎡ ⎢⎢⎢⎢⎣  c1 c2 ... cn−1  0 e2 d3 c3 ...  . . .  ⎡  ⎢⎢⎢⎢⎢⎢⎣  ⎤  ⎥⎥⎥⎥⎥⎥⎦  d1 d2 ... dn−1 dn  c =  d =  e =  ⎤ ⎥⎥⎥⎥⎦  ⎡ ⎢⎢⎢⎢⎣  e1 e2 ... en−1  As the notation implies, we are storing the non-zero elements of A in the vectors  The resulting saving of storage can be signiﬁcant. For example, a 100 × 100 tridi- agonal matrix, containing 10,000 elements, can be stored in only 99 + 100 + 99 = 298 locations, which represents a compression ratio of about 33:1.  Let us now apply LU decomposition to the coefﬁcient matrix. We reduce row k  by getting rid of ck−1 with the elementary operation  row k ← row k −  ck−1 dk−1  × row  k − 1 , k = 2, 3, . . . , n  The corresponding change in dk is  dk ← dk −  ck−1 dk−1 ek−1  whereas ek is not affected. To ﬁnish up with Doolittle’s decomposition of the form [L\U], we store the multiplier λ = ck−1 dk−1 in the location previously occupied by ck−1:  ck−1 ← ck−1 dk−1   2.21    2.22   Thus the decomposition algorithm is  for k in range 1,n :  lam = c[k-1] d[k-1]  d[k] = d[k] - lam*e[k-1]  c[k-1] = lam   61  2.4 Symmetric and Banded Coefﬁcient Matrices  Next we look at the solution phase  i.e., solution of the Ly = b , followed by Ux = y. The equations Ly = b can be portrayed by the augmented coefﬁcient matrix   cid:14    cid:15   =  L b  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  1 c1 0 0 ... 0  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  0 1 c2 0 ... 0  0 0 1 c3 ... ···  ··· ··· ··· . . . ···  0 b1 0 0 0 b2 0 0 b3 1 0 b4 ... ... ... 0 cn−1 1 bn  Note that the original contents of c were destroyed and replaced by the multipliers during the decomposition. The solution algorithm for y by forward substitution is  y[0] = b[0]  for k in range 1,n :  y[k] = b[k] - c[k-1]*y[k-1]  The augmented coefﬁcient matrix representing Ux = y is   cid:14    cid:15   =  U y  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  d1 0 0 ... 0 0  e1 d2 0 ... 0 0  0 e2 d3 ... 0 0  ··· ··· ···  0 0 0 ... ··· dn−1 ··· 0  0 0 0 ... en−1 dn  y1 y2 y3 ... yn−1 yn  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  Note again that the contents of d were altered from the original values during the decomposition phase  but e was unchanged . The solution for x is obtained by back substitution using the algorithm  x[n-1] = y[n-1] d[n-1]  for k in range n-2,-1,-1 :  x[k] =  y[k] - e[k]*x[k+1]  d[k]  end do   cid:2  LUdecomp3  This module contains the functions LUdecomp3 and LUsolve3 for the decompo- sition and solution phases of a tridiagonal matrix. In LUsolve3, the vector y writes over the constant vector b during forward substitution. Similarly, the solution vector x overwrites y in the back substitution process. In other words, b contains the solu- tion upon exit from LUsolve3.   module LUdecomp3  ’’’ c,d,e = LUdecomp3 c,d,e .  LU decomposition of tridiagonal matrix [c\d\e]. On output  {c},{d} and {e} are the diagonals of the decomposed matrix.   62  Systems of Linear Algebraic Equations  x = LUsolve c,d,e,b .  Solves [c\d\e]{x} = {b}, where {c}, {d} and {e} are the  vectors returned from LUdecomp3.  ’’’  def LUdecomp3 c,d,e :  n = len d   for k in range 1,n :  lam = c[k-1] d[k-1]  d[k] = d[k] - lam*e[k-1]  c[k-1] = lam  return c,d,e  def LUsolve3 c,d,e,b :  n = len d   for k in range 1,n :  b[k] = b[k] - c[k-1]*b[k-1]  b[n-1] = b[n-1] d[n-1]  for k in range n-2,-1,-1 :  b[k] =  b[k] - e[k]*b[k+1]  d[k]  return b  Symmetric Coefﬁcient Matrices  More often than not, coefﬁcient matrices that arise in engineering problems are symmetric as well as banded. Therefore, it is worthwhile to discover special prop- erties of such matrices and to learn how to use them in the construction of efﬁcient algorithms.  If the matrix A is symmetric, then the LU decomposition can be presented in the  form  A = LU = LDLT   2.23   where D is a diagonal matrix. An example is Choleski’s decomposition A = LLT that was discussed earlier  in this case D = I . For Doolittle’s decomposition we have  ⎡  ⎢⎢⎢⎢⎢⎢⎣  0 D1 0 D2 0 ... 0  0 0 0 D3 ... ... 0 0  ⎤  ⎥⎥⎥⎥⎥⎥⎦  ⎡  ⎢⎢⎢⎢⎢⎢⎣  ··· 0 ··· 0 ··· 0 ··· ... ··· Dn  1 L21 L31 0 L32 1 0 ... ... 0 0  1 0 ... 0  ··· ··· ··· ··· ···  ⎤  ⎥⎥⎥⎥⎥⎥⎦  Ln1 Ln2 Ln3 ... 1  U = DLT =   63  2.4 Symmetric and Banded Coefﬁcient Matrices  which gives  ⎡  ⎢⎢⎢⎢⎢⎢⎣  U =  D1 D1 L21 D1 L31 0 D2 L32 0 ... 0  D2 0 ... 0  D3 ... 0  ⎤  ⎥⎥⎥⎥⎥⎥⎦  ··· D1Ln1 ··· D2Ln2 ··· D3L3n ··· ···  ... Dn   2.24    2.25    2.26   We now see that during decomposition of a symmetric matrix only U has to be stored, because D and L can be easily recovered from U. Thus Gauss elimination, which results in an upper triangular matrix of the form shown in Eq.  2.24 , is sufﬁ- cient to decompose a symmetric matrix.  There is an alternative storage scheme that can be employed during LU decom-  position. The idea is to arrive at the matrix  ⎡  ⎢⎢⎢⎢⎢⎢⎣  ∗ =  U  D1 L21 L31 L32 0 D3 0 ... ... 0 0  D2 0 ... 0  ⎤  ⎥⎥⎥⎥⎥⎥⎦  ··· Ln1 ··· Ln2 ··· Ln3 ... ... ··· Dn  Here U can be recovered from Uij = Di L ji. It turns out that this scheme leads to a computationally more efﬁcient solution phase; therefore, we adopt it for symmetric, banded matrices.  Symmetric, Pentadiagonal Coefﬁcient Matrices We encounter pentadiagonal  bandwidth = 5  coefﬁcient matrices in the solution of fourth-order, ordinary differential equations by ﬁnite differences. Often these matri- ces are symmetric, in which case an n × n coefﬁcient matrix has the form  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  d1 e1 f1 0 ... 0 0 0  A =  e1 d2 e2 f2 ... ··· ··· ···  f1 e2 d3 e3 ... 0 0 0  0 f2 e3 d4 ... fn−4 0 0  0 0 0 0 f3 0 e4 f4 ... ... en−3 dn−2 fn−3 0  ··· ··· ··· ··· ... en−2 en−2 dn−1 fn−2 en−1  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  0 0 0 0 ... fn−2 en−1 dn   64  Systems of Linear Algebraic Equations  As in the case of tridiagonal matrices, we store the nonzero elements in the three  vectors  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  d1 d2 ... dn−2 dn−1 dn  ⎤  ⎥⎥⎥⎥⎥⎥⎦  ⎡  ⎢⎢⎢⎢⎢⎢⎣  e1 e2 ... en−2 en−1  ⎡ ⎢⎢⎢⎢⎣  ⎤ ⎥⎥⎥⎥⎦  f1 f2 ... fn−2  d =  e =  f =  Let us now look at the solution of the equations Ax = b by Doolittle’s decomposi- tion. The ﬁrst step is to transform A to upper triangular form by Gauss elimination. If elimination has progressed to the stage where the kth row has become the pivot row, we have the following situation  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  ... ··· ··· ··· ···  A =  ... ... ... 0 dk ek 0 ek dk+1 0 fk 0 0 ... ...  ... fk ek+1 ek+1 dk+2 fk+1 ...  ... 0 fk+1 ek+2 ek+2 dk+3 ... ...  ... 0 0 fk+2 ek+3 ...  ... 0 0 0 fk+3 ...  ←  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  ··· ··· ··· ··· ...  The elements ek and fk below the pivot row  the kth row  are eliminated by the oper- ations  row  k + 1  ← row  k + 1  −  ek  dk  × row k row  k + 2  ← row  k + 2  −  fk  dk  × row k  The only terms  other than those being eliminated  that are changed by the operations are  dk+1 ← dk+1 −  ek  dk ek ek+1 ← ek+1 −  ek  dk fk dk+2 ← dk+2 −  fk  dk fk   2.27a   Storage of the multipliers in the upper triangular portion of the matrix results in  ek ← ek  dk  fk ← fk  dk   2.27b    65  2.4 Symmetric and Banded Coefﬁcient Matrices  At the conclusion of the elimination phase the matrix has the form  do not confuse d, e, and f with the original contents of A   Now comes the solution phase. The equations Ly = b have the augmented co-  efﬁcient matrix  e1 d2 0 ... 0 0  f1 e2 d3 ... ··· ···  ··· 0 ··· f2 ··· e3 ··· ... 0 dn−1 0 0  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  0 0 0 ... en−1 dn  0 1 e2 f2 ... 0  0 0 1 e3 ... 0  0 0 0 1 ... fn−2  ··· 0 b1 ··· 0 b2 ··· 0 b3 ··· 0 b4 ··· ... ... en−1 1 bn  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  d1 0 0 ... 0 0  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  1 e1 f1 0 ... 0  ∗ =  U   cid:14    cid:15   =  L b  y1 = b1 y2 = b2 − e1y1  Solution by forward substitution yields   2.28   ...  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  yk = bk − fk−2yk−2 − ek−1yk−1, k = 3, 4, . . . , n  The equations to be solved by back substitution, namely Ux = y, have the augmented coefﬁcient matrix   cid:14    cid:15   =  U y  d1 d1e1 d1 f1 0 0 ... 0 0  0 d2e2 d2 f2 d3e3 d3 ... ... ··· 0 ··· 0  d2 0 ... 0 0  ··· ··· ··· ··· dn−1 dn−1en−1  0 0 0 ...  0  dn  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  y1 y2 y3 ... yn−1 yn  the solution of which is obtained by back substitution:  xn = yn dn xn−1 = yn−1 dn−1 − en−1xn xk = yk  dk − ek xk+1 − fk xk+2, k = n − 2, n − 3, . . . , 1  ...   66  Systems of Linear Algebraic Equations   cid:2  LUdecomp5  The function LUdecomp5 decomposes a symmetric, pentadiagonal matrix A of the form A = [f\e\d\e\f]. The original vectors d, e, and f are destroyed and replaced by the vectors of the decomposed matrix. After decomposition, the solution of Ax = b can be obtained by LUsolve5. During forward substitution, the original b is replaced by y. Similarly, y is written over by x in the back substitution phase, so that b contains the solution vector upon exit from LUsolve5.   module LUdecomp5  ’’’ d,e,f = LUdecomp5 d,e,f .  LU decomposition of symmetric pentadiagonal matrix [a], where  {f}, {e} and {d} are the diagonals of [a]. On output  {d},{e} and {f} are the diagonals of the decomposed matrix.  Solves [a]{x} = {b}, where {d}, {e} and {f} are the vectors  x = LUsolve5 d,e,f,b .  returned from LUdecomp5.  ’’’  def LUdecomp5 d,e,f :  n = len d   for k in range n-2 :  lam = e[k] d[k]  d[k+1] = d[k+1] - lam*e[k]  e[k+1] = e[k+1] - lam*f[k]  e[k] = lam  lam = f[k] d[k]  d[k+2] = d[k+2] - lam*f[k]  f[k] = lam  lam = e[n-2] d[n-2]  d[n-1] = d[n-1] - lam*e[n-2]  e[n-2] = lam  return d,e,f  def LUsolve5 d,e,f,b :  n = len d   b[1] = b[1] - e[0]*b[0]  for k in range 2,n :  b[k] = b[k] - e[k-1]*b[k-1] - f[k-2]*b[k-2]  b[n-1] = b[n-1] d[n-1]  b[n-2] = b[n-2] d[n-2] - e[n-2]*b[n-1]  for k in range n-3,-1,-1 :  b[k] = b[k] d[k] - e[k]*b[k+1] - f[k]*b[k+2]  return b   67  2.4 Symmetric and Banded Coefﬁcient Matrices  EXAMPLE 2.9 As a result of Gauss elimination, a symmetric matrix A was transformed to the upper triangular form  ⎡ ⎢⎢⎢⎣  ⎡ ⎢⎢⎢⎣  U =  4 −2 0 0 0  1 0 3 −3 2 1 3 −3 2 0 0 35 12 0  LT =  1 −1 2 0 0 0  1 4 1 −1 2 0 0  0 1 3 1 −1 2 1 0  ⎤ ⎥⎥⎥⎦  ⎤ ⎥⎥⎥⎦  Determine the original matrix A. Solution. First we ﬁnd L in the decomposition A = LU. Dividing each row of U by its diagonal element yields  A =  Therefore, A = LU, or ⎡ ⎢⎢⎢⎣ ⎡ ⎢⎢⎢⎣  =  1 0 −1 2 1 1 4 −1 2  0 0 0 0 1 0 1 3 −1 2 1 ⎤ ⎥⎥⎥⎦  1 0 4 −2 1 4 −2 1 −2 4  0  4 −2 −2 1 −2 0  ⎤ ⎥⎥⎥⎦  ⎡ ⎢⎢⎢⎣  ⎤ ⎥⎥⎥⎦  4 −2 0 0 0  1 0 3 −3 2 1 3 −3 2 0 0 35 12 0  EXAMPLE 2.10 Determine L and D that result from Doolittle’s decomposition A = LDLT of the sym- metric matrix  ⎡ ⎢⎣ 3 −3 −3 3  3 5 1 1 10  ⎤ ⎥⎦  A =  Solution. We use Gauss elimination, storing the multipliers in the upper triangular portion of A. At the completion of elimination, the matrix will have the form of U in Eq.  2.25 .  ∗  The terms to be eliminated in the ﬁrst pass are A 21 and A 31 using the elementary  operations  row 2 ← row 2 −  −1  × row 1 row 3 ← row 3 −  1  × row 1   68  Systems of Linear Algebraic Equations Storing the multipliers  −1 and 1  in the locations occupied by A 12 and A 13, we get  The second pass is the operation  yields, after overwriting A 23 with the multiplier 2  Hence  ⎡ ⎢⎣ 3 −1 1  ⎤ ⎥⎦  0 0  2 4 4 7   cid:3  = A  A  L =  0 0  ⎤ ⎥⎦  row 3 ← row 3 − 2 × row 2  cid:3  cid:3  = cid:28  ⎡ ⎢⎣ 1 0 0 −1 1 0 1 2 1   cid:29  = 0\D\LT ⎤ ⎥⎦ D =  ⎡ ⎢⎣ 3 −1 1 2 2 0 −1 ⎡ ⎢⎣ 3 0 0 0 2 0 0 0 −1 ⎤ ⎡  ⎤ ⎥⎦  A =  ⎢⎢⎢⎢⎢⎣  2 −1 −1 0 −1 0 0  0 0 0 2 −1 0 0 2 −1 0 2 −1 0 −1 0 −1 2 0  ⎥⎥⎥⎥⎥⎦ b =  ⎢⎢⎢⎢⎢⎣  ⎥⎥⎥⎥⎥⎦  5−5 4−5  5  EXAMPLE 2.11 Use the functions LUdecmp3 and LUsolve3 to solve Ax = b, where ⎤  ⎡  Solution  ! usr bin python   example2_11  import numpy as np  from LUdecomp3 import *  d = np.ones  5  *2.0  c = np.ones  4  * -1.0   b = np.array [5.0, -5.0, 4.0, -5.0, 5.0]   e = c.copy    c,d,e = LUdecomp3 c,d,e   x = LUsolve3 c,d,e,b   print "\nx =\n",x   input "\nPress return to exit"   The output is  x =  [ 2. -1.  1. -1.  2.]   69  2.5 Pivoting  2.5  Pivoting  Introduction  Sometimes the order in which the equations are presented to the solution algorithm has a profound effect on the results. For example, consider these equations:  The corresponding augmented coefﬁcient matrix is  2x1 − x2 = 1 −x1 + 2x2 − x3 = 0 −x2 + x3 = 0 ⎡ ⎢⎣ 2 −1 −1 0 −1   cid:15   =  0 1 2 −1 0 1 0  A b   cid:15   =  ⎡ ⎢⎣ 0 −1 −1 2 −1  1 0 2 −1 0 0 1  A b   cid:14    cid:14    cid:14   ⎤ ⎥⎦  ⎤ ⎥⎦  ⎤ ⎥⎦  Equations  a  are in the “right order” in the sense that we would have no trouble obtaining the correct solution x1 = x2 = x3 = 1 by Gauss elimination or LU decom- position. Now suppose that we exchange the ﬁrst and third equations, so that the augmented coefﬁcient matrix becomes  Because we did not change the equations  only their order was altered , the solution is still x1 = x2 = x3 = 1. However, Gauss elimination fails immediately because of the presence of the zero pivot element  the element A 11 .  This example demonstrates that it is sometimes essential to reorder the equa- tions during the elimination phase. The reordering, or row pivoting, is also required if the pivot element is not zero, but is very small in comparison to other elements in the pivot row, as demonstrated by the following set of equations:   cid:15   =  ⎡ ⎢⎣ ε −1 −1 2 −1  1 0 2 −1 0 0 1  A b  These equations are the same as Eqs.  b , except that the small number ε replaces the zero element in Eq.  b . Therefore, if we let ε → 0, the solutions of Eqs.  b  and  c  should become identical. After the ﬁrst phase of Gauss elimination, the augmented coefﬁcient matrix becomes   cid:14    cid:15    cid:3  A   cid:3  b  =  ⎡ ⎢⎣ ε 0 0 −1 + 2 ε  −1 0 2 − 1 ε −1 + 1 ε 0 1  1 −2 ε  ⎤ ⎥⎦  Because the computer works with a ﬁxed word length, all numbers are rounded off to a ﬁnite number of signiﬁcant ﬁgures. If ε is very small, then 1 ε is huge, and an   a    b    c    d    70  Systems of Linear Algebraic Equations element such as 2 − 1 ε is rounded to −1 ε. Therefore, for sufﬁciently small ε, Eqs.  d  are actually stored as   cid:14    cid:15    cid:3   A   cid:3   b  =  ⎡ ⎢⎣ ε −1 0 −1 ε 0  1 1 ε  0 0 2 ε −2 ε 1  ⎤ ⎥⎦  Because the second and third equations obviously contradict each other, the solution process fails again. This problem would not arise if the ﬁrst and second, or the ﬁrst and the third equations were interchanged in Eqs.  c  before the elimination.  The last example illustrates the extreme case where ε was so small that roundoff errors resulted in total failure of the solution. If we were to make ε somewhat bigger so that the solution would not “bomb” any more, the roundoff errors might still be large enough to render the solution unreliable. Again, this difﬁculty could be avoided by pivoting.  Diagonal Dominance An n × n matrix A is said to be diagonally dominant if each diagonal element is larger than the sum of the other elements in the same row  we are talking here about abso- lute values . Thus diagonal dominance requires that   cid:19  cid:19 Aij   cid:19  cid:19   i = 1, 2, . . . , n   Aii >  n cid:6   j=1 j cid:7 =i   2.30   For example, the matrix  ⎡ ⎢⎣−2 1 −1 4 −2  4 −1 3 1  ⎤ ⎥⎦  ⎡ ⎢⎣ 4 −2 −2 1 −1  1 4 −1 3  ⎤ ⎥⎦  is not diagonally dominant. However, if we rearrange the rows in the following man- ner  then we have diagonal dominance.  It can be shown that if the coefﬁcient matrix of the equations Ax = b is diagonally dominant, then the solution does not beneﬁt from pivoting; that is, the equations are already arranged in the optimal order. It follows that the strategy of pivoting should be to reorder the equations so that the coefﬁcient matrix is as close to diagonal dom- inance as possible. This is the principle behind scaled row pivoting, discussed next.   71  2.5 Pivoting  Gauss Elimination with Scaled Row Pivoting Consider the solution of Ax = b by Gauss elimination with row pivoting. Recall that pivoting aims at improving diagonal dominance of the coefﬁcient matrix  i.e., making the pivot element as large as possible in comparison to other elements in the pivot row . The comparison is made easier if we establish an array s with the elements   cid:19  cid:19 Aij   cid:19  cid:19  ,  si = max  j  i = 1, 2, . . . , n  Thus si, called the scale factor of row i, contains the absolute value of the largest ele- ment in the ith row of A. The vector s can be obtained with the algorithm  for i in range n :  s[i] = max abs a[i,:]    The relative size of an element Aij  that is, relative to the largest element in the  ith row  is deﬁned as the ratio   2.31    2.32    cid:19  cid:19    cid:19  cid:19 Aij  si  rij =  Suppose that the elimination phase has reached the stage where the kth row has become the pivot row. The augmented coefﬁcient matrix at this point is shown in the following matrix:  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  A 11 A 12 A 13 A 14 A 22 A 23 A 24 0 A 33 A 34 0 0 ... ... ... ... ··· A kk 0 0 ··· ... ... ... ··· Ank 0 0  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ ←  ··· A 1n b1 ··· A 2n b2 ··· A 3n b3 ··· ... ··· A kn bk ··· ... ··· A nn bn  ...  ...  We do not automatically accept A kk as the next pivot element, but look in the kth column below A kk for a “better” pivot. The best choice is the element A pk that has the largest relative size; that is, we choose p such that j ≥ k  r pk = max  r j k   cid:31    cid:30   ,  j  If we ﬁnd such an element, then we interchange the rows k and p, and proceed with the elimination pass as usual. Note that the corresponding row interchange must also be carried out in the scale factor array s. The algorithm that does all this is as follows:  for k in range 0,n-1 :   Find row containing element with largest relative size  p = argmax abs a[k:n,k]  s[k:n]  + k   If this element is very small, matrix is singular   72  Systems of Linear Algebraic Equations  if abs a[p,k]  < tol: error.err ’Matrix is singular’    Check whether rows k and p must be interchanged  if p != k:   Interchange rows if needed  swap.swapRows b,k,p   swap.swapRows s,k,p   swap.swapRows a,k,p    Proceed with elimination  The Python statement argmax v  returns the index of the largest element in the vector v. The algorithms for exchanging rows  and columns  are included in the module swap shown next.   cid:2  swap  The function swapRows interchanges rows i and j of a matrix or vector v, whereas swapCols interchanges columns i and j of a matrix.   module swap  ’’’ swapRows v,i,j .  Swaps rows i and j of a vector or matrix [v].  swapCols v,i,j .  Swaps columns of matrix [v].  ’’’  def swapRows v,i,j :  if len v.shape  == 1:  v[i],v[j] = v[j],v[i]  else:  v[[i,j],:] = v[[j,i],:]  def swapCols v,i,j :  v[:,[i,j]] = v[:,[j,i]]   cid:2  gaussPivot  The function gaussPivot performs Gauss elimination with row pivoting. Apart from row swapping, the elimination and solution phases are identical to gaussE- limin in Section 2.2.   module gaussPivot  ’’’ x = gaussPivot a,b,tol=1.0e-12 .  Solves [a]{x} = {b} by Gauss elimination with  scaled row pivoting   73  2.5 Pivoting  ’’’  import numpy as np  import swap  import error  def gaussPivot a,b,tol=1.0e-12 :  n = len b    Set up scale factors  s = np.zeros n   for i in range n :  s[i] = max np.abs a[i,:]    for k in range 0,n-1 :   Row interchange, if needed  p = np.argmax np.abs a[k:n,k]  s[k:n]  + k  if abs a[p,k]  < tol: error.err ’Matrix is singular’   if p != k:  swap.swapRows b,k,p   swap.swapRows s,k,p   swap.swapRows a,k,p    Elimination  for i in range k+1,n :  if a[i,k] != 0.0:  lam = a[i,k] a[k,k]  a[i,k+1:n] = a[i,k+1:n] - lam*a[k,k+1:n]  b[i] = b[i] - lam*b[k]  if abs a[n-1,n-1]  < tol: error.err ’Matrix is singular’    Back substitution  b[n-1] = b[n-1] a[n-1,n-1]  for k in range n-2,-1,-1 :  b[k] =  b[k] - np.dot a[k,k+1:n],b[k+1:n]   a[k,k]  return b   cid:2  LUpivot  The Gauss elimination algorithm can be changed to Doolittle’s decomposition with minor changes. The most important of these changes is keeping a record of the row interchanges during the decomposition phase. In LUdecomp this record is kept in the array seq. Initially seq contains [0, 1, 2, . . .]. Whenever two rows are in- terchanged, the corresponding interchange is also carried out in seq. Thus seq   74  Systems of Linear Algebraic Equations  shows the order in which the original rows have been rearranged. This informa- tion is passed on to the solution phase  LUsolve , which rearranges the elements of the constant vector in the same order before proceeding to forward and back substitutions.   module LUpivot  ’’’ a,seq = LUdecomp a,tol=1.0e-9 .  LU decomposition of matrix [a] using scaled row pivoting.  The returned matrix [a] = contains [U] in the upper  triangle and the nondiagonal terms of [L] in the lower triangle.  Note that [L][U] is a row-wise permutation of the original [a];  the permutations are recorded in the vector {seq}.  x = LUsolve a,b,seq .  Solves [L][U]{x} = {b}, where the matrix [a] = and the  permutation vector {seq} are returned from LUdecomp.  ’’’  import numpy as np  import swap  import error  def LUdecomp a,tol=1.0e-9 :  n = len a   seq = np.array range n     Set up scale factors  s = np.zeros  n    for i in range n :  s[i] = max abs a[i,:]    for k in range 0,n-1 :  if p != k:  swap.swapRows s,k,p   swap.swapRows a,k,p   swap.swapRows seq,k,p    Elimination  for i in range k+1,n :  if a[i,k] != 0.0:  lam = a[i,k] a[k,k]   Row interchange, if needed  p = np.argmax np.abs a[k:n,k]  s[k:n]  + k  if abs a[p,k]  < tol: error.err ’Matrix is singular’    75  2.5 Pivoting  a[i,k+1:n] = a[i,k+1:n] - lam*a[k,k+1:n]   Rearrange constant vector; store it in [x]  a[i,k] = lam  return a,seq  def LUsolve a,b,seq :  n = len a   x = b.copy    for i in range n :  x[i] = b[seq[i]]   Solution  for k in range 1,n :  return x  When to Pivot  x[k] = x[k] - np.dot a[k,0:k],x[0:k]   x[n-1] = x[n-1] a[n-1,n-1]  for k in range n-2,-1,-1 :  x[k] =  x[k] - np.dot a[k,k+1:n],x[k+1:n]   a[k,k]  Pivoting has two drawbacks. One is the increased cost of computation; the other is the destruction of symmetry and banded structure of the coefﬁcient matrix. The lat- ter is of particular concern in engineering computing, where the coefﬁcient matrices are frequently banded and symmetric, a property that is used in the solution, as seen in the previous section. Fortunately, these matrices are often diagonally dominant as well, so that they would not beneﬁt from pivoting anyway.  There are no infallible rules for determining when pivoting should be used. Ex- perience indicates that pivoting is likely to be counterproductive if the coefﬁcient matrix is banded. Positive deﬁnite and, to a lesser degree, symmetric matrices also seldom gain from pivoting. And we should not forget that pivoting is not the only means of controlling roundoff errors—there is also double precision arithmetic.  It should be strongly emphasized that these rules of thumb are only meant for equations that stem from real engineering problems. It is not difﬁcult to concoct “textbook” examples that do not conform to these rules.  EXAMPLE 2.12 Employ Gauss elimination with scaled row pivoting to solve the equations Ax = b, where  ⎡ ⎢⎣ 2 −2 6 −2 −1  4 3 8 4  ⎤ ⎥⎦  A =  ⎤ ⎥⎦  ⎡ ⎢⎣ 16 0 −1  b =   76  Systems of Linear Algebraic Equations  Solution. The augmented coefﬁcient matrix and the scale factor array are   cid:14   A b   cid:15   =  ⎡ ⎢⎣ 2 −2 6 −2 −1  16 4 3 0 8 4 −1  ⎤ ⎥⎦  ⎡ ⎢⎣ 6  ⎤ ⎥⎦  4 8  s =  Note that s contains the absolute value of the biggest element in each row of A. At this stage, all the elements in the ﬁrst column of A are potential pivots. To determine the best pivot element, we calculate the relative sizes of the elements in the ﬁrst column:  ⎤ ⎥⎦ =  ⎡ ⎢⎣r11  r21 r31  ⎤ ⎥⎦ =  ⎡ ⎢⎣A 11  s1 A 21  s2 A 31  s3  ⎤ ⎥⎦  ⎡ ⎢⎣ 1 3  1 2 1 8  Since r21 is the biggest element, we conclude that A 21 makes the best pivot element. Therefore, we exchange rows 1 and 2 of the augmented coefﬁcient matrix and the scale factor array, obtaining   cid:14   A b   cid:15   =  ⎡ ⎢⎣−2 0 4 3 2 −2 6 16 8 4 −1 −1  ⎤ ⎥⎦ ←  ⎡ ⎢⎣ 4  ⎤ ⎥⎦  6 8  s =  Now the ﬁrst pass of Gauss elimination is carried out  the arrow points to the pivot row , yielding   cid:14    cid:15    cid:3  A   cid:3  b  =  ⎡ ⎢⎣ 4  ⎤ ⎥⎦  6 8  s =  ⎤ ⎥⎦  3 9  ⎡ ⎢⎣−2 4 0 0 2 16 0 6 5 2 −1 ⎤ ⎡ ⎥⎦ = ⎢⎣  ⎤ ⎥⎦ =  ∗  A 22  s2 A 32  s3  r22 r32  ⎤ ⎥⎦  ⎡ ⎢⎣ ∗  1 3 3 4  determine the “winner” from⎡ ⎢⎣ ∗  The potential pivot elements for the next elimination pass are A   cid:3  22 and A   cid:3  32. We  Note that r12 is irrelevant, because row 1 already acted as the pivot row. Therefore, it is excluded from further consideration. Because r32 is bigger than r22, the third row is the better pivot row. After interchanging rows 2 and 3, we have   cid:15    cid:3   A   cid:3   b  =  ⎡ ⎢⎣−2 4 0 0 6 5 2 −1 0 2 16  3  9  ⎤ ⎥⎦ ← s =  ⎡ ⎢⎣ 4  ⎤ ⎥⎦  The second elimination pass now yields   cid:15    cid:14    cid:3  cid:3  A   cid:3  cid:3  b  =  U c  ⎡ ⎢⎣−2 4   cid:15   =  0 3 5 2 −1 0 6 0 0 49 6 49 3  8 6  ⎤ ⎥⎦   cid:14    cid:14    77  2.5 Pivoting  This completes the elimination phase. It should be noted that U is the matrix that would result from LU decomposition of the following row-wise permutation of A  the ordering of rows is the same as achieved by pivoting :  ⎡ ⎤ ⎢⎣−2 ⎥⎦ 4 3 −1 8 4 2 −2 6  cid:14   Because the solution of Ux = c by back substitution is not affected by pivoting, we skip the details computation. The result is xT = Alternate Solution. It is not necessary to physically exchange equations during piv- oting. We could accomplish Gauss elimination just as well by keeping the equations in place. The elimination would then proceed as follows  for the sake of brevity, the details of choosing the pivot equation are not repeated :  1 −1 2  .   cid:15    cid:14    cid:14    cid:14   =  A b   cid:3  A   cid:3  b  =   cid:15    cid:15    cid:15    cid:3  cid:3  A   cid:3  cid:3  b  =  16 0 4 3 8 4 −1  ⎤ ⎥⎦ ← ⎤ ⎥⎦  ⎡ ⎢⎣ 2 −2 6 −2 −1 ⎡ ⎢⎣ 0 2 16 −2 4 0 0 6 5 2 −1 ⎡ ⎢⎣ 0 0 49 6 49 3 −2 4 3 0 5 2 −1 0 6  9 3  ← ⎤ ⎥⎦  Yet now the back substitution phase is a little more involved, because the order in which the equations must be solved has become scrambled. In hand computations this is not a problem, because we can determine the order by inspection. Unfortu- nately, “by inspection” does not work on a computer. To overcome this difﬁculty, we have to maintain an integer array p that keeps track of the row permutations during the elimination phase. The contents of p indicate the order in which the pivot rows were chosen. In this example, we would have at the end of Gauss elimination  ⎡ ⎢⎣ 2  ⎤ ⎥⎦  3 1  p =  showing that row 2 was the pivot row in the ﬁrst elimination pass, followed by row 3 in the second pass. The equations are solved by back substitution in the reverse order: Equation 1 is solved ﬁrst for x3, then equation 3 is solved for x2, and ﬁnally equation 2 yields x1.  By dispensing with swapping of equations, the scheme outlined here is claimed to result in a faster algorithm than gaussPivot. This may be true if the programs are written in Fortran or C, but our tests show that in Python gaussPivot is about 30% faster than the in-place elimination scheme.   78  Systems of Linear Algebraic Equations  PROBLEM SET 2.2 1. Solve the equations Ax = b by utilizing Doolittle’s decomposition, where  A =  ⎡ ⎢⎣ 3 −3 3 −3 3  ⎡ ⎢⎣ 9−7 ⎡ 2. Use Doolittle’s decomposition to solve Ax = b, where ⎢⎣  ⎤ ⎥⎦ ⎤ ⎥⎦  ⎡ ⎢⎣ 4 8 20 8 13 16 20 16 −91  5 1 1 5  A =  b =  b =  12  24  18−119  ⎤ ⎥⎦  ⎤ ⎥⎦  3. Determine L and D that result from Doolittle’s decomposition of the symmetric  4. Solve the tridiagonal equations Ax = b by Doolittle’s decomposition method,  ⎡  ⎢⎢⎢⎢⎢⎣  A =  ⎤  ⎥⎥⎥⎥⎥⎦  2 −2 −2 0 −6 0 0 0 0  0 0 0 5 −6 0 0 16 12 0 39 −6 12 0 −6 14  matrix  where  5. Use Gauss elimination with scaled row pivoting to solve  ⎡  ⎢⎢⎢⎢⎢⎣  A =  6 2 0 0 0 −1 7 2 0 0 0 −2 8 2 0 0 3 7 −2 0 5 0 0 3 0  ⎡ ⎢⎣ 4 −2 −2 −2  1 1 −1 6 3  ⎤ ⎥⎦  ⎤  ⎥⎥⎥⎥⎥⎦  ⎤  ⎥⎥⎥⎥⎥⎦  ⎡  ⎢⎢⎢⎢⎢⎣  2 −3 4 −3 1  b =  0  x2 x3  ⎡ ⎤ ⎢⎣ x1 ⎥⎦ = ⎤ ⎥⎦ ⎡ ⎤ ⎢⎢⎢⎣ ⎥⎥⎥⎦  ⎡ ⎤ ⎢⎣ 2−1 ⎥⎦ ⎡ ⎢⎣ 0.02−0.73−6.63 ⎡ ⎤ ⎢⎢⎢⎣ ⎥⎥⎥⎦  b =  ⎤ ⎥⎥⎥⎦ =  x1 x2 x3 x4  1 0 0 0  ⎤ 6. Solve Ax = b by Gauss elimination with scaled row pivoting, where ⎥⎦  A =  ⎡ ⎢⎣ 2.34 −4.10 1.98 2.36 −15.17  1.78 3.47 −2.22 6.81  7. Solve the equations ⎡ ⎢⎢⎢⎣  2 −1 0 0 −1 −1  0 0 0 −1 1 2 −1 2 −1 0  by Gauss elimination with scaled row pivoting.   79  2.5 Pivoting  8.  cid:2  Solve the equations⎡ ⎢⎢⎢⎣  5 −1 0 2 0 3 1 2 −2 −1 1 3 3 −1 2 3  ⎤ ⎥⎥⎥⎦  ⎡ ⎢⎢⎢⎣  ⎤ ⎥⎥⎥⎦ =  ⎡ ⎢⎢⎢⎣  ⎤ ⎥⎥⎥⎦  −3 3−2  5  x1 x2 x3 x4  9.  cid:2  Solve the symmetric, tridiagonal equations  4x1 − x2 = 9 −xi−1 + 4xi − xi+1 = 5, −xn−1 + 4xn = 5  i = 2, . . . , n − 1  with n = 10.  10.  cid:2  Solve the equations Ax = b, where  1.3174 2.7250 2.7250 1.7181 0.4002 0.8278 1.2272 2.5322 0.8218 1.5608 0.3629 2.9210 1.9664 2.0011 0.6532 1.9945  b =  8.4855 4.9874 5.6665 6.6152  11.  cid:2  Solve the following equations:  ⎡ ⎢⎢⎢⎣  A =  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  10 −2 −1 11 3 5 12 7 1 7 −2 8 2 −15 −1 9 4 −1 −1  1 −4 3 2 10 −3 3 3 −12 5 2 3 1 4 −1 1 12 −1 2 1 4 −7 −1 1 3 −4 1 3  7 3 −4 3 2 2 4 3 8 4 1 1 −1 −3 6  7  4  ⎤ ⎥⎥⎥⎦  ⎡ ⎢⎢⎢⎣ ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  =  ⎤ ⎥⎥⎥⎦ ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  0 12 −5 3 −25−26 9 −7  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  x1 x2 x3 x4 x5 x6 x7 x8  12.  cid:2  The displacement formulation for the mass-spring system shown in Fig.  a   results in the following equilibrium equations of the masses  ⎡ ⎢⎣ k1 + k2 + k3 + k5  −k3 −k5  −k3 k3 + k4 −k4  −k5 −k4 k4 + k5  ⎤ ⎥⎦ =  ⎤ ⎥⎦  ⎡ ⎢⎣ x1  x2 x3  ⎤ ⎥⎦  ⎡ ⎢⎣ W1  W2 W3  where ki are the spring stiffnesses, Wi represent the weights of the masses, and xi are the displacements of the masses from the undeformed conﬁguration of the system. Write a program that solves these equations for given k and W. Use the program to ﬁnd the displacements if  k1 = k3 = k4 = k W1 = W3 = 2W  k2 = k5 = 2k W2 = W   80  Systems of Linear Algebraic Equations  k2  k  W k  W1  1x  k1  k3  k4  2x  W2  k5  k  x3  W3   a   1x  2x  x3  k  x 4  x 5  k  k W k W k W k W  b   ⎤  ⎥⎥⎥⎥⎥⎦  ⎡  ⎢⎢⎢⎢⎢⎣  2.4 m  1.8 m  3u  2u  1u  5u  4u 45 kN  13.  cid:2  The equlibrium equations of the mass-spring system in Fig.  b  are  ⎡  ⎢⎢⎢⎢⎢⎣  2 −1 −1 0 −1 0 0  0 0 0 4 −1 0 0 4 −1 −2 0 −1 2 −1 0 −2 −1 3  ⎤  ⎥⎥⎥⎥⎥⎦ =  ⎡  ⎢⎢⎢⎢⎢⎣  ⎤  ⎥⎥⎥⎥⎥⎦  W k W k W k W k W k  x1 x2 x3 x4 x5  where k are the spring stiffnesses, W represent the weights of the masses, and xi are the displacements of the masses from the undeformed conﬁguration of the system. Determine the displacements.  14.  cid:2   The displacement formulation for a plane truss is similar to that of a mass-spring system. The differences are  1  the stiffnesses of the members are ki =  EA  L i, where E is the modulus of elasticity, A represents the cross-sectional area, and L is the length of the member; and  2  there are two components of displacement at each joint. For the statically indeterminate truss shown, the displacement for- mulation yields the symmetric equations Ku = p, where  ⎡  ⎢⎢⎢⎢⎢⎣  K =  27.58 7.004 −7.004 −5.253 0 0 0 −24.32  cid:14   7.004 −7.004 0 0 29.57 −5.253 0 −24.32 29.57 0 0 27.58 −7.004 0 0 −7.004  cid:15  29.57  p =  0 0 0 0 −45  T kN  ⎤  ⎥⎥⎥⎥⎥⎦ MN m  Determine the displacements ui of the joints.   81  2.5 Pivoting  15.  cid:2   P6  P3 45o P1  P4  5P 45o P2 18 kN  12 kN  In the force formulation of a truss, the unknowns are the member forces Pi. For the statically determinate truss shown, force formulation is obtained by writing down the equilibrium equations of the joints  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎣  √ −1 1 −1  √ 0 0 1  0 −1 0 0 0 0 0 0  2 0 0 0 √ 2 0 0 1 0 −1  √ 0 2 0 √ 2 0 0 1  0 √ 0 0 1  2 1 0 −1 −1  2 0  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎦  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎣  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎦  P1 P2 P3 P4 P5 P6  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎣  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎦  0 18 0 12 0 0  =  where the units of Pi are kN.  a  Solve the equations as they are with a computer program.  b  Rearrange the rows and columns so as to obtain a lower triangu- lar coefﬁcient matrix, and then solve the equations by back substitution using a calculator.  16.  cid:2   4P  3P  3P  2P  θ  θ  1P  2P  θθ  1P  5P  Load = 1  5P  ⎡  ⎢⎢⎢⎢⎢⎣  ⎤  ⎥⎥⎥⎥⎥⎦  ⎡  ⎢⎢⎢⎢⎢⎣  ⎤  ⎥⎥⎥⎥⎥⎦ =  ⎡  ⎢⎢⎢⎢⎢⎣  ⎤  ⎥⎥⎥⎥⎥⎦  0 0 1 0 0  P1 P2 P3 P4 P5  c 1 0 s 0 0 2s 0 −c s 0  0 0 0 0 0 1 0 0 c 1 0 0 0 s  The force formulation of the symmetric truss shown results in the joint equilib- rium equations  where s = sin θ, c = cos θ, and Pi are the unknown forces. Write a program that computes the forces, given the angle θ. Run the program with θ = 53 ◦  .   82  Systems of Linear Algebraic Equations  17.  cid:2   Ω 20  i1  R  3  0  Ω  5  0  Ω  i2 15Ω  i3 120 V 0 V   cid:27   voltage drops = cid:27   The electrical network shown can be viewed as consisting of three loops. Apply- ing Kirchoff’s law   voltage sources  to each loop yields the following equations for the loop currents i1, i2 and i3:  50 + R i1 − Ri2 − 30i3 = 0 −Ri1 +  65 + R i2 − 15i3 = 0  −30i2 − 15i2 + 45i3 = 120  Compute the three loop currents for R = 5  cid:7 , 10  cid:7 , and 20  cid:7 .  18.  cid:2   -120 V  i 1  50Ω  30Ω  +120 V  Ω 5 1  Ω 0 1  i 2  Ω 0 1  i  3  Ω 5  25 Ω  Ω20  Ω 5 1  i 4  30Ω  Determine the loop currents i1 to i4 in the electrical network shown.  19.  cid:2  Consider the n simultaneous equations Ax = b, where  Aij =  i + j  2  i = 0, 1, . . . , n − 1,  j = 0, 1, . . . , n − 1  Aij ,  bi = n−1 cid:6   cid:14   j=0   cid:15 T  Clearly, the solution is x = . Write a program that solves these 1 equations for any given n  pivoting is recommended . Run the program with n = 2, 3 and 4, and comment on the results.  1 1 ···   83  2.5 Pivoting  20.  cid:2   8m  s3  6m  s3  3m  s3  2m  s3  C1  C2  C4  C5  4m  s3  2m  s3  5m  s3  4m  s3  C3  6m  s3  2m  s3  c = 15 mg m3  4m  s3  c = 20 mg m 3  The diagram shows ﬁve mixing vessels connected by pipes. Water is pumped through the pipes at the steady rates shown on the diagram. The incoming wa- ter contains a chemical, the amount of which is speciﬁed by its concentration c  mg m3 . Applying the principle of conservation of mass  mass of chemical ﬂowing in = mass of chemical ﬂowing out  to each vessel, we obtain the following simultaneous equations for the concen- trations ci within the vessels:  −8c1 + 4c2 = −80  8c1 − 10c2 + 2c3 = 0 6c2 − 11c3 + 5c4 = 0 3c3 − 7c4 + 4c5 = 0  2c4 − 4c5 = −30  Note that the mass ﬂow rate of the chemical is obtained by multiplying the vol- ume ﬂow rate of the water by the concentration. Verify the equations and deter- mine the concentrations.  21.  cid:2   2m  s3  c = 25 mg m3  4m  s3  4m  s3  c1  c3  4m  s3  2m  s3  m  s33  1 m  s3  c2  c4  m  s33  1 m  s3  c = 50 mg m 3  Four mixing tanks are connected by pipes. The ﬂuid in the system is pumped through the pipes at the rates shown in the ﬁgure. The ﬂuid entering the system contains a chemical of concentration c as indicated. Determine the concentra- tion of the chemical in the four tanks, assuming a steady state.   84  Systems of Linear Algebraic Equations  22.  cid:2  Solve the following equations:  23.  cid:2  Write a program that solves the complex tridiagonal equations Ax = b, where  7x1 − 4x2 + x3 = 1 −4x1 + 6x2 − 4x3 + x4 = 1  xi−2 − 4xi−1 + 6xi − 4xi+1 + xi+2 = 1  i = 3, 4, . . . , 8   x7 − 4x8 + 6x9 − 4x10 = 1 x8 − 4x9 + 7x10 = 1 ⎤  2 −i −i 0 −i ... ... 0 0 0 0  ··· 0 0 0 2 −i ··· 0 0 2 −i ··· 0 ... ... ... ... ··· −i 2 −i ··· −i 1 0  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  A =  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  b =  100 + 100i  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  0 0 ... 0 0  The program should accommodate n equations, where n is arbitrary. Test it with n = 10.  ∗2.6 Matrix Inversion  Computing the inverse of a matrix and solving simultaneous equations are related tasks. The most economical way to invert an n × n matrix A is to solve the equations  AX = I  −1AX = A  −1I, which reduces to X = A   2.33  where I is the n × n identity matrix. The solution X, also of size n × n, will be the −1 inverse of A. The proof is simple: After we pre-multiply both sides of Eq.  2.33  by A we have A Inversion of large matrices should be avoided whenever possible because of its high cost. As seen from Eq.  2.33 , inversion of A is equivalent to solving Axi= bi with i = 1, 2, . . . , n, where bi is the ith column of I. Assuming that LU decomposition is employed in the solution, the solution phase  forward and back substitution  must be repeated n times, once for each bi. Since the cost of computation is proportional to n3 for the decomposition phase and n2 for each vector of the solution phase, the cost of inversion is considerably more expensive than the solution of Ax = b  single constant vector b .  −1.  Matrix inversion has another serious drawback—a banded matrix loses its struc- −1 is  ture during inversion. In other words, if A is banded or otherwise sparse, then A fully populated.   85  ∗2.6 Matrix Inversion  EXAMPLE 2.13 Write a function that inverts a matrix using LU decomposition with pivoting. Test the function by inverting  ⎡ ⎢⎣ 0.6 −0.4 1.0 −0.3 0.2 0.5 0.6 −1.0 0.5  ⎤ ⎥⎦  A =  Solution. The following function matInv uses the decomposition and solution pro- cedures in the module LUpivot.  ! usr bin python   example2_13  import numpy as np  from LUpivot import *  def matInv a :  n = len a[0]   aInv = np.identity n   a,seq = LUdecomp a   for i in range n :  aInv[:,i] = LUsolve a,aInv[:,i],seq   return aInv  a = np.array [[ 0.6, -0.4,  1.0],\  [-0.3,  0.2,  0.5],\  [ 0.6, -1.0,  0.5]]   aOrig = a.copy     Save original [a]  aInv = matInv a    Invert [a]  original [a] is destroyed   print "\naInv =\n",aInv   print "\nCheck: a*aInv =\n", np.dot aOrig,aInv    input "\nPress return to exit"   The output is  aInv =  [[ 1.66666667 -2.22222222 -1.11111111]  [ 1.25  [ 0.5  -0.83333333 -1.66666667]  1.  0.  ]]  Check: a*aInv =  [[  1.00000000e+00  -4.44089210e-16  -1.11022302e-16]  [  [  0.00000000e+00  1.00000000e+00  5.55111512e-17]  0.00000000e+00  -3.33066907e-16  1.00000000e+00]]   86  Systems of Linear Algebraic Equations  EXAMPLE 2.14 Invert the matrix  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎣  A =  2 −1 −1 0 −1 0 0 0  0 0 0 0 2 −1 0 0 0 2 −1 0 0 0 −1 2 −1 0 2 −1 0 −1 0 0 −1 5 0 0  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎦  Solution. Because the matrix is tridiagonal, we solve AX = I using the functions in the module LUdecomp3  LU decomposition of tridiagonal matrices :  ! usr bin python   example2_14  import numpy as np  from LUdecomp3 import *  n = 6  d = np.ones  n  *2.0  e = np.ones  n-1  * -1.0   c = e.copy    d[n-1] = 5.0  aInv = np.identity n   c,d,e = LUdecomp3 c,d,e   for i in range n :  aInv[:,i] = LUsolve3 c,d,e,aInv[:,i]   print "\nThe inverse matrix is:\n",aInv   input "\nPress return to exit"   Running the program results in the following output:  The inverse matrix is:  [[ 0.84  0.68  0.52  0.36  [ 0.68  1.36  1.04  0.72  [ 0.52  1.04  1.56  1.08  [ 0.36  0.72  1.08  1.44  [ 0.2  0.4  0.6  0.8  0.2  0.4  0.6  0.8  1.  0.04]  0.08]  0.12]  0.16]  0.2 ]  [ 0.04  0.08  0.12  0.16  0.2  0.24]]]  Note that A is tridiagonal, whereas A  −1 is fully populated.   ∗2.7 Iterative Methods 87 ∗2.7 Iterative Methods  Introduction  So far, we have discussed only direct methods of solution. The common character- istic of these methods is that they compute the solution with a ﬁnite number of op- erations. Moreover, if the computer were capable of inﬁnite precision  no roundoff errors , the solution would be exact.  Iterative, or indirect methods, start with an initial guess of the solution x and then repeatedly improve the solution until the change in x becomes negligible. Because the required number of iterations can be large, the indirect methods are, in general, slower than their direct counterparts. However, iterative methods do have the follow- ing two advantages that make them attractive for certain problems:  1. It is feasible to store only the nonzero elements of the coefﬁcient matrix. This makes it possible to deal with very large matrices that are sparse, but not neces- sarily banded. In many problems, there is no need to store the coefﬁcient matrix at all.  2. Iterative procedures are self-correcting, meaning that roundoff errors  or even  arithmetic mistakes  in one iterative cycle are corrected in subsequent cycles.  A serious drawback of iterative methods is that they do not always converge to the solution. It can be shown that convergence is guaranteed only if the coefﬁcient matrix is diagonally dominant. The initial guess for x plays no role in determining whether convergence takes place—if the procedure converges for one starting vector, it would do so for any starting vector. The initial guess affects only the number of iterations that are required for convergence.  Gauss-Seidel Method The equations Ax = b are in scalar notation  Aij xj = bi,  i = 1, 2, . . . , n  Extracting the term containing xi from the summation sign yields  n cid:6   j=1  Aiixi + n cid:6   j=1 j cid:7 =i  Aij xj = bi,  i = 1, 2, . . . , n  ⎛ ⎜⎜⎝bi − n cid:6   j=1 j cid:7 =i  xi = 1 Aii  ⎞ ⎟⎟⎠ ,  Aij x j  i = 1, 2, . . . , n  Solving for xi, we get   88  Systems of Linear Algebraic Equations  The last equation suggests the following iterative scheme:  ⎛ ⎜⎜⎝bi − n cid:6   j=1 j cid:7 =i  ⎞ ⎟⎟⎠ ,  xi ← 1 Aii  Aij xj  i = 1, 2, . . . , n   2.34   We start by choosing the starting vector x. If a good guess for the solution is not avail- able, x can be chosen randomly. Equation  2.34  is then used to recompute each ele- ment of x, always using the latest available values of xj . This completes one iteration cycle. The procedure is repeated until the changes in x between successive iteration cycles become sufﬁciently small.  Convergence of the Gauss-Seidel method can be improved by a technique known as relaxation. The idea is to take the new value of xi as a weighted average of its previ- ous value and the value predicted by Eq.  2.34 . The corresponding iterative formula is  ⎞ ⎟⎟⎠ +  1 − ω xi,  Aij xj  ⎛ ⎜⎜⎝bi − n cid:6   j=1 j cid:7 =i  xi ← ω Aii  i = 1, 2, . . . , n   2.35   where the weight ω is called the relaxation factor. It can be seen that if ω = 1, no relaxation takes place, because Eqs.  2.34  and  2.35  produce the same result. If ω < 1, Eq.  2.35  represents interpolation between the old xi and the value given by Eq.  2.34 . This is called under-relaxation. In cases where ω > 1, we have extrapolation, or over-relaxation.  however, an estimate can be computed during run time. Let  cid:9 x k  = cid:19  cid:19 x k−1  − x k    cid:19  cid:19  be the magnitude of the change in x during the kth iteration  carried out without relaxation [i.e., with ω = 1] . If k is sufﬁciently large  say k ≥ 5 , it can be shown2 that an approximation of the optimal value of ω is  There is no practical method of determining the optimal value of ω beforehand;  ωopt ≈   cid:26  1 − cid:30   1 +  2   cid:9 x k+p   cid:9 x k    cid:31 1 p   2.36   where p is a positive integer.  The essential elements of a Gauss-Seidel algorithm with relaxation are as follows:  Carry out k iterations with ω = 1  k = 10 is reasonable . Record  cid:9 x k . Perform additional p iterations. Record  cid:9 x k+p . Compute ωopt from Eq.  2.36 . Perform all subsequent iterations with ω = ωopt.  2 See, for example, Terrence J. Akai, Applied Numerical Methods for Engineers, John Wiley & Sons   1994 , p. 100.   89  ∗2.7 Iterative Methods   cid:2  gaussSeidel  The function gaussSeidel is an implementation of the Gauss-Seidel method with relaxation. It automatically computes ωopt from Eq.  2.36  using k = 10 and p = 1. The user must provide the function iterEqs that computes the improved x from the iterative formulas in Eq.  2.35 —see Example 2.17. The function gaussSeidel returns the solution vector x, the number of iterations carried out, and the value of ωopt used.   module gaussSeidel  ’’’ x,numIter,omega = gaussSeidel iterEqs,x,tol = 1.0e-9   Gauss-Seidel method for solving [A]{x} = {b}.  The matrix [A] should be sparse. User must supply the  function iterEqs x,omega  that returns the improved {x},  given the current {x}  ’omega’ is the relaxation factor .  ’’’  import numpy as np  import math  def gaussSeidel iterEqs,x,tol = 1.0e-9 :  omega = 1.0  k = 10  p = 1  for i in range 1,501 :  xOld = x.copy    x = iterEqs x,omega   dx = math.sqrt np.dot x-xOld,x-xOld    if dx < tol: return x,i,omega   Compute relaxation factor after k+p iterations  if i == k: dx1 = dx  if i == k + p:  dx2 = dx  omega = 2.0  1.0 + math.sqrt 1.0  \  -  dx2 dx1 ** 1.0 p     print ’Gauss-Seidel failed to converge’   Conjugate Gradient Method  Consider the problem of ﬁnding the vector x that minimizes the scalar function  f  x  = 1 2  xTAx − bTx   2.37    90  Systems of Linear Algebraic Equations  where the matrix A is symmetric and positive deﬁnite. Because f  x  is minimized when its gradient ∇ f = Ax − b is zero, we see that minimization is equivalent to solv- ing  Gradient methods accomplish the minimization by iteration, starting with an  initial vector x0. Each iterative cycle k computes a reﬁned solution  The step length αk is chosen so that xk+1 minimizes f  xk+1  in the search direction sk. That is, xk+1 must satisfy Eq.  2.38 :  Introducing the residual  Eq.  a  becomes αAsk = rk. Pre-multiplying both sides by sT obtain  k and solving for αk, we  We are still left with the problem of determining the search direction sk. Intuition tells us to choose sk = −∇ f = rk, because this is the direction of the largest negative change in f  x . The resulting procedure is known as the method of steepest descent. It is not a popular algorithm because its convergence can be slow. The more efﬁcient conjugate gradient method uses the search direction  The constant β to each other, meaning  k is chosen so that the two successive search directions are conjugate  The great attraction of conjugate gradients is that minimization in one conju- gate direction does not undo previous minimizations  minimizations do not interfere with one another .  Substituting sk+1 from Eq.  2.42  into Eq.  b , we get  Ax = b  xk+1 = xk + αksk  A xk + αksk  = b  rk = b − Axk  αk = sT k rk sT k Ask  sk+1 = rk+1 + β  ksk  k+1Ask = 0 sT   cid:30    cid:31   + β  ksT k  rT k+1  Ask = 0  βk  = − rT k+1Ask sT k Ask   2.38    2.39    a    2.40    2.41    2.42    b    2.43   which yields   91  ∗2.7 Iterative Methods  Here is the outline of the conjugate gradient algorithm:  Choose x0  any vector will do . Let r0 ← b − Ax0. Let s0 ← r0  lacking a previous search direction, choose the direction of steepest descent . do with k = 0, 1, 2, . . .:  .  αk ← sT k rk sT k Ask xk+1 ← xk + αksk . rk+1 ← b − Axk+1. if rk+1 ≤ ε exit loop  ε is the error tolerance .  k  β  ← − rT k+1Ask sT k Ask sk+1 ← rk+1 + β  .  ksk .  It can be shown that the residual vectors r1, r2, r3, . . . produced by the algorithm are mutually orthogonal; that is, ri · rj = 0, i  cid:7 = j . Now suppose that we have carried out enough iterations to have computed the whole set of n residual vectors. The resid- ual resulting from the next iteration must be a null vector  rn+1 = 0 , indicating that the solution has been obtained. It thus appears that the conjugate gradient algorithm is not an iterative method at all, because it reaches the exact solution after n compu- tational cycles. In practice, however, convergence is usually achieved in less than n iterations.  The conjugate gradient method is not competitive with direct methods in the solution of small sets of equations. Its strength lies in the handling of large, sparse systems  where most elements of A are zero . It is important to note that A enters the algorithm only through its multiplication by a vector; that is, in the form Av, where v is a vector  either xk+1 or sk . If A is sparse, it is possible to write an efﬁcient subrou- tine for the multiplication and pass it, rather than A itself, to the conjugate gradient algorithm.   cid:2  conjGrad  The function conjGrad shown next implements the conjugate gradient algorithm. The maximum allowable number of iterations is set to n  the number of unknowns . Note that conjGrad calls the function Av that returns the product Av. This func- tion must be supplied by the user  see Example 2.18 . The user must also supply the starting vector x0 and the constant  right-hand-side  vector b. The function returns the solution vector x and the number of iterations.   module conjGrad  ’’’ x, numIter = conjGrad Av,x,b,tol=1.0e-9   Conjugate gradient method for solving [A]{x} = {b}.   92  Systems of Linear Algebraic Equations  The matrix [A] should be sparse. User must supply  the function Av v  that returns the vector [A]{v}.  ’’’  import numpy as np  import math  def conjGrad Av,x,b,tol=1.0e-9 :  n = len b   r = b - Av x   s = r.copy    for i in range n :  u = Av s   x = x + alpha*s  r = b - Av x   break  else:  alpha = np.dot s,r  np.dot s,u   if math.sqrt np.dot r,r    < tol:  beta = -np.dot r,u  np.dot s,u   s = r + beta*s  return x,i  EXAMPLE 2.15 Solve the equations  ⎡ ⎢⎣ 4 −1 −1 1 −2  1 4 −2 4  ⎤ ⎥⎦  ⎡ ⎢⎣ x1  x2 x3  ⎤ ⎥⎦ =  ⎡ ⎢⎣ 12−1  ⎤ ⎥⎦  5  by the Gauss-Seidel method without relaxation.  Solution. With the given data, the iteration formulas in Eq.  2.34  become  Choosing the starting values x1 = x2 = x3 = 0, the ﬁrst iteration gives us  x1 = 1 4 x2 = 1 4 x3 = 1 4   12 + x2 − x3   −1 + x1 + 2x3   5 − x1 + 2x2   x1 = 1 4 x2 = 1 4 x3 = 1 4   12 + 0 − 0  = 3 [−1 + 3 + 2 0 ] = 0.5 [5 − 3 + 2 0.5 ] = 0.75   93  ∗2.7 Iterative Methods  The second iteration yields x1 = 1 4 x2 = 1 4 x3 = 1 4  and the third iteration results in   12 + 0.5 − 0.75  = 2.9375 [−1 + 2.9375 + 2 0.75 ] = 0.859 38 [5 − 2.9375 + 2 0.859 38 ] = 0 .945 31  x1 = 1 4 x2 = 1 4 x3 = 1 4   12 + 0.85938 − 0 .94531  = 2.978 52 [−1 + 2.97852 + 2 0 .94531 ] = 0.967 29 [5 − 2.97852 + 2 0.96729 ] = 0.989 02  After ﬁve more iterations the results would agree with the exact solution x1 = 3,  x2 = x3 = 1 within ﬁve decimal places. EXAMPLE 2.16 Solve the equations in Example 2.15 by the conjugate gradient method.  Solution. The conjugate gradient method should converge after three iterations. Choosing again for the starting vector x0 =  0 0 0   cid:15 T   cid:14   ,  the computations outlined in the text proceed as follows:  First Iteration  r0 = b − Ax0 =  ⎤ ⎥⎦ −  ⎡ ⎢⎣ 12 −1 5  ⎤ ⎥⎦  ⎡ ⎢⎣ 0  ⎤ ⎥⎦ =  0 0  ⎤ ⎥⎦  ⎡ ⎢⎣ 12 −1 5  ⎡ ⎢⎣ 4 −1 1 4 −2 −1 1 −2 ⎡ ⎤ 4 ⎢⎣ 12−1 ⎥⎦ ⎤ ⎡ ⎤ ⎥⎦ ⎢⎣ 12−1 ⎥⎦ =  5  ⎡ ⎢⎣ 54−26  ⎤ ⎥⎦  s0 = r0 =  ⎡ ⎢⎣ 4 −1 −1 1 −2  1 4 −2 4  As0 =  α0 = sT 0 r0 sT 0 As0  =  x1 = x0 + α0s0 =  34  5 122 +  −1 2 + 52 = 0.201 42 12 54  +  −1  −26  + 5 34  ⎡ ⎡ ⎤ ⎡ ⎤ ⎢⎣ 0 ⎢⎣ 12−1 ⎥⎦ + 0.201 42 ⎢⎣ 2.41 704 ⎥⎦ = −0. 201 42 1.007 10  0 0  5  ⎤ ⎥⎦   94  Systems of Linear Algebraic Equations  Second Iteration  r1 = b − Ax1 =  ⎡ ⎢⎣ 12−1  ⎤ ⎥⎦ −  5  ⎡ ⎢⎣ 4 −1 −1 1 −2  1 4 −2 4  ⎤ ⎥⎦  ⎡ ⎢⎣ 2.417 04 −0. 201 42 1.007 10  ⎤ ⎥⎦ =  ⎡ ⎢⎣ 1.123 32 4.236 92 −1.848 28  ⎤ ⎥⎦  β0  = − rT 1 As0 sT 0 As0  s1 = r1 + β  = 0.133 107  = − 1.123 32 54  + 4.236 92 −26  − 1.848 28 34  ⎡ ⎢⎣ 2.720 76 4.103 80 −1.182 68 ⎤ ⎥⎦  12 54  +  −1  −26  + 5 34  ⎡ ⎡ ⎤ ⎤ ⎢⎣ 1.123 32 ⎢⎣ 12 ⎥⎦ + 0.133 107 ⎥⎦ = −1 4.236 92 −1.848 28 5 ⎤ ⎡ ⎤ ⎡ ⎥⎦ ⎢⎣ 2.720 76 ⎥⎦ = ⎢⎣ 4.103 80 −1.182 68  0s0 = ⎡ ⎢⎣ 4 −1 −1 1 −2  5.596 56 16.059 80 −10.217 60  1 4 −2 4  ⎤ ⎥⎦  As1 =  α1 = sT 1 r1 sT 1 As1  = 2.720 76 1.123 32  + 4.103 80 4.236 92  +  −1.182 68  −1.848 28  2.720 76 5.596 56  + 4.103 80 16.059 80  +  −1.182 68  −10.217 60   = 0.24276  x2 = x1 + α1s1 =  ⎡ ⎢⎣ 2.417 04 −0. 201 42 1.007 10  ⎤ ⎥⎦ + 0.24276  ⎡ ⎢⎣ 2. 720 76 4. 103 80 −1. 182 68  ⎤ ⎥⎦ =  ⎡ ⎢⎣ 3.077 53  0.794 82 0.719 99  ⎤ ⎥⎦  Third Iteration  r2 = b − Ax2 =  ⎡ ⎢⎣ 12−1  ⎤ ⎥⎦ −  5  ⎡ ⎢⎣ 4 −1 −1 1 −2  1 4 −2 4  ⎤ ⎥⎦  ⎡ ⎢⎣ 3.077 53  0.794 82 0.719 99  ⎤ ⎥⎦ =  ⎡ ⎢⎣−0.235 29  0.338 23 0.632 15  ⎤ ⎥⎦  β  1  = − rT 2 As1 sT 1 As1  = 0.0251 452  = −  −0.235 29  5.596 56  + 0.338 23 16.059 80  + 0.632 15 −10.217 60  2.720 76 5.596 56  + 4.103 80 16.059 80  +  −1.182 68  −10.217 60    95  s2 = r2 + β  ∗2.7 Iterative Methods ⎡ ⎢⎣−0.235 29 1s1 = ⎡ ⎢⎣ 4 −1 −1 1 −2  0.338 23 0.632 15  As2 =  1 4 −2 4  ⎤ ⎥⎦ =  ⎤ ⎥⎦ + 0.025 1452 ⎤ ⎥⎦  ⎡ ⎢⎣−0.166 876  ⎡ ⎢⎣ 2.720 76 4.103 80 −1.182 68 ⎡ ⎤ ⎥⎦ = ⎢⎣−0.506 514  ⎡ ⎢⎣−0.166 876 ⎤ ⎥⎦  0.441 421 0.602 411  ⎤ ⎥⎦  0.441 421 0.602 411  0.727 738 1.359 930  α2 = rT 2 s2 sT 2 As2 =  −0.235 29  −0.166 876  + 0.338 23 0.441 421  + 0.632 15 0.602 411   −0.166 876  −0.506 514  + 0.441 421 0.727 738  + 0.602 411 1.359 930  = 0.464 80 ⎤ ⎥⎦  ⎡ ⎢⎣−0.166 876  ⎤ ⎥⎦ + 0.464 80  ⎡ ⎢⎣ 3.077 53  ⎡ ⎢⎣ 2.999 97  x3 = x2 + α2s2 =  ⎤ ⎥⎦ =  0.441 421 0.602 411  0.999 99 0.999 99  0.794 82 0.719 99  The solution x3 is correct to almost ﬁve decimal places. The small discrepancy may be caused by roundoff errors in the computations.  EXAMPLE 2.17 Write a computer program to solve the following n simultaneous equations by the Gauss-Seidel method with relaxation  the program should work with any value of n 3:  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  2 −1 −1 0 −1 ... ... 0 0 0 0 1 0  0 0 . . . 2 −1 0 . . . 2 −1 . . . ... 0 0 0  1 0 0 0 0 0 0 0 0 0 0 0 ... ... ... ... ... 2 −1 0 . . . −1 0 2 −1 0 −1 0 . . . 0 −1 2 0 0 . . .  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  x1 x2 x3 ... xn−2 xn−1 xn  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  0 0 0 ... 0 0 1  =  Run the program with n = 20. The exact solution can be shown to be xi = −n 4 + i 2, i = 1, 2, . . . , n.  Solution. In this case the iterative formulas in Eq.  2.35  are  x1 = ω x2 − xn  2 +  1 − ω x1 xi = ω xi−1 + xi+1  2 +  1 − ω xi, xn = ω 1 − x1 + xn−1  2 +  1 − ω xn  i = 2, 3, . . . , n − 1   a   These formulas are evaluated in the function iterEqs.  3 Equations of this form are called cyclic tridiagonal. They occur in the ﬁnite difference formulation  of second-order differential equations with periodic boundary conditions.   96  Systems of Linear Algebraic Equations  ! usr bin python   example2_17  import numpy as np  from gaussSeidel import *  def iterEqs x,omega :  n = len x   x[0] = omega* x[1] - x[n-1]  2.0 +  1.0 - omega *x[0]  for i in range 1,n-1 :  x[i] = omega* x[i-1] + x[i+1]  2.0 +  1.0 - omega *x[i]  x[n-1] = omega* 1.0 - x[0] + x[n-2]  2.0 \  +  1.0 - omega *x[n-1]  return x  n = eval input "Number of equations ==> "    x = np.zeros n   x,numIter,omega = gaussSeidel iterEqs,x   print "\nNumber of iterations =",numIter   print "\nRelaxation factor =",omega   print "\nThe solution is:\n",x   input "\nPress return to exit"   The output from the program is  Number of equations ==> 20  Number of iterations = 259  Relaxation factor = 1.7054523107131399  The solution is:  [ -4.50000000e+00 -4.00000000e+00 -3.50000000e+00 -3.00000000e+00  -2.50000000e+00 -2.00000000e+00 -1.50000000e+00 -9.99999997e-01  -4.99999998e-01  2.14047151e-09  5.00000002e-01  1.00000000e+00  1.50000000e+00  2.00000000e+00  2.50000000e+00  3.00000000e+00  3.50000000e+00  4.00000000e+00  4.50000000e+00  5.00000000e+00]  The convergence is very slow, because the coefﬁcient matrix lacks diagonal dominance—substituting the elements of A into Eq.  2.30  produces an equality rather than the desired inequality. If we were to change each diagonal term of the coefﬁcient from 2 to 4, A would be diagonally dominant and the solution would con- verge in only 17 iterations.   97  ∗2.7 Iterative Methods  EXAMPLE 2.18 Solve Example 2.17 with the conjugate gradient method, also using n = 20. Solution. The program shown next uses the function conjGrad. The solution vector x is initialized to zero in the program. The function Ax v  returns the product A · v, where A is the coefﬁcient matrix and v is a vector. For the given A, the components of the vector Ax v  are   Ax 1 = 2v1 − v2 + vn  Ax i = −vi−1 + 2vi − vi+1,  Ax n = −vn−1 + 2vn + v1  i = 2, 3, . . . , n − 1  ! usr bin python   example2_18  import numpy as np  from conjGrad import *  def Ax v :  n = len v   Ax = np.zeros n   Ax[0] = 2.0*v[0] - v[1]+v[n-1]  Ax[1:n-1] = -v[0:n-2] + 2.0*v[1:n-1] -v[2:n]  Ax[n-1] = -v[n-2] + 2.0*v[n-1] + v[0]  return Ax  n = eval input "Number of equations ==> "    b = np.zeros n   b[n-1] = 1.0  x = np.zeros n   x,numIter = conjGrad Ax,x,b   print "\nThe solution is:\n",x   print "\nNumber of iterations =",numIter   input "\nPress return to exit"   Running the program results in  Number of equations ==> 20  The solution is:  [-4.5 -4. -3.5 -3. -2.5 -2. -1.5 -1. -0.5  0.  0.5  1.  1.5  2.  2.5  3.  3.5  4.  4.5  5. ]  Number of iterations = 9  Note that convergence was reached in only 9 iterations, whereas 259 iterations  were required in the Gauss-Seidel method.   98  Systems of Linear Algebraic Equations  PROBLEM SET 2.3  1. Let  ⎡ ⎢⎣ 3 −1 0 −2  A =  ⎤ ⎥⎦   note that B is obtained by interchanging the ﬁrst two rows of A . Knowing that  determine B  −1.  2. Invert the triangular matrices:  3. Invert the triangular matrix:  ⎤ ⎥⎦  ⎡ ⎢⎣ 0 3 1 3 −1 2 2 −4 −2 ⎤ ⎥⎦  B =  2 1 3 2 −4 ⎡ ⎢⎣ 0.5 0 0.25 0.3 0.4 0.45 −0.1 0.2 −0.15 ⎤ ⎥⎦  B =  ⎤ ⎥⎦  3 4 0 4 5 6  ⎡ ⎢⎣ 2 0 0 ⎤ ⎥⎥⎥⎦  1 1 2 1 4 1 8 1 1 3 1 9 0 1 1 4 0 0 0 0 0 1  A =  −1 =  A  0 6 5 0 0 2  ⎡ ⎢⎣ 2 4 3 ⎡ ⎢⎢⎢⎣  A =  ⎡ ⎢⎣ 1 2  4 9 1 3 1 4 16  ⎤ ⎥⎦ ⎡ ⎢⎣ 4 −2 −2 1 −2 ⎤ ⎥⎥⎥⎦  4. Invert the following matrices:   a  A =   b  B =  5. Invert this matrix:  A =  1 1 −1 4 ⎡ 6.  cid:2  Invert the following matrices with any method: ⎢⎢⎢⎣  5 −3 −1 0 −2 1 1 1 3 −5 1 2 8 −4 −3 0 ⎡ 7.  cid:2  Invert the matrix by any method  ⎡ ⎢⎢⎢⎣  B =  A =  ⎤ ⎥⎦  0 4 −1 4  ⎡ ⎢⎣ 4 −1 −1 0 −1 ⎤ ⎥⎦  ⎤ ⎥⎥⎥⎦  0 0 4 −1 0 4 −1 0 −1 4  4 −1 −1 0 −1 0 ⎤  A =  ⎢⎢⎢⎢⎢⎣  1 2 −1 3 8 −1 11  3 −9 6 4 6 7 1 2 −3 15 5 4 2 1 −2 18 7  1  ⎥⎥⎥⎥⎥⎦  and comment on the reliability of the result.   99  ∗2.7 Iterative Methods  8.  cid:2  The joint displacements u of the plane truss in Prob. 14, Problem Set 2.2 are  related to the applied joint forces p by  Ku = p   a   where  ⎡  ⎢⎢⎢⎢⎢⎣  K =  27.580 7.004 −7.004 −5.253 0.000 0.000 0.000 −24.320  7.004 −7.004 0.000 0.000 29.570 −5.253 0.000 −24.320 29.570 0.000 0.000 27.580 −7.004 0.000 0.000 −7.004 29.570  ⎤  ⎥⎥⎥⎥⎥⎦ MN m  −1p, where K  −1, we obtain u = K  is called the stiffness matrix of the truss. If Eq.  a  is inverted by multiplying each −1 is known as the ﬂexibility matrix. side by K = displace- The physical meaning of the elements of the ﬂexibility matrix is K ments ui  i = 1, 2, . . . 5  produced by the unit load pj = 1. Compute  a  the ﬂex- ibility matrix of the truss;  b  the displacements of the joints due to the load p5 = −45 kN  the load shown in Problem 14, Problem Set 2.2 .  −1 ij  ⎡ 9.  cid:2  Invert the matrices: ⎢⎢⎢⎣  A =  ⎤ ⎥⎥⎥⎦  3 −7 21 45 11 12 17 10 25 −80 −24 6 55 −9 7 17  ⎤ ⎥⎥⎥⎦  B =  1 1 1 1 1 2 2 2 2 3 4 4 4 5 6 7  10.  cid:2  Write a program for inverting an n × n lower triangular matrix. The inversion procedure should contain only forward substitution. Test the program by invert- ing this matrix:  ⎡ ⎢⎢⎢⎣  ⎤ ⎥⎥⎥⎦  11. Use the Gauss-Seidel method to solve  ⎡ ⎢⎢⎢⎣  A =  0 36 0 18 36 0 9 12 36 5  0 0 0 9 36  4  ⎡ ⎢⎣−2 5 9 7 1 1 −3 7 −1  ⎤ ⎥⎦  ⎡ ⎢⎣ x1  x2 x3  ⎤ ⎥⎦ =  ⎡ ⎢⎣  ⎤ ⎥⎦  1  6−26 ⎡ ⎢⎢⎢⎣  ⎤ ⎥⎥⎥⎦ =  ⎤ ⎥⎥⎥⎦  0 0 20 0  ⎤ ⎥⎥⎥⎦  ⎡ ⎢⎢⎢⎣  x1 x2 x3 x4  ⎡ ⎢⎢⎢⎣  12 −2 3 1 −2 6 −3 15 6 20 −4 1 0 −3 9  2  12. Solve the following equations with the Gauss-Seidel method:   100  Systems of Linear Algebraic Equations 13. Use the Gauss-Seidel method with relaxation to solve Ax = b, where  ⎡ ⎢⎢⎢⎣  A =  4 −1 −1 0 −1 0  0 0 4 −1 0 4 −1 0 −1 3  ⎤ ⎥⎥⎥⎦  ⎤ ⎥⎥⎥⎦  ⎡ ⎢⎢⎢⎣  15 10 10 10  b =  Take xi = bi  Aii as the starting vector, and use ω = 1.1 for the relaxation factor.  ⎤ ⎥⎦  14. Solve the equations  ⎤ ⎥⎦  ⎡ ⎢⎣ x1  ⎡ ⎢⎣ 2 −1 −1 0 −1  ⎡ ⎤ ⎢⎣ 1 ⎥⎦ = by the conjugate gradient method. Start with x = 0. ⎤ ⎡ ⎥⎦ = ⎢⎣  0 2 −1 1  ⎡ ⎢⎣ x1  ⎤ ⎥⎦  x2 x3  1 1  15. Use the conjugate gradient method to solve  ⎡ ⎢⎣ 3 0 −1 −2  0 −1 4 −2 5  x2 x3  4  10−10  ⎤ ⎥⎦  starting with x = 0.  16.  cid:2  Write a program for solving Ax = b by the Gauss-Seidel method based on the function gaussSeidel. Input should consist of the matrix A and the vector b. Test the program with  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎣  A =  3 −2 −2 1 −2 0 0 1  1 1 0 0 4 −2 0 1 0 4 −2 0 1 4 −2 1 −2 1 4 −2 1 −2 0 1 −2 3 0 0  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎦  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎦  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎣  10−8  10 10 −8 10  b =  Note that A is not diagonally dominant, but this does not necessarily preclude convergence.  17.  cid:2  Modify the program in Example 2.17  Gauss-Seidel method  so that it will solve  the following equations:  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  4 −1 −1 0 −1 ... ... 0 0 0 0 0 1  0 ··· 0 4 −1 0 ··· 4 −1 ··· ··· ... 0 0 0  0 0 0 1 0 0 0 0 0 0 0 0 ... ... ... ... ... 4 −1 0 ··· −1 0 4 −1 0 −1 0 ··· 0 −1 0 ··· 4 0  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  x1 x2 x3 ... xn−2 xn−1 xn  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  0 0 0 ... 0 0 100  =  Run the program with n = 20 and compare the number of iterations with Example 2.17.   101  ∗2.7 Iterative Methods  18.  cid:2  Modify the program in Example 2.18 to solve the equations in Prob. 17 by the  conjugate gradient method. Run the program with n = 20.  19.  cid:2   o T = 0  o T  = 100  o T = 0  1  4  7  2  5  8  3  6  9  T  = 200o  The edges of the square plate are kept at the temperatures shown. Assuming steady-state heat conduction, the differential equation governing the tempera- ture T in the interior is  ∂2T ∂x2  + ∂ 2T ∂y 2  = 0.  If this equation is approximated by ﬁnite differences using the mesh shown, we obtain the following algebraic equations for temperatures at the mesh points:  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  −4 0 1 1 0 0 0 0 0 1 −4 0 0 1 1 0 0 0 1 −4 0 0 0 0 1 0 0 0 −4 0 1 0 1 0 1 0 1 −4 1 1 0 1 0 0 0 1 −4 0 0 0 0 0 1 1 0 −4 1 0 1 0 0 0 0 1 −4 0 1 0 0 0 0 1 1 −4 0 0 0 0 0 0 1  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  T1 T2 T3 T4 T5 T6 T7 T8 T9  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  0 0 100 0 0 100 200 200 300  =  Solve these equations with the conjugate gradient method.  20.  cid:2   2 kN m  3 kN m  3 kN m 3 kN m  3 kN m  2 kN m  80 N 1  2  3  60 N 4  5  The equilibrium equations of the blocks in the spring-block system are  3 x2 − x1  − 2x1 = −80  3 x3 − x2  − 3 x2 − x1  = 0 3 x4 − x3  − 3 x3 − x2  = 0 3 x5 − x4  − 3 x4 − x3  = 60 −2x5 − 3 x5 − x4  = 0   102  Systems of Linear Algebraic Equations  where xi are the horizontal displacements of the blocks measured in mm.  a  Write a program that solves these equations by the Gauss-Seidel method without relaxation. Start with x = 0 and iterate until four-ﬁgure accuracy af- ter the decimal point is achieved. Also print the number of iterations required.  b  Solve the equations using the function gaussSeidel using the same con- vergence criterion as in part  a . Compare the number of iterations in parts  a  and  b . 21.  cid:2  Solve the equations in Prob. 20 with the conjugate gradient method using the function conjGrad. Start with x = 0 and iterate until four-ﬁgure accuracy after the decimal point is achieved.  2.8 Other Methods  A matrix can be decomposed in numerous ways, some of which are generally useful, whereas others ﬁnd use in special applications. The most important of the latter are the QR factorization and the singular value decomposition.  The QR decomposition of a matrix A is  A = QR  −1 = QT   where Q is an orthogonal matrix  recall that the matrix Q is orthogonal if Q and R is an upper triangular matrix. Unlike LU factorization, QR decomposition does not require pivoting to sustain stability, but it does involve about twice as many op- erations. Because of its relative inefﬁciency, the QR factorization is not used as a general-purpose tool, but ﬁnds it niche in applications that put a premium on stabil- ity  e.g., solution of eigenvalue problems . The numpy module includes the function qr that does the factorization:  Q,R = numpy.linalg.qr A   The singular value decomposition  SVD  is a useful diagnostic tool for singular or  ill-conditioned matrices. Here the factorization is  where U and V are orthogonal matrices and  ⎡ ⎢⎢⎢⎢⎣   cid:2  =  A = U cid:2 VT  λ1 0 0 0 ...  0 λ2 0 0 λ3 ... ...  ⎤ ⎥⎥⎥⎥⎦  ··· ··· ··· ...  is a diagonal matrix. The λ’s are called the singular values of the matrix A. They can be shown to be positive or zero. If A is symmetric and positive deﬁnite, then the   103  2.8 Other Methods  λ’s are the eigenvalues of A. A nice characteristic of the singular value decomposi- tion is that it works even if A is singular or ill conditioned. The conditioning of A can be diagnosed from magnitudes of λ’s: The matrix is singular if one or more of the λ’s are zero, and it is ill conditioned if the condition number λmax λmin is very large. The singular value decomposition function that comes with the numpy module is svd:  U,lam,V = numpy.linalg.svd A    3  Interpolation and Curve Fitting  Given the n + 1 data points  xi, yi , i = 0, 1, . . . , n, estimate y x .  3.1  Introduction  Discrete data sets, or tables of the form  x0 y0  x1 y1  x2 y2  ··· ···  xn yn  are commonly involved in technical calculations. The source of the data may be ex- perimental observations or numerical computations. There is a distinction between interpolation and curve ﬁtting. In interpolation we construct a curve through the data points. In doing so, we make the implicit assumption that the data points are accurate and distinct. In contrast, curve ﬁtting is applied to data that contain scatter  noise , usually caused by measurement errors. Here we want to ﬁnd a smooth curve that approximates the data in some sense. Thus the curve does not necessarily hit the data points. The difference between interpolation and curve ﬁtting is illustrated in Figure 3.1.  y  Curve fitting  Interpolation  Data points  x  Figure 3.1. Interpolation vs. curve ﬁtting.  104   105  3.2 Polynomial Interpolation  3.2  Polynomial Interpolation  Lagrange’s Method  The simplest form of an interpolant is a polynomial. It is always possible to construct a unique polynomial of degree n that passes through n + 1 distinct data points. One means of obtaining this polynomial is the formula of Lagrange,  Pn x  = n cid:6   i=0  yi  cid:12 i x   where the subscript n denotes the degree of the polynomial and   cid:12 i x  = x − x0 xi − x0 = n" x − xi xi − xj  j=0 j cid:7 =i  · x − x1 xi − x1  · ··· · x − xi−1 xi − xi−1  · x − xi+1 xi − xi+1  ··· x − xn xi − xn  i = 0, 1, . . . , n  ,  are called the cardinal functions.  For example, if n = 1, the interpolant is the straight line P1 x  = y0 cid:12 0 x  +  y1 cid:12 1 x , where   cid:12 0 x  = x − x1 x0 − x1   cid:12 1 x  = x − x0 x1 − x0  With n = 2, interpolation is parabolic: P2 x  = y0 cid:12 0 x  + y1 cid:12 1 x  + y2 cid:12 2 x , where now   cid:12 0 x  =  x − x1  x − x2   x0 − x1  x0 − x2   cid:12 1 x  =  x − x0  x − x2   x1 − x0  x1 − x2   cid:12 2 x  =  x − x0  x − x1   x2 − x0  x2 − x1    $  0 if i  cid:7 = j 1 if i = j  = δij   cid:12 i xj   =  The cardinal functions are polynomials of degree n and have the property  where δij is the Kronecker delta. This property is illustrated in Figure 3.2 for three- point interpolation  n = 2  with x0 = 0, x1 = 2, and x2 = 3.  1  00  l1  l2  l0  1  2  x  3  Figure 3.2. Example of quadratic cardinal functions.   3.1a    3.1b    3.2    106  Interpolation and Curve Fitting  To prove that the interpolating polynomial passes through the data points, we  substitute x = xj into Eq.  3.1a  and then use Eq.  3.2 . The result is  Pn xj   = n cid:6   yi  cid:12 i xj   = n cid:6   i=0  i=0  yi δij = y j  It can be shown that the error in polynomial interpolation is  f  x  − Pn x  =  x − x0  x − x1 ···  x − xn   f  n+1  ξ    n + 1 !   3.3   where ξ lies somewhere in the interval  x0, xn ; its value is otherwise unknown. It is instructive to note that the farther a data point is from x, the more it contributes to the error at x.  Newton’s Method  Although Lagrange’s method is conceptually simple, it does not lend itself to an efﬁcient algorithm. A better computational procedure is obtained with Newton’s method, where the interpolating polynomial is written in the form Pn x  = a0 +  x − x0 a1 +  x − x0  x − x1 a2 + ··· +  x − x0  x − x1 ···  x − xn−1 an  This polynomial lends itself to an efﬁcient evaluation procedure. Consider, for  example, four data points  n = 3 . Here the interpolating polynomial is  P3 x  = a0 +  x − x0 a1 +  x − x0  x − x1 a2 +  x − x0  x − x1  x − x2 a3  = a0 +  x − x0 {a1 +  x − x1  [a2 +  x − x2 a3]}  which can be evaluated backward with the following recurrence relations:  P0 x  = a3 P1 x  = a2 +  x − x2 P0 x  P2 x  = a1 +  x − x1 P1 x  P3 x  = a0 +  x − x0 P2 x   For arbitrary n we have  P0 x  = an  Pk x  = an−k +  x − xn−k Pk−1 x , k = 1, 2, . . . , n   3.4   Denoting the x-coordinate array of the data points by xData and the degree of the polynomial by n, we have the following algorithm for computing Pn x :  p = a[n]  for k in range 1,n+1 :  p = a[n-k] +  x - xData[n-k] *p   107  3.2 Polynomial Interpolation  The coefﬁcients of Pn are determined by forcing the polynomial to pass i = 0, 1, . . . , n. This yields these simultaneous  through each data point: yi = Pn xi , equations:  y0 = a0 y1 = a0 +  x1 − x0 a1 y2 = a0 +  x2 − x0 a1 +  x2 − x0  x2 − x1 a2  ...  yn = a0 +  xn − x0 a1 + ··· +  xn − x0  xn − x1 ···  xn − xn−1 an  Introducing the divided differences   a    3.5   ,  ∇yi = yi − y0 xi − x0 ∇2yi = ∇yi − ∇y1 xi − x1 ∇3yi = ∇2yi − ∇2y2 xi − x2  ,  i = 1, 2, . . . , n  i = 2, 3, . . . , n  i = 3, 4, . . . n  ,  ...  ∇nyn = ∇n−1yn − ∇n−1yn−1  xn − xn−1  the solution of Eqs.  a  is a0 = y0  a1 = ∇y1  a2 = ∇2y2  ··· an = ∇nyn   3.6   If the coefﬁcients are computed by hand, it is convenient to work with the format  in Table 3.1  shown for n = 4 .  x0 x1 x2 x3 x4  y0 y1 ∇y1 y2 ∇y2 ∇2y2 y3 ∇y3 ∇2y3 ∇3y3 y4 ∇y4 ∇2y4 ∇3y4 ∇4y4 Table 3.1. Tableau for Newton’s method.  The diagonal terms  y0, ∇y1, ∇2y2, ∇3y3, and ∇4y4  in the table are the coefﬁ- cients of the polynomial. If the data points are listed in a different order, the entries in the table will change, but the resultant polynomial will be the same—recall that a polynomial of degree n interpolating n + 1 distinct data points is unique.   108  Interpolation and Curve Fitting  Machine computations can be carried out within a one-dimensional array a em- ploying the following algorithm  we use the notation m = n + 1 = number of data points :  a = yData.copy    for k in range 1,m :  for i in range k,m :  a[i] =  a[i] - a[k-1]   xData[i] - xData[k-1]   Initially, a contains the y-coordinates of the data, so that it is identical to the second column in Table 3.1. Each pass through the outer loop generates the en- tries in the next column, which overwrite the corresponding elements of a. There- fore, a ends up containing the diagonal terms of Table 3.1  i.e., the coefﬁcients of the polynomial .   cid:2  newtonPoly  This module contains the two functions required for interpolation by Newton’s method. Given the data point arrays xData and yData, the function coeffts re- turns the coefﬁcient array a. After the coefﬁcients are found, the interpolant Pn x  can be evaluated at any value of x with the function evalPoly.   module newtonPoly  ’’’ p = evalPoly a,xData,x .  Evaluates Newton’s polynomial p at x. The coefficient  vector {a} can be computed by the function ’coeffts’.  a = coeffts xData,yData .  Computes the coefficients of Newton’s polynomial.  ’’’  def evalPoly a,xData,x :  n = len xData  - 1   Degree of polynomial  p = a[n]  for k in range 1,n+1 :  p = a[n-k] +  x -xData[n-k] *p  return p  def coeffts xData,yData :  m = len xData    Number of data points  a = yData.copy    for k in range 1,m :  return a  a[k:m] =  a[k:m] - a[k-1]   xData[k:m] - xData[k-1]    109  3.2 Polynomial Interpolation  Neville’s Method  Newton’s method of interpolation involved two steps: computation of the coefﬁ- cients, followed by evaluation of the polynomial. It works well if the interpolation is carried out repeatedly at different values of x using the same polynomial. If only one point is to interpolated, a method that computes the interpolant in a single step, such as Neville’s algorithm, is a more convenient choice. Let Pk[xi, xi+1, . . . , xi+k] denote the polynomial of degree k that passes through the k + 1 data points  xi, yi ,  xi+1, yi+1 , . . . ,  xi+k, yi+k . For a single data point, we have  P0[xi] = yi   3.7   The interpolant based on two data points is  P1[xi, xi+1] =  x − xi+1 P0[xi] +  xi − x P0[xi+1]  xi − xi+1  It is easily veriﬁed that P1[xi, xi+1] passes through the two data points; that is, P1[xi, xi+1] = yi when x = xi, and P1[xi, xi+1] = yi+1 when x = xi+1.  The three-point interpolant is  P2[xi, xi+1, xi+2] =  x − xi+2 P1[xi, xi+1] +  xi − x P1[xi+1, xi+2]  xi − xi+2  To show that this interpolant does intersect the data points, we ﬁrst substitute x = xi, obtaining  P2[xi, xi+1, xi+2] = P1[xi, xi+1] = yi  P2[xi, xi+1, xi+2] = P2[xi+1, xi+2] = yi+2  Similarly, x = xi+2 yields  Finally, when x = xi+1 we have  P1[xi, xi+1] = P1[xi+1, xi+2] = yi+1  P2[xi, xi+1, xi+2] =  xi+1 − xi+2 yi+1 +  xi − xi+1 yi+1  = yi+1  xi − xi+2  Having established the pattern, we can now deduce the general recursive for-  so that  mula:  Pk[xi, xi+1, . . . , xi+k] =  x − xi+k Pk−1[xi,xi+1, . . . , xi+k−1] +  xi − x Pk−1[xi+1, xi+2, . . . , xi+k]   3.8   xi − xi+k  Given the value of x, the computations can be carried out in the following tabular  format  shown for four data points :   110  Interpolation and Curve Fitting  k = 0 P0[x0] = y0 P0[x1] = y1 P0[x2] = y2 P0[x3] = y3  x0 x1 x2 x3  k = 1 P1[x0, x1] P1[x1, x2] P1[x2, x3]  k = 2  P2[x0, x1, x2] P2[x1,x2, x3]  Table 3.2. Tableau for Neville’s method.  k = 3  P3[x0, x1, x2, x3]  Denoting the number of data points by m, the algorithm that computes the ele-  ments of the table is  y = yData.copy    for k in range  1,m :  for i in range m-k :  y[i] =   x - xData[i+k] *y[i] +  xData[i] - x *y[i+1]   \   xData[i]-xData[i+k]   This algorithm works with the one-dimensional array y, which initially contains the y-values of the data  the second column in Table 3.2 . Each pass through the outer loop computes the elements of y in the next column, which overwrite the previous entries. At the end of the procedure, y contains the diagonal terms of the table. The value of the interpolant  evaluated at x  that passes through all the data points is the ﬁrst element of y.   cid:2  neville  The following function implements Neville’s method; it returns Pn x :   module neville  ’’’ p = neville xData,yData,x .  Evaluates the polynomial interpolant p x  that passes  through the specified data points by Neville’s method.  ’’’  def neville xData,yData,x :  m = len xData    number of data points  y = yData.copy    for k in range 1,m :  y[0:m-k] =   x - xData[k:m] *y[0:m-k] +   xData[0:m-k] - x *y[1:m-k+1]     xData[0:m-k] - xData[k:m]   \  \  return y[0]   111  3.2 Polynomial Interpolation  Limitations of Polynomial Interpolation  Polynomial interpolation should be carried out with the fewest feasible number of data points. Linear interpolation, using the nearest two points, is often sufﬁcient if the data points are closely spaced. Three to six nearest-neighbor points produce good results in most cases. An interpolant intersecting more than six points must be viewed with suspicion. The reason is that the data points that are far from the point of interest do not contribute to the accuracy of the interpolant. In fact, they can be detrimental.  y  1.00  0.80  0.60  0.40  0.20  0.00  -0.20  -6.0  -4.0  -2.0  2.0  4.0  6.0  0.0 x  Figure 3.3. Polynomial interpolant displaying oscillations.  The danger of using too many points is illustrated in Figure 3.3. The 11 equally spaced data points are represented by the circles. The solid line is the interpolant, a polynomial of degree 10, that intersects all the points. As seen in the ﬁgure, a polyno- mial of such a high degree has a tendency to oscillate excessively between the data points. A much smoother result would be obtained by using a cubic interpolant span- ning four nearest-neighbor points.  Polynomial extrapolation  interpolating outside the range of data points  is dan- gerous. As an example, consider Figure 3.4. Six data points are shown as circles. The ﬁfth-degree interpolating polynomial is represented by the solid line. The interpolant looks ﬁne within the range of data points, but drastically departs from the obvious trend when x > 12. Extrapolating y at x = 14, for example, would be absurd in this case.  If extrapolation cannot be avoided, the following measures can be useful:   Plot the data and visually verify that the extrapolated value makes sense.   Use a low-order polynomial based on nearest-neighbor data points. A linear or quadratic interpolant, for example, would yield a reasonable estimate of y 14  for the data in Figure 3.4.   -100  2.0  4.0  6.0  8.0  10.0  12.0  14.0  16.0  x  Figure 3.4. Extrapolation may not follow the trend of data.    Work with a plot of log x vs. log y, which is usually much smoother than the x-y curve and thus safer to extrapolate. Frequently this plot is almost a straight line. This is illustrated in Figure 3.5, which represents the logarithmic plot of the data in Figure 3.4.  112  Interpolation and Curve Fitting  y  200  400  300  100  0  100  y  10  1  10  x  Figure 3.5. Logarithmic plot of the data in Figure 3.4.  EXAMPLE 3.1 Given the data points  2 11 use Lagrange’s method to determine y at x = 1.  0 7  x y  3 28   113  3.2 Polynomial Interpolation  Solution  EXAMPLE 3.2 The data points  Solution  lie on a polynomial. Determine the degree of this polynomial by constructing a di- vided difference table, similar to Table 3.1.   cid:12 0 =  x − x1  x − x2   x0 − x1  x0 − x2   cid:12 1 =  x − x0  x − x2   x1 − x0  x1 − x2   cid:12 2 =  x − x0  x − x1   x2 − x0  x2 − x1   =  1 − 2  1 − 3   0 − 2  0 − 3  =  1 − 0  1 − 3   2 − 0  2 − 3  =  1 − 0  1 − 2   3 − 0  3 − 2   = 1 3 = 1  = − 1 3  y = y0 cid:12 0 + y1 cid:12 1 + y2 cid:12 2 = 7 3  + 11 − 28 3  = 4  x −2 −1 y  4 −1 4 59  −4 3 24 −53  1 2  ∇yi ∇2yi ∇3yi ∇4yi ∇5yi  i xi yi 0 −2 −1 1 1 2 2 4 59 3 −1 4 3 4 24 5 −4 −53  1 10 5 5 26  3 −2 2 −5  1 1 1  0 0  0  ∇y2 = y2 − y0 x2 − x0 ∇2y2 = ∇y2 − ∇y1 x2 − x1 ∇3y5 = ∇2y5 − ∇2y2 x5 − x2  = 59 −  −1  = 10 4 −  −2  = 10 − 1 = 3 4 − 1 = −5 − 3 −4 − 4  = 1  Here are a few sample calculations used in arriving at the ﬁgures in the table:  From the table we see that the last nonzero coefﬁcient  last nonzero diagonal term  of Newton’s polynomial is ∇3y3, which is the coefﬁcient of the cubic term. Hence the polynomial is a cubic.  EXAMPLE 3.3 Given the data points  x y  4.0  −0.06604 −0.02724  3.9  3.8  3.7  0.01282  0.05383  determine the root of y x  = 0 by Neville’s method.   114  Interpolation and Curve Fitting  Solution. This is an example of inverse interpolation, in which the roles of x and y are interchanged. Instead of computing y at a given x, we are ﬁnding x that corresponds to a given y  in this case, y = 0 . Employing the format of Table 3.2  with x and y interchanged, of course , we obtain  yi  i 0 −0.06604 1 −0.02724 0.01282 2 3 0.05383  P0[ ] = xi  4.0 3.9 3.8 3.7  P1[ , ] 3.8298 3.8320 3.8313  P3[ , , , ] 3.8317  P2[ , , ] 3.8316 3.8318  The following are sample computations used in the table:  P1[y0, y1] =  y − y1 P0[y0] +  y0 − y P0[y1]  =  0 + 0.02724  4.0  +  −0.06604 − 0  3.9   y0 − y1 −0.06604 + 0.02724  P2[y1, y2, y3] =  y − y3 P1[y1, y2] +  y1 − y P1[y2, y3]  = 3.8298  =  0 − 0.05383  3.8320  +  −0.02724 − 0  3.8313   = 3.8318  y1 − y3 −0.02724 − 0.05383  All the P’s in the table are estimates of the root resulting from different orders of interpolation involving different data points. For example, P1[y0, y1] is the root ob- tained from linear interpolation based on the ﬁrst two points, and P2[y1, y2, y3] is the result from quadratic interpolation using the last three points. The root obtained from cubic interpolation over all four data points is x = P3[y0, y1, y2, y3] = 3.8317. EXAMPLE 3.4 The data points in the table lie on the plot of f  x  = 4.8 cos . Interpolate this data by Newton’s method at x = 0, 0.5, 1.0, . . . , 8.0, and compare the results with the “exact” values yi = f  xi .  πx 20  x y  0.15  2.30  4.79867  4.49013  3.15 4.2243  4.85  6.25  7.95  3.47313  2.66674  1.51909  Solution  ! usr bin python   example3_4  import numpy as np  import math  from newtonPoly import *  xData = np.array [0.15,2.3,3.15,4.85,6.25,7.95]   yData = np.array [4.79867,4.49013,4.2243,3.47313,2.66674,1.51909]   a = coeffts xData,yData   print " x  yInterp  yExact"    115  3.2 Polynomial Interpolation  print "-----------------------"   for x in np.arange 0.0,8.1,0.5 :  y = evalPoly a,xData,x   yExact = 4.8*math.cos math.pi*x 20.0   print ’{:3.1f} {:9.5f} {:9.5f}’.format x,y,yExact    input "\nPress return to exit"   The results are  x  yInterp  yExact  -----------------------  0.0  0.5  1.0  1.5  2.0  2.5  3.0  3.5  4.0  4.5  5.0  5.5  6.0  6.5  7.0  7.5  8.0  4.80003  4.80000  4.78518  4.78520  4.74088  4.74090  4.66736  4.66738  4.56507  4.56507  4.43462  4.43462  4.27683  4.27683  4.09267  4.09267  3.88327  3.88328  3.64994  3.64995  3.39411  3.39411  3.11735  3.11735  2.82137  2.82137  2.50799  2.50799  2.17915  2.17915  1.83687  1.83688  1.48329  1.48328  Rational Function Interpolation  Some data are better interpolated by rational functions rather than polynomials. A rational function R x  is the quotient of two polynomials:  R x  = Pm x  Qn x   = a1xm + a2xm−1 + ··· + amx + am+1 b1xn + b2xn−1 + ··· + bnx + bn+1  Because R x  is a ratio, it can be scaled so that one of the coefﬁcients  usually bn+1  is unity. That leaves m + n + 1 undetermined coefﬁcients that must be computed by forcing R x  through m + n + 1 data points. A popular version of R x  is the so-called diagonal rational function, where the degree of the numerator is equal to that of the denominator  m = n  if m + n is even, or less by one  m = n − 1  if m + n is odd. The advantage of using the diagonal form is that the interpolation can be carried out with a Neville-type algorithm, similar to that outlined in Table 3.2. The recursive formula that is the basis of the algorithm is due   116  Interpolation and Curve Fitting  where  %  to Stoer and Bulirsch.1 It is somewhat more complex than Eq.  3.8  used in Neville’s method:  R[xi, xi+1, . . . , xi+k] = R[xi+1, xi+2, . . . , xi+k]  + R[xi+1, xi+2, . . . , xi+k] − R[xi, xi+1, . . . , xi+k−1]  S   3.9a   S = x − xi x − xi+k  1 − R[xi+1, xi+2, . . . , xi+k] − R[xi, xi+1, . . . , xi+k−1] R[xi+1, xi+2, . . . , xi+k] − R[xi+1, xi+2, . . . , xi+k−1]  − 1   3.9b   In Eqs.  3.9  R[xi, xi+1, . . . , xi+k] denotes the diagonal rational function that passes through the data points  xi, yi ,  xi+1, yi+1 , . . . ,  xi+k, yi+k . It is also understood that R[xi, xi+1, . . . , xi−1] = 0  corresponding to the case k = −1  and R[xi] = yi  the case k = 0 .  The computations can be carried out in a tableau, similar to Table 3.2 used for Neville’s method. Table 3.3 is an example of the tableau for four data points. We start by ﬁlling the column k = −1 with zeros and entering the values of yi in the column k = 0. The remaining entries are computed by applying Eqs.  3.9 .  &  k = −1  x1 x2 x3 x4  0 0 0 0  k = 0 R[x1] = y1 R[x2] = y2 R[x3] = y3 R[x4] = y4  k = 1 R[x1, x2] R[x2, x3] R[x3, x4]  k = 2  R[x1, x2, x3] R[x2, x3, x4]  Table 3.3. A Tableau for Four Data Points.  k = 3  R[x1, x2, x3, x4]   cid:2  rational  We managed to implement Neville’s algorithm with the tableau “compressed” to a one-dimensional array. This will not work with the rational function interpolation, where the formula for computing an R in the kth column involves entries in columns k − 1 as well as k − 2. However, we can work with two one-dimensional arrays, one array  called r in the program  containing the latest values of R while the other array  rOld  saves the previous entries. Here is the algorithm for diagonal rational function interpolation:   module rational  ’’’ p = rational xData,yData,x   Evaluates the diagonal rational function interpolant p x   that passes through the data points  ’’’  import numpy as np  1 Stoer, J., and Bulirsch, R., Introduction to Numerical Analysis, Springer, 1980.   117  3.2 Polynomial Interpolation  def rational xData,yData,x :  m = len xData   r = yData.copy    rOld = np.zeros m   for k in range m-1 :  for i in range m-k-1 :  if abs x - xData[i+k+1]  < 1.0e-9:  return yData[i+k+1]  else:  c1 = r[i+1] - r[i]  c2 = r[i+1] - rOld[i+1]  c3 =  x - xData[i]   x - xData[i+k+1]   r[i] = r[i+1] + c1  c3* 1.0 - c1 c2  - 1.0   rOld[i+1] = r[i+1]  return r[0]  EXAMPLE 3.5 Given the data  x y  0 0  0.6  0.8  0.95  1.3764  3.0777  12.7062  determine y 0.5  by the diagonal rational function interpolation.  Solution. The plot of the data points indicates that y may have a pole at around x = 1. Such a function is a very poor candidate for polynomial interpolation, but can be readily represented by a rational function.  y  14.0  12.0  10.0  8.0  6.0  4.0  2.0  0.0  0.0  0.2  0.4  0.6  0.8  1.0  x   118  Interpolation and Curve Fitting  We set up our work in the format of Table 3.3. After completing the computations,  the table looks like this:  i = 1 i = 2 i = 3 i = 4  0 0.6 0.8 0.95  k = −1  0 0 0 0  k = 0 0  1.3764 3.0777 12.7062  k = 1 0  1.0784 1.2235  k = 3 1.0131  k = 2 0.9544 1.0327  Let us now look at a few sample computations. We obtain, for example, R[x3, x4]  %  & by substituting i = 3, k = 1 into Eqs.  3.9 . This yields &  S = x − x3 1 − % R[x4] − R[x4, . . . , x3] x − x4 = 0.5 − 0.8 1 − 12.7062 − 3.0777 12.7062 − 0 0.5 − 0.95 R[x3, x4] = R[x4] + R[x4] − R[x3]  R[x4] − R[x3]  − 1  − 1 = −0.83852  The entry R[x2, x3, x4] is obtained with i = 2, k = 2. The result is  = 1.2235  = 12.7062 + 12.7062 − 3.0777 %  S −0.83852 & 1 − R[x3, x4] − R[x2, x3] & % R[x3, x4] − R[x3] 1 − 1.2235 − 1.0784 1.2235 − 3.0777  S = x − x2 x − x4 = 0.5 − 0.6 0.5 − 0.95 R[x2, x3, x4] = R[x3, x4] + R[x3, x4] − R[x2, x3]  − 1  − 1 = −0.76039  = 1.2235 + 1.2235 − 1.0784  S −0.76039  = 1.0327  The interpolant at x = 0.5 based on all four data points is R[x1, x2, x3, x4] = 1.0131. EXAMPLE 3.6 Interpolate the data shown at x-increments of 0.05 and plot the results. Use both the polynomial interpolation and the rational function interpolation.  x y  0.1  0.2  0.5  0.6  0.8  1.2  1.5  −1.5342 −1.0811 −0.4445 −0.3085 −0.0868  0.2281  0.3824  Solution  ! usr bin python   example 3_6  import numpy as np  from rational import *  from neville import *  import matplotlib.pyplot as plt   119  3.2 Polynomial Interpolation  xData = np.array [0.1,0.2,0.5,0.6,0.8,1.2,1.5]   yData = np.array [-1.5342,-1.0811,-0.4445,-0.3085, \  x = np.arange 0.1,1.55,0.05   -0.0868,0.2281,0.3824]   n = len x   y = np.zeros  n,2    for i in range n :  y[i,0] = rational xData,yData,x[i]   y[i,1] = neville xData,yData,x[i]   plt.plot xData,yData,’o’,x,y[:,0],’-’,x,y[:,1],’--’   plt.legend  ’Data’,’Rational’,’Neville’ ,loc = 0   plt.xlabel ’x’   plt.show    input "\nPress return to exit"   The output is shown next. In this case, the rational function interpolant is  smoother, and thus superior, to the polynomial  Neville’s  interpolant.   120  Interpolation and Curve Fitting  3.3  Interpolation with Cubic Spline  y  y  If there are more than a few data points, a cubic spline is hard to beat as a global interpolant. It is considerably “stiffer” than a polynomial in the sense that it has less tendency to oscillate between data points.  Elastic strip  Pins  data points   x  Figure 3.6. Mechanical model of a natural cubic spline.  The mechanical model of a cubic spline is shown in Figure 3.6. It is a thin, elastic beam that is attached with pins to the data points. Because the beam is unloaded be- tween the pins, each segment of the spline curve is a cubic polynomial—recall from beam theory that d4y dx4 = q  E I  , so that y x  is a cubic since q = 0. At the pins, the slope and bending moment  and hence the second derivative  are continuous. There is no bending moment at the two end pins; consequently, the second deriva- tive of the spline is zero at the end points. Because these end conditions occur natu- rally in the beam model, the resulting curve is known as the natural cubic spline. The pins  i.e., the data points  are called the knots of the spline.  f  i, i + 1   x   yn - 1 yn n - 1x nx  x  y0  y1  yi - 1 yi  yi + 1  Figure 3.7. A cubic spline.  xi  1x  i - 1x  i + 1x  x0 Figure 3.7 shows a cubic spline that spans n + 1 knots. We use the notation fi,i+1 x  for the cubic polynomial that spans the segment between knots i and i + 1. Note that the spline is a piecewise cubic curve, put together from the n cubics f0,1 x , f1,2 x , . . . , fn−1,n x , all of which have different coefﬁcients.  Denoting the second derivative of the spline at knot i by ki, continuity of second  derivatives requires that  i−1,i xi  = f  cid:3  cid:3  f At this stage, each k is unknown, except for  i,i+1 xi  = ki  cid:3  cid:3   k0 = kn = 0   a   The starting point for computing the coefﬁcients of fi,i+1 x  is the expression for  cid:3  cid:3  i,i+1 x , which we know to be linear. Using Lagrange’s two-point interpolation, we f can write  i,i+1 x  = ki  cid:12 i x  + ki+1 cid:12 i+1 x   cid:3  cid:3  f   121  3.3 Interpolation with Cubic Spline  where  Therefore,   cid:12 i x  = x − xi+1 xi − xi+1   cid:12 1+1 x  = x − xi xi+1 − xi  i,i+1 x  = ki x − xi+1  − ki+1 x − xi    cid:3  cid:3   f  xi − xi+1  Integrating twice with respect to x, we obtain  fi,i+1 x  = ki x − xi+1 3 − ki+1 x − xi 3  6 xi − xi+1   + A x − xi+1  − B x − xi   where A and B are constants of integration. The terms arising from the integration would usually be written as Cx + D. By letting C = A − B and D = −A xi+1 + Bxi, we end up with the last two terms of Eq.  c , which are more convenient to use in the computations that follow.  Imposing the condition fi.i+1 xi  = yi, we get from Eq.  c   ki xi − xi+1 3 6 xi − xi+1   + A xi − xi+1  = yi  Therefore,  A = Similarly, fi,i+1 xi+1  = yi+1 yields  yi  xi − xi+1  − ki 6   xi − xi+1   Substituting Eqs.  d  and  e  into Eq.  c  results in  − ki+1 6   xi − xi+1   B = yi+1 xi − xi+1 ’    −  x − xi+1  xi − xi+1    −  x − xi  xi − xi+1    x − xi+1 3 ’ xi − xi+1  fi,i+1 x  = ki 6 − ki+1 6 + yi x − xi+1  − yi+1 x − xi    x − xi 3 xi − xi+1  xi − xi+1  The second derivatives ki of the spline at the interior knots are obtained from the i,i+1 xi , where i = 1, 2, . . . , n − 1. After a little  cid:3   i−1,i xi  = f  cid:3   slope continuity conditions f algebra, this results in the simultaneous equations  % ki−1 xi−1 − xi  + 2ki xi−1 − xi+1  + ki+1 xi − xi+1  i = 1, 2, ··· , n − 1  &  ,  − yi − yi+1 xi − xi+1  yi−1 − yi xi−1 − xi  = 6  Because Eqs.  3.11  have a tridiagonal coefﬁcient matrix, they can be solved econom- ically with the functions in module LUdecomp3 described in Section 2.4.   b    c    d    e    3.10    3.11    122  Interpolation and Curve Fitting  If the data points are evenly spaced at intervals h, then xi−1 − xi = xi − xi+1 = −h,  and the Eqs.  3.11  simplify to  ki−1 + 4ki + ki+1 = 6  h2  yi−1 − 2yi + yi+1 ,  i = 1, 2, . . . , n − 1   3.12    cid:2  cubicSpline  The ﬁrst stage of cubic spline interpolation is to set up Eqs.  3.11  and solve them for the unknown k’s  recall that k0 = kn = 0 . This task is carried out by the function curvatures. The second stage is the computation of the interpolant at x from Eq.  3.10 . This step can be repeated any number of times with different values of x using the function evalSpline. The function findSegment embedded in evalSpline ﬁnds the segment of the spline that contains x using the method of bisection. It re- turns the segment number; that is, the value of the subscript i in Eq.  3.10 .   module cubicSpline  ’’’ k = curvatures xData,yData .  Returns the curvatures of cubic spline at its knots.  y = evalSpline xData,yData,k,x .  Evaluates cubic spline at x. The curvatures k can be  computed with the function ’curvatures’.  ’’’  import numpy as np  from LUdecomp3 import *  def curvatures xData,yData :  n = len xData  - 1  c = np.zeros n   d = np.ones n+1   e = np.zeros n   k = np.zeros n+1   c[0:n-1] = xData[0:n-1] - xData[1:n]  d[1:n] = 2.0* xData[0:n-1] - xData[2:n+1]   e[1:n] = xData[1:n] - xData[2:n+1]  k[1:n] =6.0* yData[0:n-1] - yData[1:n]  \    xData[0:n-1] - xData[1:n]  \  -6.0* yData[1:n] - yData[2:n+1]   \    xData[1:n] - xData[2:n+1]   LUdecomp3 c,d,e   LUsolve3 c,d,e,k   return k  def evalSpline xData,yData,k,x :   123  3.3 Interpolation with Cubic Spline  def findSegment xData,x :  iLeft = 0  while 1:  iRight = len xData - 1  if  iRight-iLeft  <= 1: return iLeft  i = iLeft + iRight  2  if x < xData[i]: iRight = i  else: iLeft = i  i = findSegment xData,x   h = xData[i] - xData[i+1]  y =   x - xData[i+1] **3 h -  x - xData[i+1] *h *k[i] 6.0 \  -   x - xData[i] **3 h -  x - xData[i] *h *k[i+1] 6.0  \  \  +  yData[i]* x - xData[i+1]   - yData[i+1]* x - xData[i]   h  return y  EXAMPLE 3.7 Use a natural cubic spline to determine y at x = 1.5. The data points are  x y  1 0  2 1  3 0  4 1  5 0  Solution. The ﬁve knots are equally spaced at h = 1. Recalling that the second derivative of a natural spline is zero at the ﬁrst and last knot, we have k0 = k4 = 0. The second derivatives at the other knots are obtained from Eq.  3.12 . Using i = 1, 2, 3 re- sults in the simultaneous equations  0 + 4k1 + k2 = 6 [0 − 2 1  + 0] = −12 k1 + 4k2 + k3 = 6 [1 − 2 0  + 1] = 12 k2 + 4k3 + 0 = 6 [0 − 2 1  + 0] = −12  The solution is k1 = k3 = −30 7, k2 = 36 7. The point x = 1.5 lies in the segment between knots 0 and 1. The corresponding interpolant is obtained from Eq.  3.10  by setting i = 0. With xi − xi+1 = −h = −1, we obtain from Eq.  3.10   f0,1 x  = −k0 6   cid:28   x − x1 3 −  x − x1  − [y0 x − x1  − y1 x − x0 ]   cid:29  + k1  6   cid:28   cid:29   x − x0 3 −  x − x0    124  Interpolation and Curve Fitting  Therefore,  % y 1.5  = f0,1 1.5   & cid:28   − 30 7  = 0 + 1 6 = 0.7679   1.5 − 1 3 −  1.5 − 1    cid:29  − [0 − 1 1.5 − 1 ]  The plot of the interpolant, which in this case is made up of four cubic segments,  is shown in the ﬁgure.  y  0.60  1.00  0.80  0.40  0.20  0.00  1.00  1.50  2.00  2.50  3.50  4.00  4.50  5.00  3.00 x  EXAMPLE 3.8 Sometimes it is preferable to replace one or both of the end conditions of the cu- bic spline with something other than the natural conditions. Use the end condition 0,1 0  = 0  zero slope , rather than f 0,1 0  = 0  zero curvature , to determine the cu-  cid:3   cid:3  cid:3  f bic spline interpolant at x = 2.6, given the data points  x y  0 1  1 1  2 0.5  3 0  Solution. We must ﬁrst modify Eqs.  3.12  to account for the new end condition. Set- ting i = 0 in Eq.  3.10  and differentiating, we get ’  ’  − k1 6   x − x0 2 x0 − x1  3    −  x0 − x1   + y0 − y1 x0 − x1  3  0,1 x  = k0  cid:3  f 6     x − x1 2 −  x0 − x1  x0 − x1 0,1 x0  = 0 yields  cid:3  Thus the end condition f  x0 − x1  + k1 6  k0 3   x0 − x1  + y0 − y1 x0 − x1  = 0  or  2k0 + k1 = −6  y0 − y1  x0 − x1 2   125  3.3 Interpolation with Cubic Spline From the given data we see that y0 = y1 = 1, so that the last equation becomes  The other equations in Eq.  3.12  are unchanged. Knowing that k3 = 0, they are  2k0 + k1 = 0  k0 + 4k1 + k2 = 6 [1 − 2 1  + 0.5] = −3 k1 + 4k2 = 6 [1 − 2 0.5  + 0] = 0   a    b    c   The interpolant can now be evaluated from Eq.  3.10 . Substituting i = 2 and xi −  The solution of Eqs.  a – c  is k0 = 0.4615, k1 = −0.9231, k2 = 0.2308. xi+1 = −1, we obtain  cid:28 − x − x3 3 +  x − x3   cid:28 − x − x2 3 +  x − x2   cid:29  f2,3 x  = k2 6 −y2 x − x3  + y3 x − x2    cid:29  − k3  6  Therefore,  y 2.6  = f2,3 2.6  = 0.2308  6  = 0.1871   cid:28 − −0.4 3 +  −0.4    cid:29  − 0 − 0.5 −0.4  + 0  EXAMPLE 3.9 Use the module cubicSpline to write a program that interpolates between given data points with a natural cubic spline. The program must be able to evaluate the in- terpolant for more than one value of x. As a test, use data points speciﬁed in Example 3.4 and compute the interpolant at x = 1.5 and x = 4.5  because of symmetry, these values should be equal .  Solution  ! usr bin python   example3_9  import numpy as np  from cubicSpline import *  xData = np.array [1,2,3,4,5],float   yData = np.array [0,1,0,1,0],float   k = curvatures xData,yData   while True:  try: x = eval input "\nx ==> "    except SyntaxError: break  print "y =",evalSpline xData,yData,k,x    input "Done. Press return to exit"    126  Interpolation and Curve Fitting  Running the program produces the following result:  x ==> 1.5  y = 0.767857142857  x ==> 4.5  y = 0.767857142857  x ==>  Done. Press return to exit  PROBLEM SET 3.1  1. Given the data points  −1.2 1.1 −5.76 −5.61 −3.69  0.3  x y  determine y at x = 0 using  a  Neville’s method and  b  Lagrange’s method.  2. Find the zero of y x  from the following data:  x y  0  0.5  1  1.5  2  2.5  3  1.8421  2.4694  2.4921  1.9047  0.8509 −0.4112 −1.5727  Use Lagrange’s interpolation over  a  three; and  b  four nearest-neighbor data points. Hint: After ﬁnishing part  a , part  b  can be computed with a relatively small effort. 3. The function y x  represented by the data in Prob. 2 has a maximum at x = 0.7692. Compute this maximum by Neville’s interpolation over four nearest- neighbor data points.  4. Use Neville’s method to compute y at x = π  4 from the data points  x y  0  −1.00  0.5 1.75  1  4.00  1.5 5.75  2  7.00  5. Given the data  x y  0  −0.7854  0.5  1  1.5  2  0.6529  1.7390  2.2071  1.9425  ﬁnd y at x = π  4 and at π  2. Use the method that you consider to be most con- venient.  6. The points  x −2 −1 y  1 2  4 −1 4 59  −4 3 24 −53  lie on a polynomial. Use the divided difference table of Newton’s method to de- termine the degree of the polynomial.   127  3.3 Interpolation with Cubic Spline  7. Use Newton’s method to ﬁnd the polynomial that ﬁts the following points:  x −3 0 y  2 −1 5 −4  3 12  1 0  x −1 y  1  3 17 −7 −15  8. Use Neville’s method to determine the equation of the quadratic that passes  through the points  9. Density of air ρ varies with elevation h in the following manner:  h  km   ρ  kg m3   0  3  6  1.225  0.905  0.652  Express ρ h  as a quadratic function using Lagrange’s method.  10. Determine the natural cubic spline that passes through the data points  x y  0 0  1 2  2 1  Note that the interpolant consists of two cubics, one valid in 0 ≤ x ≤ 1, the other in 1 ≤ x ≤ 2. Verify that these cubics have the same ﬁrst and second derivatives at x = 1.  11. Given the data points  x y  1 13  2 15  3 12  4 9  5 13  determine the natural cubic spline interpolant at x = 3.4.  12. Compute the zero of the function y x  from the following data:  x y  0.2 1.150  0.4 0.855  0.6 1.0 0.377 −0.266 −1.049  0.8  Use inverse interpolation with the natural cubic spline. Hint: Reorder the data so that the values of y are in ascending order.  13. Solve Example 3.8 with a cubic spline that has constant second derivatives within its ﬁrst and last segments  the end segments are parabolic . The end conditions for this spline are k0 = k1 and kn−1 = kn.  14.  cid:2  Write a computer program for interpolation by Neville’s method. The program must be able to compute the interpolant at several user-speciﬁed values of x. Test the program by determining y at x = 1.1, 1.2, and 1.3 from the following data:  −2.0 2.2796 −0.6 1.0920  −0.1 1.0025  −1.5 1.6467  2.2  1.0  1.0635  0.5  1.8  2.6291  1.2661  1.9896  x y x y   Answer: y = 1.3262, 1.3938, 1.4639    128  Interpolation and Curve Fitting  15.  cid:2  The speciﬁc heat cp of aluminum depends on temperature T as follows2:  ◦  T    C   cp  kJ kg·K   −250 −200 −100 0.699 0.0163  0.318  0  0.870  100 0.941  300 1.04  Plot the polynomial and the rational function interpolants from T = −250 ◦ ◦ 500  . Comment on the results.  to  16.  cid:2  Using the data  x y  0  0.385  0.0204 1.04  0.1055 1.79  0.241 2.63  0.582 4.39  0.712 4.99  0.981 5.27  plot the rational function interpolant from x = 0 to x = 1. 17.  cid:2  The table shows the drag coefﬁcient cD of a sphere as a function of Reynold’s number Re.3 Use a natural cubic spline to ﬁnd cD at Re = 5, 50, 500, and 5, 000. Hint: Use a log-log scale.  Re cD  0.2 103  2  13.9  20 2.72  200 0.800  2000 0.401  20 000 0.433  18.  cid:2  Solve Prob. 17 using a polynomial interpolant intersecting four nearest-  neighbor data points  do not use a log scale .  19.  cid:2  The kinematic viscosity μk of water varies with temperature T in the following  manner:  ◦ T   C  −3 m2 s  μk  10  0  1.79  21.1 1.13  ◦  37.8 0.696  54.4 0.519  71.1 0.338  87.8 0.321  100 0.296  Interpolate μk at T = 10  ◦  ◦ , 30  , 60  , and 90  C.  ◦  20.  cid:2  The table shows how the relative density ρ of air varies with altitude h. Deter-  mine the relative density of air at 10.5 km.  h  km   ρ  0 1  1.525 0.8617  3.050 0.7385  4.575 0.6292  6.10 0.5328  7.625 0.4481  9.150 0.3741  21.  cid:2  The vibrational amplitude of a driveshaft is measured at various speeds. The  results are  Speed  rpm  Amplitude  mm   0 0  400 0.072  800 0.233  1200 0.712  1600 3.400  Use rational function interpolation to plot amplitude vs. speed from 0 to 2,500 rpm. From the plot estimate the speed of the shaft at resonance.  2 Source: Black, Z.B. and Hartley, J.G., Thermodynamics, Harper & Row, 1985. 3 Source: Kreith, F., Principles of Heat Transfer, Harper & Row, 1973.   129  3.4 Least-Squares Fit  3.4  Least-Squares Fit  Overview  If the data are obtained from experiments, they typically contain a signiﬁcant amount of random noise caused by measurement errors. The task of curve ﬁtting is to ﬁnd a smooth curve that ﬁts the data points “on the average.” This curve should have a simple form  e.g., a low-order polynomial , so as to not reproduce the noise.  Let  f  x  = f  x;a0, a1, . . . , am   be the function that is to be ﬁtted to the n + 1 data points  xi, yi , i = 0, 1, . . . , n. The notation implies that we have a function of x that contains m + 1 variable parameters a0, a1, . . . , am, where m < n. The form of f  x  is determined beforehand, usually from the theory associated with the experiment from which the data are obtained. The only means of adjusting the ﬁt are the parameters. For example, if the data represent the displacements yi of an over-damped mass-spring system at time ti, the theory suggests the choice f  t  = a0te −a1t . Thus curve ﬁtting consists of two steps: choosing the form of f  x , followed by computation of the parameters that produce the best ﬁt to the data.  This brings us to the question: What is meant by the “best” ﬁt? If the noise is conﬁned to the y-coordinate, the most commonly used measure is the least-squares ﬁt, which minimizes the function  S a0, a1, . . . , am  = n cid:6    cid:28    cid:29 2 yi − f  xi   i=0   3.13   with respect to each a j . Therefore, the optimal values of the parameters are given by the solution of the equations  = 0, k = 0, 1, . . . , m.  ∂ S ∂ak   3.14  The terms ri = yi − f  xi  in Eq.  3.13  are called residuals; they represent the dis- crepancy between the data points and the ﬁtting function at xi. The function S to be minimized is thus the sum of the squares of the residuals. Equations  3.14  are gen- erally nonlinear in a j and may thus be difﬁcult to solve. Often the ﬁtting function is chosen as a linear combination of speciﬁed functions fj  x ,  f  x  = a0 f0 x  + a1 f1 x  + ··· + am fm x   in which case Eqs.  3.14  are linear. If the ﬁtting function is a polynomial, we have f0 x  = 1, f1 x  = x, f2 x  = x2, and so on.    The spread of the data about the ﬁtting curve is quantiﬁed by the standard devi-  ation, deﬁned as  σ =  S  n − m   3.15    130  Interpolation and Curve Fitting Note that if n = m, we have interpolation, not curve ﬁtting. In that case both the nu- merator and the denominator in Eq.  3.15  are zero, so that σ is indeterminate.  Fitting a Straight Line  Fitting a straight line   3.16    cid:30   f  x  = a + bx  cid:29 2 = n cid:6   i=0  yi − f  xi  * a  n + 1  + b ,  to data is also known as linear regression. In this case the function to be minimized is  S a, b  = n cid:6    cid:28   i=0   cid:31 2  yi − a − bxi  i=0  ∂ S ∂a  Equations  3.14  now become  −2 yi − a − bxi  = 2  = n cid:6  = n cid:6   xi − n cid:6  n cid:6  n cid:6  − n cid:6  i=0 Dividing both equations by 2  n + 1  and rearranging terms, we get n cid:6   −2 yi − a − bxi xi = 2 ,  xi + b -  n cid:6   n cid:6   ∂ S ∂b  i=0  i=0  i=0  i=0  i=0  x2 i  a  yi  a + ¯xb = ¯y  +  = 0 -  = 0  xi yi  1 n + 1  x2 i  i=0  ¯xa + n cid:6   xi yi  i=0  b = 1 n + 1 n cid:6   i=0  ¯x = 1 n + 1  cid:27   cid:27    cid:27  − ¯x x2 i − n ¯x2 x2 i  xi  xi yi  yi  i=0  ¯y = 1 n + 1  cid:27   cid:27  xi yi − ¯x x2 i  b =   cid:27  − n ¯x2  yi  a = ¯y   3.17    3.18   are the mean values of the x and y data. The solution for the parameters is  where  These expressions are susceptible to roundoff errors  the two terms in each numer- ator as well as in each denominator can be roughly equal . Therefore it is better to compute the parameters from b =  a = ¯y − ¯xb   cid:27   cid:27    3.19   yi xi − ¯x  xi xi − ¯x   which are equivalent to Eqs.  3.18 , but are much less affected by rounding off.  Fitting Linear Forms  Consider the least-squares ﬁt of the linear form  f  x  = a0 f0 x  + a1 f1 x  + . . . + am fm x  = m cid:6   a j fj  x   j=0   3.20    131  3.4 Least-Squares Fit  where each fj  x  is a predetermined function of x, called a basis function. Substitu- tion in Eq.  3.13  yields  S = n cid:6   i=0  a j fj  xi   ⎤ ⎦2  ⎡ ⎣yi − m cid:6  ⎫⎬ ⎤ ⎦ fk xi  ⎭ = 0, k = 0, 1, . . . , m  j=0  a j fj  xi   Dropping the constant  −2  and interchanging the order of summation, we get  fk xi yi, k = 0, 1, . . . , m  Thus Eqs.  3.14  are  ∂ S ∂ak  ⎧⎨ ⎩ n cid:6  = −2 * n cid:6   m cid:6   i=0  ⎡ ⎣yi − m cid:6  +  j=0  fj  xi fk xi   j=0  i=0  In matrix notation these equations are  a j = n cid:6   i=0  Aa = b  where  A kj = n cid:6   i=0  bk = n cid:6   i=0  fj  xi fk xi   fk xi yi   3.21b    3.21a   Equation  3.21a , known as the normal equations of the least-squares ﬁt, can be solved with the methods discussed in Chapter 2. Note that the coefﬁcient matrix is symmetric  i.e., A kj = A j k .  Polynomial Fit  we have f  x  = cid:27   A commonly used linear form is a polynomial. If the degree of the polynomial is m,  m j=0 a j x j . Here the basis functions are   j = 0, 1, . . . , m    3.22   fj  x  = x j  i=0  A kj = n cid:6   cid:27   cid:27   cid:27  ...  x2 i x3 i  xm+1  i  x j+k  i  xk i yi  i=0  bk = n cid:6  ⎤  cid:27   cid:27  ⎥⎥⎥⎥⎦  cid:27  ...  xm i xm+1  i  x2m i  . . .  . . . ... . . .  so that Eqs.  3.21b  become  or  ⎡ ⎢⎢⎢⎢⎣  n xi   cid:27   cid:27  ...  xm−1  i  A =  cid:27    cid:27   cid:27   cid:27  ...  cid:27   xi x2 i  xm i  ⎤ ⎥⎥⎥⎥⎦  ⎡ ⎢⎢⎢⎢⎣   cid:27  yi cid:27  ... cid:27   xi yi  xm i yi  b =   3.23   n i=0. The normal equations become progressively ill condi- where tioned with increasing m. Fortunately, this is of little practical consequence, because  stands for   132  Interpolation and Curve Fitting  only low-order polynomials are useful in curve ﬁtting. Polynomials of high order are not recommended, because they tend to reproduce the noise inherent in the data.   cid:2  polyFit   cid:27    cid:27   The function polyFit in this module sets up and solves the normal equations for the coefﬁcients of a polynomial of degree m. It returns the coefﬁcients of the polyno- that make up the mial. To facilitate computations, the terms n, coefﬁcient matrix in Eq.  3.23  are ﬁrst stored in the vector s and then inserted into A. The normal equations are then solved by Gauss elimination with pivoting. After the solution is found, the standard deviation σ can be computed with the function std- Dev. The polynomial evaluation in stdDev is carried out by the embedded function evalPoly—see Section 4.7 for an explanation of the algorithm.  x2 i , . . . ,   cid:27   x2m i  xi,   module polyFit  ’’’ c = polyFit xData,yData,m .  Returns coefficients of the polynomial  p x  = c[0] + c[1]x + c[2]xˆ2 +...+ c[m]xˆm  that fits the specified data in the least  squares sense.  sigma = stdDev c,xData,yData .  Computes the std. deviation between p x   and the data.  ’’’  import numpy as np  import math  from gaussPivot import *  def polyFit xData,yData,m :  a = np.zeros  m+1,m+1    b = np.zeros m+1   s = np.zeros 2*m+1   for i in range len xData  :  temp = yData[i]  for j in range m+1 :  b[j] = b[j] + temp  temp = temp*xData[i]  temp = 1.0  for j in range 2*m+1 :  s[j] = s[j] + temp  temp = temp*xData[i]  for i in range m+1 :  for j in range m+1 :   133  3.4 Least-Squares Fit  a[i,j] = s[i+j]  return gaussPivot a,b   def stdDev c,xData,yData :  def evalPoly c,x :  m = len c  - 1  p = c[m]  for j in range m :  p = p*x + c[m-j-1]  return p  n = len xData  - 1  m = len c  - 1  sigma = 0.0  for i in range n+1 :  p = evalPoly c,xData[i]   sigma = sigma +  yData[i] - p **2  sigma = math.sqrt sigma  n - m    return sigma   cid:2  plotPoly  The function plotPoly listed next is handy for plotting the data points and the ﬁt- ting polynomial.   module plotPoly  ’’’ plotPoly xData,yData,coeff,xlab=’x’,ylab=’y’   Plots data points and the fitting  polynomial defined by its coefficient  array coeff = [a0, a1. ...]  xlab and ylab are optional axis labels  ’’’  import numpy as np  import matplotlib.pyplot as plt  def plotPoly xData,yData,coeff,xlab=’x’,ylab=’y’ :  m = len coeff   x1 = min xData   x2 = max xData   dx =  x2 - x1  20.0  x = np.arange x1,x2 + dx 10.0,dx   y = np.zeros  len x   *1.0  for i in range m :   134  Interpolation and Curve Fitting  y = y + coeff[i]*x**i  plt.plot xData,yData,’o’,x,y,’-’   plt.xlabel xlab ; plt.ylabel ylab   plt.grid  True   plt.show    Weighting of Data  There are occasions when our conﬁdence in the accuracy of data varies from point to point. For example, the instrument taking the measurements may be more sensitive in a certain range of data. Sometimes the data represent the results of several exper- iments, each carried out under different conditions. Under these circumstances we may want to assign a conﬁdence factor, or weight, to each data point and minimize the sum of the squares of the weighted residuals ri = Wi , where Wi are the weights. Hence the function to be minimized is   cid:28    cid:29  yi − f  xi   cid:29 2 yi − f  xi    cid:28   S a0, a1, . . . , am  = n cid:6   W 2 i  i=0   3.24   This procedure forces the ﬁtting function f  x  closer to the data points that have higher weights.  Weighted linear regression. If the ﬁtting function is the straight line f  x  = a + bx, Eq.  3.24  becomes  i  yi − a − bxi 2 W 2   3.25   The conditions for minimizing S are  S a, b  = n cid:6   i=0  n cid:6   ∂ S ∂a  ∂ S ∂b  i=0  = −2 n cid:6   = −2  i=0  i  yi − a − bxi  = 0 W 2  i  yi − a − bxi xi = 0 W 2  n cid:6   i=0  a  n cid:6   i=0  + b  W 2 i  W 2  W2  i yi  n cid:6   i=0  n cid:6   i=0  i xi = n cid:6  = n cid:6   i=0  i=0  a  i xi + b W 2  W 2  i x2 i  W 2  i xi yi   3.26a    3.26b   or   135  3.4 Least-Squares Fit  Dividing the Eq.  3.26a  by  W2  i and introducing the weighted averages,   cid:27   ˆx =   cid:27  i xi cid:27   W2 W2 i   cid:27  i yi cid:27   W2 W2 i  ˆy =  a = ˆy − b ˆx.  cid:27   cid:27   i yi xi − ˆx  W2 i xi xi − ˆx  W2  b =  we obtain  Substituting into Eq.  3.26b  and solving for b yields, after some algebra,   3.27    3.28a    3.28b   Note that Eqs.  3.28  are quite similar to Eqs.  3.19  of unweighted data.  Fitting exponential functions. A special application of weighted linear regres- sion arises in ﬁtting various exponential functions to data. Consider as an example the ﬁtting function  f  x  = aebx  Normally, the least-squares ﬁt would lead to equations that are nonlinear in a and b. But if we ﬁt ln y rather than y, the problem is transformed to linear regression: ﬁtting the function  F  x  = ln f  x  = lna + bx  to the data points  xi, ln yi , i = 0, 1, . . . , n. This simpliﬁcation comes at a price: The least-squares ﬁt to the logarithm of the data is not quite the same as the least-squares ﬁt to the original data. The residuals of the logarithmic ﬁt are ln a + bxi  Ri = ln yi − F  xi  = ln yi − cid:30    3.29a    cid:31   whereas the residuals used in ﬁtting the original data are ri = yi − f  xi  = yi − aebxi   3.29b   This discrepancy can be largely eliminated by weighting the logarithmic ﬁt. From Eq.  3.29b  we obtain ln ri − yi  = ln aebxi   = lna + bxi, so that Eq.  3.29a  can be written as  Ri = ln yi − ln ri − yi  = ln  %  &  1 − ri yi  If the residuals ri are sufﬁciently small  ri << yi , we can use the approximation ln 1 − ri  yi  ≈ ri  yi, so that  Ri ≈ ri  yi   136  Interpolation and Curve Fitting  R2 We can now see that by minimizing i , we have inadvertently introduced the weights 1 yi. This effect can be negated if we apply the weights Wi = yi when ﬁtting F  x  to  ln yi, xi . That is, minimizing   cid:27   S = n cid:6   cid:27   i=0  y 2 i R2 i   3.30   is a good approximation to minimizing  Other examples that also beneﬁt from the weights Wi = yi are given in Table 3.4.  r 2 i .  f  x  axebx axb  F  x    cid:29  = lna + bx   cid:28  ln ln f  x  = ln a + b ln x   f  x  x  Data to be ﬁtted by F  x    cid:28   cid:30    cid:29   cid:31   xi, ln yi  xi  ln xi, ln yi  Table 3.4. Fitting exponential functions.  EXAMPLE 3.10 Fit a straight line to the data shown and compute the standard deviation.  Solution. The averages of the data are  x y  0.0 2.9  1.0 3.7  2.0 4.1  2.5 4.4  3.0 5.0  xi = 0.0 + 1.0 + 2.0 + 2.5 + 3.0 yi = 2.9 + 3.7 + 4.1 + 4.4 + 5.0  5  = 1.7  = 4. 02  5  The intercept a and slope b of the interpolant can now be determined from Eq.  3.19 :  = 2.9 −1.7  + 3.7 −0.7  + 4.1 0.3  + 4.4 0.8  + 5.0 1.3  0.0 −1.7  + 1.0 −0.7  + 2.0 0.3  + 2.5 0.8  + 3.0 1.3    cid:6  ¯x = 1 5  cid:6   ¯y = 1 5  cid:27   cid:27   b =  yi xi − ¯x  xi xi − ¯x   = 3. 73 5. 8  = 0. 6431  a = ¯y − ¯xb = 4.02 − 1.7 0.6431  = 2. 927  Therefore, the regression line is f  x  = 2.927 + 0.6431x, which is shown in the ﬁgure together with the data points.   137  3.4 Least-Squares Fit  y  4.00  5.00  4.50  3.50  3.00  2.50  0.00  0.50  1.00  2.00  2.50  3.00  1.50 x  We start the evaluation of the standard deviation by computing the residuals:  x y  0.000 2.900 f  x  2.927 y − f  x  −0.027  1.000 2.500 3.700 4.400 3.570 4.535 0.130 −0.113 −0.135  2.000 4.100 4.213  3.000 5.000 4.856 0.144  The sum of the squares of the residuals is   cid:29 2 yi − f  xi    cid:6  cid:28  S = =  −0.027 2 +  0.130 2 +  −0.113 2 +  −0.135 2 +  0.144 2 = 0.06936  so that the standard deviation in Eq.  3.15  becomes      cid:7   σ =  =  S 5 − 2  0.06936  = 0.1520  3  EXAMPLE 3.11 Determine the parameters a and b so that f  x  = aebx ﬁts the following data in the least-squares sense.  x y  1.2 7.5  2.8 16.1  4.3 38.9  5.4 67.0  6.8 146.6  7.9 266.2  Use two different methods:  1  ﬁt ln yi; and  2  ﬁt ln yi with weights Wi = yi. Compute the standard deviation in each case. Solution of Part  1 . The problem is to ﬁt the function ln aebx  = ln a + bx to the data  x  z = ln y  1.2 2.015  2.8 2.779  4.3 3.661  5.4 4.205  6.8 4.988  7.9 5.584   138  Interpolation and Curve Fitting  We are now dealing with linear regression, where the parameters to be found are A = ln a and b. Following the steps in Example 3.8, we get  skipping some of the arith- metic details ,   cid:6    cid:27   cid:27   ¯x = 1 6 zi xi − ¯x  xi xi − ¯x   b =   cid:6   xi = 4. 733  ¯z = 1 6  zi = 3. 872  = 16.716 31.153  = 0. 5366  A = ¯z − ¯xb = 1. 3323  Therefore, a = eA = 3. 790 and the ﬁtting function becomes f  x  = 3.790e0.5366. The plots of f  x  and the data points are shown in the ﬁgure.  y  300  250  200  150  100  50  0  1  2  3  4  5  6  7  8  x  Here is the computation of standard deviation:  x y  f  x  y − f  x   1.20 2.80 7.50 16.10 7.21 17.02 0.29 −0.92  cid:6  cid:28   S =  4.30 5.40 38.90 67.00 38.07 68.69 0.83 −1.69  6.80 146.60 145.60 1.00  7.90 266.20 262.72 3.48   cid:29 2 = 17.59  yi − f  xi     σ =  S 6 − 2  = 2.10  As pointed out earlier, this is an approximate solution of the stated problem, because we did not ﬁt yi, but ln yi. Judging by the plot, the ﬁt seems to be quite good.   139  3.4 Least-Squares Fit Solution of Part  2 . We again ﬁt ln aebx  = lna + bx to z = ln y, but this time using the weights Wi = yi. From Eqs.  3.27  the weighted averages of the data are  recall that we ﬁt z = ln y   y 2 y 2 i   cid:27  i xi cid:27   cid:27  i zi cid:27   y 2 y 2 i  ˆx =  ˆz =  = 737.5 × 103 98.67 × 103 = 528.2 × 103 98.67 × 103  = 7.474  = 5.353  and Eqs.  3.28  yield for the parameters   cid:27   cid:27   b =  i zi xi − ˆx  y 2 i xi xi − ˆx  y 2  = 35.39 × 103 65.05 × 103  = 0.5440  lna = ˆz − b ˆx = 5.353 − 0.5440 7.474  = 1. 287  Therefore,  a = eln a = e1.287 = 3. 622  so that the ﬁtting function is f  x  = 3.622e0.5440x. As expected, this result is somewhat different from that obtained in Part  1 .  The computations of the residuals and the standard deviation are as follows:  6.80 146.60 146.33 0.267  7.90 266.20 266.20 0.00  x y  f  x  y − f  x   4.30 5.40 38.90 67.00 37.56 68.33 1.34 −1.33  1.20 2.80 7.50 16.10 6.96 16.61 0.54 −0.51  cid:6  cid:28   cid:29  2 = 4.186 yi − f  xi     S =  σ =  S 6 − 2  = 1.023  Observe that the residuals and standard deviation are smaller than in Part  1 , indi- cating a better ﬁt, as expected. It can be shown that ﬁtting yi directly  which involves the solution of a transcen- dental equation  results in f  x  = 3.614e0.5442. The corresponding standard deviation is σ = 1.022, which is very close to the result in Part  2 .   140  Interpolation and Curve Fitting  EXAMPLE 3.12 Write a program that ﬁts a polynomial of arbitrary degree m to the data points shown in the following table. Use the program to determine m that best ﬁts this data in the least-squares sense.  x −0.04 y x y  1.95  2.90  0.93  3.83 −8.66 −6.44 −4.36 −3.27 −0.88 10.09 5.98 3.31 8.85  9.08 7.40  8.21 6.19  7.05 4.63  5.00 0.87  Solution. The program shown next prompts for m. Execution is terminated by enter- ing an invalid character  e.g., the “return” character .  ! usr bin python   example3_12  import numpy as np  from polyFit import *  xData = np.array [-0.04,0.93,1.95,2.90,3.83,5.0,  \  yData = np.array [-8.66,-6.44,-4.36,-3.27,-0.88,0.87, \  5.98,7.05,8.21,9.08,10.09]   3.31,4.63,6.19,7.4,8.85]   while True:  try:  m = eval input "\nDegree of polynomial ==> "    coeff = polyFit xData,yData,m   print "Coefficients are:\n",coeff   print "Std. deviation =",stdDev coeff,xData,yData    except SyntaxError: break  input "Finished. Press return to exit"   The results are  Degree of polynomial ==> 1  Coefficients are:  [-7.94533287  1.72860425]  Std. deviation = 0.5112788367370911  Degree of polynomial ==> 2  Coefficients are:  [-8.57005662  2.15121691 -0.04197119]  Std. deviation = 0.3109920728551074  Degree of polynomial ==> 3  Coefficients are:   141  3.4 Least-Squares Fit  [ -8.46603423e+00 1.98104441e+00 2.88447008e-03 -2.98524686e-03]  Std. deviation = 0.31948179156753187  Degree of polynomial ==>  Finished. Press return to exit  Because the quadratic f  x  = −8.5700 + 2.1512x − 0.041971x2 produces the smallest standard deviation, it can be considered as the “best” ﬁt to the data. But be warned—the standard deviation is not a reliable measure of the goodness-of-ﬁt. It is always a good idea to plot the data points and f  x  before making a ﬁnal deter- mination. The plot of our data indicates that the quadratic  solid line  is indeed a reasonable choice for the ﬁtting function.  10.0  5.0  0.0  -5.0  y  -10.0  -2.0  0.0  2.0  4.0  6.0  8.0  10.0  12.0  x  PROBLEM SET 3.2  Instructions: Plot the data points and the ﬁtting function whenever appropriate.  1. Show that the straight line obtained by least-squares ﬁt of unweighted data al-  ways passes through the point   ¯x, ¯y .  2. Use linear regression to ﬁnd the line that ﬁts the data  −1.0 −0.5 −1.00 −0.55  x y  0  0.00  0.5 0.45  1.0 1.00  and determine the standard deviation.  3. Three tensile tests were carried out on an aluminum bar. In each test the strain  was measured at the same values of stress. The results were  Stress  MPa  Strain  Test 1  Strain  Test 2  Strain  Test 3   34.5 0.46 0.34 0.73  69.0 0.95 1.02 1.10  103.5 1.48 1.51 1.62  138.0 1.93 2.09 2.12   142  Interpolation and Curve Fitting  where the units of strain are mm m. Use linear regression to estimate the mod- ulus of elasticity of the bar  modulus of elasticity = stress strain .  4. Solve Prob. 3 assuming that the third test was performed on an inferior machine,  so that its results carry only half the weight of the other two tests.  5.  cid:2  The following table shows the annual atmospheric CO2 concentration  in parts per million  in Antarctica. Fit a straight line to the data and determine the average increase of the concentration per year.  Year 1994 ppm 356.8 Year 2002 ppm 370.5  1995 358.2 2003 372.2  1996 360.3 2004 374.9  1997 361.8 2005 376.7  1998 364.0 2006 378.7  1999 365.7 2007 381.0  2000 366.7 2008 382.9  2001 368.2 2009 384.7  6.  cid:2  The following table displays the mass M and average fuel consumption φ of motor vehicles manufactured by Ford and Honda in 2008. Fit a straight line φ = a + bM to the data and compute the standard deviation.  Model Focus Crown Victoria Expedition Explorer F-150 Fusion Taurus Fit Accord CR-V Civic Ridgeline  M  kg  1198 1715 2530 2014 2136 1492 1652 1168 1492 1602 1192 2045  φ  km liter  11.90 6.80 5.53 6.38 5.53 8.50 7.65 13.60 9.78 8.93 11.90 6.38  7.  cid:2  The relative density ρ of air was measured at various altitudes h. The results  were  h  km   ρ  0 1  1.525 0.8617  3.050 0.7385  4.575 0.6292  6.10 0.5328  7.625 0.4481  9.150 0.3741  Use a quadratic least-squares ﬁt to determine the relative air density at h = 10.5 km.  This problem was solved by interpolation in Prob. 20, Problem Set 3.1.   8.  cid:2  The kinematic viscosity μk of water varies with temperature T as shown in the following table. Determine the cubic that best ﬁts the data, and use it to compute μk at T = 10 C.  This problem was solved in Prob. 19, Problem Set 3.1 by interpolation.   ◦ , and 90  ◦ , 30  , 60  ◦  ◦  ◦ T   C  −3 m2 s  μk  10  0  1.79  21.1 1.13  37.8 0.696  54.4 0.519  71.1 0.338  87.8 0.321  100 0.296   143  3.4 Least-Squares Fit  9.  cid:2  Fit a straight line and a quadratic to the data in the following table.  x y  1.0 6.008  2.5  3.5  4.0  15.722  27.130  33.772  1.1 5.257  1.8 9.549  2.2  3.7  11.098  28.828  Which is a better ﬁt?  10.  cid:2  The following table displays thermal efﬁciencies of some early steam engines4.  Use linear regression to predict the thermal efﬁciency in the year 2000.  Year 1718 1767 1774 1775 1792 1816 1828 1834 1878 1906  Efﬁciency  %   0.5 0.8 1.4 2.7 4.5 7.5 12.0 17.0 17.2 23.0  Type Newcomen Smeaton Smeaton Watt Watt Woolf compound Improved Cornish Improved Cornish Corliss compound Triple expansion  11. The following table shows the variation of relative thermal conductivity k of sodium with temperature T. Find the quadratic that ﬁts the data in the least- squares sense.  C   ◦  T   k  79 1.00  190 0.932  357 0.839  524 0.759  690 0.693  12. Let f  x  = axb be the least-squares ﬁt of the data  xi, yi , i = 0, 1, . . . , n, and let F  x  = lna + b ln x be the least-squares ﬁt of  ln xi, ln yi —see Table 3.4. Prove that Ri ≈ ri  yi, where the residuals are ri = yi − f  xi  and Ri = ln yi − F  xi . Assume that ri << yi. 13. Determine a and b for which f  x  = a sin πx 2  + b cos πx 2  ﬁts the following  data in the least-squares sense. −0.19  −0.5 0.35 −3.558 −2.874 −1.995 −1.040 −0.068  0.02  0.20  x y  0.50 0.677  14. Determine a and b so that f  x  = axb ﬁts the following data in the least-squares  x y  0.5 0.49  1.0 1.60  1.5 3.36  2.0 6.44  2.5 10.16  4 Source: Singer, C., Holmyard, E.J., Hall, A.R., and Williams, T.H., A History of Technology, Oxford  sense.  Press, 1958.   144  Interpolation and Curve Fitting 15. Fit the function f  x  = axebx to the following data and compute the standard  deviation.  x y  0.5 0.541  1.0 0.398  1.5 0.232  2.0 0.106  2.5 0.052  16.  cid:2  The intensity of radiation of a radioactive substance was measured at half-year  intervals. The results were  t  years   t  years   γ  γ  0  3  1.000  0.972  0.5 0.994 3.5 0.969  1  4  0.990  0.967  1.5 0.985 4.5 0.960  2  5  0.979  0.956  2.5 0.977 5.5 0.952  where γ is the relative intensity of radiation. Knowing that radioactivity decays exponentially with time, γ  t  = ae −bt , estimate the radioactive half-life of the substance.  17. Linear regression can be extended to data that depend on two or more variables  called multiple linear regression . If the dependent variable is z and indepen- dent variables are x and y, the data to be ﬁtted have the form  Instead of a straight line, the ﬁtting function now represents a plane:  Show that the normal equations for the coefﬁcients are  ⎡ ⎢⎣ n  f  x, y  = a + bx + cy ⎤ ⎥⎦ =  ⎡ ⎢⎣a  ⎤ ⎥⎦   cid:19 yi  cid:19 xi  cid:19 xi  cid:19 x2  cid:19 xi yi i  cid:19 yi  cid:19 xi yi  cid:19 y 2 i  ⎤ ⎥⎦  ⎡ ⎢⎣  cid:19 zi   cid:19 xizi  cid:19 yizi  f  x, y  = a + bx + cy  that ﬁts the following data:  18. Use multiple linear regression explained in Prob. 17 to determine the function  x1 x2 x3 ... xn  y1 y2 y3 ... yn  z1 z2 z3 ... zn  b c  x 0 0 1 2 2 2  y 0 1 0 0 1 2  z  1.42 1.85 0.78 0.18 0.60 1.05   4  Roots of Equations  Find the solutions of f  x  = 0, where the function f is given.  4.1  Introduction  A common problem encountered in engineering analysis is as follows: Given a func- tion f  x , determine the values of x for which f  x  = 0. The solutions  values of x  are known as the roots of the equation f  x  = 0, or the zeroes of the function f  x .  Before proceeding further, it might be helpful to review the concept of a function.  The equation  y = f  x   contains three elements: an input value x, an output value y, and the rule f for com- puting y. The function is said to be given if the rule f is speciﬁed. In numerical com- puting the rule is invariably a computer algorithm. It may be a function statement, such as  f  x  = cosh x  cos x  − 1  or a complex procedure containing hundreds or thousands of lines of code. As long as the algorithm produces an output y for each input x, it qualiﬁes as a function.  The roots of equations may be real or complex. The complex roots are seldom computed, because they rarely have physical signiﬁcance. An exception is the poly- nomial equation  a0 + a1x + a1x2 + ··· + anxn = 0  where the complex roots may be meaningful  as in the analysis of damped vibrations, for example . First we will concentrate on ﬁnding the real roots of equations. Com- plex zeros of polynomials are treated near the end of this chapter.  In general, an equation may have any number of  real  roots or no roots at all.  For example,  145  sin x − x = 0   146  Roots of Equations has a single root, namely x = 0, whereas  tan x − x = 0  has an inﬁnite number of roots  x = 0, ±4.493, ±7.725, . . . .  All methods of ﬁnding roots are iterative procedures that require a starting point  i.e., an estimate of the root . This estimate is crucial; a bad starting value may fail to converge, or it may converge to the “wrong” root  a root different from the one sought . There is no universal recipe for estimating the value of a root. If the equa- tion is associated with a physical problem, then the context of the problem  physi- cal insight  might suggest the approximate location of the root. Otherwise, you may undertake a systematic numerical search for the roots. One such search method is described in the next section. Plotting the function is another means of locating the roots, but it is a visual procedure that cannot be programmed.  It is highly advisable to go a step further and bracket the root  determine its lower and upper bounds  before passing the problem to a root-ﬁnding algorithm. Prior bracketing is, in fact, mandatory in the methods described in this chapter.  4.2  Incremental Search Method  The approximate locations of the roots are best determined by plotting the function. Often a very rough plot, based on a few points, is sufﬁcient to provide reasonable starting values. Another useful tool for detecting and bracketing roots is the incre- mental search method. It can also be adapted for computing roots, but the effort would not be worthwhile, because other methods described in this chapter are more efﬁcient for that task.  The basic idea behind the incremental search method is simple: If f  x1  and f  x2  have opposite signs, then there is at least one root in the interval  x1, x2 . If the inter- val is small enough, it is likely to contain a single root. Thus the zeros of f  x  can be detected by evaluating the function at intervals  cid:9 x and looking for a change in sign.  10.0  5.0  0.0  -5.0  -10.0  0  1  2  4  5  6  3 x  Figure 4.1. Plot of tan x.   147  4.2 Incremental Search Method  There are several potential problems with the incremental search method:  than the spacing of the roots.    It is possible to miss two closely spaced roots if the search increment  cid:9 x is larger   A double root  two roots that coincide  will not be detected.   Certain singularities  poles  of f  x  can be mistaken for roots. For example, f  x  = tan x changes sign at x = ± 1 2nπ, n = 1, 3, 5, . . ., as shown in Figure 4.1. However, these locations are not true zeroes, because the function does not cross the x-axis.   cid:2  rootsearch  This function searches for a zero of the user-supplied function f x  in the interval  a,b  in increments of dx. It returns the bounds  x1,x2  of the root if the search was successful; x1 = x2 = None indicates that no roots were detected. After the ﬁrst root  the root closest to a  has been detected, rootsearch can be called again with a replaced by x2 in order to ﬁnd the next root. This can be repeated as long as rootsearch detects a root.   module rootsearch  ’’’ x1,x2 = rootsearch f,a,b,dx .  Searches the interval  a,b  in increments dx for  the bounds  x1,x2  of the smallest root of f x .  Returns x1 = x2 = None if no roots were detected.  ’’’  from numpy import sign  def rootsearch f,a,b,dx :  x1 = a; f1 = f a   x2 = a + dx; f2 = f x2   while sign f1  == sign f2 :  if x1  >=  b: return None,None  x1 = x2; f1 = f2  x2 = x1 + dx; f2 = f x2   else:  return x1,x2  EXAMPLE 4.1 A root of x3 − 10x2 + 5 = 0 lies in the interval  0, 1 . Use rootsearch to compute this root with four-digit accuracy.  Solution. To obtain four-digit accuracy, we need a search increment no bigger than  cid:9 x = 0.0001. A search of the interval  0, 1  at increments  cid:9 x would thus entail 10,000 function evaluations. The following program reduces thenumber of function   148  Roots of Equations  evalutions to 40 by closing in on the root in four stages, each stage involving 10 search intervals  and thus 10 function evaluations .  ! usr bin python   example4_1  from rootsearch import *  def f x : return x**3 - 10.0*x**2 + 5.0  x1 = 0.0; x2 = 1.0  for i in range 4 :  dx =  x2 - x1  10.0  x1,x2 = rootsearch f,x1,x2,dx   x =  x1 + x2  2.0  print ’x =’, ’{:6.4f}’.format x    input "Press return to exit"   The output is  x = 0.7346  4.3 Method of Bisection  After a root of f  x  = 0 has been bracketed in the interval  x1, x2 , several methods can be used to close in on it. The method of bisection accomplishes this by succes- sively halving the interval until it becomes sufﬁciently small. This technique is also known as the interval halving method. Bisection is not the fastest method available for computing roots, but it is the most reliable one. Once a root has been bracketed, bisection will always close in on it.  The method of bisection uses the same principle as incremental search: If there is a root in the interval  x1, x2 , then f  x1  and f  x2  have opposite signs. To halve the 2  x1 + x2  is the midpoint of the interval. If interval, we compute f  x3 , where x3 = 1 f  x2  and f  x3  have opposite signs, then the root must be in  x2, x3 , and we record this by replacing the original bound x1 by x3. Otherwise, the root lies in  x1, x3 , in which case x2 is replaced by x3. In either case, the new interval  x1, x2  is half the size of the original interval. The bisection is repeated until the interval has been reduced to a small value ε, so that  x2 − x1 ≤ ε  It is easy to compute the number of bisections required to reach a prescribed ε. The original interval  cid:9 x is reduced to  cid:9 x 2 after one bisection,  cid:9 x 22 after two   149  4.3 Method of Bisection bisections, and after n bisections it is  cid:9 x 2n. Setting  cid:9 x 2n = ε and solving for n, we get  n = ln   cid:9 x ε   ln 2   4.1   Since n must be an integer, the ceiling of n is used  the ceiling of n is the smallest integer greater than n .   cid:2  bisection  This function uses the method of bisection to compute the root of f x  = 0 that is known to lie in the interval  x1,x2 . The number of bisections n required to reduce the interval to tol is computed from Eq.  4.1 . By setting switch = 1, we force the routine to check whether the magnitude of f x  decreases with each interval halving. If it does not, something may be wrong  probably the “root” is not a root at all, but a pole , and root = None is returned. Because this feature is not always desirable, the default value is switch = 0. The function error.err, which we use to terminate a program, is listed in Section 1.7.   module bisection  ’’’ root = bisection f,x1,x2,switch=0,tol=1.0e-9 .  Finds a root of f x  = 0 by bisection.  The root must be bracketed in  x1,x2 .  Setting switch = 1 returns root = None if  f x  increases upon bisection.  ’’’  import math  import error  from numpy import sign  def bisection f,x1,x2,switch=1,tol=1.0e-9 :  f1 = f x1   f2 = f x2   if f1 == 0.0: return x1  if f2 == 0.0: return x2  if sign f1  == sign f2 :  error.err ’Root is not bracketed’   n = int math.ceil math.log abs x2 - x1  tol  math.log 2.0     for i in range n :  x3 = 0.5* x1 + x2 ; f3 = f x3   if  switch == 1  and  abs f3  > abs f1   \  and  abs f3  > abs f2  :  return None   150  Roots of Equations  if f3 == 0.0: return x3  if sign f2 != sign f3 : x1 = x3; f1 = f3  else: x2 = x3; f2 = f3  return  x1 + x2  2.0  EXAMPLE 4.2 Use bisection to ﬁnd the root of x3 − 10x2 + 5 = 0 that lies in the interval  0, 1  to four-digit accuracy  this problem was solved with rootsearch in Example 4.1 . How many function evaluations are involved in the procedure?  Solution. Here is the program:  ! usr bin python   example4_2  from bisection import *  def f x : return x**3 - 10.0*x**2 + 5.0  x = bisection f, 0.0, 1.0, tol = 1.0e-4   print ’x =’, ’{:6.4f}’.format x    input "Press return to exit"   Note that we set ε = 0.0001  tol = 1.0e-4 in bisection  to limit the accu-  racy to four signiﬁcant ﬁgures. The result is  x = 0.7346  According to Eq.  4.1   n = ln   cid:9 x  ε   ln 2  = ln 1.0 0.0001  = 13.29  ln 2  Therefore, the number of function evaluations in the for loop of bisection is  cid:16 13.29 cid:17  = 14. There are an additional 2 evaluations at the beginning of the subrou- tine, making a total of 16 function evaluations.  EXAMPLE 4.3 Find all the zeros of f  x  = x − tan x in the interval  0, 20  by the method of bisection. Use the functions rootsearch and bisection. Solution. Note that tan x is singular and changes sign at x = π  2, 3π  2, . . . . To pre- vent bisection from mistaking these points for roots, we set switch = 1. The closeness of roots to the singularities is another potential problem that can be allevi- ated by using small  cid:9 x in rootsearch. Choosing  cid:9 x = 0.01, we arrive at the following program:  ! usr bin python   example4_3  import math  from rootsearch import *   151  4.4 Methods Based on Linear Interpolation  from bisection import *  def f x : return x - math.tan x   a,b,dx =  0.0, 20.0, 0.01   print "The roots are:"   while True:  x1,x2 = rootsearch f,a,b,dx   if x1 != None:  a = x2  root = bisection f,x1,x2,1   if root != None: print root   else:  print "\nDone"   break  input "Press return to exit"   The output from the program is  The roots are:  0.0  4.493409458100745  7.725251837074637  10.904121659695917  14.06619391292308  17.220755272209537  Done  4.4 Methods Based on Linear Interpolation  Secant and False Position Methods  The secant and the false position methods are closely related. Both methods require two starting estimates of the root, say, x1 and x2. The function f  x  is assumed to be approximately linear near the root, so that the improved value x3 of the root can be estimated by linear interpolation between x1 and x2.  Referring to Figure 4.2, the similar triangles  shaded in the ﬁgure  yield the  relationship  where we used the notation fi = f  xi . Thus the improved estimate of the root is   4.2   f2  x3 − x2  = f1 − f2 x2 − x1  x3 = x2 − f2  x2 − x1 f2 − f1   152  Roots of Equations  f  x   f1  x1  Figure 4.2. Linear interpolation.  Linear approximation  x  x3  f2 x2  The false position method  also known as regula falsi  requires x1 and x2 to bracket the root. After the improved root is computed from Eq.  4.2 , either x1 or x2 is replaced by x3. If f3 has the same sign as f1, we let x1 ← x3; otherwise we choose x2 ← x3. In this manner, the root is always bracketed in  x1, x2 . The procedure is then repeated until convergence is obtained.  The secant method differs from the false position method in two ways: It does not require prior bracketing of the root, and it discards the oldest prior estimate of the root  i.e., after x3 is computed, we let x1 ← x2, x2 ← x3 . The convergence of the secant method can be shown to be superlinear, with the error behaving as Ek+1 = cE 1.618...  the exponent 1.618 . . . is the “golden ratio” . The precise order of convergence for the false position method is impossible to calculate. Generally, it is somewhat better than linear, but not by much. However, because the false position method always brackets the root, it is more reliable. We do not delve further into these methods, because both of them are inferior to Ridder’s method as far as the order of convergence is concerned.  k  Ridder’s Method  Ridder’s method is a clever modiﬁcation of the false position method. Assuming that the root is bracketed in  x1, x2 , we ﬁrst compute f3 = f  x3 , where x3 is the midpoint of the bracket, as indicated in Figure 4.3 a . Next we the introduce the function  g x  = f  x e x−x1 Q  where the constant Q is determined by requiring the points  x1, g1 ,  x2, g2  and  x3, g3  to lie on a straight line, as shown in Figure 4.3 b . As before, the notation we use is gi = g xi . The improved value of the root is then obtained by linear inter- polation of g x  rather than f  x .  Let us now look at the details. From Eq.  a  we obtain  g1 = f1  g2 = f2e2hQ  g3 = f3ehQ   a    b   f x   g x   x2  x  x1  h  x3  h   a   x1  h  x3  x4 h   b   x2  x  Figure 4.3. Mapping used in Ridder’s method.   153  4.4 Methods Based on Linear Interpolation where h =  x2 − x1  2. The requirement that the three points in Figure 4.3b lie on a straight line is g3 =  g1 + g2  2, or  f3ehQ = 1 2   f1 + f2e2hQ   cid:26   which is a quadratic equation in ehQ. The solution is − f1 f2  ehQ = f3 ±  f 2 3 f2   c    4.3   Linear interpolation based on points  x1, g1  and  x3, g3  now yields for the im-  proved root  x4 = x3 − g3  x3 − x1 g3 − g1  = x3 − f3ehQ x3 − x1 f3ehQ − f1  where in the last step we used Eqs.  b . As the ﬁnal step, we substitute for ehQ from Eq.  c , and obtain after some algebra  x4 = x3 ±  x3 − x1   f3 cid:26   − f1 f2  f 2 3  It can be shown that the correct result is obtained by choosing the plus sign if f1 − f2 > 0, and the minus sign if f1 − f2 < 0. After the computation of x4, new brackets are determined for the root and Eq.  4.3  is applied again. The procedure is repeated until the difference between two successive values of x4 becomes negligible.  Ridder’s iterative formula in Eq.  4.3  has a very useful property: If x1 and x2 straddle the root, then x4 is always within the interval  x1, x2 . In other words, once the root is bracketed, it stays bracketed, making the method very reliable. The downside is that each iteration requires two function evaluations. There are compet- itive methods that get by with only one function evaluation per iteration  e.g., Brent’s method , but they are more complex and require elaborate bookkeeping.  Ridder’s method can be shown to converge quadratically, making it faster than either the secant or the false position method. It is the method to use if the derivative of f  x  is impossible or difﬁcult to compute.   cid:2  ridder  The following is the source code for Ridder’s method:   module ridder  ’’’ root = ridder f,a,b,tol=1.0e-9 .  Finds a root of f x  = 0 with Ridder’s method.  The root must be bracketed in  a,b .  ’’’  import error  import math  from numpy import sign   154  Roots of Equations  def ridder f,a,b,tol=1.0e-9 :  fa = f a   fb = f b   if fa == 0.0: return a  if fb == 0.0: return b  if sign f2 != sign f3 : x1 = x3; f1 = f3  for i in range 30 :   Compute the improved root x from Ridder’s formula  c = 0.5* a + b ; fc = f c   s = math.sqrt fc**2 - fa*fb   if s == 0.0: return None  dx =  c - a *fc s  if  fa - fb  < 0.0: dx = -dx  x = c + dx; fx = f x    Test for convergence  if i > 0:  xOld = x  if abs x - xOld  < tol*max abs x ,1.0 : return x   Re-bracket the root as tightly as possible  if sign fc  == sign fx :  if sign fa != sign fx : b = x; fb = fx  else: a = x; fa = fx  else:  return None  a = c; b = x; fa = fc; fb = fx  print ’Too many iterations’   EXAMPLE 4.4 Determine the root of f  x  = x3 − 10x2 + 5 = 0 that lies in  0.6, 0.8  with Ridder’s method.  Solution. The starting points are  x1 = 0.6 x2 = 0.8  f1 = 0.63 − 10 0.6 2 + 5 = 1.6160 f2 = 0.83 − 10 0.8 2 + 5 = −0.8880  First Iteration. Bisection yields the point  The improved estimate of the root can now be computed with Ridder’s formula:  x3 = 0.7  cid:26   f 2 3  s = − f1 f2 = x4 = x3 ±  x3 − x1   f3 = 0.73 − 10 0.7 2 + 5 = 0.4430  cid:25  0.43302 − 1.6160 −0.8880  = 1.2738 f3 s   155  4.4 Methods Based on Linear Interpolation  Because f1 > f2 we must use the plus sign. Therefore,  x4 = 0.7 +  0.7 − 0.6   0.4430 1.2738  = 0.7348  f4 = 0.73483 − 10 0.7348 2 + 5 = −0.0026  As the root clearly lies in the interval  x3, x4 , we let  x1 ← x3 = 0.7 x2 ← x4 = 0.7348  f1 ← f3 = 0.4430  f2 ← f4 = −0.0026  which are the starting points for the next iteration.  Second Iteration  x3 = 0.5 x1 + x2  = 0.5 0.7 + 0.7348  = 0.717 4  f3 = 0.717 43 − 10 0.717 4 2 + 5 = 0.2226   cid:26   s =  − f1 f2 =  f 2 3   cid:25  0.22262 − 0.4430 −0.0026  = 0.2252  x4 = x3 ±  x3 − x1   f3 s  Since f1 > f2, we again use the plus sign, so that  x4 = 0.717 4 +  0.717 4 − 0.7   0.2226 0.2252  = 0.7346  f4 = 0.73463 − 10 0.7346 2 + 5 = 0.0000  Thus the root is x = 0.7346, accurate to at least four decimal places.  EXAMPLE 4.5 Compute the zero of the function  f  x  =  1   x − 0.3 2 + 0.01   x − 0.8 2 + 0.04  −  1  Solution. We obtain the approximate location of the root by plotting the function.   156  Roots of Equations  100  80  60  40  20  0  -20  -40 -2  root with the following program:  ! usr bin python   example4_5  from ridder import *  def f x :  a =  x - 0.3 **2 + 0.01  b =  x - 0.8 **2 + 0.04  return 1.0 a - 1.0 b  print "root =",ridder f,0.0,1.0    input "Press return to exit"   The result is  root = 0.5800000000000001  -1  0  1  2  3  It is evident that the root of f  x  = 0 lies between x = 0 and 1. We can extract this  4.5 Newton-Raphson Method  The Newton-Raphson algorithm is the best known method of ﬁnding roots for a good reason: It is simple and fast. The only drawback of the method is that it uses the derivative f  x  of the function as well as the function f  x  itself. Therefore,  x  can be readily the Newton-Raphson method is usable only in problems where f computed.   cid:3    cid:3    157  4.5 Newton-Raphson Method  The Newton-Raphson formula can be derived from the Taylor series expansion  of f  x  about x:  f  xi+1  = f  xi  + f   cid:3    xi  xi+1 − xi  + O xi+1 − xi 2   a  where O z  is to be read as “of the order of z”—see Appendix A1. If xi+1 is a root of f  x  = 0, Eq.  a  becomes  0 = f  xi  + f   cid:3    xi   xi+1 − xi  + O xi+1 − xi 2   b   Assuming that xi is a close to xi+1, we can drop the last term in Eq.  b  and solve for xi+1. The result is the Newton-Raphson formula: xi+1 = xi − f  xi  f  cid:3  xi    4.3   f  x   Tangent line  f  x  i  xi  x  xi +1  Figure 4.4. Graphical interpretation of the Newton-Raphson formula.  The graphical depiction of the Newton-Raphson formula is shown in Figure 4.4. The formula approximates f  x  by the straight line that is tangent to the curve at xi. Thus xi+1 is at the intersection of the x-axis and the tangent line.  The algorithm for the Newton-Raphson method is simple: It repeatedly applies  Eq.  4.3 , starting with an initial value x0, until the convergence criterion  xi+1 − xi < ε  is reached, ε being the error tolerance. Only the latest value of x has to be stored. Here is the algorithm:  Let x be an estimate of the root of f  x  = 0. Do until  cid:9 x < ε:  Compute  cid:9 x = −f  x  f Let x ← x +  cid:9 x .   cid:3    x .  The truncation error E in the Newton-Raphson formula can be shown to behave  as  Ei+1 = − f   cid:3  cid:3   x  2f  cid:3  x   E 2 i  where x is the root. This indicates that the method converges quadratically  the error is the square of the error in the previous step . Consequently, the number of signiﬁ- cant ﬁgures is roughly doubled in every iteration.  Although the Newton-Raphson method converges fast near the root, its global convergence characteristics are poor. The reason is that the tangent line is not always   158  Roots of Equations  f x   f x   x0   a   x  x1  x0  x2  x   b   Figure 4.5. Examples where the Newton-Raphson method di- verges.  an acceptable approximation of the function, as illustrated in the two examples in Figure 4.5. However, the method can be made nearly fail-safe by combining it with bisection.   cid:2  newtonRaphson  The following safe version of the Newton-Raphson method assumes that the root to be computed is initially bracketed in  a,b . The midpoint of the bracket is used as the initial guess of the root. The brackets are updated after each iteration. If a Newton-Raphson iteration does not stay within the brackets, it is disregarded and replaced with bisection. Because newtonRaphson uses the function f x  as well as its derivative, function routines for both  denoted by f and df in the listing  must be provided by the user.   module newtonRaphson  ’’’ root = newtonRaphson f,df,a,b,tol=1.0e-9 .  Finds a root of f x  = 0 by combining the Newton-Raphson  method with bisection. The root must be bracketed in  a,b .  Calls user-supplied functions f x  and its derivative df x .  ’’’  def newtonRaphson f,df,a,b,tol=1.0e-9 :  if sign fa  == sign fb : error.err ’Root is not bracketed’   import error  from numpy import sign  if fa == 0.0: return a  fa = f a   fb = f b   if fb == 0.0: return b  x = 0.5* a + b   for i in range 30 :  fx = f x   if fx == 0.0: return x   Tighten the brackets on the root  if sign fa  != sign fx : b = x  else: a = x   159  4.5 Newton-Raphson Method   Try a Newton-Raphson step  dfx = df x    If division by zero, push x out of bounds  try: dx = -fx dfx  except ZeroDivisionError: dx = b - a  x = x + dx   If the result is outside the brackets, use bisection  if  b - x * x - a  < 0.0:  dx = 0.5* b - a   x = a + dx   Check for convergence  if abs dx  < tol*max abs b ,1.0 : return x  print ’Too many iterations in Newton-Raphson’   EXAMPLE 4.6 Use the Newton-Raphson method to obtain successive approximations of ratio of two integers. Solution. The problem is equivalent to ﬁnding the root of f  x  = x2 − 2 = 0. Here the Newton-Raphson formula is  √ 2 as the  x ← x − f  x  f  cid:3   x   = x − x2 − 2  = x2 + 2  2x  2x  Starting with x = 1, successive iterations yield  2 1   x ←  1 2 + 2 = 3 2 x ←  3 2 2 + 2 = 17 12 x ←  17 12 2 + 2 = 577 408  2 17 12   2 3 2   ...  Note that x = 577 408 = 1.1414216 is already very close to  2 = 1.1414214.  The results are dependent on the starting value of x. For example, x = 2 would  √  produce a different sequence of ratios.  EXAMPLE 4.7 Find the smallest positive zero of  f  x  = x4 − 6.4x3 + 6.45x2 + 20.538x − 31.752   160  Roots of Equations    x   f  60  40  20  0  -20  -40  0  1  2  3  4  5  x  Solution. Inspecting the plot of the function, we suspect that the smallest positive zero is a double root at about x = 2. Bisection and Ridder’s method would not work here, because they depend on the function changing its sign at the root. The same argument applies to the function newtonRaphson. Yet there is no reason why the unreﬁned version of the Newton-Raphson method should not succeed. We used the following program, which prints the number of iterations in addition to the root:  ! usr bin python   example4_8  def f x : return x**4 - 6.4*x**3 + 6.45*x**2 + 20.538*x - 31.752  def df x : return 4.0*x**3 - 19.2*x**2 + 12.9*x + 20.538  def newtonRaphson x,tol=1.0e-9 :  for i in range 30 :  dx = -f x  df x   x = x + dx  if abs dx  < tol: return x,i  print ’Too many iterations\n’  root,numIter = newtonRaphson 2.0   print ’Root =’,root  print ’Number of iterations =’,numIter  raw_input ’’Press return to exit’’   The output is  Root = 2.0999999786199406  Number of iterations = 22  The true value of the root is x = 2.1. It can be shown that near a multiple root the convergence of the Newton-Raphson method is linear, rather than quadratic,   161  4.6 Systems of Equations  which explains the large number of iterations. Convergence to a multiple root can be speeded up by replacing the Newton-Raphson formula in Eq.  4.3  with  xi+1 = xi − m  f  xi  f  cid:3  xi   where m is the multiplicity of the root  m = 2 in this problem . After making the change in the program, we obtained the result in only ﬁve iterations.  4.6  Systems of Equations  Introduction Up to this point, we conﬁned our attention to solving the single equation f  x  = 0. Let us now consider the n-dimensional version of the same problem, namely,  or, using scalar notation  f x  = 0  f1 x1, x2, . . . , xn  = 0 f2 x1, x2, . . . , xn  = 0 ... fn x1, x2, . . . , xn  = 0   4.4   Solving n simultaneous, nonlinear equations is a much more formidable task than ﬁnding the root of a single equation. The trouble is there is no a reliable method for bracketing the solution vector x. Therefore, we cannot always provide the solution algorithm with a good starting value of x, unless such a value is suggested by the physics of the problem.  The simplest and the most effective means of computing x is the Newton- Raphson method. It works well with simultaneous equations, provided that it is sup- plied with a good starting point. There are other methods that have better global con- vergence characteristics, but all of then are variants of the Newton-Raphson method.  Newton-Raphson Method  To derive the Newton-Raphson method for a system of equations, we start with the Taylor series expansion of fi x  about the point x:  fi x +  cid:3 x  = fi x  + n cid:6    cid:9 xj + O  cid:9 x2   ∂fi ∂xj  j=1   4.5a    162  Roots of Equations  Dropping terms of order  cid:9 x2, we can write Eq.  4.5a  as f x +  cid:3 x  = f x  + J x   cid:3 x  where J x  is the Jacobian matrix  of size n × n  made up of the partial derivatives   4.5b    4.6    4.7    4.8   Note that Eq.  4.5b  is a linear approximation  vector  cid:3 x being the variable  of the vector-valued function f in the vicinity of point x. Let us now assume that x is the current approximation of the solution of f x  = 0, and let x +  cid:3 x be the improved solution. To ﬁnd the correction  cid:3 x, we set f x +  cid:3 x  = 0 in Eq.  4.5b . The result is a set of linear equations for  cid:3 x :  Because analytical derivation of each ∂fi  ∂xj can be difﬁcult or impractical, it is preferable to let the computer calculate them from the ﬁnite difference approxima- tion  Jij = ∂fi ∂xj  J x  cid:3 x = −f x   ≈ fi x + ej h  − fi x   h  ∂fi ∂xj  where h is a small increment of x j and ej represents a unit vector in the direction of x j . This formula can be obtained from Eq.  4.5a  after dropping the terms of order  cid:9 x2 and setting  cid:3 x = e j h. We get away with the approximation in Eq.  4.8  because the Newton-Raphson method is rather insensitive to errors in J x . By using this ap- proximation, we also avoid the tedium of typing the expressions for ∂fi  ∂x j into the computer code.  The following steps constitute the Newton-Raphson method for simultaneous,  nonlinear equations:  Estimate the solution vector x. Do until  cid:3 x < ε:  Compute the matrix J x  from Eq.  4.8 . Solve J x  cid:3 x = −f x  for  cid:3 x. Let x ← x +  cid:3 x.  where ε is the error tolerance. As in the one-dimensional case, success of the Newton- Raphson procedure depends entirely on the initial estimate of x. If a good starting point is used, convergence to the solution is very rapid. Otherwise, the results are unpredictable.   cid:2  newtonRaphson2  This function is an implementation of the Newton-Raphson method. The nested function jacobian computes the Jacobian matrix from the ﬁnite difference approx- imation in Eq.  4.8 . The simultaneous equations in Eq.  4.7  are solved by Gauss   163  4.6 Systems of Equations  elimination with row pivoting using the function gaussPivot listed in Section 2.5. The function subroutine f that returns the array f x  must be supplied by the user.   module newtonRaphson2  ’’’ soln = newtonRaphson2 f,x,tol=1.0e-9 .  Solves the simultaneous equations f x  = 0 by  the Newton-Raphson method using {x} as the initial  guess. Note that {f} and {x} are vectors.  ’’’  import numpy as np  from gaussPivot import *  import math  def newtonRaphson2 f,x,tol=1.0e-9 :  def jacobian f,x :  h = 1.0e-4  n = len x   jac = np.zeros  n,n    f0 = f x   for i in range n :  temp = x[i]  x[i] = temp + h  f1 = f x   x[i] = temp  jac[:,i] =  f1 - f0  h  return jac,f0  for i in range 30 :  jac,f0 = jacobian f,x   dx = gaussPivot jac,-f0   x = x + dx  return x  print Too many iterations’   if math.sqrt np.dot f0,f0  len x   < tol: return x  if math.sqrt np.dot dx,dx   < tol*max max abs x  ,1.0 :  Note that the Jacobian matrix J x  is recomputed in each iterative loop. Since each calculation of J x  involves n + 1 evaluations of f x   n is the number of equa- tions , the expense of computation can be high depending on the size of n and the complexity of f x . It is often possible to save computer time by neglecting the changes in the Jacobian matrix between iterations, thus computing J x  only once. This approach will work, provided that the initial x is sufﬁciently close to the solu- tion.   164  Roots of Equations  EXAMPLE 4.8 Determine the points of intersection between the circle x2 + y 2 = 3 and the hyper- bola xy = 1.  Solution. The equations to be solved are  The Jacobian matrix is  f1 x, y  = x2 + y 2 − 3 = 0 f2 x, y  = xy − 1 = 0 * +  *  J x, y  =  ∂f1 ∂x ∂f2 ∂x  ∂f1 ∂y ∂f2 ∂y  =  2x y  2y x  *  +*  +  *  =  2x y  2y x   cid:9 x  cid:9 y  −x2 − y 2 + 3  −xy + 1  +  +   a    b    c   Thus the linear equations J x  cid:3 x = −f x  associated with the Newton-Raphson method are  By plotting the circle and the hyperbola, we see that there are four points of inter- section. It is sufﬁcient, however, to ﬁnd only one of these points, because the others can be deduced from symmetry. From the plot we also get a rough estimate of the coordinates of an intersection point, x = 0.5, y = 1.5, which we use as the starting values.  −2  −1  1  x  2  y 2  1  3  −1  −2  The computations then proceed as follows.  First Iteration. Substituting x = 0.5, y = 1.5 in Eq.  c , we get  +*  +  *  +  *  1.0 3.0 1.5 0.5   cid:9 x  cid:9 y  =  0.50 0.25  the solution of which is  cid:9 x =  cid:9 y = 0.125. Therefore, the improved coordinates of the intersection point are  x = 0.5 + 0.125 = 0.625  y = 1.5 + 0.125 = 1.625   165  4.6 Systems of Equations  Second Iteration. Repeating the procedure using the latest values of x and y, we obtain  *  +*  +  *  =  +  −0.031250 −0.015625  1.250 3.250 1.625 0.625   cid:9 x  cid:9 y  which yields  cid:9 x =  cid:9 y = −0.00694. Thus x = 0.625 − 0.006 94 = 0.618 06 +*  *  1.236 12 3.23612 1.618 06 0.61806  y = 1.625 − 0.006 94 = 1.618 06 +  *  +  =   cid:9 x  cid:9 y  −0.000 116 −0.000 058  Third Iteration. Substitution of the latest x and y into Eq.  c  yields  The solution is  cid:9 x =  cid:9 y = −0.00003, so that  x = 0.618 06 − 0.000 03 = 0.618 03 y = 1.618 06 − 0.000 03 = 1.618 03  Subsequent iterations would not change the results within ﬁve signiﬁcant ﬁg-  ures. Therefore, the coordinates of the four intersection points are ± 0.618 03, 1.618 03  and ±  1.618 03, 0.618 03   Alternate Solution. If there are only a few equations, it may be possible to eliminate all but one of the unknowns. Then we would be left with a single equation that can be solved by the methods described in Sections 4.2–4.5. In this problem, we obtain from Eq.  b   which upon substitution into Eq.  a  yields x2 + 1 x2 − 3 = 0, or  y = 1 x  x4 − 3x2 + 1 = 0  The solutions of this biquadratic equation are x = ±0.618 03 and ±1.618 03, which agree with the results obtained by the Newton-Raphson method.  EXAMPLE 4.9 Find a solution of  sin x + y2 + ln z − 7 = 0 3x + 2y − z3 + 1 = 0 x + y + z − 5 = 0 using newtonRaphson2. Start with the point  1, 1, 1 . Solution. Letting x1 = x, x2 = y and x3 = z, we obtain the following program:  ! usr bin python   example4_10   166  Roots of Equations  import numpy as np  import math  from newtonRaphson2 import *  def f x :  f = np.zeros len x    f[0] = math.sin x[0]  + x[1]**2 + math.log x[2]  - 7.0  f[1] = 3.0*x[0] + 2.0**x[1] - x[2]**3 + 1.0  f[2] = x[0] + x[1] + x[2] - 5.0  return f  x = np.array [1.0, 1.0, 1.0]   print newtonRaphson2 f,x    input "\nPress return to exit"   The output is  [ 0.59905376  2.3959314  2.00501484]  √ ations only  to compute 3 75 with four signiﬁcant ﬁgure accuracy.  PROBLEM SET 4.1 1. Use the Newton-Raphson method and a four-function calculator  + − ×÷ oper- 2. Find the smallest positive  real  root of x3 − 3.23x2 − 5.54x + 9.84 = 0 by the 3. The smallest positive, nonzero root of cosh x cos x − 1 = 0 lies in the interval  method of bisection.   4, 5 . Compute this root by Ridder’s method.  4. Solve Problem 3 by the Newton-Raphson method. 5. A root of the equation tan x − tanh x = 0 lies in  7.0, 7.4 . Find this root with three 6. Determine the two roots of sin x + 3 cos x − 2 = 0 that lie in the interval  −2, 2 .  decimal place accuracy by the method of bisection.  Use the Newton-Raphson method.  7. Solve Prob. 6 using the secant formula in Eq.  4.2 . 8. Draw a plot of f  x  = cosh x cos x − 1 in the range 0 ≤ x ≤ 10.  a  Verify from the plot that the smallest positive, nonzero root of f  x  = 0 lies in the interval  4, 5 .  b  Show graphically that the Newton-Raphson formula would not converge to this root if it is started with x = 4. 9. The equation x3 − 1.2x2 − 8.19x + 13.23 = 0 has a double root close to x = 2. Determine this root with the Newton-Raphson method within four decimal places. 10.  cid:2  Write a program that computes all the roots of f  x  = 0 in a given interval with Ridder’s method. Use the functions rootsearch and ridder. You may use the program in Example 4.3 as a model. Test the program by ﬁnding the roots of x sin x + 3 cos x − x = 0 in  −6, 6 .  11.  cid:2  Repeat Prob. 10 with the Newton-Raphson method. 12.  cid:2  Determine all real roots of x4 + 0.9x3 − 2.3x2 + 3.6x − 25.2 = 0.   167  4.6 Systems of Equations 13.  cid:2  Compute all positive real roots of x4 + 2x3 − 7x2 + 3 = 0. 14.  cid:2  Find all positive, nonzero roots of sin x − 0.1x = 0. 15.  cid:2  The natural frequencies of a uniform cantilever beam are related to the roots  βi of the frequency equation f  β  = cosh β cos β + 1 = 0, where  =  2π fi 2 mL3 β4 i E I fi = ith natural frequency  cps  m = mass of the beam L = length of the beam E = modulus of elasticity I = moment of inertia of the cross section  Determine the lowest two frequencies of a steel beam 0.9 m. long, with a rectan- gular cross section 25 mm wide and 2.5 mm in. high. The mass density of steel is 7850 kg m3 and E = 200 GPa.  16.  cid:2   A cable is suspended as shown in the ﬁgure. Its length s and the sag h are related to the span L by  L 2  h O  Length = s  L 2  %  & − 1  s = 2  λ  sinh  λL 2  h = 1  λ  cosh  λL 2  λ = w0 T0 w0 = weight of cable per unit length T0 = cable tension at O  P  c  e  P  L  where  Compute s for L = 160 m and h = 15 m.  17.  cid:2   The aluminum W 310 × 202  wide ﬂange  column is subjected to an eccentric axial load P as shown. The maximum compressive stress in the column is given by the so-called secant formula: σ max = ¯σ  1 + ec  -+   cid:7   ,  *  r 2 sec  L 2r  ¯σ E   168  Roots of Equations  where  ¯σ = P A = average stress A = 25 800 mm2 = cross-sectional area of the column e = 85 mm = eccentricity of the load c = 170 mm = half depth of the column r = 142 mm = radius of gyration of the cross section L = 7 100 mm = length of the column E = 71 × 109 Pa = modulus of elasticity  Determine the maximum load P that the column can carry if the maximum stress is not to exceed 120 × 106 Pa.  18.  cid:2   Bernoulli’s equation for ﬂuid ﬂow in an open channel with a small bump is  where  ho  Q  h  H  Q2  2gb2h2 0  + h0 = Q2 2gb2h2  + h + H  Q = 1.2 m3 s = volume rate of ﬂow g = 9.81 m s2 = gravitational acceleration b = 1.8 m = width of channel h0 = 0.6 m = upstream water level H = 0.075 m = height of bump h = water level above the bump  19.  cid:2  The speed v of a Saturn V rocket in vertical ﬂight near the surface of earth can  Determine h.  be approximated by  v = u ln  M0  M0 − ˙mt  − gt   169  4.6 Systems of Equations  where  u = 2 510 m s = velocity of exhaust relative to the rocket M0 = 2.8 × 106 kg = mass of rocket at liftoff ˙m = 13.3 × 103 kg s = rate of fuel consumption g = 9.81 m s2 = gravitational acceleration t = time measured from liftoff  Determine the time when the rocket reaches the speed of sound  335 m s .  20.  cid:2   P P 2  P 1  T 2  T 1  V 1  Heating at constant volume  Isothermal expansion  Volume reduced by cooling  T 2  V 2  V  The ﬁgure shows the thermodynamic cycle of an engine. The efﬁciency of this engine for monatomic gas is  η =  ln T2 T1  −  1 − T1 T2   ln T2 T1  +  1 − T1 T2   γ − 1   where T is the absolute temperature and γ = 5 3. Find T2 T1 that results in 30% efﬁciency  η = 0.3 .  21.  cid:2   y t  y t   k k  c c  m m  x t  x t   Consider the forced vibration of the spring-mass-dashpot system shown. When the harmonic displacement y t  = Y sin ωt is imposed on the support, the re- sponse of the mass is the displacement x t  = X sin ωt − β , where tan β = Z sin φ 1 + Z cos φ   cid:25   1 + Z cos φ 2 +  Z sin φ 2  X Y  =  In these two equations we used this notation:   cid:26  cid:28   Z =   ω p 2   cid:29 2 +  2ζ ω p 2  1 −  ω p 2  tan φ = 2ζ ω p 1 −  ω p 2   170  Roots of Equations   cid:7   p = k m ζ = c 2mp  = natural frequency of the system = damping factor  If m = 0.2 kg, k = 2 880 N m, and ω = 96 rad s, determine the smallest c  the coefﬁcient of damping  for which X Y does not exceed 1.5.  22.  cid:2   The cylindrical oil tank of radius r and length L is ﬁlled to depth h. The resulting volume of oil in the tank is  where  If the tank is 3 4 full, determine h r . 23.  cid:2  Determine the coordinates of the two points where the circles  x − 2 2 + y 2 = 4 and x2 +  y − 3 2 = 4 intersect. Start by estimating the locations of the points from a sketch of the circles, and then use the Newton-Raphson method to com- pute the coordinates.  24.  cid:2  The equations  have a solution in the vicinity of the point  1, 1 . Use the Newton-Raphson method to reﬁne the solution.  25.  cid:2  Use any method to ﬁnd all real solutions of the simultaneous equations  r  h  ’ φ −  V = r 2 L  φ = cos  −1     sin φ  &  % 1 − h r %  1 − h r  &  sin x + 3 cos x − 2 = 0 cos x − sin y + 0.2 = 0  tan x − y = 1 cos x − 3 sin y = 0   x − a 2 +  y − b 2 = R2  in the region 0 ≤ x ≤ 1.5.  26.  cid:2  The equation of a circle is   171  4.6 Systems of Equations  where R is the radius and  a, b  are the coordinates of the center. If the coordi- nates of three points on the circle are  determine R, a, and b.  27.  cid:2   x y  8.21 0.00  0.34 5.96 6.62 −1.12  R θ  O  The trajectory of a satellite orbiting the earth is  R =  C  1 + e sin θ + α   where  R, θ  are the polar coordinates of the satellite, and C, e, and α are con- stants  e is known as the eccentricity of the orbit . If the satellite was observed at the following three positions  θ  R  km   −30 ◦ 6870  ◦ 0  6728  ◦  30 6615  determine the smallest R of the trajectory and the corresponding value of θ.  28.  cid:2   y  v  O θ  45o  A  61 m x  300 m  x =  v cos θ t y = − 1 2  gt 2 +  v sin θ t  A projectile is launched at O with the velocity v at the angle θ to the horizontal. The parametric equations of the trajectory are  where t is the time measured from the instant of launch, and g = 9.81 m s2 rep- resents the gravitational acceleration. If the projectile is to hit the target A at the ◦ 45  angle shown in the ﬁgure, determine v, θ, and the time of ﬂight.   172  Roots of Equations  29.  cid:2   0   m m  8 1 θ2  y  150 mm θ1  200 mm  m 0 m  0 2  3θ  x  A  1θ B  4    m  16 kN  3 m D  12 m  θ2 6 m  5 m θ3  C  20 kN  T − tan θ 2 + tan θ 1  = 16 T tan θ 3 + tan θ 2  = 20  The three angles of the four-bar linkage are related by  150 cos θ 1 + 180 cos θ 2 − 200 cos θ 3 = 200 150 sin θ 1 + 180 sin θ 2 − 200 sin θ 3 = 0  Determine θ 1 and θ 2 when θ 3 = 75  ◦. Note that there are two solutions.  30.  cid:2   The 15-m cable is suspended from A and D and carries concentrated loads at B and C. The vertical equilibrium equations of joints B and C are  where T is the horizontal component of the cable force  it is the same in all seg- ments of the cable . In addition, there are two geometric constraints imposed by the positions of the supports:  −4 sin θ 1 − 6 sin θ 2 + 5 sin θ 2 = −3 4 cos θ 1 + 6 cos θ 2 + 5 cos θ 3 = 12  Determine the angles θ 1, θ 2, and θ 3.  31.  cid:2  Given the data points  x y  0.25  0 0 −1.2233 −2.2685 −2.8420 −2.2130  0.50  0.75  1.0  1.25 2.5478  1.5  55.507  determine the nonzero root of y x  = 0. Hint: Use rational function interpolation to compute y.   173  ∗4.7 Zeros of Polynomials  ∗4.7 Zeros of Polynomials  Introduction  A polynomial of degree n has the form  Pn x  = a0 + a1x + a2x2 + ··· + anxn   4.9   where the coefﬁcients ai may be real or complex. We focus here on polynomials with real coefﬁcients, but the algorithms presented in this section also work with complex coefﬁcients. The polynomial equation Pn x  = 0 has exactly n roots, which may be real or complex. If the coefﬁcients are real, the complex roots always occur in conjugate pairs  xr + ixi, xr − ixi , where xr and xi are the real and imaginary parts, respec- tively. For real coefﬁcients, the number of real roots can be estimated from the rule of Descartes:   The number of positive, real roots equals the number of sign changes in the ex-   The number of negative, real roots is equal to the number of sign changes in  pression for Pn x , or less by an even number. Pn −x , or less by an even number. As an example, consider P3 x  = x3 − 2x2 − 8x + 27. Since the sign changes twice, P3 x  = 0 has either two or zero positive real roots. In contrast, P3 −x  = −x3 − 2x2 + 8x + 27 contains a single sign change; hence P3 x  possesses one neg- ative real zero.  The real zeros of polynomials with real coefﬁcients can always be computed by one of the methods already described. Yet if complex roots are to be computed, it is best to use a method that specializes in polynomials. Here we present a method due to Laguerre, which is reliable and simple to implement. Before proceeding to Laguerre’s method, we must ﬁrst develop two numerical tools that are needed in any method capable of determining the zeros of a polynomial. The ﬁrst tool is an efﬁcient algorithm for evaluating a polynomial and its derivatives. The second algorithm we need is for the deﬂation of a polynomial; that is, for dividing the Pn x  by x − r , where r is a root of Pn x  = 0.  Evaluation of Polynomials  It is tempting to evaluate the polynomial in Eq.  4.9  from left to right by the following algorithm  we assume that the coefﬁcients are stored in the array a :  p = 0.0  for i in range n+1 :  p = p + a[i]*x**i   174  Roots of Equations  Since xk is evaluated as x × x × ··· × x  k − 1 multiplications , we deduce that  the number of multiplications in this algorithm is 1 + 2 + 3 + ··· + n − 1 = 1 2  n n − 1   If n is large, the number of multiplications can be reduced considerably if we evaluate the polynomial from right to left. For an example, take  P4 x  = a0 + a1x + a2x2 + a3x3 + a4x4  After rewriting the polynomial as  P4 x  = a0 + x {a1 + x [a2 + x  a3 + xa4 ]}  the preferred computational sequence becomes obvious:  P0 x  = a4 P1 x  = a3 + x P0 x  P2 x  = a2 + x P1 x  P3 x  = a1 + x P2 x  P4 x  = a0 + x P3 x   For a polynomial of degree n, the procedure can be summarized as  P0 x  = an Pi x  = an−i + x Pi−1 x ,  i = 1, 2, . . . , n   4.10   leading to the algorithm  p = a[n]  for i in range 1,n+1 :  p = a[n-i] + p*x  The last algorithm involves only n multiplications, making it more efﬁcient for n > 3. Yet computational economy is not the primary reason why this algorithm should be used. Because the result of each multiplication is rounded off, the pro- cedure with the least number of multiplications invariably accumulates the smallest roundoff error.  Some root-ﬁnding algorithms, including Laguerre’s method, also require eval- uation of the ﬁrst and second derivatives of Pn x . From Eq.  4.10  we obtain by differentiation  0 x  = 0 P  cid:3  P 0  x  = 0 P  cid:3  cid:3  P  i  x  = Pi−1 x  + x P  cid:3   cid:3  i−1 x , i  x  = 2P i−1 x  + x P  cid:3  cid:3   cid:3    cid:3  cid:3  i−1 x ,  i = 1, 2, . . . , n i = 1, 2, . . . , n   4.11a    4.11b    175  ∗4.7 Zeros of Polynomials   cid:2  evalPoly  Here is the function that evaluates a polynomial and its derivatives:   module evalPoly  ’’’ p,dp,ddp = evalPoly a,x .  Evaluates the polynomial  p = a[0] + a[1]*x + a[2]*xˆ2 +...+ a[n]*xˆn  with its derivatives dp = p’ and ddp = p’’  at x.  ’’’  def evalPoly a,x :  n = len a  - 1  p = a[n]  dp = 0.0 + 0.0j  ddp = 0.0 + 0.0j  for i in range 1,n+1 :  ddp = ddp*x + 2.0*dp  dp = dp*x + p  p = p*x + a[n-i]  return p,dp,ddp  Deﬂation of Polynomials After a root r of Pn x  = 0 has been computed, it is desirable to factor the polynomial as follows:  Pn x  =  x − r  Pn−1 x    4.12   This procedure, known as deﬂation or synthetic division, involves nothing more than computing the coefﬁcients of Pn−1 x . Because the remaining zeros of Pn x  are also the zeros of Pn−1 x , the root-ﬁnding procedure can now be applied to Pn−1 x  rather than Pn x . Deﬂation thus makes it progressively easier to ﬁnd successive roots, be- cause the degree of the polynomial is reduced every time a root is found. Moreover, by eliminating the roots that have already been found, the chance of computing the same root more than once is eliminated.  If we let  then Eq.  4.12  becomes  Pn−1 x  = b0 + b1x + b2x2 + ··· + bn−1xn−1  a0 + a1x + a2x2 + ··· + an−1xn−1 + anxn =  x − r   b0 + b1x + b2x2 + ··· + bn−1xn−1   Equating the coefﬁcients of like powers of x, we obtain ···  bn−2 = an−1 + rbn−1  bn−1 = an  b0 = a1 + rb1   4.13    176  Roots of Equations  which leads to Horner’s deﬂation algorithm:  b[n-1] = a[n]  for i in range n-2,-1,-1 :  b[i] = a[i+1] + r*b[i+1]  Laguerre’s Method  Laquerre’s formulas are not easily derived for a general polynomial Pn x . However, the derivation is greatly simpliﬁed if we consider the special case where the polyno- mial has a zero at x = r and an  n − 1  zeros at x = q. Hence the polynomial can be written as  Pn x  =  x − r   x − q n−1   a   Our problem is now this: Given the polynomial in Eq.  a  in the form  Pn x  = a0 + a1x + a2x2 + ··· + anxn  determine r  note that q is also unknown . It turns out that the result, which is ex- act for the special case considered here, works well as an iterative formula with any polynomial.  Differentiating Eq.  a  with respect to x, we get  n x  =  x − q n−1 +  n − 1  x − r   x − q n−2  cid:3  P  %  &  = Pn x   1 x − r  + n − 1 x − q  Thus   cid:3  n x  P Pn x   = 1 x − r  + n − 1 x − q  which upon differentiation yields  cid:3  P n x  Pn x    cid:3  cid:3  P n  x  Pn x   −  ’     2 = −  1   x − r  2  − n − 1  x − q 2  It is convenient to introduce the notation  G x  = P   cid:3  n x  Pn x   H x  = G2 x  − P   cid:3  cid:3  n  x  Pn x   so that Eqs.  b  and  c  become  G x  = 1 x − r 1  H x  =   x − r  2  + n − 1 x − q + n − 1  x − q 2   b    c    4.14    4.15a    4.15b    177  ∗4.7 Zeros of Polynomials If we solve Eq.  4.15a  for x − q and substitute the result into Eq.  4.15b , we obtain a quadratic equation for x − r. The solution of this equation is Laguerre’s formula:  x − r =   cid:26   n  n − 1    cid:28   cid:29  nH x  − G2 x   G x  ±   4.16   The procedure for ﬁnding a zero of a polynomial by Laguerre’s formula is as  follows:  Let x be a guess for the root of Pn x  = 0  any value will do . Do until Pn x  < ε or x − r < ε  ε is the error tolerance :   cid:3  n x  and P   cid:3  cid:3  n  x  using evalPoly.  Evaluate Pn x , P Compute G x  and H x  from Eqs.  4.14 . Determine the improved root r from Eq.  4.16  choosing the sign Let x ← r.  that results in the larger magnitude of the denominator.  One nice property of Laguerre’s method is that it converges to a root, with very  few exceptions, from any starting value of x.   cid:2  polyRoots The function polyRoots in this module computes all the roots of Pn x  = 0, where the polynomial Pn x  is deﬁned by its coefﬁcient array a = [a0, a1, . . . , an]. After the ﬁrst root is computed by the nested function laguerre, the polynomial is deﬂated using deflPoly, and the next zero is computed by applying laguerre to the de- ﬂated polynomial. This process is repeated until all n roots have been found. If a computed root has a very small imaginary part, it is more than likely that it repre- sents roundoff error. Therefore, polyRoots replaces a tiny imaginary part by zero.   module polyRoots  ’’’ roots = polyRoots a .  Uses Laguerre’s method to compute all the roots of  a[0] + a[1]*x + a[2]*xˆ2 +...+ a[n]*xˆn = 0.  The roots are returned in the array ’roots’,  ’’’  from evalPoly import *  import numpy as np  import cmath  from random import random  def polyRoots a,tol=1.0e-12 :  def laguerre a,tol :  n = len a  - 1  x = random     Starting value  random number    178  Roots of Equations  for i in range 30 :  p,dp,ddp = evalPoly a,x   if abs p  < tol: return x  g = dp p  h = g*g - ddp p  else: dx = n  g - f   x = x - dx  if abs dx  < tol: return x  print ’Too many iterations’   f = cmath.sqrt  n - 1 * n*h - g*g    if abs g + f  > abs g - f : dx = n  g + f   def deflPoly a,root :   Deflates a polynomial  n = len a -1  b = [ 0.0 + 0.0j ]*n  b[n-1] = a[n]  for i in range n-2,-1,-1 :  b[i] = a[i+1] + root*b[i+1]  return b  n = len a  - 1  roots = np.zeros  n ,dtype=complex   for i in range n :  x = laguerre a,tol   if abs x.imag  < tol: x = x.real  roots[i] = x  a = deflPoly a,x   return roots  Because the roots are computed with ﬁnite accuracy, each deﬂation introduces small errors in the coefﬁcients of the deﬂated polynomial. The accumulated roundoff error increases with the degree of the polynomial and can become severe if the poly- nomial is ill-conditioned  small changes in the coefﬁcients produce large changes in the roots . Hence the results should be viewed with caution when dealing with poly- nomials of high degree.  The errors caused by deﬂation can be reduced by recomputing each root using the original undeﬂated polynomial. The roots obtained previously in conjunction with deﬂation are employed as the starting values.  EXAMPLE 4.10 A zero of the polynomial P4 x  = 3x4 − 10x3 − 48x2 − 2x + 12 is x = 6. Deﬂate the polynomial with Horner’s algorithm; that is, ﬁnd P3 x  so that  x − 6 P3 x  = P4 x .   179  ∗4.7 Zeros of Polynomials Solution. With r = 6 and n = 4, Eqs.  4.13  become  b3 = a4 = 3 b2 = a3 + 6b3 = −10 + 6 3  = 8 b1 = a2 + 6b2 = −48 + 6 8  = 0 b0 = a1 + 6b1 = −2 + 6 0  = −2  P3 x  = 3x3 + 8x2 − 2  Therefore,  EXAMPLE 4.11 A root of the equation P3 x  = x3 − 4.0x2 − 4.48x + 26.1 is approximately x = 3 − i. Find a more accurate value of this root by one application of Laguerre’s iterative for- mula.  Solution. Use the given estimate of the root as the starting value. Thus  x = 3 − i  x2 = 8 − 6i  x3 = 18 − 26i  Substituting these values in P3 x  and its derivatives, we get  P3 x  = x3 − 4.0x2 − 4.48x + 26.1  =  18 − 26i  − 4.0 8 − 6i  − 4.48 3 − i  + 26.1 = −1.34 + 2.48i  3 x  = 3.0x2 − 8.0x − 4.48  cid:3  P  = 3.0 8 − 6i  − 8.0 3 − i  − 4.48 = −4.48 − 10.0i  3  x  = 6.0x − 8.0 = 6.0 3 − i  − 8.0 = 10.0 − 6.0i  cid:3  cid:3  P  Equations  4.14  then yield  G x  = P   cid:3  3 x  P3 x   H x  = G2 x  − P  = −4.48 − 10.0i −1.34 + 2.48i  cid:3  cid:3  3  x  P3 x   = 0.35995 − 12.48452i  = −2.36557 + 3.08462i  =  −2.36557 + 3.08462i 2 − 10.0 − 6.0i −1.34 + 2.48i  The term under the square root sign of the denominator in Eq.  4.16  becomes   cid:28   cid:29  n H x  − G 2 x    cid:26   n − 1   cid:26   cid:28   cid:25  2 5.67822 − 45.71946i = 5.08670 − 4.49402i  3 0.35995 − 12.48452i  −  −2.36557 + 3.08462i 2   cid:29   F  x  = = =   180  Roots of Equations  Now we must ﬁnd which sign in Eq.  4.16  produces the larger magnitude of the de- nominator:  G x  + F  x  =  −2.36557 + 3.08462i  +  5.08670 − 4.49402i   = 2.72113 − 1.40940i = 3.06448  G x  − F  x  =  −2.36557 + 3.08462i  −  5.08670 − 4.49402i   = −7.45227 + 7.57864i = 10.62884  Using the minus sign, Eq.  4.16  yields the following improved approximation for the root:  n  r = x − = 3.19790 − 0.79875i  G x  − F  x   =  3 − i  −  3  −7.45227 + 7.57864i  Thanks to the good starting value, this approximation is already quite close to the exact value r = 3.20 − 0.80i.  EXAMPLE 4.12 Use polyRoots to compute all the roots of x4 − 5x3 − 9x2 + 155x − 250 = 0.  Solution. The program  ! usr bin python   example4_12  from polyRoots import *  import numpy as np  c = np.array [-250.0,155.0,-9.0,-5.0,1.0]   print ’Roots are:\n’,polyRoots c    input ’Press return to exit’   produced the output  Roots are:  [ 2.+0.j  4.-3.j  4.+3.j -5.+0.j]  PROBLEM SET 4.2 Problems 1–5 A zero x = r of Pn x  is given. Verify that r is indeed a zero, and then deﬂate the polynomial; that is, ﬁnd Pn−1 x  so that Pn x  =  x − r  Pn−1 x . 1. P3 x  = 3x3 + 7x2 − 36x + 20, r = −5. 2. P4 x  = x4 − 3x2 + 3x − 1, r = 1. 3. P5 x  = x5 − 30x4 + 361x3 − 2178x2 + 6588x − 7992, r = 6. 4. P4 x  = x4 − 5x3 − 2x2 − 20x − 24, r = 2i. 5. P3 x  = 3x3 − 19x2 + 45x − 13, r = 3 − 2i.   181  ∗4.7 Zeros of Polynomials Problems 6–9 A zero x = r of Pn x  is given. Determine all the other zeros of Pn x  by using a calculator. You should need no tools other than deﬂation and the quadratic formula. 6. P3 x  = x3 + 1.8x2 − 9.01x − 13.398, r = −3.3. 7. P3 x  = x3 − 6.64x2 + 16.84x − 8.32, r = 0.64. 8. P3 x  = 2x3 − 13x2 + 32x − 13, r = 3 − 2i. 9. P4 x  = x4 − 3x2 + 10x2 − 6x − 20, r = 1 + 3i.  Problems 10–15 Find all the zeros of the given Pn x . 10.  cid:2 P4 x  = x4 + 2.1x3 − 2.52x2 + 2.1x − 3.52. 11.  cid:2 P5 x  = x5 − 156x4 − 5x3 + 780x2 + 4x − 624. 12.  cid:2 P6 x  = x6 + 4x5 − 8x4 − 34x3 + 57x2 + 130x − 150. 13.  cid:2 P7 x  = 8x7 + 28x6 + 34x5 − 13x4 − 124x3 + 19x2 + 220x − 100. 14.  cid:2 P3 x  = 2x3 − 6 1 + i x2 + x − 6 1 − i  15.  cid:2 P4 x  = x4 +  5 + i x3 −  8 − 5i x2 +  30 − 14i x − 84. 16.  cid:2   k  m  m  x1  x2  k  c  The two blocks of mass m each are connected by springs and a dashpot. The stiff- ness of each spring is k, and c is the coefﬁcient of damping of the dashpot. When the system is displaced and released, the displacement of each block during the ensuing motion has the form  xk t  = A keωr t cos ωit + φ  k , k = 1, 2  where A k and φk are constants, and ω = ωr ± iωi are the roots of  ω4 + 2  ω3 + 3  c m  k m  ω2 + c m  k m  ω +  k m  %  &2 = 0  Determine the two possible combinations of ωr and ωi if c m = 12 s k m = 1 500 s  −2.  −1 and  17.  L  y  w0  x   182  Roots of Equations  The lateral deﬂection of the beam shown is  y = w0 120E I   x5 − 3L2x3 + 2L3x2   where w0 is the maximum load intensity and E I represents the bending rigidity. Determine the value of x L where the maximum deﬂection occurs.  4.8 Other Methods  The most prominent root-ﬁnding algorithm omitted from this chapter is Brent’s method, which combines bisection and quadratic interpolation. It is potentially more efﬁcient than Ridder’s method, requiring only one function evaluation per iteration  as compared to two evaluations in Ridder’s method , but this advantage is somewhat negated by its required elaborate bookkeeping.  There are many methods for ﬁnding zeroes of polynomials. Of these, the Jenkins- Traub algorithm1 deserves special mention because of its robustness and widespread use in packaged software. the n × n “companion matrix”  The zeros of a polynomial can also be obtained by calculating the eigenvalues of  ⎡  ⎢⎢⎢⎢⎢⎢⎣  A =  −an−1 an −a2 an 1 0 ... 0  0 1 ... 0  ··· −a1 an −a0 an ··· . . . ... ···  0 0 ... 0  0 0 ... 1  ⎤  ⎥⎥⎥⎥⎥⎥⎦  where ai are the coefﬁcients of the polynomial. The characteristic equation  see Sec- tion 9.1  of this matrix is  xn + an−1 an  xn−1 + an−2 an  xn−2 + ··· + a1 an  x + a0 an  = 0  which is equivalent to Pn x  = 0. Thus the eigenvalues of A are the zeroes of Pn x . The eigenvalue method is robust, but considerably slower than Laguerre’s method. However, it is worthy of consideration if a good program for eigenvalue problems is available.  1 Jenkins, M. and Traub, J., SIAM Journal on Numerical Analysis, Vol. 7  1970 , p. 545.   5  Numerical Differentiation  Given the function f  x , compute dn f dxn at given x  5.1  Introduction  Numerical differentiation deals with the following problem: We are given the func- tion y = f  x  and wish to obtain one of its derivatives at the point x = xk. The term “given” means that we either have an algorithm for computing the function, or we possess a set of discrete data points  xi, yi , i = 0, 1, . . . , n. In either case, we have ac- cess to a ﬁnite number of  x, y  data pairs from which to compute the derivative. If you suspect by now that numerical differentiation is related to interpolation, you are correct—one means of ﬁnding the derivative is to approximate the function locally by a polynomial and then to differentiate it. An equally effective tool is the Taylor se- ries expansion of f  x  about the point xk, which has the advantage of providing us with information about the error involved in the approximation.  Numerical differentiation is not a particularly accurate process. It suffers from a conﬂict between roundoff errors  caused by limited machine precision  and errors inherent in interpolation. For this reason, a derivative of a function can never be com- puted with the same precision as the function itself.  5.2  Finite Difference Approximations  The derivation of the ﬁnite difference approximations for the derivatives of f  x  is based on forward and backward Taylor series expansions of f  x  about x, such as  f  x + h  = f  x  + hf   cid:3    x  + h2 2!   cid:3  cid:3    x  + h3 3!   cid:3  cid:3  cid:3    x  + h4 4!  f  4  x  + ···  f  x − h  = f  x  − hf   cid:3    x  + h2 2!   cid:3  cid:3    x  − h3 3!   cid:3  cid:3  cid:3    x  + h4 4!  f  4  x  − ···  f  f  f  f   a    b   183   184  Numerical Differentiation  f  x + 2h  = f  x  + 2hf   cid:3    x  +  2h 2 2!   x  +  2h 3 3!   cid:3  cid:3  cid:3    x  +  2h 4 4!  f  4  x  + ···   c   f  x − 2h  = f  x  − 2hf   cid:3    x  +  2h 2 2!   x  −  2h 3 3!   cid:3  cid:3  cid:3    x  +  2h 4 4!  f  4  x  − ···   d    cid:3  cid:3    cid:3  cid:3   f  f  f  f  We also record the sums and differences of the series:  f  x + h  + f  x − h  = 2f  x  + h2 f f  x + h  − f  x − h  = 2hf  x  + h3 3   cid:3    cid:3  cid:3    x  + h4 12  f  4  x  + ···   cid:3  cid:3  cid:3    x  + . . .  f  f  x + 2h  + f  x − 2h  = 2f  x  + 4h2 f  x  + 8h3 3  f  x + 2h  − f  x − 2h  = 4hf   cid:3    cid:3  cid:3    x  + 4h4 3   cid:3  cid:3  cid:3    x  + ···  f  f  4  x  + ···   e    f    g    h   Note that the sums contain only even derivatives, whereas the differences retain just the odd derivatives. Equations  a – h  can be viewed as simultaneous equations that can be solved for various derivatives of f  x . The number of equations involved and the number of terms kept in each equation depend on the order of the derivative and the desired degree of accuracy.  First Central Difference Approximations  The solution of Eq.  f  for f   x  is   cid:3    x  = f  x + h  − f  x − h    cid:3   f  2h  − h2 6   cid:3  cid:3  cid:3    x  − ···  f   x  = f  x + h  − f  x − h    cid:3   f  + O h2   2h  which is called the ﬁrst central difference approximation for f reminds us that the truncation error behaves as h2.  Similarly, Eq.  e  yields the ﬁrst central difference approximation for f   x :   x  = f  x + h  − 2f  x  + f  x − h    cid:3  cid:3   f  h2  + h2 12  f  4  x  + . . .   5.1    cid:3    x . The term O h2    cid:3  cid:3   or  or   x  = f  x + h  − 2f  x  + f  x − h    cid:3  cid:3   f  + O h2   h2   5.2    185  5.2 Finite Difference Approximations  Central difference approximations for other derivatives can be obtained from  x  from Eqs.  f  and  Eqs.  a – h  in the same manner. For example, eliminating f  h  and solving for f   x  yields   cid:3  cid:3  cid:3    cid:3    cid:3  cid:3  cid:3    x  = f  x + 2h  − 2f  x + h  + 2f  x − h  − f  x − 2h   f  + O h2   The approximation  f  4  x  = f  x + 2h  − 4f  x + h  + 6f  x  − 4f  x − h  + f  x − 2h   + O h2   is available from Eq.  e  and  g  after eliminating f results.   cid:3  cid:3    x . Table 5.1 summarizes the  2h3  h4   5.3    5.4    cid:3   x   cid:3  cid:3   x   cid:3  cid:3  cid:3   f  x − 2h   2hf h2 f 2h3 f  x  h4 f  4  x   f  x + h  1 1 −2 −4 Table 5.1. Coefﬁcients of Central Finite Difference Approximations of O h2   f  x − h  −1 1 2 −4  f  x  0 −2 0 6  −1 1  f  x + 2h   1 1  First Noncentral Finite Difference Approximations  Central ﬁnite difference approximations are not always usable. For example, con- sider the situation where the function is given at the n discrete points x0, x1, . . . , xn. Because central differences use values of the function on each side of x, we would be unable to compute the derivatives at x0 and xn. Clearly, there is a need for ﬁnite difference expressions that require evaluations of the function only on one side of x. These expressions are called forward and backward ﬁnite difference approximations. Noncentral ﬁnite differences can also be obtained from Eqs.  a – h . Solving  Eq.  a  for f   x  we get   cid:3   f   x  = f  x + h  − f  x    cid:3   h  − h 2  f   cid:3  cid:3    x  − h2 6  f   cid:3  cid:3  cid:3    x  − h3 4!  f  4  x  − ···  Keeping only the ﬁrst term on the right-hand side leads to the ﬁrst forward difference approximation:   x  = f  x + h  − f  x    cid:3   f  + O h   h   x  = f  x  − f  x − h    cid:3   + O h   Similarly, Eq.  b  yields the ﬁrst backward difference approximation:   5.6  Note that the truncation error is now O h , which is not as good as O h2  in central difference approximations.  h  f   5.5    186  Numerical Differentiation  We can derive the approximations for higher derivatives in the same manner. For  example, Eqs.  a  and  c  yield   x  = f  x + 2h  − 2f  x + h  + f  x    cid:3  cid:3   f  + O h   h2   5.7   The third and fourth derivatives can be derived in a similar fashion. The results  are shown in Tables 5.2a and 5.2b.  f  x + 2h   f  x + 3h   f  x + 4h   f  x  −1 1 −1 1  f  x + h  1 −2 3 −4   cid:3   x  hf  cid:3  cid:3  h2 f  x   cid:3  cid:3  cid:3  h3 f  x  h4 f  4  x   1 −4 Table 5.2a. Coefﬁcients of Forward Finite Difference Approximations of O h   1 −3 6  1  f  x − 4h   f  x − 3h   f  x − 2h    cid:3   x  hf  cid:3  cid:3  h2 f  x   cid:3  cid:3  cid:3  h3 f  x  h4 f  4  x   −1 −4  1  f  x − h  −1 −2 −3 −4  f  x  1 1 1 1  1 3 6  Table 5.2b. Coefﬁcients of Backward Finite Difference Approximations of O h   Second Noncentral Finite Difference Approximations Finite difference approximations of O h  are not popular because of reasons that are explained shortly. The common practice is to use expressions of O h2 . To ob- tain noncentral difference formulas of this order, we have to retain more term in the  x . We start with Taylor series. As an illustration, we derive the expression for f Eqs.  a  and  c , which are   cid:3   f  x + h  = f  x  + hf   cid:3    x  + h2 2   cid:3  cid:3   f  f  x + 2h  = f  x  + 2hf   cid:3    x  + 2h2 f   cid:3  cid:3  cid:3    x  + h3 f 6  x  + 4h3 3   cid:3  cid:3    x  + h4 24  f  4  x  + ···  f   cid:3  cid:3  cid:3    x  + 2h4 3  f  4  x  + ···   cid:3  cid:3   We eliminate f second equation. The result is   x  by multiplying the ﬁrst equation by 4 and subtracting it from the  f  x + 2h  − 4f  x + h  = −3f  x  − 2hf   cid:3    x  + h4 2  f  4  x  + ···   187  5.2 Finite Difference Approximations  Therefore,  or   x  = −f  x + 2h  + 4f  x + h  − 3f  x    cid:3   f  2h  + h2 4  f  4  x  + ···   x  = −f  x + 2h  + 4f  x + h  − 3f  x    cid:3   f  + O h2   2h   5.8   Equation  5.8  is called the second forward ﬁnite difference approximation.  Derivation of ﬁnite difference approximations for higher derivatives involves ad- ditional Taylor series. Thus the forward difference approximation for f  x  uses series for f  x + h , f  x + 2h , and f  x + 3h ; the approximation for f  x  involves Taylor ex- pansions for f  x + h , f  x + 2h , f  x + 3h , f  x + 4h , and so on. As you can see, the computations for high-order derivatives can become rather tedious. The results for both the forward and backward ﬁnite differences are summarized in Tables 5.3a and 5.3b.   cid:3  cid:3  cid:3    cid:3  cid:3   f  x + 3h   f  x + 4h   f  x + 5h   f  x + h  4 −5 18 −14  f  x + 2h  −1 4 −24 26   cid:3   x  2hf  cid:3  cid:3  h2 f  x   cid:3  cid:3  cid:3  2h3 f  x  −2 h4 f  4  x  Table 5.3a. Coefﬁcients of Forward Finite Difference Approximations of O h2   −1 14 −24  −3 11  f  x  −3 2 −5 3  f  x − 3h   f  x − 4h   f  x − 5h   f  x   cid:3   x  2hf 3  cid:3  cid:3  h2 f  x  2  cid:3  cid:3  cid:3  2h3 f  x  5 h4 f  4  x  3 Table 5.3b. Coefﬁcients of Backward Finite Difference Approximations of O h2   −1 −14 −24  3 11  f  x − 2h  1 4 24 26  f  x − h  −4 −5 −18 −14  −2  Errors in Finite Difference Approximations  Observe that in all ﬁnite difference expressions the sum of the coefﬁcients is zero. The effect on the roundoff error can be profound. If h is very small, the values of f  x , f  x ± h , f  x ± 2h , and so on, will be approximately equal. When they are multiplied by the coefﬁcients and added, several signiﬁcant ﬁgures can be lost. Yet we cannot make h too large, because then the truncation error would become excessive. This unfortunate situation has no remedy, but we can obtain some relief by taking the following precautions:   Use double-precision arithmetic.   Employ ﬁnite difference formulas that are accurate to at least O h2 .   188  Numerical Differentiation  To illustrate the errors, let us compute the second derivative of f  x  = e  −x at x = 1 from the central difference formula, Eq.  5.2 . We carry out the calculations with six- and eight-digit precision, using different values of h. The results, shown in Table 5.4, should be compared with f  −1 = 0.367 879 44.   1  = e   cid:3  cid:3   h  Six-Digit Precision  Eight-Digit Precision  0.64 0.32 0.16 0.08 0.04 0.02 0.01 0.005 0.0025 0.00125  0.380 610 0.371 035 0.368 711 0.368 281 0.368 75  0.37 0.38 0.40 0.48 1.28  0.380 609 11 0.371 029 39 0.368 664 84 0.368 076 56 0.367 831 25  0.3679 0.3679 0.3676 0.3680 0.3712  −x   cid:3  cid:3  Table 5.4.  e Approximation  at x = 1 from Central Finite Difference  In the six-digit computations, the optimal value of h is 0.08, yielding a result ac- curate to three signiﬁcant ﬁgures. Hence three signiﬁcant ﬁgures are lost due to a combination of truncation and roundoff errors. Above optimal h, the dominant er- ror is caused by truncation; below it, the roundoff error becomes pronounced. The best result obtained with the eight-digit computation is accurate to four signiﬁcant ﬁgures. Because the extra precision decreases the roundoff error, the optimal h is smaller  about 0.02  than in the six-ﬁgure calculations.  5.3  Richardson Extrapolation  Richardson extrapolation is a simple method for boosting the accuracy of certain nu- merical procedures, including ﬁnite difference approximations  we also use it later in other applications .  Suppose that we have an approximate means of computing some quantity G. Moreover, assume that the result depends on a parameter h. Denoting the approxi- mation by g h , we have G = g h  + E h , where E h  represents the error. Richard- son extrapolation can remove the error, provided that it has the form E h  = chp, c and p being constants. We start by computing g h  with some value of h, say h = h1. In that case we have  G = g h1  + chp Then we repeat the calculation with h = h2, so that G = g h2  + chp  1  2   i    j    189  5.3 Richardson Extrapolation  Eliminating c and solving for G, Eqs.  i  and  j  yield  G =  h1 h2 pg h2  − g h1    5.8  which is the Richardson extrapolation formula. It is common practice to use h2 = h1 2, in which case Eq.  5.8  becomes   h1 h2 p − 1  G = 2pg h1 2  − g h1   2p − 1   5.9   Let us illustrate Richardson extrapolation by applying it to the ﬁnite difference at x = 1. We work with six-digit precision and use the results approximation of  e in Table 5.4. Because the extrapolation works only on the truncation error, we must conﬁne h to values that produce negligible roundoff. In Table 5.4 we have  −x   cid:3  cid:3   g 0.64  = 0.380 610  g 0.32  = 0.371 035  The truncation error in central difference approximation is E h  = O h2  = c1h2 + c2h4 + c3h6 + . . . . Therefore, we can eliminate the ﬁrst  dominant  error term if we substitute p = 2 and h1 = 0.64 in Eq.  5.9 . The result is  G = 22g 0.32  − g 0.64  −x   cid:3  cid:3   22 − 1  = 4 0.371 035  − 0.380 610 with the error O h4 . Note that it is as accurate as  = 0. 367 84 3  3  which is an approximation of  e the best result obtained with eight-digit computations in Table 5.4.  EXAMPLE 5.1 Given the evenly spaced data points  x f  x   0  0.1  0.2  0.3  0.4   cid:3    cid:3  cid:3   0.0819  0.1341  0.1646   x  and f  0.0000  x  at x = 0 and 0.2 using ﬁnite difference approximations of  compute f O h2 . Solution. We will use ﬁnite difference approximations of O h2 . From the forward difference tables Table 5.3a we get  0.1797   0  = −3f  0  + 4f  0.1  − f  0.2    cid:3   f  = −3 0  + 4 0.0819  − 0.1341  = 0.967  2 0.1   0.2   0  = 2f  0  − 5f  0.1  + 4f  0.2  − f  0.3    cid:3  cid:3   f  = 2 0  − 5 0.0819  + 4 0.1341  − 0.1646  = −3.77   0.1 2   0.1 2   190  Numerical Differentiation  The central difference approximations in Table 5.1 yield   cid:3   f   0.2  = −f  0.1  + f  0.3   0.2  = f  0.1  − 2f  0.2  + f  0.3   2 0.1    cid:3  cid:3   f   0.1 2  = −0.0819 + 0.1646  = 0.4135  0.2  = 0.0819 − 2 0.1341  + 0.1646  = −2.17   0.1 2  EXAMPLE 5.2 Use the data in Example 5.1 to compute f   cid:3    0  as accurately as you can.  Solution. One solution is to apply Richardson extrapolation to ﬁnite difference ap- proximations. We start with two forward difference approximations of O h2  for f  0 : one using h = 0.2 and the other one using h = 0.1. Referring to the formulas of O h2  in Table 5.3a, we get   cid:3   0.4  2 0.2   = 0.8918  = 3 0  + 4 0.1341  − 0.1797  g 0.2  = −3f  0  + 4f  0.2  − f  0.4  g 0.1  = −3f  0  + 4f  0.1  − f  0.2  Recall that the error in both approximations is of the form E h  = c1h2 + c2h4 + c3h6 + . . .. We can now use Richardson extrapolation, Eq.  5.9 , to eliminate the dom- inant error term. With p = 2 we obtain  0  ≈ G = 22g 0.1  − g 0.2   = −3 0  + 4 0.0819  − 0.1341  = 4 0.9675  − 0.8918  = 0.9927  = 0.9675  2 0.1   0.2   cid:3   f  22 − 1  3  which is a ﬁnite difference approximation of O h4˙ .  EXAMPLE 5.3  B  a  α  A  b β  C  c  d  D  The linkage shown has the dimensions a = 100 mm, b = 120 mm, c = 150 mm, and d = 180 mm. It can be shown by geometry that the relationship between the angles α and β is  α  deg  β  rad   0  5  10  15  20  25  30  1.6595  1.5434  1.4186  1.2925  1.1712  1.0585  0.9561   191  5.4 Derivatives by Interpolation  If link A B rotates with the constant angular velocity of 25 rad s, use ﬁnite difference approximations of O h2  to tabulate the angular velocity dβ dt of link BC against α. Solution. The angular speed of BC is = dβ dα  = 25  dβ dα  dα dt  dβ dt  rad s  where dβ dα can be computed from ﬁnite difference approximations using the data in the table. Forward and backward differences of O h2  are used at the endpoints, and central differences elsewhere. Note that the increment of α is = 0.087 266 rad  h = cid:30    cid:31 1 π  rad   deg  5 deg  2  180  The computations yield −3β 0 ◦  ◦  ˙β 0    + 4β 5 ◦ 2h    = 25 = −32.01 rad s.   − β 0 ◦   = 25 2h  β 10  ◦  ◦ ˙β 5  etc.  The complete set of results is    − β 10  ◦     −3 1.6595  + 4 1.5434  − 1.4186  = 25  2  0.087 266      = 25  1.4186 − 1.6595 2 0.087 266   = −34.51 rad s  α  deg  ˙β  rad s  −32.01 −34.51 −35.94 −35.44 −33.52 −30.81 −27.86  20  25  15  10  30  5  0  5.4 Derivatives by Interpolation  If f  x  is given as a set of discrete data points, interpolation can be a very effective means of computing its derivatives. The idea is to approximate the derivative of f  x  by the derivative of the interpolant. This method is particularly useful if the data points are located at uneven intervals of x, when the ﬁnite difference approximations listed in Section 5.2 are not applicable1.  Polynomial Interpolant  The idea here is simple: Fit the polynomial of degree n  Pn−1 x  = a0 + a1x + a2x2 + ··· + anxn  through n + 1 data points and then evaluate its derivatives at the given x. As pointed out in Section 3.2, it is generally advisable to limit the degree of the polynomial to less than six in order to avoid spurious oscillations of the interpolant. Because these oscillations are magniﬁed with each differentiation, their effect can devastating.  1 It is possible to derive ﬁnite difference approximations for unevenly spaced data, but they would  not be as accurate as the formulas derived in Section 5.2.   192  Numerical Differentiation  In view of this limitation, the interpolation is usually a local one, involving no more than a few nearest-neighbor data points.  For evenly spaced data points, polynomial interpolation and ﬁnite difference approximations produce identical results. In fact, the ﬁnite difference formulas are equivalent to polynomial interpolation.  Several methods of polynomial interpolation were introduced in Section 3.2. Un- fortunately, none are suited for the computation of derivatives of the interpolant. The method that we need is one that determines the coefﬁcients a0, a1, . . . , an of the polynomial. There is only one such method discussed in Chapter 3: the least-squares ﬁt. Although this method is designed mainly for smoothing of data, it will carry out interpolation if we use m = n in Eq.  3.22 —recall that m is the degree of the inter- polating polynomial and n + 1 represents the number of data points to be ﬁtted. If the data contain noise, then the least-squares ﬁt should be used in the smoothing mode; that is, with m < n. After the coefﬁcients of the polynomial have been found, the polynomial and its ﬁrst two derivatives can be evaluated efﬁciently by the func- tion evalPoly listed in Section 4.7.  Cubic Spline Interpolant  Because of its stiffness, a cubic spline is a good global interpolant; moreover, it is easy to differentiate. The ﬁrst step is to determine the second derivatives ki of the spline at the knots by solving Eqs.  3.11 . This can be done with the function curvatures in the module cubicSpline listed in Section 3.3. The ﬁrst and second derivatives are then be computed from  i,i+1 x  = ki  cid:3  f 6  ’  3 x − xi+1 2 xi − xi+1 ’    −  xi − xi+1     −  xi − xi+1   − ki+1 6  3 x − xi 2 xi − xi+1 x − xi+1 xi − xi+1  i,i+1 x  = ki  cid:3  cid:3  f  − ki+1  x − xi xi − xi+1  + yi − yi+1 xi − xi+1   5.10    5.11   which are obtained by differentiation of Eq.  3.10 .  EXAMPLE 5.4 Given the data  1.5  1.9  2.1  2.4  2.6  3.1  1.0628  1.3961  1.5432  1.7349  1.8423  2.0397  x f  x    cid:3    cid:3  cid:3    2  and f  compute f  2  using  1  polynomial interpolation over three nearest- neighbor points, and  2  a natural cubic spline interpolant spanning all the data points.   193  5.4 Derivatives by Interpolation Solution of Part  1 . The interpolant is P2 x  = a0 + a1x + a2x2 passing through the points at x = 1.9, 2.1, and 2.4. The normal equations, Eqs.  3.22 , of the least-squares ﬁt are  ⎡ ⎢⎢⎣ n  cid:27   cid:27   xi x2 i   cid:27   cid:27   cid:27   xi x2 i x3 i   cid:27   cid:27   cid:27   ⎤ ⎥⎥⎦  ⎡ ⎢⎢⎣a0  a1 a2  x2 i x3 i x4 i  ⎤ ⎥⎥⎦ =  ⎡ ⎢⎢⎣  ⎤ ⎥⎥⎦   cid:27  yi cid:27  yixi cid:27   yix2 i  After substituting the data, we get  6.4  6.4  13.78  13.78  ⎤ ⎥⎥⎦  ⎤ ⎥⎥⎦ =  ⎡ ⎢⎢⎣ 3  cid:14   ⎡ ⎢⎢⎣ 4.6742  ⎡ ⎢⎢⎣a0  cid:15 T −0.7714 1.5075 −0.1930 2 x  = a1 + 2a2x and P  cid:3   13.78 29.944 65.6578  10.0571  21.8385  29.944  a1 a2  ⎤ ⎥⎥⎦  .  which yields a =  fore,  The derivatives of the interpolant are P   cid:3   f   2  ≈ P  2 2  = 1.5075 + 2 −0.1930  2  = 0.7355  cid:3    cid:3  cid:3   f   2  ≈ P  2  2  = 2 −0.1930  = −0.3860  cid:3  cid:3   2  x  = 2a2. There-  cid:3  cid:3   Solution of Part  2 . We must ﬁrst determine the second derivatives ki of the spline at its knots, after which the derivatives of f  x  can be computed from Eqs.  5.10  and  5.11 . The ﬁrst part can be carried out by the following small program:  ! usr bin python   example5_4  from cubicSpline import curvatures  from LUdecomp3 import *  import numpy as np  xData = np.array [1.5,1.9,2.1,2.4,2.6,3.1]   yData = np.array [1.0628,1.3961,1.5432,1.7349,1.8423, 2.0397]   print curvatures xData,yData    input "Press return to exit"   The output of the program, consisting of k0 to k5, is  [ 0.  -0.4258431 -0.37744139 -0.38796663 -0.55400477  0.  ]   Because x = 2 lies between knots 1 and 2, we must use Eqs.  5.10  and  5.11  with  194  Numerical Differentiation  i = 1. This yields   cid:3   f   2  ≈ f  ’    −  x1 − x2     −  x1 − x2   3 x − x2 2 x1 − x2  ’  1,2 2  = k1  cid:3  6 − k2 6 =  −0.4258  −  −0.3774  = 0.7351  6  6  3 x − x1 2 ’ x1 − x2 3 2 − 2.1 2 ’  −0.2  3 2 − 1.9 2  −0.2   + y1 − y2   x1 − x2 −  −0.2    −  −0.2   + 1.3961 − 1.5432   −0.2    cid:3  cid:3   f   2  ≈ f  1,2 2  = k1  cid:3  cid:3  =  −0.4258   cid:3   x − x2 x1 − x2 2 − 2.1  −0.2   x − x1 − k2 x1 − x2 −  −0.3774   2 − 1.9  −0.2   = −0. 4016   cid:3  cid:3   Note that the solutions for f  2  in parts  1  and  2  differ only in the fourth signif- icant ﬁgure, but the values of f  2  are much farther apart. This is not unexpected, considering the general rule: The higher the order of the derivative, the lower the pre- cision with which it can be computed. It is impossible to tell which of the two results is better without knowing the expression for f  x . In this particular problem, the data points fall on the curve f  x  = x2e −x 2, so that the “true” values of the derivatives are  2  = 0.7358 and f  2  = −0.3679.  cid:3  f   cid:3  cid:3   EXAMPLE 5.5  cid:3  Determine f   0  and f   1  from the following noisy data:   cid:3   x f  x  x f  x   1.9934  2.1465  2.2129  2.1790  0  0.8  0.2  1.0  0.4  1.2  0.6  1.4  2.0683  1.9448  1.7655  1.5891  Solution. We used the program listed in Example 3.10 to ﬁnd the best polynomial ﬁt  in the least-squares sense  to the data. The program was run three times with the following results:  Degree of polynomial ==> 2  Coefficients are:  [ 2.0261875  0.64703869 -0.70239583]  Std. deviation = 0.0360968935809  Degree of polynomial ==> 3  Coefficients are:  [ 1.99215  1.09276786 -1.55333333  0.40520833]   195  5.4 Derivatives by Interpolation  Std. deviation = 0.0082604082973  Degree of polynomial ==> 4  Coefficients are:  [ 1.99185568  1.10282373 -1.59056108  0.44812973 -0.01532907]  Std. deviation = 0.00951925073521  Based on standard deviation, the cubic seems to be the best candidate for the interpolant. Before accepting the result, we compare the plots of the data points and the interpolant—see the following ﬁgure. The ﬁt does appear to be satisfactory.    x   f  2.3 2.2 2.1 2.0 1.9 1.8 1.7 1.6 1.5  0.00  0.20  0.40  0.60  0.80  1.00  1.20  1.40  x  Approximating f  x  by the interpolant, we have  f  x  ≈ a0 + a1x + a2x2 + a3x3   cid:3    x  ≈ a1 + 2a2x + 3a3x2  f  so that  Therefore,   cid:3    cid:3   f  f   0  ≈ a1 = 1.093  1  = a1 + 2a2 + 3a3 = 1.093 + 2 −1.553  + 3 0.405  = −0. 798  x  = cid:28   In general, derivatives obtained from noisy data are at best rough approxima- tions. In this problem, the data represent f  x  =  x + 2   cosh x with added random   cosh x, so that the “correct” derivatives are noise. Thus f  0  = 1.000 and f f PROBLEM SET 5.1 1. Given the values of f  x  at the points x, x − h1, and x + h2, where h1  cid:7 = h2, de-  x . What is the order of the  1 −  x + 2  tanh x  1  = −0.833.  termine the ﬁnite difference approximation for f truncation error?   cid:29    cid:3  cid:3    cid:3    cid:3    cid:3    196  Numerical Differentiation   cid:3  cid:3  cid:3    cid:29  cid:3    x  = cid:28   2. Given the ﬁrst backward ﬁnite difference approximations for f  cid:3  cid:3  cid:3    x , de- rive the ﬁrst backward ﬁnite difference approximation for f  x  using the oper- ation f  x  accurate to O h4  by apply- ing Richardson extrapolation to the central difference approximation of O h2 .  x  from the  4. Derive the second forward ﬁnite difference approximation for f  3. Derive the central difference approximation for f   x  and f   x    cid:3  cid:3  cid:3    cid:3  cid:3    cid:3  cid:3   f  .   cid:3    cid:3  cid:3   5. Derive the ﬁrst central difference approximation for f  4  x  from the Taylor series. 6. Use ﬁnite difference approximations of O h2  to compute f  2.36    2.36  and f   cid:3  cid:3    cid:3   Taylor series.  from the following data:  x f  x   cid:3  cid:3    cid:3   2.36  2.37  2.38  2.39  0.85866  0.86289  0.86710  0.87129  7. Estimate f   1  and f   1  from the following data:  x f  x   0.97  1.00  1.05  0.85040  0.84147  0.82612  8. Given the data  x f  x   cid:3  cid:3   x f  x   0.84  0.92  1.00  1.08  1.16  0.431711  0.398519  0.367879  0.339596  0.313486  calculate f   1  as accurately as you can.  cid:3   9. Use the data in the table to compute f   0.2  as accurately: as possible:  0  0.1  0.2  0.3  0.4  0.000 000  0.078 348  0.138 910  0.192 916  0.244 981  10. Using ﬁve signiﬁcant ﬁgures in the computations, determine d sin x  dx at x = 0.8 from  a  the ﬁrst forward difference approximation and  b  the ﬁrst central difference approximation. In each case, use h that gives the most accurate result  this requires experimentation .  11.  cid:2  Use polynomial interpolation to compute f   cid:3   and f   cid:3  cid:3   at x = 0, using the data  x f  x   −2.2 15.180  −0.3 10.962  0.8 1.9 1.920 −2.040  Given that f  x  = x3 − 0. 3x2 − 8. 56x + 8. 448, gauge the accuracy of the result.  12.  cid:2   B  R  θ  A  2.5R  x  C  The crank A B of length R = 90 mm is rotating at a constant angular speed of dθ  dt = 5 000 rev min. The position of the piston C can be shown to vary with   197  5.4 Derivatives by Interpolation  the angle θ as  %  x = R  cos θ +   cid:26  2.52 − sin2 θ  &  Write a program that plots the acceleration of the piston at θ = 0 ◦ 180 . Use numerical differentiation to compute the acceleration.  ◦  ◦  ◦  , 5  , 10  , . . . ,  13.  cid:2   The radar stations A and B, separated by the distance a = 500 m, track the plane C by recording the angles α and β at one-second intervals. If three successive readings are  t  s   α  β  9  ◦ 54.80 ◦ 65.59  10 ◦ 54.06 ◦ 64.59  11 53.34 63.62  ◦ ◦  calculate the speed v of the plane and the climb angle γ at t = 10 s. The coordi- nates of the plane can be shown to be  x = a  tan β  tan β − tan α  y = a  tan α tan β tan β − tan α  14.  cid:2   v  γ  C  y  A  B  α β a  x  20  D  β C  190  7  0  Dimensions in mm 190 A θα  60  B  Geometric analysis of the linkage shown resulted in the following table relating the angles θ and β:  θ  deg  β  deg   0  30  60  59.96  56.42  44.10  120  90 150 25.72 −0.27 −34.29   198  Numerical Differentiation  Assuming that member A B of the linkage rotates with the constant angular ve- locity dθ  dt = 1 rad s, compute dβ dt in rad s at the tabulated values of θ. Use cubic spline interpolation.  15.  cid:2  The relationship between stress σ and strain ε of some biological materials in  uniaxial tension is  where a and b are constants  dσ  dε is called tangent modulus . The following table gives the results of a tension test on such a material:  = a + bσ  dσ dε  Strain ε  Stress σ  MPa   0  0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50  0  0.252 0.531 0.840 1.184 1.558 1.975 2.444 2.943 3.500 4.115  Determine the parameters a and b by linear regression.   6 Numerical Integration  3  Compute  b a f  x  dx, where f  x  is a given function.  Numerical integration, also known as quadrature, is intrinsically a much more accu- rate procedure than numerical differentiation. Quadrature approximates the deﬁnite integral  6.1  Introduction  by the sum  4  b  a  I = n cid:6   i=0  f  x  dx  Ai f  xi   where the nodal abscissas xi and weights Ai depend on the particular rule used for the quadrature. All rules of quadrature are derived from polynomial interpolation of the integrand. Therefore, they work best if f  x  can be approximated by a polynomial.  Methods of numerical integration can be divided into two groups: Newton-Cotes formulas and Gaussian quadrature. Newton-Cotes formulas are characterized by equally spaced abscissas and include well-known methods such as the trapezoidal rule and Simpson’s rule. They are most useful if f  x  has already been computed at equal intervals or can be computed at low cost. Because Newton-Cotes formulas are based on local interpolation, they require only a piecewise ﬁt to a polynomial.  In Gaussian quadrature the locations of the abscissas are chosen to yield the best possible accuracy. Because Gaussian quadrature requires fewer evaluations of the in- tegrand for a given level of precision, it is popular in cases where f  x  is expensive to evaluate. Another advantage of Gaussian quadrature is its ability to handle integrable singularities, enabling us to evaluate expressions such as  4  g x √ 1 − x2 provided that g x  is a well-behaved function.  1  0  dx  199   200  Numerical Integration  6.2 Newton-Cotes Formulas  f x   P  x  n  h  x1  x2  x3  x0 a  xn-1 xn b  x  Consider the deﬁnite integral  4  b  Figure 6.1. Polynomial approximation of f  x .  f  x  dx   6.1  We divide the range of integration  a, b  into n equal intervals of length h =  b − a  n, as shown in Figure 6.1, and denote the abscissas of the resulting nodes by x0, x1, . . . , xn. Next we approximate f  x  by a polynomial of degree n that intersects all the nodes. Lagrange’s form of this polynomial, Eq.  3.1a , is  a  Pn x  = n cid:6   i=0  f  xi  cid:12 i x   where  cid:12 i x  are the cardinal functions deﬁned in Eq.  3.1b . Therefore, an approxima- tion to the integral in Eq.  6.1  is  4  b  a  +  = n cid:6   i=0  f  xi    cid:12 i x dx  Ai f  xi    6.2a   where  4  b  a  I =  *  Pn x dx = n cid:6  4  i=0  Ai =  b  a   cid:12 i x dx,  i = 0, 1, . . . , n   6.2b   Equations  6.2  are the Newton-Cotes formulas. Classical examples of these formulas are the trapezoidal rule  n = 1 , Simpson’s rule  n = 2 , and 3 8 Simpson’s rule  n = 3 . The most important of these formulas is the trapezoidal rule. It can be combined with Richardson extrapolation into an efﬁcient algorithm known as Romberg integration, which makes the other classical rules somewhat redundant.  Trapezoidal Rule If n = 1  one panel , as illustrated in Figure 6.2, we have  cid:12 0 =  x − x1   x0 − x1  = − x − b  h. Therefore,  4   cid:30    cid:31   b  x − b  A 0 = 1 h  dx = 1 2h Also  cid:12 1 =  x − x0   x1 − x0  =  x − a  h, so that  x − a  dx = 1 2h  A 1 = 1 h  4  a  b  a   b − a 2 = h 2   b − a 2 = h 2   201  6.2 Newton-Cotes Formulas  f x   E  Area =I  h  0x  = a  x  x  = b 1  Figure 6.2. Trapezoidal rule.  Substitution in Eq.  6.2a  yields  I = cid:28    cid:29  h f  a  + f  b   2  The error in the trapezoidal rule E =  b  f  x dx − I  4  a  which is known as the trapezoidal rule. It represents the area of the trapezoid in Figure 6.2.  is the area of the region between f  x  and the straight-line interpolant, as indicated in Figure 6.2. It can be obtained by integrating the interpolation error in Eq.  3.3 :  4  E = 1 2! = − 1 12  a  4  a  b   x − x0  x − x1 f   cid:3  cid:3    ξ dx = 1 2   cid:3  cid:3   f   ξ   b   x − a  x − b dx   b − a 3 f   cid:3  cid:3    ξ  = − h3 12   cid:3  cid:3   f   ξ   Composite Trapezoidal Rule  In practice the trapezoidal rule is applied in a piecewise fashion. Figure 6.3 shows the region  a, b  divided into n panels, each of width h. The function f  x  to be integrated is approximated by a straight line in each panel. From the trapezoidal rule we obtain for the approximate area of a typical  ith  panel,   6.3    6.4    cid:29  h f  xi  + f  xi+1   2  Ii = cid:28  3  Hence total area, representing  b a f  x  dx, is  Ii = cid:28    cid:29  h f  x0  + 2f  x1  + 2f  x2  + . . . + 2f  xn−1  + f  xn   I = n−1 cid:6   i=0   6.5   2  which is the composite trapezoidal rule.  f x   iI h xi xi + 1  xn - 1  xxn b  x0 x1 a  Figure 6.3. Composite trapezoidal rule.   202  Numerical Integration  The truncation error in the area of a panel is–see Eq.  6.4 —  where ξi lies in  xi, xi+1 . Hence the truncation error in Eq.  6.5  is   a   But   cid:3  cid:3  where ¯f must be a point ξ in  a, b  at which f   cid:3  cid:3    ξ  = ¯f  cid:3  cid:3   is the arithmetic mean of the second derivatives. If f   x  is continuous, there  , enabling us to write   cid:3  cid:3   Ei = − h3 12   cid:3  cid:3   f   ξi   n−1 cid:6   i=0  Ei = − h3 12   cid:3  cid:3   f   ξ  i    cid:3  cid:3    ξi  = n ¯f  cid:3  cid:3   f  E = n−1 cid:6  n−1 cid:6   i=0  i=0  n−1 cid:6   i=0   cid:3  cid:3    ξi  = nf  f   cid:3  cid:3    ξ  = b − a   cid:3  cid:3   f   ξ   h  Therefore, Eq.  a  becomes  E = −  b − a h2   cid:3  cid:3    6.6  It would be incorrect to conclude from Eq.  6.6  that E = ch2  c being a constant ,  ξ  is not entirely independent of h. A deeper analysis of the error1 shows   ξ   12   cid:3  cid:3   f  because f that if f  x  and its derivatives are ﬁnite in  a, b , then  E = c1h2 + c2h4 + c3h6 + . . .   6.7   Recursive Trapezoidal Rule Let Ik be the integral evaluated with the composite trapezoidal rule using 2k−1 panels. Note that if k is increased by one, the number of panels is doubled. Using the notation  H = b − a  Eq.  6.5  yields the following results for k = 1, 2, and 3.  cid:29  H  k = 1  one panel :  I1 = cid:28  &  % a + H 2  2  f  a  + f  b     + f  b   H 4  = 1 2  I1 + f  &  % a + H 2  H 2   6.8   k = 2  two panels :  ’  I2 =  f  a  + 2f  1 The analysis requires familiarity with the Euler-Maclaurin summation formula, which is covered  in advanced texts.   203  6.2 Newton-Cotes Formulas  ’  I3 =  k = 3  four panels : f  a  + 2f ’ % a + H 4  % a + H & 4  = 1 2  I2 +  f  % a + 3H 4  &    + f  b   H 8  &  + 2f  % & a + H &  2  + 2f % a + 3H 4 ’ a +  2i − 1 H 2k−1  H 4  + f 2k−2 cid:6   f  i=1     We can now see that for arbitrary k > 1 we have  Ik = 1 2  Ik−1 + H 2k−1  , k = 2, 3, . . .   6.9a   which is the recursive trapezoidal rule. Observe that the summation contains only the new nodes that were created when the number of panels was doubled. Therefore, the computation of the sequence I1, I2, I3, . . . , Ik from Eqs.  6.8  and  6.9  involves the same amount of algebra as the calculation of Ik directly from Eq.  6.5 . The advantage of using the recursive trapezoidal rule is that it allows us to monitor convergence and terminate the process when the difference between Ik−1 and Ik becomes sufﬁciently small. A form of Eq.  6.9a  that is easier to remember is   cid:6   I  h  = 1 2  I  2h  + h  f  xnew    6.9b   where h = H n is the width of each panel.   cid:2  trapezoid  3 The function trapezoid computes Ik  Inew , given Ik−1  Iold  using Eqs.  6.8  and a f  x  dx by calling trapezoid with k = 1, 2, . . . until the  6.9 . We can compute desired precision is attained.  b   module trapezoid  ’’’ Inew = trapezoid f,a,b,Iold,k .  Recursive trapezoidal rule:  old = Integral of f x  from x = a to b computed by  trapezoidal rule with 2ˆ k-1  panels.  Inew = Same integral computed with 2ˆk panels.  ’’’  def trapezoid f,a,b,Iold,k :  if k == 1:Inew =  f a  + f b  * b - a  2.0  else:  n = 2** k -2     Number of new points  h =  b - a  n   Spacing of new points  x = a + h 2.0  sum = 0.0  for i in range n :  sum = sum + f x   x = x + h  Inew =  Iold + h*sum  2.0  return Inew   204  Numerical Integration  Simpson’s Rules  f x   Parabola  ξ h  h  x  Figure 6.4. Simpson’s 1 3 rule.   a    b   x1  x  = b 2  x = a0 Simpson’s 1 3 rule can be obtained from Newton-Cotes formulas with n = 2; that is, by passing a parabolic interpolant through three adjacent nodes, as shown in Figure 6.4. The area under the parabola, which represents an approximation of b a f  x  dx, is  see derivation in Example 6.1   3  ’  I =  f  a  + 4f  %  &    + f  b   h 3  a + b 2  f x   yields  h  h  Figure 6.5. Composite Simpson’s 1 3 rule.  x0 a  xi  xi + 1 xi + 2  xn b  To obtain the composite Simpson’s 1 3 rule, the integration range  a, b  is divided into n panels  n even  of width h =  b − a  n each, as indicated in Figure 6.5. Apply- ing Eq.  a  to two adjacent panels, we have  4  xi+2  xi   cid:28    cid:29  h f  xi  + 4f  xi+1  + f  xi+2   f  x  dx ≈  4  xm  x0  f  x  dx = n cid:6   ’4  i=0,2,...  xi  3     xi+2  f  x dx  b  f  x dx =  Substituting Eq.  b  into  4  a  4  a  b  f  x  dx ≈ I = [f  x0  + 4f  x1  + 2f  x2  + 4f  x3  + . . .   6.10   ··· + 2f  xn−2  + 4f  xn−1  + f  xn ]  h 3  The composite Simpson’s 1 3 rule in Eq.  6.10  is perhaps the best known method of numerical integration. However, its reputation is somewhat undeserved, because the trapezoidal rule is more robust and Romberg integration is more efﬁcient.  The error in the composite Simpson’s rule is  E =  b − a h4  180  f  4  ξ    6.11    205  6.2 Newton-Cotes Formulas  from which we conclude that Eq.  6.10  is exact if f  x  is a polynomial of degree three or less.  Simpson’s 1 3 rule requires the number of panels n to be even. If this condition is not satisﬁed, we can integrate over the ﬁrst  or last  three panels with Simpson’s 3 8 rule,  I = cid:28   f  x0  + 3f  x1  + 3f  x2  + f  x3    6.12    cid:29  3h  8  and use Simpson’s 1 3 rule for the remaining panels. The error in Eq.  6.12  is of the same order as in Eq.  6.10 .  EXAMPLE 6.1 Derive Simpson’s 1 3 rule from Newton-Cotes formulas. a, x1 = cid:30  Solution. Referring to Figure 6.4, Simpson’s 1 3 rule uses three nodes located at x0 =  2, and x2 = b. The spacing of the nodes is h =  b − a  2. The cardinal  a + b   cid:31   functions of Lagrange’s three-point interpolation are—see Section 3.2—   cid:12 0 x  =  x − x1  x − x2   x0 − x1  x0 − x2    cid:12 1 x  =  x − x0  x − x2   x1 − x0  x1 − x2    cid:12 2 x  =  x − x0  x − x1   x2 − x0  x2 − x1   b a   cid:12 i ξ dξ. Therefore,  becomes Ai =3  cid:12 i x  =3 The integration of these functions is easier if we introduce the variable ξ with origin at x1. Then the coordinates of the nodes are ξ 0 = −h, ξ 1 = 0 and ξ 2 = h and Eq.  6.2b  4 A 0 = 4 4  h−h  ξ − 0  ξ − h   −h  −2h   ξ + h  ξ − h   4 4 4   h  −h   A 1 =  h −h  h −h  h −h   ξ 2 − hξ dξ = h 3  ξ 2 − h2 dξ = 4h h −h 3  ξ 2 + hξ dξ = h 3  dξ = 1 2h2 dξ = − 1 h2 dξ = 1 2h2   ξ + h  ξ − 0   A 2 =   2h  h   h −h  h −h  Equation  6.2a  then yields  I = 2 cid:6   i=0  ’  Ai f  xi  =  f  a  + 4f  %  &    + f  b   h 3  a + b 2  which is Simpson’s 1 3 rule.  EXAMPLE 6.2 Evaluate the bounds on 8 panels; and  2  16 panels.  3 π  0 sin x  dx with the composite trapezoidal rule using  1    206  Numerical Integration Solution of Part  1 . With eight panels there are nine nodes spaced at h = π  8. The abscissas of the nodes are xi = iπ  8, i = 0, 1, . . . , 8. From Eq.  6.5  we get  +  I =  sin 0 + 2  sin  iπ 8  + sin π  π 16  = 1.97423  7 cid:6   i=1  The error is given by Eq.  6.6 :  E = −  b − a h2   cid:3  cid:3    ξ  = −  π − 0  π  8 2  f   − sin ξ  = π 3 768  sin ξ  12  where 0 < ξ < π. Because we do not know the value of ξ, we cannot evaluate E, but we can determine its bounds:  *  12  Therefore, I + Emin <  Emin = π 3 3 π 768 0 sin x  dx < I + Emax, or  sin 0  = 0 4 π  Emax = π 3 768  sin  π 2  = 0.040 37  1.974 23 <  sin x  dx < 2.014 60  0  The exact integral is, of course, 2.  Solution of Part  2 . The new nodes created by the doubling of panels are located at the midpoints of the old panels. Their abscissas are x j = π  16 + j π  8 =  1 + 2j  π  16,  j = 0, 1, . . . , 7  Using the recursive trapezoidal rule in Eq.  6.9b , we get  1 + 2j  π  7 cid:6   I = 1.974 23  2  + π 16  sin  j=0  16  = 1. 993 58  and the bounds on the error become  note that E is quartered when h is halved  Emin = 0, Emax = 0.040 37 4 = 0.010 09. Hence  4 π  0  1.993 58 <  sin x  dx < 2.003 67  f  x  dx from the following data:  x f  x   0  0.5  1.0  1.5  2.0  2.5  1.5000  2.0000  2.0000  1.6364  1.2500  0.9565  Solution. We use Simpson’s rules because they are more accurate than the trape- zoidal rule. Because the number of panels is odd, we compute the integral over the ﬁrst three panels by Simpson’s 3 8 rule, and use the 1 3 rule for the last two panels:  3  EXAMPLE 6.3 2.5 Estimate 0  I =  cid:28  f  0  + 3f  0.5  + 3f  1.0  + f  1.5   cid:29  0.5 + cid:28  f  1.5  + 4f  2.0  + f  2.5  = 2.8381 + 1.2655 = 4.1036  3   cid:29  3 0.5   8   207  6.3 Romberg Integration  EXAMPLE 6.4 Use the recursive trapezoidal rule to evaluate How many panels are needed to achieve this result?  0  √  3 π  x cos x dx to six decimal places.  Solution  ! usr bin python   example6_4  import math  from trapezoid import *  def f x : return math.sqrt x *math.cos x   Iold = 0.0  for k in range 1,21 :  Inew = trapezoid f,0.0,math.pi,Iold,k   if  k > 1  and  abs Inew - Iold   < 1.0e-6: break  Iold = Inew  print "Integral =",Inew   print "nPanels =",2** k-1    input "\nPress return to exit"   The output from the program is  Integral = -0.8948316648532865  nPanels = 32768  3 π  √  0  Hence  x cos x dx = −0.894 832 requiring 32 768 panels. The slow conver- gence is the result of all the derivatives of f  x  being singular at x = 0. Consequently, the error does not behave as shown in Eq.  6.7 : E = c1h2 + c2h4 + ··· , but is unpre- √ this case, we introduce t = √ dictable. Difﬁculties of this nature can often be remedied by a change in variable. In x, so that dt = dx  2 x  = dx  2t , or dx = 2t dt. Thus 4 π 4 √ √ x cos x dx =  2t 2 cost 2dt  π  0  0  Evaluation of the integral on the right-hand-side was completed with 4096 panels.  6.3  Romberg Integration  Romberg integration combines the trapezoidal rule with Richardson extrapolation  see Section 5.3 . Let us ﬁrst introduce the notation  b where, as before, Ii represents the approximate value of a f  x dx computed by the recursive trapezoidal rule using 2i−1 panels. Recall that the error in this  Ri,1 = Ii  3   208  Numerical Integration approximation is E = c1h2 + c2h4 + . . . , where h = b − a 2i−1  is the width of a panel. Romberg integration starts with the computation of R1,1 = I1  one panel  and R2,1 = I2  two panels  from the trapezoidal rule. The leading error term c1h2 is then eliminated by Richardson extrapolation. Using p = 2  the exponent in the leading error term  in Eq.  5.9  and denoting the result by R2,2, we obtain  It is convenient to store the results in an array of the form  R2,2 = 22 R2,1 − R1,1  22−1 *  R1,1  R2,1 − 1 = 4 3 3 +  R1,1 R2,1 R2,2  The next step is to calculate R3,1 = I3  four panels  and repeat Richardson extrap-  olation with R2,1 and R3,1, storing the result as R3,2:   a    b   The elements of array R calculated so far are  Both elements of the second column have an error of the form c2h4, which can also be eliminated with Richardson extrapolation. Using p = 4 in Eq.  5.9 , we get  R3,3 = 24 R3,2 − R2,2  = 16 15  R2,2   c   This result has an error of O h6 . The array has now expanded to  R2,1  R3,2 = 4 3 ⎡ ⎢⎣ R1,1  R3,1 − 1 3 ⎤ ⎥⎦  R2,1 R2,2 R3,1 R3,2,  24−1  ⎡ ⎢⎣ R1,1  R3,2 − 1 15 ⎤ ⎥⎦  ⎤ ⎥⎥⎥⎦  R2,1 R2,2 R3,1 R3,2 R3,3  ⎡ ⎢⎢⎢⎣  R1,1 R2,1 R2,2 R3,1 R3.2 R3,3 R4,1 R4,2 R4,3 R4,4  After another round of calculations we get  where the error in R4,4 is O h8 . Note that the most accurate estimate of the integral is always the last diagonal term of the array. This process is continued until the differ- ence between two successive diagonal terms becomes sufﬁciently small. The general   209  6.3 Romberg Integration  extrapolation formula used in this scheme is  Ri, j = 4j−1 Ri, j−1 − Ri−1, j−1  4j−1 − 1  A pictorial representation of Eq.  6.13a  is  ,  i > 1,  j = 2, 3, . . . , i   6.13a   Ri−1, j−1  cid:18   α   cid:18   Ri, j−1 → β → Ri, j   6.13b    6.13c   where the multipliers α and β depend on j in the following manner:  2  3  j α −1 3 −1 15 −1 63 −1 255 256 255  64 63  16 15  4 3  5  4  β  6  −1 1023 1024 1023  The triangular array is convenient for hand computations, but computer imple- mentation of the Romberg algorithm can be carried out within a one-dimensional  cid:3  array R . After the ﬁrst extrapolation—see Eq.  a — R1,1 is never used again, so that it can be replaced with R2,2. As a result, we have the array  *  +   cid:3  R 1  cid:3  R 2  = R2,2 = R2,1  In the second extrapolation round, deﬁned by Eqs.  b  and  c , R3,2 overwrites R2,1, and R3,3 replaces R2,2, so that the array contains = R3,3 = R3,2 = R3,1  ⎡ ⎢⎣ R  ⎤ ⎥⎦   cid:3  1  cid:3  R 2  cid:3  R 3   cid:3  1 always contains the best current result. The extrapola-  j = k − 1, k − 2, . . . , 1   6.14   and so on. In this manner, R tion formula for the kth round is  cid:3  j+1  = 4k−j R   cid:3  j  R  − R 4k−j − 1   cid:3  j  ,   cid:2  romberg  The algorithm for Romberg integration is implemented in the function romberg. It returns the integral and the number of panels used. Richardson’s extrapolation is car- ried out by the subfunction richardson.   module romberg  ’’’ I,nPanels = romberg f,a,b,tol=1.0e-6 .  Romberg integration of f x  from x = a to b.  Returns the integral and the number of panels used.  ’’’   210  Numerical Integration  import numpy as np  from trapezoid import *  def romberg f,a,b,tol=1.0e-6 :  def richardson r,k :  for j in range k-1,0,-1 :  const = 4.0** k-j   r[j] =  const*r[j+1] - r[j]   const - 1.0   return r  r = np.zeros 21   r[1] = trapezoid f,a,b,0.0,1   r_old = r[1]  for k in range 2,21 :  r[k] = trapezoid f,a,b,r[k-1],k   r = richardson r,k   if abs r[1]-r_old  < tol*max abs r[1] ,1.0 :  return r[1],2** k-1   r_old = r[1]  print "Romberg quadrature did not converge"   EXAMPLE 6.5 Show that Rk,2 in Romberg integration is identical to composite Simpson’s 1 3 rule in Eq.  6.10  with 2k−1 panels. Solution. Recall that in Romberg integration Rk,1 = Ik denoted the approximate in- tegral obtained by the composite trapezoidal rule with n = 2k−1 panels. Denoting the abscissas of the nodes by x0, x1, . . . , xn, we have from the composite trapezoidal rule in Eq.  6.5   *  n−1 cid:6   i=1  Rk,1 = Ik =  f  x0  + 2  f  xi  + f  xn ,  +  h 2  When we halve the number of panels  panel width 2h , only the even-numbered ab- scissas enter the composite trapezoidal rule, yielding  Rk−1,1 = Ik−1 =  ⎡ ⎣f  x0  + 2  n−2 cid:6   i=2,4,...  ⎤ ⎦ h f  xi  + f  xn   Applying Richardson extrapolation yields  Rk,2 = 4 Rk,1 − 1 ⎡ 3 3 ⎣ 1  =  f  x0  + 4 3  3  Rk−1,1  n−1 cid:6   i=1,3,...  n−2 cid:6   i=2,4,...  f  xi  + 2 3  f  xi  + 1 3  f  xn   ⎤ ⎦ h  which agrees with Eq.  6.10 .   211  6.3 Romberg Integration  EXAMPLE 6.6 Use Romberg integration to evaluate decimal places.  Solution. From the recursive trapezoidal rule in Eq.  6.9b  we get  3 π 0 f  x  dx, where f  x  = sin x. Work with four  cid:29  = 0 f  π  2  = 1.5708  cid:28   cid:28   f  π  4  + f  3π  4   cid:29  f  π  8  + f  3π  8  + f  5π  8  + f  7π  8    cid:29  = 1.8961   cid:28   R1,1 = I  π  = π 2 R2,1 = I  π  2  = 1 2 R3,1 = I  π  4  = 1 2 R4,1 = I  π  8  = 1 2  f  0  + f  π  I  π  + π 2 I  π  2  + π 4 I  π  4  + π 8  = 1.9742  Using the extrapolation formulas in Eqs.  6.13 , we can now construct the following table:  ⎡ ⎢⎢⎢⎣  R1,1 R2,1 R2,2 R3,1 R3.2 R3,3 R4,1 R4,2 R4,3 R4,4  ⎤ ⎥⎥⎥⎦ =  ⎡ ⎢⎢⎢⎣  0  1.5708 2.0944 1.8961 2.0046 1.9986 1.9742 2.0003 2.0000 2.0000  3 π 0 sin x dx = R4,4 = 2.0000,  2x2 cos x2 dx and compare the results with  It appears that the procedure has converged. Therefore, which is, of course, the correct result.  EXAMPLE 6.7 Use Romberg integration to evaluate Example 6.4.  3 √  0  π  ⎤ ⎥⎥⎥⎦  Solution  !usr bin python   example6_7  import math  from romberg import *  def f x : return 2.0* x**2 *math.cos x**2   I,n = romberg f,0,math.sqrt math.pi    print "Integral =",I   print "numEvals =",n   input "\nPress return to exit"   The results of running the program are  Integral = -0.894831469504  nPanels = 64   212  Numerical Integration  It is clear that Romberg integration is considerably more efﬁcient than the trape- zoidal rule—it required only 64 panels as compared to 4096 panels for the trapezoidal rule in Example 6.4.  3 π  4  0  PROBLEM SET 6.1  1. Use the recursive trapezoidal rule to evaluate  ln 1 + tan x dx. Explain the  results. 2. The following table shows the power P supplied to the driving wheels of a car as a function of the speed v. If the mass of the car is m = 2000 kg, determine the time  cid:9 t it takes for the car to accelerate from 1 m s to 6 m s. Use the trapezoidal rule for integration. Hint:  cid:9 t = m 6s 1s  v P  dv, which can be derived from Newton’s law F = m dv dt  and the deﬁnition of power P = Fv.  3  0 0  v  m s  P  kW   3 1−1 cos 2 cos 3 ∞ 1  1 + x4   1.0 4.7  1.8 12.2  2.4 19.0  3.5 31.8  4.4 40.1  5.1 43.8  6.0 43.2  3. Evaluate  −1 x dx with Simpson’s 1 3 rule using two, four, and six  panels. Explain the results.  −1dx with the trapezoidal rule using ﬁve panels and com- 4. Determine pare the result with the “exact” integral 0.243 75. Hint: Use the transformation x3 = 1 t.  5.  F  x  The following table gives the pull F of the bow as a function of the draw x. If the bow is drawn 0.5 m, determine the speed of the 0.075-kg arrow when it leaves the bow. Hint: The kinetic energy of the arrow equals the work done in drawing the  bow; that is, mv2 2 =3  0.5m 0  F dx.  x  m  F  N  x  m  F  N   0.00 0 0.30 185  0.05 37 0.35 207  0.10 71 0.40 225  0.15 104 0.45 239  0.20 134 0.50 250  0.25 161  3  2 0   cid:30    cid:31  x5 + 3x3 − 2  6. Evaluate  dx by Romberg integration.   213  6.3 Romberg Integration  3 π  7. Estimate ing data:  8. Evaluate  0 f  x  dx as accurately as possible, where f  x  is deﬁned by the follow-  x f  x   0  π  4  π  2  1.0000  0.3431  0.2500  3π  4 0.3431  π  1.0000  4  1  sin x√ x  0  dx  9. Newton-Cotes formulas for evaluating  with Romberg integration. Hint: Use transformation of the variable to eliminate the singularity at x = 0. b a f  x  dx were based on polynomial ap- proximations of f  x . Show that if y = f  x  is approximated by a natural cubic spline with evenly spaced knots at x0, x1, . . . , xn, the quadrature formula be- comes  3   cid:30    cid:31   I = h 2 − h3 24  y0 + 2y1 + 2y2 + ··· + 2yn−1 + yn  cid:30  k0 + 2k1 + k2 + ··· + 2kn−1 + kn   cid:31   where h is the distance between the knots and ki = y  cid:3  cid:3  i . Note that the ﬁrst part is the composite trapezoidal rule; the second part may be viewed as a “correction” for curvature.  10.  cid:2  Evaluate  with Romberg integration. Hint: Use the transformation sin x = t 2.  √ 11.  cid:2  The period of a simple pendulum of length L is τ = 4  L gh θ 0 , where g is the  gravitational acceleration, θ 0 represents the angular amplitude, and  4 π  2  0   cid:26   h θ 0  =  dθ  1 − sin2 θ 0 2  sin2 θ  ◦ Compute h 15  the approximation used for small amplitudes .  ◦  , h 30   , and h 45  ◦   , and compare these values with h 0  = π  2  12.  cid:2   4 π  4  0  dx√ sin x  q  r  a  P  The ﬁgure shows an elastic half-space that carries uniform loading of inten- sity q over a circular area of radius a. The vertical displacement of the surface   214  Numerical Integration  at point P can be shown to be w r   = w0  4 π  2  0   cid:26   cos2 θ   r  a 2 − sin2 θ  dθ  r ≥ a  where w0 is the displacement at r = a. Use numerical integration to determine w w0 at r = 2a.  13.  cid:2   x  k  m  b  The mass m is attached to a spring of free length b and stiffness k. The coefﬁcient of friction between the mass and the horizontal rod is μ. The acceleration of the mass can be shown to be  you may wish to prove this  ¨x = −f  x , where  &  f  x  = μg + k m  1 −  b√ b2 + x2  If the mass is released from rest at x = b, its speed at x = 0 is given by  %   μb + x     4  b  v0 =  2  f  x dx  0  Compute v0 by numerical integration using the data m = 0.8 kg, b = 0.4 m, μ = 0.3, k = 80 N m, and g = 9.81 m s2.  14.  cid:2  Debye’s formula for the heat capacity CV as a solid is CV = 9Nkg u , where  The terms in this equation are  4  0  g u  = u3  1 u  x4ex  ex − 1 2 dx  N = number of particles in the solid k = Boltzmann constant u = T  cid:23 D T = absolute temperature  cid:23 D = Debye temperature  Compute g u  from u = 0 to 1.0 in intervals of 0.05 and plot the results.  15.  cid:2  A power spike in an electric circuit results in the current  i t  = i0e  −t  t0 sin 2t  t0    215  6.3 Romberg Integration  across a resistor. The energy E dissipated by the resistor is  Find E using the data i0 = 100 A, R = 0.5  cid:7 , and t0 = 0.01 s.  16.  cid:2  An alternating electric current is described by − β sin  i t  = i0  sin  %  &  2πt t0  where i0 = 1 A, t0 = 0.05 s, and β = 0.2. Compute the root-mean-square current, deﬁned as  17.  a  Derive the composite trapezoidal rule for unevenly spaced data.  b  Consider  the stress-strain diagram obtained from a uniaxial tension test.  4 ∞  0  E =   cid:28    cid:29 2 dt  R  i t   πt t0     4  1 t0  t0  0  irms =  i2 t  dt  σ  0  A r  Rupture  ε  εr  4 εr  ε=0  Ar =  σ dε  σ  MPa   ε  586 662 765 841 814 122 150  0.001 0.025 0.045 0.068 0.089 0.122 0.150  The area under the diagram is  where εr is the strain at rupture. This area represents the work that must be per- formed on a unit volume of the test specimen to cause rupture; it is called the modulus of toughness. Use the result of Part  a  to estimate the modulus of tough- ness for nickel steel from the following test data:  Note that the spacing of data is uneven.   216  Numerical Integration  18.  cid:2  Write a function that computes the sine integral  4  0  Si x  =  x  −1 sint dt  t  for any given value of x. Test the program by computing Si 1.0 , and compare the result with the tabulated value 0.946 08.  6.4 Gaussian Integration  Gaussian Integration Formulas  b Newton-Cotes formulas for approximating a f  x dx work best if f  x  is a smooth function, such as a polynomial. This is also true for Gaussian quadrature. However, Gaussian formulas are also good at estimating integrals of the form  w x f  x dx   6.15   where w x , called the weighting function, can contain singularities, as long as they 0  1 + x2  ln x dx. Sometimes inﬁnite are integrable. An example of such integral is limits, as in  −x sin x dx, can also be accommodated.  3 ∞  1  0 e  Gaussian integration formulas have the same form as Newton-Cotes rules  3  3  4  b  a  I = n cid:6   i=0  Ai f  xi    6.16   where, as before, I represents the approximation to the integral in Eq.  6.15 . The difference lies in the way that the weights Ai and nodal abscissas xi are determined. In Newton-Cotes integration the nodes were evenly spaced in  a, b   i.e., their locations were predetermined . In Gaussian quadrature the nodes and weights are chosen so that Eq.  6.16  yields the exact integral if f  x  is a polynomial of degree 2n + 1 or less; that is,  w x Pm x dx = n cid:6   i=0  Ai Pm xi , m ≤ 2n + 1   6.17   One way of determining the weights and abscissas is to substitute P0 x  = 1, P1 x  = x, . . . , P2n+1 x  = x2n+1 in Eq.  6.17  and solve the resulting 2n + 2 equations  w x x j dx = n cid:6   Ai x j i ,  i=0  j = 0, 1, . . . , 2n + 1  4  b  a  4  b  a  for the unknowns Ai and xi.   217  6.4 Gaussian Integration  −x, a = 0, b = ∞, and n = 1. The four equations  determining x0, x1, A 0, and A 1 are as follows:  As an illustration, let w x  = e 4 ∞ 4 4 4  e  e  1  0  0  1  0  −xdx = A 0 + A 1  e  −xx dx = A 0x0 + A 1x1  −xx2dx = A 0x2  0  + A 1x2  1  1  −xx3dx = A 0x3  e  0  + A 1x3  1  0  After evaluating the integrals, we get these results: A 0 + A 1 = 1 A 0x0 + A 1x1 = 1 = 2 A 0x2 0 = 6 A 0x3 0  + A 1x2 + A 1x3  1  1  The solution is  x0 = 2 −  x1 = 2 +  √  √  2  2  A 0 =  A 1 =  √ 2 + 1 √ 2 2 √ 2 − 1 √ 2 2  so that the integration formula becomes 2 + 1  f  −x f  x dx ≈  √    e   cid:14   1 2 −  2  √ 2  √ +    2 − 1  f  2 cid:15   1 2 +  √ 2  √ 1 2 2  4 ∞  0  Because of the nonlinearity of the equations, this approach does not work well for large n. Practical methods of ﬁnding xi and Ai require some knowledge of orthog- onal polynomials and their relationship to Gaussian quadrature. There are, however, several “classical” Gaussian integration formulas for which the abscissas and weights have been computed with great precision and then tabulated. These formulas can be used without knowing the theory behind them, because all one needs for Gaussian integration are the values of xi and Ai. If you do not intend to venture outside the classical formulas, you can skip the next two topics.  *Orthogonal Polynomials  Orthogonal polynomials are employed in many areas of mathematics and numerical analysis. They have been studied thoroughly, and many of their properties are known. What follows is a very small compendium of a large topic.   218  Numerical Integration  The polynomials ϕn x , n = 0, 1, 2, . . .  n is the degree of the polynomial  are said to form an orthogonal set in the interval  a, b  with respect to the weighting function w x  if  b  w x ϕm x ϕn x dx = 0, m  cid:7 = n   6.18   4  a  The set is determined, except for a constant factor, by the choice of the weighting function and the limits of integration. That is, each set of orthogonal polynomials is associated with certain w x , a, and b. The constant factor is speciﬁed by stan- dardization. Some of the classical orthogonal polynomials, named after well-known mathematicians, are listed in Table 6.1. The last column in the table shows the stan- dardization used.  Name Legendre Chebyshev Laguerre Hermite  Symbol pn x  Tn x  Ln x  Hn x   a b −1 1 −1 1 0 ∞ −∞ ∞  w x  1  1 − x2  −x e −x2 e  −1 2  Table 6.1. Classical orthogonal polynomials   cid:29 2 dx  3   cid:28   b a w x   ϕn x  2  2n + 1  π  2  n > 0   √ 1 π2nn!  Orthogonal polynomials obey recurrence relations of the form  anϕn+1 x  =  bn + cnx ϕn x  − dnϕn−1 x    6.19   If the ﬁrst two polynomials of the set are known, the other members of the set can be computed from Eq.  6.19 . The coefﬁcients in the recurrence formula, together with ϕ0 x  and ϕ1 x  are given in Table 6.2.  Name Legendre Chebyshev Laguerre Hermite  ϕ  0 x  1 1 1 1  ϕ  1 x  x x 1 − x 2x  an n + 1 1 n + 1 1  bn 0 0  0  2n + 1  cn 2n + 1 2 −1 2  dn n 1 n 2  Table 6.2. Recurrence coefﬁcients  The classical orthogonal polynomials are also obtainable from these formulas  2nn!  pn x  =  −1 n dn dxn Tn x  = cos n cos  cid:30  Ln x  = ex xne n! Hn x  =  −1 nex2 dn  dn dxn   cid:15    cid:14  cid:30   cid:31 n 1 − x2 −1 x , n > 0  cid:31   −x  −x2   dxn  e   6.20    219  6.4 Gaussian Integration  and their derivatives can be calculated from   cid:29   cid:28 −xpn x  + pn−1 x   cid:28 −xTn x  + nTn−1 x   cid:28   cid:29  Ln x  − Ln−1 x    cid:29   n x  = n  1 − x2 p  cid:3  n x  = n  1 − x2 T  cid:3  n x  = n  cid:3  x L n x  = 2nHn−1 x   cid:3  H  Other properties of orthogonal polynomials that have relevance to Gaussian in-  tegration are as follows:   ϕn x  has n real, distinct zeros in the interval  a, b .   The zeros of ϕn x  lie between the zeros of ϕn+1 x .   Any polynomial Pn x  of degree n can be expressed in the form  Pn x  = n cid:6   i=0  ci ϕ  i x     It follows from Eq.  6.22  and the orthogonality property in Eq.  6.18  that  b  w x Pn x ϕn+m x dx = 0, m ≥ 0  *Determination of Nodal Abscissas and Weights  Theorem The nodal abscissas x0, x1, . . . , xn are the zeroes of the polynomial ϕn+1 x   that belongs to the orthogonal set deﬁned in Eq.  6.18 .  Proof We start the proof by letting f  x  = P2n+1 x  be a polynomial of degree 2n + 1. Because the Gaussian integration with n + 1 nodes is exact for this polynomial, we have  4  b  a  w x P2n+1 x dx = n cid:6   Ai P2n+1 xi   i=0  A polynomial of degree 2n + 1 can always be written in the form  P2n+1 x  = Qn x  + Rn x ϕn+1 x   where Qn x , Rn x , and ϕn+1 x  are polynomials of the degree indicated by the subscripts.2 Therefore,  b  w x P2n+1 x dx =  b  w x Qn x dx +  w x Rn x ϕn+1 x dx  4  a  4  a  But according to Eq.  6.23  the second integral on the right-hand-side vanishes, so that  b  w x P2n+1 x dx =  w x Qn x dx  4  b  a  4  b  a  2 It can be shown that Qn x  and Rn x  are unique for a given P2n+1 x  and ϕn+1 x .  4  a  4  a   6.21    6.22    6.23    a    b    c    220  Numerical Integration  Because a polynomial of degree n is uniquely deﬁned by n + 1 points, it is al- ways possible to ﬁnd Ai such that  4  b  a  w x Qn x dx = n cid:6   Ai Qn xi   i=0  To arrive at Eq.  a , we must choose for the nodal abscissas xi the roots of ϕn+1 x  = 0. According to Eq.  b  we then have  P2n+1 xi  = Qn xi ,  i = 0, 1, . . . , n  which together with Eqs.  c  and  d  leads to  4  a  b  w x P2n+1 x dx =  4  b  a  w x Qn x dx = n cid:6   Ai P2n+1 xi   i=0   d    e   This completes the proof.  Theorem  4  b  a  Ai =  w x  cid:12 i x dx,  i = 0, 1, . . . , n   6.24   where  cid:12 i x  are the Lagrange’s cardinal functions spanning the nodes at x0, x1, . . . xn. These functions were deﬁned in Eq.  4.2 . Proof Applying Lagrange’s formula, Eq.  4.1 , to Qn x  yields  which upon substitution in Eq.  d  gives us  or  Qn x  = n cid:6  4  i=0  b  a  *  4  b  a  Qn xi  cid:12 i x   +  = n cid:6   i=0  +  Qn xi   Ai −  w x  cid:12 i x dx  = 0  Qn xi   w x  cid:12 i x dx  Ai Qn xi   *  n cid:6   i=0  n cid:6   i=0  4  a  This equation can be satisﬁed for arbitrary Q x  of degree n only if  Ai −  b  w x  cid:12 i x dx = 0,  i = 0, 1, . . . , n  which is equivalent to Eq.  6.24 . It is not difﬁcult to compute the zeros xi, i = 0, 1, . . . , n of a polynomial ϕn+1 x  belonging to an orthogonal set by one of the methods discussed in Chapter 4. Once the zeros are known, the weights Ai, i = 0, 1, . . . , n could be found from Eq.  6.24 .   221  6.4 Gaussian Integration  However the following formulas  given without proof  are easier to compute:   cid:28    cid:29   2  2  cid:3  n+1 xi  p  Gauss-Legendre Ai =   1 − x2 i    cid:29 2  cid:28  Gauss-Laguerre Ai = 1  cid:3  n+1 xi  √  cid:29 2  cid:28  Gauss-Hermite Ai = 2n+2  n + 1 !  cid:3  n+1 xi  H  xi  L  π   6.25   Abscissas and Weights for Classical Gaussian Quadratures  Here we list some classical Gaussian integration formulas. The tables of nodal ab- scissas and weights, covering n = 1 to 5, have been rounded off to six decimal places. These tables should be adequate for hand computation, but in programming you may need more precision or a larger number of nodes. In that case you should consult other references.3 or use a subroutine to compute the abscissas and weights within the integration program.4  The truncation error in Gaussian quadrature  4  b  a  E =  w x f  x dx − n cid:6   Ai f  xi   i=0  has the form E = K  n f  2n+2  c , where a < c < b  the value of c is unknown; only its bounds are given . The expression for K  n  depends on the particular quadrature being used. If the derivatives of f  x  can be evaluated, the error formulas are useful in estimating the error bounds.  Gauss-Legendre Quadrature4  n cid:6   i=0  1 −1  f  ξ dξ ≈  Ai f  ξi    6.26   Gauss-Legendre quadrature is the most often used Gaussian integration formula. As seen in Table 6.3, the nodes are arranged symmetrically about ξ = 0, and the weights associated with a symmetric pair of nodes are equal. For example, for n = 1 we have ξ 0 = −ξ 1 and A 0 = A 1. The truncation error in Eq.  6.26  is  cid:29 3 f  2n+2  c , − 1 < c < 1   cid:28   cid:29 4  cid:28   n + 1 ! E = 22n+3  2n + 3   2n + 2 !   6.27   3 Abramowitz, M. and Stegun, I.A, Handbook of Mathematical Functions, Dover Publications, 1965;  Stroud, A.H.and Secrest, D., Gaussian Quadrature Formulas, Prentice-Hall, 1966.  4 Several such subroutines are listed in W.H. Press et al, Numerical Recipes in Fortran 90, Cambridge  University Press, 1996.   222  Numerical Integration  ±ξ  i  Ai  ±ξ  i  0.577 350  1.000 000  n = 1 n = 2  n = 3  0.000 000 0.774 597  0.339 981 0.861 136  0.000 000 0.538 469 0.906 180  0.238 619 0.661 209 0.932 470  0.888 889 0.555 556  0.652 145 0.347 855  Ai  0.568 889 0.478 629 0.236 927  0.467 914 0.360 762 0.171 324  n = 4  n = 5  3  Table 6.3. Nodes and weights for Gauss–Legendre quadrature.  To apply the Gauss-Legendre quadrature to the integral  b a f  x dx, we must ﬁrst map the integration range  a, b  into the “standard” range  −1, 1˙ . We can accomplish this by the transformation  x = b + a  + b − a Now dx = dξ b − a  2, and the quadrature becomes n cid:6   4  2  2  ξ  b  b − a 2  f  x dx ≈  a  Ai f  xi   i=1  where the abscissas xi must be computed from Eq.  6.28 . The truncation error here is   cid:28  E =  b − a 2n+3  2n + 3    cid:29 4  cid:28   cid:29 3 f  2n+2  c ,  n + 1 !  2n + 2 !  a < c < b   6.30   Gauss-Chebyshev Quadrature   cid:30   4  1 −1   cid:31 −1 2 f  x dx ≈  1 − x2  n cid:6   i=0  π  n + 1  f  xi   Note that all the weights are equal: Ai = π    n + 1 . The abscissas of the nodes, which are symmetric about x = 0, are given by xi = cos   6.32    2i + 1 π 2n + 2  The truncation error is E =  2π  22n+2 2n + 2 !  f  2n+2  c , − 1 < c < 1   6.33    6.28    6.29    6.31    223  6.4 Gaussian Integration  Gauss-Laguerre Quadrature4 ∞  −x f  x dx ≈  e  Ai f  xi    6.34   n cid:6   i=0  xi  0.263 560 1.413 403 3.596 426 7.085 810 12.640 801  0.222 847 1.188 932 2.992 736 5.775 144 9.837 467 15.982 874  n cid:6   i=0 ±xi  0.000 000 0.958 572 2.020 183  n = 1  n = 2  n = 3  xi  0.585 786 3.414 214  0.415 775 2.294 280 6.289 945  0.322 548 1.745 761 4.536 620 9.395 071  0  Ai  0.853 554 0.146 447  0.711 093 0.278 517  −1 0.103 892  0.603 154 0.357 418  −1 0.388 791  −3 0.539 295  n = 4  Ai  n = 5  0.521 756 0.398 667  −1 0.759 424  −2 0.361 175  −4 0.233 670  0.458 964 0.417 000 0.113 373  −1 0.103 992  −3 0.261 017  −6 0.898 548   6.35    6.36   Table 6.4. Nodes and weights for Gauss–Laguerre quadrature  Multiply numbers by 10k, where k is given in parentheses.    cid:28   cid:29 2  n + 1 !  2n + 2 !  E =  f  2n+2  c , 0 < c < ∞  Gauss-Hermite Quadrature4 ∞  −x2 f  x dx ≈  e  −∞  Ai f  xi   ±xi  0.707 107  0.000 000 1.224745  n = 1 n = 2  n = 3  Ai  0.886 227  1.181 636 0.295 409  Ai  0.945 308 0.393 619  −1  0.199 532  n = 4  n = 5  0.524 648 1.650 680  0.724 629 0.157 067  −2 0.453 001 Table 6.5. Nodes and weights for Gauss–Hermite quadrature  Multiply numbers by 10k, where k is given in parentheses.   0.436 077 1.335 849 2.350 605  0.804 914  −1 0.813 128   224  Numerical Integration  The nodes are placed symmetrically abut x = 0.  E =  √ π n + 1 ! 22 2n + 2 !  f  2n+2  c , 0 < c < ∞  Gauss Quadrature with Logarithmic Singularity  4  1  0  f  x  ln x dx ≈ − n cid:6   Ai f  xi    6.37    6.38   n = 4  Ai  0.297 893 0.349 776 0.234 488  −1 0.989 305  −1 0.189 116  n = 5  Ai  0.718 539 0.281 461  0.513 405 0.391 980  −1 0.946 154  n = 1  n = 2  n = 3  xi  0.112 009 0.602 277  −1 0.638 907 0.368 997 0.766 880  −1 0.414 485 0.245 275 0.556 165 0.848 982  i=0  xi   −1 0.291 345 0.173 977 0.411 703 0.677314 0.894 771  −1 0.216 344 0.129 583 0.314 020 0.538 657 0.756 916 0.922 669  0.238 764 0.308 287 0.245 317 0.142 009  −1 0.554 546  −1 0.101 690 Table 6.6. Nodes and weights for quadrature with logarithmic singularily  Multiply numbers by 10k, where k is given in parentheses.   0.383 464 0.386 875 0.190 435  −1 0.392 255  E = k n   2n + 1 !  f  2n+1  c , 0 < c < 1   6.39   where k 1  = 0.00 285, k 2  = 0.000 17, and k 3  = 0.000 01.   cid:2  gaussNodes  The function gaussNodes listed next5 computes the nodal abscissas xi and the cor- responding weights Ai used in Gauss-Legendre quadrature over the “standard” inter- val  −1, 1 . It can be shown that the approximate values of the abscissas are  xi = cos  π i + 0.75  m + 0.5  where m = n + 1 is the number of nodes, also called the integration order. Using these approximations as the starting values, the nodal abscissas are computed by ﬁnding  5 This function is an adaptation of a routine in Press, W.H et al., Numerical Recipes in Fortran 90,  Cambridge University Press, 1996.   225  6.4 Gaussian Integration  the non-negative zeros of the Legendre polynomial pm x  with Newton’s method  the negative zeros are obtained from symmetry . Note that gaussNodes calls the sub- function legendre, which returns pm t  and its derivative as the tuple  p,dp .   module gaussNodes  ’’’ x,A = gaussNodes m,tol=10e-9   Returns nodal abscissas {x} and weights {A} of  Gauss-Legendre m-point quadrature.  ’’’  import math  import numpy as np  def gaussNodes m,tol=10e-9 :  def legendre t,m :  p0 = 1.0; p1 = t  for k in range 1,m :  p =   2.0*k + 1.0 *t*p1 - k*p0   1.0 + k    p0 = p1; p1 = p  dp = m* p0 - t*p1   1.0 - t**2   return p,dp  A = np.zeros m   x = np.zeros m   for i in range nRoots :  for j in range 30 :  nRoots = int  m + 1  2    Number of non-neg. roots  t = math.cos math.pi* i + 0.75   m + 0.5   Approx. root  p,dp = legendre t,m    Newton-Raphson  dt = -p dp; t = t + dt   method  if abs dt  < tol:  x[i] = t; x[m-i-1] = -t  A[i] = 2.0  1.0 - t**2   dp**2   Eq. 6.25   A[m-i-1] = A[i]  break  return x,A   cid:2  gaussQuad  3  b The function gaussQuad uses gaussNodes to evaluate a f  x  dx with Gauss- Legendre quadrature using m nodes. The function routine for f  x  must be supplied by the user.   module gaussQuad  ’’’ I = gaussQuad f,a,b,m .   226  Numerical Integration  Computes the integral of f x  from x = a to b  with Gauss-Legendre quadrature using m nodes.  ’’’  from gaussNodes import *  def gaussQuad f,a,b,m :  c1 =  b + a  2.0  c2 =  b - a  2.0  x,A = gaussNodes m   sum = 0.0  for i in range len x  :  return c2*sum  3  EXAMPLE 6.8 Evaluate  sum = sum + A[i]*f c1 + c2*x[i]   1−1 1 − x2 3 2dx as accurately as possible with Gaussian integration.  Solution. Because the integrand is smooth and free of singularities, we could use Gauss-Legendre quadrature. However, the exact integral can obtained with the Gauss-Chebyshev formula. We write  4  1 −1   cid:30  1 − x2   cid:31 3 2 dx =  4  1 −1   cid:31 2  cid:30  1 − x2 √ 1 − x2  dx  The numerator f  x  =  1 − x2 2 is a polynomial of degree four, so that Gauss- Chebyshev quadrature is exact with three nodes. The abscissas of the nodes are obtained from Eq.  6.32 . Substituting n = 2, we  get  Therefore,  and Eq.  6.31  yields   cid:30   4  1 −1   cid:31 3 2 dx ≈  1 − x2  xi = cos   2i + 1 π  6  i = 0, 1, 2  ,  √  3 2  = = 0  √ 3 2  =  π 6 π 2 5π 6  x0 = cos x1 = cos x2 = cos 2 cid:6  *%  i=0   cid:30   1 − 3 4  π 3  = π 3  1 − x2  i   cid:31 2 &2 +  1 − 0 2 +  %  +  &2  1 − 3 4  = 3π 8   227  6.4 Gaussian Integration  EXAMPLE 6.9 Use Gaussian integration to evaluate  0.5 0 cos πx ln x dx.  Solution. We split the integral into two parts:  4  0  0.5  cos πx ln x dx =  1  cos πx ln x dx −  cos πx ln x dx  4  1  0.5  3  4  0  The ﬁrst integral on the right-hand-side, which contains a logarithmic singularity at x = 0, can be computed with the special Gaussian quadrature in Eq.  6.38 . Choosing n = 3, we have  4  1  0  cos πx ln x dx ≈ − 3 cid:6   Ai cos πxi  i=0  The sum is evaluated in the following table:  xi  cos πxi 0.041 448 0.991 534 0.245 275 0.717 525 0.556 165 −0.175 533 0.848 982 −0.889 550  Ai  0.383 464 0.386 875 0.190 435 0.039 225  Ai cos πxi  0.380 218 0.277 592 −0.033 428 −0.034 892  cid:19  = 0.589 490  Thus  4  0  1  cos πx ln x dx ≈ −0.589 490  The second integral is free of singularities, so that it can be evaluated with Gauss-  Legendre quadrature. Choosing n = 3, we have 3 cid:6   4  1  cos πx ln x dx ≈ 0.25  Ai cos πxi ln xi  0.5  i=0  where the nodal abscissas are—see Eq.  6.28 —  xi = 1 + 0.5  + 1 − 0.5  2  2  ξi = 0.75 + 0.25ξi  Looking up ξi and Ai in Table 6.3 leads to the following computations:  ξi  −0.861 136 −0.339 981 0.339 981 0.861 136  xi  0.534 716 0.665 005 0.834 995 0.965 284  cos πxi ln xi 0.068 141 0.202 133 0.156 638 0.035 123  Ai  0.347 855 0.652 145 0.652 145 0.347 855  Ai cos πxi ln xi 0.023 703 0.131 820 0.102 151 0.012 218  cid:19  = 0.269 892   228  Numerical Integration  from which  4  0.5  Therefore,4  0  1  cos πx ln x dx ≈ 0.25 0.269 892  = 0.067 473  1  cos πx ln x dx ≈ −0. 589 490 − 0.067 473 = −0. 656 96 3  which is correct to six decimal places.  EXAMPLE 6.10 Evaluate as accurately as possible  4 ∞  0  F =  x + 3√  x  −xdx  e  Solution. In its present form, the integral is not suited to any of the Gaussian quadra- tures listed in this section. But using the transformation  the integral becomes  x = t 2  4 ∞  0  dx = 2t dt 4 ∞ −∞ t 2 + 3 e  −t 2dt  F = 2   t 2 + 3 e  −t 2dt =  which can be evaluated exactly with the Gauss-Hermite formula using only two nodes  n = 1 . Thus F = A 0 t 2 = 0.886 227 = 6. 203 59   cid:29   −0.707 107 2 + 3   cid:29  + 0.886 227   0.707 107 2 + 3  + 3  + A 1 t 2  + 3    cid:28    cid:28   0  1  EXAMPLE 6.11 Determine how many nodes are required to evaluate  4 π  %  0  &2  sin x  x  dx  with Gauss-Legendre quadrature to six decimal places. The exact integral, rounded to six places, is 1.418 15.  Solution. The integrand is a smooth function; hence it is suited for Gauss-Legendre integration. There is an indeterminacy at x = 0, but it does not bother the quadra- ture because the integrand is never evaluated at that point. We used the following program that computes the quadrature with 2, 3, . . . nodes until the desired accuracy is reached:   example 6_11  import math  from gaussQuad import *   229  6.4 Gaussian Integration  def f x : return  math.sin x  x **2  a = 0.0; b = math.pi;  Iexact = 1.41815  for m in range 2,12 :  I = gaussQuad f,a,b,m   if abs I - Iexact  < 0.00001:  print "Number of nodes =",m   print "Integral =", gaussQuad f,a,b,m    break  input "\nPress return to exit"   The program output is  Number of nodes = 5  Integral = 1.41815026778  3  EXAMPLE 6.12 Evaluate numerically spaced data:  3 1.5 f  x  dx, where f  x  is represented by the following unevenly  x f  x  −0.362 36  1.2  1.7  2.0  2.4  2.9  3.3  0.128 84  0.416 15  0.737 39  0.970 96  0.987 48  Knowing that the data points lie on the curve f  x  = − cos x, evaluate the accuracy of the solution.  3  3  Solution. We approximate f  x  by the polynomial P5 x  that intersects all the data 3 1.5 P5 x dx with the Gauss-Legendre for- points, and then evaluate mula. Because the polynomial is of degree ﬁve, only three nodes  n = 2  are required in the quadrature.  3 1.5 f  x dx ≈  From Eq.  6.28  and Table 6.6, we obtain for the abscissas of the nodes  2  x0 = 3 + 1.5 x1 = 3 + 1.5 x2 = 3 + 1.5  2  2  + 3 − 1.5 2 = 2.25 + 3 − 1.5  2   −0.774597  = 1. 6691   0.774597  = 2. 8309  We now compute the values of the interpolant P5 x  at the nodes. This can be done using the functions newtonPoly or neville listed in Section 3.2. The results are  From Gauss-Legendre quadrature  P5 x0  = 0.098 08 4  I =  3  1.5  P5 x1  = 0.628 16 2 cid:6   P5 x dx = 3 − 1.5  2  i=0  Ai P5 xi   P5 x2  = 0.952 16   230  Numerical Integration  we get  Comparison with −3  roundoff error.  PROBLEM SET 6.2  1. Evaluate  I = 0.75 [0.555 556 0.098 08  + 0.888 889 0.628 16  + 0.555 556 0.952 16 ] = 0.856 37  3  1.5 cos x dx = 0. 856 38 shows that the discrepancy is within the  with Gauss-Legendre quadrature. Use  a  two nodes; and  b  four nodes.  2. Use Gauss-Laguerre quadrature to evaluate 3. Use Gauss-Chebyshev quadrature with six nodes to evaluate  ln x   dx  x2 − 2x + 2 3 ∞ 0  1 − x2 3e  −x dx.  4 π  1  4 π  2  0  dx√ sin x  3 π  Compare the result with the “exact” value 2.62206. Hint: Substitute sin x = t 2.  4. The integral  0 sin x dx is evaluated with Gauss-Legendre quadrature using four nodes. What are the bounds on the truncation error resulting from the quadra- ture?  5. How many nodes are required in Gauss-Laguerre quadrature to evaluate  3 ∞ 6. Evaluate as accurately as possible4  −x sin x dx to six decimal places?  0 e  Hint: Substitute x =  1 + t  2.  0 sin x ln x dx to four decimal places. 7. Compute 8. Calculate the bounds on the truncation error if  3 π  Gauss-Legendre quadrature using three nodes. What is the actual error?  0 x sin x dx is evaluated with  3 π 3  cid:30   2 0   cid:31   9. Evaluate sinh x x 10. Evaluate the integral  dx to four decimal places.  2x + 1 √ x 1 − x   1  0  dx  4 ∞  0  x dx ex + 1  by Gauss-Legendre quadrature to six decimal places. Hint: Substitute ex = ln 1 t . 11.  cid:2  The equation of an ellipse is x2 a 2 + y 2 b2 = 1. Write a program that computes  the length  4  a −a  S = 2   cid:25  1 +  dy dx 2 dx  of the circumference to ﬁve decimal places for a given a and b. Test the program with a = 2 and b = 1.   231  6.4 Gaussian Integration  12.  cid:2  The error function, which is of importance in statistics, is deﬁned as  4  erf x  = 2√  x  −t 2dt  e  π  0  Write a program that uses Gauss-Legendre quadrature to evaluate erf x  for a given x to six decimal places. Note that erf x  = 1.000 000  correct to 6 decimal places  when x > 5. Test the program by verifying that erf 1.0  = 0.842 701.  13.  cid:2   14.  cid:2   The sliding weight of mass m is attached to a spring of stiffness k that has an undeformed length L. When the mass is released from rest at B, the time it takes to reach A can be shown to be t = C 2 − 1  22 1 + z2 − 1   cid:25  22 −   −1 2  ’1√  m k, where  1 cid:25   C =  4  dz  1  0  Compute C to six decimal places. Hint: The integrand has singularity at z = 1 that behaves as  1 − z2   −1 2.  A  L  B m  L  k  x  h  B  A P  b  y  %  &  A uniform beam forms the semi-parabolic cantilever arch A B. The vertical dis- placement of A due to the force P can be shown to be  where E I is the bending rigidity of the beam and  C  δA = Pb3 E I   4 1 +  =  z2  1  h b  %  0  %  &  h b  C  &2  2h b  z  dz  15.  cid:2  There is no elegant way to compute I =3 π  2  Write a program that computes C h b  for any given value of h b to four decimal places. Use the program to compute C 0.5 , C 1.0 , and C 2.0 . ln sin x  dx. A “brute force” method that works is to split the integral into several parts: from x = 0 to 0.01,  0   232  Numerical Integration  from 0.01 to 0.2, and from x = 0.02 to π  2. In the ﬁrst part we can use the approx- imation sin x ≈ x, which allows us to obtain the integral analytically. The other two parts can be evaluated with Gauss-Legendre quadrature. Use this method to evaluate I to six decimal places.  16.  cid:2   h m  112 80  620 612  575 530 425 310  52 35 15 0  p  Pa   ¯h =  112 m 0  112 m 0  h p h  dh p h  dh  3 3  3  xn x1  x1 y1  x2 y2  x3 y3  ··· ···  xn yn  The pressure of wind was measured at various heights on a vertical wall, as shown on the diagram. Find the height of the pressure center, which is deﬁned as  Hint: Fit a cubic polynomial to the data and then apply Gauss-Legendre quadra- ture.  17.  cid:2  Write a function that computes  y x  dx from a given set of data points of  the form  The function must work for unevenly spaced x-values. Test the function with the data given in Prob. 17, Problem Set 6.1. Hint: Fit a cubic spline to the data points and apply Gauss-Legendre quadrature to each segment of the spline.  ∗6.5 Multiple Integrals  3 3  Multiple integrals, such as the area integral A f  x, y  dx dy, can also be evaluated by quadrature. The computations are straightforward if the region of integration has a simple geometric shape, such as a triangle or a quadrilateral. Because of compli- cations in specifying the limits of integration, quadrature is not a practical means of evaluating integrals over irregular regions. However, an irregular region A can always be approximated as an assembly of triangular or quadrilateral subregions A 1, A 2, . . ., called ﬁnite elements, as illustrated in Figure 6.6. The integral over A can then be eval- uated by summing the integrals over the ﬁnite elements:  4 4  A  f  x, y  dx dy ≈  f  x, y  dx dy  4 4   cid:6   i  Ai   233  ∗6.5 Multiple Integrals  Bounday of region A  Ai  Figure 6.6. Finite element model of an irregular region.  Volume integrals can be computed in a similar manner, using tetrahedra or rectan- gular prisms for the ﬁnite elements.  Gauss-Legendre Quadrature over a Quadrilateral Element  Consider the double integral  4  4  1 −1  1 −1  I =  f  ξ, η  dη dξ  over the rectangular element shown in Figure 6.7 a . Evaluating each integral in turn by Gauss-Legendre quadrature using n + 1 integration points in each coordinate di- rection, we obtain  +  *  n cid:6   i=0  A j  Ai f  ξ  i, η  j    4  I =  1 −1  n cid:6   i=0  Ai f  ξ  i, η  dη = n cid:6  n cid:6  I = n cid:6   j=0  i=0  j=0  AiA j f  ξi, η j     6.40   As noted previously, the number of integration points in each coordinate direc- tion, m = n + 1, is called the integration order. Figure 6.7 a  shows the locations of the integration points used in third-order integration  m = 3 . Because the integration limits are the “standard” limits  −1, 1  of Gauss-Legendre quadrature, the weights and the coordinates of the integration points are as listed in Table 6.3.  To apply quadrature to the quadrilateral element in Figure 6.7 b , we must ﬁrst map the quadrilateral into the “standard” rectangle in Figure 6.7 a . By mapping we mean a coordinate transformation x = x ξ, η , y = y ξ, η  that results in one-to-one  η  η= 1  3  4  ξ= 1  ξ= −1 ξ  y  1  x  1  0  a   η= −1  b   2  Figure 6.7. Mapping of quadrilateral into a rectangle.  or  1  0  1 1   234  Numerical Integration  correspondence between points in the quadrilateral and in the rectangle. The trans- formation that does the job is  x ξ, η  = 4 cid:6   k=1  y ξ, η  = 4 cid:6   k=1  Nk ξ, η xk  Nk ξ, η yk   6.41   where  xk, yk  are the coordinates of corner k of the quadrilateral and   6.42    6.43    6.44a   N1 ξ, η  = 1 4 N2 ξ, η  = 1 4 N3 ξ, η  = 1 4 N4 ξ, η  = 1 4   1 − ξ  1 − η   1 + ξ  1 − η   1 + ξ  1 + η   1 − ξ  1 + η   dx dy = J  ξ, η  dξ dη ⎤ ⎥⎥⎥⎦  J  ξ, η  =  ⎡ ⎢⎢⎢⎣  ∂x ∂ξ  ∂y ∂ξ  ∂x ∂η  ∂y ∂η  The functions Nk ξ, η , known as the shape functions, are bilinear  linear in each coordinate . Consequently, straight lines remain straight upon mapping. In partic- ular, note that the sides of the quadrilateral are mapped into the lines ξ = ±1 and η = ±1. Because mapping distorts areas, an inﬁnitesimal area element dA = dx dy of the  cid:3  = dξ dη of the rectangle. It can be  quadrilateral is not equal to its counterpart dA shown that the relationship between the areas is  where  is known as the Jacobian matrix of the mapping. Substituting from Eqs.  6.41  and  6.42  and differentiating, the components of the Jacobian matrix are [− 1 − η x1 +  1 − η x2 +  1 + η x3 −  1 − η x4] [− 1 − η y1 +  1 − η y2 +  1 + η y3 −  1 − η y4] [− 1 − ξ x1 −  1 + ξ x2 +  1 + ξ x3 +  1 − ξ x4] [− 1 − ξ y1 −  1 + ξ y2 +  1 + ξ y3 +  1 − ξ y4]   6.44b   J11 = 1 4 J12 = 1 4 J21 = 1 4 J22 = 1 4 4 4  We can now write  4  4  1 −1  1 −1  f  x, y  dx dy =  A  f [x ξ, η , y ξ, η ] J  ξ, η  dξ dη   6.45    235  ∗6.5 Multiple Integrals  Since the right-hand-side integral is taken over the “standard” rectangle, it can be evaluated using Eq.  6.40 . Replacing f  ξ, η  in Eq.  6.40  by the integrand in Eq.  6.45 , we get the following formula for Gauss-Legendre quadrature over a quadrilateral region:  n cid:6   I = n cid:6   i=0  j=0   cid:28    cid:29   cid:19  cid:19 J  ξi, η j    cid:19  cid:19   AiA j f  x ξi, η j  , y ξi, η j     6.46   The ξ and η-coordinates of the integration points and the weights can again be ob- tained from Table 6.3.   cid:2  gaussQuad2  3 3  The function gaussQuad2 in this module computes A f  x, y  dx dy over a quadri- lateral element with Gauss-Legendre quadrature of integration order m. The quadri- lateral is deﬁned by the arrays x and y, which contain the coordinates of the four cor- ners ordered in a counterclockwise direction around the element. The determinant of the Jacobian matrix is obtained by calling the function jac; mapping is performed by map. The weights and the values of ξ and η at the integration points are computed by gaussNodes listed in the previous section  note that ξ and η appear as s and t in the listing .   module gaussQuad2  ’’’ I = gaussQuad2 f,xc,yc,m .  Gauss-Legendre integration of f x,y  over a  quadrilateral using integration order m.  {xc},{yc} are the corner coordinates of the quadrilateral.  ’’’  from gaussNodes import *  import numpy as np  def gaussQuad2 f,x,y,m :  def jac x,y,s,t :  J = np.zeros  2,2    J[0,0] = - 1.0 - t *x[0] +  1.0 - t *x[1]  \  +  1.0 + t *x[2] -  1.0 + t *x[3]  J[0,1] = - 1.0 - t *y[0] +  1.0 - t *y[1]  \  +  1.0 + t *y[2] -  1.0 + t *y[3]  J[1,0] = - 1.0 - s *x[0] -  1.0 + s *x[1]  \  +  1.0 + s *x[2] +  1.0 - s *x[3]  J[1,1] = - 1.0 - s *y[0] -  1.0 + s *y[1]  \  +  1.0 + s *y[2] +  1.0 - s *y[3]  return  J[0,0]*J[1,1] - J[0,1]*J[1,0]  16.0   236  Numerical Integration  def map x,y,s,t :  N = np.zeros 4   N[0] =  1.0 - s * 1.0 - t  4.0  N[1] =  1.0 + s * 1.0 - t  4.0  N[2] =  1.0 + s * 1.0 + t  4.0  N[3] =  1.0 - s * 1.0 + t  4.0  xCoord = np.dot N,x   yCoord = np.dot N,y   return xCoord,yCoord  s,A = gaussNodes m   sum = 0.0  for i in range m :  for j in range m :  xCoord,yCoord = map x,y,s[i],s[j]   sum = sum + A[i]*A[j]*jac x,y,s[i],s[j]   \  *f xCoord,yCoord   return sum  EXAMPLE 6.13  y  4  2  1  3  3  2   cid:31   2  x  4 4   cid:30   A  I =  x2 + y  dx dy  Evaluate the integral  analytically by ﬁrst transforming it from the quadrilateral region A shown to the “standard” rectangle.  Solution. The corner coordinates of the quadrilateral are   cid:14    cid:15    cid:14    cid:15   xT =  0 2 2 0  yT =  0 0 3 2   237  ∗6.5 Multiple Integrals  The mapping is  x ξ, η  = 4 cid:6   y ξ, η  = 4 cid:6   Nk ξ, η xk  k=1 = 0 +  1 + ξ  1 − η  = 1 + ξ  4  Nk ξ, η yk  k=1 = 0 + 0 +  1 + ξ  1 + η  =  5 + ξ  1 + η   4   2  +  1 + ξ  1 + η    2  + 0  4   3  +  1 − ξ  1 + η    2   4  4  ⎡ ⎢⎢⎣ ∂x  ∂ξ ∂x ∂η  J  ξ, η  =  ⎤ ⎥⎥⎦ =  ⎡ ⎢⎢⎣ 1  0  ∂y ∂ξ ∂y ∂η  ⎤ ⎥⎥⎦  1 + η 4 5 + ξ 4  which yields for the Jacobian matrix  Thus the area scale factor is  J  ξ, η  = 5 + ξ  4  Now we can map the integral from the quadrilateral to the standard rectangle. Refer- ring to Eq.  6.45 , we obtain 1 + ξ 2 + 21 16  & 2 +  5 + ξ  1 + η   5 + ξ 4 η + 5 8  4 ξ 2 + 1 16  ξ η + 1 16  ξ 3 + 25 16  *% %  ξ + 1 2  4 4  4 4  I =  dξ dη  dξ dη  15 8  1 −1  1 −1  &  +  ξ 2η  =  1 −1  1 −1  Noting that only even powers of ξ and η contribute to the integral, the integral sim- pliﬁes to  %  4  4  1 −1  1 −1  I =  &  15 8  + 1 2  ξ 2  dξ dη = 49 6  EXAMPLE 6.14 Evaluate the integral  4  4  1 −1  1 −1  cos  cos  dx dy  πx 2  π y 2  by Gauss-Legendre quadrature of order three.   238  Numerical Integration  Solution. From the quadrature formula in Eq.  6.40 , we have  2 cid:6   I = 2 cid:6   i=0  j=0  AiA j cos  cos  πxi 2  π y j 2  1  0  a b  a  −1−1  y  b  b 0  a b  a  x  1  The integration points are shown in the ﬁgure; their coordinates and the correspond- ing weights are listed in Table 6.3. Note that the integrand, the integration points, and the weights are all symmetric about the coordinate axes. It follows that the points labeled a contribute equal amounts to I ; the same is true for the points labeled b. Therefore,  I = 4 0.555 556 2 cos2  π 0.774 597   2  +4 0.555 556  0.888 889  cos + 0.888 889 2 cos2  π 0   2  = 1.623 391  π 0.774 597   2  cos  π 0   2  The exact value of the integral is 16 π 2 ≈ 1.621 139. EXAMPLE 6.15  4  y 4 3  1 1  3  2  x  4  Use gaussQuad2 to evaluate I =3 3  1  A f  x, y  dx dy over the quadrilateral shown,  where  f  x, y  =  x − 2 2 y − 2 2 Use enough integration points for an “exact” answer. Solution. The required integration order is determined by the integrand in Eq.  6.45 :  I =  f [x ξ, η , y ξ, η ] J  ξ, η  dξ dη   a  We note that J  ξ, η , deﬁned in Eqs.  6.44 , is biquadratic. Because the speciﬁed f  x, y  is also biquadratic, the integrand in Eq.  a  is a polynomial of degree 4 in both ξ and η. Thus third-order integration is sufﬁcient for an “exact” result.  4  4  1 −1  1 −1   239  ∗6.5 Multiple Integrals  ! usr bin python   example 6_15  from gaussQuad2 import *  import numpy as np  def f x,y : return   x - 2.0 **2 *  y - 2.0 **2   x = np.array [0.0, 4.0, 4.0, 1.0]   y = np.array [0.0, 1.0, 4.0, 3.0]   m = eval input "Integration order ==> "    print "Integral =", gaussQuad2 f,x,y,m    input "\nPress return to exit"   Running this program produced the following result:  Integration order ==> 3  Integral = 11.3777777778  Quadrature over a Triangular Element  A triangle may be viewed as a degenerate quadrilateral with two of its corners occupy- ing the same location, as illustrated in Figure 6.8. Therefore, the integration formulas over a quadrilateral region can also be used for a triangular element. However, it is computationally advantageous to use integration formulas specially developed for triangles, which we present without derivation.6  3 4  1  y  1  2  2  3  x A2 A1 P A3  Figure 6.8. Degenerate quadrilateral.  Figure 6.9. Triangular element.  Consider the triangular element in Figure 6.9. Drawing straight lines from the point P in the triangle to each of the corners, we divide the triangle into three parts with areas A 1, A 2, and A 3. The so-called area coordinates of P are deﬁned as  αi = Ai A  ,  i = 1, 2, 3   6.47   6 The triangle formulas are extensively used in the ﬁnite method analysis. See, for example,  Zienkiewicz, O.C. and Taylor, R.L., The Finite Element Method, Vol. 1, 4th ed., McGraw-Hill, 1989.   240  Numerical Integration where A is the area of the element. Because A 1 + A 2 + A 3 = A, the area coordinated are related by   6.48    6.49   Note that αi ranges from 0  when P lies on the side opposite to corner i  to 1  when P is at corner i .  A convenient formula of computing A from the corner coordinates  xi, yi  is  α1 + α2 + α3 = 1   cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  1  x1 y1  A = 1 2  1 x2 y2  1 x3 y3   cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  y α1, α2, α3  = 3 cid:6   cid:6   i=1  x α1, α2, α3  = 3 cid:6  4 4  i=1  The area coordinates are mapped onto the Cartesian coordinates by  αi xi  αi yi   6.50   The integration formula over the element is  f [x α , y α ] dA = A  k  Wk f [x αk , y αk ]   6.51   where αk represents the area coordinates of the integration point k, and Wk are the weights. The locations of the integration points are shown in Figure 6.10, and the corresponding values of αk and Wk are listed in Table 6.7. The quadrature in Eq.  6.51  is exact if f  x, y  is a polynomial of the degree indicated.  a  a  b  b  a  c  d  Figure elements.   a  Linear   b  Quadratic   c  Cubic  6.10. Integration points for  triangular  A  c  Point  Degree of f  x, y   a  Linear  b  Quadratic   c  Cubic  αk  Wk 1 3, 1 3, 1 3 1 1 2, 0 , 1 2 1 3 1 2, 1 2, 0 1 3 0, 1 2 , 1 2 1 3 1 3, 1 3, 1 3 −27 48 25 48 1 5, 1 5, 3 5 3 5. 1 5 , 1 5 25 48 25 48 1 5, 3 5 , 1 5  a a b c a b c d  Table 6.7. Nodes and weights for quadrature over a triangle.   241  ∗6.5 Multiple Integrals   cid:2  triangleQuad  3 3  The function triangleQuad computes A f  x, y  dx dy over a triangular region using the cubic formula—case  c  in Figure 6.10. The triangle is deﬁned by its corner coordinate arrays xc and yc, where the coordinates are listed in a counterclockwise order around the triangle.   module triangleQuad  ’’’ integral = triangleQuad f,xc,yc .  Integration of f x,y  over a triangle using  the cubic formula.  {xc},{yc} are the corner coordinates of the triangle.  ’’’  import numpy as np  def triangleQuad f,xc,yc :  alpha = np.array [[1 3, 1 3.0, 1 3],  \  [0.2, 0.2, 0.6],  [0.6, 0.2, 0.2],  [0.2, 0.6, 0.2]]   \  \  W = np.array [-27 48,25 48,25 48,25 48]   x = np.dot alpha,xc   y = np.dot alpha,yc   A =  xc[1]*yc[2] - xc[2]*yc[1]  - xc[0]*yc[2] + xc[2]*yc[0]  \  \  + xc[0]*yc[1] - xc[1]*yc[0]  2.0  sum = 0.0  for i in range 4 :  sum = sum + W[i] * f x[i],y[i]   return A*sum  EXAMPLE 6.16  y  1  1  2  3  x  3  Evaluate I =3 3  A f  x, y  dx dy over the equilateral triangle shown, where7  f  x, y  = 1 2   x2 + y 2  − 1 6   x3 − 3xy 2  − 2 3  Use the quadrature formulas for  1  a quadrilateral and  2  a triangle.  7 This function is identical to the Prandtl stress function for torsion of a bar with the cross section shown; the integral is related to the torsional stiffness of the bar. See, for example Timoshenko, S.P and Goodier, J.N.,Theory of Elasticity, 3rd ed., McGraw-Hill, 1970.   242  Numerical Integration  3, − √  Solution of Part  1 . Let the triangle be formed by collapsing corners 3 and 4 of a  quadrilateral. The corner coordinates of this quadrilateral are x = cid:28  − 1, −1, 2, 2  cid:29 T and y = cid:28 √  cid:29 T . To determine the minimum required integration order for an exact result, we must examine f [x ξ, η , y ξ, η ] J  ξ, η , the integrand in Eqs.  6.44 . SinceJ  ξ, η  is biquadratic, and f  x, y  is cubic in x, the integrand is a polyno- mial of degree ﬁve in x. Therefore, third-order integration is sufﬁcient. The program used for the computations is similar to the one in Example 6.15:  3, 0, 0  ! usr bin python   example6_16a  from gaussQuad2 import *  import numpy as np  import math  def f x,y :  return  x**2 + y**2  2.0  -  x**3 - 3.0*x*y**2  6.0  - 2.0 3.0  \  \  x = np.array [-1.0,-1.0,2.0,2.0]   y = np.array [math.sqrt 3.0 ,-math.sqrt 3.0 ,0.0,0.0]   m = eval input "Integration order ==> "    print "Integral =", gaussQuad2 f,x,y,m    input "\nPress return to exit"   Solution of Part  2 . The following program uses triangleQuad:  Here is the output:  Integration order ==> 3  Integral = -1.55884572681  ! usr bin python   example6_16b  import numpy as np  import math  from triangleQuad import *  def f x,y :  return  x**2 + y**2  2.0  \  - x**3 - 3.0*x*y**2  6.0 \  -2.0 3.0  xCorner = np.array [-1.0, -1.0, 2.0]   yCorner = np.array [math.sqrt 3.0 , -math.sqrt 3.0 , 0.0]   print "Integral =",triangleQuad f,xCorner,yCorner    input "Press return to  exit"    243  ∗6.5 Multiple Integrals  Because the integrand is a cubic, this quadrature is also exact, the result being  Integral = -1.55884572681  Note that only four function evaluations were required when using the triangle  formulas. In contrast, the function had to be evaluated at nine points in part  1 .  EXAMPLE 6.17 The corner coordinates of a triangle are  0, 0 ,  16, 10 , and  12, 20 . Compute  3 3   cid:30   A   cid:31   x2 − y 2  dx dy over this triangle.  y  12  a  b  4 c 10  10 x  Solution. Because f  x, y  is quadratic, quadrature over the three integration points shown in Figure 6.10 b  is sufﬁcient for an “exact” result. Noting that the integration points lie in the middle of each side, their coordinates are  6, 10 ,  8, 5 , and  14, 15 . The area of the triangle is obtained from Eq.  6.49 :  From Eq.  6.51  we get I = A   cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  1  x1 y1   cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  = 1  2   cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  1  1 x2 y2  1 x3 y3  1  1 0 16 12 0 10 20   cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  = 100  A = 1 2  Wk f  xk, yk   c cid:6  ’ 1  cid:28  3  62 − 102  +  82 − 52  +  142 − 152   f  6, 10  + 1 3  f  8, 5  + 1 3  f  14, 15      k=a = 100  = 100 3   cid:29  = 1800  PROBLEM SET 6.3  1. Use Gauss-Legendre quadrature to compute  2. Evaluate the following integral with Gauss-Legendre quadrature:  2 y=0 3. Compute the approximate value of  3 x=0  4  4  1 −1  1 −1   1 − x2  1 − y 2  dx dy 4  4  x2y 2 dx dy  4  4  1 −1  1 −1  − x2+y 2  dx dy  e   244  Numerical Integration  with Gauss-Legendre quadrature. Use integration order  a  two and  b  three.  The “exact” value of the integral is 2.230 985.   4. Use third-order Gauss-Legendre quadrature to obtain an approximate value of  4  4  1 −1  1 −1  cos  π x − y   2  dx dy   The “exact” value of the integral is 1.621 139.   Map the integral dard” rectangle and then evaluate it analytically.  A xy dx dy from the quadrilateral region shown to the “stan-  5.  6.  7.  3 3  3 3  4  y  4  2  x  y  4  4  x  2  3  y  4  2  y  4  Compute into the “standard” rectangle and then integrating analytically.  A x dx dy over the quadrilateral region shown by ﬁrst mapping it  3 x  3 3  Use quadrature to compute  3 3 A x3 dx dy over the triangle shown in Prob. 7.  A x2 dx dy over the triangle shown.  8. Evaluate 9.  x  3  3 3 A  3 − x y dx dy over the region shown. Treat the  Use quadrature to evaluate region as  a  a triangular element and  b  a degenerate quadrilateral.  3 3  10. Evaluate  A x2y dx dy over the triangle shown in Prob. 9.   245  ∗6.5 Multiple Integrals  11.  cid:2   3  3 3 3 3 A xy 2 − x2  2 − xy  dx dy over the region shown. A xy exp −x2  dx dy over the region shown in Prob. 11 to four dec- 12.  cid:2  Compute imal places.  Evaluate  1  Evaluate  3 3 3 3 A  1 − x  y − x y dx dy over the triangle shown. 14.  cid:2  Estimate 3 3 A sin πx dx dy over the region shown in Prob. 13. Use the cubic A sin πx sin π y − x  dx dy to six decimal places, where A is the tri- angular region shown in Prob. 13. Consider the triangle as a degenerate quadri- lateral.  integration formula for a triangle.  The exact integral is 1 π.   15.  cid:2  Compute  13.  cid:2   16.  cid:2   y  1  3  2  2 1  x  y  1  x  1  1  x  y  1  1  3 3  Write a program to evaluate A f  x, y  dx dy over an irregular region that has been divided into several triangular elements. Use the program to compute  3 3  A xy y − x  dx dy over the region shown.   7  Initial Value Problems  Solve y   cid:3  = F x, y  with the auxiliary conditions y a  = α  7.1  Introduction  The general form of a ﬁrst-order differential equation is   cid:3  = f  x, y   y   7.1a   cid:3  = dy dx and f  x, y  is a given function. The solution of this equation con- where y tains an arbitrary constant  the constant of integration . To ﬁnd this constant, we must know a point on the solution curve; that is, y must be speciﬁed at some value of x, say at x = a. We write this auxiliary condition as  An ordinary differential equation of order n  y a  = α   cid:30    cid:31   y  n  = f  x, y, y   cid:3   , . . . , y  n−1   can always be transformed into n ﬁrst-order equations. Using the notation  y0 = y  y1 = y   cid:3   y2 = y   cid:3  cid:3   . . .  yn−1 = y  n−1   the equivalent ﬁrst-order equations are = y3  = y1  = y2  y  y  y   cid:3  0   cid:3  1   cid:3  2  . . .   cid:3  y n  = f  x, y0, y1, . . . , yn−1    7.4a   The solution now requires the knowledge of n auxiliary conditions. If these con- ditions are speciﬁed at the same value of x, the problem is said to be an initial value problem. Then the auxiliary conditions, called initial conditions, have the form  y0 a  = α0  y1 a  = α1  . . .  yn−1 a  = αn−1   7.4b   If yi are speciﬁed at different values of x, the problem is called a boundary value problem.   7.1b    7.2    7.3   246   247  7.2 Euler’s Method  For example,   cid:3  cid:3  = −y  y  y 0  = 1   cid:3    0  = 0  y  is an initial value problem because both auxiliary conditions imposed on the solution are given at x = 0. In contrast,   cid:3  cid:3  = −y  y  y 0  = 1  y π  = 0  is a boundary value problem because the two conditions are speciﬁed at different values of x.  In this chapter we consider only initial value problems. Boundary value prob- lems, which are more difﬁcult to solve, are discussed in the next chapter. We also make extensive use of vector notation, which allows us to manipulate sets of ﬁrst- order equations in a concise form. For example, Eqs.  7.4  are written as  where   cid:3  = F x, y   y  F x, y  =  ⎡ ⎢⎢⎢⎢⎣  y a  = α ⎤ ⎥⎥⎥⎥⎦  y1 y2 ... f  x, y    7.5a    7.5b   A numerical solution of differential equations is essentially a table of x- and y-values listed at discrete intervals of x.  7.2  Euler’s Method  Euler’s method of solution is conceptually simple. Its basis is the truncated Taylor series of y about x:  y x + h  ≈ y x  + y   cid:3    7.6  Because Eq.  7.6  predicts y at x + h from the information available at x, it can be used to move the solution forward in steps of h, starting with the given initial values of x and y.   x h  The error in Eq.  7.6  caused by truncation of the Taylor series is given by Eq.  A4 :  E = 1 2   cid:3  cid:3    ξ h2 = O h2 , x < ξ < x + h  y  A rough idea of the accumulated error Eacc can be obtained by assuming that per- step error is constant over the period of integration. Then after n integration steps covering the interval x0 to xn we have  Eacc = nE = xn − x0  h  E = O cid:30   h   cid:31   Hence the accumulated error is one order less than the per-step error.   7.7    7.8    248  Initial Value Problems  y' x   Error  f x,y   x  x  x + h  Euler's formula  Figure 7.1. Graphical representation of Euler’s formula.  Let us now take a look at the graphical interpretation of Euler’s equation. For the sake of simplicity, we assume that there is a single dependent variable y, so that the  cid:3  = f  x, y . The change in the solution y between x and x + h differential equation is y is  y x + h  − y h  =   cid:3   dx =  y  f  x, y dx  4  x+h  x   cid:3   4  x  x+h   x  plot, shown in Figure 7.1. Euler’s formula which is the area of the panel under the y approximates this area by the area of the cross-hatched rectangle. The area between the rectangle and the plot represents the truncation error. Clearly, the truncation er- ror is proportional to the slope of the plot; that is, proportional to  y   cid:3  = y     x .   cid:3  cid:3    cid:3   Euler’s method is seldom used in practice because of its computational inefﬁ- ciency. Suppressing the truncation error to an acceptable level requires a very small h, resulting in many integration steps accompanied by an increase in the roundoff error. The value of the method lies mainly in its simplicity, which facilitates the dis- cussion of certain important topics, such as stability.   cid:2  euler  This function implements Euler’s method of integration. It can handle any number of ﬁrst-order differential equations. The user is required to supply the function F x,y  that speciﬁes the differential equations in the form of the array  ⎤ ⎥⎥⎥⎥⎦  ⎡ ⎢⎢⎢⎢⎣   cid:3  y 0  cid:3  y 1  cid:3  y 2 ...  F x, y  =  The function returns the arrays X and Y that contain the values of x and y at intervals h.   module euler  ’’’ X,Y = integrate F,x,y,xStop,h .  Euler’s method for solving the  initial value problem {y}’ = {F x,{y} }, where  {y} = {y[0],y[1],...y[n-1]}.  x,y  = initial conditions  xStop = terminal value of x  h  = increment of x used in integration   249  7.2 Euler’s Method  F  = user-supplied function that returns the  array F x,y  = {y’[0],y’[1],...,y’[n-1]}.  ’’’  import numpy as np  def integrate F,x,y,xStop,h :  X = []  Y = []  X.append x   Y.append y   while x < xStop:  h = min h,xStop - x   y = y + h*F x,y   x = x + h  X.append x   Y.append y   return np.array X ,np.array Y    cid:2  printSoln  We use this function to print X and Y obtained from numerical integration. The amount of data is controlled by the parameter freq. For example, if freq = 5, ev- ery ﬁfth integration step would be displayed. If freq = 0, only the initial and ﬁnal values will be shown.   module printSoln  ’’’ printSoln X,Y,freq .  Prints X and Y returned from the differential  equation solvers using printout frequency ’freq’.  freq = n prints every nth step.  freq = 0 prints initial and final values only.  ’’’  def printSoln X,Y,freq :  def printHead n :  print "\n  x  ",end=" "   for i in range  n :  print "  y[",i,"] ",end=" "   print    def printLine x,y,n :  print "{:13.4e}".format x ,end=" "   for i in range  n :  print "{:13.4e}".format y[i] ,end=" "   print     250  Initial Value Problems  m = len Y   try: n = len Y[0]   except TypeError: n = 1  if freq == 0: freq = m  printHead n   for i in range 0,m,freq :  printLine X[i],Y[i],n   if i != m - 1: printLine X[m - 1],Y[m - 1],n   EXAMPLE 7.1 Integrate the initial value problem  in steps of h = 0.01 from x = 0 to 0.03. Also compute the analytical solution   cid:3  + 4y = x2  y  y 0  = 1  y = 31 32  e  −4x + 1 4  x2 − 1 8  x + 1 32  and the accumulated truncation error at each step.  Solution. It is convenient to use the notation  xi = ih  yi = y xi   so that Euler’s formula takes the form  yi+1 = yi + y  cid:3  i h   cid:3  y i  = x2  i  − 4yi  where  Step 1.  x0 = 0 to x1 = 0.01 : y0 = 0 = x2  cid:3  y 0 y1 = y0 + y  0  Step 2.  x1 = 0.01 to x2 = 0.02 :  − 4y0 = 02 − 4 1  = −4  0h = 1 +  −4  0.01  = 0.96  cid:3  0.01 + 1 −4 0.01  + 1 32 4  0.012 − 1 8 Eacc = 0.96 − 0.9608 = −0.0008  = 31 32  e   y1 exact  = 0.9608  = x2   cid:3  y 1 y2 = y1 + y  1  − 4y1 = 0.012 − 4 0.96  = −3.840  1h = 0.96 +  −3.840  0.01  = 0.9216  cid:3  −4 0.02  + 1 = 0.9231 4  0.02 + 1 32  0.022 − 1 8   y2 exact  = 31 32  e  Eacc = 0.9216 − 0.9231 = −0.0015   251  7.2 Euler’s Method Step 3.  x2 = 0.02 to x3 = 0.03 :  = x2   cid:3  y 2 y3 = y2 + y  2  − 4y2 = 0.022 − 4 0.9216  = −3.686  2h = 0.9216 +  −3.686  0.01  = 0.8847  cid:3  = 0.8869 −4 0.03  + 1 4  0.03 + 1 32  0.032 − 1 8   y3 exact  = 31 32  e  Eacc = 0.8847 − 0.8869 = −0.0022  We note that the magnitude of the per-step error is roughly constant at 0.008. Thus after 10 integration steps the accumulated error would be approximately 0.08, thereby reducing the solution to one signiﬁcant ﬁgure accuracy. After 100 steps all signiﬁcant ﬁgures would be lost.  EXAMPLE 7.2 Integrate the initial value problem  cid:3  cid:3  = −0.1y  y   cid:3  − x  y 0  = 0   cid:3    0  = 1  y  from x = 0 to 2 with Euler’s method using h = 0.05. Plot the computed y together with the analytical solution,  Solution. With the notation y0 = y and y1 = y and the initial conditions are.  y = 100x − 5x2 + 990 e −0.1x − 1   cid:3  + *  *  +  F x, y  =   cid:3  0  cid:3  1  y y  =  y1  −0.1y1 − x  *  +  0 1  y 0  =  Here is a program that uses the function euler:  the equivalent ﬁrst-order equations  ! usr bin python   example7_2  import numpy as np  from euler import *  import matplotlib.pyplot as plt  def F x,y :  F = np.zeros 2   F[0] = y[1]  F[1] = -0.1*y[1] - x  return F  x = 0.0  xStop = 2.0   Start of integration   End of integration  y = np.array [0.0, 1.0]    Initial values of {y}  h = 0.05   Step size   252  Initial Value Problems  X,Y = integrate F,x,y,xStop,h   yExact = 100.0*X - 5.0*X**2 + 990.0* np.exp -0.1*X  - 1.0   plt.plot X,Y[:,0],’o’,X,yExact,’-’   plt.grid True   plt.xlabel ’x’ ; plt.ylabel ’y’   plt.legend  ’Numerical’,’Exact’ ,loc=0   plt.show    input "Press return to exit"   The resulting plot is  The initial portion of the plot is almost a straight line. Because the truncation error in the numerical solution is proportional to y , the discrepancy between the two so- lutions is small. As the curvature of the plot increases, so does the truncation error. You may want to see the effect of changing h by running the program with h =   cid:3  cid:3   0.025. Doing so should halve the truncation error.  7.3  Runge-Kutta Methods  Euler’s method is classiﬁed as a ﬁrst-order method because its cumulative truncation error behaves as O h . Its basis was the truncated Taylor series  y x + h  = y x  + y   cid:3    x h   253  7.3 Runge-Kutta Methods  The accuracy of numerical integration can be greatly improved by keeping more terms of the series. Thus a nth order method would use the truncated Taylor series  y x + h  = y x  + y  cid:3    x h + 1 2!   cid:3  cid:3  y   x h2 + ··· + 1 n!  cid:3  cid:3    cid:3  cid:3  cid:3   y n  x hn  , . . . y n  by repeatedly differenti- But now we must derive the expressions for y  cid:3  = F x, y  and write subroutines to evaluate them. This extra work can be ating y avoided by using Runge-Kutta methods that are also based on truncated Taylor se- ries, but do not require computation of higher derivatives of y x .  , y   a    b   Second-Order Runge-Kutta Method  To arrive at the second-order Runge-Kutta method, we assume an integration for- mula of the form  y x + h  = y x  + c0F x, y h + c1F   cid:28    cid:29  x + ph, y + qhF x, y   h  and attempt to ﬁnd the parameters c0, c1, p, and q by matching Eq.  a  to the Taylor series:  Noting that   cid:3  F   x, y  = ∂F ∂x  y x + h  = y x  + y   x h2   x, y h2   cid:3    cid:3    cid:3  cid:3   F   x h + 1 y 2! = y x  + F x, y h + 1 2 + n−1 cid:6  + n−1 cid:6  + n−1 cid:6   = ∂F ∂x ,  ∂F ∂yi  i=0  i=0   cid:3  y i  ∂F ∂yi  Fi x, y   -  where n is the number of ﬁrst-order equations, Eq. b  can be written as  y x + h  = y x  + F x, y h + 1 2  ∂F ∂x  ∂F ∂yi  i=0  Fi x, y   h2    c   Returning to Eq.  a , we can rewrite the last term by applying Taylor series in  several variables,   cid:28   x + ph, y + qhF x, y   F  so that Eq.  a  becomes   cid:29  = F x, y  + ∂F *  ∂x  ph + qh  ∂F ∂yi  Fi x, y   n−1 cid:6   i=1  n−1 cid:6   y x + h  = y x  +  c0 + c1  F x, y h + c1  i=1 Comparing Eqs.  c  and  d , we ﬁnd that they are identical if  ph + qh  ∂F ∂x  ∂F ∂yi  +  Fi x, y   h   d   c0 + c1 = 1  c1p = 1 2  c1q = 1 2   e   Because Eqs.  e  represent three equation in four unknown parameters, we can as- sign any value to one of the parameters. Some of the popular choices and the names   254  Initial Value Problems  associated with the resulting formulas are as follows:  c0 = 0 c0 = 1 2 c0 = 1 3  c1 = 1 c1 = 1 2 c1 = 2 3  p = 1 2 p = 1 p = 3 4  q = 1 2 Modiﬁed Euler’s method q = 1 Heun’s method q = 3 4 Ralston’s method  All these formulas are classiﬁed as second-order Runge-Kutta methods, with no for- mula having a numerical superiority over the others. Choosing the modiﬁed Euler’s method, substitution of the corresponding parameters into Eq.  a  yields  ’     y x + h  = y x  + F  x + h 2  , y + h 2  F x, y   h  This integration formula can conveniently evaluated by the following sequence of operations:   f    7.9   % K0 = hF x, y  K1 = hF  x + h 2 y x + h  = y x  + K1  &  , y + 1 2  K0  Second-order methods are not popular in computer application. Most programmers prefer integration formulas of order four, which achieve a given accuracy with less computational effort.  y' x   h 2  h 2    x + h 2  f  y +,    0K 2  f x,y   x  x + h  x  Figure 7.2. Graphical representation of modiﬁed Euler’s formula.  Figure 7.2 displays the graphical interpretation of the modiﬁed Euler’s formula  cid:3  = f  x, y . The ﬁrst of Eqs.  7.9  yields an estimate for a single differential equation y of y at the midpoint of the panel by Euler’s formula: y x + h 2  = y x  + f  x, y h 2 = y x  + K0 2. The second equation then approximates the area of the panel by the  cid:30   cid:3  cid:31  cid:3  cid:3  = y area K1 of the cross-hatched rectangle. The error here is proportional to the curvature y  of the plot.   cid:3  cid:3  cid:3   Fourth-Order Runge-Kutta Method  The fourth-order Runge-Kutta method is obtained from the Taylor series along the same lines as the second-order method. Because the derivation is rather long and not very instructive, we shall skip it. The ﬁnal form of the integration formula again depends on the choice of the parameters; that is, there is no unique Runge- Kutta fourth-order formula. The most popular version, which is known simply as   255  7.3 Runge-Kutta Methods  the Runge-Kutta method, entails the following sequence of operations:  & &  % K0 = hF x, y  K1 = hF %  x + h 2 x + h 2  , y + K0 2 , y + K1 K2 = hF 2 K3 = hF x + h, y + K2    7.10   y x + h  = y x  + 1 6   K0 + 2K1 + 2K2 + K3   The main drawback of this method is that is does not lend itself to an estimate of the truncation error. Therefore, we must guess the integration step size h or deter- mine it by trial and error. In contrast, the so-called adaptive methods can evaluate the truncation error in each integration step and adjust the value of h accordingly  but at a higher cost of computation . One such adaptive method is introduced in the next section.   cid:2  run kut4  The function integrate in this module implements the Runge-Kutta method of or- der four. The user must provide integrate with the function F x,y  that deﬁnes the ﬁrst-order differential equations y   cid:3  = F x, y .   module run_kut4  ’’’ X,Y = integrate F,x,y,xStop,h .  4th-order Runge-Kutta method for solving the  initial value problem {y}’ = {F x,{y} }, where  {y} = {y[0],y[1],...y[n-1]}.  x,y  = initial conditions  xStop = terminal value of x  h  F  ’’’  = increment of x used in integration  = user-supplied function that returns the  array F x,y  = {y’[0],y’[1],...,y’[n-1]}.  import numpy as np  def integrate F,x,y,xStop,h :  def run_kut4 F,x,y,h :  K0 = h*F x,y   K1 = h*F x + h 2.0, y + K0 2.0   K2 = h*F x + h 2.0, y + K1 2.0   K3 = h*F x + h, y + K2   return  K0 + 2.0*K1 + 2.0*K2 + K3  6.0   256  Initial Value Problems  X = []  Y = []  X.append x   Y.append y   while x < xStop:  h = min h,xStop - x   y = y + run_kut4 F,x,y,h   x = x + h  X.append x   Y.append y   return np.array X ,np.array Y   EXAMPLE 7.3 Use the second-order Runge-Kutta method to integrate   cid:3  = sin y  y  y 0  = 1  from x = 0 to 0.5 in steps of h = 0.1. Keep four decimal places in the computations.  Solution. In this problem we have  F  x, y  = sin y  so that the integration formulas in Eqs.  7.9  are  %  K0 = hF  x, y  = 0.1 sin y , y + 1 K1 = hF 2  x + h 2 y x + h  = y x  + K1  K 0  &  %  &  = 0.1 sin  y + 1 2  K0  Noting that y 0  = 1, the integration then proceeds as follows:  %  K0 = 0.1 sin 1.0000 = 0.0841 K1 = 0.1 sin 1.0000 + 0.0841  2 y 0.1  = 1.0 + 0.0863 = 1.0863  &  %  K0 = 0.1 sin 1.0863 = 0.0885 1.0863 + 0.0885 K1 = 0.1 sin y 0.2  = 1.0863 + 0.0905 = 1.1768  2  &  = 0.0863  = 0.0905   257  7.3 Runge-Kutta Methods  and so on. A summary of the computations is shown in the following table.  x 0.0 0.1 0.2 0.3 0.4 0.5  y  1.0000 1.0863 1.1768 1.2708 1.3676 1.4664  K0  0.0841 0.0885 0.0923 0.0955 0.0979  K1  0.0863 0.0905 0.0940 0.0968 0.0988  The exact solution can be shown to be  x y  = ln csc y − cot y  + 0.604582  which yields x 1.4664  = 0.5000. Therefore, up to this point the numerical solution is accurate to four decimal places. However, it is unlikely that this precision would be maintained if we were to continue the integration. Because the errors  due to trun- cation and roundoff  tend to accumulate, longer integration ranges require better integration formulas and more signiﬁcant ﬁgures in the computations.  EXAMPLE 7.4 Integrate the initial value problem  cid:3  cid:3  = −0.1y  y   cid:3  − x  y 0  = 0   cid:3    0  = 1  y  with the fourth-order Runge-Kutta method from x = 0 to 2 in increments of h = 0.2. Plot the computed y together with the analytical solution −0.1x − 1   y = 100x − 5x2 + 990 e   This problem was solved by Euler’s method in Example 7.2.  * Solution. Letting y0 = y and y1 = y F x, y  =  +  +  *  =  y1   cid:3   −0.1y1 − x   cid:3  0  cid:3  1  y y  , the equivalent ﬁrst-order equations are  Except for two statements, the program shown next is identical to that used in Example 7.2.  ! usr bin python   example7_4  import numpy as np  from printSoln import *  from run_kut4 import *  import matplotlib.pyplot as plt  def F x,y :  F = np.zeros 2   F[0] = y[1]   258  Initial Value Problems  F[1] = -0.1*y[1] - x  return F  x = 0.0  xStop = 2.0   Start of integration   End of integration  y = np.array [0.0, 1.0]    Initial values of {y}  h = 0.2   Step size  X,Y = integrate F,x,y,xStop,h   yExact = 100.0*X - 5.0*X**2 + 990.0* np.exp -0.1*X  - 1.0   plt.plot X,Y[:,0],’o’,X,yExact,’-’   plt.grid True   plt.xlabel ’x’ ; plt.ylabel ’y’   plt.legend  ’Numerical’,’Exact’ ,loc=0   plt.show    input "Press return to exit"    Comparing this plot with that in Example 7.2, we see the clear superiority of the fourth-order Runge-Kutta method over Euler’s method. In this problem, the two methods involve about the same amount of computation. Euler’s method requires one evaluation of F x, y  per step, whereas the Runge-Kutta method involves four evaluations. Yet Euler’s method has four times as many steps. Considering the large   259  7.3 Runge-Kutta Methods  difference in the accuracy of the results, we conclude that the fourth-order Runge- Kutta method offers a much ”bigger bang for a buck.”  EXAMPLE 7.5 Use the fourth-order Runge-Kutta method to integrate y 0  = 1   cid:3  = 3y − 4e  −x  y  from x = 0 to 10 in steps of h = 0.1. Compare the result with the analytical solution y = e  −x.  Solution. We used the program shown next. Recalling that run kut4 expects y to be an array, we speciﬁed the initial value as y = np.array [1.0]  rather than y = 1.0.  ! usr bin python   example7_5  import numpy as np  from run_kut4 import *  from printSoln import *  from math import exp  def F x,y :  F = np.zeros 1   F[0] = 3.0*y[0] - 4.0*exp -x   return F  x = 0.0   Start of integration  xStop = 10.0   End of integration  y = np.array [1.0]    Initial values of {y}  h = 0.1  freq = 20   Step size   Printout frequency  X,Y = integrate F,x,y,xStop,h   printSoln X,Y,freq   input "\nPress return to exit"   is shown :  x  y[ 0 ]  0.0000e+000  1.0000e+000  2.0000e+000  1.3250e-001  4.0000e+000  -1.1237e+000  6.0000e+000  -4.6056e+002  8.0000e+000  -1.8575e+005  1.0000e+001  -7.4912e+007  Running the program produced the following output  every 20th integration step   260  Initial Value Problems  It is clear that something went wrong. According to the analytical solution, y should approach zero with increasing x, but the output shows the opposite trend: After an initial decrease, the magnitude of y increases dramatically. The explanation is found by taking a closer look at the analytical solution. The general solution of the given differential equation is  which can be veriﬁed by substitution. The initial condition y 0  = 1 yields C = 0, so that the solution to the problem is indeed y = e The cause of the trouble in the numerical solution is the dormant term Ce3x. Suppose that the initial condition contains a small error ε, so that we have y 0  = 1 + ε. This changes the analytical solution to  −x.  y = Ce3x + e  −x  y = εe3x + e  −x  We now see that the term containing the error ε becomes dominant as x is increased. Because errors inherent in the numerical solution have the same effect as small changes in initial conditions, we conclude that our numerical solution is the victim of numerical instability due to sensitivity of the solution to initial conditions. The lesson is not to blindly trust the results of numerical integration.  EXAMPLE 7.6  Re  r θ  v0  H  A spacecraft is launched at the altitude H = 772 km above sea level with the speed v0 = 6700 m s in the direction shown. The differential equations describing the mo- tion of the spacecraft are  ¨r = r ˙θ 2 − G Me r 2  ¨θ = − 2˙r ˙θ r  where r and θ are the polar coordinates of the spacecraft. The constants involved in the motion are  −1s  −11 m3 kg  G = 6.672 × 10 Me = 5.9742 × 1024 kg = mass of the earth Re = 6378.14 km = radius of the earth at sea level  −2 = universal gravitational constant   1  Derive the ﬁrst-order differential equations and the initial conditions of the form ˙y = F t, y , y 0  = b.  2  Use the fourth-order Runge-Kutta method to integrate the equations from the time of launch until the spacecraft hits the earth. Determine θ at the impact site.   261  7.3 Runge-Kutta Methods  Solution of Part  1 . We have 6.672 × 10  G Me = cid:30   −11   cid:31  cid:30   Letting  −2   cid:31  = 3.9860 × 1014 m3 s ⎤ ⎥⎥⎥⎦  y0 y1 y2 y3  ⎤ ⎥⎥⎥⎦ =  5.9742 × 1024 ⎡ ⎡ ⎢⎢⎢⎣ ⎢⎢⎢⎣ ⎡ ⎢⎢⎢⎣  y0y 2 3  r ˙r θ ˙θ  y =  ⎤ ⎥⎥⎥⎦ =  the equivalent ﬁrst-order equations become  ⎡ ⎢⎢⎢⎣  ˙y0 ˙y1 ˙y2 ˙y3  F t, y  =  ⎤ ⎥⎥⎥⎦  y1  − 3.9860 × 1014 y 2 −2y1y3 y0  y3  0  and the initial conditions are  r  0  = Re + H =  6378.14 + 772  × 103 = 7.15014 × 106 m ˙r  0  = 0 θ 0  = 0 ˙θ 0  = v0 r  0  =  6700    7.15014 × 106  = 0.937045 × 10  −3 rad s  Therefore,  ⎡ ⎢⎢⎢⎣  y 0  =  ⎤ ⎥⎥⎥⎦  7. 15014 × 106 0 0 0.937045 × 10  −3  Solution of Part  2 . The program used for numerical integration is listed next. Note that the independent variable t is denoted by x. The period of integration xStop  the time when the spacecraft hits  was estimated from a previous run of the program.  ! usr bin python   example7_6  import numpy as np  from run_kut4 import *  from printSoln import *  def F x,y :  F = np.zeros 4   F[0] = y[1]  F[2] = y[3]  F[1] = y[0]* y[3]**2  - 3.9860e14  y[0]**2    y = np.array [7.15014e6, 0.0, 0.0, 0.937045e-3]   262  Initial Value Problems  F[3] = -2.0*y[1]*y[3] y[0]  return F  x = 0.0  xStop = 1200.0  h = 50.0  freq = 2  X,Y = integrate F,x,y,xStop,h   printSoln X,Y,freq   input "\nPress return to exit"   Here is the output:  x  y[ 0 ]  y[ 1 ]  y[ 2 ]  y[ 3 ]  0.0000e+000  7.1501e+006  0.0000e+000  0.0000e+000  9.3704e-004  1.0000e+002  7.1426e+006 -1.5173e+002  9.3771e-002  9.3904e-004  2.0000e+002  7.1198e+006 -3.0276e+002  1.8794e-001  9.4504e-004  3.0000e+002  7.0820e+006 -4.5236e+002  2.8292e-001  9.5515e-004  4.0000e+002  7.0294e+006 -5.9973e+002  3.7911e-001  9.6951e-004  5.0000e+002  6.9622e+006 -7.4393e+002  4.7697e-001  9.8832e-004  6.0000e+002  6.8808e+006 -8.8389e+002  5.7693e-001  1.0118e-003  7.0000e+002  6.7856e+006 -1.0183e+003  6.7950e-001  1.0404e-003  8.0000e+002  6.6773e+006 -1.1456e+003  7.8520e-001  1.0744e-003  9.0000e+002  6.5568e+006 -1.2639e+003  8.9459e-001  1.1143e-003  1.0000e+003  6.4250e+006 -1.3708e+003  1.0083e+000  1.1605e-003  1.1000e+003  6.2831e+006 -1.4634e+003  1.1269e+000  1.2135e-003  1.2000e+003  6.1329e+006 -1.5384e+003  1.2512e+000  1.2737e-003  The spacecraft hits the earth when r equals Re = 6.378 14 × 106 m. This occurs between t = 1000 and 1100 s. A more accurate value of t can be obtained by polyno- mial interpolation. If no great precision is needed, linear interpolation will do. Letting 1000 +  cid:9 t be the time of impact, we can write  r  1000 +  cid:9 t  = Re  Expanding r in a two-term Taylor series, we get  r  1000  + r  6.4250 × 106 + cid:30 −1.3708 × 103   cid:3    cid:31    1000  cid:9 t = Re  x = 6378.14 × 103  from which   cid:9 t = 34.184 s   263  7.3 Runge-Kutta Methods  The coordinate θ of the impact site can be estimated in a similar manner. Using  again two terms of the Taylor series, we have θ 1000 +  cid:9 t  = θ 1000  + θ cid:3   = 1.0083 + cid:30    1000  cid:9 t 1.1605 × 10  −3   cid:31   = 1.0480 rad = 60.00  ◦   34.184   PROBLEM SET 7.1  1. Given   cid:3  + 4y = x2  y  y 0  = 1  compute y 0.03  using two steps of the second-order Runge-Kutta method. Com- pare the result with the analytical solution given in Example 7.1.  2. Solve Prob. 1 with one step of the fourth-order Runge-Kutta method. 3. Integrate  from x = 0 to 0.5 with Euler’s method using h = 0.1. Compare the result with Example 7.3.  4.  cid:2  Verify that the problem   cid:3  = sin y  y  y 0  = 1   cid:3  = y 1 3  y  y 0  = 0  has two solutions: y = 0 and y =  2x 3 3 2. Which of the solutions would be re- produced by numerical integration if the initial condition is set at  a  y = 0 and  b  y = 10 −16? Verify your conclusions by integrating with any numerical method. 5. Convert the following differential equations into ﬁrst-order equations of the  form y   cid:3  = F x, y :   a   b   c   d    cid:3  + y = sin x  cid:3  cid:3  cid:25  ln y  cid:3  − 2y 2 = 0 y − xy  cid:3  cid:3   cid:19  cid:19  2 = cid:19  cid:19 32y  cid:3  cid:3  cid:31   cid:30  y y  4  − 4y 1 − y 2 = 0 x − y 2  cid:3  y  6. In the following sets of coupled differential equations t is the independent vari- able. Convert these equations into ﬁrst-order equations of the form ˙y = F t, y :   cid:30   cid:31 1 4 ¨y = x − 2y ¨y = −y ˙y 2 + ˙x2 ¨y 2 + t sin y = 4 ˙x   cid:30  ¨x = y − x ¨x = −x x ¨x + t cos y = 4 ˙y  ˙y 2 + ˙x   cid:31 1 4 − 32   a   b   c   7.  cid:2  The differential equation for the motion of a simple pendulum is  d2θ dt 2  = − g L  sin θ   264  Initial Value Problems  where  θ = angular displacement from the vertical g = gravitational acceleration L = length of the pendulum  With the transformation τ = t  √  g L the equation becomes  = − sin θ  d2θ dτ 2  ¨y = g − cD m  ˙y 2  y  P t   m  k    ¨y = P t  m  − k m  y  √  L g.  Use numerical integration to determine the period of the pendulum if the am- plitude is θ 0 = 1 rad. Note that for small amplitudes  sin θ ≈ θ  the period is 2π 8.  cid:2  A skydiver of mass m in a vertical free fall experiences an aerodynamic drag force FD = cD ˙y 2, where y is measured downward from the start of the fall. The differential equation describing the fall is  Determine the time of a 5000 m fall. Use g = 9.80665 m s2, C D = 0.2028 kg m, and m = 80 kg.  9.  cid:2   The spring-mass system is at rest when the force P t  is applied, where  P t  =  10t N when t < 2 s 20 N when t ≥ 2 s  The differential equation of the ensuing motion is  Determine the maximum displacement of the mass. Use m = 2.5 kg and k = 75 N m.  10.  cid:2   Water level  r  h   265  7.3 Runge-Kutta Methods  The equilibrium position of the ﬂoating cylinder is h = r . If the cylinder is dis- placed to the position h = 1.5r and released, the differential equation describing its motion is  *  ¨y = 2  π  −1  tan   cid:25   1 − y 2y − y 2   cid:25  2y − y 2 +  1 − y   +  where y = h r . Plot h r from t = 0 to 6 s. Use the plot to estimate the period of the motion.  11.  cid:2  Solve the differential equation  with the initial conditions y 0  = 2.0, 2.5, 3.0, and 3.5. Plot the solutions in the range 0 ≤ x ≤ 10.  12.  cid:2    cid:3  = sin xy  y  2   m  r  θ  t   The system consisting of a sliding mass and a guide rod is at rest with the mass at r = 0.75 m. At time t = 0 a motor is turned on that imposes the motion θ t  =  π  12  cos πt on the rod. The differential equation describing the resulting motion of the slider is ¨r =  r sin2 πt − g sin  1 π  &2  cos πt  %  2  π 2 12  Determine the time when the slider reaches the tip of the rod. Use g = 9.806 65 m s2.  13.  cid:2   12  x  y m  0v 30o  R  A ball of mass m = 0.25 kg is launched with the velocity v0 = 50 m s in the di- rection shown. Assuming that the aerodynamic drag force acting on the ball is FD = C Dv3 2, the differential equations describing the motion are  ¨x = − C D m  ˙xv1 2  ¨y = − C D m  ˙yv1 2 − g   cid:25   where v = kg  m· s 1 2 and g = 9.806 65 m s2.  ˙x2 + ˙y 2. Determine the time of ﬂight and the range R. Use C D = 0.03   266  Initial Value Problems  14.  cid:2  The differential equation describing the angular position θ of a mechanical  −2 and b = 15. If θ 0  = 2π and ˙θ 0  = 0, compute θ and ˙θ when  arm is  where a = 100 s t = 0.5 s.  15.  cid:2   ¨θ = a b − θ  − θ ˙θ 2  1 + θ 2  L = undeformed length k = stiffness  θr  m  The mass m is suspended from an elastic cord with an extensional stiffness k and undeformed length L. If the mass is released from rest at θ = 60 ◦ with the cord unstretched, ﬁnd the length r of the cord when the position θ = 0 is reached for the ﬁrst time. The differential equations describing the motion are   r − L   ¨r = r ˙θ 2 + g cos θ − k m ¨θ = −2˙r ˙θ − g sin θ  r  Use g = 9.806 65 m s2, k = 40 N m, L = 0.5 m, and m = 0.25 kg. 16.  cid:2  Solve Prob. 15 if the mass is released from the position θ = 60  ◦  with the cord  stretched by 0.075 m.  17.  cid:2   y  k  m  μ  ¨y = − k m  y − μg  ˙y ˙y  Consider the mass-spring system where dry friction is present between the block and the horizontal surface. The frictional force has a constant magnitude μmg  μ is the coefﬁcient of friction  and always opposes the motion. The differential equation for the motion of the block can be expressed as  where y is measured from the position where the spring is unstretched. If the block is released from rest at y = y0, verify by numerical integration that the next positive peak value of y is y0 − 4μmg k  this relationship can be derived analyt- ically . Use k = 3000 N m, m = 6 kg, μ = 0.5, g = 9.80665 m s2, and y0 = 0.1 m.   267  7.3 Runge-Kutta Methods 18.  cid:2  Integrate the following problems from x = 0 to 20 and plot y vs. x:   cid:3  cid:3  + 0.5 y 2 − 1  + y = 0  cid:3  cid:3  = y cos 2x   a  y  b  y  y 0  = 1 y 0  = 0   cid:3   cid:3    0  = 0  0  = 1  y y  These differential equations arise in nonlinear vibration analysis.  % 19.  cid:2  The solution of the problem  &  y   cid:3  cid:3  + 1 x   cid:3  +  y  1 − 1 x2  y  y 0  = 0   cid:3    0  = 1  y  is the Bessel function J1 x . Use numerical integration to compute J1 5  and compare the result with −0.327 579, the value listed in mathematical tables. Hint: To avoid singularity at x = 0, start the integration at x = 10  −12.  20.  cid:2  Consider the following initial value problem: y 0  = 1.0   cid:3  cid:3  = 16.81y  y   cid:3    0  = −4.1  y   a  Derive the analytical solution.  b  Do you anticipate difﬁculties in numerical solution of this problem?  c  Try numerical integration from x = 0 to 8 to see if your concerns were justiﬁed.  21.  cid:2   Kirchoff’s equations for the circuit shown are  2R  i2 E t   i1  R  i1  L  R  C  i2  L  di1 dt q2 C  + Ri1 + 2R i1 + i2  = E t  + Ri2 + 2R i2 + i1  = E t   = −3Ri1 − 2Ri2 + E t  + 1 = − 2 3R 3  − i2 3RC  di1 dt  L  di1 dt di2 dt  d E dt   a    b    c    d   where i1 and i2 are the loop currents, and q2 is the charge of the condenser. Differentiating Eq.  b  and substituting the charge-current relationship dq2 dt = i2, we get  We could substitute di1 dt from Eq.  c  into Eq.  d , so that the latter would as- sume the usual form di2 dt = f  t, i1, i2 , but it is more convenient to leave the equations as they are. Assuming that the voltage source is turned on at timet = 0, plot the loop currents ii and i2 from t = 0 to 0.05 s. Use E t  = 240 sin 120πt  V, R = 1.0  cid:7 , L = 0.2 × 10  −3 H, and C = 3.5 × 10  −3 F.   268  Initial Value Problems  22.  cid:2   i1  E  i2 C  i1  C  i2  L  R  L  R  The constant voltage source of the circuit shown is turned on at t = 0, causing transient currents i1 and i2 in the two loops that last about 0.05 s. Plot these currents from t = 0 to 0.05 s, using the following data: E = 9 V, R = 0.25  cid:7 , −3 H, and C = 5 × 10 L = 1.2 × 10 −3 F. Kirchoff’s equations for the two loops are + Ri1 + q1 − q2 di1 C dt + q2 C  + Ri2 + q2 − q1  = E = 0  di2 dt  C  L  L  Two additional equations are the current-charge relationships:  = i1  dq1 dt  = i2  dq2 dt  4  23. Print a table of the sine integral  Si x  = from x = 0 to 3.6 in increments of 0.2.  x  sint  0  t  dt  7.4  Stability and Stiffness  Loosely speaking, a method of numerical integration is said to be stable if the effects of local errors do not accumulate catastrophically; that is, if the global error remains bounded. If the method is unstable, the global error will increase exponentially, even- tually causing numerical overﬂow. Stability has nothing to do with accuracy; in fact, an inaccurate method can be very stable.  Stability is determined by three factors: the differential equations, the method of solution, and the value of the increment h. Unfortunately, it is not easy to determine stability beforehand, unless the differential equation is linear.  Stability of Euler’s Method  As a simple illustration of stability, consider the linear problem   cid:3  = −λy  y  y 0  = β   7.11    269  7.4 Stability and Stiffness  where λ is a positive constant. The analytical solution of this problem is  y x  = βe  −λx  Let us now investigate what happens when we attempt to solve Eq.  7.11  numer-  ically with Euler’s formula,  Substituting y   cid:3    x  = −λy x , we get  y x + h  = y x  + hy   cid:3    x    cid:19  cid:19 1 − λh   cid:19  cid:19  > 1, the method is clearly unstable because y increases in every integra-  y x + h  =  1 − λh y x    cid:19  cid:19 1 − λh   cid:19  cid:19  ≤ 1, or  If tion step. Thus Euler’s method is stable only if h ≤ 2 λ  The results can be extended to a system of n differential equations of the form  where  cid:2  is a constant matrix with the positive eigenvalues λi, be shown that Euler’s method of integration is stable if   cid:3  = − cid:2 y y   7.14  i = 1, 2, . . . , n. It can  h < 2 λmax   7.15    7.12    7.13   where λmax is the largest eigenvalue of  cid:2 .  Stiffness  equations is y x  = cid:27   An initial value problem is called stiff if some terms in the solution vector y x  vary much more rapidly with x than others. Stiffness can be easily predicted for the differ-  cid:3  = − cid:2 y with constant coefﬁcient matrix  cid:2 . The solution of these ential equations y i Civi exp −λi x , where λi are the eigenvalues of  cid:2  and vi are the corresponding eigenvectors. It is evident that the problem is stiff if there is a large disparity in the magnitudes of the positive eigenvalues.  Numerical integration of stiff equations requires special care. The step size h needed for stability is determined by the largest eigenvalue λmax, even if the terms exp −λmaxx  in the solution decay very rapidly and become insigniﬁcant as we move away from the origin.  For example, consider the differential equation1  Using y0 = y and y1 = y1, the equivalent ﬁrst-order equations are   7.16   y   cid:3  cid:3  + 1001y *   cid:3  + 1000y = 0 +   cid:3  =  y  y1  −1000y0 − 1001y1  1 This example is taken from C.E. Pearson, Numerical Methods in Engineering and Science, van  Nostrand and Reinhold, 1986.   270  Initial Value Problems  In this case  The eigenvalues of  cid:2  are the roots of  *  +   cid:2  =  0  −1 1000 1001   cid:19  cid:19  cid:19  cid:19  cid:19  −λ  −1  1000 1001 − λ   cid:19  cid:19  cid:19  cid:19  cid:19  = 0   cid:2  − λI =  Expanding the determinant we get  −λ 1001 − λ  + 1000 = 0  which has the solutions λ1 = 1 and λ2 = 1000. These equations are clearly stiff. Ac- cording to Eq.  7.15  we would need h ≤ 2 λ2 = 0.002 for Euler’s method to be stable. The Runge-Kutta method would have approximately the same limitation on the step size.  When the problem is very stiff, the usual methods of solution, such as the Runge- Kutta formulas, become impractical because of the very small h required for stabil- ity. These problems are best solved with methods that are specially designed for stiff equations. Stiff problem solvers, which are outside the scope of this text, have much better stability characteristics; some are even unconditionally stable. However, the higher degree of stability comes at a cost—the general rule is that stability can be im- proved only by reducing the order of the method  and thus increasing the truncation error .  EXAMPLE 7.7  1  Show that the problem  y   cid:3  cid:3  = − 19 4  y − 10y   cid:3   y 0  = −9   cid:3    0  = 0  y  is moderately stiff and estimate hmax, the largest value of h for which the Runge- Kutta method would be stable.  2  Conﬁrm the estimate by computing y 10  with h ≈ hmax 2 and h ≈ 2hmax. Solution of Part  1 . With the notation y = y0 and y ⎤ differential equations are ⎦ = − cid:2    cid:3  = y1 the equivalent ﬁrst-order *  ⎡ ⎣  +   cid:3  = y  y0 y1  where  − 19 4  y1 y0 − 10y1 ⎡ ⎣ 0 −1  ⎤ ⎦  19 4  10   cid:2  =   271  7.5 Adaptive Runge-Kutta Method  The eigenvalues of  cid:2  are given by   cid:19  cid:19  cid:19  cid:19  cid:19  cid:19 −λ  19 4   cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  = 0  −1 10 − λ   cid:2  − λI =  which yields λ1 = 1 2 and λ2 = 19 2. Because λ2 is quite a bit larger than λ1, the equa- tions are moderately stiff.  Solution of Part  2 . An estimate for the upper limit of the stable range of h can be obtained from Eq.  7.15 :  hmax = 2 λmax  = 2 19 2  = 0.2153  Although this formula is strictly valid for Euler’s method, it is usually not too far off for higher order integration formulas. Here are the results from the Runge-Kutta method with h = 0.1  by specifying  freq = 0 in printSoln, only the initial and ﬁnal values were printed :  x  y[ 0 ]  y[ 1 ]  0.0000e+000  -9.0000e+000  0.0000e+000  1.0000e+001  -6.4011e-002  3.2005e-002  The analytical solution is  y x  = − 19 2  e  −x 2 + 1 2  e  −19x 2  yielding y 10  = −0.0640 11, which agrees with the value obtained numerically.  With h = 0.5 we encountered instability, as expected:  x  y[ 0 ]  y[ 1 ]  0.0000e+000  -9.0000e+000  0.0000e+000  1.0000e+001  2.7030e+020  -2.5678e+021  7.5  Adaptive Runge-Kutta Method  Determination of a suitable step size h can be a major headache in numerical inte- gration. If h is too large, the truncation error may be unacceptable; if h is too small, we are squandering computational resources. Moreover, a constant step size may not be appropriate for the entire range of integration. For example, if the solution curve starts off with rapid changes before becoming smooth  as in a stiff problem , we should use a small h at the beginning and increase it as we reach the smooth re- gion. This is where adaptive methods come in. They estimate the truncation error at each integration step and automatically adjust the step size to keep the error within prescribed limits.  The adaptive Runge-Kutta methods use so-called embedded integration formu- las. These formulas come in pairs: One formula has the integration order m, and the other one is of order m + 1. The idea is to use both formulas to advance the solution   272  Initial Value Problems from x to x + h. Denoting the results by ym x + h  and ym+1 x + h , an estimate of the truncation error in the formula of order m is obtained from E h  = ym+1 x + h  − ym x + h    7.17   What makes the embedded formulas attractive is that they share the points where F x, y  is evaluated. This means that once ym+1 x + h  has been computed, relatively small additional effort is required to calculate ym x + h .  Here are Runge-Kutta formulas of order ﬁve:  j=0  Bij Kj  ⎞ ⎛ K0 = hF x, y  ⎝x + Aih, y + i−1 cid:6  ⎠ , Ki = hF y5 x + h  = y x  + 6 cid:6  y4 x + h  = y x  + 6 cid:6   i=0  i = 1, 2, . . . , 6   7.18   CiKi   7.19a   The embedded fourth-order formula is  DiKi  i=0   7.19b   The coefﬁcients appearing in these formulas are not unique. Table 7.1 gives the coefﬁcients proposed by Dormand and Prince.2 They are claimed to provide superior error prediction than alternative values.  The solution is advanced with the ﬁfth-order formula in Eq.  7.19a . The fourth-  order formula is used only implicitly in estimating the truncation error  E h  = y5 x + h  − y4 x + h  = 6 cid:6    Ci − Di Ki  i=0  Because Eq.  7.20  actually applies to the fourth-order formula, it tends to over- estimate the error in the ﬁfth-order formula.  Note that E h  is a vector, its components Ei h  representing the errors in the dependent variables yi. This brings up the question: What is the error measure e h  that we wish to control? There is no single choice that works well in all problems. If we want to control the largest component of E h , the error measure would be  We could also control some gross measure of the error, such as the root-mean-square error deﬁned by   cid:19  cid:19 Ei h    cid:19  cid:19   e h  = max  i   cid:16  cid:17  cid:17  cid:18  1  n−1 cid:6   n  i=0  ¯E h  =  E 2 i  h    7.20    7.21    7.22   2 Dormand, R.R. and Prince, P.J., Journal of Computational and Applied Mathematics, Vol. 6, p. 1980.   273  7.5 Adaptive Runge-Kutta Method  i  Ai  0 −  3 10  1 5  4 5  1  1  1  2  3  4  5  6  −  1 5  3 40  44 45  9017 3168  35 384  −  −  9 40 − 56 15 − 25360 2187 − 355 33  0  8 9  19372 6561  Bij  −  −  −  32 9  −  −  −  −  −  −  −  −  −  64448 6561  46732 5247  500 1113  − 212 729  49 176  125 192  − 5103 18656 − 2187 6784  11 84  Ci  35 384  0  500 1113  125 192 − 2187 6784  11 84  0  5179 57600  Di  0  7571 16695  393 640  187 2100  1 40  − 92097 339200  Table 7.1. Dormand-Prince coefﬁcients.  where n is the number of ﬁrst-order equations. Then we would use  e h  = ¯E h    7.23   for the error measure. Because the root-mean-square error is easier to handle, we adopt it for our program.  Error control is achieved by adjusting the increment h so that the per-step error e h  is approximately equal to a prescribed tolerance ε. Noting that the truncation error in the fourth-order formula is O h5 , we conclude that  %  &5  ≈  e h1  e h2   h1 h2  ’   1 5  h2 = h1  ε  e h1   Let us suppose that we performed an integration step with h1 that resulted in the error e h1 . The step size h2 that we should have used can now be obtained from Eq.  a  by setting e h2  = ε:  If h2 ≥ h1, we could repeat the integration step with h2, but because the error was below the tolerance, that would be a waste of a perfectly good result. So we accept the current step and try h2 in the next step. However, if h2 < h1, we must scrap the current step and repeat it with h2.   a    b    274  Initial Value Problems  Because Eq.  b  is only a crude approximation, it is prudent to incorporate a small  margin of safety. In our program we use the formula  We also prevent excessively large changes in h by applying the constraints   7.24   ’   1 5  h2 = 0.9h1  ε  e h1   0.1 ≤ h2 h1 ≤ 10  Recall that e h  applies to a single integration step; that is, it is a measure of the lo- cal truncation error. The all-important global truncation error is caused by the accu- mulation of the local errors. What should ε be set at to achieve a global error tolerance εglobal? Because e h  is a conservative estimate of the actual error, setting ε = εglobal is usually adequate. If the number of integration steps is very large, it is advisable to decrease ε accordingly.  Is there any reason to use the nonadaptive methods at all? Usually no—however, there are special cases where adaptive methods break down. For example, adaptive methods generally do not work if F x, y  contains discontinuities. Because the error behaves erratically at the point of discontinuity, the program can get stuck in an in- ﬁnite loop trying to ﬁnd the appropriate value of h. Nonadaptive methods are also handy if the output is to have evenly spaced values of x.   cid:2  run kut5  This module is compatible with run kut4 listed in the previous section. Any pro- gram that calls integrate can choose between the adaptive and the nonadaptive methods by importing either run kut5 or run kut4. The input argument h is the trial value of the increment for the ﬁrst integration step.  Note that K0 is computed from scratch only in the ﬁrst integration step. Subse-  quently, we use  if the mth step was accepted, and  if step m was rejected due to excessive truncation error. To prove Eq.  c , we let i = 6 in Eq.  7.18 , obtaining   K6 m = hmF  B6i Ki m   K0 m+1 = hm+1 hm   K6 m   K0 m+1 = hm+1 hm   K0 m  *  xm + A 6hm, ym + 5 cid:6  *  xm + hm, ym + 5 cid:6   i=0  +  +   K6 m = hmF  Ci Ki m  i=0  Table 6.1 shows that the last row of B-coefﬁcients is identical to the C-coefﬁcients  i.e., B6i = Ci . Also note that A 6 = 1. Therefore,   c    d    e    275  7.5 Adaptive Runge-Kutta Method  But according to Eq.  7.19a  the ﬁfth-order formula is  ym+1 = ym + 6 cid:6   Ci Ki m  i=0  Since C6 = 0  see Table 7.1 , we can reduce the upper limit of the summation from 6 to 5. Therefore, Eq.  e  becomes   K6 m = hmF xm+1, ym+1  = hm hm+1   K0 m+1  which completes the proof. Eqs.  7.18 . Because step m + 1 repeats step m with a different value of h, we have  The validity of Eq.  d  is rather obvious by inspection of the ﬁrst equation of   K0 m = hmF xm, ym    K0 m+1 = hm+1F xm, ym   which leads directly to Eq.  d .   module run_kut5  ’’’ X,Y = integrate F,x,y,xStop,h,tol=1.0e-6 .  Adaptive Runge-Kutta method with Dormand-Prince  coefficients for solving the  initial value problem {y}’ = {F x,{y} }, where  {y} = {y[0],y[1],...y[n-1]}.  x,y  = initial conditions  xStop = terminal value of x  h  F  = initial increment of x used in integration  tol  = per-step error tolerance  = user-supplied function that returns the  array F x,y  = {y’[0],y’[1],...,y’[n-1]}.  ’’’  import math  import numpy as np  def integrate F,x,y,xStop,h,tol=1.0e-6 :  a1 = 0.2; a2 = 0.3; a3 = 0.8; a4 = 8 9; a5 = 1.0  a6 = 1.0  c0 = 35 384; c2 = 500 1113; c3 = 125 192  c4 = -2187 6784; c5 = 11 84  d0 = 5179 57600; d2 = 7571 16695; d3 = 393 640  d4 = -92097 339200; d5 = 187 2100; d6 = 1 40  b10 = 0.2   276  Initial Value Problems  b20 = 0.075; b21 = 0.225  b30 = 44 45; b31 = -56 15; b32 = 32 9  b40 = 19372 6561; b41 = -25360 2187; b42 = 64448 6561  b43 = -212 729  b50 = 9017 3168; b51 =-355 33; b52 = 46732 5247  b53 = 49 176; b54 = -5103 18656  b60 = 35 384; b62 = 500 1113; b63 = 125 192;  b64 = -2187 6784; b65 = 11 84  X = []  Y = []  X.append x   Y.append y   k0 = h*F x,y   stopper = 0  Integration stopper 0 = off, 1 = on   for i in range 500 :  k1 = h*F x + a1*h, y + b10*k0   k2 = h*F x + a2*h, y + b20*k0 + b21*k1   k3 = h*F x + a3*h, y + b30*k0 + b31*k1 + b32*k2   k4 = h*F x + a4*h, y + b40*k0 + b41*k1 + b42*k2 + b43*k3   k5 = h*F x + a5*h, y + b50*k0 + b51*k1 + b52*k2 + b53*k3 \  k6 = h*F x + a6*h, y + b60*k0 + b62*k2 + b63*k3 + b64*k4 \  + b54*k4   + b65*k5   dy = c0*k0 + c2*k2 + c3*k3 + c4*k4 + c5*k5  E =  c0 - d0 *k0 +  c2 - d2 *k2 +  c3 - d3 *k3  \  +  c4 - d4 *k4 +  c5 - d5 *k5 - d6*k6  e = math.sqrt np.sum E**2  len y    hNext = 0.9*h* tol e **0.2   Accept integration step if error e is within tolerance  if  e <= tol:  y = y + dy  x = x + h  X.append x   Y.append y   if stopper == 1: break   Reached end of x-range  if abs hNext  > 10.0*abs h : hNext = 10.0*h   Check if next step is the last one; if so, adjust h  if  h > 0.0  ==   x + hNext  >= xStop :  hNext = xStop - x   277  7.5 Adaptive Runge-Kutta Method  stopper = 1  k0 = k6*hNext h  else:  k0 = k0*hNext h  h = hNext  return np.array X ,np.array Y   if abs hNext  < 0.1*abs h : hNext = 0.1*h  EXAMPLE 7.8 The aerodynamic drag force acting on a certain object in free fall can be approxi- mated by  where  FD = av2e  −by  v = velocity of the object in m s y = elevation of the object in meters a = 7.45 kg m b = 10.53 × 10  −5 m  −1  The exponential term accounts for the change of air density with elevation. The dif- ferential equation describing the fall is  m ¨y = −mg + FD  where g = 9.806 65 m s2 and m = 114 kg is the mass of the object. If the object is released at an elevation of 9 km, determine its elevation and speed after a 10-s fall with the adaptive Runge-Kutta method.  Solution. The differential equation and the initial conditions are  ˙y 2 exp −by   ¨y = −g + a m = −9.80665 + 7.45 114  ˙y 2 exp −10.53 × 10  −5y   y 0  = 9000 m  ˙y 0  = 0  Letting y0 = y and y1 = ˙y, the equivalent ﬁrst-order equations become  +  *  +  *  ˙y =  =  ˙y0 ˙y1  −9.80665 + cid:30   y1 65.351 × 10 −3  1 exp −10.53 × 10 y 2  −5y0    cid:31  +  *  y 0  =  9000 m  0   278  Initial Value Problems  The driver program for run kut5 is listed next. We speciﬁed a per-step error toler- −2 in integrate. Considering the magnitude of y, this should be enough ance of 10 for ﬁve decimal point accuracy in the solution.  ! usr bin python   example7_8  import numpy as np  import math  from run_kut5 import *  from printSoln import *  def F x,y :  F = np.zeros 2   F[0] = y[1]  return F  x = 0.0  xStop = 10.0  h = 0.5  freq = 1  y = np.array [9000, 0.0]   F[1] = -9.80665 + 65.351e-3 * y[1]**2 * math.exp -10.53e-5*y[0]   X,Y = integrate F,x,y,xStop,h,1.0e-2   printSoln X,Y,freq   input "\nPress return to exit"   Running the program resulted in the following output:  x  y[ 0 ]  y[ 1 ]  0.0000e+00  9.0000e+03  0.0000e+00  5.0000e-01  8.9988e+03  -4.8043e+00  2.4229e+00  8.9763e+03  -1.6440e+01  3.4146e+00  8.9589e+03  -1.8388e+01  4.6318e+00  8.9359e+03  -1.9245e+01  5.9739e+00  8.9098e+03  -1.9501e+01  7.6199e+00  8.8777e+03  -1.9549e+01  9.7063e+00  8.8369e+03  -1.9524e+01  1.0000e+01  8.8312e+03  -1.9519e+01  The ﬁrst step was carried out with the prescribed trial value h = 0.5 s. Apparently the error was well within the tolerance, so that the step was accepted. Subsequent step sizes, determined from Eq.  7.24 , were considerably larger. Inspecting the output, we see that at t = 10 s the object is moving with the speed v = − ˙y = 19.52 m s at an elevation of y = 8831 m.   279  7.5 Adaptive Runge-Kutta Method  EXAMPLE 7.9 Integrate the moderately stiff problem y − 10y  y   cid:3    cid:3  cid:3  = − 19 4  y 0  = −9   cid:3    0  = 0  y  from x = 0 to 10 with the adaptive Runge-Kutta method and plot the results  this problem also appeared in Example 7.7 .  Solution. Because we use an adaptive method, there is no need to worry about the stable range of h, as we did in Example 7.7. As long as we specify a reasonable toler- −6 is ﬁne , the algorithm ance for the per-step error  in this case the default value 10 will ﬁnd the appropriate step size. Here is the program and its numerical output:   example7_9  import numpy as np  import matplotlib.pyplot as plt  from run_kut5 import *  from printSoln import *  def F x,y :  F = np.zeros 2   F[0] = y[1]  F[1] = -4.75*y[0] - 10.0*y[1]  return F  x = 0.0  xStop = 10.0  h = 0.1  freq = 4  y = np.array [-9.0, 0.0]   X,Y = integrate F,x,y,xStop,h   printSoln X,Y,freq   plt.plot X,Y[:,0],’o-’,X,Y[:,1],’ˆ-’   plt.xlabel ’x’   plt.legend  ’y’,’dy dx’ ,loc=0   plt.grid True   plt.show    input "\nPress return to exit"   The following printout displays every fourth integration step:  x  y[ 0 ]  y[ 1 ]  0.0000e+00  -9.0000e+00  0.0000e+00  7.7774e-02  -8.8988e+00  2.2999e+00  1.6855e-01  -8.6314e+00  3.4083e+00  2.7656e-01  -8.2370e+00  3.7933e+00   280  Initial Value Problems  4.0945e-01  -7.7311e+00  3.7735e+00  5.8108e-01  -7.1027e+00  3.5333e+00  8.2045e-01  -6.3030e+00  3.1497e+00  1.2036e+00  -5.2043e+00  2.6021e+00  2.0486e+00  -3.4110e+00  1.7055e+00  3.5357e+00  -1.6216e+00  8.1081e-01  4.9062e+00  -8.1724e-01  4.0862e-01  6.3008e+00  -4.0694e-01  2.0347e-01  7.7202e+00  -2.0012e-01  1.0006e-01  9.1023e+00  -1.0028e-01  5.0137e-02  1.0000e+01  -6.4010e-02  3.2005e-02  The results are in agreement with the analytical solution.  cid:3  The plots of y and y points near x = 0 where y distance between the points increases.  are shown in the following ﬁgure. Note the high density of  cid:3  -curve becomes smoother, the  changes rapidly. As the y   cid:3   7.6  Bulirsch-Stoer Method  Midpoint Method  The midpoint formula of numerical integration of y y x + h  = y x − h  + 2hF   cid:3  = F x, y  is  cid:28    cid:29   x, y x    7.25    281  7.6 Bulirsch-Stoer Method  y' x   f  x,y  h  h  x - h  x  x + h x  Figure 7.3. Graphical representation of formula.  the midpoint  It is a second-order formula, like the modiﬁed Euler’s formula. We discuss it here because it is the basis of the powerful Bulirsch-Stoer method, which is the technique of choice in problems where high accuracy is required.  cid:3  =  Figure 7.3 illustrates the midpoint formula for a single differential equation y  f  x, y . The change in y over the two panels shown is x+h x−h  y x + h  − y x − h  =   cid:3   y   x dx  4  which equals the area under the y area by the area 2hf  x, y  of the cross-hatched rectangle.   x  curve. The midpoint method approximates this   cid:3   h  x0  x1  x2  H x3  n - 1x  nx  x  Figure 7.4. Mesh used in the midpoint method.  Consider now advancing the solution of y   x  = F x, y  from x = x0 to x0 + H with the midpoint formula. We divide the interval of integration into n steps of length h = H n each, as shown in Figure 7.4, and then carry out the following computations:   cid:3   y1 = y0 + hF0 y2 = y0 + 2hF1 y3 = y1 + 2hF2  ...  yn = yn−2 + 2hFn−1   7.26   Here we used the notation yi = y xi  and Fi = F xi, yi . The ﬁrst of Eqs.  7.26  uses Euler’s formula to “seed” the midpoint method; the other equations are midpoint formulas. The ﬁnal result is obtained by averaging yn in Eq.  7.26  and the estimate yn ≈ yn−1 + hFn available from the Euler formula. The result is  cid:31  cid:29    cid:28  yn + cid:30   yn−1 + hFn   7.27   y x0 + H  = 1 2  Richardson Extrapolation  It can be shown that the error in Eq.  7.27  is  E = c1h2 + c2h4 + c3h6 + ···   282  Initial Value Problems  Herein lies the great utility of the midpoint method: We can eliminate as many of the leading error terms as we wish by Richarson’s extrapolation. For example, we could compute y x0 + H  with a certain value of h and then repeat the process with h 2. Denoting the corresponding results by g h  and g h 2 , Richardson’s extrapolation— see Eq.  5.9 —then yields the improved result  ybetter x0 + H  = 4g h 2  − g h   3  which is fourth-order accurate. Another round of integration with h 4 followed by Richardson’s extrapolation get us sixth-order accuracy, and so on Rather than halv- ing the interval in successive integrations, we use the sequence h 2, h 4, h 6, h 8, h 10, . . . which has been found to be more economical. y x0 + H , they cannot be reﬁned by Richardson’s extrapolation.  The y’s in Eqs.  7.26  should be viewed as a temporary variables, because unlike   cid:2  midpoint  The function integrate in this module combines the midpoint method with Richardson extrapolation. The ﬁrst application of the midpoint method uses two integration steps. The number of steps is increased by two in successive integra- tions, each integration being followed by Richardson extrapolation. The procedure is stopped when two successive solutions differ  in the root-mean-square sense  by less than a prescribed tolerance.   module midpoint  ’’’ yStop = integrate  F,x,y,xStop,tol=1.0e-6   Modified midpoint method for solving the  initial value problem y’ = F x,y}.  x,y  = initial conditions  xStop = terminal value of x  yStop = y xStop   F  = user-supplied function that returns the  array F x,y  = {y’[0],y’[1],...,y’[n-1]}.  ’’’  import numpy as np  import math  def integrate F,x,y,xStop,tol :  def midpoint F,x,y,xStop,nSteps :   Midpoint formulas  h =  xStop - x  nSteps  y0 = y  y1 = y0 + h*F x,y0    283  7.6 Bulirsch-Stoer Method  for i in range nSteps-1 :  y2 = y0 + 2.0*h*F x,y1   x = x + h  y0 = y1  y1 = y2  return 0.5* y1 + y0 + h*F x,y2    def richardson r,k :   Richardson’s extrapolation  for j in range k-1,0,-1 :  const =  k  k - 1.0  ** 2.0* k-j    r[j] =  const*r[j+1] - r[j]   const - 1.0   return  kMax = 51  n = len y   nSteps = 2  r = np.zeros  kMax,n     Start with two integration steps  r[1] = midpoint F,x,y,xStop,nSteps   r_old = r[1].copy     Increase the number of integration points by 2   and refine result by Richardson extrapolation  for k in range 2,kMax :  nSteps = 2*k  r[k] = midpoint F,x,y,xStop,nSteps   richardson r,k    Compute RMS change in solution  e = math.sqrt np.sum  r[1] - r_old **2  n    Check for convergence  if e < tol: return r[1]  r_old = r[1].copy    print "Midpoint method did not converge"   Bulirsch-Stoer Algorithm  The fundamental idea behind the Bulirsch-Stroer method is simple: Apply the mid- point method in a piecewise fashion. That is, advance the solution in stages of length H, using the midpoint method with Richardson extrapolation to perform the inte- gration in each stage. The value of H can be quite large, because the precision of the result is determined by the step length h in the midpoint method, not by H. How- ever, if H is too large, the the midpoint method may not converge. If this happens, try a smaller value of H or a larger error tolerance.   284  Initial Value Problems  The original Bulirsch and Stoer technique3 is a complex procedure that incorpo- rates many reﬁnements missing in our algorithm, such as determining the optimal value of H. However, the function bulStoer shown next retains the essential ideas of Bulirsch and Stoer.  What are the relative merits of adaptive Runge-Kutta and Bulirsch-Stoer meth- ods? The Runge-Kutta method is more robust, having higher tolerance for non- smooth functions and stiff problems. The Bulirsch-Stoer algorithm  in its original form  is used mainly in problems where high accuracy is of paramount importance. Our simpliﬁed version is no more accurate than the adaptive Runge-Kutta method, but it is useful if the output is to appear at equally spaced values of x.   cid:2  bulStoer  This function contains a simpliﬁed algorithm for the Bulirsch-Stoer method.   module bulStoer  ’’’ X,Y = bulStoer F,x,y,xStop,H,tol=1.0e-6 .  Simplified Bulirsch-Stoer method for solving the  initial value problem {y}’ = {F x,{y} }, where  {y} = {y[0],y[1],...y[n-1]}.  x,y  = initial conditions  xStop = terminal value of x  H  F  ’’’  = increment of x at which results are stored  = user-supplied function that returns the  array F x,y  = {y’[0],y’[1],...,y’[n-1]}.  import numpy as np  from midpoint import *  def bulStoer F,x,y,xStop,H,tol=1.0e-6 :  X = []  Y = []  X.append x   Y.append y   while x < xStop:  H = min H,xStop - x   x = x + H  X.append x   Y.append y   return np.array X ,np.array Y   y = integrate F,x,y,x + H,tol   Midpoint method  3 Stoer, J. and Bulirsch, R., Introduction to Numerical Analysis, Springer, 1980.   285  7.6 Bulirsch-Stoer Method  EXAMPLE 7.10 Compute the solution of the initial value problem   cid:3  = sin y  y  y 0  = 1  at x = 0.5 with the midpoint formulas using n = 2 and n = 4, followed by Richardson extrapolation  this problem was solved with the second-order Runge-Kutta method in Example 7.3 . Solution. With n = 2 the step length is h = 0.25. The midpoint formulas, Eqs.  7.26  and  7.27  yield  y1 = y0 + hf0 = 1 + 0.25 sin 1.0 = 1.210 368 y2 = y0 + 2hf1 = 1 + 2 0.25  sin 1.210 368 = 1.467 87 3   y1 + y0 + hf2   1.210 368 + 1.467 87 3 + 0.25 sin 1.467 87 3   yh 0.5  = 1 2 = 1 2 = 1.463 459  Using n = 4 we have h = 0.125, and the midpoint formulas become  y1 = y0 + hf0 = 1 + 0.125 sin 1.0 = 1.105 184 y2 = y0 + 2hf1 = 1 + 2 0.125  sin 1.105 184 = 1.223 387 y3 = y1 + 2hf2 = 1.105 184 + 2 0.125  sin 1.223 387 = 1.340 248 y4 = y2 + 2hf3 = 1.223 387 + 2 0.125  sin 1.340 248 = 1.466 772   y4 + y3 + hf4   1.466 772 + 1.340 248 + 0.125 sin 1.466 772   yh 2 0.5  = 1 2 = 1 2 = 1.465 672  Richardson extrapolation results in  y 0.5  = 4yh 2 0.5  − yh 0.5   = 4 1.465 672  − 1.463 459  = 1.466 410  3  3  which compares favorably with the “true” solution, y 0.5  = 1.466 404.  EXAMPLE 7.11  i  E t   R  i  L  C   286  Initial Value Problems  The differential equations governing the loop current i and the charge q on the ca- pacitor of the electric circuit shown are + Ri + q C  = E t   dq dt  = i  di dt  L  If the applied voltage E is suddenly increased from zero to 9 V, plot the resulting loop current during the ﬁrst 10 s. Use R = 1.0  cid:7 , L = 2 H, and C = 0.45 F. Solution. Letting  and substituting the given data, the differential equations become  y = *  =  *  +  ˙y =  ˙y0 ˙y1  *  +  *  +  =  y0 y1  q i  y1  *  +  0 0  y 0  =  +   −Ry1 − y0 C + E   L  We solved the problem with the function bul stoer with the increment H =  The initial conditions are  0.25 s:  ! usr bin python   example7_11  from bulStoer import *  import numpy as np  import matplotlib.pyplot as plt  def F x,y :  F = np.zeros 2   F[0] = y[1]  return F  H = 0.25  xStop = 10.0  x = 0.0  y = np.array [0.0, 0.0]   X,Y = bulStoer F,x,y,xStop,H   plt.plot X,Y[:,1],’o-’   plt.xlabel ’Time  s ’   plt.ylabel ’Current  A ’   plt.grid True   plt.show    input "\nPress return to exit"   F[1] = -y[1] - y[0] 0.45 + 9.0  2.0   287  7.6 Bulirsch-Stoer Method  Recall that in each interval H  the spacing of points in the plot , the integra- tion was performed by the modiﬁed midpoint method and reﬁned by Richardson’s extrapolation.  PROBLEM SET 7.2  1. Derive the analytical solution of the problem   cid:3  cid:3  + y   cid:3  − 380y = 0  y  y 0  = 1   cid:3    0  = −20  y  Would you expect difﬁculties in solving this problem numerically?  2. Consider the problem   cid:3  = x − 10y  y  y 0  = 10   a  Verify that the analytical solution is y x  = 0.1x − 0.001 + 10.01e −10x.  b  De- termine the step size h that you would use in numerical solution with the  non- adaptive  Runge-Kutta method. 3.  cid:2  Integrate the initial value problem in Prob. 2 from x = 0 to 5 with the Runge- Kutta method using  a  h = 0.1,  b  h = 0.25, and  c  h = 0.5. Comment on the results. 4.  cid:2  Integrate the initial value problem in Prob. 2 from x = 0 to 10 with the adaptive  Runge-Kutta method and plot the result.   288  Initial Value Problems  5.  cid:2   y  k c  m  ¨y + c m  ˙y + k m  y = 0  The differential equation describing the motion of the mass-spring-dashpot system is  where m = 2 kg, c = 460 N·s m, and k = 450 N m. The initial conditions are y 0  = 0.01 m and ˙y 0  = 0.  a  Show that this is a stiff problem and determine a value of h that you would use in numerical integration with the nonadaptive Runge-Kutta method.  b  Carry out the integration from t = 0 to 0.2 s with the chosen h and plot ˙y vs. t. Runge-Kutta method from t = 0 to 0.2 s, and plot ˙y vs. t.  6.  cid:2  Integrate the initial value problem speciﬁed in Prob. 5 with the adaptive  7.  cid:2  Compute the numerical solution of the differential equation   cid:3  cid:3  = 16.81y  y  from x = 0 to 2 with the adaptive Runge-Kutta method and plot the results.  0  = Use the initial conditions  a  y 0  = 1.0, y −4.11. Explain the large difference in the two solutions. Hint: Derive and plot the analytical solutions.   0  = −4.1; and  b  y 0  = 1.0, y   cid:3    cid:3   8.  cid:2  Integrate   cid:3  cid:3  + y   cid:3  − y 2 = 0  y  y 0  = 1   cid:3    0  = 0  y  from x = 0 to 3.5. Is the sudden increase in y near the upper limit real or an arti- fact caused by instability?  9.  cid:2  Solve the stiff problem—see Eq.  7.16 —   0  = 0 from x = 0 to 0.2 with the adaptive Runge-Kutta method and plot y   cid:3  + 1000y = 0   cid:3  cid:3  + 1001y  y 0  = 1  y  y   cid:3    cid:3   vs. x.  10.  cid:2  Solve   cid:3  cid:3  + 2y   cid:3  + 3y = 0  y  y 0  = 0   cid:3    0  =  y  √  2  with the adaptive Runge-Kutta method from x = 0 to 5  the analytical solution is y = e  √ 2x .  −x sin  11.  cid:2  Solve the differential equation   cid:3  cid:3  = 2yy   cid:3   y  from x = 0 to 10 with the initial conditions y 0  = 1, y  cid:3  12.  cid:2  Repeat Prob. 11 with the initial conditions y 0  = 0, y tion range x = 0 to 1.5.   0  = −1. Plot y vs. x.  0  = 1 and the integra-  cid:3    289  7.6 Bulirsch-Stoer Method  13.  cid:2  Use the adaptive Runge-Kutta method to integrate y 0  = 5  − y  x  y   cid:3  = from x = 0 to 4 and plot y vs. x.  9 y  %  &  14.  cid:2  Solve Prob. 13 with the Bulirsch-Stoer method using H = 0.25. 15.  cid:2  Integrate   cid:3  + y = 0 from x = 1 to 20, and plot y and y  cid:3    cid:3  cid:3  + xy  x2y  16.  cid:2   y 1  = 0   cid:3    1  = −2  y  vs. x. Use the Bulirsch-Stoer method.  x k  m  The magnetized iron block of mass m is attached to a spring of stiffness k and free length L. The block is at rest at x = L when the electromagnet is turned on, exerting the repulsive force F = c x2 on the block. The differential equation of the resulting motion is  m¨x = c x2  − k x − L   Determine the period of the ensuing motion by numerical integration with the adaptive Runge-Kutta method. Use c = 5 N·m2, k = 120 N m, L = 0.2 m, and m = 1.0 kg.  17.  cid:2   φ  C  B  A  θ  The bar A BC is attached to the vertical rod with a horizontal pin. The assembly is free to rotate about the axis of the rod. Neglecting friction, the equations of motion of the system are  ¨θ = ˙φ2 sin θ cos θ  ¨φ = −2 ˙θ ˙φ cot θ  The system is set into motion with the initial conditions θ 0  = π  12 rad, ˙θ 0  = 0, φ 0  = 0, and ˙φ 0  = 20 rad s. Obtain a numerical solution with the adaptive Runge-Kutta method from t = 0 to 1.5 s and plot ˙φ vs. t. 18.  cid:2  Solve the circuit problem in Example 7.11 if R = 0 and    E t  =  0 when t < 0 9 sin πt when t ≥ 0   290  Initial Value Problems 19.  cid:2  Solve Prob. 21 in Problem Set 1 if E = 240 V  constant . 20.  cid:2   L  i1  E t   i2 R 2  i1  C  i2  R  1  L    Kirchoff’s equations for the circuit in the ﬁgure are  where  Using the data R1 = 4  cid:7 , R2 = 10  cid:7 , L = 0.032 H, C = 0.53 F, and  L  di1 dt di2 dt  + R1i1 + R2 i1 − i2  = E t  + R2 i2 − i1  + q2 C  = 0  L  = i2  dq2 dt  E t  =  20 V if 0 < t < 0.005 s 0 otherwise  plot the transient loop currents i1 and i2 from t = 0 to 0.05 s.  21.  cid:2  Consider a closed biological system populated by M number of prey and N number of predators. Volterra postulated that the two populations are related by the differential equations  ˙M = a M − bMN ˙N = −cN + d MN  where a, b, c, and d are constants. The steady-state solution is M0 = c d, N0 = a  b; if numbers other than these are introduced into the system, the populations undergo periodic ﬂuctuations. Introducing the notation  y0 = M M0  y1 = N N0  allows us to write the differential equations as ˙y0 = a y0 − y0y1  ˙y1 = b −y1 + y0y1   Using a = 1.0 year, b = 0.2 year, y0 0  = 0.1, and y1 0  = 1.0, plot the two popu- lations from t = 0 to 5 years.   291  7.6 Bulirsch-Stoer Method  22.  cid:2  The equations  ˙u = −au + av ˙v = cu − v − uw ˙w = −bw + uv  known as the Lorenz equations, are encountered in the theory of ﬂuid dynamics. Letting a = 5.0, b = 0.9, and c = 8.2, solve these equations from t = 0 to 10 with the initial conditions u 0  = 0, v 0  = 1.0, w 0  = 2.0, and plot u t . Repeat the solution with c = 8.3. What conclusions can you draw from the results?  23.  cid:2   2m  s3  c = 25 mg m3  4m  s3  4m  s3  c1  c3  4m  s3  2m  s3  m  s33  1 m  s3  c2  c4  m  s33  1 m  s3  c = 50 mg m 3  Four mixing tanks are connected by pipes. The ﬂuid in the system is pumped through the pipes at the rates shown in the ﬁgure. The ﬂuid entering the system contains a chemical of concentration c as indicated. The rate at which the mass of the chemical changes in tank i is  where Vi is the volume of the tank and Q represents the ﬂow rate in the pipes connected to it. Applying this equation to each tank, we obtain  Vi  dci dt  =  cid:19   Qc in −  cid:19  Qc out  V1  V2  V3  V4  dci dt dc2 dt dc3 dt dc4 dt  = −6c1 + 4c2 + 2 25  = −7c2 + 3c3 + 4c4 = 4c1 − 4c3 = 2c1 + c3 − 4c4 + 50  Plot the concentration of the chemical in tanks 1 and 2 vs. time t from t = 0 to 100 s. Let V1 = V2 = V3 = V4 = 10 m3, and assume that the concentration in each tank is zero at t = 0. The steady-state version of this problem was solved in Prob. 21, Problem Set 2.2.   292  Initial Value Problems  7.7  Other Methods  The methods described so far belong to a group known as single-step methods. The name stems from the fact that the information at a single point on the solution curve is sufﬁcient to compute the next point. There are also multistep methods that use several points on the curve to extrapolate the solution at the next step. Well-known members of this group are the methods of Adams, Milne, Hamming, and Gere. These methods were popular once, but have lost some of their luster in the last few years. Multistep methods have two shortcomings that complicate their implementation:  1. The methods are not self-starting, but must be provided with the solution at the  ﬁrst few points by a single-step method.  2. The integration formulas assume equally spaced steps, which makes it difﬁcult to  change the step size.  Both of these hurdles can be overcome, but the price is complexity of the algo- rithm, which increases with the sophistication of the method. The beneﬁts of multi- step methods are minimal—the best of them can outperform their single-step coun- terparts in certain problems, but these occasions are rare.   8  Two-Point Boundary Value Problems  Solve y   cid:3  cid:3  = f  x, y, y   cid:3    ,  y a  = α,  y b  = β.  8.1  Introduction  In two-point boundary value problems the auxiliary conditions associated with the differential equation, called the boundary conditions, are speciﬁed at two different values of x. This seemingly small departure from initial value problems has a ma- jor repercussion—it makes boundary value problems considerably more difﬁcult to solve. In an initial value problem we were able to start at the point where the initial values were given and march the solution forward as far as needed. This technique does not work for boundary value problems, because there are not enough starting conditions available at either endpoint to produce a unique solution.  One way to overcome the lack of starting conditions is to guess the missing val- ues. The resulting solution is very unlikely to satisfy boundary conditions at the other end, but by inspecting the discrepancy we can estimate what changes to make to the initial conditions before integrating again. This iterative procedure is known as the shooting method. The name is derived from analogy with target shooting—take a shot and observe where it hits the target; then correct the aim and shoot again.  Another means of solving two-point boundary value problems is the ﬁnite differ- ence method, where the differential equations are approximated by ﬁnite differences at evenly spaced mesh points. As a consequence, a differential equation is trans- formed into a set of simultaneous algebraic equations.  The two methods have a common problem: They give rise to nonlinear sets of equations if the differential equations are not linear. As we noted in Chapter 2, all methods of solving nonlinear equations are iterative procedures that can consume a lot of computational resources. Thus solving nonlinear boundary value problems is not cheap. Another complication is that iterative methods need reasonably good starting values to converge. Since there is no set formula for determining these start- ing values, an algorithm for solving nonlinear boundary value problems requires informed input; it cannot be treated as a “black box.”  293   294  Two-Point Boundary Value Problems  8.2  Shooting Method  Second-Order Differential Equation  The simplest two-point boundary value problem is a second-order differential equa- tion with one condition speciﬁed at x = a and another one at x = b. Here is an ex- ample of such a problem:   cid:3  cid:3  = f  x, y, y   cid:3   y   ,  y a  = α,  y b  = β  Let us now attempt to turn Eqs.  8.1  into the initial value problem   cid:3  cid:3  = f  x, y, y   cid:3   y   ,  y a  = α,   cid:3    a  = u  y   8.1    8.2   The key to success is ﬁnding the correct value of u. This could be done by trial and error: Guess u and solve the initial value problem by marching from x = a to b. If the solution agrees with the prescribed boundary condition y b  = β, we are done; otherwise we have to adjust u and try again. Clearly, this procedure is very tedious.  More systematic methods become available to us if we realize that the determi- nation of u is a root-ﬁnding problem. Because the solution of the initial value prob- lem depends on u, the computed value of y b  is a function of u; that is  y b  = θ u   Hence u is a root of  r  u  = θ u  − β = 0   8.3   where r  u  is the boundary residual  the difference between the computed and speci- ﬁed boundary value at x = b . Equation  8.3  can be solved by one of the root-ﬁnding methods discussed in Chapter 4. We reject the method of bisection because it in- volves too many evaluations of θ u . In the Newton-Raphson method we run into the problem of having to compute dθ  du, which can be done but not easily. That leaves Ridder’s algorithm as our method of choice.  Here is the procedure we use in solving nonlinear boundary value problems:  Specify the starting values u1 and u2 that must bracket the  root u of Eq.  8.3 .  Apply Ridder’s method to solve Eq.  8.3  for u. Note that each  iteration requires evaluation of θ u  by solving the differential equation as an initial value problem.  Having determined the value of u, solve the differential equations  once more and record the results.  If the differential equation is linear, any root-ﬁnding method will need only one interpolation to determine u. Because Ridder’s method uses three points  u1, u2, and u3 , it is wasteful compared with linear interpolation, which uses only two points   295  8.2 Shooting Method   u1 and u2 . Therefore, we replace Ridder’s method with linear interpolation when- ever the differential equation is linear.   cid:2  linInterp  Here is the algorithm we use for linear interpolation:   module linInterp  ’’’ root = linInterp f,x1,x2 .  Finds the zero of the linear function f x  by straight  line interpolation based on x = x1 and x2.  ’’’  def linInterp f,x1,x2 :  f1 = f x1   f2 = f x2   return = x2 - f2* x2 - x1   f2 - f1   EXAMPLE 8.1 Solve the boundary value problem  cid:3  = 0 *  y 0  = 0 * Solution. The equivalent ﬁrst-order equations are   cid:3  cid:3  + 3yy  +  y  y 2  = 1 +  with the boundary conditions   cid:3  =  y   cid:3  0  cid:3  1  y y  =  y1−3y0y1  y0 0  = 0  y0 2  = 1  Now comes the daunting task of determining the trial values of y   0 . We could always pick two numbers at random and hope for the best. However, it is possible to reduce the element of chance with a little detective work. We start by making the reasonable assumption that y is smooth  does not wiggle  in the interval 0 ≤ x ≤ 2.  cid:3  > 0. Because both Next we note that y has to increase from 0 to 1, which requires y y and y must be negative to satisfy the differential equation. Now we are in a position to make a rough sketch of y:  are positive, we conclude that y   cid:3  cid:3    cid:3    cid:3   y  0  1  x  2   cid:3  Looking at the sketch it is clear that y  cid:3  reasonable values for the brackets of y an error message.   0  = 1 and 2 appear to be  0  > 0.5, so that y  0 ; if they are not, Ridder’s method will display   cid:3    296  Two-Point Boundary Value Problems  In the program listed next we chose the fourth-order Runge-Kutta method for integration. It can be replaced by the adaptive version by substituting run kut5 for run kut4 in the import statement. Note that three user-supplied functions are needed to describe the problem at hand. Apart from the function F x,y  that de- ﬁnes the differential equations, we also need the functions initCond u  to specify the initial conditions for integration, and r u  to provide Ridder’s method with the boundary condition residual. By changing a few statements in these functions, the program can be applied to any second-order boundary value problem. It also works for third-order equations if integration is started at the end where two of the three boundary conditions are speciﬁed.  ! usr bin python   example8_1  import numpy as np  from run_kut4 import *  from ridder import *  from printSoln import *  y = Y[len Y  - 1]  r = y[0] - 1.0  return r  F = np.zeros 2   F[0] = y[1]  F[1] = -3.0*y[0]*y[1]  return F  def initCond u :   Init. values of [y,y’]; use ’u’ if unknown  return np.array [0.0, u]   def r u :   Boundary condition residual--see Eq.  8.3   X,Y = integrate F,xStart,initCond u ,xStop,h   def F x,y :   First-order differential equations  xStart = 0.0  xStop = 2.0   Start of integration   End of integration  u1 = 1.0  u2 = 2.0  h = 0.1  freq = 2   1st trial value of unknown init. cond.   2nd trial value of unknown init. cond.   Step size   Printout frequency  u = ridder r,u1,u2   Compute the correct initial condition  X,Y = integrate F,xStart,initCond u ,xStop,h   printSoln X,Y,freq   input "\nPress return to exit"    297  8.2 Shooting Method  Here is the solution:  x  y[ 0 ]  y[ 1 ]  0.0000e+00  0.0000e+00  1.5145e+00  2.0000e-01  2.9404e-01  1.3848e+00  4.0000e-01  5.4170e-01  1.0743e+00  6.0000e-01  7.2187e-01  7.3287e-01  8.0000e-01  8.3944e-01  4.5752e-01  1.0000e+00  9.1082e-01  2.7013e-01  1.2000e+00  9.5227e-01  1.5429e-01  1.4000e+00  9.7572e-01  8.6471e-02  1.6000e+00  9.8880e-01  4.7948e-02  1.8000e+00  9.9602e-01  2.6430e-02  1.0000e+00  1.4522e-02  2.0000e+00  cid:3   Note that y  mark.   0  = 1.5145, so that our starting values of 1.0 and 2.0 were on the  EXAMPLE 8.2 Numerical integration of the initial value problem y 0  = 0   cid:3  cid:3  + 4y = 4x  y   cid:3    0  = 0  y   cid:3    2  = 1.653 64. Use this information to determine the value of y   cid:3    0  that  yielded y would result in y   cid:3    2  = 0.  Solution. We use linear interpolation  u2 − u1  u = u2 − θ u2   0  and θ u  = y  cid:3   θ u2  − θ u1  where in our case u = y  2 . So far we are given u1 = 0 and θ u1  = 1.653 64. To obtain the second point, we need another solution of the initial value  2  = 1.  0  = y problem. An obvious solution is y = x, which gives us y 0  = 0 and y  cid:3  Thus the second point is u2 = 1 and θ u2  = 1. Linear interpolation now yields   cid:3    cid:3    cid:3    0  = u = 1 −  1   y  1 − 0  1 − 1.653 64  = 2.529 89  EXAMPLE 8.3 Solve the third-order boundary value problem y 0  = 2   cid:3  cid:3  + 6xy  y   cid:3  cid:3  cid:3  = 2y vs. x.   cid:3   and plot y and y  y 5  = y   cid:3    5  = 0  Solution. The ﬁrst-order equations and the boundary conditions are  ⎤ ⎥⎦ =  ⎡ ⎢⎣  ⎡ ⎢⎣ y   cid:3  = y   cid:3  0  cid:3  y 1  cid:3  y 2 y0 0  = 2  y1 y2  2y2 + 6xy0  y0 5  = y1 5  = 0  ⎤ ⎥⎦   298  Two-Point Boundary Value Problems  The program listed next is based on example8 1. Because two of the three boundary conditions are speciﬁed at the right end, we start the integration at x = 5 and proceed with negative h toward x = 0. Two of the three initial conditions are prescribed— y0 5  = y1 5  = 0—whereas the third condition y2 5  is unknown. Be- cause the differential equation is linear, we replaced ridder with linInterp. In linear interpolation the two guesses for y2 5   u1 and u2  are not important, so we left them as they were in Example 8.1. The adaptive Runge-Kutta method  run kut5  was chosen for the integration.  ! usr bin python   example8_3  import matplotlib.pyplot as plt  import numpy as np  from run_kut5 import *  from linInterp import *  def initCond u :   Initial values of [y,y’,y"];   use ’u’ if unknown  return np.array [0.0, 0.0, u]   def r u :   Boundary condition residual--see Eq.  8.3   X,Y = integrate F,xStart,initCond u ,xStop,h   def F x,y :   First-order differential equations  y = Y[len Y  - 1]  r = y[0] - 2.0  return r  F = np.zeros 3   F[0] = y[1]  F[1] = y[2]  F[2] = 2.0*y[2] + 6.0*x*y[0]  return F  xStart = 5.0  xStop = 0.0   Start of integration   End of integration  u1 = 1.0  u2 = 2.0  h = -0.1  freq = 2   1st trial value of unknown init. cond.   2nd trial value of unknown init. cond.   initial step size   printout frequency  u = linInterp r,u1,u2   X,Y = integrate F,xStart,initCond u ,xStop,h   plt.plot X,Y[:,0],’o-’,X,Y[:,1],’ˆ-’   plt.xlabel ’x’    299  8.2 Shooting Method  plt.legend  ’y’,’dy dx’ ,loc = 3   plt.grid True   plt.show    input "\nPress return to exit"   Higher Order Equations  Let us consider the fourth-order differential equation  cid:3  cid:3  cid:3    cid:3  cid:3    cid:3   y  4  = f  x, y, y  , y  , y      8.4a   with the boundary conditions  y a  = α1   cid:3  cid:3    a  = α2  y b  = β  y   8.4b  To solve Eqs.  8.4  with the shooting method, we need four initial conditions at x = a, only two of which are speciﬁed. Denoting the unknown initial values by u1 and u2, the set of initial conditions is y a  = α1   a  = α2   a  = u2   a  = u1   8.5    cid:3  cid:3  cid:3   y  y  y  y   cid:3  cid:3   1  2   cid:3    cid:3  cid:3    b  = β  If Eq.  8.4a  is solved with the shooting method using the initial conditions in Eq.  8.5 , the computed boundary values at x = b depend on the choice of u1 and u2. We denote this dependence as  y b  = θ 1 u1, u2    cid:3  cid:3    b  = θ 2 u1, u2   y   8.6    300  Two-Point Boundary Value Problems The correct values u1 and u2 satisfy the given boundary conditions at x = b,  θ 1 u1, u2  = β  1  θ 2 u1, u2  = β  2  or, using vector notation  θ u  = β   8.7   These are simultaneous,  generally nonlinear  equations that can be solved by the Newton-Raphson method discussed in Section 4.6. It must be pointed out again that intelligent estimates of u1 and u2 are needed if the differential equation is not linear.  EXAMPLE 8.4  w0  x  v  L  The displacement v of the simply supported beam can be obtained by solving the boundary value problem  d4v dx4  = w0 E I  x L  v = d2v dx2  = 0 at x = 0 and x = L  where E I is the bending rigidity. Determine by numerical integration the slopes at the two ends and the displacement at mid-span.  Solution. Introducing the dimensionless variables y = E I  ξ = x L  w0 L4 v  the problem is transformed to = ξ  d 4y dξ 4  y = d2y dξ 2  = 0 at ξ = 0 and 1  The equivalent ﬁrst-order equations and the boundary conditions are  the prime de- notes d dξ   ⎡ ⎢⎢⎢⎣   cid:3  0  cid:3  1  cid:3  2  cid:3  3  y y y y  ⎤ ⎥⎥⎥⎦ =  ⎡ ⎢⎢⎢⎣  ⎤ ⎥⎥⎥⎦  y1 y2 y3 ξ   cid:3  = y  y0 0  = y2 0  = y0 1  = y2 1  = 0  The program listed next is similar to the one in Example 8.1. With appropri- ate changes in functions F x,y , initCond u , and r u  the program can solve boundary value problems of any order greater than two. For the problem at hand we   301  8.2 Shooting Method  chose the Bulirsch-Stoer algorithm to do the integration because it gives us control over the printout  we need y precisely at mid-span . The nonadaptive Runge-Kutta method could also be used here, but we would have to guess a suitable step size h. Because the differential equation is linear, the solution requires only one itera- tion with the Newton-Raphson method. In this case the initial values u1 = dy dξx=0 and u2 = d 3y dξ 3x=0 are irrelevant; convergence always occurs in one iteration.  ! usr bin python   example8_4  import numpy as np  from bulStoer import *  from newtonRaphson2 import *  from printSoln import *  def initCond u :   Initial values of [y,y’,y",y"’];   use ’u’ if unknown  return np.array [0.0, u[0], 0.0, u[1]]   def r u :   Boundary condition residuals--see Eq.  8.7   r = np.zeros len u    X,Y = bulStoer F,xStart,initCond u ,xStop,H   def F x,y :   First-order differential equations  y = Y[len Y  - 1]  r[0] = y[0]  r[1] = y[2]  return r  F = np.zeros 4   F[0] = y[1]  F[1] = y[2]  F[2] = y[3]  F[3] = x  return F  u = np.array [0.0, 1.0]    Initial guess for {u}  xStart = 0.0  xStop = 1.0  H = 0.5  freq = 1   Start of integration   End of integration   Printout increment   Printout frequency  u = newtonRaphson2 r,u,1.0e-4   X,Y = bulStoer F,xStart,initCond u ,xStop,H   printSoln X,Y,freq   input "\nPress return to exit"    302  Two-Point Boundary Value Problems  Here is the output:  x  y[ 0 ]  y[ 1 ]  y[ 2 ]  y[ 3 ]  0.0000e+00  0.0000e+00  1.9444e-02  0.0000e+00  -1.6667e-01  5.0000e-01  6.5104e-03  1.2150e-03  -6.2500e-02  -4.1667e-02  1.0000e+00  7.6881e-12  -2.2222e-02  7.5002e-11  3.3333e-01  Noting that  we obtain  %  &  dv dx  = dv dξ  dξ dx  =  w0L4 E I  dy dξ  1 L  = w0L3 E I  dy dξ   cid:19  cid:19  cid:19  cid:19   cid:19  cid:19  cid:19  cid:19   dv dx  x=0  = 19.444 × 10  −3 w0L3 E I  x=L  = −22.222 × 10  dv dx vx=0.5L = 6.5104 × 10  −3 w0L3 E I −3 w0L4 E I  EXAMPLE 8.5 Consider the differential equation  y 4  + 4 x  y 3 = 0  with the boundary conditions y 0  = y  cid:3    0  = 0   cid:3  cid:3    1  = 0  y   cid:3  cid:3  cid:3    1  = 1  y  Use numerical integration to determine y 1 .  which agree with the analytical solution  easily obtained by direct integration of the differential equation .  Solution. Our ﬁrst task is to handle the indeterminacy of the differential equation at the origin, where x = y = 0. The problem is resolved by applying L’Hospital’s rule: 4y 3 x → 12y 2y as x → 0. Thus the equivalent ﬁrst-order equations and the bound- ary conditions that we use in the solution are   cid:3   ⎡  ⎢⎢⎢⎢⎢⎢⎣    ⎤ ⎥⎥⎥⎦ =  ⎡ ⎢⎢⎢⎣   cid:3  0  cid:3  1  cid:3  2  cid:3  3  y y y y   cid:3  =  y  y0 0  = y1 0  = 0  ⎤  ⎥⎥⎥⎥⎥⎥⎦  y1 y2 y3  if x = 0 otherwise  −12y 2 0 y1 −4y3  x 0 y2 1  = 0  y3 1  = 1  Because the problem is nonlinear, we need reasonable estimates for y   cid:3  cid:3    1  = 0 and y   cid:3  cid:3  cid:3    1  = 1, the plot of y   cid:3  cid:3    0  and is   cid:3  cid:3    cid:3  cid:3  cid:3    0 . Based on the boundary conditions y  y likely to look something like this:   303  8.2 Shooting Method  y" 0  1  1  1  x  If we are correct, then y tion, we try y   0  = −1 and y   0  < 0 and y  0  = 1.   cid:3  cid:3  cid:3    cid:3  cid:3    cid:3  cid:3    cid:3  cid:3  cid:3    0  > 0. Based on this rather scanty informa-  The following program uses the adaptive Runge-Kutta method for integration:  ! usr bin python   example8_5  import numpy as np  from run_kut5 import *  from newtonRaphson2 import *  from printSoln import *  def initCond u :   Initial values of [y,y’,y",y"’];   use ’u’ if unknown  return np.array [0.0, 0.0, u[0], u[1]]   def r u :   Boundary condition residuals--see Eq.  8.7   r = np.zeros len u    X,Y = integrate F,x,initCond u ,xStop,h   y = Y[len Y  - 1]  r[0] = y[2]  r[1] = y[3] - 1.0  return r  F = np.zeros 4   F[0] = y[1]  F[1] = y[2]  F[2] = y[3]  def F x,y :   First-order differential equations  if x == 0.0: F[3] = -12.0*y[1]*y[0]**2  F[3] = -4.0* y[0]**3  x  else:  return F  x = 0.0  xStop = 1.0  h = 0.1  freq = 0   Start of integration   End of integration   Initial step size   Printout frequency  u = np.array [-1.0, 1.0]    Initial guess for u  u = newtonRaphson2 r,u,1.0e-5    304  Two-Point Boundary Value Problems  X,Y = integrate F,x,initCond u ,xStop,h   printSoln X,Y,freq   input "\nPress return to exit"   The results are as follows:  x  y[ 0 ]  y[ 1 ]  y[ 2 ]  y[ 3 ]  0.0000e+00  0.0000e+00  0.0000e+00  -9.7607e-01  9.7132e-01  1.0000e+00  -3.2607e-01  -4.8975e-01  -6.7408e-11  1.0000e+00  According to the printout, we have y 1  = −0.326 07. Note that by good fortune,   cid:3  cid:3    0  = −1 and y   cid:3  cid:3  cid:3    0  = 1 were very close to the ﬁnal values.  our initial estimates y  PROBLEM SET 8.1  1. Numerical integration of the initial value problem   cid:3  cid:3  + y  y 0  = 0 yielded y 1  = 0.741028. What is the value of y  cid:3  assuming that y 0  is unchanged?   cid:3  − y = 0  y  2. The solution of the differential equation   cid:3  cid:3  cid:3  + y   cid:3  cid:3  + 2y   cid:3  = 6  y   cid:3    0  = 1  y   0  that would result in y 1  = 1,  with the initial conditions y 0  = 2, y 3.03765. When the solution was repeated with y being unchanged , the result was y 1  = 2.72318. Determine the value of y that y 1  = 0.   0  = 1 yielded y 1  =  0  = 0  the other conditions  0  so   0  = 0, and y   cid:3  cid:3    cid:3  cid:3    cid:3  cid:3    cid:3   3. Roughly sketch the solution of the following boundary value problems. Use the  sketch to estimate y   0  for each problem.   cid:3    a    b    c    cid:3  cid:3  = −e −y  cid:3  cid:3  = 4y 2  cid:3  cid:3  = cos xy   y  y  y  y 0  = 1 y 0  = 10  y 0  = 0  y 1  = 0.5  1  = 0  cid:3  y y 1  = 2  4. Using a rough sketch of the solution, estimate y 0  for the following boundary  value problems.  5. Obtain a rough estimate of y   0  for the following boundary value problem:   cid:3  cid:3    cid:3  cid:3  = y 2 + xy  cid:3  cid:3  = − 2 y x  cid:3  cid:3  = −x y   2   cid:3  − y 2  cid:3   y  y  y   cid:3    0  = 0  y  y 1  = 2   cid:3    0  = 0  y  0  = 2   cid:3   y  y 1  = 2  y 1  = 1   a    b    c   y   cid:3  cid:3   y 2 = 0   cid:3  cid:3  cid:3  + 5y y   cid:3    0  = 1  y 1  = 0  y 0  = 0   305  8.2 Shooting Method  6. Obtain rough estimates of y   cid:3  cid:3  cid:3   0  for the boundary value problem  cid:3  cid:3  + y  sin y = 0   cid:3    cid:3  cid:3   0  and y y  4  + 2y  0  = 0  y 0  = y   cid:3   y 1  = 5   cid:3    1  = 0  y  7. Obtain rough estimates of ˙x 0  and ˙y 0  for the boundary value problem  x 0  = 1 y 0  = 0 8.  cid:2  Solve the following boundary value problem:  ¨x + 2x2 − y = 0 ¨y + y 2 − 2x = 1  x 1  = 0 y 1  = 1   cid:3  cid:3  +  1 − 0.2x  y 2 = 0  y  y 0  = 0  y π  2  = 1  y   cid:3  cid:3  + 2y  9.  cid:2  Solve the following boundary value problem: y 0  = 0   cid:3  + 3y 2 = 0 10.  cid:2  Solve this boundary value problem:  cid:3  cid:3  + sin y + 1 = 0  y 0  = 0 11.  cid:2  Solve the following boundary value problem: y 0  = 1   cid:3  + y = 0  y  y   2  = 0 and plot y vs. x. Warning: y changes very rapidly near x = 0.   cid:3  cid:3  + 1 x  y  y   cid:3   y 2  = −1  y π  = 0  12.  cid:2  Solve the boundary value problem y = 0   cid:3  cid:3  − cid:30   1 − e  −x   cid:31   y  y 0  = 1  y ∞  = 0  and plot y vs. x. Hint: Replace the inﬁnity by a ﬁnite value β. Check your choice of β by repeating the solution with 1.5β. If the results change, you must increase β.  13.  cid:2  Solve the following boundary value problem:  14.  cid:2  Solve the following boundary value problem:  y   cid:3  cid:3  cid:3  = − 1 x  y 1  = 0  y   cid:3  cid:3  + 1 x2 y  1  = 0  cid:3  cid:3   y   cid:3  + 0.1 y   cid:3    3  y 2  = 1   cid:3  cid:3  cid:3  + 4y  0  = 0  y  cid:3  cid:3    cid:3  cid:3  + 6y   cid:3  = 10 y 3  − y   cid:3    3  = 5  y 0  = y  15.  cid:2  Solve the following boundary value problem:  y y −1  = 0   cid:3  cid:3  cid:3  + 2y  cid:3    cid:3  cid:3  + sin y = 0  −1  = −1  y   cid:3    1  = 1  y   306  Two-Point Boundary Value Problems  16.  cid:2  Solve the differential equation in Prob. 15 with the boundary conditions  y −1  = 0  y 0  = 0  y 1  = 1   this is a three-point boundary value problem . 17.  cid:2  Solve the following boundary value problem: y  4  = −xy2  cid:3    cid:3  cid:3   y 0  = 5   0  = 0  y   1  = 0  y   cid:3  cid:3  cid:3    1  = 2  y  18.  cid:2  Solve the following boundary value problem: y  4  = −2yy  cid:3  cid:3  y 4  = 0  y 0  = y   0  = 0   cid:3    cid:3    4  = 1  y  19.  cid:2   20.  cid:2   A projectile of mass m in free ﬂight experiences the aerodynamic drag force Fd = cv2, where v is the velocity. The resulting equations of motion are  If the projectile hits a target 8 km away after a 10-s ﬂight, determine the launch velocity v0 and its angle of inclination θ. Use m = 20 kg, c = 3.2 × 10 −4 kg m, and g = 9.80665 m s2.  y  v0  θ  t = 0  8000 m  x  t =10 s  v ˙y − g  ¨x = − c m  v ˙x   cid:25   ¨y = − c m ˙x2 + ˙y 2  v =  w0  L  N  v  N x  The simply supported beam carries a uniform load of intensity w0 and the tensile force N. The differential equation for the vertical displacement v can be shown to be  d4v dx4  − N E I  d2v dx2  = w0 E I  where E I is the bending rigidity. The boundary conditions are v = d2v dx2 = 0 at x = 0 and L. Changing the variables to ξ = x w0L4 v transforms the L  and y = E I   307  8.3 Finite Difference Method  problem to the dimensionless form  − β  d 4y dξ 4 = d 2y dξ 2   cid:19  cid:19  cid:19  cid:19   yξ=0  = 1  d 2y dξ 2  β = NL2 E I  = yξ=0  = d 2y dξ 2  ξ=0  x=1  = 0   cid:19  cid:19  cid:19  cid:19   Determine the maximum displacement if  a  β = 1.65929 and  b  β = −1.65929  N is compressive .  21.  cid:2  Solve the boundary value problem   cid:3  cid:3  = 0   cid:3  cid:3  cid:3  + yy y  cid:3   y 0  = y   cid:3    0  = 0, y   cid:3    ∞  = 2  and plot y x  and y the boundary layer in incompressible ﬂow  Blasius solution .   x . This problem arises in determining the velocity proﬁle of  22.  cid:2   0w  v  02w x  L  2  1 1 + x L  d 4v dx4  = w0 E I  The differential equation that governs the displacement v of the beam shown is  The boundary conditions are  v = d2v dx2  = 0 at x = 0  v = dv dx  = 0 at x = L  Integrate the differential equation numerically and plot the displacement. Follow the steps used in solving a similar problem in Example 8.4.  8.3  Finite Difference Method  In the ﬁnite difference method we divide the range of integration  a, b  into m equal subintervals of length h each, as shown in Figure 8.1. The values of the numerical solution at the mesh points are denoted by yi, i = 0, 1, . . . , m; the purpose of y−1 and ym+1 is explained shortly. We now make two approximations:  1. The derivatives of y in the differential equation are replaced by the ﬁnite differ- ence expressions. It is common practice to use the ﬁrst central difference approx- imations  see Chapter 5 :  = yi+1 − yi−1   cid:3  y i  2h  = yi−1 − 2yi + yi+1   cid:3  cid:3  y i  h2  etc.   8.8   2. The differential equation is enforced only at the mesh points.   308  Two-Point Boundary Value Problems  y  y1  y2  1x  2x  y-1  -1x  y0  0x a  Figure 8.1. Finite difference mesh.  ym -2  ym - 1 ym  ym + 1  h  m - 2x  m - 1x  mx b  m + 1x  x  As a result, the differential equations are replaced by m + 1 simultaneous alge- braic equations, the unknowns being yi, i = 0, 1, . . . .m. If the differential equation is nonlinear, the algebraic equations will also be nonlinear and must be solved by the Newton-Raphson method. Since the truncation error in a ﬁrst central difference approximation is O h2 , the ﬁnite difference method is not nearly as accurate as the shooting method—recall that the Runge-Kutta method has a truncation error of O h5 . Therefore, the convergence criterion speciﬁed in the Newton-Raphson method should not be too severe.  Second-Order Differential Equation  Consider the second-order differential equation  with the boundary conditions   cid:3  cid:3  = f  x, y, y   cid:3      y  y a  = α or y b  = β or   cid:3    cid:3    a  = α  b  = β  y  y  &  yi+1 − yi−1  2h y1 − y−1 = α ym+1 − ym−1  2h  2h  = β  y0 = α or ym = β or  Approximating the derivatives at the mesh points by ﬁnite differences, the prob-  lem becomes  %  yi−1 − 2yi + yi+1  h2  = f  xi, yi,  i = 0, 1, . . . , m  ,   8.9   Note the presence of y−1 and ym+1, which are associated with points outside the solution domain  a, b . This “spillover” can be eliminated by using the boundary   8.10a    8.10b    309  8.3 Finite Difference Method  conditions. But before we do that, let us rewrite Eqs.  8.9  as y1 − y−1 &  y−1 − 2y0 + y1 − h2 f  %  yi−1 − 2yi + yi+1 − h2 f  xi, yi,  %  x0, y0, yi+1 − yi−1 %  2h  &  = 0  2h = 0, ym+1 − ym−1  &  2h  ym−1 − 2ym + ym+1 − h2 f  xm, yi,  = 0  i = 1, 2, . . . , m − 1   a    b    c   The boundary conditions on y are easily dealt with: Eq.  a  is simply replaced by y0 − α = 0 and Eq.  c  is replaced by ym − β = 0. If y are prescribed, we obtain from Eqs.  8.10  y−1 = y1 − 2hα and ym+1 = ym−1 + 2hβ, which are then substituted into Eqs.  a  and  c , respectively. Hence we ﬁnish up with m + 1 equations in the unknowns y0, y1, . . . , ym: y0 − α = 0 −2y0 + 2y1 − h2 f  x0, y0, α  − 2hα = 0 if y  if y a  = α  a  = α  ⎫⎬ ⎭   8.11a    cid:3    cid:3   yi−1 − 2yi + yi+1 − h2 f  xi, yi,  %  &  yi+1 − yi−1  2h  ym − β = 0 2ym−1 − 2ym − h2 f  xm, ym, β  + 2hβ = 0 if y  = 0 i = 1, 2, . . . , m − 1 ⎫⎬ ⎭  if y b  = β  b  = β   cid:3    8.11b    8.11c   EXAMPLE 8.6 Write out Eqs.  8.11  for the following linear boundary value problem for m = 10:   cid:3  cid:3  = −4y + 4x  y  y 0  = 0   cid:3    π  2  = 0  y  Solve these equations with a computer program. Solution. In this case α = y 0  = 0, β = y Hence Eqs.  8.11  are   cid:3    π  2  = 0, and f  x, y, y   cid:3     = −4y + 4x.  y0 = 0 yi−1 − 2yi + yi+1 − h2  −4yi + 4xi  = 0, 2y9 − 2y10 − h2 −4y10 + 4x10  = 0 ⎤  1 ... 1 −2 + 4h2  ...  1  −2 + 4h2  2  or, using matrix notation,  ⎡  ⎢⎢⎢⎢⎢⎢⎣  0  1 1 −2 + 4h2  ...  i = 1, 2, . . . , m − 1  ⎤  ⎥⎥⎥⎥⎥⎥⎦ =  ⎡  ⎢⎢⎢⎢⎢⎢⎣  ⎡  ⎢⎢⎢⎢⎢⎢⎣  ⎥⎥⎥⎥⎥⎥⎦  y0 y1 ... ym−1 ym  0 4h2x1 ... 4h2xm−1 4h2xm  ⎤  ⎥⎥⎥⎥⎥⎥⎦   310  Two-Point Boundary Value Problems  Note that the coefﬁcient matrix is tridiagonal, so that the equations can be solved efﬁciently by the decomposition and back substitution routines in module LUde- comp3, described in Section 2.4. Recalling that in LUdecomp3 the diagonals of the coefﬁcient matrix are stored in vectors c, d and e, we arrive at the following program:  def equations x,h,m :  Set up finite difference eqs.  d = np.ones m + 1 * -2.0 + 4.0*h2   ! usr bin python   example8_6  import numpy as np  from LUdecomp3 import *  import math  b = np.ones m+1 *4.0*h2*x  h2 = h*h  c = np.ones m   e = np.ones m   d[0] = 1.0  e[0] = 0.0  b[0] = 0.0  c[m-1] = 2.0  return c,d,e,b  xStart = 0.0   x at left end  xStop = math.pi 2.0   x at right end  m = 10   Number of mesh spaces  h =  xStop - xStart  m  x = np.arange xStart,xStop + h,h   c,d,e,b = equations x,h,m   c,d,e = LUdecomp3 c,d,e   y = LUsolve3 c,d,e,b   print "\n  x  y"   for i in range m + 1 :  print ’{:14.5e} {:14.5e}’.format x[i],y[i]    input "\nPress return to exit"   The solution is  x  y  0.00000e+00  0.00000e+00  1.57080e-01  3.14173e-01  3.14159e-01  6.12841e-01  4.71239e-01  8.82030e-01  6.28319e-01  1.11068e+00   311  8.3 Finite Difference Method  7.85398e-01  1.29172e+00  9.42478e-01  1.42278e+00  1.09956e+00  1.50645e+00  1.25664e+00  1.54995e+00  1.41372e+00  1.56451e+00  1.57080e+00  1.56418e+00  The exact solution of the problem is  y = x − sin 2x  which yields y π  2  = π  2 = 1. 57080. Thus the error in the numerical solution is about 0.4%. More accurate results can be achieved by increasing m. For example, with m = 100, we would get y π  2  = 1.57073, which is in error by only 0.0002%. EXAMPLE 8.7 Solve the boundary value problem  cid:3  cid:3  = −3yy  y 2  = 1  y 0  = 0  y   cid:3   with the ﬁnite difference method. Use m = 10 and compare the output with the re- sults of the shooting method in Example 8.1.  Solution. Because the problem is nonlinear, Eqs.  8.11  must be solved by the Newton-Raphson method. The program listed next can be used as a model for other second-order boundary value problems. The function residual y  returns the residuals of the ﬁnite difference equations, which are the left-hand sides of Eqs.  8.11 . The differential equation y   is deﬁned in the function F x,y,yPrime . In this problem we chose for the initial solution yi = 0.5xi, which corresponds to the dashed straight line shown in the rough plot of y in Example 8.1. The starting values of y0, y1, . . . , ym are speciﬁed by function startSoln x . Note that we relaxed the convergence criterion in the Newton-Raphson method to 1.0 × 10 −5, which is more in line with the truncation error in the ﬁnite difference method.   cid:3  cid:3  = f  x, y, y   cid:3   ! usr bin python   example8_7  import numpy as np  from newtonRaphson2 import *  r = np.zeros m + 1   r[0] = y[0]  r[m] = y[m] - 1.0  for i in range 1,m :  return r  def residual y :   Residuals of finite diff. Eqs.  8.11   r[i] = y[i-1] - 2.0*y[i] + y[i+1]  \  - h*h*F x[i],y[i], y[i+1] - y[i-1]   2.0*h     312  Two-Point Boundary Value Problems  def F x,y,yPrime :   Differential eqn. y" = F x,y,y’   F = -3.0*y*yPrime  return F  def startSoln x :   Starting solution y x   y = np.zeros m + 1   for i in range m + 1 : y[i] = 0.5*x[i]  return y  xStart = 0.0  xStop = 2.0  m = 10   x at left end   x at right end   Number of mesh intervals  h =  xStop - xStart  m  x = np.arange xStart,xStop + h,h   y = newtonRaphson2 residual,startSoln x ,1.0e-5   print "\n  x  y"  for i in range m + 1 :  print "%14.5e %14.5e" % x[i],y[i]   raw_input "\nPress return to exit"   Here is the output from our program together with the solution obtained in  Example 8.1.  x  y  y from Ex. 8.1  0.00000e+000  0.00000e+000  0.00000e+000  2.00000e-001  3.02404e-001  2.94050e-001  4.00000e-001  5.54503e-001  5.41710e-001  6.00000e-001  7.34691e-001  7.21875e-001  8.00000e-001  8.49794e-001  8.39446e-001  1.00000e+000  9.18132e-001  9.10824e-001  1.20000e+000  9.56953e-001  9.52274e-001  1.40000e+000  9.78457e-001  9.75724e-001  1.60000e+000  9.90201e-001  9.88796e-001  1.80000e+000  9.96566e-001  9.96023e-001  2.00000e+000  1.00000e+000  1.00000e+000  The maximum discrepancy between the solutions is 1.8% occurring at x = 0.6. Because the shooting method used in Example 8.1 is considerably more accurate than the ﬁnite difference method, the discrepancy can be attributed to truncation error in the ﬁnite difference solution. This error would be acceptable in many en- gineering problems. Again, accuracy can be increased by using a ﬁner mesh. With m = 100 we can reduce the error to 0.07%, but we must question whether the 10-fold increase in computation time is really worth the extra precision.   313  8.3 Finite Difference Method  Fourth-Order Differential Equation  For the sake of brevity we limit our discussion to the special case where y not appear explicitly in the differential equation; that is, we consider   cid:3    cid:3  cid:3  cid:3   and y  do  y  4  = f  x, y, y   cid:3  cid:3      We assume that two boundary conditions are prescribed at each end of the so- lution domain  a, b . Problems of this form are commonly encountered in beam theory.  Again we divide the solution domain into m intervals of length h each. Replacing the derivatives of y by ﬁnite differences at the mesh points, we get the ﬁnite difference equations  &  yi−2 − 4yi−1 + 6yi − 4yi+1 + yi+2  h4  xi, yi,  yi−1 − 2yi + yi+1  h2  y−2 − 4y−1 + 6y0 − 4y1 + y2 − h4 f  where i = 0, 1, . . . , m. It is more revealing to write these equations as & y−1 − 2y0 + y1 = 0 & y0 − 2y1 + y2 &  y−1 − 4y0 + 6y1 − 4y2 + y3 − h4 f  x1, y1,  x0, y0,  = 0  h2  h2  y0 − 4y1 + 6y2 − 4y3 + y4 − h4 f  x2, y2,  y1 − 2y2 + y3  h2  = 0  %  = f % % %   8.12    8.13a    8.13b    8.13c   &  %  ... ym−3 − 4ym−2 + 6ym−1 − 4ym + ym+1 − h4 f  ym−2 − 2ym−1 + ym &  ym−2 − 4ym−1 + 6ym − 4ym+1 + ym+2 − h4 f  = 0  8.13d  = 0  8.13e  We now see that there are four unknowns— y−2, y−1, ym+1, and ym+2— that lie outside the solution domain and must be eliminated by applying the boundary conditions, a task that is facilitated by Table 8.1.  ym−1 − 2ym + ym+1  xm−1, ym−1,  xm, ym,  %  h2  h2   cid:3  cid:3  cid:3    a  = α2. The other one is y  The astute observer may notice that some combinations of boundary condi- tions will not work in eliminating the “spillover.” One such combination is clearly y a  = α1 and y  a  = α2. In the context of beam theory, this makes sense: We can impose either a displacement y or a shear at a point, but it is impossible to enforce both simultaneously. Similarly, force E Iy and the bending moment it makes no physical sense to prescribe both the slope y E Iy   a  = α1 and y  at the same point.   cid:3  cid:3  cid:3    cid:3  cid:3    cid:3  cid:3    cid:3    cid:3    314  Two-Point Boundary Value Problems  Boundary cond. y a  = α  a  = α  cid:3  y  a  = α  cid:3  cid:3  y  a  = α  cid:3  cid:3  cid:3  y y b  = β  b  = β  cid:3  y  b  = β  cid:3  cid:3  y  b  = β  cid:3  cid:3  cid:3  y  Equivalent ﬁnite difference expression y0 = α y−1 = y1 − 2hα y−1 = 2y0 − y1 + h2α y−2 = 2y−1 − 2y1 + y2 − 2h3α ym = β ym+1 = ym−1 + 2hβ ym+1 = 2ym − ym−1 + h2β ym+2 = 2ym+1 − 2ym−1 + ym−2 + 2h3β Table 8.1. Finite difference approximations for boundary conditions.  EXAMPLE 8.8  P  L  x  v   cid:19  cid:19  cid:19  cid:19   The uniform beam of length L and bending rigidity E I is attached to rigid supports at both ends. The beam carries a concentrated load P at its mid-span. If we utilize sym- metry and model only the left half of the beam, the displacement v can be obtained by solving the boundary value problem   cid:19  cid:19  cid:19  cid:19   E I  d 4v dx4  = 0   cid:19  cid:19  cid:19  cid:19   vx=0 = 0  dv dx  x=0  = 0  dv dx  x=L 2  = 0  E I  d3v dx3  x=L 2  = −P 2  Use the ﬁnite difference method to determine the displacement and the bending mo- ment M = −E I d2v dx2 at the mid-span  the exact values are v = P L3  192E I   and M = P L 8 . Solution. By introducing the dimensionless variables  the problem becomes  ξ = x L  y = E I  P L3 v  = 0  d4y dξ 4   cid:19  cid:19  cid:19  cid:19   yξ=0 = 0  dy dξ  ξ=0  = 0  dy dξ  ξ=1 2  = 0  d3y dξ 3  = − 1 2  ξ=1 2  We now proceed to writing Eqs.  8.13 , taking into account the boundary con- ditions. Referring to Table 8.1, the ﬁnite difference expressions of the boundary   cid:19  cid:19  cid:19  cid:19    cid:19  cid:19  cid:19  cid:19    315  8.3 Finite Difference Method conditions at the left end are y0 = 0 and y−1 = y1. Hence Eqs.  8.13a  and  8.13b  become   a    b    c    d    e   Equation  8.13c  is  y0 = 0 −4y0 + 7y1 − 4y2 + y3 = 0  y0 − 4y1 + 6y2 − 4y3 + y4 = 0  At the right end the boundary conditions are equivalent to ym+1 = ym−1 and  ym+2 = 2ym+1 + ym−2 − 2ym−1 + 2h3 −1 2  = ym−2 − h3  Substitution into Eqs.  8.13d  and  8.13e  yields  ym−3 − 4ym−2 + 7ym−1 − 4ym = 0 2ym−2 − 8ym−1 + 6ym = h3  The coefﬁcient matrix of Eqs.  a - e  can be made symmetric by dividing Eq.  e  by 2.  The result is ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  0 1 0 7 −4 0 1 6 −4 0 −4 1 ... ... ... ... ... 1 −4 6 −4 1 1 −4 7 −4 1 −4 3  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  =  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  y0 y1 y2 ... ym−2 ym−1 ym  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  0 0 0 ... 0 0  0.5h3  This system of equations can be solved with the decomposition and back sub- stitution routines in module LUdecomp5—see Section 2.4. Recall that LUdecomp5 works with the vectors d, e, and f that form the diagonals of the upper half of the ma- trix. The constant vector is denoted by b. The program that sets up solves the equa- tions is as follows:  def equations x,h,m :  Set up finite difference eqs.  ! usr bin python   example8_8  import numpy as np  from LUdecomp5 import *  h4 = h**4  d = np.ones m + 1 *6.0  e = np.ones m * -4.0   f = np.ones m-1   b = np.zeros m+1    316  Two-Point Boundary Value Problems  d[0] = 1.0  d[1] = 7.0  e[0] = 0.0  f[0] = 0.0  d[m-1] = 7.0  d[m] = 3.0  b[m] = 0.5*h**3  return d,e,f,b  xStart = 0.0  xStop = 0.5  m = 20   x at left end   x at right end   Number of mesh spaces  h =  xStop - xStart  m  x = np.arange xStart,xStop + h,h   d,e,f,b = equations x,h,m   d,e,f = LUdecomp5 d,e,f   y = LUsolve5 d,e,f,b   print ’\n  x  y’   print ’{:14.5e} {:14.5e}’.format x[m-1],y[m-1]    print ’{:14.5e} {:14.5e}’.format x[m],y[m]    input "\nPress return to exit"   We ran the program with m = 20, printing only the last two lines of the solution:  x  y  4.75000e-01  5.19531e-03  5.00000e-01  5.23438e-03  Thus at the mid-span we have vx=0.5L = P L3 , E I   cid:19  cid:19  cid:19  cid:19   -   cid:19  cid:19  cid:19  cid:19   d2v dx2  x=0.5L  = P L3 E I  1 L2  d2y dξ 2  ξ=0.5  ≈ P L E I  ym−1 − 2ym + ym+1  h2  yξ=0.5 = 5.234 38 × 10  −3 P L3 E I   5.19531 − 2 5.23438  + 5.19531  × 10  −3  0.0252  = P L E I  = −0.125 024  P L E I   cid:19  cid:19  cid:19  cid:19   Mx=0.5L = −E I  d2v dx2  ξ=0.5  = 0.125 024 P L   317  8.3 Finite Difference Method  In comparison, the exact solution yields  vx=0.5L = 5.208 33 × 10 Mx=0.5L = = 0.125 000 P L  −3 P L3 E I  PROBLEM SET 8.2  Problems 1–5 Use ﬁrst central difference approximations to transform the boundary value problem shown into simultaneous equations Ay = b.  Problems 6–10 Solve the given boundary value problem with the ﬁnite difference method using m = 20.   cid:3    1  = 5.  y y 1  = 1. y 1  = 0.  0  = 1,  cid:3  y  0  = 0,  cid:3  cid:3  y 2  = 3.   cid:3  cid:3  =  2 + x y, 1. y  cid:3  cid:3  = y + x2, 2. y  cid:3  cid:3  = e −x y  cid:3  , 3. y  cid:3  cid:3  − y, 4. y  4  = y 5. y  4  = −9y + x, 6.  cid:2  y 7.  cid:2 y 8.  cid:2 x2y 9.  cid:2  y 10.  cid:2  y   cid:3  cid:3  = xy,  cid:3  cid:3  + 2y  cid:3  cid:3  + xy  cid:3  cid:3  = y 2 sin y,  cid:3  cid:3  + 2y 2xy  y 0  = 0, y 0  = 0, y 0  = 1, y 0  = 0, y 0  = y y 1  = 1.5,  cid:3  + y = 0,  cid:3  y  y 0  = 0,  0  = 0,  cid:3  + y  = 0,   cid:3  + y = 0,   2 + x2   −1.  11.  cid:2   y 1  = 0,  1  = y  cid:3  cid:3  cid:3   cid:3   y   cid:3    1  = −1.  y   1  = 0.  y 1  = 1. Exact solution is y = xe1−x.  y 1  = 0, y 2  = 0.638961. Exact solution is y = sin ln x .  1  = −2 9. Exact solution is y =  y π  = 1. y 0  = 1 2,  y   cid:3   I0 L 4  v  w0  L 2  I0 L 4  1I  x  The simply supported beam consists of three segments with the moments of in- ertia I0 and I1 as shown. A uniformly distributed load of intensity w0 acts over the middle segment. Modeling only the left half of the beam, the differential equa- tion  for the displacement v is  d2v dx2  = −w0L2 4E I0  ×  d 2v dx2  = − M E I  ⎧⎪⎪⎪⎪⎨ ⎪⎪⎪⎪⎩  *  x L  I0 I1  in 0 < x <  %  +  &2  − 2  x L  x L  − 1 4  in  < x <  L 4  L 4  L 2   318  Two-Point Boundary Value Problems  Introducing the dimensionless variables  ξ = x L  y = E I0 w0L4 v  γ = I1 I0  the differential equation becomes  ⎧⎪⎪⎪⎪⎨ ⎪⎪⎪⎪⎩  − 1 4  ξ  − 1 4γ  =  d2y dξ 2  *   cid:19  cid:19  cid:19  cid:19   %  +  &2  ξ − 2  ξ − 1 4  in 0 < ξ <  in  < ξ <  1 4  1 2  1 4   cid:19  cid:19  cid:19  cid:19   with the boundary conditions = d 2y dξ 2  yξ=0  = dy dξ  ξ=0  = d 3y dξ 3  ξ=1 2  ξ=1 2  = 0   cid:19  cid:19  cid:19  cid:19   Use the ﬁnite difference method to determine the maximum displacement of the beam using m = 20 and γ = 1.5 and compare it with the exact solution  12.  cid:2   vmax = 61 9216  w0L4 E I0  M0  d0  x  v  d  L  1d  The simply supported, tapered beam has a circular cross section. A couple of magnitude M0 is applied to the left end of the beam. The differential equation for the displacement v is  where  Substituting  the differential equation becomes  with the boundary conditions  d 2v dx2  = − M E I %  ’ 1 +  d1 d0  = − M0 1 − x L  E I0 d d0 4 & − 1     x L  I0 = πd4 0 64  d = d0  ξ = x L  y = E I0  M0 L2 v  δ = d1 d0  = −  d2y dξ 2  1 − ξ  [1 +  δ − 1 ξ]4   cid:19  cid:19  cid:19  cid:19    cid:19  cid:19  cid:19  cid:19   yξ=0  = d 2y dx2  = yξ=1  = d2y dx2  ξ=0  ξ=1  = 0   319  8.3 Finite Difference Method  Solve the problem with the ﬁnite difference method with δ = 1.5 and m = 20; plot y vs. ξ. The exact solution is  y = −  3 + 2δξ − 3ξ ξ 2 6 1 + δξ − ξ 2   + 1 3δ  13.  cid:2  Solve Example 8.4 by the ﬁnite difference method with m = 20. Hint: Compute 14.  cid:2  Solve Prob. 20 in Problem Set 8.1 with the ﬁnite difference method. Use m = 20. 15.  cid:2   end slopes from second noncentral differences in Tables 5.3.  w0  L  v  x  The simply supported beam of length L is resting on an elastic foundation of stiffness k N m2. The displacement v of the beam due to the uniformly dis- tributed load of intensity w0 N m is given by the solution of the boundary value problem  E I  d 4v dx4  + kv = w0,  vx=0 = d 2y dx2  = vx=L = d2v dx2  x=0  x=L  = 0  The nondimensional form of the problem is  + γ y = 1,  d2y dξ 4  yξ=0  = d 2y dx2  = yξ=1  = d 2y dx2  ξ−0  ξ=1  = 0  where   cid:19  cid:19  cid:19  cid:19    cid:19  cid:19  cid:19  cid:19   ξ = x L  y = E I  w0 L4 v  γ = k L4 E I  Solve this problem by the ﬁnite difference method with γ = 105 and plot y vs. ξ. 16.  cid:2  Solve Prob. 15 if the ends of the beam are free and the load is conﬁned to the middle half of the beam. Consider only the left half of the beam, in which case the nondimensional form of the problem is  d 2y dξ 2  = d 3y dξ 3  ξ=0  = dy dξ  ξ=0  = d 3y dξ 3  ξ=1 2  ξ=1 2  = 0  17.  cid:2  The general form of a linear, second-order boundary value problem is  + γ y =  d4y dξ 4  0 in 0 < ξ < 1 4 1 in 1 4 < ξ < 1 2     cid:19  cid:19  cid:19  cid:19    cid:19  cid:19  cid:19  cid:19    cid:19  cid:19  cid:19  cid:19   y   cid:3    cid:3  cid:3  = r  x  + s x y + t x y  a  = α y a  = α or y  b  = β y b  = β or y   cid:3    cid:3    cid:19  cid:19  cid:19  cid:19    cid:19  cid:19  cid:19  cid:19    cid:19  cid:19  cid:19  cid:19    320  Two-Point Boundary Value Problems  Write a program that solves this problem with the ﬁnite difference method for any user-speciﬁed r  x , s x , and t x . Test the program by solving Prob. 8.  18.  cid:2   a  r  .  a 2  200 Co  0o  ◦ C. At the same time The thick cylinder conveys a ﬂuid with a temperature of 0 C. The differential equation the cylinder is immersed in a bath that is kept at 200 and the boundary conditions that govern steady-state heat conduction in the cylinder are  ◦  d 2T dr 2  = − 1 r  dT dr  Tr=a  2 = 0  T r=a = 200 ◦  C  where T is the temperature. Determine the temperature proﬁle through the thickness of the cylinder with the ﬁnite difference method, and compare it with the analytical solution  %  &  T = 200  1 − ln r  a ln 0.5   9  Symmetric Matrix Eigenvalue Problems  Find λ for which nontrivial solutions of Ax = λx exist.  9.1  Introduction  The standard form of the matrix eigenvalue problem is   9.1  where A is a given n × n matrix. The problem is to ﬁnd the scalar λ and the vector x. Rewriting Eq.  9.1  in the form  Ax = λx  it becomes apparent that we are dealing with a system of n homogeneous equations. An obvious solution is the trivial one x = 0. A nontrivial solution can exist only if the determinant of the coefﬁcient matrix vanishes; that is, if   A − λI  x = 0  A − λI= 0  Expansion of the determinant leads to the polynomial equation, also known as the characteristic equation  a0 + a1λ + a2λ2 + ··· + anλn = 0  which has the roots λi, i = 1, 2, . . . , n, called the eigenvalues of the matrix A. The solutions xi of  A − λiI  x = 0 are known as the eigenvectors.  As an example, consider the matrix  ⎡ ⎢⎣ 1 −1 −1 0 −1  0 2 −1 1  ⎤ ⎥⎦  A =  321   9.2    9.3    a    322  Symmetric Matrix Eigenvalue Problems  The characteristic equation is  A − λI =  The roots of this equation are λ1 = 0, λ2 = 1, λ3 = 3. To compute the eigenvector cor- responding the λ3, we substitute λ = λ3 into Eq.  9.2 , obtaining  −1 0   cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  = −3λ + 4λ2 − λ3 = 0 0 2 − λ −1 1 − λ −1 ⎡ ⎤ ⎡ ⎤ ⎢⎣ x1 ⎥⎦ ⎢⎣ 0 ⎥⎦ =   cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  1 − λ −1 ⎡ ⎢⎣−2 −1 0 −1 −1 −1 0 −1 −2  ⎤ ⎥⎦  0 0  x2 x3   b    c   We know that the determinant of the coefﬁcient matrix is zero, so that the equations are not linearly independent. Therefore, we can assign an arbitrary value to any one component of x and use two of the equations to compute the other two components. Choosing x1 = 1, the ﬁrst equation of Eq.  c  yields x2 = −2, and from the third equa- tion we get x3 = 1. Thus the eigenvector associated with λ3 is  The other two eigenvectors  ⎡ ⎢⎣ 1−2  ⎤ ⎥⎦  1  x3 = ⎤ ⎥⎦  ⎡ ⎢⎣ 1 0−1  x2 =  x1 =  ⎡ ⎢⎣ 1  ⎤ ⎥⎦  1 1  can be obtained in the same manner.  For the problem at hand, this matrix is  It is sometimes convenient to display the eigenvectors as columns of a matrix X.   cid:14   X =  x1  x2  x3   cid:15   =  ⎡ ⎢⎣ 1 1 1 0 −2 1 1 −1 1  ⎤ ⎥⎦  It is clear from this example that the magnitude of an eigenvector is indetermi- nate; only its direction can be computed from Eq.  9.2 . It is customary to normalize the eigenvectors by assigning a unit magnitude to each vector. Thus the normalized eigenvectors in our example are  ⎡ ⎢⎣ 1   X =  √ √ √ √ √ 1  1  3 2 0 −2  √ √ √ 1  3 3 −1  1  2 1   6 6 6  ⎤ ⎥⎦  Throughout this chapter we assume that the eigenvectors are normalized.  Here are some useful properties of eigenvalues and eigenvectors, given without  proof:   All the eigenvalues of a symmetric matrix are real.   All eigenvalues of a symmetric, positive-deﬁnite matrix are real and positive.   The eigenvectors of a symmetric matrix are orthonormal; that is, XT X = I.   If the eigenvalues of A are λi, then the eigenvalues of A  −1 are λ−1  .  i   323  9.1 Introduction  Eigenvalue problems that originate from physical problems often end up with a symmetric A. This is fortunate, because symmetric eigenvalue problems are easier to solve than their nonsymmetric counterparts  which may have complex eigenvalues . In this chapter we largely restrict our discussion to eigenvalues and eigenvectors of symmetric matrices.  Common sources of eigenvalue problems are analyses of vibrations and stability.  These problems often have the following characteristics:   The matrices are large and sparse  e.g., have a banded structure .   We need to know only the eigenvalues; if eigenvectors are required, only a few of  them are of interest.  A useful eigenvalue solver must be able to use these characteristics to minimize the computations. In particular, it should be ﬂexible enough to compute only what we need and no more.  EXAMPLE 9.1  The stress matrix  tensor  corresponding to the state of stress shown is   each row of the matrix consists of the three stress components acting on a coordi- nate plane . It can be shown that the eigenvalues of S are the principal stresses and that the eigenvectors are normal to the principal planes. Determine the principal stresses and the eigenvectors. Solution. The characteristic equation S − λI= 0 is  40 MPa  x2  30 MPa  30 MPa  80 MPa  x1  60 MPa  x3  ⎡ ⎢⎣ 80 30  30 40 0  0 0 0 60  ⎤ ⎥⎦ MPa  S =   cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  80 − λ  30 0   cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  = 0  30 40 − λ  0  0 0  60 − λ   60 − λ  [ 80 − λ  40 − λ  − 900] = 0  60 − λ   λ2 − 120λ + 2300  = 0  Expanding the determinant gives us  which yields the principal stresses,  λ1 = 23.944 MPa  λ2 = 60 MPa  λ3 = 96.056 MPa   324  Symmetric Matrix Eigenvalue Problems  The ﬁrst eigenvector is the solution of  ⎡ ⎢⎣ 56.056  30.0  30.0 16.056  0  0  36.056   cid:31   cid:30  x = 0, or S − λ1I ⎤ ⎡ ⎤ ⎡ ⎥⎦ = ⎢⎣ 0 ⎥⎦ ⎢⎣ x1  0 0  ⎤ ⎥⎦  x2 x3  0 0  Choosing x1 = 1, we get x2 = −56.056 30 = −1.8685 and x3 = 0. Thus the normal- ized eigenvector is  The second eigenvector is obtained from the equation   cid:14    cid:30   0.4719 −0.8817 0 ⎡ ⎢⎣ 0  x1 = ⎡ ⎢⎣ 20 0 30 −20 0 0 0  ⎡ ⎢⎣ x1  ⎤ ⎥⎦ =  ⎤ ⎥⎦  x2 x3  0 0  30  0  ⎤ ⎥⎦   cid:31   S − λ2I  x = 0:  This equation is satisﬁed with x1 = x2 = 0 and any value of x3. Choosing x3 = 1, the eigenvector becomes  The third eigenvector is obtained by solving   cid:14   x2 =  0 0 1   cid:30    cid:15 T  cid:31  x = 0: S − λ3I ⎤ ⎤ ⎡ ⎥⎦ = ⎥⎦ ⎢⎣ x1  0 0  30  −56.056  −36.056  ⎡ ⎢⎣ 0  ⎤ ⎥⎦  0 0  ⎡ ⎢⎣−16.056  30 0  0   cid:14   x3 =  0.8817 0.4719 0  With the choice x1 = 1, this yields x2 = 16.056 30 = 0.5352 and x3 = 0. After normal- izing we get   cid:15 T  x2 x3   cid:15 T  9.2  Jacobi Method  Jacobi method is a relatively simple iterative procedure that extracts all the eigen- values and eigenvectors of a symmetric matrix. Its utility is limited to small matrices  say, less than 50 × 50 , because the computational effort increases very rapidly with the size of the matrix. The main strength of the method is its robustness—it seldom fails to deliver.  Similarity Transformation and Diagonalization  Consider the standard matrix eigenvalue problem  Ax = λx   9.4    or  or  325  9.2 Jacobi Method  where A is symmetric. Let us now apply the transformation  x = Px ∗   9.5   where P is a nonsingular matrix. Substituting Eq.  9.5  into Eq.  9.4  and pre- multiplying each side by P  −1, we get  −1APx  ∗ = λP  −1Px  ∗  P  ∗  ∗ = λx x  ∗  A  ∗ = P   9.6  −1AP. Because λ was untouched by the transformation, the eigenval- where A ues of A are also the eigenvalues of A . Matrices that have the same eigenvalues are deemed to be similar, and the transformation between them is called a similarity transformation.  ∗  Similarity transformations are frequently used to change an eigenvalue problem to a form that is easier to solve. Suppose that we managed by some means to ﬁnd a P that diagonalizes A  ∗  ⎡ ⎢⎢⎢⎢⎣  A  . Equations  9.6  then are − λ ∗ 11 0 ... 0  ··· 0 − λ ··· ... ... ··· A 0  ∗ 22  A  0 0 ...  − λ  ∗ nn  ⎤ ⎥⎥⎥⎥⎦  ⎡ ⎢⎢⎢⎢⎣  ⎤ ⎥⎥⎥⎥⎦ =  ⎡ ⎢⎢⎢⎢⎣  ⎤ ⎥⎥⎥⎥⎦  0 0 ... 0  ∗ 1 ∗ 2  x x ... ∗ x n  which have the solutions  ∗ 11  λ1 = A ⎡ ⎤ ⎢⎢⎢⎢⎣ ⎥⎥⎥⎥⎦  =  1 0 ... 0  ∗ 1  x  λ2 = A ⎡ ⎢⎢⎢⎢⎣  =  ∗ 2  x  ∗ 22  ···  ⎤ ⎥⎥⎥⎥⎦ ···  ∗ nn  λn = A ⎡ ⎢⎢⎢⎢⎣  ∗ x n  =  0 1 ... 0  ⎤ ⎥⎥⎥⎥⎦  0 0 ... 1   cid:14    cid:15   ∗ = X  ∗ 1  x  ∗ x 2  ···  ∗ x n  = I  According to Eq.  9.5  the eigenvectors of A are  X = PX  ∗ = PI = P  Hence the transformation matrix P contains the eigenvectors of A, and the eigenval- ∗ ues of A are the diagonal terms of A  .  Jacobi Rotation  A special similarity transformation is the plane rotation  x = Rx ∗   9.7    9.8    9.9    326  Symmetric Matrix Eigenvalue Problems  where  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  k   cid:12   1 0 0 0 1 0 0 0 c 0 0 0 0 0 0 0 0 −s 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 s 0 0 1 0 0 0 0 0 1 0 0 0 0 0 c 0 0 0 0 0 1 0 0 0 0 0 1  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  k   cid:12   R =   9.10    9.11    9.12    9.13   is called the Jacobi rotation matrix. Note that R is an identity matrix modiﬁed by the terms c = cos θ and s = sin θ appearing at the intersections of columns rows k and  cid:12 , where θ is the rotation angle. The rotation matrix has the useful property of being orthogonal, meaning that  −1 = RT R  One consequence of orthogonality is that the transformation in Eq.  9.5  has the es- sential characteristic of a rotation: It preserves the magnitude of the vector; that is, x = x  ∗.  The similarity transformation corresponding to the plane rotation in Eq.  9.9  is  ∗ = R  −1AR = RT AR  A  ∗ not only has the same eigenvalues as the original matrix A but thanks The matrix A to orthogonality of R, it is also symmetric. The transformation in Eq.  9.12  changes only the rows columns k and  cid:12  of A. The formulas for these changes are  A  A  A  A  = c2A kk + s 2A  cid:12  cid:12  − 2csA k cid:12  ∗ kk  cid:12  cid:12  = c2A  cid:12  cid:12  + s 2A kk + 2csA k cid:12  ∗ k cid:12  = A ∗ = A ∗ ki = A ∗  cid:12 i  =  c2 − s 2 A k cid:12  + cs A kk − A  cid:12  cid:12   ∗  cid:12 k = cA ki − sA  cid:12 i, i  cid:7 =  cid:12  ∗ ik i  cid:7 =  cid:12  i cid:12  = cA  cid:12 i + sA ki, ∗  i  cid:7 = k, i  cid:7 = k,  A  Jacobi Diagonalization  = 0. This The angle θ in the Jacobi rotation matrix can be chosen so that A suggests the following idea: Why not diagonalize A by looping through all the off- diagonal terms and zero them one by one? This is exactly what the Jacobi method does. However, there is a major snag—the transformation that annihilates an off- diagonal term also undoes some of the previously created zeroes. Fortunately, it turns out that the off-diagonal terms that reappear will be smaller than before. Thus the Jacobi method is an iterative procedure that repeatedly applies Jacobi rotations until  = A  ∗ k cid:12   ∗  cid:12 k   327  9.2 Jacobi Method  the off-diagonal terms have virtually vanished. The ﬁnal transformation matrix P is the accumulation of individual rotations Ri:  P = R1·R2·R3 · ··   9.14   The columns of P ﬁnish up as the eigenvectors of A, and the diagonal elements of ∗ = PT AP become the eigenvectors. A = 0  Let us now look at details of a Jacobi rotation. From Eq.  9.13  we see that A  ∗ k cid:12   if   c2 − s 2 A k cid:12  + cs A kk − A  cid:12  cid:12   = 0   a  Using the trigonometric identities c2 − s 2 = cos2 θ − sin2 θ = cos 2θ and cs = cos θ sin θ =  1 2  sin 2θ, Eq.  a  yields   b  which could be solved for θ, followed by computation of c = cos θ and s = sin θ. How- ever, the procedure described next leads to a better algorithm.1  A kk − A  cid:12  cid:12   tan 2θ = − 2A k cid:12   Introducing the notation  φ = cot 2θ = − A kk − A  cid:12  cid:12   2A k cid:12    9.15   and using the trigonometric identity  tan 2θ = where t = tan θ, Eq.  b  can be written as  2t   1 − t 2   which has the roots  t 2 + 2φt − 1 = 0   cid:26  φ2 + 1  t = −φ ±  It has been found that the root t ≤ 1, which corresponds to θ ≤ 45 ◦ , leads to the more stable transformation. Therefore, we choose the plus sign if φ > 0 and the mi- nus sign if φ ≤ 0, which is equivalent to using −φ +   cid:26   &  %  t = sgn φ   φ2 + 1  To forestall excessive roundoff error if φ is large, we multiply both sides of the equa- tion by φ +   cid:25  φ2 + 1, which yields t =  In the case of very large φ, we should replace Eq.  9.16a  by the approximation   cid:25   sgn φ   φ +  φ2 + 1  t = 1 2φ   9.16a    9.16b   1 The procedure is adapted from Press, W.H. et al., Numerical Recipes in Fortran, 2nd ed., Cambridge  University Press, 1992.   328  Symmetric Matrix Eigenvalue Problems  to prevent overﬂow in the computation of φ2. Having computed t, we can use the  1 − cos2 θ   cos θ to obtain  trigonometric relationship tan θ = sin θ   cos θ = √ s = tc  c =  1√ 1 + t 2  We now improve the transformation formulas in Eqs.  9.13 . Solving Eq.  a  for  A  cid:12  cid:12 , we obtain   9.17    c   A  cid:12  cid:12  = A kk + A k cid:12   c2 − s 2  cs  A  Replacing all occurrences of A  cid:12  cid:12  by Eq.  c  and simplifying, the transformation for- mulas in Eqs.  9.13  can be written as = A kk − tA k cid:12  ∗ kk  cid:12  cid:12  = A  cid:12  cid:12  + tA k cid:12  ∗ k cid:12  = A ∗ = A ∗ A ki = A ∗  cid:12 i  = 0 ∗  cid:12 k = A ki − s A  cid:12 i + τ A ki , ∗ ik i cid:12  = A  cid:12 i + s A ki − τ A  cid:12 i , ∗  i  cid:7 = k, i  cid:7 = k,  i  cid:7 =  cid:12  i  cid:7 =  cid:12    9.18   A  A  A  where  τ = s 1 + c   9.19   The introduction of τ allowed us to express each formula in the form,  original value  +  change , which is helpful in reducing the roundoff error.  At the start of Jacobi’s diagonalization process the transformation matrix P is ini- tialized to the identity matrix. Each Jacobi rotation changes this matrix from P to ∗ = PR. The corresponding changes in the elements of P can be shown to be  only P the columns k and  cid:12  are affected  = Pik − s Pi cid:12  + τ Pik  ∗ P ik i cid:12  = Pi cid:12  + s Pik − τ Pi cid:12   ∗ P   9.20   We still have to decide the order in which the off-diagonal elements of A are to be eliminated. Jacobi’s original idea was to attack the largest element because doing so results in the fewest number of rotations. The problem here is that A has to be searched for the largest element before every rotation, which is a time-consuming process. If the matrix is large, it is faster to sweep through it by rows or columns and annihilate every element above some threshold value. In the next sweep the thresh- old is lowered and the process repeated.  There are several ways to choose the threshold. Our implementation starts by  computing the sum S of the elements above the principal diagonal of A:  S = n−1 cid:6   n cid:6    cid:19  cid:19    cid:19  cid:19 Aij  i=1  j=i+1   a    329  9.2 Jacobi Method Because there are n n − 1  2 such elements, the average magnitude of the off- diagonal elements is  2S  n n − 1   μ = 0.5S n  n − 1   The threshold we use is  which represents 0.25 times the average magnitude of the off-diagonal elements.  In summary, one sweep of Jacobi’s diagonalization procedure  which uses only  the upper half of the matrix , is as follows:   b   Calculate the threshold μ using Eqs.  a  and  b . Sweep over off-diagonal terms of A:   cid:19  cid:19 Aij   cid:19  cid:19  ≥ μ:  If  Compute φ, t, c, and s from Eqs. 9.15 – 9.17 . Compute τ from Eq.  9.19 . Modify the elements of A according to Eqs.  9.18 . Update the transformation matrix P using Eqs.  9.20 .  The sweeps are repeated until μ ≤ ε, where ε is the error tolerance. It takes usually 6  to 10 sweeps to achieve convergence.   cid:2  jacobi This function computes all eigenvalues λi and eigenvectors xi of a symmetric, n × n matrix A by the Jacobi method. The algorithm works exclusively with the upper tri- angular part of A, which is destroyed in the process. The principal diagonal of A is re- placed by the eigenvalues, and the columns of the transformation matrix P become the normalized eigenvectors.   module jacobi  ’’’ lam,x = jacobi a,tol = 1.0e-8 .  Solution of std. eigenvalue problem [a]{x} = lam{x}  by Jacobi’s method. Returns eigenvalues in vector {lam}  and the eigenvectors as columns of matrix [x].  ’’’  import numpy as np  import math  def jacobi a,tol = 1.0e-8 :  Jacobi method  def threshold a :  sum = 0.0  for i in range n-1 :   330  Symmetric Matrix Eigenvalue Problems  for j in range  i+1,n :  sum = sum + abs a[i,j]   return 0.5*sum n  n-1   def rotate a,p,k,l :  Rotate to make a[k,l] = 0  aDiff = a[l,l] - a[k,k]  if abs a[k,l]  < abs aDiff *1.0e-36: t = a[k,l] aDiff  else:  phi = aDiff  2.0*a[k,l]   t = 1.0  abs phi  + math.sqrt phi**2 + 1.0    if phi < 0.0: t = -t  c = 1.0 math.sqrt t**2 + 1.0 ; s = t*c  tau = s  1.0 + c   temp = a[k,l]  a[k,l] = 0.0  a[k,k] = a[k,k] - t*temp  a[l,l] = a[l,l] + t*temp  for i in range k :   Case of i < k  temp = a[i,k]  a[i,k] = temp - s* a[i,l] + tau*temp   a[i,l] = a[i,l] + s* temp - tau*a[i,l]   for i in range k+1,l :   Case of k < i < l  temp = a[k,i]  a[k,i] = temp - s* a[i,l] + tau*a[k,i]   a[i,l] = a[i,l] + s* temp - tau*a[i,l]   for i in range l+1,n :   Case of i > l  temp = a[k,i]  a[k,i] = temp - s* a[l,i] + tau*temp   a[l,i] = a[l,i] + s* temp - tau*a[l,i]   for i in range n :   Update transformation matrix  temp = p[i,k]  p[i,k] = temp - s* p[i,l] + tau*p[i,k]   p[i,l] = p[i,l] + s* temp - tau*p[i,l]   n = len a   p = np.identity n,float   for k in range 20 :  mu = threshold a    Compute new threshold  for i in range n-1 :   Sweep through matrix  for j in range i+1,n :  if abs a[i,j]  >= mu:  rotate a,p,i,j   if mu <= tol: return np.diagonal a ,p  print ’Jacobi method did not converge’    331  9.2 Jacobi Method   cid:2  sortJacobi  The eigenvalues eigenvectors returned by jacobi are not ordered. The function listed next can be used to sort the eigenvalues and eigenvectors into ascending or- der of eigenvalues.  Sorts the eigenvalues {lam} and eigenvectors [x]  in order of ascending eigenvalues.   module sortJacobi  ’’’ sortJacobi lam,x .  ’’’  import swap  def sortJacobi lam,x :  n = len lam   for i in range n-1 :  index = i  val = lam[i]  for j in range i+1,n :  if lam[j] < val:  index = j  val = lam[j]  if index != i:  swap.swapRows lam,i,index   swap.swapCols x,i,index   Transformation to Standard Form  Physical problems often give rise to eigenvalue problems of the form  Ax = λBx   9.21  where A and B are symmetric n × n matrices. We assume that B is also positive deﬁ- nite. Such problems must be transformed into the standard form before they can be solved by Jacobi diagonalization. Because B is symmetric and positive deﬁnite, we can apply Choleski decompo- sition B = LLT , where L is the lower triangular matrix  see Section 3.3 . Then we in- troduce the transformation  x =  L  −1 T z   9.22   Substituting into Eq.  9.21 , we get  Pre-multiplying both sides by L  −1 T z =λLLT  L  −1 T z  A L −1 results in −1 Tz = λL  −1A L L  −1LLT  L  −1 Tz   332  Symmetric Matrix Eigenvalue Problems  Noting that L  −1L = LT  L  −1 T = I, the last equation reduces to the standard form  where  Hz = λz  H = L  −1A L  −1 T  An important property of this transformation is that it does not destroy the symmetry of the matrix  i.e., symmetric A results in symmetric H . The general procedure for solving eigenvalue problems of the form Ax = λBx is  shown in the following box:  Use Choleski decomposition B = LLT to compute L. −1  a triangular matrix is easy to invert . Compute L Compute H = L Solve the standard eigenvalue problem Hz = λz. Recover the eigenvectors of the original problem from X =  L  −1A L  −1 T .  −1 T Z.  Note that the eigenvalues were untouched by the transformation.  An important special case is where B is a diagonal matrix:  ⎤ ⎥⎥⎥⎥⎦  0 0 ...  β  n  ⎡ ⎢⎢⎢⎢⎣  B =  ⎤ ⎥⎥⎥⎥⎦  0 0 ... β1 2  n  β 0 ... 0  1 0 β2 ... 0  ··· ··· ... ··· ⎡ ⎢⎢⎢⎢⎣ Hij = Aij cid:25   −1 = L  β  β  β  i  j  −1 2 1 0 ... 0  β  0 −1 2 2 ... 0  ··· ··· ... ···  ⎤ ⎥⎥⎥⎥⎦  0 0 ... −1 2 n  β  Here  ⎡ ⎢⎢⎢⎢⎣  L =  and  β1 2 1 0 ... 0  2  0 β1 2 ... 0  ··· ··· ... ···   cid:2  stdForm   9.23    9.24    9.25    9.26a    9.26b   Given the matrices A and B, the function stdForm returns H and the transformation matrix T =  L −1 T . The inversion of L is carried out by invert  the triangular shape of L allows this to be done by back substitution . Note that original A, B, and L are destroyed.   module stdForm  ’’’ h,t = stdForm a,b .  Transforms the eigenvalue problem [a]{x} = lam*[b]{x}   to the standard form [h]{z} = lam*{z}. The eigenvectors  are related by {x} = [t]{z}.  333  9.2 Jacobi Method  ’’’  import numpy as np  from choleski import *  def stdForm a,b :  def invert L :  Inverts lower triangular matrix L  n = len L   for j in range n-1 :  L[j,j] = 1.0 L[j,j]  for i in range j+1,n :  L[i,j] = -np.dot L[i,j:i],L[j:i,j]  L[i,i]  L[n-1,n-1] = 1.0 L[n-1,n-1]  n = len a   L = choleski b   invert L   h = np.dot b,np.inner a,L    return h,np.transpose L   EXAMPLE 9.2 The stress matrix  tensor  in Example 9.1 was  ⎡ ⎢⎣ 80 30  30 40 0  0 0 0 60  ⎤ ⎥⎦ MPa  S =   1  Determine the principal stresses by diagonalizing S with one Jacobi rotation; and  2  compute the eigenvectors.  Solution of Part 1 . To eliminate S12 we must apply a rotation in the 1-2 plane. With k = 1 and  cid:12  = 2, Eq.  9.15  is  φ = − S11 − S22  = − 80 − 40  2S12  2 30   = − 2 3  Equation  9.16a  then yields   cid:25   t =  sgn φ   φ +  φ2 + 1  =  2 3 +   cid:25  −1  2 3 2 + 1  = −0.535 18  According to Eqs.  9.18 , the changes in S due to the rotation are  ∗ S 11 ∗ S 22 ∗ S 12  = S11 − t S12 = 80 −  −0.535 18   30  = 96.055 MPa = S22 + t S12 = 40 +  −0.535 18   30  = 23.945 MPa = S  = 0  ∗ 21   334  Symmetric Matrix Eigenvalue Problems  Hence the diagonalized stress matrix is  ⎡ ⎢⎣ 96.055  ∗ =  S  ⎤ ⎥⎦  0 0 23.945 0  0 0 0 60  where the diagonal terms are the principal stresses.  Solution of Part  2 . To compute the eigenvectors, we start with Eqs.  9.17  and  9.19 , which yield  c =  1√ 1 + t 2  =   cid:25  1 +  −0.535 18 2  1  = 0.88168  s = tc =  −0.535 18   0.881 68  = −0.471 86 τ = s 1 + c  = −0.47186 1 + 0.881 68  = −0.250 77  We obtain the changes in the transformation matrix P from Eqs.  9.20 . Recalling that P is initialized to the identity matrix, the ﬁrst equation gives us  ∗ P 11  ∗ P 21  = P11 − s P12 + τ P11  = 1 −  −0.471 86   0 +  −0.250 77   1   = 0.881 67 = P21 − s P22 + τ P21  = 0 −  −0.471 86  [1 +  −0.250 77   0 ] = 0.471 86  Similarly, the second equation of Eqs.  9.20  yields  The third row and column of P are not affected by the transformation. Thus  ∗ The columns of P  are the eigenvectors of S.  EXAMPLE 9.3  ∗ P 12  ∗ P 22  = −0.471 86 ⎡ ⎢⎣ 0.88167 −0.47186 0  = 0.881 67 ⎤ ⎥⎦  0.47186 0  0.88167 0 0 1  ∗ = P  L  L  2L  i1  3C i1  i2  C i2  i3  C  i3   1  Show that the analysis of the electric circuit shown leads to a matrix eigenvalue problem.  2  Determine the circular frequencies and the relative amplitudes of the currents.   335  9.2 Jacobi Method  Solution of Part 1 . Kirchoff’s equations for the three loops are  L  L  3C  3C  di2 dt  di1 dt  + q2 − q1  + q1 − q2 + q2 − q3 C + q3 C Differentiating and substituting dqk  dt = ik, we get i2 = −LC i1 − 1 1 3 3 i2 − i3 = −LC i1 + 4 3 −i2 + 2i3 = −2LC  + q3 − q2  − 1 3  di3 dt  2L  C  = 0 = 0 = 0  d 2i1 dt 2 d 2i2 dt 2 d 2i3 dt 2  These equations admit the solution  ik t  = uk sin ωt  where ω is the circular frequency of oscillation  measured in rad s  and uk are the relative amplitudes of the currents. Substitution into Kirchoff’s equations yields ⎡ ⎤ Au = λBu  sin ωt cancels out , where ⎢⎣ 1 3 −1 3 ⎥⎦ 0 4 3 −1 −1 3 −1 2 0  ⎡ ⎢⎣ 1 0 0  0 1 0 0 0 2  λ = LC ω2  ⎤ ⎥⎦  B =  A =  which represents an eigenvalue problem of the nonstandard form.  Solution of Part  2 . Because B is a diagonal matrix, we could readily transform the problem into the standard form Hz = λz using Eqs.  9.26 . However, we chose to let the computer do all the work.  ! usr bin python   example9_3  import numpy  from jacobi import *  import math  from sortJacobi import *  from stdForm import *  A = np.array [[ 1 3, -1 3,  0.0],  [-1 3,  4 3, -1.0],  [ 0.0, -1.0,  2.0]]   [0.0, 1.0, 0.0],  [0.0, 0.0, 2.0]]   \  \  \  \  B = np.array [[1.0, 0.0, 0.0],   336  Symmetric Matrix Eigenvalue Problems  H,T = stdForm A,B    Transform into std. form  lam,Z = jacobi H    Z = eigenvecs. of H  X = np.dot T,Z    Eigenvecs. of original problem  sortJacobi lam,X    Arrange in ascending order of eigenvecs.  for i in range 3 :   Normalize eigenvecs.  X[:,i] = X[:,i] math.sqrt np.dot X[:,i],X[:,i]    print ’Eigenvalues:\n’,lam   print ’Eigenvectors:\n’,X   input  "Press return to exit"   Eigenvalues:  Eigenvectors:  [ 0.1477883  0.58235144  1.93652692]  [[ 0.84021782 -0.65122529 -0.18040571]  [ 0.46769473  0.48650067  0.86767582]  [ 0.27440056  0.58242829 -0.46324126]]  The circular frequencies are given by ωi = cid:25   λi  LC:  ω1 = 0.3844√ LC  ω2 = 0.7631√ LC  ω3 = 1.3916√ LC  9.3  Power and Inverse Power Methods  Inverse Power Method  The inverse power method is a simple and efﬁcient algorithm that ﬁnds the small- est eigenvalue λ1 and the corresponding eigenvector x1 of the standard eigenvalue problem  Ax = λx   9.27   The method works like this:  Let v be an unit vector  a random vector will do . Do the following until the change in v is negligible:  Solve Az = v for the vector z. Compute z. Compute v = z z.  At the conclusion of the procedure we have z = ±1 λ1 and v = x1. The sign of λ1 is determined as follows: If z changes sign between successive iterations, λ1 is neg- ative; otherwise, use the plus sign.   337  9.3 Power and Inverse Power Methods  Let us now investigate why the method works. Since the eigenvectors xi of A are orthonormal  linearly independent , they can be used as the basis for any n-dimensional vector. Thus v and z admit the unique representations  where vi and zi are the components of v and z with respect to the eigenvectors xi. Substitution into Az = v yields   9.28   v = n cid:6   i=1  vixi  z = n cid:6   i=1  zixi  zixi − n cid:6   i=1  vixi = 0   zi λi − vi  xi = 0  A  n cid:6   i=1  n cid:6   i=1  zi = vi λi n cid:6   z = n cid:6  xi = 1 vi % λi λ1 v1x1 + v2  i=1 = 1 λ1  xi  vi  λ1 λi i=1 x2 + v3 λ1 λ2  &  x3 + ···  λ1 λ3  But Axi = λixi, so that  Hence  It follows from Eq.  9.28  that   9.29  Since λ1 λi < 1  i  cid:7 = 1 , we observe that the coefﬁcient of x1 has become more promi- nent in z than it was in v; hence z is a better approximation to x1. This completes the ﬁrst iterative cycle. In subsequent cycles we set v = z z and repeat the process. Each iteration in- creases the dominance of the ﬁrst term in Eq.  9.29  so that the process converges to   at this stage v1 = 1 since v = x1, so that v1 = 1, v2 = v3 = ··· = 0 .  The inverse power method also works with the nonstandard eigenvalue problem  provided that Az = v in the algorithm is replaced by  z = 1 λ1  v1x1 = 1 λ1  x1  Ax = λBx  Az = Bv   9.30    9.31   The alternative is, of course, to transform the problem to standard form before ap- plying the power method.   338  Symmetric Matrix Eigenvalue Problems  Eigenvalue Shifting  By inspection of Eq.  9.29  we see that the speed of convergence is determined by the strength of the inequality λ1 λ2 < 1  the second term in the equation . If λ2 is well separated from λ1, the inequality is strong and the convergence is rapid. In contrast, close proximity of these two eigenvalues results in very slow convergence.  The rate of convergence can be improved by a technique called eigenvalue shift-  ing. Letting  where s is a predetermined “shift,” the eigenvalue problem in Eq.  9.27  is trans- formed to  λ = λ∗ + s  Ax =  λ∗ + s x  ∗  x = λ∗  A  x  ∗ = A − sI   9.32    9.33   or  where   9.34  Solving the transformed problem in Eq.  9.33  by the inverse power method yields λ∗ 1 and x1, where λ∗ . The corresponding eigenvalue of the original problem, λ = λ∗  + s, is thus the eigenvalue closest to s.  ∗ 1 is the smallest eigenvalue of A  A  1  Eigenvalue shifting has two applications. An obvious one is the determination of the eigenvalue closest to a certain value s. For example, if the working speed of a shaft is s rpm, it is imperative to assure that there are no natural frequencies  which are related to the eigenvalues  close to that speed.  Eigenvalue shifting is also used to speed up convergence. Suppose that we are computing the smallest eigenvalue λ1 of the matrix A. The idea is to introduce a shift = λ1 − s, we should choose s ≈ λ1 s that makes λ∗  s = λ1 should be avoided to prevent division by zero . Of course, this method works only if we have a prior estimate of λ1.   λ∗ 2 as small as possible. Since λ∗  1  1  The inverse power method with eigenvalue shifting is a particularly powerful tool for ﬁnding eigenvectors if the eigenvalues are known. By shifting very close to an eigenvalue, the corresponding eigenvector can be computed in one or two iterations.  Power Method  The power method converges to the eigenvalue farthest from zero and the asso- ciated eigenvector. It is very similar to the inverse power method; the only dif- ference between the two methods is the interchange of v and z in Eq.  9.28 .   339  9.3 Power and Inverse Power Methods  The outline of the procedure is  Let v be an unit vector  a random vector will do . Do the following until the change in v is negligible:  Compute the vector z = Av. Compute z. Compute v = z z.  At the conclusion of the procedure we have z = ±λn and v = xn  the sign of λn  is determined in the same way as in the inverse power method .   cid:2  inversePower  Given the matrix A and the shift s, the function inversePower returns the eigen- ∗ = A − sI is value of A closest to s and the corresponding eigenvector. The matrix A decomposed as soon as it is formed, so that only the solution phase  forward and back substitution  is needed in the iterative loop. If A is banded, the efﬁciency of the program could be improved by replacing LUdecomp and LUsolve by functions that specialize in banded matrices  e.g., LUdecomp5 and LUsolve5 —see Example 9.6. The program line that forms A must also be modiﬁed to be compatible with the stor- age scheme used for A.  ∗   module inversePower  ’’’ lam,x = inversePower a,s,tol=1.0e-6 .  Inverse power method for solving the eigenvalue problem  [a]{x} = lam{x}. Returns ’lam’ closest to ’s’ and the  corresponding eigenvector {x}.  ’’’  import numpy as np  from LUdecomp import *  import math  from random import random  def inversePower a,s,tol=1.0e-6 :  n = len a   aStar = a - np.identity n *s   Form [a*] = [a] - s[I]  aStar = LUdecomp aStar    Decompose [a*]  for i in range n :   Seed [x] with random numbers  xMag = math.sqrt np.dot x,x    Normalize [x]  x = np.zeros n   x[i] = random    x =x xMag  for i in range 50 :  xOld = x.copy     Begin iterations   Save current [x]   340  Symmetric Matrix Eigenvalue Problems  x = LUsolve aStar,x    Solve [a*][x] = [xOld]  xMag = math.sqrt np.dot x,x    Normalize [x]  if np.dot xOld,x  < 0.0:   Detect change in sign of [x]  x = x xMag  sign = -1.0  x = -x  else: sign = 1.0  if math.sqrt np.dot xOld - x,xOld - x   < tol:  return s + sign xMag,x  print ’Inverse power method did not converge’   EXAMPLE 9.4 The stress matrix describing the state of stress at a point is  ⎡ ⎢⎣−30 10 20 40 −50 10 20 −50 −10  ⎤ ⎥⎦ MPa  S =  Determine the largest principal stress  the eigenvalue of S farthest from zero  by the power method.  Solution. First Iteration:  Let v =  1 0 0   cid:15 T   cid:14   z = Sv =  1  0 0  =  ⎤ ⎥⎦  ⎤ ⎥⎦  ⎤ ⎥⎦  ⎤ ⎥⎦  10.0 20.0  10.0 20.0  ⎡ ⎢⎣ 1  ⎤ ⎥⎦ =  ⎡ ⎢⎣−30.0  be the initial guess for the eigenvector. Then  ⎡ ⎢⎣−30 10 20 40 −50 10 20 −50 −10  cid:25  z = 302 + 102 + 202 = 37.417 ⎡ ⎡ ⎢⎣−0.801 77 ⎢⎣−30.0 ⎡ ⎢⎣ 37.416 −24.053 −34.744  v = zz = ⎡ ⎢⎣−30 10 20 40 −50 10 20 −50 −10  cid:25  37.4162 + 24.0532 + 34.7442 = 56. 442 ⎡ ⎡ ⎤ ⎢⎣ 37.416 ⎢⎣ 0.66291 ⎥⎦ −24.053 −0.42615 −34.744 −0.61557  ⎡ ⎢⎣−0.801 77  ⎤ ⎥⎦ =  0.267 26 0.534 52  0.267 26 0.534 52  ⎤ ⎥⎦  ⎤ ⎥⎦  56. 442  37.417  =  1  z =  v = zz =  ⎤ ⎥⎦  Second Iteration:  z = Sv =   341  9.3 Power and Inverse Power Methods  Third Iteration:  z = Sv =  ⎤ ⎥⎦  ⎡ ⎢⎣−36.460  20.362 40.721  ⎤ ⎥⎦  ⎤ ⎥⎦ =  ⎡ ⎢⎣ 0.66291 −0.42615 −0.61557  ⎡ ⎢⎣−30 10 20 40 −50 10 20 −50 −10  cid:25  36.4602 + 20.3622 + 40.7212 = 58.328 ⎡ ⎤ ⎡ ⎢⎣−0.62509 ⎥⎦ ⎢⎣−36.460  ⎤ ⎥⎦  =  1  20.362 40.721  58.328  0.34909 0.69814  v = zz =  z =  At this point the approximation of the eigenvalue we seek is λ = −58.328 MPa  the negative sign is determined by the sign reversal of z between iterations . This is actu- ally close to the second largest eigenvalue λ2 = −58.39 MPa! By continuing the itera- tive process we would eventually end up with the largest eigenvalue λ3 = 70.94 MPa. But because λ2 and λ3 are rather close, the convergence is too slow from this point on for manual labor. Here is a program that does the calculations for us:  ! usr bin python   example9_4  import numpy as np  import math  s = np.array [[-30.0,  10.0,  20.0], \  [ 10.0,  40.0, -50.0], \  [ 20.0, -50.0, -10.0]]   v = np.array [1.0, 0.0, 0.0]   for i in range 100 :  vOld = v.copy    z = np.dot s,v   zMag = math.sqrt np.dot z,z    v = z zMag  if np.dot vOld,v  < 0.0:  sign = -1.0  v = -v  else: sign = 1.0  lam = sign*zMag  print "Number of iterations =",i   print "Eigenvalue =",lam   input "Press return to exit"   The output from the program is  Number of iterations = 92  Eigenvalue = 70.94348330679053  if math.sqrt np.dot vOld - v,vOld - v   < 1.0e-6: break   342  Symmetric Matrix Eigenvalue Problems  EXAMPLE 9.5 Determine the smallest eigenvalue λ1 and the corresponding eigenvector of  ⎡  ⎢⎢⎢⎢⎢⎣  A =  ⎤  ⎥⎥⎥⎥⎥⎦  3 11 2 2 9 3 3 3 15 1 5 4 2  1 5 4 4 12 3  4 2 3 4 4 17  Use the inverse power method with eigenvalue shifting knowing that λ1 ≈ 5. Solution  ! usr bin python   example9_5  import numpy as np  from inversePower import *  s = 5.0  a = np.array [[ 11.0, 2.0,  3.0,  1.0,  4.0],  [ 2.0, 9.0,  3.0,  5.0,  2.0],  [ 3.0, 3.0, 15.0,  4.0,  3.0],  [ 1.0, 5.0,  4.0, 12.0,  4.0],  [ 4.0, 2.0,  3.0,  4.0, 17.0]]   \  \  \  \  lam,x = inversePower a,s   print "Eigenvalue =",lam   print "\nEigenvector:\n",x   input "\nPrint press return to exit"   Here is the output:  Eigenvalue = 4.873946378649195  Eigenvector:  [ 0.26726604 -0.74142853 -0.05017272  0.59491453 -0.14970634]  Convergence was achieved with four iterations. Without the eigenvalue shift 26  iterations would be required.  EXAMPLE 9.6  -1  0  1  2  n - 1 n  P  n + 1  n + 2  x  L  The propped cantilever beam carries a compressive axial load P. The lateral displace- ment u x  of the beam can be shown to satisfy the differential equation  u 4  + P E I   cid:3  cid:3  = 0 u   a    343  9.3 Power and Inverse Power Methods  where E I is the bending rigidity. The boundary conditions are  L  = 0  u 0  = u  cid:3  cid:3   u L  = u  cid:3    0  = 0   b    1  Show that buckling analysis of the beam results in a matrix eigenvalue problem if the derivatives are approximated by ﬁnite differences.  2  Write a program that com- putes the smallest buckling load of the beam, making full use of banded matrices. Run the program with 100 interior nodes  n = 100 . Solution of Part  1 . We divide the beam into n + 1 segments of length L  n + 1  each as shown. Replacing the derivatives of u in Eq.  a  by central ﬁnite differences of O h2  at the interior nodes  nodes 1 to n , we obtain  ui−2 − 4ui−1 + 6ui − 4ui+1 + ui+2  h4  −ui−1 + 2ui − ui−1  h2  = P E I  , i = 1, 2, . . . , n  After multiplication by h4, the equations become  u−1 − 4u0 + 6u1 − 4u2 + u3 = λ −u0 + 2u1 − u2  u0 − 4u1 + 6u2 − 4u3 + u4 = λ −u1 + 2u2 − u3   ...   c   un−3 − 4un−2 + 6un−1 − 4un + un+1 = λ −un−2 + 2un−1 − un  un−2 − 4un−1 + 6un − 4un+1 + un+2 = λ −un−1 + 2un − un+1   where  λ = Ph2 E I  =  P L2   n + 1 2 E I  The displacements u−1, u0, un+1 and un+2 can be eliminated by using the prescribed boundary conditions. Referring to Table 8.1, the ﬁnite difference approximations to the boundary conditions are  Substitution into Eqs.  c  yields the matrix eigenvalue problem Ax = λBx, where  un+2 = un ⎤  u0 = 0  u−1 = −u1  un+1 = 0  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  A =  5 −4 −4 1 −4 ... ... 0 ··· 0 ··· 0 ···  0 ··· 0 1 0 6 −4 0 ··· 0 1 6 −4 1 ··· 0 ... ... ... ... ... 6 −4 1 −4 1 6 −4 1 −4 0 1 −4 7 0 0  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦   344  Symmetric Matrix Eigenvalue Problems  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  B =  2 −1 −1 0 −1 ... ... ... ··· 0 ··· 0 ··· 0  0 ··· 0 0 0 2 −1 0 ··· 0 0 2 −1 0 ··· 0 ... ... ... ... ... 2 −1 0 −1 0 0 −1 2 −1 0 0 −1 2 0 0  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  Solution of Part  2 . The function inversePower5 listed next returns the smallest eigenvalue and the corresponding eigenvector of Ax = λBx, where A is a pentadiag- onal matrix and B is a sparse matrix  in this problem it is tridiagonal . The matrix A is input by its diagonals d, e, and f as was done in Section 2.4 in conjunction with the LU decomposition. The algorithm does not use B directly, but calls the function Bv v  that supplies the product Bv. Eigenvalue shifting is not used.   module inversePower5  ’’’ lam,x = inversePower5 Bv,d,e,f,tol=1.0e-6 .  Inverse power method for solving the eigenvalue problem  [A]{x} = lam[B]{x}, where [A] is pentadiagonal and [B]  is sparse. User must supply the function Bv v  that  returns the vector [B]{v}.  ’’’  import numpy as np  from LUdecomp5 import *  import math  from numpy.random import rand  def inversePower5 Bv,d,e,f,tol=1.0e-6 :  n = len d   d,e,f = LUdecomp5 d,e,f   x = rand n    Seed x with random numbers  xMag = math.sqrt np.dot x,x     Normalize {x}  x = x xMag  for i in range 30 :  xOld = x.copy    x = Bv xOld    Begin iterations   Save current {x}   Compute [B]{x}  x = LUsolve5 d,e,f,x    Solve [A]{z} = [B]{x}  xMag = math.sqrt np.dot x,x     Normalize {z}  if np.dot xOld,x  < 0.0:   Detect change in sign of {x}  x = x xMag  sign = -1.0  x = -x  else: sign = 1.0   345  9.3 Power and Inverse Power Methods  if math.sqrt np.dot xOld - x,xOld - x   < tol:  return sign xMag,x  print ’Inverse power method did not converge’   The program that utilizes inversePower5 is  n = 100   Number of interior nodes  d = np.ones n *6.0   Specify diagonals of [A] = [f\e\d\e\f]  ! usr bin python   example9_6  import numpy as np  from inversePower5 import *  def Bv v :   Compute {z} = [B]{v}  n = len v   z = np.zeros n   z[0] = 2.0*v[0] - v[1]  for i in range 1,n-1 :  z[i] = -v[i-1] + 2.0*v[i] - v[i+1]  z[n-1] = -v[n-2] + 2.0*v[n-1]  return z  d[0] = 5.0  d[n-1] = 7.0  e = np.ones n-1 * -4.0   f = np.ones n-2 *1.0  lam,x = inversePower5 Bv,d,e,f   print "PLˆ2 EI =",lam* n+1 **2   input "\nPress return to exit"   The output, which agrees with the analytical value, is  PLˆ2 EI = 20.1867306935764  PROBLEM SET 9.1  1. Given  ⎤ ⎥⎦  ⎡ ⎢⎣ 7 3 1  3 9 6 1 6 8  A =  ⎤ ⎥⎦  ⎡ ⎢⎣ 4 0 0  0 9 0 0 0 4  B =  convert the eigenvalue problem Ax = λBx to the standard form Hz = λz. What is the relationship between x and z?   346  Symmetric Matrix Eigenvalue Problems 2. Convert the eigenvalue problem Ax = λBx, where ⎡ ⎢⎣ 2 −1 −1 0 −1  ⎡ ⎢⎣ 4 −1 −1 0 −1  0 4 −1 4  ⎤ ⎥⎦  B =  A =  0 2 −1 1  ⎤ ⎥⎦  to the standard form.  3. An eigenvalue of the problem in Prob. 2 is roughly 2.5. Use the inverse power method with eigenvalue shifting to compute this eigenvalue to four decimal places. Start with x =  . Hint: Two iterations should be sufﬁcient.  1 0 0   cid:14   4. The stress matrix at a point is   cid:15 T ⎡ ⎢⎣ 150 −60 −60 0  120  0 0 0 80  S =  ⎤ ⎥⎦ MPa  Compute the principal stresses  eigenvalues of S .  5.  6.  The two pendulums are connected by a spring that is undeformed when the pen- dulums are vertical. The equations of motion of the system can be shown to be  where θ 1 and θ 2 are the angular displacements and k is the spring stiffness. Determine the circular frequencies of vibration and the relative amplitudes of the angular displacements. Use m = 0.25 kg, k = 20 N m. L = 0.75 m, and g = 9.806 65 m s2.  L  θ1  m  θ 2  L  k  m 2  k L θ 2 − θ 1  − mgθ 1 = mL ¨θ 1 −k L θ 2 − θ 1  − 2mgθ 2 = 2mL ¨θ 2  L  i1  C  i 2  C  i1  i 3  C  L  L  i 2  i 3   347  9.3 Power and Inverse Power Methods  Kirchoff’s laws for the electric circuit are  3i1 − i2 − i3 = −LC −i1 + i2 = −LC −i1 + i3 = −LC  d 2i1 dt 2 d 2i2 dt 2 d 2i3 dt 2  ⎡ ⎢⎢⎢⎣  A =  ⎤ ⎥⎥⎥⎦  4 −1 −1 0 −2 0 1  0 1 6 −2 0 3 2 2 4  ⎡ ⎢⎣ 4 −1 2 −1 −2  3 3 3 1  ⎤ ⎥⎦  A =  Compute the circular frequencies of the circuit and the relative amplitudes of the loop currents.  ∗ 7. Compute the matrix A  that results from annihilation A 14 and A 41 in the matrix  by a Jacobi rotation.  8.  cid:2  Use the Jacobi method to determine the eigenvalues and eigenvectors of  9.  cid:2  Find the eigenvalues and eigenvectors of 4 −2 −2 1 −2 −1  ⎡ ⎢⎢⎢⎣  A =  4 −2 1 −2  1 −1 1 4 −2 4  ⎤ ⎥⎥⎥⎦  with the Jacobi method.  10.  cid:2  Use the power method to compute the largest eigenvalue and the correspond-  ing eigenvector of the matrix A given in Prob. 9.  11.  cid:2  Find the smallest eigenvalue and the corresponding eigenvector of the matrix  A in Prob. 9. Use the inverse power method.  12.  cid:2  Let  ⎡ ⎢⎣ 1.4 0.8 0.4  0.8 6.6 0.8 0.4 0.8 5.0  ⎤ ⎥⎦  A =  ⎡ ⎢⎣ 0.4 −0.1 −0.1 0.0 −0.1  0.0 0.4 −0.1 0.4  ⎤ ⎥⎦  B =  Find the eigenvalues and eigenvectors of Ax = λBx by the Jacobi method.  13.  cid:2  Use the inverse power method to compute the smallest eigenvalue in Prob. 12.   348  Symmetric Matrix Eigenvalue Problems  14.  cid:2  Use the Jacobi method to compute the eigenvalues and eigenvectors of the  following matrix:  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎣  A =  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎦  3 11 2 2 9 3 3 3 15 1 5 4 2 2 1  1 5 4 4 12 3 2  4 2 2 1 3 2 4 3 4 17 5 5 8 3  ⎡ ⎢⎢⎢⎣  15.  cid:2  Find the eigenvalues of Ax = λBx by the Jacobi method, where  A =  6 −2 3 −2 Warning: B is not positive deﬁnite. Hint: Solve Bx =  1 λ Ax.  1 0 6 −4 1 6 −4 1 −4 7  B =  1 −2 −2 3 −2 −1  6 −4 −4 1 −4 0  3 −1 3 6 −2 9  ⎡ ⎢⎢⎢⎣  ⎤ ⎥⎥⎥⎦  16.  cid:2   ⎤ ⎥⎥⎥⎦  L  1  2  x  n  The ﬁgure shows a cantilever beam with a superimposed ﬁnite difference mesh. If u x, t  is the lateral displacement of the beam, the differential equation of mo- tion governing bending vibrations is  u 4  = − γ E I  ¨u  length and E I  0, t  = u  cid:3  cid:3   is the bending rigidity.  L, t  = u  L, t  = 0. With  cid:3  cid:3  cid:3   where γ is the mass per unit The boundary conditions are u 0, t  = u  cid:3  u x, t  = y x  sin ωt the problem becomes y 0  = y  cid:3   y  y  4  = ω2γ E I   0  = y   cid:3  cid:3   The corresponding ﬁnite difference equations are  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  A =  7 −4 −4 1 −4 ... ... 0 ··· 0 ··· 0 ···  0 ··· 0 1 0 6 −4 0 ··· 0 1 6 −4 1 ··· 0 ... ... ... ... ... 6 −4 1 −4 1 5 −2 1 −4 0 1 −2 1 0 0 &4 %  λ = ω2γ E I  L n  where   L  = y ⎡ ⎤   cid:3  cid:3  cid:3    L  = 0 ⎡ ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  y1 y2 y3 ... yn−2 yn−1 yn  = λ  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  y1 y2 y3 ... yn−2 yn−1 yn 2   349  9.3 Power and Inverse Power Methods   a  Write down the matrix H of the standard form Hz = λz and the transforma- tion matrix P as in y = Pz.  b  Write a program that computes the lowest two circular frequencies of the beam and the corresponding mode shapes  eigenvec- tors  using the Jacobi method. Run the program with n = 10. Note: The analytical  solution for the lowest circular frequency is ω1 = cid:30   3.515 L2   cid:31 √  E I  γ .  17.  cid:2   P  L 4 EI 0  L 2 EI2 0  a   L 4  L 4  L 4 EI0  P  0  1 2  3 4 5 6 7 8 9 10   b   The simply supported column in Figure  a  consists of three segments with the bending rigidities shown. If only the ﬁrst buckling mode is of interest, it is sufﬁ- cient to model half of the beam as shown in Figure  b . The differential equation for the lateral displacement u x  is   cid:3  cid:3  = − P u E I  0  = 0. The corresponding ﬁnite differ-  u  with the boundary conditions u 0  = u  cid:3  ence equations are  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  2 −1 0 ··· 0 0 0 0 0 −1 2 −1 0 ··· 0 0 0 0 0 −1 2 −1 0 ··· 0 0 0 2 −1 0 −1 0 ··· 0 0 0 0 −1 2 −1 0 ··· 0 0 0 0 −1 2 −1 ··· 0 0 0 0 ... ... ... ... ... ... ... ... ... 2 −1 0 −1 0 ··· 0 0 0 0 −1 0 ··· 1 0 0 0 0 &2 %  λ = P E I0  L 20  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  u1 u2 u3 u4 u5 u6 ... u9 u10  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  u1 u2 u3 u4  u5 1.5 u6 2 ... u9 2 u10 4  = λ  Write a program that computes the lowest buckling load P of the column with the inverse power method. Use the banded forms of the matrices.  where  18.  cid:2   L  θ1  k  L θ2  L θ3  k  P  k   350  Symmetric Matrix Eigenvalue Problems  The springs supporting the three-bar linkage are undeformed when the linkage is horizontal. The equilibrium equations of the linkage in the presence of the horizontal force P can be shown to be  ⎤ ⎥⎦  ⎡ ⎢⎣ 6 5 3  3 3 2 1 1 1  ⎡ ⎢⎣ θ 1  θ 2 θ 3  ⎤ ⎥⎦ = P  k L  ⎤ ⎥⎦  ⎡ ⎢⎣ 1 1 1  0 1 1 0 0 1  ⎡ ⎢⎣ θ 1  θ 2 θ 3  ⎤ ⎥⎦  where k is the spring stiffness. Determine the smallest buckling load P and the corresponding mode shape. Hint: The equations can be easily rewritten in the standard form Aθ = λθ, where A is symmetric.  19.  cid:2   20.  cid:2   The differential equations of motion for the mass-spring system are  k  m  u1  k  3m  u2  k  2m  u3  k  k  −2u1 + u2  = m¨u1 k u1 − 2u2 + u3  = 3m¨u2 k u2 − 2u3  = 2m¨u3  where ui t  is the displacement of mass i from its equilibrium position and k is the spring stiffness. Determine the circular frequencies of vibration and the cor- responding mode shapes.  Kirchoff’s equations for the circuit are  L  L  L  i1 C  i2 C 2  i1  i3 C 3  i2  i3  L  i4 C 4  C 5  i4  L  L  d 2i2 dt 2 d 2i3 dt 2  L  d2i1 dt 2 + 2 C + 3 C d2i4 dt 2  + 1 C  i1 + 2 C  i2 − i1  + 3 C  i3 − i2  + 4 C + 4 C  L   i1 − i2  = 0  i2 − i3  = 0  i3 − i4  = 0 i4 = 0   i4 − i3  + 5 C  Find the circular frequencies of the current.   351  9.4 Householder Reduction to Tridiagonal Form  21.  cid:2   C  C 2  C 3  C 4  i1L  i2 L  i1  i3 L  i2  i4 L  i3  i4  L  Determine the circular frequencies of oscillation for the circuit shown, given these Kirchoff equations:  %  &  %  L  %  d 2i2 dt 2 d2i3 dt 2  L  L  d2i1 dt 2 − d2i1 & dt 2 − d 2i2 % dt 2  L  d2i4 dt 2  + L &  %  + L %  d 2i1 dt 2 d2i2 dt 2 d 2i3 dt 2 − d2i3 dt 2  − d 2i2 dt 2 − d2i3 & dt 2 − d 2i4 dt 2 d 2i4 dt 2  + L  &  + L  i1 = 0  + 1 & C + 2 C i3 = 0  = 0  + 3 C + 4 C  i4 = 0  22.  cid:2  Several iterative methods exist for ﬁnding the eigenvalues of a matrix A. One of these is the LR method, which requires the matrix to be symmetric and positive deﬁnite. Its algorithm is very simple:  Do the following until the change in A is insigniﬁcant:  Use Choleski’s decomposition A = LLT to compute L. Compute A = LT L.  It can be shown that the diagonal elements of A converge to the eigenvalues of A. Write a program that implements the LR method and test it with  ⎤ ⎥⎦  ⎡ ⎢⎣ 4 3 1  3 4 2 1 2 3  A =  9.4 Householder Reduction to Tridiagonal Form  It was mentioned easier that similarity transformations can be used to transform an eigenvalue problem to a form that is easier to solve. The most desirable of the “easy” forms is, of course, the diagonal form that results from the Jacobi method. However, the Jacobi method requires about 10n3 to 20n3 multiplications, so that the amount of computation increases very rapidly with n. We are generally better off by reducing the matrix to the tridiagonal form, which can be done in precisely n − 2 transformations by the Householder method. Once the tridiagonal form is achieved, we still have to   352  Symmetric Matrix Eigenvalue Problems  extract the eigenvalues and the eigenvectors, but there are effective means of dealing with that, as we seen the next section.  Householder Matrix  Each Householder transformation uses the Householder matrix  where u is a vector and  Q = I − uuT H  H = 1 2  uTu = 1 2  u2   cid:30    cid:31   Note that uuT in Eq.  9.36  is the outer product; that is, a matrix with the elements uuT  = uiuj . Because Q is obviously symmetric  QT= Q , we can write  cid:31    cid:30   ij  &%  %  &  QT Q = QQ =  I − uuT H  = I − 2  uuT H  + u  uTu H2  uT  I − uuT H = I  = I − 2  uuT H  + u  2H  uT  H2  which shows that Q is also orthogonal.  Now let x be an arbitrary vector and consider the transformation Qx. Choosing   9.36    9.37    9.38   where  we get  But  so that  u = x + ke1   cid:14   e1 = &  1 0 0 ··· *   cid:30    cid:15 T +  0   cid:31 T  k = ±x  %  Qx =  I − uuT x =  cid:30  H xT x+keT 1 x  I − u  cid:31   x + ke1  cid:30  x k 2 + kx1   cid:31   H  H = x − u  cid:30    cid:31  + k2eT  1 e1  xT e1+eT 1 x  H  = x − u  cid:31 T cid:30   2H =  cid:30  x + ke1 = k2 + 2kx1 + k2 = 2  x + ke1   cid:31  = x2 + k  cid:30   cid:31  k2 + kx1  cid:14    cid:15 T  Qx = x − u = −ke1 =  −k 0 0 ···  0   9.39   Hence the transformation eliminates all elements of x except the ﬁrst one.   353  9.4 Householder Reduction to Tridiagonal Form  Householder Reduction of a Symmetric Matrix Let us now apply the following transformation to a symmetric n × n matrix A:  *  +*  +  *  +  P1A =  1 0T 0 Q  A 11 x  xT  cid:3  A  =  xT A 11 Qx QA   cid:3   Here x represents the ﬁrst column of A with the ﬁrst element omitted, and A is sim- ply A with its ﬁrst row and column removed. The matrix Q of dimensions  n − 1  ×  n − 1  is constructed using Eqs.  9.36 – 9.38 . Referring to Eq.  9.39 , we see that the transformation reduces the ﬁrst column of A to   9.40    cid:3   *  +  =  A 11 Qx  ⎡  ⎢⎢⎢⎢⎢⎢⎣  A 11−k  0 ... 0  ⎤  ⎥⎥⎥⎥⎥⎥⎦   cid:30   *  A ← P1AP1 =  Qx A 11  cid:3  Qx QA  +   cid:31 T  Q  The transformation   9.41   thus tridiagonalizes the ﬁrst row as well as the ﬁrst column of A. Here is a diagram of the transformation for a 4 × 4 matrix: A 11 A 21 A 31 A 41  1 0 0 0  A 14  A 13  A 12  Q  Q  A  0  0  0  0  0  ·  ·   cid:3   0  1 0 0 0 A 11 −k −k 0 0  =  0  0   cid:3   QA  Q  *  +  P2 =  I2 0T 0 Q  The second row and column of A are reduced next by applying the transformation to the 3 × 3 lower right portion of the matrix. This transformation can be expressed as A ← P2AP2, where now   9.42  In Eq.  9.42  I2 is a 2 × 2 identity matrix and Q is a  n − 2  ×  n − 2  matrix constructed by choosing for x the bottom n − 2 elements of the second column of A. It takes a total * of n − 2 transformations with Pi =  i = 1, 2, . . . , n − 2  +  ,  Ii 0T 0 Q  to attain the tridiagonal form.   354  Symmetric Matrix Eigenvalue Problems  It is wasteful to form Pi and then carry out the matrix multiplication PiAPi. We  %  &   cid:3  A  Q = A   cid:3   I − uuT H   cid:3   = A   cid:3  − A u H  uT = A   cid:3  − vuT  note that  where  Therefore,  where  Letting  %   cid:3  QA  Q =  I − uuT H   cid:3   v = A u H  & cid:30    cid:31  = A  cid:30   A   cid:3  − vuT  cid:30   cid:3  cid:31   = A = A  + u  cid:3  − vuT − u  cid:3  − vuT − uvT + 2guuT  uTA H  uT  uT v H   cid:30    cid:3  − vuT  A   cid:31    cid:3  − vuT − uuT  cid:31  H  g = uT v 2H  w = v − gu   9.43    9.44    9.45    9.46   it can be easily veriﬁed that the transformation can be written as   cid:3  QA  Q = A   cid:3  − wuT − uwT  which gives us the following computational procedure that is to be carried out with i = 1, 2, . . . , n − 2:  Do with i = 1, 2, . . . , n − 2:  be the  n − i  × cid:30   cid:14    cid:3  Let A Let x =  Ai+1,i Ai+2,i   cid:31    cid:15 T  n − i ··· A n,i  lower right-hand portion of A.   the column of length n − i just left of A  Compute x. Let k = x if x1 > 0 and k = −x if x1 < 0  this choice of sign minimizes the roundoff error .   .   cid:3    cid:14    cid:15   xn−i  T  .  x3  ···  Let u = k+x1 x2 Compute H = u  2. Compute v = A  cid:3  u H. Compute g = uT v  2H . Compute w = v − gu. Compute the transformation A Set Ai,i+1 = Ai+1,i = −k.   cid:3 ← A   cid:3  − wT u − uT w.   355  9.4 Householder Reduction to Tridiagonal Form  Accumulated Transformation Matrix  Because we used similarity transformations, the eigenvalues of the tridiagonal matrix are the same as those of the original matrix. However, to determine the eigenvectors X of original A we must use the transformation X = PXtridiag  where P is the accumulation of the individual transformations:  P = P1P2··· Pn−2  We build up the accumulated transformation matrix by initializing P to a n × n iden- tity matrix and then applying the transformation  *  +*  +  *  =  +  P ← PPi =  P11 P12 P21 P22  Ii 0T 0 Q  P11 P21Q P12 P22Q   b   with i = 1, 2, . . . , n − 2. It can be seen that each multiplication affects only the right- most n − i columns of P  since the ﬁrst row of P12 contains only zeroes, it can also be omitted in the multiplication . Using the notation  *  +  P12 P22  &   cid:3 = P  %   cid:3  y = P u H  we have  *  +  where  P12Q P22Q  = P   cid:3   Q = P  cid:3   I − uuT H  = P   cid:3   cid:3  − P u H  uT = P   cid:3  − yuT   9.47    9.48   The procedure for carrying out the matrix multiplication in Eq.  b  is as follows:  Retrieve u  u’s are stored by columns below the principal diagonal of A . Compute H = u  2. Compute y = P u H. Compute the transformation P   cid:3  − yuT .   cid:3 ← P   cid:3    cid:2  householder  The function householder in this module does reduction to tridiagonal form. It returns  d, c , where d and c are vectors that contain the elements of the princi- pal diagonal and the subdiagonal, respectively. Only the upper triangular portion is   356  Symmetric Matrix Eigenvalue Problems  reduced to the triangular form. The part below the principal diagonal is used to store the vectors u. This is done automatically by the statement u = a[k+1:n,k], which does not create a new object u, but simply sets up a reference to a[k+1:n,k]  makes a deep copy . Thus any changes made to u are reﬂected in a[k+1:n,k].  The function computeP returns the accumulated transformation matrix P.  There is no need to call it if only the eigenvalues are to be computed.  Householder similarity transformation of matrix [a] to  Computes the acccumulated transformation matrix [p]  after calling householder a .   module householder  ’’’ d,c = householder a .  tridiagonal form.  p = computeP a .  ’’’  import numpy as np  import math  def householder a :  n = len a   for k in range n-2 :  u = a[k+1:n,k]  uMag = math.sqrt np.dot u,u    if u[0] < 0.0: uMag = -uMag  u[0] = u[0] + uMag  h = np.dot u,u  2.0  v = np.dot a[k+1:n,k+1:n],u  h  g = np.dot u,v   2.0*h   v = v - g*u  a[k+1:n,k+1:n] = a[k+1:n,k+1:n] - np.outer v,u   \  -np.outer u,v   a[k,k+1] = -uMag  return np.diagonal a ,np.diagonal a,1   def computeP a :  n = len a   p = np.identity n *1.0  for k in range n-2 :  u = a[k+1:n,k]  h = np.dot u,u  2.0  v = np.dot p[1:n,k+1:n],u  h  p[1:n,k+1:n] = p[1:n,k+1:n] - np.outer v,u   return p   357  9.4 Householder Reduction to Tridiagonal Form  EXAMPLE 9.7 Transform the matrix  ⎡ ⎢⎢⎢⎣  A =  ⎤ ⎥⎥⎥⎦  3 −1 7 2 2 8 1 5 9 3 5 12 −1 1 9 7  into tridiagonal form.  Solution. Reduce ﬁrst row and column:  5 1 5 12 9 1 9 7  ⎡ ⎢⎣ 8 ⎡ ⎢⎣ k + x1  x2 x3   cid:3  = A  u =  uuT =  Q = I− uuT H   cid:3   Q =  QA   cid:30   +   cid:31 T  *  k = x = 3. 7417  u2 = 21. 484  9 −3  ⎤ ⎥⎦  H = 1 2  ⎡ ⎢⎣ 2 3 −1 ⎤ ⎥⎦  ⎤ ⎥⎦ x = ⎤ ⎡ ⎥⎦ = ⎢⎣ 5.7417 3−1 ⎤ ⎡ ⎥⎥⎦ ⎢⎢⎣ 32.967 17 225 −5.7417 17.225 −5.7417 ⎡ ⎢⎢⎣−0.53450 −0.80176 0.26725 = ⎤ ⎡ ⎥⎥⎦ ⎢⎢⎣ 10.642 −0.1388 −9.1294 −0.1388 −9.1294 ⎡ ⎢⎢⎢⎢⎢⎣  4.8429 7 −3.7417  −0.80176 0.26725  0.58108 0.13964  0.13964 0.95345  −3.7417  −3 1  10.4480  4.8429  5.9087  =  ⎤ ⎥⎥⎦  0  0 10.642 −0. 1388 −9.1294 4.8429  5.9087  4.8429  10.4480   cid:15 T  0  .  ⎤  ⎥⎥⎥⎥⎥⎦  A ←  Qx A 11  cid:3  Qx QA  Q  0 −0.1388 0 −9.1294  cid:14  In the last step we used the formula Qx = * Reduce the second row and column: x =  −k 0 ··· +   cid:3  =  +  *  A  5.9087 4.8429 4.8429 10.4480  −0.1388 −9.1294 +  where the negative sign on k was determined by the sign of x1.  *  +  *  =  u =  k + x1 −9.1294  −9. 2693 −9.1294  H = 1 2  u2 = 84.633  k = −x = −9.1305   358  Symmetric Matrix Eigenvalue Problems  uuT =  85.920 84.623  84.623 83.346  Q = I− uuT H  =  0.01521 −0.99988 −0.99988 0.01521   cid:3  QA   cid:30   0T  Q = ⎤ ⎥⎥⎦   cid:31 T  10.594 4.772  4.772 5.762 7 −3.742 10.642  −3.742 0  ⎡ ⎢⎢⎣ A 11 A 12  A ←  A 21 A 22 0  Qx  cid:3  Qx QA  Q  9.131 10.594 4.772  0  0  4.772 5.762  ⎤  ⎥⎥⎥⎥⎥⎦  0  0  +  +  +  0  9.131  * * * ⎡  ⎢⎢⎢⎢⎢⎣  EXAMPLE 9.8 Use the function householder to tridiagonalize the matrix in Example 9.7; also de- termine the transformation matrix P.  Solution  ! usr bin python   example9_8  import numpy as np  from householder import *  a = np.array [[ 7.0, 2.0,  3.0, -1.0],  [ 2.0, 8.0,  5.0,  1.0],  [ 3.0, 5.0, 12.0,  9.0],  [-1.0, 1.0,  9.0,  7.0]]   \  \  \  d,c = householder a   print "Principal diagonal {d}:\n", d   print "\nSubdiagonal {c}:\n",c   print "\nTransformation matrix [P]:"   print computeP a    input "\nPress return to exit"   The results of running the above program are as follows:  Principal diagonal {d}:  [  7.  10.64285714  10.59421525  5.76292761]  Subdiagonal {c}:  [-3.74165739  9.13085149  4.77158058]   359  9.5 Eigenvalues of Symmetric Tridiagonal Matrices  Transformation matrix [P]:  [[ 1.  [ 0.  [ 0.  [ 0.  0.  0.  0.  ]  -0.53452248 -0.25506831  0.80574554]  -0.80178373 -0.14844139 -0.57888514]  0.26726124 -0.95546079 -0.12516436]]  9.5  Eigenvalues of Symmetric Tridiagonal Matrices  Sturm Sequence  In principle, the eigenvalues of a matrix A can be determined by ﬁnding the roots of the characteristic equation A − λI = 0. This method is impractical for large matri- ces, because the evaluation of the determinant involves n3 3 multiplications. How- ever, if the matrix is tridiagonal  we also assume it to be symmetric , its characteristic polynomial  Pn λ  = A−λI =   cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  cid:19   d1 − λ c1 0 0 ... 0  c1 d2 − λ c2 0 ... 0  0 c2 d3 − λ c3 ...  . . .  0 0 c3 d4 − λ  ... 0   cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  cid:19   ··· ··· ··· ··· ... cn−1 dn − λ  0 0 0 0 ...  can be computed with only 3 n − 1  multiplications using the following sequence of operations:  P0 λ  = 1 P1 λ  = d1 − λ Pi λ  =  di − λ Pi−1 λ  − c2  i−1 Pi−2 λ ,  i = 2, 3, . . . , n   9.49   The polynomials P0 λ , P1 λ , . . . , Pn λ  form a Sturm sequence that has the fol- lowing property: The number of sign changes in the sequence P0 a , P1 a , . . . , Pn a  is equal to the number of roots of Pn λ  that are smaller than a. If a member Pi a  of the sequence is zero, its sign is to be taken opposite to that of Pi−1 a .  As we see later, Sturm sequence property makes it possible to bracket the eigen-  values of a tridiagonal matrix.   cid:2  sturmSeq  Given d, c, and λ, the function sturmSeq returns the Sturm sequence  P0 λ , P1 λ , . . . Pn λ   The function numLambdas returns the number of sign changes in the sequence  as noted earlier, this equals the number of eigenvalues that are smaller than λ .   360  Symmetric Matrix Eigenvalue Problems   module sturmSeq  ’’’ p = sturmSeq c,d,lam .  Returns the Sturm sequence {p[0],p[1],...,p[n]}  associated with the characteristic polynomial  [A] - lam[I] = 0, where [A] is a n x n  tridiagonal matrix.  numLam = numLambdas p .  Returns the number of eigenvalues of a tridiagonal  matrix that are smaller than ’lam’.  Uses the Sturm sequence {p} obtained from ’sturmSeq’.  ’’’  import numpy as np  def sturmSeq d,c,lam :  n = len d  + 1  p = np.ones n   p[1] = d[0] - lam  for i in range 2,n :  return p  def numLambdas p :  n = len p   signOld = 1  numLam = 0  p[i] =  d[i-1] - lam *p[i-1] -  c[i-2]**2 *p[i-2]  for i in range 1,n :  if p[i] > 0.0: sign = 1  elif p[i] < 0.0: sign = -1  else: sign = -signOld  signOld = sign  return numLam  if sign*signOld < 0: numLam = numLam + 1  EXAMPLE 9.9 Use the Sturm sequence property to show that the smallest eigenvalue of A is in the interval  0.25, 0.5 , where  ⎡ ⎢⎢⎢⎣  A =  ⎤ ⎥⎥⎥⎦  2 −1 −1 0 −1 0  0 0 2 −1 0 2 −1 0 −1 2   361  9.5 Eigenvalues of Symmetric Tridiagonal Matrices Solution. Taking λ = 0.5, we have di − λ = 1.5 and c2 i−1 in Eqs.  9.49  becomes  = 1, and the Sturm sequence  Since the sequence contains one sign change, there exists one eigenvalue smaller than 0.5. = 1, which re-  Repeating the process with λ = 0.25, we get di − λ = 1.75 and c2  i  sults in the Sturm sequence:  P0 0.5  = 1 P1 0.5  = 1.5 P2 0.5  = 1.5 1.5  − 1 = 1.25 P3 0.5  = 1.5 1.25  − 1.5 = 0.375 P4 0.5  = 1.5 0.375  − 1.25 = −0.6875  P0 0.25  = 1 P1 0.25  = 1.75 P2 0.25  = 1.75 1.75  − 1 = 2.0625 P3 0.25  = 1.75 2.0625  − 1.75 = 1.8594 P4 0.25  = 1.75 1.8594  − 2.0625 = 1.1915  There are no sign changes in the sequence, so that all the eigenvalues are greater than 0.25. We thus conclude that 0.25 < λ1 < 0.5.  Gerschgorin’s Theorem  Gerschgorin’s theorem is useful in determining the global bounds on the eigenvalues of a n × n matrix A. The term “global” means the bounds that enclose all the eigen- values. We give here a simpliﬁed version for a symmetric matrix.  If λ is an eigenvalue of A, then  where  ai − ri ≤ λ ≤ ai + ri,  i = 1, 2, . . . , n  ai = Aii  ri = n cid:6    cid:19  cid:19    cid:19  cid:19 Aij  j=1 j cid:7 =i  It follows that the limits on the smallest and the largest eigenvalues are given by  λmin ≥ min   ai − ri   λmax ≤ max   ai + ri   i  i   9.50    9.51    362  Symmetric Matrix Eigenvalue Problems   cid:2  gerschgorin  The function gerschgorin returns the lower and upper global bounds on the eigen- values of a symmetric tridiagonal matrix A = [c\d\c].   module gerschgorin  ’’’ lamMin,lamMax = gerschgorin d,c .  Applies Gerschgorin’s theorem to find the global bounds on  the eigenvalues of a symmetric tridiagonal matrix.  ’’’  def gerschgorin d,c :  n = len d   lamMin = d[0] - abs c[0]   lamMax = d[0] + abs c[0]   for i in range 1,n-1 :  lam = d[i] - abs c[i]  - abs c[i-1]   if lam < lamMin: lamMin = lam  lam = d[i] + abs c[i]  + abs c[i-1]   if lam > lamMax: lamMax = lam  lam = d[n-1] - abs c[n-2]   if lam < lamMin: lamMin = lam  lam = d[n-1] + abs c[n-2]   if lam > lamMax: lamMax = lam  return lamMin,lamMax  Solution. Referring to Eqs.  9.50 , we get  Hence  ⎡ ⎢⎣ 4 −2 −2 0 −2  0 4 −2 5  ⎤ ⎥⎦  A =  a1 = 4 r1 = 2  a2 = 4 r2 = 4  a3 = 5 r3 = 2  λmin ≥ min ai − ri  = 4 − 4 = 0 λmax ≤ max ai + ri  = 4 + 4 = 8  EXAMPLE 9.10 Use Gerschgorin’s theorem to determine the bounds on the eigenvalues of the matrix  Bracketing Eigenvalues  The Sturm sequence property used together with Gerschgorin’s theorem provides us a convenient tool for bracketing each eigenvalue of a symmetric tridiagonal matrix.   363  9.5 Eigenvalues of Symmetric Tridiagonal Matrices   cid:31    cid:2  lamRange  cid:30  The function lamRange brackets the N smallest eigenvalues of a symmetric tridi- agonal matrix A = [c\d\c]. It returns the sequence r0, r1, . . . , r N, where each interval ri−1, ri contains exactly one eigenvalue. The algorithm ﬁrst ﬁnds the bounds on all the eigenvalues by Gerschgorin’s theorem. Then the method of bisection in conjunc- tion with the Sturm sequence property is used to determine r N, r N−1, . . . , r0 in that order.   module lamRange  ’’’ r = lamRange d,c,N .  Returns the sequence {r[0],r[1],...,r[N]} that  separates the N lowest eigenvalues of the tridiagonal  matrix; that is, r[i] < lam[i] < r[i+1].  ’’’  import numpy as np  from sturmSeq import *  from gerschgorin import *  def lamRange d,c,N :  lamMin,lamMax = gerschgorin d,c   r = np.ones N+1   r[0] = lamMin   Search for eigenvalues in descending order  for k in range N,0,-1 :   First bisection of interval lamMin,lamMax   lam =  lamMax + lamMin  2.0  h =  lamMax - lamMin  2.0  for i in range 1000 :  p = sturmSeq d,c,lam   numLam = numLambdas p    Find number of eigenvalues less than lam   Bisect again & find the half containing lam  h = h 2.0  if numLam < k: lam = lam + h  elif numLam > k: lam = lam - h  else: break   If eigenvalue located, change the upper limit   of search and record it in [r]  lamMax = lam  r[k] = lam  return r  EXAMPLE 9.11 Bracket each eigenvalue of the matrix A in Example 9.10.   364  Symmetric Matrix Eigenvalue Problems  Solution. In Example 9.10 we found that all the eigenvalues lie in  0, 8 . We now bi- sect this interval and use the Sturm sequence to determine the number of eigenval- ues in  0, 4 . With λ = 4, the sequence is—see Eqs.  9.49 —  Because a zero value is assigned to the sign opposite to that of the preceding member, the signs in this sequence are  +, −, −, − . The one sign change shows the presence of one eigenvalue in  0, 4 .  Next we bisect the interval  4, 8  and compute the Sturm sequence with λ = 6:  P0 4  = 1 P1 4  = 4 − 4 = 0 P2 4  =  4 − 4  0  − 22 1  = −4 P3 4  =  5 − 4  −4  − 22 0  = −4  P0 6  = 1 P1 6  = 4 − 6 = −2 P2 6  =  4 − 6  −2  − 22 1  = 0 P3 6  =  5 − 6  0  − 22 −2  = 8  In this sequence the signs are  +, −, +, + , indicating two eigenvalues in  0, 6 .  Therefore  0 ≤ λ1 ≤ 4  4 ≤ λ2 ≤ 6  6 ≤ λ3 ≤ 8  Computation of Eigenvalues  Once the desired eigenvalues are bracketed, they can be found by determining the roots of Pn λ  = 0 with bisection or Ridder’s method.   cid:2  eigenvals3  The function eigenvals3 computes N smallest eigenvalues of a symmetric tridiag- onal matrix with the method of Ridder.  Returns the N smallest eigenvalues of a symmetric  tridiagonal matrix defined by its diagonals d and c.   module eigenvals3  ’’’ lam = eigenvals3 d,c,N .  ’’’  from lamRange import *  from ridder import *  from sturmSeq import sturmSeq  from numpy import zeros  def eigenvals3 d,c,N :   365  9.5 Eigenvalues of Symmetric Tridiagonal Matrices  def f x :   f x  = [A] - x[I]  p = sturmSeq d,c,x   return p[len p -1]  lam = zeros N   r = lamRange d,c,N    Bracket eigenvalues  for i in range N :   Solve by Ridder’s method  lam[i] = ridder f,r[i],r[i+1]   return lam  EXAMPLE 9.12 Use eigenvals3 to determine the three smallest eigenvalues of the 100 × 100 ma- trix:  ⎡  ⎢⎢⎢⎢⎢⎢⎣  A =  ⎤  ⎥⎥⎥⎥⎥⎥⎦  0 ··· 2 −1 ··· 2 ··· ... ...  2 −1 0 −1 0 0 −1 0 ... ... ... 0 ··· −1 2 0  Solution  ! usr bin python   example9_12  import numpy as np  from eigenvals3 import *  N = 3  n = 100  d = np.ones n *2.0  c = np.ones n-1 * -1.0   lambdas = eigenvals3 d,c,N   print lambdas  raw_input "\nPress return to exit"   Here are the eigenvalues:  [ 0.00096744  0.00386881  0.0087013 ]  Computation of Eigenvectors  If the eigenvalues are known  approximate values will be good enough , the best means of computing the corresponding eigenvectors is the inverse power method with eigenvalue shifting. This method was discussed earlier, but the algorithm listed did not take advantage of banding. Here we present a version of the method written for symmetric tridiagonal matrices.   366  Symmetric Matrix Eigenvalue Problems   cid:2  inversePower3  This function is very similar to inversePower listed in Section 9.3, but it executes much faster because it exploits the tridiagonal structure of the matrix.   module inversePower3  ’’’ lam,x = inversePower3 d,c,s,tol=1.0e-6 .  Inverse power method applied to a symmetric tridiagonal  matrix. Returns the eigenvalue closest to ’s’  and the corresponding eigenvector.  ’’’  from LUdecomp3 import *  import math  import numpy as np  from numpy.random import rand  def inversePower3 d,c,s,tol=1.0e-6 :  n = len d   e = c.copy    dStar = d - s  LUdecomp3 c,dStar,e    Decompose [A*]  x = rand n    Seed x with random numbers  xMag = math.sqrt np.dot x,x     Normalize [x]   Form [A*] = [A] - s[I]   Begin iterations   Save current [x]  LUsolve3 c,dStar,e,x    Solve [A*][x] = [xOld]  xMag = math.sqrt np.dot x,x    Normalize [x]  if np.dot xOld,x  < 0.0:  Detect change in sign of [x]  x =x xMag  flag = 0  for i in range 30 :  xOld = x.copy    x = x xMag  sign = -1.0  x = -x  else: sign = 1.0  if math.sqrt np.dot xOld - x,xOld - x   < tol:  return s + sign xMag,x  print ’Inverse power method did not converge’   EXAMPLE 9.13 Compute the 10th smallest eigenvalue of the matrix A given in Example 9.12.  Solution. The following program extracts the Nth eigenvalue of A by the inverse power method with eigenvalue shifting:  ! usr bin python   example9_13   367  9.5 Eigenvalues of Symmetric Tridiagonal Matrices  import numpy as np  from lamRange import *  from inversePower3 import *  N = 10  n = 100  d = np.ones n *2.0  c = np.ones n-1 * -1.0   r = lamRange d,c,N    Bracket N smallest eigenvalues  s =  r[N-1] + r[N]  2.0   Shift to midpoint of Nth bracket  lam,x = inversePower3 d,c,s    Inverse power method  print "Eigenvalue No.",N," =",lam   input "\nPress return to exit"   The result is  Eigenvalue No. 10  = 0.0959737849345  EXAMPLE 9.14 Compute the three smallest eigenvalues and the corresponding eigenvectors of the matrix A in Example 9.5.  Solution  ! usr bin python   example9_14  from householder import *  from eigenvals3 import *  from inversePower3 import *  import numpy as np  N = 3   Number of eigenvalues requested  a = np.array [[ 11.0, 2.0,  3.0,  1.0,  4.0],  [  [  [  [  2.0, 9.0,  3.0,  5.0,  2.0],  3.0, 3.0, 15.0,  4.0,  3.0],  1.0, 5.0,  4.0, 12.0,  4.0],  4.0, 2.0,  3.0,  4.0, 17.0]]   \  \  \  \  xx = np.zeros  len a ,N    d,c = householder a    Tridiagonalize [A]  p = computeP a    Compute transformation matrix  lambdas = eigenvals3 d,c,N    Compute eigenvalues  for i in range N :  s = lambdas[i]*1.0000001   Shift very close to eigenvalue  lam,x = inversePower3 d,c,s   Compute eigenvector [x]  xx[:,i] = x  xx = np.dot p,xx    Place [x] in array [xx]   Recover eigenvectors of [A]   368  Symmetric Matrix Eigenvalue Problems  print "Eigenvalues:\n",lambdas   print "\nEigenvectors:\n",xx   input "Press return to exit"   Eigenvalues:  [  4.87394638  8.66356791  10.93677451]  Eigenvectors:  [[ 0.26726603  0.72910002  0.50579164]  [-0.74142854  0.41391448 -0.31882387]  [-0.05017271 -0.4298639  0.52077788]  [ 0.59491453  0.06955611 -0.60290543]  [-0.14970633 -0.32782151 -0.08843985]]  PROBLEM SET 9.2  1. Use Gerschgorin’s theorem to determine bounds on the eigenvalues of  ⎡ ⎢⎣ 10 4 −1 4 2 −1 3  3 6  ⎤ ⎥⎦   a  A =   b  B =  ⎡ ⎢⎣ 4 2 −2 2 5 −2 3  3 4  ⎤ ⎥⎦  2. Use the Sturm sequence to show that  has one eigenvalue in the interval  2, 4 .  3. Bracket each eigenvalue of  4. Bracket each eigenvalue of  5. Bracket every eigenvalue of  ⎡ ⎢⎢⎢⎣  A =  5 −2 −2 0 −1 0  0 0 4 −1 0 4 −2 0 −2 5  A =  ⎤ ⎥⎦  0 4 −1 4  ⎡ ⎢⎣ 4 −1 −1 0 −1 ⎡ ⎢⎣ 6 1 0  1 8 2 0 2 9  ⎤ ⎥⎦  A =  ⎡ ⎢⎢⎢⎣  A =  2 −1 −1 0 −1 0  0 0 2 −1 0 2 −1 0 −1 1  ⎤ ⎥⎥⎥⎦  ⎤ ⎥⎥⎥⎦   369  9.5 Eigenvalues of Symmetric Tridiagonal Matrices  6. Tridiagonalize the matrix  with the Householder’s reduction.  7. Use the Householder’s reduction to transform the matrix  ⎡ ⎢⎣ 12 4  3 4 9 3 3 3 15  ⎤ ⎥⎦  A =  ⎡ ⎢⎢⎢⎣  A =  4 −2 −2 1 −2 −1  4 −2 1 −2  1 −1 1 4 −2 4  ⎢⎢⎢⎢⎢⎣  ⎡ ⎢⎢⎢⎣  A =  6 2 0 0 0 2 5 2 0 0 0 2 7 4 0 0 0 4 6 1 0 0 0 1 3  A =  4 −1 −1 0 −2 0 1  0 1 6 −2 0 3 2 2 4  ⎤ ⎥⎥⎥⎦  ⎤  ⎥⎥⎥⎥⎥⎦  ⎤ ⎥⎥⎥⎦  to tridiagonal form.  ⎡ 8.  cid:2  Compute all the eigenvalues of  9.  cid:2  Find the smallest two eigenvalues of  ⎡  10.  cid:2  Compute the three smallest eigenvalues of 3 −2 9 −4 3 −4  ⎢⎢⎢⎢⎢⎢⎢⎢⎣  A =  7 −4 −4 3 −4 −2 1 −2 0  8 −4 3 −4 1 −2  0 1 3 −2 1 3 −2 10 −4 3 11 −4 3 −4 12  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎦  and the corresponding eigenvectors.  11.  cid:2  Find the two smallest eigenvalues of the 6 × 6 Hilbert matrix  ⎡  ⎢⎢⎢⎢⎢⎢⎣  A =  ··· 1 1 2 ··· 1 2 1 3 ··· 1 3 1 4 ... ... ... 1 8 1 9 1 10 ···  1 3 1 4 1 5 ...  ⎤  ⎥⎥⎥⎥⎥⎥⎦  1 6 1 7 1 7 ... 1 11  Recall that this matrix is ill conditioned.   370  Symmetric Matrix Eigenvalue Problems  12.  cid:2  Rewrite the function lamRange d,c,N  so that it will bracket the N largest eigenvalues of a tridiagonal matrix. Use this function to compute the two largest eigenvalues of the Hilbert matrix in Example 9.11.  13.  cid:2   k  m1  u1  k  m2  u2  k  k  mn  nu  The differential equations governing free vibration of the mass-spring system are  k  −2u1 + u2  = m1 ¨u1 k ui−1 − 2ui + ui+1  = mi ¨ui k un−1 − un  = mn ¨un   i = 2, 3, . . . , n − 1    cid:14    cid:15 T  where ui t  is the displacement of mass i from its equilibrium position and k is the spring stiffness. Given k and the masses m = , write a program that computes N lowest circular frequencies of the system and the corresponding relative displacements of the masses. Run the program with N = 2, k = 500 kN m, and m =  1.0 1.0 1.0 8.0 1.0 1.0 8.0  ··· mn  m1 m2   cid:15 T   cid:14   kg  14.  cid:2   k1  m  u1 k2  m  u2 3k  k mn  nu  The ﬁgure shows n identical masses connected by springs of different stiffnesses. The equations governing free vibration of the system are  − cid:30  k1 + k2   cid:31   u1 + k2u2 = m¨u1 kiui−1 −  ki + ki+1 ui + ki+1ui+1 = m¨ui knun−1 − knun = m¨un   cid:14    cid:15    i = 2, 3, . . . , n − 1   where ui is the displacement of mass i from its equilibrium position. Given m and the spring stiffnesses k = , write a program that computes N lowest circular frequencies and the corresponding relative displacements of the masses. Run the program with N = 2, m = 2 kg, and  k1 k2  ···  kn  T   cid:15 T  400 400 400 0.2 400 400 200  kN m   cid:14   k =  Note that the system is weakly coupled, k4 being small. Do the results make sense?   371  9.5 Eigenvalues of Symmetric Tridiagonal Matrices  15.  cid:2   The differential equation of motion of the axially vibrating bar is  L  1  2  x  n   cid:3  cid:3  = ρ u E  ¨u  where u x, t  is the axial displacement, ρ represents the mass density, and E is the modulus of elasticity. The boundary conditions are u 0, t  = u  L, t  = 0. Letting  cid:3  u x, t  = y x  sin ωt, we obtain  The corresponding ﬁnite difference equations are   cid:3  cid:3  = −ω2  y  ρ E  y   cid:3   y 0  = y  L  = 0 ⎡ ⎤ ⎤  2 −1 0 ··· 0 0 −1 0 ··· 2 −1 0 0 −1 2 −1 ··· 0 ... ... ... ... ... ... 0 ··· −1 2 −1 0 0 −1 0 ··· 1 0  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  y1 y2 y3 ... yn−1 yn  %  =  ωL n  &2 ρ  E  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  y1 y2 y3 ... yn−1 yn 2   a  If the standard form of these equations is Hz = λz, write down H and the transformation matrix P in y = Pz.  b  Compute the lowest circular frequency of the bar with n = 10, 100, and 1,000 using the module inversePower3. Note: The analytical solution is ω1 = π E  ρ   2L .  This beam also appeared in Prob. 16, Problem Set 9.1.   √  16.  cid:2   The simply supported column is resting on an elastic foundation of stiffness k  N m per meter length . An axial force P acts on the column. The differential equation and the boundary conditions for the lateral displacement u are  u  P  1  2  n -1  n  xP  k L  u = 0  u 4  + P  cid:3  cid:3  + k u E I E I  0  = u L  = u  cid:3  cid:3   u 0  = u  cid:3  cid:3    L  = 0   372  Symmetric Matrix Eigenvalue Problems  Using the mesh shown, the ﬁnite difference approximation of these equations is   5 + α u1 − 4u2 + u3 = λ 2u1 − u2   −4u1 +  6 + α u2 − 4u3 + u4 = λ −u1 + 2u2 + u3  u1 − 4u2 +  6 + α u3 − 4u4 + u5 = λ −u2 + 2u3 − u4   ...  un−3 − 4un−2 +  6 + α un−1 − 4un = λ −un−2 + 4un−1 − un   un−2 − 4un−1 +  5 + α un = λ −un−1 + 2un   where  α = kh4 E I  =  1   n + 1 4  k L4 E I  λ = Ph2 E I  =  1   n + 1 2  P L2 E I  Write a program that computes the lowest three buckling loads P and the corre- sponding mode shapes. Run the program with k L4  E I   = 1,000 and n = 25.  17.  cid:2  Find smallest ﬁve eigenvalues of the 20 × 20 matrix  Note: This is a difﬁcult matrix that has many pairs of double eigenvalues.  18.  cid:2   ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣  A =  2 1 0 1 2 1 0 1 2 ... ... 0 0 ··· 0 0 ··· 1 0 ···  ...  0 0 1 ... 1 0 0  ··· ··· ··· ... 2 1 0  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦  0 1 0 0 0 0 ... ... 1 0 2 1 1 2  L  x  z  y  θ  P  When the depth width ratio of a beam is large, lateral buckling may occur. The differential equation that governs lateral buckling of the cantilever beam shown is  1  22  + γ 2  d2θ dx2  1 − x L  θ = 0   373  9.6 Other Methods  where θ is the angle of rotation of the cross section and  P2 L2   G J   E Iz   γ 2 = G J = torsional rigidity E Iz = bending rigidity about the z-axis  The boundary conditions are θx=0 = 0 and dθ  dxx=L = 0. Using the ﬁnite dif- ference approximation of the differential equation, determine the buckling load Pcr . The analytical solution is  19.  cid:2  Determine the value of z so that the smallest eigenvalue of the matrix   cid:25    G J   E Iz   L2  Pcr = 4.013 ⎡  ⎢⎢⎢⎢⎢⎢⎢⎢⎣  z 4 3 5 2 1 4 z 2 4 3 4 3 2 z 4 1 8 5 4 4 z 2 5 2 3 1 2 z 3 1 4 8 5 3 z  ⎤  ⎥⎥⎥⎥⎥⎥⎥⎥⎦  is equal to 1.0. Hint: This is a root-ﬁnding problem.  9.6 Other Methods  On occasions when all the eigenvalues and eigenvectors of a matrix are required, the OR algorithm is a worthy contender. It is based on the decomposition A = QR where Q and R are orthogonal and upper triangular matrices, respectively. The decompo- sition is carried out in conjunction with the Householder transformation. There is also a QL algorithm, A = QL, that works in the same manner, but here L is a lower triangular matrix. Schur’s factorization is another solid technique for determining the eigenvalues of A. Here the decomposition is A = QT UQ, where Q is orthogonal and U is an upper triangular matrix. The diagonal terms of U are the eigenvalues of A.  The LR algorithm is probably the fastest means of computing the eigenvalues; it is also very simple to implement—see Prob. 22 of Problem Set 9.1. But its stability is inferior to the other methods.   10 Introduction to Optimization  Find x that minimizes F  x  subject to g x  = 0, h x  ≥ 0.  10.1 Introduction  Optimization is the term often used for minimizing or maximizing a function. It is sufﬁcient to consider the problem of minimization only; maximization of F  x  is achieved by simply minimizing −F  x . In engineering, optimization is closely related to design. The function F  x , called the merit function or objective function, is the quantity that we wish to keep as small as possible, such as the cost or weight. The components of x, known as the design variables, are the quantities that we are free to adjust. Physical dimensions  lengths, areas, angles, and so on  are common examples of design variables.  Optimization is a large topic with many books dedicated to it. The best we can do in limited space is to introduce a few basic methods that are good enough for problems that are reasonably well behaved and do not involve too many design vari- ables. By omitting the more sophisticated methods, we may actually not miss all that much. All optimization algorithms are unreliable to a degree—any one may work on one problem and fail on another. As a rule of thumb, by going up in sophistication we gain computational efﬁciency, but not necessarily reliability.  The algorithms for minimization are iterative procedures that require starting values of the design variables x. If F  x  has several local minima, the initial choice of x determines which of these will be computed. There is no guaranteed way of ﬁnding the global optimal point. One suggested procedure is to make several computer runs using different starting points and pick the best result.  More often than not, the design variables are also subjected to restrictions, or constraints, which may have the form of equalities or inequalities. As an example, take the minimum weight design of a roof truss that has to carry a certain loading. Assume that the layout of the members is given, so that the design variables are the cross-sectional areas of the members. Here the design is dominated by inequality constraints that consist of prescribed upper limits on the stresses and possibly the displacements.  374   375  10.1 Introduction  The majority of available methods are designed for unconstrained optimization, where no restrictions are placed on the design variables. In these problems the min- ima, if they exit, are stationary points  points where the gradient vector of F  x  van- ishes . In the more difﬁcult problem of constrained optimization the minima are usually located where the F  x  surface meets the constraints. There are special al- gorithms for constrained optimization, but they are not easily accessible because of their complexity and specialization. One way to tackle a problem with constraints is to use an unconstrained optimization algorithm, but to modify the merit function so that any violation of constrains is heavily penalized.  Consider the problem of minimizing F  x  where the design variables are subject  to the constraints  gi x  = 0, hj  x  ≤ 0,  i = 1, 2, . . . , M j = 1, 2, . . . , N  We choose the new merit function be  ∗   x  = F  x  + λP x   F   10.1a   where  P x  = M cid:6   [gi x ]2 + N cid:6   5  i=1  j=1   cid:28    cid:29 62  max  0, hj  x    10.1b   is the penalty function and λ is a multiplier. The function max a, b  returns the larger of a and b. It is evident that P x  = 0 if no constraints are violated. Violation of a constraint imposes a penalty proportional to the square of the violation. Hence the minimization algorithm tends to avoid the violations, the degree of avoidance being dependent on the magnitude of λ. If λ is small, optimization will proceed faster be- cause there is more “space” in which the procedure can operate, but there may be signiﬁcant violation of constraints. In contrast, large λ can result in a poorly condi- tioned procedure, but the constraints will be tightly enforced. It is advisable to run the optimization program with λ that is on the small side. If the results show unac- ceptable constraint violation, increase λ and run the program again, starting with the results of the previous run.  An optimization procedure may also become ill conditioned when the con- straints have widely different magnitudes. This problem can be alleviated by scaling the offending constraints; that is, multiplying the constraint equations by suitable constants.  It is not always necessary  or even advisable  to employ an iterative minimization algorithm. In problems where the derivatives of F  x  can be readily computed and inequality constraints are absent, the optimal point can always be found directly by calculus. For example, if there are no constraints, the coordinates of the point where F  x  is minimized are given by the solution of the simultaneous  usually nonlinear    376  Introduction to Optimization equations ∇F  x  = 0. The direct method for ﬁnding the minimum of F  x  subject to equality constraints gi x  = 0, i = 1, 2, . . . m is to form the function   x, λ  = F  x  + m cid:6   ∗  F  λigi x   i=1   10.2a    10.2b   and solve the equations ∇F  ∗   x  = 0  gi x  = 0,  i = 1, 2, . . . , m  for x and λi. The parameters λi are known as the Lagrangian multipliers. The direct method can also be extended to inequality constraints, but the solution of the result- ing equations is not straightforward due to a lack of uniqueness.  This discussion exempts problems where the merit function and the constraints are linear functions of x. These problems, classiﬁed as linear programming prob- lems, can be solved without difﬁculty by specialized methods, such as the simplex method. Linear programming is used mainly for operations research and cost anal- ysis; there are very few engineering applications. This is not to say that linear pro- gramming has no place in nonlinear optimization. There are several effective meth- ods that rely in part on the simplex method. For example, problems with nonlinear constraints can often be solved by piecewise application of linear programming. The simplex method is also used to compute search directions in the method of feasible directions.  10.2 Minimization Along a Line  Consider the problem of minimizing a function f  x  of a single variable x with the constraints c ≤ x ≤ d. A hypothetical plot of the function is shown in Figure 10.1.  x  = 0 that There are two minimum points: a stationary point characterized by f represents a local minimum, and a global minimum at the constraint boundary. Finding the global minimum is simple. All the stationary points could be located by ﬁnding the roots of df dx = 0, and each constraint boundary may be checked for a global minimum by evaluating f  c  and f  d . Then why do we need an optimization algorithm? We need it if f  x  is difﬁcult or impossible to differentiate; for example, if f represents a complex computer algorithm.   cid:3   f x   c  Local minimum  Global minimum Constraint boundaries  x  d  Figure 10.1. Example of local and global minima.   377  10.2 Minimization Along a Line  Bracketing  Before a minimization algorithm can be entered, the minimum point must be brack- eted. The procedure of bracketing is simple: Start with an initial value of x0 and move downhill computing the function at x1, x2, x3, . . . until we reach the point xn where f  x  increases for the ﬁrst time. The minimum point is now bracketed in the inter- val  xn−2, xn . What should the step size hi = xi+1 − xi be? It is not a good idea have a constant hi because it often results in too many steps. A more efﬁcient scheme is to increase the size with every step, the goal being to reach the minimum quickly, even if the resulting bracket is wide. In our algorithm we chose to increase the step size by a constant factor; that is, we use hi+1 = chi, c > 1.  Golden Section Search  The golden section search is the counterpart of bisection used in ﬁnding roots of equations. Suppose that the minimum of f  x  has been bracketed in the interval  a, b  of length h. To telescope the interval, we evaluate the function at x1 = b − Rh and x2 = a + Rh, as shown in Figure 10.2 a . The constant R is to be determined shortly. If f1 > f2 as indicated in the ﬁgure, the minimum lies in  x1, b ; otherwise it is located in  a, x2 . Assuming that f1 > f2, we set a ← x1 and x1 ← x2, which yields a new interval  cid:3  = Rh, as illustrated in Figure 10.2 b . To carry out the next tele- and repeat the process.   a, b  of length h scoping operation we evaluate the function at x2 = a + Rh  cid:3   The procedure works only if Figures 10.1 a  and  b  are similar  i.e., if the same constant R locates x1 and x2 in both ﬁgures . Referring to Figure 10.2 a , we note that x2 − x1 = 2Rh − h. The same distance in Figure 10.2 b  is x1 − a = h . Equating the two, we get   cid:3  − Rh  cid:3   2Rh − h = h   cid:3  − Rh  cid:3   f x   a  f x   2Rh − h  f1 Rh  x1  f2  Rh  x2  h  a   a  Rh'  Rh' x1   b   x2  h'  b x  x  b  Figure 10.2. Golden section telescoping.   378  Introduction to Optimization  Substituting h   cid:3  = Rh and canceling h yields  2R − 1 = R 1 − R    cid:19  cid:19  Rn = ε   cid:19  cid:19 b − a  cid:19  cid:19    cid:19  cid:19 b − a  the solution of which is the golden ratio.1  R = −1 + √  5  2  = 0.618 033 989 . . .   10.3   Note that each telescoping decreases the interval containing the minimum by the factor R, which is not as good as the factor is 0.5 in bisection. However, the golden search method achieves this reduction with one function evaluation, whereas two evaluations would be needed in bisection.  The number of telescoping operations required to reduce h from   cid:19  cid:19 b − a   cid:19  cid:19  to an  error tolerance ε is given by  which yields  n = ln ε   ln R  = −2.078 087 ln   cid:19  cid:19 b − a  ε   cid:19  cid:19    10.4    cid:2  goldSearch  This module contains the bracketing and the golden section search algorithms. For the factor that multiplies successive search intervals in bracket we chose c = 1 + R.   module goldSearch  ’’’ a,b = bracket f,xStart,h   Finds the brackets  a,b  of a minimum point of the  user-supplied scalar function f x .  The search starts downhill from xStart with a step  length h.  x,fMin = search f,a,b,tol=1.0e-6   Golden section method for determining x that minimizes  the user-supplied scalar function f x .  The minimum must be bracketed in  a,b .  ’’’  import math  def bracket f,x1,h :  c = 1.618033989  f1 = f x1   x2 = x1 + h; f2 = f x2   1 R is the ratio of the sides of a “golden rectangle,” considered by ancient Greeks to have the perfect  proportions.   379  10.2 Minimization Along a Line   Determine downhill direction and change sign of h if needed  if f2 > f1:  h = -h  x2 = x1 + h; f2 = f x2    Check if minimum between x1 - h and x1 + h  if f2 > f1: return x2,x1 - h   Search loop  for i in range  100 :  h = c*h  x3 = x2 + h; f3 = f x3   if f3 > f2: return x1,x3  x1 = x2; x2 = x3  f1 = f2; f2 = f3  print "Bracket did not find a minimum"   def search f,a,b,tol=1.0e-9 :  nIter = int math.ceil -2.078087*math.log tol abs b-a      R = 0.618033989  C = 1.0 - R   First telescoping  x1 = R*a + C*b; x2 = C*a + R*b  f1 = f x1 ; f2 = f x2    Main loop  for i in range nIter :  if f1 > f2:  a = x1  else:  b = x2  x1 = x2; f1 = f2  x2 = C*a + R*b; f2 = f x2   x2 = x1; f2 = f1  x1 = R*a + C*b; f1 = f x1   if f1 < f2: return x1,f1  else: return x2,f2  EXAMPLE 10.1 Use goldSearch to ﬁnd x that minimizes  f  x  = 1.6x3 + 3x2 − 2x  subject to the constraint x ≥ 0. Compare the result with the analytical solution.  Solution. This is a constrained minimization problem. The minimum of f  x  is ei- ther a stationary point in x ≥ 0, or it is located at the constraint boundary x = 0.   380  Introduction to Optimization  cid:28  We handle the constraint with the penalty function method by minimizing f  x  + min 0, x  Starting at x = 1 and choosing h = 0.01 for the ﬁrst step size in bracket  both   cid:29 2.  λ  choices being rather arbitrary , we arrive at the following program:  ! usr bin python   example10_1  from goldSearch import *  def f x :  lam = 1.0   Constraint multiplier  c = min 0.0, x    Constraint function  return 1.6*x**3 + 3.0*x**2 - 2.0*x + lam*c**2  xStart = 1.0  h = 0.01  x1,x2 = bracket f,xStart,h   x,fMin = search f,x1,x2   print "x =",x   print "f x  =",fMin   input  "\nPress return to exit"   The result is  x = 0.2734941131714084  f x  = -0.28985978554959224  Because the minimum was found to be a stationary point, the constraint was not active. Therefore, the penalty function was superﬂuous, but we did not know that at the beginning.  The locations of stationary points are obtained analytically by solving   cid:3    x  = 4.8x2 + 6x − 2 = 0  f  The positive root of this equation is x = 0.273 49 4. Because this is the only positive root, there are no other stationary points in x ≥ 0 that we must check out. The only other possible location of a minimum is the constraint boundary x = 0. But here f  0  = 0 is larger than the function at the stationary point, leading to the conclusion that the global minimum occurs at x = 0.273 49 4.  EXAMPLE 10.2 The trapezoid shown is the cross section of a beam. It is formed by removing the top from a triangle of base B = 48 mm and height H = 60 mm. The problem is to ﬁnd the height y of the trapezoid that maximizes the section modulus  S = I ¯x  c   381  10.2 Minimization Along a Line  H  y  c  d  _ x  x  C  a B  b  b  where I ¯x is the second moment of the cross-sectional area about the axis that passes through the centroid C of the cross section. By optimizing the section modulus, we minimize the maximum bending stress σ max = M S in the beam, M being the bend- ing moment.  Solution. Considering the area of the trapezoid as a composite of a rectangle and two triangles, the section modulus is found through the following sequence of computa- tions:  Base of rectangle Base of triangle Area First moment of area about x-axis Location of centroid Distance involved in S Second moment of area about x-axis Parallel axis theorem Section modulus  a = B  H − y   H b =  B − a   2 A =  B + a  y 2 Qx =  ay  y 2 + 2 by 2 y 3 d = Qx  A c = y − d Ix = ay 3 3 + 2 by 3 12  I ¯x = Ix − Ad 2 S = I ¯x  c  We could use the formulas in the table to derive S as an explicit function of y, but that would involve a lot of error-prone algebra and result in an overly complicated expression. It makes more sense to let the computer do the work. The program we used and its output are listed next. Because we wish to maximize S with a minimization algorithm, the merit function is −S. There are no constraints in this problem.  ! usr bin python   example10_2  from goldSearch import *  def f y :  B = 48.0  H = 60.0  a = B* H - y  H  b =  B - a  2.0   382  Introduction to Optimization  A =  B + a *y 2.0  Q =  a*y**2  2.0 +  b*y**2  3.0  d = Q A  c = y - d  I =  a*y**3  3.0 +  b*y**3  6.0  Ibar = I - A*d**2  return -Ibar c  a,b = bracket f,yStart,h   yOpt,fOpt = search f,a,b   print "Optimal y =",yOpt   print "Optimal S =",-fOpt   print "S of triangle =",-f 60.0    input "Press return to exit"   Optimal y = 52.17627387056691  Optimal S = 7864.430941364856  S of triangle = 7200.0  yStart = 60.0   Starting value of y  h = 1.0   Size of first step used in bracketing  The printout includes the section modulus of the original triangle. The optimal  section shows a 9.2% improvement over the triangle.  10.3 Powell’s Method  Introduction  We now look at optimization in n-dimensional design space. The objective is to min- imize F  x , where the components of x are the n independent design variables. One way to tackle the problem is to use a succession of one-dimensional minimizations to close in on the optimal point. The basic strategy is as follows:  Choose a point x0 in the design space. Loop over i = 1, 2, 3, . . . : Choose a vector vi . Minimize F  x  along the line through xi−1 in the direction of vi. If xi − xi−1 < ε exit loop.  Let the minimum point be xi.  The minimization along a line can be accomplished with any one-dimensional optimization algorithm  such as the golden section search . The only question left open is how to choose the vectors vi.   383  10.3 Powell’s Method  Conjugate Directions  Consider the quadratic function F  x  = c −   cid:6   i  bi xi + 1 2   cid:6    cid:6   i  j  Aij xixj  Differentiation with respect to xi yields  which can be written in vector notation as  = c − bT x + 1 2  xT Ax  = −bi +  ∂ F ∂xi  Aij xj   cid:6   j  ∇F = −b + Ax  x = x0 + su  where ∇F is called the gradient of F .  Now consider the change in the gradient as we move from point x0 in the direc-  tion of a vector u. The motion takes place along the line   10.5    10.6   where s is the distance moved. Substitution into Eq.  10.6  yields the expression for the gradient at x:  ∇Fx0+su = −b + A  x0 + su  = ∇Fx0  + s Au  Note that the change in the gradient is s Au. If this change is perpendicular to a vector v; that is, if  vT Au = 0,   10.7   the directions of u and v are said to be mutually conjugate  non-interfering . The implication is that once we have minimized F  x  in the direction of v, we can move along u without ruining the previous minimization.  For a quadratic function of n independent variables it is possible to construct n mutually conjugate directions. Therefore, it would take precisely n line mini- mizations along these directions to reach the minimum point. If F  x  is not a quadratic function, Eq.  10.5  can be treated as a local approximation of the merit function, obtained by truncating the Taylor series expansion of F  x  about x0  see Appendix A1 :  F  x  ≈ F  x0  + ∇F  x0  x − x0  + 1 2   x − x0 TH x0  x − x0   Now the conjugate directions based on the quadratic form are only approximations, valid in the close vicinity of x0. Consequently, it would take several cycles of n line minimizations to reach the optimal point.   384  Introduction to Optimization  The various conjugate gradient methods use different techniques for construct- ing conjugate directions. The so-called zero-order methods work with F  x  only, whereas the ﬁrst-order methods use both F  x  and ∇F . The ﬁrst-order methods are computationally more efﬁcient, of course, but the input of ∇F , if it is available at all, can be very tedious.  Powell’s Algorithm  Powell’s method is a zero-order method, requiring the evaluation of F  x  only. The basic algorithm is as follows:  Choose a point x0 in the design space. Choose the starting vectors vi, 1, 2, . . . , n  the usual choice is vi = ei,  Cycle:  where ei is the unit vector in the xi-coordinate direction . Loop over i = 1, 2, . . . , n:  Minimize F  x  along the line through xi−1 in the direction of vi. Let the minimum point be xi.  End loop. vn+1 ← x0 − xn. Minimize F  x  along the line through x0 in the direction of vn+1. Let the minimum point be xn+1. if xn+1 − x0 < ε exit loop. Loop over i = 1, 2, . . . , n:  vi ← vi+1  v1 is discarded; the other vectors are reused .  End loop. x0 ← xn+1  End cycle.  Powell demonstrated that the vectors vn+1 produced in successive cycles are mu- tually conjugate, so that the minimum point of a quadratic surface is reached in pre- cisely n cycles. In practice, the merit function is seldom quadratic, but as long as it can be approximated locally by Eq.  10.5 , Powell’s method will work. Of course, it usually takes more than n cycles to arrive at the minimum of a nonquadratic function. Note that it takes n line minimizations to construct each conjugate direction. Figure 10.3 a  illustrates one typical cycle of the method in a two dimensional design space  n = 2 . We start with point x0 and vectors v1 and v2. Then we ﬁnd the distance s1 that minimizes F  x0 + sv1 , ﬁnishing up at point x1 = x0 + s1v1. Next, we determine s2 that minimizes F  x1 + sv2  which takes us to x2 = x1 + s2v2. The last search direction is v3 = x2 − x0. After ﬁnding s3 by minimizing F  x0 + sv3  we get to x3 = x0 + s3v3, completing the cycle.   385  10.3 Powell’s Method  0P  x    0  s v1 1  s v3 3  P  x   1 1 v1  v3  v2  P  x   3 3 s v2 2  P  x   2 2  a   Figure 10.3. The method of Powell.  P0  P1  P6  3P  P4  P5  P2   b   Figure 10.3 b  shows the moves carried out in two cycles superimposed on the contour map of a quadratic surface. As explained earlier, the ﬁrst cycle starts at point P0 and ends up at P3. The second cycle takes us to P6, which is the optimal point. The directions P0 P3 and P3 P6 are mutually conjugate.  Powell’s method does have a major ﬂaw that has to be remedied—if F  x  is not a quadratic, the algorithm tends to produce search directions that gradually be- come linearly dependent, thereby ruining the progress toward the minimum. The source of the problem is the automatic discarding of v1 at the end of each cycle. It has been suggested that it is better to throw out the direction that resulted in the largest decrease of F  x , a policy that we adopt. It seems counterintuitive to discard the best direction, but it is likely to be close to the direction added in the next cy- cle, thereby contributing to linear dependence. As a result of the change, the search directions cease to be mutually conjugate, so that a quadratic form is no longer min- imized in n cycles. This is not a signiﬁcant loss because in practice F  x  is seldom a quadratic.  Powell suggested a few other reﬁnements to speed up convergence. Because they  complicate the bookkeeping considerably, we did not implement them.   cid:2  powell  The algorithm for Powell’s method is listed next. It uses two arrays: df contains the decreases of the merit function in the ﬁrst n moves of a cycle, and the matrix u stores the corresponding direction vectors vi one vector per row .   module powell  ’’’ xMin,nCyc = powell F,x,h=0.1,tol=1.0e-6   Powell’s method of minimizing user-supplied function F x .  = starting point  x  h  xMin = mimimum point  = initial search increment used in ’bracket’   386  Introduction to Optimization  nCyc = number of cycles  ’’’  import numpy as np  from goldSearch import *  import math  def powell F,x,h=0.1,tol=1.0e-6 :  def f s : return F x + s*v    F in direction of v  n = len x   df = np.zeros n   u = np. identity n   for j in range 30 :  xOld = x.copy    fOld = F xOld    Number of design variables   Decreases of F stored here   Vectors v stored here by rows   Allow for 30 cycles:   Save starting point   First n line searches record decreases of F  for i in range n :  v = u[i]  a,b = bracket f,0.0,h   s,fMin = search f,a,b   df[i] = fOld - fMin  fOld = fMin  x = x + s*v   Last line search in the cycle  v = x - xOld  a,b = bracket f,0.0,h   s,fLast = search f,a,b   x = x + s*v   Check for convergence  if math.sqrt np.dot x-xOld,x-xOld  n  < tol: return x,j+1   Identify biggest decrease & update search directions  iMax = np.argmax df   for i in range iMax,n-1 :  u[i] = u[i+1]  u[n-1] = v  print "Powell did not converge"   EXAMPLE 10.3 Find the minimum of the function2  F = 100 y − x2 2 +  1 − x 2  2 From Shoup, T.E. and Mistree, F., Optimization Methods with Applications for Personal Computers,  Prentice-Hall, 1987.   387  10.3 Powell’s Method  1000 800 600 400 200 0 0.0 0.5  -1.0  z  -0.5  1.0  y  1.5  -1.5  -1.0  0.0 0.5  -0.5  x 1.0  1.5  with Powell’s method starting at the point  −1, 1 . This function has an interesting topology. The minimum value of F occurs at the point  1, 1 , But there is a con- siderable hump between the starting and minimum points that the algorithm must negotiate.  Solution. The program that solves this unconstrained optimization problem is as follows:  def F x : return 100.0* x[1] - x[0]**2 **2 +  1 - x[0] **2  ! usr bin python   example10_3  from powell import *  from numpy import array  xStart = array [-1.0, 1.0]   xMin,nIter = powell F,xStart   print "x =",xMin   print "F x  =",F xMin    print "Number of cycles =",nIter   input  "Press return to exit"   x = [ 1.  1.]  F x  = 3.71750701585e-029  Number of cycles = 12  As seen in the printout, the minimum point was obtained after 12 cycles.   388  Introduction to Optimization  EXAMPLE 10.4 Use powell to determine the smallest distance from the point  5, 8  to the curve xy = 5.  Solution. This is a constrained optimization problem: Minimize F  x, y  =  x − 5 2 +  y − 8 2  the square of the distance  subject to the equality constraint xy − 5 = 0. The following program uses Powell’s method with penalty function:   example10_4  from powell import *  from numpy import array  from math import sqrt  def F x :  lam = 1.0  c = x[0]*x[1] - 5.0   Constraint equation  return  distSq x  + lam*c**2   Penalized merit function   Penalty multiplier  def distSq x : return  x[0] - 5 **2 +  x[1] - 8 **2  xStart = array [ 1.0,5.0]   x,numIter = powell F,xStart,0.1   print "Intersection point =",x   print "Minimum distance =", sqrt distSq x     print "xy =", x[0]*x[1]   print "Number of cycles =",numIter   input  "Press return to exit"   As mentioned earlier, the value of the penalty function multiplier λ  called lam in the program  can have a profound effect on the result. We chose λ = 1  as in the program listing  with the following result:  Intersection point = [ 0.73306761  7.58776385]  Minimum distance = 4.28679958767  xy = 5.56234387462  Number of cycles = 5  The small value of λ favored speed of convergence over accuracy. Because the violation of the constraint xy = 5 is clearly unacceptable, we ran the program again with λ = 10 000 and changed the starting point to  0.73306761, 7.58776385 , the end point of the ﬁrst run. The results shown next are now acceptable:   389  10.3 Powell’s Method  Intersection point = [ 0.65561311  7.62653592]  Minimum distance = 4.36040970945  xy = 5.00005696357  Number of cycles = 5  Could we have used λ = 10 000 in the ﬁrst run? In this case we would be lucky and obtain the minimum in 17 cycles. Hence we save seven cycles by using two runs. However, a large λ often causes the algorithm to hang up, so that it is generally wise to start with a small λ.  Check. Since we have an equality constraint, the optimal point can readily be found by calculus. The function in Eq.  10.2a  is  here λ is the Lagrangian multiplier   ∗   x, y, λ  =  x − 5 2 +  y − 8 2 + λ xy − 5   F  so that Eqs.  10.2b  become  ∗  ∂ F ∂x ∗  = 2 x − 5  + λy = 0  = 2 y − 8  + λx = 0  ∂ F ∂y g x  = xy − 5 = 0  which can be solved with the Newton-Raphson method  the function newton- Raphson2 in Section 4.6 . In the following program we used the notation x = [ x  y λ ]T .  ! usr bin python   example10_4_check  import numpy as np  from newtonRaphson2 import *  def F x :  return np.array [2.0* x[0] - 5.0  + x[2]*x[1],  2.0* x[1] - 8.0  + x[2]*x[0],  x[0]*x[1] - 5.0]   \  \  xStart = np.array [1.0, 5.0, 1.0]   print "x = ", newtonRaphson2 F,xStart    input "Press return to exit"   The result is  x =  [ 0.6556053  7.62653992  1.13928328]   390  Introduction to Optimization  EXAMPLE 10.5  u3  L  1  3  2 L  u2  u1  P  The displacement formulation of the truss shown results in the following simultane- ous equations for the joint displacements u:  ⎡ ⎢⎣ 2  √ E 2L 2  √ 2A 2 + A 3 −A 3 A 3 −A 3 −A 3 √ A 3 −A 3 2 2A 1 + A 3 A 3  ⎤ ⎥⎦  ⎡ ⎢⎣ u1  u2 u3  ⎤ ⎥⎦ =  ⎡ ⎢⎣ 0−P  ⎤ ⎥⎦  0  where E represents the modulus of elasticity of the material and Ai is the cross- sectional area of member i. Use Powell’s method to minimize the structural vol- ume  i.e., the weight  of the truss while keeping the displacement u2 below a given value δ.  Solution. Introducing the dimensionless variables xi = E δ P L  vi = ui  δ  Ai,  ⎡ ⎢⎣ 2  the equations become  √ 2x2 + x3 −x3 x3 −x3 −x3 √ x3 −x3 2 2x1 + x3 x3 The structural volume to be minimized is  √ 1 2 2  ⎤ ⎥⎦ =  ⎤ ⎥⎦  ⎡ ⎢⎣ v1  v2 v3  ⎤ ⎥⎦  ⎡ ⎢⎣ 0 −1 0   a   √  V = L A 1 + A 2 +  2A 3  = P L2 E δ   x1 + x2 +  √  2x3   In addition to the displacement constraint u2 ≤ δ, we should also prevent the cross- sectional areas from becoming negative by applying the constraints Ai ≥ 0. Thus the optimization problem becomes: Minimize  F = x1 + x2 +  2x3  √  with the inequality constraints  v2 ≤ 1  xi ≥ 0  i = 1, 2, 3   Note that to obtain v2 we must solve Eqs.  a .   391  10.3 Powell’s Method  Here is the program:  ! usr bin python   example10_5  from powell import *  from numpy import array  from math import sqrt  from gaussElimin import *  def F x :  global v, weight  lam = 100.0  c = 2.0*sqrt 2.0   A = array [[c*x[1] + x[2], -x[2],  x[2]],  [-x[2],  [ x[2],  x[2], -x[2]],  -x[2],  c*x[0] + x[2]]]  c  \  \  b = array [0.0, -1.0, 0.0]   v = gaussElimin A,b   weight = x[0] + x[1] + sqrt 2.0 *x[2]  penalty = max 0.0,abs v[1]  - 1.0 **2  + max 0.0,-x[0] **2  + max 0.0,-x[1] **2  + max 0.0,-x[2] **2  return weight + penalty*lam  \  \  \  xStart = array [1.0, 1.0, 1.0]   x,numIter = powell F,xStart   print "x = ",x   print "v = ",v   print "Relative weight F = ",weight   print "Number of cycles = ",numIter   input "Press return to exit"   The subfunction F returns the penalized merit function. It includes the code that sets up and solves Eqs.  a . The displacement vector v is called u in the program.  The ﬁrst run of the program started with x =  1 1 1  T  and used λ = 100 for   cid:14    cid:15   the penalty multiplier. The results were  x =  v =  [ 3.73870376  3.73870366  5.28732564]  [-0.26747239 -1.06988953 -0.26747238]  Relative weight F =  14.9548150471  Number of cycles =  10   392  Introduction to Optimization  Because the magnitude of v2 is excessive, the penalty multiplier was increased to 10 000 and the program run again using the output x from the last run as the input. As seen, v2 is now much closer to the constraint value.  x = [ 3.99680758  3.9968077  5.65233961]  v = [-0.25019968 -1.00079872 -0.25019969]  Relative weight F = 15.9872306185  Number of cycles =  11  In this problem the use of λ = 10 000 at the outset would not work. You are invited  to try it.  10.4 Downhill Simplex Method  The downhill simplex method is also known as the Nelder-Mead method. The idea is to employ a moving simplex in the design space to surround the optimal point and then shrink the simplex until its dimensions reach a speciﬁed error tolerance. In n-dimensional space a simplex is a ﬁgure of n + 1 vertices connected by straight lines and bounded by polygonal faces. If n = 2, a simplex is a triangle; if n = 3, it is a tetrahedron. The allowed moves of a two-dimensional simplex  n = 2  are illustrated in Figure 10.4. By applying these moves in a suitable sequence, the simplex can always hunt down the minimum point, enclose it, and then shrink around it. The direction of a move is determined by the values of F  x   the function to be minimized  at the vertices. The vertex with the highest value of F is labeled Hi, and Lo denotes the ver- tex with the lowest value. The magnitude of a move is controlled by the distance d measured from the Hi vertex to the centroid of the opposing face  in the case of the triangle, the middle of the opposing side .  Hi  d  Original simplex  Hi  0.5d  Hi  Hi  2d  Reflection  3d  Expansion  Contraction  Shrinkage  Lo  Figure 10.4. Allowed moves of a simplex.   393  10.4 Downhill Simplex Method  The outline of the algorithm is as follows:  Choose a starting simplex. Cycle until d ≤ ε  ε being the error tolerance : Try reﬂection. if new vertex ≤ old Lo: accept reﬂection  Try expansion. if new vertex ≤ old Lo: accept expansion.  else:  if new vertex > old Hi:  Try contraction. if new vertex ≤ old Hi: accept contraction.  else: use shrinkage.  End cycle.  The downhill simplex algorithm is much slower than Powell’s method in most cases, but makes up for it in robustness. It often works in problems where Powell’s method hangs up.   cid:2  downhill  The implementation of the downhill simplex method is given next. The starting sim- plex has one of its vertices at x0 and the others at x0 + eib  i = 1, 2, . . . , n , where ei is the unit vector in the direction of the xi-coordinate. The user inputs the vector x0  called xStart in the program  and the edge length b of the simplex.   module downhill  ’’’ x = downhill F,xStart,side,tol=1.0e-6   Downhill simplex method for minimizing the user-supplied  scalar function F x  with respect to the vector x.  xStart = starting vector x.  side  = side length of the starting simplex  default is 0.1   def downhill F,xStart,side=0.1,tol=1.0e-6 :  n = len xStart    Number of variables  ’’’  import numpy as np  import math  x = np.zeros  n+1,n    f = np.zeros n+1    Generate starting simplex  x[0] = xStart   394  Introduction to Optimization  for i in range 1,n+1 :  x[i] = xStart  x[i,i-1] = xStart[i-1] + side   Compute values of F at the vertices of the simplex  for i in range n+1 : f[i] = F x[i]    Main loop  for k in range 500 :   Find highest and lowest vertices  iLo = np.argmin f   iHi = np.argmax f    Compute the move vector d  d =  - n+1 *x[iHi] + np.sum x,axis=0   n   Check for convergence  if math.sqrt np.dot d,d  n  < tol: return x[iLo]   Try reflection  xNew = x[iHi] + 2.0*d  fNew = F xNew   if fNew <= f[iLo]:   Accept reflection   Try expanding the reflection  x[iHi] = xNew  f[iHi] = fNew  xNew = x[iHi] + d  fNew = F xNew   if fNew <= f[iLo]:   Accept expansion  if fNew <= f[iHi]:   Accept reflection  x[iHi] = xNew  f[iHi] = fNew  else:   Try reflection again  x[iHi] = xNew  f[iHi] = fNew  else:   Try contraction  xNew = x[iHi] + 0.5*d  fNew = F xNew   if fNew <= f[iHi]:  Accept contraction  x[iHi] = xNew  f[iHi] = fNew  else:   Use shrinkage  for i in range len x  :  if i != iLo:   395  10.4 Downhill Simplex Method  x[i] =  x[i] - x[iLo] *0.5  f[i] = F x[i]   print "Too many iterations in downhill"   return x[iLo]  EXAMPLE 10.6 Use the downhill simplex method to minimize  F = 10x2  1  + 3x2  2  − 10x1x2 + 2x1  The coordinates of the vertices of the starting simplex are  0, 0 ,  0, −0.2  and  0.2, 0 . Show graphically the ﬁrst four moves of the simplex.  Solution. The ﬁgure shows the design space  the x1-x2 plane . The numbers in the ﬁgure are the values of F at the vertices. The move numbers are enclosed in circles. The starting move  move 1  is a reﬂection, followed by an expansion  move 2 . The next two moves are reﬂections. At this stage, the simplex is still moving downhill. Contraction will not start until move 8, when the simplex has surrounded the optimal point at  −0.6, −1.0 .  -0.6  -0.4  -0.2  0 0.12  0  1  0  0  -0.28  3  2  -0.4  -0.2  4  0.2  -0.2  -0.4  -0.6  -0.8  θ  θ  h  b  -0.48  EXAMPLE 10.7  The ﬁgure shows the cross section of a channel carrying water. Use the downhill sim- plex to determine h, b, and θ that minimize the length of the wetted perimeter while maintaining a cross-sectional area of 8 m2.  Minimizing the wetted perimeter results in the least resistance to the ﬂow.  Check the answer with calculus.  Solution. The cross-sectional area of the channel is   cid:28   cid:29  b +  b + 2h tan θ   A = 1 2  h =  b + h tan θ h   396  Introduction to Optimization  and the length of the wetted perimeter is  S = b + 2 h sec θ   The optimization problem is to minimize S subject to the constraint A − 8 = 0. Us- ing the penalty function to take care of the equality constraint, the function to be minimized is   cid:14   S  ∗ = b + 2h sec θ + λ  cid:15 T  b h θ  and starting with x0 =   cid:29 2  cid:28   b + h tan θ h − 8  cid:15 T   cid:14   4 2 0  , we arrive at the fol-  Letting x = lowing program:  ! usr bin python   example10_7  import numpy as np  import math  from downhill import *  def S x :  global perimeter,area  lam = 10000.0  perimeter = x[0] + 2.0*x[1] math.cos x[2]   area =  x[0] + x[1]*math.tan x[2]  *x[1]  return perimeter + lam* area - 8.0 **2  xStart = np.array [4.0, 2.0, 0.0]   x = downhill S,xStart   area =  x[0] + x[1]*math.tan x[2]  *x[1]  print "b = ",x[0]   print "h = ",x[1]   print "theta  deg  = ",x[2]*180.0 math.pi   print "area = ",area   print "perimeter = ",perimeter   input "Finished. Press return to exit"   The results are  b = 2.4816069148  h = 2.14913738694  theta  deg  =  30.0000185796  area =  7.99997671775  perimeter = 7.44482803952  Check. Because we have an equality constraint, the problem can be solved by calculus with help from a Lagrangian multiplier. Referring to Eqs.  10.2a ,   397  10.4 Downhill Simplex Method we have F = S and g = A − 8, so that ∗ = S + λ A − 8  = b + 2 h sec θ  + λ  F   cid:28   cid:29   b + h tan θ h − 8  Therefore, Eqs.  10.2b  become  ∗  ∂ F ∂b ∗ ∂ F ∂h ∗ ∂ F ∂θ  = 1 + λh = 0 = 2 sec θ + λ b + 2h tan θ  = 0 = 2h sec θ tan θ + λh2 sec2 θ = 0 g =  b + h tan θ h − 8 = 0  which can be solved with newtonRaphson2 as shown next.  ! usr bin python   example10_7_check  import numpy as np  import math  from newtonRaphson2 import *  def f x :  f = np.zeros 4   f[0] = 1.0 + x[3]*x[1]  f[1] = 2.0 math.cos x[2]  + x[3]* x[0]  \  + 2.0*x[1]*math.tan x[2]    f[2] = 2.0*x[1]*math.tan x[2]  math.cos x[2]  \  + x[3]* x[1] math.cos x[2]  **2  f[3] =  x[0] + x[1]*math.tan x[2]  *x[1] - 8.0  return f  xStart = np.array [3.0, 2.0, 0.0, 1.0]   print "x =",newtonRaphson2 f,xStart    input "Press return to exit"    cid:14    cid:15 T  The solution x =  b h θ  λ  is  x = [ 2.48161296  2.14913986  0.52359878 -0.46530243]  EXAMPLE 10.8  θ1  d1  L  d2  L  θ2   398  Introduction to Optimization  The fundamental circular frequency of the stepped shaft is required to be higher than ω0  a given value . Use the downhill simplex to determine the diameters d1 and d2 that minimize the volume of the material without violating the frequency constraint. The approximate value of the fundamental frequency can be computed by solving the eigenvalue problem  obtainable from the ﬁnite element approximation   +*  +  *  *  + d 4 4 d 4 1 2d 4 2  2   2d4 2 4d 4 2  θ 1 θ 2  = 4γ L4ω2 105E  4 d 2 1  + d2 −3d2  2   −3d 2 2 4d 2 2  2  +*  +  θ 1 θ 2  where  where  γ = mass density of the material ω = circular frequency E = modulus of elasticity  θ 1, θ 2 = rotations at the simple supports  Solution. We start by introducing the dimensionless variables xi = di  d0, where d0 is an arbitrary “base” diameter. As a result, the eigenvalue problem becomes  *  +*  +  *  = λ  θ 1 θ 2  + x4 4 x4 1 2x4 2  2  2x4 2 4x4 2  4 x2 1  + x2 −3x2  2  −3x2 2 4x2 2  2  +*  +  θ 1 θ 2   a   λ = 4γ L4ω2 105Ed2 0  In the following program we assume that the constraint on the frequency ω is equiv- alent to λ ≥ 0.4.  ! usr bin python   example10_8  import numpy as np  from stdForm import *  from inversePower import *  from downhill import *  def F x :  global eVal  lam = 1.0e6  eVal_min = 0.4  A = np.array [[4.0* x[0]**4 + x[1]**4 , 2.0*x[1]**4],  \  [2.0*x[1]**4, 4.0*x[1]**4]]   B = np.array [[4.0* x[0]**2 + x[1]**2 , -3.0*x[1]**2], \  [-3*x[1]**2, 4.0*x[1]**2]]   H,t = stdForm A,B   eVal,eVec = inversePower H,0.0   return x[0]**2 + x[1]**2 + lam* max 0.0,eVal_min - eVal  **2   399  10.4 Downhill Simplex Method  xStart = np.array [1.0,1.0]   x = downhill F,xStart,0.1   print "x = ", x   print "eigenvalue = ",eVal   input  "Press return to exit"   Although a 2 × 2 eigenvalue problem can be solved easily, we avoid the work involved by employing functions that have been already prepared— using stdForm to turn the eigenvalue problem into standard form, and inversePower to compute the eigenvalue closest to zero. The results shown next were obtained with x1 = x2 = 1 as the starting values and 106 for the penalty multiplier. The downhill simplex method is robust enough to alleviate the need for multiple runs with an increasing penalty multiplier.  x =  [ 1.07512696  0.79924677]  eigenvalue = 0.399997757238  PROBLEM SET 10.1 1.  cid:2  The Lennard-Jones potential between two molecules is  ’1 σ  r  1 σ     26  212 −  r  V = 4ε  where ε and σ are constants, and r is the distance between the molecules. Use the module goldSearch to ﬁnd σ  r that minimizes the potential and verify the result analytically.  2.  cid:2  One wave-function of a hydrogen atom is  ψ = C 27 − 18σ + 2σ 2 e  −σ  3  where  2 3  &  %  z a0  σ = zr  a0 C = √ 1 3π 81 z = nuclear charge a0 = Bohr radius r = radial distance  4 π  0  sin x cos px dx  Find σ where ψ is at a minimum. Verify the result analytically,  3.  cid:2  Determine the parameter p that minimizes the integral  Hint: Use quadrature to evaluate the integral.   400  Introduction to Optimization  4.  cid:2   R1= 2Ω  R2= 3.6 Ω  i1  E  = 120 V  i2 R  i1  R  5 = 1.2  Ω  i2  R  3  = 1.5  Ω  R  4  = 1.8  Ω  Kirchoff’s equations for the two loops of the electrical circuit are  R1i1 + R3i1 + R i1 − i2  = E R2i2 + R4i2 + R5i2 + R i2 − i1  = 0  Find the resistance R that maximizes the power dissipated by R. Hint: Solve Kirchoff’s equations numerically with one of the functions in Chapter 2.  5.  cid:2   a  T  r  T  A wire carrying an electric current is surrounded by rubber insulation of outer radius r . The resistance of the wire generates heat, which is conducted through the insulation and convected into the surrounding air. The temperature of the wire can be shown to be  %  &  T = q 2π  ln r  a   k  + 1 hr  + T∞  where  q = rate of heat generation in wire = 50 W m a = radius of wire = 5 mm k = thermal conductivity of rubber = 0.16 W m · K h = convective heat-transfer coefﬁcient = 20 W m2 · K T∞ = ambient temperature = 280 K  Find r that minimizes T. 6.  cid:2  Minimize the function  F  x, y  =  x − 1 2 +  y − 1 2  subject to the constraints x + y ≥ 1 and x ≥ 0.6.   401  10.4 Downhill Simplex Method  7.  cid:2  Find the minimum of the function  F  x, y  = 6x2 + y 3 + xy  in y ≥ 0. Verify the result analytically.  8.  cid:2  Solve Prob. 7 if the constraint is changed to y ≥ −2. 9.  cid:2  Determine the smallest distance from the point  1, 2  to the parabola y = x2. 10.  cid:2   Determine x that minimizes the distance d between the base of the area shown and its centroid C.  11.  cid:2   12.  cid:2   0.4 m  x  0.2 m C  d 0.4 m  r  C  x  H  0.43H  a  b  b  a  The cylindrical vessel of mass M has its center of gravity at C. The water in the vessel has a depth x. Determine x so that the center of gravity of the vessel-water combination is as low as possible. Use M = 115 kg, H = 0.8 m, and r = 0.25 m.  The sheet of cardboard is folded along the dashed lines to form a box with an open top. If the volume of the box is to be 1.0 m3, determine the dimen- sions a and b that would use the least amount of cardboard. Verify the result analytically.   402  Introduction to Optimization  13.  cid:2   A  a  b  C  B  v  u  B' P  V = −Pv + k a + b   δA B + k a + b   δ BC  2b  2a   cid:25   cid:25   a + u 2 + v2 − a  b − u 2 + v2 − b  δA B = δ BC =  The elastic cord A BC has an extensional stiffness k. When the vertical force P is C. The potential energy of the applied at B, the cord deforms to the shape A B system in the deformed position is   cid:3   where  14.  cid:2   are the elongations of A B and BC. Determine the displacements u and v by min- imizing V  this is an application of the principle of minimum potential energy: A system is in stable equilibrium if its potential energy is at minimum . Use a = 150 mm, b = 50 mm, k = 0.6 N mm, and P = 5 N.  b =  4 m  θ  θ  P =  50 kN  V = bA cos θ  Each member of the truss has a cross-sectional area A. Find A and the angle θ that minimize the volume  of the material in the truss without violating the constraints  σ ≤ 150 MPa  δ ≤ 5 mm  where  δ = and E = 200 × 109.  σ =  P  2A sin θ  = stress in each member  Pb  = displacement at the load P  2EA sin 2θ sin θ   403  10.4 Downhill Simplex Method  15.  cid:2  Solve Prob. 14 if the allowable displacement is changed to 2.5 mm. 16.  cid:2   1r  .  L = 1.0 m  r2. L = 1.0 m  P = 10 kN  The cantilever beam of the circular cross section is to have the smallest volume possible subject to constraints σ 1 ≤ 180 MPa  σ 2 ≤ 180 MPa  δ ≤ 25 mm  where  σ 1 = 8P L πr 3 1 σ 2 = 4P L πr 3 2 δ = P L3 3π E  = maximum stress in left half  = maximum stress in right half %  &  7 r 4 1  + 1 r 4 2  = displacement at free end  and E = 200 GPa. Determine r1 and r2. 17.  cid:2  Find the minimum of the function  F  x, y, z  = 2x2 + 3y 2 + z2 + xy + xz − 2y  and conﬁrm the result analytically.  18.  cid:2   The cylindrical container has a conical bottom and an open top. If the volume V of the container is to be 1.0 m3, ﬁnd the dimensions r , h, and b that minimize the surface area S. Note that  r  h  b  % V = πr 2 b 1 3 2h + S = πr  & + h  cid:25  b2 + r 2  2   404  Introduction to Optimization  19.  cid:2   The equilibrium equations of the truss shown are  σ 1A 1 + 4 5  σ 2A 2 = P  σ 2A 2 + σ 3A 3 = P  3 5  where σ i is the axial stress in member i and Ai is the cross-sectional area. The third equation is supplied by compatibility  geometric constraints on the elon- gations of the members :  Find the cross-sectional areas of the members that minimize the weight of the truss without the stresses exceeding 150 MPa.  20.  cid:2   3 m  4 m  2  1  3  P = 200 kN  P = 200 kN  16 5  σ 1 − 5σ 2 + 9 5  σ 3 = 0  θ1 L1  1y  W1  B  y2  θ2 2L  W2  H  θ3  3L  A cable supported at the ends carries the weights W1 and W2. The potential en- ergy of the system is  V = −W1y1 − W2y2  = −W1L1 sin θ 1 − W2 L1 sin θ 1 + L2 sin θ 2   and the geometric constraints are  L1 cos θ 1 + L2 cos θ 2 + L3 cos θ 3 = B L1 sin θ 1 + L2 sin θ 2 + L3 sin θ 3 = H  The principle of minimum potential energy states that the equilibrium conﬁgu- ration of the system is the one that satisﬁes geometric constraints and minimizes the potential energy. Determine the equilibrium values of θ 1, θ 2, and θ 3 given that L1 = 1.2 m, L2 = 1.5 m, L3 = 1.0 m, B = 3.5 m, H = 0, W1 = 20 kN, and W2 = 30 kN.   405  10.4 Downhill Simplex Method  21.  cid:2   2P uv P 3  2  L 30o  L 30o  1  The displacement formulation of the truss results in the equations  *  3A 1 + 3A 3 3A 1 + √ √  √ 3A 1 + √  3A 3 3A 3 A 1 + 8A 2 + A 3  E 4L  +*  +  *  +  =  u v  P 2P  where E is the modulus of elasticity, Ai is the cross-sectional area of member i, and u, v are the displacement components of the loaded joint. Letting A 1 = A 3  a symmetric truss , determine the cross-sectional areas that minimize the struc- tural volume without violating the constraints u ≤ δ and v ≤ δ. Hint: Nondimen- sionalize the problem as in Example 10.5.  22.  cid:2  Solve Prob. 21 if the three cross-sectional areas are independent. 23.  cid:2  A beam of rectangular cross section is cut from a cylindrical log of diameter d. Calculate the height h and the width b of the cross section that maximizes the cross-sectional moment of inertia I = bh3 12. Check the result with calculus.    Appendices  A1  Taylor Series  Function of a Single Variable Taylor series expansion of a function f  x  about the point x = a is the inﬁnite series   x − a 2   cid:3  cid:3  cid:3   + f   x − a 3  + ···  f  x  = f  a  + f   cid:3    a  x − a  + f   cid:3  cid:3   2!   a    a    A1  In the special case a = 0 the series is also known as the MacLaurin series. It can be shown that Taylor series expansion is unique in the sense that no two functions have identical Taylor series. A Taylor series is meaningful only if all the derivatives of f  x  exist at x = a and the series converges. In general, convergence occurs only if x is sufﬁciently close to a; that is, if x − a ≤ ε, where ε is called the radius of convergence. In many cases ε is inﬁnite.  3!  Another useful form of a Taylor series is the expansion about an arbitrary value  of x:  f  x + h  = f  x  + f   cid:3    x h + f   cid:3  cid:3    x   h2 2!   cid:3  cid:3  cid:3   + f   x   h3 3!  + ···  Because it is not possible to evaluate all the terms of an inﬁnite series, the effect of truncating the series in Eq.  A2  is of great practical importance. Keeping the ﬁrst n + 1 terms, we have  f  x + h  = f  x  + f   cid:3    x h + f   cid:3  cid:3    x   + ··· + f  n  x   h2 2!  + En  hn n!  where En is the truncation error  sum of the truncated terms . The bounds on the truncation error are given by Taylor’s theorem, En = f  n+1  ξ    A4  where ξ is some point in the interval  x, x + h . Note that the expression for En is identical to the ﬁrst discarded term of the series, but with x replaced by ξ. Since the value of ξ is undetermined  only its limits are known , the most we can get out of Eq.  A4  are the upper and lower bounds on the truncation error.  hn+1  n + 1 !   A2    A3   407   408  Appendices  If the expression for f  n+1  ξ  is not available, the information conveyed by  Eq.  A4  is reduced to   A5  which is a concise way of saying that the truncation error is of the order of hn+1 or behaves as hn+1. If h is within the radius of convergence, then  En = O hn+1   O hn  > O hn+1   that is, the error is always reduced if a term is added to the truncated series  this may not be true for the ﬁrst few terms . In the special case n = 1, Taylor’s theorem is known as the mean value theorem:  f  x + h  = f  x  + f   cid:3    ξ h, x ≤ ξ ≤ x + h   A6   Function of Several Variables  If f is a function of the m variables x1, x2, . . . , xm, then its Taylor series expansion about the point x = [x1, x2, . . . , xm]T is  f  x + h  = f  x  + m cid:6    cid:19  cid:19  cid:19  cid:19   m cid:6   m cid:6    cid:19  cid:19  cid:19  cid:19   ∂f ∂xi  hi + 1 2!  x  i=1  i=1  j=1  ∂xi ∂xj  x  ∂ 2 f  hihj + ···   A7   This is sometimes written as  f  x + h  = f  x  + ∇ f  x  · h + 1 2   A8  The vector ∇ f is known as the gradient of f , and the matrix H is called the Hessian matrix of f .  hT H x h + . . .  EXAMPLE A1 Derive the Taylor series expansion of f  x  = ln x  about x = 1.  Solution. The derivatives of f are  f   cid:3    x  = 1 x  f   cid:3  cid:3    x  = − 1 x2   cid:3  cid:3  cid:3   f   x  = 2! x3  f  4  = − 3!  x4 etc.  Evaluating the derivatives at x = 1, we get   cid:3    1  = 1  f   cid:3  cid:3    1  = −1  f   cid:3  cid:3  cid:3    1  = 2!  f  f  4  1  = −3! etc.  which upon substitution into Eq.  A1 , together with a = 1, yields  x − 1 4   x − 1 3  ln x  = 0 +  x − 1  −  x − 1 2  x − 1 2 + 1 3  =  x − 1  − 1 2  2!  3!  + 2!  x − 1 3 − 1 4  + ···  − 3!  x − 1 4 + ···  4!   409  A1 Taylor Series  EXAMPLE A2 Use the ﬁrst ﬁve terms of the Taylor series expansion of ex about x = 0:  ex = 1 + x + x2 2!  + x3 3!  + x4 4!  + ···  together with the error estimate to ﬁnd the bounds of e.  Solution  e = 1 + 1 + 1 2  + 1 6  E4 = f  4  ξ   h5 5!  + E4  + 1 24 = eξ 5!  + E4 = 65 24 , 0 ≤ ξ ≤ 1  The bounds on the truncation error are = 1 120   E4 min = e0 5!  Thus the lower bound on e is   E4 max = e1 5!  = e 120  emin = 65 24  + 1 120  = 163 60  emax = 65 24  + emax 120  119 120  emax = 65 24  emax = 325 119  and the upper bound is given by  which yields  Therefore,  EXAMPLE A3 Compute the gradient and the Hessian matrix of  at the point x = −2, y = 1.  Solution  1 cid:25  x2 + y 2  =  ∂f ∂x  ,  1 2  163 60  ≤ e ≤ 325 119  f  x, y  = ln  x2 + y 2   cid:25   -  2x cid:25  x2 + y 2  cid:14   cid:14   =  x  x2 + y 2  y  x2 + y 2  ∂f ∂y  =  cid:15 T  ∇ f  x, y  = ∇ f  −2, 1  =  x  x2 + y 2   cid:15 T −0.4 0.2  y  x2 + y 2    410  Appendices  ∂ 2 f ∂x2 ∂ 2 f ∂y 2 ∂ 2 f ∂x∂y   x2 + y 2 2  =  x2 + y 2  − x 2x  = x2 − y 2  x2 + y 2 2  = −2xy  x2 + y 2 2  = ∂2 f ∂y∂x * *  H x, y  =  H −2, 1  =  −x2 + y 2 −2xy x2 − y 2 −2xy + −0.12 0.16 0.16 0.12  = −x2 + y 2  x2 + y 2 2  +  1   x2 + y 2 2  A2 Matrix Algebra  A matrix is a rectangular array of numbers. The size of a matrix is determined by the number of rows and columns, also called the dimensions of the matrix. Thus a matrix of m rows and n columns is said to have the size m × n  the number of rows is always listed ﬁrst . A particularly important matrix is the square matrix, which has the same number of rows and columns.  An array of numbers arranged in a single column is called a column vector, or simply a vector. If the numbers are set out in a row, the term row vector is used. Thus a column vector is a matrix of dimensions n × 1, and a row vector can be viewed as a matrix of dimensions 1 × n. ⎡ ⎢⎣ A 11 A 12 A 13  We denote matrices by boldfaced uppercase letters. For vectors we use boldface  lowercase letters. Here are examples of the notation:  ⎡ ⎢⎣ b1  ⎤ ⎥⎦  ⎤ ⎥⎦  b =  A =   A9   A 21 A 22 A 23 A 31 A 32 A 33  b2 b3  Indices of the elements of a matrix are displayed in the same order as its dimen- sions: The row number comes ﬁrst, followed by the column number. Only one index is needed for the elements of a vector.  Transpose  The transpose of a matrix A is denoted by AT and deﬁned as  = A ji  A T ij  The transpose operation thus interchanges the rows and columns of the matrix. If applied to vectors, it turns a column vector into a row vector and vice versa.   411  A2 Matrix Algebra  For example, transposing A and b in Eq.  A9 , we get  ⎡ ⎢⎣ A 11 A 21 A 31  A 12 A 22 A 32 A 13 A 23 A 33  ⎤ ⎥⎦  AT =   cid:14    cid:15   bT =  b1 b2 b3  An n × n matrix is said to be symmetric if AT = A. This means that the elements in the upper triangular portion  above the diagonal connecting A 11 and A nn  of a sym- metric matrix are mirrored in the lower triangular portion.  Addition The sum C = A + B of two m × n matrices A and B is deﬁned as  Cij = Aij + Bij ,  i = 1, 2, . . . , m;  j = 1, 2, . . . , n   A10   Thus the elements of C are obtained by adding elements of A to the elements of B. Note that addition is deﬁned only for matrices that have the same dimensions.  Vector Products The dot or inner product c = a · b of the vectors a and b, each of size m, is deﬁned as the scalar  It can also be written in the form c = aT b. In NumPy the function for the dot product is dot a,b  or inner a,b .  The outer product C = a ⊗ b is deﬁned as the matrix   A11   c = m cid:6   k=1  akbk  Cij = aib j  An alternative notation is C = abT . The NumPy function for the outer product is outer a,b .  Array Products The matrix product C = AB of an l × m matrix A and an m × n matrix B is deﬁned by  Aik Bkj ,  i = 1, 2, . . . , l;  j = 1, 2, . . . , n   A12   Cij = m cid:6   k=1  The deﬁnition requires the number of columns in A  the dimension m  to be equal to the number of rows in B. The matrix product can also be deﬁned in terms of   412  Appendices  the dot product. Representing the ith row of A as the vector ai and the j th column of B as the vector bj , we have  ⎡ ⎢⎢⎢⎢⎣  AB =  a1 · b1 a1 · b2 a2 · b1 a2 · b2  ...  a cid:12  · b1  a cid:12  · b2  ··· ··· ... ···  a1 · bn a2 · bn  ...  a cid:12  · bn  ⎤ ⎥⎥⎥⎥⎦   A13  still applies, but now b represents the j th row of B.  NumPy treats the matrix product as the dot product for arrays, so that the function dot A,B  returns the matrix product of A and B. NumPy deﬁnes the inner product of matrices A and B to be C = ABT . Equation NumPy’s deﬁnition of the outer product of matrices A  size k ×  cid:12   and B  size m × n  is as follows. Let ai be the ith row of A, and let bj represent the j th row of B. Then the outer product is of A and B is  ⎡ ⎢⎢⎢⎢⎣  A ⊗ B =  a1 ⊗ b1 a2 ⊗ b1  a1 ⊗ b2 a2 ⊗ b2  ...  ak ⊗ b1 ak ⊗ b2  ··· ··· ... ···  a1 ⊗ bm a2 ⊗ bm  ...  ak ⊗ bm  ⎤ ⎥⎥⎥⎥⎦  The submatrices ai ⊗ bj are of dimensions  cid:12  × n. As you can see, the size of the outer product is much larger than either A or B.  Identity Matrix  A square matrix of special importance is the identity or unit matrix:  ...  ...   A13    A14    A15   ⎡  ⎢⎢⎢⎢⎢⎢⎣  I =  1 0 0 ··· 0 1 0 ··· 0 0 1 ··· ... ... 0 0 0 0  ...  ...  ⎤  ⎥⎥⎥⎥⎥⎥⎦  0 0 0 ... 1  It has the property AI = IA = A.  Inverse The inverse of an n × n matrix A, denoted by A has the property  −1, is deﬁned to be an n × n matrix that  −1A = AA A  −1 = I   A16     A17    A18    A19   413  A2 Matrix Algebra  Determinant The determinant of a square matrix A is a scalar denoted by A or det A . There is no concise deﬁnition of the determinant for a matrix of arbitrary size. We start with the determinant of a 2 × 2 matrix, which is deﬁned as  A 21 A 22   cid:19  cid:19  cid:19  cid:19  cid:19  A 11 A 12  cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  = A 11  cid:19  cid:19  cid:19  cid:19  cid:19  A 22 A 23   cid:19  cid:19  cid:19  cid:19  cid:19  = A 11A 22 − A 12A 21  cid:19  cid:19  cid:19  cid:19  cid:19  A 21 A 23  cid:19  cid:19  cid:19  cid:19  cid:19  − A 12  A 32 A 33  A 31 A 33   cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  A 11 A 12 A 13  A 21 A 22 A 23 A 31 A 32 A 33  The determinant of a 3 × 3 matrix is then deﬁned as   cid:19  cid:19  cid:19  cid:19  cid:19  + A 13   cid:19  cid:19  cid:19  cid:19  cid:19  A 21 A 22  A 31 A 32   cid:19  cid:19  cid:19  cid:19  cid:19   Having established the pattern, we can now deﬁne the determinant of an n × n  matrix in terms of the determinant of an  n − 1  ×  n − 1  matrix:  A = n cid:6   k=1   −1 k+1A 1k M1k  A = n cid:6   k=1   −1 k+iAik Mik  where Mik is the determinant of the  n − 1  ×  n − 1  matrix obtained by deleting the ith row and kth column of A. The term  −1 k+i Mik is called a cofactor of Aik.  Equation  A18  is known as Laplace’s development of the determinant on the ﬁrst row of A. Actually Laplace’s development can take place on any convenient row. Choosing the ith row, we have  The matrix A is said to be singular if A = 0.  Positive Deﬁniteness An n × n matrix A is said to be positive deﬁnite if  xT Ax > 0   A20   for all nonvanishing vectors x. It can be shown that a matrix is positive deﬁnite if the determinants of all its leading minors are positive. The leading minors of A are the n square matrices  ⎡ ⎢⎢⎢⎢⎣  A 11 A 12 A 12 A 22 ... A k1 A k2  ...  ··· A 1k ··· A 2k ... ··· A kk  ...  ⎤ ⎥⎥⎥⎥⎦ , k = 1, 2, . . . , n   414  Appendices  Therefore, positive deﬁniteness requires that   cid:19  cid:19  cid:19  cid:19  cid:19  A 11 A 12  A 21 A 22   cid:19  cid:19  cid:19  cid:19  cid:19  > 0,   cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  A 11 A 12 A 13  A 21 A 22 A 23 A 31 A 32 A 33   cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  cid:19  > 0, . . . , A > 0  A 11 > 0,  Useful Theorems  We list without proof a few theorems that are used in the main body of the text. Most proofs are easy and could be attempted as exercises in matrix algebra.   A21    A22a    A22b    A22c    A22d    A22e    AB T = BT AT −1 −1A  AB   −1 = B   cid:19  cid:19 AT   cid:19  cid:19  = A  AB = AB if C = AT BA where B = BT , then C = CT  ⎤ ⎥⎦  ⎡ ⎢⎣ 1 2 3  1 2 1 0 1 2  A =  ⎤ ⎥⎦  ⎡ ⎢⎣ 1 6−2  u =  ⎤ ⎥⎦  ⎡ ⎢⎣ 8 0−3  v =  EXAMPLE A4 Letting  compute u + v, u · v, Av, and uT Av.  Solution  ⎤ ⎥⎦ =  ⎡ ⎢⎣ 1 + 8 6 + 0 −2 − 3  ⎤ ⎥⎦  ⎡ ⎢⎣ 9 6−5  u + v =  u · v = 1 8   + 6 0  +  −2  −3  = 14 ⎡ ⎤ ⎡ ⎢⎣ a1·v ⎥⎦ = ⎢⎣−1 a2·v 5−6 a3·v  ⎡ ⎢⎣ 1 8  + 2 0  + 3 −3  1 8  + 2 0  + 1 −3  0 8  + 1 0  + 2 −3   ⎤ ⎥⎦ =  ⎤ ⎥⎦  Av =  uT Av = u ·  Av  = 1 −1  + 6 5  +  −2  −6  = 41  EXAMPLE A5 Compute A, where A is given in Example A4. Is A positive deﬁnite?   415  A2 Matrix Algebra  Solution. Laplace’s development of the determinant on the ﬁrst row yields  Development of the third row is somewhat easier because of the presence of the zero element:  To verify positive deﬁniteness, we evaluate the determinants of the leading mi-  nors:   cid:19  cid:19  cid:19  cid:19  cid:19  2 1  1 2   cid:19  cid:19  cid:19  cid:19  cid:19  − 2   cid:19  cid:19  cid:19  cid:19  cid:19  1 1  0 2   cid:19  cid:19  cid:19  cid:19  cid:19  + 3   cid:19  cid:19  cid:19  cid:19  cid:19  1 2  0 1   cid:19  cid:19  cid:19  cid:19  cid:19   A = 1  = 1 3  − 2 2  + 3 1  = 2   cid:19  cid:19  cid:19  cid:19  cid:19  2 3  2 1   cid:19  cid:19  cid:19  cid:19  cid:19  − 1   cid:19  cid:19  cid:19  cid:19  cid:19  1 3  1 1   cid:19  cid:19  cid:19  cid:19  cid:19  + 2   cid:19  cid:19  cid:19  cid:19  cid:19  1 2  1 2   cid:19  cid:19  cid:19  cid:19  cid:19   A = 0  = 0 −4  − 1 −2  + 2 0  = 2  A 11 = 1 > 0 O.K.   cid:19  cid:19  cid:19  cid:19  cid:19  A 11 A 12  A 21 A 22   cid:19  cid:19  cid:19  cid:19  cid:19  =   cid:19  cid:19  cid:19  cid:19  cid:19  1 2  1 2   cid:19  cid:19  cid:19  cid:19  cid:19  = 0 Not O.K.  A is not positive deﬁnite.  EXAMPLE A6 Evaluate the matrix product AB, where A is given in Example A4 and  ⎤ ⎥⎦  ⎡ ⎢⎣−4 1 1 −4 2 −2  B =  Solution  ⎤ ⎥⎦  ⎡ ⎢⎣ a1·b1 a1·b2 a2·b1 a2·b2 ⎡ a3·b1 a3·b2 ⎢⎣ 1 −4  + 2 1  + 3 2  1 1  + 2 −4  + 3 −2  1 −4  + 2 1  + 1 2  1 1  + 2 −4  + 1 −2  0 −4  + 1 1  + 2 2  0 1  + 1 −4  + 2 −2   AB =  =  ⎤ ⎥⎦ =  ⎤ ⎥⎦  ⎡ ⎢⎣ 4 −13 0 −9 5 −8  EXAMPLE A7 Compute A ⊗ b, where  *  +  A =  5 −2 −3 4  *  +  b =  1 3   416  Appendices  Solution  +  * * *  a1 ⊗ b a2 ⊗ b + cid:14  + cid:14   5 −2 −3 4  A ⊗ b =  a1 ⊗ b =  a2 ⊗ b =  ∴ A ⊗ b =  + +  * *   cid:15   cid:15   =  =  1 3  1 3  ⎡ ⎢⎢⎢⎣  5 15 −2 −6 −3 −9 12 4  5 15 −2 −6 −3 −9 ⎤ 12 4 ⎥⎥⎥⎦   List of Program Modules  by Chapter   1.7  error  Error-handling routine  Chapter 1  Chapter 2  2.2 2.3 2.3 2.4 2.4 2.5 2.5 2.5 2.7 2.7  3.2 3.2 3.2 3.3 3.4 3.4  gaussElimin Gauss elimination LU decomposition LUdecomp Choleski decomposition choleski LU decomposition of tridiagonal matrices LUdecomp3 LU decomposition of pentadiagonal matrices LUdecomp5 Interchanges rows or columns of a matrix swap Gauss elimination with row pivoting gaussPivot LU decomposition with row pivoting LUpivot gaussSeidel Gauss-Seidel method with relaxation conjGrad  Conjugate gradient method  Chapter 3  Newton’s method of polynomial interpolation Neville’s method of polynomial interpolation Rational function interpolation  newtonPoly neville rational cubicSpline Cubic spline interpolation polyFit plotPoly  Polynomial curve ﬁtting Plots data points and the ﬁtting polynomial  Chapter 4  4.2 4.3  rootsearch Brackets a root of an equation bisection Method of bisection  417   418  List of Program Modules  by Chapter   Ridder’s method Newton-Raphson method  ridder newtonRaphson newtonRaphson2 Newton-Raphson method for systems of equations evalPoly polyRoots  Evaluates a polynomial and its derivatives Laguerre’s method for roots of polynomials  Chapter 6  Chapter 7  Chapter 8  4.4 4.5 4.6 4.7 4.7  6.2 6.3 6.4 6.4 6.5 6.5  7.2 7.2 7.3 7.5 7.6 7.6  8.2 8.2  8.2  8.2  8.2  8.3  8.3  8.4  trapezoid romberg gaussNodes gaussQuad gaussQuad2 triangleQuad Gauss-Legendre quadrature over a triangle  Recursive trapezoidal rule Romberg integration Nodes and weights for Gauss-Legendre quadrature Gauss-Legendre quadrature Gauss-Legendre quadrature over a quadrilateral  Euler method for solution of initial value problems Prints solution of initial value problems in tabular form 4th order Runge-Kutta method Adaptive  5th order  Runge-Kutta method  euler printSoln run kut4 run kut5 midpoint Midpoint method with Richardson extrapolation bulStoer  Simpliﬁed Bulirsch-Stoer method  linInterp example8 1  Linear interpolation Shooting method example for second-order  example8 3  Shooting method example for third-order linear  example8 4  Shooting method example for fourth-order  example8 5  Shooting method example for fourth-order  example8 6  Finite difference example for second-order linear  example8 7  Finite difference example for second-order  example8 8  Finite difference example for fourth-order linear  differential eqs.  diff. eqs.  differential eqs.  differential eqs.  diff. eqs.  differential. eqs.  diff. eqs.   419  List of Program Modules  by Chapter   Chapter 9  9.2 9.2 9.2 9.3 9.3 9.4 9.5 9.5 9.5 9.5 9.5  jacobi sortJacobi stdForm inversePower inversePower5 householder sturmSeq gerschgorin lamRange eigenvals3 inversePower3  Jacobi’s method Sorts eigenvectors in ascending order of eigenvalues Transforms eigenvalue problem into standard form Inverse power method with eigenvalue shifting Inverse power method for pentadiagonal matrices Householder reduction to tridiagonal form Sturm sequence for tridiagonal matrices Computes global bounds on eigenvalues Brackets m smallest eigenvalues of a 3-diag. matrix Finds m smallest eigenvalues of a tridiagonal matrix Inverse power method for tridiagonal matrices  Chapter 10  10.2 10.3 10.4  goldSearch Golden section search for the minimum of a function powell downhill  Powell’s method of minimization Downhill simplex method of minimization    Index  adaptive Runge–Kutta method, 271 arithmetic operators, in Python, 6 arrays, in Python, 20 augmented assignment operators, in Python, 7 augmented coefﬁcient matrix, 32  banded matrix, 59 bisection method, for equation root, 148 boundary value problems, 293  shooting method, 294 ﬁnite difference method, 307  Brent’s method, 183 Bulirsch–Stoer method, 280 bulStoer.py, 284  Euler’s method, 247 evalPoly.py, 175 evaluation of polynomials, 173  false position method, 152 ﬁnite difference approximations, 183 ﬁnite elements, 232 functions, in Python, 16  gaussElimin.py, 37 Gauss elimination method, 41 with scaled row pivoting, 71  Gaussian integration, 216  choleski.py, 50 Choleski’s decomposition, 48 cmath module, 19 comparison operators, in Python, 7 conditionals, in Python, 8 conjGrad.py, 91 conjugate gradient method, 89 continuation character, in Python, 6 cubic spline, 120  deﬂation of polynomials, 175 diagonal dominance, 70 docstring, in Python, 29 Doolittle’s decomposition, 45 Dormand-Prince coefﬁcients, 273 downhill simplex method, 392  eigenvals3.py, 364 eigenvalue problems, 322  eigenvalues of tridiagonal matrices, 359 Householder reduction, 351 inverse power method, 336 Jacobi method, 329 power method, 338  elementary operations, linear algebra, 34 equivalent linear equation, 34 error control, in Python, 15 euler.py, 248  abscissas weights for Gaussian quadratures,  221  orthogonal polynomials, 217  Gauss–Jordan elimination, 36 Gauss–Legendre quadrature over quadrilateral  element, 233  gaussNodes.py, 224 gaussPivot.py, 72 gaussQuad.py, 225 gaussQuad2.py, 235 gaussSeidel.py, 89 Gauss–Seidel method, 87 gerschgorin.py, 363 Gerschgorin’s theorem, 361 golden section search, 377 goldSearch.py, 378  householder.py, 356 Householder reduction to tridiagonal form, 351  accumulated transformation matrix, 355 Householder matrix, 352 Householder reduction of symmetric matrix,  352–359  Idle  Python code editor , 3 ill-conditioning, matrices, 33 incremental search method, roots of equations,  146  initial value problems, 246 integration order, 224  421   422  Index  interpolation curve ﬁtting, 104 interval halving method. See bisection method inversePower.py, 339 inverse power method, 336 inversePower3.py, 366  jacobi.py, 326–327 Jacobian matrix, 234 Jacobi method, 324 Jenkins–Traub algorithm, 182  knots of spline, 120  Lagrange’s method, of interpolation, 104 Laguerre’s method, for roots of polynomials,  176  lamRange.py, 363 least-squares ﬁt, 129 linear algebra module, in Python, 24 linear algebraic equations, 31 linear regression, 130 linear systems, 30 linInterp.py, 295 lists, in Python, 5 loops, in Python, 9 LR algorithm, 373 LUdecomp.py, 47 LUdecomp3.py, 61 LUdecomp5.py, 66 LU decomposition methods, 44 Choleski’s decomposition, 48 Doolittle’s decomposition, 45  LUpivot.py, 73  mathematical functions, in Python, 11 math module, 18 MATLAB, 2 matplotlib.pyplot module, 25 matrix algebra, 410 matrix inversion, 84 midpoint method, 280 midpoint.py, 282 minimization along line, 376  bracketing, 377 golden section search, 377  modules, in Python, 18 multiple integrals, 232  Gauss–Legendre quadrature over  quadrilateral element, 233  Gauss–Legendre quadrature over triangular  multistep methods, for initial value problems,  element, 239  292  namespace, in Python, 28 natural cubic spline, 120 Nelder–Mead method, 392 neville.py, 110 Neville’s method, 109  Newton–Cotes formulas, 200  Simpson’s rules, 204 trapezoidal rule, 200  newtonPoly.py, 108 newtonRaphson.py, 158 newtonRaphson2.py, 162 Newton–Raphson method, 156, 161 norm of matrix, 33 numpy module, 20 numerical instability, 260 numerical integration, 199  operators, in Python  arithmetic, 6 comparison, 7 optimization, 374 orthogonal polynomials, 217  pivoting, 69 plotPoly.py, 133 polyFit.py, 132 polynomial ﬁt, 131 polynomial interpolation, 104  Lagrange’s method, 104 Neville’s method, 109 Newton’s method, 106  polynomials, zeroes of, 173  deﬂation of polynomials, 175 evaluation of polynomials, 173 Laguerre’s method, 176  polyRoots.py, 177 powell.py, 385 Powell’s method, 382 printing, in Python, 12 printSoln.py, 249  QR algorithm, 373 quadrature. See numerical integration  rational function interpolation, 115 reading input, in Python, 11 relaxation factor, 88 Richardson extrapolation, 188, 281 Ridder’s method, 152 ridder.py, 153 romberg.py, 209 Romberg integration, 207 rootsearch.py, 147 roots of equations, 145  bisection, 148 incremental search, 146 Newton-Raphson method, 156, 161 Ridder’s method, 152  Runge–Kutta methods, 252 ﬁfth-order adaptive, 271 fourth-order, 254 second-order, 253  run kut4.py, 255 run kut5.py, 274   423  Index  scaled row pivoting, 71 shape functions, 234 shooting method, 294  higher-order equations, 299 second-order equation, 294  Shur’s factorization, 373 similarity transformation, 325 Simpson’s 1 3 rule, 204 Simpson’s 3 8 rule, 205 slicing operator, Python, 3 sortJacobi.py, 331 stability of Euler’s method, 268 stiffness, in initial value problems,  267–268  stdForm.py, 332 strings, in Python, 4 Sturm sequence, 359 sturmSeq.py, 359  swap.py, 72 symmetric banded coefﬁcient matrices, 59  symmetric coefﬁcient matrix, 62 symmetric pentadiagonal matrix, 63 tridiagonal matrix, 60  synthetic division, 175 trapezoid.py, 203 trapezoidal rule, 200 triangleQuad.py, 241 tridiagonal coefﬁcient matrix, 60 tuples, in Python, 5 two-point boundary value problems, 293  ﬁnite difference method, 307 shooting method, 294  type conversion, in Python, 10  weighted linear regression, 134 writing running programs, in Python, 29

@highlight

Numerical Methods in Engineering with Python is a text for engineering students and a reference for practicing engineers, especially those who wish to explore the power and efficiency of Python. Examples and applications were chosen for their relevance to real world problems, and where numerical solutions are most efficient. Numerical methods are discussed thoroughly and illustrated with problems involving both hand computation and programming. Computer code accompanies each method and is available on the book web site. This code is made simple and easy to understand by avoiding complex bookkeeping schemes, while maintaining the essential features of the method. Python was chosen as the example language because it is elegant, easy to learn and debug, and its facilities for handling arrays are unsurpassed. Moreover, it is an open-source software package; free and available to all students and engineers. Explore numerical methods with Python, a great language for teaching scientific computation