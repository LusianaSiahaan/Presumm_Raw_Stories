This page intentionally left blank   Mathematical Methods for Physics and Engineering  The third edition of this highly acclaimed undergraduate textbook is suitable for teaching all the mathematics ever likely to be needed for an undergraduate course in any of the physical sciences. As well as lucid descriptions of all the topics covered and many worked examples, it contains more than 800 exercises. A number of additional topics have been included and the text has undergone signiﬁcant reorganisation in some areas. New stand-alone chapters:    give a systematic account of the ‘special functions’ of physical science   cover an extended range of practical applications of complex variables including   provide an introduction to quantum operators.  WKB methods and saddle-point integration techniques  Further tabulations, of relevance in statistics and numerical integration, have been added. In this edition, all 400 odd-numbered exercises are provided with complete worked solutions in a separate manual, available to both students and their teachers; these are in addition to the hints and outline answers given in the main text. The even-numbered exercises have no hints, answers or worked solutions and can be used for unaided homework; full solutions to them are available to instructors on a password-protected website.  K e n R i l e y read mathematics at the University of Cambridge and proceeded to a Ph.D. there in theoretical and experimental nuclear physics. He became a research associate in elementary particle physics at Brookhaven, and then, having taken up a lectureship at the Cavendish Laboratory, Cambridge, continued this research at the Rutherford Laboratory and Stanford; in particular he was involved in the experimental discovery of a number of the early baryonic resonances. As well as having been Senior Tutor at Clare College, where he has taught physics and mathematics for over 40 years, he has served on many committees concerned with the teaching and examining of these subjects at all levels of tertiary and undergraduate education. He is also one of the authors of 200 Puzzling Physics Problems.  M i c h a e l H o b s o n read natural sciences at the University of Cambridge, spe- cialising in theoretical physics, and remained at the Cavendish Laboratory to complete a Ph.D. in the physics of star-formation. As a research fellow at Trinity Hall, Cambridge and subsequently an advanced fellow of the Particle Physics and Astronomy Research Council, he developed an interest in cosmology, and in particular in the study of ﬂuctuations in the cosmic microwave background. He was involved in the ﬁrst detection of these ﬂuctuations using a ground-based interferometer. He is currently a University Reader at the Cavendish Laboratory, his research interests include both theoretical and observational aspects of cos- mology, and he is the principal author of General Relativity: An Introduction for   Physicists. He is also a Director of Studies in Natural Sciences at Trinity Hall and enjoys an active role in the teaching of undergraduate physics and mathematics.  S t e p h e n B e n c e obtained both his undergraduate degree in Natural Sciences and his Ph.D. in Astrophysics from the University of Cambridge. He then became a Research Associate with a special interest in star-formation processes and the structure of star-forming regions. In particular, his research concentrated on the physics of jets and outﬂows from young stars. He has had considerable experi- ence of teaching mathematics and physics to undergraduate and pre-universtiy students.  ii   Mathematical Methods  for Physics and Engineering  Third Edition  K. F. RILEY, M. P. HOBSON and S. J. BENCE   cambridge university press Cambridge, New York, Melbourne, Madrid, Cape Town, Singapore, São Paulo  Cambridge University Press The Edinburgh Building, Cambridge cb2 2ru, UK Published in the United States of America by Cambridge University Press, New York www.cambridge.org Informationo nthi stitle :www.cambri dge.org 9780521861533    K. F. Riley, M. P. Hobson and S. J. Bence 2006  This publication is in copyright. Subject to statutory exception and to the provision of relevant collective licensing agreements, no reproduction of any part may take place without the written permission of Cambridge University Press.  First published in print format  2006  isbn-13 isbn-10  978-0-511-16842-0 0-511-16842-x  eBook  EBL   eBook  EBL   isbn-13 isbn-10  978-0-521-86153-3 0-521-86153-5  hardback  hardback  isbn-13 isbn-10  978-0-521-67971-8 0-521-67971-0  paperback  paperback  Cambridge University Press has no responsibility for the persistence or accuracy of urls for external or third-party internet websites referred to in this publication, and does not guarantee that any content on such websites is, or will remain, accurate or appropriate.   Contents  Preface to the third edition Preface to the second edition Preface to the ﬁrst edition  1 1.1  1.2  1.3 1.4  1.5 1.6 1.7  Preliminary algebra Simple functions and equations Polynomial equations; factorisation; properties of roots Trigonometric identities Single angle; compound angles; double- and half-angle identities Coordinate geometry Partial fractions Complications and special cases Binomial expansion Properties of binomial coeﬃcients Some particular methods of proof Proof by induction; proof by contradiction; necessary and suﬃcient conditions Exercises  1.8 1.9 Hints and answers  Preliminary calculus  2 2.1 Diﬀerentiation  Diﬀerentiation from ﬁrst principles; products; the chain rule; quotients; implicit diﬀerentiation; logarithmic diﬀerentiation; Leibnitz’ theorem; special points of a function; curvature; theorems of diﬀerentiation  v  page xx xxiii xxv  1 1  10  15 18  25 27 30  36 39  41 41   CONTENTS  2.2  3.3  3.4  4 4.1 4.2  4.3  4.4 4.5  4.6  Integration Integration from ﬁrst principles; the inverse of diﬀerentiation; by inspec- tion; sinusoidal functions; logarithmic integration; using partial fractions; substitution method; integration by parts; reduction formulae; inﬁnite and improper integrals; plane polar coordinates; integral inequalities; applications of integration Exercises  2.3 2.4 Hints and answers  Complex numbers and hyperbolic functions The need for complex numbers  3 3.1 3.2 Manipulation of complex numbers  Addition and subtraction; modulus and argument; multiplication; complex conjugate; division Polar representation of complex numbers Multiplication and division in polar form de Moivre’s theorem trigonometric identities; ﬁnding the nth roots of unity; solving polynomial equations Complex logarithms and complex powers Applications to diﬀerentiation and integration  3.5 3.6 3.7 Hyperbolic functions  Deﬁnitions; hyperbolic–trigonometric analogies; identities of hyperbolic functions; solving hyperbolic equations; inverses of hyperbolic functions; calculus of hyperbolic functions Exercises  3.8 3.9 Hints and answers  Series and limits Series Summation of series Arithmetic series; geometric series; arithmetico-geometric series; the diﬀerence method; series involving natural numbers; transformation of series Convergence of inﬁnite series Absolute and conditional convergence; series containing only real positive terms; alternating series test Operations with series Power series Convergence of power series; operations with power series Taylor series Taylor’s theorem; approximation errors; standard Maclaurin series Evaluation of limits Exercises  4.7 4.8 4.9 Hints and answers  59  76 81  83 83 85  92  95  99 101 102  109 113  115 115 116  124  131 131  136  141 144 149  vi   CONTENTS  5  Partial diﬀerentiation  5.1 Deﬁnition of the partial derivative  The total diﬀerential and total derivative  Exact and inexact diﬀerentials  5.4 Useful theorems of partial diﬀerentiation  The chain rule  Change of variables  Taylor’s theorem for many-variable functions  Stationary values of many-variable functions  Stationary values under constraints  5.2  5.3  5.5  5.6  5.7  5.8  5.9  5.10 Envelopes  5.11 Thermodynamic relations  5.12 Diﬀerentiation of integrals  5.13 Exercises  5.14 Hints and answers  6  Multiple integrals  6.1 Double integrals  Triple integrals  6.2  6.3  Applications of multiple integrals  Areas and volumes; masses, centres of mass and centroids; Pappus’ theorems; moments of inertia; mean values of functions  6.4  Change of variables in multiple integrals  Change of variables in double integrals; evaluation of the integral I = −x2 −∞ e dx; change of variables in triple integrals; general properties of Jacobians   cid:1  ∞  6.5  Exercises  6.6 Hints and answers  Vector algebra  Scalars and vectors  7  7.1  7.2  Addition and subtraction of vectors  7.3 Multiplication by a scalar  7.4  Basis vectors and components  7.5 Magnitude of a vector  7.6 Multiplication of vectors  Scalar product; vector product; scalar triple product; vector triple product  vii  151  151  153  155  157  157  158  160  162  167  173  176  178  179  185  187  187  190  191  199  207  211  212  212  213  214  217  218  219   CONTENTS  Equations of lines, planes and spheres  7.7 7.8 Using vectors to ﬁnd distances  Point to line; point to plane; line to line; line to plane Reciprocal vectors  7.9 7.10 Exercises 7.11 Hints and answers  8 8.1  Matrices and vector spaces Vector spaces Basis vectors; inner product; some useful inequalities Linear operators  8.2 8.3 Matrices 8.4  8.5 8.6 8.7 8.8 8.9  Basic matrix algebra Matrix addition; multiplication by a scalar; matrix multiplication Functions of matrices The transpose of a matrix The complex and Hermitian conjugates of a matrix The trace of a matrix The determinant of a matrix Properties of determinants 8.10 The inverse of a matrix 8.11 The rank of a matrix 8.12 Special types of square matrix  Diagonal; triangular; symmetric and antisymmetric; orthogonal; Hermitian and anti-Hermitian; unitary; normal  8.13 Eigenvectors and eigenvalues  Of a normal matrix; of Hermitian and anti-Hermitian matrices; of a unitary matrix; of a general square matrix  8.14 Determination of eigenvalues and eigenvectors  Degenerate eigenvalues  8.15 Change of basis and similarity transformations 8.16 Diagonalisation of matrices 8.17 Quadratic and Hermitian forms  Stationary properties of the eigenvectors; quadratic surfaces  8.18 Simultaneous linear equations  Range; null space; N simultaneous linear equations in N unknowns; singular value decomposition  8.19 Exercises 8.20 Hints and answers  9 9.1 9.2  Normal modes Typical oscillatory systems Symmetry and normal modes  viii  226 229  233 234 240  241 242  247 249 250  255 255 256 258 259  263 267 268  272  280  282 285 288  292  307 314  316 317 322   CONTENTS  Rayleigh–Ritz method Exercises  9.3 9.4 9.5 Hints and answers  Vector calculus  10 10.1 Diﬀerentiation of vectors  Composite vector expressions; diﬀerential of a vector Integration of vectors  10.2 10.3 Space curves 10.4 Vector functions of several arguments 10.5 Surfaces 10.6 Scalar and vector ﬁelds 10.7 Vector operators  Gradient of a scalar ﬁeld; divergence of a vector ﬁeld; curl of a vector ﬁeld  10.8 Vector operator formulae  Vector operators acting on sums and products; combinations of grad, div and curl  Evaluating line integrals; physical examples; line integrals with respect to a scalar  10.9 Cylindrical and spherical polar coordinates 10.10 General curvilinear coordinates 10.11 Exercises 10.12 Hints and answers  Line, surface and volume integrals  11 11.1 Line integrals  11.2 Connectivity of regions 11.3 Green’s theorem in a plane 11.4 Conservative ﬁelds and potentials 11.5 Surface integrals  11.6 Volume integrals  Volumes of three-dimensional regions Integral forms for grad, div and curl  11.7 11.8 Divergence theorem and related theorems  11.9 Stokes’ theorem and related theorems  Related integral theorems; physical applications  11.10 Exercises 11.11 Hints and answers  Fourier series  12 12.1 The Dirichlet conditions  ix  Evaluating surface integrals; vector areas of surfaces; physical examples  Green’s theorems; other related integral theorems; physical applications  327 329 332  334 334  339 340 344 345 347 347  354  357 364 369 375  377 377  383 384 387 389  396  398 401  406  409 414  415 415   CONTENTS  12.2 The Fourier coeﬃcients 12.3 Symmetry considerations 12.4 Discontinuous functions 12.5 Non-periodic functions 12.6 12.7 Complex Fourier series 12.8 Parseval’s theorem 12.9 Exercises 12.10 Hints and answers  Integration and diﬀerentiation  13 Integral transforms 13.1 Fourier transforms  The uncertainty principle; Fraunhofer diﬀraction; the Dirac δ-function; relation of the δ-function to Fourier transforms; properties of Fourier transforms; odd and even functions; convolution and deconvolution; correlation functions and energy spectra; Parseval’s theorem; Fourier transforms in higher dimensions  Laplace transforms of derivatives and integrals; other properties of Laplace transforms  13.2 Laplace transforms  13.3 Concluding remarks 13.4 Exercises 13.5 Hints and answers  First-order ordinary diﬀerential equations  14 14.1 General form of solution 14.2 First-degree ﬁrst-order equations  Separable-variable equations; exact equations; inexact equations, integrat- ing factors; linear equations; homogeneous equations; isobaric equations; Bernoulli’s equation; miscellaneous equations  14.3 Higher-degree ﬁrst-order equations  Equations soluble for p; for x; for y; Clairaut’s equation  14.4 Exercises 14.5 Hints and answers  15 Higher-order ordinary diﬀerential equations 15.1 Linear equations with constant coeﬃcients  Finding the complementary function yc x ; ﬁnding the particular integral yp x ; constructing the general solution yc x  + yp x ; linear recurrence relations; Laplace transform method  15.2 Linear equations with variable coeﬃcients  The Legendre and Euler linear equations; exact equations; partially known complementary function; variation of parameters; Green’s functions; canonical form for second-order equations  x  417 419 420 422 424 424 426 427 431  433 433  453  459 460 466  468 469 470  480  484 488  490 492  503   CONTENTS  15.3 General ordinary diﬀerential equations  Dependent variable absent; independent variable absent; non-linear exact equations; isobaric or homogeneous equations; equations homogeneous in x or y alone; equations having y = Aex as a solution  15.4 Exercises 15.5 Hints and answers  Series solutions of ordinary diﬀerential equations  16 16.1 Second-order linear ordinary diﬀerential equations  Ordinary and singular points  16.2 Series solutions about an ordinary point 16.3 Series solutions about a regular singular point  Distinct roots not diﬀering by an integer; repeated root of the indicial equation; distinct roots diﬀering by an integer  16.4 Obtaining a second solution  The Wronskian method; the derivative method; series form of the second solution  16.5 Polynomial solutions 16.6 Exercises 16.7 Hints and answers  Eigenfunction methods for diﬀerential equations  17 17.1 Sets of functions  Some useful inequalities  17.2 Adjoint, self-adjoint and Hermitian operators 17.3 Properties of Hermitian operators  Reality of the eigenvalues; orthogonality of the eigenfunctions; construction of real eigenfunctions  17.4 Sturm–Liouville equations  Valid boundary conditions; putting an equation into Sturm–Liouville form  17.5 Superposition of eigenfunctions: Green’s functions 17.6 A useful generalisation 17.7 Exercises 17.8 Hints and answers  General solution for integer  cid:2 ; properties of Legendre polynomials  Special functions  18 18.1 Legendre functions  18.2 Associated Legendre functions 18.3 Spherical harmonics 18.4 Chebyshev functions 18.5 Bessel functions  General solution for non-integer ν; general solution for integer ν; properties of Bessel functions  xi  518  523 529  531 531  535 538  544  548 550 553  554 556  559 561  564  569 572 573 576  577 577  587 593 595 602   CONTENTS  18.6 Spherical Bessel functions 18.7 Laguerre functions 18.8 Associated Laguerre functions 18.9 Hermite functions 18.10 Hypergeometric functions 18.11 Conﬂuent hypergeometric functions 18.12 The gamma function and related functions 18.13 Exercises 18.14 Hints and answers  19 Quantum operators 19.1 Operator formalism  Commutators  19.2 Physical examples of operators  19.3 Exercises 19.4 Hints and answers  Uncertainty principle; angular momentum; creation and annihilation operators  20 20.1  Partial diﬀerential equations: general and particular solutions Important partial diﬀerential equations The wave equation; the diﬀusion equation; Laplace’s equation; Poisson’s equation; Schr¨odinger’s equation  20.2 General form of solution 20.3 General and particular solutions  First-order equations; inhomogeneous equations and problems; second-order equations  20.4 The wave equation 20.5 The diﬀusion equation 20.6 Characteristics and the existence of solutions  First-order equations; second-order equations  20.7 Uniqueness of solutions 20.8 Exercises 20.9 Hints and answers  21  Partial diﬀerential equations: separation of variables and other methods  21.1 Separation of variables: the general method 21.2 Superposition of separated solutions 21.3 Separation of variables in polar coordinates  Laplace’s equation in polar coordinates; spherical harmonics; other equations in polar coordinates; solution by expansion; separation of variables for inhomogeneous equations Integral transform methods  21.4  xii  614 616 621 624 628 633 635 640 646  648 648  656  671 674  675 676  680 681  693 695 699  705 707 711  713 713 717 725  747   CONTENTS  21.5  Inhomogeneous problems – Green’s functions Similarities to Green’s functions for ordinary diﬀerential equations; general boundary-value problems; Dirichlet problems; Neumann problems  21.6 Exercises 21.7 Hints and answers  Calculus of variations  22 22.1 The Euler–Lagrange equation 22.2 Special cases  F does not contain y explicitly; F does not contain x explicitly  22.3 Some extensions  Several dependent variables; several independent variables; higher-order derivatives; variable end-points  22.4 Constrained variation 22.5 Physical variational principles  Fermat’s principle in optics; Hamilton’s principle in mechanics  22.6 General eigenvalue problems 22.7 Estimation of eigenvalues and eigenfunctions 22.8 Adjustment of parameters 22.9 Exercises 22.10 Hints and answers  Integral equations  23 23.1 Obtaining an integral equation from a diﬀerential equation 23.2 Types of integral equation 23.3 Operator notation and the existence of solutions 23.4 Closed-form solutions  Separable kernels; integral transform methods; diﬀerentiation  23.5 Neumann series 23.6 Fredholm theory 23.7 Schmidt–Hilbert theory 23.8 Exercises 23.9 Hints and answers  Complex variables  24 24.1 Functions of a complex variable 24.2 The Cauchy–Riemann relations 24.3 Power series in a complex variable 24.4 Some elementary functions 24.5 Multivalued functions and branch cuts 24.6 Singularities and zeros of complex functions 24.7 Conformal transformations 24.8 Complex integrals  xiii  751  767 773  775 776 777  781  785 787  790 792 795 797 801  803 803 804 805 806  813 815 816 819 823  824 825 827 830 832 835 837 839 845   CONTENTS  24.9 Cauchy’s theorem 24.10 Cauchy’s integral formula 24.11 Taylor and Laurent series 24.12 Residue theorem 24.13 Deﬁnite integrals using contour integration 24.14 Exercises 24.15 Hints and answers  Applications of complex variables  25 25.1 Complex potentials 25.2 Applications of conformal transformations 25.3 Location of zeros 25.4 Summation of series 25.5 25.6 Stokes’ equation and Airy integrals 25.7 WKB methods 25.8 Approximations to integrals  Inverse Laplace transform  Level lines and saddle points; steepest descents; stationary phase  25.9 Exercises 25.10 Hints and answers  Tensors  Isotropic tensors  26 26.1 Some notation 26.2 Change of basis 26.3 Cartesian tensors 26.4 First- and zero-order Cartesian tensors 26.5 Second- and higher-order Cartesian tensors 26.6 The algebra of tensors 26.7 The quotient law 26.8 The tensors δij and  cid:4 ijk 26.9 26.10 Improper rotations and pseudotensors 26.11 Dual tensors 26.12 Physical applications of tensors 26.13 Integral theorems for tensors 26.14 Non-Cartesian coordinates 26.15 The metric tensor 26.16 General coordinate transformations and tensors 26.17 Relative tensors 26.18 Derivatives of basis vectors and Christoﬀel symbols 26.19 Covariant diﬀerentiation 26.20 Vector operators in tensor form  xiv  849 851 853 858 861 867 870  871 871 876 879 882 884 888 895 905  920 925  927 928 929 930 932 935 938 939 941 944 946 949 950 954 955 957 960 963 965 968 971   CONTENTS  26.21 Absolute derivatives along curves 26.22 Geodesics 26.23 Exercises 26.24 Hints and answers  Numerical methods  27 27.1 Algebraic and transcendental equations  Rearrangement of the equation; linear interpolation; binary chopping; Newton–Raphson method  27.2 Convergence of iteration schemes 27.3 Simultaneous linear equations  Gaussian elimination; Gauss–Seidel iteration; tridiagonal matrices  Trapezium rule; Simpson’s rule; Gaussian integration; Monte Carlo methods  27.4 Numerical integration  27.5 Finite diﬀerences 27.6 Diﬀerential equations  Diﬀerence equations; Taylor series solutions; prediction and correction; Runge–Kutta methods; isoclines  27.7 Higher-order equations 27.8 Partial diﬀerential equations 27.9 Exercises 27.10 Hints and answers  Group theory  28 28.1 Groups  Deﬁnition of a group; examples of groups  28.2 Finite groups 28.3 Non-Abelian groups 28.4 Permutation groups 28.5 Mappings between groups 28.6 Subgroups 28.7 Subdividing a group  Equivalence relations and classes; congruence and cosets; conjugates and classes  28.8 Exercises 28.9 Hints and answers  Representation theory  29 29.1 Dipole moments of molecules 29.2 Choosing an appropriate formalism 29.3 Equivalent representations 29.4 Reducibility of a representation 29.5 The orthogonality theorem for irreducible representations  xv  975 976 977 982  984 985  992 994  1000  1019 1020  1028 1030 1033 1039  1041 1041  1049 1052 1056 1059 1061 1063  1070 1074  1076 1077 1078 1084 1086 1090   CONTENTS  29.6 Characters  Orthogonality property of characters 29.7 Counting irreps using characters  Summation rules for irreps  29.8 Construction of a character table 29.9 Group nomenclature 29.10 Product representations 29.11 Physical applications of group theory  29.12 Exercises 29.13 Hints and answers  Probability  30 30.1 Venn diagrams 30.2 Probability  Bonding in molecules; matrix elements in quantum mechanics; degeneracy of normal modes; breaking of degeneracies  Axioms and theorems; conditional probability; Bayes’ theorem  30.3 Permutations and combinations 30.4 Random variables and distributions  Discrete random variables; continuous random variables  30.5 Properties of distributions  Mean; mode and median; variance and standard deviation; moments; central moments  30.6 Functions of random variables 30.7 Generating functions  30.8  30.9  Probability generating functions; moment generating functions; characteristic functions; cumulant generating functions Important discrete distributions Binomial; geometric; negative binomial; hypergeometric; Poisson Important continuous distributions Gaussian; log-normal; exponential; gamma; chi-squared; Cauchy; Breit– Wigner; uniform  30.10 The central limit theorem 30.11 Joint distributions  Discrete bivariate; continuous bivariate; marginal and conditional distributions  30.12 Properties of joint distributions  Means; variances; covariance and correlation  30.13 Generating functions for joint distributions 30.14 Transformation of variables in joint distributions 30.15 Important joint distributions  Multinominal; multivariate Gaussian  30.16 Exercises 30.17 Hints and answers  xvi  1092  1095  1100 1102 1103 1105  1113 1117  1119 1119 1124  1133 1139  1143  1150 1157  1168  1179  1195 1196  1199  1205 1206 1207  1211 1219   CONTENTS  Statistics  31 31.1 Experiments, samples and populations 31.2 Sample statistics  Averages; variance and standard deviation; moments; covariance and correla- tion  31.3 Estimators and sampling distributions  Consistency, bias and eﬃciency; Fisher’s inequality; standard errors; conﬁ- dence limits  31.4 Some basic estimators  31.5 Maximum-likelihood method  Mean; variance; standard deviation; moments; covariance and correlation  ML estimator; transformation invariance and bias; eﬃciency; errors and conﬁdence limits; Bayesian interpretation; large-N behaviour; extended ML method  31.6 The method of least squares  Linear least squares; non-linear least squares  31.7 Hypothesis testing  Simple and composite hypotheses; statistical tests; Neyman–Pearson; gener- alised likelihood-ratio; Student’s t; Fisher’s F; goodness of ﬁt  31.8 Exercises 31.9 Hints and answers  Index  1221 1221 1222  1229  1243  1255  1271  1277  1298 1303  1305  xvii    CONTENTS  I am the very Model for a Student Mathematical  I am the very model for a student mathematical; I’ve information rational, and logical and practical. I know the laws of algebra, and ﬁnd them quite symmetrical, And even know the meaning of ‘a variate antithetical’.  I’m extremely well acquainted, with all things mathematical. I understand equations, both the simple and quadratical. About binomial theorems I’m teeming with a lot o’news, With many cheerful facts about the square of the hypotenuse.  I’m very good at integral and diﬀerential calculus, And solving paradoxes that so often seem to rankle us. In short in matters rational, and logical and practical, I am the very model for a student mathematical.  I know the singularities of equations diﬀerential, And some of these are regular, but the rest are quite essential. I quote the results of giants; with Euler, Newton, Gauss, Laplace, And can calculate an orbit, given a centre, force and mass.  I can reconstruct equations, both canonical and formal, And write all kinds of matrices, orthogonal, real and normal. I show how to tackle problems that one has never met before, By analogy or example, or with some clever metaphor.  I seldom use equivalence to help decide upon a class, But often ﬁnd an integral, using a contour o’er a pass. In short in matters rational, and logical and practical, I am the very model for a student mathematical.  When you have learnt just what is meant by ‘Jacobian’ and ‘Abelian’; When you at sight can estimate, for the modal, mean and median; When describing normal subgroups is much more than recitation; When you understand precisely what is ‘quantum excitation’;  When you know enough statistics that you can recognise RV; When you have learnt all advances that have been made in SVD; And when you can spot the transform that solves some tricky PDE, You will feel no better student has ever sat for a degree.  Your accumulated knowledge, whilst extensive and exemplary, Will have only been brought down to the beginning of last century, But still in matters rational, and logical and practical, You’ll be the very model of a student mathematical.  KFR, with apologies to W.S. Gilbert  xix   Preface to the third edition  As is natural, in the four years since the publication of the second edition of this book we have somewhat modiﬁed our views on what should be included and how it should be presented. In this new edition, although the range of topics covered has been extended, there has been no signiﬁcant shift in the general level of diﬃculty or in the degree of mathematical sophistication required. Further, we have aimed to preserve the same style of presentation as seems to have been well received in the ﬁrst two editions. However, a signiﬁcant change has been made to the format of the chapters, speciﬁcally to the way that the exercises, together with their hints and answers, have been treated; the details of the change are explained below.  The two major chapters that are new in this third edition are those dealing with ‘special functions’ and the applications of complex variables. The former presents a systematic account of those functions that appear to have arisen in a more or less haphazard way as a result of studying particular physical situations, and are deemed ‘special’ for that reason. The treatment presented here shows that, in fact, they are nearly all particular cases of the hypergeometric or conﬂuent hypergeometric functions, and are special only in the sense that the parameters of the relevant function take simple or related values.  The second new chapter describes how the properties of complex variables can be used to tackle problems arising from the description of physical situations or from other seemingly unrelated areas of mathematics. To topics treated in earlier editions, such as the solution of Laplace’s equation in two dimensions, the summation of series, the location of zeros of polynomials and the calculation of inverse Laplace transforms, has been added new material covering Airy integrals, saddle-point methods for contour integral evaluation, and the WKB approach to asymptotic forms.  Other new material includes a stand-alone chapter on the use of coordinate-free operators to establish valuable results in the ﬁeld of quantum mechanics; amongst  xx   PREFACE TO THE THIRD EDITION  the physical topics covered are angular momentum and uncertainty principles. There are also signiﬁcant additions to the treatment of numerical integration. In particular, Gaussian quadrature based on Legendre, Laguerre, Hermite and Chebyshev polynomials is discussed, and appropriate tables of points and weights are provided.  We now turn to the most obvious change to the format of the book, namely the way that the exercises, hints and answers are treated. The second edition of Mathematical Methods for Physics and Engineering carried more than twice as many exercises, based on its various chapters, as did the ﬁrst. In its preface we discussed the general question of how such exercises should be treated but, in the end, decided to provide hints and outline answers to all problems, as in the ﬁrst edition. This decision was an uneasy one as, on the one hand, it did not allow the exercises to be set as totally unaided homework that could be used for assessment purposes but, on the other, it did not give a full explanation of how to tackle a problem when a student needed explicit guidance or a model answer. In order to allow both of these educationally desirable goals to be achieved, we have, in this third edition, completely changed the way in which this matter is handled. A large number of exercises have been included in the penultimate subsections of the appropriate, sometimes reorganised, chapters. Hints and outline answers are given, as previously, in the ﬁnal subsections, but only for the odd- numbered exercises. This leaves all even-numbered exercises free to be set as unaided homework, as described below.  For the four hundred plus odd-numbered exercises, complete solutions are available, to both students and their teachers, in the form of a separate manual, Student Solutions Manual for Mathematical Methods for Physics and Engineering  Cambridge: Cambridge University Press, 2006 ; the hints and outline answers given in this main text are brief summaries of the model answers given in the manual. There, each original exercise is reproduced and followed by a fully worked solution. For those original exercises that make internal reference to this text or to other  even-numbered  exercises not included in the solutions manual, the questions have been reworded, usually by including additional information, so that the questions can stand alone.  In many cases, the solution given in the manual is even fuller than one that might be expected of a good student that has understood the material. This is because we have aimed to make the solutions instructional as well as utilitarian. To this end, we have included comments that are intended to show how the plan for the solution is fomulated and have given the justiﬁcations for particular intermediate steps  something not always done, even by the best of students . We have also tried to write each individual substituted formula in the form that best indicates how it was obtained, before simplifying it at the next or a subsequent stage. Where several lines of algebraic manipulation or calculus are needed to obtain a ﬁnal result, they are normally included in full; this should enable the  xxi   PREFACE TO THE THIRD EDITION  student to determine whether an incorrect answer is due to a misunderstanding of principles or to a technical error.  The remaining four hundred or so even-numbered exercises have no hints or answers, outlined or detailed, available for general access. They can therefore be used by instructors as a basis for setting unaided homework. Full solutions to these exercises, in the same general format as those appearing in the manual  though they may contain references to the main text or to other exercises , are available without charge to accredited teachers as downloadable pdf ﬁles on the password-protected website http:  www.cambridge.org 9780521679718. Teachers wishing to have access to the website should contact solutions@cambridge.org for registration details.  In all new publications, errors and typographical mistakes are virtually un- avoidable, and we would be grateful to any reader who brings instances to our attention. Retrospectively, we would like to record our thanks to Reinhard Gerndt, Paul Renteln and Joe Tenn for making us aware of some errors in the second edition. Finally, we are extremely grateful to Dave Green for his considerable and continuing advice concerning LATEX.  Ken Riley, Michael Hobson, Cambridge, 2006  xxii   Preface to the second edition  Since the publication of the ﬁrst edition of this book, both through teaching the material it covers and as a result of receiving helpful comments from colleagues, we have become aware of the desirability of changes in a number of areas. The most important of these is that the mathematical preparation of current senior college and university entrants is now less thorough than it used to be. To match this, we decided to include a preliminary chapter covering areas such as polynomial equations, trigonometric identities, coordinate geometry, partial fractions, binomial expansions, necessary and suﬃcient condition and proof by induction and contradiction.  Whilst the general level of what is included in this second edition has not been raised, some areas have been expanded to take in topics we now feel were not adequately covered in the ﬁrst. In particular, increased attention has been given to non-square sets of simultaneous linear equations and their associated matrices. We hope that this more extended treatment, together with the inclusion of singular value matrix decomposition, will make the material of more practical use to engineering students. In the same spirit, an elementary treatment of linear recurrence relations has been included. The topic of normal modes has been given a small chapter of its own, though the links to matrices on the one hand, and to representation theory on the other, have not been lost.  Elsewhere, the presentation of probability and statistics has been reorganised to give the two aspects more nearly equal weights. The early part of the probability chapter has been rewritten in order to present a more coherent development based on Boolean algebra, the fundamental axioms of probability theory and the properties of intersections and unions. Whilst this is somewhat more formal than previously, we think that it has not reduced the accessibility of these topics and hope that it has increased it. The scope of the chapter has been somewhat extended to include all physically important distributions and an introduction to cumulants.  xxiii   PREFACE TO THE SECOND EDITION  Statistics now occupies a substantial chapter of its own, one that includes sys- tematic discussions of estimators and their eﬃciency, sample distributions and t- and F-tests for comparing means and variances. Other new topics are applications of the chi-squared distribution, maximum-likelihood parameter estimation and least-squares ﬁtting. In other chapters we have added material on the following topics: curvature, envelopes, curve-sketching, more reﬁned numerical methods for diﬀerential equations and the elements of integration using Monte Carlo techniques.  Over the last four years we have received somewhat mixed feedback about the number of exercises at the ends of the various chapters. After consideration, we decided to increase the number substantially, partly to correspond to the additional topics covered in the text but mainly to give both students and their teachers a wider choice. There are now nearly 800 such exercises, many with several parts. An even more vexed question has been whether to provide hints and answers to all the exercises or just to ‘the odd-numbered’ ones, as is the normal practice for textbooks in the United States, thus making the remainder more suitable for setting as homework. In the end, we decided that hints and outline solutions should be provided for all the exercises, in order to facilitate independent study while leaving the details of the calculation as a task for the student.  In conclusion, we hope that this edition will be thought by its users to be ‘heading in the right direction’ and would like to place on record our thanks to all who have helped to bring about the changes and adjustments. Naturally, those colleagues who have noted errors or ambiguities in the ﬁrst edition and brought them to our attention ﬁgure high on the list, as do the staﬀ at The Cambridge University Press. In particular, we are grateful to Dave Green for continued LATEX advice, Susan Parkinson for copy-editing the second edition with her usual keen eye for detail and ﬂair for crafting coherent prose and Alison Woollatt for once again turning our basic LATEX into a beautifully typeset book. Our thanks go to all of them, though of course we accept full responsibility for any remaining errors or ambiguities, of which, as with any new publication, there are bound to be some.  On a more personal note, KFR again wishes to thank his wife Penny for her unwavering support, not only in his academic and tutorial work, but also in their joint eﬀorts to convert time at the bridge table into ‘green points’ on their record. MPH is once more indebted to his wife, Becky, and his mother, Pat, for their tireless support and encouragement above and beyond the call of duty. MPH dedicates his contribution to this book to the memory of his father, Ronald Leonard Hobson, whose gentle kindness, patient understanding and unbreakable spirit made all things seem possible.  Ken Riley, Michael Hobson Cambridge, 2002  xxiv   Preface to the ﬁrst edition  A knowledge of mathematical methods is important for an increasing number of university and college courses, particularly in physics, engineering and chemistry, but also in more general science. Students embarking on such courses come from diverse mathematical backgrounds, and their core knowledge varies considerably. We have therefore decided to write a textbook that assumes knowledge only of material that can be expected to be familiar to all the current generation of students starting physical science courses at university. In the United Kingdom this corresponds to the standard of Mathematics A-level, whereas in the United States the material assumed is that which would normally be covered at junior college.  Starting from this level, the ﬁrst six chapters cover a collection of topics with which the reader may already be familiar, but which are here extended and applied to typical problems encountered by ﬁrst-year university students. They are aimed at providing a common base of general techniques used in the development of the remaining chapters. Students who have had additional preparation, such as Further Mathematics at A-level, will ﬁnd much of this material straightforward.  Following these opening chapters, the remainder of the book is intended to cover at least that mathematical material which an undergraduate in the physical sciences might encounter up to the end of his or her course. The book is also appropriate for those beginning graduate study with a mathematical content, and naturally much of the material forms parts of courses for mathematics students. Furthermore, the text should provide a useful reference for research workers.  The general aim of the book is to present a topic in three stages. The ﬁrst stage is a qualitative introduction, wherever possible from a physical point of view. The second is a more formal presentation, although we have deliberately avoided strictly mathematical questions such as the existence of limits, uniform convergence, the interchanging of integration and summation orders, etc. on the  xxv   PREFACE TO THE FIRST EDITION  grounds that ‘this is the real world; it must behave reasonably’. Finally a worked example is presented, often drawn from familiar situations in physical science and engineering. These examples have generally been fully worked, since, in the authors’ experience, partially worked examples are unpopular with students. Only in a few cases, where trivial algebraic manipulation is involved, or where repetition of the main text would result, has an example been left as an exercise for the reader. Nevertheless, a number of exercises also appear at the end of each chapter, and these should give the reader ample opportunity to test his or her understanding. Hints and answers to these exercises are also provided.  With regard to the presentation of the mathematics, it has to be accepted that many equations  especially partial diﬀerential equations  can be written more compactly by using subscripts, e.g. uxy for a second partial derivative, instead of the more familiar ∂2u ∂x∂y, and that this certainly saves typographical space. However, for many students, the labour of mentally unpacking such equations is suﬃciently great that it is not possible to think of an equation’s physical interpretation at the same time. Consequently, wherever possible we have decided to write out such expressions in their more obvious but longer form.  During the writing of this book we have received much help and encouragement from various colleagues at the Cavendish Laboratory, Clare College, Trinity Hall and Peterhouse. In particular, we would like to thank Peter Scheuer, whose comments and general enthusiasm proved invaluable in the early stages. For reading sections of the manuscript, for pointing out misprints and for numerous useful comments, we thank many of our students and colleagues at the University of Cambridge. We are especially grateful to Chris Doran, John Huber, Garth Leder, Tom K¨orner and, not least, Mike Stobbs, who, sadly, died before the book was completed. We also extend our thanks to the University of Cambridge and the Cavendish teaching staﬀ, whose examination questions and lecture hand-outs have collectively provided the basis for some of the examples included. Of course, any errors and ambiguities remaining are entirely the responsibility of the authors, and we would be most grateful to have them brought to our attention.  We are indebted to Dave Green for a great deal of advice concerning typesetting in LATEX and to Andrew Lovatt for various other computing tips. Our thanks also go to Anja Visser and Grac¸a Rocha for enduring many hours of  sometimes heated  debate. At Cambridge University Press, we are very grateful to our editor Adam Black for his help and patience and to Alison Woollatt for her expert typesetting of such a complicated text. We also thank our copy-editor Susan Parkinson for many useful suggestions that have undoubtedly improved the style of the book.  Finally, on a personal note, KFR wishes to thank his wife Penny, not only for a long and happy marriage, but also for her support and understanding during his recent illness – and when things have not gone too well at the bridge table! MPH is indebted both to Rebecca Morris and to his parents for their tireless  xxvi   PREFACE TO THE FIRST EDITION  support and patience, and for their unending supplies of tea. SJB is grateful to Anthony Gritten for numerous relaxing discussions about J. S. Bach, to Susannah Ticciati for her patience and understanding, and to Kate Isaak for her calming late-night e-mails from the USA.  Ken Riley, Michael Hobson and Stephen Bence Cambridge, 1997  xxvii    Preliminary algebra  This opening chapter reviews the basic algebra of which a working knowledge is presumed in the rest of the book. Many students will be familiar with much, if not all, of it, but recent changes in what is studied during secondary education mean that it cannot be taken for granted that they will already have a mastery of all the topics presented here. The reader may assess which areas need further study or revision by attempting the exercises at the end of the chapter. The main areas covered are polynomial equations and the related topic of partial fractions, curve sketching, coordinate geometry, trigonometric identities and the notions of proof by induction or contradiction.  1.1 Simple functions and equations  It is normal practice when starting the mathematical investigation of a physical problem to assign an algebraic symbol to the quantity whose value is sought, either numerically or as an explicit algebraic expression. For the sake of deﬁniteness, in this chapter we will use x to denote this quantity most of the time. Subsequent steps in the analysis involve applying a combination of known laws, consistency conditions and  possibly  given constraints to derive one or more equations satisﬁed by x. These equations may take many forms, ranging from a simple polynomial equation to, say, a partial diﬀerential equation with several boundary conditions. Some of the more complicated possibilities are treated in the later chapters of this book, but for the present we will be concerned with techniques for the solution of relatively straightforward algebraic equations.  1.1.1 Polynomials and polynomial equations  Firstly we consider the simplest type of equation, a polynomial equation, in which a polynomial expression in x, denoted by f x , is set equal to zero and thereby  1  1   PRELIMINARY ALGEBRA  forms an equation which is satisﬁed by particular values of x, called the roots of the equation:  f x  = anxn + an−1xn−1 + ··· + a1x + a0 = 0.   1.1   Here n is an integer > 0, called the degree of both the polynomial and the  equation, and the known coeﬃcients a0, a1, . . . , an are real quantities with an  cid:3 = 0.  Equations such as  1.1  arise frequently in physical problems, the coeﬃcients ai being determined by the physical properties of the system under study. What is needed is to ﬁnd some or all of the roots of  1.1 , i.e. the x-values, αk, that satisfy f αk  = 0; here k is an index that, as we shall see later, can take up to n diﬀerent values, i.e. k = 1, 2, . . . , n. The roots of the polynomial equation can equally well be described as the zeros of the polynomial. When they are real, they correspond to the points at which a graph of f x  crosses the x-axis. Roots that are complex  see chapter 3  do not have such a graphical interpretation.  For polynomial equations containing powers of x greater than x4 general methods do not exist for obtaining explicit expressions for the roots αk. Even for n = 3 and n = 4 the prescriptions for obtaining the roots are suﬃciently complicated that it is usually preferable to obtain exact or approximate values by other methods. Only for n = 1 and n = 2 can closed-form solutions be given. These results will be well known to the reader, but they are given here for the sake of completeness. For n = 1,  1.1  reduces to the linear equation  the solution  root  is α1 = −a0 a1. For n = 2,  1.1  reduces to the quadratic  a1x + a0 = 0;  equation  the two roots α1 and α2 are given by  a2x2 + a1x + a0 = 0;   cid:2   −a1 ±  − 4a2a0  .  a2 1 2a2  α1,2 =  When discussing speciﬁcally quadratic equations, as opposed to more general polynomial equations, it is usual to write the equation in one of the two notations  ax2 + bx + c = 0,  ax2 + 2bx + c = 0,   1.5   with respective explicit pairs of solutions  −b ± √  b2 − 4ac  α1,2 =  2a  −b ± √  b2 − ac  a  ,  α1,2 =  .   1.6   Of course, these two notations are entirely equivalent and the only important point is to associate each form of answer with the corresponding form of equation; most people keep to one form, to avoid any possible confusion.  2   1.2    1.3    1.4    1.1 SIMPLE FUNCTIONS AND EQUATIONS  If the value of the quantity appearing under the square root sign is positive then both roots are real; if it is negative then the roots form a complex conjugate  pair, i.e. they are of the form p ± iq with p and q real  see chapter 3 ; if it has  zero value then the two roots are equal and special considerations usually arise. Thus linear and quadratic equations can be dealt with in a cut-and-dried way. We now turn to methods for obtaining partial information about the roots of higher-degree polynomial equations. In some circumstances the knowledge that an equation has a root lying in a certain range, or that it has no real roots at all, is all that is actually required. For example, in the design of electronic circuits it is necessary to know whether the current in a proposed circuit will break into spontaneous oscillation. To test this, it is suﬃcient to establish whether a certain polynomial equation, whose coeﬃcients are determined by the physical parameters of the circuit, has a root with a positive real part  see chapter 3 ; complete determination of all the roots is not needed for this purpose. If the complete set of roots of a polynomial equation is required, it can usually be obtained to any desired accuracy by numerical methods such as those described in chapter 27.  There is no explicit step-by-step approach to ﬁnding the roots of a general polynomial equation such as  1.1 . In most cases analytic methods yield only information about the roots, rather than their exact values. To explain the relevant techniques we will consider a particular example, ‘thinking aloud’ on paper and expanding on special points about methods and lines of reasoning. In more routine situations such comment would be absent and the whole process briefer and more tightly focussed.  Let us investigate the roots of the equation  Example: the cubic case  g x  = 4x3 + 3x2 − 6x − 1 = 0   1.7   or, in an alternative phrasing, investigate the zeros of g x . We note ﬁrst of all that this is a cubic equation. It can be seen that for x large and positive g x  will be large and positive and, equally, that for x large and negative g x  will be large and negative. Therefore, intuitively  or, more formally, by continuity  g x  must cross the x-axis at least once and so g x  = 0 must have at least one real root. Furthermore, it can be shown that if f x  is an nth-degree polynomial then the graph of f x  must cross the x-axis an even or odd number of times  as x varies between −∞ and +∞, according to whether n itself is even or odd.  Thus a polynomial of odd degree always has at least one real root, but one of even degree may have no real root. A small complication, discussed later in this section, occurs when repeated roots arise.  Having established that g x  = 0 has at least one real root, we may ask how  3   PRELIMINARY ALGEBRA  many real roots it could have. To answer this we need one of the fundamental theorems of algebra, mentioned above:  An nth-degree polynomial equation has exactly n roots.  It should be noted that this does not imply that there are n real roots  only that there are not more than n ; some of the roots may be of the form p + iq.  To make the above theorem plausible and to see what is meant by repeated roots, let us suppose that the nth-degree polynomial equation f x  = 0,  1.1 , has r roots α1, α2, . . . , αr, considered distinct for the moment. That is, we suppose that f αk  = 0 for k = 1, 2, . . . , r, so that f x  vanishes only when x is equal to one of the r values αk. But the same can be said for the function  F x  = A x − α1  x − α2 ···  x − αr ,   1.8   in which A is a non-zero constant; F x  can clearly be multiplied out to form a polynomial expression.  We now call upon a second fundamental result in algebra: that if two poly- nomial functions f x  and F x  have equal values for all values of x, then their coeﬃcients are equal on a term-by-term basis. In other words, we can equate the coeﬃcients of each and every power of x in the two expressions  1.8  and  1.1 ; in particular we can equate the coeﬃcients of the highest power of x. From  this we have Axr ≡ anxn and thus that r = n and A = an. As r is both equal  to n and to the number of roots of f x  = 0, we conclude that the nth-degree polynomial f x  = 0 has n roots.  Although this line of reasoning may make the theorem plausible, it does not constitute a proof since we have not shown that it is permissible to write f x  in the form of equation  1.8 .   We next note that the condition f αk  = 0 for k = 1, 2, . . . , r, could also be met  if  1.8  were replaced by  F x  = A x − α1 m1  x − α2 m2 ···  x − αr mr ,   1.9   with A = an. In  1.9  the mk are integers ≥ 1 and are known as the multiplicities of the roots, mk being the multiplicity of αk. Expanding the right-hand side  RHS  leads to a polynomial of degree m1 + m2 + ··· + mr. This sum must be equal to n. Thus, if any of the mk is greater than unity then the number of distinct roots, r, is less than n; the total number of roots remains at n, but one or more of the αk counts more than once. For example, the equation  F x  = A x − α1 2 x − α2 3 x − α3  x − α4  = 0  has exactly seven roots, α1 being a double root and α2 a triple root, whilst α3 and α4 are unrepeated  simple  roots.  We can now say that our particular equation  1.7  has either one or three real roots but in the latter case it may be that not all the roots are distinct. To decide how many real roots the equation has, we need to anticipate two ideas from the  4   1.1 SIMPLE FUNCTIONS AND EQUATIONS  φ1 x   φ2 x   β1  β2  x  β2  β1  x  Figure 1.1 Two curves φ1 x  and φ2 x , both with zero derivatives at the same values of x, but with diﬀerent numbers of real solutions to φi x  = 0.   cid:7   next chapter. The ﬁrst of these is the notion of the derivative of a function, and the second is a result known as Rolle’s theorem.  The derivative f   x  of a function f x  measures the slope of the tangent to the graph of f x  at that value of x  see ﬁgure 2.1 in the next chapter . For the moment, the reader with no prior knowledge of calculus is asked to accept that the derivative of axn is naxn−1, so that the derivative g  x  of the curve g x  = 4x3 + 3x2 − 6x − 1 is given by g   x  = 12x2 + 6x − 6. Similar expressions   cid:7    cid:7   for the derivatives of other polynomials are used later in this chapter.  Rolle’s theorem states that if f x  has equal values at two diﬀerent values of x then at some point between these two x-values its derivative is equal to zero; i.e. the tangent to its graph is parallel to the x-axis at that point  see ﬁgure 2.2 .  Having brieﬂy mentioned the derivative of a function and Rolle’s theorem, we now use them to establish whether g x  has one or three real zeros. If g x  = 0 does have three real roots αk, i.e. g αk  = 0 for k = 1, 2, 3, then it follows from Rolle’s theorem that between any consecutive pair of them  say α1 and α2  there  x  = 0. Similarly, there must be a further must be some real value of x at which g  x  lying between α2 and α3. Thus a necessary condition for three real zero of g roots of g x  = 0 is that g   x  = 0 itself has two real roots.   cid:7    cid:7    cid:7   However, this condition on the number of roots of g   x  = 0, whilst necessary, is not suﬃcient to guarantee three real roots of g x  = 0. This can be seen by inspecting the cubic curves in ﬁgure 1.1. For each of the two functions φ1 x  and φ2 x , the derivative is equal to zero at both x = β1 and x = β2. Clearly, though, φ2 x  = 0 has three real roots whilst φ1 x  = 0 has only one. It is easy to see that the crucial diﬀerence is that φ1 β1  and φ1 β2  have the same sign, whilst φ2 β1  and φ2 β2  have opposite signs.  It will be apparent that for some equations, φ x  = 0 say, φ   x  equals zero   cid:7    cid:7   5   PRELIMINARY ALGEBRA  at a value of x for which φ x  is also zero. Then the graph of φ x  just touches the x-axis. When this happens the value of x so found is, in fact, a double real root of the polynomial equation  corresponding to one of the mk in  1.9  having the value 2  and must be counted twice when determining the number of real roots.  Finally, then, we are in a position to decide the number of real roots of the  equation  g x  = 4x3 + 3x2 − 6x − 1 = 0.   cid:7    x  = 0, with g The equation g § explicit solutions   cid:7    x  = 12x2 + 6x − 6, is a quadratic equation with  −3 ± √  9 + 72  ,  12  β1,2 =  so that β1 = −1 and β2 = 1 g β2  = − 11 has three real roots, one lying in the range −1 < x < 1  2 . The corresponding values of g x  are g β1  = 4 and 4 , which are of opposite sign. This indicates that 4x3 + 3x2− 6x− 1 = 0 2 and the others one on  each side of that range.  The techniques we have developed above have been used to tackle a cubic equation, but they can be applied to polynomial equations f x  = 0 of degree greater than 3. However, much of the analysis centres around the equation  cid:7   x  = 0 and this itself, being then a polynomial equation of degree 3 or more, f either has no closed-form general solution or one that is complicated to evaluate. Thus the amount of information that can be obtained about the roots of f x  = 0 is correspondingly reduced.  A more general case  To illustrate what can  and cannot  be done in the more general case we now investigate as far as possible the real roots of  f x  = x7 + 5x6 + x4 − x3 + x2 − 2 = 0.  The following points can be made.   i  This is a seventh-degree polynomial equation; therefore the number of  real roots is 1, 3, 5 or 7.   ii  f 0  is negative whilst f ∞  = +∞, so there must be at least one positive  root.  §  The two roots β1, β2 are written as β1,2. By convention β1 refers to the upper symbol in ±, β2 to  the lower symbol.  6   1.1 SIMPLE FUNCTIONS AND EQUATIONS   iii  The equation f   cid:7    x  = 0 can be written as x 7x5 + 30x4 + 4x2 − 3x + 2  = 0   cid:7    cid:7  cid:7   and thus x = 0 is a root. The derivative of f   x , equals  x  is positive at x = 0 indicates  subsection 2.1.8  that f x  has a minimum there. This,  42x5 + 150x4 + 12x2 − 6x + 2. That f together with the facts that f 0  is negative and f ∞  = ∞, implies that   x  is zero whilst f   x , denoted by f   cid:7  cid:7    cid:7   the total number of real roots to the right of x = 0 must be odd. Since the total number of real roots must be odd, the number to the left must be even  0, 2, 4 or 6 .  This is about all that can be deduced by simple analytic methods in this case, although some further progress can be made in the ways indicated in exercise 1.3. There are, in fact, more sophisticated tests that examine the relative signs of successive terms in an equation such as  1.1 , and in quantities derived from them, to place limits on the numbers and positions of roots. But they are not prerequisites for the remainder of this book and will not be pursued further here.  We conclude this section with a worked example which demonstrates that the practical application of the ideas developed so far can be both short and decisive.  cid:1 For what values of k, if any, does  f x  = x3 − 3x2 + 6x + k = 0  have three real roots?  Firstly we study the equation f   x  = 0, i.e. 3x2 − 6x + 6 = 0. This is a quadratic equation but, using  1.6 , because 62 < 4 × 3 × 6, it can have no real roots. Therefore, it follows immediately that f x  has no maximum or minimum; consequently f x  = 0 cannot have more than one real root, whatever the value of k.  cid:2    cid:7   1.1.2 Factorising polynomials  In the previous subsection we saw how a polynomial with r given distinct zeros αk could be constructed as the product of factors containing those zeros:  f x  = an x − α1 m1  x − α2 m2 ···  x − αr mr = anxn + an−1xn−1 + ··· + a1x + a0,  with m1 + m2 +··· + mr = n, the degree of the polynomial. It will cause no loss of generality in what follows to suppose that all the zeros are simple, i.e. all mk = 1 and r = n, and this we will do.  Sometimes it is desirable to be able to reverse this process, in particular when one exact zero has been found by some method and the remaining zeros are to be investigated. Suppose that we have located one zero, α; it is then possible to write  1.10  as   1.10    1.11   f x  =  x − α f1 x ,  7   PRELIMINARY ALGEBRA  where f1 x  is a polynomial of degree n−1. How can we ﬁnd f1 x ? The procedure  is much more complicated to describe in a general form than to carry out for an equation with given numerical coeﬃcients ai. If such manipulations are too complicated to be carried out mentally, they could be laid out along the lines of an algebraic ‘long division’ sum. However, a more compact form of calculation is as follows. Write f1 x  as  f1 x  = bn−1xn−1 + bn−2xn−2 + bn−3xn−3 + ··· + b1x + b0.  Substitution of this form into  1.11  and subsequent comparison of the coeﬃcients  of xp for p = n, n − 1, . . . , 1, 0 with those in the second line of  1.10  generates  the series of equations  bn−1 = an, bn−2 − αbn−1 = an−1, bn−3 − αbn−2 = an−2,  ...  b0 − αb1 = a1, −αb0 = a0.  These can be solved successively for the bj, starting either from the top or from the bottom of the series. In either case the ﬁnal equation used serves as a check; if it is not satisﬁed, at least one mistake has been made in the computation – or α is not a zero of f x  = 0. We now illustrate this procedure with a worked example.   cid:1 Determine by inspection the simple roots of the equation  f x  = 3x4 − x3 − 10x2 − 2x + 4 = 0  and hence, by factorisation, ﬁnd the rest of its roots.  From the pattern of coeﬃcients it can be seen that x = −1 is a solution to the equation.  We therefore write  where  f x  =  x + 1  b3x3 + b2x2 + b1x + b0 ,  These equations give b3 = 3, b2 = −4, b1 = −6, b0 = 4  check  and so f x  =  x + 1 f1 x  =  x + 1  3x3 − 4x2 − 6x + 4 .  b3 = 3,  b2 + b3 = −1, b1 + b2 = −10, b0 + b1 = −2,  b0 = 4.  8   1.1 SIMPLE FUNCTIONS AND EQUATIONS  We now note that f1 x  = 0 if x is set equal to 2. Thus x − 2 is a factor of f1 x , which  therefore can be written as  f1 x  =  x − 2 f2 x  =  x − 2  c2x2 + c1x + c0   with  c2 = 3,  c1 − 2c2 = −4, c0 − 2c1 = −6, −2c0 = 4. −1 ± √ 3  −1 +  3  x =  These equations determine f2 x  as 3x2 + 2x − 2. Since f2 x  = 0 is a quadratic equation,  its solutions can be written explicitly as  Thus the four roots of f x  = 0 are −1, 2, 1  .  1 + 6 √ 7  and 1  3  −1 − √  7 .  cid:2   1.1.3 Properties of roots  From the fact that a polynomial equation can be written in any of the alternative forms  f x  = anxn + an−1xn−1 + ··· + a1x + a0 = 0, f x  = an x − α1 m1  x − α2 m2 ···  x − αr mr = 0, f x  = an x − α1  x − α2 ···  x − αn  = 0,  it follows that it must be possible to express the coeﬃcients ai in terms of the roots αk. To take the most obvious example, comparison of the constant terms  formally the coeﬃcient of x0  in the ﬁrst and third expressions shows that  an −α1  −α2 ···  −αn  = a0,  or, using the product notation,  Only slightly less obvious is a result obtained by comparing the coeﬃcients of  xn−1 in the same two expressions of the polynomial:  Comparing the coeﬃcients of other powers of x yields further results, though they are of less general use than the two just given. One such, which the reader may wish to derive, is   1.12    1.13    1.14   n cid:3   .  k=1  an  αk =  −1 n a0 n cid:4   αk = − an−1  .  an  k=1  n cid:4   n cid:4   j=1  k>j  αjαk =  an−2 an  .  9   PRELIMINARY ALGEBRA  In the case of a quadratic equation these root properties are used suﬃciently often that they are worth stating explicitly, as follows. If the roots of the quadratic equation ax2 + bx + c = 0 are α1 and α2 then α1 + α2 = − b  ,  α1α2 =  a  c a  .  If the alternative standard form for the quadratic is used, b is replaced by 2b in both the equation and the ﬁrst of these results.   cid:1 Find a cubic equation whose roots are −4, 3 and 5.  3 cid:4   k=1  −a2 =  From results  1.12  –  1.14  we can compute that, arbitrarily setting a3 = 1,  αk = 4,  a1 =  αjαk = −17,  a0 =  −1 3  αk = 60.  Thus a possible cubic equation is x3 +  −4 x2 +  −17 x +  60  = 0. Of course, any multiple of x3 − 4x2 − 17x + 60 = 0 will do just as well.  cid:2   3 cid:4   3 cid:4   j=1  k>j  3 cid:3   k=1  1.2 Trigonometric identities  So many of the applications of mathematics to physics and engineering are concerned with periodic, and in particular sinusoidal, behaviour that a sure and ready handling of the corresponding mathematical functions is an essential skill. Even situations with no obvious periodicity are often expressed in terms of periodic functions for the purposes of analysis. Later in this book whole chapters are devoted to developing the techniques involved, but as a necessary prerequisite we here establish  or remind the reader of  some standard identities with which he or she should be fully familiar, so that the manipulation of expressions containing sinusoids becomes automatic and reliable. So as to emphasise the angular nature of the argument of a sinusoid we will denote it in this section by θ rather than x.  1.2.1 Single-angle identities  We give without proof the basic identity satisﬁed by the sinusoidal functions sin θ and cos θ, namely  cos2 θ + sin2 θ = 1.   1.15   If sin θ and cos θ have been deﬁned geometrically in terms of the coordinates of a point on a circle, a reference to the name of Pythagoras will suﬃce to establish this result. If they have been deﬁned by means of series  with θ expressed in radians  then the reader should refer to Euler’s equation  3.23  on page 93, and note that eiθ has unit modulus if θ is real.  10   1.2 TRIGONOMETRIC IDENTITIES  y   cid:7   y  R  P  M  T  N  B  A  O   cid:7   x  x  Figure 1.2 Illustration of the compound-angle identities. Refer to the main text for details.  Other standard single-angle formulae derived from  1.15  by dividing through  by various powers of sin θ and cos θ are  1 + tan2 θ = sec2 θ, cot2 θ + 1 = cosec 2θ.   1.16    1.17   1.2.2 Compound-angle identities  The basis for building expressions for the sinusoidal functions of compound angles are those for the sum and diﬀerence of just two angles, since all other cases can be built up from these, in principle. Later we will see that a study of complex numbers can provide a more eﬃcient approach in some cases.  To prove the basic formulae for the sine and cosine of a compound angle A + B in terms of the sines and cosines of A and B, we consider the construction , with a common shown in ﬁgure 1.2. It shows two sets of axes, Oxy and Ox origin but rotated with respect to each other through an angle A. The point P lies on the unit circle centred on the common origin O and has coordinates cos A + B , sin A + B  with respect to the axes Oxy and coordinates cos B, sin B with respect to the axes Ox  y  y   cid:7    cid:7    cid:7    cid:7   .  Parallels to the axes Oxy  dotted lines  and Ox  y  drawn through P . Further parallels  MR and RN  to the Ox   broken lines  have been axes have been   cid:7    cid:7   y   cid:7    cid:7   11   PRELIMINARY ALGEBRA  drawn through R, the point  0, sin A + B   in the Oxy system. That all the angles  marked with the symbol   are equal to A follows from the simple geometry of  right-angled triangles and crossing lines.  We now determine the coordinates of P in terms of lengths in the ﬁgure,  expressing those lengths in terms of both sets of coordinates:   cid:7    cid:7    i  cos B = x  = T N + NP = MR + NP   ii  sin B = y  = OR sin A + RP cos A = sin A + B  sin A + cos A + B  cos A;  = OM − T M = OM − NR = OR cos A − RP sin A = sin A + B  cos A − cos A + B  sin A.  Now, if equation  i  is multiplied by sin A and added to equation  ii  multiplied by cos A, the result is  sin A cos B + cos A sin B = sin A + B  sin2 A + cos2 A  = sin A + B .  Similarly, if equation  ii  is multiplied by sin A and subtracted from equation  i  multiplied by cos A, the result is  cos A cos B − sin A sin B = cos A + B  cos2 A + sin2 A  = cos A + B .  Corresponding graphically based results can be derived for the sines and cosines of the diﬀerence of two angles; however, they are more easily obtained by setting  B to −B in the previous results and remembering that sin B becomes − sin B  whilst cos B is unchanged. The four results may be summarised by  sin A ± B  = sin A cos B ± cos A sin B cos A ± B  = cos A cos B ∓ sin A sin B.  Standard results can be deduced from these by setting one of the two angles  equal to π or to π 2:   cid:6    cid:5  sin π − θ  = sin θ, 2 π − θ  = cos θ,  1  sin   cid:5  cos π − θ  = − cos θ,   cid:6   2 π − θ  1  cos  = sin θ,  From these basic results many more can be derived. An immediate deduction, obtained by taking the ratio of the two equations  1.18  and  1.19  and then dividing both the numerator and denominator of this ratio by cos A cos B, is  tan A ± B  =  tan A ± tan B 1 ∓ tan A tan B  .  One application of this result is a test for whether two lines on a graph are orthogonal  perpendicular ; more generally, it determines the angle between them. The standard notation for a straight-line graph is y = mx + c, in which m is the slope of the graph and c is its intercept on the y-axis. It should be noted that the slope m is also the tangent of the angle the line makes with the x-axis.   1.18    1.19    1.20    1.21    1.22   12   1.2 TRIGONOMETRIC IDENTITIES  Consequently the angle θ12 between two such straight-line graphs is equal to the diﬀerence in the angles they individually make with the x-axis, and the tangent of that angle is given by  1.22 :  tan θ12 =  tan θ1 − tan θ2  m1 − m2  .  =  1 + tan θ1 tan θ2  1 + m1m2  For the lines to be orthogonal we must have θ12 = π 2, i.e. the ﬁnal fraction on  the RHS of the above equation must equal ∞, and so  m1m2 = −1.  A kind of inversion of equations  1.18  and  1.19  enables the sum or diﬀerence of two sines or cosines to be expressed as the product of two sinusoids; the procedure is typiﬁed by the following. Adding together the expressions given by   1.18  for sin A + B  and sin A − B  yields  sin A + B  + sin A − B  = 2 sin A cos B.  cid:7  If we now write A + B = C and A − B = D, this becomes   cid:8    cid:7   sin C + sin D = 2 sin  C + D  cos   cid:8   .  C − D  2  In a similar way each of the following equations can be derived:  2  2   cid:7   cid:7   cid:7   C + D  2 C + D  2   cid:8   cid:8   cid:8   sin  cos   cid:7   cid:8  C − D  cid:7   cid:8  C − D  cid:8   cid:7  C − D  2  2  ,  ,  sin  .  2  sin C − sin D = 2 cos  C + D  cos C + cos D = 2 cos  cos C − cos D = −2 sin   1.23    1.24    1.25    1.26    1.27    1.28   The minus sign on the right of the last of these equations should be noted; it may help to avoid overlooking this ‘oddity’ to recall that if C > D then cos C < cos D.  1.2.3 Double- and half-angle identities  Double-angle and half-angle identities are needed so often in practical calculations that they should be committed to memory by any physical scientist. They can be obtained by setting B equal to A in results  1.18  and  1.19 . When this is done,  13   and use made of equation  1.15 , the following results are obtained:  PRELIMINARY ALGEBRA  sin 2θ = 2 sin θ cos θ,  cos 2θ = cos2 θ − sin2 θ = 2 cos2 θ − 1 = 1 − 2 sin2 θ, 1 − tan2 θ  tan 2θ =  2 tan θ  .  θ 2  cos  =  − sin2 θ  2  2t  1 + t2 , 1 − t2 1 + t2 ,  =  sin θ = 2 sin  θ 2 cos θ = cos2 θ 2 1 − t2 .  tan θ =  2t   1.29    1.30    1.31    1.32    1.33    1.34   A further set of identities enables sinusoidal functions of θ to be expressed in terms of polynomial functions of a variable t = tan θ 2 . They are not used in their primary role until the next chapter, but we give a derivation of them here for reference.  If t = tan θ 2 , then it follows from  1.16  that 1+t2 = sec2 θ 2  and cos θ 2  = −1 2. Now, using  1.29  and  1.30 , we may  −1 2, whilst sin θ 2  = t 1 + t2    1 + t2  write:  It can be further shown that the derivative of θ with respect to t takes the algebraic form 2  1 + t2 . This completes a package of results that enables expressions involving sinusoids, particularly when they appear as integrands, to be cast in more convenient algebraic forms. The proof of the derivative property and examples of use of the above results are given in subsection  2.2.7 .  We conclude this section with a worked example which is of such a commonly  occurring form that it might be considered a standard procedure.  cid:1 Solve for θ the equation  a sin θ + b cos θ = k,  where a, b and k are given real quantities.  To solve this equation we make use of result  1.18  by setting a = K cos φ and b = K sin φ for suitable values of K and φ. We then have  k = K cos φ sin θ + K sin φ cos θ = K sin θ + φ ,  with  Whether φ lies in 0 ≤ φ ≤ π or in −π < φ < 0 has to be determined by the individual  K 2 = a2 + b2  and  φ = tan  −1 b a  .  signs of a and b. The solution is thus  −1  θ = sin  − φ,   cid:7    cid:8   k K  14   1.3 COORDINATE GEOMETRY  with K and φ as given above. Notice that the inverse sine yields two values in the range 0  to 2π and that there is no real solution to the original equation if k > K =  a2 + b2 1 2.  cid:2   1.3 Coordinate geometry  We have already mentioned the standard form for a straight-line graph, namely  y = mx + c,   1.35   representing a linear relationship between the independent variable x and the dependent variable y. The slope m is equal to the tangent of the angle the line makes with the x-axis whilst c is the intercept on the y-axis.  An alternative form for the equation of a straight line is  ax + by + k = 0,   1.36   to which  1.35  is clearly connected by  m = − a  b  and  c = − k  .  b  This form treats x and y on a more symmetrical basis, the intercepts on the two  axes being −k a and −k b respectively. A power relationship between two variables, i.e. one of the form y = Axn, can also be cast into straight-line form by taking the logarithms of both sides. Whilst it is normal in mathematical work to use natural logarithms  to base e, written ln x , for practical investigations logarithms to base 10 are often employed. In either case the form is the same, but it needs to be remembered which has been used when recovering the value of A from ﬁtted data. In the mathematical  base e  form, the power relationship becomes  ln y = n ln x + ln A.   1.37   Now the slope gives the power n, whilst the intercept on the ln y axis is ln A, which yields A, either by exponentiation or by taking antilogarithms.  The other standard coordinate forms of two-dimensional curves that students should know and recognise are those concerned with the conic sections – so called because they can all be obtained by taking suitable sections across a  double  cone. Because the conic sections can take many diﬀerent orientations and scalings their general form is complex,  Ax2 + By2 + Cxy + Dx + Ey + F = 0,   1.38   but each can be represented by one of four generic forms, an ellipse, a parabola, a hyperbola or, the degenerate form, a pair of straight lines. If they are reduced to their standard representations, in which axes of symmetry are made to coincide  15   PRELIMINARY ALGEBRA  with the coordinate axes, the ﬁrst three take the forms  +  = 1   ellipse ,   x − α 2   y − β 2  a2   y − β 2 = 4a x − α   b2   x − α 2  −  y − β 2  = 1  a2  b2   parabola ,   hyperbola .   1.39    1.40    1.41   Here,  α, β  gives the position of the ‘centre’ of the curve, usually taken as the origin  0, 0  when this does not conﬂict with any imposed conditions. The parabola equation given is that for a curve symmetric about a line parallel to the x-axis. For one symmetrical about a parallel to the y-axis the equation would  Of course, the circle is the special case of an ellipse in which b = a and the  read  x − α 2 = 4a y − β .  equation takes the form   x − α 2 +  y − β 2 = a2.   1.42   The distinguishing characteristic of this equation is that when it is expressed in the form  1.38  the coeﬃcients of x2 and y2 are equal and that of xy is zero; this property is not changed by any reorientation or scaling and so acts to identify a general conic as a circle.  Deﬁnitions of the conic sections in terms of geometrical properties are also available; for example, a parabola can be deﬁned as the locus of a point that is always at the same distance from a given straight line  the directrix  as it is from a given point  the focus . When these properties are expressed in Cartesian coordinates the above equations are obtained. For a circle, the deﬁning property is that all points on the curve are a distance a from  α, β ;  1.42  expresses this requirement very directly. In the following worked example we derive the equation for a parabola.   cid:1 Find the equation of a parabola that has the line x = −a as its directrix and the point   a, 0  as its focus.  Figure 1.3 shows the situation in Cartesian coordinates. Expressing the deﬁning requirement that P N and P F are equal in length gives   x + a  = [ x − a 2 + y2]1 2 ⇒  x + a 2 =  x − a 2 + y2  which, on expansion of the squared terms, immediately gives y2 = 4ax. This is  1.40  with α and β both set equal to zero.  cid:2   Although the algebra is more complicated, the same method can be used to derive the equations for the ellipse and the hyperbola. In these cases the distance from the ﬁxed point is a deﬁnite fraction, e, known as the eccentricity, of the distance from the ﬁxed line. For an ellipse 0 < e < 1, for a circle e = 0, and for a hyperbola e > 1. The parabola corresponds to the case e = 1.  16   1.3 COORDINATE GEOMETRY  y  x  N  P   x, y   O  F   a, 0   x = −a  Figure 1.3 Construction of a parabola using the point  a, 0  as the focus and  the line x = −a as the directrix.  The values of a and b  with a ≥ b  in equation  1.39  for an ellipse are related  to e through  a2 − b2  a2  e2 =  and give the lengths of the semi-axes of the ellipse. If the ellipse is centred on  the origin, i.e. α = β = 0, then the focus is  −ae, 0  and the directrix is the line x = −a e.  For each conic section curve, although we have two variables, x and y, they are not independent, since if one is given then the other can be determined. However, determining y when x is given, say, involves solving a quadratic equation on each occasion, and so it is convenient to have parametric representations of the curves. A parametric representation allows each point on a curve to be associated with a unique value of a single parameter t. The simplest parametric representations for the conic sections are as given below, though that for the hyperbola uses hyperbolic functions, not formally introduced until chapter 3. That they do give valid parameterizations can be veriﬁed by substituting them into the standard forms  1.39 – 1.41 ; in each case the standard form is reduced to an algebraic or trigonometric identity.  x = α + a cos φ, x = α + at2, x = α + a cosh φ,  y = β + b sin φ y = β + 2at y = β + b sinh φ  hyperbola .   ellipse ,  parabola ,  As a ﬁnal example illustrating several topics from this section we now prove  17   PRELIMINARY ALGEBRA  the well-known result that the angle subtended by a diameter at any point on a circle is a right angle.   cid:1 Taking the diameter to be the line joining Q =  −a, 0  and R =  a, 0  and the point P to be any point on the circle x2 + y2 = a2, prove that angle QP R is a right angle.  If P is the point  x, y , the slope of the line QP is  That of RP is  Thus  y − 0 x −  −a  y − 0 x −  a   m1 =  m2 =  =  y  .  x + a  =  y  x − a  .  m1m2 =  y2 x2 − a2 .  But, since P is on the circle, y2 = a2 − x2 and consequently m1m2 = −1. From result  1.24  this implies that QP and RP are orthogonal and that QP R is therefore a right angle. Note that this is true for any point P on the circle.  cid:2   1.4 Partial fractions  In subsequent chapters, and in particular when we come to study integration in chapter 2, we will need to express a function f x  that is the ratio of two polynomials in a more manageable form. To remove some potential complexity from our discussion we will assume that all the coeﬃcients in the polynomials are real, although this is not an essential simpliﬁcation.  The behaviour of f x  is crucially determined by the location of the zeros of its denominator, i.e. if f x  is written as f x  = g x  h x  where both g x  and § then f x  changes extremely rapidly when x is close to h x  are polynomials, those values αi that are the roots of h x  = 0. To make such behaviour explicit, we write f x  as a sum of terms such as A  x− α n, in which A is a constant, α is one of the αi that satisfy h αi  = 0 and n is a positive integer. Writing a function in this way is known as expressing it in partial fractions.  Suppose, for the sake of deﬁniteness, that we wish to express the function  §  It is assumed that the ratio has been reduced so that g x  and h x  do not contain any common factors, i.e. there is no value of x that makes both vanish at the same time. We may also assume without any loss of generality that the coeﬃcient of the highest power of x in h x  has been made equal to unity, if necessary, by dividing both numerator and denominator by the coeﬃcient of this highest power.  f x  =  4x + 2  x2 + 3x + 2  18   1.4 PARTIAL FRACTIONS  in partial fractions, i.e. to write it as  f x  =  g x  h x   =  4x + 2  x2 + 3x + 2  =  A1   x − α1 n1  +  A2   x − α2 n2  + ··· .   1.43   The ﬁrst question that arises is that of how many terms there should be on the right-hand side  RHS . Although some complications occur when h x  has repeated roots  these are considered below  it is clear that f x  only becomes inﬁnite at the two values of x, α1 and α2, that make h x  = 0. Consequently the RHS can only become inﬁnite at the same two values of x and therefore contains only two partial fractions – these are the ones shown explicitly. This argument can be trivially extended  again temporarily ignoring the possibility of repeated roots of h x   to show that if h x  is a polynomial of degree n then there should be n terms on the RHS, each containing a diﬀerent root αi of the equation h αi  = 0. A second general question concerns the appropriate values of the ni. This is answered by putting the RHS over a common denominator, which will clearly  have to be the product  x − α1 n1  x − α2 n2 ··· . Comparison of the highest power of x in this new RHS with the same power in h x  shows that n1 + n2 + ··· = n. This result holds whether or not h x  = 0 has repeated roots and, although we do not give a rigorous proof, strongly suggests the following correct conclusions.    The number of terms on the RHS is equal to the number of distinct roots of h x  = 0, each term having a diﬀerent root αi in its denominator  x − αi ni .   If αi is a multiple root of h x  = 0 then the value to be assigned to ni in  1.43  is on p. 23, Ai has to be replaced by a polynomial of degree mi − 1. This is also  that of mi when h x  is written in the product form  1.9 . Further, as discussed  formally true for non-repeated roots, since then both mi and ni are equal to unity.  Returning to our speciﬁc example we note that the denominator h x  has zeros  at x = α1 = −1 and x = α2 = −2; these x-values are the simple  non-repeated   roots of h x  = 0. Thus the partial fraction expansion will be of the form  4x + 2  =  A1  +  A2  .  x2 + 3x + 2  x + 1  x + 2   1.44   We now list several methods available for determining the coeﬃcients A1 and A2. We also remind the reader that, as with all the explicit examples and techniques described, these methods are to be considered as models for the handling of any ratio of polynomials, with or without characteristics that make it a special case.   i  The RHS can be put over a common denominator, in this case  x+1  x+2 , and then the coeﬃcients of the various powers of x can be equated in the  19   PRELIMINARY ALGEBRA  numerators on both sides of the equation. This leads to  4x + 2 = A1 x + 2  + A2 x + 1 ,  4 = A1 + A2  2 = 2A1 + A2.  Solving the simultaneous equations for A1 and A2 gives A1 = −2 and  A2 = 6.   ii  A second method is to substitute two  or more generally n  diﬀerent values of x into each side of  1.44  and so obtain two  or n  simultaneous equations for the two  or n  constants Ai. To justify this practical way of proceeding it is necessary, strictly speaking, to appeal to method  i  above, which establishes that there are unique values for A1 and A2 valid for all values of x. It is normally very convenient to take zero as one of the values of x, but of course any set will do. Suppose in the present case that we use the values x = 0 and x = 1 and substitute in  1.44 . The resulting equations are  2 2 6 6  =  =  A1 1 A1 2  +  +  A2 2 A2 3  ,  ,  which on solution give A1 = −2 and A2 = 6, as before. The reader can  easily verify that any other pair of values for x  except for a pair that includes α1 or α2  gives the same values for A1 and A2.   iii  The very reason why method  ii  fails if x is chosen as one of the roots αi of h x  = 0 can be made the basis for determining the values of the Ai corresponding to non-multiple roots without having to solve simultaneous equations. The method is conceptually more diﬃcult than the other meth- ods presented here, and needs results from the theory of complex variables  chapter 24  to justify it. However, we give a practical ‘cookbook’ recipe for determining the coeﬃcients.   a  To determine the coeﬃcient Ak,  written as the product  x − α1  x − α2 ···  x − αn , with any m-fold  imagine the denominator h x   repeated root giving rise to m factors in parentheses.   b  Now set x equal to αk and evaluate the expression obtained after  omitting the factor that reads αk − αk.   c  Divide the value so obtained into g αk ; the result is the required  coeﬃcient Ak.  For our speciﬁc example we ﬁnd that in step  a  that h x  =  x + 1  x + 2   and that in evaluating A1 step  b  yields −1 + 2, i.e. 1. Since g −1  = 4 −1  + 2 = −2, step  c  gives A1 as  −2   1 , i.e in agreement with our other evaluations. In a similar way A2 is evaluated as  −6   −1  = 6.  20   1.4 PARTIAL FRACTIONS  Thus any one of the methods listed above shows that  −2  4x + 2  =  +  6  .  x2 + 3x + 2  x + 1  x + 2  The best method to use in any particular circumstance will depend on the complexity, in terms of the degrees of the polynomials and the multiplicities of the roots of the denominator, of the function being considered and, to some extent, on the individual inclinations of the student; some prefer lengthy but straightforward solution of simultaneous equations, whilst others feel more at home carrying through shorter but more abstract calculations in their heads.  1.4.1 Complications and special cases  Having established the basic method for partial fractions, we now show, through further worked examples, how some complications are dealt with by extensions to the procedure. These extensions are introduced one at a time, but of course in any practical application more than one may be involved.  The degree of the numerator is greater than or equal to that of the denominator  Although we have not speciﬁcally mentioned the fact, it will be apparent from trying to apply method  i  of the previous subsection to such a case, that if the degree of the numerator  m  is not less than that of the denominator  n  then the ratio of two polynomials cannot be expressed in partial fractions.  To get round this diﬃculty it is necessary to start by dividing the denominator h x  into the numerator g x  to obtain a further polynomial, which we will denote by s x , together with a function t x  that is a ratio of two polynomials for which the degree of the numerator is less than that of the denominator. The function t x  can therefore be expanded in partial fractions. As a formula,  f x  =  g x  h x   = s x  + t x  ≡ s x  +  r x  h x   .  It is apparent that the polynomial r x  is the remainder obtained when g x  is  divided by h x , and, in general, will be a polynomial of degree n − 1. It is also clear that the polynomial s x  will be of degree m − n. Again, the actual division  process can be set out as an algebraic long division sum but is probably more easily handled by writing  1.45  in the form  g x  = s x h x  + r x   or, more explicitly, as  g x  =  sm−nxm−n + sm−n−1xm−n−1 + ··· + s0 h x  +  rn−1xn−1 + rn−2xn−2 + ··· + r0    1.45    1.46    1.47   and then equating coeﬃcients.  21   PRELIMINARY ALGEBRA  We illustrate this procedure with the following worked example.  cid:1 Find the partial fraction decomposition of the function x3 + 3x2 + 2x + 1 x2 − x − 6  f x  =  .  Since the degree of the numerator is 3 and that of the denominator is 2, a preliminary long division is necessary. The polynomial s x  resulting from the division will have degree  3 − 2 = 1 and the remainder r x  will be of degree 2 − 1 = 1  or less . Thus we write  x3 + 3x2 + 2x + 1 =  s1x + s0  x2 − x − 6  +  r1x + r0 .  From equating the coeﬃcients of the various powers of x on the two sides of the equation, starting with the highest, we now obtain the simultaneous equations  1 = s1,  3 = s0 − s1, 2 = −s0 − 6s1 + r1, 1 = −6s0 + r0.  f x  = x + 4 +  12x + 25  x2 − x − 6  .  These are readily solved, in the given order, to yield s1 = 1, s0 = 4, r1 = 12 and r0 = 25. Thus f x  can be written as  The last term can now be decomposed into partial fractions as previously. The zeros of  the denominator are at x = 3 and x = −2 and the application of any method from the previous subsection yields the respective constants as A1 = 12 1 5 . Thus the ﬁnal partial fraction decomposition of f x  is  5 and A2 = − 1  x + 4 +  −  61  5 x − 3   1  .  cid:2   5 x + 2   Factors of the form a2 + x2 in the denominator  We have so far assumed that the roots of h x  = 0, needed for the factorisation of the denominator of f x , can always be found. In principle they always can but in some cases they are not real. Consider, for example, attempting to express in  partial fractions a polynomial ratio whose denominator is h x  = x3− x2 + 2x− 2. Clearly x = 1 is a zero of h x , and so a ﬁrst factorisation is  x − 1  x2 + 2 . However we cannot make any further progress because the factor x2 + 2 cannot be expressed as  x − α  x − β  for any real α and β.  Complex numbers are introduced later in this book  chapter 3  and, when the reader has studied them, he or she may wish to justify the procedure set out below. It can be shown to be equivalent to that already given, but the zeros of h x  are now allowed to be complex and terms that are complex conjugates of each other are combined to leave only real terms.  Since quadratic factors of the form a2+x2 that appear in h x  cannot be reduced to the product of two linear factors, partial fraction expansions including them need to have numerators in the corresponding terms that are not simply constants  22   1.4 PARTIAL FRACTIONS  Ai but linear functions of x, i.e. of the form Bix + Ci. Thus, in the expansion, linear terms  ﬁrst-degree polynomials  in the denominator have constants  zero- degree polynomials  in their numerators, whilst quadratic terms  second-degree polynomials  in the denominator have linear terms  ﬁrst-degree polynomials  in their numerators. As a symbolic formula, the partial fraction expansion of   x − α1  x − α2 ···  x − αp  x2 + a2  g x   1  x2 + a2  2 ···  x2 + a2 q   should take the form  A1  x − α1  +  A2  x − α2  + ··· +  Ap  x − αp  +  B1x + C1 x2 + a2 1  +  B2x + C2 x2 + a2 2  + ··· +  Bqx + Cq x2 + a2 q  .  Of course, the degree of g x  must be less than p + 2q; if it is not, an initial division must be carried out as demonstrated earlier.  Consider trying  incorrectly  to expand  Repeated factors in the denominator  x − 4   x + 1  x − 2 2  f x  =  in partial fraction form as follows:  x − 4   x + 1  x − 2 2 =  A1  x + 1  +  A2   x − 2 2 .  Multiplying both sides of this supposed equality by  x + 1  x − 2 2 produces an  equation whose LHS is linear in x, whilst its RHS is quadratic. This is clearly wrong and so an expansion in the above form cannot be valid. The correction we must make is very similar to that needed in the previous subsection, namely that  since  x − 2 2 is a quadratic polynomial the numerator of the term containing it  must be a ﬁrst-degree polynomial, and not simply a constant.  The correct form for the part of the expansion containing the doubly repeated  root is therefore  Bx + C   x− 2 2. Using this form and either of methods  i  and   ii  for determining the constants gives the full partial fraction expansion as  x − 4   x + 1  x − 2 2 = −  5  9 x + 1   5x − 16 9 x − 2 2 ,  +  as the reader may verify.  Since any term of the form  Bx + C   x − α 2 can be written as  B x − α  + C + Bα   x − α 2  =  B  x − α  +  C + Bα   x − α 2 ,  and similarly for multiply repeated roots, an alternative form for the part of the partial fraction expansion containing a repeated root α is  D1  x − α  +  D2   x − α 2 + ··· +  Dp   x − α p .   1.48   23   PRELIMINARY ALGEBRA  In this form, all x-dependence has disappeared from the numerators but at the  expense of p− 1 additional terms; the total number of constants to be determined  remains unchanged, as it must.  When describing possible methods of determining the constants in a partial fraction expansion, we noted that method  iii , p. 20, which avoids the need to solve simultaneous equations, is restricted to terms involving non-repeated roots. In fact, it can be applied in repeated-root situations, when the expansion is put in the form  1.48 , but only to ﬁnd the constant in the term involving the largest  inverse power of x − α, i.e. Dp in  1.48 .  We conclude this section with a more protracted worked example that contains  all three of the complications discussed.   cid:1 Resolve the following expression F x  into partial fractions:  x5 − 2x4 − x3 + 5x2 − 46x + 100   x2 + 6  x − 2 2  .  F x  =  We note that the degree of the denominator  4  is not greater than that of the numerator  5 , and so we must start by dividing the latter by the former. It follows, from the diﬀerence in degrees and the coeﬃcients of the highest powers in each, that the result will be a linear expression s1x + s0 with the coeﬃcient s1 equal to 1. Thus the numerator of F x  must be expressible as   x + s0  x4 − 4x3 + 10x2 − 24x + 24  +  r3x3 + r2x2 + r1x + r0 ,  where the second factor in parentheses is the denominator of F x  written as a polynomial.  Equating the coeﬃcients of x4 gives −2 = −4+s0 and ﬁxes s0 as 2. Equating the coeﬃcients  of powers less than 4 gives equations involving the coeﬃcients ri as follows:  −1 = −8 + 10 + r3, 5 = −24 + 20 + r2, −46 = 24 − 48 + r1,  100 = 48 + r0.  Thus the remainder polynomial r x  can be constructed and F x  written as  F x  = x + 2 +  −3x3 + 9x2 − 22x + 52   x2 + 6  x − 2 2  ≡ x + 2 + f x .  The polynomial ratio f x  can now be expressed in partial fraction form, noting that its denominator contains both a term of the form x2 + a2 and a repeated root. Thus  f x  =  Bx + C x2 + 6  +  D1  x − 2  +  D2   x − 2 2 .  We could now put the RHS of this equation over the common denominator  x2 + 6  x− 2 2 to use methods  iii  and  ii . Method  iii  gives D2 as  −24 + 36 − 44 + 52   4 + 6  = 2.  and ﬁnd B, C, D1 and D2 by equating coeﬃcients of powers of x. It is quicker, however,  We choose to evaluate the other coeﬃcients by method  ii , and setting x = 0, x = 1 and  24   1.5 BINOMIAL EXPANSION  x = −1 gives respectively  52 24 36 7 86 63  =  =  =  − D1 C 6 2 B + C  7  C − B  7  +  2 4  ,  − D1 + 2, − D1 3  +  2 9  .  4C − 12D1 = 40, B + C − 7D1 = 22, −9B + 9C − 21D1 = 72,  These equations reduce to  with solution B = 0, C = 1, D1 = −3.  Thus, ﬁnally, we may rewrite the original expression F x  in partial fractions as  F x  = x + 2 +  1  x2 + 6  − 3 x − 2  +  2   x − 2 2 .  cid:2   1.5 Binomial expansion  Earlier in this chapter we were led to consider functions containing powers of  the sum or diﬀerence of two terms, e.g.  x − α m. Later in this book we will ﬁnd  numerous occasions on which we wish to write such a product of repeated factors as a polynomial in x or, more generally, as a sum of terms each of which contains powers of x and α separately, as opposed to a power of their sum or diﬀerence. To make the discussion general and the result applicable to a wide variety of situations, we will consider the general expansion of f x  =  x + y n, where x and y may stand for constants, variables or functions and, for the time being, n is a positive integer. It may not be obvious what form the general expansion takes but some idea can be obtained by carrying out the multiplication explicitly for small values of n. Thus we obtain successively   x + y 1 = x + y,  x + y 2 =  x + y  x + y  = x2 + 2xy + y2,  x + y 3 =  x + y  x2 + 2xy + y2  = x3 + 3x2y + 3xy2 + y3,  x + y 4 =  x + y  x3 + 3x2y + 3xy2 + y3  = x4 + 4x3y + 6x2y2 + 4xy3 + y4.  This does not establish a general formula, but the regularity of the terms in the expansions and the suggestion of a pattern in the coeﬃcients indicate that a general formula for power n will have n + 1 terms, that the powers of x and y in every term will add up to n and that the coeﬃcients of the ﬁrst and last terms will be unity whilst those of the second and penultimate terms will be n.  25   PRELIMINARY ALGEBRA  In fact, the general expression, the binomial expansion for power n, is given by   x + y n =  nCkxn−kyk,   1.49   where nCk is called the binomial coeﬃcient and is expressed in terms of factorial functions by n! [k! n − k !]. Clearly, simply to make such a statement does not  constitute proof of its validity, but, as we will see in subsection 1.5.2,  1.49  can be proved using a method called induction. Before turning to that proof, we investigate some of the elementary properties of the binomial coeﬃcients.  1.5.1 Binomial coefﬁcients  As stated above, the binomial coeﬃcients are deﬁned by  nCk ≡  n!  k! n − k !  ≡  for 0 ≤ k ≤ n,   1.50   where in the second identity we give a common alternative notation for nCk. Obvious properties include   i  nC0 = nCn = 1,  ii  nC1 = nCn−1 = n,  iii  nCk = nCn−k.  We note that, for any given n, the largest coeﬃcient in the binomial expansion is the middle one  k = n 2  if n is even; the middle two coeﬃcients  k = 1 are equal largest if n is odd. Somewhat less obvious is the result  2  n ± 1    nCk + nCk−1 =  n!   k − 1 ! n − k + 1 !  n!  +  k! n − k ! n![ n + 1 − k  + k] k! n + 1 − k ! k! n + 1 − k !   n + 1 !  =  =  = n+1Ck.  nCk + nCk+1 = n+1Ck+1.  1.5.2 Proof of the binomial expansion   1.51    1.52   An equivalent statement, in which k has been redeﬁned as k + 1, is  We are now in a position to prove the binomial expansion  1.49 . In doing so, we introduce the reader to a procedure applicable to certain types of problems and known as the method of induction. The method is discussed much more fully in subsection 1.7.1.  k=n cid:4   k=0   cid:7    cid:8   n k  26   We start by assuming that  1.49  is true for some positive integer n = N. We now proceed to show that this implies that it must also be true for n = N+1, as follows:  1.6 PROPERTIES OF BINOMIAL COEFFICIENTS  N cid:4   k=0   x + y N+1 =  x + y   N cid:4  N cid:4   k=0  k=0  =  =  NCkxN−kyk N cid:4  N+1 cid:4   k=0  j=1  NCkxN+1−kyk +  NCkxN−kyk+1  NCkxN+1−kyk +  NCj−1x N+1 −jyj,   cid:5   N cid:4  N cid:4   k=1  k=1  where in the ﬁrst line we have used the assumption and in the third line have moved the second summation index by unity, by writing k + 1 = j. We now separate oﬀ the ﬁrst term of the ﬁrst sum, NC0xN+1, and write it as N+1C0xN+1; we can do this since, as noted in  i  following  1.50 , nC0 = 1 for every n. Similarly, the last term of the second summation can be replaced by N+1CN+1yN+1.  The remaining terms of each of the two summations are now written together,  with the summation index denoted by k in both terms. Thus   x + y N+1 = N+1C0xN+1 +  NCk + NCk−1  x N+1 −kyk + N+1CN+1yN+1   cid:6   = N+1C0xN+1 +  N+1Ckx N+1 −kyk + N+1CN+1yN+1  N+1 cid:4   k=0  =  N+1Ckx N+1 −kyk.  In going from the ﬁrst to the second line we have used result  1.51 . Now we observe that the ﬁnal overall equation is just the original assumed result  1.49  but with n = N + 1. Thus it has been shown that if the binomial expansion is assumed to be true for n = N, then it can be proved to be true for n = N + 1. But it holds trivially for n = 1, and therefore for n = 2 also. By the same token it is valid for n = 3, 4, . . . , and hence is established for all positive integers n.  1.6 Properties of binomial coeﬃcients  1.6.1 Identities involving binomial coefﬁcients  There are many identities involving the binomial coeﬃcients that can be derived directly from their deﬁnition, and yet more that follow from their appearance in the binomial expansion. Only the most elementary ones, given earlier, are worth committing to memory but, as illustrations, we now derive two results involving sums of binomial coeﬃcients.  27   PRELIMINARY ALGEBRA  The ﬁrst is a further application of the method of induction. Consider the  proposal that, for any n ≥ 1 and k ≥ 0,  k+sCk = n+kCk+1.   1.53   n−1 cid:4   s=0  Notice that here n, the number of terms in the sum, is the parameter that varies, k is a ﬁxed parameter, whilst s is a summation index and does not appear on the RHS of the equation.  Now we suppose that the statement  1.53  about the value of the sum of the  binomial coeﬃcients kCk, k+1Ck, . . . , k+n−1Ck is true for n = N. We next write down  a series with an extra term and determine the implications of the supposition for the new series:  N+1−1 cid:4   s=0  N−1 cid:4   s=0  k+sCk =  k+sCk + k+NCk  = N+kCk+1 + N+kCk = N+k+1Ck+1.  But this is just proposal  1.53  with n now set equal to N + 1. To obtain the last line, we have used  1.52 , with n set equal to N + k.  It only remains to consider the case n = 1, when the summation only contains  one term and  1.53  reduces to  kCk = 1+kCk+1.  This is trivially valid for any k since both sides are equal to unity, thus completing the proof of  1.53  for all positive integers n.  The second result, which gives a formula for combining terms from two sets of binomial coeﬃcients in a particular way  a kind of ‘convolution’, for readers who are already familiar with this term , is derived by applying the binomial expansion directly to the identity  Written in terms of binomial expansions, this reads   x + y p x + y q ≡  x + y p+q.  p cid:4   s=0  q cid:4   t=0  pCsxp−sys  qCtxq−tyt =  p+qCrxp+q−ryr.  We now equate coeﬃcients of xp+q−ryr on the two sides of the equation, noting that on the LHS all combinations of s and t such that s + t = r contribute. This gives as an identity that  pCr−t  qCt = p+qCr =  pCt  qCr−t.   1.54   r cid:4   t=0  p+q cid:4   r=0  r cid:4   t=0  28   1.6 PROPERTIES OF BINOMIAL COEFFICIENTS  We have speciﬁcally included the second equality to emphasise the symmetrical nature of the relationship with respect to p and q.  Further identities involving the coeﬃcients can be obtained by giving x and y special values in the deﬁning equation  1.49  for the expansion. If both are set equal to unity then we obtain  using the alternative notation so as to produce   cid:8    cid:8   familiarity with it   cid:7   cid:7   cid:8    cid:7   cid:7   cid:8  whilst setting x = 1 and y = −1 yields   cid:7   cid:8    cid:7   n 0  n 1  +  +  n 2   cid:8   + ··· +  −  n 0  n 1  +  n 2  − ··· +  −1 n   cid:7    cid:8   cid:7   n n  = 2n,   cid:8   n n  = 0.   1.55    1.56   1.6.2 Negative and non-integral values of n  Up till now we have restricted n in the binomial expansion to be a positive integer. Negative values can be accommodated, but only at the cost of an inﬁnite series of terms rather than the ﬁnite one represented by  1.49 . For reasons that are intuitively sensible and will be discussed in more detail in chapter 4, very often we require an expansion in which, at least ultimately, successive terms in the inﬁnite series decrease in magnitude. For this reason, if x > y we consider  x + y   −m, where m itself is a positive integer, in the form   x + y n =  x + y   −m = x  −m  1 +  y x   cid:9    cid:10 −m  .  Since the ratio y x is less than unity, terms containing higher powers of it will be small in magnitude, whilst raising the unit term to any power will not aﬀect its magnitude. If y > x the roles of the two must be interchanged.  We can now state, but will not explicitly prove, the form of the binomial   cid:10  expansion appropriate to negative values of n  n equal to −m :   cid:9   ∞ cid:4    x + y n =  x + y   −m = x  −m  −mCk   1.57   k  ,  y x  where the hitherto undeﬁned quantity of negative numbers, is given by  −mCk =  −1 k m m + 1 ···  m + k − 1   k!  k=0  −mCk, which appears to involve factorials =  −1 k  m + k − 1 !  m − 1 !k!  =  −1 k m+k−1Ck.   1.58   The binomial coeﬃcient on the extreme right of this equation has its normal  meaning and is well deﬁned since m + k − 1 ≥ k.  Thus we have a deﬁnition of binomial coeﬃcients for negative integer values of n in terms of those for positive n. The connection between the two may not  29   PRELIMINARY ALGEBRA  be obvious, but they are both formed in the same way in terms of recurrence relations. Whatever the sign of n, the series of coeﬃcients nCk can be generated by starting with nC0 = 1 and using the recurrence relation  nCk+1 =  nCk.  n − k  k + 1   1.59   The diﬀerence is that for positive integer n the series terminates when k = n, whereas for negative n there is no such termination – in line with the inﬁnite series of terms in the corresponding expansion.  Finally we note that, in fact, equation  1.59  generates the appropriate coef- ﬁcients for all values of n, positive or negative, integer or non-integer, with the  obvious exception of the case in which x = −y and n is negative. For non-integer  n the expansion does not terminate, even if n is positive.  1.7 Some particular methods of proof  Much of the mathematics used by physicists and engineers is concerned with obtaining a particular value, formula or function from a given set of data and stated conditions. However, just as it is essential in physics to formulate the basic laws and so be able to set boundaries on what can or cannot happen, so it is important in mathematics to be able to state general propositions about the outcomes that are or are not possible. To this end one attempts to establish theorems that state in as general a way as possible mathematical results that apply to particular types of situation. We conclude this introductory chapter by describing two methods that can sometimes be used to prove particular classes of theorems.  The two general methods of proof are known as proof by induction  which has already been met in this chapter  and proof by contradiction. They share the common characteristic that at an early stage in the proof an assumption is made that a particular  unproven  statement is true; the consequences of that assumption are then explored. In an inductive proof the conclusion is reached that the assumption is self-consistent and has other equally consistent but broader implications, which are then applied to establish the general validity of the assumption. A proof by contradiction, however, establishes an internal inconsistency and thus shows that the assumption is unsustainable; the natural consequence of this is that the negative of the assumption is established as true. Later in this book use will be made of these methods of proof to explore new territory, e.g. to examine the properties of vector spaces, matrices and groups. However, at this stage we will draw our illustrative and test examples from earlier sections of this chapter and other topics in elementary algebra and number theory.  30   1.7 SOME PARTICULAR METHODS OF PROOF  1.7.1 Proof by induction  The proof of the binomial expansion given in subsection 1.5.2 and the identity established in subsection 1.6.1 have already shown the way in which an inductive proof is carried through. They also indicated the main limitation of the method, namely that only an initially supposed result can be proved. Thus the method of induction is of no use for deducing a previously unknown result; a putative equation or result has to be arrived at by some other means, usually by noticing patterns or by trial and error using simple values of the variables involved. It will also be clear that propositions that can be proved by induction are limited to those containing a parameter that takes a range of integer values  usually inﬁnite .  For a proposition involving a parameter n, the ﬁve steps in a proof using  induction are as follows.   i  Formulate the supposed result for general n.  ii  Suppose  i  to be true for n = N  or more generally for all values of  n ≤ N; see below , where N is restricted to lie in the stated range.   iii  Show, using only proven results and supposition  ii , that proposition  i   is true for n = N + 1.   iv  Demonstrate directly, and without any assumptions, that proposition  i  is  true when n takes the lowest value in its range.   v  It then follows from  iii  and  iv  that the proposition is valid for all values  of n in the stated range.   It should be noted that, although many proofs at stage  iii  require the validity of the proposition only for n = N, some require it for all n less than or equal to N – hence the form of inequality given in parentheses in the stage  ii  assumption.  To illustrate further the method of induction, we now apply it to two worked examples; the ﬁrst concerns the sum of the squares of the ﬁrst n natural numbers.  cid:1 Prove that the sum of the squares of the ﬁrst n natural numbers is given by  r2 = 1  6 n n + 1  2n + 1 .   1.60   As previously we start by assuming the result is true for n = N. Then it follows that  n cid:4   r=1  N cid:4   N+1 cid:4   r2 =  r2 +  N + 1 2  r=1  r=1  = 1 = 1 = 1 = 1  6 N N + 1  2N + 1  +  N + 1 2 6  N + 1 [N 2N + 1  + 6N + 6] 6  N + 1 [ 2N + 3  N + 2 ] 6  N + 1 [ N + 1  + 1][2 N + 1  + 1].  31   PRELIMINARY ALGEBRA  This is precisely the original assumption, but with N replaced by N + 1. To complete the proof we only have to verify  1.60  for n = 1. This is trivially done and establishes the result for all positive n. The same and related results are obtained by a diﬀerent method in subsection 4.2.5.  cid:2   Our second example is somewhat more complex and involves two nested proofs by induction: whilst trying to establish the main result by induction, we ﬁnd that we are faced with a second proposition which itself requires an inductive proof.  cid:1 Show that Q n  = n4 + 2n3 + 2n2 + n is divisible by 6  without remainder  for all positive integer values of n.  Again we start by assuming the result is true for some particular value N of n, whilst noting that it is trivially true for n = 0. We next examine Q N + 1 , writing each of its terms as a binomial expansion:  Q N + 1  =  N + 1 4 + 2 N + 1 3 + 2 N + 1 2 +  N + 1   =  N4 + 4N3 + 6N2 + 4N + 1  + 2 N3 + 3N2 + 3N + 1   + 2 N2 + 2N + 1  +  N + 1   =  N4 + 2N3 + 2N2 + N  +  4N3 + 12N2 + 14N + 6 .  Now, by our assumption, the group of terms within the ﬁrst parentheses in the last line is divisible by 6 and clearly so are the terms 12N2 and 6 within the second parentheses. Thus it comes down to deciding whether 4N3 + 14N is divisible by 6 – or equivalently, whether R N  = 2N3 + 7N is divisible by 3.  To settle this latter question we try using a second inductive proof and assume that R N  is divisible by 3 for N = M, whilst again noting that the proposition is trivially true for N = M = 0. This time we examine R M + 1 :  R M + 1  = 2 M + 1 3 + 7 M + 1   = 2 M3 + 3M2 + 3M + 1  + 7 M + 1  =  2M3 + 7M  + 3 2M2 + 2M + 3   By assumption, the ﬁrst group of terms in the last line is divisible by 3 and the second  group is patently so. We thus conclude that R N  is divisible by 3 for all N ≥ M, and  taking M = 0 shows that it is divisible by 3 for all N.  We can now return to the main proposition and conclude that since R N  = 2N3 + 7N is divisible by 3, 4N3 + 12N2 + 14N + 6 is divisible by 6. This in turn establishes that the divisibility of Q N + 1  by 6 follows from the assumption that Q N  divides by 6. Since Q 0  clearly divides by 6, the proposition in the question is established for all values of n.  cid:2   1.7.2 Proof by contradiction  The second general line of proof, but again one that is normally only useful when the result is already suspected, is proof by contradiction. The questions it can attempt to answer are only those that can be expressed in a proposition that is either true or false. Clearly, it could be argued that any mathematical result can be so expressed but, if the proposition is no more than a guess, the chances of success are negligible. Valid propositions containing even modest formulae are either the result of true inspiration or, much more normally, yet another reworking of an old chestnut!  32   1.7 SOME PARTICULAR METHODS OF PROOF  The essence of the method is to exploit the fact that mathematics is required to be self-consistent, so that, for example, two calculations of the same quantity, starting from the same given data but proceeding by diﬀerent methods, must give the same answer. Equally, it must not be possible to follow a line of reasoning and draw a conclusion that contradicts either the input data or any other conclusion based upon the same data.  It is this requirement on which the method of proof by contradiction is based. The crux of the method is to assume that the proposition to be proved is not true, and then use this incorrect assumption and ‘watertight’ reasoning to draw a conclusion that contradicts the assumption. The only way out of the self-contradiction is then to conclude that the assumption was indeed false and therefore that the proposition is true.  It must be emphasised that once a  false  contrary assumption has been made, every subsequent conclusion in the argument must follow of necessity. Proof by contradiction fails if at any stage we have to admit ‘this may or may not be the case’. That is, each step in the argument must be a necessary consequence of results that precede it  taken together with the assumption , rather than simply a possible consequence.  It should also be added that if no contradiction can be found using sound reasoning based on the assumption then no conclusion can be drawn about either the proposition or its negative and some other approach must be tried.  We illustrate the general method with an example in which the mathematical reasoning is straightforward, so that attention can be focussed on the structure of the proof.   cid:1 A rational number r is a fraction r = p q in which p and q are integers with q positive. Further, r is expressed in its lowest terms, any integer common factor of p and q having been divided out.  Prove that the square root of an integer m cannot be a rational number, unless the square  root itself is an integer.  We begin by supposing that the stated result is not true and that we can write an equation  √  m = r =  p q  for integers m, p, q with q  cid:3 = 1.  It then follows that p2 = mq2. But, since r is expressed in its lowest terms, p and q, and hence p2 and q2, have no factors in common. However, m is an integer; this is only possible if q = 1 and p2 = m. This conclusion contradicts the requirement that q  cid:3 = 1 and so leads to the conclusion that it was wrong to suppose that rational number. This completes the proof of the statement in the question.  cid:2   √ m can be expressed as a non-integer  Our second worked example, also taken from elementary number theory, involves slightly more complicated mathematical reasoning but again exhibits the structure associated with this type of proof.  33   PRELIMINARY ALGEBRA   cid:1 The prime integers pi are labelled in ascending order, thus p1 = 1, p2 = 2, p5 = 7, etc. Show that there is no largest prime number.  Assume, on the contrary, that there is a largest prime and let it be pN. Consider now the number q formed by multiplying together all the primes from p1 to pN and then adding one to the product, i.e.  q = p1p2 ··· pN + 1.  By our assumption pN is the largest prime, and so no number can have a prime factor greater than this. However, for every prime pi, i = 1, 2, . . . , N, the quotient q pi has the form Mi +  1 pi  with Mi an integer and 1 pi non-integer. This means that q pi cannot be an integer and so pi cannot be a divisor of q.  Since q is not divisible by any of the  assumed  ﬁnite set of primes, it must be itself a prime. As q is also clearly greater than pN, we have a contradiction. This shows that our assumption that there is a largest prime integer must be false, and so it follows that there is no largest prime integer.  It should be noted that the given construction for q does not generate all the primes that actually exist  e.g. for N = 3, q = 7 rather than the next actual prime value of 5, is found , but this does not matter for the purposes of our proof by contradiction.  cid:2   1.7.3 Necessary and sufﬁcient conditions  As the ﬁnal topic in this introductory chapter, we consider brieﬂy the notion of, and distinction between, necessary and suﬃcient conditions in the context of proving a mathematical proposition. In ordinary English the distinction is well deﬁned, and that distinction is maintained in mathematics. However, in the authors’ experience students tend to overlook it and assume  wrongly  that, having proved that the validity of proposition A implies the truth of proposition B, it follows by ‘reversing the argument’ that the validity of B automatically implies that of A.  As an example, let proposition A be that an integer N is divisible without remainder by 6, and proposition B be that N is divisible without remainder by 2. Clearly, if A is true then it follows that B is true, i.e. A is a suﬃcient condition for B; it is not however a necessary condition, as is trivially shown by taking N as 8. Conversely, the same value of N shows that whilst the validity of B is a necessary condition for A to hold, it is not suﬃcient.  An alternative terminology to ‘necessary’ and ‘suﬃcient’ often employed by mathematicians is that of ‘if’ and ‘only if’, particularly in the combination ‘if and only if’ which is usually written as IFF or denoted by a double-headed arrow  ⇐⇒ . The equivalent statements can be summarised by  A if B  A is true if B is true or B is a suﬃcient condition for A  B =⇒ A, B =⇒ A, A =⇒ B, B is a necessary consequence of A A =⇒ B,  A only if B A is true only if B is true or  34   1.7 SOME PARTICULAR METHODS OF PROOF  A IFF B A is true if and only if B is true or  B ⇐⇒ A, A and B necessarily imply each other B ⇐⇒ A.  Although at this stage in the book we are able to employ for illustrative purposes only simple and fairly obvious results, the following example is given as a model of how necessary and suﬃcient conditions should be proved. The essential point is that for the second part of the proof  whether it be the ‘necessary’ part or the ‘suﬃcient’ part  one needs to start again from scratch; more often than not, the lines of the second part of the proof will not be simply those of the ﬁrst written in reverse order.  cid:1 Prove that  A  a function f x  is a quadratic polynomial with zeros at x = 2 and x = 3 if and only if  B  the function f x  has the form λ x2− 5x + 6  with λ a non-zero constant.   1  Assume A, i.e. that f x  is a quadratic polynomial with zeros at x = 2 and x = 3. Let  its form be ax2 + bx + c with a  cid:3 = 0. Then we have  4a + 2b + c = 0, 9a + 3b + c = 0,  and subtraction shows that 5a + b = 0 and b = −5a. Substitution of this into the ﬁrst of the above equations gives c = −4a − 2b = −4a + 10a = 6a. Thus, it follows that  f x  = a x2 − 5x + 6  with a  cid:3 = 0,  and establishes the ‘A only if B’ part of the stated result.   2  Now assume that f x  has the form λ x2 − 5x + 6  with λ a non-zero constant. Firstly  we note that f x  is a quadratic polynomial, and so it only remains to prove that its zeros occur at x = 2 and x = 3. Consider f x  = 0, which, after dividing through by the non-zero constant λ, gives  x2 − 5x + 6 = 0.  We proceed by using a technique known as completing the square, for the purposes of illustration, although the factorisation of the above equation should be clear to the reader. Thus we write  x2 − 5x +   5  2  2 −   5  2  2 + 6 = 0,  x − 5 2  2 = 1 4 , 2 = ± 1 x − 5  2 .  The two roots of f x  = 0 are therefore x = 2 and x = 3; these x-values give the zeros of f x . This establishes the second  ‘A if B’  part of the result. Thus we have shown that the assumption of either condition implies the validity of the other and the proof is complete.  cid:2   It should be noted that the propositions have to be carefully and precisely formulated. If, for example, the word ‘quadratic’ were omitted from A, statement B would still be a suﬃcient condition for A but not a necessary one, since f x   could then be x3 − 4x2 + x + 6 and A would not require B. Omitting the constant state that f x  = 3 x− 2  x− 3  then B would be a necessary condition for A but  λ from the stated form of f x  in B has the same eﬀect. Conversely, if A were to  not a suﬃcient one.  35   PRELIMINARY ALGEBRA  1.8 Exercises  Polynomial equations Continue the investigation of equation  1.7 , namely  1.1  g x  = 4x3 + 3x2 − 6x − 1,  as follows.   a  Make a table of values of g x  for integer values of x between −2 and 2. Use  it and the information derived in the text to draw a graph and so determine the roots of g x  = 0 as accurately as possible.   b  Find one accurate root of g x  = 0 by inspection and hence determine precise  values for the other two roots.   c  Show that f x  = 4x3 + 3x2 − 6x − k = 0 has only one real root unless  −5 ≤ k ≤ 7 4 .  1.2  Determine how the number of real roots of the equation  g x  = 4x3 − 17x2 + 10x + k = 0  depends upon k. Are there any cases for which the equation has exactly two distinct real roots? Continue the analysis of the polynomial equation  1.3  f x  = x7 + 5x6 + x4 − x3 + x2 − 2 = 0,  investigated in subsection 1.1.1, as follows.   a  By writing the ﬁfth-degree polynomial appearing in the expression for f  in the form 7x5 + 30x4 + a x − b 2 + c, show that there is in fact only one  b  By evaluating f 1 , f 0  and f −1 , and by inspecting the form of f x  for  positive root of f x  = 0.   x   negative values of x, determine what you can about the positions of the real roots of f x  = 0.   cid:7   1.4  Given that x = 2 is one root of  g x  = 2x4 + 4x3 − 9x2 − 11x − 6 = 0,  1.5  1.6  use factorisation to determine how many real roots it has. Construct the quadratic equations that have the following pairs of roots:   a  −6,−3;  b  0, 4;  c  2, 2;  d  3 + 2i, 3 − 2i, where i2 = −1. to prove that if the roots of 3x3 − x2 − 10x + 8 = 0 are α1, α2 and α3 then  Use the results of  i  equation  1.13 ,  ii  equation  1.12  and  iii  equation  1.14   −1 1 + α  a  α 1 + α2  b  α2  c  α3 1 + α3  d  Convince yourself that eliminating  say  α2 and α3 from  i ,  ii  and  iii  does  −1 3 = 5 4, 3 = 61 9, 3 = −125 27.  −1 2 + α 2 + α2 2 + α3  not give a simple explicit way of ﬁnding α1.  1.7  Prove that  by considering  Trigonometric identities  √ √ 3 + 1 2  2  cos  =  π 12  36   1.8 EXERCISES   a  the sum of the sines of π 3 and π 6,  b  the sine of the sum of π 3 and π 4.  The following exercises are based on the half-angle formulae.   a  Use the fact that sin π 6  = 1 2 to prove that tan π 12  = 2 − √  b  Use the result of  a  to show further that tan π 24  = q 2 − q  where  3.  √  1.8  1.9  q2 = 2 +  3.  Find the real solutions of   a  3 sin θ − 4 cos θ = 2,  c  12 sin θ − 5 cos θ = −6.   b  4 sin θ + 3 cos θ = 6,  1.10  If s = sin π 8 , prove that  and hence show that s = [ 2 − √  8s4 − 8s2 + 1 = 0, 2  4]1 2.  1.11  Find all the solutions of  that lie in the range −π < θ ≤ π. What is the multiplicity of the solution θ = 0?  sin θ + sin 4θ = sin 2θ + sin 3θ  Coordinate geometry  1.12  Obtain in the form  1.38  the equations that describe the following:   a  a circle of radius 5 with its centre at  1,−1 ;   b  the line 2x + 3y + 4 = 0 and the line orthogonal to it which passes through   c  an ellipse of eccentricity 0.6 with centre  1, 1  and its major axis of length 10   1, 1 ;  parallel to the y-axis.  1.13  Determine the forms of the conic sections described by the following equations:   a  x2 + y2 + 6x + 8y = 0;  b  9x2 − 4y2 − 54x − 16y + 29 = 0;  c  2x2 + 2y2 + 5xy − 4x + y − 6 = 0;  d  x2 + y2 + 2xy − 8x + 8y = 0.  1.14  For the ellipse  x2 a2  y2 b2  +  = 1  with eccentricity e, the two points  −ae, 0  and  ae, 0  are known as its foci. Show  that the sum of the distances from any point on the ellipse to the foci is 2a.  The constancy of the sum of the distances from two ﬁxed points can be used as an alternative deﬁning property of an ellipse.   1.15  Resolve the following into partial fractions using the three methods given in section 1.4, verifying that the same decomposition is obtained by each method:  Partial fractions   a   2x + 1  x2 + 3x − 10  ,   b   4  x2 − 3x  .  37   PRELIMINARY ALGEBRA  1.16  Express the following in partial fraction form:  1.17  Rearrange the following functions in partial fraction form:   a   ,  2x3 − 5x + 1 x2 − 2x − 8 x − 6   a   x3 − x2 + 4x − 4  ,  x2 + x − 1 x2 + x − 2  .   b    b   x3 + 3x2 + x + 19  x4 + 10x2 + 9  .  1.18  Resolve the following into partial fractions in such a way that x does not appear in any numerator:   a   2x2 + x + 1  x − 1 2 x + 3   ,   b   x3 + 8x2 + 16x  ,   c   x2 − 2  x3 − x − 1   x + 3 3 x + 1   .  1.19  1.20  Evaluate those of the following that are deﬁned:  a  5C3,  b  3C5,  c  −3C5. Use a binomial expansion to evaluate 1  compare it with the accurate answer obtained using a calculator.  √ 4.2 to ﬁve places of decimals, and  −5C3,  d   Binomial expansion  1.21  Prove by induction that  Proof by induction and contradiction  r = 1  2 n n + 1   and  r3 = 1  4 n2 n + 1 2.  n cid:4   r=1  n cid:4   r=1  1.22  Prove by induction that  1 + r + r2 + ··· + rk + ··· + rn =  1 − rn+1 1 − r  .  1.23 1.24  1.25  Prove that 32n + 7, where n is a non-negative integer, is divisible by 8. If a sequence of terms, un, satisﬁes the recurrence relation un+1 =  1 − x un + nx, with u1 = 0, show, by induction, that, for n ≥ 1,  cid:7   [nx − 1 +  1 − x n].  cid:8   cid:8   Prove by induction that   cid:7   un =  1 x  1 2r  tan  θ 2r  =  cot  − cot θ.  n cid:4   r=1  1.26  The quantities ai in this exercise are all positive real numbers.   a  Show that  1 2n   cid:7   θ 2n   cid:8   a1 + a2  2  2  .   b  Hence prove, by induction on m, that  a1a2 ··· ap ≤  a1 + a2 + ··· + ap  p   cid:8   p  ,  where p = 2m with m a positive integer. Note that each increase of m by unity doubles the number of factors in the product.  a1a2 ≤  cid:7   38   1.9 HINTS AND ANSWERS  1.27  1.28  Establish the values of k for which the binomial coeﬃcient pCk is divisible by p that np − n is divisible by p for all integers n and all prime numbers p. Deduce when p is a prime number. Use your result and the method of induction to prove that n5 − n is divisible by 30 for any integer n.  An arithmetic progression of integers an is one in which an = a0 + nd, where a0 and d are integers and n takes successive values 0, 1, 2, . . . .   a  Show that if any one term of the progression is the cube of an integer then   b  Show that no cube of an integer can be expressed as 7n + 5 for some positive  so are inﬁnitely many others.  integer n.  1.29  Prove, by the method of contradiction, that the equation  xn + an−1xn−1 + ··· + a1x + a0 = 0,  in which all the coeﬃcients ai are integers, cannot have a rational root, unless that root is an integer. Deduce that any integral root must be a divisor of a0 and hence ﬁnd all rational roots of   a  x4 + 6x3 + 4x2 + 5x + 4 = 0,  b  x4 + 5x3 + 2x2 − 10x + 6 = 0.  1.30  1.31  1.32  1.33  1.1  1.3  1.5 1.7 1.9  Necessary and suﬃcient conditions  Prove that the equation ax2 + bx + c = 0, in which a, b and c are real and a > 0, has two real distinct solutions IFF b2 > 4ac. For the real variable x, show that a suﬃcient, but not necessary, condition for f x  = x x + 1  2x + 1  to be divisible by 6 is that x is an integer. Given that at least one of a and b, and at least one of c and d, are non-zero, show that ad = bc is both a necessary and suﬃcient condition for the equations  ax + by = 0, cx + dy = 0,  to have a solution in which at least one of x and y is non-zero. The coeﬃcients ai in the polynomial Q x  = a4x4 + a3x3 + a2x2 + a1x are all integers. Show that Q n  is divisible by 24 for all integers n ≥ 0 if and only if all  of the following conditions are satisﬁed:  i  2a4 + a3 is divisible by 4;  ii  a4 + a2 is divisible by 12;  iii  a4 + a3 + a2 + a1 is divisible by 24.  8 and c = 23  8  −7 +  1.9 Hints and answers  8  −7 − √   b  The roots are 1, 1 7  √ 33  = −0.1569, 1 4 are the values of k that make f −1  and f  1  a  a = 4, b = 3  b  f 1  = 5, f 0  = −2 and f −1  = 5, and so there is at least one root in each of the ranges 0 < x < 1 and −1 < x < 0.  x7 + 5x6  +  x4 − x3  +  x2 − 2  is positive deﬁnite for −5 < x < −√ range, but there must be one to the left of x = −5.  33  = −1.593.  c  −5 and  cid:7    a  x2 + 9x + 18 = 0;  b  x2 − 4x = 0;  c  x2 − 4x + 4 = 0;  d  x2 − 6x + 13 = 0.  a  1.339, −2.626.  b  No solution because 62 > 42 + 32.  c  −0.0849, −2.276.  √ 2.  b  Use results  1.20  and  1.21 .  16 are all positive. Therefore f  2. There are therefore no roots in this   a  Use sin π 4  = 1    x  > 0 for all x > 0.  2   equal to zero.  39   PRELIMINARY ALGEBRA  1.11  1.13  1.15  1.17  1.19 1.21  1.23 1.25  1.27  1.29  1.31  1.33  3.  Show that the equation is equivalent to sin 5θ 2  sin θ  sin θ 2  = 0.  Solutions are −4π 5,−2π 5, 0, 2π 5, 4π 5, π. The solution θ = 0 has multiplicity  a  A circle of radius 5 centred on  −3,−4 .  b  A hyperbola with ‘centre’  3,−2  and ‘semi-axes’ 2 and 3.  c  The expression factorises into two lines, x + 2y − 3 = 0 and 2x + y + 2 = 0.  d  Write the expression as  x + y 2 = 8 x− y  to see that it represents a parabola  passing through the origin, with the line x + y = 0 as its axis of symmetry.   a    a   9  5  7 x − 2   x + 2 x2 + 4  + − 1 x − 1  ,  ,  7 x + 5    b  − 4  +  4  3 x − 3   .  2  .  3x  +   b   x + 1 x2 + 9   a  10,  b  not deﬁned,  c  −35,  d  −21.  x2 + 1   cid:11   Look for factors common to the n = N sum and the additional n = N + 1 term, so as to reduce the sum for n = N + 1 to a single term.  Write 32n as 8m − 7.  of any three consecutive integers must divide by both 2 and 3.  Use the half-angle formulae of equations  1.32  to  1.34  to relate functions of θ 2k to those of θ 2k+1. Divisible for k = 1, 2, . . . , p − 1. Expand  n + 1 p as np + pCknk + 1. Apply the stated result for p = 5. Note that n5 − n = n n− 1  n + 1  n2 + 1 ; the product By assuming x = p q with q  cid:3 = 1, show that a fraction −pn q is equal to an integer an−1pn−1 + ··· + a1pqn−2 + a0qn−1. This is a contradiction, and is only  a  The only possible candidates are ±1,±2,±4. None is a root.  b  The only possible candidates are ±1,±2,±3,±6. Only −3 is a root. f x  can be written as x x + 1  x + 2  + x x + 1  x − 1 . Each term consists of  resolved if q = 1 and the root is an integer.  p−1  1  the product of three consecutive integers, of which one must therefore divide by 2 and  a diﬀerent  one by 3. Thus each term separately divides by 6, and so  therefore does f x . Note that if x is the root of 2x3 + 3x2 + x − 24 = 0 that lies  near the non-integer value x = 1.826, then x x + 1  2x + 1  = 24 and therefore divides by 6. Note that, e.g., the condition for 6a4 + a3 to be divisible by 4 is the same as the condition for 2a4 + a3 to be divisible by 4. For the necessary  only if  part of the proof set n = 1, 2, 3 and take integer combinations of the resulting equations. For the suﬃcient  if  part of the proof use the stated conditions to prove the  proposition by induction. Note that n3 − n is divisible by 6 and that n2 + n is  even.  40   Preliminary calculus  This chapter is concerned with the formalism of probably the most widely used mathematical technique in the physical sciences, namely the calculus. The chapter divides into two sections. The ﬁrst deals with the process of diﬀerentiation and the second with its inverse process, integration. The material covered is essential for the remainder of the book and serves as a reference. Readers who have previously studied these topics should ensure familiarity by looking at the worked examples in the main text and by attempting the exercises at the end of the chapter.  2.1 Diﬀerentiation  Diﬀerentiation is the process of determining how quickly or slowly a function varies, as the quantity on which it depends, its argument, is changed. More speciﬁcally it is the procedure for obtaining an expression  numerical or algebraic  for the rate of change of the function with respect to its argument. Familiar examples of rates of change include acceleration  the rate of change of velocity  and chemical reaction rate  the rate of change of chemical composition . Both acceleration and reaction rate give a measure of the change of a quantity with respect to time. However, diﬀerentiation may also be applied to changes with respect to other quantities, for example the change in pressure with respect to a change in temperature.  Although it will not be apparent from what we have said so far, diﬀerentiation is in fact a limiting process, that is, it deals only with the inﬁnitesimal change in one quantity resulting from an inﬁnitesimal change in another.  2.1.1 Differentiation from ﬁrst principles  Let us consider a function f x  that depends on only one variable x, together with numerical constants, for example, f x  = 3x2 or f x  = sin x or f x  = 2 + 3 x.  2  41   PRELIMINARY CALCULUS  A  θ  f x + ∆x   ∆f  f x   P  ∆x  x  x + ∆x  Figure 2.1 The graph of a function f x  showing that the gradient or slope of the function at P , given by tan θ, is approximately equal to ∆f ∆x.  Figure 2.1 shows an example of such a function. Near any particular point, P , the value of the function changes by an amount ∆f, say, as x changes by a small amount ∆x. The slope of the tangent to the graph of f x  at P is then approximately ∆f ∆x, and the change in the value of the function is  ∆f = f x + ∆x  − f x . In order to calculate the true value of the gradient, or  ﬁrst derivative, of the function at P , we must let ∆x become inﬁnitesimally small. We therefore deﬁne the ﬁrst derivative of f x  as   cid:7    x  ≡ df x   f  dx  ≡ lim ∆x→0  f x + ∆x  − f x   ,  ∆x   2.1   provided that the limit exists. The limit will depend in almost all cases on the value of x. If the limit does exist at a point x = a then the function is said to be diﬀerentiable at a; otherwise it is said to be non-diﬀerentiable at a. The formal concept of a limit and its existence or non-existence is discussed in chapter 4; for present purposes we will adopt an intuitive approach.  In the deﬁnition  2.1 , we allow ∆x to tend to zero from either positive or negative values and require the same limit to be obtained in both cases. A function that is diﬀerentiable at a is necessarily continuous at a  there must be no jump in the value of the function at a , though the converse is not necessarily true. This latter assertion is illustrated in ﬁgure 2.1: the function is continuous at the ‘kink’ A but the two limits of the gradient as ∆x tends to zero from positive or negative values are diﬀerent and so the function is not diﬀerentiable at A.  It should be clear from the above discussion that near the point P we may  42    2.2    2.3    2.4   2.1 DIFFERENTIATION  approximate the change in the value of the function, ∆f, that results from a small change ∆x in x by  As one would expect, the approximation improves as the value of ∆x is reduced. In the limit in which the change ∆x becomes inﬁnitesimally small, we denote it by the diﬀerential dx, and  2.2  reads  ∆f ≈ df x   ∆x.  dx  df =  df x   dx  dx.  This equality relates the inﬁnitesimal change in the function, df, to the inﬁnitesimal change dx that causes it.  So far we have discussed only the ﬁrst derivative of a function. However, we can also deﬁne the second derivative as the gradient of the gradient of a function.  x . Hence the Again we use the deﬁnition  2.1  but now with f x  replaced by f second derivative is deﬁned by   cid:7    cid:7  cid:7    x  ≡ lim ∆x→0  f   cid:7    x + ∆x  − f   cid:7   f   x   ,  ∆x  provided that the limit exists. A physical example of a second derivative is the second derivative of the distance travelled by a particle with respect to time. Since the ﬁrst derivative of distance travelled gives the particle’s velocity, the second derivative gives its acceleration.  We can continue in this manner, the nth derivative of the function f x  being  deﬁned by  f n  x  ≡ lim ∆x→0  f n−1  x + ∆x  − f n−1  x   ∆x  .   2.5    cid:7    x  ≡ f 1  x , f   cid:7  cid:7    x  ≡ f 2  x , etc., and  It should be noted that with this notation f  that formally f 0  x  ≡ f x .  All this should be familiar to the reader, though perhaps not with such formal deﬁnitions. The following example shows the diﬀerentiation of f x  = x2 from ﬁrst principles. In practice, however, it is desirable simply to remember the derivatives of standard functions; the techniques given in the remainder of this section can be applied to ﬁnd more complicated derivatives.  43    cid:1 Find from ﬁrst principles the derivative with respect to x of f x  = x2.  Using the deﬁnition  2.1 ,  PRELIMINARY CALCULUS   cid:7   f   x  = lim ∆x→0  f x + ∆x  − f x   x + ∆x 2 − x2  ∆x  = lim ∆x→0  = lim ∆x→0 = lim ∆x→0  2x∆x +  ∆x 2  ∆x  ∆x   2x + ∆x .   cid:7   f   x  = 2x.  cid:2   As ∆x tends to zero, 2x + ∆x tends towards 2x, hence  Derivatives of other functions can be obtained in the same way. The derivatives  of some simple functions are listed below  note that a is a constant :  d dx   sin ax  = a cos ax,  d dx   ln ax  =  1 x  ,  d dx  d dx   sec ax  = a sec ax tan ax,   xn  = nxn−1,  d dx  d dx   eax  = aeax,  cos ax  = −a sin ax,  cid:9   cid:9   d dx  d dx  −1√ a2 − x2  ,  d dx  d dx   tan ax  = a sec2 ax,  cot ax  = −a cosec2 ax,  cid:9    cid:10   d dx  d dx  cos  −1 x a  =   cosec ax  = −a cosec ax cot ax,   cid:10   cid:10   sin  =  −1 x a −1 x a  tan  1√ a2 − x2  ,  a  =  a2 + x2 .  Diﬀerentiation from ﬁrst principles emphasises the deﬁnition of a derivative as the gradient of a function. However, for most practical purposes, returning to the deﬁnition  2.1  is time consuming and does not aid our understanding. Instead, as mentioned above, we employ a number of techniques, which use the derivatives listed above as ‘building blocks’, to evaluate the derivatives of more complicated functions than hitherto encountered. Subsections 2.1.2–2.1.7 develop the methods required.  2.1.2 Differentiation of products  As a ﬁrst example of the diﬀerentiation of a more complicated function, we consider ﬁnding the derivative of a function f x  that can be written as the product of two other functions of x, namely f x  = u x v x . For example, if f x  = x3 sin x then we might take u x  = x3 and v x  = sin x. Clearly the  44   2.1 DIFFERENTIATION  separation is not unique.  In the given example, possible alternative break-ups would be u x  = x2, v x  = x sin x, or even u x  = x4 tan x, v x  = x  −1 cos x.   The purpose of the separation is to split the function into two  or more  parts, of which we know the derivatives  or at least we can evaluate these derivatives more easily than that of the whole . We would gain little, however, if we did not know the relationship between the derivative of f and those of u and v. Fortunately, they are very simply related, as we shall now show.  Since f x  is written as the product u x v x , it follows that  f x + ∆x  − f x  = u x + ∆x v x + ∆x  − u x v x   From the deﬁnition of a derivative  2.1 ,  = u x + ∆x [v x + ∆x  − v x ] + [u x + ∆x  − u x ]v x .  cid:15   f x + ∆x  − f x   cid:12   cid:13    cid:13    cid:14    cid:14   ∆x  v x + ∆x  − v x   u x + ∆x  − u x   ∆x  +  ∆x  v x   .  u x + ∆x   df dx  = lim ∆x→0  = lim ∆x→0  In the limit ∆x → 0, the factors in square brackets become dv dx and du dx   by the deﬁnitions of these quantities  and u x + ∆x  simply becomes u x . Consequently we obtain  df dx  d dx  dv x   du x   dx  dx  =  [u x v x ] = u x   +  v x .   2.6   In primed notation and without writing the argument x explicitly,  2.6  is stated concisely as   cid:7   f   cid:7  =  uv    cid:7    cid:7  + u  v.  = uv   2.7   This is a general result obtained without making any assumptions about the speciﬁc forms f, u and v, other than that f x  = u x v x . In words, the result reads as follows. The derivative of the product of two functions is equal to the ﬁrst function times the derivative of the second plus the second function times the derivative of the ﬁrst.  cid:1 Find the derivative with respect to x of f x  = x3 sin x.  Using the product rule,  2.6 ,  d dx   x3 sin x  = x3 d dx   sin x  +   x3  sin x  d dx  = x3 cos x + 3x2 sin x.  cid:2   The product rule may readily be extended to the product of three or more  functions. Considering the function  f x  = u x v x w x    2.8   45   and using  2.6 , we obtain, as before omitting the argument,  Using  2.6  again to expand the ﬁrst term on the RHS gives the complete result  PRELIMINARY CALCULUS  df dx  d dx  = u   vw  +  vw.  du dx  d dx   uvw  = uv  + u  w +  vw  dv dx  du dx   cid:7   uvw   = uvw  + uv   cid:7    cid:7  w + u  vw.  dw dx   cid:7    2.9    2.10   or  It is readily apparent that this can be extended to products containing any number n of factors; the expression for the derivative will then consist of n terms with the prime appearing in successive terms on each of the n factors in turn. This is probably the easiest way to recall the product rule.  2.1.3 The chain rule  Products are just one type of complicated function that we may encounter in diﬀerentiation. Another is the function of a function, e.g. f x  =  3 + x2 3 = u x 3, where u x  = 3 + x2. If ∆f, ∆u and ∆x are small ﬁnite quantities, it follows that  ∆f ∆x  =  ∆f ∆u  ∆u ∆x  ;  df dx  =  df du  du dx  .  As the quantities become inﬁnitesimally small we obtain  This is the chain rule, which we must apply when diﬀerentiating a function of a function.  cid:1 Find the derivative with respect to x of f x  =  3 + x2 3.  Rewriting the function as f x  = u3, where u x  = 3 + x2, and applying  2.11  we ﬁnd  df dx  = 3u2 du dx  = 3u2 d dx   3 + x2  = 3u2 × 2x = 6x 3 + x2 2.  cid:2   Similarly, the derivative with respect to x of f x  = 1 v x  may be obtained by  rewriting the function as f x  = v  −1 and applying  2.11 : = −v  = − 1 v2  −2 dv dx  dv dx  .  df dx  The chain rule is also useful for calculating the derivative of a function f with respect to x when both x and f are written in terms of a variable  or parameter , say t.   2.11    2.12   46   2.1 DIFFERENTIATION   cid:1 Find the derivative with respect to x of f t  = 2at, where x = at2.  We could of course substitute for t and then diﬀerentiate f as a function of x, but in this case it is quicker to use  where we have used the fact that  df dx  =  df dt  = 2a  =  1 2at  1 t  ,  dt dx   cid:7    cid:8 −1  dt dx  =  dx dt  .  cid:2   2.1.4 Differentiation of quotients  Applying  2.6  for the derivative of a product to a function f x  = u x [1 v x ], we may obtain the derivative of the quotient of two factors. Thus   cid:9    cid:10  cid:7   u v   cid:7    cid:8  cid:7   1 v   cid:7    cid:8   1 v   cid:8    cid:7  − v v2   cid:7    cid:7  u v  ,  +   cid:7   f  =  = u   cid:7  + u  = u  . This can now be rearranged into   cid:7  where  2.12  has been used to evaluate  1 v  the more convenient and memorisable form   cid:9    cid:10  cid:7   u v   cid:7   f  =   cid:7  − uv   cid:7   vu  .  v2  =   2.13   This can be expressed in words as the derivative of a quotient is equal to the bottom times the derivative of the top minus the top times the derivative of the bottom, all over the bottom squared.   cid:1 Find the derivative with respect to x of f x  = sin x x.  Using  2.13  with u x  = sin x, v x  = x and hence u   cid:7   f   x  =  x cos x − sin x  x2   cid:7    x  = cos x, v .  cid:2   − sin x x2  x  cos x  =   cid:7    x  = 1, we ﬁnd  2.1.5 Implicit differentiation  So far we have only diﬀerentiated functions written in the form y = f x . However, we may not always be presented with a relationship in this simple  form. As an example consider the relation x3 − 3xy + y3 = 2. In this case it is  not possible to rearrange the equation to give y as a function of x. Nevertheless, by diﬀerentiating term by term with respect to x  implicit diﬀerentiation , we can ﬁnd the derivative of y.  47   PRELIMINARY CALCULUS  Diﬀerentiating each term in the equation with respect to x we obtain   cid:1 Find dy dx if x3 − 3xy + y3 = 2.  cid:7   x3  − d  d dx  dx   cid:8   d dx   3xy  +   y3  =   2 ,  d dx  ⇒ 3x2 −  3x  + 3y  dy dx  + 3y2 dy dx  = 0,  where the derivative of 3xy has been found using the product rule. Hence, rearranging for dy dx,  y − x2 y2 − x  .  dy dx  =  Note that dy dx is a function of both x and y and cannot be expressed as a function of x only.  cid:2   2.1.6 Logarithmic differentiation  In circumstances in which the variable with respect to which we are diﬀerentiating is an exponent, taking logarithms and then diﬀerentiating implicitly is the simplest way to ﬁnd the derivative.  cid:1 Find the derivative with respect to x of y = ax.  To ﬁnd the required derivative we ﬁrst take logarithms and then diﬀerentiate implicitly:  ln y = ln ax = x ln a ⇒  1 y  dy dx  = ln a.  Now, rearranging and substituting for y, we ﬁnd  = y ln a = ax ln a.  cid:2   dy dx  2.1.7 Leibnitz’ theorem  We have discussed already how to ﬁnd the derivative of a product of two or more functions. We now consider Leibnitz’ theorem, which gives the corresponding results for the higher derivatives of products.  Consider again the function f x  = u x v x . We know from the product rule v. Using the rule once more for each of the products, we obtain   cid:7  + u  = uv  that f   cid:7    cid:7    cid:7  cid:7   f   cid:7  cid:7  =  uv  cid:7  cid:7   = uv   cid:7  + u v  cid:7  + 2u   cid:7   cid:7    +  u  cid:7  cid:7   cid:7  + u  v  v.   cid:7   v   cid:7  cid:7  + u  v   Similarly, diﬀerentiating twice more gives   cid:7  cid:7  cid:7    cid:7  cid:7  cid:7   = uv   cid:7  + 3u v f  cid:7  f 4  = uv 4  + 4u   cid:7  cid:7   cid:7  cid:7  + 3u  cid:7  cid:7  cid:7   v  cid:7  cid:7  + 6u  v   cid:7   v   cid:7  cid:7  cid:7  + u  cid:7  cid:7   v,  cid:7  cid:7  cid:7  + 4u   cid:7   v  + u 4 v.  48   2.1 DIFFERENTIATION  The pattern emerging is clear and strongly suggests that the results generalise to  f n  =  n!  r! n − r !  u r v n−r  =  nCru r v n−r ,   2.14   where the fraction n! [r! n − r !] is identiﬁed with the binomial coeﬃcient nCr   see chapter 1 . To prove that this is so, we use the method of induction as follows.  Assume that  2.14  is valid for n equal to some integer N. Then  n cid:4   r=0   cid:5  u r v N−r    cid:6   f N+1  =  N Cr  d dx  N Cr[u r v N−r+1  + u r+1 v N−r ]  N Csu s v N+1−s  +  NCs−1u s v N+1−s ,  N+1 cid:4   s=1  n cid:4   r=0  N cid:4  N cid:4  N cid:4   r=0  r=0  s=0  =  =  where we have substituted summation index s for r in the ﬁrst summation, and for r + 1 in the second. Now, from our earlier discussion of binomial coeﬃcients, equation  1.51 , we have  NCs + NCs−1 = N+1Cs  and so, after separating out the ﬁrst term of the ﬁrst summation and the last term of the second, obtain  f N+1  = N C0u 0 v N+1  +  N+1Csu s v N+1−s  + NCN u N+1 v 0 .  But NC0 = 1 = N+1C0 and NCN = 1 = N+1CN+1, and so we may write  f N+1  = N+1C0u 0 v N+1  +  N+1Csu s v N+1−s  + N+1CN+1u N+1 v 0   N+1 cid:4   s=0  =  N+1Csu s v N+1−s .  This is just  2.14  with n set equal to N + 1. Thus, assuming the validity of  2.14  for n = N implies its validity for n = N + 1. However, when n = 1 equation  2.14  is simply the product rule, and this we have already proved directly. These results taken together establish the validity of  2.14  for all n and prove Leibnitz’ theorem.  N cid:4   s=1  N cid:4   s=1  49   PRELIMINARY CALCULUS  f x   Q  A  B  C  S  Figure 2.2 A graph of a function, f x , showing how diﬀerentiation corre- sponds to ﬁnding the gradient of the function at a particular point. Points B, Q and S are stationary points  see text .  x   cid:1 Find the third derivative of the function f x  = x3 sin x.  Using  2.14  we immediately ﬁnd   cid:7  cid:7  cid:7   f   x  = 6 sin x + 3 6x  cos x + 3 3x2  − sin x  + x3 − cos x   = 3 2 − 3x2  sin x + x 18 − x2  cos x.  cid:2   2.1.8 Special points of a function  We have interpreted the derivative of a function as the gradient of the function at the relevant point  ﬁgure 2.1 . If the gradient is zero for some particular value of x then the function is said to have a stationary point there. Clearly, in graphical terms, this corresponds to a horizontal tangent to the graph.  Stationary points may be divided into three categories and an example of each is shown in ﬁgure 2.2. Point B is said to be a minimum since the function increases in value in both directions away from it. Point Q is said to be a maximum since the function decreases in both directions away from it. Note that B is not the overall minimum value of the function and Q is not the overall maximum; rather, they are a local minimum and a local maximum. Maxima and minima are known collectively as turning points.  The third type of stationary point is the stationary point of inﬂection, S . In this case the function falls in the positive x-direction and rises in the negative x-direction so that S is neither a maximum nor a minimum. Nevertheless, the gradient of the function is zero at S , i.e. the graph of the function is ﬂat there, and this justiﬁes our calling it a stationary point. Of course, a point at which the  50   2.1 DIFFERENTIATION  gradient of the function is zero but the function rises in the positive x-direction and falls in the negative x-direction is also a stationary point of inﬂection.  The above distinction between the three types of stationary point has been made rather descriptively. However, it is possible to deﬁne and distinguish sta- tionary points mathematically. From their deﬁnition as points of zero gradient, all stationary points must be characterised by df dx = 0. In the case of the minimum, B, the slope, i.e. df dx, changes from negative at A to positive at C through zero at B. Thus df dx is increasing and so the second derivative d2f dx2 must be positive. Conversely, at the maximum, Q, we must have that d2f dx2 is negative.  It is less obvious, but intuitively reasonable, that at S , d2f dx2 is zero. This may be inferred from the following observations. To the left of S the curve is concave upwards so that df dx is increasing with x and hence d2f dx2 > 0. To the right of S , however, the curve is concave downwards so that df dx is decreasing with x and hence d2f dx2 < 0.  In summary, at a stationary point df dx = 0 and   i  for a minimum, d2f dx2 > 0,   ii  for a maximum, d2f dx2 < 0,   iii  for a stationary point of inﬂection, d2f dx2 = 0 and d2f dx2 changes sign  through the point.  In case  iii , a stationary point of inﬂection, in order that d2f dx2 changes sign through the point we normally require d3f dx3  cid:3 = 0 at that point. This simple  rule can fail for some functions, however, and in general if the ﬁrst non-vanishing derivative of f x  at the stationary point is f n  then if n is even the point is a maximum or minimum and if n is odd the point is a stationary point of inﬂection. This may be seen from the Taylor expansion  see equation  4.17   of the function about the stationary point, but it is not proved here.  cid:1 Find the positions and natures of the stationary points of the function  f x  = 2x3 − 3x2 − 36x + 2.  The ﬁrst criterion for a stationary point is that df dx = 0, and hence we set  from which we obtain  Hence the stationary points are at x = 3 and x = −2. To determine the nature of the stationary point we must evaluate d2f dx2:  = 6x2 − 6x − 36 = 0,  df dx   x − 3  x + 2  = 0.  = 12x − 6.  d2f dx2  51   PRELIMINARY CALCULUS  f x   G  x  Figure 2.3 The graph of a function f x  that has a general point of inﬂection at the point G.  Now, we examine each stationary point in turn. For x = 3, d2f dx2 = 30. Since this is positive, we conclude that x = 3 is a minimum. Similarly, for x = −2, d2f dx2 = −30 and so x = −2 is a maximum.  cid:2   So far we have concentrated on stationary points, which are deﬁned to have df dx = 0. We have found that at a stationary point of inﬂection d2f dx2 is also zero and changes sign. This naturally leads us to consider points at which d2f dx2 is zero and changes sign but at which df dx is not, in general, zero. Such points are called general points of inﬂection or simply points of inﬂection. Clearly, a stationary point of inﬂection is a special case for which df dx is also zero. At a general point of inﬂection the graph of the function changes from being concave upwards to concave downwards  or vice versa , but the tangent to the curve at this point need not be horizontal. A typical example of a general point of inﬂection is shown in ﬁgure 2.3.  The determination of the stationary points of a function, together with the identiﬁcation of its zeros, inﬁnities and possible asymptotes, is usually suﬃcient to enable a graph of the function showing most of its signiﬁcant features to be sketched. Some examples for the reader to try are included in the exercises at the end of this chapter.  2.1.9 Curvature of a function  In the previous section we saw that at a point of inﬂection of the function f x , the second derivative d2f dx2 changes sign and passes through zero. The corresponding graph of f shows an inversion of its curvature at the point of inﬂection. We now develop a more quantitative measure of the curvature of a function  or its graph , which is applicable at general points and not just in the neighbourhood of a point of inﬂection.  As in ﬁgure 2.1, let θ be the angle made with the x-axis by the tangent at a  52   2.1 DIFFERENTIATION  f x   C  ∆θ  ρ  P  Q  θ  θ + ∆θ  x  Figure 2.4 Two neighbouring tangents to the curve f x  whose slopes diﬀer by ∆θ. The angular separation of the corresponding radii of the circle of curvature is also ∆θ.  point P on the curve f = f x , with tan θ = df dx evaluated at P . Now consider also the tangent at a neighbouring point Q on the curve, and suppose that it makes an angle θ + ∆θ with the x-axis, as illustrated in ﬁgure 2.4.  It follows that the corresponding normals at P and Q, which are perpendicular to the respective tangents, also intersect at an angle ∆θ. Furthermore, their point of intersection, C in the ﬁgure, will be the position of the centre of a circle that approximates the arc P Q, at least to the extent of having the same tangents at the extremities of the arc. This circle is called the circle of curvature.  For a ﬁnite arc P Q, the lengths of CP and CQ will not, in general, be equal, as they would be if f = f x  were in fact the equation of a circle. But, as Q  is allowed to tend to P , i.e. as ∆θ → 0, they do become equal, their common  value being ρ, the radius of the circle, known as the radius of curvature. It follows immediately that the curve and the circle of curvature have a common tangent −1, at P and lie on the same side of it. The reciprocal of the radius of curvature, ρ deﬁnes the curvature of the function f x  at the point P .  The radius of curvature can be deﬁned more mathematically as follows. The  length ∆s of arc P Q is approximately equal to ρ∆θ and, in the limit ∆θ → 0, this  relationship deﬁnes ρ as  ρ = lim ∆θ→0  ∆s ∆θ  =  ds dθ  .   2.15   It should be noted that, as s increases, θ may increase or decrease according to whether the curve is locally concave upwards  i.e. shaped as if it were near a minimum in f x   or concave downwards. This is reﬂected in the sign of ρ, which therefore also indicates the position of the curve  and of the circle of curvature   53   PRELIMINARY CALCULUS  relative to the common tangent, above or below. Thus a negative value of ρ indicates that the curve is locally concave downwards and that the tangent lies above the curve.  We next obtain an expression for ρ, not in terms of s and θ but in terms of x and f x . The expression, though somewhat cumbersome, follows from the and  deﬁning equation  2.15 , the deﬁning property of θ that tan θ = df dx ≡ f   cid:7   the fact that the rate of change of arc length with x is given by   cid:16    cid:7    cid:17    cid:8   1 2  2  .  ds dx  =  1 +  df dx  ρ =  =  ds dθ  ds dx  dx dθ  .  sec2 θ  dθ dx  =  d2f dx2  ≡ f   cid:7  cid:7   ,  1 + tan2 θ  dx dθ  =   cid:7    2  .  =  1 +  f  cid:7  cid:7   f   cid:7    cid:19    cid:7  cid:7   f   cid:18    cid:7  1 +  f  cid:7  cid:7  f  3 2   2  .  ρ =  This last result, simply quoted here, is proved more formally in subsection 2.2.13.  From the chain rule  2.11  it follows that  Diﬀerentiating both sides of tan θ = df dx with respect to x gives  from which, using sec2 θ = 1 + tan2 θ = 1 +  f   2, we can obtain dx dθ as  Substituting  2.16  and  2.18  into  2.17  then yields the ﬁnal expression for ρ,   2.16    2.17    2.18    2.19   It should be noted that the quantity in brackets is always positive and that its three-halves root is also taken as positive. The sign of ρ is thus solely determined by that of d2f dx2, in line with our previous discussion relating the sign to whether the curve is concave or convex upwards. If, as happens at a point of inﬂection, d2f dx2 is zero then ρ is formally inﬁnite and the curvature of f x  is zero. As d2f dx2 changes sign on passing through zero, both the local tangent and the circle of curvature change from their initial positions to the opposite side of the curve.  54   2.1 DIFFERENTIATION   cid:1 Show that the radius of curvature at the point  x, y  on the ellipse  x2 a2  y2 b2  +  = 1  2x a2  +  2y b2  dy dx  = 0  = − b2x  a2y  .   cid:7   dy dx   cid:8   a2y3  = − b4  cid:19   3 2   cid:20  cid:20  cid:20  cid:20  cid:20  =  has magnitude  a4y2 + b4x2 3 2  a4b4  and the opposite sign to y. Check the special case b = a, for which the ellipse becomes a circle.  Diﬀerentiating the equation of the ellipse with respect to x gives  d2y dx2  a2  = − b2  cid:18    cid:20  cid:20  cid:20  cid:20  cid:20   and so  A second diﬀerentiation, using  2.13 , then yields   cid:7   y − xy   cid:7   y2   cid:8   y2 b2  +  x2 a2  = − b4 a2y3 ,  where we have used the fact that  x, y  lies on the ellipse. We note that d2y dx2, and hence ρ, has the opposite sign to y3 and hence to y. Substituting in  2.19  gives for the magnitude of the radius of curvature  ρ =  1 + b4x2  a4y2  −b4  a2y3  For the special case b = a, ρ reduces to a turn gives ρ = a, as expected.  cid:2    a4y2 + b4x2 3 2  .  a4b4  −2 y2 + x2 3 2 and, since x2 + y2 = a2, this in  The discussion in this section has been conﬁned to the behaviour of curves that lie in one plane; examples of the application of curvature to the bending of loaded beams and to particle orbits under the inﬂuence of a central forces can be found in the exercises at the ends of later chapters. A more general treatment of curvature in three dimensions is given in section 10.3, where a vector approach is adopted.  2.1.10 Theorems of differentiation  Rolle’s theorem  Rolle’s theorem  ﬁgure 2.5  states that if a function f x  is continuous in the  range a ≤ x ≤ c, is diﬀerentiable in the range a < x < c and satisﬁes f a  = f c    b  = 0. Thus Rolle’s then for at least one point x = b, where a < b < c, f theorem states that for a well-behaved  continuous and diﬀerentiable  function that has the same value at two points either there is at least one stationary point between those points or the function is a constant between them. The validity of the theorem is immediately apparent from ﬁgure 2.5 and a full analytic proof will not be given. The theorem is used in deriving the mean value theorem, which we now discuss.   cid:7   55   PRELIMINARY CALCULUS  f x   a  b  c  x  Figure 2.5 The graph of a function f x , showing that if f a  = f c  then at one point at least between x = a and x = c the graph has zero gradient.  f x   f c   f a   A  C  a  b  c  x  Figure 2.6 The graph of a function f x ; at some point x = b it has the same gradient as the line AC.  Mean value theorem   cid:7   f   b  =  f c  − f a   c − a  ,  The mean value theorem  ﬁgure 2.6  states that if a function f x  is continuous  in the range a ≤ x ≤ c and diﬀerentiable in the range a < x < c then   2.20   for at least one value b where a < b < c. Thus the mean value theorem states that for a well-behaved function the gradient of the line joining two points on the curve is equal to the slope of the tangent to the curve for at least one intervening point.  The proof of the mean value theorem is found by examination of ﬁgure 2.6, as  follows. The equation of the line AC is  g x  = f a  +  x − a   f c  − f a   c − a  ,  56   2.1 DIFFERENTIATION  and hence the diﬀerence between the curve and the line is  h x  = f x  − g x  = f x  − f a  −  x − a   f c  − f a   c − a  .  Since the curve and the line intersect at A and C, h x  = 0 at both of these points.  cid:7  Hence, by an application of Rolle’s theorem, h  x  = 0 for at least one point b between A and C. Diﬀerentiating our expression for h x , we ﬁnd   cid:7  and hence at b, where h   x  = 0,   cid:7  h   x  = f   cid:7    x  − f c  − f a  c − a  ,   cid:7   f   b  =  f c  − f a   c − a  .  Applications of Rolle’s theorem and the mean value theorem  Since the validity of Rolle’s theorem is intuitively obvious, given the conditions imposed on f x , it will not be surprising that the problems that can be solved by applications of the theorem alone are relatively simple ones. Nevertheless we will illustrate it with the following example.   cid:1 What semi-quantitative results can be deduced by applying Rolle’s theorem to the fol-  iii x2 − 3x + 2,  iv  x2 + 7x + 3,  v  2x3 − 9x2 − 24x + k. lowing functions f x , with a and c chosen so that f a  = f c  = 0?  i  sin x,  ii  cos x,   i  If the consecutive values of x that make sin x = 0 are α1, α2, . . .  actually x = nπ, for any integer n  then Rolle’s theorem implies that the derivative of sin x, namely cos x, has at least one zero lying between each pair of values αi and αi+1.   ii  In an exactly similar way, we conclude that the derivative of cos x, namely − sin x,  has at least one zero lying between consecutive pairs of zeros of cos x. These two re- sults taken together  but neither separately  imply that sin x and cos x have interleaving zeros.   iii  For f x  = x2 − 3x + 2, f a  = f c  = 0 if a and c are taken as 1 and 2 respectively.  x  = 2x − 3 = 0 has a solution x = b with b in the  Rolle’s theorem then implies that f range 1 < b < 2. This is obviously so, since b = 3 2.   cid:7    cid:7    iv  With f x  = x2 + 7x + 3, the theorem tells us that if there are two roots of x2 + 7x + 3 = 0 then they have the root of f there are any  real  roots of x2 + 7x + 3 = 0 then they lie one on either side of x = −7 2.  x  = 2x + 7 = 0 lying between them. Thus if The actual roots are  −7 ± √  x  = 0 is the equation 6x2 − 18x − 24 = 0,  v  If f x  = 2x3 − 9x2 − 24x + k then f which has solutions x = −1 and x = 4. Consequently, if α1 and α2 are two diﬀerent roots of f x  = 0 then at least one of −1 and 4 must lie in the open interval α1 to α2. If, as is α1 < −1 < α2 < 4 < α3 .  the case for a certain range of values of k, f x  = 0 has three roots, α1, α2 and α3, then  37  2.   cid:7   57   PRELIMINARY CALCULUS  In each case, as might be expected, the application of Rolle’s theorem does no more than focus attention on particular ranges of values; it does not yield precise answers.  cid:2   Direct veriﬁcation of the mean value theorem is straightforward when it is applied to simple functions. For example, if f x  = x2, it states that there is a value b in the interval a < b < c such that  c2 − a2 = f c  − f a  =  c − a f   cid:7    b  =  c − a 2b.  This is clearly so, since b =  a + c  2 satisﬁes the relevant criteria.  As a slightly more complicated example we may consider a cubic equation, say  f x  = x3 + 2x2 + 4x − 6 = 0, between two speciﬁed values of x, say 1 and 2. In  this case we need to verify that there is a value of x lying in the range 1 < x < 2 that satisﬁes  18 − 1 = f 2  − f 1  =  2 − 1 f   cid:7    x  = 1 3x2 + 4x + 4 .  This is easily done, either by evaluating 3x2 +4x+4−17 at x = 1 and at x = 2 and checking that the values have opposite signs or by solving 3x2 + 4x + 4 − 17 = 0  and showing that one of the roots lies in the stated interval.  The following applications of the mean value theorem establish some general  inequalities for two common functions.   cid:1 Determine inequalities satisﬁed by ln x and sin x for suitable ranges of the real variable x. −1, the mean value  Since for positive values of its argument the derivative of ln x is x theorem gives us  for some b in 0 < a < b < c. Further, since a < b < c implies that c have  −1 < b  −1 < a  −1, we  or, multiplying through by c − a and writing c a = x where x > 1,  Applying the mean value theorem to sin x shows that  for some b lying between a and c. If a and c are restricted to lie in the range 0 ≤ a < c ≤ π,  in which the cosine function is monotonically decreasing  i.e. there are no turning points , we can deduce that  ln c − ln a c − a  =  1 b  ln c − ln a c − a  1 c  <  <  1 a  ,  1 − 1  < ln x < x − 1.  x  sin c − sin a  c − a  = cos b  sin c − sin a  c − a  cos c <  < cos a.  cid:2   58   2.2 INTEGRATION  f x   a  b  x  Figure 2.7 An integral as the area under a curve.  2.2 Integration   cid:21   b  a  The notion of an integral as the area under a curve will be familiar to the reader. In ﬁgure 2.7, in which the solid line is a plot of a function f x , the shaded area represents the quantity denoted by  I =  f x  dx.   2.21   This expression is known as the deﬁnite integral of f x  between the lower limit x = a and the upper limit x = b, and f x  is called the integrand.  2.2.1 Integration from ﬁrst principles  The deﬁnition of an integral as the area under a curve is not a formal deﬁnition, but one that can be readily visualised. The formal deﬁnition of I involves  subdividing the ﬁnite interval a ≤ x ≤ b into a large number of subintervals, by deﬁning intermediate points ξi such that a = ξ0 < ξ1 < ξ2 < ··· < ξn = b, and  then forming the sum  S =  f xi  ξi − ξi−1 ,   2.22   where xi is an arbitrary point that lies in the range ξi−1 ≤ xi ≤ ξi  see ﬁgure 2.8 . If now n is allowed to tend to inﬁnity in any way whatsoever, subject only to the restriction that the length of every subinterval ξi−1 to ξi tends to zero, then S might, or might not, tend to a unique limit, I. If it does then the deﬁnite integral of f x  between a and b is deﬁned as having the value I. If no unique limit exists  the integral is undeﬁned. For continuous functions and a ﬁnite interval a ≤ x ≤ b  the existence of a unique limit is assured and the integral is guaranteed to exist.  n cid:4   i=1  59   PRELIMINARY CALCULUS  f x   a  x1  ξ1  x2  ξ2  x3  ξ3  x4  ξ4  x5  b  x  Figure 2.8 The evaluation of a deﬁnite integral by subdividing the interval  a ≤ x ≤ b into subintervals.   cid:1 Evaluate from ﬁrst principles the integral I =  b  0 x2 dx.   cid:1   n−1 cid:4   k=0  A =  k2h3 =  h3  1  We ﬁrst approximate the area under the curve y = x2 between 0 and b by n rectangles of equal width h. If we take the value at the lower end of each subinterval  in the limit of an inﬁnite number of subintervals we could equally well have chosen the value at the upper end  to give the height of the corresponding rectangle, then the area of the kth rectangle will be  kh 2h = k2h3. The total area is thus  6 n n − 1  2n − 1 ,  cid:8  cid:7    cid:7   b3 6  1 − 1  n   cid:8   .  2 − 1  n  where we have used the expression for the sum of the squares of the natural numbers derived in subsection 1.7.1. Now h = b n and so  A =   n − 1  2n − 1  =  As n → ∞, A → b3 3, which is thus the value I of the integral.  cid:2   Some straightforward properties of deﬁnite integrals that are almost self-evident  are as follows:  0 dx = 0,  f x  dx = 0,  f x  dx =  f x  dx +  f x  dx,   2.23    2.24   [ f x  + g x ] dx =  f x  dx +  g x  dx.   2.25    cid:21   b  a   cid:21   a  a   cid:21   b  a  60   cid:21   c  b   cid:21   b  a   cid:7    cid:8   b3 n3  n 6   cid:21   b  a   cid:21   c  a   cid:21   b  a   2.2 INTEGRATION   cid:21   a   cid:21   a  b   cid:21   x  a  Combining  2.23  and  2.24  with c set equal to a shows that  b  f x  dx = −  f x  dx.   2.26   2.2.2 Integration as the inverse of differentiation  The deﬁnite integral has been deﬁned as the area under a curve between two ﬁxed limits. Let us now consider the integral  F x  =  f u  du   2.27   in which the lower limit a remains ﬁxed but the upper limit x is now variable. It will be noticed that this is essentially a restatement of  2.21 , but that the variable x in the integrand has been replaced by a new variable u. It is conventional to rename the dummy variable in the integrand in this way in order that the same variable does not appear in both the integrand and the integration limits.  It is apparent from  2.27  that F x  is a continuous function of x, but at ﬁrst glance the deﬁnition of an integral as the area under a curve does not connect with our assertion that integration is the inverse process to diﬀerentiation. However, by considering the integral  2.27  and using the elementary property  2.24 , we obtain  F x + ∆x  =  f u  du  x+∆x   cid:21   cid:21   x  a  a   cid:21   x  x  x+∆x   cid:21    cid:21   =  f u  du +  f u  du  x+∆x  = F x  +  f u  du.  Rearranging and dividing through by ∆x yields  F x + ∆x  − F x   ∆x  x+∆x  =  1 ∆x  x  f u  du.  Letting ∆x → 0 and using  2.1  we ﬁnd that the LHS becomes dF dx, whereas  the RHS becomes f x . The latter conclusion follows because when ∆x is small the value of the integral on the RHS is approximately f x ∆x, and in the limit  ∆x → 0 no approximation is involved. Thus  cid:14   or, substituting for F x  from  2.27 ,   cid:13  cid:21   dF x   dx  = f x ,  d dx  x  a  f u  du  = f x .  61   2.28    PRELIMINARY CALCULUS  From the last two equations it is clear that integration can be considered as the inverse of diﬀerentiation. However, we see from the above analysis that the lower limit a is arbitrary and so diﬀerentiation does not have a unique inverse. Any function F x  obeying  2.28  is called an indeﬁnite integral of f x , though any two such functions can diﬀer by at most an arbitrary additive constant. Since the lower limit is arbitrary, it is usual to write   cid:21   x  F x  =  f u  du   2.29   and explicitly include the arbitrary constant only when evaluating F x . The evaluation is conventionally written in the form   cid:21   f x  dx = F x  + c   2.30   where c is called the constant of integration. It will be noticed that, in the absence of any integration limits, we use the same symbol for the arguments of both f and F. This can be confusing, but is suﬃciently common practice that the reader needs to become familiar with it.  We also note that the deﬁnite integral of f x  between the ﬁxed limits x = a  and x = b can be written in terms of F x . From  2.27  we have   cid:21    cid:21  f x  dx − = F b  − F a ,  x0  b  a  x0  f x  dx =  f x  dx   2.31    cid:7    x  = dF dx, we may  where x0 is any third ﬁxed point. Using the notation F rewrite  2.28  as F   x  = f x , and so express  2.31  as   cid:7   b   cid:7    x  dx = F b  − F a  ≡ [F]b a.  F   cid:21   b  a   cid:21   a  In contrast to diﬀerentiation, where repeated applications of the product rule and or the chain rule will always give the required derivative, it is not always possible to ﬁnd the integral of an arbitrary function. Indeed, in most real phys- ical problems exact integration cannot be performed and we have to revert to numerical approximations. Despite this cautionary note, it is in fact possible to integrate many simple functions and the following subsections introduce the most common types. Many of the techniques will be familiar to the reader and so are summarised by example.  2.2.3 Integration by inspection  The simplest method of integrating a function is by inspection. Some of the more elementary functions have well-known integrals that should be remembered. The reader will notice that these integrals are precisely the inverses of the derivatives  62   2.2 INTEGRATION  found near the end of subsection 2.1.1. A few are presented below, using the form given in  2.30 :   cid:21    cid:21   cid:21   cid:21    cid:21    cid:21    cid:1   a dx = ax + c,  axn dx =  axn+1 n + 1  + c,  eax dx =  + c,  dx = a ln x + c,  a x   cid:21    cid:21   cid:21   −a cos bx  b  a sinn+1 bx b n + 1   + c,  −a cosn+1 bx  cid:10   b n + 1    cid:9   −1  x a  a cos bx dx =  + c,  a sin bx dx =  + c,  a tan bx dx =  + c,  a cos bx sinn bx dx =  a sin bx cosn bx dx =  + c,  a   cid:21   a2 + x2 dx = tan −1√ a2 − x2  + c,   cid:10    cid:9   x a  −1  x a  dx = cos  + c,  dx = sin  + c,  where the integrals that depend on n are valid for all n  cid:3 = −1 and where a and b are constants. In the two ﬁnal results x ≤ a.  1√ a2 − x2  2.2.4 Integration of sinusoidal functions  cosn x dx may be found by using trigono- Integrals of the type metric expansions. Two methods are applicable, one for odd n and the other for even n. They are best illustrated by example.  sinn x dx and   cid:1 Evaluate the integral I =  Rewriting the integral as a product of sin x and an even power of sin x, and then using  the relation sin2 x = 1 − cos2 x yields   cid:21   cid:21   eax a  a sin bx  b  −a ln cos bx   cid:10    cid:9   b  −1   cid:1   I =  sin5 x dx.   cid:1   cid:21   cid:21   cid:21   cid:21  = − cos x + 2  =  =  =  sin4 x sin x dx  1 − cos2 x 2 sin x dx  1 − 2 cos2 x + cos4 x  sin x dx  sin x − 2 sin x cos2 x + sin x cos4 x  dx  3 cos3 x − 1  5 cos5 x + c,  63  where the integration has been carried out using the results of subsection 2.2.3.  cid:2     cid:1 Evaluate the integral I =  cos4 x dx.  Rewriting the integral as a power of cos2 x and then using the double-angle formula cos2 x = 1  2  1 + cos 2x  yields   cid:21   cid:7   Using the double-angle formula again we may write cos2 2x = 1  2  1 + cos 4x , and hence  PRELIMINARY CALCULUS   cid:1    cid:21   cid:21   cid:21   cid:18   1   cid:8   2   cid:19   I =   cos2 x 2 dx =  1 + cos 2x  2  dx  =  4  1 + 2 cos 2x + cos2 2x  dx.  I =  1  4 + 1  2 cos 2x + 1  8  1 + cos 4x   dx  = 1 = 3  4 x + 1 8 x + 1  4 sin 2x + 1 4 sin 2x + 1  32 sin 4x + c  8 x + 1 32 sin 4x + c.  cid:2   2.2.5 Logarithmic integration  Integrals for which the integrand may be written as a fraction in which the numerator is the derivative of the denominator may be evaluated using  dx = ln f x  + c.   2.32   This follows directly from the diﬀerentiation of a logarithm as a function of a function  see subsection 2.1.3 .  cid:1 Evaluate the integral   cid:21    cid:7    x  f f x    cid:21   I =  6x2 + 2 cos x  x3 + sin x  dx.  We note ﬁrst that the numerator can be factorised to give 2 3x2 + cos x , and then that the quantity in brackets is the derivative of the denominator. Hence dx = 2 ln x3 + sin x  + c.  cid:2   I = 2  3x2 + cos x x3 + sin x   cid:21   2.2.6 Integration using partial fractions  The method of partial fractions was discussed at some length in section 1.4, but in essence consists of the manipulation of a fraction  here the integrand  in such a way that it can be written as the sum of two or more simpler fractions. Again we illustrate the method by an example.  64    cid:21    cid:21    cid:21    cid:1 Evaluate the integral  2.2 INTEGRATION  I =  dx.  x2 + x  1  1  We note that the denominator factorises to give x x + 1 . Hence  I =  dx.  x x + 1    cid:21   cid:7    cid:8   We now separate the fraction into two partial fractions and integrate directly:  I =  − 1 x + 1  1 x  dx = ln x − ln x + 1  + c = ln  x  x + 1  + c.  cid:2    cid:7    cid:8   2.2.7 Integration by substitution  Sometimes it is possible to make a substitution of variables that turns a com- plicated integral into a simpler one, which can then be integrated by a standard method. There are many useful substitutions and knowing which to use is a matter of experience. We now present a few examples of particularly useful substitutions.   cid:1 Evaluate the integral   cid:21   1√ 1 − sin2 u  Now substituting back for u,  I =  dx.  1√ 1 − x2  cid:21   1√ cos2 u  I = sin  −1 x + c.   cid:21   Making the substitution x = sin u, we note that dx = cos u du, and hence  I =  cos u du =  cos u du =  du = u + c.  This corresponds to one of the results given in subsection 2.2.3.  cid:2   Another particular example of integration by substitution is aﬀorded by inte-  grals of the form   cid:21    cid:21   I =  1  a + b cos x  dx  or  I =  dx.   2.33   1  a + b sin x  In these cases, making the substitution t = tan x 2  yields integrals that can be solved more easily than the originals. Formulae expressing sin x and cos x in terms of t were derived in equations  1.32  and  1.33   see p. 14 , but before we can use them we must relate dx to dt as follows.  65    2.34   Since  PRELIMINARY CALCULUS   cid:9    cid:10   dt dx  =  1 2  sec2 x 2  =  1 2  1 + tan2 x 2  =  1 + t2  ,  2  the required relationship is   cid:1 Evaluate the integral  Rewriting cos x in terms of t and using  2.34  yields  dx =  2  1 + t2 dt.   cid:21   I =  2  1 + 3 cos x  dx.   cid:8   2  1 + t2  dt   cid:19  cid:7   cid:8    cid:21   cid:21   cid:21   cid:21   =  =  =   cid:18   I =  1 + 3  2   cid:7   1 − t2  1 + t2  2 1 + t2   −1  2  2  2  dt   cid:21  1 + t2 + 3 1 − t2  1 + t2 √ √  cid:7   cid:8  2 − t2 dt = 2 − t   2 + t  1√ 1√ 1√ 2 − t 2 + t 2 √ √ 1√ 2 − t  + 2 + t  + c 2  ln   ln   dt  dt  +  2      cid:17    cid:16 √ √ 2 + tan  x 2  2 − tan  x 2   + c.  cid:2   = − 1√  =  ln  1√ 2  Integrals of a similar form to  2.33 , but involving sin 2x, cos 2x, tan 2x, sin2 x, cos2 x or tan2 x instead of cos x and sin x, should be evaluated by using the substitution t = tan x. In this case  sin x =  cos x =  and  dx =   2.35   t√ 1 + t2  ,  1√ 1 + t2  dt 1 + t2 .  A ﬁnal example of the evaluation of integrals using substitution is the method  of completing the square  cf. subsection 1.7.3 .  66    cid:1 Evaluate the integral  We can write the integral in the form  2.2 INTEGRATION   cid:21    cid:21    cid:21   1  1  1  I =  x2 + 4x + 7  dx.  I =   x + 2 2 + 3  dx.  Substituting y = x + 2, we ﬁnd dy = dx and hence  Hence, by comparison with the table of standard integrals  see subsection 2.2.3   √  3 3  I =  −1  tan   cid:7   I =   cid:8   y√ 3  dy,  y2 + 3 √   cid:7    cid:8   + c =  −1  tan  3 3  x + 2√ 3  + c.  cid:2   2.2.8 Integration by parts  Integration by parts is the integration analogy of product diﬀerentiation. The principle is to break down a complicated function into two functions, at least one of which can be integrated by inspection. The method in fact relies on the result for the diﬀerentiation of a product. Recalling from  2.6  that  where u and v are functions of x, we now integrate to ﬁnd   uv  = u  +  dv dx  du dx  v,  d dx   cid:21   uv =  dx +  v dx.  u  dv dx   cid:21   dx = uv −  u  dv dx   cid:21   cid:21   du dx  du dx  Rearranging into the standard form for integration by parts gives  v dx.   2.36   Integration by parts is often remembered for practical purposes in the form  the integral of a product of two functions is equal to {the ﬁrst times the integral of the second} minus the integral of {the derivative of the ﬁrst times the integral of the second}. Here, u is ‘the ﬁrst’ and dv dx is ‘the second’; clearly the integral v  of ‘the second’ must be determinable by inspection.  cid:1 Evaluate the integral I = In the notation given above, we identify x with u and sin x with dv dx. Hence v = − cos x  x sin x dx.   cid:1   and du dx = 1 and so using  2.36    cid:21   I = x − cos x  −   1  − cos x  dx = −x cos x + sin x + c.  cid:2   67    cid:1    cid:1    cid:1   PRELIMINARY CALCULUS  The separation of the functions is not always so apparent, as is illustrated by  the following example.   cid:1 Evaluate the integral I =  −x2  x3e  dx.  Firstly we rewrite the integral as   cid:21    cid:9    cid:10   −x2  I =  x2  xe  dx.   cid:21   1 x  I =   ln x  1 dx.   cid:21   cid:7    cid:8    cid:21  Now, using the notation given above, we identify x2 with u and xe v = − 1 −x2 and du dx = 2x, so that 2 e  I = − 1  2 x2e  −x2 −   −x e  −x2  dx = − 1  −x2 − 1 2 e  −x2  2 x2e  + c.  cid:2   −x2 with dv dx. Hence  A trick that is sometimes useful is to take ‘1’ as one factor of the product, as  is illustrated by the following example.   cid:1 Evaluate the integral I =  ln x dx.  Firstly we rewrite the integral as  Now, using the notation above, we identify ln x with u and 1 with dv dx. Hence we have v = x and du dx = 1 x, and so  I =  ln x  x  −  x dx = x ln x − x + c.  cid:2   It is sometimes necessary to integrate by parts more than once. In doing so, we may occasionally re-encounter the original integral I. In such cases we can obtain a linear algebraic equation for I that can be solved to obtain its value.  where, for convenience, we have omitted the constant of integration. Integrating by parts a second time,   cid:1 Evaluate the integral I =  eax cos bx dx.  Integrating by parts, taking eax as the ﬁrst function, we ﬁnd   cid:7    cid:8   I = eax  sin bx  b  I = eax  sin bx  b  sin bx  b   cid:7   − aeax  cid:7   1 b   cid:7    cid:21   −   cid:8   cid:21   cid:7 − cos bx  b2  aeax   cid:8   dx,   cid:8   cid:7 − cos bx   cid:8   +  a2eax  dx.  b2   cid:8   − a2 b2 I.  I = eax  sin bx +  cos bx  a b2  68  Notice that the integral on the RHS is just −a2 b2 times the original integral I. Thus   2.2 INTEGRATION  Rearranging this expression to obtain I explicitly and including the constant of integration we ﬁnd  I =  eax  a2 + b2   b sin bx + a cos bx  + c.   2.37   Another method of evaluating this integral, using the exponential of a complex number, is given in section 3.6.  cid:2   Integration using reduction formulae is a process that involves ﬁrst evaluating a simple integral and then, in stages, using it to ﬁnd a more complicated integral.  cid:1 Using integration by parts, ﬁnd a relationship between In and In−1 where  and n is any positive integer. Hence evaluate I2 =  Writing the integrand as a product and separating the integral into two we ﬁnd  2.2.9 Reduction formulae   cid:21    cid:21   In =  1   cid:1   1 − x3 n dx  0  0  1 − x3 2 dx.  1   cid:21   cid:21   0  0  In =  =  1  1   cid:21   1 − x3  1 − x3 n−1 dx  1 − x3 n−1 dx −  1  0  x3 1 − x3 n−1 dx.  In = In−1 −   cid:22   1  0   x x2 1 − x3 n−1 dx.  cid:23    cid:21   1   1 − x3 n  −  1  0  1 3n  0   1 − x3 n dx  x 3n  In = In−1 + = In−1 + 0 − 1  In,  3n  In =  3n  3n + 1  In−1.   cid:21   1  0   cid:21   0  1   1 − x3 0 dx =  I0 =  dx = [x]1  0 = 1.  We now have a relation connecting successive integrals. Hence, if we can evaluate I0, we can ﬁnd I1, I2 etc. Evaluating I0 is trivial:  Hence   3 × 1   3 × 1  + 1  × 1 =  3 4  ,  I1 =   3 × 2   3 × 2  + 1  × 3 4  =  9 14  .  I2 =  Although the ﬁrst few In could be evaluated by direct multiplication, this becomes tedious for integrals containing higher values of n; these are therefore best evaluated using the reduction formula.  cid:2   69  Integrating by parts we ﬁnd  which on rearranging gives  The ﬁrst term on the RHS is clearly In−1 and so, writing the integrand in the second term on the RHS as a product,   PRELIMINARY CALCULUS  2.2.10 Inﬁnite and improper integrals  The deﬁnition of an integral given previously does not allow for cases in which either of the limits of integration is inﬁnite  an inﬁnite integral  or for cases in which f x  is inﬁnite in some part of the range  an improper integral , e.g. −1 4 near the point x = 2. Nevertheless, modiﬁcation of the  f x  =  2 − x   deﬁnition of an integral gives inﬁnite and improper integrals each a meaning.   cid:1   b a f x  dx, the inﬁnite integral, in which b tends  In the case of an integral I =   cid:21  ∞ to ∞, is deﬁned by  I =  a  f x  dx = lim b→∞  f x  dx = lim  b→∞ F b  − F a .   cid:21   b  a  As previously, F x  is the indeﬁnite integral of f x  and limb→∞ F b  means the limit  or value  that F b  approaches as b → ∞; it is evaluated after calculating  the integral. The formal concept of a limit will be introduced in chapter 4.  cid:1 Evaluate the integral   cid:21  ∞  Integrating, we ﬁnd F x  = − 1  x  I =   x2 + a2 2 dx.  0   cid:13    cid:14  −1 + c and so 2  x2 + a2  −1   cid:7  −1   cid:8   −  2 b2 + a2   2a2  I = lim b→∞  =  1  2a2 .  cid:2    cid:21   b  c+ cid:4   For the case of improper integrals, we adopt the approach of excluding the unbounded range from the integral. For example, if the integrand f x  is inﬁnite  at x = c  say , a ≤ c ≤ b then   cid:21   b  a   cid:21   c−δ  f x  dx = lim δ→0  a  f x  dx + lim  cid:4 →0  f x  dx.   cid:1 Evaluate the integral I =  −1 4 dx.  Integrating directly,   cid:1   2  0  2 − x   cid:19 2− cid:4    cid:18 − 4 3  2 − x 3 4  I = lim  cid:4 →0  0 = lim  cid:4 →0  3  cid:4 3 4  + 4  3 23 4 =  4 3   cid:18 − 4   cid:19    cid:5    cid:6   23 4.  cid:2   2.2.11 Integration in plane polar coordinates  In plane polar coordinates ρ, φ, a curve is deﬁned by its distance ρ from the origin as a function of the angle φ between the line joining a point on the curve to the origin and the x-axis, i.e. ρ = ρ φ . The area of an element is given by  70   2.2 INTEGRATION  C  ρ φ + dφ   dA  ρ φ   ρ dφ  B  x  y  O  Figure 2.9 Finding the area of a sector OBC deﬁned by the curve ρ φ  and the radii OB, OC, at angles to the x-axis φ1, φ2 respectively.  2 ρ2 dφ, as illustrated in ﬁgure 2.9, and hence the total area between two  dA = 1 angles φ1 and φ2 is given by  A =  1  2 ρ2 dφ.   2.38   An immediate observation is that the area of a circle of radius a is given by   cid:21   2π  0   cid:18    cid:19 2π  A =  1  2 a2 dφ =  1  2 a2φ  0 = πa2.   cid:1 The equation in polar coordinates of an ellipse with semi-axes a and b is  1 ρ2  =  cos2 φ  sin2 φ  a2  +  .  b2  Find the area A of the ellipse.  Using  2.38  and symmetry, we have   cid:21   2π  A =  1 2  b2 cos2 φ + a2 sin2 φ  0  dφ = 2a2b2  1  b2 cos2 φ + a2 sin2 φ  dφ.  To evaluate this integral we write t = tan φ and use  2.35 :  A = 2a2b2  1  b2 + a2t2 dt = 2b2   b a 2 + t2 dt.  Finally, from the list of standard integrals  see subsection 2.2.3 ,  A = 2b2  −1  tan  t  1   b a    b a   0  = 2ab  − 0  π 2  = πab.  cid:2   a2b2   cid:21  ∞  0   cid:13   π 2  0   cid:21   cid:21  ∞  cid:9   0  1   cid:10    cid:21   φ2  φ1   cid:14 ∞  71    cid:21   a   cid:21   1  0  PRELIMINARY CALCULUS  2.2.12 Integral inequalities  Consider the functions f x , φ1 x  and φ2 x  such that φ1 x  ≤ f x  ≤ φ2 x  for all x in the range a ≤ x ≤ b. It immediately follows that  b  φ1 x  dx ≤  b  f x  dx ≤  φ2 x  dx,   2.39    cid:21   b  a   cid:21   a  which gives us a way of estimating an integral that is diﬃcult to evaluate explicitly.   cid:1 Show that the value of the integral  I =   1 + x2 + x3 1 2 dx  1  lies between 0.810 and 0.882.  We note that for x in the range 0 ≤ x ≤ 1, 0 ≤ x3 ≤ x2. Hence   1 + x2 1 2 ≤  1 + x2 + x3 1 2 ≤  1 + 2x2 1 2,  and so  ≥  cid:21    1 + x2 1 2  Consequently, cid:21  from which we obtain cid:22   0  1  1   1 + x2 1 2 dx ≥  cid:24   ln x +  1 + x2   1  1   1 + x2 + x3 1 2   1 + 2x2 1 2 .  ≥  1  1   cid:21   cid:2   0   1 + 2x2 1 2 dx,  1   cid:8  cid:14   1  0  1 2 + x2  1  0  1   cid:13    1 + x2 + x3 1 2 dx ≥  cid:7   cid:23  0.8814 ≥ I ≥ 0.8105 0.882 ≥ I ≥ 0.810.  ≥ I ≥  1√ 2  x +  ln  1  0  In the last line the calculated values have been rounded to three signiﬁcant ﬁgures, one rounded up and the other rounded down so that the proved inequality cannot be unknowingly made invalid.  cid:2   2.2.13 Applications of integration  Mean value of a function  The mean value m of a function between two limits a and b is deﬁned by  m =  1  b − a  b  a  f x  dx.   2.40   The mean value may be thought of as the height of the rectangle that has the same area  over the same interval  as the area under the curve f x . This is illustrated in ﬁgure 2.10.   cid:21   72   2.2 INTEGRATION  f x   m  a  b  x  Figure 2.10 The mean value m of a function.   cid:1 Find the mean value m of the function f x  = x2 between the limits x = 2 and x = 4.  Using  2.40 ,   cid:21   m =  1  4 − 2  4  2  x2 dx =  1 2  x3 3  =  1 2  43 3  − 23 3  =  .  cid:2   28 3   cid:7    cid:8    cid:13    cid:14 4  2  Finding the length of a curve  Finding the area between a curve and certain straight lines provides one example of the use of integration. Another is in ﬁnding the length of a curve. If a curve is deﬁned by y = f x  then the distance along the curve, ∆s, that corresponds to small changes ∆x and ∆y in x and y is given by   ∆x 2 +  ∆y 2;   2.41   this follows directly from Pythagoras’ theorem  see ﬁgure 2.11 . Dividing  2.41    cid:7  through by ∆x and letting ∆x → 0 we obtain  §   cid:8   Clearly the total length s of the curve between the points x = a and x = b is then given by integrating both sides of the equation:  s =  1 +  dx.   2.42   ∆s ≈ cid:24   cid:25   ds dx   cid:21    cid:25   b  a  =  1 +  2  .  dy dx   cid:7    cid:8   2  dy dx  §  Instead of considering small changes ∆x and ∆y and letting these tend to zero, we could have derived  2.41  by considering inﬁnitesimal changes dx and dy from the start. After writing  ds 2 =  dx 2 + dy 2,  2.41  may be deduced by using the formal device of dividing through by dx. Although not mathematically rigorous, this method is often used and generally leads to the correct result.  73   PRELIMINARY CALCULUS  f x   y = f x   ∆s  ∆x  ∆y  Figure 2.11 The distance moved along a curve, ∆s, corresponding to the small changes ∆x and ∆y.  In plane polar coordinates,   cid:24   dr 2 +  r dφ 2 ⇒ s =  ds =   cid:25    cid:21   r2  r1  1 + r2  dr.  dφ dr   2.43   x   cid:7    cid:8   2   cid:1 Find the length of the curve y = x3 2 from x = 0 to x = 2.  Using  2.42  and noting that dy dx = 3 2   cid:21   cid:22   2   cid:2   cid:6  cid:5   cid:5   cid:22  cid:5   cid:6   4 9  0  2 3  = 8 27  11 2  s =  =  1 + 9  4 x dx  1 + 9 4 x 3 2 − 1  .  cid:2   √ x, the length s of the curve is given by  cid:6   cid:23    cid:22  cid:5   = 8 27  1 + 9   cid:23    cid:23    cid:6   4 x  3 2  3 2  2  0  0  2  Consider the surface S formed by rotating the curve y = f x  about the x-axis  see ﬁgure 2.12 . The surface area of the ‘collar’ formed by rotating an element of the curve, ds, about the x-axis is 2πy ds, and hence the total surface area is  Since  ds 2 =  dx 2 +  dy 2 from  2.41 , the total surface area between the planes x = a and x = b is  S =  2πy  1 +  dx.   2.44   Surfaces of revolution   cid:21   b  a   cid:25   74  S =  2πy ds.   cid:21   b  a   cid:7    cid:8   2  dy dx   2.2 INTEGRATION  y  f x   ds  V  dx  a  b  x  S   cid:14   cid:21   2  Figure 2.12 The surface and volume of revolution for the curve y = f x .   cid:1 Find the surface area of a cone formed by rotating about the x-axis the line y = 2x between x = 0 and x = h.  Using  2.44 , the surface area is given by   cid:25    cid:13    cid:21   cid:21   cid:22   h  h  0  S =   2π 2x  1 +  =  =  4πx √ 5πx2  0  2  1 + 22  h  0  = 2  dx   2x   d dx   cid:6  √ 5π h2 − 0  = 2  dx =  1 2  4  0  h  √ 5πx dx √ 5πh2.  cid:2   We note that a surface of revolution may also be formed by rotating a line  about the y-axis. In this case the surface area between y = a and y = b is  S =  2πx  1 +  dy.   2.45    cid:5   cid:23    cid:21   b  a  Volumes of revolution  The volume V enclosed by rotating the curve y = f x  about the x-axis can also be found  see ﬁgure 2.12 . The volume of the disc between x and x + dx is given by dV = πy2 dx. Hence the total volume between x = a and x = b is  V =  πy2 dx.   2.46    cid:25    cid:7    cid:8   2  dx dy   cid:21   b  a  75    cid:1 Find the volume of a cone enclosed by the surface formed by rotating about the x-axis the line y = 2x between x = 0 and x = h.  Using  2.46 , the volume is given by  PRELIMINARY CALCULUS   cid:21   h   cid:21   cid:18   h  0   cid:19   V =  π 2x 2 dx =  4πx2 dx  =  4  3 πx3  h  0 = 4  3 πh3.  cid:2   0  3 π h3 − 0  = 4  cid:21   V =  πx2 dy.  b  a  2.3 Exercises  As before, it is also possible to form a volume of revolution by rotating a curve  about the y-axis. In this case the volume enclosed between y = a and y = b is   2.47   2.1  Obtain the following derivatives from ﬁrst principles:  the ﬁrst derivative of 3x + 4;   a   b  the ﬁrst, second and third derivatives of x2 + x;  c   the ﬁrst derivative of sin x.  Find from ﬁrst principles the ﬁrst derivative of  x + 3 2 and compare your answer with that obtained using the chain rule. Find the ﬁrst derivatives of   a  x2 exp x,  b  2 sin x cos x,  c  sin 2x,  d  x sin ax,  e   g    exp ax  sin ax  tan −x ,  h  xx. ln ax + a  −1 ax,  f  ln xa + x  −a ,  Find the ﬁrst derivatives of   a  x  a + x 2,  b  x  1 − x 1 2,  c  tan x, as sin x  cos x,  d   3x2 + 2x + 1   8x2 − 4x + 2 .  2.5  Use result  2.12  to ﬁnd the ﬁrst derivatives of   a    2x + 3   −3,  b  sec2 x,  c  cosech33x,  d  1  ln x,  e  1 [sin  −1 x a ].  Show that the function y x  = exp −x  deﬁned by  exp x  1  exp −x   y x  =  for x < 0, for x = 0, for x > 0,  is not diﬀerentiable at x = 0. Consider the limiting process for both ∆x > 0 and ∆x < 0.  Find dy dx if x =  t − 2   t + 2  and y = 2t  t + 1  for −∞ < t < ∞. Show that  it is always non-negative, and make use of this result in sketching the curve of y as a function of x. If 2y + sin y + 5 = x4 + 4x3 + 2π, show that dy dx = 16 when x = 1. Find the second derivative of y x  = cos[ π 2  − ax]. Now set a = 1 and verify  that the result is the same as that obtained by ﬁrst setting a = 1 and simplifying y x  before diﬀerentiating.  76  2.2  2.3  2.4  2.6  2.7  2.8 2.9   2.3 EXERCISES  2.10  The function y x  is deﬁned by y x  =  1 + xm n.  a  Use the chain rule to show that the ﬁrst derivative of y is nmxm−1 1 + xm n−1.  b  The binomial expansion  see section 1.5  of  1 + z n is   1 + z n = 1 + nz +  n n − 1   2!  z2 + ··· +  n n − 1 ···  n − r + 1   zr + ··· .  r!  Keeping only the terms of zeroth and ﬁrst order in dx, apply this result twice to derive result  a  from ﬁrst principles.   c  Expand y in a series of powers of x before diﬀerentiating term by term. Show that the result is the series obtained by expanding the answer given for dy dx in  a .  2.11  Show by diﬀerentiation and substitution that the diﬀerential equation  2.12  2.13 2.14  2.15  2.16  4x2 d2y dx2  − 4x  dy dx  +  4x2 + 3 y = 0  has a solution of the form y x  = xn sin x, and ﬁnd the value of n. Find the positions and natures of the stationary points of the following functions:   a  x3 − 3x + 3;  b  x3 − 3x2 + 3x;  c  x3 + 3x + 3;  d  sin ax with a  cid:3 = 0;  e  x5 + x3;  f  x5 − x3. Show that the lowest value taken by the function 3x4 + 4x3 − 12x2 + 6 is −26.  By ﬁnding their stationary points and examining their general forms, determine the range of values that each of the following functions y x  can take. In each case make a sketch-graph incorporating the features you have identiﬁed.   a  y x  =  x − 1   x2 + 2x + 6 .  b  y x  = 1  4 + 3x − x2 .  c  y x  =  8 sin x   15 + 8 tan2 x . exp −√  √ 2 .  2  < a < exp   Show that y x  = xa2x exp x2 has no stationary points other than x = 0, if  The curve 4y3 = a2 x + 3y  can be parameterised as x = a cos 3θ, y = a cos θ.   a  Obtain expressions for dy dx  i  by implicit diﬀerentiation and  ii  in param-  eterised form. Verify that they are equivalent.   b  Show that the only point of inﬂection occurs at the origin. Is it a stationary  point of inﬂection?   c  Use the information gained in  a  and  b  to sketch the curve, paying  particular attention to its shape near the points  −a, a 2  and  a,−a 2  and to its slope at the ‘end points’  a, a  and  −a,−a .  2.17  The parametric equations for the motion of a charged particle released from rest in electric and magnetic ﬁelds at right angles to each other take the forms  x = a θ − sin θ ,  y = a 1 − cos θ .  2.18  2.19  Show that the tangent to the curve has slope cot θ 2 . Use this result at a few calculated values of x and y to sketch the form of the particle’s trajectory. Show that the maximum curvature on the catenary y x  = a cosh x a  is 1 a. You will need some of the results about hyperbolic functions stated in subsection 3.7.6. The curve whose equation is x2 3 + y2 3 = a2 3 for positive x and y and which is completed by its symmetric reﬂections in both axes is known as an astroid. Sketch it and show that its radius of curvature in the ﬁrst quadrant is 3 axy 1 3.  77   PRELIMINARY CALCULUS  c  ρ  C  r + ∆r  ρ  r  p + ∆p  P  Q  O  p  2.21  Use Leibnitz’ theorem to ﬁnd  2.20  2.22  2.23  2.24  2.25  Figure 2.13 The coordinate system described in exercise 2.20.  A two-dimensional coordinate system useful for orbit problems is the tangential- polar coordinate system  ﬁgure 2.13 . In this system a curve is deﬁned by r, the distance from a ﬁxed point O to a general point P of the curve, and p, the perpendicular distance from O to the tangent to the curve at P . By proceeding as indicated below, show that the radius of curvature, ρ, at P can be written in the form ρ = r dr dp. Consider two neighbouring points, P and Q, on the curve. The normals to the  curve through those points meet at C, with  in the limit Q → P   CP = CQ = ρ.  Apply the cosine rule to triangles OP C and OQC to obtain two expressions for c2, one in terms of r and p and the other in terms of r + ∆r and p + ∆p. By equating them and letting Q → P deduce the stated result.  the second derivative of cos x sin 2x,   a   b  the third derivative of sin x ln x,  c   the fourth derivative of  2x3 + 3x2 + x + 2  exp 2x.  If y = exp −x2 , show that dy dx = −2xy and hence, by applying Leibnitz’ theorem, prove that for n ≥ 1  y n+1  + 2xy n  + 2ny n−1  = 0.  Use the properties of functions at their turning points to do the following:   a  By considering its properties near x = 1, show that f x  = 5x4 − 11x3 +  b  Show that f x  = tan x − x cannot be negative for 0 ≤ x < π 2, and deduce  26x2 − 44x + 24 takes negative values for some range of x.  −1 sin x decreases monotonically in the same range.  that g x  = x  Determine what can be learned from applying Rolle’s theorem to the following functions f x :  a  ex;  b  x2 + 6x;  c  2x2 + 3x + 1;  d  2x2 + 3x + 2;  e  2x3 − 21x2 + 60x + k.  f  If k = −45 in  e , show that x = 3 is one root of  f x  = 0, ﬁnd the other roots, and verify that the conclusions from  e  are satisﬁed. By applying Rolle’s theorem to xn sin nx, where n is an arbitrary positive integer, show that tan nx + x = 0 has a solution α1 with 0 < α1 < π n. Apply the theorem a second time to obtain the nonsensical result that there is a real α2 in  0 < α2 < π n, such that cos2 nα2  = −n. Explain why this incorrect result arises.  78   2.31  Find the indeﬁnite integrals J of the following ratios of polynomials:  2.26  2.27  2.28  2.29 2.30  2.32  2.33  2.34  2.35  2.36  2.37  2.3 EXERCISES  Use the mean value theorem to establish bounds in the following cases.   a  For − ln 1 − y , by considering ln x in the range 0 < 1 − y < x < 1.  b  For ey − 1, by considering ex − 1 in the range 0 < x < y. For the function y x  = x2 exp −x  obtain a simple relationship between y and  dy dx and then, by applying Leibnitz’ theorem, prove that  xy n+1  +  n + x − 2 y n  + ny n−1  = 0.  Use Rolle’s theorem to deduce that, if the equation f x  = 0 has a repeated root x1, then x1 is also a root of the equation f  a  Apply this result to the ‘standard’ quadratic equation ax2 + bx + c = 0, to  b  Find all the roots of f x  = x3 + 4x2 − 3x − 18 = 0, given that one of them  show that a necessary condition for equal roots is b2 = 4ac.   x  = 0.   cid:7   is a repeated root.   c  The equation f x  = x4 + 4x3 + 7x2 + 6x + 2 = 0 has a repeated integer root. Show that the curve x3 + y3 − 12x − 8y − 16 = 0 touches the x-axis.  How many real roots does it have altogether?  Find the following indeﬁnite integrals:   cid:1   cid:1    cid:1  −1 dθ;  d   −1 dx;  b    a    c    4 + x2    1 + sin θ    cid:1   8 + 2x − x2  √ 1 − x    x  −1 2 dx for 2 ≤ x ≤ 4; −1 dx for  0 < x ≤ 1.   x + 3   x2 + x − 2 ;   3x2 + 20x + 28   x2 + 6x + 9 ;   a   b   x3 + 5x2 + 8x + 12   2x2 + 10x + 12 ;  c   d  x3  a8 + x8 . Express x2 ax + b  hence evaluate   cid:21   b a  x2  0  ax + b  dx.  −1 as the sum of powers of x and another integrable term, and  −1, with a  cid:3 = 0, distinguishing between the  Find the integral J of  ax2 + bx + c  cases  i  b2 > 4ac,  ii  b2 < 4ac and  iii  b2 = 4ac. Use logarithmic integration to ﬁnd the indeﬁnite integrals J of the following:  a  sin 2x  1 + 4 sin2 x ;  b  ex  ex − e   1 + x ln x   x ln x ;   c   d  [x xn + an ]  −x ; −1.  Find the derivative of f x  =  1 + sin x   cos x and hence determine the indeﬁnite integral J of sec x. Find the indeﬁnite integrals, J, of the following functions involving sinusoids:   a  cos5 x − cos3 x;  b   1 − cos x   1 + cos x ;  d  sec2 x  1 − tan2 x . By making the substitution x = a cos2 θ + b sin2 θ, evaluate the deﬁnite integrals J between limits a and b  > a  of the following functions:   c  cos x sin x  1 + cos x ;   a   [ x − a  b − x ]  −1 2;  b  [ x − a  b − x ]1 2;  79   2.38  Determine whether the following integrals exist and, where they do, evaluate them:  PRELIMINARY CALCULUS   c   [ x − a   b − x ]1 2.  cid:21  ∞  cid:21  ∞  cid:21   exp −λx  dx;  x + 1  dx;  1  0  1  cot θ dθ;   a    c    e    b    d    f   π 2   cid:21   cid:21   y  0  y  0  0   a    c   −∞ 1   cid:21  ∞  cid:21   cid:21   cid:21   cid:21   0 1  0  1  y  y  1  x   x2 + a2 2 dx; 1 x2 dx; x  1 − x2 1 2 dx.  2.39  Use integration by parts to evaluate the following:  x2 sin x dx; −1 x dx;  sin   b   x ln x dx;   d   ln a2 + x2  x2 dx.  2.40  Show, using the following methods, that the indeﬁnite integral of x3  x + 1 1 2 is  35  5x3 − 6x2 + 8x − 16  x + 1 1 2 + c.  J = 2  2.41   a  Repeated integration by parts.  b  Setting x + 1 = u2 and determining dJ du as  dJ dx  dx du . The gamma function Γ n  is deﬁned for all n > −1 by −x dx.  Γ n + 1  =  xne  Find a recurrence relation connecting Γ n + 1  and Γ n .   a  Deduce  i  the value of Γ n + 1  when n is a non-negative integer, and  ii   the value of Γ  , given that Γ   b  Now, taking factorial m for any m to be deﬁned by m! = Γ m + 1 , evaluate   cid:5    cid:6   7 2   cid:6    cid:5 − 3  2  !.  2.42  Deﬁne J m, n , for non-negative integers m and n, by the integral   cid:21  ∞  cid:6   0  =   cid:5   1 2  √ π.   cid:21   π 2  0  J m, n  =  cosm θ sinn θ dθ.   a  Evaluate J 0, 0 , J 0, 1 , J 1, 0 , J 1, 1 , J m, 1 , J 1, n .  b  Using integration by parts, prove that, for m and n both > 1,  J m, n  =  J m − 2, n  and J m, n  =  J m, n − 2 .  n − 1  m + n  m − 1  m + n  2.43  By integrating by parts twice, prove that In as deﬁned in the ﬁrst equality below for positive integers n has the value given in the second equality:   c  Evaluate  i  J 5, 3 ,  ii  J 6, 5  and  iii  J 4, 8 .  π 2  In =   cid:21   cid:18   cid:1  −x dx;  x3 + 1   x4 + 4x + 1  [a +  a − 1  cos θ] −1 dθ with a > 1 2 ;  sin nθ cos θ dθ =   b   1 0  0   d    cid:1  ∞  cid:1   0 xe π 2 0  .  n − sin nπ 2   cid:19   n2 − 1  cid:1  ∞   a    c   2.44  Evaluate the following deﬁnite integrals:  dx; −∞ x2 + 6x + 18   −1 dx.  80    cid:21  ∞  0   cid:21   π 2  0   cid:21   0  2.4 HINTS AND ANSWERS  2.45  If Jr is the integral  show that  xr exp −x2  dx  2.46  2.47  2.48  2.49  2.50  2.1 2.3  2.5  2.7  2.9 2.11  2.13  2.15 2.17  2.19   a  J2r+1 =  r!  2,  b  J2r = 2  −r 2r − 1  2r − 3 ···  5  3  1  J0.  Find positive constants a, b such that ax ≤ sin x ≤ bx for 0 ≤ x ≤ π 2. Use  this inequality to ﬁnd  to two signiﬁcant ﬁgures  upper and lower bounds for the integral  I =   1 + sin x 1 2 dx.  Use the substitution t = tan x 2  to evaluate I exactly.  By noting that for 0 ≤ η ≤ 1, η1 2 ≥ η3 4 ≥ η, prove that  a2 − x2 3 4 dx ≤ π  a  .  ≤ 1 a5 2  2 3  4  length of the astroid x2 3 + y2 3 = a2 3, which can be  Show that the total parameterised as x = a cos3 θ, y = a sin3 θ, is 6a. By noting that sinh x < 1 that, for x > 0, the length L of the curve y = 1 satisﬁes the inequalities sinh x < L < x + sinh x. The equation of a cardioid in plane polar coordinates is  2 ex   0, show 2 ex measured from the origin  ρ = a 1 − sin φ .  Sketch the curve and ﬁnd  i  its area,  ii  its total length,  iii  the surface area of the solid formed by rotating the cardioid about its axis of symmetry and  iv  the volume of the same solid.  2.4 Hints and answers   a  3;  b  2x + 1, 2, 0;  c  cos x. Use: the product rule in  a ,  b ,  d  and  e [ 3 factors ]; the chain rule in  c ,  f  and  g ; logarithmic diﬀerentiation in  g  and  h .   a   x2 + 2x  exp x;  b  2 cos2 x − sin2 x  = 2 cos 2x;  −2.  −1 2[sin  −1 x a ]  −a ];  g  [ ax − a   c  2 cos 2x;  d  sin ax + ax cos ax;  e   a exp ax [ sin ax + cos ax  tan  −1]; −1 ax +  sin ax  1 + a2x2  −x  ln a]  ax + a −a ] [x xa + x −x ;  h   1 + ln x xx. −4;  b  2 sec2 x tan x;  c  −9 cosech33x coth 3x; −2;  e  − a2 − x2    f  [a xa − x  a  −6 2x + 3   d  −x −1 ln x  Calculate dy dt and dx dt and divide one by the other.  t + 2 2 [2 t + 1 2]. − sin x in both cases. Alternatively, eliminate t and ﬁnd dy dx by implicit diﬀerentiation. The required conditions are 8n − 4 = 0 and 4n2 − 8n + 3 = 0; both are satisﬁed by n = 1 2 . The stationary points are the zeros of 12x3 + 12x2 − 24x. The lowest stationary value is −26 at x = −2; other stationary values are 6 at x = 0 and 1 at x = 1. Use logarithmic diﬀerentiation. Set dy dx = 0, obtaining 2x2 + 2x ln a + 1 = 0. See ﬁgure 2.14. 1 3 dy dx  = −  3x4 3y1 3  d2y dx2   cid:9    cid:10   a2 3  y x  =  ;  .  81   PRELIMINARY CALCULUS  y  2a  πa  2πa  x  Figure 2.14 The solution to exercise 2.17.  2.21  2.23  2.25  2.27 2.29  2.31  2.33  2.35 2.37  2.39  2.41 2.43 2.45 2.47 2.49   a  2 2 − 9 cos2 x  sin x;  b   2x 30x2 + 62x + 38  exp 2x.  a  f 1  = 0 whilst f   cid:7   −3 − 3x  −1  sin x −  3x  −2 + ln x  cos x;  c  8 4x3 +  1   cid:3 = 0, and so f x  must be negative in some region with  x  =  − cos x  tan x − x  x2, which is   cid:7    b  f   x  = tan2 x > 0 and f 0  = 0; g  x = 1 as an endpoint.  cid:7   never positive in the range.  By implicit diﬀerentiation, y  The false result arises because tan nx is not diﬀerentiable at x = π  2n , which lies in the range 0 < x < π n, and so the conditions for applying Rolle’s theorem are not satisﬁed.  The relationship is x dy dx =  2 − x y. y 2  = 4 and y −2  = 0, the curve touches the x-axis at the point  −2, 0 .  a  Express in partial fractions; J = 1  b  Divide the numerator by the denominator and express the remainder in   x  =  3x2 − 12   8 − 3y2 , giving y 3 ln[ x − 1 4  x + 2 ] + c. partial fractions; J = x2 4 + 4 ln x + 2  − 3 ln x + 3  + c.   ±2  = 0. Since   cid:7    cid:7    c  After division of the numerator by the denominator, the remainder can be −1 + c.  −2; J = 3x + 2 ln x + 3  + 5 x + 3   −1 − 5 x + 3  −1 tan  expressed as 2 x + 3  −1 x4 a4  + c.  d  Set x4 = u; J =  4a4  Writing b2 − 4ac as ∆2 > 0, or 4ac − b2 as ∆ −1 ln[ 2ax + b − ∆   2ax + b + ∆ ] + k;  cid:7 −1 tan   cid:7    cid:7 2 > 0:  f  ] + k;  −1 + k.  −1[ 2ax + b  ∆   i  ∆  iii  −2 2ax + b   ii  2∆  cid:7   x  =  1 + sin x   cos2 x = f x  sec x; J = ln f x   + c = ln sec x + tan x  + c. Note that dx = 2 b − a  cos θ sin θ dθ.  a  π;  b  π b − a 2 8;  c  π b − a  2.  a   2 − y2  cos y + 2y sin y − 2;  b  [ y2 ln y  2] + [ 1 − y2  4];  d  ln a2 + 1  −  1 y  ln a2 + y2  +  2 a [tan −1 1 a ].  −1 y +  1 − y2 1 2 − 1;  −1 y a  − tan   c  y sin  √ π 8;  b  −2  √ π.  Γ n + 1  = nΓ n ;  a   i  n!,  ii  15 By integrating twice, recover a multiple of In.  J2r+1 = rJ2r−1 and 2J2r =  2r − 1 J2r−2. Set η = 1 −  x a 2 throughout, and x = a sin θ in one of the bounds.   cid:1    cid:5    cid:6   L =  x 0  1 + 1  4 exp 2x  1 2  dx.  82   Complex numbers and hyperbolic functions  This chapter is concerned with the representation and manipulation of complex numbers. Complex numbers pervade this book, underscoring their wide appli- cation in the mathematics of the physical sciences. The application of complex numbers to the description of physical systems is left until later chapters and only the basic tools are presented here.  3.1 The need for complex numbers  Although complex numbers occur in many branches of mathematics, they arise most directly out of solving polynomial equations. We examine a speciﬁc quadratic equation as an example.  Consider the quadratic equation  z2 − 4z + 5 = 0.  Equation  3.1  has two solutions, z1 and z2, such that  Using the familiar formula for the roots of a quadratic equation,  1.4 , the solutions z1 and z2, written in brief as z1,2, are   z − z1  z − z2  = 0.  4 ± cid:24   −4 2 − 4 1 × 5  √−4  2  = 2 ±  .  2  z1,2 =   3.1    3.2    3.3   Both solutions contain the square root of a negative number. However, it is not true to say that there are no solutions to the quadratic equation. The fundamental theorem of algebra states that a quadratic equation will always have two solutions and these are in fact given by  3.3 . The second term on the RHS of  3.3  is called an imaginary term since it contains the square root of a negative number;  3  83   COMPLEX NUMBERS AND HYPERBOLIC FUNCTIONS  f z  5  4  3  2  1  1  2  3  4  z  Figure 3.1 The function f z  = z2 − 4z + 5.  the ﬁrst term is called a real term. The full solution is the sum of a real term and an imaginary term and is called a complex number. A plot of the function  f z  = z2 − 4z + 5 is shown in ﬁgure 3.1. It will be seen that the plot does not  intersect the z-axis, corresponding to the fact that the equation f z  = 0 has no purely real solutions.  The choice of the symbol z for the quadratic variable was not arbitrary; the conventional representation of a complex number is z, where z is the sum of a real part x and i times an imaginary part y, i.e.  z = x + iy,  where i is used to denote the square root of −1. The real part x and the imaginary  part y are usually denoted by Re z and Im z respectively. We note at this point that some physical scientists, engineers in particular, use j instead of i. However, for consistency, we will use i throughout this book.  √−1 = 2i, and hence the two solutions of  In our particular example,  √−4 = 2 z1,2 = 2 ± 2i  2  = 2 ± i.   3.1  are  Thus, here x = 2 and y = ±1.  For compactness a complex number is sometimes written in the form  where the components of z may be thought of as coordinates in an xy-plot. Such a plot is called an Argand diagram and is a common representation of complex numbers; an example is shown in ﬁgure 3.2.  z =  x, y ,  84   3.2 MANIPULATION OF COMPLEX NUMBERS  Im z  y  z = x + iy  x  Re z  Figure 3.2 The Argand diagram.  Our particular example of a quadratic equation may be generalised readily to polynomials whose highest power  degree  is greater than 2, e.g. cubic equations  degree 3 , quartic equations  degree 4  and so on. For a general polynomial f z , of degree n, the fundamental theorem of algebra states that the equation f z  = 0 will have exactly n solutions. We will examine cases of higher-degree equations in subsection 3.4.3.  The remainder of this chapter deals with: the algebra and manipulation of complex numbers; their polar representation, which has advantages in many circumstances; complex exponentials and logarithms; the use of complex numbers in ﬁnding the roots of polynomial equations; and hyperbolic functions.  3.2 Manipulation of complex numbers  This section considers basic complex number manipulation. Some analogy may be drawn with vector manipulation  see chapter 7  but this section stands alone as an introduction.  3.2.1 Addition and subtraction  The addition of two complex numbers, z1 and z2, in general gives another complex number. The real components and the imaginary components are added separately and in a like manner to the familiar addition of real numbers:  z1 + z2 =  x1 + iy1  +  x2 + iy2  =  x1 + x2  + i y1 + y2 ,  85   COMPLEX NUMBERS AND HYPERBOLIC FUNCTIONS  Im z  z2  z1 + z2  z1  Re z  Figure 3.3 The addition of two complex numbers.  or in component notation  z1 + z2 =  x1, y1  +  x2, y2  =  x1 + x2, y1 + y2 .  The Argand representation of the addition of two complex numbers is shown in ﬁgure 3.3.  By straightforward application of the commutativity and associativity of the real and imaginary parts separately, we can show that the addition of complex numbers is itself commutative and associative, i.e.  z1 + z2 = z2 + z1,  z1 +  z2 + z3  =  z1 + z2  + z3.  Thus it is immaterial in what order complex numbers are added.   cid:1 Sum the complex numbers 1 + 2i, 3 − 4i, −2 + i.  Summing the real terms we obtain  and summing the imaginary terms we obtain  1 + 3 − 2 = 2,  2i − 4i + i = −i.  Hence   1 + 2i  +  3 − 4i  +  −2 + i  = 2 − i.  cid:2   The subtraction of complex numbers is very similar to their addition. As in the case of real numbers, if two identical complex numbers are subtracted then the result is zero.  86   3.2 MANIPULATION OF COMPLEX NUMBERS  Im z  y  z  x  Re z  arg z  Figure 3.4 The modulus and argument of a complex number.  The modulus of the complex number z is denoted by z and is deﬁned as  3.2.2 Modulus and argument   cid:24   z =  x2 + y2.   cid:9    cid:10   y x  .  −1  arg z = tan   3.4    3.5   Hence the modulus of the complex number is the distance of the corresponding point from the origin in the Argand diagram, as may be seen in ﬁgure 3.4.  The argument of the complex number z is denoted by arg z and is deﬁned as  It can be seen that arg z is the angle that the line joining the origin to z on the Argand diagram makes with the positive x-axis. The anticlockwise direction is taken to be positive by convention. The angle arg z is shown in ﬁgure 3.4. Account must be taken of the signs of x and y individually in determining in which quadrant arg z lies. Thus, for example, if x and y are both negative then  arg z lies in the range −π < arg z < −π 2 rather than in the ﬁrst quadrant   0 < arg z < π 2 , though both cases give the same value for the ratio of y to x.  cid:1 Find the modulus and the argument of the complex number z = 2 − 3i.  Using  3.4 , the modulus is given by  Using  3.5 , the argument is given by   cid:24   z =  22 +  −3 2 =  √  cid:6  13.  .   cid:5 − 3  −1  The two angles whose tangents equal −1.5 are −0.9828 rad and 2.1588 rad. Since x = 2 and y = −3, z clearly lies in the fourth quadrant; therefore arg z = −0.9828 is the appropriate answer.  cid:2   2  arg z = tan  87    3.6    3.7    3.8    3.9    3.10    3.11   COMPLEX NUMBERS AND HYPERBOLIC FUNCTIONS  3.2.3 Multiplication  Complex numbers may be multiplied together and in general give a complex number as the result. The product of two complex numbers z1 and z2 is found  by multiplying them out in full and remembering that i2 = −1, i.e.  z1z2 =  x1 + iy1  x2 + iy2   = x1x2 + ix1y2 + iy1x2 + i2y1y2 =  x1x2 − y1y2  + i x1y2 + y1x2 .   cid:1 Multiply the complex numbers z1 = 3 + 2i and z2 = −1 − 4i.  By direct multiplication we ﬁnd  z1z2 =  3 + 2i  −1 − 4i  = −3 − 2i − 12i − 8i2 = 5 − 14i.  cid:2   The multiplication of complex numbers is both commutative and associative,  i.e.  The product of two complex numbers also has the simple properties  z1z2 = z2z1,   z1z2 z3 = z1 z2z3 .  z1z2 = z1z2,  arg z1z2  = arg z1 + arg z2.  These relations are derived in subsection 3.3.1.   cid:1 Verify that  3.10  holds for the product of z1 = 3 + 2i and z2 = −1 − 4i.  From  3.7   We also ﬁnd  and hence  52 +  −14 2 =  √ 221.   cid:24   √  √  32 + 22 =  13,   −1 2 +  −4 2 = √ 13  17 =  √ 221 = z1z2.  cid:2   17,  z1z2 = 5 − 14i =  cid:24   cid:24  √  z1 = z2 = z1z2 =  We now examine the eﬀect on a complex number z of multiplying it by ±1 and ±i. These four multipliers have modulus unity and we can see immediately  from  3.10  that multiplying z by another complex number of unit modulus gives a product with the same modulus as z. We can also see from  3.11  that if we  88   3.2 MANIPULATION OF COMPLEX NUMBERS  Im z  iz  −z  z  Re z  −iz  Figure 3.5 Multiplication of a complex number by ±1 and ±i.  multiply z by a complex number then the argument of the product is the sum of the argument of z and the argument of the multiplier. Hence multiplying z by unity  which has argument zero  leaves z unchanged in both modulus and argument, i.e. z is completely unaltered by the operation. Multiplying by  −1  which has argument π  leads to rotation, through an angle π, of the line joining the origin to z in the Argand diagram. Similarly, multiplication by i or −i leads to corresponding rotations of π 2 or −π 2 respectively. This geometrical  interpretation of multiplication is shown in ﬁgure 3.5.   cid:1 Using the geometrical interpretation of multiplication by i, ﬁnd the product i 1 − i . The complex number 1 − i has argument −π 4 and modulus   3.11 , its product with i has argument +π 4 and unchanged modulus number with modulus  √ 2 and argument +π 4 is 1 + i and so  √ √ 2. Thus, using  3.10  and 2. The complex  as is easily veriﬁed by direct multiplication.  cid:2   i 1 − i  = 1 + i,  The division of two complex numbers is similar to their multiplication but requires the notion of the complex conjugate  see the following subsection  and so discussion is postponed until subsection 3.2.5.  3.2.4 Complex conjugate  ∗ = x − iy. More generally, we may deﬁne the complex conjugate of z as  If z has the convenient form x + iy then the complex conjugate, denoted by z , may be found simply by changing the sign of the imaginary part, i.e. if z = x + iy then z the  complex  number having the same magnitude as z that when multiplied by z leaves a real result, i.e. there is no imaginary component in the product.  ∗  89   COMPLEX NUMBERS AND HYPERBOLIC FUNCTIONS  Im z  y  z = x + iy  x  Re z  −y  ∗  z  = x − iy  Figure 3.6 The complex conjugate as a mirror image in the real axis.  In the case where z can be written in the form x + iy it is easily veriﬁed, by gives a real result:  direct multiplication of the components, that the product zz  ∗  ∗  =  x + iy  x − iy  = x2 − ixy + ixy − i2y2 = x2 + y2 = z2.  zz  Complex conjugation corresponds to a reﬂection of z in the real axis of the Argand diagram, as may be seen in ﬁgure 3.6.   cid:1 Find the complex conjugate of z = a + 2i + 3ib.  The complex number is written in the standard form  then, replacing i by −i, we obtain  z = a + i 2 + 3b ;  ∗  z  = a − i 2 + 3b .  cid:2   In some cases, however, it may not be simple to rearrange the expression for z into the standard form x + iy. Nevertheless, given two complex numbers, z1 and z2, it is straightforward to show that the complex conjugate of their sum  or diﬀerence  is equal to the sum  or diﬀerence  of their complex conjugates, i.e. ∗ 2. Similarly, it may be shown that the complex conjugate of the product  or quotient  of z1 and z2 is equal to the product  or quotient  of their ∗ complex conjugates, i.e.  z1z2    z1 ± z2  ∗  ∗ ∗ 2 and  z1 z2   ± z  ∗ 1 z  ∗ 1z  = z  = z  = z  ∗ 2.  ∗ 1  Using these results, it can be deduced that, no matter how complicated the expression, its complex conjugate may always be found by replacing every i by  −i. To apply this rule, however, we must always ensure that all complex parts are  ﬁrst written out in full, so that no i’s are hidden.  90   3.2 MANIPULATION OF COMPLEX NUMBERS   cid:1 Find the complex conjugate of the complex number z = w 3y+2ix , where w = x + 5i.  Although we do not discuss complex powers until section 3.5, the simple rule given above still enables us to ﬁnd the complex conjugate of z.  In this case w itself contains real and imaginary components and so must be written  out in full, i.e.  Now we can replace each i by −i to obtain  z = w3y+2ix =  x + 5i 3y+2ix.  It can be shown that the product zz  ∗ z ∗  =  x − 5i  3y−2ix . is real, as required.  cid:2   The following properties of the complex conjugate are easily proved and others  may be derived from them. If z = x + iy then  ∗   z  ∗   ∗ ∗  z + z  z − z  = z,   cid:7   = 2 Re z = 2x,  = 2i Im z = 2iy,   cid:8    cid:7   x2 − y2  x2 + y2  z ∗ = z  + i  2xy  x2 + y2  .   cid:8   The derivation of this last relation relies on the results of the following subsection.  The division of two complex numbers z1 and z2 bears some similarity to their multiplication. Writing the quotient in component form we obtain  3.2.5 Division  z1 z2  =  x1 + iy1 x2 + iy2  .  In order to separate the real and imaginary components of the quotient, we multiply both numerator and denominator by the complex conjugate of the denominator. By deﬁnition, this process will leave the denominator as a real quantity. Equation  3.16  gives  z1 z2  =  =   x1 + iy1  x2 − iy2   x2 + iy2  x2 − iy2   =  x1x2 + y1y2  x2 2 + y2 2  + i  x2y1 − x1y2 x2 2 + y2 2  .   x1x2 + y1y2  + i x2y1 − x1y2   x2 2 + y2 2  Hence we have separated the quotient into real and imaginary components, as required.  1, so that x2 = x1 and y2 = −y1, the general ∗  In the special case where z2 = z  result reduces to  3.15 .  91   3.12    3.13    3.14    3.15    3.16    COMPLEX NUMBERS AND HYPERBOLIC FUNCTIONS   cid:1 Express z in the form x + iy, when  3 − 2i −1 + 4i  .  z =  Multiplying numerator and denominator by the complex conjugate of the denominator we obtain  In analogy to  3.10  and  3.11 , which describe the multiplication of two  complex numbers, the following relations apply to division:   3 − 2i  −1 − 4i   −1 + 4i  −1 − 4i   =  −11 − 10i  17  z =  = − 11  17  − 10 17  i.  cid:2    cid:20  cid:20  cid:20  cid:20  =   cid:20  cid:20  cid:20  cid:20  z1  cid:8   z2  z1 z2 ,  = arg z1 − arg z2.   cid:7   arg  z1 z2   3.17    3.18   The proof of these relations is left until subsection 3.3.1.  3.3 Polar representation of complex numbers  Although considering a complex number as the sum of a real and an imaginary part is often useful, sometimes the polar representation proves easier to manipulate. This makes use of the complex exponential function, which is deﬁned by  ez = exp z ≡ 1 + z +  z2 2!  +  z3 3!  + ··· .   3.19   Strictly speaking it is the function exp z that is deﬁned by  3.19 . The number e is the value of exp 1 , i.e. it is just a number. However, it may be shown that ez and exp z are equivalent when z is real and rational and mathematicians then deﬁne their equivalence for irrational and complex z. For the purposes of this book we will not concern ourselves further with this mathematical nicety but, rather, assume that  3.19  is valid for all z. We also note that, using  3.19 , by multiplying together the appropriate series we may show that  see chapter 24   ez1ez2 = ez1+z2 ,   3.20   which is analogous to the familiar result for exponentials of real numbers.  92   3.3 POLAR REPRESENTATION OF COMPLEX NUMBERS  Im z  y  r  θ  z = reiθ  x  Re z  Figure 3.7 The polar representation of a complex number.  From  3.19 , it immediately follows that for z = iθ, θ real,  eiθ = 1 + iθ − θ2 2! θ4 4!  = 1 − θ2  2!  +   cid:7  − iθ3 + ··· 3! θ − θ3 − ··· + i  − ···  θ5 5!  +  3!   cid:8   and hence that  eiθ = cos θ + i sin θ,   3.21    3.22    3.23   where the last equality follows from the series expansions of the sine and cosine functions  see subsection 4.6.3 . This last relationship is called Euler’s equation. It also follows from  3.23  that  for all n. From Euler’s equation  3.23  and ﬁgure 3.7 we deduce that  einθ = cos nθ + i sin nθ  reiθ = r cos θ + i sin θ   = x + iy.  Thus a complex number may be represented in the polar form  z = reiθ.   3.24   Referring again to ﬁgure 3.7, we can identify r with z and θ with arg z. The  simplicity of the representation of the modulus and argument is one of the main reasons for using the polar representation. The angle θ lies conventionally in the  range −π < θ ≤ π, but, since rotation by θ is the same as rotation by 2nπ + θ,  where n is any integer,  reiθ ≡ rei θ+2nπ .  93   COMPLEX NUMBERS AND HYPERBOLIC FUNCTIONS  Im z  r1r2ei θ1+θ2   r2eiθ2  r1eiθ1  Re z  Figure 3.8 The multiplication of two complex numbers. In this case r1 and r2 are both greater than unity.  The algebra of the polar representation is diﬀerent from that of the real and imaginary component representation, though, of course, the results are identical. Some operations prove much easier in the polar representation, others much more complicated. The best representation for a particular problem must be determined by the manipulation required.  3.3.1 Multiplication and division in polar form  Multiplication and division in polar form are particularly simple. The product of z1 = r1eiθ1 and z2 = r2eiθ2 is given by  z1z2 = r1eiθ1 r2eiθ2  = r1r2ei θ1+θ2 .  The relations z1z2 = z1z2 and arg z1z2  = arg z1 + arg z2 follow immediately.  An example of the multiplication of two complex numbers is shown in ﬁgure 3.8.  Division is equally simple in polar form; the quotient of z1 and z2 is given by   3.25    3.26   relations z1 z2 = z1 z2 and arg z1 z2  = arg z1 − arg z2 are again  The  z1 z2  =  r1eiθ1 r2eiθ2  =  r1 r2  ei θ1−θ2 .  94   3.4 DE MOIVRE’S THEOREM  Im z  r1eiθ1  r2eiθ2  ei θ1−θ2   r1 r2  Re z  Figure 3.9 The division of two complex numbers. As in the previous ﬁgure, r1 and r2 are both greater than unity.  immediately apparent. The division of two complex numbers in polar form is shown in ﬁgure 3.9.  3.4 de Moivre’s theorem   cid:5    cid:6   We now derive an extremely important theorem. Since  eiθ  n = einθ, we have   cos θ + i sin θ n = cos nθ + i sin nθ,   3.27   where the identity einθ = cos nθ + i sin nθ follows from the series deﬁnition of einθ  see  3.21  . This result is called de Moivre’s theorem and is often used in the manipulation of complex numbers. The theorem is valid for all n whether real, imaginary or complex.  There are numerous applications of de Moivre’s theorem but this section examines just three: proofs of trigonometric identities; ﬁnding the nth roots of unity; and solving polynomial equations with complex roots.  3.4.1 Trigonometric identities  The use of de Moivre’s theorem in ﬁnding trigonometric identities is best illus- trated by example. We consider the expression of a multiple-angle function in terms of a polynomial in the single-angle function, and its converse.  95   COMPLEX NUMBERS AND HYPERBOLIC FUNCTIONS   cid:1 Express sin 3θ and cos 3θ in terms of powers of cos θ and sin θ.  Using de Moivre’s theorem,  cos 3θ + i sin 3θ =  cos θ + i sin θ 3  =  cos3 θ − 3 cos θ sin2 θ  + i 3 sin θ cos2 θ − sin3 θ .  We can equate the real and imaginary coeﬃcients separately, i.e.  and  and  cos 3θ = cos3 θ − 3 cos θ sin2 θ  = 4 cos3 θ − 3 cos θ  sin 3θ = 3 sin θ cos2 θ − sin3 θ = 3 sin θ − 4 sin3 θ.  cid:2   1 zn  zn + zn − 1  zn  = 2 cos nθ,  = 2i sin nθ.  This method can clearly be applied to ﬁnding power expansions of cos nθ and  sin nθ for any positive integer n.  The converse process uses the following properties of z = eiθ,  These equalities follow from simple applications of de Moivre’s theorem, i.e.  zn +  1 zn  =  cos θ + i sin θ n +  cos θ + i sin θ  = cos nθ + i sin nθ + cos −nθ  + i sin −nθ  = cos nθ + i sin nθ + cos nθ − i sin nθ  −n  = 2 cos nθ  zn − 1  zn  =  cos θ + i sin θ n −  cos θ + i sin θ  −n = cos nθ + i sin nθ − cos nθ + i sin nθ  = 2i sin nθ.  In the particular case where n = 1,  = eiθ + e = eiθ − e  −iθ = 2 cos θ, −iθ = 2i sin θ.  z +  z − 1  1 z  z  96   3.28    3.29    3.30    3.31    3.32    3.33     cid:1 Find an expression for cos3 θ in terms of cos 3θ and cos θ.  Using  3.32 ,  3.4 DE MOIVRE’S THEOREM  cos3 θ =  z +  1 z   cid:7   cid:7   cid:7   1 23  1 8  1 8  =  =   cid:8   3   cid:8   3 z  +   cid:8    cid:7   1 z3   cid:8   z3 + 3z +  +  z3 +  1 z3  3 8  z +  .  1 z  cos3 θ = 1  4 cos 3θ + 3  4 cos θ.  cid:2   Now using  3.30  and  3.32 , we ﬁnd  This result happens to be a simple rearrangement of  3.29 , but cases involving larger values of n are better handled using this direct method than by rearranging polynomial expansions of multiple-angle functions.  3.4.2 Finding the nth roots of unity  The equation z2 = 1 has the familiar solutions z = ±1. However, now that  we have introduced the concept of complex numbers we can solve the general equation zn = 1. Recalling the fundamental theorem of algebra, we know that the equation has n solutions. In order to proceed we rewrite the equation as  where k is any integer. Now taking the nth root of each side of the equation we ﬁnd  Hence, the solutions of zn = 1 are  z1,2,...,n = 1, e2iπ n,  . . . , e2i n−1 π n,  corresponding to the values 0, 1, 2, . . . , n − 1 for k. Larger integer values of k do  not give new solutions, since the roots already listed are simply cyclically repeated for k = n, n + 1, n + 2, etc.  cid:1 Find the solutions to the equation z3 = 1.  By applying the above method we ﬁnd  Hence the three solutions are z1 = e0i = 1, z2 = e2iπ 3, z3 = e4iπ 3. We note that, as expected, the next solution, for which k = 3, gives z4 = e6iπ 3 = 1 = z1, so that there are only three separate solutions.  cid:2   zn = e2ikπ,  z = e2ikπ n.  z = e2ikπ 3.  97   COMPLEX NUMBERS AND HYPERBOLIC FUNCTIONS  Im z  e2iπ 3  −2iπ 3 e  2π 3  2π 3  1  Re z  Figure 3.10 The solutions of z3 = 1.  Not surprisingly, given that z3 = z3 from  3.10 , all the roots of unity have  unit modulus, i.e. they all lie on a circle in the Argand diagram of unit radius. The three roots are shown in ﬁgure 3.10.  The cube roots of unity are often written 1, ω and ω2. The properties ω3 = 1  and 1 + ω + ω2 = 0 are easily proved.  3.4.3 Solving polynomial equations  A third application of de Moivre’s theorem is to the solution of polynomial equations. Complex equations in the form of a polynomial relationship must ﬁrst be solved for z in a similar fashion to the method for ﬁnding the roots of real polynomial equations. Then the complex roots of z may be found.  cid:1 Solve the equation z6 − z5 + 4z4 − 6z3 + 2z2 − 8z + 8 = 0.  We ﬁrst factorise to give   z3 − 2  z2 + 4  z − 1  = 0.  Hence z3 = 2 or z2 = −4 or z = 1. The solutions to the quadratic equation are z = ±2i;  to ﬁnd the complex cube roots, we ﬁrst write the equation in the form  where k is any integer. If we now take the cube root, we get  z3 = 2 = 2e2ikπ,  z = 21 3e2ikπ 3.  98   3.5 COMPLEX LOGARITHMS AND COMPLEX POWERS  To avoid the duplication of solutions, we use the fact that −π < arg z ≤ π and ﬁnd  z1 = 21 3,  z2 = 21 3e2πi 3 = 21 3  z3 = 21 3e  −2πi 3 = 21 3   cid:30   cid:30   − 1 2 − 1 2  +  −   cid:31   cid:31   ,  .  √ 3 2 √ 3 2  i  i  The complex numbers z1, z2 and z3, together with z4 = 2i, z5 = −2i and z6 = 1 are the  solutions to the original polynomial equation. As expected from the fundamental theorem of algebra, we ﬁnd that the total number of complex roots  six, in this case  is equal to the largest power of z in the polynomial.  cid:2   A useful result is that the roots of a polynomial with real coeﬃcients occur in ∗ conjugate pairs  i.e. if z1 is a root, then z 1 is a second distinct root, unless z1 is real . This may be proved as follows. Let the polynomial equation of which z is a root be  anzn + an−1zn−1 + ··· + a1z + a0 = 0.  Taking the complex conjugate of this equation,  But the an are real, and so z  ∗  ∗ n z a  ∗  ∗ n−1 z  n + a ∗ satisﬁes  ∗  n−1 + ··· + a 1z  ∗  ∗ + a 0 = 0.  ∗  an z   n + an−1 z  ∗   n−1 + ··· + a1z  ∗  + a0 = 0,  and is also a root of the original equation.  3.5 Complex logarithms and complex powers  The concept of a complex exponential has already been introduced in section 3.3, where it was assumed that the deﬁnition of an exponential as a series was valid for complex numbers as well as for real numbers. Similarly we can deﬁne the logarithm of a complex number and we can use complex numbers as exponents. Let us denote the natural logarithm of a complex number z by w = Ln z, where  the notation Ln will be explained shortly. Thus, w must satisfy  Using  3.20 , we see that  z = ew.  z1z2 = ew1 ew2 = ew1+w2 ,  and taking logarithms of both sides we ﬁnd  Ln  z1z2  = w1 + w2 = Ln z1 + Ln z2,   3.34   which shows that the familiar rule for the logarithm of the product of two real numbers also holds for complex numbers.  99   COMPLEX NUMBERS AND HYPERBOLIC FUNCTIONS  We may use  3.34  to investigate further the properties of Ln z. We have already noted that the argument of a complex number is multivalued, i.e. arg z = θ + 2nπ, where n is any integer. Thus, in polar form, the complex number z should strictly be written as  z = rei θ+2nπ .  Taking the logarithm of both sides, and using  3.34 , we ﬁnd  Ln z = ln r + i θ + 2nπ ,   3.35   where ln r is the natural logarithm of the real positive quantity r and so is written normally. Thus from  3.35  we see that Ln z is itself multivalued. To avoid this multivalued behaviour it is conventional to deﬁne another function ln z, the principal value of Ln z, which is obtained from Ln z by restricting the argument  of z to lie in the range −π < θ ≤ π.  cid:1 Evaluate Ln  −i .  cid:18   cid:19  By rewriting −i as a complex exponential, we ﬁnd  Ln  −i  = Ln  ei −π 2+2nπ   = i −π 2 + 2nπ ,  where n is any integer. Hence Ln  −i  = −iπ 2, 3iπ 2, principal value of Ln  −i , is given by ln −i  = −iπ 2.  cid:2   . . . . We note that ln −i , the  If z and t are both complex numbers then the zth power of t is deﬁned by  tz = ezLn t.  Since Ln t is multivalued, so too is this deﬁnition.  cid:1 Simplify the expression z = i  −2i.  Firstly we take the logarithm of both sides of the equation to give  Now inverting the process we ﬁnd  We can write i = ei π 2+2nπ , where n is any integer, and hence  We can now simplify z to give  Ln z = −2i Ln i.  eLn z = z = e   cid:22    cid:5    cid:23   −2iLn i.  cid:6   Ln i = Ln  ei π 2+2nπ   = i  π 2 + 2nπ  .  −2i = e  i  −2i×i π 2+2nπ   = e π+4nπ ,  100  which, perhaps surprisingly, is a real quantity rather than a complex one.  cid:2   Complex powers and the logarithms of complex numbers are discussed further  in chapter 24.   3.6 APPLICATIONS TO DIFFERENTIATION AND INTEGRATION  3.6 Applications to diﬀerentiation and integration  We can use the exponential form of a complex number together with de Moivre’s theorem  see section 3.4  to simplify the diﬀerentiation of trigonometric functions.  cid:1 Find the derivative with respect to x of e3x cos 4x.  We could diﬀerentiate this function straightforwardly using the product rule  see subsec- tion 2.1.2 . However, an alternative method in this case is to use a complex exponential. Let us consider the complex number  z = e3x cos 4x + i sin 4x  = e3xe4ix = e 3+4i x,  where we have used de Moivre’s theorem to rewrite the trigonometric functions as a com- plex exponential. This complex number has e3x cos 4x as its real part. Now, diﬀerentiating z with respect to x we obtain  dz dx  =  3 + 4i e 3+4i x =  3 + 4i e3x cos 4x + i sin 4x ,   3.36   where we have again used de Moivre’s theorem. Equating real parts we then ﬁnd  e3x cos 4x  = e3x 3 cos 4x − 4 sin 4x .  By equating the imaginary parts of  3.36 , we also obtain, as a bonus,  e3x sin 4x  = e3x 4 cos 4x + 3 sin 4x .  cid:2    cid:6    cid:6   In a similar way the complex exponential can be used to evaluate integrals  containing trigonometric and exponential functions.  cid:1 Evaluate the integral I =  eax cos bx dx.  Let us consider the integrand as the real part of the complex number  eax cos bx + i sin bx  = eaxeibx = e a+ib x,  where we use de Moivre’s theorem to rewrite the trigonometric functions as a complex exponential. Integrating we ﬁnd  d dx   cid:5   d dx   cid:5    cid:1    cid:21   e a+ib x dx =  + c  e a+ib x a + ib  a − ib e a+ib x  a − ib  a + ib    cid:5   eax  a2 + b2  =  =  + c   cid:6   aeibx − ibeibx  + c,   3.37    cid:21   cid:21   where the constant of integration c is in general complex. Denoting this constant by c = c1 + ic2 and equating real parts in  3.37  we obtain  I =  eax cos bx dx =   a cos bx + b sin bx  + c1,  eax  a2 + b2  which agrees with result  2.37  found using integration by parts. Equating imaginary parts in  3.37  we obtain, as a bonus,  J =  eax sin bx dx =   a sin bx − b cos bx  + c2.  cid:2   eax  a2 + b2  101   COMPLEX NUMBERS AND HYPERBOLIC FUNCTIONS  3.7 Hyperbolic functions  The hyperbolic functions are the complex analogues of the trigonometric functions. The analogy may not be immediately apparent and their deﬁnitions may appear at ﬁrst to be somewhat arbitrary. However, careful examination of their properties reveals the purpose of the deﬁnitions. For instance, their close relationship with the trigonometric functions, both in their identities and in their calculus, means that many of the familiar properties of trigonometric functions can also be applied to the hyperbolic functions. Further, hyperbolic functions occur regularly, and so giving them special names is a notational convenience.  The two fundamental hyperbolic functions are cosh x and sinh x, which, as their names suggest, are the hyperbolic equivalents of cos x and sin x. They are deﬁned by the following relations:  Note that cosh x is an even function and sinh x is an odd function. By analogy with the trigonometric functions, the remaining hyperbolic functions are  3.7.1 Deﬁnitions  cosh x = 1 sinh x = 1  2  ex + e 2  ex − e  −x , −x .  tanh x =  sech x =  cosech x =  coth x =  ex − e  ex + e  −x −x ,  2  −x ,  2  ex − e ex + e ex − e  −x , −x −x .  =  =  =  =  sinh x cosh x  1  1  1  sinh x  tanh x  cosh x  ex + e   3.38    3.39    3.40    3.41    3.42    3.43   All the hyperbolic functions above have been deﬁned in terms of the real variable x. However, this was simply so that they may be plotted  see ﬁgures 3.11–3.13 ; the deﬁnitions are equally valid for any complex number z.  3.7.2 Hyperbolic–trigonometric analogies  In the previous subsections we have alluded to the analogy between trigonometric and hyperbolic functions. Here, we discuss the close relationship between the two groups of functions.  Recalling  3.32  and  3.33  we ﬁnd  cos ix = 1 sin ix = 1  2  ex + e 2 i ex − e  −x , −x .  102   3.7 HYPERBOLIC FUNCTIONS  4  3  2  1  4  2  cosh x  −2  −1  sech x  2  x  1  Figure 3.11 Graphs of cosh x and sechx.  cosech x  sinh x  −2  −1  1  2  x  −2  −4  cosech x  Figure 3.12 Graphs of sinh x and cosechx.  Hence, by the deﬁnitions given in the previous subsection,  cosh x = cos ix,  i sinh x = sin ix,  cos x = cosh ix,  i sin x = sinh ix.  103   3.44    3.45    3.46    3.47   These useful equations make the relationship between hyperbolic and trigono-   COMPLEX NUMBERS AND HYPERBOLIC FUNCTIONS  −2  −1  tanh x 1  2  x  coth x  4  2  −2  −4  coth x  Figure 3.13 Graphs of tanh x and coth x.  metric functions transparent. The similarity in their calculus is discussed further in subsection 3.7.6.  3.7.3 Identities of hyperbolic functions  The analogies between trigonometric functions and hyperbolic functions having been established, we should not be surprised that all the trigonometric identities also hold for hyperbolic functions, with the following modiﬁcation. Wherever  sin2 x occurs it must be replaced by − sinh2 x, and vice versa. Note that this replacement is necessary even if the sin2 x is hidden, e.g. tan2 x = sin2 x  cos2 x and so must be replaced by  − sinh2 x  cosh2 x  = − tanh2 x.  cid:1 Find the hyperbolic identity analogous to cos2 x + sin2 x = 1. Using the rules stated above cos2 x is replaced by cosh2 x, and sin2 x by − sinh2 x, and so  the identity becomes  cosh2 x − sinh2 x = 1.  This can be veriﬁed by direct substitution, using the deﬁnitions of cosh x and sinh x; see  3.38  and  3.39 .  cid:2   Some other identities that can be proved in a similar way are  sech2x = 1 − tanh2 x, cosech2x = coth2 x − 1,  sinh 2x = 2 sinh x cosh x, cosh 2x = cosh2 x + sinh2 x.  104   3.48    3.49    3.50    3.51    3.7 HYPERBOLIC FUNCTIONS  3.7.4 Solving hyperbolic equations  When we are presented with a hyperbolic equation to solve, we may proceed by analogy with the solution of trigonometric equations. However, it is almost always easier to express the equation directly in terms of exponentials.   cid:1 Solve the hyperbolic equation cosh x − 5 sinh x − 5 = 0.  Substituting the deﬁnitions of the hyperbolic functions we obtain  Rearranging, and then multiplying through by −ex, gives in turn  1  2  ex + e  −x  − 5  2  ex − e  −x  − 5 = 0.  and  Now we can factorise and solve:  −2ex + 3e  −x − 5 = 0  2e2x + 5ex − 3 = 0.   2ex − 1  ex + 3  = 0.  Thus ex = 1 2 or ex = −3. Hence x = − ln 2 or x = ln −3 . The interpretation of the logarithm of a negative number has been discussed in section 3.5.  cid:2   3.7.5 Inverses of hyperbolic functions  Just like trigonometric functions, hyperbolic functions have inverses. If y = −1 y, which serves as a deﬁnition of the inverse. By using cosh x then x = cosh the fundamental deﬁnitions of hyperbolic functions, we can ﬁnd closed-form expressions for their inverses. This is best illustrated by example.   cid:1 Find a closed-form expression for the inverse hyperbolic function y = sinh  −1 x.  First we write x as a function of y, i.e.  Now, since cosh y = 1  2  ey + e  y = sinh −y  and sinh y = 1  2  ey − e  −1 x ⇒ x = sinh y. −y ,  cid:2   cid:24   1 + sinh2 y + sinh y  ey = cosh y + sinh y  =  ey =  1 + x2 + x,   cid:24   y = ln   1 + x2 + x .  cid:2   and hence  In a similar fashion it can be shown that √ −1 x = ln   cosh  x2 − 1 + x .  105   COMPLEX NUMBERS AND HYPERBOLIC FUNCTIONS  −1x  sech  −1x  sech  4  2  −2  −4  1  2  3  4  x  −1 x  cosh  −1 x  cosh  Figure 3.14 Graphs of cosh  −1 x and sech  −1x.   cid:1 Find a closed-form expression for the inverse hyperbolic function y = tanh  −1 x.  First we write x as a function of y, i.e.  y = tanh  −1 x ⇒ x = tanh y.  Now, using the deﬁnition of tanh y and rearranging, we ﬁnd  Thus, it follows that  ey − e −y −y  ey + e  x =  e2y =  1 + x  1 − x  −y =  1 − x ey.  ⇒ ey =  ⇒  x + 1 e     1 − x  cid:7  1 − x 1 − x  −1 x =  y = ln  1 + x  1 + x  1 + x  1 2  ln  ,  ,  tanh   cid:8   .  cid:2   Graphs of the inverse hyperbolic functions are given in ﬁgures 3.14–3.16.  3.7.6 Calculus of hyperbolic functions  Just as the identities of hyperbolic functions closely follow those of their trigono- metric counterparts, so their calculus is similar. The derivatives of the two basic  106   3.7 HYPERBOLIC FUNCTIONS  cosech  −1x  −1 x  sinh  1  2  x  −2  −1  cosech  −1x  Figure 3.15 Graphs of sinh  −1 x and cosech  −1x.  −1 x  tanh  −1 x  coth  1  2  x  −2  −1  −1 x  coth  4  2  −2  −4  4  2  −2  −4  Figure 3.16 Graphs of tanh  −1 x and coth  −1 x.  hyperbolic functions are given by  They may be deduced by considering the deﬁnitions  3.38 ,  3.39  as follows.  d dx d dx   cosh x  = sinh x,   sinh x  = cosh x.  107   3.52    3.53    COMPLEX NUMBERS AND HYPERBOLIC FUNCTIONS   cid:1 Verify the relation  d dx  cosh x = sinh x.  Using the deﬁnition of cosh x,  and diﬀerentiating directly, we ﬁnd  cosh x = 1  2  ex + e  −x ,  d dx   cosh x  = 1  2  ex − e −x  = sinh x.  cid:2   Clearly the integrals of the fundamental hyperbolic functions are also deﬁned by these relations. The derivatives of the remaining hyperbolic functions can be derived by product diﬀerentiation and are presented below only for complete- ness.  The inverse hyperbolic functions also have derivatives, which are given by the  following:  d dx d dx   tanh x  = sech2x,  sech x  = −sech x tanh x,  cosech x  = −cosech x coth x,  coth x  = −cosech2x.  d dx  d dx   cid:10   cid:10   cid:10   cid:10    cid:9   cid:9   cid:9   cid:9   d dx d dx d dx d dx  cosh  sinh  tanh  coth  −1 x a −1 x a −1 x a −1 x a  ,  ,  1√ x2 − a2 1√ x2 + a2 a  a2 − x2 , −a x2 − a2 ,  =  =  =  =  for x2 < a2,  for x2 > a2.   3.54    3.55    3.56    3.57    3.58    3.59    3.60    3.61   These may be derived from the logarithmic form of the inverse  see subsec-  tion 3.7.5 .  108    cid:1 Evaluate  d dx  sinh  From the results of section 3.7.5,  3.8 EXERCISES   cid:6   −1 x using the logarithmic form of the inverse.  cid:5   cid:8   cid:31    cid:10  cid:23  x√ x2 + 1   cid:24   cid:7   cid:30 √  −1 x  x2 + 1  d dx  sinh  x +  1 +  =  =  d dx  x +   cid:9    cid:22  ln √ 1 x2 + 1 √ 1 x2 + 1 .  cid:2   =  =  x + 1√ x2 + 1  √ x2 + 1 + x x2 + 1  3.1  3.2  3.3 3.4  3.5  3.6  3.8  3.8 Exercises  Two complex numbers z and w are given by z = 3 + 4i and w = 2 − i. On an  a  z + w,  b  w − z,  c  wz,  d  z w,  Argand diagram, plot  ∗  ∗   e  z  w + w  z,  f  w2,  g  ln z,  h   1 + z + w 1 2.  By considering the real and imaginary parts of the product eiθeiφ prove the standard formulae for cos θ + φ  and sin θ + φ .  By writing π 12 =  π 3  −  π 4  and considering eiπ 12, evaluate cot π 12 .  Find the locus in the complex z-plane of points that satisfy the following equa- tions.   cid:7    cid:8   , where c is complex, ρ is real and t is a real parameter   a  z − c = ρ  1 + it  1 − it  that varies in the range −∞ < t < ∞.   b  z = a + bt + ct2, in which t is a real parameter and a, b, and c are complex  numbers with b c real.  Evaluate  √  a  Re exp 2iz ,  b  Im cosh2 z ,  c   −1 + 3i 1 2,  d   exp i1 2 ,  e  exp i3 ,  f  Im 2i+3 ,  g  ii,  h  ln[   √ 3 + i 3].  Find the equations in terms of x and y of the sets of points in the Argand diagram that satisfy the following:   a  Re z2 = Im z2;  b   Im z2  z2 = −i;  c  arg[z  z − 1 ] = π 2.  3.7  Show that the locus of all points z = x + iy in the complex plane that satisfy  z − ia = λz + ia,  λ > 0,  is a circle of radius 2λa  1 − λ2  centred on the point z = ia[ 1 + λ2   1 − λ2 ].  Sketch the circles for a few typical values of λ, including λ   1 and λ = 1. The two sets of points z = a, z = b, z = c, and z = A, z = B, z = C are the corners of two similar triangles in the Argand diagram. Express in terms of a, b, . . . , C  109   COMPLEX NUMBERS AND HYPERBOLIC FUNCTIONS  the equalities of corresponding angles, and   a   b  the constant ratio of corresponding sides,  in the two triangles.  By noting that any complex quantity can be expressed as  z = z exp i arg z ,  deduce that  a B − C  + b C − A  + c A − B  = 0.  cid:8  cid:15   cid:8  cid:15   c > 0,  = c,  0 ≤ k ≤ π 2.   a  Re  ln   cid:12   cid:12    cid:7   cid:7   z − ia z − ia  z + ia  z + ia   b  Im  ln  = k,  3.9  For the real constant a ﬁnd the loci of all points z = x + iy in the complex plane that satisfy  3.10  Identify the two families of curves and verify that in case  b  all curves pass  through the two points ±ia.  The most general type of transformation between one Argand diagram, in the z-plane, and another, in the Z -plane, that gives one and only one value of Z for each value of z  and conversely  is known as the general bilinear transformation and takes the form   a  Conﬁrm that the transformation from the Z -plane to the z-plane is also a  general bilinear transformation.   b  Recalling that the equation of a circle can be written in the form  z =  aZ + b cZ + d  .   cid:20  cid:20  cid:20  cid:20  z − z1  z − z2   cid:20  cid:20  cid:20  cid:20  = λ,  λ  cid:3 = 1,  show that the general bilinear transformation transforms circles into circles  or straight lines . What is the condition that z1, z2 and λ must satisfy if the transformed circle is to be a straight line?  3.11  Sketch the parts of the Argand diagram in which   a  Re z2 < 0, z1 2 ≤ 2;  b  0 ≤ arg z  ∗ ≤ π 2;   exp z3 → 0 as z → ∞.   c   3.12  What is the area of the region in which all three sets of conditions are satisﬁed? Denote the nth roots of unity by 1, ωn, ω2  .   a  Prove that  n−1 cid:4   r=0   i   ωr  n = 0,   ii   n  n, . . . , ωn−1 n−1 cid:3   n =  −1 n+1.  ωr  r=0   b  Express x2 + y2 + z2 − yz − zx− xy as the product of two factors, each linear  in x, y and z, with coeﬃcients dependent on the third roots of unity  and those of the x terms arbitrarily taken as real .  110   3.13  3.14  3.15  3.16  3.8 EXERCISES   cid:14  Prove that x2m+1 − a2m+1, where m is an integer ≥ 1, can be written as   cid:7    cid:8    cid:13   x2m+1 − a2m+1 =  x − a   x2 − 2ax cos  2πr  2m + 1  + a2  .  m cid:3   r=1  The complex position vectors of two parallel interacting equal ﬂuid vortices moving with their axes of rotation always perpendicular to the z-plane are z1 and z2. The equations governing their motions are  ∗ dz 1 dt  = − i  z1 − z2  ,  ∗ dz 2 dt  = − i  z2 − z1  .  Deduce that  a  z1 + z2,  b  z1 − z2 and  c  z12 + z22 are all constant in time,  and hence describe the motion geometrically. Solve the equation  z7 − 4z6 + 6z5 − 6z4 + 6z3 − 12z2 + 8z + 4 = 0,  a  by examining the eﬀect of setting z3 equal to 2, and then  b  by factorising and using the binomial expansion of  z + a 4.  Plot the seven roots of the equation on an Argand plot, exemplifying that complex roots of a polynomial equation always occur in conjugate pairs if the polynomial has real coeﬃcients. The polynomial f z  is deﬁned by  f z  = z5 − 6z4 + 15z3 − 34z2 + 36z − 48.   a  Show that the equation f z  = 0 has roots of the form z = λi, where λ is  real, and hence factorize f z .   b  Show further that the cubic factor of f z  can be written in the form  z + a 3 + b, where a and b are real, and hence solve the equation f z  = 0 completely.  3.17  The binomial expansion of  1 + x n, discussed in chapter 1, can be written for a positive integer n as  n cid:4   r=0   1 + x n =  nCrxr,  where nCr = n! [r! n − r !].   a  Use de Moivre’s theorem to show that the sum  S1 n  = nC0 − nC2 + nC4 − ··· +  −1 m nC2m,  n − 1 ≤ 2m ≤ n,  has the value 2n 2 cos nπ 4 .   b  Derive a similar result for the sum  S2 n  = nC1 − nC3 + nC5 − ··· +  −1 m nC2m+1,  n − 1 ≤ 2m + 1 ≤ n,  and verify it for the cases n = 6, 7 and 8.  3.18  By considering  1 + exp iθ n, prove that  n cid:4  n cid:4  where nCr = n! [r! n − r !].  r=0  r=0  nCr cos rθ = 2n cosn θ 2  cos nθ 2 ,  nCr sin rθ = 2n cosn θ 2  sin nθ 2 ,  111   COMPLEX NUMBERS AND HYPERBOLIC FUNCTIONS  3.19  Use de Moivre’s theorem with n = 4 to prove that  and deduce that  cos 4θ = 8 cos4 θ − 8 cos2 θ + 1,   cid:30    cid:31   √ 2  1 2  .  cos  =  π 8  2 + 4  3.20  3.21  Express sin4 θ entirely in terms of the trigonometric functions of multiple angles and deduce that its average value over a complete cycle is 3 8 . Use de Moivre’s theorem to prove that  3.22  where t = tan θ. Deduce the values of tan nπ 10  for n = 1, 2, 3, 4. Prove the following results involving hyperbolic functions.  tan 5θ =  ,  t5 − 10t3 + 5t 5t4 − 10t2 + 1  cid:8    cid:7    cid:7    cid:8   .  x − y  2   a  That   b  That, if y = sinh  −1 x,  cosh x − cosh y = 2 sinh  x + y  2  sinh   x2 + 1   + x  = 0.  d2y dx2  dy dx  3.23  Determine the conditions under which the equation  a cosh x + b sinh x = c,  c > 0,  3.24  has zero, one, or two real solutions for x. What is the solution if a2 = c2 + b2? Use the deﬁnitions and properties of hyperbolic functions to do the following:  3.25  3.26   a  Solve cosh x = sinh x + 2 sech x.  b  Show that the real solution x of tanh x = cosech x can be written in the  √ u . Find an explicit value for u.  form x = ln u +   c  Evaluate tanh x when x is the real solution of cosh 2x = 2 cosh x. Express sinh4 x in terms of hyperbolic cosines of multiples of x, and hence ﬁnd the real solutions of  2 cosh 4x − 8 cosh 2x + 5 = 0.  In the theory of special relativity, the relationship between the position and time coordinates of an event, as measured in two frames of reference that have parallel x-axes, can be expressed in terms of hyperbolic functions. If the coordinates are x and t in one frame and x in the other, then the relationship take the form  and t   cid:7    cid:7   Express x and ct in terms of x  , ct  and φ and show that   cid:7   cid:7   x  ct  = x cosh φ − ct sinh φ, = −x sinh φ + ct cosh φ.  cid:7    cid:7   x2 −  ct 2 =  x   cid:7    2 −  ct   cid:7    2.  112   3.9 HINTS AND ANSWERS  3.27  A closed barrel has as its curved surface the surface obtained by rotating about the x-axis the part of the curve  y = a[2 − cosh x a ] lying in the range −b ≤ x ≤ b, where b < a cosh  area, A, of the barrel is given by  A = πa[9a − 8a exp −b a  + a exp −2b a  − 2b].  −1 2. Show that the total surface  3.28  The principal value of the logarithmic function of a complex variable is deﬁned  to have its argument in the range −π < arg z ≤ π. By writing z = tan w in terms  of exponentials show that  Use this result to evaluate   cid:7    cid:8   .  tan  ln  1 2i  −1 z =  cid:30   −1  tan  1 + iz  1 − iz  cid:31   .  √ 3 − 3i  2  7  3.9 Hints and answers  ∗  3.  √  a z + z   d  exp 1   2 exp 4πi 3 ;  2 exp πi 3  or  √ 3 2. √   a  5 + 3i;  b  −1 − 5i;  c  10 + 5i;  d  2 5 + 11i 5;  e  4;  f  3 − 4i;  −1 4 3  + 2nπ];  h  ± 2.521 + 0.595i . √  √ 2, sin π 3 = 1 2 and cos π 3 =   g  ln 5 + i[tan Use sin π 4 = cos π 4 = 1  cot π 12 = 2 + √ 2 ;  e  0.540 − 0.841i;  f  8 sin ln 2  = 5.11;   a  exp −2y  cos 2x;  b   sin 2y sinh 2x  2;  c   g  exp −π 2 − 2πn ;  h  ln 8 + i 6n + 1 2 π. Starting from x + iy − ia = λx + iy + ia, show that the coeﬃcients of x and y are equal, and write the equation in the form x2 +  y − α 2 = r2.  a  Circles enclosing z = −ia, with λ = exp c > 1.  b  The condition is that arg[ z− ia   z + ia ] = k. This can be rearranged to give   =  a2 − z2  tan k, which becomes in x, y coordinates the equation  √ 2  or exp −1   of a circle with centre  −a cot k, 0  and radius a cosec k.  All three conditions are satisﬁed in 3π 2 ≤ θ ≤ 7π 4, z ≤ 4; area = 2π. Denoting exp[2πi  2m + 1 ] by Ω, express x2m+1 − a2m+1 as a product of factors like  x − aΩr  and then combine those containing Ωr and Ω2m+1−r. Use the fact The roots are 21 3 exp 2πni 3  for n = 0, 1, 2; 1 ± 31 4; 1 ± 31 4i. Consider  1 + i n.  b  S2 n  = 2n 2 sin nπ 4 . S2 6  = −8, S2 7  = −8, S2 8  = 0. Use the binomial expansion of  cos θ + i sin θ 4. Show that cos 5θ = 16c5 − 20c3 + 5c, where c = cos θ, and correspondingly for [ 5 − √ Reality of the root s  requires c2 + b2 ≥ a2 and a + b > 0. With these conditions, there are two roots if a2 > b2, but only one if b2 > a2. For a2 = c2 + b2, x = 1 Reduce the equation to 16 sinh4 x = 1, yielding x = ±0.481.  −2 θ = 1 + tan2 θ. The four required values are √ 20 1 2.  20  5]1 2,  5 − √  √ 20  5]1 2,  5 +  2 ln[ a − b   a + b ].  that Ω2m+1 = 1.  sin 5θ. Use cos  20 1 2, [ 5 +  3.1  3.3  3.5  3.7  3.9  3.11 3.13  3.15 3.17 3.19 3.21  3.23  3.25  113   COMPLEX NUMBERS AND HYPERBOLIC FUNCTIONS  3.27  Show that ds =  cosh x a  dx;  curved surface area = πa2[8 sinh b a  − sinh 2b a ] − 2πab. ﬂat ends area = 2πa2[4 − 4 cosh b a  + cosh2 b a ].  114   4  Series and limits  4.1 Series  Many examples exist in the physical sciences of situations where we are presented with a sum of terms to evaluate. For example, we may wish to add the contributions from successive slits in a diﬀraction grating to ﬁnd the total light intensity at a particular point behind the grating.  A series may have either a ﬁnite or inﬁnite number of terms. In either case, the  sum of the ﬁrst N terms of a series  often called a partial sum  is written  SN = u1 + u2 + u3 + ··· + uN,  where the terms of the series un, n = 1, 2, 3, . . . , N are numbers, that may in general be complex. If the terms are complex then SN will in general be complex also, and we can write SN = XN + iYN, where XN and YN are the partial sums of the real and imaginary parts of each term separately and are therefore real. If a series has only N terms then the partial sum SN is of course the sum of the series. Sometimes we may encounter series where each term depends on some variable, x, say. In this case the partial sum of the series will depend on the value assumed by x. For example, consider the inﬁnite series  S  x  = 1 + x +  +  x2 2!  + ··· .  x3 3!  This is an example of a power series; these are discussed in more detail in section 4.5. It is in fact the Maclaurin expansion of exp x  see subsection 4.6.3 . Therefore S  x  = exp x and, of course, varies according to the value of the variable x. A series might just as easily depend on a complex variable z.  A general, random sequence of numbers can be described as a series and a sum of the terms found. However, for cases of practical interest, there will usually be  115   SERIES AND LIMITS  some sort of relationship between successive terms. For example, if the nth term of a series is given by  un =  1 2n ,  N cid:4   n=1  for n = 1, 2, 3, . . . , N then the sum of the ﬁrst N terms will be  SN =  un =  +  +  1 2  1 4  + ··· +  1 8  1 2N .   4.1   It is clear that the sum of a ﬁnite number of terms is always ﬁnite, provided that each term is itself ﬁnite. It is often of practical interest, however, to consider the sum of a series with an inﬁnite number of ﬁnite terms. The sum of an inﬁnite number of terms is best deﬁned by ﬁrst considering the partial sum of the ﬁrst N terms, SN. If the value of the partial sum SN tends to a ﬁnite limit, S , as N tends to inﬁnity, then the series is said to converge and its sum is given by the limit S . In other words, the sum of an inﬁnite series is given by  S = lim  N→∞ SN,  provided the limit exists. For complex inﬁnite series, if SN approaches a limit  S = X + iY as N → ∞, this means that XN → X and YN → Y separately, i.e.  the real and imaginary parts of the series are each convergent series with sums X and Y respectively.  However, not all inﬁnite series have ﬁnite sums. As N → ∞, the value of the partial sum SN may diverge: it may approach +∞ or −∞, or oscillate ﬁnitely  or inﬁnitely. Moreover, for a series where each term depends on some variable, its convergence can depend on the value assumed by the variable. Whether an inﬁnite series converges, diverges or oscillates has important implications when describing physical systems. Methods for determining whether a series converges are discussed in section 4.3.  4.2 Summation of series  It is often necessary to ﬁnd the sum of a ﬁnite series or a convergent inﬁnite series. We now describe arithmetic, geometric and arithmetico-geometric series, which are particularly common and for which the sums are easily found. Other methods that can sometimes be used to sum more complicated series are discussed below.  116   4.2 SUMMATION OF SERIES  4.2.1 Arithmetic series  An arithmetic series has the characteristic that the diﬀerence between successive terms is constant. The sum of a general arithmetic series is written  SN = a +  a + d  +  a + 2d  + ··· + [a +  N − 1 d] =   a + nd .  N−1 cid:4   n=0  Rewriting the series in the opposite order and adding this term by term to the original expression for SN, we ﬁnd  SN =  N 2  [a + a +  N − 1 d] =  N 2   ﬁrst term + last term .   4.2   If an inﬁnite number of such terms are added the series will increase  or decrease  indeﬁnitely; that is to say, it diverges.   cid:1 Sum the integers between 1 and 1000 inclusive.  This is an arithmetic series with a = 1, d = 1 and N = 1000. Therefore, using  4.2  we ﬁnd   1 + 1000  = 500500, which can be checked directly only with considerable eﬀort.  cid:2   SN =  2  1000  4.2.2 Geometric series  Equation  4.1  is a particular example of a geometric series, which has the characteristic that the ratio of successive terms is a constant  one-half in this case . The sum of a geometric series is in general written  SN = a + ar + ar2 + ··· + arN−1 =  arn,  N−1 cid:4   n=0  where a is a constant and r is the ratio of successive terms, the common ratio. The sum may be evaluated by considering SN and rSN:  SN = a + ar + ar2 + ar3 + ··· + arN−1, rSN = ar + ar2 + ar3 + ar4 + ··· + arN.  If we now subtract the second equation from the ﬁrst we obtain  and hence   4.3    1 − r SN = a − arN ,  a 1 − rN  1 − r  .  SN =  117   SERIES AND LIMITS  For a series with an inﬁnite number of terms and r < 1, we have limN→∞ rN = 0,  and the sum tends to the limit  S =  a  1 − r  .   4.4   2 , a = 1  2 , and so S = 1. For r ≥ 1, however, the series either diverges  In  4.1 , r = 1 or oscillates.  cid:1 Consider a ball that drops from a height of 27 m and on each bounce retains only a third of its kinetic energy; thus after one bounce it will return to a height of 9 m, after two bounces to 3 m, and so on. Find the total distance travelled between the ﬁrst bounce and the Mth bounce.  The total distance travelled between the ﬁrst bounce and the Mth bounce is given by the  sum of M − 1 terms:  SM−1 = 2  9 + 3 + 1 + ···   = 2  M−2 cid:4   m=0  9 3m  for M > 1, where the factor 2 is included to allow for both the upward and the downward journey. Inside the parentheses we clearly have a geometric series with ﬁrst term 9 and common ratio 1 3 and hence the distance is given by  4.3 , i.e.   cid:23  where the number of terms N in  4.3  has been replaced by M − 1.  cid:2    cid:6  1 − 1  SM−1 = 2 × 9  1 − cid:5   1 − cid:5   M−1  M−1  = 27   cid:23    cid:22    cid:22    cid:6   1 3  1 3  3  ,  4.2.3 Arithmetico-geometric series  An arithmetico-geometric series, as its name suggests, is a combined arithmetic and geometric series. It has the general form  SN = a +  a + d r +  a + 2d r2 + ··· + [a +  N − 1 d] rN−1 =   a + nd rn,  N−1 cid:4   n=0  and can be summed, in a similar way to a pure geometric series, by multiplying by r and subtracting the result from the original series to obtain   1 − r SN = a + rd + r2d + ··· + rN−1d − [a +  N − 1 d] rN.  Using the expression for the sum of a geometric series  4.3  and rearranging, we ﬁnd  a − [a +  N − 1 d] rN  1 − r  rd 1 − rN−1   1 − r 2  .  +  SN =  For an inﬁnite series with r < 1, limN→∞ rN = 0 as in the previous subsection,  and the sum tends to the limit  As for a geometric series, if r ≥ 1 then the series either diverges or oscillates.  S =  a  1 − r  +  rd   1 − r 2 .   4.5   118   4.2 SUMMATION OF SERIES   cid:1 Sum the series  S = 2 +  +  +  5 2  8 22  + ··· .  11 23  This is an inﬁnite arithmetico-geometric series with a = 2, d = 3 and r = 1 2. Therefore, from  4.5 , we obtain S = 10.  cid:2   The diﬀerence method is sometimes useful in summing series that are more complicated than the examples discussed above. Let us consider the general series  4.2.4 The difference method  N cid:4   n=1  un = u1 + u2 + ··· + uN.  un = f n  − f n − 1  N cid:4   un = f N  − f 0 .  SN =  n=1  If the terms of the series, un, can be expressed in the form  for some function f n  then its  partial  sum is given by  This can be shown as follows. The sum is given by  SN = u1 + u2 + ··· + uN and since un = f n  − f n − 1 , it may be rewritten  SN = [ f 1  − f 0 ] + [ f 2  − f 1 ] + ··· + [ f N  − f N − 1 ].  By cancelling terms we see that   cid:1 Evaluate the sum  Using partial fractions we ﬁnd  SN = f N  − f 0 . N cid:4   1  .  n n + 1   n=1   cid:7   un = −  1  n + 1  − 1  n   cid:8   .  119  Hence un = f n  − f n − 1  with f n  = −1  n + 1 , and so the sum is given by  SN = f N  − f 0  = − 1  + 1 =  N + 1  N  .  cid:2   N + 1   SERIES AND LIMITS  un = f n  − f n − m , m cid:4   f N − k + 1  − m cid:4  N cid:4   k=1  k=1  1  .  n n + 2   n=1   cid:13   un = −  1  2 n + 2   − 1 2n   cid:14   .   cid:7   The diﬀerence method may be easily extended to evaluate sums in which each  term can be expressed in the form   4.6   where m is an integer. By writing out the sum to N terms with each term expressed in this form, and cancelling terms in pairs as before, we ﬁnd  SN =  f 1 − k .   cid:1 Evaluate the sum  Using partial fractions we ﬁnd  Hence un = f n  − f n − 2  with f n  = −1 [2 n + 2 ], and so the sum is given by .  cid:2   SN = f N  + f N − 1  − f 0  − f −1  =  +  1  1  − 1 2  3 4  N + 2  N + 1  In fact the diﬀerence method is quite ﬂexible and may be used to evaluate sums even when each term cannot be expressed as in  4.6 . The method still relies, however, on being able to write un in terms of a single function such that most terms in the sum cancel, leaving only a few terms at the beginning and the end. This is best illustrated by an example.   cid:1 Evaluate the sum  N cid:4   n=1  1  n n + 1  n + 2   .  Using partial fractions we ﬁnd  un =  1  2 n + 2   − 1 n + 1  +  1 2n  .  Hence un = f n  − 2f n − 1  + f n − 2  with f n  = 1 [2 n + 2 ]. If we write out the sum,  expressing each term un in this form, we ﬁnd that most terms cancel and the sum is given by   cid:7   SN = f N  − f N − 1  − f 0  + f −1  =  1 4  +  1 2  1  − 1  N + 2  N + 1  .  cid:2    cid:8    cid:8   120   4.2 SUMMATION OF SERIES  4.2.5 Series involving natural numbers  Series consisting of the natural numbers 1, 2, 3, . . . , or the square or cube of these numbers, occur frequently and deserve a special mention. Let us ﬁrst consider the sum of the ﬁrst N natural numbers,  SN = 1 + 2 + 3 + ··· + N =  This is clearly an arithmetic series with ﬁrst term a = 1 and common diﬀerence d = 1. Therefore, from  4.2 , SN = 1  2 N N + 1 .  Next, we consider the sum of the squares of the ﬁrst N natural numbers:  SN = 12 + 22 + 32 + . . . + N2 =  n2,  which may be evaluated using the diﬀerence method. The nth term in the series  is un = n2, which we need to express in the form f n − f n− 1  for some function  f n . Consider the function  f n  = n n + 1  2n + 1  ⇒ f n − 1  =  n − 1 n 2n − 1 .  For this function f n  − f n − 1  = 6n2, and so we can write  6 [ f n  − f n − 1 ].  un = 1  Therefore, by the diﬀerence method,  SN = 1  6 [ f N  − f 0 ] = 1  6 N N + 1  2N + 1 .  Finally, we calculate the sum of the cubes of the ﬁrst N natural numbers,  SN = 13 + 23 + 33 + ··· + N3 =  n3,  again using the diﬀerence method. Consider the function  f n  = [n n + 1 ]2 ⇒ f n − 1  = [ n − 1 n]2,  for which f n  − f n − 1  = 4n3. Therefore we can write the general nth term of  the series as  N cid:4   n=1  n.  N cid:4   n=1  N cid:4   n=1  and using the diﬀerence method we ﬁnd  SN = 1  4 N2 N + 1 2.  Note that this is the square of the sum of the natural numbers, i.e.  4 [ f n  − f n − 1 ],  un = 1  4 [ f N  − f 0 ] = 1  cid:31 2 N cid:4  N cid:4    cid:30   n3 =  n  .  n=1  n=1  121    cid:1 Sum the series  The nth term in this series is  and therefore we can write  N cid:4   n=1   n + 1  n + 3  =  SERIES AND LIMITS  N cid:4   n=1   n + 1  n + 3 .  un =  n + 1  n + 3  = n2 + 4n + 3,  n=1  N cid:4    n2 + 4n + 3   N cid:4  N cid:4  6 N N + 1  2N + 1  + 4 × 1 6 N 2N2 + 15N + 31 .  cid:2   N cid:4   n2 + 4  n +  n=1  n=1  n=1  3  =  = 1 = 1  4.2.6 Transformation of series  2 N N + 1  + 3N  A complicated series may sometimes be summed by transforming it into a familiar series for which we already know the sum, perhaps a geometric series or the Maclaurin expansion of a simple function  see subsection 4.6.3 . Various techniques are useful, and deciding which one to use in any given case is a matter of experience. We now discuss a few of the more common methods.  The diﬀerentiation or integration of a series is often useful in transforming an apparently intractable series into a more familiar one. If we wish to diﬀerentiate or integrate a series that already depends on some variable then we may do so in a straightforward manner.  cid:1 Sum the series  S  x  =  x4 3 0!   +  x5 4 1!   +  x6 5 2!   + ··· .  Dividing both sides by x we obtain  which is easily diﬀerentiated to give  S  x   =  x3 3 0!   +  x4 4 1!   +  x5 5 2!   + ··· ,  x   cid:13    cid:14   d dx  S  x   x  =  +  +  +  x2 0!  x3 1!  x4 2!  + ··· .  x5 3!  Recalling the Maclaurin expansion of exp x given in subsection 4.6.3, we recognise that the RHS is equal to x2 exp x. Having done so, we can now integrate both sides to obtain  S  x  x =  x2 exp x dx.   cid:21   122   4.2 SUMMATION OF SERIES  Integrating the RHS by parts we ﬁnd  S  x  x = x2 exp x − 2x exp x + 2 exp x + c,  where the value of the constant of integration c can be ﬁxed by the requirement that  S  x  x = 0 at x = 0. Thus we ﬁnd that c = −2 and that the sum is given by  S  x  = x3 exp x − 2x2 exp x + 2x exp x − 2x.  cid:2   Often, however, we require the sum of a series that does not depend on a variable. In this case, in order that we may diﬀerentiate or integrate the series, we deﬁne a function of some variable x such that the value of this function is equal to the sum of the series for some particular value of x  usually at x = 1 .  cid:1 Sum the series  S = 1 +  +  +  2 2  3 22  + ··· .  4 23  which we recognise as an inﬁnite geometric series with ﬁrst term a = x and common ratio  r = x. Therefore, from  4.4 , we ﬁnd that the sum of this series is x  1− x . In other words  Let us begin by deﬁning the function  f x  = 1 + 2x + 3x2 + 4x3 + ··· ,  cid:21  so that the sum S = f 1 2 . Integrating this function we obtain  f x  dx = x + x2 + x3 + ··· ,  cid:21   f x  dx =   cid:9   x  1 − x  ,   cid:10   so that f x  is given by   1 − x 2 . The sum of the original series is therefore S = f 1 2  = 4.  cid:2   1 − x  f x  =  =  d dx  x  1  Aside from diﬀerentiation and integration, an appropriate substitution can sometimes transform a series into a more familiar form. In particular, series with terms that contain trigonometric functions can often be summed by the use of complex exponentials.  cid:1 Sum the series  S  θ  = 1 + cos θ +  cos 2θ  cos 3θ  +  2!  3!  + ··· .  Replacing the cosine terms with a complex exponential, we obtain + ···  1 + exp iθ +  S  θ  = Re  exp 2iθ  exp 3iθ  +  = Re  1 + exp iθ +  2!  2!   exp iθ 2  +  3!  exp iθ 3  3!  + ···  .   cid:15    cid:15    cid:12   cid:12   123   SERIES AND LIMITS  Again using the Maclaurin expansion of exp x given in subsection 4.6.3, we notice that  S  θ  = Re [exp exp iθ ] = Re [exp cos θ + i sin θ ]  = Re {[exp cos θ ][exp i sin θ ]} = [exp cos θ ]Re [exp i sin θ ] = [exp cos θ ][cos sin θ ].  cid:2   4.3 Convergence of inﬁnite series  Although the sums of some commonly occurring inﬁnite series may be found, the sum of a general inﬁnite series is usually diﬃcult to calculate. Nevertheless, it is often useful to know whether the partial sum of such a series converges to a limit, even if the limit cannot be found explicitly. As mentioned at the end of section 4.1, if we allow N to tend to inﬁnity, the partial sum  N cid:4   n=1  SN =  un  of a series may tend to a deﬁnite limit  i.e. the sum S of the series , or increase or decrease without limit, or oscillate ﬁnitely or inﬁnitely.  To investigate the convergence of any given series, it is useful to have available a number of tests and theorems of general applicability. We discuss them below; some we will merely state, since once they have been stated they become almost self-evident, but are no less useful for that.  4.3.1 Absolute and conditional convergence   cid:11    cid:11   If the series   cid:11 un converges then  Let us ﬁrst consider some general points concerning the convergence, or otherwise, un can have complex terms, of an inﬁnite series. In general an inﬁnite series and in the special case of a real series the terms can be positive or negative. From any such series, however, we can always construct another series each term is simply the modulus of the corresponding term in the original series. Then each term in the new series will be a positive real number.   cid:11 un in which  cid:11   cid:11   cid:11 un diverges whilst  un is said to be absolutely convergent, i.e. the series formed by the absolute values is convergent. For an absolutely convergent series, the terms may be reordered without aﬀecting un converges the convergence of the series. However, if un is said to be conditionally convergent. For a conditionally convergent then series, rearranging the order of the terms can aﬀect the behaviour of the sum and, hence, whether the series converges or diverges. In fact, a theorem due to Riemann shows that, by a suitable rearrangement, a conditionally convergent series may be made to converge to any arbitrary limit, or to diverge, or to oscillate un consists only of positive ﬁnitely or inﬁnitely! Of course, if the original series real terms and converges then automatically it is absolutely convergent.  un also converges, and   cid:11    cid:11   124   4.3 CONVERGENCE OF INFINITE SERIES   cid:11   4.3.2 Convergence of a series containing only real positive terms   cid:11 un that consists only of real  As discussed above, in order to test for the absolute convergence of a series  un, we ﬁrst construct the corresponding series  positive terms. Therefore in this subsection we will restrict our attention to series of this type.  We discuss below some tests that may be used to investigate the convergence of such a series. Before doing so, however, we note the following crucial consideration. In all the tests for, or discussions of, the convergence of a series, it is not what happens in the ﬁrst ten, or the ﬁrst thousand, or the ﬁrst million terms  or any other ﬁnite number of terms  that matters, but what happens ultimately.  Preliminary test  n→∞ un = 0. lim  Comparison test  A necessary but not suﬃcient condition for a series of real positive terms un to be convergent is that the term un tends to zero as n tends to inﬁnity, i.e. we require  If this condition is not satisﬁed then the series must diverge. Even if it is satisﬁed, however, the series may still diverge, and further testing is required.   cid:11   cid:11   cid:11   cid:11   un and  The comparison test is the most basic test for convergence. Let us consider two vn and suppose that we know the latter to be convergent  by series some earlier analysis, for example . Then, if each term un in the ﬁrst series is less than or equal to the corresponding term vn in the second series, for all n greater than some ﬁxed number N that will vary from series to series, then the original series  un is also convergent. In other words, if  vn is convergent and   cid:11   un ≤ vn  for n > N,  then  un converges.  However, if  vn diverges and un ≥ vn for all n greater than some ﬁxed number  then  un diverges.   cid:1 Determine whether the following series converges:  ∞ cid:4   n=1  1  n! + 1  =  +  +  +  1 3  1 7  1 2  + ··· .  1 25  Let us compare this series with the series  1 n!  =  +  +  +  1 1!  1 2!  1 3!  1 0!  + ··· = 2 +  1 2!  +  1 3!  + ··· ,  125   cid:11    cid:11   ∞ cid:4   n=0   cid:11    4.7    4.8    SERIES AND LIMITS  which is merely the series obtained by setting x = 1 in the Maclaurin expansion of exp x  see subsection 4.6.3 , i.e.  exp 1  = e = 1 +  +  +  1 1!  1 2!  + ··· .  1 3!  Clearly this second series is convergent, since it consists of only positive terms and has a ﬁnite sum. Thus, since each term un in the series  4.7  is less than the corresponding term 1 n! in  4.8 , we conclude from the comparison test that  4.7  is also convergent.  cid:2   The ratio test determines whether a series converges by comparing the relative magnitude of successive terms. If we consider a series  un and set   4.9    cid:11   D’Alembert’s ratio test   cid:7    cid:8   ρ = lim n→∞  un+1 un  ,  then if ρ   1 the series is divergent; if ρ = 1 then the behaviour of the series is undetermined by this test.  To prove this we observe that if the limit  4.9  is less than unity, i.e. ρ < 1 then  we can ﬁnd a value r in the range ρ < r < 1 and a value N such that  un+1 un  < r,  for all n > N. Now the terms un of the series that follow uN are  uN+1,  uN+2,  uN+3,  . . . ,  and each of these is less than the corresponding term of  ruN,  r2uN,  r3uN,  . . . .   4.10    cid:11   However, the terms of  4.10  are those of a geometric series with a common ratio r that is less than unity. This geometric series consequently converges and therefore, by the comparison test discussed above, so must the original series un. An analogous argument may be used to prove the divergent case when  ρ > 1.  cid:1 Determine whether the following series converges:  1 n!  =  +  +  +  1 0!  1 1!  1 2!  1 3!  + ··· = 2 +  1 2!  +  1 3!  + ··· .  ∞ cid:4   n=0  As mentioned in the previous example, this series may be obtained by setting x = 1 in the Maclaurin expansion of exp x, and hence we know already that it converges and has the sum exp 1  = e. Nevertheless, we may use the ratio test to conﬁrm that it converges.  Using  4.9 , we have   cid:13    cid:14   n!   cid:7    cid:8   1  n + 1  = 0  = lim n→∞ and since ρ < 1, the series converges, as expected.  cid:2   ρ = lim n→∞   n + 1 !   4.11   126   4.3 CONVERGENCE OF INFINITE SERIES  Ratio comparison test   cid:11    cid:11   As its name suggests, the ratio comparison test is a combination of the ratio and comparison tests. Let us consider the two series vn and assume that we know the latter to be convergent. It may be shown that if  un and  for all n greater than some ﬁxed value N then  un is also convergent.  Similarly, if  for all suﬃciently large n, and  vn diverges then  un also diverges.   cid:1 Determine whether the following series converges:  un+1 un  ≤ vn+1 vn   cid:11   un+1 un  ≥ vn+1 vn   cid:11    cid:11   ∞ cid:4   n=1  1   n! 2  = 1 +  +  1 22  1 62   cid:13    cid:14   2  + ··· .  cid:7   1  n + 1   cid:8   2  ,  R = lim n→∞  n!   n + 1 !  = lim n→∞  In this case the ratio of successive terms, as n tends to inﬁnity, is given by  which is less than the ratio seen in  4.11 . Hence, by the ratio comparison test, the series converges.  It is clear that this series could also be found to be convergent using the ratio test.   cid:2   The quotient test may also be considered as a combination of the ratio and comparison tests. Let us again consider the two series vn, and deﬁne ρ as the limit  un and  Quotient test   cid:11    cid:11   ρ = lim n→∞  un vn  .   4.12    cid:11   Then, it can be shown that:   i  if ρ  cid:3 = 0 but is ﬁnite then  diverge;   cid:11   cid:11   iii  if ρ = ∞ and   ii  if ρ = 0 and  vn converges then  un converges;  vn diverges then  un diverges.  un and  vn either both converge or both   cid:7    cid:8    cid:11    cid:11   cid:11   127    cid:11 ∞   cid:13   SERIES AND LIMITS  ∞ cid:4   n=1  4n2 − n − 3  n3 + 2n  .   cid:1 Given that the series  n=1 1 n diverges, determine whether the following series converges:   4.13   If we set un =  4n2 − n − 3   n3 + 2n  and vn = 1 n then the limit  4.12  becomes  ρ = lim n→∞   4n2 − n − 3   n3 + 2n    cid:11   1 n  = lim n→∞  4n3 − n2 − 3n  cid:11   n3 + 2n  = 4.  Since ρ is ﬁnite but non-zero and  vn diverges, from  i  above  un must also diverge.  cid:2    cid:14    cid:13    cid:14    cid:11   Integral test  The integral test is an extremely powerful means of investigating the convergence un. Suppose that there exists a function f x  which monotonically of a series decreases for x greater than some ﬁxed value x0 and for which f n  = un, i.e. the value of the function at integer values of x is equal to the corresponding term in the series under investigation. Then it can be shown that, if the limit of the integral   cid:21    cid:11   N  f x  dx  lim N→∞  un is convergent. Otherwise the series diverges. Note that the exists, the series integral deﬁned here has no lower limit; the test is sometimes stated with a lower limit, equal to unity, for the integral, but this can lead to unnecessary diﬃculties.  cid:1 Determine whether the following series converges:  1   n − 3 2 2  = 4 + 4 +  +  4 9  + ··· .  4 25  Let us consider the function f x  =  x − 3 2   −2. Clearly f n  = un and f x  monotonically  decreases for x > 3 2. Applying the integral test, we consider   cid:7  −1   cid:8   lim N→∞  N  1   x − 3 2 2 dx = lim N→∞  N − 3 2  = 0.  Since the limit exists the series converges. Note, however, that if we had included a lower limit, equal to unity, in the integral then we would have run into problems, since the integrand diverges at x = 3 2.  cid:2   The integral test is also useful for examining the convergence of the Riemann  zeta series. This is a special series that occurs regularly and is of the form  ∞ cid:4   n=1   cid:21   It converges for p > 1 and diverges if p ≤ 1. These convergence criteria may be  derived as follows.  ∞ cid:4   n=1  1 np .  128   Using the integral test, we consider  4.3 CONVERGENCE OF INFINITE SERIES   cid:21   N 1  lim N→∞  xp dx = lim N→∞   cid:7    cid:8   ,  N1−p 1 − p  and it is obvious that the limit tends to zero for p > 1 and to ∞ for p ≤ 1.  Cauchy’s root test may be useful in testing for convergence, especially if the nth terms of the series contains an nth power. If we deﬁne the limit  then it may be proved that the series series diverges. Its behaviour is undetermined if ρ = 1.  cid:1 Determine whether the following series converges:  un converges if ρ   1 then the  Cauchy’s root test  ρ = lim  n→∞  un 1 n ,   cid:11    cid:7   ∞ cid:4    cid:8   n  1 n  n=1  = 1 +  +  + ··· .  1 27   cid:7    cid:8   1 4  1 n  ρ = lim n→∞  = 0,  Grouping terms  Using Cauchy’s root test, we ﬁnd  and hence the series converges.  cid:2   We now consider the Riemann zeta series, mentioned above, with an alternative proof of its convergence that uses the method of grouping terms. In general there are better ways of determining convergence, but the grouping method may be used if it is not immediately obvious how to approach a problem by a better method.  First consider the case where p > 1, and group the terms in the series as   cid:7    cid:8    cid:7    cid:8   follows:  SN =  +  1 1p  1 2p  +  1 3p  +  1 4p  + ··· +  1 7p  + ··· .  Now we can see that each bracket of this series is less than each term of the geometric series  ; since p > 1, it follows that This geometric series has common ratio r = r < 1 and that the geometric series converges. Then the comparison test shows that the Riemann zeta series also converges for p > 1.  SN =  +  +  1 1p  2 2p  4 4p   cid:5    cid:6  + ··· . p−1  1 2  129   SERIES AND LIMITS  The divergence of the Riemann zeta series for p ≤ 1 can be seen by ﬁrst  considering the case p = 1. The series is  1 4  + ··· ,  cid:7    cid:8   SN = 1 +  +  +  1 2  1 3   cid:7    cid:7    cid:8   1 2  N cid:4   n=1  which does not converge, as may be seen by bracketing the terms of the series in groups in the following way:  SN =  un = 1 +  +  1 3  +  1 4  +  +  +  +  1 5  1 6  1 7  1 8  + ··· .  The sum of the terms in each bracket is ≥ 1 2 and, since as many such groupings can be made as we wish, it is clear that SN increases indeﬁnitely as N is increased. Now returning to the case of the Riemann zeta series for p < 1, we note that each term in the series is greater than the corresponding one in the series for which p = 1. In other words 1 np > 1 n for n > 1, p < 1. The comparison test then shows us that the Riemann zeta series will diverge for all p ≤ 1.   cid:8   ∞ cid:4   n=1  4.3.3 Alternating series test   cid:11 un converges, and so whether   cid:11   The tests discussed in the last subsection have been concerned with determining whether the series of real positive terms un is absolutely convergent. Nevertheless, it is sometimes useful to consider whether a series is merely convergent rather than absolutely convergent. This is especially true for series containing an inﬁnite number of both positive and negative terms. In particular, we will consider the convergence of series in which the positive and negative terms alternate, i.e. an alternating series.  An alternating series can be written as   −1 n+1un = u1 − u2 + u3 − u4 + u5 − ··· ,  with all un ≥ 0. Such a series can be shown to converge provided  i  un → 0 as n → ∞ and  ii  un   N for some ﬁnite N. If these conditions are  not met then the series oscillates.  To prove this, suppose for deﬁniteness that N is odd and consider the series  starting at uN. The sum of its ﬁrst 2m terms is  S2m =  uN − uN+1  +  uN+2 − uN+3  + ··· +  uN+2m−2 − uN+2m−1 .  By condition  ii  above, all the parentheses are positive, and so S2m increases as m increases. We can also write, however,  S2m = uN −  uN+1 − uN+2  − ··· −  uN+2m−3 − uN+2m−2  − uN+2m−1,  and since each parenthesis is positive, we must have S2m < uN . Thus, since S2m  130   4.4 OPERATIONS WITH SERIES  is always less than uN for all m and un → 0 as n → ∞, the alternating series  converges. It is clear that an analogous proof can be constructed in the case where N is even.  cid:1 Determine whether the following series converges: = 1 − 1   −1 n+1 1  ∞ cid:4   − ··· .  +  n=1  n  2  1 3  This alternating series clearly satisﬁes conditions  i  and  ii  above and hence converges. However, as shown above by the method of grouping terms, the corresponding series with all positive terms is divergent.  cid:2   4.4 Operations with series   cid:11   cid:11   cid:11    cid:11   Simple operations with series are fairly intuitive, and we discuss them here only for completeness. The following points apply to both ﬁnite and inﬁnite series unless otherwise stated.   cid:11   cid:11    cid:11   cid:11    cid:11   cid:11    i  If  ii  If  iii  If  un = S then un = S and un = S then a +  kun = kS where k is any constant. vn = T then  un + vn  = S + T .  un = a + S . A simple extension of this trivial result shows that the removal or insertion of a ﬁnite number of terms anywhere in a series does not aﬀect its convergence.   iv  If the inﬁnite series  un and  vn are both absolutely convergent then   cid:11   the series  wn, where  wn = u1vn + u2vn−1 + ··· + unv1,  is also absolutely convergent. The series of the two original series. Furthermore, if and  vn converges to the sum T then  wn is called the Cauchy product un converges to the sum S  wn converges to the sum S T .   v  It is not true in general that term-by-term diﬀerentiation or integration of  a series will result in a new series with the same convergence properties.   cid:11   cid:11    cid:11   4.5 Power series  A power series has the form  P  x  = a0 + a1x + a2x2 + a3x3 + ··· ,  where a0, a1, a2, a3 etc. are constants. Such series regularly occur in physics and  engineering and are useful because, for x < 1, the later terms in the series may  become very small and be discarded. For example the series  P  x  = 1 + x + x2 + x3 + ··· ,  131   SERIES AND LIMITS  although in principle inﬁnitely long, in practice may be simpliﬁed if x happens to have a value small compared with unity. To see this note that P  x  for x = 0.1 has the following values: 1, if just one term is taken into account; 1.1, for two terms; 1.11, for three terms; 1.111, for four terms, etc. If the quantity that it represents can only be measured with an accuracy of two decimal places, then all but the ﬁrst three terms may be ignored, i.e. when x = 0.1 or less  P  x  = 1 + x + x2 + O x3  ≈ 1 + x + x2.  This sort of approximation is often used to simplify equations into manageable forms. It may seem imprecise at ﬁrst but is perfectly acceptable insofar as it matches the experimental accuracy that can be achieved.  The symbols O and ≈ used above need some further explanation. They are  used to compare the behaviour of two functions when a variable upon which both functions depend tends to a particular limit, usually zero or inﬁnity  and obvious from the context . For two functions f x  and g x , with g positive, the formal deﬁnitions of the above symbols are as follows:   i  If there exists a constant k such that f ≤ kg as the limit is approached  ii  If as the limit of x is approached f g tends to a limit l, where l  cid:3 = 0, then f ≈ lg. The statement f ≈ g means that the ratio of the two sides tends  then f = O g .  to unity.  4.5.1 Convergence of power series  The convergence or otherwise of power series is a crucial consideration in practical terms. For example, if we are to use a power series as an approximation, it is clearly important that it tends to the precise answer as more and more terms of the approximation are taken. Consider the general power series  Using d’Alembert’s ratio test  see subsection 4.3.2 , we see that P  x  converges absolutely if  P  x  = a0 + a1x + a2x2 + ··· .   cid:20  cid:20  cid:20  cid:20  an+1  an   cid:20  cid:20  cid:20  cid:20  = x lim  n→∞   cid:20  cid:20  cid:20  cid:20  an+1  an   cid:20  cid:20  cid:20  cid:20  < 1.  x  ρ = lim n→∞  Thus the convergence of P  x  depends upon the value of x, i.e. there is, in general, a range of values of x for which P  x  converges, an interval of convergence. Note that at the limits of this range ρ = 1, and so the series may converge or diverge. The convergence of the series at the end-points may be determined by substituting these values of x into the power series P  x  and testing the resulting series using any applicable method  discussed in section 4.3 .  132    cid:1 Determine the range of values of x for which the following power series converges:  4.5 POWER SERIES  P  x  = 1 + 2x + 4x2 + 8x3 + ··· .   cid:20  cid:20  cid:20  cid:20  2n+1  2n   cid:20  cid:20  cid:20  cid:20  = 2x,  x  ρ = lim n→∞  P  1 2  = 1 + 1 + 1 + ··· , P  −1 2  = 1 − 1 + 1 − ··· .  By using the interval-of-convergence method discussed above,  and hence the power series will converge for x < 1 2. Examining the end-points of the  interval separately, we ﬁnd  Obviously P  1 2  diverges, while P  −1 2  oscillates. Therefore P  x  is not convergent at either end-point of the region but is convergent for −1 < x < 1.  cid:2   The convergence of power series may be extended to the case where the  parameter z is complex. For the power series  we ﬁnd that P  z  converges if  P  z  = a0 + a1z + a2z2 + ··· ,   cid:20  cid:20  cid:20  cid:20  an+1  an   cid:20  cid:20  cid:20  cid:20  = z lim  n→∞   cid:20  cid:20  cid:20  cid:20  an+1  an  z   cid:20  cid:20  cid:20  cid:20  < 1.  ρ = lim n→∞  We therefore have a range in z for which P  z  converges, i.e. P  z  converges  for values of z lying within a circle in the Argand diagram  in this case centred on the origin of the Argand diagram . The radius of the circle is called the radius of convergence: if z lies inside the circle, the series will converge whereas if z lies outside the circle, the series will diverge; if, though, z lies on the circle then the convergence must be tested using another method. Clearly the radius of  convergence R is given by 1 R = limn→∞ an+1 an.  cid:1 Determine the range of values of z for which the following complex power series converges:  P  z  = 1 − z  z2 4  − z3 8  +  2  + ··· .  We ﬁnd that ρ = z 2, which shows that P  z  converges for z < 2. Therefore the circle  of convergence in the Argand diagram is centred on the origin and has a radius R = 2. On this circle we must test the convergence by substituting the value of z into P  z  and considering the resulting series. On the circle of convergence we can write z = 2 exp iθ. Substituting this into P  z , we obtain  which is a complex inﬁnite geometric series with ﬁrst term a = 1 and common ratio  P  z  = 1 − 2 exp iθ  − ··· = 1 − exp iθ + [exp iθ]2 − ··· ,  4 exp 2iθ  +  2  4  133   r = − exp iθ. Therefore, on the the circle of convergence we have  SERIES AND LIMITS  P  z  =  1  1 + exp iθ  .  Unless θ = π this is a ﬁnite complex number, and so P  z  converges at all points on the  circle z = 2 except at θ = π  i.e. z = −2 , where it diverges. Note that P  z  is just the −1, for which it is obvious that z = −2 is a singular point.  binomial expansion of  1 + z 2  In general, for power series expansions of complex functions about a given point in the complex plane, the circle of convergence extends as far as the nearest singular point. This is discussed further in chapter 24.  cid:2   Note that the centre of the circle of convergence does not necessarily lie at the  origin. For example, applying the ratio test to the complex power series  P  z  = 1 +  +  z − 1  2   z − 1 2   z − 1 3  +  4  8  + ··· ,  we ﬁnd that for it to converge we require  z − 1  2 < 1. Thus the series converges  for z lying within a circle of radius 2 centred on the point  1, 0  in the Argand diagram.  4.5.2 Operations with power series  The following rules are useful when manipulating power series; they apply to power series in a real or complex variable.   i  If two power series P  x  and Q x  have regions of convergence that overlap to some extent then the series produced by taking the sum, the diﬀerence or the product of P  x  and Q x  converges in the common region.   ii  If two power series P  x  and Q x  converge for all values of x then one series may be substituted into the other to give a third series, which also converges for all values of x. For example, consider the power series expansions of sin x and ex given below in subsection 4.6.3, sin x = x − x3  +  3!  x5 5!  − x7 7!  x2 2!  x3 3!  + ··· x4 4!  + ··· ,  ex = 1 + x +  +  +  both of which converge for all values of x. Substituting the series for sin x into that for ex we obtain  esin x = 1 + x +  x2 2!  − 3x4 4!  − 8x5 5!  + ··· ,  which also converges for all values of x.  If, however, either of the power series P  x  and Q x  has only a limited region of convergence, or if they both do so, then further care must be taken when substituting one series into the other. For example, suppose Q x  converges for all x, but P  x  only converges for x within a ﬁnite range. We may substitute  134   4.5 POWER SERIES  Q x  into P  x  to obtain P  Q x  , but we must be careful since the value of Q x  may lie outside the region of convergence for P  x , with the consequence that the resulting series P  Q x   does not converge.   iii  If a power series P  x  converges for a particular range of x then the series obtained by diﬀerentiating every term and the series obtained by integrating every term also converge in this range.  This is easily seen for the power series  P  x  = a0 + a1x + a2x2 + ··· ,  which converges if x < limn→∞ an an+1 ≡ k. The series obtained by diﬀerenti-  ating P  x  with respect to x is given by  Similarly the series obtained by integrating P  x  term by term,  and converges if  converges if  = a1 + 2a2x + 3a3x2 + ···  dP dx  x < lim n→∞  nan   n + 1 an+1   cid:20  cid:20  cid:20  cid:20  = k.   cid:20  cid:20  cid:20  cid:20    cid:21   P  x  dx = a0x +  + ··· ,  a1x2  +  2   cid:20  cid:20  cid:20  cid:20   n + 2 an   n + 1 an+1  a2x3  3   cid:20  cid:20  cid:20  cid:20  = k.  x < lim n→∞  So, series resulting from diﬀerentiation or integration have the same interval of convergence as the original series. However, even if the original series converges at either end-point of the interval, it is not necessarily the case that the new series will do so. The new series must be tested separately at the end-points in order to determine whether it converges there. Note that although power series may be integrated or diﬀerentiated without altering their interval of convergence, this is not true for series in general.  It is also worth noting that diﬀerentiating or integrating a power series term by term within its interval of convergence is equivalent to diﬀerentiating or integrating the function it represents. For example, consider the power series expansion of sin x,  sin x = x − x3  x5 5!  − x7 7!  +  3!  + ··· ,   4.14   which converges for all values of x. If we diﬀerentiate term by term, the series becomes  which is the series expansion of cos x, as we expect.  1 − x2  2!  x4 4!  − x6 6!  +  + ··· ,  135   SERIES AND LIMITS  4.6 Taylor series  Taylor’s theorem provides a way of expressing a function as a power series in x, known as a Taylor series, but it can be applied only to those functions that are continuous and diﬀerentiable within the x-range of interest.  4.6.1 Taylor’s theorem  Suppose that we have a function f x  that we wish to express as a power series  in x − a about the point x = a. We shall assume that, in a given x-range, f x  is a continuous, single-valued function of x having continuous derivatives with  x  and so on, up to and including f n−1  x . We respect to x, denoted by f shall also assume that f n  x  exists in this range.   x , f   cid:7  cid:7    cid:7   From the equation following  2.31  we may write   cid:21   a  a+h   cid:7    x  dx = f a + h  − f a ,  f  where a, a + h are neighbouring values of x. Rearranging this equation, we may express the value of the function at x = a + h in terms of its value at a by  f a + h  = f a  +  f   x  dx.  a+h   cid:7    cid:21   a   cid:7   A ﬁrst approximation for f a + h  may be obtained by substituting f  x  in  4.15 , to obtain  f  f a + h  ≈ f a  + hf   cid:7    a .  This approximation is shown graphically in ﬁgure 4.1. We may write this ﬁrst approximation in terms of x and a as   4.15    cid:7    a  for  and, in a similar way,  f x  ≈ f a  +  x − a f   cid:7    a ,   a  +  x − a f  cid:7  cid:7   cid:7   a ,  a  +  x − a f  cid:7  cid:7   cid:7  cid:7  cid:7    a ,   cid:7  f  cid:7  cid:7   f   x  ≈ f  x  ≈ f  cid:21   cid:7   f a + h  ≈ f a  +  a+h  [ f  a  ≈ f a  + hf   cid:7    a  +   cid:7  cid:7    cid:7    a  +  x − a f h2 2   a .   cid:7  cid:7   f   a ] dx  and so on. Substituting for f   x  in  4.15 , we obtain the second approximation:  We may repeat this procedure as often as we like  so long as the derivatives of f x  exist  to obtain higher-order approximations to f a + h ; we ﬁnd the  136   f x   f a   4.6 TAYLOR SERIES  Q  R   cid:7   hf   a   θ  h  a + h  x  P  a  Figure 4.1 The ﬁrst-order Taylor series approximation to a function f x .  a . Thus the value of the The slope of the function at P , i.e. tan θ, equals f function at Q, f a + h , is approximated by the ordinate of R, f a  + hf   a .   cid:7    cid:7    n − 1 th-order approximation f a + h  ≈ f a  + hf   cid:7   §  to be   a  +  h2 2!   cid:7  cid:7    a  + ··· +  f  hn−1  n − 1 !  f n−1  a .   4.16   As might have been anticipated, the error associated with approximating f a+h   by this  n − 1 th-order power series is of the order of the next term in the series.  This error or remainder can be shown to be given by  Rn h  =  f n  ξ ,  hn n!  for some ξ that lies in the range [a, a + h]. Taylor’s theorem then states that we may write the equality  f a + h  = f a  + hf   a  +   cid:7    cid:7  cid:7    a  + ··· +  f  h2 2!  h n−1   n − 1 !  f n−1  a  + Rn h .   4.17   The theorem may also be written in a form suitable for ﬁnding f x  given the value of the function and its relevant derivatives at x = a, by substituting  §  The order of the approximation is simply the highest power of h in the series. Note, though, that  the  n − 1 th-order approximation contains n terms.  137   SERIES AND LIMITS  x = a + h in the above expression. It then reads  f x  = f a  +  x − a f   cid:7    a  +   x − a 2  2!   cid:7  cid:7    a  + ··· +  f   x − a n−1  n − 1 !  f n−1  a  + Rn x ,   4.18   where the remainder now takes the form  Rn x  =  f n  ξ ,   x − a n  n!  and ξ lies in the range [a, x]. Each of the formulae  4.17 ,  4.18  gives us the Taylor expansion of the function about the point x = a. A special case occurs when a = 0. Such Taylor expansions, about x = 0, are called Maclaurin series.  Taylor’s theorem is also valid without signiﬁcant modiﬁcation for functions of a complex variable  see chapter 24 . The extension of Taylor’s theorem to functions of more than one variable is given in chapter 5.  For a function to be expressible as an inﬁnite power series we require it to be inﬁnitely diﬀerentiable and the remainder term Rn to tend to zero as n tends to inﬁnity, i.e. limn→∞ Rn = 0. In this case the inﬁnite power series will represent the function within the interval of convergence of the series.  cid:1 Expand f x  = sin x as a Maclaurin series, i.e. about x = 0.  We must ﬁrst verify that sin x may indeed be represented by an inﬁnite power series. It is easily shown that the nth derivative of f x  is given by  Therefore the remainder after expanding f x  as an  n − 1 th-order polynomial about  x = 0 is given by   cid:9   cid:9   nπ 2   cid:10   cid:10   .  nπ 2  ,  f n  x  = sin  x +  Rn x  =  sin  ξ +  xn n!  where ξ lies in the range [0, x]. Since the modulus of the sine term is always less than or  equal to unity, we can write Rn x  < xn n!. For any particular value of x, say x = c, Rn c  → 0 as n → ∞. Hence limn→∞ Rn x  = 0, and so sin x can be represented by an  inﬁnite Maclaurin series.  Evaluating the function and its derivatives at x = 0 we obtain  and so on. Therefore, the Maclaurin series expansion of sin x is given by  Note that, as expected, since sin x is an odd function, its power series expansion contains only odd powers of x.  cid:2   f 0  = sin 0 = 0,  cid:7  f  cid:7  cid:7  f  cid:7  cid:7  cid:7    0  = sin π 2  = 1,  0  = sin π = 0,   0  = sin 3π 2  = −1,  f  sin x = x − x3  +  3!  x5 5!  − ··· .  138   4.6 TAYLOR SERIES  We may follow a similar procedure to obtain a Taylor series about an arbitrary  point x = a.  cid:1 Expand f x  = cos x as a Taylor series about x = π 3.  As in the above example, it is easily shown that the nth derivative of f x  is given by  Therefore the remainder after expanding f x  as an  n − 1 th-order polynomial about  f n  x  = cos  x +  x = π 3 is given by   cid:9    cid:10   nπ 2  .   cid:9    cid:10   Rn x  =   x − π 3 n  n!  cos  ξ +  ,  nπ 2  where ξ lies in the range [π 3, x]. The modulus of the cosine term is always less than or  equal to unity, and so Rn x  <  x− π 3 n n!. As in the previous example, limn→∞ Rn x  =  0 for any particular value of x, and so cos x can be represented by an inﬁnite Taylor series about x = π 3.  Evaluating the function and its derivatives at x = π 3 we obtain  and so on. Thus the Taylor series expansion of cos x about x = π 3 is given by  f  f π 3  = cos π 3  = 1 2,  cid:7  f  cid:7  cid:7    π 3  = cos 5π 6  = −√  π 3  = cos 4π 3  = −1 2,  cid:5  √   cid:5    cid:6  − 1  x − π 3  x − π 3  −  3 2,   cid:6   2  2  2!  cos x =  1 2  3 2  + ··· .  cid:2   4.6.2 Approximation errors in Taylor series  In the previous subsection we saw how to represent a function f x  by an inﬁnite power series, which is exactly equal to f x  for all x within the interval of convergence of the series. However, in physical problems we usually do not want to have to sum an inﬁnite number of terms, but prefer to use only a ﬁnite number of terms in the Taylor series to approximate the function in some given range of x. In this case it is desirable to know what is the maximum possible error associated with the approximation.  As given in  4.18 , a function f x  can be represented by a ﬁnite  n− 1 th-order  power series together with a remainder term such that  f x  = f a  +  x − a f   cid:7    a  +   x − a 2  2!   cid:7  cid:7    a  + ··· +  f   x − a n−1  n − 1 !  f n−1  a  + Rn x ,  where  Rn x  =  f n  ξ    x − a n  n!  and ξ lies in the range [a, x]. Rn x  is the remainder term, and represents the error in approximating f x  by the above  n − 1 th-order power series. Since the exact  139   SERIES AND LIMITS  value of ξ that satisﬁes the expression for Rn x  is not known, an upper limit on the error may be found by diﬀerentiating Rn x  with respect to ξ and equating the derivative to zero in the usual way for ﬁnding maxima.  cid:1 Expand f x  = cos x as a Taylor series about x = 0 and ﬁnd the error associated with using the approximation to evaluate cos 0.5  if only the ﬁrst two non-vanishing terms are taken.  Note that the Taylor expansions of trigonometric functions are only valid for angles measured in radians.   Evaluating the function and its derivatives at x = 0, we ﬁnd  f 0  = cos 0 = 1,  cid:7  f  cid:7  cid:7  f  cid:7  cid:7  cid:7  So, for small x, we ﬁnd from  4.18    0  = − sin 0 = 0,  0  = − cos 0 = −1,   0  = sin 0 = 0.  f  cos x ≈ 1 − x2  .  2  Note that since cos x is an even function, its power series expansion contains only even powers of x. Therefore, in order to estimate the error in this approximation, we must consider the term in x4, which is the next in the series. The required derivative is f 4  x  and this is  by chance  equal to cos x. Thus, adding in the remainder term R4 x , we ﬁnd  cos x = 1 − x2  2  +  cos ξ,  x4 4!  where ξ lies in the range [0, x]. Thus, the maximum possible error is x4 4!, since cos ξ cannot exceed unity. If x = 0.5, taking just the ﬁrst two terms yields cos 0.5  ≈ 0.875 with a predicted error of less than 0.002 60. In fact cos 0.5  = 0.877 58 to 5 decimal places. Thus, to this accuracy, the true error is 0.002 58, an error of about 0.3%.  cid:2   4.6.3 Standard Maclaurin series  It is often useful to have a readily available table of Maclaurin series for standard elementary functions, and therefore these are listed below.  3!  sin x = x − x3 cos x = 1 − x2 −1 x = x − x3  2!  +  +  +  3  tan  ex = 1 + x + ln 1 + x  = x − x2  1 + x n = 1 + nx + n n − 1   2  x5 5! x4 4! x5 5  − x7 7! − x6 6! − x7 7  + ··· + ··· + ··· x3 x4 + + 3! 4! − x4 + ··· 4 + n n − 1  n − 2   + ···  x3 3  x2 2!  +  x2 2!  140  for −∞ < x < ∞, for −∞ < x < ∞, for −1 < x < 1,  for −∞ < x < ∞,  for −1 < x ≤ 1, x3 3!  + ···  for −∞ < x < ∞.   4.7 EVALUATION OF LIMITS  These can all be derived by straightforward application of Taylor’s theorem to the expansion of a function about x = 0.  4.7 Evaluation of limits  The idea of the limit of a function f x  as x approaches a value a is fairly intuitive, though a strict deﬁnition exists and is stated below. In many cases the limit of the function as x approaches a will be simply the value f a , but sometimes this is not so. Firstly, the function may be undeﬁned at x = a, as, for example, when  f x  =  sin x  ,  x  which takes the value 0 0 at x = 0. However, the limit as x approaches zero does exist and can be evaluated as unity using l’H ˆopital’s rule below. Another possibility is that even if f x  is deﬁned at x = a its value may not be equal to the limiting value limx→a f x . This can occur for a discontinuous function at a point of discontinuity. The strict deﬁnition of a limit is that if limx→a f x  = l then for any number  cid:4  however small, it must be possible to ﬁnd a number η such that f x −l <  cid:4  whenever x−a < η. In other words, as x becomes arbitrarily close to  a, f x  becomes arbitrarily close to its limit, l. To remove any ambiguity, it should be stated that, in general, the number η will depend on both  cid:4  and the form of f x . The following observations are often useful in ﬁnding the limit of a function.   i  A limit may be ±∞. For example as x → 0, 1 x2 → ∞.   ii  A limit may be approached from below or above and the value may be diﬀerent in each case. For example consider the function f x  = tan x. As x tends  to π 2 from below f x  → ∞, but if the limit is approached from above then f x  → −∞. Another way of writing this is  − tan x = ∞,  lim x→ π  2  tan x = −∞.  lim x→ π + 2   iii  It may ease the evaluation of limits if the function under consideration is split into a sum, product or quotient. Provided that in each case a limit exists, the rules for evaluating such limits are as follows. {f x  + g x } = lim x→a {f x g x } = lim f x  lim x→a x→a  f x  + lim x→a g x .   a  lim x→a  b  lim x→a  g x .  f x  g x   limx→a f x  limx→a g x   =   c  lim x→a the numerator and denominator are not both equal to zero or inﬁnity.  , provided that  Examples of cases  a – c  are discussed below.  141   SERIES AND LIMITS   cid:1 Evaluate the limits  Using  a  above,  Using  b ,  Using  c ,   x2 + 2x3 ,  lim x→1   x cos x ,  lim x→0  lim x→π 2  sin x  .  x  lim x→1   x2 + 2x3  = lim x→1  x2 + lim x→1  2x3 = 3.  lim x→0   x cos x  = lim x→0  x lim x→0  cos x = 0 × 1 = 0.  lim x→π 2  sin x  x  =  limx→π 2 sin x limx→π 2 x  =  1  π 2  =  .  cid:2   2 π   iv  Limits of functions of x that contain exponents that themselves depend on  x can often be found by taking logarithms.  cid:1 Evaluate the limit   cid:7   Let us deﬁne  and consider the logarithm of the required limit, i.e.  lim x→∞  y =  x2  .  x2   cid:8    cid:8   cid:7   x2  1 − a2  cid:7   1 − a2  cid:13   x2  x2 ln  1 − a2  x2   cid:8  cid:14   .   cid:8  cid:14   lim x→∞ ln y = lim x→∞   cid:13    cid:7   Using the Maclaurin series for ln 1 + x  given in subsection 4.6.3, we can expand the logarithm as a series and obtain  lim x→∞ ln y = lim x→∞  x2  − a2 x2  − a4 2x4  + ···  = −a2.  Therefore, since limx→∞ ln y = −a2 it follows that limx→∞ y = exp −a2 .  cid:2   v  L’H ˆopital’s rule may be used; it is an extension of  iii  c  above. In cases where both numerator and denominator are zero or both are inﬁnite, further consideration of the limit must follow. Let us ﬁrst consider limx→a f x  g x , where f a  = g a  = 0. Expanding the numerator and denominator as Taylor series we obtain  f a  +  x − a f g a  +  x − a g   cid:7   cid:7    a  + [ x − a 2 2!]f  a  + [ x − a 2 2!]g   cid:7  cid:7   cid:7  cid:7    a  + ···  a  + ··· .  f x  g x   =  However, f a  = g a  = 0 so   cid:7   cid:7    a  + [ x − a  2!]f  a  + [ x − a  2!]g  f g   cid:7  cid:7   cid:7  cid:7    a  + ···  a  + ··· .  f x  g x   =  142    cid:7    cid:7   a  and g  a  are not themselves both equal to zero. If, however,  cid:7   a  are both zero then the same process can be applied to the ratio  4.7 EVALUATION OF LIMITS  Therefore we ﬁnd  provided f  cid:7   a  and g f  cid:7   x  g f   cid:7    x  to yield   cid:1 Evaluate the limit  lim x→a  f x  g x   =   cid:7   cid:7   f g   a   a   ,   cid:7  cid:7   cid:7  cid:7   f g   a   a   ,  lim x→a  f x  g x   =  cid:7  cid:7    cid:7  cid:7   lim x→a  f x  g x   =  f n  a  g n  a   .  lim x→0  sin x  .  x   a  is non-zero. If the original limit does provided that at least one of f exist then it can be found by repeating the process as many times as is necessary for the ratio of corresponding nth derivatives not to be of the indeterminate form 0 0, i.e.   a  and g  We ﬁrst note that if x = 0, both numerator and denominator are zero. Thus we apply l’H ˆopital’s rule: diﬀerentiating, we obtain  lim x→0   sin x x  = lim x→0   cos x 1  = 1.  cid:2   So far we have only considered the case where f a  = g a  = 0. For the case  where f a  = g a  = ∞ we may still apply l’H ˆopital’s rule by writing  which is now of the form 0 0 at x = a. Note also that l’H ˆopital’s rule is still  valid for ﬁnding limits as x → ∞, i.e. when a = ∞. This is easily shown by letting  y = 1 x as follows:  lim x→a  f x  g x   = lim x→a  1 g x  1 f x   ,  lim x→∞  f x  g x   = lim y→0  f 1 y  g 1 y    1 y  y2  1 y  y2   cid:7   cid:7   −f −g  cid:7   cid:7   f g f g   1 y   1 y   cid:7   x   cid:7   x   .  = lim y→0  = lim y→0  = lim x→∞  143   SERIES AND LIMITS  Summary of methods for evaluating limits  To ﬁnd the limit of a continuous function f x  at a point x = a, simply substitute 0 = ∞. The only ∞ the value a into the function noting that ∞ ∞ results. In this case diﬃculty occurs when either of the expressions 0 diﬀerentiate top and bottom and try again. Continue diﬀerentiating until the top and bottom limits are no longer both zero or both inﬁnity. If the undetermined  0∞ = 0 and that  form 0 × ∞ occurs then it can always be rewritten as 0  0 or  ∞ ∞ .  0 or  4.1  4.2  4.4  4.5  Sum the even numbers between 1000 and 2000 inclusive. If you invest £1000 on the ﬁrst day of each year, and interest is paid at 5% on your balance at the end of each year, how much money do you have after 25 years?  4.3  How does the convergence of the series  depend on the integer r? Show that for testing the convergence of the series  x + y + x2 + y2 + x3 + y3 + ··· ,  where 0 < x < y < 1, the D’Alembert ratio test fails but the Cauchy root test is successful. Find the sum SN of the ﬁrst N terms of the following series, and hence determine whether the series are convergent, divergent or oscillatory:  ∞ cid:4   n=1   cid:7    cid:8   n + 1  n   a   ln  ,   b    −2 n,   c   ∞ cid:4   n=1   −1 n+1n  .  3n  4.6  By grouping and rearranging terms of the absolutely convergent series  4.8 Exercises  ∞ cid:4   n=r   n − r !  n!  ∞ cid:4   n=0  ∞ cid:4   n=1  1 n2 ,  S =  ∞ cid:4   n odd  So =  N cid:4   n=2  144  1 n2  =  3S 4  .  2n − 1 2n2 n − 1 2 .  show that  4.7  Use the diﬀerence method to sum the series   4.8 EXERCISES  4.8  The N + 1 complex numbers ωm are given by ωm = exp 2πim N , for m = 0, 1, 2, . . . , N.   a  Evaluate the following:   i   ωm,   ii   ω2 m,   iii   ωmxm.  N cid:4   m=0   cid:7    cid:8   N cid:4   cid:7   m=0   cid:8  cid:14   N cid:4  3 cid:4   m=0  m=0   i   cos  2πm N  − cos  4πm N  ,   ii   2m sin   cid:13   N cid:4   m=0   cid:7    cid:8   2πm  3  .   b  Use these results to evaluate:  4.9  Prove that  4.10  Determine whether the following series converge  θ and p are positive real numbers :  sin 1  2  n + 1 α sin 1  2 α  cos θ + 1  2 nα .  cos θ + cos θ + α  + ··· + cos θ + nα  = ∞ cid:4   ∞ cid:4    a   n=1  2 sin nθ n n + 1   ,   b   2 n2 ,  n=1  1 2n1 2 ,  ∞ cid:4  ∞ cid:4   n=1   c   n=1  np n!  .  ∞ cid:4   n=1  ∞ cid:4   n=2   d   ∞ cid:4   n=1  n ln n   −1 n n2 + 1 1 2 ∞ cid:4   n=1  xn  n + 1   d   ,  ∞ cid:4   n=1  ,   e   ∞ cid:4  ∞ cid:4   n=2  n=1   e    ln n x.   −1 n n5 2 ,   d    b   ∞ cid:4   n=0  n2 + 3n + 2  ∞ cid:4   n=0   c    −1 n2n  .  n1 2   −1 n 2n + 1  ∞ cid:4   n  ,  ,   e   n=1  ∞ cid:4   n=1   −1 nxn  ,  n!  enx,  ∞ cid:4   n=1  ∞ cid:4   n=1   −1 n ∞ cid:4   n=1  145  −n  .  xn 2e n  ∞ cid:4   n=1  ∞ cid:4   n=1   a   4.12  Determine whether the following series are convergent:   a   n1 2   n + 1 1 2 ,   b    c   n2 n!  ,   ln n n nn 2  ,   d   nn n!  .  4.13  Determine whether the following series are absolutely convergent, convergent or oscillatory:  4.11  Find the real values of x for which the following series are convergent:   a    b    sin x n,   c   nx,  4.14  Obtain the positive values of x for which the following series converges:   4.15  Prove that  4.16  is absolutely convergent for r = 2, but only conditionally convergent for r = 1. is positive, continuous and monotonically decreasing, for x ≥ 1, and the series An extension to the proof of the integral test  subsection 4.3.2  shows that, if f x  f 1  + f 2  + ··· is convergent, then its sum does not exceed f 1  + L, where L  is the integral   cid:11   −p, with  n  Use this result to show that the sum ζ p  of the Riemann zeta series  p > 1, is not greater than p  p − 1 .  cid:11    −1 n+1n  4.17  Demonstrate that rearranging the order of its terms can make a condition- ally convergent series converge to a diﬀerent limit by considering the series  −1 = ln 2 = 0.693. Rearrange the series as − 1  − 1  S = 1  1 + 1  3  2 + 1  5 + 1  7  4 + 1  9 + 1  11  13 + ···  6 + 1  and group each set of three successive terms. Show that the series can then be written  which is convergent  by comparison with terms. Evaluate the ﬁrst of these and hence deduce that S is not equal to ln 2. Illustrate result  iv  of section 4.4, concerning Cauchy products, by considering the double summation  n  −2  and contains only positive  4.18  SERIES AND LIMITS   cid:14    cid:13   ∞ cid:4   n=2  nr +  −1 n  nr  ln   cid:21  ∞  1  f x  dx.  − 1 ∞ cid:4   m=1  2m 4m − 3  4m − 1   ,  8m − 3  cid:11   ∞ cid:4   n cid:4   n=1  r=1  ∞ cid:4   ∞ cid:4   n=r  r=1  S =  S =  1  r2 n + 1 − r 3 .  1  r2 n + 1 − r 3 .  By examining the points in the nr-plane over which the double summation is to be carried out, show that S can be written as  Deduce that S ≤ 3.  4.19  A Fabry–P´erot interferometer consists of two parallel heavily silvered glass plates; light enters normally to the plates, and undergoes repeated reﬂections between them, with a small transmitted fraction emerging at each reﬂection. Find the  intensity of the emerging wave, B2, where ∞ cid:4  B = A 1 − r   rneinφ,  n=0  with r and φ real.  146   4.20  Identify the series  and then, by integration and diﬀerentiation, deduce the values S of the following series:  4.8 EXERCISES  ∞ cid:4  ∞ cid:4   n=1  n=1   a    c    −1 n+1n2  ,   2n !   −1 n+1nπ2n 4n 2n − 1 !  ,  ∞ cid:4   n=1  ,   −1 n+1x2n  2n − 1 ! ∞ cid:4  ∞ cid:4    b   n=1   d   n=0  2x4 3   −1 n+1n  ,   2n + 1 !   −1 n n + 1   .   2n !  4.21  4.22  Starting from the Maclaurin series for cos x, show that + ··· .   cos x   −2 = 1 + x2 +  cid:8    cid:7   Deduce the ﬁrst three terms in the Maclaurin series for tan x. Find the Maclaurin series for: 1 + x  −1,   c   sin2 x.   a   ln  1 − x  ,  4.23  Writing the nth derivative of f x  = sinh   b   x2 + 4  −1 x as Pn x   f n  x  =   1 + x2 n−1 2 ,  where Pn x  is a polynomial  of order n − 1 , show that the Pn x  satisfy the  recurrence relation  Pn+1 x  =  1 + x2 P  n x  −  2n − 1 xPn x .  cid:7   Hence generate the coeﬃcients necessary to express sinh up to terms in x5. Find the ﬁrst three non-zero terms in the Maclaurin series for the following functions:  −1 x as a Maclaurin series  4.24  −1 2,   a   x2 + 9   d  ln cos x ,   b  ln[ 2 + x 3],  e  exp[− x − a   −2],   c  exp sin x ,  f  tan  −1 x.  4.25  By using the logarithmic series, prove that if a and b are positive and nearly equal then   cid:15  2 a − b   .  a + b  ln  a b  Show that the error in this approximation is about 2 a − b 3 [3 a + b 3].  4.26  Determine whether the following functions f x  are  i  continuous, and  ii  diﬀerentiable at x = 0:   a  f x  = exp −x ;  b  f x  =  1 − cos x  x2 for x  cid:3 = 0, f 0  = 1  c  f x  = x sin 1 x  for x  cid:3 = 0, f 0  = 0; 2 ;  d  f x  = [4 − x2], where [y] denotes the integer part of y. 1 + xm−√ √ Find the limit as x → 0 of [  1 − xm] xn, in which m and n are positive  4.27  4.28  integers. Evaluate the following limits:  147   SERIES AND LIMITS   a  lim x→0   c  lim x→0  ,  sin 3x sinh x  tan x − x cos x − 1  ,  tan x − tanh x  cid:7  sinh x − x  ,  cosec x  x3  − sinh x x5   cid:8   .   b  lim x→0   d  lim x→0  4.29  Find the limits of the following functions:   a    b   ,  x3 + x2 − 5x − 2 2x3 − 7x2 + 4x + 4 sin x − x cosh x  cid:7   cid:21  sinh x − x  ,  y cos y − sin y  π 2  y2  x   c  √ 26.  as x → 0, x → ∞ and x → 2;   cid:8  as x → 0;  as x → 0.  dy,  4.30  4.31  Use Taylor expansions to three terms to ﬁnd approximations to  a  4  b  3 Using a ﬁrst-order Taylor expansion about x = x0, show that a better approxi- mation than x0 to the solution of the equation  √ 17, and  is given by x = x0 + δ, where  f x  = sin x + tan x = 2  2 − f x0   δ =  cos x0 + sec2 x0  .   a  Use this procedure twice to ﬁnd the solution of f x  = 2 to six signiﬁcant  ﬁgures, given that it is close to x = 0.9.   b  Use the result in  a  to deduce, to the same degree of accuracy, one solution  of the quartic equation  4.32  Evaluate  4.33  In quantum theory, a system of oscillators, each of fundamental frequency ν and interacting at temperature T , has an average energy ¯E given by  lim x→0  1 x3  y4 − 4y3 + 4y2 + 4y − 4 = 0.  cid:8  cid:14   cid:13    cid:7   − x 6  .  x  cosec x − 1  cid:11 ∞  cid:11 ∞  n=0 nhνe −nx  n=0 e  −nx  ,  ¯E =  4.34  where x = hν kT , h and k being the Planck and Boltzmann constants, respec- tively. Prove that both series converge, evaluate their sums, and show that at high  temperatures ¯E ≈ kT , whilst at low temperatures ¯E ≈ hν exp −hν kT  .  In a very simple model of a crystal, point-like atomic ions are regularly spaced along an inﬁnite one-dimensional row with spacing R. Alternate ions carry equal  and opposite charges ±e. The potential energy of the ith ion in the electric ﬁeld  due to another ion, the jth, is  where qi, qj are the charges on the ions and rij is the distance between them.  Write down a series giving the total contribution Vi of the ith ion to the overall  potential energy. Show that the series converges, and, if Vi is written as  qiqj  4π cid:4 0rij  ,  Vi =  αe2  ,  4π cid:4 0R  148   4.35  4.36  4.1 4.3 4.5  4.7  4.9  4.11  4.13  4.9 HINTS AND ANSWERS  ﬁnd a closed-form expression for α, the Madelung constant for this  unrealistic  lattice. One of the factors contributing to the high relative permittivity of water to static electric ﬁelds is the permanent electric dipole moment, p, of the water molecule. In an external ﬁeld E the dipoles tend to line up with the ﬁeld, but they do not do so completely because of thermal agitation corresponding to the temperature, T , of the water. A classical  non-quantum  calculation using the Boltzmann distribution shows that the average polarisability per molecule, α, is given by   coth x − x  −1 ,  α =  p E  where x = pE  kT   and k is the Boltzmann constant.  At ordinary temperatures, even with high ﬁeld strengths  104 V m −2.  −1 or more , x  cid:16  1. By making suitable series expansions of the hyperbolic functions involved, show that α = p2  3kT   to an accuracy of about one part in 15x In quantum theory, a certain method  the Born approximation  gives the  so- called  amplitude f θ  for the scattering of a particle of mass m through an angle θ by a uniform potential well of depth V0 and radius b  i.e. the potential energy  of the particle is −V0 within a sphere of radius b and zero elsewhere  as  f θ  =  2mV0  cid:1 2K 3   sin Kb − Kb cos Kb .  Here  cid:1  is the Planck constant divided by 2π, the energy of the particle is  cid:1 2k2  2m  and K is 2k sin θ 2 .  Use l’H ˆopital’s rule to evaluate the amplitude at low energies, i.e. when k and  hence K tend to zero, and so determine the low-energy total cross-section.  [ Note: the diﬀerential cross-section is given by f θ 2 and the total cross-   cid:1   section by the integral of this over all solid angles, i.e. 2π  f θ 2 sin θ dθ. ]  π 0   cid:11   n=1 n − cid:11   4.9 Hints and answers  1000  Write as 2   499 n=1 n  = 751 500. Divergent for r ≤ 1; convergent for r ≥ 2.  a  ln N + 1 , divergent;  b  1 −N] + 3 SN series; 3 Write the nth term as the diﬀerence between two consecutive values of a partial- fraction function of n. The sum equals 1 Sum the geometric series with rth term exp[i θ + rα ]. Its real part is  3 [1−  −2 n], oscillates inﬁnitely;  c  Add 1 4 N −3   −N−1, convergent to 3 16 .  16 [1 −  −3   2  1 − N  3 SN to the  −2 .  {cos θ − cos [ n + 1 α + θ] − cos θ − α  + cos θ + nα }  4 sin2 α 2 ,  which can be reduced to the given answer.   a  −1 ≤ x < 1;  b  all x except x =  2n ± 1 π 2;  c  x < −1;  d  x < 0;  e  always divergent. Clearly divergent for x > −1. For −X = x < −1, consider  where ln Mk = k and note that Mk − Mk−1 = e  −1 e − 1 Mk; hence show that the  series diverges.  a  Absolutely convergent, compare with exercise 4.10 b .  b  Oscillates ﬁnitely.  c  Absolutely convergent for all x.  d  Absolutely convergent; use partial frac- tions.  e  Oscillates inﬁnitely.  ∞ cid:4   Mk cid:4   k=1  n=Mk−1+1  1  ,   ln Mk X  149   SERIES AND LIMITS   cid:11    cid:11   4.15  4.17 4.19 4.21  4.23 4.25 4.27 4.29 4.31  4.33  n  n  Divide the series into two series, n odd and n even. For r = 2 both are absolutely −2. For r = 1 neither series is convergent, convergent, by comparison with −1. However, the sum of the two is convergent, by the by comparison with alternating sign test or by showing that the terms cancel in pairs. The ﬁrst term has value 0.833 and all other terms are positive.  A2 1 − r 2  1 + r2 − 2r cos φ . Use the binomial expansion and collect terms up to x4. Integrate both sides of the displayed equation. tan x = x + x3 3 + 2x5 15 + ··· . For example, P5 x  = 24x4 − 72x2 + 9. sinh Set a = D + δ and b = D − δ and use the expansion for ln 1 ± δ D . The limit is 0 for m > n, 1 for m = n, and ∞ for m < n.  a  − 1  −1 x = x − x3 6 + 3x5 40 − ··· .  2 , ∞;  b  −4;  c  −1 + 2 π. 2 , 1  cid:11 ∞   a  First approximation 0.886 452; second approximation 0.886 287.  b  Set y = sin x and re-express f x  = 2 as a polynomial equation. y = sin 0.886 287  = 0.774 730. If S  x  =  −nx evaluate S  x  and consider dS  x  dx.  E = hν[exp hν kT   − 1]  n=0 e   cid:8    cid:7   −1. px E  − x2 45  1 3  + ···  .  4.35  The series expansion is  150   Partial diﬀerentiation  In chapter 2, we discussed functions f of only one variable x, which were usually written f x . Certain constants and parameters may also have appeared in the deﬁnition of f, e.g. f x  = ax + 2 contains the constant 2 and the parameter a, but only x was considered as a variable and only the derivatives f n  x  = dnf dxn were deﬁned.  However, we may equally well consider functions that depend on more than one variable, e.g. the function f x, y  = x2 + 3xy, which depends on the two variables x and y. For any pair of values x, y, the function f x, y  has a well-deﬁned value, e.g. f 2, 3  = 22. This notion can clearly be extended to functions dependent on more than two variables. For the n-variable case, we write f x1, x2, . . . , xn  for a function that depends on the variables x1, x2, . . . , xn. When n = 2, x1 and x2 correspond to the variables x and y used above.  Functions of one variable, like f x , can be represented by a graph on a plane sheet of paper, and it is apparent that functions of two variables can, with little eﬀort, be represented by a surface in three-dimensional space. Thus, we may also picture f x, y  as describing the variation of height with position in a mountainous landscape. Functions of many variables, however, are usually very diﬃcult to visualise and so the preliminary discussion in this chapter will concentrate on functions of just two variables.  5.1 Deﬁnition of the partial derivative  It is clear that a function f x, y  of two variables will have a gradient in all directions in the xy-plane. A general expression for this rate of change can be found and will be discussed in the next section. However, we ﬁrst consider the simpler case of ﬁnding the rate of change of f x, y  in the positive x- and y- directions. These rates of change are called the partial derivatives with respect  5  151   PARTIAL DIFFERENTIATION  to x and y respectively, and they are extremely important in a wide range of physical applications.  For a function of two variables f x, y  we may deﬁne the derivative with respect to x, for example, by saying that it is that for a one-variable function when y is held ﬁxed and treated as a constant. To signify that a derivative is with respect to x, but at the same time to recognize that a derivative with respect to y also exists, the former is denoted by ∂f ∂x and is the partial derivative of f x, y  with respect to x. Similarly, the partial derivative of f with respect to y is denoted by ∂f ∂y.  To deﬁne formally the partial derivative of f x, y  with respect to x, we have  provided that the limit exists. This is much the same as for the derivative of a one-variable function. The other partial derivative of f x, y  is similarly deﬁned as a limit  provided it exists :  f x + ∆x, y  − f x, y   ∂f ∂x  = lim ∆x→0  ∆x  ∆y  ,  .  f x, y + ∆y  − f x, y   ∂f ∂y  = lim ∆y→0   5.1    5.2   It is common practice in connection with partial derivatives of functions involving more than one variable to indicate those variables that are held constant by writing them as subscripts to the derivative symbol. Thus, the partial derivatives deﬁned in  5.1  and  5.2  would be written respectively as   cid:7    cid:8   ∂f ∂x  y   cid:7    cid:8   ∂f ∂y  .  x  and  In this form, the subscript shows explicitly which variable is to be kept constant. A more compact notation for these partial derivatives is fx and fy. However, it is extremely important when using partial derivatives to remember which variables are being held constant and it is wise to write out the partial derivative in explicit form if there is any possibility of confusion.  The extension of the deﬁnitions  5.1 ,  5.2  to the general n-variable case is  straightforward and can be written formally as  [f x1, x2, . . . , xi + ∆xi, . . . , xn  − f x1, x2, . . . , xi, . . . , xn ]  ,  ∂f x1, x2, . . . , xn   ∂xi  = lim ∆xi→0  provided that the limit exists.  Just as for one-variable functions, second  and higher  partial derivatives may  be deﬁned in a similar way. For a two-variable function f x, y  they are  ∆xi   cid:8   cid:8    cid:7   cid:7    cid:7   cid:7    cid:8   cid:8   ∂ ∂x  ∂ ∂x  ∂f ∂x  ∂f ∂y  =  =  ∂2f ∂x2 = fxx, ∂2f ∂x∂y  = fxy,  ∂ ∂y  ∂ ∂y  ∂f ∂y  ∂f ∂x  =  =  ∂2f ∂y2 = fyy, ∂2f ∂y∂x  = fyx.  152   5.2 THE TOTAL DIFFERENTIAL AND TOTAL DERIVATIVE  Only three of the second derivatives are independent since the relation  is always obeyed, provided that the second partial derivatives are continuous at the point in question. This relation often proves useful as a labour-saving device when evaluating second partial derivatives. It can also be shown that for a function of n variables, f x1, x2, . . . , xn , under the same conditions,  ∂2f ∂x∂y  =  ∂2f ∂y∂x  ,  ∂2f  ∂2f  .  =  ∂xi∂xj  ∂xj∂xi  f x, y  = 2x3y2 + y3.   cid:1 Find the ﬁrst and second partial derivatives of the function  The ﬁrst partial derivatives are  = 6x2y2,  = 4x3y + 3y2,  ∂f ∂y  and the second partial derivatives are  ∂2f ∂x2  = 12xy2,  = 4x3 + 6y,  = 12x2y,  = 12x2y,  ∂2f ∂x∂y  ∂2f ∂y∂x  the last two being equal, as expected.  cid:2   ∂f ∂x  ∂2f ∂y2  5.2 The total diﬀerential and total derivative  Having deﬁned the  ﬁrst  partial derivatives of a function f x, y , which give the rate of change of f along the positive x- and y-axes, we consider next the rate of change of f x, y  in an arbitrary direction. Suppose that we make simultaneous small changes ∆x in x and ∆y in y and that, as a result, f changes to f + ∆f. Then we must have  ∆f = f x + ∆x, y + ∆y  − f x, y    cid:13  = f x + ∆x, y + ∆y  − f x, y + ∆y  + f x, y + ∆y  − f x, y   f x + ∆x, y + ∆y  − f x, y + ∆y   f x, y + ∆y  − f x, y    cid:14    cid:13    cid:14   =  ∆x  ∆x +  ∆y  In the last line we note that the quantities in brackets are very similar to those involved in the deﬁnitions of partial derivatives  5.1 ,  5.2 . For them to be strictly equal to the partial derivatives, ∆x and ∆y would need to be inﬁnitesimally small. But even for ﬁnite  but not too large  ∆x and ∆y the approximate formula  ∆y.   5.3    5.4   ∆f ≈ ∂f x, y   ∆x +  ∂x  ∂f x, y   ∆y,  ∂y  153   PARTIAL DIFFERENTIATION  can be obtained. It will be noticed that the ﬁrst bracket in  5.3  actually approxi- mates to ∂f x, y + ∆y  ∂x but that this has been replaced by ∂f x, y  ∂x in  5.4 . This approximation clearly has the same degree of validity as that which replaces the bracket by the partial derivative.  How valid an approximation  5.4  is to  5.3  depends not only on how small ∆x and ∆y are but also on the magnitudes of higher partial derivatives; this is discussed further in section 5.7 in the context of Taylor series for functions of more than one variable. Nevertheless, letting the small changes ∆x and ∆y in  5.4  become inﬁnitesimal, we can deﬁne the total diﬀerential df of the function f x, y , without any approximation, as  df =  dx +  dy.  ∂f ∂x  ∂f ∂y  Equation  5.5  can be extended to the case of a function of n variables, f x1, x2, . . . , xn ;  df =  dx1 +  ∂f ∂x1  dx2 + ··· +  ∂f ∂x2  ∂f ∂xn  dxn.   5.5    5.6    cid:1 Find the total diﬀerential of the function f x, y  = y exp x + y .  Evaluating the ﬁrst partial derivatives, we ﬁnd  ∂f ∂x  ∂f ∂y  = y exp x + y ,  = exp x + y  + y exp x + y .  Applying  5.5 , we then ﬁnd that the total diﬀerential is given by  df = [y exp x + y ]dx + [ 1 + y  exp x + y ]dy.  cid:2   In some situations, despite the fact that several variables xi, i = 1, 2, . . . , n, appear to be involved, eﬀectively only one of them is. This occurs if there are subsidiary relationships constraining all the xi to have values dependent on the value of one of them, say x1. These relationships may be represented by equations that are typically of the form  xi = xi x1 ,  i = 2, 3, . . . , n.   5.7   In principle f can then be expressed as a function of x1 alone by substituting from  5.7  for x2, x3, . . . , xn, and then the total derivative  or simply the derivative  of f with respect to x1 is obtained by ordinary diﬀerentiation.  Alternatively,  5.6  can be used to give   cid:7    cid:8    cid:7    cid:8   df dx1  =  ∂f ∂x1  +  ∂f ∂x2  dx2 dx1  + ··· +  ∂f ∂xn  dxn dx1  .   5.8   It should be noted that the LHS of this equation is the total derivative df dx1, whilst the partial derivative ∂f ∂x1 forms only a part of the RHS. In evaluating  154   5.3 EXACT AND INEXACT DIFFERENTIALS  this partial derivative account must be taken only of explicit appearances of x1 in the function f, and no allowance must be made for the knowledge that changing x1 necessarily changes x2, x3, . . . , xn. The contribution from these latter changes is precisely that of the remaining terms on the RHS of  5.8 . Naturally, what has been shown using x1 in the above argument applies equally well to any other of the xi, with the appropriate consequent changes.  cid:1 Find the total derivative of f x, y  = x2 + 3xy with respect to x, given that y = sin  −1 x.  We can see immediately that  ∂f ∂x  = 2x + 3y,  = 3x,  ∂f ∂y  dy dx  =  1   1 − x2 1 2  and so, using  5.8  with x1 = x and x2 = y,  df dx  = 2x + 3y + 3x  = 2x + 3 sin  1   1 − x2 1 2  −1 x +  3x   1 − x2 1 2 .  Obviously the same expression would have resulted if we had substituted for y from the start, but the above method often produces results with reduced calculation, particularly in more complicated examples.  cid:2   5.3 Exact and inexact diﬀerentials  In the last section we discussed how to ﬁnd the total diﬀerential of a function, i.e. its inﬁnitesimal change in an arbitrary direction, in terms of its gradients ∂f ∂x and ∂f ∂y in the x- and y- directions  see  5.5  . Sometimes, however, we wish to reverse the process and ﬁnd the function f that diﬀerentiates to give a known diﬀerential. Usually, ﬁnding such functions relies on inspection and experience.  As an example, it is easy to see that the function whose diﬀerential is df = x dy + y dx is simply f x, y  = xy + c, where c is a constant. Diﬀerentials such as this, which integrate directly, are called exact diﬀerentials, whereas those that do not are inexact diﬀerentials. For example, x dy + 3y dx is not the straightforward diﬀerential of any function  see below . Inexact diﬀerentials can be made exact, however, by multiplying through by a suitable function called an integrating factor. This is discussed further in subsection 14.2.3.  cid:1 Show that the diﬀerential x dy + 3y dx is inexact.  On the one hand, if we integrate with respect to x we conclude that f x, y  = 3xy + g y , where g y  is any function of y. On the other hand, if we integrate with respect to y we conclude that f x, y  = xy + h x  where h x  is any function of x. These conclusions are inconsistent for any and every choice of g y  and h x , and therefore the diﬀerential is inexact.  cid:2   It is naturally of interest to investigate which properties of a diﬀerential make  155   PARTIAL DIFFERENTIATION  it exact. Consider the general diﬀerential containing two variables,  We see that  df = A x, y  dx + B x, y  dy.  ∂f ∂x  = A x, y ,  = B x, y   ∂f ∂y  and, using the property fxy = fyx, we therefore require  ∂A ∂y  =  ∂B ∂x  .   5.9   This is in fact both a necessary and a suﬃcient condition for the diﬀerential to be exact.  cid:1 Using  5.9  show that x dy + 3y dx is inexact.  In the above notation, A x, y  = 3y and B x, y  = x and so  As these are not equal it follows that the diﬀerential is inexact.  cid:2   ∂A ∂y  = 3,  ∂B ∂x  = 1.  Determining whether a diﬀerential containing many variable x1, x2, . . . , xn is exact is a simple extension of the above. A diﬀerential containing many variables can be written in general as  n cid:4   i=1  df =  gi x1, x2, . . . , xn  dxi  and will be exact if  ∂gi ∂xj  =  ∂gj ∂xi  for all pairs i, j.   5.10   There will be 1  2 n n − 1  such relationships to be satisﬁed.   cid:1 Show that  is an exact diﬀerential.   y + z  dx + x dy + x dz  In this case, g1 x, y, z  = y + z, g2 x, y, z  = x, g3 x, y, z  = x and hence ∂g1 ∂y = 1 = ∂g2 ∂x, ∂g3 ∂x = 1 = ∂g1 ∂z, ∂g2 ∂z = 0 = ∂g3 ∂y; therefore, from  5.10 , the diﬀerential is exact. As mentioned above, it is sometimes possible to show that a diﬀerential is exact simply by ﬁnding by inspection the function from which it originates. In this example, it can be seen easily that f x, y, z  = x y + z  + c.  cid:2   156   5.4 USEFUL THEOREMS OF PARTIAL DIFFERENTIATION  5.4 Useful theorems of partial diﬀerentiation  So far our discussion has centred on a function f x, y  dependent on two variables, x and y. Equally, however, we could have expressed x as a function of f and y, or y as a function of f and x. To emphasise the point that all the variables are of equal standing, we now replace f by z. This does not imply that x, y and z are coordinate positions  though they might be . Since x is a function of y and z, it follows that  and similarly, since y = y x, z ,  dy =   cid:7    cid:8    cid:7   dx =  ∂x ∂y  z  ∂y ∂x  We may now substitute  5.12  into  5.11  to obtain  dx =  dy +  dz  y   cid:7   cid:7   cid:8   cid:7   z   cid:8   cid:8   z  ∂x ∂y  ∂y ∂x  dx +  z   cid:16  cid:7   cid:7   ∂x ∂y  dx +   cid:8    cid:8   cid:8   ∂x ∂z   cid:7   cid:7   cid:7   cid:8   cid:8 −1  ∂y ∂z  z  ∂x ∂y  =  z  ∂y ∂x  ,  z   5.11    5.12   dz.  x   cid:8    cid:7    cid:17    cid:8   ∂y ∂z  +  x  ∂x ∂z  y  dz.   5.13   Now if we hold z constant, so that dz = 0, we obtain the reciprocity relation  which holds provided both partial derivatives exist and neither is equal to zero. Note, further, that this relationship only holds when the variable being kept constant, in this case z, is the same on both sides of the equation.  Alternatively we can put dx = 0 in  5.13 . Then the contents of the square  brackets also equal zero, and we obtain the cyclic relation   cid:7    cid:8    cid:7    cid:8    cid:7    cid:8   ∂y ∂z  ∂z ∂x  ∂x ∂y  x  y  z  = −1,  which holds unless any of the derivatives vanish. In deriving this result we have used the reciprocity relation to replace  ∂x ∂z   −1 y by  ∂z ∂x y.  5.5 The chain rule  So far we have discussed the diﬀerentiation of a function f x, y  with respect to its variables x and y. We now consider the case where x and y are themselves functions of another variable, say u. If we wish to ﬁnd the derivative df du, we could simply substitute in f x, y  the expressions for x u  and y u  and then diﬀerentiate the resulting function of u. Such substitution will quickly give the desired answer in simple cases, but in more complicated examples it is easier to make use of the total diﬀerentials described in the previous section.  157   PARTIAL DIFFERENTIATION  From equation  5.5  the total diﬀerential of f x, y  is given by  df =  dx +  dy,  ∂f ∂x  ∂f ∂y  but we now note that by using the formal device of dividing through by du this immediately implies  df du  =  ∂f ∂x  dx du  +  ∂f ∂y  dy du  ,   5.14   which is called the chain rule for partial diﬀerentiation. This expression provides a direct method for calculating the total derivative of f with respect to u and is particularly useful when an equation is expressed in a parametric form.   cid:1 Given that x u  = 1 + au and y u  = bu3, ﬁnd the rate of change of f x, y  = xe respect to u.  −y with  As discussed above, this problem could be addressed by substituting for x and y to obtain f as a function only of u and then diﬀerentiating with respect to u. However, using  5.14  directly we obtain  −y a +  −xe  −y 3bu2,  =  e  df du  which on substituting for x and y gives −bu3  df du  = e   a − 3bu2 − 3bau3 .  cid:2   Equation  5.14  is an example of the chain rule for a function of two variables each of which depends on a single variable. The chain rule may be extended to functions of many variables, each of which is itself a function of a variable u, i.e. f x1, x2, x3, . . . , xn , with xi = xi u . In this case the chain rule gives  df du  =  ∂f ∂xi  dxi du  =  ∂f ∂x1  dx1 du  +  ∂f ∂x2  dx2 du  + ··· +  ∂f ∂xn  dxn du  .   5.15   n cid:4   i=1  5.6 Change of variables  It is sometimes necessary or desirable to make a change of variables during the course of an analysis, and consequently to have to change an equation expressed in one set of variables into an equation using another set. The same situation arises if a function f depends on one set of variables xi, so that f = f x1, x2, . . . , xn  but the xi are themselves functions of a further set of variables uj and given by the equations  xi = xi u1, u2, . . . , um .   5.16   158   5.6 CHANGE OF VARIABLES  y  ρ  φ  x  Figure 5.1 The relationship between Cartesian and plane polar coordinates.  For each diﬀerent value of i, xi will be a diﬀerent function of the uj. In this case the chain rule  5.15  becomes  n cid:4   i=1  ∂f ∂uj  =  ∂f ∂xi  ∂xi ∂uj  ,  j = 1, 2, . . . , m,   5.17   and is said to express a change of variables. In general the number of variables in each set need not be equal, i.e. m need not equal n, but if both the xi and the ui are sets of independent variables then m = n.   cid:1 Plane polar coordinates, ρ and φ, and Cartesian coordinates, x and y, are related by the expressions  x = ρ cos φ,  y = ρ sin φ,  as can be seen from ﬁgure 5.1. An arbitrary function f x, y  can be re-expressed as a function g ρ, φ . Transform the expression  into one in ρ and φ.  ∂2f ∂x2  +  ∂2f ∂y2  We ﬁrst note that ρ2 = x2 + y2, φ = tan derivatives  −1 y x . We can now write down the four partial  ∂ρ ∂x  ∂ρ ∂y  =  =  x  y   x2 + y2 1 2  = cos φ,   x2 + y2 1 2  = sin φ,  − y x2   1 +  y x 2  = − sin φ  ,  ρ  1 x  1 +  y x 2  =  cos φ  .  ρ  ∂φ ∂x  ∂φ ∂y  =  =  159   Thus, from  5.17 , we may write  Now it is only a matter of writing  PARTIAL DIFFERENTIATION  ∂ ∂x  = cos φ  ρ  − sin φ  cid:8   ∂ ∂φ  ,   cid:7   ∂ ∂ρ   cid:7   ∂2f ∂x2  =  ∂ ∂x   cid:7   cid:7   =  cos φ  =  cos φ  ∂f ∂x  ∂ ∂ρ  ∂ ∂ρ ∂2g ∂ρ2 sin2 φ  ρ  ∂ ∂x  = − sin φ − sin φ  ρ  ρ  ∂ ∂x  ∂ ∂φ  ∂ ∂φ  ρ2 sin2 φ  ρ2  ∂g ∂ρ  +  = cos2 φ  +  2 cos φ sin φ   cid:8   cid:8  cid:7   cid:8  cid:7   f  ∂2g ∂φ2   cid:8  cid:7   ∂2f ∂y2  =  sin φ  = sin2 φ  ∂ ∂ρ ∂2g ∂ρ2 cos2 φ  ρ  cos φ  ∂ ∂φ  ρ  + − 2 cos φ sin φ  cos2 φ  ρ2  ρ2  ∂2g ∂φ2 .  ∂g ∂ρ  +  +   cid:7   +  ∂ ∂y  = sin φ  +  ∂ ∂ρ  cos φ  ρ  ∂ ∂φ  .  cos φ  cos φ  ρ  ∂ ∂ρ  − sin φ − sin φ − 2 cos φ sin φ  ∂g ∂ρ  ρ  ∂ ∂φ  ∂g ∂φ  ∂g ∂φ  ρ   cid:8   cid:8   g  ∂2g ∂φ∂ρ   cid:8   sin φ  +  ∂ ∂ρ  cos φ  ρ  ∂ ∂φ  g  ∂g ∂φ  +  2 cos φ sin φ  ρ  ∂2g ∂φ∂ρ  and a similar expression for ∂2f ∂y2,  When these two expressions are added together the change of variables is complete and  we obtain  ∂2f ∂x2  +  ∂2f ∂y2  =  ∂2g ∂ρ2  +  1 ρ  ∂g ∂ρ  +  1 ρ2  ∂2g  ∂φ2 .  cid:2   5.7 Taylor’s theorem for many-variable functions  We have already introduced Taylor’s theorem for a function f x  of one variable, in section 4.6. In an analogous way, the Taylor expansion of a function f x, y  of two variables is given by  f x, y  = f x0, y0  +  ∆x +  ∆y  ∂f ∂y   cid:13   ∂f ∂x 1 2!  +  ∂2f ∂x2  ∆x 2 + 2  ∂2f ∂x∂y  ∆x∆y +  ∂2f ∂y2  ∆y 2  + ··· ,   5.18   where ∆x = x − x0 and ∆y = y − y0, and all the derivatives are to be evaluated   cid:14   at  x0, y0 .  160   5.7 TAYLOR’S THEOREM FOR MANY-VARIABLE FUNCTIONS   cid:1 Find the Taylor expansion, up to quadratic terms in x− 2 and y− 3, of f x, y  = y exp xy  about the point x = 2, y = 3.  We ﬁrst evaluate the required partial derivatives of the function, i.e.  = y2 exp xy,  = exp xy + xy exp xy,  ∂f ∂y  ∂2f ∂y2   cid:7   ∂ ∂x   cid:14   = y3 exp xy,  = 2x exp xy + x2y exp xy,  = 2y exp xy + xy2 exp xy.  ∂f ∂x  ∂2f ∂x2  !  ∂2f ∂x∂y   cid:18    cid:19 "   cid:8   2  Using  5.18 , the Taylor expansion of a two-variable function, we ﬁnd  f x, y  ≈ e6  3 + 9 x − 2  + 7 y − 3   −1  27 x − 2 2 + 48 x − 2  y − 3  + 16 y − 3 2  +  2!   .  cid:2    cid:8   ∂ ∂y   cid:7    cid:8   n  It will be noticed that the terms in  5.18  containing ﬁrst derivatives can be  written as   cid:13   ∂f ∂x  ∂f ∂y  ∆x +  ∆y =  ∆x  + ∆y  f x, y ,  where both sides of this relation should be evaluated at the point  x0, y0 . Similarly the terms in  5.18  containing second derivatives can be written as  1 2!  ∂2f ∂x2  ∆x 2 + 2  ∂2f ∂x∂y  ∆x∆y +  ∂2f ∂y2  ∆y 2  =  1 2!  ∆x  + ∆y  f x, y ,  ∂ ∂x  ∂ ∂y   5.19   where it is understood that the partial derivatives resulting from squaring the expression in parentheses act only on f x, y  and its derivatives, and not on ∆x or ∆y; again both sides of  5.19  should be evaluated at  x0, y0 . It can be shown that the higher-order terms of the Taylor expansion of f x, y  can be written in an analogous way, and that we may write the full Taylor series as   cid:13  cid:7   ∞ cid:4   n=0  1 n!  f x, y  =  ∆x  + ∆y  f x, y   ∂ ∂x  ∂ ∂y   cid:14   x0,y0  where, as indicated, all the terms on the RHS are to be evaluated at  x0, y0 .  The most general form of Taylor’s theorem, for a function f x1, x2, . . . , xn  of n variables, is a simple extension of the above. Although it is not necessary to do so, we may think of the xi as coordinates in n-dimensional space and write the function as f x , where x is a vector from the origin to  x1, x2, . . . , xn . Taylor’s  161   PARTIAL DIFFERENTIATION   cid:4   i   cid:4    cid:4   i  j  theorem then becomes  f x  = f x0  +  ∂f ∂xi  ∆xi +  1 2!  ∂2f  ∂xi∂xj  ∆xi∆xj + ··· ,   5.20   where ∆xi = xi − xi0 and the partial derivatives are evaluated at  x10 , x20 , . . . , xn0 .  For completeness, we note that in this case the full Taylor series can be written in the form   cid:18    cid:19   ∆x · ∇ n f x   ∞ cid:4   n=0  1 n!  ,  x=x0  f x  =  where ∇ is the vector diﬀerential operator del, to be discussed in chapter 10.  5.8 Stationary values of many-variable functions  The idea of the stationary points of a function of just one variable has already been discussed in subsection 2.1.8. We recall that the function f x  has a stationary point at x = x0 if its gradient df dx is zero at that point. A function may have any number of stationary points, and their nature, i.e. whether they are maxima, minima or stationary points of inﬂection, is determined by the value of the second derivative at the point. A stationary point is   i  a minimum if d2f dx2 > 0;  ii  a maximum if d2f dx2 < 0;  iii  a stationary point of inﬂection if d2f dx2 = 0 and changes sign through  the point.  We now consider the stationary points of functions of more than one variable; we will see that partial diﬀerential analysis is ideally suited to the determination of the position and nature of such points. It is helpful to consider ﬁrst the case of a function of just two variables but, even in this case, the general situation is more complex than that for a function of one variable, as can be seen from ﬁgure 5.2.  This ﬁgure shows part of a three-dimensional model of a function f x, y . At positions P and B there are a peak and a bowl respectively or, more mathemati- cally, a local maximum and a local minimum. At position S the gradient in any direction is zero but the situation is complicated, since a section parallel to the plane x = 0 would show a maximum, but one parallel to the plane y = 0 would show a minimum. A point such as S is known as a saddle point. The orientation of the ‘saddle’ in the xy-plane is irrelevant; it is as shown in the ﬁgure solely for ease of discussion. For any saddle point the function increases in some directions away from the point but decreases in other directions.  162   5.8 STATIONARY VALUES OF MANY-VARIABLE FUNCTIONS  S  P  B  y  x  Figure 5.2 Stationary points of a function of two variables. A minimum occurs at B, a maximum at P and a saddle point at S .  For functions of two variables, such as the one shown, it should be clear that a necessary condition for a stationary point  maximum, minimum or saddle point  to occur is that  ∂f ∂x  = 0  and  ∂f ∂y  = 0.   5.21   The vanishing of the partial derivatives in directions parallel to the axes is enough to ensure that the partial derivative in any arbitrary direction is also zero. The latter can be considered as the superposition of two contributions, one along each axis; since both contributions are zero, so is the partial derivative in the arbitrary direction. This may be made more precise by considering the total diﬀerential  df =  dx +  dy.  ∂f ∂x  ∂f ∂y  Using  5.21  we see that although the inﬁnitesimal changes dx and dy can be chosen independently the change in the value of the inﬁnitesimal function df is always zero at a stationary point.  We now turn our attention to determining the nature of a stationary point of a function of two variables, i.e. whether it is a maximum, a minimum or a saddle point. By analogy with the one-variable case we see that ∂2f ∂x2 and ∂2f ∂y2 must both be positive for a minimum and both be negative for a maximum. However these are not suﬃcient conditions since they could also be obeyed at complicated saddle points. What is important for a minimum  or maximum  is that the second partial derivative must be positive  or negative  in all directions, not just in the x- and y- directions.  163   PARTIAL DIFFERENTIATION  To establish just what constitutes suﬃcient conditions we ﬁrst note that, since f is a function of two variables and ∂f ∂x = ∂f ∂y = 0, a Taylor expansion of the type  5.18  about the stationary point yields  f x, y  − f x0, y0  ≈ 1   ∆x 2fxx + 2∆x∆yfxy +  ∆y 2fyy  ,  where ∆x = x − x0 and ∆y = y − y0 and where the partial derivatives have been  written in more compact notation. Rearranging the contents of the bracket as the weighted sum of two squares, we ﬁnd   cid:19    cid:18   2!   cid:7    cid:16    cid:8   2   cid:30  fyy − f2  xy fxx   cid:31  cid:17   .   5.22   f x, y  − f x0, y0  ≈ 1  fxx  ∆x +  2  fxy∆y  fxx  +  ∆y 2  For a minimum, we require  5.22  to be positive for all ∆x and ∆y, and hence fxx > 0 and fyy −  f2 xy fxx  > 0. Given the ﬁrst constraint, the second can be written fxxfyy > f2 xy. Similarly for a maximum we require  5.22  to be negative, and hence fxx   f2 xy. For minima and maxima, symmetry requires that fyy obeys the same criteria as fxx. When  5.22  is negative  or zero  for some values of ∆x and ∆y but positive  or zero  for others, we have a saddle point. In this case fxxfyy < f2 xy. In summary, all stationary points have fx = fy = 0 and they may be classiﬁed further as   i  minima if both fxx and fyy are positive and f2  ii  maxima if both fxx and fyy are negative and f2  iii  saddle points if fxx and fyy have opposite signs or f2  xy < fxxfyy,  xy < fxxfyy,  xy > fxxfyy.  Note, however, that if f2 of the four forms  xy = fxxfyy then f x, y  − f x0, y0  can be written in one   cid:9  ∆xfxx1 2 ± ∆yfyy1 2   cid:10   2  .  ± 1 2  For some choice of the ratio ∆y ∆x this expression has zero value, showing that, for a displacement from the stationary point in this particular direction, f x0 + ∆x, y0 + ∆y  does not diﬀer from f x0, y0  to second order in ∆x and ∆y; in such situations further investigation is required. In particular, if fxx, fyy and fxy are all zero then the Taylor expansion has to be taken to a higher order. As examples, such extended investigations would show that the function f x, y  = x4 + y4 has a minimum at the origin but that g x, y  = x4 + y3 has a saddle point there.  164   5.8 STATIONARY VALUES OF MANY-VARIABLE FUNCTIONS  a minimum at  − cid:24    cid:1 Show that the function f x, y  = x3 exp −x2− y2  has a maximum at the point    3 2, 0 , 3 2, 0  and a stationary point at the origin whose nature cannot be  determined by the above procedures.   cid:24   Setting the ﬁrst two partial derivatives to zero to locate the stationary points, we ﬁnd  =  3x2 − 2x4  exp −x2 − y2  = 0, = −2yx3 exp −x2 − y2  = 0.  ∂f ∂x ∂f ∂y  x = 0 or x = ± cid:24   For  5.24  to be satisﬁed we require x = 0 or y = 0 and for  5.23  to be satisﬁed we require 3 2, 0 .  3 2. Hence the stationary points are at  0, 0 ,    We now ﬁnd the second partial derivatives:   5.23    5.24    cid:24   3 2, 0  and  − cid:24   fxx =  4x5 − 14x3 + 6x  exp −x2 − y2 , fyy = x3 4y2 − 2  exp −x2 − y2 , fxy = 2x2y 2x2 − 3  exp −x2 − y2 .  We then substitute the pairs of values of x and y for each stationary point and ﬁnd that at  0, 0   and at  ± cid:24   cid:24   3 2, 0    cid:24   fxx = ∓6  3 2 exp −3 2 ,  3 2, 0  is a maximum and  − cid:24   fxx = 0,  fyy = 0,  fxy = 0   cid:24   Hence, applying criteria  i – iii  above, we ﬁnd that  0, 0  is an undetermined stationary point,   3 2, 0  is a minimum. The function is shown in ﬁgure 5.3.  cid:2   fyy = ∓3  3 2 exp −3 2 ,  fxy = 0.  Determining the nature of stationary points for functions of a general number of variables is considerably more diﬃcult and requires a knowledge of the eigenvectors and eigenvalues of matrices. Although these are not discussed until chapter 8, we present the analysis here for completeness. The remainder of this section can therefore be omitted on a ﬁrst reading.  For a function of n real variables, f x1, x2, . . . , xn , we require that, at all  stationary points,  ∂f ∂xi  = 0  for all xi.  In order to determine the nature of a stationary point, we must expand the function as a Taylor series about the point. Recalling the Taylor expansion  5.20  for a function of n variables, we see that  ∆f = f x  − f x0  ≈ 1  ∂2f  ∂xi∂xj  ∆xi∆xj.   5.25    cid:4    cid:4   2  i  j  165   PARTIAL DIFFERENTIATION  maximum  0.4  0.2  0  −0.2  −0.4  −3  −2  minimum  −1  0  x  1  2  3  2  0  y  −2  Figure 5.3 The function f x, y  = x3 exp −x2 − y2 .  If we deﬁne the matrix M to have elements given by  Mij =  ∂2f  ,  ∂xi∂xj  then we can rewrite  5.25  as  ∆f = 1  2 ∆xTM∆x,   5.26   where ∆x is the column vector with the ∆xi as its components and ∆xT is its transpose. Since M is real and symmetric it has n real eigenvalues λr and n orthogonal eigenvectors er, which after suitable normalisation satisfy  Mer = λrer,  eT r  es = δrs,  where the Kronecker delta, written δrs, equals unity for r = s and equals zero otherwise. These eigenvectors form a basis set for the n-dimensional space and we can therefore expand ∆x in terms of them, obtaining  ∆x =  arer,   cid:4   r  166   5.9 STATIONARY VALUES UNDER CONSTRAINTS  where the ar are coeﬃcients dependent upon ∆x. Substituting this into  5.26 , we ﬁnd  ∆f = 1  2 ∆xTM∆x = 1  2  λra2 r .   cid:4   r  r λra2  Now, for the stationary point to be a minimum, we require ∆f = 1 r > 0 2 for all sets of values of the ar, and therefore all the eigenvalues of M to be greater than zero. Conversely, for a maximum we require ∆f = 1 r < 0, 2 and therefore all the eigenvalues of M to be less than zero. If the eigenvalues have mixed signs, then we have a saddle point. Note that the test may fail if some or all of the eigenvalues are equal to zero and all the non-zero ones have the same sign.  cid:1 Derive the conditions for maxima, minima and saddle points for a function of two real variables, using the above analysis.  r λra2   cid:11   cid:11   For a two-variable function the matrix M is given by  Therefore its eigenvalues satisfy the equation  Hence  M =   cid:7   cid:20  cid:20  cid:20  cid:20  fxx − λ  fxx fyx  fxy fyy  .   cid:8   cid:20  cid:20  cid:20  cid:20  = 0.  fxy  fxy  fyy − λ  fxx − λ  fyy − λ  − f2  xy = 0 ⇒ fxxfyy −  fxx + fyy λ + λ2 − f2  xy = 0   cid:2   fxx + fyy 2 − 4 fxxfyy − f2 xy ,  cid:2    fxx − fyy 2 + 4f2  xy.   fxx + fyy 2 − 4 fxxfyy − f2 xy ,  ⇒ 2λ =  fxx + fyy  ±  2λ =  fxx + fyy  ±  cid:2   fxx + fyy >  ⇒ fxxfyy − f2  xy > 0.  which by rearrangement of the terms under the square root gives  A similar procedure will ﬁnd the criteria for maxima and saddle points.  cid:2   Now, that M is real and symmetric implies that its eigenvalues are real, and so for both eigenvalues to be positive  corresponding to a minimum , we require fxx and fyy positive and also  5.9 Stationary values under constraints  In the previous section we looked at the problem of ﬁnding stationary values of a function of two or more variables when all the variables may be independently  167   PARTIAL DIFFERENTIATION  varied. However, it is often the case in physical problems that not all the vari- ables used to describe a situation are in fact independent, i.e. some relationship between the variables must be satisﬁed. For example, if we walk through a hilly landscape and we are constrained to walk along a path, we will never reach the highest peak on the landscape unless the path happens to take us to it. Nevertheless, we can still ﬁnd the highest point that we have reached during our journey.  We ﬁrst discuss the case of a function of just two variables. Let us consider ﬁnding the maximum value of the diﬀerentiable function f x, y  subject to the constraint g x, y  = c, where c is a constant. In the above analogy, f x, y  might represent the height of the land above sea-level in some hilly region, whilst g x, y  = c is the equation of the path along which we walk.  We could, of course, use the constraint g x, y  = c to substitute for x or y in f x, y , thereby obtaining a new function of only one variable whose stationary points could be found using the methods discussed in subsection 2.1.8. However, such a procedure can involve a lot of algebra and becomes very tedious for func- tions of more than two variables. A more direct method for solving such problems is the method of Lagrange undetermined multipliers, which we now discuss.  To maximise f we require  If dx and dy were independent, we could conclude fx = 0 = fy. However, here they are not independent, but constrained because g is constant:  df =  dx +  dy = 0.  ∂f ∂x  ∂g ∂x  ∂f ∂y  ∂g ∂y  dg =  dx +  dy = 0.   cid:7    cid:8    cid:7    cid:8   Multiplying dg by an as yet unknown number λ and adding it to df we obtain  d f + λg  =  ∂f ∂x  + λ  ∂g ∂x  dx +  ∂f ∂y  + λ  ∂g ∂y  dy = 0,  where λ is called a Lagrange undetermined multiplier. In this equation dx and dy are to be independent and arbitrary; we must therefore choose λ such that  ∂f ∂x  ∂f ∂y  + λ  = 0,  + λ  = 0.  ∂g ∂x  ∂g ∂y  168   5.27    5.28   These equations, together with the constraint g x, y  = c, are suﬃcient to ﬁnd the three unknowns, i.e. λ and the values of x and y at the stationary point.   5.9 STATIONARY VALUES UNDER CONSTRAINTS   cid:1 The temperature of a point  x, y  on a unit circle is given by T  x, y  = 1 + xy. Find the temperature of the two hottest points on the circle.  We need to maximise T  x, y  subject to the constraint x2 + y2 = 1. Applying  5.27  and  5.28 , we obtain  y + 2λx = 0,  x + 2λy = 0.   5.29    5.30   These results, together with the original constraint x2 + y2 = 1, provide three simultaneous equations that may be solved for λ, x and y. From  5.29  and  5.30  we ﬁnd λ = ±1 2, which in turn implies that y = ∓x. Remem-  bering that x2 + y2 = 1, we ﬁnd that  y = x ⇒ x = ± 1√ y = −x ⇒ x = ∓ 1√  ,  ,  2  2  y = ± 1√ y = ± 1√  2  .  2  We have not yet determined which of these stationary points are maxima and which are minima. In this simple case, we need only substitute the four pairs of x- and y- values into T  x, y  = 1 + xy to ﬁnd that the maximum temperature on the unit circle is Tmax = 3 2 at  the points y = x = ±1   √ 2.  cid:2   The method of Lagrange multipliers can be used to ﬁnd the stationary points of functions of more than two variables, subject to several constraints, provided that the number of constraints is smaller than the number of variables. For example, if we wish to ﬁnd the stationary points of f x, y, z  subject to the constraints g x, y, z  = c1 and h x, y, z  = c2, where c1 and c2 are constants, then we proceed as above, obtaining  ∂ ∂x  ∂ ∂y  ∂ ∂z   f + λg + µh  =  + λ  + µ  = 0,   f + λg + µh  =  + λ  + µ  = 0,   5.31    f + λg + µh  =  + λ  + µ  = 0.  ∂g ∂x  ∂g ∂y  ∂g ∂z  ∂h ∂x  ∂h ∂y  ∂h ∂z  ∂f ∂x  ∂f ∂y  ∂f ∂z  169  We may now solve these three equations, together with the two constraints, to give λ, µ, x, y and z.   PARTIAL DIFFERENTIATION   cid:1 Find the stationary points of f x, y, z  = x3 + y3 + z3 subject to the following constraints:   i  g x, y, z  = x2 + y2 + z2 = 1;  ii  g x, y, z  = x2 + y2 + z2 = 1 and h x, y, z  = x + y + z = 0.  Case  i . Since there is only one constraint in this case, we need only introduce a single Lagrange multiplier to obtain   f + λg  = 3x2 + 2λx = 0,   f + λg  = 3y2 + 2λy = 0,   5.32   ∂ ∂x ∂ ∂y ∂ ∂z   f + λg  = 3z2 + 2λz = 0.  These equations are highly symmetrical and clearly have the solution x = y = z = −2λ 3. Using the constraint x2 + y2 + z2 = 1 we ﬁnd λ = ±√ x = y = z = ± 1√  3 2 and so stationary points occur   5.33   at  .  3  In solving the three equations  5.32  in this way, however, we have implicitly assumed that x, y and z are non-zero. However, it is clear from  5.32  that any of these values can equal zero, with the exception of the case x = y = z = 0 since this is prohibited by the constraint x2 + y2 + z2 = 1. We must consider the other cases separately.  If x = 0, for example, we require  3y2 + 2λy = 0, 3z2 + 2λz = 0, y2 + z2 = 1.  Clearly, we require λ  cid:3 = 0, otherwise these equations are inconsistent. If neither y nor z is zero we ﬁnd y = −2λ 3 = z and from the third equation we require y = z = √ 2. If y = 0, however, then z = ±1 and, similarly, if z = 0 then y = ±1. Thus the ±1  √ √ 2, ±1  stationary points having x = 0 are  0, 0,±1 ,  0,±1, 0  and  0, ±1  2 . A similar √ √ procedure can be followed for the cases y = 0 and z = 0 respectively and, in addition 2, 0, ±1  to those already obtained, we ﬁnd the stationary points  ±1, 0, 0 ,  ±1  √ √ 2  and 2, ±1   ±1  2, 0 .  Case  ii . We now have two constraints and must therefore introduce two Lagrange  multipliers to obtain  cf.  5.31    ∂ ∂x ∂ ∂y ∂ ∂z   f + λg + µh  = 3x2 + 2λx + µ = 0,   f + λg + µh  = 3y2 + 2λy + µ = 0,   f + λg + µh  = 3z2 + 2λz + µ = 0.  3 x2 − y2  + 2λ x − y  = 0 ⇒ 3 x + y  x − y  + 2λ x − y  = 0.  170  These equations are again highly symmetrical and the simplest way to proceed is to subtract  5.35  from  5.34  to obtain  This equation is clearly satisﬁed if x = y; then, from the second constraint, x + y + z = 0,   5.34    5.35    5.36    5.37    5.9 STATIONARY VALUES UNDER CONSTRAINTS  we ﬁnd z = −2x. Substituting these values into the ﬁrst constraint, x2 + y2 + z2 = 1, we  obtain  x = ± 1√  ,  6  y = ± 1√  ,  6  z = ∓ 2√  .  6  Because of the high degree of symmetry amongst the equations  5.34 – 5.36 , we may obtain by inspection two further relations analogous to  5.37 , one containing the variables y, z and the other the variables x, z. Assuming y = z in the ﬁrst relation and x = z in the second, we ﬁnd the stationary points  and  x = ± 1√  ,  6  y = ∓ 2√  ,  6  z = ± 1√  6  x = ∓ 2√  ,  6  y = ± 1√  ,  6  z = ± 1√  .  6   5.38    5.39    5.40   We note that in ﬁnding the stationary points  5.38 – 5.40  we did not need to evaluate the Lagrange multipliers λ and µ explicitly. This is not always the case, however, and in some problems it may be simpler to begin by ﬁnding the values of these multipliers.  Returning to  5.37  we must now consider the case where x  cid:3 = y; then we ﬁnd  3 x + y  + 2λ = 0.   5.41   However, in obtaining the stationary points  5.39 ,  5.40 , we did not assume x = y but  only required y = z and x = z respectively. It is clear that x  cid:3 = y at these stationary points, for which x  cid:3 = z or y  cid:3 = z have already been found.  and it can be shown that they do indeed satisfy  5.41 . Similarly, several stationary points  Thus we need to consider further only two cases, x = y = z, and x, y and z are all diﬀerent. The ﬁrst is clearly prohibited by the constraint x + y + z = 0. For the second case,  5.41  must be satisﬁed, together with the analogous equations containing y, z and x, z respectively, i.e.  3 x + y  + 2λ = 0,  3 y + z  + 2λ = 0,  3 x + z  + 2λ = 0.  Adding these three equations together and using the constraint x + y + z = 0 we ﬁnd λ = 0. However, for λ = 0 the equations are inconsistent for non-zero x, y and z. Therefore all the stationary points have already been found and are given by  5.38 – 5.40 .  cid:2   The method may be extended to functions of any number n of variables subject to any smaller number m of constraints. This means that eﬀectively there  are n − m independent variables and, as mentioned above, we could solve by  substitution and then by the methods of the previous section. However, for large n this becomes cumbersome and the use of Lagrange undetermined multipliers is a useful simpliﬁcation.  171   PARTIAL DIFFERENTIATION   cid:1 A system contains a very large number N of particles, each of which can be in any of R energy levels with a corresponding energy Ei, i = 1, 2, . . . , R. The number of particles in the ith level is ni and the total energy of the system is a constant, E. Find the distribution of particles amongst the energy levels that maximises the expression  subject to the constraints that both the number of particles and the total energy remain constant, i.e.  P =  N!  n1!n2!··· nR!  ,  h = E − R cid:4   i=1  g = N − R cid:4   i=1  ni = 0  and  niEi = 0.  The way in which we proceed is as follows. In order to maximise P , we must minimise its denominator  since the numerator is ﬁxed . Minimising the denominator is the same as minimising the logarithm of the denominator, i.e.  Using Stirling’s approximation, ln  n!  ≈ n ln n − n, we ﬁnd that  f = ln  n1!n2!··· nR!  = ln  n1!  + ln  n2!  + ··· + ln  nR!  .  cid:30   f = n1 ln n1 + n2 ln n2 + ··· + nR ln nR −  n1 + n2 + ··· + nR    cid:31   R cid:4   i=1  =  ni ln ni  − N.  It has been assumed here that, for the desired distribution, all the ni are large. Thus, we now have a function f subject to two constraints, g = 0 and h = 0, and we can apply the Lagrange method, obtaining  cf.  5.31    + λ  + µ  = 0,  ∂f ∂n1 ∂f ∂n2  ∂g ∂n1 ∂g ∂n2  + λ  + µ  ∂h ∂n1 ∂h ∂n2  = 0,  ... = 0.  ∂f ∂nR  + λ  + µ  ∂g ∂nR  ∂h ∂nR  ∂f ∂nk  ∂g ∂nk  + λ  + µ  = 0,  ∂h ∂nk  + ln nk + λ −1  + µ −Ek  = 0,  nk nk  ln nk = µEk + λ − 1,  nk = C exp µEk.  172  Since all these equations are alike, we consider the general case  for k = 1, 2, . . . , R. Substituting the functions f, g and h into this relation we ﬁnd  which can be rearranged to give  and hence   5.10 ENVELOPES  R cid:4  R cid:4   k=1  k=1  C exp µEk = N  CEk exp µEk = E.  5.10 Envelopes  We now have the general form for the distribution of particles amongst energy levels, but in order to determine the two constants µ, C we recall that  and  This is known as the Boltzmann distribution and is a well-known result from statistical mechanics.  cid:2   As noted at the start of this chapter, many of the functions with which physicists, chemists and engineers have to deal contain, in addition to constants and one or more variables, quantities that are normally considered as parameters of the system under study. Such parameters may, for example, represent the capacitance of a capacitor, the length of a rod, or the mass of a particle – quantities that are normally taken as ﬁxed for any particular physical set-up. The corresponding variables may well be time, currents, charges, positions and velocities. However, the parameters could be varied and in this section we study the eﬀects of doing so; in particular we study how the form of dependence of one variable on another, typically y = y x , is aﬀected when the value of a parameter is changed in a smooth and continuous way. In eﬀect, we are making the parameter into an additional variable.  As a particular parameter, which we denote by α, is varied over its permitted range, the shape of the plot of y against x will change, usually, but not always, in a smooth and continuous way. For example, if the muzzle speed v of a shell ﬁred from a gun is increased through a range of values then its height–distance trajectories will be a series of curves with a common starting point that are essentially just magniﬁed copies of the original; furthermore the curves do not cross each other. However, if the muzzle speed is kept constant but θ, the angle of elevation of the gun, is increased through a series of values, the corresponding trajectories do not vary in a monotonic way. When θ has been increased beyond ◦ ◦ the trajectories then do cross some of the trajectories corresponding to θ < 45 . 45 ◦ all lie within a curve that touches each individual The trajectories for θ > 45 trajectory at one point. Such a curve is called the envelope to the set of trajectory solutions; it is to the study of such envelopes that this section is devoted.  For our general discussion of envelopes we will consider an equation of the form f = f x, y, α  = 0. A function of three Cartesian variables, f = f x, y, α , is deﬁned at all points in xyα-space, whereas f = f x, y, α  = 0 is a surface in this space. A plane of constant α, which is parallel to the xy-plane, cuts such  173   PARTIAL DIFFERENTIATION  P1  y  P  P2  f x, y, α1  = 0  f x, y, α1 + h  = 0  x  Figure 5.4 Two neighbouring curves in the xy-plane of the family f x, y, α  = 0 intersecting at P . For ﬁxed α1, the point P1 is the limiting position of P as  h → 0. As α1 is varied, P1 delineates the envelope of the family  broken line .  a surface in a curve. Thus diﬀerent values of the parameter α correspond to diﬀerent curves, which can be plotted in the xy-plane. We now investigate how the envelope equation for such a family of curves is obtained.  5.10.1 Envelope equations  Suppose f x, y, α1  = 0 and f x, y, α1 + h  = 0 are two neighbouring curves of a family for which the parameter α diﬀers by a small amount h. Let them intersect at the point P with coordinates x, y, as shown in ﬁgure 5.4. Then the envelope, indicated by the broken line in the ﬁgure, touches f x, y, α1  = 0 at the point P1,  which is deﬁned as the limiting position of P when α1 is ﬁxed but h → 0. The  full envelope is the curve traced out by P1 as α1 changes to generate successive members of the family of curves. Of course, for any ﬁnite h, f x, y, α1 + h  = 0 is one of these curves and the envelope touches it at the point P2.  We are now going to apply Rolle’s theorem, see subsection 2.1.10, with the parameter α as the independent variable and x and y ﬁxed as constants. In this context, the two curves in ﬁgure 5.4 can be thought of as the projections onto the xy-plane of the planar curves in which the surface f = f x, y, α  = 0 meets the planes α = α1 and α = α1 + h.  Along the normal to the page that passes through P , as α changes from α1 to α1 + h the value of f = f x, y, α  will depart from zero, because the normal meets the surface f = f x, y, α  = 0 only at α = α1 and at α = α1 + h. However, at these end points the values of f = f x, y, α  will both be zero, and therefore equal. This allows us to apply Rolle’s theorem and so to conclude that for some  θ in the range 0 ≤ θ ≤ 1 the partial derivative ∂f x, y, α1 + θh  ∂α is zero. When  174   5.10 ENVELOPES  h is made arbitrarily small, so that P → P1, the three deﬁning equations reduce  to two, which deﬁne the envelope point P1:  f x, y, α1  = 0  and   5.42   ∂f x, y, α1   = 0.  ∂α  In  5.42  both the function and the gradient are evaluated at α = α1. The equation of the envelope g x, y  = 0 is found by eliminating α1 between the two equations. As a simple example we will now solve the problem which when posed mathe- matically reads ‘calculate the envelope appropriate to the family of straight lines in the xy-plane whose points of intersection with the coordinate axes are a ﬁxed distance apart’. In more ordinary language, the problem is about a ladder leaning against a wall.   cid:1 A ladder of length L stands on level ground and can be leaned at any angle against a vertical wall. Find the equation of the curve bounding the vertical area below the ladder.  We take the ground and the wall as the x- and y-axes respectively. If the foot of the ladder is a from the foot of the wall and the top is b above the ground then the straight-line equation of the ladder is  x a  y b  +  = 1,  where a and b are connected by a2 + b2 = L2. Expressed in standard form with only one independent parameter, a, the equation becomes  f x, y, a  =  +  x a  y   L2 − a2 1 2  − 1 = 0.   5.43   Now, diﬀerentiating  5.43  with respect to a and setting the derivative ∂f ∂a equal to  zero gives  − x a2  +  ay   L2 − a2 3 2  = 0;  from which it follows that  Lx1 3  a =   x2 3 + y2 3 1 2  and  L2 − a2 1 2 =  Ly1 3   x2 3 + y2 3 1 2 .  Eliminating a by substituting these values into  5.43  gives, for the equation of the  envelope of all possible positions on the ladder,  x2 3 + y2 3 = L2 3.  This is the equation of an astroid  mentioned in exercise 2.19 , and, together with the wall and the ground, marks the boundary of the vertical area below the ladder.  cid:2   Other examples, drawn from both geometry and and the physical sciences, are considered in the exercises at the end of this chapter. The shell trajectory problem discussed earlier in this section is solved there, but in the guise of a question about the water bell of an ornamental fountain.  175   PARTIAL DIFFERENTIATION  5.11 Thermodynamic relations  Thermodynamic relations provide a useful set of physical examples of partial diﬀerentiation. The relations we will derive are called Maxwell’s thermodynamic relations. They express relationships between four thermodynamic quantities de- scribing a unit mass of a substance. The quantities are the pressure P , the volume V , the thermodynamic temperature T and the entropy S of the substance. These four quantities are not independent; any two of them can be varied independently, but the other two are then determined.  The ﬁrst law of thermodynamics may be expressed as  dU = T dS − P dV ,  where U is the internal energy of the substance. Essentially this is a conservation of energy equation, but we shall concern ourselves, not with the physics, but rather with the use of partial diﬀerentials to relate the four basic quantities discussed above. The method involves writing a total diﬀerential, dU say, in terms of the diﬀerentials of two variables, say X and Y , thus   cid:7    cid:8    cid:7    cid:8   dU =  ∂U ∂X  dX +  Y  ∂U ∂Y  dY ,  X  and then using the relationship   5.44    5.45   ∂2U ∂X∂Y  =  ∂2U ∂Y ∂X  to obtain the required Maxwell relation. The variables X and Y are to be chosen from P , V , T and S .  cid:1 Show that  ∂T  ∂V  S = − ∂P  ∂S  V .  Here the two variables that have to be held constant, in turn, happen to be those whose diﬀerentials appear on the RHS of  5.44 . And so, taking X as S and Y as V in  5.45 , we have  T dS − P dV = dU =  and ﬁnd directly that  cid:7    cid:8   ∂U ∂S  V  = T  and   cid:7    cid:8   ∂U ∂S   cid:7   cid:8   S  dS +  V   cid:7   ∂U ∂V   cid:8   ∂U ∂V  dV ,  S  = −P .  Diﬀerentiating the ﬁrst expression with respect to V and the second with respect to S , and using  we ﬁnd the Maxwell relation  cid:7   ∂2U ∂V ∂S   cid:8   =  ∂2U ∂S ∂V   cid:7   ,   cid:8   = −  ∂T ∂V  S  ∂P ∂S  .  cid:2   V  176   5.11 THERMODYNAMIC RELATIONS   cid:1 Show that  ∂S  ∂V  T =  ∂P  ∂T  V .  Thus, equating partial derivatives,  Similarly applying  5.45  to dU, we ﬁnd  Applying  5.45  to dS , with independent variables V and T , we ﬁnd  dU = T dS − P dV = T  cid:7   cid:8    cid:8    cid:7   dU =   cid:7   ∂U ∂V  ∂U ∂V  = T  T  ∂S ∂V  dV +  T  − P  and   cid:8    cid:13  cid:7   cid:8   ∂S ∂V  dV +  T   cid:7    cid:8    cid:14   ∂S ∂T  dT  V   cid:7   cid:8   − P dV .  ∂U ∂T   cid:7   V  dT .   cid:8   = T  V  ∂S ∂T   cid:7   cid:7    cid:8   cid:8   V  .  V  ,  ∂U ∂T  =  ∂ ∂V   cid:14    cid:8   ∂U ∂T   cid:8   cid:7   T  T  ∂S ∂T  = T  ∂2S ∂V ∂T  .  V  T   cid:7   cid:13   cid:8   T   cid:7   −  cid:7   i.e.   cid:8   ∂P ∂T   cid:8   V  =   cid:7   ∂ ∂V  ∂P ∂T  ∂S ∂V  =  T  .  cid:2   V  But, since  it follows that cid:7    cid:8   ∂2U ∂T ∂V  =  ∂2U ∂V ∂T  ,  ∂ ∂T  ∂U ∂V  ∂S ∂V  ∂2S ∂T ∂V  + T  T  Thus ﬁnally we get the Maxwell relation  The above derivation is rather cumbersome, however, and a useful trick that can simplify the working is to deﬁne a new function, called a potential. The internal energy U discussed above is one example of a potential but three others are commonly deﬁned and they are described below.   cid:1 Show that  ∂S  ∂V  T =  ∂P  ∂T  V by considering the potential U − S T . We ﬁrst consider the diﬀerential d U − S T  . From  5.5 , we obtain d U − S T   = dU − S dT − T dS = −S dT − P dV when use is made of  5.44 . We rewrite U − S T as F for convenience of notation; F is  called the Helmholtz potential. Thus   cid:7    cid:8   dF = −S dT − P dV ,  cid:7    cid:8   = −S  ∂F ∂T  V  and  = −P .  ∂F ∂V  T  and it follows that  Using these results together with  we can see immediately that  which is the same Maxwell relation as before.  cid:2   ∂2F ∂T ∂V   cid:8    cid:7   =  ∂2F ∂V ∂T   cid:7   ,   cid:8   ∂S ∂V  =  T  ∂P ∂T  ,  V  177   PARTIAL DIFFERENTIATION  Although the Helmholtz potential has other uses, in this context it has simply provided a means for a quick derivation of the Maxwell relation. The other Maxwell relations can be derived similarly by using two other potentials, the  enthalpy, H = U + P V , and the Gibbs free energy, G = U + P V − S T  see  exercise 5.25 .  5.12 Diﬀerentiation of integrals  We conclude this chapter with a discussion of the diﬀerentiation of integrals. Let us consider the indeﬁnite integral  cf. equation  2.30    Assuming that the second partial derivatives of F x, t  are continuous, we have  from which it follows immediately that  F x, t  =  f x, t  dt,  ∂F x, t   ∂t  = f x, t .  and so we can write   cid:13    cid:14   ∂2F x, t   ∂2F x, t   ∂t∂x  ∂x∂t  ,   cid:14   ∂ ∂t  ∂F x, t   ∂x  =  ∂ ∂x  ∂F x, t   ∂f x, t   ∂t  =  .  ∂x  Integrating this equation with respect to t then gives  Now consider the deﬁnite integral  ∂F x, t   ∂f x, t   =  ∂x  dt.  ∂x   cid:21  = F x, v  − F x, u ,  f x, t  dt  t=u  t=v  I x  =   5.46   where u and v are constants. Diﬀerentiating this integral with respect to x, and using  5.46 , we see that  dI x   ∂F x, v   dx   cid:21   cid:21   u  =  =  =   cid:21  − ∂F x, u  dt −  ∂x  ∂x v ∂f x, t   v  ∂f x, t   dt.  ∂x  ∂x  u ∂f x, t   dt  ∂x  This is Leibnitz’ rule for diﬀerentiating integrals, and basically it states that for   cid:21   =   cid:13   cid:21   178   constant limits of integration the order of integration and diﬀerentiation can be reversed.  In the more general case where the limits of the integral are themselves functions  of x, it follows immediately that  5.13 EXERCISES   cid:21  = F x, v x   − F x, u x  ,  f x, t  dt  t=u x   t=v x   I x  =  which yields the partial derivatives  Consequently  ∂I ∂v   cid:7    cid:8   = f x, v x  ,   cid:7    cid:8   = −f x, u x  .  ∂I ∂u   cid:21   dI dx  =  ∂I ∂v  dv dx  +  ∂I ∂u  du dx  +  ∂I ∂x  = f x, v x    = f x, v x    − f x, u x   − f x, u x    du dx  du dx  +  +  dv dx  dv dx   cid:21   ∂ ∂x  v x   f x, t dt  u x  ∂f x, t   v x   u x   ∂x  dt,  where the partial derivative with respect to x in the last term has been taken inside the integral sign using  5.46 . This procedure is valid because u x  and v x  are being held constant in this term.  cid:1 Find the derivative with respect to x of the integral   5.47   Applying  5.47 , we see that   cid:21   I x  =  x2  sin xt  x  t  dt.   cid:21   t cos xt  dt  t  dI dx  =  =  sin x3  x2  2 sin x3  x   1  +   cid:13   sin xt  x  x2   cid:14   x  x2  x  x   2x  − sin x2 − sin x2 − 2  x sin x2  +  sin x3  3 sin x3 − 2 sin x2 .  cid:2   x  x  = 3  =  1 x  5.1  Using the appropriate properties of ordinary derivatives, perform the following.  5.13 Exercises  179   PARTIAL DIFFERENTIATION   a  Find all the ﬁrst partial derivatives of the following functions f x, y :   i  x2y,  ii  x2 + y2 + 4,  iii  sin x y ,  iv  tan  v  r x, y, z  =  x2 + y2 + z2 1 2.   b  For  i ,  ii  and  v , ﬁnd ∂2f ∂x2, ∂2f ∂y2 and ∂2f ∂x∂y.  c  For  iv  verify that ∂2f ∂x∂y = ∂2f ∂y∂x.  −1 y x ,  5.2  Determine which of the following are exact diﬀerentials:   3x + 2 y dx + x x + 1  dy;   a   b  y tan x dx + x tan y dy;  c  y2 ln x + 1  dx + 2xy ln x dy;  d  y2 ln x + 1  dy + 2xy ln x dx;  e   [x  x2 + y2 ] dy − [y  x2 + y2 ] dx.  5.3  Show that the diﬀerential  5.4  is not exact, but that dg =  xy2  Show that  df = x2 dy −  y2 + xy  dx  −1df is exact.  df = y 1 + x − x2  dx + x x + 1  dy  is not an exact diﬀerential.  5.5  Find the diﬀerential equation that a function g x  must satisfy if dφ = g x df −x is a solution of this equation  is to be an exact diﬀerential. Verify that g x  = e and deduce the form of φ x, y . The equation 3y = z3 + 3xz deﬁnes z implicitly as a function of x and y. Evaluate all three second partial derivatives of z with respect to x and or y. Verify that z is a solution of  x  ∂2z ∂y2  +  = 0.  − α  ∂2z ∂x2   cid:9   cid:8    cid:10   cid:7   5.6  A possible equation of state for a gas takes the form   cid:7    cid:8   P V = RT exp  in which α and R are constants. Calculate expressions for   cid:8  and show that their product is −1, as stated in section 5.4.  ∂V ∂T  ∂T ∂P   cid:7   ∂P ∂V  V RT  T  V  P  ,  ,  ,  ,  5.7  The function G t  is deﬁned by  G t  = F x, y  = x2 + y2 + 3xy,  where x t  = at2 and y t  = 2at. Use the chain rule to ﬁnd the values of  x, y  at which G t  has stationary values as a function of t. Do any of them correspond to the stationary points of F x, y  as a function of x and y? In the xy-plane, new coordinates s and t are deﬁned by  5.8  Transform the equation  s = 1  2  x + y ,  2  x − y .  t = 1  ∂2φ ∂x2  − ∂2φ ∂y2  = 0  into the new coordinates and deduce that its general solution can be written  φ x, y  = f x + y  + g x − y ,  where f u  and g v  are arbitrary functions of u and v, respectively.  180   5.9  The function f x, y  satisﬁes the diﬀerential equation  5.13 EXERCISES  y  ∂f ∂x  + x  = 0.  ∂f ∂y  By changing to new variables u = x2 − y2 and v = 2xy, show that f is, in fact, a function of x2 − y2 only.  5.10  If x = eu cos θ and y = eu sin θ, show that   cid:7    cid:8   ∂2φ ∂u2  +  ∂2φ ∂θ2  =  x2 + y2   ∂2f ∂x2  +  ∂2f ∂y2  ,  5.11  where f x, y  = φ u, θ . Find and evaluate the maxima, minima and saddle points of the function  f x, y  = xy x2 + y2 − 1 .  5.12  Show that  f x, y  = x3 − 12xy + 48x + by2,  b  cid:3 = 0,  has two, one, or zero stationary points, according to whether b is less than,  5.13  equal to, or greater than 3. Locate the stationary points of the function  f x, y  =  x2 − 2y2  exp[− x2 + y2  a2],  where a is a non-zero constant.  Sketch the function along the x- and y-axes and hence identify the nature and  values of the stationary points. Find the stationary points of the function  f x, y  = x3 + xy2 − 12x − y2  and identify their natures. Find the stationary values of  f x, y  = 4x2 + 4y2 + x4 − 6x2y2 + y4  and classify them as maxima, minima or saddle points. Make a rough sketch of  the contours of f in the quarter plane x, y ≥ 0.  5.16  The temperature of a point  x, y, z  on the unit sphere is given by  By using the method of Lagrange multipliers, ﬁnd the temperature of the hottest point on the sphere. A rectangular parallelepiped has all eight vertices on the ellipsoid  5.17  T  x, y, z  = 1 + xy + yz.  x2 + 3y2 + 3z2 = 1.  Using the symmetry of the parallelepiped about each of the planes x = 0, y = 0, z = 0, write down the surface area of the parallelepiped in terms of  the coordinates of the vertex that lies in the octant x, y, z ≥ 0. Hence ﬁnd the Two horizontal corridors, 0 ≤ x ≤ a with y ≥ 0, and 0 ≤ y ≤ b with x ≥ 0, meet  maximum value of the surface area of such a parallelepiped.  at right angles. Find the length L of the longest ladder  considered as a stick  that may be carried horizontally around the corner. A barn is to be constructed with a uniform cross-sectional area A throughout its length. The cross-section is to be a rectangle of wall height h  ﬁxed  and width w, surmounted by an isosceles triangular roof that makes an angle θ with  181  5.14  5.15  5.18  5.19   5.20  5.21  5.22  5.23  5.24  PARTIAL DIFFERENTIATION  the horizontal. The cost of construction is α per unit height of wall and β per unit  slope  length of roof. Show that, irrespective of the values of α and β, to minimise costs w should be chosen to satisfy the equation  w4 = 16A A − wh ,  and θ made such that 2 tan 2θ = w h. Show that the envelope of all concentric ellipses that have their axes along the x- and y-coordinate axes, and that have the sum of their semi-axes equal to a constant L, is the same curve  an astroid  as that found in the worked example in section 5.10. Find the area of the region covered by points on the lines  x a  y b  +  = 1,  r = a 1 + cos θ .  where the sum of any line’s intercepts on the coordinate axes is ﬁxed and equal to c. Prove that the envelope of the circles whose diameters are those chords of a given circle that pass through a ﬁxed point on its circumference, is the cardioid  Here a is the radius of the given circle and  r, θ  are the polar coordinates of the envelope. Take as the system parameter the angle φ between a chord and the polar axis from which θ is measured. A water feature contains a spray head at water level at the centre of a round basin. The head is in the form of a small hemisphere perforated by many evenly distributed small holes, through which water spurts out at the same speed, v0, in all directions.   a  What is the shape of the ‘water bell’ so formed?  b  What must be the minimum diameter of the bowl if no water is to be lost?  In order to make a focussing mirror that concentrates parallel axial rays to one spot  or conversely forms a parallel beam from a point source , a parabolic shape should be adopted. If a mirror that is part of a circular cylinder or sphere were used, the light would be spread out along a curve. This curve is known as a caustic and is the envelope of the rays reﬂected from the mirror. Denoting by θ the angle which a typical incident axial ray makes with the normal to the mirror at the place where it is reﬂected, the geometry of reﬂection  the angle of incidence equals the angle of reﬂection  is shown in ﬁgure 5.5.  Show that a parametric speciﬁcation of the caustic is   cid:5    cid:6   x = R cos θ  1  2 + sin2 θ  ,  y = R sin3 θ,  where R is the radius of curvature of the mirror. The curve is, in fact, part of an epicycloid. By considering the diﬀerential  5.25  where G is the Gibbs free energy, P the pressure, V the volume, S the entropy and T the temperature of a system, and given further that the internal energy U satisﬁes  derive a Maxwell relation connecting  ∂V  ∂T  P and  ∂S  ∂P  T .  dG = d U + P V − S T  ,  dU = T dS − P dV ,  182   5.13 EXERCISES  y  O  θ  θ  R  2θ  x  Figure 5.5 The reﬂecting mirror discussed in exercise 5.24.  5.26  Functions P  V , T  , U V , T   and S  V , T   are related by  where the symbols have the same meaning as in the previous question. The pressure P is known from experiment to have the form  T dS = dU + P dV ,  P =  T 4 3  +  T V  ,  U = αV T 4 + βT ,  in appropriate units. If  where α, β, are constants  or, at least, do not depend on T or V  , deduce that α must have a speciﬁc value, but that β may have any value. Find the corresponding form of S . As in the previous two exercises on the thermodynamics of a simple gas, the quantity dS = T  −1 dU + P dV   is an exact diﬀerential. Use this to prove that   cid:7    cid:8    cid:7    cid:8   In the van der Waals model of a gas, P obeys the equation  ∂U ∂V  = T  T  ∂P ∂T  V  − P .  P =  RT  V − b  − a V 2 ,  where R, a and b are constants. Further, in the limit V → ∞, the form of U  becomes U = cT , where c is another constant. Find the complete expression for U V , T  . The entropy S  H, T  , the magnetisation M H, T   and the internal energy U H, T   of a magnetic salt placed in a magnetic ﬁeld of strength H, at temperature T , are connected by the equation  5.27  5.28  T dS = dU − HdM.  183   PARTIAL DIFFERENTIATION   cid:7  By considering d U − T S − HM  prove that   cid:8    cid:7    cid:8   ∂M ∂T  =  H  ∂S ∂H  .  T  For a particular salt,  M H, T   = M0[1 − exp −αH T  ].  Show that if, at a ﬁxed temperature, the applied ﬁeld is increased from zero to a strength such that the magnetization of the salt is 3 4 M0, then the salt’s entropy decreases by an amount  5.29  Using the results of section 5.12, evaluate the integral  Hence show that  5.30  The integral  has the value  π α 1 2. Use this result to evaluate −x2  J n  =  −∞ x2ne  dx,  5.31  where n is a positive integer. Express your answer in terms of factorials. The function f x  is diﬀerentiable and f 0  = 0. A second function g y  is deﬁned by  I y  =  J =  −xy sin x  dx.  dx =  π 2  .  0  e  x  M0 4α   3 − ln 4 .  cid:21  ∞  cid:21  ∞  cid:21  ∞  cid:21  ∞  −∞ e  −αx2  sin x  dx  x  0   cid:21   √ f x  dx y − x  .  y  0  g y  =   cid:21   dg dy  =  dx√ y − x  .  df dx  y  0  √  = 2 n!   y.  dng dyn  f x, t  = e  F x  =  f x, t  dt.  x   cid:21  −xt,  cid:21   0  Prove that  For the case f x  = xn, prove that  5.32  The functions f x, t  and F x  are deﬁned by  Verify, by explicit calculation, that  dF dx  = f x, x  +  x  ∂f x, t   0  ∂x  dt.  184   5.14 HINTS AND ANSWERS   cid:21   I α  =  xα − 1  1  0  ln x  dx,  α > −1,  5.33  If  what is the value of I 0 ? Show that  and deduce that  d dα Hence prove that I α  = ln 1 + α . Find the derivative, with respect to x, of the integral  I α  =  α + 1  .  1  5.34  5.35  The function G t, ξ  is deﬁned for 0 ≤ t ≤ π by  G t, ξ  =  for ξ ≤ t,  for ξ > t.  Show that the function x t  deﬁned by π  satisﬁes the equation  d dα  xα = xα ln x,  I x  =  exp xt dt.   cid:21   3x  x    − cos t sin ξ − sin t cos ξ  cid:21   x t  =  G t, ξ f ξ  dξ  0  d2x dt2  + x = f t ,  where f t  can be any arbitrary  continuous  function. Show further that x 0  = [dx dt]t=π = 0, again for any f t , but that the value of x π  does depend upon the form of f t .  [ The function G t, ξ  is an example of a Green’s function, an important concept in the solution of diﬀerential equations and one studied extensively in later chapters. ]  5.14 Hints and answers   a    i  2xy, x2;  ii  2x, 2y;  iii  y  iv  −y  x2 + y2 , x  x2 + y2 ;  v  x r, y r, z r. −3,  x2 + z2 r  −1 cos x y ,  −x y2  cos x y ; −3,−xyr −3.  b   i  2y, 0, 2x;  ii  2, 2, 0;  v   y2 + z2 r  c  Both second derivatives are equal to  y2 − x2  x2 + y2  −2. 2x  cid:3 = −2y − x. For g, both sides of equation  5.9  equal y −2. ∂2z ∂x2 = 2xz z2 + x   0, 0 ,  a 4,−a  and  16a,−8a . Only the saddle point at  0, 0 . The transformed equation is 2 x2 + y2 ∂f ∂v = 0; hence f does not depend on v. Maxima, equal to 1 8, at ± 1 2,−1 2 , minima, equal to −1 8, at ± 1 2, 1 2 , saddle points, equalling 0, at  0, 0 ,  0,±1 ,  ±1, 0 . −1 at  0,±a , saddle Maxima equal to a2e Minimum at  0, 0 ; saddle points at  ±1,±1 . To help with sketching the contours, point equalling 0 at  0, 0 .  −1 at  ±a, 0 , minima equal to −2a2e  −3, ∂2z ∂x∂y =  z2 − x  z2 + x   −3, ∂2z ∂y2 = −2z z2 + x   −3.  determine the behaviour of g x  = f x, x . The Lagrange multiplier method gives z = y = x 2, for a maximal area of 4.  5.1  5.3 5.5 5.7 5.9 5.11  5.13  5.15  5.17  185   PARTIAL DIFFERENTIATION  5.19  5.21  5.23  5.25  5.27  5.29  5.31  5.33 5.35  x +  √  √  y =  to the stated results.  0   gρ . The water bell has a parabolic proﬁle z = v2  equation is f z, ρ, α  = z−ρα+[gρ2 1+α2   2v2 α = v2  The cost always includes 2αh, which can therefore be ignored in the optimisation. 2 λw tan θ = λh, leading √ c.  With Lagrange multiplier λ, sin θ = λw  4β  and β sec θ− 1 The envelope of the lines x a + y  c− a − 1 = 0, as a is varied, is Area = c2 6.  a  Using α = cot θ, where θ is the initial angle a jet makes with the vertical, the 0  2g  − gρ2  2v2 0 ], and setting ∂f ∂α = 0 gives 0 .  b  Setting z = 0 gives the minimum diameter as 2v2 Show that  ∂G ∂P  T = V and  ∂G ∂T  P = −S . From each result, obtain an expression for ∂2G ∂T ∂P and equate these, giving  ∂V  ∂T  P = − ∂S  ∂P  T . Find expressions for  ∂S  ∂V  T and  ∂S  ∂T  V , and equate ∂2S  ∂V ∂T with ∂2S  ∂T ∂V . U V , T   = cT − aV dI dy = −Im[ I ∞  = 0 and I 0  = J.  cid:1   Integrate the RHS of the equation by parts, before diﬀerentiating with respect to y. Repeated application of the method establishes the result for all orders of derivative. I 0  = 0; use Leibnitz’ rule.   cid:1  ∞ 0 exp −xy + ix  dx] = −1  1 + y2 . Integrate dI dy from 0 to ∞.  Write x t  = − cos t t cos ξ f ξ  dξ and diﬀerentiate each term as a product to obtain dx dt. Obtain d2x dt2 in a similar way. Note that integrals that have equal lower and upper limits have value zero. The value of x π  is  0 sin ξ f ξ  dξ − sin t  −1.  0 g.   cid:1    cid:1   π 0 sin ξ f ξ  dξ.  π  t  186   6  Multiple integrals  For functions of several variables, just as we may consider derivatives with respect to two or more of them, so may the integral of the function with respect to more than one variable be formed. The formal deﬁnitions of such multiple integrals are extensions of that for a single variable, discussed in chapter 2. We ﬁrst discuss double and triple integrals and illustrate some of their applications. We then consider changing the variables in multiple integrals and discuss some general properties of Jacobians.  6.1 Double integrals  For an integral involving two variables – a double integral – we have a function, f x, y  say, to be integrated with respect to x and y between certain limits. These limits can usually be represented by a closed curve C bounding a region R in the xy-plane. Following the discussion of single integrals given in chapter 2, let us divide the region R into N subregions ∆Rp of area ∆Ap, p = 1, 2, . . . , N, and let  xp, yp  be any point in subregion ∆Rp. Now consider the sum  S =  f xp, yp ∆Ap,  and let N → ∞ as each of the areas ∆Ap → 0. If the sum S tends to a unique  limit, I, then this is called the double integral of f x, y  over the region R and is written  I =  f x, y  dA,   6.1   where dA stands for the element of area in the xy-plane. By choosing the subregions to be small rectangles each of area ∆A = ∆x∆y, and letting both ∆x  N cid:4   p=1   cid:21   R  187   MULTIPLE INTEGRALS  V  dy  U  dx  dA = dxdy  R  C  T  b  x  y  d  c  S  a   cid:21  cid:21   R  Figure 6.1 A simple curve C in the xy-plane, enclosing a region R.  and ∆y → 0, we can also write the integral as  I =  f x, y  dx dy,   6.2   where we have written out the element of area explicitly as the product of the two coordinate diﬀerentials  see ﬁgure 6.1 .  Some authors use a single integration symbol whatever the dimension of the integral; others use as many symbols as the dimension. In diﬀerent circumstances both have their advantages. We will adopt the convention used in  6.1  and  6.2 , that as many integration symbols will be used as diﬀerentials explicitly written.  The form  6.2  gives us a clue as to how we may proceed in the evaluation of a double integral. Referring to ﬁgure 6.1, the limits on the integration may be written as an equation c x, y  = 0 giving the boundary curve C. However, an explicit statement of the limits can be written in two distinct ways.  One way of evaluating the integral is ﬁrst to sum up the contributions from the small rectangular elemental areas in a horizontal strip of width dy  as shown in the ﬁgure  and then to combine the contributions of these horizontal strips to cover the region R. In this case, we write   cid:21    cid:12  cid:21   y=d  x=x2 y   y=c  x=x1 y    cid:15   I =  f x, y  dx  dy,   6.3   where x = x1 y  and x = x2 y  are the equations of the curves T S V and T UV respectively. This expression indicates that ﬁrst f x, y  is to be integrated with respect to x  treating y as a constant  between the values x = x1 y  and x = x2 y  and then the result, considered as a function of y, is to be integrated between the limits y = c and y = d. Thus the double integral is evaluated by expressing it in terms of two single integrals called iterated  or repeated  integrals.  188   6.1 DOUBLE INTEGRALS  An alternative way of evaluating the integral, however, is ﬁrst to sum up the contributions from the elemental rectangles arranged into vertical strips and then to combine these vertical strips to cover the region R. We then write   cid:15   I =  f x, y  dy  dx,   6.4    cid:21    cid:12  cid:21   x=b  y=y2 x   x=a  y=y1 x   where y = y1 x  and y = y2 x  are the equations of the curves S T U and S V U respectively. In going to  6.4  from  6.3 , we have essentially interchanged the order of integration.  In the discussion above we assumed that the curve C was such that any line parallel to either the x- or y-axis intersected C at most twice. In general, provided f x, y  is continuous everywhere in R and the boundary curve C has this simple shape, the same result is obtained irrespective of the order of integration. In cases where the region R has a more complicated shape, it can usually be subdivided into smaller simpler regions R1, R2 etc. that satisfy this criterion. The double integral over R is then merely the sum of the double integrals over the subregions.  cid:1 Evaluate the double integral  where R is the triangular area bounded by the lines x = 0, y = 0 and x + y = 1. Reverse the order of integration and demonstrate that the same result is obtained.  The area of integration is shown in ﬁgure 6.2. Suppose we choose to carry out the  integration with respect to y ﬁrst. With x ﬁxed, the range of y is 0 to 1 − x. We can  therefore write  I =  x2y dx dy,   cid:21  cid:21   R   cid:15    cid:21   cid:21    cid:21   cid:21    cid:12  cid:21   cid:13    cid:12  cid:21   cid:13   I =  =  x=1  x=0  x=1  x=0  x2y dy  y=1−x   cid:14   y=0 x2y2  2  y=1−x  y=0  dx =  dx   cid:21   0  x2 1 − x 2  1  dx =  1 60  .  I =  =  y=1  y=0  y=1  y=0  x=1−y   cid:14   x=1−y  x2y dx  dx =  x=0 x3y 3  dy   cid:21   1  0   1 − y 3y  dy =  1 60  .  2  3  x=0   cid:21   b  a   cid:21   y2 x   y1 x   189  I =  dx  dy f x, y ,  As expected, we obtain the same result irrespective of the order of integration.  cid:2   We may avoid the use of braces in expressions such as  6.3  and  6.4  by writing   6.4 , for example, as  where it is understood that each integral symbol acts on everything to its right,  Alternatively, we may choose to perform the integration with respect to x ﬁrst. With y   cid:15  ﬁxed, the range of x is 0 to 1 − y, so we have   MULTIPLE INTEGRALS  y  1  dy  R  0  0  x + y = 1  dx  1  x  Figure 6.2 The triangular region whose sides are the axes x = 0, y = 0 and the line x + y = 1.  and that the order of integration is from right to left. So, in this example, the integrand f x, y  is ﬁrst to be integrated with respect to y and then with respect to x. With the double integral expressed in this way, we will no longer write the independent variables explicitly in the limits of integration, since the diﬀerential of the variable with respect to which we are integrating is always adjacent to the relevant integral sign.  Using the order of integration in  6.3 , we could also write the double integral as   cid:21   d  c   cid:21   x2 y   x1 y   I =  dy  dx f x, y .  Occasionally, however, interchange of the order of integration in a double integral is not permissible, as it yields a diﬀerent result. For example, diﬃculties might arise if the region R were unbounded with some of the limits inﬁnite, though in many cases involving inﬁnite limits the same result is obtained whichever order of integration is used. Diﬃculties can also occur if the integrand f x, y  has any discontinuities in the region R or on its boundary C.  The above discussion for double integrals can easily be extended to triple integrals. Consider the function f x, y, z  deﬁned in a closed three-dimensional region R. Proceeding as we did for double integrals, let us divide the region R into N subregions ∆Rp of volume ∆Vp, p = 1, 2, . . . , N, and let  xp, yp, zp  be any point in the subregion ∆Rp. Now we form the sum  6.2 Triple integrals  S =  f xp, yp, zp ∆Vp,  N cid:4   p=1  190   6.3 APPLICATIONS OF MULTIPLE INTEGRALS   cid:21   R  R   cid:21  cid:21  cid:21   and let N → ∞ as each of the volumes ∆Vp → 0. If the sum S tends to a unique  limit, I, then this is called the triple integral of f x, y, z  over the region R and is written  I =  f x, y, z  dV ,   6.5   where dV stands for the element of volume. By choosing the subregions to be small cuboids, each of volume ∆V = ∆x∆y∆z, and proceeding to the limit, we can also write the integral as  I =  f x, y, z  dx dy dz,   6.6   where we have written out the element of volume explicitly as the product of the three coordinate diﬀerentials. Extending the notation used for double integrals, we may write triple integrals as three iterated integrals, for example,   cid:21    cid:21    cid:21   x2  y2 x   z2 x,y   I =  dx  dy  dz f x, y, z ,  x1  y1 x   z1 x,y   where the limits on each of the integrals describe the values that x, y and z take on the boundary of the region R. As for double integrals, in most cases the order of integration does not aﬀect the value of the integral.  We can extend these ideas to deﬁne multiple integrals of higher dimensionality  in a similar way.  6.3 Applications of multiple integrals  Multiple integrals have many uses in the physical sciences, since there are numer- ous physical quantities which can be written in terms of them. We now discuss a few of the more common examples.  Multiple integrals are often used in ﬁnding areas and volumes. For example, the integral  6.3.1 Areas and volumes   cid:21    cid:21  cid:21   A =  dA =  dx dy  R  R  is simply equal to the area of the region R. Similarly, if we consider the surface z = f x, y  in three-dimensional Cartesian coordinates then the volume under this surface that stands vertically above the region R is given by the integral   cid:21   V =  z dA =  f x, y  dx dy,  R  R  where volumes above the xy-plane are counted as positive, and those below as negative.   cid:21  cid:21   191   MULTIPLE INTEGRALS  z  c  dV = dx dy dz  b  y  dx  dz  dy  a  x  Figure 6.3 The tetrahedron bounded by the coordinate surfaces and the plane x a + y b + z c = 1 is divided up into vertical slabs, the slabs into columns and the columns into small boxes.   cid:1 Find the volume of the tetrahedron bounded by the three coordinate surfaces x = 0, y = 0 and z = 0 and the plane x a + y b + z c = 1.  Referring to ﬁgure 6.3, the elemental volume of the shaded region is given by dV = z dx dy, and we must integrate over the triangular region R in the xy-plane whose sides are x = 0,  y = 0 and y = b − bx a. The total volume of the tetrahedron is therefore given by   cid:21  cid:21   R  V =  z dx dy =  dx  dy c   cid:21    cid:21   a  0   cid:21   cid:21   a  a  0  0  = c  dx  = c  dx   cid:9    cid:10   0  b−bx a   cid:13   cid:7  y − y2  − x b a y=b−bx a   cid:14  1 − y  cid:8   a  y=0  − xy  2b − bx  a  bx2 2a2  +  b 2  =  .  cid:2   abc 6   cid:21    cid:21  cid:21  cid:21   Alternatively, we can write the volume of a three-dimensional region R as  V =  dV =  dx dy dz,  R  R   6.7   where the only diﬃculty occurs in setting the correct limits on each of the integrals. For the above example, writing the volume in this way corresponds to dividing the tetrahedron into elemental boxes of volume dx dy dz  as shown in ﬁgure 6.3 ; integration over z then adds up the boxes to form the shaded column , and in the ﬁgure. The limits of integration are z = 0 to z = c   cid:5   cid:6  1 − y b − x a  192   6.3 APPLICATIONS OF MULTIPLE INTEGRALS  the total volume of the tetrahedron is given by   cid:21   a  0   cid:21   0  V =  dx   cid:21   dy  0  b−bx a  c 1−y b−x a   dz,   6.8   which clearly gives the same result as above. This method is illustrated further in the following example.  cid:1 Find the volume of the region bounded by the paraboloid z = x2 + y2 and the plane z = 2y.   cid:24   The required region is shown in ﬁgure 6.4. In order to write the volume of the region in the form  6.7 , we must deduce the limits on each of the integrals. Since the integrations can be performed in any order, let us ﬁrst divide the region into vertical slabs of thickness dy perpendicular to the y-axis, and then as shown in the ﬁgure we cut each slab into horizontal strips of height dz, and each strip into elemental boxes of volume dV = dx dy dz. Integrating ﬁrst with respect to x  adding up the elemental boxes to get a horizontal strip ,  the limits on x are x = − cid:24   cid:21   cid:18   z − y2. Now integrating with respect to z z − y2 to x =  adding up the strips to form a vertical slab  the limits on z are z = y2 to z = 2y. Finally, integrating with respect to y  adding up the slabs to obtain the required region , the limits  cid:21   cid:21  √ on y are y = 0 and y = 2, the solutions of the simultaneous equations z = 02 + y2 and z = 2y. So the volume of the region is z−y2  cid:21  √ z−y2 3  z − y2 3 2  3  2y − y2 3 2.  z − y2   cid:21   cid:21    cid:24   dx =   cid:21   dy 4  z=2y z=y2 =  V =  dz 2   cid:19   dy  dy  dy  dz  =  −  y2  y2  2y  2y  0  4  2  0  2  2  2  0  0  The integral over y may be evaluated straightforwardly by making the substitution y = 1 + sin u, and gives V = π 2.  cid:2   In general, when calculating the volume  area  of a region, the volume  area  elements need not be small boxes as in the previous example, but may be of any convenient shape. The latter is usually chosen to make evaluation of the integral as simple as possible.  6.3.2 Masses, centres of mass and centroids  It is sometimes necessary to calculate the mass of a given object having a non- uniform density. Symbolically, this mass is given simply by  where dM is the element of mass and the integral is taken over the extent of the object. For a solid three-dimensional body the element of mass is just dM = ρ dV , where dV is an element of volume and ρ is the variable density. For a laminar body  i.e. a uniform sheet of material  the element of mass is dM = σ dA, where σ is the mass per unit area of the body and dA is an area element. Finally, for a body in the form of a thin wire we have dM = λ ds, where λ is the mass per  M =  dM,   cid:21   193   MULTIPLE INTEGRALS  z  z = 2y  z = x2 + y2  x  0  2  y  dV = dx dy dz  Figure 6.4 The region bounded by the paraboloid z = x2 + y2 and the plane z = 2y is divided into vertical slabs, the slabs into horizontal strips and the strips into boxes.  unit length and ds is an element of arc length along the wire. When evaluating the required integral, we are free to divide up the body into mass elements in the most convenient way, provided that over each mass element the density is approximately constant.  cid:1  Find the mass of the tetrahedron bounded by the three coordinate surfaces and the plane x a + y b + z c = 1, if its density is given by ρ x, y, z  = ρ0 1 + x a .  From  6.8 , we can immediately write down the mass of the tetrahedron as   cid:9    cid:21   R   cid:10   x a   cid:21   a  0  M =  ρ0  1 +  dV =  dx ρ0  1 +   cid:10  cid:21   x a  0  b−bx a  c 1−y b−x a   dy  dz,  where we have taken the density outside the integrations with respect to z and y since it depends only on x. Therefore the integrations with respect to z and y proceed exactly as they did when ﬁnding the volume of the tetrahedron, and we have   cid:9    cid:10  cid:7    cid:21   a  0  bx2 2a2  − bx  a  +  b 2  M = cρ0  dx  1 +   6.9    cid:21    cid:8   0  .  We could have arrived at  6.9  more directly by dividing the tetrahedron into triangular slabs of thickness dx perpendicular to the x-axis  see ﬁgure 6.3 , each of which is of constant density, since ρ depends on x alone. A slab at a position x has volume dV = 1  2 c 1 − x a  b − bx a  dx and mass dM = ρ dV = ρ0 1 + x a  dV . Integrating over x we  again obtain  6.9 . This integral is easily evaluated and gives M = 5  24 abcρ0.  cid:2    cid:9   x a  194   6.3 APPLICATIONS OF MULTIPLE INTEGRALS  The coordinates of the centre of mass of a solid or laminar body may also be written as multiple integrals. The centre of mass of a body has coordinates ¯x, ¯y, ¯z given by the three equations   cid:21   cid:21   cid:21   ¯x  ¯y  ¯z   cid:21   cid:21   cid:21   dM =  x dM  dM =  y dM  dM =  z dM,  where again dM is an element of mass as described above, x, y, z are the coordinates of the centre of mass of the element dM and the integrals are taken over the entire body. Obviously, for any body that lies entirely in, or is symmetrical about, the xy-plane  say , we immediately have ¯z = 0. For completeness, we note that the three equations above can be written as the single vector equation  see chapter 7    cid:21   ¯r =  1 M  r dM,   cid:1   where ¯r is the position vector of the body’s centre of mass with respect to the origin, r is the position vector of the centre of mass of the element dM and M = dM is the total mass of the body. As previously, we may divide the body into the most convenient mass elements for evaluating the necessary integrals, provided each mass element is of constant density.  We further note that the coordinates of the centroid of a body are deﬁned as  those of its centre of mass if the body had uniform density.  cid:1 Find the centre of mass of the solid hemisphere bounded by the surfaces x2 + y2 + z2 = a2 and the xy-plane, assuming that it has a uniform density ρ.  Referring to ﬁgure 6.5, we know from symmetry that the centre of mass must lie on the z-axis. Let us divide the hemisphere into volume elements that are circular slabs of thickness dz parallel to the xy-plane. For a slab at a height z, the mass of the element is  dM = ρ dV = ρπ a2− z2  dz. Integrating over z, we ﬁnd that the z-coordinate of the centre  of mass of the hemisphere is given by  a  ρπ a2 − z2  dz =  a  zρπ a2 − z2  dz.   cid:21   ¯z  0  The integrals are easily evaluated and give ¯z = 3a 8. Since the hemisphere is of uniform density, this is also the position of its centroid.  cid:2   6.3.3 Pappus’ theorems  The theorems of Pappus  which are about seventeen centuries old  relate centroids to volumes of revolution and areas of surfaces, discussed in chapter 2, and may be useful for ﬁnding one quantity given another that can be calculated more easily.   cid:21   0  195   Figure 6.5 The solid hemisphere bounded by the surfaces x2 + y2 + z2 = a2 and the xy-plane.  a  x  y  MULTIPLE INTEGRALS  z  a  √ a2 − z2  dz  a  y  x  A  dA  y  ¯y  Figure 6.6 An area A in the xy-plane, which may be rotated about the x-axis to form a volume of revolution.  If a plane area is rotated about an axis that does not intersect it then the solid so generated is called a volume of revolution. Pappus’ ﬁrst theorem states that the volume of such a solid is given by the plane area A multiplied by the distance moved by its centroid  see ﬁgure 6.6 . This may be proved by considering the deﬁnition of the centroid of the plane area as the position of the centre of mass if the density is uniform, so that  Now the volume generated by rotating the plane area about the x-axis is given by   cid:21   y dA.  1 A  ¯y =   cid:21   196  V =  2πy dA = 2π¯yA,  which is the area multiplied by the distance moved by the centroid.   6.3 APPLICATIONS OF MULTIPLE INTEGRALS  y  ds  y  ¯y  x  Figure 6.7 A curve in the xy-plane, which may be rotated about the x-axis to form a surface of revolution.  Pappus’ second theorem states that if a plane curve is rotated about a coplanar axis that does not intersect it then the area of the surface of revolution so generated is given by the length of the curve L multiplied by the distance moved by its centroid  see ﬁgure 6.7 . This may be proved in a similar manner to the ﬁrst theorem by considering the deﬁnition of the centroid of a plane curve,   cid:21   1 L  ¯y =  y ds,   cid:21   S =  2πy ds = 2π¯yL,  and noting that the surface area generated is given by  which is equal to the length of the curve multiplied by the distance moved by its centroid.   cid:1  A semicircular uniform lamina is freely suspended from one of its corners. Show that its straight edge makes an angle of 23.0  with the vertical.  ◦  Referring to ﬁgure 6.8, the suspended lamina will have its centre of gravity C vertically −1 d a  with below the suspension point and its straight edge will make an angle θ = tan the vertical, where 2a is the diameter of the semicircle and d is the distance of its centre of mass from the diameter.  Since rotating the lamina about the diameter generates a sphere of volume 4  3 πa3, Pappus’  ﬁrst theorem requires that  Hence d = 4a  3π and θ = tan  3π = 23.0  −1 4  ◦  .  cid:2   3 πa3 = 2πd × 1  4  2 πa2.  197   MULTIPLE INTEGRALS  a θ  C  d  Figure 6.8 Suspending a semicircular lamina from one of its corners.  For problems in rotational mechanics it is often necessary to calculate the moment of inertia of a body about a given axis. This is deﬁned by the multiple integral  6.3.4 Moments of inertia   cid:21   I =  l2 dM,  where l is the distance of a mass element dM from the axis. We may again choose mass elements convenient for evaluating the integral. In this case, however, in addition to elements of constant density we require all parts of each element to be at approximately the same distance from the axis about which the moment of inertia is required.   cid:1  Find the moment of inertia of a uniform rectangular lamina of mass M with sides a and b about one of the sides of length b.  Referring to ﬁgure 6.9, we wish to calculate the moment of inertia about the y-axis. We therefore divide the rectangular lamina into elemental strips parallel to the y-axis of width dx. The mass of such a strip is dM = σb dx, where σ is the mass per unit area of the lamina. The moment of inertia of a strip at a distance x from the y-axis is simply dI = x2 dM = σbx2 dx. The total moment of inertia of the lamina about the y-axis is therefore   cid:21   a  0  I =  σbx2 dx =  σba3  .  3  Since the total mass of the lamina is M = σab, we can write I = 1  3 Ma2.  cid:2   198   6.4 CHANGE OF VARIABLES IN MULTIPLE INTEGRALS  y  b  dM = σb dx  Figure 6.9 A uniform rectangular lamina of mass M with sides a and b can be divided into vertical strips.  dx  a  x  6.3.5 Mean values of functions  In chapter 2 we discussed average values for functions of a single variable. This is easily extended to functions of several variables. Let us consider, for example, a function f x, y  deﬁned in some region R of the xy-plane. Then the average value ¯f of the function is given by  dA =  f x, y  dA.  R  R   6.10   This deﬁnition is easily extended to three  and higher  dimensions; if a function f x, y, z  is deﬁned in some three-dimensional region of space R then the average value ¯f of the function is given by  ¯f  dV =  f x, y, z  dV .   6.11    cid:1 A tetrahedron is bounded by the three coordinate surfaces and the plane x a+y b+z c = 1 and has density ρ x, y, z  = ρ0 1 + x a . Find the average value of the density.  From  6.11 , the average value of the density is given by  ¯ρ  dV =  ρ x, y, z  dV .  R  R  Now the integral on the LHS is just the volume of the tetrahedron, which we found in subsection 6.3.1 to be V = 1 24 abcρ0, calculated in subsection 6.3.2. Therefore ¯ρ = M V = 5  6 abc, and the integral on the RHS is its mass M = 5  4 ρ0.  cid:2   ¯f   cid:21    cid:21   R   cid:21   6.4 Change of variables in multiple integrals  It often happens that, either because of the form of the integrand involved or because of the boundary shape of the region of integration, it is desirable to   cid:21   R   cid:21    cid:21   199   MULTIPLE INTEGRALS  y  u = constant  v = constant  N  L  M  K  R  C  Figure 6.10 A region of integration R overlaid with a grid formed by the family of curves u = constant and v = constant. The parallelogram KLMN deﬁnes the area element dAuv.  x  express a multiple integral in terms of a new set of variables. We now consider how to do this.  6.4.1 Change of variables in double integrals  Let us begin by examining the change of variables in a double integral. Suppose that we require to change an integral   cid:21  cid:21   R  I =  f x, y  dx dy,  in terms of coordinates x and y, into one expressed in new coordinates u and v, given in terms of x and y by diﬀerentiable equations u = u x, y  and v = v x, y  with inverses x = x u, v  and y = y u, v . The region R in the xy-plane and the in curve C that bounds it will become a new region R the uv-plane, and so we must change the limits of integration accordingly. Also, the function f x, y  becomes a new function g u, v  of the new coordinates.  and a new boundary C   cid:7    cid:7   Now the part of the integral that requires most consideration is the area element. In the xy-plane the element is the rectangular area dAxy = dx dy generated by constructing a grid of straight lines parallel to the x- and y- axes respectively. Our task is to determine the corresponding area element in the uv-coordinates. In general the corresponding element dAuv will not be the same shape as dAxy, but this does not matter since all elements are inﬁnitesimally small and the value of the integrand is considered constant over them. Since the sides of the area element are inﬁnitesimal, dAuv will in general have the shape of a parallelogram. We can ﬁnd the connection between dAxy and dAuv by considering the grid formed by the family of curves u = constant and v = constant, as shown in ﬁgure 6.10. Since v  200   6.4 CHANGE OF VARIABLES IN MULTIPLE INTEGRALS  is constant along the line element KL, the latter has components  ∂x ∂u  du and  ∂y ∂u  du in the directions of the x- and y-axes respectively. Similarly, since u is constant along the line element KN, the latter has corresponding components  ∂x ∂v  dv and  ∂y ∂v  dv. Using the result for the area of a parallelogram given in chapter 7, we ﬁnd that the area of the parallelogram KLMN is given by   cid:20  cid:20  cid:20  cid:20  ∂x  cid:20  cid:20  cid:20  cid:20  ∂x  ∂u  ∂u  dAuv =  =  dv − ∂x  ∂y ∂v  − ∂x  ∂v  ∂y ∂u  dv  ∂y ∂u   cid:20  cid:20  cid:20  cid:20  du dv.  ∂v  du  ∂y ∂v   cid:20  cid:20  cid:20  cid:20   du  Deﬁning the Jacobian of x, y with respect to u, v as − ∂x  ≡ ∂x  J =  ∂ x, y  ∂ u, v   ∂y ∂u  ,  we have  The reader acquainted with determinants will notice that the Jacobian can also  be written as the 2 × 2 determinant  ∂y ∂v  ∂v  ∂u  ∂ u, v    cid:20  cid:20  cid:20  cid:20  du dv.  cid:20  cid:20  cid:20  cid:20  ∂ x, y   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  ∂x  cid:20  cid:20  cid:20  cid:20  du dv.  cid:20  cid:20  cid:20  cid:20  ∂ x, y   ∂y ∂u ∂y ∂v  ∂u ∂x ∂v  =  ∂ u, v    cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  .  dAuv =  J =  ∂ x, y  ∂ u, v   dx dy =  Such determinants can be evaluated using the methods of chapter 8.  So, in summary, the relationship between the size of the area element generated  by dx, dy and the size of the corresponding area element generated by du, dv is  This equality should be taken as meaning that when transforming from coordi- nates x, y to coordinates u, v, the area element dx dy should be replaced by the expression on the RHS of the above equality. Of course, the Jacobian can, and in general will, vary over the region of integration. We may express the double integral in either coordinate system as  I =  f x, y  dx dy =   cid:7  g u, v   R   6.12   When evaluating the integral in the new coordinate system, it is usually advisable to sketch the region of integration R  in the uv-plane.   cid:7    cid:20  cid:20  cid:20  cid:20  ∂ x, y   ∂ u, v    cid:20  cid:20  cid:20  cid:20  du dv.   cid:21  cid:21   R   cid:21  cid:21   201    cid:1 Evaluate the double integral  MULTIPLE INTEGRALS  I =  a +  x2 + y2  dx dy,   cid:21  cid:21    cid:9    cid:24   R   cid:21  √ −√  a2−x2 a2−x2   cid:21   a  −a   cid:9    cid:10    cid:24    cid:10   I =  dx  dy  a +  x2 + y2  ,  where R is the region bounded by the circle x2 + y2 = a2.  In Cartesian coordinates, the integral may be written  and can be calculated directly. However, because of the circular boundary of the integration region, a change of variables to plane polar coordinates ρ, φ is indicated. The relationship between Cartesian and plane polar coordinates is given by x = ρ cos φ and y = ρ sin φ. Using  6.12  we can therefore write   cid:21  cid:21   I =   cid:7   a + ρ   R  ∂ ρ, φ    cid:20  cid:20  cid:20  cid:20  ∂ x, y   cid:20  cid:20  cid:20  cid:20  dρ dφ,  cid:20  cid:20  cid:20  cid:20  = ρ cos2 φ + sin2 φ  = ρ.  J =  ∂ x, y  ∂ ρ, φ   =  cos φ  −ρ sin φ ρ cos φ  sin φ   cid:7   where R and φ = 2π. The Jacobian is easily calculated, and we obtain  is the rectangular region in the ρφ-plane whose sides are ρ = 0, ρ = a, φ = 0  So the relationship between the area elements in Cartesian and in plane polar coordinates is  Therefore, when expressed in plane polar coordinates, the integral is given by  dx dy = ρ dρ dφ.   cid:13   I =   cid:7   a + ρ ρ dρ dφ  =  dφ  dρ  a + ρ ρ = 2π  aρ2 2  +  ρ3 3  5πa3  .  cid:2   =  3   cid:20  cid:20  cid:20  cid:20    cid:21   a  0   cid:21  cid:21   cid:21   R 2π  0  6.4.2 Evaluation of the integral I =  −x2  −∞ e  dx  By making a judicious change of variables, it is sometimes possible to evaluate an integral that would be intractable otherwise. An important example of this method is provided by the evaluation of the integral   cid:21  ∞  −x2  I =  −∞ e  Its value may be found by ﬁrst constructing I 2, as follows:   cid:21  ∞   cid:21  ∞  I 2 =  −∞ e  −x2  dx  −y2  −∞ e  dy =  −∞ dx  dx.   cid:21  ∞  cid:21  cid:21    cid:21  ∞ − x2+y2  dx dy,  −∞ dy e  − x2+y2   =  e  R  202   cid:14   a  0   cid:1  ∞   6.4 CHANGE OF VARIABLES IN MULTIPLE INTEGRALS  −a  a  x   cid:1   a−a e  Figure 6.11 The regions used to illustrate the convergence properties of the integral I a  =  dx as a → ∞.  −x2  where the region R is the whole xy-plane. Then, transforming to plane polar coordinates, we ﬁnd   cid:21  cid:21    cid:21   2π   cid:21  ∞  I 2 =  −ρ2   cid:7  e  R  ρ dρ dφ =  dφ  dρ ρe  0  0   cid:23 ∞   cid:22  − 1 2 e  −ρ2  = 2π  −ρ2 √ π. Because the integrand is an  = π.  0  Therefore the original integral is given by I = even function of x, it follows that the value of the integral from 0 to ∞ is simply √ π 2. We note, however, that unlike in all the previous examples, the regions of are both inﬁnite in extent  i.e. unbounded . It is therefore integration R and R prudent to derive this result more rigorously; this we do by considering the integral   cid:7   We then have  −x2  e  dx.  a  −a  I a  =   cid:21  cid:21   I 2 a  =  − x2+y2  dx dy,  e  where R is the square of side 2a centred on the origin. Referring to ﬁgure 6.11, since the integrand is always positive the value of the integral taken over the square lies between the value of the integral taken over the region bounded by the inner circle of radius a and the value of the integral taken over the outer 2a. Transforming to plane polar coordinates as above, we may circle of radius  √  y  a  −a   cid:21   R  203   MULTIPLE INTEGRALS  z  u = c1  T  S P  v = c2  Q w = c3  R  C  y  x  Figure 6.12 A three-dimensional region of integration R, showing an el- ement of volume in u, v, w coordinates formed by the coordinate surfaces u = constant, v = constant, w = constant.   cid:10   evaluate the integrals over the inner and outer circles respectively, and we ﬁnd   cid:10  −2a2 √ Taking the limit a → ∞, we ﬁnd I 2 a  → π. Therefore I = π, as we found previ-   cid:9  1 − e √ αy shows that the corresponding integral of exp −αx2    cid:9  1 − e  < I 2 a  < π  −a2  π  .   cid:24   π α. We use this result in the discussion of the normal distribution  ously. Substituting x = has the value in chapter 30.  6.4.3 Change of variables in triple integrals  A change of variable in a triple integral follows the same general lines as that for a double integral. Suppose we wish to change variables from x, y, z to u, v, w. In the x, y, z coordinates the element of volume is a cuboid of sides dx, dy, dz and volume dVxyz = dx dy dz. If, however, we divide up the total volume into inﬁnitesimal elements by constructing a grid formed from the coordinate surfaces u = constant, v = constant and w = constant, then the element of volume dVuvw in the new coordinates will have the shape of a parallelepiped whose faces are the coordinate surfaces and whose edges are the curves formed by the intersections of these surfaces  see ﬁgure 6.12 . Along the line element P Q the coordinates v and  204   6.4 CHANGE OF VARIABLES IN MULTIPLE INTEGRALS  w are constant, and so P Q has components  ∂x ∂u  du,  ∂y ∂u  du and  ∂z ∂u  du in the directions of the x-, y- and z- axes respectively. The components of the line elements P S and S T are found by replacing u by v and w respectively.  The expression for the volume of a parallelepiped in terms of the components of its edges with respect to the x-, y- and z-axes is given in chapter 7. Using this, we ﬁnd that the element of volume in u, v, w coordinates is given by  where the Jacobian of x, y, z with respect to u, v, w is a short-hand for a 3 × 3  determinant:  dVuvw =  ≡  ∂ x, y, z  ∂ u, v, w   dx dy dz =   cid:20  cid:20  cid:20  cid:20  du dv dw,  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  .  cid:20  cid:20  cid:20  cid:20  du dv dw,  ∂y ∂u ∂y ∂v ∂y ∂w  ∂z ∂u ∂z ∂v ∂z ∂w  ∂ u, v, w    cid:20  cid:20  cid:20  cid:20  ∂ x, y, z   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20   cid:20  cid:20  cid:20  cid:20  ∂ x, y, z   cid:21  cid:21  cid:21   ∂x ∂u ∂x ∂v ∂x ∂w  ∂ u, v, w    cid:7  g u, v, w   R  So, in summary, the relationship between the elemental volumes in multiple integrals formulated in the two coordinate systems is given in Jacobian form by  and we can write a triple integral in either set of coordinates as   cid:21  cid:21  cid:21   R  I =  f x, y, z  dx dy dz =   cid:20  cid:20  cid:20  cid:20  ∂ x, y, z   ∂ u, v, w    cid:20  cid:20  cid:20  cid:20  du dv dw.   cid:1  Find an expression for a volume element in spherical polar coordinates, and hence calcu- late the moment of inertia about a diameter of a uniform sphere of radius a and mass M.  Spherical polar coordinates r, θ, φ are deﬁned by  x = r sin θ cos φ,  y = r sin θ sin φ,  z = r cos θ   and are discussed fully in chapter 10 . The required Jacobian is therefore  J =  ∂ x, y, z  ∂ r, θ, φ   =  sin θ cos φ  r cos θ cos φ r cos θ sin φ −r sin θ −r sin θ sin φ r sin θ cos φ  sin θ sin φ  cos θ  0   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  .   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20   The determinant is most easily evaluated by expanding it with respect to the last column  see chapter 8 , which gives  Therefore the volume element in spherical polar coordinates is given by  J = cos θ r2 sin θ cos θ  + r sin θ r sin2 θ   = r2 sin θ cos2 θ + sin2 θ  = r2 sin θ.  dV =  ∂ x, y, z  ∂ r, θ, φ   dr dθ dφ = r2 sin θ dr dθ dφ,  205   which agrees with the result given in chapter 10.  If we place the sphere with its centre at the origin of an x, y, z coordinate system then  its moment of inertia about the z-axis  which is, of course, a diameter of the sphere  is  where the integral is taken over the sphere, and ρ is the density. Using spherical polar coordinates, we can write this as  MULTIPLE INTEGRALS  I =  x2 + y2  dM = ρ  x2 + y2  dV ,   cid:21   cid:5    cid:6   cid:5    cid:21  cid:21  cid:21   cid:21   cid:21  = ρ × 2π × 4  = ρ  dφ  2π  V  0  3   cid:6    cid:21   cid:5    cid:21    cid:6   π  0  dθ sin3 θ × 1  5 a5 = 8  a  0  dr r4  15 πa5ρ.  I = ρ  r2 sin2 θ  r2 sin θ dr dθ dφ  Since the mass of the sphere is M = 4 I = 2  5 Ma2.  cid:2   3 πa3ρ, the moment of inertia can also be written as  6.4.4 General properties of Jacobians  dx1 dx2 ··· dxn =  Although we will not prove it, the general result for a change of coordinates in an n-dimensional integral from a set xi to a set yj  where i and j both run from 1 to n  is   cid:20  cid:20  cid:20  cid:20  ∂ x1, x2, . . . , xn   ∂ y1, y2, . . . , yn    cid:20  cid:20  cid:20  cid:20  dy1 dy2 ··· dyn,  where the n-dimensional Jacobian can be written as an n × n determinant  see  chapter 8  in an analogous way to the two- and three-dimensional cases.  For readers who already have suﬃcient familiarity with matrices  see chapter 8  and their properties, a fairly compact proof of some useful general properties of Jacobians can be given as follows. Other readers should turn straight to the results  6.16  and  6.17  and return to the proof at some later time.  Consider three sets of variables xi, yi and zi, with i running from 1 to n for each set. From the chain rule in partial diﬀerentiation  see  5.17  , we know that  Now let A, B and C be the matrices whose ijth elements are ∂xi ∂yj, ∂yi ∂zj and ∂xi ∂zj respectively. We can then write  6.13  as the matrix product  cij =  aikbkj  or  C = AB.   6.14   We may now use the general result for the determinant of the product of two  matrices, namely AB = AB, and recall that the Jacobian   6.13    6.15   n cid:4   k=1  ∂xi ∂zj  =  ∂xi ∂yk  ∂yk ∂zj  .  n cid:4   k=1  Jxy =  ∂ x1, . . . , xn  ∂ y1, . . . , yn   = A,  206   6.5 EXERCISES  Jxz = JxyJyz  and similarly for Jyz and Jxz. On taking the determinant of  6.14 , we therefore obtain  or, in the usual notation,  ∂ x1, . . . , xn  ∂ z1, . . . , zn   =  ∂ x1, . . . , xn  ∂ y1, . . . , yn   ∂ y1, . . . , yn  ∂ z1, . . . , zn   .   6.16   As a special case, if the set zi is taken to be identical to the set xi, and the  obvious result Jxx = 1 is used, we obtain  or, in the usual notation,  JxyJyx = 1   cid:13    cid:14 −1  .  ∂ x1, . . . , xn  ∂ y1, . . . , yn   =  ∂ y1, . . . , yn  ∂ x1, . . . , xn    6.17   The similarity between the properties of Jacobians and those of derivatives is apparent, and to some extent is suggested by the notation. We further note from   6.15  that since A = AT, where AT is the transpose of A, we can interchange the  rows and columns in the determinantal form of the Jacobian without changing its value.  6.5 Exercises  6.1  6.2  6.3  6.4  Identify the curved wedge bounded by the surfaces y2 = 4ax, x + z = a and z = 0, and hence calculate its volume V . Evaluate the volume integral of x2 + y2 + z2 over the rectangular parallelepiped bounded by the six surfaces x = ±a, y = ±b and z = ±c. Find the volume integral of x2y over the tetrahedral volume bounded by the planes x = 0, y = 0, z = 0, and x + y + z = 1. Evaluate the surface integral of f x, y  over the rectangle 0 ≤ x ≤ a, 0 ≤ y ≤ b  for the functions   a  f x, y  =  x  x2 + y2 ,   b  f x, y  =  b − y + x   −3 2.  6.5  Calculate the volume of an ellipsoid as follows:   a  Prove that the area of the ellipse   b  Use this result to obtain an expression for the volume of a slice of thickness  is πab.  dz of the ellipsoid  Hence show that the volume of the ellipsoid is 4πabc 3.  x2 a2  y2 b2  +  = 1  x2 a2  y2 b2  z2 c2  +  +  = 1.  207   6.6  The function  MULTIPLE INTEGRALS   cid:7    cid:8   Ψ r  = A  2 − Z r  a  −Z r 2a e   cid:1  Ψ2 dV is  gives the form of the quantum-mechanical wavefunction representing the electron in a hydrogen-like atom of atomic number Z , when the electron is in its ﬁrst allowed spherically symmetric excited state. Here r is the usual spherical polar coordinate, but, because of the spherical symmetry, the coordinates θ and φ do not appear explicitly in Ψ. Determine the value that A  assumed real  must have if the wavefunction is to be correctly normalised, i.e. if the volume integral of  Ψ2 over all space is to be equal to unity. is described by a wavefunction Ψ, which is such that Ψ2 dV is the probability of  In quantum mechanics the electron in a hydrogen atom in some particular state  ﬁnding the electron in the inﬁnitesimal volume dV . In spherical polar coordinates Ψ = Ψ r, θ, φ  and dV = r2 sin θ dr dθ dφ. Two such states are described by  6.7   cid:8    cid:7   cid:8   1 4π  1 2   cid:7    cid:8   cid:7   1 a0  1 2  3 2  sin θ eiφ  1 2a0  −r a0 ,  cid:8  2e  3 2  re  −r 2a0 √ 3  a0  .  Ψ1 =   cid:7   3 8π  Ψ2 = −   a  Show that each Ψi is normalised, i.e. the integral over all space  equal to unity – physically, this means that the electron must be somewhere.  b  The  so-called  dipole matrix element between the states 1 and 2 is given by  the integral   cid:21   px =  ∗ 1qr sin θ cos φ Ψ2 dV ,  Ψ  where q is the charge on the electron. Prove that px has the value −27qa0 35.  6.8  6.9  A planar ﬁgure is formed from uniform wire and consists of two equal semicircu- lar arcs, each with its own closing diameter, joined so as to form a letter ‘B’. The ﬁgure is freely suspended from its top left-hand corner. Show that the straight −1. edge of the ﬁgure makes an angle θ with the vertical given by tan θ =  2 + π  A certain torus has a circular vertical cross-section of radius a centred on a horizontal circle of radius c  > a .   a  Find the volume V and surface area A of the torus, and show that they can  be written as  V =  π2 4   r2 o  − r2  i   ro − ri ,  A = π2 r2 o  − r2 i  ,  where ro and ri are, respectively, the outer and inner radii of the torus.   b  Show that a vertical circular cylinder of radius c, coaxial with the torus,  divides A in the ratio  πc + 2a : πc − 2a.  6.10  A thin uniform circular disc has mass M and radius a.   a  Prove that its moment of inertia about an axis perpendicular to its plane  and passing through its centre is 1  2 Ma2.   b  Prove that the moment of inertia of the same disc about a diameter is 1  4 Ma2.  208   6.5 EXERCISES  This is an example of the general result for planar bodies that the moment of inertia of the body about an axis perpendicular to the plane is equal to the sum of the moments of inertia about two perpendicular axes lying in the plane; in an obvious notation   cid:21    cid:21    cid:21    cid:21   Iz =  r2 dm =   x2 + y2  dm =  x2 dm +  y2 dm = Iy + Ix.  6.11  In some applications in mechanics the moment of inertia of a body about a single point  as opposed to about an axis  is needed. The moment of inertia, I, about the origin of a uniform solid body of density ρ is given by the volume integral  Show that the moment of inertia of a right circular cylinder of radius a, length 2b and mass M about its centre is  I =   x2 + y2 + z2 ρ dV .   cid:21   V   cid:7    cid:8   M  a2 2  +  b2 3  .  6.12  The shape of an axially symmetric hard-boiled egg, of uniform density ρ0, is  given in spherical polar coordinates by r = a 2 − cos θ , where θ is measured  from the axis of symmetry.   a  Prove that the mass M of the egg is M = 40  b  Prove that the egg’s moment of inertia about its axis of symmetry is 342  175 Ma2. In spherical polar coordinates r, θ, φ the element of volume for a body that is symmetrical about the polar axis is dV = 2πr2 sin θ dr dθ, whilst its element of surface area is 2πr sin θ[ dr 2 + r2 dθ 2]1 2. A particular surface is deﬁned by r = 2a cos θ, where a is a constant and 0 ≤ θ ≤ π 2. Find its total surface area  3 πρ0a3.  and the volume it encloses, and hence identify the surface. By expressing both the integrand and the surface element in spherical polar coordinates, show that the surface integral  6.13  6.14  over the surface x2 + y2 = z2, 0 ≤ z ≤ 1, has the value π   √ 2.  x2  x2 + y2 dS  6.15  By transforming to cylindrical polar coordinates, evaluate the integral   cid:21    cid:21   cid:21   cid:21   I =  ln x2 + y2  dx dy dz  over the interior of the conical region x2 + y2 ≤ z2, 0 ≤ z ≤ 1.  6.16  Sketch the two families of curves  y2 = 4u u − x ,  y2 = 4v v + x ,  where u and v are parameters.  By transforming to the uv-plane, evaluate the integral of y  x2 + y2 1 2 over the part of the quadrant x > 0, y > 0 that is bounded by the lines x = 0, y = 0  and the curve y2 = 4a a − x .  6.17  By making two successive simple changes of variables, evaluate   cid:21   cid:21   cid:21   209  I =  x2 dx dy dz   over the ellipsoidal region  MULTIPLE INTEGRALS  x2 a2  +  +  y2 b2  z2 c2  ≤ 1.   cid:21    cid:21   I =  1  1 y  0  x=y  y3 x  exp[y2 x2 + x  −2 ] dx dy  6.18  Sketch the domain of integration for the integral  6.19  6.20  and characterise its boundaries in terms of new variables u = xy and v = y x. −1, and Show that the Jacobian for the change from  x, y  to  u, v  is equal to  2v  hence evaluate I. Sketch the part of the region 0 ≤ x, 0 ≤ y ≤ π 2 that is bounded by the curves  x = 0, y = 0, sinh x cos y = 1 and cosh x sin y = 1. By making a suitable change of variables, evaluate the integral   cid:21   cid:21   I =   sinh2 x + cos2 y  sinh 2x sin 2y dx dy  over the bounded subregion. Deﬁne a coordinate system u, v whose origin coincides with that of the usual x, y system and whose u-axis coincides with the x-axis, whilst the v-axis makes an angle α with it. By considering the integral I =  exp −r2  dA, where r is the radial distance from the origin, over the area deﬁned by 0 ≤ u < ∞, 0 ≤ v < ∞,   cid:1   prove that  exp −u2 − v2 − 2uv cos α  du dv =  α  .  2 sin α   cid:21  ∞   cid:21  ∞  0  0  6.21  As stated in section 5.11, the ﬁrst law of thermodynamics can be expressed as  By calculating and equating ∂2U ∂Y ∂X and ∂2U ∂X∂Y , where X and Y are an unspeciﬁed pair of variables  drawn from P , V , T and S  , prove that  dU = T dS − P dV .  ∂ S , T   ∂ X, Y    =  ∂ V , P   ∂ X, Y    .  ∂ S , T   ∂ V , P    = 1.  Using the properties of Jacobians, deduce that  6.22  The distances of the variable point P , which has coordinates x, y, z, from the ﬁxed  points  0, 0, 1  and  0, 0,−1  are denoted by u and v respectively. New variables  ξ, η, φ are deﬁned by  ξ = 1  2  u + v ,  η = 1  2  u − v ,  cid:8    cid:7   points. Prove that the Jacobian ∂ ξ, η, φ  ∂ x, y, z  has the value  ξ2 − η2   and φ is the angle between the plane y = 0 and the plane containing the three −1 and  that   cid:21   cid:21   cid:21    u − v 2  all space  uv  exp  − u + v 2  dx dy dz =  16π 3e  .  6.23  This is a more diﬃcult question about ‘volumes’ in an increasing number of dimensions.  210    a  Let R be a real positive number and deﬁne Km by  6.6 HINTS AND ANSWERS   cid:5    cid:21   R  −R   cid:6   Km =  R2 − x2  m  dx.   2m + 1 Km = 2mR2Km−1.  Show, using integration by parts, that Km satisﬁes the recurrence relation   b  For integer n, deﬁne In = Kn and Jn = Kn+1 2. Evaluate I0 and J0 directly  and hence prove that   c  A sequence of functions Vn R  is deﬁned by  In =  22n+1 n! 2R2n+1   2n + 1 !  and  Jn =  π 2n + 1 !R2n+2 22n+1n! n + 1 !  .   cid:21   V0 R  = 1,  Vn R  =  R  −R  Vn−1   cid:9 √ R2 − x2   cid:10   n ≥ 1.  dx,  Prove by induction that  V2n R  =  V2n+1 R  =  πnR2n  ,  n!  πn22n+1n!R2n+1   2n + 1 !  .   d  For interest,   i   show that V2n+2 1  < V2n 1  and V2n+1 1  < V2n−1 1  for all n ≥ 3;   ii  hence, by explicitly writing out Vk R  for 1 ≤ k ≤ 8  say , show that the  ‘volume’ of the totally symmetric solid of unit radius is a maximum in ﬁve dimensions.  6.1  6.3 6.5  6.7 6.9  6.11 6.13 6.15  6.17  6.19  6.21 6.23   cid:1   V = 16a3 15. 1 360.  a  Evaluate  For integration order z, y, x, the limits are  0, a − x ,  −√ For integration order y, x, z, the limits are  −√  6.6 Hints and answers  4ax,  √ 4ax ,  0, a − z  and  0, a .  4ax  and  0, a .  4ax,  √  2b[1 −  x a 2]1 2 dx by setting x = a cos φ;  b  dV = π × a[1 −  z c 2]1 2 × b[1 −  z c 2]1 2 dz. Write sin3 θ as  1 − cos2 θ  sin θ when integrating Ψ22.  a  V = 2πc× πa2 and A = 2πa× 2πc. Setting ro = c + a and ri = c− a gives the  stated results.  b  Show that the centre of gravity of either half is 2a π from the cylinder. Transform to cylindrical polar coordinates. 4πa2; 4πa3 3; a sphere. given by 2π[ z2 ln z  −  z2 2 ]; I = −5π 9. The volume element is ρ dφ dρ dz. The integrand for the ﬁnal z-integration is  Set ξ = x a, η = y b, ζ = z c to map the ellipsoid onto the unit sphere, and then change from  ξ, η, ζ  coordinates to spherical polar coordinates; I = 4πa3bc 15. −1 and the Set u = sinh x cos y and v = cosh x sin y; Jxy,uv =  sinh2 x + cos2 y  integrand reduces to 4uv over the region 0 ≤ u ≤ 1, 0 ≤ v ≤ 1; I = 1. Terms such as T ∂2S  ∂Y ∂X cancel in pairs. Use equations  6.17  and  6.16 .  c  Show that the two expressions mutually support the integration formula given for computing a volume in the next higher dimension.  d  ii  2, π, 4π 3, π2 2, 8π2 15, π3 6, 16π3 105, π4 24.  211   7  Vector algebra  This chapter introduces space vectors and their manipulation. Firstly we deal with the description and algebra of vectors, then we consider how vectors may be used to describe lines and planes and ﬁnally we look at the practical use of vectors in ﬁnding distances. Much use of vectors will be made in subsequent chapters; this chapter gives only some basic rules.  7.1 Scalars and vectors  The simplest kind of physical quantity is one that can be completely speciﬁed by its magnitude, a single number, together with the units in which it is measured. Such a quantity is called a scalar and examples include temperature, time and density.  A vector is a quantity that requires both a magnitude  ≥ 0  and a direction in  space to specify it completely; we may think of it as an arrow in space. A familiar example is force, which has a magnitude  strength  measured in newtons and a direction of application. The large number of vectors that are used to describe the physical world include velocity, displacement, momentum and electric ﬁeld. Vectors are also used to describe quantities such as angular momentum and surface elements  a surface element has an area and a direction deﬁned by the normal to its tangent plane ; in such cases their deﬁnitions may seem somewhat arbitrary  though in fact they are standard  and not as physically intuitive as for vectors such as force. A vector is denoted by bold type, the convention of this book, or by underlining, the latter being much used in handwritten work.  This chapter considers basic vector algebra and illustrates just how powerful vector analysis can be. All the techniques are presented for three-dimensional space but most can be readily extended to more dimensions.  Throughout the book we will represent a vector in diagrams as a line together with an arrowhead. We will make no distinction between an arrowhead at the  212   7.2 ADDITION AND SUBTRACTION OF VECTORS  b  a + b  b  a  b + a  a  Figure 7.1 Addition of two vectors showing the commutation relation. We make no distinction between an arrowhead at the end of the line and one along the line’s length, but rather use that which gives the clearer diagram.  end of the line and one along the line’s length but, rather, use that which gives the clearer diagram. Furthermore, even though we are considering three-dimensional vectors, we have to draw them in the plane of the paper. It should not be assumed that vectors drawn thus are coplanar, unless this is explicitly stated.  7.2 Addition and subtraction of vectors  The resultant or vector sum of two displacement vectors is the displacement vector that results from performing ﬁrst one and then the other displacement, as shown in ﬁgure 7.1; this process is known as vector addition. However, the principle of addition has physical meaning for vector quantities other than displacements; for example, if two forces act on the same body then the resultant force acting on the body is the vector sum of the two. The addition of vectors only makes physical sense if they are of a like kind, for example if they are both forces acting in three dimensions. It may be seen from ﬁgure 7.1 that vector addition is commutative, i.e.  The generalisation of this procedure to the addition of three  or more  vectors is clear and leads to the associativity property of addition  see ﬁgure 7.2 , e.g.  a + b = b + a.  a +  b + c  =  a + b  + c.   7.1    7.2   Thus, it is immaterial in what order any number of vectors are added.  The subtraction of two vectors is very similar to their addition  see ﬁgure 7.3 ,  that is,  where −b is a vector of equal magnitude but exactly opposite direction to vector b.  a − b = a +  −b   213   VECTOR ALGEBRA  b  c  a  b  b + c  c  b  a  a + b  a  b  b + c  a  a +  b + c   a + b  c   a + b  + c  −b  a  a − b  Figure 7.2 Addition of three vectors showing the associativity relation.  Figure 7.3 Subtraction of two vectors.  The subtraction of two equal vectors yields the zero vector, 0, which has zero magnitude and no associated direction.  7.3 Multiplication by a scalar  Multiplication of a vector by a scalar  not to be confused with the ‘scalar product’, to be discussed in subsection 7.6.1  gives a vector in the same direction as the original but of a proportional magnitude. This can be seen in ﬁgure 7.4. The scalar may be positive, negative or zero. It can also be complex in some applications. Clearly, when the scalar is negative we obtain a vector pointing in the opposite direction to the original vector. Multiplication by a scalar is associative, commutative and distributive over addition. These properties may be summarised for arbitrary vectors a and b and arbitrary scalars λ and µ by   λµ a = λ µa  = µ λa ,  λ a + b  = λa + λb,   λ + µ a = λa + µa.  214   7.3    7.4    7.5    7.3 MULTIPLICATION BY A SCALAR  B  λ  a  a  µ  a  p  Figure 7.4 Scalar multiplication of a vector  for λ > 1 .  b  P  λ  A  O  Figure 7.5 An illustration of the ratio theorem. The point P divides the line segment AB in the ratio λ : µ.  Having deﬁned the operations of addition, subtraction and multiplication by a  scalar, we can now use vectors to solve simple problems in geometry.   cid:1 A point P divides a line segment AB in the ratio λ : µ  see ﬁgure 7.5 . If the position vectors of the points A and B are a and b, respectively, ﬁnd the position vector of the point P .  As is conventional for vector geometry problems, we denote the vector from the point A to the point B by AB. If the position vectors of the points A and B, relative to some origin  O, are a and b, it should be clear that AB = b − a.  Now, from ﬁgure 7.5 we see that one possible way of reaching the point P from O is ﬁrst to go from O to A and to go along the line AB for a distance equal to the the fraction λ  λ + µ  of its total length. We may express this in terms of vectors as  OP = p = a +   cid:7   = a +  λ + µ  λ  λ  AB   cid:8   b − a   λ + µ  1 − λ  λ + µ  µ  a +  λ  b,  λ + µ  λ + µ  =  =  a +  λ  b  λ + µ  215  which expresses the position vector of the point P in terms of those of A and B. We would, of course, obtain the same result by considering the path from O to B and then to P .  cid:2    7.6    A  F  VECTOR ALGEBRA  C  G  D  E  a  c  O  B  b  Figure 7.6 The centroid of a triangle. The triangle is deﬁned by the points A, B and C that have position vectors a, b and c. The broken lines CD, BE, AF connect the vertices of the triangle to the mid-points of the opposite sides; these lines intersect at the centroid G of the triangle.  Result  7.6  is a version of the ratio theorem and we may use it in solving more  complicated problems.  cid:1 The vertices of triangle ABC have position vectors a, b and c relative to some origin O  see ﬁgure 7.6 . Find the position vector of the centroid G of the triangle.  From ﬁgure 7.6, the points D and E bisect the lines AB and AC respectively. Thus from the ratio theorem  7.6 , with λ = µ = 1 2, the position vectors of D and E relative to the origin are  d = 1 e = 1  2 a + 1 2 a + 1  2 b, 2 c.  r =  1 − λ c + λd, =  1 − λ c + 1  2 λ a + b ,  Using the ratio theorem again, we may write the position vector of a general point on the  line CD that divides the line in the ratio λ :  1 − λ  as  where we have expressed d in terms of a and b. Similarly, the position vector of a general point on the line BE can be expressed as  Thus, at the intersection of the lines CD and BE we require, from  7.7 ,  7.8 ,  By equating the coeﬃcents of the vectors a, b, c we ﬁnd  r =  1 − µ b + µe, =  1 − µ b + 1 2 µ a + c . 2 λ a + b  =  1 − µ b + 1 2 λ = 1 − µ,  1  2 µ a + c .  1 − λ = 1 2 µ.   1 − λ c + 1  λ = µ,  216   7.7    7.8    7.4 BASIS VECTORS AND COMPONENTS  These equations are consistent and have the solution λ = µ = 2 3. Substituting these values into either  7.7  or  7.8  we ﬁnd that the position vector of the centroid G is given by  g = 1  3  a + b + c .  cid:2   7.4 Basis vectors and components  Given any three diﬀerent vectors e1, e2 and e3, which do not all lie in a plane, it is possible, in a three-dimensional space, to write any other vector in terms of scalar multiples of them:  a = a1e1 + a2e2 + a3e3.   7.9   The three vectors e1, e2 and e3 are said to form a basis  for the three-dimensional space ; the scalars a1, a2 and a3, which may be positive, negative or zero, are called the components of the vector a with respect to this basis. We say that the vector has been resolved into components.  Most often we shall use basis vectors that are mutually perpendicular, for ease  of manipulation, though this is not necessary. In general, a basis set must   i  have as many basis vectors as the number of dimensions  in more formal  language, the basis vectors must span the space  and   ii  be such that no basis vector may be described as a sum of the others, or, more formally, the basis vectors must be linearly independent. Putting this mathematically, in N dimensions, we require  c1e1 + c2e2 + ··· + cNeN  cid:3 = 0,  for any set of coeﬃcients c1, c2, . . . , cN except c1 = c2 = ··· = cN = 0.  In this chapter we will only consider vectors in three dimensions; higher dimen- sionality can be achieved by simple extension.  If we wish to label points in space using a Cartesian coordinate system  x, y, z , we may introduce the unit vectors i, j and k, which point along the positive x-, y- and z- axes respectively. A vector a may then be written as a sum of three vectors, each parallel to a diﬀerent coordinate axis:  a = axi + ayj + azk.   7.10   A vector in three-dimensional space thus requires three components to describe fully both its direction and its magnitude. A displacement in space may be thought of as the sum of displacements along the x-, y- and z- directions  see ﬁgure 7.7 . For brevity, the components of a vector a with respect to a particular coordinate system are sometimes written in the form  ax, ay, az . Note that the  217   VECTOR ALGEBRA  k  a  ayj  azk  j  axi  i  Figure 7.7 A Cartesian basis set. The vector a is the sum of axi, ayj and azk.  basis vectors i, j and k may themselves be represented by  1, 0, 0 ,  0, 1, 0  and  0, 0, 1  respectively.  We can consider the addition and subtraction of vectors in terms of their components. The sum of two vectors a and b is found by simply adding their components, i.e.  a + b = axi + ayj + azk + bxi + byj + bzk  =  ax + bx i +  ay + by j +  az + bz k,   7.11   and their diﬀerence by subtracting them,  a − b = axi + ayj + azk −  bxi + byj + bzk  =  ax − bx i +  ay − by j +  az − bz k.   7.12    cid:1 Two particles have velocities v1 = i + 3j + 6k and v2 = i − 2k, respectively. Find the  velocity u of the second particle relative to the ﬁrst.  The required relative velocity is given by  u = v2 − v1 =  1 − 1 i +  0 − 3 j +  −2 − 6 k  = −3j − 8k.  cid:2   7.5 Magnitude of a vector  The magnitude of the vector a is denoted by a or a. In terms of its components  in three-dimensional Cartesian coordinates, the magnitude of a is given by  a ≡ a =  a2 x + a2  y + a2 z .   7.13   Hence, the magnitude of a vector is a measure of its length. Such an analogy is useful for displacement vectors but magnitude is better described, for example, by ‘strength’ for vectors such as force or by ‘speed’ for velocity vectors. For instance,   cid:2   218   7.6 MULTIPLICATION OF VECTORS  b  θ  O  b cos θ  a  Figure 7.8 The projection of b onto the direction of a is b cos θ. The scalar product of a and b is ab cos θ.  in the previous example, the speed of the second particle relative to the ﬁrst is given by   cid:24   −3 2 +  −8 2 =  √ 73.  u = u =  A vector whose magnitude equals unity is called a unit vector. The unit vector  in the direction a is usually notated ˆa and may be evaluated as  ˆa =  aa .   7.14   The unit vector is a useful concept because a vector written as λˆa then has mag- nitude λ and direction ˆa. Thus magnitude and direction are explicitly separated.  7.6 Multiplication of vectors  We have already considered multiplying a vector by a scalar. Now we consider the concept of multiplying one vector by another vector. It is not immediately obvious what the product of two vectors represents and in fact two products are commonly deﬁned, the scalar product and the vector product. As their names imply, the scalar product of two vectors is just a number, whereas the vector product is itself a vector. Although neither the scalar nor the vector product is what we might normally think of as a product, their use is widespread and numerous examples will be described elsewhere in this book.  7.6.1 Scalar product  The scalar product  or dot product  of two vectors a and b is denoted by a · b and is given by  a · b ≡ ab cos θ,  0 ≤ θ ≤ π,   7.15   where θ is the angle between the two vectors, placed ‘tail to tail’ or ‘head to head’. Thus, the value of the scalar product a · b equals the magnitude of a multiplied by the projection of b onto a  see ﬁgure 7.8 .  219   From  7.15  we see that the scalar product has the particularly useful property  that  is a necessary and suﬃcient condition for a to be perpendicular to b  unless either of them is zero . It should be noted in particular that the Cartesian basis vectors i, j and k, being mutually orthogonal unit vectors, satisfy the equations  VECTOR ALGEBRA  a · b = 0  i · i = j · j = k · k = 1, i · j = j · k = k · i = 0.  Examples of scalar products arise naturally throughout physics and in partic- ular in connection with energy. Perhaps the simplest is the work done F · r in moving the point of application of a constant force F through a displacement r; notice that, as expected, if the displacement is perpendicular to the direction of the force then F· r = 0 and no work is done. A second simple example is aﬀorded by the potential energy −m · B of a magnetic dipole, represented in strength and  orientation by a vector m, placed in an external magnetic ﬁeld B.  As the name implies, the scalar product has a magnitude but no direction. The  scalar product is commutative and distributive over addition:  a · b = b · a  a ·  b + c  = a · b + a · c.   cid:1 Four non-coplanar points A, B, C, D are positioned such that the line AD is perpendicular to BC and BD is perpendicular to AC. Show that CD is perpendicular to AB.  Denote the four position vectors by a, b, c, d. As none of the three pairs of lines actually intersect, it is diﬃcult to indicate their orthogonality in the diagram we would normally draw. However, the orthogonality can be expressed in vector form and we start by noting  that, since AD ⊥ BC, it follows from  7.16  that   7.16    7.17    7.18    7.19    7.20   Similarly, since BD ⊥ AC,   d − a  ·  c − b  = 0.  d − b  ·  c − a  = 0.  Combining these two equations we ﬁnd   d − a  ·  c − b  =  d − b  ·  c − a ,  which, on mutliplying out the parentheses, gives  d · c − a · c − d · b + a · b = d · c − b · c − d · a + b · a.  Cancelling terms that appear on both sides and rearranging yields  which simpliﬁes to give  From  7.16 , we see that this implies that CD is perpendicular to AB.  cid:2   d · b − d · a − c · b + c · a = 0,   d − c  ·  b − a  = 0.  220   7.6 MULTIPLICATION OF VECTORS  If we introduce a set of basis vectors that are mutually orthogonal, such as i, j, k, we can write the components of a vector a, with respect to that basis, in terms  of the scalar product of a with each of the basis vectors, i.e. ax = a·i, ay = a·j and az = a· k. In terms of the components ax, ay and az the scalar product is given by  a · b =  axi + ayj + azk  ·  bxi + byj + bzk  = axbx + ayby + azbz,   7.21   where the cross terms such as axi · byj are zero because the basis vectors are mutually perpendicular; see equation  7.18 . It should be clear from  7.15  that the value of a · b has a geometrical deﬁnition and that this value is independent of the actual basis vectors used.  cid:1 Find the angle between the vectors a = i + 2j + 3k and b = 2i + 3j + 4k.  From  7.15  the cosine of the angle θ between a and b is given by  a · b ab . From  7.21  the scalar product a · b has the value  cos θ =  and from  7.13  the lengths of the vectors are   cid:24   a =  a · b = 1 × 2 + 2 × 3 + 3 × 4 = 20,  cid:24   √ 14  b =  12 + 22 + 32 =  and  22 + 32 + 42 =  29.  √  Thus,  cos θ =  20√ √ 29 14  ≈ 0.9926 ⇒ θ = 0.12 rad.  cid:2   We can see from the expressions  7.15  and  7.21  for the scalar product that if  θ is the angle between a and b then  cos θ =  ax a  bx b  +  ay a  by b  +  az a  bz b  where ax a, ay a and az a are called the direction cosines of a, since they give the cosine of the angle made by a with each of the basis vectors. Similarly bx b, by b and bz b are the direction cosines of b.  If we take the scalar product of any vector a with itself then clearly θ = 0 and  from  7.15  we have  a =  √ a · a.  a · a = a2.  Thus the magnitude of a can be written in a coordinate-independent form as  Finally, we note that the scalar product may be extended to vectors with  complex components if it is redeﬁned as  where the asterisk represents the operation of complex conjugation. To accom-  ∗ ∗ ∗ a · b = a yby + a xbx + a zbz,  221   VECTOR ALGEBRA  a × b  θ  b  a  Figure 7.9 The vector product. The vectors a, b and a×b form a right-handed  set.  modate this extension the commutation property  7.19  must be modiﬁed to read  ∗ a · b =  b · a   .   7.22   In particular it should be noted that  λa  · b = λ ∗ However, the magnitude of a complex vector is still given by a = a · a is always real.  a · b, whereas a ·  λb  = λa · b. √ a · a, since  The vector product  or cross product  of two vectors a and b is denoted by a× b and is deﬁned to be a vector of magnitude ab sin θ in a direction perpendicular  7.6.2 Vector product  to both a and b;  a × b = ab sin θ.  The direction is found by ‘rotating’ a into b through the smallest possible angle. The sense of rotation is that of a right-handed screw that moves forward in the  direction a × b  see ﬁgure 7.9 . Again, θ is the angle between the two vectors placed ‘tail to tail’ or ‘head to head’. With this deﬁnition a, b and a × b form a  right-handed set. A more directly usable description of the relative directions in a vector product is provided by a right hand whose ﬁrst two ﬁngers and thumb are held to be as nearly mutually perpendicular as possible. If the ﬁrst ﬁnger is pointed in the direction of the ﬁrst vector and the second ﬁnger in the direction of the second vector, then the thumb gives the direction of the vector product.  The vector product is distributive over addition, but anticommutative and non-  associative:   a + b  × c =  a × c  +  b × c , b × a = − a × b ,  a × b  × c  cid:3 = a ×  b × c .  222   7.23    7.24    7.25    7.6 MULTIPLICATION OF VECTORS  P  F  r  R  θ  O  Figure 7.10 The moment of the force F about O is r×F. The cross represents the direction of r × F, which is perpendicularly into the plane of the paper.  From its deﬁnition, we see that the vector product has the very useful property  that if a × b = 0 then a is parallel or antiparallel to b  unless either of them is  zero . We also note that   7.26    7.27    7.28   a × a = 0.  a × c = b × c.  A = a × b.   cid:1 Show that if a = b + λc, for some scalar λ, then a × c = b × c.  From  7.23  we have  a × c =  b + λc  × c = b × c + λc × c.  However, from  7.26 , c × c = 0 and so  We note in passing that the fact that  7.27  is satisﬁed does not imply that a = b.  cid:2   An example of the use of the vector product is that of ﬁnding the area, A, of  a parallelogram with sides a and b, using the formula  Another example is aﬀorded by considering a force F acting through a point R, whose vector position relative to the origin O is r  see ﬁgure 7.10 . Its moment or torque about O is the strength of the force times the perpendicular distance  OP , which numerically is just Fr sin θ, i.e. the magnitude of r × F. Furthermore,  the sense of the moment is clockwise about an axis through O that points perpendicularly into the plane of the paper  the axis is represented by a cross  in the ﬁgure . Thus the moment is completely represented by the vector r × F,  in both magnitude and spatial sense. It should be noted that the same vector product is obtained wherever the point R is chosen, so long as it lies on the line of action of F.  Similarly, if a solid body is rotating about some axis that passes through the origin, with an angular velocity ω then we can describe this rotation by a vector ω that has magnitude ω and points along the axis of rotation. The direction of ω  223   VECTOR ALGEBRA  i × i = j × j = k × k = 0,  i × j = −j × i = k, j × k = −k × j = i, k × i = −i × k = j.   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20   a × b =  i ax bx  j ay by  k az bz   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  .  is the forward direction of a right-handed screw rotating in the same sense as the body. The velocity of any point in the body with position vector r is then given  by v = ω × r.  Since the basis vectors i, j, k are mutually perpendicular unit vectors, forming  a right-handed set, their vector products are easily seen to be   7.29    7.30    7.31    7.32   Using these relations, it is straightforward to show that the vector product of two general vectors a and b is given in terms of their components with respect to the basis set i, j, k, by  a × b =  aybz − azby i +  azbx − axbz j +  axby − aybx k.   7.33   For the reader who is familiar with determinants  see chapter 8 , we record that this can also be written as  That the cross product a × b is perpendicular to both a and b can be veriﬁed  in component form by forming its dot products with each of the two vectors and showing that it is zero in both cases.  cid:1 Find the area A of the parallelogram with sides a = i + 2j + 3k and b = 4i + 5j + 6k. The vector product a × b is given in component form by  a × b =  2 × 6 − 3 × 5 i +  3 × 4 − 1 × 6 j +  1 × 5 − 2 × 4 k  = −3i + 6j − 3k.  cid:24   A = a × b =  Thus the area of the parallelogram is   −3 2 + 62 +  −3 2 =  54.  cid:2   √  7.6.3 Scalar triple product  Now that we have deﬁned the scalar and vector products, we can extend our discussion to deﬁne products of three vectors. Again, there are two possibilities, the scalar triple product and the vector triple product.  224   7.6 MULTIPLICATION OF VECTORS  v  P  O  φ  c  b  θ  a  Figure 7.11 The scalar triple product gives the volume of a parallelepiped.  The scalar triple product is denoted by  [a, b, c] ≡ a ·  b × c   and, as its name suggests, it is just a number. It is most simply interpreted as the volume of a parallelepiped whose edges are given by a, b and c  see ﬁgure 7.11 .  The vector v = a× b is perpendicular to the base of the solid and has magnitude v = ab sin θ, i.e. the area of the base. Further, v · c = vc cos φ. Thus, since c cos φ = OP is the vertical height of the parallelepiped, it is clear that  a × b  · c = area of the base × perpendicular height = volume. It follows that, if the vectors a, b and c are coplanar, a ·  b × c  = 0.  Expressed in terms of the components of each vector with respect to the  Cartesian basis set i, j, k the scalar triple product is  a ·  b × c  = ax bycz − bzcy  + ay bzcx − bxcz  + az bxcy − bycx ,   7.34   which can also be written as a determinant:   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  ax  bx cx   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  .  ay by cy  az bz cz  a ·  b × c  =  a ·  b × c  =  a × b  · c,  By writing the vectors in component form, it can be shown that  so that the dot and cross symbols can be interchanged without changing the result. More generally, the scalar triple product is unchanged under cyclic permutation of the vectors a, b, c. Other permutations simply give the negative of the original scalar triple product. These results can be summarised by  [a, b, c] = [b, c, a] = [c, a, b] = −[a, c, b] = −[b, a, c] = −[c, b, a].   7.35   225   VECTOR ALGEBRA   cid:1 Find the volume V of the parallelepiped with sides a = i + 2j + 3k, b = 4i + 5j + 6k and c = 7i + 8j + 10k.  We have already found that a × b = −3i + 6j − 3k, in subsection 7.6.2. Hence the volume  of the parallelepiped is given by  V = a ·  b × c  =  a × b  · c  =  −3i + 6j − 3k  ·  7i + 8j + 10k  =  −3  7  +  6  8  +  −3  10  = 3.  cid:2   Another useful formula involving both the scalar and vector products is La-  grange’s identity  see exercise 7.9 , i.e.   a × b  ·  c × d  ≡  a · c  b · d  −  a · d  b · c .   7.36   7.6.4 Vector triple product  By the vector triple product of three vectors a, b, c we mean the vector a×  b× c . Clearly, a ×  b × c  is perpendicular to a and lies in the plane of b and c and so the vector triple product is not associative, i.e. a ×  b × c   cid:3 =  a × b  × c.  can be expressed in terms of them  see  7.37  below . We note, from  7.25 , that  Two useful formulae involving the vector triple product are  a ×  b × c  =  a · c b −  a · b c,  a × b  × c =  a · c b −  b · c a,   7.37    7.38   which may be derived by writing each vector in component form  see exercise 7.8 . It can also be shown that for any three vectors a, b, c,  a ×  b × c  + b ×  c × a  + c ×  a × b  = 0.  7.7 Equations of lines, planes and spheres  Now that we have described the basic algebra of vectors, we can apply the results to a variety of problems, the ﬁrst of which is to ﬁnd the equation of a line in vector form.  7.7.1 Equation of a line  Consider the line passing through the ﬁxed point A with position vector a and having a direction b  see ﬁgure 7.12 . It is clear that the position vector r of a general point R on the line can be written as  r = a + λb,  226   7.39    7.7 EQUATIONS OF LINES, PLANES AND SPHERES  A  b  a  R  r  O  Figure 7.12 The equation of a line. The vector b is in the direction AR and λb is the vector from A to R.  since R can be reached by starting from O, going along the translation vector a to the point A on the line and then adding some multiple λb of the vector b. Diﬀerent values of λ give diﬀerent points R on the line.  Taking the components of  7.39 , we see that the equation of the line can also  be written in the form  x − ax  bx  y − ay  by  z − az  bz  =  =  = constant.   7.40   Taking the vector product of  7.39  with b and remembering that b× b = 0 gives  an alternative equation for the line  We may also ﬁnd the equation of the line that passes through two ﬁxed points  A and C with position vectors a and c. Since AC is given by c − a, the position  vector of a general point on the line is   r − a  × b = 0.  r = a + λ c − a .  7.7.2 Equation of a plane  The equation of a plane through a point A with position vector a and perpendic- ular to a unit position vector ˆn  see ﬁgure 7.13  is   r − a  · ˆn = 0.   7.41   This follows since the vector joining A to a general point R with position vector r is r − a; r will lie in the plane if this vector is perpendicular to the normal to the plane. Rewriting  7.41  as r · ˆn = a · ˆn, we see that the equation of the plane may also be expressed in the form r · ˆn = d, or in component form as  lx + my + nz = d,   7.42   227   VECTOR ALGEBRA  A  R  a  d  r  ˆn  O  Figure 7.13 The equation of the plane is  r − a  · ˆn = 0.  where the unit normal to the plane is ˆn = li + mj + nk and d = a · ˆn is the  perpendicular distance of the plane from the origin.  The equation of a plane containing points a, b and c is  r = a + λ b − a  + µ c − a .  This is apparent because starting from the point a in the plane, all other points may be reached by moving a distance along each of two  non-parallel  directions  in the plane. Two such directions are given by b − a and c − a. It can be shown  that the equation of this plane may also be written in the more symmetrical form  r = αa + βb + γc,  where α + β + γ = 1.  cid:1 Find the direction of the line of intersection of the two planes x + 3y − z = 5 and 2x − 2y + 4z = 3. The two planes have normal vectors n1 = i + 3j − k and n2 = 2i − 2j + 4k. It is clear  that these are not parallel vectors and so the planes must intersect along some line. The direction p of this line must be parallel to both planes and hence perpendicular to both normals. Therefore  p = n1 × n2 = [ 3  4  −  −2  −1 ] i + [ −1  2  −  1  4 ] j + [ 1  −2  −  3  2 ] k = 10i − 6j − 8k.  cid:2   7.7.3 Equation of a sphere  Clearly, the deﬁning property of a sphere is that all points on it are equidistant from a ﬁxed point in space and that the common distance is equal to the radius  228    7.43    7.44    7.45   7.8 USING VECTORS TO FIND DISTANCES  of the sphere. This is easily expressed in vector notation as  r − c2 =  r − c  ·  r − c  = a2,  where c is the position vector of the centre of the sphere and a is its radius.  cid:1 Find the radius ρ of the circle that is the intersection of the plane ˆn· r = p and the sphere of radius a centred on the point with position vector c.  The equation of the sphere is  and that of the circle of intersection is  r − c2 = a2,  r − b2 = ρ2,  where r is restricted to lie in the plane and b is the position of the circle’s centre.  As b lies on the plane whose normal is ˆn, the vector b − c must be parallel to ˆn, i.e. b − c = λˆn for some λ. Further, by Pythagoras, we must have ρ2 + b − c2 = a2. Thus λ2 = a2 − ρ2.   cid:24   cid:9  a2 − ρ2 ˆn and substituting in  7.45  gives r2 − 2r ·   cid:10  a2 − ρ2 ˆn  a2 − ρ2 + a2 − ρ2 = ρ2,  + c2 + 2 c · ˆn   Writing b = c +   cid:24    cid:24   c +  whilst, on expansion,  7.44  becomes  Subtracting these last two equations, using ˆn · r = p and simplifying yields  r2 − 2r · c + c2 = a2.  cid:24  p − c · ˆn =  a2 − ρ2.   cid:24   On rearrangement, this gives ρ as constraints on the values a, c, ˆn and p can take if a real intersection between the sphere and the plane is to occur.  cid:2   a2 −  p − c · ˆn 2, which places obvious geometrical  7.8 Using vectors to ﬁnd distances  This section deals with the practical application of vectors to ﬁnding distances. Some of these problems are extremely cumbersome in component form, but they all reduce to neat solutions when general vectors, with no explicit basis set, are used. These examples show the power of vectors in simplifying geometrical problems.  7.8.1 Distance from a point to a line  Figure 7.14 shows a line having direction b that passes through a point A whose position vector is a. To ﬁnd the minimum distance d of the line from a point P whose position vector is p, we must solve the right-angled triangle shown. We see  that d = p− a sin θ; so, from the deﬁnition of the vector product, it follows that  d =  p − a  × ˆb.  229   VECTOR ALGEBRA  P  d  p − a  p  θ  b  A  a  O  Figure 7.14 The minimum distance from a point to a line.   cid:1 Find the minimum distance from the point P with coordinates  1, 2, 1  to the line r = a+λb, where a = i + j + k and b = 2i − j + 3k.  Comparison with  7.39  shows that the line passes through the point  1, 1, 1  and has  direction 2i − j + 3k. The unit vector in this direction is  2i − j + 3k .  ˆb =  1√ 14  The position vector of P is p = i + 2j + k and we ﬁnd   p − a  × ˆb =  1√ 14 1√ 14  =  [ j ×  2i − 3j + 3k ]  3i − 2k .   cid:24   13 14.  cid:2   Thus the minimum distance from the line to the point P is d =  7.8.2 Distance from a point to a plane  The minimum distance d from a point P whose position vector is p to the plane  deﬁned by  r − a  · ˆn = 0 may be deduced by ﬁnding any vector from P to the in ﬁgure 7.15. Consider the vector a − p, which is a particular vector from P to  plane and then determining its component in the normal direction. This is shown  the plane. Its component normal to the plane, and hence its distance from the plane, is given by   7.46   where the sign of d depends on which side of the plane P is situated.  d =  a − p  · ˆn,  230   7.8 USING VECTORS TO FIND DISTANCES  P  ˆn  d  p  a  O  Figure 7.15 The minimum distance d from a point to a plane.   cid:1 Find the distance from the point P with coordinates  1, 2, 3  to the plane that contains the points A, B and C having coordinates  0, 1, 0 ,  2, 3, 1  and  5, 7, 2 .  Let us denote the position vectors of the points A, B, C by a, b, c. Two vectors in the plane are  b − a = 2i + 2j + k  and  c − a = 5i + 6j + 2k,  and hence a vector normal to the plane is  n =  2i + 2j + k  ×  5i + 6j + 2k  = −2i + j + 2k,  and its unit normal is  ˆn =  nn = 1  3  −2i + j + 2k .  Denoting the position vector of P by p, the minimum distance from the plane to P is given by  d =  a − p  · ˆn =  −i − j − 3k  · 1 − 2 = − 5 = 2 3 . 3 If we take P to be the origin O, then we ﬁnd d = 1 3 , i.e. a positive quantity. It follows from this that the original point P with coordinates  1, 2, 3 , for which d was negative, is on the opposite side of the plane from the origin.  cid:2   3  −2i + j + 2k   − 1  3  7.8.3 Distance from a line to a line  Consider two lines in the directions a and b, as shown in ﬁgure 7.16. Since a × b  is by deﬁnition perpendicular to both a and b, the unit vector normal to both these lines is  a × b a × b .  ˆn =  231   VECTOR ALGEBRA  Q  P  p  q  ˆn  O  b  a  Figure 7.16 The minimum distance from one line to another.  If p and q are the position vectors of any two points P and Q on diﬀerent lines  then the vector connecting them is p − q. Thus, the minimum distance d between  the lines is this vector’s component along the unit normal, i.e.  d =  p − q  · ˆn.   cid:1 A line is inclined at equal angles to the x-, y- and z-axes and passes through the origin. Another line passes through the points  1, 2, 4  and  0, 0, 1 . Find the minimum distance between the two lines.  The ﬁrst line is given by  and the second by  Hence a vector normal to both lines is  n =  i + j + k  ×  i + 2j + 3k  = i − 2j + k,  and the unit normal is  r1 = λ i + j + k ,  r2 = k + µ i + 2j + 3k .  ˆn =  1√ 6   i − 2j + k .  A vector between the two lines is, for example, the one connecting the points  0, 0, 0  and  0, 0, 1 , which is simply k. Thus it follows that the minimum distance between the two lines is  d =  1√ 6  k ·  i − 2j + k  =  1√ 6  .  cid:2   7.8.4 Distance from a line to a plane  Let us consider the line r = a + λb. This line will intersect any plane to which it is not parallel. Thus, if a plane has a normal ˆn then the minimum distance from  232   7.9 RECIPROCAL VECTORS  the line to the plane is zero unless  in which case the distance, d, will be  b · ˆn = 0,  d =  a − r  · ˆn,  x + 2y + 3z = 6.  where r is any point in the plane.  cid:1 A line is given by r = a + λb, where a = i + 2j + 3k and b = 4i + 5j + 6k. Find the coordinates of the point P at which the line intersects the plane  A vector normal to the plane is  n = i + 2j + 3k,  from which we ﬁnd that b · n  cid:3 = 0. Thus the line does indeed intersect the plane. To ﬁnd  the point of intersection we merely substitute the x-, y- and z- values of a general point on the line into the equation of the plane, obtaining  1 + 4λ + 2 2 + 5λ  + 3 3 + 6λ  = 6 ⇒ 14 + 32λ = 6.  4  4  = 0, y = 2− 1  4 , which we may substitute into the equation for the line to obtain 2 . Thus the point of intersection is  4 and z = 3− 1  4  5  = 3  4  6  = 3  This gives λ = − 1 x = 1− 1 2  .  cid:2   0, 3 4 , 3  7.9 Reciprocal vectors  The ﬁnal section of this chapter introduces the concept of reciprocal vectors, which have particular uses in crystallography.  cid:7    cid:7   cid:7  The two sets of vectors a, b, c and a , b , c  cid:7  = c · c = b · b   cid:7  a · a   cid:7   = 1  are called reciprocal sets if   7.47   and   cid:7  · b = a   cid:7  · c = b   cid:7  · a = b   cid:7  · c = c   cid:7  · a = c  a   cid:7  · b = 0.  It can be veriﬁed  see exercise 7.19  that the reciprocal vectors of a, b and c are given by   7.48    7.49    7.50    7.51   =  =   cid:7  a  cid:7   b   cid:7   c  =  b × c a ·  b × c  c × a a ·  b × c  a × b a ·  b × c   ,  ,  ,  233  where a·  b× c   cid:3 = 0. In other words, reciprocal vectors only exist if a, b and c are   VECTOR ALGEBRA  not coplanar. Moreover, if a, b and c are mutually orthogonal unit vectors then  cid:7   cid:7  = a, b a  cid:1 Construct the reciprocal vectors of a = 2i, b = j + k, c = i + k.  = c, so that the two systems of vectors are identical.  = b and c   cid:7   First we evaluate the triple scalar product:  Now we ﬁnd the reciprocal vectors:  a ·  b × c  = 2i · [ j + k  ×  i + k ] = 2i ·  i + j − k  = 2.  a   cid:7   cid:7  b  cid:7  c  2  j + k  ×  i + k  = 1 2  i + k  × 2i = j, 2  2i  ×  j + k  = −j + k.  = 1 = 1 = 1  2  i + j − k ,  It is easily veriﬁed that these reciprocal vectors satisfy their deﬁning properties  7.47 ,  7.48 .  cid:2   We may also use the concept of reciprocal vectors to deﬁne the components of a vector a with respect to basis vectors e1, e2, e3 that are not mutually orthogonal. If the basis vectors are of unit length and mutually orthogonal, such as the Cartesian basis vectors i, j, k, then  see the text preceeding  7.21   the vector a can be written in the form  a =  a · i i +  a · j j +  a · k k.  If the basis is not orthonormal, however, then this is no longer true. Nevertheless, we may write the components of a with respect to a non-orthonormal basis  cid:7  e1, e2, e3 in terms of its reciprocal basis vectors e 3, which are deﬁned as in  7.49 – 7.51 . If we let   cid:7  1, e   cid:7  2, e  a = a1e1 + a2e2 + a3e3,  then the scalar product a · e   cid:7  1 is given by  a · e   cid:7  1 = a1e1 · e   cid:7  1 = a1, where we have used the relations  7.48 . Similarly, a2 = a·e  cid:7  3 e3.   cid:7  1 e1 +  a · e   cid:7  2 e2 +  a · e   cid:7  1 + a2e2 · e   cid:7  1 + a3e3 · e  a =  a · e  2 and a3 = a·e  cid:7    cid:7  3; so now   7.52   7.1  Which of the following statements about general vectors a, b and c are true?  7.10 Exercises   a  c ·  a × b  =  b × a  · c.  b  a ×  b × c  =  a × b  × c.  c  a ×  b × c  =  a · c b −  a · b c.  d  d = λa + µb implies  a × b  · d = 0.  e  a × c = b × c implies c · a − c · b = ca − b.   a × b  ×  c × b  = b[b ·  c × a ].   f   234   7.10 EXERCISES  A unit cell of diamond is a cube of side A, with carbon atoms at each corner, at the centre of each face and, in addition, at positions displaced by 1 4 A i + j + k  from each of those already mentioned; i, j, k are unit vectors along the cube axes. One corner of the cube is taken as the origin of coordinates. What are the vectors joining the atom at 1 4 A i + j + k  to its four nearest neighbours? Determine the angle between the carbon bonds in diamond. Identify the following surfaces:  r = k;  b  r · u = l;  c  r · u = mr for −1 ≤ m ≤ +1;   d  r −  r · u u = n. Find the angle between the position vectors to the points  3,−4, 0  and  −2, 1, 0   Here k, l, m and n are ﬁxed scalars and u is a ﬁxed unit vector.   a   and ﬁnd the direction cosines of a vector perpendicular to both. A, B, C and D are the four corners, in order, of one face of a cube of side 2 units. The opposite face has corners E, F, G and H, with AE, BF, CG and DH as parallel edges of the cube. The centre O of the cube is taken as the origin and the x-, y- and z-axes are parallel to AD, AE and AB, respectively. Find the following:  the angle between the face diagonal AF and the body diagonal AG;   a   b  the equation of the plane through B that is parallel to the plane CGE;  c   the perpendicular distance from the centre J of the face BCGF to the plane OCG;   d  the volume of the tetrahedron JOCG.  Use vector methods to prove that the lines joining the mid-points of the opposite edges of a tetrahedron OABC meet at a point and that this point bisects each of the lines. The edges OP , OQ and OR of a tetrahedron OP QR are vectors p, q and r,  respectively, where p = 2i + 4j, q = 2i − j + 3k and r = 4i − 2j + 5k. Show that  OP is perpendicular to the plane containing OQR. Express the volume of the tetrahedron in terms of p, q and r and hence calculate the volume. Prove, by writing it out in component form, that   a × b  × c =  a · c b −  b · c a,  and deduce the result, stated in equation  7.25 , that the operation of forming the vector product is non-associative. Prove Lagrange’s identity, i.e.   a × b  ·  c × d  =  a · c  b · d  −  a · d  b · c .  7.2  7.3  7.4  7.5  7.6  7.7  7.8  7.9  7.10  For four arbitrary vectors a, b, c and d, evaluate   a × b  ×  c × d   in two diﬀerent ways and so prove that  a[b, c, d] − b[c, d, a] + c[d, a, b] − d[a, b, c] = 0.  Show that this reduces to the normal Cartesian representation of the vector d, i.e. dxi + dyj + dzk, if a, b and c are taken as i, j and k, the Cartesian base vectors.  Show that the points  1, 0, 1 ,  1, 1, 0  and  1,−3, 4  lie on a straight line. Give the  7.11  equation of the line in the form  r = a + λb.  235   VECTOR ALGEBRA  7.12  7.13  7.14  7.15  The plane P1 contains the points A, B and C, which have position vectors  a = −3i + 2j, b = 7i + 2j and c = 2i + 3j + 2k, respectively. Plane P2 passes  through A and is orthogonal to the line BC, whilst plane P3 passes through B and is orthogonal to the line AC. Find the coordinates of r, the point of intersection of the three planes. Two planes have non-parallel unit normals ˆn and ˆm and their closest distances from the origin are λ and µ, respectively. Find the vector equation of their line of intersection in the form r = νp + a. Two ﬁxed points, A and B, in three-dimensional space have position vectors a and b. Identify the plane P given by   a − b  · r = 1  2  a2 − b2 ,  where a and b are the magnitudes of a and b. Show also that the equation   a − r  ·  b − r  = 0  describes a sphere S of radius a− b 2. Deduce that the intersection of P and S a − b   is also the intersection of two spheres, centred on A and B, and each of radius  √ 2.  Let O, A, B and C be four points with position vectors 0, a, b and c, and denote by g = λa + µb + νc the position of the centre of the sphere on which they all lie.   a  Prove that λ, µ and ν simultaneously satisfy   a · a λ +  a · b µ +  a · c ν = 1 2 a2  and two other similar equations.   b  By making a change of origin, ﬁnd the centre and radius of the sphere on  which the points p = 3i + j− 2k, q = 4i + 3j− 3k, r = 7i− 3k and s = 6i + j− k  all lie.  7.16  The vectors a, b and c are coplanar and related by  where λ, µ, ν are not all zero. Show that the condition for the points with position vectors αa, βb and γc to be collinear is  7.17  Using vector methods:   a  Show that the line of intersection of the planes x + 2y + 3z = 0 and 3x + 2y + z = 0 is equally inclined to the x- and z-axes and makes an angle cos  √ 6  with the y-axis.  −1 −2    b  Find the perpendicular distance between one corner of a unit cube and the  major diagonal not passing through it.  7.18  Four points Xi, i = 1, 2, 3, 4, taken for simplicity as all lying within the octant  x, y, z ≥ 0, have position vectors xi. Convince yourself that the direction of  vector xn lies within the sector of space deﬁned by the directions of the other three vectors if   cid:13   considered for i = 1, 2, 3, 4 in turn, takes its maximum value for i = n, i.e. n equals that value of i for which the largest of the set of angles which xi makes with the other vectors, is found to be the lowest. Determine whether any of the four  λa + µb + νc = 0,  λ α  µ β  ν γ  +  +  = 0.   cid:14   ,  xi · xj xixj  min over j  236   7.10 EXERCISES  a  b  c  d  a  Figure 7.17 A face-centred cubic crystal.  7.19  7.20  7.21  points with coordinates  X1 =  3, 2, 2 , X2 =  2, 3, 1 , X3 =  2, 1, 3 , X4 =  3, 0, 3    cid:7   lies within the tetrahedron deﬁned by the origin and the other three points. The vectors a, b and c are not coplanar. The vectors a are the associated reciprocal vectors. Verify that the expressions  7.49 – 7.51  deﬁne a set of reciprocal vectors a  cid:7  · a = b  cid:7  · b = c  a  a  cid:7  · b = a  cid:7  · c = b  b  a  cid:7   cid:7  ] = 1 [a, b, c]; [a  c  , c , b  cid:7  × c  cid:7   cid:7   cid:7  ].  d  a =  b , c   cid:7  , b  cid:7  · c = 1;  cid:7  · a etc = 0;  cid:7   with the following properties:   cid:7  and c   cid:7  and c    [a  , b  , b   cid:7    cid:7    cid:7    cid:7    cid:7   , b   cid:7  and c  −1b and m  −1c is in the direction of the vector ka  Three non-coplanar vectors a, b and c, have as their respective reciprocal vectors . Show that the normal to the plane containing the points the set a −1a, l k In a crystal with a face-centred cubic structure, the basic cell can be taken as a cube of edge a with its centre at the origin of coordinates and its edges parallel to the Cartesian coordinate axes; atoms are sited at the eight corners and at the centre of each face. However, other basic cells are possible. One is the rhomboid shown in ﬁgure 7.17, which has the three vectors b, c and d as edges.   cid:7  + mc  + lb   cid:7    cid:7   .  ◦   a  Show that the volume of the rhomboid is one-quarter that of the cube.  b  Show that the angles between pairs of edges of the rhomboid are 60  and that the corresponding angles between pairs of edges of the rhomboid deﬁned by the reciprocal vectors to b, c, d are each 109.5 .  This rhomboid can be used as the basic cell of a body-centred cubic structure, more easily visualised as a cube with an atom at each corner and one at its centre.  In order to use the Bragg formula, 2d sin θ = nλ, for the scattering of X-rays by a crystal, it is necessary to know the perpendicular distance d between successive planes of atoms; for a given crystal structure, d has a particular value for each set of planes considered. For the face-centred cubic structure ﬁnd the distance between successive planes with normals in the k, i + j and i + j + k directions.   c   ◦  237   VECTOR ALGEBRA  7.22  In subsection 7.6.2 we showed how the moment or torque of a force about an axis could be represented by a vector in the direction of the axis. The magnitude of the vector gives the size of the moment and the sign of the vector gives the sense. Similar representations can be used for angular velocities and angular momenta.   a  The magnitude of the angular momentum about the origin of a particle of mass m moving with velocity v on a path that is a perpendicular distance d  from the origin is given by mvd. Show that if r is the position of the particle then the vector J = r × mv represents the angular momentum.   b  Now consider a rigid collection of particles  or a solid body  rotating about an axis through the origin, the angular velocity of the collection being represented by ω.   i  Show that the velocity of the ith particle is  and that the total angular momentum J is  J =  ω −  ri · ω ri].  mi[r2 i   ii  Show further that the component of J along the axis of rotation can be written as Iω, where I, the moment of inertia of the collection about the axis or rotation, is given by  vi = ω × ri   cid:4   i   cid:4   i  I =  miρ2 i .  Interpret ρi geometrically.   iii  Prove that the total kinetic energy of the particles is 1  2 Iω2.  7.23  By proceeding as indicated below, prove the parallel axis theorem, which states that, for a body of mass M, the moment of inertia I about any axis is related to the corresponding moment of inertia I0 about a parallel axis that passes through the centre of mass of the body by  where a⊥ is the perpendicular distance between the two axes. Note that I0 can be written as   cid:21   I = I0 + Ma2⊥,   ˆn × r  ·  ˆn × r  dm,   cid:1   where r is the vector position, relative to the centre of mass, of the inﬁnitesimal mass dm and ˆn is a unit vector in the direction of the axis of rotation. Write a similar expression for I in which r is replaced by r position of any point on the axis to which I refers. Use Lagrange’s identity and the fact that r dm = 0  by the deﬁnition of the centre of mass  to establish the result. Without carrying out any further integration, use the results of the previous exercise, the worked example in subsection 6.3.4 and exercise 6.10 to prove that the moment of inertia of a uniform rectangular lamina, of mass M and sides a and b, about an axis perpendicular to its plane and passing through the point  = r− a, where a is the vector   cid:7    αa 2, βb 2 , with −1 ≤ α, β ≤ 1, is  7.24  M 12  [a2 1 + 3α2  + b2 1 + 3β2 ].  238   7.10 EXERCISES  V1  R1 = 50 Ω I1  I2  I3  V0 cos ωt  V4  L  V2  R2  C = 10 µF  V3  Figure 7.18 An oscillatory electric circuit. The power supply has angular frequency ω = 2πf = 400π s  −1.  7.25  7.26  Deﬁne a set of  non-orthogonal  base vectors a = j + k, b = i + k and c = i + j.   a  Establish their reciprocal vectors and hence express the vectors p = 3i−2j+k,  b  Verify that the scalar product p · q has the same value, −5, when evaluated  q = i + 4j and r = −2i + j + k in terms of the base vectors a, b and c.  using either set of components.  Systems that can be modelled as damped harmonic oscillators are widespread; pendulum clocks, car shock absorbers, tuning circuits in television sets and radios, and collective electron motions in plasmas and metals are just a few examples.  In all these cases, one or more variables describing the system obey s  an  equation of the form  ¨x + 2γ˙x + ω2  0 x = P cos ωt,  where ˙x = dx dt, etc. and the inclusion of the factor 2 is conventional. In the steady state  i.e. after the eﬀects of any initial displacement or velocity have been damped out  the solution of the equation takes the form  By expressing each term in the form B cos ω t +  cid:4  , and representing it by a vector of magnitude B making an angle  cid:4  with the x-axis, draw a closed vector diagram, at t = 0, say, that is equivalent to the equation.   a  Convince yourself that whatever the value of ω  > 0  φ must be negative   −π < φ ≤ 0  and that  x t  = A cos ωt + φ .   cid:8    cid:7  −2γω  − ω2  ω2 0  .  −1  φ = tan   b  Obtain an expression for A in terms of P , ω0 and ω.  7.27  According to alternating current theory, the currents and potential diﬀerences in the components of the circuit shown in ﬁgure 7.18 are determined by Kirchhoﬀ’s laws and the relationships  V1 R1  ,  V2 R2  ,  I1 =  I2 =  √−1 in the expression for I3 indicates that the phase of I3 is 90  ◦  I3 = iωCV3,  V4 = iωLI2.  The factor i = ahead of V3. Similarly the phase of V4 is 90  ◦  ahead of I2.  Measurement shows that V3 has an amplitude of 0.661V0 and a phase of relative to that of the power supply. Taking V0 = 1 V, and using a series  ◦  +13.4  239   VECTOR ALGEBRA  of vector plots for potential diﬀerences and currents  they could all be on the same plot if suitable scales were chosen , determine all unknown currents and potential diﬀerences and ﬁnd values for the inductance of L and the resistance of R2.  [ Scales of 1 cm = 0.1 V for potential diﬀerences and 1 cm = 1 mA for currents  are convenient. ]  7.11 Hints and answers   cid:24    c ,  d  and  e .  a  A sphere of radius k centred on the origin;  b  a plane with its normal in the direction of u and at a distance l from the origin;  c  a cone with its axis parallel −1 m;  d  a circular cylinder of radius n with its axis to u and of semiangle cos parallel to u.  a  cos  √ 2;  d  1 Show that q × r is parallel to p; volume = 1 3 Note that  a × b  ·  c × d  = d · [ a × b  × c] and use the result for a triple vector   cid:19  2  c × g  · j = 1 3 . 2  q × r  · p = 5 3 .  2 3;  b  z − x = 2;  c  1    cid:18   −1  1  3  1  product to expand the expression in square brackets. Show that the position vectors of the points are linearly dependent; r = a + λb  where a = i + k and b = −j + k. Show that p must have the direction ˆn× ˆm and write a as xˆn + y ˆm. By obtaining a pair of simultaneous equations for x and y, prove that x =  λ−µˆn· ˆm  [1− ˆn· ˆm 2] and that y =  µ − λˆn · ˆm  [1 −  ˆn · ˆm 2].  a  Note that a − g2 = R2 = 0 − g2, leading to a · a = 2a · g.   cid:7   radius  3  1 2.  −1 −i + j + k , c  cid:7   √ 5 centred on  5, 1,−3 .   b  Make p the new origin and solve the three simultaneous linear equations to  of the three vectors.  b  b k; successive planes through  0, 0, 0  and  a 2, 0, a 2  give a spacing of a   obtain λ = 5 18, µ = 10 18, ν = −3 18, giving g = 2i − k and a sphere of  a  Find two points on both planes, say  0, 0, 0  and  1,−2, 1 , and hence determine the direction cosines of the line of intersection;  b    2 For  c  and  d , treat  c× a ×  a× b  as a triple vector product with c× a as one −1 i + j− k ;  c  a 2 for direction direction i + j; successive planes through  −a 2, 0, 0  and  a 2, 0, 0  give a spacing Note that a2 −  ˆn · a 2 = a2⊥. 2 a− 3 p = −2a + 3b, q = 3 2 b + 5 c · c = 2 and a · b = a · c = b · c = 1. I1 =  7.76,−23.2  , I2 =  14.36,−50.8 With currents in mA and potential diﬀerences in volts: ◦ ◦  , V2 =  0.287,−50.8 V1 =  0.388,−23.2  , I3 =  8.30, 103.4 ◦ ◦  2 c and r = 2a− b− c. Remember that a· a = b· b =  = a √ 3 for direction i + j + k.   ;  , V4 =  0.596, 39.2  −1 i− j + k , d  √ 8 for  of a   = a  = a   ;  ◦  ◦   cid:7   L = 33 mH, R2 = 20 Ω.  7.1 7.3  7.5 7.7 7.9  7.11  7.13  7.15  7.17  7.19  7.21  7.23 7.25  7.27  240   Matrices and vector spaces  In the previous chapter we deﬁned a vector as a geometrical object which has both a magnitude and a direction and which may be thought of as an arrow ﬁxed in our familiar three-dimensional space, a space which, if we need to, we deﬁne by reference to, say, the ﬁxed stars. This geometrical deﬁnition of a vector is both useful and important since it is independent of any coordinate system with which we choose to label points in space.  In most speciﬁc applications, however, it is necessary at some stage to choose a coordinate system and to break down a vector into its component vectors in the directions of increasing coordinate values. Thus for a particular Cartesian coordinate system  for example  the component vectors of a vector a will be axi, ayj and azk and the complete vector will be  a = axi + ayj + azk.   8.1   Although we have so far considered only real three-dimensional space, we may extend our notion of a vector to more abstract spaces, which in general can have an arbitrary number of dimensions N. We may still think of such a vector as an ‘arrow’ in this abstract space, so that it is again independent of any  N- dimensional  coordinate system with which we choose to label the space. As an example of such a space, which, though abstract, has very practical applications, we may consider the description of a mechanical or electrical system. If the state of a system is uniquely speciﬁed by assigning values to a set of N variables, which could be angles or currents, for example, then that state can be represented by a vector in an N-dimensional space, the vector having those values as its components.  In this chapter we ﬁrst discuss general vector spaces and their properties. We then go on to discuss the transformation of one vector into another by a linear operator. This leads naturally to the concept of a matrix, a two-dimensional array of numbers. The properties of matrices are then discussed and we conclude with  8  241   MATRICES AND VECTOR SPACES  a discussion of how to use these properties to solve systems of linear equations. The application of matrices to the study of oscillations in physical systems is taken up in chapter 9.  A set of objects  vectors  a, b, c, . . . is said to form a linear vector space V if:   i  the set is closed under commutative and associative addition, so that   ii  the set is closed under multiplication by a scalar  any complex number  to form a new vector λa, the operation being both distributive and associative so that  8.1 Vector spaces  a + b = b + a,   a + b  + c = a +  b + c ;  λ a + b  = λa + λb,   λ + µ a = λa + µa,  λ µa  =  λµ a,  where λ and µ are arbitrary scalars;   iii  there exists a null vector 0 such that a + 0 = a for all a;   iv  multiplication by unity leaves any vector unchanged, i.e. 1 × a = a;  v  all vectors have a corresponding negative vector −a such that a +  −a  = 0. It follows from  8.5  with λ = 1 and µ = −1 that −a is the same vector as  −1  × a.  We note that if we restrict all scalars to be real then we obtain a real vector space  an example of which is our familiar three-dimensional space ; otherwise, in general, we obtain a complex vector space. We note that it is common to use the terms ‘vector space’ and ‘space’, instead of the more formal ‘linear vector space’. The span of a set of vectors a, b, . . . , s is deﬁned as the set of all vectors that  may be written as a linear sum of the original set, i.e. all vectors  that result from the inﬁnite number of possible values of the  in general complex  scalars α, β, . . . , σ. If x in  8.7  is equal to 0 for some choice of α, β, . . . , σ  not all zero , i.e. if  then the set of vectors a, b, . . . , s, is said to be linearly dependent. In such a set at least one vector is redundant, since it can be expressed as a linear sum of the others. If, however,  8.8  is not satisﬁed by any set of coeﬃcients  other than  x = αa + βb + ··· + σs  αa + βb + ··· + σs = 0,  242   8.2    8.3    8.4    8.5    8.6    8.7    8.8    8.1 VECTOR SPACES  the trivial case in which all the coeﬃcients are zero  then the vectors are linearly independent, and no vector in the set can be expressed as a linear sum of the others.  If, in a given vector space, there exist sets of N linearly independent vectors, but no set of N + 1 linearly independent vectors, then the vector space is said to be N-dimensional.  In this chapter we will limit our discussion to vector spaces of ﬁnite dimensionality; spaces of inﬁnite dimensionality are discussed in chapter 17.   8.1.1 Basis vectors  If V is an N-dimensional vector space then any set of N linearly independent vectors e1, e2, . . . , eN forms a basis for V . If x is an arbitrary vector lying in V then the set of N + 1 vectors x, e1, e2, . . . , eN, must be linearly dependent and therefore such that  where the coeﬃcients α, β, . . . , χ are not all equal to 0, and in particular χ  cid:3 = 0. Rearranging  8.9  we may write x as a linear sum of the vectors ei as follows:  αe1 + βe2 + ··· + σeN + χx = 0, N cid:4   x = x1e1 + x2e2 + ··· + xNeN =  xiei,  i=1   8.9    8.10   for some set of coeﬃcients xi that are simply related to the original coeﬃcients,  e.g. x1 = −α χ, x2 = −β χ, etc. Since any x lying in the span of V can be  expressed in terms of the basis or base vectors ei, the latter are said to form a complete set. The coeﬃcients xi are the components of x with respect to the ei-basis. These components are unique, since if both  x =  xiei  and  x =  yiei,  then   xi − yi ei = 0,   8.11   which, since the ei are linearly independent, has only the solution xi = yi for all i = 1, 2, . . . , N.  From the above discussion we see that any set of N linearly independent vectors can form a basis for an N-dimensional space. If we choose a diﬀerent set  cid:7  i, i = 1, . . . , N then we can write x as e   cid:7  1e   cid:7  1 + x   cid:7  2e   cid:7  2 + ··· + x   cid:7  Ne   cid:7  N =  x = x   cid:7  ie   cid:7  i.  x   8.12   N cid:4   i=1  N cid:4   i=1  N cid:4   i=1  N cid:4   i=1  243   MATRICES AND VECTOR SPACES  We reiterate that the vector x  a geometrical entity  is independent of the basis – it is only the components of x that depend on the basis. We note, however,  that given a set of vectors u1, u2, . . . , uM, where M  cid:3 = N, in an N-dimensional  vector space, then either there exists a vector that cannot be expressed as a linear combination of the ui or, for some vector that can be so expressed, the components are not unique.  8.1.2 The inner product  We may usefully add to the description of vectors in a vector space by deﬁning  the inner product of two vectors, denoted in general by  cid:20 ab cid:21 , which is a scalar function of a and b. The scalar or dot product, a · b ≡ ab cos θ, of vectors  in real three-dimensional space  where θ is the angle between the vectors , was introduced in the last chapter and is an example of an inner product. In eﬀect the  notion of an inner product  cid:20 ab cid:21  is a generalisation of the dot product to more abstract vector spaces. Alternative notations for  cid:20 ab cid:21  are  a, b , or simply a · b.  The inner product has the following properties:   i   cid:20 ab cid:21  =  cid:20 ba cid:21 ∗  ii   cid:20 aλb + µc cid:21  = λ cid:20 ab cid:21  + µ cid:20 ac cid:21 .  ,  We note that in general, for a complex vector space,  i  and  ii  imply that  ∗ cid:20 ac cid:21  + µ  cid:20 λa + µbc cid:21  = λ µ cid:20 ab cid:21 .  cid:20 λaµb cid:21  = λ ∗  ∗ cid:20 bc cid:21 ,   8.13    8.14   Following the analogy with the dot product in three-dimensional real space,  two vectors in a general vector space are deﬁned to be orthogonal if  cid:20 ab cid:21  = 0. Similarly, the norm of a vector a is given by  cid:22 a cid:22  =  cid:20 aa cid:21 1 2 and is clearly a generalisation of the length or modulus a of a vector a in three-dimensional space. In a general vector space  cid:20 aa cid:21  can be positive or negative; however, we shall be primarily concerned with spaces in which  cid:20 aa cid:21  ≥ 0 and which are thus said to have a positive semi-deﬁnite norm. In such a space  cid:20 aa cid:21  = 0 implies a = 0. Let us now introduce into our N-dimensional vector space a basis ˆe1, ˆe2, . . . , ˆeN that has the desirable property of being orthonormal  the basis vectors are mutually orthogonal and each has unit norm , i.e. a basis that has the property  Here δij is the Kronecker delta symbol  of which we say more in chapter 26  and has the properties   8.15    cid:20 ˆeiˆej cid:21  = δij.   δij =  1 for i = j,  0 for i  cid:3 = j.  244   In the above basis we may express any two vectors a and b as  a =  and  b =  bi ˆei.  8.1 VECTOR SPACES  N cid:4  N cid:4   i=1  ai ˆei  i=1  N cid:4   i=1  N cid:4   i=1  Furthermore, in such an orthonormal basis we have, for any a,   cid:20 ˆeja cid:21  =   cid:20 ˆejai ˆei cid:21  =  ai cid:20 ˆejˆei cid:21  = aj.   8.16   Thus the components of a are given by ai =  cid:20 ˆeia cid:21 . Note that this is not true  unless the basis is orthonormal. We can write the inner product of a and b in terms of their components in an orthonormal basis as   cid:20 ab cid:21  =  cid:20 a1 ˆe1 + a2 ˆe2 + ··· + aN ˆeNb1 ˆe1 + b2 ˆe2 + ··· + bN ˆeN cid:21   N cid:4   N cid:4   j cid:3 =i  i=1  i bi cid:20 ˆeiˆei cid:21  + ∗ a  i bj cid:20 ˆeiˆej cid:21  ∗ a  N cid:4  N cid:4   i=1  i=1  =  =  ∗ a i bi,  where the second equality follows from  8.14  and the third from  8.15 . This is clearly a generalisation of the expression  7.21  for the dot product of vectors in three-dimensional space.  We may generalise the above to the case where the base vectors e1, e2, . . . , eN  are not orthonormal  or orthogonal . In general we can deﬁne the N2 numbers   cid:11   Then, if a =  N i=1 aiei and b =   cid:20 ab cid:21  =   cid:11   =  =   8.17   %   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  N cid:4   Gij =  cid:20 eiej cid:21 . $ N cid:4  N i=1 biei, the inner product of a and b is given by N cid:4  N cid:4  N cid:4  N cid:4   i bj cid:20 eiej cid:21  ∗ a  bjej  aiei  j=1  j=1  i=1  i=1   8.18   ∗ a i Gijbj.  i=1  j=1  We further note that from  8.17  and the properties of the inner product we ji. This in turn ensures that  cid:22 a cid:22  =  cid:20 aa cid:21  is real, since then ∗ require Gij = G  cid:20 aa cid:21 ∗  j Gjiai =  cid:20 aa cid:21 . ∗ a  ∗ ∗ j = ija aiG  N cid:4   N cid:4   N cid:4   N cid:4   =  i=1  j=1  j=1  i=1  245   MATRICES AND VECTOR SPACES  For a set of objects  vectors  forming a linear vector space in which  cid:20 aa cid:21  ≥ 0 for  8.1.3 Some useful inequalities  all a, the following inequalities are often useful.   i  Schwarz’s inequality is the most basic result and states that   cid:20 ab cid:21  ≤  cid:22 a cid:22  cid:22 b cid:22 ,   8.19   where the equality holds when a is a scalar multiple of b, i.e. when a = λb. It is important here to distinguish between the absolute value of a scalar,  λ, and the norm of a vector,  cid:22 a cid:22 . Schwarz’s inequality may be proved by  considering   cid:22 a + λb cid:22 2 =  cid:20 a + λba + λb cid:21  =  cid:20 aa cid:21  + λ cid:20 ab cid:21  + λ  ∗ cid:20 ba cid:21  + λλ  ∗ cid:20 bb cid:21 .  If we write  cid:20 ab cid:21  as  cid:20 ab cid:21 eiα then   cid:22 a + λb cid:22 2 =  cid:22 a cid:22 2 + λ2 cid:22 b cid:22 2 + λ cid:20 ab cid:21 eiα + λ  ∗ cid:20 ab cid:21 e However,  cid:22 a + λb cid:22 2 ≥ 0 for all λ, so we may choose λ = re 0 ≤  cid:22 a + λb cid:22 2 =  cid:22 a cid:22 2 + r2 cid:22 b cid:22 2 + 2r cid:20 ab cid:21 .  that, for all r,  −iα. −iα and require  This means that the quadratic equation in r formed by setting the RHS equal to zero must have no real roots. This, in turn, implies that  4 cid:20 ab cid:21 2 ≤ 4 cid:22 a cid:22 2 cid:22 b cid:22 2,  which, on taking the square root  all factors are necessarily positive  of both sides, gives Schwarz’s inequality.   ii  The triangle inequality states that   cid:22 a + b cid:22  ≤  cid:22 a cid:22  +  cid:22 b cid:22    8.20   and may be derived from the properties of the inner product and Schwarz’s inequality as follows. Let us ﬁrst consider   cid:22 a + b cid:22 2 =  cid:22 a cid:22 2 +  cid:22 b cid:22 2 + 2 Re cid:20 ab cid:21  ≤  cid:22 a cid:22 2 +  cid:22 b cid:22 2 + 2 cid:20 ab cid:21 .  Using Schwarz’s inequality we then have   cid:22 a + b cid:22 2 ≤  cid:22 a cid:22 2 +  cid:22 b cid:22 2 + 2 cid:22 a cid:22  cid:22 b cid:22  =   cid:22 a cid:22  +  cid:22 b cid:22  2,  which, on taking the square root, gives the triangle inequality  8.20 .   iii  Bessel’s inequality requires the introduction of an orthonormal basis ˆei,  i = 1, 2, . . . , N into the N-dimensional vector space; it states that   8.21    cid:22 a cid:22 2 ≥ cid:4   i  246   cid:20 ˆeia cid:21 2,   8.2 LINEAR OPERATORS  where the equality holds if the sum includes all N basis vectors. If not all the basis vectors are included in the sum then the inequality results  though of course the equality remains if those basis vectors omitted all have ai = 0 . Bessel’s inequality can also be written  i  ai2,   cid:20 aa cid:21  ≥ cid:4  & a − cid:4   cid:4   cid:20 aˆei cid:21  cid:20 ˆeia cid:21  +  i  =  ’   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20 2   cid:20 ˆeia cid:21 ˆei   cid:20 ˆeia cid:21 ˆei  where the ai are the components of a in the orthonormal basis. From  8.16    cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20 a − cid:4   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20 2  these are given by ai =  cid:20 ˆeia cid:21 . The above may be proved by considering   cid:20  cid:20  cid:20 a − cid:4   cid:20 ˆeja cid:21 ˆej =  cid:20 aˆei cid:21 , we obtain Expanding out the inner product and using  cid:20 ˆeia cid:21 ∗  cid:4   cid:4   cid:20 aˆei cid:21  cid:20 ˆeja cid:21  cid:20 ˆeiˆej cid:21 . =  cid:22 a cid:22 2 − cid:4   Now  cid:20 ˆeiˆej cid:21  = δij , since the basis is orthonormal, and so we ﬁnd   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20 a − cid:4    cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20 a − cid:4   =  cid:20 aa cid:21  − 2   cid:20 ˆeia cid:21 2,   cid:20 ˆeia cid:21 ˆei   cid:20 ˆeia cid:21 ˆei   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20 2  0 ≤  .  j  j  i  i  i  i  i  i  which is Bessel’s inequality.  We take this opportunity to mention also   iv  the parallelogram equality   cid:22 a + b cid:22 2 +  cid:22 a − b cid:22 2 = 2   cid:5  cid:22 a cid:22 2 +  cid:22 b cid:22 2   cid:6   ,   8.22   which may be proved straightforwardly from the properties of the inner product.  8.2 Linear operators  We now discuss the action of linear operators on vectors in a vector space. A linear operator A associates with every vector x another vector  in such a way that, for two vectors a and b,  A  λa + µb  = λA a + µA b,  where λ, µ are scalars. We say that A ‘operates’ on x to give the vector y. We note that the action of A is independent of any basis or coordinate system and  y = A x,  247   MATRICES AND VECTOR SPACES  may be thought of as ‘transforming’ one geometrical entity  i.e. a vector  into another. If we now introduce a basis ei, i = 1, 2, . . . , N, into our vector space then the action of A on each of the basis vectors is to produce a linear combination of the latter; this may be written as  N cid:4   i=1  A ej =  Aij ei,   8.23   where Aij is the ith component of the vector A ej in this basis; collectively the numbers Aij are called the components of the linear operator in the ei-basis. In this basis we can express the relation y = A x in component form as  N cid:4   i=1  y =  yiei = A   N cid:4   j=1   =  N cid:4   N cid:4   j=1  i=1  xjej  xj  Aijei,  and hence, in purely component form, in this basis we have  yi =  Aij xj.   8.24    cid:7  i, y  If we had chosen a diﬀerent basis e are x represented in this new basis by  i, in which the components of x, y and A  cid:7  ij respectively then the geometrical relationship y = A x would be  cid:7   cid:7  i and A N cid:4    cid:7  i = y   cid:7  ij x A   cid:7  j.  j=1  We have so far assumed that the vector y is in the same vector space as x. If, however, y belongs to a diﬀerent vector space, which may in general be  M-dimensional  M  cid:3 = N  then the above analysis needs a slight modiﬁcation. By introducing a basis set fi, i = 1, 2, . . . , M, into the vector space to which y belongs we may generalise  8.23  as  A ej =  Aijfi,  where the components Aij of the linear operator A relate to both of the bases ej and fi.  N cid:4   j=1  M cid:4   i=1  248   If x is a vector and A and B are two linear operators then it follows that  8.2.1 Properties of linear operators  8.3 MATRICES   A + B  x = A x + B x,   λA  x = λ A x ,  AB  x = A  B x ,  where in the last equality we see that the action of two linear operators in succession is associative. The product of two linear operators is not in general  commutative, however, so that in general AB x  cid:3 = B A x. In an obvious way we  deﬁne the null  or zero  and identity operators by  O x = 0  and  I x = x,  for any vector x in our vector space. Two operators A and B are equal if A x = B x for all vectors x. Finally, if there exists an operator A−1 such that  AA−1 = A−1 A = I  then A−1 is the inverse of A . Some linear operators do not possess an inverse and are called singular, whilst those operators that do have an inverse are termed non-singular.  8.3 Matrices  We have seen that in a particular basis ei both vectors and linear operators can be described in terms of their components with respect to the basis. These components may be displayed as an array of numbers called a matrix. In general,  if a linear operator A transforms vectors from an N-dimensional vector space, for which we choose a basis ej, j = 1, 2, . . . , N, into vectors belonging to an M-dimensional vector space, with basis fi, i = 1, 2, . . . , M, then we may represent the operator A by the matrix   A11  A =  A12 A21 A22 ... ... AM1 AM2  . . . A1N . . . A2N ... . . . . . . AMN   .   8.25   The matrix elements Aij are the components of the linear operator with respect to the bases ej and fi; the component Aij of the linear operator appears in the ith row and jth column of the matrix. The array has M rows and N columns and is thus called an M × N matrix. If the dimensions of the two vector spaces may represent A by an N × N or square matrix of order N. The component Aij, which in general may be complex, is also denoted by  A ij.  are the same, i.e. M = N  for example, if they are the same vector space  then we  249   MATRICES AND VECTOR SPACES  In a similar way we may denote a vector x in terms of its components xi in a  basis ei, i = 1, 2, . . . , N, by the array   ,   x1  x2 ... xN  x =   .   x   cid:7   cid:7  1 x 2 ...  cid:7  x  N  x cid:7   =  which is a special case of  8.25  and is called a column matrix  or conventionally, and slightly confusingly, a column vector or even just a vector – strictly speaking the term ‘vector’ refers to the geometrical entity x . The column matrix x can also be written as  x =  x1 x2  ···  xN T,  which is the transpose of a row matrix  see section 8.6 .  We note that in a diﬀerent basis e  diﬀerent column matrix containing the components x   cid:7  i the vector x would be represented by a   cid:7  i in the new basis, i.e.  Thus, we use x and x cid:7  to denote diﬀerent column matrices which, in diﬀerent bases  cid:7  i, represent the same vector x. In many texts, however, this distinction is ei and e not made and x  rather than x  is equated to the corresponding column matrix; if we regard x as the geometrical entity, however, this can be misleading and so we explicitly make the distinction. A similar argument follows for linear operators; the same linear operator A is described in diﬀerent bases by diﬀerent matrices A and A cid:7   , containing diﬀerent matrix elements.  8.4 Basic matrix algebra  The basic algebra of matrices may be deduced from the properties of the linear operators that they represent. In a given basis the action of two linear operators A and B on an arbitrary vector x  see the beginning of subsection 8.2.1 , when written in terms of components using  8.24 , is given by   cid:4   cid:4   cid:4   j  j   cid:4   cid:4   cid:4   j  j   cid:4   j   cid:4    cid:4    A + B ij xj =  Aijxj +  Bijxj,   λA ij xj = λ  Aij xj,   AB ij xj =  Aik Bx k =  AikBkjxj.  j  k  j  k  250   8.4 BASIC MATRIX ALGEBRA  Now, since x is arbitrary, we can immediately deduce the way in which matrices are added or multiplied, i.e.   A + B ij = Aij + Bij,   λA ij = λAij,  AB ij =  AikBkj.   cid:4   k   8.26    8.27    8.28   We note that a matrix element may, in general, be complex. We now discuss matrix addition and multiplication in more detail.  8.4.1 Matrix addition and multiplication by a scalar  From  8.26  we see that the sum of two matrices, S = A + B, is the matrix whose elements are given by  Sij = Aij + Bij   cid:7   for every pair of subscripts i, j, with i = 1, 2, . . . , M and j = 1, 2, . . . , N. For  example, if A and B are 2 × 3 matrices then S = A + B is given by  cid:8   A11 A12 A13 A21 A22 A23  B11 B12 B13 B21 B22 B23  S11 S21  S12 S22  S13 S23  =  +   cid:7   cid:7    cid:8    cid:8    cid:7    cid:8   =  A11 + B11 A12 + B12 A13 + B13 A21 + B21 A22 + B22 A23 + B23  .   8.29   Clearly, for the sum of two matrices to have any meaning, the matrices must have  the same dimensions, i.e. both be M × N matrices. From deﬁnition  8.29  it follows that A + B = B + A and that the sum of a number of matrices can be written unambiguously without bracketting, i.e. matrix addition is commutative and associative.  The diﬀerence of two matrices is deﬁned by direct analogy with addition. The  matrix D = A − B has elements Dij = Aij − Bij,  cid:7   elements λAij , for example  for i = 1, 2, . . . , M, j = 1, 2, . . . , N.   8.30   From  8.27  the product of a matrix A with a scalar λ is the matrix with  λ  A11 A12 A13 A21 A22 A23  =  λ A11 λ A21  λ A12 λ A22  λ A13 λ A23   8.31   Multiplication by a scalar is distributive and associative.   cid:8   .   cid:8    cid:7   251   MATRICES AND VECTOR SPACES   cid:7    cid:1 The matrices A, B and C are given by   cid:7    cid:8  2 −1 Find the matrix D = A + 2B − C.  cid:8   A =   cid:7   B =  1  3  ,   cid:8    cid:7  −2  −1   cid:8   .  1 1  ,  C =  0  1  0 −2  cid:8    cid:7  −2   cid:8   cid:7   1  3  + 2  2 −1 0 −2 −1 2 + 2 × 1 −  −2  −1 + 2 × 0 − 1 1 + 2 ×  −2  − 1 3 + 2 × 0 −  −1   −  1  0   cid:8   1 1  =   cid:8   6 −2 4 −4  .  cid:2    cid:7   cid:7   D =  =  From the above considerations we see that the set of all, in general complex,  M × N matrices  with ﬁxed M and N  forms a linear vector space of dimension MN. One basis for the space is the set of M × N matrices E p,q  with the property that E p,q  ij = 0 for all other values of i and j, i.e. each matrix has only one non-zero entry, which equals unity. Here the pair  p, q  is simply a label that picks out a particular one of the matrices E p,q , the total number of which is MN.  ij = 1 if i = p and j = q whilst E p,q   8.4.2 Multiplication of matrices  Let us consider again the ‘transformation’ of one vector into another, y = A x, which, from  8.24 , may be described in terms of components with respect to a particular basis as  j=1  N cid:4   A11   =   y1  y2 ... yM  yi =  Aij xj  for i = 1, 2, . . . , M.   8.32         x1 x2 ...  xN  A12 A21 A22 ... ... AM1 AM2  . . . A1N . . . A2N ... . . . . . . AMN   8.33   Writing this in matrix form as y = Ax we have  where we have highlighted with boxes the components used to calculate the element y2: using  8.32  for i = 2,  y2 = A21x1 + A22x2 + ··· + A2N xN.  All the other components yi are calculated similarly. If instead we operate with A on a basis vector ej having all components zero  252   8.4 BASIC MATRIX ALGEBRA  except for the jth, which equals unity, then we ﬁnd   A11  Aej =  A12 A21 A22 ... ... AM1 AM2  . . . A1N . . . A2N ... . . . . . . AMN   =   A1j  A2j ... AMj   ,      0 0 ... 1 ... 0  and so conﬁrm our identiﬁcation of the matrix element Aij as the ith component of Aej in this basis.  From  8.28  we can extend our discussion to the product of two matrices P = AB, where P is the matrix of the quantities formed by the operation of the rows of A on the columns of B, treating each column of B in turn as the vector x represented in component form in  8.32 . It is clear that, for this to be a meaningful deﬁnition, the number of columns in A must equal the number of rows in B. Thus the product AB of an M × N matrix A with an N × R matrix B is itself an M × R matrix P, where  N cid:4   k=1  Pij =  AikBkj  for i = 1, 2, . . . , M,  j = 1, 2, . . . , R.  For example, P = AB may be written in matrix form   cid:30    cid:31    cid:7   P11 P21  P12 P22  =  A11 A12 A13 A21 A22 A23   cid:8  B11 B12  B21 B22 B31 B32    where  P11 = A11B11 + A12B21 + A13B31,  P21 = A21B11 + A22B21 + A23B31,  P12 = A11B12 + A12B22 + A13B32,  P22 = A21B12 + A22B22 + A23B32.  Multiplication of more than two matrices follows naturally and is associative.  So, for example,  A BC  ≡  AB C,   8.34   provided, of course, that all the products are deﬁned.  As mentioned above, if A is an M × N matrix and B is an N × M matrix then  two product matrices are possible, i.e.  P = AB  and  Q = BA.  253   MATRICES AND VECTOR SPACES  These are clearly not the same, since P is an M × M matrix whilst Q is an N × N matrix. Thus, particular care must be taken to write matrix products in the intended order; P = AB but Q = BA. We note in passing that A2 means AA, A3 means A AA  =  AA A etc. Even if both A and B are square, in general  AB  cid:3 = BA,   8.35   i.e. the multiplication of matrices is not, in general, commutative.   cid:1 Evaluate P = AB and Q = BA where 2 −1   3   ,  A =  3  0  1 −3  2 4  B =   2 −2  1 3  1 2   .  3 0 1  As we saw for the 2 × 2 case above, the element Pij of the matrix P = AB is found by mentally taking the ‘scalar product’ of the ith row of A with the jth column of B. For example, P11 = 3 × 2 + 2 × 1 +  −1  × 3 = 5, P12 = 3 ×  −2  + 2 × 1 +  −1  × 2 = −6, etc.  Thus  P = AB =  and, similarly,  2 −1  0   3  2 −2  1 −3  3   2 −2  3  1 3  1 2  2 4  2 −1  3 0 1   =  =   5 −6  9 −11  9 11  7 3  3 0 1   ,  .  8 2 7  6 1 5  Q = BA =  5 9 These results illustrate that, in general, two matrices do not commute.  cid:2   1 −3  3 10  2 4  1 3  1 2  3  0  The property that matrix multiplication is distributive over addition, i.e. that   A + B C = AC + BC  C A + B  = CA + CB,   8.36    8.37   and  follows directly from its deﬁnition.  8.4.3 The null and identity matrices  Both the null matrix and the identity matrix are frequently encountered, and we take this opportunity to introduce them brieﬂy, leaving their uses until later. The null or zero matrix 0 has all elements equal to zero, and so its properties are  A0 = 0 = 0A,  A + 0 = 0 + A = A.  254   8.5 FUNCTIONS OF MATRICES  The identity matrix I has the property  It is clear that, in order for the above products to be deﬁned, the identity matrix  must be square. The N × N identity matrix  often denoted by IN  has the form  AI = IA = A.    IN =   .  1  0  1  0 ... 0 ···  ···  . . . 0  0 ...  0 1  8.5 Functions of matrices   n times ,  An = AA··· A  cid:4   S =  anAn,  n  If a matrix A is square then, as mentioned above, one can deﬁne powers of A in a straightforward way. For example A2 = AA, A3 = AAA, or in the general case  where n is a positive integer. Having deﬁned powers of a square matrix A, we may construct functions of A of the form  where the ak are simple scalars and the number of terms in the summation may be ﬁnite or inﬁnite. In the case where the sum has an inﬁnite number of terms, the sum has meaning only if it converges. A common example of such a function is the exponential of a matrix, which is deﬁned by  exp A =   8.38   This deﬁnition can, in turn, be used to deﬁne other functions such as sin A and cos A.  8.6 The transpose of a matrix  We have seen that the components of a linear operator in a given coordinate sys- tem can be written in the form of a matrix A. We will also ﬁnd it useful, however, to consider the diﬀerent  but clearly related  matrix formed by interchanging the rows and columns of A. The matrix is called the transpose of A and is denoted by AT.  ∞ cid:4   n=0  An  n!  .  255   MATRICES AND VECTOR SPACES   cid:1 Find the transpose of the matrix  By interchanging the rows and columns of A we immediately obtain  3 0   cid:7   3  1 2  1 4  0 4 1  .  2 1   cid:8   .  cid:2   A =  AT =  It is obvious that if A is an M × N matrix then its transpose AT is a N × M  matrix. As mentioned in section 8.3, the transpose of a column matrix is a row matrix and vice versa. An important use of column and row matrices is in the representation of the inner product of two real vectors in terms of their components in a given basis. This notion is discussed fully in the next section, where it is extended to complex vectors.  The transpose of the product of two matrices,  AB T, is given by the product  of their transposes taken in the reverse order, i.e.   AB T = BTAT.   8.39   This is proved as follows:   AB T  ij =  AB ji =  AjkBki   cid:4   cid:4   k  k   cid:4   k  =   AT kj BT ik =   BT ik AT kj =  BTAT ij,  and the proof can be extended to the product of several matrices to give   ABC··· G T = GT ··· CTBTAT.  8.7 The complex and Hermitian conjugates of a matrix  Two further matrices that can be derived from a given general M × N matrix are the complex conjugate, denoted by A∗ by A†  , and the Hermitian conjugate, denoted  .  The complex conjugate of a matrix A is the matrix obtained by taking the  complex conjugate of each of the elements of A, i.e.  Obviously if a matrix is real  i.e. it contains only real elements  then A∗  = A.   A∗  ∗  ij =  Aij    .  256   8.7 THE COMPLEX AND HERMITIAN CONJUGATES OF A MATRIX   cid:1 Find the complex conjugate of the matrix 1  A =  2 1  3i 0  .  1 + i   cid:7    cid:7    cid:8    cid:8   By taking the complex conjugate of each element we obtain immediately  A∗  =  1  1 − i  2 −3i  1  0  .  cid:2   The Hermitian conjugate, or adjoint, of a matrix A is the transpose of its  complex conjugate, or equivalently, the complex conjugate of its transpose, i.e.  A† We note that if A is real  and so A∗ = AT, and taking the Hermitian conjugate is equivalent to taking the transpose. Following the previous line of argument for the transpose of the product of several matrices, the Hermitian conjugate of such a product can be shown to be given by  =  A∗ ∗  T =  AT  = A  then A†  .   8.40    cid:1 Find the Hermitian conjugate of the matrix 1  Taking the complex conjugate of A and then forming the transpose we ﬁnd  .  †  AB··· G   cid:7   1  = G† ··· B†A†  cid:8   .  1 − i  1 + i  A =  A†  3i 0  2 1  =  2  .  −3i  1 0  We obtain the same result, of course, if we ﬁrst take the transpose of A and then take the complex conjugate.  cid:2   An important use of the Hermitian conjugate  or transpose in the real case  is in connection with the inner product of two vectors. Suppose that in a given orthonormal basis the vectors a and b may be represented by the column matrices  a =  and  b =   8.41    .   b1  b2 ... bN     a1  a2 ... aN  257  Taking the Hermitian conjugate of a, to give a row matrix, and multiplying  on   the right  by b we obtain  MATRICES AND VECTOR SPACES   =  N cid:4   i=1   b1  b2 ... bN  ∗ a i bi,  ∗ a†b =  a ∗ 1 a 2  ∗ ··· a N   cid:11    cid:20 eiej cid:21  = Gij  cid:3 = δij, N cid:4   N cid:4    cid:20 ab cid:21  =  i Gij bj = a†Gb, ∗ a  which is the expression for the inner product  cid:20 ab cid:21  in that basis. We note that for real vectors  8.42  reduces to aTb =  N i=1 aibi.  If the basis ei is not orthonormal, so that, in general,  then, from  8.18 , the scalar product of a and b in terms of their components with respect to this basis is given by   8.42   where G is the N × N matrix with elements Gij.  i=1  j=1  8.8 The trace of a matrix  For a given matrix A, in the previous two sections we have considered various other matrices that can be derived from it. However, sometimes one wishes to derive a single number from a matrix. The simplest example is the trace  or spur  of a square matrix, which is denoted by Tr A. This quantity is deﬁned as the sum of the diagonal elements of the matrix,  Tr A = A11 + A22 + ··· + ANN =  Aii.   8.43   N cid:4   i=1  It is clear that taking the trace is a linear operation so that, for example,  Tr A ± B  = Tr A ± Tr B.  A very useful property of traces is that the trace of the product of two matrices is independent of the order of their multiplication; this results holds whether or not the matrices commute and is proved as follows:  N cid:4   N cid:4   N cid:4   N cid:4   Tr AB =   AB ii =  Aij Bji =  BjiAij =   BA jj = Tr BA.  i=1  j=1  i=1  j=1   8.44   N cid:4   j=1  N cid:4   i=1  The result can be extended to the product of several matrices. For example, from  8.44 , we immediately ﬁnd  Tr ABC = Tr BCA = Tr CAB,  258   8.9 THE DETERMINANT OF A MATRIX  which shows that the trace of a multiple product is invariant under cyclic permutations of the matrices in the product. Other easily derived properties of the trace are, for example, Tr AT = Tr A and Tr A†  ∗ =  Tr A   .  8.9 The determinant of a matrix  For a given matrix A, the determinant det A  like the trace  is a single number  or algebraic expression  that depends upon the elements of A. Also like the trace, the determinant is deﬁned only for square matrices. If, for example, A is a 3 × 3  matrix then its determinant, of order 3, is denoted by   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  A11 A12 A13  A21 A22 A23 A31 A32 A33   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  .  det A = A =   8.45   In order to calculate the value of a determinant, we ﬁrst need to introduce the notions of the minor and the cofactor of an element of a matrix.  We shall see that we can use the cofactors to write an order-3 determinant as the weighted sum of three order-2 determinants, thereby simplifying its evaluation.   The minor Mij of the element Aij of an N × N matrix A is the determinant of the  N − 1  ×  N − 1  matrix obtained by removing all the elements of the ith row and jth column of A; the associated cofactor, Cij , is found by multiplying the minor by  −1 i+j .  cid:1 Find the cofactor of the element A23 of the matrix   A11 A12 A13  A21 A22 A23 A31 A32 A33   .  A =   cid:20  cid:20  cid:20  cid:20  A11 A12  cid:20  cid:20  cid:20  cid:20  A11 A12  A31 A32  A31 A32   cid:20  cid:20  cid:20  cid:20  .  cid:20  cid:20  cid:20  cid:20  .  cid:2   M23 =  C23 = −  Removing all the elements of the second row and third column of A and forming the determinant of the remaining terms gives the minor  Multiplying the minor by  −1 2+3 =  −1 5 = −1 gives  We now deﬁne a determinant as the sum of the products of the elements of any row or column and their corresponding cofactors, e.g. A21C21 + A22C22 + A23C23 or A13C13 + A23C23 + A33C33. Such a sum is called a Laplace expansion. For example, in the ﬁrst of these expansions, using the elements of the second row of the  259   MATRICES AND VECTOR SPACES  determinant deﬁned by  8.45  and their corresponding cofactors, we write A as  the Laplace expansion  A = A21 −1  2+1 M21 + A22 −1  2+2 M22 + A23 −1  2+3 M23   cid:20  cid:20  cid:20  cid:20  A12 A13  A32 A33   cid:20  cid:20  cid:20  cid:20  + A22   cid:20  cid:20  cid:20  cid:20  A11 A13  A31 A33   cid:20  cid:20  cid:20  cid:20  − A23   cid:20  cid:20  cid:20  cid:20  A11 A12  A31 A32  = −A21   cid:20  cid:20  cid:20  cid:20  .  We will see later that the value of the determinant is independent of the row  or column chosen. Of course, we have not yet determined the value of A but,  rather, written it as the weighted sum of three determinants of order 2. However, applying again the deﬁnition of a determinant, we can evaluate each of the order-2 determinants.   cid:1 Evaluate the determinant  A32 A33   cid:20  cid:20  cid:20  cid:20  .   cid:20  cid:20  cid:20  cid:20  A12 A13  cid:20  cid:20  cid:20  cid:20  = A12 −1  1+1 A33 + A13 −1  1+2 A32  = A12A33 − A13A32,   cid:20  cid:20  cid:20  cid:20  A12 A13  A32 A33  By considering the products of the elements of the ﬁrst row in the determinant, and their corresponding cofactors, we ﬁnd  where the values of the order-1 determinants A33 and A32 are deﬁned to be A33 and A32 e.g. det  −2  =  − 2 = −2, not 2.  cid:2   respectively. It must be remembered that the determinant is not the same as the modulus,  We can now combine all the above results to show that the value of the  determinant  8.45  is given by  A = −A21 A12A33 − A13A32  + A22 A11A33 − A13A31  − A23 A11A32 − A12A31  = A11 A22A33 − A23A32  + A12 A23A31 − A21A33  + A13 A21A32 − A22A31 ,   8.46    8.47   where the ﬁnal expression gives the form in which the determinant is usually remembered and is the form that is obtained immediately by considering the Laplace expansion using the ﬁrst row of the determinant. The last equality, which essentially rearranges a Laplace expansion using the second row into one using the ﬁrst row, supports our assertion that the value of the determinant is unaﬀected by which row or column is chosen for the expansion.  260   8.9 THE DETERMINANT OF A MATRIX   cid:1 Suppose the rows of a real 3 × 3 matrix A are interpreted as the components in a given basis of three  three-component  vectors a, b and c. Show that one can write the determinant of A as  A = a ·  b × c .  If one writes the rows of A as the components in a given basis of three vectors a, b and c, we have from  8.47  that   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  = a1 b2c3 − b3c2  + a2 b3c1 − b1c3  + a3 b1c2 − b2c1 .   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  a1  b1 c1  A =  a2 b2 c2  a3 b3 c3  A = a ·  b × c .  From expression  7.34  for the scalar triple product given in subsection 7.6.3, it follows that we may write the determinant as  In other words, A is the volume of the parallelepiped deﬁned by the vectors a, b and c.  One could equally well interpret the columns of the matrix A as the components of  and more meaningful  expression than  8.47  for the value of a 3× 3 determinant. Indeed, three vectors, and result  8.48  would still hold.  This result provides a more memorable not linearly independent then the value of the determinant vanishes: A = 0.  cid:2   using this geometrical interpretation, we see immediately that, if the vectors a1, a2, a3 are   8.48   The evaluation of determinants of order greater than 3 follows the same general method as that presented above, in that it relies on successively reducing the order of the determinant by writing it as a Laplace expansion. Thus, a determinant of order 4 is ﬁrst written as a sum of four determinants of order 3, which are then evaluated using the above method. For higher-order determinants, one  that given in  8.48 . Nevertheless, it is still true that if the rows or columns of  cannot write down directly a simple geometrical expression for A analogous to the N × N matrix A are interpreted as the components in a given basis of N  N-component  vectors a1, a2, . . . , aN, then the determinant A vanishes if these  vectors are not all linearly independent.  8.9.1 Properties of determinants  A number of properties of determinants follow straightforwardly from the deﬁni- tion of det A; their use will often reduce the labour of evaluating a determinant. We present them here without speciﬁc proofs, though they all follow readily from the alternative form for a determinant, given in equation  26.29  on page 942, and expressed in terms of the Levi–Civita symbol  cid:4 ijk  see exercise 26.9 .   i  Determinant of the transpose. The transpose matrix AT  which, we recall, is obtained by interchanging the rows and columns of A  has the same determinant as A itself, i.e.  AT = A.  261   8.49    MATRICES AND VECTOR SPACES  It follows that any theorem established for the rows of A will apply to the columns as well, and vice versa. matrix A∗ has the determinant A∗ = A∗   ii  Determinant of the complex and Hermitian conjugate. It is clear that the obtained by taking the complex conjugate of each element of A . Combining this result with  8.49 , we ﬁnd  that  A† =  A∗   T = A∗ = A∗  .   8.50    iii  Interchanging two rows or two columns. If two rows  columns  of A are interchanged, its determinant changes sign but is unaltered in magnitude.  iv  Removing factors. If all the elements of a single row  column  of A have a common factor, λ, then this factor may be removed; the value of the determinant is given by the product of the remaining determinant and λ. Clearly this implies that if all the elements of any row  column  are zero  then A = 0. It also follows that if every element of the N × N matrix A  is multiplied by a constant factor λ then  λA = λNA.   8.51    v  Identical rows or columns. If any two rows  columns  of A are identical or  are multiples of one another, then it can be shown that A = 0.   vi  Adding a constant multiple of one row  column  to another. The determinant of a matrix is unchanged in value by adding to the elements of one row  column  any ﬁxed multiple of the elements of another row  column .   vii  Determinant of a product. If A and B are square matrices of the same order  then  AB = AB = BA.   8.52   A simple extension of this property gives, for example,  AB··· G = AB···G = AG···B = A··· GB,  which shows that the determinant is invariant under permutation of the matrices in a multiple product.  There is no explicit procedure for using the above results in the evaluation of any given determinant, and judging the quickest route to an answer is a matter of experience. A general guide is to try to reduce all terms but one in a row or column to zero and hence in eﬀect to obtain a determinant of smaller size. The steps taken in evaluating the determinant in the example below are certainly not the fastest, but they have been chosen in order to illustrate the use of most of the properties listed above.  262    cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  1  0   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  1  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  1   cid:1 Evaluate the determinant  8.10 THE INVERSE OF A MATRIX   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  .  A =  2  0  3 1  1 −2 4 −2 1 −2 −1  3 −3 −2   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  .   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  ,  Taking a factor 2 out of the third column and then adding the second column to the third gives  A = 2  1  0  3 1  1 −1 2 −2 1 −1 −1  3 −3 −2  0 1  1 0  3 1  0  3 −3 −1 −2 0 −1 −2  Subtracting the second column from the fourth gives   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  = 2   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  1   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  1  0  A = 2  0 1  1 0  0  3 −3 −1 −2  1  0 −2  3 0 1  1   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  .  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  4   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  = 2  We now note that the second row has only one non-zero element and so the determinant may conveniently be written as a Laplace expansion, i.e.  A = 2 × 1 ×  −1 2+2  1  3 −1 −2  0 −2  3 1  0  3 −1 −2  0 −2  4 1  where the last equality follows by adding the second row to the ﬁrst. It can now be seen that the ﬁrst row is minus twice the third, and so the value of the determinant is zero, by property  v  above.  cid:2   8.10 The inverse of a matrix  Our ﬁrst use of determinants will be in deﬁning the inverse of a matrix. If we were dealing with ordinary numbers we would consider the relation P = AB as equivalent to B = P A, provided that A  cid:3 = 0. However, if A, B and P are matrices  then this notation does not have an obvious meaning. What we really want to know is whether an explicit formula for B can be obtained in terms of A and P. It will be shown that this is possible for those cases in which A  cid:3 = 0. A  square matrix whose determinant is zero is called a singular matrix; otherwise it is non-singular. We will show that if A is non-singular we can deﬁne a matrix, denoted by A−1 and called the inverse of A, which has the property that if AB = P then B = A−1P. In words, B can be obtained by multiplying P from the left by A−1. Analogously, if B is non-singular then, by multiplication from the right, A = PB−1.  It is clear that   8.53  where I is the unit matrix, and so A−1A = I = AA−1. These statements are  AI = A ⇒ I = A−1A,  263   MATRICES AND VECTOR SPACES  equivalent to saying that if we ﬁrst multiply a matrix, B say, by A and then multiply by the inverse A−1, we end up with the matrix we started with, i.e.  A−1AB = B.   8.54   This justiﬁes our use of the term inverse. It is also clear that the inverse is only deﬁned for square matrices.  So far we have only deﬁned what we mean by the inverse of a matrix. Actually ﬁnding the inverse of a matrix A may be carried out in a number of ways. We will show that one method is to construct ﬁrst the matrix C containing the cofactors of the elements of A, as discussed in the last subsection. Then the required inverse A−1 can be found by forming the transpose of C and dividing by the determinant of A. Thus the elements of the inverse A−1 are given by  That this procedure does indeed result in the inverse may be seen by considering   A−1 ik =  cid:4  the components of A−1A, i.e.  A−1A ij =   C T Cki A = A . ik  cid:4   Cki A Akj =  A A δij.  The last equality in  8.56  relies on the property  k  k   A−1 ik A kj =  cid:4   CkiAkj = Aδij;  k   8.55    8.56    8.57   this can be proved by considering the matrix A cid:7  obtained from the original matrix A when the ith column of A is replaced by one of the other columns, say the jth. Thus A cid:7  is a matrix with two identical columns and so has zero determinant. However, replacing the ith column by another does not change the cofactors Cki of the elements in the ith column, which are therefore the same in A and A cid:7  .  Recalling the Laplace expansion of a determinant, i.e.  we obtain  0 = A cid:7  =   cid:7  kiC A   cid:7  ki =  AkjCki,  i  cid:3 = j,  which together with the Laplace expansion itself may be summarised by  8.57 .  It is immediately obvious from  8.55  that the inverse of a matrix is not deﬁned  if the matrix is singular  i.e. if A = 0 .  A =  cid:4   k  AkiCki,   cid:4   k   cid:4   k  264    cid:1 Find the inverse of the matrix  8.10 THE INVERSE OF A MATRIX   2  A =  4  3  1 −2 −2 −3  2  3   .  We ﬁrst determine A:  A = 2[−2 2  −  −2 3] + 4[ −2  −3  −  1  2 ] + 3[ 1  3  −  −2  −3 ]  This is non-zero and so an inverse matrix can be constructed. To do this we need the matrix of the cofactors, C, and hence CT. We ﬁnd  = 11.   2  1  −2  C =    4  −3 13 −18 −8  7  and hence  A−1 =  CT A =  1 11   ,  −2 −3 −18 −8  1 13  7  4   2  .  cid:2   7   8.58    8.59   For a 2 × 2 matrix, the inverse has a particularly simple form. If the matrix is  then its determinant A is given by A = A11A22 − A12A21, and the matrix of  cofactors is   cid:8   .  Thus the inverse of A is given by  A−1 =  CT A =  1  A11A22 − A12A21  A22 −A12 −A21  A11   8.60   It can be seen that the transposed matrix of cofactors for a 2 × 2 matrix is the  same as the matrix formed by swapping the elements on the leading diagonal  A11 and A22  and changing the signs of the other two elements  A12 and A21 .  This is completely general for a 2 × 2 matrix and is easy to remember.  The following are some further useful properties related to the inverse matrix  265  and  CT =   2  4  1 13  −2 −3 −18 −8  cid:8    cid:7   A =  A11 A12 A21 A22   cid:7   C =   cid:8   .  A11  A22 −A21 −A12  cid:7    and may be straightforwardly derived.  MATRICES AND VECTOR SPACES  −1 = A.  i   A−1  −1 =  A−1 T.  ii   AT   iii   A† −1 =  A−1  †   −1 = B−1A−1.  iv   AB   v   AB··· G   .  −1 = G−1 ··· B−1A−1.   cid:1 Prove the properties  i – v  stated above.  We begin by writing down the fundamental expression deﬁning the inverse of a non- singular square matrix A:  AA−1 = I = A−1A.   8.61   Property  i . This follows immediately from the expression  8.61 . Property  ii . Taking the transpose of each expression in  8.61  gives   AA−1 T = IT =  A−1A T.   A−1 TAT = I = AT A−1 T.  Using the result  8.39  for the transpose of a product of matrices and noting that IT = I, we ﬁnd  However, from  8.61 , this implies  A−1 T =  AT   −1 and hence proves result  ii  above.  Property  iii . This may be proved in an analogous way to property  ii , by replacing the transposes in  ii  by Hermitian conjugates and using the result  8.40  for the Hermitian conjugate of a product of matrices.  Property  iv . Using  8.61 , we may write   AB  AB −1 = I =  AB −1 AB ,  From the left-hand equality it follows, by multiplying on the left by A−1, that  A−1AB AB −1 = A−1I  and hence  B AB −1 = A−1.  Now multiplying on the left by B−1 gives  B−1B AB −1 = B−1A−1,  and hence the stated result.  Property  v . Finally, result  iv  may extended to case  v  in a straightforward manner.  For example, using result  iv  twice we ﬁnd   ABC −1 =  BC −1A−1 = C−1B−1A−1.  cid:2   We conclude this section by noting that the determinant A−1 of the inverse matrix can be expressed very simply in terms of the determinant A of the matrix  itself. Again we start with the fundamental expression  8.61 . Then, using the property  8.52  for the determinant of a product, we ﬁnd  AA−1 = AA−1 = I.  It is straightforward to show by Laplace expansion that I = 1, and so we arrive  at the useful result   8.62   A−1 =  1A .  266   8.11 THE RANK OF A MATRIX  8.11 The rank of a matrix  The rank of a general M × N matrix is an important concept, particularly in  the solution of sets of simultaneous linear equations, to be discussed in the next section, and we now discuss it in some detail. Like the trace and determinant, the rank of matrix A is a single number  or algebraic expression  that depends on the elements of A. Unlike the trace and determinant, however, the rank of a matrix can be deﬁned even when A is not square. As we shall see, there are two equivalent deﬁnitions of the rank of a general matrix.  Firstly, the rank of a matrix may be deﬁned in terms of the linear independence  of vectors. Suppose that the columns of an M × N matrix are interpreted as the components in a given basis of N  M-component  vectors v1, v2, . . . , vN, as follows:   ↑  v1 ↓  ↑ v2 ↓  A =  . . .   .  ↑ vN ↓  Then the rank of A, denoted by rank A or by R A , is deﬁned as the number of linearly independent vectors in the set v1, v2, . . . , vN, and equals the dimension of the vector space spanned by those vectors. Alternatively, we may consider the rows of A to contain the components in a given basis of the M  N-component  vectors w1, w2, . . . , wM as follows:    A =   .  ← w1 → ← w2 → ...← wM →  §  that the rank of A is also equal to the number of It may then be shown linearly independent vectors in the set w1, w2, . . . , wM. From this deﬁnition it is should be clear that the rank of A is unaﬀected by the exchange of two rows  or two columns  or by the multiplication of a row  or column  by a constant. Furthermore, suppose that a constant multiple of one row  column  is added to another row  column : for example, we might replace the row wi by wi + cwj. This also has no eﬀect on the number of linearly independent rows and so leaves the rank of A unchanged. We may use these properties to evaluate the rank of a given matrix.  A second  equivalent  deﬁnition of the rank of a matrix may be given and uses the concept of submatrices. A submatrix of A is any matrix that can be formed from the elements of A by ignoring one, or more than one, row or column. It  §  For a fuller discussion, see, for example, C. D. Cantrell, Modern Mathematical Methods for Physicists and Engineers  Cambridge: Cambridge University Press, 2000 , chapter 6.  267   MATRICES AND VECTOR SPACES  may be shown that the rank of a general M × N matrix is equal to the size of the largest square submatrix of A whose determinant is non-zero. Therefore, if a matrix A has an r× r submatrix S with S  cid:3 = 0, but no  r + 1 ×  r + 1  submatrix  with non-zero determinant then the rank of the matrix is r. From either deﬁnition it is clear that the rank of A is less than or equal to the smaller of M and N.  cid:1 Determine the rank of the matrix  The largest possible square submatrices of A must be of dimension 3 × 3. Clearly, A  possesses four such submatrices, the determinants of which are given by   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  1  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  1  2 4  2 4  2 1  2 4  2 3  1 0 1  0 −2  1 −2   .   1  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  = 0,  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  = 0,  cid:20  cid:20  cid:20  cid:20  = 1 × 0 − 2 × 1 = −2.   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  1  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  1  0 −2  2 3  0 1  0 1  2 4  2 1  2 1  A =  1 0 1  0 2 3  0 −2  2 3  2 1   cid:20  cid:20  cid:20  cid:20  1  2  1 0   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  = 0,  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  = 0.   In each case the determinant may be evaluated as described in subsection 8.9.1.   The next largest square submatrices of A are of dimension 2× 2. Consider, for example, the 2 × 2 submatrix formed by ignoring the third row and the third and fourth columns of A; this has determinant  Since its determinant is non-zero, A is of rank 2 and we need not consider any other 2× 2 submatrix.  cid:2  In the special case in which the matrix A is a square N×N matrix, by comparing section 8.9, we see that A = 0 unless the rank of A is N. In other words, A is singular unless R A  = N.  either of the above deﬁnitions of rank with our discussion of determinants in  8.12 Special types of square matrix  Matrices that are square, i.e. N × N, are very common in physical applications.  We now consider some special forms of square matrix that are of particular importance.  The unit matrix, which we have already encountered, is an example of a diagonal matrix. Such matrices are characterised by having non-zero elements only on the  8.12.1 Diagonal matrices  268   leading diagonal, i.e. only elements Aij with i = j may be non-zero. For example,  8.12 SPECIAL TYPES OF SQUARE MATRIX   1  0 0  A =   ,  0 0  0 2  0 −3  By performing a Laplace expansion, it is easily shown that the determinant of an  is a 3 × 3 diagonal matrix. Such a matrix is often denoted by A = diag  1, 2,−3 . N × N diagonal matrix is equal to the product of the diagonal elements. Thus, if the matrix has the form A = diag A11, A22, . . . , ANN  then  A = A11A22 ··· ANN.   8.63   Moreover, it is also straightforward to show that the inverse of A is also a diagonal matrix given by   cid:7    cid:8   A−1 = diag  1 A11  ,  1 A22  , . . . ,  .  1  ANN  Finally, we note that, if two matrices A and B are both diagonal then they have the useful property that their product is commutative:  This is not true for matrices in general.  AB = BA.  8.12.2 Lower and upper triangular matrices  A square matrix A is called lower triangular if all the elements above the principal diagonal are zero. For example, the general form for a 3 × 3 lower triangular  matrix is  where the elements Aij may be zero or non-zero. Similarly an upper triangular square matrix is one for which all the elements below the principal diagonal are zero. The general 3 × 3 form is thus   A11  A =  0 0 0 A21 A22 A31 A32 A33   A11 A12 A13  0 0  A22 A23 0 A33  A =   ,   .  By performing a Laplace expansion, it is straightforward to show that, in the  general N × N case, the determinant of an upper or lower triangular matrix is  equal to the product of its diagonal elements,  A = A11A22 ··· ANN.  269   8.64    MATRICES AND VECTOR SPACES  Clearly result  8.63  for diagonal matrices is a special case of this result. Moreover, it may be shown that the inverse of a non-singular lower  upper  triangular matrix is also lower  upper  triangular.  8.12.3 Symmetric and antisymmetric matrices  A square matrix A of order N with the property A = AT is said to be symmetric. Similarly a matrix for which A = −AT is said to be anti- or skew-symmetric and its diagonal elements a11, a22, . . . , aNN are necessarily zero. Moreover, if A is  anti- symmetric then so too is its inverse A−1. This is easily proved by noting that if A = ±AT then   A−1 T =  AT   −1 = ±A−1.  Any N × N matrix A can be written as the sum of a symmetric and an  antisymmetric matrix, since we may write  A = 1  2  A + AT  + 1  2  A − AT  = B + C,  where clearly B = BT and C = −CT. The matrix B is therefore called the symmetric part of A, and C is the antisymmetric part.  cid:1 If A is an N × N antisymmetric matrix, show that A = 0 if N is odd. If A is antisymmetric then AT = −A. Using the properties of determinants  8.49  and   8.51 , we have  Thus, if N is odd then A = −A, which implies that A = 0.  cid:2   A = AT =  − A =  −1 NA.  8.12.4 Orthogonal matrices  A non-singular matrix with the property that its transpose is also its inverse,  AT = A−1,   8.65   is called an orthogonal matrix. It follows immediately that the inverse of an orthogonal matrix is also orthogonal, since   A−1 T =  AT   −1 =  A−1   −1.  Moreover, since for an orthogonal matrix ATA = I, we have  ATA = ATA = A2 = I = 1.  Thus the determinant of an orthogonal matrix must be A = ±1.  An orthogonal matrix represents, in a particular basis, a linear operator that leaves the norms  lengths  of real vectors unchanged, as we will now show.  270   8.12 SPECIAL TYPES OF SQUARE MATRIX  Suppose that y = A x is represented in some coordinate system by the matrix equation y = Ax; then  cid:20 yy cid:21  is given in this coordinate system by  Hence  cid:20 yy cid:21  =  cid:20 xx cid:21 , showing that the action of a linear operator represented by  an orthogonal matrix does not change the norm of a real vector.  yTy = xTATAx = xTx.  8.12.5 Hermitian and anti-Hermitian matrices  , where A†  An Hermitian matrix is one that satisﬁes A = A† gate discussed in section 8.7. Similarly if A† A real  anti- symmetric matrix is a special case of an  anti- Hermitian matrix, in which all the elements of the matrix are real. Also, if A is an  anti- Hermitian matrix then so too is its inverse A−1, since =  A†    = −A, then A is called anti-Hermitian.  is the Hermitian conju-  −1 = ±A−1.  Any N × N matrix A can be written as the sum of an Hermitian matrix and  †  A−1   an anti-Hermitian matrix, since  2  A + A† A = 1 and C = −C†  where clearly B = B† A, and C is called the anti-Hermitian part.  2  A − A†    + 1    = B + C,  . The matrix B is called the Hermitian part of  8.12.6 Unitary matrices  A unitary matrix A is deﬁned as one for which = A−1.  A†  Clearly, if A is real then A† = AT, showing that a real orthogonal matrix is a special case of a unitary matrix, one in which all the elements are real. We note that the inverse A−1 of a unitary is also unitary, since −1. Moreover, since for a unitary matrix A†A = I, we have  −1 =  A−1   †  A−1   =  A†      8.66   A†A = A†A = A∗A = I = 1.  Thus the determinant of a unitary matrix has unit modulus. A unitary matrix represents, in a particular basis, a linear operator that leaves the norms  lengths  of complex vectors unchanged. If y = A x is represented in some coordinate system by the matrix equation y = Ax then  cid:20 yy cid:21  is given in this  coordinate system by  y†y = x†A†Ax = x†x.  271   MATRICES AND VECTOR SPACES  Hence  cid:20 yy cid:21  =  cid:20 xx cid:21 , showing that the action of the linear operator represented by  a unitary matrix does not change the norm of a complex vector. The action of a unitary matrix on a complex column matrix thus parallels that of an orthogonal matrix acting on a real column matrix.  A ﬁnal important set of special matrices consists of the normal matrices, for which  8.12.7 Normal matrices  AA†  = A†A,  i.e. a normal matrix is one that commutes with its Hermitian conjugate.  We can easily show that Hermitian matrices and unitary matrices  or symmetric matrices and orthogonal matrices in the real case  are examples of normal matrices. For an Hermitian matrix, A = A†  and so  Similarly, for a unitary matrix, A−1 = A†  and so  AA†  = AA = A†A.  AA†  = AA−1 = A−1A = A†A.  Finally, we note that, if A is normal then so too is its inverse A−1, since −1A−1 =  A−1   −1 =  A†A   −1 =  AA†  † A−1 A−1   = A−1 A†  −1 =  A†           †A−1.  This broad class of matrices is important in the discussion of eigenvectors and  eigenvalues in the next section.  8.13 Eigenvectors and eigenvalues  Suppose that a linear operator A transforms vectors x in an N-dimensional vector space into other vectors A x in the same space. The possibility then arises that there exist vectors x each of which is transformed by A into a multiple of itself. Such vectors would have to satisfy  A x = λx.   8.67   Any non-zero vector x that satisﬁes  8.67  for some value of λ is called an eigenvector of the linear operator A , and λ is called the corresponding eigenvalue. in general the operator A has N independent  As will be discussed below, eigenvectors xi, with eigenvalues λi. The λi are not necessarily all distinct. If we choose a particular basis in the vector space, we can write  8.67  in terms of the components of A and x with respect to this basis as the matrix equation  8.68   where A is an N × N matrix. The column matrices x that satisfy  8.68  obviously  Ax = λx,  272   8.13 EIGENVECTORS AND EIGENVALUES  represent the eigenvectors x of A in our chosen coordinate system. Convention- ally, these column matrices are also referred to as the eigenvectors of the matrix § Clearly, if x is an eigenvector of A  with some eigenvalue λ  then any scalar A. multiple µx is also an eigenvector with the same eigenvalue. We therefore often use normalised eigenvectors, for which  x†x = 1   note that x†x corresponds to the inner product  cid:20 xx cid:21  in our basis . Any eigen- vector x can be normalised by dividing all its components by the scalar  x†x 1 2. As will be seen, the problem of ﬁnding the eigenvalues and corresponding eigenvectors of a square matrix A plays an important role in many physical investigations. Throughout this chapter we denote the ith eigenvector of a square matrix A by xi and the corresponding eigenvalue by λi. This superscript notation for eigenvectors is used to avoid any confusion with components.  cid:1 A non-singular matrix A has eigenvalues λi and eigenvectors xi. Find the eigenvalues and eigenvectors of the inverse matrix A−1.  The eigenvalues and eigenvectors of A satisfy  Left-multiplying both sides of this equation by A−1, we ﬁnd  Since A−1A = I, on rearranging we obtain  Axi = λixi.  A−1Axi = λiA−1xi.  A−1xi =  1 λi  xi.  Thus, we see that A−1 has the same eigenvectors xi as does A, but the corresponding eigenvalues are 1 λi.  cid:2   In the remainder of this section we will discuss some useful results concerning the eigenvectors and eigenvalues of certain special  though commonly occurring  square matrices. The results will be established for matrices whose elements may be complex; the corresponding properties for real matrices may be obtained as special cases.  8.13.1 Eigenvectors and eigenvalues of a normal matrix  In subsection 8.12.7 we deﬁned a normal matrix A as one that commutes with its Hermitian conjugate, so that  §  In this context, when referring to linear combinations of eigenvectors x we will normally use the term ‘vector’.  A†A = AA†  .  273    A − λI x = 0.  †  Bx   = x†B†  = 0.  x†B†Bx = 0.  MATRICES AND VECTOR SPACES  We also showed that both Hermitian and unitary matrices  or symmetric and orthogonal matrices in the real case  are examples of normal matrices. We now discuss the properties of the eigenvectors and eigenvalues of a normal matrix.  If x is an eigenvector of a normal matrix A with corresponding eigenvalue λ  then Ax = λx, or equivalently,  Denoting B = A− λI,  8.69  becomes Bx = 0 and, taking the Hermitian conjugate,  we also have  From  8.69  and  8.70  we then have  However, the product B†B is given by  A − λI  =  A† − λ  B†B =  A − λI  †  Now since A is normal, AA† B†B = AA† − λ  = A†A and so ∗A − λA† ∗ + λλ  ∗I  A − λI  = A†A − λ  ∗A − λA†  ∗ + λλ  .  =  A − λI  A − λI  †  = BB†  ,  and hence B is also normal. From  8.71  we then ﬁnd  x†B†Bx = x†BB†x =  B†x   †B†x = 0,  from which we obtain  B†x =  A† − λ  ∗I x = 0.  Therefore, for a normal matrix A, the eigenvalues of A† of the eigenvalues of A.  are the complex conjugates  Let us now consider two eigenvectors xi and xj of a normal matrix A corre-  sponding to two diﬀerent eigenvalues λi and λj. We then have  Axi = λixi, Axj = λjxj. † Multiplying  8.73  on the left by  xi   we obtain  †Axj = λj xi   †xj.   xi   However, on the LHS of  8.74  we have  †A =  A†xi  †   xi   ∗ =  λ i  † xi   † = λi xi   ,   8.69    8.70    8.71    8.72    8.73    8.74    8.75   where we have used  8.40  and the property just proved for a normal matrix to  274   8.13 EIGENVECTORS AND EIGENVALUES  write A†xi = λ ∗  i  xi. From  8.74  and  8.75  we have †xj = 0.   λi − λj  xi   Thus, if λi  cid:3 = λj the eigenvectors xi and xj must be orthogonal, i.e.  xi   †xj = 0.  It follows immediately from  8.76  that if all N eigenvalues of a normal matrix A are distinct then all N eigenvectors of A are mutually orthogonal. If, however, two or more eigenvalues are the same then further consideration is required. An eigenvalue corresponding to two or more diﬀerent eigenvectors  i.e. they are not simply multiples of one another  is said to be degenerate. Suppose that λ1 is k-fold degenerate, i.e.   8.76   Axi = λ1xi  for i = 1, 2, . . . , k,   8.77   but that it is diﬀerent from any of λk+1, λk+2, etc. Then any linear combination of these xi is also an eigenvector with eigenvalue λ1, since, for z =  k  i=1 cixi,   cid:11   k cid:4   Az ≡ A  k cid:4   i=1  i=1  k cid:4   i=1  cixi =  ciAxi =  ciλ1xi = λ1z.   8.78   If the xi deﬁned in  8.77  are not already mutually orthogonal then we can  construct new eigenvectors zi that are orthogonal by the following procedure:   cid:23   cid:23   †x2 †x3   ˆz1    cid:22   cid:22   cid:22   ˆzk−1    ˆz2    cid:22   ˆz1, ˆz2 −  cid:23   z1 = x1, z2 = x2 − z3 = x3 −  ...  zk = xk −  †x3   ˆz1    cid:23   ˆz1,   cid:22   †xk  ˆzk−1 − ··· −  †xk   ˆz1   ˆz1.   cid:23   †zi]1 2 . Note that each factor in brackets  ˆzm   In this procedure, known as Gram–Schmidt orthogonalisation, each new eigen- vector zi is normalised to give the unit vector ˆzi before proceeding to the construc- tion of the next one  the normalisation is carried out by dividing each element of †xn is a scalar the vector zi by [ zi  product and thus only a number. It follows that, as shown in  8.78 , each vector zi so constructed is an eigenvector of A with eigenvalue λ1 and will remain so on normalisation. It is straightforward to check that, provided the previous new eigenvectors have been normalised as prescribed, each zi is orthogonal to all its predecessors.  In practice, however, the method is laborious and the example in subsection 8.14.1 gives a less rigorous but considerably quicker way.   Therefore, even if A has some degenerate eigenvalues we can by construction obtain a set of N mutually orthogonal eigenvectors. Moreover, it may be shown  although the proof is beyond the scope of this book  that these eigenvectors are complete in that they form a basis for the N-dimensional vector space. As  275   MATRICES AND VECTOR SPACES  a result any arbitrary vector y can be expressed as a linear combination of the eigenvectors xi:  y =  aixi,  i=1   8.79  †y. Thus, the eigenvectors form an orthogonal basis for the vector †xi = 1 this basis is made  where ai =  xi  space. By normalising the eigenvectors so that  xi  orthonormal.  cid:1 Show that a normal matrix A can be written in terms of its eigenvalues λi and orthonormal eigenvectors xi as  A =  λixi xi   †  .   8.80   The key to proving the validity of  8.80  is to show that both sides of the expression give the same result when acting on an arbitary vector y. Since A is normal, we may expand y in terms of the eigenvectors xi, as shown in  8.79 . Thus, we have  N cid:4   N cid:4   i=1  N cid:4   i=1  Alternatively, the action of the RHS of  8.80  on y is given by  N cid:4  N cid:4   i=1  i=1  Ay = A  aixi =  aiλixi.  N cid:4   i=1  λixi xi   †y =  aiλixi,  since ai =  xi  are identical, which implies that this relationship is indeed correct.  cid:2   †y. We see that the two expressions for the action of each side of  8.80  on y  8.13.2 Eigenvectors and eigenvalues of Hermitian and anti-Hermitian matrices  For a normal matrix we showed that if Ax = λx then A†x = λ also Hermitian, A = A† of an Hermitian matrix are real, a result which may be proved directly.  cid:1 Prove that the eigenvalues of an Hermitian matrix are real.  ∗ , it follows necessarily that λ = λ  ∗x. However, if A is . Thus, the eigenvalues  For any particular eigenvector xi, we take the Hermitian conjugate of Axi = λixi to give  Using A†  = A, since A is Hermitian, and multiplying on the right by xi, we obtain  †A†   xi   †  ∗ i  xi   .  = λ   8.81    8.82    xi   †Axi = λ  †xi. † But multiplying Axi = λixi through on the left by  xi  †Axi = λi xi  †xi. − λi  xi   Subtracting this from  8.82  yields  ∗ i  xi   0 =  λ  †xi.   xi   ∗  i  gives  276   8.13 EIGENVECTORS AND EIGENVALUES  †xi is the modulus squared of the non-zero vector xi and is thus non-zero. Hence But  xi  ∗ i must equal λi and thus be real. The same argument can be used to show that the λ eigenvalues of a real symmetric matrix are themselves real.  cid:2   The importance of the above result will be apparent to any student of quantum mechanics. In quantum mechanics the eigenvalues of operators correspond to measured values of observable quantities, e.g. energy, angular momentum, parity and so on, and these clearly must be real. If we use Hermitian operators to formulate the theories of quantum mechanics, the above property guarantees physically meaningful results.  Since an Hermitian matrix is also a normal matrix, its eigenvectors are orthog- onal  or can be made so using the Gram–Schmidt orthogonalisation procedure . Alternatively we can prove the orthogonality of the eigenvectors directly.  cid:1 Prove that the eigenvectors corresponding to diﬀerent eigenvalues of an Hermitian matrix are orthogonal.  Consider two unequal eigenvalues λi and λj and their corresponding eigenvectors satisfying  Axi = λixi, Axj = λjxj.   8.83   8.84   . Multiplying this on the  †A† Taking the Hermitian conjugate of  8.83  we ﬁnd  xi  right by xj we obtain †xj ,  †A†xj = λ   xi   ∗ i  xi   †  ∗ i  xi   = λ  and similarly multiplying  8.84  through on the left by  xi   we ﬁnd  †  Then, since A† subtraction we obtain  Finally we note that λi orthogonal.  cid:2   †Axj = λj xi   †xj.   xi   0 =  λi − λj  xi  †xj . †xj = 0,   cid:3 = λj and so  xi   = A, the two left-hand sides are equal and, because the λi are real, on  i.e. the eigenvectors xi and xj are  In the case where some of the eigenvalues are equal, further justiﬁcation of the orthogonality of the eigenvectors is needed. The Gram–Schmidt orthogonalisa- tion procedure discussed above provides a proof of, and a means of achieving, orthogonality. The general method has already been described and we will not repeat it here. anti-Hermitian matrix, for which A†  We may also consider the properties of the eigenvalues and eigenvectors of an  = −A and thus  AA†  = A −A  =  −A A = A†A.  Therefore matrices that are anti-Hermitian are also normal and so have mutu- ally orthogonal eigenvectors. The properties of the eigenvalues are also simply deduced, since if Ax = λx then  ∗x = A†x = −Ax = −λx.  λ  277   MATRICES AND VECTOR SPACES  = −λ and so λ must be pure imaginary  or zero . In a similar manner  ∗ Hence λ to that used for Hermitian matrices, these properties may be proved directly.  8.13.3 Eigenvectors and eigenvalues of a unitary matrix  A unitary matrix satisﬁes A† = A−1 and is also a normal matrix, with mutually orthogonal eigenvectors. To investigate the eigenvalues of a unitary matrix, we note that if Ax = λx then  x†x = x†A†Ax = λ ∗  λx†x,  = λ2 = 1. Thus, the eigenvalues of a unitary matrix  ∗ and we deduce that λλ have unit modulus.  8.13.4 Eigenvectors and eigenvalues of a general square matrix  When an N × N matrix is not normal there are no general properties of its  eigenvalues and eigenvectors; in general it is not possible to ﬁnd any orthogonal set of N eigenvectors or even to ﬁnd pairs of orthogonal eigenvectors  except by chance in some cases . While the N non-orthogonal eigenvectors are usually linearly independent and hence form a basis for the N-dimensional vector space, this is not necessarily so. It may be shown  although we will not prove it  that any  N × N matrix with distinct eigenvalues has N linearly independent eigenvectors,  which therefore form a basis for the N-dimensional vector space. If a general square matrix has degenerate eigenvalues, however, then it may or may not have N linearly independent eigenvectors. A matrix whose eigenvectors are not linearly independent is said to be defective.  8.13.5 Simultaneous eigenvectors  We may now ask under what conditions two diﬀerent normal matrices can have a common set of eigenvectors. The result – that they do so if, and only if, they commute – has profound signiﬁcance for the foundations of quantum mechanics.  To prove this important result let A and B be two N × N normal matrices and xi be the ith eigenvector of A corresponding to eigenvalue λi, i.e.  Axi = λixi  for  i = 1, 2, . . . , N.  For the present we assume that the eigenvalues are all diﬀerent.   i  First suppose that A and B commute. Now consider  ABxi = BAxi = Bλixi = λiBxi,  where we have used the commutativity for the ﬁrst equality and the eigenvector property for the second. It follows that A Bxi  = λi Bxi  and thus that Bxi is an  278   8.13 EIGENVECTORS AND EIGENVALUES  eigenvector of A corresponding to eigenvalue λi. But the eigenvector solutions of  A− λiI xi = 0 are unique to within a scale factor, and we therefore conclude that  Bxi = µixi  for some scale factor µi. However, this is just an eigenvector equation for B and shows that xi is an eigenvector of B, in addition to being an eigenvector of A. By reversing the roles of A and B, it also follows that every eigenvector of B is an eigenvector of A. Thus the two sets of eigenvectors are identical.   ii  Now suppose that A and B have all their eigenvectors in common, a typical  one xi satisfying both  As the eigenvectors span the N-dimensional vector space, any arbitrary vector x in the space can be written as a linear combination of the eigenvectors,  Axi = λixi  and Bxi = µixi.  x =  cixi.  i=1  N cid:4  N cid:4  N cid:4   i=1  i=1  N cid:4  N cid:4   i=1  i=1   AB − BA x = 0  N cid:4  N cid:4   i=1  i=1  Now consider both  and  ABx = AB  cixi = A  ciµixi =  ciλiµixi,  BAx = BA  cixi = B  ciλixi =  ciµiλixi.  It follows that ABx and BAx are the same for any arbitrary x and hence that  for all x. That is, A and B commute.  This completes the proof that a necessary and suﬃcient condition for two normal matrices to have a set of eigenvectors in common is that they commute. It should be noted that if an eigenvalue of A, say, is degenerate then not all of its possible sets of eigenvectors will also constitute a set of eigenvectors of B. However, provided that by taking linear combinations one set of joint eigenvectors can be found, the proof is still valid and the result still holds.  When extended to the case of Hermitian operators and continuous eigenfunc- tions  sections 17.2 and 17.3  the connection between commuting matrices and a set of common eigenvectors plays a fundamental role in the postulatory basis of quantum mechanics. It draws the distinction between commuting and non- commuting observables and sets limits on how much information about a system can be known, even in principle, at any one time.  279   MATRICES AND VECTOR SPACES  8.14 Determination of eigenvalues and eigenvectors  The next step is to show how the eigenvalues and eigenvectors of a given N × N matrix A are found. To do this we refer to  8.68  and as in  8.69  rewrite it as  Ax − λIx =  A − λI x = 0.  The slight rearrangement used here is to write x as Ix, where I is the unit matrix of order N. The point of doing this is immediate since  8.85  now has the form of a homogeneous set of simultaneous equations, the theory of which will be developed in section 8.18. What will be proved there is that the equation Bx = 0 only has a non-trivial solution x if B = 0. Correspondingly, therefore, we must  have in the present case that  A − λI = 0, if there are to be non-zero solutions x to  8.85 .  Equation  8.86  is known as the characteristic equation for A and its LHS as the characteristic or secular determinant of A. The equation is a polynomial of degree N in the quantity λ. The N roots of this equation λi, i = 1, 2, . . . , N, give the eigenvalues of A. Corresponding to each λi there will be a column vector xi, which is the ith eigenvector of A and can be found by using  8.68 . λ, the coeﬃcient of −λN−1 in the equation will be simply A11 + A22 + ··· + ANN N relative to the coeﬃcient of λN. As discussed in section 8.8, the quantity i=1 Aii is the trace of A and, from the ordinary theory of polynomial equations, will be equal to the sum of the roots of  8.86 :  It will be observed that when  8.86  is written out as a polynomial equation in   cid:11    8.85    8.86   λi = Tr A.   8.87   This can be used as one check that a computation of the eigenvalues λi has been done correctly. Unless equation  8.87  is satisﬁed by a computed set of eigenvalues, they have not been calculated correctly. However, that equation  8.87  is satisﬁed is a necessary, but not suﬃcient, condition for a correct computation. An alternative proof of  8.87  is given in section 8.16.  cid:1 Find the eigenvalues and normalised eigenvectors of the real symmetric matrix  N cid:4   i=1   1  Using  8.86 ,   .  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  = 0.  A =   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  1 − λ  1 3  1  3  1 −3 3 −3 −3  1  1  1 − λ −3  3  −3 −3 − λ  280   8.14 DETERMINATION OF EIGENVALUES AND EIGENVECTORS  Expanding out this determinant gives   1 − λ  [ 1 − λ  −3 − λ  −  −3  −3 ] + 1 [ −3  3  − 1 −3 − λ ]  + 3 [1 −3  −  1 − λ  3 ] = 0,  which simpliﬁes to give   1 − λ  λ2 + 2λ − 12  +  λ − 6  + 3 3λ − 6  = 0, ⇒  λ − 2  λ − 3  λ + 6  = 0.  Hence the roots of the characteristic equation, which are the eigenvalues of A, are λ1 = 2, λ2 = 3, λ3 = −6. We note that, as expected,  λ1 + λ2 + λ3 = −1 = 1 + 1 − 3 = A11 + A22 + A33 = Tr A.  For the ﬁrst root, λ1 = 2, a suitable eigenvector x1, with elements x1, x2, x3, must satisfy  Ax1 = 2x1 or, equivalently,  x1 + x2 + 3x3 = 2x1,  x1 + x2 − 3x3 = 2x2, 3x1 − 3x2 − 3x3 = 2x3.   8.88   √ 2. Hence  These three equations are consistent  to ensure this was the purpose in ﬁnding the particular values of λ  and yield x3 = 0, x1 = x2 = k, where k is any non-zero number. A suitable eigenvector would thus be  If we apply the normalisation condition, we require k2 + k2 + 02 = 1 or k = 1    cid:7   x1 =  k  k 0 T .   cid:8   T  x1 =  1√ 2  1√ 2  1√ 2  0  =   1 1 0 T .  Repeating the last paragraph, but with the factor 2 on the RHS of  8.88  replaced  successively by λ2 = 3 and λ3 = −6, gives two further normalised eigenvectors  x2 =  1√ 3   1 − 1 1 T ,  x3 =  1√ 6   1 − 1 − 2 T .  cid:2   In the above example, the three values of λ are all diﬀerent and A is a real symmetric matrix. Thus we expect, and it is easily checked, that the three eigenvectors are mutually orthogonal, i.e.   cid:5   x1   cid:6 T x2 =   cid:5   x1   cid:6 T x3 =   cid:5   x2   cid:6 T x3 = 0.  It will be apparent also that, as expected, the normalisation of the eigenvectors has no eﬀect on their orthogonality.  8.14.1 Degenerate eigenvalues  We return now to the case of degenerate eigenvalues, i.e. those that have two or more associated eigenvectors. We have shown already that it is always possible to construct an orthogonal set of eigenvectors for a normal matrix, see subsec- tion 8.13.1, and the following example illustrates one method for constructing such a set.  281   MATRICES AND VECTOR SPACES   cid:1 Construct an orthonormal set of eigenvectors for the matrix   1  A =  0  0 −2  3  0  3 0 1   .  We ﬁrst determine the eigenvalues using A − λI = 0:   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  1 − λ  0 3  0 =   1  0  0 −2  3  0  3 0 1   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  = − 1 − λ 2 2 + λ  + 3 3  2 + λ   1  ⇒ x1 =  x1  .  =  4 − λ  λ + 2 2.  1√ 2  0 1  x2 x3  0  −2 − λ  0  3 0  1 − λ   x1  x2 x3   = 4  Thus λ1 = 4, λ2 = −2 = λ3. The eigenvector x1 =  x1 x2 x3 T is found from  A general column vector that is orthogonal to x1 is x =  a b − a T ,  and it is easily shown that   1  Ax =  0  0 −2  3  0  3 0 1   a  b−a   = −2   a   = −2x.  b−a  Thus x is a eigenvector of A with associated eigenvalue −2. It is clear, however, that there is an inﬁnite set of eigenvectors x all possessing the required property; the geometrical analogue is that there are an inﬁnite number of corresponding vectors x lying in the plane that has x1 as its normal. We do require that the two remaining eigenvectors are orthogonal to one another, but this still leaves an inﬁnite number of possibilities. For x2, therefore, let us choose a simple form of  8.89 , suitably normalised, say,  The third eigenvector is then speciﬁed  to within an arbitrary multiplicative constant  by the requirement that it must be orthogonal to x1 and x2; thus x3 may be found by evaluating the vector product of x1 and x2 and normalising the result. This gives  x2 =  0 1 0 T .  x3 =  1√ 2   −1 0 1 T ,  to complete the construction of an orthonormal set of eigenvectors.  cid:2   8.15 Change of basis and similarity transformations  Throughout this chapter we have considered the vector x as a geometrical quantity that is independent of any basis  or coordinate system . If we introduce a basis ei, i = 1, 2, . . . , N, into our N-dimensional vector space then we may write  x = x1e1 + x2e2 + ··· + xNeN,  282   8.89    8.15 CHANGE OF BASIS AND SIMILARITY TRANSFORMATIONS  and represent x in this basis by the column matrix  x =  x1 x2 ··· xn T,  having components xi. We now consider how these components change as a result  cid:7  i, i = 1, 2, . . . , N, of a prescribed change of basis. Let us introduce a new basis e which is related to the old basis by   cid:7  j =  e  Sijei,   8.90    cid:7  j with respect to the old  unprimed   the coeﬃcient Sij being the ith component of e basis. For an arbitrary vector x it follows that  N cid:4   N cid:4   N cid:4   N cid:4   x =  xiei =   cid:7  je   cid:7  j =  x   cid:7   j  x  Sijei.  i=1  j=1  j=1  i=1  From this we derive the relationship between the components of x in the two coordinate systems as  xi =   cid:7  j,  Sijx  x = Sx cid:7   x cid:7   = S−1x,  which we can write in matrix form as   8.91    8.92   Furthermore, since the vectors e  where S is the transformation matrix associated with the change of basis.  cid:7  j are linearly independent, the matrix S is non-singular and so possesses an inverse S−1. Multiplying  8.91  on the left by S−1 we ﬁnd  which relates the components of x in the new basis to those in the old basis. Comparing  8.92  and  8.90  we note that the components of x transform inversely to the way in which the basis vectors ei themselves transform. This has to be so, as the vector x itself must remain unchanged. We may also ﬁnd the transformation law for the components of a linear operator under the same change of basis. Now, the operator equation y = A x  which is basis independent  can be written as a matrix equation in each of the two bases as  y = Ax,  y cid:7   = A cid:7 x cid:7   .   8.93   But, using  8.91 , we may rewrite the ﬁrst equation as  Sy cid:7   = ASx cid:7  ⇒ y cid:7   = S−1ASx cid:7   .  N cid:4   i=1  N cid:4   j=1  283   MATRICES AND VECTOR SPACES  Comparing this with the second equation in  8.93  we ﬁnd that the components of the linear operator A transform as A cid:7   = S−1AS.   8.94   Equation  8.94  is an example of a similarity transformation – a transformation that can be particularly useful in converting matrices into convenient forms for computation.  Given a square matrix A, we may interpret it as representing a linear operator A in a given basis ei. From  8.94 , however, we may also consider the matrix A cid:7  = S−1AS, for any non-singular matrix S, as representing the same linear operator A but in a new basis e   cid:7  j, related to the old basis by   cid:4   i   cid:7  j =  e  Sijei.  Therefore we would expect that any property of the matrix A that represents some  basis-independent  property of the linear operator A will also be shared by the matrix A cid:7   . We list these properties below.   i  If A = I then A cid:7   = I, since, from  8.94 ,  A cid:7   = S−1IS = S−1S = I.   8.95    ii  The value of the determinant is unchanged:  A cid:7  = S−1AS = S−1AS = AS−1S = AS−1S = A.  iii  The characteristic determinant and hence the eigenvalues of A cid:7    8.96   are the  same as those of A: from  8.86 ,   iv  The value of the trace is unchanged: from  8.87 ,  A cid:7  − λI = S−1AS − λI = S−1 A − λI S  cid:4   cid:4   = S−1SA − λI = A − λI.  cid:4   cid:4   cid:4    cid:4   cid:4   S−1 ij AjkSki   cid:4    cid:4    cid:7  ii = A  k  j  i  i  Ski S−1 ij Ajk =   cid:4   j  δkjAjk =  Ajj  j  k  =  i  j  k  = Tr A.   8.97    8.98   Tr A cid:7   =  An important class of similarity transformations is that for which S is a uni- tary matrix; in this case A cid:7  = S−1AS = S†AS. Unitary transformation matrices are particularly important, for the following reason. If the original basis ei is  284   8.16 DIAGONALISATION OF MATRICES  orthonormal and the transformation matrix S is unitary then   cid:20 e  e   cid:7   j   cid:7   i   cid:21  =  ’  & cid:4   cid:4   cid:4   k  k  S   cid:20  cid:20  cid:20  cid:4   cid:4   cid:4   r  r  Skiek ∗  ki  ∗  S  ki  Srjer   cid:4  Srj cid:20 eker cid:21   Srjδkr =  k  r  k  =  =  ∗ kiSkj =  S†S ij = δij,  S  showing that the new basis is also orthonormal.  Furthermore, in addition to the properties of general similarity transformations,  for unitary transformations the following hold.   i  If A is Hermitian  anti-Hermitian  then A cid:7   is Hermitian  anti-Hermitian ,  i.e. if A†  = ±A then †  A cid:7     =  S†AS  †  = S†A†S = ±S†AS = ±A cid:7   .   8.99    ii  If A is unitary  so that A†  is unitary, since   A cid:7   †A cid:7      S†AS  = S†A†SS†AS = S†A†AS  = A−1  then A cid:7  =  S†AS  † = S†IS = I.   8.100   8.16 Diagonalisation of matrices  Suppose that a linear operator A is represented in some basis ei, i = 1, 2, . . . , N, by the matrix A. Consider a new basis xj given by  where the xj are chosen to be the eigenvectors of the linear operator A , i.e.   8.101  In the new basis, A is represented by the matrix A cid:7  = S−1AS, which has a particularly simple form, as we shall see shortly. The element Sij of S is the ith component, in the old  unprimed  basis, of the jth eigenvector xj of A, i.e. the columns of S are the eigenvectors of the matrix A: ↑ xN ↓   ↑   ,  ↑ x2 ↓  ···  S =  x1 ↓  N cid:4   i=1  xj =  Sijei,  A xj = λjxj.  285   that is, Sij =  xj i. Therefore A cid:7   is given by   S−1AS ij =  MATRICES AND VECTOR SPACES   cid:4   cid:4   l  k   cid:4   cid:4   cid:4   cid:4   k  k  k  =  =  =   S−1 ikAklSlj  S−1 ikAkl xj l  l   S−1 ikλj xj k λj S−1 ikSkj = λjδij.    A cid:7   =   .  λ1  0 ... 0  0  λ2  ···  ···  . . . 0  0 ...  0 λN  So the matrix A cid:7  i.e.  is diagonal with the eigenvalues of A as the diagonal elements,  Therefore, given a matrix A, if we construct the matrix S that has the eigen- vectors of A as its columns then the matrix A cid:7  = S−1AS is diagonal and has the eigenvalues of A as its diagonal elements. Since we require S to be non-singular  S  cid:3 = 0 , the N eigenvectors of A must be linearly independent and form a basis  for the N-dimensional vector space. It may be shown that any matrix with distinct eigenvalues can be diagonalised by this procedure. If, however, a general square matrix has degenerate eigenvalues then it may, or may not, have N linearly independent eigenvectors. If it does not then it cannot be diagonalised.  For normal matrices  which include Hermitian, anti-Hermitian and unitary matrices  the N eigenvectors are indeed linearly independent. Moreover, when normalised, these eigenvectors form an orthonormal set  or can be made to do so . Therefore the matrix S with these normalised eigenvectors as columns, i.e. whose elements are Sij =  xj i, has the property   S†S ij =   S†   ik S kj =  ∗ kiSkj =  S  ∗  xi  k xj k =  xi   †  xj = δij .   cid:4   k   cid:4   k   cid:4   k  Hence S is unitary  S−1 = S†    and the original matrix A can be diagonalised by A cid:7   = S−1AS = S†AS.  Therefore, any normal matrix A can be diagonalised by a similarity transformation using a unitary transformation matrix S.  286   8.16 DIAGONALISATION OF MATRICES   cid:1 Diagonalise the matrix   1  A =  0  0 −2  3  0  3 0 1   .  The matrix A is symmetric and so may be diagonalised by a transformation of the form A cid:7  = S†AS, where S has the normalised eigenvectors of A as its columns. We have already found these eigenvectors in subsection 8.14.1, and so we can write straightaway  1√ 2  0 1   1  1  ,  3  1 0 1  0 −1 √ 2 0  0 1  0  0 −2  0  3 0 1   .  1  0 1  S =  √ 0 2 0   1  4  −1 0 −2  1 2  0  0  0  =  0 0  0 −2  S†AS =    0 −1 √ 2 0  0 1  We note that although the eigenvalues of A are degenerate, its three eigenvectors are linearly independent and so A can still be diagonalised. Thus, calculating S†AS we obtain  which is diagonal, as required, and has as its diagonal elements the eigenvalues of A.  cid:2  = S−1AS, so  If a matrix A is diagonalised by the similarity transformation A cid:7   that A cid:7   = diag λ1, λ2, . . . , λN , then we have immediately  Tr A cid:7   = Tr A =  λi,  A cid:7  = A =  N cid:4  N cid:3   i=1  λi,  i=1   exp A = exp Tr A ,   8.102    8.103    8.104   since the eigenvalues of the matrix are unchanged by the transformation. More- over, these results may be used to prove the rather useful trace formula  where the exponential of a matrix is as deﬁned in  8.38 .   cid:1 Prove the trace formula  8.104 . At the outset, we note that for the similarity transformation A cid:7   = S−1AS, we have   A cid:7    n =  S−1AS  S−1AS ···  S−1AS  = S−1AnS.  Thus, from  8.38 , we obtain exp A cid:7   = S−1 exp A S, from which it follows that  exp A cid:7  =  287   MATRICES AND VECTOR SPACES   exp A. Moreover, by choosing the similarity transformation so that it diagonalises A, we have A cid:7   exp A =  exp A cid:7  = exp[diag λ1, λ2, . . . , λN ] = diag exp λ1, exp λ2, . . . , exp λN  =  = diag λ1, λ2, . . . , λN , and so  N cid:3   exp λi.  i=1  Rewriting the ﬁnal product of exponentials of the eigenvalues as the exponential of the sum of the eigenvalues, we ﬁnd  N cid:3   i=1   exp A =   cid:31    cid:30   N cid:4   i=1  exp λi = exp  λi  = exp Tr A ,  which gives the trace formula  8.104 .  cid:2   8.17 Quadratic and Hermitian forms  Let us now introduce the concept of quadratic forms  and their complex ana- logues, Hermitian forms . A quadratic form Q is a scalar function of a real vector x given by   8.105  for some real linear operator A . In any given basis  coordinate system  we can write  8.105  in matrix form as  Q x  =  cid:20 xA x cid:21 ,  Q x  = xTAx,   8.106   where A is a real matrix. In fact, as will be explained below, we need only consider the case where A is symmetric, i.e. A = AT. As an example in a three-dimensional space,   cid:9    cid:10  1  3  1  1  1 −3 3 −3 −3   x1    x2 x3  Q = xTAx =  x1 x2 x3  = x2  1 + x2 2  − 3x2  3 + 2x1x2 + 6x1x3 − 6x2x3.   8.107   It is reasonable to ask whether a quadratic form Q = xTMx, where M is any  possibly non-symmetric  real square matrix, is a more general deﬁnition. That this is not the case may be seen by expressing M in terms of a symmetric matrix A = 1 We then have  2  M−MT  such that M = A+B.  2  M+MT  and an antisymmetric matrix B = 1  Q = xTMx = xTAx + xTBx.   8.108   However, Q is a scalar quantity and so  Q = QT =  xTAx T +  xTBx T = xTATx + xTBTx = xTAx − xTBx.   8.109   Comparing  8.108  and  8.109  shows that xTBx = 0, and hence xTMx = xTAx,  288   8.17 QUADRATIC AND HERMITIAN FORMS  i.e. Q is unchanged by considering only the symmetric part of M. Hence, with no loss of generality, we may assume A = AT in  8.106 .  From its deﬁnition  8.105 , Q is clearly a basis-  i.e. coordinate-  independent quantity. Let us therefore consider a new basis related to the old one by an orthogonal transformation matrix S, the components in the two bases of any vector x being related  as in  8.91   by x = Sx cid:7  = S−1x = STx. We then have  or, equivalently, by x cid:7   Q = xTAx =  x cid:7    TSTASx cid:7   =  x cid:7    TA cid:7 x cid:7   ,  where  as expected  the matrix describing the linear operator A in the new basis is given by A cid:7  = STAS  since ST = S−1 . But, from the last section, if we choose as S the matrix whose columns are the normalised eigenvectors of A then A cid:7  = STAS is diagonal with the eigenvalues of A as the diagonal elements.  Since A is symmetric, its normalised eigenvectors are orthogonal, or can be made so, and hence S is orthogonal with S−1 = ST.   In the new basis  Q = xTAx =  x cid:7    TΛx cid:7   = λ1x  2 + λ2x   cid:7  1   cid:7  2  2 + ··· + λN x   cid:7   N  2  ,   8.110   where Λ = diag λ1, λ2, . . . , λN  and the λi are the eigenvalues of A. It should be noted that Q contains no cross-terms of the form x   cid:7  1x   cid:7  2.   cid:1 Find an orthogonal transformation that takes the quadratic form  8.107  into the form  The required transformation matrix S has the normalised eigenvectors of A as its columns. We have already found these in section 8.14, and so we can write immediately   cid:7  1  λ1x  2 + λ2x  2 + λ3x  2  .   cid:7  2   cid:7  3   √ √ √ 3 −√ 2 3 2 −1 √ 2 −2  0  1   ,  S =  1√ 6  √ 2,  x  x  x   cid:7  √ 1 =  x1 + x2   2 =  x1 − x2 + x3    cid:7  √ 3, 3 =  x1 − x2 − 2x3    cid:7  6. 2 − 6x  cid:7  2 + 3x 2   cid:7  1   cid:7  3  which is easily veriﬁed as being orthogonal. Since the eigenvalues of A are λ = 2, 3, and −6, the general result already proved shows that the transformation x = Sx cid:7  will carry 2. This may be veriﬁed most easily by writing out  8.107  into the form 2x the inverse transformation x cid:7    cid:7  = S−1x = STx and substituting. The inverse equations are 2  2 − 6x  2 + 3x   cid:7  1   cid:7  3   8.111   If these are substituted into the form Q = 2x  8.107  is recovered.  cid:2   2 then the original expression  In the deﬁnition of Q it was assumed that the components x1, x2, x3 and the  matrix A were real. It is clear that in this case the quadratic form Q ≡ xTAx is real  289   MATRICES AND VECTOR SPACES  also. Another, rather more general, expression that is also real is the Hermitian form  H x  ≡ x†Ax,   8.112   where A is Hermitian  i.e. A† It is straightforward to show that H is real, since  = A  and the components of x may now be complex.  ∗  ∗ =  H T   = x†A†x = x†Ax = H.  H  With suitable generalisation, the properties of quadratic forms apply also to Her- mitian forms, but to keep the presentation simple we will restrict our discussion to quadratic forms.  A special case of a quadratic  Hermitian  form is one for which Q = xTAx is greater than zero for all column matrices x. By choosing as the basis the eigenvectors of A we have Q in the form  Q = λ1x2  1 + λ2x2  2 + λ3x2 3.  The requirement that Q > 0 for all x means that all the eigenvalues λi of A must be positive. A symmetric  Hermitian  matrix A with this property is called positive deﬁnite. If, instead, Q ≥ 0 for all x then it is possible that some of the eigenvalues are zero, and A is called positive semi-deﬁnite.  8.17.1 The stationary properties of the eigenvectors  Consider a quadratic form, such as Q x  =  cid:20 xA x cid:21 , equation  8.105 , in a ﬁxed  basis. As the vector x is varied, through changes in its three components x1, x2 and x3, the value of the quantity Q also varies. Because of the homogeneous form of Q we may restrict any investigation of these variations to vectors of unit length  since multiplying any vector x by any scalar k simply multiplies the value of Q by a factor k2 .  Of particular interest are any vectors x that make the value of the quadratic form a maximum or minimum. A necessary, but not suﬃcient, condition for this  is that Q is stationary with respect to small variations ∆x in x, whilst  cid:20 xx cid:21  is  maintained at a constant value  unity .  In the chosen basis the quadratic form is given by Q = xTAx and, using Lagrange undetermined multipliers to incorporate the variational constraints, we are led to seek solutions of  ∆[xTAx − λ xTx − 1 ] = 0.  This may be used directly, together with the fact that  ∆xT Ax = xTA ∆x, since A is symmetric, to obtain   8.113    8.114   Ax = λx  290   8.17 QUADRATIC AND HERMITIAN FORMS  as the necessary condition that x must satisfy. If  8.114  is satisﬁed for some eigenvector x then the value of Q x  is given by  Q = xTAx = xTλx = λ.   8.115   However, if x and y are eigenvectors corresponding to diﬀerent eigenvalues then they are  or can be chosen to be  orthogonal. Consequently the expression yTAx is necessarily zero, since  yTAx = yTλx = λyTx = 0.   8.116   Summarising, those column matrices x of unit magnitude that make the quadratic form Q stationary are eigenvectors of the matrix A, and the stationary value of Q is then equal to the corresponding eigenvalue. It is straightforward to see from the proof of  8.114  that, conversely, any eigenvector of A makes Q stationary.  Instead of maximising or minimising Q = xTAx subject to the constraint  xTx = 1, an equivalent procedure is to extremise the function  λ x  =  xTAx xTx .   cid:1 Show that if λ x  is stationary then x is an eigenvector of A and λ x  is equal to the corresponding eigenvalue.  We require ∆λ x  = 0 with respect to small variations in x. Now   cid:18    cid:5   cid:7   ∆λ =  1   xTx  − 2   xTx 2 2∆xTAx  xTx  =   cid:8   xTAx xTx  ∆xTx xTx ,   cid:6  − xTAx   cid:5   ∆xTAx + xTA ∆x  ∆xTx + xT∆x   cid:6  cid:19   since xTA ∆x =  ∆xT Ax and xT∆x =  ∆xT x. Thus  ∆λ =  2  xTx ∆xT[Ax − λ x x].  Hence, if ∆λ = 0 then Ax = λ x x, i.e. x is an eigenvector of A with eigenvalue λ x .  cid:2   Thus the eigenvalues of a symmetric matrix A are the values of the function  at its stationary points. The eigenvectors of A lie along those directions in space for which the quadratic form Q = xTAx has stationary values, given a ﬁxed magnitude for the vector x. Similar results hold for Hermitian matrices.  λ x  =  xTAx xTx  291   MATRICES AND VECTOR SPACES  8.17.2 Quadratic surfaces  The results of the previous subsection may be turned round to state that the surface given by  xTAx = constant = 1  say    8.117   and called a quadratic surface, has stationary values of its radius  i.e. origin– surface distance  in those directions that are along the eigenvectors of A. More speciﬁcally, in three dimensions the quadratic surface xTAx = 1 has its principal axes along the three mutually perpendicular eigenvectors of A, and the squares , i = 1, 2, 3. As well as of the corresponding principal radii are given by λ i having this stationary property of the radius, a principal axis is characterised by the fact that any section of the surface perpendicular to it has some degree of symmetry about it. If the eigenvalues corresponding to any two principal axes are degenerate then the quadratic surface has rotational symmetry about the third principal axis and the choice of a pair of axes perpendicular to that axis is not uniquely deﬁned.  cid:1 Find the shape of the quadratic surface  −1  1 + x2 x2  2  − 3x2  3 + 2x1x2 + 6x1x3 − 6x2x3 = 1.  If, instead of expressing the quadratic surface in terms of x1, x2, x3, as in  8.107 , we  cid:7  are along the three mutually perpendicular eigenvector directions  1, 1, 0 ,  1,−1, 1  and were to use the new variables x 3 deﬁned in  8.111 , for which the coordinate axes  1,−1,−2 , then the equation of the surface would take the form  see  8.110     cid:7  1, x   cid:7  2, x  2   cid:7  √ 1 2 2  x  1   +  2  2   cid:7  √ 2 3 2  x  1  √ 2 and 1    cid:7  √ − x 3 6 2  1  √ 3. Similarly a section in the plane  3 = 0, i.e. x1 − x2 −  cid:7   = 1.  Thus, for example, a section of the quadratic surface in the plane x 2x3 = 0, is an ellipse, with semi-axes 1   cid:7  1 = x1 + x2 = 0 is a hyperbola.  cid:2  x  Clearly the simplest three-dimensional situation to visualise is that in which all  the eigenvalues are positive, since then the quadratic surface is an ellipsoid.  8.18 Simultaneous linear equations  In physical applications we often encounter sets of simultaneous linear equations. In general we may have M equations in N unknowns x1, x2, . . . , xN of the form  A11x1 + A12x2 + ··· + A1N xN = b1, A21x1 + A22x2 + ··· + A2N xN = b2,  AM1x1 + AM2x2 + ··· + AMN xN = bM,  ...  292   8.118    8.18 SIMULTANEOUS LINEAR EQUATIONS  where the Aij and bi have known values. If all the bi are zero then the system of equations is called homogeneous, otherwise it is inhomogeneous. Depending on the given values, this set of equations for the N unknowns x1, x2, . . . , xN may have either a unique solution, no solution or inﬁnitely many solutions. Matrix analysis may be used to distinguish between the possibilities. The set of equations may be expressed as a single matrix equation Ax = b, or, written out in full, as   A11  A12 A21 A22 ... ... AM1 AM2  . . . A1N . . . A2N ... . . . . . . AMN   =       x1  x2 ... xN   .  b1  b2 ...  bM  8.18.1 The range and null space of a matrix  As we discussed in section 8.2, we may interpret the matrix equation Ax = b as representing, in some basis, the linear transformation A x = b of a vector x in an N-dimensional vector space V into a vector b in some other  in general diﬀerent  M-dimensional vector space W . In general the operator A will map any vector in V into some particular subspace of W , which may be the entire space. This subspace is called the range of A  or A  and its dimension is equal to the rank of A. Moreover, if A  and hence A  is singular then there exists some subspace of V that is mapped onto the zero vector 0 in W ; that is, any vector y that lies in the subspace satisﬁes A y = 0. This subspace is called the null space of A and the dimension of this null space is called the nullity of A. We note that the matrix A must be singular if M  cid:3 = N and may be singular even if M = N.  The dimensions of the range and the null space of a matrix are related through  the fundamental relationship  rank A + nullity A = N,   8.119   where N is the number of original unknowns x1, x2, . . . , xN.  cid:1 Prove the relationship  8.119 . As discussed in section 8.11, if the columns of an M × N matrix A are interpreted as the components, in a given basis, of N  M-component  vectors v1, v2, . . . , vN then rank A is equal to the number of linearly independent vectors in this set  this number is also equal to the dimension of the vector space spanned by these vectors . Writing  8.118  in terms of the vectors v1, v2, . . . , vN, we have  x1v1 + x2v2 + ··· + xNvN = b.   8.120   From this expression, we immediately deduce that the range of A is merely the span of the vectors v1, v2, . . . , vN and hence has dimension r = rank A.  293   MATRICES AND VECTOR SPACES  If a vector y lies in the null space of A then A y = 0, which we may write as  y1v1 + y2v2 + ··· + yNvN = 0.  As just shown above, however, only r  ≤ N  of these vectors are linearly independent. By  renumbering, if necessary, we may assume that v1, v2, . . . , vr form a linearly independent set; the remaining vectors, vr+1, vr+2, . . . , vN , can then be written as a linear superposition  of v1, v2, . . . , vr. We are therefore free to choose the N − r coeﬃcients yr+1, yr+2, . . . , yN are not all zero . The dimension of the null space is therefore N − r, and this completes arbitrarily and  8.121  will still be satisﬁed for some set of r coeﬃcients y1, y2, . . . , yr  which the proof of  8.119 .  cid:2    8.121   Equation  8.119  has far-reaching consequences for the existence of solutions to sets of simultaneous linear equations such as  8.118 . As mentioned previously, these equations may have no solution, a unique solution or inﬁnitely many solutions. We now discuss these three cases in turn.  No solution  The system of equations possesses no solution unless b lies in the range of A ; in this case  8.120  will be satisﬁed for some x1, x2, . . . , xN. This in turn requires the set of vectors b, v1, v2, . . . , vN to have the same span  see  8.8   as v1, v2, . . . , vN. In terms of matrices, this is equivalent to the requirement that the matrix A and the augmented matrix   A11  M =  A12 A22  A21 ... AM1 AM2  . . . A1N . . . A2N . . . . . . AMN  b1 b1 ... bM    have the same rank r. If this condition is satisﬁed then b does lie in the range of A , and the set of equations  8.118  will have either a unique solution or inﬁnitely many solutions. If, however, A and M have diﬀerent ranks then there will be no solution.  If b lies in the range of A and if r = N then all the vectors v1, v2, . . . , vN in  8.120  are linearly independent and the equation has a unique solution x1, x2, . . . , xN.  A unique solution  Inﬁnitely many solutions  in  8.120  are linearly independent. We may therefore choose the coeﬃcients of  If b lies in the range of A and if r < N then only r of the vectors v1, v2, . . . , vN n − r vectors in an arbitrary way, while still satisfying  8.120  for some set of coeﬃcients x1, x2, . . . , xN. There are therefore inﬁnitely many solutions, which span an  n− r -dimensional vector space. We may also consider this space of solutions in terms of the null space of A: if x is some vector satisfying A x = b and y is  294   8.18 SIMULTANEOUS LINEAR EQUATIONS  any vector in the null space of A  i.e. A y = 0  then  A  x + y  = A x + A y = A x + 0 = b,  and so x + y is also a solution. Since the null space is  n − r -dimensional, so too  is the space of solutions.  We may use the above results to investigate the special case of the solution of a homogeneous set of linear equations, for which b = 0. Clearly the set always has  the trivial solution x1 = x2 = ··· = xn = 0, and if r = N this will be the only null space of A, which has dimension n − r. In particular, we note that if M < N  solution. If r < N, however, there are inﬁnitely many solutions; they form the   i.e. there are fewer equations than unknowns  then r < N automatically. Hence a set of homogeneous linear equations with fewer equations than unknowns always has inﬁnitely many solutions.  8.18.2 N simultaneous linear equations in N unknowns  A special case of  8.118  occurs when M = N. In this case the matrix A is square and we have the same number of equations as unknowns. Since A is square, the condition r = N corresponds to A  cid:3 = 0 and the matrix A is non-singular. The case r < N corresponds to A = 0, in which case A is singular. As mentioned above, the equations will have a solution provided b lies in the range of A. If this is true then the equations will possess a unique solution when A  cid:3 = 0 or inﬁnitely many solutions when A = 0. There exist several methods  for obtaining the solution s . Perhaps the most elementary method is Gaussian elimination; this method is discussed in subsection 27.3.1, where we also address numerical subtleties such as equation interchange  pivoting . In this subsection, we will outline three further methods for solving a square set of simultaneous linear equations.  Since A is square it will possess an inverse, provided A  cid:3 = 0. Thus, if A is  Direct inversion  non-singular, we immediately obtain  x = A−1b   8.122   as the unique solution to the set of equations. However, if b = 0 then we see immediately that the set of equations possesses only the trivial solution x = 0. The direct inversion method has the advantage that, once A−1 has been calculated, one may obtain the solutions x corresponding to diﬀerent vectors b1, b2, . . . on the RHS, with little further work.  295   MATRICES AND VECTOR SPACES   cid:1 Show that the set of simultaneous equations  2x1 + 4x2 + 3x3 = 4,  x1 − 2x2 − 2x3 = 0, −3x1 + 3x2 + 2x3 = −7,   8.123    8.124   has a unique solution, and ﬁnd that solution.  The simultaneous equations can be represented by the matrix equation Ax = b, i.e.   2  =  1 11   x1  x2 x3  4  3  1 −2 −2 −3  2  3   2  −2 −3 −18 −8  1 13  7  4   4  =  .  2−3  4  =  0−7  0−7  4   .   x1  x2 x3  As we have already shown that A−1 exists and have calculated it, see  8.59 , it follows that x = A−1b or, more explicitly, that  Thus the unique solution is x1 = 2, x2 = −3, x3 = 4.  cid:2   LU decomposition  Although conceptually simple, ﬁnding the solution by calculating A−1 can be computationally demanding, especially when N is large. In fact, as we shall now show, it is not necessary to perform the full inversion of A in order to solve the simultaneous equations Ax = b. Rather, we can perform a decomposition of the matrix into the product of a square lower triangular matrix L and a square upper triangular matrix U, which are such that  A = LU,   8.125   and then use the fact that triangular systems of equations can be solved very simply.  We must begin, therefore, by ﬁnding the matrices L and U such that  8.125  is satisﬁed. This may be achieved straightforwardly by writing out  8.125  in component form. For illustration, let us consider the 3 × 3 case. It is, in fact, always possible, and convenient, to take the diagonal elements of L as unity, so we have   1  U11  0 1 L21 L31 L32  A =   U11 U12 U13  0 U22 U23 0 0 U33  0 0 1    U12  U13  =  L21U11 L31U11 L31U12 + L32U22 L31U13 + L32U23 + U33  L21U13 + U23  L21U12 + U22    8.126   The nine unknown elements of L and U can now be determined by equating  296   8.18 SIMULTANEOUS LINEAR EQUATIONS  the nine elements of  8.126  to those of the 3 × 3 matrix A. This is done in the  particular order illustrated in the example below.  Once the matrices L and U have been determined, one can use the decomposition to solve the set of equations Ax = b in the following way. From  8.125 , we have LUx = b, but this can be written as two triangular sets of equations  Ly = b  and  Ux = y,  where y is another column matrix to be determined. One may easily solve the ﬁrst triangular set of equations for y, which is then substituted into the second set. The required solution x is then obtained readily from the second triangular set of equations. We note that, as with direct inversion, once the LU decomposition has been determined, one can solve for various RHS column matrices b1, b2, . . . , with little extra work.  cid:1 Use LU decomposition to solve the set of simultaneous equations  8.123 .  We begin the determination of the matrices L and U by equating the elements of the matrix in  8.126  with those of the matrix   2  A =  4  3  1 −2 −2 −3  3  2   .  This is performed in the following order:  1st row:  U11 = 2, L21U11 = 1,  1st column:  L21U12 + U22 = −2  2nd row: 2nd column: L31U12 + L32U22 = 3 3rd row:  L31U13 + L32U23 + U33 = 2  Thus we may write the matrix A as  U12 = 4,  L31U11 = −3 2 , L31 = − 3 L21U13 + U23 = −2 ⇒ U22 = −4, U23 = − 7  2  2  U13 = 3  ⇒ L21 = 1 ⇒ L32 = − 9 ⇒ U33 = − 11  4  8  We must now solve the set of equations Ly = b, which read  A = LU =   1  1 2  − 3  2   2   1  1 2  − 3  2   2  =  0  0  0    y1  1  y2 y3  0 1 − 9  4    0 1 − 9  4  0  0  1   .  4  3  0 −4 − 7 0 − 11  2  8   .   .  0−7   4  4−2− 11  2   =  3  4  0 −4 − 7 0 − 11  0  2  8     x1  x2 x3  297  Since this set of equations is triangular, we quickly ﬁnd  y1 = 4,  y2 = 0 −   1  2   4  = −2,  y3 = −7 −  − 3  2   4  −  − 9  4   −2  = − 11  2 .  These values must then be substituted into the equations Ux = y, which read   MATRICES AND VECTOR SPACES  This set of equations is also triangular, and we easily ﬁnd the solution  x1 = 2,  x2 = −3,  x3 = 4,  which agrees with the result found above by direct inversion.  cid:2   We note, in passing, that one can calculate both the inverse and the determinant  of A from its LU decomposition. To ﬁnd the inverse A−1, one solves the system of equations Ax = b repeatedly for the N diﬀerent RHS column matrices b = ei, i = 1, 2, . . . , N, where ei is the column matrix with its ith element equal to unity and the others equal to zero. The solution x in each case gives the corresponding column of A−1. Evaluation of the determinant A is much simpler. From  8.125 ,  we have  A = LU = LU.   8.127   Since L and U are triangular, however, we see from  8.64  that their determinants are equal to the products of their diagonal elements. Since Lii = 1 for all i, we thus ﬁnd  A = U11U22 ··· UNN =  Uii.  N cid:3   i=1  As an illustration, in the above example we ﬁnd A =  2  −4  −11 8  = 11,  which, as it must, agrees with our earlier calculation  8.58 .  Finally, we note that if the matrix A is symmetric and positive semi-deﬁnite  then we can decompose it as  A = LL†  ,   8.128   where L is a lower triangular matrix whose diagonal elements are not, in general, equal to unity. This is known as a Cholesky decomposition  in the special case where A is real, the decomposition becomes A = LLT . The reason that we cannot set the diagonal elements of L equal to unity in this case is that we require the same number of independent elements in L as in A. The requirement that the matrix be positive semi-deﬁnite is easily derived by considering the Hermitian form  or quadratic form in the real case   x†Ax = x†LL†x =  L†x  †   L†x .  Denoting the column matrix L†x by y, we see that the last term on the RHS is y†y, which must be greater than or equal to zero. Thus, we require x†Ax ≥ 0 for any arbitrary column matrix x, and so A must be positive semi-deﬁnite  see section 8.17 .  We recall that the requirement that a matrix be positive semi-deﬁnite is equiv- alent to demanding that all the eigenvalues of A are positive or zero. If one of the eigenvalues of A is zero, however, then from  8.103  we have A = 0 and so A is singular. Thus, if A is a non-singular matrix, it must be positive deﬁnite  rather  298   8.18 SIMULTANEOUS LINEAR EQUATIONS  than just positive semi-deﬁnite  in order to perform the Cholesky decomposition  8.128 . In fact, in this case, the inability to ﬁnd a matrix L that satisﬁes  8.128  implies that A cannot be positive deﬁnite.  The Cholesky decomposition can be applied in an analogous way to the LU  decomposition discussed above, but we shall not explore it further.  Cramer’s rule  An alternative method of solution is to use Cramer’s rule, which also provides some insight into the nature of the solutions in the various cases. To illustrate this method let us consider a set of three equations in three unknowns,  A11x1 + A12x2 + A13x3 = b1,  A21x1 + A22x2 + A23x3 = b2,  A31x1 + A32x2 + A33x3 = b3,   8.129   which may be represented by the matrix equation Ax = b. We wish either to ﬁnd the solution s  x to these equations or to establish that there are no solutions. From result  vi  of subsection 8.9.1, the determinant A is unchanged by adding  to its ﬁrst column the combination  ×  second column of A  +  x2 x1   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  A11 A12 A13  A21 A22 A23 A31 A32 A33   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  =  We thus obtain  A =  x3 x1  ×  third column of A .  A21 +  x2 x1 A22 +  x3 x1 A23 A22 A23 A31 +  x2 x1 A32 +  x3 x1 A33 A32 A33   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  A11 +  x2 x1 A12 +  x3 x1 A13 A12 A13  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  b1 A12 A13   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  =  1 x1  b2 A22 A23 b3 A32 A33  ∆1.   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  ,  A =  1 x1  which, on substituting bi x1 for the ith entry in the ﬁrst column, yields  The determinant ∆1 is known as a Cramer determinant. Similar manipulations of  the second and third columns of A yield x2 and x3, and so the full set of results  reads  where  ∆1 =   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  b1 A12 A13  b2 A22 A23 b3 A32 A33  x1 =  ∆1A ,   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  , ∆2 =  x2 =   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  A11  A21 A31  ∆2A ,  x3 =  ∆3A ,   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  , ∆3 =   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  A11 A12  A21 A22 A31 A32   8.130    cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  .  b1 b2 b3  b1 A13 b2 A23 b3 A33  It can be seen that each Cramer determinant ∆i is simply A but with column i replaced by the RHS of the original set of equations. If A  cid:3 = 0 then  8.130  gives  299   MATRICES AND VECTOR SPACES  the unique solution. The proof given here appears to fail if any of the solutions xi is zero, but it can be shown that result  8.130  is valid even in such a case.  cid:1 Use Cramer’s rule to solve the set of simultaneous equations  8.123 .  Let us again represent these simultaneous equations by the matrix equation Ax = b, i.e.  4  1 −2 −2 −3  3   2  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  ,  2  3  x2 x3   x1  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  2  −3 −7  4  1  0−7   4  =  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  ,  3  2  0 −2   .  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  2  From  8.58 , the determinant of A is given by A = 11. Following the discussion given  above, the three Cramer determinants are  ∆1 =  3  4  0 −2 −2 −7  3  2  ∆2 =  ∆3 =  4  1 −2 −3  3 −7  4 0   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  4   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  .  These may be evaluated using the properties of determinants listed in subsection 8.9.1  and we ﬁnd ∆1 = 22, ∆2 = −33 and ∆3 = 44. From  8.130  the solution to the equations   8.123  is given by  44 11 which agrees with the solution found in the previous example.  cid:2   22 11  x2 =  x3 =  x1 =  = 2,  11  = 4,  −33  = −3,  At this point it is useful to consider each of the three equations  8.129  as rep- resenting a plane in three-dimensional Cartesian coordinates. Using result  7.42  of chapter 7, the sets of components of the vectors normal to the planes are  A11, A12, A13 ,  A21, A22, A23  and  A31, A32, A33 , and using  7.46  the perpendic- ular distances of the planes from the origin are given by   cid:5    cid:6   di =  bi i1 + A2 i2 + A2 i3  A2  1 2  for i = 1, 2, 3.  Finding the solution s  to the simultaneous equations above corresponds to ﬁnding the point s  of intersection of the planes.  If there is a unique solution the planes intersect at only a single point. This happens if their normals are linearly independent vectors. Since the rows of A represent the directions of these normals, this requirement is equivalent to A  cid:3 = 0. If b =  0 0 0 T = 0 then all the planes pass through the origin and, since there is only a single solution to the equations, the origin is that solution. Let us now turn to the cases where A = 0. The simplest such case is that in  which all three planes are parallel; this implies that the normals are all parallel and so A is of rank 1. Two possibilities exist:   i  the planes are coincident, i.e. d1 = d2 = d3, in which case there is an  inﬁnity of solutions;   ii  the planes are not all coincident, i.e. d1  cid:3 = d2 and or d1  cid:3 = d3 and or  d2  cid:3 = d3, in which case there are no solutions.  300   8.18 SIMULTANEOUS LINEAR EQUATIONS   a    b   Figure 8.1 The two possible cases when A is of rank 2. In both cases all the normals lie in a horizontal plane but in  a  the planes all intersect on a single line  corresponding to an inﬁnite number of solutions  whilst in  b  there are no common intersection points  no solutions .  It is apparent from  8.130  that case  i  occurs when all the Cramer determinants are zero and case  ii  occurs when at least one Cramer determinant is non-zero.  The most complicated cases with A = 0 are those in which the normals to the planes themselves lie in a plane but are not parallel. In this case A has rank 2. Again two possibilities exist and these are shown in ﬁgure 8.1. Just as in the rank-1 case, if all the Cramer determinants are zero then we get an inﬁnity of solutions  this time on a line . Of course, in the special case in which b = 0  and the system of equations is homogeneous , the planes all pass through the origin and so they must intersect on a line through it. If at least one of the Cramer determinants is non-zero, we get no solution. These rules may be summarised as follows.  only the trivial solution, x = 0.  origin, and so there is only one solution, given by both  8.122  and  8.130 .   i  A  cid:3 = 0, b  cid:3 = 0: The three planes intersect at a single point that is not the  ii  A  cid:3 = 0, b = 0: The three planes intersect at the origin only and there is  iii  A = 0, b  cid:3 = 0, Cramer determinants all zero: There is an inﬁnity of solutions either on a line if A is rank 2, i.e. the cofactors are not all zero, or on a plane if A is rank 1, i.e. the cofactors are all zero.   iv  A = 0, b  cid:3 = 0, Cramer determinants not all zero: No solutions.  v  A = 0, b = 0: The three planes intersect on a line through the origin  giving an inﬁnity of solutions.  8.18.3 Singular value decomposition  There exists a very powerful technique for dealing with a simultaneous set of linear equations Ax = b, such as  8.118 , which may be applied whether or not  301   MATRICES AND VECTOR SPACES  the number of simultaneous equations M is equal to the number of unknowns N. This technique is known as singular value decomposition  SVD  and is the method of choice in analysing any set of simultaneous linear equations.  We will consider the general case, in which A is an M × N  complex  matrix. § Let us suppose we can write A as the product A = USV†   8.131   ,  where the matrices U, S and V have the following properties.   i  The square matrix U has dimensions M × M and is unitary.  ii  The matrix S has dimensions M × N  the same dimensions as those of A  and is diagonal in the sense that Sij = 0 if i  cid:3 = j. We denote its diagonal  elements by si for i = 1, 2, . . . , p, where p = min M, N ; these elements are termed the singular values of A.   iii  The square matrix V has dimensions N × N and is unitary.  ,   8.133    8.132   A†A = VS†U†USV† AA† = USV†VS†U†  These two equations imply that both V−1A†AV  are diagonal matrices with dimensions N × N and M × M i , i = 1, 2, . . . , p,  We must now determine the elements of these matrices in terms of the elements of A. From the matrix A, we can construct two square matrices: A†A with dimensions with dimensions M×M. Both are clearly Hermitian. From  8.131 , N×N and AA† and using the fact that U and V are unitary, we ﬁnd = VS†SV† = USS†U† where S†S and SS†  cid:5  respectively. The ﬁrst p elements of each diagonal matrix are s2 where p = min M, N , and the rest  where they exist  are zero. = V−1A†A V† and, by a similar argument, U−1AA†U, must be diagonal. From our discussion of the diagonalisation of Hermitian matrices in section 8.16, we see that the columns of V must therefore be the normalised eigenvectors vi, i = 1, 2, . . . , N, of the matrix A†A and the columns of U must be the normalised eigenvectors uj, j = 1, 2, . . . , M, of the matrix AA† i = λi, where the λi are the eigenvalues of the smaller of A†A and AA† . Clearly, the λi are also some of the eigenvalues of the larger of these two matrices, the remaining ones being equal to zero. Since each matrix is Hermitian, the λi are real and the singular values si may be taken as real and non-negative. Finally, to make the decomposition  8.131  unique, it is customary to arrange the singular values in  . Moreover, the singular values si must satisfy s2  decreasing order of their values, so that s1 ≥ s2 ≥ ··· ≥ sp.  −1     cid:6   §  The proof that such a decomposition always exists is beyond the scope of this book. For a full account of SVD one might consult, for example, G. H. Golub and C. F. Van Loan, Matrix Computations, 3rd edn  Baltimore MD: Johns Hopkins University Press, 1996 .  302   8.18 SIMULTANEOUS LINEAR EQUATIONS   cid:1 Show that, for i = 1, 2, . . . , p, Avi = siui and A†ui = sivi, where p = min M, N . Post-multiplying both sides of  8.131  by V, and using the fact that V is unitary, we obtain  AV = US.  Avi = siui.  Since the columns of V and U consist of the vectors vi and uj respectively and S has only diagonal non-zero elements, we ﬁnd immediately that, for i = 1, 2, . . . , p,   8.134   Moreover, we note that Avi = 0 for i = p + 1, p + 2, . . . , N.  Taking the Hermitian conjugate of both sides of  8.131  and post-multiplying by U, we  obtain  A†U = VS†  = VST,  where we have used the fact that U is unitary and S is real. We then see immediately that, for i = 1, 2, . . . , p,   8.135  We also note that A†ui = 0 for i = p + 1, p + 2, . . . , M. Results  8.134  and  8.135  are useful for investigating the properties of the SVD.  cid:2   A†ui = sivi.  The decomposition  8.131  has some advantageous features for the analysis of sets of simultaneous linear equations. These are best illustrated by writing the decomposition  8.131  in terms of the vectors ui and vi as  A =  † siui vi   ,  where p = min M, N . It may be, however, that some of the singular values si are zero, as a result of degeneracies in the set of M linear equations Ax = b. Let us suppose that there are r non-zero singular values. Since our convention is to arrange the singular values in order of decreasing size, the non-zero singular values are si, i = 1, 2, . . . , r, and the zero singular values are sr+1, sr+2, . . . , sp. Therefore we can write A as  A =  † siui vi   .   8.136   Let us consider the action of  8.136  on an arbitrary vector x. This is given by  Ax =  siui vi   †x.  †x is just a number, we see immediately that the vectors ui, i = 1, 2, . . . , r, Since  vi  must span the range of the matrix A; moreover, these vectors form an orthonor- mal basis for the range. Further, since this subspace is r-dimensional, we have rank A = r, i.e. the rank of A is equal to the number of non-zero singular values. The SVD is also useful in characterising the null space of A. From  8.119 , we already know that the null space must have dimension N − r; so, if A has r  p cid:4   i=1  r cid:4  r cid:4   i=1  i=1  303   MATRICES AND VECTOR SPACES  non-zero singular values si, i = 1, 2, . . . , r, then from the worked example above we have  Avi = 0  for i = r + 1, r + 2, . . . , N.  Thus, the N − r vectors vi, i = r + 1, r + 2, . . . , N, form an orthonormal basis for the null space of A.  cid:1  Find the singular value decompostion of the matrix   2  17 10  3 5  A =   .  2 1 10  9 5  2 − 17 − 3  10  5  2 − 1 − 9  10  5  The matrix A has dimension 3 × 4  i.e. M = 3, N = 4 , and so we may construct from it the 3 × 3 matrix AA† and the 4 × 4 matrix A†A  in fact, since A is real, the Hermitian conjugates are just transposes . We begin by ﬁnding the eigenvalues λi and eigenvectors ui of the smaller matrix AA†  . This matrix is easily found to be given by  and its characteristic equation reads  0  0 29 5 12 5   ,   16  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  =  16 − λ  36 − 13λ + λ2  = 0.  0 12 5 36 5  0   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  16 − λ  0 0  AA†  =  29 5  0  − λ  12 5  36 5  0 12 5  − λ   4  0 0  S =  0 3 0  0 0 2  0 0 0  Thus, the eigenvalues are λ1 = 16, λ2 = 9, λ3 = 4. Since the singular values of A are given by si =  √ λi and the matrix S in  8.131  has the same dimensions as A, we have  where we have arranged the singular values in order of decreasing size. Now the matrix U has as its columns the normalised eigenvectors ui of the 3×3 matrix AA† . These normalised eigenvectors correspond to the eigenvalues of AA† λ1 = 16 ⇒ u1 =  1 0 0 T λ2 = 9 ⇒ u2 =  0 5  T λ3 = 4 ⇒ u3 =  0 − 4  as follows:  3 5  3  4  5  T,  and so we obtain the matrix   8.137    8.138    8.139    ,  5   .  The columns of the matrix V in  8.131  are the normalised eigenvectors of the 4 × 4  matrix A†A, which is given by  U =  0 3 5 4 5  0 − 4  5  3 5  0   1  29  0  21 3 11   .  21 29 11 3  3 11 29 21  11 3 21 29  304  A†A =  1 4   8.18 SIMULTANEOUS LINEAR EQUATIONS  We already know from the above discussion, however, that the non-zero eigenvalues of this matrix are equal to those of AA† found above, and that the remaining eigenvalue is zero. The corresponding normalised eigenvectors are easily found:  λ1 = 16 ⇒ v1 = 1 λ2 = 9 ⇒ v2 = 1 λ3 = 4 ⇒ v3 = 1 λ4 = 0 ⇒ v4 = 1  1 1 T  2  1 1 2  1 1 − 1 − 1 T 2  −1 1 1 − 1 T 2  1 − 1 1 − 1 T 1 −1   .  1  1  1  1 −1 1 −1 1 −1 −1 −1  1  1   1  V =  1 2  vi =  A†ui  1 si  for i = 1, 2, 3.  and so the matrix V is given by  Alternatively, we could have found the ﬁrst three columns of V by using the relation  8.135  to obtain   8.140   The fourth eigenvector could then be found using the Gram–Schmidt orthogonalisation procedure. We note that if there were more than one eigenvector corresponding to a zero eigenvalue then we would need to use this procedure to orthogonalise these eigenvectors before constructing the matrix V.  Collecting our results together, we ﬁnd the SVD of the matrix A:   1  0  0     4  0 0  0 3 5 4 5  0 − 4  5  3 5  0 3 0  0 0 2  0 0 0     1  2 1 2  − 1  2  1 2   ;  1 2 1 2 1 2  − 1  2  1 2  − 1  2  1 2 1 2  1 2  2  − 1 − 1 − 1  2  2  A = USV†  =  this can be veriﬁed by direct multiplication.  cid:2   Let us now consider the use of SVD in solving a set of M simultaneous linear equations in N unknowns, which we write again as Ax = b. Firstly, consider the solution of a homogeneous set of equations, for which b = 0. As mentioned previously, if A is square and non-singular  and so possesses no zero singular values  then the equations have the unique trivial solution x = 0. Otherwise, any of the vectors vi, i = r + 1, r + 2, . . . , N, or any linear combination of them, will be a solution.  In the inhomogeneous case, where b is not a zero vector, the set of equations will possess solutions if b lies in the range of A. To investigate these solutions, it is convenient to introduce the N × M matrix S, which is constructed by taking the transpose of S in  8.131  and replacing each non-zero singular value si on the diagonal by 1 si. It is clear that, with this construction, SS is an M × M which sj  cid:3 = 0, and zero otherwise.  diagonal matrix with diagonal entries that equal unity for those values of j for  Now consider the vector  ˆx = VSU†b.  305   8.141    MATRICES AND VECTOR SPACES   8.142   Using the unitarity of the matrices U and V, we ﬁnd that  Aˆx − b = USSU†b − b = U SS − I U†b.  The matrix  SS − I  is diagonal and the jth element on its leading diagonal is non-zero  and equal to −1  only when sj = 0. However, the jth element of the vector U†b is given by the scalar product  uj  †b; if b lies in the range of A, this scalar product can be non-zero only if sj  cid:3 = 0. Thus the RHS of  8.142  must equal zero, and so ˆx given by  8.141  is a solution to the equations Ax = b. We may, however, add to this solution any linear combination of the N − r vectors vi, i = r + 1, r + 2, . . . , N, that form an orthonormal basis for the null space of A; thus, in general, there exists an inﬁnity of solutions  although it is straightforward to show that  8.141  is the solution vector of shortest length . The only way in which the solution  8.141  can be unique is if the rank r equals N, so that the matrix A does not possess a null space; this only occurs if A is square and non-singular.  If b does not lie in the range of A then the set of equations Ax = b does not have a solution. Nevertheless, the vector  8.141  provides the closest possible ‘solution’ in a least-squares sense. In other words, although the vector  8.141  does not exactly solve Ax = b, it is the vector that minimises the residual   cid:4  = Ax − b,  where here the vertical lines denote the absolute value of the quantity they contain, not the determinant. This is proved as follows. Suppose we were to add some arbitrary vector x cid:7  This would result in the addition of the vector b cid:7  = Ax cid:7  the range of A since any part of x cid:7  nothing to Ax cid:7  . We would then have  to the vector ˆx in  8.141 . to Aˆx− b; b cid:7  is clearly in belonging to the null space of A contributes  Aˆx − b + b cid:7  =  USSU† − I b + b cid:7   = U[ SS − I U†b + U†b cid:7  =  SS − I U†b + U†b cid:7 ;  ]   8.143   is given by the scalar product  uj   in the last line we have made use of the fact that the length of a vector is left unchanged by the action of the unitary matrix U. Now, the jth component of the vector  SS − I U†b will only be non-zero when sj = 0. However, the jth element of the vector U†b cid:7  sj  cid:3 = 0, since b cid:7   8.143  for two disjoint sets of j-values, its minimum value, as x cid:7  when b cid:7   cid:1 Find the solution s  to the set of simultaneous linear equations Ax = b, where A is given by  8.137  and b =  1  , which is non-zero only if lies in the range of A. Thus, as these two terms only contribute to is varied, occurs  = 0; this requires x cid:7   = 0.  †b cid:7   0 T.  0  To solve the set of equations, we begin by calculating the vector given in  8.141 ,  x = VSU†b,  306   where U and V are given by  8.139  and  8.140  respectively and S is obtained by taking the transpose of S in  8.138  and replacing all the non-zero singular values si by 1 si. Thus, S reads  8.19 EXERCISES   1  4 0  0 0  S =   .  0 1 3 0 0  0  0 1 2 0  Substituting the appropriate matrices into the expression for x we ﬁnd  x = 1  8  1 1 1 1 T.   8.144   It is straightforward to show that this solves the set of equations Ax = b exactly, and so the vector b =  1 0 0 T must lie in the range of A. This is, in fact, immediately clear, since b = u1. The solution  8.144  is not, however, unique. There are three non-zero singular values, but N = 4. Thus, the matrix A has a one-dimensional null space, which is ‘spanned’ by v4, the fourth column of V, given in  8.140 . The solutions to our set of equations, consisting of the sum of the exact solution and any vector in the null space of A, therefore lie along the line  8  1 1 1 1 T + α 1 − 1 1 − 1 T,  x = 1  where the parameter α can take any real value. We note that  8.144  is the point on this line that is closest to the origin.  cid:2   8.19 Exercises  8.1  Which of the following statements about linear vector spaces are true? Where a statement is false, give a counter-example to demonstrate this.   a  Non-singular N × N matrices form a vector space of dimension N2.  b  Singular N × N matrices form a vector space of dimension N2.   c  Complex numbers form a vector space of dimension 2.  d  Polynomial functions of x form an inﬁnite-dimensional vector space.   e  Series {a0, a1, a2, . . . , aN} for which  an2 = 1 form an N-dimensional  N n=0   cid:11    f  Absolutely convergent series form an inﬁnite-dimensional vector space.  g  Convergent series with terms of alternating sign form an inﬁnite-dimensional  vector space.  vector space.  8.2  Evaluate the determinants  and   a    b    cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  a  h g  g f c   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  ,  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  gc  0 c a  h b f   c    cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  1  0  2  0  1 −2 1 −2  3 −3 −2  4 −2  3 1  1   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20    cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  .  ge b e b  a + ge  gb + ge  b e  b + f  b  b + e b + d  307   MATRICES AND VECTOR SPACES  8.3  Using the properties of determinants, solve with a minimum of calculation the following equations for x:   a    b   8.4  Consider the matrices   a  B =   b  C =  i  0 −i  i  0  i  −i   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  x  0 −i  a b x c  a x b b  a a a   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  = 0,  ,  1 1 1 1   cid:7   x − 3  x + 5 x + 1  x + 3   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  x + 2  √  x − 2  1√ 8  x + 4  x  x − 1 3 −√ √ 6 0  1 2  2 −√ −1  2   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  = 0.  .  3  Are they  i  real,  ii  diagonal,  iii  symmetric,  iv  antisymmetric,  v  singular,  vi  orthogonal,  vii  Hermitian,  viii  anti-Hermitian,  ix  unitary,  x  normal? By considering the matrices   cid:8    cid:7   A =  1 0  0 0  ,  B =  0 3  0 4   cid:8   ,  show that AB = 0 does not imply that either A or B is the zero matrix, but that it does imply that at least one of them is singular. This exercise considers a crystal whose unit cell has base vectors that are not necessarily mutually orthogonal.   cid:11    a  The basis vectors of the unit cell of a crystal, with the origin O at one corner,  are denoted by e1, e2, e3. The matrix G has elements Gij , where Gij = ei · ej and Hij are the elements of the matrix H ≡ G−1. Show that the vectors  j Hij ej are the reciprocal vectors and that Hij = fi · fj.  fi =   b  If the vectors u and v are given by   cid:4    cid:4   u =  uiei,  v =  vifi,  obtain expressions for u, v, and u · v. If the basis vectors are each of length a and the angle between each pair is π 3, write down G and hence obtain H.  i  i   c    d  Calculate  i  the length of the normal from O onto the plane containing the  −1e3, and  ii  the angle between this normal and e1.  −1e1, q  −1e2, r  points p  Prove the following results involving Hermitian matrices:  a  If A is Hermitian and U is unitary then U−1AU is Hermitian.  b  If A is anti-Hermitian then iA is Hermitian.  c  The product of two Hermitian matrices A and B is Hermitian if and only if  d  If S is a real antisymmetric matrix then A =  I − S  I + S  −1 is orthogonal.  A and B commute.  If A is given by   cid:8   8.5  8.6  8.7   e   then ﬁnd the matrix S that is needed to express A in the above form. If K is skew-hermitian, i.e. K†  = −K, then V =  I + K  I − K  A and B are real non-zero 3 × 3 matrices and satisfy the equation  −1 is unitary.  8.8   a  Prove that if B is orthogonal then A is antisymmetric.  A =  cos θ  − sin θ  sin θ cos θ   AB T + B−1A = 0.   cid:7   308   8.19 EXERCISES   b  Without assuming that B is orthogonal, prove that A is singular.  8.9  The commutator [X, Y] of two matrices is deﬁned by the equation  [X, Y] = XY − YX.  Two anticommuting matrices A and B satisfy  A2 = I,  B2 = I,  [A, B] = 2iC.   a  Prove that C2 = I and that [B, C] = 2iA.  b  Evaluate [[[A, B], [B, C]], [A, B]].  8.10  The four matrices Sx, Sy, Sz and I are deﬁned by   cid:7   cid:7    cid:8   cid:8   ,  Sx =  Sz =  0 1  1 0  0  1  0 −1  Sy =  ,  I =   cid:7   cid:7    cid:8  0 −i  cid:8   0  i  ,  1 0  0 1  ,  where i2 = −1. Show that S2 x = I and SxSy = iSz, and obtain similar results by permutting x, y and z. Given that v is a vector with Cartesian components  vx, vy, vz , the matrix S v  is deﬁned as  S v  = vxSx + vySy + vzSz.  Prove that, for general non-zero vectors a and b,  S a S b  = a · b I + i S a × b .  Without further calculation, deduce that S a  and S b  commute if and only if a and b are parallel vectors. A general triangle has angles α, β and γ and corresponding opposite sides a, b and c. Express the length of each side in terms of the lengths of the other two sides and the relevant cosines, writing the relationships in matrix and vector form, using the vectors having components a, b, c and cos α, cos β, cos γ. Invert the matrix and hence deduce the cosine-law expressions involving α, β and γ. Given a matrix   1  β 0  A =   ,  α 1 0  0 0 1  where α and β are non-zero complex numbers, ﬁnd its eigenvalues and eigenvec- tors. Find the respective conditions for  a  the eigenvalues to be real and  b  the eigenvectors to be orthogonal. Show that the conditions are jointly satisﬁed if and only if A is Hermitian. Using the Gram–Schmidt procedure:  8.11  8.12  8.13   a  construct an orthonormal set of vectors from the following:  x1 =  0 0 1 1 T, x3 =  1 2 0 2 T,  x2 =  1 0 − 1 0 T, x4 =  2 1  1 1 T;  309   MATRICES AND VECTOR SPACES   b  ﬁnd an orthonormal basis, within a four-dimensional Euclidean space, for  the subspace spanned by the three vectors  1 2 0 0 T,  3 − 1 2 0 T  and  0 0  2 1 T.  8.14  If a unitary matrix U is written as A + iB, where A and B are Hermitian with non-degenerate eigenvalues, show the following:   a  A and B commute;  b  A2 + B2 = I;  c  The eigenvectors of A are also eigenvectors of B;  d  The eigenvalues of U have unit modulus  as is necessary for any unitary  matrix .  8.15  Determine which of the matrices below are mutually commuting, and, for those that are, demonstrate that they have a complete set of eigenvectors in common:  A =  C =  9  6 −2 −2 −10   cid:7   cid:8   cid:7  −9 −10  1  5   cid:8   cid:8   ,  .   cid:8   ,  B =  , D =  1  8  8 −11  14 2  2 11   cid:7   cid:7   .  3 −1 4 −2  2  3  −1 −2  8.16  Find the eigenvalues and a set of eigenvectors of the matrix  8.17  8.18  8.19  Verify that its eigenvectors are mutually orthogonal. Find three real orthogonal column matrices, each of which is a simultaneous eigenvector of   0  0 1    0 1 0  1 0 0  A =  and  B =   0  1 1   .  1 0 1  1 1 0  Use the results of the ﬁrst worked example in section 8.14 to evaluate, without  repeated matrix multiplication, the expression A6x, where x =  2 4 − 1 T and A is the matrix given in the example. Given that A is a real symmetric matrix with normalised eigenvectors ei, obtain the coeﬃcients αi involved when column matrix x, which is the solution of  i αiei. Here µ is a given constant and v is a given column  is expanded as x = matrix.  a  Solve  ∗  when   cid:11   Ax − µx = v,   ∗    ,   2  1 0  A =  1 2 0  0 0 3  µ = 2 and v =  1 2 3 T.   b  Would  ∗  have a solution if µ = 1 and  i  v =  1 2 3 T,  ii  v =   2 2 3 T?  310   8.20  Demonstrate that the matrix  8.19 EXERCISES   2  A =  0 4  −6 3 −1  0 4 0    is defective, i.e. does not have three linearly independent eigenvectors, by showing the following:   a   its eigenvalues are degenerate and, in fact, all equal;   b  any eigenvector has the form  µ  3µ − 2ν   ν T.   c   if two pairs of values, µ1, ν1 and µ2, ν2, deﬁne two independent eigenvectors v1 and v2, then any third similarly deﬁned eigenvector v3 can be written as a linear combination of v1 and v2, i.e.  where  v3 = av1 + bv2,  µ3ν2 − µ2ν3 µ1ν2 − µ2ν1  a =  and b =  µ1ν3 − µ3ν1 µ1ν2 − µ2ν1  .  Illustrate  c  using the example  µ1, ν1  =  1, 1 ,  µ2, ν2  =  1, 2  and  µ3, ν3  =  0, 1 .  Show further that any matrix of the form     2  6n − 6 3 − 3n  0  4 − 2n n − 1   cid:7   0  2n  4 − 4n  cid:8   H =  10  −3i  3i 2  ,  8.21  is defective, with the same eigenvalues and eigenvectors as A. By ﬁnding the eigenvectors of the Hermitian matrix  construct a unitary matrix U such that U†HU = Λ, where Λ is a real diagonal matrix. Use the stationary properties of quadratic forms to determine the maximum and minimum values taken by the expression  8.22  8.23  on the unit sphere, x2 + y2 + z2 = 1. For what values of x, y and z do they occur? Given that the matrix  Q = 5x2 + 4y2 + 4z2 + 2xz + 2xy   2 −1  −1 0 −1  0  2 −1  2    A =  1 T, use the stationary property of the has two eigenvectors of the form  1 y expression J x  = xTAx  xTx  to obtain the corresponding eigenvalues. Deduce the third eigenvalue. Find the lengths of the semi-axes of the ellipse  8.24  8.25  73x2 + 72xy + 52y2 = 100,  and determine its orientation. The equation of a particular conic section is  Q ≡ 8x2  1 + 8x2  2  − 6x1x2 = 110.  Determine the type of conic section this represents, the orientation of its principal axes, and relevant lengths in the directions of these axes.  311   MATRICES AND VECTOR SPACES  8.26  Show that the quadratic surface  5x2 + 11y2 + 5z2 − 10yz + 2xz − 10xy = 4  8.27  8.28  8.29  8.31  is an ellipsoid with semi-axes of lengths 2, 1 and 0.5. Find the direction of its longest axis. Find the direction of the axis of symmetry of the quadratic surface  7x2 + 7y2 + 7z2 − 20yz − 20xz + 20xy = 3.  For the following matrices, ﬁnd the eigenvalues and suﬃcient of the eigenvectors to be able to describe the quadratic surfaces associated with them:   5  1 −1   a   1  −1  5 1  1 5   ,   1  2 2   ,  2 1 2  2 2 1   b    c    1  2 1   .  2 4 2  1 2 1  This exercise demonstrates the reverse of the usual procedure of diagonalising a matrix.  a  Rearrange the result A cid:7   = S−1AS of section 8.16 to express the original matrix A in terms of the unitary matrix S and the diagonal matrix A cid:7  . Hence show how to construct a matrix A that has given eigenvalues and given  orthogonal  column matrices as its eigenvectors.  b  Find the matrix that has as eigenvectors  1 2 1 T,  1 − 1 1 T and  c  Try a particular case, say λ = 3, µ = −2 and ν = 1, and verify by explicit   1 0 − 1 T, with corresponding eigenvalues λ, µ and ν.  solution that the matrix so found does have these eigenvalues.  8.30  Find an orthogonal transformation that takes the quadratic form  Q ≡ −x2  1  − 2x2  2  − x2  3 + 8x2x3 + 6x1x3 + 8x1x2  into the form  − 4y2  3,  1 + µ2y2 and determine µ1 and µ2  see section 8.17 .  µ1y2  2  One method of determining the nullity  and hence the rank  of an M × N matrix A is as follows.   Write down an augmented transpose of A, by adding on the right an N × N unit matrix and thus producing an N ×  M + N  array B.   Subtract a suitable multiple of the ﬁrst row of B from each of the other lower rows so as to make Bi1 = 0 for i > 1.   Subtract a suitable multiple of the second row  or the uppermost row that  does not start with M zero values  from each of the other lower rows so as to make Bi2 = 0 for i > 2.    Continue in this way until all remaining rows have zeros in the ﬁrst M places.  The number of such rows is equal to the nullity of A, and the N rightmost entries of these rows are the components of vectors that span the null space. They can be made orthogonal if they are not so already.  Use this method to show that the nullity of    A =    3  −1 −1 −2  3  2  7 17  10 −6 2 −3 3 −4 0 −8 −4  4  2 4  312   8.19 EXERCISES  8.32  is 2 and that an orthogonal base for the null space of A is provided by any two column matrices of the form  2 + αi − 2αi 1 αi T, for which the αi  i = 1, 2   are real and satisfy 6α1α2 + 2 α1 + α2  + 5 = 0. Do the following sets of equations have non-zero solutions? If so, ﬁnd them.   a  3x + 2y + z = 0,  b  2x = b y + z ,  x − 3y + 2z = 0, x = 2a y − z ,  2x + y + 3z = 0.  x =  6a − b y −  6a + b z.  8.33  Solve the simultaneous equations  8.34  Solve the following simultaneous equations for x1, x2 and x3, using matrix methods:  8.35  Show that the following equations have solutions only if η = 1 or 2, and ﬁnd them in these cases:  8.36  Find the condition s  on α such that the simultaneous equations  have  a  exactly one solution,  b  no solutions, or  c  an inﬁnite number of solutions; give all solutions where they exist. Make an LU decomposition of the matrix  8.37  8.38  and hence solve Ax = b, where  i  b =  21 9 28 T,  ii  b =  21 7 22 T. Make an LU decomposition of the matrix  8.39  Hence solve Ax = b for  i  b =  −4 1 8 −5 T,  ii  b =  −10 0 −3 −24 T. Deduce that det A = −160 and conﬁrm this by direct calculation.  Use the Cholesky separation method to determine whether the following matrices are positive deﬁnite. For each that is, determine the corresponding lower diagonal matrix L:   2  A =  3  1  3 −1  1  1  3 −1   ,  313   5  √ 0 3   .  √  3 0 3  0 3 0  B =  2x + 3y + z = 11, x + y + z = 6,  5x − y + 10z = 34.  x1 + 2x2 + 3x3 = 1, 3x1 + 4x2 + 5x3 = 2, x1 + 3x2 + 4x3 = 3.  x + y + z = 1, x + 2y + 4z = η, x + 4y + 10z = η2.  x1 + αx2 = 1,  x1 − x2 + 3x3 = −1, 2x1 − 2x2 + αx3 = −2  6 0  2 −2  9 5 16  A =  1  3 2 −3  1 5     .  A =  3  1  4 −3 −3 3 −1 −1  3 −6 −3  1   MATRICES AND VECTOR SPACES  8.40  Find the equation satisﬁed by the squares of the singular values of the matrix associated with the following over-determined set of equations:  Show that one of the singular values is close to zero. Determine the two larger singular values by an appropriate iteration process and the smallest one by indirect calculation. Find the SVD of  8.41  8.42  showing that the singular values are Find the SVD form of the matrix  Use it to determine the best solution x of the equation Ax = b when  i  b =  6 − 39 15 18 T,  ii  b =  9 − 42 15 15 T, showing that  i  has an exact  √  solution, but that the best solution to  ii  has a residual of Four experimental measurements of particular combinations of three physical variables, x, y and z, gave the following inconsistent results:  18.  8.43  2x + 3y + z = 0  x − y − z = 1 2y + z = −2.  2x + y = 0  A =  1 0  −1 1 √ 3 and 1.   0 −1  ,  22  28 −22 1 −2 −19 19 −2 −1 −6  12  6   .  A =  13x + 22y − 13z = 4, 10x − 8y − 10z = 44, 10x − 8y − 10z = 47, 9x − 18y − 9z = 72.  Find the SVD best values for x, y and z. Identify the null space of A and hence obtain the general SVD solution.   cid:8   a  False. ON, the N × N null matrix, is not non-singular.  8.20 Hints and answers   cid:8    cid:7    cid:7   8.1   b  False. Consider the sum of  1 0  0 0  0 0  0 1  .   c  True.  d  True.  e  False. Consider bn = an + an for which  is no zero vector with unit norm.   f  True.  g  False. Consider the two series deﬁned by  and   cid:11   N n=0  bn2 = 4  cid:3 = 1, or note that there  an = 2 − 1 2  n  n ≥ 1;  for  bn = − − 1 2  n  for n ≥ 0.  The series that is the sum of {an} and {bn} does not have alternating signs  a0 = 1 2 ,  8.3  and so closure does not hold.   a  x = a, b or c;  b  x = −1; the equation is linear in x.  314   8.20 HINTS AND ANSWERS   cid:7    cid:8   0  − tan θ 2   Use the property of the determinant of a matrix product.  d  S =  e  Note that  I + K  I − K  = I − K2 =  I − K  I + K .  b  32iA. a = b cos γ + c cos β, and cyclic permutations; a2 = b2 + c2 − 2bc cos α, and cyclic  tan θ 2   0  .  permutations.  a  2  −1 2 0 0 1 1 T, 6 −1 2 −1 6 − 1 1 T, 13 39 −1 2 1 2 0 0 T,  345   18 285   −1 2 −56 28 98 69 T.  −1 2 2 0 − 1 1 T,  −1 2 2 1 2 − 2 T. −1 2 14 − 7 10 0 T,   b  5  common eigenvectors.  C does not commute with the others; A, B and D have  1 − 2 T and  2 1 T as For A :  1 0 − 1 T,  1 α1 For B :  1 1 1 T,  β1 Simultaneous and orthogonal:  1 0 − 1 T,  1 1 1 T,  1 − 2 1 T. The αi, βi and γi are arbitrary. αj =  v · ej∗  a  x =  2 1 3 T.  b  Since µ is equal to one of A’s eigenvalues λj, the equation only has a solution     λj − µ , where λj is the eigenvalue corresponding to ej .  γ1 − β1 − γ1 T,  β2  γ2 − β2 − γ2 T.  1 T,  1 α2  1 T.  = 0;  i  no solution;  ii  x =  1 1  3 2 T.  if v · ej∗  √  U =  10   2 and corresponding 2. From the trace property of A, the third eigenvalue equals 2.  −1 2 1, 3i; 3i, 1 , Λ =  1, 0; 0, 11 .  J =  2y2− 4y + 4   y2 + 2 , with stationary values at y = ±√ eigenvalues 2 ∓ √ Ellipse; θ = π 4, a = √ The direction of  1, 1,−1   3.  a  A = SA cid:7 S†  b  A =  λ + 2µ + 3ν, 2λ − 2µ, λ + 2µ − 3ν; 2λ − 2µ, 4λ + 2µ, 2λ − 2µ;  matrix A to be constructed, and A cid:7   , where S is the matrix whose columns are the eigenvectors of the  eigenvector having the unrepeated eigenvalue  22; θ = 3π 4, b =  = diag  λ, µ, ν .  √  the  10.  is  λ + 2µ − 3ν, 2λ − 2µ, λ + 2µ + 3ν .  1  3  1, 5,−2; 5, 4, 5;−2, 5, 1 .   c  The null space is spanned by  2 0 x = 3, y = 1, z = 2.  1 0 T and  1 − 2 0 1 T.  √  3 , 1, 0; 2  3 , 3, 1 , U =  3, 6, 9; 0,−2, 2; 0, 0, 4 . √−6.  cid:24   cid:8   First show that A is singular. η = 1, x = 1 + 2z, y = −3z; η = 2, x = 2z, y = 1 − 3z. L =  1, 0, 0; 1  i  x =  −1 1 2 T.  ii  x =  −3 2 2 T.  cid:24  A is not positive deﬁnite, as L33 is calculated to be B = LLT, where the non-zero elements of L are  −1 L11 = √ −1 −√ 2 √ 3 and the calculated best solution is x =  1.71, y = −1.94, z = −1.71. The null space is the line x = z, y = 0 and the general SVD solution is x = 1.71 + λ, y = −1.94, z = −1.71 + λ.   , V =  1√ 6 √ 6, 0, 18  12 5. √ √ 2 √ 2 2  √ 3, L33 =  The singular values are 12  1 −1  A†A =  3 5, L22 =  1√ 2  5, L31 =  , U =   cid:7    cid:7    cid:8   3 0  1 2  2 1  3  1  1  .  8.5  8.7  8.9 8.11  8.13  8.15  8.17  8.19  8.21 8.23  8.25 8.27  8.29  8.31 8.33 8.35  8.37  8.39  8.41  8.43  315   9  Normal modes  Any student of the physical sciences will encounter the subject of oscillations on many occasions and in a wide variety of circumstances, for example the voltage and current oscillations in an electric circuit, the vibrations of a mechanical structure and the internal motions of molecules. The matrices studied in the previous chapter provide a particularly simple way to approach what may appear, at ﬁrst glance, to be diﬃcult physical problems.  We will consider only systems for which a position-dependent potential exists, i.e., the potential energy of the system in any particular conﬁguration depends upon the coordinates of the conﬁguration, which need not be be lengths, however; the potential must not depend upon the time derivatives  generalised velocities  of  these coordinates. So, for example, the potential −qv · A used in the Lagrangian  description of a charged particle in an electromagnetic ﬁeld is excluded. A further restriction that we place is that the potential has a local minimum at the equilibrium point; physically, this is a necessary and suﬃcient condition for stable equilibrium. By suitably deﬁning the origin of the potential, we may take its value at the equilibrium point as zero.  We denote the coordinates chosen to describe a conﬁguration of the system by qi, i = 1, 2, . . . , N. The qi need not be distances; some could be angles, for example. For convenience we can deﬁne the qi so that they are all zero at the equilibrium point. The instantaneous velocities of various parts of the system will depend upon the time derivatives of the qi, denoted by ˙qi. For small oscillations the velocities will be linear in the ˙qi and consequently the total kinetic energy T will be quadratic in them – and will include cross terms of the form ˙qi˙qj with i  cid:3 = j. The general expression for T can be written as the quadratic form  T =  aij ˙qi ˙qj = ˙qTA˙q,   9.1   ˙qN T and the N × N matrix A where ˙q is the column vector  ˙q1 ˙q2 is real and may be chosen to be symmetric. Furthermore, A, like any matrix  ···   cid:4    cid:4   i  j  316   9.1 TYPICAL OSCILLATORY SYSTEMS  corresponding to a kinetic energy, is positive deﬁnite; that is, whatever non-zero real values the ˙qi take, the quadratic form  9.1  has a value > 0.  Turning now to the potential energy, we may write its value for a conﬁguration  q by means of a Taylor expansion about the origin q = 0, ∂2V  0  ∂qi∂qj  V  q  = V  0  +  ∂V  0  ∂qi  qi +  1 2  qiqj + ··· .   cid:4    cid:4   i  j   cid:4   i  However, we have chosen V  0  = 0 and, since the origin is an equilibrium point, there is no force there and ∂V  0  ∂qi = 0. Consequently, to second order in the qi we also have a quadratic form, but in the coordinates rather than in their time derivatives:  V =  bijqiqj = qTBq,   9.2    cid:4    cid:4   i  j  where B is, or can be made, symmetric. In this case, and in general, the requirement that the potential is a minimum means that the potential matrix B, like the kinetic energy matrix A, is real and positive deﬁnite.  9.1 Typical oscillatory systems  We now introduce particular examples, although the results of this section are general, given the above restrictions, and the reader will ﬁnd it easy to apply the results to many other instances.  Consider ﬁrst a uniform rod of mass M and length l, attached by a light string also of length l to a ﬁxed point P and executing small oscillations in a vertical plane. We choose as coordinates the angles θ1 and θ2 shown, with exaggerated magnitude, in ﬁgure 9.1. In terms of these coordinates the centre of gravity of the rod has, to ﬁrst order in the θi, a velocity component in the x-direction equal to 2 l˙θ2 and in the y-direction equal to zero. Adding in the rotational kinetic l˙θ1 + 1 energy of the rod about its centre of gravity we obtain, to second order in the ˙θi,   cid:8   6 3 3 2  ˙q,  T ≈ 1  = 1  2   cid:5    cid:6   3˙θ2  1 + 1 4  6 Ml2  ˙θ2  + 1  1 + 3˙θ1  2 + ˙θ1 ˙θ2  2 Ml2 ˙θ2  24 Ml2˙θ2   cid:7   cid:19  ˙θ2 . The potential energy is given by 2  1 − cos θ2   cid:7    cid:18   1 − cos θ1  + 1  12 Ml2˙qT  ˙θ2 + ˙θ2 2  V = Mlg  = 1  V ≈ 1  4 Mlg 2θ2  1 + θ2  2  = 1  12 MlgqT   cid:8   6 0 0 3  q,  where ˙qT =  ˙θ1  so that   9.3    9.4    9.5   where g is the acceleration due to gravity and q =  θ1 θ2 T;  9.5  is valid to second order in the θi.  317   NORMAL MODES  P  θ1  P  θ1  θ2  θ2  P  l  θ1  θ2  l   a    b    c   Figure 9.1 A uniform rod of length l attached to the ﬁxed point P by a light string of the same length:  a  the general coordinate system;  b  approximation to the normal mode with lower frequency;  c  approximation to the mode with higher frequency.  With these expressions for T and V we now apply the conservation of energy,  d dt   T + V   = 0,  assuming that there are no external forces other than gravity. In matrix form  9.6  becomes  d dt   ˙qTA˙q + qTBq  = ¨qTA˙q + ˙qTA¨q + ˙qTBq + qTB˙q = 0,  which, using A = AT and B = BT, gives  2˙qT A¨q + Bq  = 0.  We will assume, although it is not clear that this gives the only possible solution, that the above equation implies that the coeﬃcient of each ˙qi is separately zero. Hence  For a rigorous derivation Lagrange’s equations should be used, as in chapter 22. Now we search for sets of coordinates q that all oscillate with the same period, i.e. the total motion repeats itself exactly after a ﬁnite interval. Solutions of this form will satisfy  the relative values of the elements of x in such a solution will indicate how each  A¨q + Bq = 0.  q = x cos ωt;  318   9.6    9.7    9.8    9.1 TYPICAL OSCILLATORY SYSTEMS  coordinate is involved in this special motion. In general there will be N values  of ω if the matrices A and B are N × N and these values are known as normal  frequencies or eigenfrequencies. Putting  9.8  into  9.7  yields  Our work in section 8.18 showed that this can have non-trivial solutions only if  −ω2Ax + Bx =  B − ω2A x = 0.  B − ω2A = 0.   9.9    9.10   This is a form of characteristic equation for B, except that the unit matrix I has been replaced by A. It has the more familiar form if a choice of coordinates is made in which the kinetic energy T is a simple sum of squared terms, i.e. it has been diagonalised, and the scale of the new coordinates is then chosen to make each diagonal element unity.  However, even in the present case,  9.10  can be solved to yield ω2  k for k = 1, 2, . . . , N, where N is the order of A and B. The values of ωk can be used with  9.9  to ﬁnd the corresponding column vector xk and the initial  stationary  physical conﬁguration that, on release, will execute motion with period 2π ωk.  In equation  8.76  we showed that the eigenvectors of a real symmetric matrix were, except in the case of degeneracy of the eigenvalues, mutually orthogonal. In the present situation an analogous, but not identical, result holds. It is shown in section 9.3 that if x1 and x2 are two eigenvectors satisfying  9.9  for diﬀerent values of ω2 then they are orthogonal in the sense that   x2 TAx1 = 0  and   x2 TBx1 = 0.  The direct ‘scalar product’  x2 Tx1, formally equal to  x2 TI x1, is not, in general, equal to zero.  Returning to the suspended rod, we ﬁnd from  9.10    cid:7   − ω2Ml2 12   cid:8  cid:20  cid:20  cid:20  cid:20  = 0.  cid:8   cid:20  cid:20  cid:20  cid:20  = 0 ⇒ λ2 − 10λ + 6 = 0,  6 3 3 2  12  6 0 0 3   cid:20  cid:20  cid:20  cid:20  Mlg  cid:7   cid:20  cid:20  cid:20  cid:20  6 − 6λ −3λ which has roots λ = 5 ± √ values for ω2, namely  5 − √  Writing ω2l g = λ, this becomes 3 − 2λ  −3λ  19. Thus we ﬁnd that the two normal frequencies are given by ω1 =  0.641g l 1 2 and ω2 =  9.359g l 1 2. Putting the lower of the two  x1 : x2 = 3 5 − √  √ 19  : 6   19 − 4  = 1.923 : 2.153.  19 g l, into  9.9  shows that for this mode  This corresponds to the case where the rod and string are almost straight out, i.e. they almost form a simple pendulum. Similarly it may be shown that the higher  319   NORMAL MODES  frequency corresponds to a solution where the string and rod are moving with  opposite phase and x1 : x2 = 9.359 : −16.718. The two situations are shown in  ﬁgure 9.1.  In connection with quadratic forms it was shown in section 8.17 how to make a change of coordinates such that the matrix for a particular form becomes diagonal. In exercise 9.6 a method is developed for diagonalising simultaneously two quadratic forms  though the transformation matrix may not be orthogonal . If this process is carried out for A and B in a general system undergoing stable oscillations, the kinetic and potential energies in the new variables ηi take the forms  µi˙η2  i = ˙ηTM˙η, M = diag  µ1, µ2, . . . , µN ,  νiη2  i = ηTNη,  N = diag  ν1, ν2 . . . , νN ,   9.11    9.12    cid:4   cid:4   i  i  T =  V =  and the equations of motion are the uncoupled equations  µi¨ηi + νiηi = 0,  i = 1, 2, . . . , N.   9.13   Clearly a simple renormalisation of the ηi can be made that reduces all the µi in  9.11  to unity. When this is done the variables so formed are called normal coordinates and equations  9.13  the normal equations.  When a system is executing one of these simple harmonic motions it is said to be in a normal mode, and once started in such a mode it will repeat its motion exactly after each interval of 2π ωi. Any arbitrary motion of the system may be written as a superposition of the normal modes, and each component mode will execute harmonic motion with the corresponding eigenfrequency; however, unless by chance the eigenfrequencies are in integer relationship, the system will never return to its initial conﬁguration after any ﬁnite time interval.  As a second example we will consider a number of masses coupled together by springs. For this type of situation the potential and kinetic energies are automat- ically quadratic functions of the coordinates and their derivatives, provided the elastic limits of the springs are not exceeded, and the oscillations do not have to be vanishingly small for the analysis to be valid.   cid:1 Find the normal frequencies and modes of oscillation of three particles of masses m, µ m, m connected in that order in a straight line by two equal light springs of force constant k. This arrangement could serve as a model for some linear molecules, e.g. CO2.  The situation is shown in ﬁgure 9.2; the coordinates of the particles, x1, x2, x3, are measured from their equilibrium positions, at which the springs are neither extended nor compressed.  The kinetic energy of the system is simply   cid:6    cid:5   320  T = 1  2 m  ˙x2 1 + µ ˙x2  2 + ˙x2  3  ,   9.1 TYPICAL OSCILLATORY SYSTEMS  k  µm  k  m  x2  x3  Figure 9.2 Three masses m, µm and m connected by two equal light springs of force constant k.  m  x1   a    b    c   Figure 9.3 The normal modes of the masses and springs of a linear molecule such as CO2.  a  ω2 = 0;  b  ω2 = k m;  c  ω2 = [ µ + 2  µ] k m .   cid:18   ,  0 µ 0  0 0 1   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  1 − λ  −1  0  x2 =  1√ 2  .  −1 0 −1   cid:19   1 −1  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  = 0, 1 cid:24   x3 =  whilst the potential energy stored in the springs is  V = 1  2 k   x2 − x1 2 +  x3 − x2 2  The kinetic- and potential-energy symmetric matrices are thus   1  0 0  A =  m 2   .  B =  k 2  0  2 −1  1  From  9.10 , to ﬁnd the normal frequencies we have to solve B − ω2A = 0. Thus, writing mω2 k = λ, we have  which leads to λ = 0, 1 or 1 + 2 µ. The corresponding eigenvectors are respectively   ,   1  1 1  x1 =  1√ 3   1−2 µ  .  2 +  4 µ2   1  The physical motions associated with these normal modes are illustrated in ﬁgure 9.3. The ﬁrst, with λ = ω = 0 and all the xi equal, merely describes bodily translation of the whole system, with no  i.e. zero-frequency  internal oscillations.  In the second solution the central particle remains stationary, x2 = 0, whilst the other two oscillate with equal amplitudes in antiphase with each other. This motion, which has frequency ω =  k m 1 2, is illustrated in ﬁgure 9.3 b .  0  −1 1 − λ  −1 2 − µ λ −1   1  0−1   ,  321   NORMAL MODES  The ﬁnal and most complicated of the three normal modes has angular frequency  ω = {[ µ + 2  µ] k m }1 2, and involves a motion of the central particle which is in  antiphase with that of the two outer ones and which has an amplitude 2 µ times as great. In this motion  see ﬁgure 9.3 c   the two springs are compressed and extended in turn. We also note that in the second and third normal modes the centre of mass of the molecule remains stationary.  cid:2   9.2 Symmetry and normal modes  It will have been noticed that the system in the above example has an obvious symmetry under the interchange of coordinates 1 and 3: the matrices A and B, the equations of motion and the normal modes illustrated in ﬁgure 9.3 are all unaltered by the interchange of x1 and −x3. This reﬂects the more general result  that for each physical symmetry possessed by a system, there is at least one normal mode with the same symmetry.  The general question of the relationship between the symmetries possessed by a physical system and those of its normal modes will be taken up more formally in chapter 29 where the representation theory of groups is considered. However, we can show here how an appreciation of a system’s symmetry properties will sometimes allow its normal modes to be guessed  and then veriﬁed , something that is particularly helpful if the number of coordinates involved is greater than two and the corresponding eigenvalue equation  9.10  is a cubic or higher-degree polynomial equation.  Consider the problem of determining the normal modes of a system consist- ing of four equal masses M at the corners of a square of side 2L, each pair of masses being connected by a light spring of modulus k that is unstretched in the equilibrium situation. As shown in ﬁgure 9.4, we introduce Cartesian coordinates xn, yn, with n = 1, 2, 3, 4, for the positions of the masses and de- note their displacements from their equilibrium positions Rn by qn = xni + ynj. Thus  rn = Rn + qn with Rn = ±Li ± Lj.  The coordinates for the system are thus x1, y1, x2, . . . , y4 and the kinetic en-  ergy matrix A is given trivially by MI8, where I8 is the 8 × 8 identity ma-  trix.  The potential energy matrix B is much more diﬃcult to calculate and involves, for each pair of values m, n, evaluating the quadratic approximation to the expression   cid:5 rm − rn − Rm − Rn cid:6   2  .  bmn = 1 2 k  Expressing each ri in terms of qi and Ri and making the normal assumption that  322   9.2 SYMMETRY AND NORMAL MODES  x1  x2  k  k  y1  M  k  y3  k  k  y2  M  k  y4  M  x3  x4  M  Figure 9.4 The arrangement of four equal masses and six equal springs discussed in the text. The coordinate systems xn, yn for n = 1, 2, 3, 4 measure the displacements of the masses from their equilibrium positions.  Rm − Rn  cid:26  qm − qn, we obtain bmn  = bnm : bmn = 1 2 k   cid:18  Rm − Rn  +  qm − qn  − Rm − Rn cid:19  ! cid:18 Rm − Rn2 + 2 qm − qn  ·  RM − Rn  + qm − qn 2  cid:14  2 kRm − Rn2  cid:12  ≈ 1 2 k  2 qm − qn  ·  RM − Rn    qm − qn  ·  RM − Rn    cid:15  Rm − Rn2   cid:13   + ···  = 1 2 k  = 1  1 +  2  2  .  Rm − Rn  "  cid:19 1 2 − Rm − Rn . 1 2 − 1  2  2  This ﬁnal expression is readily interpretable as the potential energy stored in the spring when it is extended by an amount equal to the component, along the equilibrium direction of the spring, of the relative displacement of its two ends.  Applying this result to each spring in turn gives the following expressions for  the elements of the potential matrix.  m n 1 2 3 1 4 1 3 2 4 2 3 4  2bmn k   x1 − x2 2  y1 − y3 2  1  2  −x1 + x4 + y1 − y4 2 2  x2 − x3 + y2 − y3 2  1   y2 − y4 2  x3 − x4 2.  323   The potential matrix is thus constructed as  NORMAL MODES    B =  k 4  3 −1 −2 −1 −2  3 0 0  0 3 1  0  0 0  0 −2 1 −1 −1 3 −1 −1  0 0  0 −1 −1 0 −2 −1 −1 −1 1 −1 0 −2  1  0  0 −2  3 1  0  0 −1  1  1 −1 0 −2  0  0  0  3 −1  0 0  3  1 −2  3 0  0 −1   .  To solve the eigenvalue equation B − λA = 0 directly would mean solving  an eigth-degree polynomial equation. Fortunately, we can exploit intuition and the symmetries of the system to obtain the eigenvectors and corresponding eigenvalues without such labour.  Firstly, we know that bodily translation of the whole system, without any internal vibration, must be possible and that there will be two independent solutions of this form, corresponding to translations in the x- and y- directions. The eigenvector for the ﬁrst of these  written in row form to save space  is  x 1  =  1 0 1 0 1 0  1 0 T.  Evaluation of Bx 1  gives  Bx 1  =  0  0 0 0 0 0 0 0 T,  showing that x 1  is a solution of  B− ω2A x = 0 corresponding to the eigenvalue ω2 = 0, whatever form Ax may take. Similarly,  x 2  =  0 1 0 1  0 1 0 1 T  is a second eigenvector corresponding to the eigenvalue ω2 = 0.  The next intuitive solution, again involving no internal vibrations, and, there- fore, expected to correspond to ω2 = 0, is pure rotation of the whole system about its centre. In this mode each mass moves perpendicularly to the line joining its position to the centre, and so the relevant eigenvector is  x 3  =  1√ 2   1 1 1 − 1 − 1 1 − 1 − 1 T.  It is easily veriﬁed that Bx 3  = 0 thus conﬁrming both the eigenvector and the corresponding eigenvalue. The three non-oscillatory normal modes are illustrated in diagrams  a – c  of ﬁgure 9.5.  We now come to solutions that do involve real  internal oscillations, and, because of the four-fold symmetry of the system, we expect one of them to be a mode in which all the masses move along radial lines – the so-called ‘breathing  324   9.2 SYMMETRY AND NORMAL MODES   a  ω2 = 0   b  ω2 = 0   c  ω2 = 0   d  ω2 = 2k M   e  ω2 = k M   f  ω2 = k M   g  ω2 = k M   h  ω2 = k M  Figure 9.5 The displacements and frequencies of the eight normal modes of the system shown in ﬁgure 9.4. Modes  a ,  b  and  c  are not true oscillations:  a  and  b  are purely translational whilst  c  is a mode of bodily rotation. Mode  d , the ‘breathing mode’, has the highest frequency and the remaining four,  e – h , of lower frequency, are degenerate.  mode’. Expressing this motion in coordinate form gives as the fourth eigenvector  x 4  =  1√ 2   −1 1 1 1 − 1 − 1 1 − 1 T.  Evaluation of Bx 4  yields  −8  Bx 4  =  k  √ 4  2  8 8 8 − 8 − 8 8 − 8 T = 2kx 4 ,  i.e. a multiple of x 4 , conﬁrming that it is indeed an eigenvector. Further, since Ax 4  = Mx 4 , it follows from  B − ω2A x = 0 that ω2 = 2k M for this normal  mode. Diagram  d  of the ﬁgure illustrates the corresponding motions of the four masses.  As the next step in exploiting the symmetry properties of the system we note that, because of its reﬂection symmetry in the x-axis, the system is invariant under  the double interchange of y1 with −y3 and y2 with −y4. This leads us to try an  eigenvector of the form  x 5  =  0  α 0 β 0 − α 0 − β T.  Substituting this trial vector into  B − ω2A x = 0 gives, of course, eight simulta-  325   NORMAL MODES  α + β = 0,  5α + β =  4Mω2  α;  k  neous equations for α and β, but they are all equivalent to just two, namely  these have the solution α = −β and ω2 = k M. The latter thus gives the frequency  of the mode with eigenvector  x 5  =  0  1 0 − 1 0 − 1 0 1 T.  Note that, in this mode, when the spring joining masses 1 and 3 is most stretched, the one joining masses 2 and 4 is at its most compressed. Similarly, based on reﬂection symmetry in the y-axis,  x 6  =  1  0 − 1 0 − 1 0 1 0 T  can be shown to be an eigenvector corresponding to the same frequency. These two modes are shown in diagrams  e  and  f  of ﬁgure 9.5.  This accounts for six of the expected eight modes, and the other two could be found by considering motions that are symmetric about both diagonals of the square or are invariant under successive reﬂections in the x- and y- axes. However, since A is a multiple of the unit matrix, and since we know that  x j  TAx i  = 0 if i  cid:3 = j, we can ﬁnd the two remaining eigenvectors more easily by requiring them  to be orthogonal to each of those found so far.  Let us take the next  seventh  eigenvector, x 7 , to be given by  x 7  =  a b  c d e f  g  h T.  Then orthogonality with each of the x n  for n = 1, 2, . . . , 6 yields six equations satisﬁed by the unknowns a, b, . . . , h. As the reader may verify, they can be reduced to the six simple equations  a + g = 0, d + f = 0, a + f = d + g, b + h = 0, b + c = e + h.  c + e = 0,  With six homogeneous equations for eight unknowns, eﬀectively separated into two groups of four, we may pick one in each group arbitrarily. Taking a = b = 1  gives d = e = 1 and c = f = g = h = −1 as a solution. Substitution of  x 7  =  1 1 − 1 1 1 − 1 − 1 − 1 T.  into the eigenvalue equation checks that it is an eigenvector and shows that the corresponding eigenfrequency is given by ω2 = k M.  We now have the eigenvectors for seven of the eight normal modes and the eighth can be found by making it simultaneously orthogonal to each of the other seven. It is left to the reader to show  or verify  that the ﬁnal solution is  x 8  =  1 − 1 1 1 − 1 − 1 − 1 1 T  326   9.3 RAYLEIGH–RITZ METHOD  and that this mode has the same frequency as three of the other modes. The general topic of the degeneracy of normal modes is discussed in chapter 29. The movements associated with the ﬁnal two modes are shown in diagrams  g  and  h  of ﬁgure 9.5; this ﬁgure summarises all eight normal modes and frequencies. Although this example has been lengthy to write out, we have seen that the actual calculations are quite simple and provide the full solution to what is  formally a matrix eigenvalue equation involving 8 × 8 matrices. It should be  noted that our exploitation of the intrinsic symmetries of the system played a crucial part in ﬁnding the correct eigenvectors for the various normal modes.  9.3 Rayleigh–Ritz method  We conclude this chapter with a discussion of the Rayleigh–Ritz method for estimating the eigenfrequencies of an oscillating system. We recall from the introduction to the chapter that for a system undergoing small oscillations the potential and kinetic energy are given by  V = qTBq  and  T = ˙qTA˙q,  where the components of q are the coordinates chosen to represent the conﬁgura- tion of the system and A and B are symmetric matrices  or may be chosen to be such . We also recall from  9.9  that the normal modes xi and the eigenfrequencies ωi are given by   B − ω2  i  A xi = 0.   9.14    cid:11   It may be shown that the eigenvectors xi corresponding to diﬀerent normal modes are linearly independent and so form a complete set. Thus, any coordinate vector j cjxj. We now consider the value of the generalised q can be written q = quadratic form   cid:11   cid:11    cid:11   cid:11   B  ∗  m A  λ x  =  xTBx xTAx =  m xm Tc ∗ j xj Tc  j  i cixi k ckxk ,  which, since both numerator and denominator are positive deﬁnite, is itself non- negative. Equation  9.14  can be used to replace Bxi, with the result that   cid:11   cid:11   cid:11   cid:11    cid:11   cid:11   cid:11   cid:11   m  ∗ A m xm Tc ∗ A j xj Tc ∗ m xm Tc ∗ j xj Tc  A  m  j  j  i cixi i ω2 k ckxk i ciAxi k ckxk  i ω2  .  λ x  =  =  327  Now the eigenvectors xi obtained by solving  B − ω2A x = 0 are not mutually orthogonal unless either A or B is a multiple of the unit matrix. However, it may   9.15    NORMAL MODES  be shown that they do possess the desirable properties   xj TAxi = 0  and   xj TBxi = 0  if i  cid:3 = j.   9.16    9.17   This result is proved as follows. From  9.14  it is clear that, for general i and j,   xj T B − ω2  i  A xi = 0.  But, by taking the transpose of  9.14  with i replaced by j and recalling that A and B are real and symmetric, we obtain  xj T B − ω2  A  = 0.  j  Forming the scalar product of this with xi and subtracting the result from  9.17  gives  − ω2   ω2 j  i   xj TAxi = 0.   cid:3 = j and non-degenerate eigenvalues ω2  j , we have that Thus, for i  xj TAxi = 0, and substituting this into  9.17  immediately establishes the corre- sponding result for  xj TBxi. Clearly, if either A or B is a multiple of the unit matrix then the eigenvectors are mutually orthogonal in the normal sense. The orthogonality relations  9.16  are derived again, and extended, in exercise 9.6.  i and ω2  Using the ﬁrst of the relationships  9.16  to simplify  9.15 , we ﬁnd that   cid:11   cid:11   ci2ω2 i  xi TAxi ck2 xk TAxk .  cid:11  ≥ ω2 Now, if ω2  xi TAxi ≥ 0 for all i the numerator of  9.18  is ≥ ω2  0 is the lowest eigenfrequency then ω2  λ x  =  k  i  i  0 for all i and, further, since  ci2 xi TAxi. Hence  0  i   9.18    9.19   λ x  ≡ xTBx  xTAx  ≥ ω2  0,  for any x whatsoever  whether x is an eigenvector or not . Thus we are able to estimate the lowest eigenfrequency of the system by evaluating λ for a variety of vectors x, the components of which, it will be recalled, give the ratios of the coordinate amplitudes. This is sometimes a useful approach if many coordinates are involved and direct solution for the eigenvalues is not possible. An additional result is that the maximum eigenfrequency ω2 ≥ ω2 ≤ ω2  m may also be estimated. It is obvious that if we replace the statement ‘ω2 0 for all i’ by i m for any x. Thus λ x  always lies between ‘ω2 i the lowest and highest eigenfrequencies of the system. Furthermore, λ x  has a k , when x is the kth eigenvector  see subsection 8.17.1 . stationary value, equal to ω2  m for all i’, then λ x  ≤ ω2  328   9.4 EXERCISES   cid:1 Estimate the eigenfrequencies of the oscillating rod of section 9.1.  Firstly we recall that   cid:7    cid:8   A =  Ml2 12  6 3  3 2  and  B =  Mlg 12  6 0  0 3   cid:7    cid:8   .  Physical intuition suggests that the slower mode will have a conﬁguration approximating that of a simple pendulum  ﬁgure 9.1 , in which θ1 = θ2, and so we use this as a trial vector. Taking x =  θ θ T,  λ x  =  xTBx xTAx  =  3Mlgθ2 4 7Ml2θ2 6  =  9g 14l  = 0.643  g l  ,  and we conclude from  9.19  that the lower  angular  frequency is ≤  0.643g l 1 2. We have already seen on p. 319 that the true answer is  0.641g l 1 2 and so we have come very close to it.  Next we turn to the higher frequency. Here, a typical pattern of oscillation is not so  obvious but, rather preempting the answer, we try θ2 = −2θ1; we then obtain λ = 9g l and so conclude that the higher eigenfrequency ≥  9g l 1 2. We have already seen that the exact answer is  9.359g l 1 2 and so again we have come close to it.  cid:2   A simpliﬁed version of the Rayleigh–Ritz method may be used to estimate the eigenvalues of a symmetric  or in general Hermitian  matrix B, the eigenvectors of which will be mutually orthogonal. By repeating the calculations leading to  9.18 , A being replaced by the unit matrix I, it is easily veriﬁed that if  is evaluated for any vector x then  where λ1, λ2. . . , λm are the eigenvalues of B in order of increasing size. A similar result holds for Hermitian matrices.  λ x  =  xTBx xTx  λ1 ≤ λ x  ≤ λm,  9.4 Exercises  9.1  Three coupled pendulums swing perpendicularly to the horizontal line containing their points of suspension, and the following equations of motion are satisﬁed:  −m¨x1 = cmx1 + d x1 − x2 , −M¨x2 = cMx2 + d x2 − x1  + d x2 − x3 , −m¨x3 = cmx3 + d x3 − x2 ,  where x1, x2 and x3 are measured from the equilibrium points; m, M and m are the masses of the pendulum bobs; and c and d are positive constants. Find the normal frequencies of the system and sketch the corresponding patterns of  oscillation. What happens as d → 0 or d → ∞?  A double pendulum, smoothly pivoted at A, consists of two light rigid rods, AB and BC, each of length l, which are smoothly jointed at B and carry masses m and αm at B and C respectively. The pendulum makes small oscillations in one plane  9.2  329   NORMAL MODES  under gravity. At time t, AB and BC make angles θ t  and φ t , respectively, with the downward vertical. Find quadratic expressions for the kinetic and potential energies of the system and hence show that the normal modes have angular frequencies given by   cid:22   1 + α ± cid:24    cid:23   α 1 + α   .  ω2 =  g l  9.3  For α = 1 3, show that in one of the normal modes the mid-point of BC does not move during the motion. Continue the worked example, modelling a linear molecule, discussed at the end of section 9.1, for the case in which µ = 2.   a  Show that the eigenvectors derived there have the expected orthogonality   b  For the situation in which the atoms are released from rest with initial  properties with respect to both A and B. displacements x1 = 2 cid:4 , x2 = − cid:4  and x3 = 0, determine their subsequent  motions and maximum displacements.  9.4  Consider the circuit consisting of three equal capacitors and two diﬀerent in- ductors shown in the ﬁgure. For charges Qi on the capacitors and currents Ii  Q1  C  Q2  C  Q3  L1  C  L2  I1  I2  through the components, write down Kirchhoﬀ’s law for the total voltage change around each of two complete circuit loops. Note that, to within an unimportant  constant, the conservation of current implies that Q3 = Q1 − Q2. Express the loop  equations in the form given in  9.7 , namely  Use this to show that the normal frequencies of the circuit are given by  A ¨Q + BQ = 0.   cid:18    cid:19   ω2 =  1  CL1L2  L1 + L2 ±  L2  1 + L2  2  − L1L2 1 2  .  9.5  Obtain the same matrices and result by ﬁnding the total energy stored in the various capacitors  typically Q2  2C   and in the inductors  typically LI 2 2 .  For the special case L1 = L2 = L determine the relevant eigenvectors and so  describe the patterns of current ﬂow in the circuit. It is shown in physics and engineering textbooks that circuits containing capaci- tors and inductors can be analysed by replacing a capacitor of capacitance C by a ‘complex impedance’ 1  iωC  and an inductor of inductance L by an impedance  iωL, where ω is the angular frequency of the currents ﬂowing and i2 = −1.  Use this approach and Kirchhoﬀ’s circuit laws to analyse the circuit shown in  330   the ﬁgure and obtain three linear equations governing the currents I1, I2 and I3. Show that the only possible frequencies of self-sustaining currents satisfy either  P  Q  9.4 EXERCISES  C  U  T  I1  L  C  L  C  S  I2  I3  R  9.6   a  ω2LC = 1 or  b  3ω2LC = 1. Find the corresponding current patterns and, in each case, by identifying parts of the circuit in which no current ﬂows, draw an equivalent circuit that contains only one capacitor and one inductor. The simultaneous reduction to diagonal form of two real symmetric quadratic forms. Consider the two real symmetric quadratic forms uTAu and uTBu, where uT stands for the row matrix  x y z , and denote by un those column matrices that satisfy  Bun = λnAun,   E9.1   in which n is a label and the λn are real, non-zero and all diﬀerent.  a  By multiplying  E9.1  on the left by  um T, and the transpose of the corre- sponding equation for um on the right by un, show that  um TAun = 0 for n  cid:3 = m. It can be shown that the un are linearly independent; the next step is to construct a matrix P whose columns are the vectors un.  −1Bun, deduce that  um TBun = 0 for m  cid:3 = n.   b  By noting that Aun =  λn   c    d  Make a change of variables u = Pv such that uTAu becomes vTCv, and uTBu becomes vTDv. Show that C and D are diagonal by showing that cij = 0 if i  cid:3 = j, and similarly for dij.  To summarise, the method is as follows:  Thus u = Pv or v = P−1u reduces both quadratics to diagonal form.  a  ﬁnd the λn that allow  E9.1  a non-zero solution, by solving B − λA = 0;  b  for each λn construct un;  c  construct the non-singular matrix P whose columns are the vectors un;  d  make the change of variable u = Pv.  9.7   It is recommended that the reader does not attempt this question until exercise 9.6 has been studied.   If, in the pendulum system studied in section 9.1, the string is replaced by a second rod identical to the ﬁrst then the expressions for the kinetic energy T and the potential energy V become  to second order in the θi    cid:6   T ≈ Ml2 V ≈ Mgl  ˙θ2 + 2  3  ˙θ2 2  ,  8 3 3  1 + 2˙θ1 ˙θ2 2 θ2 2 θ2 1 + 1  2   cid:6   .  Determine the normal frequencies of the system and ﬁnd new variables ξ and η that will reduce these two expressions to diagonal form, i.e. to  ˙ξ2 + a2˙η2  a1  and  b1ξ2 + b2η2.   cid:5   cid:5   331   NORMAL MODES  9.8  9.9  9.10  9.1 9.3  9.5  9.7 9.9   .   2 −1  −1 0 −1  0  2 −1  cid:9   2   It is recommended that the reader does not attempt this question until exercise 9.6 has been studied.   Find a real linear transformation that simultaneously reduces the quadratic  forms  3x2 + 5y2 + 5z2 + 2yz + 6zx − 2xy,  5x2 + 12y2 + 8yz + 4zx  to diagonal form. Three particles of mass m are attached to a light horizontal string having ﬁxed ends, the string being thus divided into four equal portions each of length a and under a tension T . Show that for small transverse vibrations the amplitudes xi of the normal modes satisfy Bx =  maω2 T  x, where B is the matrix   cid:10    cid:9    cid:10   Estimate the lowest and highest eigenfrequencies using trial vectors  3 4 3 T and  3 − 4 3 T. Use also the exact vectors T  1 − √  √ 2 1  2 1  and  1  T  and compare the results. Use the Rayleigh–Ritz method to estimate the lowest oscillation frequency of a heavy chain of N links, each of length a  = L N , which hangs freely from one end.  Try simple calculable conﬁgurations such as all links but one vertical, or all links collinear, etc.   9.5 Hints and answers √ 2ωt , x2 = − cid:4  cos  √  √ 2ωt .  See ﬁgure 9.6.  b  x1 =  cid:4  cos ωt + cos At various times the three displacements will reach 2 cid:4 ,  cid:4 , 2 cid:4  respectively. For exam- √ ple, x1 can be written as 2 cid:4  cos[  2+1 ωt 2], i.e. an oscillation 2+1 ω 2 and modulated amplitude 2 cid:4  cos[  of angular frequency    √ √ 2−1 ωt 2] cos[  √ 2 − 1 ]. the amplitude will reach 2 cid:4  after a time ≈ 4π [ω   2ωt, x3 =  cid:4  − cos ωt + cos  √ 2−1 ωt 2];  capacitance C and inductance L.  As the circuit loops contain no voltage sources, the equations are homogeneous, and so for a non-trivial solution the determinant of coeﬃcients must vanish.   a  I1 = 0, I2 = −I3; no current in P Q; equivalent to two separate circuits of  b  I1 = −2I2 = −2I3; no current in T U; capacitance 3C 2 and inductance 2L. ω =  2.634g l 1 2 or  0.3661g l 1 2; θ1 = ξ + η, θ2 = 1.431ξ − 2.097η. Estimated, 10 17 < Maω2 T < 58 17; exact, 2 − √  2 ≤ Maω2 T ≤ 2 +  √ 2.  332   9.5 HINTS AND ANSWERS  1 m  2  M  3 m   a  ω2 = c +  d m  label2  kM  2km  kM   c  ω2 = c +  2d M  +  d m  Figure 9.6 The normal modes, as viewed from above, of the coupled pendu- lums in example 9.1.  333   10  Vector calculus  In chapter 7 we discussed the algebra of vectors, and in chapter 8 we considered how to transform one vector into another using a linear operator. In this chapter and the next we discuss the calculus of vectors, i.e. the diﬀerentiation and integration both of vectors describing particular bodies, such as the velocity of a particle, and of vector ﬁelds, in which a vector is deﬁned as a function of the coordinates throughout some volume  one-, two- or three-dimensional . Since the aim of this chapter is to develop methods for handling multi-dimensional physical situations, we will assume throughout that the functions with which we have to deal have suﬃciently amenable mathematical properties, in particular that they are continuous and diﬀerentiable.  10.1 Diﬀerentiation of vectors  Let us consider a vector a that is a function of a scalar variable u. By this we mean that with each value of u we associate a vector a u . For example, in Cartesian coordinates a u  = ax u i + ay u j + az u k, where ax u , ay u  and az u  are scalar functions of u and are the components of the vector a u  in the x-, y- and z- directions respectively. We note that if a u  is continuous at some point u = u0 then this implies that each of the Cartesian components ax u , ay u  and az u  is also continuous there.  Let us consider the derivative of the vector function a u  with respect to u. The derivative of a vector function is deﬁned in a similar manner to the ordinary derivative of a scalar function f x  given in chapter 2. The small change in the vector a u  resulting from a small change ∆u in the value of u is given by  ∆a = a u + ∆u  − a u   see ﬁgure 10.1 . The derivative of a u  with respect to u is  deﬁned to be  da du  = lim ∆u→0  a u + ∆u  − a u   ,  ∆u  334   10.1    10.1 DIFFERENTIATION OF VECTORS  ∆a = a u + ∆u  − a u   a u + ∆u   a u   Figure 10.1 A small change in a vector a u  resulting from a small change in u.  assuming that the limit exists, in which case a u  is said to be diﬀerentiable at that point. Note that da du is also a vector, which is not, in general, parallel to a u . In Cartesian coordinates, the derivative of the vector a u  = axi + ayj + azk is given by  da du  dax du  =  i +  j +  day du  daz du  k.  Perhaps the simplest application of the above is to ﬁnding the velocity and acceleration of a particle in classical mechanics. If the time-dependent position vector of the particle with respect to the origin in Cartesian coordinates is given by r t  = x t i + y t j + z t k then the velocity of the particle is given by the vector  v t  =  =  i +  j +  dr dt  dx dt  dy dt  dz dt  k.  The direction of the velocity vector is along the tangent to the path r t  at the  instantaneous position of the particle, and its magnitude v t  is equal to the  speed of the particle. The acceleration of the particle is given in a similar manner by  a t  =  =  dv dt  d2x dt2 i +  d2y dt2 j +  d2z dt2 k.   cid:1 The position vector of a particle at time t in Cartesian coordinates is given by r t  = 2t2i +  3t − 2 j +  3t2 − 1 k. Find the speed of the particle at t = 1 and the component of  its acceleration in the direction s = i + 2j + k.  The velocity and acceleration of the particle are given by  v t  =  = 4ti + 3j + 6tk,  a t  =  = 4i + 6k.  dr dt dv dt  335   VECTOR CALCULUS  y  ˆeφ  j  ˆeρ  i  ρ  φ  x  Figure 10.2 Unit basis vectors for two-dimensional Cartesian and plane polar coordinates.  The speed of the particle at t = 1 is simply   cid:24   v 1  =  42 + 32 + 62 =  √ 61.  The acceleration of the particle is constant  i.e. independent of t , and its component in the direction s is given by  a · ˆs =   4i + 6k  ·  i + 2j + k   √  12 + 22 + 12  =  √ 6  5  3  .  cid:2   Note that in the case discussed above i, j and k are ﬁxed, time-independent basis vectors. This may not be true of basis vectors in general; when we are not using Cartesian coordinates the basis vectors themselves must also be dif- ferentiated. We discuss basis vectors for non-Cartesian coordinate systems in detail in section 10.10. Nevertheless, as a simple example, let us now consider two-dimensional plane polar coordinates ρ, φ.  Referring to ﬁgure 10.2, imagine holding φ ﬁxed and moving radially outwards, i.e. in the direction of increasing ρ. Let us denote the unit vector in this direction by ˆeρ. Similarly, imagine keeping ρ ﬁxed and moving around a circle of ﬁxed radius in the direction of increasing φ. Let us denote the unit vector tangent to the circle by ˆeφ. The two vectors ˆeρ and ˆeφ are the basis vectors for this two-dimensional coordinate system, just as i and j are basis vectors for two-dimensional Cartesian coordinates. All these basis vectors are shown in ﬁgure 10.2.  An important diﬀerence between the two sets of basis vectors is that, while i and j are constant in magnitude and direction, the vectors ˆeρ and ˆeφ have constant magnitudes but their directions change as ρ and φ vary. Therefore, when calculating the derivative of a vector written in polar coordinates we must also diﬀerentiate the basis vectors. One way of doing this is to express ˆeρ and ˆeφ  336   10.1 DIFFERENTIATION OF VECTORS  in terms of i and j. From ﬁgure 10.2, we see that  ˆeρ = cos φ i + sin φ j,  ˆeφ = − sin φ i + cos φ j.  Since i and j are constant vectors, we ﬁnd that the derivatives of the basis vectors ˆeρ and ˆeφ with respect to t are given by  dˆeρ dt dˆeφ dt  = − sin φ = − cos φ  dφ dt dφ dt  i + cos φ  i − sin φ  dφ dt dφ dt  j = ˙φ ˆeφ, j = −˙φ ˆeρ,   10.2    10.3   where the overdot is the conventional notation for diﬀerentiation with respect to time.  cid:1 The position vector of a particle in plane polar coordinates is r t  = ρ t ˆeρ. Find expres- sions for the velocity and acceleration of the particle in these coordinates.  Using result  10.4  below, the velocity of the particle is given by v t  = ˙r t  = ˙ρ ˆeρ + ρ ˙ˆeρ = ˙ρ ˆeρ + ρ˙φ ˆeφ,  where we have used  10.2 . In a similar way its acceleration is given by  a t  =   ˙ρ ˆeρ + ρ˙φ ˆeφ   d dt  = ¨ρ ˆeρ + ˙ρ ˙ˆeρ + ρ˙φ ˙ˆeφ + ρ¨φ ˆeφ + ˙ρ˙φ ˆeφ = ¨ρ ˆeρ + ˙ρ ˙φ ˆeφ  + ρ˙φ −˙φ ˆeρ  + ρ¨φ ˆeφ + ˙ρ˙φ ˆeφ =  ¨ρ − ρ˙φ2  ˆeρ +  ρ¨φ + 2˙ρ˙φ  ˆeφ.  cid:2   Here we have used  10.2  and  10.3 .  10.1.1 Differentiation of composite vector expressions  In composite vector expressions each of the vectors or scalars involved may be a function of some scalar variable u, as we have seen. The derivatives of such expressions are easily found using the deﬁnition  10.1  and the rules of ordinary diﬀerential calculus. They may be summarised by the following, in which we assume that a and b are diﬀerentiable vector functions of a scalar u and that φ is a diﬀerentiable scalar function of u:  da du   φa  = φ +  a · b  = a · db  a × b  = a × db  du  d du d du d du  du  dφ a, du · b, da du × b. da du  +  +  337   10.4    10.5    10.6    VECTOR CALCULUS  The order of the factors in the terms on the RHS of  10.6  is, of course, just as important as it is in the original vector product.  cid:1 A particle of mass m with position vector r relative to some origin O experiences a force F, which produces a torque  moment  T = r × F about O. The angular momentum of the particle about O is given by L = r × mv, where v is the particle’s velocity. Show that the  rate of change of angular momentum is equal to the applied torque.  The rate of change of angular momentum is given by  Using  10.6  we obtain  dL dt  =  d dt   r × mv .  dL dt  =  dr dt  × mv + r × d = v × mv + r × d = 0 + r × F = T,  dt  dt   mv    mv   where in the last line we use Newton’s second law, namely F = d mv  dt.  cid:2   If a vector a is a function of a scalar variable s that is itself a function of u, so  that s = s u , then the chain rule  see subsection 2.1.3  gives  da s   du  =  ds du  da ds  .   10.7   The derivatives of more complicated vector expressions may be found by repeated application of the above equations.  One further useful result can be derived by considering the derivative  since a · a = a2, where a = a, we see that  d du   a · a  = 2a · da  ;  du  a · da  du  = 0  if a is constant.   10.8   In other words, if a vector a u  has a constant magnitude as u varies then it is perpendicular to the vector da du.  10.1.2 Differential of a vector  As a ﬁnal note on the diﬀerentiation of vectors, we can also deﬁne the diﬀerential of a vector, in a similar way to that of a scalar in ordinary diﬀerential calculus. In the deﬁnition of the vector derivative  10.1 , we used the notion of a small change ∆a in a vector a u  resulting from a small change ∆u in its argument. In  the limit ∆u → 0, the change in a becomes inﬁnitesimally small, and we denote it  by the diﬀerential da. From  10.1  we see that the diﬀerential is given by  da =  du.  da du  338   10.9    10.2 INTEGRATION OF VECTORS  Note that the diﬀerential of a vector is also a vector. As an example, the inﬁnitesimal change in the position vector of a particle in an inﬁnitesimal time dt is  dr =  dt = v dt,  dr dt  where v is the particle’s velocity.  10.2 Integration of vectors  The integration of a vector  or of an expression involving vectors that may itself be either a vector or scalar  with respect to a scalar u can be regarded as the inverse of diﬀerentiation. We must remember, however, that   i  the integral has the same nature  vector or scalar  as the integrand,  ii  the constant of integration for indeﬁnite integrals must be of the same  nature as the integral.  For example, if a u  = d[A u ] du then the indeﬁnite integral of a u  is given by  where b is a constant vector. The deﬁnite integral of a u  from u = u1 to u = u2 is given by   cid:21    cid:21   u1  a u  du = A u  + b,  u2  a u  du = A u2  − A u1 .   cid:1 A small particle of mass m orbits a much larger mass M centred at the origin O. According to Newton’s law of gravitation, the position vector r of the small mass obeys the diﬀerential equation  Show that the vector r × dr dt is a constant of the motion.  m  d2r dt2  = − GMm  ˆr.  r2  Forming the vector product of the diﬀerential equation with r, we obtain  Since r and ˆr are collinear, r × ˆr = 0 and therefore we have  r × d2r  dt2  = − GM  r2 r × ˆr.   10.10   However,  r × d2r  dt2  = 0.   cid:7    cid:8   d dt  r × dr  dt  = r × d2r  +  dt2  × dr  dt  dr dt  = 0,  339   VECTOR CALCULUS  z  C  ˆn  P  ˆb  r u   ˆt  O  y  x  Figure 10.3 The unit tangent ˆt, normal ˆn and binormal ˆb to the space curve C at a particular point P .  since the ﬁrst term is zero by  10.10 , and the second is zero because it is the vector product of two parallel  in this case identical  vectors. Integrating, we obtain the required result  where c is a constant vector.  As a further point of interest we may note that in an inﬁnitesimal time dt the change in the position vector of the small mass is dr and the element of area swept out by the position vector of the particle is simply dA = 1 2 by dt, we conclude that  r× dr. Dividing both sides of this equation   10.11   and that the physical interpretation of the above result  10.11  is that the position vector r of the small mass sweeps out equal areas in equal times. This result is in fact valid for motion under any force that acts along the line joining the two particles.  cid:2   r × dr  dt  = c,   cid:20  cid:20  cid:20  cid:20 r × dr   cid:20  cid:20  cid:20  cid:20  =  dt  c  2  ,  dA dt  =  1 2  10.3 Space curves  In the previous section we mentioned that the velocity vector of a particle is a tangent to the curve in space along which the particle moves. We now give a more complete discussion of curves in space and also a discussion of the geometrical interpretation of the vector derivative.  A curve C in space can be described by the vector r u  joining the origin O of a coordinate system to a point on the curve  see ﬁgure 10.3 . As the parameter u varies, the end-point of the vector moves along the curve. In Cartesian coordinates,  where x = x u , y = y u  and z = z u  are the parametric equations of the curve.  r u  = x u i + y u j + z u k,  340   10.3 SPACE CURVES  This parametric representation can be very useful, particularly in mechanics when the parameter may be the time t. We can, however, also represent a space curve by y = f x , z = g x , which can be easily converted into the above parametric form by setting u = x, so that  r u  = ui + f u j + g u k.  Alternatively, a space curve can be represented in the form F x, y, z  = 0, G x, y, z  = 0, where each equation represents a surface and the curve is the intersection of the two surfaces.  A curve may sometimes be described in parametric form by the vector r s , where the parameter s is the arc length along the curve measured from a ﬁxed point. Even when the curve is expressed in terms of some other parameter, it is straightforward to ﬁnd the arc length between any two points on the curve. For the curve described by r u , let us consider an inﬁnitesimal vector displacement  dr = dx i + dy j + dz k  =  dr du  · dr du  .   cid:8   2  ds du   cid:21      s =  u2  u1  dr du  · dr du  du.  along the curve. The square of the inﬁnitesimal distance moved is then given by   ds 2 = dr · dr =  dx 2 +  dy 2 +  dz 2,  from which it can be shown that cid:7   Therefore, the arc length between two points on the curve r u , given by u = u1 and u = u2, is   cid:1  A curve lying in the xy-plane is given by y = y x , z = 0. Using  10.12 , show that the  cid:7 2 dx, where arc length along the curve between x = a and x = b is given by s =  cid:7  y  = dy dx.  1 + y  b a  Let us ﬁrst represent the curve in parametric form by setting u = x, so that   10.12    cid:1    cid:24   Diﬀerentiating with respect to u, we ﬁnd  from which we obtain  r u  = ui + y u j.  dr du  = i +  j,  dy du   cid:7    cid:8   2  .  dy du  dr du  · dr du  = 1 +  341   Therefore, remembering that u = x, from  10.12  the arc length between x = a and x = b is given by  VECTOR CALCULUS      cid:21   b  a  dr du  · dr du   cid:25    cid:21   b  a   cid:7    cid:8   2  dy dx  s =  du =  1 +  dx.  This result was derived using more elementary methods in chapter 2.  cid:2   If a curve C is described by r u  then, by considering ﬁgures 10.1 and 10.3, we see that, at any given point on the curve, dr du is a vector tangent to C at that point, in the direction of increasing u. In the special case where the parameter u is the arc length s along the curve then dr ds is a unit tangent vector to C and is denoted by ˆt.  The rate at which the unit tangent ˆt changes with respect to s is given by d ˆt ds, and its magnitude is deﬁned as the curvature κ of the curve C at a given point,   cid:20  cid:20  cid:20  cid:20  =   cid:20  cid:20  cid:20  cid:20  d ˆt  ds   cid:20  cid:20  cid:20  cid:20  d2 ˆr  ds2   cid:20  cid:20  cid:20  cid:20  .  κ =  We can also deﬁne the quantity ρ = 1 κ, which is called the radius of curvature. Since ˆt is of constant  unit  magnitude, it follows from  10.8  that it is perpen- dicular to d ˆt ds. The unit vector in the direction perpendicular to ˆt is denoted by ˆn and is called the principal normal at the point. We therefore have  d ˆt ds  = κ ˆn.   10.13   The unit vector ˆb = ˆt × ˆn, which is perpendicular to the plane containing ˆt and ˆn, is called the binormal to C. The vectors ˆt, ˆn and ˆb form a right-handed rectangular cooordinate system  or triad  at any given point on C  see ﬁgure 10.3 . As s changes so that the point of interest moves along C, the triad of vectors also changes.  The rate at which ˆb changes with respect to s is given by d ˆb ds and is a measure of the torsion τ of the curve at any given point. Since ˆb is of constant magnitude, from  10.8  it is perpendicular to d ˆb ds. We may further show that d ˆb ds is also perpendicular to ˆt, as follows. By deﬁnition ˆb · ˆt = 0, which on  diﬀerentiating yields   cid:9    cid:10  ˆb · ˆt  0 =  d ds  where we have used the fact that ˆb · ˆn = 0. Hence, since d ˆb ds is perpendicular to both ˆb and ˆt, we must have d ˆb ds ∝ ˆn. The constant of proportionality is −τ,  =  =  =  d ˆb ds d ˆb ds d ˆb ds  ds  · ˆt + ˆb · d ˆt · ˆt + ˆb · κ ˆn · ˆt,  342    10.14    10.15    10.16   so we ﬁnally obtain  Taking the dot product of each side with ˆn, we see that the torsion of a curve is given by  We may also deﬁne the quantity σ = 1 τ, which is called the radius of torsion.  Finally, we consider the derivative d ˆn ds. Since ˆn = ˆb × ˆt we have  10.3 SPACE CURVES  = −τ ˆn.  d ˆb ds  τ = − ˆn · d ˆb  .  ds  d ˆn ds  =  d ˆb ds  × ˆt + ˆb × d ˆt = −τ ˆn × ˆt + ˆb × κ ˆn = τ ˆb − κ ˆt.  ds  In summary, ˆt, ˆn and ˆb and their derivatives with respect to s are related to one another by the relations  10.13 ,  10.14  and  10.15 , the Frenet–Serret formulae,  d ˆt ds  = κ ˆn,  = τ ˆb − κ ˆt,  d ˆn ds  = −τ ˆn.  d ˆb ds   cid:1 Show that the acceleration of a particle travelling along a trajectory r t  is given by  where v is the speed of the particle, ˆt is the unit tangent to the trajectory, ˆn is its principal normal and ρ is its radius of curvature.  The velocity of the particle is given by  where ds dt is the speed of the particle, which we denote by v, and ˆt is the unit vector tangent to the trajectory. Writing the velocity as v = v ˆt, and diﬀerentiating once more with respect to time t, we obtain  but we note that  Therefore, we have  This shows that in addition to an acceleration dv dt along the tangent to the particle’s trajectory, there is also an acceleration v2 ρ in the direction of the principal normal. The latter is often called the centripetal acceleration.  cid:2   a t  =  dv dt  ˆt +  v2 ρ  ˆn,  v t  =  =  dr dt  dr ds  ds dt  =  ds dt  ˆt,  a t  =  =  dv dt  dv dt  ˆt + v  d ˆt dt  ;  d ˆt dt  =  ds dt  d ˆt ds  = vκ ˆn =  ˆn.  v ρ  a t  =  dv dt  ˆt +  v2 ρ  ˆn.  343   VECTOR CALCULUS  Finally, we note that a curve r u  representing the trajectory of a particle may sometimes be given in terms of some parameter u that is not necessarily equal to the time t but is functionally related to it in some way. In this case the velocity of the particle is given by  Diﬀerentiating again with respect to time gives the acceleration as  v =  =  dr dt   cid:8   dr du  du dt  .   cid:7    cid:7    cid:8   2  a =  =  dv dt  d dt  dr du  du dt  =  d2r du2  du dt  +  dr du  d2u dt2 .  10.4 Vector functions of several arguments  The concept of the derivative of a vector is easily extended to cases where the vectors  or scalars  are functions of more than one independent scalar variable, u1, u2, . . . , un. In this case, the results of subsection 10.1.1 are still valid, except that the derivatives become partial derivatives ∂a ∂ui deﬁned as in ordinary diﬀerential calculus. For example, in Cartesian coordinates,  ∂a ∂u  ∂ax ∂u  =  i +  j +  ∂ay ∂u  ∂az ∂u  k.  In particular,  10.7  generalises to the chain rule of partial diﬀerentiation discussed in section 5.5. If a = a u1, u2, . . . , un  and each of the ui is also a function ui v1, v2, . . . , vn  of the variables vi then, generalising  5.17 ,  ∂a ∂vi  =  ∂a ∂u1  ∂u1 ∂vi  +  ∂a ∂u2  ∂u2 ∂vi  + ··· +  ∂a ∂un  ∂un ∂vi  =  ∂a ∂uj  ∂uj ∂vi  .   10.17   A special case of this rule arises when a is an explicit function of some variable v, as well as of scalars u1, u2, . . . , un that are themselves functions of v; then we have  da dv  =  +  ∂a ∂v  ∂a ∂uj  ∂uj ∂v  .   10.18   n cid:4   j=1  We may also extend the concept of the diﬀerential of a vector given in  10.9   to vectors dependent on several variables u1, u2, . . . , un:  da =  du1 +  ∂a ∂u1  du2 + ··· +  ∂a ∂u2  ∂a ∂un  dun =  ∂a ∂uj  duj.   10.19   As an example, the inﬁnitesimal change in an electric ﬁeld E in moving from a position r to a neighbouring one r + dr is given by  n cid:4   j=1  n cid:4   j=1  dE =  dx +  dy +  dz.   10.20   ∂E ∂x  ∂E ∂y  344  ∂E ∂z   10.5 SURFACES  ∂r ∂u  T  P  ∂r ∂v  u = c1  S  r u, v   v = c2  y  z  O  x  Figure 10.4 The tangent plane T to a surface S at a particular point P ; u = c1 and v = c2 are the coordinate curves, shown by dotted lines, that pass through P . The broken line shows some particular parametric curve r = r λ  lying in the surface.  10.5 Surfaces  A surface S in space can be described by the vector r u, v  joining the origin O of a coordinate system to a point on the surface  see ﬁgure 10.4 . As the parameters u and v vary, the end-point of the vector moves over the surface. This is very similar to the parametric representation r u  of a curve, discussed in section 10.3, but with the important diﬀerence that we require two parameters to describe a surface, whereas we need only one to describe a curve.  In Cartesian coordinates the surface is given by  r u, v  = x u, v i + y u, v j + z u, v k,  where x = x u, v , y = y u, v  and z = z u, v  are the parametric equations of the surface. We can also represent a surface by z = f x, y  or g x, y, z  = 0. Either of these representations can be converted into the parametric form in a similar manner to that used for equations of curves. For example, if z = f x, y  then by setting u = x and v = y the surface can be represented in parametric form by  r u, v  = ui + vj + f u, v k.  Any curve r λ , where λ is a parameter, on the surface S can be represented by a pair of equations relating the parameters u and v, for example u = f λ  and v = g λ . A parametric representation of the curve can easily be found by straightforward substitution, i.e. r λ  = r u λ , v λ  . Using  10.17  for the case where the vector is a function of a single variable λ so that the LHS becomes a  345   n =  × ∂r  ∂v  .  ∂r ∂u  dr =  du +  dv.  ∂r ∂v  ∂r ∂u   cid:20  cid:20  cid:20  cid:20  =  VECTOR CALCULUS  total derivative, the tangent to the curve r λ  at any point is given by  dr dλ  =  ∂r ∂u  du dλ  +  ∂r ∂v  dv dλ  .   10.21   The two curves u = constant and v = constant passing through any point P on S are called coordinate curves. For the curve u = constant, for example, we have du dλ = 0, and so from  10.21  its tangent vector is in the direction ∂r ∂v. Similarly, the tangent vector to the curve v = constant is in the direction ∂r ∂u. If the surface is smooth then at any point P on S the vectors ∂r ∂u and ∂r ∂v are linearly independent and deﬁne the tangent plane T at the point P  see ﬁgure 10.4 . A vector normal to the surface at P is given by  In the neighbourhood of P , an inﬁnitesimal vector displacement dr is written   10.22   The element of area at P , an inﬁnitesimal parallelogram whose sides are the coordinate curves, has magnitude  dS =  dv   cid:20  cid:20  cid:20  cid:20  ∂r  ∂u  ∂v  du × ∂r  cid:21  cid:21    cid:20  cid:20  cid:20  cid:20  ∂r  ∂u  R   cid:20  cid:20  cid:20  cid:20  du dv = n du dv. × ∂r  cid:21  cid:21    cid:20  cid:20  cid:20  cid:20  ∂r  cid:20  cid:20  cid:20  cid:20  du dv =  n du dv,  ∂u  ∂v  ∂v  R  Thus the total area of the surface is × ∂r  A =   10.23    10.24   where R is the region in the uv-plane corresponding to the range of parameter values that deﬁne the surface.  cid:1  Find the element of area on the surface of a sphere of radius a, and hence calculate the total surface area of the sphere.  We can represent a point r on the surface of the sphere in terms of the two parameters θ and φ:  r θ, φ  = a sin θ cos φ i + a sin θ sin φ j + a cos θ k,  where θ and φ are the polar and azimuthal angles respectively. At any point P , vectors tangent to the coordinate curves θ = constant and φ = constant are  = a cos θ cos φ i + a cos θ sin φ j − a sin θ k, = −a sin θ sin φ i + a sin θ cos φ j.  ∂r ∂θ ∂r ∂φ  346   10.6 SCALAR AND VECTOR FIELDS  A normal n to the surface at this point is then given by  i  n =  =  ∂r ∂θ  × ∂r  a cos θ cos φ a cos θ sin φ −a sin θ −a sin θ sin φ a sin θ cos φ = a2 sin θ sin θ cos φ i + sin θ sin φ j + cos θ k ,  ∂φ  0  j  k   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20   which has a magnitude of a2 sin θ. Therefore, the element of area at P is, from  10.23 ,  and the total surface area of the sphere is given by  dS = a2 sin θ dθ dφ,   cid:21   A =  dθ  dφ a2 sin θ = 4πa2.  π  2π  0  0  This familiar result can, of course, be proved by much simpler methods!  cid:2    cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20    cid:21   10.6 Scalar and vector ﬁelds  We now turn to the case where a particular scalar or vector quantity is deﬁned not just at a point in space but continuously as a ﬁeld throughout some region of space R  which is often the whole space . Although the concept of a ﬁeld is valid for spaces with an arbitrary number of dimensions, in the remainder of this chapter we will restrict our attention to the familiar three-dimensional case. A scalar ﬁeld φ x, y, z  associates a scalar with each point in R, while a vector ﬁeld a x, y, z  associates a vector with each point. In what follows, we will assume that the variation in the scalar or vector ﬁeld from point to point is both continuous and diﬀerentiable in R.  Simple examples of scalar ﬁelds include the pressure at each point in a ﬂuid and the electrostatic potential at each point in space in the presence of an electric charge. Vector ﬁelds relating to the same physical systems are the velocity vector in a ﬂuid  giving the local speed and direction of the ﬂow  and the electric ﬁeld. With the study of continuously varying scalar and vector ﬁelds there arises the need to consider their derivatives and also the integration of ﬁeld quantities along lines, over surfaces and throughout volumes in the ﬁeld. We defer the discussion of line, surface and volume integrals until the next chapter, and in the remainder of this chapter we concentrate on the deﬁnition of vector diﬀerential operators and their properties.  10.7 Vector operators  Certain diﬀerential operations may be performed on scalar and vector ﬁelds and have wide-ranging applications in the physical sciences. The most important operations are those of ﬁnding the gradient of a scalar ﬁeld and the divergence and curl of a vector ﬁeld. It is usual to deﬁne these operators from a strictly  347   VECTOR CALCULUS  mathematical point of view, as we do below. In the following chapter, however, we will discuss their geometrical deﬁnitions, which rely on the concept of integrating vector quantities along lines and over surfaces.  Central to all these diﬀerential operations is the vector operator ∇, which is  called del  or sometimes nabla  and in Cartesian coordinates is deﬁned by  ∇ ≡ i  ∂ ∂x  + j  + k  ∂ ∂y  ∂ ∂z  .   10.25   The form of this operator in non-Cartesian coordinate systems is discussed in sections 10.9 and 10.10.  10.7.1 Gradient of a scalar ﬁeld  The gradient of a scalar ﬁeld φ x, y, z  is deﬁned by  grad φ = ∇φ = i  ∂φ ∂x  + j  + k  ∂φ ∂y  ∂φ ∂z  .   10.26   Clearly, ∇φ is a vector ﬁeld whose x-, y- and z- components are the ﬁrst partial vector ﬁeld ∇φ should not be confused with the vector operator φ∇, which has  derivatives of φ x, y, z  with respect to x, y and z respectively. Also note that the  components  φ ∂ ∂x, φ ∂ ∂y, φ ∂ ∂z .   cid:1 Find the gradient of the scalar ﬁeld φ = xy2z3.  From  10.26  the gradient of φ is given by  ∇φ = y2z3i + 2xyz3j + 3xy2z2k.  cid:2   The gradient of a scalar ﬁeld φ has some interesting geometrical properties. Let us ﬁrst consider the problem of calculating the rate of change of φ in some particular direction. For an inﬁnitesimal vector displacement dr, forming its scalar   cid:7  product with ∇φ we obtain  ∇φ · dr =  i  ∂φ ∂x   cid:8   ∂φ ∂z ∂φ ∂z  ∂φ ∂y ∂φ ∂y  dx +  dy +  dz,  =  ∂φ ∂x = dφ,  + j  + k  ·  i dx + j dy + k dx  ,   10.27   which is the inﬁnitesimal change in φ in going from position r to r + dr. In particular, if r depends on some parameter u such that r u  deﬁnes a space curve  348   10.7 VECTOR OPERATORS  ∇φ  Q  θ  dφ ds  P  in the direction a  φ = constant  a  Figure 10.5 Geometrical properties of ∇φ. P Q gives the value of dφ ds in  the direction a.  then the total derivative of φ with respect to u along the curve is simply  In the particular case where the parameter u is the arc length s along the curve, the total derivative of φ with respect to s along the curve is given by  where ˆt is the unit tangent to the curve at the given point, as discussed in section 10.3.  In general, the rate of change of φ with respect to the distance s in a particular  direction a is given by   10.28    10.29    10.30   and is called the directional derivative. Since ˆa is a unit vector we have  where θ is the angle between ˆa and ∇φ as shown in ﬁgure 10.5. Clearly ∇φ lies in the direction of the fastest increase in φ, and ∇φ is the largest possible value of dφ ds. Similarly, the largest rate of decrease of φ is dφ ds = −∇φ in the direction of −∇φ.  = ∇φ · dr  .  du  dφ du  = ∇φ · ˆt,  dφ ds  = ∇φ · ˆa  dφ ds  = ∇φ cos θ  dφ ds  349   VECTOR CALCULUS   cid:1 For the function φ = x2y + yz at the point  1, 2,−1 , ﬁnd its rate of change with distance  in the direction a = i + 2j + 3k. At this same point, what is the greatest possible rate of change with distance and in which direction does it occur?  The gradient of φ is given by  10.26 :  ∇φ = 2xyi +  x2 + z j + yk,  = 4i + 2k  at the point  1, 2,−1 .  The unit vector in the direction of a is ˆa = 1√ 14 with distance s in this direction is, using  10.30 ,   i + 2j + 3k , so the rate of change of φ  dφ ds  = ∇φ · ˆa =  1√ 14   4 + 6  =  10√ 14  .  From the above discussion, at the point  1, 2,−1  dφ ds will be greatest in the direction  of ∇φ = 4i + 2k and has the value ∇φ =  √ 20 in this direction.  cid:2   We can extend the above analysis to ﬁnd the rate of change of a vector ﬁeld  rather than a scalar ﬁeld as above  in a particular direction. The scalar  diﬀerential operator ˆa · ∇ can be shown to give the rate of change with distance  in the direction ˆa of the quantity  vector or scalar  on which it acts. In Cartesian coordinates it may be written as  ˆa · ∇ = ax  ∂ ∂x  + ay  + az  ∂ ∂y  ∂ ∂z  .   10.31   Thus we can write the inﬁnitesimal change in an electric ﬁeld in moving from r  to r + dr given in  10.20  as dE =  dr · ∇ E. A second interesting geometrical property of ∇φ may be found by considering the surface deﬁned by φ x, y, z  = c, where c is some constant. If ˆt is a unit tangent to this surface at some point then clearly dφ ds = 0 in this direction and from  10.29  we have ∇φ · ˆt = 0. In other words, ∇φ is a vector normal to  ˆn is a unit the surface φ x, y, z  = c at every point, as shown in ﬁgure 10.5. If normal to the surface in the direction of increasing φ x, y, z , then the gradient is sometimes written  ∇φ ≡ ∂φ  ˆn,  ∂n   10.32   where ∂φ ∂n ≡ ∇φ is the rate of change of φ in the direction ˆn and is called  the normal derivative.  cid:1 Find expressions for the equations of the tangent plane and the line normal to the surface φ x, y, z  = c at the point P with coordinates x0, y0, z0. Use the results to ﬁnd the equations of the tangent plane and the line normal to the surface of the sphere φ = x2 + y2 + z2 = a2 at the point  0, 0, a .  A vector normal to the surface φ x, y, z  = c at the point P is simply ∇φ evaluated at that  point; we denote it by n0. If r0 is the position vector of the point P relative to the origin,  350   10.7 VECTOR OPERATORS  z  ˆn0   0, 0, a   z = a  O  a  y  φ = x2 + y2 + z2 = a2  x  Figure 10.6 The tangent plane and the normal to the surface of the sphere φ = x2 + y2 + z2 = a2 at the point r0 with coordinates  0, 0, a .  and r is the position vector of any point on the tangent plane, then the vector equation of the tangent plane is, from  7.41 ,   r − r0  · n0 = 0.  Similarly, if r is the position vector of any point on the straight line passing through P  with position vector r0  in the direction of the normal n0 then the vector equation of this line is, from subsection 7.7.1,   r − r0  × n0 = 0. For the surface of the sphere φ = x2 + y2 + z2 = a2,  ∇φ = 2xi + 2yj + 2zk  = 2ak  at the point  0, 0, a .  Therefore the equation of the tangent plane to the sphere at this point is  This gives 2a z − a  = 0 or z = a, as expected. The equation of the line normal to the  sphere at the point  0, 0, a  is  which gives 2ayi − 2axj = 0 or x = y = 0, i.e. the z-axis, as expected. The tangent plane and normal to the surface of the sphere at this point are shown in ﬁgure 10.6.  cid:2    r − r0  · 2ak = 0.   r − r0  × 2ak = 0,  Further properties of the gradient operation, which are analogous to those of the ordinary derivative, are listed in subsection 10.8.1 and may be easily proved.  351   In addition to these, we note that the gradient operation also obeys the chain rule as in ordinary diﬀerential calculus, i.e. if φ and ψ are scalar ﬁelds in some region R then  VECTOR CALCULUS  ∇ [φ ψ ] =  ∇ψ.  ∂φ ∂ψ  10.7.2 Divergence of a vector ﬁeld  The divergence of a vector ﬁeld a x, y, z  is deﬁned by  div a = ∇ · a =  ∂ax ∂x  +  ∂ay ∂y  +  ∂az ∂z  ,   10.33   where ax, ay and az are the x-, y- and z- components of a. Clearly, ∇· a is a scalar ﬁeld. Any vector ﬁeld a for which ∇ · a = 0 is said to be solenoidal.  cid:1 Find the divergence of the vector ﬁeld a = x2y2i + y2z2j + x2z2k.  From  10.33  the divergence of a is given by  ∇ · a = 2xy2 + 2yz2 + 2x2z = 2 xy2 + yz2 + x2z .  cid:2   We will discuss fully the geometric deﬁnition of divergence and its physical meaning in the next chapter. For the moment, we merely note that the divergence can be considered as a quantitative measure of how much a vector ﬁeld diverges  spreads out  or converges at any given point. For example, if we consider the  vector ﬁeld v x, y, z  describing the local velocity at any point in a ﬂuid then ∇ · v  is equal to the net rate of outﬂow of ﬂuid per unit volume, evaluated at a point  by letting a small volume at that point tend to zero .  Now if some vector ﬁeld a is itself derived from a scalar ﬁeld via a = ∇φ then ∇ · a has the form ∇ · ∇φ or, as it is usually written, ∇2φ, where ∇2  del squared   is the scalar diﬀerential operator  ∇2 ≡ ∂2  ∂x2 +  ∂2 ∂y2 +  ∂2 ∂z2 .   10.34   ∇2φ is called the Laplacian of φ and appears in several important partial diﬀer-  ential equations of mathematical physics, discussed in chapters 20 and 21.   cid:1 Find the Laplacian of the scalar ﬁeld φ = xy2z3.  From  10.34  the Laplacian of φ is given by  ∇2φ =  ∂2φ ∂x2  +  ∂2φ ∂y2  +  ∂2φ ∂z2  = 2xz3 + 6xy2z.  cid:2   352   10.7 VECTOR OPERATORS  The curl of a vector ﬁeld a x, y, z  is deﬁned by  10.7.3 Curl of a vector ﬁeld   cid:7    cid:8    cid:7    cid:8    cid:7   curl a = ∇ × a =  ∂az ∂y  − ∂ay  ∂z  i +  ∂ax ∂z  − ∂az  ∂x  j +  ∂ay ∂x  − ∂ax  ∂y  k,  where ax, ay and az are the x-, y- and z- components of a. The RHS can be written in a more memorable form as a determinant:   cid:8    cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20    cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  ,  ∇ × a =  i ∂ ∂x ax  j ∂ ∂y ay  k ∂ ∂z az   10.35   where it is understood that, on expanding the determinant, the partial derivatives  in the second row act on the components of a in the third row. Clearly, ∇ × a is itself a vector ﬁeld. Any vector ﬁeld a for which ∇× a = 0 is said to be irrotational.   cid:1 Find the curl of the vector ﬁeld a = x2y2z2i + y2z2j + x2z2k.  The curl of a is given by   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20   ∇φ =  i ∂ ∂x  x2y2z2  j ∂ ∂y y2z2  k ∂ ∂z x2z2   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  = −2   cid:18   y2zi +  xz2 − x2y2z j + x2yz2k  .  cid:2    cid:19   For a vector ﬁeld v x, y, z  describing the local velocity at any point in a ﬂuid,  that point. If a small paddle wheel were placed at various points in the ﬂuid then  ∇ × v is a measure of the angular velocity of the ﬂuid in the neighbourhood of it would tend to rotate in regions where ∇ × v  cid:3 = 0, while it would not rotate in regions where ∇ × v = 0.  Another insight into the physical interpretation of the curl operator is gained by considering the vector ﬁeld v describing the velocity at any point in a rigid body rotating about some axis with angular velocity ω. If r is the position vector of the point with respect to some origin on the axis of rotation then the velocity of the point is given by v = ω × r. Without any loss of generality, we may take ω to lie along the z-axis of our coordinate system, so that ω = ω k. The velocity ﬁeld is then v = −ωy i + ωx j. The curl of this vector ﬁeld is easily found to be   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20   ∇ × v =  i ∂ ∂x  j ∂ ∂y  −ωy ωx  k ∂ ∂z 0  353   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  = 2ωk = 2ω.   10.36    VECTOR CALCULUS  ∇ φ + ψ  = ∇φ + ∇ψ ∇ ·  a + b  = ∇ · a + ∇ · b ∇ ×  a + b  = ∇ × a + ∇ × b ∇ φψ  = φ∇ψ + ψ∇φ ∇ a · b  = a ×  ∇ × b  + b ×  ∇ × a  +  a · ∇ b +  b · ∇ a ∇ ·  φa  = φ∇ · a + a · ∇φ ∇ ·  a × b  = b ·  ∇ × a  − a ·  ∇ × b  ∇ ×  φa  = ∇φ × a + φ∇ × a ∇ ×  a × b  = a ∇ · b  − b ∇ · a  +  b · ∇ a −  a · ∇ b  Table 10.1 Vector operators acting on sums and products. The operator ∇ is  deﬁned in  10.25 ; φ and ψ are scalar ﬁelds, a and b are vector ﬁelds.  Therefore the curl of the velocity ﬁeld is a vector equal to twice the angular velocity vector of the rigid body about its axis of rotation. We give a full geometrical discussion of the curl of a vector in the next chapter.  10.8 Vector operator formulae  In the same way as for ordinary vectors  chapter 7 , for vector operators certain identities exist. In addition, we must consider various relations involving the action of vector operators on sums and products of scalar and vector ﬁelds. Some of these relations have been mentioned earlier, but we list all the most important ones here for convenience. The validity of these relations may be easily veriﬁed by direct calculation  a quick method of deriving them using tensor notation is given in chapter 26 .  Although some of the following vector relations are expressed in Cartesian coordinates, it may be proved that they are all independent of the choice of coordinate system. This is to be expected since grad, div and curl all have clear geometrical deﬁnitions, which are discussed more fully in the next chapter and which do not rely on any particular choice of coordinate system.  10.8.1 Vector operators acting on sums and products  Let φ and ψ be scalar ﬁelds and a and b be vector ﬁelds. Assuming these ﬁelds are diﬀerentiable, the action of grad, div and curl on various sums and products of them is presented in table 10.1.  These relations can be proved by direct calculation.  354   10.8 VECTOR OPERATOR FORMULAE   cid:1 Show that  ∇ ×  φa  = ∇φ × a + φ∇ × a.  The x-component of the LHS is   φaz  − ∂  ∂z  ∂ ∂y   φay  = φ  +   cid:7   ∂φ ∂y  ∂az ∂y  ∂ay ∂z   cid:8  az − φ   cid:7  = φ ∇ × a x +  ∇φ × a x,  − ∂ay  ∂az ∂y  = φ  ∂z  +  ∂φ ∂y  ay,  − ∂φ az − ∂φ  ∂z  ∂z   cid:8   ay  ,  where, for example,  ∇φ× a x denotes the x-component of the vector ∇φ× a. Incorporating the y- and z- components, which can be similarly found, we obtain the stated result.  cid:2   Some useful special cases of the relations in table 10.1 are worth noting. If r is  the position vector relative to some origin and r = r, then  ∇φ r  =  dφ dr  ˆr,  ∇ · [φ r r] = 3φ r  + r  ∇2φ r  = ∇ × [φ r r] = 0.  d2φ r  dr2 +  dφ r   ,  dr dφ r  2 r  dr  ,  ∇r = ˆr,  cid:7   cid:7  ∇  1 r   cid:8   cid:8   ∇ ·  ˆr r2  ∇ · r = 3,  ∇ × r = 0,   cid:7   = − ˆr r2 , = −∇2   cid:8   1 r  = 4πδ r ,  These results may be proved straightforwardly using Cartesian coordinates but far more simply using spherical polar coordinates, which are discussed in subsec- tion 10.9.2. Particular cases of these results are  together with  where δ r  is the Dirac delta function, discussed in chapter 13. The last equation is important in the solution of certain partial diﬀerential equations and is discussed further in chapter 20.  10.8.2 Combinations of grad, div and curl  We now consider the action of two vector operators in succession on a scalar or vector ﬁeld. We can immediately discard four of the nine obvious combinations of grad, div and curl, since they clearly do not make sense. If φ is a scalar ﬁeld and  355   VECTOR CALCULUS  a is a vector ﬁeld, these four combinations are grad grad φ , div div a , curl div a  and grad curl a . In each case the second  outer  vector operator is acting on the wrong type of ﬁeld, i.e. scalar instead of vector or vice versa. In grad grad φ , for example, grad acts on grad φ, which is a vector ﬁeld, but we know that grad only acts on scalar ﬁelds  although in fact we will see in chapter 26 that we can form the outer product of the del operator with a vector to give a tensor, but that need not concern us here .  Of the ﬁve valid combinations of grad, div and curl, two are identically zero,  namely  since  curl grad φ = ∇ × ∇φ = 0, div curl a = ∇ ·  ∇ × a  = 0.   10.37    10.38   From  10.37 , we see that if a is derived from the gradient of some scalar function  such that a = ∇φ then it is necessarily irrotational  ∇ × a = 0 . We also note a + ∇φ + c, where φ is any scalar ﬁeld and c is a constant vector. This follows  that if a is an irrotational vector ﬁeld then another irrotational vector ﬁeld is  ∇ ×  a + ∇φ + c  = ∇ × a + ∇ × ∇φ = 0.  Similarly, from  10.38  we may infer that if b is the curl of some vector ﬁeld a  such that b = ∇ × a then b is solenoidal  ∇ · b = 0 . Obviously, if b is solenoidal  and c is any constant vector then b + c is also solenoidal.  The three remaining combinations of grad, div and curl are  ∂2φ ∂z2 ,  ∂2φ ∂y2 +   cid:8    cid:7   ∂2φ ∂x2 +  div grad φ = ∇ · ∇φ = ∇2φ = grad div a = ∇ ∇ · a ,  cid:7  ∂2ax ∂x2 + ∂2ax ∂z∂x   cid:8  curl curl a = ∇ ×  ∇ × a  = ∇ ∇ · a  − ∇2a,  ∂2ay ∂x∂y  ∂2az ∂x∂z  ∂2ay ∂z∂y  ∂2az ∂z2  i +   cid:7   k,  +  +  =  +  +  ∂2ax ∂y∂x  +  ∂2ay ∂y2 +  ∂2az ∂y∂z  j   10.39    cid:8    10.40    10.41   where  10.39  and  10.40  are expressed in Cartesian coordinates. In  10.41 , the  term ∇2a has the linear diﬀerential operator ∇2 acting on a vector  as opposed to  a scalar as in  10.39  , which of course consists of a sum of unit vectors multiplied by components. Two cases arise.   i  If the unit vectors are constants  i.e. they are independent of the values of the coordinates  then the diﬀerential operator gives a non-zero contribution only when acting upon the components, the unit vectors being merely multipliers.  356   10.9 CYLINDRICAL AND SPHERICAL POLAR COORDINATES   ii  If the unit vectors vary as the values of the coordinates change  i.e. are not constant in direction throughout the whole space  then the derivatives  of these vectors appear as contributions to ∇2a.  Cartesian coordinates are an example of the ﬁrst case in which each component  satisﬁes  ∇2a i = ∇2ai. In this case  10.41  can be applied to each component  [∇ ×  ∇ × a ]i = [∇ ∇ · a ]i − ∇2ai.   10.42   However, cylindrical and spherical polar coordinates come in the second class. For them  10.41  is still true, but the further step to  10.42  cannot be made.  More complicated vector operator relations may be proved using the relations  separately:  given above.  cid:1 Show that  ∇ ·  ∇φ × ∇ψ  = 0,  where φ and ψ are scalar ﬁelds.  From the previous section we have  ∇ ·  a × b  = b ·  ∇ × a  − a ·  ∇ × b .  If we let a = ∇φ and b = ∇ψ then we obtain since ∇ × ∇φ = 0 = ∇ × ∇ψ, from  10.37 .  cid:2   ∇ ·  ∇φ × ∇ψ  = ∇ψ ·  ∇ × ∇φ  − ∇φ ·  ∇ × ∇ψ  = 0,   10.43   10.9 Cylindrical and spherical polar coordinates  The operators we have discussed in this chapter, i.e. grad, div, curl and ∇2,  have all been deﬁned in terms of Cartesian coordinates, but for many physical situations other coordinate systems are more natural. For example, many systems, such as an isolated charge in space, have spherical symmetry and spherical polar coordinates would be the obvious choice. For axisymmetric systems, such as ﬂuid ﬂow in a pipe, cylindrical polar coordinates are the natural choice. The physical laws governing the behaviour of the systems are often expressed in terms of the vector operators we have been discussing, and so it is necessary to be able to express these operators in these other, non-Cartesian, coordinates. We ﬁrst consider the two most common non-Cartesian coordinate systems, i.e. cylindrical and spherical polars, and go on to discuss general curvilinear coordinates in the next section.  10.9.1 Cylindrical polar coordinates  As shown in ﬁgure 10.7, the position of a point in space P having Cartesian coordinates x, y, z may be expressed in terms of cylindrical polar coordinates  357   VECTOR CALCULUS  ρ, φ, z, where  be written  and ρ ≥ 0, 0 ≤ φ < 2π and −∞ < z < ∞. The position vector of P may therefore  x = ρ cos φ,  y = ρ sin φ,  z = z,   10.44   r = ρ cos φ i + ρ sin φ j + z k.   10.45   If we take the partial derivatives of r with respect to ρ, φ and z respectively then we obtain the three vectors  eρ =  = cos φ i + sin φ j,  eφ =  = −ρ sin φ i + ρ cos φ j,  ∂r ∂ρ ∂r ∂φ ∂r ∂z  ez =  = k.  ˆeρ = eρ = cos φ i + sin φ j,  eφ = − sin φ i + cos φ j,  ˆeφ =  1 ρ  ˆez = ez = k.  dr =  dρ +  dφ +  ∂r ∂ρ  ∂r ∂φ  ∂r ∂z = dρ eρ + dφ eφ + dz ez = dρ ˆeρ + ρ dφ ˆeφ + dz ˆez.  dz  These vectors lie in the directions of increasing ρ, φ and z respectively but are not all of unit length. Although eρ, eφ and ez form a useful set of basis vectors in their own right  we will see in section 10.10 that such a basis is sometimes the most useful , it is usual to work with the corresponding unit vectors, which are obtained by dividing each vector by its modulus to give  These three unit vectors, like the Cartesian unit vectors i, j and k, form an orthonormal triad at each point in space, i.e. the basis vectors are mutually orthogonal and of unit length  see ﬁgure 10.7 . Unlike the ﬁxed vectors i, j and k, however, ˆeρ and ˆeφ change direction as P moves.  The expression for a general inﬁnitesimal vector displacement dr in the position  of P is given, from  10.19 , by  This expression illustrates an important diﬀerence between Cartesian and cylin- drical polar coordinates  or non-Cartesian coordinates in general . In Cartesian coordinates, the distance moved in going from x to x + dx, with y and z held constant, is simply ds = dx. However, in cylindrical polars, if φ changes by dφ, with ρ and z held constant, then the distance moved is not dφ, but ds = ρ dφ.  358   10.46    10.47    10.48    10.49    10.50    10.51    10.52    10.9 CYLINDRICAL AND SPHERICAL POLAR COORDINATES  z  k  O  i  φ  z  ˆez  P  ˆeφ  ˆeρ  z  r  j  ρ  ρ dφ  dz  dρ  φ  dφ  ρ  ρ dφ  y  y  Figure 10.7 Cylindrical polar coordinates ρ, φ, z.  x  x  Figure 10.8 The element of volume in cylindrical polar coordinates is given by ρ dρ dφ dz.  Factors, such as the ρ in ρ dφ, that multiply the coordinate diﬀerentials to give distances are known as scale factors. From  10.52 , the scale factors for the ρ-, φ- and z- coordinates are therefore 1, ρ and 1 respectively.  The magnitude ds of the displacement dr is given in cylindrical polar coordinates  by   ds 2 = dr · dr =  dρ 2 + ρ2 dφ 2 +  dz 2,  where in the second equality we have used the fact that the basis vectors are orthonormal. We can also ﬁnd the volume element in a cylindrical polar system  see ﬁgure 10.8  by calculating the volume of the inﬁnitesimal parallelepiped  359   VECTOR CALCULUS  ∇Φ = ∇ · a =  ∇ × a =  ∇2Φ =   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20   1 ρ  ˆeρ +  ∂Φ ∂ρ 1 ∂ ρ ∂ρ  1 ρ  ∂Φ ˆeφ + ∂φ 1 ρ  ∂aφ ∂φ  ∂Φ ∂z  +  ˆez  ∂az ∂z   ρaρ  +   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20   ˆeρ ∂ ∂ρ aρ   cid:7   ρˆeφ ∂ ∂φ ρaφ   cid:8   ˆez ∂ ∂z az  1 ρ  ∂ ∂ρ  ρ  ∂Φ ∂ρ  +  1 ρ2  ∂2Φ ∂φ2  +  ∂2Φ ∂z2  Table 10.2 Vector operators in cylindrical polar coordinates; Φ is a scalar ﬁeld and a is a vector ﬁeld.  deﬁned by the vectors dρ ˆeρ, ρ dφ ˆeφ and dz ˆez:  dV = dρ ˆeρ ·  ρ dφ ˆeφ × dz ˆez  = ρ dρ dφ dz,  which again uses the fact that the basis vectors are orthonormal. For a simple coordinate system such as cylindrical polars the expressions for  ds 2 and dV are obvious from the geometry.  We will now express the vector operators discussed in this chapter in terms of cylindrical polar coordinates. Let us consider a vector ﬁeld a ρ, φ, z  and a scalar ﬁeld Φ ρ, φ, z , where we use Φ for the scalar ﬁeld to avoid confusion with the azimuthal angle φ. We must ﬁrst write the vector ﬁeld in terms of the basis vectors of the cylindrical polar coordinate system, i.e.  a = aρ ˆeρ + aφ ˆeφ + az ˆez,  where aρ, aφ and az are the components of a in the ρ-, φ- and z- directions respectively. The expressions for grad, div, curl and ∇2 can then be calculated  and are given in table 10.2. Since the derivations of these expressions are rather complicated we leave them until our discussion of general curvilinear coordinates in the next section; the reader could well postpone examination of these formal proofs until some experience of using the expressions has been gained.   cid:1 Express the vector ﬁeld a = yz i − y j + xz2 k in cylindrical polar coordinates, and hence  calculate its divergence. Show that the same result is obtained by evaluating the divergence in Cartesian coordinates.  The basis vectors of the cylindrical polar coordinate system are given in  10.49 – 10.51 . Solving these equations simultaneously for i, j and k we obtain  i = cos φ ˆeρ − sin φ ˆeφ  j = sin φ ˆeρ + cos φ ˆeφ k = ˆez.  360   10.9 CYLINDRICAL AND SPHERICAL POLAR COORDINATES  ˆer ˆeφ  ˆeθ  P  z  θ  r  j  k  O  i  φ  y  x  Figure 10.9 Spherical polar coordinates r, θ, φ.  Substituting these relations and  10.44  into the expression for a we ﬁnd  a = zρ sin φ  cos φ ˆeρ − sin φ ˆeφ  − ρ sin φ  sin φ ˆeρ + cos φ ˆeφ  + z2ρ cos φ ˆez =  zρ sin φ cos φ − ρ sin2 φ  ˆeρ −  zρ sin2 φ + ρ sin φ cos φ  ˆeφ + z2ρ cos φ ˆez.  Substituting into the expression for ∇ · a given in table 10.2,  ∇ · a = 2z sin φ cos φ − 2 sin2 φ − 2z sin φ cos φ − cos2 φ + sin2 φ + 2zρ cos φ  = 2zρ cos φ − 1.  Alternatively, and much more quickly in this case, we can calculate the divergence  directly in Cartesian coordinates. We obtain  ∇ · a =  ∂ax ∂x  +  ∂ay ∂y  +  ∂az ∂z  = 2zx − 1,  which on substituting x = ρ cos φ yields the same result as the calculation in cylindrical polars.  cid:2   Finally, we note that similar results can be obtained for  two-dimensional  polar coordinates in a plane by omitting the z-dependence. For example,  ds 2 =  dρ 2 + ρ2 dφ 2, while the element of volume is replaced by the element of area dA = ρ dρ dφ.  10.9.2 Spherical polar coordinates  As shown in ﬁgure 10.9, the position of a point in space P , with Cartesian coordinates x, y, z, may be expressed in terms of spherical polar coordinates r, θ, φ, where  x = r sin θ cos φ,  y = r sin θ sin φ,  z = r cos θ,   10.53   361   VECTOR CALCULUS  and r ≥ 0, 0 ≤ θ ≤ π and 0 ≤ φ < 2π. The position vector of P may therefore be  written as  r = r sin θ cos φ i + r sin θ sin φ j + r cos θ k.  If, in a similar manner to that used in the previous section for cylindrical polars, we ﬁnd the partial derivatives of r with respect to r, θ and φ respectively and divide each of the resulting vectors by its modulus then we obtain the unit basis vectors  ˆer = sin θ cos φ i + sin θ sin φ j + cos θ k,  ˆeθ = cos θ cos φ i + cos θ sin φ j − sin θ k, ˆeφ = − sin φ i + cos φ j.  These unit vectors are in the directions of increasing r, θ and φ respectively and are the orthonormal basis set for spherical polar coordinates, as shown in ﬁgure 10.9.  A general inﬁnitesimal vector displacement in spherical polars is, from  10.19 ,  dr = dr ˆer + r dθ ˆeθ + r sin θ dφ ˆeφ;   10.54   thus the scale factors for the r-, θ- and φ- coordinates are 1, r and r sin θ respectively. The magnitude ds of the displacement dr is given by  ds 2 = dr · dr =  dr 2 + r2 dθ 2 + r2 sin2 θ dφ 2,  since the basis vectors form an orthonormal set. The element of volume in spherical polar coordinates  see ﬁgure 10.10  is the volume of the inﬁnitesimal parallelepiped deﬁned by the vectors dr ˆer, r dθ ˆeθ and r sin θ dφ ˆeφ and is given by  dV = dr ˆer ·  r dθ ˆeθ × r sin θ dφ ˆeφ  = r2 sin θ dr dθ dφ,  where again we use the fact that the basis vectors are orthonormal. The expres- sions for  ds 2 and dV in spherical polars can be obtained from the geometry of this coordinate system.  We will now express the standard vector operators in spherical polar coordi- nates, using the same techniques as for cylindrical polar coordinates. We consider a scalar ﬁeld Φ r, θ, φ  and a vector ﬁeld a r, θ, φ . The latter may be written in terms of the basis vectors of the spherical polar coordinate system as  a = ar ˆer + aθ ˆeθ + aφ ˆeφ,  where ar, aθ and aφ are the components of a in the r-, θ- and φ- directions respectively. The expressions for grad, div, curl and ∇2 are given in table 10.3. As a ﬁnal note, we mention that, in the expression for ∇2Φ given in table 10.3,  The derivations of these results are given in the next section.  362   10.9 CYLINDRICAL AND SPHERICAL POLAR COORDINATES  ∇Φ = ∇ · a =  ∇ × a =  ∇2Φ =  ∂Φ ∂r  ˆer +  1 r  ∂Φ ∂θ  ˆeθ +  ∂Φ ∂φ  ˆeφ  r sin θ  1 r2  ∂ ∂r   r2ar  +  1  r sin θ   sin θ aθ  +  1  r sin θ  ∂aφ ∂φ  1  ∂ ∂θ   cid:7   ˆer ∂ ∂r ar r2 ∂Φ ∂r   cid:8   rˆeθ ∂ ∂θ raθ  +  1  r2 sin θ  1 r2  ∂ ∂r  r sin θ ˆeφ  ∂ ∂φ  r sin θ aφ   cid:7   1  r2 sin θ  ∂ ∂θ  sin θ  +  ∂Φ ∂θ  1  r2 sin2 θ  ∂2Φ ∂φ2   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20    cid:8   Table 10.3 Vector operators in spherical polar coordinates; Φ is a scalar ﬁeld and a is a vector ﬁeld.   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20   z  dφ  dr  θ  r dθ r  dθ  dφ  φ  r sin θ  r sin θ dφ  y  r sin θ dφ  x  Figure 10.10 The element of volume in spherical polar coordinates is given by r2 sin θ dr dθ dφ.  we can rewrite the ﬁrst term on the RHS as follows:   cid:7    cid:8   1 r2  ∂ ∂r  r2 ∂Φ ∂r  =  1 r  ∂2 ∂r2  rΦ ,  which can often be useful in shortening calculations.  363   VECTOR CALCULUS  10.10 General curvilinear coordinates  As indicated earlier, the contents of this section are more formal and technically complicated than hitherto. The section could be omitted until the reader has had some experience of using its results.  Cylindrical and spherical polars are just two examples of what are called general curvilinear coordinates. In the general case, the position of a point P having Cartesian coordinates x, y, z may be expressed in terms of the three curvilinear coordinates u1, u2, u3, where  x = x u1, u2, u3 ,  y = y u1, u2, u3 ,  z = z u1, u2, u3 ,  and similarly  u1 = u1 x, y, z ,  u2 = u2 x, y, z ,  u3 = u3 x, y, z .  We assume that all these functions are continuous, diﬀerentiable and have a single-valued inverse, except perhaps at or on certain isolated points or lines, so that there is a one-to-one correspondence between the x, y, z and u1, u2, u3 systems. The u1-, u2- and u3- coordinate curves of a general curvilinear system are analogous to the x-, y- and z- axes of Cartesian coordinates. The surfaces u1 = c1, u2 = c2 and u3 = c3, where c1, c2, c3 are constants, are called the coordinate surfaces and each pair of these surfaces has its intersection in a curve called a coordinate curve or line  see ﬁgure 10.11 .  If at each point in space the three coordinate surfaces passing through the point meet at right angles then the curvilinear coordinate system is called orthogonal. For example, in spherical polars u1 = r, u2 = θ, u3 = φ and the three coordinate surfaces passing through the point  R, Θ, Φ  are the sphere r = R, the circular cone θ = Θ and the plane φ = Φ, which intersect at right angles at that point. Therefore spherical polars form an orthogonal coordinate system  as do cylindrical polars  .  If r u1, u2, u3  is the position vector of the point P then e1 = ∂r ∂u1 is a vector tangent to the u1-curve at P  for which u2 and u3 are constants  in the direction of increasing u1. Similarly, e2 = ∂r ∂u2 and e3 = ∂r ∂u3 are vectors tangent to the u2- and u3- curves at P in the direction of increasing u2 and u3 respectively. Denoting the lengths of these vectors by h1, h2 and h3, the unit vectors in each of these directions are given by  ˆe1 =  1 h1  ∂r ∂u1  ,  ˆe2 =  1 h2  ∂r ∂u2  ,  ˆe3 =  1 h3  ∂r ∂u3  ,  where h1 = ∂r ∂u1, h2 = ∂r ∂u2 and h3 = ∂r ∂u3.  The quantities h1, h2, h3 are the scale factors of the curvilinear coordinate system. The element of distance associated with an inﬁnitesimal change dui in one of the coordinates is hi dui. In the previous section we found that the scale  364   10.10 GENERAL CURVILINEAR COORDINATES  u3  ˆe1  j  k  O  i  z  ˆe3  ˆ cid:2 3  u1 = c1  u2 = c2  u1  ˆ cid:2 1  P  ˆ cid:2 2  ˆe2  u2  u3 = c3  y  x  Figure 10.11 General curvilinear coordinates.  factors for cylindrical and spherical polar coordinates were  for cylindrical polars  for spherical polars  hρ = 1, hr = 1,  hφ = ρ, hθ = r,  hz = 1, hφ = r sin θ.  Although the vectors e1, e2, e3 form a perfectly good basis for the curvilinear coordinate system, it is usual to work with the corresponding unit vectors ˆe1, ˆe2, ˆe3. For an orthogonal curvilinear coordinate system these unit vectors form an orthonormal basis.  An inﬁnitesimal vector displacement in general curvilinear coordinates is given  by, from  10.19 ,  dr =  du1 +  du2 +  du3  ∂r ∂u1  ∂r ∂u2  ∂r ∂u3  = du1 e1 + du2 e2 + du3 e3  = h1 du1 ˆe1 + h2 du2 ˆe2 + h3 du3 ˆe3.   10.55    10.56    10.57   In the case of orthogonal curvilinear coordinates, where the ˆei are mutually perpendicular, the element of arc length is given by   ds 2 = dr · dr = h2  1 du1 2 + h2  2 du2 2 + h2  3 du3 2.   10.58   The volume element for the coordinate system is the volume of the inﬁnitesimal parallelepiped deﬁned by the vectors  ∂r ∂ui  dui = dui ei = hi dui ˆei, for i = 1, 2, 3.  365   VECTOR CALCULUS  For orthogonal coordinates this is given by  dV = du1 e1 ·  du2 e2 × du3 e3   = h1 ˆe1 ·  h2 ˆe2 × h3 ˆe3  du1 du2 du3  = h1h2h3 du1 du2 du3.  Now, in addition to the set {ˆei}, i = 1, 2, 3, there exists another useful set of three unit basis vectors at P . Since ∇u1 is a vector normal to the surface u1 = c1, a unit vector in this direction is ˆ cid:2 1 = ∇u1 ∇u1. Similarly, ˆ cid:2 2 = ∇u2 ∇u2 and ˆ cid:2 3 = ∇u3 ∇u3 are unit vectors normal to the surfaces u2 = c2 and u3 = c3  respectively.  Therefore at each point P in a curvilinear coordinate system, there exist, in  general, two sets of unit vectors: {ˆei}, tangent to the coordinate curves, and {ˆ cid:2 i},  normal to the coordinate surfaces. A vector a can be written in terms of either set of unit vectors:  a = a1 ˆe1 + a2 ˆe2 + a3 ˆe3 = A1 ˆ cid:2 1 + A2 ˆ cid:2 2 + A3 ˆ cid:2 3,  where a1, a2, a3 and A1, A2, A3 are the components of a in the two systems. It may be shown that the two bases become identical if the coordinate system is orthogonal.  Instead of the unit vectors discussed above, we could instead work directly with  the two sets of vectors {ei = ∂r ∂ui} and { cid:2 i = ∇ui}, which are not, in general, of  unit length. We can then write a vector a as  a = α1e1 + α2e2 + α3e3 = β1 cid:2 1 + β2 cid:2 2 + β3 cid:2 3,  or more explicitly as  a = α1  + α2  + α3  ∂r ∂u1  ∂r ∂u2  ∂r ∂u3  = β1∇u1 + β2∇u2 + β3∇u3,  where α1, α2, α3 and β1, β2, β3 are called the contravariant and covariant com- ponents of a respectively. A more detailed discussion of these components, in the context of tensor analysis, is given in chapter 26. The  in general  non-unit  bases {ei} and { cid:2 i} are often the most natural bases in which to express vector  quantities.  obtain   cid:1 Show that {ei} and { cid:2 i} are reciprocal systems of vectors. Let us consider the scalar product ei ·  cid:2 j; using the Cartesian expressions for r and ∇, we  ei ·  cid:2 j =   cid:7   ∂r ∂ui  · ∇uj  ∂x ∂ui  =  =  ∂y ∂ui ∂y ∂ui  i +  j +  ∂z ∂ui  k  i +  j +  ∂uj ∂y  ∂uj ∂z  k  ∂x ∂ui  ∂uj ∂x  +  ∂uj ∂y  +  ∂z ∂ui  ∂uj ∂z  ∂uj ∂ui  .   cid:8    cid:7   ·  ∂uj ∂x  =  366   cid:8    10.10 GENERAL CURVILINEAR COORDINATES  In the last step we have used the chain rule for partial diﬀerentiation. Therefore ei ·  cid:2 j = 1 if i = j, and ei ·  cid:2 j = 0 otherwise. Hence {ei} and { cid:2 j} are reciprocal systems of vectors.  cid:2   We now derive expressions for the standard vector operators in orthogonal curvilinear coordinates. Despite the useful properties of the non-unit bases dis- cussed above, the remainder of our discussion in this section will be in terms of  the unit basis vectors {ˆei}. The expressions for the vector operators in cylindrical  and spherical polar coordinates given in tables 10.2 and 10.3 respectively can be found from those derived below by inserting the appropriate scale factors.  The change dΦ in a scalar ﬁeld Φ resulting from changes du1, du2, du3 in the coordinates u1, u2, u3 is given by, from  5.5 ,  Gradient  dΦ =  du1 +  du2 +  du3.  ∂Φ ∂u2  ∂Φ ∂u3  ∂Φ ∂u1  For orthogonal curvilinear coordinates u1, u2, u3 we ﬁnd from  10.57 , and com- parison with  10.27 , that we can write this as  dΦ = ∇Φ · dr,  where ∇Φ is given by  ∇Φ =  1 h1  ∂Φ ∂u1  ˆe1 +  1 h2  ∂Φ ∂u2  ˆe2 +  1 h3  ∂Φ ∂u3  ˆe3.  This implies that the del operator can be written  ∇ =  ˆe1 h1  ∂ ∂u1  +  ˆe2 h2  ∂ ∂u2  +  ˆe3 h3  ∂ ∂u3  .   10.59    10.60    cid:1 Show that for orthogonal curvilinear coordinates ∇ui = ˆei hi. Hence show that the two sets of vectors {ˆei} and {ˆ cid:2 i} are identical in this case. Letting Φ = ui in  10.60  we ﬁnd immediately that ∇ui = ˆei hi. Therefore ∇ui = 1 hi, and so ˆ cid:2 i = ∇ui ∇ui = hi∇ui = ˆei.  cid:2   Divergence  In order to derive the expression for the divergence of a vector ﬁeld in orthogonal curvilinear coordinates, we must ﬁrst write the vector ﬁeld in terms of the basis vectors of the coordinate system:  The divergence is then given by  a = a1 ˆe1 + a2 ˆe2 + a3 ˆe3.   cid:13    cid:14   ∇ · a =  1  h1h2h3  ∂ ∂u1   h2h3a1  +   h3h1a2  +   h1h2a3   .  ∂ ∂u3   10.61   ∂ ∂u2  367   VECTOR CALCULUS   cid:1 Prove the expression for ∇ · a in orthogonal curvilinear coordinates. Let us consider the sub-expression ∇ ·  a1 ˆe1 . Now ˆe1 = ˆe2 × ˆe3 = h2∇u2 × h3∇u3. Therefore  ∇ ·  a1 ˆe1  = ∇ ·  a1h2h3∇u2 × ∇u3 ,  = ∇ a1h2h3  ·  ∇u2 × ∇u3  + a1h2h3∇ ·  ∇u2 × ∇u3 .  However, ∇ ·  ∇u2 × ∇u3  = 0, from  10.43 , so we obtain   cid:7    cid:8   ∇ ·  a1 ˆe1  = ∇ a1h2h3  ·  × ˆe3 h3  ˆe2 h2  = ∇ a1h2h3  ·  ˆe1 h2h3  ;  letting Φ = a1h2h3 in  10.60  and substituting into the above equation, we ﬁnd  ∇ ·  a1 ˆe1  =  1  h1h2h3  ∂ ∂u1   a1h2h3 .  Repeating the analysis for ∇·  a2ˆe2  and ∇·  a3ˆe3 , and adding the results we obtain  10.61 , as required.  cid:2   Laplacian  In the expression for the divergence  10.61 , let  where we have used  10.60 . We then obtain  a = ∇Φ =  cid:7   cid:13   1 h1  ∂Φ ∂u1  ˆe1 +  ˆe2 +  ∂Φ ∂u3  ˆe3,  1 h2  ∂Φ ∂u2   cid:7    cid:8   1 h3   cid:8    cid:7   ∇2Φ =  1  h1h2h3  ∂ ∂u1  h2h3 h1  ∂Φ ∂u1  +  ∂ ∂u2  h3h1 h2  ∂Φ ∂u2  +  ∂ ∂u3  h1h2 h3  ∂Φ ∂u3   cid:8  cid:14   ,  which is the expression for the Laplacian in orthogonal curvilinear coordinates.  The curl of a vector ﬁeld a = a1 ˆe1 + a2 ˆe2 + a3 ˆe3 in orthogonal curvilinear coordinates is given by  Curl   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20   h1 ˆe1  h2 ˆe2  h3 ˆe3  ∂ ∂u1 h1a1  ∂ ∂u2 h2a2  ∂ ∂u3 h3a3   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  .  ∇ × a =  1  h1h2h3   10.62    cid:1 Prove the expression for ∇ × a in orthogonal curvilinear coordinates. Let us consider the sub-expression ∇ ×  a1 ˆe1 . Since ˆe1 = h1∇u1 we have  ∇ ×  a1 ˆe1  = ∇ ×  a1h1∇u1 ,  = ∇ a1h1  × ∇u1 + a1h1∇ × ∇u1.  But ∇ × ∇u1 = 0, so we obtain  ∇ ×  a1 ˆe1  = ∇ a1h1  × ˆe1  .  h1  368   10.11 EXERCISES   cid:14    h2h3a1  +   h3h1a2  +   h1h2a3   ∂ ∂u3  ∇Φ = ∇ · a =  ∇ × a =  ∇2Φ =  ˆe1 +  1 h2  ∂ ∂u1   cid:13   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  h1 ˆe1  cid:7   cid:13   ∂ ∂u1 h1a1  1 h1  ∂Φ ∂u1  h1h2h3  h1h2h3  1  1  1  ∂Φ ∂u2  ˆe2 +  1 h3  ∂Φ ∂u3  ˆe3   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20   ∂ ∂u2  h3 ˆe3  ∂ ∂u3 h3a3   cid:8   h2 ˆe2  ∂ ∂u2 h2a2   cid:7    cid:8    cid:7    cid:8  cid:14   ∂ ∂u1  h2h3 h1  ∂Φ ∂u1  +  ∂ ∂u2  h3h1 h2  ∂Φ ∂u2  +  ∂ ∂u3  h1h2 h3  ∂Φ ∂u3  h1h2h3  Table 10.4 Vector operators in orthogonal curvilinear coordinates u1, u2, u3. Φ is a scalar ﬁeld and a is a vector ﬁeld.  Letting Φ = a1h1 in  10.60  and substituting into the above equation, we ﬁnd  ∇ ×  a1 ˆe1  =  ˆe2 h3h1  ∂ ∂u3   a1h1  − ˆe3  ∂ ∂u2  h1h2   a1h1 .  The corresponding analysis of ∇ ×  a2 ˆe2  produces terms in ˆe3 and ˆe1, whilst that of ∇ ×  a3 ˆe3  produces terms in ˆe1 and ˆe2. When the three results are added together, the coeﬃcients multiplying ˆe1, ˆe2 and ˆe3 are the same as those obtained by writing out  10.62  explicitly, thus proving the stated result.  cid:2   The general expressions for the vector operators in orthogonal curvilinear coordinates are shown for reference in table 10.4. The explicit results for cylindrical and spherical polar coordinates, given in tables 10.2 and 10.3 respectively, are obtained by substituting the appropriate set of scale factors in each case.  A discussion of the expressions for vector operators in tensor form, which are valid even for non-orthogonal curvilinear coordinate systems, is given in chapter 26.  10.1  Evaluate the integral   cid:21   cid:18   10.11 Exercises  a ˙b · a + b · ˙a  + ˙a b · a  − 2 ˙a · a b − ˙ba2  dt   cid:19   10.2  in which ˙a, ˙b are the derivatives of a, b with respect to t. At time t = 0, the vectors E and B are given by E = E0 and B = B0, where the unit vectors, E0 and B0 are ﬁxed and orthogonal. The equations of motion are  Find E and B at a general time t, showing that after a long time the directions of E and B have almost interchanged.  = E0 + B × E0, = B0 + E × B0.  dE dt dB dt  369   VECTOR CALCULUS  10.3  The general equation of motion of a  non-relativistic  particle of mass m and charge q when it is placed in a region where there is a magnetic ﬁeld B and an electric ﬁeld E is  m¨r = q E + ˙r × B ;  here r is the position of the particle at time t and ˙r = dr dt, etc. Write this as three separate equations in terms of the Cartesian components of the vectors involved.  For the simple case of crossed uniform ﬁelds E = Ei, B = Bj, in which the particle starts from the origin at t = 0 with ˙r = v0k, ﬁnd the equations of motion and show the following:  if v0 = E B then the particle continues its initial motion;   a   b  if v0 = 0 then the particle follows the space curve given in terms of the  parameter ξ by  x =  mE B2q   1 − cos ξ ,   ξ − sin ξ .  mE B2q  Interpret this curve geometrically and relate ξ to t. Show that the total distance travelled by the particle after time t is given by  y = 0,  z =   cid:21    cid:20  cid:20  cid:20  cid:20 sin  2E B  t  0  Bqt 2m   cid:20  cid:20  cid:20  cid:20  dt   cid:7   .   cid:7   10.4  10.5  Use vector methods to ﬁnd the maximum angle to the horizontal at which a stone may be thrown so as to ensure that it is always moving away from the thrower. If two systems of coordinates with a common origin O are rotating with respect to each other, the measured accelerations diﬀer in the two systems. Denoting by r and r , respectively, the connection between the two is  position vectors in frames OXY Z and OX  Z  Y   cid:7    cid:7    cid:7    cid:7    cid:7  ¨r  = ¨r + ˙ω × r + 2ω × ˙r + ω ×  ω × r ,   cid:7    cid:7    cid:7   where ω is the angular velocity vector of the rotation of OXY Z with respect to  taken as ﬁxed . The third term on the RHS is known as the Coriolis OX acceleration, whilst the ﬁnal term gives rise to a centrifugal force.  Z  Y  ω  = 7.3× 10  Consider the application of this result to the ﬁring of a shell of mass m from a stationary ship on the steadily rotating earth, working to the ﬁrst order in −1 . If the shell is ﬁred with velocity v at time t = 0 and only reaches a height that is small compared with the radius of the earth, show that its acceleration, as recorded on the ship, is given approximately by  −5 rad s  ¨r = g − 2ω ×  v + gt ,  where mg is the weight of the shell measured on the ship’s deck.  The shell is ﬁred at another stationary ship  a distance s away  and v is such  that the shell would have hit its target had there been no Coriolis eﬀect.   a  Show that without the Coriolis eﬀect the time of ﬂight of the shell would  have been τ = −2g · v g2.   b  Show further that when the shell actually hits the sea it is oﬀ-target by  approximately  [ g × ω  · v] gτ + v  −  ω × v τ2 − 1   ω × g τ3.  3  2τ g2   c  Estimate the order of magnitude ∆ of this miss for a shell for which the −1, ﬁring close to its maximum range  v makes an initial speed v is 300 m s angle of π 4 with the vertical  in a northerly direction, whilst the ship is ◦ stationed at latitude 45  North.  370   10.11 EXERCISES   cid:7    cid:8   × d2r ds2  dr ds  · d3r ds3  10.6  Prove that for a space curve r = r s , where s is the arc length measured along the curve from a ﬁxed point, the triple scalar product   a  ds du = 3 the origin; √  at any point on the curve has the value κ2τ, where κ is the curvature and τ the torsion at that point. For the twisted space curve y3 + 27axz − 81a2y = 0, given parametrically by  10.7  show that the following hold:  x = au 3 − u2 , √ 2a 1 + u2 , where s is the distance along the curve measured from  z = au 3 + u2 ,  y = 3au2,   b  the length of the curve from the origin to the Cartesian point  2a, 3a, 4a  is  2a;  4 the radius of curvature at the point with parameter u is 3a 1 + u2 2;   c   d  the torsion τ and curvature κ at a general point are equal;  e  any of the Frenet–Serret formulae that you have not already used directly  are satisﬁed.  10.8  The shape of the curving slip road joining two motorways, that cross at right angles and are at vertical heights z = 0 and z = h, can be approximated by the space curve  √  2h π   cid:9    cid:10   zπ 2h  √ 2h π   cid:9    cid:10   zπ 2h  r =  ln cos  i +  ln sin  j + zk.  10.9  Show that the radius of curvature ρ of the slip road is  2h π  cosec  zπ h  at  height z and that the torsion τ = −1 ρ. To shorten the algebra, set z = 2hθ π  and use θ as the parameter. In a magnetic ﬁeld, ﬁeld lines are curves to which the magnetic induction B is everywhere tangential. By evaluating dB ds, where s is the distance measured along a ﬁeld line, prove that the radius of curvature at any point on a line is given by  ρ =  B3  B ×  B · ∇ B .  10.10  Find the areas of the given surfaces using parametric coordinates.   a  Using the parameterisation x = u cos φ, y = u sin φ, z = u cot Ω, ﬁnd the sloping surface area of a right circular cone of semi-angle Ω whose base has radius a. Verify that it is equal to 1 2  ×perimeter of the base ×slope height.   b  Using the same parameterization as in  a  for x and y, and an appropriate choice for z, ﬁnd the surface area between the planes z = 0 and z = Z of the paraboloid of revolution z = α x2 + y2 .  10.11  Parameterising the hyperboloid  by x = a cos θ sec φ, y = b sin θ sec φ, z = c tan φ, show that an area element on its surface is  dS = sec2 φ  c2 sec2 φ  b2 cos2 θ + a2 sin2 θ  + a2b2 tan2 φ  dθ dφ.   cid:6    cid:19 1 2   cid:18    cid:5   x2 a2  +  y2 b2  − z2 c2  = 1  371   VECTOR CALCULUS  Use this formula to show that the area of the curved surface x2 + y2 − z2 = a2   cid:7   between the planes z = 0 and z = 2a is 1√ 2  πa2  6 +  −1 2  sinh   cid:8   .  √ 2  10.12  For the function  z x, y  =  x2 − y2 e  −x2−y2  ,  ﬁnd the location s  at which the steepest gradient occurs. What are the magnitude and direction of that gradient? The algebra involved is easier if plane polar coordinates are used. Verify by direct calculation that  10.13  ∇ ·  a × b  = b ·  ∇ × a  − a ·  ∇ × b .  10.14  In the following exercises, a, b and c are vector ﬁelds.   a  Simplify  ∇ × a ∇ · a  + a × [∇ ×  ∇ × a ] + a × ∇2a.   b  By explicitly writing out the terms in Cartesian coordinates, prove that  [c ·  b · ∇  − b ·  c · ∇ ] a =  ∇ × a  ·  b × c .   c  Prove that a ×  ∇ × a  = ∇  1  2 a2  −  a · ∇ a.  10.15  Evaluate the Laplacian of the function  ψ x, y, z  =  zx2  x2 + y2 + z2   a  directly in Cartesian coordinates, and  b  after changing to a spherical polar coordinate system. Verify that, as they must, the two methods give the same result. Verify that  10.42  is valid for each component separately when a is the Cartesian vector x2y i + xyz j + z2y k, by showing that each side of the equation is equal to z i +  2x + 2z  j + x k. The  Maxwell  relationship between a time-independent magnetic ﬁeld B and the current density J  measured in SI units in A m  −2  producing it,  10.16  10.17  ∇ × B = µ0J,  can be applied to a long cylinder of conducting ionised gas which, in cylindrical polar coordinates, occupies the region ρ < a.   a  Show that a uniform current density  0, C, 0  and a magnetic ﬁeld  0, 0, B , with B constant  = B0  for ρ > a and B = B ρ  for ρ < a, are consistent with this equation. Given that B 0  = 0 and that B is continuous at ρ = a, obtain expressions for C and B ρ  in terms of B0 and a.   b  The magnetic ﬁeld can be expressed as B = ∇ × A, where A is known as  the vector potential. Show that a suitable A that has only one non-vanishing component, Aφ ρ , can be found, and obtain explicit expressions for Aφ ρ  for both ρ   a. Like B, the vector potential is continuous at ρ = a.   c  The gas pressure p ρ  satisﬁes the hydrostatic equation ∇p = J × B and  vanishes at the outer wall of the cylinder. Find a general expression for p.  10.18  Evaluate the Laplacian of a vector ﬁeld using two diﬀerent coordinate systems as follows.  372   10.11 EXERCISES   a  For cylindrical polar coordinates ρ, φ, z, evaluate the derivatives of the three unit vectors with respect to each of the coordinates, showing that only ∂ˆeρ ∂φ and ∂ˆeφ ∂φ are non-zero.   i  Hence evaluate ∇2a when a is the vector ˆeρ, i.e. a vector of unit magnitude  everywhere directed radially outwards and expressed by aρ = 1, aφ =  ii  Note that it is trivially obvious that ∇ × a = 0 and hence that equation az = 0.  10.41  requires that ∇ ∇ · a  = ∇2a.  iii  Evaluate ∇ ∇ · a  and show that the latter equation holds, but that  [∇ ∇ · a ]ρ  cid:3 = ∇2aρ.   b  Rework the same problem in Cartesian coordinates  where, as it happens,  the algebra is more complicated .  10.19 Maxwell’s equations for electromagnetism in free space  i.e. in the absence of  charges, currents and dielectric or magnetic media  can be written   i  ∇ · B = 0,  iii  ∇ × E +   ii  ∇ · E = 0,  iv  ∇ × B − 1  ∂E ∂t  c2  = 0.  ∂B ∂t  = 0,  A vector A is deﬁned by B = ∇ × A, and a scalar φ by E = −∇φ − ∂A ∂t. Show  that if the condition  is imposed  this is known as choosing the Lorentz gauge , then A and φ satisfy wave equations as follows:   v  ∇ · A +  1 c2  ∂φ ∂t  = 0   vi  ∇2φ − 1  vii  ∇2A − 1  c2  ∂2φ ∂t2 ∂2A ∂t2  = 0,  = 0.  c2 The reader is invited to proceed as follows.   a  Verify that the expressions for B and E in terms of A and φ are consistent  with  i  and  iii .   b  Substitute for E in  ii  and use the derivative with respect to time of  v  to  eliminate A from the resulting expression. Hence obtain  vi .   c  Substitute for B and E in  iv  in terms of A and φ. Then use the gradient of   v  to simplify the resulting equation and so obtain  vii .  10.20  In a description of the ﬂow of a very viscous ﬂuid that uses spherical polar coordinates with axial symmetry, the components of the velocity ﬁeld u are given in terms of the stream function ψ by  Find an explicit expression for the diﬀerential operator E deﬁned by  ur =  1  r2 sin θ  ∂ψ ∂θ  ,  −1  r sin θ  ∂ψ ∂r  .  uθ =  Eψ = − r sin θ  ∇ × u φ.  The stream function satisﬁes the equation of motion E2ψ = 0 and, for the ﬂow of a ﬂuid past a sphere, takes the form ψ r, θ  = f r  sin2 θ. Show that f r  satisﬁes the  ordinary  diﬀerential equation  r4f 4  − 4r2f   cid:7  cid:7    cid:7  − 8f = 0.  + 8rf  373   VECTOR CALCULUS  10.21  Paraboloidal coordinates u, v, φ are deﬁned in terms of Cartesian coordinates by  x = uv cos φ,  y = uv sin φ,  2  u2 − v2 .  z = 1  10.22  Identify the coordinate surfaces in the u, v, φ system. Verify that each coordinate surface  u = constant, say  intersects every coordinate surface on which one of the other two coordinates  v, say  is constant. Show further that the system of coordinates is an orthogonal one and determine its scale factors. Prove that the  u-component of ∇ × a is given by   cid:7    cid:8   1   u2 + v2 1 2  aφ v  +  ∂aφ ∂v  − 1  uv  ∂av ∂φ  .  Non-orthogonal curvilinear coordinates are diﬃcult to work with and should be avoided if at all possible, but the following example is provided to illustrate the content of section 10.10.  In a new coordinate system for the region of space in which the Cartesian  coordinate z satisﬁes z ≥ 0, the position of a point r is given by  α1, α2, R , where coordinate axes of a Cartesian system and R = r. The ranges are −1 ≤ αi ≤ 1, 0 ≤ R < ∞.  α1 and α2 are respectively the cosines of the angles made by r with the x- and y-   a  Express r in terms of α1, α2, R and the unit Cartesian vectors i, j, k.  b  Obtain expressions for the vectors ei  = ∂r ∂α1, . . .   and hence show that the  scale factors hi are given by R 1 − α2 2 1 2 − α2  1 − α2 2 1 2 ,  R 1 − α2 1 1 2 − α2  1 − α2 2 1 2 ,  c  Verify formally that the system is not an orthogonal one.  d  Show that the volume element of the coordinate system is  h2 =  h1 =  1  1  h3 = 1.  and demonstrate that this is always less than or equal to the corresponding expression for an orthogonal curvilinear system.   e  Calculate the expression for  ds 2 for the system, and show that it diﬀers  from that for the corresponding orthogonal system by  dV =  R2 dα1 dα2 dR  1 − α2 − α2 2 1 2 ,  1  2α1α2R2 − α2 1 − α2  1  2  dα1dα2.  10.23  Hyperbolic coordinates u, v, φ are deﬁned in terms of Cartesian coordinates by  x = cosh u cos v cos φ,  y = cosh u cos v sin φ,  z = sinh u sin v.  Sketch the coordinate curves in the φ = 0 plane, showing that far from the origin they become concentric circles and radial lines. In particular, identify the curves u = 0, v = 0, v = π 2 and v = π. Calculate the tangent vectors at a general point, show that they are mutually orthogonal and deduce that the appropriate scale factors are  hu = hv =  cosh2 u − cos2 v 1 2,  hφ = cosh u cos v.  Find the most general function ψ u  of u only that satisﬁes Laplace’s equation  ∇2ψ = 0. In a Cartesian system, A and B are the points  0, 0,−1  and  0, 0, 1  respectively. 2  r1 − r2 , u3 = φ; here r1 and r2 are the distances AP and In a new coordinate system a general point P is given by  u1, u2, u3  with u1 = 1 BP and φ is the angle between the plane ABP and y = 0.  2  r1 + r2 , u2 = 1  10.24  374   10.12 HINTS AND ANSWERS   a  Express z and the perpendicular distance ρ from P to the z-axis in terms of  u1, u2, u3.   b  Evaluate ∂x ∂ui, ∂y ∂ui, ∂z ∂ui, for i = 1, 2, 3.  c  Find the Cartesian components of ˆuj and hence show that the new coordi- nates are mutually orthogonal. Evaluate the scale factors and the inﬁnitesimal volume element in the new coordinate system.   d  Determine and sketch the forms of the surfaces ui = constant.   e  Find the most general function f of u1 only that satisﬁes ∇2f = 0.  10.12 Hints and answers  Group the term so that they form the total derivatives of compound vector  expressions. The integral has the value a ×  a × b  + h. For crossed uniform ﬁelds, ¨x+ Bq m 2x = q E− Bv0  m, ¨y = 0, m˙z = qBx+ mv0;  b  ξ = Bqt m; the path is a cycloid in the plane y = 0; ds = [ dx dt 2 +  dz dt 2]1 2 dt. g = ¨r  cid:7  ﬁxed in space. To ﬁrst order in ω, the direction of g is radial, i.e. parallel to ¨r   cid:7  − ω ×  ω × r , where ¨r  cid:7   is the shell’s acceleration measured by an observer  .   a  Note that s is orthogonal to g.   b  If the actual time of ﬂight is T , use  s + ∆  · g = 0 to show that  T ≈ τ 1 + 2g  −2 g × ω  · v + ···  .  to the East.  In the Coriolis terms, it is suﬃcient to put T ≈ τ.  √ 2 1 + u2 ]   c  For this situation  g × ω  · v = 0 and ω × v = 0; τ ≈ 43 s and ∆ = 10–15 m  a  Evaluate  dr du  ·  dr du .  b  Integrate the previous result between u = 0 and u = 1.  c   ˆt = [ −1 = dˆt ds. ρ ˆn =  1 + u2  Use dˆb ds =  dˆb du   ds du  and show that this equals −[3a 1 + u2 2]  −1[ 1 − u2 i + 2uj +  1 + u2 k]. Use dˆt ds =  dˆt du   ds du ; √ −1[−2ui +  1− u2 j]. ˆb = [ −1[ u2 − 1 i− 2uj +  1 + u2 k]. 2 1 + u2 ] −1 ˆn. √ −1[ 1 − u2 i + 2uj].  e  Show that dˆn ds = τ ˆb − ˆt  = −2[3 2a 1 + u2 3] Note that dB =  dr · ∇ B and that B = Bˆt, with ˆt = dr ds. Obtain  B · ∇ B B = ˆt dB ds  + ˆn B ρ  and then take the vector product of ˆt with this equation. To integrate sec2 φ sec2 φ + tan2 φ 1 2 dφ put tan φ = 2  −1 2 sinh ψ.   d   −1 cos θ  1−5 sin2 θ cos2 φ ;  −3[ y2+z2  y2+z2−3x2 −4x4];  b  2r  divergence on the LHS.  a  2z x2+y2+z2  both are equal to 2zr Use the formulae given in table 10.2.  −4 r2 − 5x2 .  a  C = −B0  µ0a ; B ρ  = B0ρ a.  b  B0ρ2  3a  for ρ   a. Recall that ∇ × ∇φ = 0 for any scalar φ and that ∂ ∂t and ∇ act on diﬀerent  0   2µ0 ][1 −  ρ a 2].  [B2   c   variables. Two sets of paraboloids of revolution about the z-axis and the sheaf of planes  containing the z-axis. For constant u,−∞ < z < u2 2; for constant v,−v2 2 < z < ∞. The scale factors are hu = hv =  u2 + v2 1 2, hφ = uv.  375  10.1  10.3  10.5  10.7  10.9  10.15  10.17  10.19  10.21  10.11 10.13 Work in Cartesian coordinates, regrouping the terms obtained by evaluating the   VECTOR CALCULUS  10.23  The tangent vectors are as follows: for u = 0, the line joining  1, 0, 0  and   −1, 0, 0 ; for v = 0, the line joining  1, 0, 0  and  ∞, 0, 0 ; for v = π 2, the line  0, 0, z ; for v = π, the line joining  −1, 0, 0  and  −∞, 0, 0 . −1 eu + c, derived from ∂[cosh u ∂ψ ∂u ] ∂u = 0.  ψ u  = 2 tan  376   11  Line, surface and volume integrals  In the previous chapter we encountered continuously varying scalar and vector ﬁelds and discussed the action of various diﬀerential operators on them. In addition to these diﬀerential operations, the need often arises to consider the integration of ﬁeld quantities along lines, over surfaces and throughout volumes. In general the integrand may be scalar or vector in nature, but the evaluation of such integrals involves their reduction to one or more scalar integrals, which are then evaluated. In the case of surface and volume integrals this requires the evaluation of double and triple integrals  see chapter 6 .  11.1 Line integrals  In this section we discuss line or path integrals, in which some quantity related to the ﬁeld is integrated between two given points in space, A and B, along a prescribed curve C that joins them. In general, we may encounter line integrals of the forms   cid:21    cid:21    cid:21   φ dr,  C  a · dr,  a × dr,  C  C   11.1   where φ is a scalar ﬁeld and a is a vector ﬁeld. The three integrals themselves are respectively vector, scalar and vector in nature. As we will see below, in physical applications line integrals of the second type are by far the most common.  The formal deﬁnition of a line integral closely follows that of ordinary integrals and can be considered as the limit of a sum. We may divide the path C joining the points A and B into N small line elements ∆rp, p = 1, . . . , N. If  xp, yp, zp  is any point on the line element ∆rp then the second type of line integral in  11.1 ,  for example, is deﬁned as cid:21   a · dr = lim N→∞  C  a xp, yp, zp  · ∆rp,  where it is assumed that all ∆rp → 0 as N → ∞.  N cid:4   p=1  377   LINE, SURFACE AND VOLUME INTEGRALS     Each of the line integrals in  11.1  is evaluated over some curve C that may be either open  A and B being distinct points  or closed  the curve C forms a loop, so that A and B are coincident . In the case where C is closed, the line integral C to indicate this. The curve may be given either parametrically by is written r u  = x u i + y u j + z u k or by means of simultaneous equations relating x, y, z for the given path  in Cartesian coordinates . A full discussion of the diﬀerent representations of space curves was given in section 10.3.  In general, the value of the line integral depends not only on the end-points A and B but also on the path C joining them. For a closed curve we must also specify the direction around the loop in which the integral is taken. It is usually taken to be such that a person walking around the loop C in this direction always has the region R on his her left; this is equivalent to traversing C in the anticlockwise direction  as viewed from above .  11.1.1 Evaluating line integrals  The method of evaluating a line integral is to reduce it to a set of scalar integrals. It is usual to work in Cartesian coordinates, in which case dr = dx i + dy j + dz k. The ﬁrst type of line integral in  11.1  then becomes simply   cid:21    cid:21   φ dr = i  φ x, y, z  dx + j  φ x, y, z  dy + k  φ x, y, z  dz.  C  C  The three integrals on the RHS are ordinary scalar integrals that can be evaluated in the usual way once the path of integration C has been speciﬁed. Note that in the above we have used relations of the form   cid:21    cid:21    cid:21   C   cid:21   C  φ i dx = i  φ dx,  which is allowable since the Cartesian unit vectors are of constant magnitude and direction and hence may be taken out of the integral. If we had been using a diﬀerent coordinate system, such as spherical polars, then, as we saw in the previous chapter, the unit basis vectors would not be constant. In that case the basis vectors could not be factorised out of the integral.  The second and third line integrals in  11.1  can also be reduced to a set of scalar integrals by writing the vector ﬁeld a in terms of its Cartesian components as a = axi + ayj + azk, where ax, ay, az are each  in general  functions of x, y, z. The second line integral in  11.1 , for example, can then be written as  a · dr =   axi + ayj + azk  ·  dx i + dy j + dz k    cid:21   C   cid:21   cid:21   cid:21   C  C  C  =  =   ax dx + ay dy + az dz    cid:21   C   cid:21   C  378  ax dx +  ay dy +  az dz.   11.2    11.1 LINE INTEGRALS  A similar procedure may be followed for the third type of line integral in  11.1 , which involves a cross product.  Line integrals have properties that are analogous to those of ordinary integrals. In particular, the following are useful properties  which we illustrate using the second form of line integral in  11.1  but which are valid for all three types .   i  Reversing the path of integration changes the sign of the integral. If the path C along which the line integrals are evaluated has A and B as its end-points then   cid:21    cid:21   B  a · dr = −  A  a · dr.  A  B   cid:21   A   cid:1   This implies that if the path C is a loop then integrating around the loop in the opposite direction changes the sign of the integral.   ii  If the path of integration is subdivided into smaller segments then the sum of the separate line integrals along each segment is equal to the line integral along the whole path. So, if P is any point on the path of integration that lies between the path’s end-points A and B then   cid:21    cid:21   B  a · dr =  P  a · dr +  B  a · dr.  A  P   cid:1 Evaluate the line integral I = paths in the xy-plane shown in ﬁgure 11.1, namely  C a · dr, where a =  x + y i +  y − x j, along each of the   i  the parabola y2 = x from  1, 1  to  4, 2 ,  ii  the curve x = 2u2 + u + 1, y = 1 + u2 from  1, 1  to  4, 2 ,  iii  the line y = 1 from  1, 1  to  4, 1 , followed by the line x = 4 from  4, 1   to  4, 2 .   cid:21   I =   1,1   Since each of the paths lies entirely in the xy-plane, we have dr = dx i + dy j. We can therefore write the line integral as  a · dr =  [ x + y  dx +  y − x  dy].  I =  C  C   11.3    cid:21    cid:21   We must now evaluate this line integral along each of the prescribed paths.  Case  i . Along the parabola y2 = x we have 2y dy = dx. Substituting for x in  11.3   and using just the limits on y, we obtain   4,2   [ x + y  dx +  y − x  dy] =  2  [ y2 + y 2y +  y − y2 ] dy = 11 1 3 .  Note that we could just as easily have substituted for y and obtained an integral in x, which would have given the same result.  Case  ii . The second path is given in terms of a parameter u. We could eliminate u between the two equations to obtain a relationship between x and y directly and proceed as above, but it is usually quicker to write the line integral in terms of the parameter u. Along the curve x = 2u2 + u + 1, y = 1 + u2 we have dx =  4u + 1  du and dy = 2u du.   cid:21   1  379   y   cid:21   cid:21   I =  =  0   cid:21    cid:21    1,1    4,1   LINE, SURFACE AND VOLUME INTEGRALS   i    4, 2    ii    iii    1, 1   x  Figure 11.1 Diﬀerent possible paths between the points  1, 1  and  4, 2 .  Substituting for x and y in  11.3  and writing the correct limits on u, we obtain   4,2   [ x + y  dx +  y − x  dy] [ 3u2 + u + 2  4u + 1  −  u2 + u 2u] du = 10 2 3 .   1,1   1  4  1   cid:21    cid:21   1  Case  iii . For the third path the line integral must be evaluated along the two line segments separately and the results added together. First, along the line y = 1 we have dy = 0. Substituting this into  11.3  and using just the limits on x for this segment, we obtain   4,1   [ x + y  dx +  y − x  dy] =   x + 1  dx = 10 1 2 .  Next, along the line x = 4 we have dx = 0. Substituting this into  11.3  and using just the limits on y for this segment, we obtain   4,2   [ x + y  dx +  y − x  dy] =  2   y − 4  dy = −2 1 2 .  The value of the line integral along the whole path is just the sum of the values of the line integrals along each segment, and is given by I = 10 1 2  2 = 8.  cid:2   − 2 1  When calculating a line integral along some curve C, which is given in terms of x, y and z, we are sometimes faced with the problem that the curve C is such that x, y and z are not single-valued functions of one another over the entire length of the curve. This is a particular problem for closed loops in the xy-plane  and also for some open curves . In such cases the path may be subdivided into shorter line segments along which one coordinate is a single-valued function of the other two. The sum of the line integrals along these segments is then equal to the line integral along the entire curve C. A better solution, however, is to represent the curve in a parametric form r u  that is valid for its entire length.  380   11.1 LINE INTEGRALS     0   cid:1 Evaluate the line integral I = x2 + y2 = a2, z = 0.  C x dy, where C is the circle in the xy-plane deﬁned by  the left of x = 0. The required line integral is then the sum of the integrals along the two semicircles. Substituting for x, it is given by  Adopting the usual convention mentioned above, the circle C is to be traversed in the anticlockwise direction. Taking the circle as a whole means x is not a single-valued function of y. We must therefore divide the path into two parts with x = +  the semicircle lying to the right of x = 0, and x = − cid:24  a2 − y2 for a2 − y2 for the semicircle lying to  cid:21  −a  cid:9   cid:24  a2 − y2 dy +  cid:24  a2 − y2 dy = πa2.  − cid:24   a2 − y2  x dy =   cid:10    cid:21    cid:21   0  I =  = 4  −a  dy  C  a  a  a   cid:24   Alternatively, we can represent the entire circle parametrically, in terms of the azimuthal angle φ, so that x = a cos φ and y = a sin φ with φ running from 0 to 2π. The integral can therefore be evaluated over the whole circle at once. Noting that dy = a cos φ dφ, we can rewrite the line integral completely in terms of the parameter φ and obtain  0  C   cid:21   0  I =  x dy = a2  2π  cos2 φ dφ = πa2.  cid:2   11.1.2 Physical examples of line integrals  There are many physical examples of line integrals, but perhaps the most common is the expression for the total work done by a force F when it moves its point of application from a point A to a point B along a given curve C. We allow the magnitude and direction of F to vary along the curve. Let the force act at a point r and consider a small displacement dr along the curve; then the small amount of work done is dW = F · dr, as discussed in subsection 7.6.1  note that dW can  be either positive or negative . Therefore, the total work done in traversing the path C is   cid:21   Naturally, other physical quantities can be expressed in such a way. For example, the electrostatic potential energy gained by moving a charge q along a path C in   cid:1  C E· dr. We may also note that Amp`ere’s law concerning  an electric ﬁeld E is −q  the magnetic ﬁeld B associated with a current-carrying wire can be written as  C  where I is the current enclosed by a closed path C traversed in a right-handed sense with respect to the current direction.  Magnetostatics also provides a physical example of the third type of line  WC =  F · dr.  0  C  B · dr = µ0I,  381   LINE, SURFACE AND VOLUME INTEGRALS  integral in  11.1 . If a loop of wire C carrying a current I is placed in a magnetic  ﬁeld B then the force dF on a small length dr of the wire is given by dF = I dr× B,  and so the total  vector  force on the loop is  0  C  F = I  dr × B.  11.1.3 Line integrals with respect to a scalar  In addition to those listed in  11.1 , we can form other types of line integral, which depend on a particular curve C but for which we integrate with respect to a scalar du, rather than the vector diﬀerential dr. This distinction is somewhat arbitrary, however, since we can always rewrite line integrals containing the vector diﬀerential dr as a line integral with respect to some scalar parameter. If the path C along which the integral is taken is described parametrically by r u  then  and the second type of line integral in  11.1 , for example, can be written as  A similar procedure can be followed for the other types of line integral in  11.1 . Commonly occurring special cases of line integrals with respect to a scalar are  where s is the arc length along the curve C. We can always represent C paramet- rically by r u , and from section 10.3 we have · dr du  dr du  ds =  du.     The line integrals can therefore be expressed entirely in terms of the parameter u and thence evaluated.  cid:1 Evaluate the line integral I = from A =  a, 0  to B =  −a, 0  and for which y ≥ 0.  C  x− y 2 ds, where C is the semicircle of radius a running   cid:1   The semicircular path from A to B can be described in terms of the azimuthal angle φ  measured from the x-axis  by  where φ runs from 0 to π. Therefore the element of arc length is given, from section 10.3, by  r φ  = a cos φ i + a sin φ j,   cid:25   dr dφ  · dr dφ  382  ds =  dφ = a cos2 φ + sin2 φ  dφ = a dφ.  dr =  du,  dr du   cid:21   C  a · dr =   cid:21   C   cid:21   du.  du  a · dr  cid:21   φ ds,  C  a ds,  C   11.2 CONNECTIVITY OF REGIONS   a    b    c   Figure 11.2  a  A simply connected region;  b  a doubly connected region;  c  a triply connected region.  Since  x − y 2 = a2 1 − sin 2φ , the line integral becomes   cid:21   I =  C   cid:21   0   x − y 2 ds =  π  a3 1 − sin 2φ  dφ = πa3.  cid:2   As discussed in the previous chapter, the expression  10.58  for the square of the element of arc length in three-dimensional orthogonal curvilinear coordinates u1, u2, u3 is   ds 2 = h2  1  du1 2 + h2  2  du2 2 + h2  3  du3 2,  where h1, h2, h3 are the scale factors of the coordinate system. If a curve C in three dimensions is given parametrically by the equations ui = ui λ  for i = 1, 2, 3 then the element of arc length along the curve is   cid:25    cid:7    cid:8   2   cid:7    cid:8   2   cid:7    cid:8   2  ds =  h2 1  du1 dλ  + h2 2  du2 dλ  + h2 3  du3 dλ  dλ.  11.2 Connectivity of regions  In physical systems it is usual to deﬁne a scalar or vector ﬁeld in some region R. In the next and some later sections we will need the concept of the connectivity of such a region in both two and three dimensions.  We begin by discussing planar regions. A plane region R is said to be simply connected if every simple closed curve within R can be continuously shrunk to a point without leaving the region  see ﬁgure 11.2 a  . If, however, the region R contains a hole then there exist simple closed curves that cannot be shrunk to a point without leaving R  see ﬁgure 11.2 b  . Such a region is said to be doubly connected, since its boundary has two distinct parts. Similarly, a region  with n − 1 holes is said to be n-fold connected, or multiply connected  the region  in ﬁgure 11.2 c  is triply connected .  383   LINE, SURFACE AND VOLUME INTEGRALS  y  d  c  S  a  V  R  U  C  T  b  x  Figure 11.3 A simply connected region R bounded by the curve C.  These ideas can be extended to regions that are not planar, such as general three-dimensional surfaces and volumes. The same criteria concerning the shrink- ing of closed curves to a point also apply when deciding the connectivity of such regions. In these cases, however, the curves must lie in the surface or volume in question. For example, the interior of a torus is not simply connected, since there exist closed curves in the interior that cannot be shrunk to a point without leaving the torus. The region between two concentric spheres of diﬀerent radii is simply connected.  11.3 Green’s theorem in a plane  In subsection 11.1.1 we considered  amongst other things  the evaluation of line integrals for which the path C is closed and lies entirely in the xy-plane. Since the path is closed it will enclose a region R of the plane. We now discuss how to express the line integral around the loop as a double integral over the enclosed region R.  Suppose the functions P  x, y , Q x, y  and their partial derivatives are single- valued, ﬁnite and continuous inside and on the boundary C of some simply connected region R in the xy-plane. Green’s theorem in a plane  sometimes called the divergence theorem in two dimensions  then states − ∂P   cid:21  cid:21    cid:7    cid:8   0   P dx + Q dy  =   11.4   dx dy,  ∂Q ∂x  R  ∂y  C  and so relates the line integral around C to a double integral over the enclosed region R. This theorem may be proved straightforwardly in the following way. Consider the simply connected region R in ﬁgure 11.3, and let y = y1 x  and  384   11.3 GREEN’S THEOREM IN A PLANE  y = y2 x  be the equations of the curves S T U and S V U respectively. We then  write  cid:21  cid:21   ∂P ∂y  R  dx dy =   cid:21   cid:21   b  a  a  b   cid:21    cid:21    cid:21   b  dy  dx  y2 x    cid:22   cid:22   cid:23  P  x, y2 x   − P  x, y1 x   P  x, y1 x   dx −  ∂P ∂y   cid:21   y1 x   dx  =  a  a  b  =  = −  a  b   cid:23   P  x, y   y=y2 x   y=y1 x   dx  P  x, y2 x   dx = −  cid:21    cid:23   0  P dx.  C  If we now let x = x1 y  and x = x2 y  be the equations of the curves T S V and T UV respectively, we can similarly show that   cid:21  cid:21   ∂Q ∂x  R  dx dy =   cid:21   x2 y    cid:22   cid:23  Q x2 y , y  − Q x1 y , y   ∂Q ∂x  x1 y   dx  dy  =  d  c  dy   cid:22   dy  Q x, y   x=x2 y   x=x1 y    cid:21   d  c  0  C  Q x1, y  dy +  Q x2, y  dy =  Q dy.   cid:21   cid:21   cid:21   d  d  c  c  c  d  =  =  Subtracting these two results gives Green’s theorem in a plane.   cid:1 Show that the area of a region R enclosed by a simple closed curve C is given by A = C y dx. Hence calculate the area of the ellipse x = a cos φ,     C x dy = −      1 2 y = b sin φ.  C  x dy−y dx  = 0  In Green’s theorem  11.4  put P = −y and Q = x; then   cid:21  cid:21   Therefore the area of the region is A = 1 2 and Q = x and obtain A =  The area of the ellipse x = a cos φ, y = b sin φ is given by   x dy − y dx  =  C  dx dy = 2A.   1 + 1  dx dy = 2  R  R     C x dy, or put P = −y and Q = 0, which gives A = −   C  x dy− y dx . Alternatively, we could put P = 0  cid:21  C y dx.  cid:21   ab cos2 φ + sin2 φ  dφ   x dy − y dx  =  1 2  2π  0  0  A =  1 2  C   cid:21  cid:21      =  ab 2  0  2π  dφ = πab.  cid:2   It may further be shown that Green’s theorem in a plane is also valid for multiply connected regions. In this case, the line integral must be taken over all the distinct boundaries of the region. Furthermore, each boundary must be traversed in the positive direction, so that a person travelling along it in this direction always has the region R on their left. In order to apply Green’s theorem  385   LINE, SURFACE AND VOLUME INTEGRALS  y  R  C2  C1  x  Figure 11.4 A doubly connected region R bounded by the curves C1 and C2.  to the region R shown in ﬁgure 11.4, the line integrals must be taken over both boundaries, C1 and C2, in the directions indicated, and the results added together.  We may also use Green’s theorem in a plane to investigate the path indepen- dence  or not  of line integrals when the paths lie in the xy-plane. Let us consider the line integral   cid:21   B  A  I =   P dx + Q dy .  For the line integral from A to B to be independent of the path taken, it must have the same value along any two arbitrary paths C1 and C2 joining the points.  Moreover, if we consider as the path the closed loop C formed by C1 − C2 then  the line integral around this loop must be zero. From Green’s theorem in a plane,  11.4 , we see that a suﬃcient condition for I = 0 is that  ∂P ∂y  =  ∂Q ∂x  ,   11.5   throughout some simply connected region R containing the loop, where we assume that these partial derivatives are continuous in R.     It may be shown that  11.5  is also a necessary condition for I = 0 and is equivalent to requiring P dx + Q dy to be an exact diﬀerential of some function A  P dx + Q dy  = φ B − φ A  φ x, y  such that P dx + Q dy = dφ. It follows that and that C  P dx + Q dy  around any closed loop C in the region R is identically zero. These results are special cases of the general results for paths in three dimensions, which are discussed in the next section.  B   cid:1   386   11.4 CONSERVATIVE FIELDS AND POTENTIALS   cid:1 Evaluate the line integral  0  C  around the ellipse x2 a2 + y2 b2 = 1.  I =  [ exy + cos x sin y  dx +  ex + sin x cos y  dy] ,  Clearly, it is not straightforward to calculate this line integral directly. However, if we let  P = exy + cos x sin y  and  Q = ex + sin x cos y,  then ∂P  ∂y = ex + cos x cos y = ∂Q ∂x, and so P dx + Q dy is an exact diﬀerential  it is actually the diﬀerential of the function f x, y  = exy + sin x sin y . From the above discussion, we can conclude immediately that I = 0.  cid:2   11.4 Conservative ﬁelds and potentials  So far we have made the point that, in general, the value of a line integral between two points A and B depends on the path C taken from A to B. In the previous section, however, we saw that, for paths in the xy-plane, line integrals whose integrands have certain properties are independent of the path taken. We now extend that discussion to the full three-dimensional case.   cid:1  C a · dr, there exists a class of vector ﬁelds for  For line integrals of the form  which the line integral between two points is independent of the path taken. Such vector ﬁelds are called conservative. A vector ﬁeld a that has continuous partial derivatives in a simply connected region R is conservative if, and only if, any of the following is true.   cid:1  A a· dr, where A and B lie in the region R, is independent of C a · dr around any closed loop   i  The integral  the path from A to B. Hence the integral in R is zero.   ii  There exists a single-valued function φ of position such that a = ∇φ.  iii  ∇ × a = 0.  iv  a · dr is an exact diﬀerential.     B  The validity or otherwise of any of these statements implies the same for the other three, as we will now show.  First, let us assume that  i  above is true. If the line integral from A to B is independent of the path taken between the points then its value must be a function only of the positions of A and B. We may therefore write  B  a · dr = φ B  − φ A ,   11.6    cid:21   A  which deﬁnes a single-valued scalar function of position φ. If the points A and B are separated by an inﬁnitesimal displacement dr then  11.6  becomes  a · dr = dφ,  387   LINE, SURFACE AND VOLUME INTEGRALS  which shows that we require a· dr to be an exact diﬀerential: condition  iv . From  10.27  we can write dφ = ∇φ · dr, and so we have  a − ∇φ  · dr = 0.  Since dr is arbitrary, we ﬁnd that a = ∇φ; this immediately implies ∇ × a = 0,  condition  iii   see  10.37  .  0  0  Alternatively, if we suppose that there exists a single-valued function of position  φ such that a = ∇φ then ∇ × a = 0 follows as before. The line integral around a closed loop then becomes0  Since we deﬁned φ to be single-valued, this integral is zero as required.  a · dr =   Now suppose ∇ × a = 0. From Stoke’s theorem, which is discussed in sec- C a· dr = 0; then a = ∇φ and a· dr = dφ follow Finally, let us suppose a · dr = dφ. Then immediately we have a = ∇φ, and the  tion 11.9, we immediately obtain as above.  ∇φ · dr =  dφ.  C  C  other results follow as above.   cid:1 Evaluate the line integral I = point  c, c, h  and B is the point  2c, c 2, h , along the diﬀerent paths  A a · dr, where a =  xy2 + z i +  x2y + 2 j + xk, A is the  B   i  C1, given by x = cu, y = c u, z = h,   ii  C2, given by 2y = 3c − x, z = h.  Show that the vector ﬁeld a is in fact conservative, and ﬁnd φ such that a = ∇φ.  Expanding out the integrand, we have   cid:21    2c, c 2, h   I =   c, c, h    xy2 + z  dx +  x2y + 2  dy + x dz  ,   11.7   which we must evaluate along each of the paths C1 and C2.   i  Along C1 we have dx = c du, dy = − c u2  du, dz = 0, and on substituting in  11.7   and ﬁnding the limits on u, we obtain   cid:21    cid:7    cid:8   I =  2  c  1  h − 2  u2  du = c h − 1 .   ii  Along C2 we have 2 dy = −dx, dz = 0 and, on substituting in  11.7  and using the  limits on x, we obtain   cid:6   2 x3 − 9  1  4 cx2 + 9  4 c2x + h − 1  dx = c h − 1 .   cid:21    cid:5   I =  2c  c  Hence the line integral has the same value along paths C1 and C2. Taking the curl of a,  we have  ∇ × a =  0 − 0 i +  1 − 1 j +  2xy − 2xy k = 0,  so a is a conservative vector ﬁeld, and the line integral between two points must be   cid:1    cid:18    cid:19   388   11.5 SURFACE INTEGRALS  independent of the path taken. Since a is conservative, we can write a = ∇φ. Therefore, φ  must satisfy  which implies that φ = 1  2 x2y2 + zx + f y, z  for some function f. Secondly, we require  ∂φ ∂x  = xy2 + z,  ∂φ ∂y  = x2y +  = x2y + 2,  ∂f ∂y  ∂φ ∂z  = x +  = x,  ∂g ∂z  which implies f = 2y + g z . Finally, since  we have g = constant = k. It can be seen that we have explicitly constructed the function φ = 1  2 x2y2 + zx + 2y + k.  cid:2   The quantity φ that ﬁgures so prominently in this section is called the scalar  potential function of the conservative vector ﬁeld a  which satisﬁes ∇× a = 0 , and  is unique up to an arbitrary additive constant. Scalar potentials that are multi- valued functions of position  but in simple ways  are also of value in describing some physical situations, the most obvious example being the scalar magnetic potential associated with a current-carrying wire. When the integral of a ﬁeld quantity around a closed loop is considered, provided the loop does not enclose a net current, the potential is single-valued and all the above results still hold. If the loop does enclose a net current, however, our analysis is no longer valid and extra care must be taken.  is solenoidal  then it is both possible and useful, for example in the theory of  If, instead of being conservative, a vector ﬁeld b satisﬁes ∇ · b = 0  i.e. b electromagnetism, to deﬁne a vector ﬁeld a such that b = ∇× a. It may be shown that such a vector ﬁeld a always exists. Further, if a is one such vector ﬁeld then = a +∇ψ + c, where ψ is any scalar function and c is any constant vector, also  cid:7  a satisﬁes the above relationship, i.e. b = ∇ × a  cid:7   . This was discussed more fully in  subsection 10.8.2.  11.5 Surface integrals  As with line integrals, integrals over surfaces can involve vector and scalar ﬁelds and, equally, can result in either a vector or a scalar. The simplest case involves  entirely scalars and is of the form  cid:21   As analogues of the line integrals listed in  11.1 , we may also encounter surface integrals involving vectors, namely   cid:21   φ dS,  S   cid:21   S  φ dS .  S  a · dS,  389   cid:21   S  a × dS.   11.8    11.9    LINE, SURFACE AND VOLUME INTEGRALS  S  dS  dS  S  V   a   C   b    cid:1   Figure 11.5  a  A closed surface and  b  an open surface. In each case a normal to the surface is shown: dS = ˆn dS .     All the above integrals are taken over some surface S , which may be either open or closed, and are therefore, in general, double integrals. Following the S is replaced notation for line integrals, for surface integrals over a closed surface by  S .  The vector diﬀerential dS in  11.9  represents a vector area element of the surface S . It may also be written dS = ˆn dS , where ˆn is a unit normal to the surface at the position of the element and dS is the scalar area of the element used in  11.8 . The convention for the direction of the normal ˆn to a surface depends on whether the surface is open or closed. A closed surface, see ﬁgure 11.5 a , does not have to be simply connected  for example, the surface of a torus is not , but it does have to enclose a volume V , which may be of inﬁnite extent. The direction of ˆn is taken to point outwards from the enclosed volume as shown. An open surface, see ﬁgure 11.5 b , spans some perimeter curve C. The direction of ˆn is then given by the right-hand sense with respect to the direction in which the perimeter is traversed, i.e. follows the right-hand screw rule discussed in subsection 7.6.2. An open surface does not have to be simply connected but for our purposes it must be two-sided  a M¨obius strip is an example of a one-sided surface .  The formal deﬁnition of a surface integral is very similar to that of a line integral. We divide the surface S into N elements of area ∆Sp, p = 1, 2, . . . , N, each with a unit normal ˆnp. If  xp, yp, zp  is any point in ∆Sp then the second type of surface integral in  11.9 , for example, is deﬁned as   cid:21   a · dS = lim N→∞  S  a xp, yp, zp  · ˆnp∆Sp,  where it is required that all ∆Sp → 0 as N → ∞.  N cid:4   p=1  390   11.5 SURFACE INTEGRALS  z  k  α  dS  S  y  R  dA  x  Figure 11.6 A surface S  or part thereof  projected onto a region R in the xy-plane; dS is a surface element.  11.5.1 Evaluating surface integrals  We now consider how to evaluate surface integrals over some general surface. This involves writing the scalar area element dS in terms of the coordinate diﬀerentials of our chosen coordinate system. In some particularly simple cases this is very straightforward. For example, if S is the surface of a sphere of radius a  or some part thereof  then using spherical polar coordinates θ, φ on the sphere we have dS = a2 sin θ dθ dφ. For a general surface, however, it is not usually possible to represent the surface in a simple way in any particular coordinate system. In such cases, it is usual to work in Cartesian coordinates and consider the projections of the surface onto the coordinate planes.  Consider a surface  or part of a surface  S as in ﬁgure 11.6. The surface S is projected onto a region R of the xy-plane, so that an element of surface area dS  projects onto the area element dA. From the ﬁgure, we see that dA =  cos α dS ,  where α is the angle between the unit vector k in the z-direction and the unit normal ˆn to the surface at P . So, at any given point of S , we have simply  dS =  dA cos α =  dAˆn · k .  Now, if the surface S is given by the equation f x, y, z  = 0 then, as shown in sub-  section 10.7.1, the unit normal at any point of the surface is given by ˆn = ∇f ∇f  evaluated at that point, cf.  10.32 . The scalar element of surface area then becomes  dS =  dAˆn · k =  ∇f dA ∇f · k  =  ∇f dA  ∂f ∂z  ,  391   11.10    LINE, SURFACE AND VOLUME INTEGRALS  where ∇f and ∂f ∂z are evaluated on the surface S . We can therefore express  any surface integral over S as a double integral over the region R in the xy-plane. S a · dS, where a = xi and S is the surface of the   cid:1   cid:1 Evaluate the surface integral I = hemisphere x2 + y2 + z2 = a2 with z ≥ 0.  The surface of the hemisphere is shown in ﬁgure 11.7. In this case dS may be easily expressed in spherical polar coordinates as dS = a2 sin θ dθ dφ, and the unit normal to the surface at any point is simply ˆr. On the surface of the hemisphere we have x = a sin θ cos φ and so  a · dS = x  i · ˆr  dS =  a sin θ cos φ  sin θ cos φ  a2 sin θ dθ dφ .  Therefore, inserting the correct limits on θ and φ, we have   cid:21    cid:21   I =  S  a · dS = a3  π 2  0  dθ sin3 θ  dφ cos2 φ =  2πa3  .  3  We could, however, follow the general prescription above and project the hemisphere S onto the region R in the xy-plane that is a circle of radius a centred at the origin. Writing  the equation of the surface of the hemisphere as f x, y  = x2 + y2 + z2 − a2 = 0 and using   11.10 , we have  I =  x  i · ˆr  dS =  x  i · ˆr   ∇f dA  ∂f ∂z  .   cid:21   2π  0   cid:21   R  Now ∇f = 2xi + 2yj + 2zk = 2r, so on the surface S we have ∇f = 2r = 2a. On S we a2 − x2 − y2 and i · ˆr = x a. Therefore, the integral becomes  also have ∂f ∂z = 2z = 2   cid:21    cid:21  a · dS =  cid:24   cid:21  cid:21   S  I =  S  R   cid:24   Although this integral may be evaluated directly, it is quicker to transform to plane polar coordinates:  x2  a2 − x2 − y2  dx dy.   cid:21  cid:21   cid:21    cid:7   R  2π  I =  =  0   cid:24    cid:21  ρ2 cos2 φ a2 − ρ2  cos2 φ dφ  ρ dρ dφ  ρ3 dρ cid:24  a2 − ρ2  .  a  0  Making the substitution ρ = a sin u, we ﬁnally obtain   cid:21   2π  0   cid:21   π 2  0  I =  cos2 φ dφ  a3 sin3 u du =  2πa3  .  cid:2   3  In the above discussion we assumed that any line parallel to the z-axis intersects S only once. If this is not the case, we must split up the surface into smaller surfaces S1, S2 etc. that are of this type. The surface integral over S is then the sum of the surface integrals over S1, S2 and so on. This is always necessary for closed surfaces.  Sometimes we may need to project a surface S  or some part of it  onto the zx- or yz-plane, rather than the xy-plane; for such cases, the above analysis is easily modiﬁed.  392   11.5 SURFACE INTEGRALS  z  a  dS  S  a  C  dA = dx dy  a  y  x  Figure 11.7 The surface of the hemisphere x2 + y2 + z2 = a2, z ≥ 0.  11.5.2 Vector areas of surfaces  The vector area of a surface S is deﬁned as   cid:21   S  S =  dS,  where the surface integral may be evaluated as above.   cid:1 Find the vector area of the surface of the hemisphere x2 + y2 + z2 = a2 with z ≥ 0.  As in the previous example, dS = a2 sin θ dθ dφ ˆr in spherical polar coordinates. Therefore the vector area is given by   cid:21  cid:21   S  S =  a2 sin θ ˆr dθ dφ.  Now, since ˆr varies over the surface S , it also must be integrated. This is most easily achieved by writing ˆr in terms of the constant Cartesian basis vectors. On S we have  S = i  a2  cos φ dφ  sin2 θ dθ  a2  sin φ dφ  sin2 θ dθ  ˆr = sin θ cos φ i + sin θ sin φ j + cos θ k,   cid:31    cid:30    cid:21   + j   cid:31   2π  0   cid:31    cid:21   π 2  0  so the expression for the vector area becomes   cid:30    cid:21   cid:30   2π   cid:21   0   cid:21   π 2   cid:21   0  π 2  + k  a2  2π  dφ  0  0  = 0 + 0 + πa2k = πa2k.  sin θ cos θ dθ  Note that the magnitude of S is the projected area, of the hemisphere onto the xy-plane, and not the surface area of the hemisphere.  cid:2   393   LINE, SURFACE AND VOLUME INTEGRALS  dr  C  r  O  Figure 11.8 The conical surface spanning the perimeter C and having its vertex at the origin.  The hemispherical shell discussed above is an example of an open surface. For a closed surface, however, the vector area is always zero. This may be seen by projecting the surface down onto each Cartesian coordinate plane in turn. For each projection, every positive element of area on the upper surface is cancelled by the corresponding negative element on the lower surface. Therefore, each component of S =     S dS vanishes.  An important corollary of this result is that the vector area of an open surface depends only on its perimeter, or boundary curve, C. This may be proved as  follows. If surfaces S1 and S2 have the same perimeter then S1 − S2 is a closed  surface, for which  0   cid:21    cid:21   dS =  dS = 0.  dS −  S1  S2  Hence S1 = S2. Moreover, we may derive an expression for the vector area of an open surface S solely in terms of a line integral around its perimeter C. Since we may choose any surface with perimeter C, we will consider a cone with its vertex at the origin  see ﬁgure 11.8 . The vector area of the elementary triangular region shown in the ﬁgure is dS = 1 of the cone, and hence of any open surface with perimeter C, is given by the line integral  2 r × dr. Therefore, the vector area    C  x dy − y dx , as we found in section 11.3.  For a surface conﬁned to the xy-plane, r = xi + yj and dr = dx i + dy j, and we obtain for this special case that the area of the surface is given by A = 1 2  S =  1 2  C  r × dr.  0  394   11.5 SURFACE INTEGRALS      cid:1 Find the vector area of the surface of the hemisphere x2 + y2 + z2 = a2, z ≥ 0, by evaluating the line integral S = 1 2  C r × dr around its perimeter.  The perimeter C of the hemisphere is the circle x2 + y2 = a2, on which we have   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20   r × dr =  Therefore the cross product r × dr is given by  r = a cos φ i + a sin φ j,  i  j  a sin φ  a cos φ  −a sin φ dφ a cos φ dφ 0  cid:21   and the vector area becomes  dr = −a sin φ dφ i + a cos φ dφ j.   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  = a2 cos2 φ + sin2 φ  dφ k = a2 dφ k,  k 0  S = 1  2 a2k  0  2π  dφ = πa2 k.  cid:2   11.5.3 Physical examples of surface integrals  There are many examples of surface integrals in the physical sciences. Surface integrals of the form  11.8  occur in computing the total electric charge on a  cid:1  surface or the mass of a shell, S ρ r  dS , given the charge or mass density ρ r . For surface integrals involving vectors, the second form in  11.9  is the most S a · dS is called the ﬂux common. For a vector ﬁeld a, the surface integral of a through S . Examples of physically important ﬂux integrals are numerous. For example, let us consider a surface S in a ﬂuid with density ρ r  that has a  cid:1  velocity ﬁeld v r . The mass of ﬂuid crossing an element of surface area dS in time dt is dM = ρv · dS dt. Therefore the net total mass ﬂux of ﬂuid crossing S S ρ r v r  · dS. As a another example, the electromagnetic ﬂux of energy is M = out of a given volume V bounded by a surface S is    S  E × H  · dS.  The solid angle, to be deﬁned below, subtended at a point O by a surface  closed or otherwise  can also be represented by an integral of this form, although it is not strictly a ﬂux integral  unless we imagine isotropic rays radiating from O . The integral   cid:21    cid:1    cid:21   Ω =  r · dS r3 =  S  ˆr · dS r2  ,  S   11.11   gives the solid angle Ω subtended at O by a surface S if r is the position vector measured from O of an element of the surface. A little thought will show that  11.11  takes account of all three relevant factors: the size of the element of surface, its inclination to the line joining the element to O and the distance from O. Such a general expression is often useful for computing solid angles when the three-dimensional geometry is complicated. Note that  11.11  remains valid when the surface S is not convex and when a single ray from O in certain directions would cut S in more than one place  but we exclude multiply connected regions .  395   LINE, SURFACE AND VOLUME INTEGRALS  In particular, when the surface is closed Ω = 0 if O is outside S and Ω = 4π if O is an interior point.  is everywhere inwardly directed and the resultant force is F = −   Surface integrals resulting in vectors occur less frequently. An example is aﬀorded, however, by the total resultant force experienced by a body immersed in a stationary ﬂuid in which the hydrostatic pressure is given by p r . The pressure S p dS, taken  over the whole surface.  11.6 Volume integrals  Volume integrals are deﬁned in an obvious way and are generally simpler than line or surface integrals since the element of volume dV is a scalar quantity. We may encounter volume integrals of the forms   cid:21    cid:21   φ dV ,  V  a dV .  V   11.12    cid:1    cid:1   Clearly, the ﬁrst form results in a scalar, whereas the second form yields a vector. Two closely related physical examples, one of each kind, are provided by the total V ρ r  dV , and the total linear mass of a ﬂuid contained in a volume V , given by V ρ r v r  dV where v r  is the velocity momentum of that same ﬂuid, given by ﬁeld in the ﬂuid. As a slightly more complicated example of a volume integral we may consider the following.  cid:1 Find an expression for the angular momentum of a solid body rotating with angular velocity ω about an axis through the origin.  Consider a small volume element dV situated at position r; its linear momentum is ρ dV˙r,  where ρ = ρ r  is the density distribution, and its angular momentum about O is r× ρ˙r dV .  Thus for the whole body the angular momentum L is   cid:21   L =   r × ˙r ρ dV .  cid:21   V   cid:21    cid:21  Putting ˙r = ω × r yields  [r ×  ω × r ] ρ dV =  ωr2ρ dV −   r · ω rρ dV .  cid:2   V  V  L =  V  The evaluation of the ﬁrst type of volume integral in  11.12  has already been considered in our discussion of multiple integrals in chapter 6. The evaluation of the second type of volume integral follows directly since we can write   cid:21    cid:21    cid:21    cid:21   a dV = i  ax dV + j  ay dV + k  az dV ,  V  V  V  V   11.13   where ax, ay, az are the Cartesian components of a. Of course, we could have written a in terms of the basis vectors of some other coordinate system  e.g. spherical polars  but, since such basis vectors are not, in general, constant, they  396   11.6 VOLUME INTEGRALS  S  V  dS  r  O  Figure 11.9 A general volume V containing the origin and bounded by the closed surface S .  cannot be taken out of the integral sign as in  11.13  and must be included as part of the integrand.   cid:1   11.6.1 Volumes of three-dimensional regions  As discussed in chapter 6, the volume of a three-dimensional region V is simply V dV , which may be evaluated directly once the limits of integration have V = been found. However, the volume of the region obviously depends only on the surface S that bounds it. We should therefore be able to express the volume V in terms of a surface integral over S . This is indeed possible, and the appropriate expression may derived as follows. Referring to ﬁgure 11.9, let us suppose that the origin O is contained within V . The volume of the small shaded cone is dV = 1  3 r · dS; the total volume of the region is thus given by  0  V =  r · dS.  1 3  S  It may be shown that this expression is valid even when O is not contained in V . Although this surface integral form is available, in practice it is usually simpler to evaluate the volume integral directly.  cid:1 Find the volume enclosed between a sphere of radius a centred on the origin and a circular cone of half-angle α with its vertex at the origin.  The element of vector area dS on the surface of the sphere is given in spherical polar coordinates by a2 sin θ dθ dφ ˆr. Now taking the axis of the cone to lie along the z-axis  from which θ is measured  the required volume is given by  0   cid:21   cid:21    cid:21   cid:21   V =  1 3  S  r · dS =  1 3  1 3  2π  2π  0  0  dφ  dφ  α  α  0  0  =  a2 sin θ r · ˆr dθ  a3 sin θ dθ =   1 − cos α .  cid:2   2πa3  3  397    11.14    11.15    11.16    11.17   LINE, SURFACE AND VOLUME INTEGRALS  11.7 Integral forms for grad, div and curl  In the previous chapter we deﬁned the vector operators grad, div and curl in purely mathematical terms, which depended on the coordinate system in which they were expressed. An interesting application of line, surface and volume integrals is the expression of grad, div and curl in coordinate-free, geometrical terms. If φ is a scalar ﬁeld and a is a vector ﬁeld then it may be shown that at any point P   cid:7   cid:7   cid:7   1 V 1 V 1 V  0 0 0  S  S  S   cid:8   cid:8  φ dS  cid:8  a · dS dS × a  ∇φ = lim V→0 ∇ · a = lim V→0 ∇ × a = lim V→0  where V is a small volume enclosing P and S is its bounding surface. Indeed, we may consider these equations as the  geometrical  deﬁnitions of grad, div and  curl. An alternative, but equivalent, geometrical deﬁnition of ∇ × a at a point P ,  which is often easier to use than  11.16 , is given by   cid:7   0   cid:8    ∇ × a  · ˆn = lim A→0  a · dr  ,  1 A  C  where C is a plane contour of area A enclosing the point P and ˆn is the unit normal to the enclosed planar area.  It may be shown, in any coordinate system, that all the above equations are consistent with our deﬁnitions in the previous chapter, although the diﬃculty of proof depends on the chosen coordinate system. The most general coordinate system encountered in that chapter was one with orthogonal curvilinear coordi- nates u1, u2, u3, of which Cartesians, cylindrical polars and spherical polars are all special cases. Although it may be shown that  11.14  leads to the usual expression for grad in curvilinear coordinates, the proof requires complicated manipulations of the derivatives of the basis vectors with respect to the coordinates and is not presented here. In Cartesian coordinates, however, the proof is quite simple.   cid:1 Show that the geometrical deﬁnition of grad leads to the usual expression for ∇φ in  Cartesian coordinates.  Consider the surface S of a small rectangular volume element ∆V = ∆x ∆y ∆z that has its faces parallel to the x, y, and z coordinate surfaces; the point P  see above  is at one corner. We must calculate the surface integral  11.14  over each of its six faces. Remembering that the normal to the surface points outwards from the volume on each face, the two faces  with x = constant have areas ∆S = −i ∆y ∆z and ∆S = i ∆y ∆z respectively. Furthermore,  over each small surface element, we may take φ to be constant, so that the net contribution  398   11.7 INTEGRAL FORMS FOR grad, div AND curl  to the surface integral from these two faces is then  [ φ + ∆φ  − φ] ∆y ∆z i =  ∆x − φ  φ +  ∂φ ∂x  ∆y ∆z i   cid:8    cid:7   ∂φ ∂x  The surface integral over the pairs of faces with y = constant and z = constant respectively may be found in a similar way, and we obtain  =  ∆x ∆y ∆z i.   cid:8   0   cid:7   ∂φ ∂x  1   cid:13   φ dS =  i +  j +  k  ∆x ∆y ∆z.   cid:7   ∂φ ∂y  ∂φ ∂x  ∂φ ∂z  ∂φ ∂y   cid:8   ∂φ ∂z  i +  j +  k  ∆x ∆y ∆z   cid:14   Therefore ∇φ at the point P is given by  S  ∇φ = lim  ∆x,∆y,∆z→0 ∂φ ∂x  i +  ∂φ ∂y  ∆x ∆y ∆z  j +  k.  cid:2   ∂φ ∂z  =  We now turn to  11.15  and  11.17 . These geometrical deﬁnitions may be shown straightforwardly to lead to the usual expressions for div and curl in orthogonal curvilinear coordinates.  cid:1 By considering the inﬁnitesimal volume element dV = h1h2h3 ∆u1 ∆u2 ∆u3 shown in ﬁg- ure 11.10, show that  11.15  leads to the usual expression for ∇·a in orthogonal curvilinear  coordinates.  Let us write the vector ﬁeld in terms of its components with respect to the basis vectors of the curvilinear coordinate system as a = a1 ˆe1 + a2 ˆe2 + a3 ˆe3. We consider ﬁrst the contribution to the RHS of  11.15  from the two faces with u1 = constant, i.e. P QRS and the face opposite it  see ﬁgure 11.10 . Now, the volume element is formed from the orthogonal vectors h1 ∆u1 ˆe1, h2 ∆u2 ˆe2 and h3 ∆u3 ˆe3 at the point P and so for P QRS we have  ∆S = h2h3 ∆u2 ∆u3 ˆe3 × ˆe2 = −h2h3 ∆u2 ∆u3 ˆe1.  Reasoning along the same lines as in the previous example, we conclude that the contri-  bution to the surface integral of a · dS over P QRS and its opposite face taken together is  given by   a · ∆S  ∆u1 =  ∂ ∂u1   a1h2h3  ∆u1 ∆u2 ∆u3.  The surface integrals over the pairs of faces with u2 = constant and u3 = constant respectively may be found in a similar way, and we obtain  0  a · dS =   cid:13   ∂ ∂u1  ∂ ∂u1  Therefore ∇ · a at the point P is given by  S  ∂ ∂u2   cid:13    a1h2h3  +   a2h3h1  +   a3h1h2   ∆u1 ∆u2 ∆u3.  ∇ · a =   cid:13   lim  ∆u1,∆u2,∆u3→0 ∂ ∂u1  h1h2h3  1  =  h1h2h3 ∆u1 ∆u2 ∆u3  S   a1h2h3  +   a2h3h1  +   a3h1h2    cid:14   .  cid:2    cid:14   ∂ ∂u3  0  a · dS   cid:14   ∂ ∂u3  1  ∂ ∂u2  399   LINE, SURFACE AND VOLUME INTEGRALS  z  h1∆u1 ˆe1  S  T  R  Q  P  h3∆u3 ˆe3  h2∆u2 ˆe2  y  x  0  Figure 11.10 A general volume ∆V in orthogonal curvilinear coordinates u1, u2, u3. P T gives the vector h1 ∆u1 ˆe1, P S gives h2 ∆u2 ˆe2 and P Q gives h3 ∆u3 ˆe3.   cid:1 By considering the inﬁnitesimal planar surface element P QRS in ﬁgure 11.10, show that  11.17  leads to the usual expression for ∇ × a in orthogonal curvilinear coordinates.  The planar surface P QRS is deﬁned by the orthogonal vectors h2 ∆u2 ˆe2 and h3 ∆u3 ˆe3 at the point P . If we traverse the loop in the direction P S RQ then, by the right-hand convention, the unit normal to the plane is ˆe1. Writing a = a1 ˆe1 + a2 ˆe2 + a3 ˆe3, the line integral around the loop in this direction is given by   cid:14    cid:13   ∂ ∂u3   cid:13    cid:13  −  =  ∂ ∂u2  a · dr = a2h2 ∆u2 +  P S RQ  a3h3 +   a3h3  ∆u2  ∆u3  ∂ ∂u2  a2h2 +   a2h2  ∆u3  ∆u2 − a3h3 ∆u3   a3h3  − ∂  cid:13   ∂u3  1   a2h2   ∆u2 ∆u3.   cid:14    cid:14  a · dr   cid:13   ∇ × a 1 = lim ∆u2,∆u3→0 1 ∂ ∂u2  h2h3  =  h2h3 ∆u2 ∆u3   h3a3  − ∂  ∂u3  P S RQ   h2a2   .   cid:14   cid:14  0  Therefore from  11.17  the component of ∇ × a in the direction ˆe1 at P is given by  The other two components are found by cyclically permuting the subscripts 1, 2, 3.  cid:2  Finally, we note that we can also write the ∇2 operator as a surface integral by setting a = ∇φ in  11.15 , to obtain   cid:7   0  1 V  S   cid:8   ∇φ · dS  .  ∇2φ = ∇ · ∇φ = lim V→0  400   11.8 DIVERGENCE THEOREM AND RELATED THEOREMS  11.8 Divergence theorem and related theorems  The divergence theorem relates the total ﬂux of a vector ﬁeld out of a closed surface S to the integral of the divergence of the vector ﬁeld over the enclosed volume V ; it follows almost immediately from our geometrical deﬁnition of divergence  11.15 .  Imagine a volume V , in which a vector ﬁeld a is continuous and diﬀerentiable, to be divided up into a large number of small volumes Vi. Using  11.15 , we have for each small volume  0   ∇ · a Vi ≈  a · dS,  Si  where Si is the surface of the small volume Vi. Summing over i we ﬁnd that contributions from surface elements interior to S cancel since each surface element appears in two terms with opposite signs, the outward normals in the two terms being equal and opposite. Only contributions from surface elements that are also is allowed to tend to zero then we obtain the parts of S survive. If each Vi divergence theorem,  ∇ · a dV =  a · dS.   11.18    cid:21   V  0  S  We note that the divergence theorem holds for both simply and multiply con- nected surfaces, provided that they are closed and enclose some non-zero volume V . The divergence theorem may also be extended to tensor ﬁelds  see chapter 26 . The theorem ﬁnds most use as a tool in formal manipulations, but sometimes   cid:1   it is of value in transforming surface integrals of the form integrals or vice versa. For example, setting a = r we immediately obtain  S a · dS into volume  0  S  3 dV = 3V =  r · dS,   cid:21    cid:21   V  V  ∇ · r dV =  cid:1   which gives the expression for the volume of a region found in subsection 11.6.1. The use of the divergence theorem is further illustrated in the following example.  cid:1 Evaluate the surface integral I = is the open surface of the hemisphere x2 + y2 + z2 = a2, z ≥ 0.  S a · dS, where a =  y − x  i + x2z j +  z + x2  k and S  We could evaluate this surface integral directly, but the algebra is somewhat lengthy. We will therefore evaluate it by use of the divergence theorem. Since the latter only holds for closed surfaces enclosing a non-zero volume V , let us ﬁrst consider the closed surface  cid:7   cid:7  S then encloses a hemispherical volume V . By the divergence theorem we have  = S + S1, where S1 is the circular area in the xy-plane given by x2 + y2 ≤ a2, z = 0; S   cid:21   0   cid:21   cid:7  a · dS =  cid:21  Now ∇ · a = −1 + 0 + 1 = 0, so we can write a · dS = −  ∇ · a dV =  cid:21   V  S   cid:21   S1  a · dS +  a · dS.  S  S1  a · dS.  S  401   LINE, SURFACE AND VOLUME INTEGRALS  y  R  C  dr  dy  dx  ˆn ds  x  Figure 11.11 A closed curve C in the xy-plane bounding a region R. Vectors tangent and normal to the curve at a given point are also shown.   cid:21   The surface integral over S1 is easily evaluated. Remembering that the normal to the  surface points outward from the volume, a surface element on S1 is simply dS = −k dx dy. On S1 we also have a =  y − x  i + x2 k, so that a · dS =  cid:21   where R is the circular region in the xy-plane given by x2 + y2 ≤ a2. Transforming to plane  polar coordinates we have  I = −   cid:21  cid:21    cid:21  cid:21   x2 dx dy,   cid:21   S1  R  I =   cid:7  ρ2 cos2 φ ρ dρ dφ =  R  2π  0  cos2 φ dφ  ρ3 dρ =  a  0  .  cid:2   πa4 4  It is also interesting to consider the two-dimensional version of the divergence theorem. As an example, let us consider a two-dimensional planar region R in the xy-plane bounded by some closed curve C  see ﬁgure 11.11 . At any point on the curve the vector dr = dx i + dy j is a tangent to the curve and the vector  ˆn ds = dy i − dx j is a normal pointing out of the region R. If the vector ﬁeld a is  continuous and diﬀerentiable in R then the two-dimensional divergence theorem in Cartesian coordinates gives   cid:8   0   cid:21  cid:21    cid:7   0  ∂ax ∂x  +  ∂ay ∂y  dx dy =  a · ˆn ds =   ax dy − ay dx .  Letting P = −ay and Q = ax, we recover Green’s theorem in a plane, which was  C  R  discussed in section 11.3.  Consider two scalar functions φ and ψ that are continuous and diﬀerentiable in some volume V bounded by a surface S . Applying the divergence theorem to the  11.8.1 Green’s theorems  402   11.8 DIVERGENCE THEOREM AND RELATED THEOREMS  vector ﬁeld φ∇ψ we obtain  φ∇ψ · dS =   cid:21   cid:21   V  V  =  0  S  0  S  ∇ ·  φ∇ψ  dV  cid:18   cid:19  φ∇2ψ +  ∇φ  ·  ∇ψ   cid:21   dV .   11.19   Reversing the roles of φ and ψ in  11.19  and subtracting the two equations gives   φ∇ψ − ψ∇φ  · dS =   φ∇2ψ − ψ∇2φ  dV .   11.20   Equation  11.19  is usually known as Green’s ﬁrst theorem and  11.20  as his second. Green’s second theorem is useful in the development of the Green’s functions used in the solution of partial diﬀerential equations  see chapter 21 .  11.8.2 Other related integral theorems  There exist two other integral theorems which are closely related to the divergence theorem and which are of some use in physical applications. If φ is a scalar ﬁeld and b is a vector ﬁeld and both φ and b satisfy our usual diﬀerentiability conditions in some volume V bounded by a closed surface S then  ∇φ dV = ∇ × b dV =  V  φ dS,  dS × b.   11.21    11.22   V  0 0  S  S  0   cid:21    cid:21   V   cid:21   In the divergence theorem  11.18  let a = φc, where c is a constant vector. We then have   cid:1 Use the divergence theorem to prove  11.21 .  ∇ ·  φc  dV =  φc · dS.  S Expanding out the integrand on the LHS we have  V  since c is constant. Also, φc · dS = c · φdS, so we obtain  ∇ ·  φc  = φ∇ · c + c · ∇φ = c · ∇φ,  cid:21   0  Since c is constant we may take it out of both integrals to give  c ·  ∇φ  dV =  cid:21   ∇φ dV = c ·  V  c ·  V  S  c · φ dS. 0  φ dS,  S  and since c is arbitrary we obtain the stated result  11.21 .  cid:2  Equation  11.22  may be proved in a similar way by letting a = b × c in the  divergence theorem, where c is again a constant vector.  403   LINE, SURFACE AND VOLUME INTEGRALS  11.8.3 Physical applications of the divergence theorem  The divergence theorem is useful in deriving many of the most important partial diﬀerential equations in physics  see chapter 20 . The basic idea is to use the divergence theorem to convert an integral form, often derived from observation, into an equivalent diﬀerential form  used in theoretical statements .  cid:1 For a compressible ﬂuid with time-varying position-dependent density ρ r, t  and velocity ﬁeld v r, t , in which ﬂuid is neither being created nor destroyed, show that  + ∇ ·  ρv  = 0.  ∂ρ ∂t  0 0  S  = −  dM dt  ρv · dS,   cid:21   d dt  V  ρ dV +  For an arbitrary volume V in the ﬂuid, the conservation of mass tells us that the rate of increase or decrease of the mass M of ﬂuid in the volume must equal the net rate at which ﬂuid is entering or leaving the volume, i.e.   cid:1   where S is the surface bounding V . But the mass of ﬂuid in V is simply M = we have  V ρ dV , so  Taking the derivative inside the ﬁrst integral on the RHS and using the divergence theorem to rewrite the second integral, we obtain  ρv · dS = 0.  cid:13   S   cid:21    cid:14    cid:21    cid:21   V  ∂ρ ∂t  V  dV +  ∇ ·  ρv  dV =  + ∇ ·  ρv   ∂ρ ∂t  V  dV = 0.  Since the volume V is arbitrary, the integrand  which is assumed continuous  must be identically zero, so we obtain  + ∇ ·  ρv  = 0.  ∂ρ ∂t  This is known as the continuity equation. It can also be applied to other systems, for example those in which ρ is the density of electric charge or the heat content, etc. For the ﬂow of an incompressible ﬂuid, ρ = constant and the continuity equation becomes simply  ∇ · v = 0.  cid:2   In the previous example, we assumed that there were no sources or sinks in the volume V , i.e. that there was no part of V in which ﬂuid was being created or destroyed. We now consider the case where a ﬁnite number of point sources and or sinks are present in an incompressible ﬂuid. Let us ﬁrst consider the simple case where a single source is located at the origin, out of which a quantity of ﬂuid ﬂows radially at a rate Q  m3 s  −1 . The velocity ﬁeld is given by  Now, for a sphere S1 of radius r centred on the source, the ﬂux across S1 is  v =  Qr 4πr3 =  Qˆr 4πr2 .  v · dS = v4πr2 = Q.  0  S1  404   11.8 DIVERGENCE THEOREM AND RELATED THEOREMS  Since v has a singularity at the origin it is not diﬀerentiable there, i.e. ∇· v is not deﬁned there, but at all other points ∇ · v = 0, as required for an incompressible not enclose the origin we have0  ﬂuid. Therefore, from the divergence theorem, for any closed surface S2 that does   cid:21   v · dS =  S2  V  ∇ · v dV = 0.   S v · dS has value Q or zero depending  Thus we see that the surface integral  on whether or not S encloses the source. In order that the divergence theorem is valid for all surfaces S , irrespective of whether they enclose the source, we write  ∇ · v = Qδ r ,  where δ r  is the three-dimensional Dirac delta function. The properties of this function are discussed fully in chapter 13, but for the moment we note that it is deﬁned in such a way that   cid:21   V  δ r − a  = 0  for r  cid:3 = a,    f r δ r − a  dV =  f a   if a lies in V  0  otherwise   cid:21   for any well-behaved function f r . Therefore, for any volume V containing the  source at the origin, we have cid:21  ∇ · v dV = Q   S v · dS = Q for a closed surface enclosing the source.  which is consistent with Hence, by introducing the Dirac delta function the divergence theorem can be made valid even for non-diﬀerentiable point sources.  δ r  dV = Q,  V  V  The generalisation to several sources and sinks is straightforward. For example,  if a source is located at r = a and a sink at r = b then the velocity ﬁeld is   r − a Q 4πr − a3  −  r − b Q 4πr − b3  v =  and its divergence is given by    ∇ · v = Qδ r − a  − Qδ r − b . S v · dS has the value Q if S encloses the source, −Q if  Therefore, the integral S encloses the sink and 0 if S encloses neither the source nor sink or encloses them both. This analysis also applies to other physical systems – for example, in electrostatics we can regard the sources and sinks as positive and negative point charges respectively and replace v by the electric ﬁeld E.  405   LINE, SURFACE AND VOLUME INTEGRALS  11.9 Stokes’ theorem and related theorems  Stokes’ theorem is the ‘curl analogue’ of the divergence theorem and relates the integral of the curl of a vector ﬁeld over an open surface S to the line integral of the vector ﬁeld around the perimeter C bounding the surface.  Following the same lines as for the derivation of the divergence theorem, we can divide the surface S into many small areas Si with boundaries Ci and unit normals ˆni. Using  11.17 , we have for each small area a · dr.   ∇ × a  · ˆni Si ≈  0  Ci  0  Summing over i we ﬁnd that on the RHS all parts of all interior boundaries that are not part of C are included twice, being traversed in opposite directions on each occasion and thus contributing nothing. Only contributions from line elements that are also parts of C survive. If each Si is allowed to tend to zero  then we obtain Stokes’ theorem, cid:21    ∇ × a  · dS =  S  a · dr.  C   11.23   We note that Stokes’ theorem holds for both simply and multiply connected open surfaces, provided that they are two-sided. Stokes’ theorem may also be extended to tensor ﬁelds  see chapter 26 .  Just as the divergence theorem  11.18  can be used to relate volume and surface integrals for certain types of integrand, Stokes’ theorem can be used in evaluating surface integrals of the form    S  ∇ × a  · dS as line integrals or vice versa.   cid:1 Given the vector ﬁeld a = y i − x j + z k, verify Stokes’ theorem for the hemispherical surface x2 + y2 + z2 = a2, z ≥ 0. Let us ﬁrst evaluate the surface integral cid:21   over the hemisphere. It is easily shown that ∇ × a = −2 k, and the surface element is dS = a2 sin θ dθ dφ ˆr in spherical polar coordinates. Therefore   cid:21   S   ∇ × a  · dS =  2π  dφ  S   ∇ × a  · dS  cid:21   π 2   cid:21    cid:21   cid:21   0  0  0  = −2a2 = −2a2  0 2π  dφ  dφ  dθ   cid:21   cid:21   0  0   cid:5 −2a2 sin θ  cid:6   cid:10   cid:9   π 2  sin θ  dθ  z a  ˆr · k  2π  π 2  sin θ cos θ dθ = −2πa2.  We now evaluate the line integral around the perimeter curve C of the surface, which  406   0 0   cid:21   C  C  0   cid:21    cid:21   S   cid:21   S  11.9 STOKES’ THEOREM AND RELATED THEOREMS  0  C  is the circle x2 + y2 = a2 in the xy-plane. This is given by  a · dr =  =   y i − x j + z k  ·  dx i + dy j + dz k   y dx − x dy .  Using plane polar coordinates, on C we have x = a cos φ, y = a sin φ so that dx =  −a sin φ dφ, dy = a cos φ dφ, and the line integral becomes   y dx − x dy  = −a2  2π   sin2 φ + cos2 φ  dφ = −a2  2π  dφ = −2πa2.   cid:21   0  0  C  Since the surface and line integrals have the same value, we have veriﬁed Stokes’ theorem in this case.  cid:2   The two-dimensional version of Stokes’ theorem also yields Green’s theorem in a plane. Consider the region R in the xy-plane shown in ﬁgure 11.11, in which a  vector ﬁeld a is deﬁned. Since a = ax i + ay j, we have ∇× a =  ∂ay ∂x− ∂ax ∂y  k,  and Stokes’ theorem becomes   cid:8    cid:21  cid:21    cid:7   ∂ay ∂x  − ∂ax  ∂y  R  dx dy =   ax dx + ay dy .  Letting P = ax and Q = ay we recover Green’s theorem in a plane,  11.4 .  11.9.1 Related integral theorems  As for the divergence theorem, there exist two other integral theorems that are closely related to Stokes’ theorem. If φ is a scalar ﬁeld and b is a vector ﬁeld, and both φ and b satisfy our usual diﬀerentiability conditions on some two-sided open surface S bounded by a closed perimeter curve C, then  dS × ∇φ =  dS × ∇  × b =  S  φ dr,  dr × b.   cid:1 Use Stokes’ theorem to prove  11.24 .  In Stokes’ theorem,  11.23 , let a = φc, where c is a constant vector. We then have  [∇ ×  φc ] · dS =  φc · dr.  Expanding out the integrand on the LHS we have  ∇ ×  φc  = ∇φ × c + φ∇ × c = ∇φ × c,  since c is constant, and the scalar triple product on the LHS of  11.26  can therefore be written  [∇ ×  φc ] · dS =  ∇φ × c  · dS = c ·  dS × ∇φ .  407  0  C  0 0  C  C  0  C   11.24    11.25    11.26    LINE, SURFACE AND VOLUME INTEGRALS  Substituting this into  11.26  and taking c out of both integrals because it is constant, we ﬁnd  c ·  dS × ∇φ = c ·  φ dr.  C  Since c is an arbitrary constant vector we therefore obtain the stated result  11.24 .  cid:2  Equation  11.25  may be proved in a similar way, by letting a = b× c in Stokes’  theorem, where c is again a constant vector. We also note that by setting b = r in  11.25  we ﬁnd   dS × ∇  × r =  dr × r.  Expanding out the integrand on the LHS gives   dS × ∇  × r = dS − dS ∇ · r  = dS − 3 dS = −2 dS.  Therefore, as we found in subsection 11.5.2, the vector area of an open surface S is given by   cid:21   S   cid:21   S   cid:21   S =  dS =  S  C  r × dr.  11.9.2 Physical applications of Stokes’ theorem  Like the divergence theorem, Stokes’ theorem is useful in converting integral equations into diﬀerential equations.  cid:1 From Amp`ere’s law, derive Maxwell’s equation in the case where the currents are steady, i.e. ∇ × B − µ0J = 0.  Amp`ere’s rule for a distributed current with current density J is  0  0  C  0  1 2   cid:21   S  for any circuit C bounding a surface S . Using Stokes’ theorem, the LHS can be transformed into  S  ∇ × B  · dS; hence  0  cid:21   C  B · dr = µ0  J · dS,   ∇ × B − µ0J  · dS = 0   cid:1   for any surface S . This can only be so if ∇ × B − µ0J = 0, which is the required relation. equation ∇ × E = −∂B ∂t.  cid:2   Similarly, from Faraday’s law of electromagnetic induction we can derive Maxwell’s  S  In subsection 11.8.3 we discussed the ﬂow of an incompressible ﬂuid in the presence of several sources and sinks. Let us now consider vortex ﬂow in an incompressible ﬂuid with a velocity ﬁeld  in cylindrical polar coordinates ρ, φ, z. For this velocity ﬁeld ∇ × v equals zero  v =  ˆeφ,  1 ρ  408   11.10 EXERCISES    C v· dr  everywhere except on the axis ρ = 0, where v has a singularity. Therefore equals zero for any path C that does not enclose the vortex line on the axis and 2π if C does enclose the axis. In order for Stokes’ theorem to be valid for all paths C, we therefore set  ∇ × v = 2πδ ρ ,  where δ ρ  is the Dirac delta function, to be discussed in subsection 13.1.3. Now,  since ∇ × v = 0, except on the axis ρ = 0, there exists a scalar potential ψ such that v = ∇ψ. It may easily be shown that ψ = φ, the polar angle. Therefore, if C does not enclose the axis then0 and if C does enclose the axis,0  v · dr =  dφ = 0,  0  C  v · dr = ∆φ = 2πn,  C  where n is the number of times we traverse C. Thus φ is a multivalued potential. Similar analyses are valid for other physical systems – for example, in magneto- statics we may replace the vortex lines by current-carrying wires and the velocity ﬁeld v by the magnetic ﬁeld B.  11.1  The vector ﬁeld F is deﬁned by  11.10 Exercises  i +   cid:19    cid:18    cid:1   Q =  3x2 y + z  + y3 + z3  of φ. The vector ﬁeld Q is deﬁned by  F = 2xzi + 2yz2j +  x2 + 2y2z − 1 k.  cid:19   cid:18   Calculate ∇ × F and deduce that F can be written F = ∇φ. Determine the form  cid:18   cid:19  3y2 z + x  + z3 + x3 k. Q · dr along any line connecting the point A at  Show that Q is a conservative ﬁeld, construct its potential function and hence evaluate the integral J =   1,−1, 1  to B at  2, 1, 2 . F is a vector ﬁeld xy2i + 2j + xk, and L is a path parameterised by x = ct, y = c t, z = d for the range 1 ≤ t ≤ 2. Evaluate  a  L F · dr. in Green’s theorem in a plane, show that the integral of x− y over the upper half of the unit circle centred on the origin has the value − 2 3 . Show the same result  By making an appropriate choice for the functions P  x, y  and Q x, y  that appear  3z2 x + y  + x3 + y3  L F dy and  c   L F dt,  b   by direct integration in Cartesian coordinates. Determine the point of intersection P , in the ﬁrst quadrant, of the two ellipses   cid:1    cid:1    cid:1   j +  11.2  11.3  11.4  11.5  x2 a2  y2 b2  +  = 1 and  +  = 1.  x2 b2  y2 a2  Taking b < a, consider the contour L that bounds the area in the ﬁrst quadrant that is common to the two ellipses. Show that the parts of L that lie along the  coordinate axes contribute nothing to the line integral around L of x dy − y dx.  Using a parameterisation of each ellipse similar to that employed in the example  409   LINE, SURFACE AND VOLUME INTEGRALS  11.6  in section 11.3, evaluate the two remaining line integrals and hence ﬁnd the total area common to the two ellipses. By using parameterisations of the form x = a cosn θ and y = a sinn θ for suitable values of n, ﬁnd the area bounded by the curves  x2 5 + y2 5 = a2 5  and x2 3 + y2 3 = a2 3.  11.7  Evaluate the line integral   cid:19   I =  y 4x2 + y2  dx + x 2x2 + 3y2  dy   cid:18   0  C  11.8  around the ellipse x2 a2 + y2 b2 = 1. Criticise the following ‘proof’ that π = 0.   a  Apply Green’s theorem in a plane to the functions P  x, y  = tan  −1 y x  and −1 x y , taking the region R to be the unit circle centred on the  Q x, y  = tan origin.   b  The RHS of the equality so produced is   cid:21   cid:21   y − x x2 + y2 dx dy,  R  which, either from symmetry considerations or by changing to plane polar coordinates, can be shown to have zero value. In the LHS of the equality, set x = cos θ and y = sin θ, yielding P  θ  = θ  and Q θ  = π 2 − θ. The line integral becomes   c    cid:21    cid:22  cid:9   2π  0  π 2   cid:10   − θ  cos θ − θ sin θ  dθ,   cid:23   which has the value 2π.   d  Thus 2π = 0 and the stated result follows.  11.9  A single-turn coil C of arbitrary shape is placed in a magnetic ﬁeld B and carries a current I. Show that the couple acting upon the coil can be written as   cid:21    cid:21   M = I   B · r  dr − I  B r · dr .  C  C  11.10  11.11  For a planar rectangular coil of sides 2a and 2b placed with its plane vertical and at an angle φ to a uniform horizontal ﬁeld B, show that M is, as expected, 4abBI cos φ k. Find the vector area S of the part of the curved surface of the hyperboloid of revolution  − y2 + z2 that lies in the region z ≥ 0 and a ≤ x ≤ λa.  x2 a2  b2  = 1  An axially symmetric solid body with its axis AB vertical is immersed in an incompressible ﬂuid of density ρ0. Use the following method to show that, whatever the shape of the body, for ρ = ρ z  in cylindrical polars the Archimedean upthrust is, as expected, ρ0gV , where V is the volume of the body.  Express the vertical component of the resultant force on the body, − cid:1   where p is the pressure, in terms of an integral; note that p = −ρ0gz and that for an annular surface element of width dl, n · nz dl = −dρ. Integrate by parts and  p dS,  use the fact that ρ zA  = ρ zB  = 0.  410   11.10 EXERCISES   cid:21   b  0   cid:21   11.12  Show that the expression below is equal to the solid angle subtended by a rectangular aperture, of sides 2a and 2b, at a point on the normal through its centre, and at a distance c from the aperture:  11.13  11.14  11.15  11.16  Ω = 4   y2 + c2  y2 + c2 + a2 1 2 dy.  ac  By setting y =  a2 + c2 1 2 tan φ, change this integral into the form  φ1  4ac cos φ  c2 + a2 sin2 φ  0  dφ,   cid:13   −1 −3i−zyr   cid:14   .  where tan φ1 = b  a2 + c2 1 2, and hence show that  Ω = 4 tan  ab  A vector ﬁeld a is given by −zxr −3k, where r2 = x2 +y2 +z2. Establish that the ﬁeld is conservative  a  by showing that ∇ × a = 0, and  b  by  constructing its potential function φ. A vector ﬁeld a is given by  z2 + 2xy  i +  x2 + 2yz  j +  y2 + 2zx  k. Show that a · dr along any line joining  1, 1, 1  a is conservative and that the line integral and  1, 2, 2  has the value 11. A force F r  acts on a particle at r. In which of the following cases can F be represented in terms of a potential? Where it can, ﬁnd the potential.  c a2 + b2 + c2 1 2 −3j+ x2 +y2 r  cid:1    cid:13  i − j − 2 x − y   cid:13   cid:13   cid:14    cid:14   x2 + y2 − a2  a r × k   zk +  a2  a2  r   cid:7   cid:14    cid:7  − r2 a2  r  exp   cid:8   ; − r2 a2   cid:8   ;  exp   a  F = F0   b  F =  F0 a   c  F = F0  k +  .  r2   a   could represent a real magnetic ﬁeld; where it could, try to ﬁnd a suitable vector  One of Maxwell’s electromagnetic equations states that all magnetic ﬁelds B  are solenoidal  i.e. ∇ · B = 0 . Determine whether each of the following vectors potential A, i.e. such that B = ∇× A.  Hint: seek a vector potential that is parallel to ∇ × B. : [ x − y z i +  x − y z j +  x2 − y2  k] in Cartesians with r2 = x2 + y2 + z2;  cid:13  [cos θ cos φ ˆer − sin θ cos φ ˆeθ + sin 2θ sin φ ˆeφ] in spherical polars;  B0b r3 B0b3 r3  c  B0b2 The vector ﬁeld f has components yi−xj+k and γ is a curve given parametrically  in cylindrical polars.   b2 + z2 2  b2 + z2  ˆeρ +   cid:14    b   zρ  ˆez  1  11.17  by  r =  a − c + c cos θ i +  b + c sin θ j + c2θk,   cid:1  0 ≤ θ ≤ 2π.  γ f · dr vanishes.  11.18  Describe the shape of the path γ and show that the line integral Does this result imply that f is a conservative ﬁeld? A vector ﬁeld a = f r r is spherically symmetric and everywhere directed away from the origin. Show that a is irrotational, but that it is also solenoidal only if f r  is of the form Ar  −3.  411   LINE, SURFACE AND VOLUME INTEGRALS   cid:1   11.19  Evaluate the surface integral  r· dS, where r is the position vector, over that part of the surface z = a2− x2− y2 for which z ≥ 0, by each of the following methods.  a  Parameterise the surface as x = a sin θ cos φ, y = a sin θ sin φ, z = a2 cos2 θ,  r · dS = a4 2 sin3 θ cos θ + cos3 θ sin θ  dθ dφ.   b  Apply the divergence theorem to the volume bounded by the surface and  and show that  the plane z = 0.  11.20  Obtain an expression for the value φP at a point P of a scalar function φ that  satisﬁes ∇2φ = 0, in terms of its value and normal derivative on a surface S that  encloses it, by proceeding as follows.   a  In Green’s second theorem, take ψ at any particular point Q as 1 r, where r  is the distance of Q from P . Show that ∇2ψ = 0, except at r = 0.   b  Apply the result to the doubly connected region bounded by S and a small  sphere Σ of radius δ centred on P.   c  Apply the divergence theorem to show that the surface integral over Σ involving 1 δ vanishes, and prove that the term involving 1 δ2 has the value 4πφP .   cid:21    cid:7    cid:8    cid:21    d  Conclude that  φP = − 1  φ  ∂ ∂n  1 r  dS +  1 4π  1 r  ∂φ ∂n  S  dS .  4π  S  This important result shows that the value at a point P of a function φ  that satisﬁes ∇2φ = 0 everywhere within a closed surface S that encloses P  may be expressed entirely in terms of its value and normal derivative on S . This matter is taken up more generally in connection with Green’s functions in chapter 21 and in connection with functions of a complex variable in section 24.10.  11.21  Use result  11.21 , together with an appropriately chosen scalar function φ, to prove that the position vector ¯r of the centre of mass of an arbitrarily shaped body of volume V and uniform density can be written  11.22  A rigid body of volume V and surface S rotates with angular velocity ω. Show that  0  S  0  ¯r =  1 V  1  2 r2 dS.  ω = − 1  u × dS,  2V  S  11.23  where u x  is the velocity of the point x on the surface S . Demonstrate the validity of the divergence theorem:   a  by calculating the ﬂux of the vector   b  by showing that  through the spherical surface r = ∇ · F =  F =  αr  √  r2 + a2 3 2  3a;  3αa2   r2 + a2 5 2  and evaluating the volume integral of ∇ · F over the interior of the sphere r =  √ 3a. The substitution r = a tan θ will prove useful in carrying out the  integration.  412   11.10 EXERCISES  11.24  Prove equation  11.22  and, by taking b = zx2i + zy2j +  x2 − y2 k, show that the  two integrals   cid:21    cid:21   I =  x2 dV and J =  cos2 θ sin3 θ cos2 φ dθ dφ,  11.25  both taken over the unit sphere, must have the same value. Evaluate both directly to show that the common value is 4π 15. In a uniform conducting medium with unit relative permittivity, charge density ρ, current density J, electric ﬁeld E and magnetic ﬁeld B, Maxwell’s electromagnetic equations take the form  with µ0 cid:4 0 = c   i  ∇ · B = 0,  iii  ∇ × E + ˙B = 0,  −2   ii  ∇ · E = ρ  cid:4 0,  iv  ∇ × B −  ˙E c2  = µ0J. 0  The density of stored energy in the medium is given by 1 that the rate of change of the total stored energy in a volume V is equal to  2   cid:4 0E2 + µ  −1 0 B2 . Show   cid:21   V  −  J · E dV − 1   E × B  · dS,  µ0  S  where S is the surface bounding V .  [ The ﬁrst integral gives the ohmic heating loss, whilst the second gives the  electromagnetic energy ﬂux out of the bounding surface. The vector µ is known as the Poynting vector. ] A vector ﬁeld F is deﬁned in cylindrical polar coordinates ρ, θ, z by  11.26  0  E × B  −1   cid:8   F = F0  x cos λz  y cos λz  i +  j +  sin λz k  a  a  ≡ F0ρ  a   cos λz eρ + F0 sin λz k,  where i, j and k are the unit vectors along the Cartesian axes and eρ is the unit vector  x ρ i +  y ρ j.   a  Calculate, as a surface integral, the ﬂux of F through the closed surface  bounded by the cylinders ρ = a and ρ = 2a and the planes z = ±aπ 2.   b  Evaluate the same integral using the divergence theorem.  11.27  The vector ﬁeld F is given by  F =  3x2yz + y3z + xe  −x i +  3xy2z + x3z + yex j +  x3y + y3x + xy2z2 k.  11.28  Calculate  a  directly, and  b  by using Stokes’ theorem the value of the line integral deﬁned by the successive vertices  0, 0, 0 ,  1, 0, 0 ,  1, 0, 1 ,  1, 1, 1 ,  1, 1, 0 ,  0, 1, 0 ,  0, 0, 0 . A vector force ﬁeld F is deﬁned in Cartesian coordinates by  L F · dr, where L is the  three-dimensional  closed contour OABCDEO  cid:13  cid:7    cid:8    cid:8    cid:7    cid:14   F = F0  y3 3a3  y a  +  exy a2  + 1  i +  xy2 a3  +  x + y  a  exy a2  j +  exy a2  k  .  z a  Use Stokes’ theorem to calculate   cid:7    cid:1   where L is the perimeter of the rectangle ABCD given by A =  0, 1, 0 , B =  1, 1, 0 , C =  1, 3, 0  and D =  0, 3, 0 .  0  F · dr,  L  413   LINE, SURFACE AND VOLUME INTEGRALS  11.1 11.3 11.5  11.7 11.9  11.11 11.13 11.15  11.17  11.19  11.23 11.25 11.27  11.11 Hints and answers  tan  Show that ∇ × F = 0. The potential φF  r  = x2z + y2z2 − z.  a  c3 ln 2 i + 2 j +  3c 2 k;  b   −3c4 8 i − c j −  c2 ln 2 k;  c  c4 ln 2 − c. For P , x = y = ab  a2 + b2 1 2. The relevant limits are 0 ≤ θ1 ≤ tan −1 b a  and Show that, in the notation of section 11.3, ∂Q ∂x − ∂P  ∂y = 2x2; I = πa3b 2. C r ×  dr × B . Show that the horizontal sides in the ﬁrst term and the   cid:1  −1 a b  ≤ θ2 ≤ π 2. The total common area is 4ab tan  M = I whole of the second term contribute nothing to the couple.  Note that, if ˆn is the outward normal to the surface, ˆnz · ˆn dl is equal to −dρ.  a  Yes, F0 x − y  exp −r2 a2 ;  b  yes, −F0[ x2 + y2   2a ] exp −r2 a2 ;  c  no, ∇ × F  cid:3 = 0.   b  φ = c + z r.  −1 b a .  A spiral of radius c with its axis parallel to the z-direction and passing through  a, b . The pitch of the spiral is 2πc2. No, because  i  γ is not a closed loop and one. In fact ∇ × f = −2k  cid:3 = 0 shows that f is not conservative.  ii  the line integral must be zero for every closed loop, not just for a particular  a  dS =  2a3 cos θ sin2 θ cos φ i + 2a3 cos θ sin2 θ sin φ j + a2 cos θ sin θ k  dθ dφ.  b  ∇ · r = 3; over the plane z = 0, r · dS = 0. The necessarily common value is 3πa4 2.  The answer is 3  −1, 0, 2 + 1   a  The successive contributions to the integral are:  Identify the expression for ∇ ·  E × B  and use the divergence theorem. 1 − 2e  b  ∇ × F = 2xyz2i − y2z2j + yexk. Show that the contour is equivalent to the sum of two plane square contours in the planes z = 0 and x = 1, the latter being traversed in the negative sense. Integral = 1  3 , −1 + 2e  −1, − 1 2 .  2 e, − 7  6  3e − 5 .  11.21 Write r as ∇  1  √ 2 r2 . 3πα 2 in each case.  414   12  Fourier series  We have already discussed, in chapter 4, how complicated functions may be expressed as power series. However, this is not the only way in which a function may be represented as a series, and the subject of this chapter is the expression of functions as a sum of sine and cosine terms. Such a representation is called a Fourier series. Unlike Taylor series, a Fourier series can describe functions that are not everywhere continuous and or diﬀerentiable. There are also other advantages in using trigonometric terms. They are easy to diﬀerentiate and integrate, their moduli are easily taken and each term contains only one characteristic frequency. This last point is important because, as we shall see later, Fourier series are often used to represent the response of a system to a periodic input, and this response often depends directly on the frequency content of the input. Fourier series are used in a wide variety of such physical situations, including the vibrations of a ﬁnite string, the scattering of light by a diﬀraction grating and the transmission of an input signal by an electronic circuit.  12.1 The Dirichlet conditions  We have already mentioned that Fourier series may be used to represent some functions for which a Taylor series expansion is not possible. The particular conditions that a function f x  must fulﬁl in order that it may be expanded as a Fourier series are known as the Dirichlet conditions, and may be summarised by the following four points:   i  the function must be periodic;  ii  it must be single-valued and continuous, except possibly at a ﬁnite number  of ﬁnite discontinuities;   iii  it must have only a ﬁnite number of maxima and minima within one  period;   iv  the integral over one period of f x  must converge.  415   FOURIER SERIES  f x   x  L  L  Figure 12.1 An example of a function that may be represented as a Fourier series without modiﬁcation.  If the above conditions are satisﬁed then the Fourier series converges to f x  at all points where f x  is continuous. The convergence of the Fourier series at points of discontinuity is discussed in section 12.4. The last three Dirichlet conditions are almost always met in real applications, but not all functions are periodic and hence do not fulﬁl the ﬁrst condition. It may be possible, however, to represent a non-periodic function as a Fourier series by manipulation of the function into a periodic form. This is discussed in section 12.5. An example of a function that may, without modiﬁcation, be represented as a Fourier series is shown in ﬁgure 12.1.  We have stated without proof that any function that satisﬁes the Dirichlet conditions may be represented as a Fourier series. Let us now show why this is a plausible statement. We require that any reasonable function  one that satisﬁes the Dirichlet conditions  can be expressed as a linear sum of sine and cosine terms. We ﬁrst note that we cannot use just a sum of sine terms since sine, being  an odd function  i.e. a function for which f −x  = −f x  , cannot represent even functions  i.e. functions for which f −x  = f x  . This is obvious when we try  to express a function f x  that takes a non-zero value at x = 0. Clearly, since sin nx = 0 for all values of n, we cannot represent f x  at x = 0 by a sine series. Similarly odd functions cannot be represented by a cosine series since cosine is an even function. Nevertheless, it is possible to represent all odd functions by a sine series and all even functions by a cosine series. Now, since all functions may be written as the sum of an odd and an even part,  f x  = 1  2 [ f x  + f −x ] + 1  2 [ f x  − f −x ]  = feven x  + fodd x ,  416   12.2 THE FOURIER COEFFICIENTS  we can write any function as the sum of a sine series and a cosine series.  All the terms of a Fourier series are mutually orthogonal, i.e. the integrals, over  one period, of the product of any two terms have the following properties:  x0+L  sin  2πrx  2πpx  cos  dx = 0  for all r and p,   12.1    cid:21   cid:21   cid:21   x0  x0   cid:7   cid:7   cid:7   L  L   cid:8   cid:8   cid:8    cid:8   cid:8   cid:8    cid:7   cid:7   cid:7   L  L  L  x0+L  cos  2πrx  2πpx  cos  dx =  x0+L  sin  x0  2πrx  L  2πpx  sin  dx =   L  0  for r = p = 0, 1 2 L for r = p > 0, 0  for r  cid:3 = p,  for r = p = 0, 1 2 L for r = p > 0, 0  for r  cid:3 = p,   12.2    12.3   where r and p are integers greater than or equal to zero; these formulae are easily derived. A full discussion of why it is possible to expand a function as a sum of mutually orthogonal functions is given in chapter 17.  The Fourier series expansion of the function f x  is conventionally written   cid:13   ∞ cid:4   r=1  a0 2   cid:7    cid:8   2πrx  L   cid:7    cid:8  cid:14   ,  2πrx  L  f x  =  +  ar cos  + br sin   12.4   where a0, ar, br are constants called the Fourier coeﬃcients. These coeﬃcients are analogous to those in a power series expansion and the determination of their numerical values is the essential step in writing a function as a Fourier series.  This chapter continues with a discussion of how to ﬁnd the Fourier coeﬃcients for particular functions. We then discuss simpliﬁcations to the general Fourier series that may save considerable eﬀort in calculations. This is followed by the alternative representation of a function as a complex Fourier series, and we conclude with a discussion of Parseval’s theorem.  We have indicated that a series that satisﬁes the Dirichlet conditions may be written in the form  12.4 . We now consider how to ﬁnd the Fourier coeﬃcients for any particular function. For a periodic function f x  of period L we will ﬁnd that the Fourier coeﬃcients are given by  12.2 The Fourier coeﬃcients   cid:21   cid:21    cid:7   cid:7    cid:8   cid:8   ar =  br =  2 L  2 L  x0+L  x0+L  x0  x0  f x  cos  f x  sin  2πrx  2πrx  L  L  dx,  dx,  where x0 is arbitrary but is often taken as 0 or −L 2. The apparently arbitrary factor 1 2 which appears in the a0 term in  12.4  is included so that  12.5  may  417   12.5    12.6    FOURIER SERIES  apply for r = 0 as well as r > 0. The relations  12.5  and  12.6  may be derived as follows.  Suppose the Fourier series expansion of f x  can be written as in  12.4 ,   cid:13   ∞ cid:4   r=1   cid:7    cid:8   2πrx  L  f x  =  +  ar cos  + br sin  Then, multiplying by cos 2πpx L , integrating over one full period in x and changing the order of the summation and integration, we get   cid:8  cid:14   .  2πrx  L   cid:7    cid:8   a0 2   cid:7    cid:8    cid:21   x0+L  x0  f x  cos  dx =  2πpx  L   cid:7   cos  x0+L   cid:21  ∞ cid:4  ∞ cid:4   r=1  x0  r=1  a0 2  +  +  x0+L   cid:21   cid:21   x0  x0  ar  br  x0+L  sin  2πpx  L   cid:7   cid:7   cos   cid:8   cid:8   dx  2πrx  L  2πrx  L   cid:7   cid:7    cid:8   cid:8   cos  cos  2πpx  L  2πpx  L  dx  dx.   12.7   We can now ﬁnd the Fourier coeﬃcients by considering  12.7  as p takes diﬀerent values. Using the orthogonality conditions  12.1 – 12.3  of the previous section, we ﬁnd that when p = 0  12.7  becomes   cid:21   x0+L  x0  f x dx =  L.  a0 2   cid:8    cid:7   When p  cid:3 = 0 the only non-vanishing term on the RHS of  12.7  occurs when  r = p, and so   cid:21   x0+L  x0  f x  cos  2πrx  L  dx =  L.  ar 2  The other Fourier coeﬃcients br may be found by repeating the above process but multiplying by sin 2πpx L  instead of cos 2πpx L   see exercise 12.2 .   cid:1 Express the square-wave function illustrated in ﬁgure 12.2 as a Fourier series.  Physically this might represent the input to an electrical circuit that switches between a high and a low state with time period T . The square wave may be represented by    f t  =  −1 for − 1 +1 for 0 ≤ t < 1  2 T ≤ t < 0,  2 T .  In deriving the Fourier coeﬃcients, we note ﬁrstly that the function is an odd function and so the series will contain only sine terms  this simpliﬁcation is discussed further in the  418   12.3 SYMMETRY CONSIDERATIONS  f t   1  0  −1  − T  2  T 2  t  Figure 12.2 A square-wave function.  br =  f t  sin  dt   cid:21   cid:21   T  2  −T  2  T  2   cid:7    cid:8   2πrt T   cid:8   dt   cid:7   sin  2πrt  T  0  [1 −  −1 r] .  2 T  4 T 2 πr  =  =   cid:7   4 π  following section . To evaluate the coeﬃcients in the sine series we use  12.6 . Hence  Thus the sine coeﬃcients are zero if r is even and equal to 4  πr  if r is odd. Hence the Fourier series for the square-wave function may be written as  + where ω = 2π T is called the angular frequency.  cid:2   sin ωt +  f t  =  3  5  sin 3ωt  sin 5ωt  + ···  ,   12.8    cid:8   12.3 Symmetry considerations  The example in the previous section employed the useful property that since the function to be represented was odd, all the cosine terms of the Fourier series were absent. It is often the case that the function we wish to express as a Fourier series has a particular symmetry, which we can exploit to reduce the calculational labour of evaluating Fourier coeﬃcients. Functions that are symmetric or antisymmetric about the origin  i.e. even and odd functions respectively  admit particularly useful simpliﬁcations. Functions that are odd in x have no cosine terms  see section 12.1  and all the a-coeﬃcients are equal to zero. Similarly, functions that are even in x have no sine terms and all the b-coeﬃcients are zero. Since the Fourier series of odd or even functions contain only half the coeﬃcients required for a general periodic function, there is a considerable reduction in the algebra needed to ﬁnd a Fourier series.  The consequences of symmetry or antisymmetry of the function about the quarter period  i.e. about L 4  are a little less obvious. Furthermore, the results  419   FOURIER SERIES  are not used as often as those above and the remainder of this section can be omitted on a ﬁrst reading without loss of continuity. The following argument gives the required results.  Suppose that f x  has even or odd symmetry about L 4, i.e. f L 4 − x  = ±f x − L 4 . For convenience, we make the substitution s = x − L 4 and hence f −s  = ±f s . We can now see that   cid:21   br =  2 L  x0+L  f s  sin  2πrs  L  +  πr 2  ds,  where the limits of integration have been left unaltered since f is, of course, periodic in s as well as in x. If we use the expansion   cid:7    cid:8   sin  2πrs  L  +  πr 2  x0   cid:7    cid:8   2πrs  L  = sin  cos  + cos   cid:8    cid:9    cid:10   2πrs  L  sin  πr 2  ,  we can immediately see that the trigonometric part of the integrand is an odd function of s if r is even and an even function of s if r is odd. Hence if f s  is even and r is even then the integral is zero, and if f s  is odd and r is odd then the integral is zero. Similar results can be derived for the Fourier a-coeﬃcients and we conclude that   cid:7    cid:9    cid:10   πr 2   cid:8    cid:7    i  if f x  is even about L 4 then a2r+1 = 0 and b2r = 0,  ii  if f x  is odd about L 4 then a2r = 0 and b2r+1 = 0.  All the above results follow automatically when the Fourier coeﬃcients are evaluated in any particular case, but prior knowledge of them will often enable some coeﬃcients to be set equal to zero on inspection and so substantially reduce the computational labour. As an example, the square-wave function shown in ﬁgure 12.2 is  i  an odd function of t, so that all ar = 0, and  ii  even about the point t = T  4, so that b2r = 0. Thus we can say immediately that only sine terms of odd harmonics will be present and therefore will need to be calculated; this is conﬁrmed in the expansion  12.8 .  12.4 Discontinuous functions  The Fourier series expansion usually works well for functions that are discon- tinuous in the required range. However, the series itself does not produce a discontinuous function and we state without proof that the value of the ex- panded f x  at a discontinuity will be half-way between the upper and lower values. Expressing this more mathematically, at a point of ﬁnite discontinuity, xd, the Fourier series converges to  [ f xd +  cid:4   + f xd −  cid:4  ].  1 2 lim  cid:4 →0  At a discontinuity, the Fourier series representation of the function will overshoot its value. Although as more terms are included the overshoot moves in position  420   12.4 DISCONTINUOUS FUNCTIONS   a   − T  2   c   − T  2  1  1  −1  −1   b   − T  2   d   − T  2  T 2  T 2  1  1  −1  −1  T 2  T 2  δ  Figure 12.3 The convergence of a Fourier series expansion of a square-wave function, including  a  one term,  b  two terms,  c  three terms and  d  20 terms. The overshoot δ is shown in  d .  arbitrarily close to the discontinuity, it never disappears even in the limit of an inﬁnite number of terms. This behaviour is known as Gibbs’ phenomenon. A full discussion is not pursued here but suﬃce it to say that the size of the overshoot is proportional to the magnitude of the discontinuity.  cid:1 Find the value to which the Fourier series of the square-wave function discussed in sec- tion 12.2 converges at t = 0.  It can be seen that the function is discontinuous at t = 0 and, by the above rule, we expect the series to converge to a value half-way between the upper and lower values, in other words to converge to zero in this case. Considering the Fourier series of this function,  12.8 , we see that all the terms are zero and hence the Fourier series converges to zero as expected. The Gibbs phenomenon for the square-wave function is shown in ﬁgure 12.3.  cid:2   421   FOURIER SERIES  0  0  0  0  L  L  L  L  2L  2L  2L   a    b    c    d   Figure 12.4 Possible periodic extensions of a function.  12.5 Non-periodic functions  We have already mentioned that a Fourier representation may sometimes be used for non-periodic functions. If we wish to ﬁnd the Fourier series of a non-periodic function only within a ﬁxed range then we may continue the function outside the range so as to make it periodic. The Fourier series of this periodic function would then correctly represent the non-periodic function in the desired range. Since we are often at liberty to extend the function in a number of ways, we can sometimes make it odd or even and so reduce the calculation required. Figure 12.4 b  shows the simplest extension to the function shown in ﬁgure 12.4 a . However, this extension has no particular symmetry. Figures 12.4 c ,  d  show extensions as odd and even functions respectively with the beneﬁt that only sine or cosine terms appear in the resulting Fourier series. We note that these last two extensions give a function of period 2L.  In view of the result of section 12.4, it must be added that the continuation must not be discontinuous at the end-points of the interval of interest; if it is the series will not converge to the required value there. This requirement that the series converges appropriately may reduce the choice of continuations. This is discussed further at the end of the following example.   cid:1 Find the Fourier series of f x  = x2 for 0 < x ≤ 2.  We must ﬁrst make the function periodic. We do this by extending the range of interest to  −2 < x ≤ 2 in such a way that f x  = f −x  and then letting f x + 4k  = f x , where k is Fourier series will faithfully represent f x  in the range, −2 < x ≤ 2, although not outside  any integer. This is shown in ﬁgure 12.5. Now we have an even function of period 4. The  it. Firstly we note that since we have made the speciﬁed function even in x by extending  422   12.5 NON-PERIODIC FUNCTIONS  f x  = x2  0  L  −2  2  x  Figure 12.5 f x  = x2, 0 < x ≤ 2, with the range extended to give periodicity.  the range, all the coeﬃcients br will be zero. Now we apply  12.5  and  12.6  with L = 4 to determine the remaining coeﬃcients:  ar =  x2 cos  2 4  2  −2  2πrx  4  dx =  x2 cos  dx,  where the second equality holds because the function is even in x. Thus   cid:7   cid:9    cid:8   cid:10  cid:14   cid:10  cid:23   2  0   cid:21   2  0  4 4   cid:21    cid:9    cid:10   πrx  2   cid:10   πrx   cid:9   2  dx   cid:10    cid:9   πrx   cid:9   2  πrx  2  x sin  2   cid:21   − 4  πr  0  − 8 π2r2  2  0  2  0  cos  πrx  2  dx   cid:21   cid:13   ar =  x2 sin   cid:22   2 πr  8  =  =  =  π2r2 16 π2r2 16 π2r2  x cos  cos πr   −1 r.  Since this expression for ar has r2 in its denominator, to evaluate a0 we must return to the original deﬁnition,  ar =  f x  cos  dx.   cid:10   πrx  2   cid:9   cid:21   cid:10   0  2  4 4   cid:9    cid:21   2  −2  2 4   cid:21   2  −2  2 4  ∞ cid:4   r=1  a0 =  x2 dx =  x2 dx =  8 3  .  From this we obtain  The ﬁnal expression for f x  is then  x2 =  + 16  4 3   −1 r  π2r2  cos  πrx  2  for 0 < x ≤ 2.  cid:2   We note that in the above example we could have extended the range so as  to make the function odd. In other words we could have set f x  = −f −x  and  then made f x  periodic in such a way that f x + 4  = f x . In this case the resulting Fourier series would be a series of just sine terms. However, although this will faithfully represent the function inside the required range, it does not  423   FOURIER SERIES  converge to the correct values of f x  = ±4 at x = ±2; it converges, instead, to  zero, the average of the values at the two ends of the range.  12.6 Integration and diﬀerentiation  It is sometimes possible to ﬁnd the Fourier series of a function by integration or diﬀerentiation of another Fourier series. If the Fourier series of f x  is integrated term by term then the resulting Fourier series converges to the integral of f x . Clearly, when integrating in such a way there is a constant of integration that must be found. If f x  is a continuous function of x for all x and f x  is also periodic then the Fourier series that results from diﬀerentiating term by term converges to  cid:7   x  itself satisﬁes the Dirichlet conditions. These properties f of Fourier series may be useful in calculating complicated Fourier series, since simple Fourier series may easily be evaluated  or found from standard tables  and often the more complicated series can then be built up by integration and or diﬀerentiation.   x , provided that f   cid:7    cid:1 Find the Fourier series of f x  = x3 for 0 < x ≤ 2.  In the example discussed in the previous section we found the Fourier series for f x  = x2 in the required range. So, if we integrate this term by term, we obtain  ∞ cid:4   r=1   −1 r  π3r3   cid:9    cid:10   sin  + c,  πrx  2  x3 3  4 3  =  x + 32  where c is, so far, an arbitrary constant. We have not yet found the Fourier series for x3 because the term 4 3 x appears in the expansion. However, by now diﬀerentiating the same initial expression for x2 we obtain   cid:10   sin  πrx  2  .  ∞ cid:4  2x = −8  cid:9   r=1  πr   −1 r  cid:10   sin  + 96  πrx  2  ∞ cid:4   r=1   −1 r  πr   cid:9  ∞ cid:4   r=1   cid:9    cid:10    −1 r  π3r3  sin  + c.  πrx  2  x3 = −16  We can now write the full Fourier expansion of x3 as  Finally, we can ﬁnd the constant, c, by considering f 0 . At x = 0, our Fourier expansion gives x3 = c since all the sine terms are zero, and hence c = 0.  cid:2   12.7 Complex Fourier series  As a Fourier series expansion in general contains both sine and cosine parts, it may be written more compactly using a complex exponential expansion. This simpliﬁcation makes use of the property that exp irx  = cos rx + i sin rx. The complex Fourier series expansion is written  f x  =  cr exp   12.9    cid:7    cid:8   2πirx  L  ,  ∞ cid:4   r=−∞  424   where the Fourier coeﬃcients are given by  12.7 COMPLEX FOURIER SERIES   cid:21   x0+L  cr =  1 L  x0   cid:8    cid:7  − 2πirx  L  f x  exp  dx.   12.10   relation  This relation can be derived, in a similar manner to that of section 12.2, by mul-  tiplying  12.9  by exp −2πipx L  before integrating and using the orthogonality   cid:21   x0+L  exp  x0   cid:8    cid:7  − 2πipx  L   cid:7    cid:8     exp  2πirx  L  dx =  L for r = p,  for r  cid:3 = p.  0  The complex Fourier coeﬃcients in  12.9  have the following relations to the real Fourier coeﬃcients:  cr = 1 c−r = 1  2  ar − ibr , 2  ar + ibr . ∗ r , where the asterisk represents complex   12.11   Note that if f x  is real then c−r = c conjugation.   cid:1 Find a complex Fourier series for f x  = x in the range −2 < x < 2.  cid:21  Using  12.10 , for r  cid:3 = 0,  cid:13    cid:8   cid:8  cid:14   x exp  cr =  dx   cid:21   2   cid:7   cid:7  − πirx 2 − πirx 2  1 −2 4 − x 2πir   cid:7   cid:7   2   cid:13   1  1  exp  2πir  exp  r2π2  − πirx 2 − πirx 2   cid:8   cid:8  cid:14   dx  2  −2  2  +  =  = − 1  πir  =  2i πr  exp  −2  −2 [exp −πir  + exp πir ] + cos πr − 2i ∞ cid:4   sin πr =  2i πr  r2π2   −1 r.  cid:7   2i −1 r   cid:8   x =  r=−∞ r cid:3 =0  rπ  exp  πirx  2  .  For r = 0, we ﬁnd c0 = 0 and hence   12.12   We note that the Fourier series derived for x in section 12.6 gives ar = 0 for all r and  and so, using  12.11 , we conﬁrm that cr and c−r have the forms derived above. It is also ∗ r = c−r holds, as we expect since f x  is real.  cid:2  apparent that the relationship c  br = − 4 −1 r  ,  πr  425   FOURIER SERIES  12.8 Parseval’s theorem   cid:21   1 L  x0  x0+L  f x 2dx =  ∞ cid:4   cid:5   r=−∞  1 2 a0  cr2  cid:6   2  ∞ cid:4   r=1  Parseval’s theorem gives a useful way of relating the Fourier coeﬃcients to the function that they describe. Essentially a conservation law, it states that  =  + 1 2   a2  r + b2 r  .   12.13   In a more memorable form, this says that the sum of the moduli squared of  the complex Fourier coeﬃcients is equal to the average value of f x 2 over one  period. Parseval’s theorem can be proved straightforwardly by writing f x  as a Fourier series and evaluating the required integral, but the algebra is messy. Therefore, we shall use an alternative method, for which the algebra is simple and which in fact leads to a more general form of the theorem.  Let us consider two functions f x  and g x , which are  or can be made  periodic with period L and which have Fourier series  expressed in complex form   f x  =  cr exp  g x  =  γr exp  ∞ cid:4  ∞ cid:4   r=−∞  r=−∞  ∞ cid:4   r=−∞   cid:8   cid:8   ,  ,   cid:7   cid:7   2πirx  L  L  2πirx   cid:7    cid:8   2πirx  L  .  ∗  f x g   x  =  crg   x  exp  where cr and γr are the complex Fourier coeﬃcients of f x  and g x  respectively. Thus  Integrating this equation with respect to x over the interval  x0, x0 + L  and dividing by L, we ﬁnd   cid:21   x0+L  1 L  x0  ∗  f x g   x  dx =   cid:7   cid:8   cid:7 −2πirx  2πirx  L  L  dx   cid:8    cid:14 ∗  dx  x0+L  ∗  g   x  exp  x0+L  x0  g x  exp  where the last equality uses  12.10 . Finally, if we let g x  = f x  then we obtain Parseval’s theorem  12.13 . This result can be proved in a similar manner using  ∗   cid:21    cid:21   x0  ∞ cid:4  ∞ cid:4  ∞ cid:4   r=−∞  r=−∞  1 L   cid:13   cr  cr  1 L ∗ r ,  crγ  r=−∞  =  =  426   12.9 EXERCISES  the sine and cosine form of the Fourier series, but the algebra is slightly more complicated.  Parseval’s theorem is sometimes used to sum series. However, if one is presented with a series to sum, it is not usually possible to decide which Fourier series should be used to evaluate it. Rather, useful summations are nearly always found serendipitously. The following example shows the evaluation of a sum by a Fourier series method.  cid:1 Using Parseval’s theorem and the Fourier series for f x  = x2 found in section 12.5, calculate the sum   cid:11 ∞  −4.  r=1 r  Firstly we ﬁnd the average value of [ f x ]2 over the interval −2 < x ≤ 2:  Now we evaluate the right-hand side of  12.13 :   cid:5    cid:6   ∞ cid:4   1 4  2  −2  x4 dx =  ∞ cid:4   16 5  .   cid:5    cid:6   1 2 a0  2  + 1 2  a2 r + 1  2  b2 n =  4 3  2  + 1 2  1  1  ∞ cid:4   r=1  162 π4r4 .  Equating the two expression we ﬁnd   cid:21   ∞ cid:4   r=1  1 r4  =  .  cid:2   π4 90  12.9 Exercises  12.1 12.2  12.3  Prove the orthogonality relations stated in section 12.1. Derive the Fourier coeﬃcients br in a similar manner to the derivation of the ar in section 12.2. Which of the following functions of x could be represented by a Fourier series over the range indicated?   a  tanh  b  tan x,  −1 x ,  c   sin x−1 2,   d  cos  e  x sin 1 x ,  −1 sin 2x ,  −∞ < x < ∞; −∞ < x < ∞; −∞ < x < ∞; −∞ < x < ∞; −1 < x ≤ π −π  −1, cyclically repeated.  12.4  12.5  By moving the origin of t to the centre of an interval in which f t  = +1, i.e. by changing to a new independent variable t 4 T , express the square-wave function in the example in section 12.2 as a cosine series. Calculate the Fourier Find the Fourier series of the function f x  = x in the range −π < x ≤ π. Hence coeﬃcients involved  a  directly and  b  by changing the variable in result  12.8 .  = t − 1   cid:7   show that  12.6  For the function  ﬁnd  a  the Fourier sine series and  b  the Fourier cosine series. Which would  1 − 1  3  − 1 7  1 5  +  + ··· =  π 4  .  f x  = 1 − x,  0 ≤ x ≤ 1,  427   FOURIER SERIES  12.7  12.8  12.9  12.10  12.11  12.12  be better for numerical evaluation? Relate your answer to the relevant periodic continuations. For the continued functions used in exercise 12.6 and the derived corresponding series, consider  i  their derivatives and  ii  their integrals. Do they give meaningful equations? You will probably ﬁnd it helpful to sketch all the functions involved.  The function y x  = x sin x for 0 ≤ x ≤ π is to be represented by a Fourier series  −3 and that alternate terms are missing.  Find the Fourier coeﬃcients in the expansion of f x  = exp x over the range  of period 2π that is either even or odd. By sketching the function and considering its derivative, determine which series will have the more rapid convergence. Find the full expression for the better of these two series, showing that the convergence  ∼ n −1 < x < 1. What value will the expansion have when x = 2?  cid:1  Consider the function f x  = exp −x2  in the range 0 ≤ x ≤ 1. Show how it  By integrating term by term the Fourier series found in the previous question and using the Fourier series for f x  = x found in section 12.6, show that exp x dx = exp x + c. Why is it not possible to show that d exp x  dx = exp x  by diﬀerentiating the Fourier series of f x  = exp x in a similar manner?  should be continued to give as its Fourier series a series  the actual form is not wanted   a  with only cosine terms,  b  with only sine terms,  c  with period 1 and  d  with period 2.  Would there be any diﬀerence between the values of the last two series at  i   by:  x = 0,  ii  x = 1? Find, without calculation, which terms will be present in the Fourier series for  the periodic functions f t , of period T , that are given in the range −T  2 to T  2  a  f t  = 2 for 0 ≤ t < T  4, f = 1 for T  4 ≤ t < T  2;  b  f t  = exp[− t − T  4 2];  c  f t  = −1 for −T  2 ≤ t < −3T  8 and 3T  8 ≤ t < T  2, f t  = 1 for −T  8 ≤ t < T  8; the graph of f is completed by two straight lines in the  remaining ranges so as to form a continuous function.  12.13  Consider the representation as a Fourier series of the displacement of a string  lying in the interval 0 ≤ x ≤ L and ﬁxed at its ends, when it is pulled aside by y0  at the point x = L 4. Sketch the continuations for the region outside the interval that will   a  produce a series of period L,  b  produce a series that is antisymmetric about x = 0, and  c  produce a series that will contain only cosine terms.  d  What are  i  the periods of the series in  b  and  c  and  ii  the value of the  ‘a0-term’ in  c ?   e  Show that a typical term of the series obtained in  b  is  12.14  Show that the Fourier series for the function y x  = x in the range −π ≤ x < π  is  32y0 3n2π2  nπ 4  sin  nπx  .  L  By integrating this equation term by term from 0 to x, ﬁnd the function g x  whose Fourier series is  y x  =  cos 2m + 1 x   2m + 1 2  .  sin  ∞ cid:4   m=0  π 2  π  − 4 ∞ cid:4   m=0  428  4 π  sin 2m + 1 x   2m + 1 3  .   12.9 EXERCISES  Deduce the value of the sum S of the series − 1 73  1 − 1  1 53  33  +  + ··· .  12.15  Using the result of exercise 12.14, determine, as far as possible by inspection, the forms of the functions of which the following are the Fourier series:  1 9  1 27   cid:13   ∞ cid:4   cid:11   n=1   a    b    c   cos θ +  cos 3θ +  cos 5θ + ··· ;  sin θ +  sin 3θ +  sin 5θ + ··· ;  1 25  1  125   cid:14   − 4L2 π2  cos  − 1 4  πx L  cos  +  cos  2πx L  1 9  3πx L  − ···  .  L2 3   cid:11    You may ﬁnd it helpful to ﬁrst set x = 0 in the quoted result and so obtain values for So = By ﬁnding a cosine Fourier series of period 2 for the function f t  that takes the  −2 and other sums derivable from it.   form f t  = cosh t − 1  in the range 0 ≤ t ≤ 1, prove that   2m + 1   1  n2π2 + 1  =  1  e2 − 1  .  Deduce values for the sums Find the  real  Fourier series of period 2 for f x  = cosh x and g x  = x2 in the range −1 ≤ x ≤ 1. By integrating the series for f x  twice, prove that  −1 over odd n and even n separately.  cid:7    n2π2 + 1    cid:8    −1 n+1  n2π2 n2π2 + 1   =  1 2  1  sinh 1  − 5 6  .  ∞ cid:4   n=1  Express the function f x  = x2 as a Fourier sine series in the range 0 < x ≤ 2 and show that it converges to zero at x = ±2.  Demonstrate explicitly for the square-wave function discussed in section 12.2 that Parseval’s theorem  12.13  is valid. You will need to use the relationship  12.16  12.17  12.18  12.19  Show that a ﬁlter that transmits frequencies only up to 8π T will still transmit more than 90% of the power in such a square-wave voltage signal.  Show that the Fourier series for  sin θ in the range −π ≤ θ ≤ π is given by  12.20  ∞ cid:4   m=0  1   2m + 1 2  =  π2 8  .  cos 2mθ  ∞ cid:4  4m2 − 1 ∞ cid:4   m=1  .   sin θ =  − 4  π  2 π  ∞ cid:4   m=1  1  4m2 − 1  and  1  16m2 − 1  .  m=1  429  By setting θ = 0 and θ = π 2, deduce values for   12.21  Find the complex Fourier series for the periodic function of period 2π deﬁned in  the range −π ≤ x ≤ π by y x  = cosh x. By setting x = 0 prove that  12.22  The repeating output from an electronic oscillator takes the form of a sine wave  f t  = sin t for 0 ≤ t ≤ π 2; it then drops instantaneously to zero and starts  again. The output is to be represented by a complex Fourier series of the form  ∗ Sketch the function and ﬁnd an expression for cn. Verify that c−n = c n. Demon- strate that setting t = 0 and t = π 2 produces diﬀering values for the sum  FOURIER SERIES  ∞ cid:4   n=1   cid:9    cid:10   − 1  .   −1 n  n2 + 1  =  1 2  π  sinh π  ∞ cid:4   n=−∞  ∞ cid:4   n=1  cne4nti.  1  16n2 − 1  .  12.23  12.24  Determine the correct value and check it using the result of exercise 12.20. Apply Parseval’s theorem to the series found in the previous exercise and so derive a value for the sum of the series  17  15 2  +  65  63 2  +  145  143 2  + ··· +  16n2 + 1  16n2 − 1 2  + ··· .  A string, anchored at x = ±L 2, has a fundamental vibration frequency of 2L c,  where c is the speed of transverse waves on the string. It is pulled aside at its centre point by a distance y0 and released at time t = 0. Its subsequent motion can be described by the series  ∞ cid:4   n=1  y x, t  =  an cos  nπx  L  cos  nπct  .  L   cid:11 ∞  Find a general expression for an and show that only the odd harmonics of the fundamental frequency are present in the sound generated by the released string. By applying Parseval’s theorem, ﬁnd the sum S of the series Show that Parseval’s theorem for two real functions whose Fourier expansions have cosine and sine coeﬃcients an, bn and αn, βn takes the form  0  2m + 1   −4.  12.25   cid:21   1 L  L  0  f x g   x  dx =  a0α0 +   anαn + bnβn .  ∗  1 4  ∞ cid:4   n=1  1 2   a  Demonstrate that for g x  = sin mx or cos mx this reduces to the deﬁnition  of the Fourier coeﬃcients.   b  Explicitly verify the above result for the case in which f x  = x and g x  is  the square-wave function, both in the interval −1 ≤ x ≤ 1.  12.26  An odd function f x  of period 2π is to be approximated by a Fourier sine series having only m terms. The error in this approximation is measured by the square deviation   cid:16    cid:21   π  −π  f x  − m cid:4   n=1   cid:17   2  Em =  bn sin nx  dx.  By diﬀerentiating Em with respect to the coeﬃcients bn, ﬁnd the values of bn that minimise Em.  430   12.10 HINTS AND ANSWERS  0  1  0  1  0  1  0  2  4   a    b    c    d   Figure 12.6 Continuations of exp −x2  in 0 ≤ x ≤ 1 to give:  a  cosine terms  only;  b  sine terms only;  c  period 1;  d  period 2.  Sketch the graph of the function f x , where   cid:12 −x π + x   x x − π   f x  =  for −π ≤ x < 0, for 0 ≤ x < π.  If f x  is to be approximated by the ﬁrst three terms of a Fourier sine series, what values should the coeﬃcients have so as to minimise E3? What is the resulting value of E3?  12.1  12.3  12.5 12.7  12.9  12.11 12.13 12.15  12.17  12.19  12.10 Hints and answers   12.8 .  2 x2 − 1  −1 sin nx; set x = π 2.   cid:11 ∞ 1  −1 n+1n  Note that the only integral of a sinusoid around a complete cycle of length L that is not zero is the integral of cos 2πnx L  when n = 0. Only  c . In terms of the Dirichlet conditions  section 12.1 , the others fail as follows:  a   i ;  b   ii ;  d   ii ;  e   iii . f x  = 2  i  Series  a  from exercise 12.6 does not converge and cannot represent the  function y x  = −1. Series  b  reproduces the square-wave function of equation  ii  Series  a  gives the series for y x  = −x − 1 2 in the range −1 ≤ x ≤ 0 2 x2 − 1 2 in the range 0 ≤ x ≤ 1. Series  b  gives the series for and for y x  = x − 1 1  cid:11 ∞ 2 in the range −1 ≤ x ≤ 0 and for y x  = x − 1 y x  = x + 1 2 x2 + 1 range 0 ≤ x ≤ 1. 2 in the −1[cos nπx  − nπ sin nπx ] 1  −1 n 1 + n2π2  f x  =  sinh 1  The series will converge to the same value as it does at x = 0, i.e. f 0  = 1. −4  2,  ii  e −1. −1  2,  ii   1 + e −1  2;  d   i   1 + e See ﬁgure 12.6.  c   i   1 + e  d   i  The periods are both 2L;  ii  y0 2. 4  Se + So , yielding So − Se = π2 12 and −2 then Se = 1 So = π2 8. If Se = Se + So = π2 6.  a   π 4  π 2−θ ;  b   πθ 4  π 2−θ 2  from integrating  a .  c  Even function; average value L2 3; y 0  = 0; y L  = L2; probably y x  = x2. Compare with the worked example in section 12.5. cosh x =  sinh 1 [1 + 2 this form must be recovered. Use x2 = 1 the quadratic term arising from the constants of integration; there is no linear term.   cid:11 ∞ n=1 −1 n cos nπx   n2π2 + 1 ] and after integrating twice  −1 n cos nπx   n2π2 ] to eliminate   cid:11  Cn2 =  4 π2  × 2 ×  π2 8 ; the values n = ±1,  C± 2m+1  = ∓2i [ 2m + 1 π]; ±3 contribute > 90% of the total.  2 x2 + 1   cid:11    cid:11   3 +4  2  1 + 2   2m   .  431   FOURIER SERIES  12.21  12.23 12.25  cn = [ −1 n sinh π] [π 1 + n2 ]. Having set x = 0, separate out the n = 0 term and note that  −1 n =  −1   π2 − 8  16.  b  All an and αn are zero; bn = 2 −1 n+1  nπ  and βn = 4  nπ . You will need  −n.  the result quoted in exercise 12.19.  432   13  Integral transforms  In the previous chapter we encountered the Fourier series representation of a periodic function in a ﬁxed interval as a superposition of sinusoidal functions. It is often desirable, however, to obtain such a representation even for functions deﬁned over an inﬁnite interval and with no particular periodicity. Such a representation is called a Fourier transform and is one of a class of representations called integral transforms.  We begin by considering Fourier transforms as a generalisation of Fourier series. We then go on to discuss the properties of the Fourier transform and its applications. In the second part of the chapter we present an analogous discussion of the closely related Laplace transform.  13.1 Fourier transforms  The Fourier transform provides a representation of functions deﬁned over an inﬁnite interval and having no particular periodicity, in terms of a superposition of sinusoidal functions. It may thus be considered as a generalisation of the Fourier series representation of periodic functions. Since Fourier transforms are often used to represent time-varying functions, we shall present much of our discussion in terms of f t , rather than f x , although in some spatial examples f x  will be the more natural notation and we shall use it as appropriate. Our only requirement on f t  will be that   cid:1  ∞ −∞ f t  dt is ﬁnite.  In order to develop the transition from Fourier series to Fourier transforms, we ﬁrst recall that a function of period T may be represented as a complex Fourier series, cf.  12.9 ,  f t  =  cr e2πirt T =  cr eiωrt,   13.1   where ωr = 2πr T . As the period T tends to inﬁnity, the ‘frequency quantum’  ∞ cid:4   r=−∞  ∞ cid:4   r=−∞  433   INTEGRAL TRANSFORMS  c ω  exp iωt  − 2π −1  T  0  0  2π T  1  4π T  2  ωr  r  Figure 13.1 The relationship between the Fourier terms for a function of period T and the Fourier integral  the area below the solid line  of the function.  ∆ω = 2π T becomes vanishingly small and the spectrum of allowed frequencies ωr becomes a continuum. Thus, the inﬁnite sum of terms in the Fourier series becomes an integral, and the coeﬃcients cr become functions of the continuous variable ω, as follows.  We recall, cf.  12.10 , that the coeﬃcients cr in  13.1  are given by −iωr t dt,  −2πirt T dt =  f t  e  f t  e  T  2  T  2  cr =  ∆ω 2π  −T  2   cid:21   1 T  −T  2   cid:21   where we have written the integral in two alternative forms and, for convenience,  made one period run from −T  2 to +T  2 rather than from 0 to T . Substituting  from  13.2  into  13.1  gives   cid:21   ∞ cid:4   r=−∞  f t  =  ∆ω 2π  T  2  −T  2  −iωr u du eiωrt.  f u  e   13.2    13.3   At this stage ωr is still a discrete function of r equal to 2πr T .  The solid points in ﬁgure 13.1 are a plot of  say, the real part of  cr eiωr t as a function of r  or equivalently of ωr  and it is clear that  2π T  cr eiωrt gives the area of the rth broken-line rectangle. If T tends to ∞ then ∆ω  = 2π T    becomes inﬁnitesimal, the width of the rectangles tends to zero and, from the mathematical deﬁnition of an integral,   cid:21  ∞  ∞ cid:4   r=−∞ In this particular case  g ωr  eiωr t → 1  ∆ω 2π  −∞ g ω  eiωt dω.  2π  g ωr  =  −iωr u du,  f u  e  T  2  −T  2   cid:21   434   and  13.3  becomes  13.1 FOURIER TRANSFORMS   cid:21  ∞   cid:21  ∞  f t  =  1 2π  −∞ dω eiωt  −∞ du f u  e  −iωu.   13.4    13.5    13.6   This result is known as Fourier’s inversion theorem.  From it we may deﬁne the Fourier transform of f t  by  3  f ω  =  1√ 2π  −iωt dt,  −∞ f t  e  and its inverse by   cid:21  ∞  cid:21  ∞  3  1√ 2π  f t  = √ Including the constant 1   2π in the deﬁnition of 3  f ω  eiωt dω.  −∞  existence as T → ∞ is assumed here without proof  is clearly arbitrary, the only  f ω   whose mathematical  requirement being that the product of the constants in  13.5  and  13.6  should equal 1  2π . Our deﬁnition is chosen to be as symmetric as possible.   cid:1  Find the Fourier transform of the exponential decay function f t  = 0 for t < 0 and f t  = A e  −λt for t ≥ 0  λ > 0 .  Using the deﬁnition  13.5  and separating the integral into two parts, −iωt dt  −λt e  f ω  =  e  0  3   cid:21    cid:21  ∞  0  −iωt dt + − λ+iω t   cid:14 ∞  A√ 2π  λ + iω  0   cid:13   −∞ 0  e − e  1√ 2π A√ 2π A√ 2π λ + iω   ,  = 0 +  =  which is the required transform. It is clear that the multiplicative constant A does not aﬀect the form of the transform, merely its amplitude. This transform may be veriﬁed by resubstitution of the above result into  13.6  to recover f t , but evaluation of the integral requires the use of complex-variable contour integration  chapter 24 .  cid:2   13.1.1 The uncertainty principle  An important function that appears in many areas of physical science, either precisely or as an approximation to a physical situation, is the Gaussian or normal distribution. Its Fourier transform is of importance both in itself and also because, when interpreted statistically, it readily illustrates a form of uncertainty principle.  435   INTEGRAL TRANSFORMS   cid:1 Find the Fourier transform of the normalised Gaussian distribution −∞ < t < ∞.  f t  =  √ 1  exp  ,  τ  2π   cid:8    cid:7   − t2 2τ2  This Gaussian distribution is centred on t = 0 and has a root mean square deviation ∆t = τ.  Any reader who is unfamiliar with this interpretation of the distribution should refer to chapter 30.   Using the deﬁnition  13.5 , the Fourier transform of f t  is given by  3  f ω  =  1√ 2π 1√ 2π  =   cid:21  ∞  cid:21  ∞  −∞  −∞  √ 1 2π √ 1  2π  τ  τ  exp  exp  − t2 2τ2 − 1 2τ2   cid:8   cid:18   3  f ω  =  exp − 1 √ 2 τ2ω2  2π  √ 1  τ  2π   cid:7   cid:12    cid:12    cid:19  cid:15   dt,  exp −iωt  dt t2 + 2τ2iωt +  τ2iω 2 −  τ2iω 2  cid:15   cid:21  ∞   cid:13    cid:14   −∞ exp  −  t + iτ2ω 2  2τ2  dt  .  where the quantity − τ2iω 2  2τ2  has been both added and subtracted in the exponent  in order to allow the factors involving the variable of integration t to be expressed as a complete square. Hence the expression can be written  The quantity inside the braces is the normalisation integral for the Gaussian and equals unity, although to show this strictly needs results from complex variable theory  chapter 24 . That it is equal to unity can be made plausible by changing the variable to s = t + iτ2ω and assuming that the imaginary parts introduced into the integration path and limits  where the integrand goes rapidly to zero anyway  make no diﬀerence.  We are left with the result that3   cid:7 −τ2ω2   cid:8   ,  2  f ω  =  exp  1√ 2π   13.7   which is another Gaussian distribution, centred on zero and with a root mean square deviation ∆ω = 1 τ. It is interesting to note, and an important property, that the Fourier transform of a Gaussian is another Gaussian.  cid:2   In the above example the root mean square deviation in t was τ, and so it is  seen that the deviations or ‘spreads’ in t and in ω are inversely related:  ∆ω ∆t = 1,  independently of the value of τ. In physical terms, the narrower in time is, say, an electrical impulse the greater the spread of frequency components it must contain. Similar physical statements are valid for other pairs of Fourier-related variables, such as spatial position and wave number. In an obvious notation, ∆k∆x = 1 for a Gaussian wave packet.  The uncertainty relations as usually expressed in quantum mechanics can be related to this if the de Broglie and Einstein relationships for momentum and energy are introduced; they are  Here  cid:1  is Planck’s constant h divided by 2π. In a quantum mechanics setting f t   p =  cid:1 k  and  E =  cid:1 ω.  436   13.1 FOURIER TRANSFORMS  by 3 is a wavefunction and the distribution of the wave intensity in time is given by f2  also a Gaussian . Similarly, the intensity distribution in frequency is given f2. These two distributions have respective root mean square deviations of √  √ 2τ , giving, after incorporation of the above relations, 2 and 1    τ   ∆E ∆t =  cid:1  2  and  ∆p ∆x =  cid:1  2.  The factors of 1 2 that appear are speciﬁc to the Gaussian form, but any distribution f t  produces for the product ∆E∆t a quantity λ cid:1  in which λ is strictly positive  in fact, the Gaussian value of 1 2 is the minimum possible .  13.1.2 Fraunhofer diffraction  We take our ﬁnal example of the Fourier transform from the ﬁeld of optics. The pattern of transmitted light produced by a partially opaque  or phase-changing  object upon which a coherent beam of radiation falls is called a diﬀraction pattern and, in particular, when the cross-section of the object is small compared with the distance at which the light is observed the pattern is known as a Fraunhofer diﬀraction pattern.   cid:7   We will consider only the case in which the light is monochromatic with wavelength λ. The direction of the incident beam of light can then be described by the wave vector k; the magnitude of this vector is given by the wave number k = 2π λ of the light. The essential quantity in a Fraunhofer diﬀraction pattern is the dependence of the observed amplitude  and hence intensity  on the angle θ and the direction k of the incident beam. This between the viewing direction k is entirely determined by the spatial distribution of the amplitude and phase of being the light at the object, the transmitted intensity in a particular direction k determined by the corresponding Fourier component of this spatial distribution. As an example, we take as an object a simple two-dimensional screen of width 2Y on which light of wave number k is incident normally; see ﬁgure 13.2. We suppose that at the position  0, y  the amplitude of the transmitted light is f y  per unit length in the y-direction  f y  may be complex . The function f y  is called an aperture function. Both the screen and beam are assumed inﬁnite in the z-direction.   cid:7   Denoting the unit vectors in the x- and y- directions by i and j respectively, the total light amplitude at a position r0 = x0i + y0j, with x0 > 0, will be the superposition of all the  Huyghens’  wavelets originating from the various parts  of the screen. For large r0  = r0 , these can be treated as plane waves to give  §   cid:21   A r0  =  Y  −Y  f y  exp[ik  r0 − yj   cid:7  ·  r0 − yj ]  dy.   13.8   §  This is the approach ﬁrst used by Fresnel. For simplicity we have omitted from the integral a multiplicative inclination factor that depends on angle θ and decreases as θ increases.  437   INTEGRAL TRANSFORMS  k  θ   cid:7   k  x  y  Y  0  −Y  Figure 13.2 Diﬀraction grating of width 2Y with light of wavelength 2π k being diﬀracted through an angle θ.   cid:7  ·  r0 − yj ] represents the phase change undergone by the light  The factor exp[ik in travelling from the point yj on the screen to the point r0, and the denominator represents the reduction in amplitude with distance.  Recall that the system is inﬁnite in the z-direction and so the ‘spreading’ is eﬀectively in two dimensions only.   If the medium is the same on both sides of the screen then k   cid:7   cid:21  ∞ and if r0  cid:26  Y then expression  13.8  can be approximated by −∞ f y  exp −iky sin θ  dy.  exp ik r0   cid:7  · r0   A r0  =  We have used that f y  = 0 for y > Y to extend the integral to inﬁnite limits.  The intensity in the direction θ is then given by  = k cos θ i+k sin θ j,  I θ  = A2 =  3 f q 2,  2π 2 r0   13.9    13.10   where q = k sin θ.  cid:1 Evaluate I θ  for an aperture consisting of two long slits each of width 2b whose centres are separated by a distance 2a, a > b; the slits are illuminated by light of wavelength λ.  The aperture function is plotted in ﬁgure 13.3. We ﬁrst need to ﬁnd3  f q :   cid:21   cid:13   3  f q  =   cid:21  −a+b  cid:13   1√ 2π 1√ 2π −1 √  iq  2π  =  =  −iqx dx +  e −iqx   cid:14 −a+b −a−b −iq −a+b  − e  iq  −a−b − e  cid:18   e  1√ 2π 1√ 2π   cid:14  −iqx dx  a+b  a+b  e −iqx  a−b − e  a−b  + −iq −a−b  + e  iq   cid:19   −iq a+b  − e  −iq a−b   .  438   13.1 FOURIER TRANSFORMS  f y   1  −a − b  −a  −a + b  a − b  a  a + b  x  Figure 13.3 The aperture function f y  for two wide slits.  After some manipulation we obtain3  f q  =  4 cos qa sin qb  .  √  q  2π  Now applying  13.10 , and remembering that q =  2π sin θ  λ, we ﬁnd  , where r0 is the distance from the centre of the aperture.  cid:2   q2r0  I θ  =  2  16 cos2 qa sin2 qb  13.1.3 The Dirac δ-function  Before going on to consider further properties of Fourier transforms we make a digression to discuss the Dirac δ-function and its relation to Fourier transforms. The δ-function is diﬀerent from most functions encountered in the physical sciences but we will see that a rigorous mathematical deﬁnition exists; the utility of the δ-function will be demonstrated throughout the remainder of this chapter. It can be visualised as a very sharp narrow pulse  in space, time, density, etc.  which produces an integrated eﬀect having a deﬁnite magnitude. The formal properties of the δ-function may be summarised as follows.  The Dirac δ-function has the property that  but its fundamental deﬁning property is   cid:21   δ t  = 0  for t  cid:3 = 0,  f t δ t − a  dt = f a ,  439  provided the range of integration includes the point t = a; otherwise the integral   13.11    13.12    equals zero. This leads immediately to two further useful results:  δ t  dt = 1  for all a, b > 0   13.13   and  δ-function:  provided the range of integration includes t = a.  Equation  13.12  can be used to derive further useful properties of the Dirac  INTEGRAL TRANSFORMS   cid:21   b  −a   cid:21   δ t − a  dt = 1,  δ t  = δ −t ,  δ at  =  1a δ t ,  tδ t  = 0.   13.14    13.15    13.16    13.17    cid:1 Prove that δ bt  = δ t  b.   cid:21  ∞   cid:21  ∞   cid:7    cid:8    cid:7   δ t      cid:7   dt b   cid:7   t b  cid:7   Let us ﬁrst consider the case where b > 0. It follows that  −∞ f t δ bt  dt =  −∞ f  =  f 0  =  1 b  1 b  −∞ f t δ t  dt,  where we have made the substitution t  see that δ bt  = δ t  b = δ t  b for b > 0.  cid:8    cid:21  ∞ Now consider the case where b = −c < 0. It follows that   cid:21  −∞   cid:21  ∞   cid:7    cid:7    cid:7    cid:8   = bt. But f t  is arbitrary and so we immediately   cid:21  ∞  dt   cid:8   cid:7   cid:21  ∞ −c 1b   cid:7   t  t     =  1 c  δ t   cid:7  −c   cid:7  −c 1b f 0  = = bt = −ct. But f t  is arbitrary and so  cid:7   −∞ f t δ t  dt,  −∞    dt  δ t  f   cid:7    cid:7   −∞ f t δ bt  dt =  f  ∞ 1 c  =  f 0  =  where we have made the substitution t  for all b, which establishes the result.  cid:2   Furthermore, by considering an integral of the form  and making a change of variables to z = h t , we may show that  δ h t   =   13.18    cid:7  where the ti are those values of t for which h t  = 0 and h   t  stands for dh dt.  δ bt  =  1b δ t ,   cid:21   f t δ h t   dt,  δ t − ti  h  ti   cid:7   ,   cid:4   i  440   13.1 FOURIER TRANSFORMS  The derivative of the delta function, δ   t , is deﬁned by   cid:21  ∞   cid:7   −∞ f t δ   cid:7    cid:23 ∞   cid:21  ∞  −  −∞   cid:7   −∞ f   cid:22  = −f   cid:7    0 ,   t  dt =  f t δ t    t δ t  dt   13.19   and similarly for higher derivatives.  For many practical purposes, eﬀects that are not strictly described by a δ- function may be analysed as such, if they take place in an interval much shorter than the response interval of the system on which they act. For example, the idealised notion of an impulse of magnitude J applied at time t0 can be represented by  j t  = Jδ t − t0 .   13.20   Many physical situations are described by a δ-function in space rather than in time. Moreover, we often require the δ-function to be deﬁned in more than one dimension. For example, the charge density of a point charge q at a point r0 may be expressed as a three-dimensional δ-function  ρ r  = qδ r − r0  = qδ x − x0 δ y − y0 δ z − z0 ,   13.21   so that a discrete ‘quantum’ is expressed as if it were a continuous distribution. From  13.21  we see that  as expected  the total charge enclosed in a volume V is given by   cid:21    cid:21     ρ r  dV =  V  V  qδ r − r0  dV =  q  if r0 lies in V ,  0 otherwise.  Closely related to the Dirac δ-function is the Heaviside or unit step function  H t , for which  This function is clearly discontinuous at t = 0 and it is usual to take H 0  = 1 2. The Heaviside function is related to the delta function by    H t  =  1 for t > 0,  0 for t < 0.   cid:7   H   t  = δ t .  441   13.22    13.23    INTEGRAL TRANSFORMS   cid:1 Prove relation  13.23 .  Considering the integral cid:21  ∞   cid:7   −∞ f t H   t  dt =  f t H t    t H t  dt   cid:13   = f ∞  − = f ∞  −   cid:21  ∞   cid:7   −∞ f   cid:14 ∞  cid:21  ∞  cid:13   0  −∞  −   cid:7    cid:14 ∞  0  f   t  dt  f t   = f 0 ,  and comparing it with  13.12  when a = 0 immediately shows that H   cid:7    t  = δ t .  cid:2   13.1.4 Relation of the δ-function to Fourier transforms  In the previous section we introduced the Dirac δ-function as a way of repre- senting very sharp narrow pulses, but in no way related it to Fourier transforms. We now show that the δ-function can equally well be deﬁned in a way that more naturally relates it to the Fourier transform.  Referring back to the Fourier inversion theorem  13.4 , we have   cid:21  ∞  cid:21  ∞  1 2π  f t  =  −∞ dω eiωt  =  −∞ du f u   δ t − u  =  1 2π   cid:15   .   cid:21  ∞  cid:21  ∞ −iωu −∞ du f u  e −∞ eiω t−u  dω   cid:12   cid:21  ∞ −∞ eiω t−u  dω.  1 2π  Comparison of this with  13.12  shows that we may write the δ-function as  Considered as a Fourier transform, this representation shows that a very narrow time peak at t = u results from the superposition of a complete spectrum of harmonic waves, all frequencies having the same amplitude and all waves being in phase at t = u. This suggests that the δ-function may also be represented as the limit of the transform of a uniform distribution of unit height as the width of this distribution becomes inﬁnite.  Consider the rectangular distribution of frequencies shown in ﬁgure 13.4 a .  From  13.6 , taking the inverse Fourier transform,   13.24    13.25   fΩ t  =  Ω  1 × eiωt dω  1√ 2π 2Ω√ 2π  =  −Ω sin Ωt  .  Ωt   cid:21   442  This function is illustrated in ﬁgure 13.4 b  and it is apparent that, for large Ω, it becomes very large at t = 0 and also very narrow about t = 0, as we qualitatively   13.1 FOURIER TRANSFORMS  fΩ t   2Ω   2π 1 2  3  fΩ  1   a   −Ω  ω  Ω  t  π Ω   b   Figure 13.4  a  A Fourier transform showing a rectangular distribution of  frequencies between ±Ω;  b  the function of which it is the transform, which  is proportional to t  −1 sin Ωt.  expect and require. We also note that, in the limit Ω → ∞, fΩ t , as deﬁned by the inverse Fourier transform, tends to  2π 1 2δ t  by virtue of  13.24 . Hence we may conclude that the δ-function can also be represented by   cid:7    cid:8   δ t  = lim Ω→∞  sin Ωt  πt  .   13.26   Several other function representations are equally valid, e.g. the limiting cases of rectangular, triangular or Gaussian distributions; the only essential requirements are a knowledge of the area under such a curve and that undeﬁned operations such as dividing by zero are not inadvertently carried out on the δ-function whilst some non-explicit representation is being employed.  We also note that the Fourier transform deﬁnition of the delta function,  13.24 ,  shows that the latter is real since   cid:21  ∞ −iωt dω = δ −t  = δ t .  cid:21  ∞  −∞ e  δ   t  =  1 2π  ∗  3  Finally, the Fourier transform of a δ-function is simply 1√ 2π  −iωt dt =  −∞ δ t  e  1√ 2π  δ ω  =  .   13.27   13.1.5 Properties of Fourier transforms  Having considered the Dirac δ-function, we now return to our discussion of the properties of Fourier transforms. As we would expect, Fourier transforms have many properties analogous to those of Fourier series in respect of the connection between the transforms of related functions. Here we list these properties without proof; they can be veriﬁed by working from the deﬁnition of the transform. As  previously, we denote the Fourier transform of f t  by3  f ω  or F[ f t ].  443    i  Diﬀerentiation:  F  f   t   = iω   13.28   This may be extended to higher derivatives, so that  F  f   t   = iωF  f   t   f ω ,  INTEGRAL TRANSFORMS  f ω .  3  cid:19  = −ω23   cid:18    cid:7    cid:14    cid:19   cid:18    cid:7   3  1 iω   cid:19    cid:18   cid:13  cid:21    cid:7  cid:7   t  F  f s  ds  =  f ω  + 2πcδ ω ,   13.29   where the term 2πcδ ω  represents the Fourier transform of the constant of integration associated with the indeﬁnite integral.  and so on.   ii  Integration:   iii  Scaling:   iv  Translation:   cid:10   .  f  1 a  ω a  F[ f at ] =   cid:9  3 F[ f t + a ] = eiaω3  cid:18    cid:19   =3  F  eαtf t    13.30   f ω .   13.31   f ω + iα ,   13.32    v  Exponential multiplication:  where α may be real, imaginary or complex.   cid:1 Prove relation  13.28 .  Calculating the Fourier transform of f   cid:18    cid:19    cid:7   F  f   t   =  1√ 2π 1√ 3 2π  =   cid:7    cid:21  ∞  cid:13    t  e −∞ f −iωtf t   e   cid:14 ∞   t  directly, we obtain  cid:7   −iωt dt   cid:21  ∞  1√ 2π  −∞ iω e  −∞ +   cid:1  ∞ −∞ f t  dt is ﬁnite.  cid:2   −iωtf t  dt  if f t  → 0 at t = ±∞, as it must since  = iω  f ω ,  To illustrate a use and also a proof of  13.32 , let us consider an amplitude- modulated radio wave. Suppose a message to be broadcast is represented by f t . The message can be added electronically to a constant signal a of magnitude such that a + f t  is never negative, and then the sum can be used to modulate the amplitude of a carrier signal of frequency ωc. Using a complex exponential notation, the transmitted amplitude is now  g t  = A [a + f t ] eiωct.   13.33   444   Ignoring in the present context the eﬀect of the term Aa exp iωct , which gives a contribution to the transmitted spectrum only at ω = ωc, we obtain for the new spectrum  13.1 FOURIER TRANSFORMS  3g ω  =  A   cid:21  ∞  cid:21  ∞ 1√ 2π 1√ 3 2π f ω − ωc ,  A  =  = A  −∞ f t  eiωct e  −iωt dt −i ω−ωc t dt  −∞ f t  e  which is simply a shift of the whole spectrum by the carrier frequency. The use of diﬀerent carrier frequencies enables signals to be separated.   13.34   13.1.6 Odd and even functions  If f t  is odd or even then we may derive alternative forms of Fourier’s inversion theorem, which lead to the deﬁnition of diﬀerent transform pairs. Let us ﬁrst  consider an odd function f t  = −f −t , whose Fourier transform is given by   cid:21  ∞  cid:21  ∞ −∞ f t  e  cid:21  ∞ −∞ f t  cos ωt − i sin ωt  dt  −iωt dt  f t  sin ωt dt,  3  0  =  =  2π  f ω  =  1√ 2π 1√ 2π −2i√ f −ω  = −3 f ω , i.e.3  cid:21  ∞ 3  cid:12  cid:21  ∞  cid:21  ∞  cid:21  ∞  cid:21  ∞       1√ 2π  dω sin ωt  fs ω  =  3  −∞  2 π  2 π  =  0  0  0  3  2 π  0  We note that3  where in the last line we use the fact that f t  and sin ωt are odd, whereas cos ωt is even.  f ω  is an odd function of ω. Hence  f t  =  f ω  eiωt dω =  f ω  sin ωt dω   cid:21  ∞  0  3   cid:15   2i√ 2π  f u  sin ωu du  .  Thus we may deﬁne the Fourier sine transform pair for odd functions:  f t  sin ωt dt,  f t  =  fs ω  sin ωt dω.   13.35    13.36   require f t  and 3  Note that although the Fourier sine transform pair was derived by considering an odd function f t  deﬁned over all t, the deﬁnitions  13.35  and  13.36  only fs ω  to be deﬁned for positive t and ω respectively. For an  445   INTEGRAL TRANSFORMS  g y    a    b    c    d   0  y  Figure 13.5 Resolution functions:  a  ideal δ-function;  b  typical unbiased resolution;  c  and  d  biases tending to shift observations to higher values than the true one.  even function, i.e. one for which f t  = f −t , we can deﬁne the Fourier cosine  transform pair in a similar way, but with sin ωt replaced by cos ωt.  13.1.7 Convolution and deconvolution  It is apparent that any attempt to measure the value of a physical quantity is limited, to some extent, by the ﬁnite resolution of the measuring apparatus used. On the one hand, the physical quantity we wish to measure will be in general a function of an independent variable, x say, i.e. the true function to be measured takes the form f x . On the other hand, the apparatus we are using does not give the true output value of the function; a resolution function g y  is involved. By this we mean that the probability that an output value y = 0 will be recorded instead as being between y and y +dy is given by g y  dy. Some possible resolution functions of this sort are shown in ﬁgure 13.5. To obtain good results we wish the resolution function to be as close to a δ-function as possible  case  a  . A typical piece of apparatus has a resolution function of ﬁnite width, although if it is accurate the mean is centred on the true value  case  b  . However, some apparatus may show a bias that tends to shift observations to higher or lower values than the true ones  cases  c  and  d  , thereby exhibiting systematic error. Given that the true distribution is f x  and the resolution function of our measuring apparatus is g y , we wish to calculate what the observed distribution h z  will be. The symbols x, y and z all refer to the same physical variable  e.g.  446   13.1 FOURIER TRANSFORMS  f x   ∗  g y   1  =  h z   −a  x  a  −b  b  y  Figure 13.6 The convolution of two functions f x  and g y .  2b  −a  2b  z  a  length or angle , but are denoted diﬀerently because the variable appears in the analysis in three diﬀerent roles.  The probability that a true reading lying between x and x + dx, and so having probability f x  dx of being selected by the experiment, will be moved by the  instrumental resolution by an amount z − x into a small interval of width dz is g z − x  dz. Hence the combined probability that the interval dx will give rise to an observation appearing in the interval dz is f x  dx g z − x  dz. Adding together  the contributions from all values of x that can lead to an observation in the range z to z + dz, we ﬁnd that the observed distribution is given by   cid:21  ∞ −∞ f x g z − x  dx.  h z  =   13.37   The integral in  13.37  is called the convolution of the functions f and g and is  often written f ∗ g. The convolution deﬁned above is commutative  f ∗ g = g ∗ f ,  associative and distributive. The observed distribution is thus the convolution of the true distribution and the experimental resolution function. The result will be that the observed distribution is broader and smoother than the true one and, if g y  has a bias, the maxima will normally be displaced from their true positions. It is also obvious from  13.37  that if the resolution is the ideal δ-function, g y  = δ y  then h z  = f z  and the observed distribution is the true one.  It is interesting to note, and a very important property, that the convolution of any function g y  with a number of delta functions leaves a copy of g y  at the position of each of the delta functions.   cid:1 Find the convolution of the function f x  = δ x + a  + δ x − a  with the function g y   plotted in ﬁgure 13.6.  Using the convolution integral  13.37    cid:21  ∞ −∞ f x g z − x  dx =   cid:21  ∞ −∞[δ x + a  + δ x − a ]g z − x  dx = g z + a  + g z − a .  h z  =  This convolution h z  is plotted in ﬁgure 13.6.  cid:2   Let us now consider the Fourier transform of the convolution  13.37 ; this is  447   INTEGRAL TRANSFORMS  =  3  −ikz  h k  =  given by  −∞ dz e   cid:15   cid:15   1√ 2π 1√ 2π   cid:12  cid:21  ∞  cid:12  cid:21  ∞ −∞ f x g z − x  dx −∞ g z − x  e −ikz dz  cid:15   cid:12  cid:21  ∞ If we let u = z − x in the second integral we have  cid:21  ∞ −ik u+x  du −iku du 3 2π3g k  = f k 3g k . −∞ g u  e √   cid:21  ∞  cid:21  ∞  cid:21  ∞  cid:21  ∞ −∞ f x  e × √  −∞ dx f x   −∞ dx f x   −∞ g u  e  h k  =  3  2π  2π  =  =  .  1√ 2π 1√ 2π 1√ 2π  −ikx dx 3 f k  × √ √  Hence the Fourier transform of a convolution f ∗ g is equal to the product of the  separate Fourier transforms multiplied by theorem.  2π; this result is called the convolution  It may be proved similarly that the converse is also true, namely that the  Fourier transform of the product f x g x  is given by   13.38   F[ f x g x ] =   13.39   3 f k  ∗3g k .  1√ 2π   cid:1 Find the Fourier transform of the function in ﬁgure 13.3 representing two wide slits by considering the Fourier transforms of  i  two δ-functions, at x = ±a,  ii  a rectangular  function of height 1 and width 2b centred on x = 0.   i  The Fourier transform of the two δ-functions is given by  3  f q  =  1√ 2π 1√ 2π  =   cid:21  ∞ −∞ δ x − a  e  cid:5   cid:6  −iqa + eiqa  cid:21   e  −∞ δ x + a  e  −iqx dx  1√ 2π  −iqx dx + 2 cos qa√ 2π  =  .   cid:21  ∞   cid:13    cid:14   b  −b  3g q  =   ii  The Fourier transform of the broad slit is 1√ −iqx dx = −b 2π √ −iqb − eiqb  = 2 sin qb 2π  1√ 2π −1 √  2π   e  =  iq  −iqx−iq  q  e  e  b  .  We have already seen that the convolution of these functions is the required function √ representing two wide slits  see ﬁgure 13.6 . So, using the convolution theorem, the Fourier √ transform of the convolution is 2π times the product of the individual transforms, i.e. 2π . This is, of course, the same result as that obtained in the example 4 cos qa sin qb  q in subsection 13.1.2.  cid:2   448   13.1 FOURIER TRANSFORMS  The inverse of convolution, called deconvolution, allows us to ﬁnd a true distribution f x  given an observed distribution h z  and a resolution function g y .   cid:1 An experimental quantity f x  is measured using apparatus with a known resolution func- tion g y  to give an observed distribution h z . How may f x  be extracted from the mea- sured distribution?  From the convolution theorem  13.38 , the Fourier transform of the measured distribution is  from which we obtain  Then on inverse Fourier transforming we ﬁnd  √ 2π  h k  =  3  3  f k  =  1√ 2π  3 f k 3g k , 3 h k 3g k   cid:16 3 h k 3g k   .   cid:17   .  f x  =  F −1  1√ 2π  In words, to extract the true distribution, we divide the Fourier transform of the observed distribution by that of the resolution function for each value of k and then take the inverse Fourier transform of the function so generated.  cid:2   This explicit method of extracting true distributions is straightforward for exact functions but, in practice, because of experimental and statistical uncertainties in the experimental data or because data over only a limited range are available, it is often not very precise, involving as it does three  numerical  transforms each requiring in principle an integral over an inﬁnite range.  13.1.8 Correlation functions and energy spectra  The cross-correlation of two functions f and g is deﬁned by  C z  =   x g x + z  dx.   13.40   Despite the formal similarity between  13.40  and the deﬁnition of the convolution in  13.37 , the use and interpretation of the cross-correlation and of the convo- lution are very diﬀerent; the cross-correlation provides a quantitative measure of the similarity of two functions f and g as one is displaced through a distance z  relative to the other. The cross-correlation is often notated as C = f⊗ g, and, like  convolution, it is both associative and distributive. Unlike convolution, however, it is not commutative, in fact  [ f ⊗ g] z  = [g ⊗ f] ∗   −z .   13.41    cid:21  ∞  ∗  −∞ f  449   INTEGRAL TRANSFORMS  ∗3g k .  √  C k  =   cid:1 Prove the Wiener–Kinchin theorem,3  cid:21  ∞  cid:21  ∞  C k  =  3  f k ]  2π [3  cid:12  cid:21  ∞  cid:12  cid:21  ∞  Following a method similar to that for the convolution of f and g, let us consider the Fourier transform of  13.40 : 1√ 2π 1√ 2π  −∞ g x + z  e  −∞ dx f  −∞ dz e  −∞ f  −ikz   x   =  ∗  ∗  .   cid:15   cid:15    13.42   Making the substitution u = x + z in the second integral we obtain   x g x + z  dx −ikz dz  cid:15   3  C k  =  1√ 2π 1√ 2π 1√ 2π  =  =  ∗  ∗   x    x  eikx dx  −∞ dx f  −∞ g u  e  −∞ g u  e  −ik u−x  du −iku du √   cid:12  cid:21  ∞  cid:21  ∞  cid:21  ∞  cid:21  ∞ 2π [3 2π3g k  = −∞ f × √ ∗ × √ and 3g k  multiplied by √  cid:18  3 f ⊗3g.   x g x   f k ]   cid:19   F  =  ∗  f  1√ 2π  2π [3  f k ]  ∗3g k .  cid:2   the product of [3  Thus the Fourier transform of the cross-correlation of f and g is equal to 2π. This a statement of the  ∗ f k ]  Wiener–Kinchin theorem. Similarly we can derive the converse theorem  If we now consider the special case where g is taken to be equal to f in  13.40  then, writing the LHS as a z , we have  a z  =   x f x + z  dx;   13.43   this is called the auto-correlation function of f x . Using the Wiener–Kinchin theorem  13.42  we see that  ∗  −∞ f   cid:21  ∞   cid:21  ∞  cid:21  ∞  −∞  −∞  3a k  eikz dk 2π [3  √  a z  =  1√ 2π 1√ 2π  =  ∗3 2π 3 f k ] √ f k 2, which is in turn called  f k  eikz dk,  so that a z  is the inverse Fourier transform of the energy spectrum of f.  Using the results of the previous section we can immediately obtain Parseval’s theorem. The most general form of this  also called the multiplication theorem  is  13.1.9 Parseval’s theorem  450   obtained simply by noting from  13.42  that the cross-correlation  13.40  of two functions f and g can be written as  C z  =   x g x + z  dx =  Then, setting z = 0 gives the multiplication theorem  13.1 FOURIER TRANSFORMS  ∗   cid:21  ∞  cid:21  ∞  −∞ f  ∗  −∞ f   cid:21  ∞  −∞   x g x  dx =  f x 2 dx =  ∗3g k  eikz dk.  f k ]   cid:21    cid:21  ∞ −∞[3 [3 ∗3g k  dk.  cid:21  ∞  3 f k 2 dk.  f k ]  −∞   13.44    13.45    13.46   Specialising further, by letting g = f, we derive the most common form of Parseval’s theorem,  When f is a physical amplitude these integrals relate to the total intensity involved in some physical process. We have already met a form of Parseval’s theorem for Fourier series in chapter 12; it is in fact a special case of  13.46 .   cid:1 The displacement of a damped harmonic oscillator as a function of time is given by    Find the Fourier transform of this function and so give a physical interpretation of Parseval’s theorem.  Using the usual deﬁnition for the Fourier transform we ﬁnd  f t  =  0 −t τ sin ω0t e  for t < 0,  for t ≥ 0.  3 3   cid:21   1 2  0  −∞ 0 × e −iωt dt +  cid:21  ∞  cid:18  −iω0t  2i we obtain  cid:13   1 2i  e  0   cid:21  ∞  0  −t τ sin ω0t e  e  −iωt dt.  cid:19    cid:14   f ω  = 0 +  −it ω−ω0−i τ  − e  −it ω+ω0−i τ   dt  f ω  =  Writing sin ω0t as  eiω0t − e  1  −  1  which is the required Fourier transform. The physical interpretation of 3 f ω 2 is the energy content per unit frequency interval  i.e. the energy spectrum  whilst f t 2 is proportional to  ω − ω0 − i τ  ω + ω0 − i τ  =  ,  the sum of the kinetic and potential energies of the oscillator. Hence  to within a constant  Parseval’s theorem shows the equivalence of these two alternative speciﬁcations for the total energy.  cid:2   13.1.10 Fourier transforms in higher dimensions  The concept of the Fourier transform can be extended naturally to more than one dimension. For instance we may wish to ﬁnd the spatial Fourier transform of  451   INTEGRAL TRANSFORMS  two- or three-dimensional functions of position. For example, in three dimensions we can deﬁne the Fourier transform of f x, y, z  as −iky ye  −ikz z dx dy dz,  −ikxxe  f x, y, z  e  f kx, ky, kz  =   13.47   3  1  and its inverse as  f x, y, z  =  f kx, ky, kz  eikxxeiky yeikz z dkx dky dkz.   13.48   Denoting the vector with components kx, ky, kz by k and that with components x, y, z by r, we can write the Fourier transform pair  13.47 ,  13.48  as   cid:21  cid:21  cid:21   cid:21  cid:21  cid:21 3   2π 3 2  1   2π 3 2  3   cid:21   cid:21  3 −ik·r d3r, f r  e f k  eik·r d3k.  cid:21   eik·r d3k.  1  1  f k  =   2π 3 2  f r  =   2π 3 2  δ r  =  1   2π 3   13.49    13.50    13.51   From these relations we may deduce that the three-dimensional Dirac δ-function can be written as  Similar relations to  13.49 ,  13.50  and  13.51  exist for spaces of other dimen- sionalities.  cid:1 In three-dimensional space a function f r  possesses spherical symmetry, so that f r  = f r . Find the Fourier transform of f r  as a one-dimensional integral.  Let us choose spherical polar coordinates in which the vector k of the Fourier transform lies along the polar axis  θ = 0 . This we can do since f r  is spherically symmetric. We then have  where k = k. The Fourier transform is then given by  d3r = r2 sin θ dr dθ dφ  and  k · r = kr cos θ,  3  f k  =   2π 3 2  f r  e   cid:21   cid:21  ∞  cid:21  ∞  0  1  1  1   2π 3 2  0   cid:21   cid:21  −ik·r d3r  π  2π   cid:21   0  π  0   2π 3 2  dr  dθ  0  dφ f r r2 sin θ e  −ikr cos θ  dr 2πf r r2  dθ sin θ e  −ikr cos θ.  The integral over θ may be straightforwardly evaluated by noting that  =  =  3  d dθ  −ikr cos θ. −ikr cos θ  = ikr sin θ e  cid:13   e  cid:8   cid:7    cid:21  ∞  cid:21  ∞  dr 2πf r r2  −ikr cos θ e  ikr  1  0   2π 3 2   cid:14   θ=π  θ=0  1  =   2π 3 2  0  4πr2f r   sin kr  kr  dr.  cid:2   f k  =  452  Therefore   13.2 LAPLACE TRANSFORMS  A similar result may be obtained for two-dimensional Fourier transforms in which f r  = f ρ , i.e. f r  is independent of azimuthal angle φ. In this case, using the integral representation of the Bessel function J0 x  given at the very end of  subsection 18.5.3, we ﬁnd3   cid:21  ∞  1 2π  0  f k  =  2πρf ρ J0 kρ  dρ.   13.52   13.2 Laplace transforms  exist because f  cid:3 → 0 as t → ∞, and so the integral deﬁning3  Often we are interested in functions f t  for which the Fourier transform does not f does not converge. This would be the case for the function f t  = t, which does not possess a Fourier transform. Furthermore, we might be interested in a given function only for t > 0, for example when we are given the value at t = 0 in an initial-value problem. This leads us to consider the Laplace transform, ¯f s  or L [ f t ], of f t , which is deﬁned by  ¯f s  ≡  −st dt,  f t e   13.53    cid:21  ∞  0  provided that the integral exists. We assume here that s is real, but complex values would have to be considered in a more detailed study. In practice, for a given function f t  there will be some real number s0 such that the integral in  13.53   exists for s > s0 but diverges for s ≤ s0. Through  13.53  we deﬁne a linear transformation L that converts functions  of the variable t to functions of a new variable s:  L [af1 t  + bf2 t ] = aL [ f1 t ] + bL [ f2 t ] = a ¯f1 s  + b ¯f2 s .   13.54    cid:1 Find the Laplace transforms of the functions  i  f t  = 1,  ii  f t  = eat,  iii  f t  = tn, for n = 0, 1, 2, . . . .   i  By direct application of the deﬁnition of a Laplace transform  13.53 , we ﬁnd  where the restriction s > 0 is required for the integral to exist.   ii  Again using  13.53  directly, we ﬁnd  L [1] =  −st dt =  e  −st e  =  1 s  ,  if s > 0,   cid:13 −1  s   cid:14 ∞  0   cid:21  ∞   cid:21  ∞  0   cid:21  ∞  cid:13   0  ¯f s  =  =  −st dt =   cid:14 ∞  =  0  0  1  s − a  eate e a−s t a − s  e a−s t dt  if s > a.  453    iii  Once again using the deﬁnition  13.53  we have  Integrating by parts we ﬁnd  INTEGRAL TRANSFORMS   cid:21  ∞  cid:14 ∞  0  tne  −st dt.   cid:21  ∞  ¯fn s  =   cid:13 −tne  −st  s n s  ¯fn s  =  +  n s  0  0  tn−1e  −st dt  = 0 +  ¯fn−1 s ,  if s > 0.  We now have a recursion relation between successive transforms and by calculating ¯f0 we can infer ¯f1, ¯f2, etc. Since t0 = 1,  i  above gives  ¯f0 =  1 s  ,  if s > 0,   13.55   and  ¯f1 s  =  1 s2 ,  ¯f2 s  =  2! s3 ,  . . . ,  ¯fn s  =  if s > 0.  n! sn+1  Thus, in each case  i – iii , direct application of the deﬁnition of the Laplace transform  13.53  yields the required result.  cid:2   Unlike that for the Fourier transform, the inversion of the Laplace transform is not an easy operation to perform, since an explicit formula for f t , given ¯f s , is not straightforwardly obtained from  13.53 . The general method for obtaining an inverse Laplace transform makes use of complex variable theory and is not discussed until chapter 25. However, progress can be made without having to ﬁnd an explicit inverse, since we can prepare from  13.53  a ‘dictionary’ of the Laplace transforms of common functions and, when faced with an inversion to carry out, hope to ﬁnd the given transform  together with its parent function  in the listing. Such a list is given in table 13.1.  When ﬁnding inverse Laplace transforms using table 13.1, it is useful to note and linear  that for all practical purposes the inverse Laplace transform is unique so that  §   cid:18   L −1   cid:19   a ¯f1 s  + b ¯f2 s   = af1 t  + bf2 t .   13.56   In many practical problems the method of partial fractions can be useful in producing an expression from which the inverse Laplace transform can be found.  cid:1 Using table 13.1 ﬁnd f t  if  Using partial fractions ¯f s  may be written  §  This is not strictly true, since two functions can diﬀer from one another at a ﬁnite number of isolated points but have the same Laplace transform.  ¯f s  =  s + 3  s s + 1   .  ¯f s  =  − 2 s + 1  .  3 s  454   13.2 LAPLACE TRANSFORMS  f t   c ctn sin bt cos bt eat tneat sinh at cosh at eat sin bt eat cos bt t1 2 −1 2 t δ t − t0  H t − t0  =    1 for t ≥ t0  0 for t < t0  ¯f s   c s cn! sn+1 b  s2 + b2  s  s2 + b2  1  s − a  n!  s − a n+1 a  s2 − a2  s  s2 − a2  b [ s − a 2 + b2]  s − a  [ s − a 2 + b2] 2  π s3 1 2  π s 1 2 −st0 e −st0  s  e  1  s0  0 0 0 0  a  a  a a  a  a 0 0 0  0  Table 13.1 Standard Laplace transforms. The transforms are valid for s > s0.  Comparing this with the standard Laplace transforms in table 13.1, we ﬁnd that the inverse transform of 3 s is 3 for s > 0 and the inverse transform of 2  s + 1  is 2e and so  −t for s > −1,  f t  = 3 − 2e −t,  if s > 0.  cid:2   13.2.1 Laplace transforms of derivatives and integrals  One of the main uses of Laplace transforms is in solving diﬀerential equations. Diﬀerential equations are the subject of the next six chapters and we will return to the application of Laplace transforms to their solution in chapter 15. In the meantime we will derive the required results, i.e. the Laplace transforms of derivatives.  The Laplace transform of the ﬁrst derivative of f t  is given by   cid:13    cid:14   L  df dt  =   cid:21  ∞  cid:19 ∞  cid:18  −st dt 0 + s = −f 0  + s ¯f s ,  df e dt −st  f t e  =  0   cid:21  ∞  0  455  −st dt  f t e  for s > 0.   13.57   The evaluation relies on integration by parts and higher-order derivatives may be found in a similar manner.   INTEGRAL TRANSFORMS   cid:1 Find the Laplace transform of d2f dt2.  Using the deﬁnition of the Laplace transform and integrating by parts we obtain   cid:21  ∞  cid:13   0  df dt  dt  =  =  L  d2f dt2   cid:13    cid:13   −st dt   cid:14 ∞  d2f dt2 e −st e   cid:21  ∞  + s  0  0  −st dt  e  df dt  = − df   0  + s[s ¯f s  − f 0 ],  for s > 0,  where  13.57  has been substituted for the integral. This can be written more neatly as  L  d2f dt2  = s2 ¯f s  − sf 0  − df   0 ,  dt  for s > 0.  cid:2    cid:13    cid:14   L  dnf dtn  In general the Laplace transform of the nth derivative is given by  = sn ¯f − sn−1f 0  − sn−2 df   0  − ··· − dn−1f  dtn−1  0 ,  for s > 0.   13.58   We now turn to integration, which is much more straightforward. From the  deﬁnition  13.53 ,   cid:13  cid:21   t  0  L  f u  du  =  dt e  f u  du   cid:14 ∞   cid:21  ∞  f u  du  +  0  0  −stf t  dt.  e  1 s  The ﬁrst term on the RHS vanishes at both limits, and so  dt   cid:21   t  0  0  t   cid:14   −st   cid:21   −st  e  0   cid:21  ∞  cid:13  − 1  cid:13  cid:21   s  t  0  L  f u  du  =  L [ f] .  1 s   13.59   13.2.2 Other properties of Laplace transforms  From table 13.1 it will be apparent that multiplying a function f t  by eat has the eﬀect on its transform that s is replaced by s − a. This is easily proved generally:   cid:14    cid:14    cid:14   =   cid:18   L  eatf t   f t eate   cid:19   =   cid:21  ∞  cid:21  ∞ = ¯f s − a .  =  0  0  f t e  −st dt − s−a t dt  456  As it were, multiplying f t  by eat moves the origin of s by an amount a.   13.60    We may now consider the eﬀect of multiplying the Laplace transform ¯f s  by −bs  b > 0 . From the deﬁnition  13.53 ,  e  13.2 LAPLACE TRANSFORMS   cid:21  ∞  cid:21  ∞  0  =  0  −bs ¯f s  =  e  −s t+b f t  dt −szf z − b  dz,  e  e  on putting t + b = z. Thus e deﬁned by  −bs ¯f s  is the Laplace transform of a function g t   f t − b   for 0 < t ≤ b,  for t > b.  0  g t  =  In other words, the function f has been translated to ‘later’ t  larger values of t  by an amount b.  Further properties of Laplace transforms can be proved in similar ways and  for n = 1, 2, 3, . . . ,   13.62    13.61    13.63    cid:9    cid:10   ¯f  1 a  s a  ,  are listed below.   i    ii    iii   L [ f at ] =  cid:14   cid:21  ∞ L [tnf t ] =  −1 n dn ¯f s    cid:13   dsn  ,  f t   L  ¯f u  du,  =  s  t provided limt→0[ f t  t] exists.  Related results may be easily proved.  cid:1 Find an expression for the Laplace transform of t d2f dt2.  From the deﬁnition of the Laplace transform we have   cid:13    cid:14    cid:21  ∞  L  d2f dt2  t  0  =  e  −stt  d2f dt2 dt −st d2f e   cid:21  ∞ [s2 ¯f s  − sf 0  − f − 2s ¯f + f 0 .  cid:2   dt2 dt  0   cid:7   ds  = − d = − d = −s2 d ¯f  ds  ds   0 ]  Finally we mention the convolution theorem for Laplace transforms  which is analogous to that for Fourier transforms discussed in subsection 13.1.7 . If the functions f and g have Laplace transforms ¯f s  and ¯g s  then  = ¯f s ¯g s ,   13.64    cid:14  f u g t − u  du   cid:13  cid:21   t  0  L  457   INTEGRAL TRANSFORMS  Figure 13.7 Two representations of the Laplace transform convolution  see text .  where the integral in the brackets on the LHS is the convolution of f and g,  denoted by f ∗ g. As in the case of Fourier transforms, the convolution deﬁned above is commutative, i.e. f ∗ g = g ∗ f, and is associative and distributive. From   13.64  we also see that   cid:18    cid:19    cid:21   0  L −1  ¯f s ¯g s   =  t  f u g t − u  du = f ∗ g.   cid:1 Prove the convolution theorem  13.64  for Laplace transforms.  From the deﬁnition  13.64 ,  Now letting u + v = t changes the limits on the integrals, with the result that  As shown in ﬁgure 13.7 a  the shaded area of integration may be considered as the sum of vertical strips. However, we may instead integrate over this area by summing over horizontal strips as shown in ﬁgure 13.7 b . Then the integral can be written as   cid:21  ∞  0   cid:21  ∞  cid:21  ∞  cid:21  ∞  0  ¯f s ¯g s  =  −suf u  du  e   cid:21  ∞  e  −svg v  dv −s u+v f u g v .  0  =  du  0  dv e   cid:21  ∞   cid:21  ∞  cid:12  cid:21   0  ¯f s ¯g s  =  du f u   0  u  dt g t − u  e  −st.  ¯f s ¯g s  =  du f u   t   cid:21   cid:21  ∞  cid:13  cid:21   0  0  =  = L   cid:15   dt g t − u  e −st  cid:14  f u g t − u  du  t  0  t  f u g t − u  du  .  cid:2   −st  dt e  0  458   13.3 CONCLUDING REMARKS  The properties of the Laplace transform derived in this section can sometimes  be useful in ﬁnding the Laplace transforms of particular functions.   cid:1 Find the Laplace transform of f t  = t sin bt.  Although we could calculate the Laplace transform directly, we can use  13.62  to give  ¯f s  =  −1   L [sin bt] = − d  d ds  b  ds  s2 + b2  2bs  =   s2 + b2 2 ,  for s > 0.  cid:2    cid:7    cid:8   13.3 Concluding remarks  In this chapter we have discussed Fourier and Laplace transforms in some detail. Both are examples of integral transforms, which can be considered in a more general context.  A general integral transform of a function f t  takes the form  F α  =  K α, t f t  dt,   13.65   where F α  is the transform of f t  with respect to the kernel K α, t , and α is −st, the transform variable. For example, in the Laplace transform case K s, t  = e  a = 0, b = ∞.  Very often the inverse transform can also be written straightforwardly and we obtain a transform pair similar to that encountered in Fourier transforms. Examples of such pairs are   i  the Hankel transform  where the Jn are Bessel functions of order n, and   ii  the Mellin transform  F k  =  f x Jn kx x dx,  f x  =  F k Jn kx k dk,  F z  =  f t  =   cid:21  tz−1f t  dt, i∞ −i∞ t  −zF z  dz.  Although we do not have the space to discuss their general properties, the  reader should at least be aware of this wider class of integral transforms.   cid:21   b  a   cid:21  ∞  cid:21  ∞  0  0   cid:21  ∞  0 1 2πi  459   INTEGRAL TRANSFORMS  13.1  Find the Fourier transform of the function f t  = exp −t .  13.4 Exercises   a  By applying Fourier’s inversion theorem prove that   cid:21  ∞  0  exp −t  =  π 2  cos ωt 1 + ω2 dω.  13.2  13.3 13.4   b  By making the substitution ω = tan θ, demonstrate the validity of Parseval’s  theorem for this function.  Use the general deﬁnition and properties of Fourier transforms to show the following.  a  If f x  is periodic with period a then ˜f k  = 0, unless ka = 2πn for integer n.  b  The Fourier transform of tf t  is id˜f ω  dω.  c  The Fourier transform of f mt + c  is   cid:9  ˜f Find the Fourier transform of H x− a e −bx, where H x  is the Heaviside function. straight-line segments joining  −T , 0  to  0, 1  to  T , 0 , with f t  = 0 outside Prove that the Fourier transform of the function f t  deﬁned in the tf-plane by t < T , is  eiωc m   cid:10   ω m  m  .   cid:7    cid:8   ωT 2  ,  ˜f ω  =  sinc2  T√ 2π  where sinc x is deﬁned as  sin x  x.  Use the general properties of Fourier transforms to determine the transforms of the following functions, graphically deﬁned by straight-line segments and equal to zero outside the ranges speciﬁed:   a    0, 0  to  0.5, 1  to  1, 0  to  2, 2  to  3, 0  to  4.5, 3  to  6, 0 ;   b   −2, 0  to  −1, 2  to  1, 2  to  2, 0 ;   0, 0  to  0, 1  to  1, 2  to  1, 0  to  2,−1  to  2, 0 .   c   13.5  By taking the Fourier transform of the equation  show that its solution, φ x , can be written as  d2φ dx2  − K 2φ = f x ,  cid:21  ∞ eikx3  f k   −1√  2π  φ x  =  −∞  k2 + K 2 dk,  where3  13.6  13.7  f k  is the Fourier transform of f x . By diﬀerentiating the deﬁnition of the Fourier sine transform ˜fs ω  of the function −1 2 with respect to ω, and then integrating the resulting expression by f t  = t parts, ﬁnd an elementary diﬀerential equation satisﬁed by ˜fs ω . Hence show that this function is its own Fourier sine transform, i.e. ˜fs ω  = Af ω , where A is a limit as x → ∞ of x1 2 sin αx can be taken as zero. constant. Show that it is also its own Fourier cosine transform. Assume that the t < 1,  Find the Fourier transform of the unit rectangular distribution   cid:12   f t  =  1 0  otherwise.  460   Determine the convolution of f with itself and, without further integration,  13.4 EXERCISES  deduce its transform. Deduce that cid:21  ∞  cid:21  ∞  −∞  sin2 ω  ω2  sin4 ω  −∞  ω4  dω = π,  dω =  2π 3  .  13.8  Calculate the Fraunhofer spectrum produced by a diﬀraction grating, uniformly illuminated by light of wavelength 2π k, as follows. Consider a grating with 4N equal strips each of width a and alternately opaque and transparent. The aperture function is then   a  Show, for diﬀraction at angle θ to the normal to the grating, that the required  Fourier transform can be written  −N ≤ n < N,  f y  =  0  otherwise.  A for  2n + 1 a ≤ y ≤  2n + 2 a,  cid:21   N−1 cid:4   2a  f q  =  2π   −1 2  exp −2iarq   A exp −iqu  du,  r=−N  a    3  where q = k sin θ.   b  Evaluate the integral and sum to show that  3  f q  =  2π   −1 2 exp −iqa 2   A sin 2qaN  q cos qa 2   ,  and hence that the intensity distribution I θ  in the spectrum is proportional to  sin2 2qaN  q2 cos2 qa 2   .   c  For large values of N, the numerator in the above expression has very closely spaced maxima and minima as a function of θ and eﬀectively takes its mean value, 1 2, giving a low-intensity background. Much more signiﬁcant peaks in I θ  occur when θ = 0 or the cosine term in the denominator vanishes.  Show that the corresponding values of 3  f q  are  2aNA  2π 1 2  and  4aNA   2π 1 2 2m + 1 π  ,  with m integral.  Note that the constructive interference makes the maxima in I θ  ∝ N2, not N. Of course, observable maxima only occur for 0 ≤ θ ≤ π 2.  13.9  By ﬁnding the complex Fourier series for its LHS show that either side of the equation  δ t + nT   =  −2πnit T e  ∞ cid:4   n=−∞  ∞ cid:4   n=−∞  ∞ cid:4   n=−∞  1 T  ∞ cid:4   461  can represent a periodic train of impulses. By expressing the function f t + nX , in which X is a constant, in terms of the Fourier transform ˜f ω  of f t , show that   cid:7    cid:8   √  f t + nX  =  2π X  ˜f  n=−∞  2nπ X  e2πnit X .  This result is known as the Poisson summation formula.   INTEGRAL TRANSFORMS  13.10  In many applications in which the frequency spectrum of an analogue signal is required, the best that can be done is to sample the signal f t  a ﬁnite number of times at ﬁxed intervals, and then use a discrete Fourier transform Fk to estimate discrete points on the  true  frequency spectrum ˜f ω .   a  By an argument that is essentially the converse of that given in section 13.1, show that, if N samples fn, beginning at t = 0 and spaced τ apart, are taken,  then ˜f 2πk  Nτ   ≈ Fkτ where   b  For the function f t  deﬁned by  N−1 cid:4   n=0  1√ 2π   Fk =  −2πnki N.  fne  f t  =  1 for 0 ≤ t < 1,  0 otherwise,  13.11  13.12  from which eight samples are drawn at intervals of τ = 0.25, ﬁnd a formula  for Fk and evaluate it for k = 0, 1, . . . , 7.   c  Find the exact frequency spectrum of f t  and compare the actual and  √ 2π˜f ω  at ω = kπ for k = 0, 1, . . . , 7. Note the  estimated values of relatively good agreement for k < 4 and the lack of agreement for larger values of k.  For a function f t  that is non-zero only in the range t < T  2, the full frequency spectrum ˜f ω  can be constructed, in principle exactly, from values at discrete sample points ω = n 2π T  . Prove this as follows.   a  Show that the coeﬃcients of a complex Fourier series representation of f t   with period T can be written as  √ 2π T   cid:7   ˜f   cid:8   2πn T  .   cid:8    cid:7   cn =   cid:7   ∞ cid:4   ˜f ω  =  ˜f  n=−∞  2πn T  sinc  nπ − ωT  2   cid:8   ,   b  Use this result to represent f t  as an inﬁnite sum in the deﬁning integral for  ˜f ω , and hence show that  where sinc x is deﬁned as  sin x  x.  A signal obtained by sampling a function x t  at regular intervals T is passed through an electronic ﬁlter, whose response g t  to a unit δ-function input is represented in a tg-plot by straight lines joining  0, 0  to  T , 1 T   to  2T , 0  and is zero for all other values of t. The output of the ﬁlter is the convolution of the input, Using the convolution theorem, and the result given in exercise 13.4, show that the output of the ﬁlter can be written   cid:11 ∞ −∞ x t δ t − nT  , with g t .  cid:21  ∞   cid:7    cid:8   ∞ cid:4   y t  =  1 2π  n=−∞  x nT    −∞ sinc2  ωT 2  −iω[ n+1 T−t]dω.  e  13.13  Find the Fourier transform speciﬁed in part  a  and then use it to answer part  b .  462    a  Find the Fourier transform of  13.4 EXERCISES    f γ, p, t  =  −γt sin pt e 0  t > 0, t < 0,   cid:21  ∞ −∞ K t − u V  u  du,  I t  =  where γ  > 0  and p are constant parameters.   b  The current I t  ﬂowing through a certain system is related to the applied  voltage V  t  by the equation  where  K τ  = a1f γ1, p1, τ  + a2f γ2, p2, τ .  The function f γ, p, t  is as given in  a  and all the ai, γi  > 0  and pi are ﬁxed parameters. By considering the Fourier transform of I t , ﬁnd the relationship that must hold between a1 and a2 if the total net charge Q passed through the system  over a very long time  is to be zero for an arbitrary applied voltage.  Prove the equality  cid:21  ∞   cid:21  ∞  −2at sin2 at dt =  e  1 π  0  a2  4a4 + ω4 dω.  0  A linear ampliﬁer produces an output that is the convolution of its input and its response function. The Fourier transform of the response function for a particular ampliﬁer is  ˜K ω  =  √  iω  2π α + iω 2  .  Determine the time variation of its output g t  when its input is the Heaviside step function.  Consider the Fourier transform of a decaying exponential function and the result of exercise 13.2 b .  In quantum mechanics, two equal-mass particles having momenta pj =  cid:1 kj and energies Ej =  cid:1 ωj and represented by plane wavefunctions φj = exp[i kj·rj−ωj t ], j = 1, 2, interact through a potential V = V  r1 − r2 . In ﬁrst-order perturbation  cid:7   cid:7  j, E  theory the probability of scattering to a state with momenta and energies p is determined by the modulus squared of the quantity  j   cid:21  cid:21  cid:21   M =  ∗ f V ψi dr1 dr2 dt.  ψ   cid:7  1φ   cid:7  2.  The initial state, ψi, is φ1φ2 and the ﬁnal state, ψf , is φ  form of δ-functions.  show that M can be written as the product of three one-dimensional integrals.  b  From two of the integrals deduce energy and momentum conservation in the   a  By writing r1 + r2 = 2R and r1 − r2 = r and assuming that dr1 dr2 = dR dr,  c  Show that M is proportional to the Fourier transform of V , i.e. to 3 where 2 cid:1 k =  p2 − p1  −  p may be approximated by V = r1 − r2−1 exp −µr1 − r2 . Show, using the result −2, where k = k and k  of the worked example in subsection 13.1.10, that the probability that the ion will scatter from, say, p1 to p is as given in part  c  of that exercise.  For some ion–atom scattering processes, the potential V of the previous exercise   cid:7  1 is proportional to  µ2 + k2    cid:7  1  or, alternatively,  cid:1 k = p  − p1.  − p  V  k    cid:7  2   cid:7  1  463  13.14  13.15  13.16  13.17   INTEGRAL TRANSFORMS   cid:21  ∞  cid:21  ∞  1  1  Te =  x 0   −∞ x t  dt,  Be =  ˜x 0   −∞ ˜x ω  dω,  x t  = exp −t T  .  13.18  The equivalent duration and bandwidth, Te and Be, of a signal x t  are deﬁned in terms of the latter and its Fourier transform ˜x ω  by  where neither x 0  nor ˜x 0  is zero. Show that the product TeBe = 2π  this is a form of uncertainty principle , and ﬁnd the equivalent bandwidth of the signal  For this signal, determine the fraction of the total energy that lies in the frequency  range ω < Be 4. You will need the indeﬁnite integral with respect to x of  a2 + x2   −2, which is  13.19  Calculate directly the auto-correlation function a z  for the product f t  of the exponential decay distribution and the Heaviside step function,  Use the Fourier transform and energy spectrum of f t  to deduce that  x  2a2 a2 + x2   +  1 2a3  tan  −1 x a  .  f t  =  −λtH t .  e   cid:21  ∞   cid:7   1 λ   cid:8   eiωz  −∞  λ2 + ω2 dω =  −λz e  π λ  .   cid:9    cid:10   13.20  Prove that the cross-correlation C z  of the Gaussian and Lorentzian distributions  g t  =  a π  1  t2 + a2 ,  f t  =  exp  √ 1 2π  τ  − t2  cid:7  2τ2  ,   cid:8   has as its Fourier transform the function − τ2ω2  cid:7  2  Hence show that  1√ 2π  exp  C z  =  exp  2π  exp −aω .  cid:8   cid:9    cid:10   cos  az τ2  .  a2 − z2  2τ2  √ 1  τ   cid:21  ∞  0  exp −x2  dx = 1  2  √  π.  13.21  Prove the expressions given in table 13.1 for the Laplace transforms of t t1 2, by setting x2 = ts in the result  −1 2 and  13.22  Find the functions y t  whose Laplace transforms are the following:   a  1  s2 − s − 2 ;  b  2s [ s + 1  s2 + 4 ];  c  e  − γ+s t0  [ s + γ 2 + b2].  cid:18   cid:18   cid:18    cid:19   13.23  Use the properties of Laplace transforms to prove the following without evaluat- ing any Laplace integrals explicitly:  a  L  b  L   s + a   s − a   = 15 8  sinh at  t  s > a;   cid:19  √  −7 2; πs = 1 2 ln   cid:19   t5 2  ,  464   13.4 EXERCISES   c  L [sinh at cos bt] = a s2 − a2 + b2 [ s − a 2 + b2]  −1[ s + a 2 + b2]  −1.  13.24  Find the solution  the so-called impulse response or Green’s function  of the equation  T  dx dt  + x = δ t   by proceeding as follows.   a  Show by substitution that  is a solution, for which x 0  = 0, of  x t  = A 1 − e  −t T  H t   T  dx dt  + x = AH t ,   ∗   where H t  is the Heaviside step function.   b  Construct the solution when the RHS of  ∗  is replaced by AH t − τ , with  dx dt = x = 0 for t < τ, and hence ﬁnd the solution when the RHS is a rectangular pulse of duration τ.   c  By setting A = 1 τ and taking the limit as τ → 0, show that the impulse  response is x t  = T  −1e  −t T .   d  Obtain the same result much more directly by taking the Laplace transform of each term in the original equation, solving the resulting algebraic equation and then using the entries in table 13.1.  13.25  This exercise is concerned with the limiting behaviour of Laplace transforms.   a  If f t  = A + g t , where A is a constant and the indeﬁnite integral of g t  is  bounded as its upper limit tends to ∞, show that   b  For t > 0, the function y t  obeys the diﬀerential equation  s¯f s  = A.  lim s→0  d2y dt2  dy dt  + a  + by = c cos2 ωt,  where a, b and c are positive constants. Find ¯y s  and show that s¯y s  → c 2b as s → 0. Interpret the result in the t-domain. By writing f x  as an integral involving the δ-function δ ξ − x  and taking the  Laplace transforms of both sides, show that the transform of the solution of the equation  13.26  for which y and its ﬁrst three derivatives vanish at x = 0 can be written as  Use the properties of Laplace transforms and the entries in table 13.1 to show that  y x  =  x  f ξ  [sinh x − ξ  − sin x − ξ ] dξ.  d4y dx4  − y = f x   cid:21  ∞  ¯y s  =  f ξ   dξ.  0  −sξ e s4 − 1   cid:21   1 2  0  465   13.27  The function fa x  is deﬁned as unity for 0 < x < a and zero otherwise. Find its Laplace transform ¯fa s  and deduce that the transform of xfa x  is  Write fa x  in terms of Heaviside functions and hence obtain an explicit expres- sion for  Use the expression to write ¯ga s  in terms of the functions ¯fa s  and ¯f2a s , and their derivatives, and hence show that ¯ga s  is equal to the square of ¯fa s , in Show that the Laplace transform of f t− a H t− a , where a ≥ 0, is e accordance with the convolution theorem. −as¯f s  and  that, if g t  is a periodic function of period T , ¯g s  can be written as  13.28  INTEGRAL TRANSFORMS   cid:19   −sa  .   cid:18   1 s2  1 −  1 + as e  cid:21   x  fa y fa x − y  dy.  ga x  =  0   cid:21   −sT  1  1 − e  cid:12   T  −stg t  dt.  e  0   a  Sketch the periodic function deﬁned in 0 ≤ t ≤ T by 0 ≤ t < T  2, 2 1 − t T   T  2 ≤ t ≤ T , ∞ cid:4   and, using the previous result, ﬁnd its Laplace transform.   b  Show, by sketching it, that  g t  =  2t T  is another representation of g t  and hence derive the relationship  2 nT  H t − 1  2 nT  ]  2 T  [tH t  + 2  n=1   −1 n t − 1 ∞ cid:4   tanh x = 1 + 2   −1 ne  −2nx.  n=1  13.5 Hints and answers   cid:7  cid:7   φ  −a b+ik .  −1.  k  = −k2 ˜φ k  to obtain an algebraic equation for ˜φ k  and then  √ 2π [ b − ik   b2 + k2 ]e √ use the Fourier inversion formula. 2π  sin ω ω .  2  √ 2π  sin2 ω ω2 .  4   Note that the integrand has diﬀerent analytic forms for t < 0 and t ≥ 0. Use or derive4  2 π 1 2 1 + ω2   1  The convolution is 2−t for t < 2, zero otherwise. Use the convolution theorem. Apply Parseval’s theorem to f and to f ∗ f. t → ω, n → −n and T → 2π X and apply the translation theorem.  b  Recall that the inﬁnite integral involved in deﬁning ˜f ω  has a non-zero integrand only in t < T  2.  −1, independent of n. Make the changes of variables  The Fourier coeﬃcient is T  √ 2π {p [ γ + iω 2 + p2]}. √ 2 + p2 2π α + iω 2], leading to g t  = te   a   1   b  Show that Q = relationship is a1p1  γ2 ˜g ω  = 1 [  √  2  = 0. −αt.  2π ˜I 0  and use the convolution theorem. The required 1 + p2  1  + a2p2  γ2  13.1  13.3 13.5  13.7  13.9  13.11  13.13  13.15  466   13.5 HINTS AND ANSWERS   cid:1  {exp[− µ − ik r] − exp[− µ + ik r]} dr.  cid:1   3 V  k  ∝ [−2π  ik ] Note that the lower limit in the calculation of a z  is 0, for z > 0, and z, for z < 0. Auto-correlation a z  = [ 1  2λ3 ] exp −λz . Prove the result for t1 2 by integrating that for t  a  Use  13.62  with n = 2 on L  c  consider L [exp ±at  cos bt] and use the translation property, subsection 13.2.2.  a  Note that  lim  b   s2 + as + b ¯y s  = {c s2 + 2ω2  [s s2 + 4ω2 ]} +  a + s y 0  + y For this damped system, at large t  corresponding to s → 0  rates of change are negligible and the equation reduces to by = c cos2 ωt. The average value of cos2 ωt is 1 2 . −1[1 − exp −sa ]; ga x  = x for 0 < x < a, ga x  = 2a − x for a ≤ x ≤ 2a, s ga x  = 0 otherwise.   cid:19   cid:18 √  cid:1  −st dt ≤  lim  −1 2 by parts.  ;  b  use  13.63 ;  g t  dt.  g t e   0 .  t   cid:7   13.17 13.19  13.21 13.23  13.25  13.27  467   First-order ordinary diﬀerential  14  equations  Diﬀerential equations are the group of equations that contain derivatives. Chap- ters 14–21 discuss a variety of diﬀerential equations, starting in this chapter and the next with those ordinary diﬀerential equations  ODEs  that have closed-form solutions. As its name suggests, an ODE contains only ordinary derivatives  no partial derivatives  and describes the relationship between these derivatives of the dependent variable, usually called y, with respect to the independent variable, usually called x. The solution to such an ODE is therefore a function of x and is written y x . For an ODE to have a closed-form solution, it must be possible to express y x  in terms of the standard elementary functions such as exp x, ln x, sin x etc. The solutions of some diﬀerential equations cannot, however, be written in closed form, but only as an inﬁnite series; these are discussed in chapter 16.  Ordinary diﬀerential equations may be separated conveniently into diﬀer- ent categories according to their general characteristics. The primary grouping adopted here is by the order of the equation. The order of an ODE is simply the order of the highest derivative it contains. Thus equations containing dy dx, but no higher derivatives, are called ﬁrst order, those containing d2y dx2 are called second order and so on. In this chapter we consider ﬁrst-order equations, and in the next, second- and higher-order equations.  Ordinary diﬀerential equations may be classiﬁed further according to degree. The degree of an ODE is the power to which the highest-order derivative is raised, after the equation has been rationalised to contain only integer powers of derivatives. Hence the ODE   cid:7    cid:8   3 2  d3y dx3 + x  dy dx  + x2y = 0,  is of third order and second degree, since after rationalisation it contains the term  d3y dx3 2.  The general solution to an ODE is the most general function y x  that satisﬁes the equation; it will contain constants of integration which may be determined by  468   14.1 GENERAL FORM OF SOLUTION  the application of some suitable boundary conditions. For example, we may be told that for a certain ﬁrst-order diﬀerential equation, the solution y x  is equal to zero when the parameter x is equal to unity; this allows us to determine the value of the constant of integration. The general solutions to nth-order ODEs, which are considered in detail in the next chapter, will contain n  essential  arbitrary constants of integration and therefore we will need n boundary conditions if these constants are to be determined  see section 14.1 . When the boundary conditions have been applied, and the constants found, we are left with a particular solution to the ODE, which obeys the given boundary conditions. Some ODEs of degree greater than unity also possess singular solutions, which are solutions that contain no arbitrary constants and cannot be found from the general solution; singular solutions are discussed in more detail in section 14.3. When any solution to an ODE has been found, it is always possible to check its validity by substitution into the original equation and veriﬁcation that any given boundary conditions are met.  In this chapter, ﬁrstly we discuss various types of ﬁrst-degree ODE and then go on to examine those higher-degree equations that can be solved in closed form. At the outset, however, we discuss the general form of the solutions of ODEs; this discussion is relevant to both ﬁrst- and higher-order ODEs.  14.1 General form of solution  It is helpful when considering the general form of the solution of an ODE to consider the inverse process, namely that of obtaining an ODE from a given group of functions, each one of which is a solution of the ODE. Suppose the members of the group can be written as  each member being speciﬁed by a diﬀerent set of values of the parameters ai. For example, consider the group of functions  y = f x, a1, a2, . . . , an ,  y = a1 sin x + a2 cos x;   14.1    14.2   here n = 2.  Since an ODE is required for which any of the group is a solution, it clearly must not contain any of the ai. As there are n of the ai in expression  14.1 , we must obtain n + 1 equations involving them in order that, by elimination, we can obtain one ﬁnal equation without them.  Initially we have only  14.1 , but if this is diﬀerentiated n times, a total of n + 1 equations is obtained from which  in principle  all the ai can be eliminated, to give one ODE satisﬁed by all the group. As a result of the n diﬀerentiations, dny dxn will be present in one of the n + 1 equations and hence in the ﬁnal equation, which will therefore be of nth order.  469   FIRST-ORDER ORDINARY DIFFERENTIAL EQUATIONS  In the case of  14.2 , we have  dy dx d2y  = a1 cos x − a2 sin x, dx2 = −a1 sin x − a2 cos x.  d2y dx2 + y = 0,  Here the elimination of a1 and a2 is trivial  because of the similarity of the forms of y and d2y dx2 , resulting in  a second-order equation.  Thus, to summarise, a group of functions  14.1  with n parameters satisﬁes an nth-order ODE in general  although in some degenerate cases an ODE of less than nth order is obtained . The intuitive converse of this is that the general solution of an nth-order ODE contains n arbitrary parameters  constants ; for our purposes, this will be assumed to be valid although a totally general proof is diﬃcult.  As mentioned earlier, external factors aﬀect a system described by an ODE, by ﬁxing the values of the dependent variables for particular values of the independent ones. These externally imposed  or boundary  conditions on the solution are thus the means of determining the parameters and so of specifying precisely which function is the required solution. It is apparent that the number of boundary conditions should match the number of parameters and hence the order of the equation, if a unique solution is to be obtained. Fewer independent boundary conditions than this will lead to a number of undetermined parameters in the solution, whilst an excess will usually mean that no acceptable solution is possible.  For an nth-order equation the required n boundary conditions can take many forms, for example the value of y at n diﬀerent values of x, or the value of any  n− 1 of the n derivatives dy dx, d2y dx2, . . . , dny dxn together with that of y, all  for the same value of x, or many intermediate combinations.  14.2 First-degree ﬁrst-order equations  First-degree ﬁrst-order ODEs contain only dy dx equated to some function of x and y, and can be written in either of two equivalent standard forms,  dy dx  = F x, y ,  A x, y  dx + B x, y  dy = 0,  where F x, y  = −A x, y  B x, y , and F x, y , A x, y  and B x, y  are in general  functions of both x and y. Which of the two above forms is the more useful for ﬁnding a solution depends on the type of equation being considered. There  470   14.2 FIRST-DEGREE FIRST-ORDER EQUATIONS  are several diﬀerent types of ﬁrst-degree ﬁrst-order ODEs that are of interest in the physical sciences. These equations and their respective solutions are discussed below.  14.2.1 Separable-variable equations  A separable-variable equation is one which may be written in the conventional form  dy dx  = f x g y ,   14.3   where f x  and g y  are functions of x and y respectively, including cases in which f x  or g y  is simply a constant. Rearranging this equation so that the terms depending on x and on y appear on opposite sides  i.e. are separated , and integrating, we obtain   cid:21    cid:21   dy g y   =  f x  dx.  Finding the solution y x  that satisﬁes  14.3  then depends only on the ease with which the integrals in the above equation can be evaluated. It is also worth noting that ODEs that at ﬁrst sight do not appear to be of the form  14.3  can sometimes be made separable by an appropriate factorisation.  cid:1 Solve  Since the RHS of this equation can be factorised to give x 1 + y , the equation becomes separable and we obtain  dy dx  = x + xy.   cid:21   dy  1 + y  =  x dx.  ln 1 + y  =  + c,   cid:21   x2 2   cid:8    cid:7   x2 2   cid:7    cid:8   x2 2  ,  Now integrating both sides separately, we ﬁnd  and so  1 + y = exp  + c  = A exp  where c and hence A is an arbitrary constant.  cid:2   Solution method. Factorise the equation so that it becomes separable. Rearrange it so that the terms depending on x and those depending on y appear on opposite sides and then integrate directly. Remember the constant of integration, which can be evaluated if further information is given.  471   FIRST-ORDER ORDINARY DIFFERENTIAL EQUATIONS  An exact ﬁrst-degree ﬁrst-order ODE is one of the form  14.2.2 Exact equations  A x, y  dx + B x, y  dy = 0  and for which   14.4   ∂A ∂y  =  ∂B ∂x  .  In this case A x, y  dx + B x, y  dy is an exact diﬀerential, dU x, y  say  see section 5.3 . In other words  A dx + B dy = dU =  dx +  dy,  ∂U ∂y  from which we obtain   14.5    14.6    14.7   Since ∂2U ∂x∂y = ∂2U ∂y∂x we therefore require  If  14.7  holds then  14.4  can be written dU x, y  = 0, which has the solution U x, y  = c, where c is a constant and from  14.5  U x, y  is given by  U x, y  =  A x, y  dx + F y .   14.8   The function F y  can be found from  14.6  by diﬀerentiating  14.8  with respect to y and equating to B x, y .  cid:1 Solve  Rearranging into the form  14.4  we have  x  dy dx  + 3x + y = 0.   3x + y  dx + x dy = 0,   cid:21   i.e. A x, y  = 3x + y and B x, y  = x. Since ∂A ∂y = 1 = ∂B ∂x, the equation is exact, and by  14.8  the solution is given by  U x, y  =   3x + y  dx + F y  = c1 ⇒ 3x2  + yx + F y  = c1.  2  Diﬀerentiating U x, y  with respect to y and equating it to B x, y  = x we obtain dF dy = 0,  which integrates immediately to give F y  = c2. Therefore, letting c = c1 − c2, the solution  to the original ODE is  ∂U ∂x  ∂U ∂x ∂U ∂y  ,  .  A x, y  =  B x, y  =  ∂A ∂y  =  ∂B ∂x  .   cid:21   + xy = c.  cid:2   3x2 2  472   14.2 FIRST-DEGREE FIRST-ORDER EQUATIONS  Solution method. Check that the equation is an exact diﬀerential using  14.7  then solve using  14.8 . Find the function F y  by diﬀerentiating  14.8  with respect to y and using  14.6 .  14.2.3 Inexact equations: integrating factors  Equations that may be written in the form  A x, y  dx + B x, y  dy = 0  but for which   14.9    cid:3 =  ∂B ∂x  ∂A ∂y  are known as inexact equations. However, the diﬀerential A dx + B dy can always be made exact by multiplying by an integrating factor µ x, y , which obeys  ∂ µA   ∂ µB   =  .  ∂x  ∂y   14.10   For an integrating factor that is a function of both x and y, i.e. µ = µ x, y , there exists no general method for ﬁnding it; in such cases it may sometimes be found by inspection. If, however, an integrating factor exists that is a function of either x or y alone then  14.10  can be solved to ﬁnd it. For example, if we assume that the integrating factor is a function of x alone, i.e. µ = µ x , then  14.10  reads  Rearranging this expression we ﬁnd  µ  ∂A ∂y  = µ  + B  ∂B ∂x  dµ dx  .   cid:7    cid:8   dµ µ  =  1 B  − ∂B  ∂x  ∂A ∂y  dx = f x  dx,  where we require f x  also to be a function of x only; indeed this provides a general method of determining whether the integrating factor µ is a function of x alone. This integrating factor is then given by  µ x  = exp  f x  dx  where  f x  =   14.11    cid:12  cid:21    cid:12  cid:21    cid:15    cid:15   Similarly, if µ = µ y  then   cid:7    cid:7   1 B  1 A  − ∂B  ∂x  ∂A ∂y  − ∂A  ∂y  ∂B ∂x   cid:8    cid:8   .  .  µ y  = exp  g y  dy  where  g y  =   14.12   473   ∂B ∂x   cid:8   = 2y,  =  2 x  ,  ∂A ∂y  = 6y,   cid:7   1 B   cid:12   ∂x  − ∂B  cid:15   ∂A ∂y   cid:21   dx x  FIRST-ORDER ORDINARY DIFFERENTIAL EQUATIONS   cid:1 Solve  = − 2  y  − 3y 2x  .  dy dx  Rearranging into the form  14.9 , we have  i.e. A x, y  = 4x + 3y2 and B x, y  = 2xy. Now   4x + 3y2  dx + 2xy dy = 0,   14.13   so the ODE is not exact in its present form. However, we see that  a function of x alone. Therefore an integrating factor exists that is also a function of x alone and, ignoring the arbitrary constant of integration, is given by  µ x  = exp  2  = exp 2 ln x  = x2.  Multiplying  14.13  through by µ x  = x2 we obtain   4x3 + 3x2y2  dx + 2x3y dy = 4x3 dx +  3x2y2 dx + 2x3y dy  = 0.  By inspection this integrates immediately to give the solution x4 + y2x3 = c, where c is a constant.  cid:2   Solution method. Examine whether f x  and g y  are functions of only x or y respectively. If so, then the required integrating factor is a function of either x or y only, and is given by  14.11  or  14.12  respectively. If the integrating factor is a function of both x and y, then sometimes it may be found by inspection or by trial and error. In any case, the integrating factor µ must satisfy  14.10 . Once the equation has been made exact, solve by the method of subsection 14.2.2.  14.2.4 Linear equations  Linear ﬁrst-order ODEs are a special case of inexact ODEs  discussed in the previous subsection  and can be written in the conventional form  dy dx  + P  x y = Q x .   14.14   Such equations can be made exact by multiplying through by an appropriate integrating factor in a similar manner to that discussed above. In this case, however, the integrating factor is always a function of x alone and may be expressed in a particularly simple form. An integrating factor µ x  must be such that  µ x   + µ x P  x y =  [ µ x y] = µ x Q x ,   14.15   dy dx  d dx  474   14.2 FIRST-DEGREE FIRST-ORDER EQUATIONS  which may then be integrated directly to give   cid:21   µ x y =  µ x Q x  dx.   14.16   The required integrating factor µ x  is determined by the ﬁrst equality in  14.15 , i.e.  which immediately gives the simple relation  d dx  dy dx  dµ dx   µy  = µ  +  y = µ  + µPy,  dy dx   cid:12  cid:21    cid:15   = µ x P  x  ⇒ µ x  = exp  dµ dx  P  x  dx  .   14.17    cid:1 Solve  + 2xy = 4x.   cid:15   dy dx   cid:12  cid:21    cid:21   The integrating factor is given immediately by  µ x  = exp  2x dx  = exp x2.  Multiplying through the ODE by µ x  = exp x2 and integrating, we have  y exp x2 = 4  x exp x2 dx = 2 exp x2 + c.  The solution to the ODE is therefore given by y = 2 + c exp −x2 .  cid:2   Solution method. Rearrange the equation into the form  14.14  and multiply by the integrating factor µ x  given by  14.17 . The left- and right-hand sides can then be integrated directly, giving y from  14.16 .  Homogeneous equation are ODEs that may be written in the form  14.2.5 Homogeneous equations   cid:9    cid:10   y x  ,  dy dx  =  A x, y  B x, y   = F   14.18   where A x, y  and B x, y  are homogeneous functions of the same degree. A function f x, y  is homogeneous of degree n if, for any λ, it obeys  f λx, λy  = λnf x, y .  For example, if A = x2y − xy2 and B = x3 + y3 then we see that A and B are  both homogeneous functions of degree 3. In general, for functions of the form of A and B, we see that for both to be homogeneous, and of the same degree, we require the sum of the powers in x and y in each term of A and B to be the same  475   FIRST-ORDER ORDINARY DIFFERENTIAL EQUATIONS   in this example equal to 3 . The RHS of a homogeneous ODE can be written as a function of y x. The equation may then be solved by making the substitution y = vx, so that  This is now a separable equation and can be integrated directly to give   14.19   dy dx   cid:21   = v + x  = F v .  dv dx  dv  F v  − v  =  dx x  .   cid:21    cid:9    cid:10   y x  .  dy dx  y x  =  + tan   cid:1 Solve  Substituting y = vx we obtain  Cancelling v on both sides, rearranging and integrating gives  v + x  = v + tan v.   cid:21    cid:21   dv dx   cid:21    cid:21   cot v dv =  = ln x + c1.  dx x  But  so the solution to the ODE is y = x sin  cot v dv =  dv = ln sin v  + c2,  cos v sin v −1 Ax, where A is a constant.  cid:2   Solution method. Check to see whether the equation is homogeneous. If so, make the substitution y = vx, separate variables as in  14.19  and then integrate directly. Finally replace v by y x to obtain the solution.  14.2.6 Isobaric equations  An isobaric ODE is a generalisation of the homogeneous ODE discussed in the previous section, and is of the form  dy dx  =  A x, y  B x, y   ,   14.20   where the equation is dimensionally consistent if y and dy are each given a weight m relative to x and dx, i.e. if the substitution y = vxm makes it separable.  476   14.2 FIRST-DEGREE FIRST-ORDER EQUATIONS   cid:1 Solve  Rearranging we have   cid:7    cid:8   dy dx  =  y2 +  .  2 x  2yx  −1  cid:8    cid:7   y2 +  2 x  dx + 2yx dy = 0.  v dv +  = 0,  dx x  Giving y and dy the weight m and x and dx the weight 1, the sums of the powers in each term on the LHS are 2m + 1, 0 and 2m + 1 respectively. These are equal if 2m + 1 = 0, i.e. −3 2 dx,  −1 2, with the result that dy = x  −1 2 dv − 1  if m = − 1  2 . Substituting y = vxm = vx  2 vx  we obtain  √ which is separable and may be integrated directly to give 1 y  x we obtain the solution 1  2 y2x + ln x = c.  cid:2   2 v2 + ln x = c. Replacing v by  Solution method. Write the equation in the form A dx + B dy = 0. Giving y and dy each a weight m and x and dx each a weight 1, write down the sum of powers in each term. Then, if a value of m that makes all these sums equal can be found, substitute y = vxm into the original equation to make it separable. Integrate the separated equation directly, and then replace v by yx  −m to obtain the solution.  14.2.7 Bernoulli’s equation  Bernoulli’s equation has the form  dy dx  + P  x y = Q x yn  where n  cid:3 = 0 or 1.   14.21   This equation is very similar in form to the linear equation  14.14 , but is in fact non-linear due to the extra yn factor on the RHS. However, the equation can be made linear by substituting v = y1−n and correspondingly   cid:7    cid:8   dy dx  =  yn 1 − n  dv dx  .  Substituting this into  14.21  and dividing through by yn, we ﬁnd  +  1 − n P  x v =  1 − n Q x ,  dv dx  which is a linear equation and may be solved by the method described in subsection 14.2.4.  477   FIRST-ORDER ORDINARY DIFFERENTIAL EQUATIONS   cid:1 Solve  If we let v = y1−4 = y  −3 then  dy dx  y x  +  = 2x3y4.  = − y4  3  dv dx  .  dy dx  = −6x3,  dv dx  x  − 3v  cid:15   dx x   cid:12    cid:21   −3  exp  = exp −3 ln x  =  1 x3 .  Substituting this into the ODE and rearranging, we obtain  which is linear and may be solved by multiplying through by the integrating factor  see subsection 14.2.4   This yields the solution  Remembering that v = y  v x3 −3, we obtain y  = −6x + c. −3 = −6x4 + cx3.  cid:2   Solution method. Rearrange the equation into the form  14.21  and make the sub-  stitution v = y1−n. This leads to a linear equation in v, which can be solved by the method of subsection 14.2.4. Then replace v by y1−n to obtain the solution.  14.2.8 Miscellaneous equations  There are two further types of ﬁrst-degree ﬁrst-order equation that occur fairly regularly but do not fall into any of the above categories. They may be reduced to one of the above equations, however, by a suitable change of variable.  Firstly, we consider  = F ax + by + c ,   14.22   where a, b and c are constants, i.e. x and y only appear on the RHS in the particular combination ax + by + c and not in any other combination or by themselves. This equation can be solved by making the substitution v = ax + by + c, in which case  = a + b  = a + bF v ,   14.23   which is separable and may be integrated directly.  dy dx  dv dx  dy dx  478   14.2 FIRST-DEGREE FIRST-ORDER EQUATIONS   cid:1 Solve  dy dx  =  x + y + 1 2.  Making the substitution v = x + y + 1, we obtain, as in  14.23 ,  which is separable and integrates directly to give   cid:21   = v2 + 1,  dv dx   cid:21   dv  1 + v2  =  −1 v = x + c1.  dx ⇒ tan −1 x + y + 1  = x + c1, where c1 is a constant of  So the solution to the original ODE is tan integration.  cid:2   Solution method. In an equation such as  14.22 , substitute v = ax+by+c to obtain a separable equation that can be integrated directly. Then replace v by ax + by + c to obtain the solution.  Secondly, we discuss  where a, b, c, e, f and g are all constants. This equation may be solved by letting x = X + α and y = Y + β, where α and β are constants found from   14.24    14.25    14.26   Then  14.24  can be written as  which is homogeneous and can be solved by the method of subsection 14.2.5. Note, however, that if a e = b f then  14.25  and  14.26  are not independent and so cannot be solved uniquely for α and β. However, in this case,  14.24  reduces to an equation of the form  14.22 , which was discussed above.  cid:1 Solve  Let x = X + α and y = Y + β, where α and β obey the relations  which solve to give α = β = 1. Making these substitutions we ﬁnd  dy dx  =  ax + by + c ex + fy + g  ,  aα + bβ + c = 0  eα + fβ + g = 0.  dY dX  =  aX + bY eX + fY  ,  2x − 5y + 3 2x + 4y − 6  .  dy dx  =  2α − 5β + 3 = 0 2α + 4β − 6 = 0, 2X − 5Y  2X + 4Y  ,  dY dX  =  479   FIRST-ORDER ORDINARY DIFFERENTIAL EQUATIONS  which is a homogeneous ODE and can be solved by substituting Y = vX  see subsec- tion 14.2.5  to obtain  This equation is separable, and using partial fractions we ﬁnd   cid:21   dv dX  =  2 − 7v − 4v2  cid:21   X 2 + 4v   dv  4v − 1  − 2 3  .   cid:21    cid:21   dv  v + 2  =  dX X  ,  2 + 4v  2 − 7v − 4v2 dv = − 4  3  which integrates to give  or  ln X + 1  3 ln 4v − 1  + 2  3 ln v + 2  = c1,  X 3 4v − 1  v + 2 2 = exp 3c1.  Remembering that Y = vX, x = X + 1 and y = Y + 1, the solution to the original ODE  is given by  4y − x − 3  y + 2x − 3 2 = c2, where c2 = exp 3c1.  cid:2  Solution method. If in  14.24  a e  cid:3 = b f then make the substitution x = X + α,  y = Y + β, where α and β are given by  14.25  and  14.26 ; the resulting equation is homogeneous and can be solved as in subsection 14.2.5. Substitute v = Y  X,  X = x − α and Y = y − β to obtain the solution. If a e = b f then  14.24  is of  the same form as  14.22  and may be solved accordingly.  14.3 Higher-degree ﬁrst-order equations  First-order equations of degree higher than the ﬁrst do not occur often in the description of physical systems, since squared and higher powers of ﬁrst- order derivatives usually arise from resistive or driving mechanisms, when an acceleration or other higher-order derivative is also present. They do sometimes appear in connection with geometrical problems, however.  Higher-degree ﬁrst-order equations can be written as F x, y, dy dx  = 0. The  most general standard form is  pn + an−1 x, y pn−1 + ··· + a1 x, y p + a0 x, y  = 0,   14.27   where for ease of notation we write p = dy dx. If the equation can be solved for one of x, y or p then either an explicit or a parametric solution can sometimes be obtained. We discuss the main types of such equations below, including Clairaut’s equation, which is a special case of an equation explicitly soluble for y.  14.3.1 Equations soluble for p  Sometimes the LHS of  14.27  can be factorised into the form   p − F1  p − F2 ···  p − Fn  = 0,   14.28   480   14.3 HIGHER-DEGREE FIRST-ORDER EQUATIONS   14.29    14.30   where Fi = Fi x, y . We are then left with solving the n ﬁrst-degree equations p = Fi x, y . Writing the solutions to these ﬁrst-degree equations as Gi x, y  = 0, the general solution to  14.28  is given by the product  G1 x, y G2 x, y ··· Gn x, y  = 0.   cid:1 Solve   x3 + x2 + x + 1 p2 −  3x2 + 2x + 1 yp + 2xy2 = 0.  This equation may be factorised to give  [ x + 1 p − y][ x2 + 1 p − 2xy] = 0.  Taking each bracket in turn we have  dy dx  − y = 0, − 2xy = 0,   x + 1    x2 + 1   dy dx  which have the solutions y − c x + 1  = 0 and y − c x2 + 1  = 0 respectively  see  section 14.2 on ﬁrst-degree ﬁrst-order equations . Note that the arbitrary constants in these two solutions can be taken to be the same, since only one is required for a ﬁrst-order equation. The general solution to  14.30  is then given by   cid:18    cid:19   [y − c x + 1 ]  y − c x2 + 1   = 0.  cid:2   Solution method. If the equation can be factorised into the form  14.28  then solve  the ﬁrst-order ODE p − Fi = 0 for each factor and write the solution in the form  Gi x, y  = 0. The solution to the original equation is then given by the product  14.29 .  14.3.2 Equations soluble for x  Equations that can be solved for x, i.e. such that they may be written in the form  x = F y, p ,   14.31   can be reduced to ﬁrst-degree ﬁrst-order equations in p by diﬀerentiating both sides with respect to y, so that  This results in an equation of the form G y, p  = 0, which can be used together with  14.31  to eliminate p and give the general solution. Note that often a singular solution to the equation will be found at the same time  see the introduction to this chapter .  dx dy  =  =  1 p  ∂F ∂y  +  ∂F ∂p  dp dy  .  481   FIRST-ORDER ORDINARY DIFFERENTIAL EQUATIONS   cid:1 Solve  6y2p2 + 3xp − y = 0.   14.32   This equation can be solved for x explicitly to give 3x =  y p − 6y2p. Diﬀerentiating both  sides with respect to y, we ﬁnd  which factorises to give  − 12yp,  3  dx dy  3 p  =  1 p  =   cid:5   − y p2   cid:6  cid:7   dp dy  − 6y2 dp  cid:8   dy  dp dy  1 + 6yp2  2p + y  = 0.   14.33   Setting the factor containing dp dy equal to zero gives a ﬁrst-degree ﬁrst-order equation in p, which may be solved to give py2 = c. Substituting for p in  14.32  then yields the general solution of  14.32 :  If we now consider the ﬁrst factor in  14.33 , we ﬁnd 6p2y = −1 as a possible solution.  y3 = 3cx + 6c2.   14.34   Substituting for p in  14.32  we ﬁnd the singular solution  8y3 + 3x2 = 0.  Note that the singular solution contains no arbitrary constants and cannot be found from the general solution  14.34  by any choice of the constant c.  cid:2   Solution method. Write the equation in the form  14.31  and diﬀerentiate both sides with respect to y. Rearrange the resulting equation into the form G y, p  = 0, which can be used together with the original ODE to eliminate p and so give the general solution. If G y, p  can be factorised then the factor containing dp dy should be used to eliminate p and give the general solution. Using the other factors in this fashion will instead lead to singular solutions.  14.3.3 Equations soluble for y  Equations that can be solved for y, i.e. are such that they may be written in the form  y = F x, p ,   14.35   can be reduced to ﬁrst-degree ﬁrst-order equations in p by diﬀerentiating both sides with respect to x, so that  This results in an equation of the form G x, p  = 0, which can be used together with  14.35  to eliminate p and give the general solution. An additional  singular  solution to the equation is also often found.  dy dx  = p =  ∂F ∂x  +  ∂F ∂p  dp dx  .  482   14.3 HIGHER-DEGREE FIRST-ORDER EQUATIONS   cid:1 Solve  xp2 + 2xp − y = 0.   14.36   This equation can be solved for y explicitly to give y = xp2 + 2xp. Diﬀerentiating both sides with respect to x, we ﬁnd  which after factorising gives  dy dx  = p = 2xp  + p2 + 2x  + 2p,  dp dx   cid:7   dp dx   cid:8   dp dx   p + 1   p + 2x  = 0.   14.37   To obtain the general solution of  14.36 , we consider the factor containing dp dx. This ﬁrst-degree ﬁrst-order equation in p has the solution xp2 = c  see subsection 14.3.1 , which we then use to eliminate p from  14.36 . Thus we ﬁnd that the general solution to  14.36  is   y − c 2 = 4cx.   14.38   If instead, we set the other factor in  14.37  equal to zero, we obtain the very simple  solution p = −1. Substituting this into  14.36  then gives  which is a singular solution to  14.36 .  cid:2   x + y = 0,  Solution method. Write the equation in the form  14.35  and diﬀerentiate both sides with respect to x. Rearrange the resulting equation into the form G x, p  = 0, which can be used together with the original ODE to eliminate p and so give the general solution. If G x, p  can be factorised then the factor containing dp dx should be used to eliminate p and give the general solution. Using the other factors in this fashion will instead lead to singular solutions.  Finally, we consider Clairaut’s equation, which has the form  14.3.4 Clairaut’s equation  y = px + F p    14.39   and is therefore a special case of equations soluble for y, as in  14.35 . It may be solved by a similar method to that given in subsection 14.3.3, but for Clairaut’s equation the form of the general solution is particularly simple. Diﬀerentiating  14.39  with respect to x, we ﬁnd   cid:7    cid:8   dy dx  = p = p + x  +  dp dx  dF dp  dp dx  ⇒ dp  dx  dF dp  + x  = 0.   14.40   Considering ﬁrst the factor containing dp dx, we ﬁnd  dp dx  =  d2y  dx2 = 0 ⇒ y = c1x + c2.   14.41   483   FIRST-ORDER ORDINARY DIFFERENTIAL EQUATIONS  Since p = dy dx = c1, if we substitute  14.41  into  14.39  we ﬁnd c1x + c2 = c1x + F c1 . Therefore the constant c2 is given by F c1 , and the general solution to  14.39  is  y = c1x + F c1 ,   14.42   i.e. the general solution to Clairaut’s equation can be obtained by replacing p in the ODE by the arbitrary constant c1. Now, considering the second factor in  14.40 , we also have  dF dp  + x = 0,   14.43   which has the form G x, p  = 0. This relation may be used to eliminate p from  14.39  to give a singular solution.  cid:1 Solve  y = px + p2.   14.44   From  14.42  the general solution is y = cx + c2. But from  14.43  we also have 2p + x = 0 ⇒ p = −x 2. Substituting this into  14.44  we ﬁnd the singular solution x2 + 4y = 0.  cid:2   Solution method. Write the equation in the form  14.39 , then the general solution is given by replacing p by some constant c, as shown in  14.42 . Using the relation dF dp + x = 0 to eliminate p from the original equation yields the singular solution.  14.4 Exercises  = −λN.  dN dt  14.1  A radioactive isotope decays in such a way that the number of atoms present at a given time, N t , obeys the equation  14.2  If there are initially N0 atoms present, ﬁnd N t  at later times. Solve the following equations by separation of the variables:   cid:7  − xy3 = 0;  cid:7  tan  cid:7   −1 x − y 1 + x2  + xy2 = 4y2.   a  y  b  y  c  x2y  −1 = 0;  14.3  Show that the following equations either are exact or can be made exact, and solve them:   cid:7   + x y4 + 1  = 0;   cid:7    a  y 2x2y2 + 1 y  b  2xy  c   + 3x + y = 0;  cos2 x + y sin 2x y   cid:7   + y2 = 0.   cid:7    cid:8   14.4  Find the values of α and β that make  dF x, y  =  dx +  xyβ + 1  dy  1  x2 + 2  +  α y  an exact diﬀerential. For these values solve F x, y  = 0.  484   14.5  By ﬁnding suitable integrating factors, solve the following equations:   cid:7   + 2xy =  1 − x2 3 2;   1 − x2 y  cid:7  − y cot x + cosec x = 0;  x + y3 y   cid:7    a   b  y  c   = y  treat y as the independent variable .  14.6  By ﬁnding an appropriate integrating factor, solve  14.7  Find, in the form of an integral, the solution of the equation  14.4 EXERCISES  = − 2x2 + y2 + x  .  xy  dy dx  α  dy dt  + y = f t   for a general function f t . Find the speciﬁc solutions for   a  f t  = H t ,  b  f t  = δ t , −1e  c  f t  = β  −t βH t  with β < α. For case  c , what happens if β → 0?  14.8  A series electric circuit contains a resistance R, a capacitance C and a battery supplying a time-varying electromotive force V  t . The charge q on the capacitor therefore obeys the equation  R  dq dt  q C  +  = V  t .  14.9  Assuming that initially there is no charge on the capacitor, and given that V  t  = V0 sin ωt, ﬁnd the charge on the capacitor as a function of time. Using tangential–polar coordinates  see exercise 2.20 , consider a particle of mass m moving under the inﬂuence of a force f directed towards the origin O. By resolving forces along the instantaneous tangent and normal and making use of the result of exercise 2.20 for the instantaneous radius of curvature, prove that  f = −mv  dv dr  and  mv2 = fp  dr dp  .  Show further that h = mpv is a constant of the motion and that the law of force can be deduced from  f =  h2 mp3  dp dr  .  14.10  Use the result of exercise 14.9 to ﬁnd the law of force, acting towards the origin, under which a particle must move so as to describe the following trajectories:   a  A circle of radius a that passes through the origin;  b  An equiangular spiral, which is deﬁned by the property that the angle α  between the tangent and the radius vector is constant along the curve.  14.11  Solve  14.12  A mass m is accelerated by a time-varying force α exp −βt v3, where v is its  velocity. It also experiences a resistive force ηv, where η is a constant, owing to its motion through the air. The equation of motion of the mass is therefore   y − x   dy dx  + 2x + 3y = 0.  = α exp −βt v3 − ηv.  m  dv dt  485   FIRST-ORDER ORDINARY DIFFERENTIAL EQUATIONS  14.13  Find an expression for the velocity v of the mass as a function of time, given that it has an initial velocity v0. Using the results about Laplace transforms given in chapter 13 for df dt and tf t , show, for a function y t  that satisﬁes  with y 0  ﬁnite, that ¯y s  = C 1 + s   Given that   ∗   −2 for some constant C.  t  dy dt  +  t − 1 y = 0 ∞ cid:4   y t  = t +  antn,  n=2  determine C and show that an =  −1 n−1  n − 1 !. Compare this result with that obtained by integrating  ∗  directly.  14.16  If u = 1 + tan y, calculate d ln u  dy; hence ﬁnd the general solution of  14.14  Solve  14.15  Solve  14.17  Solve  dy dx  =  1  x + 2y + 1  .  = − x + y  3x + 3y − 4  .  dy dx  dy dx  = tan x cos y  cos y + sin y .  x 1 − 2x2y   dy dx  + y = 3x2y2,  y x  =  2p  1 − p2 ,   cid:7    cid:8   2  x  dy dx  +  dy dx  − y = 0  p = p2 +  2px + 1   dp dx  .  486  14.18  given that y 1  = 1 2. A reﬂecting mirror is made in the shape of the surface of revolution generated by revolving the curve y x  about the x-axis. In order that light rays emitted from a point source at the origin are reﬂected back parallel to the x-axis, the curve y x  must obey  14.19  14.20  where p = dy dx. By solving this equation for x, ﬁnd the curve y x . Find the curve with the property that at each point on it the sum of the intercepts on the x- and y-axes of the tangent to the curve  taking account of sign  is equal to 1. Find a parametric solution of  as follows.   a  Write an equation for y in terms of p = dy dx and show that   b  Using p as the independent variable, arrange this as a linear ﬁrst-order  equation for x.   14.4 EXERCISES   c  Find an appropriate integrating factor to obtain  ln p − p + c  1 − p 2  ,  x =  which, together with the expression for y obtained in  a , gives a parameter- isation of the solution. −1, and   d  Reverse the roles of x and y in steps  a  to  c , putting dx dy = p  show that essentially the same parameterisation is obtained.  14.21  Using the substitutions u = x2 and v = y2, reduce the equation   cid:7    cid:8   xy  dy dx  2 −  x2 + y2 − 1   dy dx  + xy = 0  14.22  to Clairaut’s form. Hence show that the equation represents a family of conics and the four sides of a square. The action of the control mechanism on a particular system for an input f t  is  described, for t ≥ 0, by the coupled ﬁrst-order equations:  ˙y + 4z = f t ,  ˙z − 2z = ˙y + 1 2 y.  Use Laplace transforms to ﬁnd the response y t  of the system to a unit step input, f t  = H t , given that y 0  = 1 and z 0  = 0.  Questions 23 to 31 are intended to give the reader practice in choosing an approp- riate method. The level of diﬃculty varies within the set; if necessary, the hints may be consulted for an indication of the most appropriate approach.  14.24  Solve the following ﬁrst-order equations for the boundary conditions given:  14.23  Find the general solutions of the following:   a   dy dx  +  xy  a2 + x2  = x;   b   =  dy dx  − y2.  4y2 x2   cid:7  −  y x  = 1,  cid:7  − y tan x = 1,  cid:7  − y2 x2 = 1 4,  cid:7  − y2 x2 = 1 4,   a  y  b  y  c  y  d  y  y 1  = −1;  y π 4  = 3; y 1  = 1; y 1  = 1 2.  14.25  An electronic system has two inputs, to each of which a constant unit signal is applied, but starting at diﬀerent times. The equations governing the system thus take the form  ˙x + 2y = H t ,  ˙y − 2x = H t − 3 .  14.26  Initially  at t = 0 , x = 1 and y = 0; ﬁnd x t  at later times. Solve the diﬀerential equation  14.27  subject to the boundary condition y π 2  = 1.  Find the complete solution of cid:7   sin x  + 2y cos x = 1,  dy dx  2 − y  dy dx  x  +  = 0,  A x  where A is a positive constant.  dy dx   cid:8   487   FIRST-ORDER ORDINARY DIFFERENTIAL EQUATIONS  14.28  Find the solution of   5x + y − 7   dy dx  = 3 x + y + 1 .  14.29  Find the solution y = y x  of  14.30  subject to y 1  = 1. Find the solution of  14.31  if  a  y 0  = 0, and  b  y 0  = π 2. Find the family of solutions of  that satisfy y 0  = 0.  x  dy dx  + y − y2  x3 2  = 0,  = tan y,  dy dx   2 sin y − x   cid:8   cid:7   d2y dx2  +  dy dx  2  +  dy dx  = 0  14.1 14.3  14.5  14.7  14.9  14.11  14.13  14.15 14.17  14.19  14.21  14.23  14.25  14.27  14.29  14.5 Hints and answers  t   cid:7    cid:7    dt  −t.  4 tan   αf t  1   cid:7  −1et  −1 dv = x  −1[ x + y  x]  ;  a  y t  = 1 − e  N t  = N0 exp −λt .  a  exact, x2y4 + x2 + y2 = c;  b  IF = x sec2 x, y2 tan x + y = c.  a  IF =  1 − x2   −2, y =  1 − x2  k + sin  −1 2, x1 2 x + y  = c;  c  IF = −1 x ;  b  IF = cosec x, leading to −1 dx dy  − xy −2 = y, leading to −t α;  b  y t  = α −1e −t α;  c  y t  =   cid:1  y = k sin x + cos x;  c  exact equation is y x = y k + y2 2 . −t α y t  = e α −t β   α − β . It becomes case  b . −t α − e  e Note that, if the angle between the tangent and the radius vector is α, then 2 cos α = dr ds and sin α = p r. −1 dx;  Homogeneous equation, put y = vx to obtain  1 − v  v2 + 2v + 2  write 1 − v as 2 −  1 + v , and v2 + 2v + 2 as 1 +  1 + v 2; A[x2 +  x + y 2] = exp  1 + s  d¯y ds  + 2¯y = 0. C = 1; use separation of variables to show directly that y t  = te  −1 1−x+px , which has solution x =  p−1   The equation is of the form of  14.22 , set v = x + y; x + 3y + 2 ln x + y− 2  = A. The equation is isobaric with weight y = −2; setting y = vx −1 1 − v  The curve must satisfy y =  1−p leading to y =  1 ± √ straight lines joining  θ, 0  and  0, 1 − θ  for any θ. v = qu + q  q − 1 , where q = dv du. General solution y2 = cx2 + c  c − 1 , hyperbolae for c > 0 and ellipses for c < 0. Singular solution y = ± x ± 1 . −1 2;  b  separable,  a  Integrating factor is  a2 + x2 1 2, y =  a2 + x2  3 + A a2 + x2  y = x x2 + Ax + 4  Use Laplace transforms; ¯xs s2 + 4  = s + s2 − 2e x t  = 1 This is Clairaut’s equation with F p  = A p. General solution y = cx + A c; singular solution, y = 2 Either Bernoulli’s equation with n = 2 or an isobaric equation with m = 3 2; y x  = 5x3 2  2 + 3x5 2 .  2 sin 2t + cos 2t − 1 2 H t − 3  + 1 √ Ax.  x 2 or x =  1 ± √  −1 dx; 4xy 1 − x2y  = 1.  2 cos 2t − 6 H t − 3 .  −1 1 − 2v  dv = x  y 2; the singular solution p  −2, = 0 gives  −2 gives  −3s;  −1   −1.  v   cid:7   .  488   14.5 HINTS AND ANSWERS  14.31  Show that p =  Cex − 1  ln[D −  D − 1 e −x] or ln e  −1, where p = dy dx; y = ln[C − e −K + 1 − e  −x  + K.  −x   C − 1 ] or  489   Higher-order ordinary diﬀerential  15  equations  Following on from the discussion of ﬁrst-order ordinary diﬀerential equations  ODEs  given in the previous chapter, we now examine equations of second and higher order. Since a brief outline of the general properties of ODEs and their solutions was given at the beginning of the previous chapter, we will not repeat it here. Instead, we will begin with a discussion of various types of higher-order equation. This chapter is divided into three main parts. We ﬁrst discuss linear equations with constant coeﬃcients and then investigate linear equations with variable coeﬃcients. Finally, we discuss a few methods that may be of use in solving general linear or non-linear ODEs. Let us start by considering some general points relating to all linear ODEs.  Linear equations are of paramount importance in the description of physical processes. Moreover, it is an empirical fact that, when put into mathematical form, many natural processes appear as higher-order linear ODEs, most often as second-order equations. Although we could restrict our attention to these second-order equations, the generalisation to nth-order equations requires little extra work, and so we will consider this more general case.  A linear ODE of general order n has the form  an x   dny dxn  + an−1 x   dn−1y dxn−1 + ··· + a1 x   dy dx  + a0 x y = f x .   15.1   If f x  = 0 then the equation is called homogeneous; otherwise it is inhomogeneous. The ﬁrst-order linear equation studied in subsection 14.2.4 is a special case of  15.1 . As discussed at the beginning of the previous chapter, the general solution to  15.1  will contain n arbitrary constants, which may be determined if n boundary conditions are also provided.  In order to solve any equation of the form  15.1 , we must ﬁrst ﬁnd the general solution of the complementary equation, i.e. the equation formed by setting  490   HIGHER-ORDER ORDINARY DIFFERENTIAL EQUATIONS  f x  = 0:  an x   dny dxn  + an−1 x   dn−1y dxn−1 + ··· + a1 x   dy dx  + a0 x y = 0.   15.2   To determine the general solution of  15.2 , we must ﬁnd n linearly independent functions that satisfy it. Once we have found these solutions, the general solution is given by a linear superposition of these n functions. In other words, if the n solutions of  15.2  are y1 x , y2 x , . . . , yn x , then the general solution is given by the linear superposition  yc x  = c1y1 x  + c2y2 x  + ··· + cnyn x ,   15.3   where the cm are arbitrary constants that may be determined if n boundary conditions are provided. The linear combination yc x  is called the complementary function of  15.1 .  The question naturally arises how we establish that any n individual solutions to  15.2  are indeed linearly independent. For n functions to be linearly independent over an interval, there must not exist any set of constants c1, c2, . . . , cn such that  c1y1 x  + c2y2 x  + ··· + cnyn x  = 0   15.4   over the interval in question, except for the trivial case c1 = c2 = ··· = cn = 0.  A statement equivalent to  15.4 , which is perhaps more useful for the practical determination of linear independence, can be found by repeatedly diﬀerentiating   15.4 , n − 1 times in all, to obtain n simultaneous equations for c1, c2, . . . , cn:  c1y1 x  + c2y2 x  + ··· + cnyn x  = 0  cid:7    x  + ··· + cnyn   x  + c2y2   x  = 0   cid:7    cid:7   c1y1  c1y n−1    x  + c2y n−1   1  2  + ··· + cny n−1   n   x  = 0,  ...   15.5   where the primes denote diﬀerentiation with respect to x. Referring to the discussion of simultaneous linear equations given in chapter 8, if the determinant of the coeﬃcients of c1, c2, . . . , cn is non-zero then the only solution to equations  15.5  is the trivial solution c1 = c2 = ··· = cn = 0. In other words, the n functions y1 x , y2 x , . . . , yn x  are linearly independent over an interval if   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20   y1  cid:7   y1 ... y n−1   1  y2  cid:7   y2  . . .  . . .  . . . . . .  yn ... ...  y n−1   n   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20   cid:3 = 0  W  y1, y2, . . . , yn  =   15.6   over that interval; W  y1, y2, . . . , yn  is called the Wronskian of the set of functions. It should be noted, however, that the vanishing of the Wronskian does not guarantee that the functions are linearly dependent.  491   HIGHER-ORDER ORDINARY DIFFERENTIAL EQUATIONS  If the original equation  15.1  has f x  = 0  i.e. it is homogeneous  then of course the complementary function yc x  in  15.3  is already the general solution.  If, however, the equation has f x   cid:3 = 0  i.e. it is inhomogeneous  then yc x  is only  one part of the solution. The general solution of  15.1  is then given by  y x  = yc x  + yp x ,   15.7   where yp x  is the particular integral, which can be any function that satisﬁes  15.1  directly, provided it is linearly independent of yc x . It should be emphasised for practical purposes that any such function, no matter how simple  or complicated , is equally valid in forming the general solution  15.7 .  It is important to realise that the above method for ﬁnding the general solution to an ODE by superposing particular solutions assumes crucially that the ODE is linear. For non-linear equations, discussed in section 15.3, this method cannot be used, and indeed it is often impossible to ﬁnd closed-form solutions to such equations.  15.1 Linear equations with constant coeﬃcients  If the am in  15.1  are constants rather than functions of x then we have  an  dny dxn  + an−1  dn−1y dxn−1 + ··· + a1  dy dx  + a0y = f x .   15.8   Equations of this sort are very common throughout the physical sciences and engineering, and the method for their solution falls into two parts as discussed in the previous section, i.e. ﬁnding the complementary function yc x  and ﬁnding the particular integral yp x . If f x  = 0 in  15.8  then we do not have to ﬁnd a particular integral, and the complementary function is by itself the general solution.  15.1.1 Finding the complementary function yc x   The complementary function must satisfy  an  dny dxn  + an−1  dn−1y dxn−1 + ··· + a1  dy dx  + a0y = 0   15.9   and contain n arbitrary constants  see equation  15.3  . The standard method for ﬁnding yc x  is to try a solution of the form y = Aeλx, substituting this into  15.9 . After dividing the resulting equation through by Aeλx, we are left with a polynomial equation in λ of order n; this is the auxiliary equation and reads  anλn + an−1λn−1 + ··· + a1λ + a0 = 0.   15.10   492   15.1 LINEAR EQUATIONS WITH CONSTANT COEFFICIENTS  In general the auxiliary equation has n roots, say λ1, λ2, . . . , λn. In certain cases, some of these roots may be repeated and some may be complex. The three main cases are as follows.   i  All roots real and distinct. In this case the n solutions to  15.9  are exp λmx for m = 1 to n. It is easily shown by calculating the Wronskian  15.6  of these functions that if all the λm are distinct then these solutions are linearly independent. We can therefore linearly superpose them, as in  15.3 , to form the complementary function  yc x  = c1eλ1x + c2eλ2x + ··· + cneλnx.   15.11    ii  Some roots complex. For the special  but usual  case that all the coeﬃcients am in  15.9  are real, if one of the roots of the auxiliary equation  15.10  is complex, say α + iβ, then its complex conjugate α − iβ is also a root. In  this case we can write  c1e α+iβ x + c2e α−iβ x = eαx d1 cos βx + d2 sin βx   = Aeαx   βx + φ ,   15.12    cid:12    cid:15   sin cos  where A and φ are arbitrary constants.   iii  Some roots repeated. If, for example, λ1 occurs k times  k > 1  as a root of the auxiliary equation, then we have not found n linearly independent solutions of  15.9 ; formally the Wronskian  15.6  of these solutions, having two or more identical columns, is equal to zero. We must therefore ﬁnd  k− 1 further solutions that are linearly independent of those already found  and also of each other. By direct substitution into  15.9  we ﬁnd that  xeλ1x,  x2eλ1x,  . . . ,  xk−1eλ1x  are also solutions, and by calculating the Wronskian it is easily shown that they, together with the solutions already found, form a linearly independent set of n functions. Therefore the complementary function is given by  yc x  =  c1 + c2x + ··· + ckxk−1 eλ1x + ck+1eλk+1x + ck+2eλk+2x + ··· + cneλnx.   15.13   If more than one root is repeated the above argument is easily extended. For example, suppose as before that λ1 is a k-fold root of the auxiliary equation and, further, that λ2 is an l-fold root  of course, k > 1 and l > 1 . Then, from the above argument, the complementary function reads  yc x  =  c1 + c2x + ··· + ckxk−1 eλ1x  +  ck+1 + ck+2x + ··· + ck+lxl−1 eλ2x + ck+l+1eλk+l+1x + ck+l+2eλk+l+2x + ··· + cneλnx.   15.14   493   HIGHER-ORDER ORDINARY DIFFERENTIAL EQUATIONS   cid:1 Find the complementary function of the equation  d2y dx2  − 2  dy dx  + y = ex.  λ2 − 2λ + 1 = 0.  Setting the RHS to zero, substituting y = Aeλx and dividing through by Aeλx we obtain the auxiliary equation  The root λ = 1 occurs twice and so, although ex is a solution to  15.15 , we must ﬁnd a further solution to the equation that is linearly independent of ex. From the above discussion, we deduce that xex is such a solution, so that the full complementary function is given by the linear superposition  yc x  =  c1 + c2x ex.  cid:2    15.15   Solution method. Set the RHS of the ODE to zero  if it is not already so , and substitute y = Aeλx. After dividing through the resulting equation by Aeλx, obtain an nth-order polynomial equation in λ  the auxiliary equation, see  15.10  . Solve the auxiliary equation to ﬁnd the n roots, λ1, λ2, . . . , λn, say. If all these roots are real and distinct then yc x  is given by  15.11 . If, however, some of the roots are complex or repeated then yc x  is given by  15.12  or  15.13 , or the extension  15.14  of the latter, respectively.  15.1.2 Finding the particular integral yp x   There is no generally applicable method for ﬁnding the particular integral yp x  but, for linear ODEs with constant coeﬃcients and a simple RHS, yp x  can often be found by inspection or by assuming a parameterised form similar to f x . The latter method is sometimes called the method of undetermined coeﬃcients. If f x  contains only polynomial, exponential, or sine and cosine terms then, by assuming a trial function for yp x  of similar form but one which contains a number of undetermined parameters and substituting this trial function into  15.9 , the parameters can be found and yp x  deduced. Standard trial functions are as follows.   i  If f x  = aerx then try  yp x  = berx.   ii  If f x  = a1 sin rx + a2 cos rx  a1 or a2 may be zero  then try   iii  If f x  = a0 + a1x + ··· + aNxN  some am may be zero  then try  yp x  = b1 sin rx + b2 cos rx.  yp x  = b0 + b1x + ··· + bNxN.  494   15.1 LINEAR EQUATIONS WITH CONSTANT COEFFICIENTS   iv  If f x  is the sum or product of any of the above then try yp x  as the  sum or product of the corresponding individual trial functions.  It should be noted that this method fails if any term in the assumed trial function is also contained within the complementary function yc x . In such a case the trial function should be multiplied by the smallest integer power of x such that it will then contain no term that already appears in the complementary function. The undetermined coeﬃcients in the trial function can now be found by substitution into  15.8 .  Three further methods that are useful in ﬁnding the particular integral yp x  are those based on Green’s functions, the variation of parameters, and a change in the dependent variable using knowledge of the complementary function. However, since these methods are also applicable to equations with variable coeﬃcients, a discussion of them is postponed until section 15.2.   cid:1 Find a particular integral of the equation − 2  d2y dx2  dy dx  + y = ex.  From the above discussion our ﬁrst guess at a trial particular integral would be yp x  = bex. However, since the complementary function of this equation is yc x  =  c1 + c2x ex  as in the previous subsection , we see that ex is already contained in it, as indeed is xex. Multiplying our ﬁrst guess by the lowest integer power of x such that the result does not appear in yc x , we therefore try yp x  = bx2ex. Substituting this into the ODE, we ﬁnd that b = 1 2, so the particular integral is given by yp x  = x2ex 2.  cid:2   Solution method. If the RHS of an ODE contains only functions mentioned at the start of this subsection then the appropriate trial function should be substituted into it, thereby ﬁxing the undetermined parameters. If, however, the RHS of the equation is not of this form then one of the more general methods outlined in sub- sections 15.2.3–15.2.5 should be used; perhaps the most straightforward of these is the variation-of-parameters method.  15.1.3 Constructing the general solution yc x  + yp x   As stated earlier, the full solution to the ODE  15.8  is found by adding together the complementary function and any particular integral. In order to illustrate further the material discussed in the last two subsections, let us ﬁnd the general solution to a new example, starting from the beginning.  495   HIGHER-ORDER ORDINARY DIFFERENTIAL EQUATIONS   cid:1 Solve  d2y dx2  + 4y = x2 sin 2x.   15.16    15.17    15.18   First we set the RHS to zero and assume the trial solution y = Aeλx. Substituting this into  15.16  leads to the auxiliary equation  λ2 + 4 = 0 ⇒ λ = ±2i.  Therefore the complementary function is given by  yc x  = c1e2ix + c2e  −2ix = d1 cos 2x + d2 sin 2x.  We must now turn our attention to the particular integral yp x . Consulting the list of standard trial functions in the previous subsection, we ﬁnd that a ﬁrst guess at a suitable trial function for this case should be   ax2 + bx + c  sin 2x +  dx2 + ex + f  cos 2x.   15.19   However, we see that this trial function contains terms in sin 2x and cos 2x, both of which already appear in the complementary function  15.18 . We must therefore multiply  15.19  by the smallest integer power of x which ensures that none of the resulting terms appears in yc x . Since multiplying by x will suﬃce, we ﬁnally assume the trial function   ax3 + bx2 + cx  sin 2x +  dx3 + ex2 + fx  cos 2x.   15.20   Substituting this into  15.16  to ﬁx the constants appearing in  15.20 , we ﬁnd the particular integral to be  yp x  = − x3  12  x2 16  x 32  cos 2x +  sin 2x +  cos 2x.   15.21   The general solution to  15.16  then reads  y x  = yc x  + yp x   = d1 cos 2x + d2 sin 2x − x3  12  cos 2x +  sin 2x +  x2 16  cos 2x.  cid:2   x 32  15.1.4 Linear recurrence relations  Before continuing our discussion of higher-order ODEs, we take this opportunity to introduce the discrete analogues of diﬀerential equations, which are called recurrence relations  or sometimes diﬀerence equations . Whereas a diﬀerential equation gives a prescription, in terms of current values, for the new value of a dependent variable at a point only inﬁnitesimally far away, a recurrence relation describes how the next in a sequence of values un, deﬁned only at  non-negative  integer values of the ‘independent variable’ n, is to be calculated.  In its most general form a recurrence relation expresses the way in which un+1 is to be calculated from all the preceding values u0, u1, . . . , un. Just as the most general diﬀerential equations are intractable, so are the most general recurrence relations, and we will limit ourselves to analogues of the types of diﬀerential equations studied earlier in this chapter, namely those that are linear, have  496   15.1 LINEAR EQUATIONS WITH CONSTANT COEFFICIENTS  constant coeﬃcients and possess simple functions on the RHS. Such equations occur over a broad range of engineering and statistical physics as well as in the realms of ﬁnance, business planning and gambling! They form the basis of many numerical methods, particularly those concerned with the numerical solution of ordinary and partial diﬀerential equations.  A general recurrence relation is exempliﬁed by the formula  un+1 =  arun−r + k,   15.22   N−1 cid:4   r=0  where N and the ar are ﬁxed and k is a constant or a simple function of n. Such an equation, involving terms of the series whose indices diﬀer by up to N  ranging from n− N + 1 to n , is called an Nth-order recurrence relation. It is clear that, given values for u0, u1, . . . , uN−1, this is a deﬁnitive scheme for generating the series and therefore has a unique solution.  Parallelling the nomenclature of diﬀerential equations, if the term not involving any un is absent, i.e. k = 0, then the recurrence relation is called homogeneous. The parallel continues with the form of the general solution of  15.22 . If vn is the general solution of the homogeneous relation, and wn is any solution of the full relation, then  is the most general solution of the complete recurrence relation. This is straight- forwardly veriﬁed as follows:  un = vn + wn  un+1 = vn+1 + wn+1  arvn−r +  arwn−r + k  N−1 cid:4   r=0  ar vn−r + wn−r  + k  arun−r + k.  N−1 cid:4  N−1 cid:4  N−1 cid:4   r=0  r=0  r=0  =  =  =  Of course, if k = 0 then wn = 0 for all n is a trivial particular solution and the complementary solution, vn, is itself the most general solution.  First-order relations, for which N = 1, are exempliﬁed by  First-order recurrence relations  with u0 speciﬁed. The solution to the homogeneous relation is immediate,  un+1 = aun + k,   15.23   un = Can,  497   HIGHER-ORDER ORDINARY DIFFERENTIAL EQUATIONS  and, if k is a constant, the particular solution is equally straightforward: wn = K for all n, provided K is chosen to satisfy  i.e. K = k 1− a   −1. This will be suﬃcient unless a = 1, in which case un = u0 + nk  K = aK + k,  is obvious by inspection.  Thus the general solution of  15.23  is    Can + k  1 − a  a  cid:3 = 1,  u0 + nk  a = 1.  un =  If u0 is speciﬁed for the case of a  cid:3 = 1 then C must be chosen as C = u0−k  1−a ,  resulting in the equivalent form   15.24    15.25   un = u0an + k  1 − an 1 − a  .  We now illustrate this method with a worked example.  cid:1 A house-buyer borrows capital B from a bank that charges a ﬁxed annual rate of interest R%. If the loan is to be repaid over Y years, at what value should the ﬁxed annual payments P , made at the end of each year, be set? For a loan over 25 years at 6%, what percentage of the ﬁrst year’s payment goes towards paying oﬀ the capital?  Let un denote the outstanding debt at the end of year n, and write R 100 = r. Then the relevant recurrence relation is  with u0 = B. From  15.25  we have  As the loan is to be repaid over Y years, uY = 0 and thus  un+1 = un 1 + r  − P  un = B 1 + r n − P  1 −  1 + r n 1 −  1 + r   .  P =  Br 1 + r Y   1 + r Y − 1  .  The ﬁrst year’s interest is rB and so the fraction of the ﬁrst year’s payment going  towards capital repayment is  P − rB  P , which, using the above expression for P , is equal  −Y . With the given ﬁgures, this is  only  23%.  cid:2   to  1 + r   With only small modiﬁcations, the method just described can be adapted to handle recurrence relations in which the constant k in  15.23  is replaced by kαn, i.e. the relation is  un+1 = aun + kαn.   15.26   As for an inhomogeneous linear diﬀerential equation  see subsection 15.1.2 , we may try as a potential particular solution a form which resembles the term that makes the equation inhomogeneous. Here, the presence of the term kαn indicates  498   15.1 LINEAR EQUATIONS WITH CONSTANT COEFFICIENTS  that a particular solution of the form un = Aαn should be tried. Substituting this into  15.26  gives  Aαn+1 = aAαn + kαn,  from which it follows that A = k  α − a  and that there is a particular solution having the form un = kαn  α − a , provided α  cid:3 = a. For the special case α = a, the reader can readily verify that a particular solution of the form un = Anαn is appro- priate. This mirrors the corresponding situation for linear diﬀerential equations when the RHS of the diﬀerential equation is contained in the complementary function of its LHS.  In summary, the general solution to  15.26  is    C1an + kαn  α − a  C2an + knαn−1  α  cid:3 = a,  α = a,  un =  with C1 = u0 − k  α − a  and C2 = u0.  Second-order recurrence relations  We consider next recurrence relations that involve un−1 in the prescription for un+1 and treat the general case in which the intervening term, un, is also present. A typical equation is thus  As previously, the general solution of this is un = vn + wn, where vn satisﬁes  un+1 = aun + bun−1 + k.  vn+1 = avn + bvn−1   15.27    15.28    15.29   and wn is any particular solution of  15.28 ; the proof follows the same lines as that given earlier.  We have already seen for a ﬁrst-order recurrence relation that the solution to the homogeneous equation is given by terms forming a geometric series, and we consider a corresponding series of powers in the present case. Setting vn = Aλn in  15.29  for some λ, as yet undetermined, gives the requirement that λ should satisfy  Aλn+1 = aAλn + bAλn−1.  Dividing through by Aλn−1  assumed non-zero  shows that λ could be either of the roots, λ1 and λ2, of  λ2 − aλ − b = 0,   15.30   which is known as the characteristic equation of the recurrence relation.  That there are two possible series of terms of the form Aλn is consistent with the fact that two initial values  boundary conditions  have to be provided before the series can be calculated by repeated use of  15.28 . These two values are suﬃcient to determine the appropriate coeﬃcient A for each of the series. Since  15.29  is  499   HIGHER-ORDER ORDINARY DIFFERENTIAL EQUATIONS  both linear and homogeneous, and is satisﬁed by both vn = Aλn general solution is  1 and vn = Bλn  2, its  vn = Aλn  1 + Bλn 2.  If the coeﬃcients a and b are such that  15.30  has two equal roots, i.e. a2 = −4b,  then, as in the analogous case of repeated roots for diﬀerential equations  see subsection 15.1.1 iii  , the second term of the general solution is replaced by Bnλn 1 to give  vn =  A + Bn λn 1.  adequate solution is wn = k 1 − a − b   Finding a particular solution is straightforward if k is a constant: a trivial but −1 for all n. As with ﬁrst-order equations, particular solutions can be found for other simple forms of k by trying functions similar to k itself. Thus particular solutions for the cases k = Cn and k = Dαn can be found by trying wn = E + Fn and wn = Gαn respectively.   cid:1 Find the value of u16 if the series un satisﬁes for n ≥ 1, with u0 = 1 and u1 = −1.  un+1 + 4un + 3un−1 = n  We ﬁrst solve the characteristic equation,  to obtain the roots λ = −1 and λ = −3. Thus the complementary function is  λ2 + 4λ + 3 = 0,  vn = A −1 n + B −3 n.  wn = E + Fn  In view of the form of the RHS of the original relation, we try  as a particular solution and obtain  E + F n + 1  + 4 E + Fn  + 3[E + F n − 1 ] = n,  yielding F = 1 8 and E = 1 32.  Thus the complete general solution is  and now using the given values for u0 and u1 determines A as 7 8 and B as 3 32. Thus  Finally, substituting n = 16 gives u16 = 4 035 633, a value the reader may  or may not  wish to verify by repeated application of the initial recurrence relation.  cid:2   un = A −1 n + B −3 n +  n 8  +  1 32  ,  un =  1 32  [28 −1 n + 3 −3 n + 4n + 1] .  500   15.1 LINEAR EQUATIONS WITH CONSTANT COEFFICIENTS  Higher-order recurrence relations  It will be apparent that linear recurrence relations of order N > 2 do not present any additional diﬃculty in principle, though two obvious practical diﬃculties are  i  that the characteristic equation is of order N and in general will not have roots that can be written in closed form and  ii  that a correspondingly large number of given values is required to determine the N otherwise arbitrary constants in the solution. The algebraic labour needed to solve the set of simultaneous linear equations that determines them increases rapidly with N. We do not give speciﬁc examples here, but some are included in the exercises at the end of the chapter.  15.1.5 Laplace transform method  Having brieﬂy discussed recurrence relations, we now return to the main topic of this chapter, i.e. methods for obtaining solutions to higher-order ODEs. One such method is that of Laplace transforms, which is very useful for solving linear ODEs with constant coeﬃcients. Taking the Laplace transform of such an equation transforms it into a purely algebraic equation in terms of the Laplace transform of the required solution. Once the algebraic equation has been solved for this Laplace transform, the general solution to the original ODE can be obtained by performing an inverse Laplace transform. One advantage of this method is that, for given boundary conditions, it provides the solution in just one step, instead of having to ﬁnd the complementary function and particular integral separately.  In order to apply the method we need only two results from Laplace transform theory  see section 13.2 . First, the Laplace transform of a function f x  is deﬁned by   cid:21  ∞  0  ¯f s  ≡  −sxf x  dx,  e  from which we can derive the second useful relation. This concerns the Laplace transform of the nth derivative of f x :  f n  s  = sn¯f s  − sn−1f 0  − sn−2f   cid:7    0  − ··· − sf n−2  0  − f n−1  0 ,   15.31    15.32   where the primes and superscripts in parentheses denote diﬀerentiation with respect to x. Using these relations, along with table 13.1, on p. 455, which gives Laplace transforms of standard functions, we are in a position to solve a linear ODE with constant coeﬃcients by this method.  501   HIGHER-ORDER ORDINARY DIFFERENTIAL EQUATIONS   cid:1 Solve  subject to the boundary conditions y 0  = 2, y   0  = 1.  d2y dx2  − 3  dy dx  + 2y = 2e  −x,   cid:7    15.33    15.34    15.35   Taking the Laplace transform of  15.33  and using the table of standard results we obtain  s2¯y s  − sy 0  − y   cid:7    0  − 3 [s¯y s  − y 0 ] + 2¯y s  =  2  ,  s + 1  which reduces to   s2 − 3s + 2 ¯y s  − 2s + 5 =  2  .  s + 1  Solving this algebraic equation for ¯y s , the Laplace transform of the required solution to  15.33 , we obtain  2s2 − 3s − 3   s + 1  s − 1  s − 2   =  ¯y s  =  1  3 s + 1   −  +  2  s − 1  1  3 s − 2   ,  where in the ﬁnal step we have used partial fractions. Taking the inverse Laplace transform of  15.35 , again using table 13.1, we ﬁnd the speciﬁc solution to  15.33  to be  −x + 2ex − 1 y x  = 1 3 e  3 e2x.  cid:2   Note that if the boundary conditions in a problem are given as symbols, rather than just numbers, then the step involving partial fractions can often involve a considerable amount of algebra. The Laplace transform method is also very convenient for solving sets of simultaneous linear ODEs with constant coeﬃcients.  cid:1 Two electrical circuits, both of negligible resistance, each consist of a coil having self- inductance L and a capacitor having capacitance C. The mutual inductance of the two circuits is M. There is no source of e.m.f. in either circuit. Initially the second capacitor is given a charge CV0, the ﬁrst capacitor being uncharged, and at time t = 0 a switch in the second circuit is closed to complete the circuit. Find the subsequent current in the ﬁrst circuit.  Subject to the initial conditions q1 0  = ˙q1 0  = ˙q2 0  = 0 and q2 0  = CV0 = V0 G, say, we have to solve  On taking the Laplace transform of the above equations, we obtain  L¨q1 + M¨q2 + Gq1 = 0, M¨q1 + L¨q2 + Gq2 = 0.   Ls2 + G ¯q1 + Ms2¯q2 = sMV0C, Ms2¯q1 +  Ls2 + G ¯q2 = sLV0C.  Eliminating ¯q2 and rewriting as an equation for ¯q1, we ﬁnd  ¯q1 s  =  MV0s   cid:13  [ L + M s2 + G ][ L − M s2 + G ] −  L − M s  L − M s2 + G   L + M s2 + G   L + M s  V0 2G   cid:14   .  =  502   15.2 LINEAR EQUATIONS WITH VARIABLE COEFFICIENTS  Using table 13.1,  where ω2  1 L + M  = G and ω2  2 V0C cos ω1t − cos ω2t ,  q1 t  = 1 2 L − M  = G. Thus the current is given by  2 V0C ω2 sin ω2t − ω1 sin ω1t .  cid:2   i1 t  = 1  Solution method. Perform a Laplace transform, as deﬁned in  15.31 , on the entire equation, using  15.32  to calculate the transform of the derivatives. Then solve the resulting algebraic equation for ¯y s , the Laplace transform of the required solution to the ODE. By using the method of partial fractions and consulting a table of Laplace transforms of standard functions, calculate the inverse Laplace transform. The resulting function y x  is the solution of the ODE that obeys the given boundary conditions.  15.2 Linear equations with variable coeﬃcients  There is no generally applicable method of solving equations with coeﬃcients that are functions of x. Nevertheless, there are certain cases in which a solution is possible. Some of the methods discussed in this section are also useful in ﬁnding the general solution or particular integral for equations with constant coeﬃcients that have proved impenetrable by the techniques discussed above.  15.2.1 The Legendre and Euler linear equations  Legendre’s linear equation has the form  an αx + β n dny dxn  + ··· + a1 αx + β   dy dx  + a0y = f x ,   15.36   where α, β and the an are constants and may be solved by making the substitution αx + β = et. We then have  and so on for higher derivatives. Therefore we can write the terms of  15.36  as  dy dx  =  dt dx  dy dt  α   cid:7   αx + β  dy dt  =  α2   cid:8   d2y dx2 =  d dx  dy dx  =   αx + β 2  d2y dt2  − dy  dt  dy  αx + β  dx  αx + β 2 d2y  = α  dy dt  ,  dx2 = α2 d  dt  ... = αn d dt   αx + β n dny dxn   cid:8   cid:8   − 1  y,  − 1  ···   cid:7   cid:7   d dt  d dt  503   15.37    cid:8  − n + 1  y.   cid:7   d dt   HIGHER-ORDER ORDINARY DIFFERENTIAL EQUATIONS  Substituting equations  15.37  into the original equation  15.36 , the latter becomes a linear ODE with constant coeﬃcients, i.e.   cid:7    cid:8   anαn d dt  d dt  − 1  ···   cid:8  − n + 1   cid:7   d dt  y + ··· + a1α  dy dt  + a0y = f   cid:7    cid:8   ,  et − β  α  which can be solved by the methods of section 15.1.  A special case of Legendre’s linear equation, for which α = 1 and β = 0, is  Euler’s equation,  anxn dny dxn  + ··· + a1x  dy dx  + a0y = f x ;   15.38   it may be solved in a similar manner to the above by substituting x = et. If f x  = 0 in  15.38  then substituting y = xλ leads to a simple algebraic equation in λ, which can be solved to yield the solution to  15.38 . In the event that the algebraic equation for λ has repeated roots, extra care is needed. If λ1 is a k-fold root  k > 1  then the k linearly independent solutions corresponding to this root are xλ1 , xλ1 ln x, . . . , xλ1  ln x k−1.  cid:1 Solve  x2 d2y dx2 by both of the methods discussed above.  + x  dy dx  − 4y = 0   15.39    cid:7    cid:8   First we make the substitution x = et, which, after cancelling et, gives an equation with constant coeﬃcients, i.e.  d dt  − 1  d dt  y +  dy dt  − 4y = 0 ⇒ d2y  − 4y = 0.  dt2   15.40   Using the methods of section 15.1, the general solution of  15.40 , and therefore of  15.39 , is given by  y = c1e2t + c2e  −2t = c1x2 + c2x  −2.  Since the RHS of  15.39  is zero, we can reach the same solution by substituting y = xλ  into  15.39 . This gives  which reduces to  λ λ − 1 xλ + λxλ − 4xλ = 0,   λ2 − 4 xλ = 0.  y = c1x2 + c2x  −2.  cid:2   This has the solutions λ = ±2, so we obtain again the general solution  Solution method. If the ODE is of the Legendre form  15.36  then substitute αx + β = et. This results in an equation of the same order but with constant coeﬃcients, which can be solved by the methods of section 15.1. If the ODE is of the Euler form  15.38  with a non-zero RHS then substitute x = et; this again leads to an equation of the same order but with constant coeﬃcients. If, however, f x  = 0 in the Euler equation  15.38  then the equation may also be solved by substituting  504   15.2 LINEAR EQUATIONS WITH VARIABLE COEFFICIENTS  y = xλ. This leads to an algebraic equation whose solution gives the allowed values of λ; the general solution is then the linear superposition of these functions.  15.2.2 Exact equations  Sometimes an ODE may be merely the derivative of another ODE of one order lower. If this is the case then the ODE is called exact. The nth-order linear ODE  + a0 x y = f x ,   15.41   an x   dny dxn  + ··· + a1 x   cid:13   dy dx  is exact if the LHS can be written as a simple derivative, i.e. if  an x   dny dxn  + ··· + a0 x y =  d dx  bn−1 x   dn−1y dxn−1 + ··· + b0 x y  It may be shown that, for  15.42  to hold, we require  a0 x  − a 2 x  − ··· +  −1 na n   cid:7  cid:7   cid:7  1 x  + a  n  x  = 0,   cid:14   .  where the prime again denotes diﬀerentiation with respect to x. If  15.43  is satisﬁed then straightforward integration leads to a new equation of one order lower. If this simpler equation can be solved then a solution to the original equation is obtained. Of course, if the above process leads to an equation that is itself exact then the analysis can be repeated to reduce the order still further.  cid:1 Solve  Comparing with  15.41 , we have a2 = 1 − x2, a1 = −3x and a0 = −1. It is easily shown that a0 − a   cid:7  cid:7  2 = 0, so  15.44  is exact and can therefore be written in the form   cid:7  1 + a  − y = 1.  cid:14   d2y dx2  − 3x  dy dx   1 − x2   cid:13   d dx  dy dx   cid:8   b1 x   + b0 x y  = 1.   cid:7   Expanding the LHS of  15.45  we ﬁnd  d dx  b1  dy dx  + b0y  = b1  d2y dx2   cid:7  1 + b0   +  b   cid:7  0y.  + b  dy dx  Comparing  15.44  and  15.46  we ﬁnd  These relations integrate consistently to give b1 = 1 − x2 and b0 = −x, so  15.44  can be  b1 = 1 − x2,  b  1 + b0 = −3x,  cid:7   cid:14    cid:13   0 = −1.  cid:7   b  written as   15.42    15.43    15.44    15.45    15.46    15.47   Integrating  15.47  gives us directly the ﬁrst-order linear ODE  which can be solved by the method of subsection 14.2.4 and has the solution  − xy  = 1.   1 − x2   cid:9   dy dx   cid:10   d dx  −  dy dx  x  1 − x2  y =  x + c1  1 − x2 ,  −1 x + c2 √ c1 sin 1 − x2  − 1.  cid:2   y =  505   HIGHER-ORDER ORDINARY DIFFERENTIAL EQUATIONS  It is worth noting that, even if a higher-order ODE is not exact in its given form, it may sometimes be made exact by multiplying through by some suitable function, an integrating factor, cf. subsection 14.2.3. Unfortunately, no straightforward method for ﬁnding an integrating factor exists and one often has to rely on inspection or experience.   cid:1 Solve  x 1 − x2   d2y dx2  − 3x2 dy  − xy = x.  dx   15.48   It is easily shown that  15.48  is not exact, but we also see immediately that by multiplying it through by 1 x we recover  15.44 , which is exact and is solved above.  cid:2   Another important point is that an ODE need not be linear to be exact, although no simple rule such as  15.43  exists if it is not linear. Nevertheless, it is often worth exploring the possibility that a non-linear equation is exact, since it could then be reduced in order by one and may lead to a soluble equation. This is discussed further in subsection 15.3.3.  Solution method. For a linear ODE of the form  15.41  check whether it is exact using equation  15.43 . If it is not then attempt to ﬁnd an integrating factor which when multiplying the equation makes it exact. Once the equation is exact write the LHS as a derivative as in  15.42  and, by expanding this derivative and comparing with the LHS of the ODE, determine the functions bm x  in  15.42 . Integrate the resulting equation to yield another ODE, of one order lower. This may be solved or simpliﬁed further if the new ODE is itself exact or can be made so.  15.2.3 Partially known complementary function  Suppose we wish to solve the nth-order linear ODE  an x   dny dxn  + ··· + a1 x   dy dx  + a0 x y = f x ,   15.49   and we happen to know that u x  is a solution of  15.49  when the RHS is set to zero, i.e. u x  is one part of the complementary function. By making the substitution y x  = u x v x , we can transform  15.49  into an equation of order  n − 1 in dv dx. This simpler equation may prove soluble.  In particular,  if the original equation is of second order then we obtain a ﬁrst-order equation in dv dx, which may be soluble using the methods of section 14.2. In this way both the remaining term in the complementary function and the particular integral are found. This method therefore provides a useful way of calculating particular integrals for second-order equations with variable  or constant  coeﬃcients.  506   15.2 LINEAR EQUATIONS WITH VARIABLE COEFFICIENTS   cid:1 Solve  d2y dx2  + y = cosec x.   15.50   We see that the RHS does not fall into any of the categories listed in subsection 15.1.2, and so we are at an initial loss as to how to ﬁnd the particular integral. However, the complementary function of  15.50  is  and so let us choose the solution u x  = cos x  we could equally well choose sin x  and make the substitution y x  = v x u x  = v x  cos x into  15.50 . This gives  yc x  = c1 sin x + c2 cos x,  cos x  d2v dx2  − 2 sin x  dv dx  = cosec x,  which is a ﬁrst-order linear ODE in dv dx and may be solved by multiplying through by a suitable integrating factor, as discussed in subsection 14.2.4. Writing  15.51  as   15.51    15.52   we see that the required integrating factor is given by  dv dx  =  cosec x cos x  ,  d2v dx2  − 2 tan x  cid:15    cid:12    cid:21   −2  exp  tan x dx  = exp [2 ln cos x ] = cos2 x.  Multiplying both sides of  15.52  by the integrating factor cos2 x we obtain   cid:7    cid:8   d dx  cos2 x  dv dx  = cot x,  which integrates to give  After rearranging and integrating again, this becomes  dv dx  cos2 x  = ln sin x  + c1.   cid:21  = tan x ln sin x  − x + c1 tan x + c2.  sec2 x ln sin x  dx + c1   cid:21   v =  sec2 x dx  Therefore the general solution to  15.50  is given by y = uv = v cos x, i.e.  y = c1 sin x + c2 cos x + sin x ln sin x  − x cos x,  which contains the full complementary function and the particular integral.  cid:2   Solution method. If u x  is a known solution of the nth-order equation  15.49  with f x  = 0, then make the substitution y x  = u x v x  in  15.49 . This leads to an  equation of order n − 1 in dv dx, which might be soluble.  507   HIGHER-ORDER ORDINARY DIFFERENTIAL EQUATIONS  15.2.4 Variation of parameters  The method of variation of parameters proves useful in ﬁnding particular integrals for linear ODEs with variable  and constant  coeﬃcients. However, it requires knowledge of the entire complementary function, not just of one part of it as in the previous subsection.  Suppose we wish to ﬁnd a particular integral of the equation  an x   dny dxn  + ··· + a1 x   dy dx  + a0 x y = f x ,   15.53   and the complementary function yc x   the general solution of  15.53  with f x  = 0  is  yc x  = c1y1 x  + c2y2 x  + ··· + cnyn x ,  where the functions ym x  are known. We now assume that a particular integral of  15.53  can be expressed in a form similar to that of the complementary function, but with the constants cm replaced by functions of x, i.e. we assume a particular integral of the form  yp x  = k1 x y1 x  + k2 x y2 x  + ··· + kn x yn x .   15.54   This will no longer satisfy the complementary equation  i.e.  15.53  with the RHS set to zero  but might, with suitable choices of the functions ki x , be made equal to f x , thus producing not a complementary function but a particular integral. Since we have n arbitrary functions k1 x , k2 x , . . . , kn x , but only one restric- tion on them  namely the ODE , we may impose a further n − 1 constraints. We  can choose these constraints to be as convenient as possible, and the simplest choice is given by   cid:7  1 x y1 x  + k  cid:7   cid:7  1 x  + k 1 x y   cid:7  2 x y2 x  + ··· + k  cid:7   cid:7  2 x  + ··· + k 2 x y   cid:7  n x yn x  = 0  cid:7   cid:7  n x  = 0 n x y  k  k  1 x y n−2   cid:7  1 x y n−1   cid:7   1  1  k  k   x  + k   x  + k  2 x y n−2   cid:7  2 x y n−1   cid:7   2  2   x  + ··· + k  x  + ··· + k   cid:7  n x y n−2  n x y n−1   cid:7   n  n   x  = 0   x  =  f x  an x   ,  ...   15.55   where the primes denote diﬀerentiation with respect to x. The last of these  equations is not a freely chosen constraint; given the previous n − 1 constraints  and the original ODE, it must be satisﬁed.  This choice of constraints is easily justiﬁed  although the algebra is quite  messy . Diﬀerentiating  15.54  with respect to x, we obtain   cid:7  p = k1y y   cid:7  1 + k2y   cid:7  2 + ··· + kny   cid:7  n + [ k   cid:7  1y1 + k   cid:7  2y2 + ··· + k   cid:7  nyn ],  where, for the moment, we drop the explicit x-dependence of these functions. Since  508   15.2 LINEAR EQUATIONS WITH VARIABLE COEFFICIENTS  we are free to choose our constraints as we wish, let us deﬁne the expression in parentheses to be zero, giving the ﬁrst equation in  15.55 . Diﬀerentiating again we ﬁnd   cid:7  cid:7  p = k1y y   cid:7  cid:7  1 + k2y   cid:7  cid:7  2 + ··· + kny   cid:7  cid:7  n + [ k   cid:7  1y   cid:7  1 + k   cid:7   cid:7  2 + ··· + k 2y   cid:7  ny   cid:7  n ].  Once more we can choose the expression in brackets to be zero, giving the second equation in  15.55 . We can repeat this procedure, choosing the corresponding  expression in each case to be zero. This yields the ﬁrst n − 1 equations in  15.55 .  The mth derivative of yp for m < n is then given by  p = k1y m  y m   1 + k2y m   2 + ··· + kny m   n  .  Diﬀerentiating yp once more we ﬁnd that its nth derivative is given by  cid:7  ny n−1  + ··· + k  p = k1y n  y n   2y n−1   cid:7   1y n−1   cid:7   n + [ k  + k  n  2  1  ].  1 + k2y n   2 + ··· + kny n  Substituting the expressions for y m  we obtain  p , m = 0 to n, into the original ODE  15.53 ,  am[ k1y m   1 + k2y m   2 +···+ kny m   n  ]+ an[ k  1y n−1   cid:7  1 + k  2y n−1   cid:7  2 +···+ k   cid:7  ny n−1   n  ] = f x ,  am  kjy m   j + an[ k1   cid:7   y n−1   1   cid:7   y n−1   2  + ··· + kn   cid:7   y n−1   n  + k2  ] = f x .  Rearranging the order of summation on the LHS, we ﬁnd  kj[ any n   j + ··· + a1y   cid:7  j + a0yj ] + an[ k  1y n−1   cid:7   1   cid:7   y n−1   2  + ··· + k   cid:7  ny n−1   n  + k2  ] = f x .  15.56   But since the functions yj are solutions of the complementary equation of  15.53  we have  for all j   j + ··· + a1y  any n    cid:7  j + a0yj = 0.  Therefore  15.56  becomes  1y n−1   cid:7   1  an[ k   cid:7   y n−1   2  + ··· + k   cid:7  ny n−1   n  + k2  ] = f x ,  which is the ﬁnal equation given in  15.55 .   cid:7  2, . . . , k  Considering  15.55  to be a set of simultaneous equations in the set of unknowns  cid:7   cid:7  n x , we see that the determinant of the coeﬃcients of these functions 1 x , k k is equal to the Wronskian W  y1, y2, . . . , yn , which is non-zero since the solutions ym x  are linearly independent; see equation  15.6 . Therefore  15.55  can be solved  cid:7  m x , which in turn can be integrated, setting all constants of for the functions k  509  n cid:4   m=0  i.e.  n cid:4   j=1  n cid:4   n cid:4   m=0  j=1   HIGHER-ORDER ORDINARY DIFFERENTIAL EQUATIONS  integration equal to zero, to give km x . The general solution to  15.53  is then given by  y x  = yc x  + yp x  =  [cm + km x ]ym x .  n cid:4   m=1  Note that if the constants of integration are included in the km x  then, as well as ﬁnding the particular integral, we redeﬁne the arbitrary constants cm in the complementary function.  cid:1 Use the variation-of-parameters method to solve  d2y dx2  + y = cosec x,   15.57   subject to the boundary conditions y 0  = y π 2  = 0.  The complementary function of  15.57  is again  We therefore assume a particular integral of the form  yc x  = c1 sin x + c2 cos x.  and impose the additional constraints of  15.55 , i.e.  Solving these equations for k  yp x  = k1 x  sin x + k2 x  cos x,   cid:7   cid:7  2 x  cos x = 0, 1 x  sin x + k k 1 x  cos x − k  cid:7   cid:7  2 x  sin x = cosec x. k  cid:7  1 x  and k  cid:7  1 x  = cos x cosec x = cot x, k 2 x  = − sin x cosec x = −1.  cid:7  k   cid:7  2 x  gives  k1 x  = ln sin x ,  k2 x  = −x.  Hence, ignoring the constants of integration, k1 x  and k2 x  are given by  The general solution to the ODE  15.57  is therefore  y x  = [c1 + ln sin x ] sin x +  c2 − x  cos x,  which is identical to the solution found in subsection 15.2.3. Applying the boundary conditions y 0  = y π 2  = 0 we ﬁnd c1 = c2 = 0 and so  y x  = ln sin x  sin x − x cos x.  cid:2   Solution method. If the complementary function of  15.53  is known then assume a particular integral of the same form but with the constants replaced by functions of x. Impose the constraints in  15.55  and solve the resulting system of equations  cid:7  n x . Integrate these functions, setting constants of for the unknowns k integration equal to zero, to obtain k1 x , k2 x , . . . , kn x  and hence the particular integral.   cid:7  2, . . . , k   cid:7  1 x , k  510   15.2 LINEAR EQUATIONS WITH VARIABLE COEFFICIENTS  15.2.5 Green’s functions  The Green’s function method of solving linear ODEs bears a striking resemblance to the method of variation of parameters discussed in the previous subsection; it too requires knowledge of the entire complementary function in order to ﬁnd the particular integral and therefore the general solution. The Green’s function approach diﬀers, however, since once the Green’s function for a particular LHS of  15.1  and particular boundary conditions has been found, then the solution for any RHS  i.e. any f x   can be written down immediately, albeit in the form of an integral.  Although the Green’s function method can be approached by considering the superposition of eigenfunctions of the equation  see chapter 17  and is also applicable to the solution of partial diﬀerential equations  see chapter 21 , this section adopts a more utilitarian approach based on the properties of the Dirac delta function  see subsection 13.1.3  and deals only with the use of Green’s functions in solving ODEs.  Let us again consider the equation  an x   dny dxn  + ··· + a1 x   dy dx  + a0 x y = f x ,   15.58   but for the sake of brevity we now denote the LHS by Ly x , i.e. as a linear diﬀerential operator acting on y x . Thus  15.58  now reads  Ly x  = f x .  cid:21   b  a  Let us suppose that a function G x, z   the Green’s function  exists such that the general solution to  15.59 , which obeys some set of imposed boundary conditions  in the range a ≤ x ≤ b, is given by  y x  =  G x, z f z  dz,   15.60   where z is the integration variable. If we apply the linear diﬀerential operator L  to both sides of  15.60  and use  15.59  then we obtain  Comparison of  15.61  with a standard property of the Dirac delta function  see subsection 13.1.3 , namely  for a ≤ x ≤ b, shows that for  15.61  to hold for any arbitrary function f x , we require  for a ≤ x ≤ b  that  a   cid:21   b  a  Ly x  =  [LG x, z ] f z  dz = f x .  cid:21   b  δ x − z f z  dz,  f x  =  LG x, z  = δ x − z ,  511   15.59    15.61    15.62    HIGHER-ORDER ORDINARY DIFFERENTIAL EQUATIONS  obtain   cid:21   n cid:4   lim  cid:4 →0  m=0  i.e. the Green’s function G x, z  must satisfy the original ODE with the RHS set equal to a delta function. G x, z  may be thought of physically as the response of a system to a unit impulse at x = z.  In addition to  15.62 , we must impose two further sets of restrictions on G x, z . The ﬁrst is the requirement that the general solution y x  in  15.60  obeys the boundary conditions. For homogeneous boundary conditions, in which y x  and or its derivatives are required to be zero at speciﬁed points, this is most simply arranged by demanding that G x, z  itself obeys the boundary conditions when it is considered as a function of x alone; if, for example, we require y a  = y b  = 0 then we should also demand G a, z  = G b, z  = 0. Problems having inhomogeneous boundary conditions are discussed at the end of this subsection.  The second set of restrictions concerns the continuity or discontinuity of G x, z  and its derivatives at x = z and can be found by integrating  15.62  with respect  to x over the small interval [z −  cid:4 , z +  cid:4 ] and taking the limit as  cid:4  → 0. We then  z+ cid:4   z− cid:4   am x   dmG x, z   dxm  dx = lim  cid:4 →0  z+ cid:4   z− cid:4   δ x − z  dx = 1.   15.63   must have a ﬁnite discontinuity there, whereas all the lower-order derivatives,  Since dnG dxn exists at x = z but with value inﬁnity, the  n− 1 th-order derivative dmG dxm for m < n − 1, must be continuous at this point. Therefore the terms  cid:1   containing these derivatives cannot contribute to the value of the integral on the LHS of  15.63 . Noting that, apart from an arbitrary additive constant,   dmG dxm  dx = dm−1G dxm−1, and integrating the terms on the LHS of  15.63   by parts we ﬁnd  lim  cid:4 →0  z+ cid:4   z− cid:4   am x   dmG x, z   dxm  dx = 0  for m = 0 to n − 1. Thus, since only the term containing dnG dxn contributes to  the integral in  15.63 , we conclude, after performing an integration by parts, that   cid:21    cid:13    15.64    15.65   lim  cid:4 →0  an x   dn−1G x, z   dxn−1  z+ cid:4   z− cid:4   = 1.  Thus we have the further n constraints that G x, z  and its derivatives up to order  n− 2 are continuous at x = z but that dn−1G dxn−1 has a discontinuity of 1 an z   at x = z.  Thus the properties of the Green’s function G x, z  for an nth-order linear ODE  may be summarised by the following.   i  G x, z  obeys the original ODE but with f x  on the RHS set equal to a  delta function δ x − z .   cid:21    cid:14   512   15.2 LINEAR EQUATIONS WITH VARIABLE COEFFICIENTS   ii  When considered as a function of x alone G x, z  obeys the speciﬁed   homogeneous  boundary conditions on y x .   iii  The derivatives of G x, z  with respect to x up to order n−2 are continuous at x = z, but the  n − 1 th-order derivative has a discontinuity of 1 an z   at this point.   cid:1 Use Green’s functions to solve  d2y dx2  + y = cosec x,   15.66    15.67   subject to the boundary conditions y 0  = y π 2  = 0.  From  15.62  we see that the Green’s function G x, z  must satisfy  d2G x, z   dx2  + G x, z  = δ x − z .  Now it is clear that for x  cid:3 = z the RHS of  15.67  is zero, and we are left with the  task of ﬁnding the general solution to the homogeneous equation, i.e. the complementary function. The complementary function of  15.67  consists of a linear superposition of sin x and cos x and must consist of diﬀerent superpositions on either side of x = z, since its   n − 1 th derivative  i.e. the ﬁrst derivative in this case  is required to have a discontinuity  there. Therefore we assume the form of the Green’s function to be   cid:12   G x, z  =  A z  sin x + B z  cos x C z  sin x + D z  cos x  for x < z, for x > z.  Note that we have performed a similar  but not identical  operation to that used in the variation-of-parameters method, i.e. we have replaced the constants in the complementary function with functions  this time of z .  We must now impose the relevant restrictions on G x, z  in order to determine the functions A z , . . . , D z . The ﬁrst of these is that G x, z  should itself obey the homogeneous boundary conditions G 0, z  = G π 2, z  = 0. This leads to the conclusion that B z  = C z  = 0, so we now have   cid:12   G x, z  =  A z  sin x D z  cos x  for x < z, for x > z.  The second restriction is the continuity conditions given in equations  15.64 ,  15.65 , namely that, for this second-order equation, G x, z  is continuous at x = z and dG dx has a discontinuity of 1 a2 z  = 1 at this point. Applying these two constraints we have  Solving these equations for A z  and D z , we ﬁnd  Thus we have  D z  cos z − A z  sin z = 0 −D z  sin z − A z  cos z = 1.  A z  = − cos z,  D z  = − sin z.   cid:12 − cos z sin x  − sin z cos x  G x, z  =  for x < z, for x > z.  513  Therefore, from  15.60 , the general solution to  15.66  that obeys the boundary conditions    cid:21   π 2  0   cid:21    cid:21   π 2  HIGHER-ORDER ORDINARY DIFFERENTIAL EQUATIONS  y 0  = y π 2  = 0 is given by  y x  =  G x, z  cosec z dz  x  sin z cosec z dz − sin x  = − cos x = −x cos x + sin x ln sin x ,  0  π 2  x  cos z cosec z dz   cid:21   which agrees with the result obtained in the previous subsections.  cid:2   As mentioned earlier, once a Green’s function has been obtained for a given LHS and boundary conditions, it can be used to ﬁnd a general solution for any RHS; thus, the solution of d2y dx2 + y = f x , with y 0  = y π 2  = 0, is given immediately by  y x  =  G x, z f z  dz  0  = − cos x  x  sin z f z  dz − sin x   cid:21   π 2  x  cos z f z  dz.   15.68   As an example, the reader may wish to verify that if f x  = sin 2x then  15.68   gives y x  =  − sin 2x  3, a solution easily veriﬁed by direct substitution. In  general, analytic integration of  15.68  for arbitrary f x  will prove intractable; then the integrals must be evaluated numerically.  Another important point is that although the Green’s function method above has provided a general solution, it is also useful for ﬁnding a particular integral if the complementary function is known. This is easily seen since in  15.68  the constant integration limits 0 and π 2 lead merely to constant values by which the factors sin x and cos x are multiplied; thus the complementary function is reconstructed. The rest of the general solution, i.e. the particular integral, comes x, and so from the variable integration limit x. Therefore by changing dropping the constant integration limits, we can ﬁnd just the particular integral. For example, a particular integral of d2y dx2 + y = f x  that satisﬁes the above boundary conditions is given by  to − cid:1   π 2 x   cid:1   yp x  = − cos x  x  sin z f z  dz + sin x  cos z f z  dz.   cid:21   x   cid:21   0   cid:21   A very important point to realise about the Green’s function method is that a particular G x, z  applies to a given LHS of an ODE and the imposed boundary conditions, i.e. the same equation with diﬀerent boundary conditions will have a diﬀerent Green’s function. To illustrate this point, let us consider again the ODE solved in  15.68 , but with diﬀerent boundary conditions.  514   15.2 LINEAR EQUATIONS WITH VARIABLE COEFFICIENTS   cid:1 Use Green’s functions to solve  subject to the one-point boundary conditions y 0  = y   0  = 0.  d2y dx2  + y = f x ,   cid:7    15.69   We again require  15.67  to hold and so again we assume a Green’s function of the form  G x, z  =  A z  sin x + B z  cos x C z  sin x + D z  cos x  for x < z, for x > z.   cid:7  However, we now require G x, z  to obey the boundary conditions G 0, z  = G which imply A z  = B z  = 0. Therefore we have   0, z  = 0,   cid:12    cid:12   G x, z  =  0 C z  sin x + D z  cos x  for x < z, for x > z.  Applying the continuity conditions on G x, z  as before now gives  which are solved to give  So ﬁnally the Green’s function is given by  C z  sin z + D z  cos z = 0,  C z  cos z − D z  sin z = 1,  C z  = cos z,  D z  = − sin z.  G x, z  =  for x < z, for x > z,  0   cid:12  sin x − z   cid:21  ∞  cid:21   0  y x  =  G x, z f z  dz  x  sin x − z f z  dz.  cid:2   =  0  and the general solution to  15.69  that obeys the boundary conditions y 0  = y   0  = 0 is   cid:7    cid:7   Finally, we consider how to deal with inhomogeneous boundary conditions  0  = γ, where α, β, γ are non-zero. The such as y a  = α, y b  = β or y 0  = y simplest method of solution in this case is to make a change of variable such that the boundary conditions in the new variable, u say, are homogeneous, i.e. u a  =  cid:7   0  = 0 etc. For nth-order equations we generally require u b  = 0 or u 0  = u n boundary conditions to ﬁx the solution, but these n boundary conditions can be of various types: we could have the n-point boundary conditions y xm  = ym  x0  = ··· = for m = 1 to n, or the one-point boundary conditions y x0  = y y n−1  x0  = y0, or something in between. In all cases a suitable change of variable   cid:7   is  where h x  is an  n − 1 th-order polynomial that obeys the boundary conditions.  u = y − h x ,  515   HIGHER-ORDER ORDINARY DIFFERENTIAL EQUATIONS  For example, if we consider the second-order case with boundary conditions y a  = α, y b  = β then a suitable change of variable is  u = y −  mx + c ,  where y = mx + c is the straight line through the points  a, α  and  b, β , for which  m =  α − β   a − b  and c =  βa − αb   a − b . Alternatively, if the boundary   0  = γ then we would conditions for our second-order equation are y 0  = y make the same change of variable, but this time y = mx + c would be the straight line through  0, γ  with slope γ, i.e. m = c = γ.   cid:7   Solution method. Require that the Green’s function G x, z  obeys the original ODE,  but with the RHS set to a delta function δ x − z . This is equivalent to assuming  that G x, z  is given by the complementary function of the original ODE, with the constants replaced by functions of z; these functions are diﬀerent for x   z. Now require also that G x, z  obeys the given homogeneous boundary conditions and impose the continuity conditions given in  15.64  and  15.65 . The general solution to the original ODE is then given by  15.60 . For inhomogeneous boundary  conditions, make the change of dependent variable u = y − h x , where h x  is a  polynomial obeying the given boundary conditions.  15.2.6 Canonical form for second-order equations  In this section we specialise from nth-order linear ODEs with variable coeﬃcients to those of order 2. In particular we consider the equation  d2y dx2 + a1 x   dy dx  + a0 x y = f x ,   15.70   which has been rearranged so that the coeﬃcient of d2y dx2 is unity. By making the substitution y x  = u x v x  we obtain  cid:7  cid:7    cid:8    cid:8    cid:7    cid:7  cid:7   v  +   cid:7   u  + a1  v  +   cid:7  + a1u u  + a0u  v =  f u  ,   cid:7    cid:7  2u u   15.71   where the prime denotes diﬀerentiation with respect to x. Since  15.71  would be  cid:12  much simpliﬁed if there were no term in v , let us choose u x  such that the ﬁrst factor in parentheses on the LHS of  15.71  is zero, i.e. − 1  + a1 = 0 ⇒ u x  = exp  a1 z  dz   15.72    cid:15    cid:21    cid:7   .  2   cid:7  2u u  We then obtain an equation of the form  d2v dx2 + g x v = h x ,  516   15.73    15.2 LINEAR EQUATIONS WITH VARIABLE COEFFICIENTS  where   cid:12   cid:21  4 [a1 x ]2 − 1 g x  = a0 x  − 1   cid:15    cid:7  1 x  2 a  h x  = f x  exp  a1 z  dz  .  1 2  Since  15.73  is of a simpler form than the original equation,  15.70 , it may  prove easier to solve.  cid:1 Solve  Dividing  15.74  through by 4x2, we see that it is of the form  15.70  with a1 x  = 1 x, a0 x  =  x2 − 1  4x2 and f x  = 0. Therefore, making the substitution  4x2 d2y dx2  + 4x  y = vu = v exp  +  x2 − 1 y = 0.  cid:21    cid:8   dy dx   cid:7   −  1 2x  dx  =  Av√  ,  x  we obtain  Equation  15.75  is easily solved to give  so the solution of  15.74  is  d2v dx2  +  = 0.  v 4  v = c1 sin 1  2 x + c2 cos 1  2 x,  y =  v√  =  x  c1 sin 1  2 x + c2 cos 1 2 x  .  cid:2   √  x   15.74    15.75   As an alternative to choosing u x  such that the coeﬃcient of v  in  15.71  is zero, we could choose a diﬀerent u x  such that the coeﬃcient of v vanishes. For this to be the case, we see from  15.71  that we would require   cid:7    cid:7  cid:7  u   cid:7  + a1u  + a0u = 0,  so u x  would have to be a solution of the original ODE with the RHS set to zero, i.e. part of the complementary function. If such a solution were known then the substitution y = uv would yield an equation with no term in v, which could be solved by two straightforward integrations. This is a special  second-order  case of the method discussed in subsection 15.2.3.  Solution method. Write the equation in the form  15.70 , then substitute y = uv, where u x  is given by  15.72 . This leads to an equation of the form  15.73 , in which there is no term in dv dx and which may be easier to solve. Alternatively, if part of the complementary function is known then follow the method of subsec- tion 15.2.3.  517   HIGHER-ORDER ORDINARY DIFFERENTIAL EQUATIONS  15.3 General ordinary diﬀerential equations  In this section, we discuss miscellaneous methods for simplifying general ODEs. These methods are applicable to both linear and non-linear equations and in some cases may lead to a solution. More often than not, however, ﬁnding a closed-form solution to a general non-linear ODE proves impossible.  15.3.1 Dependent variable absent  If an ODE does not contain the dependent variable y explicitly, but only its derivatives, then the change of variable p = dy dx leads to an equation of one order lower.   cid:1 Solve  This is transformed by the substitution p = dy dx to the ﬁrst-order equation  The solution to  15.77  is then found by the method of subsection 14.2.4 and reads   15.76    15.77   d2y dx2  + 2  = 4x  dy dx  dp dx  + 2p = 4x.  p =  = ae  dy dx  −2x + 2x − 1,  y x  = c1e  −2x + x2 − x + c2.  cid:2   where a is a constant. Thus by direct integration the solution to the original equation,  15.76 , is  An extension to the above method is appropriate if an ODE contains only derivatives of y that are of order m and greater. Then the substitution p = dmy dxm reduces the order of the ODE by m.  Solution method. If the ODE contains only derivatives of y that are of order m and greater then the substitution p = dmy dxm reduces the order of the equation by m.  15.3.2 Independent variable absent  If an ODE does not contain the independent variable x explicitly, except in d dx, d2 dx2 etc., then as in the previous subsection we make the substitution p = dy dx  518   15.3 GENERAL ORDINARY DIFFERENTIAL EQUATIONS  but also write  d2y dx2 = d3y dx3 =  dp dx  d dx   cid:8   dp dy   cid:7   =  dy dx  dp dy  p  = p  dp dy   cid:7    cid:8   =  dy dx  d dy  p  dp dy  = p2 d2p  dy2 + p  dp dy  2  ,   15.78    cid:7    cid:8   and so on for higher-order derivatives. This leads to an equation of one order lower.  cid:1 Solve  1 + y  +  = 0.   15.79    cid:7    cid:8   2  dy dx  d2y dx2  Making the substitutions dy dx = p and d2y dx2 = p dp dy  we obtain the ﬁrst-order ODE  which is separable and may be solved as in subsection 14.2.1 to obtain  Using p = dy dx we therefore have  1 + yp  + p2 = 0,  dp dy   1 + p2 y2 = c1.   cid:25   = ±  p =  dy dx  c2 1  − y2  y2  ,   x + c2 2 + y2 = c2  1.  cid:2   which may be integrated to give the general solution of  15.79 ; after squaring this reads  Solution method. If the ODE does not contain x explicitly then substitute p = dy dx, along with the relations for higher derivatives given in  15.78 , to obtain an equation of one order lower, which may prove easier to solve.  15.3.3 Non-linear exact equations  As discussed in subsection 15.2.2, an exact ODE is one that can be obtained by straightforward diﬀerentiation of an equation of one order lower. Moreover, the notion of exact equations is useful for both linear and non-linear equations, since an exact equation can be immediately integrated. It is possible, of course, that the resulting equation may itself be exact, so that the process can be repeated. In the non-linear case, however, there is no simple relation  such as  15.43  for the linear case  by which an equation can be shown to be exact. Nevertheless, a general procedure does exist and is illustrated in the following example.  519   HIGHER-ORDER ORDINARY DIFFERENTIAL EQUATIONS   cid:1 Solve  Directing our attention to the term on the LHS of  15.80  that contains the highest-order derivative, i.e. 2y d3y dx3, we see that it can be obtained by diﬀerentiating 2y d2y dx2 since  2y  d3y dx3  + 6  dy dx  d2y dx2  = x.   cid:7    cid:8   d dx  2y  d2y dx2  = 2y  + 2  dy dx  d2y dx2 .  d3y dx3   cid:16    cid:7    cid:17    cid:8   2  .  4  dy dx  d2y dx2  =  d dx  2  dy dx   15.80    15.81    15.82   Rewriting the LHS of  15.80  using  15.81 , we are left with 4 dy dx  d2y dy2 , which may itself be written as a derivative, i.e.  Since, therefore, we can write the LHS of  15.80  as a sum of simple derivatives of other functions,  15.80  is exact. Integrating  15.80  with respect to x, and using  15.81  and  15.82 , now gives   cid:7    cid:8   2   cid:21   =  x dx =  + c1.   15.83   Now we can repeat the process to ﬁnd whether  15.83  is itself exact. Considering the term on the LHS of  15.83  that contains the highest-order derivative, i.e. 2y d2y dx2, we note that we obtain this by diﬀerentiating 2y dy dx, as follows:  2y  d2y dx2  + 2  dy dx   cid:8    cid:7   x2 2   cid:7    cid:8   d dx  2y  dy dx  = 2y  + 2  d2y dx2  2  .  dy dx  The above expression already contains all the terms on the LHS of  15.83 , so we can integrate  15.83  to give  Integrating once more we obtain the solution  2y  dy dx  x3 6  =  + c1x + c2.  y2 =  x4 24  +  c1x2  2  + c2x + c3.  cid:2   It is worth noting that both linear equations  as discussed in subsection 15.2.2  and non-linear equations may sometimes be made exact by multiplying through by an appropriate integrating factor. Although no general method exists for ﬁnding such a factor, one may sometimes be found by inspection or inspired guesswork.  Solution method. Rearrange the equation so that all the terms containing y or its derivatives are on the LHS, then check to see whether the equation is exact by attempting to write the LHS as a simple derivative. If this is possible then the equation is exact and may be integrated directly to give an equation of one order lower. If the new equation is itself exact the process can be repeated.  520   15.3 GENERAL ORDINARY DIFFERENTIAL EQUATIONS  15.3.4 Isobaric or homogeneous equations  It is straightforward to generalise the discussion of ﬁrst-order isobaric equations given in subsection 14.2.6 to equations of general order n. An nth-order isobaric equation is one in which every term can be made dimensionally consistent upon giving y and dy each a weight m, and x and dx each a weight 1. Then the nth derivative of y with respect to x, for example, would have dimensions m in y  and −n in x. In the special case m = 1, for which the equation is dimensionally  consistent, the equation is called homogeneous  not to be confused with linear equations with a zero RHS . If an equation is isobaric or homogeneous then the change in dependent variable y = vxm  y = vx in the homogeneous case  followed by the change in independent variable x = et leads to an equation in which the new independent variable t is absent except in the form d dt.  cid:1 Solve  x3 d2y dx2  −  x2 + xy   dy dx  +  y2 + xy  = 0.   15.84   Assigning y and dy the weight m, and x and dx the weight 1, the weights of the ﬁve terms on the LHS of  15.84  are, from left to right: m + 1, m + 1, 2m, 2m, m + 1. For these weights all to be equal we require m = 1; thus  15.84  is a homogeneous equation. Since it is homogeneous we now make the substitution y = vx, which, after dividing the resulting equation through by x3, gives  +  1 − v   x  d2v dx2  dv dx  = 0.  Now substituting x = et into  15.85  we obtain  after some working   d2v dt2 which can be integrated directly to give  − v  dv dt  = 0,  2 v2 + c1. Equation  15.87  is separable, and integrates to give  = 1  dv dt   cid:21    15.85    15.86    15.87   1 2 t + d2 =   cid:7   dv  v2 + d2 1 −1  tan   cid:8   cid:6   .  v d1  1 d1  =   cid:5   y = d1x tan  1 2 d1 ln x + d1d2  .  cid:2   Rearranging and using x = et and y = vx we ﬁnally obtain the solution to  15.84  as  Solution method. Assume that y and dy have weight m, and x and dx weight 1, and write down the combined weights of each term in the ODE. If these weights can be made equal by assuming a particular value for m then the equation is isobaric  or homogeneous if m = 1 . Making the substitution y = vxm followed by x = et leads to an equation in which the new independent variable t is absent except in the form d dt.  521   HIGHER-ORDER ORDINARY DIFFERENTIAL EQUATIONS  15.3.5 Equations homogeneous in x or y alone  It will be seen that the intermediate equation  15.85  in the example of the previous subsection was simpliﬁed by the substitution x = et, in that this led to an equation in which the new independent variable t occurred only in the form d dt; see  15.86 . A closer examination of  15.85  reveals that it is dimensionally consistent in the independent variable x taken alone; this is equivalent to giving the dependent variable and its diﬀerential a weight m = 0. For any equation that is homogeneous in x alone, the substitution x = et will lead to an equation that does not contain the new independent variable t except as d dt. Note that the Euler equation of subsection 15.2.1 is a special, linear example of an equation homogeneous in x alone. Similarly, if an equation is homogeneous in y alone, then substituting y = ev leads to an equation in which the new dependent variable, v, occurs only in the form d dv.  cid:1 Solve  This equation is homogeneous in x alone, and on substituting x = et we obtain  which does not contain the new independent variable t except as d dt. Such equations may often be solved by the method of subsection 15.3.2, but in this case we can integrate directly to obtain  This equation is separable, and we ﬁnd  By multiplying the numerator and denominator of the integrand on the LHS by y, we ﬁnd the solution  x2 d2y dx2  + x  +  = 0.  dy dx  2 y3  d2y dt2  +  = 0,  2 y3   cid:24   dy   cid:21   dy dt   cid:24   cid:24    cid:24   =  2 c1 + 1 y2 .  2 c1 + 1 y2   = t + c2.  √ c1y2 + 1 2c1  = t + c2.  √ c1y2 + 1 2c1  = ln x + c2.  cid:2   Remembering that t = ln x, we ﬁnally obtain  Solution method. If the weight of x taken alone is the same in every term in the ODE then the substitution x = et leads to an equation in which the new independent variable t is absent except in the form d dt. If the weight of y taken alone is the same in every term then the substitution y = ev leads to an equation in which the new dependent variable v is absent except in the form d dv.  522   15.4 EXERCISES  15.3.6 Equations having y = Aex as a solution  Finally, we note that if any general  linear or non-linear  nth-order ODE is satisﬁed identically by assuming that  y =  dy dx  = ··· =  dny dxn   15.88   then y = Aex is a solution of that equation. This must be so because y = Aex is a non-zero function that satisﬁes  15.88 .  cid:1 Find a solution of   cid:7    cid:8   2   x2 + x   dy dx  d2y dx2  − x2y  − x  dy dx  dy dx  = 0.   15.89   Setting y = dy dx = d2y dx2 in  15.89 , we obtain   x2 + x y2 − x2y2 − xy2 = 0,  which is satisﬁed identically. Therefore y = Aex is a solution of  15.89 ; this is easily veriﬁed by directly substituting y = Aex into  15.89 .  cid:2   Solution method. If the equation is satisﬁed identically by making the substitutions  y = dy dx = ··· = dny dxn then y = Aex is a solution.  15.1  A simple harmonic oscillator, of mass m and natural frequency ω0, experiences an oscillating driving force f t  = ma cos ωt. Therefore, its equation of motion is  15.4 Exercises  d2x dt2  + ω2  0 x = a cos ωt,  15.2  15.3  where x is its position. Given that at t = 0 we have x = dx dt = 0, ﬁnd the function x t . Describe the solution if ω is approximately, but not exactly, equal to ω0. Find the roots of the auxiliary equation for the following. Hence solve them for the boundary conditions stated.   a    b   d2f dt2 d2f dt2  df dt df dt  + 2  + 5f = 0,  with f 0  = 1, f   0  = 0.  + 2  + 5f = e  with f 0  = 0, f   0  = 0.  −t cos 3t,   cid:7    cid:7   The theory of bent beams shows that at any point in the beam the ‘bending moment’ is given by K ρ, where K is a constant  that depends upon the beam material and cross-sectional shape  and ρ is the radius of curvature at that point. Consider a light beam of length L whose ends, x = 0 and x = L, are supported at the same vertical height and which has a weight W suspended from its centre.  Verify that at any point x  0 ≤ x ≤ L 2 for deﬁniteness  the net magnitude of the bending moment  bending moment = force × perpendicular distance  due to  the weight and support reactions, evaluated on either side of x, is Wx 2.  523   HIGHER-ORDER ORDINARY DIFFERENTIAL EQUATIONS  If the beam is only slightly bent, so that  dy dx 2  cid:16  1, where y = y x  is the  downward displacement of the beam at x, show that the beam proﬁle satisﬁes the approximate equation  d2y dx2  = − Wx  .  2K  d2f dt2  df dt  + 6  + 9f = e  −t,  By integrating this equation twice and using physically imposed conditions on your solution at x = 0 and x = L 2, show that the downward displacement at the centre of the beam is W L3  48K . Solve the diﬀerential equation  15.4  subject to the conditions f = 0 and df dt = λ at t = 0.  Find the equation satisﬁed by the positions of the turning points of f t  and hence, by drawing suitable sketch graphs, determine the number of turning points  the solution has in the range t > 0 if  a  λ = 1 4, and  b  λ = −1 4.  15.5  The function f t  satisﬁes the diﬀerential equation  d2f dt2  df dt  + 8  + 12f = 12e  −4t.  For the following sets of boundary conditions determine whether it has solutions, and, if so, ﬁnd them:   a  f 0  = 0,  b  f 0  = 0,  f f   0  = 0,   0  = −2,  f ln  f ln   cid:7   cid:7   √ √ 2  = 0; 2  = 0.  15.6  Determine the values of α and β for which the following four functions are linearly dependent:  You will ﬁnd it convenient to work with those linear combinations of the yi x  that can be written the most compactly. A solution of the diﬀerential equation  takes the value 1 when x = 0 and the value e when x = 2? The two functions x t  and y t  satisfy the simultaneous equations  −1 when x = 1. What is its value  15.7  15.8  Find explicit expressions for x t  and y t , given that x 0  = 3 and y 0  = 2.  Sketch the solution trajectory in the xy-plane for 0 ≤ t < 2π, showing that the trajectory crosses itself at  0, 1 2  and passes through the points  0,−3  and  0,−1  in the negative x-direction.  y1 x  = x cosh x + sinh x, y2 x  = x sinh x + cosh x, y3 x  =  x + α ex, −x. y4 x  =  x + β e  d2y dx2  dy dx  + 2  + y = 4e  −x  − 2y = − sin t,  + 2x = 5 cos t.  dx dt dy dt  524   15.4 EXERCISES  15.9  Find the general solutions of   cid:7  − 12   cid:8   dy dx   cid:7  + 16y = 32x − 8,   cid:8   1 y  dy dx  +  2a coth 2ax   = 2a2,  1 y  dy dx  15.10  where a is a constant. Use the method of Laplace transforms to solve  + 5  + 6f = 0,  f 0  = 1, f  + 2  + 5f = 0,  f 0  = 1, f   0  = 0.   0  = −4,   cid:7    cid:7   df dt df dt   a    b    a    b   d3y dx3 d dx  d2f dt2 d2f dt2  15.11  The quantities x t , y t  satisfy the simultaneous equations  where x 0  = y 0  = ˙y 0  = 0 and ˙x 0  = λ. Show that  15.12  Use Laplace transforms to solve, for t ≥ 0, the diﬀerential equations  ¨x + 2n˙x + n2x = 0, ¨y + 2n˙y + n2y = µ˙x,   cid:5    cid:6   y t  = 1  2 µλt2  1 − 1 3 nt  exp −nt .  ¨x + 2x + y = cos t, ¨y + 2x + 3y = 2 cos t,  which describe a coupled system that starts from rest at the equilibrium position. Show that the subsequent motion takes place along a straight line in the xy-plane. Verify that the frequency at which the system is driven is equal to one of the resonance frequencies of the system; explain why there is no resonant behaviour in the solution you have obtained. Two unstable isotopes A and B and a stable isotope C have the following decay −1. Initially a quantity x0 of A is present, but there are no atoms of the other two types. Using Laplace transforms, ﬁnd the amount of C present at a later time t. For a lightly damped  γ < ω0  harmonic oscillator driven at its undamped resonance frequency ω0, the displacement x t  at time t satisﬁes the equation  rates per atom present: A → B, 3 s  −1; B → C, 2 s  −1; A → C, 1 s  15.13  15.14  d2x dt2  dx dt  + 2γ  + ω2  0 x = F sin ω0t.  Use Laplace transforms to ﬁnd the displacement at a general time if the oscillator starts from rest at its equilibrium position.   a  Show that ultimately the oscillation has amplitude F  2ω0γ , with a phase  lag of π 2 relative to the driving force per unit mass F.   b  By diﬀerentiating the original equation, conclude that if x t  is expanded as a power series in t for small t, then the ﬁrst non-vanishing term is Fω0t3 6. Conﬁrm this conclusion by expanding your explicit solution.  15.15  The ‘golden mean’, which is said to describe the most aesthetically pleasing proportions for the sides of a rectangle  e.g. the ideal picture frame , is given by the limiting value of the ratio of successive terms of the Fibonacci series un, which is generated by  with u0 = 0 and u1 = 1. Find an expression for the general term of the series and  un+2 = un+1 + un,  525   HIGHER-ORDER ORDINARY DIFFERENTIAL EQUATIONS  15.16  verify that the golden mean is equal to the larger root of the recurrence relation’s characteristic equation. In a particular scheme for numerically modelling one-dimensional ﬂuid ﬂow, the  successive values, un, of the solution are connected for n ≥ 1 by the diﬀerence  equation  c un+1 − un−1  = d un+1 − 2un + un−1 ,  where c and d are positive constants. The boundary conditions are u0 = 0 and uM = 1. Find the solution to the equation, and show that successive values of un will have alternating signs if c > d.  The ﬁrst few terms of a series un, starting with u0, are 1, 2, 2, 1, 6,−3. The series  15.17  is generated by a recurrence relation of the form un = P un−2 + Qun−4,  where P and Q are constants. Find an expression for the general term of the series and show that, in fact, the series consists of two interleaved series given by  u2m = 2 u2m+1 = 7 3  3 + 1 − 1  3 4m, 3 4m,  15.18  for m = 0, 1, 2, . . . . Find an explicit expression for the un satisfying  given that u0 = u1 = 1. Deduce that 2n − 26 −3 n is divisible by 5 for all  un+1 + 5un + 6un−1 = 2n,  15.19  non-negative integers n. Find the general expression for the un satisfying  with u0 = u1 = 0 and u2 = 1, and show that they can be written in the form  un+1 = 2un−2 − un   cid:7    cid:8   un =  − 2n 2√  5  1 5  cos  − φ  ,  3πn  4  15.20  where tan φ = 2. Consider the seventh-order recurrence relation  un+7 − un+6 − un+5 + un+4 − un+3 + un+2 + un+1 − un = 0.  Find the most general form of its solution, and show that:   a   if only the four initial values u0 = 0, u1 = 2, u2 = 6 and u3 = 12, are speciﬁed, then the relation has one solution that cycles repeatedly through this set of four numbers;   b  but if, in addition, it is required that u4 = 20, u5 = 30 and u6 = 42 then the  solution is unique, with un = n n + 1 .  15.21  Find the general solution of  x2 d2y dx2 given that y 1  = 1 and y e  = 2e. Find the general solution of  15.22  − x  dy dx  + y = x,   x + 1 2 d2y dx2  + 3 x + 1   + y = x2.  dy dx  526   15.4 EXERCISES  15.23  Prove that the general solution of  is given by   x − 2   d2y dx2  + 3  +  = 0  dy dx   cid:7    cid:13   4y x2   cid:8    cid:14   y x  =  1   x − 2 2  k  − 1 2  2 3x  + cx2  .  15.24  Use the method of variation of parameters to ﬁnd the general solutions of   a   d2y dx2  − y = xn,   b   d2y dx2  − 2  dy dx  + y = 2xex.  15.25  Use the intermediate result of exercise 15.24 a  to ﬁnd the Green’s function that satisﬁes  d2G x, ξ   dx2  − G x, ξ  = δ x − ξ   with  G 0, ξ  = G 1, ξ  = 0.  15.26  Consider the equation  F x, y  = x x + 1   d2y dx2  +  2 − x2   −  2 + x y = 0.  dy dx   a  Given that y1 x  = 1 x is one of its solutions, ﬁnd a second linearly inde-   i  by setting y2 x  = y1 x u x , and  ii  by noting the sum of the coeﬃcients in the equation.   b  Hence, using the variation of parameters method, ﬁnd the general solution  pendent one,  of  15.27  Show generally that if y1 x  and y2 x  are linearly independent solutions of  F x, y  =  x + 1 2.  + p x   + q x y = 0,  dy dx  d2y dx2    G x, ξ  =  y1 x y2 ξ  W  ξ  0 < x < ξ,  y2 x y1 ξ  W  ξ  ξ < x < 1,  with y1 0  = 0 and y2 1  = 0, then the Green’s function G x, ξ  for the interval  0 ≤ x, ξ ≤ 1 and with G 0, ξ  = G 1, ξ  = 0 can be written in the form  15.28  where W  x  = W [y1 x , y2 x ] is the Wronskian of y1 x  and y2 x . Use the result of the previous exercise to ﬁnd the Green’s function G x, ξ  that satisﬁes  in the interval 0 ≤ x, ξ ≤ 1, with G 0, ξ  = G 1, ξ  = 0. Hence obtain integral  expressions for the solution of  d2G dx2  + 3  dG dx  + 2G = δ x − x ,   d2y dx2  dy dx  + 3  + 2y =  0 0 < x < x0, 1 x0 < x < 1,  distinguishing between the cases  a  x   x0.  527   HIGHER-ORDER ORDINARY DIFFERENTIAL EQUATIONS  15.29  The equation of motion for a driven damped harmonic oscillator can be written  ¨x + 2˙x +  1 + κ2 x = f t ,  with κ  cid:3 = 0. If it starts from rest with x 0  = 0 and ˙x 0  = 0, ﬁnd the corresponding Green’s function G t, τ  and verify that it can be written as a function of t − τ  only. Find the explicit solution when the driving force is the unit step function, i.e. f t  = H t . Conﬁrm your solution by taking the Laplace transforms of both it and the original equation. Show that the Green’s function for the equation  15.30  subject to the boundary conditions y 0  = y π  = 0, is given by  y 4  d2y dx2  +  −2 cos 1  −2 sin 1  = f x ,  2 x sin 1 2 z 2 x cos 1 2 z  0 ≤ z ≤ x, x ≤ z ≤ π.  G x, z  =  15.31  Find the Green’s function x = G t, t0  that solves  under the initial conditions x = dx dt = 0 at t = 0. Hence solve  where f t  = 0 for t < 0. Evaluate your answer explicitly for f t  = Ae Consider the equation  −at  t > 0 .  15.32  d2x dt2  + α  dx dt  = δ t − t0   d2x dt2  dx dt  + α  = f t ,  d2y dx2  + f y  = 0,  where f y  can be any function.   a  By multiplying through by dy dx, obtain the general solution relating x and  y.   b  A mass m, initially at rest at the point x = 0, is accelerated by a force   cid:13    cid:7    cid:8  cid:14   .  f x  = A x0 − x   1 + 2 ln  1 − x  x0  15.33  Solve  Its equation of motion is m d2x dt2 = f x . Find x as a function of time, and show that ultimately the particle has travelled a distance x0.   cid:7    cid:8    cid:7    cid:8   2  2y  d3y dx3  + 2  y + 3  dy dx  d2y dx2  + 2  dy dx  = sin x.  15.34  Find the general solution of the equation  x  d3y dx3  d2y dx2  + 2  = Ax.  15.35  Express the equation  + 4x  +  4x2 + 6 y = e  sin 2x  −x2  d2y dx2  dy dx  in canonical form and hence ﬁnd its general solution.  528   15.36  Find the form of the solutions of the equation  that have y 0  = ∞.  [ You will need the result  15.37  Consider the equation  z cosech u du = − ln cosech z + coth z . ]   cid:7  cid:7   xpy  +  xp−1y   cid:7   +  2  xp−2y = yn,  in which p  cid:3 = 2 and n > −1 but n  cid:3 = 1. For the boundary conditions y 1  = 0 and  cid:7   1  = λ, show that the solution is y x  = v x x p−2   n−1 , where v x  is given by  y  15.5 HINTS AND ANSWERS   cid:7    cid:8 2   cid:7   d2y dx2  dy dx  = 0   cid:8   2   cid:8   +   cid:7   p − 2 n − 1  cid:19   1 2  − 2  dy dx  d3y dx3   cid:1  n + 3 − 2p  cid:21   n − 1  cid:18   v x   dz  = ln x.  0  λ2 + 2zn+1  n + 1   15.1  15.3  15.5  15.7  15.9  15.5 Hints and answers  −1.  − ω2   −1 cos ωt− cos ω0t ; for moderate t, x t  is a sine wave − ω2   The function is a ω2 0 of linearly increasing amplitude  t sin ω0t   2ω0 ; for large t it shows beats of maximum amplitude 2 ω2  cid:7 2, compared with 1, in the expression for ρ. y = 0 at x = 0. 0 Ignore the term y From symmetry, dy dx = 0 at x = L 2. −6t + Be General solution f t  = Ae −6t + e boundary conditions;  b  f t  = 2e The auxiliary equation has repeated roots and the RHS is contained in the −2.  cid:1   a  The auxiliary equation has roots 2, 2, −4;  A+Bx  exp 2x+C exp −4x +2x+1; complementary function. The solution is y x  =  A+Bx e   b  multiply through by sinh 2ax and note that  −4t.  a  No solution,  −x. y 2  = 5e  −2t − 3e  −2t − 3e  −1 ln  tanh ax ; y = B sinh 2ax 1 2  tanh ax A.  cosech 2ax dx =  2a   −x +2x2e  inconsistent  −4t.  −4 as  s + n   −3 − n s + n   −4.  5 .  √  √  √  15.21  15.15  un = [ 1 +  15.17 15.19  2 exp −2t ].  Use Laplace transforms; write s s + n   2 exp −4t  − 3 5 n −  1 − √  15.11 15.13 L [C t ] = x0 s + 8  [s s + 2  s + 4 ], yielding C t  = x0[1 + 1 The characteristic equation is λ2 − λ − 1 = 0. From u4 and u5, P = 5, Q = −4. un = 3 2 − 5 −1 n 6 +  −2 n 4 + 2n 12. The general solution is A+B2n 2 exp i3πn 4 +C2n 2 exp i5πn 4 . The initial values This is Euler’s equation; setting x = exp t produces d2z dt2 − 2 dz dt + z = exp t, imply that A = 1 5, B =   with complementary function  A + Bt  exp t and particular integral t2 exp t  2; y x  = x + [x ln x 1 + ln x ] 2. After multiplication through by x2 the coeﬃcients are such that this is an integrating factor  x − 2 2 x2. exact equation. The resulting ﬁrst-order equation, in standard form, needs an Given the boundary conditions, it is better to work with sinh x and sinh 1 − x  ±x; G x, ξ  = −[sinh 1 − ξ  sinh x]  sinh 1 for x < ξ and −[sinh 1 −  5 n]  2n √ 5 10  exp[i π − φ ] and C =    than with e x  sinh ξ]  sinh 1 for x > ξ. Follow the method of subsection 15.2.5, but using general rather than speciﬁc functions. −1 1 − e G t, τ  = 0 for t < τ and κ x t  =  1 + κ2  s[ s + 1 2 + κ2 ]¯x = 1.  − t−τ  sin[κ t − τ ] for t > τ. For a unit step input, −t sin κt . Both transforms are equivalent to  −t cos κt − κ  5 10  exp[i π + φ ].  −1e  −1e  15.25  15.29  15.23  15.27  529   HIGHER-ORDER ORDINARY DIFFERENTIAL EQUATIONS  15.31  15.33  15.35  15.37  Use continuity and the step condition on ∂G ∂t at t = t0 to show that G t, t0  = α  −1{1 − exp[α t0 − t ]} for 0 ≤ t0 ≤ t;  x t  = A α − a   −1{a  −1[1 − exp −at ] − α  −1[1 − exp −αt ]}.  Follow the method of subsection 15.2.6; u x  = e  The LHS of the equation is exact for two stages of integration and then needs an integrating factor exp x; 2y d2y dx2 + 2y dy dx + 2 dy dx 2; 2y dy dx + y2 = d y2  dx + y2; y2 = A exp −x  + Bx + C −  sin x − cos x  2. sin 2x, for which a particular integral is  −x cos 2x  4. The general solution is y x  = [A sin 2x +  B − 1 The equation is isobaric, with y of weight m, where m + p − 2 = mn; v x  satisﬁes x2v  cid:7  u 0  = 0, u  = vn. Set x = et and v x  = u t , leading to u  −x2 and v x  satisﬁes v   0  = λ. Multiply both sides by u  to make the equation exact.  4 x  cos 2x]e  = un with  + 4v =  + xv  −x2   cid:7  cid:7    cid:7  cid:7    cid:7  cid:7    cid:7    cid:7   .  530   16  Series solutions of ordinary  diﬀerential equations  In the previous chapter the solution of both homogeneous and non-homogeneous  linear ordinary diﬀerential equations  ODEs  of order ≥ 2 was discussed. In par-  ticular we developed methods for solving some equations in which the coeﬃcients were not constant but functions of the independent variable x. In each case we were able to write the solutions to such equations in terms of elementary func- tions, or as integrals. In general, however, the solutions of equations with variable coeﬃcients cannot be written in this way, and we must consider alternative approaches.  In this chapter we discuss a method for obtaining solutions to linear ODEs in the form of convergent series. Such series can be evaluated numerically, and those occurring most commonly are named and tabulated. There is in fact no distinct borderline between this and the previous chapter, since solutions in terms of elementary functions may equally well be written as convergent series  i.e. the relevant Taylor series . Indeed, it is partly because some series occur so frequently that they are given special names such as sin x, cos x or exp x.  Since we shall be concerned principally with second-order linear ODEs in this chapter, we begin with a discussion of these equations, and obtain some general results that will prove useful when we come to discuss series solutions.  16.1 Second-order linear ordinary diﬀerential equations  Any homogeneous second-order linear ODE can be written in the form   cid:7  cid:7    cid:7   y  + p x y  + q x y = 0,   16.1    cid:7   where y chapter, we recall that the most general form of the solution to  16.1  is  = dy dx and p x  and q x  are given functions of x. From the previous  y x  = c1y1 x  + c2y2 x ,   16.2   531   SERIES SOLUTIONS OF ORDINARY DIFFERENTIAL EQUATIONS  where y1 x  and y2 x  are linearly independent solutions of  16.1 , and c1 and c2 are constants that are ﬁxed by the boundary conditions  if supplied .  A full discussion of the linear independence of sets of functions was given at the beginning of the previous chapter, but for just two functions y1 and y2 to be linearly independent we simply require that y2 is not a multiple of y1. Equivalently, y1 and y2 must be such that the equation  c1y1 x  + c2y2 x  = 0  is only satisﬁed for c1 = c2 = 0. Therefore the linear independence of y1 x  and y2 x  can usually be deduced by inspection but in any case can always be veriﬁed by the evaluation of the Wronskian of the two solutions,  cid:7  1.   cid:20  cid:20  cid:20  cid:20  = y1y   cid:20  cid:20  cid:20  cid:20  y1  − y2y  W  x  =   16.3    cid:7  2  If W  x   cid:3 = 0 anywhere in a given interval then y1 and y2 are linearly independent   cid:7  y 1  y2  cid:7  y 2  in that interval.  An alternative expression for W  x , of which we will make use later, may be  derived by diﬀerentiating  16.3  with respect to x to give  cid:7  cid:7   cid:7  1 = y1y 2  − y2y   cid:7  cid:7  2 + y  − y   cid:7   cid:7  1y 2  = y1y   cid:7  2y  W   cid:7  cid:7  1   cid:7   Since both y1 and y2 satisfy  16.1 , we may substitute for y   cid:7   = −y1 py   cid:7  2 + qy2  +  py  W   cid:7  cid:7  1 y2.  − y  cid:7  cid:7  1 and y − y 1y2  = −pW .  cid:7    cid:7  cid:7  2 to obtain  Integrating, we ﬁnd  1 + qy1 y2 = −p y1y  cid:7   cid:15   cid:12    cid:21    cid:7  2  −  x  W  x  = C exp  p u  du  ,   16.4   where C is a constant. We note further that in the special case p x  ≡ 0 we obtain  W = constant.  cid:1 The functions y1 = sin x and y2 = cos x are both solutions of the equation y + y = 0. Evaluate the Wronskian of these two solutions, and hence show that they are linearly independent.   cid:7  cid:7   The Wronskian of y1 and y2 is given by 1 = − sin2 x − cos2 x = −1.  cid:7   − y2y  W = y1y   cid:7  2  Since W  cid:3 = 0 the two solutions are linearly independent. We also note that y + y = 0 is a special case of  16.1  with p x  = 0. We therefore expect, from  16.4 , that W will be a constant, as is indeed the case.  cid:2    cid:7  cid:7   From the previous chapter we recall that, once we have obtained the general solution to the homogeneous second-order ODE  16.1  in the form  16.2 , the general solution to the inhomogeneous equation   cid:7  cid:7    cid:7   y  + p x y  + q x y = f x    16.5   532   16.1 SECOND-ORDER LINEAR ORDINARY DIFFERENTIAL EQUATIONS  can be written as the sum of the solution to the homogeneous equation yc x   the complementary function  and any function yp x   the particular integral  that satisﬁes  16.5  and is linearly independent of yc x . We have therefore  y x  = c1y1 x  + c2y2 x  + yp x .   16.6   General methods for obtaining yp, that are applicable to equations with variable coeﬃcients, such as the variation of parameters or Green’s functions, were dis- cussed in the previous chapter. An alternative description of the Green’s function method for solving inhomogeneous equations is given in the next chapter. For the present, however, we will restrict our attention to the solutions of homogeneous ODEs in the form of convergent series.  16.1.1 Ordinary and singular points of an ODE  So far we have implicitly assumed that y x  is a real function of a real variable x. However, this is not always the case, and in the remainder of this chapter we broaden our discussion by generalising to a complex function y z  of a complex variable z.  Let us therefore consider the second-order linear homogeneous ODE   cid:7  cid:7    cid:7   y  + p z y  + q z  = 0,   16.7   where now y = dy dz; this is a straightforward generalisation of  16.1 . A full discussion of complex functions and diﬀerentiation with respect to a complex variable z is given in chapter 24, but for the purposes of the present chapter we need not concern ourselves with many of the subtleties that exist. In particular, we may treat diﬀerentiation with respect to z in a way analogous to ordinary diﬀerentiation with respect to a real variable x.  In  16.7 , if, at some point z = z0, the functions p z  and q z  are ﬁnite and can  be expressed as complex power series  see section 4.5 , i.e.   cid:7   p z  =  pn z − z0 n,  q z  =  qn z − z0 n,  ∞ cid:4   n=0  ∞ cid:4   n=0  then p z  and q z  are said to be analytic at z = z0, and this point is called an ordinary point of the ODE. If, however, p z  or q z , or both, diverge at z = z0 then it is called a singular point of the ODE.  Even if an ODE is singular at a given point z = z0, it may still possess a non-singular  ﬁnite  solution at that point. In fact the necessary and suﬃcient condition both analytic at z = z0. Singular points that have this property are called regular  for such a solution to exist is that  z − z0 p z  and  z − z0 2q z  are  §  §  See, for example, H. Jeﬀreys and B. S. Jeﬀreys, Methods of Mathematical Physics, 3rd edn  Cam- bridge: Cambridge University Press, 1966 , p. 479.  533   SERIES SOLUTIONS OF ORDINARY DIFFERENTIAL EQUATIONS  singular points, whereas any singular point not satisfying both these criteria is termed an irregular or essential singularity.  cid:1 Legendre’s equation has the form  1 − z2 y  where  cid:2  is a constant. Show that z = 0 is an ordinary point and z = ±1 are regular singular  +  cid:2   cid:2  + 1 y = 0,   cid:7  cid:7  − 2zy   16.8    cid:7   points of this equation.  Firstly, divide through by 1 − z2 to put the equation into our standard form  16.7 :   cid:7  cid:7  − 2z  1 − z2 y   cid:7   y  +   cid:2   cid:2  + 1   1 − z2 y = 0.  Comparing this with  16.7 , we identify p z  and q z  as  −2z 1 − z2  −2z  =   1 + z  1 − z   ,  p z  =  q z  =   cid:2   cid:2  + 1   1 − z2  =   cid:2   cid:2  + 1    1 + z  1 − z   .  By inspection, p z  and q z  are analytic at z = 0, which is therefore an ordinary point,  but both diverge for z = ±1, which are thus singular points. However, at z = 1 we see that both  z − 1 p z  and  z − 1 2q z  are analytic and hence z = 1 is a regular singular point. Similarly, at z = −1 both  z + 1 p z  and  z + 1 2q z  are analytic, and it too is a regular singular point.  cid:2  determine the nature of the point z → ∞. This may be achieved straightforwardly  So far we have assumed that z0 is ﬁnite. However, we may sometimes wish to  by substituting w = 1 z into the equation and investigating the behaviour at w = 0.  cid:1 Show that Legendre’s equation has a regular singularity at z → ∞.  cid:7   Letting w = 1 z, the derivatives with respect to z become   cid:8    cid:8   dy dw  dw  z2  =  ,  − w2 d2y  dw2  = w3  2  dy dw  + w  d2y dw2  .  dy dz d2y dz2  =   cid:7   cid:8  = − 1  cid:8   dy dz  w3  dw dz d dw  dy dw dw dz   cid:7   1 − 1  w2   cid:7  = −w2 dy −2w  = −w2  cid:7   dy dw   cid:8   2  dy dw  + w  d2y dw2  + 2  1 w  w2 dy dw  +  cid:2   cid:2  + 1 y = 0,  If we substitute these derivatives into Legendre’s equation  16.8  we obtain  which simpliﬁes to give  w2 w2 − 1   d2y dw2  + 2w3 dy dw  +  cid:2   cid:2  + 1 y = 0.  Dividing through by w2 w2 − 1  to put the equation into standard form, and comparing  with  16.7 , we identify p w  and q w  as  p w  =  2w  w2 − 1  ,  q w  =   cid:2   cid:2  + 1   w2 w2 − 1   .  At w = 0, p w  is analytic but q w  diverges, and so the point z → ∞ is a singular point of Legendre’s equation. However, since wp and w2q are both analytic at w = 0, z → ∞ is a regular singular point.  cid:2   534   16.2 SERIES SOLUTIONS ABOUT AN ORDINARY POINT  Equation  Regular  Essential  singularities  singularities  Hypergeometric  z 1 − z y  1 − z2 y   cid:7  cid:7   + [c −  a + b + 1 z]y  cid:7  cid:7  − 2zy  +  cid:2   cid:2  + 1 y = 0  Legendre   cid:7    cid:7  − aby = 0  cid:14   Associated Legendre   1 − z2 y   cid:7  cid:7  − 2zy   cid:7   +   cid:2   cid:2  + 1  − m2 1 − z2  y = 0   cid:13   Conﬂuent hypergeometric   cid:7   Chebyshev   1 − z2 y  cid:7  cid:7    cid:7  cid:7  − zy +  c − z y Bessel  cid:7  cid:7  z2y  + zy  zy   cid:7   + ν2y = 0  cid:7  − ay = 0  +  z2 − ν2 y = 0  zy  Laguerre   cid:7  cid:7    cid:7  cid:7    cid:7   +  1 − z y +  m + 1 − z y  Associated Laguerre  zy  + νy = 0   cid:7   +  ν − m y = 0  Hermite  y   cid:7    cid:7  cid:7  − 2zy  cid:7  cid:7   y  + ω2y = 0  + 2νy = 0  Simple harmonic oscillator  0, 1,∞ −1, 1,∞  −1, 1,∞  −1, 1,∞  0  0  0  0  —  —  —  —  —  — ∞ ∞ ∞ ∞ ∞ ∞  Table 16.1 Important second-order linear ODEs in the physical sciences and engineering.  Table 16.1 lists the singular points of several second-order linear ODEs that play important roles in the analysis of many problems in physics and engineering. A full discussion of the solutions to each of the equations in table 16.1 and their properties is left until chapter 18. We now discuss the general methods by which series solutions may be obtained.  16.2 Series solutions about an ordinary point  If z = z0 is an ordinary point of  16.7  then it may be shown that every solution y z  of the equation is also analytic at z = z0. From now on we will take z0 as the  origin, i.e. z0 = 0. If this is not already the case, then a substitution Z = z − z0  will make it so. Since every solution is analytic, y z  can be represented by a  535   SERIES SOLUTIONS OF ORDINARY DIFFERENTIAL EQUATIONS  ∞ cid:4   n=0  power series of the form  see section 24.11   y z  =  anzn.   16.9   Moreover, it may be shown that such a power series converges for z < R, where  R is the radius of convergence and is equal to the distance from z = 0 to the nearest singular point of the ODE  see chapter 24 . At the radius of convergence, however, the series may or may not converge  as shown in section 4.5 .  Since every solution of  16.7  is analytic at an ordinary point, it is always possible to obtain two independent solutions  from which the general solution  16.2  can be constructed  of the form  16.9 . The derivatives of y with respect to z are given by  ∞ cid:4  ∞ cid:4   n=0  n=0   cid:7    cid:7  cid:7   y  =  y  =  ∞ cid:4    n + 1 an+1zn,  nanzn−1 = n n − 1 anzn−2 =  n=0  ∞ cid:4   n=0   16.10    n + 2  n + 1 an+2zn.   16.11   Note that, in each case, in the ﬁrst equality the sum can still start at n = 0 since the ﬁrst term in  16.10  and the ﬁrst two terms in  16.11  are automatically zero. The second equality in each case is obtained by shifting the summation index so that the sum can be written in terms of coeﬃcients of zn. By substituting  16.9 – 16.11  into the ODE  16.7 , and requiring that the coeﬃcients of each power of z sum to zero, we obtain a recurrence relation expressing each an in  terms of the previous ar  0 ≤ r ≤ n − 1 .  cid:1 Find the series solutions, about z = 0, of  cid:7  cid:7  y  + y = 0.  By inspection, z = 0 is an ordinary point of the equation, and so we may obtain two n=0 anzn. Using  16.9  and  16.11  independent solutions by making the substitution y = we ﬁnd   cid:11 ∞ ∞ cid:4   n=0  ∞ cid:4  ∞ cid:4   n=0  n=0   n + 2  n + 1 an+2zn +  anzn = 0,  [ n + 2  n + 1 an+2 + an]zn = 0.  which may be written as  For this equation to be satisﬁed we require that the coeﬃcient of each power of z vanishes separately, and so we obtain the two-term recurrence relation  an+2 = −  an   n + 2  n + 1   for n ≥ 0.  Using this relation, we can calculate, say, the even coeﬃcients a2, a4, a6 and so on, for  536   16.2 SERIES SOLUTIONS ABOUT AN ORDINARY POINT  a given a0. Alternatively, starting with a1, we obtain the odd coeﬃcients a3, a5, etc. Two independent solutions of the ODE can be obtained by setting either a0 = 0 or a1 = 0. Firstly, if we set a1 = 0 and choose a0 = 1 then we obtain the solution  y1 z  = 1 − z2  +  2!  z4 4!  ∞ cid:4  − ··· = ∞ cid:4   n=0   −1 n   2n !  z2n.  y2 z  = z − z3  +  3!  z5 5!  − ··· =   −1 n  z2n+1.   2n + 1 !  n=0  Secondly, if we set a0 = 0 and choose a1 = 1 then we obtain a second, independent, solution  Recognising these two series as cos z and sin z, we can write the general solution as  y z  = c1 cos z + c2 sin z,  where c1 and c2 are arbitrary constants that are ﬁxed by boundary conditions  if supplied . We note that both solutions converge for all z, as might be expected since the ODE  possesses no singular points  except z → ∞ .  cid:2   Solving the above example was quite straightforward and the resulting series were easily recognised and written in closed form  i.e. in terms of elementary functions ; this is not usually the case. Another simplifying feature of the previous example was that we obtained a two-term recurrence relation relating an+2 and an, so that the odd- and even-numbered coeﬃcients were independent of one another. In general, the recurrence relation expresses an in terms of any number  of the previous ar  0 ≤ r ≤ n − 1 .  cid:1 Find the series solutions, about z = 0, of   cid:7  cid:7  −  y  2   1 − z 2 y = 0.   cid:11 ∞  ∞ cid:4   n=0  ∞ cid:4   n=0  anzn = 0,   1 − 2z + z2  ∞ cid:4   n n − 1 anzn−2 − 2 ∞ cid:4   By inspection, z = 0 is an ordinary point, and therefore we may ﬁnd two independent n=0 anzn. Using  16.10  and  16.11 , and multiplying through solutions by substituting y =  by  1 − z 2, we ﬁnd  which leads to  ∞ cid:4   n=0  In order to write all these series in terms of the coeﬃcients of zn, we must shift the summation index in the ﬁrst two sums, obtaining  n n − 1 anzn−2 − 2 ∞ cid:4   n n − 1 anzn−1 + ∞ cid:4    n + 2  n + 1 an+2zn − 2  n=0  n=0  n n − 1 anzn − 2 ∞ cid:4    n + 1 nan+1zn +  n=0  n=0   n2 − n − 2 anzn = 0,  ∞ cid:4   n=0  anzn = 0.  n=0  which can be written as  ∞ cid:4   n=0   n + 1 [ n + 2 an+2 − 2nan+1 +  n − 2 an]zn = 0.  537   SERIES SOLUTIONS OF ORDINARY DIFFERENTIAL EQUATIONS  By demanding that the coeﬃcients of each power of z vanish separately, we obtain the three-term recurrence relation   n + 2 an+2 − 2nan+1 +  n − 2 an = 0  for n ≥ 0,  which determines an for n ≥ 2 in terms of a0 and a1. Three-term  or more  recurrence  relations are a nuisance and, in general, can be diﬃcult to solve. This particular recurrence relation, however, has two straightforward solutions. One solution is an = a0 for all n, in which case  choosing a0 = 1  we ﬁnd  y1 z  = 1 + z + z2 + z3 + ··· =  1  1 − z  .  The other solution to the recurrence relation is a1 = −2a0, a2 = a0 and an = 0 for n > 2,  so that  again choosing a0 = 1  we obtain a polynomial solution to the ODE:  y2 z  = 1 − 2z + z2 =  1 − z 2.  The linear independence of y1 and y2 is obvious but can be checked by computing the  Wronskian  W = y1y  − y   cid:7  1y2 =   cid:7  2  1  1 − z  [−2 1 − z ] −  1   1 − z 2   1 − z 2 = −3.  Since W  cid:3 = 0, the two solutions y1 and y2 are indeed linearly independent. The general  solution of the ODE is therefore  y z  =  c1  1 − z  + c2 1 − z 2.  We observe that y1  and hence the general solution  is singular at z = 1, which is the singular point of the ODE nearest to z = 0, but the polynomial solution, y2, is valid for all ﬁnite z.  cid:2   The above example illustrates the possibility that, in some cases, we may ﬁnd that the recurrence relation leads to an = 0 for n > N, for one or both of the two solutions; we then obtain a polynomial solution to the equation. Polynomial solutions are discussed more fully in section 16.5, but one obvious property of such solutions is that they converge for all ﬁnite z. By contrast, as mentioned above, for solutions in the form of an inﬁnite series the circle of convergence extends only as far as the singular point nearest to that about which the solution is being obtained.  16.3 Series solutions about a regular singular point  From table 16.1 we see that several of the most important second-order linear ODEs in physics and engineering have regular singular points in the ﬁnite complex plane. We must extend our discussion, therefore, to obtaining series solutions to ODEs about such points. In what follows we assume that the regular singular point about which the solution is required is at z = 0, since, as we have seen, if  this is not already the case then a substitution of the form Z = z − z0 will make  it so.  If z = 0 is a regular singular point of the equation   cid:7  cid:7   y  + p z y  + q z y = 0   cid:7   538   16.3 SERIES SOLUTIONS ABOUT A REGULAR SINGULAR POINT  ∞ cid:4   n=0  then at least one of p z  and q z  is not analytic at z = 0, and in general we should not expect to ﬁnd a power series solution of the form  16.9 . We must therefore extend the method to include a more general form for the solution. In fact, it may be shown  Fuch’s theorem  that there exists at least one solution to the above equation, of the form  y = zσ  anzn,   16.12   where the exponent σ is a number that may be real or complex and where a0  cid:3 = 0  since, if it were otherwise, σ could be redeﬁned as σ + 1 or σ + 2 or ··· so as to make a0  cid:3 = 0 . Such a series is called a generalised power series or Frobenius series.  As in the case of a simple power series solution, the radius of convergence of the Frobenius series is, in general, equal to the distance to the nearest singularity of the ODE.  Since z = 0 is a regular singularity of the ODE, it follows that zp z  and z2q z   are analytic at z = 0, so that we may write  zp z  ≡ s z  =  z2q z  ≡ t z  =  ∞ cid:4  ∞ cid:4   n=0  n=0  snzn,  tnzn,   cid:7  cid:7   y  +  s z    cid:7   y  +  z  t z  z2 y = 0.  where we have deﬁned the analytic functions s z  and t z  for later convenience. The original ODE therefore becomes  ∞ cid:4  ∞ cid:4   n=0  n=0  Let us substitute the Frobenius series  16.12  into this equation. The derivatives  of  16.12  with respect to z are given by   cid:7    cid:7  cid:7   y  =  y  =   n + σ anzn+σ−1,  n + σ  n + σ − 1 anzn+σ−2,   16.13    16.14   and we obtain  ∞ cid:4   n=0  ∞ cid:4   n=0   n + σ  n + σ − 1 anzn+σ−2 + s z    n + σ anzn+σ−2 + t z   anzn+σ−2 = 0.  Dividing this equation through by zσ−2, we ﬁnd  [ n + σ  n + σ − 1  + s z  n + σ  + t z ] anzn = 0.   16.15   ∞ cid:4   n=0  ∞ cid:4   n=0  539   SERIES SOLUTIONS OF ORDINARY DIFFERENTIAL EQUATIONS  Setting z = 0, all terms in the sum with n > 0 vanish, implying that  [σ σ − 1  + s 0 σ + t 0 ]a0 = 0, which, since we require a0  cid:3 = 0, yields the indicial equation  σ σ − 1  + s 0 σ + t 0  = 0.   16.16   This equation is a quadratic in σ and in general has two roots, the nature of which determines the forms of possible series solutions.  The two roots of the indicial equation, σ1 and σ2, are called the indices of the regular singular point. By substituting each of these roots into  16.15  in turn and requiring that the coeﬃcients of each power of z vanish separately, we obtain a recurrence relation  for each root  expressing each an as a function of  the previous ar  0 ≤ r ≤ n − 1 . We will see that the larger root of the indicial  equation always yields a solution to the ODE in the form of a Frobenius series  16.12 . The form of the second solution depends, however, on the relationship between the two indices σ1 and σ2. There are three possible general cases:  i  distinct roots not diﬀering by an integer;  ii  repeated roots;  iii  distinct roots diﬀering by an integer  not equal to zero . Below, we discuss each of these in turn. Before continuing, however, we note that, as was the case for solutions in the form of a simple power series, it is always worth investigating whether a Frobenius series found as a solution to a problem is summable in closed form or expressible in terms of known functions. We illustrate this point below, but the reader should avoid gaining the impression that this is always so or that, if one worked hard enough, a closed-form solution could always be found without using the series method. As mentioned earlier, this is not the case, and very often an inﬁnite series solution is the best one can do.  16.3.1 Distinct roots not differing by an integer  If the roots of the indicial equation, σ1 and σ2, diﬀer by an amount that is not an integer then the recurrence relations corresponding to each root lead to two linearly independent solutions of the ODE:  ∞ cid:4   n=0  ∞ cid:4   n=0  y1 z  = zσ1  anzn,  y2 z  = zσ2  bnzn,  with both solutions taking the form of a Frobenius series. The linear independence of these two solutions follows from the fact that y2 y1 is not a constant since  σ1 − σ2 is not an integer. Because y1 and y2 are linearly independent, we may use ∗ 1,  We also note that this case includes complex conjugate roots where σ2 = σ  them to construct the general solution y = c1y1 + c2y2.  ∗ 1 = 2i Im σ1 cannot be equal to a real integer.  since σ1 − σ2 = σ1 − σ  540   16.3 SERIES SOLUTIONS ABOUT A REGULAR SINGULAR POINT   cid:1 Find the power series solutions about z = 0 of   cid:7  cid:7    cid:7   4zy  + 2y  + y = 0.  Dividing through by 4z to put the equation into standard form, we obtain   cid:7  cid:7   y  +   cid:7   y  +  1 2z  1 4z  y = 0,   16.17    cid:11 ∞  and on comparing with  16.7  we identify p z  = 1  2z  and q z  = 1  4z . Clearly z = 0 is a singular point of  16.17 , but since zp z  = 1 2 and z2q z  = z 4 are ﬁnite there, it is a regular singular point. We therefore substitute the Frobenius series y = zσ n=0 anzn into  16.17 . Using  16.13  and  16.14 , we obtain  ∞ cid:4    n + σ  n + σ − 1 anzn+σ−2 + 1 2z which, on dividing through by zσ−2, gives  n=0  ∞ cid:4   n=0  1 4z  anzn+σ = 0,  ∞ cid:4   n=0   n + σ anzn+σ−1 +  cid:19    n + σ  n + σ − 1  + 1  2  n + σ  + 1 4 z  anzn = 0.   16.18   ∞ cid:4    cid:18   n=0  If we set z = 0 then all terms in the sum with n > 0 vanish, and we obtain the indicial equation  σ σ − 1  + 1  2 σ = 0,  which has roots σ = 1 2 and σ = 0. Since these roots do not diﬀer by an integer, we expect to ﬁnd two independent solutions to  16.17 , in the form of Frobenius series.  Demanding that the coeﬃcients of zn vanish separately in  16.18 , we obtain the  recurrence relation   n + σ  n + σ − 1 an + 1  2  n + σ an + 1   16.19   If we choose the larger root, σ = 1 2, of the indicial equation then  16.19  becomes  Setting a0 = 1, we ﬁnd an =  −1 n  2n + 1 !, and so the solution to  16.17  is given by  4 an−1 = 0. −an−1  2n 2n + 1   .   4n2 + 2n an + an−1 = 0 ⇒ an =  y1 z  =  z  ∞ cid:4   √  n=0  √ z −    =   −1 n  zn √ √  2n + 1 ! z 3 3!  +     z 5 5!  − ··· = sin  √  z.  To obtain the second solution we set σ = 0  the smaller root of the indicial equation  in  16.19 , which gives   4n2 − 2n an + an−1 = 0 ⇒ an = − an−1  2n 2n − 1   .  Setting a0 = 1 now gives an =  −1 n  2n !, and so the second  independent  solution to  ∞ cid:4   n=0  y2 z  =   −1 n   2n !  zn = 1 −    √ z 2 2!  √ 4 4 4!     +  − ··· = cos  √  z.   16.17  is  541   SERIES SOLUTIONS OF ORDINARY DIFFERENTIAL EQUATIONS  We may check that y1 z  and y2 z  are indeed linearly independent by computing the  Wronskian as follows:   cid:7  − y2y  cid:7  W = y1y √ 2  cid:5    cid:7  1 √ − 1 2 √  = sin  z  z  √ = − 1  2  z  √  sin  z   cid:8    cid:7   √  z   cid:8  − cos  cid:6  √  √ = − 1  2  z  √  cos  z  2  √ 1  cid:3 = 0.  z  sin2  z + cos2  z  Since W  cid:3 = 0, the solutions y1 z  and y2 z  are linearly independent. Hence, the general  solution to  16.17  is given by  √  √ z.  cid:2   y z  = c1 sin  z + c2 cos  16.3.2 Repeated root of the indicial equation  If the indicial equation has a repeated root, so that σ1 = σ2 = σ, then obviously only one solution in the form of a Frobenius series  16.12  may be found as described above, i.e.  ∞ cid:4   n=0  y1 z  = zσ  anzn.  Methods for obtaining a second, linearly independent, solution are discussed in section 16.4.  16.3.3 Distinct roots differing by an integer  Whatever the roots of the indicial equation, the recurrence relation corresponding to the larger of the two always leads to a solution of the ODE. However, if the roots of the indicial equation diﬀer by an integer then the recurrence relation corresponding to the smaller root may or may not lead to a second linearly independent solution, depending on the ODE under consideration. Note that for complex roots of the indicial equation, the ‘larger’ root is taken to be the one with the larger real part.  cid:1 Find the power series solutions about z = 0 of + 3zy  z z − 1 y  + y = 0.   16.20    cid:7  cid:7    cid:7   Dividing through by z z − 1  to put the equation into standard form, we obtain   cid:7  cid:7   y  +  3   z − 1    cid:7   y  +  1  z z − 1   y = 0,   16.21   and on comparing with  16.7  we identify p z  = 3  z − 1  and q z  = 1 [z z − 1 ]. We immediately see that z = 0 is a singular point of  16.21 , but since zp z  = 3z  z − 1  and z2q z  = z  z−1  are ﬁnite there, it is a regular singular point and we expect to ﬁnd at least  542   16.3 SERIES SOLUTIONS ABOUT A REGULAR SINGULAR POINT  one solution in the form of a Frobenius series. We therefore substitute y = zσ into  16.21  and, using  16.13  and  16.14 , we obtain  n=0 anzn  ∞ cid:4   n=0   n + σ  n + σ − 1 anzn+σ−2 +  3  z − 1   cid:11 ∞  ∞ cid:4   n=0   n + σ anzn+σ−1 ∞ cid:4  z z − 1   cid:14   n=0  +  1  anzn+σ = 0,  3z  z − 1   n + σ  +  anzn = 0.  z  z − 1  which, on dividing through by zσ−2, gives  n + σ  n + σ − 1  +   cid:13   ∞ cid:4   n=0  ∞ cid:4   n=0  Although we could use this expression to ﬁnd the indicial equation and recurrence relations,  the working is simpler if we now multiply through by z − 1 to give  [ z − 1  n + σ  n + σ − 1  + 3z n + σ  + z] anzn = 0.   16.22   If we set z = 0 then all terms in the sum with the exponent of z greater than zero vanish, and we obtain the indicial equation  σ σ − 1  = 0,  which has the roots σ = 1 and σ = 0. Since the roots diﬀer by an integer  unity , it may not be possible to ﬁnd two linearly independent solutions of  16.21  in the form of Frobenius series. We are guaranteed, however, to ﬁnd one such solution corresponding to the larger root, σ = 1.  Demanding that the coeﬃcients of zn vanish separately in  16.22 , we obtain the  recurrence relation   n − 1 + σ  n − 2 + σ an−1 −  n + σ  n + σ − 1 an + 3 n − 1 + σ an−1 + an−1 = 0,  which can be simpliﬁed to give   n + σ − 1 an =  n + σ an−1.   16.23   On substituting σ = 1 into this expression, we obtain  an =  n + 1  n  an−1,  ∞ cid:4   n=0 z   1 − z 2 .  =  and on setting a0 = 1 we ﬁnd an = n + 1; so one solution to  16.21  is given by  y1 z  = z   n + 1 zn = z 1 + 2z + 3z2 + ···     16.24   If we attempt to ﬁnd a second solution  corresponding to the smaller root of the indicial  equation  by setting σ = 0 in  16.23 , we ﬁnd n  an =  n − 1  an−1.  But we require a0  cid:3 = 0, so a1 is formally inﬁnite and the method fails. We discuss how to ﬁnd a second linearly independent solution in the next section.  cid:2   One particular case is worth mentioning. If the point about which the solution   cid:7    cid:9    cid:8    cid:10   543   SERIES SOLUTIONS OF ORDINARY DIFFERENTIAL EQUATIONS  is required, i.e. z = 0, is in fact an ordinary point of the ODE rather than a regular singular point, then substitution of the Frobenius series  16.12  leads to an indicial equation with roots σ = 0 and σ = 1. Although these roots diﬀer by an integer  unity , the recurrence relations corresponding to the two roots yield two linearly independent power series solutions  one for each root , as expected from section 16.2.  16.4 Obtaining a second solution  Whilst attempting to construct solutions to an ODE in the form of Frobenius series about a regular singular point, we found in the previous section that when the indicial equation has a repeated root, or roots diﬀering by an integer, we can  in general  ﬁnd only one solution of this form. In order to construct the general solution to the ODE, however, we require two linearly independent solutions y1 and y2. We now consider several methods for obtaining a second solution in this case.  16.4.1 The Wronskian method   cid:7  cid:7   y  + p z y  + q z y = 0  If y1 and y2 are two linearly independent solutions of the standard equation   cid:7  then the Wronskian of these two solutions is given by W  z  = y1y 2 Dividing the Wronskian by y2  1 we obtain   cid:7    cid:8  cid:14    cid:7    cid:8   − y2y  cid:7  1.   cid:7  y 2 y1   cid:7  − y 1 y2 1  W y2 1  =   cid:7  y 2 y1  y2 =  +  1 y1  y2 =  d dz  y2 y1  ,  which integrates to give  y2 z  = y1 z    cid:21   z  1 y2 1 u   exp  z W  u  y2 1 u   du.   cid:12    cid:21   −  u   cid:15   Now using the alternative expression for W  z  given in  16.4  with C = 1  since we are not concerned with this normalising factor , we ﬁnd  y2 z  = y1 z   p v  dv  du.   16.25   Hence, given y1, we can in principle compute y2. Note that the lower limits of integration have been omitted. If constant lower limits are included then they merely lead to a constant times the ﬁrst solution.  cid:1 Find a second solution to  16.21  using the Wronskian method. For the ODE  16.21  we have p z  = 3  z − 1 , and from  16.24  we see that one solution   cid:7    cid:13   d dz   cid:21   544   16.4 OBTAINING A SECOND SOLUTION  to  16.21  is y1 = z  1 − z 2. Substituting for p and y1 in  16.25  we have   cid:8    cid:21    cid:7   −  exp  u  3  v − 1  dv  du  exp [−3 ln u − 1 ] du   cid:21   cid:21   cid:21   cid:7   u2  z  1 − u 4 z  1 − u 4 z u − 1  cid:8   u2  u2  du  ln z +  .  1 z  y2 z  =  z  z   1 − z 2  1 − z 2  1 − z 2  1 − z 2  z  z  =  =  =  By calculating the Wronskian of y1 and y2 it is easily shown that, as expected, the two solutions are linearly independent. In fact, as the Wronskian has already been evaluated  as W  u  = exp[−3 ln u − 1 ], i.e. W  z  =  z − 1   −3, no calculation is needed.  cid:2   An alternative  but equivalent  method of ﬁnding a second solution is simply to assume that the second solution has the form y2 z  = u z y1 z  for some function u z  to be determined  this method was discussed more fully in subsection 15.2.3 . From  16.25 , we see that the second solution derived from the Wronskian is indeed of this form. Substituting y2 z  = u z y1 z  into the ODE leads to a  cid:7  is the dependent variable; this may then be solved. ﬁrst-order ODE in which u  16.4.2 The derivative method  The derivative method of ﬁnding a second solution begins with the derivation of a recurrence relation for the coeﬃcients an in a Frobenius series solution, as in the previous section. However, rather than putting σ = σ1 in this recurrence relation to evaluate the ﬁrst series solution, we now keep σ as a variable parameter. This means that the computed an are functions of σ and the computed solution is now a function of z and σ:  y z, σ  = zσ  an σ zn.   16.26   Of course, if we put σ = σ1 in this, we obtain immediately the ﬁrst series solution, but for the moment we leave σ as a parameter. ODE  16.7  by L, so that  For brevity let us denote the diﬀerential operator on the LHS of our standard  L =  d2 dz2 + p z   d dz  + q z ,  and examine the eﬀect of L on the series y z, σ  in  16.26 . It is clear that the series Ly z, σ  will contain only a term in zσ, since the recurrence relation deﬁning the an σ  is such that these coeﬃcients vanish for higher powers of z. But the coeﬃcient of zσ is simply the LHS of the indicial equation. Therefore, if the roots  ∞ cid:4   n=0  545   SERIES SOLUTIONS OF ORDINARY DIFFERENTIAL EQUATIONS  of the indicial equation are σ = σ1 and σ = σ2 then it follows that  Ly z, σ  = a0 σ − σ1  σ − σ2 zσ.   16.27   Therefore, as in the previous section, we see that for y z, σ  to be a solution of the ODE Ly = 0, σ must equal σ1 or σ2. For simplicity we shall set a0 = 1 in the  following discussion.  Let us ﬁrst consider the case in which the two roots of the indicial equation  are equal, i.e. σ2 = σ1. From  16.27  we then have  Ly z, σ  =  σ − σ1 2zσ.  Diﬀerentiating this equation with respect to σ we obtain  [Ly z, σ ] =  σ − σ1 2zσ ln z + 2 σ − σ1 zσ,  ∂ ∂σ  which equals zero if σ = σ1. But since ∂ ∂σ and L are operators that diﬀerentiate  with respect to diﬀerent variables, we can reverse their order, implying that  y z, σ   = 0  at σ = σ1.   cid:14    cid:13   L  ∂ ∂σ   cid:13    cid:14   ∂ ∂σ  y z, σ   ,  σ=σ1  Hence, the function in square brackets, evaluated at σ = σ1 and denoted by   16.28   is also a solution of the original ODE Ly = 0, and is in fact the second linearly  independent solution that we were looking for. The case in which the roots of the indicial equation diﬀer by an integer is slightly more complicated but can be treated in a similar way. In  16.27 , since L diﬀerentiates with respect to z we may multiply  16.27  by any function of σ, say σ − σ2, and take this function inside the operator L on the LHS to obtain  L [ σ − σ2 y z, σ ] =  σ − σ1  σ − σ2 2zσ.   16.29   Therefore the function  [ σ − σ2 y z, σ ]σ=σ2  is also a solution of the ODE Ly = 0. However, it can be proved that this function is a simple multiple of the ﬁrst solution y z, σ1 , showing that it is not linearly independent and that we must ﬁnd another solution. To do this we diﬀerentiate  16.29  with respect to σ and ﬁnd  §  {L [ σ − σ2 y z, σ ]} =  σ − σ2 2zσ + 2 σ − σ1  σ − σ2 zσ  ∂ ∂σ  +  σ − σ1  σ − σ2 2zσ ln z,  §  For a fuller discussion see, for example, K. F. Riley, Mathematical Methods for the Physical Sciences  Cambridge: Cambridge University Press, 1974 , pp. 158–9.  546   16.4 OBTAINING A SECOND SOLUTION  which is equal to zero if σ = σ2. As previously, since ∂ ∂σ and L are operators  that diﬀerentiate with respect to diﬀerent variables, we can reverse their order to obtain   cid:15    cid:12   L  ∂ ∂σ  and so the function  = 0  at σ = σ2,  [ σ − σ2 y z, σ ]  cid:12    cid:15   [ σ − σ2 y z, σ ]  ∂ ∂σ  σ=σ2  is also a solution of the original ODE Ly = 0, and is in fact the second linearly  independent solution.  cid:1 Find a second solution to  16.21  using the derivative method.  From  16.23  the recurrence relation  with σ as a parameter  is given by   16.30   Setting a0 = 1 we ﬁnd that the coeﬃcients have the particularly simple form an σ  =  σ + n  σ. We therefore consider the function   n + σ − 1 an =  n + σ an−1. ∞ cid:4   ∞ cid:4   y z, σ  = zσ  an σ zn = zσ  n=0  n=0  σ + n  zn.  σ  [σy z, σ ]  =   σ + n zn  .   cid:12   ∂ ∂σ   cid:16   ∂ ∂σ  ∞ cid:4   n=0     cid:16   ∂ ∂σ   cid:15   σ=0   cid:17   ∞ cid:4   n=0  zσ  ∞ cid:4  ∞ cid:4  ∞ cid:4   n=0   cid:17 .  σ=0  ∞ cid:4   n=0  zσ   σ + n zn  = zσ ln z   σ + n zn + zσ  zn,  y2 z  = ln z  nzn +  zn  ln z +   cid:7   n=0 z   1 − z 2  1 − z 2  z  =  =  n=0   cid:8   .  1  1 − z − 1  1 z  ln z +  The derivative with respect to σ is given by  which on setting σ = 0 gives the second solution  The smaller root of the indicial equation for  16.21  is σ2 = 0, and so from  16.30  a second, linearly independent, solution to the ODE is given by  This second solution is the same as that obtained by the Wronskian method in the previous subsection except for the addition of some of the ﬁrst solution.  cid:2   16.4.3 Series form of the second solution  Using any of the methods discussed above, we can ﬁnd the general form of the second solution to the ODE. This form is most easily found, however, using the  547   SERIES SOLUTIONS OF ORDINARY DIFFERENTIAL EQUATIONS   cid:14  ∞ cid:4   n=0  ∞ cid:4   n=1   cid:15    cid:17   σ=σ2  derivative method. Let us ﬁrst consider the case where the two solutions of the indicial equation are equal. In this case a second solution is given by  16.28 , which may be written as   cid:13   y2 z  =  ∂y z, σ   ∂σ  σ=σ1  =  ln z zσ1  an σ1 zn + zσ1   cid:13   ∞ cid:4   n=1   cid:14   dan σ   dσ  zn  σ=σ1  = y1 z  ln z + zσ1  bnzn,   16.31    cid:12   where bn = [dan σ  dσ]σ=σ1 . One could equally obtain the coeﬃcients bn by direct substitution of the form  16.31  into the original ODE.  In the case where the roots of the indicial equation diﬀer by an integer  not  equal to zero , then from  16.30  a second solution is given by  y2 z  =  ∂ ∂σ   cid:16   [ σ − σ2 y z, σ ] ∞ cid:4   σ − σ2 zσ  = ln z   cid:13   ∞ cid:4    cid:14   σ − σ2 an σ   zn.  σ=σ2  an σ zn  + zσ2  n=0  σ=σ2  n=0  d dσ  But, as we mentioned in the previous section, [ σ − σ2 y z, σ ] at σ = σ2 is just a  multiple of the ﬁrst solution y z, σ1 . Therefore the second solution is of the form  y2 z  = cy1 z  ln z + zσ2  bnzn,   16.32   ∞ cid:4   n=0  where c is a constant. In some cases, however, c might be zero, and so the second solution would not contain the term in ln z and could be written simply as a Frobenius series. Clearly this corresponds to the case in which the substitution of a Frobenius series into the original ODE yields two solutions automatically. In either case, the coeﬃcients bn may also be found by direct substitution of the form  16.32  into the original ODE.  16.5 Polynomial solutions  We have seen that the evaluation of successive terms of a series solution to a diﬀerential equation is carried out by means of a recurrence relation. The form of the relation for an depends upon n, the previous values of ar  r < n  and the parameters of the equation. It may happen, as a result of this, that for some value of n = N + 1 the computed value aN+1 is zero and that all higher ar also vanish. If this is so, and the corresponding solution of the indicial equation σ  548   16.5 POLYNOMIAL SOLUTIONS  is a positive integer or zero, then we are left with a ﬁnite polynomial of degree N  = N + σ as a solution of the ODE:   cid:7   y z  =  anzn+σ.   16.33   In many applications in theoretical physics  particularly in quantum mechanics  the termination of a potentially inﬁnite series after a ﬁnite number of terms is of crucial importance in establishing physically acceptable descriptions and properties of systems. The condition under which such a termination occurs is therefore of considerable importance.  cid:1 Find power series solutions about z = 0 of  cid:7  cid:7  − 2zy  cid:7   + λy = 0.   16.34   y  For what values of λ does the equation possess a polynomial solution? Find such a solution for λ = 4.  Clearly z = 0 is an ordinary point of  16.34  and so we look for solutions of the form y =  n=0 anzn. Substituting this into the ODE and multiplying through by z2 we ﬁnd   cid:11 ∞  [n n − 1  − 2z2n + λz2]anzn = 0.  n n − 1 an − 2 n − 2 an−2 + λan−2 = 0,  2 n − 2  − λ n n − 1   an−2  an =  for n ≥ 2.  By demanding that the coeﬃcients of each power of z vanish separately we derive the recurrence relation  which may be rearranged to give   16.35   The odd and even coeﬃcients are therefore independent of one another, and two solutions to  16.34  may be derived. We either set a1 = 0 and a0 = 1 to obtain − ···  − λ 4 − λ  8 − λ   y1 z  = 1 − λ  − λ 4 − λ    16.36   z6 6!  or set a0 = 0 and a1 = 1 to obtain  y2 z  = z +  2 − λ   +  2 − λ  6 − λ   +  2 − λ  6 − λ  10 − λ   + ··· .  z7 7!  Now, from the recurrence relation  16.35   or in this case from the expressions for y1 and y2 themselves  we see that for the ODE to possess a polynomial solution we require  λ = 2 n − 2  for n ≥ 2 or, more simply, λ = 2n for n ≥ 0, i.e. λ must be an even positive  integer. If λ = 4 then from  16.36  the ODE has the polynomial solution  y1 z  = 1 − 4z2  = 1 − 2z2.  cid:2   2!  A simpler method of obtaining ﬁnite polynomial solutions is to assume a  solution of the form  16.33 , where aN  cid:3 = 0. Instead of starting with the lowest  power of z, as we have done up to now, this time we start by considering the  ∞ cid:4   n=0  z2 2!  z3 3!  N cid:4   n=0  z4 4!  z5 5!  549   SERIES SOLUTIONS OF ORDINARY DIFFERENTIAL EQUATIONS  coeﬃcient of the highest power zN; such a power now exists because of our assumed form of solution.  cid:1 By assuming a polynomial solution ﬁnd the values of λ in  16.34  for which such a solution exists.  We assume a polynomial solution to  16.34  of the form y = form into  16.34  we ﬁnd  N  n=0 anzn. Substituting this   cid:11   cid:19   n n − 1 anzn−2 − 2znanzn−1 + λanzn  = 0.  N cid:4    cid:18   n=0  Now, instead of starting with the lowest power of z, we start with the highest. Thus,  demanding that the coeﬃcient of zN vanishes, we require −2N + λ = 0, i.e. λ = 2N, as we found in the previous example. By demanding that the coeﬃcient of a general power of z is zero, the same recurrence relation as above may be derived and the solutions found.  cid:2   16.1  Find two power series solutions about z = 0 of the diﬀerential equation  16.6 Exercises   1 − z2 y   cid:7  cid:7  − 3zy   cid:7   + λy = 0.  16.2  16.3  16.4  Deduce that the value of λ for which the corresponding power series becomes an Nth-degree polynomial UN z  is N N + 2 . Construct U2 z  and U3 z . Find solutions, as power series in z, of the equation   cid:7  cid:7   + 2 1 − z y   cid:7  − y = 0.  4zy  Identify one of the solutions and verify it by direct substitution. Find power series solutions in z of the diﬀerential equation   cid:7  cid:7  − 2y   cid:7   zy  + 9z5y = 0.  Identify closed forms for the two series, calculate their Wronskian, and verify that they are linearly independent. Compare the Wronskian with that calculated from the diﬀerential equation. Change the independent variable in the equation  d2f dz2  + 2 z − a   df dz  + 4f = 0   ∗   from z to x = z − α, and ﬁnd two independent series solutions, expanded about x = 0, of the resulting equation. Deduce that the general solution of  ∗  is  f z, α  = A z − α e  − z−α 2  + B  ∞ cid:4   m=0   −4 mm!   2m !   z − α 2m,  16.5  with A and B arbitrary constants. Investigate solutions of Legendre’s equation at one of its singular points as follows.   a  Verify that z = 1 is a regular singular point of Legendre’s equation and that  the indicial equation for a series solution in powers of  z − 1  has roots 0   b  Obtain the corresponding recurrence relation and show that σ = 0 does not  and 3.  give a valid series solution.  550   16.6 EXERCISES   c  Determine the radius of convergence R of the σ = 3 series and relate it to  the positions of the singularities of Legendre’s equation.  16.6  Verify that z = 0 is a regular singular point of the equation   cid:7  cid:7  − 3   cid:7   2 zy  z2y  +  1 + z y = 0,  and that the indicial equation has roots 2 and 1 2. Show that the general solution is given by  y z  = 6a0z2  ∞ cid:4   cid:30   n=0   −1 n n + 1 22nzn ∞ cid:4    2n + 3 !  + b0  z1 2 + 2z3 2 − z1 2  cid:30   4  J0 z  ln z − ∞ cid:4    −1 n   n! 2  n=1  n=2  n cid:4   r=1   cid:31   .   −1 n22nzn n n − 1  2n − 3 !  cid:31  cid:9    cid:10   1 r  2n  ,  z 2  16.7  Use the derivative method to obtain, as a second solution of Bessel’s equation for the case when ν = 0, the following expression:  16.8  given that the ﬁrst solution is J0 z , as speciﬁed by  18.79 . Consider a series solution of the equation   cid:7  cid:7  − 2y   cid:7   zy  + yz = 0   ∗   about its regular singular point.   a  Show that its indicial equation has roots that diﬀer by an integer but that  the two roots nevertheless generate linearly independent solutions  ∞ cid:4  ∞ cid:4   n=1   −1 n+1 2nz2n+1   2n + 1 !   −1 n+1 2n − 1 z2n  ,  .   2n !  y1 z  = 3a0  y2 z  = a0   b  Show that y1 z  is equal to 3a0 sin z − z cos z  by expanding the sinusoidal functions. Then, using the Wronskian method, ﬁnd an expression for y2 z  in terms of sinusoids. You will need to write z2 as  z  sin z  z sin z  and integrate by parts to evaluate the integral involved.  n=0   c  Conﬁrm that the two solutions are linearly independent by showing that  their Wronskian is equal to −z2, as would be expected from the form of  ∗ .  cid:7  − 2y = 0. Identify one of the series Find series solutions of the equation y as y1 z  = exp z2 and verify this by direct substitution. By setting y2 z  = u z y1 z  and solving the resulting equation for u z , ﬁnd an explicit form for y2 z  and deduce that   cid:7  cid:7  − 2zy ∞ cid:4    cid:21   x  −v2 e  0  −x2  dv = e  n!   2x 2n+1.  2 2n + 1 !  n=0  16.9  16.10  Solve the equation  as follows.  z 1 − z   d2y dz2  +  1 − z   dy dz  + λy = 0   a  Identify and classify its singular points and determine their indices.  551   SERIES SOLUTIONS OF ORDINARY DIFFERENTIAL EQUATIONS   b  Find one series solution in powers of z. Give a formal expression for a  second linearly independent solution.   c  Deduce the values of λ for which there is a polynomial solution PN z  of degree N. Evaluate the ﬁrst four polynomials, normalised in such a way that PN 0  = 1.  16.11  Find the general power series solution about z = 0 of the equation  +  2z − 3   z  d2y dz2  dy dz  4 z  +  y = 0.  16.12  Find the radius of convergence of a series solution about the origin for the equation  z2 + az + b y  + 2y = 0 in the following cases:   cid:7  cid:7    a  a = 5, b = 6;   b  a = 5, b = 7.  16.13  Show that if a and b are real and 4b > a2, then the radius of convergence is always given by b1 2.  cid:7  cid:7  −3y = 0, show that the origin becomes a regular singular For the equation y point if the independent variable is changed from z to x = 1 z. Hence ﬁnd a −n. By setting y2 z  = u z y1 z  and series solution of the form y1 z  = −1, show that y2 z  expanding the resulting expression for du dz in powers of z has the asymptotic form   cid:11 ∞  0 anz  + z   cid:13    cid:7    cid:8  cid:14   ,  y2 z  = c  z + ln z − 1  2 + O  ln z z  16.14  where c is an arbitrary constant. Prove that the Laguerre equation,  has polynomial solutions LN z  if λ is a non-negative integer N, and determine the recurrence relationship for the polynomial coeﬃcients. Hence show that an expression for LN z , normalised in such a way that LN 0  = N!, is  z  d2y dz2  +  1 − z   dy dz  + λy = 0,  N cid:4   n=0  LN z  =   −1 n N! 2  N − n ! n! 2 zn.   1 − z2 y   cid:7  cid:7  − zy   cid:7   + m2y = 0,   cid:11 ∞  16.15  Evaluate L3 z  explicitly. The origin is an ordinary point of the Chebyshev equation,  which therefore has series solutions of the form zσ  0 anzn for σ = 0 and σ = 1.   a  Find the recurrence relationships for the an in the two cases and show that  there exist polynomial solutions Tm z :   i    ii   for σ = 0, when m is an even integer, the polynomial having 1 terms; for σ = 1, when m is an odd integer, the polynomial having 1 terms.  2  m + 2   2  m + 1    b  Tm z  is normalised so as to have Tm 1  = 1. Find explicit forms for Tm z   for m = 0, 1, 2, 3.  552    c  Show that the corresponding non-terminating series solutions Sm z  have as  their ﬁrst few terms  16.7 HINTS AND ANSWERS  S0 z  = a0  S1 z  = a0  S2 z  = a0   cid:8   cid:8   cid:8   cid:8   ,  ,  ,  .  9 5!  z3 + z2 − 3 z3 − 15  4!  5!  45 4!  z5 + ··· z4 − ··· z5 − ··· z4 + ···  1 3!  z +   cid:7   cid:7   cid:7  1 − 1  cid:7  z − 3 1 − 9  cid:11   3!  2!  2!  S3 z  = a0  z2 +  16.16  Obtain the recurrence relations for the solution of Legendre’s equation  18.1  in inverse powers of z, i.e. set y z  = integer, then the series with σ =  cid:2  will terminate and hence converge for all z,  anzσ−n, with a0  cid:3 = 0. Deduce that, if  cid:2  is an whilst the series with σ = −  cid:2  + 1  does not terminate and hence converges only for z > 1.  16.7 Hints and answers  16.1  16.3  16.5  16.7  16.9  16.11  Note that z = 0 is an ordinary point of the equation.  For σ = 0, an+2 an = [n n + 2  − λ] [ n + 1  n + 2 ] and, correspondingly, for σ = 1, U2 z  = a0 1 − 4z2  and U3 z  = a0 z − 2z3 . σ = 0 and 3; a6m a0 =  −1 m  2m ! and a6m a0 =  −1 m  2m + 1 !, respectively. y1 z  = a0 cos z3 and y2 z  = a0 sin z3. The Wronskian is ±3a2  b  an+1 an = [  cid:2   cid:2  + 1  − n n + 1  ] [ 2 n + 1 2 ]. z = −1.   c  R = 2, equal to the distance between z = 1 and the closest singularity at  0z2  cid:3 = 0.   −1 nz2n  [  σ + 2  σ + 4 ···  σ + 2n  ]2  .  A typical term in the series for y σ, z  is  The origin is an ordinary point. Determine the constant of integration by exam- ining the behaviour of the related functions for small x. y2 z  =  exp z2  Repeated roots σ = 2.  0 exp −x2  dx.  z   n + 1  −2z n+2  n!  !  a 4  "  + b [ln z + g n ]  ,  y z  = az2 − 4az3 + 6bz3 +   cid:1   where  g n  =  1  n + 1  − ··· − 1 2  − 1 n − 1 + y = 0; an =  −1 n n + 1   cid:7   − 2.  −1 n!   −2a0;  16.13  16.15  The transformed equation is xy du dz = A[ y1 z  ]  −2.   a   i  an+2 = [an n2 − m2 ] [ n + 2  n + 1 ],  ii  an+2 = {an[ n + 1 2 − m2]} [ n + 3  n + 2 ];  b  1, z, 2z2 − 1, 4z3 − 3z.  + 2y  ∞ cid:4   n=2  − 1  cid:7  cid:7   n  553   17  Eigenfunction methods for  diﬀerential equations  In the previous three chapters we dealt with the solution of diﬀerential equations of order n by two methods. In one method, we found n independent solutions of the equation and then combined them, weighted with coeﬃcients determined by the boundary conditions; in the other we found solutions in terms of series whose coeﬃcients were related by  in general  an n-term recurrence relation and thence ﬁxed by the boundary conditions. For both approaches the linearity of the equation was an important or essential factor in the utility of the method, and in this chapter our aim will be to exploit the superposition properties of linear diﬀerential equations even further.  We will be concerned with the solution of equations of the inhomogeneous  form  Ly x  = f x ,   17.1   where f x  is a prescribed or general function and the boundary conditions to be satisﬁed by the solution y = y x , for example at the limits x = a and x = b, are given. The expression Ly x  stands for a linear diﬀerential operator L acting upon the function y x .  In general, unless f x  is both known and simple, it will not be possible to ﬁnd particular integrals of  17.1 , even if complementary functions can be found that satisfy Ly = 0. The idea is therefore to exploit the linearity of L by building up the required solution y x  as a superposition, generally containing an inﬁnite number of terms, of some set of functions {yi x } that each individually satisfy  the boundary conditions. Clearly this brings in a quite considerable complication but since, within reason, we may select the set of functions to suit ourselves, we can obtain sizeable compensation for this complication. Indeed, if the set chosen is one containing functions that, when acted upon by L, produce particularly simple results then we can ‘show a proﬁt’ on the operation. In particular, if the  554   EIGENFUNCTION METHODS FOR DIFFERENTIAL EQUATIONS  set consists of those functions yi for which  Lyi x  = λiyi x ,   17.2    17.3   where λi is a constant  and which satisfy the boundary conditions , then a distinct advantage may be obtained from the manoeuvre because all the diﬀerentiation will have disappeared from  17.1 .  Equation  17.2  is clearly reminiscent of the equation satisﬁed by the eigenvec-  tors xi of a linear operator A , namely  A xi = λixi,  where λi is a constant and is called the eigenvalue associated with xi. By analogy, in the context of diﬀerential equations a function yi x  satisfying  17.2  is called an eigenfunction of the operator L  under the imposed boundary conditions  and λi is then called the eigenvalue associated with the eigenfunction yi x . Clearly, the eigenfunctions yi x  of L are only determined up to an arbitrary scale factor  by  17.2 .  a simple harmonic oscillator, i.e.  Probably the most familiar equation of the form  17.2  is that which describes  Ly ≡ − d2y  dt2 = ω2y,  where L ≡ −d2 dt2.   17.4   Imposing the boundary condition that the solution is periodic with period T , the eigenfunctions in this case are given by yn t  = Aneiωnt, where ωn = 2πn T , n = 0,±1,±2, . . . and the An are constants. The eigenvalues are ω2 1 = n2 2π T  2.  Sometimes ωn is referred to as the eigenvalue of this equation, but we will avoid such confusing terminology here.   n = n2ω2  We may discuss a somewhat wider class of diﬀerential equations by considering  a slightly more general form of  17.2 , namely  Lyi x  = λiρ x yi x ,   17.5   where ρ x  is a weight function. In many applications ρ x  is unity for all x, in which case  17.2  is recovered; in general, though, it is a function determined by the choice of coordinate system used in describing a particular physical situation. The only requirement on ρ x  is that it is real and does not change sign in the  range a ≤ x ≤ b, so that it can, without loss of generality, be taken to be non-  negative throughout; of course, ρ x  must be the same function for all values of λi. A function yi x  that satisﬁes  17.5  is called an eigenfunction of the operator L with respect to the weight function ρ x .  This chapter will not cover methods used to determine the eigenfunctions of  17.2  or  17.5 , since we have discussed those in previous chapters, but, rather, will use the properties of the eigenfunctions to solve inhomogeneous equations of the form  17.1 . We shall see later that the sets of eigenfunctions yi x  of a particular  555   EIGENFUNCTION METHODS FOR DIFFERENTIAL EQUATIONS  class of operators called Hermitian operators  the operator in the simple harmonic oscillator equation is an example  have particularly useful properties and these will be studied in detail. It turns out that many of the interesting diﬀerential operators met within the physical sciences are Hermitian. Before continuing our discussion of the eigenfunctions of Hermitian operators, however, we will consider some properties of general sets of functions.  17.1 Sets of functions  In chapter 8 we discussed the deﬁnition of a vector space but concentrated on spaces of ﬁnite dimensionality. We consider now the inﬁnite-dimensional space of all reasonably well-behaved functions f x , g x , h x , . . . on the interval  a ≤ x ≤ b. That these functions form a linear vector space is shown by noting  the following properties. The set is closed under   i  addition, which is commutative and associative, i.e.  f x  + g x  = g x  + f x ,  [f x  + g x ] + h x  = f x  + [g x  + h x ] ,   ii  multiplication by a scalar, which is distributive and associative, i.e.  λ [f x  + g x ] = λf x  + λg x ,  λ [µf x ] =  λµ f x ,   λ + µ f x  = λf x  + µf x .  Furthermore, in such a space   iii  there exists a ‘null vector’ 0 such that f x  + 0 = f x ,   iv  multiplication by unity leaves any function unchanged, i.e. 1× f x  = f x ,  v  each function has an associated negative function −f x  that is such that  f x  + [−f x ] = 0.  By analogy with ﬁnite-dimensional vector spaces we now introduce a set  of linearly independent basis functions yn x , n = 0, 1, . . . ,∞, such that any ‘reasonable’ function in the interval a ≤ x ≤ b  i.e. it obeys the Dirichlet conditions  discussed in chapter 12  can be expressed as the linear sum of these functions:  Clearly if a diﬀerent set of linearly independent basis functions un x  is chosen then the function can be expressed in terms of the new basis,  f x  =  cnyn x .  f x  =  dnun x ,  ∞ cid:4   n=0  ∞ cid:4   n=0  556   17.1 SETS OF FUNCTIONS   cid:21    cid:20 fg cid:21  =  b  ∗  a  where the dn are a diﬀerent set of coeﬃcients. In each case, provided the basis functions are linearly independent, the coeﬃcients are unique.  We may also deﬁne an inner product on our function space by  f   x g x ρ x  dx,   17.6   where ρ x  is the weight function, which we require to be real and non-negative  in the interval a ≤ x ≤ b. As mentioned above, ρ x  is often unity for all x. Two  functions are said to be orthogonal  with respect to the weight function ρ x   on the interval [a, b] if  f   x g x ρ x  dx = 0,   17.7    cid:20 fg cid:21  =  b  ∗  a   cid:13  cid:21   b  ∗  a   cid:14 1 2   cid:13  cid:21   a  and the norm of a function is deﬁned as   cid:22 f cid:22  =  cid:20 ff cid:21 1 2 =  f   x f x ρ x  dx  =  b  f x 2ρ x  dx  .   17.8   It is also common practice to deﬁne a normalised function by ˆf = f  cid:22 f cid:22 , which   cid:14 1 2  has unit norm.  An inﬁnite-dimensional vector space of functions, for which an inner product is deﬁned, is called a Hilbert space. Using the concept of the inner product, we can choose a basis of linearly independent functions ˆφn x , n = 0, 1, 2, . . . that are orthonormal, i.e. such that   cid:20  ˆφi ˆφj cid:21  =  b  ∗ i  x  ˆφj x ρ x  dx = δij .  ˆφ   17.9    cid:21    cid:21   a  If yn x , n = 0, 1, 2, . . . , are a linearly independent, but not orthonormal, basis for the Hilbert space then an orthonormal set of basis functions ˆφn may be produced  in a similar manner to that used in the construction of a set of orthogonal eigenvectors of an Hermitian matrix; see chapter 8  by the following procedure:  φ0 = y0,  φ1 = y1 − ˆφ0 cid:20  ˆφ0y1 cid:21 , φ2 = y2 − ˆφ1 cid:20  ˆφ1y2 cid:21  − ˆφ0 cid:20  ˆφ0y2 cid:21 ,  φn = yn − ˆφn−1 cid:20  ˆφn−1yn cid:21  − ··· − ˆφ0 cid:20  ˆφ0yn cid:21 ,  ...  ...  It is straightforward to check that each φn is orthogonal to all its predecessors  φi, i = 0, 1, 2, . . . , n − 1. This method is called Gram–Schmidt orthogonalisation.  Clearly the functions φn form an orthogonal set, but in general they do not have unit norms.  557   EIGENFUNCTION METHODS FOR DIFFERENTIAL EQUATIONS   cid:1 Starting from the linearly independent functions yn x  = xn, n = 0, 1, . . . , construct three orthonormal functions over the range −1 < x < 1, assuming a weight function of unity.  The ﬁrst unnormalised function φ0 is simply equal to the ﬁrst of the original functions, i.e.  The normalisation is carried out by dividing by   cid:20 φ0φ0 cid:21 1 2 =   cid:8 1 2  √ 2,  =  φ0 = 1.   cid:7  cid:21   1  −1  1 × 1 du  cid:2   =  1 2 .  ˆφ0 =  φ0√ 2  φ1 = y1 − ˆφ0 cid:20  ˆφ0y1 cid:21 .  cid:7  cid:21   cid:8 −1 2  u × u du  1  −1   cid:2   =  3 2 x.  ˆφ1 = φ1  with the result that the ﬁrst normalised function ˆφ0 is given by  The second unnormalised function is found by applying the above Gram–Schmidt orthog- onalisation procedure, i.e.  It can easily be shown that  cid:20  ˆφ0y1 cid:21  = 0, and so φ1 = x. Normalising then gives  The third unnormalised function is similarly given by  which, on normalising, gives  φ2 = y2 − ˆφ1 cid:20  ˆφ1y2 cid:21  − ˆφ0 cid:20  ˆφ0y2 cid:21   cid:7  cid:21   cid:2   = x2 − 0 − 1 3 ,  cid:5   cid:6    cid:8 −1 2  1  u2 − 1  3  2  du  −1  2  3x2 − 1 .  5  = 1 2  ˆφ2 = φ2  By comparing the functions ˆφ0, ˆφ1 and ˆφ2 with the list in subsection 18.1.1, we see that this procedure has generated  multiples of  the ﬁrst three Legendre polynomials.  cid:2   If a function is expressed in terms of an orthonormal basis ˆφn x  as  f x  =  ˆφn x   cn  then the coeﬃcients cn are given by  cn =  cid:20  ˆφnf cid:21  =  b  ∗ n x f x ρ x  dx.  ˆφ  Note that this is true only if the basis is orthonormal.   17.10    17.11   ∞ cid:4   n=0   cid:21   a  558   17.2 ADJOINT, SELF-ADJOINT AND HERMITIAN OPERATORS  17.1.1 Some useful inequalities  Since for a Hilbert space  cid:20 ff cid:21  ≥ 0, the inequalities discussed in subsection 8.1.3  hold. The proofs are not repeated here, but the relationships are listed for completeness.   i  The Schwarz inequality states that   cid:20 fg cid:21  ≤  cid:20 ff cid:21 1 2 cid:20 gg cid:21 1 2,  where the equality holds when f x  is a scalar multiple of g x , i.e. when they are linearly dependent.   ii  The triangle inequality states that   cid:22 f + g cid:22  ≤  cid:22 f cid:22  +  cid:22 g cid:22 ,  where again equality holds when f x  is a scalar multiple of g x .   iii  Bessel’s inequality requires the introduction of an orthonormal basis ˆφn x   so that any function f x  can be written as  where cn =  cid:20  ˆφnf cid:21 . Bessel’s inequality then states that  f x  =  ∞ cid:4   cid:20 ff cid:21  ≥ cid:4   n=0  cn  ˆφn x ,  cn2.  n   17.12    17.13    17.14   The equality holds if the summation is over all the basis functions. If some values of n are omitted from the sum then the inequality results  unless, of course, the cn happen to be zero for all values of n omitted, in which case the equality remains .  17.2 Adjoint, self-adjoint and Hermitian operators  Having discussed general sets of functions, we now return to the discussion of eigenfunctions of linear operators. We begin by introducing the adjoint of an operator L, denoted by L†  , which is deﬁned by  b  ∗   x  [Lg x ] dx =  f  b  [L†  ∗ f x ]  g x  dx + boundary terms,   cid:21   a   cid:21   a   17.15   where the boundary terms are evaluated at the end-points of the interval [a, b]. Thus, for any given linear diﬀerential operator L, the adjoint operator L† can be found by repeated integration by parts. = L. If, in addition, certain boundary conditions are met by the functions f and g on which a self-adjoint operator acts,  An operator is said to be self-adjoint if L†  559   EIGENFUNCTION METHODS FOR DIFFERENTIAL EQUATIONS  or by the operator itself, such that the boundary terms in  17.15  vanish, then the  operator is said to be Hermitian over the interval a ≤ x ≤ b. Thus, in this case,  b  ∗   x  [Lg x ] dx =  f  b  ∗ [Lf x ]  g x  dx.   17.16   A little careful study will reveal the similarity between the deﬁnition of an Hermitian operator and the deﬁnition of an Hermitian matrix given in chapter 8.  cid:1 Show that the linear operator L = d2 dt2 is self-adjoint, and determine the required boundary conditions for the operator to be Hermitian over the interval t0 to t0 + T .  Substituting into the LHS of the deﬁnition of the adjoint operator  17.15  and integrating by parts gives   cid:21   a   cid:21   t0+T  ∗ d2g dt2 dt =  f  ∗ dg dt  f  t0  t0+T  t0+T  −  dg dt  dt.  Integrating the second term on the RHS by parts once more yields  t0+T  ∗ d2g dt2 dt =  f  ∗ dg dt  f  t0  t0+T  t0  +  t0+T  t0+T  ∗  g  d2f dt2 dt,  which, by comparison with  17.15 , proves that L is a self-adjoint operator. Moreover, from  17.16 , we see that L is an Hermitian operator over the required interval provided  t0  t0  ∗  − df  dt  g   cid:14    cid:13    cid:14    cid:14    cid:13    cid:13    cid:21   cid:14   t0  ∗  df dt   cid:21   +   cid:21   a   cid:14   cid:13   t0   cid:13    cid:21   ∗ dg dt  f  t0+T  t0  =  ∗  df dt  g  t0+T  .  cid:2   t0  We showed in chapter 8 that the eigenvalues of Hermitian matrices are real and that their eigenvectors can be chosen to be orthogonal. Similarly, the eigenvalues of Hermitian operators are real and their eigenfunctions can be chosen to be orthogonal  we will prove these properties in the following section . Hermitian operators  or matrices  are often used in the formulation of quantum mechanics. The eigenvalues then give the possible measured values of an observable quantity such as energy or angular momentum, and the physical requirement that such quantities must be real is ensured by the reality of these eigenvalues. Furthermore, the inﬁnite set of eigenfunctions of an Hermitian operator form a complete basis set over the relevant interval, so that it is possible to expand any function y x  obeying the appropriate conditions in an eigenfunction series over this interval:  y x  =  cnyn x ,   17.17   where the choice of suitable values for the cn will make the sum arbitrarily close § These useful properties provide the motivation for a detailed study of to y x . Hermitian operators.  §  The proof of the completeness of the eigenfunctions of an Hermitian operator is beyond the scope of this book. The reader should refer, for example, to R. Courant and D. Hilbert, Methods of Mathematical Physics  New York: Interscience, 1953 .  ∞ cid:4   n=0  560   17.3 PROPERTIES OF HERMITIAN OPERATORS  17.3 Properties of Hermitian operators  We now provide proofs of some of the useful properties of Hermitian operators. Again much of the analysis is similar to that for Hermitian matrices in chapter 8, although the present section stands alone.  Here, and throughout the remainder of this chapter, we will write out inner products in full. We note, however, that the inner product notation often provides a neat form in which to express results.   17.3.1 Reality of the eigenvalues  Consider an Hermitian operator for which  17.5  is satisﬁed by at least two eigenfunctions yi x  and yj x , which have corresponding eigenvalues λi and λj, so that  where we have allowed for the presence of a weight function ρ x . Multiplying  17.18  by y  ∗ j and  17.19  by y  Lyi = λiρ x yi, Lyj = λjρ x yj,  cid:21   cid:21   ∗ i and then integrating gives ∗ Lyi dx = λi Lyj dx = λj  ∗ y j yiρ dx,  ∗ y i yjρ dx.  ∗  b  a  b  y j  y i  b  ∗ yj Lyi   ∗ dx = λ i  b  ∗ y i yjρ dx,  a   cid:21   a   cid:21    cid:21   cid:21   b  a  b  a   cid:21   a  Remembering that we have required ρ x  to be real, the complex conjugate of  17.20  becomes  and using the deﬁnition of an Hermitian operator  17.16  it follows that the LHS of  17.22  is equal to the LHS of  17.21 . Thus  ∗  λ i  b  ∗ i yjρ dx = 0. y  − λj   cid:1  i yiρ dx  cid:3 = 0 , which is a statement that the ∗   17.23   b a y  a   since  ∗ If i = j then λi = λ i eigenvalue λi is real.  17.3.2 Orthogonality and normalisation of the eigenfunctions  From  17.23 , it is immediately apparent that two eigenfunctions yi and yj that  correspond to diﬀerent eigenvalues, i.e. such that λi  cid:3 = λj, satisfy   cid:21   a  b  ∗ i yjρ dx = 0, y  561   17.18    17.19    17.20    17.21    17.22    17.24    EIGENFUNCTION METHODS FOR DIFFERENTIAL EQUATIONS  which is a statement of the orthogonality of yi and yj.  If one  or more  of the eigenvalues is degenerate, however, we have diﬀerent eigenfunctions corresponding to the same eigenvalue, and the proof of orthogo- nality is not so straightforward. Nevertheless, an orthogonal set of eigenfunctions may be constructed using the Gram–Schmidt orthogonalisation method mentioned earlier in this chapter and used in chapter 8 to construct a set of orthogonal eigenvectors of an Hermitian matrix. We repeat the analysis here for complete- ness.  Suppose, for the sake of our proof, that λ0 is k-fold degenerate, i.e.  but that λ0 is diﬀerent from any of λk, λk+1, etc. Then any linear combination of these yi is also an eigenfunction with eigenvalue λ0 since  Lyi = λ0ρyi  for i = 0, 1, . . . , k − 1, k−1 cid:4   k−1 cid:4   Lz ≡ L k−1 cid:4   ciyi =  ciLyi =  i=0  i=0  i=0  ciλ0ρyi = λ0ρz.   17.26    17.25   If the yi deﬁned in  17.25  are not already mutually orthogonal then consider the new eigenfunctions zi constructed by the following procedure, in which each of the new functions zi is to be normalised, to give ˆzi, before proceeding to the construction of the next one  the normalisation can be carried out by dividing the eigenfunction zi by    ∗ i ziρ dx 1 2 :  b a z   cid:1   cid:21   cid:21   a  a   cid:7   cid:7   ˆz0  ˆz1   cid:7   z0 = y0,  z1 = y1 − z2 = y2 −  ...   cid:8   cid:8   b  b  ∗ 0y1ρ dx  ˆz  ∗ 1y2ρ dx  ˆz   cid:7   ,  −   cid:21   a   cid:21   cid:8   b  ∗ 0y2ρ dx  ˆz  ˆz0  a   cid:8    cid:7   ,   cid:21   ˆz0  a  zk−1 = yk−1 −  ˆzk−2  b  ∗ k−2yk−1ρ dx  ˆz  − ··· −  b  ∗ 0yk−1ρ dx  ˆz  .  Each of the integrals is just a number and thus each new function zi is, as can be shown from  17.26 , an eigenvector of L with eigenvalue λ0. It is straightforward to check that each zi is orthogonal to all its predecessors. Thus, by this explicit construction we have shown that an orthogonal set of eigenfunctions of an Hermitian operator L can be obtained. Clearly the orthogonal set obtained, zi, is In general, since L is linear, the normalisation of its eigenfunctions yi x  is  not unique.  arbitrary. It is often convenient, however, to work in terms of the normalised ∗ i ˆyiρ dx = 1. These therefore form an orthonormal eigenfunctions ˆyi x , so that  b a ˆy   cid:1   562   cid:8    17.3 PROPERTIES OF HERMITIAN OPERATORS   cid:21   a  n   cid:4   cid:21   cid:4   b  a  n  set and we can write  b  ∗ ˆy i ˆyjρ dx = δij,   17.27   which is valid for all pairs of values i, j.  17.3.3 Completeness of the eigenfunctions  As noted earlier, the eigenfunctions of an Hermitian operator may be shown to form a complete basis set over the relevant interval. One may thus expand any  reasonable  function y x  obeying appropriate boundary conditions in an eigenfunction series over the interval, as in  17.17 . Working in terms of the normalised eigenfunctions ˆyn x , we may thus write   cid:21   b   cid:4   a  n  f x  =  ˆyn x   ∗ ˆy n z f z ρ z  dz  =  f z ρ z   ˆyn x  ˆy  ∗ n z  dz.  Since this is true for any f x , we must have that  ρ z   ˆyn x  ˆy  n z  = δ x − z . ∗   17.28   This is called the completeness or closure property of the eigenfunctions. It deﬁnes a complete set. If the spectrum of eigenvalues of L is anywhere continuous then the eigenfunction yn x  must be treated as y n, x  and an integration carried out over n.  We also note that the RHS of  17.28  is a δ-function and so is only non-zero  when z = x; thus ρ z  on the LHS can be replaced by ρ x  if required, i.e.  ρ z   ˆyn x  ˆy  ∗ n z  = ρ x   ∗ n z .  ˆyn x  ˆy   17.29    cid:4   n   cid:4   n  17.3.4 Construction of real eigenfunctions  Recall that the eigenfunction yi satisﬁes  Lyi = λiρyi   17.30   and that the complex conjugate of this gives  Ly  ∗ ∗ i = λ i ρy  ∗ i = λiρy  ∗ i ,   17.31  ∗ where the last equality follows because the eigenvalues are real, i.e. λi = λ i . ∗ i are eigenfunctions corresponding to the same eigenvalue and Thus, yi and y − yi , which hence, because of the linearity of L, at least one of y  ∗ i + yi and i y i  ∗  563   EIGENFUNCTION METHODS FOR DIFFERENTIAL EQUATIONS  are both real, is a non-zero eigenfunction corresponding to that eigenvalue. It follows that the eigenfunctions can always be made real by taking suitable linear combinations, though taking such linear combinations will only be necessary in cases where a particular λ is degenerate, i.e. corresponds to more than one linearly independent eigenfunction.  17.4 Sturm–Liouville equations  One of the most important applications of our discussion of Hermitian operators is to the study of Sturm–Liouville equations, which take the general form  p x   d2y dx2 + r x   dy dx  + q x y + λρ x y = 0,  where r x  =   17.32   § and p, q and r are real functions of x. A variational approach to the Sturm– Liouville equation, which is useful in estimating the eigenvalues λ for a given set of boundary conditions on y, is discussed in chapter 22. For now, however, we concentrate on demonstrating that solutions of the Sturm–Liouville equation that satisfy appropriate boundary conditions are the eigenfunctions of an Hermitian operator.  It is clear that  17.32  can be written  dp x   dx   cid:14   Ly = λρ x y,  where L ≡ −  p x   d2 dx2 + r x   d dx  + q x   .   17.33    cid:7  Using the condition that r x  = p Liouville equation  17.32  can also be rewritten as   x , it will be seen that the general Sturm–   cid:13    py  + qy + λρy = 0,   17.34   where primes denote diﬀerentiation with respect to x. Using  17.33  this may also   cid:7  − qy = λρy, which deﬁnes a more useful form for the  be written Ly ≡ − py      cid:7   Sturm–Liouville linear operator, namely   cid:7    cid:7      cid:13    cid:7   L ≡ −  d dx   cid:14    cid:8   d dx  p x   + q x   .   17.35   17.4.1 Hermitian nature of the Sturm–Liouville operator  As we now show, the linear operator of the Sturm–Liouville equation  17.35  is self-adjoint. Moreover, the operator is Hermitian over the range [a, b] provided  §  We note that sign conventions vary in this expression for the general Sturm–Liouville equation;  some authors use −λρ x y on the LHS of  17.32 .  564   17.4 STURM–LIOUVILLE EQUATIONS  certain boundary conditions are met, namely that any two eigenfunctions yi and   cid:19   yj of  17.33  must satisfy cid:18  Rearranging  17.36 , we can write cid:22   ∗ i py y j  x=a   cid:7   =   cid:18    cid:19    cid:23    cid:7   ∗ y i py j  x=b   cid:7   ∗ y i py j  x=b  x=a  = 0  for all i, j.   17.36    17.37    cid:7   as an equivalent statement of the required boundary conditions. These boundary conditions are in fact not too restrictive and are met, for instance, by the sets  b  = 0; p a  = p b  = 0 and by many other sets. It y a  = y b  = 0; y a  = y is important to note that in order to satisfy  17.36  and  17.37  one boundary condition must be speciﬁed at each end of the range.  cid:1 Prove that the Sturm–Liouville operator is Hermitian over the range [a, b] and under the boundary conditions  17.37 .  Hermitian operator, the LHS may be written as a sum of two terms, i.e.  The boundary-value term in this is zero because of the boundary conditions, and so   cid:7    cid:7  − qy into the deﬁnition  17.16  of an      cid:21   a  b   cid:7   ∗ i  py   cid:7  j    y  dx −  b  ∗ i qyj dx.  y  Putting the Sturm–Liouville form Ly = − py  cid:21    cid:21    cid:18   b   cid:19   −   cid:7   ∗ i  py   cid:7  j   y  ∗ i qyj  + y  a The ﬁrst term may be integrated by parts to give  a  dx = −  cid:14   cid:21    cid:13   − integrating by parts again yields cid:13   ∗ i py   cid:7   j  y  b  a  +  b   cid:7   ∗ i     y   cid:7  j dx.  py   cid:7   ∗ i     y  pyj  −  b  a   cid:7    cid:7   ∗ i      y  p   yj dx.   cid:14   cid:19    cid:21   a  b  a   cid:18    cid:21   b  a   cid:21    cid:18   −  b  a   cid:19   ∗  i  Again, the boundary-value term is zero, leaving us with   cid:7   ∗ i  py   cid:7  j   y  ∗ i qyj  + y  dx = −  yj p y  + yjqy  dx,   cid:7   ∗ i     cid:7      which proves that the Sturm–Liouville operator is Hermitian over the prescribed interval.  cid:2   It is also worth noting that, since p a  = p b  = 0 is a valid set of boundary conditions, many Sturm–Liouville equations possess a ‘natural’ interval [a, b] over which the corresponding diﬀerential operator L is Hermitian irrespective of the boundary conditions satisﬁed by its eigenfunctions at x = a and x = b  the only requirement being that they are regular at these end-points .  17.4.2 Transforming an equation into Sturm–Liouville form  Many of the second-order diﬀerential equations encountered in physical problems are examples of the Sturm–Liouville equation  17.34 . Moreover, any second-order  565   EIGENFUNCTION METHODS FOR DIFFERENTIAL EQUATIONS  Equation Hypergeometric Legendre Associated Legendre Chebyshev Conﬂuent hypergeometric Bessel Laguerre Associated Laguerre Hermite Simple harmonic  ∗  1 − x2 1 − x2  1 − x2 1 2 −x xce x −x xe −x xm+1e −x2 e 1  p x   xc 1 − x a+b−c+1  q x   λ  −ab  ρ x   xc−1 1 − x a+b−c  −m2  1 − x2    cid:2   cid:2  + 1   cid:2   cid:2  + 1   −ν2 x  0 0  0 0  0 0 0 0  1 1   1 − x2  −1 2 xc−1e −x x −x e −x xme −x2 e 1  ν2 −a α2 ν ν 2ν ω2  Table 17.1 The Sturm–Liouville form  17.34  for important ODEs in the physical sciences and engineering. The asterisk denotes that, for Bessel’s equa-  tion, a change of variable x → x a is required to give the conventional  normalisation used here, but is not needed for the transformation into Sturm– Liouville form.   cid:7  cid:7    cid:7    cid:7  ]  diﬀerential equation of the form  p x y  + r x y  + q x y + λρ x y = 0   17.38   can be converted into Sturm–Liouville form by multiplying through by a suitable integrating factor, which is given by   cid:7    cid:12  cid:21    cid:15   x r u  − p  cid:7    u   du  .  p u   F x  = exp   17.39   It is easily veriﬁed that  17.38  then takes the Sturm–Liouville form,  [F x p x y  + F x q x y + λF x ρ x y = 0,   17.40   with a diﬀerent, but still non-negative, weight function F x ρ x . Table 17.1 summarises the Sturm–Liouville form  17.34  for several of the equations listed in table 16.1. These forms can be determined using  17.39 , as illustrated in the following example.  cid:1 Put the following equations into Sturm–Liouville  SL  form:  Chebyshev equation ;  Laguerre equation ;  Hermite equation .   1 − x2 y  cid:7  cid:7  − xy +  1 − x y  cid:7  cid:7   cid:7   cid:7  cid:7  − 2xy  cid:7   + ν2y = 0 + νy = 0   i   ii   iii   + 2νy = 0  xy  y   cid:7    cid:7  cid:21    cid:8    i  From  17.39 , the required integrating factor is  x  u  F x  = exp  1 − u2 du  Thus, the Chebyshev equation becomes  =  1 − x2   cid:7  cid:19  cid:7   1 − x2 1 2y which is in SL form with p x  =  1 − x2 1 2, q x  = 0, ρ x  =  1 − x2    cid:7  cid:7  − x 1 − x2   + ν2 1 − x2    1 − x2 1 2y  −1 2y =  −1 2y  = exp   cid:7   −1 2.  + ν2 1 − x2  −1 2 and λ = ν2.  −1 2y = 0,   cid:19    cid:18 − 1 2 ln 1 − x2   cid:18   566   17.4 STURM–LIOUVILLE EQUATIONS   ii  From  17.39 , the required integrating factor is   cid:7  cid:21    cid:8   F x  = exp  x −1 du  = exp −x .   cid:7    cid:7  cid:7   xe  −xy  +  1 − x e  Thus, the Laguerre equation becomes −xy −xy −x, q x  = 0, ρ x  = e  cid:7  cid:21    iii  From  17.39 , the required integrating factor is  which is in SL form with p x  = xe  −xy =  xe   cid:8   + νe   cid:7    cid:7      + νe  −xy = 0, −x and λ = ν.  F x  = exp  x −2u du  = exp −x2 .  Thus, the Hermite equation becomes  −x2 e   cid:7  cid:7  − 2xe  y  −x2   cid:7   −x2  −x2   cid:7    cid:7   −x2  y  + 2νe  y =  e  y     + 2νe  y = 0,  which is in SL form with p x  = e  −x2 , q x  = 0, ρ x  = e  −x2 and λ = 2ν.  cid:2   From the p x  entries in table 17.1, we may read oﬀ the natural interval over which the corresponding Sturm–Liouville operator  17.35  is Hermitian; in each case this is given by [a, b], where p a  = p b  = 0. Thus, the natural interval for the Legendre equation, the associated Legendre equation and the Chebyshev  equation is [−1, 1]; for the Laguerre and associated Laguerre equations the interval is [0,∞]; and for the Hermite equation it is [−∞,∞]. In addition, from   17.37 , one sees that for the simple harmonic equation one requires only that [a, b] = [x0, x0 + 2π]. We also note that, as required, the weight function in each case is ﬁnite and non-negative over the natural interval. Occasionally, a little more care is required when determining the conditions for a Sturm–Liouville operator of the form  17.35  to be Hermitian over some natural interval, as is illustrated in the following example.   cid:1 Express the hypergeometric equation,  x 1 − x y   cid:7  cid:7   + [ c −  a + b + 1 x ]y   cid:7  − aby = 0,  in Sturm–Liouville form. Hence determine the natural interval over which the resulting Sturm–Liouville operator is Hermitian and the corresponding conditions that one must im- pose on the parameters a, b and c.  As usual for an equation not already in SL form, we ﬁrst determine the appropriate  567   EIGENFUNCTION METHODS FOR DIFFERENTIAL EQUATIONS  integrating factor. This is given, as in equation  17.39 , by   cid:13  cid:21   cid:13  cid:21   cid:13  cid:21   F x  = exp  = exp  x c −  a + b + 1 u − 1 + 2u  cid:14   u 1 − u   x c − 1 −  a + b − 1 u  cid:7   u 1 − u  c − 1  du   cid:14   du   cid:8  cid:14   x  +  = exp  c − 1 1 − u  − a + b − 1 1 − u = exp [  a + b − c  ln 1 − x  +  c − 1  ln x ] = xc−1 1 − x a+b−c.  u  du   cid:7  cid:19  cid:7  − abxc−1 1 − x a+b−cy = 0.  xc 1 − x a+b−c+1y   cid:18   When the equation is multiplied through by F x  it takes the form  Now, for the corresponding Sturm–Liouville operator to be Hermitian, the conditions  to be imposed are as follows.   i  The boundary condition  17.37 ; if c > 0 and a + b − c + 1 > 0, this is satisﬁed automatically for 0 ≤ x ≤ 1, which is thus the natural interval in this case.  ii  The weight function xc−1 1 − x a+b−c must be ﬁnite and not change sign in the interval 0 ≤ x ≤ 1. This means that both exponents in it must be positive, i.e. c − 1 > 0 and a + b − c > 0.  Putting together the conditions on the parameters gives the double inequality a + b > c > 1.  cid:2   Finally, we consider Bessel’s equation,   cid:7  cid:7   x2y  + xy   cid:7   +  x2 − ν2 y = 0,  which may be converted into Sturm–Liouville form, but only in a somewhat unorthodox fashion. It is conventional ﬁrst to divide the Bessel equation by x and then to change variables to ¯x = x α. In this case, it becomes   cid:7  cid:7   ¯xy   α¯x  + y   cid:7    α¯x  − ν2  ¯x  y α¯x  + α2¯xy α¯x  = 0,   17.41   where a prime now indicates diﬀerentiation with respect to ¯x. Dropping the bars on the independent variable, we thus have   cid:7   [xy   αx ]   cid:7  − ν2  x  y αx  + α2xy αx  = 0,   17.42   which is in SL form with p x  = x, q x  = −ν2 x, ρ x  = x and λ = α2. It  should be noted, however, that in this case the eigenvalue  actually its square root  appears in the argument of the dependent variable.  568   17.5 SUPERPOSITION OF EIGENFUNCTIONS: GREEN’S FUNCTIONS  17.5 Superposition of eigenfunctions: Green’s functions  We have already seen that if  Lyn x  = λnρ x yn x ,   17.43   where L is an Hermitian operator, then the eigenvalues λn are real and the eigenfunctions yn x  are orthogonal  or can be made so . Let us assume that we know the eigenfunctions yn x  of L that individually satisfy  17.43  and some imposed boundary conditions  for which L is Hermitian .  Now let us suppose we wish to solve the inhomogeneous diﬀerential equation  Ly x  = f x ,   17.44  subject to the same boundary conditions. Since the eigenfunctions of L form a complete set, the full solution, y x , to  17.44  may be written as a superposition of eigenfunctions, i.e.  y x  =  cnyn x ,   17.45   for some choice of the constants cn. Making full use of the linearity of L, we have  f x  = Ly x  = L  cnyn x   =  cnLyn x  =  cnλnρ x yn x .  ∞ cid:4   n=0  Multiplying the ﬁrst and last terms of  17.46  by y  ∗ j and integrating, we obtain   cid:30  ∞ cid:4   n=0  b  ∗ j  z f z  dz = y  b  ∗ j  z yn z ρ z  dz,  cnλny   cid:21   a  where we have used z as the integration variable for later convenience. Finally, using the orthogonality condition  17.27 , we see that the integrals on the RHS are zero unless n = j, and so obtain   17.46    17.47    17.48   cn =  1 λn  ∗ n z f z  dz  b a y ∗ n z yn z ρ z  dz  .  b a y  Thus, if we can ﬁnd all the eigenfunctions of a diﬀerential operator then  17.48  can be used to ﬁnd the weighting coeﬃcients for the superposition, to give as the full solution  ∗ n z f z  dz  b a y ∗ n z yn z ρ z  dz  b a y  1 λn  yn x .   17.49   ∞ cid:4   cid:31   n=0   cid:21   ∞ cid:4   n=0  a  ∞ cid:4   n=0   cid:1    cid:1    cid:1    cid:1   569  If we work with normalised eigenfunctions ˆyn x , so that  b  ∗ ˆy n z  ˆyn z ρ z  dz = 1  for all n,  ∞ cid:4   n=0  y x  =   cid:21   a   EIGENFUNCTION METHODS FOR DIFFERENTIAL EQUATIONS   ∞ cid:4    cid:13    cid:21   b  a  n=0   cid:21   b  a  ∞ cid:4   n=0  and we assume that we may interchange the order of summation and integration, then  17.49  can be written as   cid:14 .  y x  =  ∗ n z   ˆyn x  ˆy  1 λn  f z  dz.  The quantity in braces, which is a function of x and z only, is usually written G x, z , and is the Green’s function for the problem. With this notation,  y x  =  G x, z f z  dz,   17.50   where  G x, z  =  ∗ n z .  ˆyn x  ˆy  1 λn   17.51   We note that G x, z  is determined entirely by the boundary conditions and the eigenfunctions ˆyn, and hence by L itself, and that f z  depends purely on the RHS of the inhomogeneous equation  17.44 . Thus, for a given L and boundary conditions we can establish, once and for all, a function G x, z  that will enable us to solve the inhomogeneous equation for any RHS. From  17.51  we also note that  ∗ G x, z  = G   z, x .   17.52   We have already met the Green’s function in the solution of second-order dif- ferential equations in chapter 15, as the function that satisﬁes the equation  L[G x, z ] = δ x − z   and the boundary conditions . The formulation given  above is an alternative, though equivalent, one.  cid:1 Find an appropriate Green’s function for the equation   cid:7  cid:7   y  + 1  4 y = f x ,  with boundary conditions y 0  = y π  = 0. Hence, solve for  i  f x  = sin 2x and  ii  f x  = x 2.  One approach to solving this problem is to use the methods of chapter 15 and ﬁnd a complementary function and particular integral. However, in order to illustrate the techniques developed in the present chapter we will use the superposition of eigenfunctions, which, as may easily be checked, produces the same solution.  The operator on the LHS of this equation is already Hermitian under the given boundary  conditions, and so we seek its eigenfunctions. These satisfy the equation  This equation has the familiar solution   cid:7  cid:7   y  + 1  4 y = λy.   cid:7  cid:2    cid:8   − λ  1 4   cid:7  cid:2    cid:8   − λ  1 4  x.  y x  = A sin  x + B cos  570   17.5 SUPERPOSITION OF EIGENFUNCTIONS: GREEN’S FUNCTIONS   cid:10    cid:9  cid:2  − λ where n = 0,±1,±2, . . . .  1 4   cid:2   − λ = n,  1 4  Now, the boundary conditions require that B = 0 and sin  π = 0, and so  Therefore, the independent eigenfunctions that satisfy the boundary conditions are  where n is any non-negative integer, and the corresponding eigenvalues are λn = 1 4 The normalisation condition further requires  − n2.  yn x  = An sin nx,  Comparison with  17.51  shows that the appropriate Green’s function is therefore given  by  Thus   cid:7    cid:8   1 2  .  2 π   cid:21   0   cid:21    cid:30  ∞ cid:4   π  n sin2 nx dx = 1 ⇒ An = A2  ∞ cid:4   n=0  2 π  G x, z  =  sin nx sin nz  .  1 4  − n2 ∞ cid:4    cid:31    cid:21   π  0  sin2 2z dz =  π 2  .  y x  = − 2  sin 2x 15 4  π 2  π  = − 4  15  sin 2x   cid:21    cid:21   Case  i . Using  17.50 , the solution with f x  = sin 2x is given by  y x  =  2 π  π  0  sin nx sin nz  − n2  1 4  n=0  sin 2z dz =  sin nz sin 2z dz.  2 π  n=0  sin nx 1 4  − n2  π  0  Now the integral is zero unless n = 2, in which case it is  is the full solution for f x  = sin 2x. This is, of course, exactly the solution found by using the methods of chapter 15.  Case  ii . The solution with f x  = x 2 is given by   cid:31   π  y x  =  − n2 The integral may be evaluated by integrating by parts. For n  cid:3 = 0,  dz =  n=0  n=0  1 4  0  sin nx 1 4  sin nx sin nz  1 π  2 π  z 2  π  0  − n2  cid:16   z sin nz dz.   cid:21   ∞ cid:4    cid:30   cid:21   π  0  ∞ cid:4   cid:21    cid:17   cid:13   0  π  +  π   cid:14   0  cos nz  n  dz  sin nz  n2  π  0  z sin nz dz =  +  n  =  − z cos nz −π cos nπ = − π −1 n ∞ cid:4   n  n  .   −1 n+1   cid:5   y x  =  n=1   cid:6  ,  sin nx 1 4  − n2  n  For n = 0 the integral is zero, and thus  is the full solution for f x  = x 2. Using the methods of subsection 15.1.2, the solution  is found to be y x  = 2x − 2π sin x 2 , which may be shown to be equal to the above solution by expanding 2x − 2π sin x 2  as a Fourier sine series.  cid:2   571    17.54   EIGENFUNCTION METHODS FOR DIFFERENTIAL EQUATIONS  17.6 A useful generalisation  Sometimes we encounter inhomogeneous equations of a form slightly more gen- eral than  17.1 , given by  Ly x  − µρ x y x  = f x    17.53   for some Hermitian operator L, with y subject to the appropriate boundary conditions and µ a given  i.e. ﬁxed  constant. To solve this equation we expand y x  and f x  in terms of the eigenfunctions yn x  of the operator L, which satisfy  Working in terms of the normalised eigenfunctions ˆyn x , we ﬁrst expand f x   as follows:  ˆyn x   b  ∗ ˆy n z f z  dz.   17.55   Next, we expand y x  as y = this and  17.55  into  17.53  we have  n=0 cn ˆyn x  and seek the coeﬃcients cn. Substituting  b  ˆyn x   Lyn x  = λnρ x yn x . ∞ cid:4   cid:21   cid:21    cid:21  ∞ cid:4  ∞ cid:4   ˆyn x  ˆy  ρ z   n=0  n=0  b  a  a  b  ∗ ˆy n z f z ρ z  dz  ∗ n z f z  dz.  ∗ n z f z  dz  ˆyn x  ˆy  f x  =  =  f x  =  a  n=0  ρ x   ∞ cid:4   n=0  = ρ x    cid:11 ∞   cid:21   a  ∞ cid:4   n=0  ˆyn x   b  ∗ ˆy n z f z  dz,   λn − µ cn ˆyn x  = ρ x   cid:1   cn =  b a ˆy  ∗ n z f z  dz λn − µ   cid:21   a  .   cid:21   ∞ cid:4   b  a  n=0  ∞ cid:4   cid:21   n=0  b  a  Using  17.29  this becomes  ∞ cid:4   n=0  ρ x   from which we ﬁnd that  ∞ cid:4   n=0  Hence the solution of  17.53  is given by  y =  cn ˆyn x  =  ˆyn x   λn − µ  ∗ ˆy n z f z  dz =  From this we may identify the Green’s function  ∞ cid:4   n=0  G x, z  =  ˆyn x  ˆy  ∗ n z  λn − µ  .  ∞ cid:4   n=0  572  ˆyn x  ˆy  ∗ n z  λn − µ  f z  dz.   We note that if µ = λn, i.e. if µ equals one of the eigenvalues of L, then G x, z   becomes inﬁnite and this method runs into diﬃculty. No solution then exists unless the RHS of  17.53  satisﬁes the relation  If the spectrum of eigenvalues of the operator L is anywhere continuous, the orthonormality and closure relationships of the normalised eigenfunctions become  17.7 EXERCISES   cid:21   a  b  ∗ ˆy n x f x  dx = 0.   cid:21   cid:21  ∞  a  b  0  ˆy  n x  ˆym x ρ x  dx = δ n − m , ∗ n z  ˆyn x ρ x  dn = δ x − z . ∗  ˆy   cid:21  ∞  0  G x, z  =  ˆyn x  ˆy  ∗ n z  λn − µ  dn.  Repeating the above analysis we then ﬁnd that the Green’s function is given by  17.1  By considering  cid:20 hh cid:21 , where h = f + λg with λ real, prove that, for two functions  17.7 Exercises  f and g,  The function y x  is real and positive for all x. Its Fourier cosine transform ˜yc k  is deﬁned by  4 [ cid:20 fg cid:21  +  cid:20 gf cid:21 ]2.   cid:20 ff cid:21  cid:20 gg cid:21  ≥ 1  cid:21  ∞  ˜yc k  =  −∞ y x  cos kx  dx,  ˜yc 2k  ≥ 2[˜yc k ]2 − 1.  and it is given that ˜yc 0  = 1. Prove that  17.2  Write the homogeneous Sturm-Liouville eigenvalue equation for which y a  = y b  = 0 as  L y; λ  ≡  py   cid:7    cid:7      + qy + λρy = 0,  where p x , q x  and ρ x  are continuously diﬀerentiable functions. Show that if z x  and F x  satisfy L z; λ  = F x , with z a  = z b  = 0, then   cid:21   b  a  y x F x  dx = 0.  Demonstrate the validity of this general result by direct calculation for the  speciﬁc case in which p x  = ρ x  = 1, q x  = 0, a = −1, b = 1 and z x  = 1− x2.  17.3  Consider the real eigenfunctions yn x  of a Sturm–Liouville equation,   cid:7    cid:7    py     + qy + λρy = 0,  a ≤ x ≤ b,  in which p x , q x  and ρ x  are continuously diﬀerentiable real functions and  p x  does not change sign in a ≤ x ≤ b. Take p x  as positive throughout the  573   EIGENFUNCTION METHODS FOR DIFFERENTIAL EQUATIONS  interval, if necessary by changing the signs of all eigenvalues. For a ≤ x1 ≤ x2 ≤ b,  establish the identity   cid:18    cid:19    λn − λm   ρynym dx =  yn p y  m   cid:7   − ym p y   cid:7   n  x2 x1  .   cid:21   x2  x1  Deduce that if λn > λm then yn x  must change sign between two successive zeros of ym x . [ The reader may ﬁnd it helpful to illustrate this result by sketching the ﬁrst few eigenfunctions of the system y + λy = 0, with y 0  = y π  = 0, and the Legendre polynomials Pn z  for n = 2, 3, 4, 5. ] Show that the equation   cid:7  cid:7   with y ±π  = 0 and a real, has a set of eigenvalues λ satisfying   cid:7  cid:7   y  + aδ x y + λy = 0,  √ λ  =  tan π  √  a  2  λ  .  Investigate the conditions under which negative eigenvalues, λ = −µ2, with µ  real, are possible. Use the properties of Legendre polynomials to carry out the following exercises.   a  Find the solution of  1 − x2 y   cid:7  cid:7  − 2xy   cid:7   −1 ≤ x ≤ 1 and ﬁnite at x = 0, in terms of Legendre polynomials.  + by = f x , valid in the range   b  If b = 14 and f x  = 5x3, ﬁnd the explicit solution and verify it by direct  substitution.  [ The ﬁrst six Legendre polynomials are listed in Subsection 18.1.1. ] Starting from the linearly independent functions 1, x, x2, x3, . . . , in the range 0 ≤ x < ∞, ﬁnd the ﬁrst three orthogonal functions φ0, φ1 and φ2, with respect −x. By comparing your answers with the Laguerre to the weight function ρ x  = e polynomials generated by the recurrence relation  18.115 , deduce the form of φ3 x .  Consider the set of functions, {f x }, of the real variable x, deﬁned in the interval −∞ < x < ∞, that → 0 at least as quickly as x −1 as x → ±∞. For unit weight when acting upon {f x }:  function, determine whether each of the following linear operators is Hermitian   a   + x;  d dx   b  − i  d dx  + x2;   c  ix  d dx  ;   d  i  d3 dx3 .  A particle moves in a parabolic potential in which its natural angular frequency of oscillation is 1 is then suddenly subjected to an additional acceleration, of +1 for 0 ≤ t ≤ π 2, 2 . At time t = 0 it passes through the origin with velocity v. It followed by −1 for π 2 < t ≤ π. At the end of this period it is again at the  origin. Apply the results of the worked example in section 17.5 to show that  ∞ cid:4   v = − 8  1   4m + 2 2 − 1  4  π  m=0  ≈ −0.81.  17.4  17.5  17.6  17.7  17.8  17.9  Find an eigenfunction expansion for the solution, with boundary conditions y 0  = y π  = 0, of the inhomogeneous equation  + κy = f x ,  d2y dx2  574      cid:12   17.7 EXERCISES  where κ is a constant and  f x  =  x  0 ≤ x ≤ π 2, π − x π 2 < x ≤ π.  17.10  Consider the following two approaches to constructing a Green’s function.   a  Find those eigenfunctions yn x  of the self-adjoint linear diﬀerential operator d2 dx2 that satisfy the boundary conditions yn 0  = yn π  = 0, and hence construct its Green’s function G x, z .   b  Construct the same Green’s function using a method based on the comple- mentary function of the appropriate diﬀerential equation and the boundary conditions to be satisﬁed at the position of the δ-function, showing that it is  G x, z  =  x z − π  π z x − π  π  0 ≤ x ≤ z, z ≤ x ≤ π.  verify that it is the same function as that derived in  a .   c  By expanding the function given in  b  in terms of the eigenfunctions yn x ,  cid:7  The diﬀerential operator L is deﬁned by   cid:8   17.11  Determine the eigenvalues λn of the problem  Ly = − d  dx  ex dy dx  − 1  4 exy.  Lyn = λnexyn  0 < x < 1,  with boundary conditions  y 0  = 0,  + 1  2 y = 0  at x = 1.  dy dx   a  Find the corresponding unnormalised yn, and also a weight function ρ x  with respect to which the yn are orthogonal. Hence, select a suitable normalisation for the yn.   b  By making an eigenfunction expansion, solve the equation  Ly = −ex 2,  0 < x < 1,  subject to the same boundary conditions as previously.  17.12  Show that the linear operator  L ≡ 1  4  1 + x2 2 d2 dx2  + 1  2 x 1 + x2   + a,  d dx  acting upon functions deﬁned in −1 ≤ x ≤ 1 and vanishing at the end-points of the interval, is Hermitian with respect to the weight function  1 + x2   −1.  By making the change of variable x = tan θ 2 , ﬁnd two even eigenfunctions,  f1 x  and f2 x , of the diﬀerential equation Lu = λu. By substituting x = exp t, ﬁnd the normalised eigenfunctions yn x  and the eigenvalues λn of the operator L deﬁned by  cid:11  + 1 4 y, anyn x , the solution of Ly = x  with y 1  = y e  = 0. Find, as a series  1 ≤ x ≤ e,  Ly = x2y  −1 2.  + 2xy   cid:7  cid:7    cid:7   17.13  575   EIGENFUNCTION METHODS FOR DIFFERENTIAL EQUATIONS  17.14  Express the solution of Poisson’s equation in electrostatics,  ∇2φ r  = −ρ r   cid:4 0,  17.15  17.1  17.3  17.5  17.7 17.9  17.11  17.13  17.15  where ρ is the non-zero charge density over a ﬁnite part of space, in the form of  an integral and hence identify the Green’s function for the ∇2 operator.  In the quantum-mechanical study of the scattering of a particle by a potential, a Born-approximation solution can be obtained in terms of a function y r  that satisﬁes an equation of the form   −∇2 − K 2 y r  = F r . −3 2 exp ik·r  is a suitably normalised eigenfunction of  Assuming that yk r  =  2π   −∇2 corresponding to eigenvalue k2, ﬁnd a suitable Green’s function GK  r, r taking the direction of the vector r− r  cid:21  ∞ show that GK  r, r   . By as the polar axis for a k-space integration,   cid:7    cid:7    cid:7     can be reduced to  cid:7   4π2r − r  1  w sin w  w2 − w2  0  −∞  dw,  [ This integral can be evaluated using a contour integration  chapter 24  to give  where w0 = Kr − r  4πr − r  −1 exp iKr − r   cid:7     cid:7 .   cid:7  . ]   cid:21   1  17.8 Hints and answers   cid:1  Express the condition  cid:20 hh cid:21  ≥ 0 as a quadratic equation in λ and then apply the condition for no real roots, noting that  cid:20 fg cid:21  +  cid:20 gf cid:21  is real. To put a limit on y cos2 kx dx, set f = y1 2 cos kx and g = y1 2 in the inequality.  Follow an argument similar to that used for proving the reality of the eigenvalues, but integrate from x1 to x2, rather than from a to b. Take x1 and x2 as two  cid:7  m x2  is −α. Now assume that yn x  does not change sign in successive zeros of ym x  and note that, if the sign of ym is α then the sign of y m x1   cid:7  is α whilst that of y the interval and has a constant sign β; show that this leads to a contradiction between the signs of the two sides of the identity.  a  y =   cid:11   anPn x  with  n + 1 2  ∗  f  ∗ cid:7   gf   cid:1   an =  f z Pn z  dz;  −1 gdx  cid:3 = 0;  d  yes.   b  5x3 = 2P3 x +3P1 x , giving a1 = 1 4 and a3 = 1, leading to y = 5 2x3−x  4.   a  No, The normalised eigenfunctions are  2 π 1 2 sin nx, with n an integer. y x  =  4 π  λn =  n + 1 2 2π2, n = 0, 1, 2, . . . .  a  Since yn 1 y √ ﬁed and the appropriate weight function has to be justiﬁed by inspection. The normalised eigenfunctions are  b − n n + 1   cid:1   cid:11  dx  cid:3 = 0;  b  yes;  c  no, i n odd[ −1  n−1  2 sin nx] [n2 κ − n2 ]. m 1   cid:3 = 0, the Sturm–Liouville boundary conditions are not satis-  cid:7   cid:11 ∞  cid:12  − nπ   cid:1   −x 2 sin[ n + 1 2 πx]  n + 1 2 3. n=0 e √ 2x  −1 2 sin nπ ln x  with λn = −n2π2;  −x 2 sin[ n + 1 2 πx], with ρ x  = ex. 2e  −1 sin nπ ln x  dx = −√   b  y x  =  −2 π3   yn x  =  8 nπ   √  −2  −3  2x  e 1  for n odd, for n even.  an =  0  Use the form of Green’s function that is the integral over all eigenvalues of the ‘outer product’ of two eigenfunctions corresponding to the same eigenvalue, but with arguments r and r   cid:7   .  576   18  Special functions  In the previous two chapters, we introduced the most important second-order linear ODEs in physics and engineering, listing their regular and irregular sin- gular points in table 16.1 and their Sturm–Liouville forms in table 17.1. These equations occur with such frequency that solutions to them, which obey particu- lar commonly occurring boundary conditions, have been extensively studied and given special names. In this chapter, we discuss these so-called ‘special functions’ and their properties. In addition, we also discuss some special functions that are not derived from solutions of important second-order ODEs, namely the gamma function and related functions. These convenient functions appear in a number of contexts, and so in section 18.12 we gather together some of their properties, with a minimum of formal proofs.  18.1 Legendre functions  Legendre’s diﬀerential equation has the form   1 − x2 y   cid:7  cid:7  − 2xy   cid:7   +  cid:2   cid:2  + 1 y = 0,   18.1   physical applications and particularly in problems with axial symmetry that  and has three regular singular points, at x = −1, 1,∞. It occurs in numerous involve the ∇2 operator, when they are expressed in spherical polar coordinates. angle in spherical polars, and thus −1 ≤ x ≤ 1. The parameter  cid:2  is a given real  In normal usage the variable x in Legendre’s equation is the cosine of the polar  number, and any solution of  18.1  is called a Legendre function.  In subsection 16.1.1, we showed that x = 0 is an ordinary point of  18.1 , and so n=0 anxn.  we expect to ﬁnd two linearly independent solutions of the form y = Substituting, we ﬁnd   cid:18  n n − 1 anxn−2 − n n − 1 anxn − 2nanxn +  cid:2   cid:2  + 1 anxn  ∞ cid:4   = 0,   cid:19    cid:11 ∞  n=0  577   SPECIAL FUNCTIONS  which on collecting terms gives  { n + 2  n + 1 an+2 − [n n + 1  −  cid:2   cid:2  + 1 ]an} xn = 0.  ∞ cid:4   n=0  The recurrence relation is therefore  [n n + 1  −  cid:2   cid:2  + 1 ]   n + 1  n + 2   an,  an+2 =   18.2    18.3   x2 2!  x3 3!  for n = 0, 1, 2, . . . . If we choose a0 = 1 and a1 = 0 then we obtain the solution  y1 x  = 1 −  cid:2   cid:2  + 1   +   cid:2  − 2  cid:2   cid:2  + 1   cid:2  + 3   − ··· ,  x4 4!  whereas on choosing a0 = 0 and a1 = 1 we ﬁnd a second solution  y2 x  = x −   cid:2  − 1   cid:2  + 2   +   cid:2  − 3   cid:2  − 1   cid:2  + 2   cid:2  + 4   − ··· .   18.4   x5 5!  By applying the ratio test to these series  see subsection 4.3.2 , we ﬁnd that both  series converge for x < 1, and so their radius of convergence is unity, which   as expected  is the distance to the nearest singular point of the equation. Since  18.3  contains only even powers of x and  18.4  contains only odd powers, these two solutions cannot be proportional to one another, and are therefore linearly  independent. Hence, the general solution to  18.1  for x < 1 is  In many physical applications the parameter  cid:2  in Legendre’s equation  18.1  is an integer, i.e.  cid:2  = 0, 1, 2, . . . . In this case, the recurrence relation  18.2  gives  y x  = c1y1 x  + c2y2 x .  18.1.1 Legendre functions for integer  cid:2   [ cid:2   cid:2  + 1  −  cid:2   cid:2  + 1 ]    cid:2  + 1   cid:2  + 2   a cid:2  = 0,  a cid:2 +2 =  i.e. the series terminates and we obtain a polynomial solution of order  cid:2 . In particular, if  cid:2  is even, then y1 x  in  18.3  reduces to a polynomial, whereas if  cid:2  is odd the same is true of y2 x  in  18.4 . These solutions  suitably normalised  are called the Legendre polynomials of order  cid:2 ; they are written P cid:2  x  and are valid for all ﬁnite x. It is conventional to normalise P cid:2  x  in such a way that P cid:2  1  = 1,  and as a consequence P cid:2  −1  =  −1  cid:2 . The ﬁrst few Legendre polynomials are  easily constructed and are given by  P0 x  = 1,  P1 x  = x,  P2 x  = 1  P4 x  = 1  2  3x2 − 1 , P3 x  = 1 8  35x4 − 30x2 + 3 , P5 x  = 1  2  5x3 − 3x , 8  63x5 − 70x3 + 15x .  578   18.1 LEGENDRE FUNCTIONS  P1  −1  −0.5  0.5  x  1  P2  P0  P3  2  1  −1  −2  Figure 18.1 The ﬁrst four Legendre polynomials.  The ﬁrst four Legendre polynomials are plotted in ﬁgure 18.1.  Although, according to whether  cid:2  is an even or odd integer, respectively, either y1 x  in  18.3  or y2 x  in  18.4  terminates to give a multiple of the corresponding Legendre polynomial P cid:2  x , the other series in each case does not terminate and therefore converges only for x < 1. According to whether  cid:2  is even or odd, we deﬁne Legendre functions of the second kind as Q cid:2  x  = α cid:2 y2 x  or Q cid:2  x  = β cid:2 y1 x , respectively, where the constants α cid:2  and β cid:2  are conventionally taken to have the values   −1  cid:2  22 cid:2 [  cid:2  2 !]2  −1   cid:2 +1  22 cid:2 −1{[  cid:2  − 1  2]!}2   cid:2 !  α cid:2  =  β cid:2  =   cid:2 !  for  cid:2  even,  for  cid:2  odd.   18.5    18.6   These normalisation factors are chosen so that the Q cid:2  x  obey the same recurrence relations as the P cid:2  x   see subsection 18.1.2 .  The general solution of Legendre’s equation for integer  cid:2  is therefore  y x  = c1P cid:2  x  + c2Q cid:2  x ,   18.7   579   SPECIAL FUNCTIONS  where P cid:2  x  is a polynomial of order  cid:2 , and so converges for all x, and Q cid:2  x  is an inﬁnite series that converges only for x < 1. §  By using the Wronskian method, section 16.4, we may obtain closed forms for  the Q cid:2  x .  cid:1 Use the Wronskian method to ﬁnd a closed-form expression for Q0 x .  From  16.25  a second solution to Legendre’s equation  18.1 , with  cid:2  = 0, is   cid:21    cid:7  cid:21   du  y2 x  = P0 x    cid:21   cid:21   =  =  x  1  exp  [P0 u ]2   cid:18 − ln 1 − u2   cid:19   cid:7   exp  x  x  u  2v  1 − v2 dv  cid:8   du   1 − u2   = 1  2 ln  1 + x  1 − x  ,   cid:8   du   18.8   where in the second line we have used the fact that P0 x  = 1.  All that remains is to adjust the normalisation of this solution so that it agrees with   18.5 . Expanding the logarithm in  18.8  as a Maclaurin series we obtain  Comparing this with the expression for Q0 x , using  18.4  with  cid:2  = 0 and the normalisation  18.5 , we ﬁnd that y2 x  is already correctly normalised, and so  Of course, we might have recognised the series  18.4  for  cid:2  = 0, but to do so for larger  cid:2  would prove progressively more diﬃcult.  cid:2   Using the above method for  cid:2  = 1, we ﬁnd  y2 x  = x +  Q0 x  = 1  2 ln  x3 3  x5 5  +   cid:7   + ··· .  cid:8   1 + x  1 − x  .   cid:7    cid:8   − 1.  1 + x  1 − x  Q1 x  = 1  2 x ln  Closed forms for higher-order Q cid:2  x  may now be found using the recurrence relation  18.27  derived in the next subsection. The ﬁrst few Legendre functions of the second kind are plotted in ﬁgure 18.2.  18.1.2 Properties of Legendre polynomials  As stated earlier, when encountered in physical problems the variable x in Legendre’s equation is usually the cosine of the polar angle θ in spherical polar  coordinates, and we then require the solution y x  to be regular at x = ±1, which  corresponds to θ = 0 or θ = π. For this to occur we require the equation to have a polynomial solution, and so  cid:2  must be an integer. Furthermore, we also require  §  It is possible, in fact, to ﬁnd a second solution in terms of an inﬁnite series of negative powers of  x that is ﬁnite for x > 1  see exercise 16.16 .  580   18.1 LEGENDRE FUNCTIONS  1  0.5  Q0  −1  −0.5  0.5  x  1  −0.5  −1  Q1  Q2  Figure 18.2 The ﬁrst three Legendre functions of the second kind.  the coeﬃcient c2 of the function Q cid:2  x  in  18.7  to be zero, since Q cid:2  x  is singular at x = ±1, with the result that the general solution is simply some multiple of the relevant Legendre polynomial P cid:2  x . In this section we will study the properties of the Legendre polynomials P cid:2  x  in some detail.  Rodrigues’ formula  As an aid to establishing further properties of the Legendre polynomials we now develop Rodrigues’ representation of these functions. Rodrigues’ formula for the P cid:2  x  is  P cid:2  x  =  1  2 cid:2  cid:2 !  d cid:2  dx cid:2    x2 − 1  cid:2 .  To prove that this is a representation we let u =  x2−1  cid:2 , so that u  cid:7   = 2 cid:2 x x2−1  cid:2 −1  and   x2 − 1 u   cid:7  − 2 cid:2 xu = 0.   cid:18   x2 − 1 u  cid:2 +2  + 2x  cid:2  + 1 u  cid:2 +1  +  cid:2   cid:2  + 1 u  cid:2     cid:19  − 2 cid:2    cid:18   If we diﬀerentiate this expression  cid:2  + 1 times using Leibnitz’ theorem, we obtain  xu  cid:2 +1  +   cid:2  + 1 u  cid:2    = 0,   18.9    cid:19   581   SPECIAL FUNCTIONS  which reduces to   x2 − 1 u  cid:2 +2  + 2xu  cid:2 +1  −  cid:2   cid:2  + 1 u  cid:2   = 0.  Changing the sign all through, we recover Legendre’s equation  18.1  with u  cid:2   as the dependent variable. Since, from  18.9 ,  cid:2  is an integer and u  cid:2   is regular at x = ±1, we may make the identiﬁcation  u  cid:2   x  = c cid:2 P cid:2  x ,   18.10   for some constant c cid:2  that depends on  cid:2 . To establish the value of c cid:2  we note that the only term in the expression for the  cid:2 th derivative of  x2 − 1  cid:2  that does not contain a factor x2 − 1, and therefore does not vanish at x = 1, is  2x  cid:2  cid:2 ! x2 − 1 0. Putting x = 1 in  18.10  and recalling that P cid:2  1  = 1, therefore shows that c cid:2  = 2 cid:2  cid:2 !, thus completing the proof of Rodrigues’ formula  18.9 .  cid:1 Use Rodrigues’ formula to show that  I cid:2  =  P cid:2  x P cid:2  x  dx =   18.11   The result is trivially obvious for  cid:2  = 0 and so we assume  cid:2  ≥ 1. Then, by Rodrigues’  formula,  I cid:2  =  1  22 cid:2   cid:2 ! 2  1  −1  d cid:2  x2 − 1  cid:2   d cid:2  x2 − 1  cid:2   dx cid:2   dx cid:2   dx.  Repeated integration by parts, with all boundary terms vanishing, reduces this to  2  .  2 cid:2  + 1   cid:14  cid:13    cid:14   1  −1   cid:21    cid:21    cid:13   cid:21   cid:21   If we write   −1  cid:2   22 cid:2   cid:2 ! 2  2 cid:2  ! 22 cid:2   cid:2 ! 2  I cid:2  =  =   x2 − 1  cid:2  dx  −1  1  −1  1  dx2 cid:2    x2 − 1  cid:2  d2 cid:2   1 − x2  cid:2  dx.  cid:21   1   1 − x2  cid:2  dx,  K cid:2  =  −1   cid:21   1  2 cid:2 x2 1 − x2  cid:2 −1 dx.  then integration by parts  taking a factor 1 as the second part  gives  −1 Writing 2 cid:2 x2 as 2 cid:2  − 2 cid:2  1 − x2  we obtain  K cid:2  =   cid:21    cid:21   −1  1   1 − x2  cid:2 −1 dx − 2 cid:2   1   1 − x2  cid:2  dx  K cid:2  = 2 cid:2   −1  = 2 cid:2 K cid:2 −1 − 2 cid:2 K cid:2   and hence the recurrence relation  2 cid:2  + 1 K cid:2  = 2 cid:2 K cid:2 −1. We therefore ﬁnd 22 cid:2 +1  cid:2 ! 2  2 cid:2  + 1 !  2 cid:2  − 2 2 cid:2  − 1  ··· 2 3  K0 = 2 cid:2  cid:2 !   2 cid:2  + 1 !  2 cid:2  + 1  K cid:2  =  2 cid:2  cid:2 !  2 =  2 cid:2   ,  which, when substituted into the expression for I cid:2 , establishes the required result.  cid:2   582   18.1 LEGENDRE FUNCTIONS  Mutual orthogonality  In section 17.4, we noted that Legendre’s equation was of Sturm–Liouville form  with p = 1 − x2, q = 0, λ =  cid:2   cid:2  + 1  and ρ = 1, and that its natural interval was [−1, 1]. Since the Legendre polynomials P cid:2  x  are regular at the end-points x = ±1, they must be mutually orthogonal over this interval, i.e.  P cid:2  x Pk x  dx = 0  if  cid:2   cid:3 = k.   18.12    cid:21   1  −1  Although this result follows from the general considerations of the previous chapter, it may also be proved directly, as shown in the following example.  cid:1 Prove directly that the Legendre polynomials P cid:2  x  are mutually orthogonal over the interval −1 < x < 1.  Since the P cid:2  x  satisfy Legendre’s equation we may write   cid:2  = dP cid:2  dx. Multiplying through by Pk and integrating from x = −1 to x = 1, we  cid:7    cid:2   +  cid:2   cid:2  + 1 P cid:2  = 0,  where P obtain   cid:18    cid:19  cid:7    cid:7    cid:21    1 − x2 P  cid:19  cid:7    cid:18   1  Pk   1 − x2 P   cid:7   dx +  Pk cid:2   cid:2  + 1 P cid:2  dx = 0.  Integrating the ﬁrst term by parts and noting that the boundary contribution vanishes at  −1  −1  cid:21  both limits because of the factor 1 − x2, we ﬁnd   cid:21    cid:2   −  1  −1  k 1 − x2 P  cid:7    cid:7   cid:2  dx +  P  Pk cid:2   cid:2  + 1 P cid:2  dx = 0.  Now, if we reverse the roles of  cid:2  and k and subtract one expression from the other, we conclude that  [k k + 1  −  cid:2   cid:2  + 1 ]  PkP cid:2  dx = 0,  and therefore, since k  cid:3 =  cid:2 , we must have the result  18.12 . As a particular case, we note that if we put k = 0 we obtain cid:21    cid:21   1  1  −1   cid:21   1  −1  1  −1  P cid:2  x  dx = 0  for  cid:2   cid:3 = 0.  cid:2   As we discussed in the previous chapter, the mutual orthogonality  and com- pleteness  of the P cid:2  x  means that any reasonable function f x   i.e. one obeying the Dirichlet conditions discussed at the start of chapter 12  can be expressed in the interval x < 1 as an inﬁnite sum of Legendre polynomials,  f x  =  a cid:2 P cid:2  x ,   18.13   where the coeﬃcients a cid:2  are given by  a cid:2  =  2 cid:2  + 1  2  f x P cid:2  x  dx.   18.14   ∞ cid:4   cid:21    cid:2 =0  1  −1  583   SPECIAL FUNCTIONS   cid:1 Prove the expression  18.14  for the coeﬃcients in the Legendre polynomial expansion of a function f x .  If we multiply  18.13  by Pk x  and integrate from x = −1 to x = 1 then we obtain   cid:21   1  −1   cid:21   1  −1  ∞ cid:4   cid:21    cid:2 =0  a cid:2   1  −1  Pk x f x  dx =  Pk x P cid:2  x  dx  = ak  Pk x Pk x  dx =  2ak  ,  2k + 1  where we have used the orthogonality property  18.12  and the normalisation property  18.11 .  cid:2   Generating function  A useful device for manipulating and studying sequences of functions or quantities labelled by an integer variable  here, the Legendre polynomials P cid:2  x  labelled by  cid:2   is a generating function. The generating function has perhaps its greatest utility in the area of probability theory  see chapter 30 . However, it is also a great convenience in our present study.  The generating function for, say, a series of functions fn x  for n = 0, 1, 2, . . . is  a function G x, h  containing, as well as x, a dummy variable h such that  ∞ cid:4   n=0  G x, h  =  fn x hn,  i.e. fn x  is the coeﬃcient of hn in the expansion of G in powers of h. The utility of the device lies in the fact that sometimes it is possible to ﬁnd a closed form for G x, h .  For our study of Legendre polynomials let us consider the functions Pn x   deﬁned by the equation  G x, h  =  1 − 2xh + h2   −1 2 =  Pn x hn.   18.15   ∞ cid:4   n=0  nomials and the function  1 − 2xh + h2   As we show below, the functions so deﬁned are identical to the Legendre poly- −1 2 is in fact the generating function for them. In the process we will also deduce several useful relationships between the various polynomials and their derivatives.  cid:1 Show that the functions Pn x  deﬁned by  18.15  satisfy Legendre’s equation   cid:7  n. Firstly, we diﬀerentiate the deﬁning  In the following dPn x  dx will be denoted by P equation  18.15  with respect to x and get  h 1 − 2xh + h2   −3 2 =   cid:7  nhn.  P  Also, we diﬀerentiate  18.15  with respect to h to yield   x − h  1 − 2xh + h2   −3 2 =  nPnhn−1.   cid:4   cid:4    18.16    18.17   584   Equation  18.16  can then be written, using  18.15 , as  18.1 LEGENDRE FUNCTIONS   cid:4   h   cid:4   Pnhn =  1 − 2xh + h2    cid:7  nhn,  P   18.18    18.19    18.20    18.21   and equating the coeﬃcients of hn+1 we obtain the recurrence relation   cid:7  n + P Equations  18.16  and  18.17  can be combined as  Pn = P   cid:7  n+1  − 2xP   cid:7  n−1.   cid:4    cid:4    x − h    cid:7  nhn = h  P  nPnhn−1,  from which the coeﬃcent of hn yields a second recurrence relation,  eliminating P   cid:7  n−1 between  18.18  and  18.19  then gives the further result  If we now take the result  18.20  with n replaced by n − 1 and add x times  18.19  to it  we obtain  − P   cid:7   n  xP   cid:7  n−1 = nPn; − xP   cid:7  n+1   cid:7  n.   n + 1 Pn = P   1 − x2 P  n = n Pn−1 − xPn .  cid:7   Finally, diﬀerentiating both sides with respect to x and using  18.19  again, we ﬁnd   1 − x2 P   cid:7  cid:7   n  − 2xP   cid:7  n = n[ P = n −nPn − Pn  = −n n + 1 Pn,  n  − Pn]  cid:7   − xP   cid:7  n−1  and so the Pn deﬁned by  18.15  do indeed satisfy Legendre’s equation.  cid:2   The above example shows that the functions Pn x  deﬁned by  18.15  satisfy Legendre’s equation with  cid:2  = n  an integer  and, also from  18.15 , these functions  are regular at x = ±1. Thus Pn must be some multiple of the nth Legendre  polynomial. It therefore remains only to verify the normalisation. This is easily done at x = 1, when G becomes  G 1, h  = [ 1 − h 2]  −1 2 = 1 + h + h2 + ··· ,  and we can see that all the Pn so deﬁned have Pn 1  = 1 as required, and are thus identical to the Legendre polynomials.  A particular use of the generating function  18.15  is in representing the inverse distance between two points in three-dimensional space in terms of Legendre , respectively, from polynomials. If two points r and r the origin, with r  are at distances r and r  < r, then   cid:7    cid:7    cid:7   1r − r   cid:7  =  1   cid:7    r2 + r   cid:7 2 − 2rr  cid:7   cid:8  ∞ cid:4  r[1 − 2 r  cid:7   cid:7    cid:2   r r   cid:2 =0  =  =  1 r  cos θ 1 2 1   cid:7    r  cos θ +  r   r 2]1 2  585  where θ is the angle between the two position vectors r and r  . If r  > r, however,   cid:7    cid:7   P cid:2  cos θ ,   18.22    SPECIAL FUNCTIONS   cid:7   must be exchanged in  18.22  or the series would not converge. This r and r result may be used, for example, to write down the electrostatic potential at a < r, this is given by point r due to a charge q at the point r  . Thus, in the case r   cid:7    cid:7    cid:7   ∞ cid:4    cid:8    cid:2    cid:7   r r  V  r  =  q  4π cid:4 0r   cid:2 =0  P cid:2  cos θ .  We note that in the special case where the charge is at the origin, and r = 0, only the  cid:2  = 0 term in the series is non-zero and the expression reduces correctly to the familiar form V  r  = q  4π cid:4 0r .   cid:7   Recurrence relations  In our discussion of the generating function above, we derived several useful recurrence relations satisﬁed by the Legendre polynomials Pn x . In particular, from  18.18 , we have the four-term recurrence relation  Also, from  18.19 – 18.21 , we have the three-term recurrence relations   cid:7  n+1 + P   cid:7   cid:7  n−1 = Pn + 2xP n.  P   cid:7  n,  P   cid:7  n+1 =  n + 1 Pn + xP n−1 = −nPn + xP  cid:7   cid:7  n, n = n Pn−1 − xPn ,  1 − x2 P  cid:7  − P  cid:7  n−1,  2n + 1 Pn = P   cid:7  n+1  P  where the ﬁnal relation is obtained immediately by subtracting the second from the ﬁrst. Many other useful recurrence relations can be derived from those given above and from the generating function.   18.23    18.24    18.25    18.26    18.27    cid:1 Prove the recurrence relation  Substituting from  18.15  into  18.17 , we ﬁnd   n + 1 Pn+1 =  2n + 1 xPn − nPn−1.  cid:4   cid:4   Pnhn =  1 − 2xh + h2   nPnhn−1.   x − h   Equating coeﬃcients of hn we obtain  xPn − Pn−1 =  n + 1 Pn+1 − 2xnPn +  n − 1 Pn−1,  which on rearrangement gives the stated result.  cid:2   The recurrence relation derived in the above example is particularly useful in evaluating Pn x  for a given value of x. One starts with P0 x  = 1 and P1 x  = x and iterates the recurrence relation until Pn x  is obtained.  586   18.2 ASSOCIATED LEGENDRE FUNCTIONS  18.2 Associated Legendre functions   cid:13    cid:14   The associated Legendre equation has the form   1 − x2 y   cid:7  cid:7  − 2xy   cid:7   +   cid:2   cid:2  + 1  − m2 1 − x2  y = 0,   18.28   which has three regular singular points at x = −1, 1,∞ and reduces to Legendre’s operator ∇2, when expressed in spherical polars. In such cases, − cid:2  ≤ m ≤  cid:2  and  equation  18.1  when m = 0. It occurs in physical applications involving the  m is restricted to integer values, which we will assume from here on. As was the case for Legendre’s equation, in normal usage the variable x is the cosine of the  polar angle in spherical polars, and thus −1 ≤ x ≤ 1. Any solution of  18.28  is  called an associated Legendre function.  The point x = 0 is an ordinary point of  18.28 , and one could obtain series n=0 anxn in the same manner as that used for solutions of the form y = Legendre’s equation. In this case, however, it is more instructive to note that if u x  is a solution of Legendre’s equation  18.1 , then   cid:11   y x  =  1 − x2   m u m  m 2 d dx   18.29   is a solution of the associated equation  18.28 .  cid:1 Prove that if u x  is a solution of Legendre’s equation, then y x  given in  18.29  is a solution of the associated equation.  For simplicity, let us begin by assuming that m is non-negative. Legendre’s equation for u reads   1 − x2 u   cid:7  cid:7  − 2xu   cid:7   +  cid:2   cid:2  + 1 u = 0,  and, on diﬀerentiating this equation m times using Leibnitz’ theorem, we obtain   1 − x2 v   cid:7  cid:7  − 2x m + 1 v   cid:7   +   cid:2  − m   cid:2  + m + 1 v = 0,   18.30   where v x  = dmu dxm. On setting  the derivatives v  cid:7   y x  =  1 − x2 m 2v x ,   cid:9   cid:13    cid:7   v   cid:7  cid:7   v   cid:7  cid:7   and v  =  1 − x2  =  1 − x2   may be written as mx  −m 2   cid:7   y  +   cid:7  cid:7   y  +  −m 2   cid:10   ,  +   cid:7   2mx  1 − x2 y 1 − x2 y  cid:13    cid:14   m  1 − x2 y +  m m + 2 x2  1 − x2 2 y  .   cid:14   Substituting these expressions into  18.30  and simplifying, we obtain   1 − x2 y   cid:7  cid:7  − 2xy   cid:7   +   cid:2   cid:2  + 1  − m2 1 − x2  y = 0,  which shows that y is a solution of the associated Legendre equation  18.28 . Finally, we note that if m is negative, the value of m2 is unchanged, and so a solution for positive m is also a solution for the corresponding negative value of m.  cid:2   From the two linearly independent series solutions to Legendre’s equation given  587   SPECIAL FUNCTIONS  in  18.3  and  18.4 , which we now denote by u1 x  and u2 x , we may obtain two linearly-independent series solutions, y1 x  and y2 x , to the associated equation by using  18.29 . From the general discussion of the convergence of power series given in section 4.5.1, we see that both y1 x  and y2 x  will also converge for  x < 1. Hence the general solution to  18.28  in this range is given by  y x  = c1y1 x  + c2y2 x .  18.2.1 Associated Legendre functions for integer  cid:2   If  cid:2  and m are both integers, as is the case in many physical applications, then the general solution to  18.28  is denoted by  y x  = c1P m   cid:2   x  + c2Qm   cid:2   x ,   18.31    cid:2   x  and Qm  where P m  cid:2   x  are associated Legendre functions of the ﬁrst and second kind, respectively. For non-negative values of m, these functions are related to the ordinary Legendre functions for integer  cid:2  by   cid:2   x  =  1 − x2 m 2 dmP cid:2  dxm ,  P m   cid:2   x  =  1 − x2 m 2 dmQ cid:2  dxm .  Qm   18.32   We see immediately that, as required, the associated Legendre functions reduce to the ordinary Legendre functions when m = 0. Since it is m2 that appears in the associated Legendre equation  18.28 , the associated Legendre functions for negative m values must be proportional to the corresponding function for non- negative m. The constant of proportionality is a matter of convention. For the P m  cid:2   x  it is usual to regard the deﬁnition  18.32  as being valid also for negative m values. Although diﬀerentiating a negative number of times is not deﬁned, when P cid:2  x  is expressed in terms of the Rodrigues’ formula  18.9 , this problem does not occur for − cid:2  ≤ m ≤  cid:2 . §  In this case, −m   x  =  −1 m   cid:2  − m !    cid:2  + m !  P  cid:2   P m   cid:2   x .   18.33    cid:1 Prove the result  18.33 .  From  18.32  and the Rodrigues’ formula  18.9  for the Legendre polynomials, we have  P m   cid:2   x  =  1  2 cid:2  cid:2 !   1 − x2 m 2 d cid:2 +m   x2 − 1  cid:2 ,  dx cid:2 +m  and, without loss of generality, we may assume that m is non-negative. It is convenient to  §  −m  Some authors deﬁne P  m in the deﬁnitions  18.32 . It should be noted that, in this case, many of the results presented in  cid:2   x , in which case m is replaced by this section also require m to be replaced by m.   cid:2   x , and similarly for the Qm   x  = P m   cid:2   588   18.2 ASSOCIATED LEGENDRE FUNCTIONS  write  x2 − 1  =  x + 1  x − 1  and use Leibnitz’ theorem to evaluate the derivative, which  yields  P m   cid:2   x  =   1 − x2 m 2  1  2 cid:2  cid:2 !    cid:2  + m !  r!  cid:2  + m − r !  dr x + 1  cid:2   dxr  d cid:2 +m−r x − 1  cid:2   dx cid:2 +m−r  .   cid:2 +m cid:4   r=0  Considering the two derivative factors in a term in the summation, we note that the ﬁrst  is non-zero only for r ≤  cid:2  and the second is non-zero for  cid:2  + m − r ≤  cid:2 . Combining these conditions yields m ≤ r ≤  cid:2 . Performing the derivatives, we thus obtain  P m   cid:2   x  =   1 − x2 m 2  1  2 cid:2  cid:2 !  =  −1 m 2  cid:2 !  cid:2  + m !  2 cid:2   r=m  −m   x  =  −1   P   cid:2   −m 2  cid:2 !  cid:2  − m !  −m 2  cid:2 !  cid:2  − m !  =  −1   2 cid:2   2 cid:2    cid:2  cid:4  r!  cid:2  + m − r !  cid:2  cid:4     cid:2  + m !  r=m   cid:2 ! x − 1 r−m  r − m !   cid:2 ! x + 1  cid:2 −r   cid:2  − r ! 2  x − 1 r− m  2   x + 1  cid:2 −r+ m   cid:2   .  r!  cid:2  + m − r !  cid:2  − r ! r − m ! −m  cid:2 −m cid:4   cid:2  cid:4    x + 1  cid:2 −r− m  r!  cid:2  − m − r !  cid:2  − r ! r + m !  2  x − 1 r+ m  2  x − 1 ¯r− m   x + 1  cid:2 −¯r+ m   ¯r − m !  cid:2  − ¯r !  cid:2  + m − ¯r !¯r!  r=0  2  2  ¯r=m   18.34   ,   18.35   Repeating the above calculation for P   x  and identifying once more those terms in  the sum that are non-zero, we ﬁnd  where, in the second equality, we have rewritten the summation in terms of the new index ¯r = r + m. Comparing  18.34  and  18.35 , we immediately arrive at the required result  18.33 .  cid:2    cid:2   x  = 0 for m >  cid:2 . From Since P cid:2  x  is a polynomial of order  cid:2 , we have P m its deﬁnition, it is clear that P m  cid:2   x  is also a polynomial of order  cid:2  if m is even, but contains the factor  1 − x2  to a fractional power if m is odd. In either case,  cid:2   x  is regular at x = ±1. The ﬁrst few associated Legendre functions of the P m ﬁrst kind are easily constructed and are given by  omitting the m = 0 cases   P 2  P 1  1  x  =  1 − x2 1 2, 2  x  = 3 1 − x2 , 3  x  = 15x 1 − x2 , like Q cid:2  x , are singular at x = ±1.  P 2  P 1  2  x  = 3x 1 − x2 1 2, 3  x  = 3 P 1 3  x  = 15 1 − x2 3 2. P 3  2  5x2 − 1  1 − x2 1 2,  Finally, we note that the associated Legendre functions of the second kind Qm   cid:2   x ,  18.2.2 Properties of associated Legendre functions P m   cid:2   x   When encountered in physical problems, the variable x in the associated Legendre equation  as in the ordinary Legendre equation  is usually the cosine of the polar angle θ in spherical polar coordinates, and we then require the solution y x  to  be regular at x = ±1  corresponding to θ = 0 or θ = π . For this to occur, we require  cid:2  to be an integer and the coeﬃcient c2 of the function Qm  cid:2   x  in  18.31   589   SPECIAL FUNCTIONS   cid:2   x  is singular at x = ±1, with the result that the general  to be zero, since Qm solution is simply some multiple of one of the associated Legendre functions of the ﬁrst kind, P m  cid:2   x . We will study the further properties of these functions in the remainder of this subsection.   cid:21   1  −1  I cid:2 m ≡  cid:21   1  −1   cid:13    cid:21   1  −1   cid:21   Mutual orthogonality  As noted in section 17.4, the associated Legendre equation is of Sturm–Liouville  cid:7  form  py   + qy + λρy = 0, with p = 1 − x2, q = −m2  1 − x2 , λ =  cid:2   cid:2  + 1  and ρ = 1, and its natural interval is thus [−1, 1]. Since the associated Legendre  cid:2   x  are regular at the end-points x = ±1, they must be mutually functions P m orthogonal over this interval for a ﬁxed value of m, i.e.  P m  cid:2   x P m  k  x  dx = 0  if  cid:2   cid:3 = k.   18.36   This result may also be proved directly in a manner similar to that used for demon- strating the orthogonality of the Legendre polynomials P cid:2  x  in section 18.1.2. Note that the value of m must be the same for the two associated Legendre functions for  18.36  to hold. The normalisation condition when  cid:2  = k may be obtained using the Rodrigues’ formula, as shown in the following example.  cid:1 Show that  2  2 cid:2  + 1  .    cid:2  + m !    cid:2  − m !  cid:14  cid:13   P m  cid:2   x P m   cid:2   x  dx =   18.37    cid:14    cid:14   From the deﬁnition  18.32  and the Rodrigues’ formula  18.9  for P cid:2  x , we may write  I cid:2 m =  1  22 cid:2   cid:2 ! 2   1 − x2 m d cid:2 +m x2 − 1  cid:2   dx cid:2 +m  d cid:2 +m x2 − 1  cid:2   dx cid:2 +m  dx,  where the square brackets identify the factors to be used when integrating by parts. Performing the integration by parts  cid:2  + m times, and noting that all boundary terms vanish, we obtain   cid:13    1 − x2 m d cid:2 +m x2 − 1  cid:2   dx cid:2 +m  dx.   −1  cid:2 +m  22 cid:2   cid:2 ! 2  I cid:2 m =   cid:13   1  −1  dx cid:2 +m   x2 − 1  cid:2  d cid:2 +m  cid:14   cid:2 +m cid:4   Using Leibnitz’ theorem, the second factor in the integrand may be written as  d cid:2 +m dx cid:2 +m   1 − x2 m d cid:2 +m x2 − 1  cid:2   dx cid:2 +m    cid:2  + m !  r!  cid:2  + m − r !  =  r=0  dr 1 − x2 m  d2 cid:2 +2m−r x2 − 1  cid:2   dx2 cid:2 +2m−r  .  dxr  Considering the two derivative factors in a term in the summation on the RHS, we  see that the ﬁrst is non-zero only for r ≤ 2m, whereas the second is non-zero only for 2 cid:2  + 2m − r ≤ 2 cid:2 . Combining these conditions, we ﬁnd that the only non-zero term in the  sum is that for which r = 2m. Thus, we may write   −1  cid:2 +m  22 cid:2   cid:2 ! 2  I cid:2 m =    cid:2  + m !   2m !  cid:2  − m !  1  −1   1 − x2  cid:2  d2m 1 − x2 m  d2 cid:2  1 − x2  cid:2   dx2m  dx2 cid:2   dx.   cid:21   590   18.2 ASSOCIATED LEGENDRE FUNCTIONS  Since d2 cid:2  1 − x2  cid:2  dx2 cid:2  =  −1  cid:2  2 cid:2  !, and noting that  −1 2 cid:2 +2m = 1, we have   cid:21   We have already shown in section 18.1.2 that  and so we obtain the ﬁnal result  I cid:2 m =  22 cid:2   cid:2 ! 2   2 cid:2  !  cid:2  + m !    cid:2  − m !  −1  1   1 − x2  cid:2  dx.  1   cid:21   K cid:2  ≡  1   1 − x2  cid:2  dx =  −1  22 cid:2 +1  cid:2 ! 2  2 cid:2  + 1 !  ,  I cid:2 m =  2  2 cid:2  + 1    cid:2  + m !    cid:2  − m !  .  cid:2   ∞ cid:4   k=0   cid:21   The orthogonality and normalisation conditions,  18.36  and  18.37  respec- tively, mean that the associated Legendre functions P m  cid:2   x , with m ﬁxed, may be used in a similar way to the Legendre polynomials to expand any reasonable function f x  on the interval x < 1 in a series of the form  f x  =  am+kP m  m+k x ,   18.38   where, in this case, the coeﬃcients are given by  a cid:2  =  2 cid:2  + 1  2    cid:2  − m !    cid:2  + m !  1  −1  f x P m   cid:2   x  dx.  We note that the series takes the form  18.38  because P m   cid:2   x  = 0 for m >  cid:2 .  Finally, it is worth noting that the associated Legendre functions P m  +qy+λρy = 0, with p = 1−x2, q =  cid:2   cid:2 +1 , λ = −m2 and ρ =  1−x2    cid:2   x  must also obey a second orthogonality relationship. This has to be so because one may equally well write the associated Legendre equation  18.28  in Sturm–Liouville  cid:7  −1; form  py  once again the natural interval is [−1, 1]. Since the associated Legendre functions  cid:2   x  are regular at the end-points x = ±1, they must therefore be mutually P m orthogonal with respect to the weight function  1 − x2  −1 over this interval for a ﬁxed value of  cid:2 , i.e. cid:21   P m  cid:2   x P k   cid:2   x  1 − x2   −1 dx = 0  if m  cid:3 = k.   18.39   One may also show straightforwardly that the corresponding normalisation con- dition when m = k is given by  1  −1   cid:21   P m  cid:2   x P m   cid:2   x  1 − x2   −1 dx =  1  −1    cid:2  + m !  m  cid:2  − m !  .  In solving physical problems, however, the orthogonality condition  18.39  is not of any practical use.  591   SPECIAL FUNCTIONS  Generating function  The generating function for associated Legendre functions can be easily derived by combining their deﬁnition  18.32  with the generating function for the Legendre polynomials given in  18.15 . We ﬁnd that  G x, h  =   2m ! 1 − x2 m 2  2mm! 1 − 2hx + h2 m+1 2  =  n+m x hn. P m   18.40   ∞ cid:4   n=0   cid:1 Derive the expression  18.40  for the associated Legendre generating function.  The generating function  18.15  for the Legendre polynomials reads  ∞ cid:4   n=0  Pnhn =  1 − 2xh + h2   −1 2.  Diﬀerentiating both sides of this result m times  assumimg m to be non-negative , mutli-  plying through by  1 − x2 m 2 and using the deﬁnition  18.32  of the associated Legendre  functions, we obtain  Performing the derivatives on the RHS gives  ∞ cid:4  ∞ cid:4   n=0  n=0  n hn =  1 − x2 m 2 dm  P m   1 − 2xh + h2   −1 2.  dxm  P m n hn =  1 · 3 · 5···  2m − 1  1 − x2 m 2hm   1 − 2xh + h2 m+1 2  .  Dividing through by hm, re-indexing the summation on the LHS and noting that, quite generally,  1 · 3 · 5···  2r − 1  =  1 · 2 · 3··· 2r 2 · 4 · 6··· 2r  =   2r ! 2rr!  ,  we obtain the ﬁnal result  18.40 .  cid:2   Recurrence relations  As one might expect, the associated Legendre functions satisfy certain recurrence relations. Indeed, the presence of the two indices n and m means that a much wider range of recurrence relations may be derived. Here we shall content ourselves with quoting just four of the most useful relations:  P m+1  n =  2mx   1 − x2 1 2 P m  n + [m m − 1  − n n + 1 ]P m−1  n  ,  n =  n + m P m   2n + 1 xP m  2n + 1  1 − x2 1 2P m n = P m+1 n+1 2 1 − x2 1 2 P m  cid:7  n    = P m+1  n  n−1 +  n − m + 1 P m − P m+1 n−1 , −  n + m  n − m + 1 P m−1  n+1,  .  n   18.41    18.42    18.43    18.44   We note that, by virtue of our adopted deﬁnition  18.32 , these recurrence relations are equally valid for negative and non-negative values of m. These relations may  592   18.3 SPHERICAL HARMONICS  be derived in a number of ways, such as using the generating function  18.40  or by diﬀerentiation of the recurrence relations for the Legendre polynomials P cid:2  x .  cid:1 Use the recurrence relation  2n + 1 Pn = P the result  18.43 .   cid:7  n−1 for Legendre polynomials to derive  − P   cid:7  n+1  Diﬀerentiating the recurrence relation for the Legendre polynomials m times, we have   2n + 1   dmPn dxm  =  dm+1Pn+1  dxm+1  − dm+1Pn−1 dxm+1  .  Multiplying through by  1 − x2  m+1  2 and using the deﬁnition  18.32  immediately gives the result  18.43 .  cid:2   18.3 Spherical harmonics  The associated Legendre functions discussed in the previous section occur most commonly when obtaining solutions in spherical polar coordinates of Laplace’s  equation ∇2u = 0  see section 21.3.1 . In particular, one ﬁnds that, for solutions  that are ﬁnite on the polar axis, the angular part of the solution is given by  Θ θ Φ φ  = P m   cid:2   cos θ  C cos mφ + D sin mφ ,  where  cid:2  and m are integers with − cid:2  ≤ m ≤  cid:2 . This general form is suﬃciently  common that particular functions of θ and φ called spherical harmonics are deﬁned and tabulated. The spherical harmonics Y m   cid:2   θ, φ  are deﬁned by   cid:2   θ, φ  =  −1 m  Y m    cid:2  − m !  1 2  2 cid:2  + 1  4π    cid:2  + m !  P m   cid:2   cos θ  exp imφ .   18.45    cid:13   Using  18.33 , we note that  −m   θ, φ  =  −1 m  Y   cid:2   Y m   cid:2   θ, φ   ,   cid:14   cid:18   where the asterisk denotes complex conjugation. The ﬁrst few spherical harmonics Y m   cid:2  are as follows:   cid:2   cid:2   θ, φ  ≡ Y m  cid:2  Y 0 0 =  cid:2  1 = ∓ ±1 2 = ∓ ±1  Y  Y  1 4π ,  Y 0  1 =  3  8π sin θ exp ±iφ , 8π sin θ cos θ exp ±iφ , Y  15  Y 0 2 = ±2 2 =  5  3 4π cos θ, 16π  3 cos2 θ − 1 , 32π sin2 θ exp ±2iφ .  15   cid:19 ∗  cid:2   cid:2   cid:2   Since they contain as their θ-dependent part the solution P m  cid:2  to the associated  cid:2  are mutually orthogonal when integrated from −1 to Legendre equation, the Y m +1 over d cos θ . Their mutual orthogonality with respect to φ  0 ≤ φ ≤ 2π  is even more obvious. The numerical factor in  18.45  is chosen to make the Y m  cid:2  an  593   SPECIAL FUNCTIONS  orthonormal set, i.e.   cid:21    cid:21   1  −1  2π  0   cid:18    cid:19 ∗  Y m   cid:2   θ, φ    cid:7   cid:7   θ, φ  dφ d cos θ  = δ cid:2  cid:2  Y m  cid:2    cid:7  δmm   cid:7 .   18.46   In addition, the spherical harmonics form a complete set in that any reasonable function  i.e. one that is likely to be met in a physical situation  of θ and φ can be expanded as a sum of such functions,  ∞ cid:4    cid:2  cid:4   m=− cid:2    cid:2 =0   cid:19 ∗  f θ, φ  =   cid:21    cid:21   1  −1  2π  0   cid:18   a cid:2 mY m   cid:2   θ, φ ,   18.47   the constants a cid:2 m being given by  a cid:2 m =  Y m   cid:2   θ, φ   f θ, φ  dφ d cos θ .   18.48    cid:2  cid:4   This is in exact analogy with a Fourier series and is a particular example of the general property of Sturm–Liouville solutions.  Aside from the orthonormality condition  18.46 , the most important relation-  ship obeyed by the Y m  cid:2   is the spherical harmonic addition theorem. This reads  4π  2 cid:2  + 1  m=− cid:2   Y m  cid:2   θ, φ [Y m   cid:2   θ  , φ   cid:7    cid:7   ∗  ]  ,   18.49   P cid:2  cos γ  =   cid:7    cid:7   , φ    denote two diﬀerent directions in our spherical polar coor- where  θ, φ  and  θ dinate system that are separated by an angle γ. In general, spherical trigonometry  or vector methods  shows that these angles obey the identity  cid:7    cid:7    cid:7   cos γ = cos θ cos θ  + sin θ sin θ  cos φ − φ   .   18.50    cid:1 Prove the spherical harmonic addition theorem  18.49 .    by For the sake of brevity, it will be useful to denote the directions  θ, φ  and  θ  cid:7  Ω and Ω , respectively. We will also denote the element of solid angle on the sphere by dΩ = dφ d cos θ . We begin by deriving the form of the closure relationship obeyed by the spherical harmonics. Using  18.47  and  18.48 , and reversing the order of the summation and integration, we may write  , φ   cid:7    cid:7   f Ω  =  4π   cid:7  dΩ   cid:7  f Ω     Y m∗   cid:2    cid:7   Ω   Y m   cid:2   Ω ,   cid:2 m is a convenient shorthand for the double summation in  18.47 . Thus we may  where write the closure relationship for the spherical harmonics as   = δ Ω − Ω  cid:7    cid:2   Ω Y m∗ Y m   cid:7   Ω   cid:2    ,   18.51   where δ Ω− Ω  cid:7     is a Dirac delta function with the properties that δ Ω− Ω  cid:7     = 0 if Ω  cid:3 = Ω  cid:7   and  4π δ Ω  dΩ = 1.   cid:21    cid:4    cid:2 m   cid:11    cid:1    cid:4    cid:2 m  594   18.4 CHEBYSHEV FUNCTIONS  Since δ Ω − Ω  cid:7    cid:7    can depend only on the angle γ between the two directions Ω and Ω  ,  we may also expand it in terms of a series of Legendre polynomials of the form    =  b cid:2 P cid:2  cos γ .   18.52   From  18.14 , the coeﬃcients in this expansion are given by   cid:4    cid:2   δ Ω − Ω  cid:7   cid:21   cid:21   −1  1  2π   cid:21  δ Ω − Ω  cid:7   b cid:2  =  2 cid:2  + 1  2   P cid:2  cos γ  d cos γ   2 cid:2  + 1  =  4π  0  δ Ω − Ω  cid:7   1  −1   P cid:2  cos γ  d cos γ  dψ,  in the second equality, we have introduced an additional integration over an where,  cid:7  azimuthal angle ψ about the direction Ω  and γ is now the polar angle measured from  cid:7  Ω to Ω . Since the rest of the integrand does not depend upon ψ, this is equivalent to multiplying it by 2π 2π. However, the resulting double integral now has the form of  cid:7  a solid-angle integration over the whole sphere. Moreover, when Ω = Ω , the angle γ separating the two directions is zero, and so cos γ = 1. Thus, we ﬁnd  and combining this expression with  18.51  and  18.52  gives  b cid:2  =  2 cid:2  + 1  4π  P cid:2  1  =  2 cid:2  + 1  ,  4π   cid:2   Ω Y m∗ Y m   cid:2    cid:7   Ω    =  2 cid:2  + 1  4π  P cid:2  cos γ .   18.53    cid:4    cid:2   Comparing this result with  18.49 , we see that, to complete the proof of the addition theorem, we now only need to show that the summations in  cid:2  on either side of  18.53  can be equated term by term.  That such a procedure is valid may be shown by considering an arbitrary rigid rotation of the coordinate axes, thereby deﬁning new spherical polar coordinates ¯Ω on the sphere.  cid:2    ¯Ω  in the new coordinates can be written as a linear Any given spherical harmonic Y m combination of the spherical harmonics Y m  cid:2   Ω  of the old coordinates, all having the same value of  cid:2 . Thus,   cid:2  cid:4    cid:7   =− cid:2   m   cid:2    ¯Ω  = Y m   cid:7    cid:7   Dmm   cid:2  Y m   cid:2   Ω ,  depend on the rotation; note that in this expression Ω and ¯Ω where the coeﬃcients Dmm refer to the same direction, but expressed in the two diﬀerent coordinate systems. If we  cid:7  choose the polar axis of the new coordinate system to lie along the Ω direction, then from  18.45 , with m in that equation set equal to zero, we may write   cid:2       cid:2  cid:4    cid:7   =− cid:2   m   cid:7    cid:7   C 0m   cid:2  Y m   cid:2   Ω   P cid:2  cos γ  =  4π  2 cid:2  + 1   cid:2    ¯Ω  = Y 0   cid:7  that depend on Ω  for some set of coeﬃcients C 0m does indeed hold term by term in  cid:2 , thus proving the addition theorem  18.49 .  cid:2   cid:2   . Thus, we see that the equality  18.53    cid:4    cid:2 m   cid:7   Chebyshev’s equation has the form  18.4 Chebyshev functions   1 − x2 y   cid:7  cid:7  − xy   cid:7   + ν2y = 0,  595   18.54    SPECIAL FUNCTIONS  and has three regular singular points, at x = −1, 1,∞. By comparing it with   18.1 , we see that the Chebyshev equation is very similar in form to Legendre’s equation. Despite this similarity, equation  18.54  does not occur very often in physical problems, though its solutions are of considerable importance in numerical analysis. The parameter ν is a given real number, but in nearly all practical applications it takes an integer value. From here on we thus assume that ν = n, where n is a non-negative integer. As was the case for Legendre’s equation, in normal usage the variable x is the cosine of an angle, and so  −1 ≤ x ≤ 1. Any solution of  18.54  is called a Chebyshev function.  The point x = 0 is an ordinary point of  18.54 , and so we expect to ﬁnd m=0 amxm. One could ﬁnd two linearly independent solutions of the form y = the recurrence relations for the coeﬃcients am in a similar manner to that used for Legendre’s equation in section 18.1  see exercise 16.15 . For Chebyshev’s equation, however, it is easier and more illuminating to take a diﬀerent approach. In particular, we note that, on making the substitution x = cos θ, and consequently  d dx =  −1  sin θ  d dθ, Chebyshev’s equation becomes  with ν = n    cid:11 ∞  d2y dθ2 + n2y = 0,  which is the simple harmonic equation with solutions cos nθ and sin nθ. The corresponding linearly independent solutions of Chebyshev’s equation are thus given by  Tn x  = cos n cos  −1 x  and Vn x  = sin n cos  −1 x .   18.55   It is straightforward to show that the Tn x  are polynomials of order n, whereas the Vn x  are not polynomials  cid:1 Find explicit forms for the series expansions of Tn x  and Vn x .  Writing x = cos θ, it is convenient ﬁrst to form the complex superposition  Tn x  + iVn x  = cos nθ + i sin nθ √ =  cos θ + i sin θ n 1 − x2 x + i  =   cid:9    cid:10   n  for x ≤ 1.  Then, on expanding out the last expression using the binomial theorem, we obtain  Tn x  = xn − nC2xn−2 1 − x2  + nC4xn−4 1 − x2 2 − ··· ,  nC1xn−1 − nC3xn−3 1 − x2  + nC5xn−5 1 − x2 2 − ··· cid:19   √ 1 − x2  Vn x  =  where nCr = n! [r! n− r !] is a binomial coeﬃcient. We thus see that Tn x  is a polynomial of order n, but Vn x  is not a polynomial.  cid:2    18.56    18.57    cid:18   ,  It is conventional to deﬁne the additional functions  Wn x  =  1 − x2   −1 2Tn+1 x   and Un x  =  1 − x2   −1 2Vn+1 x .   18.58   596   18.4 CHEBYSHEV FUNCTIONS  T2  1  T0  0.5  T1  −1  −0.5  0.5  1  −0.5  −1  T3  Figure 18.3 The ﬁrst four Chebyshev polynomials of the ﬁrst kind.  From  18.56  and  18.57 , we see immediately that Un x  is a polynomial of order n, but that Wn x  is not a polynomial. In practice, it is usual to work entirely in terms of Tn x  and Un x , which are known, respectively, as Chebyshev polynomials of the ﬁrst and second kind. In particular, we note that the general solution to Chebyshev’s equation can be written in terms of these polynomials as  1 − x2 Un−1 x   for n = 1, 2, 3, . . . ,  c1Tn x  + c2  c1 + c2 sin  −1 x  √  y x  =  The n = 0 solution could also be written as d1 + c2 cos  2 πc2. The ﬁrst few Chebyshev polynomials of the ﬁrst kind are easily constructed  and are given by  for n = 0.  −1 x with d1 = c1 + 1  T0 x  = 1,  T2 x  = 2x2 − 1, T4 x  = 8x4 − 8x2 + 1,  T1 x  = x,  T3 x  = 4x3 − 3x, T5 x  = 16x5 − 20x3 + 5x.  The functions T0 x , T1 x , T2 x  and T3 x  are plotted in ﬁgure 18.3. In general,  the Chebyshev polynomials Tn x  satisfy Tn −x  =  −1 nTn x , which is easily  deduced from  18.56 . Similarly, it is straightforward to deduce the following  597   SPECIAL FUNCTIONS  U1  U0  −1  −0.5  0.5  1  U2  U3  4  2  −2  −4  Figure 18.4 The ﬁrst four Chebyshev polynomials of the second kind.  Tn −1  =  −1 n,  T2n 0  =  −1 n,  T2n+1 0  = 0.  The ﬁrst few Chebyshev polynomials of the second kind are also easily found  special values:  Tn 1  = 1,  and read  U0 x  = 1,  U2 x  = 4x2 − 1, U4 x  = 16x4 − 12x2 + 1,  U1 x  = 2x,  U3 x  = 8x3 − 4x, U5 x  = 32x5 − 32x3 + 6x.  The functions U0 x , U1 x , U2 x  and U3 x  are plotted in ﬁgure 18.4. The  Chebyshev polynomials Un x  also satisfy Un −x  =  −1 nUn x , which may be  deduced from  18.57  and  18.58 , and have the special values:  Un 1  = n + 1,  Un −1  =  −1 n n + 1 ,  U2n 0  =  −1 n,  U2n+1 0  = 0.   cid:1 Show that the Chebyshev polynomials Un x  satisfy the diﬀerential equation   1 − x2 U  n  x  − 3xU  cid:7  cid:7    cid:7  n x  + n n + 2 Un x  = 0.   18.59   From  18.58 , we have Vn+1 =  1 − x2 1 2Un and these functions satisfy the Chebyshev  equation  18.54  with ν = n + 1, namely   1 − x2 V   cid:7  cid:7  n+1  − xV   cid:7  n+1 +  n + 1 2Vn+1 = 0.   18.60   598   18.4 CHEBYSHEV FUNCTIONS  Evaluating the ﬁrst and second derivatives of Vn+1, we obtain  n+1 =  1 − x2 1 2U  cid:7  n+1 =  1 − x2 1 2U  cid:7  cid:7    cid:7   cid:7  cid:7   n  n  V  V  − x 1 − x2  − 2x 1 − x2   −1 2Un −1 2U   cid:7   n  −  1 − x2   −1 2Un − x2 1 − x2   −3 2Un.  Substituting these expressions into  18.60  and dividing through by  1 − x2 1 2, we ﬁnd  − Un +  n + 1 2Un = 0, which immediately simpliﬁes to give the required result  18.59 .  cid:2    1 − x2 U  − 3xU   cid:7  cid:7    cid:7   n  n  18.4.1 Properties of Chebyshev polynomials  The Chebyshev polynomials Tn x  and Un x  have their principal applications in numerical analysis. Their use in representing other functions over the range x < 1 plays an important role in numerical integration; Gauss–Chebyshev integration is of particular value for the accurate evaluation of integrals whose integrands contain factors  1 − x2  ±1 2. It is therefore worthwhile outlining some  of their main properties.  Rodrigues’ formula  The Chebyshev polynomials Tn x  and Un x  may be expressed in terms of a Rodrigues’ formula, in a similar way to that used for the Legendre polynomials discussed in section 18.1.2. For the Chebyshev polynomials, we have   −1 n  √ π 1 − x2 1 2 2n n − 1 √ 2  !  −1 n π n + 1  2  ! 1 − x2 1 2 2n+1 n + 1  dn dxn  2 ,   1 − x2 n− 1  1 − x2 n+  dn dxn  1 2 .  Tn x  =  Un x  =  These Rodrigues’ formulae may be proved in an analogous manner to that used in section 18.1.2 when establishing the corresponding expression for the Legendre polynomials.  Mutual orthogonality  In section 17.4, we noted that Chebyshev’s equation could be put into Sturm– Liouville form with p =  1 − x2 1 2, q = 0, λ = n2 and ρ =  1 − x2  −1 2, and its natural interval is thus [−1, 1]. Since the Chebyshev polynomials of the ﬁrst kind, Tn x , are solutions of the Chebyshev equation and are regular at the end-points x = ±1, they must be mutually orthogonal over this interval with respect to the weight function ρ =  1 − x2   −1 2, i.e.  Tn x Tm x  1 − x2   −1 2 dx = 0  if n  cid:3 = m.   18.61    cid:21   1  −1  599   SPECIAL FUNCTIONS    ∞ cid:4   n=1  The normalisation, when m = n, is easily found by making the substitution x = cos θ and using  18.55 . We immediately obtain  Tn x Tn x  1 − x2   −1 2 dx =  π  for n = 0,  π 2 for n = 1, 2, 3, . . . .   18.62    cid:21   1  −1  The orthogonality and normalisation conditions mean that any  reasonable   function f x  can be expanded over the interval x < 1 in a series of the form  f x  = 1  2 a0 +  anTn x ,  where the coeﬃcients in the expansion are given by  an =  2 π  1  −1  f x Tn x  1 − x2   −1 2 dx.  For the Chebyshev polynomials of the second kind, Un x , we see from  18.58   that  1 − x2 1 2Un x  = Vn+1 x  satisﬁes Chebyshev’s equation  18.54  with ν =  n + 1. Thus, the orthogonality relation for the Un x , obtained by replacing Ti x  by Vi+1 x  in equation  18.61 , reads  Un x Um x  1 − x2 1 2 dx = 0  if n  cid:3 = m.  The corresponding normalisation condition, when n = m, can again be found by making the substitution x = cos θ, as illustrated in the following example.   cid:1 Show that  I ≡  Un x Un x  1 − x2 1 2 dx =  π 2  .  From  18.58 , we see that  I =  Vn+1 x Vn+1 x  1 − x2   −1 2 dx,  which, on substituting x = cos θ, gives  I =  sin n + 1 θ sin n + 1 θ  1  sin θ   − sin θ  dθ =  .  cid:2   π 2   cid:21    cid:21    cid:21   1  −1  1  −1   cid:21   1  −1   cid:21   0  π  The above orthogonality and normalisation conditions allow one to expand  any  reasonable  function in the interval x < 1 in a series of the form  f x  =  anUn x ,  ∞ cid:4   n=0  600   18.4 CHEBYSHEV FUNCTIONS  in which the coeﬃcients an are given by   cid:21   an =  2 π  1  −1  f x Un x  1 − x2 1 2 dx.  Generating functions  GI x, h  =  GII x, h  =  1 − xh  1 − 2xh + h2 =  1  1 − 2xh + h2 =  ∞ cid:4  ∞ cid:4   n=0  n=0  Tn x hn,  Un x hn.  The generating functions for the Chebyshev polynomials of the ﬁrst and second kinds are given, respectively, by   18.63    18.64    18.65    18.66    18.67    18.68    18.69   18.70   These prescriptions may be proved in a manner similar to that used in sec- tion 18.1.2 for the generating function of the Legendre polynomials. For the Chebyshev polynomials, however, the generating functions are of less practical use, since most of the useful results can be obtained more easily by taking advantage of the trigonometric forms  18.55 , as illustrated below.  Recurrence relations  There exist many useful recurrence relationships for the Chebyshev polynomials Tn x  and Un x . They are most easily derived by setting x = cos θ and using  18.55  and  18.58  to write  Tn x  = Tn cos θ  = cos nθ,  Un x  = Un cos θ  =  sin n + 1 θ  .  sin θ  One may then use standard formulae for the trigonometric functions to derive a wide variety of recurrence relations. Of particular use are the trigonometric identities  cos n ± 1 θ = cos nθ cos θ ∓ sin nθ sin θ, sin n ± 1 θ = sin nθ cos θ ± cos nθ sin θ.   cid:1 Show that the Chebyshev polynomials satisfy the recurrence relations  Tn+1 x  − 2xTn x  + Tn−1 x  = 0, Un+1 x  − 2xUn x  + Un−1 x  = 0.  Adding the result  18.67  with the plus sign to the corresponding result with a minus sign gives  cos n + 1 θ + cos n − 1 θ = 2 cos nθ cos θ.  601   SPECIAL FUNCTIONS  Using  18.65  and setting x = cos θ immediately gives a rearrangement of the required result  18.69 . Similarly, adding the plus and minus cases of result  18.68  gives  sin n + 1 θ + sin n − 1 θ = 2 sin nθ cos θ.  Dividing through on both sides by sin θ and using  18.66  yields  18.70 .  cid:2   The recurrence relations  18.69  and  18.70  are extremely useful in the practical computation of Chebyshev polynomials. For example, given the values of T0 x  and T1 x  at some point x, the result  18.69  may be used iteratively to obtain the value of any Tn x  at that point; similarly,  18.70  may be used to calculate the value of any Un x  at some point x, given the values of U0 x  and U1 x  at that point.  Further recurrence relations satisﬁed by the Chebyshev polynomials are  Tn x  = Un x  − xUn−1 x ,  1 − x2 Un x  = xTn+1 x  − Tn+2 x ,   18.71    18.72   which establish useful relationships between the two sets of polynomials Tn x  and Un x . The relation  18.71  follows immediately from  18.68 , whereas  18.72  follows from  18.67 , with n replaced by n + 1, on noting that sin2 θ = 1 − x2.  Additional useful results concerning the derivatives of Chebyshev polynomials may be obtained from  18.65  and  18.66 , as illustrated in the following example.  cid:1 Show that  T   1 − x2 U   cid:7  n x  = nUn−1 x , n x  = xUn x  −  n + 1 Tn+1 x .  cid:7   These results are most easily derived from the expressions  18.65  and  18.66  by noting  that d dx =  −1  sin θ  d dθ. Thus, n x  = − 1  cid:7   cid:13   Similarly, we ﬁnd  T  sin θ  d cos nθ   n sin nθ  =  sin θ  = nUn−1 x .  n x  = − 1  cid:7   U  sin θ  d dθ  sin n + 1 θ  sin n + 1 θ cos θ  sin θ  sin3 θ  −  n + 1  cos n + 1 θ  sin2 θ  which rearranges immediately to yield the stated result.  cid:2   x Un x   1 − x2  −  n + 1 Tn+1 x   1 − x2  ,  18.5 Bessel functions  Bessel’s equation has the form  cid:7  cid:7   x2y  + xy   cid:7   +  x2 − ν2 y = 0,  which has a regular singular point at x = 0 and an essential singularity at x = ∞. The parameter ν is a given number, which we may take as ≥ 0 with no loss of   18.73   dθ   cid:14   =  =  602   18.5 BESSEL FUNCTIONS  generality. The equation arises from physical situations similar to those involving Legendre’s equation but when cylindrical, rather than spherical, polar coordinates are employed. The variable x in Bessel’s equation is usually a multiple of a radial  We shall seek solutions to Bessel’s equation in the form of inﬁnite series. Writing  By inspection, x = 0 is a regular singular point; hence we try a solution of the form y = xσ n=0 anxn. Substituting this into  18.74  and multiplying the resulting equation by x2−σ, we obtain  distance and therefore ranges from 0 to ∞.  cid:7  1 − ν2 x2  1 x  +  +   cid:7  cid:7    cid:7    cid:8    18.73  in the standard form used in chapter 16, we have  y = 0.  y  y   cid:11 ∞ ∞ cid:4   cid:19   cid:18   σ + n  σ + n − 1  +  σ + n  − ν2 ∞ cid:4    cid:18   σ + n 2 − ν2  ∞ cid:4   anxn +   cid:19   n=0  n=0  n=0  ∞ cid:4   n=0  anxn+2 = 0.  which simpliﬁes to  anxn +  anxn+2 = 0,   18.74   Considering the coeﬃcient of x0, we obtain the indicial equation  and so σ = ±ν. For coeﬃcients of higher powers of x we ﬁnd   cid:18   σ + 1 2 − ν2  σ2 − ν2 = 0,  cid:19    cid:19   a1 = 0, an + an−2 = 0   cid:18   σ + n 2 − ν2  for n ≥ 2.  Substituting σ = ±ν into  18.75  and  18.76 , we obtain the recurrence relations   1 ± 2ν a1 = 0, n n ± 2ν an + an−2 = 0  for n ≥ 2.   18.75    18.76    18.77    18.78   We consider now the form of the general solution to Bessel’s equation  18.73  for two cases: the case for which ν is not an integer and that for which it is  including zero .  18.5.1 Bessel functions for non-integer ν  If ν is a non-integer then, in general, the two roots of the indicial equation,  σ1 = ν and σ2 = −ν, will not diﬀer by an integer, and we may obtain two linearly arise, however, when ν = m 2 for m = 1, 3, 5, . . . , and σ1 − σ2 = 2ν = m is an  independent solutions in the form of Frobenius series. Special considerations do   odd positive  integer. When this happens, we may always obtain a solution in  603   SPECIAL FUNCTIONS  the form of a Frobenius series corresponding to the larger root, σ1 = ν = m 2,  as described above. However, for the smaller root, σ2 = −ν = −m 2, we must  determine whether a second Frobenius series solution is possible by examining the recurrence relation  18.78 , which reads  for n ≥ 2.  n n − m an + an−2 = 0  starting with a0  cid:3 = 0  to calculate a2, a4, a6, . . .  Since m is an odd positive integer in this case, we can use this recurrence relation in the knowledge that all these terms will remain ﬁnite. It is possible in this case, therefore, to ﬁnd a second solution in the form of a Frobenius series, one that corresponds to the smaller root σ2.  Thus, in general, for non-integer ν we have from  18.77  and  18.78   an = −  1  n n ± 2ν   an−2  for n = 2, 4, 6, . . . ,  for n = 1, 3, 5, . . . .  = 0   cid:13   Setting a0 = 1 in each case, we obtain the two solutions  y±ν x  = x  ±ν  1 −  x2  2 2 ± 2ν   +  2 × 4 2 ± 2ν  4 ± 2ν   x4  − ···  .  It is customary, however, to set   cid:14   a0 =  1  ±νΓ 1 ± ν   ,  2  where Γ x  is the gamma function, described in subsection 18.12.1; it may be regarded as the generalisation of the factorial function to non-integer and or § The two solutions of  18.73  are then written as Jν x  and negative arguments. J−ν x , where   cid:9    cid:10    cid:9   cid:10   −1 n  x 2  ν   cid:13  1 − 1  cid:9   cid:10   x 2  ;  ν + 1  ν+2n  x 2  Jν x  =  1  ∞ cid:4   Γ ν + 1   =  n=0  n!Γ ν + n + 1   2  +  1   ν + 1  ν + 2   1 2!  x 2   cid:9    cid:14    cid:10  4 − ···   18.79   replacing ν by −ν gives J−ν x . The functions Jν x  and J−ν x  are called Bessel functions of the ﬁrst kind, of order ν. Since the ﬁrst term of each series is a −ν, respectively, if ν is not an integer then ﬁnite non-zero multiple of xν and x Jν x  and J−ν x  are linearly independent. This may be conﬁrmed by calculating the Wronskian of these two functions. Therefore, for non-integer ν the general solution of Bessel’s equation  18.73  is given by  y x  = c1Jν x  + c2J−ν x .   18.80   §  In particular, Γ n + 1  = n! for n = 0, 1, 2,. . . , and Γ n  is inﬁnite if n is any integer ≤ 0.  604   18.5 BESSEL FUNCTIONS  We note that Bessel functions of half-integer order are expressible in closed form in terms of trigonometric functions, as illustrated in the following example.   cid:1 Find the general solution of   cid:7  cid:7   x2y  + xy   cid:7   +  x2 − 1  4  y = 0.  This is Bessel’s equation with ν = 1 2, so from  18.80  the general solution is simply  However, Bessel functions of half-integral order can be expressed in terms of trigonometric functions. To show this, we note from  18.79  that  y x  = c1J1 2 x  + c2J−1 2 x .  J±1 2 x  = x  ±1 2  ∞ cid:4   n=0  22n±1 2n!Γ 1 + n ± 1 2    .   −1 nx2n √  Using the fact that Γ x + 1  = xΓ x  and Γ  1  2   =  J1 2 x  =    1 2 x 1 2 Γ  3 2   √   1 2 x 1 2   1 2   π √   1 2 x 1 2   1 2   π  +  −   1 2 x 5 2 1!Γ  5 2   −   1  cid:7  √ 2 x 5 2 1!  3 2    1 2   1 − x2  +  3!  π  x4 5!  =  =    1 2 x 9 2 2!Γ  7 2    π, we ﬁnd that, for ν = 1 2, − ···  cid:8   − ···     √  sin x  x  =  2 πx  sin x,  +  2!  5  − ···    1 2 x 9 2 2    1 2    3 2   π √   1 2 x 1 2   1 2   π  =  whereas for ν = −1 2 we obtain  J−1 2 x  =  −1 2    1 2 x  Γ  1 2   −1 2√   1 2 x  π  −   1  cid:7   2 x 3 2 1!Γ  3 2   1 − x2  +  2!  =    − ···  cid:8     1 2 x 7 2 2!Γ  5 2   − ···  +  x4 4!     2 πx  =  cos x.  2 πx     cos x.  cid:2   2 πx  y x  = c1J1 2 x  + c2J−1 2 x  = c1  sin x + c2  Therefore the general solution we require is  18.5.2 Bessel functions for integer ν  The deﬁnition of the Bessel function Jν x  given in  18.79  is, of course, valid for all values of ν, but, as we shall see, in the case of integer ν the general solution of Bessel’s equation cannot be written in the form  18.80 . Firstly, let us consider the case ν = 0, so that the two solutions to the indicial equation are equal, and we clearly obtain only one solution in the form of a Frobenius series. From  18.79 ,  605   SPECIAL FUNCTIONS  1.5  1  J0  J1  0.5  J2  2  4  6  8  x  10  −0.5  this is given by  Figure 18.5 The ﬁrst three integer-order Bessel functions of the ﬁrst kind.  ∞ cid:4   J0 x  =   −1 nx2n  22nn!Γ 1 + n   n=0  = 1 − x2  22 +  x4 2242  − x6 224262 + ··· .  In general, however, if ν is a positive integer then the solutions of the indicial equation diﬀer by an integer. For the larger root, σ1 = ν, we may ﬁnd a solution Jν x , for ν = 1, 2, 3, . . . , in the form of the Frobenius series given by  18.79 . Graphs of J0 x , J1 x  and J2 x  are plotted in ﬁgure 18.5 for real x. For the  smaller root, σ2 = −ν, however, the recurrence relation  18.78  becomes  n n − m an + an−2 = 0  for n ≥ 2,  where m = 2ν is now an even positive integer, i.e. m = 2, 4, 6, . . . . Starting with  a0  cid:3 = 0 we may then calculate a2, a4, a6, . . . , but we see that when n = m the  coeﬃcient an is formally inﬁnite, and the method fails to produce a second solution in the form of a Frobenius series.  In fact, by replacing ν by −ν in the deﬁnition of Jν x  given in  18.79 , it can  be shown that, for integer ν,  J−ν x  =  −1 ν Jν x ,  606   18.5 BESSEL FUNCTIONS  and hence that Jν x  and J−ν x  are linearly dependent. So, in this case, we cannot write the general solution to Bessel’s equation in the form  18.80 . One therefore deﬁnes the function  Yν x  =  Jν x  cos νπ − J−ν x   ,  sin νπ   18.81   which is called a Bessel function of the second kind of order ν  or, occasionally, a Weber or Neumann function . As Bessel’s equation is linear, Yν x  is clearly a solution, since it is just the weighted sum of Bessel functions of the ﬁrst kind. Furthermore, for non-integer ν it is clear that Yν x  is linearly independent of Jν x . It may also be shown that the Wronskian of Jν x  and Yν x  is non-zero for all values of ν. Hence Jν x  and Yν x  always constitute a pair of independent solutions.   cid:1 If n is an integer, show that Yn+1 2 x  =  −1 n+1J−n−1 2 x .  From  18.81 , we have  Yn+1 2 x  =  Jn+1 2 x  cos n + 1  2  π − J−n−1 2 x  2  π  .  sin n + 1  If n is an integer, cos n + 1 Yn+1 2 x  =  −1 n+1J−n−1 2 x , as required.  cid:2   2  π = 0 and sin n + 1  2  π =  −1 n, and so we immediately obtain  The expression  18.81  becomes an indeterminate form 0 0 when ν is an  integer, however. This is so because for integer ν we have cos νπ =  −1 ν and J−ν x  =  −1 ν Jν x . Nevertheless, this indeterminate form can be evaluated using  l’H ˆopital’s rule  see chapter 4 . Therefore, for integer ν, we set  Yν x  = lim µ→ν  Jµ x  cos µπ − J−µ x   sin µπ   18.82    cid:14   ,   cid:13   which gives a linearly independent second solution for this case. Thus, we may write the general solution of Bessel’s equation, valid for all ν, as  y x  = c1Jν x  + c2Yν x .   18.83   The functions Y0 x , Y1 x  and Y2 x  are plotted in ﬁgure 18.6  Finally, we note that, in some applications, it is convenient to work with complex linear combinations of Bessel functions of the ﬁrst and second kinds given by  H  1   ν  x  = Jν x  + iYν x ,  ν  x  = Jν x  − iYν x ; H  2   these are called, respectively, Hankel functions of the ﬁrst and second kind of order ν.  607   SPECIAL FUNCTIONS  Y0  Y1  Y2  2  4  6  8  x  10  1  0.5  −0.5  −1  Figure 18.6 The ﬁrst three integer-order Bessel functions of the second kind.  18.5.3 Properties of Bessel functions Jν x   In physical applications, we often require that the solution is regular at x = 0, but, from its deﬁnition  18.81  or  18.82 , it is clear that Yν x  is singular at the origin, and so in such physical situations the coeﬃcient c2 in  18.83  must be set to zero; the solution is then simply some multiple of Jν x . These Bessel functions of the ﬁrst kind have various useful properties that are worthy of further discussion. Unless otherwise stated, the results presented in this section apply to Bessel functions Jν x  of integer and non-integer order.  Mutual orthogonality  In section 17.4, we noted that Bessel’s equation  18.73  could be put into con-  ventional Sturm–Liouville form with p = x, q = −ν2 x, λ = α2 and ρ = x,  provided αx is the argument of y. From the form of p, we see that there is no natural interval over which one would expect the solutions of Bessel’s equation corresponding to diﬀerent eigenvalues λ  but ﬁxed ν  to be automatically orthog- onal. Nevertheless, provided the Bessel functions satisﬁed appropriate boundary conditions, we would expect them to obey an orthogonality relationship over some interval [a, b] of the form  xJν αx Jν βx  dx = 0   18.84   for α  cid:3 = β.   cid:21   b  a  608   18.5 BESSEL FUNCTIONS  To determine the required boundary conditions for this result to hold, let us consider the functions f x  = Jν αx  and g x  = Jν βx , which, as will be proved below, respectively satisfy the equations   cid:7  cid:7  x2f  cid:7  cid:7  x2g   cid:7  + xf  cid:7   + xg  +  α2x2 − ν2 f = 0, +  β2x2 − ν2 g = 0.   18.85    18.86    cid:1 Show that f x  = Jν αx  satisﬁes  18.85 .  If f x  = Jν αx  and we write w = αx, then  df dx  = α  dJν w   dw  and  d2f dx2  = α2 d2Jν w  dw2  .  When these expressions are substituted into  18.85 , its LHS becomes  x2α2 d2Jν w  dw2  + xα  dJν w   dw  + α2x2 − ν2 Jν w  = w2 d2Jν w  dw2  + w  dJν w   dw  +  w2 − ν2 Jν w .  But, from Bessel’s equation itself, this ﬁnal expression is equal to zero, thus verifying that f x  does satisfy  18.85 .  cid:2   Now multiplying  18.86  by f x  and  18.85  by g x  and subtracting them gives   cid:7  − gf   cid:7    ] =  α2 − β2 xfg,  [x fg  d dx   18.87   where we have used the fact that  cid:7    cid:7  − gf  [x fg  d dx   ] = x fg    +  fg   cid:7  − gf   cid:7    .  By integrating  18.87  over any given range x = a to x = b, we obtain  xf x g x  dx =   cid:7    x  − xg x f   cid:7    x    cid:7  cid:7    cid:7  cid:7  − gf  cid:22   xf x g   cid:23   b  a  ,  which, on setting f x  = Jν αx  and g x  = Jν βx , becomes  xJν αx Jν βx  dx =  βxJν αx J  ν βx  − αxJν βx J  cid:7    cid:7  ν αx    cid:21   b  a   cid:21   b  a  1  α2 − β2  cid:22   1  α2 − β2   cid:23   b  a  .  18.88   If α  cid:3 = β, and the interval [a, b] is such that the expression on the RHS of  18.88   equals zero, then we obtain the orthogonality condition  18.84 . This happens, for  cid:7  ν βx  example, if Jν αx  and Jν βx  vanish at x = a and x = b, or if J vanish at x = a and x = b, or for many more general conditions. It should be noted that the boundary term is automatically zero at the point x = 0, as one might expect from the fact that the Sturm–Liouville form of Bessel’s equation has p x  = x.   cid:7  ν αx  and J  If α = β, the RHS of  18.88  takes the indeterminant form 0 0. This may be  609   SPECIAL FUNCTIONS   cid:21   b  a  J 2 ν  αx x dx.   cid:21   1 α2   cid:21   evaluated using l’H ˆopital’s rule, or alternatively we may calculate the relevant integral directly.   cid:1 Evaluate the integral   cid:21    cid:21   Ignoring the integration limits for the moment,  J 2 ν  αx x dx =  J 2 ν  u u du,  where u = αx. Integrating by parts yields  I =  J 2 ν  u u du = 1  2 u2J 2  ν  u  −  Jν u J   cid:7  ν u u2 du.  Now Bessel’s equation  18.73  can be rearranged as  u2Jν u  = ν2Jν u  − uJ  ν u  − u2J  cid:7    cid:7  cid:7  ν  u ,  which, on substitution into the expression for I, gives   cid:21  ν  u  − ν  u  − 1  I = 1  2 u2J 2  = 1  2 u2J 2  2 ν2J 2   cid:21   b  a  ν u  − u2J  cid:7    cid:7  cid:7  ν  u ] du  J  ν u [ν2Jν u  − uJ  cid:7  ν  u  + 1  cid:13  cid:7   2 u2[J   cid:8    cid:7  ν u ]2 + c.   cid:14   Since u = αx, the required integral is given by  J 2 ν  αx x dx =  1 2  x2 − ν2  α2  J 2 ν  αx  + x2[J   cid:7  ν αx ]2  b  a  ,   18.89   which gives the normalisation condition for Bessel functions of the ﬁrst kind.  cid:2   Since the Bessel functions Jν x  possess the orthogonality property  18.88 , we may expand any reasonable function f x , i.e. one obeying the Dirichlet conditions discussed in chapter 12, in the interval 0 ≤ x ≤ b as a sum of Bessel functions of  a given  non-negative  order ν,  f x  =  cnJν αnx ,   18.90   provided that the αn are chosen such that Jν αnb  = 0. The coeﬃcients cn are then given by  2  cn =  b2J 2  ν+1 αnb   0  f x Jν αnx x dx.   18.91   The interval is taken to be 0 ≤ x ≤ b, as then one need only ensure that the  appropriate boundary condition is satisﬁed at x = b, since the boundary condition at x = 0 is met automatically.  ∞ cid:4   n=0   cid:21   b  610   18.5 BESSEL FUNCTIONS   cid:1 Prove the expression  18.91 .   cid:21   b  0  If we multiply  18.90  by xJν αmx  and integrate from x = 0 to x = b then we obtain  xJν αmx f x  dx =  xJν αmx Jν αnx  dx   cid:21   b  0  ∞ cid:4   cid:21   n=0  cn  b  0  = cm  J 2 ν  αmx x dx  cid:7 2 ν αmb  = 1  where in the last two lines we have used  18.88  with αm = α  cid:3 = β = αn,  18.89 , the fact that Jν αmb  = 0 and  18.95 , which is proved below.  cid:2   = 1  2 cmb2J  2 cmb2J 2  ν+1 αmb ,  Recurrence relations  The recurrence relations enjoyed by Bessel functions of the ﬁrst kind, Jν x , can be derived directly from the power series deﬁnition  18.79 .  cid:1 Prove the recurrence relation  From the power series deﬁnition  18.79  of Jν x  we obtain  d dx  [xν Jν x ] = xν Jν−1 x .   −1 nx2ν+2n  ∞ cid:4   −1 nx2ν+2n−1 2ν+2n−1n!Γ ν + n   n=0  2ν+2nn!Γ ν + n + 1   d dx  [xν Jν x ] =  d dx  ∞ cid:4  ∞ cid:4   n=0  =  = xν  n=0   −1 nx ν−1 +2n  2 ν−1 +2nn!Γ  ν − 1  + n + 1   = xν Jν−1 x .  cid:2   It may similarly be shown that  −νJν x ] = −x  −νJν+1 x .  [x  d dx  From  18.92  and  18.93  the remaining recurrence relations may be derived. Expanding out the derivative on the LHS of  18.92  and dividing through by  xν−1, we obtain the relation  Similarly, by expanding out the derivative on the LHS of  18.93 , and multiplying through by xν+1, we ﬁnd   18.92    18.93    18.94    18.95    18.96    cid:7  ν x  + νJν x  = xJν−1 x .  xJ  ν x  − νJν x  = −xJν+1 x .  cid:7   xJ  Jν−1 x  − Jν+1 x  = 2J   cid:7  ν x .  611  Adding  18.94  and  18.95  and dividing through by x gives   SPECIAL FUNCTIONS  Finally, subtracting  18.95  from  18.94  and dividing by x gives  Jν−1 x  + Jν+1 x  =  Jν x .  2ν x   18.97    cid:1 Given that J1 2 x  =  2 πx 1 2 sin x and that J−1 2 x  =  2 πx 1 2 cos x, express J3 2 x  and J−3 2 x  in terms of trigonometric functions.  From  18.95  we have  J3 2 x  =   cid:8   cid:7  J1 2 x  − J  cid:7   cid:8   2 πx 1 2  1 2  1 2x 1 2x   cid:7    cid:7    cid:7  1 2 x  sin x − sin x − cos x  2 πx   cid:8   cid:8   .  1 2  =  =  2 πx  1 x   cid:7    cid:8   cos x +  1 2x  2 πx  1 2  sin x  Similarly, from  18.94  we have  J−1 2 x  + J  J−3 2 x  = − 1  cid:7  = − 1  2x  2x   cid:7   cid:8    cid:8   cid:7   2 πx 1 2   cid:7    cid:7  −1 2 x  cos x − cos x − sin x  2 πx   cid:8   cid:8   .  1 2  − 1  x  =  2 πx   cid:7    cid:8   1 2  sin x − 1  2x  2 πx  1 2  cos x  We see that, by repeated use of these recurrence relations, all Bessel functions Jν x  of half- integer order may be expressed in terms of trigonometric functions. From their deﬁnition  18.81 , Bessel functions of the second kind, Yν  x , of half-integer order can be similarly expressed.  cid:2   Finally, we note that the relations  18.92  and  18.93  may be rewritten in  integral form as   cid:21   cid:21   xνJν−1 x  dx = xνJν x , −νJν+1 x  dx = −x  −νJν x .  x  If ν is an integer, the recurrence relations of this section may be proved using the generating function for Bessel functions discussed below. It may be shown that Bessel functions of the second kind, Yν x , also satisfy the recurrence relations derived above.  The Bessel functions Jν x , where ν = n is an integer, can be described by a generating function in a way similar to that discussed for Legendre polynomials  Generating function  612   in subsection 18.1.2. The generating function for Bessel functions of integer order is given by  G x, h  = exp   18.98   18.5 BESSEL FUNCTIONS   cid:13    cid:8  cid:14    cid:7  h − 1  h  x 2  ∞ cid:4   Jn x hn.  =  n=−∞  By expanding the exponential as a power series, it is straightfoward to verify that the functions Jn x  deﬁned by  18.98  are indeed Bessel functions of the ﬁrst kind, as given by  18.79 .  The generating function  18.98  is useful for ﬁnding, for Bessel functions of integer order, properties that can often be extended to the non-integer case. In particular, the Bessel function recurrence relations may be derived.  cid:1 Use the generating function to prove, for integer ν, the recurrence relation  18.97 , i.e.  Diﬀerentiating G x, h  with respect to h we obtain  which can be written using  18.98  again as  Jν−1 x  + Jν+1 x  =  Jν x .   cid:8    cid:7   cid:8  ∞ cid:4   1 +  1 h2  n=−∞  ∂G x, h   =  x 2  ∂h   cid:7   x 2  1 +  1 h2  G x, h  =  nJn x hn−1,  Jn x hn =  n=−∞  nJn x hn−1.  2ν x  ∞ cid:4  ∞ cid:4   n=−∞  Equating coeﬃcients of hn we obtain  which, on replacing n by ν − 1, gives the required recurrence relation.  cid:2   [Jn x  + Jn+2 x ] =  n + 1 Jn+1 x ,  x 2  Integral representations  The generating function  18.98  is also useful for deriving integral representations of Bessel functions of integer order.  cid:1 Show that for integer n the Bessel function Jn x  is given by cos nθ − x sin θ  dθ.   18.99    cid:21   Jn x  =  π   cid:21   1 π  π  0  By expanding out the cosine term in the integrand in  18.99  we obtain the integral  I =  [cos x sin θ  cos nθ + sin x sin θ  sin nθ] dθ.   18.100   Now, we may express cos x sin θ  and sin x sin θ  in terms of Bessel functions by setting h = exp iθ in  18.98  to give   exp iθ − exp −iθ    exp  = exp  ix sin θ  =  Jm x  exp imθ.   cid:22   x 2  ∞ cid:4   m=−∞  1 π  0   cid:23   613   Using de Moivre’s theorem, exp iθ = cos θ + i sin θ, we then obtain  exp  ix sin θ  = cos x sin θ  + i sin x sin θ  =  Jm x  cos mθ + i sin mθ .  Equating the real and imaginary parts of this expression gives  SPECIAL FUNCTIONS  ∞ cid:4   m=−∞  ∞ cid:4  ∞ cid:4   m=−∞  m=−∞  cos x sin θ  =  Jm x  cos mθ,  sin x sin θ  =  Jm x  sin mθ.  Substituting these expressions into  18.100  then yields  [Jm x  cos mθ cos nθ + Jm x  sin mθ sin nθ] dθ.   cid:21   ∞ cid:4   π  m=−∞  0  I =  1 π  However, using the orthogonality of the trigonometric functions [ see equations  12.1 –  12.3  ], we obtain  which proves the integral representation  18.99 .  cid:2   I =  [Jn x  + Jn x ] = Jn x ,  1 π  π 2  Finally, we mention the special case of the integral representation  18.99  for  n = 0:  J0 x  =  cos x sin θ  dθ =  cos x sin θ  dθ,  since cos x sin θ  repeats itself in the range θ = π to θ = 2π. However, sin x sin θ  changes sign in this range and so   cid:21   1 π  π  0   cid:21   1 2π  2π  0   cid:21   1 2π  2π  0   cid:21   1 2π  2π  0   cid:21   1 2π  2π  0  Using de Moivre’s theorem, we can therefore write  sin x sin θ  dθ = 0.  J0 x  =  exp ix sin θ  dθ =  exp ix cos θ  dθ.  There are in fact many other integral representations of Bessel functions; they can be derived from those given.  18.6 Spherical Bessel functions  When obtaining solutions of Helmholtz’ equation  ∇2 + k2 u = 0 in spherical  polar coordinates  see section 21.3.2 , one ﬁnds that, for solutions that are ﬁnite on the polar axis, the radial part R r  of the solution must satisfy the equation   cid:7  cid:7   r2R  + 2rR   cid:7   + [k2r2 −  cid:2   cid:2  + 1 ]R = 0,   18.101   614   18.6 SPHERICAL BESSEL FUNCTIONS  where  cid:2  is an integer. This equation looks very much like Bessel’s equation and −1 2S  r , in which case S  r  then  cid:23   cid:6  can in fact be reduced to it by writing R r  = r satisﬁes   cid:7  cid:7    cid:7   r2S  + rS  +  2   cid:2  + 1 2  S = 0.   cid:22  k2r2 − cid:5   On making the change of variable x = kr and letting y x  = S  kr , we obtain   cid:7  cid:7   x2y  + xy   cid:7   + [x2 −   cid:2  + 1  2  2]y = 0,  where the primes now denote d dx. This is Bessel’s equation of order  cid:2  + 1 2 and has as its solutions y x  = J cid:2 +1 2 x  and Y cid:2 +1 2 x . The general solution of  18.101  can therefore be written  R r  = r  −1 2[c1J cid:2 +1 2 kr  + c2Y cid:2 +1 2 kr ],  where c1 and c2 are constants that may be determined from the boundary conditions on the solution. In particular, for solutions that are ﬁnite at the origin we require c2 = 0. The functions x  called spherical Bessel functions of the ﬁrst and second kind, respectively, and are denoted as follows:  −1 2J cid:2 +1 2 x  and x −1 2Y cid:2 +1 2 x , when suitably normalised, are      J cid:2 +1 2 x ,  j cid:2  x  =   18.102   π 2x  n cid:2  x  =  Y cid:2 +1 2 x .  π 2x   18.103   For integer  cid:2 , we also note that Y cid:2 +1 2 x  =  −1  cid:2 +1J− cid:2 −1 2 x , as discussed in  section 18.5.2. Moreover, in section 18.5.1, we noted that Bessel functions of the ﬁrst kind, Jν x , of half-integer order are expressible in closed form in terms of trigonometric functions. Thus, all spherical Bessel functions of both the ﬁrst and second kinds may be expressed in such a form. In particular, using the results of the worked example in section 18.5.1, we ﬁnd that  Expressions for higher-order spherical Bessel functions are most easily obtained by using the recurrence relations for Bessel functions.   18.104    18.105   sin x  ,  j0 x  =  x  n0 x  = − cos x  .  x  615    cid:1 Show that the  cid:2 th spherical Bessel function is given by  f cid:2  x  =  −1  cid:2 x cid:2    18.106   where f cid:2  x  denotes either j cid:2  x  or n cid:2  x .  The recurrence relation  18.93  for Bessel functions of the ﬁrst kind reads  SPECIAL FUNCTIONS   cid:7    cid:8    cid:2   1 x  d dx  f0 x ,   cid:19   .   cid:18   x  −νJν x   cid:13   Jν+1 x  = −xν d  dx  2 and rearranging, we ﬁnd −1 2J cid:2 +3 2 x  = −x cid:2  d x  x  −1 2J cid:2 +1 2  dx  x cid:2    cid:14   ,  Thus, on setting ν =  cid:2  + 1  which on using  18.102  yields the recurrence relation  [x  dx  j cid:2 +1 x  = −x cid:2  d We now change  cid:2  + 1 →  cid:2  and iterate this result:  cid:12  − cid:2 +1j cid:2 −1 x  ]  cid:12  − cid:2 +1 −1 x cid:2 −2 d  cid:18   cid:8   j cid:2  x  = −x cid:2 −1 d = −x cid:2 −1 d =  −1 2 x cid:2   cid:7  = ··· =  −1  cid:2 x cid:2   d dx  d dx  j0 x .  1 x  [ x  dx  dx  x  x  x   cid:2   dx  − cid:2 j cid:2  x ].  cid:18   − cid:2 +2j cid:2 −2 x   1 x  d dx   cid:19  cid:15   − cid:2 +2j cid:2 −2 x   x   cid:19  cid:15   n cid:2  x  in an analogous manner by setting ν =  cid:2  − 1 This is the expression for j cid:2  x  as given in  18.106 . One may prove the result  18.106  for 2 in the recurrence relation  18.92  for Bessel functions of the ﬁrst kind and using the relationship Y cid:2 +1 2 x  =  −1  cid:2 +1J− cid:2 −1 2 x .  cid:2   cid:7   Using result  18.106  and the expressions  18.104  and  18.105 , one quickly  ﬁnds, for example,   cid:8   j1 x  =  sin x x2  n1 x  = − cos x  x2  − cos x  x  , − sin x  ,  x  j2 x  =  n2 x  = −  − 1  x  3 x3   cid:7   − 1  x  3 x3   cid:8  sin x − 3 cos x cos x − 3 sin x  x2  ,  .  x2  Finally, we note that the orthogonality properties of the spherical Bessel functions follow directly from the orthogonality condition  18.88  for Bessel functions of the ﬁrst kind.  Laguerre’s equation has the form  18.7 Laguerre functions   cid:7  cid:7   +  1 − x y   cid:7   xy  + νy = 0;  616   18.107    18.7 LAGUERRE FUNCTIONS  ∞ cid:4   m=0  it has a regular singularity at x = 0 and an essential singularity at x = ∞. The  parameter ν is a given real number, although it nearly always takes an integer value in physical applications. The Laguerre equation appears in the description of the wavefunction of the hydrogen atom. Any solution of  18.107  is called a Laguerre function.  Since the point x = 0 is a regular singularity, we may ﬁnd at least one solution  in the form of a Frobenius series  see section 16.3 :  y x  =  amxm+σ.   18.108   Substituting this series into  18.107  and dividing through by xσ−1, we obtain  [ m + σ  m + σ − 1  +  1 − x  m + σ  + νx] amxm = 0.   18.109   ∞ cid:4   m=0  Setting x = 0, so that only the m = 0 term remains, we obtain the indicial equation σ2 = 0, which trivially has σ = 0 as its repeated root. Thus, Laguerre’s equation has only one solution of the form  18.108 , and it, in fact, reduces to a simple power series. Substituting σ = 0 into  18.109  and demanding that the coeﬃcient of xm+1 vanishes, we obtain the recurrence relation  m − ν  m + 1 2 am.  am+1 =  As mentioned above, in nearly all physical applications, the parameter ν takes integer values. Therefore, if ν = n, where n is a non-negative integer, we see that  an+1 = an+2 = ··· = 0, and so our solution to Laguerre’s equation is a polynomial of order n. It is conventional to choose a0 = 1, so that the solution is given by   cid:13    −1 n n cid:4   n!  Ln x  =  xn − n2  xn−1 +  1!  n2 n − 1 2  2!   −1 m  n!   m! 2 n − m !  xm,  =  m=0   cid:14  xn−2 − ··· +  −1 nn!   18.110    18.111   where Ln x  is called the nth Laguerre polynomial. We note in particular that Ln 0  = 1. The ﬁrst few Laguerre polynomials are given by  L0 x  = 1,  L1 x  = −x + 1, 2!L2 x  = x2 − 4x + 2,  3!L3 x  = −x3 + 9x2 − 18x + 6, 4!L4 x  = x4 − 16x3 + 72x2 − 96x + 24, 5!L5 x  = −x5 + 25x4 − 200x3 + 600x2 − 600x + 120.  The functions L0 x , L1 x , L2 x  and L3 x  are plotted in ﬁgure 18.7.  617   SPECIAL FUNCTIONS  L0  1  2  3  4  5  6  x  7  L2  L3  L1  10  5  −5  −10  Figure 18.7 The ﬁrst four Laguerre polynomials.  18.7.1 Properties of Laguerre polynomials  The Laguerre polynomials and functions derived from them are important in the analysis of the quantum mechanical behaviour of some physical systems. We therefore brieﬂy outline their useful properties in this section.  The Laguerre polynomials can be expressed in terms of a Rodrigues’ formula given by  which may be proved straightforwardly by calculating the nth derivative explicitly using Leibnitz’ theorem and comparing the result with  18.111 . This is illustrated in the following example.   18.112   Rodrigues’ formula   cid:5    cid:6   Ln x  =  ex n!  dn dxn  −x  xne  ,  618   18.7 LAGUERRE FUNCTIONS   cid:1 Prove that the expression  18.112  yields the nth Laguerre polynomial.  Evaluating the nth derivative in  18.112  using Leibnitz’ theorem, we ﬁnd  Ln x  =  nCr  drxn dxr  dn−re −x dxn−r  ex n!  n cid:4  n cid:4  n cid:4   ex n!  r=0  r=0  =  =   −1 n−r n cid:4   n!  r! n − r !  n!   n − r !  xn−r  −1 n−re −x  n!  r! n − r ! n − r !  xn−r.  Relabelling the summation using the index m = n − r, we obtain  r=0  which is precisely the expression  18.111  for the nth Laguerre polynomial.  cid:2   Ln x  =  m=0   −1 m  n!   m! 2 n − m !  xm,  Mutual orthogonality   cid:21  ∞  0   cid:21  ∞  In section 17.4, we noted that Laguerre’s equation could be put into Sturm– −x, and its natural interval Liouville form with p = xe is thus [0,∞]. Since the Laguerre polynomials Ln x  are solutions of the equation  −x, q = 0, λ = ν and ρ = e  and are regular at the end-points, they must be mutually orthogonal over this interval with respect to the weight function ρ = e  Ln x Lk x e  −x dx = 0  −x, i.e. if n  cid:3 = k.  This result may also be proved directly using the Rodrigues’ formula  18.112 . Indeed, the normalisation, when k = n, is most easily found using this method.  cid:1 Show that  I ≡  Ln x Ln x e  −x dx = 1.   18.113    cid:21  ∞  0  Using the Rodrigues’ formula  18.112 , we may write  I =  1 n!  0  Ln x   dn dxn  −x  dx =   xne  −x dx,  xne  dnLn dxn   cid:21  ∞   −1 n  n!  0  where, in the second equality, we have integrated by parts n times and used the fact that the boundary terms all vanish. When dnLn dxn is evaluated using  18.111 , only the derivative of the m = n term survives and that has the value [  −1 nn! n! ] [ n! 2 0!] =  −1 n. Thus  we have  where, in the second equality, we use the expression  18.153  deﬁning the gamma function  see section 18.12 .  cid:2   I =  1 n!  0  −x dx = 1,  xne   cid:21  ∞  619   SPECIAL FUNCTIONS  ∞ cid:4   n=0   cid:21  ∞  0  f x  =  anLn x ,  an =  f x Ln x e  −x dx.  The above orthogonality and normalisation conditions allow us to expand any   reasonable  function in the interval 0 ≤ x < ∞ in a series of the form  in which the coeﬃcients an are given by  We note that it is sometimes convenient to deﬁne the orthonormal Laguerre func- tions φn x  = e of a function in the interval 0 ≤ x < ∞.  −x 2Ln x , which may also be used to produce a series expansion  Generating function  ∞ cid:4   n=0  e  −xh  1−h  1 − h  The generating function for the Laguerre polynomials is given by  G x, h  =  =  Ln x hn.   18.114   We may prove this result by diﬀerentiating the generating function with respect to x and h, respectively, to obtain recurrence relations for the Laguerre polynomials, which may then be combined to show that the functions Ln x  in  18.114  do indeed satisfy Laguerre’s equation  as discussed in the next subsection .  Recurrence relations  The Laguerre polynomials obey a number of useful recurrence relations. The three most important relations are as follows:   n + 1 Ln+1 x  =  2n + 1 − x Ln x  − nLn−1 x ,  n−1 x  − L  cid:7   Ln−1 x  = L xL   cid:7  n x , n x  = nLn x  − nLn−1 x .  cid:7    18.115    18.116    18.117   The ﬁrst two relations are easily derived from the generating function  18.114 , and may be combined straightforwardly to yield the third result.  cid:1 Derive the recurrence relations  18.115  and  18.116 .  Diﬀerentiating the generating function  18.114  with respect to h, we ﬁnd  ∂G ∂h  =   1 − x − h e  1 − h 3  cid:4   −xh  1−h   =  nLnhn−1.   1 − x − h   Lnhn =  1 − h 2  nLnhn−1,   cid:4   cid:4   Thus, we may write  and, on equating coeﬃcients of hn on each side, we obtain   1 − x Ln − Ln−1 =  n + 1 Ln+1 − 2nLn +  n − 1 Ln−1,  620   18.8 ASSOCIATED LAGUERRE FUNCTIONS  which trivially rearranges to give the recurrence relation  18.115 .  To obtain the recurrence relation  18.116 , we begin by diﬀerentiating the generating  function  18.114  with respect to x, which yields −xh  1−h   1 − h 2  ∂G ∂x  =  = − he  cid:4   −h  Lnhn =  1 − h   and thus we have  Equating coeﬃcients of hn on each side then gives  −Ln−1 = L  cid:7  which immediately simpliﬁes to give  18.116 .  cid:2   − L  n   cid:7  n−1,   cid:4   cid:4    cid:7  nhn,  L   cid:7  nhn.  L  18.8 Associated Laguerre functions  The associated Laguerre equation has the form   cid:7  cid:7   +  m + 1 − x y   cid:7   xy  + ny = 0;   18.118   it has a regular singularity at x = 0 and an essential singularity at x = ∞. We  restrict our attention to the situation in which the parameters n and m are both non-negative integers, as is the case in nearly all physical problems. The associated Laguerre equation occurs most frequently in quantum-mechanical applications. Any solution of  18.118  is called an associated Laguerre function.  Solutions of  18.118  for non-negative integers n and m are given by the  associated Laguerre polynomials  n  x  =  −1 m dm  Lm  dxm Ln+m x , § where Ln x  are the ordinary Laguerre polynomials.  cid:1 Show that the functions Lm  n  x  deﬁned in  18.119  are solutions of  18.118 .   18.119   Since the Laguerre polynomials Ln x  are solutions of Laguerre’s equation  18.107 , we have  n+m +  1 − x L  cid:7  cid:7   xL   cid:7  n+m +  n + m Ln+m = 0.  Diﬀerentiating this equation m times using Leibnitz’ theorem and rearranging, we ﬁnd  xL m+2   n+m +  m + 1 − x L m+1  On multiplying through by  −1 m and setting Lm +  m + 1 − x  Lm n   n are indeed solutions of  18.118 .  cid:2   n+m + nL m  n =  −1 mL m   cid:7   which shows that the functions Lm  we obtain  n+m = 0.  x Lm n    n = 0,  + nLm   cid:7  cid:7   n+m, in accord with  18.119 ,  §  Note that some authors deﬁne the associated Laguerre polynomials as Lm which is thus related to our expression  18.119  by Lm n+m x .  n  x  =  −1 mLm  n  x  =  dm dxm Ln x ,  621   SPECIAL FUNCTIONS  In particular, we note that L0  Ln x  is a polynomial of order n and so it follows that Lm associated Laguerre polynomials are easily found using  18.119 :  n x  = Ln x . As discussed in the previous section, n  x  is also. The ﬁrst few  Lm Lm 2!Lm 3!Lm  0  x  = 1, 1  x  = −x + m + 1, 2  x  = x2 − 2 m + 2 x +  m + 1  m + 2 , 3  x  = −x3 + 3 m + 3 x2 − 3 m + 2  m + 3 x +  m + 1  m + 2  m + 3 .  Indeed, in the general case, one may show straightforwardly, from the deﬁnition  18.119  and the expression  18.111  for the ordinary Laguerre polynomials, that  Lm  n  x  =   −1 k   n + m !  k! n − k ! k + m !  xk.   18.120   n cid:4   k=0  18.8.1 Properties of associated Laguerre polynomials  The properties of the associated Laguerre polynomials follow directly from those of the ordinary Laguerre polynomials through the deﬁnition  18.119 . We shall therefore only brieﬂy outline the most useful results here.  A Rodrigues’ formula for the associated Laguerre polynomials is given by  Rodrigues’ formula  Lm  n  x  =  −m  exx n!  dn dxn   xn+me  −x .   18.121   It can be proved by evaluating the nth derivative using Leibnitz’ theorem  see exercise 18.7 .  Mutual orthogonality  In section 17.4, we noted that the associated Laguerre equation could be trans- −x, formed into a Sturm–Liouville one with p = xm+1e and its natural interval is thus [0,∞]. Since the associated Laguerre polynomials Lm n  x  are solutions of the equation and are regular at the end-points, those with the same m but diﬀering values of the eigenvalue λ = n must be mutually −x, i.e. orthogonal over this interval with respect to the weight function ρ = xme  −x, q = 0, λ = n and ρ = xme  Lm  n  x Lm  k  x xme  −x dx = 0  if n  cid:3 = k.   cid:21  ∞  0  This result may also be proved directly using the Rodrigues’ formula  18.121 , as may the normalisation condition when k = n.  622   18.8 ASSOCIATED LAGUERRE FUNCTIONS   cid:1 Show that   cid:21  ∞  0  I ≡   cid:21  ∞  Lm  n  x Lm  n  x xme  −x dx =   n + m !  .   18.122   n!   cid:21  ∞   −1 n  n!  0  Using the Rodrigues’ formula  18.121 , we may write  I =  1 n!  0  Lm  n  x   dn dxn   xn+me  −x  dx =  xn+me  −x dx,  dnLm n dxn  where, in the second equality, we have integrated by parts n times and used the fact that the boundary terms all vanish. From  18.120  we see that dnLm  n  dxn =  −1 n. Thus we have   cid:21  ∞  I =  1 n!  0  xn+me  −x dx =   n + m !  ,  n!  where, in the second equality, we use the expression  18.153  deﬁning the gamma function  see section 18.12 .  cid:2   The above orthogonality and normalisation conditions allow us to expand any   reasonable  function in the interval 0 ≤ x < ∞ in a series of the form  f x  =  anLm  n  x ,  ∞ cid:4   cid:21  ∞  n=0  in which the coeﬃcients an are given by  n!  an =   n + m !  0  f x Lm  n  x xme  −x dx.  We note that it is sometimes convenient to deﬁne the orthogonal associated Laguerre functions φm n  x , which may also be used to produce a series expansion of a function in the interval 0 ≤ x < ∞.  n  x  = xm 2e  −x 2Lm  The generating function for the associated Laguerre polynomials is given by  Generating function  G x, h  =  e  −xh  1−h   1 − h m+1 =  ∞ cid:4   n=0  Lm  n  x hn.   18.123   This can be obtained by diﬀerentiating the generating function  18.114  for the ordinary Laguerre polynomials m times with respect to x, and using  18.119 .  cid:1 Use the generating function  18.123  to obtain an expression for Lm  n  0 .  ∞ cid:4   n=0  From  18.123 , we have  Lm  n  0 hn =  1   1 − h m+1  = 1 +  m + 1 h +   m + 1  m + 2   h2 + ··· +  2!   m + 1  m + 2 ···  m + n   hn + ··· ,  n!  623   where, in the second equality, we have expanded the RHS using the binomial theorem. On equating coeﬃcients of hn, we immediately obtain  SPECIAL FUNCTIONS  Lm  n  0  =   n + m !  n!m!  .  cid:2   Recurrence relations  The various recurrence relations satisﬁed by the associated Laguerre polynomials may be derived by diﬀerentiating the generating function  18.123  with respect to either or both of x and h, or by diﬀerentiating with respect to x the recurrence relations obeyed by the ordinary Laguerre polynomials, discussed in section 18.7.1. Of the many recurrence relations satisﬁed by the associated Laguerre polynomials, two of the most useful are as follows:  n+1 x  =  2n + m + 1 − x Lm  n + 1 Lm n  x  −  n + m Lm  cid:7  x Lm n     x  = nLm  n  x  −  n + m Lm n−1 x .  n−1 x ,   18.124    18.125   For proofs of these relations the reader is referred to exercise 18.7.  Hermite’s equation has the form  18.9 Hermite functions   cid:7  cid:7  − 2xy   cid:7   y  + 2νy = 0,  and has an essential singularity at x = ∞. The parameter ν is a given real  number, although it nearly always takes an integer value in physical applications. The Hermite equation appears in the description of the wavefunction of the harmonic oscillator. Any solution of  18.126  is called a Hermite function.  Since x = 0 is an ordinary point of the equation, we may ﬁnd two linearly  independent solutions in the form of a power series  see section 16.2 :   18.126   y x  =  amxm.   18.127   Substituting this series into  18.107  yields  [ m + 1  m + 2 am+2 + 2 ν − m am] xm = 0.  ∞ cid:4   m=0  Demanding that the coeﬃcient of each power of x vanishes, we obtain the recurrence relation  am+2 = − 2 ν − m    m + 1  m + 2   am.  As mentioned above, in nearly all physical applications, the parameter ν takes integer values. Therefore, if ν = n, where n is a non-negative integer, we see that  ∞ cid:4   m=0  624   18.9 HERMITE FUNCTIONS  H2  H0  −1.5  −1  −0.5  H1  10  5  −5  −10  0.5  1  x  1.5  H3  Figure 18.8 The ﬁrst four Hermite polynomials.  an+2 = an+4 = ··· = 0, and so one solution of Hermite’s equation is a polynomial of order n. For even n, it is conventional to choose a0 =  −1 n 2n!  n 2 !, whereas 2  n− 1 ]!. These choices allow a general for odd n one takes a1 =  −1  n−1  22n! [ 1 n n − 1  n − 2  n − 3   solution to be written as  Hn x  =  2x n − n n − 1  2x n−1 +   2x n−4 − ··· 18.128   2!  [n 2] cid:4   −1 m  m=0  =  n!  m! n − 2m !   2x n−2m,   18.129   where Hn x  is called the nth Hermite polynomial and the notation [n 2] denotes  the integer part of n 2. We note in particular that Hn −x  =  −1 nHn x . The  ﬁrst few Hermite polynomials are given by  H0 x  = 1,  H1 x  = 2x,  H2 x  = 4x2 − 2,  H3 x  = 8x2 − 12x, H4 x  = 16x4 − 48x2 + 12, H5 x  = 32x5 − 160x3 + 120x.  The functions H0 x , H1 x , H2 x  and H3 x  are plotted in ﬁgure 18.8.  625   SPECIAL FUNCTIONS  18.9.1 Properties of Hermite polynomials  The Hermite polynomials and functions derived from them are important in the analysis of the quantum mechanical behaviour of some physical systems. We therefore brieﬂy outline their useful properties in this section.  The Rodrigues’ formula for the Hermite polynomials is given by  Rodrigues’ formula  Hn x  =  −1 nex2 dn  −x2   e   .  dxn   18.130   This can be proved using Leibnitz’ theorem.  cid:1 Prove the Rodrigues’ formula  18.130  for the Hermite polynomials.  Letting u = e  −x2 and diﬀerentiating with respect to x, we quickly ﬁnd that   cid:7   u  + 2xu = 0.  Diﬀerentiating this equation n + 1 times using Leibnitz’ theorem then gives  u n+2  + 2xu n+1  + 2 n + 1 u n  = 0, which, on introducing the new variable v =  −1 nu n , reduces to   cid:7  cid:7    cid:7   v  + 2xv  + 2 n + 1 v = 0.   18.131   Now letting y = ex2  v, we may write the derivatives of v as   cid:7  v  cid:7  cid:7   v  −x2 −x2   cid:7  − 2xy ,  cid:7  cid:7  − 4xy  cid:7    y   y  = e  = e  + 4x2y − 2y .  Substituting these expressions into  18.131 , and dividing through by e Hermite’s equation,   cid:7  cid:7  − 2xy + 2ny = 0,  y  thus demonstrating that y =  −1 nex2  −x2   dxn is indeed a solution. Moreover, since this solution is clearly a polynomial of order n, it must be some multiple of Hn x . The normalisation is easily checked by noting that, from  18.130 , the highest-order term is  2x n, which agrees with the expression  18.128 .  cid:2   dn e  −x2 , ﬁnally yields  Mutual orthogonality  We saw in section 17.4 that Hermite’s equation could be cast in Sturm–Liouville −x2 , and its natural interval is thus form with p = e [−∞,∞]. Since the Hermite polynomials Hn x  are solutions of the equation and  −x2 , q = 0, λ = 2n and ρ = e  are regular at the end-points, they must be mutually orthogonal over this interval with respect to the weight function ρ = e −x2  −x2 , i.e.   cid:21  ∞  if n  cid:3 = k.  −∞ Hn x Hk x e  dx = 0  This result may also be proved directly using the Rodrigues’ formula  18.130 . Indeed, the normalisation, when k = n, is most easily found in this way.  626   dn dxn   cid:21  ∞  ∞ cid:4   cid:21  ∞  n=0  18.9 HERMITE FUNCTIONS   cid:21  ∞ I ≡  cid:21  ∞   cid:1 Show that  √ dx = 2nn!  π.  −x2  −∞ Hn x Hn x e   18.132   Using the Rodrigues’ formula  18.130 , we may write  I =  −1 n  Hn x   0  −x2   e    dx =  dnHn dxn  −∞  −x2 e  dx,   cid:21  ∞  where, in the second equality, we have integrated by parts n times and used the fact that the boundary terms all vanish. From  18.128  we see that dnHn dxn = 2nn!. Thus we have  √ dx = 2nn!  π,  −x2  I = 2nn!  −∞ e  where, in the second equality, we use the standard result for the area under a Gaussian  see section 6.4.2 .  cid:2   The above orthogonality and normalisation conditions allow any  reasonable   function in the interval −∞ ≤ x < ∞ to be expanded in a series of the form  f x  =  anHn x ,  in which the coeﬃcients an are given by  an =  √ 1 2nn!  π  −∞ f x Hn x e  −x2  dx.  We note that it is sometimes convenient to deﬁne the orthogonal Hermite functions φn x  = e  −x2 2Hn x ; they also may be used to produce a series expansion of a function in the interval −∞ ≤ x < ∞. Indeed, φn x  is proportional to the  wavefunction of a particle in the nth energy level of a quantum harmonic oscillator.  The generating function equation for the Hermite polynomials reads  Generating function  G x, h  = e2hx−h2  =  Hn x   n!  hn,   18.133   a result that may be proved using the Rodrigues’ formula  18.130 .  cid:1 Show that the functions Hn x  in  18.133  are the Hermite polynomials.  It is often more convenient to write the generating function  18.133  as  ∞ cid:4   n=0  ∞ cid:4   n=0  G x, h  = ex2  − x−h 2 e  =  Hn x   n!  hn.  627   SPECIAL FUNCTIONS  ∞ cid:4   Diﬀerentiating this form k times with respect to h gives  Hn   n − k !  ∂kG ∂hk  = ex2 ∂k ∂hk  − x−h 2 e  =  −1 kex2 ∂k  − x−h 2 e  .  ∂xk  Relabelling the summation on the LHS using the new index m = n − k, we obtain  n=k  hn−k = ∞ cid:4   Hm+k m!  m=0  hm =  −1 kex2 ∂k  − x−h 2 e  .  ∂xk  Setting h = 0 in this equation, we ﬁnd  which is the Rodrigues’ formula  18.130  for the Hermite polynomials.  cid:2   Hk x  =  −1 kex2 dk  −x2   e   ,  dxk  The generating function  18.133  is also useful for determining special values of the Hermite polynomials. In particular, it is straightforward to show that  H2n 0  =  −1 n 2n ! n! and H2n+1 0  = 0.  Recurrence relations  The two most useful recurrence relations satisﬁed by the Hermite polynomials are given by  Hn+1 x  = 2xHn x  − 2nHn−1 x ,   cid:7  n x  = 2nHn−1 x .  H   18.134    18.135   The ﬁrst relation provides a simple iterative way of evaluating the nth Hermite polynomials at some point x = x0, given the values of H0 x  and H1 x  at that point. For proofs of these recurrence relations, see exercise 18.5.  18.10 Hypergeometric functions  The hypergeometric equation has the form  x 1 − x y   cid:7  cid:7   + [c −  a + b + 1 x]y   cid:7  − aby = 0,   18.136   and has three regular singular points, at x = 0, 1,∞, but no essential singularities.  The parameters a, b and c are given real numbers.  In our discussions of Legendre functions, associated Legendre functions and Chebyshev functions in sections 18.1, 18.2 and 18.4, respectively, it was noted that in each case the corresponding second-order diﬀerential equation had three regular  singular points, at x = −1, 1,∞, and no essential singularities. The hypergeometric  equation can, in fact, be considered as the ‘canonical form’ for second-order § that, diﬀerential equations with this number of singularities. It may be shown  §  See, for example, J. Mathews and R. L. Walker, Mathematical Methods of Physics, 2nd edn  Reading MA: Addision–Wesley, 1971 .  628   18.10 HYPERGEOMETRIC FUNCTIONS  by making appropriate changes of the independent and dependent variables, any second-order diﬀerential equation with three regular singularities and an ordinary point at inﬁnity can be transformed into the hypergeometric equation   18.136  with the singularities at = −1, 1 and ∞. As we discuss below, this allows  Legendre functions, associated Legendre functions and Chebyshev functions, for example, to be written as particular cases of hypergeometric functions, which are the solutions to  18.136 .  Since the point x = 0 is a regular singularity of  18.136 , we may ﬁnd at least  one solution in the form of a Frobenius series  see section 16.3 :  y x  =  anxn+σ.   18.137   ∞ cid:4  Substituting this series into  18.136  and dividing through by xσ−1, we obtain { 1 − x  n + σ  n + σ − 1  + [c −  a + b + 1 x] n + σ  − abx} anxn = 0.  n=0   18.138   Setting x = 0, so that only the n = 0 term remains, we obtain the indicial equation  σ σ − 1  + cσ = 0, which has the roots σ = 0 and σ = 1 − c. Thus, provided  c is not an integer, one can obtain two linearly independent solutions of the hypergeometric equation in the form  18.137 .  For σ = 0 the corresponding solution is a simple power series. Substituting σ = 0 into  18.138  and demanding that the coeﬃcient of xn vanishes, we ﬁnd the recurrence relation  n[ n − 1  + c]an − [ n − 1  a + b + n − 1  + ab]an−1 = 0,  which, on simplifying and replacing n by n + 1, yields the recurrence relation  an+1 =   a + n  b + n   n + 1  c + n   an.  It is conventional to make the simple choice a0 = 1. Thus, provided c is not a negative integer or zero, we may write the solution as follows:  F a, b, c; x  = 1 +  ab c  x 1!  a a + 1 b b + 1   c c + 1   + ···  x2 2!  Γ c   Γ a + n Γ b + n   =  Γ a Γ b   Γ c + n   xn n!  ,   18.139    18.140    18.141    18.142   where F a, b, c; x  is known as the hypergeometric function or hypergeometric series, and in the second equality we have used the property  18.154  of the  ∞ cid:4   n=0  +  ∞ cid:4   n=0  629   SPECIAL FUNCTIONS  § gamma function.  It is straightforward to show that the hypergeometric series  converges in the range x   a + b and at x = −1 if c > a + b − 1. We also note that F a, b, c; x  is symmetric in the  parameters a and b, i.e. F a, b, c; x  = F b, a, c; x .  The hypergeometric function y x  = F a, b, c; x  is clearly not the general solution to the hypergeometric equation  18.136 , since we must also consider the  second root of the indicial equation. Substituting σ = 1 − c into  18.138  and demanding that the coeﬃcient of xn vanishes, we ﬁnd that we must have  n n + 1 − c an − [ n − c  a + b + n − c  + ab]an−1 = 0,  which, on comparing with  18.139  and replacing n by n + 1, yields the recurrence relation   a − c + 1 + n  b − c + 1 + n    n + 1  2 − c + n   an.  an+1 =  We see that this recurrence relation has the same form as  18.140  if one makes  the replacements a → a − c + 1, b → b − c + 1 and c → 2 − c. Thus, provided c, a− b and c− a− b are all non-integers, the general solution to the hypergeometric equation, valid for x < 1, may be written as  y x  = AF a, b, c; x  + Bx1−cF a − c + 1, b − c + 1, 2 − c; x ,   18.143   where A and B are arbitrary constants to be ﬁxed by the boundary conditions on the solution. If the solution is to be regular at x = 0, one requires B = 0.  18.10.1 Properties of hypergeometric functions  Since the hypergeometric equation is so general in nature, it is not feasible to present a comprehensive account of the hypergeometric functions. Nevertheless, we outline here some of their most important properties.  Special cases  As mentioned above, the general nature of the hypergeometric equation allows us to write a large number of elementary functions in terms of the hypergeometric functions F a, b, c; x . Such identiﬁcations can be made from the series expansion  18.142  directly, or by transformation of the hypergeometric equation into a more familiar equation, the solutions to which are already known. Some particular examples of well known special cases of the hypergeometric function are as follows:  §  We note that it is also common to denote the hypergeometric function by 2F1 a, b, c; x . This slightly odd-looking notation is meant to signify that, in the coeﬃcient of each power of x, there are two parameters  a and b  in the numerator and one parameter  c  in the denominator.  630   18.10 HYPERGEOMETRIC FUNCTIONS  −1 ln 1 + x ,  −a,  F a, b, b; x  =  1 − x  F 1, 1, 2;−x  = x m→∞ F 1, m, 1; x m  = ex, lim 2 ,− 1 2 ; sin2 x  = cos x, 2 , 1 F  1 2 , p, p; sin2 x  = sec x,  F  1  F  1  F  1  2 , 3 2 , 1 2 ; x2  = x 2 ;−x2  = x 2 , 1, 3 2 ; x2  = 1 2 , 1, 3 2 x  −1 x, −1 x,  −1 sin −1 tan −1 ln[ 1 + x   1 − x ],  F  1 F m + 1,−m, 1;  1 − x  2  = Pm x , F m,−m, 1  2 ;  1 − x  2  = Tm x ,  where m is an integer, Pm x  is the mth Legendre polynomial and Tm x  is the mth Chebyshev polynomial of the ﬁrst kind. Some of these results are proved in exercise 18.11.   cid:1 Show that F m,−m, 1  2 ;  1 − x  2  = Tm x .  Let us prove this result by transforming the hypergeometric equation. The form of the  result suggests that we should make the substitution x =  1 − z  2 into  18.136 , in which case d dx = −2d dz. Thus, letting u z  = y x  and setting a = m, b = −m and c = 1 2,   18.136  becomes   1 − z    1 + z   2  2   −2 2 d2u  dz2  +  1 2  −  m − m + 1   1 − z  2   −2   du dz  −  m  −m u = 0.   cid:14    cid:13   On simplifying, we obtain  − z its power series solution, and so F m,−m, 1 2 ;  1 − z  2  and Tm z  are equal to within a  which has the form of Chebyshev’s equation,  18.54 . This equation has u z  = Tm z  as   1 − z2   normalisation factor. On comparing the expressions  18.141  and  18.56  at x = 0, i.e. at z = 1, we see that they both have value 1. Hence, the normalisations already agree and we obtain the required result.  cid:2   + m2u = 0,  d2u dz2  du dz  Integral representation  One of the most useful representations for the hypergeometric functions is in terms of an integral, which may be derived using the properties of the gamma and beta functions discussed in section 18.12. The integral representation reads  F a, b, c; x  =  Γ c   Γ b Γ c − b   1  tb−1 1 − t c−b−1 1 − tx   −a dt,   18.144    cid:21   0  and requires c > b > 0 for the integral to converge.  cid:1 Prove the result  18.144 .  From the series expansion  18.142 , we have  F a, b, c; x  =  Γ c   Γ a + n Γ b + n   Γ a Γ b   Γ c + n   xn n!  =  Γ c   Γ a Γ b Γ c − b   Γ a + n B b + n, c − b   xn n!  ,  ∞ cid:4   n=0  ∞ cid:4   n=0  631   SPECIAL FUNCTIONS  where in the second equality we have used the expression  18.165  relating the gamma and beta functions. Using the deﬁnition  18.162  of the beta function, we then ﬁnd   cid:21   ∞ cid:4   n=0  F a, b, c; x  =  Γ c    cid:21  Γ a Γ b Γ c − b   =  Γ c   Γ b Γ c − b   0  Γ a + n   xn n!  1  dt tb−1 1 − t c−b−1  1  tb+n−1 1 − t c−b−1 dt ∞ cid:4   Γ a + n   0   tx n n!  ,  Γ a   n=0  where in the second equality we have rearranged the expression and reversed the order of integration and summation. Finally, one recognises the sum over n as being equal to  −a, and so we obtain the ﬁnal result  18.144 .  cid:2    1 − tx   The integral representation may be used to prove a wide variety of properties of the hypergeometric functions. As a simple example, on setting x = 1 in  18.144 , and using properties of the beta function discussed in section 18.12.2, one quickly ﬁnds that, provided c is not a negative integer or zero and c > a + b,  F a, b, c; 1  =  Γ c Γ c − a − b  Γ c − a Γ c − b   .  Relationships between hypergeometric functions  There exist a great many relationships between hypergeometric functions with diﬀerent arguments. These are most easily derived by making use of the integral representation  18.144  or the series form  18.141 . It is not feasible to list all the relationships here, so we simply note two useful examples, which read  F a, b, c; x  =  1 − x c−a−bF c − a, c − b, c; x ,  cid:7   F   a, b, c; x  =  F a + 1, b + 1, c + 1; x ,  ab c   18.145    18.146   where the prime in the second relation denotes d dx. The ﬁrst result follows straightforwardly from the integral representation using the substitution t =   1 − u   1 − ux , whereas the second result may be proved more easily from the  series expansion.  In addition to the above results, one may also derive relationships between  F a, b, c; x  and any two of the six ‘contiguous functions’ F a ± 1, b, c; x , F a, b± 1, c; x  and F a, b, c ± 1; x . These ‘contiguous relations’ serve as the recurrence  relations for the hypergeometric functions. An example of such a relationship is   c− a F a− 1, b, c; x  +  2a− c− ax + bx F a, b, c; x  + a x− 1 F a + 1, b, c; x  = 0.  Repeated application of such relationships allows one to express F a + l, b + m, c + n; x , where l, m, n are integers  with c + n not equalling a negative integer or zero , as a linear combination of F a, b, c; x  and one of its contiguous functions.  632   18.11 CONFLUENT HYPERGEOMETRIC FUNCTIONS  18.11 Conﬂuent hypergeometric functions  The conﬂuent hypergeometric equation has the form   cid:7  cid:7   +  c − x y   cid:7  − ay = 0;  xy   18.147   it has a regular singularity at x = 0 and an essential singularity at x = ∞.  This equation can be obtained by merging two of the singularities of the ordinary hypergeometric equation  18.136 . The parameters a and c are given real numbers.  cid:1 Show that setting x = z b in the hypergeometric equation, and letting b → ∞, yields the  conﬂuent hypergeometric equation.   cid:9    cid:10   Substituting x = z b into  18.136 , with d dx = bd dz, and letting u z  = y x , we obtain  bz  1 − z  b  d2u dz2  + [bc −  a + b + 1 z]  − abu = 0,  du dz  which clearly has regular singular points at z = 0, b and ∞. If we now merge the last two singularities by letting b → ∞, we obtain   cid:7  cid:7   +  c − z u   cid:7  − au = 0,  zu  where the primes denote d dz. Hence u z  must satisfy the conﬂuent hypergeometric equation.  cid:2   In our discussion of Bessel, Laguerre and associated Laguerre functions, it was noted that the corresponding second-order diﬀerential equation in each case had a  single regular singular point at x = 0 and an essential singularity at x = ∞. From  table 16.1, we see that this is also true for the conﬂuent hypergeometric equation. Indeed, this equation can be considered as the ‘canonical form’ for second- order diﬀerential equations with this pattern of singularities. Consequently, as we mention below, the Bessel, Laguerre and associated Laguerre functions can all be written in terms of the conﬂuent hypergeometric functions, which are the solutions of  18.147 .  The solutions of the conﬂuent hypergeometric equation are obtained from  those of the ordinary hypergeometric equation by again letting x → x b and carrying out the limiting process b → ∞. Thus, from  18.141  and  18.143 , two  linearly independent solutions of  18.147  are  when c is not an integer   y1 x  = 1 +  a c  x 1!  +  a a + 1  c c + 1   z2 2!  y2 x  = x1−cM a − c + 1, 2 − c; x ,  + ··· ≡ M a, c; x ,   18.148    18.149   where M a, c; x  is called the conﬂuent hypergeometric function  or Kummer § function . and y2 x  is singular when c = 2, 3, 4, . . . . Thus, it is conventional to take the  It is worth noting, however, that y1 x  is singular when c = 0,−1,−2, . . .  §  We note that an alternative notation for the conﬂuent hypergeometric function is 1F1 a, c; x .  633   SPECIAL FUNCTIONS  second solution to  18.147  as a linear combination of  18.148  and  18.149  given by  U a, c; x  ≡ π  sin πc  M a, c; x   Γ a − c + 1 Γ c   − x1−c M a − c + 1, 2 − c; x   Γ a Γ 2 − c    cid:14   .   cid:13   This has a well behaved limit as c approaches an integer.  18.11.1 Properties of conﬂuent hypergeometric functions  The properties of conﬂuent hypergeometric functions can be derived from those  of ordinary hypergeometric functions by letting x → x b and taking the limit b → ∞, in the same way as both the equation and its solution were derived. A  general procedure of this sort is called a conﬂuence process.  Special cases  The general nature of the conﬂuent hypergeometric equation allows one to write a large number of elementary functions in terms of the conﬂuent hypergeometric functions M a, c; x . Once again, such identiﬁcations can be made from the series expansion  18.148  directly, or by transformation of the conﬂuent hypergeometric equation into a more familiar equation for which the solutions are already known. Some particular examples of well known special cases of the conﬂuent hypergeometric function are as follows:  M a, a; x  = ex,  M −n, 1; x  = Ln x , M −n, 1  2 ; x2  =   −1 nn!   2n !  ex sinh x  ,  x  M 1, 2; 2x  =  M −n, m + 1; x  = M −n, 3  n!m!   n + m !   −1 nn! √ 2 2n + 1 !  Lm  n  x ,  H2n+1 x   ,  x  H2n x ,  2 ; x2 , = 2 ;−x2  = −ν Jν x , M  1 2 , 3  M ν + 1  2 , 2ν + 1; 2ix  = ν!eix   x 2 where n and m are integers, Lm n  x  is an associated Legendre polynomial, Hn x  is a Hermite polynomial, Jν x  is a Bessel function and erf x  is the error function discussed in section 18.12.4.  π 2x  erf x ,     Integral representation  Using the integral representation  18.144  of the ordinary hypergeometric func- tion, exchanging a and b and carrying out the process of conﬂuence gives  M a, c, x  =  Γ c   Γ a Γ c − a   1  etxta−1 1 − t c−a−1 dt,   18.150    cid:21   0  634   18.12 THE GAMMA FUNCTION AND RELATED FUNCTIONS  which converges provided c > a > 0.  cid:1 Prove the result  18.150 .   cid:21   Since F a, b, c; x  is unchanged by swapping a and b, we may write its integral representation  18.144  as  F a, b, c; x  =  Setting x = z b and taking the limit b → ∞, we obtain  0  Γ c   Γ a Γ c − a   cid:21   1  ta−1 1 − t c−a−1 1 − tx   −b dt.   cid:7    cid:8 −b  M a, c; z  =  Γ c   Γ a Γ c − a   1  ta−1 1 − t c−a−1 lim b→∞  0  1 − tz  b  dt.  Since the limit is equal to etz, we obtain result  18.150 .  cid:2   Relationships between conﬂuent hypergeometric functions  A large number of relationships exist between conﬂuent hypergeometric functions with diﬀerent arguments. These are straightforwardly derived using the integral representation  18.150  or the series form  18.148 . Here, we simply note two useful examples, which read  M a, c; x  = exM c − a, c;−x ,  cid:7   M   a, c; x  =  M a + 1, c + 1; x ,  a c   18.151    18.152   where the prime in the second relation denotes d dx. The ﬁrst result follows straightforwardly from the integral representation, and the second result may be proved from the series expansion  see exercise 18.19 .  In an analogous manner to that used for the ordinary hypergeometric func- tions, one may also derive relationships between M a, c; x  and any two of the  four ‘contiguous functions’ M a ± 1, c; x  and M a, c ± 1; x . These serve as the  recurrence relations for the conﬂuent hypergeometric functions. An example of such a relationship is   c − a M a − 1, c; x  +  2a − c + x M a, c; x  − aM a + 1, c; x  = 0.  18.12 The gamma function and related functions  Many times in this chapter, and often throughout the rest of the book, we have made mention of the gamma function and related functions such as the beta and error functions. Although not derived as the solutions of important second-order ODEs, these convenient functions appear in a number of contexts, and so here we gather together some of their properties. This ﬁnal section should be regarded merely as a reference containing some useful relations obeyed by these functions; a minimum of formal proofs is given.  635   The gamma function Γ n  is deﬁned by  which converges for n > 0, where in general n is a real number. Replacing n by n + 1 in  18.153  and integrating the RHS by parts, we ﬁnd  18.12.1 The gamma function  SPECIAL FUNCTIONS   cid:21  ∞  0  xn−1e  −x dx,  Γ n  =   cid:21  ∞  cid:18 −xne  cid:21  ∞  0  =  = n  0   cid:21  ∞   cid:19 ∞ −x dx xne −x 0 + xn−1e −x dx,  0  nxn−1e  −x dx  Γ n + 1  =  from which we obtain the important result  From  18.153 , we see that Γ 1  = 1, and so, if n is a positive integer,  Γ n + 1  = nΓ n .  Γ n + 1  = n!.  In fact, equation  18.155  serves as a deﬁnition of the factorial function even for non-integer n. For negative n the factorial function is deﬁned by  n! =   n + m !   n + m  n + m − 1 ···  n + 1   ,  where m is any positive integer that makes n + m > 0. Diﬀerent choices of m   > −n  do not lead to diﬀerent values for n!. A plot of the gamma function is  given in ﬁgure 18.9, where it can be seen that the function is inﬁnite for negative integer values of n, in accordance with  18.156 . For an extension of the factorial function to complex arguments, see exercise 18.15.  By letting x = y2 in  18.153 , we immediately obtain another useful represen-  tation of the gamma function given by  Setting n = 1  2 we ﬁnd the result  Γ n  = 2   cid:5    cid:6   Γ  1 2  = 2   cid:21  ∞  0  −y2  dy.  y2n−1e  cid:21  ∞  −y2  e  dy =  −y2  −∞ e  √  dy =  π,  where have used the standard integral discussed in section 6.4.2. From this result, Γ n  for half-integral n can be found using  18.154 . Some immediately derivable factorial values of half integers are   cid:5    cid:6   √   cid:5    cid:6    cid:5 − 3  2   cid:6  √ ! = −2  π,   cid:6    cid:5 − 1  2  ! =  π,  1 2  ! = 1 2  π,  3 2  ! = 3 4  π.  √   cid:21  ∞  0  √  636   18.153    18.154    18.155    18.156    18.157    18.12 THE GAMMA FUNCTION AND RELATED FUNCTIONS  −4  −3  −2  −1  1  2  3  4  n  Γ     n 6  4  2  −2 −4 −6  Figure 18.9 The gamma function Γ n .  Moreover, it may be shown for non-integral n that the gamma function satisﬁes the important identity  This is proved for a restricted range of n in the next section, once the beta function has been introduced.  It can also be shown that the gamma function is given by − 139  √  −n  1  Γ n + 1  =  2πn nne  1 +  +  288n2  51 840n3 + . . .  = n!,   18.159   which is known as Stirling’s asymptotic series. For large n the ﬁrst term dominates, and so  π  .  sin nπ  Γ n Γ 1 − n  =  cid:7   1 12n  n! ≈ √  2πn nne  −n;   18.158    cid:8    18.160   this is known as Stirling’s approximation. This approximation is particularly useful in statistical thermodynamics, when arrangements of a large number of particles are to be considered.   cid:1 Prove Stirling’s approximation n! ≈ √ From  18.153 , the extended deﬁnition of the factorial function  which is valid for n > −1   cid:21  ∞  −n for large n.   cid:21  ∞  is given by  2πn nne   18.161   −x dx =  xne  en ln x−x dx.  n! =  0  0  637   SPECIAL FUNCTIONS   cid:10    cid:9  1 + − y2 2n2  y n  +  y3 3n3  If we let x = n + y, then  ln x = ln n + ln  = ln n +  y n   cid:21  ∞  −n   cid:7    cid:13   cid:21  ∞  −∞ e  Substituting this result into  18.161 , we obtain  n! =  exp  n  ln n +  − y2 2n2  y n  + ···  − n − y  dy.  Thus, when n is suﬃciently large, we may approximate n! by  n! ≈ en ln n−n  −y2  2n  dy = en ln n−n  2πn =  √  √ 2πn nne  −n,  which is Stirling’s approximation  18.160 .  cid:2   − ··· .  cid:8    cid:14   The beta function is deﬁned by  18.12.2 The beta function   cid:21   0  B m, n  =   cid:21   0  B m, n  =  1  xm−1 1 − x n−1 dx,   18.162   which converges for m > 0, n > 0, where m and n are, in general, real numbers.  By letting x = 1 − y in  18.162  it is easy to show that B m, n  = B n, m . Other  useful representations of the beta function may be obtained by suitable changes of variable. For example, putting x =  1 + y    cid:21  ∞  0  −1 in  18.162 , we ﬁnd that yn−1 dy  1 + y m+n .  Alternatively, if we let x = sin2 θ in  18.162 , we obtain immediately  B m, n  = 2  π 2  sin2m−1 θ cos2n−1 θ dθ.  The beta function may also be written in terms of the gamma function as   18.163    18.164    18.165   B m, n  =  Γ m Γ n  Γ m + n   .   cid:1 Prove the result  18.165 .  Using  18.157 , we have   cid:21  ∞  cid:21  ∞  cid:21  ∞  0  Γ n Γ m  = 4  x2n−1e  −x2  dx  x2n−1y2m−1e  = 4  0  0  638   cid:21  ∞  0  dy  −y2  y2m−1e − x2+y2  dx dy.   18.12 THE GAMMA FUNCTION AND RELATED FUNCTIONS  Changing variables to plane polar coordinates  ρ, φ  given by x = ρ cos φ, y = ρ sin φ, we obtain  Γ n Γ m  = 4  π 2  ρ2 m+n−1 e  −ρ2  sin2m−1 φ cos2n−1 φ ρ dρ dφ  = 4  π 2  sin2m−1 φ cos2n−1 φ dφ  ρ2 m+n −1e  −ρ2  dρ   cid:21  ∞  0   cid:21   cid:21   0  0  where in the last line we have used the results  18.157  and  18.164 .  cid:2   = B m, n Γ m + n ,  The result  18.165  is useful in proving the identity  18.158  satisﬁed by the  gamma function, since  Γ n Γ 1 − n  = B 1 − n, n  =  yn−1 dy 1 + y  ,  where, in the second equality, we have used the integral representation  18.163 . For 0 < n < 1 this integral can be evaluated using contour integration and has the value π  sin nπ   see exercise 24.19 , thereby proving result  18.158  for this range of n. Extensions to other ranges require more sophisticated methods.   cid:21  ∞  0   cid:21  ∞  0  18.12.3 The incomplete gamma function  In the deﬁnition  18.153  of the gamma function, we may divide the range of integration into two parts and write  Γ n  =  x  un−1e  −u du +  un−1e  −u du ≡ γ n, x  + Γ n, x ,   cid:21   0   cid:21  ∞  x   18.166   whereby we have deﬁned the incomplete gamma functions γ n, x  and Γ n, x , respectively. The choice of which of these two functions to use is merely a matter of convenience.  cid:1 Show that if n is a positive integer  Γ n, x  =  n − 1 !e −x   cid:21  ∞  n−1 cid:4   k=0  xk k!  .   cid:21  ∞  From  18.166 , on integrating by parts we ﬁnd −udu = xn−1e = xn−1e  un−1e  Γ n, x  =  x  −x +  n − 1  un−2e −x +  n − 1 Γ n − 1, x ,  x  −udu  which is valid for arbitrary n. If n is an integer, however, we obtain  Γ n, x  = e  −x[xn−1 +  n − 1 xn−2 +  n − 1  n − 2 xn−3 + ··· +  n − 1 !]  =  n − 1 ! e −x  n−1 cid:4   k=0  xk k!  ,  639   SPECIAL FUNCTIONS  which is the required result.  cid:2   We note that is it conventional to deﬁne, in addition, the functions  P  a, x  ≡ γ a, x   ,  Γ a   Q a, x  ≡ Γ a, x   ,  Γ a   which are also often called incomplete gamma functions; it is clear that Q a, x  =  1 − P  a, x .  18.12.4 The error function  Finally, we mention the error function, which is encountered in probability theory and in the solutions of some partial diﬀerential equations. The error function is related to the incomplete gamma function by erf x  = γ  1 π and is thus given by  √ 2 , x2    erf x  =  x  −u2  e  du = 1 − 2√  −u2  e  du.   18.167    cid:21   2√  π  0   cid:21  ∞  π  x  From this deﬁnition we can easily see that  erf 0  = 0,  By making the substitution y =  erf −x  = −erf x .  erf ∞  = 1, √  cid:21  √     2u in  18.167 , we ﬁnd −y2 2 dy.  2x  e  2 π  0  erf x  =  The cumulative probability function Φ x  for the standard Gaussian distribution  discussed in section 30.9.1  may be written in terms of the error function as follows:   cid:21   Φ x  =  1√ 2π  =  +  x  −∞ e 1√ 2π  =  +  erf  1 2  1 2 1 2  0  x  e   cid:21  −y2 2 dy  cid:7   cid:8  −y2 2 dy  cid:21  ∞  x√ 2  .  −u2  e  du =  It is also sometimes useful to deﬁne the complementary error function  erfc x  = 1 − erf x  =  2√  π  x  2 , x2 √ Γ  1  .  π   18.168   18.1  Use the explicit expressions  18.13 Exercises  640    cid:2   cid:2  Y 0 0 =  cid:2  1 = ∓ ±1 2 = ∓ ±1  1 4π ,  Y  Y  18.13 EXERCISES  3  8π sin θ exp ±iφ , 8π sin θ cos θ exp ±iφ ,  15  Y 0  1 =  Y 0 2 = ±2 2 =  Y  5  3 4π cos θ, 16π  3 cos2 θ − 1 , 32π sin2 θ exp ±2iφ ,  15   cid:2   cid:2   cid:2   to verify for  cid:2  = 0, 1, 2 that   cid:2  cid:4   m=− cid:2   Y m  cid:2   θ, φ 2 =  2 cid:2  + 1  ,  4π  and so is independent of the values of θ and φ. This is true for any  cid:2 , but a general proof is more involved. This result helps to reconcile intuition with the apparently arbitrary choice of polar axis in a general quantum mechanical system. Express the function  f θ, φ  = sin θ[sin2 θ 2  cos φ + i cos2 θ 2  sin φ] + sin2 θ 2   as a sum of spherical harmonics. Use the generating function for the Legendre polynomials Pn x  to show that   cid:21   18.2  18.3  1  P2n+1 x  dx =  −1 n   2n !  22n+1n! n + 1 !  and that, except for the case n = 0, cid:21   0  P2n x  dx = 0.  18.4  Carry through the following procedure as a proof of the result  1  0   cid:21   1  −1  In =  Pn z Pn z  dz =  2  .  2n + 1   1 − 2zh + h2   −1 2 =  Pn z hn.   a  Square both sides of the generating-function deﬁnition of the Legendre  polynomials,   b  Express the RHS as a sum of powers of h, obtaining expressions for the  coeﬃcients.  Integrate the RHS from −1 to 1 and use the orthogonality property of the   c   Legendre polynomials.   d  Similarly integrate the LHS and expand the result in powers of h.  e  Compare coeﬃcients.  18.5  The Hermite polynomials Hn x  may be deﬁned by  Φ x, h  = exp 2xh − h2  =  1 n!  Hn x hn.  Show that  ∂2Φ ∂x2  − 2x  ∂Φ ∂x  + 2h  = 0,  ∂Φ ∂h  641  ∞ cid:4   n=0  ∞ cid:4   n=0   SPECIAL FUNCTIONS  and hence that the Hn x  satisfy the Hermite equation   cid:7  cid:7  − 2xy   cid:7   y  + 2ny = 0,  where n is an integer ≥ 0.  Use Φ to prove that  cid:7  n x  = 2nHn−1 x ,  18.6   a  H   b  Hn+1 x  − 2xHn x  + 2nHn−1 x  = 0. A charge +2q is situated at the origin and charges of −q are situated at distances ±a from it along the polar axis. By relating it to the generating function for the  Legendre polynomials, show that the electrostatic potential Φ at a point  r, θ, φ  with r > a is given by  ∞ cid:4    cid:9    cid:10   2s  2q  4π cid:4 0r  s=1  a r  Φ r, θ, φ  =  P2s cos θ .  18.7  For the associated Laguerre polynomials, carry through the following exercises.   a  Prove the Rodrigues’ formula  Lm  n  x  =  −m  exx n!  dn dxn   xn+me  −x ,  taking the polynomials to be deﬁned by  Lm  n  x  =   −1 k   n + m !  k! n − k ! k + m !  xk.  n cid:4   k=0   b  Prove the recurrence relations   n + 1 Lm  n  x  −  n + m Lm n−1 x , but this time taking the polynomial as deﬁned by  n+1 x  =  2n + m + 1 − x Lm n  x  −  n + m Lm  cid:7  x Lm n     x  = nLm  n−1 x ,  n  x  =  −1 m dm  Lm  dxm  Ln+m x   or the generating function.  18.8  The quantum mechanical wavefunction for a one-dimensional simple harmonic oscillator in its nth energy level is of the form  where Hn x  is the nth Hermite polynomial. The generating function for the polynomials is   a  Find Hi x  for i = 1, 2, 3, 4.   b  Evaluate by direct calculation cid:21  ∞  ψ x  = exp −x2 2 Hn x ,  G x, h  = e2hx−h2  =  Hn x   n!  hn.  ∞ cid:4   n=0  −x2  −∞ e  Hp x Hq x  dx,  642  √  i  for p = 2, q = 3;  ii  for p = 2, q = 4;  iii  for p = q = 3. Check your answers against the expected values 2p p! π δpq.   18.13 EXERCISES   cid:21  ∞  [ You will ﬁnd it convenient to use −x2  −∞ x2ne  dx =  √  2n !  π  22nn!  18.9  for integer n ≥ 0. ] By initially writing y x  as x1 2f x  and then making subsequent changes of variable, reduce Stokes’ equation,  to Bessel’s equation. Hence show that a solution that is ﬁnite at x = 0 is a multiple of x1 2J1 3  2 3 By choosing a suitable form for h in their generating function,  18.10  √ λx3 .  d2y dx2   cid:13    cid:7   + λxy = 0,   cid:8  cid:14   ∞ cid:4   G z, h  = exp  z 2  h − 1  h  Jn z hn,  =  n=−∞  show that integral repesentations of the Bessel functions of the ﬁrst kind are given, for integral m, by  J2m z  =  cos z cos θ  cos 2mθ dθ  J2m+1 z  =  cos z cos θ  sin 2m + 1 θ dθ  m ≥ 1, m ≥ 0.   cid:21   −1 m  −1 m+1  π  π  2π   cid:21   0  2π  0  18.11  Identify the series for the following hypergeometric functions, writing them in terms of better known functions:   a  F a, b, b; z ,   b  F 1, 1, 2;−x , 2 ;−x2 , 2 , 1, 3  c  F  1  d  F  1 2 , 3 2 , 1 2 ; x2 ,  e  F −a, a, 1 2 ; sin2 x ; this is a much more diﬃcult exercise. By making the substitution z =  1 − x  2 and suitable choices for a, b and c,  18.12  convert the hypergeometric equation,  z 1 − z   d2u dz2 into the Legendre equation,  + [ c −  a + b + 1 z ]  − abu = 0,  du dz  Hence, using the hypergeometric series, generate the Legendre polynomials P cid:2  x  for the integer values  cid:2  = 0, 1, 2, 3. Comment on their normalisations. Find a change of variable that will allow the integral  18.13  +  cid:2   cid:2  + 1 y = 0.   1 − x2   d2y dx2  dy dx  − 2x  cid:21  ∞  √ u − 1  u + 1 2 du  I =  1  18.14  to be expressed in terms of the beta function, and so evaluate it.  Prove that, if m and n are both greater than −1, then  I =  0   au2 + b  m+n+2  2 du =  um  Γ[ 1  2  m + 1  ] Γ[ 1 2a m+1  2 b n+1  2 Γ[ 1  2  n + 1  ] 2  m + n + 2  ]  .   cid:21  ∞  643   SPECIAL FUNCTIONS   cid:21  ∞   u + 2 2  u2 + 4 5 2 du.  18.15  Deduce the value of  J =  0  The complex function z! is deﬁned by −u du   cid:21  ∞ For Re z ≤ −1 it is deﬁned by  z! =  uze  0  for Re z > −1.  z! =   z + n !   z + n  z + n − 1 ···  z + 1   ,  where n is any  positive  integer > −Re z. Being the ratio of two polynomials, z!  is analytic everywhere in the ﬁnite complex plane except at the poles that occur when z is a negative integer.   a  Show that the deﬁnition of z! for Re z ≤ −1 is independent of the value of  b  Prove that the residue of z! at the pole z = −m, where m is an integer > 0, For −1 < Re z < 1, use the deﬁnition and value of the beta function to show  is  −1 m−1  m − 1 !.  n chosen.  18.16  that  Contour integration gives the value of the integral on the RHS of the above  equation as πz cosec πz. Use this to deduce the value of  − 1 2  !.  18.17  The integral   cid:21  ∞  uz   1 + u 2 du.  0  −k2 e k2 + a2 dk,   ∗   z!  −z ! =  cid:21  ∞  I =  −∞   cid:21  ∞  0  J =  eiu k+ia  du,  in which a > 0, occurs in some statistical mechanics problems. By ﬁrst considering the integral  and a suitable variation of it, show that I =  π a  exp a2  erfc a , where erfc x  is the complementary error function. Consider two series expansions of the error function as follows.  18.18   a  Obtain a series expansion of the error function erf x  in ascending powers of x. How many terms are needed to give a value correct to four signiﬁcant ﬁgures for erf 1 ?   b  Obtain an asymptotic expansion that can be used to estimate erfc x  for  large x  > 0  in the form of a series  erfc x  = R x  = e  −x2  ∞ cid:4   n=0  an xn  .  Consider what bounds can be put on the estimate and at what point the inﬁnite series should be terminated in a practical estimate. In particular, estimate erfc 1  and test the answer for compatibility with that in part  a .  18.19  For the functions M a, c; z  that are the solutions of the conﬂuent hypergeometric equation,  644   18.13 EXERCISES   a  use their series representation to prove that  b  d dz  M a, c; z  = a M a + 1, c + 1; z ;   b  use an integral representation to prove that  M a, c; z  = ezM c − a, c;−z .  18.20  The Bessel function Jν z  can be considered as a special case of the solution M a, c; z  of the conﬂuent hypergeometric equation, the connection being  M a, ν + 1;−z a   lim a→∞  Γ ν + 1   −ν 2Jν 2  = z  √ z .  Prove this equality by writing each side in terms of an inﬁnite series and showing that the series are the same. Find the diﬀerential equation satisﬁed by the function y x  deﬁned by  18.21  y x  = Ax  −n  x  −ttn−1 dt ≡ Ax  e  −nγ n, x ,  and, by comparing it with the conﬂuent hypergeometric function, express y as a multiple of the solution M a, c; z  of that equation. Determine the value of A that makes y equal to M. Show, from its deﬁnition, that the Bessel function of the second kind, and of integral order ν, can be written as  18.22   cid:21   0   cid:13    cid:14   .  µ=ν  Yν z  =  1 π  ∂Jµ z   ∂µ  −  −1 ν ∂J−µ z   cid:10   ∂µ   cid:9   z 2  Jν z  ln  + g ν, z ,   cid:9    cid:10   z 2  Yν  z  =  Jν z  ln  + h ν, z ,  2 π  Using the explicit series expression for Jµ z , show that ∂Jµ z  ∂µ can be written as  and deduce that Yν z  can be expressed as  where h ν, z , like g ν, z , is a power series in z.  18.23  Prove two of the properties of the incomplete gamma function P  a, x2  as follows.   a  By considering its form for a suitable value of a, show that the error function  can be expressed as a particular case of the incomplete gamma function.   b  The Fresnel integrals, of importance in the study of the diﬀraction of light,  are given by  C x  =  cos  t2  dt,  S  x  =  sin  t2  dt.  Show that they can be expressed in terms of the error function by   cid:21   x  0   cid:10    cid:9   π 2  C x  + iS  x  = A erf   cid:21   x  0   cid:10   π 2   cid:9   cid:14   ,   cid:13  √   1 − i x  π 2  where A is a  complex  constant, which you should determine. Hence express C x  + iS  x  in terms of the incomplete gamma function.  645   SPECIAL FUNCTIONS  18.24  The solutions y x, a  of the equation  −   1  d2y dx2  4 x2 + a y = 0   ∗   are known as parabolic cylinder functions.  solutions:  i  y a,−x ,  ii  y −a, x ,  iii  y a, ix  and  iv  y −a, ix .   a  If y x, a  is a solution of  ∗ , determine which of the following are also  b  Show that one solution of  ∗ , even in x, is −x2 4M  1  y1 x, a  = e  2 a + 1  4 , 1  2 , 1  2 x2 ,  where M α, c, z  is the conﬂuent hypergeometric function satisfying  z  d2M dz2  +  c − z   dM dz  − αM = 0.  You may assume  or prove  that a second solution, odd in x, is given by y2 x, a  = xe  −x2 4M  1  2 a + 3  4 , 3  2 , 1  2 x2 .   c  Find, as an inﬁnite series, an explicit expression for ex2 4y1 x, a .  d  Using the results from part  a , show that y1 x, a  can also be written as  2 a + 1  e  By making a suitable choice for a deduce that  y1 x, a  = ex2 4M − 1  cid:30  ∞ cid:4   2 ,− 1 4 , 1  2 x2 .  5  bn x2n  2n !  1 +  n=1  r=1 2r − 3 2  .  n  where bn =  = ex2 2  1 +  ∞ cid:4   n=1   −1 n bn x2n   2n !   cid:31   ,  18.14 Hints and answers  18.1 18.3  18.5  18.7  18.9  18.11  18.13  Note that taking the square of the modulus eliminates all mention of φ. Integrate both sides of the generating function deﬁnition from x = 0 to x = 1, and then expand the resulting term,  1 + h2 1 2, using a binomial expansion. Show that 1 2Cm can be written as [  −1 m−1 2m − 2 ! ] [ 22m−1m! m − 1 ! ].  Prove the stated equation using the explicit closed form of the generating function. Then substitute the series and require the coeﬃcient of each power of h to vanish.  b  Diﬀerentiate result  a  and then use  a  again to replace the derivatives. −x as a  a  Write the result of using Leibnitz’ theorem on the product of xn+m and e ﬁnite sum, evaluate the separated derivatives, and then re-index the summation.  b  For the ﬁrst recurrence relation, diﬀerentiate the generating function with respect to h and then use the generating function again to replace the exponential. Equating coeﬃcients of hn then yields the result. For the second, diﬀerentiate the corresponding relationship for the ordinary Laguerre polynomials m times.  cid:7  3 λ1 2u = v; then v x2f satisﬁes Bessel’s equation with ν = 1 3 .  a   1 − z  −1 ln 1 + x .  c  Compare the calculated coeﬃcients with those 2 ;−x2  = x −1 x.  e  Note that a term −1 x.  d  x −1 tan of tan containing x2n can only arise from the ﬁrst n + 1 terms of an expansion in powers of sin2 x; make a few trials. F −a, a, 1 Looking for f x  = u such that u + 1 is an inverse power of x with f 0  = ∞ and  +  λx3 − 1 −a.  b  x 2 , 1, 3  4  f = 0. Then, in turn, set x3 2 = u, and 2  2 ; sin2 x  = cos 2ax.  −1 x. F  1  −1 sin  + xf   cid:7  cid:7   −1 − 1. I = B  1 2 , 3  2     √ 2 = π  2  √ 2 .  f 1  = 1 leads to f x  = 2x  646   18.14 HINTS AND ANSWERS  18.15  18.17  18.19  18.21  18.23  is unity, independent of the actual values of m and n.   a  Show that the ratio of two deﬁnitions based on m and n, with m > n > −Re z,  b  Consider the limit as z → −m of  z + m z!, with the deﬁnition of z! based on  cid:1  ∞ 0 exp[−iu k− ia  ] du to express I as the sum of two double integral expressions.  n where n > m. Express the integrand in partial fractions and use J, as given, and J  Reduce them using the standard Gaussian integral, and then make a change of variable 2v = u + 2a.  b  Using the representation  =   cid:7   M a, b; z  =  Γ b   Γ b − a  Γ a   1  ezt ta−1  1 − t b−a−1 dt   cid:21   0  allows the equality to be established, without actual integration, by changing the  integration variable to s = 1 − t.  cid:7  ny = 0; M n, n + 1;−x . Comparing the expansion of the hypergeometric series  −x to obtain xy   x  and then eliminate x  +  n + 1 + x y  Calculate y   x  and y  −1e  +   cid:7  cid:7    cid:7  cid:7    cid:7   with the result of term by term integration of the expansion of the integrand shows that A = n. √ t. Now choose a so that 2 a − 1  + 1 = 0; erf x  = P   1  a  If the dummy variable in the incomplete gamma function is t, make the change 2 , x2 . of variable y = + √ π 1 − i s, and note that  1 − i 2 = −2i. A =  1 + i  2. From  b  Change the integration variable u in the standard representation of the RHS to s, given by u = 1 2 part  a , C x  + iS  x  = 1  2  1 + i P   1  2 ,− 1  2 πi x2 .  647   19  Quantum operators  Although the previous chapter was principally concerned with the use of linear operators and their eigenfunctions in connection with the solution of given diﬀerential equations, it is of interest to study the properties of the operators themselves and determine which of them follow purely from the nature of the operators, without reference to speciﬁc forms of eigenfunctions.  19.1 Operator formalism  The results we will obtain in this chapter have most of their applications in the ﬁeld of quantum mechanics and our descriptions of the methods will reﬂect this. In particular, when we discuss a function ψ that depends upon variables such as space coordinates and time, and possibly also on some non-classical variables, ψ will usually be a quantum-mechanical wavefunction that is being used to describe  the state of a physical system. For example, the value of ψ2 for a particular  set of values of the variables is interpreted in quantum mechanics as being the probability that the system’s variables have that set of values.  To this end, we will be no more speciﬁc about the functions involved than attaching just enough labels to them that a particular function, or a particular set of functions, is identiﬁed. A convenient notation for this kind of approach is that already hinted at, but not speciﬁcally stated, in subsection 17.1, where the deﬁnition of an inner product is given. This notation, often called the Dirac  must not be thought of as completely analogous to physical vectors. Quantum  notation, denotes a state whose wavefunction is ψ by  ψ cid:21 ; since ψ belongs to a vector space of functions,  ψ cid:21  is known as a ket vector. Ket vectors, or simply kets, mechanics associates the same physical state with keiθ ψ cid:21  as it does with  ψ cid:21  as 0. On the other hand, the combination c1 ψ1 cid:21  + c2 ψ2 cid:21 , where  ψ1 cid:21  and  ψ2 cid:21   for all real k and θ and so there is no loss of generality in taking k as 1 and θ  648   19.1 OPERATOR FORMALISM  represent diﬀerent states, is a ket that represents a continuum of diﬀerent states as the complex numbers c1 and c2 are varied.  If we need to specify a state more closely – say we know that it corresponds to a plane wave with a wave number whose magnitude is k – then we indicate  this with a label; the corresponding ket vector would be written as  k cid:21 . If we also knew the direction of the wave then  k cid:21  would be the appropriate form. Clearly,  in general, the more labels we include, the more precisely the corresponding state is speciﬁed.  The Dirac notation for the Hermitian conjugate  dual vector  of the ket vector   cid:1   ψ cid:21  is written as  cid:20 ψ and is known as a bra vector; the wavefunction describing this φ dv is then denoted by  cid:20 ψ φ cid:21  or, more generally if a non-unit weight function  , the complex conjugate of ψ. The inner product of two wavefunctions  state is ψ  ψ  ∗  ∗  ρ is involved, by   cid:20 ψ ρ φ cid:21 ,  evaluated as  ψ   r φ r ρ r  dr.   19.1    cid:21   ∗  Given the  contrived  names for the two sorts of vectors, an inner product like   cid:20 ψ φ cid:21  becomes a particular type of ‘bra c ket’. Despite its somewhat whimsical  construction, this type of quantity has a fundamental role to play in the interpre- tation of quantum theory, because expectation values, probabilities and transition rates are all expressed in terms of them. For physical states the inner product of the corresponding ket with itself, with or without an explicit weight function, is non-zero, and it is usual to take   cid:20 ψψ cid:21  = 1.  Although multiplying a ket vector by a constant does not change the state described by the vector, acting upon it with a more general linear operator A results  in general  in a ket describing a diﬀerent state. For example, if ψ is a state  that is described in one-dimensional x-space by the wavefunction ψ x  = exp −x2   and A is the diﬀerential operator ∂ ∂x, then   ψ1 cid:21  = A ψ cid:21  ≡  Aψ cid:21   is the ket associated with the state whose wavefunction is ψ1 x  = −2x exp −x2 , as  cid:20 φA ψ cid:21  through the equation  clearly a diﬀerent state. This allows us to attach a meaning to an expression such   cid:20 φA ψ cid:21  =  cid:20 φ ψ1 cid:21 ,   19.2   i.e. it is the inner product of  ψ1 cid:21  and  φ cid:21 . We have already used this notation in  equation  19.1 , but there the eﬀect of the operator A was merely multiplication by a weight function.  If it should happen that the eﬀect of an operator acting upon a particular ket  649   QUANTUM OPERATORS  is to produce a scalar multiple of that ket, i.e.  A ψ cid:21  = λ ψ cid:21 ,  more usually, an eigenstate of A, with corresponding eigenvalue λ; to mark this  then, just as for matrices and diﬀerential equations,  ψ cid:21  is called an eigenket or, special property the state will normally be denoted by  λ cid:21 , rather than by the more general  ψ cid:21 . Taking the Hermitian conjugate of this ket vector eigenequation  gives a bra vector equation,   19.3    19.4    cid:20 ψA †  ∗ cid:20 ψ.  = λ  It should be noted that the complex conjugate of the eigenvalue appears in this  equation. Should the action of A on ψ cid:21  produce an unphysical state  usually  one whose wavefunction is identically zero, and is therefore unacceptable as a quantum-mechanical wavefunction because of the required probability interpre-  tation  we denote the result either by 0 or by the ket vector ∅ cid:21  according to context. Formally, ∅ cid:21  can be considered as an eigenket of any operator, but one  for which the eigenvalue is always zero. † If an operator A is Hermitian  A  = A  then its eigenvalues are real and the eigenstates can be chosen to be orthogonal; this can be shown in the same way as in chapter 17  but using a diﬀerent notation . As indicated there, the reality of their eigenvalues is one reason why Hermitian operators form the basis of measurement in quantum mechanics; in that formulation of physics, the eigenvalues of an operator are the only possible values that can be obtained when a measurement of the physical quantity corresponding to the operator is made. Actual individual measurements must always result in real values, even if they are combined in a complex form  x + iy or reiθ  for ﬁnal presentation or analysis, and using only Hermitian operators ensures this. The proof of the reality of the eigenvalues using the Dirac notation is given below in a worked example.  In the same notation the Hermitian property of an operator A is represented  by the double equality   cid:20 A φψ cid:21  =  cid:20 φA ψ cid:21  =  cid:20 φA ψ cid:21 .  It should be remembered that the deﬁnition of an Hermitian operator involves specifying boundary conditions that the wavefunctions considered must satisfy. Typically, they are that the wavefunctions vanish for large values of the spatial variables upon which they depend; this deals with most physical systems since they are nearly all formally inﬁnite in extent. Some model systems require the wavefunction to be periodic or to vanish at ﬁnite values of a spatial variable.  Depending on the nature of the physical system, the eigenvalues of a particular linear operator may be discrete, part of a continuum, or a mixture of both. For example, the energy levels of the bound proton–electron system  the hydrogen atom  are discrete, but if the atom is ionised and the electron is free, the energy  650   19.1 OPERATOR FORMALISM  spectrum of the system is continuous. This system has discrete negative and continuous positive eigenvalues for the operator corresponding to the total energy  the Hamiltonian .   cid:1 Using the Dirac notation, show that the eigenvalues of an Hermitian operator are real. Let  a cid:21  be an eigenstate of Hermitian operator A corresponding to eigenvalue a, then  A a cid:21  = a a cid:21 ,  ⇒  cid:20 aA a cid:21  =  cid:20 aa a cid:21  = a cid:20 a a cid:21 ,  ⇒  cid:20 aA   a − a  ∗ cid:20 a, ∗ cid:20 a a cid:21 , ∗ cid:20 a a cid:21 ,  and = a   cid:20 aA † † a cid:21  = a  cid:20 aA a cid:21  = a   cid:20 a a cid:21  = 0, ∗ ⇒ a = a ∗  , since  cid:20 a a cid:21   cid:3 = 0.  since A is Hermitian.  Hence,  Thus a is real.  cid:2   It is not our intention to describe the complete axiomatic basis of quantum mechanics, but rather to show what can be learned about linear operators, and in particular about their eigenvalues, without recourse to explicit wavefunctions on which the operators act.  Before we proceed to do that, we close this subsection with a number of results, expressed in Dirac notation, that the reader should verify by inspection or by following the lines of argument sketched in the statements. Where a sum over a complete set of eigenvalues is shown, it should be replaced by an integral for those parts of the eigenvalue spectrum that are continuous. With the notation  that an cid:21  is an eigenstate of Hermitian operator A with non-degenerate eigenvalue  an  or, if an is k-fold degenerate, then a set of k mutually orthogonal eigenstates has been constructed and the states relabelled , we have the following results.  A an cid:21  = an an cid:21 ,  cid:20 aman cid:21  = δm n  A cn an cid:21  + cm am cid:21   = cnan an cid:21  + cmam am cid:21    linearity .   orthonormality of eigenstates ,   19.5    19.6   The deﬁnitions of the sum and product of two operators are   A + B  ψ cid:21  ≡ A ψ cid:21  + B ψ cid:21 , ⇒ Ap an cid:21  = ap  AB ψ cid:21  ≡ A B ψ cid:21    an cid:21 .  n    cid:3 = BA ψ cid:21  in general ,   19.7    19.8    19.9   651   QUANTUM OPERATORS  If A an cid:21  = a an cid:21  for all N1 ≤ n ≤ N2, then  N2 cid:4    ψ cid:21  =  dn an cid:21  satisﬁes A ψ cid:21  = a ψ cid:21  for any set of di.  For a general state  ψ cid:21 ,  n=N1  This can also be expressed as the operator identity,  ∞ cid:4    ψ cid:21  =  n=0  n=0  1 =   an cid:21  cid:20 an,  cn an cid:21 , where cn =  cid:20 anψ cid:21 . ∞ cid:4  ∞ cid:4   cid:31  cid:30  ∞ cid:4   cid:20 am ∞ cid:4  ∞ cid:4   ∞ cid:4   an cid:21  cid:20 anψ cid:21  =  cid:31  ∞ cid:4    cid:20 am A an cid:21 cn = ∞ cid:4   ∞ cid:4   cnan cid:21   n=0  n=0  n=0  =  m,n  m,n  m,n  ∗  ∗  ∗  c  c  c  m  m  m  ∗ mcnanδm n =  c  cn2an.  n=0  =  m,n   ψ cid:21  = 1 ψ cid:21  =   cid:30  ∞ cid:4   c  m=0  19.1.1 Commutation and commutators  cn an cid:21 .  ∞ cid:4   n=0  ∗ mcnδm n =  cn2.   19.12    19.10    19.11    19.13   Similarly, the expectation value of the physical variable corresponding to A is   cid:20 ψA ψ cid:21  =   cid:20 am an an cid:21 cn  in the sense that  It also follows that  1 =  cid:20 ψψ cid:21  =  As has been noted above, the product AB of two linear operators may or may not be equal to the product BA. That is  AB ψ cid:21  is not necessarily equal to BA ψ cid:21 .  If A and B are both purely multiplicative operators, multiplication by f r  and g r  say, then clearly the order of the operations is immaterial, the result   f r g r ψ cid:21  being obtained in both cases. However, consider a case in which A wavefunction describing AB ψ cid:21  is  is the diﬀerential operator ∂ ∂x and B is the operator ‘multiply by x’. Then the  ∂ ∂x   xψ x   = ψ x  + x  ∂ψ ∂x  ,  652   19.1 OPERATOR FORMALISM  whilst that for BA ψ cid:21  is simply  which is not the same.  If the result  x  ∂ψ ∂x  ,  AB ψ cid:21  = BA ψ cid:21   is true for all ket vectors  ψ cid:21 , then A and B are said to commute; otherwise they  are non-commuting operators.  A convenient way to express the commutation properties of two linear operators  is to deﬁne their commutator, [ A, B ], by  Clearly two operators that commute have a zero commutator. But, for the example given above we have that  , x  ψ x  =  ψ x  + x  ∂ψ ∂x  −  x  ∂ψ ∂x  = ψ x  = 1 × ψ   cid:14    cid:13   ∂ ∂x  or, more simply, that  [ A, B ] ψ cid:21  ≡ AB ψ cid:21  − BA ψ cid:21 .  cid:7   cid:8    cid:7    cid:8   cid:14    cid:13   ∂ ∂x  , x  = 1;  in words, the commutator of the diﬀerential operator ∂ ∂x and the multiplicative operator x is the multiplicative operator 1. It should be noted that the order of the linear operators is important and that   19.14    19.15    19.16   [ A, B ] = − [ B, A ] .  Clearly any linear operator commutes with itself and some other obvious zero commutators  when operating on wavefunctions with ‘reasonable’ properties  are:  [ A, I ] , where I is the identity operator;  [ An, Am ] , for any positive integers n and m;  [ A, p A  ] , where p x  is any polynomial in x;  [ A, c ] , where A is any linear operator and c is any constant;  [ f x , g x  ] , where the functions are mutiplicative;  [ A x , B y  ] , where the operators act on diﬀerent variables, with   cid:13    cid:14   ∂ ∂x  ,  ∂ ∂y  as a speciﬁc example.  653   QUANTUM OPERATORS  Simple identities amongst commutators include the following:  [ A, B + C ] = [ A, B ] + [ A, C ] ,  [ A + B, C ] = [ A, C ] + [ B, C ] ,  [ A, BC ] = ABC − BCA + BAC − BAC =  AB − BA C + B AC − CA   = [ A, B ] C + B [ A, C ] ,  [ AB, C ] = A [ B, C ] + [ A, C ] B.   19.17    19.18    19.19    19.20    cid:1 If A and B are two linear operators that both commute with their commutator, prove that [ A, Bn ] = nBn−1 [ A, B ] and that [ An, B ] = nAn−1 [ A, B ].  Deﬁne Cn by Cn = [ A, Bn ]. We aim to ﬁnd a reduction formula for Cn:  Cn =   cid:19   cid:19    cid:18   cid:18   A, B Bn−1  A, Bn−1 A, Bn−1   cid:19   cid:18  = [ A, B ] Bn−1 + B = Bn−1 [ A, B ] + B = Bn−1 [ A, B ] + BCn−1, the required reduction formula, = Bn−1 [ A, B ] + B{Bn−2 [ A, B ] + BCn−2}, applying the formula, = 2Bn−1 [ A, B ] + B2Cn−2 = ··· = nBn−1 [ A, B ] + BnC0.  , since [ [ A, B ] , B ] = 0,  , using  19.19 ,  However, C0 = [ A, I ] = 0 and so Cn = nBn−1 [ A, B ].  Using equation  19.16  and interchanging A and B in the result just obtained, we ﬁnd  [ An, B ] = − [ B, An ] = −nAn−1 [ B, A ] = nAn−1 [ A, B ] ,  as stated in the question.  cid:2   As the power of a linear operator can be deﬁned, so can its exponential; this situation parallels that for matrices, which are of course a particular set of operators that act upon state functions represented by vectors. The deﬁnition follows that for the exponential of a scalar or matrix, namely  exp A =   19.21   Related functions of A, such as sin A and cos A, can be deﬁned in a similar way. Since any linear operator commutes with itself, when two functions of it are combined in some way, the result takes a form similar to that for the corresponding functions of scalar quantities. Consider, for example, the function f A  deﬁned by f A  = 2 sin A cos A. Expressing sin A and cos A in terms of their  ∞ cid:4   n=0  An n!  .  654   deﬁning series, we have  Writing m + n as r and replacing n by s, we have  19.1 OPERATOR FORMALISM  f A  = 2   −1 m A2m+1   2m + 1 !   −1 n A2n  .   2n !  ∞ cid:4   n=0   cid:30  r cid:4    cid:31    −1 r−s   2r − 2s + 1 !   −1 s   2s !  f A  = 2  A2r+1  s=0   −1 r cr A2r+1,  = 2  ∞ cid:4   m=0  ∞ cid:4  ∞ cid:4   r=0  r=0  where  cr =  1   2r − 2s + 1 !  2s !  =  1   2r + 1 !  2r+1C2s.  By adding the binomial expansions of 22r+1 =  1 + 1 2r+1 and 0 =  1 − 1 2r+1, it  can easily be shown that  r cid:4   s=0  r cid:4   s=0  r cid:4   s=0  22r+1 = 2  22r  .   2r + 1 !  2r+1C2s ⇒ cr = ∞ cid:4    −1 r A2r+1 22r   2r + 1 !   −1 r  2A 2r+1   2r + 1 !  =  r=0  = sin 2A,  ∞ cid:4   r=0  It then follows that  2 sin A cos A = 2  a not unexpected result.  However, if two  or more  linear operators that do not commute are involved, combining functions of them is more complicated and the results less intuitively obvious. We take as a particular case the product of two exponential functions and, even then, take the simpliﬁed case in which each linear operator commutes with their commutator  so that we may use the results from the previous worked example .   cid:1 If A and B are two linear operators that both commute with their commutator, show that  exp A  exp B  = exp A + B + 1  2 [ A, B ]  .  We ﬁrst ﬁnd the commutator of A and exp λB, where λ is a scalar quantity introduced for  655   later algebraic convenience:   cid:18    cid:19   QUANTUM OPERATORS   cid:17    λB n  n!  ∞ cid:4   n=0  λn n!  A, eλB  =  =  [ A, Bn ]  ∞ cid:4   n=0  A,   cid:16  ∞ cid:4  ∞ cid:4  ∞ cid:4   n=1  n=0  λn n!  λn n!  =  =  = λ  m=0  λmBm  m!  = λeλB [ A, B ] .  nBn−1 [ A, B ] , using the earlier result,  nBn−1 [ A, B ]  [ A, B ] , writing m = n − 1,  Now consider the derivative with respect to λ of the function  f λ  = eλAeλBe  −λ A+B .  In the following calculation we use the fact that the derivative of eλC is CeλC ; this is the same as eλC C, since any two functions of the same operator commute. Diﬀerentiating the three-factor product gives  df dλ  = eλAAeλBe  −λ A+B  + eλAeλBBe = eλA eλBA + λeλB [ A, B ]  e  −λ A+B  + eλAeλB −A − B e −λ A+B  + eλAeλBBe  −λ A+B   −λ A+B   − eλAeλBAe = eλAλeλB [ A, B ] e = λ [ A, B ] f λ .  −λ A+B  − eλAeλBBe −λ A+B   −λ A+B   In the second line we have used the result obtained above to replace AeλB, and in the last line have used the fact that [ A, B ] commutes with each of A and B, and hence with any function of them.  Integrating this scalar diﬀerential equation with respect to λ and noting that f 0  = 1,  we obtain  ln f = 1  2 λ2 [ A, B ] ⇒ eλAeλBe  −λ A+B  = f λ  = e  1  2 λ2[ A,B ].  Finally, post-multiplying both sides of the equation by eλ A+B  and setting λ = 1 yields  eAeB = e  1  2 [ A,B ]+A+B.  cid:2   19.2 Physical examples of operators  We now turn to considering some of the speciﬁc linear operators that play a part in the description of physical systems. In particular, we will examine the properties of some of those that appear in the quantum-mechanical description of the physical world.  As stated earlier, the operators corresponding to physical observables are re- stricted to Hermitian operators  which have real eigenvalues  as this ensures the reality of predicted values for experimentally measured quantities. The two basic  656   19.2 PHYSICAL EXAMPLES OF OPERATORS  quantum-mechanical operators are those corresponding to position r and mo- mentum p. One prescription for making the transition from classical to quantum mechanics is to express classical quantities in terms of these two variables in Cartesian coordinates and then make the component by component substitutions  r → multiplicative operator r  and p → diﬀerential operator − i cid:1 ∇.   19.22   This generates the quantum operators corresponding to the classical quantities. For the sake of completeness, we should add that if the classical quantity contains a product of factors whose corresponding operators A and B do not commute, then the operator 1  2  AB + BA  is to be substituted for the product.  The substitutions  19.22  invoke operators that are closely connected with the two that we considered at the start of the previous subsection, namely x and ∂ ∂x. One, x, corresponds exactly to the x-component of the prescribed quantum position operator; the other, however, has been multiplied by the imaginary  constant −i cid:1 , where  cid:1  is the Planck constant divided by 2π. This has the  subtle   eﬀect of converting the diﬀerential operator into the x-component of an Hermitian operator; this is easily veriﬁed using integration by parts to show that it satisﬁes equation  17.16 . Without the extra imaginary factor  which changes sign under complex conjugation  the two sides of the equation diﬀer by a minus sign.  Making the diﬀerential operator Hermitian does not change in any essential way its commutation properties, and the commutation relation of the two basic quantum operators reads  [ px, x ] =  −i cid:1   ∂ ∂x  , x  = −i cid:1 .   cid:13    cid:19    cid:14    cid:19    19.23    19.24   Corresponding results hold when x is replaced, in both operators, by y or z. However, it should be noted that if diﬀerent Cartesian coordinates appear in the two operators then the operators commute, i.e.   cid:18    cid:18   [ px, y ] = [ px, z ] =  py, x  =  py, z  = [ pz, x ] = [ pz, y ] = 0.  As an illustration of the substitution rules, we now construct the Hamiltonian  the quantum-mechanical energy operator  H for a particle of mass m moving in a potential V  x, y, z  when it has one of its allowed energy values, i.e its  energy is En, where Hψn cid:21  = Enψn cid:21 . This latter equation when expressed in a  particular coordinate system is the Schr¨odinger equation for the particle. In terms of position and momentum, the total classical energy of the particle is given by  E =  + V  x, y, z  =  + V  x, y, z .  p2 2m  y + p2  z  x + p2 p2 2m  Substituting −i cid:1 ∂ ∂x for px  and similarly for py and pz  in the ﬁrst term on the  657   QUANTUM OPERATORS   cid:7   2m  H = −  cid:1 2  cid:7   2m   −i cid:1  2  cid:8    cid:8   RHS gives   −i cid:1  2  2m  ∂ ∂x  ∂ ∂x  +   −i cid:1  2  2m  ∂ ∂y  ∂ ∂y  +  ∂ ∂z  ∂ ∂z  .  The potential energy V , being a function of position only, becomes a purely multiplicative operator, thus creating the full expression for the Hamiltonian,  ∂2 ∂x2 +  ∂2 ∂y2 +  ∂2 ∂z2  + V  x, y, z ,  and giving the corresponding Schr¨odinger equation as  Hψn = −  cid:1 2  2m  ∂2ψn ∂x2 +  ∂2ψn ∂y2 +  ∂2ψn ∂z2  + V  x, y, z ψn = Enψn.  We are not so much concerned in this section with solving such diﬀerential equations, but with the commutation properties of the operators from which they are constructed. To this end, we now turn our attention to the topic of angular momentum, the operators for which can be constructed in a straightforward manner from the two basic sets.  19.2.1 Angular momentum operators  As required by the substitution rules, we start by expressing angular momentum  in terms of the classical quantities r and p, namely L = r × p with Cartesian  components  Lz = xpy − ypx, Lx = ypz − zpy, Ly = zpx − xpz.  Making the substitutions  19.22  yields as the corresponding quantum-mechanical operators   cid:7   cid:7   cid:7   Lz = −i cid:1  Lx = −i cid:1  Ly = −i cid:1   x  y  z  ∂ ∂y  ∂ ∂z  ∂ ∂x  − y − z − x  ∂ ∂x  ∂ ∂y  ∂ ∂z   cid:8   cid:8   cid:8   ,  ,  .   19.25   It should be noted that for xpy, say, x and ∂ ∂y commute, and there is no ambiguity about the way it is to be carried into its quantum form. Further, since the operators corresponding to each of its factors commute and are Hermitian, the operator corresponding to the product is Hermitian. This was shown directly for matrices in exercise 8.7, and can be veriﬁed using equation  17.16 .  The ﬁrst question that arises is whether or not these three operators commute.  658    cid:7   cid:7    cid:7   cid:7    cid:19   19.2 PHYSICAL EXAMPLES OF OPERATORS  Consider ﬁrst  Now consider  LxLy = − cid:1 2 = − cid:1 2  − z  ∂ ∂y  + yz  y  y  ∂ ∂z  ∂ ∂x  ∂ ∂z  z  ∂ ∂x  − yx  − x ∂2 ∂z2   cid:8  cid:7   ∂2  ∂z∂x   cid:8  cid:7    cid:8   + zx  ∂2 ∂y∂z  ∂y∂x  − z2 ∂2  cid:8    cid:8    cid:8   .  .  LyLx = − cid:1 2 = − cid:1 2  z  ∂ ∂x  ∂ ∂z  − x ∂2  zy  ∂x∂z  y  ∂ ∂z  − z − xy  ∂ ∂y ∂2 ∂z2 + x  − z2 ∂2  ∂x∂y  ∂ ∂y  + xz  ∂2 ∂z∂y  These two expressions are not the same. The diﬀerence between them, i.e. the commutator of Lx and Ly, is given by   cid:7    cid:8   = LxLy − LyLx =  cid:1 2  Lx, Ly  x  ∂ ∂y  − y  ∂ ∂x  = i cid:1 Lz.   19.26    cid:18   This, and two similar results obtained by permutting x, y and z cyclically, summarise the commutation relationships between the quantum operators corre- sponding to the three Cartesian components of angular momentum:   cid:18   cid:18    cid:19   cid:19   Lx, Ly  = i cid:1 Lz, = i cid:1 Lx, Ly, Lz [ Lz, Lx ] = i cid:1 Ly.   19.27   As well as its separate components of angular momentum, the total angular  momentum associated with a particular state ψ cid:21  is a physical quantity of interest.  This is measured by the operator corresponding to the sum of squares of its components,  L2 = L2  x + L2  y + L2 z.   19.28   This is an Hermitian operator, as each term in it is the product of two Hermitian operators that  trivially  commute. It might seem natural to want to ‘take the square root’ of this operator, but such a process is undeﬁned and we will not pursue the matter.  We next show that, although no two of its components commute, the total angular momentum operator does commute with each of its components. In the proof we use some of the properties  19.17  to  19.20  and result  19.27 . We begin  659   QUANTUM OPERATORS  with   cid:18    cid:19    cid:18    cid:19   cid:19    cid:18   L2, Lz  =  L2 x + L2  y + L2  z, Lz  = Lx [ Lx, Lz ] + [ Lx, Lz ] Lx   cid:18    cid:19    cid:18    cid:19   + Ly  Ly, Lz  +  Ly, Lz  = Lx −i cid:1  Ly +  −i cid:1  LyLx + Ly i cid:1  Lx +  i cid:1  LxLy + 0  Ly +  L2  z, Lz  = 0.   cid:18   Thus operators L2 and Lz commute and, continuing in the same way, it can be shown that   cid:19    cid:18    cid:19    cid:18    cid:19   L2, Lx  =  L2, Ly  =  L2, Lz  = 0.   19.29   Eigenvalues of the angular momentum operators  We will now use the commutation relations for L2 and its components to ﬁnd the eigenvalues of L2 and Lz, without reference to any speciﬁc wavefunction. In other words, the eigenvalues of the operators follow from the structure of their commutators. There is nothing particular about Lz, and Lx or Ly could equally well have been chosen, though, in general, it is not possible to ﬁnd states that are simultaneously eigenstates of two or more of Lx, Ly and Lz.  To help with the calculation, it is convenient to deﬁne the two operators  U ≡ Lx + iLy  and D ≡ Lx − iLy.  †  These operators are not Hermitian; they are in fact Hermitian conjugates, in that = U, but they do not represent measurable physical quantities. U We ﬁrst note their multiplication and commutation properties:  = D and D  †   cid:18   cid:18   y + i − i  x + L2  Ly, Lx  x + L2  y  Ly, Lx   cid:19   cid:19   UD =  Lx + iLy  Lx − iLy  = L2 DU =  Lx − iLy  Lx + iLy  = L2  = L2 − L2 = L2 − L2  z +  cid:1 Lz,  cid:18  −  cid:1 Lz,  cid:18   z   cid:19   cid:19   [ Lz, U ] = [ Lz, Lx ] + i  [ Lz, D ] = [ Lz, Lx ] − i  Lz, Ly  Lz, Ly  = i cid:1 Ly +  cid:1 Lx =  cid:1 U, = i cid:1 Ly −  cid:1 Lx = − cid:1 D.   19.30    19.31    19.32    19.33   In the same way as was shown for matrices, it can be demonstrated that if two operators commute they have a common set of eigenstates. Since L2 and Lz commute they possess such a set; let one of the set be ψ cid:21  with  L2ψ cid:21  = aψ cid:21   and Lzψ cid:21  = bψ cid:21 .  Now consider the state ψ   cid:7  cid:21  = Uψ cid:21  and the actions of L2 and Lz upon it.  660   19.2 PHYSICAL EXAMPLES OF OPERATORS  Consider ﬁrst L2ψ L2ψ  with U:   cid:7  cid:21 , recalling that L2 commutes with both Lx and Ly and hence  cid:7  cid:21  = L2Uψ cid:21  = UL2ψ cid:21  = Uaψ cid:21  = aUψ cid:21  = aψ   cid:7  cid:21 .   cid:7  cid:21  is also an eigenstate of L2, corresponding to the same eigenvalue as  Lzψ  Thus, ψ ψ cid:21 . Now consider the action of Lz:  cid:7  cid:21  = LzUψ cid:21  =  ULz +  cid:1 U ψ cid:21 , using [ Lz, U ] =  cid:1 U, = Ubψ cid:21  +  cid:1 Uψ cid:21  =  b +  cid:1  Uψ cid:21   cid:7  cid:21 . =  b +  cid:1  ψ   cid:7  cid:21  is also an eigenstate of Lz, but with eigenvalue b +  cid:1 .  Thus, ψ In summary, the eﬀect of U acting upon ψ cid:21  is to produce a new state that has the same eigenvalue for L2 and is still an eigenstate of Lz, though with that eigenvalue increased by  cid:1 . An exactly analogous calculation shows that the eﬀect of D acting upon ψ cid:21  is to produce another new state, one that also has the same eigenvalue for L2 and is also still an eigenstate of Lz, though with the eigenvalue decreased by  cid:1  in this case. For these reasons, U and D are usually known as ladder operators.  It is clear that, by starting from any arbitrary eigenstate and repeatedly applying either U or D, we could generate a series of eigenstates, all of which have the  eigenvalue a for L2, but increment in their Lz eigenvalues by ± cid:1 . However, we square cannot exceed the square of the total angular momentum, i.e. b2 ≤ a. Thus  also have the physical requirement that, for real values of the z-component, its  b has a maximum value c that satisﬁes  c2 ≤ a but   c +  cid:1  2 > a;  let the corresponding eigenstate be ψu cid:21  with Lzψu cid:21  = cψu cid:21 . Now it is still true  that  LzUψu cid:21  =  c +  cid:1  Uψu cid:21 ,  and, to make this compatible with the physical constraint, we must have that  Uψu cid:21  is the zero ket vector ∅ cid:21 . Now, using result  19.31 , we have  −  cid:1 Lz ψu cid:21 , ⇒ 0∅ cid:21  = D∅ cid:21  =  a2 − c2 −  cid:1 c ψu cid:21 ,  DUψu cid:21  =  L2 − L2 ⇒ a = c c +  cid:1  .  z  This gives the relationship between a and c. We now establish the possible forms for c.  If we start with eigenstate ψu cid:21 , which has the highest eigenvalue c for Lz, and  661   QUANTUM OPERATORS  operate repeatedly on it with the  down  ladder operator D, we will generate a  state ψd cid:21  which, whilst still an eigenstate of L2 with eigenvalue a, has the lowest physically possible value, d say, for the eigenvalue of Lz. If this happens after n operations we will have that d = c − n cid:1  and  Lzψd cid:21  =  c − n cid:1  ψd cid:21 .  Arguing in the same way as previously that Dψd cid:21  must be an unphysical ket  vector, we conclude that  0∅ cid:21  = U∅ cid:21  = UDψd cid:21  =  L2 − L2 = [ a −  c − n cid:1  2 +  cid:1  c − n cid:1   ]ψd cid:21   z +  cid:1 Lz ψd cid:21 , using  19.30 ,  ⇒ a =  c − n cid:1  2 −  cid:1  c − n cid:1  .  Equating the two results for a gives  c2 + c cid:1  = c2 − 2cn cid:1  + n2 cid:1 2 − c cid:1  + n cid:1 2, 2c n + 1  = n n + 1  cid:1 ,  c = 1  2 n cid:1 .  Since n is necessarily integral, c is an integer multiple of 1 irrespective of which eigenstate ψ cid:21  we started with, though the actual value of 2 the integer n depends on ψu cid:21  and hence upon ψ cid:21 . Denoting 1  2 n by  cid:2  we can say that the possible eigenvalues of the operator Lz, and hence the possible results of a measurement of the z-component of the angular momentum of a system, are given by   cid:1 . This result is valid    cid:2  − 1  cid:1 ,    cid:2  − 2  cid:1 ,  . . . , − cid:2  cid:1 .   cid:2  cid:1 ,  The value of a for all 2 cid:2  + 1 of the corresponding states,  ψu cid:21 , Dψu cid:21 , D2ψu cid:21 ,  . . . , D2 cid:2 ψu cid:21 ,  is  cid:2   cid:2  + 1  cid:1 2.  The similarity of form between this eigenvalue and that appearing in Legendre’s equation is not an accident. It is intimately connected with the facts  i  that L2 is a measure of the rotational kinetic energy of a particle in a system centred on the origin, and  ii  that in spherical polar coordinates L2 has the same form as the angle-dependent part of ∇2, which, as we have seen, is itself proportional the associated Legendre equation arise naturally when ∇2ψ = f r  is solved in  to the quantum-mechanical kinetic energy operator. Legendre’s equation and  spherical polar coordinates using the method of separation of variables discussed in chapter 21.  The derivation of the eigenvalues  cid:2   cid:2  + 1  cid:1 2 and m cid:1 , with − cid:2  ≤ m ≤  cid:2 , depends  only on the commutation relationships between the corresponding operators. Any  662   19.2 PHYSICAL EXAMPLES OF OPERATORS  other set of four operators with the same commutation structure would result in the same eigenvalue spectrum. In fact, quantum mechanically, orbital angular momentum is restricted to cases in which n is even and so  cid:2  is an integer; this  is in accord with the requirement placed on  cid:2  if solutions to ∇2ψ = f r  that are  ﬁnite on the polar axis are to be obtained. The non-classical notion of internal angular momentum  spin  for a particle provides a set of operators that are able to take both integral and half-integral multiples of  cid:1  as their eigenvalues.  We have already seen that, for a state  cid:2 , m cid:21  that has a z-component of angular momentum m cid:1 , the state U cid:2 , m cid:21  is one with its z-component of angular momentum equal to  m + 1  cid:1 . But the new state ket vector so produced is not necessarily normalised so as to make  cid:20  cid:2 , m + 1  cid:2 , m + 1 cid:21  = 1. We will conclude this discussion of angular momentum by calculating the coeﬃcients µm and νm in the equations  U cid:2 , m cid:21  = µm cid:2 , m + 1 cid:21   and D cid:2 , m cid:21  = νm cid:2 , m − 1 cid:21   on the basis that  cid:20  cid:2 , r   cid:2 , r cid:21  = 1 for all  cid:2  and r. To do so, we consider the inner product I =  cid:20  cid:2 , mDU  cid:2 , m cid:21 , evaluated in two  diﬀerent ways. We have already noted that U and D are Hermitian conjugates and so I can be written as  I =  cid:20  cid:2 , mU  †  U  cid:2 , m cid:21  = µ  ∗   cid:20  cid:2 , m  cid:2 , m cid:21 µm = µm2.  m  But, using equation  19.31 , it can also be expressed as  z  −  cid:1 Lz  cid:2 , m cid:21   I =  cid:20  cid:2 , mL2 − L2 =  cid:20  cid:2 , m cid:2   cid:2  + 1  cid:1 2 − m2 cid:1 2 − m cid:1 2  cid:2 , m cid:21  = [  cid:2   cid:2  + 1  cid:1 2 − m2 cid:1 2 − m cid:1 2 ] cid:20  cid:2 , m  cid:2 , m cid:21  = [  cid:2   cid:2  + 1  − m m + 1  ]  cid:1 2.  Thus we are required to have  µm2 = [  cid:2   cid:2  + 1  − m m + 1  ]  cid:1 2,  but can choose that all µm are real and non-negative  recall that m ≤  cid:2  . A  similar calculation can be used to calculate νm. The results are summarised in the equations   cid:24   cid:24   cid:2   cid:2  + 1  − m m + 1   cid:1   cid:2 , m + 1 cid:21 ,  cid:2   cid:2  + 1  − m m − 1   cid:1   cid:2 , m − 1 cid:21 .  U  cid:2 , m cid:21  = D  cid:2 , m cid:21  =   19.34    19.35   It can easily be checked that U cid:2 ,  cid:2  cid:21  = ∅ cid:21  = D cid:2 ,− cid:2  cid:21 .  663   QUANTUM OPERATORS  19.2.2 Uncertainty principles  The next topic we explore is the quantitative consequences of a non-zero com- mutator for two quantum  Hermitian  operators that correspond to physical variables.  As previously noted, the expectation value in a state ψ cid:21  of the physical quantity A corresponding to the operator A is E[A] =  cid:20 ψ Aψ cid:21 . Any one measurement of  A can only yield one of the eigenvalues of A. But if repeated measurements could be made on a large number of identical systems, a discrete or continuous range of values would be obtained. It is a natural extension of normal data analysis to measure the uncertainty in the value of A by the observed variance in the measured values of A, denoted by  ∆A 2 and calculated as the average value of  A − E[A] 2. The expected value of this variance for the state ψ cid:21  is given by  cid:20 ψ  A − E[A]  2ψ cid:21 .  We now give a mathematical proof that there is a theoretical lower limit for the product of the uncertainties in any two physical quantities, and we start by  proving a result similar to the Schwarz inequality. Let u cid:21  and v cid:21  be any two state vectors and let λ be any real scalar. Then consider the vector w cid:21  = u cid:21  + λv cid:21  and,  in particular, note that  0 ≤  cid:20 w  w cid:21  =  cid:20 u u cid:21  + λ  cid:20 u v cid:21  +  cid:20 v  u cid:21   + λ2 cid:20 v  v cid:21 .  This is a quadratic inequality in λ and therefore the quadratic equation formed by equating the RHS to zero must have no real roots. The coeﬃcient of λ is    cid:20 u v cid:21  + cid:20 v  u cid:21   = 2 Re  cid:20 u v cid:21  and its square is thus ≥ 0. The condition for no real  roots of the quadratic is therefore  0 ≤   cid:20 u v cid:21  +  cid:20 v  u cid:21  2 ≤ 4 cid:20 u u cid:21  cid:20 v  v cid:21 .   19.36   This result will now be applied to state vectors constructed from ψ cid:21 , the state  vector of the particular system for which we wish to establish a relationship be- tween the uncertainties in the two physical variables corresponding to  Hermitian  operators A and B. We take  u cid:21  =  A − E[A]  ψ cid:21   and v cid:21  = i B − E[B]  ψ cid:21 .   19.37   Then   cid:20 u u cid:21  =  cid:20 ψ  A − E[A]  2ψ cid:21  =  ∆A 2,  cid:20 v  v cid:21  =  cid:20 ψ  B − E[B]  2ψ cid:21  =  ∆B 2.  Further,   cid:20 u v cid:21  =  cid:20 ψ   A − E[A] i B − E[B]  ψ cid:21   = i cid:20 ψ  AB  ψ cid:21  − iE[A] cid:20 ψ  B  ψ cid:21  − iE[B] cid:20 ψ  A ψ cid:21  + iE[A]E[B] cid:20 ψ  ψ cid:21  = i cid:20 ψ  AB  ψ cid:21  − iE[A]E[B].  664   19.2 PHYSICAL EXAMPLES OF OPERATORS  In the second line, we have moved expectation values, which are purely numbers,  out of the inner products and used the normalisation condition  cid:20 ψψ cid:21  = 1.  Similarly  Adding these two results gives   cid:20 v  u cid:21  = −i cid:20 ψ  BA ψ cid:21  + iE[A]E[B].   cid:20 u v cid:21  +  cid:20 v  u cid:21  = i cid:20 ψ  AB − BAψ cid:21 ,  and substitution into  19.36  yields  0 ≤  i cid:20 ψ  AB − BAψ cid:21  2 ≤ 4 ∆A 2 ∆B 2  At ﬁrst sight, the middle term of this inequality might appear to be negative, but  this is not so. Since A and B are Hermitian, AB−BA is anti-Hermitian, as is easily  demonstrated. Since i is also anti-Hermitian, the quantity in the parentheses in the middle term is real and its square non-negative. Rearranging the equation and expressing it in terms of the commutator of A and B gives the generalised form  of the Uncertainty Principle. For any particular state ψ cid:21  of a system, this provides  the quantitative relationship between the minimum value that the product of the uncertainties in A and B can have and the expectation value, in that state, of their commutator,   ∆A 2 ∆B 2 ≥ 1   cid:20 ψ  [ A, B ]  ψ cid:21 2.  4   19.38   Immediate observations include the following:   i  If A and B commute there is no absolute restriction on the accuracy with which the corresponding physical quantities may be known. That is not to say that ∆A and ∆B will always be zero, only that they may be.  equation  19.38  is necessarily equal to 1 4 and it is not possible to have ∆A = ∆B = 0.   ii  If the commutator of A and B is a constant, k  cid:3 = 0, then the RHS of k2, whatever the form of ψ cid:21 ,  iii  Since the RHS depends upon ψ cid:21 , it is possible, even for two operators that do not commute, for the lower limit of  ∆A 2 ∆B 2 to be zero. This will occur if the commutator [ A, B ] is itself an operator whose expectation value in the particular state ψ cid:21  happens to be zero.  To illustrate the third of these, we might consider the components of angular momentum discussed in the previous subsection. There, in equation  19.27 , we found that the commutator of the operators corresponding to the x- and y- components of angular momentum is non-zero; in fact, it has the value i cid:1 Lz. This means that if the state ψ cid:21  of a system happened to be such that  cid:20 ψLzψ cid:21  = 0, as it would if, for example, it were the eigenstate of Lz, ψ cid:21  =  cid:2 , 0 cid:21 , then there  would be no fundamental reason why the physical values of both Lx and Ly should not be known exactly. Indeed, if the state were spherically symmetric, and  665   QUANTUM OPERATORS  hence formally an eigenstate of L2 with  cid:2  = 0, all three components of angular momentum could be  and are  known to be zero.   cid:1 Working in one dimension, show that the minimum value of the product ∆px × ∆x for a particle is 1 2 particle whose expectation values for position and momentum are ¯x and ¯p, respectively.   cid:1 . Find the form of the wavefunction that attains this minimum value for a  We have already seen, in  19.23  that the commutator of px and x is −i cid:1 , a constant. Therefore, irrespective of the actual form of ψ cid:21 , the RHS of  19.38  is 1   cid:1 2  see observation  ii  above . Thus, since all quantities are positive, taking the square roots of both sides of the equation shows directly that  4  ∆px × ∆x ≥ 1   cid:1 .  2  Returning to the derivation of the Uncertainty Principle, we see that the inequality becomes an equality only when    cid:20 u v cid:21  +  cid:20 v  u cid:21  2 = 4 cid:20 u u cid:21  cid:20 v  v cid:21 .  The RHS of this equality has the value 4u2v2 and so, by virtue of Schwarz’s inequality,  we have  4 cid:22 u cid:22 2 cid:22 v cid:22 2 =   cid:20 u v cid:21  +  cid:20 v  u cid:21  2 ≤   cid:20 u v cid:21  +  cid:20 v  u cid:21  2 ≤   cid:22 u cid:22  cid:22 v +  cid:22 v cid:22  cid:22 u cid:22  2 = 4 cid:22 u cid:22 2 cid:22 v cid:22 2.  Since the LHS is less than or equal to something that has the same value as itself, all of  We now transform this condition into a constraint that the wavefunction ψ = ψ x   the inequalities are, in fact, equalities. Thus  cid:20 uv cid:21  =  cid:22 u cid:22  cid:22 v cid:22 , showing that u cid:21  and v cid:21  are parallel vectors, i.e. u cid:21  = µv cid:21  for some scalar µ.  cid:8  must satisfy. Recalling the deﬁnitions  19.37  of u cid:21  and v cid:21  in terms of ψ cid:21 , we have − ¯p   cid:7   dψ dx  +  1  cid:1    cid:13    cid:12   d dx  ψ exp  d dx  −i cid:1  ψ = µi x − ¯x ψ, [ µ x − ¯x  − i¯p ]ψ = 0.  cid:14  µ x − ¯x 2  cid:13   cid:13   − i¯px µ x − ¯x 2  − i¯px  cid:14    cid:14  cid:15   cid:7   , giving  = 0,   cid:8   2 cid:1   2 cid:1    cid:1    cid:1   − µ x − ¯x 2  2 cid:1   exp  i¯px  cid:1   .  ψ x  = A exp  The IF for this equation is exp  which, in turn, leads to  From this it is apparent that the minimum uncertainty product ∆px × ∆x is obtained when the probability density ψ x 2 has the form of a Gaussian distribution centred on ¯x. The value of µ is not ﬁxed by this consideration and it could be anything  positive ; a large value for µ would yield a small value for ∆x but a correspondingly large one for ∆px.  cid:2   666   19.2 PHYSICAL EXAMPLES OF OPERATORS  19.2.3 Annihilation and creation operators  As a ﬁnal illustration of the use of operator methods in physics we consider their application to the quantum mechanics of a simple harmonic oscillator  s.h.o. . Although we will start with the conventional description of a one-dimensional oscillator, using its position and momentum, we will recast the description in terms of two operators and their commutator and show that many important conclusions can be reached from studying these alone.  The Hamiltonian for a particle of mass m with momentum p moving in a  one-dimensional parabolic potential V  x  = 1  H =  +  kx2 =  p2 2m  1 2  2 kx2 is p2 1 2m 2  +  mω2x2,  where its classical frequency of oscillation ω is given by ω2 = k m. We recall that the corresponding operators, p and x, do not commute and that [ p, x ] = −i cid:1 .  In analogy with the ladder operators used when discussing angular momentum,  we deﬁne two new operators:     A ≡  mω 2  x +  ip√ 2mω  † ≡  and A  mω 2  x − ip√  .  2mω   19.39   † are Hermitian conjugates, though Since both x and p are Hermitian, A and A neither is Hermitian and they do not represent physical quantities that can be measured.  † Now consider the two products A  † A and AA  :     † A  A = † AA  =  mω 2 mω 2  x2 − ipx  ixp + 2 − ixp 2  +  +  p2 2mω p2 2mω  =  =  2 ipx 2  − i 2 i 2  +  H ω H ω  x2 +  [ p, x ] =  [ p, x ] =  −  cid:1  2  cid:1   +  2  ,  .  H ω H ω  From these it follows that  and that  Further,   cid:18   † cid:19   H = 1 =  cid:1 .  A, A   cid:18   [ H, A ] =   cid:18   † cid:19   = 1 = 1 † =  cid:1 ωA  † 2 ω A  † A + AA      cid:19    cid:18    cid:19    cid:5  † 1 2 ω A † 2 ω A 2 ω − cid:1 A − A cid:1   = − cid:1 ωA.  † A + AA † A  0 +   , A  , A  A + A   cid:18    cid:19   † cid:6   † A  , A  + 0 A   19.40    19.41    19.42    19.43  .  Similarly,  H, A  Before we apply these relationships to the question of the energy spectrum of the s.h.o., we need to prove one further result. This is that if B is an Hermitian  operator then  cid:20 ψ  B2  ψ cid:21  ≥ 0 for any ψ cid:21 . The proof, which involves introducing  667   QUANTUM OPERATORS  an arbitrary complete set of orthonormal base states φi cid:21  and using equation   19.11 , is as follows:   cid:20 ψ  B2  ψ cid:21  =  cid:20 ψ  B × 1 × B  ψ  cid:21   i   cid:4   cid:4   cid:4   cid:4   cid:4   i  i  i  i  =  =  =  =  =   cid:20 ψ  B φi cid:21  cid:20 φi  B ψ cid:21    cid:20 ψ  B φi cid:21  cid:5  cid:20 φi  B ψ cid:21 ∗ cid:6 ∗  cid:20 ψ  B φi cid:21  cid:5  cid:20 ψ  B † φi cid:21  cid:6 ∗   cid:20 ψ  B φi cid:21  cid:20 ψ  B φi cid:21 ∗  cid:20 ψ  B φi cid:21 2 ≥ 0.  ,  since B is Hermitian,  We note, for future reference, that the Hamiltonian H for the s.h.o. is the sum of  two terms each of this form and therefore conclude that  cid:20 ψHψ cid:21  ≥ 0 for all ψ cid:21 .  Let the normalised ket vector n cid:21   or En cid:21   denote the nth energy state of the s.h.o.  The energy spectrum of the simple harmonic oscillator  with energy En. Then it must be an eigenstate of the  Hermitian  Hamiltonian H and satisfy  Hn cid:21  = Enn cid:21  with  cid:20 mn cid:21  = δmn. Now consider the state An cid:21  and the eﬀect of H upon it:  HAn cid:21  = AHn cid:21  −  cid:1 ωAn cid:21 , = AEnn cid:21  −  cid:1 ωAn cid:21  =  En −  cid:1 ω An cid:21 .  using  19.42 ,  Thus An cid:21  is an eigenstate of H corresponding to energy En −  cid:1 ω and must be some multiple of the normalised ket vector En −  cid:1 ω cid:21 , i.e.  A En cid:21  ≡ An cid:21  = cnEn −  cid:1 ω cid:21 ,  where cn is not necessarily of unit modulus. Clearly, A is an operator that generates a new state that is lower in energy by  cid:1 ω; it can thus be compared to the operator D, which has a similar eﬀect in the context of the z-component of angular momentum. Because it possesses the property of reducing the energy of the state by  cid:1 ω, which, as we will see, is one quantum of excitation energy for the oscillator, the operator A is called an annihilation operator. Repeated application of A, m times say, will produce a state whose energy is m cid:1 ω lower than that of the original:  AmEn cid:21  = cncn−1 ··· cn−m+1En − m cid:1 ω cid:21 .   19.44   668   19.2 PHYSICAL EXAMPLES OF OPERATORS  † parallels the operator U of our angular In a similar way it can be shown that A momentum discussion and creates an additional quantum of energy each time it is applied:  †  A   mEn cid:21  = dndn+1 ··· dn+m−1En + m cid:1 ω cid:21 .   19.45   It is therefore known as a creation operator.  As noted earlier, the expectation value of the oscillator’s energy operator   cid:20 ψHψ cid:21  must be non-negative, and therefore it must have a lowest value. Let this be E0, with corresponding eigenstate 0 cid:21 . Since the energy-lowering property have that A0 cid:21  = ∅ cid:21 . It then follows from  19.40  that  of A applies to any eigenstate of H, in order to avoid a contradiction we must  † A + AA  H0 cid:21  = 1 = 1 = 0 + 0 + 1 2  † 2 ω A A0 cid:21  + 1 † 2 ωA  cid:1 ω0 cid:21 .   0 cid:21  † 2 ω A  A +  cid:1  0 cid:21 ,  using  19.41 ,   19.46   This shows that the commutator structure of the operators and the form of the Hamiltonian imply that the lowest energy  its ground-state energy  is 1  cid:1 ω; this 2 is a result that has been derived without explicit reference to the corresponding wavefunction. This non-zero lowest value for the energy, known as the zero-point energy of the oscillator, and the discrete values for the allowed energy states are quantum-mechanical in origin; classically such an oscillator could have any non-negative energy, including zero.  Working back from this result, we see that the energy levels of the s.h.o. are  cid:1 ω, 3 2   cid:1 ω, . . . , and that the corresponding  unnormalised  2   cid:1 ω, . . . ,  m + 1 1 2 ket vectors can be written as   cid:1 ω, 5 2  0 cid:21 ,  †0 cid:21 ,  A  †  A   20 cid:21 ,  . . . ,  †  A   m0 cid:21 ,  . . . .  This notation, and elaborations of it, are often used in the quantum treatment of classical ﬁelds such as the electromagnetic ﬁeld. Thus, as the reader should verify, † A A not a physical state at all.   40 cid:21  is a state with energy 9  †  cid:1 ω, whilst A A   40 cid:21  is  †  3A2A  †  3A5A  † A A  † A A  2  The normalisation of the eigenstates  In order to make quantitative calculations using the previous results we need to establish the values of the cn and dn that appear in equations  19.44  and  19.45 . To do this, we ﬁrst establish the operator recurrence relation  † Am A   m = Am−1 A †   mA + m cid:1 Am−1 A †   m−1.   19.47   669   QUANTUM OPERATORS   cid:18   † cid:19   The proof, which makes repeated use of  A, A  =  cid:1 , is as follows:  † Am A   m−1 †  A † A +  cid:1   A   m−1   m = Am−1AA † = Am−1 A † = Am−1A † † A A = Am−1A † †  A = Am−1 A † †  2A A = Am−1 A † †  2 A ... = Am−1 A †   mA + m cid:1 Am−1 A †   m−1.   m−1   m−1 +  cid:1 Am−1 A † † A +  cid:1   A   m−2 + Am−1A † A +  cid:1   A  †  m−2 +  cid:1 Am−1 A  m−2 +  cid:1 Am−1 A †  † cid:1  A †   m−1   m−3 + 2 cid:1 Am−1 A †   m−1   m−1  this operator equation and note that the ﬁrst term on the RHS is zero since it  Now we take the expectation values in the ground state 0 cid:21  of both sides of contains the term A0 cid:21 . The non-vanishing terms are  m  0 cid:21  = m cid:1  cid:20 0 Am−1 A †   cid:20 0 Am A †   m−1  0 cid:21 .  † The LHS is the square of the norm of  A equal to   m0 cid:21 , and, from equation  19.45 , it is  d02d12 ···dm−12  cid:20 0 0 cid:21 .  Similarly, the RHS is equal to  m cid:1 d02d12 ···dm−22  cid:20 0 0 cid:21 .  It follows that dm−12 = m cid:1  and, taking all coeﬃcients as real, dm = Thus the correctly normalised state of energy  n + 1 † application of A  to the ground state, is given by   m + 1  cid:1 . 2   cid:1 , obtained by repeated  √  † To evaluate the cn, we note that, from the commutator of A and A  ,   19.48    cid:18   A, A  †  A   n  0 cid:21 .  An cid:21    n!  cid:1 n 1 2  n cid:21  = † cid:19  n cid:21  = AA  cid:24  †n cid:21  − A †  cid:24   n + 1  cid:1  An + 1 cid:21  − cn A  cid:24  √  n + 1  cid:1  cn+1 n cid:21  − cn √  n + 1  cid:1  cn+1 − cn  cid:24   √ n cid:1 . To summarise:   cid:1 n cid:21  =   cid:1  =  n cid:1 ,  =  √ n cid:1   cn =  † n − 1 cid:21  n cid:1 n cid:21 ,  which has the obvious solution cn =  and  dn =   n + 1  cid:1 .   19.49   We end this chapter with another worked example. This one illustrates how the operator formalism that we have developed can be used to obtain results  670   19.3 EXERCISES  that would involve a number of non-trivial integrals if tackled using explicit wavefunctions.  cid:1 Given that the ﬁrst-order change in the ground-state energy of a quantum system when  cid:7 0 cid:21 , ﬁnd the ﬁrst- it is perturbed by a small additional term H order change in the energy of a simple harmonic oscillator in the presence of an additional potential V  in the Hamiltonian is  cid:20 0H   x  = λx3 + µx4.   cid:7    cid:7   From the deﬁnitions of A and A  , equation  19.39 , we can write  †  x =  1√ 2mω  †    ⇒ H   cid:7   =   A + A  We now compute successive values of  A + A  λ   2mω 3 2   A + A   3 +  µ   2mω 2  †   A + A   4.  †   n 0 cid:21  for n = 1, 2, 3, 4, remembering that † n cid:21  =   n + 1  cid:1 n + 1 cid:21  :  †   cid:24   An cid:21  =   A + A † †   A + A   A + A  †   A + A  A  and  √ n cid:1 n − 1 cid:21   0 cid:21  = 0 +  cid:1 1 2 1 cid:21 , † √ 2  cid:1 2 cid:21 ,  2 0 cid:21  =  cid:1 0 cid:21  +  3 0 cid:21  = 0 +  cid:1 3 2 1 cid:21  + 2 cid:1 3 2 1 cid:21  + √ = 3 cid:1 3 2 1 cid:21  + 6  cid:1 3 2 3 cid:21 , √ √  4 0 cid:21  = 3 cid:1 2 0 cid:21  + 18  cid:1 2 2 cid:21  +  √ 6  cid:1 3 2 3 cid:21  √ 24  cid:1 2 4 cid:21 .  18  cid:1 2 2 cid:21  +  To ﬁnd the energy shift we need to form the inner product of each of these state vectors  with 0 cid:21 . But 0 cid:21  is orthogonal to all n cid:21  if n  cid:3 = 0. Consequently, the term  cid:20 0  A + A in the expectation value is zero, and in the expression for  cid:20 0  A + A   3 0 cid:21   4 0 cid:21  only the ﬁrst  †  †  term is non-zero; its value is 3 cid:1 2. The perturbation energy is thus given by   cid:20 0 H   cid:7   0 cid:21  =  3µ cid:1 2  2mω 2 .  It could have been anticipated on symmetry grounds that the expectation of λx3, an odd function of x, would be zero, but the calculation gives this result automatically. The contribution of the quadratic term in the perturbation would have been much harder to anticipate!  cid:2   19.3 Exercises  19.1  19.2  19.3  Show that the commutator of two operators that correspond to two physical observables cannot itself correspond to another physical observable. By expressing the operator Lz, corresponding to the z-component of angular momentum, in spherical polar coordinates  r, θ, φ , show that the angular mo- mentum of a particle about the polar axis cannot be known at the same time as its azimuthal position around that axis.  In quantum mechanics, the time dependence of the state function ψ cid:21  of a system  is given, as a further postulate, by the equation  where H is the Hamiltonian of the system. Use this to ﬁnd the time dependence  of the expectation value  cid:20 A cid:21  of an operator A that itself has no explicit time  dependence. Hence show that operators that commute with the Hamiltonian correspond to the classical ‘constants of the motion’.  ψ cid:21  = Hψ cid:21 ,  i cid:1   ∂ ∂t  671   19.4  Show that the Pauli matrices  QUANTUM OPERATORS  For a particle of mass m moving in a one-dimensional potential V  x , prove  Ehrenfest’s theorem:  6  7  dV dx  d cid:20 px cid:21  = −  cid:8   dt   cid:7   d cid:20 x cid:21   dt  =  .   cid:20 px cid:21   cid:7   m   cid:7   and  0 −i   cid:8    cid:8   i   cid:1    cid:1   0  0 1  1 0  Sx = 1 2  , Sz = 1 2  , Sy = 1 2  0 −1 which are used as the operators corresponding to intrinsic spin of 1  cid:1  in non- 2 relativistic quantum mechanics, satisfy S2  cid:1 2I, and have the Deduce that any state ψ cid:21  represented by the column vector  a, b T is an eigenstate same commutation properties as the components of orbital angular momentum. of S2 with eigenvalue 3 cid:1 2 4. Find closed-form expressions for cos C and sin C, where C is the matrix  x = S2  y = S2  z = 1  0  1   cid:1   4  ,   cid:7   C =  1  1  1 −1   cid:8   .  Demonstrate that the ‘expected’ relationships  cos2 C + sin2 C = I  and  sin 2C = 2 sin C cos C  are valid. Operators A and B anticommute. Evaluate  A + B 2n for a few values of n and hence propose an expression for cnr in the expansion cnr A2n−2r B2r.   A + B 2n =  n cid:4   Prove your proposed formula for general values of n, using the method of induction.  Show that  cos A + B  =  r=0  n cid:4   r=0  ∞ cid:4   cid:7   n=0  dnr A2n−2rB2r,  cid:8   where the dnr are constants whose values you should determine.  0 1  , conﬁrm that your answer is  By taking as A the matrix A =  1 0 consistent with that obtained in exercise 19.5. Expressed in terms of the annihilation and creation operators A and A discussed in the text, a system has an unperturbed Hamiltonian H0 =  cid:1 ωA A. The system † is disturbed by the addition of a perturbing Hamiltonian H1 = g cid:1 ω A + A  , where g is real. Show that the eﬀect of the perturbation is to move the whole energy spectrum of the system down by g2 cid:1 ω. For a system of N electrons in their ground state 0 cid:21 , the Hamiltonian is  †  †  Show that double commutator [ [ x, H ] , x ], where x =  p2 xn , xn  = −2i cid:1 pxn , and hence that the expectation value of the   cid:18   H =   cid:19   N cid:4   n=1  +  p2 xn  + p2 zn  + p2 yn 2m  N cid:4   cid:11   cid:20 0 [ [ x, H ] , x ]  0 cid:21  =  n=1  V  xn, yn, zn .  N n=1 xn, is given by  N cid:1 2 m  .  672  19.5  19.6  19.7  19.8   Now evaluate the expectation value using the eigenvalue properties of H, namely  Hr cid:21  = Err cid:21 , and deduce the sum rule for oscillation strengths,  19.3 EXERCISES  ∞ cid:4   r=0   Er − E0  cid:20 r x 0 cid:21 2 =  N cid:1 2 2m  .  F λ  = exp λA B exp −λA ,  19.9  By considering the function  19.10  where A and B are linear operators and λ is a parameter, and ﬁnding its derivatives with respect to λ, prove that  eABe  −A = B + [ A, B ] +  cid:7   1 2!   cid:8   Use this result to express  [ A, [ A, B ] ] +  exp  iLxθ   cid:1   Ly exp  [ A, [ A, [ A, B ] ] ] + ··· .  cid:8   1 3!   cid:7 −iLxθ   cid:1   as a linear combination of the angular momentum operators Lx, Ly and Lz. For a system containing more than one particle, the total angular momentum J and its components are represented by operators that have completely analogous commutation relations to those for the operators for a single particle, i.e. J 2 has eigenvalue j j + 1  cid:1 2 and Jz has eigenvalue mj  cid:1  for the state j, mj cid:21 . The usual orthonormality relationship  cid:20 j an  cid:2  = 3 state and can have state functions of the form A, 3, mA cid:21 , whilst B is in an  cid:2  = 2 state with possible state functions B, 2, mB cid:21 . The range of possible values for j is 3 − 2 ≤ j ≤ 3 + 2, i.e. 1 ≤ j ≤ 5, and the overall state function  A system consists of two  distinguishable  particles A and B. Particle A is in   j, mj cid:21  = δj  is also valid.   cid:7  , m j   cid:7  j mj  j δm   cid:7    cid:7   can be written as  j, mj cid:21  =   A, 3, mA cid:21  B, 2, mB cid:21 .  C  j mj mA mB   cid:4   mA+mB =mj  The numerical coeﬃcients C  j mj mA mB are known as Clebsch–Gordon coeﬃcients.  Assume  as can be shown  that the ladder operators U AB  and D AB  for the system can be written as U A  + U B  and D A  + D B , respectively, and that they lead to relationships equivalent to  19.34  and  19.35  with  cid:2  replaced by j and m by mj .   a  Apply the operators to the  obvious  relationship  to show that  AB, 5, 4 cid:21  =  AB, 5, 5 cid:21  = A, 3, 3 cid:21 B, 2, 2 cid:21   cid:2   cid:2   A, 3, 2 cid:21 B, 2, 2 cid:21  +  6 10  4 10  A, 3, 3 cid:21 B, 2, 1 cid:21 .   b  Find, to within an overall sign, the real coeﬃcients c and d in the expansion  AB, 4, 4 cid:21  = cA, 3, 2 cid:21 B, 2, 2 cid:21  + dA, 3, 3 cid:21 B, 2, 1 cid:21   by requiring it to be orthogonal to AB, 5, 4 cid:21 . Check your answer by considering U AB AB, 4, 4 cid:21 . for AB, 4,−3 cid:21  as a sum of products of the form A, 3, mA cid:21 B, 2, mB cid:21 .   c  Find, to within an overall sign, and as eﬃciently as possible, an expression  673   QUANTUM OPERATORS  19.4 Hints and answers  19.1 19.3  19.5  19.7  19.9  Show that the commutator is anti-Hermitian. Use the Hermitian conjugate of the given equation to obtain the time dependence  of  cid:20 ψ. The rate of change of  cid:20 ψAψ cid:21  is i cid:20 ψ [ H, A ]ψ cid:21 . Note that [ H, px ] =   cid:19    cid:18  p2 x, x √   cid:7    cid:8    2m.  1 0  0 1  [ V , px ] and [ H, x ] = Show that C2 = 2I.   cid:18   † cid:19   cos C = cos  2  ,  sin C =  .  B, B  Express the total Hamiltonian in terms of B = A + gI and determine the value of Show that, if F  n  is the nth derivative of F λ , then F  n+1  = . Use a Taylor series in λ to evaluate F 1 , using derivatives evaluated at λ = 0. Successively reduce the level of nesting of each multiple commutator by using the result of  evaluating the previous term. The given expression reduces to cos θ Ly − sin θ Lz.  A, F  n    cid:7   √ 2√ sin 2   cid:8   .  1  1  1 −1  cid:19    cid:18   674   20  Partial diﬀerential equations: general and particular solutions  In this chapter and the next the solution of diﬀerential equations of types typically encountered in the physical sciences and engineering is extended to situations involving more than one independent variable. A partial diﬀerential equation  PDE  is an equation relating an unknown function  the dependent variable  of two or more variables to its partial derivatives with respect to those variables. The most commonly occurring independent variables are those describing position and time, and so we will couch our discussion and examples in notation appropriate to them.  As in other chapters we will focus our attention on the equations that arise most often in physical situations. We will restrict our discussion, therefore, to linear PDEs, i.e. those of ﬁrst degree in the dependent variable. Furthermore, we will discuss primarily second-order equations. The solution of ﬁrst-order PDEs will necessarily be involved in treating these, and some of the methods discussed can be extended without diﬃculty to third- and higher-order equations. We shall also see that many ideas developed for ordinary diﬀerential equations  ODEs  can be carried over directly into the study of PDEs.  In this chapter we will concentrate on general solutions of PDEs in terms of arbitrary functions and the particular solutions that may be derived from them in the presence of boundary conditions. We also discuss the existence and uniqueness of the solutions to PDEs under given boundary conditions.  In the next chapter the methods most commonly used in practice for obtaining solutions to PDEs subject to given boundary conditions will be considered. These methods include the separation of variables, integral transforms and Green’s functions. This division of material is rather arbitrary and has been made only to emphasise the general usefulness of the latter methods. In particular, it will be readily apparent that some of the results of the present chapter are in fact solutions in the form of separated variables, but arrived at by a diﬀerent approach.  675   PDES: GENERAL AND PARTICULAR SOLUTIONS  20.1 Important partial diﬀerential equations  Most of the important PDEs of physics are second-order and linear. In order to gain familiarity with their general form, some of the more important ones will now be brieﬂy discussed. These equations apply to a wide variety of diﬀerent physical systems.  Since, in general, the PDEs listed below describe three-dimensional situations, the independent variables are r and t, where r is the position vector and t is time. The actual variables used to specify the position vector r are dictated by the coordinate system in use. For example, in Cartesian coordinates the independent variables of position are x, y and z, whereas in spherical polar coordinates they are r, θ and φ. The equations may be written in a coordinate-independent manner,  however, by the use of the Laplacian operator ∇2.  The wave equation  20.1.1 The wave equation  ∇2u =  1 c2  ∂2u ∂t2   20.1   describes as a function of position and time the displacement from equilibrium, u r, t , of a vibrating string or membrane or a vibrating solid, gas or liquid. The equation also occurs in electromagnetism, where u may be a component of the electric or magnetic ﬁeld in an elecromagnetic wave or the current or voltage along a transmission line. The quantity c is the speed of propagation of the waves.  cid:1  Find the equation satisﬁed by small transverse displacements u x, t  of a uniform string of mass per unit length ρ held under a uniform tension T , assuming that the string is initially located along the x-axis in a Cartesian coordinate system.  Figure 20.1 shows the forces acting on an elemental length ∆s of the string. If the tension T in the string is uniform along its length then the net upward vertical force on the element is  ∆F = T sin θ2 − T sin θ1.  cid:14    cid:13   Assuming that the angles θ1 and θ2 are both small, we may make the approximation  sin θ ≈ tan θ. Since at any point on the string the slope tan θ = ∂u ∂x, the force can be  written  ∆F = T  ∂u x + ∆x, t   ∂x  − ∂u x, t   ∂x  ≈ T  ∂2u x, t   ∂x2  ∆x,  where we have used the deﬁnition of the partial derivative to simplify the RHS.  This upward force may be equated, by Newton’s second law, to the product of the mass of the element and its upward acceleration. The element has a mass ρ ∆s, which is approximately equal to ρ ∆x if the vibrations of the string are small, and so we have  ρ ∆x  ∂2u x, t   ∂2u x, t   = T  ∆x.  ∂t2  ∂x2  676   20.1 IMPORTANT PARTIAL DIFFERENTIAL EQUATIONS  u  θ1  T  T  θ2  ∆s  x  x + ∆x  x  Figure 20.1 The forces acting on an element of a string under uniform tension T .  Dividing both sides by ∆x we obtain, for the vibrations of the string, the one-dimensional wave equation  where c2 = T  ρ.  cid:2   The longitudinal vibrations of an elastic rod obey a very similar equation to  that derived in the above example, namely  ∂2u ∂x2  =  1 c2  ∂2u ∂t2 ,  ∂2u ∂x2 =  ρ E  ∂2u ∂t2 ;  here ρ is the mass per unit volume and E is Young’s modulus.  The wave equation can be generalised slightly. For example, in the case of the vibrating string, there could also be an external upward vertical force f x, t  per unit length acting on the string at time t. The transverse vibrations would then satisfy the equation  T  ∂2u ∂x2 + f x, t  = ρ  ∂2u ∂t2 ,  which is clearly of the form ‘upward force per unit length = mass per unit length  × upward acceleration’.  cid:7   Similar examples, but involving two or three spatial dimensions rather than one, are provided by the equation governing the transverse vibrations of a stretched membrane subject to an external vertical force density f x, y, t ,  T  ∂2u ∂x2 +  ∂2u ∂y2  + f x, y, t  = ρ x, y   ∂2u ∂t2 ,  where ρ is the mass per unit area of the membrane and T is the tension.   cid:8   677   PDES: GENERAL AND PARTICULAR SOLUTIONS  20.1.2 The diffusion equation  The diﬀusion equation  κ∇2u =  ∂u ∂t   20.2   describes the temperature u in a region containing no heat sources or sinks; it also applies to the diﬀusion of a chemical that has a concentration u r, t . The constant κ is called the diﬀusivity. The equation is clearly second order in the three spatial variables, but ﬁrst order in time.  cid:1 Derive the equation satisﬁed by the temperature u r, t  at time t for a material of uniform thermal conductivity k, speciﬁc heat capacity s and density ρ. Express the equation in Cartesian coordinates.  Let us consider an arbitrary volume V lying within the solid and bounded by a surface S  this may coincide with the surface of the solid if so desired . At any point in the solid the rate of heat ﬂow per unit area in any given direction ˆr is proportional to minus the  component of the temperature gradient in that direction and so is given by  −k∇u · ˆr. The  total ﬂux of heat out of the volume V per unit time is given by  where Q is the total heat energy in V at time t and ˆn is the outward-pointing unit normal to S ; note that we have used the divergence theorem to convert the surface integral into a volume integral.  We can also express Q as a volume integral over V ,  and its rate of change is then given by  where we have taken the derivative with respect to time inside the integral  see section 5.12 . Comparing  20.3  and  20.4 , and remembering that the volume V is arbitrary, we obtain  the three-dimensional diﬀusion equation  where the diﬀusion coeﬃcient κ = k  sρ . To express this equation in Cartesian coordinates,   cid:8  we simply write ∇2 in terms of x, y and z to obtain ∂2u ∂z2  ∂2u ∂x2  ∂2u ∂y2   cid:7   +  +  κ  =  .  cid:2   ∂u ∂t  The diﬀusion equation just derived can be generalised to   cid:21  cid:21   cid:21  cid:21  cid:21   −k∇u  · ˆn dS ∇ ·  −k∇u  dV ,  S  =  =  − dQ  dt  V   cid:21  cid:21  cid:21   cid:21  cid:21  cid:21   V  V  Q =  sρu dV ,  dQ dt  =  sρ  dV ,  ∂u ∂t  κ∇2u =  ∂u ∂t  ,  k∇2u + f r, t  = sρ  ∂u ∂t  .  678   20.3    20.4    20.1 IMPORTANT PARTIAL DIFFERENTIAL EQUATIONS  The second term, f r, t , represents a varying density of heat sources throughout the material but is often not required in physical applications. In the most general case, k, s and ρ may depend on position r, in which case the ﬁrst term becomes  ∇ ·  k∇u . However, in the simplest application the heat ﬂow is one-dimensional  with no heat sources, and the equation becomes  in Cartesian coordinates   20.1.3 Laplace’s equation  ∂2u ∂x2 =  sρ k  ∂u ∂t  .  ∇2u = 0,  Laplace’s equation,   20.5   may be obtained by setting ∂u ∂t = 0 in the diﬀusion equation  20.2 , and describes  for example  the steady-state temperature distribution in a solid in which there are no heat sources – i.e. the temperature distribution after a long time has elapsed.  Laplace’s equation also describes the gravitational potential in a region con- taining no matter or the electrostatic potential in a charge-free region. Further, it applies to the ﬂow of an incompressible ﬂuid with no sources, sinks or vortices;  in this case u is the velocity potential, from which the velocity is given by v = ∇u.  Poisson’s equation,  20.1.4 Poisson’s equation  ∇2u = ρ r ,   20.6   describes the same physical situations as Laplace’s equation, but in regions containing matter, charges or sources of heat or ﬂuid. The function ρ r  is called the source density and in physical applications usually contains some multiplicative physical constants. For example, if u is the electrostatic potential in some region of space, in which case ρ is the density of electric charge, then  ∇2u = −ρ r   cid:4 0, where  cid:4 0 is the permittivity of free space. Alternatively, u might given by ρ; then ∇2u = 4πGρ r , where G is the gravitational constant.  represent the gravitational potential in some region where the matter density is  The Schr¨odinger equation  20.1.5 Schr-odinger’s equation  −  cid:1 2 2m  ∇2u + V  r u = i cid:1   ∂u ∂t  ,  679   20.7    PDES: GENERAL AND PARTICULAR SOLUTIONS  describes the quantum mechanical wavefunction u r, t  of a non-relativistic particle of mass m;  cid:1  is Planck’s constant divided by 2π. Like the diﬀusion equation it is second order in the three spatial variables and ﬁrst order in time.  20.2 General form of solution  Before turning to the methods by which we may hope to solve PDEs such as those listed in the previous section, it is instructive, as for ODEs in chapter 14, to study how PDEs may be formed from a set of possible solutions. Such a study can provide an indication of how equations obtained not from possible solutions but from physical arguments might be solved.  For deﬁniteness let us suppose we have a set of functions involving two independent variables x and y. Without further speciﬁcation this is of course a very wide set of functions, and we could not expect to ﬁnd a useful equation that they all satisfy. However, let us consider a type of function ui x, y  in which x and y appear in a particular way, such that ui can be written as a function  however complicated  of a single variable p, itself a simple function of x and y.  Let us illustrate this by considering the three functions  u1 x, y  = x4 + 4 x2y + y2 + 1 , u2 x, y  = sin x2 cos 2y + cos x2 sin 2y,  u3 x, y  =  x2 + 2y + 2 3x2 + 6y + 5  .  These are all fairly complicated functions of x and y and a single diﬀerential equation of which each one is a solution is not obvious. However, if we observe that in fact each can be expressed as a function of the variable p = x2 + 2y alone  with no other x or y involved  then a great simpliﬁcation takes place. Written in terms of p the above equations become  u1 x, y  =  x2 + 2y 2 + 4 = p2 + 4 = f1 p , u2 x, y  = sin x2 + 2y  = sin p = f2 p ,  u3 x, y  =   x2 + 2y  + 2 3 x2 + 2y  + 5  =  p + 2 3p + 5  = f3 p .  Let us now form, for each ui, the partial derivatives ∂ui ∂x and ∂ui ∂y. In each case these are  writing both the form for general p and the one appropriate to our particular case, p = x2 + 2y   for i = 1, 2, 3. All reference to the form of fi can be eliminated from these  ∂ui ∂x ∂ui ∂y  =  =  dfi p   dfi p   dp  dp  ∂p ∂x ∂p ∂y   cid:7  = 2xf i ,  cid:7  = 2f i ,  680   20.3 GENERAL AND PARTICULAR SOLUTIONS  equations by cross-multiplication, obtaining  or, for our speciﬁc form, p = x2 + 2y,  ∂p ∂y  ∂ui ∂x  =  ∂p ∂x  ∂ui ∂y  ,  ∂ui ∂x  = x  ∂ui ∂y  .  It is thus apparent that not only are the three functions u1, u2 u3 solutions of the PDE  20.8  but so also is any arbitrary function f p  of which the argument p has the form x2 + 2y.   20.8   20.3 General and particular solutions  In the last section we found that the ﬁrst-order PDE  20.8  has as a solution any function of the variable x2 + 2y. This points the way for the solution of PDEs of other orders, as follows. It is not generally true that an nth-order PDE can always be considered as resulting from the elimination of n arbitrary functions from its solution  as opposed to the elimination of n arbitrary constants for an nth-order ODE, see section 14.1 . However, given speciﬁc PDEs we can try to solve them by seeking combinations of variables in terms of which the solutions may be expressed as arbitrary functions. Where this is possible we may expect n combinations to be involved in the solution.  Naturally, the exact functional form of the solution for any particular situation must be determined by some set of boundary conditions. For instance, if the PDE contains two independent variables x and y then for complete determination of its solution the boundary conditions will take a form equivalent to specifying u x, y  along a suitable continuum of points in the xy-plane  usually along a line . We now discuss the general and particular solutions of ﬁrst- and second- order PDEs. In order to simplify the algebra, we will restrict our discussion to equations containing just two independent variables x and y. Nevertheless, the method presented below may be extended to equations containing several independent variables.  20.3.1 First-order equations  Although most of the PDEs encountered in physical contexts are second order  i.e. they contain ∂2u ∂x2 or ∂2u ∂x∂y, etc. , we now discuss ﬁrst-order equations to illustrate the general considerations involved in the form of the solution and in satisfying any boundary conditions on the solution.  The most general ﬁrst-order linear PDE  containing two independent variables   681   PDES: GENERAL AND PARTICULAR SOLUTIONS  is of the form  A x, y   + B x, y   + C x, y u = R x, y ,   20.9   ∂u ∂x  ∂u ∂y  where A x, y , B x, y , C x, y  and R x, y  are given functions. Clearly, if either A x, y  or B x, y  is zero then the PDE may be solved straightforwardly as a ﬁrst-order linear ODE  as discussed in chapter 14 , the only modiﬁcation being that the arbitrary constant of integration becomes an arbitrary function of x or y respectively.   cid:1 Find the general solution u x, y  of  x  ∂u ∂x  + 3u = x2.  Dividing through by x we obtain  which is a linear equation with integrating factor  see subsection 14.2.4   ∂u ∂x  3u x  = x,  +   cid:8    cid:7  cid:21   3 x  exp  dx  = exp 3 ln x  = x3.  Multiplying through by this factor we ﬁnd  which, on integrating with respect to x, gives  where f y  is an arbitrary function of y. Finally, dividing through by x3, we obtain the solution  When the PDE contains partial derivatives with respect to both independent variables then, of course, we cannot employ the above procedure but must seek an alternative method. Let us for the moment restrict our attention to the special case in which C x, y  = R x, y  = 0 and, following the discussion of the previous section, look for solutions of the form u x, y  = f p  where p is some, at present unknown, combination of x and y. We then have  ∂ ∂x   x3u  = x4,  x3u =  + f y ,  x5 5  x2 5  u x, y  =  +  f y   x3 .  cid:2   ∂u ∂x ∂u ∂y  =  =  df p   df p   dp  dp  ∂p ∂x ∂p ∂y  ,  ,  682    cid:14   ∂p ∂y  20.3 GENERAL AND PARTICULAR SOLUTIONS   cid:13   which, when substituted into the PDE  20.9 , give  A x, y   + B x, y   ∂p ∂x  ∂p ∂y  df p   dp  = 0.  This removes all reference to the actual form of the function f p  since for non-trivial p we must have  A x, y   + B x, y   = 0.   20.10   Let us now consider the necessary condition for f p  to remain constant as x and y vary; this is that p itself remains constant. Thus for f to remain constant implies that x and y must vary in such a way that  dp =  dx +  dy = 0.   20.11   ∂p ∂y  The forms of  20.10  and  20.11  are very alike and become the same if we  require that  ∂p ∂x  ∂p ∂x   20.12    20.13   By integrating this expression the form of p can be found.  cid:1 For  ﬁnd  i  the solution that takes the value 2y + 1 on the line x = 1, and  ii  a solution that has the value 4 at the point  1, 1 .  If we seek a solution of the form u x, y  = f p , we deduce from  20.12  that u x, y  will be constant along lines of  x, y  that satisfy  −1 2. Identifying the constant of integration c with p1 2 which on integrating gives x = cy  to avoid fractional powers , we conclude that p = x2y. Thus the general solution of the PDE  20.13  is  where f is an arbitrary function.  We must now ﬁnd the particular solutions that obey each of the imposed boundary conditions. For boundary condition  i  a little thought shows that the particular solution required is  u x, y  = 2 x2y  + 1 = 2x2y + 1.   20.14   For boundary condition  ii  some obviously acceptable solutions are  dx  =  dy  .  A x, y   B x, y   x  ∂u ∂x  − 2y  ∂u ∂y  = 0,  dx x  =  dy−2y  ,  u x, y  = f x2y ,  u x, y  = x2y + 3, u x, y  = 4x2y, u x, y  = 4.  683   PDES: GENERAL AND PARTICULAR SOLUTIONS  Each is a valid solution  the freedom of choice of form arises from the fact that u is speciﬁed at only one point  1, 1 , and not along a continuum  say , as in boundary condition  i  . All three are particular examples of the general solution, which may be written, for example, as  u x, y  = x2y + 3 + g x2y ,  where g = g x2y  = g p  is an arbitrary function subject only to g 1  = 0. For this g p  = 3p − 3, g p  = 1 − p.  cid:2  example, the forms of g corresponding to the particular solutions listed above are g p  = 0,  As mentioned above, in order to ﬁnd a solution of the form u x, y  = f p  we require that the original PDE contains no term in u, but only terms containing  its partial derivatives. If a term in u is present, so that C x, y   cid:3 = 0 in  20.9 ,  then the procedure needs some modiﬁcation, since we cannot simply divide out the dependence on f p  to obtain  20.10 . In such cases we look instead for a solution of the form u x, y  = h x, y f p . We illustrate this method in the following example.  cid:1 Find the general solution of  x  ∂u ∂x  + 2  ∂u ∂y  − 2u = 0.   20.15   We seek a solution of the form u x, y  = h x, y f p , with the consequence that   cid:7   Substituting these expressions into the PDE  20.15  and rearranging, we obtain  x  ∂h ∂x  + 2  ∂h ∂y  f p  +  x  + 2  ∂p ∂x  ∂p ∂y  h  df p   dp  = 0.  The ﬁrst factor in parentheses is just the original PDE with u replaced by h. Therefore, if h is any solution of the PDE, however simple, this term will vanish, to leave  =  f p  + h  f p  + h  df p   df p   dp  dp  ∂p ∂x ∂p ∂y  ,  .   cid:8   ∂u ∂x ∂u ∂y  ∂h ∂x ∂h ∂y  =   cid:8   − 2h  cid:7    cid:7    cid:8   x  ∂p ∂x  + 2  ∂p ∂y  h  df p   dp  = 0,  from which, as in the previous case, we obtain  From  20.11  and  20.12  we see that u x, y  will be constant along lines of  x, y  that satisfy  which integrates to give x = c exp y 2 . Identifying the constant of integration c with p we  ﬁnd p = x exp −y 2 . Thus the general solution of  20.15  is 2 y  ,  u x, y  = h x, y f x exp − 1  where f p  is any arbitrary function of p and h x, y  is any solution of  20.15 .  x  ∂p ∂x  + 2  = 0.  ∂p ∂y  dx x  =  dy 2  ,  684   20.3 GENERAL AND PARTICULAR SOLUTIONS  If we take, for example, h x, y  = exp y, which clearly satisﬁes  20.15 , then the general  solution is  u x, y  =  exp y f x exp − 1  2 y  .  u x, y  = x2g x exp − 1  2 y  ,  Alternatively, h x, y  = x2 also satisﬁes  20.15  and so the general solution to the equation can also be written  where g is an arbitrary function of p; clearly g p  = f p  p2.  cid:2   20.3.2 Inhomogeneous equations and problems  Let us discuss in a more general form the particular solutions of  20.13  found in the second example of the previous subsection. It is clear that, so far as this equation is concerned, if u x, y  is a solution then so is any multiple of u x, y  or any linear sum of separate solutions u1 x, y  + u2 x, y . However, when it comes to ﬁtting the boundary conditions this is not so.  For example, although u x, y  in  20.14  satisﬁes the PDE and the boundary condition u 1, y  = 2y + 1, the function u1 x, y  = 4u x, y  = 8xy + 4, whilst satisfying the PDE, takes the value 8y +4 on the line x = 1 and so does not satisfy the required boundary condition. Likewise the function u2 x, y  = u x, y +f1 x2y , for arbitrary f1, satisﬁes  20.13  but takes the value u2 1, y  = 2y + 1 + f1 y  on the line x = 1, and so is not of the required form unless f1 is identically zero.  Thus we see that when treating the superposition of solutions of PDEs two considerations arise, one concerning the equation itself and the other connected to the boundary conditions. The equation is said to be homogeneous if the fact that u x, y  is a solution implies that λu x, y , for any constant λ, is also a solution. However, the problem is said to be homogeneous if, in addition, the boundary conditions are such that if they are satisﬁed by u x, y  then they are also satisﬁed by λu x, y . The last requirement itself is referred to as that of homogeneous boundary conditions.  For example, the PDE  20.13  is homogeneous but the general ﬁrst-order equation  20.9  would not be homogeneous unless R x, y  = 0. Furthermore, the boundary condition  i  imposed on the solution of  20.13  in the previous subsection is not homogeneous though, in this case, the boundary condition  u x, y  = 0  on the line y = 4x  −2  would be, since u x, y  = λ x2y − 4  satisﬁes this condition for any λ and, being a function of x2y, satisﬁes  20.13 .  The reason for discussing the homogeneity of PDEs and their boundary condi- tions is that in linear PDEs there is a close parallel to the complementary-function and particular-integral property of ODEs. The general solution of an inhomo- geneous problem can be written as the sum of any particular solution of the problem and the general solution of the corresponding homogeneous problem  as  685   PDES: GENERAL AND PARTICULAR SOLUTIONS  for ODEs, we require that the particular solution is not already contained in the general solution of the homogeneous problem . Thus, for example, the general solution of  − x  ∂u ∂y  ∂u ∂x  + au = f x, y ,   20.16   subject to, say, the boundary condition u 0, y  = g y , is given by  u x, y  = v x, y  + w x, y ,  where v x, y  is any solution  however simple  of  20.16  such that v 0, y  = g y  and w x, y  is the general solution of  − x  ∂w ∂y  ∂w ∂x  + aw = 0,   20.17   with w 0, y  = 0. If the boundary conditions are suﬃciently speciﬁed then the only  possible solution of  20.17  will be w x, y  ≡ 0 and v x, y  will be the complete  solution by itself.  Alternatively, we may begin by ﬁnding the general solution of the inhomoge- neous equation  20.16  without regard for any boundary conditions; it is just the sum of the general solution to the homogeneous equation and a particular inte- gral of  20.16 , both without reference to the boundary conditions. The boundary conditions can then be used to ﬁnd the appropriate particular solution from the general solution.  We will not discuss at length general methods of obtaining particular integrals of PDEs but merely note that some of those methods available for ordinary § diﬀerential equations can be suitably extended.  cid:1 Find the general solution of  y  ∂u ∂x  − x  ∂u ∂y  = 3x.   20.18   Hence ﬁnd the most general particular solution  i  which satisﬁes u x, 0  = x2 and  ii  which has the value u x, y  = 2 at the point  1, 0 .  This equation is inhomogeneous, and so let us ﬁrst ﬁnd the general solution of  20.18  without regard for any boundary conditions. We begin by looking for the solution of the corresponding homogeneous equation   20.18  but with the RHS equal to zero  of the form u x, y  = f p . Following the same procedure as that used in the solution of  20.13  we ﬁnd that u x, y  will be constant along lines of  x, y  that satisfy  dx y  =  dy−x  ⇒ x2 2  +  = c.  y2 2  Identifying the constant of integration c with p 2, we ﬁnd that the general solution of the  §  See for example H. T. H. Piaggio, An Elementary Treatise on Diﬀerential Equations and their Applications  London: G. Bell and Sons, Ltd, 1954 , pp. 175 ﬀ.  686   20.3 GENERAL AND PARTICULAR SOLUTIONS  homogeneous equation is u x, y  = f x2 + y2  for arbitrary function f. Now by inspection a particular integral of  20.18  is u x, y  = −3y, and so the general solution to  20.18  is  u x, y  = f x2 + y2  − 3y.  u x, y  = x2 + y2 − 3y.  Boundary condition  i  requires u x, 0  = f x2  = x2, i.e. f z  = z, and so the particular  solution in this case is  Similarly, boundary condition  ii  requires u 1, 0  = f 1  = 2. One possibility is f z  = 2z, and if we make this choice, then one way of writing the most general particular solution is  u x, y  = 2x2 + 2y2 − 3y + g x2 + y2 ,  where g is any arbitrary function for which g 1  = 0. Alternatively, a simpler choice would be f z  = 2, leading to  u x, y  = 2 − 3y + g x2 + y2 .  cid:2   Although we have discussed the solution of inhomogeneous problems only for ﬁrst-order equations, the general considerations hold true for linear PDEs of higher order.  20.3.3 Second-order equations  As noted in section 20.1, second-order linear PDEs are of great importance in describing the behaviour of many physical systems. As in our discussion of ﬁrst- order equations, for the moment we shall restrict our discussion to equations with just two independent variables; extensions to a greater number of independent variables are straightforward.  The most general second-order linear PDE  containing two independent vari-  ables  has the form  A  ∂2u ∂x2 + B  ∂2u ∂x∂y  + C  ∂2u ∂y2 + D  ∂u ∂x  ∂u ∂y  + E  + Fu = R x, y ,   20.19   where A, B, . . . , F and R x, y  are given functions of x and y. Because of the nature of the solutions to such equations, they are usually divided into three classes, a division of which we will make further use in subsection 20.6.2. The equation  20.19  is called hyperbolic if B2 > 4AC, parabolic if B2 = 4AC and elliptic if B2 < 4AC. Clearly, if A, B and C are functions of x and y  rather than just constants  then the equation might be of diﬀerent types in diﬀerent parts of the xy-plane.  Equation  20.19  obviously represents a very large class of PDEs, and it is usually impossible to ﬁnd closed-form solutions to most of these equations. Therefore, for the moment we shall consider only homogeneous equations, with R x, y  = 0, and make the further  greatly simplifying  restriction that, throughout the remainder of this section, A, B, . . . , F are not functions of x and y but merely constants.  687   PDES: GENERAL AND PARTICULAR SOLUTIONS  We now tackle the problem of solving some types of second-order PDE with constant coeﬃcients by seeking solutions that are arbitrary functions of particular combinations of independent variables, just as we did for ﬁrst-order equations.  Following the discussion of the previous section, we can hope to ﬁnd such solutions only if all the terms of the equation involve the same total number of diﬀerentiations, i.e. all terms are of the same order, although the number of diﬀerentiations with respect to the individual independent variables may be diﬀerent. This means that in  20.19  we require the constants D, E and F to be identically zero  we have, of course, already assumed that R x, y  is zero , so that we are now considering only equations of the form  A  ∂2u ∂x2 + B  ∂2u ∂x∂y  + C  ∂2u ∂y2 = 0,   20.20   where A, B and C are constants. We note that both the one-dimensional wave equation,  ∂2u ∂x2  − 1 c2  ∂2u ∂t2 = 0,  and the two-dimensional Laplace equation,  ∂2u ∂y2 = 0, are of this form, but that the diﬀusion equation,  ∂2u ∂x2 +  κ  ∂2u ∂x2  − ∂u  ∂t  = 0,  is not, since it contains a ﬁrst-order derivative.  Since all the terms in  20.20  involve two diﬀerentiations, by assuming a solution of the form u x, y  = f p , where p is some unknown function of x and y  or t , we may be able to obtain a common factor d2f p  dp2 as the only appearance of f on the LHS. Then, because of the zero RHS, all reference to the form of f can be cancelled out.  We can gain some guidance on suitable forms for the combination p = p x, y   by considering ∂u ∂x when u is given by u x, y  = f p , for then  Clearly diﬀerentiation of this equation with respect to x  or y  will not lead to a single term on the RHS, containing f only as d2f p  dp2, unless the factor ∂p ∂x is a constant so that ∂2p ∂x2 and ∂2p ∂x∂y are necessarily zero. This shows that p must be a linear function of x. In an exactly similar way p must also be a linear function of y, i.e. p = ax + by.  If we assume a solution of  20.20  of the form u x, y  = f ax + by , and evaluate  ∂u ∂x  =  df p   dp  ∂p ∂x  .  688   20.3 GENERAL AND PARTICULAR SOLUTIONS  the terms ready for substitution into  20.20 , we obtain  ∂u ∂x  = a  df p   ,  dp  ∂u ∂y  = b  df p   ,  dp  ∂2u  ∂x2 = a2 d2f p  which on substitution give cid:5   dp2  ,  ∂2u ∂x∂y  = ab  ∂2u  ∂y2 = b2 d2f p   dp2  ,  d2f p   ,  dp2   cid:6  d2f p   Aa2 + Bab + Cb2  dp2 = 0.   20.21   This is the form we have been seeking, since now a solution independent of  the form of f can be obtained if we require that a and b satisfy  Aa2 + Bab + Cb2 = 0.  From this quadratic, two values for the ratio of the two constants a and b are obtained,  b a = [−B ±  B2 − 4AC 1 2] 2C.  If we denote these two ratios by λ1 and λ2 then any functions of the two variables  p1 = x + λ1y,  p2 = x + λ2y  will be solutions of the original equation  20.20 . The omission of the constant factor a from p1 and p2 is of no consequence since this can always be absorbed into the particular form of any chosen function; only the relative weighting of x and y in p is important.  Since p1 and p2 are in general diﬀerent, we can thus write the general solution  of  20.20  as  u x, y  = f x + λ1y  + g x + λ2y ,   20.22   where f and g are arbitrary functions.  Finally, we note that the alternative solution d2f p  dp2 = 0 to  20.21  leads only to the trivial solution u x, y  = kx + ly + m, for which all second derivatives are individually zero.  cid:1  Find the general solution of the one-dimensional wave equation  This equation is  20.20  with A = 1, B = 0 and C = −1 c2, and so the values of λ1 and λ2  are the solutions of  namely λ1 = −c and λ2 = c. This means that arbitrary functions of the quantities  p1 = x − ct,  p2 = x + ct  ∂2u ∂x2  − 1 c2  ∂2u ∂t2  = 0.  1 − λ2  c2  = 0,  689   PDES: GENERAL AND PARTICULAR SOLUTIONS  will be satisfactory solutions of the equation and that the general solution will be  u x, t  = f x − ct  + g x + ct ,   20.23  where f and g are arbitrary functions. This solution is discussed further in section 20.4.  cid:2  The method used to obtain the general solution of the wave equation may also  be applied straightforwardly to Laplace’s equation.  cid:1  Find the general solution of the two-dimensional Laplace equation  ∂2u ∂x2  +  ∂2u ∂y2  = 0.   20.24   Following the established procedure, we look for a solution that is a function f p  of p = x + λy, where from  20.24  λ satisﬁes  This requires that λ = ±i, and satisfactory variables p are p = x± iy. The general solution  1 + λ2 = 0.  required is therefore, in terms of arbitrary functions f and g,  u x, y  = f x + iy  + g x − iy .  cid:2   It will be apparent from the last two examples that the nature of the appropriate linear combination of x and y depends upon whether B2 > 4AC or B2 < 4AC. This is exactly the same criterion as determines whether the PDE is hyperbolic or elliptic. Hence as a general result, hyperbolic and elliptic equations of the form  20.20 , given the restriction that the constants A, B and C are real, have as solutions functions whose arguments have the form x+αy and x+iβy respectively, where α and β themselves are real.  The one case not covered by this result is that in which B2 = 4AC, i.e. a parabolic equation. In this case λ1 and λ2 are not diﬀerent and only one suitable combination of x and y results, namely  u x, y  = f x −  B 2C y .  To ﬁnd the second part of the general solution we try, in analogy with the corresponding situation for ordinary diﬀerential equations, a solution of the form  Substituting this into  20.20  and using A = B2 4C results in  u x, y  = h x, y g x −  B 2C y .  cid:7    cid:8   A  ∂2h ∂x2 + B  ∂2h ∂x∂y  ∂2h ∂y2  + C  g = 0.  Therefore we require h x, y  to be any solution of the original PDE. There are several simple solutions of this equation, but as only one is required we take the simplest non-trivial one, h x, y  = x, to give the general solution of the parabolic equation  u x, y  = f x −  B 2C y  + xg x −  B 2C y .   20.25   690   20.3 GENERAL AND PARTICULAR SOLUTIONS  We could, of course, have taken h x, y  = y, but this only leads to a solution that is already contained in  20.25 .   cid:1 Solve  ∂2u ∂x2  + 2  ∂2u ∂x∂y  +  ∂2u ∂y2  = 0,  subject to the boundary conditions u 0, y  = 0 and u x, 1  = x2.  From our general result, functions of p = x + λy will be solutions provided  i.e. λ = −1 and the equation is parabolic. The general solution is therefore  1 + 2λ + λ2 = 0,  u x, y  = f x − y  + xg x − y .  The boundary condition u 0, y  = 0 implies f p  ≡ 0, whilst u x, 1  = x2 yields  xg x − 1  = x2,  which gives g p  = p + 1, Therefore the particular solution required is  u x, y  = x p + 1  = x x − y + 1 .  cid:2   To reinforce the material discussed above we will now give alternative deriva- tions of the general solutions  20.22  and  20.25  by expressing the original PDE in terms of new variables before solving it. The actual solution will then become almost trivial; but, of course, it will be recognised that suitable new variables could hardly have been guessed if it were not for the work already done. This does not detract from the validity of the derivation to be described, only from the likelihood that it would be discovered by inspection.  We start again with  20.20  and change to new variables  ζ = x + λ1y,  η = x + λ2y.  With this change of variables, we have from the chain rule that  ∂ ∂x ∂ ∂y  =  +  ∂ ∂ζ  ∂ ∂η  ,  = λ1  + λ2  ∂ ∂ζ  ∂ ∂η  .  Using these and the fact that  equation  20.20  becomes  A + Bλi + Cλ2  i = 0  for i = 1, 2,  [2A + B λ1 + λ2  + 2Cλ1λ2]  = 0.  ∂2u ∂ζ∂η  691   PDES: GENERAL AND PARTICULAR SOLUTIONS  Then, providing the factor in brackets does not vanish, for which the required  condition is easily shown to be B2  cid:3 = 4AC, we obtain  ∂2u ∂ζ∂η  = 0,  which has the successive integrals  This solution is just the same as  20.22 ,  ∂u ∂η  = F η ,  u ζ, η  = f η  + g ζ .  u x, y  = f x + λ2y  + g x + λ1y .  If the equation is parabolic  i.e. B2 = 4AC , we instead use the new variables  and recalling that λ = − B 2C  we can reduce  20.20  to  ζ = x + λy,  η = x,  Two straightforward integrations give as the general solution  A  ∂2u ∂η2 = 0.  u ζ, η  = ηg ζ  + f ζ ,  which in terms of x and y has exactly the form of  20.25 ,  u x, y  = xg x + λy  + f x + λy .  Finally, as hinted at in subsection 20.3.2 with reference to ﬁrst-order linear PDEs, some of the methods used to ﬁnd particular integrals of linear ODEs can be suitably modiﬁed to ﬁnd particular integrals of PDEs of higher order. In simple cases, however, an appropriate solution may often be found by inspection.   cid:1 Find the general solution of  ∂2u ∂x2  +  ∂2u ∂y2  = 6 x + y .  Following our previous methods and results, the complementary function is  u x, y  = f x + iy  + g x − iy ,  and only a particular integral remains to be found. By inspection a particular integral of the equation is u x, y  = x3 + y3, and so the general solution can be written  u x, y  = f x + iy  + g x − iy  + x3 + y3.  cid:2   692   20.4 THE WAVE EQUATION  20.4 The wave equation  We have already found that the general solution of the one-dimensional wave equation is  u x, t  = f x − ct  + g x + ct ,   20.26   where f and g are arbitrary functions. However, the equation is of such general importance that further discussion will not be out of place.  Let us imagine that u x, t  = f x− ct  represents the displacement of a string at time t and position x. It is clear that all positions x and times t for which x− ct = constant will have the same instantaneous displacement. But x − ct = constant  is exactly the relation between the time and position of an observer travelling with speed c along the positive x-direction. Consequently this moving observer sees a constant displacement of the string, whereas to a stationary observer, the initial proﬁle u x, 0  moves with speed c along the x-axis as if it were a rigid  system. Thus f x − ct  represents a wave form of constant shape travelling along  the positive x-axis with speed c, the actual form of the wave depending upon the function f. Similarly, the term g x + ct  is a constant wave form travelling with speed c in the negative x-direction. The general solution  20.23  represents a superposition of these.  If the functions f and g are the same then the complete solution  20.23  represents identical progressive waves going in opposite directions. This may result in a wave pattern whose proﬁle does not progress, described as a standing wave. As a simple example, suppose both f p  and g p  have the form  §  f p  = g p  = A cos kp +  cid:4  .  Then  20.23  can be written as  u x, t  = A[cos kx − kct +  cid:4   + cos kx + kct +  cid:4  ]  = 2A cos kct  cos kx +  cid:4  .  The important thing to notice is that the shape of the wave pattern, given by the factor in x, is the same at all times but that its amplitude 2A cos kct  depends upon time. At some points x that satisfy  cos kx +  cid:4   = 0  there is no displacement at any time; such points are called nodes.  So far we have not imposed any boundary conditions on the solution  20.26 . The problem of ﬁnding a solution to the wave equation that satisﬁes given bound- ary conditions is normally treated using the method of separation of variables  §  In the usual notation, k is the wave number  = 2π wavelength  and kc = ω, the angular frequency of the wave.  693   PDES: GENERAL AND PARTICULAR SOLUTIONS  discussed in the next chapter. Nevertheless, we now consider D’Alembert’s solution u x, t  of the wave equation subject to initial conditions  boundary conditions  in the following general form:  initial displacement, u x, 0  = φ x ;  initial velocity,  = ψ x .  ∂u x, 0   ∂t  The functions φ x  and ψ x  are given and describe the displacement and velocity of each part of the string at the  arbitrary  time t = 0.  It is clear that what we need are the particular forms of the functions f and g  in  20.26  that lead to the required values at t = 0. This means that  φ x  = u x, 0  = f x − 0  + g x + 0 ,  x − 0  + cg  cid:7   ψ x  =  ∂u x, 0   = −cf  x − 0  stands for df p  dp evaluated, after the diﬀerentiation, at p = x − c × 0; likewise for g Looking on the above two left-hand sides as functions of p = x ± ct, but  where it should be noted that f   x + 0 .   x + 0 ,   20.28   ∂t   cid:7    cid:7    cid:7    20.27   everywhere evaluated at t = 0, we may integrate  20.28  between an arbitrary  and irrelevant  lower limit p0 and an indeﬁnite upper limit p to obtain  the constant of integration K depending on p0. Comparing this equation with  20.27 , with x replaced by p, we can establish the forms of the functions f and g as   cid:21   1 c  p0  p  ψ q  dq + K = −f p  + g p ,   cid:21   cid:21   p0 p  p0  f p  =  g p  =  φ p   φ p   2  2  − 1 2c 1 2c  +  p  ψ q  dq − K  ψ q  dq +  ,  .  2  K 2   cid:21   Adding  20.29  with p = x − ct to  20.30  with p = x + ct gives as the solution to  the original problem  u x, t  =  1 2  [φ x − ct  + φ x + ct ] +  1 2c  x+ct  x−ct  ψ q  dq,   20.31   in which we notice that all dependence on p0 has disappeared.  Each of the terms in  20.31  has a fairly straightforward physical interpretation. In each case the factor 1 2 represents the fact that only half a displacement proﬁle that starts at any particular point on the string travels towards any other position x, the other half travelling away from it. The ﬁrst term 1 the initial displacement at a distance ct to the left of x; this travels forward arriving at x at time t. Similarly, the second contribution is due to the initial displacement at a distance ct to the right of x. The interpretation of the ﬁnal  2 φ x − ct  arises from  694   20.29    20.30    20.5 THE DIFFUSION EQUATION  term is a little less obvious. It can be viewed as representing the accumulated transverse displacement at position x due to the passage past x of all parts of the initial motion whose eﬀects can reach x within a time t, both backward and forward travelling.  The extension to the three-dimensional wave equation of solutions of the type we have so far encountered presents no serious diﬃculty. In Cartesian coordinates the three-dimensional wave equation is  ∂2u ∂x2 +  ∂2u ∂y2 +  ∂2u ∂z2  − 1 c2  ∂2u ∂t2 = 0.   20.32   In close analogy with the one-dimensional case we try solutions that are functions of linear combinations of all four variables,  It is clear that a solution u x, y, z, t  = f p  will be acceptable provided that  p = lx + my + nz + µt.   cid:7  l2 + m2 + n2 − µ2   cid:8   d2f p  dp2 = 0.  c2  Thus, as in the one-dimensional case, f can be arbitrary provided that  Using an obvious normalisation, we take µ = ±c and l, m, n as three numbers  l2 + m2 + n2 = µ2 c2.  such that  l2 + m2 + n2 = 1.  In other words  l, m, n  are the Cartesian components of a unit vector ˆn that points along the direction of propagation of the wave. The quantity p can be  written in terms of vectors as the scalar expression p = ˆn · r ± ct, and the general  solution of  20.32  is then  u x, y, z, t  = u r, t  = f ˆn · r − ct  + g ˆn · r + ct ,   20.33   where ˆn is any unit vector. It would perhaps be more transparent to write ˆn explicitly as one of the arguments of u.  20.5 The diﬀusion equation  One important class of second-order PDEs, which we have not yet considered in detail, is that in which the second derivative with respect to one variable appears, but only the ﬁrst derivative with respect to another  usually time . This is exempliﬁed by the one-dimensional diﬀusion equation  κ  ∂2u x, t   ∂x2 =  ∂u ∂t  ,  695   20.34    PDES: GENERAL AND PARTICULAR SOLUTIONS  in which κ is a constant with the dimensions length2 × time  −1. The physical constants that go to make up κ in a particular case depend upon the nature of the process  e.g. solute diﬀusion, heat ﬂow, etc.  and the material being described. With  20.34  we cannot hope to repeat successfully the method of subsection 20.3.3, since now u x, t  is diﬀerentiated a diﬀerent number of times on the two sides of the equation; any attempted solution in the form u x, t  = f p  with p = ax + bt will lead only to an equation in which the form of f cannot be cancelled out. Clearly we must try other methods.  Solutions may be obtained by using the standard method of separation of variables discussed in the next chapter. Alternatively, a simple solution is also given if both sides of  20.34 , as it stands, are separately set equal to a constant α  say , so that  ∂2u ∂x2 =  α κ  ,  ∂u ∂t  = α.  These equations have the general solutions  u x, t  =  x2 + xg t  + h t   and u x, t  = αt + m x   α 2κ  respectively and may be made compatible with each other if g t  is taken as constant, g t  = g  where g could be zero , h t  = αt and m x  =  α 2κ x2 + gx. An acceptable solution is thus  u x, t  =  x2 + gx + αt + constant.   20.35   α 2κ  Let us now return to seeking solutions of equations by combining the inde- pendent variables in particular ways. Having seen that a linear combination of x and t will be of no value, we must search for other possible combinations. It −1 and so the  has been noted already that κ has the dimensions length2 × time  combination of variables  will be dimensionless. Let us see if we can satisfy  20.34  with a solution of the form u x, t  = f η . Evaluating the necessary derivatives we have  η =  x2 κt  df η   dη  ∂η ∂x  =   cid:7   2x κt  df η    cid:8   dη 2  ,  2 κt  df η   dη  +  2x κt  d2f η   dη2  ,  =  ∂u ∂x ∂2u ∂x2 = ∂u ∂t  = − x2 κt2  df η   .  dη  696  Substituting these expressions into  20.34  we ﬁnd that the new equation can be   20.5 THE DIFFUSION EQUATION  written entirely in terms of η,  4η  d2f η  dη2 +  2 + η   df η   dη  = 0.  This is a straightforward ODE, which can be solved as follows. Writing f df η  dη, etc., we have   cid:7    η  =  2η   η   η    cid:7  cid:7  = − 1 f  cid:7  f  η ] = − η ⇒ ln[η1 2f  cid:7   cid:21  ⇒ f  cid:7  ⇒ f η  = A  4 A η1 2   η  =  − 1 4  + c  exp   cid:9 −η   cid:10   4  η  −1 2 exp  µ  η0   cid:9 −µ   cid:10   4  dµ.  If we now write this in terms of a slightly diﬀerent variable  then dζ = 1  4 η  −1 2 dη, and the solution to  20.34  is given by exp −ν2  dν.  u x, t  = f η  = g ζ  = B  ζ   20.36   ζ =  η1 2 2  =  2 κt 1 2 ,  x   cid:21   ζ0  Here B is a constant and it should be noticed that x and t appear on the RHS −1 2. If only in the indeﬁnite upper limit ζ, and then only in the combination xt § ζ0 is chosen as zero then u x, t  is, to within a constant factor, the error function erf[x 2 κt 1 2], which is tabulated in many reference books. Only non-negative values of x and t are to be considered here, so that ζ ≥ ζ0.  Let us try to determine what kind of  say  temperature distribution and ﬂow this represents. For deﬁniteness we take ζ0 = 0. Firstly, since u x, t  in  20.36  −1 2, it is clear that all points x at times t such depends only upon the product xt −1 2 has the same value have the same temperature. Put another way, at that xt any speciﬁc time t the region having a particular temperature has moved along the positive x-axis a distance proportional to the square root of t. This is a typical diﬀusion process.  Notice that, on the one hand, at t = 0 the variable ζ → ∞ and u becomes  quite independent of x  except perhaps at x = 0 ; the solution then represents a uniform spatial temperature distribution. On the other hand, at x = 0 we have that u x, t  is identically zero for all t.  §  Take B = 2π the Appendix.  −1 2 to give the usual error function normalised in such a way that erf ∞  = 1. See  697   PDES: GENERAL AND PARTICULAR SOLUTIONS   cid:1 An infrared laser delivers a pulse of  heat  energy E to a point P on a large insulated sheet of thickness b, thermal conductivity k, speciﬁc heat s and density ρ. The sheet is initially at a uniform temperature. If u r, t  is the excess temperature a time t later, at a  point that is a distance r   cid:26  b  from P , then show that a suitable expression for u is   cid:7    cid:8   ,  u r, t  =  exp  α t  − r2 2βt   20.37   where α and β are constants.  Note that we use r instead of ρ to denote the radial coordinate in plane polars so as to avoid confusion with the density.   Further,  i  show that β = 2k  sρ ;  ii  demonstrate that the excess heat energy in the sheet is independent of t, and hence evaluate α; and  iii  prove that the total heat ﬂow past any circle of radius r is E.  The equation to be solved is the heat diﬀusion equation  k∇2u r, t  = sρ  ∂u r, t   .  ∂t  Since we only require the solution for r  cid:26  b we can treat the problem as two-dimensional with obvious circular symmetry. Thus only the r-derivative term in the expression for ∇2u  is non-zero, giving  k r  ∂ ∂r  r  ∂u ∂r  = sρ  ∂u ∂t  ,   20.38   where now u r, t  = u r, t .   i  Substituting the given expression  20.37  into  20.38  we obtain   cid:7    cid:8    cid:7   2kα βt2  r2 2βt  exp  − r2 2βt  =  sρα t2  r2 2βt  from which we ﬁnd that  20.37  is a solution, provided β = 2k  sρ .   ii  The excess heat in the system at any time t is   cid:7    cid:21  ∞   cid:8    cid:7   exp   cid:8   ,  − r2 2βt  cid:8   dr  − 1  cid:7   r t  exp  − r2 2βt  − 1  cid:21  ∞  0  bρs  u r, t 2πr dr = 2πbρsα  0 = 2πbρsαβ.   cid:7    cid:8    cid:8   The excess heat is therefore independent of t and so must be equal to the total heat input E, implying that   iii  The total heat ﬂow past a circle of radius r is   cid:21  ∞  −2πrbk  ∂u r, t   0  ∂r  E  =  E  .  α =  2πbρsβ   cid:21  ∞  cid:7   cid:13  dt = −2πrbk  0   cid:8   4πbk   cid:7 −r  cid:8  cid:14 ∞  βt  E  4πbkt   cid:7    cid:8   exp  dt  − r2 2βt  = E  exp  = E  for all r.  − r2 2βt  0  As we would expect, all the heat energy E deposited by the laser will eventually ﬂow past a circle of any given radius r.  cid:2   698   20.6 CHARACTERISTICS AND THE EXISTENCE OF SOLUTIONS  20.6 Characteristics and the existence of solutions  So far in this chapter we have discussed how to ﬁnd general solutions to various types of ﬁrst- and second-order linear PDE. Moreover, given a set of boundary conditions we have shown how to ﬁnd the particular solution  or class of solutions  that satisﬁes them. For ﬁrst-order equations, for example, we found that if the value of u x, y  is speciﬁed along some curve in the xy-plane then the solution to the PDE is in general unique, but that if u x, y  is speciﬁed at only a single point then the solution is not unique: there exists a class of particular solutions all of which satisfy the boundary condition. In this section and the next we make more rigorous the notion of the respective types of boundary condition that cause a PDE to have a unique solution, a class of solutions, or no solution at all.  20.6.1 First-order equations  Let us consider the general ﬁrst-order PDE  20.9  but now write it as  A x, y   + B x, y   = F x, y, u .   20.39   ∂u ∂x  ∂u ∂y  Suppose we wish to solve this PDE subject to the boundary condition that u x, y  = φ s  is speciﬁed along some curve C in the xy-plane that is described parametrically by the equations x = x s  and y = y s , where s is the arc length along C. The variation of u along C is therefore given by  du ds  =  ∂u ∂x  dx ds  +  ∂u ∂y  dy ds  =  dφ ds  .   20.40   We may then solve the two  inhomogeneous  simultaneous linear equations  20.39  and  20.40  for ∂u ∂x and ∂u ∂y, unless the determinant of the coeﬃcients vanishes  see section 8.18 , i.e. unless   cid:20  cid:20  cid:20  cid:20  dx ds  A   cid:20  cid:20  cid:20  cid:20  = 0.  dy ds  B  At each point in the xy-plane this equation determines a set of curves called characteristic curves  or just characteristics , which thus satisfy  or, multiplying through by ds dx and dividing through by A,  However, we have already met  20.41  in subsection 20.3.1 on ﬁrst-order PDEs, where solutions of the form u x, y  = f p , where p is some combination of x and y,   20.41   B  dx ds  − A  dy ds  = 0,  dy dx  =  B x, y  A x, y   .  699   PDES: GENERAL AND PARTICULAR SOLUTIONS  were discussed. Comparing  20.41  with  20.12  we see that the characteristics are merely those curves along which p is constant.  Since the partial derivatives ∂u ∂x and ∂u ∂y may be evaluated provided the boundary curve C does not lie along a characteristic, deﬁning u x, y  = φ s  along C is suﬃcient to specify the solution to the original problem  equation plus boundary conditions  near the curve C, in terms of a Taylor expansion about C. Therefore the characteristics can be considered as the curves along which information about the solution u x, y  ‘propagates’. This is best understood by using an example.  cid:1 Find the general solution of  x  ∂u ∂x  − 2y  ∂u ∂y  = 0   20.42   that takes the value 2y + 1 on the line x = 1 between y = 0 and y = 1.  We solved this problem in subsection 20.3.1 for the case where u x, y  takes the value 2y + 1 along the entire line x = 1. We found then that the general solution to the equation  ignoring boundary conditions  is of the form  u x, y  = f p  = f x2y ,  for some arbitrary function f. Hence the characteristics of  20.42  are given by x2y = c where c is a constant; some of these curves are plotted in ﬁgure 20.2 for various values of c. Furthermore, we found that the particular solution for which u 1, y  = 2y + 1 for all y was given by  u x, y  = 2x2y + 1.  In the present case the value of x2y is ﬁxed by the boundary conditions only between y = 0 and y = 1. However, since the characteristics are curves along which x2y, and hence f x2y , remains constant, the solution is determined everywhere along any characteristic that intersects the line segment denoting the boundary conditions. Thus u x, y  = 2x2y + 1 0 ≤ c ≤ 1 . is the particular solution that holds in the shaded region in ﬁgure 20.2  corresponding to  Outside this region, however, the solution is not precisely speciﬁed, and any function of  will satisfy both the equation and the boundary condition, provided g p  = 0 for  u x, y  = 2x2y + 1 + g x2y   the form  0 ≤ p ≤ 1.  cid:2   In the above example the boundary curve was not itself a characteristic and furthermore it crossed each characteristic once only. For a general boundary curve C this may not be the case. Firstly, if C is itself a characteristic  or is just a single point  then information about the solution cannot ‘propagate’ away from C, and so the solution remains unspeciﬁed everywhere except on C.  The second possibility is that C  although not a characteristic itself  crosses some characteristics more than once, as in ﬁgure 20.3. In this case specifying the value of u x, y  along the curve P Q determines the solution along all the character- istics that intersect it. Therefore, also specifying u x, y  along QR can overdetermine the problem solution and generally results in there being no solution.  700   20.6 CHARACTERISTICS AND THE EXISTENCE OF SOLUTIONS  y  2  1  −1  c = 1  1  x  y = c x2  x = 1  R  C  Figure 20.2 The characteristics of equation  20.42 . The shaded region shows where the solution to the equation is deﬁned, given the imposed boundary condition at x = 1 between y = 0 and y = 1, shown as a bold vertical line.  y  P  Figure 20.3 A boundary curve C that crosses characteristics more than once.  20.6.2 Second-order equations  The concept of characteristics can be extended naturally to second-  and higher-  order equations. In this case let us write the general second-order linear PDE  20.19  as   cid:7   A x, y   ∂2u ∂x2 + B x, y   ∂2u ∂x∂y  + C x, y   ∂2u ∂y2 = F  x, y, u,  ∂u ∂x  ,  ∂u ∂y  .   20.43   x   cid:8   Q  701   PDES: GENERAL AND PARTICULAR SOLUTIONS  y  C  dr  dy  dx  ˆn ds  x  Figure 20.4 A boundary curve C and its tangent and unit normal at a given point.  For second-order equations we might expect that relevant boundary conditions would involve specifying u, or some of its ﬁrst derivatives, or both, along a suitable set of boundaries bordering or enclosing the region over which a solution is sought. Three common types of boundary condition occur and are associated with the names of Dirichlet, Neumann and Cauchy. They are as follows.   i  Dirichlet: The value of u is speciﬁed at each point of the boundary.   ii  Neumann: The value of ∂u ∂n, the normal derivative of u, is speciﬁed at  each point of the boundary. Note that ∂u ∂n = ∇u · ˆn, where ˆn is the  normal to the boundary at each point.   iii  Cauchy: Both u and ∂u ∂n are speciﬁed at each point of the boundary.  Let us consider for the moment the solution of  20.43  subject to the Cauchy boundary conditions, i.e. u and ∂u ∂n are speciﬁed along some boundary curve C in the xy-plane deﬁned by the parametric equations x = x s , y = y s , s being the arc length along C  see ﬁgure 20.4 . Let us suppose that along C we have u x, y  = φ s  and ∂u ∂n = ψ s . At any point on C the vector dr = dx i + dy j is  a tangent to the curve and ˆn ds = dy i− dx j is a vector normal to the curve. Thus  on C we have  ≡ ∇u · dr ≡ ∇u · ˆn =  ds  ∂u ∂s ∂u ∂n  =  ∂u ∂x  dx ds  ∂u ∂y  dy ds  =  dφ s   ,  ds  + − ∂u  ∂y  dx ds  ∂u ∂x  dy ds  = ψ s .  These two equations may then be solved straightforwardly for the ﬁrst partial derivatives ∂u ∂x and ∂u ∂y along C. Using the chain rule to write  d ds  =  dx ds  ∂ ∂x  +  dy ds  ∂ ∂y  ,  702   20.6 CHARACTERISTICS AND THE EXISTENCE OF SOLUTIONS  we may diﬀerentiate the two ﬁrst derivatives ∂u ∂x and ∂u ∂y along the boundary to obtain the pair of equations   cid:8   cid:8    cid:7   cid:7   d ds  d ds  ∂u ∂x  ∂u ∂y  =  =  dx ds  dx ds  ∂2u ∂x2 + ∂2u ∂x∂y  dy ds  ,  ∂2u ∂x∂y ∂2u ∂y2 .  dy ds  +   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  = 0.   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20   A dx ds  0   cid:7   B dy ds dx ds   cid:8  cid:7   C  0  dy ds   cid:8   2 − B  cid:7   A  dy dx  dx ds  dy ds   cid:8  2 − B  We may now solve these two equations, together with the original PDE  20.43 , for the second partial derivatives of u, except where the determinant of their coeﬃcients equals zero,  Expanding out the determinant,   cid:7    cid:8   A  dy ds   cid:7    cid:8   2  dx ds  + C  = 0.  Multiplying through by  ds dx 2 we obtain  dy dx  + C = 0,   20.44   which is the ODE for the curves in the xy-plane along which the second partial derivatives of u cannot be found.  As for the ﬁrst-order case, the curves satisfying  20.44  are called characteristics of the original PDE. These characteristics have tangents at each point given by   when A  cid:3 = 0   B ± √  B2 − 4AC  2A  .  dy dx  =   20.45   Clearly, when the original PDE is hyperbolic  B2 > 4AC , equation  20.45  deﬁnes two families of real curves in the xy-plane; when the equation is parabolic  B2 = 4AC  it deﬁnes one family of real curves; and when the equation is elliptic  B2 < 4AC  it deﬁnes two families of complex curves. Furthermore, when A, B and C are constants, rather than functions of x and y, the equations of the characteristics will be of the form x + λy = constant, which is reminiscent of the form of solution discussed in subsection 20.3.3.  703   PDES: GENERAL AND PARTICULAR SOLUTIONS  ct  0  x − ct = constant  L  x  x + ct = constant  Figure 20.5 The characteristics for the one-dimensional wave equation. The shaded region indicates the region over which the solution is determined by specifying Cauchy boundary conditions at t = 0 on the line segment x = 0 to x = L.   cid:1 Find the characteristics of the one-dimensional wave equation  ∂2u ∂x2   cid:7   − 1 c2  cid:8   dx dt  ∂2u ∂t2  = 0.  2  = c2,  This is a hyperbolic equation with A = 1, B = 0 and C = −1 c2. Therefore from  20.44   the characteristics are given by  and so the characteristics are the straight lines x− ct = constant and x + ct = constant.  cid:2   The characteristics of second-order PDEs can be considered as the curves along which partial information about the solution u x, y  ‘propagates’. Consider a point in the space that has the independent variables as its coordinates; unless both of the two characteristics that pass through the point intersect the curve along which the boundary conditions are speciﬁed, the solution will not be determined at that point. In particular, if the equation is hyperbolic, so that we obtain two families of real characteristics in the xy-plane, then Cauchy boundary conditions propagate partial information concerning the solution along the characteristics, belonging to each family, that intersect the boundary curve C. The solution u is then speciﬁed in the region common to these two families of characteristics. For instance, the characteristics of the hyperbolic one-dimensional wave equation in the last example are shown in ﬁgure 20.5. By specifying Cauchy boundary  704   20.7 UNIQUENESS OF SOLUTIONS  Equation type Boundary Conditions hyperbolic parabolic elliptic  Cauchy Dirichlet or Neumann Dirichlet or Neumann  open open closed  Table 20.1 The appropriate boundary conditions for diﬀerent types of partial diﬀerential equation.  conditions u and ∂u ∂t on the line segment t = 0, x = 0 to L, the solution is speciﬁed in the shaded region.  As in the case of ﬁrst-order PDEs, however, problems can arise. For example, if for a hyperbolic equation the boundary curve intersects any characteristic more than once then Cauchy conditions along C can overdetermine the problem, resulting in there being no solution. In this case either the boundary curve C must be altered, or the boundary conditions on the oﬀending parts of C must be relaxed to Dirichlet or Neumann conditions.  The general considerations involved in deciding which boundary conditions are appropriate for a particular problem are complex, and we do not discuss them § We merely note that whether the various types of boundary any further here. condition are appropriate  in that they give a solution that is unique, sometimes to within a constant, and is well deﬁned  depends upon the type of second-order equation under consideration and on whether the region of solution is bounded by a closed or an open curve  or a surface if there are more than two independent variables . Note that part of a closed boundary may be at inﬁnity if conditions are imposed on u or ∂u ∂n there.  It may be shown that the appropriate boundary-condition and equation-type  pairings are as given in table 20.1.  For example, Laplace’s equation ∇2u = 0 is elliptic and thus requires either  Dirichlet or Neumann boundary conditions on a closed boundary which, as we have already noted, may be at inﬁnity if the behaviour of u is speciﬁed there   most often u or ∂u ∂n → 0 at inﬁnity .  20.7 Uniqueness of solutions  Although we have merely stated the appropriate boundary types and conditions for which, in the general case, a PDE has a unique, well-deﬁned solution, some- times to within an additive constant, it is often important to be able to prove that a unique solution is obtained.  §  For a discussion the reader is referred, for example, to P. M. Morse and H. Feshbach, Methods of Theoretical Physics, Part I  New York: McGraw-Hill, 1953 , chap. 6.  705   PDES: GENERAL AND PARTICULAR SOLUTIONS  As an important example let us consider Poisson’s equation in three dimensions,  ∇2u r  = ρ r ,   20.46   with either Dirichlet or Neumann conditions on a closed boundary appropriate to such an elliptic equation; for brevity, in  20.46 , we have absorbed any physical constants into ρ. We aim to show that, to within an unimportant constant, the solution of  20.46  is unique if either the potential u or its normal derivative ∂u ∂n is speciﬁed on all surfaces bounding a given region of space  including, if necessary, a hypothetical spherical surface of indeﬁnitely large radius on which u or ∂u ∂n is prescribed to have an arbitrarily small value . Stated more formally this is as follows.  Uniqueness theorem. If u is real and its ﬁrst and second partial derivatives are  continuous in a region V and on its boundary S , and ∇2u = ρ in V and either  u = f or ∂u ∂n = g on S , where ρ, f and g are prescribed functions, then u is unique  at least to within an additive constant .   cid:1 Prove the uniqueness theorem for Poisson’s equation.  Let us suppose on the contrary that two solutions u1 r  and u2 r  both satisfy the conditions  given above, and denote their diﬀerence by the function w = u1 − u2. We then have  ∇2w = ∇2u1 − ∇2u2 = ρ − ρ = 0,  so that w satisﬁes Laplace’s equation in V . Furthermore, since either u1 = f = u2 or ∂u1 ∂n = g = ∂u2 ∂n on S , we must have either w = 0 or ∂w ∂n = 0 on S .  If we now use Green’s ﬁrst theorem,  11.19 , for the case where both scalar functions  are taken as w we have cid:21    cid:19    cid:21   w∇2w +  ∇w  ·  ∇w   dV =  w  ∂w ∂n  S  dS .   cid:18   V  However, either condition, w = 0 or ∂w ∂n = 0, makes the RHS vanish whilst the ﬁrst  term on the LHS vanishes since ∇2w = 0 in V . Thus we are left with   cid:21   Since ∇w2 can never be negative, this can only be satisﬁed if  V  ∇w2 dV = 0.  ∇w = 0,  i.e. if w, and hence u1 − u2, is a constant in V .  If Dirichlet conditions are given then u1 ≡ u2 on  some part of  S and hence u1 = u2  everywhere in V . For Neumann conditions, however, u1 and u2 can diﬀer throughout V by an arbitrary  but unimportant  constant.  cid:2   The importance of this uniqueness theorem lies in the fact that if a solution to Poisson’s  or Laplace’s  equation that ﬁts the given set of Dirichlet or Neumann conditions can be found by any means whatever, then that solution is the correct one, since only one exists. This result is the mathematical justiﬁcation for the method of images, which is discussed more fully in the next chapter.  706   20.8 EXERCISES  We also note that often the same general method, used in the above example for proving the uniqueness theorem for Poisson’s equation, can be employed to prove the uniqueness  or otherwise  of solutions to other equations and boundary conditions.  20.8 Exercises  20.1  Determine whether the following can be written as functions of p = x2 + 2y only, and hence whether they are solutions of  20.8 :  a  x2 x2 − 4  + 4y x2 − 2  + 4 y2 − 1 ;  b  x4 + 2x2y + y2;  c   [x4 + 4x2y + 4y2 + 4] [2x4 + x2 8y + 1  + 8y2 + 2y].  20.2  Find partial diﬀerential equations satisﬁed by the following functions u x, y  for all arbitrary functions f and all arbitrary constants a and b:   a  u x, y  = f x2 − y2 ;  b  u x, y  =  x − a 2 +  y − b 2;   c  u x, y  = ynf y x ;  d  u x, y  = f x + ay .  20.3  Solve the following partial diﬀerential equations for u x, y  with the boundary conditions given:   a  x  + xy = u,  u = 2y on the line x = 1;   b  1 + x  = xu,  u x, 0  = x.  20.4  Find the most general solutions u x, y  of the following equations, consistent with the boundary conditions stated:   a  y  = 0, u x, 0  = 1 + sin x;   b  i  = 3  u =  4 + 3i x2 on the line x = y;   c   sin x sin y  + cos x cos y  = 0, u = cos 2y on x + y = π 2;  ∂u ∂y   d   + 2x  = 0, u = 2 on the parabola y = x2.  20.5  Find solutions of  1 x  ∂u ∂x  +  1 y  ∂u ∂y  = 0  20.6  for which  a  u 0, y  = y and  b  u 1, 1  = 1. Find the most general solutions u x, y  of the following equations consistent with the boundary conditions stated:   a  y  − x  ∂u ∂y  ∂u ∂x  = 3x, u = x2 on the line y = 0;  707  ∂u ∂x  ∂u ∂x  ∂u ∂x  ∂u ∂x  ∂u ∂y  − x  ∂u ∂y  ∂u ∂y  ,  ∂u ∂x  ∂u ∂y   PDES: GENERAL AND PARTICULAR SOLUTIONS   b  y  ∂u ∂x  c  y2 ∂u ∂x  − x  ∂u ∂y + x2 ∂u ∂y  20.7  Solve  = 3x, u 1, 0  = 2;  = x2y2 x3 + y3 , no boundary conditions.  20.8  subject to  a  u π 2, y  = 0 and  b  u π 2, y  = y y + 1 . A function u x, y  satisﬁes  20.9  and takes the value 3 on the line y = 4x. Evaluate u 2, 4 . If u x, y  satisﬁes  sin x  + cos x  = cos x  ∂u ∂x  ∂u ∂y  2  ∂u ∂x  + 3  = 10,  ∂u ∂y  ∂2u ∂x2  − 3  ∂2u ∂x∂y  + 2  = 0  ∂2u ∂y2  and u = −x2 and ∂u ∂y = 0 for y = 0 and all x, ﬁnd the value of u 0, 1 .  20.10  Consider the partial diﬀerential equation  ∂2u ∂x2  − 3  ∂2u ∂x∂y  + 2  = 0.  ∂2u ∂y2   ∗    a  Find the function u x, y  that satisﬁes  ∗  and the boundary condition u =  ∂u ∂y = 1 when y = 0 for all x. Evaluate u 0, 1 .   b  In which region of the xy-plane would u be determined if the boundary  condition were u = ∂u ∂y = 1 when y = 0 for all x > 0?  20.11  In those cases in which it is possible to do so, evaluate u 2, 2 , where u x, y  is the solution of  2y  ∂u ∂x  − x  ∂u ∂y  = xy 2y2 − x2   that satisﬁes the  separate  boundary conditions given below.   a  u x, 1  = x2 for all x.  b  u x, 1  = x2 for x ≥ 0.  c  u x, 1  = x2 for 0 ≤ x ≤ 3.  d  u x, 0  = x for x ≥ 0.  √  e  u x, 0  = x for all x. √ 10  = 5.  f  u 1,  g  u  10, 1  = 5.  20.12  Solve  subject to u = 2x + 1 and ∂u ∂y = 4 − 6x, both on the line y = 0.  20.13  By changing the independent variables in the previous exercise to  − 5  6  ∂2u ∂x2  ∂2u ∂x∂y  +  ∂2u ∂y2  = 14,  ξ = x + 2y  and  η = x + 3y,  show that it must be possible to write 14 x2 + 5xy + 6y2  in the form  f1 x + 2y  + f2 x + 3y  −  x2 + y2 ,  and determine the forms of f1 z  and f2 z .  708   20.14  Solve  20.8 EXERCISES  ∂2u ∂x∂y  ∂2u ∂y2  + 3  = x 2y + 3x .  20.15 20.16  Find the most general solution of ∂2u ∂x2 + ∂2u ∂y2 = x2y2. An inﬁnitely long string on which waves travel at speed c has an initial displace- ment    sin πx a , −a ≤ x ≤ a,  x > a.  y x  =  0,  It is released from rest at time t = 0, and its subsequent displacement is described by y x, t .  By expressing the initial displacement as one explicit function incorporating Heaviside step functions, ﬁnd an expression for y x, t  at a general time t > 0. In particular, determine the displacement as a function of time  a  at x = 0,  b  at x = a, and  c  at x = a 2. The non-relativistic Schr¨odinger equation  20.7  is similar to the diﬀusion equa- tion in having diﬀerent orders of derivatives in its various terms; this precludes solutions that are arbitrary functions of particular linear combinations of vari- ables. However, since exponential functions do not change their forms under diﬀerentiation, solutions in the form of exponential functions of combinations of the variables may still be possible.  Consider the Schr¨odinger equation for the case of a constant potential, i.e. for a free particle, and show that it has solutions of the form A exp lx + my + nz + λt , where the only requirement is that   cid:5   −  cid:1 2 2m   cid:6   l2 + m2 + n2  = i cid:1 λ.  In particular, identify the equation and wavefunction obtained by taking λ as  −iE  cid:1 , and l, m and n as ipx  cid:1 , ipy  cid:1  and ipz  cid:1 , respectively, where E is the  energy and p the momentum of the particle; these identiﬁcations are essentially the content of the de Broglie and Einstein relationships. Like the Schr¨odinger equation of the previous exercise, the equation describing the transverse vibrations of a rod,  20.18  a4 ∂4u ∂x4  +  ∂2u ∂t2  = 0,  has diﬀerent orders of derivatives in its various terms. Show, however, that it has solutions of exponential form, u x, t  = A exp λx + iωt , provided that the relation a4λ4 = ω2 is satisﬁed.  Use a linear combination of such allowed solutions, expressed as the sum of sinusoids and hyperbolic sinusoids of λx, to describe the transverse vibrations of a rod of length L clamped at both ends. At a clamped point both u and ∂u ∂x must vanish; show that this implies that cos λL  cosh λL  = 1, thus determining the frequencies ω at which the rod can vibrate. An incompressible ﬂuid of density ρ and negligible viscosity ﬂows with velocity v along a thin, straight, perfectly light and ﬂexible tube, of cross-section A which is held under tension T . Assume that small transverse displacements u of the tube are governed by   cid:7    cid:8   ∂2u ∂t2  + 2v  ∂2u ∂x∂t  +  v2 − T  ρA  ∂2u ∂x2  = 0.   a  Show that the general solution consists of a superposition of two waveforms  travelling with diﬀerent speeds.  709  20.17  20.19   PDES: GENERAL AND PARTICULAR SOLUTIONS   b  The tube initially has a small transverse displacement u = a cos kx and is  suddenly released from rest. Find its subsequent motion.  20.20  20.21  A sheet of material of thickness w, speciﬁc heat capacity c and thermal con- ductivity k is isolated in a vacuum, but its two sides are exposed to ﬂuxes of radiant heat of strengths J1 and J2. Ignoring short-term transients, show that the  temperature diﬀerence between its two surfaces is steady at  J2 − J1 w 2k, whilst  their average temperature increases at a rate  J2 + J1  cw. In an electrical cable of resistance R and capacitance C, each per unit length, voltage signals obey the equation ∂2V  ∂x2 = RC∂V  ∂t. This has solutions of the form given in  20.36  and also of the form V = Ax + D.   a  Find a combination of these that represents the situation after a steady  voltage V0 is applied at x = 0 at time t = 0.   b  Obtain a solution describing the propagation of the voltage signal resulting from the application of the signal V = V0 for 0 < t < T , V = 0 otherwise, to the end x = 0 of an inﬁnite cable.   c  Show that for t  cid:26  T the maximum signal occurs at a value of x proportional  to t1 2 and has a magnitude proportional to t  −1.  20.22  The daily and annual variations of temperature at the surface of the earth may be represented by sine-wave oscillations, with equal amplitudes and periods of 1 day and 365 days respectively. Assume that for  angular  frequency ω the  temperature at depth x in the earth is given by u x, t  = A sin ωt + µx  exp −λx ,  where λ and µ are constants.   a  Use the diﬀusion equation to ﬁnd the values of λ and µ.  b  Find the ratio of the depths below the surface at which the two amplitudes  have dropped to 1 20 of their surface values.   c  At what time of year is the soil coldest at the greater of these depths, assuming that the smoothed annual variation in temperature at the surface has a minimum on February 1st?  20.23  Consider each of the following situations in a qualitative way and determine the equation type, the nature of the boundary curve and the type of boundary conditions involved:   a  a conducting bar given an initial temperature distribution and then thermally  isolated;   b  two long conducting concentric cylinders, on each of which the voltage   c   distribution is speciﬁed; two long conducting concentric cylinders, on each of which the charge distribution is speciﬁed;   d  a semi-inﬁnite string, the end of which is made to move in a prescribed way.  20.24  This example gives a formal demonstration that the type of a second-order PDE  elliptic, parabolic or hyperbolic  cannot be changed by a new choice of independent variable. The algebra is somewhat lengthy, but straightforward.  If a change of variable ξ = ξ x, y , η = η x, y  is made in  20.19 , so that it  reads  show that   cid:7  ∂2u ∂ξ2  A  + B   cid:7  ∂2u ∂ξ∂η   cid:7  ∂2u ∂η2   cid:7  ∂u ∂ξ  + C  + D  + E  + F  u = R   ξ, η ,   cid:7    cid:7  ∂u ∂η   cid:13    cid:7    cid:14   ∂ ξ, η  ∂ x, y   2  .   cid:7 2 − 4A   cid:7   B  C   cid:7   =  B2 − 4AC   Hence deduce the conclusion stated above.  710   20.9 HINTS AND ANSWERS  20.25  The Klein–Gordon equation  which is satisﬁed by the quantum-mechanical wave- function Φ r  of a relativistic spinless particle of non-zero mass m  is  ∇2Φ − m2Φ = 0.  Show that the solution for the scalar ﬁeld Φ r  in any volume V bounded by a surface S is unique if either Dirichlet or Neumann boundary conditions are speciﬁed on S .  20.1 20.3  20.5 20.7 20.9  20.11  20.13  20.15  20.17  20.19  20.21   a  Yes, p2 − 4p − 4;  b  no,  p − y 2;  c  yes,  p2 + 4   2p2 + p .  20.9 Hints and answers  Each equation is eﬀectively an ordinary diﬀerential equation, but with a function of the non-integrated variable as the constant of integration;   a  u = xy 2 − ln x ;  b  u = x  a   y2 − x2 1 2;  b  1 + f y2 − x2 , where f 0  = 0. u = y + f y − ln sin x  ;  a  u = ln sin x ;  b  u = y + [y − ln sin x ]2. General solution is u x, y  = f x + y  + g x + y 2 . Show that 2p = −g and hence g p  = k − 2p2, whilst f p  = p2 − k, leading to u x, y  = −x2 + y2 2;  −1 1 − ey  + xey.   p  2,   cid:7   u 0, 1  = 1 2. p = x2 + 2y2; u x, y  = f p  + x2y2 2.  a  u x, y  =  x2 + 2y2 + x2y2 − 2  2; u 2, 2  = 13. The line y = 1 cuts each  characteristic in zero or two distinct points, but this causes no diﬃculty with the given boundary conditions.   b  As in  a .  c  The solution is deﬁned over the space between the ellipses p = 2 and p = 11;  √  2, 2  lies on p = 12, and so u 2, 2  is undetermined. 12.   d  u x, y  =  x2 + 2y2 1 2 + x2y2 2; u 2, 2  = 8 +  e  The line y = 0 cuts each characteristic in two distinct points. No diﬀerentiable  form of f p  gives f ±a  = ±a respectively, and so there is no solution.  2  4  4  = 13.   f  The solution is only speciﬁed on p = 21, and so u 2, 2  is undetermined.  g  The solution is speciﬁed on p = 12, and so u 2, 2  = 5 + 1 The equation becomes ∂2f ∂ξ∂η = −14, with solution f ξ, η  = f ξ +g η −14ξη, which can be compared with the answer from the previous question; f1 z  = 10z2 and f2 z  = 5z2. u x, y  = f x + iy  + g x − iy  +  1 12 x4 y2 −  1 15 x2 . In the last term, x and speciﬁc PI, e.g. [ 15x2y2 x2 + y2  −  x6 + y6  ] 360. E = p2  2m , the relationship between energy and momentum for a non- relativistic particle; u r, t  = A exp[i p · r − Et   cid:1 ], a plane wave of wave number k = p  cid:1  and angular frequency ω = E  cid:1  travelling in the direction p p.  a  c = v ± α where α2 = T  ρA;  b  u x, t  = a cos[k x − vt ] cos kαt  −  va α  sin[k x − vt ] sin kαt .  y may be interchanged. There are  inﬁnitely  many other possibilities for the   cid:13    cid:14   ;   cid:1  1  √ π    a  V0  2 x CR t 1 2  1 −  2   exp −ν2  dν  cid:21  1 −V0 applied at t = T and continued;  V  x, t  =  2V0√  π  2 x[CR  t−T  ]1 2  1 2 x CR t 1 2   b  consider the input as equivalent to V0 applied at t = 0 and continued and   cid:5 −ν2  exp   cid:6  V0T exp − 1 2    dν;   2π 1 2t  .   c  For t  cid:26  T , maximum at x = [2t  CR ]1 2 with value  711   PDES: GENERAL AND PARTICULAR SOLUTIONS  20.23   a  Parabolic, open, Dirichlet u x, 0  given, Neumann ∂u ∂x = 0 at x = ±L 2  for all t;   b  elliptic, closed, Dirichlet;  c  elliptic, closed, Neumann ∂u ∂n = σ  cid:4 0;  d  hyperbolic, open, Cauchy. Follow an argument similar to that in section 20.7 and argue that the additional term  m2w2 dV must be zero, and hence that w = 0 everywhere.   cid:1   20.25  712   Partial diﬀerential equations:  separation of variables and other  21  methods  In the previous chapter we demonstrated the methods by which general solutions of some partial diﬀerential equations  PDEs  may be obtained in terms of arbitrary functions. In particular, solutions containing the independent variables in deﬁnite combinations were sought, thus reducing the eﬀective number of them. In the present chapter we begin by taking the opposite approach, namely that of trying to keep the independent variables as separate as possible, using the method of separation of variables. We then consider integral transform methods by which one of the independent variables may be eliminated, at least from diﬀerential coeﬃcients. Finally, we discuss the use of Green’s functions in solving inhomogeneous problems.  21.1 Separation of variables: the general method  Suppose we seek a solution u x, y, z, t  to some PDE  expressed in Cartesian § coordinates . Let us attempt to obtain one that has the product form  u x, y, z, t  = X x Y  y Z z T  t .   21.1   A solution that has this form is said to be separable in x, y, z and t, and seeking solutions of this form is called the method of separation of variables.  As simple examples we may observe that, of the functions   i  xyz2 sin bt,   ii  xy + zt,   iii   x2 + y2 z cos ωt,   i  is completely separable,  ii  is inseparable in that no single variable can be separated out from it and written as a multiplicative factor, whilst  iii  is separable in z and t but not in x and y.  §  It should be noted that the conventional use here of upper-case  capital  letters to denote the functions of the corresponding lower-case variable is intended to enable an easy correspondence between a function and its argument to be made.  713   PDES: SEPARATION OF VARIABLES AND OTHER METHODS  When seeking PDE solutions of the form  21.1 , we are requiring not that there is no connection at all between the functions X, Y , Z and T  for example, certain parameters may appear in two or more of them , but only that X does not depend upon y, z, t, that Y does not depend on x, z, t, and so on.  For a general PDE it is likely that a separable solution is impossible, but certainly some common and important equations do have useful solutions of this form, and we will illustrate the method of solution by studying the three- dimensional wave equation  ∇2u r  =  1 c2  ∂2u r   ∂t2  .   21.2    21.3   We will work in Cartesian coordinates for the present and assume a solution of the form  21.1 ; the solutions in alternative coordinate systems, e.g. spherical or cylindrical polars, are considered in section 21.3. Expressed in Cartesian coordinates  21.2  takes the form  ∂2u ∂x2 +  ∂2u ∂y2 +  ∂2u ∂z2 =  1 c2  ∂2u ∂t2 ;  substituting  21.1  gives  d2X dx2 Y Z T + X which can also be written as  d2Y dy2 Z T + XY  d2Z dz2 T =  1 c2 XY Z  d2T dt2 ,   cid:7  cid:7    cid:7  cid:7   X  Y Z T + XY  Z T + XY Z  T =   cid:7  cid:7   1 c2 XY Z T   cid:7  cid:7   ,   21.4   where in each case the primes refer to the ordinary derivative with respect to the independent variable upon which the function depends. This emphasises the fact that each of the functions X, Y , Z and T has only one independent variable and thus its only derivative is its total derivative. For the same reason, in each term in  21.4  three of the four functions are unaltered by the partial diﬀerentiation and behave exactly as constant multipliers.  If we now divide  21.4  throughout by u = XY Z T we obtain   cid:7  cid:7   X X   cid:7  cid:7   Y Y   cid:7  cid:7   Z Z  +  +  =   cid:7  cid:7   .  1 c2  T T   21.5   This form shows the particular characteristic that is the basis of the method of separation of variables, namely that of the four terms the ﬁrst is a function of x only, the second of y only, the third of z only and the RHS a function of t only and yet there is an equation connecting them. This can only be so for all x, y, z and t if each of the terms does not in fact, despite appearances, depend upon the corresponding independent variable but is equal to a constant, the four constants being such that  21.5  is satisﬁed.  714   21.1 SEPARATION OF VARIABLES: THE GENERAL METHOD  Since there is only one equation to be satisﬁed and four constants involved, there is considerable freedom in the values they may take. For the purposes of  our illustrative example let us make the choice of −l2, −m2, −n2, for the ﬁrst −µ2 = − l2 + m2 + n2 .  three constants. The constant associated with c   T must then have the value  −2T   cid:7  cid:7   Having recognised that each term of  21.5  is individually equal to a constant  or parameter , we can now replace  21.5  by four separate ordinary diﬀerential equations  ODEs :   cid:7  cid:7   X X  = −l2,   cid:7  cid:7   Y Y  = −m2,   cid:7  cid:7   Z Z  = −n2,   cid:7  cid:7   1 c2  T T  = −µ2.   21.6   The important point to notice is not the simplicity of the equations  21.6   the corresponding ones for a general PDE are usually far from simple  but that, by the device of assuming a separable solution, a partial diﬀerential equation  21.3 , containing derivatives with respect to the four independent variables all in one equation, has been reduced to four separate ordinary diﬀerential equations  21.6 . The ordinary equations are connected through four constant parameters that satisfy an algebraic relation. These constants are called separation constants.  The general solutions of the equations  21.6  can be deduced straightforwardly  and are  where A, B, . . . , H are constants, which may be determined if boundary condtions are imposed on the solution. Depending on the geometry of the problem and any boundary conditions, it is sometimes more appropriate to write the solutions  21.7  in the alternative form  X x  = A exp ilx  + B exp −ilx , Y  y  = C exp imy  + D exp −imy , Z z  = E exp inz  + F exp −inz , T  t  = G exp icµt  + H exp −icµt ,  Y  y  = C   cid:7  X x  = A  cid:7   cid:7   cid:7  T  t  = G  Z z  = E   cid:7   cos lx + B  cos my + D  cid:7   cos nz + F  sin lx,  cid:7   sin my,  sin nz,  cid:7   cos cµt  + H  sin cµt ,   21.7    21.8    cid:7  for some diﬀerent set of constants A to represent the solution depends on the problem being considered.  , . . . , H  , B   cid:7    cid:7   . Clearly the choice of how best  As an example, suppose that we take as particular solutions the four functions  X x  = exp ilx ,  Z z  = exp inz ,  Y  y  = exp imy ,  T  t  = exp −icµt .  715   PDES: SEPARATION OF VARIABLES AND OTHER METHODS  This gives a particular solution of the original PDE  21.3   u x, y, z, t  = exp ilx  exp imy  exp inz  exp −icµt   = exp[i lx + my + nz − cµt ],  which is a special case of the solution  20.33  obtained in the previous chapter and represents a plane wave of unit amplitude propagating in a direction given by the vector with components l, m, n in a Cartesian coordinate system. In the conventional notation of wave theory, l, m and n are the components of the wave-number vector k, whose magnitude is given by k = 2π λ, where λ is the wavelength of the wave; cµ is the angular frequency ω of the wave. This gives the equation in the form  u x, y, z, t  = exp[i kxx + kyy + kzz − ωt ]  = exp[i k · r − ωt ],  and makes the exponent dimensionless.  The method of separation of variables can be applied to many commonly  occurring PDEs encountered in physical applications.  cid:1 Use the method of separation of variables to obtain for the one-dimensional diﬀusion equation  a solution that tends to zero as t → ∞ for all x.  κ  ∂2u ∂x2  =  ∂u ∂t  ,  Here we have only two independent variables x and t and we therefore assume a solution of the form  Substituting this expression into  21.9  and dividing through by u = XT  and also by κ  we obtain  Now, arguing exactly as above that the LHS is a function of x only and the RHS is a function of t only, we conclude that each side must equal a constant, which, anticipating  the result and noting the imposed boundary condition, we will take as −λ2. This gives us  two ordinary equations,   21.9    21.10   21.11   which have the solutions  X x  = A cos λx + B sin λx,  T  t  = C exp −λ2κt .  Combining these to give the assumed solution u = XT yields  absorbing the constant C into A and B   u x, t  =  A cos λx + B sin λx  exp −λ2κt .   21.12   u x, t  = X x T  t .   cid:7  cid:7   X X   cid:7   T κT  .  =   cid:7  cid:7   X  cid:7   T  + λ2X = 0, + λ2κT = 0,  716   21.2 SUPERPOSITION OF SEPARATED SOLUTIONS  In order to satisfy the boundary condition u → 0 as t → ∞, λ2κ must be > 0. Since κ  is real and > 0, this implies that λ is a real non-zero number and that the solution is sinusoidal in x and is not a disguised hyperbolic function; this was our reason for choosing  the separation constant as −λ2.  cid:2   As a ﬁnal example we consider Laplace’s equation in Cartesian coordinates;  this may be treated in a similar manner.  cid:1 Use the method of separation of variables to obtain a solution for the two-dimensional Laplace equation,  ∂2u ∂x2  +  ∂2u ∂y2  = 0.   21.13   If we assume a solution of the form u x, y  = X x Y  y  then, following the above method, and taking the separation constant as λ2, we ﬁnd  cid:7  cid:7    cid:7  cid:7   = −λ2Y .  Y Taking λ2 as > 0, the general solution becomes  X  = λ2X,  u x, y  =  A cosh λx + B sinh λx  C cos λy + D sin λy .   21.14   An alternative form, in which the exponentials are written explicitly, may be useful for other geometries or boundary conditions:  u x, y  = [A exp λx + B exp −λx ] C cos λy + D sin λy ,   21.15   with diﬀerent constants A and B.  If λ2 < 0 then the roles of x and y interchange. The particular combination of sinusoidal and hyperbolic functions and the values of λ allowed will be determined by the geometrical properties of any speciﬁc problem, together with any prescribed or necessary boundary conditions.  cid:2   We note here that a particular case of the solution  21.14  links up with the ‘combination’ result u x, y  = f x + iy  of the previous chapter  equations  20.24  and following , namely that if A = B and D = iC then the solution is the same as f p  = AC exp λp with p = x + iy.  21.2 Superposition of separated solutions  It will be noticed in the previous two examples that there is considerable freedom in the values of the separation constant λ, the only essential requirement being that λ has the same value in both parts of the solution, i.e. the part depending on x and the part depending on y  or t . This is a general feature for solutions in separated form, which, if the original PDE has n independent variables, will  contain n − 1 separation constants. All that is required in general is that we  associate the correct function of one independent variable with the appropriate functions of the others, the correct function being the one with the same values of the separation constants.  If the original PDE is linear  as are the Laplace, Schr¨odinger, diﬀusion and wave equations  then mathematically acceptable solutions can be formed by  717   PDES: SEPARATION OF VARIABLES AND OTHER METHODS  superposing solutions corresponding to diﬀerent allowed values of the separation constants. To take a two-variable example: if  uλ1  x, y  = Xλ1  x Yλ1  y   is a solution of a linear PDE obtained by giving the separation constant the value λ1, then the superposition  u x, y  = a1Xλ1  x Yλ1  y  + a2Xλ2  x Yλ2  y  + ··· =  aiXλi  x Yλi  y    21.16    cid:4   i  is also a solution for any constants ai, provided that the λi are the allowed values of the separation constant λ given the imposed boundary conditions. Note that if the boundary conditions allow any of the separation constants to be zero then the form of the general solution is normally diﬀerent and must be deduced by returning to the separated ordinary diﬀerential equations. We will encounter this behaviour in section 21.3.  The value of the superposition approach is that a boundary condition, say that u x, y  takes a particular form f x  when y = 0, might be met by choosing the constants ai such that   cid:4   i  f x  =  aiXλi  x Yλi  0 .  In general, this will be possible provided that the functions Xλi  x  form a complete set – as do the sinusoidal functions of Fourier series or the spherical harmonics discussed in subsection 18.3.   cid:1 A semi-inﬁnite rectangular metal plate occupies the region 0 ≤ x ≤ ∞ and 0 ≤ y ≤ b in  ◦  the xy-plane. The temperature at the far end of the plate and along its two long sides is ﬁxed at 0 C. If the temperature of the plate at x = 0 is also ﬁxed and is given by f y , ﬁnd the steady-state temperature distribution u x,y  of the plate. Hence ﬁnd the temperature distribution if f y  = u0, where u0 is a constant.  The physical situation is illustrated in ﬁgure 21.1. With the notation we have used several times before, the two-dimensional heat diﬀusion equation satisﬁed by the temperature u x, y, t  is   cid:7    cid:8   with κ = k  sρ . In this case, however, we are asked to ﬁnd the steady-state temperature, which corresponds to ∂u ∂t = 0, and so we are led to consider the  two-dimensional  Laplace equation  We saw that assuming a separable solution of the form u x, y  = X x Y  y  led to solutions such as  21.14  or  21.15 , or equivalent forms with x and y interchanged. In the current problem we have to satisfy the boundary conditions u x, 0  = 0 = u x, b  and so a solution that is sinusoidal in y seems appropriate. Furthermore, since we require  u ∞, y  = 0 it is best to write the x-dependence of the solution explicitly in terms of  κ  ∂2u ∂x2  +  ∂2u ∂y2  =  ∂u ∂t  ,  ∂2u ∂x2  +  ∂2u ∂y2  = 0.  718   21.2 SUPERPOSITION OF SEPARATED SOLUTIONS  y  b  u = f y   u = 0  u → 0  x  0  u = 0  Figure 21.1 A semi-inﬁnite metal plate whose edges are kept at ﬁxed tem- peratures.  exponentials rather than of hyperbolic functions. We therefore write the separable solution in the form  21.15  as  u x, y  = [A exp λx + B exp −λx ] C cos λy + D sin λy .  Applying the boundary conditions, we see ﬁrstly that u ∞, y  = 0 implies A = 0 if we  take λ > 0. Secondly, since u x, 0  = 0 we may set C = 0, which, if we absorb the constant D into B, leaves us with  u x, y  = B exp −λx  sin λy.  But, using the condition u x, b  = 0, we require sin λb = 0 and so λ must be equal to nπ b, where n is any positive integer.  Using the principle of superposition  21.16 , the general solution satisfying the given  boundary conditions can therefore be written  u x, y  =  Bn exp −nπx b  sin nπy b ,   21.17   for some constants Bn. Notice that in the sum in  21.17  we have omitted negative values of  n since they would lead to exponential terms that diverge as x → ∞. The n = 0 term is also  omitted since it is identically zero. Using the remaining boundary condition u 0, y  = f y  we see that the constants Bn must satisfy  f y  =  Bn sin nπy b .   21.18   ∞ cid:4   n=1  ∞ cid:4   n=1  This is clearly a Fourier sine series expansion of f y   see chapter 12 . For  21.18  to  hold, however, the continuation of f y  outside the region 0 ≤ y ≤ b must be an odd  periodic function with period 2b  see ﬁgure 21.2 . We also see from ﬁgure 21.2 that if the original function f y  does not equal zero at either of y = 0 and y = b then its continuation has a discontinuity at the corresponding point s ; nevertheless, as discussed in chapter 12, the Fourier series will converge to the mid-points of these jumps and hence tend to zero in this case. If, however, the top and bottom edges of the plate were held not at 0 C but at some other non-zero temperature, then, in general, the ﬁnal solution would possess discontinuities at the corners x = 0, y = 0 and x = 0, y = b.  ◦  Bearing in mind these technicalities, the coeﬃcients Bn in  21.18  are given by  Bn =  f y  sin  dy.   21.19    cid:21   2 b  b  0   cid:9    cid:10   nπy  b  719   PDES: SEPARATION OF VARIABLES AND OTHER METHODS  f y   −b  0  b  y  Figure 21.2 The continuation of f y  for a Fourier sine series.  Therefore, if f y  = u0  i.e. the temperature of the side at x = 0 is constant along its length ,  21.19  becomes  Bn =  u0 sin   cid:21    cid:13   2 b  b  0  =  − 2u0 = − 2u0  b  nπ   cid:9    cid:10   nπy   cid:9   b  dy   cid:10  cid:14   cid:12   b  b nπ  cos  nπy  b  [ −1 n − 1] =  cid:9   cid:4   4u0 nπ  exp  n odd  u x, y  =  0 4u0 nπ 0  for n odd, for n even.   cid:10    cid:9    cid:10   − nπx  b  sin  nπy  b  .  cid:2   Therefore the required solution is  In the above example the boundary conditions meant that one term in each part of the separable solution could be immediately discarded, making the prob- lem much easier to solve. Sometimes, however, a little ingenuity is required in writing the separable solution in such a way that certain parts can be neglected immediately.  cid:1 Suppose that the semi-inﬁnite rectangular metal plate in the previous example is replaced by one that in the x-direction has ﬁnite length a. The temperature of the right-hand edge is ﬁxed at 0 C and all other boundary conditions remain as before. Find the steady-state temperature in the plate.  ◦  As in the previous example, the boundary conditions u x, 0  = 0 = u x, b  suggest a solution that is sinusoidal in y. In this case, however, we require u = 0 on x = a  rather than at inﬁnity  and so a solution in which the x-dependence is written in terms of hyperbolic functions, such as  21.14 , rather than exponentials is more appropriate. Moreover, since the constants in front of the hyperbolic functions are, at this stage, arbitrary, we may write the separable solution in the most convenient way that ensures that the condition u a, y  = 0 is straightforwardly satisﬁed. We therefore write  u x, y  = [A cosh λ a − x  + B sinh λ a − x ] C cos λy + D sin λy .  Now the condition u a, y  = 0 is easily satisﬁed by setting A = 0. As before the conditions u x, 0  = 0 = u x, b  imply C = 0 and λ = nπ b for integer n. Superposing the  720   21.2 SUPERPOSITION OF SEPARATED SOLUTIONS  ∞ cid:4   n=1  ∞ cid:4   n=1  solutions for diﬀerent n we then obtain  u x, y  =  Bn sinh[nπ a − x  b] sin nπy b ,   21.20   for some constants Bn. We have omitted negative values of n in the sum  21.20  since the relevant terms are already included in those obtained for positive n. Again the n = 0 term is identically zero. Using the ﬁnal boundary condition u 0, y  = f y  as above we ﬁnd that the constants Bn must satisfy  f y  =  Bn sinh nπa b  sin nπy b ,   cid:21   b  0  and, remembering the caveats discussed in the previous example, the Bn are therefore given by  2  Bn =  b sinh nπa b   f y  sin nπy b  dy.   21.21   For the case where f y  = u0, following the working of the previous example gives   21.21  as  4u0   cid:4   n odd  Bn =  nπ sinh nπa b   for n odd,  Bn = 0 for n even.   21.22   The required solution is thus  u x, y  =  4u0  nπ sinh nπa b   sinh[nπ a − x  b] sin  nπy b  .  We note that, as required, in the limit a → ∞ this solution tends to the solution of the previous example.  cid:2    cid:5    cid:6   Often the principle of superposition can be used to write the solution to problems with more complicated boundary conditions as the sum of solutions to problems that each satisfy only some part of the boundary condition but when added togther satisfy all the conditions.   cid:1 Find the steady-state temperature in the  ﬁnite  rectangular plate of the previous example, subject to the boundary conditions u x, b  = 0, u a, y  = 0 and u 0, y  = f y  as before, but now, in addition, u x, 0  = g x .  Figure 21.3 c  shows the imposed boundary conditions for the metal plate. Although we could ﬁnd a solution to this problem using the methods presented above, we can arrive at the answer almost immediately by using the principle of superposition and the result of the previous example.  Let us suppose the required solution u x, y  is made up of two parts:  where v x, y  is the solution satisfying the boundary conditions shown in ﬁgure 21.3 a ,  u x, y  = v x, y  + w x, y ,  721   PDES: SEPARATION OF VARIABLES AND OTHER METHODS  y  b  0  0  a  x  0  g x    b   0  a  x  y  b  f y   0  0   a   y  b  f y   0  a  x  Figure 21.3 Superposition of boundary conditions for a metal plate.  whilst w x, y  is the solution satisfying the boundary conditions in ﬁgure 21.3 b . It is clear that v x, y  is simply given by the solution to the previous example,   cid:4   n odd   cid:13    cid:14    cid:9    cid:10   sin  nπy  b  ,  nπ a − x   b  v x, y  =  Bn sinh  where Bn is given by  21.21 . Moreover, by symmetry, w x, y  must be of the same form as v x, y  but with x and a interchanged with y and b, respectively, and with f y  in  21.21  replaced by g x . Therefore the required solution can be written down immediately without further calculation as   cid:13   nπ a − x   b   cid:9    cid:10    cid:4   n odd  nπy  sin  +  Cn sinh   cid:13    cid:14    cid:9    cid:10   sin  nπx  a  ,  nπ b − y   a   cid:4   n odd  u x, y  =  Bn sinh  the Bn being given by  21.21  and the Cn by   cid:14   2  Cn =  a sinh nπb a   g x  sin nπx a  dx.  Clearly, this method may be extended to cases in which three or four sides of the plate have non-zero boundary conditions.  cid:2   As a ﬁnal example of the usefulness of the principle of superposition we now consider a problem that illustrates how to deal with inhomogeneous boundary conditions by a suitable change of variables.  0  g x    c   b   cid:21   a  0  722   21.2 SUPERPOSITION OF SEPARATED SOLUTIONS  ◦   cid:1 A bar of length L is initially at a temperature of 0 at 0 temperature distribution within the bar after a time t.  C. One end of the bar  x = 0  is held C and the other is supplied with heat at a constant rate per unit area of H. Find the  ◦  With our usual notation, the heat diﬀusion equation satisﬁed by the temperature u x, t  is  κ  ∂2u ∂x2  =  ∂u ∂t  ,  with κ = k  sρ , where k is the thermal conductivity of the bar, s is its speciﬁc heat capacity and ρ is its density.  The boundary conditions can be written as  u x, 0  = 0,  u 0, t  = 0,  ∂u L, t   ∂x  =  H k  ,  the last of which is inhomogeneous. In general, inhomogeneous boundary conditions can cause diﬃculties and it is usual to attempt a transformation of the problem into an equivalent homogeneous one. To this end, let us assume that the solution to our problem takes the form  where the function w x  is to be suitably determined. In terms of v and w the problem becomes  u x, t  = v x, t  + w x ,   cid:7   κ   cid:8   +  =  d2w dx2  ∂2v ∂v ∂x2 ∂t v x, 0  + w x  = 0, v 0, t  + w 0  = 0, H k  dw L   dx  =  +  ,  .  ∂v L, t   ∂x  w x  =  Hx k  .  There are several ways of choosing w x  so as to make the new problem straightforward.  Using some physical insight, however, it is clear that ultimately  at t = ∞ , when all and there will be a constant temperature gradient u x,∞  = u0x L. We therefore choose  transients have died away, the end x = L will attain a temperature u0 such that ku0 L = H  Since the second derivative of w x  is zero, v satisﬁes the diﬀusion equation and the boundary conditions on v are now given by  v x, 0  = − Hx  ,  k  v 0, t  = 0,  ∂v L, t   ∂x  = 0,  which are homogeneous in x.  From  21.12  a separated solution for the one-dimensional diﬀusion equation is  v x, t  =  A cos λx + B sin λx  exp −λ2κt ,  corresponding to a separation constant −λ2. If we restrict λ to be real then all these solutions are transient ones decaying to zero as t → ∞. These are just what is required to add to w x  to give the correct solution as t → ∞. In order to satisfy v 0, t  = 0, however,  we require A = 0. Furthermore, since  = B exp −λ2κt λ cos λx,  ∂v ∂x  723   PDES: SEPARATION OF VARIABLES AND OTHER METHODS  −L  L  x  f x   0  −HL k  Figure 21.4 The appropriate continuation for a Fourier series containing only sine terms.  in order to satisfy ∂v L, t  ∂x = 0 we require cos λL = 0, and so λ is restricted to the values  where n is an odd non-negative integer, i.e. n = 1, 3, 5, . . . .  Thus, to satisfy the boundary condition v x, 0  = −Hx k, we must have  λ =  nπ 2L  ,   cid:9    cid:10    cid:4   n odd  Bn sin  nπx 2L  = − Hx  ,  k  in the range x = 0 to x = L. In this case we must be more careful about the continuation  of the function −Hx k, for which the Fourier sine series is required. We want a series that  is odd in x  sine terms only  and continuous as x = 0 and x = L  no discontinuities, since the series must converge at the end-points . This leads to a continuation of the function = 4L. Following the discussion of section 12.3, as shown in ﬁgure 21.4, with a period of L since this continuation is odd about x = 0 and even about x = L  4 = L it can indeed be expressed as a Fourier sine series containing only odd-numbered terms.   cid:7    cid:7   The corresponding Fourier series coeﬃcients are found to be   −1  n−1  2  n2  Bn =  kπ2  −8HL  cid:4   for n odd,   cid:9    cid:10    cid:7   and thus the ﬁnal formula for u x, t  is  u x, t  =  Hx k  − 8HL kπ2   −1  n−1  2  n odd  n2  sin  exp  nπx 2L  − kn2π2t 4L2sρ  giving the temperature for all positions 0 ≤ x ≤ L and for all times t ≥ 0.  cid:2    cid:8   ,  We note that in all the above examples the boundary conditions restricted the separation constant s  to an inﬁnite number of discrete values, usually integers. If, however, the boundary conditions allow the separation constant s  λ to take a continuum of values then the summation in  21.16  is replaced by an integral over λ. This is discussed further in connection with integral transform methods in section 21.4.  724   21.3 SEPARATION OF VARIABLES IN POLAR COORDINATES  21.3 Separation of variables in polar coordinates  So far we have considered the solution of PDEs only in Cartesian coordinates, but many systems in two and three dimensions are more naturally expressed in some form of polar coordinates, in which full advantage can be taken of any inherent symmetries. For example, the potential associated with an isolated point charge has a very simple expression, q  4π cid:4 0r , when polar coordinates are used, but involves all three coordinates and square roots when Cartesians are employed. For these reasons we now turn to the separation of variables in plane polar, cylindrical polar and spherical polar coordinates.  Most of the PDEs we have considered so far have involved the operator ∇2, e.g.  the wave equation, the diﬀusion equation, Schr¨odinger’s equation and Poisson’s equation  and of course Laplace’s equation . It is therefore appropriate that we  recall the expressions for ∇2 when expressed in polar coordinate systems. From  chapter 10, in plane polars, cylindrical polars and spherical polars, respectively, we have   cid:8   cid:8   cid:8    cid:7   cid:7   cid:7   ρ  ∂ ∂ρ  ρ  ∂ ∂ρ r2 ∂ ∂r  ∇2 = ∇2 = ∇2 =  1 ρ  1 ρ  ∂ ∂ρ  ∂ ∂ρ  1 r2  ∂ ∂r  +  +  1 ρ2 1 ρ2  ∂2 ∂φ2 , ∂2 ∂φ2 + 1 ∂ ∂θ   cid:7   ∂2 ∂z2 ,  +  r2 sin θ   cid:8   sin θ  +  ∂ ∂θ  1  r2 sin2 θ  ∂2 ∂φ2 .   21.23    21.24    21.25   Of course the ﬁrst of these may be obtained from the second by taking z to be identically zero.  21.3.1 Laplace’s equation in polar coordinates  The simplest of the equations containing ∇2 is Laplace’s equation,  ∇2u r  = 0.   21.26   Since it contains most of the essential features of the other more complicated equations, we will consider its solution ﬁrst.  Laplace’s equation in plane polars  Suppose that we need to ﬁnd a solution of  21.26  that has a prescribed behaviour on the circle ρ = a  e.g. if we are ﬁnding the shape taken up by a circular drumskin when its rim is slightly deformed from being planar . Then we may seek solutions of  21.26  that are separable in ρ and φ  measured from some arbitrary radius as φ = 0  and hope to accommodate the boundary condition by examining the solution for ρ = a.  725   PDES: SEPARATION OF VARIABLES AND OTHER METHODS  Thus, writing u ρ, φ  = P  ρ Φ φ  and using the expression  21.23 , Laplace’s  equation  21.26  becomes   cid:7    cid:7    cid:8    cid:8   Φ ρ  ∂ ∂ρ  ρ  ∂P ∂ρ  +  P ρ2  ∂2Φ ∂φ2 = 0.  ρ P  ∂ ∂ρ  ρ  ∂P ∂ρ  +  1 Φ  ∂2Φ ∂φ2 = 0.  Now, employing the same device as previously, that of dividing through by u = P Φ and multiplying through by ρ2, results in the separated equation  Following our earlier argument, since the ﬁrst term on the RHS is a function of ρ only, whilst the second term depends only on φ, we obtain the two ordinary equations   cid:7    cid:8   ρ P  d dρ  = n2,  ρ  dP dρ d2Φ  dφ2 = −n2,  1 Φ   21.27    21.28   where we have taken the separation constant to have the form n2 for later convenience; for the present, n is a general  complex  number. Let us ﬁrst consider the case in which n  cid:3 = 0. The second equation,  21.28 , then  has the general solution  Φ φ  = A exp inφ  + B exp −inφ .   21.29   Equation  21.27 , on the other hand, is the homogeneous equation   cid:7  cid:7   ρ2P  + ρP   cid:7  − n2P = 0,  which must be solved either by trying a power solution in ρ or by making the substitution ρ = exp t as described in subsection 15.2.1 and so reducing it to an equation with constant coeﬃcients. Carrying out this procedure we ﬁnd  P  ρ  = Cρn + Dρ  −n.   21.30   Returning to the solution  21.29  of the azimuthal equation  21.28 , we can see that if Φ, and hence u, is to be single-valued and so not change when φ increases by 2π then n must be an integer. Mathematically, other values of n are permissible, but for the description of real physical situations it is clear that this limitation must be imposed. Having thus restricted the possible values of n in one part of the solution, the same limitations must be carried over into the radial part,  21.30 . Thus we may write a particular solution of the two-dimensional Laplace equation as  u ρ, φ  =  A cos nφ + B sin nφ  Cρn + Dρ  −n ,  726   21.3 SEPARATION OF VARIABLES IN POLAR COORDINATES  where A, B, C, D are arbitrary constants and n is any integer.  We have not yet, however, considered the solution when n = 0. In this case, the solutions of the separated ordinary equations  21.28  and  21.27 , respectively, are easily shown to be  Φ φ  = Aφ + B,  P  ρ  = C ln ρ + D.  u ρ, φ  = C ln ρ + D.  But, in order that u = P Φ is single-valued, we require A = 0, and so the solution for n = 0 is simply  absorbing B into C and D   Superposing the solutions for the diﬀerent allowed values of n, we can write  the general solution to Laplace’s equation in plane polars as  u ρ, φ  =  C0 ln ρ + D0  +   An cos nφ + Bn sin nφ  Cnρn + Dnρ  −n ,   21.31   ∞ cid:4   n=1  where n can take only integer values. Negative values of n have been omitted from the sum since they are already included in the terms obtained for positive n. We note that, since ln ρ is singular at ρ = 0, whenever we solve Laplace’s equation in a region containing the origin, C0 must be identically zero.   cid:1 A circular drumskin has a supporting rim at ρ = a. If the rim is twisted so that it is displaced vertically by a small amount  cid:4  sin φ + 2 sin 2φ , where φ is the azimuthal angle with respect to a given radius, ﬁnd the resulting displacement u ρ, φ  over the entire drumskin.  The transverse displacement of a circular drumskin is usually described by the two- dimensional wave equation. In this case, however, there is no time dependence and so u ρ, φ  solves the two-dimensional Laplace equation, subject to the imposed boundary condition.  Referring to  21.31 , since we wish to ﬁnd a solution that is ﬁnite everywhere inside ρ = a, we require C0 = 0 and Dn = 0 for all n > 0. Now the boundary condition at the rim requires  u a, φ  = D0 +  Cnan An cos nφ + Bn sin nφ  =  cid:4  sin φ + 2 sin 2φ .  ∞ cid:4   n=1  Firstly we see that we require D0 = 0 and An = 0 for all n. Furthermore, we must have C1B1a =  cid:4 , C2B2a2 = 2 cid:4  and Bn = 0 for n > 2. Hence the appropriate shape for the drumskin  valid over the whole skin, not just the rim  is  u ρ, φ  =  sin φ +  sin 2φ =  sin φ +  sin 2φ   cid:4 ρ a  2 cid:4 ρ2 a2  2ρ a   cid:8   .  cid:2    cid:7    cid:4 ρ a  727   PDES: SEPARATION OF VARIABLES AND OTHER METHODS  Laplace’s equation in cylindrical polars  Passing to three dimensions, we now consider the solution of Laplace’s equation in cylindrical polar coordinates,   cid:7    cid:8   1 ρ  ∂ ∂ρ  ρ  ∂u ∂ρ  +  1 ρ2  ∂2u ∂φ2 +  ∂2u ∂z2 = 0.   21.32   We note here that, even when considering a cylindrical physical system, if there is no dependence of the physical variables on z  i.e. along the length of the cylinder  then the problem may be treated using two-dimensional plane polars, as discussed above.  For the more general case, however, we proceed as previously by trying a  solution of the form  which, on substitution into  21.32  and division through by u = P ΦZ, gives  u ρ, φ, z  = P  ρ Φ φ Z z ,   cid:7    cid:8   1 P ρ  d dρ  ρ  dP dρ  +  1  Φρ2  d2Φ dφ2 +  1 Z  d2Z dz2 = 0.  The last term depends only on z, and the ﬁrst and second  taken together  depend only on ρ and φ. Taking the separation constant to be k2, we ﬁnd  The ﬁrst of these equations has the straightforward solution  Multiplying the second equation through by ρ2, we obtain  in which the second term depends only on Φ and the other terms depend only on ρ. Taking the second separation constant to be m2, we ﬁnd  The equation in the azimuthal angle φ has the very familiar solution   21.33    21.34    cid:7    cid:8  d2Z dz2 = k2, d2Φ dφ2 + k2 = 0.  Φρ2  +  1  1 Z dP dρ  1 P ρ  d dρ  ρ  Z z  = E exp −kz  + F exp kz.  cid:7    cid:8   ρ P  d dρ  ρ  dP dρ  +  1 Φ  d2Φ dφ2 + k2ρ2 = 0,   cid:7   1 Φ  d2Φ  dφ2 = −m2,  cid:8  +  k2ρ2 − m2 P = 0.  ρ  d dρ  ρ  dP dρ  Φ φ  = C cos mφ + D sin mφ.  728   21.3 SEPARATION OF VARIABLES IN POLAR COORDINATES  As in the two-dimensional case, single-valuedness of u requires that m is an integer. However, in the particular case m = 0 the solution is  Φ φ  = Cφ + D.  This form is appropriate to a solution with axial symmetry  C = 0  or one that is multivalued, but manageably so, such as the magnetic scalar potential associated with a current I  in which case C = I  2π  and D is arbitrary .  Finally, the ρ-equation  21.34  may be transformed into Bessel’s equation of  order m by writing µ = kρ. This has the solution  P  ρ  = AJm kρ  + BYm kρ .  The properties of these functions were investigated in chapter 16 and will not be pursued here. We merely note that Ym kρ  is singular at ρ = 0, and so, when seeking solutions to Laplace’s equation in cylindrical coordinates within some region containing the ρ = 0 axis, we require B = 0.  The complete separated-variable solution in cylindrical polars of Laplace’s  equation ∇2u = 0 is thus given by u ρ, φ, z  = [AJm kρ  + BYm kρ ][C cos mφ + D sin mφ][E exp −kz  + F exp kz].   21.35   Of course we may use the principle of superposition to build up more general solutions by adding together solutions of the form  21.35  for all allowed values of the separation constants k and m.  cid:1 A semi-inﬁnite solid cylinder of radius a has its curved surface held at 0 held at a temperature T0. Find the steady-state temperature distribution in the cylinder.  C and its base  ◦  The physical situation is shown in ﬁgure 21.5. The steady-state temperature distribution u ρ, φ, z  must satisfy Laplace’s equation subject to the imposed boundary conditions. Let us take the cylinder to have its base in the z = 0 plane and to extend along the positive z-axis. From  21.35 , in order that u is ﬁnite everywhere in the cylinder we immediately require B = 0 and F = 0. Furthermore, since the boundary conditions, and hence the temperature distribution, are axially symmetric, we require m = 0, and so the general  solution must be a superposition of solutions of the form J0 kρ  exp −kz  for all allowed  values of the separation constant k.  The boundary condition u a, φ, z  = 0 restricts the allowed values of k, since we must have J0 ka  = 0. The zeros of Bessel functions are given in most books of mathematical tables, and we ﬁnd that, to two decimal places,  J0 x  = 0  for x = 2.40, 5.52, 8.65, . . . .  Writing the allowed values of k as kn for n = 1, 2, 3, . . .  so, for example, k1 = 2.40 a , the required solution takes the form  u ρ, φ, z  =  AnJ0 knρ  exp −knz .  ∞ cid:4   n=1  729   PDES: SEPARATION OF VARIABLES AND OTHER METHODS  z  u = 0  u = 0  a  y  x  u = T0  Figure 21.5 A uniform metal cylinder whose curved surface is kept at 0 and whose base is held at a temperature T0.  ◦  C  By imposing the remaining boundary condition u ρ, φ, 0  = T0, the coeﬃcients An can be found in a similar way to Fourier coeﬃcients but this time by exploiting the orthogonality of the Bessel functions, as discussed in chapter 16. From this boundary condition we require  u ρ, φ, 0  =  AnJ0 knρ  = T0.  If we multiply this expression by ρJ0 krρ  and integrate from ρ = 0 to ρ = a, and use the orthogonality of the Bessel functions J0 knρ , then the coeﬃcients are given by  18.91  as  An =  2T0 1  kna   a2J 2  a  0  J0 knρ ρ dρ.   21.36   The integral on the RHS can be evaluated using the recurrence relation  18.92  of  chapter 16,  ∞ cid:4   n=1   cid:21   which on setting z = knρ yields  d dz  [zJ1 z ] = zJ0 z ,  Therefore the integral in  21.36  is given by   cid:21   a  0  1 kn  d dρ  [knρJ1 knρ ] = knρJ0 knρ .   cid:14   a  0  1 kn   cid:13   1 kn  730  J0 knρ ρ dρ =  ρJ1 knρ   =  aJ1 kna ,   21.3 SEPARATION OF VARIABLES IN POLAR COORDINATES   cid:13    cid:14   and the coeﬃcients An may be expressed as  An =  2T0 1  kna   a2J 2  aJ1 kna   kn  2T0  =  knaJ1 kna   .  The steady-state temperature in the cylinder is then given by  u ρ, φ, z  =  2T0  knaJ1 kna   J0 knρ  exp −knz .  cid:2   ∞ cid:4   n=1  We note that if, in the above example, the base of the cylinder were not kept at a uniform temperature T0, but instead had some ﬁxed temperature distribution T  ρ, φ , then the solution of the problem would become more complicated. In such a case, the required temperature distribution u ρ, φ, z  is in general not axially symmetric, and so the separation constant m is not restricted to be zero but may take any integer value. The solution will then take the form  u ρ, φ, z  =  Jm knmρ  Cnm cos mφ + Dnm sin mφ  exp −knmz ,  where the separation constants knm are such that Jm knma  = 0, i.e. knma is the nth zero of the mth-order Bessel function. At the base of the cylinder we would then require  u ρ, φ, 0  =  Jm knmρ  Cnm cos mφ + Dnm sin mφ  = T  ρ, φ .   21.37   The coeﬃcients Cnm could be found by multiplying  21.37  by Jq krqρ  cos qφ, integrating with respect to ρ and φ over the base of the cylinder and exploiting the orthogonality of the Bessel functions and of the trigonometric functions. The Dnm could be found in a similar way by multiplying  21.37  by Jq krqρ  sin qφ.  ∞ cid:4   ∞ cid:4   m=0  n=1  ∞ cid:4   ∞ cid:4   m=0  n=1  Laplace’s equation in spherical polars  We now come to an equation that is very widely applicable in physical science,   cid:8  namely ∇2u = 0 in spherical polar coordinates:   cid:7    cid:8    cid:7   1 r2  ∂ ∂r  r2 ∂u ∂r  1  +  r2 sin θ  ∂ ∂θ  sin θ  +  ∂u ∂θ  1  r2 sin2 θ  ∂2u ∂φ2 = 0.   21.38   Our method of procedure will be as before; we try a solution of the form  u r, θ, φ  = R r Θ θ Φ φ .   cid:7    cid:8   Substituting this in  21.38 , dividing through by u = RΘΦ and multiplying by r2, we obtain  1 R  d dr  r2 dR dr  1  +  Θ sin θ  d dθ  sin θ  +  dΘ dθ  1  Φ sin2 θ  d2Φ dφ2 = 0.   21.39    cid:7    cid:8   731   PDES: SEPARATION OF VARIABLES AND OTHER METHODS  The ﬁrst term depends only on r and the second and third terms  taken together  depend only on θ and φ. Thus  21.39  is equivalent to the two equations   cid:7    cid:8   1 R  d dr   cid:7   r2 dR dr   cid:8   = λ,  1  Θ sin θ  d dθ  sin θ  +  dΘ dθ  1  Φ sin2 θ  d2Φ  dφ2 = −λ.   21.40    21.41   which can be reduced, by the substitution r = exp t  and writing R r  = S  t  , to  Equation  21.40  is a homogeneous equation,  r2 d2R  dr2 + 2r  dR dr  − λR = 0,  d2S dt2 + This has the straightforward solution  dS dt  − λS = 0.  S  t  = A exp λ1t + B exp λ2t,  and so the solution to the radial equation is  R r  = Arλ1 + Brλ2 ,  where λ1 + λ2 = −1 and λ1λ2 = −λ. We can thus take λ1 and λ2 as given by  cid:2  and −  cid:2  + 1 ; λ then has the form  cid:2   cid:2  + 1 .  It should be noted that at this stage  nothing has been either assumed or proved about whether  cid:2  is an integer.   Hence we have obtained some information about the ﬁrst factor in the  separated-variable solution, which will now have the form  u r, θ, φ  =  Ar cid:2  + Br  Θ θ Φ φ ,   21.42   where Θ and Φ must satisfy  21.41  with λ =  cid:2   cid:2  + 1 .  The next step is to take  21.41  further. Multiplying through by sin2 θ and  substituting for λ, it too takes a separated form:   cid:13    cid:7   sin θ  Θ  d dθ  sin θ  dΘ dθ  +  cid:2   cid:2  + 1  sin2 θ  +  1 Φ  d2Φ dφ2 = 0.   21.43   Taking the separation constant as m2, the equation in the azimuthal angle φ  has the same solution as in cylindrical polars, namely   cid:18    cid:8    cid:19   −  cid:2 +1    cid:14   Φ φ  = C cos mφ + D sin mφ.  As before, single-valuedness of u requires that m is an integer; for m = 0 we again have Φ φ  = Cφ + D.  732   21.3 SEPARATION OF VARIABLES IN POLAR COORDINATES  Having settled the form of Φ φ , we are left only with the equation satisﬁed by  Θ θ , which is  +  cid:2   cid:2  + 1  sin2 θ = m2.   21.44    cid:7    cid:8   sin θ  Θ  d dθ  sin θ  dΘ dθ  A change of independent variable from θ to µ = cos θ will reduce this to a form for which solutions are known, and of which some study has been made in chapter 16. Putting  µ = cos θ,  = − sin θ,  cid:13   cid:14  the equation for M µ  ≡ Θ θ  reads  dµ dθ   cid:13   = − 1 − µ2 1 2 d  ,  dµ  d dθ   cid:14   d dµ   1 − µ2   dM dµ  +   cid:2   cid:2  + 1  − m2 1 − µ2  M = 0.   21.45   This equation is the associated Legendre equation, which was mentioned in sub- section 18.2 in the context of Sturm–Liouville equations.  We recall that for the case m = 0,  21.45  reduces to Legendre’s equation, which  was studied at length in chapter 16, and has the solution  M µ  = EP cid:2  µ  + FQ cid:2  µ .   21.46   We have not solved  21.45  explicitly for general m, but the solutions were given in subsection 18.2 and are the associated Legendre functions P m  cid:2   µ , where   cid:2   µ  and Qm   cid:2   µ  =  1 − µ2   P m  m m P cid:2  µ ,  m 2 d dµ  and similarly for Qm   cid:2   µ . We then have  M µ  = EP m   cid:2   µ  + FQm   cid:2   µ ;  here m must be an integer, 0 ≤ m ≤  cid:2 . We note that if we require solutions to Laplace’s equation that are ﬁnite when µ = cos θ = ±1  i.e. on the polar axis where θ = 0, π , then we must have F = 0 in  21.46  and  21.48  since Qm  cid:2   µ  diverges at µ = ±1. ﬁnite polynomial solutions of Legendre’s equation is that  cid:2  is an integer ≥ 0.  It will be remembered that one of the important conditions for obtaining  This condition therefore applies also to the solutions  21.46  and  21.48  and is reﬂected back into the radial part of the general solution given in  21.42 .  Now that the solutions of each of the three ordinary diﬀerential equations governing R, Θ and Φ have been obtained, we may assemble a complete separated-   21.47    21.48   733   PDES: SEPARATION OF VARIABLES AND OTHER METHODS  variable solution of Laplace’s equation in spherical polars. It is  u r, θ, φ  =  Ar cid:2  + Br  −  cid:2 +1   C cos mφ + D sin mφ [EP m   cid:2   cos θ  + FQm   cid:2   cos θ ],   21.49   where the three bracketted factors are connected only through the integer pa-  rameters  cid:2  and m, 0 ≤ m ≤  cid:2 . As before, a general solution may be obtained  by superposing solutions of this form for the allowed values of the separation constants  cid:2  and m. As mentioned above, if the solution is required to be ﬁnite on the polar axis then F = 0 for all  cid:2  and m.   cid:1 An uncharged conducting sphere of radius a is placed at the origin in an initially uniform electrostatic ﬁeld E. Show that it behaves as an electric dipole.  The uniform ﬁeld, taken in the direction of the polar axis, has an electrostatic potential  u = −Ez = −Er cos θ,  where u is arbitrarily taken as zero at z = 0. This satisﬁes Laplace’s equation ∇2u = 0, as still be −Er cos θ.  must the potential v when the sphere is present; for large r the asymptotic form of v must  Since the problem is clearly axially symmetric, we have immediately that m = 0, and since we require v to be ﬁnite on the polar axis we must have F = 0 in  21.49 . Therefore the solution must be of the form  ∞ cid:4    cid:2 =0  v r, θ, φ  =   A cid:2 r cid:2  + B cid:2 r  −  cid:2 +1  P cid:2  cos θ .  Now the cos θ-dependence of v for large r indicates that the  θ, φ -dependence of v r, θ, φ  is given by P 0  cid:2  = 1 solution, and the most general such solution  outside the sphere, i.e. for r ≥ a  is 1  cos θ  = cos θ. Thus the r-dependence of v must also correspond to an  The asymptotic form of v for large r immediately gives A1 = −E and so yields the solution  v r, θ, φ  =  A1r + B1r  v r, θ, φ  =  −2 P1 cos θ .  cid:8   B1 r2  cos θ.   cid:7   −Er +  cid:7   Since the sphere is conducting, it is an equipotential region and so v must not depend on θ for r = a. This can only be the case if B1 a2 = Ea, thus ﬁxing B1. The ﬁnal solution is therefore   cid:8   v r, θ, φ  = −Er  1 − a3  r3  cos θ.  Since a dipole of moment p gives rise to a potential p  4π cid:4 0r2 , this result shows that the sphere behaves as a dipole of moment 4π cid:4 0a3E, because of the charge distribution induced on its surface; see ﬁgure 21.6.  cid:2   Often the boundary conditions are not so easily met, and it is necessary to use the mutual orthogonality of the associated Legendre functions  and the trigonometric functions  to obtain the coeﬃcients in the general solution.  734   21.3 SEPARATION OF VARIABLES IN POLAR COORDINATES  − − − − − − − − −  −  +  +  +  θ  a  + + + + + +  − −  +  +  +  Figure 21.6 Induced charge and ﬁeld lines associated with a conducting sphere placed in an initially uniform electrostatic ﬁeld.   cid:1 A hollow split conducting sphere of radius a is placed at the origin. If one half of its surface is charged to a potential v0 and the other half is kept at zero potential, ﬁnd the potential v inside and outside the sphere.  Let us choose the top hemisphere to be charged to v0 and the bottom hemisphere to be at zero potential, with the plane in which the two hemispheres meet perpendicular to the polar axis; this is shown in ﬁgure 21.7. The boundary condition then becomes   cid:12   v a, θ, φ  =  v0 0  for 0 < θ < π 2 for π 2 < θ < π   0 < cos θ < 1 ,   −1 < cos θ < 0 .   21.50   The problem is clearly axially symmetric and so we may set m = 0. Also, we require the solution to be ﬁnite on the polar axis and so it cannot contain Q cid:2  cos θ . Therefore the general form of the solution to  21.38  is  v r, θ, φ  =   A cid:2 r cid:2  + B cid:2 r  −  cid:2 +1  P cid:2  cos θ .   21.51   ∞ cid:4    cid:2 =0  Inside the sphere  for r < a  we require the solution to be ﬁnite at the origin and so B cid:2  = 0 for all  cid:2  in  21.51 . Imposing the boundary condition at r = a we must then have  v a, θ, φ  =  A cid:2 a cid:2 P cid:2  cos θ ,  where v a, θ, φ  is also given by  21.50 . Exploiting the mutual orthogonality of the Legendre polynomials, the coeﬃcients in the Legendre polynomial expansion are given by  18.14  as  writing µ = cos θ   ∞ cid:4    cid:2 =0   cid:21   1   cid:21   −1  1  0  735  2 cid:2  + 1  A cid:2 a cid:2  =  v a, θ, φ P cid:2  µ dµ  2  2  2 cid:2  + 1  =  v0  P cid:2  µ dµ,   PDES: SEPARATION OF VARIABLES AND OTHER METHODS  θ  r  z  a  φ  −a  v = v0  x  y  v = 0  ··· ,  cid:14   .   cid:14   Figure 21.7 A hollow split conducting sphere with its top half charged to a potential v0 and its bottom half at zero potential.  where in the last line we have used  21.50 . The integrals of the Legendre polynomials are easily evaluated  see exercise 17.3  and we ﬁnd  A0 =  A1 =  A2 = 0,  v0 2  ,  3v0 4a  ,  A3 = − 7v0 16a3 ,  so that the required solution inside the sphere is  v r, θ, φ  =  1 +  P1 cos θ  − 7r3  8a3 P3 cos θ  + ···  3r 2a   cid:13   v0 2  Outside the sphere  for r > a  we require the solution to be bounded as r tends to inﬁnity and so in  21.51  we must have A cid:2  = 0 for all  cid:2 . In this case, by imposing the boundary condition at r = a we require  where v a, θ, φ  is given by  21.50 . Following the above argument the coeﬃcients in the expansion are given by  ∞ cid:4    cid:2 =0  v a, θ, φ  =  B cid:2 a  2 cid:2  + 1  2  B cid:2 a  −  cid:2 +1  =  cid:13   −  cid:2 +1 P cid:2  cos θ ,  cid:21   v0  P cid:2  µ dµ,  1  0  736  so that the required solution outside the sphere is  v r, θ, φ  =  1 +  v0a 2r  P1 cos θ  − 7a3  8r3 P3 cos θ  + ···  3a 2r  .  cid:2    21.3 SEPARATION OF VARIABLES IN POLAR COORDINATES  In the above example, on the equator of the sphere  i.e. at r = a and θ = π 2   the potential is given by  v a, π 2, φ  = v0 2,  i.e. mid-way between the potentials of the top and bottom hemispheres. This is so because a Legendre polynomial expansion of a function behaves in the same way as a Fourier series expansion, in that it converges to the average of the two values at any discontinuities present in the original function.  If the potential on the surface of the sphere had been given as a function of θ and φ, then we would have had to consider a double series summed over  cid:2  and  m  for − cid:2  ≤ m ≤  cid:2  , since, in general, the solution would not have been axially  symmetric.  Finally, we note in general that, when obtaining solutions of Laplace’s equation in spherical polar coordinates, one ﬁnds that, for solutions that are ﬁnite on the polar axis, the angular part of the solution is given by  Θ θ Φ φ  = P m   cid:2   cos θ  C cos mφ + D sin mφ ,  where  cid:2  and m are integers with − cid:2  ≤ m ≤  cid:2 . This general form is suﬃciently  common that particular functions of θ and φ called spherical harmonics are deﬁned and tabulated  see section 18.3 .  21.3.2 Other equations in polar coordinates  The development of the solutions of ∇2u = 0 carried out in the previous subsection can be employed to solve other equations in which the ∇2 operator appears. Since  we have discussed the general method in some depth already, only an outline of the solutions will be given here.  Let us ﬁrst consider the wave equation  ∇2u =  1 c2  ∂2u ∂t2 ,  and look for a separated solution of the form u = F r T  t , so that initially we are separating only the spatial and time dependences. Substituting this form into  21.52  and taking the separation constant as k2 we obtain  ∇2F + k2F = 0,  d2T dt2 + k2c2T = 0.  The second equation has the simple solution  T  t  = A exp iωt  + B exp −iωt ,  where ω = kc; this may also be expressed in terms of sines and cosines, of course. The ﬁrst equation in  21.53  is referred to as Helmholtz’s equation; we discuss it below.   21.52    21.53    21.54   737   PDES: SEPARATION OF VARIABLES AND OTHER METHODS  We may treat the diﬀusion equation  κ∇2u =  ∂u ∂t  in a similar way. Separating the spatial and time dependences by assuming a solution of the form u = F r T  t , and taking the separation constant as k2, we ﬁnd  ∇2F + k2F = 0,  dT dt  + k2κT = 0.  Just as in the case of the wave equation, the spatial part of the solution satisﬁes Helmholtz’s equation. It only remains to consider the time dependence, which has the simple solution  T  t  = A exp −k2κt .  Helmholtz’s equation is clearly of central importance in the solutions of the wave and diﬀusion equations. It can be solved in polar coordinates in much the same way as Laplace’s equation, and indeed reduces to Laplace’s equation when k = 0. Therefore, we will merely sketch the method of its solution in each of the three polar coordinate systems.  In two-dimensional plane polar coordinates, Helmholtz’s equation takes the form  If we try a separated solution of the form F r  = P  ρ Φ φ , and take the separation constant as m2, we ﬁnd  Helmholtz’s equation in plane polars   cid:7    cid:8   1 ρ  ∂ ∂ρ  ρ  ∂F ∂ρ  +  1 ρ2  ∂2F ∂φ2 + k2F = 0.   cid:7  d2Φ dφ2 + m2φ = 0, k2 − m2  +   cid:8   ρ2  d2P dρ2 +  1 ρ  dP dρ  P = 0.  Φ φ  = A cos mφ + B sin mφ,  As for Laplace’s equation, the angular part has the familiar solution  if m  cid:3 = 0   or an equivalent form in terms of complex exponentials. The radial equation diﬀers from that found in the solution of Laplace’s equation, but by making the substitution µ = kρ it is easily transformed into Bessel’s equation of order m  discussed in chapter 16 , and has the solution  where Ym is a Bessel function of the second kind, which is inﬁnite at the origin  P  ρ  = CJm kρ  + DYm kρ ,  738   21.3 SEPARATION OF VARIABLES IN POLAR COORDINATES  and is not to be confused with a spherical harmonic  these are written with a superscript as well as a subscript .  Putting the two parts of the solution together we have  F ρ, φ  = [A cos mφ + B sin mφ][CJm kρ  + DYm kρ ].   21.55   Clearly, for solutions of Helmholtz’s equation that are required to be ﬁnite at the origin, we must set D = 0.  cid:1 Find the four lowest frequency modes of oscillation of a circular drumskin of radius a whose circumference is held ﬁxed in a plane.  The transverse displacement u r, t  of the drumskin satisﬁes the two-dimensional wave equation  ∇2u =  1 c2  ∂2u ∂t2 ,  with c2 = T  σ, where T is the tension of the drumskin and σ is its mass per unit area. From  21.54  and  21.55  a separated solution of this equation, in plane polar coordinates, that is ﬁnite at the origin is  u ρ, φ, t  = Jm kρ  A cos mφ + B sin mφ  exp ±iωt ,  where ω = kc. Since we require the solution to be single-valued we must have m as an integer. Furthermore, if the drumskin is clamped at its outer edge ρ = a then we also require u a, φ, t  = 0. Thus we need  which in turn restricts the allowed values of k. The zeros of Bessel functions can be obtained from most books of tables; the ﬁrst few are  Jm ka  = 0,  J0 x  = 0 J1 x  = 0 J2 x  = 0  for x ≈ 2.40, 5.52, 8.65, . . . , for x ≈ 3.83, 7.02, 10.17, . . . , for x ≈ 5.14, 8.42, 11.62 . . . .  The smallest value of x for which any of the Bessel functions is zero is x ≈ 2.40, which  occurs for J0 x . Thus the lowest-frequency mode has k = 2.40 a and angular frequency ω = 2.40c a. Since m = 0 for this mode, the shape of the drumskin is  this is illustrated in ﬁgure 21.8.  Continuing in the same way, the next three modes are given by   cid:10   2.40  ;  ρ a   cid:9   cid:10   cid:10   cid:10   ρ a ρ a ρ a  u ∝ J0  cid:9   cid:9   cid:9   5.14  3.83  5.52  .   cid:9   cid:9    cid:10   cid:10   ρ a ρ a  cos φ,  J1  3.83  sin φ;  cos 2φ,  J2  5.14  sin 2φ;  ω = 3.83  ω = 5.14  ω = 5.52  c a c a c a  ,  ,  ,  u ∝ J1 u ∝ J2 u ∝ J0  These modes are also shown in ﬁgure 21.8. We note that the second and third frequencies have two corresponding modes of oscillation; these frequencies are therefore two-fold degenerate.  cid:2   739   PDES: SEPARATION OF VARIABLES AND OTHER METHODS  a  ω = 2.40c a  ω = 3.83c a  ω = 5.14c a  ω = 5.52c a  Figure 21.8 The modes of oscillation with the four lowest frequencies for a circular drumskin of radius a. The dashed lines indicate the nodes, where the displacement of the drumskin is always zero.  Helmholtz’s equation in cylindrical polars  Generalising the above method to three-dimensional cylindrical polars is straight- forward, and following a similar procedure to that used for Laplace’s equation we ﬁnd the separated solution of Helmholtz’s equation takes the form   cid:9 √ k2 − α2 ρ   cid:22  ×  C cos mφ + D sin mφ [E exp iαz  + F exp −iαz ],  k2 − α2 ρ   cid:9 √  F ρ, φ, z  =   cid:10  cid:23   + BYm   cid:10   AJm  where α and m are separation constants. We note that the angular part of the solution is the same as for Laplace’s equation in cylindrical polars.  Helmholtz’s equation in spherical polars  In spherical polars, we ﬁnd again that the angular parts of the solution Θ θ Φ φ  are identical to those of Laplace’s equation in this coordinate system, i.e. they are the spherical harmonics Y m   cid:2   θ, φ , and so we shall not discuss them further.  The radial equation in this case is given by   cid:7  cid:7   r2R  + 2rR   cid:7   + [k2r2 −  cid:2   cid:2  + 1 ]R = 0,   21.56   which has an additional term k2r2R compared with the radial equation for the Laplace solution. The equation  21.56  looks very much like Bessel’s equation. −1 2S  r  and making the change of variable µ = kr, In fact, by writing R r  = r it can be reduced to Bessel’s equation of order  cid:2  + 1 2 , which has as its solutions S  µ  = J cid:2 +1 2 µ  and Y cid:2 +1 2 µ   see section 18.6 . The separated solution to  740   21.3 SEPARATION OF VARIABLES IN POLAR COORDINATES  Helmholtz’s equation in spherical polars is thus  F r, θ, φ  = r  −1 2[AJ cid:2 +1 2 kr  + BY cid:2 +1 2 kr ] C cos mφ + D sin mφ  ×[EP m   cid:2   cos θ  + FQm   cid:2   cos θ ].   21.57   For solutions that are ﬁnite at the origin we require B = 0, and for solutions that are ﬁnite on the polar axis we require F = 0. It is worth mentioning that the solutions proportional to r normalised, are called spherical Bessel functions of the ﬁrst and second kind, respectively, and are denoted by j cid:2  kr  and n cid:2  µ   see section 18.6 .  −1 2Y cid:2 +1 2 kr , when suitably  −1 2J cid:2 +1 2 kr  and r  As mentioned at the beginning of this subsection, the separated solution of the wave equation in spherical polars is the product of a time-dependent part  21.54  and a spatial part  21.57 . It will be noticed that, although this solution corresponds to a solution of deﬁnite frequency ω = kc, the zeros of the radial function j cid:2  kr  are not equally spaced in r, except for the case  cid:2  = 0 involving j0 kr , and so there is no precise wavelength associated with the solution.  To conclude this subsection, let us mention brieﬂy the Schr¨odinger equation for the electron in a hydrogen atom, the nucleus of which is taken at the origin and is assumed massive compared with the electron. Under these circumstances the Schr¨odinger equation is  −  cid:1 2 2m  ∇2u − e2  4π cid:4 0  u r  = i cid:1   ∂u ∂t  .  For a ‘stationary-state’ solution, for which the energy is a constant E and the time-  dependent factor T in u is given by T  t  = A exp −iEt  cid:1  , the above equation is  § However, as with similar to, but not quite the same as, the Helmholtz equation. the wave equation, the angular parts of the solution are identical to those for Laplace’s equation and are expressed in terms of spherical harmonics.  The important point to note is that for any equation involving ∇2, provided θ and φ do not appear in the equation other than as part of ∇2, a separated-variable  solution in spherical polars will always lead to spherical harmonic solutions. This is the case for the Schr¨odinger equation describing an atomic electron in a central potential V  r .  21.3.3 Solution by expansion  It is sometimes possible to use the uniqueness theorem discussed in the previous chapter, together with the results of the last few subsections, in which Laplace’s equation  and other equations  were considered in polar coordinates, to obtain solutions of such equations appropriate to particular physical situations.  §  For the solution by series of the r-equation in this case the reader may consult, for example, L. Schiﬀ, Quantum Mechanics  New York: McGraw-Hill, 1955 , p. 82.  741   PDES: SEPARATION OF VARIABLES AND OTHER METHODS  z  P  θ  r  O  a  y  −a  x  ∞ cid:4    cid:2  cid:4   m=− cid:2    cid:2 =0  Figure 21.9 The polar axis Oz is taken as normal to the plane of the ring of matter and passing through its centre.  We will illustrate the method for Laplace’s equation in spherical polars and ﬁrst  assume that the required solution of ∇2u = 0 can be written as a superposition  in the normal way:  u r, θ, φ  =   Ar cid:2  + Br  −  cid:2 +1  P m   cid:2   cos θ  C cos mφ + D sin mφ .   21.58   Here, all the constants A, B, C, D may depend upon  cid:2  and m, and we have assumed that the required solution is ﬁnite on the polar axis. As usual, boundary conditions of a physical nature will then ﬁx or eliminate some of the constants; for example, u ﬁnite at the origin implies all B = 0, or axial symmetry implies that only m = 0 terms are present.  The essence of the method is then to ﬁnd the remaining constants by determin- ing u at values of r, θ, φ for which it can be evaluated by other means, e.g. by direct calculation on an axis of symmetry. Once the remaining constants have been ﬁxed by these special considerations to have particular values, the uniqueness theorem can be invoked to establish that they must have these values in general.   cid:1 Calculate the gravitational potential at a general point in space due to a uniform ring of matter of radius a and total mass M.  Everywhere except on the ring the potential u r  satisﬁes the Laplace equation, and so if we use polar coordinates with the normal to the ring as polar axis, as in ﬁgure 21.9, a solution of the form  21.58  can be assumed.  We expect the potential u r, θ, φ  to tend to zero as r → ∞, and also to be ﬁnite at r = 0.  At ﬁrst sight this might seem to imply that all A and B, and hence u, must be identically zero, an unacceptable result. In fact, what it means is that diﬀerent expressions must apply  to diﬀerent regions of space. On the ring itself we no longer have ∇2u = 0 and so it is not  742   21.3 SEPARATION OF VARIABLES IN POLAR COORDINATES  surprising that the form of the expression for u changes there. Let us therefore take two separate regions.  In the region r > a   i  we must have u → 0 as r → ∞, implying that all A = 0, and   ii  the system is axially symmetric and so only m = 0 terms appear.  With these restrictions we can write as a trial form  u r, θ, φ  =  −  cid:2 +1 P 0  B cid:2 r   cid:2   cos θ .   21.59   ∞ cid:4    cid:2 =0  The constants B cid:2  are still to be determined; this we do by calculating directly the potential where this can be done simply – in this case, on the polar axis.  Considering a point P on the polar axis at a distance z  > a  from the plane of the ring  taken as θ = π 2 , all parts of the ring are at a distance  z2 + a2 1 2 from it. The potential at P is thus straightforwardly  where G is the gravitational constant. This must be the same as  21.59  for the particular values r = z, θ = 0, and φ undeﬁned. Since P 0  cid:2   cos θ  = P cid:2  cos θ  with P cid:2  1  = 1, putting r = z in  21.59  gives  u z, 0, φ  = −  GM   z2 + a2 1 2 ,  ∞ cid:4   cid:10   cid:9    cid:2 =0  u z, 0, φ  =  B cid:2  z cid:2 +1 .   cid:13    cid:9    cid:10   a z  2  +  3 8  a z  4 − ···   cid:14   ,   21.60    21.61   However, expanding  21.60  for z > a  as it applies to this region of space  we obtain  1 − 1 § which on comparison with  21.61  gives  u z, 0, φ  = − GM  2  z  B0 = −GM, B2 cid:2  = − GMa2 cid:2  −1  cid:2  2 cid:2  − 1 !!  2 cid:2  cid:2 !  B2 cid:2 +1 = 0.  for  cid:2  ≥ 1,   21.62   We now conclude the argument by saying that if a solution for a general point  r, θ, φ  exists at all, which of course we very much expect on physical grounds, then it must be  21.59  with the B cid:2  given by  21.62 . This is so because thus deﬁned it is a function with no arbitrary constants and which satisﬁes all the boundary conditions, and the uniqueness theorem states that there is only one such function. The expression for the potential in the region r > a is therefore   cid:17   P2 cid:2  cos θ   .   cid:9    cid:10   2 cid:2   a r   cid:16   ∞ cid:4    cid:2 =1  u r, θ, φ  = − GM  1 +  r  2 cid:2  cid:2 !   −1  cid:2  2 cid:2  − 1 !! ∞ cid:4   u r, θ, φ  =  A cid:2 r cid:2 P 0   cid:2   cos θ .   cid:2 =0  The expression for r < a can be found in a similar way. The ﬁniteness of u at r = 0 and  the axial symmetry give  §   2 cid:2  − 1 !! = 1 × 3 × ··· ×  2 cid:2  − 1 .  743   PDES: SEPARATION OF VARIABLES AND OTHER METHODS  Comparing this expression for r = z, θ = 0 with the z < a expansion of  21.60 , which is  valid for any z, establishes A2 cid:2 +1 = 0, A0 = −GM a and  −1  cid:2  2 cid:2  − 1 !!  cid:9   A2 cid:2  = − GM  cid:16  ∞ cid:4   so that the ﬁnal expression valid, and convergent, for r < a is thus   cid:10   a2 cid:2 +1  2 cid:2  cid:2 !  ,   −1  cid:2  2 cid:2  − 1 !!  2 cid:2   u r, θ, φ  = − GM  1 +  a   cid:2 =1  2 cid:2  cid:2 !  r a  P2 cid:2  cos θ   .   cid:17   It is easy to check that the solution obtained has the expected physical value for large r and for r = 0 and is continuous at r = a.  cid:2   21.3.4 Separation of variables for inhomogeneous equations  So far our discussion of the method of separation of variables has been limited to the solution of homogeneous equations such as the Laplace equation and the wave equation. The solutions of inhomogeneous PDEs are usually obtained using the Green’s function methods to be discussed below in section 21.5. However, as a ﬁnal illustration of the usefulness of the separation of variables, we now consider its application to the solution of inhomogeneous equations.  Because of the added complexity in dealing with inhomogeneous equations, we  shall restrict our discussion to the solution of Poisson’s equation,  ∇2u = ρ r ,   21.63   in spherical polar coordinates, although the general method can accommodate other coordinate systems and equations. In physical problems the RHS of  21.63  usually contains some multiplicative constant s . If u is the electrostatic potential  in some region of space in which ρ is the density of electric charge then ∇2u = −ρ r   cid:4 0. Alternatively, u might represent the gravitational potential in some region where the matter density is given by ρ, so that ∇2u = 4πGρ r .  We will simplify our discussion by assuming that the required solution u is ﬁnite on the polar axis and also that the system possesses axial symmetry about that axis – in which case ρ does not depend on the azimuthal angle φ. The key to the method is then to assume a separated form for both the solution u and the density term ρ.  From the discussion of Laplace’s equation, for systems with axial symmetry only m = 0 terms appear, and so the angular part of the solution can be expressed in terms of Legendre polynomials P cid:2  cos θ . Since these functions form an orthogonal set let us expand both u and ρ in terms of them:  ∞ cid:4  ∞ cid:4    cid:2 =0   cid:2 =0  744  u =  R cid:2  r P cid:2  cos θ ,  ρ =  F cid:2  r P cid:2  cos θ ,   21.64    21.65    21.3 SEPARATION OF VARIABLES IN POLAR COORDINATES  where the coeﬃcients R cid:2  r  and F cid:2  r  in the Legendre polynomial expansions are functions of r. Since in any particular problem ρ is given, we can ﬁnd the coeﬃcients F cid:2  r  in the expansion in the usual way  see subsection 18.1.2 . It then only remains to ﬁnd the coeﬃcients R cid:2  r  in the expansion of the solution u. Writing ∇2 in spherical polars and substituting  21.64  and  21.65  into  21.63   cid:13  ∞ cid:4   ∞ cid:4    cid:8  cid:14   we obtain   cid:7    cid:8    cid:7   P cid:2  cos θ   r2  d dr  r2 dR cid:2  dr  +  R cid:2   r2 sin θ  d dθ  dP cid:2  cos θ   sin θ  dθ   cid:2 =0  =  F cid:2  r P cid:2  cos θ .   cid:2 =0   21.66   However, if, in equation  21.44  of our discussion of the angular part of the solution to Laplace’s equation, we set m = 0 we conclude that  1  sin θ  d dθ  dP cid:2  cos θ   sin θ  dθ  = − cid:2   cid:2  + 1 P cid:2  cos θ .  Substituting this into  21.66 , we ﬁnd that the LHS is greatly simpliﬁed and we obtain   cid:7    cid:8    cid:13   ∞ cid:4    cid:2 =0   cid:7   1 r2  d dr  r2 dR cid:2  dr  P cid:2  cos θ  =  F cid:2  r P cid:2  cos θ .  ∞ cid:4    cid:2 =0   cid:8    cid:14   This relation is most easily satisﬁed by equating terms on both sides for each value of  cid:2  separately, so that for  cid:2  = 0, 1, 2, . . . we have  1 r2  d dr  r2 dR cid:2  dr  −  cid:2   cid:2  + 1 R cid:2   r2  = F cid:2  r .   21.67   This is an ODE in which F cid:2  r  is given, and it can therefore be solved for R cid:2  r . The solution to Poisson’s equation, u, is then obtained by making the superposition  21.64 .  cid:1 In a certain system, the electric charge density ρ is distributed as follows:   cid:12   ρ =  Ar cos θ 0  for 0 ≤ r < a, for r ≥ a.  Find the electrostatic potential inside and outside the charge distribution, given that both the potential and its radial derivative are continuous everywhere.  r2  −  cid:2   cid:2  + 1 R cid:2   cid:7    cid:8   The electrostatic potential u satisﬁes  For r < a the RHS can be written − A  cid:4 0 rP1 cos θ , and the coeﬃcients in  21.65  are simply F1 r  = − Ar  cid:4 0  and F cid:2  r  = 0 for  cid:2   cid:3 = 1. Therefore we need only calculate R1 r ,  which satisﬁes  21.67  for  cid:2  = 1:  ∇2u =   cid:12 − A  cid:4 0 r cos θ  cid:8   cid:7   0  for 0 ≤ r < a, for r ≥ a.  1 r2  d dr  r2 dR1 dr  − 2R1 r2  = − Ar  .   cid:4 0  745   PDES: SEPARATION OF VARIABLES AND OTHER METHODS  This can be rearranged to give   cid:7  cid:7  1 + 2rR   cid:7  1  r2R  − 2R1 = − Ar3  ,   cid:4 0  where the prime denotes diﬀerentiation with respect to r. The LHS is homogeneous and the equation can be reduced by the substitution r = exp t, and writing R1 r  = S  t , to  ¨S + ˙S − 2S = − A  exp 3t,   cid:4 0   21.68   where the dots indicate diﬀerentiation with respect to t.  This is an inhomogeneous second-order ODE with constant coeﬃcients and can be  straightforwardly solved by the methods of subsection 15.2.1 to give  S  t  = c1 exp t + c2 exp −2t  − A  exp 3t.  10 cid:4 0  Recalling that r = exp t we ﬁnd  R1 r  = c1r + c2r  r3.  −2 − A 10 cid:4 0  cid:8    cid:7   c1r − A  10 cid:4 0  Since we are interested in the region r < a we must have c2 = 0 for the solution to remain ﬁnite. Thus inside the charge distribution the electrostatic potential has the form  u1 r, θ, φ  =  r3  P1 cos θ .   21.69   Outside the charge distribution  for r ≥ a , however, the electrostatic potential obeys Laplace’s equation, ∇2u = 0, and so given the symmetry of the problem and the requirement that u → ∞ as r → ∞ the solution must take the form  u2 r, θ, φ  =  B cid:2  r cid:2 +1 P cid:2  cos θ .   21.70   ∞ cid:4    cid:2 =0  We can now use the boundary conditions at r = a to ﬁx the constants in  21.69  and  21.70 . The requirement of continuity of the potential and its radial derivative at r = a imply that  Clearly B cid:2  = 0 for  cid:2   cid:3 = 1; carrying out the necessary diﬀerentiations and setting r = a in   21.69  and  21.70  we obtain the simultaneous equations  which may be solved to give c1 = Aa2  6 cid:4 0  and B1 = Aa5  15 cid:4 0 . Since P1 cos θ  = cos θ, the electrostatic potentials inside and outside the charge distribution are given, respectively, by  u1 r, θ, φ  =  cos θ,  u2 r, θ, φ  =  Aa5 15 cid:4 0  cos θ  .  cid:2   r2  u1 a, θ, φ  = u2 a, θ, φ ,  ∂u1 ∂r   a, θ, φ  =   a, θ, φ .  ∂u2 ∂r  B1 a3 = a2 , a2 = − 2B1 a3 ,  10 cid:4 0  c1a − A c1 − 3A  cid:8   10 cid:4 0   cid:7   A  cid:4 0  a2r 6  − r3 10  746   21.4 INTEGRAL TRANSFORM METHODS  21.4 Integral transform methods  In the method of separation of variables our aim was to keep the independent variables in a PDE as separate as possible. We now discuss the use of integral transforms in solving PDEs, a method by which one of the independent variables can be eliminated from the diﬀerential coeﬃcients. It will be assumed that the reader is familiar with Laplace and Fourier transforms and their properties, as discussed in chapter 13.  The method consists simply of transforming the PDE into one containing derivatives with respect to a smaller number of variables. Thus, if the original equation has just two independent variables, it may be possible to reduce the PDE into a soluble ODE. The solution obtained can then  where possible  be transformed back to give the solution of the original PDE. As we shall see, boundary conditions can usually be incorporated in a natural way.  Which sort of transform to use, and the choice of the variable s  with respect to which the transform is to be taken, is a matter of experience; we illustrate this in the example below. In practice, transforms can be taken with respect to each variable in turn, and the transformation that aﬀords the greatest simpliﬁcation can be pursued further.  cid:1 A semi-inﬁnite tube of constant cross-section contains initially pure water. At time t = 0, one end of the tube is put into contact with a salt solution and maintained at a concentration u0. Find the total amount of salt that has diﬀused into the tube after time t, if the diﬀusion constant is κ.  The concentration u x, t  at time t and distance x from the end of the tube satisﬁes the diﬀusion equation  κ  ∂2u ∂x2  =  ∂u ∂t  ,   21.71   which has to be solved subject to the boundary conditions u 0, t  = u0 for all t and u x, 0  = 0 for all x > 0.  Since we are interested only in t > 0, the use of the Laplace transform is suggested. Furthermore, it will be recalled from chapter 13 that one of the major virtues of Laplace transformations is the possibility they aﬀord of replacing derivatives of functions by simple multiplication by a scalar. If the derivative with respect to time were so removed, equation  21.71  would contain only diﬀerentiation with respect to a single variable. Let us therefore take the Laplace transform of  21.71  with respect to t:   cid:21  ∞  exp −st  dt =  κ  ∂2u ∂x2  ∂u ∂t  0  exp −st  dt.   cid:21  ∞  0  On the LHS the  double  diﬀerentiation is with respect to x, whereas the integration is with respect to the independent variable t. Therefore the derivative can be taken outside the integral. Denoting the Laplace transform of u x, t  by ¯u x, s  and using result  13.57  to rewrite the transform of the derivative on the RHS  or by integrating directly by parts , we obtain  But from the boundary condition u x, 0  = 0 the last term on the RHS vanishes, and the  = s¯u x, s  − u x, 0 .  κ  ∂2¯u ∂x2  747   PDES: SEPARATION OF VARIABLES AND OTHER METHODS  solution is immediate:   cid:7     cid:8   s κ   cid:7      −   cid:8   s κ  x  ,  ¯u x, s  = A exp  x  + B exp  where the constants A and B may depend on s.  We require u x, t  → 0 as x → ∞ and so we must also have ¯u ∞, s  = 0; consequently  we require that A = 0. The value of B is determined by the need for u 0, t  = u0 and hence that  We thus conclude that the appropriate expression for the Laplace transform of u x, t  is  ¯u x, s  =  exp   21.72   To obtain u x, t  from this result requires the inversion of this transform – a task that is generally diﬃcult and requires a contour integration. This is discussed in chapter 24, but for completeness we note that the solution is  where erf x  is the error function discussed in the Appendix.  The more complete sets of mathematical tables list this inverse Laplace transform.   In the present problem, however, an alternative method is available. Let w t  be the  amount of salt that has diﬀused into the tube in time t; then  ¯u 0, s  =   cid:21  ∞  0  u0 s   cid:13   u0 exp −st  dt =  cid:8       cid:7   u0 s  .  s κ  x  .  −   cid:7    cid:8  cid:14   ,  u x, t  = u0  1 − erf  x√ 4κt   cid:21  ∞  0  w t  =  u x, t  dx,  ¯w s  =   cid:21  ∞  cid:21  ∞  cid:21  ∞  0  0  0  =  =   cid:21  ∞  u x, t  dx  0  u x, t  exp −st  dt   cid:21  ∞ dt exp −st   dx  0  ¯u x, s  dx.  ¯w s  = u0κ1 2s  −3 2.  Substituting for ¯u x, s  from  21.72  into the last integral and integrating, we obtain  This expression is much simpler to invert, and referring to the table of standard Laplace transforms  table 13.1  we ﬁnd  which is thus the required expression for the amount of diﬀused salt at time t.  cid:2   w t  = 2 κ π 1 2u0t1 2,  The above example shows that in some circumstances the use of a Laplace transformation can greatly simplify the solution of a PDE. However, it will have been observed that  as with ODEs  the easy elimination of some derivatives is usually paid for by the introduction of a diﬃcult inverse transformation. This problem, although still present, is less severe for Fourier transformations.  748  and its transform is given by   21.4 INTEGRAL TRANSFORM METHODS   cid:1 An inﬁnite metal bar has an initial temperature distribution f x  along its length. Find the temperature distribution at a later time t.  We are interested in values of x from −∞ to ∞, which suggests Fourier transformation with respect to x. Assuming that the solution obeys the boundary conditions u x, t  → 0 and ∂u ∂x → 0 as x → ∞, we may Fourier-transform the one-dimensional diﬀusion  equation  21.71  to obtain   cid:21  ∞  κ√ 2π  ∂2u x, t   exp −ikx  dx =  1√ 2π  ∂ ∂t  −∞  integral. Denoting the Fourier transform of u x, t  by3u k, t , and using equation  13.28  to  where on the RHS we have taken the partial derivative with respect to t outside the  ∂x2  rewrite the Fourier transform of the second derivative on the LHS, we then have   cid:21  ∞ −∞ u x, t  exp −ikx  dx,  .  ∂t  ∂3u k, t  −κk23u k, t  = 3u k, t  =3u k, 0  exp −κk2t ,  cid:21  ∞  cid:21  ∞ −∞ u x, 0  exp −ikx  dx −∞ f x  exp −ikx  dx =3 3 f k 3 −1 exp −κk2t . Since 3u k, t  can be  √ f k  exp −κk2t  = √ 2π   1√ 2π 1√ 2π  G k, t ,   21.73   f k .  3u k, 0  =  =  3u k, t  =3  This ﬁrst-order equation has the simple solution  where the initial conditions give  written as the product of two Fourier transforms, we can use the convolution theorem, subsection 13.1.7, to write the solution as  ,   cid:7    cid:7    cid:7   2π    dx  , t f x  u x, t  =  G k, t  =    Thus we may write the Fourier transform of the solution as  where we have deﬁned the function 3  cid:21  ∞ −∞ G x − x is the inverse Fourier transform of 3  cid:21  ∞  cid:13   cid:7   cid:8  cid:14   cid:21  ∞ −∞ exp −κk2t  exp ikx  dk −κt k2 − ix −∞ exp  cid:16   cid:8  cid:21  ∞  cid:7   cid:8  cid:21  ∞  cid:9   cid:8   Completing the square in the integrand we ﬁnd  G k, t  and is thus given by  −κt −κtk  1 2π 1 2π   cid:7   cid:7   −∞ exp  −∞ exp  G x, t  =  G x, t  =  1 2π  exp  exp  κt  =  =   cid:7 2  k  dk  k − ix  cid:10   2κt  cid:7   − x2 4κt  cid:7  − x2 4κt − x2 4κt  1 2π 1√ 4πκt  =  exp  ,  where in the second line we have made the substitution k  dk.   cid:17    cid:8   2  dk  where G x, t  is the Green’s function for this problem  see subsection 15.2.5 . This function   cid:7   = k − ix  2κt , and in the last  749   PDES: SEPARATION OF VARIABLES AND OTHER METHODS  u  t1  t2  t3  x = a  x  Figure 21.10 Diﬀusion of heat from a point source in a metal bar: the curves show the temperature u at position x for various times t1 < t2 < t3. The area under the curves remains constant, since the total heat energy is conserved.  line we have used the standard result for the integral of a Gaussian, given in subsection 6.4.2.  Strictly speaking the change of variable from k to k shifts the path of integration oﬀ the real axis, since k is complex for real k, and so results in a complex integral, as will be discussed in chapter 24. Nevertheless, in this case the path of integration can be shifted back to the real axis without aﬀecting the value of the integral.    cid:7    cid:7   Thus the temperature in the bar at a later time t is given by  cid:7    2   cid:7   −  x − x  u x, t  =   cid:7   f x    dx  ,  −∞ exp  4κt  1√ 4πκt   cid:21  ∞   cid:13    cid:14   which may be evaluated  numerically if necessary  when the form of f x  is given.  cid:2    21.74   As we might expect from our discussion of Green’s functions in chapter 15,  we see from  21.74  that, if the initial temperature distribution is f x  = δ x − a ,  i.e. a ‘point’ source at x = a, then the temperature distribution at later times is simply given by  u x, t  = G x − a, t  =  1√ 4πκt  exp   cid:13  −  x − a 2   cid:14   .  4κt  The temperature at several later times is illustrated in ﬁgure 21.10, which shows √ that the heat diﬀuses out from its initial position; the width of the Gaussian t, a dependence on time which is characteristic of diﬀusion processes. increases as The reader may have noticed that in both examples using integral transforms the solutions have been obtained in closed form – albeit in one case in the form of an integral. This diﬀers from the inﬁnite series solutions usually obtained via the separation of variables. It should be noted that this behaviour is a result of  750   21.5 INHOMOGENEOUS PROBLEMS – GREEN’S FUNCTIONS  the inﬁnite range in x, rather than of the transform method itself. In fact the method of separation of variables would yield the same solutions, since in the inﬁnite-range case the separation constant is not restricted to take on an inﬁnite set of discrete values but may have any real value, with the result that the sum over λ becomes an integral, as mentioned at the end of section 21.2.  cid:1 An inﬁnite metal bar has an initial temperature distribution f x  along its length. Find the temperature distribution at a later time t using the method of separation of variables.  This is the same problem as in the previous example, but we now seek a solution by separating variables. From  21.12  a separated solution for the one-dimensional diﬀusion equation is given by  u x, t  = [A exp iλx  + B exp −iλx ] exp −κλ2t ,  where −λ2 is the separation constant. Since the bar is inﬁnite we do not require the  solution to take a given form at any ﬁnite value of x  for instance at x = 0  and so there is no restriction on λ other than its being real. Therefore instead of the superposition of such solutions in the form of a sum over allowed values of λ we have an integral over all λ,   cid:21  ∞ −∞ A λ  exp −κλ2t  exp iλx  dλ,  1√ 2π  where in taking λ from −∞ to ∞ we need include only one of the complex exponentials;  u x, t  = √ 2π out of A λ  for convenience. We can see from  21.75  we have taken a factor 1  that the expression for u x, t  has the form of an inverse Fourier transform  where λ is the transform variable . Therefore, Fourier-transforming both sides and using the Fourier inversion theorem, we ﬁnd   21.75   3u λ, t  = A λ  exp −κλ2t .  Now, the initial boundary condition requires  from which, using the Fourier inversion theorem once more, we see that A λ  = 3  −∞ A λ  exp iλx  dλ = f x ,  u x, 0  =  f λ .  Therefore we have  f λ  exp −κλ2t ,  which is identical to  21.73  in the previous example  but with k replaced by λ , and hence leads to the same result.  cid:2    cid:21  ∞  1√ 2π  3u λ, t  =3  21.5 Inhomogeneous problems – Green’s functions  In chapters 15 and 17 we encountered Green’s functions and found them a useful tool for solving inhomogeneous linear ODEs. We now discuss their usefulness in solving inhomogeneous linear PDEs.  For the sake of brevity we shall again denote a linear PDE by   21.76  where L is a linear partial diﬀerential operator. For example, in Laplace’s equation  Lu r  = ρ r ,  751   PDES: SEPARATION OF VARIABLES AND OTHER METHODS  we have L = ∇2, whereas for Helmholtz’s equation L = ∇2 +k2. Note that we have  not speciﬁed the dimensionality of the problem, and  21.76  may, for example, represent Poisson’s equation in two or three  or more  dimensions. The reader will also notice that for the sake of simplicity we have not included any time dependence in  21.76 . Nevertheless, the following discussion can be generalised to include it.  As we discussed in subsection 20.3.2, a problem is inhomogeneous if the fact that u r  is a solution does not imply that any constant multiple λu r  is also a solution. This inhomogeneity may derive from either the PDE itself or from the boundary conditions imposed on the solution.  In our discussion of Green’s function solutions of inhomogeneous ODEs  see subsection 15.2.5  we dealt with inhomogeneous boundary conditions by making a suitable change of variable such that in the new variable the boundary conditions were homogeneous. In an analogous way, as illustrated in the ﬁnal example of section 21.2, it is usually possible to make a change of variables in PDEs to transform between inhomogeneity of the boundary conditions and inhomogeneity of the equation. Therefore let us assume for the moment that the boundary conditions imposed on the solution u r  of  21.76  are homogeneous. This most commonly means that if we seek a solution to  21.76  in some region V then on the surface S that bounds V the solution obeys the conditions u r  = 0 or ∂u ∂n = 0, where ∂u ∂n is the normal derivative of u at the surface S .  We shall discuss the extension of the Green’s function method to the direct so- lution of problems with inhomogeneous boundary conditions in subsection 21.5.2, but we ﬁrst highlight how the Green’s function approach to solving ODEs can be simply extended to PDEs for homogeneous boundary conditions.  21.5.1 Similarities to Green’s functions for ODEs  As in the discussion of ODEs in chapter 15, we may consider the Green’s function for a system described by a PDE as the response of the system to a ‘unit impulse’ or ‘point source’. Thus if we seek a solution to  21.76  that satisﬁes some homogeneous boundary conditions on u r  then the Green’s function G r, r0  for the problem is a solution of  LG r, r0  = δ r − r0 ,   21.77   where r0 lies in V . The Green’s function G r, r0  must also satisfy the imposed  homogeneous  boundary conditions. It is understood that in  21.77  the L operator expresses diﬀerentiation with respect to r as opposed to r0. Also, δ r − r0  is the Dirac delta function  see  chapter 13  of dimension appropriate to the problem; it may be thought of as representing a unit-strength point source at r = r0.  Following an analogous argument to that given in subsection 15.2.5 for ODEs,  752   21.5 INHOMOGENEOUS PROBLEMS – GREEN’S FUNCTIONS  if the boundary conditions on u r  are homogeneous then a solution to  21.76  that satisﬁes the imposed boundary conditions is given by   cid:21   u r  =  G r, r0 ρ r0  dV  r0 ,   21.78   where the integral on r0 is over some appropriate ‘volume’. In two or more dimensions, however, the task of ﬁnding directly a solution to  21.77  that satisﬁes the imposed boundary conditions on S can be a diﬃcult one, and we return to this in the next subsection.  An alternative approach is to follow a similar argument to that presented in chapter 17 for ODEs and so to construct the Green’s function for  21.76  as a superposition of eigenfunctions of the operator L, provided L is Hermitian. By Hermitian if it satisﬁes cid:21  analogy with an ordinary diﬀerential operator, a partial diﬀerential operator is   cid:13  cid:21    cid:14 ∗  ∗   r Lw r  dV =  v  ∗   r Lv r  dV  w  ,  V  V  where the asterisk denotes complex conjugation and v and w are arbitrary func- tions obeying the imposed  homogeneous  boundary condition on the solution of Lu r  = 0. The eigenfunctions un r , n = 0, 1, 2, . . . , of L satisfy  where λn are the corresponding eigenvalues, which are all real for an Hermitian operator L. Furthermore, each eigenfunction must obey any imposed  homo- geneous  boundary conditions. Using an argument analogous to that given in chapter 17, the Green’s function for the problem is given by  G r, r0  =   21.79   From  21.79  we see immediately that the Green’s function  irrespective of how  it is found  enjoys the property  Lun r  = λnun r ,  ∞ cid:4   n=0  ∗ n r0   .  un r u λn  ∗ G r, r0  = G   r0, r .  Thus, if the Green’s function is real then it is symmetric in its two arguments.  Once the Green’s function has been obtained, the solution to  21.76  is again given by  21.78 . For PDEs this approach can become very cumbersome, however, and so we shall not pursue it further here.  21.5.2 General boundary-value problems  As mentioned above, often inhomogeneous boundary conditions can be dealt with by making an appropriate change of variables, such that the boundary  753   PDES: SEPARATION OF VARIABLES AND OTHER METHODS  conditions in the new variables are homogeneous although the equation itself is generally inhomogeneous. In this section, however, we extend the use of Green’s functions to problems with inhomogeneous boundary conditions  and equations . This provides a more consistent and intuitive approach to the solution of such boundary-value problems.  For deﬁniteness we shall consider Poisson’s equation  ∇2u r  = ρ r ,   21.80   but the material of this section may be extended to other linear PDEs of the form  21.76 . Clearly, Poisson’s equation reduces to Laplace’s equation for ρ r  = 0 and so our discussion is equally applicable to this case.  We wish to solve  21.80  in some region V bounded by a surface S , which may consist of several disconnected parts. As stated above, we shall allow the possibility that the boundary conditions on the solution u r  may be inhomogeneous on S , although as we shall see this method reduces to those discussed above in the special case that the boundary conditions are in fact homogeneous.  The two common types of inhomogeneous boundary condition for Poisson’s  equation are  as discussed in subsection 20.6.2 :   i  Dirichlet conditions, in which u r  is speciﬁed on S , and  ii  Neumann conditions, in which ∂u ∂n is speciﬁed on S .  In general, specifying both Dirichlet and Neumann conditions on S overdetermines the problem and leads to there being no solution.  The speciﬁcation of the surface S requires some further comment, since S may have several disconnected parts. If we wish to solve Poisson’s equation inside some closed surface S then the situation is straightforward and is shown in ﬁgure 21.11 a . If, however, we wish to solve Poisson’s equation in the gap between two closed surfaces  for example in the gap between two concentric conducting cylinders  then the volume V is bounded by a surface S that has two disconnected parts S1 and S2, as shown in ﬁgure 21.11 b ; the direction of the normal to the surface is always taken as pointing out of the volume V . A similar situation arises when we wish to solve Poisson’s equation outside some closed surface S1. In this case the volume V is inﬁnite but is treated formally by taking the surface S2 as a large sphere of radius R and letting R tend to inﬁnity.  In order to solve  21.80  subject to either Dirichlet or Neumann boundary conditions on S , we will remind ourselves of Green’s second theorem, equation  11.20 , which states that, for two scalar functions φ r  and ψ r  deﬁned in some volume V bounded by a surface S ,   φ∇2ψ − ψ∇2φ  dV =   φ∇ψ − ψ∇φ  · ˆn dS ,   21.81    cid:21   V   cid:21   S  754   21.5 INHOMOGENEOUS PROBLEMS – GREEN’S FUNCTIONS  S  ˆn  V   a   V  ˆn  S1   b   ˆn  S2  Figure 21.11 Surfaces used for solving Poisson’s equation in diﬀerent regions V .  where on the RHS it is common to write, for example, ∇ψ · ˆn dS as  ∂ψ ∂n  dS . The expression ∂ψ ∂n stands for ∇ψ · ˆn, the rate of change of ψ in the direction  of the unit outward normal ˆn to the surface S .  The Green’s function for Poisson’s equation  21.80  must satisfy  ∇2G r, r0  = δ r − r0 ,   21.82   where r0 lies in V .  As mentioned above, we may think of G r, r0  as the solution to Poisson’s equation for a unit-strength point source located at r = r0.  Let us for the moment impose no boundary conditions on G r, r0 .  If we now let φ = u r  and ψ = G r, r0  in Green’s theorem  21.81  then we  obtain  cid:21    cid:18    cid:19  u r ∇2G r, r0  − G r, r0  ∇2u r    cid:21    cid:13   dV  r   ∂G r, r0   =  u r   S  ∂n  − G r, r0   ∂u r   ∂n  dS  r ,  where we have made explicit that the volume and surface integrals are with respect to r. Using  21.80  and  21.82  the LHS can be simpliﬁed to give  [u r δ r − r0  − G r, r0 ρ r ] dV  r    cid:21    cid:13   ∂G r, r0   =  u r   S  ∂n  − G r, r0   ∂u r   ∂n  dS  r .   21.83    cid:14    cid:14   V   cid:21   V  Since r0 lies within the volume V ,  u r δ r − r0  dV  r  = u r0 ,   cid:21   V  and thus on rearranging  21.83  the solution to Poisson’s equation  21.80  can be  755   PDES: SEPARATION OF VARIABLES AND OTHER METHODS   cid:14   written as   cid:21   V   cid:21    cid:13   S  u r0  =  G r, r0 ρ r  dV  r  +  u r   ∂G r, r0   ∂n  − G r, r0   ∂u r   ∂n  dS  r .   21.84   Clearly, we can interchange the roles of r and r0 in  21.84  if we wish.  Remember also that, for a real Green’s function, G r, r0  = G r0, r .   Equation  21.84  is central to the extension of the Green’s function method to problems with inhomogeneous boundary conditions, and we next discuss its application to both Dirichlet and Neumann boundary-value problems. But, before doing so, we also note that if the boundary condition on S is in fact homogeneous, so that u r  = 0 or ∂u r  ∂n = 0 on S , then demanding that the Green’s function G r, r0  also obeys the same boundary condition causes the surface integral in  21.84  to vanish, and we are left with the familiar form of solution given in  21.78 . The extension of  21.84  to a PDE other than Poisson’s equation is discussed in exercise 21.28.  21.5.3 Dirichlet problems  In a Dirichlet problem we require the solution u r  of Poisson’s equation  21.80  to take speciﬁc values on some surface S that bounds V , i.e. we require that u r  = f r  on S where f is a given function.  If we seek a Green’s function G r, r0  for this problem it must clearly satisfy  21.82 , but we are free to choose the boundary conditions satisﬁed by G r, r0  in such a way as to make the solution  21.84  as simple as possible. From  21.84 , we see that by choosing  G r, r0  = 0  for r on S   21.85   the second term in the surface integral vanishes. Since u r  = f r  on S ,  21.84  then becomes  u r0  =  G r, r0 ρ r  dV  r  +  f r   dS  r .   21.86    cid:21   S  ∂G r, r0   ∂n   cid:21   V  Thus we wish to ﬁnd the Dirichlet Green’s function that   i  satisﬁes  21.82  and hence is singular at r = r0, and  ii  obeys the boundary condition G r, r0  = 0 for r on S .  In general, it is diﬃcult to obtain this function directly, and so it is useful to separate these two requirements. We therefore look for a solution of the form  G r, r0  = F r, r0  + H r, r0 ,  where F r, r0  satisﬁes  21.82  and has the required singular character at r = r0 but does not necessarily obey the boundary condition on S , whilst H r, r0  satisﬁes  756   21.5 INHOMOGENEOUS PROBLEMS – GREEN’S FUNCTIONS  the corresponding homogeneous equation  i.e. Laplace’s equation  inside V but is adjusted in such a way that the sum G r, r0  equals zero on S . The Green’s function G r, r0  is still a solution of  21.82  since  ∇2G r, r0  = ∇2F r, r0  + ∇2H r, r0  = ∇2F r, r0  + 0 = δ r − r0 .  The function F r, r0  is called the fundamental solution and will clearly take diﬀerent forms depending on the dimensionality of the problem. Let us ﬁrst consider the fundamental solution to  21.82  in three dimensions.  cid:1 Find the fundamental solution to Poisson’s equation in three dimensions that tends to zero as r → ∞.  We wish to solve  ∇2F r, r0  = δ r − r0   in three dimensions, subject to the boundary condition F r, r0  → 0 as r → ∞. Since the  problem is spherically symmetric about r0, let us consider a large sphere S of radius R centred on r0, and integrate  21.87  over the enclosed volume V . We then obtain  since V encloses the point r0. However, using the divergence theorem,   cid:21   cid:21   V  ∇2F r, r0  dV =  δ r − r0  dV = 1,  ∇2F r, r0  dV =  ∇F r, r0  · ˆn dS ,  V  S  where ˆn is the unit normal to the large sphere S at any point.  Since the problem is spherically symmetric about r0, we expect that  F r, r0  = F r − r0  = F r ,  i.e. that F has the same value everywhere on S . Thus, evaluating the surface integral in §  21.89  and equating it to unity from  21.88 , we have  Integrating this expression we obtain  but, since we require F r, r0  → 0 as r → ∞, the constant must be zero. The fundamental  solution in three dimensions is consequently given by  4πr2 dF dr  = 1.  r=R  F r  = − 1  4πr  + constant,  F r, r0  = −  1  4πr − r0 .  This is clearly also the full Green’s function for Poisson’s equation subject to the boundary  condition u r  → 0 as r → ∞.  cid:2   Using  21.90  we can write down the solution of Poisson’s equation to ﬁnd,  §  A vertical bar to the right of an expression is a common alternative to enclosing the expression in square brackets; as usual, the subscript shows the value of the variable at which the expression is to be evaluated.   21.87    21.88    21.89    21.90    cid:21   cid:21   V   cid:20  cid:20  cid:20  cid:20   757   PDES: SEPARATION OF VARIABLES AND OTHER METHODS  for example, the electrostatic potential u r  due to some distribution of electric charge ρ r . The electrostatic potential satisﬁes  ∇2u r  = − ρ  ,   cid:4 0  where u r  → 0 as r → ∞. Since the boundary condition on the surface at  inﬁnity is homogeneous the surface integral in  21.86  vanishes, and using  21.90  we recover the familiar solution   cid:21   u r0  =  ρ r   4π cid:4 0r − r0 dV  r ,  where the volume integral is over all space.  We can develop an analogous theory in two dimensions. As before the funda-  mental solution satisﬁes  ∇2F r, r0  = δ r − r0 ,  where δ r−r0  is now the two-dimensional delta function. Following an analogous  method to that used in the previous example, we ﬁnd the fundamental solution in two dimensions to be given by   21.91    21.92    21.93   F r, r0  =  1 2π  lnr − r0 + constant.  From the form of the solution we see that in two dimensions we cannot apply  the condition F r, r0  → 0 as r → ∞, and in this case the constant does not  necessarily vanish.  We now return to the task of constructing the full Dirichlet Green’s function. To do so we wish to add to the fundamental solution a solution of the homogeneous equation  in this case Laplace’s equation  such that G r, r0  = 0 on S , as required by  21.86  and its attendant conditions. The appropriate Green’s function is constructed by adding to the fundamental solution ‘copies’ of itself that represent ‘image’ sources at diﬀerent locations outside V . Hence this approach is called the method of images.  In summary, if we wish to solve Poisson’s equation in some region V subject to Dirichlet boundary conditions on its surface S then the procedure and argument are as follows.   i  To the single source δ r − r0  inside V add image sources outside V  N cid:4   n=1  qnδ r − rn  with rn outside V ,  where the positions rn and the strengths qn of the image sources are to be determined as described in step  iii  below.  758   21.5 INHOMOGENEOUS PROBLEMS – GREEN’S FUNCTIONS   ii  Since all the image sources lie outside V , the fundamental solution cor- responding to each source satisﬁes Laplace’s equation inside V . Thus we may add the fundamental solutions F r, rn  corresponding to each image source to that corresponding to the single source inside V , obtaining the Green’s function  N cid:4   n=1  G r, r0  = F r, r0  +  qnF r, rn .   iii  Now adjust the positions rn and strengths qn of the image sources so that the required boundary conditions are satisﬁed on S . For a Dirichlet Green’s function we require G r, r0  = 0 for r on S .   iv  The solution to Poisson’s equation subject to the Dirichlet boundary  condition u r  = f r  on S is then given by  21.86 .  In general it is very diﬃcult to ﬁnd the correct positions and strengths for the images, i.e. to make them such that the boundary conditions on S are satisﬁed. Nevertheless, it is possible to do so for certain problems that have simple geometry. In particular, for problems in which the boundary S consists of straight lines  in two dimensions  or planes  in three dimensions , positions of the image points can be deduced simply by imagining the boundary lines or planes to be mirrors in which the single source in V  at r0  is reﬂected.  cid:1 Solve Laplace’s equation ∇2u = 0 in three dimensions in the half-space z > 0, given that  u r  = f r  on the plane z = 0.  The surface S bounding V consists of the xy-plane and the surface at inﬁnity. Therefore, the Dirichlet Green’s function for this problem must satisfy G r, r0  = 0 on z = 0 and  G r, r0  → 0 as r → ∞. Thus it is clear in this case that we require one image source at a  position r1 that is the reﬂection of r0 in the plane z = 0, as shown in ﬁgure 21.12  so that r1 lies in z < 0, outside the region in which we wish to obtain a solution . It is also clear  that the strength of this image should be −1.  Therefore by adding the fundamental solutions corresponding to the original source  and its image we obtain the Green’s function  G r, r0  = −  1  4πr − r0 +  1  4πr − r1 ,  where r1 is the reﬂection of r0 in the plane z = 0, i.e. if r0 =  x0, y0, z0  then r1 =  x0, y0,−z0 . Clearly G r, r0  → 0 as r → ∞ as required. Also G r, r0  = 0 on z = 0, and so  21.94  is  the desired Dirichlet Green’s function.  The solution to Laplace’s equation is then given by  21.86  with ρ r  = 0,   cid:21   S  u r0  =  f r   ∂G r, r0   ∂n  dS  r .  Clearly the surface at inﬁnity makes no contribution to this integral. The outward-pointing  unit vector normal to the xy-plane is simply ˆn = −k  where k is the unit vector in the  z-direction , and so   21.94    21.95   ∂G r, r0   ∂n  = − ∂G r, r0   = −k · ∇G r, r0 .  ∂z  759   PDES: SEPARATION OF VARIABLES AND OTHER METHODS  z  V  +  r0  y  x  −  r1  Figure 21.12 The arrangement of images for solving Laplace’s equation in the half-space z > 0.  We may evaluate this normal derivative by writing the Green’s function  21.94  explicitly in terms of x, y and z  and x0, y0 and z0  and calculating the partial derivative with respect § to z directly. It is usually quicker, however, to use the fact that   21.96   thus  Since r0 =  x0, y0, z0  and r1 =  x0, y0,−z0  the normal derivative is given by  ∇r − r0 =  ∇G r, r0  =  r − r0 4πr − r03  r − r0 r − r0 ; − r − r1 4πr − r13 .  − ∂G r, r0   ∂z  = −k · ∇G r, r0  = − z − z0 4πr − r03  +  z + z0  4πr − r13 .  − ∂G r, r0   ∂z   cid:20  cid:20  cid:20  cid:20   cid:21  ∞  z=0  Therefore on the surface z = 0, and writing out the dependence on x, y and z explicitly, we have  2z0  =  4π[ x − x0 2 +  y − y0 2 + z2  cid:21  ∞  0 ]3 2 .  Inserting this expression into  21.95  we obtain the solution  u x0, y0, z0  =  z0 2π  −∞  −∞  [ x − x0 2 +  y − y0 2 + z2  f x, y   0 ]3 2 dx dy.  cid:2   An analogous procedure may be applied in two-dimensional problems. For  §  Since r − r02 =  r − r0  ·  r − r0  we have ∇r − r02 = 2 r − r0 , from which we obtain  ∇ r − r02 1 2 =  2 r − r0   r − r02 1 2  1 2  r − r0 r − r0 .  =  Note that this result holds in two and three dimensions.  760   21.5 INHOMOGENEOUS PROBLEMS – GREEN’S FUNCTIONS  example, in solving Poisson’s equation in two dimensions in the half-space x > 0  we again require just one image charge, of strength q1 = −1, at a position r1 that  is the reﬂection of r0 in the line x = 0. Since we require G r, r0  = 0 when r lies on x = 0, the constant in  21.93  must equal zero, and so the Dirichlet Green’s function is   cid:5  lnr − r0 − lnr − r1 cid:6   .  Clearly G r, r0  tends to zero as r → ∞. If, however, we wish to solve the two-  dimensional Poisson equation in the quarter space x > 0, y > 0, then more image points are required.  G r, r0  =  1 2π   cid:1 A line charge in the z-direction of charge density λ is placed at some position r0 in the quarter-space x > 0, y > 0. Calculate the force per unit length on the line charge due to the presence of thin earthed plates along x = 0 and y = 0.  Here we wish to solve Poisson’s equation,  ∇2u = − λ  δ r − r0 ,   cid:4 0  in the quarter space x > 0, y > 0. It is clear that we require three image line charges with positions and strengths as shown in ﬁgure 21.13  all of which lie outside the region in which we seek a solution . The boundary condition that the electrostatic potential u is zero on x = 0 and y = 0  shown as the ‘curve’ C in ﬁgure 21.13  is then automatically satisﬁed, and so this system of image charges is directly equivalent to the original situation of a single line charge in the presence of the earthed plates along x = 0 and y = 0. Thus the electrostatic potential is simply equal to the Dirichlet Green’s function   cid:5   lnr − r0 − lnr − r1 + lnr − r2 − lnr − r3 cid:6   u r  = G r, r0  = − λ  ,  2π cid:4 0  which equals zero on C and on the ‘surface’ at inﬁnity.  The force on the line charge at r0, therefore, is simply that due to the three line charges at r1, r2 and r3. The elecrostatic potential due to a line charge at ri, i = 1, 2 or 3, is given by the fundamental solution  the upper or lower sign being taken according to whether the line charge is positive or negative, respectively. Therefore the force per unit length on the line charge at r0, due to the one at ri, is given by  Adding the contributions from the three image charges shown in ﬁgure 21.13, the total force experienced by the line charge at r0 is given by  F =  λ2 2π cid:4 0  − r0 − r1 r0 − r12  r0 − r2 r0 − r22  − r0 − r3 r0 − r32  +  where, from the ﬁgure, r0 − r1 = 2y0j, r0 − r2 = 2x0i + 2y0j and r0 − r3 = 2x0i. Thus, in   cid:8   ,  ui r  = ∓ λ  lnr − ri + c,  2π cid:4 0   cid:20  cid:20  cid:20  cid:20   −λ∇ui r   cid:7   = ± λ2  2π cid:4 0  r0 − ri r0 − ri2 .  r=r0  761   PDES: SEPARATION OF VARIABLES AND OTHER METHODS  y  −λ  r3  +λ  x0  r0  y0  V  C  x  +λ  r2  −λ  r1  Figure 21.13 The arrangement of images for ﬁnding the force on a line charge situated in the  two-dimensional  quarter-space x > 0, y > 0, when the planes x = 0 and y = 0 are earthed.  terms of x0 and y0, the total force on the line charge due to the charge induced on the plates is given by   cid:7   F =  λ2 2π cid:4 0  j +  − 1 2y0 λ2 0 + y2 4π cid:4 0 x2 0   = −   cid:7   2x0 i + 2y0 j 0 + 4y2 4x2 0 x2 y2 0 0 x0 y0  i +  j   cid:8  − 1 2x0 .  cid:2    cid:8   i  Further generalisations are possible. For instance, solving Poisson’s equation in  the two-dimensional strip −∞ < x < ∞, 0 < y < b requires an inﬁnite series of  image points.  So far we have considered problems in which the boundary S consists of straight lines  in two dimensions  or planes  in three dimensions , in which simple reﬂections of the source at r0 in these boundaries ﬁx the positions of the image points. For more complicated  curved  boundaries this is no longer possible, and ﬁnding the appropriate position s  and strength s  of the image source s  requires further work.  cid:1 Use the method of images to ﬁnd the Dirichlet Green’s function for solving Poisson’s equation outside a sphere of radius a centred at the origin.  We need to ﬁnd a solution of Poisson’s equation valid outside the sphere of radius a. Since an image point r1 cannot lie in this region, it must be located within the sphere. The Green’s function for this problem is therefore  G r, r0  = −  1  4πr − r0 −  q  4πr − r1 ,  where r0 > a, r1 < a and q is the strength of the image which we have yet to determine. Clearly, G r, r0  → 0 on the surface at inﬁnity.  762   21.5 INHOMOGENEOUS PROBLEMS – GREEN’S FUNCTIONS  z  a  V  +1  r0  A  −ar0 r1  x  y  B  −a  Figure 21.14 The arrangement of images for solving Poisson’s equation outside a sphere of radius a centred at the origin. For a charge +1 at r0, the  image point r1 is given by  a r0 2r0 and the strength of the image charge is −a r0.  By symmetry we expect the image point r1 to lie on the same radial line as the original source, r0, as shown in ﬁgure 21.14, and so r1 = kr0 where k < 1. However, for a Dirichlet  Green’s function we require G r− r0  = 0 on r = a, and the form of the Green’s function  suggests that we need  r − r0 ∝ r − r1  for all r = a.   21.97   Referring to ﬁgure 21.14, if this relationship is to hold over the whole surface of the  sphere, then it must certainly hold for the points A and B. We thus require  which reduces to r1 = a2 r0. Therefore the image point must be located at the position  r0 − a a − r1 =  r0 + a a + r1 ,  r1 =  a2r02 r0.  It may now be checked that, for this location of the image point,  21.97  is satisﬁed over  the whole sphere. Using the geometrical result  we see that, on the surface of the sphere,  r − r12 = r2 − 2a2  r02 r · r0 +  cid:5 r02 − 2r · r0 + a2 a4r02   cid:6   =  a2r02  for r = a,  r − r1 =  ar0r − r0  for r = a.  763   21.98    21.99    PDES: SEPARATION OF VARIABLES AND OTHER METHODS  Therefore, in order that G = 0 at r = a, the strength of the image charge must be  −a r0. Consequently, the Dirichlet Green’s function for the exterior of the sphere is  G r, r0  = −  1  4πr − r0 +  4π r −  a2 r02 r0 .  a r0  For a less formal treatment of the same problem see exercise 21.22.  cid:2   If we seek solutions to Poisson’s equation in the interior of a sphere then the above analysis still holds, but r and r0 are now inside the sphere and the image r1 lies outside it.  For two-dimensional Dirichlet problems outside the circle r = a, we are led  by arguments similar to those employed previously to use the same image point as in the three-dimensional case, namely  r1 =  a2r02 r0.   21.100   As illustrated below, however, it is usually necessary to take the image strength  as −1 in two-dimensional problems.  cid:1 Solve Laplace’s equation in the two-dimensional region r ≤ a, subject to the boundary condition u = f φ  on r = a.  In this case we wish to ﬁnd the Dirichlet Green’s function in the interior of a disc of radius a, so the image charge must lie outside the disc. Taking the strength of the image  to be −1, we have  G r, r0  =  1 2π  lnr − r0 − 1  lnr − r1 + c,  2π  where r1 =  a2 r02 r0 lies outside the disc, and c is a constant that includes the strength Since we require G r, r0  = 0 when r = a, the value of the constant c is determined,  of the image charge and does not necessarily equal zero.  and the Dirichlet Green’s function for this problem is given by   cid:20  cid:20  cid:20  cid:20  − ln  r0   cid:8   .  r0  a   21.101   Using plane polar coordinates, the solution to the boundary-value problem can be written as a line integral around the circle ρ = a:  G r, r0  =  1 2π   cid:7    cid:20  cid:20  cid:20  cid:20 r − a2r02  cid:20  cid:20  cid:20  cid:20   dl  ρ=a  lnr − r0 − ln  cid:21   cid:21   f r   C  2π  =  f r   0  ∂G r, r0   ∂n ∂G r, r0   ∂ρ  u r0  =  a dφ.   21.102   The normal derivative of the Green’s function  21.101  is given by  ∂G r, r0   ∂ρ   cid:7  rr · ∇G r, r0  r − r0 2πr · r − r02  r  =  =   cid:8   .  − r − r1 r − r12  764   21.103    21.5 INHOMOGENEOUS PROBLEMS – GREEN’S FUNCTIONS  Using the fact that r1 =  a2 r02 r0 and the geometrical result  21.99 , we ﬁnd that  ∂G r, r0   a2 − r02 2πar − r02 .  =   cid:20  cid:20  cid:20  cid:20   ρ=a   cid:8    cid:20  cid:20  cid:20  cid:20   ∂ρ   cid:7   cid:21   In plane polar coordinates, r = ρ cos φ i + ρ sin φ j and r0 = ρ0 cos φ0 i + ρ0 sin φ0 j,  and so  ∂G r, r0   ∂ρ  ρ=a  =  1  2πa  a2 + ρ2 0  a2 − ρ2 − 2aρ0 cos φ − φ0   0  .  On substituting into  21.102 , we obtain  u ρ0, φ0  =  2π  1 2π  0  a2 + ρ2 0   a2 − ρ2 − 2aρ0 cos φ − φ0   0 f φ  dφ  ,   21.104   which is the solution to the problem.  cid:2   21.5.4 Neumann problems  In a Neumann problem we require the normal derivative of the solution of Poisson’s equation to take on speciﬁc values on some surface S that bounds V , i.e. we require ∂u r  ∂n = f r  on S , where f is a given function. As we shall see, much of our discussion of Dirichlet problems can be immediately taken over into the solution of Neumann problems.  As we proved in section 20.7 of the previous chapter, specifying Neumann boundary conditions determines the relevant solution of Poisson’s equation to within an  unimportant  additive constant. Unlike Dirichlet conditions, Neumann conditions impose a self-consistency requirement. In order for a solution u to exist, it is necessary that the following consistency condition holds:   cid:21   ∇u · ˆn dS =  ∇2u dV =  ρ dV ,  V   21.105    cid:21    cid:21   f dS =  S  S   cid:21   V  where we have used the divergence theorem to convert the surface integral into a volume integral. As a physical example, the integral of the normal component of an electric ﬁeld over a surface bounding a given volume cannot be chosen arbitrarily when the charge inside the volume has already been speciﬁed  Gauss’s theorem .  Let us again consider  21.84 , which is central to our discussion of Green’s  functions in inhomogeneous problems. It reads   cid:14   ∂G r, r0   ∂n  − G r, r0   ∂u r   ∂n  dS  r .   cid:21   V  u r0  =  G r, r0 ρ r  dV  r  +  u r   As always, the Green’s function must obey  ∇2G r, r0  = δ r − r0 ,  where r0 lies in V . In the solution of Dirichlet problems in the previous subsection, we chose the Green’s function to obey the boundary condition G r, r0  = 0 on S   cid:21    cid:13   S  765   PDES: SEPARATION OF VARIABLES AND OTHER METHODS  and, in a similar way, we might wish to choose ∂G r, r0  ∂n = 0 in the solution of Neumann problems. However, in general this is not permitted since the Green’s function must obey the consistency condition   cid:21   ∂G r, r0   S  ∂n  dS =  ∇G r, r0  · ˆn dS =  ∇2G r, r0  dV = 1.   cid:21   V   cid:21   S  The simplest permitted boundary condition is therefore   cid:21   cid:21   V  V  ∂G r, r0   ∂n  =  1 A  for r on S ,   cid:21    cid:21  u r  dS  r  −  S  1 A   cid:21   S  where A is the area of the surface S ; this deﬁnes a Neumann Green’s function.  If we require ∂u r  ∂n = f r  on S , the solution to Poisson’s equation is given by  u r0  =  G r, r0 ρ r  dV  r  +  G r, r0 f r  dS  r   G r, r0 ρ r  dV  r  +  cid:20 u r  cid:21 S −  =  G r, r0 f r  dS  r ,   21.106   S  where  cid:20 u r  cid:21 S is the average of u over the surface S and is a freely speciﬁable con- inﬁnity, we do not need the  cid:20 u r  cid:21 S term. For example, if we wish to solve a Neu-  stant. For Neumann problems in which the volume V is bounded by a surface S at  mann problem outside the unit sphere centred at the origin then r > a is the region V throughout which we require the solution; this region may be considered as be- ing bounded by two disconnected surfaces, the surface of the sphere and a surface  at inﬁnity. By requiring that u r  → 0 as r → ∞, the term  cid:20 u r  cid:21 S becomes zero.  As mentioned above, much of our discussion of Dirichlet problems can be taken over into the solution of Neumann problems. In particular, we may use the method of images to ﬁnd the appropriate Neumann Green’s function.   cid:1 Solve Laplace’s equation in the two-dimensional region r ≤ a subject to the boundary condition ∂u ∂n = f φ  on r = a, with 2π 0 f φ  dφ = 0 as required by the consistency   cid:1   condition  21.105 .  Let us assume, as in Dirichlet problems with this geometry, that a single image charge is placed outside the circle at  r1 =  a2r02  r0,  where r0 is the position of the source inside the circle  see equation  21.100  . Then, from  21.99 , we have the useful geometrical result  Leaving the strength q of the image as a parameter, the Green’s function has the form  for r = a.  r − r1 = ar0r − r0  cid:5    cid:6  lnr − r0 + q lnr − r1 + c  .  G r, r0  =  1 2π  766   21.107    21.108    21.6 EXERCISES  Using plane polar coordinates, the radial  i.e. normal  derivative of this function is given by  Using  21.107 , on the perimeter of the circle ρ = a the radial derivative takes the form  or 1 L, where L is the circumference, and so  21.108  with q = 1 is the required Neumann Green’s function.  Since ρ r  = 0, the solution to our boundary-value problem is now given by  21.106  as  where we have set r2 = a2 in the second term on the RHS, but not in the ﬁrst. If we take  q = 1, the radial derivative simpliﬁes to  ∂G r, r0   ∂ρ   cid:20  cid:20  cid:20  cid:20   =  =  r   cid:13  rr · ∇G r, r0  r − r0 2πr · r − r02  cid:13 r2 − r · r0  r − r02  ∂G r, r0   ∂ρ  ρ=a  1  2πr  =  =  1  2πa  1  r − r02   cid:14   .  q r − r1  r − r12  +   cid:14   ,  qr2 − q a2 r02 r · r0  cid:19   a2 r02 r − r02  +   cid:18 r2 + qr02 −  1 + q r · r0  cid:20  cid:20  cid:20  cid:20   cid:21   2πa  ρ=a  =  1  ,  ∂G r, r0   ∂ρ  u r0  =  cid:20 u r  cid:21 C −  cid:13   cid:7   cid:7  lnr − r0 + ln  cid:12  lnr − r02 + ln  cid:18   ln  a2 + ρ2 0  G r, r0 f r  dl r ,  C   cid:8    cid:14   + c   cid:19    cid:8  ar0r − r0 ar0 + c   cid:15   where the integral is around the circumference of the circle C. In plane polar coordinates r = ρ cos φ i + ρ sin φ j and r0 = ρ0 cos φ0 i + ρ0 sin φ0 j, and again using  21.107  we ﬁnd that on C the Green’s function is given by  G r, r0   ρ=a =  1 2π 1 2π 1 2π  =  =   cid:1   − 2aρ0 cos φ − φ0   a ρ0  + ln  + c  .   21.109    cid:21   2π  2π  0  Since dl = a dφ on C, the solution to the problem is given by  u ρ0, φ0  =  cid:20 u cid:21 C − a  f φ  ln[a2 + ρ2 0  − 2aρ0 cos φ − φ0 ] dφ.  2π  0 f φ  dφ = 0. The average value of u around the circumference,  cid:20 u cid:21 C, is a freely  The contributions of the ﬁnal two terms terms in the Green’s function  21.109  vanish because speciﬁable constant as we would expect for a Neumann problem. This result should be compared with the result  21.104  for the corresponding Dirichlet problem, but it should be remembered that in the one case f φ  is a potential, and in the other the gradient of a potential.  cid:2   21.1  Solve the following ﬁrst-order partial diﬀerential equations by separating the variables:   a   − x  ∂u ∂y  ∂u ∂x  = 0;   b  x  − 2y  ∂u ∂y  ∂u ∂x  = 0.  21.6 Exercises  767   PDES: SEPARATION OF VARIABLES AND OTHER METHODS  21.2  A cube, made of material whose conductivity is k, has as its six faces the planes  x = ±a, y = ±a and z = ±a, and contains no internal heat sources. Verify that  the temperature distribution   cid:7    cid:8   u x, y, z, t  = A cos  sin  exp  πx a  πz a  − 2κπ2t a2  obeys the appropriate diﬀusion equation. Across which faces is there heat ﬂow? What is the direction and rate of heat ﬂow at the point  3a 4, a 4, a  at time t = a2  κπ2 ? The wave equation describing the transverse vibrations of a stretched membrane under tension T and having a uniform surface density ρ is   cid:7    cid:8   21.3  T  ∂2u ∂x2  +  ∂2u ∂y2  = ρ  ∂2u ∂t2 .  ω2 =  π2T ρ  n2 a2  +  m2 b2  ,   cid:8    cid:8   Find a separable solution appropriate to a membrane stretched on a frame of length a and width b, showing that the natural angular frequencies of such a membrane are given by   cid:7   21.4  where n and m are any positive integers. Schr¨odinger’s equation for a non-relativistic particle in a constant potential region can be taken as   cid:7   −  cid:1 2 2m  ∂2u ∂x2  +  ∂2u ∂y2  +  ∂2u ∂z2  = i cid:1   ∂u ∂t  .   a  Find a solution, separable in the four independent variables, that can be  written in the form of a plane wave,  ψ x, y, z, t  = A exp[i k · r − ωt ].  Using the relationships associated with de Broglie  p =  cid:1 k  and Einstein  E =  cid:1 ω , show that the separation constants must be such that   b  Obtain a diﬀerent separable solution describing a particle conﬁned to a box of side a  ψ must vanish at the walls of the box . Show that the energy of the particle can only take the quantised values  p2 x + p2  y + p2  z = 2mE.  E =   cid:1 2π2 2ma2   n2  x + n2  y + n2 z ,  21.5  where nx, ny and nz are integers.  Denoting the three terms of ∇2 in spherical polars by ∇2 φ in an obvious way, evaluate ∇2 case, although the individual terms are not necessarily zero their sum ∇2u is zero. r u, etc. for the two functions given below and verify that, in each  cid:7   cid:7   Identify the corresponding values of  cid:2  and m.  3 cos2 θ − 1   cid:8   cid:8    a  u r, θ, φ  =  θ, ∇2  r , ∇2  Ar2 +  2  .   b  u r, θ, φ  =  Ar +  sin θ exp iφ.  B r3 B r2  21.6  Prove that the expression given in equation  21.47  for the associated Legendre function P m   cid:2   µ  satisﬁes the appropriate equation,  21.45 , as follows.  768   21.6 EXERCISES   a  Evaluate dP m   cid:2   µ  dµ and d2P m   cid:2   µ  dµ2, using the forms given in  21.47 , and  substitute them into  21.45 .   b  Diﬀerentiate Legendre’s equation m times using Leibnitz’ theorem.  c  Show that the equations obtained in  a  and  b  are multiples of each other,  and hence that the validity of  b  implies that of  a .  21.7  Continue the analysis of exercise 10.20, concerned with the ﬂow of a very viscous ﬂuid past a sphere, to ﬁnd the full expression for the stream function ψ r, θ . At the surface of the sphere r = a, the velocity ﬁeld u = 0, whilst far from the sphere  ψ  cid:15   Ur2 sin2 θ  2.  Show that f r  can be expressed as a superposition of powers of r, and  determine which powers give acceptable solutions. Hence show that   cid:7    cid:8   ψ r, θ  =  U 4  2r2 − 3ar +  a3 r  sin2 θ.  21.8  The motion of a very viscous ﬂuid in the two-dimensional  wedge  region −α <  φ < α can be described, in  ρ, φ  coordinates, by the  biharmonic  equation  ∇2∇2ψ ≡ ∇4ψ = 0,  together with the boundary conditions ∂ψ ∂φ = 0 at φ = ±α, which represent because of the viscosity, and ∂ψ ∂ρ = ±ρ at φ = ±α, which impose the condition  the fact that there is no radial ﬂuid velocity close to either of the bounding walls  that azimuthal ﬂow increases linearly with r along any radial line. Assuming a solution in separated-variable form, show that the full expression for ψ is  ψ ρ, φ  =  sin 2φ − 2φ cos 2α sin 2α − 2α cos 2α  .  ρ2 2  21.9  21.10  A circular disc of radius a is heated in such a way that its perimeter ρ = a has a steady temperature distribution A + B cos2 φ, where ρ and φ are plane polar coordinates and A and B are constants. Find the temperature T  ρ, φ  everywhere in the region ρ < a. Consider possible solutions of Laplace’s equation inside a circular domain as follows.   a  Find the solution in plane polar coordinates ρ, φ, that takes the value +1  for 0 < φ < π and the value −1 for −π < φ < 0, when ρ = a.   b  For a point  x, y  on or inside the circle x2 + y2 = a2, identify the angles α  and β deﬁned by  α = tan  −1  y  a + x  and  β = tan  −1  y  a − x  .  Show that u x, y  =  2 π  α + β  is a solution of Laplace’s equation that satisﬁes the boundary conditions given in  a .   c  Deduce a Fourier series expansion for the function  −1  tan  sin φ  1 + cos φ  −1  + tan  sin φ  1 − cos φ  .  21.11  The free transverse vibrations of a thick rod satisfy the equation  Obtain a solution in separated-variable form and, for a rod clamped at one end,  a4 ∂4u ∂x4  +  ∂2u ∂t2  = 0.  769   PDES: SEPARATION OF VARIABLES AND OTHER METHODS  x = 0, and free at the other, x = L, show that the angular frequency of vibration ω satisﬁes   cid:7    cid:8    cid:7    cid:8   cosh  ω1 2L  a  = − sec  ω1 2L  a  .  21.12  [ At a clamped end both u and ∂u ∂x vanish, whilst at a free end, where there is no bending moment, ∂2u ∂x2 and ∂3u ∂x3 are both zero. ] A membrane is stretched between two concentric rings of radii a and b  b > a . If the smaller ring is transversely distorted from the planar conﬁguration by an  amount cφ, −π ≤ φ ≤ π, show that the membrane then has a shape given by   cid:4   m odd  u ρ, φ  =  cπ 2  ln b ρ  ln b a   − 4c  π  ∞ cid:4   n=0  u x, t  =  8A  −1 n  π2 2n + 1 2  sin  am  m2 b2m − a2m   cid:14   cid:13    2n + 1 πx   cid:8   − ρm  b2m ρm  cos mφ.   cid:7    cid:13    cid:14   ,   2n + 1 πct  cos  L  L  21.13  A string of length L, ﬁxed at its two ends, is plucked at its mid-point by an amount A and then released. Prove that the subsequent displacement is given by  where, in the usual notation, c2 = T  ρ.  Find the total kinetic energy of the string when it passes through its unplucked  position, by calculating it in each mode  each n  and summing, using the result  ∞ cid:4   1  =  π2 8  .   2n + 1 2  0  21.14  21.16  21.17  Conﬁrm that the total energy is equal to the work done in plucking the string initially. Prove that the potential for ρ < a associated with a vertical split cylinder of radius a, the two halves of which  cos φ > 0 and cos φ < 0  are maintained at   cid:10   cid:9  equal and opposite potentials ±V , is given by  ∞ cid:4   u ρ, φ  =   −1 n  2n + 1  4V π  n=0  2n+1  ρ a  cos 2n + 1 φ.  21.15  A conducting spherical shell of radius a is cut round its equator and the two  halves connected to voltages of +V and −V . Show that an expression for the  potential at the point  r, θ, φ  anywhere inside the two hemispheres is  ∞ cid:4   n=0   −1 n 2n ! 4n + 3   22n+1n! n + 1 !   cid:9    cid:10   r a  u r, θ, φ  = V  2n+1  P2n+1 cos θ .  [ This is the spherical polar analogue of the previous question. ] A slice of biological material of thickness L is placed into a solution of a radioactive isotope of constant concentration C0 at time t = 0. For a later time t ﬁnd the concentration of radioactive ions at a depth x inside one of its surfaces if the diﬀusion constant is κ. Two identical copper bars are each of length a. Initially, one is at 0 C and the other is at 100 C; they are then joined together end to end and thermally isolated. Obtain in the form of a Fourier series an expression u x, t  for the temperature at any point a distance x from the join at a later time t. Bear in mind the heat ﬂow conditions at the free ends of the bars.  ◦  ◦  Taking a = 0.5 m estimate the time it takes for one of the free ends to  C. The thermal conductivity of copper is 3.8 ×  attain a temperature of 55 102 J m  −1, and its speciﬁc heat capacity is 3.4 × 106 J m  −1 K  −3 K  −1 s  −1.  ◦  770   21.6 EXERCISES  21.18  A sphere of radius a and thermal conductivity k1 is surrounded by an inﬁnite medium of conductivity k2 in which far away the temperature tends to T∞. A distribution of heat sources q θ  embedded in the sphere’s surface establish steady temperature ﬁelds T1 r, θ  inside the sphere and T2 r, θ  outside it. It can be shown, by considering the heat ﬂow through a small volume that includes part of the sphere’s surface, that  Given that  k1  ∂T1 ∂r  − k2  ∂T2 ∂r  = q θ  on r = a.  ∞ cid:4   n=0  1 a  q θ  =  qnPn cos θ ,  21.19  ﬁnd complete expressions for T1 r, θ  and T2 r, θ . What is the temperature at the centre of the sphere? Using result  21.74  from the worked example in the text, ﬁnd the general expression for the temperature u x, t  in the bar, given that the temperature  distribution at time t = 0 is u x, 0  = exp −x2 a2 .  21.20 Working in spherical polar coordinates r =  r, θ, φ , but for a system that has azimuthal symmetry around the polar axis, consider the following gravitational problem.   a  Show that the gravitational potential due to a uniform disc of radius a and  mass M, centred at the origin, is given for r < a by   cid:13   2GM  a   cid:9    cid:10   2  1 2  r a  a  1 − r  cid:13   GM  r  P1 cos θ  +   cid:9    cid:10   2  1 − 1  4  a r  P2 cos θ  − 1  cid:10    cid:9   8  1 8  a r  P2 cos θ  +  4  P4 cos θ  − ···  ,   cid:9    cid:10   r a   cid:14   4  P4 cos θ  + ···  ,   cid:14   and for r > a by  where the polar axis is normal to the plane of the disc.   b  Reconcile the presence of a term P1 cos θ , which is odd under θ → π − θ,  with the symmetry with respect to the plane of the disc of the physical system.   c  Deduce that the gravitational ﬁeld near an inﬁnite sheet of matter of constant  density ρ per unit area is 2πGρ.  21.21  In the region −∞ < x, y < ∞ and −t ≤ z ≤ t, a charge-density wave ρ r  =  A cos qx, in the x-direction, is represented by  The resulting potential is represented by  ρ r  =  −∞ ˜ρ α eiαz dα.  V  r  =  ˜V  α eiαz dα.  Determine the relationship between ˜V  α  and ˜ρ α , and hence show that the potential at the point  0, 0, 0  is   cid:21  ∞  cid:21  ∞  −∞  eiqx√ 2π  eiqx√ 2π   cid:21  ∞  771  A π cid:4 0  sin kt  −∞  k k2 + q2   dk.   PDES: SEPARATION OF VARIABLES AND OTHER METHODS  21.22  Point charges q and −qa b  with a < b  are placed, respectively, at a point P , a distance b from the origin O, and a point Q between O and P , a distance a2 b from O. Show, by considering similar triangles QOS and S OP , where S is any point on the surface of the sphere centred at O and of radius a, that the net potential anywhere on the sphere due to the two charges is zero.  Use this result  backed up by the uniqueness theorem  to ﬁnd the force with which a point charge q placed a distance b from the centre of a spherical conductor of radius a  < b  is attracted to the sphere  i  if the sphere is earthed, and  ii  if the sphere is uncharged and insulated. Find the Green’s function G r, r0  in the half-space z > 0 for the solution of  ∇2Φ = 0 with Φ speciﬁed in cylindrical polar coordinates  ρ, φ, z  on the plane  21.23  z = 0 by    Φ ρ, φ, z  =  for ρ ≤ 1,  1 1 ρ for ρ > 1.  21.24  Determine the variation of Φ 0, 0, z  along the z-axis. Electrostatic charge is distributed in a sphere of radius R centred on the origin. Determine the form of the resultant potential φ r  at distances much greater than R, as follows.   a  Express in the form of an integral over all space the solution of   b  Show that, for r  cid:26  r   cid:7   ,  ∇2φ = − ρ r   .   cid:4 0  r − r   cid:7  = r − r · r   cid:7   + O   cid:8   cid:8    cid:7   cid:7   1 r  1 r3  .  .  r  d · r r3  φ r  =  +  + O   c  Use results  a  and  b  to show that φ r  has the form  M r    21.25  Find expressions for M and d, and identify them physically.  Find, in the form of an inﬁnite series, the Green’s function of the ∇2 operator for the Dirichlet problem in the region −∞ < x < ∞, −∞ < y < ∞, −c ≤ z ≤ c.  21.26  Find the Green’s function for the three-dimensional Neumann problem  ∇2φ = 0  Determine φ x, y, z  if  for z > 0  and  = f x, y   on z = 0.  ∂φ ∂z  f x, y  =  for x < a, for x ≥ a.  δ y  0  21.27  Determine the Green’s function for the Klein–Gordon equation in a half-space as follows.   a  By applying the divergence theorem to the volume integral  φ ∇2 − m2 ψ − ψ ∇2 − m2 φ  dV ,   cid:19    cid:21    cid:18   V  obtain a Green’s function expression, as the sum of a volume integral and a surface integral, for the function φ r    that satisﬁes   cid:7   ∇2φ − m2φ = ρ  772   21.7 HINTS AND ANSWERS   cid:7    cid:7    cid:7       , to be used satisﬁes  in V and takes the speciﬁed form φ = f on S , the boundary of V . The Green’s function, G r, r  and vanishes when r is on S .   b  When V is all space, G r, r  and g t  is bounded as t → ∞. Find the form of G t . and as r → ∞.  ∇2G − m2G = δ r − r  cid:7    can be written as G t  = g t  t, where t = r− r  c  Find φ r  in the half-space x > 0 if ρ r  = δ r− r1  and φ = 0 both on x = 0 Consider the PDE Lu r  = ρ r , for which the diﬀerential operator L is given by Green’s theorem, cid:21   where p r  and q r  are functions of position. By proving the generalised form of  L = ∇ · [ p r ∇ ] + q r ,  p φ∇ψ − ψ∇φ  · ˆn dS ,  V   φLψ − ψLφ  dV = 0   cid:21   0  cid:13   S  show that the solution of the PDE is given by  u r0  =  G r, r0 ρ r  dV  r  +  p r   u r   where G r, r0  is the Green’s function satisfying LG r, r0  = δ r − r0 .  V  S  ∂G r, r0   ∂n  − G r, r0   ∂u r   ∂n  dS  r ,   cid:14   21.7 Hints and answers   a  C exp[λ x2 + 2y ];  b  C x2y λ. u x, y, t  = sin nπx a  sin mπy b  A sin ωt + B cos ωt .  a  6u r2, −6u r2, 0,  cid:2  = 2  or −3 , m = 0;  b  2u r2,  cot2 θ − 1 u r2; −u  r2 sin2 θ ,  cid:2  = 1  or −2 , m = ±1. Solutions of the form r cid:2  give  cid:2  as −1, 1, 2, 4. Because of the asymptotic form of ψ, an r4 term cannot be present. The coeﬃcients of the three remaining terms are determined by the two boundary conditions u = 0 on the sphere and the form of ψ for large r. Express cos2 φ in terms of cos 2φ; T  ρ, φ  = A + B 2 +  Bρ2 2a2  cos 2φ.  A cos mx + B sin mx + C cosh mx + D sinh mx  cos ωt +  cid:4  , with m4a4 = ω2. En = 16ρA2c2 [ 2n + 1 2π2L]; E = 2ρc2A2 L = Note that the boundary value function is a square wave that is symmetric in φ. Since there is no heat ﬂow at x = ±a, use a series of period 4a, u x, 0  = 100 for  cid:13  0 < x ≤ 2a, u x, 0  = 0 for −2a ≤ x < 0.  0 [2T v   1  2 L ] dv.   cid:13    cid:14    cid:14    cid:1   A  u x, t  = 50 +  200 π  1  2n + 1  sin   2n + 1 πx  2a  exp  − k 2n + 1 2π2t  .  4a2s  Taking only the n = 0 term gives t ≈ 2300 s. u x, t  = [a  a2 + 4κt 1 2] exp[−x2  a2 + 4κt ]. Fourier-transform Poisson’s equation to show that ˜ρ α  =  cid:4 0 α2 + q2  ˜V  α . Follow the worked example that includes result  21.95 . For part of the explicit integration, substitute ρ = z tan α.  ∞ cid:4   n=0  Φ 0, 0, z  =  z 1 + z2 1 2 − z2 +  1 + z2 1 2 − 1  .  z 1 + z2 1 2  773  21.28  21.1 21.3 21.5  21.7  21.9 21.11 21.13 21.15 21.17  21.19 21.21 21.23   PDES: SEPARATION OF VARIABLES AND OTHER METHODS  21.25  The terms in G r, r0  that are additional to the fundamental solution are  ∞ cid:4   n=2  1 4π   cid:19 −1 2 "  x − x0 2 +  y − y0 2 +  z +  −1 nz0 − nc 2  cid:19 −1 2  ! cid:18   x − x0 2 +  y − y0 2 +  z +  −1 nz0 + nc 2  .   −1 n  cid:18   +  21.27   cid:7    cid:21    cid:7    a  As given in equation  21.86 , but with r0 replaced by r  b  Move the origin to r  and integrate the deﬁning Green’s equation to obtain 4πt2 dG dt leading to G t  = [−1  4πt ]e −mp − q r1 =  x1, y1, z1  and r2 =  −x1, y1, z1 .  −mq , where p = r− r1 and q = r− r2 with  − m2 −mt. −1e   c  φ r  = [−1  4π ] p  −1e    4πt  = 1,  G t  dt   cid:7 2   cid:7    cid:7   0  .  t  774   22  Calculus of variations  In chapters 2 and 5 we discussed how to ﬁnd stationary values of functions of a single variable f x , of several variables f x, y, . . .   and of constrained variables, where x, y, . . . are subject to the n constraints gi x, y, . . .   = 0, i = 1, 2, . . . , n. In all these cases the forms of the functions f and gi were known, and the problem was one of ﬁnding the appropriate values of the variables x, y etc.  We now turn to a diﬀerent kind of problem in which we are interested in bringing about a particular condition for a given expression  usually maximising or minimising it  by varying the functions on which the expression depends. For instance, we might want to know in what shape a ﬁxed length of rope should be arranged so as to enclose the largest possible area, or in what shape it will hang when suspended under gravity from two ﬁxed points. In each case we are concerned with a general maximisation or minimisation criterion by which the function y x  that satisﬁes the given problem may be found.  The calculus of variations provides a method for ﬁnding the function y x . The problem must ﬁrst be expressed in a mathematical form, and the form most commonly applicable to such problems is an integral. In each of the above questions, the quantity that has to be maximised or minimised by an appropriate choice of the function y x  may be expressed as an integral involving y x  and the variables describing the geometry of the situation.  In our example of the rope hanging from two ﬁxed points, we need to ﬁnd the shape function y x  that minimises the gravitational potential energy of the rope. Each elementary piece of the rope has a gravitational potential energy proportional both to its vertical height above an arbitrary zero level and to the length of the piece. Therefore the total potential energy is given by an integral for the whole rope of such elementary contributions. The particular function y x  for which the value of this integral is a minimum will give the shape assumed by the hanging rope.  775   CALCULUS OF VARIATIONS  y  a  b  x  Figure 22.1 Possible paths for the integral  22.1 . The solid line is the curve along which the integral is assumed stationary. The broken curves represent small variations from this path.  So in general we are led by this type of question to study the value of an integral whose integrand has a speciﬁed form in terms of a certain function and its derivatives, and to study how that value changes when the form of the function is varied. Speciﬁcally, we aim to ﬁnd the function that makes the integral stationary, i.e. the function that makes the value of the integral a local is used to denote maximum or minimum. Note that, unless stated otherwise, y dy dx throughout this chapter. We also assume that all the functions we need to deal with are suﬃciently smooth and diﬀerentiable.   cid:7   22.1 The Euler–Lagrange equation  Let us consider the integral  I =  F y, y  , x  dx,   cid:7    22.1   where a, b and the form of the function F are ﬁxed by given considerations, e.g. the physics of the problem, but the curve y x  is to be chosen so as to make stationary the value of I, which is clearly a function, or more accurately a functional, of this curve, i.e. I = I[ y x ]. Referring to ﬁgure 22.1, we wish to ﬁnd the function y x   given, say, by the solid line  such that ﬁrst-order small changes in it  for example the two broken lines  will make only second-order changes in the value of I.  Writing this in a more mathematical form, let us suppose that y x  is the  function required to make I stationary and consider making the replacement  y x  → y x  + αη x ,   22.2   where the parameter α is small and η x  is an arbitrary function with suﬃciently amenable mathematical properties. For the value of I to be stationary with respect   cid:21   b  a  776   22.2 SPECIAL CASES  to these variations, we require   cid:20  cid:20  cid:20  cid:20   dI dα  α=0   cid:7    cid:7    cid:7    cid:21    cid:21   cid:21   b  a  b  a  = 0  for all η x .   22.3   Substituting  22.2  into  22.1  and expanding as a Taylor series in α we obtain  I y, α  =  F y + αη, y  + αη  , x  dx  =  F y, y  , x  dx +  ∂F ∂y  αη +   cid:7   ∂F ∂y   cid:7  αη  dx + O α2 .   cid:8    cid:7    cid:21    cid:7   b  a   cid:8   δI =  b  a  ∂F ∂y  η +   cid:7   ∂F ∂y   cid:7  η  dx = 0,  With this form for I y, α  the condition  22.3  implies that for all η x  we require  where δI denotes the ﬁrst-order variation in the value of I due to the variation  22.2  in the function y x . Integrating the second term by parts this becomes   cid:13    cid:14    cid:21    cid:13    cid:7    cid:8  cid:14   η  ∂F  cid:7  ∂y  b  a  +  b  a  − d  dx  ∂F ∂y  ∂F  cid:7  ∂y  η x  dx = 0.   22.4   In order to simplify the result we will assume, for the moment, that the end-points are ﬁxed, i.e. not only a and b are given but also y a  and y b . This restriction means that we require η a  = η b  = 0, in which case the ﬁrst term on the LHS of  22.4  equals zero at both end-points. Since  22.4  must be satisﬁed for arbitrary η x , it is easy to see that we require  This is known as the Euler–Lagrange  EL  equation, and is a diﬀerential equation for y x , since the function F is known.  In certain special cases a ﬁrst integral of the EL equation can be obtained for a general form of F.  22.2.1 F does not contain y explicitly  In this case ∂F ∂y = 0, and  22.5  can be integrated immediately giving   cid:7    cid:8   ∂F ∂y  =  d dx  ∂F  cid:7  ∂y  .  22.2 Special cases  ∂F ∂y   cid:7  = constant.  777   22.5    22.6    CALCULUS OF VARIATIONS  B   b, y b    ds  dx  dy  A  a, y a    Figure 22.2 An arbitrary path between two ﬁxed points.   cid:1 Show that the shortest curve joining two points is a straight line.  Let the two points be labelled A and B and have coordinates  a, y a   and  b, y b   respectively  see ﬁgure 22.2 . Whatever the shape of the curve joining A to B, the length of an element of path ds is given by  ds =   dx 2 +  dy 2  1 2  =  1 + y   cid:7 2 1 2dx,  and hence the total path length along the curve is given by  L =   1 + y   cid:7 2 1 2 dx.   22.7    cid:18    cid:19    cid:21   b  a  We must now apply the results of the previous section to determine that path which makes L stationary  clearly a minimum in this case . Since the integral does not contain y  or indeed x  explicitly, we may use  22.6  to obtain  where k is a constant. This is easily rearranged and integrated to give  which, as expected, is the equation of a straight line in the form y = mx + c, with  m = k  1 − k2 1 2. The value of m  or k  can be found by demanding that the straight line passes through the points A and B and is given by m = [ y b  − y a ]  b − a . Substituting  the equation of the straight line into  22.7  we ﬁnd that, again as expected, the total path length is given by  L2 = [ y b  − y a ]2 +  b − a 2.  cid:2   k =  ∂F ∂y   cid:7  =  y  1 + y   cid:7   cid:7 2 1 2  .  y =  k   1 − k2 1 2 x + c,  778   22.2 SPECIAL CASES  y  ds  dy  dx  x  Figure 22.3 A convex closed curve that is symmetrical about the x-axis.  In this case, multiplying the EL equation  22.5  by y  22.2.2 F does not contain x explicitly   cid:7    cid:8   d dx   cid:7  ∂F  cid:7  ∂y  y  = y   cid:7  d dx   cid:7    cid:8   ∂F  cid:7  ∂y   cid:7    cid:7    cid:8   and using  cid:7  cid:7  ∂F  cid:7  ∂y  + y   cid:7  ∂F ∂y  y  + y   cid:7  =  d dx   cid:7  ∂F  cid:7  ∂y  y  .   cid:7  cid:7  ∂F ∂y  cid:7   we obtain  only, and not explicitly of x, the LHS of But since F is a function of y and y this equation is just the total derivative of F, namely dF dx. Hence, integrating we obtain  F − y   cid:7  ∂F ∂y   cid:7  = constant.   22.8    cid:1 Find the closed convex curve of length l that encloses the greatest possible area.  Without any loss of generality we can assume that the curve passes through the origin and can further suppose that it is symmetric with respect to the x-axis; this assumption is not essential. Using the distance s along the curve, measured from the origin, as the independent variable and y as the dependent one, we have the boundary conditions y 0  = y l 2  = 0. The element of area shown in ﬁgure 22.3 is then given by  dA = y dx = y   ds 2 −  dy 2  1 2  ,   cid:19   and the total area by  A = 2  l 2  y 1 − y   cid:7 2 1 2 ds;   22.9    cid:7   here y  stands for dy ds rather than dy dx. Since the integrand does not contain s explicitly,   cid:21   0   cid:18   779   CALCULUS OF VARIATIONS  we can use  22.8  to obtain a ﬁrst integral of the EL equation for y, namely  y 1 − y   cid:7 2 1 2 + yy   cid:7 2 1 − y   cid:7 2   −1 2 = k,  where k is a constant. On rearranging this gives   cid:7   = ± k2 − y2 1 2,  ky  which, using y 0  = 0, integrates to  The other end-point, y l 2  = 0, ﬁxes the value of k as l  2π  to yield  y k = sin s k .   22.10   From this we obtain dy = cos 2πs l  ds and since  ds 2 =  dx 2 +  dy 2 we ﬁnd also that dx = ± sin 2πs l  ds. This in turn can be integrated and, using x 0  = 0, gives x in terms  of s as  We thus obtain the expected result that x and y lie on the circle of radius l  2π  given by  y =  sin  l 2π  2πs  .  l  2π  = − l  cid:8   2  2π  x − l  cid:7   x − l  2π  cos  2πs  .  l  + y2 =  l2 4π2 .  Substituting the solution  22.10  into the expression for the total area  22.9 , it is easily veriﬁed that A = l2  4π . A much quicker derivation of this result is possible using plane polar coordinates.  cid:2   The previous two examples have been carried out in some detail, even though the answers are more easily obtained in other ways, expressly so that the method is transparent and the way in which it works can be ﬁlled in mentally at almost every step. The next example, however, does not have such an intuitively obvious solution.  cid:1 Two rings, each of radius a, are placed parallel with their centres 2b apart and on a common normal. An open-ended axially symmetric soap ﬁlm is formed between them  see ﬁgure 22.4 . Find the shape assumed by the ﬁlm.  Creating the soap ﬁlm requires an energy γ per unit area  numerically equal to the surface tension of the soap solution . So the stable shape of the soap ﬁlm, i.e. the one that minimises the energy, will also be the one that minimises the surface area  neglecting gravitational eﬀects .  It is obvious that any convex surface, shaped such as that shown as the broken line in ﬁgure 22.4 a , cannot be a minimum but it is not clear whether some shape intermediate between the cylinder shown by solid lines in  a , with area 4πab  or twice this for the double surface of the ﬁlm , and the form shown in  b , with area approximately 2πa2, will produce a lower total area than both of these extremes. If there is such a shape  e.g. that in ﬁgure 22.4 c  , then it will be that which is the best compromise between two requirements, the need to minimise the ring-to-ring distance measured on the ﬁlm surface  a  and the need to minimise the average waist measurement of the surface  b .  We take cylindrical polar coordinates as in ﬁgure 22.4 c  and let the radius of the soap  ﬁlm at height z be ρ z  with ρ ±b  = a. Counting only one side of the ﬁlm, the element of  780   22.3 SOME EXTENSIONS  z  b  ρ  −b  a   c    a    b   Figure 22.4 Possible soap ﬁlms between two parallel circular rings.  surface area between z and z + dz is   cid:19   dS = 2πρ   dz 2 +  dρ 2  1 2  ,  so the total surface area is given by   cid:18    cid:21   b  −b  S = 2π  ρ 1 + ρ   cid:7 2 1 2 dz.   22.11   Since the integrand does not contain z explicitly, we can use  22.8  to obtain an equation for ρ that minimises S , i.e.   cid:7 2 1 2 − ρρ   cid:7 2 1 + ρ  ρ 1 + ρ  −1 2 = k,  cid:7 2   cid:7 2 1 2, rearranging to ﬁnd an explicit  where k is a constant. Multiplying through by  1 + ρ expression for ρ  and integrating we ﬁnd   cid:7   where c is the constant of integration. Using the boundary conditions ρ ±b  = a, we  require c = 0 and k such that a k = cosh b k  if b a is too large, no such k can be found . Thus the curve that minimises the surface area is  cosh  =  + c.  −1 ρ k  z k  ρ k = cosh z k ,  and in proﬁle the soap ﬁlm is a catenary  see section 22.4  with the minimum distance from the axis equal to k.  cid:2   22.3 Some extensions  It is quite possible to relax many of the restrictions we have imposed so far. For example, we can allow end-points that are constrained to lie on given curves rather than being ﬁxed, or we can consider problems with several dependent and or independent variables or higher-order derivatives of the dependent variable. Each of these extensions is now discussed.  781   CALCULUS OF VARIATIONS  22.3.1 Several dependent variables   cid:7  Here we have F = F y1, y n, x  where each yi = yi x . The analysis in this case proceeds as before, leading to n separate but simultaneous equations for the yi x ,   cid:7  2, . . . , yn, y   cid:7  1, y2, y   cid:7    cid:8   ∂F ∂yi  =  d dx  ∂F  cid:7  ∂y i  ,  i = 1, 2, . . . , n.   22.12   With n independent variables, we need to extremise multiple integrals of the form   cid:21   cid:21    cid:21   ···   cid:7   I =  22.3.2 Several independent variables   cid:8   F  y,  ∂y ∂x1  ,  ∂y ∂x2  , . . . ,  , x1, x2, . . . , xn  dx1 dx2 ··· dxn.  Using the same kind of analysis as before, we ﬁnd that the extremising function y = y x1, x2, . . . , xn  must satisfy  ∂y ∂xn   cid:7    cid:8   n cid:4   i=1  ∂F ∂y  =  ∂ ∂xi  ∂F ∂yxi  ,   22.13   where yxi stands for ∂y ∂xi.  22.3.3 Higher-order derivatives   cid:7    cid:7  cid:7    cid:7    cid:8   , . . . , y n , x  then using the same method as before If in  22.1  F = F y, y and performing repeated integration by parts, it can be shown that the required extremising function y x  satisﬁes  , y  − d  dx  ∂F ∂y  +  d2 dx2  ∂F  cid:7  cid:7  ∂y  − ··· +  −1 n dn  dxn  ∂F ∂y n   = 0,   22.14   = ··· = y n−1  = 0 at both end-points. If y, or any of its  provided that y = y derivatives, is not zero at the end-points then a corresponding contribution or contributions will appear on the RHS of  22.14 .   cid:7    cid:8   ∂F  cid:7  ∂y  cid:7    cid:7    cid:8   We now discuss the very important generalisation to variable end-points. Suppose, as before, we wish to ﬁnd the function y x  that extremises the integral  but this time we demand only that the lower end-point is ﬁxed, while we allow y b  to be arbitrary. Repeating the analysis of section 22.1, we ﬁnd from  22.4   22.3.4 Variable end-points  I =  F y, y  , x  dx,   cid:7    cid:21   b  a  782   22.3 SOME EXTENSIONS  ∆y  y x  + η x   y x   h x, y  = 0  ∆x  b  Figure 22.5 Variation of the end-point b along the curve h x, y  = 0.  that we require  cid:13    cid:14    cid:21    cid:13    cid:7    cid:8  cid:14   η  ∂F  cid:7  ∂y  b  a  +  b  a  − d  dx  ∂F ∂y  ∂F  cid:7  ∂y  η x  dx = 0.   22.15    cid:20  cid:20  cid:20  cid:20   ∂F  cid:7  ∂y  = 0.  x=b  Obviously the EL equation  22.5  must still hold for the second term on the LHS to vanish. Also, since the lower end-point is ﬁxed, i.e. η a  = 0, the ﬁrst term on the LHS automatically vanishes at the lower limit. However, in order that it also vanishes at the upper limit, we require in addition that   22.16   Clearly if both end-points may vary then ∂F ∂y  must vanish at both ends.  An interesting and more general case is where the lower end-point is again ﬁxed at x = a, but the upper end-point is free to lie anywhere on the curve h x, y  = 0. Now in this case, the variation in the value of I due to the arbitrary variation  22.2  is given to ﬁrst order by   cid:13    cid:14    cid:21    cid:7    cid:7    cid:8   δI =  ∂F ∂y   cid:7  η  b  a  +  b  a  − d  dx  ∂F  cid:7  ∂y  ∂F ∂y  η dx + F b ∆x,   22.17   where ∆x is the displacement in the x-direction of the upper end-point, as indicated in ﬁgure 22.5, and F b  is the value of F at x = b. In order for  22.17  to be valid, we of course require the displacement ∆x to be small.  From the ﬁgure we see that ∆y = η b  + y   b ∆x. Since the upper end-point  must lie on h x, y  = 0 we also require that, at x = b,  which on substituting our expression for ∆y and rearranging becomes  + y  ∆x +  η = 0.   22.18    cid:7   ∂h ∂y  ∂h ∂x  ∆x +  ∆y = 0,   cid:7   ∂h ∂x   cid:7  ∂h ∂y  ∂h ∂y   cid:8   783   CALCULUS OF VARIATIONS  x = x0  x  B  A  y  Figure 22.6 A frictionless wire along which a small bead slides. We seek the shape of the wire that allows the bead to travel from the origin O to the line x = x0 in the least possible time.  Now, from  22.17  the condition δI = 0 requires, besides the EL equation, that at x = b, the other two contributions cancel, i.e.  Eliminating ∆x and η between  22.18  and  22.19  leads to the condition that at the end-point  F∆x +  ∂F ∂y   cid:7  η = 0.   cid:7  F − y   cid:8    cid:7  ∂F  cid:7  ∂y  − ∂F  ∂y   cid:7  ∂h ∂x  ∂h ∂y  = 0.   22.19    22.20   In the special case where the end-point is free to lie anywhere on the vertical line x = b, we have ∂h ∂x = 1 and ∂h ∂y = 0. Substituting these values into  22.20 , we recover the end-point condition given in  22.16 .  cid:1 A frictionless wire in a vertical plane connects two points A and B, A being higher than B. Let the position of A be ﬁxed at the origin of an xy-coordinate system, but allow B to lie anywhere on the vertical line x = x0  see ﬁgure 22.6 . Find the shape of the wire such that a bead placed on it at A will slide under gravity to B in the shortest possible time.  This is a variant of the famous brachistochrone  shortest time  problem, which is often used to illustrate the calculus of variations. Conservation of energy tells us that the particle speed is given by  where s is the path length along the wire and g is the acceleration due to gravity. Since  cid:7 2 1 2dx, the total time taken to travel to the line the element of path length is ds =  1 + y x = x0 is given by   cid:25    cid:21   t =  x=x0  x=0  ds v  =  1√ 2g  x0  0   cid:7 2  1 + y  y  dx.  Because the integrand does not contain x explicitly, we can use  22.8  with the speciﬁc form F =  √ y to ﬁnd a ﬁrst integral; on simpliﬁcation this yields  1 + y   cid:7 2    cid:24    cid:24    cid:21    cid:23   v =  =  2gy,  ds dt   cid:22   y 1 + y  1 2   cid:7 2   = k,  784   22.4 CONSTRAINED VARIATION  where k is a constant. Letting a = k2 and solving for y a − y   cid:7   we ﬁnd  which on substituting y = a sin2 θ integrates to give      cid:7   ,  y  =  =  dy dx  y   2θ − sin 2θ  + c.  x =  a 2  Thus the parametric equations of the curve are given by  x = b φ − sin φ  + c,  y = b 1 − cos φ ,  where b = a 2 and φ = 2θ; they deﬁne a cycloid, the curve traced out by a point on the rim of a wheel of radius b rolling along the x-axis. We must now use the end-point conditions to determine the constants b and c. Since the curve passes through the origin, we see immediately that c = 0. Now since y x0  is arbitrary, i.e. the upper end-point can lie anywhere on the curve x = x0, the condition  22.20  reduces to  22.16 , so that we also require   cid:20  cid:20  cid:20  cid:20    cid:7  cid:24   y   cid:20  cid:20  cid:20  cid:20  cid:20   ∂F  cid:7  ∂y  =  x=x0  y 1 + y   cid:7 2   x=x0  = 0,  which implies that y parallel to the x-axis; this requires πb = x0.  cid:2   = 0 at x = x0. In words, the tangent to the cycloid at B must be   cid:7   22.4 Constrained variation  Just as the problem of ﬁnding thestationary values of a function f x, y  subject to the constraint g x, y  = constant is solved by means of Lagrange’s undetermined multipliers  see chapter 5 , so the corresponding problem in the calculus of variations is solved by an analogous method.  Suppose that we wish to ﬁnd the stationary values of  subject to the constraint that the value of  is held constant. Following the method of Lagrange undetermined multipliers let us deﬁne a new functional  K = I + λJ =   F + λG  dx,  and ﬁnd its unconstrained stationary values. Repeating the analysis of section 22.1 we ﬁnd that we require   cid:7    cid:8    cid:13    cid:7    cid:8  cid:14   − d  dx  ∂F ∂y  ∂F  cid:7  ∂y  + λ  − d  dx  ∂G ∂y  ∂G  cid:7  ∂y  = 0,   cid:21   cid:21   b  a  b  a  I =  F y, y  , x  dx,  J =  G y, y  , x  dx   cid:7    cid:7    cid:21   b  a  785   CALCULUS OF VARIATIONS  −a  y  O  a  x  Figure 22.7 A uniform rope with ﬁxed end-points suspended under gravity.  which, together with the original constraint J = constant, will yield the required solution y x .  This method is easily generalised to cases with more than one constraint by the introduction of more Lagrange multipliers. If we wish to ﬁnd the stationary values of an integral I subject to the multiple constraints that the values of the integrals Ji be held constant for i = 1, 2, . . . , n, then we simply ﬁnd the unconstrained stationary values of the new integral  K = I +  λiJi.  n cid:4   1   cid:21   a  −a   cid:21   I = −ρg  cid:21    cid:1 Find the shape assumed by a uniform rope when suspended by its ends from two points at equal heights.  We will solve this problem using x  see ﬁgure 22.7  as the independent variable. Let  the rope of length 2L be suspended between the points x = ±a, y = 0  L > a  and  have uniform linear density ρ. We then need to ﬁnd the stationary value of the rope’s gravitational potential energy,  y 1 + y   cid:7 2 1 2 dx,  with respect to small changes in the form of the rope but subject to the constraint that the total length of the rope remains constant, i.e.  J =  ds =   1 + y   cid:7 2 1 2 dx = 2L.  We thus deﬁne a new integral  omitting the factor −1 from I for brevity   K = I + λJ =   ρgy + λ  1 + y  and ﬁnd its stationary values. Since the integrand does not contain the independent variable x explicitly, we can use  22.8  to ﬁnd the ﬁrst integral:   ρgy + λ   1 + y   cid:7 2  1 2 −  ρgy + λ    cid:7 2  1 + y   cid:7 2 = k,  y   cid:9    cid:10    cid:7 2 1 2 dx   cid:10 −1 2   cid:9   y ds = −ρg  cid:21   cid:21   −a  a  a  −a  786   22.5 PHYSICAL VARIATIONAL PRINCIPLES  where k is a constant; this reduces to  cid:7 2 =  y  ρgy + λ   cid:8  2 − 1.  cid:8    cid:7   cid:7   k  k  −1  cosh  k ρg  ρgy + λ  = x + c,  Making the substitution ρgy + λ = k cosh z, this can be integrated easily to give  where c is the constant of integration.  We now have three unknowns, λ, k and c, that must be evaluated using the two end  conditions y ±a  = 0 and the constraint J = 2L. The end conditions give  and since a  cid:3 = 0, these imply c = 0 and λ k = cosh ρga k . Putting c = 0 into the  constraint, in which y  = sinh ρgx k , we obtain   cid:7   ρg a + c   cosh  =  = cosh  λ k   cid:22   k   cid:21   a  −a 2k ρg   cid:22    cid:9   cid:9    cid:9   cid:10   cid:10   .  2L =  1 + sinh2  ρgx  k  =  sinh  ρga  k  ,  k  ρg −a + c   cid:10  cid:23   1 2  dx   cid:9    cid:10  cid:23   ,  y x  =  cosh  k ρg  ρgx  k  − cosh  ρga  k  Collecting together the values for the constants, the form adopted by the rope is therefore  where k is the solution of sinh ρga k  = ρgL k. This curve is known as a catenary.  cid:2   22.5 Physical variational principles  Many results in both classical and quantum physics can be expressed as varia- tional principles, and it is often when expressed in this form that their physical meaning is most clearly understood. Moreover, once a physical phenomenon has been written as a variational principle, we can use all the results derived in this chapter to investigate its behaviour. It is usually possible to identify conserved quantities, or symmetries of the system of interest, that otherwise might be found only with considerable eﬀort. From the wide range of physical variational princi- ples we will select two examples from familiar areas of classical physics, namely geometric optics and mechanics.  22.5.1 Fermat’s principle in optics  Fermat’s principle in geometrical optics states that a ray of light travelling in a region of variable refractive index follows a path such that the total optical path  length  physical length × refractive index  is stationary.  787   CALCULUS OF VARIATIONS  B  x  θ2  n2  n1  y  θ1  A  Figure 22.8 Path of a light ray at the plane interface between media with refractive indices n1 and n2, where n2 < n1.   cid:1 From Fermat’s principle deduce Snell’s law of refraction at an interface.  Let the interface be at y = constant  see ﬁgure 22.8  and let it separate two regions with refractive indices n1 and n2 respectively. On a ray the element of physical path length is  cid:7 2 1 2dx, and so for a ray that passes through the points A and B, the total ds =  1 + y optical path length is   cid:21   B  A   cid:9   P =  n y  1 + y   cid:7 2 1 2 dx.   cid:10 −1 2   cid:7 2   cid:7   n y   1 + y  = k,  n cos φ = constant  Since the integrand does not contain the independent variable x explicitly, we use  22.8  to obtain a ﬁrst integral, which, after some rearrangement, reads  is the tangent of the angle φ between the where k is a constant. Recalling that y instantaneous direction of the ray and the x-axis, this general result, which is not dependent on the conﬁguration presently under consideration, can be put in the form  along a ray, even though n and φ vary individually.  cid:7   For our particular conﬁguration n is constant in each medium and therefore so is . Thus the rays travel in straight lines in each medium  as anticipated in ﬁgure 22.8, y but not assumed in our analysis , and since k is constant along the whole path we have n1 cos φ1 = n2 cos φ2, or in terms of the conventional angles in the ﬁgure  n1 sin θ1 = n2 sin θ2.  cid:2   22.5.2 Hamilton’s principle in mechanics  Consider a mechanical system whose conﬁguration can be uniquely deﬁned by a number of coordinates qi  usually distances and angles  together with time t and which experiences only forces derivable from a potential. Hamilton’s principle  788   22.5 PHYSICAL VARIATIONAL PRINCIPLES  y  O  dx  l  x  Figure 22.9 Transverse displacement on a taut string that is ﬁxed at two points a distance l apart.   cid:21   L =  t1  t0  states that in moving from one conﬁguration at time t0 to another at time t1 the motion of such a system is such as to make  L q1, q2. . . , qn, ˙q1, ˙q2, . . . , ˙qn, t  dt   22.21   stationary. The Lagrangian L is deﬁned, in terms of the kinetic energy T and  the potential energy V  with respect to some reference situation , by L = T − V . Here V is a function of the qi only, not of the ˙qi. Applying the EL equation to L  we obtain Lagrange’s equations,   cid:7    cid:8   ∂L ∂qi  =  d dt  ∂L ∂˙qi  ,  i = 1, 2, . . . , n.   cid:1 Using Hamilton’s principle derive the wave equation for small transverse oscillations of a taut string.  In this example we are in fact considering a generalisation of  22.21  to a case involving one isolated independent coordinate t, together with a continuum in which the qi become the continuous variable x. The expressions for T and V therefore become integrals over x rather than sums over the label i.  If ρ and τ are the local density and tension of the string, both of which may depend on x, then, referring to ﬁgure 22.9, the kinetic and potential energies of the string are given by  and  22.21  becomes   cid:21   l  τ 2  0   cid:7    cid:7    cid:8   cid:17   ∂y ∂x   cid:8   2  2  dx  T =   cid:21   l  0   cid:7   ∂y ∂t  ρ 2   cid:21    cid:8   2   cid:21   l  0  dx,  V =   cid:16    cid:7    cid:8   789  L =  1 2  t1  t0  dt  ρ  2 − τ  ∂y ∂t  ∂y ∂x  dx.   Using  22.13  and the fact that y does not appear explicitly, we obtain  If, in addition, ρ and τ do not depend on x or t then  CALCULUS OF VARIATIONS   cid:7    cid:8    cid:7    cid:8   ∂ ∂t  ρ  ∂y ∂t  − ∂  ∂x  τ  ∂y ∂x  = 0.  ∂2y ∂x2  =  1 c2  ∂2y ∂t2 ,  where c2 = τ ρ. This is the wave equation for small transverse oscillations of a taut uniform string.  cid:2   22.6 General eigenvalue problems  We have seen in this chapter that the problem of ﬁnding a curve that makes the value of a given integral stationary when the integral is taken along the curve results, in each case, in a diﬀerential equation for the curve. It is not a great extension to ask whether this may be used to solve diﬀerential equations, by setting up a suitable variational problem and then seeking ways other than the Euler equation of ﬁnding or estimating stationary solutions. where the diﬀerential operator L is self-adjoint, so that L = L†  with appropriate boundary conditions on the solution y  and ρ x  is some weight function, as discussed in chapter 17. In particular, we will concentrate on the Sturm–Liouville equation as an explicit example, but much of what follows can be applied to other equations of this type.  We shall be concerned with diﬀerential equations of the form Ly = λρ x y,  We have already discussed the solution of equations of the Sturm–Liouville type in chapter 17 and the same notation will be used here. In this section, however, we will adopt a variational approach to estimating the eigenvalues of such equations.  Suppose we search for stationary values of the integral   cid:23   cid:7 2 x  − q x y2 x   dx,  I =  p x y   22.22   with y a  = y b  = 0 and p and q any suﬃciently smooth and diﬀerentiable functions of x. However, in addition we impose a normalisation condition   cid:21    cid:22   a  b   cid:21   b  J =  ρ x y2 x  dx = constant.   22.23   Here ρ x  is a positive weight function deﬁned in the interval a ≤ x ≤ b, but  a  which may in particular cases be a constant.  Then, as in section 22.4, we use undetermined Lagrange multipliers,  and  §  §  We use −λ, rather than λ, so that the ﬁnal equation  22.24  appears in the conventional Sturm–  Liouville form.  790   22.6 GENERAL EIGENVALUE PROBLEMS   cid:21  consider K = I − λJ given by   cid:22   b   cid:23   dx.  K =  py   cid:7 2 −  q + λρ y2  cid:8   a   cid:7   d dx  p  dy dx  On application of the EL equation  22.5  this yields  + qy + λρy = 0,   22.24   which is exactly the Sturm–Liouville equation  17.34 , with eigenvalue λ. Now, since both I and J are quadratic in y and its derivative, ﬁnding stationary values of K is equivalent to ﬁnding stationary values of I J. This may also be shown by considering the functional Λ = I J, for which  δΛ =  δI J  −  I J 2  δJ  =  δI − ΛδJ  J  = δK J.  Hence, extremising Λ is equivalent to extremising K. Thus we have the important result that ﬁnding functions y that make I J stationary is equivalent to ﬁnding functions y that are solutions of the Sturm–Liouville equation; the resulting value of I J equals the corresponding eigenvalue of the equation.  Of course this does not tell us how to ﬁnd such a function y and, naturally, to have to do this by solving  22.24  directly defeats the purpose of the exercise. We will see in the next section how some progress can be made. It is worth recalling that the functions p x , q x  and ρ x  can have many diﬀerent forms, and so  22.24  represents quite a wide variety of equations.  We now recall some properties of the solutions of the Sturm–Liouville equation. The eigenvalues λi of  22.24  are real and will be assumed non-degenerate  for simplicity . We also assume that the corresponding eigenfunctions have been made real, so that normalised eigenfunctions yi x  satisfy the orthogonality relation  as in  17.24    yiyjρ dx = δij .   22.25   Further, we take the boundary condition in the form  yipy j  = 0;  x=b  x=a   22.26   this can be satisﬁed by y a  = y b  = 0, but also by many other sets of boundary conditions.   cid:21   b  a   cid:22    cid:23    cid:7   791   CALCULUS OF VARIATIONS   cid:1 Show that   cid:7  jpy   cid:7   i  y  − yjqyi  dx = λiδij.   22.27    cid:21   a  b   cid:5   cid:5   cid:21   a   cid:6  cid:7    cid:7   i  py   cid:13    cid:5   yj  py   cid:6  cid:14    cid:7   i  −  b  a   cid:6    cid:21   b  a  Let yi be an eigenfunction of  22.24 , corresponding to a particular eigenvalue λi, so that  +  q + λiρ yi = 0.  Multiplying this through by yj and integrating from a to b  the ﬁrst term by parts  we obtain  b   cid:7  j py   cid:7  i  dx +  y  yj q + λiρ yi dx = 0.   22.28   The ﬁrst term vanishes by virtue of  22.26 , and on rearranging the other terms and using  22.25 , we ﬁnd the result  22.27 .  cid:2   We see at once that, if the function y x  minimises I J, i.e. satisﬁes the Sturm– Liouville equation, then putting yi = yj = y in  22.25  and  22.27  yields J and I respectively on the left-hand sides; thus, as mentioned above, the minimised value of I J is just the eigenvalue λ, introduced originally as the undetermined multiplier.   cid:1 For a function y satisfying the Sturm–Liouville equation verify that, provided  22.26  is satisﬁed, λ = I J.  Firstly, we multiply  22.24  through by y to give  Now integrating this expression by parts we have   cid:7    cid:7   y py     + qy2 + λρy2 = 0.   cid:13    cid:14    cid:7    cid:21    cid:9   ypy  −  b  a  b  a   cid:7 2 − qy2  py   cid:10    cid:21   b  a  dx + λ  ρy2 dx = 0.  The ﬁrst term on the LHS is zero, the second is simply −I and the third is λJ. Thus λ = I J.  cid:2   22.7 Estimation of eigenvalues and eigenfunctions  Since the eigenvalues λi of the Sturm–Liouville equation are the stationary values of I J  see above , it follows that any evaluation of I J must yield a value that lies between the lowest and highest eigenvalues of the corresponding Sturm–Liouville equation, i.e.  where, depending on the equation under consideration, either λmin = −∞ and  λmin ≤ I  ≤ λmax,  J  792   22.7 ESTIMATION OF EIGENVALUES AND EIGENFUNCTIONS  λmax is ﬁnite, or λmax = ∞ and λmin is ﬁnite. Notice that here we have departed  from direct consideration of the minimising problem and made a statement about a calculation in which no actual minimisation is necessary.  Thus, as an example, for an equation with a ﬁnite lowest eigenvalue λ0 any evaluation of I J provides an upper bound on λ0. Further, we will now show that the estimate λ obtained is a better estimate of λ0 than the estimated  guessed  function y is of y0, the true eigenfunction corresponding to λ0. The sense in which ‘better’ is used here will be clear from the ﬁnal result.  Firstly, we expand the estimated or trial function y in terms of the complete  set yi:  y = y0 + c1y1 + c2y2 + ··· ,   cid:11   where, if a good trial function has been guessed, the ci will be small. Using  22.25  we have immediately that J = 1 +   cid:16    cid:7    cid:21   b  a   cid:7  0 + y  p   cid:4   ciy i  i  I =  i   cid:7   ci2. The other required integral is  cid:8  2 − q   cid:4    cid:8    cid:17   y0 +  dx.  ciyi  2   cid:7   i  On multiplying out the squared terms, all the cross terms vanish because of  22.27  to leave  λ =  =  I J λ0 + 1 +  = λ0 +   cid:11   cid:11   cid:4   i  j  i  ci2λi cj2 ci2 λi − λ0  + O c4 .  Hence λ diﬀers from λ0 by a term second order in the ci, even though y diﬀered from y0 by a term ﬁrst order in the ci; this is what we aimed to show. We notice  incidentally that, since λ0 < λi for all i, λ is shown to be necessarily ≥ λ0, with equality only if all ci = 0, i.e. if y ≡ y0.  The method can be extended to the second and higher eigenvalues by imposing, in addition to the original constraints and boundary conditions, a restriction of the trial functions to only those that are orthogonal to the eigenfunctions corresponding to lower eigenvalues.  Of course, this requires complete or nearly complete knowledge of these latter eigenfunctions.  An example is given at the end of the chapter  exercise 22.25 .  We now illustrate the method we have discussed by considering a simple  example, one for which, as on previous occasions, the answer is obvious.  793   CALCULUS OF VARIATIONS   c    b    a    d   y x   1  0.8  0.6  0.4  0.2  0.2  0.4  0.6  0.8  x  1   cid:7  cid:7   Figure 22.10 Trial solutions used to estimate the lowest eigenvalue λ of  1  = 0. They are:  a  y = sin πx 2 , the exact result;  −y  b  y = 2x − x2;  c  y = x3 − 3x2 + 3x;  d  y = sin2 πx 2 .  = λy with y 0  = y   cid:7    cid:1 Estimate the lowest eigenvalue of the equation  with boundary conditions  − d2y dx2  = λy,  0 ≤ x ≤ 1,  y 0  = 0,  y   1  = 0.   cid:7    22.29    22.30   We need to ﬁnd the lowest value λ0 of λ for which  22.29  has a solution y x  that satisﬁes   22.30 . The exact answer is of course y = A sin xπ 2  and λ0 = π2 4 ≈ 2.47.  Firstly we note that the Sturm–Liouville equation reduces to  22.29  if we take p x  = 1, q x  = 0 and ρ x  = 1 and that the boundary conditions satisfy  22.26 . Thus we are able to apply the previous theory.  We will use three trial functions so that the eﬀect on the estimate of λ0 of making better or worse ‘guesses’ can be seen. One further preliminary remark is relevant, namely that the estimate is independent of any constant multiplicative factor in the function used. This is easily veriﬁed by looking at the form of I J. We normalise each trial function so that y 1  = 1, purely in order to facilitate comparison of the various function shapes.  Figure 22.10 illustrates the trial functions used, curve  a  being the exact solution  y = sin πx 2 . The other curves are  b  y x  = 2x − x2,  c  y x  = x3 − 3x2 + 3x, and  d  y x  = sin2 πx 2 . The choice of trial function is governed by the following considerations:   i  the boundary conditions  22.30  must be satisﬁed.  ii  a ‘good’ trial function ought to mimic the correct solution as far as possible, but it may not be easy to guess even the general shape of the correct solution in some cases.   iii  the evaluation of I J should be as simple as possible.  794   It is easily veriﬁed that functions  b ,  c  and  d  all satisfy  22.30  but, so far as mimicking the correct solution is concerned, we would expect from the ﬁgure that  b  would be superior to the other two. The three evaluations are straightforward, using  22.22  and  22.23 :  22.8 ADJUSTMENT OF PARAMETERS  1  1   cid:1   cid:1  0  2 − 2x 2 dx  cid:1  0  2x − x2 2 dx  cid:1  0  3x2 − 6x + 3 2 dx  cid:1  0  x3 − 3x2 + 3x 2 dx  cid:1  0  π2 4  sin2 πx  dx  =  1  1  1  1  0 sin4 πx 2  dx  λb =  λc =  λd =  4 3 8 15  = 2.50  =  9 5 9 14  = 2.80  =  π2 8 3 8  = 3.29.  We expected all evaluations to yield estimates greater than the lowest eigenvalue, 2.47,  and this is indeed so. From these trials alone we are able to say  only  that λ0 ≤ 2.50. As expected, the best approximation  b  to the true eigenfunction yields the lowest, and therefore the best, upper bound on λ0.  cid:2   We may generalise the work of this section to other diﬀerential equations of  the form Ly = λρy, where L = L†  . In particular, one ﬁnds  λmin ≤ I  ≤ λmax,  J  where I and J are now given by  Ly  dx  I =  ∗  y  b   cid:21   a   cid:21   b  a  ∗  and  J =  ρy  y dx.   22.31   It is straightforward to show that, for the special case of the Sturm–Liouville equation, for which  Ly = − py   cid:7    cid:7  − qy,     the expression for I in  22.31  leads to  22.22 .  22.8 Adjustment of parameters  Instead of trying to estimate λ0 by selecting a large number of diﬀerent trial functions, we may also use trial functions that include one or more parameters which themselves may be adjusted to give the lowest value to λ = I J and hence the best estimate of λ0. The justiﬁcation for this method comes from the knowledge that no matter what form of function is chosen, nor what values are assigned to the parameters, provided the boundary conditions are satisﬁed λ can never be less than the required λ0.  To illustrate this method an example from quantum mechanics will be used. The time-independent Schr¨odinger equation is formally written as the eigenvalue equation Hψ = Eψ, where H is a linear operator, ψ the wavefunction describing a quantum mechanical system and E the energy of the system. The energy  795   CALCULUS OF VARIATIONS  operator H is called the Hamiltonian and for a particle of mass m moving in a one-dimensional harmonic oscillator potential is given by  H = −  cid:1 2  2m  d2 dx2 +  kx2 2  ,   22.32   where  cid:1  is Planck’s constant divided by 2π.   cid:1 Estimate the ground-state energy of a quantum harmonic oscillator.  Using  22.32  in Hψ = Eψ, the Schr¨odinger equation is  −  cid:1 2 2m  d2ψ dx2  kx2 2  +  ψ = Eψ,  −∞ < x < ∞.   22.33   The boundary conditions are that ψ should vanish as x → ±∞. Equation  22.33  is a form of the Sturm–Liouville equation in which p =  cid:1 2  2m , q = −kx2 2, ρ = 1 and λ = E; it  can be solved by the methods developed previously, e.g. by writing the eigenfunction ψ as a power series in x.  However, our purpose here is to illustrate variational methods and so we take as a trial  wavefunction ψ = exp −αx2 , where α is a positive parameter whose value we will choose later. This function certainly → 0 as x → ±∞ and is convenient for calculations. Whether  it approximates the true wave function is unknown, but if it does not our estimate will still be valid, although the upper bound will be a poor one.  = −2αx exp −αx2 , the required estimate is  With y = exp −αx2  and therefore y  cid:1  ∞  E = λ =   cid:7    cid:1  ∞  −∞[  cid:1 2 2m 4α2x2 +  k 2 x2]e  −2αx2 dx  −∞ e  dx  =   cid:1 2α 2m  +  k 8α  .  −2αx2  This evaluation is easily carried out using the reduction formula  n − 1  4α  In−2,  In =  for integrals of the form In =   cid:21  ∞  −2αx2  dx.  −∞ xne  So, we have obtained the estimate  22.34 , involving the parameter α, for the oscillator’s ground-state energy, i.e. the lowest eigenvalue of H. In line with our previous discussion we now minimise λ with respect to α. Putting dλ dα = 0  clearly a minimum , yields α =  km 1 2  2 cid:1  , which in turn gives as the minimum value for λ   22.34    22.35    22.36    cid:7    cid:8   E =   cid:1   2  k m  1 2  =   cid:1 ω 2  ,  where we have put  k m 1 2 equal to the classical angular frequency ω.  The method thus leads to the conclusion that the ground-state energy E0 is ≤ 1   cid:1 ω. In fact, as is well known, the equality sign holds, 1  cid:1 ω being just the zero-point energy 2 exp −αx2  is the correct functional form for the ground state wavefunction and the of a quantum mechanical oscillator. Our estimate gives the exact value because ψ x  = particular value of α that we have found is that needed to make ψ an eigenfunction of H with eigenvalue 1 2   cid:1 ω.  cid:2   2  An alternative but equivalent approach to this is developed in the exercises that follow, as is an extension of this particular problem to estimating the second- lowest eigenvalue  see exercise 22.25 .  796   22.9 EXERCISES  22.9 Exercises   cid:1   22.1  ρ z , is bounded by the circles ρ = a, z = ±c  a > c . Show that the function A surface of revolution, whose equation in cylindrical polar coordinates is ρ = −1 2 dS stationary with respect to small variations is given by ρ z  = k + z2  4k , where k = [a ±  a2 − c2 1 2] 2.  that makes the surface integral I =  ρ  22.2  Show that the lowest value of the integral  where A is  −1, 1  and B is  1, 1 , is 2 ln 1 +  B  A   1 + y y  dx,   cid:7 2 1 2 √ 2 . Assume that the Euler–Lagrange   cid:21   22.3  equation gives a minimising curve. The refractive index n of a medium is a function only of the distance r from a ﬁxed point O. Prove that the equation of a light ray, assumed to lie in a plane through O, travelling in the medium satisﬁes  in plane polar coordinates    cid:7    cid:8   2  1 r2  dr dφ  =  r2 a2  n2 r  n2 a   − 1,  22.4  22.5  where a is the distance of the ray from O at the point at which dr dφ = 0.  If n = [1 +  α2 r2 ]1 2 and the ray starts and ends far from O, ﬁnd its deviation  the angle through which the ray is turned , if its minimum distance from O is a. The Lagrangian for a π-meson is given by  L x, t  = 1  2  ˙φ2 − ∇φ2 − µ2φ2 ,  where µ is the meson mass and φ x, t  is its wavefunction. Assuming Hamilton’s principle, ﬁnd the wave equation satisﬁed by φ. Prove the following results about general systems.   a  For a system described in terms of coordinates qi and t, show that if t does not appear explicitly in the expressions for x, y and z  x = x qi, t , etc.  then the kinetic energy T is a homogeneous quadratic function of the ˙qi  it may also involve the qi . Deduce that  i ˙qi ∂T  ∂˙qi  = 2T .   b  Assuming that the forces acting on the system are derivable from a potential  V , show, by expressing dT  dt in terms of qi and ˙qi, that d T + V   dt = 0.   cid:11   22.6  For a system speciﬁed by the coordinates q and t, show that the equation of motion is unchanged if the Lagrangian L q, ˙q, t  is replaced by  L1 = L +  dφ q, t   ,  dt  where φ is an arbitrary function. Deduce that the equation of motion of a particle  that moves in one dimension subject to a force −dV  x  dx  x being measured  from a point O  is unchanged if O is forced to move with a constant velocity v  x still being measured from O . In cylindrical polar coordinates, the curve  ρ θ , θ, αρ θ   lies on the surface of the cone z = αρ. Show that geodesics  curves of minimum length joining two points  on the cone satisfy  22.7  ρ4 = c2[β2ρ   cid:7 2 + ρ2],  between the points  R,−θ0, αR  and  R, θ0, αR .  where c is an arbitrary constant, but β has to have a particular value. Determine the form of ρ θ  and hence ﬁnd the equation of the shortest path on the cone −1 . ]  [ You will ﬁnd it useful to determine the form of the derivative of cos  −1 u  797   22.8  22.9  22.10  22.11  22.12  22.13  22.14  22.15  CALCULUS OF VARIATIONS  Derive the diﬀerential equations for the plane-polar coordinates, r and φ, of a particle of unit mass moving in a ﬁeld of potential V  r . Find the form of V if the path of the particle is given by r = a sin φ. You are provided with a line of length πa 2 and negligible mass and some lead shot of total mass M. Use a variational method to determine how the lead shot must be distributed along the line if the loaded line is to hang in a circular arc of radius a when its ends are attached to two points at the same height. Measure the distance s along the line from its centre. Extend the result of subsection 22.2.2 to the case of several dependent variables yi x , showing that, if x does not appear explicitly in the integrand, then a ﬁrst integral of the Euler–Lagrange equations is  F − n cid:4   i=1   cid:7   i  y  ∂F  cid:7  ∂y  i  = constant.  A general result is that light travels through a variable medium by a path which minimises the travel time  this is an alternative formulation of Fermat’s principle . With respect to a particular cylindrical polar coordinate system  ρ, φ, z , the speed of light v ρ, φ  is independent of z. If the path of the light is parameterised as ρ = ρ z , φ = φ z , use the result of the previous exercise to show that   cid:7 2 + ρ2φ   cid:7 2 + 1   v2 ρ  is constant along the path.  For the particular case when v = v ρ  = b a2 + ρ2 1 2, show that the two Euler– Lagrange equations have a common solution in which the light travels along a helical path given by φ = Az + B, ρ = C, provided that A has a particular value. Light travels in the vertical xz-plane through a slab of material which lies between the planes z = z0 and z = 2z0, and in which the speed of light v z  = c0z z0. Using the alternative formulation of Fermat’s principle, given in the previous question, show that the ray paths are arcs of circles.  Deduce that, if a ray enters the material at  0, z0  at an angle to the vertical,  π 2 − θ, of more than 30 A dam of capacity V  less than πb2h 2  is to be constructed on level ground next to a long straight wall which runs from  −b, 0  to  b, 0 . This is to be achieved by  , then it does not reach the far side of the slab.  ◦  joining the ends of a new wall, of height h, to those of the existing wall. Show that, in order to minimise the length L of new wall to be built, it should form part of a circle, and that L is then given by   cid:21   where λ is found from  b  −b  dx   1 − λ2x2 1 2 ,  sin  −1 µ µ2  V hb2  =  −  1 − µ2 1 2  µ  and µ = λb. In the brachistochrone problem of subsection 22.3.4 show that if the upper end- point can lie anywhere on the curve h x, y  = 0, then the curve of quickest descent y x  meets h x, y  = 0 at right angles. The Schwarzchild metric for the static ﬁeld of a non-rotating spherically sym- metric black hole of mass M is given by   ds 2 = c2  1 − 2GM  c2r   dt 2 −   dr 2  1 − 2GM  c2r   − r2  dθ 2 − r2 sin2 θ  dφ 2.   cid:7    cid:8   Considering only motion conﬁned to the plane θ = π 2, and assuming that the  798   22.9 EXERCISES   cid:1    cid:21    cid:21   −1  1  0  1   1 − x2 P   cid:7  m x P   cid:7  n x  dx,  J =  J =  [x4 y   2 + 4x2 y   2] dx   cid:7  cid:7    cid:7   path of a small test particle is such as to make ds stationary, ﬁnd two ﬁrst integrals of the equations of motion. From their Newtonian limits, in which  GM r, ˙r2 and r2 ˙φ2 are all  cid:16  c2, identify the constants of integration.  22.16  Use result  22.27  to evaluate  22.17  where Pm x  is a Legendre polynomial of order m. Determine the minimum value that the integral  can have, given that y is not singular at x = 0 and that y 1  = y  1  = 1. Assume that the Euler–Lagrange equation gives the lower limit, and verify retrospectively that your solution makes the ﬁrst term on the LHS of equation  22.15  vanish. Show that y   cid:7  cid:7  − xy + λx2y = 0 has a solution for which y 0  = y 1  = 0 and   cid:7   Find an appropriate, but simple, trial function and use it to estimate the lowest eigenvalue λ0 of Stokes’ equation,  22.18  22.19  λ ≤ 147 4.  + λxy = 0,  with y 0  = y π  = 0.  22.20  Explain why your estimate must be strictly greater than λ0. Estimate the lowest eigenvalue, λ0, of the equation  − x2y + λy = 0,  y −1  = y 1  = 0,  d2y dx2  d2y dx2  22.21  using a quadratic trial function. A drumskin is stretched across a ﬁxed circular rim of radius a. Small transverse vibrations of the skin have an amplitude z ρ, φ, t  that satisﬁes  ∇2z =  1 c2  ∂2z ∂t2  in plane polar coordinates. For a normal mode independent of azimuth, z = Z  ρ  cos ωt, ﬁnd the diﬀerential equation satisﬁed by Z  ρ . By using a trial  function of the form aν − ρν, with adjustable parameter ν, obtain an estimate for  the lowest normal mode frequency.  [ The exact answer is  5.78 1 2c a. ]  22.22  Consider the problem of ﬁnding the lowest eigenvalue, λ0, of the equation   1 + x2   + 2x  + λy = 0,  d2y dx2  dy dx  y ±1  = 0.   a  Recast the problem in variational form, and derive an approximation λ1 to  λ0 by using the trial function y1 x  = 1 − x2.   b  Show that an improved estimate λ2 is obtained by using y2 x  = cos πx 2 .  c  Prove that the estimate λ γ  obtained by taking y1 x  + γy2 x  as the trial  function is  64 15 + 64γ π − 384γ π3 +  π2 3 + 1 2 γ2  16 15 + 64γ π3 + γ2  .  λ γ  =  Investigate λ γ  numerically as γ is varied, or, more simply, show that  λ −1.80  = 3.668, an improvement on both λ1 and λ2.  799   CALCULUS OF VARIATIONS  22.23  For the boundary conditions given below, obtain a functional Λ y  whose sta- tionary values give the eigenvalues of the equation   1 + x   +  2 + x   + λy = 0,  y 0  = 0, y   2  = 0.  d2y dx2  dy dx   cid:7   Derive an approximation to the lowest eigenvalue λ0 using the trial function y x  = xe  −x 2. For what value s  of γ would  y x  = xe  −x 2 + β sin γx  22.24   cid:1   be a suitable trial function for attempting to obtain an improved estimate of λ0? This is an alternative approach to the example in section 22.8. Using the notation of that section, the expectation value of the energy of the state ψ is given by Hψ dv. Denote the eigenfunctions of H by ψi, so that Hψi = Eiψi, and, since  ψ  ∗   cid:1   H is self-adjoint  Hermitian ,  ∗ j ψi dv = δij .  ψ   a  By writing any function ψ as that in section 22.7, show that  cj ψj and following an argument similar to   cid:11   cid:1   cid:1   E =  ∗  ψ  Hψ dv ∗  ψ dv  ψ  ≥ E0,  22.25  22.26  the energy of the lowest state. This is the Rayleigh–Ritz principle.   b  Using the same trial function as in section 22.8, ψ = exp −αx2 , show that  the same result is obtained.  This is an extension to section 22.8 and the previous question. With the ground-  state  i.e. the lowest-energy  wavefunction as exp −αx2 , take as a trial function the orthogonal wave function x2n+1 exp −αx2 , using the integer n as a variable  parameter. Use either Sturm–Liouville theory or the Rayleigh–Ritz principle to show that the energy of the second lowest state of a quantum harmonic oscillator  is ≤ 3 cid:1 ω 2.  The Hamiltonian H for the hydrogen atom is ∇2 − q2 4π cid:4 0r  −  cid:1 2 2m  .   cid:21  ∞  For a spherically symmetric state, as may be assumed for the ground state, the  only relevant part of ∇2 is that involving diﬀerentiation with respect to r.   a  Deﬁne the integrals Jn by  −2βr dr   cid:1  and show that, for a trial wavefunction of the form exp −βr  with β > 0, ψ dv  see exercise 22.24 a   can be expressed as aJ1− bJ2  Hψ dv and  Jn =   cid:1   rne  ψ  ψ  ∗  ∗  0  and cJ2 respectively, where a, b and c are factors which you should determine.   b  Show that the estimate of E is minimised when β = mq2  4π cid:4 0 cid:1 2 .  c  Hence ﬁnd an upper limit for the ground-state energy of the hydrogen atom.  In fact, exp −βr  is the correct form for the wavefunction and the limit gives  the actual value.  22.27  The upper and lower surfaces of a ﬁlm of liquid, which has surface energy per unit area  surface tension  γ and density ρ, have equations z = p x  and z = q x , respectively. The ﬁlm has a given volume V  per unit depth in the y-direction   and lies in the region −L < x < L, with p 0  = q 0  = p L  = q L  = 0. The  800   22.10 HINTS AND ANSWERS  total energy  per unit depth  of the ﬁlm consists of its surface energy and its gravitational energy, and is expressed by  E = ρg 2   p2 − q2  dx + γ   cid:7 2 1 2 +  1 + q   cid:7 2 1 2   1 + p  dx.   cid:23    cid:22    cid:21   L  −L   cid:21   L  −L   a  Express V in terms of p and q.  b  Show that, if the total energy is minimised, p and q must satisfy   cid:7 2  cid:7 2 1 2  p  1 + p  −   cid:7 2  cid:7 2 1 2  q  1 + q  = constant.   c  As an approximate solution, consider the equations  p = a L − x ,  q = b L − x ,  where a and b are suﬃciently small that a3 and b3 can be neglected compared with unity. Find the values of a and b that minimise E.  22.28  A particle of mass m moves in a one-dimensional potential well of the form  V  x  = −µ  cid:1    cid:1 2α2 m  sech 2αx,  where µ and α are positive constants. As in exercise 22.26, the expectation value Hψ dx, where the self-adjoint operator   cid:20 E cid:21  of the energy of the system is H is given by −  cid:1 2 2m d2 dx2 + V  x . Using trial wavefunctions of the form for µ = 1, there is an exact eigenfunction of H, with a corresponding  cid:20 E cid:21  of  y = A sech βx, show the following:   a   ψ  ∗  half of the maximum depth of the well;   b  for µ = 6, the ‘binding energy’ of the ground state is at least 10 cid:1 2α2  3m . [ You will ﬁnd it useful to note that for u, v ≥ 0, sech u sech v ≥ sech  u + v . ]  The Sturm–Liouville equation can be extended to two independent variables, x and z, with little modiﬁcation. In equation  22.22 , y the integrals of the various functions of y x, z  become two-dimensional, i.e. the inﬁnitesimal is dx dz.   cid:7 2 is replaced by  ∇y 2 and  The vibrations of a trampoline 4 units long and 1 unit wide satisfy the equation  ∇2y + k2y = 0.  By taking the simplest possible permissible polynomial as a trial function, show  that the lowest mode of vibration has k2 ≤ 10.63 and, by direct solution, that the  actual value is 10.49.  22.29  22.10 Hints and answers   cid:1  Note that the integrand, 2πρ1 2 1 + ρ If β =  π − deviation angle  2 then β = φ at r = a, and the equation reduces to I =   cid:7 2 1 2, does not contain z explicitly. n r [r2 +  dr dφ 2]1 2 dφ. Take axes such that φ = 0 when r = ∞.  22.1 22.3  β  r r2 − a2 1 2 , y = exp ψ to yield a deviation of π[ a2 + α2 1 2 − a] a.  which can be evaluated by putting r = a y + y   a2 + α2 1 2  −∞  =  dr  −1  2, or successively r = a cosh ψ,   cid:21  ∞  801   22.5   a  ∂x ∂t = 0 and so ˙x =  i ˙qi∂x ∂qi;  b  use  CALCULUS OF VARIATIONS   cid:11    cid:7    cid:8    cid:4   i  ˙qi  d dt  ∂T ∂˙qi  =  d dt  ¨qi  ∂T ∂˙qi  .   2T   − cid:4   cid:1   i  22.7  22.9  22.11 22.13  22.15  22.17  22.19  22.21  22.23  22.25  22.27  22.29   cid:7    cid:7 2   −1 is a   1 − y  −1 2 = 2gP  s , y = y s , P  s  =  multivalued function; ρ θ  = [R cos θ0 β ] [cos θ β ]. s 0 ρ s  Use result  22.8 ; β2 = 1 + α2. Put ρ = uc to obtain dθ du = β [u u2 − 1 1 2]. Remember that cos −λy −a cos s a , and 2P  πa 4  = M together give λ = −gM. The required ρ s  is given by [M  2a ] sec2 s a . Note that the φ E–L equation is automatically satisﬁed if v  cid:3 = v φ . A = 1 a. Circle is λ2x2 + [λy +  1 − λ2b2 1 2]2 = 1. Use the fact that determine the condition on λ. Denoting  ds 2  dt 2 by f2, the Euler–Lagrange equation for φ gives r2 ˙φ = Af, of exercise 22.10 to obtain c2 −  2GM r  = Bf, where, to ﬁrst order in small where A corresponds to the angular momentum of the particle. Use the result  . The solution, y =  y dx = V  h to   cid:1     ds   cid:7    cid:7   quantities,  cB = c2 − GM  r  1 2  +   ˙r2 + r2 ˙φ2 ,   cid:7    cid:7    cid:7    cid:7    cid:7    cid:7    cid:7  cid:7   −1]  + 4xu  = 2x4u  = 0 at x = 0.   1  is ﬁxed, and ∂F ∂u  +  ω c 2Z = 0, with Z  a  = 0 and Z  −1, which minimises to c2 2 +   cid:7  − 4u = 0 with general solution Ax  > λ0 since the trial function does not satisfy the original equation. Z  which reads ‘total energy = rest mass + gravitational energy + radial and azimuthal kinetic energy’.  x  = u x , and obtain Convert the equation to the usual form, by writing y −4 + Bx. Integrating a second time x2u and using the boundary conditions gives y x  =  1 + x2  2 and J = 1; η 1  = 0, Using y = sin x as a trial function shows that λ0 ≤ 2 π. The estimate must be since y  cid:7  cid:7  with p = ρ, q = 0 and weight function ρ c2. Estimate of ω2 = [c2ν  2a2 ][0.5 − 2 2  2a2  = 5.83c2 a2 when  −1Z + ρ −1 +  2ν + 2  √  cid:1  2 ν + 2  2. ν = Note that the original equation is not self-adjoint; it needs an integrating factor of ex. Λ y  = [  2  must equal 0, γ =  π 2  n + 1 E1 ≤   cid:1 ω 2  8n2 + 12n + 3   4n + 1 , which has a minimum value 3 cid:1 ω 2 when L−L  p − q  dx.  c  Use V =  a − b L2 to eliminate b from the expression  integer n = 0.  a  V = for E; now the minimisation is with respect to a alone. The values for a and b  are ±V   2L2  − V ρg  6γ . Use u x, z  = x 4 − x z 1− z  as a trial function; numerator = 1088 90, denomi- nator = 512 450. Direct solution k2 = 17π2 16.   cid:7 2 dx] [ 0  1 + x exy 2   for some integer n.   0  = 0; this is an SL equation √  0 exy2 dx; λ0 ≤ 3 8. Since y  The SL equation has p = 1, q = 0, and ρ = 1.   cid:1    cid:1    cid:7   2  2  802   23  Integral equations  It is not unusual in the analysis of a physical system to encounter an equation in which an unknown but required function y x , say, appears under an integral sign. Such an equation is called an integral equation, and in this chapter we discuss several methods for solving the more straightforward examples of such equations. Before embarking on our discussion of methods for solving various integral equations, we begin with a warning that many of the integral equations met in practice cannot be solved by the elementary methods presented here but must instead be solved numerically, usually on a computer. Nevertheless, the regular occurrence of several simple types of integral equation that may be solved analytically is suﬃcient reason to explore these equations more fully.  We shall begin this chapter by discussing how a diﬀerential equation can be transformed into an integral equation and by considering the most common types of linear integral equation. After introducing the operator notation and considering the existence of solutions for various types of equation, we go on to discuss elementary methods of obtaining closed-form solutions of simple integral equations. We then consider the solution of integral equations in terms of inﬁnite series and conclude by discussing the properties of integral equations with Hermitian kernels, i.e. those in which the integrands have particular symmetry properties.  23.1 Obtaining an integral equation from a diﬀerential equation  Integral equations occur in many situations, partly because we may always rewrite a diﬀerential equation as an integral equation. It is sometimes advantageous to make this transformation, since questions concerning the existence of a solu- tion are more easily answered for integral equations  see section 23.3 , and, furthermore, an integral equation can incorporate automatically any boundary conditions on the solution.  803   INTEGRAL EQUATIONS   cid:21   cid:21   x  0  u  x  du  0  0   cid:7    cid:21    cid:21   cid:21   x  0  x  0  We shall illustrate the principles involved by considering the diﬀerential equa-  tion   cid:7  cid:7   y   x  = f x, y ,   23.1    cid:7   where f x, y  can be any function of x and y but not of y  x . Equation  23.1  thus represents a large class of linear and non-linear second-order diﬀerential equations.  We can convert  23.1  into the corresponding integral equation by ﬁrst inte-  grating with respect to x to obtain  y   x  =  f z, y z   dz + c1.  Integrating once more, we ﬁnd  y x  =  f z, y z   dz + c1x + c2.  Provided we do not change the region in the uz-plane over which the double integral is taken, we can reverse the order of the two integrations. Changing the integration limits appropriately, we ﬁnd  y x  =  f z, y z   dz  du + c1x + c2   x − z f z, y z   dz + c1x + c2;  =   23.2    23.3   this is a non-linear  for general f x, y   Volterra integral equation.  It is straightforward to incorporate any boundary conditions on the solution y x  by ﬁxing the constants c1 and c2 in  23.3 . For example, we might have the  0  = b, for which it is clear that one-point boundary condition y 0  = a and y we must set c1 = b and c2 = a.   cid:7   23.2 Types of integral equation  From  23.3 , we can see that even a relatively simple diﬀerential equation such as  23.1  can lead to a corresponding integral equation that is non-linear. In this chapter, however, we will restrict our attention to linear integral equations, which have the general form  g x y x  = f x  + λ  K x, z y z  dz.   23.4   In  23.4 , y x  is the unknown function, while the functions f x , g x  and K x, z  are assumed known. K x, z  is called the kernel of the integral equation. The integration limits a and b are also assumed known, and may be constants or functions of x, and λ is a known constant or parameter.   cid:21   x  z   cid:21   b  a  804   23.3 OPERATOR NOTATION AND THE EXISTENCE OF SOLUTIONS  In fact, we shall be concerned with various special cases of  23.4 , which are known by particular names. Firstly, if g x  = 0 then the unknown function y x  appears only under the integral sign, and  23.4  is called a linear integral equation of the ﬁrst kind. Alternatively, if g x  = 1, so that y x  appears twice, once inside the integral and once outside, then  23.4  is called a linear integral equation of the second kind. In either case, if f x  = 0 the equation is called homogeneous, otherwise inhomogeneous.  We can distinguish further between diﬀerent types of integral equation by the form of the integration limits a and b. If these limits are ﬁxed constants then the equation is called a Fredholm equation. If, however, the upper limit b = x  i.e. it is variable  then the equation is called a Volterra equation; such an equation is analogous to one with ﬁxed limits but for which the kernel K x, z  = 0 for z > x. Finally, we note that any equation for which either  or both  of the integration limits is inﬁnite, or for which K x, z  becomes inﬁnite in the range of integration, is called a singular integral equation.  23.3 Operator notation and the existence of solutions  There is a close correspondence between linear integral equations and the matrix equations discussed in chapter 8. However, the former involve linear, integral rela- tions between functions in an inﬁnite-dimensional function space  see chapter 17 , whereas the latter specify linear relations among vectors in a ﬁnite-dimensional vector space. Since we are restricting our attention to linear integral equations, it will be convenient to introduce the linear integral operator K, whose action on an arbitrary function y is given by  Ky =  b   23.5  This is analogous to the introduction in chapters 16 and 17 of the notation L to describe a linear diﬀerential operator. Furthermore, we may deﬁne the Hermitian conjugate K†  K x, z y z  dz.  by  a   cid:21    cid:21   K†  ∗  b  a  y =  K   z, x y z  dz,  where the asterisk denotes complex conjugation and we have reversed the order of the arguments in the kernel. It is clear from  23.5  that K is indeed linear. Moreover, since K operates on the inﬁnite-dimensional space of  reasonable  functions, we may make an obvious analogy with matrix equations and consider the action of K on a function f as  that of a matrix on a column vector  both of inﬁnite dimension .  When written in operator form, the integral equations discussed in the pre- vious section resemble equations familiar from linear algebra. For example, the  805   INTEGRAL EQUATIONS  0 = f + λKy,  inhomogeneous Fredholm equation of the ﬁrst kind may be written as  which has the unique solution y = −K−1f λ, provided that f  cid:3 = 0 and the inverse operator K−1 exists.  Similarly, we may write the corresponding Fredholm equation of the second  kind as  y = f + λKy.   23.6   In the homogeneous case, where f = 0, this reduces to y = λKy, which is reminiscent of an eigenvalue problem in linear algebra  except that λ appears on the other side of the equation  and, similarly, only has solutions for at most a countably inﬁnite set of eigenvalues λi. The corresponding solutions yi are called the eigenfunctions. In the inhomogeneous case  f  cid:3 = 0 , the solution to  23.6  can be written  symbolically as  y =  1 − λK   −1f,  again provided that the inverse operator exists. It may be shown that, in general,   23.6  does possess a unique solution if λ  cid:3 = λi, i.e. when λ does not equal one of  the eigenvalues of the corresponding homogeneous equation.  When λ does equal one of these eigenvalues,  23.6  may have either many solutions or no solution at all, depending on the form of f. If the function f is orthogonal to every eigenfunction of the equation   23.7   ∗ that belongs to the eigenvalue λ  cid:20 gf cid:21  =  ∗K†  g  g = λ   cid:21   , i.e.  b  ∗  a  g   x f x  dx = 0  for every function g obeying  23.7 , then it can be shown that  23.6  has many solutions. Otherwise the equation has no solution. These statements are discussed further in section 23.7, for the special case of integral equations with Hermitian kernels, i.e. those for which K = K†  .  23.4 Closed-form solutions  In certain very special cases, it may be possible to obtain a closed-form solution of an integral equation. The reader should realise, however, when faced with an integral equation, that in general it will not be soluble by the simple methods presented in this section but must instead be solved using  numerical  iterative methods, such as those outlined in section 23.5.  806   23.4 CLOSED-FORM SOLUTIONS  23.4.1 Separable kernels  The most straightforward integral equations to solve are Fredholm equations with separable  or degenerate  kernels. A kernel is separable if it has the form  K x, z  =  φi x ψi z ,   23.8   where φi x  are ψi z  are respectively functions of x only and of z only and the number of terms in the sum, n, is ﬁnite.  Let us consider the solution of the  inhomogeneous  Fredholm equation of the  second kind,  y x  = f x  + λ  K x, z y z  dz,   23.9   which has a separable kernel of the form  23.8 . Writing the kernel in its separated form, the functions φi x  may be taken outside the integral over z to obtain  y x  = f x  + λ  φi x   ψi z y z  dz.  Since the integration limits a and b are constant for a Fredholm equation, the integral over z in each term of the sum is just a constant. Denoting these constants by  ci =  ψi z y z  dz,   23.10   the solution to  23.9  is found to be  y x  = f x  + λ  ciφi x ,   23.11   where the constants ci can be evalutated by substituting  23.11  into  23.10 .  cid:1 Solve the integral equation  y x  = x + λ   xz + z2 y z  dz.   23.12   n cid:4   i=1   cid:21   b  a   cid:21   b  a  n cid:4   i=1  n cid:4   i=1   cid:21   b  a   cid:21   1  0  The kernel for this equation is K x, z  = xz + z2, which is clearly separable, and using the notation in  23.8  we have φ1 x  = x, φ2 x  = 1, ψ1 z  = z and ψ2 z  = z2. From  23.11  the solution to  23.12  has the form  where the constants c1 and c2 are given by  23.10  as  y x  = x + λ c1x + c2 ,   cid:21   cid:21   1  1  0  0  c1 =  z[z + λ c1z + c2 ] dz = 1  3 + 1  3 λc1 + 1  2 λc2,  c2 =  z2[z + λ c1z + c2 ] dz = 1  4 + 1  4 λc1 + 1  3 λc2.  807   INTEGRAL EQUATIONS  These two simultaneous linear equations may be straightforwardly solved for c1 and c2 to give  c1 =  24 + λ  72 − 48λ − λ2  and  c2 =  18  72 − 48λ − λ2 ,  so that the solution to  23.12  is   72 − 24λ x + 18λ 72 − 48λ − λ2  .  cid:2   y x  =  In the above example, we see that  23.12  has a  ﬁnite  unique solution provided that λ is not equal to either root of the quadratic in the denominator of y x . The roots of this quadratic are in fact the eigenvalues of the corresponding homogeneous equation, as mentioned in the previous section. In general, if the separable kernel contains n terms, as in  23.8 , there will be n such eigenvalues, although they may not all be diﬀerent.  Kernels consisting of trigonometric  or hyperbolic  functions of sums or diﬀer-  ences of x and z are also often separable.   cid:1 Find the eigenvalues and corresponding eigenfunctions of the homogeneous Fredholm equation  y x  = λ  sin x + z  y z  dz.   23.13    cid:21   π  0  The kernel of this integral equation can be written in separated form as  K x, z  = sin x + z  = sin x cos z + cos x sin z,  so, comparing with  23.8 , we have φ1 x  = sin x, φ2 x  = cos x, ψ1 z  = cos z and ψ2 z  = sin z.  Thus, from  23.11 , the solution to  23.13  has the form   cid:21   cid:21   π  π  0  0  where the constants c1 and c2 are given by  y x  = λ c1 sin x + c2 cos x ,  c1 = λ  cos z  c1 sin z + c2 cos z  dz =  c2,  c2 = λ  sin z  c1 sin z + c2 cos z  dz =  c1.  λπ 2 λπ 2   23.14    23.15   Combining these two equations we ﬁnd c1 =  λπ 2 2c1, and, assuming that c1  cid:3 = 0, this gives λ = ±2 π, the two eigenvalues of the integral equation  23.13 . the eigenfunctions corresponding to the eigenvalues λ1 = 2 π and λ2 = −2 π are given  By substituting each of the eigenvalues back into  23.14  and  23.15 , we ﬁnd that  respectively by  y2 x  = B sin x − cos x ,   23.16   y1 x  = A sin x + cos x   and  where A and B are arbitrary constants.  cid:2   808   23.4 CLOSED-FORM SOLUTIONS  23.4.2 Integral transform methods  If the kernel of an integral equation can be written as a function of the diﬀerence  x − z of its two arguments, then it is called a displacement kernel. An integral equation having such a kernel, and which also has the integration limits −∞ to ∞, may be solved by the use of Fourier transforms  chapter 13 .  If we consider the following integral equation with a displacement kernel,   cid:21  ∞ −∞ K x − z y z  dz,  y x  = f x  + λ   23.17   the integral over z clearly takes the form of a convolution  see chapter 13 . Therefore, Fourier-transforming  23.17  and using the convolution theorem, we obtain  √  ˜y k  = ˜f k  +  2πλ ˜K k ˜y k ,  which may be rearranged to give  Taking the inverse Fourier transform, the solution to  23.17  is given by  ˜y k  =  ˜f k  2πλ ˜K k   .  1 − √  cid:21  ∞  y x  =  1√ 2π  1 − √  ˜f k  exp ikx  2πλ ˜K k   −∞  dk.   23.18   If we can perform this inverse Fourier transformation then the solution can be found explicitly; otherwise it must be left in the form of an integral.  cid:1 Find the Fourier transform of the function  g x  =  if x ≤ a, if x > a.  sin x − z  x − z    cid:21  ∞  1 0  −∞   cid:13   Hence ﬁnd an explicit expression for the solution of the integral equation  y x  = f x  + λ  y z  dz.   23.19   Find the solution for the special case f x  =  sin x  x.  The Fourier transform of g x  is given directly by 1√ 2π  1√ 2π  ˜g k  =  −a  a  exp −ikx   −ik   a  −a  =  2 π  sin ka  .  k   23.20   The kernel of the integral equation  23.19  is K x − z  = [sin x − z ]  x − z . Using   23.20 , it is straightforward to show that the Fourier transform of the kernel is   cid:14       cid:21   exp −ikx  dx =  cid:24   ˜K k  =   23.21   π 2 if k ≤ 1, if k > 1.  0  809    23.22   Thus, using  23.18 , we ﬁnd the Fourier transform of the solution to be  INTEGRAL EQUATIONS    ˜y k  =   cid:7   ˜f k   1 − πλ  ˜f k   cid:8   cid:21  − 1 1 − πλ 1√ 1 − πλ 2π  −1  πλ  1  1  if k ≤ 1, if k > 1.  cid:21   Inverse Fourier-transforming, and writing the result in a slightly more convenient form, the solution to  23.19  is given by  y x  = f x  +  1√ 2π  1  −1  ˜f k  exp ikx  dk  = f x  +  ˜f k  exp ikx  dk.   23.23   It is clear from  23.22  that when λ = 1 π, which is the only eigenvalue of the corresponding homogeneous equation to  23.19 , the solution becomes inﬁnite, as we would expect.  For the special case f x  =  sin x  x, the Fourier transform ˜f k  is identical to that in   23.21 , and the solution  23.23  becomes   cid:7   cid:7   cid:7   y x  =  sin x  x  x  x  sin x  sin x  +  +  +  =  =  πλ  πλ  1 − πλ 1 − πλ 1 − πλ  πλ   cid:8   cid:8   cid:8    cid:21      1  −1   cid:7   ix  π 2   cid:14    cid:13  1√ 2π  1 2  sin x  x  =   cid:8   k=−1 1 1 − πλ  exp ikx   k=1  exp ikx  dk  sin x  .  cid:2   x  If, instead, the integral equation  23.17  had integration limits 0 and x  so making it a Volterra equation  then its solution could be found, in a similar way, by using the convolution theorem for Laplace transforms  see chapter 13 . We would ﬁnd  ¯y s  =  ¯f s   1 − λ ¯K s   ,  where s is the Laplace transform variable. Often one may use the dictionary of Laplace transforms given in table 13.1 to invert this equation and ﬁnd the solution y x . In general, however, the evaluation of inverse Laplace transform integrals is diﬃcult, since  in principle  it requires a contour integration; see chapter 24.  As a ﬁnal example of the use of Fourier transforms in solving integral equations,  we mention equations that have integration limits −∞ and ∞ and a kernel of the  form  Consider, for example, the inhomogeneous Fredholm equation  y x  = f x  + λ   23.24   The integral over z is clearly just  a multiple of  the Fourier transform of y z ,  K x, z  = exp −ixz .   cid:21  ∞ −∞ exp −ixz  y z  dz.  810   23.4 CLOSED-FORM SOLUTIONS  so we can write  √  y x  = f x  +  2πλ˜y x .   23.25   Substituting  23.26  into  23.25  we ﬁnd  If we now take the Fourier transform of  23.25  but continue to denote the independent variable by x  i.e. rather than k, for example , we obtain  ˜y x  = ˜f x  +  √  2πλy −x .   23.26    cid:23  √ 2πλy −x   ,  √   cid:22   cid:22   y x  = f x  +  2πλ  ˜f x  +  √  cid:22   but on making the change x → −x and substituting back in for y −x , this gives   cid:23  2πλ˜f −x  + 2πλ2y x   .  √  f −x  +  y x  = f x  +  2πλ˜f x  + 2πλ2  Thus the solution to  23.24  is given by   cid:23  f x  +  2π 1 2λ˜f x  + 2πλ2f −x  +  2π 3 2λ3˜f −x   .  y x  =  1  1 −  2π 2λ4  2π; these are easily shown to be the eigenvalues of the corresponding homogeneous  √ Clearly,  23.24  possesses a unique solution provided λ  cid:3 = ±1  equation  for which f x  ≡ 0 .  cid:7   cid:1 Solve the integral equation   23.27  √ 2π or ±i    cid:8   y x  = exp  + λ  − x2 2   cid:21  ∞ −∞ exp −ixz  y z  dz,   23.28   where λ is a real constant. Show that the solution is unique unless λ has one of two particular values. Does a solution exist for either of these two values of λ?  Following the argument given above, the solution to  23.28  is given by  23.27  with  f x  = exp −x2 2 . In order to write the solution explicitly, however, we must calculate the Fourier transform of f x . Using equation  13.7 , we ﬁnd ˜f k  = exp −k2 2 , from  which we note that f x  has the special property that its functional form is identical to that of its Fourier transform. Thus, the solution to  23.28  is given by  y x  =  1  1 −  2π 2λ4  1 +  2π 1 2λ + 2πλ2 +  2π 3 2λ3  exp  Since λ is restricted to be real, the solution to  23.28  will be unique unless λ = ±1    23.29  √ 2π, at which points  23.29  becomes inﬁnite. In order to ﬁnd whether solutions exist for either of these values of λ we must return to equations  23.25  and  23.26 .  √ 2π. Putting this value into  23.25  and  23.26 ,  Let us ﬁrst consider the case λ = +1    cid:19    cid:7   − x2 2   cid:8   .   cid:18   we obtain  y x  = f x  + ˜y x ,  ˜y x  = ˜f x  + y −x .  811   23.30   23.31    INTEGRAL EQUATIONS  Substituting  23.31  into  23.30  we ﬁnd  y x  = f x  + ˜f x  + y −x ,  but on changing x to −x and substituting back in for y −x , this gives  y x  = f x  + ˜f x  + f −x  + ˜f −x  + y x .  Thus, in order for a solution to exist, we require that the function f x  obeys  f x  + ˜f x  + f −x  + ˜f −x  = 0.  This is satisﬁed if f x  = −˜f x , i.e. if the functional form of f x  is minus the form of its Fourier transform. We may repeat this analysis for the case λ = −1  way, we ﬁnd that this time we require f x  = ˜f x .  In our case f x  = exp −x2 2 , for which, as we mentioned above, f x  = ˜f x .  √ 2π, and, in a similar  √ 2π but has many solutions when  Therefore,  23.28  possesses no solution when λ = +1   λ = −1   √ 2π.  cid:2   A similar approach to the above may be taken to solve equations with kernels of the form K x, y  = cos xy or sin xy, either by considering the integral over y in each case as the real or imaginary part of the corresponding Fourier transform or by using Fourier cosine or sine transforms directly.  23.4.3 Differentiation  A closed-form solution to a Volterra equation may sometimes be obtained by diﬀerentiating the equation to obtain the corresponding diﬀerential equation, which may be easier to solve.  cid:1 Solve the integral equation  y x  = x −  x  xz2y z  dz.   23.32   Dividing through by x, we obtain  which may be diﬀerentiated with respect to x to give   cid:13   y x   x   cid:13   z2y z  dz,  = 1 −  x  0   cid:14  = −x2y x  = −x3  cid:13    cid:14    cid:14   y x   x  .  d dx  y x   x  ln  y x   x  = − x4  cid:7   4  + c,   cid:8   − x4 4  ,  This equation may be integrated straightforwardly, and we ﬁnd  where c is a constant of integration. Thus the solution to  23.32  has the form  y x  = Ax exp   23.33   where A is an arbitrary constant.  Since the original integral equation  23.32  contains no arbitrary constants, neither should its solution. We may calculate the value of the constant, A, by substituting the solution  23.33  back into  23.32 , from which we ﬁnd A = 1.  cid:2    cid:21   cid:21   0  812   23.5 NEUMANN SERIES  23.5 Neumann series   cid:21   b  a  As mentioned above, most integral equations met in practice will not be of the simple forms discussed in the last section and so, in general, it is not possible to ﬁnd closed-form solutions. In such cases, we might try to obtain a solution in the form of an inﬁnite series, as we did for diﬀerential equations  see chapter 16 .  Let us consider the equation  y x  = f x  + λ  K x, z y z  dz,   23.34   where either both integration limits are constants  for a Fredholm equation  or the upper limit is variable  for a Volterra equation . Clearly, if λ were small then a crude  but reasonable  approximation to the solution would be  y x  ≈ y0 x  = f x ,  where y0 x  stands for our ‘zeroth-order’ approximation to the solution  and is not to be confused with an eigenfunction .  Substituting this crude guess under the integral sign in the original equation,  we obtain what should be a better approximation:  y1 x  = f x  + λ  K x, z y0 z  dz = f x  + λ  K x, z f z  dz,  which is ﬁrst order in λ. Repeating the procedure once more results in the second-order approximation  y2 x  = f x  + λ  K x, z y1 z  dz  = f x  + λ  K x, z1 f z1  dz1 + λ2  dz1  K x, z1 K z1, z2 f z2  dz2.  It is clear that we may continue this process to obtain progressively higher-order  approximations to the solution. Introducing the functions   cid:21   b  a   cid:21   b  a   cid:21   b  a  K1 x, z  = K x, z ,  K2 x, z  =  K x, z1 K z1, z  dz1,  K3 x, z  =  dz1  K x, z1 K z1, z2 K z2, z  dz2,  and so on, which obey the recurrence relation  Kn x, z  =  K x, z1 Kn−1 z1, z  dz1,   cid:21   cid:21   a  b  a  b  813   cid:21   cid:21   b  a  b  a   cid:21   b  a   cid:21   cid:21   b  a  b  a   INTEGRAL EQUATIONS  we may write the nth-order approximation as  n cid:4   m=1   cid:21   b  a  b   cid:21  ∞ cid:4   a  m=0  yn x  = f x  +  λm  Km x, z f z  dz.   23.35   The solution to the original  integral equation is then given by y x  = limn→∞ yn x , provided the inﬁnite series converges. Using  23.35 , this solution may be written as  y x  = f x  + λ  R x, z; λ f z  dz,   23.36   where the resolvent kernel R x, z; λ  is given by  R x, z; λ  =  λmKm+1 x, z .   23.37   Clearly, the resolvent kernel, and hence the series solution, will converge provided λ is suﬃciently small. In fact, it may be shown that the series converges  in some domain of λ provided the original kernel K x, z  is bounded in such a  way that   23.38    cid:1 Use the Neumann series method to solve the integral equation  y x  = x + λ  xzy z  dz.   23.39   Following the method outlined above, we begin with the crude approximation y x  ≈  y0 x  = x. Substituting this under the integral sign in  23.39 , we obtain the next approxi- mation  y1 x  = x + λ  xzy0 z  dz = x + λ  xz2dz = x +   cid:21    cid:21   λ2  b  b  dx  a  a  K x, z 2 dz < 1.  cid:21    cid:21   cid:21   cid:21   0  1  1  1  0  0   cid:7    cid:16    cid:21   1  0  1  0   cid:8   λz 3   cid:7    cid:8   2   cid:7    cid:8   814  λx 3  ,   cid:8   λ 3  +  λ2 9  x.   cid:7    cid:17   For this simple example, it is easy to see that by continuing this process the solution to   23.39  is obtained as  y x  = x +  λ 3  +  λ 3  +  λ 3  3  + ···  x.  Clearly the expression in brackets is an inﬁnite geometric series with ﬁrst term λ 3 and  Repeating the procedure once more, we obtain  y2 x  = x + λ  xzy1 z  dz  = x + λ  xz  z +  dz = x +   23.6 FREDHOLM THEORY  common ratio λ 3. Thus, provided λ < 3, this inﬁnite series converges to the value λ  3 − λ , and the solution to  23.39  is  Finally, we note that the requirement that λ < 3 may also be derived very easily from  the condition  23.38 .  cid:2   y x  = x +  λx  3 − λ  =  3x  3 − λ  .   23.40   23.6 Fredholm theory  In the previous section, we found that a solution to the integral equation  23.34  can be obtained as a Neumann series of the form  23.36 , where the resolvent kernel R x, z; λ  is written as an inﬁnite power series in λ. This solution is valid provided the inﬁnite series converges.  A related, but more elegant, approach to the solution of integral equations using inﬁnite series was found by Fredholm. We will not reproduce Fredholm’s analysis here, but merely state the results we need. Essentially, Fredholm theory provides a formula for the resolvent kernel R x, z; λ  in  23.36  in terms of the ratio of two inﬁnite series:  The numerator and denominator in  23.41  are given by  R x, z; λ  =  D x, z; λ   .  d λ   ∞ cid:4  ∞ cid:4   n=0  n=0   −1 n  n!   −1 n  n!  D x, z; λ  =  Dn x, z λn,  d λ  =  dnλn,   23.41    23.42    23.43   where the functions Dn x, z  and the constants dn are found from recurrence relations as follows. We start with  D0 x, z  = K x, z   and  d0 = 1,   23.44   where K x, z  is the kernel of the original integral equation  23.34 . The higher- order coeﬃcients of λ in  23.43  and  23.42  are then obtained from the two recurrence relations  dn =  Dn−1 x, x  dx,  Dn x, z  = K x, z dn − n  K x, z1 Dn−1 z1, z  dz1.   23.45    23.46    cid:21   b  a  Although the formulae for the resolvent kernel appear complicated, they are often simple to apply. Moreover, for the Fredholm solution the power series  23.42  and  23.43  are both guaranteed to converge for all values of λ, unlike   cid:21   b  a  815   INTEGRAL EQUATIONS   cid:21   1  0   cid:21   1  0  xz 3  Neumann series, which converge only if the condition  23.38  is satisﬁed. Thus the  In fact, as we might suspect, the solutions of d λ  = 0 give the eigenvalues of the  Fredholm method leads to a unique, non-singular solution, provided that d λ   cid:3 = 0. homogeneous equation corresponding to  23.34 , i.e. with f x  ≡ 0.  cid:1 Use Fredholm theory to solve the integral equation  23.39 .  Using  23.36  and  23.41 , the solution to  23.39  can be written in the form  y x  = x + λ  R x, z; λ z dz = x + λ   23.47    cid:21   1  D x, z; λ   0  d λ   z dz.  In order to ﬁnd the form of the resolvent kernel R x, z; λ , we begin by setting  D0 x, z  = K x, z  = xz  and  d0 = 1  and use the recurrence relations  23.45  and  23.46  to obtain   cid:21   1  0  d1 =  D1 x, z  =   cid:21   0  D0 x, x  dx = −  xz2  1  1 z dz1 =  x2 dx = − xz  xz 3  1 3   cid:13   ,  z3 1 3   cid:14 1  0  = 0.  Applying the recurrence relations again we ﬁnd that dn = 0 and Dn x, z  = 0 for n > 1. Thus, from  23.42  and  23.43 , the numerator and denominator of the resolvent respectively are given by  D x, z; λ  = xz  and  d λ  = 1 − λ  .  3  Substituting these expressions into  23.47 , we ﬁnd that the solution to  23.39  is given  by   cid:21   cid:13   y x  = x + λ  = x + λ  0  1  xz2 1 − λ 3 z3 1 − λ 3 3  x  dz   cid:14 1  0  = x +  λx  3 − λ  =  3x  3 − λ  ,  which, as expected, is the same as the solution  23.40  found by constructing a Neumann series.  cid:2   23.7 Schmidt–Hilbert theory  The Schmidt–Hilbert  SH  theory of integral equations may be considered as analogous to the Sturm–Liouville  SL  theory of diﬀerential equations, discussed in chapter 17, and is concerned with the properties of integral equations with Hermitian kernels. An Hermitian kernel enjoys the property  K x, z  = K   z, x ,  ∗   23.48   and it is clear that a special case of  23.48  occurs for a real kernel that is also symmetric with respect to its two arguments.  816   23.7 SCHMIDT–HILBERT THEORY  Let us begin by considering the homogeneous integral equation  y = λKy,  where the integral operator K has an Hermitian kernel. As discussed in sec- tion 23.3, in general this equation will have solutions only for λ = λi, where the λi are the eigenvalues of the integral equation, the corresponding solutions yi being the eigenfunctions of the equation.  By following similar arguments to those presented in chapter 17 for SL theory, it may be shown that the eigenvalues λi of an Hermitian kernel are real and that the corresponding eigenfunctions yi belonging to diﬀerent eigenvalues are orthogonal and form a complete set. If the eigenfunctions are suitably normalised, we have   cid:20 yiyj cid:21  =  b  ∗ i  x yj x  dx = δij. y   23.49    cid:21   a  If an eigenvalue is degenerate then the eigenfunctions corresponding to that eigenvalue can be made orthogonal by the Gram–Schmidt procedure, in a similar way to that discussed in chapter 17 in the context of SL theory.  Like SL theory, SH theory does not provide a method of obtaining the eigen- values and eigenfunctions of any particular homogeneous integral equation with an Hermitian kernel; for this we have to turn to the methods discussed in the previous sections of this chapter. Rather, SH theory is concerned with the gen- eral properties of the solutions to such equations. Where SH theory becomes applicable, however, is in the solution of inhomogeneous integral equations with Hermitian kernels for which the eigenvalues and eigenfunctions of the corre- sponding homogeneous equation are already known.  Let us consider the inhomogeneous equation  y = f + λKy,   23.50   where K = K† and for which we know the eigenvalues λi and normalised eigenfunctions yi of the corresponding homogeneous problem. The function f may or may not be expressible solely in terms of the eigenfunctions yi, and to i aiyi, accommodate this situation we write the unknown solution y as y = f + where the ai are expansion coeﬃcients to be determined.   cid:11   Substituting this into  23.50 , we obtain   cid:4   i   cid:4   i  + λKf,  aiyi λi  f +  aiyi = f + λ   23.51   where we have used the fact that yi = λiKyi. Forming the inner product of both  817   INTEGRAL EQUATIONS  sides of  23.51  with yj, we ﬁnd cid:4    cid:4   ai cid:20 yjyi cid:21  = λ   cid:20 yjyi cid:21  + λ cid:20 yjKf cid:21 .   23.52  Since the eigenfunctions are orthonormal and K is an Hermitian operator,  cid:20 yjf cid:21 . Thus the we have that both  cid:20 yjyi cid:21  = δij and  cid:20 yjKf cid:21  =  cid:20 Kyjf cid:21  = λ −1  i  i  j  ai λi  coeﬃcients aj are given by  This also shows, incidentally, that a formal representation for the resolvent kernel is  aj =  =  λλ j   cid:20 yjf cid:21  −1 1 − λλ −1  cid:4   j  ,  λ cid:20 yjf cid:21  λj − λ  cid:4   y = f +  aiyi = f + λ  i  i   cid:20 yif cid:21  λi − λ  yi.   cid:4   i  yi x y  ∗ i  z  λi − λ  .  R x, z; λ  =  y =  bi  1 − λλ −1  i  yi.   23.53    23.54    23.55    cid:11    23.56   If f can be expressed as a linear superposition of the yi, i.e. f =  bi =  cid:20 yif cid:21  and the solution can be written more brieﬂy as  i biyi, then  and the solution is  We see from  23.54  that the inhomogeneous equation  23.50  has a unique  solution provided λ  cid:3 = λi, i.e. when λ is not equal to one of the eigenvalues of  the corresponding homogeneous equation. However, if λ does equal one of the eigenvalues λj then, in general, the coeﬃcients aj become singular and no  ﬁnite  solution exists.  Returning to  23.53 , we notice that even if λ = λj a non-singular solution to the integral equation is still possible provided that the function f is orthogonal to every eigenfunction corresponding to the eigenvalue λj, i.e.   cid:20 yjf cid:21  =  b  ∗ j  x f x  dx = 0. y  The following worked example illustrates the case in which f can be expressed in terms of the yi. One in which it cannot is considered in exercise 23.14.  cid:1 Use Schmidt–Hilbert theory to solve the integral equation  y x  = sin x + α  + λ  sin x + z y z  dz.   23.57   It is clear that the kernel K x, z  = sin x + z  is real and symmetric in x and z and is   cid:4   i   cid:21   a   cid:21   π  0  818   23.8 EXERCISES  thus Hermitian. In order to solve this inhomogeneous equation using SH theory, however, we must ﬁrst ﬁnd the eigenvalues and eigenfunctions of the corresponding homogeneous equation.  In fact, we have considered the solution of the corresponding homogeneous equation  23.13  already, in subsection 23.4.1, where we found that it has two eigenvalues λ1 = 2 π  and λ2 = −2 π, with eigenfunctions given by  23.16 . The normalised eigenfunctions are  y1 x  =   sin x + cos x   and  y2 x  =   23.58    sin x − cos x   1√  π  and are easily shown to obey the orthonormality condition  23.49 .  Using  23.54 , the solution to the inhomogeneous equation  23.57  has the form  where the coeﬃcients a1 and a2 are given by  23.53  with f x  = sin x + α . Therefore, using  23.58 ,  y x  = a1y1 x  + a2y2 x ,   23.59   a1 =  a2 =  1  1 − πλ 2  1  1 + πλ 2  1√ 1√  π  π  π  π  0  0   sin z + cos z  sin z + α  dz =   cos α + sin α ,   sin z − cos z  sin z + α  dz =  1√  π   cid:21   cid:21   Substituting these expressions for a1 and a2 into  23.59  and simplifying, we ﬁnd that  the solution to  23.57  is given by  √ 2 − πλ √  π  π  2 + πλ   cos α − sin α .  cid:19    cid:18   y x  =  1  1 −  πλ 2 2  sin x + α  +  πλ 2  cos x − α   .  cid:2   Solve the integral equation cid:21  ∞  23.8 Exercises  for the function y = y x  for x > 0. Note that for x < 0, y x  can be chosen as is most convenient. Solve  23.1  23.2  23.3  Convert  into a diﬀerential equation, and hence show that its solution is   α + βx  exp x + γ exp −x ,  23.4  where α, β and γ are constants that should be determined. Use the fact that its kernel is separable, to solve for y x  the integral equation  y x  = A cos x + a  + λ  sin x + z y z  dz.  [ This equation is an inhomogeneous extension of the homogeneous Fredholm equation  23.13 , and is similar to equation  23.57 . ]  0  cos xv y v  dv = exp −x2 2   cid:21  ∞  f t  exp −st  dt =  a  a2 + s2 .  0  f x  = exp x +  x   x − y f y  dy   cid:21   0   cid:21   π  0  819   23.5  Solve for φ x  the integral equation  INTEGRAL EQUATIONS   cid:13  cid:7    cid:8   n   cid:9    cid:14    cid:10   n  x y  +  y x   cid:21   1  0   cid:1   φ x  = f x  + λ  φ y  dy,  where f x  is bounded for 0 < x < 1 and − 1 0 f y ym dy.  in terms of the quantities Fm =  1  2 < n < 1  2 , expressing your answer   a  Give the explicit solution when λ = 1.  b  For what values of λ are there no solutions unless F±n are in a particular  ratio? What is this ratio?  23.6  Consider the inhomogeneous integral equation  f x  = g x  + λ  K x, y f y  dy,  for which the kernel K x, y  is real, symmetric and continuous in a ≤ x ≤ b, a ≤ y ≤ b.  a   a  If λ is one of the eigenvalues λi of the homogeneous equation  fi x  = λi  K x, y fi y  dy,  prove that the inhomogeneous equation can only a have non-trivial solution if g x  is orthogonal to the corresponding eigenfunction fi x .   b  Show that the only values of λ for which  f x  = λ  xy x + y f y  dy  has a non-trivial solution are the roots of the equation  λ2 + 120λ − 240 = 0.  23.7  The kernel of the integral equation  f x  = µx2 +  2xy x + y f y  dy.   c  Solve  has the form   cid:21   b   cid:21    cid:21   b  a  1  0   cid:21   1  0  b  a  ∞ cid:4   n=0   cid:21   ψ x  = λ  K x, y ψ y  dy  K x, y  =  hn x gn y ,  where the hn x  form a complete orthonormal set of functions over the interval [a, b].   a  Show that the eigenvalues λi are given by  M − λ  cid:21  where M is the matrix with elements b  −1I = 0,  cid:11 ∞  Mkj =  gk u hj u  du.  a  If the corresponding solutions are ψ i  x  = for a i  n .  n=0 a i   n hn x , ﬁnd an expression  820   23.8 EXERCISES  1 n  ∞ cid:4   cid:21   n=1  x  0   cid:21  ∞  0  and   b  Obtain the eigenvalues and eigenfunctions over the interval [0, 2π] if  K x, y  =  cos nx cos ny.  23.8  By taking its Laplace transform, and that of xne of  f x  = e  x +   x − u euf u  du  .  −ax, obtain the explicit solution   cid:14   23.9  Verify your answer by substitution.  For f t  = exp −t2 2 , use the relationships of the Fourier transforms of f  t  and tf t  to that of f t  itself to ﬁnd a simple diﬀerential equation satisﬁed by ˜f ω , the Fourier transform of f t , and hence determine ˜f ω  to within a constant. Use this result to solve the integral equation   cid:7    cid:13   −x   cid:21  ∞  −∞ e  −t t−2x  2h t  dt = e3x2 8  23.10  for h t . Show that the equation  f x  = x  −1 3 + λ  f y  exp −xy  dy  has a solution of the form Axα + Bxβ. Determine the values of α and β, and show that those of A and B are  1  1 − λ2Γ  1  3  Γ  2 3    λΓ  2 3   1 − λ2Γ  1 3  Γ  2 3    ,  23.11  where Γ z  is the gamma function. At an international ‘peace’ conference a large number of delegates are seated around a circular table with each delegation sitting near its allies and diametrically opposite the delegation most bitterly opposed to it. The position of a delegate is  denoted by θ, with 0 ≤ θ ≤ 2π. The fury f θ  felt by the delegate at θ is the sum delegates; a delegate at position φ contributes an amount K θ − φ f φ . Thus  of his own natural hostility h θ  and the inﬂuences on him of each of the other  f θ  = h θ  +  2π  K θ − φ f φ  dφ.  Show that if K ψ  takes the form K ψ  = k0 + k1 cos ψ then  f θ  = h θ  + p + q cos θ + r sin θ  and evaluate p, q and r. A positive value for k1 implies that delegates tend to placate their opponents but upset their allies, whilst negative values imply that they calm their allies but infuriate their opponents. A walkout will occur if f θ  exceeds a certain threshold value for some θ. Is this more likely to happen for positive or for negative values of k1? By considering functions of the form h x  = solution f x  of the integral equation  0  x − y f y  dy, show that the   cid:1   x  23.12  f x  = x + 1 2  1  x − yf y  dy  satisﬁes the equation f   x  = f x .   cid:7  cid:7    cid:21   0   cid:21   0  821   INTEGRAL EQUATIONS  By examining the special cases x = 0 and x = 1, show that −x].  [ e + 2 ex − ee  f x  =  2  23.13  The operator M is deﬁned by   e + 3  e + 1    cid:21  ∞  Mf x  ≡  −∞ K x, y f y  dy,   cid:21  ∞  cid:21   b  a  f x  = g x  + λ  −∞ K x, y f y  dy.  y x  = x  −3 + λ  x2z2y z  dz,  where K x, y  = 1 inside the square x < a,y < a and K x, y  = 0 elsewhere. Consider the possible eigenvalues of M and the eigenfunctions that correspond  to them; show that the only possible eigenvalues are 0 and 2a and determine the corresponding eigenfunctions. Hence ﬁnd the general solution of  23.14  For the integral equation  show that the resolvent kernel is 5x2z2 [5 − λ b5 − a5 ] and hence solve the  23.15  equation. For what range of λ is the solution valid? Use Fredholm theory to show that, for the kernel  K x, z  =  x + z  exp x − z   over the interval [0, 1], the resolvent kernel is  R x, z; λ  =  exp x − z [ x + z  − λ  1 1 − λ − 1  2 x + 1 12 λ2  2 z − xz − 1 3  ]  ,  and hence solve  y x  = x2 + 2  1   x + z  exp x − z  y z  dz,   cid:1   0 un exp −u  du.  1  23.16  expressing your answer in terms of In, where In = This exercise shows that following formal theory is not necessarily the best way to get practical results!  a  Determine the eigenvalues λ± of the kernel K x, z  =  xz 1 2 x1 2 + z1 2  and  show that the corresponding eigenfunctions have the forms  where A2± = 5  10 ± 4  √  6 .   b  Use Schmidt–Hilbert theory to solve  y± x  = A±   3x ,  y x  = 1 + 5 2  K x, z y z  dz.  √ 2x1 2 ± √  cid:21   1  0   c  As will have been apparent, the algebra involved in the formal method used in  b  is long and error-prone, and it is in fact much more straightforward to use a trial function 1 + αx1 2 + βx. Check your answer by doing so.   cid:21   0  822   23.9 HINTS AND ANSWERS  23.9 Hints and answers  b  f   cid:1   n =  −1 2]  a hn x ψ i  x  dx;  b  use  1   −n.  b  There are no solutions for  −1 unless F±n = 0 or Fn F−n = ∓[ 1 − 2n   1 + 2n ]1 2.  √ √ π  sin nx; M is diago- π  cos kx. −ω2 2. Rearrange the integral as a convolution −t2 6, where resubstitution and  Deﬁne y −x  = y x  and use the cosine Fourier transform inversion theorem; y x  =  2 π 1 2 exp −x2 2 .  x  − f x  = exp x; α = 3 4, β = 1 2, γ = 1 4.  cid:7  cid:7   a  φ x  = f x  −  1 + 2n Fnxn −  1 − 2n F−nx λ = [1 ±  1 − 4n2   a  a i  nal; eigenvalues λk = k π with eigenfunctions ψ k  x  =  1  d˜f dω = −ω˜f, leading to ˜f ω  = Ae and deduce that ˜h ω  = Be  cid:1   cid:1  p = k0H  1 − 2πk0 , q = k1Hc  1 − πk1  and r = k1Hs  1 − πk1 , Gaussian normalisation show that C =  cid:1  values of k1 ≈ π For eigenvalue 0 : f x  = 0 for x < a or f x  is such that For eigenvalue 2a : f x  = µS  x, a  with µ a constant and S  x, a  ≡ [H a + x  − H x − a ], where H z  is the Heaviside step function. a−a g z  dz. Show that c = λ  1 − 2aλ . y x  = x2 −  3I3x + I2  exp x.  √ π  cos nx and  1   cid:24   −1  are most likely to cause a conference breakdown.  −3ω2 2; h t  = Ce  cid:1  2  3π .  Take f x  = g x  + cGS  x, a , where G =  2π 0 h z  cos z dz, and Hs =  2π 0 h z  sin z dz. Positive  2π 0 h z  dz, Hc =  a−a f y dy = 0.  where H =   cid:1   23.1  23.3 23.5  23.7  23.9  23.11  23.13  23.15  823   24  Complex variables  Throughout this book references have been made to results derived from the the- ory of complex variables. This theory thus becomes an integral part of the mathe- matics appropriate to physical applications. Indeed, so numerous and widespread are these applications that the whole of the next chapter is devoted to a systematic presentation of some of the more important ones. This current chapter develops the general theory on which these applications are based. The diﬃculty with it, from the point of view of a book such as the present one, is that the underlying basis has a distinctly pure mathematics ﬂavour.  Thus, to adopt a comprehensive rigorous approach would involve a large amount of groundwork in analysis, for example formulating precise deﬁnitions of continuity and diﬀerentiability, developing the theory of sets and making a detailed study of boundedness. Instead, we will be selective and pursue only those parts of the formal theory that are needed to establish the results used in the next chapter and elsewhere in this book.  In this spirit, the proofs that have been adopted for some of the standard results of complex variable theory have been chosen with an eye to simplicity rather than sophistication. This means that in some cases the imposed conditions are more stringent than would be strictly necessary if more sophisticated proofs were used; where this happens the less restrictive results are usually stated as well. The reader who is interested in a fuller treatment should consult one of the § many excellent textbooks on this fascinating subject.  One further concession to ‘hand-waving’ has been made in the interests of keeping the treatment to a moderate length. In several places phrases such as ‘can be made as small as we like’ are used, rather than a careful treatment in terms of ‘given  cid:4  > 0, there exists a δ > 0 such that’. In the authors’ experience, some  §  For example, K. Knopp, Theory of Functions, Part I  New York: Dover, 1945 ; E. G. Phillips, Functions of a Complex Variable with Applications 7th edn  Edinburgh: Oliver and Boyd, 1951 ; E. C. Titchmarsh, The Theory of Functions  Oxford: Oxford University Press, 1952 .  824   24.1 FUNCTIONS OF A COMPLEX VARIABLE  students are more at ease with the former type of statement, despite its lack of precision, whilst others, those who would contemplate only the latter, are usually well able to supply it for themselves.  24.1 Functions of a complex variable  The quantity f z  is said to be a function of the complex variable z if to every value of z in a certain domain R  a region of the Argand diagram  there corresponds one or more values of f z . Stated like this f z  could be any function consisting of a real and an imaginary part, each of which is, in general, itself a function of x and y. If we denote the real and imaginary parts of f z  by u and v, respectively, then  f z  = u x, y  + iv x, y .  In this chapter, however, we will be primarily concerned with functions that are single-valued, so that to each value of z there corresponds just one value of f z , and are diﬀerentiable in a particular sense, which we now discuss.  A function f z  that is single-valued in some domain R is diﬀerentiable at the  point z in R if the derivative   cid:13    cid:14    cid:7   f   z  = lim ∆z→0  f z + ∆z  − f z   ∆z   24.1   exists and is unique, in that its value does not depend upon the direction in the Argand diagram from which ∆z tends to zero.  cid:1 Show that the function f z  = x2 − y2 + i2xy is diﬀerentiable for all values of z.  Considering the deﬁnition  24.1 , and taking ∆z = ∆x + i∆y, we have  f z + ∆z  − f z    x + ∆x 2 −  y + ∆y 2 + 2i x + ∆x  y + ∆y  − x2 + y2 − 2ixy 2x∆x +  ∆x 2 − 2y∆y −  ∆y 2 + 2i x∆y + y∆x + ∆x∆y   ∆x + i∆y  ∆z  =  =  = 2x + i2y +  ∆x + i∆y   ∆x 2 −  ∆y 2 + 2i∆x∆y  ∆x + i∆y  .  Now, in whatever way ∆x and ∆y are allowed to tend to zero  e.g. taking ∆y = 0 and  letting ∆x → 0 or vice versa , the last term on the RHS will tend to zero and the unique limit 2x + i2y will be obtained. Since z was arbitrary, f z  with u = x2 − y2 and v = 2xy is diﬀerentiable at all points in the  ﬁnite  complex plane.  cid:2   We note that the above working can be considerably reduced by recognising  that, since z = x + iy, we can write f z  as  f z  = x2 − y2 + 2ixy =  x + iy 2 = z2.  825   COMPLEX VARIABLES  We then ﬁnd that   cid:13    cid:7   f   z  = lim ∆z→0   z + ∆z 2 − z2  ∆z   cid:14    cid:14    cid:13    cid:9   = lim ∆z→0   ∆z 2 + 2z∆z   cid:10   ∆z  =  lim ∆z→0  ∆z  + 2z = 2z,  from which we see immediately that the limit both exists and is independent of  the way in which ∆z → 0. Thus we have veriﬁed that f z  = z2 is diﬀerentiable  for all  ﬁnite  z. We also note that the derivative is analogous to that found for real variables.  Although the deﬁnition of a diﬀerentiable function clearly includes a wide class of functions, the concept of diﬀerentiability is restrictive and, indeed, some functions are not diﬀerentiable at any point in the complex plane.   cid:1 Show that the function f z  = 2y + ix is not diﬀerentiable anywhere in the complex plane.  In this case f z  cannot be written simply in terms of z, and so we must consider the limit  24.1  in terms of x and y explicitly. Following the same procedure as in the previous example we ﬁnd  f z + ∆z  − f z   2y + 2∆y + ix + i∆x − 2y − ix  ∆z  ∆x + i∆y  =  =  2∆y + i∆x ∆x + i∆y  .  In this case the limit will clearly depend on the direction from which ∆z → 0. Suppose ∆z → 0 along a line through z of slope m, so that ∆y = m∆x, then  cid:14    cid:14    cid:13    cid:13   f z + ∆z  − f z   lim ∆z→0  ∆z  = lim  ∆x, ∆y→0  2∆y + i∆x ∆x + i∆y  =  2m + i 1 + im  .  This limit is dependent on m and hence on the direction from which ∆z → 0. Since this conclusion is independent of the value of z, and hence true for all z, f z  = 2y + ix is nowhere diﬀerentiable.  cid:2   A function that is single-valued and diﬀerentiable at all points of a domain R is said to be analytic  or regular  in R. A function may be analytic in a domain except at a ﬁnite number of points  or an inﬁnite number if the domain is inﬁnite ; in this case it is said to be analytic except at these points, which are called the singularities of f z . In our treatment we will not consider cases in which an inﬁnite number of singularities occur in a ﬁnite domain.  826   24.2 THE CAUCHY–RIEMANN RELATIONS   cid:1 Show that the function f z  = 1  1 − z  is analytic everywhere except at z = 1.  Since f z  is given explicitly as a function of z, evaluation of the limit  24.1  is somewhat easier. We ﬁnd   cid:14    cid:13   cid:13   cid:13    cid:7   f   z  = lim ∆z→0  = lim ∆z→0  = lim ∆z→0  f z + ∆z  − f z   cid:7   ∆z  1 ∆z  1  1 − z − ∆z   cid:8  cid:14    cid:14  − 1 1 − z  1   1 − z − ∆z  1 − z   =  1   1 − z 2 ,  independently of the way in which ∆z → 0, provided z  cid:3 = 1. Hence f z  is analytic everywhere except at the singularity z = 1.  cid:2   24.2 The Cauchy–Riemann relations  From examining the previous examples, it is apparent that for a function f z  to be diﬀerentiable and hence analytic there must be some particular connection between its real and imaginary parts u and v.  By considering a general function we next establish what this connection must  be. If the limit   cid:13    cid:14   f z + ∆z  − f z   L = lim ∆z→0  ∆z  is to exist and be unique, in the way required for diﬀerentiability, then any two  speciﬁc ways of letting ∆z → 0 must produce the same limit. In particular, moving  parallel to the real axis and moving parallel to the imaginary axis must do so. This is certainly a necessary condition, although it may not be suﬃcient.  If we let f z  = u x, y  + iv x, y  and ∆z = ∆x + i∆y then we have  f z + ∆z  = u x + ∆x, y + ∆y  + iv x + ∆x, y + ∆y ,  and the limit  24.2  is given by  u x + ∆x, y + ∆y  + iv x + ∆x, y + ∆y  − u x, y  − iv x, y   L = lim  ∆x, ∆y→0  ∆x + i∆y  If we ﬁrst suppose that ∆z is purely real, so that ∆y = 0, we obtain  u x + ∆x, y  − u x, y   v x + ∆x, y  − v x, y   L = lim ∆x→0  ∆x  + i  ∆x  =  + i  ∂u ∂x  ∂v ∂x  ,  24.3   provided each limit exists at the point z. Similarly, if ∆z is taken as purely imaginary, so that ∆x = 0, we ﬁnd  u x, y + ∆y  − u x, y   v x, y + ∆y  − v x, y   i∆y  + i  i∆y  L = lim ∆y→0  =  1 i  ∂u ∂y  +  ∂v ∂y  .  24.4    cid:14    cid:14    24.2    cid:14   .   cid:13    cid:13    cid:13   827   COMPLEX VARIABLES  For f to be diﬀerentiable at the point z, expressions  24.3  and  24.4  must be identical. It follows from equating real and imaginary parts that necessary conditions for this are  ∂u ∂x  =  ∂v ∂y  and  = − ∂u  .  ∂y  ∂v ∂x   24.5   These two equations are known as the Cauchy–Riemann relations.  We can now see why for the earlier examples  i  f z  = x2 − y2 + i2xy might  i  u = x2 − y2, v = 2xy:  be diﬀerentiable and  ii  f z  = 2y + ix could not be.   ii  u = 2y, v = x:  = 2x =  and  ∂v ∂y  = 2y = − ∂u  ,  ∂y  ∂v ∂x  = 0 =  but  ∂v ∂y  = 1  cid:3 = −2 = − ∂u  .  ∂y  ∂v ∂x  ∂u ∂x  ∂u ∂x  It is apparent that for f z  to be analytic something more than the existence of the partial derivatives of u and v with respect to x and y is required; this something is that they satisfy the Cauchy–Riemann relations.  §  We may enquire also as to the suﬃcient conditions for f z  to be analytic in that a suﬃcient condition is that the four partial derivatives R. It can be shown exist, are continuous and satisfy the Cauchy–Riemann relations. It is the addi- tional requirement of continuity that makes the diﬀerence between the necessary conditions and the suﬃcient conditions.   cid:1 In which domain s  of the complex plane is f z  = x − iy an analytic function?  Writing f = u + iv it is clear that both ∂u ∂y and ∂v ∂x are zero in all four quadrants and hence that the second Cauchy–Riemann relation in  24.5  is satisﬁed everywhere.  Turning to the ﬁrst Cauchy–Riemann relation, in the ﬁrst quadrant  x > 0, y > 0  we  have f z  = x − iy so that  ∂u ∂x  = 1,  = −1,  ∂v ∂y  which clearly violates the ﬁrst relation in  24.5 . Thus f z  is not analytic in the ﬁrst quadrant.  Following a similiar argument for the other quadrants, we ﬁnd  = −1 or + 1 = −1 or + 1  ∂u ∂x ∂v ∂y  for x   0, respectively,  for y > 0 and y < 0, respectively.  Therefore ∂u ∂x and ∂v ∂y are equal, and hence f z  is analytic only in the second and fourth quadrants.  cid:2   §  See, for example, any of the references given on page 824.  828   24.2 THE CAUCHY–RIEMANN RELATIONS  Since x and y are related to z and its complex conjugate z ∗  ∗  x =   z + z     and  y =   z − z   ,  1 2i  ∗  by  we may formally regard any function f = u + iv as a function of z and z than x and y. If we do this and examine ∂f ∂z  we obtain  ∗  ∂f ∂z  ∗ =   cid:7   ∂u ∂x  ∂x ∂z  ∗ +  ∂f ∂y  ∂v ∂x  + i − ∂v  ∂y  ∂u ∂x  =  =  1 2   cid:7    cid:8   ∂y ∗ ∂z   cid:8  cid:7   cid:8   1 2   cid:7   +  ∂u ∂y  + i   cid:8   ∂v ∂y  .  +  i 2  ∂v ∂x  +  ∂u ∂y   cid:8  cid:7    cid:8   − 1 2i  1 2   cid:7   ∂f ∂x   24.6   ∗  , rather   24.7   Now, if f is analytic then the Cauchy–Riemann relations  24.5  must be satisﬁed, is identically zero. Thus we conclude that and these immediately give that ∂f ∂z and any expression representing if f is analytic then f cannot be a function of z an analytic function of z can contain x and y only in the combination x + iy, not  ∗  ∗  in the combination x − iy.  We conclude this section by discussing some properties of analytic functions that are of great practical importance in theoretical physics. These can be obtained simply from the requirement that the Cauchy–Riemann relations must be satisﬁed by the real and imaginary parts of an analytic function.  The most important of these results can be obtained by diﬀerentiating the ﬁrst Cauchy–Riemann relation with respect to one independent variable, and the second with respect to the other independent variable, to obtain the two chains of equalities   cid:7   cid:7    cid:8   cid:8   ∂ ∂x  ∂ ∂x  ∂u ∂x  ∂v ∂x  =  ∂ ∂x  = − ∂  ∂x   cid:7    cid:8   ∂v ∂y   cid:7   =   cid:8   ∂ ∂y   cid:7    cid:8   cid:7   ∂v ∂x   cid:8  = − ∂  ∂y   cid:7    cid:8   cid:7   ∂u ∂y  ∂u ∂y  = − ∂  ∂y  ∂u ∂x  = − ∂  ∂y   cid:8   .  ,  ∂v ∂y  Thus both u and v are separately solutions of Laplace’s equation in two dimen- sions, i.e.  ∂2v ∂y2 = 0. We will make signiﬁcant use of this result in the next chapter.  ∂2u ∂y2 = 0  ∂2u ∂x2 +  ∂2v ∂x2 +  and  A further useful result concerns the two families of curves u x, y  = constant and v x, y  = constant, where u and v are the real and imaginary parts of any analytic function f = u + iv. As discussed in chapter 10, the vector normal to the curve u x, y  = constant is given by   24.8    24.9   ∇u =  ∂u ∂x  i +  ∂u ∂y  j,  829   where i and j are the unit vectors along the x- and y-axes, respectively. A similar  expression exists for ∇v, the normal to the curve v x, y  = constant. Taking the  scalar product of these two normal vectors, we obtain  COMPLEX VARIABLES  ∇u · ∇v =  ∂u ∂x  ∂v ∂x  +  ∂u ∂y  ∂v ∂y  = − ∂u  ∂x  ∂u ∂y  +  ∂u ∂y  ∂u ∂x  = 0,  where in the last line we have used the Cauchy–Riemann relations to rewrite the partial derivatives of v as partial derivatives of u. Since the scalar product of the normal vectors is zero, they must be orthogonal, and the curves u x, y  = constant and v x, y  = constant must therefore intersect at right angles.  cid:1 Use the Cauchy–Riemann relations to show that, for any analytic function f = u + iv, the relation ∇u = ∇v must hold.  From  24.9  we have   cid:7    cid:8   2   cid:7    cid:8   +  2  .  ∂u ∂y  ∂u ∂x   cid:7    cid:8   2  +  ∂v ∂y  ∂v ∂x  2  = ∇v2,  ∇u2 = ∇u · ∇u =  cid:8    cid:7   ∇u2 =  from which the result ∇u = ∇v follows immediately.  cid:2   Using the Cauchy–Riemann relations to write the partial derivatives of u in terms of those of v, we obtain  24.3 Power series in a complex variable  The theory of power series in a real variable was considered in chapter 4, which also contained a brief discussion of the natural extension of this theory to a series such as  f z  =  anzn,   24.10   where z is a complex variable and the an are, in general, complex. We now consider complex power series in more detail.  Expression  24.10  is a power series about the origin and may be used for general discussion, since a power series about any other point z0 can be obtained  by a change of variable from z to z − z0. If z were written in its modulus and  argument form, z = r exp iθ, expression  24.10  would become  f z  =  anrn exp inθ .   24.11   ∞ cid:4   n=0  ∞ cid:4   n=0  830   24.3 POWER SERIES IN A COMPLEX VARIABLE  This series is absolutely convergent if  ∞ cid:4   n=0  anrn,  n→∞an1 n,  = lim  1 R   24.12    24.13   which is a series of positive real terms, is convergent. Thus tests for the absolute convergence of real series can be used in the present context, and of these the most appropriate form is based on the Cauchy root test. With the radius of convergence R deﬁned by   cid:11   the series  24.10  is absolutely convergent if z   R. If z = R then no particular conclusion may be drawn, and this case must be  considered separately, as discussed in subsection 4.5.1.  A circle of radius R centred on the origin is called the circle of convergence  anzn. The cases R = 0 and R = ∞ correspond, respectively, to  of the series convergence at the origin only and convergence everywhere. For R ﬁnite the convergence occurs in a restricted part of the z-plane  the Argand diagram . For a power series about a general point z0, the circle of convergence is, of course, centred on that point.  cid:1 Find the parts of the z-plane for which the following series are convergent:  ∞ cid:4   n=0  zn n!  ,  ∞ cid:4   n=0   i    ii   n!zn,   iii   ∞ cid:4   n=1  zn n  .   i  Since  n! 1 n behaves like n as n → ∞ we ﬁnd lim 1 n! 1 n = 0. Hence R = ∞ and the series is convergent for all z.  ii  Correspondingly, lim n! 1 n = ∞. Thus R = 0 and the series converges only at z = 0.  iii  As n → ∞,  n 1 n has a lower limit of 1 and hence lim 1 n 1 n = 1 1 = 1. Thus the series is absolutely convergent if the condition z < 1 is satisﬁed.  cid:2   Case  iii  in the above example provides a good illustration of the fact that on its circle of convergence a power series may or may not converge. For this  particular series, the circle of convergence is z = 1, so let us consider the  convergence of the series at two diﬀerent points on this circle. Taking z = 1, the series becomes  which is easily shown to diverge  by, for example, grouping terms, as discussed in  subsection 4.3.2 . Taking z = −1, however, the series is given by  ∞ cid:4   n=1  ∞ cid:4   n=1   −1 n  n  1 n  = 1 +  +  +  1 2  1 3  + ··· ,  1 4  = −1 +  − 1 3  1 2  +  1 4  − ··· ,  831   COMPLEX VARIABLES  which is an alternating series whose terms decrease in magnitude and which therefore converges.  The ratio test discussed in subsection 4.3.2 may also be employed to investi- gate the absolute convergence of a complex power series. A series is absolutely convergent if  an+1zn+1 anzn  lim n→∞  an+1z an < 1  = lim n→∞   24.14   and hence the radius of convergence R of the series is given by  1 R  = lim n→∞  an+1 an  .  For instance, in case  i  of the previous example, we have  1 R  n!  1  = lim n→∞   n + 1 !  = lim n→∞  n + 1  = 0.  Thus the series is absolutely convergent for all  ﬁnite  z, conﬁrming the previous result.  Before turning to particular power series, we conclude this section by stating 0 anzn has a sum that is an analytic  § the important result function of z inside its circle of convergence.  that the power series   cid:11 ∞  As a corollary to the above theorem, it may further be shown that if f z  = anzn then, inside the circle of convergence of the series,   cid:11   ∞ cid:4   n=0   cid:7   f   z  =  nanzn−1.  Repeated application of this result demonstrates that any power series can be diﬀerentiated any number of times inside its circle of convergence.  24.4 Some elementary functions  In the example at the end of the previous section it was shown that the function exp z deﬁned by  exp z =   24.15   is convergent for all z of ﬁnite modulus and is thus, by the discussion of ¶ the previous section, an analytic function over the whole z-plane. Like its  §  ¶  For a proof see, for example, K. F. Riley, Mathematical Methods for the Physical Sciences  Cam- bridge: Cambridge University Press, 1974 , p. 446.  Functions that are analytic in the whole z-plane are usually called integral or entire functions.  ∞ cid:4   n=0  zn n!  832   24.4 SOME ELEMENTARY FUNCTIONS  real-variable counterpart it is called the exponential function; also like its real counterpart it is equal to its own derivative.  The multiplication of two exponential functions results in a further exponential  function, in accordance with the corresponding result for real variables.  cid:1 Show that exp z1 exp z2 = exp z1 + z2 .  From the series expansion  24.15  of exp z1 and a similar expansion for exp z2, it is clear that the coeﬃcient of zr 2 in the corresponding series expansion of exp z1 exp z2 is simply 1  r!s! .  1zs  But, from  24.15  we also have  ∞ cid:4   n=0  exp z1 + z2  =   z1 + z2 n  .  n!  In order to ﬁnd the coeﬃcient of zr term in which n = r + s, namely  1zs  2 in this expansion, we clearly have to consider the   z1 + z2 r+s  1  =   r + s !   r + s !  r+sC0zr+s  1 + ··· + r+sCszr 2 + ··· + r+sCr+szr+s 1zs  2  .   cid:5    cid:6   The coeﬃcient of zr  1zs  2 in this is given by  r+sCs  1   r + s !  1  =   r + s !  s!r!   r + s !  =  1  .  r!s!  Thus, since the corresponding coeﬃcients on the two sides are equal, and all the series involved are absolutely convergent for all z, we can conclude that exp z1 exp z2 = exp z1 + z2 .  cid:2   As an extension of  24.15  we may also deﬁne the complex exponent of a real  number a > 0 by the equation  az = exp z ln a ,   24.16   where ln a is the natural logarithm of a. The particular case a = e and the fact that ln e = 1 enable us to write exp z interchangeably with ez. If z is real then the deﬁnition agrees with the familiar one.  The result for z = iy,  exp iy = cos y + i sin y,   24.17   has been met already in equation  3.23 . Its immediate extension is  exp z =  exp x  cos y + i sin y .   24.18   As z varies over the complex plane, the modulus of exp z takes all real positive values, except that of 0. However, two values of z that diﬀer by 2πki, for any integer k, produce the same value of exp z, as given by  24.18 , and so exp z is  periodic with period 2πi. If we denote exp z by t, then the strip −π < y ≤ π in  the z-plane corresponds to the whole of the t-plane, except for the point t = 0.  The sine, cosine, sinh and cosh functions of a complex variable are deﬁned from the exponential function exactly as are those for real variables. The functions  833   COMPLEX VARIABLES  derived from them  e.g. tan and tanh , the identities they satisfy and their derivative properties are also just as for real variables. In view of this we will not give them further attention here.  The inverse function of exp z is given by w, the solution of  exp w = z.   24.19   This inverse function was discussed in chapter 3, but we mention it again here for completeness. By virtue of the discussion following  24.18 , w is not uniquely deﬁned and is indeterminate to the extent of any integer multiple of 2πi. If we express z as  z = r exp iθ,  where r is the  real  modulus of z and θ is its argument  −π < θ ≤ π , then  multiplying z by exp 2ikπ , where k is an integer, will result in the same complex number z. Thus we may write  where k is an integer. If we denote w in  24.19  by  z = r exp[i θ + 2kπ ],  w = Ln z = ln r + i θ + 2kπ ,   24.20   where ln r is the natural logarithm  to base e  of the real positive quantity r, then Ln z is an inﬁnitely multivalued function of z. Its principal value, denoted by ln z,  is obtained by taking k = 0 so that its argument lies in the range −π to π. Thus  ln z = ln r + iθ,  with −π < θ ≤ π.   24.21   Now that the logarithm of a complex variable has been deﬁned, deﬁnition  24.16  of a general power can be extended to cases other than those in which a is real  and positive. If t   cid:3 = 0  and z are both complex, then the zth power of t is deﬁned  by  tz = exp z Ln t .   24.22   Since Ln t is multivalued, so is this deﬁnition. Its principal value is obtained by giving Ln t its principal value, ln t.  If t   cid:3 = 0  is complex but z is real and equal to 1 n, then  24.22  provides a  deﬁnition of the nth root of t. Because of the multivaluedness of Ln t, there will be more than one nth root of any given t.  cid:1 Show that there are exactly n distinct nth roots of t.  From  24.22  the nth roots of t are given by  t1 n = exp  Ln t  .   cid:8    cid:7   1 n  834   24.5 MULTIVALUED FUNCTIONS AND BRANCH CUTS  On the RHS let us write t as follows:  where k is an integer. We then obtain   cid:14   t1 n = exp  ln r + i   θ + 2kπ   t = r exp[i θ + 2kπ ],   cid:13    cid:13   1 n  = r1 n exp  i   cid:14   ,  n  θ + 2kπ   n  where k = 0, 1, . . . , n − 1; for other values of k we simply recover the roots already found. Thus t has n distinct nth roots.  cid:2   24.5 Multivalued functions and branch cuts  In the deﬁnition of an analytic function, one of the conditions imposed was that the function is single-valued. However, as shown in the previous section, the logarithmic function, a complex power and a complex root are all multivalued. Nevertheless, it happens that the properties of analytic functions can still be applied to these and other multivalued functions of a complex variable provided that suitable care is taken. This care amounts to identifying the branch points of the multivalued function f z  in question. If z is varied in such a way that its path in the Argand diagram forms a closed curve that encloses a branch point, then, in general, f z  will not return to its original value.  For deﬁniteness let us consider the multivalued function f z  = z1 2 and express z as z = r exp iθ. From ﬁgure 24.1 a , it is clear that, as the point z traverses any closed contour C that does not enclose the origin, θ will return to its original that does value after one complete circuit. However, for any closed contour C  enclose the origin, after one circuit θ → θ + 2π  see ﬁgure 24.1 b  . Thus, for the function f z  = z1 2, after one circuit   cid:7   r1 2 exp iθ 2  → r1 2 exp[i θ + 2π  2] = −r1 2 exp iθ 2 .  In other words, the value of f z  changes around any closed loop enclosing the  origin; in this case f z  → −f z . Thus z = 0 is a branch point of the function f z  = z1 2.  We note in this case that if any closed contour enclosing the origin is traversed twice then f z  = z1 2 returns to its original value. The number of loops around a branch point required for any given function f z  to return to its original value depends on the function in question, and for some functions  e.g. Ln z, which also has a branch point at the origin  the original value is never recovered.  In order that f z  may be treated as single-valued, we may deﬁne a branch cut in the Argand diagram. A branch cut is a line  or curve  in the complex plane and may be regarded as an artiﬁcial barrier that we must not cross. Branch cuts are positioned in such a way that we are prevented from making a complete  835   y  C  r  θ  COMPLEX VARIABLES  y  y  x  x  x  r  θ   cid:7   C   a    b    c   Figure 24.1  a  A closed contour not enclosing the origin;  b  a closed contour enclosing the origin;  c  a possible branch cut for f z  = z1 2.  circuit around any one branch point, and so the function in question remains single-valued.  For the function f z  = z1 2, we may take as a branch cut any curve starting at the origin z = 0 and extending out to z = ∞ in any direction, since all  such curves would equally well prevent us from making a closed loop around the branch point at the origin. It is usual, however, to take the cut along either the real or the imaginary axis. For example, in ﬁgure 24.1 c , we take the cut as the positive real axis. By agreeing not to cross this cut, we restrict θ to lie in the  range 0 ≤ θ < 2π, and so keep f z  single-valued.  These ideas are easily extended to functions with more than one branch point.   cid:1 Find the branch points of f z  = branch cuts.  We begin by writing f z  as  √  cid:24    cid:24   f z  =  z2 + 1 =   z − i  z + i .  z2 + 1, and hence sketch suitable arrangements of  As shown above, the function g z  = z1 2 has a branch point at z = 0. Thus we might root equal to zero, i.e. at z = i and z = −i. expect f z  to have branch points at values of z that make the expression under the square  As shown in ﬁgure 24.2 a , we use the notation  We can therefore write f z  as  z − i = r1 exp iθ1 √ r1r2 exp iθ1 2  exp iθ2 2  =  and  z + i = r2 exp iθ2. √   cid:18    cid:19   r1r2 exp  i θ1 + θ2  2  .  f z  =  Let us now consider how f z  changes as we make one complete circuit around various  closed loops C in the Argand diagram. If C encloses   i  neither branch point, then θ1 → θ1, θ2 → θ2 and so f z  → f z ;  ii  z = i but not z = −i, then θ1 → θ1 + 2π, θ2 → θ2 and so f z  → −f z ;  836   24.6 SINGULARITIES AND ZEROS OF COMPLEX FUNCTIONS  z  x  y  i  r1  θ1  r2  −i  θ2  y  i  −i  y  i  −i  x  x   a    b    c   Figure 24.2  a  Coordinates used in the analysis of the branch points of f z  =  z2 + 1 1 2;  b  one possible arrangement of branch cuts;  c  another possible branch cut, which is ﬁnite.   iii  z = −i but not z = i, then θ1 → θ1, θ2 → θ2 + 2π and so f z  → −f z ;  iv  both branch points, then θ1 → θ1 + 2π, θ2 → θ2 + 2π and so f z  → f z . Thus, as expected, f z  changes value around loops containing either z = i or z = −i  but  not both . We must therefore choose branch cuts that prevent us from making a complete loop around either branch point; one suitable choice is shown in ﬁgure 24.2 b .  For this f z , however, we have noted that after traversing a loop containing both branch points the function returns to its original value. Thus we may choose an alternative, ﬁnite, branch cut that allows this possibility but still prevents us from making a complete loop around just one of the points. A suitable cut is shown in ﬁgure 24.2 c .  cid:2   24.6 Singularities and zeros of complex functions  A singular point of a complex function f z  is any point in the Argand diagram at which f z  fails to be analytic. We have already met one sort of singularity, the branch point, and in this section we will consider other types of singularity as well as discuss the zeros of complex functions.  If f z  has a singular point at z = z0 but is analytic at all points in some neighbourhood containing z0 but no other singularities, then z = z0 is called an isolated singularity.  Clearly, branch points are not isolated singularities.   The most important type of isolated singularity is the pole. If f z  has the form  where n is a positive integer, g z  is analytic at all points in some neighbourhood  containing z = z0 and g z0   cid:3 = 0, then f z  has a pole of order n at z = z0. An  alternative  though equivalent  deﬁnition is that  f z  =  g z    z − z0 n ,  [ z − z0 nf z ] = a,  lim z→z0  837   24.23    24.24    COMPLEX VARIABLES  where a is a ﬁnite, non-zero complex number. We note that if the above limit is equal to zero, then z = z0 is a pole of order less than n, or f z  is analytic there; if the limit is inﬁnite then the pole is of an order greater than n. It may also be  shown that if f z  has a pole at z = z0, then f z  → ∞ as z → z0 from any  § direction in the Argand diagram.  24.24  is satisﬁed, then z = z0 is called an essential singularity.  cid:1 Find the singularities of the functions  If no ﬁnite value of n can be found such that   i  f z  =  1  1 − z  − 1 1 + z  ,   ii  f z  = tanh z.   i  If we write f z  as  poles  at z = 1 and z = −1.   ii  In this case we write  f z  =  1  1 − z  − 1 1 + z  =  2z   1 − z  1 + z   ,  we see immediately from either  24.23  or  24.24  that f z  has poles of order 1  or simple  f z  = tanh z =  sinh z cosh z  =  exp z − exp −z  exp z + exp −z   .  Thus f z  has a singularity when exp z = − exp −z  or, equivalently, when  exp z = exp[i 2n + 1 π] exp −z ,  where n is any integer. Equating the arguments of the exponentials we ﬁnd z =  n + 1 for integer n.  2  πi,  Furthermore, using l’H ˆopital’s rule  see chapter 4  we have    [z −  n + 1  2  πi] sinh z  cosh z  lim z→ n+ 1  2  πi  .    .  [z −  n + 1  2  πi] cosh z + sinh z  = 1.  sinh z Therefore, from  24.24 , each singularity is a simple pole.  cid:2   2  πi  = lim z→ n+ 1  Another type of singularity exists at points for which the value of f z  takes an indeterminate form such as 0 0 but limz→z0 f z  exists and is independent of the direction from which z0 is approached. Such points are called removable singularities.  cid:1 Show that f z  =  sin z  z has a removable singularity at z = 0.  It is clear that f z  takes the indeterminate form 0 0 at z = 0. However, by expanding sin z as a power series in z, we ﬁnd   cid:7   f z  =  z − z3  3!  +  z5 5!  − ···  = 1 − z2  +  3!  z4 5!  − ··· .  1 z  §  Although perhaps intuitively obvious, this result really requires formal demonstration by analysis.   cid:8   838   24.7 CONFORMAL TRANSFORMATIONS  Thus limz→0 f z  = 1 independently of the way in which z → 0, and so f z  has a removable singularity at z = 0.  cid:2   as z or R,  An expression common in mathematics, but which we have so far avoided using explicitly in this chapter, is ‘z tends to inﬁnity’. For a real variable such ‘tending to inﬁnity’ has a reasonably well deﬁned meaning. For a complex variable needing a two-dimensional plane to represent it, the meaning is not intrinsically well deﬁned. However, it is convenient to have a unique meaning and this is provided by the following deﬁnition: the behaviour of f z  at inﬁnity is given by that of f 1 ξ  at ξ = 0, where ξ = 1 z.  cid:1 Find the behaviour at inﬁnity of  i  f z  = a + bz f z  = exp z.  −2,  ii  f z  = z 1 + z2  and  iii    cid:11 ∞   i  f z  = a + bz   iii  f z  = exp z : f 1 ξ  =  −2: on putting z = 1 ξ, f 1 ξ  = a + bξ2, which is analytic at ξ = 0;  thus f is analytic at z = ∞.  ii  f z  = z 1 + z2 : f 1 ξ  = 1 ξ + 1 ξ3; thus f has a pole of order 3 at z = ∞.  −n; thus f has an essential singularity at z = ∞.  cid:2  We conclude this section by brieﬂy mentioning the zeros of a complex function. As the name suggests, if f z0  = 0 then z = z0 is called a zero of the function f z . Zeros are classiﬁed in a similar way to poles, in that if  −1ξ  0  n!   f z  =  z − z0 ng z ,  where n is a positive integer and g z0   cid:3 = 0, then z = z0 is called a zero of order  n of f z . If n = 1 then z = z0 is called a simple zero. It may further be shown that if z = z0 is a zero of order n of f z  then it is also a pole of order n of the function 1 f z .  We will return in section 24.11 to the classiﬁcation of zeros and poles in terms  of their series expansions.  24.7 Conformal transformations  We now turn our attention to the subject of transformations, by which we mean a change of coordinates from the complex variable z = x + iy to another, say w = r + is, by means of a prescribed formula:  w = g z  = r x, y  + is x, y .  Under such a transformation, or mapping, the Argand diagram for the z-variable is transformed into one for the w-variable, although the complete z-plane might be mapped onto only a part of the w-plane, or onto the whole of the w-plane, or onto some or all of the w-plane covered more than once.  We shall consider only those mappings for which w and z are related by a function w = g z  and its inverse z = h w  with both functions analytic, except possibly at a few isolated points; such mappings are called conformal. Their  839   COMPLEX VARIABLES  y  s  C1  z2  C2  z1  z0  w = g z   θ2  θ1  x  w1  w0  φ2  φ1   cid:7  1  C  w2 C   cid:7  2  r  Figure 24.3 Two curves C1 and C2 in the z-plane, which are mapped onto C   cid:7  2 in the w-plane.   cid:7  1 and C  important properties are that, except at points at which g zero or inﬁnite:   cid:7    cid:7   z , and hence h   z , is   i  continuous lines in the z-plane transform into continuous lines in the   ii  the angle between two intersecting curves in the z-plane equals the angle  between the corresponding curves in the w-plane;   iii  the magniﬁcation, as between the z-plane and the w-plane, of a small line element in the neighbourhood of any particular point is independent of the direction of the element;   iv  any analytic function of z transforms to an analytic function of w and  w-plane;  vice versa.  Result  i  is immediate, and results  ii  and  iii  can be justiﬁed by the following argument. Let two curves C1 and C2 pass through the point z0 in the z-plane and let z1 and z2 be two points on their respective tangents at z0, each a distance ρ from z0. The same prescription with w replacing z describes the transformed situation; however, the transformed tangents may not be straight lines and the distances of w1 and w2 from w0 have not yet been shown to be equal. This situation is illustrated in ﬁgure 24.3.  In the z-plane z1 and z2 are given by  z1 − z0 = ρ exp iθ1  z2 − z0 = ρ exp iθ2.  The corresponding descriptions in the w-plane are  w1 − w0 = ρ1 exp iφ1  w2 − w0 = ρ2 exp iφ2.  The angles θi and φi are clear from ﬁgure 24.3. The transformed angles φi are those made with the r-axis by the tangents to the transformed curves at their  and  and  840   24.7 CONFORMAL TRANSFORMATIONS  point of intersection. Since any ﬁnite-length tangent may be curved, wi is more strictly given by wi − w0 = ρi exp i φi + δφi , where δφi → 0 as ρi → 0, i.e. as ρ → 0.  Now since w = g z , where g is analytic, we have   cid:7    cid:8  w1 − w0 z1 − z0  cid:15  exp[i φ1 + δφ1 − θ1 ]  lim z1→z0   cid:20  cid:20  cid:20  cid:20    cid:7    cid:8   w2 − w0 z2 − z0  =  dg dz  ,  z=z0   cid:15  exp[i φ2 + δφ2 − θ2 ]  = lim z2→z0   cid:12   = lim ρ→0  ρ2 ρ   cid:12   lim ρ→0  ρ1 ρ  which may be written as   cid:7   = g   z0 .   24.25   Comparing magnitudes and phases  i.e. arguments  in the equalities  24.25  gives the stated results  ii  and  iii  and adds quantitative information to them, namely that for small line elements   cid:7   ρ1 ρ  ≈ g  ≈ ρ2   z0 , φ1 − θ1 ≈ φ2 − θ2 ≈ arg g  ρ   cid:7    z0 .   24.26    24.27   with an ordinary equality sign, since the angles are only deﬁned in the limit  For strict comparison with result  ii ,  24.27  must be written as θ1− θ2 = φ1− φ2, ρ → 0 when  24.27  becomes a true identity. We also see from  24.26  that the linear magniﬁcation factor is g  z0 ; similarly, small areas are magniﬁed by  z0 2. g  cid:7    cid:7   Since in the neighbourhoods of corresponding points in a transformation angles are preserved and magniﬁcations are independent of direction, it follows that small plane ﬁgures are transformed into ﬁgures of the same shape, but, in general, ones that are magniﬁed and rotated  though not distorted . However, we also note  z  through which line elements are that at points where g rotated is undeﬁned; these are called critical points of the transformation.   z  = 0, the angle arg g   cid:7    cid:7   The ﬁnal result  iv  is perhaps the most important property of conformal transformations. If f z  is an analytic function of z and z = h w  is also analytic, then F w  = f h w   is analytic in w. Its importance lies in the further conclusions it allows us to draw from the fact that, since f is analytic, the real and imaginary parts of f = φ + iψ are necessarily solutions of  ∂2φ ∂x2 +  ∂2φ ∂y2 = 0  and  ∂2ψ ∂x2 +  ∂2ψ ∂y2 = 0.   24.28   Since the transformation property ensures that F = Φ + iΨ is also analytic, we can conclude that its real and imaginary parts must themselves satisfy Laplace’s equation in the w-plane:  ∂2Φ ∂r2 +  ∂2Φ ∂s2 = 0  ∂2Ψ ∂r2 +  ∂2Ψ ∂s2 = 0.   24.29   and  841   COMPLEX VARIABLES  y  i  P  s   cid:7   R  w = g z   Q  R  S  T  x   cid:7   P   cid:7  Q  cid:7   T  r   cid:7   S  Figure 24.4 Transforming the upper half of the z-plane into the interior of the unit circle in the w-plane, in such a way that z = i is mapped onto w = 0  and the points x = ±∞ are mapped onto w = 1.  Further, suppose that  say  Re f z  = φ is constant over a boundary C in the z-plane; then Re F w  = Φ is constant over C in the z-plane. But this is the same being as saying that Re F w  is constant over the boundary C the curve into which C is transformed by the conformal transformation w = g z . This result is exploited extensively in the next chapter to solve Laplace’s equation for a variety of two-dimensional geometries.  in the w-plane, C   cid:7    cid:7   Examples of useful conformal transformations are numerous. For instance, w = z + b, w =  exp iφ z and w = az correspond, respectively, to a translation by b, a rotation through an angle φ and a stretching  or contraction  in the radial direction  for a real . These three examples can be combined into the general linear transformation w = az + b, where, in general, a and b are complex. Another example is the inversion mapping w = 1 z, which maps the interior of the unit circle to the exterior and vice versa. Other, more complicated, examples also exist.  cid:1 Show that if the point z0 lies in the upper half of the z-plane then the transformation  w =  exp iφ   z − z0 z − z ∗ 0  maps the upper half of the z-plane into the interior of the unit circle in the w-plane. Hence  ﬁnd a similar transformation that maps the point z = i onto w = 0 and the points x = ±∞  onto w = 1.  Taking the modulus of w, we have   cid:20  cid:20  cid:20  cid:20  exp iφ    cid:20  cid:20  cid:20  cid:20  =   cid:20  cid:20  cid:20  cid:20  z − z0  z − z  ∗ 0   cid:20  cid:20  cid:20  cid:20  .  z − z0 z − z ∗ 0  w =  However, since the complex conjugate z  both lie in the upper half of the z-plane then z − z0 ≤ z − z  ∗ ; thus w ≤ 1, as required. 0 is the reﬂection of z0 in the real axis, if z and z0  We also note that  i  the equality holds only when z lies on the real axis, and so this axis is mapped onto the boundary of the unit circle in the w-plane;  ii  the point z0 is mapped onto w = 0, the origin of the w-plane.  ∗ 0  By ﬁxing the images of two points in the z-plane, the constants z0 and φ can also be ﬁxed. Since we require the point z = i to be mapped onto w = 0, we have immediately  842   24.7 CONFORMAL TRANSFORMATIONS  y  s  w = g z   w1  φ5  φ1  φ2  φ3  w2  w4  r  w5  φ4  w3  x1 x2  x3  x4  x5  x  Figure 24.5 Transforming the upper half of the z-plane into the interior of a polygon in the w-plane, in such a way that the points x1, x2, . . . , xn are mapped onto the vertices w1, w2, . . . , wn of the polygon with interior angles φ1, φ2, . . . , φn.  z0 = i. By further requiring z = ±∞ to be mapped onto w = 1, we ﬁnd 1 = w = exp iφ  and so φ = 0. The required transformation is therefore  and is illustrated in ﬁgure 24.4.  cid:2   z − i  z + i  ,  w =  We conclude this section by mentioning the rather curious Schwarz–Christoﬀel § Suppose, as shown in ﬁgure 24.5, that we are interested in a transformation.  ﬁnite  number of points x1, x2, . . . , xn on the real axis in the z-plane. Then by means of the transformation  w =  A  z   ξ − x1  φ1 π −1 ξ − x2  φ2 π −1 ···  ξ − xn  φn π −1 dξ  + B,   24.30    cid:15    cid:12    cid:21   0  we may map the upper half of the z-plane onto the interior of a closed polygon in the w-plane having n vertices w1, w2, . . . , wn  which are the images of x1, x2, . . . , xn  with corresponding interior angles φ1, φ2, . . . , φn, as shown in ﬁgure 24.5. The real axis in the z-plane is transformed into the boundary of the polygon itself. The constants A and B are complex in general and determine the position, size and orientation of the polygon. It is clear from  24.30  that dw dz = 0 at x = x1, x2, . . . , xn, and so the transformation is not conformal at these points.  There are various subtleties associated with the use of the Schwarz–Christoﬀel transformation. For example, if one of the points on the real axis in the z-plane  usually xn  is taken at inﬁnity, then the corresponding factor in  24.30   i.e. the  one involving xn  is not present. In this case, the point s  x = ±∞ are considered  as one point, since they transform to a single vertex of the polygon in the w-plane.  §  Strictly speaking, the use of this transformation requires an understanding of complex integrals, which are discussed in section 24.8.  843   COMPLEX VARIABLES  y  s  ib  w3  φ3  w = g z   x1  −1  x2  1  x  φ1  φ2  w1  −a  w2 a  r  Figure 24.6 Transforming the upper half of the z-plane into the interior of a triangle in the w-plane.  We can also map the upper half of the z-plane into an inﬁnite open polygon  by considering it as the limiting case of some closed polygon.   cid:1 Find a transformation that maps the upper half of the z-plane into the triangular region shown in ﬁgure 24.6 in such a way that the points x1 = −1 and x2 = 1 are mapped into the points w = −a and w = a, respectively, and the point x3 = ±∞ is mapped into −a   0 of the w-plane, as shown in ﬁgure 24.7.  w = ib. Hence ﬁnd a transformation that maps the upper half of the z-plane into the region  Let us denote the angles at w1 and w2 in the w-plane by φ1 = φ2 = φ, where φ = tan Since x3 is taken at inﬁnity, we may omit the corresponding factor in  24.30  to obtain   cid:12   cid:12    cid:21   cid:21   0  0  w =  A  =  A   cid:15   + B  z  z   ξ + 1  φ π −1 ξ − 1  φ π −1 dξ  ξ2 − 1  φ π −1 dξ   cid:15   + B.  −1 b a .   24.31   The required transformation may then be found by ﬁxing the constants A and B as follows. Since the point z = 0 lies on the line segment x1x2, it will be mapped onto the line segment w1w2 in the w-plane, and by symmetry must be mapped onto the point w = 0. Thus setting z = 0 and w = 0 in  24.31  we obtain B = 0. An expression for A can be found in the form of an integral by setting  for example  z = 1 and w = a in  24.31 .  We may consider the region in the w-plane in ﬁgure 24.7 to be the limiting case of the triangular region in ﬁgure 24.6 with the vertex w3 at inﬁnity. Thus we may use the above, but with the angles at w1 and w2 set to φ = π 2. From  24.31 , we obtain   cid:21   dξ cid:24  ξ2 − 1  z  0  w = A  = iA sin  −1 z.  By setting z = 1 and w = a, we ﬁnd iA = 2a π, so the required transformation is  w =  2a π  −1 z.  cid:2   sin  844   24.8 COMPLEX INTEGRALS  y  w3  s  w3  w = g z   x1  −1  x2  1  x  φ1  w1  −a  φ2  w2 a  r  Figure 24.7 Transforming the upper half of the z-plane into the interior of  the region −a   0 in the w-plane.  24.8 Complex integrals  Corresponding to integration with respect to a real variable, it is possible to deﬁne integration with respect to a complex variable between two complex limits. Since the z-plane is two-dimensional there is clearly greater freedom and hence ambiguity in what is meant by a complex integral. If a complex function f z  is single-valued and continuous in some region R in the complex plane, then we can deﬁne the complex integral of f z  between two points A and B along some curve in R; its value will depend, in general, upon the path taken between A and B  see ﬁgure 24.8 . However, we will ﬁnd that for some paths that are diﬀerent but bear a particular relationship to each other the value of the integral does not depend upon which of the paths is adopted.  Let a particular path C be described by a continuous  real  parameter t   α ≤ t ≤ β  that gives successive positions on C by means of the equations  x = x t ,  y = y t ,   24.32   with t = α and t = β corresponding to the points A and B, respectively. Then the integral along path C of a continuous function f z  is written   cid:21   C  and can be given explicitly as a sum of real integrals as follows:  f z  dz =   u + iv  dx + idy    cid:21    cid:21   u dx −   cid:21    cid:21    cid:21   v dy + i  u dy + i  v dx  C  dt −  β  v  dy dt  α  C  dt + i  C dy dt  β  u  α  u  dx dt  dt + i  β  v  dx dt  α  dt.   cid:21    cid:21   cid:21   cid:21   C  C  β  α  =  =   24.33    24.34    cid:21   f z  dz  C  845   COMPLEX VARIABLES  y  C2  C1  A  B  C3  x  Figure 24.8 Some alternative paths for the integral of a function f z  between A and B.  The question of when such an integral exists will not be pursued, except to state that a suﬃcient condition is that dx dt and dy dt are continuous.  cid:1 Evaluate the complex integral of f z  = z at z = R.  −1 along the circle z = R, starting and ﬁnishing  The path C1 is parameterised as follows  ﬁgure 24.9 a  :  z t  = R cos t + iR sin t,  0 ≤ t ≤ 2π,  whilst f z  is given by  f z  =  1  x + iy  =  x − iy x2 + y2 .  Thus the real and imaginary parts of f z  are  u =  x  R cos t  =  x2 + y2  R2  Hence, using expression  24.34 ,   cid:21   1 z  C1  dz =  2π  cos t   −R sin t  dt −   cid:21   R  2π  cos t  + i  R cos t dt + i = 0 + 0 + iπ + iπ = 2πi.  cid:2   R  0   cid:21   0  and   cid:21   x2 + y2  v =  −y  cid:8   cid:7 − sin t  cid:7 − sin t   cid:8   R  2π  R  2π   cid:21   0  0  = − R sin t  .  R2  R cos t dt   −R sin t  dt   24.35   With a bit of experience, the reader may be able to evaluate integrals like the LHS of  24.35  directly without having to write them as four separate real integrals. In the present case,   cid:21    cid:21   dt =  i dt = 2πi.   24.36    cid:21   2π  0  dz z  =  C1  0  −R sin t + iR cos t  2π  R cos t + iR sin t  846   24.8 COMPLEX INTEGRALS  y  y  C1  R  t  x  −R  C2  R  t  C3b  s = 1  −R  R  x  y  iR  C3a  t = 0  R  x   a    b    c   Figure 24.9 Diﬀerent paths for an integral of f z  = z details.  −1. See the text for  This very important result will be used many times later, and the following should be carefully noted:  i  its value,  ii  that this value is independent of R.  In the above example the contour was closed, and so it began and ended at the same point in the Argand diagram. We can evaluate complex integrals along open paths in a similar way.   cid:1 Evaluate the complex integral of f z  = z  −1 along the following paths  see ﬁgure 24.9 :   i  the contour C2 consisting of the semicircle z = R in the half-plane y ≥ 0,   ii  the contour C3 made up of the two straight lines C3a and C3b.   i  This is just as in the previous example, except that now 0 ≤ t ≤ π. With this change, we have from  24.35  or  24.36  that  cid:21   dz z  C2  = πi.   24.37    ii  The straight lines that make up the countour C3 may be parameterised as follows:  With these parameterisations the required integrals may be written  for 0 ≤ t ≤ 1; for 0 ≤ s ≤ 1.  C3a, C3b,   cid:21    cid:21   z =  1 − t R + itR z = −sR + i 1 − s R  cid:21   dz z  =  C3  −R + iR  R + t −R + iR   1  0  dt +  1  0  iR + s −R − iR   −R − iR  cid:1   −1 ln a+bt  If we could take over from real-variable theory that, for real t, even if a and b are complex, then these integrals could be evaluated immediately. However, to do this would be presuming to some extent what we wish to show, and so the evaluation  −1 dt = b   a+bt   ds.   24.38   847   must be made in terms of entirely real integrals. For example, the ﬁrst is given by   cid:21   −R + iR  R 1 − t  + itR  1  0  dt =  COMPLEX VARIABLES   cid:21   cid:21   1 2  1  0  0  1   cid:13   =  =  = 0 +   cid:21    −1 + i  1 − t − it   cid:21   1 − t 2 + t2 2t − 1  cid:14 1 1 − 2t + 2t2 dt + i ln 1 − 2t + 2t2   cid:10  cid:23   cid:22  − π 2   cid:9   −  π 2  i 2  +  =  0  dt  1  0  i 2  πi 2  .  = πi.  cid:2   dz z  C3  1   cid:16  1 − 2t + 2t2 dt t − 1   cid:30   −1  2  2 tan   cid:31  cid:17   1 2  1  0  The second integral on the RHS of  24.38  can also be shown to have the value πi 2. Thus  Considering the results of the preceding two examples, which have common integrands and limits, some interesting observations are possible. Firstly, the two  integrals from z = R to z = −R, along C2 and C3, respectively, have the same value, even though the paths taken are diﬀerent. It also follows that if we took a closed path C4, given by C2 from R to −R and C3 traversed backwards from −R −1 would be zero  both parts contributing to R, then the integral round C4 of z equal and opposite amounts . This is to be compared with result  24.36 , in which closed path C1, beginning and ending at the same place as C4, yields a value 2πi. It is not true, however, that the integrals along the paths C2 and C3 are equal for any function f z , or, indeed, that their values are independent of R in general.  cid:1 Evaluate the complex integral of f z  = Re z along the paths C1, C2 and C3 shown in ﬁgure 24.9.   cid:21   cid:21   0   cid:21   cid:21   C1  C2   cid:21    i  If we take f z  = Re z and the contour C1 then  Re z dz =  2π  R cos t −R sin t + iR cos t  dt = iπR2.   ii  Using C2 as the contour,   iii  Finally the integral along C3 = C3a + C3b is given by  Re z dz =  π  R cos t −R sin t + iR cos t  dt = 1  2 iπR2.  0   cid:21  2 R2 −1 + i  + 1  1  0  = 1  Re z dz =  C3   1 − t R −R + iR  dt +  1   −sR  −R − iR  ds  2 R2 1 + i  = iR2.  cid:2    cid:21   0  The results of this section demonstrate that the value of an integral between the same two points may depend upon the path that is taken between them but, at the same time, suggest that, under some circumstances, the value is independent of the path. The general situation is summarised in the result of the next section,  848   24.9 CAUCHY’S THEOREM  namely Cauchy’s theorem, which is the cornerstone of the integral calculus of complex variables.  Before discussing Cauchy’s theorem, however, we note an important result concerning complex integrals that will be of some use later. Let us consider the integral of a function f z  along some path C. If M is an upper bound on the  value of f z  on the path, i.e. f z  ≤ M on C, and L is the length of the path C,   cid:20  cid:20  cid:20  cid:20  cid:21   C   cid:21    cid:20  cid:20  cid:20  cid:20  ≤  c   cid:21   C  f z  dz  f z dz ≤ M  dl = ML.   24.39   then  It is straightforward to verify that this result does indeed hold for the complex integrals considered earlier in this section.     C .  24.9 Cauchy’s theorem  Cauchy’s theorem states that if f z  is an analytic function, and f at each point within and on a closed contour C, then   cid:7    z  is continuous  f z  dz = 0.   24.40   0  C  In this statement and from now on we denote an integral around a closed contour by  To prove this theorem we will need the two-dimensional form of the divergence theorem, known as Green’s theorem in a plane  see section 11.3 . This says that if p and q are two functions with continuous ﬁrst derivatives within and on a closed contour C  bounding a domain R  in the xy-plane, then  With f z  = u + iv and dz = dx + i dy, this can be applied to   cid:7    cid:21  cid:21  0  R  C   cid:8   0  ∂p ∂x  +  ∂q ∂y  dxdy =  0  cid:14   C   p dy − q dx . 0  C   u dx − v dy  + i  cid:13    cid:21  cid:21   C  ∂ −v   dx dy + i  R  ∂y   cid:14   +  ∂u ∂x  I =  f z  dz =   v dx + u dy   to give   cid:21  cid:21    cid:13   ∂ −u   ∂ −v   ∂x  +  R  ∂y  I =  dx dy.   24.42    24.41   Now, recalling that f z  is analytic and therefore that the Cauchy–Riemann relations  24.5  apply, we see that each integrand is identically zero and thus I is also zero; this proves Cauchy’s theorem.  In fact, the conditions of the above proof are more stringent than they need  z  is not necessary for the proof of Cauchy’s theorem,  be. The continuity of f   cid:7   849   COMPLEX VARIABLES  y  R  B  C2  x  C1  A  Figure 24.10 Two paths C1 and C2 enclosing a region R.  analyticity of f z  within and on C being suﬃcient. However, the proof then § becomes more complicated and is too long to be given here.  The connection between Cauchy’s theorem and the zero value of the integral  −1 around the composite path C4 discussed towards the end of the previous −1 is analytic in the two regions of the z-plane  of z section is apparent: the function z enclosed by contours  C2 and C3a  and  C2 and C3b .  cid:1 Suppose two points A and B in the complex plane are joined by two diﬀerent paths C1 and C2. Show that if f z  is an analytic function on each path and in the region enclosed by the two paths, then the integral of f z  is the same along C1 and C2.  The situation is shown in ﬁgure 24.10. Since f z  is analytic in R, it follows from Cauchy’s  theorem that we have cid:21   since C1 − C2 forms a closed contour enclosing R. Thus we immediately obtain  C1  C2  f z  dz =  f z  dz = 0,  C1−C2   cid:21   f z  dz −  cid:21   0   cid:21   and so the values of the integrals along C1 and C2 are equal.  cid:2   f z  dz =  f z  dz,  C1  C2  An important application of Cauchy’s theorem is in proving that, in some cases, it is possible to deform a closed contour C into another contour γ in such a way that the integrals of a function f z  around each of the contours have the same value.  §  The reader may refer to almost any book that is devoted to complex variables and the theory of functions.  850   24.10 CAUCHY’S INTEGRAL FORMULA  y  C  C1  γ  C2  Figure 24.11 The contour used to prove the result  24.43 .  x  0  C  0  γ  0  Γ  0  C  0  γ   cid:1 Consider two closed contours C and γ in the Argand diagram, γ being suﬃciently small that it lies completely within C. Show that if the function f z  is analytic in the region between the two contours then  f z  dz =  f z  dz.   24.43   To prove this result we consider a contour as shown in ﬁgure 24.11. The two close parallel lines C1 and C2 join γ and C, which are ‘cut’ to accommodate them. The new contour Γ so formed consists of C, C1, γ and C2.  Within the area bounded by Γ, the function f z  is analytic, and therefore, by Cauchy’s  theorem  24.40 ,  f z  dz = 0.   24.44   Now the parts C1 and C2 of Γ are traversed in opposite directions, and in the limit lie on top of each other, and so their contributions to  24.44  cancel. Thus  f z  dz +  f z  dz = 0.   24.45   The sense of the integral round γ is opposite to the conventional  anticlockwise  one, and so by traversing γ in the usual sense, we establish the result  24.43 .  cid:2   A sort of converse of Cauchy’s theorem is known as Morera’s theorem, which states that if f z  is a continuous function of z in a closed domain R bounded by a curve C and, further,  C f z  dz = 0, then f z  is analytic in R.     24.10 Cauchy’s integral formula  Another very important theorem in the theory of complex variables is Cauchy’s integral formula, which states that if f z  is analytic within and on a closed  851   contour C and z0 is a point within C then  COMPLEX VARIABLES  0  f z0  =  1 2πi  f z   z − z0  C  dz.   24.46   This formula is saying that the value of an analytic function anywhere inside a and that the  § closed contour is uniquely determined by its values on the contour speciﬁc expression  24.46  can be given for the value at the interior point.  We may prove Cauchy’s integral formula by using  24.43  and taking γ to be a circle centred on the point z = z0, of small enough radius ρ that it all lies inside  C. Then, since f z  is analytic inside C, the integrand f z   z − z0  is analytic  in the space between C and γ. Thus, from  24.43 , the integral around γ has the same value as that around C.  We then use the fact that any point z on γ is given by z = z0 + ρ exp iθ  and  so dz = iρ exp iθ dθ . Thus the value of the integral around γ is given by  0  I =  f z   z − z0  γ  dz =  2π  f z0 + ρ exp iθ   ρ exp iθ  iρ exp iθ dθ  If the radius of the circle γ is now shrunk to zero, i.e. ρ → 0, then I → 2πif z0 ,  = i  f z0 + ρ exp iθ  dθ.  thus establishing the result  24.46 .  expression for f   z0 :   cid:7   An extension to Cauchy’s integral formula can be made, yielding an integral   cid:21   cid:21   0  2π  0   cid:21    cid:7   f   z0  =  1 2πi  f z    z − z0 2 dz,  C   24.47   under the same conditions as previously stated.  cid:1 Prove Cauchy’s integral formula for f   cid:7    z0  given in  24.47 .  To show this, we use the deﬁnition of a derivative and  24.46  itself to evaluate   cid:7   f   z0  = lim h→0  0 0  h f z   f z0 + h  − f z0   cid:7   cid:13   cid:13  0  1 2πi 1 2πi  h  C  1  z − z0 − h  z − z0 − h  z − z0   f z   C f z    z − z0 2 dz,  C  = lim h→0  = lim h→0 1 2πi  =   cid:14  − 1 z − z0  dz   cid:8    cid:14   dz  which establishes result  24.47 .  cid:2   §  The similarity between this and the uniqueness theorem for the Laplace equation with Dirichlet boundary conditions  see chapter 20  is apparent.  852   24.11 TAYLOR AND LAURENT SERIES  0   cid:20  cid:20  cid:20  cid:20 0  Further, it may be proved by induction that the nth derivative of f z  is also  given by a Cauchy integral,  f n  z0  =  n! 2πi  f z  dz   z − z0 n+1 .  C   24.48   Thus, if the value of the analytic function is known on C then not only may the value of the function at any interior point be calculated, but also the values of all its derivatives.  The observant reader will notice that  24.48  may also be obtained by the formal device of diﬀerentiating under the integral sign with respect to z0 in Cauchy’s integral formula  24.46 :   cid:13   0 0  f n  z0  =  1 2πi n! 2πi  f z   ∂n ∂zn 0 f z  dz   z − z0   z − z0 n+1 .  C  C  =   cid:14   dz   cid:1 Suppose that f z  is analytic inside and on a circle C of radius R centred on the point z = z0. If f z  ≤ M on the circle, where M is some constant, show that  f n  z0  ≤ Mn!  .  Rn   24.49   From  24.48  we have  f n  z0  =  n! 2π  f z  dz   z − z0 n+1  C   cid:20  cid:20  cid:20  cid:20  ,  and on using  24.39  this becomes  M Rn+1 This result is known as Cauchy’s inequality.  cid:2   f n  z0  ≤ n!  2π  2πR =  Mn! Rn  .   24.49  and letting R → ∞, we ﬁnd f  We may use Cauchy’s inequality to prove Liouville’s theorem, which states that if f z  is analytic and bounded for all z then f is a constant. Setting n = 1 in  z0  = 0. Since f z  is analytic for all z, we may take z0 as any point in the z-plane and thus f  z  = 0 for all z; this implies f z  = constant. Liouville’s theorem may be used in turn to prove the fundamental theorem of algebra  see exercise 24.9 .   z0  = 0 and hence f   cid:7    cid:7    cid:7   24.11 Taylor and Laurent series  Following on from  24.48 , we may establish Taylor’s theorem for functions of a complex variable. If f z  is analytic inside and on a circle C of radius R centred on the point z = z0, and z is a point inside C, then  f z  =  an z − z0 n,   24.50   ∞ cid:4   n=0  853   COMPLEX VARIABLES  where an is given by f n  z0  n!. The Taylor expansion is valid inside the region of analyticity and, for any particular z0, can be shown to be unique.  To prove Taylor’s theorem  24.50 , we note that, since f z  is analytic inside  and on C, we may use Cauchy’s formula to write f z  as  0   24.51  −1 as a geometric series  where ξ lies on C. Now we may expand the factor  ξ − z   cid:8  in  z − z0   ξ − z0 ,  cid:8   so  24.51  becomes  ξ − z0  ξ − z  =  1  n  ,  f z  =  C  1  n=0  dξ,  f ξ   1 2πi  ξ − z  cid:7  ∞ cid:4  z − z0 ξ − z0  cid:7  ∞ cid:4  0  z − z0 ξ − z0  0 ∞ cid:4  ξ − z0  z − z0 n ∞ cid:4   z − z0 n 2πif n  z0   f ξ   n=0  n=0  C  C  ,  n!  n=0  n  dξ  f ξ    ξ − z0 n+1 dξ  f z  =  1 2πi  1 2πi  1 2πi  =  =   24.52   where we have used Cauchy’s integral formula  24.48  for the derivatives of f z . Cancelling the factors of 2πi, we thus establish the result  24.50  with an = f n  z0  n!.  cid:1 Show that if f z  and g z  are analytic in some region R, and f z  = g z  within some subregion S of R, then f z  = g z  throughout R.  It is simpler to consider the  analytic  function h z  = f z − g z , and to show that because  h z  = 0 in S it follows that h z  = 0 throughout R.  If we choose a point z = z0 in S , then we can expand h z  in a Taylor series about z0,  h z  = h z0  + h   cid:7    z0  z − z0  + 1 2 h   cid:7  cid:7    z0  z − z0 2 + ··· ,  which will converge inside some circle C that extends at least as far as the nearest part of the boundary of R, since h z  is analytic in R. But since z0 lies in S , we have   cid:7   h z0  = h   z0  = h   cid:7  cid:7    z0  = ··· = 0,  and so h z  = 0 inside C. We may now expand about a new point, which can lie anywhere within C, and repeat the process. By continuing this procedure we may show that h z  = 0 throughout R.  This result is called the identity theorem and, in fact, the equality of f z  and g z  throughout R follows from their equality along any curve of non-zero length in R, or even at a countably inﬁnite number of points in R.  cid:2   So far we have assumed that f z  is analytic inside and on the  circular  contour C. If, however, f z  has a singularity inside C at the point z = z0, then it cannot be expanded in a Taylor series. Nevertheless, suppose that f z  has a pole  854   24.11 TAYLOR AND LAURENT SERIES  of order p at z = z0 but is analytic at every other point inside and on C. Then  the function g z  =  z − z0 pf z  is analytic at z = z0, and so may be expanded  as a Taylor series about z = z0:  g z  =  bn z − z0 n.   24.53   ∞ cid:4   n=0  Thus, for all z inside C, f z  will have a power series representation of the form  f z  =  a−p  z − z0 p  + ··· +  a−1 z − z0  + a0 + a1 z − z0  + a2 z − z0 2 + ··· ,  with a−p  cid:3 = 0. Such a series, which is an extension of the Taylor expansion, is  called a Laurent series. By comparing the coeﬃcients in  24.53  and  24.54 , we see that an = bn+p. Now, the coeﬃcients bn in the Taylor expansion of g z  are seen from  24.52  to be given by   24.54   0  0  bn =  g n  z0   n!  =  1 2πi  g z    z − z0 n+1 dz,  0  +∞ cid:4   an z − z0 n.  and so for the coeﬃcients an in  24.54  we have  an =  1 2πi  g z    z − z0 n+1+p dz =  1 2πi  f z    z − z0 n+1 dz,  an expression that is valid for both positive and negative n.  The terms in the Laurent series with n ≥ 0 are collectively called the analytic z − z0, is called the principal part. Depending on the nature of the point z = z0,  part, whilst the remainder of the series, consisting of terms in inverse powers of  the principal part may contain an inﬁnite number of terms, so that  f z  =  n=−∞  In this case we would expect the principal part to converge only for  z − z0    24.55  −1 less than some constant, i.e. outside some circle centred on z0. However, the analytic part will converge inside some  diﬀerent  circle also centred on z0. If the latter circle has the greater radius then the Laurent series will converge in the region R between the two circles  see ﬁgure 24.12 ; otherwise it does not converge at all.  In fact, it may be shown that any function f z  that is analytic in a region R between two such circles C1 and C2 centred on z = z0 can be expressed as a Laurent series about z0 that converges in R. We note that, depending on the nature of the point z = z0, the inner circle may be a point  when the principal part contains only a ﬁnite number of terms  and the outer circle may have an inﬁnite radius.  We may use the Laurent series of a function f z  about any point z = z0 to  855   COMPLEX VARIABLES  y  C2  R  C1  z0  Figure 24.12 The region of convergence R for a Laurent series of f z  about a point z = z0 where f z  has a singularity.  x  classify the nature of that point. If f z  is actually analytic at z = z0, then in  24.55  all an for n < 0 must be zero. It may happen that not only are all an . . . , am−1 are all zero as well. In this case, the ﬁrst zero for n < 0 but a0, a1, non-vanishing term in  24.55  is am z − z0 m, with m > 0, and f z  is then said to  have a zero of order m at z = z0.  here taken as positive :  If f z  is not analytic at z = z0, then two cases arise, as discussed above  p is   i  it is possible to ﬁnd an integer p such that a−p  cid:3 = 0 but a−p−k = 0 for all  ii  it is not possible to ﬁnd such a lowest value of −p.  integers k > 0;  In case  i , f z  is of the form  24.54  and is described as having a pole of order p at z = z0; the value of a−1  not a−p  is called the residue of f z  at the pole z = z0, and will play an important part in later applications. in which the negatively decreasing powers of z − z0 do not  For case  ii ,  terminate, f z  is said to have an essential singularity. These deﬁnitions should be compared with those given in section 24.6.   cid:1 Find the Laurent series of  about the singularities z = 0 and z = 2  separately . Hence verify that z = 0 is a pole of order 1 and z = 2 is a pole of order 3, and ﬁnd the residue of f z  at each pole.  To obtain the Laurent series about z = 0, we make the factor in parentheses in the  f z  =  1  z z − 2 3  856   24.11 TAYLOR AND LAURENT SERIES  denominator take the form  1 − αz , where α is some constant, and thus obtain  cid:10   f z  = − =− 1   cid:13  8z 1 − z 2 3 1 +  −3    −3  −4  −5    −3  −4    cid:9    cid:10    cid:9    cid:10    cid:9   +  +  1  2  3  − z 2  + ···  3!  − z 2   cid:14   8z  = − 1  8z  − 3 16  − 3z 16  − z 2 − 5z2 32  2!  − ··· .  Since the lowest power of z is −1, the point z = 0 is a pole of order 1. The residue of f z  −1 in the Laurent expansion about that point and is equal to −1 8. The Laurent series about z = 2 is most easily found by letting z = 2 + ξ  or z − 2 = ξ   at z = 0 is simply the coeﬃcient of z  and substituting into the expression for f z  to obtain  f z  =   2 + ξ ξ3  1   cid:16   1 2ξ3  1 2ξ3  ξ 2  1 − − 1 + 4ξ2 −  =  =  =  =   cid:7    cid:8   1   cid:7    cid:8   2ξ3 1 + ξ 2  2 −   cid:7    cid:8   ξ + 2 − 1 16  1 8ξ  +  ξ 32  +  3  ξ 2 − ···   cid:7    cid:8   ξ 2   cid:17   4 − ···  1  2 z − 2 3  1  4 z − 2 2  +  1  8 z − 2   − 1 16  +  z − 2  32  − ··· .  From this series we see that z = 2 is a pole of order 3 and that the residue of f z  at z = 2 is 1 8.  cid:2   Laurent series about z0 and identify the coeﬃcient of  z − z0   As we shall see in the next few sections, ﬁnding the residue of a function at a singularity is of crucial importance in the evaluation of complex integrals. Speciﬁcally, formulae exist for calculating the residue of a function at a particular  singular  point z = z0 without having to expand the function explicitly as a −1. The type of formula generally depends on the nature of the singularity at which the residue is required.  cid:1 Suppose that f z  has a pole of order m at the point z = z0. By considering the Laurent series of f z  about z0, derive a general expression for the residue R z0  of f z  at z = z0. Hence evaluate the residue of the function  at the point z = i.  f z  =  exp iz   z2 + 1 2  If f z  has a pole of order m at z = z0, then its Laurent series about this point has the form  f z  =  a−m  z − z0 m  + ··· +  a−1  z − z0   + a0 + a1 z − z0  + a2 z − z0 2 + ··· ,  which, on multiplying both sides of the equation by  z − z0 m, gives   z − z0 mf z  = a−m + a−m+1 z − z0  + ··· + a−1 z − z0 m−1 + ··· .  857   ∞ cid:4   bn z − z0 n,  cid:15   COMPLEX VARIABLES  Diﬀerentiating both sides m − 1 times, we obtain  dm−1 dzm−1  [ z − z0 mf z ] =  m − 1 ! a−1 +  for some coeﬃcients bn. In the limit z → z0, however, the terms in the sum disappear, and  n=1  after rearranging we obtain the formula  R z0  = a−1 = lim z→z0  1   m − 1 !  dm−1 dzm−1  [ z − z0 mf z ]  ,   24.56   which gives the value of the residue of f z  at the point z = z0.  If we now consider the function  we see immediately that it has poles of order 2  double poles  at z = i and z = −i. To  calculate the residue at  for example  z = i, we may apply the formula  24.56  with m = 2. Performing the required diﬀerentiation, we obtain  f z  =  exp iz   z2 + 1 2  =  exp iz   z + i 2 z − i 2 ,  cid:14   [ z − i 2f z ] =  d dz  d dz  exp iz  z + i 2   cid:12    cid:13   1  Setting z = i, we ﬁnd the residue is given by  [ z + i 2i exp iz − 2 exp iz  z + i ].  =   z + i 4   cid:5 −4ie  R i  =  1 1!  1 16   cid:6   −1 − 4ie −1  = − i  .  cid:2   2e  An important special case of  24.56  occurs when f z  has a simple pole  a pole  of order 1  at z = z0. Then the residue at z0 is given by  R z0  = lim z→z0  [ z − z0 f z ] .  If f z  has a simple pole at z = z0 and, as is often the case, has the form g z  h z , where g z  is analytic and non-zero at z0 and h z0  = 0, then  24.57  becomes   z − z0 g z   R z0  = lim z→z0  = g z0  lim z→z0  h z  1  z    cid:7  h  =  g z0   cid:7   z0  h  ,  = g z0  lim z→z0   z − z0   h z    24.57    24.58   where we have used l’H ˆopital’s rule. This result often provides the simplest way of determining the residue at a simple pole.  24.12 Residue theorem  Having seen from Cauchy’s theorem that the value of an integral round a closed contour C is zero if the integrand is analytic inside the contour, it is natural to ask what value it takes when the integrand is not analytic inside C. The answer to this is contained in the residue theorem, which we now discuss.  858   I =  f z  dz  0 ∞ cid:4  ∞ cid:4   n=−m  γ  n=−m  0  cid:21   an  an  =  =  24.12 RESIDUE THEOREM  Suppose the function f z  has a pole of order m at the point z = z0, and so  can be written as a Laurent series about z0 of the form  f z  =  an z − z0 n.   24.59   ∞ cid:4   n=−m  Now consider the integral I of f z  around a closed contour C that encloses z = z0, but no other singular points. Using Cauchy’s theorem, this integral has the same value as the integral around a circle γ of radius ρ centred on z = z0, since f z  is analytic in the region between C and γ. On the circle we have z = z0 + ρ exp iθ  and dz = iρ exp iθ dθ , and so   z − z0 n dz   cid:13   2π  0  2π  0  For every term in the series with n  cid:3 = −1, we have   cid:21   2π  iρn+1 exp[i n + 1 θ] dθ.   cid:14 2π  0  iρn+1 exp[i n + 1 θ]  i n + 1   = 0,  iρn+1 exp[i n + 1 θ] dθ =  but for the n = −1 term we obtain cid:21   0  Therefore only the term in  z − z0  0  around γ  and therefore C , and I takes the value  i dθ = 2πi. −1 contributes to the value of the integral  I =  f z  dz = 2πia−1.  C   24.60   Thus the integral around any closed contour containing a single pole of general order m  or, by extension, an essential singularity  is equal to 2πi times the residue of f z  at z = z0.  If we extend the above argument to the case where f z  is continuous within and on a closed contour C and analytic, except for a ﬁnite number of poles, within C, then we arrive at the residue theorem  f z  dz = 2πi  Rj,   24.61    cid:11   where  j Rj is the sum of the residues of f z  at its poles within C.  The method of proof is indicated by ﬁgure 24.13, in which  a  shows the original giving the same value  contour C referred to in  24.61  and  b  shows a contour C   cid:7   0  C   cid:4   j  859   COMPLEX VARIABLES  C   cid:7   C   b    cid:7   C   a   Figure 24.13 The contours used to prove the residue theorem:  a  the original contour;  b  the contracted contour encircling each of the poles.   cid:7   . Now the contribution to to the integral, because f is analytic between C and C the C integral from the polygon  a triangle for the case illustrated  joining the  cid:7  . Hence the whole value of small circles is zero, since f is also analytic inside C the integral comes from the circles and, by result  24.60 , each of these contributes 2πi times the residue at the pole it encloses. All the circles are traversed in their positive sense if C is thus traversed and so the residue theorem follows. Formally, Cauchy’s theorem  24.40  is a particular case of  24.61  in which C encloses no poles.  Finally we prove another important result, for later use. Suppose that f z  has  a simple pole at z = z0 and so may be expanded as the Laurent series  f z  = φ z  + a−1 z − z0   −1,  where φ z  is analytic within some neighbourhood surrounding z0. We wish to ﬁnd an expression for the integral I of f z  along an open contour C, which is the arc of a circle of radius ρ centred on z = z0 given by  θ1 ≤ arg z − z0  ≤ θ2,   24.62   where ρ is chosen small enough that no singularity of f, other than z = z0, lies within the circle. Then I is given by  z − z0 = ρ,  cid:21    cid:21   I =  f z  dz =  C  C  φ z  dz + a−1   z − z0   −1 dz.  If the radius of the arc C is now allowed to tend to zero, then the ﬁrst integral tends to zero, since the path becomes of zero length and φ is analytic and therefore continuous along it. On C, z = ρeiθ and hence the required expression for I is   cid:21   I = lim ρ→0  f z  dz = lim ρ→0  C  1 ρeiθ iρeiθ dθ  = ia−1 θ2 − θ1 .   24.63    cid:21   C   cid:8    cid:7    cid:21   a−1  θ2  θ1  860   24.13 DEFINITE INTEGRALS USING CONTOUR INTEGRATION  We note that result  24.60  is a special case of  24.63  in which θ2 is equal to θ1 + 2π.  24.13 Deﬁnite integrals using contour integration  The remainder of this chapter is devoted to methods of applying contour integra- tion and the residue theorem to various types of deﬁnite integral. However, three applications of contour integration, in which obtaining a value for the integral is not the prime purpose of the exercise, have been postponed until chapter 25. They are the location of the zeros of a complex polynomial, the evaluation of the sums of certain inﬁnite series and the determination of inverse Laplace transforms.  For the integral evalations considered here, not much preamble is given since, for this material, the simplest explanation is felt to be via a series of worked examples that can be used as models.  24.13.1 Integrals of sinusoidal functions  Suppose that an integral of the form cid:21   2π  0  F cos θ, sin θ  dθ   24.64   is to be evaluated. It can be made into a contour integral around the unit circle C by writing z = exp iθ, and hence  cos θ = 1  2  z + z  −1 ,  sin θ = − 1  2 i z − z  −1 ,  dθ = −iz  −1 dz.   24.65   This contour integral can then be evaluated using the residue theorem, provided the transformed integrand has only a ﬁnite number of poles inside the unit circle and none on it.  cid:1 Evaluate   cid:21   I =  2π  0  cos 2θ  a2 + b2 − 2ab cos θ  dθ,  b > a > 0.   24.66   By de Moivre’s theorem  section 3.4 ,  Using n = 2 in  24.67  and straightforward substitution for the other functions of θ in  24.66  gives   24.67   cos nθ = 1  2  zn + z  −n .  0  i  I =  2ab  C  z4 + 1  z2 z − a b  z − b a   dz.  Thus there are two poles inside C, a double pole at z = 0 and a simple pole at z = a b  recall that b > a .  We could ﬁnd the residue of the integrand at z = 0 by expanding the integrand as a −1. Alternatively, we may use the  Laurent series in z and identifying the coeﬃcient of z  861   COMPLEX VARIABLES  formula  24.56  with m = 2. Choosing the latter method and denoting the integrand by f z , we have   cid:14    cid:13   z − a b  z − b a 4z3 −  z4 + 1 [ z − a b  +  z − b a ]   z − a b  z − b a   z4 + 1  d dz   z − a b 2 z − b a 2  .  d dz  [z2f z ] =  =  Now setting z = 0 and applying  24.56 , we ﬁnd  For the simple pole at z = a b, equation  24.57  gives the residue as  R 0  =  +  a b  b a  .   z − a b f z   =   cid:19    cid:18  R a b  = lim z→ a b  = − a4 + b4 ab b2 − a2   cid:13   .   a b 4 + 1   a b 2 a b − b a   cid:14   I = 2πi × i  a2 + b2  2ab  ab  − a4 + b4 ab b2 − a2   =  2πa2  b2 b2 − a2   .  cid:2   Therefore by the residue theorem  We next consider the evaluation of an integral of the form  24.13.2 Some inﬁnite integrals   cid:21  ∞  −∞ f x  dx,  Since  where f z  has the following properties:  of poles, none of which is on the real axis;   i  f z  is analytic in the upper half-plane, Im z ≥ 0, except for a ﬁnite number  ii  on a semicircle Γ of radius R  ﬁgure 24.14 , R times the maximum of f on Γ tends to zero as R → ∞  a suﬃcient condition is that zf z  → 0 as  cid:1  z → ∞ ; 0−∞ f x  dx and  0 f x  dx both exist.   cid:1  ∞   iii    cid:20  cid:20  cid:20  cid:20  ≤ 2πR ×  maximum of f on Γ ,  f z  dz   cid:20  cid:20  cid:20  cid:20  cid:21   Γ  condition  ii  ensures that the integral along Γ tends to zero as R → ∞, after  which it is obvious from the residue theorem that the required integral is given by   cid:21  ∞ −∞ f x  dx = 2πi ×  sum of the residues at poles with Im z > 0 .   24.68   862   24.13 DEFINITE INTEGRALS USING CONTOUR INTEGRATION  y  Γ  −R  O  R  x  Figure 24.14 A semicircular contour in the upper half-plane.   cid:1 Evaluate   cid:21  ∞  I =  0  dx   x2 + a2 4 ,  where a is real.  The complex function  z2 + a2  is in the upper half-plane. Conditions  ii  and  iii  are clearly satisﬁed. For higher-order poles, formula  24.56  for evaluating residues can be tiresome to apply. So, instead, we put z = ai + ξ and expand for small ξ to obtain  −4 has poles of order 4 at z = ±ai, of which only z = ai  §   cid:7   1 − iξ  2a   cid:8 −4  .  The coeﬃcient of ξ  1  =   z2 + a2 4 −1 is given by   2aiξ + ξ2 4   2aiξ 4  1  3!  1   cid:8   =   cid:7 −i  2a   −4  −5  −6   1   2a 4  3  =  −5i 32a7 ,  and hence by the residue theorem cid:21  ∞  and so I = 5π  32a7 .  cid:2   dx  −∞   x2 + a2 4  =  10π 32a7 ,  Condition  i  of the previous method required there to be no poles of the integrand on the real axis, but in fact simple poles on the real axis can be accommodated by indenting the contour as shown in ﬁgure 24.15. The indentation at the pole z = z0 is in the form of a semicircle γ of radius ρ in the upper half- plane, thus excluding the pole from the interior of the contour.  §  This illustrates another useful technique for determining residues.  863   COMPLEX VARIABLES  y  Γ  γ  −R  O  R  x  Figure 24.15 An indented contour used when the integrand has a simple pole on the real axis.  What is then obtained from a contour integration, apart from the contributions  for Γ and γ, is called the principal value of the integral, deﬁned as ρ → 0 by   cid:21    cid:21   f x  dx ≡  P  R  −R  z0−ρ −R   cid:21   R  z0+ρ  f x  dx +  f x  dx.  The remainder of the calculation goes through as before, but the contribution from the semicircle, γ, must be included. Result  24.63  of section 24.12 shows that since only a simple pole is involved its contribution is   24.69  where a−1 is the residue at the pole and the minus sign arises because γ is traversed in the clockwise  negative  sense.  We defer giving an example of an indented contour until we have established Jordan’s lemma; we will then work through an example illustrating both. Jordan’s lemma enables inﬁnite integrals involving sinusoidal functions to be evaluated.  −ia−1π,  For a function f z  of a complex variable z, if   i  f z  is analytic in the upper half-plane except for a ﬁnite number of poles in Im z > 0,   ii  the maximum of f z  → 0 as z → ∞ in the upper half-plane,   iii  m > 0,  then   cid:21   IΓ =  Γ  eimzf z  dz → 0  as R → ∞,   24.70   where Γ is the same semicircular contour as in ﬁgure 24.14.  Note that this condition  ii  is less stringent than the earlier condition  ii   see the  start of this section , since we now only require M R  → 0 and not RM R  → 0,  where M is the maximum  §  of f z  on z = R.  §  More strictly, the least upper bound.  864   24.13 DEFINITE INTEGRALS USING CONTOUR INTEGRATION  The proof of the lemma is straightforward once it has been observed that, for  0 ≤ θ ≤ π 2,  cid:21   ≥ 2  .  π  1 ≥ sin θ  cid:21   θ  Then, since on Γ we have  exp imz  =  exp −mR sin θ , −mR sin θ dθ = 2MR  eimzf z  dz ≤ MR  IΓ ≤  e  π  0  Γ  Thus, using  24.71 ,   cid:5  1 − e IΓ ≤ 2MR −mR hence, as R → ∞, IΓ tends to zero since M tends to zero.  cid:1 Find the principal value of cid:21  ∞  −mR 2θ π  dθ =  πM m  π 2  e  0   cid:21    cid:21   cid:6   0   24.71   π 2  −mR sin θ dθ.  e  <  πM m  ;  cos mx  dx,  −∞ Consider the function  z − a  it does have a simple pole at z = a, and further  z − a   cid:21   x − a −1 exp imz ; although it has no poles in the upper half-plane −1 → 0 as z → ∞. We will use a  cid:21   contour like that shown in ﬁgure 24.15 and apply the residue theorem. Symbolically,  for a real, m > 0.   cid:21   R  +  +  +  = 0.   24.72   γ  a+ρ  Γ   24.69  we obtain  Now as R → ∞ and ρ → 0 we have  cid:21  ∞  24.73  −∞ where a−1 is the residue of  z − a  −1 exp imz  at z = a, which is exp ima . Then taking the  → 0, by Jordan’s lemma, and from  24.68  and  dx − iπa−1 = 0,  x − a  eimx  P  Γ   cid:21   cid:1   a−ρ −R  real and imaginary parts of  24.73  gives   cid:21  ∞  cid:21  ∞  −∞  −∞  P  P  cos mx  x − a x − a  sin mx  dx = −π sin ma,  dx = π cos ma,  as required,  as a bonus.  cid:2   24.13.3 Integrals of multivalued functions  We have discussed brieﬂy some of the properties and diﬃculties associated with certain multivalued functions such as z1 2 or Ln z. It was mentioned that one method of managing such functions is by means of a ‘cut plane’. A similar technique can be used with advantage to evaluate some kinds of inﬁnite integral involving real functions for which the corresponding complex functions are multi- valued. A typical contour employed for functions with a single branch point  865   COMPLEX VARIABLES  y  γ  A  C  Γ  B  D  x  Figure 24.16 A typical cut-plane contour for use with multivalued functions that have a single branch point located at the origin.  located at the origin is shown in ﬁgure 24.16. Here Γ is a large circle of radius R and γ is a small one of radius ρ, both centred on the origin. Eventually we will  let R → ∞ and ρ → 0.  The success of the method is due to the fact that because the integrand is multivalued, its values along the two lines AB and CD joining z = ρ to z = R are not equal and opposite although both are related to the corresponding real integral. Again an example provides the best explanation.  cid:1 Evaluate   cid:21  ∞  I =  0  dx   x + a 3x1 2 ,  a > 0.  integral.  We consider the integrand f z  =  z + a   −1 2 and note that zf z  → 0 on the two circles as ρ → 0 and R → ∞. Thus the two circles make no contribution to the contour The only pole of the integrand inside the contour is at z = −a  and is of order 3 . To determine its residue we put z = −a + ξ and expand  noting that  −a 1 2 equals a1 2 exp iπ 2  = ia1 2 :  −3z  1   z + a 3z1 2  =  =  ξ3ia1 2 1 − ξ a 1 2  1  iξ3a1 2  1 +  +  1 2  ξ a  The residue is thus −3i  8a5 2 .  cid:21   The residue theorem  24.61  now gives   cid:21    cid:21   +  +  +  = 2πi  AB  Γ  DC  γ   cid:8   .  3 8  ξ2 a2   cid:7  −3i  8a5 2  + ···  cid:8   .  1   cid:7    cid:21   866   24.14 EXERCISES   cid:1    cid:1   We have seen that γ vanish, and if we denote z by x along the line AB then it has the value z = x exp 2πi along the line DC  note that exp 2πi must not be set equal to 1 until after the substitution for z has been made in DC . Substituting these expressions,  Γ and   cid:21  ∞   cid:21   +  0 ∞   cid:8  cid:21  ∞  dx   cid:7    x + a 3x1 2  0  [x exp 2πi + a]3x1 2 exp  1  2 2πi   =  3π 4a5 2 .   cid:1   dx  Thus  and  1 − 1  exp πi  dx   x + a 3x1 2  =  3π 4a5 2  0  1 2  I =  × 3π 4a5 2 .  cid:2   Several other examples of integrals of multivalued functions around a variety  of contours are included in the exercises that follow.  24.1  Find an analytic function of z = x + iy whose imaginary part is  24.14 Exercises   y cos y + x sin y  exp x.  24.2  Find a function f z , analytic in a suitable part of the Argand diagram, for which  24.3  Where are the singularities of f z ? Find the radii of convergence of the following Taylor series:  Re f =  ∞ cid:4  ∞ cid:4   n=2  n=1   a    c   zn ln n  ,   b   znnln n,   d   .  sin 2x  cosh 2y − cos 2x ∞ cid:4  ∞ cid:4   n!zn nn   cid:7    cid:8   n=1  n2  ,  n + p  n=1  ∞ cid:4   r=1  f z  =   −1 r+1 sin   cid:9    cid:10   pz r  ,  zn, with p real.  n  24.4  Find the Taylor series expansion about the origin of the function f z  deﬁned by  where p is a constant. Hence verify that f z  is a convergent series for all z. Determine the types of singularities  if any  possessed by the following functions  24.5  at z = 0 and z = ∞:   a   z − 2   d  ez z3,  −1,   b   1 + z3  z2,  e  z1 2  1 + z2 1 2.   c  sinh 1 z ,  24.6  Identify the zeros, poles and essential singularities of the following functions:   a  tan z,  d  tan 1 z ,   b  [ z − 2  z2] sin[1  1 − z ],  e  z2 3.   c  exp 1 z ,  867   COMPLEX VARIABLES  24.7  Find the real and imaginary parts of the functions  i  z2,  ii  ez, and  iii  cosh πz. 0 ≤ x, y ≤ 1, determine the solution of Laplace’s equation in that region that By considering the values taken by these parts on the boundaries of the region  satisﬁes the boundary conditions  24.8  Show that the transformation  φ x, 0  = 0, φ x, 1  = x,  φ 0, y  = 0, φ 1, y  = y + sin πy.   cid:21    cid:21   w =  z  0  1   ζ3 − ζ 1 2 dζ  L =  cosec 1 2 θ dθ.  π 2  0  transforms the upper half-plane into the interior of a square that has one corner at the origin of the w-plane and sides of length L, where  24.9  24.10  The fundamental theorem of algebra states that, for a complex polynomial pn z  of degree n, the equation pn z  = 0 has precisely n complex roots. By applying Liouville’s theorem  see the end of section 24.10  to f z  = 1 pn z , prove that pn z  = 0 has at least one complex root. Factor out that root to obtain pn−1 z  and, by repeating the process, prove the above theorem. Show that, if a is a positive real constant, the function exp iaz2  is analytic and → 0 as z → ∞ for 0 < arg z ≤ π 4. By applying Cauchy’s theorem to a suitable  contour prove that  24.11  The function  of the complex variable z is deﬁned to be real and positive on the real axis in  the range −1 < x < 1. Using cuts running along the real axis for 1 < x < +∞ and −∞ < x < −1, show how f z  is made single-valued and evaluate it on the  upper and lower sides of both cuts.  Use these results and a suitable contour in the complex z-plane to evaluate the  integral  Conﬁrm your answer by making the substitution x = sec θ.  24.12  By considering the real part of cid:21   cid:21   where z = exp iθ and n is a non-negative integer, evaluate   cid:21  ∞  0     π 8a  .  cos ax2  dx =  f z  =  1 − z2 1 2   cid:21  ∞  I =  1  dx  x x2 − 1 1 2 .  −izn−1 dz  1 − a z + z  −1  + a2 ,  π  0  cos nθ  1 − 2a cos θ + a2 dθ  cid:21   π  −π  sin θ  a − sin θ  dθ,  868  24.13  for a real and > 1. Prove that if f z  has a simple zero at z0, then 1 f z  has residue 1 f Hence evaluate   cid:7    z0  there.  where a is real and > 1.   24.14 EXERCISES  24.14  24.15  Prove that, for α > 0, the integral cid:21  ∞ has the value  π 2  exp −α . Prove that  cid:21  ∞  0  cos mx  4x4 + 5x2 + 1  0  24.16  Show that the principal value of the integral   cid:6   −m 2 − e −m  4e  for m > 0.  t sin αt 1 + t2 dt   cid:5   π 6  dx =   cid:21  ∞  cos x a   x2 − a2 dx  −∞  is − π a  sin 1.  24.17  The following is an alternative  and roundabout!  way of evaluating the Gaussian integral.   a  Prove that the integral of [exp iπz2 ]cosec πz around the parallelogram with  corners ±1 2 ± R exp iπ 4  has the value 2i. when R → ∞.   b  Show that the parts of the contour parallel to the real axis do not contribute   c  Evaluate the integrals along the other two sides by putting z   cid:7   + 1  2 and z   cid:7  − 1  2 . Hence, by letting R → ∞ show  = r exp iπ 4   and working in terms of z that   cid:7   24.18  By applying the residue theorem around a wedge-shaped contour of angle 2π n, with one side along the real axis, prove that the integral  where n is real and ≥ 2, has the value  π n cosec  π n .  24.19  Using a suitable cut plane, prove that if α is real and 0 < α < 1 then  24.20  has the value π cosec πα. Show that   cid:21  ∞  dx = −√  2π2.  x3 4 1 + x   0  24.21  By integrating a suitable function around a large semicircle in the upper half- plane and a small semicircle centred on the origin, determine the value of  −πr2  dr = 1.   cid:21  ∞  −∞ e   cid:21  ∞  0   cid:21  ∞  0  ln x  dx  ,  1 + xn  −α  x 1 + x  dx   cid:21  ∞  0  I =   cid:21  ∞  0  869   ln x 2 1 + x2 dx  ln x 1 + x2 dx = 0.  and deduce, as a by-product of your calculation, that   COMPLEX VARIABLES  = 1 −  cid:4  cos θ,  l r  24.22  The equation of an ellipse in plane polar coordinates r, θ, with one of its foci at the origin, is  where l is a length  that of the latus rectum  and  cid:4   0 <  cid:4  < 1  is the eccentricity of the ellipse. Express the area of the ellipse as an integral around the unit circle in the complex plane, and show that the only singularity of the integrand inside the circle is a double pole at z0 =  cid:4   −2 − 1 1 2.  −1 −   cid:4   By setting z = z0 + ξ and expanding the integrand in powers of ξ, ﬁnd the  residue at z0 and hence show that the area is equal to πl2 1 −  cid:4 2   [ In terms of the semi-axes a and b of the ellipse, l = b2 a and  cid:4 2 =  a2−b2  a2. ]  −3 2.  24.1 24.3 24.5  ∂u ∂y = − exp x  y cos y + x sin y + sin y ; z exp z.  24.15 Hints and answers  −p.  24.9  24.7  φ x, y  = xy +  sinh πx sin πy   sinh π.   a  1;  b  1;  c  1;  d  e  a  Analytic, analytic;  b  double pole, single pole;  c  essential singularity, ana- lytic;  d  triple pole, essential singularity;  e  branch point, branch point.   i  x2 − y2, 2xy;  ii  ex cos y, ex sin y;  iii  cosh πx cos πy, sinh πx sin πy; Assume that pr x   r = n, n− 1, . . . , 1  has no roots and then argue by the method 24.11 With 0 ≤ θ1 < 2π and −π < θ2 ≤ π, f z  =  r1r2 1 2 exp[ i θ1 + θ2 − π  ]. The four values are ±i x2 − 1 1 2, with the plus sign corresponding to points near the cut The only pole inside the unit circle is at z = ia − i a2 − 1 1 2; the residue is given by − i 2  a2 − 1   −1 2; the integral has value 2π[a a2 − 1   that lie in the second and fourth quadrants. I = π 2.  −1 2 − 1].  of contradiction.  24.13  24.15  24.17  24.19 24.21  Factorise the denominator, showing that the relevant simple poles are at i 2 and i.  a  The only pole is at the origin with residue π   b  each is O[ exp −πR2 ∓ √ Note that ρ lnn ρ → 0 as ρ → 0 for all n. When z is on the negative real axis,  ln z 2 contains three terms; one of the corresponding integrals is a standard form. The residue at z = i is iπ2 8; I = π3 8.   c  the sum of the integrals is 2i Use a contour like that shown in ﬁgure 24.16.  −1; R−R exp −πr2  dr.  2πR  ];   cid:1   870   25  Applications of complex variables  In chapter 24, we developed the basic theory of the functions of a complex variable, z = x + iy, studied their analyticity  diﬀerentiability  properties and derived a number of results concerned with values of contour integrals in the complex plane. In this current chapter we will show how some of those results and properties can be exploited to tackle problems arising directly from physical situations or from apparently unrelated parts of mathematics.  In the former category will be the use of the diﬀerential properties of the real and imaginary parts of a function of a complex variable to solve problems involv- ing Laplace’s equation in two dimensions, whilst an example of the latter might be the summation of certain types of inﬁnite series. Other applications, such as the Bromwich inversion formula for Laplace transforms, appear as mathematical problems that have their origins in physical applications; the Bromwich inversion enables us to extract the spatial or temporal response of a system to an initial input from the representation of that response in ‘frequency space’ – or, more correctly, imaginary frequency space.  Other topics that will be considered are the location of the  complex  zeros of a polynomial, the approximate evaluation of certain types of contour integrals using the methods of steepest descent and stationary phase, and the so-called ‘phase-integral’ solutions to some diﬀerential equations. For each of these a brief introduction is given at the start of the relevant section and to repeat them here would be pointless. We will therefore move on to our ﬁrst topic of complex potentials.  25.1 Complex potentials  Towards the end of section 24.2 of the previous chapter it was shown that the real and the imaginary parts of an analytic function of z are separately solutions of Laplace’s equation in two dimensions. Analytic functions thus oﬀer a possible way  871   APPLICATIONS OF COMPLEX VARIABLES  y  x  Figure 25.1 The equipotentials  dashed circles  and ﬁeld lines  solid lines  for a line charge perpendicular to the z-plane.  of solving some two-dimensional physical problems describable by a potential  satisfying ∇2φ = 0. The general method is known as that of complex potentials.  We also found that if f = u + iv is an analytic function of z then any curve u = constant intersects any curve v = constant at right angles. In the context of solutions of Laplace’s equation, this result implies that the real and imaginary parts of f z  have an additional connection between them, for if the set of contours on which one of them is a constant represents the equipotentials of a system then the contours on which the other is constant, being orthogonal to each of the ﬁrst set, must represent the corresponding ﬁeld lines or stream lines, depending on the context. The analytic function f is the complex potential. It is conventional to use φ and ψ  rather than u and v  to denote the real and imaginary parts of a complex potential, so that f = φ + iψ.  As an example, consider the function  −q  2π cid:4 0  f z  =  ln z   25.1   in connection with the physical situation of a line charge of strength q per unit length passing through the origin, perpendicular to the z-plane  ﬁgure 25.1 . Its real and imaginary parts are  −q  2π cid:4 0  lnz,  φ =  −q  2π cid:4 0  ψ =  arg z.   25.2   The contours in the z-plane of φ = constant are concentric circles and those of ψ = constant are radial lines. As expected these are orthogonal sets, but in addi- tion they are, respectively, the equipotentials and electric ﬁeld lines appropriate to  872   25.1 COMPLEX POTENTIALS  the ﬁeld produced by the line charge. The minus sign is needed in  25.1  because the value of φ must decrease with increasing distance from the origin.  Suppose we make the choice that the real part φ of the analytic function f gives the conventional potential function; ψ could equally well be selected. Then we may consider how the direction and magnitude of the ﬁeld are related to f.  cid:1 Show that for any complex  electrostatic  potential f z  the strength of the electric ﬁeld is given by E = f  z ] with the   z  and that its direction makes an angle of π − arg[ f   cid:7    cid:7   x-axis.  Because φ = constant is an equipotential, the ﬁeld has components  Ex = − ∂φ  ∂x  and  Ey = − ∂φ  .  ∂y  Since f is analytic,  i  we may use the Cauchy–Riemann relations  24.5  to change the second of these, obtaining   25.3    25.4   Ex = − ∂φ  ∂x  and  Ey =  ∂ψ ∂x  ;   ii  the direction of diﬀerentiation at a point is immaterial and so  = −Ex + iEy. From these it can be seen that the ﬁeld at a point is given in magnitude by E = f and that it makes an angle with the x-axis given by π − arg[ f   z ].  cid:2   ∂φ ∂x  ∂ψ ∂x  ∂f ∂x  df dz  + i  =  =   cid:7    25.5    cid:7    z   It will be apparent from the above that much of physical interest can be calculated by working directly in terms of f and z. In particular, the electric ﬁeld vector E may be represented, using  25.5  above, by the quantity  E = Ex + iEy = −[ f   cid:7   ∗  z ]  .  Complex potentials can be used in two-dimensional ﬂuid mechanics problems in a similar way. If the ﬂow is stationary  i.e. the velocity of the ﬂuid does not depend on time  and irrotational, and the ﬂuid is both incompressible and non-  viscous, then the velocity of the ﬂuid can be described by V = ∇φ, where φ is the velocity potential and satisﬁes ∇2φ = 0. If, for a complex potential f = φ + iψ,  the real part φ is taken to represent the velocity potential then the curves ψ = constant will be the streamlines of the ﬂow. In a direct parallel with the electric ﬁeld, the velocity may be represented in terms of the complex potential by  V = Vx + iVy = [ f   cid:7   ∗  z ]  ,  the diﬀerence of a minus sign reﬂecting the same diﬀerence between the deﬁnitions  z  = 0, and  of E and V. The speed of the ﬂow is equal to f   z . Points where f   cid:7    cid:7   thus the velocity is zero, are called stagnation points of the ﬂow.  Analogously to the electrostatic case, a line source of ﬂuid at z = z0, perpendic- ular to the z-plane  i.e. a point from which ﬂuid is emerging at a constant rate ,  873   APPLICATIONS OF COMPLEX VARIABLES  is described by the complex potential  f z  = k ln z − z0 ,  where k is the strength of the source. A sink is similarly represented, but with k  replaced by −k. Other simple examples are as follows.   i  The ﬂow of a ﬂuid at a constant speed V0 and at an angle α to the x-axis  is described by f z  = V0 exp iα z.   ii  Vortex ﬂow, in which ﬂuid ﬂows azimuthally in an anticlockwise direction around some point z0, the speed of the ﬂow being inversely proportional  to the distance from z0, is described by f z  = −ik ln z − z0 , where k is the strength of the vortex. For a clockwise vortex k is replaced by −k.   cid:1  Verify that the complex potential   cid:7    cid:8   a2 z  f z  = V0  z +  is appropriate to a circular cylinder of radius a placed so that it is perpendicular to a uniform ﬂuid ﬂow of speed V0 parallel to the x-axis.  Firstly, since f z  is analytic except at z = 0, both its real and imaginary parts satisfy  Laplace’s equation in the region exterior to the cylinder. Also f z  → V0z as z → ∞, so that Re f z  → V0x, which is appropriate to a uniform ﬂow of speed V0 in the x-direction  far from the cylinder.  Writing z = r exp iθ and using de Moivre’s theorem we have  f z  = V0  r exp iθ +   cid:14   cid:7  exp −iθ    cid:8   r − a2  r   cid:13   cid:7   a2 r   cid:8    cid:8   a2 r   cid:7   r − a2  r  ψ = V0  sin θ = constant.  = V0  r +  cos θ + iV0  sin θ.  Thus we see that the streamlines of the ﬂow described by f z  are given by  In particular, ψ = 0 on r = a, independently of the value of θ, and so r = a must be a streamline. Since there can be no ﬂow of ﬂuid across streamlines, r = a must correspond to a boundary along which the ﬂuid ﬂows tangentially. Thus f z  is a solution of Laplace’s equation that satisﬁes all the physical boundary conditions of the problem, and so, by the uniqueness theorem, it is the appropriate complex potential.  cid:2  By a similar argument, the complex potential f z  = −E z − a2 z   note the  minus signs  is appropriate to a conducting circular cylinder of radius a placed perpendicular to a uniform electric ﬁeld E in the x-direction.  The real and imaginary parts of a complex potential f = φ + iψ have another interesting relationship in the context of Laplace’s equation in electrostatics or ﬂuid mechanics. Let us choose φ as the conventional potential, so that ψ represents the stream function  or electric ﬁeld, depending on the application , and consider  874   25.1 COMPLEX POTENTIALS  y  Q  x  ˆn  P   cid:21   Figure 25.2 A curve joining the points P and Q. Also shown is ˆn, the unit vector normal to the curve.  the diﬀerence in the values of ψ at any two points P and Q connected by some path C, as shown in ﬁgure 25.2. This diﬀerence is given by   cid:21    cid:7   Q   cid:8   which, on using the Cauchy–Riemann relations, becomes  ψ Q  − ψ P   =  ψ Q  − ψ P   =  dx +  dy  ,  ∂ψ ∂y   cid:8   dy  Q  P  dψ =  ∂ψ ∂x  P   cid:7  − ∂φ ∇φ · ˆn ds =  ∂y  dx +   cid:21   cid:21   Q  P  Q  P  =   cid:21   ∂φ ∂x Q  P  ∂φ ∂n  ds,  where ˆn is the vector unit normal to the path C and s is the arc length along the  path; the last equality is written in terms of the normal derivative ∂φ ∂n ≡ ∇φ· ˆn.  Now suppose that in an electrostatics application, the path C is the surface of  a conductor; then  where σ is the surface charge density per unit length normal to the xy-plane.  Therefore − cid:4 0[ψ Q  − ψ P  ] is equal to the charge per unit length normal to the  xy-plane on the surface of the conductor between the points P and Q. Similarly, in ﬂuid mechanics applications, if the density of the ﬂuid is ρ and its velocity is V then  ρ[ψ Q  − ψ P  ] = ρ  Q  ∇φ · ˆn ds = ρ  Q  V · ˆn ds   cid:21   P  is equal to the mass ﬂux between P and Q per unit length perpendicular to the xy-plane.  ∂φ ∂n  = − σ  ,   cid:4 0   cid:21   P  875   APPLICATIONS OF COMPLEX VARIABLES   cid:1  A conducting circular cylinder of radius a is placed with its centre line passing through the origin and perpendicular to a uniform electric ﬁeld E in the x-direction. Find the charge per unit length induced on the half of the cylinder that lies in the region x < 0.  As mentioned immediately following the previous example, the appropriate complex  potential for this problem is f z  = −E z − a2 z . Writing z = r exp iθ this becomes   cid:14    cid:13   cid:8   cid:7  r exp iθ − a2 r − a2  r  r   cid:7  exp −iθ   cos θ − iE   cid:8   a2 r  r +  sin θ,  f z  = −E = −E  so that on r = a the imaginary part of f is given by  ψ = −2Ea sin θ.  Therefore the induced charge q per unit length on the left half of the cylinder, between θ = π 2 and θ = 3π 2, is given by  q = 2 cid:4 0Ea[sin 3π 2  − sin π 2 ] = −4 cid:4 0Ea.  cid:2   25.2 Applications of conformal transformations  In section 24.7 of the previous chapter it was shown that, under a conformal transformation w = g z  from z = x + iy to a new variable w = r + is, if a solution of Laplace’s equation in some region R of the xy-plane can be found as the real of z, then the same expression put in or imaginary part of an analytic function terms of r and s will be a solution of Laplace’s equation in the corresponding of the w-plane, and vice versa. In addition, if the solution is constant region R over the boundary C of the region R in the xy-plane, then the solution in the that w-plane will take the same constant value over the corresponding curve C bounds R  §   cid:7    cid:7    cid:7   .  Thus, from any two-dimensional solution of Laplace’s equation for a particular geometry, typiﬁed by those discussed in the previous section, further solutions for other geometries can be obtained by making conformal transformations. From the physical point of view the given geometry is usually complicated, and so the solution is sought by transforming to a simpler one. However, working from simpler to more complicated situations can provide useful experience and make it more likely that the reverse procedure can be tackled successfully.  §  In fact, the original solution in the xy-plane need not be given explicitly as the real or imaginary  part of an analytic function. Any solution of ∇2φ = 0 in the xy-plane is carried over into another solution of ∇2φ = 0 in the new variables by a conformal transformation, and vice versa.  876   25.2 APPLICATIONS OF CONFORMAL TRANSFORMATIONS  y  s  s  x  r  r   a  z-plane   b  w-plane   c  w-plane  Figure 25.3 The equipotential lines  broken  and ﬁeld lines  solid   a  for an inﬁnite charged conducting plane at y = 0, where z = x + iy, and after the transformations  b  w = z2 and  c  w = z1 2 of the situation shown in  a .   cid:1  Find the complex electrostatic potential associated with an inﬁnite charged conducting plate y = 0, and thus obtain those associated with   i  a semi-inﬁnite charged conducting plate  r > 0, s = 0 ;  ii  the inside of a right-angled charged conducting wedge  r > 0, s = 0 and  r = 0, s > 0 .  Figure 25.3 a  shows the equipotentials  broken lines  and ﬁeld lines  solid lines  for the inﬁnite charged conducting plane y = 0. Suppose that we elect to make the real part of the complex potential coincide with the conventional electrostatic potential. If the plate is charged to a potential V then clearly  φ x, y  = V − ky,  has components  0, σ  cid:4 0  and E = −∇φ.  where k is related to the charge density σ by k = σ  cid:4 0, since physically the electric ﬁeld E  Thus what is needed is an analytic function of z, of which the real part is V − ky. This  can be obtained by inspection, but we may proceed formally and use the Cauchy–Riemann relations to obtain the imaginary part ψ x, y  as follows:  ∂ψ ∂y  ∂φ ∂x  =  = 0  and  = − ∂φ  ∂y  ∂ψ ∂x  = k.  Hence ψ = kx + c and, absorbing c into V , the required complex potential is  f z  = V − ky + ikx = V + ikz.   i  Now consider the transformation  w = g z  = z2.   25.6    25.7    25.8   This satisﬁes the criteria for a conformal mapping  except at z = 0  and carries the upper half of the z-plane into the entire w-plane; the equipotential plane y = 0 goes into the half-plane r > 0, s = 0.  By the general results proved, f z , when expressed in terms of r and s, will give a complex potential whose real part will be constant on the half-plane in question; we  877   APPLICATIONS OF COMPLEX VARIABLES  is the required potential. Expressed in terms of r, s and ρ =  r2 + s2 1 2, w1 2 is given by  deduce that  F w  = f z  = V + ikz = V + ikw1 2   cid:16  cid:7    cid:8   w1 2 = ρ1 2  ρ + r  2ρ  1 2  + i   cid:17    cid:8   1 2   cid:7   ρ − r  2ρ  and, in particular, the electrostatic potential is given by  Φ r, s  = Re F w  = V − k√   r2 + s2 1 2 − r   cid:18   2  ,   cid:19 1 2  .  The corresponding equipotentials and ﬁeld lines are shown in ﬁgure 25.3 b . Using results  25.3 – 25.5 , the magnitude of the electric ﬁeld is  E = F   cid:7    w  =  1  2 ikw  −1 2 = 1  2 k r2 + s2   −1 4.   ii  A transformation ‘converse’ to that used in  i ,  w = g z  = z1 2,  has the eﬀect of mapping the upper half of the z-plane into the ﬁrst quadrant of the w-plane and the conducting plane y = 0 into the wedge r > 0, s = 0 and r = 0, s > 0.  The complex potential now becomes  showing that the electrostatic potential is V−2krs and that the electric ﬁeld has components  F w  = V + ikw2  = V + ik[ r2 − s2  + 2irs],  E =  2ks, 2kr .   25.9    25.10    25.11    25.12    25.13   Figure 25.3 c  indicates the approximate equipotentials and ﬁeld lines.  Note that, in both transformations, g conformal there. Consequently there is no violation of result  ii , given at the start of section 24.7, concerning the angles between intersecting lines.   cid:2    z  is either 0 or ∞ at the origin, and so neither transformation is   cid:7   The method of images, discussed in section 21.5, can be used in conjunction with conformal transformations to solve some problems involving Laplace’s equation in two dimensions.   cid:1 A wedge of angle π α with its vertex at z = 0 is formed by two semi-inﬁnite conducting plates, as shown in ﬁgure 25.4 a . A line charge of strength q per unit length is positioned at z = z0, perpendicular to the z-plane. By considering the transformation w = zα, ﬁnd the complex electrostatic potential for this situation.  Let us consider the action of the transformation w = zα on the lines deﬁning the positions of the conducting plates. The plate that lies along the positive x-axis is mapped onto the positive r-axis in the w-plane, whereas the plate that lies along the direction exp iπ α  is mapped into the negative r-axis, as shown in ﬁgure 25.4 b . Similarly the line charge at z0 is mapped onto the point w0 = zα 0 .  From ﬁgure 25.4 b , we see that in the w-plane the problem can be solved by introducing ∗ 0, so that the potential Φ = 0 along  a second line charge of opposite sign at the point w the r-axis. The complex potential for such an arrangement is simply  F w  = − q  ln w − w0  +  2π cid:4 0  ln w − w  ∗ 0 .  q  2π cid:4 0  878   25.3 LOCATION OF ZEROS  y  s  w = zα  z0  π α  φ = 0  x  φ = 0   a   w0  r  ∗ 0  w  Φ = 0  Φ = 0   b   Figure 25.4  a  An inﬁnite conducting wedge with interior angle π α and a line charge at z = z0;  b  after the transformation w = zα, with an additional image charge placed at w = w  ∗ 0.  Substituting w = zα into the above shows that the required complex potential in the original z-plane is   cid:7    cid:8   zα − z ∗α zα − zα  0  0  .  cid:2   f z  =  q  2π cid:4 0  ln  It should be noted that the appearance of a complex conjugate in the ﬁnal expression is not in conﬂict with the general requirement that the complex potential be analytic. It is z is no more than a parameter of the problem.  that must not appear; here, z  ∗α  ∗  0  25.3 Location of zeros  The residue theorem, relating the value of a closed contour integral to the sum of the residues at the poles enclosed by the contour, was discussed in the previous chapter. One important practical use of an extension to the theorem is that of locating the zeros of functions of a complex variable. The location of such zeros has a particular application in electrical network and general oscillation theory, since the complex zeros of certain functions  usually polynomials  give the system parameters  usually frequencies  at which system instabilities occur. As the basis of a method for locating these zeros we next prove three important theorems.   i  If f z  has poles as its only singularities inside a closed contour C and is  not zero at any point on C then  0   cid:7    z  f f z   C   cid:4   Nj − Pj .  j  dz = 2πi   25.14   Here Nj is the order of the jth zero of f z  enclosed by C. Similarly Pj is the order of the jth pole of f z  inside C.  To prove this we note that, at each position zj, f z  can be written as  f z  =  z − zj mj φ z ,  879   25.15    APPLICATIONS OF COMPLEX VARIABLES   cid:4   j   cid:7    z  f f z   0  where φ z  is analytic and non-zero at z = zj and mj is positive for a zero and negative for a pole. Then the integrand f   z  f z  takes the form   cid:7    cid:7    z  f f z    cid:7   =  mj  z − zj  +   z  φ φ z   .   25.16   Since φ zj   cid:3 = 0, the second term on the RHS is analytic; thus the integrand has a simple pole at z = zj, with residue mj. For zeros mj = Nj and for poles mj = −Pj,  and thus  25.14  follows from the residue theorem.   ii  If f z  is analytic inside C and not zero at any point on it then  2π  Nj = ∆C[arg f z ],   25.17   where ∆C [x] denotes the variation in x around the contour C.  Since f is analytic, there are no Pj; further, since   25.18   equation  25.14  can be written   cid:4   =  [Ln f z ],  d dz   cid:7    z  f f z   C  2πi  Nj =  dz = ∆C[Ln f z ].   25.19   However,  ∆C[Ln f z ] = ∆C[lnf z ] + i∆C[arg f z ],   25.20   and, since C is a closed contour, lnf z  must return to its original value; so the  real term on the RHS is zero. Comparison of  25.19  and  25.20  then establishes  25.17 , which is known as the principle of the argument.   iii  If f z  and g z  are analytic within and on a closed contour C and  g z  < f z  on C then f z  and f z  + g z  have the same number of zeros  inside C; this is Rouch´e’s theorem.  With the conditions given, neither f z  nor f z  + g z  can have a zero on C.  So, applying theorem  ii  with an obvious notation,   cid:11   2π  jNj f + g  = ∆C[arg f + g ]  = ∆C[arg f] + ∆C[arg 1 + g f ] = 2π  kNk f  + ∆C[arg 1 + g f ].   25.21   Further, since g < f on C, 1 + g f always lies within a unit circle centred on z = 1; thus its argument always lies in the range −π 2 < arg 1 + g f  < π 2  and cannot change by any multiple of 2π. It must therefore return to its original value when z returns to its starting point having traversed C. Hence the second term on the RHS of  25.21  is zero and the theorem is estab- lished.  The importance of Rouch´e’s theorem is that for some functions, in particular   cid:11   880   25.3 LOCATION OF ZEROS   cid:11   polynomials, only the behaviour of a single term in the function need be con- sidered if the contour is chosen appropriately. For example, for a polynomial, N 0 bizi, only the properties of its largest power, taken as f z , need f z  + g z  = be investigated if a circular contour is chosen with radius R suﬃciently large that, on the contour, the magnitude of the largest power term, bN RN, is greater than the sum of the magnitudes of all other terms. It is obvious that f z  = bN zN has N zeros inside z = R  all at the origin ; consequently, f + g also has N zeros  inside the same circle.  The corresponding situation, in which only the properties of the polynomial’s smallest power, again taken as f z , need be investigated is a circular contour with a radius R chosen suﬃciently small that, on the contour, the magnitude of the smallest power term  usually the constant term in a polynomial  is greater than the sum of the magnitudes of all other terms. Then, a similar argument to  that given above shows that, since f z  = b0 has no zeros inside z = R, neither  does f + g.  A weak form of the maximum-modulus theorem may also be deduced. This  attains its maximum value on the boundary of C. The proof is as follows.  states that if f z  is analytic within and on a simple closed contour C then f z  Let f z  ≤ M on C with equality at at least one point of C. Now suppose that there is a point z = a inside C such that f a  > M. Then the function h z  ≡ f a  is such that h z  >  − f z  on C, and thus, by Rouch´e’s theorem, h z  and h z  − f z  have the same number of zeros inside C. But h z   ≡ f a   f a  − f z  has no zeros in C. However, f a  − f z  clearly has a zero at z = a, inside C such that f a  > M must be invalid. This establishes the theorem.  has no zeros inside C, and, again by Rouch´e’s theorem, this would imply that  and so we have a contradiction; the assumption of the existence of a point z = a  The stronger form of the maximum-modulus theorem, which we do not prove, states, in addition, that the maximum value of f z  is not attained at any interior point except for the case where f z  is a constant.   cid:1  Show that the four zeros of h z  = z4 + z + 1 occur one in each quadrant of the Argand diagram and that all four lie between the circles z = 2 3 and z = 3 2.  Putting z = x and z = iy shows that no zeros occur on the real or imaginary axes. They must therefore occur in conjugate pairs, as can be shown by taking the complex conjugate of h z  = 0.  Now take C as the contour OXY O shown in ﬁgure 25.5 and consider the changes  ∆[arg h] in the argument of h z  as z traverses C.   i  OX: arg h is everywhere zero, since h is real, and thus ∆OX[arg h] = 0.  ii  XY : z = R exp iθ and so arg h changes by an amount  ∆XY [arg h] = ∆XY [arg z4] + ∆XY [arg 1 + z  = ∆XY [arg R4e4iθ] + ∆XY = 2π + O R  −3 .  1  2  −4 ] −3 + z −3 ] arg[1 + O R   25.22   881   APPLICATIONS OF COMPLEX VARIABLES  y  Y  R  O  X  x  Figure 25.5 A contour for locating the zeros of a polynomial that occur in the ﬁrst quadrant of the Argand diagram.   iii  Y O: z = iy and so arg h = tan  −3  and ﬁnishes at 0 as y goes from large R to 0. It never reaches π 2 because y4 + 1 = 0 has no real positive root. Thus ∆Y O[arg h] = 0.  −1 y  y4 + 1 , which starts at O R  Hence for the complete contour ∆C [arg h] = 0 + 2π + 0 + O R  −3 , and, if R is allowed to tend to inﬁnity, we deduce from  25.17  that h z  has one zero in the ﬁrst quadrant. Furthermore, since the roots occur in conjugate pairs, a second root must lie in the fourth quadrant, and the other pair must lie in the second and third quadrants.  To show that the zeros lie within the given annulus in the z-plane we apply Rouch´e’s  theorem, as follows.   i  With C as z = 3 2, f = z4, g = z + 1. Now f = 81 16 on C and g ≤ 1 + z < 5 2 < 81 16. Thus, since z4 = 0 has four roots inside z = 3 2, so also does z4 + z + 1 = 0.  ii  With C as z = 2 3, f = 1, g = z4 + z. Now f = 1 on C and g ≤ z4 + z = 16 81 + 2 3 = 70 81 < 1. Thus, since f = 0 has no roots inside z = 2 3, neither does 1 + z + z4 = 0.  Hence the four zeros of h z  = z4 + z + 1 occur one in each quadrant and all lie between the circles z = 2 3 and z = 3 2.  cid:2   A further technique useful for locating the zeros of functions is explained in  exercise 25.8.  25.4 Summation of series  We now turn to an application of contour integration which at ﬁrst sight might seem to lie in an unrelated area of mathematics, namely the summation of inﬁnite series. Sometimes a real inﬁnite series with index n, say, can be summed with the help of a suitable complex function that has poles on the real axis at the various positions z = n with the corresponding residues at those poles equal to the values of the terms of the series. A worked example provides the best explanation of how the technique is applied; other examples will be found in the exercises.  882    cid:1 By considering  where a is not an integer and C is a circle of large radius, evaluate  25.4 SUMMATION OF SERIES  0 ∞ cid:4   C  n=−∞  π cot πz  a + z 2 dz,  1   a + n 2 .  The integrand has  i  simple poles at z = integer n, for −∞ < n < ∞, due to the factor cot πz and  ii  a double pole at z = −a.   i  To ﬁnd the residue of cot πz, put z = n + ξ for small ξ:  cot πz =  cos nπ + ξπ  sin nπ + ξπ   ≈ cos nπ  cos nπ ξπ −1.  −2π  =  1 ξπ  .  The residue of the integrand at z = n is thus π a + n    ii  Putting z = −a + ξ for small ξ and determining the coeﬃcient of ξ  cid:15   π cot πz  a + z 2   cid:13    cid:14   =  −1 gives  §   cid:12  cot −aπ + ξπ  cot −aπ  + ξ  π ξ2 π ξ2  =  d dz   cot πz   z=−a  + ···  ,  so that the residue at the double pole z = −a is given by  π[−π cosec 2πz]z=−a = −π2 cosec 2πa.  Collecting together these results to express the residue theorem gives   cid:17   0  I =  π cot πz  a + z 2 dz = 2πi  C  n=−N   a + n 2  1  − π2cosec 2πa  ,   25.23   where N equals the integer part of R. But as the radius R of C tends to ∞, cot πz → ∓i   depending on whether Im z is greater or less than zero, respectively . Thus   cid:16   N cid:4    cid:21   which tends to 0 as R → ∞. Thus I → 0 as R  and hence N  → ∞, and  25.23  establishes  the result  I < k  dz   a + z 2 ,  ∞ cid:4   1  =  π2  .  cid:2   sin2 πa  n=−∞   a + n 2  Series with alternating signs in the terms, i.e.  −1 n, can also be attempted  −1 nπ  in this way but using cosec πz rather than cot πz, since the former has residue  −1 at z = n  see exercise 25.11 .  §  This again illustrates one of the techniques for determining residues.  883   APPLICATIONS OF COMPLEX VARIABLES  Im s  Re s  L  λ  Figure 25.6 The integration path of the inverse Laplace transform is along the inﬁnite line L. The quantity λ must be positive and large enough for all poles of the integrand to lie to the left of L.  25.5 Inverse Laplace transform  As a further example of the use of contour integration we now discuss a method whereby the process of Laplace transformation, discussed in chapter 13, can be inverted.  It will be recalled that the Laplace transform ¯f s  of a function f x , x ≥ 0, is  given by  ¯f s  =  −sxf x  dx,  e  Re s > s0.   25.24    cid:21  ∞  0  In chapter 13, functions f x  were deduced from the transforms by means of a prepared dictionary. However, an explicit formula for an unknown inverse may be written in the form of an integral. It is known as the Bromwich integral and is given by   cid:21  λ+i∞ λ−i∞ esx¯f s  ds,  f x  =  1 2πi  λ > 0,   25.25   where s is treated as a complex variable and the integration is along the line L indicated in ﬁgure 25.6. The position of the line is dictated by the requirements that λ is positive and that all singularities of ¯f s  lie to the left of the line.  That  25.25  really is the unique inverse of  25.24  is diﬃcult to show for general functions and transforms, but the following veriﬁcation should at least make it  884   25.5 INVERSE LAPLACE TRANSFORM  Γ  R  Γ  R  Γ  R  L  L  L   a    b    c   Figure 25.7 Some contour completions for the integration path L of the inverse Laplace transform. For details of when each is appropriate see the main text.  plausible:  f x  =  0   cid:21  ∞ λ+i∞  cid:21  −suf u  du, λ−i∞ ds esx e λ+i∞  cid:21  ∞ λ−i∞ es x−u  ds −∞ eλ x−u eip x−u i dp,   cid:21   cid:21  ∞  cid:21  ∞  cid:21  ∞ f u eλ x−u 2πδ x − u  du f x  x ≥ 0,  du f u   du f u   0  0  0  1 2πi 1 2πi 1 2πi 1 2π    =  =  =  =  0  x < 0.  Re s  > 0, i.e. λ > 0,  putting s = λ + ip,   25.26   Our main purpose here is to demonstrate the use of contour integration. To employ it in the evaluation of the line integral  25.25 , the path L must be made part of a closed contour in such a way that the contribution from the completion either vanishes or is simply calculable.  A typical completion is shown in ﬁgure 25.7 a  and would be appropriate if ¯f s  had a ﬁnite number of poles. For more complicated cases, in which ¯f s  has an inﬁnite sequence of poles but all to the left of L as in ﬁgure 25.7 b , a sequence of circular-arc completions that pass between the poles must be used and f x  is obtained as a series. If ¯f s  is a multivalued function then a cut plane is needed and a contour such as that shown in ﬁgure 25.7 c  might be appropriate.  We consider here only the simple case in which the contour in ﬁgure 25.7 a  is used; we refer the reader to the exercises at the end of the chapter for others.  885   APPLICATIONS OF COMPLEX VARIABLES  Ideally, we would like the contribution to the integral from the circular arc Γ to  tend to zero as its radius R → ∞. Using a modiﬁed version of Jordan’s lemma,  it may be shown that this is indeed the case if there exist constants M > 0 and α > 0 such that on Γ  Moreover, this condition always holds when ¯f s  has the form  ¯f s  ≤ M Rα .  ¯f s  =  P  s  Q s   ,  where P  s  and Q s  are polynomials and the degree of Q s  is greater than that of P  s .  When the contribution from the part-circle Γ tends to zero as R → ∞, we  have from the residue theorem that the inverse Laplace transform  25.25  is given simply by   cid:4  cid:5    cid:6   f t  =  residues of ¯f s esx at all poles  .   25.27    cid:1 Find the function f x  whose Laplace transform is s2 − k2 ,  ¯f s  =  s  where k is a constant.  It is clear that ¯f s  is of the form required for the integral over the circular arc Γ to tend to zero as R → ∞, and so we may use the result  25.27  directly. Now  and thus has simple poles at s = k and s = −k. Using  24.57  the residues at each pole can  ¯f s esx =  sesx   s − k  s + k   ,  be easily calculated as  R k  =  and  kekx 2k   cid:5   R −k  =  −kx  ke  2k  .   cid:6   Thus the inverse Laplace transform is given by −kx  f x  = 1 2  ekx + e  = cosh kx.  This result may be checked by computing the forward transform of cosh kx.  cid:2   Sometimes a little more care is required when deciding in which half-plane to  close the contour C.  cid:1 Find the function f x  whose Laplace transform is −as − e  ¯f s  =   e  −bs ,  where a and b are ﬁxed and positive, with b > a.  From  25.25  we have the integral  1 s   cid:21   886  f x  =  λ+i∞ λ−i∞  1 2πi  e x−a s − e x−b s  ds.  s   25.28    25.5 INVERSE LAPLACE TRANSFORM  f x   1  a  b  x  Figure 25.8 The result of the Laplace inversion of ¯f s  = s b > a.  −1 e  −as− e  −bs  with  Now, despite appearances to the contrary, the integrand has no poles, as may be conﬁrmed by expanding the exponentials as Taylor series about s = 0. Depending on the value of x, several cases arise.   i  For x < a both exponentials in the integrand will tend to zero as Re s → ∞. Thus and we observe that s × integrand tends to zero everywhere on Γ as R → ∞. With no  we may close L with a circular arc Γ in the right half-plane  λ can be as small as desired ,  poles enclosed and no contribution from Γ, the integral along L must also be zero. Thus  f x  = 0  for x < a.   25.29    ii  For x > b the exponentials in the integrand will tend to zero as Re s → −∞, and  so we may close L in the left half-plane, as in ﬁgure 25.7 a . Again the integral around Γ vanishes for inﬁnite R, and so, by the residue theorem,  f x  = 0  for x > b.   25.30    iii  For a < x < b the two parts of the integrand behave in diﬀerent ways and have to  be treated separately:   cid:21    cid:21   I1 − I2 ≡ 1  e x−a s  ds − 1  e x−b s  ds.  2πi  L  s  2πi  L  s  The integrand of I1 then vanishes in the far left-hand half-plane, but does now have a  simple  pole at s = 0. Closing L in the left half-plane, and using the residue theorem, we obtain  I1 = residue at s = 0 of s  −1e x−a s = 1.   25.31   The integrand of I2, however, vanishes in the far right-hand half-plane  and also has a simple pole at s = 0  and is evaluated by a circular-arc completion in that half-plane. Such a contour encloses no poles and leads to I2 = 0.  Thus, collecting together results  25.29 – 25.31  we obtain   0  1 0  f x  =  for x < a, for a < x < b, for x > b,  887  as shown in ﬁgure 25.8.  cid:2    APPLICATIONS OF COMPLEX VARIABLES  25.6 Stokes’ equation and Airy integrals  Much of the analysis of situations occurring in physics and engineering is con- cerned with what happens at a boundary within or surrounding a physical system. Sometimes the existence of a boundary imposes conditions on the behaviour of variables describing the state of the system; obvious examples include the zero displacement at its end-points of an anchored vibrating string and the zero potential contour that must coincide with a grounded electrical conductor.  More subtle are the eﬀects at internal boundaries, where the same non-vanishing variable has to describe the situation on either side of the boundary but its behaviour is quantitatively, or even qualitatively, diﬀerent in the two regions. In this section we will study an equation, Stokes’ equation, whose solutions have this latter property; as well as solutions written as series in the usual way, we will ﬁnd others expressed as complex integrals.  The Stokes’ equation can be written in several forms, e.g.  d2y dx2 + λxy = 0;  d2y dx2 + xy = 0;  d2y dx2 = xy.  We will adopt the last of these, but write it as  d2y dz2 = zy   25.32   to emphasis that its complex solutions are valid for a complex independent variable z, though this also means that particular care has to be exercised when examining their behaviour in diﬀerent parts of the complex z-plane. The other forms of Stokes’ equation can all be reduced to that of  25.32  by suitable  complex  scaling of the independent variable.  25.6.1 The solutions of Stokes’ equation   cid:7  cid:7   It will be immediately apparent that, even for z restricted to be real and denoted by x, the behaviour of the solutions to  25.32  will change markedly as x passes through x = 0. For positive x they will have similar characteristics to the solutions = k2y, where k is real; these have monotonic exponential forms, either of y increasing or decreasing. On the other hand, when x is negative the solutions + k2y = 0, i.e. oscillatory functions of x. This is will be similar to those of y just the sort of behaviour shown by the wavefunction describing light diﬀracted by a sharp edge or by the quantum wavefunction describing a particle near to the boundary of a region which it is classically forbidden to enter on energy grounds. Other examples could be taken from the propagation of electromagnetic radiation in an ion plasma or wave-guide.   cid:7  cid:7   Let us examine in a bit more detail the behaviour of plots of possible solutions y z  of Stokes’ equation in the region near z = 0 and, in particular, what may  888   25.6 STOKES’ EQUATION AND AIRY INTEGRALS  y   b    c    a    a    c    b   z  Figure 25.9 Behaviour of the solutions y z  of Stokes’ equation near z = 0  0 .  a  with λ small,  b  with λ large and  c  with  for various values of λ = −y   cid:7   λ appropriate to the Airy function Ai z .  happen in the region z > 0. For deﬁniteness and ease of illustration  see ﬁgure 25.9 , let us suppose that both y and z, and hence the derivatives of y, are real and that y 0  is positive; if it were negative, our conclusions would not be changed  since equation  25.32  is invariant under y z  → −y z . The only diﬀerence would  be that all plots of y z  would be reﬂected in the z-axis.   cid:7   We ﬁrst note that d2y dx2, and hence also the curvature of the plot, has the same sign as z, i.e. it has positive curvature when z > 0, for so long as y z  remains positive there. What will happen to the plot for z > 0 therefore depends  0 . If this slope is positive or only slightly negative crucially on the value of y the positive curvature will carry the plot, either immediately or ultimately, further  0  is negative but suﬃciently large away from the z-axis. On the other hand, if y in magnitude, the plot will cross the y = 0 line; if this happens the sign of the curvature reverses and again the plot will be carried ever further from the z-axis, only this time towards large negative values.   cid:7    cid:7   Between these two extremes it seems at least plausible that there is a particular  0  that leads to a plot that approaches the z-axis asymptot- negative value of y ically, never crosses it  and so always has positive curvature , and has a slope that, whilst always negative, tends to zero in magnitude. There is such a solu- tion, known as Ai z , whose properties we will examine further in the following subsections. The three cases are illustrated in ﬁgure 25.9.  The behaviour of the solutions of  25.32  in the region z < 0 is more straight-  889   APPLICATIONS OF COMPLEX VARIABLES  forward, in that, whatever the sign of y at any particular point z, the curvature always has the opposite sign. Consequently the curve always bends towards the z-axis, crosses it, and then bends towards the axis again. Thus the curve exhibits  oscillatory behaviour. Furthermore, as −z increases, the curvature for any given y gets larger; as a consequence, the oscillations become increasingly more rapid  and their amplitude decreases.  25.6.2 Series solution of Stokes’ equation  Obtaining a series solution of Stokes’ equation presents no particular diﬃculty when the methods of chapter 16 are used. The equation, written in the form  − zy = 0,  d2y dz2  has no singular points except at z = ∞. Every other point in the z-plane is  an ordinary point and so two linearly independent series expansions about it  formally with indicial values σ = 0 and σ = 1  can be found. Those about z = 0 0 bnzn+1. The corresponding recurrence relations take the forms are  0 anzn and   cid:11 ∞   cid:11 ∞   n + 3  n + 2 an+3 = an  and  n + 4  n + 3 bn+3 = bn,  and the two series  with a0 = b0 = 1  take the forms  y1 z  = 1 +  y2 z  = z +  z3  z4  +  +  z6  z7   3  2    6  5  3  2    4  3    7  6  4  3   + ··· , + ··· .  The ratios of successive terms for the two series are thus  an+3zn+3  anzn  =  z3   n + 3  n + 2   and  bn+3zn+4 bnzn+1 =  z3   n + 4  n + 3   .  It follows from the ratio test that both series are absolutely convergent for all z. A similar argument shows that the series for their derivatives are also absolutely convergent for all z. Any solution of the Stokes’ equation is representable as a superposition of the two series and so is analytic for all ﬁnite z; it is therefore an integral function with its only singularity at inﬁnity.  25.6.3 Contour integral solutions  We now move on to another form of solution of the Stokes’ equation  25.32 , one that takes the form of a contour integral in which z appears as a parameter in  890   25.6 STOKES’ EQUATION AND AIRY INTEGRALS   cid:21   b  a  the integrand. Consider the contour integral  y z  =  f t  exp zt  dt,   25.33   in which a, b and f t  are all yet to be chosen. Note that the contour is in the complex t-plane and that the path from a to b can be distorted as required so long as no poles of the integrand are trapped between an original path and its distortion.  Substitution of  25.33  into  25.32  yields   cid:21   b  a  t2 f t  exp zt  dt =   cid:21   b  a   cid:21   z f t  exp zt  dt −  = [ f t  exp zt  ] b a  b  df t   a  dt  exp zt  dt.  If we could choose the limits a and b so that the end-point contributions vanish then Stokes’ equation would be satisﬁed by  25.33 , provided f t  satisﬁes  + t2f t  = 0 ⇒ f t  = A exp − 1  3 t3 ,  df t   dt   25.34   where A is any constant.  To make the end-point contributions vanish we must choose a and b such that  3 t3 + zt  = 0 for both values of t. This can only happen if a → ∞ and exp − 1 b → ∞ and, even then, only if Re  t3  is positive. This condition is satisﬁed if  2nπ − 1  2 π < 3 arg t  < 2nπ + 1  2 π for some integer n.  Thus a and b must each be at inﬁnity in one of the three shaded areas shown in ﬁgure 25.10, but clearly not in the same area as this would lead to a zero value for the contour integral. This leaves three contours  marked C1, C2 and C3 in the ﬁgure  that start and end in diﬀerent sectors. However, only two of them give rise to independent integrals since the path C2 + C3 is equivalent to  can be distorted into  the path C1.  The two integral functions given particular names are   cid:21   Ai z  =  1 2πi  C1  exp − 1  3 t3 + zt  dt   cid:21    cid:21   Bi z  =  1 2π  C2  exp − 1  3 t3 + zt  dt − 1  exp − 1  3 t3 + zt  dt.  2π  C3   25.35    25.36   and  Stokes’ equation is unchanged if the independent variable is changed from z  to ζ, where ζ = exp 2πi 3 z ≡ Ωz. This is also true for the repeated change z → Ωζ = Ω2z. The same changes of variable, rotations of the complex plane  through 2π 3 or 4π 3, carry the three contours C1, C2 and C3 into each other,  891   APPLICATIONS OF COMPLEX VARIABLES  Im t  C3  C2  C1  Re t  Figure 25.10 The contours used in the complex t-plane to deﬁne the functions Ai z  and Bi z .  though sometimes the sense of traversal is reversed. Consequently there are relationships connecting Ai and Bi when the rotated variables are used as their arguments. As two examples,  Ai z  + ΩAi Ωz  + Ω2Ai Ω2z  = 0,  Bi z  = i[ Ω2Ai Ω2z  − ΩAi Ωz  ] = e  −πi 6Ai ze  −2πi 3  + eπi 6Ai ze2πi 3 .   25.37    25.38   Since the only requirements for the integral paths is that they start and end in the correct sectors, we can distort path C1 so that it lies on the imaginary axis for virtually its whole length and just to the left of the axis at its two ends. This enables us to obtain an alternative expression for Ai z , as follows.  Setting t = is, where s is real and −∞ < s < ∞, converts the integral represen-  tation of Ai z  to  Ai z  =  1 2π  −∞ exp[ i  1  3 s3 + zs  ] ds.  Now, the exponent in this integral is an odd function of s and so the imaginary part of the integrand contributes nothing to the integral. What is left is therefore  Ai z  =  cos  1  3 s3 + zs  ds.   25.39   This form shows explicitly that when z is real, so is Ai z .  This same representation can also be used to justify the association of the   cid:21  ∞  cid:21  ∞  1 π  0  892   25.6 STOKES’ EQUATION AND AIRY INTEGRALS  contour integral  25.35  with the particular solution of Stokes’ equation that  decays monotonically to zero for real z > 0 as z → ∞. As discussed in subsection 25.6.1, all solutions except the one called Ai z  tend to ±∞ as z  real  takes on  increasingly large positive values and so their asymptotic forms reﬂect this. In a worked example in subsection 25.8.2 we use the method of steepest descents  a saddle-point method  to show that the function deﬁned by  25.39  has exactly the characteristic asymptotic property expected of Ai z   see page 911 . It follows that it is the same function as Ai z , up to a real multiplicative constant.  The choice of deﬁnition  25.36  as the other named solution Bi z  of Stokes’ equation is a less obvious one. However, it is made on the basis of its behaviour for negative real values of z. As discussed earlier, Ai z  oscillates almost sinusoidally in this region, except for a relatively slow increase in frequency and an even  slower decrease in amplitude as −z increases. The solution Bi z  is chosen to be in quadrature with Ai, i.e. it is π 2 out of phase with it. Speciﬁcally, as x → −∞,  the particular function that exhibits the same behaviour as Ai z  except that it is   cid:7  2x3 2  cid:7  2x3 2  3  3   cid:8   cid:8   ,  .  +  +  π 4  π 4  Ai x  ∼ Bi x  ∼  1√ 2πx1 4 1√ 2πx1 4  sin  cos   25.40    25.41   There is a close parallel between this choice and that of taking sine and cosine functions as the basic independent solutions of the simple harmonic oscillator equation. Plots of Ai z  and Bi z  for real z are shown in ﬁgure 25.11.   cid:1 By choosing a suitable contour for C1 in  25.35 , express Ai 0  in terms of the gamma function.  With z set equal to zero,  25.35  takes the form  Ai 0  =  exp − 1  3 t3  dt.  1 2πi  C1  We again use the freedom to choose the speciﬁc line of the contour so as to make the actual integration as simple as possible.  Here we consider C1 as made up of two straight-line segments: one along the line arg t = 4π 3, starting at inﬁnity in the correct sector and ending at the origin; the other starting at the origin and going to inﬁnity along the line arg t = 2π 3, thus ending in the correct ﬁnal sector. On each, we set 1 3 t3 = s, where s is real and positive on both lines. Then dt = e4πi 3 3s   −2 3 ds on the ﬁrst segment and dt = e2πi 3 3s   −2 3 ds on the second.   cid:21   893   APPLICATIONS OF COMPLEX VARIABLES  1  0.5  −0.5  −10  −5  z  Figure 25.11 The functions Ai z   full line  and Bi z   broken line  for real z.  Then we have  Ai 0  =   cid:21  ∞  −se2πi 3 3s   −2 3 ds  e   cid:21   1 2πi −2 3 3 √ 2πi 3i 3 2πi −1 6  3 2π  =  =  =  0  Γ  1  3  ,  0   cid:21  ∞  ∞ e  0  −2 3  −se4πi 3 3s  −2 3 ds +  cid:21  ∞ −s −e4πi 3 + e2πi 3 s  e  1 2πi −2 3 ds  0  −ss  −2 3 ds  e  where we have used the standard integral deﬁning the gamma function in the last line.  cid:2   derivatives are closely related to Bessel functions of orders ± 1  Finally in this subsection we should mention that the Airy functions and their 3 and that  3 and ± 2  894   25.7 WKB METHODS  there exist many representations, both as linear combinations and as indeﬁnite § integrals, of one in terms of the other.  25.7 WKB methods  Throughout this book we have had many occasions on which it has been necessary to solve the equation  d2y dx2 + k2  0f x y = 0   25.42   when the notionally general function f x  has been, in fact, a constant, usually the unit function f x  = 1. Then the solutions have been elementary and of the form A sin k0x or A cos k0x with arbitrary but constant amplitude A.  Explicit solutions of  25.42  for a non-constant f x  are only possible in a limited number of cases, but, as we will show, some progress can be made if f x  is a slowly varying function of x, in the sense that it does not change much in a range of x of the order of k  −1 0 .  We will also see that it is possible to handle situations in which f x  is complex; this enables us to deal with, for example, the passage of waves through an absorbing medium. Developing such solutions will involve us in ﬁnding the integrals of some complex quantities, integrals that will behave diﬀerently in the various parts of the complex plane – hence their inclusion in this chapter.  25.7.1 Phase memory  ¶ Before moving on to the formal development of WKB methods concept of phase memory which is the underlying idea behind them.  we discuss the  Let us ﬁrst suppose that f x  is real, positive and essentially constant over a range of x and deﬁne n x  as the positive square root of f x ; n x  is then also real, positive and essentially constant over the same range of x. We adopt this notation so that the connection can be made with the description of an electromagnetic wave travelling through a medium of dielectric constant f x  and, consequently, refractive index n x . The quantity y x  would be the electric or magnetic ﬁeld of the wave. For this simpliﬁed case, in which we can omit the  §  ¶  These relationships and many other properties of the Airy functions can be found in, for example, M. Abramowitz and I. A. Stegun  eds , Handbook of Mathematical Functions  New York: Dover, 1965  pp. 446–50.  So called because they were used, independently, by Wentzel, Kramers and Brillouin to tackle certain wave-mechanical problems in 1926, though they had earlier been studied in some depth by Jeﬀreys and used as far back as the ﬁrst half of the nineteenth century by Green.  895   APPLICATIONS OF COMPLEX VARIABLES  x-dependence of n x , the solution would be  as usual   y x  = A exp −ik0nx ,   25.43   with both A and n constant. The quantity k0nx would be real and would be called the ‘phase’ of the wave; it increases linearly with x.  As a ﬁrst variation on this simple picture, we may allow f x  to be complex, though, for the moment, still constant. Then n x  is still a constant, albeit a complex one:  n = µ + iν.  The solution is formally the same as before; however, whilst it still exhibits oscillatory behaviour, the amplitude of the oscillations either grows or declines, depending upon the sign of ν:  y x  = A exp −ik0nx  = A exp[−ik0 µ + iν x ] = A exp k0νx  exp −ik0µx .  This solution with ν negative is the appropriate description for a wave travelling in a uniform absorbing medium. The quantity k0 µ + iν x is usually called the complex phase of the wave.  We now allow f x , and hence n x , to be both complex and varying with position, though, as we have noted earlier, there will be restrictions on how rapidly f x  may vary if valid solutions are to be obtained. The obvious extension of solution  25.43  to the present case would be  y x  = A exp[−ik0n x x ],  but direct substitution of this into equation  25.42  gives x y − ik0 n  cid:7  cid:7   0n2y = −k2  cid:7 2 0 n   cid:7  x2 + 2nn  + k2   cid:7  cid:7   y   cid:7  x + 2n   y.   cid:7  Clearly the RHS can only be zero, as is required by the equation, if n both very small, or if some unlikely relationship exists between them.  To try to improve on this situation, we consider how the phase φ of the solution changes as the wave passes through an inﬁnitesimal thickness dx of the medium. The inﬁnitesimal  complex  phase change dφ for this is clearly k0n x  dx, and therefore will be   25.44    cid:7  cid:7  and n  are  for a ﬁnite thickness x of the medium. This suggests that an improvement on  25.44  might be  ∆φ = k0  n u  du   cid:8   y x  = A exp  n u  du  .   25.45   This is still not an exact solution as now   cid:7  cid:7   y   x  + k2  0n2 x y x  = −ik0n  cid:7    x y x .   cid:21   x   cid:21   0   cid:7  −ik0  x  0  896   25.7 WKB METHODS   cid:7  0n2 x  , but is some This still requires k0n improvement  not least in complexity!  on  25.44  and gives some measure of the conditions under which the solution might be a suitable approximation.   x  to be small  compared with, say, k2  The integral in equation  25.45  embodies what is sometimes referred to as the phase memory approach; it expresses the notion that the phase of the wave-like solution is the cumulative eﬀect of changes it undergoes as it passes through the medium. If the medium were uniform the overall change would be proportional to nx, as in  25.43 ; the extent to which it is not uniform is reﬂected in the amount by which the integral diﬀers from nx.  k  −1   cid:16  n2 or, in words, the change in n over an x-range of k  The condition for solution  25.45  to be a reasonable approximation can be  cid:7  should written as n be small compared with n2. For light in an optical medium, this means that the refractive index n, which is of the order of unity, must change very little over a distance of a few wavelengths.  −1  0  0  For some purposes the above approximation is adequate, but for others further reﬁnement is needed. This comes from considering solutions that are still wave- like but have amplitudes, as well as phases, that vary with position. These are the WKB solutions developed and studied in the next three subsections.  25.7.2 Constructing the WKB solutions  Having formulated the notion of phase memory, we now construct the WKB solutions of the general equation  25.42 , in which f x  can now be both position- dependent and complex. As we have already seen, it is the possibility of a complex phase that permits the existence of wave-like solutions with varying amplitudes. Since n x  is calculated as the square root of f x , there is an ambiguity in its overall sign. In physical applications this is normally resolved unambiguously by considerations such as the inevitable increase in entropy of the system, but, so far as dealing with purely mathematical questions is concerned, the ambiguity must be borne in mind.  The process we adopt is an iterative one based on the assumption that the second derivative of the complex phase with respect to x is very small and can be approximated at each stage of the iteration. So we start with equation  25.42  and look for a solution of the form  where A is a constant. When this is substituted into  25.42  the equation becomes  y x  = A exp[ iφ x  ],   25.46    cid:16    cid:7   −   cid:8   2   cid:17   dφ dx  + i  d2φ dx2 + k2  0n2 x   y x  = 0.   25.47   Setting the quantity in square brackets to zero produces a non-linear equation for  897   APPLICATIONS OF COMPLEX VARIABLES  which there is no obvious solution for a general n x . However, on the assumption that d2φ dx2 is small, an iterative solution can be found.  As a ﬁrst approximation φ  is ignored, and the solution  is obtained. From this, diﬀerentiation gives an approximate value for  which can be substituted into equation  25.47  to give, as a second approximation for dφ dx, the expression  ≈ ± k0n x   dφ dx  d2φ dx2  ≈ ± k0  dn dx  ,   cid:7  cid:7    cid:13   dφ dx  dn dx   cid:7  ≈ ± 0n2 x  ± ik0 k2 = ± k0n 1 ± i 2k0n2 ≈ ± k0n + dn  cid:21  dx  i 2n  .  φ x  = ± k0  x  x0   cid:14   1 2   cid:8   + ···  dn dx  i 2   cid:21   x  x0  This can now be integrated to give an approximate expression for φ x  as follows:  n u  du +  ln[ n x  ],   25.48   where the constant of integration has been formally incorporated into the lower −1 2, substitution of limit x0 of the integral. Now, noting that exp i 1  25.48  into equation  25.46  gives  2 i ln n  = n   cid:13    cid:14   y± x  =  A n1 2  exp  ± ik0  n u  du   25.49   as two independent WKB solutions of the original equation  25.42 . This result is √ essentially the same as that in  25.45  except that the amplitude has been divided n x , i.e. by [ f x  ]1 4. Since f x  may be complex, this may introduce an by additional x-dependent phase into the solution as well as the more obvious change in amplitude.  cid:1 Find two independent WKB solutions of Stokes’ equation in the form  d2y dx2  + λxy = 0, with λ real and > 0.  The form of the equation is the same as that in  25.42  with f x  = x, and therefore n x  = x1 2. The WKB solutions can be read oﬀ immediately using  25.49 , so long as we remember that although f x  is real, it has four fourth roots and that therefore the constant appearing in a solution can be complex. Two independent WKB solutions are   cid:13    cid:21   √ ± i  λ  x √   cid:14    cid:17    cid:16   √  3  λ  ± i  2  u du  =  exp  x3 2  .  A±x1 4  y± x  =  A±x1 4  exp   25.50   898   25.7 WKB METHODS  The precise combination of these two solutions that is required for any particular problem has to be determined from the problem.  cid:2   When Stokes’ equation is applied more generally to functions of a complex variable, i.e. the real variable x is replaced by the complex variable z, it has solutions whose type of behaviour depends upon where z lies in the complex  plane. For the particular case λ = −1, when Stokes’ equation takes the form  and the two WKB solutions  with the inverse fourth root written explicitly  are  d2y dz2 = zy   cid:13    cid:14   y1,2 z  =  exp  A1,2 z1 4  ∓ 2 3  z3 2  ,   25.51   one of the solutions, Ai z   see section 25.6 , has the property that it is real whenever z is real, whether positive or negative. For negative real z it has sinusoidal behaviour, but it becomes an evanescent wave for real positive z.  Since the function z3 2 has a branch point at z = 0 and therefore has an abrupt  complex  change in its argument there, it is clear that neither of the two functions in  25.51 , nor any ﬁxed combination of them, can be equal to Ai z  for all values of z. More explicitly, for z real and positive, Ai z  is proportional to y1 z , which is real and has the form of a decaying exponential function, whilst for z real and negative, when z3 2 is purely imaginary and y1 z  and y2 z  are both oscillatory, it is clear that Ai z  must contain both y1 and y2 with equal amplitudes.  The actual combinations of y1 z  and y2 z  needed to coincide with these two  asymptotic forms of Ai z  are as follows. √ 2  For z real and > 0,  c1y1 z  =  For z real and < 0,  1 πz1 4   cid:13   c2[ y1 z eiπ 4 − y2 z e 1√ π −z 1 4  −iπ 4 ]  −z 3 2 +  sin  2 3  =   cid:14   π 4  .   cid:14    cid:13   − 2 3  exp  z3 2  .   25.52    25.53   Therefore it must be the case that the constants used to form Ai z  from the solutions  25.51  change as z moves from one part of the complex plane to another. In fact, the changes occur for particular values of the argument of z; these boundaries are therefore radial lines in the complex plane and are known as Stokes lines. For Stokes’ equation they occur when arg z is equal to 0, 2π 3 or 4π 3.  The general occurrence of a change in the arbitrary constants used to make up a solution, as its argument crosses certain boundaries in the complex plane, is known as the Stokes phenomenon and is discussed further in subsection 25.7.4.  899   APPLICATIONS OF COMPLEX VARIABLES   cid:1 Apply the WKB method to the problem of ﬁnding the quantum energy levels E of a particle of mass m bound in a symmetrical one-dimensional potential well V  x  that has only a single minimum. The relevant Schr¨odinger equation is  −  cid:1 2 2m  d2ψ dx2  + V  x ψ = Eψ.  Relate the problem close to each of the classical ‘turning points’, x = ± a at which E −  V  x  = 0, to Stokes’ equation and assume that it is appropriate to use the solution Ai x  given in equations  25.52  and  25.53  at x = a. Show that if the general WKB solution in  the ‘classically allowed’ region −a < x < a is to match such Airy solutions at both turning  points, then   cid:21   a  −a  2  π,  k x  dx =  n + 1 where k2 x  = 2m[ E − V  x  ]  cid:1 2 and n = 0, 1, 2, . . . . approximation the energy of the nth level is given by En = cs n + 1 constant depending on s but not upon n.  For a symmetric potential V  x  = V0x2s, where s is a positive integer, show that in this 2  2s  s+1 , where cs is a We start by multiplying the equation through by 2m  cid:1 2, writing 2m[ E− V  x  ]  cid:1 2 as k2 x ,  and rearranging the equation to read  d2ψ dx2  + k2 x ψ = 0,   25.54   noting that, with E and V  x  given, the equation E = V  a  determines the value of a and that k a  = 0.  For −a < x < a, where k2 x  is positive, the form of the WKB solutions are given  directly by  25.49  as   cid:13    cid:21    cid:14   ψ± =  C√ k x   x  ± i  exp  k u  du  .  Just beyond the turning point x = a, where  E − V  x  = 0 − V   cid:7    a  x − a  + O[  x − a 2 ],  equation  25.54  can be approximated by  This, in turn, can be reduced to Stokes’ equation by ﬁrst setting x−a = µz and ψ x  ≡ y z ,   cid:7    a   d2ψ dx2  − 2mV   cid:1 2   x − a ψ = 0.   25.55   so converting it into  and then choosing µ = [  cid:1 2 2mV   a  ]1 3. The equation then reads  − 2µmV   cid:1 2  d2y dz2   cid:7    a   zy = 0,  1 µ2  cid:7   Since the solution must be evanescent for x > a, i.e. for z > 0, we assume that the appropriate solution there is Ai z ; this implies that, for z small and negative  just inside the classically allowed region , the solution has the form given by  25.53 , namely   cid:14   A   −z 1 4  sin  2 3   −z 3 2 +  π 4  ,  = zy.  d2y dz2   cid:13   900   for some constant A. This form is only valid for negative z close to z = 0 and is not appropriate within the well as a whole, where the approximation  25.55  leading to Stokes’ equation is not valid. However, it does allow us to determine the correct combination of the WKB solutions found earlier for the proper continuation inside the well of the solution found for z > 0. This is  A similar argument gives the continuation inside the well of the evanescent solution  required in the region x < −a as  25.7 WKB METHODS  ψ1 x  =  sin  k u  du +  ψ2 x  =  sin  k u  du +   cid:7  cid:21   a  x   cid:7  cid:21   x  −a  A√ k x   B√ k x   cid:8    cid:8    cid:8   .  .  π 4  π 4  a  sin   cid:7  cid:21   However, for a consistent solution to the problem, these two functions must match, both in magnitude and slope, at any arbitrary point x inside the well.We therefore require both of the equalities A√ k x   cid:7  cid:24  − 1  cid:7  cid:24  2   cid:7  cid:21  [−k x  ] cos  cid:7  cid:21    cid:7  cid:21   cid:7  cid:21   cid:7  cid:21    cid:8   cid:8   B√ k x   A√ k x   Ak k3 x   k u  du +  k u  du +  k u  du +  k u  du +   cid:8    cid:8    cid:8   and  π 4  π 4  π 4  π 4  −a  sin  sin   i   +  =  x  x  x  x  x  x  a  a  k u  du +  .   ii   [ k x  ] cos  k u  du +  sin  +  = − 1  2  Bk k3 x   −a  π 4  B√ k x   π 4  −a  The general condition for the validity of the WKB solutions is that the derivatives of the function appearing in the phase integral are small in some sense  see subsection 25.7.3 for a more general discussion ; here, if k  cid:7  terms in equation  ii  above. In fact, for this particular situation, this approximation k is not needed since the ﬁrst of the equalities, equation  i , ensures that the k -dependent terms in the second equality  ii  cancel. Either way, we are left with a pair of homogeneous equations for A and B. For them to give consistent values for the ratio A B, it must be that   cid:7   cid:16  k2, then we can ignore the  √ k3  cid:16  k   √ k, i.e. k      cid:7    cid:7    cid:7  cid:21    cid:8   sin  k u  du +  [ k x  ] cos  k u  du +   cid:8   cid:8   π 4  π 4  × B√ k x   k u  du +  .  A√ k x  A√ k x   =   cid:7  cid:21   a  x  [−k x  ] cos  cid:13  cid:7  cid:21   a  x  x  k u  du +  × B√ k x    cid:8    cid:7  cid:21   cid:13  cid:7  cid:21   +  π 4  x  −a  a  π 4   cid:8   π 4  sin  This condition reduces to a  x  −a   cid:7  cid:21   cid:7  cid:21   cid:8  cid:14   cid:8  cid:14   sin  x  −a  = 0,  π 4  π 2  sin  k u  du +  k u  du +  = 0,  2  π. Since k x  > 0 in the range −a < x < a, n may take the values 0, 1, 2, . . . .  k u  du =  n + 1  −a  If V  x  has the form V  x  = V0x2s then, for the nth allowed energy level, En = V0a2s n  and   cid:21   k u  du + −a ⇒  a  k2 x  =   En − V0x2s .  2m  cid:1 2  901   APPLICATIONS OF COMPLEX VARIABLES  The result just proved gives cid:21   √  2mV0  cid:1    a2s n  − x2s 1 2 dx =  n + 1  2  π.  an  −an  ∝  n + 1  2  , implying that En ∝  n + 1  Writing x = van shows that the integral is proportional to as+1 between −1 and +1 of  1 − v2s 1 2 and does not depend upon n. Thus En ∝ a2s as+1 n harmonic oscillator, for which s = 1, the energy levels [ En ∼  n + 1 Although not asked for, we note that the above result indicates that, for a simple 2   ] are equally spaced, whilst for very large s, corresponding to a square well, the energy levels vary as n2. Both of these results agree with what is found from detailed analyses of the individual cases.  cid:2   Is, where Is is the integral n and  2  2s s+1.  n  25.7.3 Accuracy of the WKB solutions  We may also ask when we can expect the WKB solutions to the Stokes’ equation to be reasonable approximations. Although our ﬁnal form for the WKB solutions  is not exactly that used when the condition n  cid:7     cid:16  n2 was derived, it should 0 = −1, n z  = [ f z  ]1 2 = z1 2, and the criterion becomes  give the same order of magnitude restriction as a more careful analysis. For the derivation of  25.51 , k2 z 1 2 of the WKB solutions can usually be satisﬁed by making some quantity, often z,  −1 2  cid:16  z, or, in round terms, z3  cid:26  1.  For the more general equation, typiﬁed by  25.42 , the condition for the validity  −1  k  0  suﬃciently large. Alternatively, a parameter such as k0 can be made large enough that the validity criterion is satisﬁed to any pre-speciﬁed level. However, from a practical point of view, natural physical parameters cannot be varied at will, and requiring z to be large may well reduce the value of the method to virtually zero. It is normally more useful to try to obtain an improvement on a WKB solution by multiplying it by a series whose terms contain increasing inverse powers of the variable, so that the result can be applied successfully for moderate, and not just excessively large, values of the variable.  We do not have the space to discuss the properties and pitfalls of such asymptotic expansions in any detail, but exercise 25.18 will provide the reader with a model of the general procedure. A few particular points that should be noted are given as follows.   i  If the multiplier is analytic as z → ∞, then it will be represented by a series that is convergent for z greater than some radius of convergence  ii  If the multiplier is not analytic as z → ∞, as is usually the case, then the  R.  multiplier series eventually diverges and there is a z-dependent optimal number of terms that the series should contain in order to give the best accuracy.   iii  For a ﬁxed value of arg z, the asymptotic expansion of the multiplier is unique. However, the same asymptotic expansion can represent more than  902   25.7 WKB METHODS  one function and the same function may need diﬀerent expansions for diﬀerent values of arg z.  Finally in this subsection we note that, although the form of equation  25.42  may , the results obtained so appear rather restrictive, in that it contains no term in y far can be applied to an equation such as   cid:7   To make this possible, a change of either the dependent or the independent variable is made. For the former we write  d2y dz2 + P  z   dy dz   cid:8   + Q z y = 0.   25.56    cid:8   Y = 0,   cid:7  Q − 1  cid:7   4  dP dz  2  P 2 − 1  cid:8   2  y = 0.  dz2 +  ⇒ d2Y  cid:8   ⇒ d2y   cid:7   1 2  z   cid:21   cid:7  −   cid:21   z  dζ dz  = exp  P  u  du  dζ2 + Q  dz dζ  whilst for the latter we introduce a new independent variable ζ deﬁned by  Y  z  = y z  exp  P  u  du  In either case, equation  25.56  is reduced to the form of  25.42 , though it will be clear that the two sets of WKB solutions  which are, of course, only approximations  will not be the same.  25.7.4 The Stokes phenomenon  As we saw in subsection 25.7.2, the combination of WKB solutions of a diﬀerential equation required to reproduce the asymptotic form of the accurate solution y z  of the same equation, varies according to the region of the z-plane in which z lies. We now consider this behaviour, known as the Stokes phenomenon, in a little more detail.  Let y1 z  and y2 z  be the two WKB solutions of a second-order diﬀerential equation. Then any solution Y  z  of the same equation can be written asymptot- ically as  Y  z  ∼ A1y1 z  + A2y2 z ,   25.57   where, although we will be considering  abrupt  changes in them, we will continue to refer to A1 and A2 as constants, as they are within any one region. In order to produce the required change in the linear combination, as we pass over a Stokes line from one region of the z-plane to another, one of the constants must change  relative to the other  as the border between the regions is crossed.  At ﬁrst sight, this may seem impossible without causing a discernible discon- tinuity in the representation of Y  z . However, we must recall that the WKB solutions are approximations, and that, as they contain a phase integral, for certain values of arg z the phase φ z  will be purely imaginary and the factors  903   APPLICATIONS OF COMPLEX VARIABLES  exp[± iφ z  ] will be purely real. What is more, one such factor, known as the  dominant term, will be exponentially large, whilst the other  the subdominant term  will be exponentially small. A Stokes line is precisely where this happens.  We can now see how the change takes place without an observable discontinuity occurring. Suppose that y1 z  is very large and y2 z  is very small on a Stokes line. Then a ﬁnite change in A2 will have a negligible eﬀect on Y  z ; in fact, Stokes showed, for some particular cases, that the change is less than the uncertainty in y1 z  arising from the approximations made in deriving it. Since the solution with any particular asymptotic form is determined in a region bounded by two Stokes lines to within an overall multiplicative constant and the original equation is linear, the change in A2 when one of the Stokes lines is crossed must be proportional to A1, i.e. A2 changes to A2 + S A1, where S is a constant  the Stokes constant  characteristic of the particular line but independent of A1 and A2. It should be emphasised that, at a Stokes line, if the dominant term is not present in a solution, then the multiplicative constant in the subdominant term cannot change as the line is crossed.  As an example, consider the Bessel function J0 z  of zero order. It is single- valued, diﬀerentiable everywhere, and can be written as a series in powers of z2. It is therefore an integral even function of z. However, its asymptotic approximations for two regions of the z-plane, Re z > 0 and z real and negative, are given by  J0 z  ∼ 1√  −iπ 4 + e  eize   arg z  < 1  2 π,  arg z  −1 2  < 1 4 π,   cid:10   ,  J0 z  ∼ 1√  eize3iπ 4 + e  ,  arg z  = π, arg z  −1 2  = − 1 2 π.   cid:9   1√  cid:9   z  2π 1√  2π  z  −izeiπ 4  cid:10   −izeiπ 4  We note in passing that neither of these expressions is naturally single-valued, and a prescription for taking the square root has to be given. Equally, neither is an even function of z. For our present purpose the important point to note is that, for both expressions, on the line arg z = π 2 both z-dependent exponents  become real. For large z the second term in each expression is large; this is the dominant term, and its multiplying constant eiπ 4 is the same in both expressions. Contrarywise, the ﬁrst term in each expression is small, and its multiplying −iπ 4 to e3iπ 4, as arg z passes through π 2 whilst constant does change, from e increasing from 0 to π. It is straightforward to calculate the Stokes constant for this Stokes line as follows:  A2 new  − A2 old   e3iπ 4 − e  −iπ 4  A1  =  eiπ 4  S =  = eiπ 2 − e  −iπ 2 = 2i.  If we had moved  in the negative sense  from arg z = 0 to arg z = −π, the relevant Stokes line would have been arg z = −π 2. There the ﬁrst term in each expression is dominant, and it would have been the constant eiπ 4 in the second term that would have changed. The ﬁnal argument of z  −1 2 would have been +π 2.  904   25.8 APPROXIMATIONS TO INTEGRALS  Finally, we should mention that the lines in the z-plane on which the expo- nents in the WKB solutions are purely imaginary, and the two solutions have equal amplitudes, are usually called the anti-Stokes lines. For the general Bessel’s equation they are the real positive and real negative axes.  25.8 Approximations to integrals  In this section we will investigate a method of ﬁnding approximations to the values or forms of certain types of inﬁnite integrals. The class of integrals to be considered is that containing integrands that are, or can be, represented by exponential functions of the general form g z  exp[ f z  ]. The exponents f z  may be complex, and so integrals of sinusoids can be handled as well as those with more obvious exponential properties. We will be using the analyticity properties of the functions of a complex variable to move the integration path to a part of the complex plane where a general integrand can be approximated well by a standard form; the standard form is then integrated explicitly.  The particular standard form to be employed is that of a Gausssian function of a real variable, for which the integral between inﬁnite limits is well known. This form will be generated by expressing f z  as a Taylor series expansion about a point z0, at which the linear term in the expansion vanishes, i.e. where f  z  = 0. Then, apart from a constant multiplier, the exponential function will behave like exp[ 1 to take as it passes through the point, this can be made into a normal Gaussian function of a real variable and its integral may then be found.   z0  z − z0 2 ] and, by choosing an appropriate direction for the contour  2 f   cid:7  cid:7    cid:7   25.8.1 Level lines and saddle points  Before we can discuss the method outlined above in more detail, a number of observations about functions of a complex variable and, in particular, about the properties of the exponential function need to be made. For a general analytic function,  f z  = φ x, y  + iψ x, y ,   25.58   of the complex variable z = x + iy, we recall that, not only do both φ and ψ  satisfy Laplace’s equation, but ∇φ and ∇ψ are orthogonal. This means that the  lines on which one of φ and ψ is constant are exactly the lines on which the other is changing most rapidly.  Let us apply these observations to the function  h z  ≡ exp[ f z  ] = exp φ  exp iψ ,   25.59   recalling that the functions φ and ψ are themselves real. The magnitude of h z , given by exp φ , is constant on the lines of constant φ, which are known as the  905   APPLICATIONS OF COMPLEX VARIABLES  Figure 25.12 A greyscale plot with associated contours of the value of h z , where h z  = exp[i z3 + 6z2 − 15z + 8 ], in the neighbourhood of one of its  saddle points; darker shading corresponds to larger magnitudes. The plot also shows the two level lines  thick solid lines  through the saddle and part of the line of steepest descents  dashed line  passing over it. At the saddle point, the angle between the line of steepest descents and a level line is π 4.  level lines of the function. It follows that the direction in which the magnitude of h z  changes most rapidly at any point z is in a direction perpendicular to the level line passing through that point. This is therefore the line through z on which the phase of h z , namely ψ z , is constant. Lines of constant phase are therefore sometimes referred to as lines of steepest descent  or steepest ascent .  We further note that h z  can never be negative and that neither φ nor ψ  can have a ﬁnite maximum at any point at which f z  is analytic. This latter observation follows from the fact that at a maximum of, say, φ x, y , both ∂2φ ∂x2 and ∂2φ ∂y2 would have to be negative; if this were so, Laplace’s equation could not be satisﬁed, leading to a contradiction. A similar argument shows that a minimum of either φ or ψ is not possible wherever f z  is analytic. A more positive conclusion is that, since the two unmixed second partial derivatives ∂2φ ∂x2 and ∂2φ ∂y2 must have opposite signs, the only possible conclusion about a point at which ∇φ is deﬁned and equal to zero is that the point is a  saddle point of h z . An example of a saddle point is shown as a greyscale plot in ﬁgure 25.12 and, more pictorially, in ﬁgure 5.2.  906   25.8 APPROXIMATIONS TO INTEGRALS  From the observations contained in the two previous paragraphs, we deduce that a path that follows the lines of steepest descent  or ascent  can never form  a closed loop. On such a path, φ, and hence h z , must continue to decrease   increase  until the path meets a singularity of f z . It also follows that if a level line of h z  forms a closed loop in the complex plane, then the loop must enclose  a singularity of f z . This may  if φ → ∞  or may not  if φ → −∞  produce a  singularity in h z .  We now turn to the study of the behaviour of h z  at a saddle point and how this enables us to ﬁnd an approximation to the integral of h z  along a contour that can be deformed to pass through the saddle point. At a saddle point z0, at which f phase of h z  are both stationary. The Taylor expansion of f z  at such a point takes the form   z0  = 0, both ∇φ and ∇ψ are zero, and consequently the magnitude and   cid:7   f z  = f z0  + 0 +   cid:7  cid:7    z0  z − z0 2 + O z − z0 3.  f  1 2!   25.60    cid:7  cid:7    cid:7  cid:7    z0   cid:3 = 0 and write it explicitly as f  cid:7  cid:7   We assume that f  z0  = 0, then two or more the real quantities A and α. If it happens that f saddle points coalesce and the Taylor expansion must be continued until the ﬁrst non-vanishing term is reached; we will not consider this case further, though the general method of proceeding will be apparent from what follows. If we also abbreviate the  in general  complex quantity f z0  to f0, then  25.60  takes the form   z0  ≡ Aeiα, thus deﬁning  f z  = f0 + 1  2 Aeiα z − z0 2 + O z − z0 3.   25.61   To study the implications of this approximation for h z , we write z − z0 as  ρ eiθ with ρ and θ both real. Then  This shows that there are four values of θ for which h z  is independent of ρ  to  2 Aρ2 cos 2θ + α  + O ρ3  ].   25.62   second order . These therefore correspond to two crossing level lines given by  h z  =  exp f0  exp[ 1  cid:5 ± 1  cid:6  2 π − α  θ = 1 2   cid:5 ± 3  cid:6  2 π − α  .  and θ = 1 2   25.63   The two level lines cross at right angles to each other. It should be noted that the continuations of the two level lines away from the saddle are not straight in general. At the saddle they have to satisfy  25.63 , but away from it the lines  must take whatever directions are needed to make ∇φ = 0. In ﬁgure 25.12 one of the level lines  h = 1  has a continuation  y = 0  that is straight; the other does  not and bends away from its initial direction x = 1. So far as the phase of h z  is concerned, we have  arg[ h z  ] = arg f0  + 1  2 Aρ2 sin 2θ + α  + O ρ3 ,  which shows that there are four other directions  two lines crossing at right  907   APPLICATIONS OF COMPLEX VARIABLES  angles  in which the phase of h z  is independent of ρ. They make angles of π 4 with the level lines through z0 and are given by  θ = − 1 2 α,  2  ±π − α ,  θ = 1  θ = π − 1 2 α.  From our previous discussion it follows that these four directions will be the lines of steepest descent  or ascent  on moving away from the saddle point. In particular, the two directions for which the term cos 2θ + α  in  25.62  is negative  will be the directions in which h z  decreases most rapidly from its value at the  saddle point. These two directions are antiparallel, and a steepest descents path following them is a smooth locally straight line passing the saddle point. It is known as the line of steepest descents  l.s.d.  through the saddle point. Note that  ‘descents’ is plural as on this line the value of h z  decreases on both sides of the  saddle. This is the line which we will make the path of the contour integral of h z  follow. Part of a typical l.s.d. is indicated by the dashed line in ﬁgure 25.12.  25.8.2 Steepest descents method  z = z0 = 0, with f0 = f z0  = 1 and f  To help understand how an integral along the line of steepest descents can be handled in a mechanical way, it is instructive to consider the case where the  function f z  = −βz2 and h z  = exp −βz2 . The saddle point is situated at  z0  = −2β, implying that A = 2β and α = ±π + arg β, with the ± sign chosen to put α in the range 0 ≤ α < 2π. Then the l.s.d. is determined by the requirement that sin 2θ + α  = 0 whilst cos 2θ + α  is negative; together these imply that, for the l.s.d., θ = − 1 2 arg β. Since the Taylor series for f z  = −βz2 terminates after three terms, expansion  2 arg β or θ = π − 1   cid:7  cid:7    25.61  for this particular function is not an approximation to h z , but is exact. Consequently, a contour integral starting and endingin regions of the complex plane where the function tends to zero and following the l.s.d. through the saddle point at z = 0 will not only have a straight-line path, but will yield an exact − 1 2 arg β will reduce the integral to that of a Gaussian function: result. Setting z = te − 1 2 arg β   cid:21  ∞  − 1 2 arg β     −βt2  e  −∞ e  dt = e  πβ .  The saddle-point method for a more general function aims to simulate this approach by deforming the integration contour C and forcing it to pass through a saddle point z = z0, where, whatever the function, the leading z-dependent term  in the exponent will be a quadratic function of z − z0, thus turning the integrand  into one that can be approximated by a Gaussian.  The path well away from the saddle point may be changed in any convenient way so long as it remains within the relevant sectors, as determined by the end- points of C. By a ‘sector’ we mean a region of the complex plane, any part of which can be reached from any other part of the same region without crossing  908   25.8 APPROXIMATIONS TO INTEGRALS  any of the continuations to inﬁnity of the level lines that pass through the saddle. In practical applications the start- and end-points of the path are nearly always  at singularities of f z  with Re f z  → −∞ and h z  → 0.  We now set out the complete procedure for the simplest form of integral evaluation that uses a method of steepest descents. Extensions, such as including higher terms in the Taylor expansion or having to pass through more than one saddle point in order to have appropriate termination points for the contour, can be incorporated, but the resulting calculations tend to be long and complicated, and we do not have space to pursue them in a book such as this one.  As our general integrand we take a function of the form g z h z , where, as before, h z  = exp[ f z  ]. The function g z  should neither vary rapidly nor have zeros or singularities close to any saddle point used to evaluate the integral. Rapidly varying factors should be incorporated in the exponent, usually in the form of a logarithm. Provided g z  satisﬁes these criteria, it is suﬃcient to treat it as a constant multiplier when integrating, assigning to it its value at the saddle point, g z0 .  Incorporating this and retaining only the ﬁrst two non-vanishing terms in  equation  25.61  gives the integrand as  g z0  exp f0  exp[ 1  2 Aeiα  z − z0 2].   25.64   From the way in which it was deﬁned, it follows that on the l.s.d. the imaginary part of f z  is constant  = Im f0  and that the ﬁnal exponent in  25.64  is either  zero  at z0  or negative. We can therefore write it as −s2, where s is real. Further, since exp[ f z  ] → 0 at the start- and end-points of the contour, we must have that s runs from −∞ to +∞, the sense of s being chosen so that it is negative  approaching the saddle and positive when leaving it.  Making this change of variable,     2 Aeiα z − z0 2 = −s2, with dz = ±  1  2 i π − α  ] ds,  exp[ 1  2 A   25.65   allows us to express the contribution to the integral from the neighbourhood of the saddle point as    ±g z0  exp f0    cid:21  ∞ −∞ exp −s2  ds.  2 i π − α  ]  exp[ 1  2 A  The simple saddle-point approximation assumes that this is the only contribution, and gives as the value of the contour integral  g z  exp[ f z  ] dz = ±  2 i π − α  ],  2π A   cid:1  ∞ g z0  exp f0  exp[ 1 −∞ exp −s2  ds =  √   25.66   π. The overall ±  where we have used the standard result that   cid:21   C     909   APPLICATIONS OF COMPLEX VARIABLES  contour passes through the saddle point. If − 1  sign is determined by the direction θ in the complex plane in which the distorted 2 π, then the positive sign is taken; if not, then the negative sign is appropriate. In broad terms, if the integration path through the saddle is in the direction of an increasing real part for z, then the overall sign is positive.  2 π < θ ≤ 1  Formula  25.66  is the main result from a steepest descents approach to evalu- ating a contour integral of the type considered, in the sense that it is the leading term in any more reﬁned calculation of the same integral. As can be seen, it is as an ‘omnibus’ formula, the various components of which can be found by considering a number of separate, less-complicated, calculations.  Before presenting a worked example that generates a substantial result, useful in another connection, it is instructive to consider an integral that can be simply and exactly evaluated by other means and then apply the saddle-point result to it. Of course, the steepest descents method will appear heavy-handed, but our purpose is to show it in action and to try to see why it works.  Consider the real integral   cid:21  ∞ −∞ exp 10t − t2  dt =  I =  I =   cid:21  ∞ −∞ exp 10t − t2  dt.   cid:21  ∞ −∞ exp 25 − s2  ds = e25  This can be evaluated directly by making the substitution s = t − 5 as follows:   cid:21  ∞ −∞ exp −s2  ds =  √  πe25.   cid:7   The saddle-point approach to the same problem is to consider the integral as a contour integral in the complex plane, but one that lies along the real axis. The saddle points of the integrand occur where f single saddle point at t = t0 = 5. This is on the real axis, and no distortion of the   t  = 10 − 2t = 0; there is thus a contour is necessary. The value f0 of the exponent is f 5  = 50 − 25 = 25, whilst  5  = −2. Thus, A = 2 and α = π.  its second derivative at the saddle point is f The contour clearly passes through the saddle point in the direction θ = 0, i.e. in the positive sense on the real axis, and so the overall sign must be +. Since g t0  is formally unity, we have all the ingredients needed for substitution in formula  25.66 , which reads   cid:7  cid:7   I = +  1 exp 25  exp[ 1  2 i π − π  ] =  √  πe25.     2π 2  As it happens, this is exactly the same result as that obtained by accurate calculation. This would not normally be the case, but here it is, because of the  quadratic nature of 10t−t2; all of its derivatives beyond the second are identically  zero and no approximation of the exponent is involved.  Given the very large value of the integrand at the saddle point itself, the reader may wonder whether there really is a saddle there. However, evaluating the integrand at points lying on a line through the saddle point perpendicular  910   25.8 APPROXIMATIONS TO INTEGRALS  to the l.s.d., i.e. on the imaginary t-axis, provides some reassurance. Whether µ is positive or negative,  h 5 + iµ  = exp 50 + 10iµ − 25 − 10iµ + µ2  = exp 25 + µ2 .  This is greater than h 5  for all µ and increases as µ increases, showing that  the integration path really does lie at a minimum of h t  for a traversal in this direction.  We now give a fully worked solution to a problem that could not be easily  tackled by elementary means.  cid:1  Apply the saddle-point method to the function deﬁned by   cid:21  ∞  1 π  0  F x  =  cos  1  3 s3 + xs  ds  to show that its form for large positive real x is one that tends asymptotically to zero, hence enabling F x  to be identiﬁed with the Airy function, Ai x .  We ﬁrst express the integral as an exponential function and then make the change of variable s = x1 2t to bring it into the canonical form  g t  exp[ f t  ] dt as follows:   cid:1    cid:21  ∞  cid:21  ∞  cid:21  ∞  0  1 π 1 2π 1 2π  =  =  F x  =  cos  1  3 s3 + xs  ds  −∞ exp[ i  1  3 s3 + xs  ] ds  −∞ x1 2 exp[ ix3 2  1  3 t3 + t  ] dt.  We now seek to ﬁnd an approximate expression for this contour integral by deforming its path along the real t-axis into one passing over a saddle point of the integrand. Considered as a function of t, the multiplying factor x1 2 2π is a constant, and any eﬀects due to the proximity of its zeros and singularities to any saddle point do not arise.  The saddle points are situated where   cid:7    t  = ix3 2 t2 + 1  ⇒ t = ±i.  0 = f  For reasons discussed later, we choose to use the saddle point at t = t0 = i . At this point,  f i  = ix3 2 − 1  3 i + i  = − 2  3 x3 2 and Aeiα ≡ f   cid:7  cid:7    i  = ix3 2 2i  = −2x3 2,  and so A = 2x3 2 and α = π.  Now, expanding f t  around t = i by setting t = i + ρ eiθ, we have  f t  = f i  + 0 +   cid:7  cid:7    i  t − i 2 + O[  t − i 3 ]  f  1 2! 1 2  = − 2  3  x3 2 +  2x3 2eiπρ2e2iθ + O ρ3 .  For the l.s.d. contour that crosses the saddle point we need the second term in this last  line to decrease as ρ increases. This happens if π + 2θ = ±π, i.e. if θ = 0 or θ = −π  or  +π ; thus, the l.s.d. through the saddle is orientated parallel to the real t-axis. Given the initial contour direction, the deformed contour should approach the saddle point from the  direction θ = −π and leave it along the line θ = 0. Since −π 2 < 0 ≤ π 2, the overall sign  of the ‘omnibus’ approximation formula is determined as positive.  911   APPLICATIONS OF COMPLEX VARIABLES  Finally, putting the various values into the formula yields  F x  ∼ +  g i  exp[ f i  ] exp[ 1   cid:7   cid:7    cid:8   2π A  1 2   cid:8   1 2  2π = + 2x3 2 √ 1 πx1 4  =  2   cid:7   x1 2 2π − 2 3  exp  x3 2  .   cid:8  2 i π − α  ]   cid:7   cid:8  − 2 3  exp  x3 2  exp[ 1  2 i π − π  ]  This is the leading term in the asymptotic expansion of F x , which, as shown in equation  25.39 , is a particular contour integral solution of Stokes’ equation. The fact that it tends  to zero in a monotonic way as x → +∞ allows it to be identiﬁed with the Airy function, We may ask why the saddle point at t = −i was not used. The answer to this is as  Ai x .  follows. Of course, any path that starts and ends in the right sectors will suﬃce, but if another saddle point exists close to the one used, then the Taylor expansion actually employed is likely to be less eﬀective than if there were no other saddle points or if there were only distant ones.  An investigation of the same form as that used at t = +i shows that the saddle at t = −i is higher by a factor of exp  4 3 x3 2  and that its l.s.d. is orientated parallel to the imaginary imaginary t, over the saddle at t = −i, and then, when it reached the col at t = +i, bend t-axis. Thus a path that went through it would need to go via a region of largish negative the t = −i saddle would be incomplete and roughly half of that from the t = +i saddle  sharply and follow part of the same l.s.d. as considered earlier. Thus the contribution from  would still have to be included. The more serious error would come from the ﬁrst of these, as, clearly, the part of the path that lies in the plane Re t = 0 is not symmetric and is far from Guassian-like on the side nearer the origin. The Gaussian-path approximation used will therefore not be a good one, and, what is more, the resulting error will be magniﬁed by a factor exp  4 3 x3 2  compared with the best estimate. So, both on the grounds of simplicity and because the eﬀect of the other  neglected  saddle point is likely to be less severe, we choose to use the one at t = +i.  cid:2   25.8.3 Stationary phase method  In the previous subsection we showed how to use the saddle points of an exponential function of a complex variable to evaluate approximately a contour integral of that function. This was done by following the lines of steepest descent that passed through the saddle point; these are lines on which the phase of the exponential is constant but its amplitude is varying at the maximum possible rate for that function. We now introduce an alternative method, one that entirely reverses the roles of amplitude and phase. To see how such an alternative approach might work, it is useful to study how the integral of an exponential function of a complex variablecan be represented as the sum of inﬁnitesimal vectors in the complex plane.  We start by studying the familiar integral   25.67    cid:21  ∞ −∞ exp −z2  dz,  I0 =  912   25.8 APPROXIMATIONS TO INTEGRALS  √  π when z is real. This choice of demon- which we already know has the value stration model is not accidental, but is motivated by the fact that, as we have already shown, in the neighbourhood of a saddle point all exponential integrands can be approximated by a Gaussian function of this form.  The same integral can also be thought of as an integral in the complex plane, in which the integration contour happens to be along the real axis. Since the integrand is analytic, the contour could be distorted into any other that had the  As a particular possibility, we consider an arc of a circle of radius R centred  and that the larger R is, the more precipitous is the ‘drop or rise’ in its value on  same end-points, z = −∞ and z = +∞, both on the real axis. on z = 0. It is easily shown that cos 2θ ≥ 1 + 4θ π for −π 4 < θ ≤ 0, where θ is measured from the positive real z-axis and −π < θ ≤ π. It follows from writing z = R eiθ on the arc that, if the arc is conﬁned to the region −π 4 < θ ≤ 0  actually, θ < π 4 is suﬃcient , then the integral of exp −z2  tends to zero as R → ∞ anywhere on the arc. A similar result holds for an arc conﬁned to the region θ − π < π 4. We also note for future use that, for π 4 < θ < 3π 4 or −π 4 > θ > −3π 4, the integrand exp −z2  grows without limit as R → ∞, crossing the four radial lines θ = ±π 4 and θ = ±3π 4. to θ = π − α joined to a straight line, θ = −α, which passes through z = 0 and continues to inﬁnity, where it in turn joins an arc at inﬁnity running from θ = −α and so the integral of exp −z2  along it must also have the value that the integral of exp −z2  along the inﬁnite line θ = −α is α arbitrarily close to π 4, we may substitute z = s exp −iπ 4  into  25.67  and obtain √  to θ = 0. This contour has the same start- and end-points as that used in I0, π. As the contributions to the integral from the arcs vanish, provided α < π 4, it follows π. If we now take  Now consider a contour that consists of an arc at inﬁnity running from θ = π  √  √  π =   cid:21  ∞  cid:21  ∞ −∞ exp −z2  dz = exp −iπ 4  √ 2π exp −iπ 4   =   cid:13  cid:21  ∞  −∞ exp is2  ds cos  1  0   cid:21   x  0   cid:14    25.68   2 πu2  du + i  sin  1  2 πu2  du  .   25.69    cid:21  ∞  0   cid:24    cid:21   x  0  The ﬁnal line was obtained by making a scale change s = the two integrals to be identiﬁed with the Fresnel integrals C x  and S  x ,  π 2 u. This enables  C x  =  cos  1  2 πu2  du and S  x  =  sin  1  2 πu2  du,  mentioned on page 645. Equation  25.69  can be rewritten as  √ π√  1 + i  2  √ 2π [ C ∞  + iS  ∞  ],  =  913   APPLICATIONS OF COMPLEX VARIABLES  We are now in a position to examine these two equivalent ways of evaluating I0 in terms of sums of inﬁnitesmal vectors in the complex plane. When the integral  from which it follows that C ∞  = S  ∞  = 1  cid:1  ∞ −∞ exp −z2  dz is evaluated as a real integral, or a complex one along the real z-axis, each element dz generates a vector of length exp −z2  dz in an Argand  2 . Clearly, C −∞  = S  −∞  = − 1 2 .  diagram, usually called the amplitude–phase diagram for the integral. For this integration, whilst all vector contributions lie along the real axis, they do diﬀer in  magnitude, starting vanishingly small, growing to a maximum length of 1 × dz,  and then reducing until they are again vanishingly small. At any stage, their vector sum  in this case, the same as their algebraic sum  is a measure of the indeﬁnite integral   cid:21  −∞ exp −z2  dz.  x  I x  =   25.70   The total length of the vector sum when x → ∞ is, of course,  √ π, and it should not be overlooked that the sum is a vector parallel to  actually coinciding with  the real axis in the amplitude–phase diagram. Formally this indicates that the integral is real. This ‘ordinary’ view of evaluating the integral generates the same amplitude–phase diagram as does the method of steepest descents. This is because for this particular integrand the l.s.d. never leaves the real axis.  Now consider the same integral evaluated using the form of equation  25.69 . Here, each contribution, as the integration variable goes from u to u + du, is of the form  g u  du = cos  1  2 πu2  du + i sin  1  2 πu2  du.  As inﬁnitesimal vectors in the amplitude–phase diagram, all g u  du have the same magnitude du, but their directions change continuously. Near u = 0, where u2 √ is small, the change is slow and each vector element is approximately equal to 2π exp −iπ 4  du; these contributions are all in phase and add up to a signiﬁcant vector contribution in the direction θ = −π 4. This is illustrated by the central  part of the curve in part  b  of ﬁgure 25.13, in which the amplitude–phase diagram for the ‘ordinary’ integration, discussed above, is drawn as part  a .  Part  b  of the ﬁgure also shows that the vector representing the indeﬁnite integral  25.70  initially  s large and negative  spirals out, in a clockwise sense, from around the point 0 + i0 in the amplitude–phase diagram and ultimately  s π + i0. large and positive  spirals in, in an anticlockwise direction, to the point The total curve is called a Cornu spiral. In physical applications, such as the diﬀraction of light at a straight edge, the relevant limits of integration are  typically −∞ and some ﬁnite value x. Then, as can be seen, the resulting vector  √  sum is complex in general, with its magnitude  the distance from 0 + i0 to the point on the spiral corresponding to z = x  growing steadily for x < 0 but showing oscillations when x > 0.  914   25.8 APPROXIMATIONS TO INTEGRALS   a    c   √  π  π 4   b   β  the steepest descents method.  b  Using the level line z = u exp − 1  Figure 25.13 Amplitude–phase diagrams for the integral using diﬀerent contours in the complex z-plane.  a  Using the real axis, as in 4 iπ  that passes through the saddle point, as in the stationary phase method.  c  Using a path that makes a positive angle β  < π 4  with the z-axis.   cid:1  ∞ −∞ exp −z2  dz  The ﬁnal curve, 25.13 c , shows the amplitude-phase diagram corresponding to an integration path that is along a line making a positive angle β  0 < β < π 4  with the real z-axis. In this case, the constituent inﬁnitesimal vectors vary in both length and direction. Note that the curve passes through its centre point with the positive gradient tan β and that the directions of the spirals around the winding points are reversed as compared with case  b .  It is important to recognise that, although the three paths illustrated  and the inﬁnity of other similar paths not illustrated  each produce a diﬀerent phase– amplitude diagram, the vectors joining the initial and ﬁnal points in the diagrams are all the same. For this particular integrand they are all  i  parallel to the positive real axis, showing that the integral is real and giving its sign, and  ii  of length  π, giving its magnitude.  √  What is apparent from ﬁgure 25.13 b , is that, because of the rapidly varying phase at either end of the spiral, the contributions from the inﬁnitesimal vectors in those regions largely cancel each other. It is only in the central part of the spiral where the individual contributions are all nearly in phase that a substantial net contribution arises. If, on this part of the contour, where the phase is virtually  915   APPLICATIONS OF COMPLEX VARIABLES  stationary, the magnitude of any factor, g z , multiplying the exponential function,  exp[ f z  ] ∼ exp[ Aeiα z− z0 2 ], is at least comparable to its magnitude elsewhere,  then this result can be used to obtain an approximation to the value of the integral of h z  = g z  exp[ f z  ]. This is the basis of the method of stationary phase.  Returning to the behaviour of a function exp[ f z  ] at one of its saddle points, we can now see how the considerations of the previous paragraphs can be applied there. We already know, from equation  25.62  and the discussion immediately following it, that in the equation  h z  ≈ g z0  exp f0  exp{ 1  2 Aρ2[ cos 2θ + α  + i sin 2θ + α  ]}   25.71   the second exponent is purely imaginary on a level line, and equal to zero at  the saddle point itself. What is more, since ∇ψ = 0 at the saddle, the phase is  stationary there; on one level line it is a maximum and on the other it is a minimum. As there are two level lines through a saddle point, a path on which the amplitude of the integrand is constant could go straight on at the saddle point or it could turn through a right angle. For the moment we assume that it runs continuously through the saddle.  On the level line for which the phase at the saddle point is a minimum, we can  write the phase of h z  as approximately  where v is real, iv2 = 1  arg g z0  + Im f0 + v2,  2 Aeiα z − z0 2 and, as previously, Aeiα = f   cid:7  cid:7    z0 . Then     leading to an approximation to the integral of   cid:21   eiπ 4 dv = ±  cid:21  ∞  A 2  eiα 2 dz,       A 2  A 2  h z  dz ≈ ± g z0  exp f0  −∞ exp iv2  √ = ± g z0  exp f0  = ±  g z0  exp f0  exp[ 1     π exp iπ 4   2π A  2 i π − α  ].  exp[ i  1  2 α  ] dv  4 π − 1 4 π − 1  exp[ i  1  2 α  ]   25.72    25.73   Result  25.68  was used to obtain the second line above. The ± ambiguity is again resolved by the direction θ of the contour; it is positive if −3π 4 < θ ≤ π 4;  otherwise, it is negative.  What we have ignored in obtaining result  25.73  is that we have integrated along a level line and that therefore the integrand has the same magnitude far from the saddle as it has at the saddle itself. This could be dismissed by referring to the fact that contributions to the integral from the ends of the Cornu spiral  916   25.8 APPROXIMATIONS TO INTEGRALS  are self-cancelling, as discussed previously. However, the ends of the contour must be in regions where the integrand is vanishingly small, and so at each end of the level line we need to add a further section of path that makes the contour terminate correctly.  Fortunately, this can be done without adding to the value of the integral. This is because, as noted in the second paragraph following equation  25.67 , far from the saddle the level line will be at a ﬁnite height up a ‘precipitous cliﬀ’ that separates the region where the integrand grows without limit from the one where it tends to zero. To move down the cliﬀ-face into the zero-level valley requires an ever smaller step the further we move away from the saddle; as the integrand is ﬁnite, the contribution to the integral is vanishingly small. In ﬁgure 25.12, this additional piece of path length might, for example, correspond to the inﬁnitesimal move from a point on the large positive x-axis  where h z  has value 1  to a point  just above it  where h z  ≈ 0 .  Now that formula  25.73  has been justiﬁed, we may note that it is exactly the same as that for the method of steepest descents, equation  25.66 . A similar calculation using the level line on which the phase is a maximum also reproduces the steepest-descents formula. It would appear that ‘all roads lead to Rome’. However, as we explain later, some roads are more diﬃcult than others. Where a problem involves using more than one saddle point, if the steepest-descents approach is tractable, it will usually be the more straight forward to apply.  Typical amplitude-phase diagrams for an integration along a level line that goes straight through the saddle are shown in parts  a  and  b  of ﬁgure 25.14. The value of the integral is given, in both magnitude and phase, by the vector v joining the initial to the ﬁnal winding points and, of course, is the same in both cases. Part  a  corresponds to the case of the phase being a minimum at the  saddle; the vector path crosses v at an angle of −π 4. When a path on which the  phase at the saddle is a maximum is used, the Cornu spiral is as in part  b  of the ﬁgure; then the vector path crosses v at an angle of +π 4. As can be seen, the two spirals are mirror images of each other.  Clearly a straight-through level line path will start and end in diﬀerent zero- level valleys. For one that turns through a right angle at the saddle point, the  end-point could be in a diﬀerent valley  for a function such as exp −z2 , there is  only one other  or in the same one. In the latter case the integral will give a zero value, unless a singularity of h z  happens to have been enclosed by the contour. Parts  c  and  d  of ﬁgure 25.14 illustrate the phase–amplitude diagrams for these two cases. In  c  the path turns through a right angle  +π 2, as it happens  at the saddle point, but ﬁnishes up in a diﬀerent valley from that in which it started. In  d  it also turns through a right angle but returns to the same valley, albeit close to the other precipice from that near its starting point. This makes no diﬀerence and the result is zero, the two half spirals in the diagram producing resultants that cancel.  917   APPLICATIONS OF COMPLEX VARIABLES  v   a   v   c   v   b    d   Figure 25.14 Amplitude–phase diagrams for stationary phase integration.  a  Using a straight-through path on which the phase is a minimum.  b  Using a straight-through path on which the phase is a maximum.  c  Using a level line that turns through +π 2 at the saddle point but starts and ﬁnishes in diﬀerent valleys.  d  Using a level line that turns through a right angle but ﬁnishes in the same valley as it started. In cases  a ,  b  and  c  the integral value is represented by v  see text . In case  d  the integral has value zero.  We do not have the space to consider cases with two or more saddle points, but even more care is needed with the stationary phase approach than when using the steepest-descents method. At a saddle point there is only one l.s.d. but there are two level lines. If more than one saddle point is required to reach the appropriate end-point of an integration, or an intermediate zero-level valley has to be used, then care is needed in linking the corresponding level lines in such a way that the links do not make a signiﬁcant, but unknown, contribution to the integral. Yet more complications can arise if a level line through one saddle point crosses a line of steepest ascent through a second saddle.  We conclude this section with a worked example that has direct links to the  two preceding sections of this chapter.  918   25.8 APPROXIMATIONS TO INTEGRALS   cid:21  ∞  1 π  0  F x  ∼  1√ π −x 1 4  cid:21  ∞   cid:1 In the worked example in subsection 25.8.2 the function  F x  =  cos  1  3 s3 + xs  ds  was shown to have the properties associated with the Airy function, Ai x , when x > 0. Use  the stationary phase method to show that, for x < 0 and −x suﬃciently large,   ∗    cid:14    cid:13   sin   −x 3 2 +  2 3  π 4  ,  in accordance with equation  25.53  for Ai z .  Since the cosine function is an even function and its argument in  ∗  is purely real, we  may consider F x  as the real part of  G x  =  1 2π  −∞ exp[ i  1  3 s3 + xs  ].  3 s3 + xs . The latter has f  This is of the standard form for a saddle-point approach with g s  = 1 2π and f s  = i  1 at s = + 2   s  = 0 when s2 = −x. Since x < 0 there are two saddle points √−x and s = −√−x. These are both on the real axis separated by a distance √−x. If −x is suﬃciently large, the Gaussian-like stationary phase integrals can be treated   cid:7   separately and their contributions simply added. In terms of a phase–amplitude diagram, the Cornu spiral from the ﬁrst saddle will have eﬀectively reached its ﬁnal winding point before the spiral from the second saddle begins. The second spiral therefore takes the ﬁnal point of the ﬁrst as its starting point; the vector representing its net contribution need not be in the same direction as that arising from the ﬁrst spiral, and in general it will not be.  √−x the form of f s  is, in the usual notation,  Near the saddle at s = +  f s  = f0 + 1 = − 2i = − 2i  3  2 Aeiα s − s0 2  −x 3 2 +  −x 3 2 +  1 2  2  √−x eiπ 2  ρ eiθ 2 √−x eiπ 2 ρ2 cos 2θ + i sin 2θ .  3  For the exponent to be purely imaginary requires sin 2θ = 0, implying that the level lines  which diﬀers only in that the sign of f0 is reversed and α = 3π 2 rather than π 2; exp iα  is imaginary in both cases. Thus the obvious path is one that approaches both saddles  are given by θ = 0, π 2, π or 3π 2. The same conclusions hold at the saddle at s = −√−x, from the direction θ = π and leaves them in the direction θ = 0. As −3π 4 < 0 < π 4, the ± choice is resolved as positive at both saddles.  cid:14   Next we calculate the approximate values of the integrals from equation  25.73 . At  √−x it is   cid:25   s = +   cid:13    cid:13    cid:10  cid:14    cid:9   2  +  2π  exp  1 2π  π − π  cid:8  cid:14   −x 3 2 − π The corresponding contribution from the saddle at s = −√−x is  cid:7   cid:13  π − 3π  √−x √ π −x 1 4  cid:13    −x 3 2 − 2i  cid:13   cid:7  3  cid:14    −x 3 2   cid:25   −i  = +  exp  exp  exp  exp  2 3  i 2  +  2  1  2  4  .  2π  √−x  2  1 2π  2i 3  i 2   cid:8  cid:14   ,  2  919   APPLICATIONS OF COMPLEX VARIABLES  V0eiωt 4  L  C  D  C  A  R  B  IR  E  L   cid:8  cid:14   Figure 25.15 The inductor–capacitor–resistor network for exercise 25.1.  Adding the two contributions and taking the real part of the sum, though this is not necessary here because the sum is real anyway, we obtain  which can also be simpliﬁed, and gives   cid:13    cid:7   1  √ π −x 1 4  2  +  exp  i  F x  =  cos  2  √ π −x 1 4  2 1√ π −x 1 4  .  4  2 3   −x 3 2 − π  cid:7   cid:7    cid:8   −x 3 2 − π  cid:8   2 3  4   −x 3 2 + in agreement with the asymptotic form given in  25.53 .  cid:2   sin  2 3  =  π 4  ,  25.9 Exercises  25.1  In the method of complex impedances for a.c. circuits, an inductance L is represented by a complex impedance ZL = iωL and a capacitance C by ZC = 1  iωC . Kirchhoﬀ’s circuit laws,   cid:4    cid:4   Ii = 0 at a node and  ZiIi =  Vj around any closed loop,   cid:4   i  i  j  are then applied as if the circuit were a d.c. one.  Apply this method to the a.c. bridge connected as in ﬁgure 25.15 to show that if the resistance R is chosen as R =  L C 1 2 then the amplitude of the current, IR, through it is independent of the angular frequency ω of the applied a.c. voltage V0 eiωt.  Determine how the phase of IR, relative to that of the voltage source, varies  with the angular frequency ω. A long straight fence made of conducting wire mesh separates two ﬁelds and stands one metre high. Sometimes, on ﬁne days, there is a vertical electric ﬁeld over ﬂat open countryside. Well away from the fence the strength of the ﬁeld is  E0. By considering the eﬀect of the transformation w =  1− z2 1 2 on the real and  25.2  920   25.9 EXERCISES  25.3  25.4  25.6  imaginary z-axes, ﬁnd the strengths of the ﬁeld  a  at a point one metre directly above the fence,  b  at ground level one metre to the side of the fence, and  c  at a point that is level with the top of the fence but one metre to the side of it. What is the direction of the ﬁeld in case  c ? For the function   cid:7    cid:8   f z  = ln  z + c  z − c  ,  where c is real, show that the real part u of f is constant on a circle of radius c cosech u centred on the point z = c coth u. Use this result to show that the electrical capacitance per unit length of two parallel cylinders of radii a, placed with their axes 2d apart, is proportional to [cosh Find a complex potential in the z-plane appropriate to a physical situation in which the half-plane x > 0, y = 0 has zero potential and the half-plane x < 0, y = 0 has potential V . −1  2, with a real and positive, ﬁnd the electrostatic potential associated with the half-plane r > a, s = 0 and the  By making the transformation w = a z + z  half-plane r < −a, s = 0 at potentials 0 and V , respectively.  −1 d a ]  −1.  25.5  By considering in turn the transformations  z = 1  2 c w + w  −1   and w = exp ζ,  where z = x + iy, w = r exp iθ, ζ = ξ + iη and c is a real positive constant, show  that z = c cosh ζ maps the strip ξ ≥ 0, 0 ≤ η ≤ 2π, onto the whole z-plane. Which  curves in the z-plane correspond to the lines ξ = constant and η = constant? Identify those corresponding to ξ = 0, η = 0 and η = 2π.  The electric potential φ of a charged conducting strip −c ≤ x ≤ c, y = 0,  satisﬁes  φ ∼ −k ln x2 + y2 1 2 for large values of  x2 + y2 1 2,  with φ constant on the strip. Show that φ = Re[−k cosh −1 z c ] and that the magnitude of the electric ﬁeld near the strip is k c2 − x2  −1 2. For the equation 8z3 + z + 1 = 0:  a  show that all three roots lie between the circles z = 3 8 and z = 5 8;   b  ﬁnd the approximate location of the real root, and hence deduce that the complex ones lie in the ﬁrst and fourth quadrants and have moduli greater than 0.5.  25.7  Use contour integration to answer the following questions about the complex zeros of a polynomial equation.   a  Prove that z8 + 3z3 + 7z + 5 has two zeros in the ﬁrst quadrant.  b  Find in which quadrants the zeros of 2z3 + 7z2 + 10z + 6 lie. Try to locate  them.  25.8  The following is a method of determining the number of zeros of an nth-degree  polynomial f z  inside the contour C given by z = R:  a  put z = R 1 + it   1 − it , with t = tan θ 2 , in the range −∞ ≤ t ≤ ∞;   b  obtain f z  as  A t  + iB t    1 + it n  1 − it n ;  1 + it n −1 t; −1 B A  + n tan −1 B A ] + nπ; −1 B A  at t = ±∞ and ﬁnding −1 B A ] by evaluating tan  it follows that arg f z  = tan   c   d  and that ∆C[arg f z ] = ∆C [tan  e  determine ∆C [tan  the discontinuities in B A by inspection or using a sketch graph.  921   APPLICATIONS OF COMPLEX VARIABLES  Then, by the principle of the argument, the number of zeros inside C is given by the integer  2π   It can be shown that the zeros of z4 + z + 1 lie one in each quadrant. Use the above method to show that the zeros in the second and third quadrants have  z < 1.  25.9  Prove that  −1∆C [arg f z ]. ∞ cid:4   1 4 n + 1 n2 + 3  8  −∞  = 4π.  Carry out the summation numerically, say between −4 and 4, and note how  25.10  much of the sum comes from values near the poles of the contour integration. This exercise illustrates a method of summing some inﬁnite series.   a  Determine the residues at all the poles of the function   b  By evaluating, in two diﬀerent ways, the integral I of f z  along the straight  f z  =  π cot πz a2 + z2 ,  where a is a positive real constant.  line joining −∞ − ia 2 and +∞ − ia 2, show that − 1 2a2 .  π coth πa  2a  =  1  n=1   c  Deduce the value of  ∞ cid:4   cid:11 ∞ By considering the integral of cid:7  ∞ cid:4   1 n  αz  sin αz  a2 + n2 −2.   cid:8   2  π  ,  sin πz  α <  π 2  ,  around a circle of large radius, prove that   −1 m−1 sin2 mα  =   mα 2  1 2  .  m=1  25.11  25.12  Use the Bromwich inversion, and contours similar to that shown in ﬁgure 25.7 a , to ﬁnd the functions of which the following are the Laplace transforms:   a  s s2 + b2   b  n! s − a   c  a s2 − a2   −1; − n+1 , with n a positive integer and s > a; −1, with s > a.  25.13  Compare your answers with those given in a table of standard Laplace transforms. Find the function f t  whose Laplace transform is  25.14  A function f t  has the Laplace transform  the complex logarithm being deﬁned by a ﬁnite branch cut running along the  imaginary axis from −i to i.   a  Convince yourself that, for t > 0, f t  can be expressed as a closed contour  integral that encloses only the branch cut.  ¯f s  =  e  −s − 1 + s  cid:8   cid:7   s2  .  F s  =  ln  1 2i  s + i  s − i  ,  922   25.9 EXERCISES   b  Calculate F s  on either side of the branch cut, evaluate the integral and  hence determine f t .   c  Conﬁrm that the derivative with respect to s of the Laplace transform  integral of your answer is the same as that given by dF ds.  25.15  25.16  Use the contour in ﬁgure 25.7 c  to show that the function with Laplace transform −1 2 is  πx  s [ For an integrand of the form r  −1 2 exp −rx  change variable to t = r1 2. ]  −1 2.  Transverse vibrations of angular frequency ω on a string stretched with constant tension T are described by u x, t  = y x  e  −iωt, where  d2y dx2  +  ω2 m x   T  y x  = 0.  Here, m x  = m0f x  is the mass per unit length of the string and, in the general case, is a function of x. Find the ﬁrst-order W.K.B. solution for y x .  Due to imperfections in its manufacturing process, a particular string has a small periodic variation in its linear density of the form m x  = m0[ 1 +   cid:4  sin 2πx L  ], where  cid:4   cid:16  1. A progressive wave  i.e. one in which no energy is ﬂuctuates by ± 1  lost  travels in the positive x-direction along the string. Show that its amplitude 4  cid:4  of its value A0 at x = 0 and that, to ﬁrst order in  cid:4 , the phase  of the wave is     25.17  ahead of what it would be if the string were uniform, with m x  = m0. The equation   cid:4  ω L  2π   cid:7   m0 T  sin2 πx L   cid:8   d2y dz2  +  ν +  z2  y = 0,  − 1 4  1 2  sometimes called the Weber–Hermite equation, has solutions known as parabolic cylinder functions. Find, to within  possibly complex  multiplicative constants,  the two W.K.B. solutions of this equation that are valid for large z. In each case,  determine the leading term and show that the multiplicative correction factor is of the form 1 + O ν2 z2 .  Identify the Stokes and anti-Stokes lines for the equation. On which of the Stokes lines is the W.K.B. solution that tends to zero for z large, real and negative, the dominant solution? A W.K.B. solution of Bessel’s equation of order zero,  25.18  d2y dz2  +  1 z  dy dz  + y = 0,   ∗   valid for large z and −π 2 < arg z < 3π 2, is y z  = Az  −1 2eiz. Obtain an improvement on this by ﬁnding a multiplier of y z  in the form of an asymptotic expansion in inverse powers of z as follows.   cid:11 ∞  a  Substitute for y z  in  ∗  and show that the equation is satisﬁed to O z  −5 2 .  b  Now replace the constant A by A z  and ﬁnd the equation that must be −n, satisﬁed by A z . Look for a solution of the form A z  = zσ where a0 = 1. Show that σ = 0 is the only acceptable solution to the indicial equation and obtain a recurrence relation for the an. −1 2eiz is the asymptotic expansion of the Hankel function H  1  0  z . Show that it is a divergent expansion for all values of z and estimate, in terms of z, the value of N such that  −n−1 2eiz gives the best estimate of H  1    c  To within a  complex  constant, the expression y z  = A z z   cid:11   n=0 anz  0  z .  N n=0 anz  923   25.19  The function h z  of the complex variable z is deﬁned by the integral  APPLICATIONS OF COMPLEX VARIABLES   cid:21   h z  =  i∞  −i∞ exp t2 − 2zt  dt.   a  Make a change of integration variable, t = iu, and evaluate h z  using a  standard integral. Is your answer valid for all ﬁnite z?   b  Evaluate the integral using the method of steepest descents, considering in particular the cases  i  z is real and positive,  ii  z is real and negative and  iii  z is purely imaginary and equal to iβ, where β is real. In each case sketch the corresponding contour in the complex t-plane.   c  Evaluate the integral for the same three cases as speciﬁed in part  b  using the method of stationary phases. To determine an appropriate contour that passes through a saddle point t = t0, write t = t0 +  u + iv  and apply the criterion for determining a level line. Sketch the relevant contour in each case, indicating what freedom there is to distort it.  25.20  Comment on the accuracy of the results obtained using the approximate methods adopted in  b  and  c . Use the method of steepest descents to show that an approximate value for the integral   cid:21  ∞  where z is real and positive, is  F z  =  −∞ exp[ iz  1  5 t5 + t  ] dt,  1 2  exp −βz  cos βz − 1  8 π ,  √ 2 .   cid:7    cid:8   2π z   cid:21   b  a  F ν  =  g t eiνf t  dt,  25.21  where β = 4  5 The stationary phase approximation to an integral of the form  ν  cid:26  1,  cid:9   π 4  !   cid:22    cid:7    cid:8   1 2 N cid:4   where f t  is a real function of t and g t  is a slowly varying function  when compared with the argument of the exponential , can be written as  F ν  ∼  g tn √ An  exp  2πν  tn , and sgn x  is the sign of x.  n=1  i  where the tn are the N stationary points of f t  that lie in a < t1 < t2 < ··· < tN < b, An =  f   cid:7  cid:7   Use this result to ﬁnd an approximation, valid for large positive values of ν,  νf tn  +  sgn  νf   tn   ,   cid:10  cid:23 "   cid:7  cid:7   to the integral   cid:21  ∞  F ν, z  =  1  −∞  1 + t2  cos[  2t3 − 3zt2 − 12z2t ν ] dt,  where z is a real positive parameter.  The Bessel function Jν z  is given for  arg z < 1  cid:7  2 π by the integral around a t − 1  contour C of the function  − ν+1  exp   cid:8  cid:14   g z  =   cid:13   t  .  1 2πi  z 2  t  The contour starts and ends along the negative real t-axis and encircles the origin in the positive sense. It can be considered to be made up of two contours. One  of them, C2, starts at t = −∞, runs through the third quadrant to the point  924  25.22   25.10 HINTS AND ANSWERS  t = −i and then approaches the origin in the fourth quadrant in a curve that  is ultimately antiparallel to the positive real axis. The other contour, C1, is the mirror image of this in the real axis; it is conﬁned to the upper half-plane, passes through t = i and is antiparallel to the real t-axis at both of its extremities. The contribution to Jν z  from the curve Ck is 1 ν being known as a Hankel function.  , the function H  k   2 H  k   ν  Using the method of steepest descents, establish the leading term in an asymp- for z real, large and positive. Deduce, without detailed . Hence establish the asymptotic  totic expansion for H  1  calculation, the corresponding result for H  2  form of Jν z  for the same range of z. Use the method of steepest descents to ﬁnd an asymptotic approximation, valid for z large, real and positive, to the function deﬁned by  ν  ν   cid:21   Fν  z  =  exp −iz sin t + iνt  dt,  where ν is real and non-negative and C is a contour that starts at t = −π + i∞ and ends at t = −i∞.  C  25.10 Hints and answers  tan  0 + ω2  ], where ω2  −1[  −2ωω0   ω2  − Apply Kirchhoﬀ’s laws to three independent loops, say ADBA, ADEA and DBED. Eliminate other currents from the equations to obtain IR = ω0CV0[  ω2 −1; IR = ω0CV0; the phase of IR is ω2 − 2iωω0   ω2 0 − ω2  ]. Set c coth u1 = −d, c coth u2 = +d, c cosech u = a and note that the capacitance is proportional to  u2 − u1  −1. −2+y2 a−1  −2 = c2  4a2 ; η = constant, hyperbolae ξ = constant, ellipses x2 a+1  −2 = c2. The curves are the cuts −c ≤ x ≤ c, y = 0 and x2 cos α  x ≥ c, y = 0. The curves for η = 2π are the same as those for η = 0.  −2 − y2 sin α   0 =  LC   0   a  For a quarter-circular contour enclosing the ﬁrst quadrant, the change in the argument of the function is 0 + 8 π 2  + 0  since y8 + 5 = 0 has no real roots ; − 3 2 , −1 ± i.  b  one negative real zero; a conjugate pair in the second and third quadrants, Evaluate   cid:21    cid:5    cid:6  cid:5   π cot πz  1 2 + z  1 4 + z   cid:6  dz  nothing.  around a large circle centred on the origin; residue at z = −1 2 is 0; residue at z = −1 4 is 4π cot −π 4 . The behaviour of the integrand for large z is z−2 exp [  2α− π z ]. The residue at z = ±m, for each integer m, is sin2 mα  −1 m  mα 2. The contour contributes Required summation = [ total sum −  m = 0 term  ] 2. Note that ¯f s  has no pole at s = 0. For t < 0 close the Bromwich contour in the −s and s − 1 and the completions right half-plane, and for t > 1 in the left half-plane. For 0 < t < 1 the integrand has to be split into separate terms containing e  cid:1  contours now contains a second-order pole at s = 0. f t  = 1 − t for 0 < t < 1, made in the right and left half-planes, respectively. The last of these completed γ tend to 0 as R → ∞ and ρ → 0. Put s = r exp iπ and s = r exp −iπ  on 2  π x 1 2. There are no poles   cid:1  ∞ 0 exp −t2x  dt = 1  but is 0 otherwise. Γ and the two sides of the cut and use inside the contour.   cid:1   925  25.23  25.1  25.3  25.5  25.7  25.9  25.11  25.13  25.15   APPLICATIONS OF COMPLEX VARIABLES  25.17  25.19  25.21  −z2 4zν and y2 z  = Dez2  4z  Use the binomial theorem to expand, in inverse powers of z, both the square −2 . root in the exponent and the fourth root in the multiplier, working to O z − ν+1 . Stokes lines: The leading terms are y1 z  = Ce arg z = 0, π 2, π, 3π 2; anti-Stokes lines: arg z =  2n + 1 π 4 for n = 0, 1, 2, 3. y1 √ √ is dominant on arg z = π 2 or 3π 2. −z2 , valid for all z, including i  a  i  b  The same values as in  a . The  only  saddle point, at t0 = z, is traversed in the direction θ = + 1 2 π in all cases, though the path in the complex t-plane varies  c  The same values as in  a . The level lines are v = ±u. In cases  i  and  ii  the with each case.  π exp β2  in case  iii .  πe  contour turns through a right angle at the saddle point. All three methods give exact answers in this case of a quadratic exponent.  Saddle points at t1 = −z and t2 = 2z with f Approximation is cid:9    cid:10    cid:16   cos 7νz3 − 1 4 π   1 2  1 = −18z and f  cid:7  cid:7  cos 20νz3 − 1 4 π    cid:7  cid:7  2 = 18z.   cid:17   1 + z2  1 + 4z2  +  .  π 9zν  25.23  Saddle point at t0 = cos   2π z 1 2 exp [ i z − 1  2 νπ − 1  −1 ν z  is traversed in the direction θ = − 1 4 π  ].  4 π. Fν z  ≈  926   26  Tensors  It may seem obvious that the quantitative description of physical processes cannot depend on the coordinate system in which they are represented. However, we may turn this argument around: since physical results must indeed be independent of the choice of coordinate system, what does this imply about the nature of the quantities involved in the description of physical processes? The study of these implications and of the classiﬁcation of physical quantities by means of them forms the content of the present chapter.  Although the concepts presented here may be applied, with little modiﬁ- cation, to more abstract spaces  most notably the four-dimensional space–time of special or general relativity , we shall restrict our attention to our familiar three- dimensional Euclidean space. This removes the need to discuss the properties of diﬀerentiable manifolds and their tangent and dual spaces. The reader who is interested in these more technical aspects of tensor calculus in general spaces, and in particular their application to general relativity, should consult one of the § many excellent textbooks on the subject.  Before the presentation of the main development of the subject, we begin by introducing the summation convention, which will prove very useful in writing tensor equations in a more compact form. We then review the eﬀects of a change of basis in a vector space; such spaces were discussed in chapter 8. This is followed by an investigation of the rotation of Cartesian coordinate systems, and ﬁnally we broaden our discussion to include more general coordinate systems and transformations.  §  For example, R. D’Inverno, Introducing Einstein’s Relativity  Oxford: Oxford University Press, 1992 ; J. Foster and J. D. Nightingale, A Short Course in General Relativity  New York: Springer, 2006 ; B. F. Schutz, A First Course in General Relativity  Cambridge; Cambridge University Press 1985 .  927   TENSORS  26.1 Some notation  Before proceeding further, we introduce the summation convention for subscripts, since its use looms large in the work of this chapter. The convention is that any lower-case alphabetic subscript that appears exactly twice in any term of an expression is understood to be summed over all the values that a subscript in that position can take  unless the contrary is speciﬁcally stated . The subscripted quantities may appear in the numerator and or the denominator of a term in an expression. This naturally implies that any such pair of repeated subscripts must occur only in subscript positions that have the same range of values. Sometimes the ranges of values have to be speciﬁed but usually they are apparent from the context.  The following simple examples illustrate what is meant  in the three-dimensional  case :   i  aixi stands for a1x1 + a2x2 + a3x3;   ii  aijbjk stands for ai1b1k + ai2b2k + ai3b3k;   iii  aijbjkck stands for  3 j=1  3 k=1 aijbjkck;   cid:11    cid:11    iv   stands for  ∂vi ∂xi ∂2φ ∂xi∂xi  ∂v1 ∂x1  +  ∂v2 ∂x2  +  ∂v3 ∂x3  ;  ∂2φ ∂x2 1  ∂2φ ∂x2 2  ∂2φ ∂x2 3  .   v   stands for  +  +  Subscripts that are summed over are called dummy subscripts and the others free subscripts. It is worth remarking that when introducing a dummy subscript into an expression, care should be taken not to use one that is already present, either as a free or as a dummy subscript. For example, aij bjkckl cannot, and must not, be replaced by aijbjjcjl or by ailblkckl, but could be replaced by aimbmkckl or by aimbmncnl. Naturally, free subscripts must not be changed at all unless the working calls for it.  Furthermore, as we have done throughout this book, we will make frequent  use of the Kronecker delta δij, which is deﬁned by    δij =  1 if i = j,  0 otherwise.  When the summation convention has been adopted, the main use of δij is to replace one subscript by another in certain expressions. Examples might include  and  aijδjk = aij δkj = aik.   26.1   bjδij = bi,  928   26.2 CHANGE OF BASIS  In the second of these the dummy index shared by both terms on the left-hand side  namely j  has been replaced by the free index carried by the Kronecker delta  namely k , and the delta symbol has disappeared. In matrix language,  26.1  can be written as AI = A, where A is the matrix with elements aij and I is the unit matrix having the same dimensions as A.  In some expressions we may use the Kronecker delta to replace indices in a  number of diﬀerent ways, e.g.  aijbjkδki = aijbji  or  akjbjk,  where the two expressions on the RHS are totally equivalent to one another.  26.2 Change of basis  In chapter 8 some attention was given to the subject of changing the basis set  or coordinate system  in a vector space and it was shown that, under such a change, diﬀerent types of quantity behave in diﬀerent ways. These results are given in section 8.15, but are summarised below for convenience, using the summation convention. Although throughout this section we will remind the reader that we are using this convention, it will simply be assumed in the remainder of the chapter.  If we introduce a set of basis vectors e1, e2, e3 into our familiar three-dimensional  vector  space, then we can describe any vector x in terms of its components x1, x2, x3 with respect to this basis:  x = x1e1 + x2e2 + x3e3 = xiei,  where we have used the summation convention to write the sum in a more  cid:7  3 related to the old one by compact form. If we now introduce a new basis e   cid:7  1, e   cid:7  2, e   cid:7  j = Sijei  e   sum over i ,  where the coeﬃcient Sij is the ith component of the vector e unprimed basis, then we may write x with respect to the new basis as   cid:7  1e   cid:7  1 + x   cid:7  2e   cid:7  2 + x   cid:7  3e   cid:7  3 = x   cid:7  ie   cid:7   i  x = x   sum over i .  If we denote the matrix with elements Sij by S, then the components x  in the two bases are related by   cid:7  i and xi   26.2    cid:7  j with respect to the   cid:7  i =  S−1 ij xj  x   sum over j ,  where, using the summation convention, there is an implicit sum over j from j = 1 to j = 3. In the special case where the transformation is a rotation of the coordinate axes, the transformation matrix S is orthogonal and we have   sum over j .   26.3    cid:7  i =  ST ij xj = Sjixj  x  929   TENSORS  Scalars behave diﬀerently under transformations, however, since they remain unchanged. For example, the value of the scalar product of two vectors x · y  which is just a number  is unaﬀected by the transformation from the unprimed to the primed basis. Diﬀerent again is the behaviour of linear operators. If a linear operator A is represented by some matrix A in a given coordinate system then in the new  primed  coordinate system it is represented by a new matrix, A cid:7   = S−1AS. In this chapter we develop a general formulation to describe and classify these diﬀerent types of behaviour under a change of basis  or coordinate transfor- mation . In the development, the generic name tensor is introduced, and certain scalars, vectors and linear operators are described respectively as tensors of ze- roth, ﬁrst and second order  the order – or rank – corresponds to the number of subscripts needed to specify a particular element of the tensor . Tensors of third and fourth order will also occupy some of our attention.  26.3 Cartesian tensors  We begin our discussion of tensors by considering a particular class of coordinate transformation – namely rotations – and we shall conﬁne our attention strictly to the rotation of Cartesian coordinate systems. Our object is to study the prop- erties of various types of mathematical quantities, and their associated physical interpretations, when they are described in terms of Cartesian coordinates and the axes of the coordinate system are rigidly rotated from a basis e1, e2, e3  lying  cid:7  1, along the Ox1, Ox2 and Ox3 axes  to a new one e Ox   cid:7  2 and Ox Since we shall be more interested in how the components of a vector or linear operator are changed by a rotation of the axes than in the relationship between  cid:7  i, let us deﬁne the transformation matrix L the two sets of basis vectors ei and e as the inverse of the matrix S in  26.2 . Thus, from  26.2 , the components of a position vector x, in the old and new bases respectively, are related by   cid:7  3  lying along the Ox   cid:7  3 axes .   cid:7  1, e   cid:7  2, e  Because we are considering only rigid rotations of the coordinate axes, the transformation matrix L will be orthogonal, i.e. such that L−1 = LT. Therefore the inverse transformation is given by  The orthogonality of L also implies relations among the elements of L that  express the fact that LLT = LTL = I. In subscript notation they are given by  LikLjk = δij  and  LkiLkj = δij.   26.6   Furthermore, in terms of the basis vectors of the primed and unprimed Cartesian   26.4    26.5    cid:7  i = Lijxj.  x  xi = Ljix   cid:7  j.  930   26.3 CARTESIAN TENSORS  x2   cid:7  2  x  θ  θ  O  θ   cid:7  1  x  x1  Figure 26.1 Rotation of Cartesian axes by an angle θ about the x3-axis. The three angles marked θ and the parallels  broken lines  to the primed axes show how the ﬁrst two equations of  26.7  are constructed.  coordinate systems, the transformation matrix is given by  Lij = e   cid:7   i  · ej.   cid:7  cid:7  i = Mijx  We note that the product of two rotations is also a rotation. For example,  cid:7  j; then the composite rotation is described suppose that x by   cid:7  i = Lijxj and x   cid:7  j = MijLjkxk =  ML ikxk,   cid:7  cid:7  i = Mijx corresponding to the matrix ML.  cid:1 Find the transformation matrix L corresponding to a rotation of the coordinate axes through an angle θ about the e3-axis  or x3-axis , as shown in ﬁgure 26.1.  x  Taking x as a position vector – the most obvious choice – we see from the ﬁgure that the components of x with respect to the new  primed  basis are given in terms of the components in the old  unprimed  basis by  The  orthogonal  transformation matrix is thus  The inverse equations are  in line with  26.5 .  cid:2    cid:7  1 = x1 cos θ + x2 sin θ, 2 = −x1 sin θ + x2 cos θ,  cid:7   cid:7  3 = x3.  x  x  x   cos θ  − sin θ  0  L =   .  sin θ cos θ  0  0 0 1  1 cos θ − x  cid:7   cid:7  2 sin θ,  cid:7   cid:7  1 sin θ + x 2 cos θ,  cid:7  3,  x1 = x x2 = x x3 = x  931   26.7    26.8    26.4 First- and zero-order Cartesian tensors  Using the above example as a guide, we may consider any set of three quantities vi, which are directly or indirectly functions of the coordinates xi and possibly involve some constants, and ask how their values are changed by any rotation of the Cartesian axes. The speciﬁc question to be answered is whether the speciﬁc  cid:7  i in the new variables can be obtained from the old ones vi using  26.4 , forms v  If so, the vi are said to form the components of a vector or ﬁrst-order Cartesian tensor v. By deﬁnition, the position coordinates are themselves the components of such a tensor.The ﬁrst-order tensor v does not change under rotation of the coordinate axes; nevertheless, since the basis set does change, from e1, e2, e3 to  cid:7   cid:7  3, the components of v must also change. The changes must be such that 1, e e   cid:7  2, v  is unchanged.  Since the transformation  26.9  is orthogonal, the components of any such  ﬁrst-order Cartesian tensor also obey a relation that is the inverse of  26.9 ,  TENSORS   cid:7  i = Lijvj. v  v = viei = v   cid:7  ie   cid:7   i  vi = Ljiv   cid:7  j.  We now consider explicit examples. In order to keep the equations to reasonable i.e. there are proportions, the examples will be restricted to the x1x2-plane, no components in the x3-direction. Three-dimensional cases are no diﬀerent in principle – but much longer to write out.  cid:1 Which of the following pairs  v1, v2  form the components of a ﬁrst-order Cartesian tensor in two dimensions?:   i   x2,−x1 ,   ii   x2, x1 ,   iii   x2  1, x2 2 .  We shall consider the rotation discussed in the previous example, and to save space we denote cos θ by c and sin θ by s.   i  Here v1 = x2 and v2 = −x1, referred to the old axes. In terms of the new coordinates  they will be v   cid:7  1 = x   cid:7  2 and v  2 = −x  cid:7   Now if we start again and evaluate v   cid:7  2 as given by  26.9  we ﬁnd that   cid:7  1, i.e. 2 = −sx1 + cx2  cid:7   cid:7  1 = x v 1 = −cx1 − sx2. 2 = −x  cid:7   cid:7  v  cid:7  1 and v  1 = L11v1 + L12v2 = cx2 + s −x1   cid:7  2 = L21v1 + L22v2 = −s x2  + c −x1 .  cid:7   v v  The expressions for v   cid:7  of θ  i.e. for all rotations  and thus by deﬁnition  26.9  the pair  x2,−x1  is a ﬁrst-order 2 in  26.12  and  26.13  are the same whatever the values   cid:7  1 and v  Cartesian tensor.  932   26.9    26.10    26.11    26.12    26.13    26.4 FIRST- AND ZERO-ORDER CARTESIAN TENSORS   ii  Here v1 = x2 and v2 = x1. Following the same procedure,  But, by  26.9 , for a Cartesian tensor we must have   cid:7  1 = x v  cid:7  2 = x v  2 = −sx1 + cx2  cid:7   cid:7  1 = cx1 + sx2.   cid:7  1 = cv1 + sv2 = cx2 + sx1 v 2 =  −s v1 + cv2 = −sx2 + cx1.  cid:7   v  These two sets of expressions do not agree and thus the pair  x2, x1  is not a ﬁrst-order Cartesian tensor.   iii  v1 = x2  suﬃcient to show that this pair is also not a ﬁrst-order tensor. Evaluating v  2. As in  ii  above, considering the ﬁrst component alone is  cid:7  1 directly gives  1 and v2 = x2   cid:7  1 = x v   cid:7  1  2 = c2x2  1 + 2csx1x2 + s2x2 2,   cid:7  1 = cv1 + sv2 = cx2 v  1 + sx2 2,  whilst  26.9  requires that  which is quite diﬀerent.  cid:2   There are many physical examples of ﬁrst-order tensors  i.e. vectors  that will be familiar to the reader. As a straightforward one, we may take the set of Cartesian components of the momentum of a particle of mass m,  m˙x1, m˙x2, m˙x3 . This set transforms in all essentials as  x1, x2, x3 , since the other operations involved, multiplication by a number and diﬀerentiation with respect to time, are quite unaﬀected by any orthogonal transformation of the axes. Similarly, acceleration and force are represented by the components of ﬁrst-order tensors.  Other more complicated vectors involving the position coordinates more than once, such as the angular momentum of a particle of mass m, namely J =  x × p = m x × ˙x , are also ﬁrst-order tensors. That this is so is less obvious in  component form than for the earlier examples, but may be veriﬁed by writing out the components of J explicitly or by appealing to the quotient law to be discussed in section 26.7 and using the Cartesian tensor  cid:4 ijk from section 26.8.  Having considered the eﬀects of rotations on vector-like sets of quantities we may consider quantities that are unchanged by a rotation of axes. In our previous nomenclature these have been called scalars but we may also describe them as tensors of zero order. They contain only one element  formally, the number of subscripts needed to identify a particular element is zero ; the most obvious non- trivial example associated with a rotation of axes is the square of the distance of a point from the origin, r2 = x2 3. In the new coordinate system it will 2, which for any rotation has the same value as 2 + x have the form r x2 1 + x2  1 + x2  cid:7  3   cid:7 2 = x  2 + x2 3.  2 + x2  2 + x   cid:7  2   cid:7  1  933   TENSORS  In fact any scalar product of two ﬁrst-order tensors  vectors  is a zero-order tensor  scalar , as might be expected since it can be written in a coordinate-free way as u · v.   cid:1 By considering the components of the vectors u and v with respect to two Cartesian coordinate systems  related by a rotation , show that the scalar product u · v is invariant  under rotation.  In the original  unprimed  system the scalar product is given in terms of components by uivi  summed over i , and in the rotated  primed  system by   cid:7  iv   cid:7  i = Lijuj Likvk = Lij Likuj vk = δjkuj vk = ujvj ,  u  where we have used the orthogonality relation  26.6 . Since the resulting expression in the rotated system is the same as that in the original system, the scalar product is indeed invariant under rotations.  cid:2   The above result leads directly to the identiﬁcation of many physically im- portant quantities as zero-order tensors. Perhaps the most immediate of these is energy, either as potential energy or as an energy density  e.g. F· dr, eE· dr, D· E, B · H, µ · B , but others, such as the angle between two directed quantities, are important.  As mentioned in the ﬁrst paragraph of this chapter, in most analyses of physical situations it is a scalar quantity  such as energy  that is to be determined. Such quantities are invariant under a rotation of axes and so it is possible to work with the most convenient set of axes and still have conﬁdence in the results.  Complementing the way in which a zero-order tensor was obtained from two ﬁrst-order tensors, so a ﬁrst-order tensor can be obtained from a zero-order tensor  i.e. a scalar . We show this by taking a speciﬁc example, that of the electric ﬁeld  E = −∇φ; this is derived from a scalar, the electrostatic potential φ, and has  components  Ei = − ∂φ  .  ∂xi   26.14   Clearly, E is a ﬁrst-order tensor, but we may prove this more formally by considering the behaviour of its components  26.14  under a rotation of the coordinate axes, since the components of the electric ﬁeld E   cid:7  i are then given by   cid:8  cid:7    cid:7  − ∂φ ∂xi   cid:7  i =  E   cid:7  = − ∂φ  cid:7   ∂x  i  = − ∂xj  cid:7   ∂x  i  ∂φ ∂xj  = LijEj,   26.15    cid:7  where  26.5  has been used to evaluate ∂xj ∂x i. Now  26.15  is in the form  26.9 , thus conﬁrming that the components of the electric ﬁeld do behave as the components of a ﬁrst-order tensor.  934   26.5 SECOND- AND HIGHER-ORDER CARTESIAN TENSORS   cid:1 If vi are the components of a ﬁrst-order tensor, show that ∇ · v = ∂vi ∂xi is a zero-order In the rotated coordinate system ∇ · v is given by  tensor.   cid:7    cid:8  cid:7   ∂vi ∂xi   cid:7   cid:7   i  ∂v i ∂x  ∂xj  cid:7  ∂x  i  ∂ ∂xj  =  =   Likvk  = Lij Lik  ∂vk ∂xj  ,   cid:7   cid:7   i  ∂v i ∂x  = LijLik  = δjk  ∂vk ∂xj  ∂vk ∂xj  =  ∂vj ∂xj  .  since the elements Lij are not functions of position. Using the orthogonality relation  26.6  we then ﬁnd  was to be expected since it can be written in a coordinate-free way as ∇ · v.  cid:2  Hence ∂vi ∂xi is invariant under rotation of the axes and is thus a zero-order tensor; this  26.5 Second- and higher-order Cartesian tensors  Following on from scalars with no subscripts and vectors with one subscript, we turn to sets of quantities that require two subscripts to identify a particular element of the set. Let these quantities by denoted by Tij.  Taking  26.9  as a guide we deﬁne a second-order Cartesian tensor as follows: the Tij form the components of such a tensor if, under the same conditions as for  26.9 ,  At the same time we may deﬁne a Cartesian tensor of general order as follows. The set of expressions Tij···k form the components of a Cartesian tensor if, for all rotations of the axes of coordinates given by  26.4  and  26.5 , subject to  26.6 , the expressions using the new coordinates, T   cid:7  ij···k are given by  T   cid:7  ij = LikLjlTkl  cid:7  kl.  Tij = LkiLljT  T   cid:7  ij···k = LipLjq ··· LkrTpq···r  cid:7  Tij···k = LpiLqj ··· LrkT pq···r.   26.16    26.17    26.18    26.19   and  and  It is apparent that in three dimensions, an Nth-order Cartesian tensor has 3N components.  Since a second-order tensor has two subscripts, it is natural to display its components in matrix form. The notation [Tij ] is used, as well as T, to denote § the matrix having Tij as the element in the ith row and jth column.  We may think of a second-order tensor T as a geometrical entity in a similar way to that in which we viewed linear operators  which transform one vector into  §  We can also denote the column matrix containing the elements vi of a vector by [vi].  935   TENSORS  another, without reference to any coordinate system  and consider the matrix containing its components as a representation of the tensor with respect to a particular coordinate system. Moreover, the matrix T = [Tij ], containing the components of a second-order tensor, behaves in the same way under orthogonal transformations T cid:7   = LTLT as a linear operator.  However, not all linear operators are second-order tensors. More speciﬁcally, the two subscripts in a second-order tensor must refer to the same coordinate system. In particular, this means that any linear operator that transforms a vector into a vector in a diﬀerent vector space cannot be a second-order tensor. Thus, although the elements Lij of the transformation matrix are written with two subscripts, they cannot be the components of a tensor since the two subscripts each refer to a diﬀerent coordinate system.  As examples of sets of quantities that are readily shown to be second-order  tensors we consider the following.   i  The outer product of two vectors. Let ui and vi, i = 1, 2, 3, be the components  of two vectors u and v, and consider the set of quantities Tij deﬁned by  The set Tij are called the components of the the outer product of u and v. Under rotations the components Tij become   cid:7   cid:7  ij = u iv   cid:7  j = LikukLjlvl = LikLjlukvl = LikLjlTkl,  T  which shows that they do transform as the components of a second-order tensor. Use has been made in  26.21  of the fact that ui and vi are the components of ﬁrst-order tensors.  The outer product of two vectors is often denoted, without reference to any  coordinate system, as  Tij = uivj.  T = u ⊗ v.   26.20    26.21    26.22    This is not to be confused with the vector product of two vectors, which is itself a vector and is discussed in chapter 7.  The expression  26.22  gives the basis to which the components Tij of the second-order tensor refer: since u = uiei and v = viei, we may write the tensor T as  T = uiei ⊗ vjej = uivjei ⊗ ej = Tij ei ⊗ ej.   26.23   Moreover, as for the case of ﬁrst-order tensors  see equation  26.10   we note  cid:7  ij are the components of the same tensor T, but referred to  that the quantities T a diﬀerent coordinate system, i.e.  These concepts can be extended to higher-order tensors.  T = Tij ei ⊗ ej = T  cid:7  ij e   cid:7   i  ⊗ e   cid:7  j.  936   26.5 SECOND- AND HIGHER-ORDER CARTESIAN TENSORS   ii  The gradient of a vector. Suppose vi represents the components of a vector; let us consider the quantities generated by forming the derivatives of each vi, i = 1, 2, 3, with respect to each xj, j = 1, 2, 3, i.e.  Tij =  ∂vi ∂xj  .  These nine quantities form the components of a second-order tensor, as can be seen from the fact that   cid:7  ij =  T   cid:7   cid:7   j  ∂v i ∂x  ∂ Likvk   =  ∂xl  ∂xl  cid:7  ∂x  j  = Lik  Ljl = LikLjlTkl.  ∂vk ∂xl  In coordinate-free language the tensor T may be written as T = ∇v and hence  gives meaning to the concept of the gradient of a vector, a quantity that was not discussed in the chapter on vector calculus  chapter 10 .  A test of whether any given set of quantities forms the components of a second-  cid:7  i in terms of the order tensor can always be made by direct substitution of the x xi, followed by comparison with the right-hand side of  26.16 . This procedure is extremely laborious, however, and it is almost always better to try to recognise the set as being expressible in one of the forms just considered, or to make alternative tests based on the quotient law of section 26.7 below.  cid:1 Show that the Tij given by   cid:8    cid:7   T = [Tij] =  x2 −x1x2 2  −x1x2 x2 1   26.24   are the components of a second-order tensor.  Again we consider a rotation θ about the e3-axis. Carrying out the direct evaluation ﬁrst we obtain, using  26.7 ,   cid:7   cid:7  11 = x 12 = −x 2  cid:7  21 = −x  cid:7   cid:7   cid:7  22 = x 1  T  T  T  T   cid:7  2 = scx2  cid:7  2 = scx2  2 = s2x2 1  cid:7  1x  cid:7  1x 2 = c2x2  − 2scx1x2 + c2x2 1 +  s2 − c2 x1x2 − scx2 1 +  s2 − c2 x1x2 − scx2  2,  2, 2,  1 + 2scx1x2 + s2x2 2.  Now, evaluating the right-hand side of  26.16 ,   cid:7  11 = ccx2 12 = c −s x2  cid:7  21 =  −s cx2  cid:7  22 =  −s  −s x2  cid:7   T  T  T  T  2 + cs −x1x2  + sc −x1x2  + ssx2  1,  2 + cc −x1x2  + s −s  −x1x2  + scx2 2 +  −s s −x1x2  + cc −x1x2  + csx2  1, 1,  2 +  −s c −x1x2  + c −s  −x1x2  + ccx2  1.  After reorganisation, the corresponding expressions are seen to be the same, showing, as required, that the Tij are the components of a second-order tensor.  The same result could be inferred much more easily, however, by noting that the Tij  are in fact the components of the outer product of the vector  x2,−x1  with itself. That  x2,−x1  is indeed a vector was established by  26.12  and  26.13 .  cid:2   937   TENSORS  Physical examples involving second-order tensors will be discussed in the later sections of this chapter, but we might note here that, for example, magnetic susceptibility and electrical conductivity are described by second-order tensors.  26.6 The algebra of tensors  Because of the similarity of ﬁrst- and second-order tensors to column vectors and matrices, it would be expected that similar types of algebraic operation can be carried out with them and so provide ways of constructing new tensors from old ones. In the remainder of this chapter, instead of referring to the Tij  say  as the components of a second-order tensor T, we may sometimes simply refer to Tij as the tensor. It should always be remembered, however, that the Tij are in fact  cid:7  ij refers to the just the components of T in a given coordinate system and that T components of the same tensor T in a diﬀerent coordinate system.  The addition and subtraction of tensors follows an obvious deﬁnition; namely that if Vij···k and Wij···k are  the components of  tensors of the same order, then their sum and diﬀerence, Sij···k and Dij···k respectively, are given by  Sij···k = Vij···k + Wij···k, Dij···k = Vij···k − Wij···k,  for each set of values i, j, . . . , k. That Sij···k and Dij···k are the components of tensors follows immediately from the linearity of a rotation of coordinates.  It is equally straightforward to show that if the Tij···k are the components of a tensor, then so is the set of quantities formed by interchanging the order of  a pair of  indices, e.g. Tji···k.  If Tji···k is found to be identical with Tij···k then Tij···k is said to be symmetric with respect to its ﬁrst two subscripts  or simply ‘symmetric’, for second-order tensors . If, however, Tji···k = −Tij···k for every element then it is an antisymmetric  tensor. An arbitrary tensor is neither symmetric nor antisymmetric but can always be written as the sum of a symmetric tensor Sij···k and an antisymmetric tensor Aij···k:  Tij···k = 1  2  Tij···k + Tji···k  + 1  = Sij···k + Aij···k.  2  Tij···k − Tji···k   Of course these properties are valid for any pair of subscripts.  In  26.20  in the previous section we had an example of a kind of ‘multiplication’ of two tensors, thereby producing a tensor of higher order – in that case two ﬁrst-order tensors were multiplied to give a second-order tensor. Inspection of  26.21  shows that there is nothing particular about the orders of the tensors involved and it follows as a general result that the outer product of an Nth-order tensor with an Mth-order tensor will produce an  M + N th-order tensor.  938   26.7 THE QUOTIENT LAW  An operation that produces the opposite eﬀect – namely, generates a tensor of smaller rather than larger order – is known as contraction and consists of making two of the subscripts equal and summing over all values of the equalised subscripts.  cid:1 Show that the process of contraction of an Nth-order tensor produces another tensor, of order N − 2. Let Tij···l···m···k be the components of an Nth-order tensor, then  ; ij···l···m···k = LipLjq ··· Llr ··· Lms ··· Lkn  cid:7   9:  8  T  Tpq···r···s···n.  N factors  Thus if, for example, we make the two subscripts l and m equal and sum over all values of these subscripts, we obtain  ij···l···l···k = LipLjq ··· Llr ··· Lls ··· LknTpq···r···s···n  cid:7   T  = LipLjq ··· δrs ··· LknTpq···r···s···n 8 ; = LipLjq ··· Lkn  N − 2  factors  Tpq···r···r···n,  9:  showing that Tij···l···l···k are the components of a  diﬀerent  Cartesian tensor of order N − 2.  cid:2   For a second-rank tensor, the process of contraction is the same as taking the trace of the corresponding matrix. The trace Tii itself is thus a zero-order tensor  or scalar  and hence invariant under rotations, as was noted in chapter 8.  The process of taking the scalar product of two vectors can be recast into tensor language as forming the outer product Tij = uivj of two ﬁrst-order tensors u and v and then contracting the second-order tensor T so formed, to give Tii = uivi, a scalar  invariant under a rotation of axes .  As yet another example of a familiar operation that is a particular case of a contraction, we may note that the multiplication of a column vector [ui] by a matrix [Bij] to produce another column vector [vi],  Bij uj = vi,  can be looked upon as the contraction Tijj of the third-order tensor Tijk formed from the outer product of Bij and uk.  26.7 The quotient law  The previous paragraph appears to give a heavy-handed way of describing a familiar operation, but it leads us to ask whether it has a converse. To put the question in more general terms: if we know that B and C are tensors and also that  Apq···k···mBij···k···n = Cpq···mij···n,   26.25   939   TENSORS  does this imply that the Apq···k···m also form the components of a tensor A? Here A, B and C are respectively of Mth, Nth and  M +N−2 th order and it should be  noted that the subscript k that has been contracted may be any of the subscripts in A and B independently.  The quotient law for tensors states that if  26.25  holds in all rotated coordinate frames then the Apq···k···m do indeed form the components of a tensor A. To prove it for general M and N is no more diﬃcult regarding the ideas involved than to show it for speciﬁc M and N, but this does involve the introduction of a large number of subscript symbols. We will therefore take the case M = N = 2, but it will be readily apparent that the principle of the proof holds for general M and N.  We thus start with  say   ApkBik = Cpi,   26.26   where Bik and Cpi are arbitrary second-order tensors. Under a rotation of coor- dinates the set Apk  tensor or not  transforms into a new set of quantities that  cid:7  pk. We thus obtain in succession the following steps, using we will denote by A  26.16 ,  26.17  and  26.6 :  cid:7    cid:7  A pkB   cid:7  ik = C pi  = LpqLijCqj = LpqLijAqlBjl = LpqLijAqlLmjLnlB  cid:7  = LpqLnlAqlB  in   cid:7   mn   transforming  26.26  ,  since C is a tensor ,  from  26.26  ,  since B is a tensor ,  since LijLmj = δim .  Now k on the left and n on the right are dummy subscripts and thus we may  write   cid:7   A  pk  − LpqLklAql B   cid:7  ik = 0.   26.27   Since Bik, and hence B   cid:7  ik, is an arbitrary tensor, we must have   cid:7  pk = LpqLklAql, A   cid:7  showing that the A pk are given by the general formula  26.18  and hence that the Apk are the components of a second-order tensor. By following an analogous argument, the same result  26.27  and deduction could be obtained if  26.26  were replaced by  i.e. the contraction being now with respect to a diﬀerent pair of indices.  Use of the quotient law to test whether a given set of quantities is a tensor is generally much more convenient than making a direct substitution. A particular way in which it is applied is by contracting the given set of quantities, having  ApkBki = Cpi,  940   26.8 THE TENSORS δij AND  cid:4 ijk  N subscripts, with an arbitrary Nth-order tensor  i.e. one having independently variable components  and determining whether the result is a scalar.  cid:1 Use the quotient law to show that the elements of T, equation  26.24 , are the components of a second-order tensor.  The outer product xixj is a second-order tensor. Contracting this with the Tij given in  26.24  we obtain  Tijxixj = x2  2x2  1  − x1x2x1x2 − x1x2x2x1 + x2 1x2  2 = 0,  which is clearly invariant  a zeroth-order tensor . Hence by the quotient theorem Tij must also be a tensor.  cid:2   In many places throughout this book we have encountered and used the two- subscript quantity δij deﬁned by  26.8 The tensors δij and  cid:4 ijk    δij =  1 if i = j,  0 otherwise.  Let us now also introduce the three-subscript Levi–Civita symbol  cid:4 ijk, the value of which is given by  +1 if i, j, k is an even permutation of 1, 2, 3,  −1 if i, j, k is an odd permutation of 1, 2, 3,  0  otherwise.   cid:4 ijk =  We will now show that δij and  cid:4 ijk are respectively the components of a second- and a third-order Cartesian tensor. Notice that the coordinates xi do not appear explicitly in the components of these tensors, their components consisting entirely of 0 and 1.  In passing, we also note that  cid:4 ijk is totally antisymmetric, i.e. it changes sign under the interchange of any pair of subscripts. In fact  cid:4 ijk, or any scalar multiple of it, is the only three-subscript quantity with this property.  Treating δij ﬁrst, the proof that it is a second-order tensor is straightforward  since if, from  26.16 , we consider the equation   cid:7  kl = LkiLljδij = LkiLli = δkl, δ  we see that the transformation of δij generates the same expression  a pattern  cid:7  ij in the transformed coordinates. Thus of 0’s and 1’s  as does the deﬁnition of δ δij transforms according to the appropriate tensor transformation law and is therefore a second-order tensor.  Turning now to  cid:4 ijk, we have to consider the quantity   cid:7  lmn = LliLmjLnk cid:4 ijk.  cid:4   941   26.28    Let us begin, however, by noting that we may use the Levi–Civita symbol to  write an expression for the determinant of a 3 × 3 matrix A,  which may be shown to be equivalent to the Laplace expansion  see chapter 8 . Indeed many of the properties of determinants discussed in chapter 8 can be proved very eﬃciently using this expression  see exercise 26.9 .   cid:1 Evaluate the determinant of the matrix   26.29  §  TENSORS  A cid:4 lmn = AliAmjAnk cid:4 ijk,   2  A =  1 −3  3  4  1 −2  0 1   .  Setting l = 1, m = 2 and n = 3 in  26.29  we ﬁnd  A =  cid:4 ijkA1iA2j A3k  =  2  4  1  −  2  0  −2  −  1  3  1  +  −3  3  −2   +  1  0  1  −  −3  4  1  = 35,  which may be veriﬁed using the Laplace expansion method.  cid:2   We can now show that the  cid:4 ijk are in fact the components of a third-order tensor. Using  26.29  with the general matrix A replaced by the speciﬁc transformation matrix L, we can rewrite the RHS of  26.28  in terms of L  lmn = LliLmjLnk cid:4 ijk = L cid:4 lmn.  cid:7   cid:4    cid:7  Since L is orthogonal its determinant has the value unity, and so  cid:4  lmn =  cid:4 lmn.  cid:7  lmn has exactly the properties of  cid:4 ijk but with i, j, k replaced by Thus we see that  cid:4  l, m, n, i.e. it is the same as the expression  cid:4 ijk written using the new coordinates. This shows that  cid:4 ijk is a third-order Cartesian tensor.  In addition to providing a convenient notation for the determinant of a matrix, δij and  cid:4 ijk can be used to write many of the familiar expressions of vector algebra and calculus as contracted tensors. For example, provided we are using right-handed Cartesian coordinates, the vector product a = b × c has as its ith component ai =  cid:4 ijkbjck; this should be contrasted with the outer product T = b ⊗ c, which is a second-order tensor having the components Tij = bicj.  §  This may be readily extended to an N × N matrix A, i.e.  A cid:4 i1i2···iN = Ai1j1 Ai2j2  ··· AiN jN  cid:4 j1j2···jN ,  where  cid:4 i1i2···iN equals 1 if i1i2 ··· iN is an even permutation of 1, 2,. . . , N and equals −1 if it is an  odd permutation; otherwise it equals zero.  942   26.8 THE TENSORS δij AND  cid:4 ijk   cid:1 Write the following as contracted Cartesian tensors: a· b, ∇2φ, ∇× v, ∇ ∇· v , ∇×  ∇× v ,  a × b  · c.  The corresponding  contracted  tensor expressions are readily seen to be as follows:  a · b = aibi = δijaibj , ∇2φ =  = δij  ∂2φ ∂xi∂xi ∂vk ∂xj   cid:7    cid:8   ,   cid:7   ∂vj ∂xj   ∇ × v i =  cid:4 ijk [∇ ∇ · v ]i =  ∂ ∂xi  ∂2φ ∂xi∂xj  ,  = δjk   cid:8   ∂2vj ∂xi∂xk  ,  [∇ ×  ∇ × v ]i =  cid:4 ijk  ∂ ∂xj   cid:4 klm  ∂vm ∂xl  =  cid:4 ijk cid:4 klm  ∂2vm ∂xj∂xl  ,   a × b  · c = δij ci cid:4 jklakbl =  cid:4 iklciakbl.  cid:2   An important relationship between the  cid:4 - and δ- tensors is expressed by the  identity   cid:4 ijk cid:4 klm = δilδjm − δimδjl.  To establish the validity of this identity between two fourth-order tensors  the LHS is a once-contracted sixth-order tensor  we consider the various possible cases.  The RHS of  26.30  has the values  +1 if i = l and j = m  cid:3 = i, −1 if i = m and j = l  cid:3 = i,   26.30    26.31    26.32    26.33   0 for any other set of subscript values i, j, l, m.  In each product on the LHS k has the same value in both factors and for a non-zero contribution none of i, l, j, m can have the same value as k. Since there are only three values, 1, 2 and 3, that any of the subscripts may take, the only non-zero possibilities are i = l and j = m or vice versa but not all four subscripts equal  since then each  cid:4  factor is zero, as it would be if i = j or l = m . This reproduces  26.33  for the LHS of  26.30  and also the conditions  26.31  and  26.32 . The values in  26.31  and  26.32  are also reproduced in the LHS of  26.30  since   i  if i = l and j = m,  cid:4 ijk =  cid:4 lmk =  cid:4 klm and, whether  cid:4 ijk is +1 or −1, the  ii  if i = m and j = l,  cid:4 ijk =  cid:4 mlk = − cid:4 klm and thus the product  cid:4 ijk cid:4 klm  no  product of the two factors is +1; and  summation  has the value −1.  This concludes the establishment of identity  26.30 .  943   TENSORS  A useful application of  26.30  is in obtaining alternative expressions for vector  quantities that arise from the vector product of a vector product.  cid:1 Obtain an alternative expression for ∇ ×  ∇ × v . As shown in the previous example, ∇ ×  ∇ × v  can be expressed in tensor form as  [∇ ×  ∇ × v ]i =  cid:4 ijk cid:4 klm  cid:7    cid:8  =  δilδjm − δimδjl   ∂2vm ∂xj∂xl  ∂2vm ∂xj∂xl − ∂2vi ∂xj∂xj  =  ∂ ∂xi  ∂vj ∂xj  = [∇ ∇ · v ]i − ∇2vi,   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  δip  δjp δkp   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20   δiq δjq δkq  δir δjr δkr   cid:4 ijk cid:4 pqr =   cid:4 ijk cid:4 ilm = δjlδkm − δjmδkl.  where in the second line we have used the identity  26.30 . This result has already been mentioned in chapter 10 and the reader is referred there for a discussion of its applicability.  cid:2   By examining the various possibilities, it is straightforward to verify that, more  generally,   26.34    26.35   and it is easily seen that  26.30  is a special case of this result. From  26.34  we can derive alternative forms of  26.30 , for example,  The pattern of subscripts in these identities is most easily remembered by noting that the subscripts on the ﬁrst δ on the RHS are those that immediately follow  cyclically, if necessary  the common subscript, here i, in each  cid:4 -term on the LHS; the remaining combinations of j, k, l, m as subscripts in the other δ-terms on the RHS can then be ﬁlled in automatically.  Contracting  26.35  by setting j = l  say  we obtain, since δkk = 3 when using  the summation convention,   cid:4 ijk cid:4 ijm = 3δkm − δkm = 2δkm,  and by contracting once more, setting k = m, we further ﬁnd that   cid:4 ijk cid:4 ijk = 6.   26.36   26.9 Isotropic tensors  It will have been noticed that, unlike most of the tensors discussed  except for scalars , δij and  cid:4 ijk have the property that all their components have values that are the same whatever rotation of axes is made, i.e. the component values  944   26.9 ISOTROPIC TENSORS  are independent of the transformation Lij. Speciﬁcally, δ11 has the value 1 in all coordinate frames, whereas for a general second-order tensor T all we know  cid:7   cid:7  is that if T11 = f11 x1, x2, x3  then T 3 . Tensors with the former 1, x property are called isotropic  or invariant  tensors.   cid:7  11 = f11 x   cid:7  2, x  It is important to know the most general form that an isotropic tensor can take, since the description of the physical properties, e.g. the conductivity, magnetic susceptibility or tensile strength, of an isotropic medium  i.e. a medium having the same properties whichever way it is orientated  involves an isotropic tensor. In the previous section it was shown that δij and  cid:4 ijk are second- and third-order isotropic tensors; we will now show that, to within a scalar multiple, they are the only such isotropic tensors.  Let us begin with isotropic second-order tensors. Suppose Tij is an isotropic  tensor; then, by deﬁnition, for any rotation of the axes we must have that  Tij = T   cid:7  ij = LikLjlTkl   26.37   for each of the nine components.  First consider a rotation of the axes by 2π 3 about the  1, 1, 1  direction; this  cid:7  1 respectively. For this rotation L13 = 1,  cid:7  11 = T33.  takes Ox1, Ox2, Ox3 into Ox L21 = 1, L32 = 1 and all other Lij = 0. This requires that T11 = T Similarly T12 = T   cid:7  12 = T31. Continuing in this way, we ﬁnd:   cid:7  3, Ox   cid:7  2, Ox   a  T11 = T22 = T33;   b  T12 = T23 = T31;   c  T21 = T32 = T13.  T13 =  −1  × 1 × T23; T23 = 1 × 1 × T13.  Next, consider a rotation of the axes  from their original position  by π 2  about the Ox3-axis. In this case L12 = −1, L21 = 1, L33 = 1 and all other Lij = 0.  Amongst other relationships, we must have from  26.37  that:  Hence T13 = T23 = 0 and therefore, by parts  b  and  c  above, each element Tij = 0 except for T11, T22 and T33, which are all the same. This shows that Tij = λδij.  cid:1 Show that λ cid:4 ijk is the only isotropic third-order Cartesian tensor.  The general line of attack is as above and so only a minimum of explanation will be given.   cid:7  ijk = LilLjmLknTlmn   in all, there are 27 elements .  Rotate about the  1, 1, 1  direction: this is equivalent to making subscript permutations  Tijk = T 1 → 2 → 3 → 1. We ﬁnd   a  T111 = T222 = T333,  b  T112 = T223 = T331  c  T123 = T231 = T312   and two similar sets ,  and a set involving odd permutations of 1, 2, 3 .  945   TENSORS  Rotate by π 2 about the Ox3-axis: L12 = −1, L21 = 1, L33 = 1, the other Lij = 0.  d  T111 =  −1  ×  −1  ×  −1  × T222 = −T222,  e  T112 =  −1  ×  −1  × 1 × T221,  f  T221 = 1 × 1 ×  −1  × T112,  g  T123 =  −1  × 1 × 1 × T213.  Relations  a  and  d  show that elements with all subscripts the same are zero. Relations  e ,  f  and  b  show that all elements with repeated subscripts are zero. Relations  g  and   c  show that T123 = T231 = T312 = −T213 = −T321 = −T132.  In total, Tijk diﬀers from  cid:4 ijk by at most a scalar factor, but since  cid:4 ijk  and hence λ cid:4 ijk  has already been shown to be an isotropic tensor, Tijk must be the most general third-order isotropic Cartesian tensor.  cid:2   Using exactly the same procedures as those employed for δij and  cid:4 ijk, it may be shown that the only isotropic ﬁrst-order tensor is the trivial one with all elements zero.  26.10 Improper rotations and pseudotensors  So far we have considered rigid rotations of the coordinate axes described by  an orthogonal matrix L with L = +1,  26.4 . Strictly speaking such transfor-  mations are called proper rotations. We now broaden our discussion to include transformations that are still described by an orthogonal matrix L but for which L = −1; these are called improper rotations.  This kind of transformation can always be considered as an inversion of the  coordinate axes through the origin represented by the equation  i = −xi,  cid:7   x   26.38   combined with a proper rotation. The transformation may be looked upon alternatively as one that changes an initially right-handed coordinate system into a left-handed one; any prior or subsequent proper rotation will not change this  state of aﬀairs. The most obvious example of a transformation with L = −1 is the matrix corresponding to  26.38  itself; in this case Lij = −δij.  As we have emphasised in earlier chapters, any real physical vector v may be considered as a geometrical object  i.e. an arrow in space , which can be referred to independently of any coordinate system and whose direction and magnitude cannot be altered merely by describing it in terms of a diﬀerent coordinate system.  cid:7  i = Lijvj under all rotations  proper and Thus the components of v transform as v improper .  We can deﬁne another type of object, however, whose components may also  cid:7  i = Lijvj under proper i = −Lijvj  note the minus sign  under improper rotations. In  cid:7   be labelled by a single subscript but which transforms as v rotations and as v this case, the vi are not strictly the components of a true ﬁrst-order Cartesian tensor but instead are said to form the components of a ﬁrst-order Cartesian pseudotensor or pseudovector.  946   26.10 IMPROPER ROTATIONS AND PSEUDOTENSORS  v  p  x3  O  x2  x1   cid:7  1  x   cid:7  2  x   cid:7  v  O   cid:7   p   cid:7  3  x  Figure 26.2 The behaviour of a vector v and a pseudovector p under a reﬂection through the origin of the coordinate system x1, x2, x3 giving the new system x   cid:7  1, x   cid:7  2, x   cid:7  3.  It is important to realise that a pseudovector  as its name suggests  is not a geometrical object in the usual sense. In particular, it should not be considered as a real physical arrow in space, since its direction is reversed by an improper transformation of the coordinate axes  such as an inversion through the origin . This is illustrated in ﬁgure 26.2, in which the pseudovector p is shown as a broken line to indicate that it is not a real physical vector.  Corresponding to vectors and pseudovectors, zeroth-order objects may be divided into scalars and pseudoscalars – the latter being invariant under rotation but changing sign on reﬂection.  We may also extend the notion of scalars and pseudoscalars, vectors and pseu- dovectors, to objects with two or more subscripts. For two subcripts, as deﬁned  cid:7  ij = LikLjlTkl un- previously, any quantity with components that transform as T der all rotations  proper and improper  is called a second-order Cartesian tensor. ij = −LikLjlTkl under  cid:7  If, however, T improper ones  which include reﬂections , then the Tij are the components of a second-order Cartesian pseudotensor. In general the components of Cartesian pseudotensors of arbitary order transform as   cid:7  ij = LikLjlTkl under proper rotations but T  ij···k = LLilLjm ··· LknTlm···n,  cid:7   T   26.39   where L is the determinant of the transformation matrix.  For example, from  26.29  we have that  L cid:4 ijk = LilLjmLkn cid:4 lmn,  947   but since L = ±1 we may rewrite this as  TENSORS   cid:4 ijk = LLilLjmLkn cid:4 lmn.  From this expression, we see that although  cid:4 ijk behaves as a tensor under proper rotations, as discussed in section 26.8, it should properly be regarded as a third- order Cartesian pseudotensor.  cid:1 If bj and ck are the components of vectors, show that the quantities ai =  cid:4 ijkbj ck form the components of a pseudovector.  In a new coordinate system we have  cid:7   cid:7  i =  cid:4  ijkb = LLilLjmLkn cid:4 lmnLjpbpLkqcq = LLil cid:4 lmnδmpδnqbpcq = LLil cid:4 lmnbmcn = LLilal,   cid:7   cid:7  jc k  a  from which we see immediately that the quantities ai form the components of a pseu- dovector.  cid:2   The above example is worth some further comment. If we denote the vec- tors with components bj and ck by b and c respectively then, as mentioned in section 26.8, the quantities ai =  cid:4 ijkbjck are the components of the real vector a = b × c, provided that we are using a right-handed Cartesian coordinate system.  cid:7   cid:7   cid:7   cid:7  i =  cid:4  However, in a coordinate system that is left-handed the quantitites a ijkb jc are not the components of the physical vector a = b × c, which has, instead, the components −a  cid:7  i. It is therefore important to note the handedness of a coordinate system before attempting to write in component form the vector relation a = b×c  k   which is true without reference to any coordinate system .  It is worth noting that, although pseudotensors can be useful mathematical objects, the description of the real physical world must usually be in terms of § For example, the temperature or density of a tensors  i.e. scalars, vectors, etc. . gas must be a scalar quantity  rather than a pseudoscalar , since its value does not change when the coordinate system used to describe it is inverted through the origin. Similarly, velocity, magnetic ﬁeld strength or angular momentum can only be described by a vector, and not by a pseudovector.  At this point, it may be useful to make a brief comment on the distinction between active and passive transformations of a physical system, as this diﬀerence often causes confusion. In this chapter, we are concerned solely with passive trans-  §  In fact the quantum-mechanical description of elementary particles, such as electrons, protons and neutrons, requires the introduction of a new kind of mathematical object called a spinor, which is not a scalar, vector, or more general tensor. The study of spinors, however, falls beyond the scope of this book.  948   26.11 DUAL TENSORS  formations, for which the physical system of interest is left unaltered, and only the coordinate system used to describe it is changed. In an active transformation, however, the system itself is altered.  As an example, let us consider a particle of mass m that is located at a position x relative to the origin O and hence has velocity ˙x. The angular momentum of  the particle about O is thus J = m x × ˙x . If we merely invert the Cartesian  coordinates used to describe this system through O, neither the magnitude nor direction of any these vectors will be changed, since they may be considered simply as arrows in space that are independent of the coordinates used to de- scribe them. If, however, we perform the analogous active transformation on the system, by inverting the position vector of the particle through O, then it is clear that the direction of particle’s velocity will also be reversed, since it is simply the time derivative of the position vector, but that the direction of its angular momentum vector remains unaltered. This suggests that vectors can be divided into two categories, as follows: polar vectors  such as position and velocity , which reverse direction under an active inversion of the physical sys- tem through the origin, and axial vectors  such as angular momentum , which remain unchanged. It should be emphasised that at no point in this discus- sion have we used the concept of a pseudovector to describe a real physical § quantity.  26.11 Dual tensors  Although pseudotensors are not themselves appropriate for the description of physical phenomena, they are sometimes needed; for example, we may use the pseudotensor  cid:4 ijk to associate with every antisymmetric second-order tensor Aij  in three dimensions  a pseudovector pi given by   26.40   pi is called the dual of Aij . Thus if we denote the antisymmetric tensor A by the matrix  pi = 1  2  cid:4 ijkAjk;   0  A12 −A31  −A12 A31 −A23  0  A23 0    A = [Aij ] =  then the components of its dual pseudovector are  p1, p2, p3  =  A23, A31, A12 .  §  The scalar product of a polar vector and an axial vector is a pseudoscalar. It was the experimental detection of the dependence of the angular distribution of electrons of  polar vector  momentum  pe emitted by polarised nuclei of  axial vector  spin JN upon the pseudoscalar quantity JN · pe that  established the existence of the non-conservation of parity in β-decay.  949   TENSORS   cid:1 Using  26.40 , show that Aij =  cid:4 ijkpk.  By contracting both sides of  26.40  with  cid:4 ijk, we ﬁnd  Using the identity  26.30  then gives  2  cid:4 ijk cid:4 klmAlm.   cid:4 ijkpk = 1 2  δilδjm − δimδjl Alm 2  Aij − Aji  = 1   cid:4 ijkpk = 1 = 1  where in the last line we use the fact that Aij = −Aji.  cid:2  By a simple extension, we may associate a dual pseudoscalar s with every totally antisymmetric third-rank tensor Aijk, i.e. one that is antisymmetric with respect to the interchange of every possible pair of subscripts; s is given by  2  Aij + Aij  = Aij,  s =   cid:4 ijkAijk.  1 3!   26.41   Since Aijk is a totally antisymmetric three-subscript quantity, we expect it to equal some multiple of  cid:4 ijk  since this is the only such quantity . In fact Aijk = s cid:4 ijk, as can be proved by substituting this expression into  26.41  and using  26.36 .  26.12 Physical applications of tensors  In this section some physical applications of tensors will be given. First-order tensors are familiar as vectors and so we will concentrate on second-order tensors, starting with an example taken from mechanics.  Consider a collection of rigidly connected point particles of which the αth, which has mass m α  and is positioned at r α  with respect to an origin O, is typical. Suppose that the rigid assembly is rotating about an axis through O with angular velocity ω.  The angular momentum J about O of the assembly is given by   cid:4    cid:5  r α  × p α    cid:6   .  J =  But p α  = m α ˙r α  and ˙r α  = ω × r α , for any α, and so in subscript form the  α  components of J are given by  Ji =  m α  cid:4 ijkx α   j ˙x α   k  m  j  cid:4 klmωlx α   m α  cid:4 ijkx α   cid:22  cid:5  m α  δilδjm − δimδjl x α  δil − x α    cid:6 2  m α   r α   i x α   l   cid:23   j x α   m ωl  ωl ≡ Iilωl,   26.42   α   cid:4   cid:4   cid:4   cid:4   α  α  α  =  =  =  where Iil is a symmetric second-order Cartesian tensor  by the quotient rule, see  950   26.12 PHYSICAL APPLICATIONS OF TENSORS  section 26.7, since J and ω are vectors . The tensor is called the inertia tensor at O of the assembly and depends only on the distribution of masses in the assembly and not upon the direction or magnitude of ω.  A more realistic situation obtains if a continuous rigid body is considered. In this case, m α  must be replaced everywhere by ρ r  dx dy dz and all summations by integrations over the volume of the body. Written out in full in Cartesians, the inertia tensor for a continuous body would have the form   cid:1     − cid:1  − cid:1    cid:1   − cid:1  − cid:1   I = [Iij ] =   y2 + z2 ρ dV  xyρ dV xzρ dV  xyρ dV   z2 + x2 ρ dV  xzρ dV yzρ dV  yzρ dV   x2 + y2 ρ dV  − cid:1  − cid:1    cid:1    ,  where ρ = ρ x, y, z  is the mass distribution and dV stands for dx dy dz; the integrals are to be taken over the whole body. The diagonal elements of this tensor are called the moments of inertia and the oﬀ-diagonal elements without the minus signs are known as the products of inertia.  cid:1 Show that the kinetic energy of the rotating system is given by T = 1  2 Ijlωj ωl.  By an argument parallel to that already made for J, the kinetic energy is given by   cid:5  ˙r α  · ˙r α    cid:6   T = 1 2  m α   α   cid:4   cid:4   cid:4   cid:4   α  α  α  = 1 2  = 1 2  = 1 2  m  m α  cid:4 ijkωj x α  k  cid:4 ilmωlx α   cid:22  m α  δjlδkm − δjmδkl x α  2 − x α   m α    cid:5    cid:6   r α   δjl  j x α   l   cid:23   k x α   m ωj ωl  ωj ωl  = 1  2 Ijlωj ωl.  Alternatively, since Jj = Ijlωl we may write the kinetic energy of the rotating system as T = 1  2 Jj ωj .  cid:2   The above example shows that the kinetic energy of the rotating body can be expressed as a scalar obtained by twice contracting ω with the inertia tensor. It also shows that the moment of inertia of the body about a line given by the unit vector ˆn is Ijl ˆnj ˆnl  or ˆnTI ˆn in matrix form . Since I  ≡ Ijl  is a real symmetric second-order tensor, it has associated with it  three mutually perpendicular directions that are its principal axes and have the following properties  proved in chapter 8 :   i  with each axis is associated a principal moment of inertia λµ, µ = 1, 2, 3;  ii  when the rotation of the body is about one of these axes, the angular  velocity and the angular momentum are parallel and given by  i.e. ω is an eigenvector of I with eigenvalue λµ;  J = Iω = λµω,  951   TENSORS  Mi = χij Hj,  ji = σijEj.   iii  referred to these axes as coordinate axes, the inertia tensor is diagonal  with diagonal entries λ1, λ2, λ3.  Two further examples of physical quantities represented by second-order tensors are magnetic susceptibility and electrical conductivity. In the ﬁrst case we have  in standard notation   and in the second case   26.43    26.44   Here M is the magnetic moment per unit volume and j the current density  current per unit perpendicular area . In both cases we have on the left-hand side a vector and on the right-hand side the contraction of a set of quantities with another vector. Each set of quantities must therefore form the components of a second-order tensor.  For isotropic media M ∝ H and j ∝ E, but for anisotropic materials such as  crystals the susceptibility and conductivity may be diﬀerent along diﬀerent crystal axes, making χij and σij general second-order tensors, although they are usually symmetric.  cid:1 The electrical conductivity σ in a crystal is measured by an observer to have components as shown:   1  √  2 0   .  √ 2 3 1  0 1 1  [σij] =   26.45   Show that there is one direction in the crystal along which no current can ﬂow. Does the current ﬂow equally easily in the two perpendicular directions?  The current density in the crystal is given by ji = σijEj , where σij, relative to the observer’s coordinate system, is given by  26.45 . Since [σij] is a symmetric matrix, it possesses three mutually perpendicular eigenvectors  or principal axes  with respect to which the conductivity tensor is diagonal, with diagonal entries λ1, λ2, λ3, the eigenvalues of [σij].  As discussed in chapter 8, the eigenvalues of [σij] are given by σ − λI = 0. Thus we  require   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  1 − λ  √ 2 0  √ 3 − λ  2  1  0 1  1 − λ   1 − λ [ 3 − λ  1 − λ  − 1] − 2 1 − λ  = 0.  cid:7  ij given by  cid:7  ij] =   4  [σ  0 1 0  0 0 0  0 0   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  = 0,  .  952  from which we ﬁnd  This simpliﬁes to give λ = 0, 1, 4 so that, with respect to its principal axes, the conductivity tensor has components σ  Since j ﬂow and along the two perpendicular directions the current ﬂows are not equal.  cid:2    cid:7  j, we see immediately that along one of the principal axes there is no current   cid:7  i = σ   cid:7  ij E   26.12 PHYSICAL APPLICATIONS OF TENSORS  We can extend the idea of a second-order tensor that relates two vectors to a situation where two physical second-order tensors are related by a fourth-order tensor. The most common occurrence of such relationships is in the theory of elasticity. This is not the place to give a detailed account of elasticity theory, but suﬃce it to say that the local deformation of an elastic body at any interior point P can be described by a second-order symmetric tensor eij called the strain tensor. It is given by   cid:8    cid:7   eij =  1 2  ∂ui ∂xj  +  ∂uj ∂xi  ,  where u is the displacement vector describing the strain of a small volume element whose unstrained position relative to the origin is x. Similarly we can describe the stress in the body at P by the second-order symmetric stress tensor pij; the quantity pij is the xj-component of the stress vector acting across a plane through P whose normal lies in the xi-direction. A generalisation of Hooke’s law then relates the stress and strain tensors by  pij = cijklekl   26.46   where cijkl is a fourth-order Cartesian tensor.  cid:1 Assuming that the most general fourth-order isotropic tensor is  cijkl = λδijδkl + ηδikδjl + νδilδjk,   26.47   ﬁnd the form of  26.46  for an isotropic medium having Young’s modulus E and Poisson’s ratio σ.  For an isotropic medium we must have an isotropic tensor for cijkl, and so we assume the form  26.47 . Substituting this into  26.46  yields  pij = λδijekk + ηeij + νeji.  But eij is symmetric, and if we write η + ν = 2µ, then this takes the form  pij = λekkδij + 2µeij ,  in which λ and µ are known as Lam´e constants. It will be noted that if eij = 0 for i  cid:3 = j Then denoting ekk  summed over k  by θ we have, in addition to eij = 0 for i  cid:3 = j, the three  then the same is true of pij , i.e. the principal axes of the stress and strain tensors coincide. Now consider a simple tension in the x1-direction, i.e. p11 = S but all other pij = 0.  equations  Adding them gives  Substituting for θ from this into the ﬁrst of the three, and recalling that Young’s modulus is deﬁned by S = Ee11, gives E as   26.48   S = λθ + 2µe11, 0 = λθ + 2µe22, 0 = λθ + 2µe33.  S = θ 3λ + 2µ .  E =  µ 3λ + 2µ   .  λ + µ  953   Further, Poisson’s ratio is deﬁned as σ = −e22 e11  or −e33 e11  and is thus   cid:7    cid:8   TENSORS   cid:7    cid:8  cid:7    cid:8   σ =  1 e11  λθ 2µ  =  1 e11  λ 2µ  Ee11  =  λ  .  3λ + 2µ  2 λ + µ    26.49   Solving  26.48  and  26.49  for λ and µ gives ﬁnally  pij =  σE   1 + σ  1 − 2σ   ekkδij +  E  eij .  cid:2    1 + σ   26.13 Integral theorems for tensors  0  0  0  S  S  S  In chapter 11, we discussed various integral theorems involving vector and scalar ﬁelds. Most notably, we considered the divergence theorem, which states that, for any vector ﬁeld a,   cid:21   ∇ · a dV =  a · ˆn dS ,  where S is the surface enclosing the volume V and ˆn is the outward-pointing unit normal to S at each point.  Writing  26.50  in subscript notation, we have  V   cid:21    26.50    26.51   ∂ak ∂xk  V  dV =  ak ˆnk dS .  Although we shall not prove it rigorously,  26.51  can be extended in an obvious manner to relate integrals of tensor ﬁelds, rather than just vector ﬁelds, over volumes and surfaces, with the result   cid:21   ∂Tij···k···m  V  ∂xk  dV =  Tij···k···m ˆnk dS .  This form of the divergence theorem for general tensors can be very useful in vector calculus manipulations.   cid:1   cid:1 A vector ﬁeld a satisﬁes ∇ · a = 0 inside some volume V and a · ˆn = 0 on the bound-  ary surface S . By considering the divergence theorem applied to Tij = xiaj, show that V a dV = 0.  Applying the divergence theorem to Tij = xiaj we ﬁnd  since aj ˆnj = 0. By expanding the volume integral we obtain  ∂Tij ∂xj  dV =  ∂ xiaj    V  ∂xj  dV =  xiaj ˆnj dS = 0,  ∂ xiaj   V  ∂xj  dV =  aj dV +  xi  ∂aj ∂xj  V  dV  0  S   cid:21    cid:21   V   cid:21   where in going from the ﬁrst to the second line we used ∂xi ∂xj = δij and ∂aj ∂xj = 0.  cid:2    cid:21    cid:21   cid:21   cid:21   V  V  V  =  =  ∂xi ∂xj  δij aj dV  ai dV = 0,  954   The other integral theorems discussed in chapter 11 can be extended in a similar way. For example, written in tensor notation Stokes’ theorem states that, for a vector ﬁeld ai,   cid:21   For a general tensor ﬁeld this has the straightforward extension  26.14 NON-CARTESIAN COORDINATES  0 0  C  C  ∂ak ∂xj   cid:4 ijk  S  ˆni dS =  ak dxk.   cid:21   ∂Tlm···k···n   cid:4 ijk  S  ∂xj  ˆni dS =  Tlm···k···n dxk.  26.14 Non-Cartesian coordinates  So far we have restricted our attention to the study of tensors when they are described in terms of Cartesian coordinates and the axes of coordinates are rigidly rotated, sometimes together with an inversion of axes through the origin. In the remainder of this chapter we shall extend the concepts discussed in the previous sections by considering arbitrary coordinate transformations from one general coordinate system to another. Although this generalisation brings with it several complications, we shall ﬁnd that many of the properties of Cartesian tensors are still valid for more general tensors. Before considering general coordinate transformations, however, we begin by reminding ourselves of some properties of general curvilinear coordinates, as discussed in chapter 10.  The position of an arbitrary point P in space may be expressed in terms of the three curvilinear coordinates u1, u2, u3. We saw in chapter 10 that if r u1, u2, u3  is the position vector of the point P then at P there exist two sets of basis vectors  ei =  ∂r ∂ui  and   cid:2 i = ∇ui,   26.52    26.53   where i = 1, 2, 3. In general, the vectors in each set neither are of unit length nor form an orthogonal basis. However, the sets ei and  cid:2 i are reciprocal systems of vectors and so  ei ·  cid:2 j = δij .  In the context of general tensor analysis, it is more usual to denote the second set of vectors  cid:2 i in  26.52  by ei, the index being placed as a superscript to distinguish it from the  diﬀerent  vector ei, which is a member of the ﬁrst set in  26.52 . Although this positioning of the index may seem odd  not least because of the possibility of confusion with powers  it forms part of a slight modiﬁcation to the summation convention that we will adopt for the remainder of this chapter. This is as follows: any lower-case alphabetic index that appears exactly twice in any term of an expression, once as a subscript and once as a superscript, is to be summed over all the values that an index in that position can take  unless the  955   TENSORS  ei · e j = δj i .  contrary is speciﬁcally stated . All other aspects of the summation convention remain unchanged.  With the introduction of superscripts, the reciprocity relation  26.53  should be rewritten so that both sides of  26.54  have one subscript and one superscript, i.e. as  The alternative form of the Kronecker delta is deﬁned in a similar way to previously, i.e. it equals unity if i = j and is zero otherwise.  For similar reasons it is usual to denote the curvilinear coordinates themselves  by u1, u2, u3, with the index raised, so that  ei =  ∂r ∂ui  and  ei = ∇ui.  From the ﬁrst equality we see that we may consider a superscript that appears in the denominator of a partial derivative as a subscript.  Given the two bases ei and ei, we may write a general vector a equally well in  terms of either basis as follows:   26.54    26.55   a = a1e1 + a2e2 + a3e3 = aiei; a = a1e1 + a2e2 + a3e3 = aiei.  The ai are called the contravariant components of the vector a and the ai the covariant components, the position of the index  either as a subscript or superscript  serving to distinguish between them. Similarly, we may call the ei the covariant basis vectors and the ei the contravariant ones.  cid:1 Show that the contravariant and covariant components of a vector a are given by ai = a·ei and ai = a · ei respectively.  For the contravariant components, we ﬁnd  where we have used the reciprocity relation  26.54 . Similarly, for the covariant components,  a · ei = a jej · ei = a jδi  j = ai,  a · ei = aj e j · ei = ajδj  i = ai.  cid:2   The reason that the notion of contravariant and covariant components of a vector  and the resulting superscript notation  was not introduced earlier is that for Cartesian coordinate systems the two sets of basis vectors ei and ei are identical and, hence, so are the components of a vector with respect to either basis. Thus, for Cartesian coordinates, we may speak simply of the components of the vector and there is no need to diﬀerentiate between contravariance and covariance, or to introduce superscripts to make a distinction between them.  If we consider the components of higher-order tensors in non-Cartesian co- ordinates, there are even more possibilities. As an example, let us consider a  956   26.15 THE METRIC TENSOR  second-order tensor T. Using the outer product notation in  26.23 , we may write T in three diﬀerent ways:  T = T ij ei ⊗ ej = T i  jei ⊗ e j = Tij ei ⊗ e j,  where T ij , T i j and Tij are called the contravariant, mixed and covariant com- ponents of T respectively. It is important to remember that these three sets of quantities form the components of the same tensor T but refer to diﬀerent  tensor  bases made up from the basis vectors of the coordinate system. Again, if we are using Cartesian coordinates then all three sets of components are identical.  We may generalise the above equation to higher-order tensors. Components carrying only superscripts or only subscripts are referred to as the contravariant and covariant components respectively; all others are called mixed components.  26.15 The metric tensor  gij = ei · ej,  Any particular curvilinear coordinate system is completely characterised at each point in space by the nine quantities  which, as we will show, are the covariant components of a symmetric second-order tensor g called the metric tensor.  Since an inﬁnitesimal vector displacement can be written as dr = duiei, we ﬁnd that the square of the inﬁnitesimal arc length  ds 2 can be written in terms of the metric tensor as   ds 2 = dr · dr = duiei · du jej = gij duidu j.  It may further be shown that the volume element dV is given by  √ g du1 du2 du3,  dV =   26.56    26.57    26.58   where g is the determinant of the matrix [ gij], which has the covariant components of the metric tensor as its elements.  If we compare equations  26.57  and  26.58  with the analogous ones in section 10.10 then we see that in the special case where the coordinate system is orthogonal   so that ei · ej = 0 for i  cid:3 = j  the metric tensor can be written in terms of the  coordinate-system scale factors hi, i = 1, 2, 3 as  gij =  h2 i 0  i = j,  i  cid:3 = j.    957  Its determinant is then given by g = h2  1h2  2h2 3.   TENSORS   cid:1 Calculate the elements gij of the metric tensor for cylindrical polar coordinates. Hence ﬁnd the square of the inﬁnitesimal arc length  ds 2 and the volume dV for this coordinate system.  As discussed in section 10.9, in cylindrical polar coordinates  u1, u2, u3  =  ρ, φ, z  and so the position vector r of any point P may be written  From this we obtain the  covariant  basis vectors:  r = ρ cos φ i + ρ sin φ j + z k.  Thus the components of the metric tensor [gij] = [ei · ej ] are found to be  ∂r ∂ρ ∂r ∂φ ∂r ∂z  e1 =  = cos φ i + sin φ j;  e2 =  = −ρ sin φ i + ρ cos φ j;  e3 =  = k.  G = [gij] =   1  0 0   ,  0 ρ2 0  0 0 1  from which we see that, as expected for an orthogonal coordinate system, the metric tensor is diagonal, the diagonal elements being equal to the squares of the scale factors of the coordinate system.  From  26.57 , the square of the inﬁnitesimal arc length in this coordinate system is given   ds 2 = gij dui du j =  dρ 2 + ρ2 dφ 2 +  dz 2,  and, using  26.58 , the volume element is found to be  √  These expressions are identical to those derived in section 10.9.  cid:2   dV =  g du1 du2 du3 = ρ dρ dφ dz.  by  tensor:  We may also express the scalar product of two vectors in terms of the metric  a · b = aiei · b j ej = gij aib j ,  a · b = aiei · bje j = gijaibj,  where we have used the contravariant components of the two vectors. Similarly, using the covariant components, we can write the same scalar product as  where we have deﬁned the nine quantities gij = ei·e j. As we shall show, they form  the contravariant components of the metric tensor g and are, in general, diﬀerent from the quantities gij . Finally, we could express the scalar product in terms of the contravariant components of one vector and the covariant components of the other,  a · b = aiei · b j ej = aib jδi  j = aibi,   26.63   958   26.59    26.60    26.61    26.62    26.15 THE METRIC TENSOR  where we have used the reciprocity relation  26.54 . Similarly, we could write  a · b = aiei · bje j = aibjδj  i = aibi.   26.64   By comparing the four alternative expressions  26.61 – 26.64  for the scalar product of two vectors we can deduce one of the most useful properties of the quantities gij and gij. Since gij aib j = aibi holds for any arbitrary vector components ai, it follows that  gijb j = bi,  gij bj = bi,  which illustrates the fact that the covariant components gij of the metric tensor can be used to lower an index. In other words, it provides a means of obtaining the covariant components of a vector from its contravariant components. By a similar argument, we have  so that the contravariant components gij can be used to perform the reverse operation of raising an index.  It is straightforward to show that the contravariant and covariant basis vectors,  ei and ei respectively, are related in the same way as other vectors, i.e. by  ei = gijej  and  ei = gije j.  We also note that, since ei and ei are reciprocal systems of vectors in three- dimensional space  see chapter 7 , we may write  ej × ek ei ·  ej × ek   ,  ei =  √  for the combination of subscripts i, j, k = 1, 2, 3 and its cyclic permutations. A similar expression holds for ei in terms of the ei-basis. Moreover, it may be shown that e1 ·  e2 × e3  =  cid:1 Show that the matrix [gij] is the inverse of the matrix [gij]. Hence calculate the con- travariant components gij of the metric tensor in cylindrical polar coordinates.  g.  Using the index-lowering and index-raising properties of gij and gij on an arbitrary vector a, we ﬁnd  But, since a is arbitrary, we must have  δi kak = ai = gijaj = gij gjkak.  gij gjk = δi k.   26.65   Denoting the matrix [gij ] by G and [gij] by ˆG, equation  26.65  can be written in matrix form as ˆGG = I, where I is the unit matrix. Hence G and ˆG are inverse matrices of each other.  959   Thus, by inverting the matrix G in  26.60 , we ﬁnd that the elements gij are given in  cylindrical polar coordinates by  TENSORS   1  0 0   .  cid:2   1 ρ2  0  0  0 0 1  ˆG = [gij ] =  So far we have not considered the components of the metric tensor gi  j with one subscript and one superscript. By analogy with  26.56 , these mixed components are given by  j = ei · ej = δj gi i ,  and so the components of gi consider the δi  j to be the mixed components of the metric tensor g.  j are identical to those of δi  j. We may therefore  26.16 General coordinate transformations and tensors  We now discuss the concept of general transformations from one coordinate  cid:7 3. We can describe the coordinate transform system, u1, u2, u3, to another, u using the three equations   cid:7 1, u   cid:7 2, u   cid:7 i = u   cid:7 i u1, u2, u3 ,  u   cid:7 i can be arbitrary functions of the old for i = 1, 2, 3, in which the new coordinates u ones ui rather than just represent linear orthogonal transformations  rotations  of the coordinate axes. We shall assume also that the transformation can be inverted, so that we can write the old coordinates in terms of the new ones as  As an example, we may consider the transformation from spherical polar to  Cartesian coordinates, given by  which is clearly not a linear transformation.  The two sets of basis vectors in the new coordinate system, u   cid:7 1, u   cid:7 2, u   cid:7 3, are given  as in  26.55  by   cid:7  i =  e  ∂r  cid:7 i ∂u  and   cid:7 i = ∇u  cid:7 i.  e   26.66   Considering the ﬁrst set, we have from the chain rule that   cid:7 1 ui = ui u   cid:7 2 , u   cid:7 3 ,  , u  x = r sin θ cos φ,  y = r sin θ sin φ,  z = r cos θ,  ∂r ∂u j  =   cid:7 i ∂u ∂u j  ∂r  cid:7 i ,  ∂u  960   26.16 GENERAL COORDINATE TRANSFORMATIONS AND TENSORS  so that the basis vectors in the old and new coordinate systems are related by  Now, since we can write any arbitrary vector a in terms of either basis as  it follows that the contravariant components of a vector must transform as   cid:7 ie a = a   cid:7 i  cid:7  i = a jej = a j ∂u ∂u j   cid:7  i,  e   cid:7 i ∂u ∂u j   cid:7  i.  e  ej =   cid:7 i =  a   cid:7 i ∂u ∂u j a j.  In fact, we use this relation as the deﬁning property for a set of quantities ai to form the contravariant components of a vector.   cid:7 i in the two  cid:1 Find an expression analogous to  26.67  relating the basis vectors ei and e coordinate systems. Hence deduce the way in which the covariant components of a vector change under a coordinate transformation.  If we consider the second set of basis vectors in  26.66 , e rule that   cid:7 i = ∇u   cid:7 i, we have from the chain  and similarly for ∂u j ∂y and ∂u j ∂z. So the basis vectors in the old and new coordinate systems are related by  For any arbitrary vector a,  and so the covariant components of a vector must transform as  Analogously to the contravariant case  26.68 , we take this result as the deﬁning property of the covariant components of a vector.  cid:2   We may compare the transformation laws  26.68  and  26.70  with those for a ﬁrst-order Cartesian tensor under a rigid rotation of axes. Let us consider a rotation of Cartesian axes xi through an angle θ about the 3-axis to a new  cid:7 i, i = 1, 2, 3, as given by  26.7  and the inverse transformation  26.8 . It is set x straightforward to show that   cid:7 i  ∂u j ∂x  =  ∂u j  cid:7 i ∂u  ∂u ∂x  e j =   cid:7 i.  cid:7 i e  ∂u j ∂u  a = a   cid:7   cid:7 i = aje j = aj ie   cid:7 i  cid:7 i e  ∂u j ∂u   cid:7  i =  a  ∂u j ∂u   cid:7 i aj.  ∂x j  cid:7 i ∂x  =   cid:7 i  ∂x ∂x j  961  = Lij,   26.67    26.68    26.69    26.70    where the elements Lij are given by  TENSORS   cos θ  − sin θ  0  L =   .  sin θ cos θ  0  0 0 1  Thus  26.68  and  26.70  agree with our earlier deﬁnition in the special case of a rigid rotation of Cartesian axes.  Following on from  26.68  and  26.70 , we proceed in a similar way to de- ﬁne general tensors of higher rank. For example, the contravariant, mixed and covariant components, respectively, of a second-order tensor must transform as follows:  contravariant components, T  mixed components,  covariant components,   cid:7 ij =  cid:7 i j =  T   cid:7   T  ij =   cid:7 i ∂u ∂uk  cid:7 i ∂u ∂uk ∂uk  cid:7 i ∂u   cid:7 j ∂u ∂ul T kl ; ∂ul  cid:7 j T k ∂u ∂ul ∂u   cid:7 j Tkl.  l ;  It is important to remember that these quantities form the components of the same tensor T but refer to diﬀerent tensor bases made up from the basis vectors of the diﬀerent coordinate systems. For example, in terms of the contravariant components we may write  T = T ij ei ⊗ ej = T   cid:7 ij e   cid:7   i  ⊗ e   cid:7  j.  We can clearly go on to deﬁne tensors of higher order, with arbitrary numbers of covariant  subscript  and contravariant  superscript  indices, by demanding that their components transform as follows:   cid:7  ij···k  T  lm···n =   cid:7 i ∂u ∂ua   cid:7 j ∂u ∂ub   cid:7 k ··· ∂u ∂uc  ∂ud  cid:7 l ∂u  ∂ue  cid:7 m ∂u  ··· ∂uf ∂u   cid:7 n T ab···c  de···f.   26.71   Using the revised summation convention described in section 26.14, the algebra of general tensors is completely analogous to that of the Cartesian tensors discussed earlier. For example, as with Cartesian coordinates, the Kronecker delta is a tensor provided it is written as the mixed tensor δi  j since   cid:7 i j =  δ   cid:7 i ∂u ∂uk  ∂ul ∂u   cid:7 j δk  l =   cid:7 i ∂u ∂uk  ∂uk  cid:7 j ∂u   cid:7 i ∂u  cid:7 j ∂u  =  = δi j,  where we have used the chain rule to justify the third equality. This also shows that δi j can be j considered as the mixed components of the metric tensor g.  is isotropic. As discussed at the end of section 26.15, the δi  962   26.17 RELATIVE TENSORS   cid:1 Show that the quantities gij = ei · ej form the covariant components of a second-order  tensor.  In the new  primed  coordinate system we have · e  cid:7   cid:7   cid:7  ij = e j , i  g  but using  26.67  for the inverse transformation, we have   cid:7  and similarly for e j . Thus we may write   cid:7  i = e  ∂uk ∂u   cid:7 i ek,   cid:7  ij =  g  ∂uk  cid:7 i ∂u  ∂ul ∂u   cid:7 j ek · el =  ∂uk  cid:7 i ∂u  ∂ul ∂u   cid:7 j gkl,  which shows that the gij are indeed the covariant components of a second-order tensor  the metric tensor g .  cid:2   A similar argument to that used in the above example shows that the quantities gij form the contravariant components of a second-order tensor which transforms according to   cid:7 ij =  g   cid:7 i ∂u ∂uk   cid:7 j ∂u ∂ul gkl.  In the previous section we discussed the use of the components gij and gij in the raising and lowering of indices in contravariant and covariant vectors. This can be extended to tensors of arbitrary rank. In general, contraction of a tensor with gij will convert the contracted index from being contravariant  superscript  to covariant  subscript , i.e. it is lowered. This can be repeated for as many indices are required. For example,  Similarly contraction with gij raises an index, i.e.  Tij = gikT k  j = gikgjlT kl.  T ij = gikT j  k = gikgjlTkl.   26.72    26.73   That  26.72  and  26.73  are mutually consistent may be shown by using the fact that gikgkj = δi j.  26.17 Relative tensors  In section 26.10 we introduced the concept of pseudotensors in the context of the rotation  proper or improper  of a set of Cartesian axes. Generalising to arbitrary coordinate transformations leads to the notion of a relative tensor.  For an arbitrary coordinate transformation from one general coordinate system  963   TENSORS   cid:20  cid:20  cid:20  cid:20  ∂u  ∂u   cid:7    cid:20  cid:20  cid:20  cid:20  .  J =   cid:7 i, we may deﬁne the Jacobian of the transformation  see chapter 6   cid:7 i ∂u j]: this is usually denoted  ui to another u as the determinant of the transformation matrix [∂u by  Alternatively, we may interchange the primed and unprimed coordinates to   cid:7  = 1 J: unfortunately this also is often called the Jacobian of the  obtain ∂u ∂u  transformation.  Using the Jacobian J, we deﬁne a relative tensor of weight w as one whose  components transform as follows:  cid:7 k ··· ∂u ∂uc   cid:7 j ∂u ∂ub   cid:7 i ∂u ∂ua  lm···n =   cid:7  ij···k  T  ∂ud  cid:7 l ∂u  ∂ue  cid:7 m ∂u  ··· ∂uf ∂u   cid:7 n T ab···c  de···f   cid:20  cid:20  cid:20  cid:20  ∂u   cid:7  ∂u   cid:20  cid:20  cid:20  cid:20 w  .   26.74   Comparing this expression with  26.71 , we see that a true  or absolute  general  tensor may be considered as a relative tensor of weight w = 0. If w = −1, on the  other hand, the relative tensor is known as a general pseudotensor and if w = 1 as a tensor density.  It is worth comparing  26.74  with the deﬁnition  26.39  of a Cartesian pseu- dotensor. For the latter, we are concerned only with its behaviour under a rotation   proper or improper  of Cartesian axes, for which the Jacobian J = ±1. Thus, general relative tensors of weight w = −1 and w = 1 would both satisfy the  deﬁnition  26.39  of a Cartesian pseudotensor.  cid:1 If the gij are the covariant components of the metric tensor, show that the determinant g of the matrix [gij] is a relative scalar of weight w = 2.  The components gij transform as  Deﬁning the matrices U = [∂ui ∂u as   cid:7  ij], we may write this expression  = [g  Taking the determinant of both sides, we obtain   cid:7  ij =  ∂uk  cid:7 i ∂u  ∂ul ∂u  g   cid:7 j gkl.  cid:7 j], G = [gij] and G cid:7   G cid:7   = UTGU.   cid:20  cid:20  cid:20  cid:20  ∂u  ∂u   cid:7    cid:20  cid:20  cid:20  cid:20 2  g,   cid:7   g  = U2g =  which shows that g is a relative scalar of weight w = 2.  cid:2   From the discussion in section 26.8, it can be seen that  cid:4 ijk is a covariant relative tensor of weight −1. We may also deﬁne the contravariant tensor  cid:4 ijk, which is numerically equal to  cid:4 ijk but is a relative tensor of weight +1.  If two relative tensors have weights w1 and w2 respectively then, from  26.74 ,  964   26.18 DERIVATIVES OF BASIS VECTORS AND CHRISTOFFEL SYMBOLS  the outer product of the two tensors, or any contraction of them, is a relative tensor of weight w1 + w2. As a special case, we may use  cid:4 ijk and  cid:4 ijk to construct pseudovectors from antisymmetric tensors and vice versa, in an analogous way to that discussed in section 26.11.  For example, if the Aij are the contravariant components of an antisymmetric  tensor  w = 0  then  are the covariant components of a pseudovector  w = −1 , since  cid:4 ijk has weight w = −1. Similarly, we may show that  pi = 1  2  cid:4 ijkAjk  Aij =  cid:4 ijkpk.  26.18 Derivatives of basis vectors and Christoﬀel symbols  In Cartesian coordinates, the basis vectors ei are constant and so their derivatives with respect to the coordinates vanish. In a general coordinate system, however, the basis vectors ei and ei are functions of the coordinates. Therefore, in order that we may diﬀerentiate general tensors we must consider the derivatives of the basis vectors.  First consider the derivative ∂ei ∂u j. Since this is itself a vector, it can be written as a linear combination of the basis vectors ek, k = 1, 2, 3. If we introduce the symbol Γk  ij to denote the coeﬃcients in this combination, we have  The coeﬃcient Γk procity relation ei · ej = δi  ij is the kth component of the vector ∂ei ∂u j. Using the reci- j, these 27 numbers are given  at each point in space   by  Furthermore, by diﬀerentiating the reciprocity relation ei · ej = δi  j with respect it is straightforward to show that the  to the coordinates, and using  26.76 , derivatives of the contravariant basis vectors are given by   26.75    26.76    26.77   The symbol Γk  ij is called a Christoﬀel symbol  of the second kind , but, despite appearances to the contrary, these quantities do not form the components of a third-order tensor. It is clear from  26.76  that in Cartesian coordinates Γk ij = 0 for all values of the indices i, j and k.  ∂ei ∂u j  = Γk  ij ek.  Γk  ij = ek · ∂ei ∂u j .  = −Γi  kjek.  ∂ei ∂u j  965   TENSORS   cid:1 Using  26.76 , deduce the way in which the quantities Γk ij transform under a general coordinate transformation, and hence show that they do not form the components of a third-order tensor.  In a new coordinate system   cid:7 k  Γ  ij = e   cid:7   cid:7 k · ∂e  cid:7 j , ∂u  i  but from  26.69  and  26.67  respectively we have, on reversing primed and unprimed variables,  Therefore in the new coordinate system the quantities Γ   cid:7 k  Γ  ij =   cid:7 k = e   cid:7 k  ∂u ∂un  en   cid:7 k  ∂u ∂un  cid:7 k  ∂u ∂un  cid:7 k  ∂u ∂un  cid:7 k  ∂u ∂ul   cid:7  en · ∂  cid:7 j  ∂u  en ·  ∂2ul  cid:7 j ∂u ∂2ul  cid:7 j ∂u  ∂u  ∂u  =  =  =   cid:7 i el.   cid:7  ∂ul i = e ∂u  cid:7 k ij are given by   cid:7   and   cid:8   ∂ul ∂u   cid:7 i el   cid:8   ∂el  cid:7 j ∂u  ∂u  ∂2ul  cid:7 j ∂u  cid:7 i el +  cid:7 i en · el +  cid:7 k  ∂ul  cid:7 i ∂u  cid:7 k  ∂u ∂un   cid:7 i +  ∂u ∂un  ∂ul  cid:7 i ∂u  ∂um ∂u   cid:7 j Γn  lm,  ∂ul  cid:7 i ∂u  ∂um ∂u   cid:7 j en · ∂el  ∂um   26.78   l . From  26.78 , because of the presence of the ﬁrst term on the right-hand side, we conclude immediately that the Γk  where in the last line we have used  26.76  and the reciprocity relation en · el = δn ij do not form the components of a third-order tensor.  cid:2  In a given coordinate system, in principle we may calculate the Γk  ij using  26.76 . In practice, however, it is often quicker to use an alternative expression, which we now derive, for the Christoﬀel symbol in terms of the metric tensor gij and its derivatives with respect to the coordinates. Firstly we note that the Christoﬀel symbol Γk  ij is symmetric with respect to  the interchange of its two subscripts i and j. This is easily shown: since  ∂ei ∂u j  =  ∂2r  ∂2r  =  =  ∂ej ∂ui ,  ∂ui∂u j  ∂u j∂ui it follows from  26.75  that Γk ij ek = Γk using the reciprocity relation ek · el = δl Γl ij = Γl ij we then use gij = ei · ej and consider the  jiek. Taking the scalar product with el and k gives immediately that  To obtain an expression for Γk  ji.  derivative  ∂gij ∂uk  =  · ej + ei · ∂ej ∂ei ∂uk ikel · ej + ei · Γl ikglj + Γl jkgil,  ∂uk  = Γl = Γl  jkel  966   26.79    26.18 DERIVATIVES OF BASIS VECTORS AND CHRISTOFFEL SYMBOLS  where we have used the deﬁnition  26.75 . By cyclically permuting the free indices i, j, k in  26.79 , we obtain two further equivalent relations,  and  we ﬁnd  ∂gjk ∂ui  ∂gki ∂u j  = Γl  jiglk + Γl  kigjl  = Γl  kjgli + Γl  ij gkl.   26.80    26.81   If we now add  26.80  and  26.81  together and subtract  26.79  from the result,  ∂gjk ∂ui  +  ∂gki ∂u j  − ∂gij ∂uk  = Γl  jiglk + Γl  kigjl + Γl  kjgli + Γl  ij gkl − Γl  ikglj − Γl  jkgil  = 2Γl  ij gkl,  where we have used the symmetry properties of both Γl ij and gij . Contracting both sides with gmk leads to the required expression for the Christoﬀel symbol in terms of the metric tensor and its derivatives, namely − ∂gij ∂uk  ∂gjk ∂ui  ∂gki ∂u j  ij = 1   26.82    cid:7    cid:8   2 gmk  Γm  +  .   cid:1 Calculate the Christoﬀel symbols Γm  ij for cylindrical polar coordinates.  We may use either  26.75  or  26.82  to calculate the Γm ij for this simple coordinate system. In cylindrical polar coordinates  u1, u2, u3  =  ρ, φ, z , the basis vectors ei are given by  26.59 . It is straightforward to show that the only derivatives of these vectors with respect to the coordinates that are non-zero are  ∂eρ ∂φ  =  eφ,  1 ρ  ∂eφ ∂ρ  =  eφ,  1 ρ  = −ρeρ.  ∂eφ ∂φ  Thus, from  26.75 , we have immediately that  Γ2  12 = Γ2  21 =  and  1 ρ  22 = −ρ.  Γ1   26.83   Alternatively, using  26.82  and the fact that g11 = 1, g22 = ρ2, g33 = 1 and the other components are zero, we see that the only three non-zero Christoﬀel symbols are indeed Γ2  22. These are given by  21 and Γ1  12 = Γ2  which agree with the expressions found directly from  26.75  and given in  26.83  .  cid:2   Γ2  Γ1  21 =  12 = Γ2 22 = − 1  2g11  1  2g22  ∂g22 ∂u1  =  ∂g22 ∂u1  = − 1  2  ∂ ∂ρ  1 2ρ2  ∂ ∂ρ   ρ2  =  ρ2  = −ρ,  1 ρ  ,  967   For Cartesian tensors we noted that the derivative of a scalar is a  covariant  vector. This is also true for general tensors, as may be shown by considering the diﬀerential of a scalar  TENSORS  26.19 Covariant diﬀerentiation  dφ =  ∂φ ∂ui dui.  Since the dui are the components of a contravariant vector and dφ is a scalar, we have by the quotient law, discussed in section 26.7, that the quantities ∂φ ∂ui must form the components of a covariant vector. As a second example, if the contravariant components in Cartesian coordinates of a vector v are vi, then the quantities ∂vi ∂x j form the components of a second-order tensor.  However, it is straightforward to show that in non-Cartesian coordinates diﬀer- entiation of the components of a general tensor, other than a scalar, with respect to the coordinates does not in general result in the components of another tensor.   cid:1 Show that, in general coordinates, the quantities ∂vi ∂u j do not form the components of a tensor.  We may show this directly by considering   cid:7    cid:8  cid:7   ∂vi ∂u j   cid:7 i  cid:7 j =  ∂v  ∂u  =   cid:31    cid:7 i  ∂u ∂ul  vl   cid:30    cid:7 i  ∂v ∂uk  ∂ ∂uk  cid:7 i  ∂u ∂ul  ∂uk  cid:7 j ∂u  ∂uk  cid:7 j ∂u  ∂uk  cid:7 j ∂u  =  =   cid:7 i  ∂vl ∂uk  +  ∂uk  cid:7 j ∂u  ∂2u ∂uk∂ul  vl.   26.84   The presence of the second term on the right-hand side of  26.84  shows that the ∂vi ∂x j do not form the components of a second-order tensor. This term arises because the  cid:7 i ∂u j] changes as the position in space at which it is evaluated ‘transformation matrix’ [∂u is changed. This is not true in Cartesian coordinates, for which the second term vanishes and ∂vi ∂x j is a second-order tensor.  cid:2   We may, however, use the Christoﬀel symbols discussed in the previous section to deﬁne a new covariant derivative of the components of a tensor that does result in the components of another tensor.  Let us ﬁrst consider the derivative of a vector v with respect to the coordinates.  Writing the vector in terms of its contravariant components v = viei, we ﬁnd  ∂v ∂u j  =  ∂vi ∂u j  ei + vi ∂ei ∂u j ,   26.85   where the second term arises because, in general, the basis vectors ei are not  968   26.19 COVARIANT DIFFERENTIATION  constant  this term vanishes in Cartesian coordinates . Using  26.75  we write  ∂v ∂u j  =  ∂vi ∂u j  ei + viΓk  ij ek.   cid:8    cid:7   ∂vi ∂u j  Since i and k are dummy indices in the last term on the right-hand side, we may interchange them to obtain  ∂v ∂u j  =  ∂vi ∂u j  ei + vkΓi  kjei =  + vkΓi  kj  ei.   26.86   The reason for the interchanging the dummy indices, as shown in  26.86 , is that we may now factor out ei. The quantity in parentheses is called the covariant derivative, for which the standard notation is  vi  ; j  ≡ ∂vi ∂u j  + Γi  kjvk,   26.87   the semicolon subscript denoting covariant diﬀerentiation. A similar short-hand notation also exists for the partial derivatives, a comma being used for these instead of a semicolon; for example, ∂vi ∂u j , j. In Cartesian coordinates all the Γi kj are zero, and so the covariant derivative reduces to the simple partial derivative ∂vi ∂u j.  is denoted by vi  Using the short-hand semicolon notation, the derivative of a vector may be  written in the very compact form  ∂v ∂u j  = vi  ; jei  and, by the quotient rule  section 26.7 , it is clear that the vi ; j are the  mixed  components of a second-order tensor. This may also be veriﬁed directly, using the transformation properties of ∂vi ∂u j and Γi kj given in  26.84  and  26.78  respectively.  In general, we may regard the vi ; j as the mixed components of a second- order tensor called the covariant derivative of v and denoted by ∇v. In Cartesian coordinates, the components of this tensor are just ∂vi ∂x j.   cid:1 Calculate vi  ; i in cylindrical polar coordinates.  Contracting  26.87  we obtain  Now from  26.83  we have  vi  ; i =  + Γi  kivk.  ∂vi ∂ui  Γi Γi Γi  1i = Γ1 2i = Γ1 3i = Γ1  11 + Γ2 21 + Γ2 31 + Γ2  12 + Γ3 22 + Γ3 32 + Γ3  13 = 1 ρ, 23 = 0, 33 = 0,  969   and so  TENSORS  vi  ; i =  +  ∂vφ ∂φ  +  ∂vρ ∂ρ 1 ρ  ∂ ∂ρ  ∂vz ∂z ∂vφ ∂φ  +  vρ  1 ρ ∂vz ∂z  =   ρvρ  +  +  .  This result is identical to the expression for the divergence of a vector ﬁeld in cylindrical polar coordinates given in section 10.9. This is discussed further in section 26.20.  cid:2   So far we have considered only the covariant derivative of the contravariant components vi of a vector. The corresponding result for the covariant components vi may be found in a similar way, by considering the derivative of v = viei and using  26.77  to obtain  vi; j =  ∂vi ∂u j  − Γk  ij vk.   26.88   Comparing the expressions  26.87  and  26.88  for the covariant derivative of the contravariant and covariant components of a vector respectively, we see that there are some similarities and some diﬀerences. It may help to remember that the index with respect to which the covariant derivative is taken  j in this case , is also the last subscript on the Christoﬀel symbol; the remaining indices can then be arranged in only one way without raising or lowering them. It only remains to note that for a covariant index  subscript  the Christoﬀel symbol carries a minus sign, whereas for a contravariant index  superscript  the sign is positive.  Following a similar procedure to that which led to equation  26.87 , we may  obtain expressions for the covariant derivatives of higher-order tensors.   cid:1 By considering the derivative of the second-order tensor T with respect to the coordinate uk, ﬁnd an expression for the covariant derivative T ij  ; k of its contravariant components.  Expressing T in terms of its contravariant components, we have  Using  26.75 , we can rewrite the derivatives of the basis vectors in terms of Christoﬀel symbols to obtain  Interchanging the dummy indices i and l in the second term and j and l in the third term on the right-hand side, this becomes  ∂T ∂uk  =  =  ∂ ∂uk ∂T ij ∂uk   T ijei ⊗ ej   ei ⊗ ej + T ij ∂ei  ∂uk  ⊗ ej + T ijei ⊗ ∂ej  .  ∂uk  ikel ⊗ ej + T ijei ⊗ Γl  jkel.  ∂T ∂uk  =  ∂T ij ∂uk  ei ⊗ ej + T ijΓl  cid:7    cid:8   ∂T ∂uk  =  ∂T ij ∂uk  + Γi  lkT lj + Γ j  lkT il  ei ⊗ ej ,  970   26.20 VECTOR OPERATORS IN TENSOR FORM  where the expression in parentheses is the required covariant derivative  T ij  ; k =  + Γi  lkT lj + Γ j  lkT il.  ∂T ij ∂uk   26.89   Using  26.89 , the derivative of the tensor T with respect to uk can now be written in terms of its contravariant components as  ; kei ⊗ ej .  cid:2   = T ij  ∂T ∂uk  Results similar to  26.89  may be obtained for the the covariant derivatives of the mixed and covariant components of a second-order tensor. Collecting these results together, we have  T ij ; k = T ij , k + Γi j, k + Γi T i j; k = T i Tij; k = Tij, k − Γl  lkT lj + Γ j − Γl lkT l ikTlj − Γl  lkT il, jkT i l, jkTil,  j  where we have used the comma notation for partial derivatives. The position of the indices in these expressions is very systematic: for each contravariant index  superscript  on the LHS we add a term on the RHS containing a Christoﬀel symbol with a plus sign, and for every covariant index  subscript  we add a corresponding term with a minus sign. This is extended straightforwardly to tensors with an arbitrary number of contravariant and covariant indices.  We note that the quantities T ij j; k and Tij; k are the components of the same third-order tensor ∇T with respect to diﬀerent tensor bases, i.e. j; kei ⊗ e j ⊗ ek = Tij; kei ⊗ e j ⊗ ek.  ; k, T i ; kei ⊗ ej ⊗ ek = T i  ∇T = T ij  We conclude this section by considering brieﬂy the covariant derivative of a scalar. The covariant derivative diﬀers from the simple partial derivative with respect to the coordinates only because the basis vectors of the coordinate system change with position in space  hence for Cartesian coordinates there is no diﬀerence . However, a scalar φ does not depend on the basis vectors at all and so its covariant derivative must be the same as its partial derivative, i.e.  φ; j =  = φ, j.  ∂φ ∂u j   26.90   26.20 Vector operators in tensor form  In section 10.10 we used vector calculus methods to ﬁnd expressions for vector diﬀerential operators, such as grad, div, curl and the Laplacian, in general orthog- onal curvilinear coordinates, taking cylindrical and spherical polars as particular examples. In this section we use the framework of general tensors that we have developed to obtain, in tensor form, expressions for these operators that are valid in all coordinate systems, whether orthogonal or not.  971   TENSORS  In order to compare the results obtained here with those given in section 10.10 for orthogonal coordinates, it is necessary to remember that here we are  working with the  in general  non-unit basis vectors ei = ∂r ∂ui or ei = ∇ui. Thus the components of a vector v = viei are not the same as the components ˆvi appropriate to the corresponding unit basis ˆei. In fact, if the scale factors of the coordinate system are hi, i = 1, 2, 3, then vi = ˆvi hi  no summation over i .  As mentioned in section 26.15, for an orthogonal coordinate system with scale  factors hi we have      gij =  h2 i 0  if i = j,  otherwise  and  gij =  1 h2 i 0  if i = j,  otherwise,  and so the determinant g of the matrix [gij] is given by g = h2  1h2  2h2 3.  The gradient of a scalar φ is given by  since the covariant derivative of a scalar is the same as its partial derivative.   26.91   Gradient  ∇φ = φ; iei =  ∂φ ∂ui  ei,  Divergence  Replacing the partial derivatives that occur in Cartesian coordinates with covari- ant derivatives, the divergence of a vector ﬁeld v in a general coordinate system is given by  ∇ · v = vi  cid:7   ; i =  + Γi  kivk.  ∂vi ∂ui   cid:8   Using the expression  26.82  for the Christoﬀel symbol in terms of the metric  tensor, we ﬁnd  Γi  ki = 1  2 gil  ∂gil ∂uk  +  ∂gkl ∂ui  − ∂gki ∂ul  = 1  2 gil ∂gil ∂uk .   26.92   The last two terms have cancelled because  gil ∂gkl ∂ui  = gli ∂gki ∂ul  = gil ∂gki ∂ul ,  where in the ﬁrst equality we have interchanged the dummy indices i and l, and in the second equality have used the symmetry of the metric tensor.  We may simplify  26.92  still further by using a result concerning the derivative of the determinant of a matrix whose elements are functions of the coordinates.  972   26.20 VECTOR OPERATORS IN TENSOR FORM   cid:1 Suppose A = [aij], B = [bij ] and that B = A−1. By considering the determinant a = A,  show that  ∂a ∂uk  = ab ji ∂aij ∂uk  .  bij =  ∆ji.  1 a   cid:4   j  a =  aij∆ij ,  ∂a ∂aij  = ∆ij,  If we denote the cofactor of the element aij by ∆ij then the elements of the inverse matrix are given by  see chapter 8   However, the determinant of A is given by  in which we have ﬁxed i and written the sum over j explicitly, for clarity. Partially diﬀerentiating both sides with respect to aij, we then obtain  since aij does not occur in any of the cofactors ∆ij.  Now, if the aij depend on the coordinates then so will the determinant a and, by the  chain rule, we have  ∂a ∂uk  =  ∂a ∂aij  ∂aij ∂uk  = ∆ij ∂aij ∂uk  = ab ji ∂aij ∂uk  ,  in which we have used  26.93  and  26.94 .  cid:2   Applying the result  26.95  to the determinant g of the metric tensor, and  remembering both that gikgkj = δi  j and that gij is symmetric, we obtain  Substituting  26.96  into  26.92  we ﬁnd that the expression for the Christoﬀel  symbol can be much simpliﬁed to give  Thus ﬁnally we obtain the expression for the divergence of a vector ﬁeld in a  general coordinate system as   26.93    26.94    26.95    26.96    26.97   ∂g ∂uk  = ggij ∂gij ∂uk .  Γi  ki =  1 2g  ∂g ∂uk  =  1√  g  √ g ∂ ∂uk .  ∇ · v = vi  ; i =  1√  ∂ ∂u j  g  √    gv j .  viei = v = ∇φ =  ∂φ ∂ui  ei,  973  If we replace v by ∇φ in ∇ · v then we obtain the Laplacian ∇2φ. From  26.91 ,  Laplacian  we have   TENSORS  and so the covariant components of v are given by vi = ∂φ ∂ui. In  26.97 , however, we require the contravariant components vi. These may be obtained by raising the index using the metric tensor, to give v j = g jkvk = g jk ∂φ ∂uk .  Substituting this into  26.97  we obtain  ∇2φ =  1√  ∂ ∂u j  g  gg jk ∂φ ∂uk  .   26.98    cid:1 Use  26.98  to ﬁnd the expression for ∇2φ in an orthogonal coordinate system with scale  factors hi, i = 1, 2, 3.  For an orthogonal coordinate system otherwise. Therefore, from  26.98  we have  √ g = h1h2h3; further, gij = 1 h2  i if i = j and gij = 0   cid:8    cid:31   ∇2φ =  1  h1h2h3  ∂ ∂u j  h1h2h3  h2 j  ∂φ ∂u j  ,  which agrees with the results of section 10.10.  cid:2    cid:7 √   cid:30   Curl  The special vector form of the curl of a vector ﬁeld exists only in three dimensions. We therefore consider a more general form valid in higher-dimensional spaces as well. In a general space the operation curl v is deﬁned by   curl v ij = vi; j − vj; i,  which is an antisymmetric covariant tensor.  In fact the diﬀerence of derivatives can be simpliﬁed, since  vi; j − vj; i =  ijvl − ∂vj  ∂ui  ∂vi ∂u j ∂vi ∂u j  − Γl − ∂vj ∂ui ,  =  + Γl  jivl   curl v ij =  ∂vi ∂u j  − ∂vj ∂ui .  where the Christoﬀel symbols have cancelled because of their symmetry properties. Thus curl v can be written in terms of partial derivatives as  Generalising slightly the discussion of section 26.17, in three dimensions we may associate with this antisymmetric second-order tensor a vector with contravariant components,   ∇ × v i = − 1 √ 2 = − 1 √ 2  g  g   cid:4 ijk curl v jk   cid:7    cid:8    cid:4 ijk  ∂vj ∂uk  − ∂vk ∂u j  1√  =   cid:4 ijk ∂vk ∂u j  ;  g  974   26.21 ABSOLUTE DERIVATIVES ALONG CURVES  this is the analogue of the expression in Cartesian coordinates discussed in section 26.8.  26.21 Absolute derivatives along curves  In section 26.19 we discussed how to diﬀerentiate a general tensor with respect to the coordinates and introduced the covariant derivative. In this section we consider the slightly diﬀerent problem of calculating the derivative of a tensor along a curve r t  that is parameterised by some variable t.  Let us begin by considering the derivative of a vector v along the curve. If we introduce an arbitrary coordinate system ui with basis vectors ei, i = 1, 2, 3, then we may write v = viei and so obtain  here the chain rule has been used to rewrite the last term on the right-hand side. Using  26.75  to write the derivatives of the basis vectors in terms of Christoﬀel symbols, we obtain  Interchanging the dummy indices i and j in the last term, we may factor out the basis vector and ﬁnd  dv dt  =  =  dvi dt dvi dt  ei + vi dei dt ei + vi ∂ei ∂uk  duk dt  ;  dv dt  =  dvi dt  ei + Γ j   cid:7   ej.  dt  ikvi duk  cid:8   dv dt  =  dvi dt  + Γi  jkv j duk  dt  ei.  The term in parentheses is called the absolute  or intrinsic  derivative of the components vi along the curve r t and is usually denoted by  δvi δt  ≡ dvi  dt  + Γi  jkv j duk  dt  = vi  ; k  duk dt  .  With this notation, we may write  dv dt  =  δvi δt  ei = vi  ; k  duk dt  ei.   26.99   Using the same method, the absolute derivative of the covariant components  vi of a vector is given by  Similarly, the absolute derivatives of the contravariant, mixed and covariant  ≡ vi; k  duk dt  .  δvi δt  975   components of a second-order tensor T are  TENSORS  j  δT ij δt δT i δt δTij δt  ; k  ≡ T ij ≡ T i ≡ Tij; k  j; k  duk dt duk dt duk dt  ,  ,  .  The derivative of T along the curve r t  may then be written in terms of, for example, its contravariant components as  dT dt  =  δT ij δt  ei ⊗ ej = T ij  ei ⊗ ej.  duk dt  ; k  26.22 Geodesics  As an example of the use of the absolute derivative, we conclude this chapter with a brief discussion of geodesics. A geodesic in real three-dimensional space is a straight line, which has two equivalent deﬁning properties. Firstly, it is the curve of shortest length between two points and, secondly, it is the curve whose tangent vector always points in the same direction  along the line . Although in this chapter we have considered explicitly only our familiar three-dimensional space, much of the mathematical formalism developed can be generalised to more abstract spaces of higher dimensionality in which the familiar ideas of Euclidean geometry are no longer valid. It is often of interest to ﬁnd geodesic curves in such spaces by using the deﬁning properties of straight lines in Euclidean space. We shall not consider these more complicated spaces explicitly but will de- termine the equation that a geodesic in Euclidean three-dimensional space  i.e. a straight line  must satisfy, deriving it in a suﬃciently general way that our method may be applied with little modiﬁcation to ﬁnding the equations satisﬁed by geodesics in more abstract spaces.  Let us consider a curve r s , parameterised by the arc length s from some point on the curve, and choose as our deﬁning property for a geodesic that its tangent vector t = dr ds always points in the same direction everywhere on the curve, i.e.  dt ds  = 0.   26.100   Alternatively, we could exploit the property that the distance between two points is a minimum along a geodesic and use the calculus of variations  see chapter 22 ; this would lead to the same ﬁnal result  26.101 .  If we now introduce an arbitrary coordinate system ui with basis vectors ei,  i = 1, 2, 3, then we may write t = tiei, and from  26.99  we ﬁnd  dt ds  = ti  ; k  ei = 0.  duk ds  976   Writing out the covariant derivative, we obtain  26.23 EXERCISES   cid:7    cid:8   dti ds  + Γi  jkt j duk  ds  ei = 0.  But, since t j = du j ds, it follows that the equation satisﬁed by a geodesic is  d2ui ds2 + Γi  jk  du j ds  duk ds  = 0.   26.101    cid:1 Find the equations satisﬁed by a geodesic  straight line  in cylindrical polar coordinates.  From  26.83 , the only non-zero Christoﬀel symbols are Γ1 Thus the required geodesic equations are  12 = Γ2  21 = 1 ρ.  d2u1 ds2 d2u2 ds2  + Γ1  22  + 2Γ2  12  du2 ds du1 ds  du2 ds du2 ds d2u3 ds2  = 0  = 0  = 0  ⇒ ⇒ ⇒  22 = −ρ and Γ2  cid:7    cid:8   2  = 0,  dφ ds  = 0,  − ρ  dφ ds  +  2 ρ  dρ ds = 0.  cid:2   d2ρ ds2 d2φ ds2 d2z ds2  26.1  Use the basic deﬁnition of a Cartesian tensor to show the following.  26.23 Exercises   a  That for any general, but ﬁxed, φ,   u1, u2  =  x1 cos φ − x2 sin φ, x1 sin φ + x2 cos φ   are the components of a ﬁrst-order tensor in two dimensions.   b  That   cid:7    cid:8   x2 2 x1x2  x1x2 x2 1  is not a tensor of order 2. To establish that a single element does not transform correctly is suﬃcient.  26.2  The components of two vectors, A and B, and a second-order tensor, T, are given in one coordinate system by  A =   1  0 0   0  , B =  ,  , B cid:7   √  1 0  =  1 2  3 0 1  A cid:7   √ 3 0   2  −1  0√ 3  T =  =  1 2  √  3 4 0   .   .  0 0 2  In a second coordinate system, obtained from the ﬁrst by rotation, the components of A and B are  Find the components of T in this new coordinate system and hence evaluate, with a minimum of calculation,  TijTji, TkiTjkTij, TikTmnTniTkm.  977   TENSORS  26.3  26.4  26.5  26.6  In section 26.3 the transformation matrix for a rotation of the coordinate axes was derived, and this approach is used in the rest of the chapter. An alternative view is that of taking the coordinate axes as ﬁxed and rotating the components of the system; this is equivalent to reversing the signs of all rotation angles.  Using this alternative view, determine the matrices representing  a  a positive  rotation of π 4 about the x-axis and  b  a rotation of −π 4 about the y-axis.  Determine the initial vector r which, when subjected to  a  followed by  b , ﬁnishes at  3, 2, 1 . Show how to decompose the Cartesian tensor Tij into three tensors,  Tij = Uij + Vij + Sij ,  where Uij is symmetric and has zero trace, Vij is isotropic and Sij has only three independent components. Use the quotient law discussed in section 26.7 to show that the array   y2 + z2 − x2  −2yx −2zx    x2 + z2 − y2  −2xy −2zy  −2xz −2yz  x2 + y2 − z2  forms a second-order tensor. Use tensor methods to establish the following vector identities:   a    u × v  × w =  u · w v −  v · w u;  b  curl  φu  = φ curl u +  grad φ  × u;  c  div  u × v  = v · curl u − u · curl v;  d  curl  u × v  =  v · grad u −  u · grad v + u div v − v div u;  e  grad 1  2  u · u  = u × curl u +  u · grad u.  cid:21   cid:21    cid:19   A A · dS  − 1  2 A2dS  =  V   cid:18   S  [A divA − A × curl A] dV ,  26.7  Use result  e  of the previous question and the general divergence theorem for tensors to show that, for a vector ﬁeld A,  26.8  where S is the surface enclosing volume V . A column matrix a has components ax, ay, az and A is the matrix with elements Aij = − cid:4 ijkak.  a  What is the relationship between column matrices b and c if Ab = c?  b  Find the eigenvalues of A and show that a is one of its eigenvectors. Explain  why this must be so.  26.9  Equation  26.29 ,  A cid:4 lmn = AliAmjAnk cid:4 ijk,  is a more general form of the expression  8.47  for the determinant of a 3 × 3 matrix A. The latter could have been written as A =  cid:4 ijkAi1Aj2Ak3,  whilst the former removes the explicit mention of 1, 2, 3 at the expense of an additional Levi–Civita symbol. As stated in the footnote on p. 942,  26.29  can  be readily extended to cover a general N × N matrix.  Use the form given in  26.29  to prove properties  i ,  iii ,  v ,  vi  and  vii  of determinants stated in subsection 8.9.1. Property  iv  is obvious by inspection. For deﬁniteness take N = 3, but convince yourself that your methods of proof would be valid for any positive integer N.  978   26.23 EXERCISES  26.10  A symmetric second-order Cartesian tensor is deﬁned by  Evaluate the following surface integrals, each taken over the surface of the unit sphere:   cid:21   Tij = δij − 3xixj.  cid:21    cid:21    a   Tij dS ;   b   TikTkj dS ;   c   xiTjk dS .  26.11  Given a non-zero vector v, ﬁnd the value that should be assigned to α to make  Pij = αvivj  and  Qij = δij − αvivj  26.12  26.13  26.14  26.15  26.16  into parallel and orthogonal projection tensors, respectively, i.e. tensors that satisfy, respectively, Pij vj = vi, Pijuj = 0 and Qij vj = 0, Qij uj = ui, for any vector u that is orthogonal to v.  Show, in particular, that Qij is unique, i.e. that if another tensor Tij has the  same properties as Qij then  Qij − Tij  wj = 0 for any vector w.  In four dimensions, deﬁne second-order antisymmetric tensors, Fij and Qij , and a ﬁrst-order tensor, Si, as follows:   a  F23 = H1, Q23 = B1 and their cyclic permutations;   b  Fi4 = −Di, Qi4 = Ei for i = 1, 2, 3;   c  S4 = ρ, Si = Ji for i = 1, 2, 3.   cid:11   Then, taking x4 as t and the other symbols to have their usual meanings in electromagnetic theory, show that the equations j ∂Fij  ∂xj = Si and ∂Qjk ∂xi + ∂Qki ∂xj + ∂Qij  ∂xk = 0 reproduce Maxwell’s equations. In the latter i, j, k is any set of three subscripts selected from 1, 2, 3, 4, but chosen in such a way that they are all diﬀerent. In a certain crystal the unit cell can be taken as six identical atoms lying at the corners of a regular octahedron. Convince yourself that these atoms can also be considered as lying at the centres of the faces of a cube and hence that the crystal has cubic symmetry. Use this result to prove that the conductivity tensor for the crystal, σij, must be isotropic. Assuming that the current density j and the electric ﬁeld E appearing in equation  26.44  are ﬁrst-order Cartesian tensors, show explicitly that the electrical con- ductivity tensor σij transforms according to the law appropriate to a second-order tensor.  The rate W at which energy is dissipated per unit volume, as a result of the current ﬂow, is given by E· j. Determine the limits between which W must lie for a given value of E as the direction of E is varied. In a certain system of units, the electromagnetic stress tensor Mij is given by  Mij = EiEj + BiBj − 1  2 δij  EkEk + BkBk ,  where the electric and magnetic ﬁelds, E and B, are ﬁrst-order tensors. Show that Consider a situation in which E = B, but the directions of E and B are Mij is a second-order tensor. not parallel. Show that E ± B are principal axes of the stress tensor and ﬁnd  the corresponding principal values. Determine the third principal axis and its corresponding principal value. A rigid body consists of four particles of masses m, 2m, 3m, 4m, respectively  situated at the points  a, a, a ,  a,−a,−a ,  −a, a,−a ,  −a,−a, a  and connected  together by a light framework.   a  Find the inertia tensor at the origin and show that the principal moments of  inertia are 20ma2 and  20 ± 2  √ 5 ma2.  979   TENSORS   b  Find the principal axes and verify that they are orthogonal.  26.17  A rigid body consists of eight particles, each of mass m, held together by light rods. In a certain coordinate frame the particles are at positions  ±a 3, 1,−1 ,  ±a 1,−1, 3 ,  ±a 1, 3,−1 ,  ±a −1, 1, 3 .  Show that, when the body rotates about an axis through the origin, if the angular velocity and angular momentum vectors are parallel then their ratio must be 40ma2, 64ma2 or 72ma2. The paramagnetic tensor χij of a body placed in a magnetic ﬁeld, in which its energy density is − 1  2 µ0M · H with Mi =  j χijHj , is   cid:11   26.18   2k  0 0   .  0 3k k  0 k 3k  Assuming depolarizing eﬀects are negligible, ﬁnd how the body will orientate itself if the ﬁeld is horizontal, in the following circumstances:   a   the body can rotate freely;   b  the body is suspended with the  1, 0, 0  axis vertical;   c   the body is suspended with the  0, 1, 0  axis vertical.  26.19  26.20  26.21  Show that all the nails lie in parallel planes.  A block of wood contains a number of thin soft-iron nails  of constant perme- ability . A unit magnetic ﬁeld directed eastwards induces a magnetic moment in  the block having components  3, 1,−2 , and similar ﬁelds directed northwards and vertically upwards induce moments  1, 3,−2  and  −2,−2, 2  respectively.  cid:6   For tin, the conductivity tensor is diagonal, with entries a, a, and b when referred to its crystal axes. A single crystal is grown in the shape of a long wire of length L and radius r, the axis of the wire making polar angle θ with respect to the crystal’s 3-axis. Show that the resistance of the wire is L πr2ab   a cos2 θ + b sin2 θ   cid:5   −1  .  By considering an isotropic body subjected to a uniform hydrostatic pressure  no shearing stress , show that the bulk modulus k, deﬁned by the ratio of the  pressure to the fractional decrease in volume, is given by k = E [3 1− 2σ ] where  E is Young’s modulus and σ is Poisson’s ratio.  26.22  For an isotropic elastic medium under dynamic stress, at time t the displacement ui and the stress tensor pij satisfy   cid:7    cid:8   pij = cijkl  ∂uk ∂xl  +  ∂ul ∂xk  and  ∂pij ∂xj  = ρ  ∂2ui ∂t2 ,  Show that both ∇· u and ∇× u satisfy wave equations and ﬁnd the corresponding where cijkl is the isotropic tensor given in equation  26.47  and ρ is a constant.  wave speeds.  980   26.23 EXERCISES  26.23  A fourth-order tensor Tijkl has the properties  Tjikl = −Tijkl,  Tijlk = −Tijkl.  Prove that for any such tensor there exists a second-order tensor Kmn such that  Tijkl =  cid:4 ijm cid:4 klnKmn  and give an explicit expression for Kmn. Consider two  separate  special cases, as follows.   a  Given that Tijkl is isotropic and Tijji = 1, show that Tijkl is uniquely deter-  mined and express it in terms of Kronecker deltas.   b  If now Tijkl has the additional property  show that Tijkl has only three linearly independent components and ﬁnd an expression for Tijkl in terms of the vector  Tklij = −Tijkl,  Vi = − 1  4  cid:4 jklTijkl.  26.24 Working in cylindrical polar coordinates ρ, φ, z, parameterise the straight line  geodesic  joining  1, 0, 0  to  1, π 2, 1  in terms of s, the distance along the line. Show by substitution that the geodesic equations, derived at the end of section 26.22, are satisﬁed. In a general coordinate system ui, i = 1, 2, 3, in three-dimensional Euclidean space, a volume element is given by  26.25  Show that an alternative form for this expression, written in terms of the deter- minant g of the metric tensor, is given by  dV = e1 du1 ·  e2 du2 × e3 du3 .  √  dV =  g du1 du2 du3.  26.26  26.27  Show that, under a general coordinate transformation to a new coordinate  cid:7 i, the volume element dV remains unchanged, i.e. show that it is a scalar system u quantity. By writing down the expression for the square of the inﬁnitesimal arc length  ds 2 in spherical polar coordinates, ﬁnd the components gij of the metric tensor in this coordinate system. Hence, using  26.97 , ﬁnd the expression for the divergence of a vector ﬁeld v in spherical polars. Calculate the Christoﬀel symbols  of the second kind  Γi  Find an expression for the second covariant derivative vi; jk ≡  vi; j  ; k of a vector  jk in this coordinate system.  vi  see  26.88  . By interchanging the order of diﬀerentiation and then subtracting the two expressions, we deﬁne the components Rl  ijk of the Riemann tensor as  vi; jk − vi; kj ≡ Rl  ijkvl.  Show that in a general coordinate system ui these components are given by  Rl  ijk =  ∂Γl ik ∂u j  − ∂Γl ij ∂uk  + Γm  ikΓl  mj  − Γm  ij Γl  mk.  By ﬁrst considering Cartesian coordinates, show that all the components Rl for any coordinate system in three-dimensional Euclidean space. In such a space, therefore, we may change the order of the covariant derivatives without changing the resulting expression.  ijk  ≡ 0  981   TENSORS      cid:21   B  A  L =  gij  dui dt  du j dt  dt.  d2ui dt2  + Γi  jk  du j dt  duk dt  =  ¨s ˙s  dui dt  ,  26.28  A curve r t  is parameterised by a scalar variable t. Show that the length of the curve between two points, A and B, is given by  Using the calculus of variations  see chapter 22 , show that the curve r t  that minimises L satisﬁes the equation  where s is the arc length along the curve, ˙s = ds dt and ¨s = d2s dt2. Hence, show that if the parameter t is of the form t = as + b, where a and b are constants, then we recover the equation for a geodesic  26.101 .  [ A parameter which, like t, is the sum of a linear transformation of s and a  translation is called an aﬃne parameter. ]  26.29 We may deﬁne Christoﬀel symbols of the ﬁrst kind by  Show that these are given by  Γijk = gilΓl  jk.   cid:7   Γkij =  1 2  ∂gik ∂u j  +  ∂gjk ∂ui  − ∂gij ∂uk   cid:8   .  By permuting indices, verify that  Using the fact that Γl  jk = Γl  kj , show that  ∂gij ∂uk  = Γijk + Γjik.  gij; k ≡ 0,  i.e. that the covariant derivative of the metric tensor is identically zero in all coordinate systems.  26.1  26.3  26.5  26.7 26.9  26.24 Hints and answers  2  √   cid:3 = c2x2  2 + csx1x2 + scx1x2 + s2x2 1.  1 = x1 cos φ − θ  − x2 sin φ − θ , etc.;  cid:7  − 2scx1x2 + c2x2  a  u  cid:7  √ √ √ 11 = s2x2  b  u 2, 0, 0; 0, 1,−1; 0, 1, 1 .  b   1  2  1, 0,−1; 0, 1 √ 2, −1 − √ 2    a   1  2, −1 + r =  2 Twice contract the array with the outer product of  x, y, z  with itself, thus  obtaining the expression − x2 + y2 + z2 2, which is an invariant and therefore a Write Aj ∂Ai ∂xj  as ∂ AiAj   ∂xj − Ai ∂Aj ∂xj .  i  Write out the expression for AT, contract both sides of the equation with  cid:4 lmn and pick out the expression for A on the RHS. Note that  cid:4 lmn cid:4 lmn is a numerical  √ 2, 0; 1, 0, 1 .  scalar.  2 T.  scalar.  iii  Each non-zero term on the RHS contains any particular row index once and only once. The same can be said for the Levi–Civita symbol on the LHS. Thus interchanging two rows is equivalent to interchanging two of the subscripts of   cid:4 lmn, and thereby reversing its sign. Consequently, the magnitude of A remains  the same but its sign is changed.  v  If, say, Api = λApj, for some particular pair of values i and j and all p then,  982   26.24 HINTS AND ANSWERS  in the  multiple  summation on the RHS, each Ank appears multiplied by  with no summation over i and j    cid:4 ijkAliAmj +  cid:4 jikAljAmi =  cid:4 ijkλAljAmj +  cid:4 jikAljλAmj = 0,  C cid:4 lmn = AlxBxiAmyByjAnzBzk cid:4 ijk.  since  cid:4 ijk = − cid:4 jik. Consequently, grouped in this way all terms are zero and A = 0.  vi  Replace Amj by Amj + λAlj and note that λAliAlj Ank cid:4 ijk = 0 by virtue of result  v .  vii  If C = AB, Contract this with  cid:4 lmn and show that the RHS is equal to  cid:4 xyzAT cid:4 xyzB. It then follows from result  i  that C = AB. α = v−2. Note that the most general vector has components wi = λvi+µu 1  i +νu 2  where both u 1  and u 2  are orthogonal to v. Construct the orthogonal transformation matrix S for the symmetry operation of  say  a rotation of 2π 3 about a body diagonal and, setting L = S−1 = ST, construct σ = σ. Repeat the procedure for  say  a rotation of π 2 about the x3-axis. These together show that σ11 = σ22 = σ33 and that all other σij = 0. Further symmetry requirements do not provide any additional constraints.  The transformation of δij has to be included; the principal values are ±E · B. The third axis is in the direction ±B × E with principal value −E2.  = LσLT and require σ   cid:7    cid:7   ,  i  The principal moments give the required ratios. The principal permeability, in direction  1, 1, 2 , has value 0. Thus all the nails lie in planes to which this is the normal.  Take p11 = p22 = p33 = −p, and pij = eij = 0 for i  cid:3 = j, leading to −p =   b  By relabelling dummy subscripts and using the stated antisymmetry property,   λ + 2µ 3 eii. The fractional volume change is eii; λ and µ are as deﬁned in  26.46  and the worked example that follows it. Consider Qpq =  cid:4 pij  cid:4 qklTijkl and show that Kmn = Qmn 4 has the required property.  a  Argue from the isotropy of Tijkl and  cid:4 ijk for that of Kmn and hence that it must be a multiple of δmn. Show that the multiplier is uniquely determined and  that Tijkl =  δilδjk − δikδjl  6. show that Knm = −Kmn. Show that −2Vi =  cid:4 minKmn and hence that Kmn =  cid:4 imnVi. Tijkl =  cid:4 kliVj −  cid:4 kljVi. Use e1 ·  e2 × e3  =  vi; j  ; k =  vi; j  , k − Γl   ∂u du1 du2 du3. ij vm. If all components of a tensor equal zero in one coordinate system, then they are zero in all coordinate systems. Use gilgln = δn   cid:7 3 = ∂u  cid:7  jkvi; l and vi; j = vi, j − Γm  cid:8   cid:7   √ g. = ∂u ∂u   cid:7 √ ikvl; j − Γl  i and gij = gji. Show that  Recall that  g and du   cid:7 2 du   cid:7 1 du  √  g   cid:7   gij;k =  ∂gij ∂uk  − Γjik − Γijk  ei ⊗ ej  and then use the earlier result.  26.11  26.13  26.15  26.17 26.19  26.21  26.23  26.25  26.27  26.29  983   27  Numerical methods  It happens frequently that the end product of a calculation or piece of analysis is one or more algebraic or diﬀerential equations, or an integral that cannot be evaluated in closed form or in terms of tabulated or pre-programmed functions. From the point of view of the physical scientist or engineer, who needs numerical values for prediction or comparison with experiment, the calculation or analysis is thus incomplete.  With the ready availability of standard packages on powerful computers for the numerical solution of equations, both algebraic and diﬀerential, and for the evaluation of integrals, in principle there is no need for the investigator to do anything other than turn to them. However, it should be a part of every engineer’s or scientist’s repertoire to have some understanding of the kinds of procedure that are being put into practice within those packages. The present chapter indicates  at a simple level  some of the ways in which analytically intractable problems can be tackled using numerical methods.  In the restricted space available in a book of this nature, it is clearly not possible to give anything like a full discussion, even of the elementary points that will be made in this chapter. The limited objective adopted is that of explaining and illustrating by simple examples some of the basic principles involved. In many cases, the examples used can be solved in closed form anyway, but this ‘obviousness’ of the answers should not detract from their illustrative usefulness, and it is hoped that their transparency will help the reader to appreciate some of the inner workings of the methods described.  The student who proposes to study complicated sets of equations or make repeated use of the same procedures by, for example, writing computer programs to carry out the computations, will ﬁnd it essential to acquire a good under- standing of topics hardly mentioned here. Amongst these are the sensitivity of the adopted procedures to errors introduced by the limited accuracy with which a numerical value can be stored in a computer  rounding errors  and to the  984   27.1 ALGEBRAIC AND TRANSCENDENTAL EQUATIONS  errors introduced as a result of approximations made in setting up the numerical procedures  truncation errors . For this scale of application, books speciﬁcally devoted to numerical analysis, data analysis and computer programming should be consulted.  So far as is possible, the method of presentation here is that of indicating and discussing in a qualitative way the main steps in the procedure, and then of following this with an elementary worked example. The examples have been restricted in complexity to a level at which they can be carried out with a pocket calculator. Naturally it will not be possible for the student to check all the numerical values presented, unless he or she has a programmable calculator or computer readily available, and even then it might be tedious to do so. However, it is advisable to check the initial step and at least one step in the middle of each repetitive calculation given in the text, so that how the symbolic equations are used with actual numbers is understood. Clearly the intermediate step should be chosen to be at a point in the calculation at which the changes are still suﬃciently large that they can be detected by whatever calculating device is used.  Where alternative methods for solving the same type of problem are discussed, for example in ﬁnding the roots of a polynomial equation, we have usually taken the same example to illustrate each method. This could give the mistaken impression that the methods are very restricted in applicability, but it is felt by the authors that using the same examples repeatedly has suﬃcient advantages, in terms of illustrating the relative characteristics of competing methods, to justify doing so. Once the principles are clear, little is to be gained by using new examples each time, and, in fact, having some prior knowledge of the ‘correct answer’ should allow the reader to judge the eﬃciency and dangers of particular methods as the successive steps are followed through.  One other point remains to be mentioned. Here, in contrast with every other chapter of this book, the value of a large selection of exercises is not clear cut. The reader with suﬃcient computing resources to tackle them can easily devise algebraic or diﬀerential equations to be solved, or functions to be integrated  which perhaps have arisen in other contexts . Further, the solutions of these problems will be self-checking, for the most part. Consequently, although a number of exercises are included, no attempt has been made to test the full range of ideas treated in this chapter.  27.1 Algebraic and transcendental equations  The problem of ﬁnding the real roots of an equation of the form f x  = 0, where f x  is an algebraic or transcendental function of x, is one that can sometimes be treated numerically, even if explicit solutions in closed form are not feasible.  985   NUMERICAL METHODS  f x  = x5 − 2x2 − 3  0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8  x  f x   14  12  10  8  6  4  2  0  −2 −4  Figure 27.1 A graph of the function f x  = x5 − 2x2 − 3 for x in the range 0 ≤ x ≤ 1.9.  Examples of the types of equation mentioned are the quartic equation,  and the transcendental equation,  ax4 + bx + c = 0,  x − 3 tanh x = 0.  The latter type is characterised by the fact that it contains, in eﬀect, a polynomial of inﬁnite order on the LHS.  We will discuss four methods that, in various circumstances, can be used to obtain the real roots of equations of the above types. In all cases we will take as the speciﬁc equation to be solved the ﬁfth-order polynomial equation  f x  ≡ x5 − 2x2 − 3 = 0.   27.1   The reasons for using the same equation each time were discussed in the intro- duction to this chapter.  For future reference, and so that the reader may follow some of the calculations leading to the evaluation of the real root of  27.1 , a graph of f x  in the range  0 ≤ x ≤ 1.9 is shown in ﬁgure 27.1.  Equation  27.1  is one for which no solution can be found in closed form, that is in the form x = a, where a does not explicitly contain x. The general scheme to be employed will be an iterative one in which successive approximations to a real root of  27.1  will be obtained, each approximation, it is to be hoped, being better than the preceding one; certainly, we require that the approximations converge and that they have as their limit the sought-for root. Let us denote the required  986   27.1 ALGEBRAIC AND TRANSCENDENTAL EQUATIONS  root by ξ and the values of successive approximations by x1, x2, . . . , xn, . . . . Then, for any particular method to be successful,  n→∞ xn = ξ, lim  where f ξ  = 0.   27.2   However, success as deﬁned here is not the only criterion. Since, in practice, only a ﬁnite number of iterations will be possible, it is important that the values of xn be close to that of ξ for all n > N, where N is a relatively low number; exactly how low it is naturally depends on the computing resources available and the accuracy required in the ﬁnal answer.  So that the reader may assess the progress of the calculations that follow, we record that to nine signiﬁcant ﬁgures the real root of equation  27.1  has the value  We now consider in turn four methods for determining the value of this root.  ξ = 1.495 106 40.   27.3   27.1.1 Rearrangement of the equation  If equation  27.1 , f x  = 0, can be recast into the form  where φ x  is a slowly varying function of x, then an iteration scheme  x = φ x ,  xn+1 = φ xn    27.4    27.5   will often produce a fair approximation to the root ξ after a few iterations, as follows. Clearly, ξ = φ ξ , since f ξ  = 0; thus, when xn is close to ξ, the next approximation, xn+1, will diﬀer little from xn, the actual size of the diﬀerence giving an order-of-magnitude indication of the inaccuracy in xn+1  when compared with ξ .  In the present case, the equation can be written  x =  2x2 + 3 1 5.   27.6   Because of the presence of the one-ﬁfth power, the RHS is rather insensitive to the value of x used to compute it, and so the form  27.6  ﬁts the general requirements for the method to work satisfactorily. It remains only to choose a starting approximation. It is easy to see from ﬁgure 27.1 that the value x = 1.5 would be a good starting point, but, so that the behaviour of the procedure at values some way from the actual root can be studied, we will make a poorer choice, x1 = 1.7.  With this starting value and the general recurrence relationship  xn+1 =  2x2  n + 3 1 5,   27.7   987   NUMERICAL METHODS  n 1 2 3 4 5 6 7 8  xn 1.7 1.544 18 1.506 86 1.497 92 1.495 78 1.495 27 1.495 14 1.495 12  f xn  5.42 1.01  2.28 × 10 5.37 × 10 1.28 × 10 3.11 × 10 7.34 × 10 1.76 × 10  −1 −2 −2 −3 −4 −4  Table 27.1 Successive approximations to the root of  27.1  using the method of rearrangement.  f An   n An 1 1.0 2 3 4 5 6  −4.0000 1.2973 −2.6916 1.4310 −1.0957 1.4762 −0.3482 1.4897 −0.1016 1.4936 −0.0289  Bn 1.7 1.7 1.7 1.7 1.7 1.7  f Bn  5.4186 5.4186 5.4186 5.4186 5.4186 5.4186  xn  f xn   1.2973 −2.6916 1.4310 −1.0957 1.4762 −0.3482 1.4897 −0.1016 1.4936 −0.0289 1.4947 −0.0082  Table 27.2 Successive approximations to the root of  27.1  using linear interpolation.  successive values can be found. These are recorded in table 27.1. Although not  strictly necessary, the value of f xn  ≡ x5 − 3 is also shown at each stage. It will be seen that x7 and all later xn agree with the precise answer  27.3  to within one part in 104. However, f xn  and xn − ξ are both reduced by a factor  − 2x2  n  n  of only about 4 for each iteration; thus a large number of iterations would be needed to produce a very accurate answer. The factor 4 is, of course, speciﬁc to this particular problem and would be diﬀerent for a diﬀerent equation. The successive values of xn are shown in graph  a  of ﬁgure 27.2.  27.1.2 Linear interpolation  In this approach two values, A1 and B1, of x are chosen with A1 < B1 and such that f A1  and f B1  have opposite signs. The chord joining the two points  A1, f A1   and  B1, f B1   is then notionally constructed, as illustrated in graph  b  of ﬁgure 27.2, and the value x1 at which the chord cuts the x-axis is determined by the interpolation formula  Anf Bn  − Bnf An  f Bn  − f An   ,  xn =  988   27.8     a    c   −2 −4  6  4  2  6  4  2  −2 −4  27.1 ALGEBRAIC AND TRANSCENDENTAL EQUATIONS  x1  x2  x3  ξ  1.0  1.2  1.4  1.6  1.0  1.2  1.6  1.4 x2  ξ x3  x1   b    d   −2 −4  6  4  2  6  4  2  −2 −4  1.0  1.2  1.4  1.6  1.0  1.2  x1  x3  x4  x2  ξ  ξ  1.4  x3  1.6  x2  x1  Figure 27.2 Graphical illustrations of the iteration methods discussed in the text:  a  rearrangement;  b  linear interpolation;  c  binary chopping;  d  Newton–Raphson.  with n = 1. Next, f x1  is evaluated and the process repeated after replacing either A1 or B1 by x1, according to whether f x1  has the same sign as f A1  or f B1 , respectively. In ﬁgure 27.2 b , A1 is the one replaced.  As can be seen in the particular example that we are considering, with this method there is a tendency, if the curvature of f x  is of constant sign near the root, for one of the two ends of the successive chords to remain un- changed.  Starting with the initial values A1 = 1 and B1 = 1.7, the results of the ﬁrst ﬁve iterations using  27.8  are given in table 27.2 and indicated in graph  b  of ﬁgure 27.2. As with the rearrangement method, the improvement in accuracy,  as measured by f xn  and xn − ξ, is a fairly constant factor at each iteration   approximately 3 in this case , and for our particular example there is little to choose between the two. Both tend to their limiting value of ξ monotonically, from either higher or lower values, and this makes it diﬃcult to estimate limits within which ξ can safely be presumed to lie. The next method to be described gives at any stage a range of values within which ξ is known to lie.  989   NUMERICAL METHODS  f An   n An 1 2 3 4 5 6 7 8  1.0000 −4.0000 1.3500 −2.1610 1.3500 −2.1610 1.4375 −0.9946 1.4813 −0.2573 1.4813 −0.2573 1.4922 −0.0552 1.4922 −0.0552  Bn 1.7000 1.7000 1.5250 1.5250 1.5250 1.5031 1.5031 1.4977  f Bn  5.4186 5.4186 0.5968 0.5968 0.5968 0.1544 0.1544 0.0487  xn  f xn   1.5250  0.5968  1.3500 −2.1610 1.4375 −0.9946 1.4813 −0.2573 1.4922 −0.0552 1.4949 −0.0085  0.0487  0.1544  1.4977  1.5031  Table 27.3 Successive approximations to the root of  27.1  using binary chopping.  27.1.3 Binary chopping  Again two values of x, A1 and B1, that straddle the root are chosen, such that A1 < B1 and f A1  and f B1  have opposite signs. The interval between them is then halved by forming  xn = 1  2  An + Bn ,   27.9   with n = 1, and f x1  is evaluated. It should be noted that x1 is determined solely by A1 and B1, and not by the values of f A1  and f B1  as in the linear interpolation method. Now x1 is used to replace either A1 or B1, depending on which of f A1  or f B1  has the same sign as f x1 , i.e. if f A1  and f x1  have the same sign then x1 replaces A1. The process isthen repeated to obtain x2, x3, etc. This has been carried through in table 27.3 for our standard equation  27.1  and is illustrated in ﬁgure 27.2 c . The entries have been rounded to four places of decimals. It is suggested that the reader follows through the sequential replace- ments of the An and Bn in the table and correlates the ﬁrst few of these with graph  c  of ﬁgure 27.2.  Clearly, the accuracy with which ξ is known in this approach increases by only a factor of 2 at each step, but this accuracy is predictable at the outset of the calculation and  unless f x  has very violent behaviour near x = ξ  a range of x in which ξ lies can be safely stated at any stage. At the stage reached in the last row of table 27.3 it may be stated that 1.4949 < ξ < 1.4977. Thus binary chopping gives a simple approximation method  it involves less multiplication than linear interpolation, for example  that is predictable and relatively safe, although its convergence is slow.  27.1.4 Newton–Raphson method  The Newton–Raphson  NR  procedure is somewhat similar to the interpolation method, but, as will be seen, has one distinct advantage over the latter. Instead  990   27.1 ALGEBRAIC AND TRANSCENDENTAL EQUATIONS  n 1 2 3 4 5 6  f xn  5.42 1.03  xn 1.7 1.545 01 1.498 87 1.495 13 1.495 106 40 1.495 106 40 —  7.20 × 10 −2 4.49 × 10 −4 2.6 × 10 −8  Table 27.4 Successive approximations to the root of  27.1  using the Newton– Raphson method.  of  notionally  constructing the chord between two points on the curve of f x  against x, the tangent to the curve is notionally constructed at each successive value of xn, and the next value, xn+1, is taken as the point at which the tangent cuts the axis f x  = 0. This is illustrated in graph  d  of ﬁgure 27.2.  cid:7   If the nth value is xn, the tangent to the curve of f x  at that point has slope  xn  and passes through the point x = xn, y = f xn . Its equation is thus  f  y x  =  x − xn f   cid:7    xn  + f xn .  The value of x at which y = 0 is then taken as xn+1; thus the condition y xn+1  = 0 yields, from  27.10 , the iteration scheme  xn+1 = xn − f xn    cid:7   .  f   xn   This is the Newton–Raphson iteration formula. Clearly, if xn is close to ξ then xn+1 is close to xn, as it should be. It is also apparent that if any of the xn comes close  xn  is close to zero, the scheme is not going to a stationary point of f, so that f to work well.   cid:7   For our standard example,  27.11  becomes  xn+1 = xn − x5  − 2x2 − 3 − 4xn  n 5x4 n  n  =  − 2x2 n + 3 − 4xn  4x5 n 5x4 n  .  Again taking a starting value of x1 = 1.7, we obtain in succession the entries in table 27.4. The diﬀerent values are given to an increasing number of decimal places as the calculation proceeds; f xn  is also recorded.  It is apparent that this method is unlike the previous ones in that the increase in accuracy of the answer is not constant throughout the iterations but improves dramatically as the required root is approached. Away from the root the behaviour of the series is less satisfactory, and from its geometrical interpretation it can be seen that if, for example, there were a maximum or minimum near the root then the series could oscillate between values on either side of it  instead of ‘homing in’ on the root . The reason for the good convergence near the root is discussed in the next section.  991   27.10    27.11    27.12    NUMERICAL METHODS  Of the four methods mentioned, no single one is ideal, and, in practice, some mixture of them is usually to be preferred. The particular combination of methods selected will depend a great deal on how easily the progress of the calculation may be monitored, but some combination of the ﬁrst three methods mentioned, followed by the NR scheme if great accuracy were required, would be suitable for most situations.  27.2 Convergence of iteration schemes  For iteration schemes in which xn+1 can be expressed as a diﬀerentiable function of xn, for example the rearrangement or NR methods of the previous section, a partial analysis of the conditions necessary for a successful scheme can be made as follows.  Suppose the general iteration formula is expressed as  xn+1 = F xn    27.13     27.7  and  27.12  are examples . Then the sequence of values x1, x2, . . . , xn, . . . is required to converge to the value ξ that satisﬁes both  f ξ  = 0  and  ξ = F ξ .   27.14   If the error in the solution at the nth stage is  cid:4 n, i.e. xn = ξ +  cid:4 n, then  ξ +  cid:4 n+1 = xn+1 = F xn  = F ξ +  cid:4 n .   27.15   For the iteration process to converge, a decreasing error is required, i.e.  cid:4 n+1 <  cid:4 n. To see what this implies about F, we expand the right-hand term of  27.15   by means of a Taylor series and use  27.14  to replace  27.15  by  ξ +  cid:4 n+1 = ξ +  cid:4 nF   ξ  + 1  2  cid:4 2 nF   cid:7    cid:7  cid:7    ξ  + ··· .   27.16   This shows that, for small  cid:4 n,  and that a necessary  but not suﬃcient  condition for convergence is that   cid:7    27.17    cid:7    ξ , which It should be noted that this is a condition on F may have any ﬁnite value. Figure 27.3 illustrates in a graphical way how the convergence proceeds for the case 0 < F   ξ  and not on f   ξ  < 1.   cid:7   Equation  27.16  suggests that if F x  can be chosen so that F  ratio  cid:4 n+1  cid:4 n could be made very small, of order  cid:4 n in fact. To go even further,   ξ  = 0 then the  if it can be arranged that the ﬁrst few derivatives of F vanish at x = ξ then the   cid:7    cid:4 n+1 ≈ F   cid:7    ξ  cid:4 n  F   cid:7    ξ  < 1.  992   27.2 CONVERGENCE OF ITERATION SCHEMES  y  y = x  y = F x   ξ  xn  xn+1 xn+2  x  Figure 27.3 Illustration of the convergence of the iteration scheme xn+1 =  ξ  < 1, where ξ = F ξ . The line y = x makes an angle F xn  when 0 < F  ξ  with the x-axis. π 4 with the axes. The broken line makes an angle tan  −1 F   cid:7    cid:7   convergence, once xn has become close to ξ, could be very rapid indeed. If the ﬁrst N − 1 derivatives of F vanish at x = ξ, i.e.   cid:7   F   ξ  = F   cid:7  cid:7    ξ  = ··· = F  N−1  ξ  = 0  and consequently   cid:4 n+1 = O  cid:4 N  n  ,  then the scheme is said to have Nth-order convergence.  This is the explanation of the signiﬁcant diﬀerence in convergence between the NR scheme and the others discussed  judged by reference to  27.19 , so that the diﬀerentiability of the function F is not a prerequisite . The NR procedure has second-order convergence, as is shown by the following analysis. Since   27.18    27.19    cid:7  cid:7   +  f x f [ f   x   x ]2   cid:7   ,  F   x   F x  = x − f x   x  = 1 − f  cid:7    cid:7  f  cid:7   x   cid:7   x   cid:7  cid:7   x   x ]2 .  ξ   cid:3 = 0, it follows that F  cid:7   f x f [ f  =  f   cid:7    cid:7   993  Now, provided f   ξ  = 0 because f x  = 0 at x = ξ.   Table 27.5 Successive approximations to  27.20 .  16 using the iteration scheme   cid:1 The following is an iteration scheme for ﬁnding the square root of X:  Show that it has second-order convergence and illustrate its eﬃciency by ﬁnding, say, starting with a very poor guess,  16 = 1.  If this scheme does converge to ξ then ξ will satisfy   27.20   √  16  NUMERICAL METHODS  n 1 2 3 4 5 6  xn+1 8.5 5.191 4.137 4.002 257 4.000 000 637 4   cid:4 n 4.5 1.19  1.4 × 10 2.3 × 10 6.4 × 10 — √  −1 −3 −7  ξ =  ξ +  ⇒ ξ2 = X,   cid:7    cid:8   1 2  xn +  .  X xn  xn+1 = √   cid:7   1 2   cid:8   cid:7   X ξ  1 2   cid:8   X x   cid:8   F x  =  x +  ,   cid:7   cid:7   1 2  x2  1 − X  cid:8   X x3  x=ξ   cid:7   F   ξ  =   cid:7  cid:7   F   ξ  =  = 0,  x=ξ   cid:3 = 0.  =  1 ξ  as required. The iteration function F is given by  and so, since ξ2 = X,  whilst  Thus the procedure has second-order, but not third-order, convergence.  We now show the procedure in action. Table 27.5 gives successive values of xn and of  cid:4 n, the diﬀerence between xn and the true value, 4. As we can see, the scheme is crude initially, but once xn gets close to ξ, it homes in on the true value extremely rapidly.  cid:2   27.3 Simultaneous linear equations  As we saw in chapter 8, many situations in physical science can be described approximately or exactly by a set of N simultaneous linear equations in N  994   27.3 SIMULTANEOUS LINEAR EQUATIONS  variables  unknowns , xi, i = 1, 2, . . . , N. The equations take the general form  A11x1 + A12x2 + ··· + A1N xN = b1, A21x1 + A22x2 + ··· + A2N xN = b2,  AN1x1 + AN2x2 + ··· + ANNxN = bN,  ...   27.21   where the Aij are constants and form the elements of a square matrix A. The bi are given and form a column matrix b. If A is non-singular then  27.21  can be solved for the xi using the inverse of A, according to the formula  x = A−1b.  This approach was discussed at length in chapter 8 and will not be considered further here.  27.3.1 Gaussian elimination  We follow instead a continuation of one of the earliest techniques acquired by a student of algebra, namely the solving of simultaneous equations  initially only two in number  by the successive elimination of all the variables but one. This  known as Gaussian elimination  is achieved by using, at each stage, one of the equations to obtain an explicit expression for one of the remaining xi in terms of the others and then substituting for that xi in all other remaining equations. Eventually a single linear equation in just one of the unknowns is obtained. This is then solved and the result is resubstituted in previously derived equations  in reverse order  to establish values for all the xi.  This method is probably very familiar to the reader, and so a speciﬁc example to illustrate this alone seems unnecessary. Instead, we will show how a calculation along such lines might be arranged so that the errors due to the inherent lack of precision in any calculating equipment do not become excessive. This can happen if the value of N is large and particularly  and we will merely state this  if the elements A11, A22, . . . , ANN on the leading diagonal of the matrix in  27.21  are small compared with the oﬀ-diagonal elements.  The process to be described is known as Gaussian elimination with interchange. The only, but essential, diﬀerence from straightforward elimination is that before each variable xi is eliminated, the equations are reordered to put the largest  in modulus  remaining coeﬃcient of xi on the leading diagonal.  We will take as an illustration a straightforward three-variable example, which can in fact be solved perfectly well without any interchange since, with simple numbers and only two eliminations to perform, rounding errors do not have a chance to build up. However, the important thing is that the reader should  995   NUMERICAL METHODS  appreciate how this would apply in  say  a computer program for a 1000-variable case, perhaps with unforseeable zeros or very small numbers appearing on the leading diagonal.  cid:1 Solve the simultaneous equations  x1  3x1 −20x2   a   b    c  −x1  +6x2 −4x3 = 8,  +x3 = 12, +3x2 +5x3 = 3.   27.22   Firstly, we interchange rows  a  and  b  to bring the term 3x1 onto the leading diagonal. In the following, we label the important equations  I ,  II ,  III , and the others alphabetically. A general  i.e. variable  label will be denoted by j.  For  j  =  d  and  e , replace row  j  by  where aj1 is the coeﬃcient of x1 in row  j , to give the two equations  3x1 −20x2   I   d    e  −x1  x1  +6x2 −4x3 = 8,  +x3 = 12,  +3x2 +5x3 = 3.   cid:5   cid:5    II    f   6 + 20 3 3 − 20  x3 = 8 − 12 3 ,  x3 = 3 + 12 3 .  3  × row  I ,  cid:6   cid:6   x2 +  row  j  − aj1  cid:5 −4 − 1  cid:6   cid:6   cid:5   cid:6   cid:5 − 11  5 + 1 3  x2  +  3  3  row  f  −  cid:18   3 38 3  × row  II .  cid:19   ×  −13   3  16  38  3 + 11 3x1 −20x2  x3 = 7 + 11 38  × 4.  38x2 −13x3 = 12,  +x3 = 12,  x3 = 2.   III    I   II   III   This gives  Collecting together and tidying up the ﬁnal equations, we have  Now 6 + 20   > 3 − 20   and so no interchange is required before the next elimination. To  eliminate x2, replace row  f  by  3  3  Starting with  III  and working backwards, it is now a simple matter to obtain  x1 = 10,  x2 = 1,  x3 = 2.  cid:2   27.3.2 Gauss–Seidel iteration  In the example considered in the previous subsection an explicit way of solving a set of simultaneous equations was given, the accuracy obtainable being limited only by the rounding errors in the calculating facilities available, and the calcula- tion was planned to minimise these. However, in some situations it may be that only an approximate solution is needed. If, for a large number of variables, this is  996   27.3 SIMULTANEOUS LINEAR EQUATIONS  the case then an iterative method may produce a satisfactory degree of precision with less calculation. Such a method, known as Gauss–Seidel iteration, is based upon the following analysis.  The problem is again that of ﬁnding the components of the column matrix x  that satisﬁes  Ax = b   27.23   when A and b are a given matrix and column matrix, respectively.  The steps of the Gauss–Seidel scheme are as follows.   i  Rearrange the equations  usually by simple division on both sides of each equation  so that all diagonal elements of the new matrix C are unity, i.e.  27.23  becomes  where C = I − F, and F has zeros as its diagonal elements.  Cx = d,   ii  Step  i  produces  and this forms the basis of an iteration scheme,  where xn is the nth approximation to the required solution vector ξ.   iii  To improve the convergence, the matrix F, which has zeros on its leading diagonal, can be written as the sum of two matrices L and U that have non-zero elements only below and above the leading diagonal, respectively:  Fx + d = Ix = x,  xn+1 = Fxn + d,     Fij 0  Fij 0  Lij =  Uij =  if i > j,  otherwise,  if i < j,  otherwise.   27.24    27.25    27.26    27.27   This allows the latest values of the components of x to be used at each stage and an improved form of  27.26  to be obtained:  xn+1 = Lxn+1 + Uxn + d.   27.28   To see why this is possible, we note, for example, that when calculating, say, the fourth component of xn+1, its ﬁrst three components are already known, and, because of the structure of L, these are the only ones needed to evaluate the fourth component of Lxn+1.  997   NUMERICAL METHODS  n 1 2 3 4 5 6 7  x1 2 4 12.76 9.008 10.321 9.902 10.029  x2 2 0.1 1.381 0.867 1.042 0.987 1.004  x3 2 1.34 2.323 1.881 2.039 1.988 2.004  Table 27.6 Successive approximations to the solution of simultaneous equa- tions  27.29  using the Gauss–Seidel iteration method.   27.29    cid:1 Obtain an approximate solution to the simultaneous equations  x1  3x1 −20x2 −x1  +6x2 −4x3 = 8,  +x3 = 12, +3x2 +5x3 = 3.  These are the same equations as were solved in subsection 27.3.1.  Divide the equations by 1, −20 and 5, respectively, to give x1 + 6x2 − 4x3 = 8, −0.15x1 + x2 − 0.05x3 = −0.6, −0.2x1 + 0.6x2 + x3 = 0.6.  Thus, set out in matrix form,  27.28  is, in this case, given by     x1  x2 x3  =  n+1   0  0 −6  0.15  0.2 −0.6  0 0  +  0 0  0 0  0 0 0  0.05  4  0   x1  x1  x2 x3     x2 x3   8−0.6  0.6   .  n+1  +  n  Suppose initially  n = 1  we guess each component to have the value 2. Then the successive sets of values of the three quantities generated by this scheme are as shown in table 27.6. Even with the rather poor initial guess, a close approximation to the exact result, x1 = 10, x2 = 1, x3 = 2, is obtained in only a few iterations.  cid:2   27.3.3 Tridiagonal matrices  Although for the solution of most matrix equations Ax = b the number of operations required increases rapidly with the size N × N of the matrix  roughly as N3 , for one particularly simple kind of matrix the computing required increases only linearly with N. This type often occurs in physical situations in which objects in an ordered set interact only with their nearest neighbours and is one in which only the leading diagonal and the diagonals immediately above and below it  998   27.3 SIMULTANEOUS LINEAR EQUATIONS  contain non-zero entries. Such matrices are known as tridiagonal matrices. They may also be used in numerical approximations to the solutions of certain types of diﬀerential equation.  A typical matrix equation involving a tridiagonal matrix is as follows:  0  b1 c1 a2  b2 c2 a3  b3 c3 .  .  .  .  .  . aN–1  .  .  . bN–1 cN–1 aN bN  0  x1 x2 x3  .    .    .  xN–1 xN  y1 y2 y3  .    .    .  yN–1 yN  =   27.30   So as to keep the entries in the matrix as free from subscripts as possible, we have used a, b and c to indicate subdiagonal, leading diagonal and superdiagonal elements, respectively. As a consequence, we have had to change the notation for the column matrix on the RHS from b to  say  y.  In such an equation the ﬁrst and last rows involve x1 and xN, respectively, and so the solution could be found by letting x1 be unknown and then solving in turn each row of the equation in terms of x1, and ﬁnally determining x1 by requiring the next-to-last line to generate for xN an equation compatible with that given by the last line. However, if the matrix is large this becomes a very cumbersome operation, and a simpler method is to assume a form of solution  xi−1 = θi−1xi + φi−1.   27.31   Since the ith line of the matrix equation is  aixi−1 + bixi + cixi+1 = yi,  we must have, by substituting for xi−1, that   aiθi−1 + bi xi + cixi+1 = yi − aiφi−1.  This is also in the form of  27.31 , but with i replaced by i+1. Thus the recurrence formulae for θi and φi are  −ci  θi =  aiθi−1 + bi  ,  φi =  yi − aiφi−1 aiθi−1 + bi  ,   27.32   provided the denominator does not vanish for any i. From the ﬁrst of the matrix  equations it follows that θ1 = −c1 b1 and φ1 = y1 b1. The equations may now  be solved for the xi in two stages without carrying through an unknown quantity. First, all the θi and φi are generated using  27.32  and the values of θ1 and φ1, then, as a second stage,  27.31  is used to evaluate the xi, starting with xN  = φN  and working backwards.  999   NUMERICAL METHODS   cid:1 Solve the following tridiagonal matrix equation, in which only non-zero elements are shown:    1  −1  2 2  1  2 −1  3       =     .  4  3−3  10  7−2  x1 x2 x3 x4 x5 x6  2 1 3  1 4  −2  2 2   27.33   The solution is set out in table 27.7, in which the arrows indicate the general ﬂow of the calculation. First, the columns of ai, bi, ci and yi are ﬁlled in from the original equation  27.33  and then the recurrence relations  27.32  are used to ﬁll in the successive rows starting at the top; on each row we work from left to right as far as and including the φi column. Finally, the bottom entry in the the xi column is set equal to the bottom entry in the completed φi column and the rest of the xi column is completed by using  27.31  and  working up from the bottom. Thus the solution is x1 = 2; x2 = 1; x3 = 3; x4 = −1; x5 =  ai 0  ↓ ↓ −1 ↓ ↓ ↓ ↓ −2  2 3 3  bi 1 2  −1  1 4 2  ci  2 → 1 → 2 → 1 → 2 → 0 →  aiθi−1 + bi  −3 2  1 4  5  17 5 54 17  θi  −2 −1 4 −1 5 −10 17  4 3  0  yi 4 3  −3  aiφi−1 −4  0  7 2 13  10 7  −9 5 −2 −88 17  φi 4  xi 2 1 3  7 4 13 3  −3 5 −1 1 →  44 17  2 1  ↑ ↑ ↑ ↑ ↑ ↑  Table 27.7 The solution of tridiagonal matrix equation  27.33 . The arrows indicate the general ﬂow of the calculation, as described in the text.  2; x6 = 1.  cid:2   27.4 Numerical integration  As noted at the start of this chapter, with modern computers and computer packages – some of which will present solutions in algebraic form, where that is possible – the inability to ﬁnd a closed-form expression for an integral no longer presents a problem. But, just as for the solution of algebraic equations, it is extremely important that scientists and engineers should have some idea of the procedures on which such packages are based. In this section we discuss some of the more elementary methods used to evaluate integrals numerically and at the same time indicate the basis of more sophisticated procedures.  The standard integral evaluation has the form  I =  f x  dx,   27.34   where the integrand f x  may be given in analytic or tabulated form, but for the cases under consideration no closed-form expression for I can be obtained. All   cid:21   b  a  1000   27.4 NUMERICAL INTEGRATION   a   f x    b   f x    c   fi  fi+1  fi+1  fi−1  fi  h  h  h  h  fi+1  xi  xi+1 2  xi+1  xi  xi+1  xi−1  xi  xi+1  Figure 27.4  a  Deﬁnition of nomenclature.  b  The approximation in using the trapezium rule; f x  is indicated by the broken curve.  c  Simpson’s rule approximation; f x  is indicated by the broken curve. The solid curve is part of the approximating parabola.  numerical evaluations of I are based on regarding I as the area under the curve of f x  between the limits x = a and x = b and attempting to estimate that area.  The simplest methods of doing this involve dividing up the interval a ≤ x ≤ b into N equal sections, each of length h =  b − a  N. The dividing points are labelled xi, with x0 = a, xN = b, i running from 0 to N. The point xi is a distance ih from a. The central value of x in a strip  x = xi + h 2  is denoted for brevity by xi+1 2, and for the same reason f xi  is written as fi. This nomenclature is indicated graphically in ﬁgure 27.4 a .  So that we may compare later estimates of the area under the curve with the true value, we next obtain an exact expression for I, even though we cannot evaluate it. To do this we need to consider only one strip, say that between xi and xi+1. For this strip the area is, using Taylor’s expansion,   cid:21   h 2  −h 2  f xi+1 2 + y  dy =  f n  xi+1 2   dy  yn n!  h 2  −h 2  yn n!  dy   cid:7    cid:8   f n  i+1 2  2   n + 1 !  n+1  .  h 2   27.35   It should be noted that, in this exact expression, only the even derivatives of f survive the integration and all derivatives are evaluated at xi+1 2. Clearly  ∞ cid:4   n=0   cid:21   f n  i+1 2  h 2  −h 2   cid:21  ∞ cid:4  ∞ cid:4   n=0  n even  =  =  1001   NUMERICAL METHODS  other exact expressions are possible, e.g. the integral of f xi + y  over the range 0 ≤ y ≤ h, but we will ﬁnd  27.35  the most useful for our purposes.  Although the preceding discussion has implicitly assumed that both of the limits a and b are ﬁnite, with the consequence that N is ﬁnite, the general method can be adapted to treat some cases in which one of the limits is inﬁnite. It is  suﬃcient to consider one inﬁnite limit, as an integral with limits −∞ and ∞ can  be considered as the sum of two integrals, each with one inﬁnite limit.  Consider the integral   cid:21  ∞  a  I =  f x  dx,  where a is chosen large enough that the integrand is monotonically decreasing −2. The change of variable t = 1 x  cid:7   cid:8  for x > a and falls oﬀ more quickly than x converts this integral into   cid:21   I =  1 a  0  1 t2 f  1 t  dt.  known to behave asymptotically as g x e  It is now an integral over a ﬁnite range and the methods indicated earlier can be applied to it. The value of the integrand at the lower end of the t-range is zero. In a similar vein, integrals with an upper limit of ∞ and an integrand that is −αx, where g x  is a smooth function, can be converted into an integral over a ﬁnite range by setting x = −α −1 ln αt. Again, the lower limit, a, for this part of the integral should be positive and chosen beyond the last turning point of g x . The part of the integral for x < a is treated in the normal way. However, it should be added that if the asymptotic form of the integrand is known to be a linear or quadratic  decreasing  exponential then there are better ways of estimating it numerically; these are discussed in subsection 27.4.3 on Gaussian integration.  We now turn to practical ways of approximating I, given the values of fi, or a  means to calculate them, for i = 0, 1, . . . , N.  27.4.1 Trapezium rule  In this simple case the area shown in ﬁgure 27.4 a  is approximated as shown in ﬁgure 27.4 b , i.e. by a trapezium. The area Ai of the trapezium is  Ai = 1  2  fi + fi+1 h,   27.36   and if such contributions from all strips are added together then the estimate of the total, and hence of I, is  I estim.  =  Ai =   f0 + 2f1 + 2f2 + ··· + 2fN−1 + fN .   27.37   h 2  N−1 cid:4   i=0  1002   27.4 NUMERICAL INTEGRATION  This provides a very simple expression for estimating integral  27.34 ; its accuracy is limited only by the extent to which h can be made very small  and hence N very large  without making the calculation excessively long. Clearly the estimate provided is exact only if f x  is a linear function of x.  The error made in calculating the area of the strip when the trapezium rule is used may be estimated as follows. The values used are fi and fi+1, as in  27.36 . These can be expressed accurately in terms of fi+1 2 and its derivatives by the Taylor series   cid:7    cid:8    cid:7    cid:8   fi+1 2±1 2 = fi+1 2 ± h   cid:7  i+1 2 + f  2  1 2!  2   cid:7  cid:7  i+1 2  f  ± 1 3!  h 2  h 2  3  i+1 2 + ··· . f 3   Thus   cid:16   Ai estim.  = 1  2 h fi + fi+1  1 2!  fi+1 2 +  = h   cid:17   2   cid:7  cid:7  i+1 2 + O h4  f  ,  whilst, from the ﬁrst few terms of the exact result  27.35 ,  Thus the error ∆Ai = Ai estim.  − Ai exact  is given by  cid:7  cid:7  i+1 2 + O h5   h3f  Ai exact  = hfi+1 2 +  2 3!   cid:6    cid:5  ∆Ai = ≈ 1  1 8  − 1 24  cid:7  cid:7  12 h3f i+1 2.  3   cid:7  cid:7  i+1 2 + O h5 . f   cid:8   cid:8    cid:7   cid:7   h 2  h 2  The total error in I estim.  is thus given approximately by  ∆I estim.  ≈ 1  12 nh3 cid:20 f   cid:7  cid:7  cid:21  = 1  12  b − a h2 cid:20 f   cid:7  cid:7  cid:21 ,  where  cid:20 f   cid:7  cid:7  cid:21  represents an average value for the second derivative of f over the   27.38   interval a to b.  cid:1 Use the trapezium rule with h = 0.5 to evaluate  2   x2 − 3x + 4  dx,  I =  0  and, by evaluating the integral exactly, examine how well  27.38  estimates the error.  With h = 0.5, we will need ﬁve values of f x  = x2 − 3x + 4 for use in formula  27.37 .  They are f 0  = 4, f 0.5  = 2.75, f 1  = 2, f 1.5  = 1.75 and f 2  = 2. Putting these into  27.37  gives  I estim.  =  0.5 2   4 + 2 × 2.75 + 2 × 2 + 2 × 1.75 + 2  = 4.75.  The exact value is  I exact  =  + 4x  = 4 2 3 .   cid:14 2  0  x3 3  − 3x2 2  1003   cid:21    cid:13    NUMERICAL METHODS  The diﬀerence between the estimate of the integral and the exact answer is 1 12. Equation   27.38  estimates this error as 2 × 0.25 ×  cid:20 f one for which  cid:20 f its second derivative is constant, and equal to 2 in this case. Thus  cid:20 f   cid:7  cid:7  cid:21  12. Our  deliberately chosen!  integrand is  cid:7  cid:7  cid:21  can be evaluated trivially. Because f x  is a quadratic function of x,  cid:7  cid:7  cid:21  has value 2 and   27.38  estimates the error as 1 12; that the estimate is exactly right should be no surprise since the Taylor expansion for a quadratic polynomial about any point always terminates after three terms and so no higher-order terms in h have been ignored in  27.38 .  cid:2   27.4.2 Simpson’s rule  Whereas the trapezium rule makes a linear interpolation of f, Simpson’s rule eﬀectively mimics the local variation of f x  using parabolas. The strips are treated two at a time  ﬁgure 27.4 c   and therefore their number, N, should be made even.  In the neighbourhood of xi, for i odd, it is supposed that f x  can be adequately  represented by a quadratic form,  In particular, applying this to y = ±h yields two expressions involving b  f xi + y  = fi + ay + by2.   27.39   fi+1 = f xi + h  = fi + ah + bh2, fi−1 = f xi − h  = fi − ah + bh2;  bh2 = 1  2  fi+1 + fi−1 − 2fi .  thus  xi+1 is given by   cid:21   h  −h   cid:30   Now, in the representation  27.39 , the area of the double strip from xi−1 to  Ai estim.  =   fi + ay + by2  dy = 2hfi + 2  3 bh3.  Substituting for bh2 then yields, for the estimated area,  Ai estim.  = 2hfi + 2  3 h × 1  2  fi+1 + fi−1 − 2fi   = 1  3 h 4fi + fi+1 + fi−1 ,  an expression involving only given quantities. It should be noted that the values of neither b nor a need be calculated.  For the full integral,  I estim.  = 1  3 h  f0 + fN + 4  fm + 2  fm  .   27.40   It can be shown, by following the same procedure as in the trapezium rule case, that the error in the estimated area is approximately   cid:4    cid:31    cid:4   m odd  m even  ∆I estim.  ≈  b − a   h4 cid:20 f 4  cid:21 .  180  1004   27.4 NUMERICAL INTEGRATION  27.4.3 Gaussian integration  In the cases considered in the previous two subsections, the function f was mimicked by linear and quadratic functions. These yield exact answers if f itself is a linear or quadratic function  respectively  of x. This process could be continued by increasing the order of the polynomial mimicking-function so as to increase the accuracy with which more complicated functions f could be numerically integrated. However, the same eﬀect can be achieved with less eﬀort by not insisting upon equally spaced points xi.  The detailed analysis of such methods of numerical integration, in which the integration points are not equally spaced and the weightings given to the values at each point do not fall into a few simple groups, is too long to be given in full here. Suﬃce it to say that the methods are based upon mimicking the given function with a weighted sum of mutually orthogonal polynomials. The polynomials, Fn x , are chosen to be orthogonal with respect to a particular weight function w x , i.e.   cid:21   b  a  Fn x Fm x w x  dx = knδnm,  where kn is some constant that may depend upon n. Often the weight function is unity and the polynomials are mutually orthogonal in the most straightforward sense; this is the case for Gauss–Legendre integration for which the appropriate polynomials are the Legendre polynomials, Pn x . This particular scheme is discussed in more detail below.  Other schemes cover cases in which one or both of the integral limits a and b are not ﬁnite. For example, if the limits are 0 and ∞ and the integrand contains −αx, a simple change of variable can cast it a negative exponential function e into a form for which Gauss–Laguerre integration would be particularly well suited. This form of quadrature is based upon the Laguerre polynomials, for −x. Advantage is taken of this, which the appropriate weight function is w x  = e and the handling of the exponential factor in the integrand is eﬀectively carried out analytically. If the other factors in the integrand can be well mimicked by low-order polynomials, then a Gauss–Laguerre integration using only a modest number of points gives accurate results.  If we also add that the integral over the range −∞ to ∞ of an integrand containing an explicit factor exp −βx2  may be conveniently calculated using a  scheme based on the Hermite polynomials, the reader will appreciate the close connection between the various Gaussian quadrature schemes and the sets of eigenfunctions discussed in chapter 18. As noted above, the Gauss–Legendre scheme, which we discuss next, is just such a scheme, though its weight function, being unity throughout the range, is not explicitly displayed in the integrand.  Gauss–Legendre quadrature can be applied to integrals over any ﬁnite range though the Legendre polynomials P cid:2  x  on which it is based are only deﬁned  1005   NUMERICAL METHODS  and orthogonal over the interval −1 ≤ x ≤ 1, as discussed in subsection 18.1.2.  27.34  has to be changed to one between the limits −1 and +1. This is easily  Therefore, in order to use their properties, the integral between limits a and b in  done with a change of variable from x to z given by  z =  2x − b − a b − a  cid:21   ,  b − a  2  1  −1  so that I becomes  I =  g z  dz,   27.41   in which g z  ≡ f x . The n integration points xi for an n-point Gauss–Legendre integration are given by the zeros of Pn x , i.e. the xi are such that Pn xi  = 0. The integrand g x  is mimicked by the  n − 1 th-degree polynomial Pn x   n cid:4   G x  =   x − xi P   cid:7  n xi   g xi ,  i=1  which coincides with g x  at each of the points xi, i = 1, 2, . . . , n. To see this it should be noted that  It then follows, to the extent that g x  is well reproduced by G x , that  g xi   cid:7  n xi  P  1  −1  Pn x   x − xi  dx.   27.42   The expression  can be shown, using the properties of Legendre polynomials, to be equal to  lim x→xk  Pn x    x − xi P   cid:7  n xi   = δik.   cid:21   1  −1  g x  dx ≈ n cid:4   i=1   cid:21    cid:21   w xi  ≡ 1  cid:7  n xi   P  1  −1  Pn x   x − xi  dx  wi =  2   1 − x2  i  P  n xi 2 ,  cid:7    cid:21   1  −1  g x  dx ≈ n cid:4   wig xi .  i=1  which is thus the weighting to be attached to the factor g xi  in the sum  27.42 . The latter then becomes   27.43   In fact, because of the particular properties of Legendre polynomials, it can be  shown that  27.43  integrates exactly any polynomial of degree up to 2n− 1. The  error in the approximate equality is of the order of the 2nth derivative of g, and  1006   27.4 NUMERICAL INTEGRATION  so, provided g x  is a reasonably smooth function, the approximation is a good one.  Taking 3-point integration as an example, the three xi are the zeros of P3 x  =  2  5x3 − 3x , namely 0 and ±0.774 60, and the corresponding weights are  1   cid:6   1 × cid:5 − 3  2  2  2 =  8 9   1 − 0.6 × cid:5   2   cid:6   6 2  2 =  5 9  .  Table 27.8 gives the integration points  in the range −1 ≤ xi ≤ 1  and the  corresponding weights wi for a selection of n-point Gauss–Legendre schemes.  cid:1 Using a 3-point formula in each case, evaluate the integral  and   cid:21   I =  1  0  1  1 + x2 dx,   i  using the trapezium rule,  ii  using Simpson’s rule,  iii  using Gaussian integration. Also evaluate the integral analytically and compare the results.   cid:18   i  Using the trapezium rule, we obtain × 1 1 + 8  cid:18  × 1 1 + 16   ii  Using Simpson’s rule, we obtain  I = 1 2 = 1 4  I = 1 3 = 1 6  2  2   iii  Using Gaussian integration, we obtain   cid:21    cid:18   cid:18    cid:19   cid:19    cid:5   cid:5    cid:6   cid:6   1 2  1 2   cid:19   cid:19   f 0  + 2f 5 + 1  2  = 0.7750.  + f 1   f 0  + 4f 5 + 1  2  = 0.7833.  + f 1   2  1 − 0 ! !  I =  = 1 2  = 1 2  1  −1  dz 4  z + 1 2  1 + 1  " 0.555 56 [ f −0.774 60  + f 0.774 60 ] + 0.888 89f 0  0.555 56 [0.987 458 + 0.559 503] + 0.888 89 × 0.8  "   iv  Exact evaluation gives  = 0.785 27.   cid:21    cid:18    cid:19   I =  1  dx  1 + x2  0  =  tan  −1 x  1 0 =  π 4  = 0.785 40.  In practice, a compromise has to be struck between the accuracy of the result achieved and the calculational labour that goes into obtaining it.  cid:2   integrands involve factors of the form  1− x2   Further Gaussian quadrature procedures, ones that utilise the properties of the Chebyshev polynomials, are available for integrals over ﬁnite ranges when the ±1 2. In the same way as decreasing linear and quadratic exponentials are handled through the weight functions in Gauss–Laguerre and Gauss–Hermite quadrature, respectively, the square root  1007   NUMERICAL METHODS  Gauss–Legendre integration  f x  dx =  wi f xi    cid:21   1  −1  wi  ±xi  n = 2 0.57735 02692  n = 3 0.00000 00000 0.77459 66692  n = 4 0.33998 10436 0.86113 63116  n = 5 0.00000 00000 0.53846 93101 0.90617 98459  n = 6 0.23861 91861 0.66120 93865 0.93246 95142  n = 7 0.00000 00000 0.40584 51514 0.74153 11856 0.94910 79123  n = 8 0.18343 46425 0.52553 24099 0.79666 64774 0.96028 98565  1.00000 00000  0.88888 88889 0.55555 55556  0.65214 51549 0.34785 48451  0.56888 88889 0.47862 86705 0.23692 68851  0.46791 39346 0.36076 15730 0.17132 44924  0.41795 91837 0.38183 00505 0.27970 53915 0.12948 49662  0.36268 37834 0.31370 66459 0.22238 10345 0.10122 85363  n cid:4   i=1  ±xi  n = 9 0.00000 00000 0.32425 34234 0.61337 14327 0.83603 11073 0.96816 02395  n = 10 0.14887 43390 0.43339 53941 0.67940 95683 0.86506 33667 0.97390 65285  n = 12 0.12523 34085 0.36783 14990 0.58731 79543 0.76990 26742 0.90411 72564 0.98156 06342  n = 20 0.07652 65211 0.22778 58511 0.37370 60887 0.51086 70020 0.63605 36807 0.74633 19065 0.83911 69718 0.91223 44283 0.96397 19272 0.99312 85992  wi  0.33023 93550 0.31234 70770 0.26061 06964 0.18064 81607 0.08127 43884  0.29552 42247 0.26926 67193 0.21908 63625 0.14945 13492 0.06667 13443  0.24914 70458 0.23349 25365 0.20316 74267 0.16007 83285 0.10693 93260 0.04717 53364  0.15275 33871 0.14917 29865 0.14209 61093 0.13168 86384 0.11819 45320 0.10193 01198 0.08327 67416 0.06267 20483 0.04060 14298 0.01761 40071  Table 27.8 The integration points and weights for a number of n-point  Gauss–Legendre integration formulae. The points are given as ±xi and the contributions from both +xi and −xi must be included. However, the contri-  bution from any point xi = 0 must be counted only once.  1008   factor is treated accurately in Gauss–Chebyshev integration. Thus  27.4 NUMERICAL INTEGRATION   cid:21   dx ≈ n cid:4   i=1  1  −1  f x √ 1 − x2  wif xi ,   27.44   where the integration points xi are the zeros of the Chebyshev polynomials of the ﬁrst kind Tn x  and wi are the corresponding weights. Fortunately, both sets are analytic and can be written compactly for all n as   i − 1 2  π  n  π n  xi = cos  ,  wi =  for i = 1, . . . , n.   27.45   dealt with automatically through the weight function.  Note that, for any given n, all points are weighted equally and that no special  action is required to deal with the integrable singularities at x = ±1; they are For integrals involving factors of the form  1−x2 1 2, the corresponding formula, based on Chebyshev polynomials of the second kind Un x , is   cid:21   √ f x   1  −1  1 − x2 dx ≈ n cid:4   wif xi ,  i=1  with integration points and weights given, for i = 1, . . . , n, by  xi = cos  iπ  ,  n + 1  wi =  π  n + 1  sin2  iπ  .  n + 1  For discussions of the many other schemes available, as well as their relative merits, the reader is referred to books devoted speciﬁcally to the theory of numerical analysis. There, details of integration points and weights, as well as quantitative estimates of the error involved in replacing an integral by a ﬁnite sum, will be found. Table 27.9 gives the points and weights for a selection of Gauss–Laguerre and Gauss–Hermite schemes.  §   27.46    27.47   27.4.4 Monte Carlo methods  Surprising as it may at ﬁrst seem, random numbers may be used to carry out numerical integration. The random element comes in principally when selecting the points at which the integrand is evaluated, and naturally does not extend to the actual values of the integrand!  For the most part we will continue to use as our model one-dimensional integrals between ﬁnite limits, as typiﬁed by equation  27.34 . Extensions to cover inﬁnite or multidimensional integrals will be indicated brieﬂy at the end of the section. It should be noted here, however, that Monte Carlo methods – the name  §  They, and those presented in table 27.8 for Gauss–Legendre integration, are taken from the much more comprehensive sets to be found in M. Abramowitz and I. A. Stegun  eds , Handbook of Mathematical Functions  New York: Dover, 1965 .  1009   NUMERICAL METHODS  Gauss–Laguerre and Gauss–Hermite integration  −x f x  dx =  e  wi f xi   f x  dx =  wi f xi    cid:21  ∞  0  xi  n = 2 0.58578 64376 3.41421 35624  n = 3 0.41577 45568 2.29428 03603 6.28994 50829  n = 4 0.32254 76896 1.74576 11012 4.53662 02969 9.39507 09123  n = 5 0.26356 03197 1.41340 30591 3.59642 57710 7.08581 00059 12.6408 00844  n = 6 0.22284 66042 1.18893 21017 2.99273 63261 5.77514 35691 9.83746 74184 15.9828 73981  n = 7 0.19304 36766 1.02666 48953 2.56787 67450 4.90035 30845 8.18215 34446 12.7341 80292 19.3957 27862  n cid:4   i=1  wi  0.85355 33906 0.14644 66094  0.71109 30099 0.27851 77336 0.01038 92565  0.60315 41043 0.35741 86924 0.03888 79085 0.00053 92947  0.52175 56106 0.39866 68111 0.07594 24497 0.00361 17587 0.00002 33700  0.45896 46740 0.41700 08308 0.11337 33821 0.01039 91975 0.00026 10172 0.00000 08985  0.40931 89517 0.42183 12779 0.14712 63487 0.02063 35145 0.00107 40101 0.00001 58655 0.00000 00317   cid:21  ∞  −x2  −∞ e  ±xi  n = 2 0.70710 67812  n = 3 0.00000 00000 1.22474 48714  n = 4 0.52464 76233 1.65068 01239  n = 5 0.00000 00000 0.95857 24646 2.02018 28705  n = 6 0.43607 74119 1.33584 90740 2.35060 49737  n = 7 0.00000 00000 0.81628 78829 1.67355 16288 2.65196 13568  n = 8 0.38118 69902 1.15719 37124 1.98165 67567 2.93063 74203  n = 9 0.00000 00000 0.72355 10188 1.46855 32892 2.26658 05845 3.19099 32018  n cid:4   i=1  wi  0.88622 69255  1.18163 59006 0.29540 89752  0.80491 40900 0.08131 28354  0.94530 87205 0.39361 93232 0.01995 32421  0.72462 95952 0.15706 73203 0.00453 00099  0.81026 46176 0.42560 72526 0.05451 55828 0.00097 17812  0.66114 70126 0.20780 23258 0.01707 79830 0.00019 96041  0.72023 52156 0.43265 15590 0.08847 45274 0.00494 36243 0.00003 96070  Table 27.9 The integration points and weights for a number of n-point Gauss–Laguerre and Gauss–Hermite integration formulae. Where the points  are given as ±xi, the contributions from both +xi and −xi must be included.  However, the contribution from any point xi = 0 must be counted only once.  1010   27.4 NUMERICAL INTEGRATION  has become attached to methods based on randomly generated numbers – in many ways come into their own when used on multidimensional integrals over regions with complicated boundaries.  It goes without saying that in order to use random numbers for calculational purposes a supply of them must be available. There was a time when they were provided in book form as a two-dimensional array of random digits in the range 0 to 9. The user could generate the successive digits of a random number of any desired length by selecting their positions in the table in any predetermined and systematic way. Nowadays all computers and nearly all pocket calculators oﬀer a function which supplies a sequence of decimal numbers, ξ, that, for all  practical purposes, are randomly and uniformly chosen in the range 0 ≤ ξ < 1.  The maximum number of signiﬁcant ﬁgures available in each random number depends on the precision of the generating device. We will defer the details of how these numbers are produced until later in this subsection, where it will also be shown how random numbers distributed in a prescribed way can be generated.  All integrals of the general form shown in equation  27.34  can, by a suitable  change of variable, be brought to the form  θ =  f x  dx,   27.48   and we will use this as our standard model.  All approaches to integral evaluation based on random numbers proceed by estimating a quantity whose expectation value is equal to the sought-for value θ. The estimator t must be unbiased, i.e. we must have E[t] = θ, and the method must provide some measure of the likely error in the result. The latter will appear generally as the variance of the estimate, with its usual statistical interpretation, and not as a band in which the true answer is known to lie with certainty.  The various approaches really diﬀer from each other only in the degree of sophistication employed to keep the variance of the estimate of θ small. The overall eﬃciency of any particular method has to take into account not only the variance of the estimate but also the computing and book-keeping eﬀort required to achieve it.  We do not have the space to describe even the most elementary methods in full detail, but the main thrust of each approach should be apparent to the reader from the brief descriptions that follow.   cid:21   1  0  The most straightforward application is one in which the random numbers are used to pick sample points at which f x  is evaluated. These values are then  Crude Monte Carlo  1011   averaged:  NUMERICAL METHODS  n cid:4   i=1  t =  1 n  f ξi .  Stratiﬁed sampling   27.49    cid:17   2  f x  dx  .   cid:16  cid:21   αj  αj−1  and crude Monte Carlo evaluation is carried out in each subrange. The estimate E[t] is then calculated as   cid:5   cid:6  αj−1 + ξij  αj − αj−1   .   27.50   Here the range of x is broken up into k subranges,  k cid:4   j=1  0 = α0 < α1 < ··· < αk = 1, nj cid:4  αj − αj−1  cid:21   [f x ]2 dx − k cid:4   cid:11   1 nj  αj−1  j=1  nj  i=1  f  αj  E[t] =  k cid:4   j=1  αj − αj−1  nj  σ2 t =  This is an unbiased estimator of θ with variance  This variance can be made less than that for crude Monte Carlo, whilst using nj, if the diﬀerences between the same total number of random numbers, n = the average values of f x  in the various subranges are signiﬁcantly greater than the variations in f within each subrange. It is easier administratively to make all subranges equal in length, but better, if it can be managed, to make them such that the variations in f are approximately equal in all the individual subranges.  Importance sampling  Although we cannot integrate f x  analytically – we would not be using Monte Carlo methods if we could – if we can ﬁnd another function g x  that can be integrated analytically and mimics the shape of f then the variance in the estimate of θ can be reduced signiﬁcantly compared with that resulting from the use of crude Monte Carlo evaluation.  Firstly, if necessary the function g must be renormalised, so that G x  = x 0 g y dy has the property G 1  = 1. Clearly, it also has the property G 0  = 0. Then, since   cid:1    cid:21   θ =  dG x ,  1  0  f x  g x   it follows that ﬁnding the expectation value of f η  g η  using a random number η, distributed in such a way that ξ = G η  is uniformly distributed on  0, 1 , is equivalent to estimating θ. This involves being able to ﬁnd the inverse function of G; a discussion of how to do this is given towards the end of this subsection. If g η  mimics f η  well, f η  g η  will be nearly constant and the estimation  1012   27.4 NUMERICAL INTEGRATION  will have a very small variance. Further, any error in inverting the relationship between η and ξ will not be important since f η  g η  will be largely independent of the value of η. −1 x ]1 2, which is not analyt- ically integrable over the range  0, 1  but is well mimicked by the easily integrated function g x  = x1 2 1 − x2 6 . The ratio of the two varies from 1.00 to 1.06 as x  As an example, consider the function f x  = [tan  varies from 0 to 1. The integral of g over this range is 0.619 048, and so it has to be renormalised by the factor 1.615 38. The value of the integral of f x  from 0 to 1 can then be estimated by averaging the value of  −1 η ]1 2  1.615 38  η1 2 1 − 1  [tan  6 η2   for random variables η which are such that G η  is uniformly distributed on  0, 1 . Using batches of as few as ten random numbers gave a value 0.630 for θ, with standard deviation 0.003. The corresponding result for crude Monte Carlo,  using the same random numbers, was 0.634 ± 0.065. The increase in precision is  obvious, though the additional labour involved would not be justiﬁed for a single application.  Control variates  The control-variate method is similar to, but not the same as, importance sam- pling. Again, an analytically integrable function that mimics f x  in shape has to be found. The function, known as the control variate, is ﬁrst scaled so as to match f as closely as possible in magnitude and then its integral is found in closed form. If we denote the scaled control variate by h x , then the estimate of θ is computed as  1  [f x  − h x ] dx +  h x  dx.   27.51    cid:21   t =  0   cid:21   1  0  The ﬁrst integral in  27.51  is evaluated using  crude  Monte Carlo, whilst the second is known analytically. Although the ﬁrst integral should have been ren- dered small by the choice of h x , it is its variance that matters. The method relies on the following result  see equation  30.136  :  V [t − t  cid:7   ] − 2 Cov[t, t  cid:7   cid:7    cid:7  ] = V [t] + V [t  cid:7  and on the fact that if t estimates θ whilst t  cid:7  numbers, then the covariance of t and t and indeed will be so if the integrands producing θ and θ  using the same random  cid:7  can be larger than the variance of t , are highly correlated. To evaluate the same integral as was estimated previously using importance sampling, we take as h x  the function g x  used there, before it was renormalised. Again using batches of ten random numbers, the estimated value for θ was found  to be 0.629 ± 0.004, a result almost identical to that obtained using importance  estimates θ  ],   cid:7   1013   NUMERICAL METHODS  sampling, in both value and precision. Since we knew already that f x  and g x  diverge monotonically by about 6% as x varies over the range  0, 1 , we could have made a small improvement to our control variate by scaling it by 1.03 before using it in equation  27.51 .  Antithetic variates  As a ﬁnal example of a method that improves on crude Monte Carlo, and one that is particularly useful when monotonic functions are to be integrated, we mention the use of antithetic variates. This method relies on ﬁnding two estimates t and  cid:7  ] is large and negative  and t using the result   cid:7  of θ that are strongly anticorrelated  i.e. Cov[t, t  V [ 1  ] + 1   ] = 1   cid:7  4 V [t  4 V [t] + 1   cid:7  2 Cov[t, t   cid:7  2  t + t 2 [f ξ  + f 1 − ξ ] instead of f ξ  involves only twice  For example, the use of 1 as many evaluations of f, and no more random variables, but generally gives an improvement in precision signiﬁcantly greater than this. For the integral of −1 x ]1 2, using as previously a batch of ten random variables, an f x  = [tan estimate of 0.623 ± 0.018 was found. This is to be compared with the crude Monte Carlo result, 0.634 ± 0.065, obtained using the same number of random  ].  variables.  For a fuller discussion of these methods, and of theoretical estimates of their eﬃciencies, the reader is referred to more specialist treatments. For practical imple- § mentation schemes, a book dedicated to scientiﬁc computing should be consulted.  Hit or miss method  We now come to the approach that, in spirit, is closest to the activities that gave Monte Carlo methods their name. In this approach, one or more straightforward yes no decisions are made on the basis of numbers drawn at random – the end result of each trial is either a hit or a miss! In this section we are concerned with numerical integration, but the general Monte Carlo approach, in which one estimates a physical quantity that is hard or impossible to calculate directly by simulating the physical processes that determine it, is widespread in modern science. For example, the calculation of the eﬃciencies of detector arrays in experiments to study elementary particle interactions are nearly always carried out in this way. Indeed, in a normal experiment, far more simulated interactions are generated in computers than ever actually occur when the experiment is taking real data.  As was noted in chapter 2, the process of evaluating a one-dimensional integral b a f x dx can be regarded as that of ﬁnding the area between the curve y = f x   e.g. W. H. Press, S. A. Teukolsky, W. T. Vetterling and B. P. Flannery, Numerical Recipes in C: The Art of Scientiﬁc Computing, 2nd edn  Cambridge: Cambridge University Press, 1992 .   cid:1   §  1014   27.4 NUMERICAL INTEGRATION  y = f x   y = c   cid:1   x = a  x = b  x  Figure 27.5 A simple rectangular ﬁgure enclosing the area  shown shaded  which is equal to  b  a f x  dx.  and the x-axis in the range a ≤ x ≤ b. It may not be possible to do this  analytically, but if, as shown in ﬁgure 27.5, we can enclose the curve in a simple ﬁgure whose area can be found trivially then the ratio of the required  shaded   area to that of the bounding ﬁgure, c b− a , is the same as the probability that a  randomly selected point inside the boundary will lie below the line.  In order to accommodate cases in which f x  can be negative in part of the  x-range, we treat a slightly more general case. Suppose that, for a ≤ x ≤ b, f x  is bounded and known to lie in the range A ≤ f x  ≤ B; then the transformation  will reduce the integral  z =  x − a  cid:1  b − a b a f x  dx to the form A b − a  +  B − A  b − a    cid:21   1  0  h z  dz,   27.52   where  h z  =  1  B − A  [f   b − a z + a  − A] .  In this form z lies in the range 0 ≤ z ≤ 1 and h z  lies in the range 0 ≤ h z  ≤ 1,  i.e. both are suitable for simulation using the standard random-number generator. It should be noted that, for an eﬃcient estimation, the bounds A and B should be drawn as tightly as possible –preferably, but not necessarily, they should be equal to the minimum and maximum values of f in the range. The reason for this is that random numbers corresponding to values which f x  cannot reach add nothing to the estimation but do increase its variance.  It only remains to estimate the ﬁnal integral on the RHS of equation  27.52 . This we do by selecting pairs of random numbers, ξ1 and ξ2, and testing whether  1015   NUMERICAL METHODS  h ξ1  > ξ2. The fraction of times that this inequality is satisﬁed estimates the value  of the integral  without the scaling factors  B − A  b − a   since the expectation  value of this fraction is the ratio of the area below the curve y = h z  to the area of a unit square.  To illustrate the evaluation of multiple integrals using Monte Carlo techniques, consider the relatively elementary problem of ﬁnding the volume of an irregular solid bounded by planes, say an octahedron. In order to keep the description brief, but at the same time illustrate the general principles involved, let us suppose that the octahedron has two vertices on each of the three Cartesian axes, one on either side of the origin for each axis. Denote those on the x-axis by x1 < 0  and x2 > 0 , and similarly for the y- and z-axes. Then the whole of the octahedron can be enclosed by the rectangular parallelepiped  x1 ≤ x ≤ x2,  y1 ≤ y ≤ y2,  z1 ≤ z ≤ z2.  Any point in the octahedron lies inside or on the parallelepiped, but any point in the parallelepiped may or may not lie inside the octahedron.  The equation of the plane containing the three vertex points  xi, 0, 0 ,  0, yj , 0   and  0, 0, zk  is  x xi  y yj  z zk  +  +  = 1  for i, j, k = 1, 2,   27.53   and the condition that any general point  x, y, z  lies on the same side of the plane as the origin is that  x xi  +  +  y yj  z zk  − 1 ≤ 0.   27.54   For the point to be inside or on the octahedron, equation  27.54  must therefore be satisﬁed for all eight of the sets of i, j and k given in  27.53 .  Thus an estimate of the volume of the octahedron can be made by generating random numbers ξ from the usual uniform distribution and then using them in sets of three, according to the following scheme.  With integer m labelling the mth set of three random numbers, calculate  x = x1 + ξ3m−2 x2 − x1 , y = y1 + ξ3m−1 y2 − y1 , z = z1 + ξ3m z2 − z1 .  Deﬁne a variable nm as 1 if  27.54  is satisﬁed for all eight combinations of i, j, k values and as 0 otherwise. The volume V can then be estimated using 3M random numbers from the formula  M cid:4   m=1   x2 − x1  y2 − y1  z2 − z1   V  =  1 M  nm.  1016   27.4 NUMERICAL INTEGRATION  It will be seen that, by replacing each nm in the summation by f x, y, z nm, this procedure could be extended to estimate the integral of the function f over the volume of the solid. The method has special valueif f is too complicated to have analytic integrals with respect to x, y and z or if the limits of any of these integrals are determined by anything other than the simplest combinations of the other variables. If large values of f are known to be concentrated in particular regions of the integration volume, then some form of stratiﬁed sampling should be used.  It will be apparent that this general method can be extended to integrals of general functions, bounded but not necessarily continuous, over volumes with complicated bounding surfaces and, if appropriate, in more than three dimensions.  Random number generation  Earlier in this subsection we showed how to evaluate integrals using sequences of  numbers that we took to be distributed uniformly on the interval 0 ≤ ξ < 1. In  reality the sequence of numbers is not truly random, since each is generated in a mechanistic way from its predecessor and eventually the sequence will repeat itself. However, the cycle is so long that in practice this is unlikely to be a problem, and the reproducibility of the sequence can even be turned to advantage when checking the accuracy of the rest of a calculational program. Much research has gone into the best ways to produce such ‘pseudo-random’ sequences of numbers. We do not have space to pursue them here and will limit ourselves to one recipe that works well in practice.  Given any particular starting  integer  value x0, the following algorithm will  generate a full cycle of m values for ξi, uniformly distributed on 0 ≤ ξi < 1,  before repeats appear:  xi = axi−1 + c   mod m ;  ξi =  xi m  .  Here c is an odd integer and a has the form a = 4k + 1, with k an integer. For practical reasons, in computers and calculators m is taken as a  fairly high  power of 2, typically the 32nd power.  The uniform distribution can be used to generate random numbers y distributed  according to a more general probability distribution f y  on the range a ≤ y ≤ b  if the inverse of the indeﬁnite integral of f can be found, either analytically or by means of a look-up table. In other words, if  for which F a  = 0 and F b  = 1, then F y  is uniformly distributed on  0, 1 .  This approach is not limited to ﬁnite a and b; a could be −∞ and b could be ∞.  The procedure is thus to select a random number ξ from a uniform distribution  F y  =  f t  dt,   cid:21   y  a  1017   NUMERICAL METHODS  on  0, 1  and then take as the random number y the value of F illustrate this with a worked example.   cid:1 Find an explicit formula that will generate a random number y distributed on  −∞,∞   according to the Cauchy distribution  −1 ξ . We now   cid:9    cid:10   a π  dy  a2 + y2 , given a random number ξ uniformly distributed on  0, 1 .  f y  dy =  The ﬁrst task is to determine the indeﬁnite integral:   cid:21    cid:9    cid:10   F y  =  y  −∞  a π  dt  a2 + t2  =  tan  1 π  −1 y a  +  1 2  .  Now, if y is distributed as we wish then F y  is uniformly distributed on  0, 1 . This follows from the fact that the derivative of F y  is f y . We therefore set F y  equal to ξ and obtain  yielding  ξ =  tan  1 π  −1 y a  +  1 2  ,  y = a tan[π ξ − 1 2  ].  This explicit formula shows how to change a random number ξ drawn from a population uniformly distributed on  0, 1  into a random number y distributed according to the Cauchy distribution.  cid:2   Look-up tables operate as described below for cumulative distributions F y  −1 y  cannot be expressed in closed form. They  that are non-invertible, i.e. F are especially useful if many random numbers are needed but great sampling accuracy is not essential. The method for an N-entry table can be summarised as follows. Deﬁne wm by F wm  = m N for m = 1, 2, . . . , N, and store a table of  y m  = 1  2  wm + wm−1 .  As each random number y is needed, calculate k as the integral part of Nξ and take y as given by y k .  Normally, such a look-up table would have to be used for generating random numbers with a Gaussian distribution, as the cumulative integral of a Gaussian is non-invertible. It would be, in essence, table 30.3, with the roles of argument and value interchanged. In this particular case, an alternative, based on the central limit theorem, can be considered.  With ξi generated in the usual way, i.e. uniformly distributed on the interval 0 ≤ ξ < 1, the random variable   27.55   is normally distributed with mean 0 and variance n 12 when n is large. This approach does produce a continuous spectrum of possible values for y, but needs  n cid:4   i=1  y =  ξi − 1 2 n  1018   27.5 FINITE DIFFERENCES  many values of ξi for each value of y and is a very poor approximation if the wings of the Gaussian distribution have to be sampled accurately. For nearly all practical purposes a Gaussian look-up table is to be preferred.  27.5 Finite diﬀerences  It will have been noticed that earlier sections included several equations linking sequential values of fi and the derivatives of f evaluated at one of the xi. In this section, by way of preparation for the numerical treatment of diﬀerential equations, we establish these relationships in a more systematic way.  Again we consider a set of values fi of a function f x  evaluated at equally spaced points xi, their separation being h. As before, the basis for our discussion will be a Taylor series expansion, but on this occasion about the point xi:  fi±1 = fi ± hf   cid:7  i +   cid:7  cid:7   f i  h2 2!  ± h3 3!  i + ··· . f 3    27.56   i  .  In this section, and subsequently, we denote the nth derivative evaluated at xi by f n  From  27.56 , three diﬀerent expressions that approximate f 1  The ﬁrst of these, obtained by subtracting the ± equations, is − ··· .  fi+1 − fi−1  can be derived.   27.57    cid:8    cid:7   ≡  =  i  f 1  i  − h2 3!  f 3  i  The quantity  fi+1 − fi−1   2h  is known as the central diﬀerence approximation and can be seen from  27.57  to be in error by approximately  h2 6 f 3  to f 1   .  df dx  2h  i  i  An alternative approximation, obtained from  27.56+  alone, is given by  xi   cid:8    cid:7   ≡  f 1  i  df dx  =  xi  fi+1 − fi  h  − h 2!  f 2  i  − ··· .  The forward diﬀerence approximation,  fi+1 − fi  h, is clearly a poorer approxi- as compared with  h2 6 f 3  mation, since it is in error by approximately  h 2 f 2  Similarly, the backward diﬀerence  fi − fi−1  h obtained from  27.56−  is not as  .  i  i  good as the central diﬀerence; the sign of the error is reversed in this case.  This type of diﬀerencing approximation can be continued to the higher deriva-  tives of f in an obvious manner. By adding the two equations  27.56± , a central diﬀerence approximation to f 2   can be obtained:   27.58   ≡  f 2  i  ≈ fi+1 − 2fi + fi−1  .  h2   27.59   The error in this approximation  also known as the second diﬀerence of f  is easily shown to be about  h2 12 f 4   .  Of course, if the function f x  is a suﬃciently simple polynomial in x, all   cid:7   i   cid:8   d2f dx2  i  1019   NUMERICAL METHODS  derivatives beyond a particular one will vanish and there is no error in taking the diﬀerences to obtain the derivatives.  cid:1 The following is copied from the tabulation of a second-degree polynomial f x  at values of x from 1 to 12 inclusive:  2, 2, ?, 8, 14, 22, 32, 46, ?, 74, 92, 112.  The entries marked ? were illegible and in addition one error was made in transcription. Complete and correct the table. Would your procedure have worked if the copying error had been in f 6 ?  Write out the entries again in row  a  below, and where possible calculate ﬁrst diﬀerences in row  b  and second diﬀerences in row  c . Denote the jth entry in row  n  by  n j.   a  2  b   c   0  ?  ?  6  8  10  14  ?  ?  18  20  2  ?  ?  ?  8  ?  14  2  22  2  32  4  46  ?  ?  ?  74  ?  92  2  112  Because the polynomial is second-degree, the second diﬀerences  c j, which are proportional to d2f dx2, should be constant, and clearly the constant should be 2. That is,  c 6 should equal 2 and  b 7 should equal 12  not 14 . Since all the  c j = 2, we can conclude that  b 2 = 2,  b 3 = 4,  b 8 = 14, and  b 9 = 16. Working these changes back to row  a  shows that  a 3 = 4,  a 8 = 44  not 46 , and  a 9 = 58.  The entries therefore should read   a  2, 2, 4, 8, 14, 22, 32, 44, 58, 74, 92, 112,  where the amended entries are shown in bold type.  It is easily veriﬁed that if the error were in f 6  no two computable entries in row  c  would be equal, and it would not be clear what the correct common entry should be. Nevertheless, trial and error might arrive at a self-consistent scheme.  cid:2   27.6 Diﬀerential equations  For the remaining sections of this chapter our attention will be on the solution of diﬀerential equations by numerical methods. Some of the general diﬃculties of applying numerical methods to diﬀerential equations will be all too apparent. Initially we consider only the simplest kind of equation – one of ﬁrst order, typically represented by  dy dx  = f x, y ,   27.60   where y is taken as the dependent variable and x the independent one. If this equation can be solved analytically then that is the best course to adopt. But sometimes it is not possible to do so and a numerical approach becomes the only one available. In fact, most of the examples that we will use can be solved easily by an explicit integration, but, for the purposes of illustration, this is an advantage rather than the reverse since useful comparisons can then be made between the numerically derived solution and the exact one.  1020   27.6 DIFFERENTIAL EQUATIONS  x  0 0.5 1.0 1.5 2.0 2.5 3.0  0.01  1   0.605 0.366 0.221 0.134 0.081 0.049  0.1  1   0.590 0.349 0.206 0.122 0.072 0.042  0.5  1   0.500 0.250 0.125 0.063 0.032 0.016  h 1.0  1  0 0 0 0 0 0  1.5  1   2  1   3  1   0.250  −0.500 −1 −2 −0.125 −1 −8 −0.032 −1 −32  0.063  16  1  4  1  0.016  1  64  y exact    1  0.607 0.368 0.223 0.135 0.082 0.050  Table 27.10 The solution y of diﬀerential equation  27.61  using the Euler forward diﬀerence method for various values of h. The exact solution is also shown.  27.6.1 Difference equations  Consider the diﬀerential equation  = −y,  cid:8   dy dx   cid:7   ,  xi  h  ≈ yi+1 − yi  cid:8   dy dx   cid:7   y 0  = 1,   27.61   and the possibility of solving it numerically by approximating dy dx by a ﬁnite diﬀerence along the lines indicated in section 27.5. We start with the forward diﬀerence   27.62    27.63   where we use the notation of section 27.5 but with f replaced by y. In this particular case, it leads to the recurrence relation  yi+1 = yi + h  = yi − hyi =  1 − h yi.  dy dx  i  Thus, since y0 = y 0  = 1 is given, y1 = y 0 + h  = y h  can be calculated, and so on  this is the Euler method . Table 27.10 shows the values of y x  obtained if this is done using various values of h and for selected values of x. The exact  solution, y x  = exp −x , is also shown.  It is clear that to maintain anything like a reasonable accuracy only very small steps, h, can be used. Indeed, if h is taken to be too large, not only is the accuracy bad but, as can be seen, for h > 1 the calculated solution oscillates  when it should be monotonic , and for h > 2 it diverges. Equation  27.63  is of the form  yi+1 = λyi, and a necessary condition for non-divergence is λ < 1, i.e. 0 < h < 2,  though in no way does this ensure accuracy.  Part of this diﬃculty arises from the poor approximation  27.62 ; its right- hand side is a closer approximation to dy dx evaluated at x = xi + h 2 than to dy dx at x = xi. This is the result of using a forward diﬀerence rather than the  1021   NUMERICAL METHODS  x  −0.5  0 0.5 1.0 1.5 2.0 2.5  y estim.   1.648   1.000  0.648 0.352 0.296 0.056 0.240  3.0 −0.184  y exact  —  1.000  0.607 0.368 0.223 0.135 0.082 0.050   cid:7    cid:8   dy dx  i  yi+1 = yi−1 + 2h  yi+1 = yi−1 − 2hyi.  Table 27.11 The solution of diﬀerential equation  27.61  using the Milne central diﬀerence method with h = 0.5 and accurate starting values.  more accurate, but of course still approximate, central diﬀerence. A more accurate method based on central diﬀerences  Milne’s method  gives the recurrence relation   27.64    27.65   in general and, in this particular case,  An additional diﬃculty now arises, since two initial values of y are needed. The second must be estimated by other means  e.g. by using a Taylor series, as discussed later , but for illustration purposes we will take the accurate value,  y −h  = exp h, as the value of y−1. If h is taken as, say, 0.5 and  27.65  is applied  repeatedly, then the results shown in table 27.11 are obtained.  Although some improvement in the early values of the calculated y x  is noticeable, as compared with the corresponding  h = 0.5  column of table 27.10, this scheme soon runs into diﬃculties, as is obvious from the last two rows of the table.  Some part of this poor performance is not really attributable to the approxi- mations made in estimating dy dx but to the form of the equation itself and hence of its solution. Any rounding error occurring in the evaluation eﬀectively introduces into y some contamination by the solution of  This equation has the solution y x  = exp x and so grows without limit; ultimately it will dominate the sought-for solution and thus render the calculations totally inaccurate.  We have only illustrated, rather than analysed, some of the diﬃculties associated with simple ﬁnite-diﬀerence iteration schemes for ﬁrst-order diﬀerential equations,  dy dx  = +y.  1022   27.6 DIFFERENTIAL EQUATIONS  but they may be summarised as  i  insuﬃciently precise approximations to the derivatives and  ii  inherent instability due to rounding errors.  27.6.2 Taylor series solutions  Since a Taylor series expansion is exact if all its terms are included, and the limits of convergence are not exceeded, we may seek to use one to evaluate y1, y2, etc. for an equation  dy dx  = f x, y ,   27.66   when the initial value y x0  = y0 is given.  The Taylor series is  y x + h  = y x  + hy   x  +  y   x  +   27.67    cid:7  cid:7   h2 2!  y 3  x  + ··· .  h3 3!  In the present notation, at the point x = xi this is written  yi+1 = yi + hy 1   i +  y 2  i +  i + ··· . y 3   h3 3!   27.68   But, for the required solution y x , we know that   cid:7    cid:7   h2 2!   cid:8   dy dx  xi  ≡  y 1  i  = f xi, yi ,   27.69   and the value of the second derivative at x = xi, y = yi can be obtained from it:  y 2  i =  ∂f ∂x  +  ∂f ∂y  dy dx  =  + f  ∂f ∂x  ∂f ∂y  .   27.70   This process can be continued for the third and higher derivatives, all of which are to be evaluated at  xi, yi .  Having obtained expressions for the derivatives y n   in  27.67 , two alternative  i  ways of proceeding are open to us:   i  equation  27.68  is used to evaluate yi+1, the whole process is repeated to  obtain yi+2, and so on;   ii  equation  27.68  is applied several times but using a diﬀerent value of h  each time, and so the corresponding values of y x + h  are obtained.  It is clear that, on the one hand, approach  i  does not require so many terms of  27.67  to be kept, but, on the other hand, the yi n  have to be recalculated at each step. With approach  ii , fairly accurate results for y may be obtained for values of x close to the given starting value, but for large values of h a large number of terms of  27.67  must be kept. As an example of approach  ii  we solve the following problem.  1023   NUMERICAL METHODS  y estim.  1.0000 1.2346 1.5619 2.0331 2.7254 3.7500  y exact  1.0000 1.2346 1.5625 2.0408 2.7778 4.0000  x 0 0.1 0.2 0.3 0.4 0.5  dy dx  y n+1  i  =  ∂y n  ∂y  dy dx  = f  ∂y n  ∂y  .  y 0  = 1  cid:7   cid:7  cid:7   = 2y3 2  y  y  =  3 2  2y1 2  2y3 2  = 6y2  y 3  =  12y 2y3 2 = 24y5 2  y 4  =  60y3 2 2y3 2 = 120y3  y 5  =  360y2 2y3 2 = 720y7 2  1  2  6  24  120  720  Table 27.12 The solution of diﬀerential equation  27.71  using a Taylor series.   cid:1 Find the numerical solution of the equation  = 2y3 2,  y 0  = 1,   27.71   for x = 0.1 to 0.5 in steps of 0.1. Compare it with the exact solution obtained analytically.  Since the right-hand side of the equation does not contain x explicitly,  27.70  is greatly simpliﬁed and the calculation becomes a repeated application of  The necessary derivatives and their values at x = 0, where y = 1, are given below:  Thus the Taylor expansion of the solution about the origin  in fact a Maclaurin series  is  y x  = 1 + 2x +  x2 +  x3 +  x4 +  6 2!  24 3!  120 4!  x5 + ··· .  720 5!  Hence, y estim.  = 1 + 2x + 3x2 + 4x3 + 5x4 + 6x5. Values calculated from this are given in table 27.12. Comparison with the exact values shows that using the ﬁrst six terms gives a value that is correct to one part in 100, up to x = 0.3.  cid:2   27.6.3 Prediction and correction  An improvement in the accuracy obtainable using diﬀerence methods is possible if steps are taken, sometimes retrospectively, to allow for inaccuracies in approx- imating derivatives by diﬀerences. We will describe only the simplest schemes of this kind and begin with a prediction method, usually called the Adams method.  1024   27.6 DIFFERENTIAL EQUATIONS   cid:7    cid:8   dy dx  The forward diﬀerence estimate of yi+1, namely  yi+1 = yi + h  = yi + hf xi, yi ,   27.72   would give exact results if y were a linear function of x in the range xi ≤ x ≤ xi+h.  i  The idea behind the Adams method is to allow some relaxation of this and suppose that y can be adequately approximated by a parabola over the interval  xi−1 ≤ x ≤ xi+1. In the same interval, dy dx can then be approximated by a linear  function:  f x, y  =  dy dx  ≈ a + b x − xi   for xi − h ≤ x ≤ xi + h.  The values of a and b are ﬁxed by the calculated values of f at xi−1 and xi, which we may denote by fi−1 and fi:  fi − fi−1  h  a = fi,  b =   cid:21    cid:13   xi+h  xi  fi +   fi − fi−1   h  .   cid:14   x − xi   dx,  yi+1 − yi ≈  Thus  which yields  yi+1 = yi + hfi + 1  2 h fi − fi−1 .   27.73   The last term of this expression is seen to be a correction to result  27.72 . That it is, in some sense, the second-order correction,  1  2 h2y 2   i−1 2,  to a ﬁrst-order formula is apparent.  Such a procedure requires, in addition to a value for y0, a value for either y1 or y−1, so that f1 or f−1 can be used to initiate the iteration. This has to be obtained by other methods, e.g. a Taylor series expansion.  Improvements to simple diﬀerence formulae can also be obtained by using correction methods. In these, a rough prediction of the value yi+1 is made ﬁrst, and then this is used in a better formula, not originally usable since it, in turn, requires a value of yi+1 for its evaluation. The value of yi+1 is then recalculated, using this better formula.  Such a scheme based on the forward diﬀerence formula might be as follows:   i  predict yi+1 using yi+1 = yi + hfi;  ii  calculate fi+1 using this value;  iii  recalculate yi+1 using yi+1 = yi + h fi + fi+1  2. Here  fi + fi+1  2 has replaced the fi used in  i , since it better represents the average value of  dy dx in the interval xi ≤ x ≤ xi + h.  1025   NUMERICAL METHODS  Steps  ii  and  iii  can be iterated to improve further the approximation to the average value of dy dx, but this will not compensate for the omission of higher- order derivatives in the forward diﬀerence formula.  Many more complex schemes of prediction and correction,  in most cases combining the two in the same process, have been devised, but the reader is referred to more specialist texts for discussions of them. However, because it oﬀers some clear advantages, one group of methods will be set out explicitly in the next subsection. This is the general class of schemes known as Runge–Kutta methods.  27.6.4 Runge–Kutta methods  The Runge–Kutta method of integrating  dy dx  = f x, y    27.74   is a step-by-step process of obtaining an approximation for yi+1 by starting from the value of yi. Among its advantages are that no functions other than f are used, no subsidiary diﬀerentiation is needed and no additional starting values need be calculated.  To be set against these advantages is the fact that f is evaluated using somewhat complicated arguments and that this has to be done several times for each increase in the value of i. However, once a procedure has been established, for example on a computer, the method usually gives good results.  The basis of the method is to simulate the  accurate  Taylor series for y xi + h , not by calculating all the higher derivatives of y at the point xi but by taking a particular combination of the values of the ﬁrst derivative of y evaluated at a number of carefully chosen points. Equation  27.74  is used to evaluate these derivatives. The accuracy can be made to be up to whatever power of h is desired, but, naturally, the greater the accuracy, the more complex the calculation, and, in any case, rounding errors cannot ultimately be avoided.  The setting up of the calculational scheme may be illustrated by considering the particular case in which second-order accuracy in h is required. To second order, the Taylor expansion is   27.75    cid:8   df dx  ,  xi   cid:7   cid:8   h2 2  yi+1 = yi + hfi +   cid:7    cid:8    cid:7   df dx  =  xi  ∂f ∂x  + f  ∂f ∂y  xi  ≡ ∂fi  ∂x  + fi  ∂fi ∂y  ,  where  the last step being merely the deﬁnition of an abbreviated notation.  1026   27.6 DIFFERENTIAL EQUATIONS  We assume that this can be simulated by a form  yi+1 = yi + α1hfi + α2hf xi + β1h, yi + β2hfi ,   27.76   which in eﬀect uses a weighted mean of the value of dy dx at xi and its value at some point yet to be determined. The object is to choose values of α1, α2, β1 and β2 such that  27.76  coincides with  27.75  up to the coeﬃcient of h2.  Expanding the function f in the last term of  27.76  in a Taylor series of its  own, we obtain  f xi + β1h, yi + β2hfi  = f xi, yi  + β1h  + β2hfi  ∂fi ∂x   cid:7   ∂fi ∂y  + O h2 .   cid:8   Putting this result into  27.76  and rearranging in powers of h, we obtain  yi+1 = yi +  α1 + α2 hfi + α2h2  β1  + β2fi  .   27.77   ∂fi ∂x  ∂fi ∂y  Comparing this with  27.75  shows that there is, in fact, some freedom remaining  in the choice of the α’s and β’s. In terms of an arbitrary α1   cid:3 = 1 ,  α2 = 1 − α1,  β1 = β2 =  1  2 1 − α1   .  One possible choice is α1 = 0.5, giving α2 = 0.5, β1 = β2 = 1. In this case the procedure  equation  27.76   can be summarised by  where  yi+1 = yi + 1  2  a1 + a2 ,   27.78   a1 = hf xi, yi , a2 = hf xi + h, yi + a1 .  Similar schemes giving higher-order accuracy in h can be devised. Two such schemes, given without derivation, are as follows.   i  To order h3,  where  yi+1 = yi + 1  6  b1 + 4b2 + b3 ,   27.79   b1 = hf xi, yi , b2 = hf xi + 1 b3 = hf xi + h, yi + 2b2 − b1 .  2 h, yi + 1  2 b1 ,  1027    ii  To order h4,  where  yi+1 = yi + 1  6  c1 + 2c2 + 2c3 + c4 ,   27.80   NUMERICAL METHODS  c1 = hf xi, yi , 2 h, yi + 1 c2 = hf xi + 1 c3 = hf xi + 1 2 h, yi + 1 c4 = hf xi + h, yi + c3 .  2 c1 , 2 c2 ,  27.6.5 Isoclines  The ﬁnal method to be described for ﬁrst-order diﬀerential equations is not so much numerical as graphical, but since it is sometimes useful it is included here. The method, known as that of isoclines, involves sketching for a number of values of a parameter c those curves  the isoclines  in the xy-plane along which f x, y  = c, i.e. those curves along which dy dx is a constant of known value. It should be noted that isoclines are not generally straight lines. Since a straight line of slope dy dx at and through any particular point is a tangent to the curve y = y x  at that point, small elements of straight lines, with slopes appropriate to the isoclines they cut, eﬀectively form the curve y = y x .  Figure 27.6 illustrates in outline the method as applied to the solution of  = −2xy.  dy dx   27.81   The thinner curves  rectangular hyperbolae  are a selection of the isoclines along  which −2xy is constant and equal to the corresponding value of c. The small  cross lines on each curve show the slopes  = c  that solutions of  27.81  must have if they cross the curve. The thick line is the solution for which y = 1 at x = 0; it takes the slope dictated by the value of c on each isocline it crosses. The  analytic solution with these properties is y x  = exp −x2 .  27.7 Higher-order equations  So far the discussion of numerical solutions of diﬀerential equations has been in terms of one dependent and one independent variable related by a ﬁrst-order equation. It is straightforward to carry out an extension to the case of several dependent variables y[r] governed by R ﬁrst-order equations:  dy[r] dx  = f[r] x, y[1], y[2], . . . , y[R] ,  r = 1, 2, . . . , R.  We have enclosed the label r in brackets so that there is no confusion between, say, the second dependent variable y[2] and the value y2 of a variable y at the  1028   27.7 HIGHER-ORDER EQUATIONS  y 1.0  0.8  0.6 y  0.4  0.2  c  −1.0−0.8−0.6−0.4−0.2−0.1  0.2  0.4  0.6  0.8  1.0  x  Figure 27.6 The isocline method. The cross lines on each isocline show the  slopes that solutions of dy dx = −2xy must have at the points where they exp −x2 .  cross the isoclines. The heavy line is the solution with y 0  = 1, namely  second calculational point x2. The integration of these equations by the methods discussed in the previous section presents no particular diﬃculty, provided that all the equations are advanced through each particular step before any of them is taken through the following step.  Higher-order equations in one dependent and one independent variable can be reduced to a set of simultaneous equations, provided that they can be written in the form  where R is the order of the equation. To do this, a new set of variables p[r] is deﬁned by  = f x, y, y   cid:7   , . . . , y R−1  ,  dRy dxR  p[r] =  dry dxr ,  r = 1, 2, . . . , R − 1.   27.82    27.83   Equation  27.82  is then equivalent to the following set of simultaneous ﬁrst-order equations:  r = 1, 2, . . . , R − 2,   27.84   = p[1],  = p[r+1],  dy dx dp[r] dx dp[R−1]  dx  = f x, y, p[1], . . . , p[R−1] .  1029   NUMERICAL METHODS  These can then be treated in the way indicated in the previous paragraph. The extension to more than one dependent variable is straightforward.  In practical problems it often happens that boundary conditions applicable to a higher-order equation consist not of the values of the function and all its derivatives at one particular point but of, say, the values of the function at two separate end-points. In these cases a solution cannot be found using an explicit step-by-step ‘marching’ scheme, in which the solutions at successive values of the independent variable are calculated using solution values previously found. Other methods have to be tried.  One obvious method is to treat the problem as a ‘marching one’, but to use a number of  intelligently guessed  initial values for the derivatives at the starting point. The aim is then to ﬁnd, by interpolation or some other form of iteration, those starting values for the derivatives that will produce the given value of the function at the ﬁnishing point.  In some cases the problem can be reduced by a diﬀerencing scheme to a matrix equation. Such a case is that of a second-order equation for y x  with constant coeﬃcients and given values of y at the two end-points. Consider the second-order equation   cid:7  cid:7    cid:7   y  + 2ky  + µy = f x ,   27.85   with the boundary conditions  y 0  = A,  y 1  = B.  If  27.85  is replaced by a central diﬀerence equation,  yi+1 − 2yi + yi−1  h2  yi+1 − yi−1  2h  + 2k  + µyi = f xi ,  we obtain from it the recurrence relation   1 + kh yi+1 +  µh2 − 2 yi +  1 − kh yi−1 = h2f xi .  For h = 1  N − 1  this is in exactly the form of the N × N tridiagonal matrix  equation  27.30 , with  b1 = bN = 1,  c1 = aN = 0,  ai = 1 − kh,  bi = µh2 − 2,  ci = 1 + kh,  i = 2, 3, . . . , N − 1,  and y1 replaced by A, yN by B and yi by h2f xi  for i = 2, 3, . . . , N − 1. The  solutions can be obtained as in  27.31  and  27.32 .  27.8 Partial diﬀerential equations  The extension of previous methods to partial diﬀerential equations, thus involving two or more independent variables, proceeds in a more or less obvious way. Rather  1030   27.8 PARTIAL DIFFERENTIAL EQUATIONS  than an interval divided into equal steps by the points at which solutions to the equations are to be found, a mesh of points in two or more dimensions has to be set up and all the variables given an increased number of subscripts.  Considerations of the stability, accuracy and feasibility of particular calcula- tional schemes are the same as for the one-dimensional case in principle, but in practice are too complicated to be discussed here.  Rather than note generalities that we are unable to pursue in any quantitative way, we will conclude this chapter by indicating in outline how two familiar partial diﬀerential equations of physical science can be set up for numerical solution. The ﬁrst of these is Laplace’s equation in two dimensions,  ∂2φ ∂x2 +  ∂2φ ∂y2 = 0,   27.86   the value of φ being given on the perimeter of a closed domain.  A grid with spacings ∆x and ∆y in the two directions is ﬁrst chosen, so that, for example, xi stands for the point x0 + i∆x and φi,j for the value φ xi, yj . Next, using a second central diﬀerence formula,  27.86  is turned into  φi+1,j − 2φi,j + φi−1,j  φi,j+1 − 2φi,j + φi,j−1   ∆x 2  +   ∆y 2  = 0,   27.87   for i = 0, 1, . . . , N and j = 0, 1, . . . , M. If  ∆x 2 = λ ∆y 2 then this becomes the recurrence relationship  φi+1,j + φi−1,j + λ φi,j+1 + φi,j−1  = 2 1 + λ φi,j .   27.88   The boundary conditions in their simplest form  i.e. for a rectangular domain  mean that  φ0,j , φN,j, φi,0, φi,M   27.89   have predetermined values. Non-rectangular boundaries can be accommodated, either by more complex boundary-value prescriptions or by using non-Cartesian coordinates.  To ﬁnd a set of values satisfying  27.88 , an initial guess of a complete set of values for the φi,j is made, subject to the requirement that the quantities listed in  27.89  have the given ﬁxed values; those values that are not on the boundary are then adjusted iteratively in order to try to bring about condition  27.88  everywhere. Clearly one scheme is to set λ = 1 and recalculate each φi,j as the mean of the four current values at neighbouring grid-points, using  27.88  directly, and then to iterate this recalculation until no value of φ changes signiﬁcantly after a complete cycle through all values of i and j. This procedure is the simplest of such ‘relaxation’ methods; for a slightly more sophisticated scheme see exercise 27.26 at the end of this chapter. The reader is referred to specialist books for fuller accounts of how this approach can be made faster and more accurate.  1031   NUMERICAL METHODS  Our ﬁnal example is based upon the one-dimensional diﬀusion equation for  the temperature φ of a system:  ∂φ ∂t  = κ  ∂2φ ∂x2 .   27.90   If φi,j stands for φ x0 + i∆x, t0 + j∆t  then a forward diﬀerence representation of the time derivative and a central diﬀerence representation for the spatial derivative lead to the following relationship:  φi,j+1 − φi,j  φi+1,j − 2φi,j + φi−1,j  = κ  ∆t   ∆x 2  .   27.91   This allows the construction of an explicit scheme for generating the temperature distribution at later times, given that it is known at some earlier time:  φi,j+1 = α φi+1,j + φi−1,j   +  1 − 2α φi,j ,   27.92   where α = κ∆t  ∆x 2.  Although this scheme is explicit, it is not a good one because of the asymmetric way in which the diﬀerences are formed. However, the eﬀect of this can be minimised if we study and correct for the errors introduced in the following way. Taylor’s series for the time variable gives  φi,j+1 = φi,j + ∆t  ∂φi,j ∂t  +   ∆t 2 2!  ∂2φi,j  ∂t2 + ··· ,  using the same notation as previously. Thus the ﬁrst correction term to the LHS of  27.91  is  − ∆t 2  ∂2φi,j ∂t2 .  −κ  2 ∆x 2  4!  ∂4φi,j ∂x4 .   cid:7    cid:8    cid:7    cid:8   The ﬁrst term omitted on the RHS of the same equation is, by a similar argument,  But, using the fact that φ satisﬁes  27.90 , we obtain  ∂2φ ∂t2 =  ∂ ∂t  κ  ∂2φ ∂x2  = κ  ∂2 ∂x2  ∂φ ∂t  = κ2 ∂4φ ∂x4 ,  and so, to this accuracy, the two errors  27.94  and  27.95  can be made to cancel if α is chosen such that  − κ2∆t 2  = − 2κ ∆x 2  ,  4!  i.e. α =  1 6  .  1032   27.93    27.94    27.95    27.96    27.1  27.2  27.3  27.4  27.5  27.7  27.9 EXERCISES  27.9 Exercises  Use an iteration procedure to ﬁnd the root of the equation 40x = exp x to four signiﬁcant ﬁgures. Using the Newton–Raphson procedure ﬁnd, correct to three decimal places, the  root nearest to 7 of the equation 4x3 + 2x2 − 200x − 50 = 0.  Show the following results about rearrangement schemes for polynomial equa- tions.   a  That if a polynomial equation g x  ≡ xm − f x  = 0, where f x  is a polynomial of degree less than m and for which f 0   cid:3 = 0, is solved using a rearrangement iteration scheme xn+1 = [ f xn ]1 m, then, in general, the scheme will have only ﬁrst-order convergence.   b  By considering the cubic equation  x3 − ax2 + 2abx −  b3 + ab2  = 0  for arbitrary non-zero values of a and b, demonstrate that, in special cases, the same rearrangement scheme can give second-  or higher-  order convergence.  The square root of a number N is to be determined by means of the iteration scheme   cid:18  √ Determine how to choose f N  so that the process has second-order convergence. 7 as accurately as a single application of the  7 ≈ 2.65, calculate  1 − cid:5   N − x2  Given that  xn+1 = xn  f N    cid:6   √   cid:19   n  .  formula will allow. Solve the following set of simultaneous equations using Gaussian elimination  including interchange where it is formally desirable :  27.6  The following table of values of a polynomial p x  of low degree contains an error. Identify and correct the erroneous value and extend the table up to x = 1.2.  x1 + 3x2 + 4x3 + 2x4 = 0,  2x1 + 10x2 − 5x3 + x4 = 6, −3x1 + 6x2 + 12x3 − 4x4 = 16.  4x2 + 3x3 + 3x4 = 20,  x 0.0 0.1 0.2 0.3 0.4  p x  0.000 0.011 0.040 0.081 0.128  x 0.5 0.6 0.7 0.8 0.9  p x  0.165 0.216 0.245 0.256 0.243  Simultaneous linear equations that result in tridiagonal matrices can sometimes be treated as three-term recurrence relations, and their solution may be found in a similar manner to that described in chapter 15. Consider the tridiagonal simultaneous equations  xi−1 + 4xi + xi+1 = 3 δi+1,0 − δi−1,0 ,  i = 0,±1,±2, . . . .  Prove that, for i > 0, the equations have a general solution of the form xi = αpi + βqi, where p and q are the roots of a certain quadratic equation. Show that a similar result holds for i < 0. In each case express x0 in terms of the arbitrary constants α, β, . . . .  Now impose the condition that xi is bounded as i → ±∞ and obtain a unique  solution.  1033   NUMERICAL METHODS   cid:21   x0+∆x  x0  27.8  A possible rule for obtaining an approximation to an integral is the mid-point rule, given by  f x  dx = ∆x f x0 + 1  2 ∆x  + O ∆x3 .  27.9  27.10  Writing h for ∆x, and evaluating all derivates at the mid-point of the interval  x, x + ∆x , use a Taylor series expansion to ﬁnd, up to O h5 , the coeﬃcients of the higher-order errors in both the trapezium and mid-point rules. Hence ﬁnd a linear combination of these two rules that gives O h5  accuracy for each step ∆x. Although it can easily be shown, by direct calculation, that   cid:21  ∞  0  −x cos kx  dx =  e  1  1 + k2 ,  the form of the integrand is appropriate for Gauss–Laguerre numerical integra- tion. Using a 5-point formula, investigate the range of values of k for which the formula gives accurate results. At about what value of k do the results become inaccurate at the 1% level? Using the points and weights given in table 27.9, answer the following questions.   a  A table of unnormalised Hermite polynomials Hn x  has been spattered with  ink blots and gives H5 x  as 32x5−?x3 + 120x and H4 x  as ?x4−?x2 + 12,  where the coeﬃcients marked ? cannot be read. What should they read?   b  What is the value of the integral   cid:21  ∞   cid:21   −2x2 e  I =  −∞  4x2 + 3x + 1  dx,  Ip =  1  −1  x2p√ 1 − x2  dx.  2p − 1  2p  2p − 3 2p − 2  ··· 1 2  π 2  .  Ip = 2  as given by a 7-point integration routine?  27.11  Consider the integrals Ip deﬁned by   a  By setting x = sin θ and using the results given in exercise 2.42, show that Ip  has the value   b  Evaluate Ip for p = 1, 2, . . . , 6 using 5- and 6-point Gauss–Chebyshev inte- gration  conveniently run on a spreadsheet such as Excel  and compare the results with those in  a . In particular, show that, as expected, the 5-point scheme ﬁrst fails to be accurate when the order of the polynomial numerator   2p  exceeds  2×5 −1 = 9. Likewise, verify that the 6-point scheme evaluates  I5 accurately but is in error for I6.  27.12  In normal use only a single application of n-point Gaussian quadrature is made, using a value of n that is estimated from experience to be ‘safe’. However, it is instructive to examine what happens when n is changed in a controlled way.   a  Evaluate the integral   cid:21   In =  2  5  √ 7x − x2 − 10 dx  using n-point Gauss–Legendre formulae for n = 2, 3, . . . , 6. Estimate  to 4 s.f.  the value I∞ you would obtain for very large n and compare it with the result I obtained by exact integration. Explain why the variation of In with n is monotonically decreasing.  1034    b  Try to repeat the processes described in  a  for the integrals  27.9 EXERCISES   cid:21   Jn =  5  2  1  √ 7x − x2 − 10  dx.  Why is it very diﬃcult to estimate J∞?  27.13  Given a random number η uniformly distributed on  0, 1 , determine the function ξ = ξ η  that would generate a random number ξ distributed as   a  2ξ on 0 ≤ ξ < 1, ξ on 0 ≤ ξ < 1,  √   b   3 2 π 4a 1   c    d   cos  πξ 2a  on − a ≤ ξ < a,  2 exp −  ξ   on − ∞ < ξ < ∞.  27.14  27.15  27.16  27.17  A, B and C are three circles of unit radius with centres in the xy-plane at  1, 2 ,  2.5, 1.5  and  2, 3 , respectively. Devise a hit or miss Monte Carlo calculation to determine the size of the area that lies outside C but inside A and B, as well as inside the square centred on  2, 2.5 , that has sides of length 2 parallel to the coordinate axes. You should choose your sampling region so as to make the estimation as eﬃcient as possible. Take the random number distribution to be uniform on  0, 1  and determine the inequalities that have to be tested using the random numbers chosen. Use a Taylor series to solve the equation  evaluating y x  for x = 0.0 to 0.5 in steps of 0.1. Consider the application of the predictor–corrector method described near the end of subsection 27.6.3 to the equation  dy dx  dy dx  + xy = 0,  y 0  = 1,  = x + y,  y 0  = 0.  Show, by comparison with a Taylor series expansion, that the expression obtained for yi+1 in terms of xi and yi by applying the three steps indicated  without any repeat of the last two  is correct to O h2 . Using steps of h = 0.1 compute the value of y 0.3  and compare it with the value obtained by solving the equation analytically. A more reﬁned form of the Adams predictor–corrector method for solving the ﬁrst-order diﬀerential equation  dy dx  = f x, y   is known as the Adams–Moulton–Bashforth scheme. At any stage  say the nth  in an Nth-order scheme, the values of x and y at the previous N solution points are ﬁrst used to predict the value of yn+1. This approximate value of y at the previous N − 1 solution points to make a more reﬁned  corrected  estimation of next solution point, xn+1, denoted by ¯yn+1, is then used together with those at the  y xn+1 . The calculational procedure for a third-order scheme is summarised by the two following two equations:  ¯yn+1 = yn + h a1fn + a2fn−1 + a3fn−2  yn+1 = yn + h b1f xn+1, ¯yn+1  + b2fn + b3fn−1    predictor ,   corrector .   a  Find Taylor series expansions for fn−1 and fn−2 in terms of the function  fn = f xn, yn  and its derivatives at xn.  1035   NUMERICAL METHODS   b  Substitute them into the predictor equation and, by making that expression for ¯yn+1 coincide with the true Taylor series for yn+1 up to order h3, establish simultaneous equations that determine the values of a1, a2 and a3.   c  Find the Taylor series for fn+1 and substitute it and that for fn−1 into the corrector equation. Make the corrected prediction for yn+1 coincide with the true Taylor series by choosing the weights b1, b2 and b3 appropriately.   d  The values of the numerical solution of the diﬀerential equation  dy dx  =  2 1 + x y + x3 2  2x 1 + x   at three values of x are given in the following table:  x  y x   0.1 0.030 628  0.2 0.084 107  0.3 0.150 328  Use the above predictor–corrector scheme to ﬁnd the value of y 0.4  and compare your answer with the accurate value, 0.225 577.  27.18  If dy dx = f x, y  then show that  d2f dx2  =  ∂2f ∂x2  + 2f  ∂2f ∂x∂y  + f2 ∂2f ∂y2  +  ∂f ∂x  ∂f ∂y  + f  ∂f ∂y  Hence verify, by substitution and the subsequent expansion of arguments in Taylor series of their own, that the scheme given in  27.79  coincides with the Taylor expansion  27.68 , i.e.   cid:7    cid:8   2  .  yi+1 = yi + hy 1   i +  y 2  i +  h2 2!  i + ··· , y 3   h3 3!  27.19  up to terms in h3. To solve the ordinary diﬀerential equation  du dt  = f u, t   for f = f t , the explicit two-step ﬁnite diﬀerence scheme un+1 = αun + βun−1 + h µfn + νfn−1   may be used. Here, in the usual notation, h is the time step, tn = nh, un = u tn  and fn = f un, tn ; α, β, µ, and ν are constants.   a  A particular scheme has α = 1, β = 0, µ = 3 2 and ν = −1 2. By considering  Taylor expansions about t = tn for both un+j and fn+j, show that this scheme gives errors of order h3.   b  Find the values of α, β, µ and ν that will give the greatest accuracy.  27.20  Set up a ﬁnite diﬀerence scheme to solve the ordinary diﬀerential equation  x  d2φ dx2  +  dφ dx  = 0  in the range 1 ≤ x ≤ 4, subject to the boundary conditions φ 1  = 2 and  dφ dx = 2 at x = 4. Using N equal increments, ∆x, in x, obtain the general diﬀerence equation and state how the boundary conditions are incorporated into the scheme. Setting ∆x equal to the  crude  value 1, obtain the relevant simultaneous equations and so obtain rough estimates for φ 2 , φ 3  and φ 4 .  Finally, solve the original equation analytically and compare your numerical  estimates with the accurate values.  1036   27.9 EXERCISES  1 cid:24   dy dx  =  ,  y 0  = 1,  x2 + λy2  a cid:24   dy dx  =  .  x2 + y2  ∂u ∂t  + A  = 0,  ∂u ∂x  27.21 Write a computer program that would solve, for a range of values of λ, the  diﬀerential equation  27.22  using a third-order Runge–Kutta scheme. Consider the diﬃculties that might arise when λ < 0. Use the isocline approach to sketch the family of curves that satisﬁes the non- linear ﬁrst-order diﬀerential equation  27.23  For some problems, numerical or algebraic experimentation may suggest the form of the complete solution. Consider the problem of numerically integrating the ﬁrst-order wave equation  in which A is a positive constant. A ﬁnite diﬀerence scheme for this partial diﬀerential equation is  u p, n + 1  − u p, n   u p, n  − u p − 1, n   ∆t  + A  ∆x  = 0,  where x = p∆x and t = n∆t, with p any integer and n a non-negative integer.  The initial values are u 0, 0  = 1 and u p, 0  = 0 for p  cid:3 = 0.   a  Carry the diﬀerence equation forward in time for two or three steps and attempt to identify the pattern of solution. Establish the criterion for the method to be numerically stable.   b  Suggest a general form for u p, n , expressing it in generator function form,  i.e. as ‘u p, n  is the coeﬃcient of sp in the expansion of G n, s ’.   c  Using your form of solution  or that given in the answers! , obtain an explicit general expression for u p, n  and verify it by direct substitution into the diﬀerence equation.   d  An analytic solution of the original PDE indicates that an initial distur- bance propagates undistorted. Under what circumstances would the diﬀer- ence scheme reproduce that behaviour?  27.24  In exercise 27.23 the diﬀerence scheme for solving  ∂u ∂t  +  ∂u ∂x  = 0,  in which A has been set equal to unity, was one-sided in both space  x  and time  t . A more accurate procedure  known as the Lax–Wendroﬀ scheme  is  u p, n + 1  − u p, n   ∆t  u p + 1, n  − u p − 1, n   cid:13   2∆x  u p + 1, n  − 2u p, n  + u p − 1, n    cid:14   .  +  =  ∆t 2   ∆x 2   a  Establish the orders of accuracy of the two ﬁnite diﬀerence approximations   b  Establish the accuracy with which the expression in the brackets approxi-  on the LHS of the equation.  mates ∂2u ∂x2.   c  Show that the RHS of the equation is such as to make the whole diﬀerence  scheme accurate to second order in both space and time.  1037   NUMERICAL METHODS  27.25  Laplace’s equation,  is to be solved for the region and boundary conditions shown in ﬁgure 27.7.  −∞  40  40  40  40  40  40  40  ∞  ∂2V ∂x2  +  ∂2V ∂y2  = 0,  V = 80  20  20  20  V = 0  Figure 27.7 Region, boundary values and initial guessed solution values.  Starting from the given initial guess for the potential values V , and using the simplest possible form of relaxation, obtain a better approximation to the actual  solution. Do not aim to be more accurate than ± 0.5 units, and so terminate the  process when subsequent changes would be no greater than this. Consider the solution, φ x, y , of Laplace’s equation in two dimensions using a relaxation method on a square grid with common spacing h. As in the main text, denote φ x0 + ih, y0 + jh  by φi,j. Further, deﬁne φm,n  i,j by  27.26  φm,n i,j  ≡ ∂m+nφ ∂xm ∂yn  φ4,0 i,j + 2φ2,2  i,j + φ0,4  i,j = 0.  S±,0 = φi+1,j + φi−1,j, S0,± = φi,j+1 + φi,j−1.  evaluated at  x0 + ih, y0 + jh .   a  Show that   b  Working up to terms of order h5, ﬁnd Taylor series expansions, expressed in  terms of the φm,n  i,j , for   c  Find a corresponding expansion, to the same order of accuracy, for φi±1,j+1 +  φi±1,j−1 and hence show that  S±,± = φi+1,j+1 + φi+1,j−1 + φi−1,j+1 + φi−1,j−1  has the form  4φ0,0  i,j + 2h2 φ2,0  i,j + φ0,2  i,j   +   φ4,0  i,j + 6φ2,2  i,j + φ0,4 i,j  .  h4 6   d  Evaluate the expression 4 S±,0 + S0,±  + S±,± and hence deduce that a possible relaxation scheme, good to the ﬁfth order in h, is to recalculate each φi,j as the weighted mean of the current values of its four nearest neighbours  each with weight 1  5   and its four next-nearest neighbours  each with weight 1  20  .  1038   27.10 HINTS AND ANSWERS  27.27  The Schr¨odinger equation for a quantum mechanical particle of mass m moving in a one-dimensional harmonic oscillator potential V  x  = kx2 2 is  −  cid:1 2 2m  d2ψ dx2  +  kx2ψ  2  = Eψ.   cid:1  ψ2 dx = 1. In practice,  For physically acceptable solutions, the wavefunction ψ x  must be ﬁnite at x = 0,  tend to zero as x → ±∞ and be normalised, so that  these constraints mean that only certain  quantised  values of E, the energy of the particle, are allowed. The allowed values fall into two groups: those for which  ψ 0  = 0 and those for which ψ 0   cid:3 = 0.  Show that if the unit of length is taken as [ cid:1 2  mk ]1 4 and the unit of energy  is taken as  cid:1  k m 1 2, then the Schr¨odinger equation takes the form   cid:7  − y2 ψ = 0.  +  2E  d2ψ dy2  Devise an outline computerised scheme, using Runge–Kutta integration, that will enable you to:   a  determine the three lowest allowed values of E;  b  tabulate the normalised wavefunction corresponding to the lowest allowed  energy.  You should consider explicitly:   i  the variables to use in the numerical integration;  ii  how starting values near y = 0 are to be chosen;   iii  how the condition on ψ as y → ±∞ is to be implemented;   iv  how the required values of E are to be extracted from the results of the  integration;   v  how the normalisation is to be carried out.  27.1 27.3 27.5  27.7  27.9 27.11  27.13  27.10 Hints and answers   cid:7    cid:7   5.370.   b  = 0 whilst f b   cid:3 = 0.  Interchange is formally needed for the ﬁrst two steps, though in this case no error   ξ   cid:3 = 0 in general;  b  ξ = b, but f   a  ξ  cid:3 = 0 and f will result if it is not carried out; x1 = −12, x2 = 2, x3 = −1, x4 = 5. The quadratic equation is z2 + 4z + 1 = 0; α + β − 3 = x0 = α  cid:7  With p = −2 + zero for i < 0; xi = 3 −2 + The error is 1% or less for k less than about 1.1.  √ 3 i for i > 0, xi = 0 for i = 0, xi = −3 −2 − √  3 and q = −2 − √ √  3, β must be zero for i > 0 and α  must be 3 i  for i < 0.  + 3.  cid:7   + β   cid:7   Exact values  6 s.f.  for p = 1, 2, . . . , 6 are 1.570 796, 1.178 097, 0.981 748, 0.859 029, 0.773 126, 0.708 699. The Gauss–Chebyshev integration is in error by about 1% when n = p. Listed below are the relevant indeﬁnite integrals F y  of the distributions together with the functions ξ = ξ η :   a  y2, ξ =  b  y3 2, ξ = η2 3;  c   d   {sin[πy  2a ] + 1}, ξ =  2a π  sin 2 exp y for y ≤ 0, 1 ξ = − ln[2 1 − η ] for 1  1 2 1  −1 2η − 1 ;  2 [2 − exp  −y ] for y > 0; ξ = ln 2η for 0 < η ≤ 1 2 , 2 < η < 1.  √ η;  1039   −∞  40.5  41.8  46.7  48.4  46.7  41.8  40.5  ∞  NUMERICAL METHODS  V = 80  16.8  20.4  16.8  V = 0  Figure 27.8 The solution to exercise 27.25.  27.15  27.17  27.19  27.21  27.23  1 − x2 2 + x4 8 − x6 48; 1.0000, 0.9950, 0.9802, 0.9560, 0.9231, 0.8825; exact solution y = exp −x2 2 .  b  a1 = 23 12, a2 = −4 3, a3 = 5 12.  c  b1 = 5 12, b2 = 2 3, b3 = −1 12.  d  ¯y 0.4  = 0.224 582, y 0.4  = 0.225 527 after correction.  a  The error is 5h3u 3  n  12 + O h4 .  b  α = −4, β = 5, µ = 4 and ν = 2  For λ positive the solutions are  boringly  monotonic functions of x. With y 0  given, there are no real solutions at all for any negative λ!   a  Setting A∆t = c∆x gives, for example, u 0, 2  =  1 − c 2, u 1, 2  = 2c 1 − c , u 2, 2  = c2. For stability, 0 < c < 1.  b  G n, s  = [ 1 − c  + cs]n for 0 ≤ p ≤ n. [n! 1 − c n−pcp] [p! n − p !].  d  When c = 1 and the diﬀerence equation becomes u p, n + 1  = u p − 1, n .   c   27.25 27.27  See ﬁgure 27.8. If x = αy then  d2ψ dy2  − α4 mk   cid:1 2 y2ψ + α2 2mE   cid:1 2 ψ = 0.  Solutions will be either symmetric or antisymmetric with ψ 0   cid:3 = 0 but ψ   0  = 0 for the former and vice versa for the latter. Integration to a largish but ﬁnite value of y followed by an interpolation procedure to estimate the values of E  that lead to ψ ∞  = 0 needs to be incorporated. Simple numerical integration   cid:7   such as Simpson’s rule will suﬃce for the normalisation integral. The solutions should be λ = 1, 3, 5, . . . .  1040   28  Group theory  For systems that have some degree of symmetry, full exploitation of that symmetry is desirable. Signiﬁcant physical results can sometimes be deduced simply by a study of the symmetry properties of the system under investigation. Consequently it becomes important, for such a system, to identify all those operations  rotations, reﬂections, inversions  that carry the system into a physically indistinguishable copy of itself.  The study of the properties of the complete set of such operations forms one application of group theory. Though this is the aspect of most interest to the physical scientist, group theory itself is a much larger subject and of great importance in its own right. Consequently we leave until the next chapter any direct applications of group theoretical results and concentrate on building up the general mathematical properties of groups.  28.1 Groups  As an example of symmetry properties, let us consider the sets of operations, such as rotations, reﬂections, and inversions, that transform physical objects, for example molecules, into physically indistinguishable copies of themselves, so that only the labelling of identical components of the system  the atoms  changes in the process. For diﬀerently shaped molecules there are diﬀerent sets of operations, but in each case it is a well-deﬁned set, and with a little practice all members of each set can be identiﬁed.  As simple examples, consider  a  the hydrogen molecule, and  b  the ammonia molecule illustrated in ﬁgure 28.1. The hydrogen molecule consists of two atoms H of hydrogen and is carried into itself by any of the following operations:   i  any rotation about its long axis;  ii  rotation through π about an axis perpendicular to the long axis and  passing through the point M that lies midway between the atoms;  1041   GROUP THEORY  N  H  H  H  M   a   H   b   H  Figure 28.1  a  The hydrogen molecule, and  b  the ammonia molecule.   iii  inversion through the point M;  iv  reﬂection in the plane that passes through M and has its normal parallel  to the long axis.  These operations collectively form the set of symmetry operations for the hydro- gen molecule.  The somewhat more complex ammonia molecule consists of a tetrahedron with an equilateral triangular base at the three corners of which lie hydrogen atoms H, whilst a nitrogen atom N is sited at the fourth vertex of the tetrahedron. The set of symmetry operations on this molecule is limited to rotations of π 3 and 2π 3 about the axis joining the centroid of the equilateral triangle to the nitrogen atom, and reﬂections in the three planes containing that axis and each of the hydrogen atoms in turn. However, if the nitrogen atom could be replaced by a fourth hydrogen atom, and all interatomic distances equalised in the process, the number of symmetry operations would be greatly increased.  Once all the possible operations in any particular set have been identiﬁed, it must follow that the result of applying two such operations in succession will be identical to that obtained by the sole application of some third  usually diﬀerent  operation in the set – for if it were not, a new member of the set would have been found, contradicting the assumption that all members have been identiﬁed. Such observations introduce two of the main considerations relevant to decid- ing whether a set of objects, here the rotation, reﬂection and inversion operations, qualiﬁes as a group in the mathematically tightly deﬁned sense. These two consid- erations are  i  whether there is some law for combining two members of the set, and  ii  whether the result of the combination is also a member of the set. The obvious rule of combination has to be that the second operation is carried out on the system that results from application of the ﬁrst operation, and we have already seen that the second requirement is satisﬁed by the inclusion of all such operations in the set. However, for a set to qualify as a group, more than these two conditions have to be satisﬁed, as will now be made clear.  1042   28.1 GROUPS   28.1    28.2    28.3   28.1.1 Deﬁnition of a group  A group G is a set of elements {X, Y , . . .}, together with a rule for combining X   Y for which the following conditions must be satisﬁed.  them that associates with each ordered pair X, Y a ‘product’ or combination law   i  For every pair of elements X, Y that belongs to G, the product X   Y also  belongs to G.  This is known as the closure property of the group.    ii  For all triples X, Y , Z the associative law holds; in symbols,   iii  There exists a unique element I, belonging to G, with the property that  X    Y   Z  =  X   Y     Z .  I   X = X = X   I  for all X belonging to G. This element I is known as the identity element −1, also belonging to   iv  For every element X of G, there exists an element X  of the group. G, such that  −1   X = I = X   X  −1.  X  −1 is called the inverse of X.  X  An alternative notation in common use is to write the elements of a group G as the set {G1, G2, . . .} or, more brieﬂy, as {Gi}, a typical element being denoted by Gi. It should be noticed that, as given, the nature of the operation   is not stated. It  should also be noticed that the more general term element, rather than operation, has been used in this deﬁnition. We will see that the general deﬁnition of a group allows as elements not only sets of operations on an object but also sets of  numbers, of functions and of other objects, provided that the interpretation of    is appropriately deﬁned.  In one of the simplest examples of a group, namely the group of all integers  role of the identity I is played by the integer 0, and the inverse of an integer X is  under addition, the operation   is taken to be ordinary addition. In this group the −X. That requirements  i  and  ii  are satisﬁed by the integers under addition is by the two numbers 1 and −1; in this group, closure is obvious, 1 is the identity  trivially obvious. A second simple group, under ordinary multiplication, is formed  element, and each element is its own inverse.  It will be apparent from these two examples that the number of elements in a group can be either ﬁnite or inﬁnite. In the former case the group is called a ﬁnite group and the number of elements it contains is called the order of the group,  which we will denote by g; an alternative notation is G but has obvious dangers  1043   GROUP THEORY  if matrices are involved. In the notation in which G = {G1, G2, . . . , Gn} the order  of the group is clearly n.  As we have noted, for the integers under addition zero is the identity. For the group of rotations and reﬂections, the operation of doing nothing, i.e. the null operation, plays this role. This latter identiﬁcation may seem artiﬁcial, but it is an operation, albeit trivial, which does leave the system in a physically indistinguishable state, and needs to be included. One might add that without it the set of operations would not form a group and none of the powerful results we will derive later in this and the next chapter could be justiﬁably applied to give deductions of physical signiﬁcance.  In the examples of rotations and reﬂections mentioned earlier,   has been taken  to mean that the left-hand operation is carried out on the system that results from application of the right-hand operation. Thus  Z = X   Y   28.4   means that the eﬀect on the system of carrying out Z is the same as would be obtained by ﬁrst carrying out Y and then carrying out X. The order of the operations should be noted; it is arbitrary in the ﬁrst instance but, once chosen, must be adhered to. The choice we have made is dictated by the fact that most of our applications involve the eﬀect of rotations and reﬂections on functions of space coordinates, and it is usual, and our practice in the rest of this book, to write operators acting on functions to the left of the functions.  It will be apparent that for the above-mentioned group, integers under ordinary  addition, it is true that  Y   X = X   Y   28.5   for all pairs of integers X, Y . If any two particular elements of a group satisfy   28.5 , they are said to commute under the operation  ; if all pairs of elements in  a group satisfy  28.5 , then the group is said to be Abelian. The set of all integers forms an inﬁnite Abelian group under  ordinary  addition.  As we show below, requirements  iii  and  iv  of the deﬁnition of a group are over-demanding  but self-consistent , since in each of equations  28.2  and  28.3  the second equality can be deduced from the ﬁrst by using the associativity required by  28.1 . The mathematical steps in the following arguments are all very simple, but care has to be taken to make sure that nothing that has not yet been proved is used to justify a step. For this reason, and to act as a model in logical deduction, a reference in Roman numerals to the previous result, or to the group deﬁnition used, is given over each equality sign. Such explicit detailed referencing soon becomes tiresome, but it should always be available if needed.  1044   28.1 GROUPS   cid:1 Using only the ﬁrst equalities in  28.2  and  28.3 , deduce the second ones.  Consider the expression X  −1    X   X −1   −1    X   X  −1 ;  ii  =  X  X  −1   X    X −1.   iii  = X  −1  iv   = I   X  −1  −1 belongs to G, and so from  iv  there is an element U in G such that  But X  U   X  −1 = I.   v   Form the product of U with the ﬁrst and last expressions in  28.6  to give  U    X  −1    X   X  −1   = U   X  −1  v   = I.  Transforming the left-hand side of this equation gives  U    X  −1    X   X  −1    −1     X   X  −1    v    ii   =  U   X = I    X   X = X   X −1.   iii   −1   Comparing  28.7 ,  28.8  shows that  X   X  −1 = I,   cid:7    iv   i.e. the second equality in group deﬁnition  iv . Similarly  X   I  −1   X    ii   =  X   X  −1    X   iv   = X    X  cid:7  = I   X   iv    iii  = X.   cid:7   iii     i.e. the second equality in group deﬁnition  iii .  cid:2   The uniqueness of the identity element I can also be demonstrated rather than  assumed. Suppose that I   cid:7   , belonging to G, also has the property for all X belonging to G.   cid:7    X = X = X   I   cid:7   I  Take X as I, then  Further, from  iii   ,   cid:7    cid:7    I = I.  I  X = X   I  for all X belonging to G,  1045   28.6    28.7    28.8    28.9    GROUP THEORY  and setting X = I  gives   cid:7    cid:7   I  = I   cid:7    I.  It then follows from  28.9 ,  28.10  that I = I group the identity element is unique.   cid:7   , showing that in any particular   28.10   In a similar way it can be shown that the inverse of any particular element  is unique. If U and V are two postulated inverses of an element X of G, by  considering the product  U    X   V   =  U   X    V ,  it can be shown that U = V . The proof is left to the reader.  Given the uniqueness of the inverse of any particular group element, it follows  that   U   V  ···   Y   Z     Z  −1   Y  −1   ···   V  −1   U −1     Y −1   ···   V  −1  −1   ···   V −1   U −1   −1   U  −1   =  U   V   ···   Y      Z   Z =  U   V   ···   Y      Y ... = I,  where use has been made of the associativity and of the two equations Z   Z −1 = I and I   X = X. Thus the inverse of a product is the product of the inverses in  reverse order, i.e.   U   V   ···   Y   Z   −1 =  Z  −1   Y  −1   ···   V  −1   U  −1 .   28.11   Further elementary results that can be obtained by arguments similar to those above are as follows.   i  Given any pair of elements X, Y belonging to G, there exist unique  elements U, V , also belonging to G, such that  X   U = Y  and  V   X = Y .  −1   Y , and V = Y   X  −1, and they can be shown to be  Clearly U = X unique. This result is sometimes called the division axiom.   ii  The cancellation law can be stated as follows. If  for some X belonging to G, then Y = Z . Similarly,  implies the same conclusion.  X   Y = X   Z  Y   X = Z   X  1046   28.1 GROUPS  L  M  K  Figure 28.2 Reﬂections in the three perpendicular bisectors of the sides of an equilateral triangle take the triangle into itself.   iii  Forming the product of each element of G with a ﬁxed element X of G simply permutes the elements of G; this is often written symbolically as G   X = G. If this were not so, and X   Y and X   Z were not diﬀerent  even though Y and Z were, application of the cancellation law would lead to a contradiction. This result is called the permutation law.  In any ﬁnite group of order g, any element X when combined with itself to  form successively X2 = X   X, X3 = X   X2, . . . will, after at most g − 1 such combinations, produce the group identity I. Of course X2, X3, . . . are some of the original elements of the group, and not new ones. If the actual number of combinations needed is m− 1, i.e. Xm = I, then m is called the order of the element X in G. The order of the identity of a group is always 1, and that of any other  element of a group that is its own inverse is always 2.   cid:1 Determine the order of the group of  two-dimensional  rotations and reﬂections that take a plane equilateral triangle into itself and the order of each of the elements. The group is usually known as 3m  to physicists and crystallographers  or C3v  to chemists .  There are two  clockwise  rotations, by 2π 3 and 4π 3, about an axis perpendicular to the plane of the triangle. In addition, reﬂections in the perpendicular bisectors of the three sides  see ﬁgure 28.2  have the deﬁning property. To these must be added the identity operation. Thus in total there are six distinct operations and so g = 6 for this group. To reproduce the identity operation either of the rotations has to be applied three times, whilst any of the reﬂections has to be applied just twice in order to recover the original situation. Thus each rotation element of the group has order 3, and each reﬂection element has order 2.  cid:2   A so-called cyclic group is one for which all members of the group can be generated from just one element X  say . Thus a cyclic group of order g can be written as  2  1  G =  I, X, X2, X3, . . . , Xg−1  .  1047   GROUP THEORY  It is clear that cyclic groups are always Abelian and that each element, apart from the identity, has order g, the order of the group itself.  28.1.2 Further examples of groups  In this section we consider some sets of objects, each set together with a law of combination, and investigate whether they qualify as groups and, if not, why not. We have already seen that the integers form a group under ordinary addition, but it is immediately apparent that  even if zero is excluded  they do not do so under ordinary multiplication. Unity must be the identity of the set, but the requisite inverse of any integer n, namely 1 n, does not belong to the set of integers for any n other than unity.  Other inﬁnite sets of quantities that do form groups are the sets of all real numbers, or of all complex numbers, under addition, and of the same two sets excluding 0 under multiplication. All these groups are Abelian.  Although subtraction and division are normally considered the obvious coun- terparts of the operations of  ordinary  addition and multiplication, they are not acceptable operations for use within groups since the associative law,  28.1 , does not hold. Explicitly,  X −  Y − Z   cid:3 =  X − Y   − Z , X ÷  Y ÷ Z   cid:3 =  X ÷ Y   ÷ Z .  From within the ﬁeld of all non-zero complex numbers we can select just those  that have unit modulus, i.e. are of the form eiθ where 0 ≤ θ < 2π, to form a  group under multiplication, as can easily be veriﬁed:  Closely related to the above group is the set of 2 × 2 rotation matrices that take  the form  eiθ1 × eiθ2 ei0 ei 2π−θ  × eiθ = ei2π ≡ ei0 = 1  = ei θ1+θ2  = 1   closure ,  identity ,  inverse .   cid:7   M θ  =  cos θ − sin θ  sin θ  cos θ   cid:8   where, as before, 0 ≤ θ < 2π. These form a group when the law of combination  is that of matrix multiplication. The reader can easily verify that  M θ M φ  = M θ + φ  M 2π − θ  = M−1 θ   M 0  = I2   closure ,  identity ,  inverse .  Here I2 is the unit 2 × 2 matrix.  1048   28.2 FINITE GROUPS  28.2 Finite groups  Whilst many properties of physical systems  e.g. angular momentum  are related to the properties of inﬁnite, and, in particular, continuous groups, the symmetry properties of crystals and molecules are more intimately connected with those of ﬁnite groups. We therefore concentrate in this section on ﬁnite sets of objects that can be combined in a way satisfying the group postulates.  Although it is clear that the set of all integers does not form a group under ordinary multiplication, restricted sets can do so if the operation involved is multi- plication  mod N  for suitable values of N; this operation will be explained below. As a simple example of a group with only four members, consider the set S  deﬁned as follows:  S = {1, 3, 5, 7}  under multiplication  mod 8 .  To ﬁnd the product  mod 8  of any two elements, we multiply them together in the ordinary way, and then divide the answer by 8, treating the remainder after  doing so as the product of the two elements. For example, 5 × 7 = 35, which on dividing by 8 gives a remainder of 3. Clearly, since Y × Z = Z × Y , the full set  of diﬀerent products is  1 × 1 = 1, 3 × 3 = 1, 5 × 5 = 1, 7 × 7 = 1.  1 × 3 = 3, 3 × 5 = 7, 5 × 7 = 3,  1 × 5 = 5, 3 × 7 = 5,  1 × 7 = 7,  The ﬁrst thing to notice is that each multiplication produces a member of the original set, i.e. the set is closed. Obviously the element 1 takes the role of the  identity, i.e. 1× Y = Y for all members Y of the set. Further, for each element Y of the set there is an element Z  equal to Y , as it happens, in this case  such that Y × Z = 1, i.e. each element has an inverse. These observations, together with the associativity of multiplication  mod 8 , show that the set S is an Abelian group of order 4.  It is convenient to present the results of combining any two elements of a group in the form of multiplication tables – akin to those which used to appear in elementary arithmetic books before electronic calculators were invented! Written in this much more compact form the above example is expressed by table 28.1. Although the order of the two elements being combined does not matter here because the group is Abelian, we adopt the convention that if the product in a  general multiplication table is written X   Y then X is taken from the left-hand result of 3 × 5, rather than of 5 × 3.  column and Y is taken from the top row. Thus the bold ‘7’ in the table is the  Whilst it would make no diﬀerence to the basic information content in a table to present the rows and columns with their headings in random orders, it is  1049   GROUP THEORY  1 1 3 5 7  3 3 1 7 5  5 5 7 1 3  7 7 5 3 1  1 3 5 7  Table 28.1 The table of products for the elements of the group S = {1, 3, 5, 7}  under multiplication  mod 8 .  usual to list the elements in the same order in both the vertical and horizontal headings in any one table. The actual order of the elements in the common list, whilst arbitrary, is normally chosen to make the table have as much symmetry as possible. This is initially a matter of convenience, but, as we shall see later, some of the more subtle properties of groups are revealed by putting next to each other elements of the group that are alike in certain ways.  Some simple general properties of group multiplication tables can be deduced immediately from the fact that each row or column constitutes the elements of the group.   i  Each element appears once and only once in each row or column of the  table; this must be so since G   X = G  the permutation law  holds.   ii  The inverse of any element Y can be found by looking along the row in which Y appears in the left-hand column  the Y th row , and noting the element Z at the head of the column  the Zth column  in which the identity appears as the table entry. An immediate corollary is that whenever the identity appears on the leading diagonal, it indicates that the corresponding header element is of order 2  unless it happens to be the identity itself .   iii  For any Abelian group the multiplication table is symmetric about the  leading diagonal.  To get used to the ideas involved in using group multiplication tables, we now  consider two more sets of integers under multiplication  mod N :  S cid:7  S cid:7  cid:7   = {1, 5, 7, 11} under multiplication  mod 24 , and = {1, 2, 3, 4}  under multiplication  mod 5 .  These have group multiplication tables 28.2 a  and  b  respectively, as the reader should verify.  are compared, it will be seen that they have essentially the same structure, i.e if the elements are written as  If tables 28.1 and 28.2 a  for the groups S and S cid:7  {I, A, B, C} in both cases, then the two tables are each equivalent to table 28.3. For S, I = 1, A = 3, B = 5, C = 7 and the law of combination is multiplication  mod 8 , whilst for S cid:7  , I = 1, A = 5, B = 7, C = 11 and the law of combination  1050   28.2 FINITE GROUPS  1 1 5 7 11  5 5 1 11 7  7 7 11 1 5  11 11 7 5 1   a   1 5 7 11  1 2 3 4 Table 28.2  a  The multiplication table for the group S cid:7  multiplication  mod 24 .  b  The multiplication table for the group S cid:7  cid:7  {1, 2, 3, 4} under multiplication  mod 5 .  = {1, 5, 7, 11} under   b   =  2 2 4 1 3  3 3 1 4 2  1 1 2 3 4  4 4 3 2 1  Table 28.3 The common structure exempliﬁed by tables 28.1 and 28.2 a .  I I A B C  A A I C B  B B C I A  C C B A I  I A B C  1 1 i  1 i  −1 −1 −i −i −i  1  −1 −i −1 −i  i i  −1 −i  1 i  −1  1 i  Table 28.4 The group table for the set {1, i,−1,−i} under ordinary multipli-  cation of complex numbers.  is multiplication  mod 24 . However, the really important point is that the two groups S and S cid:7  have equivalent group multiplication tables – they are said to be isomorphic, a matter to which we will return more formally in section 28.5.  cid:1 Determine the behaviour of the set of four elements {1, i,−1,−i}  under the ordinary multiplication of complex numbers. Show that they form a group and determine whether the group is isomorphic to either of the groups S  itself isomorphic to S cid:7    and S cid:7  cid:7   deﬁned above.  That the elements form a group under the associative operation of complex multiplication is immediate; there is an identity  1 , each possible product generates a member of the set  and each element has an inverse  1, −i, −1, i, respectively . The group table has the form  shown in table 28.4. We now ask whether this table can be made to look like table 28.3, which is the standardised form of the tables for S and S cid:7  . Since the identity element of the group  1  will have to be represented by I, and ‘1’ only appears on the leading diagonal twice whereas I appears on the leading diagonal four times in table 28.3, it is clear that no  1051   1 1 i  1 i  −1 −1 −i −i  −i  1  −1 −i −1 −i  i i  −1 −i  1 i  −1  1 1 2 4 3  2 2 4 3 1  4 4 3 1 2  3 3 1 2 4  1 2 4 3  Table 28.5 A comparison between tables 28.4 and 28.2 b , the latter with its columns reordered.  GROUP THEORY  1 i  I A B C  I I A B C  A A B C I  B B C I A  C C I A B  Table 28.6 The common structure exempliﬁed by tables 28.4 and 28.2 b , the latter with its columns reordered.  amount of relabelling  or, equivalently, no allocation of the symbols A, B, C, amongst  i, −1, −i  can bring table 28.4 into the form of table 28.3. We conclude that the group {1, i,−1,−i} is not isomorphic to S or S cid:7   . An alternative way of stating the observation is to say that the group contains only one element of order 2 whilst a group corresponding to table 28.3 contains three such elements.  However, if the rows and columns of table 28.2 b  – in which the identity does appear twice on the diagonal and which therefore has the potential to be equivalent to table 28.4 – are rearranged by making the heading order 1, 2, 4, 3 then the two tables can be compared in the forms shown in table 28.5. They can thus be seen to have the same structure, namely that shown in table 28.6.  We therefore conclude that the group of four elements {1, i,−1,−i} under ordinary mul- tiplication of complex numbers is isomorphic to the group {1, 2, 3, 4} under multiplication  mod 5 .  cid:2   What we have done does not prove it, but the two tables 28.3 and 28.6 are in fact the only possible tables for a group of order 4, i.e. a group containing exactly four elements.  28.3 Non-Abelian groups  So far, all the groups for which we have constructed multiplication tables have been based on some form of arithmetic multiplication, a commutative operation, with the result that the groups have been Abelian and the tables symmetric about the leading diagonal. We now turn to examples of groups in which some non-commutation occurs. It should be noted, in passing, that non-commutation cannot occur throughout a group, as the identity always commutes with any element in its group.  1052   28.3 NON-ABELIAN GROUPS  As a ﬁrst example we consider again as elements of a group the two-dimensional operations which transform an equilateral triangle into itself  see the end of subsection 28.1.1 . It has already been shown that there are six such operations: the null operation, two rotations  by 2π 3 and 4π 3 about an axis perpendicular to the plane of the triangle  and three reﬂections in the perpendicular bisectors of the three sides. To abbreviate we will denote these operations by symbols as follows.   i  I is the null operation.  ii  R and R  iii  K, L, M are reﬂections in the three lines indicated in ﬁgure 28.2.  are  clockwise  rotations by 2π 3 and 4π 3 respectively.   cid:7   Some products of the operations of the form X   Y  where it will be recalled that the symbol   means that the second operation X is carried out on the system  resulting from the application of the ﬁrst operation Y   are easily calculated:  R   R = R   cid:7   ,   cid:7    cid:7    R  R   R  cid:7  K   K = L   L = M   M = I.  = R,  R  = I = R   cid:7    R   28.12   Others, such as K   M, are more diﬃcult, but can be found by a little thought,  or by making a model triangle or drawing a sequence of diagrams such as those following.  K   M  x  x  = K  =   cid:7   R  =  x  showing that K   M = R   cid:7   . In the same way,  = M x  x  =  x  =  R  shows that M   K = R, and  M   K  R   L  = R  =  =  K  x  x  x  shows that R   L = K.  Proceeding in this way we can build up the complete multiplication table it is not necessary to draw any more diagrams, as all  table 28.7 . In fact, remaining products can be deduced algebraically from the three found above and  1053  x  x  x   GROUP THEORY   cid:7   cid:7   K K  R R I M K R  R I R I  cid:7  R R  cid:7  I R K L M I L M K R R  I R  cid:7  R K L M M K  L M L M L L M K  cid:7  R  cid:7  R I  R I  cid:7  R  L  Table 28.7 The group table for the two-dimensional symmetry operations on an equilateral triangle.  the more self-evident results given in  28.12 . A number of things may be noticed about this table.   i  It is not symmetric about the leading diagonal, indicating that some pairs  of elements in the group do not commute.   ii  There is some symmetry within the 3×3 blocks that form the four quarters  of the table. This occurs because we have elected to put similar operations close to each other when choosing the order of table headings – the two rotations  or three if I is viewed as a rotation by 0π 3  are next to each other, and the three reﬂections also occupy adjacent columns and rows. We will return to this later.  That two groups of the same order may be isomorphic carries over to non- Abelian groups. The next two examples are each concerned with sets of six objects; they will be shown to form groups that, although very diﬀerent in nature from the rotation–reﬂection group just considered, are isomorphic to it.   cid:31  We consider ﬁrst the set M of six orthogonal 2 × 2 matrices given by  cid:31   28.13    cid:30  − 1 − √  cid:30    cid:31   cid:31    cid:30   cid:30   − 1 √ 2 3 2  1 0 0 1  B =  A =  I =  3 2  2  3  2  2   cid:7   cid:8   cid:7  −1 0   cid:8   C =  0  1  D =  1 2  − √  3 2  E =  1 √ 2  3 2  √ 3 2 − 1 −√ − 1  2  2  3 2  −√ − 1 √ − 1  3 2  2  the combination law being that of ordinary matrix multiplication. Here we use italic, rather than the sans serif used for matrices elsewhere, to emphasise that the matrices are group elements.  Although it is tedious to do so, it can be checked that the product of any two of these matrices, in either order, is also in the set. However, the result is generally diﬀerent in the two cases, as matrix multiplication is non-commutative. The matrix I clearly acts as the identity element of the set, and during the checking for closure it is found that the inverse of each matrix is contained in the set, I, C, D and E being their own inverses. The group table is shown in table 28.8.  1054   28.3 NON-ABELIAN GROUPS  A A B I  I I A B C D E C  I A B C D D E E  B B I A D E A I B  C D E C D E C D E C B A I  I B C D A  E  Table 28.8 The group table, under matrix multiplication, for the set M of six orthogonal 2 × 2 matrices given by  28.13 .  The similarity to table 28.7 is striking. If {R, R , K, L, M} of that table are replaced by {A, B, C, D, E} respectively, the two tables are identical, without even   cid:7   the need to reshuﬄe the rows and columns. The two groups, one of reﬂections and rotations of an equilateral triangle, the other of matrices, are isomorphic.  Our second example of a group isomorphic to the same rotation–reﬂection group is provided by a set of functions of an undetermined variable x. The functions are as follows:  f1 x  = x,  f4 x  = 1 x,  f2 x  = 1  1 − x , f5 x  = 1 − x,  f3 x  =  x − 1  x, f6 x  = x  x − 1 ,  and the law of combination is  fi x    fj x  = fi fj  x  ,  i.e. the function on the right acts as the argument of the function on the left to produce a new function of x. It should be emphasised that it is the functions that are the elements of the group. The variable x is the ‘system’ on which they act, and plays much the same role as the triangle does in our ﬁrst example of a non-Abelian group.  To show an explicit example, we calculate the product f6   f3. The product will be the function of x obtained by evaluating y  y − 1 , when y is set equal to  x − 1  x. Explicitly  f6 f3  =   x − 1  x   x − 1  x − 1  = 1 − x = f5 x .  Thus f6   f3 = f5. Further examples are 1 − 1  1 − x   f2   f2 =  1  x − 1  x  =  = f3,  and  f6   f6 =  x  x − 1   x  x − 1  − 1  1055  = x = f1.   28.14    GROUP THEORY  The multiplication table for this set of six functions has all the necessary proper- ties to show that they form a group. Further, if the symbols f1, f2, f3, f4, f5, f6 are replaced by I, A, B, C, D, E respectively the table becomes identical to table 28.8. This justiﬁes our earlier claim that this group of functions, with argument sub- stitution as the law of combination, is isomorphic to the group of reﬂections and rotations of an equilateral triangle.  28.4 Permutation groups  The operation of rearranging n distinct objects amongst themselves is called a permutation of degree n, and since many symmetry operations on physical systems can be viewed in that light, the properties of permutations are of interest. For example, the symmetry operations on an equilateral triangle, to which we have already given much attention, can be considered as the six possible rearrangements of the marked corners of the triangle amongst three ﬁxed points in space, much as in the diagrams used to compute table 28.7. In the same way, the symmetry operations on a cube can be viewed as a rearrangement of its corners amongst eight points in space, albeit with many constraints, or, with fewer complications, as a rearrangement of its body diagonals in space. The details will be left until we review the possible ﬁnite groups more systematically.  The notations and conventions used in the literature to describe permutations are very varied and can easily lead to confusion. We will try to avoid this by using letters a, b, c, . . .  rather than numbers  for the objects that are rearranged by a permutation and by adopting, before long, a ‘cycle notation’ for the permutations themselves. It is worth emphasising that it is the permutations, i.e. the acts of rearranging, and not the objects themselves  represented by letters  that form the elements of permutation groups. The complete group of all permutations of degree n is usually denoted by Sn or Σn. The number of possible permutations of degree n is n!, and so this is the order of Sn. Suppose the ordered set of six distinct objects {a b c d e f} is rearranged by some process into {b e f a d c}; then we can represent this mathematically as  θ{a b c d e f} = {b e f a d c},  where θ is a permutation of degree 6. The permutation θ can be denoted by [2 5 6 1 4 3], since the ﬁrst object, a, is replaced by the second, b, the second object, b, is replaced by the ﬁfth, e, the third by the sixth, f, etc. The equation can then be written more explicitly as  θ{a b c d e f} = [2 5 6 1 4 3]{a b c d e f} = {b e f a d c}.  If φ is a second permutation, also of degree 6, then the obvious interpretation of  the product φ   θ of the two permutations is  φ   θ{a b c d e f} = φ θ{a b c d e f} .  1056   28.4 PERMUTATION GROUPS  Suppose that φ is the permutation [4 5 3 6 2 1]; then  φ   θ{a b c d e f} = [4 5 3 6 2 1][2 5 6 1 4 3]{a b c d e f}  = [4 5 3 6 2 1]{b e f a d c} = {a d f c e b} = [1 4 6 3 5 2]{a b c d e f}.  Written in terms of the permutation notation this result is  [4 5 3 6 2 1][2 5 6 1 4 3] = [1 4 6 3 5 2].  A concept that is very useful for working with permutations is that of decom- position into cycles. The cycle notation is most easily explained by example. For the permutation θ given above:  the 1st object, a, has been replaced by the 2nd, b; the 2nd object, b, has been replaced by the 5th, e; the 5th object, e, has been replaced by the 4th, d; the 4th object, d, has been replaced by the 1st, a.  This brings us back to the beginning of a closed cycle, which is conveniently represented by the notation  1 2 5 4 , in which the successive replacement positions are enclosed, in sequence, in parentheses. Thus  1 2 5 4  means 2nd  → 1st, 5th → 2nd, 4th → 5th, 1st → 4th. It should be noted that the object  initially in the ﬁrst listed position replaces that in the ﬁnal position indicated in the bracket – here ‘a’ is put into the fourth position by the permutation. Clearly the cycle  5 4 1 2 , or any other that involved the same numbers in the same relative order, would have exactly the same meaning and eﬀect. The remaining two objects, c and f, are interchanged by θ or, more formally, are rearranged according to a cycle of length 2, a transposition, represented by  3 6 . Thus the complete representation  speciﬁcation  of θ is  θ =  1 2 5 4  3 6 .  The positions of objects that are unaltered by a permutation are either placed by themselves in a pair of parentheses or omitted altogether. The former is recom- mended as it helps to indicate how many objects are involved – important when the object in the last position is unchanged, or the permutation is the identity, which leaves all objects unaltered in position! Thus the identity permutation of degree 6 is  I =  1  2  3  4  5  6 ,  though in practice it is often shortened to  1 .  It will be clear that the cycle representation is unique, to within the internal absolute ordering of the numbers in each bracket as already noted, and that  1057   GROUP THEORY  each number appears once and only once in the representation of any particular permutation.  The order of any permutation of degree n within the group Sn can be read oﬀ from the cyclic representation and is given by the lowest common multiple  LCM  of the lengths of the cycles. Thus I has order 1, as it must, and the permutation θ discussed above has order 4  the LCM of 4 and 2 .  Expressed in cycle notation our second permutation φ is  3  1 4 6  2 5 , and  the product φ   θ is calculated as   3  1 4 6  2 5     1 2 5 4  3 6 {a b c d e f} =  3  1 4 6  2 5 {b e f a d c} = {a d f c e b} =  1  5  2 4 3 6 {a b c d e f}.  i.e. expressed as a relationship amongst the elements of the group of permutations of degree 6  not yet proved as a group, but reasonably anticipated , this result reads   3  1 4 6  2 5     1 2 5 4  3 6  =  1  5  2 4 3 6 .  We note, for practice, that φ has order 6  the LCM of 1, 3, and 2  and that the  product φ   θ has order 4. The number of elements in the group Sn of all permutations of degree n is n! and clearly increases very rapidly as n increases. Fortunately, to illustrate the essential features of permutation groups it is suﬃcient to consider the case n = 3, which involves only six elements. They are as follows  with labelling which the reader will by now recognise as anticipatory :  I =  1  2  3  A =  1 2 3  B =  1 3 2  C =  1  2 3  D =  3  1 2  E =  2  1 3   It will be noted that A and B have order 3, whilst C, D and E have order 2. As perhaps anticipated, their combination products are exactly those corresponding to table 28.8, I, C, D and E being their own inverses. For example, putting in all steps explicitly,  D   C{a b c} =  3  1 2     1  2 3 {a b c}  In brief, the six permutations belonging to S3 form yet another non-Abelian group isomorphic to the rotation–reﬂection symmetry group of an equilateral triangle.  =  3  12 {a c b} = {c a b} =  3 2 1 {a b c} =  1 3 2 {a b c} = B{a b c}.  1058   28.5 MAPPINGS BETWEEN GROUPS  28.5 Mappings between groups  Now that we have available a range of groups that can be used as examples, we return to the study of more general group properties. From here on, when  there is no ambiguity we will write the product of two elements, X   Y , simply  as XY , omitting the explicit combination symbol. We will also continue to use ‘multiplication’ as a loose generic name for the combination process between elements of a group.  are two groups, we can study the eﬀect of a mapping  Φ : G → G cid:7   . If X is an element of G we denote its image in G cid:7   under the mapping  If G and G cid:7   of G onto G cid:7  Φ by X   cid:7   = Φ X .  A technical term that we have already used is isomorphic. We will now deﬁne  it formally. Two groups G = {X, Y , . . .} and G cid:7   = {X   cid:7    cid:7   , . . .} are said to be  , Y  isomorphic if there is a one-to-one correspondence  X ↔ X   cid:7   , Y ↔ Y   cid:7   , ···  between their elements such that  XY = Z  implies  X  Y  = Z   cid:7    cid:7    cid:7   and vice versa.  In other words, isomorphic groups have the same  multiplication  structure, although they may diﬀer in the nature of their elements, combination law and notation. Clearly if groups G and G cid:7  are isomorphic, then it follows that G cid:7  are isomorphic. We have already seen an example of four groups  of functions of x, of orthogonal matrices, of permutations and of the symmetries of an equilateral triangle  that are isomorphic, all having table 28.8 as their multiplication table.  are isomorphic, and G and G cid:7  cid:7   and G cid:7  cid:7   Although our main interest is in isomorphic relationships between groups, the wider question of mappings of one set of elements onto another is of some importance, and we start with the more general notion of a homomorphism.  be two groups and Φ a mapping of G → G cid:7   . If for every pair of  Let G and G cid:7  elements X and Y in G   cid:7   XY   then Φ is called a homomorphism, and G cid:7    cid:7    cid:7   Y  = X is said to be a homomorphic image of G.   cid:7  The essential deﬁning relationship, expressed by  XY    , is that the same result is obtained whether the product of two elements is formed ﬁrst and the image then taken or the images are taken ﬁrst and the product then formed.  = X  Y   cid:7    cid:7   1059   GROUP THEORY  Three immediate consequences of the above deﬁnition are proved as follows.   i  If I is the identity of G then IX = X for all X in G. Consequently   cid:7  = I is the identity in G cid:7  for all X of G maps into the identity element of G cid:7  .   cid:7  =  IX   in G cid:7   . Thus I  X   cid:7    cid:7    cid:7    cid:7   X  ,  . In words, the identity element   ii  Further,  −1   cid:7  That is,  X element in G cid:7    cid:7   I  =  XX  = X   X  −1   cid:7    cid:7   −1   cid:7   .   cid:7   −1. In words, the image of an inverse is the same     =  X   iii  If element X in G is of order m, i.e. I = Xm, then  as the inverse of the image.   cid:7   I   cid:7  =  Xm   =  XXm−1   cid:7    cid:7    Xm−1   cid:7   = X  8 = ··· = X   cid:7   9: ;  cid:7  ··· X m factors  X   cid:7   .  In words, the image of an element has the same order as the element.  What distinguishes an isomorphism from the more general homomorphism are  the requirements that in an isomorphism:   I  diﬀerent elements in G must map into diﬀerent elements in G cid:7    whereas in a homomorphism several elements in G may have the same image in G cid:7   , that is, x  must imply x = y;  = y   cid:7    cid:7    II  any element in G cid:7   must be the image of some element in G.  An immediate consequence of  I  and result  iii  for homomorphisms is that isomorphic groups each have the same number of elements of any given order. For a general homomorphism, the set of elements of G whose image in G cid:7   cid:7  is I is called the kernel of the homomorphism; this is discussed further in the next section. In an isomorphism the kernel consists of the identity I alone. To illustrate both this point and the general notion of a homomorphism, consider  a mapping between the additive group of real numbers   and the multiplicative   → U 1  is  group of complex numbers with unit modulus, U 1 . Suppose that the mapping  Φ : x → eix;  then this is a homomorphism since   cid:7  → ei x+y  = eixeiy = x   cid:7    cid:7   y  .   x + y   However, it is not an isomorphism because many  an inﬁnite number  of the  elements of   have the same image in U 1 . For example, π, 3π, 5π, . . . in   all have the image −1 in U 1  and, furthermore, all elements of   of the form 2πn,  where n is an integer, map onto the identity element in U 1 . The latter set forms the kernel of the homomorphism.  1060   28.6 SUBGROUPS   a    b   I I  I A A B I B C D D E E  B C D E C  A B A B I A D E A I B  C D E C D E C D E C B A I  I B C D A  E  I I I A A B B C C  A B A B I C I C B A  C C B A I  Table 28.9 Reproduction of  a  table 28.8 and  b  table 28.3 with the relevant subgroups shown in bold.  For the sake of completeness, we add that a homomorphism for which  I  above holds is said to be a monomorphism  or an isomorphism into , whilst a homomor- phism for which  II  holds is called an epimorphism  or an isomorphism onto . If, in either case, the other requirement is met as well then the monomorphism or epimorphism is also an isomorphism. G → G cid:7   Finally, if the initial and ﬁnal groups are the same, G = G cid:7   , then the isomorphism  is termed an automorphism.  28.6 Subgroups  More detailed inspection of tables 28.8 and 28.3 shows that not only do the complete tables have the properties associated with a group multiplication table  see section 28.2  but so do the upper left corners of each table taken on their own. The relevant parts are shown in bold in the tables 28.9 a  and  b .  This observation immediately prompts the notion of a subgroup. A subgroup of a group G can be formally deﬁned as any non-empty subset H = {Hi} of G, the elements of which themselves behave as a group under the same rule of combination as applies in G itself. As for all groups, the order of the subgroup is equal to the number of elements it contains; we will denote it by h or H. Any group G contains two trivial subgroups:  i  G itself;  ii  the set I consisting of the identity element alone.  All other subgroups of G are termed proper subgroups. In a group with multipli- cation table 28.8 the elements {I, A, B} form a proper subgroup, as do {I, A} in a  group with table 28.3 as its group table.  Some groups have no proper subgroups. For example, the so-called cyclic groups, mentioned at the end of subsection 28.1.1, have no subgroups other than the whole group or the identity alone. Tables 28.10 a  and  b  show the multiplication tables for two of these groups. Table 28.6 is also the group table for a cyclic group, that of order 4.  1061   GROUP THEORY   a    b   I I  I A A B I B  B  A B A B I A  I I A B C D I  A A B C D I A  B B C D I A B  C D C D I A B C  I A B C D D  Table 28.10 The group tables of two cyclic groups, of orders 3 and 5. They have no proper subgroups.  It will be clear that for a cyclic group G repeated combination of any element with itself generates all other elements of G, before ﬁnally reproducing itself. So, for example, in table 28.10 b , starting with  say  D, repeated combination with itself produces, in turn, C, B, A, I and ﬁnally D again. As noted earlier, in any cyclic group G every element, apart from the identity, is of order g, the order of  the group itself.  The two tables shown are for groups of orders 3 and 5. It will be proved in subsection 28.7.2 that the order of any group is a multiple of the order of any of its subgroups  Lagrange’s theorem , i.e. in our general notation, g is a multiple of h. It thus follows that a group of order p, where p is any prime, must be cyclic and cannot have any proper subgroups. The groups for which tables 28.10 a  and  b  are the group tables are two such examples. Groups of non-prime order may  table 28.3  or may not  table 28.6  have proper subgroups.  As we have seen, repeated multiplication of an element X  not the identity   by itself will generate a subgroup {X, X2, X3, . . .}. The subgroup will clearly be Abelian, and if X is of order m, i.e. Xm = I, the subgroup will have m distinct members. If m is less than g – though, in view of Lagrange’s theorem, m must be a factor of g – the subgroup will be a proper subgroup. We can deduce, in passing, that the order of any element of a group is an exact divisor of the order of the group. Some obvious properties of the subgroups of a group G, which can be listed  without formal proof, are as follows.   i  The identity element of G belongs to every subgroup H.  ii  If element X belongs to a subgroup H, so does X  iii  The set of elements in G that belong to every subgroup of G themselves  −1.  form a subgroup, though this may consist of the identity alone.  Properties of subgroups that need more explicit proof are given in the follow- ing sections, though some need the development of new concepts before they can be established. However, we can begin with a theorem, applicable to all homomorphisms, not just isomorphisms, that requires no new concepts.  Let Φ : G → G cid:7   be a homomorphism of G into G cid:7   ; then  1062   28.7 SUBDIVIDING A GROUP   i  the set of elements H cid:7   ii  the set of elements K in G that are mapped onto the identity I  that are images of the elements of G forms a  in G cid:7   in G cid:7   forms   cid:7   subgroup of G cid:7  ; a subgroup of G.  As indicated in the previous section, the subgroup K is called the kernel of the  homomorphism.  To prove  i , suppose Z and W belong to H cid:7   X and Y belong to G. Then  , with Z = X  and W = Y  , where   cid:7    cid:7   and therefore belongs to H cid:7   , and  Z W = X  Y   cid:7    cid:7    cid:7  =  XY    −1 =  X   cid:7   −1 =  X  −1   cid:7      Z  and therefore belongs to H cid:7  belongs to H cid:7   , are enough to establish result  i .  To prove  ii , suppose X and Y belong to K; then  . These two results, together with the fact that I   cid:7    cid:7   XY     cid:7    cid:7   Y  = X −1   cid:7    cid:7   I  =  XX  = X   cid:7    cid:7    cid:7   = I  cid:7    X  I  = I −1   cid:7   = I   cid:7    closure ,  −1   cid:7    X  −1   cid:7   =  X  −1 belongs to K. These two results, together with the fact that I and therefore X belongs to K, are enough to establish  ii . An illustration of this result is provided by the mapping Φ of   → U 1  considered in the previous section. Its kernel consists of the set of real numbers of the form 2πn, where n is an integer; it forms a subgroup of R, the additive group of real numbers. In fact the kernel K of a homomorphism is a normal subgroup of G. The deﬁning property of such a subgroup is that for every element X in G and every −1 belongs to the subgroup. This property is element Y in the subgroup, XY X easily veriﬁed for the kernel K, since −1   cid:7   −1   cid:7   −1   cid:7   −1   cid:7    XY X  = X  = X  = X  = I   X   X   X  Y  I   cid:7    cid:7    cid:7    cid:7    cid:7    cid:7   .  Anticipating the discussion of subsection 28.7.2, the cosets of a normal subgroup themselves form a group  see exercise 28.16 .  28.7 Subdividing a group  We have already noted, when looking at the  arbitrary  order of headings in a group table, that some choices appear to make the table more orderly than do others. In the following subsections we will identify ways in which the elements of a group can be divided up into sets with the property that the members of any one set are more like the other members of the set, in some particular regard,  1063   GROUP THEORY  than they are like any element that does not belong to the set. We will ﬁnd that these divisions will be such that the group is partitioned, i.e. the elements will be divided into sets in such a way that each element of the group belongs to one, and only one, such set.  We note in passing that the subgroups of a group do not form such a partition, not least because the identity element is in every subgroup, rather than being in precisely one. In other words, despite the nomenclature, a group is not simply the aggregate of its proper subgroups.  28.7.1 Equivalence relations and classes  We now specify in a more mathematical manner what it means for two elements of a group to be ‘more like’ one another than like a third element, as mentioned in section 28.2. Our introduction will apply to any set, whether a group or not, but our main interest will ultimately be in two particular applications to groups. We start with the formal deﬁnition of an equivalence relation.  An equivalence relation on a set S is a relationship X ∼ Y , between two elements X and Y belonging to S, in which the deﬁnition of the symbol ∼ must  satisfy the requirements of   i  reﬂexivity, X ∼ X;  ii  symmetry, X ∼ Y implies Y ∼ X;  iii  transitivity, X ∼ Y and Y ∼ Z imply X ∼ Z.  Any particular two elements either satisfy or do not satisfy the relationship.  The general notion of an equivalence relation is very straightforward, and  the requirements on the symbol ∼ seem undemanding; but not all relationships  qualify. As an example within the topic of groups, if it meant ‘has the same order as’ then clearly all the requirements would be satisﬁed. However, if it meant ‘commutes with’ then it would not be an equivalence relation, since although A commutes with I, and I commutes with C, this does not necessarily imply that A commutes with C, as is obvious from table 28.8.  It may be shown that an equivalence relation on S divides up S into classes Ci  such that:   i  X and Y belong to the same class if, and only if, X ∼ Y ;  ii  every element W of S belongs to exactly one class.  This may be shown as follows. Let X belong to S, and deﬁne the subset SX of S to be the set of all elements U of S such that X ∼ U. Clearly by reﬂexivity X belongs to SX. Suppose ﬁrst that X ∼ Y , and let Z be any element of SY . Then Y ∼ Z, and hence by transitivity X ∼ Z, which means that Z belongs to SX. Conversely, since the symmetry law gives Y ∼ X, if Z belongs to SX then  1064   28.7 SUBDIVIDING A GROUP  this implies that Z belongs to SY . These two results together mean that the two subsets SX and SY have the same members and hence are equal. Now suppose that SX equals SY . Since Y belongs to SY it also belongs to SX and hence X ∼ Y . This completes the proof of  i , once the distinct subsets of type SX are identiﬁed as the classes Ci. Statement  ii  is an immediate corollary, the class in question being identiﬁed as SW .  The most important property of an equivalence relation is as follows.  only one, of the classes.  Two diﬀerent subsets SX and SY can have no element in common, and the collection of all the classes Ci is a ‘partition’ of S, i.e. every element in S belongs to one, and To prove this, suppose SX and SY have an element Z in common; then X ∼ Z and Y ∼ Z and so by the symmetry and transitivity laws X ∼ Y . By the above theorem this implies SX equals SY . But this contradicts the fact that SX and SY are diﬀerent subsets. Hence SX and SY can have no element in common. Finally, if the elements of S are used in turn to deﬁne subsets and hence classes in S, every element U is in the subset SU that is either a class already found or constitutes a new one. It follows that the classes exhaust S, i.e. every element is in some class. Having established the general properties of equivalence relations, we now turn to two speciﬁc examples of such relationships, in which the general set S has the more specialised properties of a group G and the equivalence relation ∼ is chosen  in such a way that the relatively transparent general results for equivalence relations can be used to derive powerful, but less obvious, results about the properties of groups.  28.7.2 Congruence and cosets  As the ﬁrst application of equivalence relations we now prove Lagrange’s theorem which is stated as follows.  Lagrange’s theorem. If G is a ﬁnite group of order g and H is a subgroup of G of order h then g is a multiple of h. We take as the deﬁnition of ∼ that, given X and Y belonging to G, X ∼ Y if −1Y belongs to H. This is the same as saying that Y = XHi for some element Hi belonging to H; technically X and Y are said to be left-congruent with respect to H.  X  This deﬁnes an equivalence relation, since it has the following properties.   i  Reﬂexivity: X ∼ X, since X  ii  Symmetry: X ∼ Y implies that X  its inverse, since H is a group. But  X to H, it follows that Y ∼ X.  −1X = I and I belongs to any subgroup.  −1Y belongs to H and so, therefore, does −1X and, as this belongs  −1 = Y  −1Y    1065   GROUP THEORY   iii  Transitivity: X ∼ Y and Y ∼ Z imply that X −1Y   Y  and so, therefore, does their product  X  −1Y and Y −1Z  = X  −1Z belong to H −1Z, from which  it follows that X ∼ Z.  With ∼ proved as an equivalence relation, we can immediately deduce that it divides G into disjoint  non-overlapping  classes. For this particular equivalence relation the classes are called the left cosets of H. Thus each element of G is in one and only one left coset of H. The left coset containing any particular X is usually written XH, and denotes the set of elements of the form XHi  one of which is X itself since H contains the identity element ; it must contain h diﬀerent  elements, since if it did not, and two elements were equal,  XHi = XHj,  From our general results about equivalence relations it now follows that the  we could deduce that Hi = Hj and that H contained fewer than h elements. left cosets of H are a ‘partition’ of G into a number of sets each containing h members. Since there are g members of G and each must be in just one of the sets, it follows that g is a multiple of h. This concludes the proof of Lagrange’s theorem. The number of left cosets of H in G is known as the index of H in G and is written [G : H]; numerically the index = g h. For the record we note that, for the trivial subgroup I, which contains only the identity element, [G : I] = g and that, for a subgroup J of subgroup H, [G : H][H : J ] = [G : J ].  The validity of Lagrange’s theorem was established above using the far-reaching properties of equivalence relations. However, for this speciﬁc purpose there is a more direct and self-contained proof, which we now give.  Let X be some particular element of a ﬁnite group G of order g, and H be a subgroup of G of order h, with typical element Yi. Consider the set of elements  XH ≡ {XY1, XY2, . . . , XYh}.  This set contains h distinct elements, since if any two were equal, i.e. XYi = XYj with i  cid:3 = j, this would contradict the cancellation law. As we have already seen, the set is called a left coset of H.  We now prove three simple results.    Two cosets are either disjoint or identical. Suppose cosets X1H and X2H have an element in common, i.e. X1Y1 = X2Y2 for some Y1, Y2 in H. Then X1 = ; thus X1 belongs to the left coset X2H. Similarly X2 belongs to the left coset X1H.  , and since Y1 and Y2 both belong to H so does Y2Y  X2Y2Y  −1  −1  1  1  Consequently, either the two cosets are identical or it was wrong to assume that they have an element in common.  1066   28.7 SUBDIVIDING A GROUP    Two cosets X1H and X2H are identical if, and only if, X  2 X1 belongs to H then X1 = X2Yi for some i, and −1 X1H = X2YiH = X2H,  X  2 X1 belongs to H. If −1  since by the permutation law YiH = H. Thus the two cosets are identical. Conversely, suppose X1H = X2H. Then X 2 X1H = H. But one element of −1 H  on the left of the equation  is I; thus X 2 X1 must also be an element of H −1   Every element of G is in some left coset XH. This follows trivially since H contains I, and so the element Xi is in the coset XiH.   on the right . This proves the stated result.  The ﬁnal step in establishing Lagrange’s theorem is, as previously, to note that each coset contains h elements, that the cosets are disjoint and that every one of the g elements in G appears in one and only one distinct coset. It follows that g = kh for some integer k.  As noted earlier, Lagrange’s theorem justiﬁes our statement that any group of order p, where p is prime, must be cyclic and cannot have any proper subgroups: since any subgroup must have an order that divides p, this can only be 1 or p, corresponding to the two trivial subgroups I and the whole group.  It may be helpful to see an example worked through explicitly, and we again  use the same six-element group.  cid:1 Find the left cosets of the proper subgroup H of the group G that has table 28.8 as its  multiplication table.  The subgroup consists of the set of elements H = {I, A, B}. We note in passing that it has order 3, which, as required by Lagrange’s theorem, is a divisor of 6, the order of G. As in all cases, H itself provides the ﬁrst  left  coset, formally the coset  IH = {II, IA, IB} = {I, A, B}.  CH = {CI, CA, CB} = {C, D, E}.  We continue by choosing an element not already selected, C say, and form  These two cosets of H exhaust G, and are therefore the only cosets, the index of H in G being equal to 2.  This completes the example, but it is useful to demonstrate that it would not have  mattered if we had taken D, say, instead of I to form a ﬁrst coset  DH = {DI, DA, DB} = {D, E, C},  and then, from previously unselected elements, picked B, say:  BH = {BI, BA, BB} = {B, I, A}.  The same two cosets would have resulted.  cid:2   It will be noticed that the cosets are the same groupings of the elements of G which we earlier noted as being the choice of adjacent column and row headings that give the multiplication table its ‘neatest’ appearance. Furthermore,  1067   GROUP THEORY  if H is a normal subgroup of G then its  left  cosets themselves form a group  see  exercise 28.16 .  28.7.3 Conjugates and classes  Our second example of an equivalence relation is concerned with those elements  X and Y of a group G that can be connected by a transformation of the form i XGi, where Gi is an  appropriate  element of G. Thus X ∼ Y if there −1 Y = G exists an element Gi of G such that Y = G −1 i XGi. Diﬀerent pairs of elements X and Y will, in general, require diﬀerent group elements Gi. Elements connected in this way are said to be conjugates.  We ﬁrst need to establish that this does indeed deﬁne an equivalence relation,  as follows.   i  Reﬂexivity: X ∼ X, since X = I −1XI and I belongs to the group.  ii  Symmetry: X ∼ Y implies Y = G −1 −1 i XGi and therefore X =  G , and it follows that Y ∼ X. Since Gi belongs to G so does G −1 i  iii  Transitivity: X ∼ Y and Y ∼ Z imply Y = G −1 i XGi and Z = G  −1 −1Y G −1 j Y Gj −1X GiGj . Since Gi and Gj  belong to G so does GiGj, from which it follows that X ∼ Z.  −1 i XGiGj =  GiGj   and therefore Z = G  −1 j G     i  i  .  These results establish conjugacy as an equivalence relation and hence show that it divides G into classes, two elements being in the same class if, and only if, they are conjugate.  Immediate corollaries are:   i  If Z is in the class containing I then  Z = G  −1 i IGi = G  −1 i Gi = I.  Thus, since any conjugate of I can be shown to be I, the identity must be in a class by itself.   ii  If X is in a class by itself then  must imply that Y = X. But  Y = G  −1 i XGi  X = GiG  −1 i XGiG i  −1  for any Gi, and so  X = Gi G  −1 i XGi G  −1 i = GiY G  −1 i = GiXG i  −1  ,  i.e. XGi = GiX for all Gi. Thus commutation with all elements of the group is a necessary  and suﬃcient  condition for any particular group element to be in a class by itself. In an Abelian group each element is in a class by itself.  1068   28.7 SUBDIVIDING A GROUP   iii  In any group G the set S of elements in classes by themselves is an Abelian subgroup  known as the centre of G . We have shown that I belongs to S, and so if, further, XGi = GiX and Y Gi = GiY for all Gi belonging to G  then:   a   XY  Gi = XGiY = Gi XY  , i.e. the closure of S, and  b  XGi = GiX implies X  −1Gi = GiX Hence S is a group, and clearly Abelian.  to S.  −1, i.e. the inverse of X belongs  Yet again for illustration purposes, we use the six-element group that has  table 28.8 as its group table.  cid:1 Find the conjugacy classes of the group G having table 28.8 as its multiplication table.  As always, I is in a class by itself, and we need consider it no further.  −1AX, as X runs through the elements of G.  Consider next the results of forming X  −1AA B  −1AI A I = IA = IA = A = A  −1AB C = AI = A  −1AC D = CE = B  −1AD E = DC = B  −1AE = ED = B  Only A and B are generated. It is clear that {A, B} is one of the conjugacy classes of G.  −1BX; again only A and B appear.  This can be veriﬁed by forming all elements X  We now need to pick an element not in the two classes already found. Suppose we  −1CX, as X runs through the elements of G. The  pick C. Just as for A, we compute X calculations can be done directly using the table and give the following:  X X  : I  −1CX : C E D C E D  A B C D E  Thus C, D and E belong to the same class. The group is now exhausted, and so the three conjugacy classes are  {I},  {A, B},  {C, D, E}.  cid:2   In the case of this small and simple, but non-Abelian, group, only the identity is in a class by itself  i.e. only I commutes with all other elements . It is also the only member of the centre of the group.  Other areas from which examples of conjugacy classes can be taken include permutations and rotations. Two permutations can only be  but are not nec- essarily  in the same class if their cycle speciﬁcations have the same structure. For example, in S5 the permutations  1 3 5  2  4  and  2 5 3  1  4  could be in the same class as each other but not in the class that contains  1 5  2 4  3 . An example of permutations with the same cycle structure yet in diﬀerent conjugacy classes is given in exercise 29. 10.  In the case of the continuous rotation group, rotations by the same angle θ about any two axes labelled i and j are in the same class, because the group contains a rotation that takes the ﬁrst axis into the second. Without going into  1069   GROUP THEORY  Rj θ  = φ  −1 ij Ri θ φij ,  mathematical details, a rotation about axis i can be represented by the operator Ri θ , and the two rotations are connected by a relationship of the form  in which φij is the member of the full continuous rotation group that takes axis i into axis j.  28.8 Exercises  28.1  For each of the following sets, determine whether they form a group under the op- eration indicated  where it is relevant you may assume that matrix multiplication is associative :  the integers  mod 10  under addition;   a   b  the integers  mod 10  under multiplication;  c   d  the integers 1, 2, 3, 4, 5 under multiplication  mod 6 ;  e  all matrices of the form  the integers 1, 2, 3, 4, 5, 6 under multiplication  mod 7 ;   cid:7   where a and b are integers  mod 5  and a  cid:3 = 0  cid:3 = b, under matrix multiplica-  tion; those elements of the set in  e  that are of order 1 or 2  taken together ;   f   g  all matrices of the form   cid:8   ,   ,  a − b  b  a 0   1  a b  0 1 c  0 0 1  where a, b, c are integers, under matrix multiplication.  28.2  Which of the following relationships between X and Y are equivalence relations? Give a proof of your conclusions in each case:   a  X and Y are integers and X − Y is odd;  b  X and Y are integers and X − Y is even;  28.3   c  X and Y are people and have the same postcode;  d  X and Y are people and have a parent in common;  e  X and Y are people and have the same mother;  of a group G of n × n matrices.   f  X and Y are n×n matrices satisfying Y = P XQ, where P and Q are elements Deﬁne a binary operation   on the set of real numbers by  x   y = x + y + rxy, −1 if, and only if, x = −r  Prove that x   y = −r  where r is a non-zero real number. Show that the operation   is associative. that the set of all real numbers excluding −r ation  .  −1. Hence prove −1 forms a group under the oper-  −1 or y = −r  1070   28.8 EXERCISES  Y =  aX + b cX + d  ,  28.4  Prove that the relationship X ∼ Y , deﬁned by X ∼ Y if Y can be expressed in  the form  28.5  with a, b, c and d as integers, is an equivalence relation on the set of real numbers   . Identify the class that contains the real number 1.  The following is a ‘proof’ that reﬂexivity is an unnecessary axiom for an equiva- lence relation.  Because of symmetry X ∼ Y implies Y ∼ X. Then by transitivity X ∼ Y and Y ∼ X imply X ∼ X. Thus symmetry and transitivity imply reﬂexivity, which  therefore need not be separately required.  Demonstrate the ﬂaw in this proof using the set consisting of all real numbers plus the number i. Show by investigating the following speciﬁc cases that, whether or not reﬂexivity actually holds, it cannot be deduced from symmetry and transitivity alone.   a  X ∼ Y if X + Y is real.  b  X ∼ Y if XY is real. Prove that the set M of matrices  28.6   cid:7   A =  a 0  b c   cid:8   ,  where a, b, c are integers  mod 5  and a  cid:3 = 0  cid:3 = c, form a non-Abelian group Show that the subset containing elements of M that are of order 1 or 2 do  under matrix multiplication. not form a proper subgroup of M,  a  using Lagrange’s theorem,  cid:8   b  by direct demonstration that the set is not closed.  S is the set of all 2 × 2 matrices of the form   cid:7   Show that S is a group under matrix multiplication. Which element s  have order 2? Prove that an element A has order 3 if w + z + 1 = 0. Show that, under matrix multiplication, matrices of the form  A =  w x y z  ,  where wz − xy = 1.  cid:8   a0 + a1i −a2 + a3i a0 − a1i  a2 + a3i  ,   cid:7   M a0, a  =  where a0 and the components of column matrix a =  a1 0 + a2 = 1, constitute a group. Deduce that, under the numbers satisfying a2 transformation z → Mz, where z is any column matrix, z2 is invariant. If A is a group in which every element other than the identity, I, has order 2, prove that A is Abelian. Hence show that if X and Y are distinct elements of A, neither being equal to the identity, then the set {I, X, Y , XY } forms a subgroup of A. Deduce that if B is a group of order 2p, with p a prime greater than 2, then B  a3 T are real  a2  must contain an element of order p. The group of rotations  excluding reﬂections and inversions  in three dimensions that take a cube into itself is known as the group 432  or O in the usual chemical notation . Show by each of the following methods that this group has 24 elements.  28.7  28.8  28.9  28.10  1071   GROUP THEORY  28.11  28.12  28.13  28.14  28.15   a  Identify the distinct relevant axes and count the number of qualifying rota-  tions about each.   b  The orientation of the cube is determined if the directions of two of its body diagonals are given. Consider the number of distinct ways in which one body diagonal can be chosen to be ‘vertical’, say, and a second diagonal made to lie along a particular direction.  Identify the eight symmetry operations on a square. Show that they form a  group D4  known to crystallographers as 4mm and to chemists as C4v  having one   cid:7    cid:7    cid:7   , Y    =  XX  element of order 1, ﬁve of order 2 and two of order 4. Find its proper subgroups and the corresponding cosets.  If A and B are two groups, then their direct product, A × B, is deﬁned to be the set of ordered pairs  X, Y  , with X an element of A, Y an element of B  . Prove that A × B is a and multiplication given by  X, Y   X group. Denote the cyclic group of order n by Cn and the symmetry group of a regular n-sided ﬁgure  an n-gon  by Dn – thus D3 is the symmetry group of an equilateral  a  By considering the orders of each of their elements, show  i  that C2 × C3 is  b  Are any of D4, C8, C2 × C4, C2 × C2 × C2 isomorphic?  cid:8  Find the group G generated under matrix multiplication by the matrices  isomorphic to C6, and  ii  that C2 × D3 is isomorphic to D6.  triangle, as discussed in the text.   cid:8    cid:7    cid:7   , Y Y   cid:7   A =  0 1  1 0  ,  B =  0 i  i 0  .  Determine its proper subgroups, and verify for each of them that its cosets exhaust G. Show that if p is prime then the set of rational number pairs  a, b , excluding  0, 0 , with multiplication deﬁned by  √  √  √  forms an Abelian group. Show further that the mapping  a, b  →  a,−b  is an  automorphism. Consider the following mappings between a permutation group and a cyclic group.   a + b  p  c + d  p  = e + f  p,   a, b     c, d  =  e, f , where   a  Denote by An the subset of the permutation group Sn that contains all the  even permutations. Show that An is a subgroup of Sn.   b  List the elements of S3 in cycle notation and identify the subgroup A3.   c  For each element X of S3, let p X  = 1 if X belongs to A3 and p X  = −1 if it does not. Denote by C2 the multiplicative cyclic group of order 2. Determine the images of each of the elements of S3 for the following four mappings:  Φ1 : S3 → C2 Φ2 : S3 → C2 Φ3 : S3 → A3 Φ4 : S3 → S3  X → p X , X → −p X , X → X 2, X → X 3.  if so, whether the mapping is a homomorphism.   d  For each mapping, determine whether the kernel K is a subgroup of S3 and, For the group G with multiplication table 28.8 and proper subgroup H = {I, A, B}, denote the coset {I, A, B} by C1 and the coset {C, D, E} by C2. Form the set of all possible products of a member of C1 with itself, and denote this by C1C1.  28.16  1072   28.17  28.18  28.19  28.20  28.21  28.22  28.8 EXERCISES  GL n, R  → R ∗  Similarly compute C2C2, C1C2 and C2C1. Show that each product coset is equal to C1 or to C2, and that a 2 × 2 multiplication table can be formed, demonstrating that C1 and C2 are themselves the elements of a group of order 2. A subgroup like H whose cosets themselves form a group is a normal subgroup. The group of all non-singular n × n matrices is known as the general linear ∗ group GL n  and that with only real elements as GL n, R . If R denotes the multiplicative group of non-zero real numbers, prove that the mapping Φ :  , deﬁned by Φ M  = det M, is a homomorphism.  and show that they themselves form a group.  Show that the kernel K of Φ is a subgroup of GL n, R . Determine its cosets The group of reﬂection–rotation symmetries of a square is known as D4; let X be one of its elements. Consider a mapping Φ : D4 → S4, the permutation the set {x, y, d, d  cid:7 }, where x and y are the two principal axes, and d and d group on four objects, deﬁned by Φ X  = the permutation induced by X on  cid:7  the two principal diagonals, of the square. For example, if R is a rotation by π 2, Φ R  =  12  34 . Show that D4 is mapped onto a subgroup of S4 and, by constructing the multiplication tables for D4 and the subgroup, prove that the  mapping is a homomorphism. Given that matrix M is a member of the multiplicative group GL 3, R , determine, for each of the following additional constraints on M  applied separately , whether the subset satisfying the constraint is a subgroup of GL 3, R :   a  MT = M;  b  MT M = I; M = 1;  c   d  Mij = 0 for j > i and Mii  cid:3 = 0. The elements of the quaternion group, Q, are the set {1,−1, i,−i, j,−j, k,−k},  with i2 = j2 = k2 = −1, ij = k and its cyclic permutations, and ji = −k and its cyclic permutations. Find the proper subgroups of Q and the corresponding cosets. Show that the subgroup of order 2 is a normal subgroup, but that the other subgroups are not. Show that Q cannot be isomorphic to the group 4mm  C4v  considered in exercise 28.11. Show that D4, the group of symmetries of a square, has two isomorphic subgroups of order 4. Show further that there exists a two-to-one homomorphism from the quaternion group Q, of exercise 28.20, onto one  and hence either  of these two subgroups, and determine its kernel. Show that the matrices   cos θ − sin θ  cos θ  sin θ  0  0   ,  x y 1  M θ, x, y  =  where 0 ≤ θ < 2π, −∞ < x < ∞, −∞ < y < ∞, form a group under matrix  multiplication.  Show that those M θ, x, y  for which θ = 0 form a subgroup and identify its  cosets. Show that the cosets themselves form a group. Find  a  all the proper subgroups and  b  all the conjugacy classes of the symmetry group of a regular pentagon.  28.23  1073   GROUP THEORY  m1 π   m2 π   m3 π   m4 π   Figure 28.3 The notation for exercise 28.11.  28.9 Hints and answers  §   a  Yes,  b  no, there is no inverse for 2,  c  yes,  d  no, 2 × 3 is not in the set,   e  yes,  f  yes, they form a subgroup of order 4, [1, 0; 0, 1] [4, 0; 0, 4] [1, 2; 0, 4] [4, 3; 0, 1],  g  yes.  x   y   z  = x + y + z + r xy + xz + yz  + r2xyz =  x  y   z. Show that assuming x  y = −r −1 = −x  1 + rx ; show that this is not equal to −r  a  Consider both X = i and X  cid:3 = i. Here, i  cid:3 ∼ i.  b  In this case i ∼ i, but the  −1 leads to  rx + 1  ry + 1  = 0. The inverse of x is x  −1.  conclusion cannot be deduced from the other axioms. In both cases i is in a class by itself and no Y , as used in the false proof, can be found. Use AB = AB = 1×1 = 1 to prove closure. The inverse has w ↔ z, x ↔ −x, † y ↔ −y, giving A −1 = 1, i.e. it is in the set. The only element of order 2 is −I; A2 can be simpliﬁed to [− w + 1 ,−x;−y,− z + 1 ]. If XY = Z , show that Y = XZ and X = Z Y , then form Y X. Note that the elements of B can only have orders 1, 2 or p. Suppose they all have order 1 or 2; then using the earlier result, whilst noting that 4 does not divide 2p, leads to a contradiction. Using the notation indicated in ﬁgure 28.3, R being a rotation of π 2 about an axis perpendicular to the square, we have: I has order 1; R2, m1, m2, m3, m4 have order 2; R, R3 have order 4. subgroup {I, R, R2, R3} has cosets {I, R, R2, R3}, {m1, m2, m3, m4}; subgroup {I, R2, m1, m2} has cosets {I, R2, m1, m2}, {R, R3, m3, m4}; subgroup {I, R2, m3, m4} has cosets {I, R2, m3, m4}, {R, R3, m1, m2}; subgroup {I, R2} has cosets {I, R2}, {R, R3}, {m1, m2}, {m3, m4}; subgroup {I, m1} has cosets {I, m1}, {R, m3}, {R2, m2}, {R3, m4}; subgroup {I, m2} has cosets {I, m2}, {R, m4}, {R2, m1}, {R3, m3}; subgroup {I, m3} has cosets {I, m3}, {R, m2}, {R2, m4}, {R3, m1}; subgroup {I, m4} has cosets {I, m4}, {R, m1}, {R2, m3}, {R3, m2}. G = {I, A, B, B2, B3, AB, AB2, AB3}. The proper subgroups are as follows: {I, A}, {I, B2}, {I, AB2}, {I, B, B2, B3}, {I, B2, AB, AB3}.  b  A3 = { 1 ,  123 ,  132 }.  d  For Φ1, K = { 1 ,  123 ,  132 } is a subgroup. For Φ2, K = { 23 ,  13 ,  12 } is not a subgroup because it has no identity element. For Φ3, K = { 1 ,  23 ,  13 ,  12 } is not a subgroup because it is not closed.  28.1  28.3  28.5  28.7  28.9  28.11  28.13  28.15  §  Where matrix elements are given as a list, the convention used is [row 1; row 2; . . . ], individual entries in each row being separated by commas.  1074   28.9 HINTS AND ANSWERS  28.17  28.19 28.21  28.23  .   cid:7    cid:7   For Φ4, K = { 1 ,  123 ,  132 } is a subgroup.  is a homomorphism; Φ4  fails because, for example, [ 23  13 ]   cid:3 = Only Φ1  cid:7  Recall that, for any pair of matrices P and Q, PQ = PQ. K is the set of all  23   13  matrices with unit determinant. The cosets of K are the sets of matrices whose determinants are equal; K itself is the identity in the group of cosets.  a  No, because the set is not closed,  b  yes,  c  yes,  d  yes. Each subgroup contains the identity, a rotation by π, and two reﬂections. The  homomorphism is ±1 → I, ±i → R2, ±j → mx, ±k → my with kernel {1,−1}. There are 10 elements in all: I, rotations Ri  i = 1, 4  and reﬂections mj  j = 1, 5 .  a  There are ﬁve proper subgroups of order 2, {I, mj} and one proper subgroup of order 5, {I, R, R2, R3, R4}.  b  Four conjugacy classes, {I},{R, R4},{R2, R3},{m1, m2, m3, m4, m5}.  1075   29  Representation theory  As indicated at the start of the previous chapter, signiﬁcant conclusions can often be drawn about a physical system simply from the study of its symmetry properties. That chapter was devoted to setting up a formal mathematical basis, group theory, with which to describe and classify such properties; the current chapter shows how to implement the consequences of the resulting classiﬁcations and obtain concrete physical conclusions about the system under study. The connection between the two chapters is akin to that between working with coordinate-free vectors, each denoted by a single symbol, and working with a coordinate system in which the same vectors are expressed in terms of components. The ‘coordinate systems’ that we will choose will be ones that are expressed in terms of matrices; it will be clear that ordinary numbers would not be suﬃcient, as they make no provision for any non-commutation amongst the elements of a group. Thus, in this chapter the group elements will be represented by matrices that have the same commutation relations as the members of the group, whatever the group’s original nature  symmetry operations, functional forms, matrices, permutations, etc. . For some abstract groups it is diﬃcult to give a written description of the elements and their properties without recourse to such representations. Most of our applications will be concerned with representations of the groups that consist of the symmetry operations on molecules containing two or more identical atoms.  Firstly, in section 29.1, we use an elementary example to demonstrate the kind of conclusions that can be reached by arguing purely on symmetry grounds. Then in sections 29.2–29.10 we develop the formal side of representation theory and establish general procedures and results. Finally, these are used in section 29.11 to tackle a variety of problems drawn from across the physical sciences.  1076   29.1 DIPOLE MOMENTS OF MOLECULES  A   cid:7   A  b  CO2  B   cid:7   B   c  O3   a  HCl  Figure 29.1 Three molecules,  a  hydrogen chloride,  b  carbon dioxide and  c  ozone, for which symmetry considerations impose varying degrees of constraint on their possible electric dipole moments.  29.1 Dipole moments of molecules  Some simple consequences of symmetry can be demonstrated by considering whether a permanent electric dipole moment can exist in any particular molecule; three simple molecules, hydrogen chloride, carbon dioxide and ozone, are illus- trated in ﬁgure 29.1. Even if a molecule is electrically neutral, an electric dipole moment will exist in it if the centres of gravity of the positive charges  due to protons in the atomic nuclei  and of the negative charges  due to the electrons  do not coincide.  For hydrogen chloride there is no reason why they should coincide; indeed, the normal picture of the binding mechanism in this molecule is that the electron from the hydrogen atom moves its average position from that of its proton nucleus to somewhere between the hydrogen and chlorine nuclei. There is no compensating movement of positive charge, and a net dipole moment is to be expected – and is found experimentally.  For the linear molecule carbon dioxide it seems obvious that it cannot have a dipole moment, because of its symmetry. Putting this rather more rigorously, we note that any rotation about the long axis of the molecule leaves it totally unchanged; consequently, any component of a permanent electric dipole perpen- dicular to that axis must be zero  a non-zero component would rotate although no physical change had taken place in the molecule . That only leaves the pos- sibility of a component parallel to the axis. However, a rotation of π radians  cid:7  shown in ﬁgure 29.1 b  carries the molecule into itself, as about the axis AA does a reﬂection in a plane through the carbon atom and perpendicular to the molecular axis  i.e. one with its normal parallel to the axis . In both cases the two oxygen atoms change places but, as they are identical, the molecule is indistin- guishable from the original. Either ‘symmetry operation’ would reverse the sign of any dipole component directed parallel to the molecular axis; this can only be compatible with the indistinguishability of the original and ﬁnal systems if the parallel component is zero. Thus on symmetry grounds carbon dioxide cannot have a permanent electric dipole moment.  1077   REPRESENTATION THEORY   cid:7   Finally, for ozone, which is angular rather than linear, symmetry does not place such tight constraints. A dipole-moment component parallel to the axis  ﬁgure 29.1 c   is possible, since there is no symmetry operation that reverses BB the component in that direction and at the same time carries the molecule into an indistinguishable copy of itself. However, a dipole moment perpendicular to would both reverse any BB such component and carry the ozone molecule into itself – two contradictory conclusions unless the component is zero.  is not possible, since a rotation of π about BB   cid:7    cid:7   In summary, symmetry requirements appear in the form that some or all components of permanent electric dipoles in molecules are forbidden; they do not show that the other components do exist, only that they may. The greater the symmetry of the molecule, the tighter the restrictions on potentially non-zero components of its dipole moment.  In section 23.11 other, more complicated, physical situations will be analysed using results derived from representation theory. In anticipation of these results, and since it may help the reader to understand where the developments in the next nine sections are leading, we make here a broad, powerful, but rather formal, statement as follows.  If a physical system is such that after the application of particular rotations or reﬂections  or a combination of the two  the ﬁnal system is indistinguishable from the original system then its behaviour, and hence the functions that describe its behaviour, must have the corresponding property of invariance when subjected to the same rotations and reﬂections.  29.2 Choosing an appropriate formalism  As mentioned in the introduction to this chapter, the elements of a ﬁnite group  G can be represented by matrices; this is done in the following way. A suitable § column matrix u, known as a basis vector, its components ui, the basis functions, as u =  u1 u2 ··· un T. The ui may be of  is chosen and is written in terms of  a variety of natures, e.g. numbers, coordinates, functions or even a set of labels, though for any one basis vector they will all be of the same kind.  Once chosen, the basis vector can be used to generate an n-dimensional rep- resentation of the group as follows. An element X of the group is selected and its eﬀect on each basis function ui is determined. If the action of X on u1 is to  cid:7  1, etc. then the set of equations produce u   29.1   §  This usage of the term basis vector is not exactly the same as that introduced in subsection 8.1.1.   cid:7  i = Xui u  1078   29.2 CHOOSING AN APPROPRIATE FORMALISM  generates a new column matrix u cid:7  we can determine the n × n matrix, M X  say, that connects them by  ··· u  cid:7  n T. Having established u and u cid:7    cid:7   cid:7  =  u 1 u 2  u cid:7   = M X u.   29.2   It may seem natural to use the matrix M X  so generated as the representative matrix of the element X; in fact, because we have already chosen the convention whereby Z = XY implies that the eﬀect of applying element Z is the same as that of ﬁrst applying Y and then applying X to the result, one further step has to be taken. So that the representative matrices D X  may follow the same convention, i.e.  and at the same time respect the normal rules of matrix multiplication, it is necessary to take the transpose of M X  as the representative matrix D X . Explicitly,  D Z  = D X D Y  ,  D X  = MT X   u cid:7   = DT X u.   29.3    29.4   and  29.2  becomes  Thus the procedure for determining the matrix D X  that represents the group element X in a representation based on basis vector u is summarised by equations §  29.1 – 29.4 .  This procedure is then repeated for each element X of the group, and the resulting set of n × n matrices D = {D X } is said to be the n-dimensional representation of G having u as its basis. The need to take the transpose of each matrix M X  is not of any fundamental signiﬁcance, since the only thing that really matters is whether the matrices D X  have the appropriate multiplication properties – and, as deﬁned, they do.  In cases in which the basis functions are labels, the actions of the group elements are such as to cause rearrangements of the labels. Correspondingly the matrices D X  contain only ‘1’s and ‘0’s as entries; each row and each column contains a single ‘1’.  §  An alternative procedure in which a row vector is used as the basis vector is possible. Deﬁning equations of the form uTX = uTD X  are used, and no additional transpositions are needed to deﬁne the representative matrices. However, row-matrix equations are cumbersome to write out and in all other parts of this book we have adopted the convention of writing operators  here the group element  to the left of the object on which they operate  here the basis vector .  1079   REPRESENTATION THEORY   cid:1 For the group S3 of permutations on three objects, which has group multiplication ta- ble 28.8 on p. 1055, with  in cycle notation   I =  1  2  3 , A =  1 2 3 , C =  1  2 3 , D =  3  1 2 , E =  2  1 3 ,  B =  1 3 2  use as the components of a basis vector the ordered letter triplets  u1 = {P Q R}, u4 = {P R Q},  u2 = {Q R P}, u5 = {Q P R},  u3 = {R P Q}, u6 = {R Q P}.  Generate a six-dimensional representation D = {D X } of the group and conﬁrm that the  representative matrices multiply according to table 28.8, e.g.  D C D B  = D E .  i = ui for all i. The representative matrix D I  is thus I6, the 6 × 6 unit matrix. It is immediate that the identity permutation I =  1  2  3  leaves all ui unchanged, i.e.  cid:7  u  We next take X as the permutation A =  1 2 3  and, using  29.1 , let it act on each of  the components of the basis vector:  1 = Au1 =  1 2 3 {P Q R} = {Q R P} = u2  cid:7  2 = Au2 =  1 2 3 {Q R P} = {R P Q} = u3  cid:7  ... 6 = Au6 =  1 2 3 {R Q P} = {Q P R} = u5.  cid:7   ...  u  u  u  The matrix M A  has to be such that u cid:7  = M A u  here dots replace zeros to aid readability : 1 · · · · ·  · · · 1 · ·  · · · · 1 ·  · 1 · · · ·  · · 1 · · ·  u cid:7   =   ≡ M A u.   =          · · · · · 1  u1 u2 u3 u4 u5 u6  u2 u3 u1 u6 u4 u5  D A  is then equal to MT A .  The other D X  are determined in a similar way. In general, if then [M X ]ij = 1, leading to [D X ]ji = 1 and [D X ]jk = 0 for k  cid:3 = i. For example,  Xui = uj,  Cu3 =  1  23 {R P Q} = {R Q P} = u6  implies that [D C ]63 = 1 and [D C ]6k = 0 for k = 1, 2, 4, 5, 6. When calculated in full  D C  =  D B  =    · · · 1 · ·  · · · · 1 ·  · · · · · 1  1 · · · · ·   ,  · 1 · · · ·  · · 1 · · ·    · · · · · 1  D E  =   ,  · 1 · · · ·  · · · · 1 ·  · · · · · 1  · · · 1 · ·    1 · · · · ·  · · 1 · · ·   ,  · 1 · · · ·  · · 1 · · ·  1 · · · · ·  · · · 1 · ·  · · · · 1 ·  1080   29.2 CHOOSING AN APPROPRIATE FORMALISM  P  1   a   P  3   b   P  3   c   3  R  2  Q  2  R  1  Q  1  R  2  Q  Figure 29.2 Diagram  a  shows the deﬁnition of the basis vector,  b  shows the eﬀect of applying a clockwise rotation of 2π 3 and  c  shows the eﬀect of applying a reﬂection in the mirror axis through Q.  from which it can be veriﬁed that D C D B  = D E .  cid:2   Whilst a representation obtained in this way necessarily has the same dimension as the order of the group it represents, there are, in general, square matrices of both smaller and larger dimensions that can be used to represent the group, though their existence may be less obvious.  One possibility that arises when the group elements are symmetry opera- tions on an object whose position and orientation can be referred to a space coordinate system is called the natural representation. In it the representative matrices D X  describe, in terms of a ﬁxed coordinate system, what happens to a coordinate system that moves with the object when X is applied. There is usually some redundancy of the coordinates used in this type of represen- tation, since interparticle distances are ﬁxed and fewer than 3N coordinates, where N is the number of identical particles, are needed to specify uniquely the object’s position and orientation. Subsection 29.11.1 gives an example that illustrates both the advantages and disadvantages of the natural representation. We continue here with an example of a natural representation that has no such redundancy.  cid:1 Use the fact that the group considered in the previous worked example is isomorphic to the group of two-dimensional symmetry operations on an equilateral triangle to generate a three-dimensional representation of the group.  Label the triangle’s corners as 1, 2, 3 and three ﬁxed points in space as P, Q, R, so that initially corner 1 lies at point P, 2 lies at point Q, and 3 at point R. We take P, Q, R as the components of the basis vector.  In ﬁgure 29.2,  a  shows the initial conﬁguration and also, formally, the result of applying  the identity I to the triangle; it is therefore described by the basis vector,  P Q R T.  Diagram  b  shows the the eﬀect of a clockwise rotation by 2π 3, corresponding to  element A in the previous example; the new column matrix is  Q R P T.  Diagram  c  shows the eﬀect of a typical mirror reﬂection – the one that leaves the corner at point Q unchanged  element D in table 28.8 and the previous example ; the new column matrix is now  R Q P T.  In similar fashion it can be concluded that the column matrix corresponding to element B, rotation by 4π 3, is  R P Q T, and that the other two reﬂections C and E result in  1081   column matrices  P R Q T and  Q P R T respectively. The forms of the representative matrices Mnat X ,  29.2 , are now determined by equations such as, for element E,  REPRESENTATION THEORY  Dnat E  =   0   =  Q  0  P R  1 0 0  1 0   ,  , Dnat D  =  Dnat A  =  1 0 0  1 0  0 0 1  T  0  0  1 0  0 1  0 0 1  =  0 0 1  0 1 0  Q R     P  .  0  , Dnat B  =  , Dnat E  =  1 0 0  0 0 1  1 0 0  1 0  1 0 0  implying that   1  1  0 0  0 0  0 1 0  0 0 1  0 0 1  0 1 0  Dnat I  =  Dnat C  =  In this way the complete representation is obtained as   0  0  0 1  1 0   ,  .  1 0 0  1 0 0  0 1 0  0 0 1  It should be emphasised that although the group contains six elements this representation is three-dimensional.  cid:2   We will concentrate on matrix representations of ﬁnite groups, particularly rotation and reﬂection groups  the so-called crystal point groups . The general ideas carry over to inﬁnite groups, such as the continuous rotation groups, but in a book such as this, which aims to cover many areas of applicable mathematics, some topics can only be mentioned and not explored. We now give the formal deﬁnition of a representation.  Deﬁnition. A representation D = {D X } of a group G is an assignment of a non- singular square n × n matrix D X  to each element X belonging to G, such that   i  D I  = In, the unit n × n matrix,  ii  D X D Y   = D XY   for any two elements X and Y belonging to G, i.e. the  matrices multiply in the same way as the group elements they represent.  As mentioned previously, a representation by n × n matrices is said to be an n-dimensional representation of G. The dimension n is not to be confused with g, the order of the group, which gives the number of matrices needed in the representation, though they might not all be diﬀerent.  A consequence of the two deﬁning conditions for a representation is that the matrix associated with the inverse of X is the inverse of the matrix associated with X. This follows immediately from setting Y = X  −1 in  ii :  D X D X  −1  = D XX  −1  = D I  = In;  hence  −1  = [D X ]  −1 .  D X  1082   29.2 CHOOSING AN APPROPRIATE FORMALISM  As an example, the four-element Abelian group that consists of the set {1, i,−1,−i}  under ordinary multiplication has a two-dimensional representation based on the column matrix  1 i T:   cid:8    cid:7   cid:7  −1  1 0 0 1  0  0 −1   cid:8   ,  D i  = , D −i  =   cid:7   cid:7    cid:8   cid:8   ,  .  0 −1  1  0  0  1−1 0  D 1  = D −1  =  The reader should check that D i D −i  = D 1 , D i D i  = D −1  etc., i.e. that  the matrices do have exactly the same multiplication properties as the elements of the group. Having done so, the reader may also wonder why anybody would bother with the representative matrices, when the original elements are so much simpler to handle! As we will see later, once some general properties of matrix representations have been established, the analysis of large groups, both Abelian and non-Abelian, can be reduced to routine, almost cookbook, procedures.  An n-dimensional representation of G is a homomorphism of G into the set of invertible n × n matrices  i.e. n × n matrices that have inverses or, equivalently,  have non-zero determinants ; this set is usually known as the general linear group and denoted by GL n . In general the same matrix may represent more than one element of G; if, however, all the matrices representing the elements of G are diﬀerent then the representation is said to be faithful, and the homomorphism becomes an isomorphism onto a subgroup of GL n .  A trivial but important representation is D X  = In for all elements X of G.  Clearly both of the deﬁning relationships are satisﬁed, and there is no restriction on the value of n. However, such a representation is not a faithful one.  ···  T, or of functions,  Ψ1 Ψ2  To sum up, in the context of a rotation–reﬂection group, the transposes of  the set of n × n matrices D X  that make up a representation D may be thought ···  T, the Ψi themselves being functions  of as describing what happens to an n-component basis vector of coordinates,  x y of coordinates, when the group operation X is carried out on each of the coordinates or functions. For example, to return to the symmetry operations on an equilateral triangle, the clockwise rotation by 2π 3, R, carries the three- dimensional basis vector  x y  z T into the column matrix  whilst the two-dimensional basis vector of functions  r2 as neither r nor z is changed by the rotation. The fact that z is unchanged by any of the operations of the group shows that the components x, y, z actually divide  i.e. are ‘reducible’, to anticipate a more formal description  into two sets:  3z2 − r2 T is unaltered,   − 1  √ 3 2 x + 2 y −√ 2 x − 1 3 2 y    z  1083   REPRESENTATION THEORY  one comprises z, which is unchanged by any of the operations, and the other comprises x, y, which change as a pair into linear combinations of themselves. This is an important observation to which we return in section 29.4.  29.3 Equivalent representations  If D is an n-dimensional representation of a group G, and Q is any ﬁxed invert- ible n × n matrix  Q  cid:3 = 0 , then the set of matrices deﬁned by the similarity  transformation  DQ X  = Q−1D X Q   29.5   also forms a representation DQ of G, said to be equivalent to D. We can see from a  comparison with the deﬁnition in section 29.2 that they do form a representation:   i  DQ I  = Q−1D I Q = Q−1InQ = In,  ii  DQ X DQ Y   = Q−1D X QQ−1D Y  Q = Q−1D X D Y  Q  = Q−1D XY  Q = DQ XY  .  Since we can always transform between equivalent representations using a non- singular matrix Q, we will consider such representations to be one and the same. Despite the similarity of words and manipulations to those of subsection 28.7.1, that two representations are equivalent does not constitute an ‘equivalence re- lation’ – for example, the reﬂexive property does not hold for a general ﬁxed matrix Q. However, if Q were not ﬁxed, but simply restricted to belonging to a set of matrices that themselves form a group, then  29.5  would constitute an equivalence relation.  The general invertible matrix Q that appears in the deﬁnition  29.5  of equiv- alent matrices describes changes arising from a change in the coordinate system  i.e. in the set of basis functions . As before, suppose that the eﬀect of an opera- tion X on the basis functions is expressed by the action of M X   which is equal to DT X   on the corresponding basis vector:  u cid:7   = M X u = DT X u. A change of basis would be given by uQ = Qu and u cid:7   Q = Qu cid:7  = QM X u = QDT X Q−1uQ.  u cid:7  Q = Qu cid:7   , and we may write  This is of the same form as  29.6 , i.e.  u cid:7  Q = DT  QT  X uQ,  −1D X QT is related to D X  by a similarity transforma- where DQT  X  =  QT  tion. Thus DQT  X  represents the same linear transformation as D X , but with   29.6    29.7    29.8   1084   29.3 EQUIVALENT REPRESENTATIONS  respect to a new basis vector uQ; this supports our contention that representa- tions connected by similarity transformations should be considered as the same representation.   cid:1 For the four-element Abelian group consisting of the set {1, i,−1,−i} under ordinary multiplication, discussed near the end of section 29.2, change the basis vector from u = 2i − 5 T. Find the real transformation matrix Q. Show that the  1 transformed representative matrix for element i, DQT  i , is given by  i T to uQ =  3 − i  and verify that DT  QT  i uQ = iuQ.  Firstly, we solve the matrix equation  DQT  i  =  17 −29 10 −17   cid:8    cid:7    cid:8  cid:7    cid:8   ,  1 i  a c  b d  3 − i 2i − 5   cid:7    cid:7    cid:8    cid:7    cid:7   =   cid:8   ,  with a, b, c, d real. This gives Q and hence Q−1 as  Q =  3 −1 −5  2  Q−1 =  2 5  1 3   cid:8   .  Following  29.7  we now ﬁnd the transpose of DQT  i  as   cid:7    cid:8  cid:7    cid:8  cid:7    cid:8    cid:7    cid:8   QDT i Q−1 =  3 −1 −5  2  0  −1  1 0  2 5  1 3  =  17  −29 −17  10  and hence DQT  i  is as stated. Finally,  DT  QT  i uQ =   cid:8  cid:7    cid:8    cid:7   3 − i 2i − 5  =  1 + 3i  −2 − 5i   cid:8    cid:7   cid:7   10  17   cid:8  −29 −17 3 − i 2i − 5  = i  = iuQ,  as required.  cid:2   Although we will not prove it, it can be shown that any ﬁnite representation of a ﬁnite group of linear transformations that preserve spatial length  or, in quantum mechanics, preserve the magnitude of a wavefunction  is equivalent to  1085   REPRESENTATION THEORY  a representation in which all the matrices are unitary  see chapter 8  and so from now on we will consider only unitary representations.  29.4 Reducibility of a representation  We have seen already that it is possible to have more than one representation  of any particular group. For example, the group {1, i,−1,−i} under ordinary multiplication has been shown to have a set of 2 × 2 matrices, and a set of four unit n × n matrices In, as two of its possible representations. Consider two or more representations, D 1 , D 2 , . . . , D N , which may be of diﬀerent dimensions, of a group G. Now combine the matrices D 1  X , . . . , D N  X  that correspond to element X of G into a larger block- D 2  X , diagonal matrix:  D X   =  D 2  X     29.9   D 1  X    0  .  .  .  D N  X    0  Then D = {D X } is the matrix representation of the group obtained by combining the basis vectors of D 1 , D 2 , . . . , D N  into one larger basis vector. If, knowingly or unknowingly, we had started with this larger basis vector and found the matrices of the representation D to have the form shown in  29.9 , or to have a form that can be transformed into this by a similarity transformation  29.5   using, of course, the same matrix Q for each of the matrices D X   then we would say that D is reducible and that each matrix D X  can be written as the direct sum of smaller representations:  D X  = D 1  X  ⊕ D 2  X  ⊕ ··· ⊕ D N  X . It may be that some or all of the matrices D 1  X , D 2  X ,  . . . , D N  themselves can be further reduced – i.e. written in block diagonal form. For example, suppose that the representation D 1 , say, has a basis vector  x y z T; then, for the symmetry group of an equilateral triangle, whilst x and y are mixed together for at least one of the operations X, z is never changed. In this case the 3 × 3 representative matrix D 1  X  can itself be written in block diagonal form as a  1086   29.4 REDUCIBILITY OF A REPRESENTATION  2× 2 matrix and a 1× 1 matrix. The direct-sum matrix D X  can now be written  D X   =  D 2  X     29.10   a  c  b  d  1  0  0  .  .  .  D N  X    but the ﬁrst two blocks can be reduced no further.  When all the other representations D 2  X ,  . . . have been similarly treated, what remains is said to be irreducible and has the characteristic of being block diagonal, with blocks that individually cannot be reduced further. The blocks are known as the irreducible representations of G, often abbreviated to the irreps of G, and we denote them by ˆD i  . They form the building blocks of representation theory, and it is their properties that are used to analyse any given physical situation which is invariant under the operations that form the elements of G. Any representation can be written as a linear combination of irreps.  If, however, the initial choice u of basis vector for the representation D is arbitrary, as it is in general, then it is unlikely that the matrices D X  will assume obviously block diagonal forms  it should be noted, though, that since the matrices are square, even a matrix with non-zero entries only in the extreme top right and bottom left positions is technically block diagonal . In general, it will be possible to reduce them to block diagonal matrices with more than one block; this reduction corresponds to a transformation Q to a new basis vector uQ, as described in section 29.3.  In any particular representation D, each constituent irrep ˆD i   may appear any number of times, or not at all, subject to the obvious restriction that the sum of all the irrep dimensions must add up to the dimension of D itself. Let us say that ˆD i   appears mi times. The general expansion of D is then written  D = m1 ˆD 1  ⊕ m2 ˆD 2  ⊕ ··· ⊕ mN  ˆD N   ,   29.11   where if G is ﬁnite so is N.  This is such an important result that we shall now restate the situation in somewhat diﬀerent language. When the set of matrices that forms a representation  1087   REPRESENTATION THEORY  of a particular group of symmetry operations has been brought to irreducible form, the implications are as follows.   i  Those components of the basis vector that correspond to rows in the  representation matrices with a single-entry block, i.e. a 1 × 1 block, are unchanged by the operations of the group. Such a coordinate or function is said to transform according to a one-dimensional irrep of G. In the example given in  29.10 , that the entry on the third row forms a 1 × 1 ···  T, block implies that the third entry in the basis vector  x y namely z, is invariant under the two-dimensional symmetry operations on an equilateral triangle in the xy-plane.  z   ii  If, in any of the g matrices of the representation, the largest-sized block located on the row or column corresponding to a particular coordinate   or function  in the basis vector is n× n, then that coordinate  or function  is mixed by the symmetry operations with n − 1 others and is said to transform according to an n-dimensional irrep of G. Thus in the matrix  29.10 , x is the ﬁrst entry in the complete basis vector; the ﬁrst row of the matrix contains two non-zero entries, as does the ﬁrst column, and so x is part of a two-component basis vector whose components are mixed by the symmetry operations of G. The other component is y.  The result  29.11  may also be formulated in terms of the more abstract notion of vector spaces  chapter 8 . The set of g matrices that forms an n-dimensional representation D of the group G can be thought of as acting on column matrices corresponding to vectors in an n-dimensional vector space V spanned by the basis functions of the representation. If there exists a proper subspace W of V , such that if a vector whose column matrix is w belongs to W then the vector whose column matrix is D X w also belongs to W , for all X belonging to G, then it follows that D is reducible. We say that the subspace W is invariant under the actions of the elements of G. With D unitary, the orthogonal complement W⊥ of W , i.e. the vector space V remaining when the subspace W has been removed, is also invariant, and all the matrices D X  split into two blocks acting separately on W and W⊥. Both W and W⊥ may contain further invariant subspaces, in which case the matrices will be split still further.  As a concrete example of this approach, consider in plane polar coordinates ρ, φ the eﬀect of rotations about the polar axis on the inﬁnite-dimensional vector space V of all functions of φ that satisfy the Dirichlet conditions for expansion as a Fourier series  see section 12.1 . We take as our basis functions the set  {sin mφ, cos mφ} for integer values m = 0, 1, 2, . . . ; this is an inﬁnite-dimensional representation  n = ∞  and, since a rotation about the polar axis can be through any angle α  0 ≤ α < 2π , the group G is a subgroup of the continuous rotation  group and has its order g formally equal to inﬁnity.  1088   29.4 REDUCIBILITY OF A REPRESENTATION  Now, for some k, consider a vector w in the space Wk spanned by {sin kφ, cos kφ},  say w = a sin kφ + b cos kφ. Under a rotation by α about the polar axis, a sin kφ becomes a sin k φ + α , which can be written as a cos kα sin kφ + a sin kα cos kφ, i.e as a linear combination of sin kφ and cos kφ; similarly cos kφ becomes another  cid:7  , linear combination of the same two functions. The newly generated vector w whose column matrix w cid:7  = D α w, therefore belongs to Wk for any α and we can conclude that Wk is an invariant irreducible two-dimensional subspace of V . It follows that D α  is reducible and that, since the result holds for every k, in its reduced form D α  has an inﬁnite series of identical 2× 2 blocks  is given by w cid:7   on its leading diagonal; each block will have the form   cid:7    cid:8   .  cos α − sin α  sin α  cos α  We note that the particular case k = 0 is special, in that then sin kφ = 0 and  cos kφ = 1, for all φ; consequently the ﬁrst 2× 2 block in D α  is reducible further  and becomes two single-entry blocks.  A second illustration of the connection between the behaviour of vector spaces under the actions of the elements of a group and the form of the matrix repre- sentation of the group is provided by the vector space spanned by the spherical harmonics Y cid:2 m θ, φ . This contains subspaces, corresponding to the diﬀerent values of  cid:2 , that are invariant under the actions of the elements of the full three- dimensional rotation group; the corresponding matrices are block-diagonal, and those entries that correspond to the part of the basis containing Y cid:2 m θ, φ  form a  2 cid:2  + 1  ×  2 cid:2  + 1  block. To illustrate further the irreps of a group, we return again to the group G of two-dimensional rotation and reﬂection symmetries of an equilateral triangle, or equivalently the permutation group S3; this may be shown, using the methods of section 29.7 below, to have three irreps. Firstly, we have already seen that the set M of six orthogonal 2 × 2 matrices given in section  28.3 , equation  28.13 , is isomorphic to G. These matrices therefore form not only a representation of G, but a faithful one. It should be noticed that, although G contains six elements, the matrices are only 2 × 2. However, they contain no invariant 1 × 1 sub-block  which for 2 × 2 matrices would require them all to be diagonal  and neither can all the matrices be made block-diagonal by the same similarity transformation; they therefore form a two-dimensional irrep of G. Secondly, as previously noted, every group has one  unfaithful  irrep in which every element is represented by the 1 × 1 matrix I1, or, more simply, 1. Thirdly an  unfaithful  irrep of G is given by assignment of the one-dimensional set of six ‘matrices’ {1, 1, 1,−1,−1,−1} to the symmetry operations {I, R, R L, M} respectively, or to the group elements {I, A, B, C, D, E} respectively; see permutations and −1 to odd permutations, ‘odd’ or ‘even’ referring to the number  section 28.3. In terms of the permutation group S3, 1 corresponds to even  , K,   cid:7   1089   REPRESENTATION THEORY  of simple pair interchanges to which a permutation is equivalent. That these assignments are in accord with the group multiplication table 28.8 should be checked.  Thus the three irreps of the group G  i.e. the group 3m or C3v or S3 , are, using the conventional notation A1, A2, E  see section 29.8 , as follows:  A1 Irrep A2  I 1 1  A 1 1  Element B 1 1  C 1  D 1  E 1  −1 −1 −1 E MI MA MB MC MD ME  cid:30   cid:31   cid:30   cid:31    cid:30   cid:30   − 1 − √  MB =  MA =  3 2  2  2  ,  , MD =  , ME =  √ 3 − 1 2 −√ − 1  2  3 2  1 2  − √  3 2   29.12    cid:31   cid:31   ,  .  3 2  −√ − 1 √ − 1  3 2  2  2  − 1 √ 2 3 2  1 √ 2 3 2   cid:30   cid:31   cid:31   cid:30  −1 0  1 0  0 1  ,  0  1  where  MI =  MC =  29.5 The orthogonality theorem for irreducible representations  We come now to the central theorem of representation theory, a theorem that justiﬁes the relatively routine application of certain procedures to determine the restrictions that are inherent in physical systems that have some degree of rotational or reﬂection symmetry. The development of the theorem is long and quite complex when presented in its entirety, and the reader will have to refer § elsewhere for the proof. The theorem states that, in a certain sense, the irreps of a group G are as orthogonal as possible, as follows. If, for each irrep, the elements in any one position in each of the g matrices are used to make up g-component column matrices then   i  any two such column matrices coming from diﬀerent irreps are orthogonal;  ii  any two such column matrices coming from diﬀerent positions in the  matrices of the same irrep are orthogonal.  This orthogonality is in addition to the irreps’ being in the form of orthogo- nal  unitary  matrices and thus each comprising mutually orthogonal rows and columns.  §  See, e.g., H. F. Jones, Groups, Representations and Physics  Bristol: Institute of Physics, 1998 ; J. F. Cornwell, Group Theory in Physics, vol 2  London: Academic Press, 1984 ; J-P. Serre, Linear Representations of Finite Groups  New York: Springer, 1977 .  1090   29.5 THE ORTHOGONALITY THEOREM FOR IRREDUCIBLE REPRESENTATIONS  matrix D X  by [D X ]ij, and ˆD λ   More mathematically, if we denote the entry in the ith row and jth column of a are two irreps of G having dimensions  cid:23    cid:22  nλ and nµ respectively, then cid:4   and ˆD µ    cid:22    cid:23 ∗  ˆD λ    X   ˆD µ    X   ij  =  kl  g nλ  δikδjlδλµ.  X   29.13   This rather forbidding-looking equation needs some further explanation.  Firstly, the asterisk indicates that the complex conjugate should be taken if necessary, though all our representations so far have involved only real matrix elements. Each Kronecker delta function on the right-hand side has the value 1 if its two subscripts are equal and has the value 0 otherwise. Thus the right-hand side is only non-zero if i = k, j = l and λ = µ, all at the same time.  Secondly, the summation over the group elements X means that g contributions have to be added together, each contribution being a product of entries drawn from the representative matrices in the two irreps ˆD λ  = { ˆD µ    X }. The g contributions arise as X runs over the g elements of G.   X } and ˆD µ   = { ˆD λ   Thus, putting these remarks together, the summation will produce zero if either   i  the matrix elements are not taken from exactly the same position in every matrix, including cases in which it is not possible to do so because the irreps ˆD λ   ii  even if ˆD λ   have diﬀerent dimensions, or do have the same dimensions and the matrix elements are from the same positions in every matrix, they are diﬀerent irreps, i.e.  and ˆD µ  and ˆD µ   λ  cid:3 = µ.  Some numerical illustrations based on the irreps A1, A2 and E of the group 3m  or C3v or S3  will probably provide the clearest explanation  see  29.12  .   a  Take i = j = k = l = 1, with ˆD λ   = A1 and ˆD µ   = A2. Equation  29.13   then reads  1 1  + 1 1  + 1 1  + 1 −1  + 1 −1  + 1 −1  = 0,  as expected, since λ  cid:3 = µ.  cid:9 √  positions within the same irrep ˆD λ  gives   cid:6    cid:6    cid:9  −√  0 1  +  +  3 2   b  Take  i, j  as  1, 2  and  k, l  as  2, 2 , corresponding to diﬀerent matrix = E. Substituting in  29.13   = ˆD µ   2   cid:10  cid:5 − 1  cid:10  cid:9 √   cid:10   + 0 1  +  +  3 2  = 0.   cid:9  −√  3 2   cid:6   2   cid:10  cid:5 − 1  cid:10  cid:9  −√   cid:9  = ˆD µ  −√   cid:9 √  2   cid:10  cid:5 − 1  cid:10  cid:9 √   cid:6    cid:10    cid:10    cid:9 √   c  Take  i, j  as  1, 2 , and  k, l  as  1, 2 , corresponding to the same matrix = E. Substituting in  29.13   positions within the same irrep ˆD λ  gives   cid:10    cid:9 √  0 0  +  3 2  +  3 2  3 2  + 0 0  +  3 2  3 2  +  3 2  3 2  = 6 2 .   cid:9  −√  3 2  3 2  2   cid:10  cid:5 − 1  cid:10  cid:9   −√  1091   REPRESENTATION THEORY  = ˆD µ    d  No explicit calculation is needed to see that if i = j = k = l = 1, with  = A1  or A2 , then each term in the sum is either 12 or  −1 2  ˆD λ  and the total is 6, as predicted by the right-hand side of  29.13  since g = 6 and nλ = 1.  29.6 Characters  The actual matrices of general representations and irreps are cumbersome to work with, and they are not unique since there is always the freedom to change the coordinate system, i.e. the components of the basis vector  see section 29.3 , and hence the entries in the matrices. However, one thing that does not change for a matrix under such an equivalence  similarity  transformation – i.e. under a change of basis – is the trace of the matrix. This was shown in chapter 8, but is repeated here. The trace of a matrix A is the sum of its diagonal ele- ments,  n cid:4   i=1  Tr A =  Aii  or, using the summation convention  section 26.1 , simply Aii. Under a similarity transformation, again using the summation convention,  [DQ X ]ii = [Q−1]ij [D X ]jk[Q]ki = [D X ]jk[Q]ki[Q−1]ij = [D X ]jk[I]kj = [D X ]jj ,  showing that the traces of equivalent matrices are equal.  This fact can be used to greatly simplify work with representations, though with some partial loss of the information content of the full matrices. For example, using trace values alone it is not possible to distinguish between the two groups known as 4mm and ¯42m, or as C4v and D2d respectively, even though the two groups are not isomorphic. To make use of these simpliﬁcations we now deﬁne the characters of a representation.  Deﬁnition. The characters χ D  of a representation D of a group G are deﬁned as the traces of the matrices D X , one for each element X of G.  At this stage there will be g characters, but, as we noted in subsection 28.7.3, elements A, B of G in the same conjugacy class are connected by equations of −1AX. It follows that their matrix representations are connected the form B = X −1 D A D X , and so by the by corresponding equations of the form D B  = D X argument just given their representations will have equal traces and hence equal characters. Thus elements in the same conjugacy class have the same characters,  1092   29.6 CHARACTERS  3m I A1 A2 E  1 1 2  A, B  C, D, E  1 1  −1  1  −1  0  z; z2; x2 + y2  x, y ;  xz, yz ;  Rx, Ry ;  x2 − y2, 2xy  Rz  Table 29.1 The character table for the irreps of group 3m  C3v or S3 . The right-hand column lists some common functions that transform according to the irrep against which each is shown  see text .  though, in general, these will vary from one representation to another. However, it might also happen that two or more conjugacy classes have the same characters in a representation – indeed, in the trivial irrep A1, see  29.12 , every element inevitably has the character 1.  For the irrep A2 of the group 3m, the classes {I}, {A, B} and {C, D, E} have characters 1, 1 and −1, respectively, whilst they have characters 2, −1 and 0  respectively in irrep E.  We are thus able to draw up a character table for the group 3m as shown in table 29.1. This table holds in compact form most of the important infor- mation on the behaviour of functions under the two-dimensional rotational and reﬂection symmetries of an equilateral triangle, i.e. under the elements of group 3m. The entry under I for any irrep gives the dimension of the irrep, since it is equal to the trace of the unit matrix whose dimension is equal to that of the irrep. In other words, for the λth irrep χ λ  I  = nλ, where nλ is its dimen- sion.  In the extreme right-hand column we list some common functions of Cartesian coordinates that transform, under the group 3m, according to the irrep on whose line they are listed. Thus, as we have seen, z, z2, and x2 + y2 are all unchanged by the group operations  though x and y individually are aﬀected  and so are listed against the one-dimensional irrep A1. Each of the pairs  x, y ,  xz, yz , and   x2− y2, 2xy , however, is mixed as a pair by some of the operations, and so these  pairs are listed against the two-dimensional irrep E: each pair forms a basis set for this irrep.  The quantities Rx, Ry and Rz refer to rotations about the indicated axes; they transform in the same way as the corresponding components of angular momentum J, and their behaviour can be established by examining how the  components of J = r × p transform under the operations of the group. To do  this explicitly is beyond the scope of this book. However, it can be noted that Rz, being listed opposite the one-dimensional A2, is unchanged by I and by the rotations A and B but changes sign under the mirror reﬂections C, D, and E, as would be expected.  1093   REPRESENTATION THEORY  29.6.1 Orthogonality property of characters  Some of the most important properties of characters can be deduced from the  orthogonality theorem  29.13 , cid:4    cid:22   ˆD λ    X   ˆD µ    X   =  kl  g nλ  δikδjlδλµ.  If we set j = i and l = k, so that both factors in any particular term in the summation refer to diagonal elements of the representative matrices, and then sum both sides over i and k, we obtain   cid:23    cid:23    cid:23 ∗   cid:22   ij   cid:23 ∗   cid:22   ii  ˆD λ    X   ˆD µ    X   =  kk  g nλ  δikδikδλµ.  k=1  Expressed in term of characters, this reads   cid:4   nλ cid:4   nµ cid:4    cid:22   i=1  k=1  X   cid:19 ∗   cid:4   X   cid:18   X  χ λ  X   χ µ  X  =  g nλ  δ2 iiδλµ =  g nλ  1 × δλµ = gδλµ.   29.14   nλ cid:4   nµ cid:4   i=1  nλ cid:4   i=1  In words, the  g-component  ‘vectors’ formed from the characters of the various irreps of a group are mutually orthogonal, but each one has a squared magnitude  the sum of the squares of its components  equal to the order of the group.  Since, as noted in the previous subsection, group elements in the same class have the same characters,  29.14  can be written as a sum over classes rather than  elements. If ci denotes the number of elements in class Ci and Xi any element of Ci, then  ci  χ λ  Xi   χ µ  Xi  = gδλµ.   29.15   Although we do not prove it here, there also exists a ‘completeness’ relation for characters. It makes a statement about the products of characters for a ﬁxed pair of group elements, X1 and X2, when the products are summed over all possible irreps of the group. This is the converse of the summation process deﬁned by  29.14 . The completeness relation states that  χ λ  X1   χ λ  X2  =  g c1  δC1C2 ,   29.16   where element X1 belongs to conjugacy class C1 and X2 belongs to C2. Thus the sum is zero unless X1 and X2 belong to the same class. For table 29.1 we can verify that these results are valid.   i  For ˆD λ   = ˆD µ   = A1 or A2,  29.15  reads   cid:18    cid:4   i   cid:4    cid:18   λ  nλ cid:4   i=1   cid:19 ∗   cid:19 ∗  1 1  + 2 1  + 3 1  = 6,  1094   29.7 COUNTING IRREPS USING CHARACTERS  whilst for ˆD λ   = ˆD µ   = E, it gives   ii  For ˆD λ   = A2 and ˆD µ   1 22  + 2 1  + 3 0  = 6.  = E, say,  29.15  reads  1 1  2  + 2 1  −1  + 3 −1  0  = 0.   iii  For X1 = A and X2 = D, say,  29.16  reads  1 1  + 1 −1  +  −1  0  = 0,  whilst for X1 = C and X2 = E, both of which belong to class C3 for which c3 = 3,  1 1  +  −1  −1  +  0  0  = 2 =  6 3  .   X  ⊕ m2 ˆD 2  N cid:4   χ X  =  mλχ λ  X .  λ=1  The expression of a general representation D = {D X } in terms of irreps, as  29.7 Counting irreps using characters  given in  29.11 , can be simpliﬁed by going from the full matrix form to that of characters. Thus  D X  = m1 ˆD 1    X  ⊕ ··· ⊕ mN  ˆD N    X   becomes, on taking the trace of both sides,   29.17   Given the characters of the irreps of the group G to which the elements X belong, and the characters of the representation D = {D X }, the g equations  29.17  can be solved as simultaneous equations in the mλ, either by inspection or by and summing over X, making use of  29.14  multiplying both sides by and  29.15 , to obtain  χ µ  X   mµ =  χ µ  X   χ X  =  ci  χ µ  Xi   χ Xi .   29.18    cid:19 ∗  cid:19 ∗   cid:18   cid:18    cid:4   X  1 g   cid:18    cid:4   i  1 g   cid:19 ∗  That an unambiguous formula can be given for each mλ, once the character set  the set of characters of each of the group elements or, equivalently, of each of the conjugacy classes  of D is known, shows that, for any particular group, two representations with the same characters are equivalent. This strongly suggests something that can be shown, namely, the number of irreps = the number of conjugacy classes. The argument is as follows. Equation  29.17  is a set of simultaneous equations for N unknowns, the mλ, some of which may be zero. The value of N is equal to the number of irreps of G. There are g diﬀerent values of X, but the number of diﬀerent equations is only equal to the number of distinct  1095   REPRESENTATION THEORY  conjugacy classes, since any two elements of G in the same class have the same character set and therefore generate the same equation. For a unique solution to simultaneous equations in N unknowns, exactly N independent equations are needed. Thus N is also the number of classes, establishing the stated result.  cid:1 Determine the irreps contained in the representation of the group 3m in the vector space spanned by the functions x2, y2, xy.   29.19   We ﬁrst note that although these functions are not orthogonal they form a basis set for a representation, since they are linearly independent quadratic forms in x and y and any other quadratic form can be written  uniquely  in terms of them. We must establish how they transform under the symmetry operations of group 3m. We need to do so only for a repre- sentative element of each conjugacy class, and naturally we take the simplest in each case.  The ﬁrst class contains only I  as always  and clearly D I  is the 3 × 3 unit matrix. The second class contains the rotations, A and B, and we choose to ﬁnd D A . Since,  under A,  it follows that  2  x → − 1 4 x2 − √  x2 → 1  √ 3 2  x +  y  and  y → −  3  2 xy + 3  4 y2, xy → √  4 x2 +  y2 → 3 2 xy − √  x − 1  √ 3 2 √ 2 xy + 1  2  3  y,  4 y2  and  4 x2 − 1 Hence D A  can be deduced and is given below. easiest to deal with. Under C, x → −x and y → y, causing xy to change sign but leaving x2 and y2 unaltered. The three matrices needed are thus  The third and ﬁnal class contains the reﬂections, C, D and E; of these C is much the   29.20   4 y2.  3  3   1  0 0  0 0  0 1  0 −1   , D A  =   1  4 3 √ 4 3 4   ;  − √ √ − 1  3 2 3 2  2  3 4 1 4  − √  3 4  D I  = I3, D C  =  their traces are respectively 3, 1 and 0.  It should be noticed that much more work has been done here than is necessary, since the traces can be computed immediately from the eﬀects of the symmetry operations on the basis functions. All that is needed is the weight of each basis function in the transformed expression for that function; these are clearly 1, 1, 1 for I, and 1 and  29.20 , and 1, 1, −1 for C, from the observations made just above the displayed 2 for A, from  29.19   4 , − 1 4 , 1  matrices. The traces are then the sums of these weights. The oﬀ-diagonal elements of the matrices need not be found, nor need the matrices be written out.  From  29.17  we now need to ﬁnd a superposition of the characters of the irreps that  gives representation D in the bottom line of table 29.2.  By inspection it is obvious that D = A1 ⊕ E, but we can use  29.18  formally:  mA1 = 1 mA2 = 1 mE = 1  6 [1 1  3  + 2 1  0  + 3 1  1 ] = 1, 6 [1 1  3  + 2 1  0  + 3 −1  1 ] = 0, 6 [1 2  3  + 2 −1  0  + 3 0  1 ] = 1.  Thus A1 and E appear once each in the reduction of D, and A2 not at all. Table 29.1 gives the further information, not needed here, that it is the combination x2 + y2 that transforms as a one-dimensional irrep and the pair  x2 − y2, 2xy  that forms a basis of the two-dimensional irrep, E.  cid:2   1096   29.7 COUNTING IRREPS USING CHARACTERS  Irrep  A1 A2 E  D  I  1 1 2  3  Classes  AB  1 1  −1  0  CDE  1  −1  0  1  Table 29.2 The characters of the irreps of the group 3m and of the represen- tation D, which must be a superposition of some of them.  29.7.1 Summation rules for irreps  The ﬁrst summation rule for irreps is a simple restatement of  29.14 , with µ set  equal to λ; it then reads  cid:4    cid:18    cid:19 ∗  χ λ  X   χ λ  X  = g.  X  In words, the sum of the squares  modulus squared if necessary  of the characters of an irrep taken over all elements of the group adds up to the order of the group. For group 3m  table 29.1 , this takes the following explicit forms:  for A1, for A2, for E,  1 12  + 2 12  + 3 12  = 6; 1 12  + 2 12  + 3 −1 2 = 6; 1 22  + 2 −1 2 + 3 02  = 6.  We next prove a theorem that is concerned not with a summation within an irrep but with a summation over irreps.  Theorem. If nµ is the dimension of the µth irrep of a group G then  where g is the order of the group.  Proof. Deﬁne a representation of the group in the following way. Rearrange the rows of the multiplication table of the group so that whilst the elements in a particular order head the columns, their inverses in the same order head the  rows. In this arrangement of the g × g table, the leading diagonal is entirely  occupied by the identity element. Then, for each element X of the group, take as representative matrix the multiplication-table array obtained by replacing X by 1 and all other element symbols by 0. The matrices Dreg X  so obtained form the regular representation of G; they are each g × g, have a single non-zero entry ‘1’  in each row and column and  as will be veriﬁed by a little experimentation  have   cid:4   µ  n2 µ = g,  1097   REPRESENTATION THEORY   a    b   I  A B  I  I A A B I B  A B I A  B  I  A B  I B  A B I A I B A A B I  Table 29.3  a  The multiplication table of the cyclic group of order 3, and  b  its reordering used to generate the regular representation of the group.  the same multiplication structure as the group G itself, i.e. they form a faithful representation of G.  Although not part of the proof, a simple example may help to make these ideas more transparent. Consider the cyclic group of order 3. Its multiplication table is shown in table 29.3 a   a repeat of table 28.10 a  of the previous chapter , whilst table 29.3 b  shows the same table reordered so that the columns are still labelled in the order I, A, B but the rows are now labelled in the order −1 = I, A −1 = A. The three matrices of the regular representation are I then  −1 = B, B   1 0 0  0 1 0 0 0 1   , Dreg A  =   0 1 0  0 0 1 1 0 0   , Dreg B  =   0 0 1  1 0 0 0 1 0   .  Dreg I  =  An alternative, more mathematical, deﬁnition of the regular representation of a group is   cid:18      cid:19   Dreg Gk   =  ij  1 if GkGj = Gi, 0 otherwise.  We now return to the proof. With the construction given, the regular representa- tion has characters as follows:  χreg I  = g,  χreg X  = 0 if X  cid:3 = I.  We now apply  29.18  to Dreg to obtain for the number mµ of times that the irrep ˆD µ   appears in Dreg  see 29.11     cid:4    cid:18   1 g  X   cid:19 ∗   cid:19 ∗   cid:18   1 g  1 g  mµ =  χ µ  X   χreg X  =  χ µ  I   χreg I  =  nµg = nµ.  Thus an irrep ˆD µ  of dimension nµ appears nµ times in Dreg, and so by counting the total number of basis functions, or by considering χreg I , we can conclude  1098   that  29.7 COUNTING IRREPS USING CHARACTERS   cid:4   µ  n2 µ = g.   29.21   This completes the proof.  As before, our standard demonstration group 3m provides an illustration. In this case we have seen already that there are two one-dimensional irreps and one two-dimensional irrep. This is in accord with  29.21  since  12 + 12 + 22 = 6,  which is the order g of the group.  Another straightforward application of the relation  29.21 , to the group with multiplication table 29.3 a , yields immediate results. Since g = 3, none of its irreps can have dimension 2 or more, as 22 = 4 is too large for  29.21  to be satisﬁed. Thus all irreps must be one-dimensional and there must be three of them  consistent with the fact that each element is in a class of its own, and that  there are therefore three classes . The three irreps are the sets of 1 × 1 matrices   numbers   A1 = {1, 1, 1}  A2 = {1, ω, ω2}  2 = {1, ω2, ω}, ∗ A  where ω = exp 2πi 3 ; since the matrices are 1× 1, the same set of nine numbers  would be, of course, the entries in the character table for the irreps of the group. The fact that the numbers in each irrep are all cube roots of unity is discussed below. As will be noticed, two of these irreps are complex – an unusual occurrence in most applications – and form a complex conjugate pair of one-dimensional irreps. In practice, they function much as a two-dimensional irrep, but this is to be ignored for formal purposes such as theorems.  A further property of characters can be derived from the fact that all elements in a conjugacy class have the same order. Suppose that the element X has order m, i.e. Xm = I. This implies for a representation D of dimension n that  [D X ]m = In.   29.22   Representations equivalent to D are generated as before by using similarity transformations of the form  In particular, if we choose the columns of Q to be the eigenvectors of D X  then, as discussed in chapter 8,  DQ X  = Q−1D X Q.    ···  . . . 0  0 ...  0 λn    λ1  0 ... 0  0  λ2  ···  1099  DQ X  =   where the λi are the eigenvalues of D X . Therefore, from  29.22 , we have that  REPRESENTATION THEORY    λm 1  0 ... 0  0  λm 2  ···  ···  . . . 0  0 ...  0 λm n   =     .  1 0  0 1 ... 0 ···  ···  . . . 0  0 ...  0 1  Hence all the eigenvalues λi are mth roots of unity, and so χ X , the trace of D X , is the sum of n of these. In view of the implications of Lagrange’s theorem  section 28.6 and subsection 28.7.2 , the only values of m allowed are the divisors of the order g of the group.  29.8 Construction of a character table  In order to decompose representations into irreps on a routine basis using characters, it is necessary to have available a character table for the group in question. Such a table gives, for each irrep µ of the group, the character χ µ  X  of the class to which group element X belongs. To construct such a table the following properties of a group, established earlier in this chapter, may be used:   i  the number of classes equals the number of irreps;  ii  the ‘vector’ formed by the characters from a given irrep is orthogonal to  the ‘vector’ formed by the characters from a diﬀerent irrep;   iii   µ n2  µ = g, where nµ is the dimension of the µth irrep and g is the order   iv  the identity irrep  one-dimensional with all characters equal to 1  is present   cid:11   cid:11    cid:20  cid:20   of the group;  for every group;   cid:20  cid:20 2  χ µ  X    v   vi  χ µ  X  is the sum of nµ mth roots of unity, where m is the order of X.  = g.  X   cid:1 Construct the character table for the group 4mm  or C4v  using the properties of classes, irreps and characters so far established.  The group 4mm is the group of two-dimensional symmetries of a square, namely rotations of 0, π 2, π and 3π 2 and reﬂections in the mirror planes parallel to the coordinate axes and along the main diagonals. These are illustrated in ﬁgure 29.3. For this group there are eight elements:    the identity, I;   rotations by π 2 and 3π 2, R and R   a rotation by π, Q ;   four mirror reﬂections mx, my, md and md  ;   cid:7    cid:7  .  Requirements  i  to  iv  at the start of this section put tight constraints on the possible character sets, as the following argument shows.  The group is non-Abelian  clearly Rmx  cid:3 = mxR , and so there are fewer than eight  classes, and hence fewer than eight irreps. But requirement  iii , with g = 8, then implies  1100   29.8 CONSTRUCTION OF A CHARACTER TABLE   cid:7   md  mx  md  my  Figure 29.3 The mirror planes associated with 4mm, the group of two- dimensional symmetries of a square.  that at least one irrep has dimension 2 or greater. However, there can be no irrep with dimension 3 or greater, since 32 > 8, nor can there be more than one two-dimensional irrep, since 22 + 22 = 8 would rule out a contribution to the sum in  iii  of 12 from the identity irrep, and this must be present. Thus the only possibility is one two-dimensional irrep and, to make the sum in  iii  correct, four one-dimensional irreps.  Therefore using  i  we can now deduce that there are ﬁve classes. This same conclusion  certainly much longer than the above. The ﬁve classes are I, Q, {R, R  can be reached by evaluating X of conjugacy classes given in the previous chapter. However, it is tedious to do so and  −1Y X for every pair of elements in G, as in the description  cid:7 }, {mx, my}, {md, md  cid:7 }. at least 2 members, but, as there are three classes to accommodate 8 − 2 = 6 elements,  It is straightforward to show that only I and Q commute with every element of the group, so they are the only elements in classes of their own. Each other class must have  there must be exactly 2 in each class. This does not pair up the remaining 6 elements, but does say that the ﬁve classes have 1, 1, 2, 2, and 2 elements. Of course, if we had started by dividing the group into classes, we would know the number of elements in each class directly.  We cannot entirely ignore the group structure  though it sometimes happens that the results are independent of the group structure – for example, all non-Abelian groups of order 8 have the same character table! ; thus we need to note in the present case that m2 for the same four values of label i. We also recall that for any pair of elements X and Y , D XY   = D X D Y  . We may conclude the following for the one-dimensional irreps.  and, as can be proved directly, Rmi = miR  i = I for i = x, y, d or d   cid:7    cid:7    a  In view of result  vi , χ mi  = D mi  = ±1.  b  Since R4 = I, result  vi  requires that χ R  is one of 1,   cid:7   D R D mi  = D mi D R   , and the D mi  are just numbers, D R  = D R   . Further  i, −1, −i. But, since   cid:7   D R D R  = D R D R    = D RR    = D I  = 1,   cid:7    cid:7   and so D R  = ±1 = D R   cid:7    .   c  D Q  = D RR  = D R D R  = 1.  If we add this to the fact that the characters of the identity irrep A1 are all unity then we can ﬁll in those entries in character table 29.4 shown in bold.  Suppose now that the three missing entries in a one-dimensional irrep are p, q and r,  where each can only be ±1. Then, allowing for the numbers in each class, orthogonality  1101   REPRESENTATION THEORY  4mm I A1 1 A2 1 B1 1 B2 1 E  Q 1 1 1 1  2 −2   cid:7   R, R  1 1  −1 −1  0   cid:7   1  1  mx, my md, md −1 −1 −1 −1  1  1 0  0  Table 29.4 The character table deduced for the group 4mm. For an explana- tion of the entries in bold see the text.  with the characters of A1 requires that  The only possibility is that two of p, q, and r equal −1 and the other equals +1. This  1 1  1  + 1 1  1  + 2 1  p  + 2 1  q  + 2 1  r  = 0.  can be achieved in three diﬀerent ways, corresponding to the need to ﬁnd three further diﬀerent one-dimensional irreps. Thus the ﬁrst four lines of entries in character table 29.4 can be completed. The ﬁnal line can be completed by requiring it to be orthogonal to the other four. Property  v  has not been used here though it could have replaced part of the argument given.  cid:2   29.9 Group nomenclature  The nomenclature of published character tables, as we have said before, is erratic and sometimes unfortunate; for example, often E is used to represent, not only a two-dimensional irrep, but also the identity operation, where we have used I. Thus the symbol E might appear in both the column and row headings of a table, though with quite diﬀerent meanings in the two cases. In this book we use roman capitals to denote irreps.  One-dimensional irreps are regularly denoted by A and B, B being used if a  rotation about the principal axis of 2π n has character −1. Here n is the highest  integer such that a rotation of 2π n is a symmetry operation of the system, and the principal axis is the one about which this occurs. For the group of operations on a square, n = 4, the axis is the perpendicular to the square and the rotation in question is R. The names for the group, 4mm and C4v, derive from the fact that here n is equal to 4. Similarly, for the operations on an equilateral triangle, n = 3 and the group names are 3m and C3v, but because the rotation by 2π 3 has character +1 in all its one-dimensional irreps  see table 29.1 , only A appears in the irrep list.  Two-dimensional irreps are denoted by E, as we have already noted, and three- dimensional irreps by T, although in many cases the symbols are modiﬁed by primes and other alphabetic labels to denote variations in behaviour from one irrep to another in respect of mirror reﬂections and parity inversions. In the study of molecules, alternative names based on molecular angular momentum properties are common. It is beyond the scope of this book to list all these variations, or to  1102   29.10 PRODUCT REPRESENTATIONS  give a large selection of character tables; our aim is to demonstrate and justify the use of those found in the literature speciﬁcally dedicated to crystal physics or molecular chemistry.  Variations in notation are not restricted to the naming of groups and their irreps, but extend to the symbols used to identify a typical element, and hence all members, of a conjugacy class in a group. In physics these are usually of the types nz, ¯nz or mx. The ﬁrst of these denotes a rotation of 2π n about the z-axis, and the second the same thing followed by parity inversion  all vectors r go to −r , whilst the third indicates a mirror reﬂection in a plane, in this case the plane  x = 0.  n , NC x  Typical chemistry symbols for classes are NCn, NC 2  n , NSn, σv, σxy. Here the ﬁrst symbol N, where it appears, shows that there are N elements in the class  a useful feature . The subscript n has the same meaning as in the physics notation, but σ rather than m is used for a mirror reﬂection, subscripts v, d or h or superscripts xy, xz or yz denoting the various orientations of the relevant mirror planes. Symmetries involving parity inversions are denoted by S ; thus Sn is the chemistry analogue of ¯n. None of what is said in this and the previous paragraph should be taken as deﬁnitive, but merely as a warning of common variations in nomenclature and as an initial guide to corresponding entities. Before using any set of group character tables, the reader should ensure that he or she understands the precise notation being employed.  29.10 Product representations  In quantum mechanical investigations we are often faced with the calculation of what are called matrix elements. These normally take the form of integrals over all space of the product of two or more functions whose analytic forms depend on the microscopic properties  usually angular momentum and its components  of the electrons or nuclei involved. For ‘bonding’ calculations involving ‘overlap integrals’ there are usually two functions involved, whilst for transition probabilities a third function, giving the spatial variation of the interaction Hamiltonian, also appears under the integral sign.  If the environment of the microscopic system under investigation has some symmetry properties, then sometimes these can be used to establish, without detailed evaluation, that the multiple integral must have zero value. We now express the essential content of these ideas in group theoretical language.  Suppose we are given an integral of the form   cid:21    cid:21   J =  Ψφ dτ  or  J =  Ψξφ dτ  to be evaluated over all space in a situation in which the physical system is invariant under a particular group G of symmetry operations. For the integral to  1103   REPRESENTATION THEORY  be non-zero the integrand must be invariant under each of these operations. In group theoretical language, the integrand must transform as the identity, the one-  dimensional representation A1 of G; more accurately, some non-vanishing part of  the integrand must do so. An alternative way of saying this is that if under the symmetry operations of G the integrand transforms according to a representation D and D does not contain A1 amongst its irreps then the integral J is necessarily zero. It should be noted that the converse is not true; J may be zero even if A1 is present, since the integral, whilst showing the required invariance, may still have the value zero.  It is evident that we need to establish how to ﬁnd the irreps that go to make up a representation of a double or triple product when we already know the irreps according to which the factors in the product transform. The method is established by the following theorem.  Theorem. For each element of a group the character in a product representation is the product of the corresponding characters in the separate representations.  Proof. Suppose that {ui} and {vj} are two sets of basis functions, that transform under the operations of a group G according to representations D λ  and D µ  respectively. Denote by u and v the corresponding basis vectors and let X be an element of the group. Then the functions generated from ui and vj by the action of X are calculated as follows, using  29.1  and  29.4 :  D λ  X   =  D λ  X   ii ui +  D λ  X   ul,  D µ  X   =  D µ  X   jj vj +  D µ  X   vm.  jm   cid:19   cid:19    cid:22  cid:5   cid:4   cid:22  cid:5   cid:4   l cid:3 =i  m cid:3 =j   cid:23   cid:6 T  cid:23   cid:6 T  il   cid:22  cid:5   cid:22  cid:5    cid:7  i = Xui = u   cid:7  j = Xvj = v  Here [D X ]ij is just a single element of the matrix D X  and [D X ]kk = [DT X ]kk is simply a diagonal element from the matrix – the repeated subscript does not indicate summation. Now, if we take as basis functions for a product represen- tation Dprod X  the products wk = uivj  where the nλnµ various possible pairs of values i, j are labelled by k , we have also that   cid:7  k = Xwk = Xuivj =  Xui  Xvj    w   cid:18    cid:19    cid:18   ii   cid:22  cid:5   This is to be compared with   cid:7  k = Xwk =  w  Dprod X   =  D λ  X   D µ  X   jj uivj + terms not involving the product uivj.  where Dprod X  is the product representation matrix for element X of the group. The comparison shows that   cid:18   =  Dprod X   kk wk +  k  Dprod X   wn,  kn   cid:23    cid:6 T   cid:4    cid:22  cid:5   n cid:3 =k   cid:19    cid:19    cid:19    cid:18   ii  Dprod X   =  D λ  X   D µ  X   kk  jj .  1104   cid:23   cid:6 T u  cid:23   cid:6 T v  i  j   cid:19   cid:23   cid:6 T w  cid:19    cid:18   cid:18    cid:18    cid:18    29.11 PHYSICAL APPLICATIONS OF GROUP THEORY  It follows that  χprod X  =  Dprod X   kk  nλnµ cid:4   cid:18  nλ cid:4  nµ cid:4   nλ cid:4   cid:18   k=1  j=1  i=1   cid:18   =  =   cid:19   jj   cid:18   cid:19  .  ii  D µ  X   nµ cid:4    cid:18   .   cid:19   D λ  X   D λ  X   ii  D µ  X   jj  i=1  j=1   cid:19    cid:19   = χ λ  X  χ µ  X .   29.23   This proves the theorem, and a similar argument leads to the corresponding result for integrands in the form of a product of three or more factors.  An immediate corollary is that an integral whose integrand is the product of two functions transforming according to two diﬀerent irreps is necessarily zero. To see this, we use  29.18  to determine whether irrep A1 appears in the product character set χprod X :   cid:4    cid:18   1 g  X   cid:19 ∗   cid:4   X  1 g   cid:4   X  1 g  mA1 =  χ A1  X   χprod X  =  χprod X  =  χ λ  X χ µ  X .  We have used the fact that χ A1  X  = 1 for all X but now note that, by virtue of  29.14 , the expression on the right of this equation is equal to zero unless λ = µ. Any complications due to non-real characters have been ignored – in practice, they are handled automatically as it is usually Ψ φ, rather than Ψφ, that appears in integrands, though many functions are real in any case, and nearly all characters are.  ∗  Equation  29.23  is a general result for integrands but, speciﬁcally in the context of chemical bonding, it implies that for the possibility of bonding to exist, the two quantum wavefunctions must transform according to the same irrep. This is discussed further in the next section.  29.11 Physical applications of group theory  As we indicated at the start of chapter 28 and discussed in a little more detail at the beginning of the present chapter, some physical systems possess symmetries that allow the results of the present chapter to be used in their analysis. We consider now some of the more common sorts of problem in which these results ﬁnd ready application.  1105   REPRESENTATION THEORY  y  4  2  x  1  3  Figure 29.4 A molecule consisting of four atoms of iodine and one of manganese.  29.11.1 Bonding in molecules  We have just seen that whether chemical bonding can take place in a molecule is strongly dependent upon whether the wavefunctions of the two atoms forming a bond transform according to the same irrep. Thus it is sometimes useful to be able to ﬁnd a wavefunction that does transform according to a particular irrep of a group of transformations. This can be done if the characters of the irrep are known and a sensible starting point can be guessed. We state without proof that  starting from any n-dimensional basis vector Ψ ≡  Ψ1 Ψ2 ··· Ψn T, where {Ψi} is a set of wavefunctions, the new vector Ψ λ  ≡  Ψ λ  n  T generated  ··· Ψ λ   1 Ψ λ   2  by  Ψ λ   i =  ∗  χ λ    X XΨi   29.24    cid:4   X  will transform according to the λth irrep. If the randomly chosen Ψ happens not to contain any component that transforms in the desired way then the Ψ λ  so generated is found to be a zero vector and it is necessary to select a new starting vector. An illustration of the use of this ‘projection operator’ is given in the next example.  cid:1 Consider a molecule made up of four iodine atoms lying at the corners of a square in the xy-plane, with a manganese atom at its centre, as shown in ﬁgure 29.4. Investigate whether the molecular orbital given by the superposition of p-state  angular momentum l = 1  atomic orbitals  Ψ1 = Ψy r − R1  + Ψx r − R2  − Ψy r − R3  − Ψx r − R4   can bond to the d-state atomic orbitals of the manganese atom described by either  i  φ1 =   3z2− r2 f r  or  ii  φ2 =  x2− y2 f r , where f r  is a function of r and so is unchanged by  any of the symmetry operations of the molecule. Such linear combinations of atomic orbitals are known as ring orbitals.  We have eight basis functions, the atomic orbitals Ψx N  and Ψy N , where N = 1, 2, 3, 4 and indicates the position of an iodine atom. Since the wavefunctions are those of p-states they have the forms xf r  or yf r  and lie in the directions of the x- and y-axes shown in the ﬁgure. Since r is not changed by any of the symmetry operations, f r  can be treated as a constant. The symmetry group of the system is 4mm, whose character table is table 29.4.  1106   29.11 PHYSICAL APPLICATIONS OF GROUP THEORY  Case  i . The manganese atomic orbital φ1 =  3z2 − r2 f r , lying at the centre of the  molecule, is not aﬀected by any of the symmetry operations since z and r are unchanged by them. It clearly transforms according to the identity irrep A1. We therefore need to know which combination of the iodine orbitals Ψx N  and Ψy N , if any, also transforms according to A1.  We use the projection operator  29.24 . If we choose Ψx 1  as the arbitrary one- dimensional starting vector, we unfortunately obtain zero  as the reader may wish to verify , but Ψy 1  is found to generate a new non-zero one-dimensional vector transforming according to A1. The results of acting on Ψy 1  with the various symmetry elements X can be written down by inspection  see the discussion in section 29.2 . So, for example, the Ψy 1  orbital centred on iodine atom 1 and aligned along the positive y-axis is changed by the anticlockwise rotation of π 2 produced by R into an orbital centred on atom 4 and aligned along the negative x-axis; thus R actions on Ψy 1  is:  Ψy 1  = −Ψx 4 . The complete set of group   cid:7    cid:7   I, Ψy 1 ;  mx, Ψy 1 ;  Q, −Ψy 3 ; my, −Ψy 3 ;  R, Ψx 2 ;  md, Ψx 2 ;  R   cid:7   , −Ψx 4 ;  cid:7  , −Ψx 4 .  md  Now χ A1  X  = 1 for all X, so  29.24  states that the sum of the above results for XΨy 1 , all with weight 1, gives a vector  here, since the irrep is one-dimensional, just a wave- function  that transforms according to A1 and is therefore capable of forming a chemical bond with the manganese wavefunction φ1. It is  Ψ A1  = 2[Ψy 1  − Ψy 3  + Ψx 2  − Ψx 4 ],  though, of course, the factor 2 is irrelevant. This is precisely the ring orbital Ψ1 given in the problem, but here it is generated rather than guessed beforehand.  Case  ii . The atomic orbital φ2 =  x2 − y2 f r  behaves as follows under the action of  typical conjugacy class members:  I, φ2;  Q, φ2;  R,  y2 − x2 f r  = −φ2;  mx, φ2;  md, −φ2.  From this we see that φ2 transforms as a one-dimensional irrep, but, from table 29.4, that irrep is B1 not A1  the irrep according to which Ψ1 transforms, as already shown . Thus φ2 and Ψ1 cannot form a bond.  cid:2   The original question did not ask for the the ring orbital to which φ2 may bond, but it can be generated easily by using the values of XΨy 1  calculated in case  i  and now weighting them according to the characters of B1:  Ψ B1  = Ψy 1  − Ψy 3  +  −1 Ψx 2  −  −1 Ψx 4   + Ψy 1  − Ψy 3  +  −1 Ψx 2  −  −1 Ψx 4   = 2[Ψy 1  − Ψx 2  − Ψy 3  + Ψx 4 ].  Now we will ﬁnd the other irreps of 4mm present in the space spanned by the basis functions Ψx N  and Ψy N ; at the same time this will illustrate the important point that since we are working with characters we are only interested in the diagonal elements of the representative matrices. This means  section 29.2  that if we work in the natural representation Dnat we need consider only those functions that transform, wholly or partially, into themselves. Since we have no need to write out the matrices explicitly, their size  8× 8  is no drawback. All the irreps spanned by the basis functions Ψx N  and Ψy N  can be determined by considering the actions of the group elements upon them, as follows.  1107   REPRESENTATION THEORY   cid:7    i  Under I all eight basis functions are unchanged, and χ I  = 8.  ii  The rotations R, R  and Q change the value of N in every case and so all diagonal elements of the natural representation are zero and χ R  = χ Q  = 0.   iii  mx takes x into −x and y into y and, for N = 1 and 3, leaves N unchanged,  with the consequences  remember the forms of Ψx N  and Ψy N   that  Ψx 1  → −Ψx 1 , Ψx 3  → −Ψx 3 , Ψy 1  → Ψy 1 , Ψy 3  → Ψy 3 .  Thus χ mx  has four non-zero contributions, −1, −1, 1 and 1, together  with four zero contributions. The total is thus zero.   iv  md and md   cid:7  leave no atom unchanged and so χ md  = 0.  The character set of the natural representation is thus 8, 0, 0, 0, 0, which, either by inspection or by applying formula  29.18 , shows that  Dnat = A1 ⊕ A2 ⊕ B1 ⊕ B2 ⊕ 2E,  i.e. that all possible irreps are present. We have constructed previously the combinations of Ψx N  and Ψy N  that transform according to A1 and B1. The others can be found in the same way.  29.11.2 Matrix elements in quantum mechanics  In section 29.10 we outlined the procedure for determining whether a matrix element that involves the product of three factors as an integrand is necessarily zero. We now illustrate this with a speciﬁc worked example.   cid:1 Determine whether a ‘dipole’ matrix element of the form   cid:21   J =  Ψd1 xΨd2 dτ,  where Ψd1 and Ψd2 are d-state wavefunctions of the forms xyf r  and  x2 − y2 g r  respec-  tively, can be non-zero  i  in a molecule with symmetry C3v  or 3m , such as ammonia, and  ii  in a molecule with symmetry C4v  or 4mm , such as the MnI4 molecule considered in the previous example.  We will need to make reference to the character tables of the two groups. The table for C3v is table 29.1  section 29.6 ; that for C4v is reproduced as table 29.5 from table 29.4 but with the addition of another column showing how some common functions transform.  We make use of  29.23 , extended to the product of three functions. No attention need  be paid to f r  and g r  as they are unaﬀected by the group operations.  Case  i . From the character table 29.1 for C3v, we see that each of xy, x and x2 − y2  forms part of a basis set transforming according to the two-dimensional irrep E. Thus we may ﬁll in the array of characters  using chemical notation for the classes, except that we continue to use I rather than E  as shown in table 29.6. The last line is obtained by  1108   29.11 PHYSICAL APPLICATIONS OF GROUP THEORY  4mm I A1 A2 B1 B2 E  1 1 1 1  Q  1 1 1 1  2 −2   cid:7   R, R  1 1  −1 −1  0   cid:7   1  1  mx, my md, md −1 −1 −1 −1  1  0  z; z2; x2 + y2 x2 − y2 Rz  1 0  xy  x, y ;  xz, yz ;  Rx, Ry   Table 29.5 The character table for the irreps of group 4mm  or C4v . The right-hand column lists some common functions, or, for the two-dimensional irrep E, pairs of functions, that transform according to the irrep against which they are shown.  Table 29.6 The character sets, for the group C3v  or 3mm , of three functions  and of their product x2y x2 − y2 .  Function  Irrep  E E E  xy x  x2 − y2  product  Classes 2C3  I  2 −1 2 −1 2 −1 8 −1  3σv 0 0 0  0  Function  Irrep  B2 E B1  xy x  x2 − y2  product  Classes 2C6  2σv −1 −1 −1  0 1  0  2σd 1 0  −1  0  0  0  1  2 −2  C2 1  1  I  1  2 −2  Table 29.7 The character sets, for the group C4v  or 4mm , of three functions,  and of their product x2y x2 − y2 .  multiplying together the corresponding characters for each of the three elements. Now, by inspection, or by applying  29.18 , i.e.  mA1 = 1  6 [1 1  8  + 2 1  −1  + 3 1  0 ] = 1,  we see that irrep A1 does appear in the reduced representation of the product, and so J is not necessarily zero.  Case  ii . From table 29.5 we ﬁnd that, under the group C4v, xy and x2 − y2 transform  as irreps B2 and B1 respectively and that x is part of a basis set transforming as E. Thus the calculation table takes the form of table 29.7  again, chemical notation for the classes has been used . Here inspection is suﬃcient, as the product is exactly that of irrep E and irrep A1 is certainly not present. Thus J is necessarily zero and the dipole matrix element vanishes.  cid:2   1109   REPRESENTATION THEORY  y3  x3  y1  x1  y2  x2  Figure 29.5 An equilateral array of masses and springs.  29.11.3 Degeneracy of normal modes  As our ﬁnal area for illustrating the usefulness of group theoretical results we consider the normal modes of a vibrating system  see chapter 9 . This analysis has far-reaching applications in physics, chemistry and engineering. For a given system, normal modes that are related by some symmetry operation have the same frequency of vibration; the modes are said to be degenerate. It can be shown that such modes span a vector space that transforms according to some irrep of the group G of symmetry operations of the system. Moreover, the degeneracy of the modes equals the dimension of the irrep. As an illustration, we consider the following example.   cid:1 Investigate the possible vibrational modes of the equilateral triangular arrangement of equal masses and springs shown in ﬁgure 29.5. Demonstrate that two are degenerate.  Clearly the symmetry group is that of the symmetry operations on an equilateral triangle, namely 3m  or C3v , whose character table is table 29.1. As on a previous occasion, it is most convenient to use the natural representation Dnat of this group  it almost always saves having to write out matrices explicitly  acting on the six-dimensional vector space  x1, y1, x2, y2, x3, y3 . In this example the natural and regular representations coincide, but this is not usually the case.  We note that in table 29.1 the second class contains the rotations A  by π 3  and B  by . This class is known as 3z in crystallographic notation, or 2π 3 , also known as R and R C3 in chemical notation, as explained in section 29.9. The third class contains C, D, E, the three mirror reﬂections.   cid:7   Clearly χ I  = 6. Since all position labels are changed by a rotation, χ 3z  = 0. For the mirror reﬂections the simplest representative class member to choose is the reﬂection my in  the plane containing the y3-axis, since then only label 3 is unchanged; under my, x3 → −x3 and y3 → y3, leading to the conclusion that χ my  = 0. Thus the character set is 6, 0, 0.  Using  29.18  and the character table 29.1 shows that  Dnat = A1 ⊕ A2 ⊕ 2E.  1110   29.11 PHYSICAL APPLICATIONS OF GROUP THEORY  However, we have so far allowed xi, yi to be completely general, and we must now identify and remove those irreps that do not correspond to vibrations. These will be the irreps corresponding to bodily translations of the triangle and to its rotation without relative motion of the three masses.  Bodily translations are linear motions of the centre of mass, which has coordinates  x =  x1 + x2 + x3  3 and y =  y1 + y2 + y3  3 .  Table 29.1 shows that such a coordinate pair  x, y  transforms according to the two- dimensional irrep E; this accounts for one of the two such irreps found in the natural representation.  It can be shown that, as stated in table 29.1, planar bodily rotations of the triangle – rotations about the z-axis, denoted by Rz – transform as irrep A2. Thus, when the linear motions of the centre of mass, and pure rotation about it, are removed from our reduced  representation, we are left with E⊕A1. So, E and A1 must be the irreps corresponding to the  internal vibrations of the triangle – one doubly degenerate mode and one non-degenerate mode.  The physical interpretation of this is that two of the normal modes of the system have the same frequency and one normal mode has a diﬀerent frequency  barring accidental coincidences for other reasons . It may be noted that in quantum mechanics the energy quantum of a normal mode is proportional to its frequency.  cid:2   In general, group theory does not tell us what the frequencies are, since it is entirely concerned with the symmetry of the system and not with the values of masses and spring constants. However, using this type of reasoning, the results from representation theory can be used to predict the degeneracies of atomic energy levels and, given a perturbation whose Hamiltonian  energy operator  has some degree of symmetry, the extent to which the perturbation will resolve the degeneracy. Some of these ideas are explored a little further in the next section and in the exercises.  29.11.4 Breaking of degeneracies  If a physical system has a high degree of symmetry, invariant under a group G of reﬂections and rotations, say, then, as implied above, it will normally be the case that some of its eigenvalues  of energy, frequency, angular momentum etc.  are degenerate. However, if a perturbation that is invariant only under the operations of the elements of a smaller symmetry group  a subgroup of G  is added, some of the original degeneracies may be broken. The results derived from representation theory can be used to decide the extent of the degeneracy-breaking.  The normal procedure is to use an N-dimensional basis vector, consisting of the N degenerate eigenfunctions, to generate an N-dimensional representation of the symmetry group of the perturbation. This representation is then decomposed into irreps. In general, eigenfunctions that transform according to diﬀerent irreps no longer share the same frequency of vibration. We illustrate this with the following example.  1111   REPRESENTATION THEORY  M  M  M  Figure 29.6 A circular drumskin loaded with three symmetrically placed masses.   cid:1 A circular drumskin has three equal masses placed on it at the vertices of an equilateral triangle, as shown in ﬁgure 29.6. Determine which degenerate normal modes of the drumskin can be split in frequency by this perturbation.  When no masses are present the normal modes of the drum-skin are either non-degenerate or two-fold degenerate  see chapter 21 . The degenerate eigenfunctions Ψ of the nth normal mode have the forms  Jn kr  cos nθ e  or  Jn kr  sin nθ e  ±iωt  ±iωt.  Therefore, as explained above, we need to consider the two-dimensional vector space spanned by Ψ1 = sin nθ and Ψ2 = cos nθ. This will generate a two-dimensional representa- tion of the group 3m  or C3v , the symmetry group of the perturbation. Taking the easiest element from each of the three classes  identity, rotations, and reﬂections  of group 3m, we have  n   cid:18   cid:18    cid:5   cid:5    cid:6  cid:19   cid:6  cid:19   IΨ2 = Ψ2, =  IΨ1 = Ψ1, AΨ1 = sin   cid:5   cid:5  θ − 2 3 π θ − 2 3 π   cid:6   cid:6  cos 2 3 nπ cos 2 3 nπ CΨ1 = sin[n π − θ ] = − cos nπ Ψ1, CΨ2 = cos[n π − θ ] =  cos nπ Ψ2.  cid:31   AΨ2 = cos   cid:30   =  n  3 nπ − sin 2  cos 2  sin 2  3 nπ  3 nπ 3 nπ  cos 2  Ψ1 − cid:5   cid:5   Ψ2 +   cid:6   cid:6   sin 2 sin 2  3 nπ 3 nπ  Ψ2,  Ψ1,   cid:30  − cos nπ   cid:31   .  0  0  cos nπ  The three representative matrices are therefore  D I  = I2, D A  =  , D C  =  The characters of this representation are χ I  = 2, χ A  = 2 cos 2nπ 3  and χ C  = 0. Using  29.18  and table 29.1, we ﬁnd that  Thus   cid:5   cid:5    cid:6   cid:6     mA1 = 1 6 mE = 1 6  2 + 4 cos 2 4 − 4 cos 2  3 nπ 3 nπ  = mA2 .  D =  A1 ⊕ A2  E  if n = 3, 6, 9, otherwise.  . . . ,  1112  Hence the normal modes n = 3, 6, 9,  . . . each transform under the operations of 3m   29.12 EXERCISES  as the sum of two one-dimensional irreps and, using the reasoning given in the previous example, are therefore split in frequency by the perturbation. For other values of n the representation is irreducible and so the degeneracy cannot be split.  cid:2   29.1  A group G has four elements I, X, Y and Z , which satisfy X 2 = Y 2 = Z 2 = XY Z = I. Show that G is Abelian and hence deduce the form of its character  29.12 Exercises  table.  Show that the matrices   cid:8   cid:7   cid:7  −1 −p  0 1  1 0  ,  0  1   cid:8   ,  D I  =  D Y   =   cid:8   ,   cid:7  −1  cid:7   0   cid:8  0 −1 0 −1  1  p  ,  D X  =  D Z   =  decompose it into irreps.  where p is a real number, form a representation D of G. Find its characters and Using a square whose corners lie at coordinates  ±1,±1 , form a natural rep- resentation of the dihedral group D4. Find the characters of the representation,  and, using the information  and class order  in table 29.4  p. 1102 , express the representation in terms of irreps.  Now form a representation in terms of eight 2 × 2 orthogonal matrices, by considering the eﬀect of each of the elements of D4 on a general vector  x, y .  Conﬁrm that this representation is one of the irreps found using the natural representation.  The quaternion group Q  see exercise 28.20  has eight elements {±1,±i,±j,±k}  obeying the relations  i2 = j2 = k2 = −1,  ij = k = −ji.  Determine the conjugacy classes of Q and deduce the dimensions of its irreps. Show that Q is homomorphic to the four-element group V, which is generated by two distinct elements a and b with a2 = b2 =  ab 2 = I. Find the one-dimensional irreps of V and use these to help determine the full character table for Q. Construct the character table for the irreps of the permutation group S4 as follows.   a  By considering the possible forms of its cycle notation, determine the number of elements in each conjugacy class of the permutation group S4, and show that S4 has ﬁve irreps. Give the logical reasoning that shows they must consist of two three-dimensional, one two-dimensional, and two one-dimensional irreps.   b  By considering the odd and even permutations in the group S4, establish the  characters for one of the one-dimensional irreps.   c  Form a natural matrix representation of 4 × 4 matrices based on a set of objects {a, b, c, d}, which may or may not be equal to each other, and, by  selecting one example from each conjugacy class, show that this natural representation has characters 4, 2, 1, 0, 0. In the four-dimensional vector space in which each of the four coordinates takes on one of the four values a, b, c or d, the one-dimensional subspace consisting of the four points with  coordinates of the form {a, a, a, a} is invariant under the permutation group  and hence transforms according to the invariant irrep A1. The remaining three-dimensional subspace is irreducible; use this and the characters deduced above to establish the characters for one of the three-dimensional irreps, T1.  1113  29.2  29.3  29.4   REPRESENTATION THEORY   d  Complete the character table using orthogonality properties, and check the  summation rule for each irrep. You should obtain table 29.8.  Typical element and class size  Irrep   12    123    1234    12  34    1  1 1 1 2 3 3  6 1  −1  0 1  −1  −1  8 1 1  0 0  6 1  −1 −1  0  1  3 1 1 2  −1 −1  A1 A2 E T1 T2  Table 29.8 The character table for the permutation group S4.  29.5  In exercise 28.10, the group of pure rotations taking a cube into itself was found to have 24 elements. The group is isomorphic to the permutation group S4, considered in the previous question, and hence has the same character table, once corresponding classes have been established. By counting the number of elements in each class, make the correspondences below  the ﬁnal two cannot be decided purely by counting, and should be taken as given .  29.6  29.7  Action  Permutation  class type   1   123   12  34   1234   12   Symbol  physics  I 3 2z 4z 2d  none rotations about a body diagonal rotation of π about the normal to a face  rotations of ±π 2 about the normal to a face  rotation of π about an axis through the  centres of opposite edges  Reformulate the character table 29.8 in terms of the elements of the rotation symmetry group  432 or O  of a cube and use it when answering exercises 29.7 and 29.8. Consider a regular hexagon orientated so that two of its vertices lie on the x-axis. Find matrix representations of a rotation R through 2π 6 and a reﬂection my in the y-axis by determining their eﬀects on vectors lying in the xy-plane . Show that a reﬂection mx in the x-axis can be written as mx = myR3, and that the 12 elements of the symmetry group of the hexagon are given by Rn or Rnmy.  Using the representations of R and my as generators, ﬁnd a two-dimensional representation of the symmetry group, C6, of the regular hexagon. Is it a faithful representation? In a certain crystalline compound, a thorium atom lies at the centre of a regular  octahedron of six sulphur atoms at positions  ±a, 0, 0 ,  0,±a, 0 ,  0, 0,±a . These  can be considered as being positioned at the centres of the faces of a cube of side 2a. The sulphur atoms produce at the site of the thorium atom an electric ﬁeld that has the same symmetry group as a cube  432 or O .  The ﬁve degenerate d-electron orbitals of the thorium atom can be expressed,  relative to any arbitrary polar axis, as   3 cos2 θ − 1 f r ,  ±iφ sin θ cos θf r ,  e  ±2iφ sin2 θf r .  e  A rotation about that polar axis by an angle φ   cid:7   eﬀectively changes φ to φ − φ   cid:7   .  1114   29.12 EXERCISES  29.8  29.9  Use this to show that the character of the rotation in a representation based on the orbital wavefunctions is given by   cid:7    cid:7   1 + 2 cos φ  + 2 cos 2φ  and hence that the characters of the representation, in the order of the symbols  given in exercise 29.5, is 5, −1, 1, −1, 1. Deduce that the ﬁve-fold degenerate  level is split into two levels, a doublet and a triplet. Sulphur hexaﬂuoride is a molecule with the same structure as the crystalline compound in exercise 29.7, except that a sulphur atom is now the central atom. The following are the forms of some of the electronic orbitals of the sulphur atom, together with the irreps according to which they transform under the symmetry group 432  or O .  A1 T1  Ψs = f r  Ψd1 =  3z2 − r2 f r  E Ψp1 = zf r  Ψd2 =  x2 − y2 f r  Ψd3 = xyf r   E T2   cid:1   The function x transforms according to the irrep T1. Use the above data to determine whether dipole matrix elements of the form J = φ1xφ2 dτ can be non-zero for the following pairs of orbitals φ1, φ2 in a sulphur hexaﬂuoride molecule:  a  Ψd1, Ψs;  b  Ψd1, Ψp1;  c  Ψd2, Ψd1;  d  Ψs, Ψd3;  e  Ψp1, Ψs. The hydrogen atoms in a methane molecule CH4 form a perfect tetrahedron with the carbon atom at its centre. The molecule is most conveniently described  mathematically by placing the hydrogen atoms at the points  1, 1, 1 ,  1,−1,−1 ,  −1, 1,−1  and  −1,−1, 1 . The symmetry group to which it belongs, the tetrahe- dral group  ¯43m or Td , has classes typiﬁed by I, 3, 2z, md and ¯4z, where the ﬁrst three are as in exercise 29.5, md is a reﬂection in the mirror plane x − y = 0 and ¯4z is a rotation of π 2 about the z-axis followed by an inversion in the origin. A reﬂection in a mirror plane can be considered as a rotation of π about an axis perpendicular to the plane, followed by an inversion in the origin.  The character table for the group ¯43m is very similar to that for the group  432, and has the form shown in table 29.9.  Irreps  Typical element and class size md I 1 6  ¯4z 6  2z 3  3 8  Functions transforming according to irrep  A1 A2 E T1 T2  1 1 2 3 3  1 1  −1  0 0  1 1 2  −1 −1  1  −1  0 1  −1  1  −1 −1  0  1  x2 + y2 + z2  x2 − y2, 3z2 − r2    Rx, Ry, Rz   x, y, z ;  xy, yz, zx   Table 29.9 The character table for group ¯43m.  By following the steps given below, determine how many diﬀerent internal vibra- tion frequencies the CH4 molecule has.   a  Consider a representation based on the twelve coordinates xi, yi, zi for i = 1, 2, 3, 4. For those hydrogen atoms that transform into themselves, a rotation through an angle θ about an axis parallel to one of the coordinate axes gives rise in the natural representation to the diagonal elements 1 for  1115   REPRESENTATION THEORY  the corresponding coordinate and 2 cos θ for the two orthogonal coordinates. If the rotation is followed by an inversion then these entries are multiplied  by −1. Atoms not transforming into themselves give a zero diagonal contri-  bution. Show that the characters of the natural representation are 12, 0, 0, 0, 2 and hence that its expression in terms of irreps is  A1 ⊕ E ⊕ T1 ⊕ 2T2.   b  The irreps of the bodily translational and rotational motions are included in this expression and need to be identiﬁed and removed. Show that when this is done it can be concluded that there are three diﬀerent internal vibration frequencies in the CH4 molecule. State their degeneracies and check that they are consistent with the expected number of normal coordinates needed to describe the internal motions of the molecule.  29.10  Investigate the properties of an alternating group and construct its character table as follows.   a  The set of even permutations of four objects  a proper subgroup of S4  is known as the alternating group A4. List its twelve members using cycle notation.   b  Assume that all permutations with the same cycle structure belong to the same conjugacy class. Show that this leads to a contradiction, and hence demonstrates that, even if two permutations have the same cycle structure, they do not necessarily belong to the same class.   c  By evaluating the products  p1 =  123  4     12  34     132  4  and p2 =  132  4     12  34     123  4   deduce that the three elements of A4 with structure of the form  12  34  belong to the same class.   d  By evaluating products of the form  1α  βγ    123  4    1α  βγ , where α, β, γ  are various combinations of 2, 3, 4, show that the class to which  123  4  belongs contains at least four members. Show the same for  124  3 .   e  By combining results  b ,  c  and  d  deduce that A4 has exactly four classes,  and determine the dimensions of its irreps.   f  Using the orthogonality properties of characters and noting that elements of  the form  124  3  have order 3, ﬁnd the character table for A4.  29.11  29.12  Use the results of exercise 28.23 to ﬁnd the character table for the dihedral group  D5, the symmetry group of a regular pentagon.  Demonstrate that equation  29.24  does, indeed, generate a set of vectors trans- forming according to an irrep λ, by sketching and superposing drawings of an equilateral triangle of springs and masses, based on that shown in ﬁgure 29.5.  C   a   A  ◦  30  B  A  B  ◦  30  A  C  B   c   Figure 29.7 The three normal vibration modes of the equilateral array. Mode  a  is known as the ‘breathing mode’. Modes  b  and  c  transform according to irrep E and have equal vibrational frequencies.  C   b   1116   29.13 HINTS AND ANSWERS   a  Make an initial sketch showing an arbitrary small mass displacement from, say, vertex C. Draw the results of operating on this initial sketch with each of the symmetry elements of the group 3m  C3v .   b  Superimpose the results, weighting them according to the characters of irrep A1  table 29.1 in section 29.6  and verify that the resultant is a symmetrical arrangement in which all three masses move symmetrically towards  or away from  the centroid of the triangle. The mode is illustrated in ﬁgure 29.7 a .   c  Start again, this time considering a displacement δ of C parallel to the x-axis. Form a similar superposition of sketches weighted according to the characters of irrep E  note that the reﬂections are not needed . The resultant contains some bodily displacement of the triangle, since this also transforms according to E. Show that the displacement of the centre of mass is ¯x = δ, ¯y = 0. Subtract this out, and verify that the remainder is of the form shown in ﬁgure 29.7 c .   d  Using an initial displacement parallel to the y-axis, and an analogous proce- dure, generate the remaining normal mode, degenerate with that in  c  and shown in ﬁgure 29.7 b .  Further investigation of the crystalline compound considered in exercise 29.7 shows that the octahedron is not quite perfect but is elongated along the  1, 1, 1   direction with the sulphur atoms at positions ± a+δ, δ, δ , ± δ, a+δ, δ , ± δ, δ, a+ δ , where δ  cid:16  a. This structure is invariant under the  crystallographic  symmetry group 32 with three two-fold axes along directions typiﬁed by  1,−1, 0 . The  latter axes, which are perpendicular to the  1, 1, 1  direction, are axes of two- fold symmetry for the perfect octahedron. The group 32 is really the three- dimensional version of the group 3m and has the same character table as table 29.1  section 29.6 . Use this to show that, when the distortion of the octahedron is included, the doublet found in exercise 29.7 is unsplit but the triplet breaks up into a singlet and a doublet.  29.13 Hints and answers  , 1, 1,−1, 1,−1;  , 1, 1, 1,−1,−1; ˆD 3   , 1, 1, 1, 1, 1; ˆD 2  , 1, 1,−1,−1, 1; ˆD 5   There are four classes and hence four one-dimensional irreps, which must have  irreps and one two-dimensional irrep. Show that ab = ba. The homomorphism  entries as follows: 1, 1, 1, 1; 1, 1, −1, −1; 1, −1, 1, −1; 1, −1, −1, 1. The characters of D are 2, −2, 0, 0 and so the irreps present are the last two of these. There are ﬁve classes {1},{−1},{±i},{±j},{±k}; there are four one-dimensional is ±1 → I, ±i → a, ±j → b, ±k → ab. V is Abelian and hence has four one-dimensional irreps. In the class order given above, the characters for Q are as follows: ˆD 1  ˆD 4  Note that the fourth and ﬁfth classes each have 6 members. The ﬁve basis functions of the representation are multiplied by 1, e  cid:7  −2iφ e rotations of 0, 2π 3, π, π 2, π; Drep = E + T2.  b  The bodily translation has irrep T2 and the rotation has irrep T1. The irreps of the internal vibrations are A1, E, T2, with respective degeneracies 1, 2, 3, making six internal coordinates  12 in total, minus three translational, minus three rotational . There are four classes and hence four irreps, which can only be the identity irrep, one other one-dimensional irrep, and two two-dimensional irreps. In the   cid:7  , e+iφ , as a result of the rotation. The character is the sum of these for  class order {I}, {R, R4}, {R2, R3}, {mi} the second one-dimensional irrep must  , 2,−2, 0, 0, 0.   cid:7  , e+2iφ   cid:7  −iφ  1117  29.13  29.1  29.3  29.5 29.7  29.9  29.11   REPRESENTATION THEORY  5  2, 0 and 2,  −1−√   because of orthogonality  have characters 1, 1, 1, −1. The summation rules and orthogonality require the other two character sets to be 2,  −1 +  −1−√ that, e.g.,  −1 + The doublet irrep E  characters 2, −1, 0  appears in both 432 and 32 and so  characters 2, −1, 0  and singlet A1  characters 1, 1, 1 .  √ √ 5  2, 5  2, 0. Note that R has order 5 and  is unsplit. The triplet T1  characters 3, 0, 1  splits under 32 into doublet E  5  2 = exp 2πi 5  + exp 8πi 5 .  5  2,  −1 +  √  29.13  1118   30  Probability  All scientists will know the importance of experiment and observation and, equally, be aware that the results of some experiments depend to a degree on chance. For example, in an experiment to measure the heights of a random sample of people, we would not be in the least surprised if all the heights were found to be diﬀerent; but, if the experiment were repeated often enough, we would expect to ﬁnd some sort of regularity in the results. Statistics, which is the subject of the next chapter, is concerned with the analysis of real experimental data of this sort. First, however, we discuss probability. To a pure mathematician, probability is an entirely theoretical subject based on axioms. Although this axiomatic approach is important, and we discuss it brieﬂy, an approach to probability more in keeping with its eventual applications in statistics is adopted here.  We ﬁrst discuss the terminology required, with particular reference to the convenient graphical representation of experimental results as Venn diagrams. The concepts of random variables and distributions of random variables are then introduced. It is here that the connection with statistics is made; we assert that the results of many experiments are random variables and that those results have some sort of regularity, which is represented by a distribution. Precise deﬁnitions of a random variable and a distribution are then given, as are the deﬁning equations for some important distributions. We also derive some useful quantities associated with these distributions.  30.1 Venn diagrams  We call a single performance of an experiment a trial and each possible result an outcome. The sample space S of the experiment is then the set of all possible outcomes of an individual trial. For example, if we throw a six-sided die then there are six possible outcomes that together form the sample space of the experiment. At this stage we are not concerned with how likely a particular outcome might  1119   PROBABILITY  A  i  iii  ii  B  iv  S  Figure 30.1 A Venn diagram.  be  we will return to the probability of an outcome in due course  but rather will concentrate on the classiﬁcation of possible outcomes. It is clear that some sample spaces are ﬁnite  e.g. the outcomes of throwing a die  whilst others are inﬁnite  e.g. the outcomes of measuring people’s heights . Most often, one is not interested in individual outcomes but in whether an outcome belongs to a given subset A  say  of the sample space S ; these subsets are called events. For example, we might be interested in whether a person is taller or shorter than 180 cm, in which case we divide the sample space into just two events: namely, that the outcome  height measured  is  i  greater than 180 cm or  ii  less than 180 cm.  A common graphical representation of the outcomes of an experiment is the Venn diagram. A Venn diagram usually consists of a rectangle, the interior of which represents the sample space, together with one or more closed curves inside it. The interior of each closed curve then represents an event. Figure 30.1 shows a typical Venn diagram representing a sample space S and two events A and B. Every possible outcome is assigned to an appropriate region; in this example there are four regions to consider  marked i to iv in ﬁgure 30.1 :   i  outcomes that belong to event A but not to event B;  ii  outcomes that belong to event B but not to event A;  iii  outcomes that belong to both event A and event B;  iv  outcomes that belong to neither event A nor event B.   cid:1 A six-sided die is thrown. Let event A be ‘the number obtained is divisible by 2’ and event B be ‘the number obtained is divisible by 3’. Draw a Venn diagram to represent these events.  It is clear that the outcomes 2, 4, 6 belong to event A and that the outcomes 3, 6 belong to event B. Of these, 6 belongs to both A and B. The remaining outcomes, 1, 5, belong to neither A nor B. The appropriate Venn diagram is shown in ﬁgure 30.2.  cid:2   In the above example, one outcome, 6, is divisible by both 2 and 3 and so belongs to both A and B. This outcome is placed in region iii of ﬁgure 30.1, which  is called the intersection of A and B and is denoted by A ∩ B  see ﬁgure 30.3 a  .  If no events lie in the region of intersection then A and B are said to be mutually exclusive or disjoint. In this case, often the Venn diagram is drawn so that the closed curves representing the events A and B do not overlap, so as to make  1120   30.1 VENN DIAGRAMS  A  4  2  B  6  3  1  5  S   a    c   Figure 30.2 The Venn diagram for the outcomes of the die-throwing trials described in the worked example.  A  B  A  S  S  A  ¯A  A  S  S   b    d   Figure 30.3 Venn diagrams: the shaded regions show  a  A ∩ B, the inter- section of two events A and B,  b  A ∪ B, the union of events A and B,  c  the complement ¯A of an event A,  d  A − B, those outcomes in A that do not  belong to B.  B  B  graphically explicit the fact that A and B are disjoint. It is not necessary, however, to draw the diagram in this way, since we may simply assign zero outcomes to the shaded region in ﬁgure 30.3 a . An event that contains no outcomes is called  the empty event and denoted by ∅. The event comprising all the elements that by A ∪ B  see ﬁgure 30.3 b  . In the previous example, A ∪ B = {2, 3, 4, 6}.  belong to either A or B, or to both, is called the union of A and B and is denoted  It is sometimes convenient to talk about those outcomes that do not belong to a particular event. The set of outcomes that do not belong to A is called the complement of A and is denoted by ¯A  see ﬁgure 30.3 c  ; this can also be written as ¯A = S − A. It is clear that A ∪ ¯A = S and A ∩ ¯A = ∅. The above notation can be extended in an obvious way, so that A− B denotes A − B can also be written as A ∩ ¯B. Finally, when all the outcomes in event B  the outcomes in A that do not belong to B. It is clear from ﬁgure 30.3 d  that   say  also belong to event A, but A may contain, in addition, outcomes that do  1121   PROBABILITY  B  5  3  2  7  6  8  C  4  1  A  S  Figure 30.4 The general Venn diagram for three events is divided into eight regions.  not belong to B, then B is called a subset of A, a situation that is denoted by  B ⊂ A; alternatively, one may write A ⊃ B, which states that A contains B. In this  case, the closed curve representing the event B is often drawn lying completely within the closed curve representing the event A.  The operations ∪ and ∩ are extended straightforwardly to more than two events. If there exist n events A1, A2, . . . , An, in some sample space S , then the event consisting of all those outcomes that belong to one or more of the Ai is the union of A1, A2, . . . , An and is denoted by  Similarly, the event consisting of all the outcomes that belong to every one of the Ai is called the intersection of A1, A2, . . . , An and is denoted by  If, for any pair of values i, j with i  cid:3 = j,  A1 ∪ A2 ∪ ··· ∪ An.  A1 ∩ A2 ∩ ··· ∩ An.  Ai ∩ Aj = ∅   30.1    30.2    30.3   then the events Ai and Aj are said to be mutually exclusive or disjoint.  Consider three events A, B and C with a Venn diagram such as is shown in ﬁgure 30.4. It will be clear that, in general, the diagram will be divided into eight regions and they will be of four diﬀerent types. Three regions correspond to a single event; three regions are each the intersection of exactly two events; one region is the three-fold intersection of all three events; and ﬁnally one region corresponds to none of the events. Let us now consider the numbers of diﬀerent regions in a general n-event Venn diagram.  For one-event Venn diagrams there are two regions, for the two-event case there are four regions and, as we have just seen, for the three-event case there are eight. In the general n-event case there are 2n regions, as is clear from the fact that any particular region R lies either inside or outside the closed curve of any particular event. With two choices  inside or outside  for each of n closed curves, there are 2n diﬀerent possible combinations with which to characterise R. Once n  1122   30.1 VENN DIAGRAMS  gets beyond three it becomes impossible to draw a simple two-dimensional Venn diagram, but this does not change the results.  The 2n regions will break down into n + 1 types, with the numbers of each type  §  as follows  no events, one event but no intersections, two-fold intersections,  three-fold intersections,  nC0 = 1; nC1 = n; nC2 = 1 nC3 = 1  2 n n − 1 ; 3! n n − 1  n − 2 ;  ...  an n-fold intersection,  nCn = 1.  That this makes a total of 2n can be checked by considering the binomial expansion  2n =  1 + 1 n = 1 + n + 1  2 n n − 1  + ··· + 1.  Using Venn diagrams, it is straightforward to show that the operations ∩ and ∪ obey the following algebraic laws: commutativity, A ∩ B = B ∩ A, A ∪ B = B ∪ A;  A ∩ B  ∩ C = A ∩  B ∩ C , A ∩  B ∪ C  =  A ∩ B  ∪  A ∩ C , A ∪  B ∩ C  =  A ∪ B  ∩  A ∪ C ; A ∩ A = A, A ∪ A = A.   A ∪ B  ∪ C = A ∪  B ∪ C ;  associativity, distributivity,  idempotency,   cid:1 Show that  i  A ∪  A ∩ B  = A ∩  A ∪ B  = A,  ii   A − B  ∪  A ∩ B  = A.   i  Using the distributivity and idempotency laws above, we see that  A ∪  A ∩ B  =  A ∪ A  ∩  A ∪ B  = A ∩  A ∪ B .  By sketching a Venn diagram it is immediately clear that both expressions are equal to A. Nevertheless, we here proceed in a more formal manner in order to deduce this result algebraically. Let us begin by writing  X = A ∪  A ∩ B  = A ∩  A ∪ B , in  30.4  and the algebraic laws for ∩ and ∪, we may write  from which we want to deduce a simpler expression for the event X. Using the ﬁrst equality   30.4   A ∩ X = A ∩ [A ∪  A ∩ B ] =  A ∩ A  ∪ [A ∩  A ∩ B ] = A ∪  A ∩ B  = X.  1123  §  The symbols nCi, for i = 0, 1, 2,. . . , n, are a convenient notation for combinations; they and their properties are discussed in chapter 1.   Since A ∩ X = X we must have X ⊂ A. Now, using the second equality in  30.4  in a  similar way, we ﬁnd  PROBABILITY  A ∪ X = A ∪ [A ∩  A ∪ B ] =  A ∪ A  ∩ [A ∪  A ∪ B ] = A ∩  A ∪ B  = X,  from which we deduce that A ⊂ X. Thus, since X ⊂ A and A ⊂ X, we must conclude that   ii  Since we do not know how to deal with compound expressions containing a minus  sign, we begin by writing A− B = A∩ ¯B as mentioned above. Then, using the distributivity  X = A.  law, we obtain   A − B  ∪  A ∩ B  =  A ∩ ¯B  ∪  A ∩ B   = A ∩   ¯B ∪ B  = A ∩ S = A.  In fact, this result, like the ﬁrst one, can be proved trivially by drawing a Venn diagram.  cid:2   Further useful results may be derived from Venn diagrams. In particular, it is  simple to show that the following rules hold:   i  if A ⊂ B then ¯A ⊃ ¯B;  ii  A ∪ B = ¯A ∩ ¯B;  iii  A ∩ B = ¯A ∪ ¯B.  Statements  ii  and  iii  are known jointly as de Morgan’s laws and are sometimes useful in simplifying logical expressions.  cid:1 There exist two events A and B such that   X ∪ A  ∪  X ∪ ¯A  = B.  Find an expression for the event X in terms of A and B.  We begin by taking the complement of both sides of the above expression: applying de Morgan’s laws we obtain  ¯B =  X ∪ A  ∩  X ∪ ¯A .  We may then use the algebraic laws obeyed by ∩ and ∪ to yield  ¯B = X ∪  A ∩ ¯A  = X ∪ ∅ = X.  Thus, we ﬁnd that X = ¯B.  cid:2   30.2 Probability  In the previous section we discussed Venn diagrams, which are graphical repre- sentations of the possible outcomes of experiments. We did not, however, give any indication of how likely each outcome or event might be when any particular experiment is performed. Most experiments show some regularity. By this we mean that the relative frequency of an event is approximately the same on each occasion that a set of trials is performed. For example, if we throw a die N  1124   30.2 PROBABILITY  times then we expect that a six will occur approximately N 6 times  assuming, of course, that the die is not biased . The regularity of outcomes allows us to deﬁne the probability, Pr A , as the expected relative frequency of event A in a large number of trials. More quantitatively, if an experiment has a total of nS outcomes in the sample space S , and nA of these outcomes correspond to the event A, then the probability that event A will occur is  Pr A  =  nA nS  .  30.2.1 Axioms and theorems  From  30.5  we may deduce the following properties of the probability Pr A .   i  For any event A in a sample space S ,  0 ≤ Pr A  ≤ 1.   ii  For the entire sample space S we have nS nS  Pr S   =  = 1,  If Pr A  = 1 then A is a certainty; if Pr A  = 0 then A is an impossibility.  which simply states that we are certain to obtain one of the possible outcomes.   iii  If A and B are two events in S then, from the Venn diagrams in ﬁgure 30.3,  we see that  nA∪B = nA + nB − nA∩B,  the ﬁnal subtraction arising because the outcomes in the intersection of A and B are counted twice when the outcomes of A are added to those of B. Dividing both sides of  30.8  by nS , we obtain the addition rule for probabilities  Pr A ∪ B  = Pr A  + Pr B  − Pr A ∩ B .  However, if A and B are mutually exclusive events  A ∩ B = ∅  then Pr A ∩ B  = 0 and we obtain the special case  Pr A ∪ B  = Pr A  + Pr B .   iv  If ¯A is the complement of A then ¯A and A are mutually exclusive events.  Thus, from  30.7  and  30.10  we have  1 = Pr S   = Pr A ∪ ¯A  = Pr A  + Pr  ¯A ,  from which we obtain the complement law  Pr  ¯A  = 1 − Pr A .  1125   30.5    30.6    30.7    30.8    30.9    30.10    30.11    PROBABILITY  This is particularly useful for problems in which evaluating the probability of the complement is easier than evaluating the probability of the event itself.   cid:1 Calculate the probability of drawing an ace or a spade from a pack of cards.  Let A be the event that an ace is drawn and B the event that a spade is drawn. It immediately follows that Pr A  = 4 13 and Pr B  = 13 4 . The intersection of A and B consists of only the ace of spades and so Pr A ∩ B  = 1 52 = 4 13 .  52 = 1 52 . Thus, from  30.9   Pr A ∪ B  = 1  52 = 1  13 + 1  − 1  4  In this case it is just as simple to recognise that there are 16 cards in the pack that satisfy the required condition  13 spades plus three other aces  and so the probability is 16  52 .  cid:2   The above theorems can easily be extended to a greater number of events. For  example, if A1, A2, . . . , An are mutually exclusive events then  30.10  becomes  Pr A1 ∪ A2 ∪ ··· ∪ An  = Pr A1  + Pr A2  + ··· + Pr An .   30.12   Furthermore, if A1, A2, . . . , An  whether mutually exclusive or not  exhaust S , i.e.  are such that A1 ∪ A2 ∪ ··· ∪ An = S , then  Pr A1 ∪ A2 ∪ ··· ∪ An  = Pr S   = 1.   30.13    cid:1 A biased six-sided die has probabilities 1 respectively. Calculate p.  2 p, p, p, p, p, 2p of showing 1, 2, 3, 4, 5, 6  Given that the individual events are mutually exclusive,  30.12  can be applied to give  Pr 1 ∪ 2 ∪ 3 ∪ 4 ∪ 5 ∪ 6  = 1  2 p + p + p + p + p + 2p = 13  2 p.  The union of all possible outcomes on the LHS of this equation is clearly the sample space, S , and so  Now using  30.7 ,  Pr S   = 13  2 p.  2 p = Pr S   = 1 ⇒ p = 2  13 .  cid:2   13  When the possible outcomes of a trial correspond to more than two events, and those events are not mutually exclusive, the calculation of the probability of the union of a number of events is more complicated, and the generalisation of the addition law  30.9  requires further work. Let us begin by considering the union of three events A1, A2 and A3, which need not be mutually exclusive. We  ﬁrst deﬁne the event B = A2 ∪ A3 and, using the addition law  30.9 , we obtain  Pr A1 ∪ A2 ∪ A3  = Pr A1 ∪ B  = Pr A1  + Pr B  − Pr A1 ∩ B .   30.14   1126   30.2 PROBABILITY  However, we may write Pr A1 ∩ B  as  Pr A1 ∩ B  = Pr[A1 ∩  A2 ∪ A3 ]  = Pr[ A1 ∩ A2  ∪  A1 ∩ A3 ] = Pr A1 ∩ A2  + Pr A1 ∩ A3  − Pr A1 ∩ A2 ∩ A3 .  Substituting this expression, and that for Pr B  obtained from  30.9 , into  30.14  we obtain the probability addition law for three general events,  Pr A1 ∪ A2 ∪ A3  = Pr A1  + Pr A2  + Pr A3  − Pr A2 ∩ A3  − Pr A1 ∩ A3   − Pr A1 ∩ A2  + Pr A1 ∩ A2 ∩ A3 .   30.15    cid:1 Calculate the probability of drawing from a pack of cards one that is an ace or is a spade or shows an even number  2, 4, 6, 8, 10 .  If, as previously, A is the event that an ace is drawn, Pr A  = 4 that a spade is drawn, has Pr B  = 13 not a picture card  has Pr C  = 20  52 . Similarly the event B, 52 . The further possibility C, that the card is even  but  Pr A ∩ B  =  52 . The two-fold intersections have probabilities , Pr A ∩ C  = 0, Pr B ∩ C  =  .  5 52  There is no three-fold intersection as events A and C are mutually exclusive. Hence  Pr A ∪ B ∪ C  =  [ 4 + 13 + 20  −  1 + 0 + 5  +  0 ] =  31 52  .  The reader should identify the 31 cards involved.  cid:2   1 52  1 52  When the probabilities are combined to calculate the probability for the union of the n general events, the result, which may be proved by induction upon n  see the answer to exercise 30.4 , is  Pr Ai  − cid:4    cid:4  −··· +  −1 n+1 Pr A1 ∩ A2 ∩ ··· ∩ An .  Pr Ai ∩ Aj  +   cid:4   i,j,k  i,j  i  Pr Ai ∩ Aj ∩ Ak    30.16   Pr A1 ∪ A2 ∪ ··· ∪ An  =  Each summation runs over all possible sets of subscripts, except those in which any two subscripts in a set are the same. The number of terms in the summation of probabilities of m-fold intersections of the n events is given by nCm  as discussed in section 30.1 . Equation  30.9  is a special case of  30.16  in which n = 2 and only the ﬁrst two terms on the RHS survive. We now illustrate this result with a worked example that has n = 4 and includes a four-fold intersection.  1127   PROBABILITY   cid:1 Find the probability of drawing from a pack a card that has at least one of the following properties:  A, it is an ace; B, it is a spade; C, it is a black honour card  ace, king, queen, jack or 10 ; D, it is a black ace.  Measuring all probabilities in units of 1  Pr A  = 4,  Pr B  = 13,  52 , the single-event probabilities are Pr D  = 2.  Pr C  = 10,  The two-fold intersection probabilities, measured in the same units, are  Pr A ∩ B  = 1, Pr B ∩ C  = 5,  Pr A ∩ C  = 2, Pr B ∩ D  = 1,  Pr A ∩ D  = 2, Pr C ∩ D  = 2.  The three-fold intersections have probabilities  Pr A ∩ B ∩ C  = 1,  Pr A ∩ B ∩ D  = 1,  Pr A ∩ C ∩ D  = 2,  Pr B ∩ C ∩ D  = 1.  Finally, the four-fold intersection, requiring all four conditions to hold, is satisﬁed only by the ace of spades, and hence  again in units of 1  Pr A ∩ B ∩ C ∩ D  = 1.  52    Substituting in  30.16  gives  P =  1 52  [ 4 + 13 + 10 + 2  −  1 + 2 + 2 + 5 + 1 + 2  +  1 + 1 + 2 + 1  −  1 ] =  .  cid:2   20 52  We conclude this section on basic theorems by deriving a useful general  expression for the probability Pr A ∩ B  that two events A and B both occur in the case where A  say  is the union of a set of n mutually exclusive events Ai. In this case  A ∩ B =  A1 ∩ B  ∪ ··· ∪  An ∩ B ,  where the events Ai ∩ B are also mutually exclusive. Thus, from the addition law   30.12  for mutually exclusive events, we ﬁnd  Moreover, in the special case where the events Ai exhaust the sample space S , we have A ∩ B = S ∩ B = B, and we obtain the total probability law   cid:4   Pr Ai ∩ B .  Pr Ai ∩ B .  Pr A ∩ B  =  cid:4   i  Pr B  =  i  30.2.2 Conditional probability   30.17    30.18   So far we have deﬁned only probabilities of the form ‘what is the probability that event A happens?’. In this section we turn to conditional probability, the probability that a particular event occurs given the occurrence of another, possibly related, event. For example, we may wish to know the probability of event B, drawing an  1128   30.2 PROBABILITY  Pr A ∩ B  = Pr A  Pr BA  = Pr B  Pr AB .  Pr AB  =  Pr BA  =  Pr A ∩ B   Pr B   Pr B ∩ A   .  Pr A   ace from a pack of cards from which one has already been removed, given that event A, the card already removed was itself an ace, has occurred.  We denote this probability by Pr BA  and may obtain a formula for it by considering the total probability Pr A ∩ B  = Pr B ∩ A  that both A and B will  occur. This may be written in two ways, i.e.  From this we obtain  and   30.19    30.20    30.21   In terms of Venn diagrams, we may think of Pr BA  as the probability of B in  the reduced sample space deﬁned by A. Thus, if two events A and B are mutually exclusive then  Pr AB  = 0 = Pr BA .  When an experiment consists of drawing objects at random from a given set of objects, it is termed sampling a population. We need to distinguish between two diﬀerent ways in which such a sampling experiment may be performed. After an object has been drawn at random from the set it may either be put aside or returned to the set before the next object is randomly drawn. The former is termed ‘sampling without replacement’, the latter ‘sampling with replacement’.  cid:1 Find the probability of drawing two aces at random from a pack of cards  i  when the ﬁrst card drawn is replaced at random into the pack before the second card is drawn, and  ii  when the ﬁrst card is put aside after being drawn.  Let A be the event that the ﬁrst card is an ace, and B the event that the second card is an ace. Now  Pr A ∩ B  = Pr A  Pr BA ,  and for both  i  and  ii  we know that Pr A  = 4   i  If the ﬁrst card is replaced in the pack before the next is drawn then Pr BA  =  52 = 1 13 .  Pr B  = 4  52 = 1  13 , since A and B are independent events. We then have  Pr A ∩ B  = Pr A  Pr B  =  × 1 13  1 13  =  1  .  169   ii  If the ﬁrst card is put aside and the second then drawn, A and B are not independent  and Pr BA  = 3  51 , with the result that  Pr A ∩ B  = Pr A  Pr BA  =  × 3 51  1 13  =  1  .  cid:2   221  1129   PROBABILITY  Two events A and B are statistically independent if Pr AB  = Pr A   or equiva- lently if Pr BA  = Pr B  . In words, the probability of A given B is then the same  as the probability of A regardless of whether B occurs. For example, if we throw a coin and a die at the same time, we would normally expect that the probability of throwing a six was independent of whether a head was thrown. If A and B are statistically independent then it follows that  Pr A ∩ B  = Pr A  Pr B .   30.22   In fact, on the basis of intuition and experience,  30.22  may be regarded as the deﬁnition of the statistical independence of two events.  The idea of statistical independence is easily extended to an arbitrary number  of events A1, A2, . . . , An. The events are said to be  mutually  independent if  Pr Ai ∩ Aj  = Pr Ai  Pr Aj ,  Pr Ai ∩ Aj ∩ Ak  = Pr Ai  Pr Aj  Pr Ak ,  Pr A1 ∩ A2 ∩ ··· ∩ An  = Pr A1  Pr A2 ··· Pr An ,  for all combinations of indices i, j and k for which no two indices are the same. Even if all n events are not mutually independent, any two events for which  Pr Ai ∩ Aj  = Pr Ai  Pr Aj  are said to be pairwise independent.  We now derive two results that often prove useful when working with condi- tional probabilities. Let us suppose that an event A is the union of n mutually exclusive events Ai. If B is some other event then from  30.17  we have  Pr A ∩ B  =  Pr Ai ∩ B .  Pr AB  =  Pr AiB ,  Dividing both sides of this equation by Pr B , and using  30.19 , we obtain   30.23    30.24   which is the addition law for conditional probabilities.  Furthermore, if the set of mutually exclusive events Ai exhausts the sample space S then, from the total probability law  30.18 , the probability Pr B  of some event B in S can be written as   cid:4   i  Pr B  =  Pr Ai  Pr BAi .   cid:1 A collection of traﬃc islands connected by a system of one-way roads is shown in ﬁg- ure 30.5. At any given island a car driver chooses a direction at random from those available. What is the probability that a driver starting at O will arrive at B?  In order to leave O the driver must pass through one of A1, A2, A3 or A4, which thus form a complete set of mutually exclusive events. Since at each island  including O  the driver chooses a direction at random from those available, we have that Pr Ai  = 1 4 for  ...   cid:4   cid:4   i  i  1130   30.2 PROBABILITY  A4  A3  O  A2  B  A1   cid:5   Figure 30.5 A collection of traﬃc islands connected by one-way roads.  i = 1, 2, 3, 4. From ﬁgure 30.5, we see also that  Pr BA1  = 1  3 , Pr BA3  = 0, Pr BA4  = 2  4 = 1 2 .  Thus, using the total probability law  30.24 , we ﬁnd that the probability of arriving at B is given by  Pr Ai  Pr BAi  = 1  4  1  3 + 1  3 + 0 + 1  2  = 7  24 .  cid:2   Pr B  =  i   cid:6   3 , Pr BA2  = 1  cid:4   Finally, we note that the concept of conditional probability may be straightfor- wardly extended to several compound events. For example, in the case of three  events A, B, C, we may write Pr A ∩ B ∩ C  in several ways, e.g.  Pr A ∩ B ∩ C  = Pr C  Pr A ∩ BC   = Pr B ∩ C  Pr AB ∩ C  = Pr C  Pr BC  Pr AB ∩ C .  cid:4   Pr AiC  Pr BAi ∩ C .  Pr BC  =  i   cid:1 Suppose {Ai} is a set of mutually exclusive events that exhausts the sample space S . If B  and C are two other events in S , show that  Using  30.19  and  30.17 , we may write  Pr C  Pr BC  = Pr B ∩ C  =  Pr Ai ∩ B ∩ C .   30.25   Each term in the sum on the RHS can be expanded as an appropriate product of conditional probabilities,  Pr Ai ∩ B ∩ C  = Pr C  Pr AiC  Pr BAi ∩ C .  Substituting this form into  30.25  and dividing through by Pr C  gives the required result.  cid:2    cid:4   i  1131   PROBABILITY  30.2.3 Bayes’ theorem  In the previous section we saw that the probability that both an event A and a  related event B will occur can be written either as Pr A  Pr BA  or Pr B  Pr AB .  Hence  Pr A  Pr BA  = Pr B  Pr AB ,  from which we obtain Bayes’ theorem,  Pr AB  =  Pr BA .  Pr A  Pr B    30.26   This theorem clearly shows that Pr BA   cid:3 = Pr AB , unless Pr A  = Pr B . It is  sometimes useful to rewrite Pr B , if it is not known directly, as  Pr B  = Pr A  Pr BA  + Pr  ¯A  Pr B ¯A   so that Bayes’ theorem becomes  Pr AB  =  Pr A  Pr BA   Pr A  Pr BA  + Pr  ¯A  Pr B ¯A   .   30.27    cid:1 Suppose that the blood test for some disease is reliable in the following sense: for people who are infected with the disease the test produces a positive result in 99.99% of cases; for people not infected a positive test result is obtained in only 0.02% of cases. Furthermore, assume that in the general population one person in 10 000 people is infected. A person is selected at random and found to test positive for the disease. What is the probability that the individual is actually infected?  Let A be the event that the individual is infected and B be the event that the individual tests positive for the disease. Using Bayes’ theorem the probability that a person who tests positive is actually infected is  Pr AB  =  Pr A  Pr BA   Pr A  Pr BA  + Pr  ¯A  Pr B ¯A   .  Now Pr A  = 1 10000 = 1 − Pr  ¯A , and we are told that Pr BA  = 9999 10000 and Pr B ¯A  = 2 10000. Thus we obtain  Pr AB  =  1 10000 × 9999 10000   1 10000 × 9999 10000  +  9999 10000 × 2 10000   =  1 3  .  Thus, there is only a one in three chance that a person chosen at random, who tests positive for the disease, is actually infected.  At a ﬁrst glance, this answer may seem a little surprising, but the reason for the counter- intuitive result is that the probability that a randomly selected person is not infected is 9999 10000, which is very high. Thus, the 0.02% chance of a positive test for an uninfected person becomes signiﬁcant.  cid:2   1132   30.3 PERMUTATIONS AND COMBINATIONS  We note that  30.27  may be written in a more general form if S is not simply divided into A and ¯A but, rather, into any set of mutually exclusive events Ai that exhaust S . Using the total probability law  30.24 , we may then write   cid:4   i  Pr B  =  Pr Ai  Pr BAi ,  Pr AB  =   cid:11  Pr A  Pr BA  i Pr Ai  Pr BAi   ,  so that Bayes’ theorem takes the form  where the event A need not coincide with any of the Ai.  As a ﬁnal point, we comment that sometimes we are concerned only with the relative probabilities of two events A and C  say , given the occurrence of some other event B. From  30.26  we then obtain a diﬀerent form of Bayes’ theorem,  Pr AB  Pr CB   Pr A  Pr BA  Pr C  Pr BC   ,  =  which does not contain Pr B  at all.   30.28    30.29   30.3 Permutations and combinations  In equation  30.5  we deﬁned the probability of an event A in a sample space S as  where nA is the number of outcomes belonging to event A and nS is the total number of possible outcomes. It is therefore necessary to be able to count the number of possible outcomes in various common situations.  Pr A  =  nA nS  ,  30.3.1 Permutations  Let us ﬁrst consider a set of n objects that are all diﬀerent. We may ask in how many ways these n objects may be arranged, i.e. how many permutations of these objects exist. This is straightforward to deduce, as follows: the object in the ﬁrst position may be chosen in n diﬀerent ways, that in the second position in  n− 1 ways, and so on until the ﬁnal object is positioned. The number of possible  arrangements is therefore  n n − 1  n − 2 ···  1  = n!  Generalising  30.30  slightly, let us suppose we choose only k  < n  objects from n. The number of possible permutations of these k objects selected from n is given by  ; 8 n n − 1  n − 2 ···  n − k + 1   9:  k factors  =  n!   n − k !  ≡ nPk.   30.30    30.31   1133   PROBABILITY  In calculating the number of permutations of the various objects we have so far assumed that the objects are sampled without replacement – i.e. once an object has been drawn from the set it is put aside. As mentioned previously, however, we may instead replace each object before the next is chosen. The number of permutations of k objects from n with replacement may be calculated very easily since the ﬁrst object can be chosen in n diﬀerent ways, as can the second, the third, etc. Therefore the number of permutations is simply nk. This may also be viewed as the number of permutations of k objects from n where repetitions are allowed, i.e. each object may be used as often as one likes.  cid:1 Find the probability that in a group of k people at least two have the same birthday  ignoring 29 February .  It is simplest to begin by calculating the probability that no two people share a birthday, as follows. Firstly, we imagine each of the k people in turn pointing to their birthday on a year planner. Thus, we are sampling the 365 days of the year ‘with replacement’ and so the total number of possible outcomes is  365 k. Now  for the moment  we assume that no two people share a birthday and imagine the process being repeated, except that as each person points out their birthday it is crossed oﬀ the planner. In this case, we are sampling the days of the year ‘without replacement’, and so the possible number of outcomes for which all the birthdays are diﬀerent is  Hence the probability that all the birthdays are diﬀerent is  365Pk =  365!   365 − k !  .  p =  365!   365 − k ! 365k  .  q = 1 − p = 1 −  .  365!   365 − k ! 365k  cid:9   2πn  ,  n   cid:10   cid:8   n e  n! ∼ √  cid:7   q ≈ 1 − e −k  365  365 − k  365−k+0.5  .  Now using the complement rule  30.11 , the probability q that two or more people have the same birthday is simply  This expression may be conveniently evaluated using Stirling’s approximation for n! when n is large, namely  to give  It is interesting to note that if k = 23 the probability is a little greater than a half that at least two people have the same birthday, and if k = 50 the probability rises to 0.970. This can prove a good bet at a party of non-mathematicians!  cid:2   So far we have assumed that all n objects are diﬀerent  or distinguishable . Let us now consider n objects of which n1 are identical and of type 1, n2 are identical and of type 2, . . . , nm are identical and of type m  clearly n = n1 + n2 + ··· + nm .  From  30.30  the number of permutations of these n objects is again n!. However,  1134   30.3 PERMUTATIONS AND COMBINATIONS  the number of distinguishable permutations is only  n!  n1!n2!··· nm!  ,   30.32   since the ith group of identical objects can be rearranged in ni! ways without changing the distinguishable permutation.  cid:1 A set of snooker balls consists of a white, a yellow, a green, a brown, a blue, a pink, a black and 15 reds. How many distinguishable permutations of the balls are there?  In total there are 22 balls, the 15 reds being indistinguishable. Thus from  30.32  the number of distinguishable permutations is  22!   1!  1!  1!  1!  1!  1!  1!  15!   =  22! 15!  = 859 541 760.  cid:2   30.3.2 Combinations  We now consider the number of combinations of various objects when their order is immaterial. Assuming all the objects to be distinguishable, from  30.31  we see  that the number of permutations of k objects chosen from n is nPk = n!  n − k !.  Now, since we are no longer concerned with the order of the chosen objects, which can be internally arranged in k! diﬀerent ways, the number of combinations of k objects from n is  n!   n − k !k!  ≡ nCk ≡  for 0 ≤ k ≤ n,   30.33   where, as noted in chapter 1, nCk is called the binomial coeﬃcient since it also appears in the binomial expansion for positive integer n, namely   a + b n =  nCkakbn−k.   30.34    cid:7    cid:8   n k  n cid:4   k=0   cid:1 A hand of 13 playing cards is dealt from a well-shuﬄed pack of 52. What is the probability that the hand contains two aces?  Since the order of the cards in the hand is immaterial, the total number of distinct hands is simply equal to the number of combinations of 13 objects drawn from 52, i.e. 52C13. However, the number of hands containing two aces is equal to the number of ways, 4C2, in which the two aces can be drawn from the four available, multiplied by the number of ways, 48C11, in which the remaining 11 cards in the hand can be drawn from the 48 cards that are not aces. Thus the required probability is given by  4C2  48C11  52C13  48!  13!39!  11!37!  52!  =  =  4! 2!2!  3  4   2   12  13  38  39   49  50  51  52   = 0.213  cid:2   1135   PROBABILITY  Another useful result that may be derived using the binomial coeﬃcients is the number of ways in which n distinguishable objects can be divided into m piles, with ni objects in the ith pile, i = 1, 2, . . . , m  the ordering of objects within each pile being unimportant . This may be straightforwardly calculated as follows. We may choose the n1 objects in the ﬁrst pile from the original n objects in nCn1 ways. The n2 objects in the second pile can then be chosen from the n − n1 remaining objects in n−n1 Cn2 ways, etc. We may continue in this fashion until we reach the  m − 1 th pile, which may be formed in n−n1−···−nm−2Cnm−1 ways. The remaining  objects then form the mth pile and so can only be ‘chosen’ in one way. Thus the total number of ways of dividing the original n objects into m piles is given by the product  ··· n−n1−···−nm−2 Cnm−1 ··· n2! n − n1 − n2 ! ···  n − n1 − n2 − ··· − nm−2 ! n2! n − n1 − n2 !   n − n1 !  n − n1 !   n − n1 − n2 − ··· − nm−2 !  nm−1! n − n1 − n2 − ··· − nm−2 − nm−1 !  nm−1!nm!   30.35   N = nCn1  =  =  =  n!  n!  n−n1 Cn2 n1! n − n1 ! n1! n − n1 ! n1!n2!··· nm! ··· xnm  n!  .   cid:4   n1,n2 ,... ,nm  n1+n2+···+nm =n  These numbers are called multinomial coeﬃcients since  30.35  is the coeﬃcient of xn1 1 xn2 integer n  m in the multinomial expansion of  x1 + x2 +··· + xm n, i.e. for positive  2   x1 + x2 + ··· + xm n =  n!  n1!n2!··· nm!  xn1 1 xn2  2  ··· xnm m .  For the case m = 2, n1 = k, n2 = n− k,  30.35  reduces to the binomial coeﬃcient nCk. Furthermore, we note that the multinomial coeﬃcient  30.35  is identical to the expression  30.32  for the number of distinguishable permutations of n objects, ni of which are identical and of type i  for i = 1, 2, . . . , m and n1 +n2 +···+nm = n .  A few moments’ thought should convince the reader that the two expressions  30.35  and  30.32  must be identical.  cid:1 In the card game of bridge, each of four players is dealt 13 cards from a full pack of 52. What is the probability that each player is dealt an ace?  From  30.35 , the total number of distinct bridge dealings is 52!  13!13!13!13! . However, the number of ways in which the four aces can be distributed with one in each hand is 4!  1!1!1!1!  = 4!; the remaining 48 cards can then be dealt out in 48!  12!12!12!12!  ways. Thus the probability that each player receives an ace is  4!  48!  12! 4   13! 4 52!  =  24 13 4  = 0.105.  cid:2    49  50  51  52   As in the case of permutations we might ask how many combinations of k objects can be chosen from n with replacement  repetition . To calculate this, we  1136   30.3 PERMUTATIONS AND COMBINATIONS  may imagine the n  distinguishable  objects set out on a table. Each combination of k objects can then be made by pointing to k of the n objects in turn  with repetitions allowed . These k equivalent selections distributed amongst n diﬀerent but re-choosable objects are strictly analogous to the placing of k indistinguishable ‘balls’ in n diﬀerent boxes with no restriction on the number of balls in each box. A particular selection in the case k = 7, n = 5 may be symbolised as  xxx  xxxx.  This denotes three balls in the ﬁrst box, none in the second, one in the third, two in the fourth and one in the ﬁfth. We therefore need only consider the number of   distinguishable  ways in which k crosses and n− 1 vertical lines can be arranged, i.e. the number of permutations of k + n − 1 objects of which k are identical crosses and n − 1 are identical lines. This is given by  30.33  as   k + n − 1 ! k! n − 1 !  = n+k−1Ck.   30.36   We note that this expression also occurs in the binomial expansion for negative integer powers. If n is a positive integer, it is straightforward to show that  see chapter 1   −n =   a + b    −1 k n+k−1Cka  −n−kbk,  ∞ cid:4   k=0  where a is taken to be larger than b in magnitude.  cid:1 A system contains a number N of  non-interacting  particles, each of which can be in any of the quantum states of the system. The structure of the set of quantum states is such that there exist R energy levels with corresponding energies Ei and degeneracies gi  i.e. the ith energy level contains gi quantum states . Find the numbers of distinct ways in which the particles can be distributed among the quantum states of the system such that the ith energy level contains ni particles, for i = 1, 2, . . . , R, in the cases where the particles are   i  distinguishable with no restriction on the number in each state;  ii  indistinguishable with no restriction on the number in each state;  iii  indistinguishable with a maximum of one particle in each state;  iv  distinguishable with a maximum of one particle in each state.  It is easiest to solve this problem in two stages. Let us ﬁrst consider distributing the N particles among the R energy levels, without regard for the individual degenerate quantum states that comprise each level. If the particles are distinguishable then the number of distinct arrangements with ni particles in the ith level, i = 1, 2, . . . , R, is given by  30.35  as  If, however, the particles are indistinguishable then clearly there exists only one distinct arrangement having ni particles in the ith level, i = 1, 2, . . . , R . If we suppose that there exist wi ways in which the ni particles in the ith energy level can be distributed among the gi degenerate states, then it follows that the number of distinct ways in which the N  N!  n1!n2!··· nR!  .  1137   particles can be distributed among all R quantum states of the system, with ni particles in the ith level, is given by  PROBABILITY    W{ni} =  N!  R cid:3  n1!n2!··· nR!  wi  i=1  R cid:3   i=1  wi  for distinguishable particles,  for indistinguishable particles.   30.37   It therefore remains only for us to ﬁnd the appropriate expression for wi in each of the cases  i – iv  above.  Case  i . If there is no restriction on the number of particles in each quantum state, then in the ith energy level each particle can reside in any of the gi degenerate quantum states. Thus, if the particles are distinguishable then the number of distinct arrangements is simply wi = gni  . Thus, from  30.37 ,  i  W{ni} =  N!  n1!n2!··· nR!  gni i = N!  R cid:3   i=1  R cid:3   i=1  gni i ni!  .  Such a system of particles  for example atoms or molecules in a classical gas  is said to obey Maxwell–Boltzmann statistics.  Case  ii . If the particles are indistinguishable and there is no restriction on the number in each state then, from  30.36 , the number of distinct arrangements of the ni particles among the gi states in the ith energy level is  Substituting this expression in  30.37 , we obtain  Such a system of particles  for example a gas of photons  is said to obey Bose–Einstein statistics.  Case  iii . If a maximum of one particle can reside in each of the gi degenerate quantum states in the ith energy level then the number of particles in each state is either 0 or 1. Since the particles are indistinguishable, wi is equal to the number of distinct arrangements  in which ni states are occupied and gi − ni states are unoccupied; this is given by  Thus, from  30.37 , we have  wi =  W{ni} =  .   ni + gi − 1 ! ni! gi − 1 ! R cid:3    ni + gi − 1 ! ni! gi − 1 !  .  i=1  wi = gi Cni =  gi!  ni! gi − ni !  .  W{ni} =  gi!  ni! gi − ni !  .  R cid:3   i=1  wi = gi Pni =  gi!   gi − ni !  .  1138  Such a system is said to obey Fermi–Dirac statistics, and an example is provided by an electron gas.  Case  iv . Again, the number of particles in each state is either 0 or 1. If the particles are distinguishable, however, each arrangement identiﬁed in case  iii  can be reordered in ni! diﬀerent ways, so that   30.4 RANDOM VARIABLES AND DISTRIBUTIONS  Substituting this expression into  30.37  gives  R cid:3   i=1  W{ni} = N!  gi!  ni! gi − ni !  .  Such a system of particles has the names of no famous scientists attached to it, since it appears that it never occurs in nature.  cid:2   30.4 Random variables and distributions  Suppose an experiment has an outcome sample space S . A real variable X that is deﬁned for all possible outcomes in S  so that a real number – not necessarily unique – is assigned to each possible outcome  is called a random variable  RV . The outcome of the experiment may already be a real number and hence a random variable, e.g. the number of heads obtained in 10 throws of a coin, or the sum of the values if two dice are thrown. However, more arbitrary assignments are possi- ble, e.g. the assignment of a ‘quality’ rating to each successive item produced by a manufacturing process. Furthermore, assuming that a probability can be assigned to all possible outcomes in a sample space S , it is possible to assign a probability distribution to any random variable. Random variables may be divided into two classes, discrete and continuous, and we now examine each of these in turn.  30.4.1 Discrete random variables  A random variable X that takes only discrete values x1, x2, . . . , xn, with proba- bilities p1, p2, . . . , pn, is called a discrete random variable. The number of values n for which X has a non-zero probability is ﬁnite or at most countably inﬁnite. As mentioned above, an example of a discrete random variable is the number of heads obtained in 10 throws of a coin. If X is a discrete random variable, we can deﬁne a probability function  PF  f x  that assigns probabilities to all the distinct values that X can take, such that    f x  = Pr X = x  =   30.38   pi 0  if x = xi, otherwise.  A typical PF  see ﬁgure 30.6  thus consists of spikes, at valid values of X, whose height at x corresponds to the probability that X = x. Since the probabilities must sum to unity, we require  f xi  = 1.   30.39   We may also deﬁne the cumulative probability function  CPF  of X, F x , whose   cid:4  value gives the probability that X ≤ x, so that F x  = Pr X ≤ x  =  f xi .  xi≤x   30.40   n cid:4   i=1  1139   PROBABILITY  F x   1  f x   2p  p  1 2 p  1  2  4  5  6  x  3  a   1  2  5  6  3  4  b   Figure 30.6  a  A typical probability function for a discrete distribution, that for the biased die discussed earlier. Since the probabilities must sum to unity we require p = 2 13.  b  The cumulative probability function for the same discrete distribution.  Note that a diﬀerent scale has been used for  b .   Hence F x  is a step function that has upward jumps of pi at x = xi, i = 1, 2, . . . , n, and is constant between possible values of X. We may also calculate the probability that X lies between two limits, l1 and l2  l1 < l2 ; this is given by  Pr l1 < X ≤ l2  =  f xi  = F l2  − F l1 ,   30.41    cid:4   l1<xi≤l2  i.e. it is the sum of all the probabilities for which xi lies within the relevant interval.  cid:1 A bag contains seven red balls and three white balls. Three balls are drawn at random and not replaced. Find the probability function for the number of red balls drawn.  Let X be the number of red balls drawn. Then  Pr X = 0  = f 0  =  Pr X = 1  = f 1  =  × 1 8 × 7 8 × 6 8 × 5 8 i=0 f i  = 1, as expected.  cid:2   × 2 9 × 2 9 × 7 9 × 6 9  3 10 3 10 3 10 7 10  3   cid:11   Pr X = 3  = f 3  =  Pr X = 2  = f 2  =  ,  1  =  120  × 3 = × 3 =  =  7 24  .  7 40 21 40  ,  ,  30.4.2 Continuous random variables  It should be noted that  A random variable X is said to have a continuous distribution if X is deﬁned for a  continuous range of values between given limits  often −∞ to ∞ . An example of  a continuous random variable is the height of a person drawn from a population, which can take any value  within limits! . We can deﬁne the probability density function  PDF  f x  of a continuous random variable X such that  Pr x < X ≤ x + dx  = f x  dx,  1140   30.4 RANDOM VARIABLES AND DISTRIBUTIONS  f x   l2   cid:21   cid:21  ∞  l1  f x  dx = 1.  −∞ f x  dx = 1.  l1  a  b  x  l2  Figure 30.7 The probability density function for a continuous random vari- able X that can take values only between the limits l1 and l2. The shaded area  under the curve gives Pr a < X ≤ b , whereas the total area under the curve,  between the limits l1 and l2, is equal to unity.  i.e. f x  dx is the probability that X lies in the interval x < X ≤ x + dx. Clearly f x  must be a real function that is everywhere ≥ 0. If X can take only values  between the limits l1 and l2 then, in order for the sum of the probabilities of all possible outcomes to be equal to unity, we require  Often X can take any value between −∞ and ∞ and so  The probability that X lies in the interval a < X ≤ b is then given by  Pr a < X ≤ b  =  f x  dx,   30.42   i.e. Pr a < X ≤ b  is equal to the area under the curve of f x  between these  limits  see ﬁgure 30.7 .  random variable by  We may also deﬁne the cumulative probability function F x  for a continuous   cid:21   b  a   cid:21    30.43   where u is a  dummy  integration variable. We can then write  F x  = Pr X ≤ x  =  x  l1  f u  du,  Pr a < X ≤ b  = F b  − F a .  From  30.43  it is clear that f x  = dF x  dx.  1141   PROBABILITY   cid:18 −Ae  −x   cid:19 ∞  0 = A,   cid:21  ∞  Ae  −x dx =  cid:21   f x  dx =  1   cid:21   0  2  1   cid:1 A random variable X has a PDF f x  given by Ae in the interval 1 < X ≤ 2. elsewhere. Find the value of the constant A and hence calculate the probability that X lies We require the integral of f x  between 0 and ∞ to equal unity. Evaluating this integral,  −x in the interval 0 < x < ∞ and zero  we ﬁnd  and hence A = 1. From  30.42 , we then obtain  Pr 1 < X ≤ 2  =  2  −x dx = −e  −2 −  −e  e  −1  = 0.23.  cid:2   It is worth mentioning here that a discrete RV can in fact be treated as continuous and assigned a corresponding probability density function. If X is a discrete RV that takes only the values x1, x2, . . . , xn with probabilities p1, p2, . . . , pn then we may describe X as a continuous RV with PDF  f x  =  piδ x − xi ,   30.44   n cid:4   i=1  where δ x  is the Dirac delta function discussed in subsection 13.1.3. From  30.42  and the fundamental property of the delta function  13.12 , we see that  Pr a < X ≤ b  =   cid:21  n cid:4   a  b  f x  dx,   cid:21   =  pi  i=1  a  b  δ x − xi  dx =   cid:4   pi,  i  where the ﬁnal sum extends over those values of i for which a < xi ≤ b.  30.4.3 Sets of random variables  It is common in practice to consider two or more random variables simultane- ously. For example, one might be interested in both the height and weight of a person drawn at random from a population. In the general case, these vari- ables may depend on one another and are described by joint probability density functions; these are discussed fully in section 30.11. We simply note here that if we have  say  two random variables X and Y then by analogy with the single- variable case we deﬁne their joint probability density function f x, y  in such a way that, if X and Y are discrete RVs,  Pr X = xi, Y = yj  = f xi, yj ,  or, if X and Y are continuous RVs,  Pr x < X ≤ x + dx, y < Y ≤ y + dy  = f x, y  dx dy.  1142   30.5 PROPERTIES OF DISTRIBUTIONS  In many circumstances, however, random variables do not depend on one another, i.e. they are independent. As an example, for a person drawn at random from a population, we might expect height and IQ to be independent random variables. Let us suppose that X and Y are two random variables with probability density functions g x  and h y  respectively. In mathematical terms, X and Y are independent RVs if their joint probability density function is given by f x, y  = g x h y . Thus, for independent RVs, if X and Y are both discrete then  Pr X = xi, Y = yj  = g xi h yj   or, if X and Y are both continuous, then  Pr x < X ≤ x + dx, y < Y ≤ y + dy  = g x h y  dx dy.  The important point in each case is that the RHS is simply the product of the  individual probability density functions  compare with the expression for Pr A∩B   in  30.22  for statistically independent events A and B . By a simple extension, one may also consider the case where one of the random variables is discrete and the other continuous. The above discussion may also be trivially extended to any number of independent RVs Xi, i = 1, 2, . . . , N. −2y  cid:1 The independent random variables X and Y have the PDFs g x  = e respectively. Calculate the probability that X lies in the interval 1 < X ≤ 2 and Y lies in the interval 0 < Y ≤ 1.  −x and h y  = 2e  Since X and Y are independent RVs, the required probability is given by  Pr 1 < X ≤ 2, 0 < Y ≤ 1  =  2   cid:21   cid:21   cid:18 −e  2  1  1  1   cid:21   cid:21  × cid:18 −e  0 1  0  g x  dx  cid:19  −x dx e −x  2  1  =  =  h y  dy  cid:19  −2y dy 2e 0 = 0.23 × 0.86 = 0.20.  cid:2  −2y  1  30.5 Properties of distributions  For a single random variable X, the probability density function f x  contains all possible information about how the variable is distributed. However, for the purposes of comparison, it is conventional and useful to characterise f x  by certain of its properties. Most of these standard properties are deﬁned in terms of averages or expectation values. In the most general case, the expectation value E[g X ] of any function g X  of the random variable X is deﬁned as  E[ g X ] =  i g xi f xi  g x f x  dx for a continuous distribution,  for a discrete distribution,   30.45   where the sum or integral is over all allowed values of X. It is assumed that   cid:11   cid:1   1143   PROBABILITY  the series is absolutely convergent or that the integral exists, as the case may be. From its deﬁnition it is straightforward to show that the expectation value has the following properties:   i  if a is a constant then E[a] = a;  ii  if a is a constant then E[ag X ] = aE[g X ];  iii  if g X  = s X  + t X  then E[ g X ] = E[ s X ] + E[t X ].  It should be noted that the expectation value is not a function of X but is instead a number that depends on the form of the probability density function f x  and the function g x . Most of the standard quantities used to characterise f x  are simply the expectation values of various functions of the random variable X. We now consider these standard quantities.  30.5.1 Mean   cid:11   cid:1   The property most commonly used to characterise a probability distribution is its mean, which is deﬁned simply as the expectation value E[X] of the variable X itself. Thus, the mean is given by  E[X] =  for a discrete distribution,  i xif xi  xf x  dx for a continuous distribution.   30.46   The alternative notations µ and  cid:20 x cid:21  are also commonly used to denote the mean.  If in  30.46  the series is not absolutely convergent, or the integral does not exist, we say that the distribution does not have a mean, but this is very rare in physical applications.   cid:1 The probability of ﬁnding a 1s electron in a hydrogen atom in a given inﬁnitesimal volume dV is ψ  ψ dV , where the quantum mechanical wavefunction ψ is given by  ∗  ψ = Ae  −r a0 .  Find the value of the real constant A and thereby deduce the mean distance of the electron from the origin.  Let us consider the random variable R = ‘distance of the electron from the origin’. Since the 1s orbital has no θ- or φ-dependence  it is spherically symmetric , we may consider the inﬁnitesimal volume element dV as the spherical shell with inner radius r and outer radius r + dr. Thus, dV = 4πr2 dr and the PDF of R is simply Pr r < R ≤ r + dr  ≡ f r  dr = 4πr2A2e  −2r a0 dr.  The value of A is found by requiring the total probability  i.e. the probability that the electron is somewhere  to be unity. Since R must lie between zero and inﬁnity, we require that   cid:21  ∞  A2  0  −2r a0 4πr2 dr = 1.  e  1144   Integrating by parts we ﬁnd A = 1  πa3 we ﬁnd  0 1 2. Now, using the deﬁnition of the mean  30.46 ,  30.5 PROPERTIES OF DISTRIBUTIONS   cid:21  ∞  0   cid:21  ∞  4 a3 0  0  E[R] =  rf r  dr =  −2r a0 dr.  r3e  The integral on the RHS may be integrated by parts and takes the value 3a4 quently we ﬁnd that E[R] = 3a0 2.  cid:2   0 8; conse-  30.5.2 Mode and median  Although the mean discussed in the last section is the most common measure of the ‘average’ of a distribution, two other measures, which do not rely on the concept of expectation values, are frequently encountered.  The mode of a distribution is the value of the random variable X at which the probability  density  function f x  has its greatest value. If there is more than one value of X for which this is true then each value may equally be called the mode of the distribution.  The median M of a distribution is the value of the random variable X at which the cumulative probability function F x  takes the value 1 2 . Related to the median are the lower and upper quartiles Ql and Qu of the PDF, which are deﬁned such that  2 , i.e. F M  = 1  F Ql  = 1 4 ,  F Qu  = 3 4 .  Thus the median and lower and upper quartiles divide the PDF into four regions each containing one quarter of the probability. Smaller subdivisions are also possible, e.g. the nth percentile, Pn, of a PDF is deﬁned by F Pn  = n 100.   cid:1 Find the mode of the PDF for the distance from the origin of the electron whose wave- function was given in the previous example.  We found in the previous example that the PDF for the electron’s distance from the origin was given by   30.47   Diﬀerentiating f r  with respect to r, we obtain  Thus f r  has turning points at r = 0 and r = a0, where df dr = 0. It is straightforward to show that r = 0 is a minimum and r = a0 is a maximum. Moreover, it is also clear that r = a0 is a global maximum  as opposed to just a local one . Thus the mode of f r  occurs at r = a0.  cid:2   f r  =  4r2 a3 0  e   cid:7   −2r a0 .  cid:8   df dr  =  8r a3 0  1 − r  a0  −2r a0 .  e  1145   PROBABILITY  The variance of a distribution, V [X], also written σ2, is deﬁned by   cid:18   X − µ 2   cid:19   =  30.5.3 Variance and standard deviation   cid:11   cid:1  j xj − µ 2f xj    x − µ 2f x  dx for a continuous distribution.  for a discrete distribution,  V [X] = E   30.48   Here µ has been written for the expectation value E[X] of X. As in the case of the mean, unless the series and the integral in  30.48  converge the distribution does not have a variance. From the deﬁnition  30.48  we may easily derive the following useful properties of V [X]. If a and b are constants then   i  V [a] = 0,  ii  V [aX + b] = a2V [X].  The variance of a distribution is always positive; its positive square root is known as the standard deviation of the distribution and is often denoted by σ. Roughly speaking, σ measures the spread  about x = µ  of the values that X can assume.   cid:1 Find the standard deviation of the PDF for the distance from the origin of the electron whose wavefunction was discussed in the previous two examples.  Inserting the expression  30.47  for the PDF f r  into  30.48 , the variance of the random variable R is given by  V [R] =   r − µ 2 4r2 a3 0  0  −2r a0 dr =  e   r4 − 2r3µ + r2µ2 e  −2r a0 dr,   cid:21  ∞  4 a3 0  0   cid:21  ∞  where the mean µ = E[R] = 3a0 2. Integrating each term in the integrand by parts we obtain  Thus the standard deviation of the distribution is σ =  V [R] = 3a2 0  − 3µa0 + µ2 = 3a2 0 4 √ 3a0 2.  cid:2   .  We may also use the deﬁnition  30.48  to derive the Bienaym´e–Chebyshev inequality, which provides a useful upper limit on the probability that random variable X takes values outside a given range centred on the mean. Let us consider the case of a continuous random variable, for which  Pr X − µ ≥ c  =  f x  dx,  x−µ≥c  where the integral on the RHS extends over all values of x satisfying the inequality   cid:21   1146   30.5 PROPERTIES OF DISTRIBUTIONS  x − µ ≥ c. From  30.48 , we ﬁnd that   cid:21    cid:21   σ2 ≥   x − µ 2f x  dx ≥ c2  x−µ≥c  f x  dx.  x−µ≥c   30.49   The ﬁrst inequality holds because both  x − µ 2 and f x  are non-negative for all x, and the second inequality holds because  x − µ 2 ≥ c2 over the range of integration. However, the RHS of  30.49  is simply equal to c2 Pr X − µ ≥ c ,  and thus we obtain the required inequality  Pr X − µ ≥ c  ≤ σ2 c2 .  A similar derivation may be carried through for the case of a discrete random variable. Thus, for any distribution f x  that possesses a variance we have, for example,  Pr X − µ ≥ 2σ  ≤ 1  and Pr X − µ ≥ 3σ  ≤ 1  .  4  9  30.5.4 Moments  The mean  or expectation  of X is sometimes called the ﬁrst moment of X, since it is deﬁned as the sum or integral of the probability density function multiplied by the ﬁrst power of x. By a simple extension the kth moment of a distribution is deﬁned by  µk ≡ E[Xk] =  j f xj   j xk xkf x  dx for a continuous distribution.  for a discrete distribution,   30.50    cid:11   cid:1   For notational convenience, we have introduced the symbol µk to denote E[Xk], the kth moment of the distribution. Clearly, the mean of the distribution is then denoted by µ1, often abbreviated simply to µ, as in the previous subsection, as this rarely causes confusion.  A useful result that relates the second moment, the mean and the variance of  a distribution is proved using the properties of the expectation operator:  V [X] = E   cid:18   cid:18   X − µ 2  cid:18  X2 − 2µX + µ2  cid:18   cid:18  X2 X2 X2   cid:19   cid:19   cid:19  − 2µE[X] + µ2  cid:19  − 2µ2 + µ2  cid:19  − µ2.  = E  = E  = E  = E  In alternative notations, this result can be written   cid:20  x − µ 2 cid:21  =  cid:20 x2 cid:21  −  cid:20 x cid:21 2  or  σ2 = µ2 − µ2  1.  1147   30.51    PROBABILITY   cid:4   j   cid:4   j   cid:1 A biased die has probabilities p 2, p, p, p, p, 2p of showing 1, 2, 3, 4, 5, 6 respectively. Find  i  the mean,  ii  the second moment and  iii  the variance of this probability distribution.  By demanding that the sum of the probabilities equals unity we require p = 2 13. Now, using the deﬁnition of the mean  30.46  for a discrete distribution,  E[X] =  xj f xj  = 1 × 1  2 p + 2 × p + 3 × p + 4 × p + 5 × p + 6 × 2p  =  p =  53 2  × 2 13  53 2  =  53 13  .  Similarly, using the deﬁnition of the second moment  30.50 ,  E[X 2] =  j f xj  = 12 × 1  2 p + 22p + 32p + 42p + 52p + 62 × 2p  x2  253  253 13  Finally, using the deﬁnition of the variance  30.48 , with µ = 53 13, we obtain  .  2  =  p =   cid:4   xj − µ 2f xj   cid:8   cid:7  =  1 − µ 2 1 2 p +  2 − µ 2p +  3 − µ 2p +  4 − µ 2p +  5 − µ 2p +  6 − µ 22p  j  V [X] =  =  3120 169  p =  480 169  .   cid:18    cid:19  −  E[X] 2.  cid:2   It is easy to verify that V [X] = E  X 2  In practice, to calculate the moments of a distribution it is often simpler to use the moment generating function discussed in subsection 30.7.2. This is particularly true for higher-order moments, where direct evaluation of the sum or integral in  30.50  can be somewhat laborious.  30.5.5 Central moments  The variance V [X] is sometimes called the second central moment of the distribu- tion, since it is deﬁned as the sum or integral of the probability density function  multiplied by the second power of x− µ. The origin of the term ‘central’ is that by  subtracting µ from x before squaring we are considering the moment about the mean of the distribution, rather than about x = 0. Thus the kth central moment of a distribution is deﬁned as   cid:11   cid:1  j xj − µ kf xj   x − µ kf x  dx for a continuous distribution.   cid:18   X − µ k  for a discrete distribution,  νk ≡ E   30.52    cid:19   =  V [X] ≡ ν2 and we may write  30.51  as ν2 = µ2 − µ2  It is convenient to introduce the notation νk for the kth central moment. Thus 1. Clearly, the ﬁrst central moment of a distribution is always zero since, for example in the continuous case,   x − µ f x  dx =  xf x  dx − µ  f x  dx = µ −  µ × 1  = 0.  ν1 =   cid:21    cid:21    cid:21   1148   30.5 PROPERTIES OF DISTRIBUTIONS  We note that the notation µk and νk for the moments and central moments respectively is not universal. Indeed, in some books their meanings are reversed. We can write the kth central moment of a distribution in terms of its kth and  lower-order moments by expanding  X − µ k in powers of X. We have already noted that ν2 = µ2− µ2 1, and similar expressions may be obtained for higher-order  central moments. For example,  ν3 = E   cid:19    cid:18   cid:18   X − µ1 3 X3 − 3µ1X2 + 3µ2  = E  = µ3 − 3µ1µ2 + 3µ2 = µ3 − 3µ1µ2 + 2µ3  1.  1µ1 − µ3  1  1X − µ3  1   cid:19    30.53    30.54   In general, it is straightforward to show that  νk = µk − kC1µk−1µ1 + ··· +  −1 r kCrµk−rµr  1 + ··· +  −1 k−1 kCk−1 − 1 µk  1.  Once again, direct evaluation of the sum or integral in  30.52  can be rather tedious for higher moments, and it is usually quicker to use the moment generating function  see subsection 30.7.2 , from which the central moments can be easily evaluated as well.   cid:1 The PDF for a Gaussian distribution  see subsection 30.9.1  with mean µ and variance σ2 is given by   cid:13    cid:14   .  f x  =  exp  √ 1  σ  2π  −  x − µ 2  2σ2  Obtain an expression for the kth central moment of this distribution.  As an illustration, we will perform this calculation by evaluating the integral in  30.52  directly. Thus, the kth central moment of f x  is given by   cid:21  ∞  cid:21  ∞ −∞ x − µ kf x  dx  cid:7   cid:21  ∞ √ −∞ x − µ k exp 1 2π √ − y2 1 −∞ yk exp 2σ2   cid:13   σ  2π  σ  νk =  =  =   cid:14   dx  −  x − µ 2  cid:8   2σ2  dy,  where in the last line we have made the substitution y = x − µ. It is clear that if k is  odd then the integrand is an odd function of y and hence the integral equals zero. Thus, νk = 0 if k is odd. When k is even, we could calculate νk by integrating by parts to obtain a reduction formula, but it is more elegant to consider instead the standard integral  see subsection 6.4.2    cid:21  ∞ −∞ exp −αy2  dy = π1 2α  −1 2,  I =   30.55   1149   and diﬀerentiate it repeatedly with respect to α  see section 5.12 . Thus, we obtain  PROBABILITY  −3 2   cid:21  ∞  cid:21  ∞ −∞ y2 exp −αy2  dy = − 1 = − −∞ y4 exp −αy2  dy =   1  cid:21  ∞ −∞ y2n exp −αy2  dy =  −1 n  1  ... =  −1 n  2  π1 2α  2 π1 2α  2    3  =  −5 2  dI dα d2I dα2  dnI dαn  2  ···   1  2  2n − 1  π1 2α  2    3  − 2n+1  2.  Setting α = 1  2σ2  and substituting the above result into  30.55 , we ﬁnd  for k even   νk =   1  2    3  2  ···   1  2  k − 1   2σ2 k 2 =  1  3 ···  k − 1 σk.  cid:2   One may also characterise a probability distribution f x  using the closely  related normalised and dimensionless central moments  From this set, γ3 and γ4 are more commonly called, respectively, the skewness and kurtosis of the distribution. The skewness γ3 of a distribution is zero if it is symmetrical about its mean. If the distribution is skewed to values of x smaller than the mean then γ3   0 if the distribution is skewed to higher values of x.  From the above example, we see that the kurtosis of the Gaussian distribution   subsection 30.9.1  is given by  γk ≡ νk ν k 2 2  =  νk σk .  γ4 =  =  ν4 ν2 2  3σ4 σ4 = 3.  It is therefore common practice to deﬁne the excess kurtosis of a distribution  as γ4 − 3. A positive value of the excess kurtosis implies a relatively narrower  peak and wider wings than the Gaussian distribution with the same mean and variance. A negative excess kurtosis implies a wider peak and shorter wings.  Finally, we note here that one can also describe a probability density function f x  in terms of its cumulants, which are again related to the central moments. However, we defer the discussion of cumulants until subsection 30.7.4, since their deﬁnition is most easily understood in terms of generating functions.  30.6 Functions of random variables  Suppose X is some random variable for which the probability density function f x  is known. In many cases, we are more interested in a related random variable Y = Y  X , where Y  X  is some function of X. What is the probability density  1150   30.6 FUNCTIONS OF RANDOM VARIABLES  function g y  for the new random variable Y ? We now discuss how to obtain this function.  30.6.1 Discrete random variables  If X is a discrete RV that takes only the values xi, i = 1, 2, . . . , n, then Y must also be discrete and takes the values yi = Y  xi , although some of these values may be identical. The probability function for Y is given by  g y  =  j f xj   if y = yi, otherwise,   30.56   where the sum extends over those values of j for which yi = Y  xj . The simplest case arises when the function Y  X  possesses a single-valued inverse X Y  . In this case, only one x-value corresponds to each y-value, and we obtain a closed-form expression for g y  given by   cid:11   0    g y  =  f x yi   0  if y = yi, otherwise.  If Y  X  does not possess a single-valued inverse then the situation is more complicated and it may not be possible to obtain a closed-form expression for g y . Nevertheless, whatever the form of Y  X , one can always use  30.56  to obtain the numerical values of the probability function g y  at y = yi.  30.6.2 Continuous random variables  If X is a continuous RV, then so too is the new random variable Y = Y  X . The probability that Y lies in the range y to y + dy is given by  g y  dy =  f x  dx,   30.57   where dS corresponds to all values of x for which Y lies in the range y to y + dy. Once again the simplest case occurs when Y  X  possesses a single-valued inverse X Y  . In this case, we may write  g y  dy =  x y+dy    cid:7    cid:7   f x    dx  x y   x y + dx   dy  dy   cid:7    cid:7   f x    dx  ,   cid:20  cid:20  cid:20  cid:20  cid:21   from which we obtain  g y  = f x y     30.58    cid:21   dS   cid:20  cid:20  cid:20  cid:20  =  cid:21   cid:20  cid:20  cid:20  cid:20  dx  dy  x y    cid:20  cid:20  cid:20  cid:20  .  1151   PROBABILITY  lighthouse  θ  L  beam  O  coastline  y  Figure 30.8 The illumination of a coastline by the beam from a lighthouse.   cid:1 A lighthouse is situated at a distance L from a straight coastline, opposite a point O, and sends out a narrow continuous beam of light simultaneously in opposite directions. The beam rotates with constant angular velocity. If the random variable Y is the distance along the coastline, measured from O, of the spot that the light beam illuminates, ﬁnd its probability density function.  velocity, θ is distributed uniformly between −π 2 and π 2, and so f θ  = 1 π. Now The situation is illustrated in ﬁgure 30.8. Since the light beam rotates at a constant angular −1 y L , provided that θ lies between −π 2 and π 2. Since dy dθ = L sec2 θ = L 1 + tan2 θ  = L[1 +  y L 2], from  y = L tan θ, which possesses the single-valued inverse θ = tan   30.58  we ﬁnd   cid:20  cid:20  cid:20  cid:20  =   cid:20  cid:20  cid:20  cid:20  dθ  dy  1 π  g y  =  1  πL[1 +  y L 2]  for −∞ < y < ∞.  A distribution of this form is called a Cauchy distribution and is discussed in subsec- tion 30.9.5.  cid:2   If Y  X  does not possess a single-valued inverse then we encounter complica- tions, since there exist several intervals in the X-domain for which Y lies between y and y + dy. This is illustrated in ﬁgure 30.9, which shows a function Y  X  such that X Y   is a double-valued function of Y . Thus the range y to y + dy corresponds to X’s being either in the range x1 to x1 + dx1 or in the range x2 to x2 + dx2. In general, it may not be possible to obtain an expression for g y  in closed form, although the distribution may always be obtained numerically using  30.57 . However, a closed-form expression may be obtained in the case where there exist single-valued functions x1 y  and x2 y  giving the two values of x that correspond to any given value of y. In this case,  x1 y+dy   x2 y+dy   g y  dy =  f x  dx  f x  dx   cid:20  cid:20  cid:20  cid:20  cid:21   x1 y    cid:20  cid:20  cid:20  cid:20  ,   cid:20  cid:20  cid:20  cid:20  +  cid:20  cid:20  cid:20  cid:20  cid:21   cid:20  cid:20  cid:20  cid:20  + f x2 y    x2 y    cid:20  cid:20  cid:20  cid:20  dx2  dy   cid:20  cid:20  cid:20  cid:20  .   cid:20  cid:20  cid:20  cid:20  dx1  dy  1152  g y  = f x1 y     30.59   from which we obtain   30.6 FUNCTIONS OF RANDOM VARIABLES  Y  y + dy y  dx1  dx2  X  Figure 30.9 Illustration of a function Y  X  whose inverse X Y   is a double- valued function of Y . The range y to y + dy corresponds to X being either in the range x1 to x1 + dx1 or in the range x2 to x2 + dx2.  This result may be generalised straightforwardly to the case where the range y to y + dy corresponds to more than two x-intervals.  cid:1 The random variable X is Gaussian distributed  see subsection 30.9.1  with mean µ and variance σ2. Find the PDF of the new variable Y =  X − µ 2 σ2.  It is clear that X Y   is a double-valued function of Y . However, it is straightforward to obtain single-valued functions giving the two values of x that correspond y is taken to  to a given value of y; these are x1 = µ − σ  y and x2 = µ + σ  in this case,  y, where  √  √  √  mean the positive square root.  The PDF of X is given by  Since dx1 dy = −σ  2  √ 1  f x  =  σ  2π   cid:20  cid:20  cid:20  cid:20  −σ  √ y  and dx2 dy = σ  2 √ exp − 1 √ 1 2 y  2π √ −1 2 exp − 1 1  σ  2  y    1 2 y   2 y .  =  2  π  g y  =   cid:14   −  x − µ 2   cid:13  exp  cid:20  cid:20  cid:20  cid:20  + √ y , from  30.59  we obtain   cid:20  cid:20  cid:20  cid:20  σ  exp − 1 2 y    cid:20  cid:20  cid:20  cid:20   √ 1  √  2σ2  .  2  y  σ  2π  As we shall see in subsection 30.9.3, this is the gamma distribution γ  1  2  .  cid:2  2 , 1  30.6.3 Functions of several random variables  We may extend our discussion further, to the case in which the new random variable is a function of several other random variables. For deﬁniteness, let us consider the random variable Z = Z X, Y  , which is a function of two other RVs X and Y . Given that these variables are described by the joint probability density function f x, y , we wish to ﬁnd the probability density function p z  of the variable Z.  1153   PROBABILITY   cid:4   i,j   cid:21  cid:21   dS  If X and Y are both discrete RVs then  p z  =  f xi, yj ,   30.60   where the sum extends over all values of i and j for which Z xi, yj  = z. Similarly, if X and Y are both continuous RVs then p z  is found by requiring that  p z  dz =  f x, y  dx dy,   30.61   where dS is the inﬁnitesimal area in the xy-plane lying between the curves Z x, y  = z and Z x, y  = z + dz.  cid:1 Suppose X and Y are independent continuous random variables in the range −∞ to ∞,  with PDFs g x  and h y  respectively. Obtain expressions for the PDFs of Z = X + Y and W = XY .  Since X and Y are independent RVs, their joint PDF is simply f x, y  = g x h y . Thus, from  30.61 , the PDF of the sum Z = X + Y is given by  Thus p z  is the convolution of the PDFs of g and h  i.e. p = g ∗ h, see subsection 13.1.7 .  In a similar way, the PDF of the product W = XY is given by  p z  dz =  =  dy h y   dz.   cid:8    cid:21   z+dz−x z−x   cid:21  ∞  cid:7  cid:21  ∞ −∞ dx g x  −∞ g x h z − x  dx  cid:21  ∞  cid:7  cid:21  ∞  −∞ dx g x    w+dw  x w x   cid:8    cid:21   =  −∞ g x h w x   dw  cid:2   dxx  q w  dw =  dy h y   The prescription  30.61  is readily generalised to functions of n random variables Z = Z X1, X2, . . . , Xn , in which case the inﬁnitesimal ‘volume’ element dS is the region in x1x2 ··· xn-space between the  hyper surfaces Z x1, x2, . . . , xn  = z and Z x1, x2, . . . , xn  = z + dz. In practice, however, the integral is diﬃcult to evaluate, since one is faced with the complicated geometrical problem of determining the limits of integration. Fortunately, an alternative  and powerful  technique exists for evaluating integrals of this kind. One eliminates the geometrical problem by integrating over all values of the variables xi without restriction, while shifting the constraint on the variables to the integrand. This is readily achieved by multiplying the integrand by a function that equals unity in the inﬁnitesimal region dS and zero elsewhere. From the discussion of the Dirac delta function in  subsection 13.1.3, we see that δ Z x1, x2, . . . , xn −z  dz satisﬁes these requirements,  and so in the most general case we have   cid:21  cid:21    cid:21   p z  =  ···  f x1, x2, . . . , xn δ Z x1, x2, . . . , xn  − z  dx1dx2 . . . dxn,   30.62   1154   30.6 FUNCTIONS OF RANDOM VARIABLES  where the range of integration is over all possible values of the variables xi. This integral is most readily evaluated by substituting in  30.62  the Fourier integral representation of the Dirac delta function discussed in subsection 13.1.4, namely   cid:21  ∞ −∞ eik Z  x1,x2,...,xn −z  dk.  1 2π   30.63   δ Z x1, x2, . . . , xn  − z  =  This is best illustrated by considering a speciﬁc example.  cid:1 A general one-dimensional random walk consists of n independent steps, each of which can be of a diﬀerent length and in either direction along the x-axis. If g x  is the PDF for the  positive or negative  displacement X along the x-axis achieved in a single step, obtain an expression for the PDF of the total displacement S after n steps.  The total displacement S is simply the algebraic sum of the displacements Xi achieved in each of the n steps, so that  S = X1 + X2 + ··· + Xn.  Since the random variables Xi are independent and have the same PDF g x , their joint PDF is simply g x1 g x2 ··· g xn . Substituting this into  30.62 , together with  30.63 , we   cid:21  ∞ −∞ eik[ x1+x2+···+xn −s] dk dx1dx2 ··· dxn  1 2π   30.64   obtain  p s  =   cid:21  ∞  cid:21  ∞  cid:21  ∞  −∞  −∞ 1 2π  ···  =  −∞ dk e  It is convenient to deﬁne the characteristic function C k  of the variable X as  n   cid:21  ∞  cid:7  cid:21  ∞  cid:8  −∞ g x1 g x2 ··· g xn  −iks  cid:21  ∞  cid:21  ∞  −∞ g x eikx dx  C k  =  .  −∞ g x eikx dx,  p s  =  1 2π  −∞ e  −iks[C k ]n dk.  which is simply related to the Fourier transform of g x . Then  30.64  may be written as  Thus p s  can be found by evaluating two Fourier integrals. Characteristic functions will be discussed in more detail in subsection 30.7.3.  cid:2   30.6.4 Expectation values and variances  In some cases, one is interested only in the expectation value or the variance of the new variable Z rather than in its full probability density function. For deﬁniteness, let us consider the random variable Z = Z X, Y  , which is a function of two RVs X and Y with a known joint distribution f x, y ; the results we will obtain are readily generalised to more  or fewer  variables.  It is clear that E[Z] and V [Z] can be obtained, in principle, by ﬁrst using the methods discussed above to obtain p z  and then evaluating the appropriate sums or integrals. The intermediate step of calculating p z  is not necessary, however, since it is straightforward to obtain expressions for E[Z] and V [Z] in terms of  1155   PROBABILITY  the variables X and Y . For example, if X and Y are continuous RVs then the expectation value of Z is given by   cid:21    cid:21  cid:21   E[Z] =  zp z  dz =  Z x, y f x, y  dx dy.   30.65   An analogous result exists for discrete random variables.  Integrals of the form  30.65  are often diﬃcult to evaluate. Nevertheless, we may use  30.65  to derive an important general result concerning expectation values. If X and Y are any two random variables and a and b are arbitrary constants then by letting Z = aX + bY we ﬁnd  E[aX + bY ] = aE[X] + bE[Y ].   cid:7    cid:8    cid:7    cid:8   Furthermore, we may use this result to obtain an approximate expression for the expectation value E[ Z X, Y  ] of any arbitrary function of X and Y . Letting µX = E[X] and µY = E[Y ], and provided Z X, Y   can be reasonably approximated by the linear terms of its Taylor expansion about the point  µX , µY  , we have   Y − µY  ,  Z X, Y   ≈ Z µX , µY   +  cid:7   ∂Z ∂X   cid:8    X − µX  +  cid:7   ∂Z ∂Y   cid:8   where the partial derivatives are evaluated at X = µX and Y = µY . Taking the expectation values of both sides, we ﬁnd  E[ Z X, Y  ] ≈ Z µX , µY  + which gives the approximate result E[ Z X, Y  ] ≈ Z µX , µY  .   E[X]−µX +  ∂Z ∂X  ∂Z ∂Y   E[Y ]−µY   = Z µX , µY  ,  By analogy with  30.65 , the variance of Z = Z X, Y   is given by   30.66    cid:21  cid:21    cid:21   V [Z] =   z − µZ  2p z  dz =  [Z x, y  − µZ ]2f x, y  dx dy,   30.67   where µZ = E[Z]. We may use this expression to derive a second useful result. If X and Y are two independent random variables, so that f x, y  = g x h y , and a, b and c are constants then by setting Z = aX + bY + c in  30.67  we obtain  V [aX + bY + c] = a2V [X] + b2V [Y ].   30.68   From  30.68  we also obtain the important special case  V [X + Y ] = V [X − Y ] = V [X] + V [Y ].  Provided X and Y are indeed independent random variables, we may obtain an approximate expression for V [ Z X, Y  ], for any arbitrary function Z X, Y  , in a similar manner to that used in approximating E[ Z X, Y  ] above. Taking the  1156   variance of both sides of  30.66 , and using  30.68 , we ﬁnd  30.7 GENERATING FUNCTIONS   cid:7    cid:8   2  ∂Z ∂X   cid:7    cid:8   2  ∂Z ∂Y  V [ Z X, Y  ] ≈  the partial derivatives being evaluated at X = µX and Y = µY .  V [X] +  V [Y ],   30.69   30.7 Generating functions  As we saw in chapter 16, when dealing with particular sets of functions fn, each member of the set being characterised by a diﬀerent non-negative integer n, it is sometimes possible to summarise the whole set by a single function of a dummy variable  say t , called a generating function. The relationship between the generating function and the nth member fn of the set is that if the generating function is expanded as a power series in t then fn is the coeﬃcient of tn. For example, in the expansion of the generating function G z, t  =  1 − 2zt + t2  −1 2, the coeﬃcient of tn is the nth Legendre polynomial Pn z , i.e.  G z, t  =  1 − 2zt + t2   −1 2 =  Pn z tn.  ∞ cid:4   n=0  We found that many useful properties of, and relationships between, the members of a set of functions could be established using the generating function and other functions obtained from it, e.g. its derivatives.  Similar ideas can be used in the area of probability theory, and two types of generating function can be usefully deﬁned, one more generally applicable than the other. The more restricted of the two, applicable only to discrete integral distributions, is called a probability generating function; this is discussed in the next section. The second type, a moment generating function, can be used with both discrete and continuous distributions and is considered in subsection 30.7.2. From the moment generating function, we may also construct the closely re- lated characteristic and cumulant generating functions; these are discussed in subsections 30.7.3 and 30.7.4 respectively.  30.7.1 Probability generating functions  As already indicated, probability generating functions are restricted in applicabil- ity to integer distributions, of which the most common  the binomial, the Poisson and the geometric  are considered in this and later subsections. In such distribu- tions a random variable may take only non-negative integer values. The actual possible values may be ﬁnite or inﬁnite in number, but, for formal purposes, all integers, 0, 1, 2, . . . are considered possible. If only a ﬁnite number of integer values can occur in any particular case then those that cannot occur are included but are assigned zero probability.  1157   If, as previously, the probability that the random variable X takes the value xn  is f xn , then  In the present case, however, only non-negative integer values of xn are possible, and we can, without ambiguity, write the probability that X takes the value n as fn, with  We may now deﬁne the probability generating function ΦX t  by   30.70    30.71   PROBABILITY   cid:4   n  f xn  = 1.  fn = 1.  ∞ cid:4  ΦX t  ≡ ∞ cid:4   n=0  fntn.  n=0    X =  It is immediately apparent that ΦX t  = E[tX] and that, by virtue of  30.70 , ΦX 1  = 1.  Probably the simplest example of a probability generating function  PGF  is  provided by the random variable X deﬁned by  1 if the outcome of a single trial is a ‘success’,  0 if the trial ends in ‘failure’.  If the probability of success is p and that of failure q  = 1 − p  then  ΦX t  = qt0 + pt1 + 0 + 0 + ··· = q + pt.   30.72   This type of random variable is discussed much more fully in subsection 30.8.1. In a similar but slightly more complicated way, a Poisson-distributed integer variable with mean λ  see subsection 30.8.4  has a PGF −λeλt.  ∞ cid:4   tn = e   30.73   ΦX  t  =  e  −λλn n!  n=0  We note that, as required, ΦX  1  = 1 in both cases.  Useful results will be obtained from this kind of approach only if the summation  30.71  can be carried out explicitly in particular cases and the functions derived from ΦX t  can be shown to be related to meaningful parameters. Two such relationships can be obtained by diﬀerentiating  30.71  with respect to t. Taking the ﬁrst derivative we ﬁnd  dΦX  t   =  dt  nfntn−1 ⇒ Φ  cid:7  X  1  =  nfn = E[X],   30.74   ∞ cid:4   n=0  ∞ cid:4   n=0  1158   30.7 GENERATING FUNCTIONS  and diﬀerentiating once more we obtain  ∞ cid:4   n=0  d2ΦX t   dt2 =  Equation  30.74  shows that Φ  30.51  allows us to write  X  1  − cid:18    cid:7  Φ X  1    cid:19   2   cid:7  cid:7   cid:7  X  1  + Φ Φ  ∞ cid:4   n=0  n n − 1 fntn−2 ⇒ Φ  cid:7  cid:7  X 1  =  n n − 1 fn = E[X X − 1 ].   cid:7  X  1  gives the mean of X. Using both  30.75  and   30.75    cid:18   cid:18   = E[X X − 1 ] + E[X] −  E[X] 2   cid:19  − E[X] + E[X] −  E[X] 2  cid:19  −  E[X] 2  = E  X2 X2 = V [X],  = E   30.76   and so express the variance of X in terms of the derivatives of its probability generating function.  cid:1 A random variable X is given by the number of trials needed to obtain a ﬁrst success when the chance of success at each trial is constant and equal to p. Find the probability generating function for X and use it to determine the mean and variance of X.  Clearly, at least one trial is needed, and so f0 = 0. If n  ≥ 1  trials are needed for the ﬁrst success, the ﬁrst n − 1 trials must have resulted in failure. Thus where q = 1 − p is the probability of failure in each individual trial.  n ≥ 1,   30.77   The corresponding probability generating function is thus  ΦX  t  =   qn−1p tn  Pr X = n  = qn−1p, ∞ cid:4  ∞ cid:4  ∞ cid:4   fntn =  n=1  n=0  =  p q  n=1   qt n =  × qt 1 − qt  p q  =  pt  1 − qt  ,   30.78   where we have used the result for the sum of a geometric series, given in chapter 4, to obtain a closed-form expression for ΦX  t . Again, as must be the case, ΦX  1  = 1.   cid:7  cid:7   cid:7  To ﬁnd the mean and variance of X we need to evaluate Φ X  1  and Φ X  1 . Diﬀerentiating   30.78  gives  Thus, using  30.74  and  30.76 ,   cid:7  Φ X  t  =  cid:7  cid:7  X  t  = Φ  p   1 − qt 2  1 − qt 3  2pq  ⇒ Φ  cid:7  X  1  = ⇒ Φ  cid:7  cid:7  X  1  =  =  1 p  ,  p p2 2pq p3  =  2q p2 .   cid:7  1 X  1  = E[X] = Φ , p X  1  − [Φ  cid:7   cid:7   cid:7  cid:7  X  1 ]2 X  1  + Φ V [X] = Φ − 1 2q q p2 . p2 p2  1 p  =  +  =  A distribution with probabilities of the general form  30.77  is known as a geometric distribution and is discussed in subsection 30.8.2. This form of distribution is common in ‘waiting time’ problems  subsection 30.9.3 .  cid:2   1159   PROBABILITY  n  r = n  r  Figure 30.10 The pairs of values of n and r used in the evaluation of ΦX+Y  t .  Sums of random variables  We now turn to considering the sum of two or more independent random variables, say X and Y , and denote by S2 the random variable  S2 = X + Y .  If ΦS2  t  is the PGF for S2, the coeﬃcient of tn in its expansion is given by the probability that X + Y = n and is thus equal to the sum of the probabilities that X = r and Y = n − r for all values of r in 0 ≤ r ≤ n. Since such outcomes for  diﬀerent values of r are mutually exclusive, we have  Pr X + Y = n  =  Pr X = r  Pr Y = n − r .   30.79   ∞ cid:4   r=0  Multiplying both sides of  30.79  by tn and summing over all values of n enables us to express this relationship in terms of probability generating functions as follows:  ΦX+Y  t  =  Pr X + Y = n tn =  Pr X = r tr Pr Y = n − r tn−r  ∞ cid:4   n=0  =  Pr X = r tr Pr Y = n − r tn−r.  ∞ cid:4  ∞ cid:4   n=0  n cid:4  ∞ cid:4   r=0  r=0  n=r  The change in summation order is justiﬁed by reference to ﬁgure 30.10, which illustrates that the summations are over exactly the same pairs of values of n and r, but with the ﬁrst  inner  summation over the points in a column rather than over the points in a row. Now, setting n = r + s gives the ﬁnal result,  ΦX+Y  t  =  Pr X = r tr  Pr Y = s ts  ∞ cid:4   s=0  ∞ cid:4   r=0  = ΦX t ΦY  t ,  1160   30.80    30.7 GENERATING FUNCTIONS  i.e. the PGF of the sum of two independent random variables is equal to the product of their individual PGFs. The same result can be deduced in a less formal way by noting that if X and Y are independent then   cid:18    cid:19    cid:18    cid:19    cid:18    cid:19   E  tX+Y  = E  tX  E  tY  .  Clearly result  30.80  can be extended to more than two random variables by writing S3 = S2 + Z etc., to give  n cid:3   i=1  Φ   n  i=1 Xi  t  =  ΦXi  t ,   cid:11    cid:11   Φ   n  i=1 Xi  t  = [ΦX t ]n .   30.81    30.82   and, further, if all the Xi have the same probability distribution,  This latter result has immediate application in the deduction of the PGF for the binomial distribution from that for a single trial, equation  30.72 .  Variable-length sums of random variables  As a ﬁnal result in the theory of probability generating functions we show how to calculate the PGF for a sum of N random variables, all with the same probability distribution, when the value of N is itself a random variable but one with a known probability distribution. In symbols, we wish to ﬁnd the distribution of  SN = X1 + X2 + ··· + XN,   30.83   where N is a random variable with Pr N = n  = hn and PGF χN t  =  hntn. The probability ξk that SN = k is given by a sum of conditional probabilities,   cid:11   §  namely  ∞ cid:4  ∞ cid:4   n=0  n=0  ξk =  =  Pr N = n  Pr X0 + X1 + X2 + ··· + Xn = k  hn × coeﬃcient of tk in [ΦX  t ]n.  Multiplying both sides of this equation by tk and summing over all k, we obtain  §  Formally X0 = 0 has to be included, since Pr N = 0  may be non-zero.  1161   an expression for the PGF ΞS  t  of SN:  ∞ cid:4   k=0  ΞS  t  =  ξktk =  hn × coeﬃcient of tk in [ΦX  t ]n tk × coeﬃcient of tk in [ΦX  t ]n  PROBABILITY  ∞ cid:4  ∞ cid:4   n=0  k=0  tk  hn  ∞ cid:4  ∞ cid:4  ∞ cid:4   n=0  k=0  n=0  =  =  = χN ΦX  t  .  hn[ΦX  t ]n   30.84   In words, the PGF of the sum SN is given by the compound function χN ΦX  t   obtained by substituting ΦX  t  for t in the PGF for the number of terms N in the sum. We illustrate this with the following example.  cid:1 The probability distribution for the number of eggs in a clutch is Poisson distributed with mean λ, and the probability that each egg will hatch is p  and is independent of the size of the clutch . Use the results stated in  30.72  and  30.73  to show that the PGF  and hence the probability distribution  for the number of chicks that hatch corresponds to a Poisson distribution having mean λp.  The number of chicks that hatch is given by a sum of the form  30.83  in which Xi = 1 if  the ith chick hatches and Xi = 0 if it does not. As given by  30.72 , ΦX  t  is thus  1−p +pt.  The value of N is given by a Poisson distribution with mean λ; thus, from  30.73 , in the terminology of our previous discussion,  −λeλt. We now substitute these forms into  30.84  to obtain  χN t  = e  ΞS  t  = exp −λ  exp[λΦX t ] = exp −λ  exp{λ[ 1 − p  + pt]} = exp −λp  exp λpt .  But this is exactly the PGF of a Poisson distribution with mean λp.  That this implies that the probability is Poisson distributed is intuitively obvious since, in the expansion of the PGF as a power series in t, every coeﬃcient will be precisely that implied by such a distribution. A solution of the same problem by direct calculation appears in the answer to exercise 30.29.  cid:2   30.7.2 Moment generating functions  As we saw in section 30.5 a probability function is often expressed in terms of its moments. This leads naturally to the second type of generating function, a moment generating function. For a random variable X, and a real number t, the moment generating function  MGF  is deﬁned by   cid:18    cid:19    cid:11   cid:1   MX  t  = E  etX  =  i etxi f xi  etxf x  dx for a continuous distribution.  for a discrete distribution,   30.85   1162   30.7 GENERATING FUNCTIONS  The MGF will exist for all values of t provided that X is bounded and always exists at the point t = 0 where M 0  = E 1  = 1.  It will be apparent that the PGF and the MGF for a random variable X are closely related. The former is the expectation of tX whilst the latter is the expectation of etX:   cid:18    cid:19   ΦX  t  = E  tX  ,  MX t  = E  etX  .  The MGF can thus be obtained from the PGF by replacing t by et, and vice versa. The MGF has more general applicability, however, since it can be used with both continuous and discrete distributions whilst the PGF is restricted to non-negative integer distributions.  As its name suggests, the MGF is particularly useful for obtaining the moments  of a distribution, as is easily seen by noting that   cid:18    cid:19    cid:14    cid:18    cid:19    cid:13   E  etX  = E  1 + tX +  t2X2 2!   cid:18   + ···   cid:19  t2  2!  + ··· .  = 1 + E[X]t + E  X2  Assuming that the MGF exists for all t around the point t = 0, we can deduce that the moments of a distribution are given in terms of its MGF by  E[Xn] =  dnMX t    cid:20  cid:20  cid:20  cid:20   t=0  .   cid:19    cid:6    cid:7  cid:7   dtn  X 0  − cid:18   cid:5   V [X] = M   cid:7  X 0   M  2  ,  Similarly, by substitution in  30.51 , the variance of the distribution is given by  where the prime denotes diﬀerentiation with respect to t.  cid:1 The MGF for the Gaussian distribution  see the end of subsection 30.9.1  is given by  MX t  = exp  µt + 1  2 σ2t2  .  Find the expectation and variance of this distribution.   cid:5   cid:18    cid:6    cid:5    cid:19    cid:6    cid:5   µ + σ2t  exp  µt + 1  2 σ2t2  σ2 +  µ + σ2t 2  exp  µt + 1  2 σ2t2   cid:6   Using  30.86 ,   cid:7  X  t  =  cid:7  cid:7  X  t  =  M  M  Thus, using  30.87 ,  V [X] = σ2 + µ2 − µ2 = σ2.  ⇒ E[X] = M ⇒ M   cid:7  cid:7  X  0  = σ2 + µ2.   cid:7  X  0  = µ,  That the mean is found to be µ and the variance σ2 justiﬁes the use of these symbols in the Gaussian distribution.  cid:2   The moment generating function has several useful properties that follow from  its deﬁnition and can be employed in simplifying calculations.  1163   30.86    30.87    PROBABILITY  If Y = aX + b, where a and b are arbitrary constants, then  Scaling and shifting   cid:18    cid:19    cid:18    cid:19    cid:19   MY  t  = E  etY  = E  et aX+b   = ebtE  eatX  = ebtMX at .   30.88   This result is often useful for obtaining the central moments of a distribution. If the  MFG of X is MX  t  then the variable Y = X−µ has the MGF MY  t  = e  −µtMX  t ,  which clearly generates the central moments of X, i.e.   cid:18    cid:7   E[ X − µ n] = E[Y n] = M n   Y  0  =   cid:8  −µtMX t ]  dn dtn  [e  If X1, X2, . . . , XN are independent random variables and SN = X1 + X2 +··· + XN  Sums of random variables  then  MSN  t  = E  etSN  = E  = E  etXi  .   cid:18    cid:19    cid:18    cid:19    cid:16   N cid:3   i=1  et X1+X2+···+XN   N cid:3   cid:18    cid:19   E  etXi  =  N cid:3   i=1  MXi  t .  i=1  MSN  t  =   30.89   Since the Xi are independent,  In words, the MGF of the sum of N independent random variables is the product of their individual MGFs. By combining  30.89  with  30.88 , we obtain the more  general result that the MGF of SN = c1X1 + c2X2 + ··· + cNXN  where the ci are  constants  is given by  .  t=0   cid:17   MSN  t  =  MXi  cit .   30.90   Variable-length sums of random variables  Let us consider the sum of N independent random variables Xi  i = 1, 2, . . . , N , all with the same probability distribution, and let us suppose that N is itself a random variable with a known distribution. Following the notation of section 30.7.1,  SN = X1 + X2 + ··· + XN,   cid:11   where N is a random variable with Pr N = n  = hn and probability generating hntn. For deﬁniteness, let us assume that the Xi are continuous function χN t  = RVs  an analogous discussion can be given in the discrete case . Thus, the  N cid:3   i=1  1164   30.7 GENERATING FUNCTIONS  ∞ cid:4   §  probability that value of SN lies in the interval s to s + ds is given by  sk  n=0   cid:21   µk =  ∞ cid:4   skfN s  ds =  Pr N = n fn s  ds  as fn s  ds. The kth moment of the PDF fN s  is given by  Pr N = n  Pr s < X0 + X1 + X2 ··· + Xn ≤ s + ds . Pr s < SN ≤ s + ds  = Write Pr s < SN ≤ s + ds  as fN s  ds and Pr s < X0 + X1 + X2 ··· + Xn ≤ s + ds   cid:21  ∞ cid:4  ∞ cid:4  hn ×  k!× coeﬃcient of tk in [MX  t ]n  ∞ cid:4  ∞ cid:4  ∞ cid:4   tk × coeﬃcient of tk in [MX  t ]n  Thus the MGF of SN is given by  ∞ cid:4   skfn s  ds  MSN  t  =  Pr N = n   µk k!   cid:21   tk =  k=0  n=0  n=0  k=0  n=0  n=0  hn  =  =  =  hn[MX  t ]n  n=0  = χN MX  t  .  Uniqueness  In words, the MGF of the sum SN is given by the compound function χN MX t   obtained by substituting MX t  for t in the PGF for the number of terms N in the sum.  If the MGF of the random variable X1 is identical to that for X2 then the probability distributions of X1 and X2 are identical. This is intuitively reasonable although a rigorous proof is complicated,  and beyond the scope of this book.  ¶  30.7.3 Characteristic function   cid:18    cid:19    cid:11   cid:1   The characteristic function  CF  of a random variable X is deﬁned as  CX t  = E  eitX  =  j eitxj f xj  eitxf x  dx  for a discrete distribution,  for a continuous distribution   30.91   §  ¶  As in the previous section, X0 has to be formally included, since Pr N = 0  may be non-zero.  See, for example, P. A. Moran, An Introduction to Probability Theory  New York: Oxford Science Publications, 1984 .  1165   PROBABILITY  so that CX t  = MX  it , where MX  t  is the MGF of X. Clearly, the characteristic function and the MGF are very closely related and can be used interchangeably. Because of the formal similarity between the deﬁnitions of CX t  and MX t , the characteristic function possesses analogous properties to those listed in the previ- ous section for the MGF, with only minor modiﬁcations. Indeed, by substituting it for t in any of the relations obeyed by the MGF and noting that CX t  = MX it , we obtain the corresponding relationship for the characteristic function. Thus, for example, the moments of X are given in terms of the derivatives of CX t  by  E[Xn] =  −i nC  n  Similarly, if Y = aX + b then CY  t  = eibtCX  at .  X  0 .  Whether to describe a random variable by its characteristic function or by its MGF is partly a matter of personal preference. However, the use of the CF does have some advantages. Most importantly, the replacement of the exponential etX in the deﬁnition of the MGF by the complex oscillatory function eitX in the CF means that in the latter we avoid any diﬃculties associated with convergence of the relevant sum or integral. Furthermore, when X is a continous RV, we see from  30.91  that CX t  is related to the Fourier transform of the PDF f x . As a consequence of Fourier’s inversion theorem, we may obtain f x  from CX  t  by performing the inverse transform   cid:21  ∞  f x  =  1 2π  −∞ CX t e  −itx dt.  30.7.4 Cumulant generating function  As mentioned at the end of subsection 30.5.5, we may also describe a probability density function f x  in terms of its cumulants. These quantities may be expressed in terms of the moments of the distribution and are important in sampling theory, which we discuss in the next chapter. The cumulants of a distribution are best deﬁned in terms of its cumulant generating function  CGF , given by KX t  = ln MX t  where MX t  is the MGF of the distribution. If KX t  is expanded as a power series in t then the kth cumulant κk of f x  is the coeﬃcient of tk k!:  KX t  = ln MX t  ≡ κ1t + κ2  t2 2!  + κ3  + ··· .  t3 3!  Since MX 0  = 1, KX t  contains no constant term.  cid:1 Find all the cumulants of the Gaussian distribution discussed in the previous example.   30.92    cid:5    cid:6   µt + 1  2 σ2t2  .  The moment generating function for the Gaussian distribution is MX  t  = exp Thus, the cumulant generating function has the simple form 2 σ2t2.  KX  t  = ln MX  t  = µt + 1  1166   30.7 GENERATING FUNCTIONS  Comparing this expression with  30.92 , we ﬁnd that κ1 = µ, κ2 = σ2 and all other cumulants are equal to zero.  cid:2   We may obtain expressions for the cumulants of a distribution in terms of its  moments by diﬀerentiating  30.92  with respect to t to give   cid:7   dKX dt  =  1 MX  dMX  dt  .   cid:8    cid:7    cid:8  cid:7   Expanding each term as power series in t and cross-multiplying, we obtain + ···  + ···  + ···  κ1 + κ2t + κ3  µ1 + µ2t + µ3  1 + µ1t + µ2  =  t2 2!  t2 2!   cid:8   ,  and, on equating coeﬃcients of like powers of t on each side, we ﬁnd  t2 2!  ...  µ1 = κ1,  µ2 = κ2 + κ1µ1,  µ3 = κ3 + 2κ2µ1 + κ1µ2,  µ4 = κ4 + 3κ3µ1 + 3κ2µ2 + κ1µ3,  µk = κk + k−1C1κk−1µ1 + ··· + k−1Crκk−rµr + ··· + κ1µk−1.  Solving these equations for the κk, we obtain  for the ﬁrst four cumulants   κ1 = µ1,  1 = ν2,  κ2 = µ2 − µ2 κ3 = µ3 − 3µ2µ1 + 2µ3 1 = ν3, κ4 = µ4 − 4µ3µ1 + 12µ2µ2  1  − 3µ2  2  − 6µ4  1 = ν4 − 3ν2  2 .   30.93   Higher-order cumulants may be calculated in the same way but become increas- ingly lengthy to write out in full.  The principal property of cumulants is their additivity, which may be proved . . . , XN are independent random by combining  30.92  with  30.90 . If X1, X2, variables and KXi  t  for i = 1, 2, . . . , N is the CGF for Xi then the CGF of SN = c1X1 + c2X2 + ··· + cNXN  where the ci are constants  is given by  KSN  t  =  KXi  cit .  Cumulants also have the useful property that, under a change of origin X → X + a the ﬁrst cumulant undergoes the change κ1 → κ1 + a but all higher-order cumulants remain unchanged. Under a change of scale X → bX, cumulant κr undergoes the change κr → brκr.  N cid:4   i=1  1167   PROBABILITY  MGF  E[X]  V [X]  Distribution  binomial  negative binomial  geometric  hypergeometric  Poisson  Probability law f x  nCxpxqn−x r+x−1Cxprqx qx−1p   Np ! Nq !n! N−n !  x! Np−x ! n−x ! Nq−n+x !N! λx x!  −λ e   cid:7    cid:8   r   pet + q n  p  1 − qet 1 − qet  pet  eλ et−1   np  rq p 1 p  np  λ  npq  rq p2 q p2  λ  N − n N − 1  npq  Table 30.1 Some important discrete probability distributions.  30.8 Important discrete distributions  Having discussed some general properties of distributions, we now consider the more important discrete distributions encountered in physical applications. These are discussed in detail below, and summarised for convenience in table 30.1; we refer the reader to the relevant section below for an explanation of the symbols used.  30.8.1 The binomial distribution  Perhaps the most important discrete probability distribution is the binomial dis- tribution. This distribution describes processes that consist of a number of inde- pendent identical trials with two possible outcomes, A and B = ¯A. We may call these outcomes ‘success’ and ‘failure’ respectively. If the probability of a success is Pr A  = p then the probability of a failure is Pr B  = q = 1 − p. If we perform  n trials then the discrete random variable  X = number of times A occurs  can take the values 0, 1, 2, . . . , n; its distribution amongst these values is described by the binomial distribution.  We now calculate the probability that in n trials we obtain x successes  and so  n− x failures . One way of obtaining such a result is to have x successes followed by n−x failures. Since the trials are assumed independent, the probability of this is  This is, however, just one permutation of x successes and n− x failures. The total  8 9: ; pp··· p x times  8 9: ; × qq ··· q n − x times  = pxqn−x.  1168   30.8 IMPORTANT DISCRETE DISTRIBUTIONS  f x   f x   n = 5, p = 0.6  n = 5, p = 0.167  0  1  2  3  4  5  x  0  1  2  3  4  5  x  f x   f x   n = 10, p = 0.6  n = 10, p = 0.167  0.4  0.3  0.2  0.1  0  0.4  0.3  0.2  0.1  0  0.4  0.3  0.2  0.1  0  0.4  0.3  0.2  0.1  0  0  1  2  3  4  5  6  7 8  9  10  x  0  1  2  3  4  5  6  7  8  9 10  x  Figure 30.11 Some typical binomial distributions with various combinations of parameters n and p.  number of permutations of n objects, of which x are identical and of type 1 and  n − x are identical and of type 2, is given by  30.33  as  n!  x! n − x !  ≡ nCx.  Therefore, the total probability of obtaining x successes from n trials is  f x  = Pr X = x  = nCx pxqn−x = nCx px 1 − p n−x,   30.94   which is the binomial probability distribution formula. When a random variable X follows the binomial distribution for n trials, with a probability of success p,  we write X ∼ Bin n, p . Then the random variable X is often referred to as a  binomial variate. Some typical binomial distributions are shown in ﬁgure 30.11.  cid:1 If a single six-sided die is rolled ﬁve times, what is the probability that a six is thrown exactly three times?  Here the number of ‘trials’ n = 5, and we are interested in the random variable  Since the probability of a ‘success’ is p = 1 in ﬁve throws is given by  30.94  as  6 , the probability of obtaining exactly three sixes  X = number of sixes thrown.   cid:7    cid:8   3   cid:7    cid:8    5−3   Pr X = 3  =  5!  3! 5 − 3 !  1 6  5 6  = 0.032.  cid:2   1169   PROBABILITY   cid:8    cid:7   n − x  x + 1  p q  For evaluating binomial probabilities a useful result is the binomial recurrence  formula  Pr X = x + 1  =  Pr X = x ,   30.95   which enables successive probabilities Pr X = x + k , k = 1, 2, . . . , to be calculated once Pr X = x  is known; it is often quicker to use than  30.94 .  cid:1 The random variable X is distributed as X ∼ Bin 3, 1  cid:6   cid:5   The probability Pr X = 0  may be calculated using  30.94  and is  2  . Evaluate the probability function  f x  using the binomial recurrence formula.   cid:6    cid:5   Pr X = 0  = 3C0  1 2  0  3  1 2  = 1 8 .  The ratio p q = 1  30.95 , we ﬁnd  2   1  2 = 1 in this case and so, using the binomial recurrence formula  0 + 1  Pr X = 1  = 1 × 3 − 0 Pr X = 2  = 1 × 3 − 1 Pr X = 3  = 1 × 3 − 2  1 + 1  2 + 1  × 1 8 × 3 8 × 3 8  =  =  =  3 8  3 8  1 8  ,  ,  ,  results which may be veriﬁed by direct application of  30.94 .  cid:2   We note that, as required, the binomial distribution satiﬁes  n cid:4   x=0  n cid:4   x=0  f x  =  nCx pxqn−x =  p + q n = 1.  Furthermore, from the deﬁnitions of E[X] and V [X] for a discrete distribution, we may show that for the binomial distribution E[X] = np and V [X] = npq. The direct summations involved are, however, rather cumbersome and these results are obtained much more simply using the moment generating function.  The moment generating function for the binomial distribution  To ﬁnd the MGF for the binomial distribution we consider the binomial random variable X to be the sum of the random variables Xi, i = 1, 2, . . . , n, which are deﬁned by    Xi =  1 if a ‘success’ occurs on the ith trial,  0 if a ‘failure’ occurs on the ith trial.  1170   30.8 IMPORTANT DISCRETE DISTRIBUTIONS  Thus   cid:18    cid:19   Mi t  = E  etXi  = e0t × Pr Xi = 0  + e1t × Pr Xi = 1  = 1 × q + et × p = pet + q. n cid:3   From  30.89 , it follows that the MGF for the binomial distribution is given by  M t  =  Mi t  =  pet + q n.   30.96   i=1  We can now use the moment generating function to derive the mean and  variance of the binomial distribution. From  30.96    cid:7    t  = npet pet + q n−1,  M  and from  30.86   E[X] = M   cid:7    0  = np p + q n−1 = np,  where the last equality follows from p + q = 1.  Diﬀerentiating with respect to t once more gives   cid:7  cid:7    t  = et n − 1 np2 pet + q n−2 + etnp pet + q n−1,  M  E[X2] = M   cid:7  cid:7    0  = n2p2 − np2 + np.  and from  30.86   Thus, using  30.87   V [X] = M   0  − cid:18    cid:7  cid:7    cid:19    cid:7   M   0   2  = n2p2 − np2 + np − n2p2 = np 1 − p  = npq.  Multiple binomial distributions  Suppose X and Y are two independent random variables, both of which are described by binomial distributions with a common probability of success p, but  with  in general  diﬀerent numbers of trials n1 and n2, so that X ∼ Bin n1, p  and Y ∼ Bin n2, p . Now consider the random variable Z = X + Y . We could  calculate the probability distribution of Z directly using  30.60 , but it is much easier to use the MGF  30.96 .  Since X and Y are independent random variables, the MGF MZ  t  of the new variable Z = X + Y is given simply by the product of the individual MGFs MX  t  and MY  t . Thus, we obtain  MZ  t  = MX t MY  t  =  pet + q n1  pet + q n1 =  pet + q n1+n2 ,  which we recognise as the MGF of Z ∼ Bin n1 + n2, p . Hence Z is also described  by a binomial distribution.  This result may be extended to any number of binomial distributions. If Xi,  1171   PROBABILITY  i = 1, 2, . . . , N, is distributed as Xi ∼ Bin ni, p  then Z = X1 + X2 + ··· + XN is  cid:11  distributed as Z ∼ Bin n1 + n2 +··· + nN, p , as would be expected since the result i ni trials cannot depend on how they are split up. A similar proof is also  of possible using either the probability or cumulant generating functions.  Unfortunately, no equivalent simple result exists for the probability distribution  of the diﬀerence Z = X − Y of two binomially distributed variables.  30.8.2 The geometric and negative binomial distributions  A special case of the binomial distribution occurs when instead of the number of successes we consider the discrete random variable  X = number of trials required to obtain the ﬁrst success.  The probability that x trials are required in order to obtain the ﬁrst success, is  simply the probability of obtaining x − 1 failures followed by one success. If the  probability of a success on each trial is p, then for x > 0  f x  = Pr X = x  =  1 − p x−1p = qx−1p,  where q = 1 − p. This distribution is sometimes called the geometric distribution.  The probability generating function for this distribution is given in  30.78 . By replacing t by et in  30.78  we immediately obtain the MGF of the geometric distribution  from which its mean and variance are found to be  M t  =  pet 1 − qet ,  E[X] =  V [X] =  1 p  ,  q p2 .  Another distribution closely related to the binomial is the negative binomial distribution. This describes the probability distribution of the random variable  X = number of failures before the rth success.  One way of obtaining x failures before the rth success is to have r − 1 successes  followed by x failures followed by the rth success, for which the probability is  8 9: ; pp··· p r − 1 times  8 9: ; × qq ··· q  x times  × p = prqx.  However, the ﬁrst r + x − 1 factors constitute just one permutation of r − 1 successes and x failures. The total number of permutations of these r + x − 1 objects, of which r − 1 are identical and of type 1 and x are identical and of type  1172   30.8 IMPORTANT DISCRETE DISTRIBUTIONS  2, is r+x−1Cx. Therefore, the total probability of obtaining x failures before the  rth success is  f x  = Pr X = x  = r+x−1Cxprqx,  which is called the negative binomial distribution  see the related discussion on p. 1137 . It is straightforward to show that the MGF of this distribution is   cid:7    cid:8   M t  =  p  1 − qet  r  ,  and that its mean and variance are given by  E[X] =  and  V [X] =  rq p  rq p2 .  30.8.3 The hypergeometric distribution  In subsection 30.8.1 we saw that the probability of obtaining x successes in n independent trials was given by the binomial distribution. Suppose that these n ‘trials’ actually consist of drawing at random n balls, from a set of N such balls of which M are red and the rest white. Let us consider the random variable X = number of red balls drawn.  On the one hand, if the balls are drawn with replacement then the trials are independent and the probability of drawing a red ball is p = M N each time. Therefore, the probability of drawing x red balls in n trials is given by the binomial distribution as  Pr X = x  =  n!  x! n − x !  px 1 − p n−x.  On the other hand, if the balls are drawn without replacement the trials are not independent and the probability of drawing a red ball depends on how many red balls have already been drawn. We can, however, still derive a general formula for the probability of drawing x red balls in n trials, as follows.  The number of ways of drawing x red balls from M is M Cx, and the number of ways of drawing n − x white balls from N − M is N−M Cn−x. Therefore, the N−MCn−x. However, total number of ways to obtain x red balls in n trials is M Cx the total number of ways of drawing n objects from N is simply NCn. Hence the probability of obtaining x red balls in n trials is  Pr X = x  =  M Cx  N−MCn−x N Cn M!  n − x ! N − M − n + x ! x! M − x ! x! Np − x ! n − x ! Nq − n + x ! N!   N − M !  Np ! Nq ! n! N − n !  ,  =  =  n! N − n !  N!  ,   30.97    30.98   1173   PROBABILITY  where in the last line p = M N and q = 1 − p. This is called the hypergeometric  distribution.  By performing the relevant summations directly, it may be shown that the  hypergeometric distribution has mean  E[X] = n  = np  M N  and variance  V [X] =  nM N − M  N − n   N2 N − 1   N − n N − 1  =  npq.   cid:1 In the UK National Lottery each participant chooses six diﬀerent numbers between 1 and 49. In each weekly draw six numbered winning balls are subsequently drawn. Find the probabilities that a participant chooses 0, 1, 2, 3, 4, 5, 6 winning numbers correctly.  The probabilities are given by a hypergeometric distribution with N  the total number of balls  = 49, M  the number of winning balls drawn  = 6, and n  the number of numbers chosen by each participant  = 6. Thus, substituting in  30.97 , we ﬁnd  Pr 0  =  Pr 2  =  Pr 4  =  6C0  43C6  49C6  6C2  43C4  49C6  6C4  43C2  49C6  =  =  =  ,  ,  2.29  1  1  7.55 1  1032  Pr 1  =  Pr 3  =  , Pr 5  =  6C1  43C5  49C6  6C3  43C3  49C6  6C5  43C1  49C6  =  =  =  2.42  1  1  ,  ,  56.6 1  ,  54 200  Pr 6  =  6C6  43C0  49C6  =  1  13.98 × 106 .  Pr i  = 0.44 + 0.41 + 0.13 + 0.02 + O 10  −3  = 1,  It can easily be seen that  6 cid:4   i=0  as expected.  cid:2   Note that if the number of trials  balls drawn  is small compared with N, M  and N − M then not replacing the balls is of little consequence, and we may  approximate the hypergeometric distribution by the binomial distribution  with p = M N ; this is much easier to evaluate.  30.8.4 The Poisson distribution  We have seen that the binomial distribution describes the number of successful outcomes in a certain number of trials n. The Poisson distribution also describes the probability of obtaining a given number of successes but for situations in which the number of ‘trials’ cannot be enumerated; rather it describes the situation in which discrete events occur in a continuum. Typical examples of  1174   30.8 IMPORTANT DISCRETE DISTRIBUTIONS  discrete random variables X described by a Poisson distribution are the number of telephone calls received by a switchboard in a given interval, or the number of stars above a certain brightness in a particular area of the sky. Given a mean rate of occurrence λ of these events in the relevant interval or area, the Poisson distribution gives the probability Pr X = x  that exactly x events will occur.  We may derive the form of the Poisson distribution as the limit of the binomial  distribution when the number of trials n → ∞ and the probability of ‘success’ p → 0, in such a way that np = λ remains ﬁnite. Thus, in our example of a  telephone switchboard, suppose we wish to ﬁnd the probability that exactly x calls are received during some time interval, given that the mean number of calls in such an interval is λ. Let us begin by dividing the time interval into a large number, n, of equal shorter intervals, in each of which the probability of receiving  a call is p. As we let n → ∞ then p → 0, but since we require the mean number  of calls in the interval to equal λ, we must have np = λ. The probability of x successes in n trials is given by the binomial formula as  Pr X = x  =  n!  x! n − x !  px 1 − p n−x.   30.99   Now as n → ∞, with x ﬁnite, the ratio of the n-dependent factorials in  30.99   behaves asymptotically as a power of n, i.e.  lim n→∞  n!   n − x !  n→∞ n n − 1  n − 2 ···  n − x + 1  ∼ nx.  = lim  Also   1 − p n−x = lim p→0  n→∞ lim lim p→0   1 − p λ p  1 − p x  −λ  e  1  .  =  Thus, using λ = np,  30.99  tends to the Poisson distribution  f x  = Pr X = x  =   30.100   e  −λλx x!  ,  which gives the probability of obtaining exactly x calls in the given time interval. As we shall show below, λ is the mean of the distribution. Events following a Poisson distribution are usually said to occur randomly in time.  Alternatively we may derive the Poisson distribution directly, without consid- ering a limit of the binomial distribution. Let us again consider our example of a telephone switchboard. Suppose that the probability that x calls have been received in a time interval t is Px t . If the average number of calls received in a unit time is λ then in a further small time interval ∆t the probability of receiving a call is λ∆t, provided ∆t is short enough that the probability of receiving two or more calls in this small interval is negligible. Similarly the probability of receiving  no call during the same small interval is simply 1 − λ∆t.  Thus, for x > 0, the probability of receiving exactly x calls in the total interval  1175   PROBABILITY  t + ∆t is given by  Px t + ∆t  = Px t  1 − λ∆t  + Px−1 t λ∆t.  Rearranging the equation, dividing through by ∆t and letting ∆t → 0, we obtain  the diﬀerential recurrence equation  = λPx−1 t  − λPx t .  dPx t   dt   30.101   For x = 0  i.e. no calls received , however,  30.101  simpliﬁes to  = −λP0 t ,  dP0 t   dt  −λt. But since the probability P0 0  which may be integrated to give P0 t  = P0 0 e −λt. of receiving no calls in a zero time interval must equal unity, we have P0 t  = e This expression for P0 t  may then be substituted back into  30.101  with x = 1 −λt. to obtain a diﬀerential equation for P1 t  that has the solution P1 t  = λte We may repeat this process to obtain expressions for P2 t , P3 t , . . . , Px t , and we ﬁnd  Px t  =  −λt.  e   λt x x!   30.102   By setting t = 1 in  30.102 , we again obtain the Poisson distribution  30.100  for obtaining exactly x calls in a unit time interval.  If a discrete random variable is described by a Poisson distribution of mean λ  then we write X ∼ Po λ . As it must be, the sum of the probabilities is unity:  ∞ cid:4   x=0  ∞ cid:4   x=0  −λ  Pr X = x  = e  −λeλ = 1.  = e  λx x!  From  30.100  we may also derive the Poisson recurrence formula,  Pr X = x + 1  =  Pr X = x   for x = 0, 1, 2, . . . ,  λ  x + 1   30.103   which enables successive probabilities to be calculated easily once one is known.  cid:1 A person receives on average one e-mail message per half-hour interval. Assuming that the e-mails are received randomly in time, ﬁnd the probabilities that in any particular hour 0, 1, 2, 3, 4, 5 messages are received.  Let X = number of e-mails received per hour. Clearly the mean number of e-mails per hour is two, and so X follows a Poisson distribution with λ = 2, i.e.  2x Pr X = x  = x! −2 = 0.135, Pr X = 1  = 2e −2 = 0.271, Pr X = 2  = 22e −2 3! = 0.180, Pr X = 4  = 24e −2 4! = 0.090, Pr X = 5  = 25e  Thus Pr X = 0  = e Pr X = 3  = 23e 0.036. These results may also be calculated using the recurrence formula  30.103 .  cid:2   −2 2! = 0.271, −2 5! =  e  −2.  1176   30.8 IMPORTANT DISCRETE DISTRIBUTIONS  λ = 1  λ = 2  f x   0.3  0.2  0.1  0  λ = 5  f x   0.3  0.2  0.1  0  f x   0.3  0.2  0.1  0  0  1  2  3  4  5  x  0  1  2  3  4  5  6  7  x  0  1  2  3  4  5  6  7 8 9 10 11  x  Figure 30.12 Three Poisson distributions for diﬀerent values of the parame- ter λ.  The above example illustrates the point that a Poisson distribution typically rises and then falls. It either has a maximum when x is equal to the integer part  of λ or, if λ happens to be an integer, has equal maximal values at x = λ− 1 and  x = λ. The Poisson distribution always has a long ‘tail’ towards higher values of X but the higher the value of the mean the more symmetric the distribution becomes. Typical Poisson distributions are shown in ﬁgure 30.12. Using the deﬁnitions of mean and variance, we may show that, for the Poisson distribution, E[X] = λ and V [X] = λ. Nevertheless, as in the case of the binomial distribution, performing the relevant summations directly is rather tiresome, and these results are much more easily proved using the MGF.  The moment generating function for the Poisson distribution  The MGF of the Poisson distribution is given by   cid:18    cid:19   ∞ cid:4   x=0  MX t  = E  etX  =  ∞ cid:4   x=0  etxe  −λλx x!  −λ  = e  1177  −λeλet  = eλ et−1   = e   λet x  x!   30.104    from which we obtain  PROBABILITY  X t  = λeteλ et−1 ,  cid:7   cid:7  cid:7  X t  =  λ2e2t + λet eλ et−1 .  M  M  Thus, the mean and variance of the Poisson distribution are given by  E[X] = M   cid:7  X 0  = λ  and  V [X] = M  X 0  − [M  cid:7  cid:7    cid:7  X  0 ]2 = λ.  The Poisson approximation to the binomial distribution  Earlier we derived the Poisson distribution as the limit of the binomial distribution  when n → ∞ and p → 0 in such a way that np = λ remains ﬁnite, where λ is the  mean of the Poisson distribution. It is not surprising, therefore, that the Poisson distribution is a very good approximation to the binomial distribution for large  n  ≥ 50, say  and small p  ≤ 0.1, say . Moreover, it is easier to calculate as it  involves fewer factorials.  cid:1 In a large batch of light bulbs, the probability that a bulb is defective is 0.5%. For a sample of 200 bulbs taken at random, ﬁnd the approximate probabilities that 0, 1 and 2 of the bulbs respectively are defective.  Let the random variable X = number of defective bulbs in a sample. This is distributed  as X ∼ Bin 200, 0.005 , implying that λ = np = 1.0. Since n is large and p small, we may approximate the distribution as X ∼ Po 1 , giving Pr X = x  ≈ e  from which we ﬁnd Pr X = 0  ≈ 0.37, Pr X = 1  ≈ 0.37, Pr X = 2  ≈ 0.18. For comparison, it may be noted that the exact values calculated from the binomial distribution are identical to those found here to two decimal places.  cid:2   −1 1x x!  ,  Multiple Poisson distributions  Mirroring our discussion of multiple binomial distributions in subsection 30.8.1, let us suppose X and Y are two independent random variables, both of which are described by Poisson distributions with  in general  diﬀerent means, so that  X ∼ Po λ1  and Y ∼ Po λ2 . Now consider the random variable Z = X + Y . We  may calculate the probability distribution of Z directly using  30.60 , but we may derive the result much more easily by using the moment generating function  or indeed the probability or cumulant generating functions .  Since X and Y are independent RVs, the MGF for Z is simply the product of  the individual MGFs for X and Y . Thus, from  30.104 ,  MZ  t  = MX  t MY  t  = eλ1 et−1 eλ2 et−1  = e λ1+λ2  et−1 ,  which we recognise as the MGF of Z ∼ Po λ1 + λ2 . Hence Z is also Poisson the diﬀerence Z = X − Y of two independent Poisson variates. A closed-form  distributed and has mean λ1 + λ2. Unfortunately, no such simple result holds for  1178   30.9 IMPORTANT CONTINUOUS DISTRIBUTIONS  expression for the PDF of this Z does exist, but it is a rather complicated § combination of exponentials and a modiﬁed Bessel function.   cid:1 Two types of e-mail arrive independently and at random: external e-mails at a mean rate of one every ﬁve minutes and internal e-mails at a rate of two every ﬁve minutes. Calculate the probability of receiving two or more e-mails in any two-minute interval.  Let  X = number of external e-mails per two-minute interval, Y = number of internal e-mails per two-minute interval.  Since we expect on average one external e-mail and two internal e-mails every ﬁve minutes  we have X ∼ Po 0.4  and Y ∼ Po 0.8 . Letting Z = X + Y we have Z ∼ Po 0.4 + 0.8  =  Po 1.2 . Now  Pr Z ≥ 2  = 1 − Pr Z < 2  = 1 − Pr Z = 0  − Pr Z = 1   and  −1.2 = 0.301, −1.2 1.2 1 Hence Pr Z ≥ 2  = 1 − 0.301 − 0.361 = 0.338.  cid:2   Pr Z = 1  = e  Pr Z = 0  = e  = 0.361.  The above result can be extended, of course, to any number of Poisson processes, so that if Xi = Po λi , i = 1, 2, . . . , n then the random variable Z = X1 + X2 +  ··· + Xn is distributed as Z ∼ Po λ1 + λ2 + ··· + λn .  30.9 Important continuous distributions  Having discussed the most commonly encountered discrete probability distri- butions, we now consider some of the more important continuous probability distributions. These are summarised for convenience in table 30.2; we refer the reader to the relevant subsection below for an explanation of the symbols used.  30.9.1 The Gaussian distribution  By far the most important continuous probability distribution is the Gaussian or normal distribution. The reason for its importance is that a great many random variables of interest, in all areas of the physical sciences and beyond, are described either exactly or approximately by a Gaussian distribution. Moreover, the Gaussian distribution can be used to approximate other, more complicated, probability distributions.  §  For a derivation see, for example, M. P. Hobson and A. N. Lasenby, Monthly Notices of the Royal Astronomical Society, 298, 905  1998 .  1179   PROBABILITY  Distribution  Gaussian  exponential  λe  gamma  chi-squared  uniform   cid:14    cid:13   −  x − µ 2  Probability law f x  √ 1 2π −λx  2σ2  exp  σ   λx r−1e  −λx  λ  Γ r   1  x n 2 −1e  −x 2  2n 2Γ n 2   1  b − a  exp µt + 1  2 σ2t2    cid:8   cid:8   cid:8   r  MGF  λ  λ   cid:7   cid:7  λ − t  cid:7  λ − t 1 − 2t ebt − eat  b − a t  1  n 2  µ  1 λ  r λ  n  E[X]  V [X]  σ2  1 λ2 r λ2  2n  a + b  2   b − a 2  12  Table 30.2 Some important continuous probability distributions.  The probability density function for a Gaussian distribution of a random  variable X, with mean E[X] = µ and variance V [X] = σ2, takes the form  f x  =  exp   30.105   √ The factor 1   2π arises from the normalisation of the distribution,   cid:13    cid:9   − 1 2  x − µ  σ   cid:14    cid:10 2  .  √ 1  σ  2π   cid:21  ∞  −∞ f x dx = 1;  the evaluation of this integral is discussed in subsection 6.4.2. The Gaussian distribution is symmetric about the point x = µ and has the characteristic ‘bell’ shape shown in ﬁgure 30.13. The width of the curve is described by the standard deviation σ: if σ is large then the curve is broad, and if σ is small then the curve  −1 2 ≈ 0.61 of its peak is narrow  see the ﬁgure . At x = µ ± σ, f x  falls to e value; these points are points of inﬂection, where d2f dx2 = 0. When a random variable X follows a Gaussian distribution with mean µ and variance σ2, we write X ∼ N µ, σ2 .  The eﬀects of changing µ and σ are only to shift the curve along the x-axis or to broaden or narrow it, respectively. Thus all Gaussians are equivalent in that a change of origin and scale can reduce them to a standard form. We therefore  consider the random variable Z =  X − µ  σ, for which the PDF takes the form   cid:8    cid:7  − z2 2  ,  φ z  =  exp  1√ 2π   30.106   which is called the standard Gaussian distribution and has mean µ = 0 and variance σ2 = 1. The random variable Z is called the standard variable.  From  30.105  we can deﬁne the cumulative probability function for a Gaussian  1180   30.9 IMPORTANT CONTINUOUS DISTRIBUTIONS  µ = 3  σ = 1  σ = 2  0.4  0.3  0.2  0.1  σ = 3  −6 −4 −2  2 3 4  6  8  10  12  Figure 30.13 The Gaussian or normal distribution for mean µ = 3 and various values of the standard deviation σ.  φ z   0.4  0.3  0.2  0.1  Φ a   Φ z  1  Φ a   0.8  0.6  0.4  0.2  z 4   cid:21   −4  −2  0  a  2  −2  −1  z  a  y  2  Figure 30.14 On the left, the standard Gaussian distribution φ z ; the shaded area gives Pr Z < a  = Φ a . On the right, the cumulative probability function Φ z  for a standard Gaussian distribution φ z .  distribution as  F x  = Pr X < x  =  √ 1 2π  σ  x  −∞ exp   cid:9    cid:13  − 1 2  u − µ  σ   cid:14    cid:10 2  du,   30.107   where u is a  dummy  integration variable. Unfortunately, this  indeﬁnite  integral cannot be evaluated analytically. It is therefore standard practice to tabulate val- ues of the cumulative probability function for the standard Gaussian distribution  see ﬁgure 30.14 , i.e.  Φ z  = Pr Z < z  =  du.   30.108    cid:8    cid:7  − u2 2  z  −∞ exp   cid:21   1√ 2π  1181   PROBABILITY  It is usual only to tabulate Φ z  for z > 0, since it can be seen easily, from  ﬁgure 30.14 and the symmetry of the Gaussian distribution, that Φ −z  = 1−Φ z ;  see table 30.3. Using such a table it is then straightforward to evaluate the probability that Z lies in a given range of z-values. For example, for a and b constant,  Remembering that Z =  X − µ  σ and comparing  30.107  and  30.108 , we see  that  Pr Z < a  = Φ a ,  Pr Z > a  = 1 − Φ a ,  Pr a < Z ≤ b  = Φ b  − Φ a .   cid:9    cid:10   ,  x − µ  σ  F x  = Φ  and so we may also calculate the probability that the original random variable X lies in a given x-range. For example,  Pr a < X ≤ b  =   cid:14    cid:10 2   cid:21   cid:8  = F b  − F a  b − µ  √ 1  cid:7  2π  σ  b  a  = Φ  σ   cid:9    cid:13  − 1  cid:9  2  u − µ  cid:10   σ  .  a − µ  σ  − Φ  exp  du   30.109    30.110    30.111    cid:1 If X is described by a Gaussian distribution of mean µ and variance σ2, calculate the probabilities that X lies within 1σ, 2σ and 3σ of the mean.  From  30.111   and so from table 30.3  Pr µ − nσ < X ≤ µ + nσ  = Φ n  − Φ −n  = Φ n  − [1 − Φ n ],  Pr µ − σ < X ≤ µ + σ  = 2Φ 1  − 1 = 0.6826 ≈ 68.3%, Pr µ − 2σ < X ≤ µ + 2σ  = 2Φ 2  − 1 = 0.9544 ≈ 95.4%, Pr µ − 3σ < X ≤ µ + 3σ  = 2Φ 3  − 1 = 0.9974 ≈ 99.7%.  Thus we expect X to be distributed in such a way that about two thirds of the values will  lie between µ − σ and µ + σ, 95% will lie within 2σ of the mean and 99.7% will lie within  3σ of the mean. These limits are called the one-, two- and three-sigma limits respectively; it is particularly important to note that they are independent of the actual values of the mean and variance.  cid:2   There are many other ways in which the Gaussian distribution may be used.  We now illustrate some of the uses in more complicated examples.  1182   30.9 IMPORTANT CONTINUOUS DISTRIBUTIONS  Φ z  0.0 0.1 0.2 0.3 0.4  0.5 0.6 0.7 0.8 0.9  1.0 1.1 1.2 1.3 1.4  1.5 1.6 1.7 1.8 1.9  2.0 2.1 2.2 2.3 2.4  2.5 2.6 2.7 2.8 2.9  3.0 3.1 3.2 3.3 3.4  .00  .5000 .5398 .5793 .6179 .6554  .6915 .7257 .7580 .7881 .8159  .8413 .8643 .8849 .9032 .9192  .9332 .9452 .9554 .9641 .9713  .9772 .9821 .9861 .9893 .9918  .9938 .9953 .9965 .9974 .9981  .9987 .9990 .9993 .9995 .9997  .01  .5040 .5438 .5832 .6217 .6591  .6950 .7291 .7611 .7910 .8186  .8438 .8665 .8869 .9049 .9207  .9345 .9463 .9564 .9649 .9719  .9778 .9826 .9864 .9896 .9920  .9940 .9955 .9966 .9975 .9982  .9987 .9991 .9993 .9995 .9997  .02  .5080 .5478 .5871 .6255 .6628  .6985 .7324 .7642 .7939 .8212  .8461 .8686 .8888 .9066 .9222  .9357 .9474 .9573 .9656 .9726  .9783 .9830 .9868 .9898 .9922  .9941 .9956 .9967 .9976 .9982  .9987 .9991 .9994 .9995 .9997  .03  .5120 .5517 .5910 .6293 .6664  .7019 .7357 .7673 .7967 .8238  .8485 .8708 .8907 .9082 .9236  .9370 .9484 .9582 .9664 .9732  .9788 .9834 .9871 .9901 .9925  .9943 .9957 .9968 .9977 .9983  .9988 .9991 .9994 .9996 .9997  .05  .5199 .5596 .5987 .6368 .6736  .7088 .7422 .7734 .8023 .8289  .8531 .8749 .8944 .9115 .9265  .9394 .9505 .9599 .9678 .9744  .9798 .9842 .9878 .9906 .9929  .9946 .9960 .9970 .9978 .9984  .9989 .9992 .9994 .9996 .9997  .06  .5239 .5636 .6026 .6406 .6772  .7123 .7454 .7764 .8051 .8315  .8554 .8770 .8962 .9131 .9279  .9406 .9515 .9608 .9686 .9750  .9803 .9846 .9881 .9909 .9931  .9948 .9961 .9971 .9979 .9985  .9989 .9992 .9994 .9996 .9997  .07  .5279 .5675 .6064 .6443 .6808  .7157 .7486 .7794 .8078 .8340  .8577 .8790 .8980 .9147 .9292  .9418 .9525 .9616 .9693 .9756  .9808 .9850 .9884 .9911 .9932  .9949 .9962 .9972 .9979 .9985  .9989 .9992 .9995 .9996 .9997  .08  .5319 .5714 .6103 .6480 .6844  .7190 .7517 .7823 .8106 .8365  .8599 .8810 .8997 .9162 .9306  .9429 .9535 .9625 .9699 .9761  .9812 .9854 .9887 .9913 .9934  .9951 .9963 .9973 .9980 .9986  .9990 .9993 .9995 .9996 .9997  .09  .5359 .5753 .6141 .6517 .6879  .7224 .7549 .7852 .8133 .8389  .8621 .8830 .9015 .9177 .9319  .9441 .9545 .9633 .9706 .9767  .9817 .9857 .9890 .9916 .9936  .9952 .9964 .9974 .9981 .9986  .9990 .9993 .9995 .9997 .9998  Table 30.3 The cumulative probability function Φ z  for the standard Gaus- sian distribution, as given by  30.108 . The units and the ﬁrst decimal place of z are speciﬁed in the column under Φ z  and the second decimal place is speciﬁed by the column headings. Thus, for example, Φ 1.23  = 0.8907.  .04  .5160 .5557 .5948 .6331 .6700  .7054 .7389 .7704 .7995 .8264  .8508 .8729 .8925 .9099 .9251  .9382 .9495 .9591 .9671 .9738  .9793 .9838 .9875 .9904 .9927  .9945 .9959 .9969 .9977 .9984  .9988 .9992 .9994 .9996 .9997  1183   PROBABILITY   cid:1 Sawmill A produces boards whose lengths are Gaussian distributed with mean 209.4 cm and standard deviation 5.0 cm. A board is accepted if it is longer than 200 cm but is rejected otherwise. Show that 3% of boards are rejected.  Sawmill B produces boards of the same standard deviation but of mean length 210.1 cm. Find the proportion of boards rejected if they are drawn at random from the outputs of A and B in the ratio 3 : 1.  Let X = length of boards from A, so that X ∼ N 209.4,  5.0 2  and   cid:8    cid:7    cid:7   200 − µ  σ   cid:8   200 − 209.4  5.0  = Φ −1.88 .  Pr X < 200  = Φ  = Φ  But, since Φ −z  = 1 − Φ z  we have, using table 30.3,  cid:8   i.e. 3.0% of boards are rejected.   cid:7   Now let Y = length of boards from B, so that Y ∼ N 210.1,  5.0 2  and  Pr X < 200  = 1 − Φ 1.88  = 1 − 0.9699 = 0.0301,  Pr Y < 200  = Φ  200 − 210.1  = Φ −2.02   5.0  = 1 − Φ 2.02  = 1 − 0.9783 = 0.0217.  Therefore, when taken alone, only 2.2% of boards from B are rejected. If, however, boards are drawn at random from A and B in the ratio 3 : 1 then the proportion rejected is  4  3 × 0.030 + 1 × 0.022  = 0.028 = 2.8%.  cid:2   1  We may sometimes work backwards to derive the mean and standard deviation  of a population that is known to be Gaussian distributed.  cid:1 The time taken for a computer ‘packet’ to travel from Cambridge UK to Cambridge MA is Gaussian distributed. 6.8% of the packets take over 200 ms to make the journey, and 3.0% take under 140 ms. Find the mean and standard deviation of the distribution.  Let X = journey time in ms; we are told that X ∼ N µ, σ2  where µ and σ are unknown.  Since 6.8% of journey times are longer than 200 ms,   cid:7    cid:8   200 − µ  σ  = 0.068,  = 1 − 0.068 = 0.932.  Pr X > 200  = 1 − Φ  cid:8    cid:7   200 − µ  Φ  σ  from which we ﬁnd  Using table 30.3, we have therefore   30.112   Also, 3.0% of journey times are under 140 ms, so  Pr X < 140  = Φ  = 0.030.  200 − µ  cid:7   σ   cid:8   = 1.49.  140 − µ  σ  1184   30.9 IMPORTANT CONTINUOUS DISTRIBUTIONS   cid:7  Now using Φ −z  = 1 − Φ z  gives   cid:8   µ − 140  σ  Φ  = 1 − 0.030 = 0.970.  Using table 30.3 again, we ﬁnd  µ − 140  σ  = 1.88.   30.113   Solving the simultaneous equations  30.112  and  30.113  gives µ = 173.5, σ = 17.8.  cid:2    cid:18    cid:19    cid:21  ∞  The moment generating function for the Gaussian distribution   cid:14    cid:13  tx −  x − µ 2  cid:6   2σ2  dx  =  etX  exp  µt + 1  MX  t  = E  −∞ σ = c exp  Using the deﬁnition of the MGF  30.85 ,  cid:5  √ 1 2π  cid:12  − [x −  µ + σ2t ]2  cid:6   cid:5    cid:21  ∞  2 σ2t2  −∞  2σ2  c =  exp  2π  σ  ,  where the ﬁnal equality is established by completing the square in the argument of the exponential and writing √ 1  dx.  However, the ﬁnal integral is simply the normalisation integral for the Gaussian distribution, and so c = 1 and the MGF is given by   cid:15   MX t  = exp  µt + 1  2 σ2t2  .   30.114   We showed in subsection 30.7.2 that this MGF leads to E[X] = µ and V [X] = σ2, as required.  Gaussian approximation to the binomial distribution  We may consider the Gaussian distribution as the limit of the binomial distribu-  tion when the number of trials n → ∞ but the probability of a success p remains ﬁnite, so that np → ∞ also.  This contrasts with the Poisson distribution, which corresponds to the limit n → ∞ and p → 0 with np = λ remaining ﬁnite.  In  other words, a Gaussian distribution results when an experiment with a ﬁnite probability of success is repeated a large number of times. We now show how this Gaussian limit arises.  The binomial probability function gives the probability of x successes in n trials  as  Taking the limit as n → ∞  and x → ∞  we may approximate the factorials by  Stirling’s approximation  f x  =  n!  x! n − x !  px 1 − p n−x.  cid:9   cid:10   n  n! ∼ √  2πn  n e  1185   PROBABILITY  x 0 1 2 3 4 5 6 7 8 9 10  f x   binomial  0.0001 0.0016 0.0106 0.0425 0.1115 0.2007 0.2508 0.2150 0.1209 0.0403 0.0060  f x   Gaussian  0.0001 0.0014 0.0092 0.0395 0.1119 0.2091 0.2575 0.2091 0.1119 0.0395 0.0092  Table 30.4 Comparison of the binomial distribution for n = 10 and p = 0.6 with its Gaussian approximation.  to obtain  f x  ≈ 1√ 2πn 1√ 2πn  =  n  x n   cid:9    cid:9   n − x  cid:6    cid:10 −x−1 2  cid:22  − cid:5    cid:10 −n+x−1 2 − cid:5   cid:23  n − x + 1 + x ln p +  n − x  ln 1 − p   cid:13   x + 1 2  exp  x n  ln  2  .  px 1 − p n−x n − x   cid:6   ln  n   cid:14   ,   x − np 2 − 1 np 1 − p  2 √ np 1 − p .  f x  ≈ 1√  1√ p 1 − p   exp  2πn  By expanding the argument of the exponential in terms of y = x − np, where 1  cid:16  y  cid:16  np and keeping only the dominant terms, it can be shown that  which is of Gaussian form with µ = np and σ =  Thus we see that the value of the Gaussian probability density function f x  is a good approximation to the probability of obtaining x successes in n trials. This approximation is actually very good even for relatively small n. For example, if n = 10 and p = 0.6 then the Gaussian approximation to the binomial distribution  is  30.105  with µ = 10 × 0.6 = 6 and σ =  √ 10 × 0.6 1 − 0.6  = 1.549. The  probability functions f x  for the binomial and associated Gaussian distributions for these parameters are given in table 30.4, and it can be seen that the Gaussian approximation is a good one.  Strictly speaking, however, since the Gaussian distribution is continuous and the binomial distribution is discrete, we should use the integral of f x  for the Gaussian distribution in the calculation of approximate binomial probabilities. More speciﬁcally, we should apply a continuity correction so that the discrete  integer x in the binomial distribution becomes the interval [x − 0.5, x + 0.5] in  1186   30.9 IMPORTANT CONTINUOUS DISTRIBUTIONS  the Gaussian distribution. Explicitly,  √ Pr X = x  ≈ 1  σ  2π  x+0.5  x−0.5  exp   cid:21    cid:21    cid:13   − 1 2  cid:13    cid:9   u − µ   cid:14    cid:10 2  σ   cid:9   du.   cid:14    cid:10 2  u − µ  σ  du.  The Gaussian approximation is particularly useful for estimating the binomial probability that X lies between the  integer  values x1 and x2,  √ Pr x1 < X ≤ x2  ≈ 1 2π  σ  x2+0.5  x1−0.5  exp  − 1 2   cid:1 A manufacturer makes computer chips of which 10% are defective. For a random sample of 200 chips, ﬁnd the approximate probability that more than 15 are defective.  We ﬁrst deﬁne the random variable  which has a binomial distribution X∼ Bin 200, 0.1 . Therefore, the mean and variance of  X = number of defective chips in the sample,  this distribution are  E[X] = 200 × 0.1 = 20  and  V [X] = 200 × 0.1 ×  1 − 0.1  = 18,  and we may approximate the binomial distribution with a Gaussian distribution such that  X ∼ N 20, 18 . The standard variable is  cid:7   X − 20√  ,  18  Z =   cid:8   15.5 − 20√  18  and so, using X = 15.5 to allow for the continuity correction,  Pr X > 15.5  = Pr  Z >  = Pr Z < 1.06  = 0.86.  cid:2   = Pr Z > −1.06   Gaussian approximation to the Poisson distribution  We ﬁrst met the Poisson distribution as the limit of the binomial distribution for  n → ∞ and p → 0, taken in such a way that np = λ remains ﬁnite. Further, in the binomial distribution when n → ∞ but p remains ﬁnite, so that np → ∞ also.  the previous subsection, we considered the Gaussian distribution as the limit of  It should come as no surprise, therefore, that the Gaussian distribution can also be used to approximate the Poisson distribution when the mean λ becomes large. The probability function for the Poisson distribution is  which, on taking the logarithm of both sides, gives  ln f x  = −λ + x ln λ − ln x!.   30.115   f x  = e  −λ λx x!  ,  1187   Stirling’s approximation for large x gives  implying that  PROBABILITY   cid:9    cid:10   x  x! ≈ √ √ 2πx + x ln x − x, ln x! ≈ ln  2πx  x e  Since we expect the Poisson distribution to peak around x = λ, we substitute  which, on substituting into  30.115 , yields  ln f x  ≈ −λ + x ln λ −  x ln x − x  − ln   cid:4  = x − λ to obtain  ln f x  ≈ −λ +  λ +  cid:4    !   cid:10  cid:23 "   cid:22    cid:9   λ  1 +   cid:4  λ   cid:8   −  cid:4 2 2λ2  Using the expansion ln 1 + z  = z − z2 2 + ··· , we ﬁnd √ 2πλ −  − ln  ln λ − ln  cid:7  ln f x  ≈  cid:4  −  λ +  cid:4   √ − ln 2πλ,   cid:4  λ  ≈ −  cid:4 2 2λ  √ 2πx.   cid:24    cid:8    cid:7   −  cid:4 2 2λ2   cid:4  λ  +  λ +  cid:4   − ln  2π λ +  cid:4  .  when only the dominant terms are retained, after using the fact that  cid:4  is of the order of the standard deviation of x, i.e. of order λ1 2. On exponentiating this result we obtain   cid:13   f x  ≈ 1√  exp  2πλ  −  x − λ 2  2λ   cid:14   ,  which is the Gaussian distribution with µ = λ and σ2 = λ. Poisson distribution; the approximation is reasonable even for λ = 5, but λ ≥ 10  The larger the value of λ, the better is the Gaussian approximation to the  is safer. As in the case of the Gaussian approximation to the binomial distribution, a continuity correction is necessary since the Poisson distribution is discrete.  cid:1 E-mail messages are received by an author at an average rate of one per hour. Find the probability that in a day the author receives 24 messages or more.  We ﬁrst deﬁne the random variable  Thus E[X] = 1 × 24 = 24, and so X ∼ Po 24 . Since λ > 10 we may approximate the Poisson distribution by X ∼ N 24, 24 . Now the standard variable is  X = number of messages received in a day.  and, using the continuity correction, we ﬁnd  Pr X > 23.5  = Pr  Z >  = Pr Z > −0.102  = Pr Z < 0.102  = 0.54.  cid:2   24   cid:7   Z =  X − 24√  cid:8   24  ,  23.5 − 24√  1188   30.9 IMPORTANT CONTINUOUS DISTRIBUTIONS  In fact, almost all probability distributions tend towards a Gaussian when the numbers involved become large – that this should happen is required by the central limit theorem, which we discuss in section 30.10.  Multiple Gaussian distributions   cid:5   cid:5    cid:6    cid:6   cid:6    cid:19   that X ∼ N µ1, σ2  Suppose X and Y are independent Gaussian-distributed random variables, so 2 . Let us now consider the random variable Z = X + Y . The PDF for this random variable may be found directly using  30.61 , but it is easier to use the MGF. From  30.114 , the MGFs of X and Y are  1  and Y ∼ N µ2, σ2  cid:6    cid:5   MX  t  = exp  µ1t + 1  2 σ2  1t2  ,  MY  t  = exp  µ2t + 1  2 σ2  2t2  .   cid:5   cid:18   Using  30.89 , since X and Y are independent RVs, the MGF of Z = X + Y is simply the product of MX t  and MY  t . Thus, we have  1t2  2 σ2  1 + σ2  exp 2  σ2  2t2 2 σ2 ,  µ1t + 1  µ1 + µ2 t + 1  MZ  t  = MX t MY  t  = exp = exp  µ2t + 1 2 t2 2. Thus, Z is also Gaussian distributed: Z ∼ N µ1 + µ2, σ2  which we recognise as the MGF for a Gaussian with mean µ1 + µ2 and variance σ2 1 + σ2 A similar calculation may be performed to calculate the PDF of the random variable W = X − Y . If we introduce the variable ˜Y = −Y then W = X + ˜Y , 1 . Thus, using the result above, we ﬁnd W ∼ N µ1 − where ˜Y ∼ N −µ1, σ2 µ2, σ2  cid:1 An executive travels home from her oﬃce every evening. Her journey consists of a train ride, followed by a bicycle ride. The time spent on the train is Gaussian distributed with mean 52 minutes and standard deviation 1.8 minutes, while the time for the bicycle journey is Gaussian distributed with mean 8 minutes and standard deviation 2.6 minutes. Assuming these two factors are independent, estimate the percentage of occasions on which the whole journey takes more than 65 minutes.  1 + σ2 2 .  1 + σ2 2 .  We ﬁrst deﬁne the random variables  so that X ∼ N 52,  1.8 2  and Y ∼ N 8,  2.6 2 . Since X and Y are independent, the total  X = time spent on train,  Y = time spent on bicycle,  journey time T = X + Y is distributed as  T ∼ N 52 + 8,  1.8 2 +  2.6 2  = N 60,  3.16 2 .  The standard variable is thus  and the required probability is given by  Pr T > 65  = Pr  Z >  = Pr Z > 1.58  = 1 − 0.943 = 0.057.  Thus the total journey time exceeds 65 minutes on 5.7% of occasions.  cid:2   T − 60  3.16  ,   cid:7   Z =   cid:8   65 − 60  3.16  1189   PROBABILITY  Xi, i = 1, 2, . . . , n, are distributed as Xi ∼ N µi, σ2   cid:11   cid:11  i ciXi  where the ci are constants  is distributed as Z ∼ N   The above results may be extended. For example, if the random variables i   then the random variable i σ2 i  .   cid:11   i ciµi,  Z =  i c2  30.9.2 The log-normal distribution  If the random variable X follows a Gaussian distribution then the variable Y = eX is described by a log-normal distribution. Clearly, if X can take values in the range −∞ to ∞, then Y will lie between 0 and ∞. The probability density  function for Y is found using the result  30.58 . It is   cid:20  cid:20  cid:20  cid:20  =   cid:20  cid:20  cid:20  cid:20  dx  dy   cid:13   √ 1  σ  2π  1 y  exp  −  ln y − µ 2  2σ2   cid:14   .  g y  = f x y    We note that µ and σ2 are not the mean and variance of the log-normal distribution, but rather the parameters of the corresponding Gaussian distribution for X. The mean and variance of Y , however, can be found straightforwardly using the MGF of X, which reads MX t  = E[etX] = exp µt + 1 2 σ2t2 . Thus, the mean of Y is given by  E[Y ] = E[eX] = MX  1  = exp µ + 1  2 σ2 ,  and the variance of Y reads  V [Y ] = E[Y 2] −  E[Y ] 2 = E[e2X] −  E[eX] 2  = MX 2  − [MX  1 ]2 = exp 2µ + σ2 [exp σ2  − 1].  In ﬁgure 30.15, we plot some examples of the log-normal distribution for various values of the parameters µ and σ2.  30.9.3 The exponential and gamma distributions  The exponential distribution with positive parameter λ is given by    f x  =  −λx  λe  0  for x > 0,  for x ≤ 0   cid:1  ∞  −∞ f x  dx = 1 as required. The exponential distribution occurs nat- and satisﬁes urally if we consider the distribution of the length of intervals between successive events in a Poisson process or, equivalently, the distribution of the interval  i.e. the waiting time  before the ﬁrst event. If the average number of events per unit interval is λ then on average there are λx events in interval x, so that from the Poisson distribution the probability that there will be no events in this interval is given by  Pr no events in interval x  = e  −λx.   30.116   1190   30.9 IMPORTANT CONTINUOUS DISTRIBUTIONS  µ = 0, σ = 0 µ = 0, σ = 0.5 µ = 0, σ = 1.5 µ = 1, σ = 1  g y   1  0.8  0.6  0.4  0.2  0  0  1  2  3  y  4  Figure 30.15 The PDF g y  for the log-normal distribution for various values of the parameters µ and σ.  The probability that an event occurs in the next inﬁnitestimal interval [x, x + dx] is given by λ dx, so that  Pr the ﬁrst event occurs in interval [x, x + dx]  = e  −λxλ dx.  Hence the required probability density function is given by  f x  = λe  −λx.  M t  =  λ  λ − t  .  The expectation and variance of the exponential distribution can be evaluated as 1 λ and  1 λ 2 respectively. The MGF is given by  We may generalise the above discussion to obtain the PDF for the interval between every rth event in a Poisson process or, equivalently, the interval  waiting time  before the rth event. We begin by using the Poisson distribution to give  Pr r − 1 events occur in interval x  = e   30.117   from which we obtain  Thus the required PDF is  Pr rth event occurs in the interval [x, x + dx]  = e  λ dx.  f x  =  λ   r − 1 !   λx r−1e  −λx,   30.118   which is known as the gamma distribution of order r with parameter λ. Although our derivation applies only when r is a positive integer, the gamma distribution is  1191  −λx  λx r−1  r − 1 !  ,  −λx  λx r−1  r − 1 !   PROBABILITY  0.6  r = 1  f x   1  0.8  0.4  0.2  0  r = 2  r = 5  r = 10  0  2  4  6  8  10  12 14  16  18  20  x  Figure 30.16 The PDF f x  for the gamma distributions γ λ, r  with λ = 1 and r = 1, 2, 5, 10.  deﬁned for all positive r by replacing  r− 1 ! by Γ r  in  30.118 ; see the appendix by a gamma distribution of order r with parameter λ, we write X ∼ γ λ, r ;  for a discussion of the gamma function Γ x . If a random variable X is described  we note that the exponential distribution is the special case γ λ, 1 . The gamma distribution γ λ, r  is plotted in ﬁgure 30.16 for λ = 1 and r = 1, 2, 5, 10. For large r, the gamma distribution tends to the Gaussian distribution whose mean and variance are speciﬁed by  30.120  below.  The MGF for the gamma distribution is obtained from that for the exponential distribution, by noting that we may consider the interval between every rth event in a Poisson process as the sum of r intervals between successive events. Thus the rth-order gamma variate is the sum of r independent exponentially distributed random variables. From  30.117  and  30.90 , the MGF of the gamma distribution is therefore given by   cid:7    cid:8   M t  =  r  ,  λ  λ − t   30.119   from which the mean and variance are found to be  E[X] =  V [X] =   30.120   r λ  ,  We may also use the above MGF to prove another useful theorem regarding  multiple gamma distributions. If Xi ∼ γ λ, ri , i = 1, 2, . . . , n, are independent gamma variates then the random variable Y = X1 + X2 + ··· + Xn has MGF   cid:7   n cid:3    cid:8   ri   cid:7   r1+r2+···+rn  M t  =  λ  λ − t  =  λ  λ − t  .   30.121   Thus Y is also a gamma variate, distributed as Y ∼ γ λ, r1 + r2 + ··· + rn .  i=1  r λ2 .   cid:8   1192   30.9 IMPORTANT CONTINUOUS DISTRIBUTIONS  30.9.4 The chi-squared distribution  In subsection 30.6.2, we showed that if X is Gaussian distributed with mean µ and  variance σ2, such that X ∼ N µ, σ2 , then the random variable Y =  x − µ 2 σ2 is distributed as the gamma distribution Y ∼ γ  1 2 , 1 2  . Let us now consider n independent Gaussian random variables Xi ∼ N µi, σ2 i  , i = 1, 2, . . . , n, and deﬁne  the new variable  n cid:4   i=1  χ2 n =   Xi − µi 2  .  σ2 i  Using the result  30.121  for multiple gamma distributions, χ2 as the gamma variate χ2 n  2 n , which from  30.118  has the PDF  ∼ γ  1 2 , 1  n must be distributed  f χ2  n  =  1 2 Γ  1 2 n  1  n  n 2 −1 exp − 1 2 χ2 2 χ2   1 n  n  n 2 −1 exp − 1 2 χ2  χ2 n .  =  2n 2Γ  1  2 n    30.122    30.123   This is known as the chi-squared distribution of order n and has numerous applications in statistics  see chapter 31 . Setting λ = 1 2 n in  30.120 , we ﬁnd that  2 and r = 1  E[χ2  n] = n,  V [χ2  n] = 2n.  An important generalisation occurs when the n Gaussian variables Xi are not linearly independent but are instead required to satisfy a linear constraint of the form  c1X1 + c2X2 + ··· + cnXn = 0,   30.124   in which the constants ci are not all zero. In this case, it may be shown  see exercise 30.40  that the variable χ2 n deﬁned in  30.122  is still described by a chi- squared distribution, but one of order n − 1. Indeed, this result may be trivially extended to show that if the n Gaussian variables Xi satisfy m linear constraints of the form  30.124  then the variable χ2 n deﬁned in  30.122  is described by a chi-squared distribution of order n − m.  30.9.5 The Cauchy and Breit–Wigner distributions  A random variable X  in the range −∞ to ∞  that obeys the Cauchy distribution  is described by the PDF  f x  =  1 π  1  1 + x2 .  1193   PROBABILITY  x0 = 0, Γ = 1  x0 = 2, Γ = 1  f x   0.8  0.6  0.4  0.2  0  x0 = 0, Γ = 3  −4  −2  0  2  4  x  Figure 30.17 The PDF f x  for the Breit–Wigner distribution for diﬀerent values of the parameters x0 and Γ.  This is a special case of the Breit–Wigner distribution  f x  =  1 π  1 2 Γ  4 Γ2 +  x − x0 2  1  ,  which is encountered in the study of nuclear and particle physics. In ﬁgure 30.17, we plot some examples of the Breit–Wigner distribution for several values of the parameters x0 and Γ.  We see from the ﬁgure that the peak  or mode  of the distribution occurs at x = x0. It is also straightforward to show that the parameter Γ is equal to the width of the peak at half the maximum height. Although the Breit–Wigner distribution is symmetric about its peak, it does not formally possess a mean since the integrals 0 xf x  dx both diverge. Similar divergences occur for all higher moments of the distribution.  0−∞ xf x  dx and   cid:1  ∞   cid:1   30.9.6 The uniform distribution  Finally we mention the very simple, but common, uniform distribution, which describes a continuous random variable that has a constant PDF over its allowed range of values. If the limits on X are a and b then    f x  =  1  b − a   for a ≤ x ≤ b,  0  otherwise.  The MGF of the uniform distribution is found to be  M t  =  ebt − eat  b − a t  ,  1194   30.10 THE CENTRAL LIMIT THEOREM  and its mean and variance are given by  E[X] =  V [X] =  a + b  ,  2   b − a 2  .  12  30.10 The central limit theorem  In subsection 30.9.1 we discussed approximating the binomial and Poisson distri- butions by the Gaussian distribution when the number of trials is large. We now discuss why the Gaussian distribution is so common and therefore so important. The central limit theorem may be stated as follows.  Central limit theorem. Suppose that Xi, i = 1, 2, . . . , n, are independent random variables, each of which is described by a probability density function fi x   these may all be diﬀerent  with a mean µi and a variance σ2 i . The random variable Z =   cid:5  cid:11    cid:6    n, i.e. the ‘mean’ of the Xi, has the following properties:  i Xi  i  its expectation value is given by E[Z] = i σ2  ii  its variance is given by V [Z] =   iii  as n → ∞ the probability function of Z tends to a Gaussian with corre-  i  i µi   n;   cid:5  cid:11    n2;   cid:6    cid:5  cid:11    cid:6   sponding mean and variance.  We note that for the theorem to hold, the probability density functions fi x  must possess formal means and variances. Thus, for example, if any of the Xi were described by a Cauchy distribution then the theorem would not apply. Properties  i  and  ii  of the theorem are easily proved, as follows. Firstly  E[Z] =  1 n   E[X1] + E[X2] + ··· + E[Xn]  =   µ1 + µ2 + ··· + µn  =  1 n  a result which does not require that the Xi are independent random variables. If µi = µ for all i then this becomes   cid:11   i µi n  ,  Secondly, if the Xi are independent, it follows from an obvious extension of  30.68  that  E[Z] =  = µ.  nµ n   cid:13    cid:14   X1 + X2 + ··· + Xn   V [X1] + V [X2] + ··· + V [Xn]  =  1 n   cid:11   i  i σ2 n2  .  V [Z] = V  =  1 n2  Let us now consider property  iii , which is the reason for the ubiquity of the Gaussian distribution and is most easily proved by considering the moment generating function MZ  t  of Z. From  30.90 , this MGF is given by  MZ  t  =  MXi   cid:7    cid:8   t n  ,  n cid:3   i=1  1195   where MXi  t  is the MGF of fi x . Now  and as n becomes large  PROBABILITY  MXi  = 1 +  E[Xi] + 1 2  = 1 + µi  + 1  2  σ2  i + µ2 i    i ] + ··· t2 n2 E[X2 n2 + ··· , t2  cid:8   µit n  + 1  2 σ2  i  t2 n2  ,   cid:7    cid:8   t n  MXi   cid:7    cid:7    cid:8   t n  t n  t n   cid:7   ≈ exp  cid:8   MZ  t  ≈ n cid:3   cid:11   i=1  as may be veriﬁed by expanding the exponential up to terms including  t n 2. Therefore   cid:7  cid:11    cid:11    cid:8   exp  µit n  + 1  2 σ2  i  t2 n2  = exp  i µi n  t + 1 2  i  i σ2 n2  t2  .   cid:11   i µi n and variance  Comparing this with the form of the MGF for a Gaussian distribution,  30.114 , we can see that the probability density function g z  of Z tends to a Gaussian dis- i  n2. In particular, if we consider tribution with mean Z to be the mean of n independent measurements of the same random variable X  so that Xi = X for i = 1, 2, . . . , n  then, as n → ∞, Z has a Gaussian distribution with mean µ and variance σ2 n. above for the product W = X1X2 ··· Xn of the n independent random variables Xi. Provided the Xi only take values between zero and inﬁnity, we may write  We may use the central limit theorem to derive an analogous result to  iii   i σ2  ln W = ln X1 + ln X2 + ··· + ln Xn,  which is simply the sum of n new random variables ln Xi. Thus, provided these new variables each possess a formal mean and variance, the PDF of ln W will tend to a Gaussian in the limit n → ∞, and so the product W will be described  by a log-normal distribution  see subsection 30.9.2 .  30.11 Joint distributions  As mentioned brieﬂy in subsection 30.4.3, it is common in the physical sciences to consider simultaneously two or more random variables that are not independent, in general, and are thus described by joint probability density functions. We will return to the subject of the interdependence of random variables after ﬁrst presenting some of the general ways of characterising joint distributions. We will concentrate mainly on bivariate distributions, i.e. distributions of only two random variables, though the results may be extended readily to multivariate distributions. The subject of multivariate distributions is large and a detailed study is beyond the scope of this book; the interested reader should therefore  1196   30.11 JOINT DISTRIBUTIONS  consult one of the many specialised texts. However, we do discuss the multinomial and multivariate Gaussian distributions, in section 30.15.  The ﬁrst thing to note when dealing with bivariate distributions is that the distinction between discrete and continuous distributions may not be as clear as for the single variable case; the random variables can both be discrete, or both continuous, or one discrete and the other continuous. In general, for the random variables X and Y , the joint distribution will take an inﬁnite number of values unless both X and Y have only a ﬁnite number of values. In this chapter we will consider only the cases where X and Y are either both discrete or both continuous random variables.  30.11.1 Discrete bivariate distributions  In direct analogy with the one-variable  univariate  case, if X is a discrete random  variable that takes the values {xi} and Y one that takes the values {yj} then the  probability function of the joint distribution is deﬁned as    f x, y  =  Pr X = xi, Y = yj  0  for x = xi, y = yj, otherwise.  We may therefore think of f x, y  as a set of spikes at valid points in the xy-plane, whose height at  xi, yi  represents the probability of obtaining X = xi and Y = yj.   cid:4  The normalisation of f x, y  implies cid:4   f xi, yj  = 1,  i  j   30.125   where the sums over i and j take all valid pairs of values. We can also deﬁne the cumulative probability function  F x, y  =  f xi, yj ,   30.126    cid:4    cid:4   xi≤x  yj≤y  from which it follows that the probability that X lies in the range [a1, a2] and Y lies in the range [b1, b2] is given by  Pr a1 < X ≤ a2, b1 < Y ≤ b2  = F a2, b2  − F a1, b2  − F a2, b1  + F a1, b1 .  Finally, we deﬁne X and Y to be independent if we can write their joint distribution in the form  f x, y  = fX x fY  y ,   30.127   i.e. as the product of two univariate distributions.  1197   PROBABILITY  30.11.2 Continuous bivariate distributions  In the case where both X and Y are continuous random variables, the PDF of the joint distribution is deﬁned by  f x, y  dx dy = Pr x < X ≤ x + dx, y < Y ≤ y + dy ,   30.128   so f x, y  dx dy is the probability that x lies in the range [x, x + dx] and y lies in the range [y, y + dy]. It is clear that the two-dimensional function f x, y  must be everywhere non-negative and that normalisation requires   cid:21  ∞   cid:21  ∞  −∞  −∞ f x, y  dx dy = 1.  Pr a1 < X ≤ a2, b1 < Y ≤ b2  =  cid:21   F x, y  = Pr X ≤ x, Y ≤ y  =   cid:21    cid:21   b2  a2  b1  a1   cid:21   x  y  −∞  −∞ f u, v  du dv,  f x, y  dx dy.   30.129   We can also deﬁne the cumulative probability function by  It follows further that  from which we see that  as for the discrete case ,  Pr a1 < X ≤ a2, b1 < Y ≤ b2  = F a2, b2  − F a1, b2  − F a2, b1  + F a1, b1 .  Finally we note that the deﬁnition of independence  30.127  for discrete bivariate distributions also applies to continuous bivariate distributions.  cid:1 A ﬂat table is ruled with parallel straight lines a distance D apart, and a thin needle of length l < D is tossed onto the table at random. What is the probability that the needle will cross a line?  Let θ be the angle that the needle makes with the lines, and let x be the distance from the centre of the needle to the nearest line. Since the needle is tossed ‘at random’ onto the table, the angle θ is uniformly distributed in the interval [0, π], and the distance x is uniformly distributed in the interval [0, D 2]. Assuming that θ and x are independent, their joint distribution is just the product of their individual distributions, and is given by  f θ, x  =  1 π  1  D 2  =  2 πD  .  The needle will cross a line if the distance x of its centre from that line is less than 1 Thus the required probability is  2 l sin θ.   cid:21    cid:21  1   cid:21   π  2 l sin θ  2 πD  0  0  dx dθ =  sin θ dθ =  2 πD  l 2  π  0  2l πD  .  This gives an experimental  but cumbersome  method of determining π.  cid:2   1198   30.12 PROPERTIES OF JOINT DISTRIBUTIONS  30.11.3 Marginal and conditional distributions   cid:11   cid:1   Given a bivariate distribution f x, y , we may be interested only in the proba- bility function for X irrespective of the value of Y  or vice versa . This marginal distribution of X is obtained by summing or integrating, as appropriate, the joint probability distribution over all allowed values of Y . Thus, the marginal distribution of X  for example  is given by  fX x  =  j f x, yj  f x, y  dy  for a discrete distribution,  for a continuous distribution.   30.130   It is clear that an analogous deﬁnition exists for the marginal distribution of Y . Alternatively, one might be interested in the probability function of X given  that Y takes some speciﬁc value of Y = y0, i.e. Pr X = xY = y0 . This conditional  distribution of X is given by  g x  =  f x, y0  fY  y0   ,  where fY  y  is the marginal distribution of Y . The division by fY  y0  is necessary in order that g x  is properly normalised.  30.12 Properties of joint distributions  The probability density function f x, y  contains all the information on the joint probability distribution of two random variables X and Y . In a similar manner to that presented for univariate distributions, however, it is conventional to characterise f x, y  by certain of its properties, which we now discuss. Once again, most of these properties are based on the concept of expectation values, which are deﬁned for joint distributions in an analogous way to those for single- variable distributions  30.46 . Thus, the expectation value of any function g X, Y   of the random variables X and Y is given by  E[g X, Y  ] =  j g xi, yj f xi, yj  −∞ g x, y f x, y  dx dy  for the discrete case,  for the continuous case.  30.12.1 Means  The means of X and Y are deﬁned respectively as the expectation values of the variables X and Y . Thus, the mean of X is given by  E[X] = µX =  j xif xi, yj  −∞ xf x, y  dx dy  for the discrete case,  for the continuous case.   30.131    cid:11   cid:1  ∞  i   cid:11   cid:1  ∞  −∞   cid:11   cid:1  ∞  i   cid:11   cid:1  ∞  −∞  E[Y ] is obtained in a similar manner.  1199   PROBABILITY   cid:1 Show that if X and Y are independent random variables then E[XY ] = E[X]E[Y ].  Let us consider the case where X and Y are continuous random variables. Since X and Y are independent f x, y  = fX  x fY  y , so that   cid:21  ∞   cid:21  ∞   cid:21  ∞   cid:21  ∞  E[XY ] =  −∞  −∞ xyfX x fY  y  dx dy =  −∞ xfX x  dx  −∞ yfY  y  dy = E[X]E[Y ].  An analogous proof exists for the discrete case.  cid:2   The deﬁnitions of the variances of X and Y are analogous to those for the single-variable case  30.48 , i.e. the variance of X is given by  30.12.2 Variances   cid:11   cid:1  ∞  i  −∞   cid:11   cid:1  ∞ j xi − µX 2f xi, yj  −∞ x − µX 2f x, y  dx dy  V [X] = σ2  X =  Equivalent deﬁnitions exist for the variance of Y .  for the discrete case, for the continuous case.  30.132   30.12.3 Covariance and correlation  Means and variances of joint distributions provide useful information about their marginal distributions, but we have not yet given any indication of how to measure the relationship between the two random variables. Of course, it may be that the two random variables are independent, but often this is not so. For example, if we measure the heights and weights of a sample of people we would not be surprised to ﬁnd a tendency for tall people to be heavier than short people and vice versa. We will show in this section that two functions, the covariance and the correlation, can be deﬁned for a bivariate distribution and that these are useful in characterising the relationship between the two random variables.  The covariance of two random variables X and Y is deﬁned by  Cov[X, Y ] = E[ X − µX  Y − µY  ],   30.133   where µX and µY are the expectation values of X and Y respectively. Clearly related to the covariance is the correlation of the two random variables, deﬁned by  Corr[X, Y ] =  Cov[X, Y ]  ,  σX σY   30.134   where σX and σY are the standard deviations of X and Y respectively. It can be shown that the correlation function lies between −1 and +1. If the value assumed  is negative, X and Y are said to be negatively correlated, if it is positive they are said to be positively correlated and if it is zero they are said to be uncorrelated. We will now justify the use of these terms.  1200   30.12 PROPERTIES OF JOINT DISTRIBUTIONS  One particularly useful consequence of its deﬁnition is that the covariance of two independent variables, X and Y , is zero. It immediately follows from  30.134  that their correlation is also zero, and this justiﬁes the use of the term ‘uncorrelated’ for two such variables. To show this extremely important property we ﬁrst note that  Cov[X, Y ] = E[ X − µX  Y − µY  ]  = E[XY − µX Y − µY X + µX µY ] = E[XY ] − µX E[Y ] − µY E[X] + µXµY = E[XY ] − µX µY .   30.135   Now, if X and Y are independent then E[XY ] = E[X]E[Y ] = µX µY and so Cov[X, Y ] = 0. It is important to note that the converse of this result is not necessarily true; two variables dependent on each other can still be uncorrelated. In other words, it is possible  and not uncommon  for two variables X and Y to be described by a joint distribution f x, y  that cannot be factorised into a product of the form g x h y , but for which Corr[X, Y ] = 0. Indeed, from the deﬁnition  30.133 , we see that for any joint distribution f x, y  that is symmetric in x about µX  or similarly in y  we have Corr[X, Y ] = 0.  We have already asserted that if the correlation of two random variables is positive  negative  they are said to be positively  negatively  correlated. We have  that if the two RVs are identical  i.e. X = Y   then they are completely correlated  also stated that the correlation lies between −1 and +1. The terminology suggests and that their correlation should be +1. Likewise, if X = −Y then the functions are completely anticorrelated and their correlation should be −1. Values of the  correlation function between these extremes show the existence of some degree of correlation. In fact it is not necessary that X = Y for Corr[X, Y ] = 1; it is suﬃcient that Y is a linear function of X, i.e. Y = aX + b  with a positive . If a  is negative then Corr[X, Y ] = −1. To show this we ﬁrst note that µY = aµX + b.  Now  Y = aX + b = aX + µY − aµX ⇒ Y − µY = a X − µX ,  and so using the deﬁnition of the covariance  30.133   Cov[X, Y ] = aE[ X − µX 2] = aσ2  X .  It follows from the properties of the variance  subsection 30.5.3  that σY = aσX  and so, using the deﬁnition  30.134  of the correlation,  which is the stated result.  It should be noted that, even if the possibilities of X and Y being non-zero are  mutually exclusive, Corr[X, Y ] need not have value ±1.  Corr[X, Y ] =  aσ2 Xaσ2  X  =  aa ,  1201   PROBABILITY   cid:1 A biased die gives probabilities 1 2 p, p, p, p, p, 2p of throwing 1, 2, 3, 4, 5, 6 respectively. If the random variable X is the number shown on the die and the random variable Y is deﬁned as X 2, calculate the covariance and correlation of X and Y .  We have already calculated in subsections 30.2.1 and 30.5.4 that  p =  , E[X] =  , E  X 2  =  , V [X] =  2 13  53 13  253 13  480 169  .   cid:18    cid:19   Using  30.135 , we obtain  Now E[X 3] is given by  Cov[X, Y ] = Cov[X, X 2] = E[X 3] − E[X]E[X 2].  E[X 3] = 13 × 1  2 p +  23 + 33 + 43 + 53 p + 63 × 2p  =  1313  2  p = 101,  and the covariance of X and Y is given by  Cov[X, Y ] = 101 − 53  × 253 13  13  =  3660 169  .  The correlation is deﬁned by Corr[X, Y ] = Cov[X, Y ] σX σY . The standard deviation of Y may be calculated from the deﬁnition of the variance. Letting µY = E[X 2] = 253 13 gives   cid:6    cid:5   32 − µY  2  + p  42 − µY   cid:6   2  σ2 Y =  p 2   cid:5    cid:6   cid:5  12 − µY 52 − µY  2   cid:6   + p 187 356  169  =  p =  + p  2  + 2p 28 824  .  169   cid:5    cid:5    cid:6  62 − µY   cid:5  22 − µY  2   cid:6   2  + p        We deduce that  Corr[X, Y ] =  3660 169  169  28 824  169 480  ≈ 0.984.  Thus the random variables X and Y display a strong degree of positive correlation, as we would expect.  cid:2   V [X + Y ] = E  We note that the covariance of X and Y occurs in various expressions. For  example, if X and Y are not independent then   cid:18   cid:18    cid:19    X + Y  2 X2   cid:19  −  E[X + Y ] 2  cid:18   = E  + 2E[XY ] + E  = V [X] + V [Y ] + 2 E[XY ] − E[X]E[Y ]   Y 2  = V [X] + V [Y ] + 2 Cov[X, Y ].   cid:19  − { E[X] 2 + 2E[X]E[Y ] +  E[Y ] 2}  1202   30.12 PROPERTIES OF JOINT DISTRIBUTIONS  More generally, we ﬁnd  for a, b and c constant   V [aX + bY + c] = a2V [X] + b2V [Y ] + 2ab Cov[X, Y ].   30.136   Note that if X and Y are in fact independent then Cov[X, Y ] = 0 and we recover the expression  30.68  in subsection 30.6.4.  We may use  30.136  to obtain an approximate expression for V [ f X, Y  ] for any arbitrary function f, even when the random variables X and Y are correlated. Approximating f X, Y   by the linear terms of its Taylor expansion about the point  µX , µY  , we have  f X, Y   ≈ f µX , µY   +   X − µX  +   Y − µY  ,   cid:7    cid:8   ∂f ∂X   cid:8   2   cid:7   ∂f ∂Y   cid:7    cid:8   ∂f ∂Y   30.137    cid:7    cid:8  cid:7    cid:8   ∂f ∂X  ∂f ∂Y  Cov[X, Y ].   30.138   where the partial derivatives are evaluated at X = µX and Y = µY . Taking the variance of both sides, and using  30.136 , we ﬁnd   cid:7    cid:8   2  ∂f ∂X  V [ f X, Y  ] ≈  V [X] +  V [Y ] + 2  Clearly, if Cov[X, Y ] = 0, we recover the result  30.69  derived in subsection 30.6.4. We note that  30.138  is exact if f X, Y   is linear in X and Y .  For several variables Xi, i = 1, 2, . . . , n, we can deﬁne the symmetric  positive  deﬁnite  covariance matrix whose elements are  Vij = Cov[Xi, Xj],   30.139   and the symmetric  positive deﬁnite  correlation matrix  ρij = Corr[Xi, Xj].  The diagonal elements of the covariance matrix are the variances of the variables, whilst those of the correlation matrix are unity. For several variables,  30.138  generalises to  V [f X1, X2, . . . , Xn ] ≈ cid:4    cid:7    cid:8   2  ∂f ∂Xi  i  V [Xi] +   cid:4    cid:4    cid:7   j cid:3 =i  i   cid:8  cid:7    cid:8   ∂f ∂Xi  ∂f ∂Xj  Cov[Xi, Xj],  where the partial derivatives are evaluated at Xi = µXi .  1203   PROBABILITY   cid:1 A card is drawn at random from a normal 52-card pack and its identity noted. The card is replaced, the pack shuﬄed and the process repeated. Random variables W , X, Y , Z are deﬁned as follows:  W = 2 X = 4  Y = 1 Z = 2  if the drawn card is a heart; W = 0 otherwise. if the drawn card is an ace, king, or queen; X = 2 if the card is a jack or ten; X = 0 otherwise. if the drawn card is red; Y = 0 otherwise. if the drawn card is black and an ace, king or queen; Z = 0 otherwise.  2  2  2  1 2  1 2  µW = 2 × 1 µY = 1 × 1  cid:6   cid:6   cid:6    cid:6  − cid:5   cid:6  − cid:5   cid:5   cid:5   cid:6   cid:5   cid:6   cid:5   cid:6   3 52  6 52  1 4  Establish the correlation matrix for W , X, Y , Z .  The means of the variables are given by  4 = 1 2 , 2 = 1 2 , The variances, calculated from σ2 Z , are   cid:5   cid:5  4 × 1 1 × 1  4  σ2 W = σ2 Y =   cid:5   13  µX =  4 × 3  cid:18  + µZ = 2 × 6 52 = 3 13 .  cid:5  U2 U = V [U] = E  cid:5  16 × 3 4 × 6  σ2 X = σ2 Z =  13  13   cid:6   = 16 13 ,  2 × 2   cid:6   cid:5   cid:19  −  E[U] 2, where U = W , X, Y or  cid:6   cid:5   cid:6  − cid:5    cid:6  − cid:5    cid:6  4 × 2  = 472 169 ,   cid:6   16 13  +  2  2  13 = 69 169 .  3 13  = 3 4 , = 1 4 ,  The covariances are found by ﬁrst calculating E[W X] etc. and then forming E[W X]−µW µX  52  etc.  E[W X] = 2  4   + 2  2   2 52  = 8   cid:5    cid:6    cid:5    cid:6   E[W Y ] = 2 1   = 1 2 ,  E[W Z ] = 0,  E[XY ] = 4 1   + 2 1   4 52  = 8 13 ,  E[XZ ] = 4 2   6 52  = 12 13 ,  E[Y Z ] = 0,   cid:5  Corr[W , Y ] = 1 4 Corr[W , Z ] = − 3 Corr[X, Z ] = 108 169 Corr[Y , Z ] = − 3  26  26  3 4  4   cid:5  × 1  cid:5   cid:5   472 169  3 4  1 4   cid:6   1 2  3 13  16 13  = 0,   cid:6   cid:6  = 1 4 ,  cid:6  = − 3 26 ,  cid:6   cid:6  = 108 169 , = − 3 26 .  = 0,  1 2  3 13  2  2  2  13 , Cov[W , X] = 8  13  13  Cov[X, Y ] = 8 13   cid:5   cid:5  − 1  cid:5  − 1 Cov[W , Y ] = 1 2  cid:5  Cov[W , Z ] = 0 − 1  cid:5  − 16  cid:5  − 16 Cov[X, Z ] = 12 13 Cov[Y , Z ] = 0 − 1  cid:6 −1 2  cid:6 −1 2  cid:6 −1 2  cid:6 −1 2  = −0.209,  = 0.577,  = 0.598,  = −0.361.  169  169  13  × 69 × 69 × 69  3 13  2  169  The correlations Corr[W , X] and Corr[X, Y ] are clearly zero; the remainder are given by  Finally, then, we can write down the correlation matrix:   1  0  ρ =  0.58 −0.21 −0.36  0.60  0 1  0 1 0  0.58  −0.21  0.60 −0.36  1   .  1204   30.13 GENERATING FUNCTIONS FOR JOINT DISTRIBUTIONS  As would be expected, X is uncorrelated with either W or Y , colour and face-value being two independent characteristics. Positive correlations are to be expected between W and Y and between X and Z ; both correlations are fairly strong. Moderate anticorrelations exist between Z and both W and Y , reﬂecting the fact that it is impossible for W and Y to be positive if Z is positive.  cid:2   Finally, let us suppose that the random variables Xi, i = 1, 2, . . . , n, are related to a second set of random variables Yk = Yk X1, X2, . . . , Xn , k = 1, 2, . . . , m. By expanding each Yk as a Taylor series as in  30.137  and inserting the resulting expressions into the deﬁnition of the covariance  30.133 , we ﬁnd that the elements of the covariance matrix for the Yk variables are given by  Cov[Yk, Yl] ≈ cid:4    cid:7    cid:4    cid:8  cid:7    cid:8   ∂Yk ∂Xi  ∂Yl ∂Xj  i  j  Cov[Xi, Xj].  It is straightforward to show that this relation is exact if the Yk are linear combinations of the Xi. Equation  30.140  can then be written in matrix form as   30.140    30.141   VY = SVXST,  where VY and VX are the covariance matrices of the Yk and Xi variables respec- tively and S is the rectangular m × n matrix with elements Ski = ∂Yk ∂Xi.  30.13 Generating functions for joint distributions  It is straightforward to generalise the discussion of generating function in section 30.7 to joint distributions. For a multivariate distribution f X1, X2, . . . , Xn  of non-negative integer random variables Xi, i = 1, 2, . . . , n, we deﬁne the probability generating function to be  Φ t1, t2, . . . , tn  = E[tX1  1 tX2  2  ··· tXn n ].  As in the single-variable case, we may also deﬁne the closely related moment generating function, which has wider applicability since it is not restricted to non-negative integer random variables but can be used with any set of discrete or continuous random variables Xi  i = 1, 2, . . . , n . The MGF of the multivariate distribution f X1, X2, . . . , Xn  is deﬁned as  M t1, t2, . . . , tn  = E[et1X1 et2X2 ··· etnXn ] = E[et1X1+t2X2+···+tnXn ]  and may be used to evaluate  joint  moments of f X1, X2, . . . , Xn . By performing a derivation analogous to that presented for the single-variable case in subsection 30.7.2, it can be shown that   30.142    30.143   E[Xm1  1 Xm2  2  ··· Xmn  n ] =  ∂m1+m2+···+mnM 0, 0, . . . , 0   ∂tm1  1 ∂tm2  2  ··· ∂tmn  n  .  1205   PROBABILITY  Finally we note that, by analogy with the single-variable case, the characteristic function and the cumulant generating function of a multivariate distribution are deﬁned respectively as  C t1, t2, . . . , tn  = M it1, it2, . . . , itn   and  K t1, t2, . . . , tn  = ln M t1, t2, . . . , tn .   cid:1 Suppose that the random variables Xi, i = 1, 2, . . . , n, are described by the PDF   cid:21   where the column vector x =  x1 x2 is a normalisation constant such that ···  ∞ f x  dnx ≡  −∞  −∞  Find the MGF of f x .  f x  = f x1, x2, . . . , xn  = N exp − 1  cid:21  ∞   cid:21  ∞  2  xTAx ,  ···   cid:21  ∞ −∞ f x1, x2, . . . , xn  dx1 dx2 ··· dxn = 1.  cid:21   From  30.142 , the MGF is given by  xn T, A is an n × n symmetric matrix and N  M t1, t2, . . . , tn  = N ···  t2  where the column vector t =  t1 we begin by noting that  ∞ exp − 1  2  xTAx + tTx  dnx,   30.144   tn T. In order to evaluate this multiple integral,  xTAx − 2tTx =  x − A−1t TA x − A−1t  − tTA−1t,  which is the matrix equivalent of ‘completing the square’. Using this expression in  30.144   and making the substitution y = x − A−1t, we obtain M t1, t2, . . . , tn  = c exp  1 2  tTA−1t ,   30.145   where the constant c is given by   cid:21   c = N  yTAy  dny.  ∞ exp − 1  2  From the normalisation condition for N, we see that c = 1, as indeed it must be in order that M 0, 0, . . . , 0  = 1.  cid:2   30.14 Transformation of variables in joint distributions  Suppose the random variables Xi, i = 1, 2, . . . , n, are described by the multivariate PDF f x1, x2. . . , xn . If we wish to consider random variables Yj, j = 1, 2, . . . , m, related to the Xi by Yj = Yj X1, X2, . . . , Xm  then we may calculate g y1, y2, . . . , ym , the PDF for the Yj, in a similar way to that in the univariate case by demanding that  f x1, x2. . . , xn  dx1 dx2 ··· dxn = g y1, y2, . . . , ym  dy1 dy2 ··· dym.  From the discussion of changing the variables in multiple integrals given in  chapter 6 it follows that, in the special case where n = m,  g y1, y2, . . . , ym  = f x1, x2. . . , xn J,  1206   30.15 IMPORTANT JOINT DISTRIBUTIONS   cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20    cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  cid:20  ,  where  =  . . . . . .  J ≡ ∂ x1, x2. . . , xn  ∂ y1, y2, . . . , yn   ∂x1 ∂y1 ... ∂x1 ∂yn is the Jacobian of the xi with respect to the yj.  cid:1 Suppose that the random variables Xi, i = 1, 2, . . . , n, are independent and Gaussian dis- tributed with means µi and variances σ2 Zi =  Xi − µi  σi, i = 1, 2, . . . , n. By considering an elemental spherical shell in Z-space, i respectively. Find the PDF for the new variables ﬁnd the PDF of the chi-squared random variable χ2  ∂xn ∂y1 ... ∂xn ∂yn   cid:11   . . .  n  n =  i=1 Z 2 i .  Since the Xi are independent random variables,  f x1, x2, . . . , xn  = f x1 f x2 ··· f xn  =  1   2π n 2σ1σ2 ··· σn  exp  To derive the PDF for the variables Zi, we require  f x1, x2, . . . , xn  dx1 dx2 ··· dxn = g z1, z2, . . . , zn  dz1 dz2 ··· dzn,  and, noting that dzi = dxi σi, we obtain   cid:16   − n cid:4   i=1   cid:17   .   xi − µi 2  2σ2 i  n cid:4    cid:31   z2 i  .   cid:30  − 1  cid:11  2  g z1, z2, . . . , zn  =  1   2π n 2  exp  i=1 i=1 Z 2 g z1, z2, . . . , zn  dz1 dz2 ··· dzn = h χ2  n  n dχ2 n.  Let us now consider the random variable χ2  i , which we may regard as the square of the distance from the origin in the n-dimensional Z-space. We now require that  n =  If we consider the inﬁnitesimal volume dV = dz1 dz2 ··· dzn to be that enclosed by the n-dimensional spherical shell of radius χn and thickness dχn then we may write dV = Aχn−1  n dχn, for some constant A. We thus obtain ∝ exp − 1 n χn−1 2 χ2 n = 2χn dχn. Thus we see that the PDF for χ2 where we have used the fact that dχ2 by n  = B exp − 1 n χn−2  cid:21  ∞ 2 χ2  n dχn ∝ exp − 1 n χn−2 2 χ2  for some constant B. This constant may be determined from the normalisation condition  n is given  n dχ2 n,  n dχ2  h χ2  h χ2  n  n  ,  and is found to be B = [2n 2Γ  1 discussed in subsection 30.9.4.  cid:2   h χ2  n  dχ2  n = 1  0 2 n ]  −1. This is the nth-order chi-squared distribution  30.15 Important joint distributions  In this section we will examine two important multivariate distributions, the multinomial distribution, which is an extension of the binomial distribution, and the multivariate Gaussian distribution.  1207   PROBABILITY  30.15.1 The multinomial distribution  The binomial distribution describes the probability of obtaining x ‘successes’ from n independent trials, where each trial has only two possible outcomes. This may be generalised to the case where each trial has k possible outcomes with respective probabilities p1, p2, . . . , pk. If we consider the random variables Xi, i = 1, 2, . . . , n, to be the number of outcomes of type i in n trials then we may calculate their joint probability function  f x1, x2, . . . , xk  = Pr X1 = x1, X2 = x2,  . . . , Xk = xk ,  where we must have outcomes of type 1, followed by x2 outcomes of type 2 etc. is given by  k i=1 xi = n. In n trials the probability of obtaining x1   cid:11   However, the number of distinguishable permutations of this result is  px1 1 px2  2  ··· pxk k .  n!  x1!x2!··· xk!  ,  and thus  f x1, x2, . . . , xk  =  n!  x1!x2!··· xk!  px1 1 px2  2  ··· pxk k .   30.146   This is the multinomial probability distribution.  If k = 2 then the multinomial distribution reduces to the familiar binomial distribution. Although in this form the binomial distribution appears to be a function of two random variables, it must be remembered that, in fact, since  p2 = 1 − p1 and x2 = n − x1, the distribution of X1 is entirely determined by the  parameters p and n. That X1 has a binomial distribution is shown by remembering that it represents the number of objects of a particular type obtained from sampling with replacement, which led to the original deﬁnition of the binomial distribution. In fact, any of the random variables Xi has a binomial distribution, i.e. the marginal distribution of each Xi is binomial with parameters n and pi. It immediately follows that  E[Xi] = npi  and  V [Xi]2 = npi 1 − pi .   30.147    cid:1 At a village f ˆete patrons were invited, for a 10 p entry fee, to pick without looking six tickets from a drum containing equal large numbers of red, blue and green tickets. If ﬁve or more of the tickets were of the same colour a prize of 100 p was awarded. A consolation award of 40 p was made if two tickets of each colour were picked. Was a good time had by all?  In this case, all types of outcome  red, blue and green  have the same probabilities. The probability of obtaining any given combination of tickets is given by the multinomial distribution with n = 6, k = 3 and pi = 1  3 , i = 1, 2, 3.  1208   30.15 IMPORTANT JOINT DISTRIBUTIONS   i  The probability of picking six tickets of the same colour is given by  Pr  six of the same colour  = 3 × 6!  1 3  6!0!0!   cid:7    cid:8   6   cid:7    cid:8   0   cid:7    cid:8   0  The factor of 3 is present because there are three diﬀerent colours.   ii  The probability of picking ﬁve tickets of one colour and one ticket of another  colour is  Pr ﬁve of one colour; one of another  = 3 × 2 × 6!  5!1!0!  1 3   cid:7   =  1  .  243   cid:8   1   cid:7    cid:8   1 3   cid:7    cid:8   5  1 3  1 3  1 3  0  =  4 81  .  The factors of 3 and 2 are included because there are three ways to choose the colour of the ﬁve matching tickets, and then two ways to choose the colour of the remaining ticket.   iii  Finally, the probability of picking two tickets of each colour is   cid:7   2   cid:8   2   cid:7    cid:8   2  1 3  1 3  =  10 81  .   cid:7    cid:8   cid:8   1 3  6!  2!2!2!   cid:8    cid:7   Thus the expected return to any patron was, in pence,  Pr  two of each colour  =   cid:7   100  1  243  +  4 81  +  40 × 10  81  = 10.29.  A good time was had by all but the stallholder!  cid:2   30.15.2 The multivariate Gaussian distribution  A particularly interesting multivariate distribution is provided by the generalisa- tion of the Gaussian distribution to multiple random variables Xi, i = 1, 2, . . . , n. If the expectation value of Xi is E Xi  = µi then the general form of the PDF is given by   cid:13  − 1  2   cid:4    cid:4   i  j   cid:14  aij xi − µi  xj − µj   ,  f x1, x2, . . . , xn  = N exp  where aij = aji and N is a normalisation constant that we give below. If we write the column vectors x =  x1 x2 µn T, and denote the matrix with elements aij by A then  ···  ···   cid:18 − 1  cid:19  xn T and µ =  µ1 µ2 2  x − µ TA x − µ   ,  f x  = f x1, x2, . . . , xn  = N exp  where A is symmetric. Using the same method as that used to derive  30.145  it is straightforward to show that the MGF of f x  is given by  where the column matrix t =  t1  t2  M t1, t2, . . . , tn  = exp ···  E[XiXj] =  ∂2M 0, 0, . . . , 0   tn T. From the MGF, we ﬁnd that = µiµj +  A−1 ij ,   cid:5    cid:6   ,  tTA−1t  µTt + 1 2  ∂ti∂tj  1209   PROBABILITY  and thus, using  30.135 , we obtain  Cov[Xi, Xj] = E[ Xi − µi  Xj − µj ] =  A−1 ij .  Hence A is equal to the inverse of the covariance matrix V of the Xi, see  30.139 . Thus, with the correct normalisation, f x  is given by   cid:18 − 1  cid:19  2  x − µ TV−1 x − µ   .  f x  =   2π n 2 det V 1 2  exp  1   30.148    cid:1 Evaluate the integral   cid:21   I =  ∞ exp   cid:19    cid:18 − 1 2  x − µ TV−1 x − µ   cid:21   dnx,  where V is a symmetric matrix, and hence verify the normalisation in  30.148 . We begin by making the substitution y = x − µ to obtain yTV−1y  dny.  I =  ∞ exp − 1  2  Thus we can write I as  Since V is a symmetric matrix, it may be diagonalised by an orthogonal transformation to the new set of variables y cid:7  = STy, where S is the orthogonal matrix with the normalised eigenvectors of V as its columns  see section 8.16 . In this new basis, the matrix V becomes V cid:7   where the λi are the eigenvalues of V. Also, since S is orthogonal, det S = ±1, and so   cid:21  ∞  cid:21  ∞  cid:21  ∞ n cid:3   −∞  −∞  I =  =  i=1  −∞ exp  = STVS = diag λ1, λ2, . . . , λn , dny = det S dny cid:7   cid:21  ∞  cid:30   = dny cid:7   cid:31    cid:30   cid:31   .  ···  −∞ exp − y i 2λi   cid:7   2  2   cid:7    cid:7    cid:7  2  dy  y i 2λi   cid:7  1 dy  ··· dy  − n cid:4   cid:1  ∞ i =  2π n 2 λ1λ2 ··· λn 1 2,  cid:7  −∞ exp −αy2  dy =  π α 1 2  see subsection   30.149   i=1  n  dy  where we have used the standard integral 6.4.2 . From section 8.16, however, we note that the product of eigenvalues in  30.149  is equal to det V. Thus we ﬁnally obtain  and hence the normalisation in  30.148  ensures that f x  integrates to unity.  cid:2   I =  2π n 2 det V 1 2,  The above example illustrates some importants points concerning the multi-  cid:7  i are independent variate Gaussian distribution. In particular, we note that the Y Gaussian variables with mean zero and variance λi. Thus, given a general set of n Gaussian variables x with means µ and covariance matrix V, one can always perform the above transformation to obtain a new set of variables y cid:7  , which are linear combinations of the old ones and are distributed as independent Gaussians with zero mean and variances λi.  This result is extremely useful in proving many of the properties of the mul-  1210   tivariate Gaussian. For example, let us consider the quadratic form  multiplied by 2  appearing in the exponent of  30.148  and write it as χ2  n, i.e.   30.150   From  30.149 , we see that we may also write it as  30.16 EXERCISES  n =  x − µ TV−1 x − µ . χ2  n cid:4   i=1   cid:7   2  y i λi  ,  χ2 n =  which is the sum of n independent Gaussian variables with mean zero and unit variance. Thus, as our notation implies, the quantity χ2 n is distributed as a chi- squared variable of order n. As illustrated in exercise 30.40, if the variables Xi are required to satisfy m linear constraints of the form n deﬁned in  30.150  is distributed as a chi-squared variable of order n − m.  i=1 ciXi = 0 then χ2   cid:11   n  30.16 Exercises  30.1  By shading or numbering Venn diagrams, determine which of the following are valid relationships between events. For those that are, prove the relationship using de Morgan’s laws.   a     ¯X ∪ Y   = X ∩ ¯Y .  b  ¯X ∪ ¯Y =  X ∪ Y  .  X ∪ Y   ∩ Z =  X ∪ Z   ∩ Y .  d  X ∪  Y ∩ Z   =  X ∪ ¯Y   ∩ ¯Z .  e  X ∪  Y ∩ Z   =  X ∪ ¯Y   ∪ ¯Z .   c   30.2  Given that events X, Y and Z satisfy  30.3  30.4  30.5   X ∩ Y   ∪  Z ∩ X  ∪   ¯X ∪ ¯Y   =  Z ∪ ¯Y   ∪ {[  ¯Z ∪ ¯X  ∪   ¯X ∩ Z  ] ∩ Y },  prove that X ⊃ Y , and that either X ∩ Z = ∅ or Y ⊃ Z .  A and B each have two unbiased four-faced dice, the four faces being numbered 1, 2, 3, 4. Without looking, B tries to guess the sum x of the numbers on the bottom faces of A’s two dice after they have been thrown onto a table. If the guess is correct B receives x2 euros, but if not he loses x euros.  Determine B’s expected gain per throw of A’s dice when he adopts each of the  following strategies:   a  he selects x at random in the range 2 ≤ x ≤ 8;   b  he throws his own two dice and guesses x to be whatever they indicate;  c  he takes your advice and always chooses the same value for x. Which number  would you advise?  Use the method of induction to prove equation  30.16 , the probability addition law for the union of n general events. Two duellists, A and B, take alternate shots at each other, and the duel is over when a shot  fatal or otherwise!  hits its target. Each shot ﬁred by A has a probability α of hitting B, and each shot ﬁred by B has a probability β of hitting A. Calculate the probabilities P1 and P2, deﬁned as follows, that A will win such a duel: P1, A ﬁres the ﬁrst shot; P2, B ﬁres the ﬁrst shot.  If they agree to ﬁre simultaneously, rather than alternately, what is the proba-  bility P3 that A will win, i.e. hit B without being hit himself?  1211   PROBABILITY  30.6  X1, X2, . . . , Xn are independent, identically distributed, random variables drawn from a uniform distribution on [0, 1]. The random variables A and B are deﬁned by  A = min X1, X2, . . . , Xn ,  For any ﬁxed k such that 0 ≤ k ≤ 1  B = max X1, X2, . . . , Xn .  A ≤ k  2 , ﬁnd the probability, pn, that both and  B ≥ 1 − k.  30.7  Check your general formula by considering directly the cases  a  k = 0,  b  k = 1 2 ,  c  n = 1 and  d  n = 2. A tennis tournament is arranged on a straight knockout basis for 2n players, and for each round, except the ﬁnal, opponents for those still in the competition are drawn at random. The quality of the ﬁeld is so even that in any match it is equally likely that either player will win. Two of the players have surnames that begin with ‘Q’. Find the probabilities that they play each other  in the ﬁnal,   a   b  at some stage in the tournament.  30.8  This exercise shows that the odds are hardly ever ‘evens’ when it comes to dice rolling.  30.9  30.10   a  Gamblers A and B each roll a fair six-faced die, and B wins if his score is  strictly greater than A’s. Show that the odds are 7 to 5 in A’s favour.   b  Calculate the probabilities of scoring a total T from two rolls of a fair die for T = 2, 3, . . . , 12. Gamblers C and D each roll a fair die twice and score respective totals TC and TD, D winning if TD > TC . Realising that the odds are not equal, D insists that C should increase her stake for each game. C agrees to stake £1.10 per game, as compared to D’s £1.00 stake. Who will show a proﬁt?  An electronics assembly ﬁrm buys its microchips from three diﬀerent suppliers; half of them are bought from ﬁrm X, whilst ﬁrms Y and Z supply 30% and 20%, respectively. The suppliers use diﬀerent quality-control procedures and the percentages of defective chips are 2%, 4% and 4% for X, Y and Z , respectively. The probabilities that a defective chip will fail two or more assembly-line tests are 40%, 60% and 80%, respectively, whilst all defective chips have a 10% chance of escaping detection. An assembler ﬁnds a chip that fails only one test. What is the probability that it came from supplier X? As every student of probability theory will know, Bayesylvania is awash with natives, not all of whom can be trusted to tell the truth, and lost, and apparently somewhat deaf, travellers who ask the same question several times in an attempt to get directions to the nearest village.  One such traveller ﬁnds himself at a T-junction in an area populated by the Asciis and Bisciis in the ratio 11 to 5. As is well known, the Biscii always lie, but the Ascii tell the truth three quarters of the time, giving independent answers to all questions, even to immediately repeated ones.   a  The traveller asks one particular native twice whether he should go to the left or to the right to reach the local village. Each time he is told ‘left’. Should he take this advice, and, if he does, what are his chances of reaching the village?   b  The traveller then asks the same native the same question a third time, and for a third time receives the answer ‘left’. What should the traveller do now? Have his chances of ﬁnding the village been altered by asking the third question?  1212   30.16 EXERCISES  30.11  A boy is selected at random from amongst the children belonging to families with n children. It is known that he has at least two sisters. Show that the probability  that he has k − 1 brothers is   n − 1 !   2n−1 − n  k − 1 ! n − k !  ,  for 1 ≤ k ≤ n − 2 and zero for other values of k. Assume that boys and girls are  equally likely. Villages A, B, C and D are connected by overhead telephone lines joining AB, AC, BC, BD and CD. As a result of severe gales, there is a probability p  the same for each link  that any particular link is broken.   a  Show that the probability that a call can be made from A to B is   b  Show that the probability that a call can be made from D to A is  1 − p2 − 2p3 + 3p4 − p5.  1 − 2p2 − 2p3 + 5p4 − 2p5.  A set of 2N + 1 rods consists of one of each integer length 1, 2, . . . , 2N, 2N + 1. Three, of lengths a, b and c, are selected, of which a is the longest. By considering the possible values of b and c, determine the number of ways in which a non- degenerate triangle  i.e. one of non-zero area  can be formed  i  if a is even, and  ii  if a is odd. Combine these results appropriately to determine the total number of non-degenerate triangles that can be formed with the 2N + 1 rods, and hence show that the probability that such a triangle can be formed from a random selection  without replacement  of three rods is   N − 1  4N + 1   2 4N2 − 1   .  30.12  30.13  30.14  A certain marksman never misses his target, which consists of a disc of unit radius with centre O. The probability that any given shot will hit the target within a  distance t of O is t2, for 0 ≤ t ≤ 1. The marksman ﬁres n independendent shots  at the target, and the random variable Y is the radius of the smallest circle with centre O that encloses all the shots. Determine the PDF for Y and hence ﬁnd the expected area of the circle.  The shot that is furthest from O is now rejected and the corresponding circle  determined for the remaining n − 1 shots. Show that its expected area is  30.15  The duration  in minutes  of a telephone call made from a public call-box is a random variable T . The probability density function of T is  n − 1  n + 1  π.  0  f t  =  t < 0,  0 ≤ t < 1, t ≥ 1,  1 2 ke  −2t  where k is a constant. To pay for the call, 20 pence has to be inserted at the beginning, and a further 20 pence after each subsequent half-minute. Determine by how much the average cost of a call exceeds the cost of a call of average length charged at 40 pence per minute. Kittens from diﬀerent litters do not get on with each other, and ﬁghting breaks out whenever two kittens from diﬀerent litters are present together. A cage initially contains x kittens from one litter and y from another. To quell the  30.16  1213   30.17  PROBABILITY  N x, y  =  x  +  y  .  y + 1  x + 1  ﬁghting, kittens are removed at random, one at a time, until peace is restored. Show, by induction, that the expected number of kittens ﬁnally remaining is  If the scores in a cup football match are equal at the end of the normal period of play, a ‘penalty shoot-out’ is held in which each side takes up to ﬁve shots  from the penalty spot  alternately, the shoot-out being stopped if one side acquires an unassailable lead  i.e. has a lead greater than its opponents have shots remaining . If the scores are still level after the shoot-out a ‘sudden death’ competition takes place.  In sudden death each side takes one shot and the competition is over if one side scores and the other does not; if both score, or both fail to score, a further shot is taken by each side, and so on. Team 1, which takes the ﬁrst penalty, has a probability p1, which is independent of the player involved, of scoring and a  probability q1  = 1 − p1  of missing; p2 and q2 are deﬁned likewise.  Deﬁne Pr i : x, y  as the probability that team i has scored x goals after y attempts, and let f M  be the probability that the shoot-out terminates after a total of M shots.   a  Prove that the probability that ‘sudden death’ will be needed is  5 cid:4   r=0  f 11+  =   5Cr 2 p1p2 r q1q2 5−r.   b  Give reasoned arguments  preferably without ﬁrst looking at the expressions  involved  which show that   cid:12   2N−6 cid:4   r=0  f M = 2N  =  for N = 3, 4, 5 and  f M = 2N + 1  =  for N = 3, 4.  p2 Pr 1 : r, N  Pr 2 : 5 − N + r, N − 1  + q2 Pr 1 : 6 − N + r, N  Pr 2 : r, N − 1   cid:12  2N−5 cid:4   p1 Pr 1 : 5 − N + r, N  Pr 2 : r, N  + q1 Pr 1 : r, N  Pr 2 : 5 − N + r, N   r=0   cid:15    cid:15    c  Give an explicit expression for Pr i : x, y  and hence show that if the teams  are so well matched that p1 = p2 = 1 2 then   cid:8   cid:8    cid:7   cid:7   2N−6 cid:4  2N−5 cid:4   r=0  r=0  1 22N  1 22N  f 2N  =  f 2N + 1  =  N! N − 1 !6  r! N − r ! 6 − N + r ! 2N − 6 − r !  ,  r! N − r ! 5 − N + r ! 2N − 5 − r !  .   N! 2   d  Evaluate these expressions to show that, expressing f M  in units of 2  −8, we  have  M f M   6 8  7 24  8 42  9 56  10 63  11+ 63  Give a simple explanation of why f 10  = f 11+ .  1214   30.16 EXERCISES  30.18  30.19  30.20  A particle is conﬁned to the one-dimensional space 0 ≤ x ≤ a, and classically  it can be in any small interval dx with equal probability. However, quantum mechanics gives the result that the probability distribution is proportional to sin2 nπx a , where n is an integer. Find the variance in the particle’s position in both the classical and quantum-mechanical pictures, and show that, although they diﬀer, the latter tends to the former in the limit of large n, in agreement with the correspondence principle of physics. A continuous random variable X has a probability density function f x ; the corresponding cumulative probability function is F x . Show that the random variable Y = F X  is uniformly distributed between 0 and 1. For a non-negative integer random variable X, in addition to the probability generating function ΦX  t  deﬁned in equation  30.71 , it is possible to deﬁne the probability generating function  where gn is the probability that X > n.   a  Prove that ΦX and ΨX are related by  ∞ cid:4   n=0  ΨX t  =  gntn,  ΨX  t  =  1 − ΦX  t  1 − t  .   b  Show that E[X] is given by ΨX 1  and that the variance of X can be  expressed as 2Ψ  X 1  + ΨX 1  − [ΨX 1 ]2.  cid:7    c  For a particular random variable X, the probability that X > n is equal to −2.  αn+1, with 0 < α < 1. Use the results in  b  to show that V [X] = α 1 − α   30.21  This exercise is about interrelated binomial trials.   a  In two sets of binomial trials T and t, the probabilities that a trial has a successful outcome are P and p, respectively, with corresponding probabilites  of failure of Q = 1 − P and q = 1 − p. One ‘game’ consists of a trial T ,  followed, if T is successful, by a trial t and then a further trial T . The two trials continue to alternate until one of the T -trials fails, at which point the game ends. The score S for the game is the total number of successes in the t-trials. Find the PGF for S and use it to show that  E[S ] =  V [S ] =  P p Q  ,  P p 1 − P q   .  Q2   b  Two normal unbiased six-faced dice A and B are rolled alternately starting with A; if A shows a 6 the experiment ends. If B shows an odd number no points are scored, if it shows a 2 or a 4 then one point is scored, whilst if it records a 6 then two points are awarded. Find the average and standard deviation of the score for the experiment and show that the latter is the greater.  30.22  Use the formula obtained in subsection 30.8.2 for the moment generating function of the geometric distribution to determine the CGF, Kn t , for the number of trials needed to record n successes. Evaluate the ﬁrst four cumulants, and use them to conﬁrm the stated results for the mean and variance, and to show that the distribution has skewness and kurtosis given, respectively, by  2 − p √ n 1 − p   and  3 +  6 − 6p + p2 n 1 − p   .  1215   30.23  30.24  30.25  30.26  30.27  30.28  30.29  30.30  30.31  PROBABILITY  A point P is chosen at random on the circle x2 + y2 = 1. The random variable X denotes the distance of P from  1, 0 . Find the mean and variance of X and the probability that X is greater than its mean. As assistant to a celebrated and imperious newspaper proprietor, you are given the job of running a lottery, in which each of his ﬁve million readers will have an equal independent chance, p, of winning a million pounds; you have the job of choosing p. However, if nobody wins it will be bad for publicity, whilst if more than two readers do so, the prize cost will more than oﬀset the proﬁt from extra circulation – in either case you will be sacked! Show that, however you choose p, there is more than a 40% chance you will soon be clearing your desk. The number of errors needing correction on each page of a set of proofs follows a Poisson distribution of mean µ. The cost of the ﬁrst correction on any page is α and that of each subsequent correction on the same page is β. Prove that the average cost of correcting a page is  α + β µ − 1  −  α − β e  −µ.  In the game of Blackball, at each turn Muggins draws a ball at random from a bag containing ﬁve white balls, three red balls and two black balls; after being recorded, the ball is replaced in the bag. A white ball earns him $1, whilst a red ball gets him $2; in either case, he also has the option of leaving with his current winnings or of taking a further turn on the same basis. If he draws a black ball the game ends and he loses all he may have gained previously. Find an expression for Muggins’ expected return if he adopts the strategy of drawing up to n balls, provided he has not been eliminated by then.  Show that, as the entry fee to play is $3, Muggins should be dissuaded from playing Blackball, but, if that cannot be done, what value of n would you advise him to adopt? Show that, for large r, the value at the maximum of the PDF for the gamma distribution of order r with parameter λ is approximately λ  A husband and wife decide that their family will be complete when it includes two boys and two girls – but that this would then be enough! The probability that a new baby will be a girl is p. Ignoring the possibility of identical twins, show that the expected size of their family is  √ 2π r − 1 .   cid:7    cid:8   where q = 1 − p.  − 1 − pq  ,  2  1 pq  The probability distribution for the number of eggs in a clutch is Po λ , and the probability that each egg will hatch is p  independently of the size of the clutch . Show by direct calculation that the probability distribution for the number of chicks that hatch is Po λp  and so justify the assumptions made in the worked example at the end of subsection 30.7.1. A shopper buys 36 items at random in a supermarket, where, because of the sales tax imposed, the ﬁnal digit  the number of pence  in the price is uniformly and randomly distributed from 0 to 9. Instead of adding up the bill exactly, she rounds each item to the nearest 10 pence, rounding up or down with equal probability if the price ends in a ‘5’. Should she suspect a mistake if the cashier asks her for 23 pence more than she estimated? Under EU legislation on harmonisation, all kippers are to weigh 0.2000 kg, and vendors who sell underweight kippers must be ﬁned by their government. The weight of a kipper is normally distributed, with a mean of 0.2000 kg and a standard deviation of 0.0100 kg. They are packed in cartons of 100 and large quantities of them are sold.  Every day, a carton is to be selected at random from each vendor and tested  1216   30.16 EXERCISES  according to one of the following schemes, which have been approved for the purpose.   a  The entire carton is weighed, and the vendor is ﬁned 2500 euros if the  average weight of a kipper is less than 0.1975 kg.   b  Twenty-ﬁve kippers are selected at random from the carton; the vendor is  ﬁned 100 euros if the average weight of a kipper is less than 0.1980 kg.   c  Kippers are removed one at a time, at random, until one has been found  that weighs more than 0.2000 kg; the vendor is ﬁned 4n n − 1  euros, where  n is the number of kippers removed.  Which scheme should the Chancellor of the Exchequer be urging his government to adopt? In a certain parliament, the government consists of 75 New Socialites and the opposition consists of 25 Preservatives. Preservatives never change their mind, always voting against government policy without a second thought; New Socialites vote randomly, but with probability p that they will vote for their party leader’s policies.  Following a decision by the New Socialites’ leader to drop certain manifesto commitments, N of his party decide to vote consistently with the opposition. The leader’s advisors reluctantly admit that an election must be called if N is such that, at any vote on government policy, the chance of a simple majority in favour would be less than 80%. Given that p = 0.8, estimate the lowest value of N that would precipitate an election. A practical-class demonstrator sends his twelve students to the storeroom to collect apparatus for an experiment, but forgets to tell each which type of component to bring. There are three types, A, B and C, held in the stores  in large numbers  in the proportions 20%, 30% and 50%, respectively, and each student picks a component at random. In order to set up one experiment, one unit each of A and B and two units of C are needed. Let Pr N  be the probability that at least N experiments can be set up.   a  Evaluate Pr 3 .  b  Find an expression for Pr N  in terms of k1 and k2, the numbers of compo- nents of types A and B respectively selected by the students. Show that Pr 2  can be written in the form  Pr 2  =  0.5 12  12Ci  0.4 i  12−iCj  0.6 j.  6 cid:4   i=2  8−i cid:4   j=2  30.32  30.33   c  By considering the conditions under which no experiments can be set up,  show that Pr 1  = 0.9145.  The random variables X and Y take integer values, x and y, both ≥ 1, and such that 2x + y ≤ 2a, where a is an integer greater than 1. The joint probability  30.34  within this region is given by  Pr X = x, Y = y  = c 2x + y ,  where c is a constant, and it is zero elsewhere.  Show that the marginal probability Pr X = x  is  Pr X = x  =  6 a − x  2x + 2a + 1   a a − 1  8a + 5   ,  and obtain expressions for Pr Y = y ,  a  when y is even and  b  when y is odd. Show further that  E[Y ] =  6a2 + 4a + 1  .  8a + 5  1217   PROBABILITY  30.35  30.36  [ You will need the results about series involving the natural numbers given in subsection 4.2.5. ] The continuous random variables X and Y have a joint PDF proportional to  xy x − y 2 with 0 ≤ x ≤ 1 and 0 ≤ y ≤ 1. Find the marginal distributions coeﬃcient − 2 3 . ities pn. A second random variable Y is deﬁned as Y =  X − µ 2, where µ is the  for X and Y and show that they are negatively correlated with correlation  A discrete random variable X takes integer values n = 0, 1, . . . , N with probabil-  expectation value of X. Prove that the covariance of X and Y is given by  Cov[X, Y ] =  n3pn − 3µ  n2pn + 2µ3.  N cid:4   n=0  N cid:4   n=0  Now suppose that X takes all of its possible values with equal probability, and hence demonstrate that two random variables can be uncorrelated, even though one is deﬁned in terms of the other. Two continuous random variables X and Y have a joint probability distribution  30.37  f x, y  = A x2 + y2 ,  where A is a constant and 0 ≤ x ≤ a, 0 ≤ y ≤ a. Show that X and Y are negatively correlated with correlation coeﬃcient −15 73. By sketching a rough contour  map of f x, y  and marking oﬀ the regions of positive and negative correlation, convince yourself that this  perhaps counter-intuitive  result is plausible.  A continuous random variable X is uniformly distributed over the interval [−c, c]. Z is deﬁned as the median of that sample. Show that Z is distributed over [−c, c]  A sample of 2n + 1 values of X is selected at random and the random variable  with probability density function  30.38  fn z  =   2n + 1 !   n! 2 2c 2n+1   c2 − z2 n.  30.39  Find the variance of Z .  Show that, as the number of trials n becomes large but npi = λi, i = 1, 2, . . . , k− 1,  remains ﬁnite, the multinomial probability distribution  30.146 ,  Mn x1, x2, . . . , xk  =  px1 1 px2  2  ··· pxk k ,  can be approximated by a multiple Poisson distribution with k − 1 factors:  n!  x1!x2!··· xk! k−1 cid:3    cid:7  n x1, x2, . . . , xk−1  =  M  e  −λi λxi xi!  i  .  i=1   Write  k−1 δ, either exactly or approximately. You will need to use n! ≈ n cid:4 [ n −  cid:4  !] and  1 − a n n ≈ e  pi = δ and express all terms involving subscript k in terms of n and −a for large n.   i   cid:11    cid:7  n when summed over all values of x1, x2, . . . , xk−1   a  Verify that the terms of M  add up to unity.   b  If k = 7 and λi = 9 for all i = 1, 2, . . . , 6, estimate, using the appropriate Gaussian approximation, the chance that at least three of x1, x2, . . . , x6 will be 15 or greater.  30.40  The variables Xi, i = 1, 2, . . . , n, are distributed as a multivariate Gaussian, with means µi and a covariance matrix V. If the Xi are required to satisfy the linear  1218   30.17 HINTS AND ANSWERS   cid:11   n i=1 ciXi = 0, where the ci are constants  and not all equal to zero ,  constraint show that the variable  follows a chi-squared distribution of order n − 1.  n =  x − µ TV−1 x − µ  χ2  30.17 Hints and answers  30.1 30.3  30.5 30.7   a  Yes,  b  no,  c  no,  d  no,  e  yes. Show that, if px 16 is the probability that the total will be x, then the corrsponding  gain is [px x2 + x − 16x] 16.  a  A loss of 0.36 euros;  b  a gain of 27 64 euros; P1 = α α + β − αβ    c  a gain of 2.5 euros, provided he takes your advice and guesses ‘5’ each time.  −1; P2 = α 1 − β  α + β − αβ   If pr is the probability that before the rth round both players are still in the tournament  and therefore have not met each other , show that  −1; P3 = P2.  cid:8   cid:7   pr+1 =  pr  and hence that  pr =  1 2  r−1 2n+1−r − 1 2n − 1 −1.  − n−1  2n − 1   .  1 4  2n+1−r − 2 2n+1−r − 1  cid:11    a  The probability that they meet in the ﬁnal is pn = 2  b  The probability that they meet at some stage in the tournament is given by  the sum  r=1 pr 2n+1−r − 1   n  −1 = 2  − n−1 .  the event that the boy has at least two sisters. Apply Bayes’ theorem.  The relative probabilities are X : Y : Z = 50 : 36 : 8  in units of 10  with m running from 2 to N, to show that the total number of non-degenerate  −4 ; 25 47. Take Aj as the event that a family consists of j boys and n − j girls, and B as  i  For a even, the number of ways is 1 + 3 + 5 + ··· +  a − 3 , and  ii  for a odd it is 2 + 4 + 6 + ··· +  a − 3 . Combine the results for a = 2m and a = 2m + 1, triangles is given by N 4N + 1  N − 1  6. The number of possible selections of a set of three rods is  2N + 1  2N  2N − 1  6. Show that k = e2 and that the average duration of a call is 1 minute. Let pn be the probability that the call ends during the interval 0.5 n − 1  ≤ t < 0.5n and cn = 20n be the corresponding cost. Prove that p1 = p2 = 1 4 and that pn = 1  −n, for n ≥ 3. It follows that the average cost is  2 e2 e − 1 e  −n. −2   e − 1 2 and the total charge is 5 e + 1   e − 1  = 10.82 pence more than the 40 pence a uniform rate  The arithmetico-geometric series has sum  3e  E[C] =  30 2  + 20  ne  n=3  2  would cost.  a  The scores must be equal, at r each, after ﬁve attempts each.  b  M can only be even if team 2 gets too far ahead  or drops too far behind  to be caught  or catch up , with conditional probability p2  or q2 . Conversely, M can only be odd as a result of a ﬁnal action by team 1.  c  Pr i : x, y  = yCxpx  d  If the match is still alive at the tenth kick, team 2 is just as likely to lose it as to take it into sudden death.  Show that dY  dX = f and use g y  = f x dx dy.  a  Use result  30.84  to show that the PGF for S is Q  1− P q − P pt . Then use  b  The PGF for the score is 6  21 − 10t − 5t2  and the average score is 10 3.  equations  30.74  and  30.76 .  i qy−x  .  i  The variance is 145 9 and the standard deviation is 4.01.  e2 e − 1   ∞ cid:4  −1 − 2e  30.9 30.11  30.13  30.15  30.17  30.19 30.21  1219   PROBABILITY   cid:11 ∞  30.23 Mean = 4 π. Variance = 2 −  16 π2 . Probability that X exceeds its mean  = 1 −  2 π  sin Consider, separately, 0, 1 and ≥ 2 errors on a page. Show that the maximum occurs at x =  r − 1  λ, and then use Stirling’s approx-  −1 2 π  = 0.561.  30.25 30.27  30.29 30.31  30.33 30.35  30.37  30.39  n=k Po n, λ  Bin n, p .  imation to ﬁnd the maximum value. Pr k chicks hatching  = There is not much to choose between the schemes. In  a  the critical value of  the standard variable is −2.5 and the average ﬁne would be 15.5 euros. For  cid:11 ∞  b  the corresponding ﬁgures are −1.0 and 15.9 euros. Scheme  c  is governed  cid:11 ∞ by a geometric distribution with p = q = 1 2 , and leads to an expected ﬁne n=1 4n n − 1   1 2  n. The sum can be evaluated by diﬀerentiating the result of n=1 pn = p  1 − p  with respect to p, and gives the expected ﬁne as 16 euros.   a  [12! 0.5 6 0.3 3 0.2 3]  6! 3! 3!  = 0.0624. You will need to establish the normalisation constant for the distribution  36 , the common mean value  3 5  and the common standard deviation  3 10 . The  marginal distributions are f x  = 3x 6x2 − 8x + 3 , and the same function of y. The covariance has the value −3 50, yielding a correlation of −2 3. A = 3  24a4 ; µX = µY = 5a 8; σ2 Y = 73a2 960; E[XY ] = 3a2 8; Cov[X, Y ] = −a2 64.  b  With the continuity correction Pr xi ≥ 15  = 0.0334. The probability that at least three are 15 or greater is 7.5 × 10  X = σ2  −4.  1220   31  Statistics  In this chapter, we turn to the study of statistics, which is concerned with the analysis of experimental data. In a book of this nature we cannot hope to do justice to such a large subject; indeed, many would argue that statistics belongs to the realm of experimental science rather than in a mathematics textbook. Nevertheless, physical scientists and engineers are regularly called upon to perform a statistical analysis of their data and to present their results in a statistical context. Therefore, we will concentrate on this aspect of a much more § extensive subject.  31.1 Experiments, samples and populations  We may regard the product of any experiment as a set of N measurements of some quantity x or set of quantities x, y, . . . , z. This set of measurements constitutes the data. Each measurement  or data item  consists accordingly of a single number xi or a set of numbers  xi, yi, . . . , , zi , where i = 1, . . . , , N. For the moment, we will assume that each data item is a single number, although our discussion can be extended to the more general case.  As a result of inaccuracies in the measurement process, or because of intrinsic variability in the quantity x being measured, one would expect the N measured values x1, x2, . . . , xN to be diﬀerent each time the experiment is performed. We may  §  There are, in fact, two separate schools of thought concerning statistics: the frequentist approach and the Bayesian approach. Indeed, which of these approaches is the more fundamental is still a matter of heated debate. Here we shall concentrate primarily on the more traditional frequentist approach  despite the preference of some of the authors for the Bayesian viewpoint! . For a fuller discussion of the frequentist approach one could refer to, for example, A. Stuart and K. Ord, Kendall’s Advanced Theory of Statistics, vol. 1  London: Edward Arnold, 1994  or J. F. Kenney and E. S. Keeping, Mathematics of Statistics  New York: Van Nostrand, 1954 . For a discussion of the Bayesian approach one might consult, for example, D. S. Sivia, Data Analysis: A Bayesian Tutorial  Oxford: Oxford University Press, 1996 .  1221   STATISTICS  therefore consider the xi as a set of N random variables. In the most general case, these random variables will be described by some N-dimensional joint probability § density function P  x1, x2, . . . , xN . In other words, an experiment consisting of N measurements is considered as a single random sample from the joint distribution  or population  P  x , where x denotes a point in the N-dimensional data space having coordinates  x1, x2, . . . , xN .  The situation is simpliﬁed considerably if the sample values xi are independent. In this case, the N-dimensional joint distribution P  x  factorises into the product of N one-dimensional distributions,  P  x  = P  x1 P  x2 ··· P  xN .   31.1   In the general case, each of the one-dimensional distributions P  xi  may be diﬀerent. A typical example of this occurs when N independent measurements are made of some quantity x but the accuracy of the measuring procedure varies between measurements.  It is often the case, however, that each sample value xi is drawn independently from the same population. In this case, P  x  is of the form  31.1 , but, in addition, P  xi  has the same form for each value of i. The measurements x1, x2, . . . , xN are then said to form a random sample of size N from the one-dimensional population P  x . This is the most common situation met in practice and, unless stated otherwise, we will assume from now on that this is the case.  31.2 Sample statistics  Suppose we have a set of N measurements x1, x2, . . . , xN. Any function of these measurements  that contains no unknown parameters  is called a sample statistic, or often simply a statistic. Sample statistics provide a means of characterising the data. Although the resulting characterisation is inevitably incomplete, it is useful to be able to describe a set of data in terms of a few pertinent numbers. We now discuss the most commonly used sample statistics.  §  In this chapter, we will adopt the common convention that P  x  denotes the particular probability density function that applies to its argument, x. This obviates the need to use a diﬀerent letter for the PDF of each new variable. For example, if X and Y are random variables with diﬀerent PDFs, then properly one should denote these distributions by f x  and g y , say. In our shorthand notation, these PDFs are denoted by P  x  and P  y , where it is understood that the functional form of the PDF may be diﬀerent in each case.  1222   31.2 SAMPLE STATISTICS  188.7 168.1  204.7 189.8  193.2 166.3  169.0 200.0  31.2.1 Averages  N cid:4   i=1  ¯x =  1 N  xi.  Table 31.1 Experimental data giving eight measurements of the round trip time in milliseconds for a computer ‘packet’ to travel from Cambridge UK to Cambridge MA.  The simplest number used to characterise a sample is the mean, which for N values xi, i = 1, 2, . . . , N, is deﬁned by  In words, the sample mean is the sum of the sample values divided by the number of values in the sample.  cid:1 Table 31.1 gives eight values for the round trip time in milliseconds for a computer ‘packet’ to travel from Cambridge UK to Cambridge MA. Find the sample mean.  Using  31.2  the sample mean in milliseconds is given by  ¯x = 1  8  188.7 + 204.7 + 193.2 + 169.0 + 168.1 + 189.8 + 166.3 + 200.0  1479.8  =  8  = 184.975.  Since the sample values in table 31.1 are quoted to an accuracy of one decimal place, it is usual to quote the mean to the same accuracy, i.e. as ¯x = 185.0.  cid:2   Strictly speaking the mean given by  31.2  is the arithmetic mean and this is by far the most common deﬁnition used for a mean. Other deﬁnitions of the mean are possible, though less common, and include   i  the geometric mean,   ii  the harmonic mean,   iii  the root mean square,   cid:30   N cid:3    cid:31 1 N  ¯xg =  xi  ,  i=1  N cid:11   cid:30  cid:11   ,   cid:31   ¯xh =  N i=1 1 xi  ¯xrms =  1 2  .  N  i=1 x2 N  i  1223   31.2    31.3    31.4    31.5    STATISTICS  It should be noted that, ¯x, ¯xh and ¯xrms would remain well deﬁned even if some sample values were negative, but the value of ¯xg could then become complex. The geometric mean should not be used in such cases.  cid:1 Calculate ¯xg, ¯xh and ¯xrms for the sample given in table 31.1.  The geometric mean is given by  31.3  to be  ¯xg =  188.7 × 204.7 × ··· × 200.0 1 8 = 184.4.  The harmonic mean is given by  31.4  to be  Finally, the root mean square is given by  31.5  to be  ¯xh =  ¯xrms =  8   1 188.7  +  1 204.7  + ··· +  1 200.0   cid:18   cid:19   8  188.72 + 204.72 + ··· + 200.02   1 2  1  = 183.9.  = 185.5.  cid:2   Two other measures of the ‘average’ of a sample are its mode and median. The mode is simply the most commonly occurring value in the sample. A sample may possess several modes, however, and thus it can be misleading in such cases to use the mode as a measure of the average of the sample. The median of a sample is the halfway point when the sample values xi  i = 1, 2, . . . , N  are arranged in ascending  or descending  order. Clearly, this depends on whether the size of the sample, N, is odd or even. If N is odd then the median is simply equal to x N+1  2, whereas if N is even the median of the sample is usually taken to be 1 2  xN 2 + x N 2 +1 .  cid:1 Find the mode and median of the sample given in table 31.1.  From the table we see that each sample value occurs exactly once, and so any value may be called the mode of the sample.  To ﬁnd the sample median, we ﬁrst arrange the sample values in ascending order and  obtain  166.3, 168.1, 169.0, 188.7, 189.8, 193.2, 200.0, 204.7.  Since the number of sample values N = 8, which is even, the median of the sample is  1  2  x4 + x5  = 1  2  188.7 + 189.8  = 189.25.  cid:2   31.2.2 Variance and standard deviation  The variance and standard deviation both give a measure of the spread of values in a sample about the sample mean ¯x. The sample variance is deﬁned by   31.6   s2 =  1 N   xi − ¯x 2,  N cid:4   i=1  1224   and the sample standard deviation is the positive square root of the sample variance, i.e.  31.2 SAMPLE STATISTICS    1  N cid:4   N  i=1  s =   xi − ¯x 2.   31.7    cid:1 Find the sample variance and sample standard deviation of the data given in table 31.1.  We have already found that the sample mean is 185.0 to one decimal place. However, when the mean is to be used in the subsequent calculation of the sample variance it is better to use the most accurate value available. In this case the exact value is 184.975, and so using  31.6 ,  s2 =   188.7 − 184.975 2 + ··· +  200.0 − 184.975 2   cid:19    cid:18   1 8 1608.36  8  =  = 201.0, √ where once again we have quoted the result to one decimal place. The sample standard 201.0 = 14.2. As it happens, in this case the diﬀerence deviation is then given by s = between the true mean and the rounded value is very small compared with the variation of the individual readings about the mean and using the rounded value has a negligible eﬀect; however, this would not be so if the diﬀerence were comparable to the sample standard deviation.  cid:2   Using the deﬁnition  31.7 , it is clear that in order to calculate the standard deviation of a sample we must ﬁrst calculate the sample mean. This requirement can be avoided, however, by using an alternative form for s2. From  31.6 , we see that  s2 =  N cid:4  N cid:4   i=1  1 N  1 N   xi − ¯x 2  − 1  N cid:4   x2 i  N  =  i=1  i=1  2xi¯x + = x2 − 2¯x2 + ¯x2 = x2 − ¯x2  cid:30   s2 = x2 − ¯x2 =  1 N  −  1 N  ¯x2   cid:31 2  N cid:4   i=1  1 N  N cid:4   cid:11   i=1  N cid:4   cid:11   i=1  x2 i  1225  We may therefore write the sample variance s2 as  xi  ,   31.8   from which the sample standard deviation is found by taking the positive square i for our sample, we root. Thus, by evaluating the quantities can calculate the sample mean and sample standard deviation at the same time.  N i=1 xi and  i=1 x2  N   i for the data given in table 31.1 and hence ﬁnd the mean  STATISTICS   cid:11    cid:11    cid:1 Calculate and standard deviation of the sample.  N i=1 xi and  i=1 x2  N  From table 31.1, we obtain  N cid:4  N cid:4   i=1  i=1  xi = 188.7 + 204.7 + ··· + 200.0 = 1479.8,  i =  188.7 2 +  204.7 2 + ··· +  200.0 2 = 275 334.36. x2  Since N = 8, we ﬁnd as before  quoting the ﬁnal results to one decimal place    cid:25    cid:7   −   cid:8   2  ¯x =  1479.8  8  = 185.0,  s =  275 334.36  1479.8  8  8  = 14.2.  cid:2   31.2.3 Moments and central moments  By analogy with our discussion of probability distributions in section 30.5, the sample mean and variance may also be described respectively as the ﬁrst moment and second central moment of the sample. In general, for a sample xi, i = 1, 2, . . . , N, we deﬁne the rth moment mr and rth central moment nr as  N cid:4  N cid:4   i=1  i=1  1 N  1 N  mr =  xr i ,  nr =   xi − m1 r.   31.9    31.10   Thus the sample mean ¯x and variance s2 may also be written as m1 and n2 respectively. As is common practice, we have introduced a notation in which a sample statistic is denoted by the Roman letter corresponding to whichever Greek letter is used to describe the corresponding population statistic. Thus, we use mr and nr to denote the rth moment and central moment of a sample, since in section 30.5 we denoted the rth moment and central moment of a population by µr and νr respectively.  This notation is particularly useful, since the rth central moment of a sample, mr, may be expressed in terms of the rth- and lower-order sample moments nr in a way exactly analogous to that derived in subsection 30.5.5 for the corresponding population statistics. As discussed in the previous section, the sample variance is 1, which is to be 1 derived in subsection 30.5.3 for population statistics. This correspondence also holds for higher-order central  given by s2 = x2− ¯x2 but this may also be written as n2 = m2 − m2 compared with the corresponding relation ν2 = µ2−µ2  1226   moments of the sample. For example,  31.2 SAMPLE STATISTICS  n3 =  1 N   xi − m1 3  N cid:4  N cid:4   i=1  1xi − m3 1   =   x3 i  1 N  − 3m1x2 = m3 − 3m1m2 + 3m2 = m3 − 3m1m2 + 2m3  i=1  1,  i + 3m2 1m1 − m3  1  which may be compared with equation  30.53  in the previous chapter.  Mirroring our discussion of the normalised central moments γr of a population in subsection 30.5.5, we can also describe a sample in terms of the dimensionless quantities   31.11   gk =  nk nk 2 2  =  nk sk  ;  g3 and g4 are called the sample skewness and kurtosis. Likewise, it is common to  deﬁne the excess kurtosis of a sample by g4 − 3.  31.2.4 Covariance and correlation  So far we have assumed that each data item of the sample consists of a single number. Now let us suppose that each item of data consists of a pair of numbers, so that the sample is given by  xi, yi , i = 1, 2, . . . , N.  We may calculate the sample means, ¯x and ¯y, and sample variances, s2  x and s2 y, of the xi and yi values individually but these statistics do not provide any measure of the relationship between the xi and yi. By analogy with our discussion in subsection 30.12.3 we measure any interdependence between the xi and yi in terms of the sample covariance, which is given by  Writing out the last expression in full, we obtain the form most useful for calculations, which reads   31.12   N cid:4   Vxy =  1 N   xi − ¯x  yi − ¯y   i=1  =  x − ¯x  y − ¯y  = xy − ¯x¯y.  cid:31   cid:30  N cid:4    cid:30  N cid:4    cid:31  cid:30   N cid:4    cid:31   xi  yi  .  i=1  i=1  Vxy =  1 N  xiyi  i=1  − 1 N2  1227   STATISTICS  rxy = 0.0  rxy = 0.1  rxy = 0.5  y  x  rxy = −0.7  rxy = −0.9  rxy = 0.99  Figure 31.1 Scatter plots for two-dimensional data samples of size N = 1000, with various values of the correlation r. No scales are plotted, since the value of r is unaﬀected by shifts of origin or changes of scale in x and y.  We may also deﬁne the closely related sample correlation by  rxy =  Vxy sxsy  ,  which can take values between −1 and +1. If the xi and yi are independent then   cid:7   Vxy = 0 = rxy, and from  31.12  we see that xy = ¯x¯y. It should also be noted that the value of rxy is not altered by shifts in the origin or by changes in the scale of the xi or yi. In other words, if x = cy + d, where a,  cid:7  = rxy. Figure 31.1 shows scatter plots for several  cid:7  b, c, d are constants, then rx two-dimensional random samples xi, yi of size N = 1000, each with a diﬀerent value of rxy.  cid:1 Ten UK citizens are selected at random and their heights and weights are found to be as follows  to the nearest cm or kg respectively :  = ax + b and y   cid:7   y  Person Height  cm  Weight  kg   A 194 75  B 168 53  C 177 72  D 180 80  E 171 75  F 190 75  G 151 57  H 169 67  I 175 46  J 182 68  Calculate the sample correlation between the heights and weights.  In order to ﬁnd the sample correlation, we begin by calculating the following sums  where xi are the heights and yi are the weights    cid:4   i   cid:4   i  1228  xi = 1757,  yi = 668,   31.3 ESTIMATORS AND SAMPLING DISTRIBUTIONS   cid:4   i   cid:4   i   cid:4   i  x2 i = 310 041,  y2 i = 45 746,  xiyi = 118 029.  The sample consists of N = 10 pairs of numbers, so the means of the xi and of the yi are  cid:7  given by ¯x = 175.7 and ¯y = 66.8. Also, xy = 11 802.9. Similarly, the standard deviations of the xi and yi are calculated, using  31.8 , as −  cid:7    cid:25   cid:25   310 041  = 11.6,   cid:8    cid:8   sx =  1757  10  10  2  sy =  −  45 746  10  2  668 10  = 10.6.  Thus the sample correlation is given by  xy − ¯x¯y  sxsy  =  rxy =  11 802.9 −  175.7  66.8    11.6  10.6   = 0.54.  Thus there is a moderate positive correlation between the heights and weights of the people measured.  cid:2   It is straightforward to generalise the above discussion to data samples of arbitrary dimension, the only complication being one of notation. We choose to denote the i th data item from an n-dimensional sample as  x 1  , . . . , x n   , i i where the bracketted superscript runs from 1 to n and labels the elements within a given data item whereas the subscript i runs from 1 to N and labels the data items within the sample. In this n-dimensional case, we can deﬁne the sample covariance matrix whose elements are  , x 2  i  and the sample correlation matrix with elements  Vkl = x k x l  − x k  x l   rkl =  Vkl sksl  .  Both these matrices are clearly symmetric but are not necessarily positive deﬁnite.  31.3 Estimators and sampling distributions  In general, the population P  x  from which a sample x1, x2, . . . , xN is drawn is unknown. The central aim of statistics is to use the sample values xi to infer certain properties of the unknown population P  x , such as its mean, variance and higher moments. To keep our discussion in general terms, let us denote the various parameters of the population by a1, a2, . . . , or collectively by a. Moreover, we make the dependence of the population on the values of these quantities explicit by  writing the population as P  xa . For the moment, we are assuming that the sample values xi are independent and drawn from the same  one-dimensional  population P  xa , in which case  P  xa  = P  x1a P  x2a ··· P  xNa .  1229   STATISTICS  Suppose, we wish to estimate the value of one of the quantities a1, a2, . . . , which we will denote simply by a. Since the sample values xi provide our only source of information, any estimate of a must be some function of the xi, i.e. some sample statistic. Such a statistic is called an estimator of a and is usually denoted by ˆa x , where x denotes the sample elements x1, x2, . . . , xN.  Since an estimator ˆa is a function of the sample values of the random variables x1, x2, . . . , xN, it too must be a random variable. In other words, if a number of random samples, each of the same size N, are taken from the  one-dimensional  population P  xa  then the value of the estimator ˆa will vary from one sample to estimator is described by its sampling distribution P  ˆaa . From section 30.14, this  the next and in general will not be equal to the true value a. This variation in the  is given by  P  ˆaa  dˆa = P  xa  dN x,  where dN x is the inﬁnitesimal ‘volume’ in x-space lying between the ‘surfaces’ ˆa x  = ˆa and ˆa x  = ˆa + dˆa. The form of the sampling distribution generally depends upon the estimator under consideration and upon the form of the population from which the sample was drawn, including, as indicated, the true values of the quantities a. It is also usually dependent on the sample size N.  cid:1 The sample values x1, x2, . . . , xN are drawn independently from a Gaussian distribution with mean µ and variance σ. Suppose that we choose the sample mean ¯x as our estimator ˆµ of the population mean. Find the sampling distributions of this estimator.  The sample mean ¯x is given by  ¯x =  1 N   x1 + x2 + ··· + xN ,  where the xi are independent random variables distributed as xi ∼ N µ, σ2 . From our discussion of multiple Gaussian distributions on page 1189, we see immediately that ¯x will also be Gaussian distributed as N µ, σ2 N . In other words, the sampling distribution of ¯x is given by  1 cid:24   P  ¯xµ, σ  =  exp  2πσ2 N   cid:13   −  ¯x − µ 2  2σ2 N   cid:14   .   31.13   Note that the variance of this distribution is σ2 N.  cid:2   31.3.1 Consistency, bias and efﬁciency of estimators  For any particular quantity a, we may in fact deﬁne any number of diﬀerent estimators, each of which will have its own sampling distribution. The quality of a given estimator ˆa may be assessed by investigating certain properties of its  sampling distribution P  ˆaa . In particular, an estimator ˆa is usually judged on  the three criteria of consistency, bias and eﬃciency, each of which we now discuss.  1230   31.3 ESTIMATORS AND SAMPLING DISTRIBUTIONS  An estimator ˆa is consistent if its value tends to the true value a in the large-sample limit, i.e.  Consistency is usually a minimum requirement for a useful estimator. An equiv- alent statement of consistency is that in the limit of large N the sampling  distribution P  ˆaa  of the estimator must satisfy  Consistency  N→∞ ˆa = a. lim  N→∞ P  ˆaa  → δ ˆa − a .  lim   cid:21   Bias   cid:21   The expectation value of an estimator ˆa is given by  E[ˆa] =  ˆaP  ˆaa  dˆa =  ˆa x P  xa  dN x,   31.14   where the second integral extends over all possible values that can be taken by the sample elements x1, x2, . . . , xN. This expression gives the expected mean value of ˆa from an inﬁnite number of samples, each of size N. The bias of an estimator ˆa is then deﬁned as  b a  = E[ˆa] − a.   31.15   We note that the bias b does not depend on the measured sample values x1, x2, . . . , xN. In general, though, it will depend on the sample size N, the func- tional form of the estimator ˆa and, as indicated, on the true properties a of the population, including the true value of a itself. If b = 0 then ˆa is called an unbiased estimator of a.  cid:1 An estimator ˆa is biased in such a way that E[ˆa] = a + b a , where the bias b a  is given by  b1 − 1 a + b2 and b1 and b2 are known constants. Construct an unbiased estimator of a.  Let us ﬁrst write E[ˆa] is the clearer form  E[ˆa] = a +  b1 − 1 a + b2 = b1a + b2. =  ˆa − b2  b1, which  as required  has the expectation value  The task of constructing an unbiased estimator is now trivial, and an appropriate choice is ˆa   cid:7    cid:7   E[ˆa  ] =  E[ˆa] − b2  b1  = a.  cid:2   The variance of an estimator is given by  V [ˆa] =   ˆa − E[ˆa] 2P  ˆaa  dˆa =   ˆa x  − E[ˆa] 2P  xa  dN x   31.16    cid:21   Eﬃciency   cid:21   1231   STATISTICS  and describes the spread of values ˆa about E[ˆa] that would result from a large number of samples, each of size N. An estimator with a smaller variance is said to be more eﬃcient than one with a larger variance. As we show in the next section, for any given quantity a of the population there exists a theoretical lower limit on the variance of any estimator ˆa. This result is known as Fisher’s inequality  or the Cram´er–Rao inequality  and reads   cid:7    cid:8   2  ?   cid:13   V [ˆa] ≥  1 +  ∂b ∂a  − ∂2 ln P ∂a2  E   cid:14   ,  where P stands for the population P  xa  and b is the bias of the estimator.  Denoting the quantity on the RHS of  31.17  by Vmin, the eﬃciency e of an estimator is deﬁned as   31.17   e = Vmin V [ˆa].  An estimator for which e = 1 is called a minimum-variance or eﬃcient estimator. Otherwise, if e < 1, ˆa is called an ineﬃcient estimator.  It should be noted that, in general, there is no unique ‘optimal’ estimator ˆa for a particular property a. To some extent, there is always a trade-oﬀ between bias and eﬃciency. One must often weigh the relative merits of an unbiased, ineﬃcient estimator against another that is more eﬃcient but slightly biased. Nevertheless, a common choice is the best unbiased estimator  BUE , which is simply the unbiased estimator ˆa having the smallest variance V [ˆa].  Finally, we note that some qualities of estimators are related. For example,  suppose that ˆa is an unbiased estimator, so that E[ˆa] = a and V [ˆa] → 0 as N → ∞. Using the Bienaym´e–Chebyshev inequality discussed in subsection 30.5.3,  it follows immediately that ˆa is also a consistent estimator. Nevertheless, it does not follow that a consistent estimator is unbiased.  cid:1 The sample values x1, x2, . . . , xN are drawn independently from a Gaussian distribution with mean µ and variance σ. Show that the sample mean ¯x is a consistent, unbiased, minimum-variance estimator of µ.  We found earlier that the sampling distribution of ¯x is given by  1 cid:24   P  ¯xµ, σ  =  exp  2πσ2 N   cid:13   −  ¯x − µ 2  2σ2 N   cid:14   ,  from which we see immediately that E[¯x] = µ and V [¯x] = σ2 N. Thus ¯x is an unbiased estimator of µ. Moreover, since it is also true that V [¯x] → 0 as N → ∞, ¯x is a consistent  estimator of µ.  In order to determine whether ¯x is a minimum-variance estimator of µ, we must use Fisher’s inequality  31.17 . Since the sample values xi are independent and drawn from a Gaussian of mean µ and standard deviation σ, we have  ln P  xµ, σ  = − 1  ln 2πσ2  +   cid:14   ,   xi − µ 2  σ2   cid:13   N cid:4   2  i=1  1232   31.3 ESTIMATORS AND SAMPLING DISTRIBUTIONS  and, on diﬀerentiating twice with respect to µ, we ﬁnd  ∂2 ln P  ∂µ2  = − N σ2 .  V [ ˆµ] ≥ σ2  .  N  This is independent of the xi and so its expectation value is also equal to −N σ2. With b  set equal to zero in  31.17 , Fisher’s inequality thus states that, for any unbiased estimator ˆµ of the population mean,  Since V [¯x] = σ2 N, the sample mean ¯x is a minimum-variance estimator of µ.  cid:2   31.3.2 Fisher’s inequality   cid:7    cid:8   2  ?   cid:13   V [ˆa] ≥  1 +  ∂b ∂a  − ∂2 ln P ∂a2  E   cid:14   ,  As mentioned above, Fisher’s inequality provides a lower limit on the variance of any estimator ˆa of the quantity a; it reads  where P stands for the population P  xa  and b is the bias of the estimator.  We now present a proof of this inequality. Since the derivation is somewhat complicated, and many of the details are unimportant, this section can be omitted on a ﬁrst reading. Nevertheless, some aspects of the proof will be useful when the eﬃciency of maximum-likelihood estimators is discussed in section 31.5.  cid:1 Prove Fisher’s inequality  31.18 .  The normalisation of P  xa  is given by cid:21   where dNx = dx1dx2 ··· dxN and the integral extends over all the allowed values of the sample items xi. Diﬀerentiating  31.19  with respect to the parameter a, we obtain   cid:21   P  xa  dNx = 1,  cid:21   ∂P ∂a  dNx =  ∂ ln P  ∂a  P dNx = 0.  We note that the second integral is simply the expectation value of ∂ ln P  ∂a, where the average is taken over all possible samples xi, i = 1, 2, . . . , N. Further, by equating the two expressions for ∂E[ˆa] ∂a obtained by diﬀerentiating  31.15  and  31.14  with respect to a we obtain, dropping the functional dependencies, a second relationship,   cid:21    cid:21   1 +  =  ∂b ∂a  ˆa  ∂P ∂a  dNx =  ˆa  P dNx.  ∂ ln P  ∂a  Now, multiplying  31.20  by α a , where α a  is any function of a, and subtracting the  result from  31.21 , we obtain cid:21   [ˆa − α a ]  ∂ ln P  ∂a  P dNx = 1 +  ∂b ∂a  .  At this point we must invoke the Schwarz inequality proved in subsection 8.1.3. The proof   31.18    31.19    31.20    31.21   1233   STATISTICS  is trivially extended to multiple integrals and shows that for two real functions, g x  and h x ,  If we now let g = [ˆa − α a ]  h2 x  dNx  g2 x  dNx √ P and h =  ∂ ln P  ∂a    cid:7  cid:21   cid:12  cid:21    cid:8  cid:7  cid:21   cid:15  cid:16  cid:21   cid:7    cid:8    cid:7  cid:21  √ P , we ﬁnd ≥   cid:17   P dNx   cid:7   ≥  cid:8   2   cid:8   2  .   cid:8   [ˆa − α a ]2P dNx  ∂ ln P  ∂a  1 +  ∂b ∂a  2  .  g x h x  dNx   31.22   On the LHS, the factor in braces represents the expected spread of ˆa-values around the point α a . The minimum value that this integral may take occurs when α a  = E[ˆa]. Making this substitution, we recognise the integral as the variance V [ˆa], and so obtain the result   cid:16  cid:21   cid:7    cid:8   2   cid:8   2   cid:17 −1   cid:7   P dNx  .   31.23   We note that the factor in brackets is the expectation value of  ∂ ln P  ∂a 2.  Fisher’s inequality is, in fact, often quoted in the form  31.23 . We may recover the form   31.18  by noting that on diﬀerentiating  31.20  with respect to a we obtain  V [ˆa] ≥  1 +  ∂b ∂a   cid:21   cid:7   cid:21   cid:7    cid:8   2  ∂ ln P  ∂a   cid:8   ∂P ∂a   cid:21   ∂2 ln P ∂a2 P +  ∂ ln P  ∂a  dNx = 0.  ∂ ln P  ∂a   cid:7   P dNx = −  cid:8   cid:13  cid:21   2  ∂2 ln P ∂a2 P dNx.   cid:14 −1  .  V [ˆa] ≥ −  1 +  ∂b ∂a  ∂2 ln P ∂a2 P dNx  Writing ∂P  ∂a as  ∂ ln P  ∂a P and rearranging we ﬁnd that  Substituting this result in  31.23  gives  Since the factor in brackets is the expectation value of ∂2 ln P  ∂a2, we have recovered result  31.18 .  cid:2   31.3.3 Standard errors on estimators  For a given sample x1, x2, . . . , xN, we may calculate the value of an estimator ˆa x  for the quantity a. It is also necessary, however, to give some measure of the statistical uncertainty in this estimate. One way of characterising this uncertainty  is with the standard deviation of the sampling distribution P  ˆaa , which is given  simply by  σˆa =  V [ˆa] 1 2.   31.24   If the estimator ˆa x  were calculated for a large number of samples, each of size N, then the standard deviation of the resulting ˆa values would be given by  31.24 . Consequently, σˆa is called the standard error on our estimate.  In general, however, the standard error σˆa depends on the true values of some  1234   31.3 ESTIMATORS AND SAMPLING DISTRIBUTIONS  or all of the quantities a and they may be unknown. When this occurs, one must substitute estimated values of any unknown quantities into the expression for σˆa in order to obtain an estimated standard error ˆσˆa. One then quotes the result as  a = ˆa ± ˆσˆa.   cid:1 Ten independent sample values xi, i = 1, 2, . . . , 10, are drawn at random from a Gaussian distribution with standard deviation σ = 1. The sample values are as follows  to two decimal places :  2.22  2.56  1.07  0.24  0.18  0.95  2.09  1.81  0.73 −0.79  Estimate the population mean µ, quoting the standard error on your result.  We have shown in the ﬁnal worked example of subsection 31.3.1 that, in this case, ¯x is a consistent, unbiased, minimum-variance estimator of µ and has variance V [¯x] = σ2 N. Thus, our estimate of the population mean with its associated standard error is  ˆµ = ¯x ± σ√  = 1.11 ± 0.32.  N  If the true value of σ had not been known, we would have needed to use an estimated value ˆσ in the expression for the standard error. Useful basic estimators of σ are discussed in subsection 31.4.2.  cid:2   It should be noted that the above approach is most meaningful for unbiased estimators. In this case, E[ˆa] = a and so σˆa describes the spread of ˆa-values about the true value a. For a biased estimator, however, the spread about the true value a is given by the root mean square error  cid:4 ˆa, which is deﬁned by  ˆa = E[ ˆa − a 2]  cid:4 2 = E[ ˆa − E[ˆa] 2] +  E[ˆa] − a 2 = V [ˆa] + b a 2.  We see that  cid:4 2 ˆa is the sum of the variance of ˆa and the square of the bias and so can be interpreted as the sum of squares of statistical and systematic errors. For a biased estimator, it is often more appropriate to quote the result as  As above, it may be necessary to use estimated values ˆa in the expression for the root mean square error and thus to quote only an estimate ˆ cid:4 ˆa of the error.  31.3.4 Conﬁdence limits on estimators  An alternative  and often equivalent  way of quoting a statistical error is with a conﬁdence interval. Let us assume that, other than the quantity of interest a, the quantities a have known ﬁxed values. Thus we denote the sampling distribution  a = ˆa ±  cid:4 ˆa.  1235   STATISTICS  P  ˆaa   α  β  ˆaα a   ˆaβ a   ˆa  Figure 31.2 The sampling distribution P  ˆaa  of some estimator ˆa for a given value of a. The shaded regions indicate the two probabilities Pr ˆa < ˆaα a   = α and Pr ˆa > ˆaβ a   = β.  of ˆa by P  ˆaa . For any particular value of a, one can determine the two values ˆaα a  and ˆaβ a  such that  ˆaα a    cid:21   cid:21  ∞ −∞ P  ˆaa  dˆa = α, P  ˆaa  dˆa = β.  ˆaβ  a   Pr ˆa < ˆaα a   =  Pr ˆa > ˆaβ a   =   31.25    31.26   This is illustrated in ﬁgure 31.2. Thus, for any particular value of a, the probability that the estimator ˆa lies within the limits ˆaα a  and ˆaβ a  is given by P  ˆaa  dˆa = 1 − α − β.  Pr ˆaα a  < ˆa < ˆaβ a   =  ˆaβ  a    cid:21   ˆaα a   Now, let us suppose that from our sample x1, x2, . . . , xN, we actually obtain the value ˆaobs for our estimator. If ˆa is a good estimator of a then we would expect ˆaα a  and ˆaβ a  to be monotonically increasing functions of a  i.e. ˆaα and ˆaβ both change in the same sense as a when the latter is varied . Assuming this to be the case, we can uniquely deﬁne the two numbers a− and a+ by the relationships  ˆaα a+  = ˆaobs  and  ˆaβ a−  = ˆaobs.  From  31.25  and  31.26  it follows that  Pr a+ < a  = α  and  Pr a− > a  = β,  which when taken together imply  Pr a− < a < a+  = 1 − α − β.   31.27  Thus, from our estimate ˆaobs, we have determined two values a− and a+ such that this interval contains the true value of a with probability 1 − α − β. It should be emphasised that a− and a+ are random variables. If a large number of samples,  1236   31.3 ESTIMATORS AND SAMPLING DISTRIBUTIONS  P  ˆaa−   P  ˆaa+   α  β  ˆaobs  ˆa  Figure 31.3 An illustration of how the observed value of the estimator, ˆaobs, and the given values α and β determine the two conﬁdence limits a− and a+, which are such that ˆaα a+  = ˆaobs = ˆaβ a− .  each of size N, were analysed then the interval [a−, a+] would contain the true value a on a fraction 1 − α − β of the occasions. The interval [a−, a+] is called a conﬁdence interval on a at the conﬁdence level 1 − α − β. The values a− and a+ themselves are called respectively the  lower conﬁdence limit and the upper conﬁdence limit at this conﬁdence level. In practice, the conﬁdence level is often quoted as a percentage. A convenient way of presenting our results is  ˆaobs   cid:21   cid:21  ∞ −∞ P  ˆaa+  dˆa = α, P  ˆaa−  dˆa = β.  ˆaobs   31.28    31.29   The conﬁdence limits may then be found by solving these equations for a− and a+ either analytically or numerically. The situation is illustrated graphically in ﬁgure 31.3.  Occasionally one might not combine the results  31.28  and  31.29  but use either one or the other to provide a one-sided conﬁdence interval on a. Whenever the results are combined to provide a two-sided conﬁdence interval, however, the  interval is not speciﬁed uniquely by the conﬁdence level 1− α− β. In other words, there are generally an inﬁnite number of intervals [a−, a+] for which  31.27  holds. To specify a unique interval, one often chooses α = β, resulting in the central conﬁdence interval on a. All cases can be covered by calculating the quantities  c = ˆa − a− and d = a+ − ˆa and quoting the result of an estimate as  So far we have assumed that the quantities a other than the quantity of interest a are known in advance. If this is not the case then the construction of conﬁdence limits is considerably more complicated. This is discussed in subsection 31.3.6.  a = ˆa+d−c.  1237   STATISTICS   cid:13    cid:8   cid:8   31.3.5 Conﬁdence limits for a Gaussian sampling distribution  An important special case occurs when the sampling distribution is Gaussian; if the mean is a and the standard deviation is σˆa then  1 cid:2   exp  2πσ2 ˆa  −  ˆa − a 2  2σ2 ˆa   cid:14   .  P  ˆaa, σˆa  =   31.30   For almost any  consistent  estimator ˆa, the sampling distribution will tend to  this form in the large-sample limit N → ∞, as a consequence of the central limit  theorem. For a sampling distribution of the form  31.30 , the above procedure for determining conﬁdence intervals becomes straightforward. Suppose, from our sample, we obtain the value ˆaobs for our estimator. In this case, equations  31.28  and  31.29  become   cid:7   cid:7   Φ  1 − Φ  ˆaobs − a+ ˆaobs − a−  σˆa  σˆa  = α,  = β,  a− = ˆaobs − σˆaΦ  a+ = ˆaobs + σˆaΦ −1 α  = −Φ  −1 1 − β , −1 1 − α ;  a = ˆa ± σˆa,  where Φ z  is the cumulative probability function for the standard Gaussian distri- bution, discussed in subsection 30.9.1. Solving these equations for a− and a+ gives   31.31    31.32    31.33   we have used the fact that Φ The value of the inverse function Φ given in subsection 30.9.1. For the normally used central conﬁdence interval one has α = β. In this case, we see that quoting a result using the standard error, as  −1 1−α  to make the equations symmetric. −1 z  can be read oﬀ directly from table 30.3,  is equivalent to taking Φ  −1 1− α  = 1. From table 30.3, we ﬁnd α = 1− 0.8413 = 0.1587, and so this corresponds to a conﬁdence level of 1 − 2 0.1587  ≈ 0.683.  Thus, the standard error limits give the 68.3% central conﬁdence interval.  cid:1 Ten independent sample values xi, i = 1, 2, . . . , 10, are drawn at random from a Gaussian distribution with standard deviation σ = 1. The sample values are as follows  to two decimal places :  2.22  2.56  1.07  0.24  0.18  0.95  2.09  1.81  0.73 −0.79  Find the 90% central conﬁdence interval on the population mean µ.  Our estimator ˆµ is the sample mean ¯x. As shown towards the end of section 31.3, the √ sampling distribution of ¯x is Gaussian with mean E[¯x] and variance V [¯x] = σ2 N. Since N = 0.32. Moreover, in σ = 1 in this case, the standard error is given by σ ˆx = σ  subsection 31.3.3, we found the mean of the above sample to be ¯x = 1.11.  1238   31.3 ESTIMATORS AND SAMPLING DISTRIBUTIONS  For the 90% central conﬁdence interval, we require α = β = 0.05. From table 30.3, we  ﬁnd  −1 1 − α  = Φ  Φ  −1 0.95  = 1.65,  and using  31.31  and  31.32  we obtain  a− = ¯x − 1.65σ¯x = 1.11 −  1.65  0.32  = 0.58,  a+ = ¯x + 1.65σ¯x = 1.11 +  1.65  0.32  = 1.64.  Thus, the 90% central conﬁdence interval on µ is [0.58, 1.64]. For comparison, the true value used to create the sample was µ = 1.  cid:2   In the case where the standard error σˆa in  31.33  is not known in advance, one must use a value ˆσˆa estimated from the sample. In principle, this complicates somewhat the construction of conﬁdence intervals, since properly one should  consider the two-dimensional joint sampling distribution P  ˆa, ˆσˆaa . Nevertheless,  in practice, provided ˆσˆa is a fairly good estimate of σˆa the above procedure may be applied with reasonable accuracy. In the special case where the sample values xi are drawn from a Gaussian distribution with unknown µ and σ, it is in fact possible to obtain exact conﬁdence intervals on the mean µ, for a sample of any size N, using Student’s t-distribution. This is discussed in subsection 31.7.5.  31.3.6 Estimation of several quantities simultaneously  Suppose one uses a sample x1, x2, . . . , xN to calculate the values of several es- timators ˆa1, ˆa2, . . . , ˆaM  collectively denoted by ˆa  of the quantities a1, a2, . . . , aM  collectively denoted by a  that describe the population from which the sample was drawn. The joint sampling distribution of these estimators is an M-dimensional  PDF P  ˆaa  given by  P  ˆaa  dM ˆa = P  xa  dN x.   cid:1 Sample values x1, x2, . . . , xN are drawn independently from a Gaussian distribution with mean µ and standard deviation σ. Suppose we choose the sample mean ¯x and sample stan- dard deviation s respectively as estimators ˆµ and ˆσ. Find the joint sampling distribution of these estimators.  Since each data value xi in the sample is assumed to be independent of the others, the joint probability distribution of sample values is given by   cid:13    cid:11   i xi − µ 2  2σ2   cid:14   .   cid:4   i   xi − ¯x  +   ¯x − µ 2  We may rewrite the sum in the exponent as follows:   cid:4   i   xi − µ 2 =  P  xµ, σ  =  2πσ2   −N 2 exp  −   cid:4   cid:4  = Ns2 + N ¯x − µ 2,   xi − ¯x + ¯x − µ 2  xi − ¯x 2 + 2 ¯x − µ   =  i  i   cid:4   i  1239    cid:4   cid:4   i  i  xi = N¯x,   xi − ¯x 2 = Ns2,  STATISTICS   cid:11   cid:12   where in the last line we have used the fact that of µ and σ, the sampling distribution is in fact a function only of the sample mean ¯x and the standard deviation s. Thus the sampling distribution of ¯x and s must satisfy  i xi − ¯x  = 0. Hence, for given values  P  ¯x, sµ, σ  d¯x ds =  2πσ2   −N 2 exp  dV ,   31.34    cid:15   − N[ ¯x − µ 2 + s2]  2σ2  where dV = dx1 dx2 ··· dxN is an element of volume in the sample space which yields simultaneously values of ¯x and s that lie within the region bounded by [¯x, ¯x + d¯x] and [s, s + ds]. Thus our only remaining task is to express dV in terms of ¯x and s and their diﬀerentials.  Let S be the point in sample space representing the sample  x1, x2, . . . , xN . For given  values of ¯x and s, we require the sample values to satisfy both the condition  which deﬁnes an  N − 1 -dimensional hyperplane in the sample space, and the condition  which deﬁnes an  N − 1 -dimensional hypersphere. Thus S is constrained to lie in the intersection of these two hypersurfaces, which is itself an  N− 2 -dimensional hypersphere. Now, the volume of an  N − 2 -dimensional hypersphere is proportional to sN−1. It follows that the volume dV between two concentric  N − 2 -dimensional hyperspheres of radius √ √ N s + ds  and two  N − 1 -dimensional hyperplanes corresponding to ¯x and Ns and ¯x + d¯x is   cid:13   dV = AsN−2 ds d¯x,   cid:14    cid:7    cid:8   where A is some constant. Thus, substituting this expression for dV into  31.34 , we ﬁnd  P  ¯x, sµ, σ  = C1 exp  − N ¯x − µ 2  2σ2  C2sN−2 exp  − Ns2 2σ2  = P  ¯xµ, σ P  sσ ,  where C1 and C2 are constants. We have written P  ¯x, sµ, σ  in this form to show that it  separates naturally into two parts, one depending only on ¯x and the other only on s. Thus, ¯x and s are independent variables. Separate normalisations of the two factors in  31.35  require   31.35    cid:7    cid:8   N 2σ2   N−1  2   cid:5    cid:6  ,  1  2  N − 1   1  Γ  and  C2 = 2   cid:7    cid:8   1 2  C1 =  N  2πσ2  where the calculation of C2 requires the use of the gamma function, discussed in the Appendix.  cid:2   The marginal sampling distribution of any one of the estimators ˆai is given  simply by   cid:21    cid:21   P  ˆaia  =  ···  P  ˆaa  dˆa1 ··· dˆai−1dˆai+1 ··· dˆaM,  and the expectation value E[ˆai] and variance V [ˆai] of ˆai are again given by  31.14  and  31.16  respectively. By analogy with the one-dimensional case, the standard error σˆai on the estimator ˆai is given by the positive square root of V [ˆai]. With  1240   31.3 ESTIMATORS AND SAMPLING DISTRIBUTIONS  several estimators, however, it is usual to quote their full covariance matrix. This  M × M matrix has elements  Vij = Cov[ˆai, ˆaj] =   ˆai − E[ˆai]  ˆaj − E[ˆaj] P  ˆaa  dM ˆa  ˆai − E[ˆai]  ˆaj − E[ˆaj] P  xa  dN x.   cid:21   cid:21   =  Fisher’s inequality can be generalised to the multi-dimensional case. Adapting the proof given in subsection 31.3.2, one may show that, in the case where the estimators are eﬃcient and have zero bias, the elements of the inverse of the covariance matrix are given by   cid:13    cid:14   ,  −1 ij = E   V  − ∂2 ln P ∂ai∂aj   31.36   where P denotes the population P  xa  from which the sample is drawn. The quantity on the RHS of  31.36  is the element Fij of the so-called Fisher matrix F of the estimators.  cid:1 Calculate the covariance matrix of the estimators ¯x and s in the previous example. As shown in  31.35 , the joint sampling distribution P  ¯x, sµ, σ  factorises, and so the  estimators ¯x and s are independent. Thus, we conclude immediately that  Since we have already shown in the worked example at the end of subsection 31.3.1 that V [¯x] = σ2 N, it only remains to calculate V [s]. From  31.35 , we ﬁnd  E[sr] = C2  sN−2+r exp  r 2 Γ  ds =   cid:21  ∞  0  where we have evaluated the integral using the deﬁnition of the gamma function given in the Appendix. Thus, the expectation value of the sample standard deviation is  Cov[¯x, s] = 0.   cid:7    cid:8   − Ns2 2σ2  cid:7   cid:8   2 N   cid:7    cid:8   2 N   cid:5   Γ   cid:6   cid:5   cid:6  σr, 2  N − 1 + r  2  N − 1   1  1  1 2  Γ 1  1 2 N   cid:6   cid:5   cid:5  2  N − 1  N − 1 − 2  cid:16   Γ   cid:6  σ,  cid:5   Γ  σ2 N   cid:6   cid:5  2  N − 1   1 2 N  Γ 1   cid:6  cid:17   2    and its variance is given by  V [s] = E[s2] −  E[s] 2 =  E[s] =   31.37   We note, in passing, that  31.37  shows that s is a biased estimator of σ.  cid:2   The idea of a conﬁdence interval can also be extended to the case where several quantities are estimated simultaneously but then the practical construction of an interval is considerably more complicated. The general approach is to construct an M-dimensional conﬁdence region R in a-space. By analogy with the one-  dimensional case, for a given conﬁdence level of  say  1 − α, one ﬁrst constructs  1241   a region ˆR in ˆa-space, such that cid:21  cid:21   STATISTICS  P  ˆaa  dM ˆa = 1 − α.  ˆR  A common choice for such a region is that bounded by the ‘surface’ P  ˆaa  =  constant. By considering all possible values a and the values of ˆa lying within the region ˆR, one can construct a 2M-dimensional region in the combined space  ˆa, a . Suppose now that, from our sample x, the values of the estimators are ˆai,obs, i = 1, 2, . . . , M. The intersection of the M ‘hyperplanes’ ˆai = ˆai,obs with the 2M-dimensional region will determine an M-dimensional region which, when projected onto a-space, will determine a conﬁdence limit R at the conﬁdence  level 1 − α. It is usually the case that this conﬁdence region has to be evaluated  numerically.  The above procedure is clearly rather complicated in general and a simpler approximate method that uses the likelihood function is discussed in subsec- tion 31.5.5. As a consequence of the central in the  large-sample limit, N → ∞, the joint sampling distribution P  ˆaa  will tend, in  limit theorem, however,  general, towards the multivariate Gaussian  P  ˆaa  =  1   2π M 2V1 2  exp  2 Q ˆa, a   ,   31.38    cid:18 − 1   cid:19   where V is the covariance matrix of the estimators and the quadratic form Q is given by  Q ˆa, a  =  ˆa − a TV−1 ˆa − a .  Moreover, in the limit of large N, the inverse covariance matrix tends to the  Fisher matrix F given in  31.36 , i.e. V−1 → F. dence intervals is greatly simpliﬁed. The surfaces of constant P  ˆaa  correspond  For the Gaussian sampling distribution  31.38 , the process of obtaining conﬁ-  to surfaces of constant Q ˆa, a , which have the shape of M-dimensional ellipsoids in ˆa-space, centred on the true values a. In particular, let us suppose that the  ellipsoid Q ˆa, a  = c  where c is some constant  contains a fraction 1 − α of the total probability. Now suppose that, from our sample x, we obtain the values ˆaobs for our estimators. Because of the obvious symmetry of the quadratic form Q with respect to a and ˆa, it is clear that the ellipsoid Q a, ˆaobs  = c in a-space that  is centred on ˆaobs should contain the true values a with probability 1 − α. Thus  Q a, ˆaobs  = c deﬁnes our required conﬁdence region R at this conﬁdence level. This is illustrated in ﬁgure 31.4 for the two-dimensional case.  It remains only to determine the constant c corresponding to the conﬁdence  level 1 − α. As discussed in subsection 30.15.2, the quantity Q ˆa, a  is distributed as a χ2 variable of order M. Thus, the conﬁdence region corresponding to the  1242   31.4 SOME BASIC ESTIMATORS  ˆa2   a   a2   b   atrue  ˆaobs  atrue  ˆaobs   cid:21   c  0  ˆa1  a1  Figure 31.4  a  The ellipse Q ˆa, a  = c in ˆa-space.  b  The ellipse Q a, ˆaobs  = c  in a-space that corresponds to a conﬁdence region R at the level 1 − α, when  c satisﬁes  31.39 .  conﬁdence level 1 − α is given by Q a, ˆaobs  = c, where the constant c satisﬁes  P  χ2  M  d χ2  M  = 1 − α,   31.39   and P  χ2 M   is the chi-squared PDF of order M, discussed in subsection 30.9.4. This integral may be evaluated numerically to determine the constant c. Alternatively, some reference books tabulate the values of c corresponding to given conﬁdence levels and various values of M. A representative selection of values of c is given in table 31.2; there the number of degrees of freedom is denoted by the more usual n, rather than M.  31.4 Some basic estimators  In many cases, one does not know the functional form of the population from which a sample is drawn. Nevertheless, in a case where the sample values x1, x2, . . . , xN are each drawn independently from a one-dimensional population P  x , it is possible to construct some basic estimators for the moments and central moments of P  x . In this section, we investigate the estimating properties of the common sample statistics presented in section 31.2. In fact, expectation values and variances of these sample statistics can be calculated without prior knowledge of the functional form of the population; they depend only on the sample size N and certain moments and central moments of P  x .  31.4.1 Population mean µ  Let us suppose that the parent population P  x  has mean µ and variance σ2. An obvious estimator ˆµ of the population mean is the sample mean ¯x. Provided µ and σ2 are both ﬁnite, we may apply the central limit theorem directly to obtain  1243   %  n = 1 2 3 4  1.57 10 2.01 10  3.93 10  95  −3 0.103 0.352 0.711  99  −4 −2 0.115 0.297  0.554 0.872 1.24 1.65 2.09  2.56 3.05 3.57 4.11 4.66  5.23 5.81 6.41 7.01 7.63  8.26 8.90 9.54 10.20 10.86  11.52 14.95 22.16 29.71 37.48  45.44 53.54 61.75 70.06  5 6 7 8 9  10 11 12 13 14  15 16 17 18 19  20 21 22 23 24  25 30 40 50 60  70 80 90 100  1.15 1.64 2.17 2.73 3.33  3.94 4.57 5.23 5.89 6.57  7.26 7.96 8.67 9.39 10.12  10.85 11.59 12.34 13.09 13.85  14.61 18.49 26.51 34.76 43.19  51.74 60.39 69.13 77.93  2.5  5.02 7.38 9.35 11.14  12.83 14.45 16.01 17.53 19.02  20.48 21.92 23.34 24.74 26.12  27.49 28.85 30.19 31.53 32.85  34.17 35.48 36.78 38.08 39.36  40.65 46.98 59.34 71.42 83.30  95.02 106.6 118.1 129.6  1  0.5  0.1  6.63 9.21 11.34 13.28  15.09 16.81 18.48 20.09 21.67  23.21 24.73 26.22 27.69 29.14  30.58 32.00 33.41 34.81 36.19  37.57 38.93 40.29 41.64 42.98  44.31 50.89 63.69 76.15 88.38  100.4 112.3 124.1 135.8  7.88 10.60 12.84 14.86  16.75 18.55 20.28 21.95 23.59  25.19 26.76 28.30 29.82 31.32  32.80 34.27 35.72 37.16 38.58  40.00 41.40 42.80 44.18 45.56  46.93 53.67 66.77 79.49 91.95  104.2 116.3 128.3 140.2  10.83 13.81 16.27 18.47  20.52 22.46 24.32 26.12 27.88  29.59 31.26 32.91 34.53 36.12  37.70 39.25 40.79 42.31 43.82  45.31 46.80 48.27 49.73 51.18  52.62 59.70 73.40 86.66 99.61  112.3 124.8 137.2 149.4  STATISTICS  10  2.71 4.61 6.25 7.78  9.24 10.64 12.02 13.36 14.68  15.99 17.28 18.55 19.81 21.06  22.31 23.54 24.77 25.99 27.20  28.41 29.62 30.81 32.01 33.20  34.38 40.26 51.81 63.17 74.40  85.53 96.58 107.6 118.5  5  3.84 5.99 7.81 9.49  11.07 12.59 14.07 15.51 16.92  18.31 19.68 21.03 22.36 23.68  25.00 26.30 27.59 28.87 30.14  31.41 32.67 33.92 35.17 36.42  37.65 43.77 55.76 67.50 79.08  90.53 101.9 113.1 124.3  1244  Table 31.2 The tabulated values are those which a variable distributed as χ2 with n degrees of freedom exceeds with the given percentage probability. For example, a variable having a χ2 distribution with 14 degrees of freedom takes values in excess of 21.06 on 10% of occasions.   31.4 SOME BASIC ESTIMATORS  exact expressions, valid for samples of any size N, for the expectation value and variance of ¯x. From parts  i  and  ii  of the central limit theorem, discussed in section 30.10, we immediately obtain  σ2 N  E[¯x] = µ, √ Thus we see that ¯x is an unbiased estimator of µ. Moreover, we note that the N, and so the sampling distribution of ¯x becomes more standard error in ¯x is σ   tightly centred around µ as the sample size N increases. Indeed, since V [¯x] → 0 as N → ∞, ¯x is also a consistent estimator of µ.  V [¯x] =   31.40   .  In the limit of large N, we may in fact obtain an approximate form for the full sampling distribution of ¯x. Part  iii  of the central limit theorem  see section 30.10  tells us immediately that, for large N, the sampling distribution of ¯x is given approximately by the Gaussian form  1 cid:24   P  ¯xµ, σ  ≈  exp  2πσ2 N   cid:13   −  ¯x − µ 2  2σ2 N   cid:14   .  Note that this does not depend on the form of the original parent population. If, however, the parent population is in fact Gaussian then this result is exact for samples of any size N  as is immediately apparent from our discussion of multiple Gaussian distributions in subsection 30.9.1 .  C   cid:1 Show that  31.4.2 Population variance σ2  An estimator for the population variance σ2 is not so straightforward to deﬁne as one for the mean. Complications arise because, in many cases, the true mean of the population µ is not known. Nevertheless, let us begin by considering the case where in fact µ is known. In this event, a useful estimator is  N cid:4   xi − µ 2 =  C  σ2 =  1 N  i=1   cid:31    cid:30   N cid:4   i=1  1 N  − µ2.  x2 i   31.41   The expectation value of  σ2 is given by  σ2 is an unbiased and consistent estimator of the population variance σ2.  E[  σ2] =  1 N  E  x2 i  i ] − µ2 = µ2 − µ2 = σ2,  C   cid:16  N cid:4   cid:16  N cid:4   i=1  i=1  C C   cid:17  − µ2 = E[x2  cid:17   V [  σ2] =  1 N2 V  x2 i  + V [µ2] =  V [x2  i ] =  1 N  1 N   µ4 − µ2 2 , i ] −  E[x2  i ] 2 = µ4 − µ2  2,  in which we have used that fact that V [µ2] = 0 and V [x2  i ] = E[x4  from which we see that the estimator is unbiased. The variance of the estimator is  1245   where µr is the rth population moment. Since showing that it is also a consistent estimator of σ2, the result is established.  cid:2   σ2 is unbiased and V [  If the true mean of the population is unknown, however, a natural alternative is to replace µ by ¯x in  31.41 , so that our estimator is simply the sample variance s2 given by  C σ2] → 0 as N → ∞,  STATISTICS  C   cid:30   N cid:4   i=1  s2 =  1 N  −  x2 i  1 N   cid:31 2  xi  .  N cid:4   i=1  In order to determine the properties of this estimator, we must calculate E[s2] and V [s2]. This task is straightforward but lengthy. However, for the investigation of the properties of a central moment of the sample, there exists a useful trick that simpliﬁes the calculation. We can assume, with no loss of generality, that the mean µ1 of the population from which the sample is drawn is equal to zero. With this assumption, the population central moments, νr, are identical to the corresponding moments µr, and we may perform our calculation in terms of the latter. At the end, however, we replace µr by νr in the ﬁnal result and so obtain  a general expression that is valid even in cases where µ1  cid:3 = 0.  cid:1 Calculate E[s2] and V [s2] for a sample of size N.  The expectation value of the sample variance s2 for a sample of size N is given by  E[s2] =  1 N  E   cid:16  cid:4    cid:17   x2 i  i  − 1  N2 E    2   cid:30  cid:4   cid:4   x2 i +  i   cid:31   cid:4   xi  i  i,j  j cid:3 =i  xixj   .  =  NE[x2  1 N  i ] − 1  N2 E  The number of terms in the double summation in  31.42  is N N − 1 , so we ﬁnd  E[s2] = E[x2  i ] − 1  N2   NE[x2  i ] + N N − 1 E[xixj] .  Now, since the sample elements xi and xj are independent, E[xixj] = E[xi]E[xj] = 0, assuming the mean µ1 of the parent population to be zero. Denoting the rth moment of the population by µr, we thus obtain  E[s2] = µ2 − µ2  =  N − 1  µ2 =  N − 1  N  σ2,  N  N  where in the last line we have used the fact that the population mean is zero, and so  µ2 = ν2 = σ2. However, the ﬁnal result is also valid in the case where µ1  cid:3 = 0. rather heavy going. The variance of s2 is given by  Using the above method, we can also ﬁnd the variance of s2, although the algebra is   31.42    31.43    31.44   where E[s2] is given by  31.43 . We therefore need only consider how to calculate E[s4],  V [s2] = E[s4] −  E[s2] 2,  1246    cid:11   − 2  cid:31   2   cid:31   2   cid:4    cid:4   i,j  j cid:3 =i  x2 i  =  x4 i +  i x2 x2 j ,  i  i   cid:30  cid:4   cid:31   i  2  xi  =  x2 i   cid:4    = Nµ4 + N N − 1 µ2  cid:4   cid:4   2.  31.4 SOME BASIC ESTIMATORS   cid:8   2   cid:17   cid:11   2   cid:7  cid:11   cid:11   i xi N     −  where s4 is given by  s4 =   cid:16  cid:11   cid:11  i  2 can be written as  cid:30  cid:4   i x2 N2  i x2 N  =     i  i  2   cid:11   i x2  We will consider in turn each of the three terms on the RHS. In the ﬁrst term, the sum    i x2  i    N3  i xi 2     +  i xi 4 N4  .   31.45   where the ﬁrst sum contains N terms and the second contains N N − 1  terms. Since the sample elements xi and xj are assumed independent, we have E[x2 j ] = µ2 2, and so  j ] = E[x2  i ]E[x2  i x2   cid:30  cid:4   E   cid:31  cid:30  cid:4   x2 i   cid:4   i,j,k  k cid:3 =j cid:3 =i  Turning to the second term on the RHS of  31.45 ,  i  i  i  i,j  j cid:3 =i  i,j  j cid:3 =i  x4 i +  x3 i xj +  x2 i x2  j +  x2 i xjxk.  Since the mean of the population has been assumed to equal zero, the expectation values of the second and fourth sums on the RHS vanish. The ﬁrst and third sums contain N  and N N − 1  terms respectively, and so   cid:30  cid:4   cid:4   i   cid:31  cid:30  cid:4   cid:4   i  x2 i  E  2   cid:31    = Nµ4 + N N − 1 µ2  cid:4   cid:4    cid:4   2.  xi   cid:30  cid:4    cid:31   4  Finally, we consider the third term on the RHS of  31.45 , and write  xi  =  x4 i +  x3 i xj +  x2 i x2  j +  x2 i xjxk +  xixjxkxl.  i  i  i,j  j cid:3 =i  i,j  j cid:3 =i  i,j,k  k cid:3 =j cid:3 =i  i,j,k,l  l cid:3 =k cid:3 =j cid:3 =i  The expectation values of the second, fourth and ﬁfth sums are zero, and the ﬁrst and third  sums contain N and 3N N− 1  terms respectively  for the third sum, there are N N − 1  2 ways of choosing i and j, and the multinomial coeﬃcient of x2 j is 4!  2!2!  = 6 . Thus  i x2   cid:30  cid:4   E  xi  i   cid:31   4   = Nµ4 + 3N N − 1 µ2  2.  Collecting together terms, we therefore obtain  E[s4] =   N − 1 2  N3  µ4 +   N − 1  N2 − 2N + 3   N3  µ2 2,   31.46   which, together with the result  31.43 , may be substituted into  31.44  to obtain ﬁnally  V [s2] =   N − 1 2 N − 1  µ4 −  N − 1  N − 3  [ N − 1 ν4 −  N − 3 ν2 2 ],  N3  N3  µ2 2  =  N3  1247   31.47    STATISTICS  where in the last line we have used again the fact that, since the population mean is zero, µr = νr. However, result  31.47  holds even when the population mean is not zero.  cid:2   From  31.43 , we see that s2 is a biased estimator of σ2, although the bias becomes negligible for large N. However, it immediately follows that an unbiased  estimator of σ2 is given simply byC where the multiplicative factor N  N − 1  is often called Bessel’s correction. Thus in terms of the sample values xi, i = 1, 2, . . . , N, an unbiased estimator of the population variance σ2 is given by  N − 1   31.48   σ2 =  s2,  N  Using  31.47 , we ﬁnd that the variance of the estimator  σ2 is  C  N cid:4   1  i=1  σ2 =  N − 1  cid:8    xi − ¯x 2. C  cid:7   cid:7  ν4 − N − 3 N − 1 N − 1 C C σ2] → 0 as N → ∞, the statistic  V [s2] =  1 N  N  2   cid:8   ν2 2  ,  C  V [  σ2] =  where νr since E[ estimator of the population variance.  σ2] = σ2 and V [  is the rth central moment of the parent population. We note that, σ2 is also a consistent   31.49   31.4.3 Population standard deviation σ  The standard deviation σ of a population is deﬁned as the positive square root of the population variance σ2  as, indeed, our notation suggests . Thus, it is common practice to take the positive square root of the variance estimator as our estimator for σ. Thus, we take   cid:10    cid:9 C  σ2  1 2  ,  ˆσ =   31.50   σ2 is given by either  31.41  or  31.48 , depending on whether the population where mean µ is known or unknown. Because of the square root in the deﬁnition of ˆσ, it is not possible in either case to obtain an exact expression for E[ ˆσ] and V [ ˆσ]. Indeed, although in each case the estimator is the positive square root of an unbiased estimator of σ2, it is not itself an unbiased estimator of σ. However, the bias does becomes negligible for large N.  cid:1 Obtain approximate expressions for E[ ˆσ] and V [ ˆσ] for a sample of size N in the case where the population mean µ is unknown.  As the population mean is unknown, we use  31.50  and  31.48  to write our estimator in  1248  C  C   the form  31.4 SOME BASIC ESTIMATORS   cid:7   ˆσ =   cid:8   1 2  s,  N  N − 1  cid:7    cid:8   1 2   cid:7    cid:8   where s is the sample standard deviation. The expectation value of this estimator is given by  E[ ˆσ] =  N  N − 1  1 2  E[ s2 1 2] ≈   E[s2] 1 2 = σ.  An approximate expression for the variance of ˆσ may be found using  31.47  and is given by  N  N − 1  cid:13   cid:13   d s2    cid:14   d  1 4s2   cid:14   2  s2=E[s2]  V [s2].   s2 1 2  V [s2]  V [ ˆσ] =  N  N − 1  V [ s2 1 2] ≈ N N − 1 ≈ N N − 1  cid:7   V [ ˆσ] ≈ 1  4Nν2  ν4 − N − 3 N − 1  ν2 2  s2=E[s2]   cid:8   .  cid:2   Using the expressions  31.43  and  31.47  for E[s2] and V [s2] respectively, we obtain  31.4.4 Population moments µr  We may straightforwardly generalise our discussion of estimation of the popula- tion mean µ  = µ1  in subsection 31.4.1 to the estimation of the rth population moment µr. An obvious choice of estimator is the rth sample moment mr. The expectation value of mr is given by  N cid:4   i=1  1 N  E[mr] =  E[xr  i ] =  Nµr N  = µr,  The variance of mr may be found in a similar manner, although the calculation  and so it is an unbiased estimator of µr.  is a little more complicated. We ﬁnd that  V [mr] = E[ mr − µr 2]   cid:30  cid:4   cid:4   i  xr i  =  1 N2 E  x2r i +  =  =  1 N2 E µ2r − µ2 r +  i  1 N  1 N2    2   cid:31  − Nµr  cid:4   cid:4   cid:4   cid:4   j cid:3 =i  i  j cid:3 =i  i  1249   cid:4   i  − 2Nµr  xr i xr  j  i + N2µ2 xr  r    E[xr  i xr j].   31.51    STATISTICS  However, since the sample values xi are assumed to be independent, we have  The number of terms in the sum on the RHS of  31.51  is N N−1 , and so we ﬁnd  E[xr  i xr  j] = E[xr  i ]E[xr  j] = µ2 r .  V [mr] =  µ2r − µ2 r +  1 N  N − 1  N  µ2 r =  µ2r − µ2  r  .  N  Since E[mr] = µr and V [mr] → 0 as N → ∞, the rth sample moment mr is also  a consistent estimator of µr.  cid:1 Find the covariance of the sample moments mr and ms for a sample of size N.   31.52    31.53   We obtain the covariance of the sample moments mr and ms in a similar manner to that used above to obtain the variance of mr. From the deﬁnition of covariance, we have  Cov[mr, ms] = E[ mr − µr  ms − µs ] − Nµr  cid:4   1 N2 E  xr i  =  i   cid:16  cid:30  cid:4   cid:4    cid:31  cid:30  cid:4   cid:4   j  xs j   cid:31  cid:17  − Nµs  cid:4   =  1 N2 E  xr+s i +  i  j cid:3 =i  i  j  − Nµr  xr i xs  j  − Nµs  xs j  i + N2µrµs xr     cid:4   i  Assuming the xi to be independent, we may again use result  31.52  to obtain  Cov[mr, ms] =  [Nµr+s + N N − 1 µrµs − N2µrµs − N2µsµr + N2µrµs]  1 N2 1 N  µr+s +  µr+s − µrµs  =  =  N  .  N  N − 1  µrµs − µrµs  We note that by setting r = s, we recover the expression  31.53  for V [mr].  cid:2   31.4.5 Population central moments νr  We may generalise the discussion of estimators for the second central moment ν2  or equivalently σ2  given in subsection 31.4.2 to the estimation of the rth central moment νr. In particular, we saw in that subsection that our choice of estimator for ν2 depended on whether the population mean µ1 is known; the same is true for the estimation of νr.  Let us ﬁrst consider the case in which µ1 is known. From  30.54 , we may write  νr as  νr = µr − rC1µr−1µ1 + ··· +  −1 k rCkµr−kµk  1 + ··· +  −1 r−1 rCr−1 − 1 µr  1.  If µ1 is known, a suitable estimator is obviously  ˆνr = mr − rC1mr−1µ1 + ··· +  −1 k rCkmr−kµk  1 + ··· +  −1 r−1 rCr−1 − 1 µr  1,  where mr is the rth sample moment. Since µ1 and the binomial coeﬃcients are  1250   31.4 SOME BASIC ESTIMATORS   known  constants, it is immediately clear that E[ˆνr] = νr, and so ˆνr is an unbiased estimator of νr. It is also possible to obtain an expression for V [ˆνr], though the calculation is somewhat lengthy.  In the case where the population mean µ1 is not known, the situation is more complicated. We saw in subsection 31.4.2 that the second sample moment n2  or s2  is not an unbiased estimator of ν2  or σ2 . Similarly, the rth central moment of a sample, nr, is not an unbiased estimator of the rth population central moment νr. However, in all cases the bias becomes negligible in the limit of large N.  As we also found in the same subsection, there are complications in calculating the expectation and variance of n2; these complications increase considerably for general r. Nevertheless, we have derived already in this chapter exact expressions for the expectation value of the ﬁrst few sample central moments, which are valid for samples of any size N. From  31.40 ,  31.43  and  31.46 , we ﬁnd   31.54    31.55    31.56    31.57   E[n1] = 0,  E[n2] =  ν2,  N − 1 N − 1  N  N3  E[n2  2] =  [ N − 1 ν4 +  N2 − 2N + 3 ν2 2 ].  By similar arguments it can be shown that  E[n3] =  E[n4] =   N − 1  N − 2  N − 1  N2  ν3,  N3  [ N2 − 3N + 3 ν4 + 3 2N − 3 ν2 2 ].  From  31.54  and  31.55 , we see that unbiased estimators of ν2 and ν3 are  ˆν2 =  ˆν3 =  N  n2,  N − 1  N − 1  N − 2   N2  n3,   31.58   C σ2 = Ns2  N − 1  is an  where  31.57  simply re-establishes our earlier result that unbiased estimator of σ2.  Unfortunately, the pattern that appears to be emerging in  31.57  and  31.58  is not continued for higher r, as is seen immediately from  31.56 . Nevertheless, in the limit of large N, the bias becomes negligible, and often one simply takes ˆνr = nr. For large N, it may be shown that  E[nr] ≈ νr V [nr] ≈ 1 Cov[nr, ns] ≈ 1  N  N  r + r2ν2ν2   ν2r − ν2  νr+s − νrνs + rsν2νr−1νs−1 − rνr−1νs+1 − sνs−1νr+1   − 2rνr−1νr+1   r−1  1251   STATISTICS  31.4.6 Population covariance Cov[x, y] and correlation Corr[x, y]  So far we have assumed that each of our N independent samples consists of a single number xi. Let us now extend our discussion to a situation in which each sample consists of two numbers xi, yi, which we may consider as being drawn randomly from a two-dimensional population P  x, y . In particular, we now consider estimators for the population covariance Cov[x, y] and for the correlation Corr[x, y].  When µx and µy are known, an appropriate estimator of the population covari-   cid:31    cid:30   N cid:4   i=1  1 N  − µxµy.  xiyi   31.59   ance is  JCov[x, y] = xy − µxµy =  cid:17   cid:23   cid:22 JCov[x, y]  N cid:4    cid:16   =  E  xiyi  1 N  i=1  E  This estimator is unbiased since  − µxµy = E[xiyi] − µxµy = Cov[x, y].  Alternatively, if µx and µy are unknown, it is natural to replace µx and µy in  31.59  by the sample means ¯x and ¯y respectively, in which case we recover the  sample covariance Vxy = xy − ¯x¯y discussed in subsection 31.2.4. This estimator  is biased but an unbiased estimator of the population covariance is obtained by forming  JCov[x, y] =  N  N − 1  Vxy.   31.60    cid:1 Calculate the expectation value of the sample covariance Vxy for a sample of size N.  The sample covariance is given by  Thus its expectation value is given by  Vxy =  xiyi   cid:30   1 N   cid:4   cid:16  cid:4   i   cid:31    cid:30   −  cid:17   1 N  E[Vxy] =  1 N  E  xiyi  i  = E[xiyi] − 1  N2 E   cid:31    cid:31  cid:30   i  xi   cid:4   cid:16  cid:30  cid:4   cid:4   i  1 N  yj  j   cid:4   cid:31  cid:30  cid:4    j  xiyi +  xiyj  i,j  j cid:3 =i  xi  xj  .   cid:31  cid:17   N2 E  − 1   cid:4   i  1252   31.4 SOME BASIC ESTIMATORS  Since the number of terms in the double sum on the RHS is N N − 1 , we have  E[Vxy] = E[xiyi] − 1 = E[xiyi] − 1 = E[xiyi] − 1  N2  N2   NE[xiyi] + N N − 1 E[xiyj]   cid:5   NE[xiyi] + N N − 1 E[xi]E[yj]  N − 1 E[xiyi] +  N − 1 µxµy   cid:6   =  N  Cov[x, y],  N  where we have used the fact that, since the samples are independent, E[xiyj ] = E[xi]E[yj].  cid:2   It is possible to obtain expressions for the variances of the estimators  31.59  and  31.60  but these quantities depend upon higher moments of the population P  x, y  and are extremely lengthy to calculate.  Whether the means µx and µy are known or unknown, an estimator of the  population correlation Corr[x, y] is given by  where JCov[x, y], ˆσx and ˆσy are the appropriate estimators of the population co-  ˆσx ˆσy  ,   31.61    cid:2 Corr[x, y] =  variance and standard deviations. Although this estimator is only asymptotically unbiased, i.e. for large N, it is widely used because of its simplicity. Once again the variance of the estimator depends on the higher moments of P  x, y  and is diﬃcult to calculate.  In the case in which the means µx and µy are unknown, a suitable  but biased   estimator is   cid:2 Corr[x, y] =  N  N − 1  Vxy sxsy  =  N  N − 1  rxy,   31.62   where sx and sy are the sample standard deviations of the xi and yi respectively and rxy is the sample correlation. In the special case when the parent population P  x, y  is Gaussian, it may be shown that, if ρ = Corr[x, y],  JCov[x, y]  E[rxy] = ρ − ρ 1 − ρ2   −2 ,  2N   1 − ρ2 2 + O N  + O N −2 ,  V [rxy] =  1 N   31.63    31.64   from which the expectation value and variance of the estimator  cid:2 Corr[x, y] may be found immediately.  We note ﬁnally that our discussion may be extended, without signiﬁcant al- teration, to the general case in which each data item consists of n numbers xi, yi, . . . , zi.  1253   STATISTICS  31.4.7 A worked example  To conclude our discussion of basic estimators, we reconsider the set of experi- mental data given in subsection 31.2.4. We carry the analysis as far as calculating the standard errors in the estimated population parameters, including the popu- lation correlation.  cid:1 Ten UK citizens are selected at random and their heights and weights are found to be as follows  to the nearest cm or kg respectively :  Person Height  cm  Weight  kg   A 194 75  B 168 53  C 177 72  D 180 80  E 171 75  F 190 75  G 151 57  H 169 67  I 175 46  J 182 68  Estimate the means, µx and µy, and standard deviations, σx and σy, of the two-dimensional joint population from which the sample was drawn, quoting the standard error on the esti- mate in each case. Estimate also the correlation Corr[x, y] of the population, and quote the standard error on the estimate under the assumption that the population is a multivariate Gaussian.  In subsection 31.2.4, we calculated various sample statistics for these data. In particular, we found that for our sample of size N = 10,  ¯x = 175.7,  ¯y = 66.8,  sx = 11.6,  sy = 10.6,  rxy = 0.54.  Let us begin by estimating the means µx and µy. As discussed in subsection 31.4.1, the √ sample mean is an unbiased, consistent estimator of the population mean. Moreover, the standard error on ¯x  say  is σx  N. In this case, however, we do not know the true value   cid:24   µy, with associated standard errors, are  of σx and we must estimate it using Cσx = sx√ N − 1 sy√ N − 1  cid:7   ˆµx = ¯x ± ˆµy = ¯y ±  is Cσx =  approximately by   cid:24   N  N − 1 sx. Thus, our estimates of µx and = 175.7 ± 3.9, = 66.8 ± 3.5.  cid:8   We now turn to estimating σx and σy. As just mentioned, our estimate of σx  say   N  N − 1 sx. Its variance  see the ﬁnal line of subsection 31.4.3  is given  V [ ˆσ] ≈ 1  4Nν2  ν4 − N − 3 N − 1  ν2 2  .  Since we do not know the true values of the population central moments ν2 and ν4, we x =   ˆσ 2, which we σ2 must use their estimated values in this expression. We may take ˆν2 = have already calculated. It still remains, however, to estimate ν4. As implied near the end of subsection 31.4.5, it is acceptable to take ˆν4 = n4. Thus for the xi and yi values, we have  C  N cid:4  N cid:4   i=1  i=1  1 N  1 N   ˆν4 x =   ˆν4 y =   xi − ¯x 4 = 53 411.6   yi − ¯y 4 = 27 732.5  1254   31.5 MAXIMUM-LIKELIHOOD METHOD  Substituting these values into  31.50 , we obtain   cid:7   cid:7   ˆσx =  ˆσy =  N  N − 1 N − 1  N   cid:8   cid:8    cid:7   1 2  1 2  sx ±   ˆV [ ˆσx] 1 2 = 12.2 ± 6.7, sy ±   ˆV [ ˆσy] 1 2 = 11.2 ± 3.6.   31.65    31.66   Finally, we estimate the population correlation Corr[x, y], which we shall denote by ρ.  From  31.62 , we have  Under the assumption that the sample was drawn from a two-dimensional Gaussian population P  x, y , the variance of our estimator is given by  31.64 . Since we do not know the true value of ρ, we must use our estimate ˆρ. Thus, we ﬁnd that the standard error ∆ρ in our estimate is given approximately by  ˆρ =  rxy = 0.60.  N  N − 1  cid:8   ∆ρ ≈ 10  9  1 10  [1 −  0.60 2]2 = 0.05.  cid:2   31.5 Maximum-likelihood method  The population from which the sample x1, x2, . . . , xN is drawn is, in general, unknown. In the previous section, we assumed that the sample values were inde- pendent and drawn from a one-dimensional population P  x , and we considered basic estimators of the moments and central moments of P  x . We did not, how- ever, assume a particular functional form for P  x . We now discuss the process of data modelling, in which a speciﬁc form is assumed for the population.  In the most general case, it will not be known whether the sample values are independent, and so let us consider the full joint population P  x , where x is the point in the N-dimensional data space with coordinates x1, x2, . . . , xN. We then adopt the hypothesis H that the probability distribution of the sample values has some particular functional form L x; a , dependent on the values of some set of parameters ai, i = 1, 2, . . . , m. Thus, we have  P  xa, H  = L x; a ,  where we make explicit the conditioning on both the assumed functional form and on the parameter values. L x; a  is called the likelihood function. Hypotheses of this type form the basis of data modelling and parameter estimation. One proposes a particular model for the underlying population and then attempts to estimate from the sample values x1, x2, . . . , xN the values of the parameters a deﬁning this model.  cid:1 A company measures the duration  in minutes  of the N intervals xi, i = 1, 2, . . . , N values xi are drawn independently from the distribution P  xτ  =  1 τ  exp −x τ , where τ between successive telephone calls received by its switchboard. Suppose that the sample  is the mean interval between calls. Calculate the likelihood function L x; τ .  Since the sample values are independent and drawn from the stated distribution, the  1255   STATISTICS  L x; τ   1  N = 5  N = 10  L x; τ   1  0.5  0  1  0  0.5  0  2 4 6 8 10 12 14 16 18 20  0  2 4 6 8 10 12 14 16 18 20  τ  L x; τ   L x; τ   N = 20  N = 50  0.5  0  1  0.5  τ  τ  0  2 4 6 8 10 12 14 16 18 20  0  2 4 6 8 10 12 14 16 18 20  τ  0  Figure 31.5 Examples of the likelihood function  31.67  for samples of dif- ferent size N. In each case, the true value of the parameter is τ = 4 and the sample values xi are indicated by the short vertical lines. For the purposes of illustration, in each case the likelihood function is normalised so that its maximum value is unity.  likelihood is given by   cid:9  L x; τ  = P  xiτ P  x2τ ··· P  xNτ  − x2   cid:10   =  exp   cid:10    cid:9   cid:13  − x1 − 1  τ  1 τ 1 τN  1 τ  exp  ··· 1 τ  x1 + x2 + ··· + xN   τ   cid:9    cid:10    cid:14   exp  − xN  τ  .  τ  =  exp   31.67  which is to be considered as a function of τ, given that the sample values xi are ﬁxed.  cid:2  The likelihood function  31.67  depends on just a single parameter τ. Plots of the likelihood function, considered as a function of τ, are shown in ﬁgure 31.5 for samples of diﬀerent size N. The true value of the parameter τ used to generate the sample values was 4. In each case, the sample values xi are indicated by the short vertical lines. For the purposes of illustration, the likelihood function in each case has been scaled so that its maximum value is unity  this is, in fact, common practice . We see that when the sample size is small, the likelihood function is √ very broad. As N increases, however, the function becomes narrower  its width is N  and tends to a Gaussian-like shape, with its peak inversely proportional to centred on 4, the true value of τ. We discuss these properties of the likelihood function in more detail in subsection 31.5.6.  1256   31.5 MAXIMUM-LIKELIHOOD METHOD  L x; a   L x; a   ˆa  L x; a   ˆa  L x; a    a    c   a  a   b    d   a  a  ˆa  ˆa  Figure 31.6 Typical shapes of one-dimensional likelihood functions L x; a  encountered in practice, when, for illustration purposes, it is assumed that the parameter a is restricted to the range zero to inﬁnity. The ML estimator in the various cases occurs at:  a  the only stationary point;  b  one of several stationary points;  c  an end-point of the allowed parameter range that is not a stationary point  although stationary points do exist ;  d  an end-point of the allowed parameter range in which no stationary point exists.  31.5.1 The maximum-likelihood estimator  Since the likelihood function L x; a  gives the probability density associated with any particular set of values of the parameters a, our best estimate ˆa of these parameters is given by the values of a for which L x; a  is a maximum. This is called the maximum-likelihood estimator  or ML estimator .  In general, the likelihood function can have a complicated shape when con- sidered as a function of a, particularly when the dimensionality of the space of parameters a1, a2, . . . , aM is large. It may be that the values of some parameters are either known or assumed in advance, in which case the eﬀective dimension- ality of the likelihood function is reduced accordingly. However, even when the likelihood depends on just a single parameter a  either intrinsically or as the result of assuming particular values for the remaining parameters , its form may be complicated when the sample size N is small. Frequently occurring shapes of one-dimensional likelihood functions are illustrated in ﬁgure 31.6, where we have assumed, for deﬁniteness, that the allowed range of the parameter a is zero to inﬁnity. In each case, the ML estimate ˆa is also indicated. Of course, the ‘shape’ of higher-dimensional likelihood functions may be considerably more complicated. In many simple cases, however, the likelihood function L x; a  has a single  1257   STATISTICS  maximum that occurs at a stationary point  the likelihood function is then termed unimodal . In this case, the ML estimators of the parameters ai, i = 1, 2, . . . , M, may be found without evaluating the full likelihood function L x; a . Instead, one simply solves the M simultaneous equations  = 0  for i = 1, 2, . . . , M.   31.68   Since ln z is a monotonically increasing function of z  and therefore has the same stationary points , it is often more convenient, in fact, to maximise the log-likelihood function, ln L x; a , with respect to the ai. Thus, one may, as an alternative, solve the equations   cid:20  cid:20  cid:20  cid:20   ∂L ∂ai  a=ˆa   cid:20  cid:20  cid:20  cid:20   ∂ ln L  ∂ai  a=ˆa  = 0  for i = 1, 2, . . . , M.   31.69   Clearly,  31.68  and  31.69  will lead to the same ML estimates ˆa of the parameters. In either case, it is, of course, prudent to check that the point a = ˆa is a local maximum.   cid:1 Find the ML estimate of the parameter τ in the previous example, in terms of the measured values xi, i = 1, 2, . . . , N.  From  31.67 , the log-likelihood function in this case is given by  ln L x; τ  =  ln   31.70   Diﬀerentiating with respect to the parameter τ and setting the result equal to zero, we ﬁnd   cid:10   ln τ +  .  xi τ   cid:7   N cid:4   i=1  −xi τ e  1 τ   cid:7   = − N cid:4   i=1   cid:8    cid:9   = − N cid:4   cid:8   i=1  − xi τ2  1 τ  = 0.  ∂ ln L  ∂τ  Thus the ML estimate of the parameter τ is given by   31.71   which is simply the sample mean of the N measured intervals.  cid:2   In the previous example we assumed that the sample values xi were drawn independently from the same parent distribution. The ML method is more ﬂexible than this restriction might seem to imply, and it can equally well be applied to the common case in which the samples xi are independent but each is drawn from a diﬀerent distribution.  N cid:4   i=1  ˆτ =  1 N  xi,  1258   and so the full log-likelihood function is given by  31.5 MAXIMUM-LIKELIHOOD METHOD   cid:1 In an experiment, N independent measurements xi of some quantity are made. Suppose that the random measurement error on the i th sample value is Gaussian distributed with mean zero and known standard deviation σi. Calculate the ML estimate of the true value µ of the quantity being measured.  As the measurements are independent, the likelihood factorises:  where {σk} denotes collectively the set of known standard deviations σ1, σ2, . . . , σN. The  individual distributions are given by  L x; µ,{σk}  =  N cid:3   i=1  P  xiµ, σi ,  cid:13   −  xi − µ 2  2σ2 i   cid:14   .  P  xiµ, σi  =  exp  1 cid:24  N cid:4   2πσ2 i   cid:13   ln L x; µ,{σk}  = − 1  2  ln 2πσ2  i   +   cid:14   .   xi − µ 2  σ2 i  i=1  N cid:4   cid:11   cid:11   i=1  N  ∂ ln L  ∂µ  =  xi − µ  σ2 i  = 0,  ˆµ =  i=1 xi σ2 i   i=1 1 σ2 i    N  .  Diﬀerentiating this expression with respect to µ and setting the result equal to zero, we ﬁnd  from which we obtain the ML estimator   31.72   This estimator is commonly used when averaging data with diﬀerent statistical weights wi = 1 σ2 i have the same value the estimator reduces to the sample mean of the data xi.  cid:2   i . We note that when all the variances σ2  There is, in fact, no requirement in the ML method that the sample values be independent. As an illustration, we shall generalise the above example to a case in which the measurements xi are not all independent. This would occur, for example, if these measurements were based at least in part on the same data.  cid:1 In an experiment N measurements xi of some quantity are made. Suppose that the random measurement errors on the samples are drawn from a joint Gaussian distribution with mean zero and known covariance matrix V. Calculate the ML estimate of the true value µ of the quantity being measured.   cid:18 − 1 2  x − µ1 TV−1 x − µ1    cid:19   ,  exp  From  30.148 , the likelihood in this case is given by  L x; µ, V  =  1   2π N 2V1 2  cid:18   where x is the column matrix with components x1, x2, . . . , xN and 1 is the column matrix with all components equal to unity. Thus, the log-likelihood function is given by  ln L x; µ, V  = − 1  2  N ln 2π  + lnV +  x − µ1 TV−1 x − µ1   .   cid:19   1259   Diﬀerentiating with respect to µ and setting the result equal to zero gives  Thus, the ML estimator is given by  STATISTICS  = 1TV−1 x − µ1  = 0.  ∂ ln L  ∂µ  1TV−1x 1TV−11  =  ˆµ =  −1 ijxj −1 ij  i,j V i,j V  .   cid:11   cid:11   In the case of uncorrelated errors in measurement,  V reduces to that given in  31.72 .  cid:2   −1 ij = δij  σ2  i and our estimator  In all the examples considered so far, the likelihood function has been eﬀectively one-dimensional, either instrinsically or under the assumption that the values of all but one of the parameters are known in advance. As the following example involving two parameters shows, the application of the ML method to the estimation of several parameters simultaneously is straightforward.  cid:1 In an experiment N measurements xi of some quantity are made. Suppose the random error on each sample value is drawn independently from a Gaussian distribution of mean zero but unknown standard deviation σ  which is the same for each measurement . Calculate the ML estimates of the true value µ of the quantity being measured and the standard deviation σ of the random errors.  In this case the log-likelihood function is given by   cid:13   N cid:4   i=1  ln 2πσ2  +   cid:14   .   xi − µ 2  σ2  Taking partial derivatives of ln L with respect to µ and σ and setting the results equal to zero at the joint estimate ˆµ, ˆσ, we obtain  2  ln L x; µ, σ  = − 1 N cid:4    31.73    31.74   In principle, one should solve these two equations simultaneously for ˆµ and ˆσ, but in this case we notice that the ﬁrst is solved immediately by  where ¯x is the sample mean. Substituting this result into the second equation, we ﬁnd  N cid:4   i=1  i=1   xi − ˆµ 2  ˆσ3  xi − ˆµ  = 0,  ˆσ2  − N cid:4  N cid:4   i=1  xi = ¯x,  1 ˆσ  = 0.  ˆσ =   xi − ¯x 2 = s,  1 N  ˆµ =    1  i=1  N cid:4   N  i=1  1260  where s is the sample standard deviation. As shown in subsection 31.4.3, s is a biased estimator of σ. The reason why the ML method may produce a biased estimator is discussed in the next subsection.  cid:2    31.5 MAXIMUM-LIKELIHOOD METHOD  31.5.2 Transformation invariance and bias of ML estimators  An extremely useful property of ML estimators is that they are invariant to parameter transformations. Suppose that, instead of estimating some parameter a of the assumed population, we wish to estimate some function α a  of the parameter. The ML estimator ˆα a  is given by the value assumed by the function α a  at the maximum point of the likelihood, which is simply equal to α ˆa . Thus, we have the very convenient property  ˆα a  = α ˆa .  We do not have to worry about the distinction between estimating a and estimat- ing a function of a. This is not true, in general, for other estimation procedures.  cid:1 A company measures the duration  in minutes  of the N intervals xi, i = 1, 2, . . . , N, values xi are drawn independently from the distribution P  xτ  =  1 τ  exp −x τ . Find the between successive telephone calls received by its switchboard. Suppose that the sample  ML estimate of the parameter λ = 1 τ.  This is the same problem as the ﬁrst one considered in subsection 31.5.1. In terms of the new parameter λ, the log-likelihood function is given by  ln L x; λ  =  ln λe   ln λ − λxi .  Diﬀerentiating with respect to λ and setting the result equal to zero, we have  N cid:4   cid:8   i=1  = 0.  N cid:4   i=1  ∂ ln L  ∂λ  =   cid:30   −λxi   =  cid:7   1 λ  − xi  cid:31 −1  N cid:4  N cid:4   i=1  ˆλ =  1 N  xi  i=1  −1.  = ¯x  Thus, the ML estimator of the parameter λ is given by   31.75   Referring back to  31.71 , we see that, as expected, the ML estimators of λ and τ are related by ˆλ = 1 ˆτ.  cid:2   Although this invariance property is useful it also means that, in general, ML estimators may be biased. In particular, one must be aware of the fact that even if ˆa is an unbiased ML estimator of a it does not follow that the estimator ˆα a  is also unbiased. In the limit of large N, however, the bias of ML estimators always tends to zero. As an illustration, it is straightforward to show  see exercise 31.8  that the ML estimators ˆτ and ˆλ in the above example have expectation values  E[ˆτ] = τ  and  E[ˆλ] =  N  N − 1  λ.   31.76   In fact, since ˆτ = ¯x and the sample values are independent, the ﬁrst result follows immediately from  31.40 . Thus, ˆτ is unbiased, but ˆλ = 1 ˆτ is biased, albeit that the bias tends to zero for large N.  1261   STATISTICS  31.5.3 Efﬁciency of ML estimators  We showed in subsection 31.3.2 that Fisher’s inequality puts a lower limit on the variance V [ˆa] of any estimator of the parameter a. Under our hypothesis H on p. 1255, the functional form of the population is given by the likelihood function,  i.e. P  xa, H  = L x; a . Thus, if this hypothesis is correct, we may replace P by  L in Fisher’s inequality  31.18 , which then reads   cid:7    cid:8   2  ?   cid:13    cid:14   V [ˆa] ≥  1 +  ∂b ∂a  − ∂2 ln L ∂a2  ,  E  where b is the bias in the estimator ˆa. We usually denote the RHS by Vmin.  An important property of ML estimators is that if there exists an eﬃcient estimator ˆaeﬀ , i.e. one for which V [ˆaeﬀ ] = Vmin, then it must be the ML estimator or some function thereof. This is easily shown by replacing P by L in the proof of Fisher’s inequality given in subsection 31.3.2. In particular, we note that the equality in  31.22  holds only if h x  = cg x , where c is a constant. Thus, if an eﬃcient estimator ˆaeﬀ exists, this is equivalent to demanding that  = c[ˆaeﬀ − α a ].  ∂ ln L  ∂a   cid:20  cid:20  cid:20  cid:20   Now, the ML estimator ˆaML is given by ⇒  ∂ ln L  = 0  ∂a  a=ˆaML  c[ˆaeﬀ − α ˆaML ] = 0,  which, in turn, implies that ˆaeﬀ must be some function of ˆaML.  cid:1 Show that the ML estimator ˆτ given in  31.71  is an eﬃcient estimator of the parameter τ.  As shown in  31.70 , the log-likelihood function in this case is  i=1 Diﬀerentiating twice with respect to τ, we ﬁnd   cid:10    cid:9   ln L x; τ  = − N cid:4   cid:8   cid:7  N cid:4  − 2xi  cid:7  τ3   cid:14   1 τ2  i=1  =  N τ2  ln τ +  .  xi τ   cid:30   τN  1 − 2  cid:8    cid:31   xi  ,  N cid:4   i=1  E  ∂2 ln L  ∂τ2  =  N τ2  1 − 2  τ  E[xi]  = − N τ2 ,  ∂2 ln L  ∂τ2  =   cid:13   and so the expectation value of this expression is   31.77   where we have used the fact that E[x] = τ. Setting b = 0 in  31.18 , we thus ﬁnd that for any unbiased estimator of τ,  N From  31.76 , we see that the ML estimator ˆτ = i xi N is unbiased. Moreover, using the fact that V [x] = τ2, it follows immediately from  31.40  that V [ˆτ] = τ2 N. Thus ˆτ is a minimum-variance estimator of τ.  cid:2   V [ˆτ] ≥ τ2  .   cid:11   1262   31.5 MAXIMUM-LIKELIHOOD METHOD  31.5.4 Standard errors and conﬁdence limits on ML estimators  The ML method provides a procedure for obtaining a particular set of estimators  ˆaML for the parameters a of the assumed population P  xa . As for any other set  of estimators, the associated standard errors, covariances and conﬁdence intervals can be found as described in subsections 31.3.3 and 31.3.4.  cid:1 A company measures the duration  in minutes  of the 10 intervals xi, i = 1, 2, . . . , 10, between successive telephone calls made to its switchboard to be as follows:  0.43  0.24  3.03  1.93  1.16  8.65  5.33  6.06  5.62  5.22.  Supposing that the sample values are drawn independently from the probability distribution  P  xτ  =  1 τ  exp −x τ , ﬁnd the ML estimate of the mean τ and quote an estimate of  the standard error on your result.  As shown in  31.71  the  unbiased  ML estimator ˆτ in this case is simply the sample mean ¯x = 3.77. Also, as shown in subsection 31.5.3, ˆτ is a minimum-variance estimator with V [ˆτ] = τ2 N. Thus, the standard error in ˆτ is simply  σˆτ =  τ√  .  N   31.78    31.79    31.80   Since we do not know the true value of τ, however, we must instead quote an estimate ˆσˆτ of the standard error, obtained by substituting our estimate ˆτ for τ in  31.78 . Thus, we quote our ﬁnal result as  For comparison, the true value used to create the sample was τ = 4.  cid:2   τ = ˆτ ± ˆτ√  = 3.77 ± 1.19.  N  For the particular problem considered in the above example, it is in fact possible to derive the full sampling distribution of the ML estimator ˆτ using characteristic functions, and it is given by  P  ˆττ  =  NN  N − 1 !  ˆτN−1 τN  exp  − N ˆτ  τ   cid:7    cid:8   ,  where N is the size of the sample. This function is plotted in ﬁgure 31.7 for the case τ = 4 and N = 10, which pertains to the above example. Knowledge of the analytic form of the sampling distribution allows one to place conﬁdence limits on the estimate ˆτ obtained, as discussed in subsection 31.3.4.  cid:1 Using the sample values in the above example, obtain the 68% central conﬁdence interval on the value of τ.  For the sample values given, our observed value of the ML estimator is ˆτobs = 3.77. Thus, from  31.28  and  31.29 , the 68% central conﬁdence interval [τ−, τ+] on the value of τ is  found by solving the equations  cid:21   cid:21  ∞ −∞ P  ˆττ+  dˆτ = 0.16, P  ˆττ−  dˆτ = 0.16,  ˆτobs  ˆτobs  1263   STATISTICS  P  ˆττ   0.4  0.3  0.2  0.1  0  0  2  4  6  8  10  12  14  ˆτ  Figure 31.7 The sampling distribution P  ˆττ  for the estimator ˆτ for the case  τ = 4 and N = 10.  where P  ˆττ  is given by  31.80  with N = 10. The above integrals can be evaluated analytically but the calculations are rather cumbersome. It is much simpler to evaluate them by numerical integration, from which we ﬁnd [τ−, τ+] = [2.86, 5.46]. Alternatively, we could quote the estimate and its 68% conﬁdence interval as  τ = 3.77 +1.69−0.91.  Thus we see that the 68% central conﬁdence interval is not symmetric about the estimated value, and diﬀers from the standard error calculated above. This is a result of the  non-  Gaussian  shape of the sampling distribution P  ˆττ , apparent in ﬁgure 31.7.  cid:2   In many problems, however, it is not possible to derive the full sampling distribution of an ML estimator ˆa in order to obtain its conﬁdence intervals. Indeed, one may not even be able to obtain an analytic formula for its standard error σˆa. This is particularly true when one is estimating several parameter ˆa simultaneously, since the joint sampling distribution will be, in general, very complicated. Nevertheless, as we discuss below, the likelihood function L x; a  itself can be used very simply to obtain standard errors and conﬁdence intervals. The justiﬁcation for this has its roots in the Bayesian approach to statistics, as opposed to the more traditional frequentist approach we have adopted here. We now give a brief discussion of the Bayesian viewpoint on parameter estimation.  31.5.5 The Bayesian interpretation of the likelihood function  As stated at the beginning of section 31.5, the likelihood function L x; a  is deﬁned by  P  xa, H  = L x; a ,  1264   31.5 MAXIMUM-LIKELIHOOD METHOD  where H denotes our hypothesis of an assumed functional form. Now, using Bayes’ theorem  see subsection 30.2.3 , we may write  P  ax, H  =  P  xa, H P  aH   P  xH   ,   31.81   which provides us with an expression for the probability distribution P  ax, H   of the parameters a, given the  ﬁxed  data x and our hypothesis H, in terms of other quantities that we may assign. The various terms in  31.81  have special formal names, as follows.    The quantity P  aH  on the RHS is the prior probability, which represents our  state of knowledge of the parameter values  given the hypothesis H  before we have analysed the data.    This probability is modiﬁed by the experimental data x through the likelihood P  xa, H .   When appropriately normalised by the evidence P  xH , this yields the posterior probability P  ax, H , which is the quantity of interest.   The posterior encodes all our inferences about the values of the parameters a. Strictly speaking, from a Bayesian viewpoint, this entire function, P  ax, H , is  the ‘answer’ to a parameter estimation problem.  Given a particular hypothesis, the  normalising  evidence factor P  xH  is  unimportant, since it does not depend explicitly upon the parameter values a. Thus, it is often omitted and one considers only the proportionality relation  P  ax, H  ∝ P  xa, H P  aH .  cid:1    31.82   If necessary, the posterior distribution can be normalised empirically, by requiring P  ax, H  dma = 1, where the integral extends over that it integrates to unity, i.e. all values of the parameters a1, a2, . . . , am. The prior P  aH  in  31.82  should reﬂect our entire knowledge concerning the  values of the parameters a, before the analysis of the current data x. For example, there may be some physical reason to require some or all of the parameters to lie in a given range. If we are largely ignorant of the values of the parameters, we often indicate this by choosing a uniform  or very broad  prior,  P  aH  = constant,  in which case the posterior distribution is simply proportional to the likelihood. In this case, we thus have  P  ax, H  ∝ L x; a .   31.83   In other words, if we assume a uniform prior then we can identify the posterior distribution  up to a normalising factor  with L x; a , considered as a function of the parameters a.  1265   STATISTICS  Thus, a Bayesian statistician considers the ML estimates ˆaML of the parameters  to be the values that maximise the posterior P  ax, H  under the assumption of  a uniform prior. More importantly, however, a Bayesian would not calculate the standard error or conﬁdence interval on this estimate using the  classical  method employed in subsection 31.3.4. Instead, a far more straightforward approach is adopted. Let us assume, for the moment, that one is estimating just a single parameter a. Using  31.83 , we may determine the values a− and a+ such that  a− −∞ L x; a  da = α,  L x; a  da = β.  Pr a < a−x, H  = Pr a > a+x, H  =  cid:21    cid:21   cid:21  ∞  a+  a−   cid:1   where it is assumed that the likelihood has been normalised in such a way that  L x; a  da = 1. Combining these equations gives  Pr a− ≤ a < a+x, H  =  a+  L x; a  da = 1 − α − β,   31.84   and [a−, a+] is the Bayesian conﬁdence interval on the value of a at the conﬁdence level 1 − α − β. As in the case of classical conﬁdence intervals, one often quotes  the central conﬁdence interval, for which α = β. Another common choice  where possible  is to use the two values a− and a+ satisfying  31.84 , for which L x; a−  = L x; a+ .  It should be understood that a frequentist would consider the Bayesian conﬁ- dence interval as an approximation to the  classical  conﬁdence interval discussed in subsection 31.3.4. Conversely, a Bayesian would consider the conﬁdence inter- val deﬁned in  31.84  to be the more meaningful. In fact, the diﬀerence between the Bayesian and classical conﬁdence intervals is rather subtle. The classical con- ﬁdence interval is deﬁned in such a way that if one took a large number of samples each of size N and constructed the conﬁdence interval in each case then the proportion of cases in which the true value of a would be contained within the  interval is 1− α− β. For the Bayesian conﬁdence interval, one does not rely on the  frequentist concept of a large number of repeated samples. Instead, its meaning is that, given the single sample x  and our hypothesis H for the functional form of  the population , the probability that a lies within the interval [a−, a+] is 1− α− β.  By adopting the Bayesian viewpoint, the likelihood function L x; a  may also be used to obtain an approximation ˆσˆa to the standard error in the ML estimator; the approximation is given by   cid:7  − ∂2 ln L ∂a2   cid:20  cid:20  cid:20  cid:20   a=ˆa   cid:8 −1 2  .  ˆσˆa =   31.85   Clearly, if L x; a  were a Gaussian centred on a = ˆa then ˆσˆa would be its standard deviation. Indeed, in this case, the resulting ‘one-sigma’ limits would constitute a  1266   31.5 MAXIMUM-LIKELIHOOD METHOD  L x; τ   0.4  0.3  0.2  0.1  0  0  2  4  6  8  10  12  14  τ  Figure 31.8 The likelihood function L x; τ   normalised to unit area  for the sample values given in the worked example in subsection 31.5.4 and indicated here by short vertical lines.  68.3% Bayesian central conﬁdence interval. Even when L x; a  is not Gaussian, however,  31.85  is often used as a measure of the standard error.   cid:1 For the sample data given in subsection 31.5.4, use the likelihood function to estimate the standard error ˆσˆτ in the ML estimator ˆτ and obtain the Bayesian 68% central conﬁdence interval on τ.  We showed in  31.67  that the likelihood function in this case is given by   cid:14   L x; τ  =  exp  1 τN   x1 + x2 + ··· + xN   .  where xi, i = 1, 2, . . . , N, denotes the sample value and N = 10. This likelihood function is plotted in ﬁgure 31.8, after normalising  numerically  to unit area. The short vertical lines in the ﬁgure indicate the sample values. We see that the likelihood function peaks at the ML estimate ˆτ = 3.77 that we found in subsection 31.5.4. Also, from  31.77 , we have   cid:13   − 1  τ   cid:30    cid:20  cid:20  cid:20  cid:20    cid:31   xi  .  N cid:4   i=1  ∂2 ln L  ∂τ2  =  2  N τ   cid:11    cid:7   τN  1 − 2  cid:8 −1 2  ˆσˆτ =  − ∂2 ln L ∂τ2  τ=ˆτ  =  = 1.19,  ˆτ√  N  Remembering that ˆτ =  i xi N, our estimate of the standard error in ˆτ is  which is precisely the estimate of the standard error we obtained in subsection 31.5.4. It should be noted, however, that in general we would not expect the two estimates of standard error made by the diﬀerent methods to be identical.  In order to calculate the Bayesian 68% central conﬁdence interval, we must determine the values a− and a+ that satisfy  31.84  with α = β = 0.16. In this case, the calculation can be performed analytically but is somewhat tedious. It is trivial, however, to determine a− and a+ numerically and we ﬁnd the conﬁdence interval to be [3.16, 6.20]. Thus we can quote our result with 68% central conﬁdence limits as  τ = 3.77 +2.43−0.61.  1267   STATISTICS  By comparing this result with that given towards the end of subsection 31.5.4, we see that, as we might expect, the Bayesian and classical conﬁdence intervals diﬀer somewhat.  cid:2   The above discussion is generalised straightforwardly to the estimation of several parameters a1, a2, . . . , aM simultaneously. The elements of the inverse of the covariance matrix of the ML estimators can be approximated by   cid:20  cid:20  cid:20  cid:20    V−1 ij = − ∂2 ln L  .  ∂ai∂aj  a=ˆa   31.86   From  31.36 , we see that  at least for unbiased estimators  the expectation value of  31.86  is equal to the element Fij of the Fisher matrix. straightforward. For a given conﬁdence level 1 − α  say , it is most common  The construction of a multi-dimensional Bayesian conﬁdence region is also  to construct the conﬁdence region as the M-dimensional region R in a-space, bounded by the ‘surface’ L x; a  = constant, for which   cid:21   R  L x; a  dMa = 1 − α,  where it is assumed that L x; a  is normalised to unit volume. Moreover, we see from  31.83  that  assuming a uniform prior probability  we may obtain the marginal posterior distribution for any parameter ai simply by integrating the likelihood function L x; a  over the other parameters:   cid:21    cid:21   P  aix, H  =  ···  L x; a  da1 ··· dai−1dai+1 ··· daM.   cid:1   Here the integral extends over all possible values of the parameters, and again is it assumed that the likelihood function is normalised in such a way that L x; a  dMa = 1. This marginal distribution can then be used as above to  determine Bayesian conﬁdence intervals on each ai separately.  cid:1 Ten independent sample values xi, i = 1, 2, . . . , 10, are drawn at random from a Gaussian distribution with unknown mean µ and standard deviation σ. The sample values are as follows  to two decimal places :  2.22  2.56  1.07  0.24  0.18  0.95  2.09  1.81  Find the Bayesian 95% central conﬁdence intervals on µ and σ separately.  The likelihood function in this case is  L x; µ, σ  =  2πσ2   −N 2 exp  − 1 2σ2  Assuming uniform priors on µ and σ  over their natural ranges of −∞ → ∞ and 0 → ∞  i=1  respectively , we may identify this likelihood function with the posterior probability, as in  31.83 . Thus, the marginal posterior distribution on µ is given by   31.87   0.73 −0.79 N cid:4    cid:16    cid:17   .   xi − µ 2  cid:17   N cid:4   i=1  P  µx, H  ∝  1 σN  exp  − 1 2σ2   xi − µ 2  dσ.   cid:21  ∞  0   cid:16   1268   31.5 MAXIMUM-LIKELIHOOD METHOD  By substituting σ = 1 u  so that dσ = −du u2  and integrating by parts either  N − 2  2 or  N − 3  2 times, we ﬁnd  P  µx, H  ∝ cid:18   cid:11    cid:19 − N−1  2 N ¯x − µ 2 + Ns2  cid:21  ∞  ,  i xi − µ 2 = N ¯x− µ 2 + Ns2, ¯x being the sample mean  where we have used the fact that and s2 the sample variance. We may now obtain the 95% central conﬁdence interval by ﬁnding the values µ− and µ+ for which −∞ P  µx, H  dµ = 0.025  P  µx, H  dµ = 0.025.   cid:21   and  µ−  µ+  The normalisation of the posterior distribution and the values µ− and µ+ are easily obtained by numerical integration. Substituting in the appropriate values N = 10, ¯x = 1.11 and s = 1.01, we ﬁnd the required conﬁdence interval to be [0.29, 1.97].  To obtain a conﬁdence interval on σ, we must ﬁrst obtain the corresponding marginal  i xi−µ 2 = N ¯x−µ 2 +Ns2,  posterior distribution. From  31.87 , again using the fact that this is given by   cid:7    cid:8  cid:21  ∞  P  σx, H  ∝ 1  exp  σN  − Ns2 2σ2   cid:14    cid:13    cid:11  − N ¯x − µ 2  cid:8   2σ2  dµ.  Noting that the integral of a one-dimensional Gaussian is proportional to σ, we conclude that  P  σx, H  ∝ 1 σN−1  exp  − Ns2 2σ2  .  The 95% central conﬁdence interval on σ can then be found in an analogous manner to that on µ, by solving numerically the equations  P  σx, H  dσ = 0.025  and  P  σx, H  dσ = 0.025.  −∞ exp   cid:7    cid:21  ∞  σ+   cid:21   σ−  0  We ﬁnd the required interval to be [0.76, 2.16].  cid:2   31.5.6 Behaviour of ML estimators for large N  As mentioned in subsection 31.3.6, in the large-sample limit N → ∞, the sampling  distribution of a set of  consistent  estimators ˆa, whether ML or not, will tend, in general, to a multivariate Gaussian centred on the true values a. This is a  direct consequence of the central limit theorem. Similarly, in the limit N → ∞ the  likelihood function L x; a  also tends towards a multivariate Gaussian but one centred on the ML estimate s  ˆa. Thus ML estimators are always asymptotically consistent. This limiting process was illustrated for the one-dimensional case by ﬁgure 31.5.  Thus, as N becomes large, the likelihood function tends to the form   cid:18 − 1   cid:19   L x; a  = Lmax exp  2 Q a, ˆa   ,  where Q denotes the quadratic form  Q a, ˆa  =  a − ˆa TV−1 a − ˆa   1269   and the matrix V−1 is given by cid:5    cid:6   STATISTICS   cid:20  cid:20  cid:20  cid:20   V−1  = − ∂2 ln L  ∂ai∂aj  ij  .  a=ˆa  Moreover, in the limit of large N, this matrix tends to the Fisher matrix given in   31.36 , i.e. V−1 → F. Hence ML estimators are asymptotically minimum-variance.  Comparison of the above results with those in subsection 31.3.6 shows that the large-sample limit of the likelihood function L x; a  has the same form as the  large-sample limit of the joint estimator sampling distribution P  ˆaa . The only diﬀerence is that P  ˆaa  is centred in ˆa-space on the true values ˆa = a whereas  L x; a  is centred in a-space on the ML estimates a = ˆa. From ﬁgure 31.4 and its accompanying discussion, we therefore conclude that, in the large-sample limit, the Bayesian and classical conﬁdence limits on the parameters coincide.  31.5.7 Extended maximum-likelihood method  It is sometimes the case that the number of data items N in our sample is itself a random variable. Such experiments are typically those in which data are collected for a certain period of time during which events occur at random in some way, as opposed to those in which a prearranged number of data items are collected. In particular, let us consider the case where the sample values x1, x2, . . . , xN are drawn independently from some distribution P  xa  and the sample size N is a random variable described by a Poisson distribution with mean λ, i.e. N ∼ Po λ .  The likelihood function in this case is given by  L x, N; λ, a  =  −λ  e  λN N!  P  xia ,   31.88   N cid:3   i=1  and is often called the extended likelihood function. The function L x; λ, a  can be used as before to estimate parameter values or obtain conﬁdence intervals. Two distinct cases arise in the use of the extended likelihood function, depending on whether the Poisson parameter λ is a function of the parameters a or is an independent parameter.  Let us ﬁrst consider the case in which λ is a function of the parameters a. From   31.88 , we can write the extended log-likelihood function as  ln L = N ln λ a  − λ a  +  ln P  xia  = −λ a  +  ln[λ a P  xia ].  N cid:4   i=1  N cid:4   i=1  where we have ignored terms not depending on a. The ML estimates ˆa of the parameters can then be found in the usual way, and the ML estimate of the Poisson parameter is simply ˆλ = λ ˆa . The errors on our estimators ˆa will be, in general, smaller than those obtained in the usual likelihood approach, since our estimate includes information from the value of N as well as the sample values xi.  1270   31.6 THE METHOD OF LEAST SQUARES  The other possibility is that λ is an independent parameter and not a function  of the parameters a. In this case, the extended log-likelihood function is  ln L = N ln λ − λ +  ln P  xia ,   31.89   where we have omitted terms not depending on λ or a. Diﬀerentiating with respect to λ and setting the result equal to zero, we ﬁnd that the ML estimate of λ is simply  N cid:4   i=1  ˆλ = N.  By diﬀerentiating  31.89  with respect to the parameters ai and setting the results equal to zero, we obtain the usual ML estimates ˆai of their values. In this case, however, the errors in our estimates will be larger, in general, than those in the standard likelihood approach, since they must include the eﬀect of statistical uncertainty in the parameter λ.  31.6 The method of least squares  The method of least squares is, in fact, just a special case of the method of maximum likelihood. Nevertheless, it is so widely used as a method of parameter estimation that it has acquired a special name of its own. At the outset, let us suppose that a data sample consists of a set of pairs  xi, yi , i = 1, 2, . . . , N. For example, these data might correspond to the temperature yi measured at various points xi along some metal rod.  For the moment, we will suppose that the xi are known exactly, whereas there exists a measurement error  or noise  ni on each of the values yi. Moreover, let us assume that the true value of y at any position x is given by some function y = f x; a  that depends on the M unknown parameters a. Then  yi = f xi; a  + ni.  Our aim is to estimate the values of the parameters a from the data sample.  Bearing in mind the central limit theorem, let us suppose that the ni are drawn from a Gaussian distribution with no systematic bias and hence zero mean. In the most general case the measurement errors ni might not be independent but be described by an N-dimensional multivariate Gaussian with non-trivial covariance matrix N, whose elements Nij = Cov[ni, nj] we assume to be known. Under these assumptions it follows from  30.148 , that the likelihood function is  L x, y; a  =  1   2π N 2N1 2  exp  2 χ2 a   ,   cid:18 − 1   cid:19   1271   STATISTICS  N cid:4   i,j=1  where the quantity denoted by χ2 is given by the quadratic form  χ2 a  =  [yi − f xi; a ] N−1 ij [yj − f xj; a ] =  y − f TN−1 y − f .   31.90   In the last equality, we have rewritten the expression in matrix notation by deﬁning the column vector f with elements fi = f xi; a . We note that in the  common  special case in which the measurement errors ni are independent, their covariance matrix takes the diagonal form N = diag σ2 N , where σi is the standard deviation of the measurement error ni. In this case, the expression  31.90  for χ2 reduces to  2, . . . , σ2  1, σ2   cid:13   N cid:4   i=1   cid:14   2  .  yi − f xi; a   σi  χ2 a  =  The least-squares  LS  estimators ˆaLS of the parameter values are deﬁned as those that minimise the value of χ2 a ; they are usually determined by solving the M equations  = 0  for i = 1, 2, . . . , M.   31.91    cid:20  cid:20  cid:20  cid:20   ∂χ2 ∂ai  a=ˆaLS  Clearly, if the measurement errors ni are indeed Gaussian distributed, as assumed above, then the LS and ML estimators of the parameters a coincide. Because of its relative simplicity, the method of least squares is often applied to cases in which the ni are not Gaussian distributed. The resulting estimators ˆaLS are not the ML estimators, and the best that can be said in justiﬁcation is that the method is an obviously sensible procedure for parameter estimation that has stood the test of time.  Finally, we note that the method of least squares is easily extended to the case in which each measurement yi depends on several variables, which we denote by xi. For example, yi might represent the temperature measured at the  three- dimensional  position xi in a room. In this case, the data is modelled by a function y = f xi; a , and the remainder of the above discussion carries through unchanged.  31.6.1 Linear least squares  We have so far made no restriction on the form of the function f x; a . It so happens, however, that, for a model in which f x; a  is a linear function of the parameters a1, a2, . . . , aM, one can always obtain analytic expressions for the LS estimators ˆaLS and their variances. The general form of this kind of model is  f x; a  =  aihi x ,   31.92   M cid:4   i=1  1272   M cid:4   j=1  31.6 THE METHOD OF LEAST SQUARES  where {h1 x , h2 x , . . . , hM x } is some set of linearly independent ﬁxed functions  of x, often called the basis functions. Note that the functions hi x  themselves may be highly non-linear functions of x. The ‘linear’ nature of the model  31.92  refers only to its dependence on the parameters ai. Furthermore, in this case, it may be shown that the LS estimators ˆai have zero bias and are minimum-variance, irrespective of the probability density function from which the measurement errors ni are drawn.  In order to obtain analytic expressions for the LS estimators ˆaLS, it is convenient  to write  31.92  in the form  f x; a  =  Rijaj,   31.93   where Rij = hj xi  is an element of the response matrix R of the experiment. The expression for χ2 given in  31.90  can then be written, in matrix notation, as  χ2 a  =  y − Ra TN−1 y − Ra .  The LS estimates of the parameters a are now found, as shown in  31.91 , by diﬀerentiating  31.94  with respect to the ai and setting the resulting expressions  equal to zero. Denoting by ∇χ2 the vector with elements ∂χ2 ∂ai, we ﬁnd  ∇χ2 = −2RTN−1 y − Ra .   31.94    31.95   This can be veriﬁed by writing out the expression  31.94  in component form and diﬀerentiating directly.  cid:1 Verify result  31.95  by formulating the calculation in component form.  To make the derivation less cumbersome, let us adopt the summation convention discussed in section 26.1, in which it is understood that any subscript that appears exactly twice in any term of an expression is to be summed over all the values that a subscript in that position can take. Thus, writing  31.94  in component form, we have  χ2 a  =  yi − Rikak  N  −1 ij yj − Rjlal .  Diﬀerentiating with respect to ap gives  ∂χ2 ∂ap  = −Rikδkp N = −Rip N  −1 ij  yj − Rjlal  +  yi − Rikak  N  −1 ij −Rjlδlp   −1 ij yj − Rjlal  −  yi − Rikak  N  −1 ij Rjp,   31.96  where δij is the Kronecker delta symbol discussed in section 26.1. By swapping the indices i and j in the second term on the RHS of  31.96  and using the fact that the matrix N−1 is symmetric, we obtain  ∂χ2 ∂ap  = −2Rip N = −2 RT pi N  −1 ij  yj − Rjkak   −1 ij  yj − Rjkak .  If we denote the vector with components ∂χ2 ∂ap, p = 1, 2, . . . , M, by ∇χ2 and write the RHS of  31.97  in matrix notation, we recover the result  31.95 .  cid:2    31.97   1273   STATISTICS  Setting the expression  31.95  equal to zero at a = ˆa, we ﬁnd  −2RTN−1y + 2RTN−1R ˆa = 0.  Provided the matrix RTN−1R is not singular, we may solve this equation for ˆa to obtain  ˆa =  RTN−1R   −1RTN−1y ≡ Sy,   31.98   thus deﬁning the M×N matrix S. It follows that the LS estimates ˆai, i = 1, 2, . . . , M,  are linear functions of the original measurements yj, j = 1, 2, . . . , N. Moreover, using the error propagation formula  30.141  derived in subsection 30.12.3, we ﬁnd that the covariance matrix of the estimators ˆai is given by  V ≡ Cov[ˆai, ˆaj] = SNST =  RTN−1R   −1.   31.99   The two equations  31.98  and  31.99  contain the complete method of least squares. In particular, we note that, if one calculates the LS estimates using  31.98  then one has already obtained their covariance matrix  31.99 .  cid:1 Prove result  31.99 .  Using the deﬁnition of S given in  31.98 , the covariance matrix  31.99  becomes  V = SNST = [ RTN−1R   −1RTN−1]N[ RTN−1R  Using the result  AB··· C T = CT ··· BTAT for the transpose of a product of matrices and noting that, for any non-singular matrix,  A−1 T =  AT   −1RTN−1]T. −1 we ﬁnd −1R[ RTN−1R T]  −1  V =  RTN−1R  =  RTN−1R  =  RTN−1R   −1RTN−1N NT  −1RTN−1R RTN−1R  −1,  −1  where we have also used the fact that N is symmetric and so NT = N.  cid:2   It is worth noting that one may also write the elements of the  inverse   covariance matrix as   cid:7    cid:8   −1 ij =   V  1 2  ∂2χ2 ∂ai∂aj  ,  a=ˆa  which is the same as the Fisher matrix  31.36  in cases where the measurement  errors are Gaussian distributed  and so the log-likelihood is ln L = −χ2 2 . This  proves, at least for this case, our earlier statement that the LS estimators are minimum-variance. In fact, since f x; a  is linear in the parameters a, one can write χ2 exactly as   cid:7   M cid:4    cid:8   χ2 a  = χ2 ˆa  +  1 2  i,j=1  ∂2χ2 ∂ai∂aj  a=ˆa   ai − ˆai  aj − ˆaj ,  which is quadratic in the parameters ai. Hence the form of the likelihood function  1274   31.6 THE METHOD OF LEAST SQUARES  y  7  6  5  4  3  2  1  0  0  1  2  3  4  5  x  Figure 31.9 A set of data points with error bars indicating the uncertainty σ = 0.5 on the y-values. The straight line is y = ˆmx + ˆc, where ˆm and ˆc are the least-squares estimates of the slope and intercept.  L ∝ exp −χ2 2  is Gaussian. From the discussions of subsections 31.3.6 and 31.5.6, it follows that the ‘surfaces’ χ2 a  = c, where c is a constant, bound ellipsoidal conﬁdence regions for the parameters ai. The relationship between the value of the constant c and the conﬁdence level is given by  31.39 .  cid:1 An experiment produces the following data sample pairs  xi, yi :  xi: yi:  1.85 2.26  2.72 3.10  2.81 3.80  3.06 4.11  3.42 4.74  3.76 4.31  4.31 5.24  4.47 4.03  4.64 5.69  4.99 6.57  where the xi-values are known exactly but each yi-value is measured only to an accuracy of σ = 0.5. Assuming the underlying model for the data to be a straight line y = mx + c, ﬁnd the LS estimates of the slope m and intercept c and quote the standard error on each estimate.  The data are plotted in ﬁgure 31.9, together with error bars indicating the uncertainty in the yi-values. Our model of the data is a straight line, and so we have  In the language of  31.92 , our basis functions are h1 x  = 1 and h2 x  = x and our model parameters are a1 = c and a2 = m. From  31.93  the elements of the response matrix are Rij = hj  xi , so that  where xi are the data values and N = 10 in our case. Further, since the standard deviation  on each measurement error is σ, we have N = σ2I, where I is the N × N identity matrix. Because of this simple form for N, the expression  31.98  for the LS estimates reduces to  Note that we cannot expand the inverse in the last line, since R itself is not square and  ˆa = σ2 RTR   RTy =  RTR   −1RTy.  −1 1 σ2  1275   31.100    31.101   f x; c, m  = c + mx.   1  1 ... 1   ,  x1 x2 ... xN  R =   hence does not possess an inverse. Inserting the form for R in  31.100  into the expression  31.101 , we ﬁnd  STATISTICS   cid:7    cid:8   ˆc ˆm   cid:7   cid:11   cid:11   i 1 i xi 1  N x2 − ¯x2    cid:8    cid:11   cid:11   cid:7    cid:7   cid:11   cid:8 −1 i yi cid:11   cid:8  cid:7  i xi i x2 i x2 −¯x −¯x  1  i xiyi N¯y Nxy   cid:8   .  =  =  We thus obtain the LS estimates  xy − ¯x ¯y x2 − ¯x2  ˆm =  and  ˆc =  = ¯y − ˆm¯x,   31.102   where the last expression for ˆc shows that the best-ﬁt line passes through the ‘centre of mass’  ¯x, ¯y  of the data sample. To ﬁnd the standard errors on our results, we must calculate the covariance matrix of the estimators. This is given by  31.99 , which in our case reduces to  x2¯y − ¯x xy x2 − ¯x2  cid:7    cid:8   .  x2 −¯x −¯x  1   31.103   V = σ2 RTR   −1 =  σ2  N x2 − ¯x2   √  √  The standard error on each estimator is simply the positive square root of the corresponding V22, and the covariance of the estimators ˆm diagonal element, i.e. σˆc = and ˆc is given by Cov[ˆc, ˆm] = V12 = V21. Inserting the data sample averages and moments into  31.102  and  31.103 , we ﬁnd  V11 and σ ˆm =  c = ˆc ± σˆc = 0.40 ± 0.62  and  m = ˆm ± σ ˆm = 1.11 ± 0.17.  The ‘best-ﬁt’ straight line y = ˆmx + ˆc is plotted in ﬁgure 31.9. For comparison, the true values used to create the data were m = 1 and c = 1.  cid:2   The extension of the method to ﬁtting data to a higher-order polynomial, such as f x; a  = a1 + a2x + a3x2, is obvious. However, as the order of the polynomial increases the matrix inversions become rather complicated. Indeed, even when the matrices are inverted numerically, the inversion is prone to numerical instabilities. A better approach is to replace the basis functions hm x  = xm, m = 1, 2, . . . , M, with a set of polynomials that are ‘orthogonal over the data’, i.e. such that  N cid:4   i=1  hl xi hm xi  = 0  for l  cid:3 = m.  Such a set of polynomial basis functions can always be found by using the Gram– Schmidt orthogonalisation procedure presented in section 17.1. The details of this approach are beyond the scope of our discussion but we note that, in this case, the matrix RTR is diagonal and may be inverted easily.  31.6.2 Non-linear least squares  If the function f x; a  is not linear in the parameters a then, in general, it is not possible to obtain an explicit expression for the LS estimates ˆa. Instead, one must use an iterative  numerical  procedure, which we now outline. In practice,  1276   31.7 HYPOTHESIS TESTING   cid:20  cid:20  cid:20  cid:20   ∂χ2 ∂ai  a=a0   cid:3 = 0.  however, such problems are best solved using one of the many commercially available software packages.  One begins by making a ﬁrst guess a0 for the values of the parameters. At this point in parameter space, the components of the gradient ∇χ2 will not be equal  to zero, in general  unless one makes a very lucky guess! . Thus, for at least some values of i, we have  = 0  for all i.   31.104   Our aim is to ﬁnd a small increment δa in the values of the parameters, such that   cid:20  cid:20  cid:20  cid:20   ∂χ2 ∂ai  a=a0+δa   cid:20  cid:20  cid:20  cid:20   ≈ ∂χ2 ∂ai   cid:20  cid:20  cid:20  cid:20    cid:20  cid:20  cid:20  cid:20   ∂χ2 ∂ai  M cid:4   j=1  If our ﬁrst guess a0 were suﬃciently close to the true  local  minimum of χ2, we could ﬁnd the required increment δa by expanding the LHS of  31.104  as a Taylor series about a = a0, keeping only the zeroth-order and ﬁrst-order terms:  a=a0+δa  a=a0  j=1  ∂2χ2 ∂ai∂aj  δaj.  a=a0   31.105   Setting this expression to zero, we ﬁnd that the increments δaj may be found by solving the set of M linear equations  M cid:4   +   cid:20  cid:20  cid:20  cid:20    cid:20  cid:20  cid:20  cid:20   ∂2χ2 ∂ai∂aj  a=a0  δaj = − ∂χ2  .  ∂ai  a=a0  It most cases, however, our ﬁrst guess a0 will not be suﬃciently close to the true minimum for  31.105  to be an accurate approximation, and consequently  31.104  will not be satisﬁed. In this case, a1 = a0 + δa is  hopefully  an improved guess at the parameter values; the whole process is then repeated until convergence is achieved.  It is worth noting that, when one is estimating several parameters a, the function χ2 a  may be very complicated. In particular, it may possess numerous local extrema. The procedure outlined above will converge to the local extremum ‘nearest’ to the ﬁrst guess a0. Since, in fact, we are interested only in the local minimum that has the absolute lowest value of χ2 a , it is clear that a large part of solving the problem is to make a ‘good’ ﬁrst guess.  31.7 Hypothesis testing  So far we have concentrated on using a data sample to obtain a number or a set of numbers. These numbers may be estimated values for the moments or central moments of the population from which the sample was drawn or, more generally, the values of some parameters a in an assumed model for the data. Sometimes,  1277   STATISTICS  however, one wishes to use the data to give a ‘yes’ or ‘no’ answer to a particular question. For example, one might wish to know whether some assumed model does, in fact, provide a good ﬁt to the data, or whether two parameters have the same value.  31.7.1 Simple and composite hypotheses  In order to use data to answer questions of this sort, the question must be posed precisely. This is done by ﬁrst asserting that some hypothesis is true. The hypothesis under consideration is traditionally called the null hypothesis  and is denoted by H0. In particular, this usually speciﬁes some form P  xH0   for the probability density function from which the data x are drawn. If the hypothesis determines the PDF uniquely, then it is said to be a simple hypothesis. If, however, the hypothesis determines the functional form of the PDF but not the values of certain parameters a on which it depends then it is called a composite hypothesis.  One decides whether to accept or reject the null hypothesis H0 by performing some statistical test, as described below in subsection 31.7.2. In fact, formally one uses a statistical test to decide between the null hypothesis H0 and the alternative hypothesis H1. We deﬁne the latter to be the complement H 0 of the null hypothesis within some restricted hypothesis space known  or assumed  in advance. Hence, rejection of H0 implies acceptance of H1, and vice versa.  As an example, let us consider the case in which a sample x is drawn from a Gaussian distribution with a known variance σ2 but with an unknown mean µ. If one adopts the null hypothesis H0 that µ = 0, which we write as H0 : µ = 0, then the corresponding alternative hypothesis must be H1 : µ  cid:3 = 0. Note that,  in this case, H0 is a simple hypothesis whereas H1 is a composite hypothesis. If, however, one adopted the null hypothesis H0 : µ < 0 then the alternative  hypothesis would be H1 : µ ≥ 0, so that both H0 and H1 would be composite  hypotheses. Very occasionally both H0 and H1 will be simple hypotheses. In our illustration, this would occur, for example, if one knew in advance that the mean µ of the Gaussian distribution were equal to either zero or unity. In this case, if one adopted the null hypothesis H0 : µ = 0 then the alternative hypothesis would be H1 : µ = 1.  31.7.2 Statistical tests  In our discussion of hypothesis testing we will restrict our attention to cases in which the null hypothesis H0 is simple  see above . We begin by constructing a test statistic t x  from the data sample. Although, in general, the test statistic need not be just a  scalar  number, and could be a multi-dimensional  vector  quantity, we will restrict our attention to the former case. Like any statistic, t x  will be a  1278   31.7 HYPOTHESIS TESTING  P  tH0   α  tcrit  P  tH1   β  tcrit  t  t  Figure 31.10 The sampling distributions P  tH0  and P  tH1  of a test statistic t. The shaded areas indicate the  one-tailed  regions for which Pr t > tcritH0  = α and Pr t < tcritH1  = β respectively.  random variable. Moreover, given the simple null hypothesis H0 concerning the PDF from which the sample was drawn, we may determine  in principle  the  sampling distribution P  tH0  of the test statistic. A typical example of such a  sampling distribution is shown in ﬁgure 31.10. One deﬁnes for t a rejection region containing some fraction α of the total probability. For example, the  one-tailed  rejection region could consist of values of t greater than some value tcrit, for which  Pr t > tcritH0  =  P  tH0  dt = α;   31.106   this is indicated by the shaded region in the upper half of ﬁgure 31.10. Equally, a  one-tailed  rejection region could consist of values of t less than some value tcrit. Alternatively, one could deﬁne a  two-tailed  rejection region by two values  t1 and t2 such that Pr t1 < t < t2H0  = α. In all cases, if the observed value of t  lies in the rejection region then H0 is rejected at signiﬁcance level α; otherwise H0 is accepted at this same level.  It is clear that there is a probability α of rejecting the null hypothesis H0 even if it is true. This is called an error of the ﬁrst kind. Conversely, an error of the second kind occurs when the hypothesis H0 is accepted even though it is   cid:21  ∞  tcrit  1279   STATISTICS  false  in which case H1 is true . The probability β  say  that such an error will occur is, in general, diﬃcult to calculate, since the alternative hypothesis H1 is often composite. Nevertheless, in the case where H1 is a simple hypothesis, it is straightforward  in principle  to calculate β. Denoting the corresponding sampling  distribution of t by P  tH1 , the probability β is the integral of P  tH1  over the  complement of the rejection region, called the acceptance region. For example, in the case corresponding to  31.106  this probability is given by  β = Pr t < tcritH1  =   cid:21  −∞ P  tH1  dt.  tcrit  This is illustrated in ﬁgure 31.10. The quantity 1 − β is called the power of the  statistical test to reject the wrong hypothesis.  31.7.3 The Neyman–Pearson test  In the case where H0 and H1 are both simple hypotheses, the Neyman–Pearson lemma  which we shall not prove  allows one to determine the ‘best’ rejection region and test statistic to use.  We consider ﬁrst the choice of rejection region. Even in the general case, in which the test statistic t is a multi-dimensional  vector  quantity, the Neyman– Pearson lemma states that, for a given signiﬁcance level α, the rejection region for H0 giving the highest power for the test is the region of t-space for which  where c is some constant determined by the required signiﬁcance level.  In the case where the test statistic t is a simple scalar quantity, the Neyman– Pearson lemma is also useful in deciding which such statistic is the ‘best’ in the sense of having the maximum power for a given signiﬁcance level α. From  31.107 , we can see that the best statistic is given by the likelihood ratio   31.107    31.108   and that the corresponding rejection region for H0 is given by t < tcrit. In fact, it is clear that any statistic u = f t  will be equally good, provided that f t  is a monotonically increasing function of t. The rejection region is then u < f tcrit . Alternatively, one may use any test statistic v = g t  where g t  is a monotonically decreasing function of t; in this case the rejection region becomes v > g tcrit . To  construct such statistics, however, one must know P  xH0  and P  xH1  explicitly,  and such cases are rare.  P  tH0  P  tH1   > c,  P  xH0  P  xH1   .  t x  =  1280   31.7 HYPOTHESIS TESTING   cid:1 Ten independent sample values xi, i = 1, 2, . . . , 10, are drawn at random from a Gaussian distribution with standard deviation σ = 1. The mean µ of the distribution is known to equal either zero or unity. The sample values are as follows:  2.22  2.56  1.07  0.24  0.18  0.95  2.09  1.81  0.73 −0.79  Test the null hypothesis H0 : µ = 0 at the 10% signiﬁcance level.  The restricted nature of the hypothesis space means that our null and alternative hypotheses are H0 : µ = 0 and H1 : µ = 1 respectively. Since H0 and H1 are both simple hypotheses, the best test statistic is given by the likelihood ratio  31.108 . Thus, denoting the means by µ0 and µ1, we have   cid:18 − 1  cid:18 − 1  cid:18    cid:11   cid:11   µ0 − µ1    cid:19   cid:19  = i xi − µ0 2  cid:11  i xi − µ1 2 i xi − 1  2  2   cid:18 − 1  cid:18 − 1  2   cid:11   cid:11   cid:19   .  exp  exp 2 N µ2  0  − µ2 2 1   t x  =  exp  exp  = exp   cid:19   cid:19   − 2µ0xi + µ2 0  − 2µ1xi + µ2 1   i  i x2 i x2  i  Inserting the values µ0 = 0 and µ1 = 1, yields t = exp −N¯x + 1 sample mean. Since − ln t is a monotonically decreasing function of t, however, we may 2 N , where ¯x is the  equivalently use as our test statistic  v = − 1  N  ln t + 1  2 = ¯x,  where we have divided by the sample size N and added 1 2 for convenience. Thus we may take the sample mean as our test statistic. From  31.13 , we know that the sampling distribution of the sample mean under our null hypothesis H0 is the Gaussian distribution  N µ0, σ2 N , where µ0 = 0, σ2 = 1 and N = 10. Thus ¯x ∼ N 0, 0.1 .  Since ¯x is a monotonically decreasing function of t, our best rejection region for a given  signiﬁcance α is ¯x > ¯xcrit, where ¯xcrit depends on α. Thus, in our case, ¯xcrit is given by   cid:7    cid:8   α = 1 − Φ  ¯xcrit − µ0  σ  = 1 − Φ 10¯xcrit ,  where Φ z  is the cumulative distribution function for the standard Gaussian. For a 10% signiﬁcance level we have α = 0.1 and, from table 30.3 in subsection 30.9.1, we ﬁnd ¯xcrit = 0.128. Thus the rejection region on ¯x is  ¯x > 0.128.  From the sample, we deduce that ¯x = 1.11, and so we can clearly reject the null hypothesis H0 : µ = 0 at the 10% signiﬁcance level. It can, in fact, be rejected at a much higher signiﬁcance level. As revealed on p. 1239, the data was generated using µ = 1.  cid:2   31.7.4 The generalised likelihood-ratio test  If the null hypothesis H0 or the alternative hypothesis H1 is composite  or both  are composite  then the corresponding distributions P  xH0  and P  xH1  are  not uniquely determined, in general, and so we cannot use the Neyman–Pearson lemma to obtain the ‘best’ test statistic t. Nevertheless, in many cases, there still exists a general procedure for constructing a test statistic t which has useful  1281   STATISTICS  properties and which reduces to the Neyman–Pearson statistic  31.108  in the special case where H0 and H1 are both simple hypotheses.  Consider the quite general, and commonly occurring, case in which the  data sample x is drawn from a population P  xa  with a known  or as-  sumed  functional form but depends on the unknown values of some parameters a1, a2, . . . , aM. Moreover, suppose we wish to test the null hypothesis H0 that the parameter values a lie in some subspace S of the full parameter space A. In other words, on the basis of the sample x it is desired to test the null hypothesis H0 :  a1, a2, . . . , aM lies in S  against the alternative hypothesis H1 :  a1, a2, . . . , aM lies in S , where S is A − S.  Since the functional form of the population is known, we may write down the likelihood function L x; a  for the sample. Ordinarily, the likelihood will have a maximum as the parameters a are varied over the entire parameter space A. This is the usual maximum-likelihood estimate of the parameter values, which we denote by ˆa. If, however, the parameter values are allowed to vary only over the subspace S then the likelihood function will be maximised at the point ˆaS , which may or may not coincide with the global maximum ˆa. Now, let us take as our test statistic the generalised likelihood ratio  t x  =  L x; ˆaS   L x; ˆa   ,   31.109   where L x; ˆaS   is the maximum value of the likelihood function in the subspace S and L x; ˆa  is its maximum value in the entire parameter space A. It is clear that t is a function of the sample values only and must lie between 0 and 1. We will concentrate on the special case where H0 is the simple hypothesis H0 : a = a0. The subspace S then consists of only the single point a0. Thus   31.109  becomes  t x  =  L x; a0  L x; ˆa   ,   31.110   and the sampling distribution P  tH0  can be determined  in principle . As in the  previous subsection, the best rejection region for a given signiﬁcance α is simply t < tcrit, where the value tcrit depends on α. Moreover, as before, an equivalent procedure is to use as a test statistic u = f t , where f t  is any monotonically increasing function of t; the corresponding rejection region is then u < f tcrit . Similarly, one may use a test statistic v = g t , where g t  is any monotonically decreasing function of t; the rejection region then becomes v > g tcrit . Finally, we note that if H1 is also a simple hypothesis H1 : a = a1, then  31.110  reduces to the Neyman–Pearson test statistic  31.108 .  1282   31.7 HYPOTHESIS TESTING  which has its global maximum at µ = ¯x. The test statistic t is then given by   cid:1 Ten independent sample values xi, i = 1, 2, . . . , 10, are drawn at random from a Gaussian distribution with standard deviation σ = 1. The sample values are as follows:  2.22  2.56  1.07  0.24  0.18  0.95  2.09  1.81  0.73 −0.79  Test the null hypothesis H0 : µ = 0 at the 10% signiﬁcance level.  We must test the  simple  null hypothesis H0 : µ = 0 against the  composite  alternative  hypothesis H1 : µ  cid:3 = 0. Thus, the subspace S is the single point µ = 0, whereas A is the  entire µ-axis. The likelihood function is  L x; µ  =  t x  =  L x; 0  L x; ¯x   =   cid:19  i xi − µ 2  cid:5 − 1  cid:19  = exp  ,   cid:6   2 N¯x2  .  2  1  exp  exp   2π N 2   cid:18 − 1  cid:11   cid:18 − 1  cid:19   cid:11   cid:18 − 1  cid:11  i xi − ¯x 2 v = −2 ln t = N¯x2.  cid:21  ∞  i x2  2  2  i  P  vH0  dv = α,  exp  vcrit  It is in fact more convenient to consider the test statistic  Since −2 ln t is a monotonically decreasing function of t, the rejection region now becomes  v > vcrit, where   31.111   with mean zero and variance 1 N. Thus, from subsection 30.9.4, v will follow a chi-squared  α being the signiﬁcance level of the test. Thus it only remains to determine the sampling  distribution P  vH0 . Under the null hypothesis H0, we expect ¯x to be Gaussian distributed, distribution of order 1. Substituting the appropriate form for P  vH0  in  31.111  and setting α = 0.1, we ﬁnd by numerical integration  or from table 31.2  that vcrit = N¯x2 crit = 2.71. Since N = 10, the rejection region on ¯x at the 10% signiﬁcance level is thus  ¯x < −0.52  and  ¯x > 0.52.  As noted before, for this sample ¯x = 1.11, and so we may reject the null hypothesis H0 : µ = 0 at the 10% signiﬁcance level.  cid:2   The above example illustrates the general situation that if the maximum- likelihood estimates ˆa of the parameters fall in or near the subspace S then the sample will be considered consistent with H0 and the value of t will be near unity. If ˆa is distant from S then the sample will not be in accord with H0 and ordinarily t will have a small  positive  value.  It is clear that in order to prescribe the rejection region for t, or for a related  statistic u or v, it is necessary to know the sampling distribution P  tH0 . If H0 is simple then one can in principle determine P  tH0 , although this may prove to obtain P  tH0 , even in principle. Nevertheless, a useful approximate form for P  tH0  exists in the large-sample limit. Consider the null hypothesis  diﬃcult in practice. Moreover, if H0 is composite, then it may not be possible  H0 :  a1 = a0  1, a2 = a0  2, . . . , aR = a0  R , where R ≤ M  and the a0  i are ﬁxed numbers.  In fact, we may ﬁx the values of any subset  1283   STATISTICS  containing R of the M parameters.  If H0 is true then it follows from our discussion in subsection 31.5.6  although we shall not prove it  that, when the  sample size N is large, the quantity −2 ln t follows approximately a chi-squared  distribution of order R.  31.7.5 Student’s t-test  Student’s t-test is just a special case of the generalised likelihood ratio test applied to a sample x1, x2, . . . , xN drawn independently from a Gaussian distribution for which both the mean µ and variance σ2 are unknown, and for which one wishes to distinguish between the hypotheses  H0 : µ = µ0,  0 < σ2 < ∞,  and  H1 : µ  cid:3 = µ0,  0 < σ2 < ∞,  where µ0 is a given number. Here, the parameter space A is the half-plane −∞ < µ < ∞, 0 < σ2 < ∞, whereas the subspace S characterised by the null hypothesis H0 is the line µ = µ0, 0 < σ2 < ∞.  The likelihood function for this situation is given by  L x; µ, σ2  =  1   2πσ2 N 2  exp   cid:13   −   cid:11  i xi − µ 2   cid:14   .  2σ2  On the one hand, as shown in subsection 31.5.1, the values of µ and σ2 that maximise L in A are µ = ¯x and σ2 = s2, where ¯x is the sample mean and s2 is the sample variance. On the other hand, to maximise L in the subspace S we set µ = µ0, and the only remaining parameter is σ2; the value of σ2 that maximises L is then easily found to be C  N cid:4   σ2 =  1 N  i=1   xi − µ0 2.  To retain, in due course, the standard notation for Student’s t-test, in this section we will denote the generalised likelihood ratio by λ  rather than t ; it is thus given by  C  cid:11   cid:11  i xi − µ0 2] i xi − ¯x 2]  λ x  =  σ2  L x; µ0, L x; ¯x, s2   =  [ 2π N  [ 2π N   −N 2 exp −N 2  −N 2 exp −N 2   =   cid:14    cid:13  cid:11   cid:11  i xi − ¯x 2 i xi − µ0 2  N 2  .   31.112   Normally, our next step would be to ﬁnd the sampling distribution of λ under the assumption that H0 were true. It is more conventional, however, to work in terms of a related test statistic t, which was ﬁrst devised by William Gossett, who wrote under the pen name of ‘Student’.  1284   The sum of squares in the denominator of  31.112  may be put into the form  31.7 HYPOTHESIS TESTING   cid:11   cid:11  i xi − µ0 2 = N ¯x − µ0 2 +  cid:8 −N 2   cid:7   i xi − ¯x 2.   cid:11   i xi − ¯x 2 and  Thus, on dividing the numerator and denominator in  31.112  by rearranging, the generalised likelihood ratio λ can be written  λ =  t2 N − 1 where we have deﬁned the new variable ¯x − µ0 √ N − 1  1 +  t =  s   .  ,   31.113   Since t2 is a monotonically decreasing function of λ, the corresponding rejection region is t2 > c, where c is a positive constant depending on the required signiﬁcance level α. It is conventional, however, to use t itself as our test statistic, in which case our rejection region becomes two-tailed and is given by  t < −tcrit  and  t > tcrit,   31.114   where tcrit is the positive square root of the constant c.  The deﬁnition  31.113  and the rejection region  31.114  form the basis of  At the outset, it is worth noting that if we write the expression  31.113  for t in terms of the standard estimator ˆσ = then we obtain   cid:24  Student’s t-test. It only remains to determine the sampling distribution P  tH0 . Ns2  N − 1  of the standard deviation ¯x − µ0 √ ˆσ    31.115   t =  N  .  If, in fact, we knew the true value of σ and used it in this expression for t then it is clear from our discussion in section 31.3 that t would follow a Gaussian  distribution with mean 0 and variance 1, i.e. t ∼ N 0, 1 . When σ is not known,  however, we have to use our estimate ˆσ in  31.115 , with the result that t is no longer distributed as the standard Gaussian. As one might expect from the central limit theorem, however, the distribution of t does tend towards the standard Gaussian for large values of N.  As noted earlier, the exact distribution of t, valid for any value of N, was ﬁrst discovered by William Gossett. From  31.35 , if the hypothesis H0 is true then the joint sampling distribution of ¯x and s is given by   cid:13    cid:8    cid:7  − Ns2 2σ2  − N ¯x − µ0 2  2σ2  exp   cid:14   ,  P  ¯x, sH0  = CsN−2 exp   31.116   where C is a normalisation constant. We can use this result to obtain the joint sampling distribution of s and t by demanding that  P  ¯x, sH0  d¯x ds = P  t, sH0  dt ds.  1285   STATISTICS   cid:7    cid:13    cid:13    cid:21  ∞  0  Using  31.113  to substitute for ¯x − µ0 in  31.116 , and noting that d¯x =  √  s   N − 1  dt, we ﬁnd   cid:8  cid:14   P  ¯x, sH0  d¯x ds = AsN−1 exp  − Ns2 2σ2  1 +  t2 N − 1  dt ds,  where A is another normalisation constant. In order to obtain the sampling  distribution of t alone, we must integrate P  t, sH0  with respect to s over its allowed range, from 0 to ∞. Thus, the required distribution of t alone is given by   cid:7    cid:8  cid:14   P  tH0  =  P  t, sH0  ds = A  sN−1 exp  − Ns2 2σ2  1 +  t2 N − 1   cid:21  ∞  0  ds.  31.117   To carry out this integration, we set y = s{1 + [t2  N − 1 ]}1 2, which on substi-  tution into  31.117  yields   cid:7    cid:8 −N 2 cid:21  ∞  P  tH0  = A  1 +  t2 N − 1  yN−1 exp  0   cid:8    cid:7  − Ny2 2σ2  dy.  P  tH0  =  Since the integral over y does not depend on t, it is simply a constant. We thus ﬁnd that that the sampling distribution of the variable t is   cid:6   cid:5   cid:5  2  N − 1   cid:1  ∞ −∞ P  tH0  dt = 1 to determine the normali- The distribution  31.118  is called Student’s t-distribution with N − 1 degrees of  where we have used the condition sation constant  see exercise 31.18 .  1√  N − 1 π   cid:8 −N 2   cid:6  cid:7   t2 N − 1   31.118   1 2 N  1 +  Γ 1  Γ  ,  freedom. A plot of Student’s t-distribution is shown in ﬁgure 31.11 for various values of N. For comparison, we also plot the standard Gaussian distribution, to which the t-distribution tends for large N. As is clear from the ﬁgure, the t-distribution is symmetric about t = 0. In table 31.3 we list some critical points of the cumulative probability function Cn t  of the t-distribution, which is deﬁned by   cid:21   Cn t  =   cid:7 H0  dt  cid:7   ,  t  −∞ P  t  where n = N − 1 is the number of degrees of freedom. Clearly, Cn t  is analogous  to the cumulative probability function Φ z  of the Gaussian distribution, discussed in subsection 30.9.1. For comparison purposes, we also list the critical points of  Φ z , which corresponds to the t-distribution for N = ∞.  1286   31.7 HYPOTHESIS TESTING  N = 10 N = 5  P  tH0   0.5  0.4  0.3  0.2  0.1  N = 3 N = 2  0  −4 −3 −2 −1  0  1  2  3  4  t  Figure 31.11 Student’s t-distribution for various values of N. The broken curve shows the standard Gaussian distribution for comparison.   cid:1 Ten independent sample values xi, i = 1, 2, . . . , 10, are drawn at random from a Gaussian distribution with unknown mean µ and unknown standard deviation σ. The sample values are as follows:  2.22  2.56  1.07  0.24  0.18  0.95  2.09  1.81  0.73 −0.79  Test the null hypothesis H0 : µ = 0 at the 10% signiﬁcance level.  For our null hypothesis, µ0 = 0. Since for this sample ¯x = 1.11, s = 1.01 and N = 10, it follows from  31.113  that  The rejection region for t is given by  31.114  where tcrit is such that  t =  ¯x  √ N − 1  s   = 3.33.  CN−1 tcrit  = 1 − α 2,  and α is the required signiﬁcance of the test. In our case α = 0.1 and N = 10, and from table 31.3 we ﬁnd tcrit = 1.83. Thus our rejection region for H0 at the 10% signiﬁcance level is  t < −1.83  and  t > 1.83.  For our sample t = 3.30 and so we can clearly reject the null hypothesis H0 : µ = 0 at this level.  cid:2   It is worth noting the connection between the t-test and the classical conﬁdence interval on the mean µ. The central conﬁdence interval on µ at the conﬁdence  level 1 − α is the set of values for which ¯x − µ √ N − 1  −tcrit <  s   < tcrit,  1287   STATISTICS  0.950  0.975  0.990  0.995  Cn t  n = 1 2 3 4  5 6 7 8 9  10 11 12 13 14  15 16 17 18 19  20 25 30 40 50  100 200 ∞  0.5  0.00 0.00 0.00 0.00  0.00 0.00 0.00 0.00 0.00  0.00 0.00 0.00 0.00 0.00  0.00 0.00 0.00 0.00 0.00  0.00 0.00 0.00 0.00 0.00  0.00 0.00 0.00  0.6  0.33 0.29 0.28 0.27  0.27 0.27 0.26 0.26 0.26  0.26 0.26 0.26 0.26 0.26  0.26 0.26 0.26 0.26 0.26  0.26 0.26 0.26 0.26 0.26  0.25 0.25 0.25  0.7  0.73 0.62 0.58 0.57  0.56 0.55 0.55 0.55 0.54  0.54 0.54 0.54 0.54 0.54  0.54 0.54 0.53 0.53 0.53  0.53 0.53 0.53 0.53 0.53  0.53 0.53 0.52  0.8  1.38 1.06 0.98 0.94  0.92 0.91 0.90 0.89 0.88  0.88 0.88 0.87 0.87 0.87  0.87 0.87 0.86 0.86 0.86  0.86 0.86 0.85 0.85 0.85  0.85 0.84 0.84  0.9  3.08 1.89 1.64 1.53  1.48 1.44 1.42 1.40 1.38  1.37 1.36 1.36 1.35 1.35  1.34 1.34 1.33 1.33 1.33  1.33 1.32 1.31 1.30 1.30  1.29 1.29 1.28  6.31 2.92 2.35 2.13  2.02 1.94 1.90 1.86 1.83  1.81 1.80 1.78 1.77 1.76  1.75 1.75 1.74 1.73 1.73  1.73 1.71 1.70 1.68 1.68  1.66 1.65 1.65  12.7 4.30 3.18 2.78  2.57 2.45 2.37 2.31 2.26  2.23 2.20 2.18 2.16 2.15  2.13 2.12 2.11 2.10 2.09  2.09 2.06 2.04 2.02 2.01  1.98 1.97 1.96  31.8 6.97 4.54 3.75  3.37 3.14 3.00 2.90 2.82  2.76 2.72 2.68 2.65 2.62  2.60 2.58 2.57 2.55 2.54  2.53 2.49 2.46 2.42 2.40  2.37 2.35 2.33  0.999  318.3 22.3 10.2 7.17  5.89 5.21 4.79 4.50 4.30  4.14 4.03 3.93 3.85 3.79  3.73 3.69 3.65 3.61 3.58  3.55 3.46 3.39 3.31 3.26  3.17 3.13 3.09  63.7 9.93 5.84 4.60  4.03 3.71 3.50 3.36 3.25  3.17 3.11 3.06 3.01 2.98  2.95 2.92 2.90 2.88 2.86  2.85 2.79 2.75 2.70 2.68  2.63 2.60 2.58  Table 31.3 The conﬁdence limits t of the cumulative probability function Cn t  for Student’s t-distribution with n degrees of freedom. For example,  C5 0.92  = 0.8. The row n = ∞ is also the corresponding result for the  standard Gaussian distribution.  where tcrit satisﬁes CN−1 tcrit  = α 2. Thus the required conﬁdence interval is  ¯x − tcrits√ N − 1  < µ < ¯x +  tcrits√ N − 1  .  Hence, in the above example, the 90% classical central conﬁdence interval on µ is  The t-distribution may also be used to compare diﬀerent samples from Gaussian  0.49 < µ < 1.73.  1288   31.7 HYPOTHESIS TESTING  distributions. In particular, let us consider the case where we have two independent samples of sizes N1 and N2, drawn respectively from Gaussian distributions with a common variance σ2 but with possibly diﬀerent means µ1 and µ2. On the basis of the samples, one wishes to distinguish between the hypotheses  H0 : µ1 = µ2,  0 < σ2 < ∞  and  H1 : µ1  cid:3 = µ2,  0 < σ2 < ∞.  In other words, we wish to test the null hypothesis that the samples are drawn from populations having the same mean. Suppose that the measured sample means and standard deviations are ¯x1, ¯x2 and s1, s2 respectively. In an analogous way to that presented above, one may show that the generalised likelihood ratio can be written as   cid:7   λ =  1 +  t2  N1 + N2 − 2  .   cid:8 − N1+N2  2  cid:8   cid:14 1 2  1 2  ,   cid:7   N1N2  N1 + N2  ˆσ =  N1s2 1 + N2s2 N1 + N2 − 2 2  .  In this case, the variable t is given by  ¯w − ω  cid:13  where ¯w = ¯x1 − ¯x2, ω = µ1 − µ2 and  t =  ˆσ   31.119   It is straightforward  albeit with complicated algebra  to show that the variable t  in  31.119  follows Student’s t-distribution with N1 + N2 − 2 degrees of freedom,  and so we may use an appropriate form of Student’s t-test to investigate the null hypothesis H0 : µ1 = µ2  or equivalently H0 : ω = 0 . As above, the t-test can be  used to place a conﬁdence interval on ω = µ1 − µ2.  cid:1 Suppose that two classes of students take the same mathematics examination and the following percentage marks are obtained:  Class 1: Class 2:  66 64  62 90  34 76  55 56  77 81  80 72  55 70  60  69  47  50  Assuming that the two sets of examinations marks are drawn from Gaussian distributions with a common variance, test the hypothesis H0 : µ1 = µ2 at the 5% signiﬁcance level. Use  your result to obtain the 95% classical central conﬁdence interval on ω = µ1 − µ2.  We begin by calculating the mean and standard deviation of each sample. The number of values in each sample is N1 = 11 and N2 = 7 respectively, and we ﬁnd  ¯x1 = 59.5, s1 = 12.8 and ¯x2 = 72.7, s2 = 10.3,  leading to ¯w = ¯x1 − ¯x2 = −13.2 and ˆσ = 12.6. Setting ω = 0 in  31.119 , we thus ﬁnd t = −2.17.  The rejection region for H0 is given by  31.114 , where tcrit satisﬁes   31.120   CN1+N2−2 tcrit  = 1 − α 2,  1289   STATISTICS  where α is the required signiﬁcance level of the test. In our case we set α = 0.05, and from table 31.3 with n = 16 we ﬁnd that tcrit = 2.12. The rejection region is therefore  Since t = −2.17 for our samples, we can reject the null hypothesis H0 : µ1 = µ2, although signiﬁcance level . The 95% central conﬁdence interval on ω = µ1 − µ2 is given by  only by a small margin.  Indeed, it is easily shown that one cannot reject H0 at the 2%  and  t > 2.12.  t < −2.12  cid:8   1 2   cid:7   ¯w − ˆσtcrit  N1 + N2  N1N2  < ω < ¯w + ˆσtcrit   cid:7    cid:8   N1 + N2  N1N2  1 2  ,  where tcrit is given by  31.120 . Thus, we ﬁnd  −26.1 < ω < −0.28, which, as expected, does not  quite  contain ω = 0.  cid:2   In order to apply Student’s t-test in the above example, we had to make the assumption that the samples were drawn from Gaussian distributions possessing a common variance, which is clearly unjustiﬁed a priori. We can, however, perform another test on the data to investigate whether the additional hypothesis σ2 1 = σ2 2 is reasonable; this test is discussed in the next subsection. If this additional test shows that the hypothesis σ2 2 may be accepted  at some suitable signiﬁcance level , then we may indeed use the analysis in the above example to infer that the null hypothesis H0 : µ1 = µ2 may be rejected at the 5% signiﬁcance level. If, however, we ﬁnd that the additional hypothesis σ2 2 must be rejected, then we can only infer from the above example that the hypothesis that the two samples were drawn from the same Gaussian distribution may be rejected at the 5% signiﬁcance level.  1 = σ2  1 = σ2  Throughout the above discussion, we have assumed that samples are drawn from a Gaussian distribution. Although this is true for many random variables, in practice it is usually impossible to know a priori whether this is case. It can be shown, however, that Student’s t-test remains reasonably accurate even if the sampled distribution s  diﬀer considerably from a Gaussian. Indeed, for sampled distributions that diﬀer only slightly from a Gaussian form, the accuracy of the test is remarkably good. Nevertheless, when applying the t-test, it is always important to remember that the assumption of a Gaussian parent population is central to the method.  31.7.6 Fisher’s F-test  Having concentrated on tests for the mean µ of a Gaussian distribution, we now consider tests for its standard deviation σ. Before discussing Fisher’s F-test for comparing the standard deviations of two samples, we begin by considering the case when an independent sample x1, x2, . . . , xN is drawn from a Gaussian  1290   31.7 HYPOTHESIS TESTING  λ u   0.10  0.05  λcrit  0  0  a  10  b  20  30  Figure 31.12 The sampling distribution P  uH0  for N = 10; this is a chi- squared distribution for N − 1 degrees of freedom.  u  40  distribution with unknown µ and σ, and we wish to distinguish between the two hypotheses  H0 : σ2 = σ2  0, −∞ < µ < ∞  and  H1 : σ2  cid:3 = σ2  0, −∞ < µ < ∞,  0 is a given number. Here, the parameter space A is the half-plane where σ2 −∞ < µ < ∞, 0 < σ2 < ∞, whereas the subspace S characterised by the null hypothesis H0 is the line σ2 = σ2  The likelihood function for this situation is given by  L x; µ, σ2  =  The maximum of L in A occurs at µ = ¯x and σ2 = s2, whereas the maximum of L in S is at µ = ¯x and σ2 = σ2 0. Thus, the generalised likelihood ratio is given by  0, −∞ < µ < ∞.  cid:13  −  1   2πσ2 N 2  .  exp  2σ2   cid:14    cid:11  i xi − µ 2  cid:10   cid:18 − 1  cid:19  2  u − N   cid:11  i xi − ¯x 2  exp  N 2  .  ,   cid:9   u =  =  Ns2 σ2 0  σ2 0  λ x  =  L x; ¯x, σ2 0  L x; ¯x, s2   =  u N  where we have introduced the variable   31.121   An example of this distribution is plotted in ﬁgure 31.12 for N = 10. From the ﬁgure, we see that the rejection region λ < λcrit corresponds to a two-tailed rejection region on u given by  0 < u < a  and  b < u < ∞,  where a and b are such that λcrit a  = λcrit b , as shown in ﬁgure 31.12. In practice,  1291   STATISTICS  however, it is diﬃcult to determine a and b for a given signiﬁcance level α, so a slightly diﬀerent rejection region, which we now describe, is usually adopted.  The sampling distribution P  uH0  may be found straightforwardly from the sampling distribution of s given in  31.35 . Let us ﬁrst determine P  s2H0  by  demanding that  P  sH0  ds = P  s2H0  d s2 ,  cid:7    cid:8    N−1  2   cid:8   Γ  1   cid:7  − Ns2 2σ2 0  cid:6    cid:5   s2  N−3  2 2  N − 1   cid:6  u N−3  2 exp   cid:6  exp  cid:5 − 1  0 is given by  .  2 u  from which we ﬁnd  P  s2H0  =  P  sH0   2s  =  N 2σ2 0  Thus, the sampling distribution of u = Ns2 σ2  P  uH0  =  2 N−1  2Γ  2  N − 1    cid:5   1 1  .   31.122   We note, in passing, that the distribution of u is precisely that of an  N − 1  th- order chi-squared variable  see subsection 30.9.4 , i.e. u ∼ χ2 N−1. Although it does  not give quite the best test, one then takes the rejection region to be  0 < u < a  and  b < u < ∞,  with a and b chosen such that the two tails have equal areas; the advantage of this choice is that tabulations of the chi-squared distribution make the size of this region relatively easy to estimate. Thus, for a given signiﬁcance level α, we have  a  P  uH0  du = α 2  and  P  uH0  du = α 2.   cid:21  ∞  b   cid:21   0   cid:1 Ten independent sample values xi, i = 1, 2, . . . , 10, are drawn at random from a Gaussian distribution with unknown mean µ and standard deviation σ. The sample values are as follows:  0.73 −0.79 Test the null hypothesis H0 : σ2 = 2 at the 10% signiﬁcance level.  2.22  0.18  2.56  0.95  1.07  0.24  2.09  1.81  For our null hypothesis σ2 0 = 2. Since for this sample s = 1.01 and N = 10, from  31.121  we have u = 5.10. For α = 0.1 we ﬁnd, either numerically or using table 31.2, that a = 3.33 and b = 16.92. Thus, our rejection region is  0 < u < 3.33  and  16.92 < u < ∞.  The value u = 5.10 from our sample does not lie in the rejection region, and so we cannot reject the null hypothesis H0 : σ2 = 2.  cid:2   1292   31.7 HYPOTHESIS TESTING  We now turn to Fisher’s F-test. Let us suppose that two independent samples of sizes N1 and N2 are drawn from Gaussian distributions with means and variances µ1, σ2 2 respectively, and we wish to distinguish between the two hypotheses  1 and µ2, σ2  H0 : σ2  1 = σ2 2  and  H1 : σ2 1   cid:3 = σ2  2.  In this case, the generalised likelihood ratio is found to be   cid:18   cid:19   cid:18   cid:19  F N1 − 1   N2 − 1  1 + F N1 − 1   N2 − 1   N1 2  N1+N2  2 ,  λ =   N1 + N2  N1+N2  2  NN1 2  1 NN2 2  2  where F is given by the variance ratio  1  N1 − 1  2  N2 − 1   N1s2 N2s2  ≡ u2 v2  F =   31.123   and s1 and s2 are the standard deviations of the two samples. On plotting λ as a function of F, it is apparent that the rejection region λ < λcrit corresponds to a two-tailed test on F. Nevertheless, as will shall see below, by deﬁning the fraction  31.123  appropriately, it is customary to make a one-tailed test on F.  The distribution of F may be obtained in a reasonably straightforward manner by making use of the distribution of the sample variance s2 given in  31.122 . Under our null hypothesis H0, the two Gaussian distributions share a common variance, which we denote by σ2. Changing the variable in  31.122  from s2 to u2 we ﬁnd that u2 has the sampling distribution   cid:8    N−1  2   cid:7   N − 1  2σ2   cid:5   1  2  N − 1   1  Γ   cid:13    cid:6   u2  N−3  2 exp  −  N − 1 u2  2σ2  P  u2H0  =   cid:14   .  Since u2 and v2 are independent, their joint distribution is simply the product of their individual distributions and is given by  P  u2H0 P  v2H0  = A u2  N1−3  2 v2  N2−3  2 exp  −  N1 − 1 u2 +  N2 − 1 v2  2σ2   cid:14   ,   cid:13   where the constant A is given by   cid:5   N1 − 1  N1−1  2 N2 − 1  N2−1  2   cid:6  2  N1 − 1    cid:6  . 2  N2 − 1    cid:5   Γ  1  1  2 N1+N2−2  2σ N1+N2−2 Γ  A =   31.124   Now, for ﬁxed v we have u2 = Fv2 and d u2  = v2dF. Thus, the joint sampling  1293   STATISTICS  distribution P  v2, FH0  is obtained by requiring that  P  v2, FH0  d v2  dF = P  u2H0 P  v2H0  d u2  d v2 .   31.125   In order to ﬁnd the distribution of F alone, we now integrate P  v2, FH0  with respect to v2 from 0 to ∞, from which we obtain  cid:8 − N1+N2−2  2  cid:7  P  FH0    cid:8    cid:6  cid:7    cid:5   =  where B  P  FH0  is called the F-distribution  or occasionally the Fisher distribution  with  N1 − 1, N2 − 1  degrees of freedom.  is the beta function deﬁned in the Appendix.  1  1  B  1 +  N1 − 1 N2 − 1  cid:5    N1−1  2  N1 − 1 N2 − 1  2  N2 − 1   F  N1−3  2 2  N1 − 1 , 1  cid:6  2  N2 − 1  2  N1 − 1 , 1  cid:1  ∞ 0 P  v2, FH0  d v2  to obtain result  31.126 .  cid:21  ∞   cid:12    cid:1 Evaluate the integral  From  31.125 , we have  P  FH0  = AF  N1−3  2   v2  N1+N2−4  2 exp  0  − [ N1 − 1 F +  N2 − 1 ]v2  d v2 .  F  ,   31.126    cid:15   Making the substitution x = [ N1 − 1 F +  N2 − 1 ]v2  2σ2 , we obtain   cid:14  N1+N2−2  2  cid:14  N1+N2−2  2  2σ2   cid:21  ∞  cid:5   0  P  FH0  = A  2σ2   N1 − 1 F +  N2 − 1   N1 − 1 F +  N2 − 1   2σ2  = A  F  N1−3  2  F  N1−3  2Γ  x N1+N2−4  2e 2  N1 + N2 − 2   −x dx  cid:6   1  ,   cid:13   cid:13   where in the last line we have used the deﬁnition of the gamma function given in the the gamma function, and the expression for A given in  31.124 , we see that P  FH0  is Appendix. Using the further result  18.165 , which expresses the beta function in terms of indeed given by  31.126 .  cid:2   As it does not matter whether the ratio F given in  31.123  is deﬁned as u2 v2 or as v2 u2, it is conventional to put the larger sample variance on the top, so that F is always greater than or equal to unity. A large value of F indicates that the sample variances u2 and v2 are very diﬀerent whereas a value of F close to unity means that they are very similar. Therefore, for a given signiﬁcance α, it is  1294   31.7 HYPOTHESIS TESTING  ∞ 3.84  Cn1,n2  F  n2 = 1 2 3 4 5 6 7 8 9 10 20 30 40 50 100  n2 = 1 2 3 4 5 6 7 8 9 10 20 30 40 50 100  n1 = 1 161 18.5 10.1 7.71 6.61 5.99 5.59 5.32 5.12 4.96 4.35 4.17 4.08 4.03 3.94  n1 = 9 241 19.4 8.81 6.00 4.77 4.10 3.68 3.39 3.18 3.02 2.39 2.21 2.12 2.07 1.97  ∞ 1.88  2 200 19.0 9.55 6.94 5.79 5.14 4.74 4.46 4.26 4.10 3.49 3.32 3.23 3.18 3.09 3.00 10 242 19.4 8.79 5.96 4.74 4.06 3.64 3.35 3.14 2.98 2.35 2.16 2.08 2.03 1.93 1.83  4 225 19.2 9.12 6.39 5.19 4.53 4.12 3.84 3.63 3.48 2.87 2.69 2.61 2.56 2.46 2.37 30 250 19.5 8.62 5.75 4.50 3.81 3.38 3.08 2.86 2.70 2.04 2.69 1.74 1.69 1.57 1.46  5 230 19.3 9.01 6.26 5.05 4.39 3.97 3.69 3.48 3.33 2.71 2.53 2.45 2.40 2.31 2.21 40 251 19.5 8.59 5.72 4.46 3.77 3.34 3.04 2.83 2.66 1.99 1.79 1.69 1.63 1.52 1.39  6 234 19.3 8.94 6.16 4.95 4.28 3.87 3.58 3.37 3.22 2.60 2.42 2.34 2.29 2.19 2.10 50 252 19.5 8.58 5.70 4.44 3.75 3.32 3.02 2.80 2.64 1.97 1.76 1.66 1.60 1.48 1.35  7 237 19.4 8.89 6.09 4.88 4.21 3.79 3.50 3.29 3.14 2.51 2.33 2.25 2.20 2.10 2.01  253 19.5 8.55 5.66 4.41 3.71 3.27 2.97 2.76 2.59 1.91 1.70 1.59 1.52 1.39 1.24  8 239 19.4 8.85 6.04 4.82 4.15 3.73 3.44 3.23 3.07 2.45 2.27 2.18 2.13 2.03 1.94  254 19.5 8.53 5.63 4.37 3.67 3.23 2.93 2.71 2.54 1.84 1.62 1.51 1.44 1.28 1.00  100 ∞  3 216 19.2 9.28 6.59 5.41 4.76 4.35 4.07 3.86 3.71 3.10 2.92 2.84 2.79 2.70 2.60 20 248 19.4 8.66 5.80 4.56 3.87 3.44 3.15 2.94 2.77 2.12 1.93 1.84 1.78 1.68 1.57   cid:21   1  Table 31.4 Values of F for which the cumulative probability function Cn1,n2  F  of the F-distribution with  n1, n2  degrees of freedom has the value 0.95. For example, for n1 = 10 and n2 = 6, Cn1,n2  4.06  = 0.95.  customary to deﬁne the rejection region on F as F > Fcrit, where  Cn1,n2  Fcrit  =  Fcrit  P  FH0  dF = α,  and n1 = N1 − 1 and n2 = N2 − 1 are the numbers of degrees of freedom.  Table 31.4 lists values of Fcrit corresponding to the 5% signiﬁcance level  i.e. α = 0.05  for various values of n1 and n2.  1295   STATISTICS   cid:1 Suppose that two classes of students take the same mathematics examination and the following percentage marks are obtained:  Class 1: Class 2:  66 64  62 90  34 76  55 56  77 81  80 72  55 70  60  69  47  50  Assuming that the two sets of examinations marks are drawn from Gaussian distributions, test the hypothesis H0 : σ2  2 at the 5% signiﬁcance level.  1 = σ2  The variances of the two samples are s2 are N1 = 11 and N2 = 7. Thus, we have  1 =  12.8 2 and s2  2 =  10.3 2 and the sample sizes  u2 =  N1s2 N1 − 1 1  = 180.2 and v2 =  = 123.8,  N2s2 N2 − 1 2  where we have taken u2 to be the larger value. Thus, F = u2 v2 = 1.46 to two decimal places. Since the ﬁrst sample contains eleven values and the second contains seven values, we take n1 = 10 and n2 = 6. Consulting table 31.4, we see that, at the 5% signiﬁcance level, Fcrit = 4.06. Since our value lies comfortably below this, we conclude that there is no statistical evidence for rejecting the hypothesis that the two samples were drawn from Gaussian distributions with a common variance.  cid:2   It is also common to deﬁne the variable z = 1  2 ln F, the distribution of which can be found straightfowardly from  31.126 . This is a useful change of variable since it can be shown that, for large values of n1 and n2, the variable z is − n −1 distributed approximately as a Gaussian with mean 1 1   and variance 1 2  n  −1 2 + n  −1 1  .  2  n 2  −1  31.7.7 Goodness of ﬁt in least-squares problems  We conclude our discussion of hypothesis testing with an example of a goodness- of-ﬁt test. In section 31.6, we discussed the use of the method of least squares in estimating the best-ﬁt values of a set of parameters a in a given model y = f x; a  for a data set  xi, yi , i = 1, 2, . . . , N. We have not addressed, however, the question of whether the best-ﬁt model y = f x; ˆa  does, in fact, provide a good ﬁt to the data. In other words, we have not considered thus far how to verify that the functional form f of our assumed model is indeed correct. In the language of hypothesis testing, we wish to distinguish between the two hypotheses  H0 : model is correct  and  H1 : model is incorrect.  Given the vague nature of the alternative hypothesis H1, we clearly cannot use the generalised likelihood-ratio test. Nevertheless, it is still possible to test the null hypothesis H0 at a given signiﬁcance level α.  The least-squares estimates of the parameters ˆa1, ˆa2, . . . , ˆaM, as discussed in  section 31.6, are those values that minimise the quantity  χ2 a  =  [yi − f xi; a ] N−1 ij [yj − f xj; a ] =  y − f TN−1 y − f .  N cid:4   i,j=1  1296   31.7 HYPOTHESIS TESTING  In the last equality, we rewrote the expression in matrix notation by deﬁning the column vector f with elements fi = f xi; a . The value χ2 ˆa  at this minimum can be used as a statistic to test the null hypothesis H0, as follows. The N quantities yi − f xi; a  are Gaussian distributed. However, provided the function f xj; a  is  linear in the parameters a, the equations  31.98  that determine the least-squares estimate ˆa constitute a set of M linear constraints on these N quantities. Thus, as discussed in subsection 30.15.2, the sampling distribution of the quantity χ2 ˆa  will be a chi-squared distribution with N− M degrees of freedom  d.o.f , which has  the expectation value and variance  E[χ2 ˆa ] = N − M  and  V [χ2 ˆa ] = 2 N − M .  Thus we would expect the value of χ2 ˆa  to lie typically in the range  N − M  ± √ 2 N − M . A value lying outside this range may suggest that the assumed model for the data is incorrect. A very small value of χ2 ˆa  is usually an indication that the model has too many free parameters and has ‘over-ﬁtted’ the data. More commonly, the assumed model is simply incorrect, and this usually results in a value of χ2 ˆa  that is larger than expected.  One can choose to perform either a one-tailed or a two-tailed test on the value of χ2 ˆa . It is usual, for a given signiﬁcance level α, to deﬁne the one-tailed rejection region to be χ2 ˆa  > k, where the constant k satisﬁes  P  χ2  n  dχ2  n = α   31.127   n  is the PDF of the chi-squared distribution with n = N − M degrees of  and P  χ2 freedom  see subsection 30.9.4 .  cid:1 An experiment produces the following data sample pairs  xi, yi :  xi: yi:  1.85 2.26  2.72 3.10  2.81 3.80  3.06 4.11  3.42 4.74  3.76 4.31  4.31 5.24  4.47 4.03  4.64 5.69  4.99 6.57  where the xi-values are known exactly but each yi-value is measured only to an accuracy of σ = 0.5. At the one-tailed 5% signiﬁcance level, test the null hypothesis H0 that the underlying model for the data is a straight line y = mx + c.  These data are the same as those investigated in section 31.6 and plotted in ﬁgure 31.9. As shown previously, the least squares estimates of the slope m and intercept c are given by  ˆm = 1.11  and  ˆc = 0.4.   31.128   Since the error on each yi-value is drawn independently from a Gaussian distribution with standard deviation σ, we have   cid:13   N cid:4   i=1   cid:14   2  N cid:4    cid:22   =  i=1   cid:23 2  .  yi − mxi − c  σ  yi − f xi; a   σ  χ2 a  =   31.129   Inserting the values  31.128  into  31.129 , we obtain χ2  ˆm, ˆc  = 11.5. In our case, the number of data points is N = 10 and the number of ﬁtted parameters is M = 2. Thus, the   cid:21  ∞  k  1297   STATISTICS  χ2  ˆm, ˆc  > 15.51.  number of degrees of freedom is n = N − M = 8. Setting n = 8 and α = 0.05 in  31.127   we ﬁnd from table 31.2 that k = 15.51. Hence our rejection region is  Since above we found χ2  ˆm, ˆc  = 11.5, we cannot reject the null hypothesis that the underlying model for the data is a straight line y = mx + c.  cid:2   As mentioned above, our analysis is only valid if the function f x; a  is linear in the parameters a. Nevertheless, it is so convenient that it is sometimes applied in non-linear cases, provided the non-linearity is not too severe.  31.8 Exercises  31.1  31.2  31.3  A group of students uses a pendulum experiment to measure g, the acceleration −2 : 9.80, 9.84, 9.72, 9.74, of free fall, and obtains the following values  in m s 9.87, 9.77, 9.28, 9.86, 9.81, 9.79, 9.82. What would you give as the best value and standard error for g as measured by the group? Measurements of a certain quantity gave the following values: 296, 316, 307, 278, 312, 317, 314, 307, 313, 306, 320, 309. Within what limits would you say there is a 50% chance that the correct value lies? The following are the values obtained by a class of 14 students when measuring a physical quantity x: 53.8, 53.1, 56.9, 54.7, 58.2, 54.1, 56.4, 54.8, 57.3, 51.0, 55.1, 55.0, 54.2, 56.6.   a  Display these results as a histogram and state what you would give as the   b  Without calculation, estimate how much reliance could be placed upon your  best value for x.  answer to  a .   c  Databooks give the value of x as 53.6 with negligible error. Are the data  obtained by the students in conﬂict with this?  31.4  Two physical quantities x and y are connected by the equation  and measured pairs of values for x and y are as follows:  y1 2 =  x  ax1 2 + b  ,  x: y:  10 409  12 196  16 114  20 94  Determine the best values for a and b by graphical means, and  either by hand or by using a built-in calculator routine  by a least-squares ﬁt to an appropriate straight line. Measured quantities x and y are known to be connected by the formula  31.5  where a and b are constants. Pairs of values obtained experimentally are  x: y:  2.0 0.32  3.0 0.29  4.0 0.25  5.0 0.21  6.0 0.18  Use these data to make best estimates of the values of y that would be obtained  for  a  x = 7.0, and  b  x = −3.5. As measured by fractional error, which estimate  is likely to be the more accurate?  y =  ax  ,  x2 + b  1298   31.8 EXERCISES  31.6  Prove that the sample mean is the best linear unbiased estimator of the population mean µ as follows.   cid:11    cid:11    cid:11   n  i=1 a2   a  If the real numbers a1, a2, . . . , an satisfy the constraint  n i=1 ai = C, where C  is a given constant, show that   b  Consider the linear estimator ˆµ =  is minimised by ai = C n for all i.  i n i=1 aixi. Impose the conditions  i  that it  is unbiased and  ii  that it is as eﬃcient as possible.  31.7  A population contains individuals of k types in equal proportions. A quantity X has mean µi amongst individuals of type i and variance σ2, which has the same value for all types. In order to estimate the mean of X over the whole population, two schemes are considered; each involves a total sample size of nk. In the ﬁrst the sample is drawn randomly from the whole population, whilst in the second  stratiﬁed sampling  n individuals are randomly selected from each of the k types.  Show that in both cases the estimate has expectation  but that the variance of the ﬁrst scheme exceeds that of the second by an amount  k cid:4   i=1  µ =  1 k  µi,  k cid:4   i=1  1 k2n   µi − µ 2.  31.8  Carry through the following proofs of statements made in subsections 31.5.2 and 31.5.3 about the ML estimators ˆτ and ˆλ.   a  Find the expectation values of the ML estimators ˆτ and ˆλ given, respectively, in  31.71  and  31.75 . Hence verify equations  31.76 , which show that, even though an ML estimator is unbiased, it does not follow that functions of it are also unbiased.   b  Show that E[ˆτ2] =  N+1 τ2 N and hence prove that ˆτ is a minimum-variance  estimator of τ.  31.9  unknown. In the ith experiment, i = 1, 2, . . . , N, the total number of successes is  Each of a series of experiments consists of a large, but unknown, number n    cid:26  1  of trials in each of which the probability of success p is the same, but also xi   cid:26  1 . Determine the log-likelihood function. Using Stirling’s approximation to ln n − x , show that + ln n − x ,  d ln n − x   ≈  1  dn  2 n − x   and hence evaluate ∂ nCx  ∂n.  By ﬁnding the  coupled  equations determining the ML estimators ˆp and ˆn, −1, they must satisfy the simultaneous ‘arithmetic’ and  show that, to order n ‘geometric’ mean constraints  N cid:4   i=1  1 N  ˆnˆp =  xi  and  1 − ˆp N =  N cid:3    cid:9   i=1  1 − xi  ˆn   cid:10   .  1299   STATISTICS  31.10  This exercise is intended to illustrate the dangers of applying formalised estimator techniques to distributions that are not well behaved in a statistical sense.  The following are ﬁve sets of 10 values, all drawn from the same Cauchy  distribution with parameter a.   i    ii   4.81 −1.24 −1.13 −8.32 1.86 −4.75 −2.00  iv  −0.15  0.07  0.72   iii   2.98  1.54  1.30 −0.23 2.62 −0.79 −2.85 0.38 −2.76 −8.82 1.14 −0.66 0.86 −3.86 2.65 −17.44 −2.26 −8.83 −0.21 −0.58 −0.14 3.36 −2.96 −1.30  5.51 3.99  4.57  4.81  0.30   v   0.91  3.05  2.80 −6.46  202.76 0.44  0.36  0.24 −3.33 1.59 −7.76 10 cid:4   s ˆa  =  1 1 + x2  i  ˆa2  = 5.  i=1   ∗   Ignoring the fact that the Cauchy distribution does not have a ﬁnite variance  or even a formal mean , show that ˆa, the ML estimator of a, has to satisfy  31.11  31.12  Using a programmable calculator, spreadsheet or computer, ﬁnd the value of  ˆa that satisﬁes  ∗  for each of the data sets and compare it with the value  a = 1.6 used to generate the data. Form an opinion regarding the variance of the estimator.  1 2 Show further that if it is assumed that  E[ˆa] 2 = E[ˆa2], then E[ˆa] = ν 2  , where ν2 is the second  central  moment of the distribution, which for the Cauchy distribution is inﬁnite! According to a particular theory, two dimensionless quantities X and Y have equal values. Nine measurements of X gave values of 22, 11, 19, 19, 14, 27, 8, 24 and 18, whilst seven measured values of Y were 11, 14, 17, 14, 19, 16 and 14. Assuming that the measurements of both quantities are Gaussian distributed with a common variance, are they consistent with the theory? An alternative theory predicts that Y 2 = π2X; are the data consistent with this proposal? On a certain  testing  steeplechase course there are 12 fences to be jumped, and any horse that falls is not allowed to continue in the race. In a season of racing a total of 500 horses started the course and the following numbers fell at each fence:  Fence: Falls:  1 62  2 75  3 49  4 29  5 33  6 25  7 30  8 17  9 19  10 11  11 15  12 12  Use this data to determine the overall probability of a horse’s falling at a fence, and test the hypothesis that it is the same for all horses and fences as follows.   a  Draw up a table of the expected number of falls at each fence on the basis  of the hypothesis.   b  Consider for each fence i the standardised variable  estimated falls − actual falls  zi =  standard deviation of estimated falls  ,  and use it in an appropriate χ2 test.   c  Show that the data indicates that the odds against all fences being equally testing are about 40 to 1. Identify the fences that are signiﬁcantly easier or harder than the average.  1300   31.8 EXERCISES  31.13  A similar technique to that employed in exercise 31.12 can be used to test correlations between characteristics of sampled data. To illustrate this consider the following problem.  During an investigation into possible links between mathematics and classical music, pupils at a school were asked whether they had preferences  a  between mathematics and english, and  b  between classical and pop music. The results are given below.  31.14  31.15  Mathematics None English  Classical None 23 17 30  13 17 10  Pop 14 36 40  By computing tables of expected numbers, based on the assumption that no correlations exist, and calculating the relevant values of χ2, determine whether there is any evidence for   a  a link between academic and musical tastes, and  b  a claim that pupils either had preferences in both areas or had no preference.  You will need to consider the appropriate value for the number of degrees of freedom to use when applying the χ2 test. Three candidates X, Y and Z were standing for election to a vacant seat on their college’s Student Committee. The members of the electorate  current ﬁrst- year students, consisting of 150 men and 105 women  were each allowed to cross out the name of the candidate they least wished to be elected, the other two candidates then being credited with one vote each. The following data are known.   a  X received 100 votes from men, whilst Y received 65 votes from women.  b  Z received ﬁve more votes from men than X received from women.  c  The total votes cast for X and Y were equal. Analyse this data in such a way that a χ2 test can be used to determine whether voting was other than random  i  amongst men and  ii  amongst women. A particle detector consisting of a shielded scintillator is being tested by placing it near a particle source whose intensity can be controlled by the use of absorbers. It might register counts even in the absence of particles from the source because of the cosmic ray background.  The number of counts n registered in a ﬁxed time interval as a function of the  source strength s is given in as:  source strength s: counts n:  0 6  1 11  2 20  3 42  4 44  5 62  6 61  At any given source strength, the number of counts is expected to be Poisson distributed with mean  n = a + bs,  where a and b are constants. Analyse the data for a ﬁt to this relationship and obtain the best values for a and b together with their standard errors.   a  How well is the cosmic ray background determined?  b  What is the value of the correlation coeﬃcient between a and b? Is this consistent with what would happen if the cosmic ray background were imagined to be negligible?   c  Do the data ﬁt the expected relationship well? Is there any evidence that the  reported data ‘are too good a ﬁt’?  1301   STATISTICS  31.16  The function y x  is known to be a quadratic function of x. The following table gives the measured values and uncorrelated standard errors of y measured at various values of x  in which there is negligible error :  x y x   1  3.5 ± 0.5  2  2.0 ± 0.5  3  3.0 ± 0.5  4  6.5 ± 1.0  5  10.5 ± 1.0  Construct the response matrix R using as basis functions 1, x, x2. Calculate the matrix RTN−1R and show that its inverse, the covariance matrix V, has the form  12 592 −9708  −9708 1580 −1461  8413 −1461  1580  269   .  V =  1  9184  31.17  Use this matrix to ﬁnd the best values, and their uncertainties, for the coeﬃcients of the quadratic form for y x . The following are the values and standard errors of a physical quantity f θ  measured at various values of θ  in which there is negligible error :  θ f θ   0  3.72 ± 0.2  π 6  1.98 ± 0.1 −0.06 ± 0.1 −2.05 ± 0.1  π 3  π 4  θ  f θ  −2.83 ± 0.2  π 2  2π 3  1.15 ± 0.1  3π 4  3.99 ± 0.2  π  9.71 ± 0.4  Theory suggests that f should be of the form a1 + a2 cos θ + a3 cos 2θ. Show that the normal equations for the coeﬃcients ai are  481.3a1 + 158.4a2 − 43.8a3 = 284.7, 158.4a1 + 218.8a2 + 62.1a3 = −31.1, −43.8a1 + 62.1a2 + 131.3a3 = 368.4.  31.18  31.19  31.20   a  If you have matrix inversion routines available on a computer, determine the best values and variances for the coeﬃcients ai and the correlation between the coeﬃcients a1 and a2.   b  If you have only a calculator available, solve for the values using a Gauss–  Seidel iteration and start from the approximate solution a1 = 2, a2 = −2,  a3 = 4.  Prove that the expression given for the Student’s t-distribution in equation  31.118  is correctly normalised. Verify that the F-distribution P  F  given explicitly in equation  31.126  is symme- tric between the two data samples, i.e. that it retains the same form but with N1 and N2 interchanged, if F is replaced by F   is the distribution of F It is claimed that the two following sets of values were obtained  a  by ran- domly drawing from a normal distribution that is N 0, 1  and then  b  randomly assigning each reading to one of two sets A and B:  −1. Symbolically, if P  cid:7  , N2, N1 .  and P  F  = η F, N1, N2 , then P    = η F  = F   F   F   cid:7    cid:7    cid:7    cid:7    cid:7    cid:7   Set A: −0.314 Set B: −0.691  0.610  0.603 −0.551 −0.537 −0.160 −1.635 0.482 −1.757 1.515 −1.642 −1.736  1.423  0.058  1.224  0.719  1.165  Make tests, including t- and F-tests, to establish whether there is any evidence that either claims is, or both claims are, false.  1302   31.9 HINTS AND ANSWERS  31.1  31.3  31.5  31.7  31.9  31.9 Hints and answers  −2 is clearly in error, and should not be used in the calculation; 9.80 ± 0.02 m s Note that the reading of 9.28 m s −2.  a  55.1.  b  Note that two thirds of the readings lie within ±2 of the mean and that 14 readings are being used. This gives a standard error in the mean ≈ 0.6.   c  Student’s t has a value of about 2.5 for 13 d.o.f.  degrees of freedom , and therefore it is likely at the 3% signiﬁcance level that the data are in conﬂict with the accepted value. Plot or calculate a least-squares ﬁt of either x2 versus x y or xy versus y x to obtain a ≈ 1.19 and b ≈ 3.4.  a  0.16;  b  −0.27. Estimate  b  is the more accurate because, using the fact that y −x  = −y x , it is eﬀectively obtained by  interpolation rather than extrapolation. Recall that, because of the equal proportions of each type, the expected numbers of each type in the ﬁrst scheme is n. Show that the variance of the estimator for the second scheme is σ2  kn . When calculating that for the ﬁrst scheme, recall that x2 The log-likelihood function is  i can be written as  µi − µ + µ 2. N cid:4   i + σ2 and note that µ2  i = µ2   cid:31    cid:30   Nn − N cid:4   xi  i=1  ln 1 − p ;  ln L =  ≈ ln  ∂ nCx   ∂n  i=1  N cid:4   cid:9  n − x N cid:4    cid:10   cid:7   ln nCxi + −  n  xi ln p +  i=1  .  x  2n n − x   cid:8   Ignore the second term on the RHS of the above to obtain  ln  i=1  n  n − xi  + N ln 1 − p  = 0.  31.11  31.13  ¯X = 18.0 ± 2.2, ¯Y = 15.0 ± 1.1. ˆσ = 4.92 giving t = 1.21 for 14 d.o.f., and  is signiﬁcant only at the 75% level. Thus there is no signiﬁcant disagreement between the data and the theory. For the second theory, only the mean values can be tested as Y 2 will not be Gaussian distributed. The diﬀerence in the means is ¯Y 2 − π2 ¯X = 47 ± 36 and is only signiﬁcantly diﬀerent from zero at the 82%  level. Again the data is consistent with the proposed theory. Consider how many entries may be chosen freely in the table if all row and column totals are to match the observed values. It should be clear that for an  m × n table the number of degrees of freedom is  m − 1  n − 1 .   a  In order to make the fractions expressing each preference or lack of pref- erence correct, the expected distribution, if there were no correlation, must be  Mathematics None English  Classical None 17.5 24.5 28  10 14 16  Pop 22.5 31.5 36  This gives a χ2 of 12.3 for four d.o.f., making it less than 2% likely, that no correlation exists.   b  The expected distribution, if there were no correlation, is  Academic preference No academic preference  104 56  26 14  Music preference No music preference  1303   STATISTICS  31.15  31.17 31.19  This gives a χ2 of 1.2 for one d.o.f and no evidence for the claim. √  As the distribution at each value of s is Poisson, the best estimate of the measurement error is the square root of the number of counts, i.e. n s . Linear  regression gives a = 4.3 ± 2.1 and b = 10.06 ± 0.94.  a  The cosmic ray background must be present, since n 0   cid:3 = 0 but its value of  b  The correlation coeﬃcient between a and b is −0.63. Yes; if a were reduced  about 4 is uncertain to within a factor 2.  towards zero then b would have to be increased to compensate.  neither too good nor too bad.   c  Yes, χ2 = 4.9 for ﬁve d.o.f., which is almost exactly the ‘expected’ value, a1 = 2.02 ± 0.06, a2 = −2.99 ± 0.09, a3 = 4.90 ± 0.10; r12 = −0.60.  cid:14  Note that dF = dF  N2 − 1 F  cid:7  N1 − 1   cid:7  N1 − 1  N2 − 1 F   cid:13   cid:7 2 and write  N1 − 1  N2 − 1 F   cid:14  cid:13   1 +  1 +   F  as   cid:7    cid:7   .  1304   Index  Where the discussion of a topic runs over two consecutive pages, reference is made only to the ﬁrst of these. For discussions spread over three or more pages the ﬁrst and last page numbers are given; these references are usually to the major treatment of the corresponding topic. Isolated refences to a topic, including those appearing on consecutive pages, are listed individually. Some long topics are split, e.g. ‘Fourier transforms’ and ‘Fourier transforms, examples’. The letter ‘n’ after a page number indicates that the topic is discussed in a footnote on the relevant page.  A, B, one-dimensional irreps, 1090, 1102, 1109 Abelian groups, 1044 absolute convergence of series, 124, 831 absolute derivative, 975–977 acceleration vector, 335 Adams method, 1024 Adams–Moulton–Bashforth, predictor-corrector  scheme, 1035  addition rule for probabilities, 1125, 1130 addition theorem for spherical harmonics  Y m  cid:2   θ, φ , 594  adjoint, see Hermitian conjugate adjoint operators, 559–564 adjustment of parameters, 795 Airy integrals, 890–894  Ai z , 889  algebra of  complex numbers, 85 functions in a vector space, 556 matrices, 251 power series, 134 series, 131 tensors, 938–941 vectors, 213  in a vector space, 242 in component form, 218  Amp`ere’s rule  law , 381, 408 amplitude modulation of radio waves, 444 amplitude-phase diagram, 914 analytic  regular  functions, 826 angle between two vectors, 221 angular frequency, 693n in Fourier series, 419  angular momentum, 933, 949  and irreps, 1093 of particle system, 950–952 of particles, 338 of solid body, 396, 951 vector representation, 238 angular momentum operator  component, 658 total, 659  353  angular velocity, vector representation, 223, 238,  annihilation and creation operators, 667 anti-Hermitian matrices, 271  eigenvalues, 276–278  imaginary nature, 277  eigenvectors, 276–278 orthogonality, 277  anti-Stokes line, 905 anticommutativity of vector or cross product,  algebraic equations, numerical methods for, see numerical methods for algebraic equations  222  alternating group, 1116 alternating series test, 130 ammonia molecule, symmetries of, 1042  antisymmetric functions, 416  and Fourier series, 419 and Fourier transforms, 445  antisymmetric matrices, 270  1305   INDEX  general properties, see anti-Hermitian  antisymmetric tensors, 938, 941 antithetic variates, in Monte Carlo methods,  matrices  1014  aperture function, 437  approximately equal ≈, deﬁnition, 132  arbitrary parameters for ODE, 469 arc length of  plane curves, 73 space curves, 341  arccosech, arccosh, arccoth, arcsech, arcsinh,  arctanh, see hyperbolic functions, inverses  Archimedean upthrust, 396, 410 area element in  Cartesian coordinates, 188 plane polars, 202  area of  circle, 71 ellipse, 71, 207 parallelogram, 223 region, using multiple integrals, 191–193 surfaces, 346  as vector, 393–395, 408  area, maximal enclosure, 779 arg, argument of a complex number, 87 Argand diagram, 84, 825 argument, principle of the, 880 arithmetic series, 117 arithmetico-geometric series, 118 arrays, see matrices associated Laguerre equation, 535, 621–624  as example of Sturm–Liouville equation, 566,  622  natural interval, 567, 622  associated Laguerre polynomials Lm  n  x , 621  as special case of conﬂuent hypergeometric  function, 634  generating function, 623 orthogonality, 622 recurrence relations, 624 Rodrigues’ formula, 622  associated Legendre equation, 535, 587–593, 733,  general solution, 588 as example of Sturm–Liouville equation, 566,  768  590, 591  general solution, 588 natural interval, 567, 590, 591  associated Legendre functions, 587–593  of ﬁrst kind P m   cid:2   x , 588, 733, 768  generating function, 592 normalisation, 590 orthogonality, 590, 591 recurrence relations, 592 Rodrigues’ formula, 588  cid:2   x , 588  of second kind Qm  associative law for  addition  in a vector space of ﬁnite dimensionality, 242 in a vector space of inﬁnite dimensionality, 556 of complex numbers, 86 of matrices, 251 of vectors, 213  convolution, 447, 458 group operations, 1043 linear operators, 249 multiplication  of a matrix by a scalar, 251 of a vector by a scalar, 214 of complex numbers, 88 of matrices, 253  multiplication by a scalar  in a vector space of ﬁnite dimensionality, 242 in a vector space of inﬁnite dimensionality, 556  atomic orbitals, 1115  d-states, 1106, 1108, 1114 p-states, 1106 s-states, 1144  auto-correlation functions, 450 automorphism, 1061 auxiliary equation, 493  repeated roots, 493  average value, see mean value axial vectors, 949  backward diﬀerences, 1019 basis functions  for linear least squares estimation, 1273 in a vector space of inﬁnite dimensionality,  556  of a representation, 1078  change in, 1084, 1087, 1092 basis vectors, 217, 243, 929, 1078  derivatives, 965–968  Christoﬀel symbol Γk  ij , 965  for particular irrep, 1106–1108, 1116 linear dependence and independence, 217 non-orthogonal, 245 orthonormal, 244 required properties, 217  Bayes’ theorem, 1132 Bernoulli equation, 477 Bessel correction to variance estimate, 1248 Bessel equation, 535, 602–607, 614, 615  as example of Sturm–Liouville equation, 566 natural interval, 608  Bessel functions Jν  x , 602–614, 729, 738  as special case of conﬂuent hypergeometric  function, 634  generating function, 613 graph of, 606 integral relationships, 610 integral representation, 613–614 orthogonality, 608–611  1306   recurrence relations, 611–612 second kind Yν  x , 607  graph of, 607  series, 604  ν = 0, 606  ν = ±1 2, 605 spherical j cid:2  x , 615, 741 zeros of, 729, 739  Bessel inequality, 246, 559 best unbiased estimator, 1232 beta function, 638 bias of estimator, 1231 bilinear transformation, general, 110 binary chopping, 990 binomial coeﬃcient nCk, 27–30, 1135–1137  elementary properties, 26 identities, 27 in Leibnitz’ theorem, 49 negative n, 29 non-integral n, 29  binomial distribution Bin n, p , 1168–1171  and Gaussian distribution, 1185 and Poisson distribution, 1174, 1177 mean and variance, 1171 MGF, 1170 recurrence formula, 1169  binomial expansion, 25–30, 140 binormal to space curves, 342 birthdays, diﬀerent, 1134 bivariate distributions, 1196–1207  conditional, 1198 continuous, 1197 correlation, 1200–1207  and independence, 1200 matrix, 1203–1207 positive and negative, 1200 uncorrelated, 1200  covariance, 1200–1207  matrix, 1203  expectation  mean , 1199 independent, 1197, 1200 marginal, 1198 variance, 1200  INDEX  brachistochrone problem, 784 Bragg formula, 237 branch cut, 835 branch points, 835 Bromwich integral, 884 bulk modulus, 980  calculus of residues, see zeros of a function of a  complex variable and contour integration  calculus of variations  constrained variation, 785–787 estimation of ODE eigenvalues, 790 Euler–Lagrange equation, 776 Fermat’s principle, 787 Hamilton’s principle, 788 higher-order derivatives, 782 several dependent variables, 782 several independent variables, 782 soap ﬁlms, 780 variable end-points, 782–785  calculus, elementary, 41–76 cancellation law in a group, 1046 canonical form, for second-order ODE, 516 card drawing, see probability carrier frequency of radio waves, 445 Cartesian coordinates, 217 Cartesian tensors, 930–955  algebra, 938–941 contraction, 939 deﬁnition, 935 ﬁrst-order, 932–935 from scalar, 934  general order, 935–954 integral theorems, 954 isotropic, 944–946 physical applications, 934, 939–941, 950–954 second-order, 935–954, 968 symmetry and antisymmetry, 938 tensor ﬁelds, 954 zero-order, 932–935 from vector, 935  Cartesian tensors, particular  Boltzmann distribution, 171 bonding in molecules, 1103, 1105–1108 Born approximation, 149, 575 Bose–Einstein statistics, 1138 boundary conditions  and characteristics, 700 and Laplace equation, 764, 766 for Green’s functions, 512, 514–516  inhomogeneous, 515 for ODE, 468, 470, 501 for PDE, 681, 685–687 for Sturm–Liouville equations, 564 homogeneous and inhomogeneous, 685, 723,  752, 754  superposition solutions, 718–724 types, 702–705  bra vector  cid:20 ψ, 649  conductivity, 952 inertia, 951 strain, 953 stress, 953 susceptibility, 952  catenary, 781, 787 Cauchy  boundary conditions, 702 distribution, 1152 inequality, 853 integrals, 851–853 product, 131 root test, 129, 831 theorem, 849  875  ∗ central diﬀerences, 1019  in terms of z and z  , 829  1307  Cauchy–Riemann relations, 827–830, 849, 873,   INDEX  central limit theorem, 1194–1196 central moments, see moments, central centre of a group, 1069 centre of mass, 195  of hemisphere, 195 of semicircular lamina, 197  centroid, 195  of plane area, 195 of plane curve, 197 of triangle, 216  CF, see complementary function chain rule for functions of  one real variable, 46 several real variables, 157  change of basis, see similarity transformations change of variables  and coordinate systems, 158–160 in multiple integrals, 199–207  evaluation of Gaussian integral, 202–204 general properties, 206  in RVD, 1150–1157 character tables, 1093  3m or 32 or C3v or S3, 1093, 1097, 1108, 1110,  1117  4mm or C4v or D4, 1102, 1106, 1108, 1113 A4, 1116 D5, 1116 S4 or 432 or O, 1114, 1115 ¯43m or Td, 1115 construction of, 1100–1102 quaternion, 1113  characteristic equation, 280  normal mode form, 319 of recurrence relation, 499  functions  MGFs   characteristics  and boundary curves, 700  characteristic functions, see moment generating  multiple intersections, 700, 705  and the existence of solutions, 699–705 ﬁrst-order equations, 699 second-order equations, 703  and equation type, 703  characters, 1092–1096, 1100–1102  and conjugacy classes, 1092, 1095 counting irreps, 1095 deﬁnition, 1092 of product representation, 1104 orthogonality properties, 1094, 1102 summation rules, 1097  Chebyshev polynomials  of ﬁrst kind Tn x , 596  as special case of hypergeometric function, 631 generating function, 601 graph of, 597 normalisation, 600 orthogonality, 599 recurrence relations, 601 Rodrigues’ formula, 599 of second kind Un x , 597 generating function, 601 graph of, 598 normalisation, 600 orthogonality, 599 Rodrigues’ formula, 599  chi-squared  χ2  distribution, 1192  and goodness of ﬁt, 1297 and likelihood-ratio test, 1283, 1292 and multiple estimators, 1243 percentage points tabulation, 1244 test for correlation, 1301  Cholesky separation, 313 Christoﬀel symbol Γk  ij , 965–968  from metric tensor, 966, 973  circle  area of, 71 equation for, 16  circle of convergence, 831 Clairaut equation, 483 classes and equivalence relations, 1064 closure of a group, 1043 closure property of eigenfunctions of an  Hermitian operator, 563  cofactor of a matrix element, 259 column matrix, 250 column vector, 250 combinations  probability , 1133–1139 common ratio in geometric series, 117 commutation law for group elements, 1044 commutative law for  addition  in a vector space of ﬁnite dimensionality, 242 in a vector space of inﬁnite dimensionality, 556 of complex numbers, 86 of matrices, 251 of vectors, 213  charge  point , Dirac δ-function respresentation,  charged particle in electromagnetic ﬁelds, 370 Chebyshev equation, 535, 595–602  as example of Sturm–Liouville equation, 566,  441  599  general solution, 597 natural interval, 567, 599 polynomial solutions, 552  Chebyshev functions, 595–602  complex scalar or dot product, 222 convolution, 447, 458 inner product, 244 multiplication  of a vector by a scalar, 214 of complex numbers, 88 scalar or dot product, 220  commutator  of two matrices, 309 of two operators, 653, 656  comparison test, 125  1308   INDEX  complement, 1121  probability for, 1125  complementary equation, 490 complementary error function, 640 complementary function  CF , 491  for ODE, 492 partially known, 506 repeated roots of auxiliary equation, 493  completeness of  basis vectors, 243 eigenfunctions of an Hermitian operator, 560,  563  eigenvectors of a normal matrix, 275 spherical harmonics Y m   cid:2   θ, φ , 594  completing the square  complex conjugate  as a means of integration, 66 for quadratic equations, 35 for quadratic forms, 1206 to evaluate Gaussian integral, 436, 749 ∗ z of a matrix, 256–258 of scalar or dot product, 222 properties of, 90  , of complex number, 89–91, 829  complex exponential function, 92, 833 complex Fourier series, 424 complex integrals, 845–849, see also zeros of a  function of a complex variable and contour integration  Airy integrals, 890–894 Cauchy integrals, 851–853 Cauchy’s theorem, 849 deﬁnition, 845 Jordan’s lemma, 864 Morera’s theorem, 851 of z principal value, 864 residue theorem, 858–860 WKB methods, 895–905  −1, 846  complex logarithms, 99, 834  principal value of, 100, 834  complex numbers, 83–114  101  argument of, 87 associativity of addition, 86 multiplication, 88  commutativity of  addition, 86 multiplication, 88  addition and subtraction of, 85 applications to diﬀerentiation and integration,  as rotation in the Argand diagram, 88  notation, 84 polar representation of, 92–95 real part of, 83 trigonometric representation of, 93  complex potentials, 871–876  and ﬂuid ﬂow, 873 equipotentials and ﬁeld lines, 872 for circular and elliptic cylinders, 876 for parallel cylinders, 921 for plates, 877–879, 921 for strip, 921 for wedges, 878 under conformal transformations, 876–879  complex power series, 133 complex powers, 99 complex variables, see functions of a complex  variable and power series in a complex variable and complex integrals  components  of a complex number, 84 of a vector, 217  in a non-orthogonal basis, 234 uniqueness, 243  conditional  constrained  variation, 785–787 conditional convergence, 124 conditional distributions, 1198 conditional probability, see probability,  conditional  cone  surface area of, 74 volume of, 75  conﬁdence interval, 1236 conﬁdence region, 1241 conﬂuence process, 634 conﬂuent hypergeometric equation, 535, 633  as example of Sturm–Liouville equation, 566 general solution, 633  conﬂuent hypergeometric functions, 633  contiguous relations, 635 integral representation, 634 recurrence relations, 635 special cases, 634  conformal transformations  mappings , 839–879  applications, 876–879 examples, 842–844 properties, 839–842 Schwarz–Christoﬀel transformation, 843  congruence, 1065 conic sections, 15 eccentricity, 17 parametric forms, 17 standard forms, 16  complex conjugate of, see complex conjugate components of, 84 de Moivre’s theorem, see de Moivre’s theorem division of, 91, 94 from roots of polynomial equations, 83 imaginary part of, 83 modulus of, 87 multiplication of, 88, 94  conjugacy classes, 1068–1070  element in a class by itself, 1068  conjugate roots of polynomial equations, 99 connectivity of regions, 383 conservative ﬁelds, 387–389  necessary and suﬃcient conditions, 387–389 potential  function , 389  1309   INDEX  control variates, in Monte Carlo methods, 1013 convergence of inﬁnite series, 831  hyperbolic functions  cosine, cos x   consistency, of estimator, 1230 constant coeﬃcients in ODE, 492–503  auxiliary equation, 493  constants of integration, 62, 468 constrained variation, 785–787 constraints, stationary values under, see Lagrange undetermined multiplers  continuity correction for discrete RV, 1186 continuity equation, 404 contour integration, 861–867, 887  inﬁnite integrals, 862–867 inverse Laplace transforms, 884–887 residue theorem, 858–867 sinusoidal functions, 861 summing series, 882  contraction of tensors, 939 contradiction, proof by, 32–34 contravariant  basis vectors, 961 derivative, 965  components of tensor, 956  deﬁnition, 961  absolute, 124, 831 complex power series, 133 conditional, 124 necessary condition, 125 power series, 132  under various manipulations, see power series, manipulation  ratio test, 832 rearrangement of terms, 124 tests for convergence, 125–131  alternating series test, 130 comparison test, 125 grouping terms, 129 integral test, 128 quotient test, 127 ratio comparison test, 127 ratio test  D’Alembert , 126, 132 root test  Cauchy , 129, 831  992–994 convolution  convolution  convolution  Fourier tranforms, see Fourier transforms,  Laplace tranforms, see Laplace transforms,  convolution theorem  Fourier transforms, 448 Laplace transforms, 457  coordinate geometry, 15–18  conic sections, 15 straight line, 15  and matrices, see similarity transformations general, 960–965  relative tensors, 963 tensor transformations, 962 weight, 964  orthogonal, 932  coplanar vectors, 225 Cornu spiral, 914 correlation functions, 449–451  auto-correlation, 450 cross-correlation, 449 energy spectrum, 450 Parseval’s theorem, 451 Wiener–Kinchin theorem, 450  correlation matrix, of sample, 1229 correlation of bivariate distributions, 1200–1207 correlation of sample data, 1229 correlation, chi-squared test, 1301 correspondence principle in quantum mechanics,  1215  cosets and congruence, 1065 cosh, hyperbolic cosine, 102, 833, see also  in terms of exponential functions, 102 Maclaurin series for, 140 orthogonality relations, 417  counting irreps, see characters, counting irreps coupled pendulums, 329, 331 covariance matrix  of linear least squares estimators, 1274 of sample, 1229  covariance of bivariate distributions, 1200–1207 covariance of sample data, 1229 covariant  basis vector, 961 derivative, 965  deﬁnition, 961  derivative, 968  components of tensor, 956  of scalar, 971 semi-colon notation, 969  CPF, see probability functions, cumulative Cram´er–Rao  Fisher’s  inequality, 1232, 1233 Cramer determinant, 299 Cramer’s rule, 299 cross product, see vector product cross-correlation functions, 449 crystal lattice, 148 crystal point groups, 1082 cube roots of unity, 98 cube, rotational symmetries of, 1114 cumulants, 1166 curl of a vector ﬁeld, 353  convergence of numerical iteration schemes,  diﬀerentiation, 968–971  coordinate systems, see Cartesian, curvilinear,  cylindrical polar, plane polar and spherical polar coordinates  coordinate transformations  and integrals, see change of variables  as a determinant, 353 as integral, 398, 400 curl curl, 356 in curvilinear coordinates, 368 in cylindrical polars, 360  1310   INDEX  in spherical polars, 362 Stoke’s theorem, 406–409 tensor form, 974  current-carrying wire, magnetic potential, 729 curvature, 52–55  circle of, 53 of a function, 52 of space curves, 342 radius of, 53  curves, see plane curves and space curves curvilinear coordinates, 364–369  basis vectors, 364 length and volume elements, 365 scale factors, 364 surfaces and curves, 364 tensors, 955–977 vector operators, 367–369  cut plane, 865 cycle notation for permutations, 1057 cyclic groups, 1061, 1098 cyclic relation for partial derivatives, 157 cycloid, 370, 785 cylinders, conducting, 874, 876 cylindrical polar coordinates, 357–361  area element, 360 basis vectors, 358 Laplace equation, 728–731 length element, 360 vector operators, 357–361 volume element, 360  δ-function  Dirac , see Dirac δ-function δij , δj  i , Kronecker delta, tensor, see Kronecker delta, δij , δj  i , tensor  D’Alembert’s ratio test, 126, 832  in convergence of power series, 132  D’Alembert’s solution to wave equation, 694 damped harmonic oscillators, 239  and Parseval’s theorem, 451  data modelling, maximum-likelihood, 1255 de Broglie relation, 436, 709, 768 de Moivre’s theorem, 95, 861  applications, 95–99  ﬁnding the nth roots of unity, 97 solving polynomial equations, 98 trigonometric identities, 95–97  deconvolution, 449 defective matrices, 278, 311 degeneracy  breaking of, 1111–1113 of normal modes, 1110  degenerate  separable  kernel, 807 degenerate eigenvalues, 275, 282 degree  of ODE, 468 of polynomial equation, 2  del ∇, see gradient operator  grad  del squared ∇2  Laplacian , 352, 676  as integral, 400  in curvilinear coordinates, 368 in cylindrical polar coordinates, 360 in polar coordinates, 725 in spherical polar coordinates, 362, 741 tensor form, 973  delta function  Dirac , see Dirac δ-function dependent random variables, 1196–1205 derivative, see also diﬀerentiation  absolute, 975–977 covariant, 968 Fourier transform of, 444 Laplace transform of, 455 normal, 350 of basis vectors, 336 of composite vector expressions, 337 of function of a complex variable, 825 of function of a function, 46 of hyperbolic functions, 106–109 of products, 44–46, 48–50 of quotients, 47 of simple functions, 44 of vectors, 334 ordinary, ﬁrst, second and nth, 42 partial, see partial diﬀerentiation total, 154  derivative method for second series solution of  ODE, 545–548  determinant form  and  cid:4 ijk, 942 for curl, 353  determinants, 259–263  adding rows or columns, 262 and singular matrices, 263 as product of eigenvalues, 287 evaluation  using  cid:4 ijk, 942 using Laplace expansion, 259 identical rows or columns, 262 in terms of cofactors, 259 interchanging two rows or two columns, 262 Jacobian representation, 201, 205, 207 notation, 259 of Hermitian conjugate matrices, 262 of order three, in components, 260 of transpose matrices, 261 product rule, 262 properties, 261–263, 978 relationship with rank, 267 removing factors, 262 secular, 280  diagonal matrices, 268 diagonalisation of matrices, 285–288  normal matrices, 286 properties of eigenvalues, 287 simultaneous, 331  diamond, unit cell, 234 die throwing, see probability diﬀerence method for summation of series, 119 diﬀerence schemes for diﬀerential equations,  1020–1023, 1030–1032  1311   INDEX  diﬀerence, ﬁnite, see ﬁnite diﬀerences diﬀerentiable  function of a complex variable, 825–827 function of a real variable, 42  diﬀerential  deﬁnition, 43 exact and inexact, 155 of vector, 338, 344 total, 154  diﬀerential equations, see ordinary diﬀerential  equations and partial diﬀerential equations  diﬀerential equations, particular  associated Laguerre, 535, 566, 621–624 associated Legendre, 535, 566, 587–593 Bernoulli, 477 Bessel, 535, 566, 602–607, 614 Chebyshev, 535, 566, 595–602 Clairaut, 483 conﬂuent hypergeometric, 535, 566, 633 diﬀusion, 678, 695–698, 716, 723, 1032 Euler, 504 Euler–Lagrange, 776 Helmholtz, 737–741 Hermite, 535, 566, 624–628 hypergeometric, 535, 566, 628–632 Lagrange, 789 Laguerre, 535, 566, 616–621 Laplace, 679, 690, 717, 718, 1031 Legendre, 534, 535, 566, 577–586 Legendre linear, 503–505 Poisson, 679, 744–746 Schr¨odinger, 679, 741, 768, 795 simple harmonic oscillator, 535, 566 Sturm–Liouville, 790 wave, 676, 689, 693–695, 714, 737, 790  diﬀerential operators, see linear diﬀerential  operator  diﬀerentiation, see also derivative  as gradient, 42 as rate of change, 41 chain rule, 46 covariant, 968–971 from ﬁrst principles, 41–44 implicit, 47 logarithmic, 48 notation, 43 of Fourier series, 424 of integrals, 178 of power series, 135 partial, see partial diﬀerentiation product rule, 44–46, 48–50 quotient rule, 47 theorems, 55–57 using complex numbers, 101  diﬀraction, see Fraunhofer diﬀraction diﬀusion equation, 678, 688, 695–698 combination of variables, 696–698 integral transforms, 747 numerical methods, 1032 separation of variables, 716  simple solution, 696 superposition, 723  diﬀusion of solute, 678, 696, 747 dihedral group, 1113, 1116 dimension of irrep, 1088 dimensionality of vector space, 243 dipole matrix elements, 208, 1108, 1115 dipole moments of molecules, 1077 Dirac δ-function, 355, 405, 439–443  and convolution, 447 and Green’s functions, 511, 512 as limit of various distributions, 443 as sum of harmonic waves, 442 deﬁnition, 439 Fourier transform of, 443 impulses, 441 point charges, 441 properties, 439 reality of, 443 relation to Fourier transforms, 442 relation to Heaviside  unit step  function, 441 three-dimensional, 441, 452  Dirac notation, 648 direct product, of groups, 1072  direct sum ⊕, 1086  direction cosines, 221 Dirichlet boundary conditions, 702, 852n  Green’s functions, 754, 756–765 method of images, 758–765  Dirichlet conditions, for Fourier series, 415 disc, moment of inertia, 208 discontinuous functions and Fourier series,  420–422  discrete Fourier transforms, 462 disjoint events, see mutually exclusive events displacement kernel, 809 distance from a  line to a line, 231 line to a plane, 232 point to a line, 229 point to a plane, 230  distributive law for  addition of matrix products, 254 convolution, 447, 458 inner product, 244 linear operators, 249 multiplication  of a matrix by a scalar, 251 of a vector by a complex scalar, 222 of a vector by a scalar, 214  multiplication by a scalar  in a vector space of ﬁnite dimensionality, 242 in a vector space of inﬁnite dimensionality, 556  scalar or dot product, 220 vector or cross product, 222  div, divergence of vector ﬁelds, 352  as integral, 398 in curvilinear coordinates, 367  1312   INDEX  in cylindrical polars, 360 in spherical polars, 362 tensor form, 972 divergence theorem  for tensors, 954 for vectors, 401 in two dimensions, 384 physical applications, 404 related theorems, 403  division axiom in a group, 1046 division of complex numbers, 91 dominant term, in Stokes phenomenon, 904 dot product, see scalar product double integrals, see multiple integrals drumskin, see membrane dual tensors, 949 dummy variable, 61   cid:4 ijk, Levi-Civita symbol, tensor, 941–946  and determinant, 942 identities, 943 isotropic, 945 vector products, 942 weight, 964  ex, see exponential function E, two-dimensional irrep, 1090, 1102, 1108 eccentricity, of conic sections, 17 eﬃciency, of estimator, 1231 eigenequation for diﬀerential operators, 554  more general form, 555, 571–573  eigenfrequencies, 319  estimation using Rayleigh–Ritz method,  327–329  eigenfunctions  563  operator, 563  completeness for Hermitian operators, 560,  construction of a real set for an Hermitian  deﬁnition, 555 normalisation for Hermitian operators, 562 of integral equations, 817 of simple harmonic oscillators, 555 orthogonality for Hermitian operators,  561–563  eigenvalues, 272–282, see Hermitian operators  characteristic equation, 280 continuous and discrete, 650 deﬁnition, 272 degenerate, 282 determination, 280–282 estimation for ODE, 790 estimation using Rayleigh–Ritz method,  327–329  notation, 273 of anti-Hermitian matrices, see  anti-Hermitian matrices  of Fredholm equations, 808 of general square matrices, 278 of Hermitian matrices, see Hermitian matrices of integral equations, 808, 816  of linear diﬀerential operators  adjustment of parameters, 795 deﬁnition, 555 error in estimate of, 793 estimation, 790–796 higher eigenvalues, 793, 800 simple harmonic oscillator, 555  of linear operators, 272 of normal matrices, 273–276 of representative matrices, 1100 of unitary matrices, 278 under similarity transformation, 287  eigenvectors, 272–282  characteristic equation, 280 deﬁnition, 272 determination, 280–282 normalisation condition, 273 notation, 273 of anti-Hermitian matrices, see  anti-Hermitian matrices  of commuting matrices, 278 of general square matrices, 278 of Hermitian matrices, see Hermitian matrices of linear operators, 272 of normal matrices, 273–276 of unitary matrices, 278 stationary properties for quadratic and  Hermitian forms, 290  Einstein relation, 436, 709, 768 elastic deformations, 953 electromagnetic ﬁelds  ﬂux, 395 Maxwell’s equations, 373, 408, 979  electrostatic ﬁelds and potentials  charged split sphere, 735 conducting cylinder in uniform ﬁeld, 876 conducting sphere in uniform ﬁeld, 734 from charge density, 745, 758 from complex potential, 873 inﬁnite charged plate, 759, 877 inﬁnite wedge with line charge, 878 ininite charged wedge, 877 of line charges, 761, 872 semi-inﬁnite charged plate, 877 sphere with point charge, 764  ellipse  area of, 71, 207, 385 as section of quadratic surface, 292 equation for, 16  ellipsoid, volume of, 207 elliptic PDE, 687, 690  empty event ∅, 1121  end-points for variations  contributions from, 782 ﬁxed, 777 variable, 782–785  energy levels of  particle in a box, 768 simple harmonic oscillator, 642  1313   INDEX  energy spectrum and Fourier transforms, 450,  451  entire functions, 832n envelopes, 173–175 equations of, 174 to a family of curves, 173  epimorphism, 1061 equilateral triangle, symmetries of, 1047, 1052,  1081, 1110  equivalence relations, 1064–1066, 1068  and classes, 1064 congruence, 1065–1067 examples, 1070  equivalence transformations, see similarity  transformations  equivalent representations, 1084–1086, 1099 error function, erf x , 640, 697, 748  as special case of conﬂuent hypergeometric  function, 634  error terms  in Fourier series, 430 in Taylor series, 139  errors, ﬁrst and second kind, 1280 essential singularity, 838, 856 estimation of eigenvalues  linear diﬀerential operator, 792–795 Rayleigh–Ritz method, 327–329  estimators  statistics , 1229  best unbiased, 1232 bias, 1231 central conﬁdence interval, 1237 conﬁdence interval, 1236 conﬁdence limits, 1236 conﬁdence region, 1241 consistency, 1230 eﬃciency, 1231 maximum-likelihood, 1256 minimum-variance, 1232 standard error, 1234  Euler equation  diﬀerential, 504, 522 trigonometric, 93  Euler method, numerical, 1021 Euler–Lagrange equation, 776  special cases, 777–781  even functions, see symmetric functions events, 1120  complement of, 1121  empty ∅, 1121 intersection of ∩, 1120  mutually exclusive, 1129 statistically independent, 1129  union of ∪, 1121  exact diﬀerentials, 155 exact equations, 472, 505  condition for, 472 non-linear, 519  from Poisson, 1190 MGF, 1191  exponential function  Maclaurin series for, 140 of a complex variable, 92, 833 relation with hyperbolic functions, 102  F-distribution  Fisher , 1290–1296  critical points table, 1295 logarithmic form, 1296  Fabry–P´erot interferometer, 146 factorial function, general, 636 factorisation, of a polynomial equation, 7 faithful representation, 1083, 1098 Fermat’s principle, 787, 798 Fermi–Dirac statistics, 1138 Fibonacci series, 525 ﬁeld lines and complex potentials, 872 ﬁelds  conservative, 387–389 scalar, 347 tensor, 954 vector, 347  ﬁelds, electrostatic, see electrostatic ﬁelds and  potentials  potentials  ﬁelds, gravitational, see gravitational ﬁelds and  ﬁnite diﬀerences, 1019  central, 1019 for diﬀerential equations, 1020–1023 forward and backward, 1019 from Taylor series, 1019, 1026 schemes for diﬀerential equations, 1030–1032  ﬁnite groups, 1043 ﬁrst law of thermodynamics, 176 ﬁrst-order diﬀerential equations, see ordinary  diﬀerential equations  Fisher distribution, see F-distribution  Fisher  Fisher matrix, 1241, 1268 Fisher’s inequality, 1232, 1233 ﬂuids  Archimedean upthrust, 396, 410 complex velocity potential, 873 continuity equation, 404 cylinder in uniform ﬂow, 874 ﬂow, 873 ﬂux, 395, 875 irrotational ﬂow, 353 sources and sinks, 404, 873 stagnation points, 873 velocity potential, 409, 679 vortex ﬂow, 408, 874  forward diﬀerences, 1019 Fourier cosine transforms, 446 Fourier series, 415–432  expectation values, see probability distributions,  mean  exponential distribution, 1190  and separation of variables, 719–722, 724 coeﬃcients, 417–419, 425 complex, 424 diﬀerentiation, 424 Dirichlet conditions, 415  1314   INDEX  discontinuous functions, 420–422 error term, 430 integration, 424 non-periodic functions, 422–424 orthogonality of terms, 417  complex case, 425  Parseval’s theorem, 426 standard form, 417 summation of series, 427 symmetry considerations, 419 uses, 415  Fourier series, examples  square-wave, 418 x, 424, 425 x2, 422 x3, 424  Fourier sine transforms, 445 Fourier transforms, 433–453  as generalisation of Fourier series, 433–435 convolution, 446–449  and the Dirac δ-function, 447 associativity, commutativity, distributivity, 447 deﬁnition, 447 resolution function, 446 convolution theorem, 448 correlation functions, 449–451 cosine transforms, 446 deconvolution, 449 deﬁnition, 435 discrete, 462 evaluation using convolution theorem, 448 for integral equations, 809–812 for PDE, 749–751 Fourier-related  conjugate  variables, 436 in higher dimensions, 451–453 inverse, deﬁnition, 435 odd and even functions, 445 Parseval’s theorem, 450 properties: diﬀerentiation, exponential  multiplication, integration, scaling, translation, 444  relation to Dirac δ-function, 442 sine transforms, 445  Fourier transforms, examples  convolution, 448 damped harmonic oscillator, 451 Dirac δ-function, 443 exponential decay function, 435 Gaussian  normal  distribution, 435 rectangular distribution, 442 spherically symmetric functions, 452 two narrow slits, 448 two wide slits, 438, 448  Fourier’s inversion theorem, 435 Fraunhofer diﬀraction, 437–439  diﬀraction grating, 461 two narrow slits, 448 two wide slits, 438, 448  Fredholm integral equations, 805  eigenvalues, 808 operator form, 806 with separable kernel, 807  Fredholm theory, 815 Frenet–Serret formulae, 343 Fresnel integrals, 913 Frobenius series, 539 Fuch’s theorem, 539 function of a matrix, 255 functional, 776 functions of a complex variable, 825–839,  853–858  analyticity, 826 behaviour at inﬁnity, 839 branch points, 835 Cauchy integrals, 851–853 Cauchy–Riemann relations, 827–830 conformal transformations, 839–844 derivative, 825 diﬀerentiation, 825–830 identity theorem, 854 Laplace equation, 829, 871 Laurent expansion, 855–858 multivalued and branch cuts, 835–837, 885 particular functions, 832–835 poles, 837 power series, 830–832 real and imaginary parts, 825, 830 singularities, 826, 837–839 Taylor expansion, 853–855 zeros, 839, 879–882  functions of one real variable  decomposition into even and odd functions,  416  diﬀerentiation of, 41–50 Fourier series, see Fourier series integration of, 59–72 limits, see limits maxima and minima of, 50–52 stationary values of, 50–52 Taylor series, see Taylor series functions of several real variables  chain rule, 157 diﬀerentiation of, 151–179 integration of, see multiple integrals,  evaluation  maxima and minima, 162–167 points of inﬂection, 162–167 rates of change, 153–155 saddle points, 162–167 stationary values, 162–167 Taylor series, 160–162  fundamental solution, 757 fundamental theorem of  algebra, 83, 85, 868 calculus, 61 complex numbers, see de Moivre’s theorem  gamma distribution, 1153, 1191 gamma function  1315   as general factorial function, 636 deﬁnition and properties, 636 graph of, 637  Gauss’s theorem, 765 Gauss–Seidel iteration, 996–998 Gaussian  normal  distribution N µ, σ2 ,  1179–1189  and binomial distribution, 1185 and central limit theorem, 1195 and Poisson distribution, 1187 continuity correction, 1186 CPF, 1018, 1181  tabulation, 1182  Fourier transform, 435 integration with inﬁnite limits, 202–204 mean and variance, 1180–1184 MGF, 1185, 1188 multiple, 1188 multivariate, 1209 random number generation, 1018 sigma limits, 1183 standard variable, 1180  Gaussian elimination with interchange, 995 Gaussian integration, 1005–1009 points and weights, 1008, 1010  general tensors  algebra, 938–941 contraction, 939 contravariant, 961 covariant, 961 dual, 949 metric, 957–960 physical applications, 957–960, 976 pseudotensors, 964 tensor densities, 964  generalised likelihood ratio, 1282 generating functions  associated Laguerre polynomials, 623 associated Legendre polynomials, 592 Bessel functions, 613 Chebyshev polynomials, 601 Hermite polynomials, 627 Laguerre polynomials, 620 Legendre polynomials, 584–586  generating functions in probability, 1157–1167, see also moment generating functions and probability generating functions  geodesics, 797, 976, 982 geometric distribution, 1159, 1172 geometric series, 117 Gibbs’ free energy, 178 Gibbs’ phenonmenon, 421 gradient of a function of  one variable, 42 several real variables, 153–155  gradient of scalar, 348–352  tensor form, 972  gradient of vector, 937, 969 gradient operator  grad , 348  as integral, 398  INDEX  in curvilinear coordinates, 367 in cylindrical polars, 360 in spherical polars, 362 tensor form, 972  Gram–Schmidt orthogonalisation of  eigenfunctions of Hermitian operators, 562 eigenvectors of  Hermitian matrices, 277 normal matrices, 275  functions in a Hilbert space, 557  gravitational ﬁelds and potentials  Laplace equation, 679 Newton’s law, 339 Poisson equation, 679, 744 uniform disc, 771 uniform ring, 742  Green’s functions, 568–571, 751–767 and boundary conditions, 512, 514 and Dirac δ-function, 511 and partial diﬀerential operators, 753 and Wronskian, 527 diﬀusion equation, 749 Dirichlet problems, 756–765 for ODE, 185, 511–516 Neumann problems, 765–767 particular integrals from, 514 Poisson’s equation, 755  Green’s theorems  applications, 706, 754, 849 in a plane, 384–387, 407 in three dimensions, 402  ground-state energy  harmonic oscillator, 796 hydrogen atom, 800  group multiplication tables, 1050  order three, 1062 order four, 1050, 1052, 1061 order ﬁve, 1062 order six, 1055, 1061  grouping terms as a test for convergence, 129 groups  Abelian, 1044 associative law, 1043 cancellation law, 1046 centre, 1069 closure, 1043 cyclic, 1061 deﬁnition, 1043–1046 direct product, 1072 division axiom, 1046 elements, 1043 order, 1047  ﬁnite, 1043 identity element, 1043–1046 inverse, 1043, 1046 isomorphic, 1051 mappings between, 1059–1061  homomorphic, 1059–1061 image, 1059 isomorphic, 1059  1316   INDEX  nomenclature, 1102 non-Abelian, 1052–1056 order, 1043, 1081, 1082, 1094, 1097, 1100 permutation law, 1047 subgroups, see subgroups  groups, examples  1 and −1 under multiplication, 1043  alternating, 1116 complex numbers eiθ, 1048 functions, 1055 general linear, 1073 integers under addition, 1043 integers under multiplication  mod N ,  1049–1051  matrices, 1054 permutations, 1056–1058 quaternion, 1073 rotation matrices, 1048 symmetries of a square, 1100 symmetries of an equilateral triangle, 1047   1  ν  Hn x , see Hermite polynomials Hamilton’s principle, 788 Hamiltonian, 796 Hankel functions H Hankel transforms, 459 harmonic oscillators damped, 239, 451 ground-state energy, 796 Schr¨odinger equation, 796 simple, see simple harmonic oscillator   x , 607   x , H   2  ν  heat ﬂow  diﬀusion equation, 678, 696, 723 in bar, 723, 749, 770 in thin sheet, 698  Heaviside function, 441  relation to Dirac δ-function, 441  Heisenberg’s uncertainty principle, 435–437 Helmholtz equation, 737–741  cylindrical polars, 740 plane polars, 738 spherical polars, 740–741  Helmholtz potential, 177 hemisphere, centre of mass and centroid, 195 Hermite equation, 535, 624–628  as example of Sturm–Liouville equation, 566 natural interval, 567  Hermite polynomials Hn x , 625  as special case of conﬂuent hypergeometric  function, 634  generating function, 627 graph of, 625 normalisation, 626 orthogonality, 626 recurrence relations, 628 Rodrigues’ formula, 626  Hermitian conjugate, 256–258  and inner product, 258 product rule, 257  Hermitian forms, 288–292  positive deﬁnite and semi-deﬁnite, 290 stationary properties of eigenvectors, 290  Hermitian kernel, 816 Hermitian matrices, 271 eigenvalues, 276–278  reality, 276  eigenvectors, 276–278 orthogonality, 277  Hermitian operators, 559–564 and physical variables, 650 boundary condition for simple harmonic  oscillators, 560  eigenfunctions  completeness, 560, 563 orthogonality, 561–563  eigenvalues  reality, 561  Green’s functions, 568–571 importance of, 555, 560 in Sturm–Liouville equations, 564 properties, 561–564 superposition methods, 568–571  higher-order diﬀerential equations, see ordinary  diﬀerential equations  Hilbert spaces, 557–559 hit or miss, in Monte Carlo methods, 1014 homogeneous  boundary conditions, see boundary  conditions, homogeneous and inhomogeneous  diﬀerential equations, 490 dimensionally consistent, 475, 521 simultaneous linear equations, 293  homomorphism, 1059–1061  kernel of, 1060 representation as, 1083  Hooke’s law, 953 hydrogen atom s-states, 1144 electron wavefunction, 208 ground-state energy, 800  hydrogen molecule, symmetries of, 1041 hyperbola  as section of quadratic surface, 292 equation for, 16  hyperbolic functions, 102–109, 833  calculus of, 106–109 deﬁnitions, 102, 833 graphs, 102 identities, 104 in equations, 105 inverses, 105  graphs, 106  trigonometric analogies, 102–104  hyperbolic PDE, 687, 690 hypergeometric distribution, 1173  mean and variance, 1173  hypergeometric equation, 535, 628–632  as example of Sturm–Liouville equation, 566,  567  1317   general solution, 630 natural interval, 567  hypergeometric functions, 628–632  contiguous relations, 632 integral representation, 631 recurrence relations, 632 special cases, 630  hypothesis testing, 1277–1298  errors, ﬁrst and second kind, 1280 generalised likelihood ratio, 1282 generalised likelihood-ratio test, 1281 goodness of ﬁt, 1296 Neyman–Pearson test, 1280 null, 1278 power, 1280 rejection region, 1279 simple or composite, 1278 statistical tests, 1278 test statistic, 1278  i, j, k  unit vectors , 219  i, square root of −1, 84  identity element of a group, 1043–1046  uniqueness, 1043, 1045  identity matrices, 254, 255 identity operator, 249 images, method of, see method of images imaginary part or term of a complex number, 83 importance sampling, in Monte Carlo methods,  1012 improper  integrals, 70 rotations, 946–948  impulses, δ-function respresentation, 441 incomplete gamma function, 639 independent random variables, 1156, 1200 index of a subgroup, 1066 indices, of regular singular points, 540 indicial equation, 540  distinct roots with non-integral diﬀerence,  540–542  repeated roots, 542, 546, 547 roots diﬀer by integer, 542, 546  induction, proof by, 31 inequalities  amongst integrals, 72 Bessel, 246, 559 Schwarz, 246, 559 triangle, 246, 559  inertia, see also moments of inertia  moments and products, 951 tensor, 951  inexact diﬀerentials, 155 inexact equation, 473 inﬁnite integrals, 70  contour integration, 862–867  inﬁnite series, see series inﬂection  general points of, 52 stationary points of, 50–52  INDEX  inner product in a vector space, see also scalar  inhomogeneous  boundary conditions, see boundary  conditions, homogeneous and inhomogeneous  diﬀerential equations, 490 simultaneous linear equations, 293  product  of ﬁnite dimensionality, 244  and Hermitian conjugate, 258 commutativity, 244 distributivity over addition, 244  of inﬁnite dimensionality, 557  integral equations  eigenfunctions, 817 eigenvalues, 808, 816 Fredholm, 805 from diﬀerential equations, 803 homogeneous, 805 linear, 804  ﬁrst kind, 805 second kind, 805 nomenclature, 804 singular, 805 Volterra, 805  integral equations, methods for  diﬀerentiation, 812 Fredholm theory, 815 integral transforms, 809–812  Fourier, 809–812 Laplace, 810  Neumann series, 813–815 Schmidt–Hilbert theory, 816–819 separable  degenerate  kernels, 807  integral functions, 832n integral test for convergence of series, 128 integral transforms, see also Fourier transforms  and Laplace transforms  general form, 459 Hankel transforms, 459 Mellin transforms, 459  integrals, see also integration  complex, see complex integrals deﬁnite, 59 double, see multiple integrals Fourier transform of, 444 improper, 70 indeﬁnite, 62 inequalities, 72, 559 inﬁnite, 70 Laplace transform of, 456 limits  containing variables, 188 ﬁxed, 59 variable, 61  line, see line integrals multiple, see multiple integrals non-zero, 1104 of vectors, 339 properties, 60  1318   INDEX  triple, see multiple integrals undeﬁned, 59  integrand, 59 integrating factor  IF , 506 ﬁrst-order ODE, 473–475  integration, see also integrals  applications, 72–76  ﬁnding the length of a curve, 73 mean value of a function, 72 surfaces of revolution, 74 volumes of revolution, 75  as area under a curve, 59 as the inverse of diﬀerentiation, 61 formal deﬁnition, 59 from ﬁrst principles, 59 in plane polar coordinates, 70 logarithmic, 64 multiple, see multiple integrals multivalued functions, 865–867, 885 of Fourier series, 424 of functions of several real variables, see  multiple integrals  of hyperbolic functions, 106–109 of power series, 135 of simple functions, 62 of singular functions, 70 of sinusoidal functions, 63, 861  integration constant, 62 integration, methods for  by inspection, 62 by parts, 67–69 by substitution, 65–67  t substitution, 65  change of variables, see change of variables completing the square, 66 contour, see contour integration Gaussian, 1005–1009 numerical, 1000–1009 partial fractions, 64 reduction formulae, 69 stationary phase, 912–920 steepest descents, 908–912 trigonometric expansions, 63 using complex numbers, 101  intersection ∩, probability for, 1120, 1128  intrinsic derivative, see absolute derivative invariant tensors, see isotropic tensors inverse hyperbolic functions, 105 inverse integral transforms  Fourier, 435 Laplace, 454, 884–887  uniqueness, 454  inverse matrices, 263–266  elements, 264 in solution of simultaneous linear equations,  295  product rule, 266 properties, 265  inverse of a linear operator, 249 inverse of a product in a group, 1046  inverse of element in a group  uniqueness, 1043, 1046  inversion theorem, Fourier’s, 435 inversions as  improper rotations, 946 symmetry operations, 1041 irregular singular points, 534 irreps, 1087  counting, 1095 direct sum ⊕, 1086 dimension nλ, 1097 identity A1, 1100, 1104 n-dimensional, 1088, 1089, 1102 number in a representation, 1087, 1095 one-dimensional, 1088, 1089, 1093, 1099, 1102 orthogonality theorem, 1090–1092 projection operators for, 1107 reduction to, 1096 summation rules for, 1097–1099  irrotational vectors, 353 isobaric ODE, 476  non-linear, 521  isoclines, method of, 1028, 1037 isomorphic groups, 1051–1056, 1058, 1059 isomorphism  mapping , 1060 isotope decay, 484, 525 isotropic  invariant  tensors, 944–946, 953 iteration schemes  convergence of, 992–994 for algebraic equations, 986–994 for diﬀerential equations, 1025 for integral equations, 813–816 Gauss–Seidel, 996–998 order of convergence, 993  j, square root of −1, 84 Jν  x , see Bessel functions j cid:2  x , see spherical Bessel functions Jacobians  analogy with derivatives, 207 and change of variables, 206 deﬁnition in  two dimensions, 201 three dimensions, 205  general properties, 206 in terms of a determinant, 201, 205, 207  joint distributions, see bivariate distributions  and multivariate distributions  Jordan’s lemma, 864  kernel of a homomorphism, 1060, 1063 kernel of an integral transform, 459 kernel of integral equations  displacement, 809 Hermitian, 816  of form exp −ixz , 810–812  of linear integral equations, 804 resolvent, 814, 815 separable  degenerate , 807  ket vector ψ cid:21 , 648  1319   INDEX  kinetic energy of oscillating system, 316 Klein–Gordon equation, 711, 772 Kronecker delta δij and orthogonality, 244 Kronecker delta, δij , δj  i , tensor, 928, 941–946,  956, 962  identities, 943 isotropic, 945 vector products, 942 Kummer function, 633 kurtosis, 1150, 1227  n  x , see associated Laguerre polynomials  Ln x , see Laguerre polynomials Lm L’H ˆopital’s rule, 142–144 Lagrange equations, 789  and energy conservation, 797  Lagrange undetermined multipliers, 167–173  and ODE eigenvalue estimation, 792 application to stationary properties of the eigenvectors of quadratic and Hermitian forms, 290  for functions of more than two variables,  in deriving the Boltzmann distribution,  169–173  171–173  integral constraints, 785 with several constraints, 169–173  Lagrange’s identity, 226 Lagrange’s theorem, 1065  and the order of a subgroup, 1062 and the order of an element, 1062  Lagrangian, 789, 797 Laguerre equation, 535, 616–621  function, 634  generating function, 620 graph of, 617 normalisation, 619 orthogonality, 619 recurrence relations, 620 Rodrigues’ formula, 618  Lam´e constants, 953 lamina: mass, centre of mass and centroid,  193–195  Laplace equation, 679  expansion methods, 741–744 in two dimensions, 688, 690, 717, 718  and analytic functions, 829 and conformal transformations, 876–879 numerical method for, 1031, 1038 plane polars, 725–727 separated variables, 717  in three dimensions  cylindrical polars, 728–731 spherical polars, 731–737 uniqueness of solution, 741  with speciﬁed boundary values, 764, 766  Laplace expansion, 259 Laplace transforms, 453–459, 884  convolution  associativity, commutativity, distributivity, 458 deﬁnition, 457  convolution theorem, 457 deﬁnition, 453 for ODE with constant coeﬃcients, 501–503 for PDE, 747–748 inverse, 454, 884–887  uniqueness, 454  properties: translation, exponential  multiplication, etc., 456  table for common functions, 455  Laplace transforms, examples  constant, 453 derivatives, 455 exponential function, 453 integrals, 456 polynomial, 453  Laplacian, see del squared ∇2  Laplacian   Laurent expansion, 855–858  analytic and principal parts, 855 region of convergence, 855  least squares, method of, 1271–1277  basis functions, 1273 linear, 1272 non-linear, 1276 response matrix, 1273  Legendre equation, 534, 535, 577–586  as example of Sturm–Liouville equation, 566,  associated, see associated Legendre equation general solution, 578, 580 natural interval, 567, 583  Legendre functions P cid:2  x , 577–586  associated Legendre functions, 768 of second kind Q cid:2  x , 579  graph of, 580  Legendre linear equation, 503 Legendre polynomials P cid:2  x , 578  as special case of hypergeometric function,  631  associated Legendre functions, 733 generating function, 584–586 graph of, 579 in Gaussian integration, 1006 normalisation, 578, 582 orthogonality, 583, 735 recurrence relations, 585, 586 Rodrigues’ formula, 581  Leibnitz’ rule for diﬀerentiation of integrals, 178 Leibnitz’ theorem, 48–50 length of  a vector, 218 plane curves, 73, 341 space curves, 341  tensor form, 982  1320  as example of Sturm–Liouville equation, 566,  583  619  natural interval, 567, 619  Laguerre polynomials Ln x , 617  as special case of conﬂuent hypergeometric   INDEX  level lines, 905, 906 Levi-Civita symbol, see  cid:4 ijk, Levi-Civita symbol,  particular: identity, null or zero, singular and  line charge, electrostatic potential, 872, 878 line integrals  1174  lower triangular matrices, 269  tensor  likelihood function, 1255 limits, 141–144  deﬁnition, 141 L’H ˆopital’s rule, 142–144 of functions containing exponents, 142 of integrals, 59  containing variables, 188  of products, 141 of quotients, 141–144 of sums, 141  and Cauchy integrals, 851–853 and Stokes’ theorem, 406–409 of scalars, 377–387 of vectors, 377–389 physical examples, 381 round closed loop, 386  line of steepest descents, 908 line, vector equation of, 226 linear dependence and independence  deﬁnition in a vector space, 242 of basis vectors, 217 relationship with rank, 267  linear diﬀerential operator L, 511, 545, 554 adjoint L†  , 559  eigenfunctions, see eigenfunctions eigenvalues, see eigenvalues, of linear  diﬀerential operators  for Sturm-Liouville equation, 564–568 Hermitian, 555, 559–564 self-adjoint, 559  linear equations, diﬀerential  ﬁrst-order ODE, 474 general ODE, 490–517 ODE with constant coeﬃcients, 492–503 ODE with variable coeﬃcients, 503–517  linear equations, simultaneous, see simultaneous  linear equations  linear independence of functions, 491 linear integral operator K, 805  Wronskian test, 491, 532  and Schmidt–Hilbert theory, 816–818 Hermitian conjugate, 805 inverse, 806  linear interpolation for algebraic equations, 988 linear least squares, method of, 1272 linear molecules  normal modes of, 320–322 symmetries of, 1077  linear operators, 247–249  associativity, 249 distributivity over addition, 249 eigenvalues and eigenvectors, 272 in a particular basis, 248 inverse, 249 non-commutativity, 249  non-singular, 249  properties, 249  linear vector spaces, see vector spaces lines of steepest descent, 906 Liouville’s theorem, 853 Ln of a complex number, 99, 834 ln  natural logarithm   Maclaurin series for, 140 of a complex number, 99, 834  log-likelihood function, 1258 longitudinal vibrations in a rod, 677 lottery  UK , and hypergeometric distribution,  Maclaurin series, 138  standard expressions, 140  Madelung constant, 149 magnetic dipole, 220 magnitude of a vector, 218  in terms of scalar or dot product, 221  mappings between groups, see groups, mappings  between  marginal distributions, 1198 mass of non-uniform bodies, 193 matrices, 241–307  as a vector space, 252 as arrays of numbers, 249 as representation of a linear operator, 249 column, 250 elements, 249  minors and cofactors, 259  identity or unit, 254 row, 250 zero or null, 254  matrices, algebra of, 250  addition, 251 change of basis, 283–285 Cholesky separation, 313 diagonalisation, see diagonalisation of  matrices  multiplication, 252–254  and common eigenvalues, 278 commutator, 309 non-commutativity, 254  multiplication by a scalar, 251 normal modes, see normal modes numerical methods, see numerical methods  for simultaneous linear equations  similarity transformations, see similarity  transformations  simultaneous linear equations, see  simultaneous linear equations  subtraction, 251  matrices, derived  adjoint, 256–258 complex conjugate, 256–258 Hermitian conjugate, 256–258 inverse, see inverse matrices  1321   INDEX  transpose, 250  matrices, properties of  anti- or skew-symmetric, 270 anti-Hermitian, see anti-Hermitian matrices determinant, see determinants diagonal, 268 eigenvalues, see eigenvalues eigenvectors, see eigenvectors Hermitian, see Hermitian matrices normal, see normal matrices nullity, 293 order, 249 orthogonal, 270 rank, 267 square, 249 symmetric, 270 trace or spur, 258 triangular, 269 tridiagonal, 998–1000, 1030 unitary, see unitary matrices  matrix elements in quantum mechanics  as integrals, 1103 dipole, 1108, 1115  maxima and minima  local  of a function of  constrained variables, see Lagrange  undetermined multipliers  one real variable, 50–52  suﬃcient conditions, 51  several real variables, 162–167  suﬃcient conditions, 164, 167 maximum modulus theorem, 881 maximum-likelihood, method of, 1255–1271  and Bayesian approach, 1264 bias, 1260 data modelling, 1255 estimator, 1256 extended, 1270 log-likelihood function, 1258 parameter estimation, 1255 transformation invariance, 1260  Maxwell’s  electromagnetic equations, 373, 408, 979 thermodynamic relations, 176–178 Maxwell–Boltzmann statistics, 1138 mean µ  from MGF, 1163 from PGF, 1158 of RVD, 1144 of sample, 1223 of sample: geometric, harmonic, root mean  square, 1223  mean value of a function of  one variable, 72 several variables, 199 mean value theorem, 56 median of RVD, 1145 membrane  deformed rim, 725–727 normal modes, 739, 1112 transverse vibrations, 677, 739, 768, 1112  method of images, 706, 758–765, 878 disc  section of cylinder , 764, 766 inﬁnite plate, 759 intersecting plates in two dimensions, 761 sphere, 762–764, 772  metric tensor, 957–960, 963  and Christoﬀel symbols, 966 and scale factors, 957, 972 covariant derivative of, 982 determinant, 957, 964  derivative of, 973 length element, 957 raising or lowering index, 959, 963 scalar product, 958 volume element, 957, 981  MGF, see moment generating functions Milne’s method, 1022 minimum-variance estimator, 1232 minor of a matrix element, 259 mixed, components of tensor, 957, 962, 969 ML estimators, 1256  bias, 1260 conﬁdence limits, 1262 eﬃciency, 1261 transformation invariance, 1260  mode of RVD, 1145 modulo, mod N, multiplication, 1049 modulus  of a complex number, 87 of a vector, see magnitude of a vector  molecules  bonding in, 1103, 1105–1108 dipole moments of, 1077 symmetries of, 1077  moment generating functions  MGFs ,  1162–1167  and central limit theorem, 1195 and PGF, 1163 mean and variance, 1163 particular distributions  binomial, 1170 exponential, 1191 Gaussian, 1163, 1185 Poisson, 1177 properties, 1163  moments  of distributions   central, 1148 of RVD, 1147  moments  of forces , vector representation of,  223  moments of inertia  and inertia tensor, 951 deﬁnition, 198 of disc, 208 of rectangular lamina, 198 of right circular cylinder, 209 of sphere, 205 perpendicular axes theorem, 209  momentum as ﬁrst-order tensor, 933 monomorphism, 1061  1322   Monte Carlo methods, of integration, 1009–1017  antithetic variates, 1014 control variates, 1013 crude, 1011 hit or miss, 1014 importance sampling, 1012 multiple integrals, 1016 random number generation, 1017 stratiﬁed sampling, 1012  Morera’s theorem, 851 multinomial distribution, 1208  and multiple Poisson distribution, 1218  multiple angles, trigonometric formulae, 10 multiple integrals  application in ﬁnding  area and volume, 191–193 mass, centre of mass and centroid, 193–195 mean value of a function of several variables, 199 moments of inertia, 198  change of variables  double integrals, 200–204 general properties, 206 triple integrals, 204  deﬁnitions of  double integrals, 187 triple integrals, 190  evaluation, 188–190 notation, 188, 189, 191 order of integration, 188, 191  multiplication tables for groups, see group  multiplication tables  multiplication theorem, see Parseval’s theorem multivalued functions, 835–837  integration of, 865–867  multivariate distributions, 1196, 1207–1211  change of variables, 1206 Gaussian, 1209 multinomial, 1208  mutually exclusive events, 1120, 1129  nabla ∇, see gradient operator  grad  n cid:2  x , see spherical Bessel functions  natural interval  for associated Laguerre equation, 567, 622 for associated Legendre equation, 567, 590,  591  for Bessel equation, 608 for Chebyshev equation, 567, 599 for Hermite equation, 567 for Laguerre equation, 567, 619 for Legendre equation, 567, 583 for simple harmonic oscillator equation, 567 for Sturm–Liouville equations, 565, 567  natural logarithm, see ln and Ln natural numbers, in series, 31, 121 natural representations, 1081, 1110 necessary and suﬃcient conditions, 34 negative binomial distribution, 1172 negative function, 556  INDEX  negative vector, 242 Neumann boundary conditions, 702  Green’s functions, 754, 765–767 method of images, 765–767 self-consistency, 765  Neumann functions Yν  x , 607 Neumann series, 813–815 Newton–Raphson  NR  method, 990–992  order of convergence, 993 Neyman–Pearson test, 1280 nodes of oscillation, 693 non-Abelian groups, 1052–1056  of functions, 1055 of matrices, 1054 of permutations, 1056–1058 of rotations and reﬂections, 1052  non-Cartesian coordinates, see curvilinear,  cylindrical polar, plane polar and spherical polar coordinates  non-linear diﬀerential equations, see ordinary  diﬀerential equations, non-linear  non-linear least squares, method of, 1276 norm of  function, 557 vector, 244  normal  to coordinate surface, 366 to plane, 228 to surface, 346, 350, 390  distribution  normal matrices, 272  eigenvectors  completeness, 275 orthogonality, 275  normal derivative, 350 normal distribution, see Gaussian  normal   eigenvectors and eigenvalues, 273–276  normal modes, 316–329  characteristic equation, 319 coupled pendulums, 329, 331 deﬁnition, 320 degeneracy, 1110–1113 frequencies of, 319 linear molecular system, 320–322 membrane, 739, 1112 normal coordinates, 320 normal equations, 320 rod–string system, 317–320 symmetries of, 322  normal subgroups, 1063 normalisation of  eigenfunctions, 562 eigenvectors, 273 functions, 557 vectors, 219  null  zero   matrix, 254, 255 operator, 249 space, of a matrix, 293 vector, 214, 242, 556  1323   INDEX  null operation, as identity element of group,  order of  numerical methods for integration, 1000–1009  tensor, 930  ordinary diﬀerential equations  ODE , see also  nullity, of a matrix, 293 numerical methods for algebraic equations,  1044  985–992  binary chopping, 990 convergence of iteration schemes, 992–994 linear interpolation, 988 Newton–Raphson, 990–992 rearrangement methods, 987  Gaussian integration, 1005–1009 mid-point rule, 1034 Monte Carlo, 1009 nomenclature, 1001 Simpson’s rule, 1004 trapezium rule, 1002–1004  numerical methods for ordinary diﬀerential  equations, 1020–1030  accuracy and convergence, 1021 Adams method, 1024 diﬀerence schemes, 1021–1023 Euler method, 1021 ﬁrst-order equations, 1021–1028 higher-order equations, 1028–1030 isoclines, 1028 Milne’s method, 1022 prediction and correction, 1024–1026 reduction to matrix form, 1030 Runge–Kutta methods, 1026–1028 Taylor series methods, 1023  numerical methods for partial diﬀerential  equations, 1030–1032 diﬀusion equation, 1032 Laplace’s equation, 1031 minimising error, 1032  numerical methods for simultaneous linear  equations, 994–1000  Gauss–Seidel iteration, 996–998 Gaussian elimination with interchange, 995 matrix form, 994–1000 tridiagonal matrices, 998–1000  O x , order of, 132 observables in quantum mechanics, 277, 560 odd functions, see antisymmetric functions ODE, see ordinary diﬀerential equations  ODEs  operators  Hermitian, see Hermitian operators linear, see linear operators and linear  diﬀerential operator and linear integral operator  operators  quantum   angular momentum, 656–663 annihilation and creation, 667 coordinate-free, 648–671 eigenvalues and eigenstates, 649 physical examples  angular momentum, 658 Hamiltonian, 657  approximation in Taylor series, 137n convergence of iteration schemes, 993 group, 1043 group element, 1047 ODE, 468 permutation, 1058 recurrence relations  series , 497 subgroup, 1061  and Lagrange’s theorem, 1065  diﬀerential equations, particular boundary conditions, 468, 470, 501 complementary function, 491 degree, 468 dimensionally homogeneous, 475 exact, 472, 505 ﬁrst-order, 468–484 ﬁrst-order higher-degree, 480–484  soluble for p, 480 soluble for x, 481 soluble for y, 482  general form of solution, 468–470 higher-order, 490–523 homogeneous, 490 inexact, 473 isobaric, 476, 521 linear, 474, 490–517 non-linear, 518–523  exact, 519 isobaric  homogeneous , 521 x absent, 518 y absent, 518  order, 468 ordinary point, see ordinary points of ODE particular integral  solution , 469, 492, 494 singular point, see singular points of ODE singular solution, 469, 481, 482, 484  ordinary diﬀerential equations, methods for  canonical form for second-order equations,  516  eigenfunctions, 554–573 equations containing linear forms, 478–480 equations with constant coeﬃcients, 492–503 Green’s functions, 511–516 integrating factors, 473–475 Laplace transforms, 501–503 numerical, 1020–1030 partially known CF, 506 separable variables, 471 series solutions, 531–550, 604 undetermined coeﬃcients, 494 variation of parameters, 508–510  ordinary points of ODE, 533, 535–538  indicial equation, 543  orthogonal lines, condition for, 12 orthogonal matrices, 270, 929, 930  general properties, see unitary matrices  orthogonal systems of coordinates, 364  1324   INDEX  under unitary transformation, 285  partial diﬀerentiation, 151–179  orthogonal transformations, 932 orthogonalisation  Gram–Schmidt  of  eigenfunctions of an Hermitian operator, 562 eigenvectors of a normal matrix, 275 functions in a Hilbert space, 557  orthogonality of  561–563  eigenfunctions of an Hermitian operator,  eigenvectors of a normal matrix, 275 eigenvectors of an Hermitian matrix, 277 functions, 557 terms in Fourier series, 417, 425 vectors, 219, 244  orthogonality properties of characters, 1094,  1102  orthogonality theorem for irreps, 1090–1092 orthonormal  basis functions, 557 basis vectors, 244  oscillations, see normal modes outcome, of trial, 1119 outer product of two vectors, 936  P cid:2  x , see Legendre polynomials P m  cid:2   x , see associated Legendre functions Pappus’ theorems, 195–197 parabola, equation for, 16 parabolic PDE, 687, 690 parallel axis theorem, 238 parallel vectors, 223 parallelepiped, volume of, 225 parallelogram equality, 247 parallelogram, area of, 223, 224 parameter estimation  statistics , 1229–1255,  1298  Bessel correction, 1248 error in mean, 1298 maximum-likelihood, 1255 mean, 1243 variance, 1245–1248  parameters, variation of, 508–510 parametric equations  of conic sections, 17 of cycloid, 370, 785 of space curves, 340 of surfaces, 345  parity inversion, 1102 Parseval’s theorem  conservation of energy, 451 for Fourier series, 426 for Fourier transforms, 450  partial derivative, see partial diﬀerentiation partial diﬀerential equations  PDE , 675–707,  713–767, see also diﬀerential equations, particular  arbitrary functions, 680–685 boundary conditions, 681, 699–707, 723 characteristics, 699–705  and equation type, 703  1325  partial diﬀerential equations  PDE , methods for  equation types, 687, 710 ﬁrst-order, 681–687 general solution, 681–692 homogeneous, 685 inhomogeneous equation and problem,  685–687, 744–746, 751–767  particular solutions  integrals , 685–692 second-order, 687–698  change of variables, 691, 696–698 constant coeﬃcients, 687  general solution, 689  integral transform methods, 747–751 method of images, see method of images numerical, 1030–1032 separation of variables, see separation of  variables  superposition methods, 717–724 with no undiﬀerentiated term, 684  as gradient of a function of several real  variables, 151  chain rule, 157 change of variables, 158–160 deﬁnitions, 151–153 properties, 157  cyclic relation, 157 reciprocity relation, 157  partial fractions, 18–25  and degree of numerator, 21 as a means of integration, 64 complex roots, 22 in inverse Laplace transforms, 454, 502 repeated roots, 23  partial sum, 115 particular integrals  PI , 469, see also ordinary  diﬀerential equation, methods for and partial diﬀerential equations, methods for  partition of a group, 1064 set, 1065  parts, integration by, 67–69 path integrals, see line integrals PDE, see partial diﬀerential equations PDFs, 1140 pendulums, coupled, 329, 331 periodic function representation, see Fourier  series  permutation groups Sn, 1056–1058  cycle notation, 1057  permutation law in a group, 1047 permutations, 1133–1139  degree, 1056 distinguishable, 1135 order of, 1058 symbol nPk, 1133  perpendicular axes theorem, 209 perpendicular vectors, 219, 244 PF, see probability functions PGF, see probability generating functions   INDEX  phase memory, 895 phase, complex, 896 PI, see particular integrals plane curves, length of, 73  in Cartesian coordinates, 73 in plane polar coordinates, 74 plane polar coordinates, 70, 336  arc length, 74, 361 area element, 202, 361 basis vectors, 336 velocity and acceleration, 337  plane waves, 695, 716 planes  and simultaneous linear equations, 300 vector equation of, 227  plates, conducting, see also complex potentials,  for plates  line charge near, 761 point charge near, 759  point charges, δ-function respresentation, 441 point groups, 1082 points of inﬂection of a function of  one real variable, 50–52 several real variables, 162–167  Poisson distribution Po λ , 1174–1179  and Gaussian distribution, 1187 as limit of binomial distribution, 1174, 1177 mean and variance, 1176 MGF, 1177 multiple, 1178 recurrence formula, 1176  Poisson equation, 575, 679, 744–746  fundamental solution, 757 Green’s functions, 753–767 uniqueness, 705–707  Poisson summation formula, 461 Poisson’s ratio, 953 polar coordinates, see plane polar and cylindrical  polar and spherical polar coordinates  polar representation of complex numbers, 92–95 polar vectors, 949 pole, of a function of a complex variable contours containing, 861–867, 884–887 order, 837, 856 residue, 856–858  polynomial equations, 1–10  conjugate roots, 99 factorisation, 7 multiplicities of roots, 4 number of roots, 83, 85, 868 properties of roots, 9 real roots, 1 solution of, using de Moivre’s theorem, 98 polynomial solutions of ODE, 538, 548–550 populations, sampling of, 1222 positive  semi-  deﬁnite quadratic and Hermitian  forms, 290  positive semi-deﬁnite norm, 244 potential energy of  ion in a crystal lattice, 148  magnetic dipole in a ﬁeld, 220 oscillating system, 317  potential function  and conservative ﬁelds, 389 complex, 871–876 electrostatic, see electrostatic ﬁelds and  gravitational, see gravitational ﬁelds and  potentials  potentials  vector, 389 power series  and diﬀerential equations, see series solutions  of diﬀerential equations  interval of convergence, 132 Maclaurin, see Maclaurin series manipulation: diﬀerence, diﬀerentiation,  integration, product, substitution, sum, 134  Taylor, see Taylor series  power series in a complex variable, 133, 830–832  analyticity, 832 circle and radius of convergence, 133, 831 convergence tests, 831, 832 form, 830  power, in hypothesis testing, 1280 powers, complex, 99, 833 prediction and correction methods, 1024–1026,  1035  prime, non-existence of largest, 34 principal axes of  Cartesian tensors, 951–953 conductivity tensors, 952 inertia tensors, 951 quadratic surfaces, 292 rotation symmetry, 1102  principal normals of space curves, 342 principal value of  complex integrals, 864 complex logarithms, 100, 834  principle of the argument, 880 probability, 1124–1211  axioms, 1125 conditional, 1128–1133 Bayes’ theorem, 1132 combining, 1130  deﬁnition, 1125  for intersection ∩, 1120 for union ∪, 1121, 1125–1128  probability distributions, 1139, see also  individual distributions  bivariate, see bivariate distributions change of variables, 1150–1157 cumulants, 1166 generating functions, see moment generating  functions and probability generating functions  mean µ, 1144 mean of functions, 1145 mode, median and quartiles, 1145 moments, 1147–1150 multivariate, see multivariate distributions  1326   INDEX  standard deviation σ, 1146 variance σ2, 1146  probability functions  PFs , 1139 cumulative  CPFs , 1139, 1141 density functions  PDFs , 1140  probability generating functions  PGFs ,  discrete, 1139 independent, 1156, 1200 sums of, 1160–1162 uncorrelated, 1200  range of a matrix, 293 rank of matrices, 267  1157–1162  and MGF, 1163 binomial, 1161 deﬁnition, 1158 geometric, 1159 mean and variance, 1158 Poisson, 1158 sums of RV, 1161 trials, 1158 variable sums of RV, 1161  product rule for diﬀerentiation, 44–46, 48–50 products of inertia, 951 projection operators for irreps, 1107, 1116 projection tensors, 979 proper rotations, 946 proper subgroups, 1061 pseudoscalars, 947, 950 pseudotensors, 946–950, 964 pseudovectors, 946–950  Q cid:2  x , see Legendre polynomials Qm quadratic equations   cid:2   x , see associated Legendre functions  complex roots of, 83 properties of roots, 10 roots of, 2  quadratic forms, 288–292  completing the square, 1206 positive deﬁnite and semi-deﬁnite, 290 quadratic surfaces, 292 removing cross terms, 289 stationary properties of eigenvectors, 290  quantum mechanics, from classical mechanics,  657  quantum operators, see operators  quantum  quartiles, of RVD, 1145 quaternion group, 1073, 1113 quotient law for tensors, 939–941 quotient rule for diﬀerentiation, 47 quotient test for series, 127  radius of convergence, 133, 831 radius of curvature  of plane curves, 53 of space curves, 342  radius of torsion of space curves, 343 random number generation, 1017 random numbers, non-uniform distribution,  random variable distributions, see probability  1035  distributions  random variables  RV , 1119, 1139–1143  continuous, 1140–1143 dependent, 1196–1205  and determinants, 267 and linear dependence, 267  rank of tensors, see order of tensor rate of change of a function of  one real variable, 41 several real variables, 153–155  ratio comparison test, 127 ratio test  D’Alembert , 126, 832  in convergence of power series, 132  ratio theorem, 215  and centroid of a triangle, 216  Rayleigh–Ritz method, 327–329, 800 real part or term of a complex number, 83 real roots, of a polynomial equation, 1 rearrangement methods for algebraic equations,  987  reciprocal vectors, 233, 366, 955, 959 reciprocity relation for partial derivatives, 157 rectangular distribution, 1194  Fourier transform of, 442  recurrence relations  functions , 585, 611 associated Laguerre polynomials, 624 associated Legendre functions, 592 Chebyshev polynomials, 601 conﬂuent hypergeometric functions, 635 Hermite polynomials, 628 hypergeometric functions, 632 Laguerre polynomials, 620 Legendre polynomials, 586  recurrence relations  series , 496–501  characteristic equation, 499 coeﬃcients, 536, 538, 999 ﬁrst-order, 497 second-order, 499 higher-order, 501  reducible representations, 1084, 1086 reduction formulae for integrals, 69 reﬂections  and improper rotations, 946 as symmetry operations, 1041  reﬂexivity, and equivalence relations, 1064 regular functions, see analytic functions regular representations, 1097, 1110 regular singular points, 534, 538–540 relative velocities, 218 remainder term in Taylor series, 138 repeated roots of auxiliary equation, 493 representation, 1076  deﬁnition, 1082 dimension of, 1078, 1082 equivalent, 1084–1086 faithful, 1083, 1098 generation of, 1078–1084, 1112 irreducible, see irreps  1327   INDEX  natural, 1081, 1110 product, 1103–1105 reducible, 1084, 1086 regular, 1097, 1110  counting irreps, 1098  unitary, 1086  sampling  correlation, 1227 covariance, 1227 space, 1119 statistics, 1222–1229 with or without replacement, 1129  representative matrices, 1079  scalar ﬁelds, 347  block-diagonal, 1086 eigenvalues, 1100 inverse, 1083 number needed, and order of group, 1082 of identity, 1082  derivative along a space curve, 349 gradient, 348–352 line integrals, 377–387 rate of change, 349  scalar product, 219–222  residue  at a pole, 856–858 theorem, 858–860  resolution function, 446 resolvent kernel, 814, 815 response matrix, for linear least squares, 1273 rhomboid, volume of, 237 Riemann tensor, 981 Riemann theorem for conditional convergence,  124  Riemann zeta series, 128, 129 right hand screw rule, 222 Rodrigues’ formula for  associated Laguerre polynomials, 622 associated Legendre functions, 588 Chebyshev polynomials, 599 Hermite polynomials, 626 Laguerre polynomials, 618 Legendre polynomials, 581  Rolle’s theorem, 55 root test  Cauchy , 129, 831 roots  of a polynomial equation, 2  properties, 9  of unity, 97  rope, suspended at its ends, 786 rotation groups  continuous , invariant  subspaces, 1088  rotation matrices as a group, 1048 rotation of a vector, see curl rotations  as symmetry operations, 1041 axes and orthogonal matrices, 930, 931, 961 improper, 946–948 invariance under, 934 product of, 931 proper, 946  Rouch´e’s theorem, 880–882 row matrix, 250 Runge–Kutta methods, 1026–1028 RV, see random variables RVD  random variable distributions , see  probability distributions  saddle point method of integration, 908 saddle points, 162  and integral evaluation, 905 suﬃcient conditions, 164, 167  and inner product, 244 and metric tensor, 958 and perpendicular vectors, 219, 244 for vectors with complex components, 221 in Cartesian coordinates, 221 invariance, 930, 939  scalar triple product, 224–226  cyclic permutation of, 225 in Cartesian coordinates, 225  determinant form, 225  interchange of dot and cross, 225  scalars, 212  invariance, 930 zero-order tensors, 933  scale factors, 359, 362, 364  and metric tensor, 957, 972  scattering in quantum mechanics, 463 Schmidt–Hilbert theory, 816–819 Schr¨odinger equation, 679 constant potential, 768 hydrogen atom, 741 numerical solution, 1039 variational approach, 795 Schwarz inequality, 246, 559 Schwarz–Christoﬀel transformation, 843 second diﬀerences, 1019 second-order diﬀerential equations, see ordinary diﬀerential equations and partial diﬀerential equations  secular determinant, 280 self-adjoint operators, 559–564, see also  Hermitian operators  semicircle, angle in, 18 semicircular lamina, centre of mass, 197 separable kernel in integral equations, 807 separable variables in ODE, 471 separation constants, 715, 717 separation of variables, for PDE, 713–746  diﬀusion equation, 716, 722–724, 737, 751 expansion methods, 741–744 general method, 713–717 Helmholtz equation, 737–741 inhomogeneous boundary conditions, 722–724 inhomogeneous equations, 744–746 Laplace equation, 717–722, 725–737, 741 polar coordinates, 725–746 separation constants, 715, 717 superposition methods, 717–724  1328   INDEX  wave equation, 714–716, 737, 739  series, 115–141  series  convergence of, see convergence of inﬁnite  linear operators, 249 matrices, 263  singular integrals, see improper integrals singular point  singularity , 826, 837–839  series solutions of diﬀerential equations, 531–550  about ordinary points, 535–538 about regular singular points, 538–540  solid: mass, centre of mass and centroid,  diﬀerentiation of, 131 ﬁnite and inﬁnite, 116 integration of, 131 multiplication by a scalar, 131 multiplication of  Cauchy product , 131 notation, 116 operations, 131 summation, see summation of series  series, particular arithmetic, 117 arithmetico-geometric, 118 Fourier, see Fourier series geometric, 117 Maclaurin, 138, 140 power, see power series powers of natural numbers, 121 Riemann zeta, 128, 129 Taylor, see Taylor series  Frobenius series, 539  convergence, 536 indicial equation, 540 linear independence, 540 polynomial solutions, 538, 548–550 recurrence relation, 536, 538 second solution, 537, 544–548 derivative method, 545–548 Wronskian method, 544, 580  shortest path, 778  and geodesics, 976, 982  similarity transformations, 283–285, 929, 1092  properties of matrix under, 284 unitary transformations, 285 simple harmonic oscillator, 555  energy levels of, 642, 902 equation, 535, 566 operator formalism, 667  simple poles, 838 Simpson’s rule, 1004 simultaneous linear equations, 292–307  and intersection of planes, 300 homogeneous and inhomogeneous, 293 singular value decomposition, 301–307 solution using  Cramer’s rule, 299 inverse matrix, 295 numerical methods, see numerical methods for simultaneous linear equations  sine, sin x   in terms of exponential functions, 102 Maclaurin series for, 140 orthogonality relations, 417  singular and non-singular integral equations, 805  essential, 838, 856 removable, 838  singular points of ODE, 533  irregular, 534 particular equations, 535 regular, 534  singular solution of ODE, 469, 481, 482, 484 singular value decomposition  and simultaneous linear equations, 301–307 singular values, 302  sinh, hyperbolic sine, 102, 833, see also  hyperbolic functions  skew-symmetric matrices, 270 skewness, 1150, 1227 Snell’s law, 788 soap ﬁlms, 780 solenoidal vectors, 352, 389 solid angle  as surface integral, 395 subtended by rectangle, 411  193–195  source density, 679 space curves, 340–344  arc length, 341 binormal, 342 curvature, 342 Frenet–Serret formulae, 343 parametric equations, 340 principal normal, 342 radius of curvature, 342 radius of torsion, 343 tangent vector, 342 torsion, 342  spaces, see vector spaces span of a set of vectors, 242 sphere, vector equation of, 228 spherical Bessel functions, 615, 741  of ﬁrst kind j cid:2  z , 615 of second kind n cid:2  z , 615  spherical harmonics Y m addition theorem, 594   cid:2   θ, φ , 593–595  spherical polar coordinates, 361–363  area element, 362 basis vectors, 362 length element, 362 vector operators, 361–363 volume element, 205, 362  spur of a matrix, see trace of a matrix square matrices, 249 square, symmetries of, 1100 square-wave, Fourier series for, 418 stagnation points of ﬂuid ﬂow, 873 standard deviation σ, 1146  of sample, 1224  standing waves, 693  1329   INDEX  stationary phase, method of, 912–920 stationary values of functions of  one real variable, 50–52 several real variables, 162–167  natural interval, 565, 567 two independent variables, 801 variational approach, 790–795 weight function, 790 zeros of eigenfunctions, 573  of integrals, 776 under constraints, see Lagrange undetermined  subdominant term, in Stokes phenomenon, 904 subgroups, 1061–1063  multipliers  statistical tests, and hypothesis testing , 1278 statistics, 1119, 1221–1298  describing data, 1222–1229 estimating parameters, 1229–1255, 1298  steepest descents, method of, 908–912 Stirling’s  approximation, 637, 1185 asymptotic series, 637  Stokes constant, in Stokes phenomenon, 904 Stokes line, 899, 903 Stokes phenomenon, 903  dominant term, 904 Stokes constant, 904 subdominant term, 904  Stokes’ equation, 643, 799, 888–894  Airy integrals, 890–894 qualitative solutions, 888 series solution, 890  Stokes’ theorem, 388, 406–409  for tensors, 955 physical applications, 408 related theorems, 407  strain tensor, 953 stratiﬁed sampling, in Monte Carlo methods,  1012  streamlines and complex potentials, 873 stress tensor, 953 stress waves, 980 string  loaded, 798 plucked, 770 transverse vibrations of, 676, 789  Student’s t-distribution normalisation, 1286 plots, 1287  Student’s t-test, 1284–1290  comparison of means, 1289 critical points table, 1288 one- and two-tailed conﬁdence limits, 1288  Sturm–Liouville equations, 564  boundary conditions, 564 examples, 566  associated Laguerre, 566, 622 associated Legendre, 566, 590, 591 Bessel, 566, 608–611 Chebyshev, 566, 599 conﬂuent hypergeometric, 566 Hermite, 566 hypergeometric, 566 Laguerre, 566, 619 Legendre, 566, 583  index, 1066 normal, 1063 order, 1061  proper, 1061 trivial, 1061  Lagrange’s theorem, 1065  submatrices, 267 subscripts and superscripts, 928  contra- and covariant, 956 covariant derivative, 969 dummy, 928 free, 928 partial derivative, 969 summation convention, 928, 955 substitution, integration by, 65–67 summation convention, 928, 955 summation of series, 116–124  arithmetic, 117 arithmetico-geometric, 118 contour integration method, 882 diﬀerence method, 119 Fourier series method, 427 geometric, 117 powers of natural numbers, 121 transformation methods, 122–124  diﬀerentiation, 122 integration, 122 substitution, 123  superposition methods  for ODE, 554, 568–571 for PDE, 717–724  surface integrals  and divergence theorem, 401 Archimedean upthrust, 396, 410 of scalars, vectors, 389–396 physical examples, 395  surfaces, 345–347  area of, 346  cone, 74 solid, and Pappus’ theorem, 195–197 sphere, 346  coordinate curves, 346 normal to, 346, 350 of revolution, 74 parametric equations, 345 quadratic, 292 tangent plane, 346  SVD, see singular value decomposition symmetric functions, 416 and Fourier series, 419 and Fourier transforms, 445  symmetric matrices, 270  manipulation to self-adjoint form, 565–568  general properties, see Hermitian matrices  1330   INDEX  rod, 769 string, 676  trapezium rule, 1002–1004 trial functions  for eigenvalue estimation, 793 for particular integrals of ODE, 494  trials, 1119 triangle inequality, 246, 559 triangle, centroid of, 216 triangular matrices, 269 tridiagonal matrices, 998–1000, 1030, 1033 trigonometric identities, 10–15 triple integrals, see multiple integrals triple scalar product, see scalar triple product triple vector product, see vector triple product turning point, 50  symmetric tensors, 938 symmetry in equivalence relations, 1064 symmetry operations on molecules, 1041 order of application, 1044  −1 x, Maclaurin series for, 140  t-test, see Student’s t-test t substitution, 65 tan tangent planes to surfaces, 346 tangent vectors to space curves, 342 tanh, hyperbolic tangent, see hyperbolic  functions  Taylor series, 136–141  and ﬁnite diﬀerences, 1019, 1026 and Taylor’s theorem, 136–139, 853 approximation errors, 139  in numerical methods, 992, 1003  as solution of ODE, 1023 for functions of a complex variable, 853–855 for functions of several real variables, 160–162 remainder term, 138 required properties, 136 standard forms, 136  uncertainty principle  Heisenberg , 435–437  from commutator, 664  undetermined coeﬃcients, method of, 494 undetermined multipliers, see Lagrange  undetermined multipliers  uniform distribution, 1194  union ∪, probability for, 1121, 1125, 1128  tensors, see Cartesian tensors and Cartesian  uniqueness theorem  tensors, particular and general tensors  test statistic, 1278 tetrahedral group, 1115 tetrahedron  mass of, 194 volume of, 192 thermodynamics  ﬁrst law of, 176 Maxwell’s relations, 176–178  top-hat function, see rectangular distribution torque, vector representation of, 223 torsion of space curves, 342 total derivative, 154 total diﬀerential, 154 trace of a matrix, 258  and second-order tensors, 939 as sum of eigenvalues, 280, 287 invariance under similarity transformations,  284, 1092  trace formula, 287  transcendental equations, 986 transformation matrix, 283, 289 transformations  active and passive, 948 conformal, 839–844 coordinate, see coordinate transformations similarity, see similarity transformations  transforms, integral, see integral transforms and Fourier transforms and Laplace transforms  transients, and the diﬀusion equation, 723 transitivity in equivalence relations, 1064 transpose of a matrix, 250, 255  product rule, 256  transverse vibrations  membrane, 677, 739, 768  Klein–Gordon equation, 711 Laplace equation, 741 Poisson equation, 705–707  unit step function, see Heaviside function unit vectors, 219 unitary  matrices, 271  eigenvalues and eigenvectors, 278  representations, 1086 transformations, 285  upper triangular matrices, 269  variable end-points, 782–785 variable, dummy, 61 variables, separation of, see separation of  variables  variance σ2, 1146  from MGF, 1163 from PGF, 1159 of dependent RV, 1203 of sample, 1224  variation of parameters, 508–510 variation, constrained, 785–787 variational principles, physical, 787–790  Fermat, 787 Hamilton, 788  variations, calculus of, see calculus of variations vector operators, 347–369  acting on sums and products, 354 combinations of, 355–357 curl, 353, 368  del ∇, 348 del squared ∇2, 352  divergence  div , 352 geometrical deﬁnitions, 398–400  1331   INDEX  gradient operator  grad , 348–352, 367 identities, 356, 978 Laplacian, 352, 368 non-Cartesian, 357–369 tensor forms, 971–975  commutativity of addition and subtraction,  213  multiplication by a complex scalar, 222 multiplication by a scalar, 214 multiplication of, see scalar product and  curl, 974 divergence, 972 gradient, 972 Laplacian, 973  vector product, 222–224  anticommutativity, 222 deﬁnition, 222 determinant form, 224 in Cartesian coordinates, 224 non-associativity, 222  vector spaces, 242–247, 1113  associativity of addition, 242 basis vectors, 243 commutativity of addition, 242 complex, 242 deﬁning properties, 242 dimensionality, 243 group actions on, 1088 inequalities: Bessel, Schwarz, triangle, 246 invariant, 1088, 1113 matrices as an example, 252 of inﬁnite dimensionality, 556–559  associativity of addition, 556 basis functions, 556 commutativity of addition, 556 deﬁning properties, 556 Hilbert spaces, 557–559 inequalities: Bessel, Schwarz, triangle, 559  parallelogram equality, 247 real, 242 span of a set of vectors in, 242  vector triple product, 226  identities, 226 non-associativity, 226  vectors  as ﬁrst-order tensors, 932 as geometrical objects, 241 base, 336 column, 250 compared with scalars, 212 component form, 217 examples of, 212 graphical representation of, 212 irrotational, 353 magnitude of, 218 non-Cartesian, 336, 358, 362 notation, 212 polar and axial, 949 solenoidal, 352, 389 span of, 242  vectors, algebra of, 212–234  addition and subtraction, 213  in component form, 218  angle between, 221 associativity of addition and subtraction, 213  1332  vector product  outer product, 936 vectors, applications  centroid of a triangle, 216 equation of a line, 226 equation of a plane, 227 equation of a sphere, 228 ﬁnding distance from a  line to a line, 231 line to a plane, 232 point to a line, 229 point to a plane, 230  intersection of two planes, 228  vectors, calculus of, 334–369  diﬀerentiation, 334–339, 344 integration, 339 line integrals, 377–389 surface integrals, 389–396 volume integrals, 396  vectors, derived quantities  curl, 353 derivative, 334 diﬀerential, 338, 344 divergence  div , 352 reciprocal, 233, 366, 955, 959 vector ﬁelds, 347  curl, 406 divergence, 352 ﬂux, 395 rate of change, 350  vectors, physical  acceleration, 335 angular momentum, 238 angular velocity, 223, 238, 353 area, 393–395, 408 area of parallelogram, 223, 224 force, 212, 213, 220 moment or torque of a force, 223 velocity, 335  velocity vectors, 335 Venn diagrams, 1119–1124 vibrations  internal, see normal modes longitudinal, in a rod, 677 transverse  membrane, 677, 739, 768, 799, 801 rod, 769 string, 676, 789  Volterra integral equation, 804, 805  diﬀerentiation methods, 812 Laplace transform methods, 810  volume elements  curvilinear coordinates, 365 cylindrical polars, 360 spherical polars, 205, 362   volume integrals, 396  and divergence theorem, 401  zero-order tensors, 932–935 zeros of a function of a complex variable, 839  INDEX  location of, 879–882, 921 order, 839, 856 principle of the argument, 880 Rouch´e’s theorem, 880, 882  zeros of Sturm-Liouville eigenfunctions, 573 zeros, of a polynomial, 2 zeta series  Riemann , 128, 129 z-plane, see Argand diagram  volume of cone, 75 ellipsoid, 207 parallelepiped, 225 rhomboid, 237 tetrahedron, 192  volumes  as surface integrals, 397, 401 in many dimensions, 210 of regions, using multiple integrals, 191–193  volumes of revolution, 75  and surface area & centroid, 195–197  wave equation, 676, 688, 790  boundary conditions, 693–695 characteristics, 704 from Maxwell’s equations, 373 in one dimension, 689, 693–695 in three dimensions, 695, 714, 737 standing waves, 693  wave number, 437, 693n wave packet, 436 wave vector, k, 437 wavefunction of electron in hydrogen atom, 208 Weber functions Yν  x , 607 wedge product, see vector product weight  of relative tensor, 964 of variable, 477  weight function, 555, 790 Wiener–Kinchin theorem, 450 WKB methods, 895–905  accuracy, 902 general solutions, 897 phase memory, 895 the Stokes phenomenon, 903  work done  by force, 381 vector representation, 220  Wronskian  and Green’s functions, 527 for second solution of ODE, 544, 580 from ODE, 532 test for linear independence, 491, 532  X-ray scattering, 237  Y m  cid:2   θ, φ , see spherical harmonics Yν  x , Bessel functions of second kind, 607 Young’s modulus, 677, 953  z, as a complex number, 84 ∗ z zero  null   , as complex conjugate, 89–91  matrix, 254, 255 operator, 249  unphysical state ∅ cid:21 , 650  vector, 214, 242, 556  1333

@highlight

The third edition of this highly acclaimed undergraduate textbook is suitable for teaching all the mathematics for an undergraduate course in any of the physical sciences. As well as lucid descriptions of all the topics and many worked examples, it contains over 800 exercises. New stand-alone chapters give a systematic account of the 'special functions' of physical science, cover an extended range of practical applications of complex variables, and give an introduction to quantum operators. Further tabulations, of relevance in statistics and numerical integration, have been added. In this edition, half of the exercises are provided with hints and answers and, in a separate manual available to both students and their teachers, complete worked solutions. The remaining exercises have no hints, answers or worked solutions and can be used for unaided homework; full solutions are available to instructors on a password-protected web site