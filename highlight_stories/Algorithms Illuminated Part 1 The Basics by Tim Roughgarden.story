Algorithms Illuminated  Part 1: The Basics  Tim Roughgarden   c  2017 by Tim Roughgarden All rights reserved. No portion of this book may be reproduced in any form without permission from the publisher, except as permitted by U. S. copyright law.  Printed in the United States of America  First Edition  Cover image: Stanza, by Andrea Belag  ISBN: 978-0-9992829-0-8  Paperback  ISBN: 978-0-9992829-1-5  ebook   Library of Congress Control Number: 2017914282  Soundlikeyourself Publishing, LLC San Francisco, CA tim.roughgarden@gmail.com www.algorithmsilluminated.org   To Emma    Contents  Preface  1  Integer Multiplication  Introduction 1.1 Why Study Algorithms? 1.2 1.3 Karatsuba Multiplication 1.4 MergeSort: The Algorithm 1.5 MergeSort: The Analysis 1.6 Guiding Principles for the Analysis of Algorithms Problems  2 Asymptotic Notation  2.1 The Gist 2.2 Big-O Notation 2.3 Two Basic Examples 2.4 Big-Omega and Big-Theta Notation 2.5 Additional Examples Problems  3 Divide-and-Conquer Algorithms  3.1 The Divide-and-Conquer Paradigm 3.2 Counting Inversions in O n log n  Time 3.3 Strassen’s Matrix Multiplication Algorithm *3.4 An O n log n -Time Algorithm for the Closest Pair Problems  4 The Master Method  Integer Multiplication Revisited  4.1 4.2 Formal Statement 4.3 Six Examples  v  vii  1 1 3 6 12 18 26 33  36 36 45 47 50 54 57  60 60 61 71 77 90  92 92 95 97   vi  Contents  *4.4 Proof of the Master Method Problems  5 QuickSort  5.1 Overview 5.2 Partitioning Around a Pivot Element 5.3 The Importance of Good Pivots 5.4 Randomized QuickSort *5.5 Analysis of Randomized QuickSort *5.6 Sorting Requires ⌦ n log n  Comparisons Problems  6 Linear-Time Selection  6.1 The RSelect Algorithm *6.2 Analysis of RSelect *6.3 The DSelect Algorithm *6.4 Analysis of DSelect Problems  A Quick Review of Proofs By Induction  A.1 A Template for Proofs by Induction A.2 Example: A Closed-Form Formula A.3 Example: The Size of a Complete Binary Tree  B Quick Review of Discrete Probability  B.1 Sample Spaces B.2 Events B.3 Random Variables B.4 Expectation B.5 Linearity of Expectation B.6 Example: Load Balancing  Index  103 114  117 117 121 128 132 135 145 151  155 155 163 167 172 180  183 183 184 185  186 186 187 189 190 192 195  199   Preface  This book is the ﬁrst of a four-part series based on my online algorithms courses that have been running regularly since 2012, which in turn are based on an undergraduate course that I’ve taught many times at Stanford University.  What We’ll Cover  Algorithms Illuminated, Part 1 provides an introduction to and basic literacy in the following four topics.  Asymptotic analysis and big-O notation. Asymptotic notation provides the basic vocabulary for discussing the design and analysis of algorithms. The key concept here is “big-O” notation, which is a modeling choice about the granularity with which we measure the running time of an algorithm. We’ll see that the sweet spot for clear high-level thinking about algorithm design is to ignore constant factors and lower-order terms, and to concentrate on how an algorithm’s performance scales with the size of the input.  Divide-and-conquer algorithms and the master method. There’s no silver bullet in algorithm design, no single problem-solving method that cracks all computational problems. However, there are a few general algorithm design techniques that ﬁnd successful ap- plication across a range of diﬀerent domains. In this part of the series, we’ll cover the “divide-and-conquer” technique. The idea is to break a problem into smaller subproblems, solve the subproblems recursively, and then quickly combine their solutions into one for the original problem. We’ll see fast divide-and-conquer algorithms for sorting, integer and matrix multiplication, and a basic problem in computational geometry. We’ll also cover the master method, which is  vii   viii  Preface  a powerful tool for analyzing the running time of divide-and-conquer algorithms.  Randomized algorithms. A randomized algorithm “ﬂips coins” as it runs, and its behavior can depend on the outcomes of these coin ﬂips. Surprisingly often, randomization leads to simple, elegant, and practical algorithms. The canonical example is randomized QuickSort, and we’ll explain this algorithm and its running time analysis in detail. We’ll see further applications of randomization in Part 2.  Sorting and selection. As a byproduct of studying the ﬁrst three topics, we’ll learn several famous algorithms for sorting and selection, including MergeSort, QuickSort, and linear-time selection  both ran- domized and deterministic . These computational primitives are so blazingly fast that they do not take much more time than that needed just to read the input. It’s important to cultivate a collection of such “for-free primitives,” both to apply directly to data and to use as the building blocks for solutions to more diﬃcult problems.  For a more detailed look into the book’s contents, check out the “Upshot” sections that conclude each chapter and highlight the most important points.  Topics covered in the other three parts. Algorithms Illumi- nated, Part 2 covers data structures  heaps, balanced search trees, hash tables, bloom ﬁlters , graph primitives  breadth- and depth-ﬁrst search, connectivity, shortest paths , and their applications  rang- ing from deduplication to social network analysis . Part 3 focuses on greedy algorithms  scheduling, minimum spanning trees, cluster- ing, Huﬀman codes  and dynamic programming  knapsack, sequence alignment, shortest paths, optimal search trees . Part 4 is all about N P -completeness, what it means for the algorithm designer, and strategies for coping with computationally intractable problems, in- cluding the analysis of heuristics and local search.  Skills You’ll Learn  Mastering algorithms takes time and eﬀort. Why bother?  Become a better programmer. You’ll learn several blazingly fast subroutines for processing data and several useful data structures for   Preface  ix  organizing data that can be deployed directly in your own programs. Implementing and using these algorithms will stretch and improve your programming skills. You’ll also learn general algorithm design paradigms that are relevant for many diﬀerent problems across diﬀer- ent domains, as well as tools for predicting the performance of such algorithms. These “algorithmic design patterns” can help you come up with new algorithms for problems that arise in your own work.  Sharpen your analytical skills. You’ll get lots of practice describ- ing and reasoning about algorithms. Through mathematical analysis, you’ll gain a deep understanding of the speciﬁc algorithms and data structures covered in these books. You’ll acquire facility with sev- eral mathematical techniques that are broadly useful for analyzing algorithms.  Think algorithmically. After learning about algorithms it’s hard not to see them everywhere, whether you’re riding an elevator, watch- ing a ﬂock of birds, managing your investment portfolio, or even watching an infant learn. Algorithmic thinking is increasingly useful and prevalent in disciplines outside of computer science, including biology, statistics, and economics.  Literacy with computer science’s greatest hits. Studying al- gorithms can feel like watching a highlight reel of many of the greatest hits from the last sixty years of computer science. No longer will you feel excluded at that computer science cocktail party when someone cracks a joke about Dijkstra’s algorithm. After reading these books, you’ll know exactly what they mean.  Ace your technical interviews. Over the years, countless stu- dents have regaled me with stories about how mastering the concepts in these books enabled them to ace every technical interview question they were ever asked.  How These Books Are Diﬀerent  This series of books has only one goal: to teach the basics of algorithms in the most accessible way possible. Think of them as a transcript of what an expert algorithms tutor would say to you over a series of one-on-one lessons.   x  Preface  There are a number of excellent more traditional and more encyclo- pedic textbooks on algorithms, any of which usefully complement this book series with additional details, problems, and topics. I encourage you to explore and ﬁnd your own favorites. There are also several books that, unlike these books, cater to programmers looking for ready-made algorithm implementations in a speciﬁc programming language. Many such implementations are freely available on the Web as well.  Who Are You?  The whole point of these books and the online courses they are based on is to be as widely and easily accessible as possible. People of all ages, backgrounds, and walks of life are well represented in my online courses, and there are large numbers of students  high-school, college, etc. , software engineers  both current and aspiring , scientists, and professionals hailing from all corners of the world.  This book is not an introduction to programming, and ideally you’ve acquired basic programming skills in a standard language  like Java, Python, C, Scala, Haskell, etc. . For a litmus test, check out Section 1.4—if it makes sense, you’ll be ﬁne for the rest of the book. If you need to beef up your programming skills, there are several outstanding free online courses that teach basic programming.  We also use mathematical analysis as needed to understand how and why algorithms really work. The freely available lecture notes Mathematics for Computer Science, by Eric Lehman and Tom Leighton, are an excellent and entertaining refresher on mathematical  notation  likeP and 8 , the basics of proofs  induction, contradiction,  etc. , discrete probability, and much more.1 Appendices A and B also provide quick reviews of proofs by induction and discrete probability, respectively. The starred sections are the most mathematically intense ones. The math-phobic or time-constrained reader can skip these on a ﬁrst reading without loss of continuity.  Additional Resources  These books are based on online courses that are currently running on the Coursera and Stanford Lagunita platforms. There are several  1http:  www.boazbarak.org cs121 LehmanLeighton.pdf.   Preface  xi  resources available to help you replicate as much of the online course experience as you like.  If you’re more in the mood to watch and listen than Videos. to read, check out the YouTube video playlists available from www.algorithmsilluminated.org. These videos cover all of the top- ics of this book series. I hope they exude a contagious enthusiasm for algorithms that, alas, is impossible to replicate fully on the printed page.  Quizzes. How can you know if you’re truly absorbing the concepts in this book? Quizzes with solutions and explanations are scattered throughout the text; when you encounter one, I encourage you to pause and think about the answer before reading on.  End-of-chapter problems. At the end of each chapter you’ll ﬁnd several relatively straightforward questions to test your understanding, followed by harder and more open-ended challenge problems. Solutions to these end-of-chapter problems are not included here, but readers can interact with me and each other about them through the book’s discussion forum  see below .  Programming problems. Many of the chapters conclude with a suggested programming project, where the goal is to develop a detailed understanding of an algorithm by creating your own working implementation of it. Data sets, along with test cases and their solutions, can be found at www.algorithmsilluminated.org. Discussion forums. A big reason for the success of online courses is the opportunities they provide for participants to help each other understand the course material and debug programs through discus- sion forums. Readers of these books have the same opportunity, via the forums available from www.algorithmsilluminated.org.  Acknowledgments  These books would not exist without the passion and hunger supplied by the thousands of participants in my algorithms courses over the years, both on-campus at Stanford and on online platforms. I am par- ticularly grateful to those who supplied detailed feedback on an earlier draft of this book: Tonya Blust, Yuan Cao, Jim Humelsine, Bayram   xii  Preface  Kuliyev, Patrick Monkelban, Kyle Schiller, Nissanka Wickremasinghe, and Daniel Zingaro.  I always appreciate suggestions and corrections from readers, which are best communicated through the discussion forums mentioned above.  Stanford University Stanford, California  Tim Roughgarden September 2017   Chapter 1  Introduction  The goal of this chapter is to get you excited about the study of algorithms. We begin by discussing algorithms in general and why they’re so important. Then we use the problem of multiplying two integers to illustrate how algorithmic ingenuity can improve on more straightforward or naive solutions. We then discuss the MergeSort algorithm in detail, for several reasons: it’s a practical and famous algorithm that you should know; it’s a good warm-up to get you ready for more intricate algorithms; and it’s the canonical introduction to the “divide-and-conquer” algorithm design paradigm. The chapter concludes by describing several guiding principles for how we’ll analyze algorithms throughout the rest of the book.  1.1 Why Study Algorithms?  Let me begin by justifying this book’s existence and giving you some reasons why you should be highly motivated to learn about algorithms. So what is an algorithm, anyway? It’s a set of well-deﬁned rules—a recipe, in eﬀect—for solving some computational problem. Maybe you have a bunch of numbers and you want to rearrange them so that they’re in sorted order. Maybe you have a road map and you want to compute the shortest path from some origin to some destination. Maybe you need to complete several tasks before certain deadlines, and you want to know in what order you should ﬁnish the tasks so that you complete them all by their respective deadlines.  So why study algorithms?  Important for all other branches of computer science. First, understanding the basics of algorithms and the closely related ﬁeld of data structures is essential for doing serious work in pretty much any branch of computer science. For example, at Stanford University,  1   2  Introduction  every degree the computer science department oﬀers  B.S., M.S., and Ph.D.  requires an algorithms course. To name just a few examples:  1. Routing protocols in communication networks piggyback on  classical shortest path algorithms.  2. Public-key cryptography relies on eﬃcient number-theoretic  algorithms.  3. Computer graphics requires the computational primitives sup-  plied by geometric algorithms.  4. Database indices rely on balanced search tree data structures.  5. Computational biology uses dynamic programming algorithms  to measure genome similarity.  And the list goes on.  Driver of technological innovation. Second, algorithms play a key role in modern technological innovation. To give just one obvious example, search engines use a tapestry of algorithms to eﬃciently compute the relevance of various Web pages to a given search query. The most famous such algorithm is the PageRank algorithm currently in use by Google. Indeed, in a December 2010 report to the United States White House, the President’s council of advisers on science and technology wrote the following:  “Everyone knows Moore’s Law –– a prediction made in 1965 by Intel co-founder Gordon Moore that the density of transistors in integrated circuits would continue to double every 1 to 2 years. . . in many areas, performance gains due to improvements in algorithms have vastly exceeded even the dramatic performance gains due to increased processor speed.”1  1Excerpt from Report to the President and Congress: Designing a Digital  Future, December 2010  page 71 .   1.2  Integer Multiplication  3  Lens on other sciences. Third, although this is beyond the scope of this book, algorithms are increasingly used to provide a novel “lens” on processes outside of computer science and technology. For example, the study of quantum computation has provided a new computational viewpoint on quantum mechanics. Price ﬂuctuations in economic markets can be fruitfully viewed as an algorithmic process. Even evolution can be thought of as a surprisingly eﬀective search algorithm.  Good for the brain. Back when I was a student, my favorite classes were always the challenging ones that, after I struggled through them, left me feeling a few IQ points smarter than when I started. I hope this material provides a similar experience for you.  Fun! Finally, I hope that by the end of the book you can see why the design and analysis of algorithms is simply fun. It’s an endeavor that requires a rare blend of precision and creativity. It can certainly be frustrating at times, but it’s also highly addictive. And let’s not forget that you’ve been learning about algorithms since you were a little kid.  1.2  Integer Multiplication  1.2.1 Problems and Solutions  When you were in third grade or so, you probably learned an algorithm for multiplying two numbers—a well-deﬁned set of rules for trans- forming an input  two numbers  into an output  their product . It’s important to distinguish between two diﬀerent things: the description of the problem being solved, and that of the method of solution  that is, the algorithm for the problem . In this book, we’ll repeatedly follow the pattern of ﬁrst introducing a computational problem  the inputs and desired output , and then describing one or more algorithms that solve the problem.  1.2.2 The Integer Multiplication Problem  In the integer multiplication problem, the input is two n-digit numbers, which we’ll call x and y. The length n of x and y could be any positive integer, but I encourage you to think of n as large, in the thousands or   4  Introduction  even more.2  Perhaps you’re implementing a cryptographic application that must manipulate very large numbers.  The desired output in the integer multiplication problem is just the product x · y.  Problem: Integer Multiplication  Input: Two n-digit nonnegative integers, x and y.  Output: The product x · y.  1.2.3 The Grade-School Algorithm  Having deﬁned the computational problem precisely, we describe an algorithm that solves it—the same algorithm you learned in third grade. We will assess the performance of this algorithm through the number of “primitive operations” it performs, as a function of the number of digits n in each input number. For now, let’s think of a primitive operation as any of the following:  i  adding two single-digit numbers;  ii  multiplying two single-digit numbers; or  iii  adding a zero to the beginning or end of a number.  To jog your memory, consider the concrete example of multiplying x = 5678 and y = 1234  so n = 4 . See also Figure 1.1. The algorithm ﬁrst computes the “partial product” of the ﬁrst number and the last digit of the second number 5678 · 4 = 22712. Computing this partial product boils down to multiplying each of the digits of the ﬁrst number by 4, and adding in “carries” as necessary.3 When computing the next partial product  5678 · 3 = 17034 , we do the same thing, shifting the result one digit to the left, eﬀectively adding a “0” at the end. And so on for the ﬁnal two partial products. The ﬁnal step is to add up all the partial products.  Back in third grade, you probably accepted that this algorithm is correct, meaning that no matter what numbers x and y you start with, provided that all intermediate computations are done properly, it eventually terminates with the product x· y of the two input numbers. 2If you want to multiply numbers with diﬀerent lengths  like 1234 and 56 , a simple hack is to just add some zeros to the beginning of the smaller number  for example, treat 56 as 0056 . Alternatively, the algorithms we’ll discuss can be modiﬁed to accommodate numbers with diﬀerent lengths.  38 · 4 = 32, carry the 3, 7 · 4 = 28, plus 3 is 31, carry the 3, . . .   1.2  Integer Multiplication  5  5678  × 1234  22712  17034  11356  5678  7006652   n rows    cid:1 2n operations    per row    Figure 1.1: The grade-school integer multiplication algorithm.  That is, you’re never going to get a wrong answer, and the algorithm can’t loop forever.  1.2.4 Analysis of the Number of Operations  Your third-grade teacher might not have discussed the number of primitive operations needed to carry out this procedure to its con- clusion. To compute the ﬁrst partial product, we multiplied 4 times each of the digits 5, 6, 7, 8 of the ﬁrst number. This is 4 primitive operations. We also performed a few additions because of the carries. In general, computing a partial product involves n multiplications  one per digit  and at most n additions  at most one per digit , for a total of at most 2n primitive operations. There’s nothing special about the ﬁrst partial product: every partial product requires at most 2n operations. Since there are n partial products—one per digit of the second number—computing all of them requires at most n · 2n = 2n2 primitive operations. We still have to add them all up to compute the ﬁnal answer, but this takes a comparable number of operations  at most another 2n2 . Summarizing:  total number of operations  constant {z }    =4  ·n2.  Thinking about how the amount of work the algorithm performs scales as the input numbers grow bigger and bigger, we see that the work required grows quadratically with the number of digits. If you double the length of the input numbers, the work required jumps by   6  Introduction  a factor of 4. Quadruple their length and it jumps by a factor of 16, and so on.  1.2.5 Can We Do Better?  Depending on what type of third-grader you were, you might well have accepted this procedure as the unique or at least optimal way to multiply two numbers. If you want to be a serious algorithm designer, you’ll need to grow out of that kind of obedient timidity. The classic algorithms book by Aho, Hopcroft, and Ullman, after iterating through a number of algorithm design paradigms, has this to say:  “Perhaps the most important principle for the good algorithm designer is to refuse to be content.”4  Or as I like to put it, every algorithm designer should adopt the mantra:  Can we do better?  This question is particularly apropos when you’re faced with a naive or straightforward solution to a computational problem. In the third grade, you might not have asked if one could do better than the straightforward integer multiplication algorithm. Now is the time to ask, and answer, this question.  1.3 Karatsuba Multiplication  The algorithm design space is surprisingly rich, and there are certainly other interesting methods of multiplying two integers beyond what you learned in the third grade. This section describes a method called Karatsuba multiplication.5  4Alfred V. Aho, John E. Hopcroft, and Jeﬀrey D. Ullman, The Design and  Analysis of Computer Algorithms, Addison-Wesley, 1974, page 70.  5Discovered in 1960 by Anatoly Karatsuba, who at the time was a 23-year-old  student.   1.3 Karatsuba Multiplication  7  1.3.1 A Concrete Example  To get a feel for Karatsuba multiplication, let’s re-use our previous example with x = 5678 and y = 1234. We’re going to execute a sequence of steps, quite diﬀerent from the grade-school algorithm, culminating in the product x · y. The sequence of steps should strike you as very mysterious, like pulling a rabbit out of a hat; later in the section we’ll explain exactly what Karatsuba multiplication is and why it works. The key point to appreciate now is that there’s a dazzling array of options for solving computational problems like integer multiplication.  First, to regard the ﬁrst and second halves of x as numbers in their own right, we give them the names a and b  so a = 56 and b = 78 . Similarly, c and d denote 12 and 34, respectively  Figure 1.2 .  5678  × 1234   a   c   b   d   Figure 1.2: Thinking of 4-digit numbers as pairs of double-digit numbers.  Next we’ll perform a sequence of operations that involve only the double-digit numbers a, b, c, and d, and ﬁnally collect all the terms together in a magical way that results in the product of x and y.  Step 1: Compute a · c = 56 · 12, which is 672  as you’re welcome to check .  Step 2: Compute b · d = 78 · 34 = 2652.  The next two steps are still more inscrutable.  Step 3: Compute  a + b  ·  c + d  = 134 · 46 = 6164. Step 4: Subtract the results of the ﬁrst two steps from the result of the third step: 6164   672   2652 = 2840.   8  Introduction  Finally, we add up the results of steps 1, 2, and 4, but only after adding four trailing zeroes to the answer in step 1 and 2 trailing zeroes to the answer in step 4. Step 6720000 + 284000 + 2652 = 70066552.  · 2840 + 2652  · 672 + 102  5: Compute  104  =  This is exactly the same  correct  result computed by the grade-  school algorithm in Section 1.2!  You should not have any intuition about what just happened. Rather, I hope that you feel some mixture of baﬄement and intrigue, and appreciate the fact that there seem to be fundamentally diﬀerent algorithms for multiplying integers than the one you learned as a kid. Once you realize how rich the space of algorithms is, you have to wonder: can we do better than the third-grade algorithm? Does the algorithm above already do better?  1.3.2 A Recursive Algorithm  Before tackling full-blown Karatsuba multiplication, let’s explore a simpler recursive approach to integer multiplication.6 A recursive algorithm for integer multiplication presumably involves multiplica- tions of numbers with fewer digits  like 12, 34, 56, and 78 in the computation above .  In general, a number x with an even number n of digits can be expressed in terms of two n 2-digit numbers, its ﬁrst half and second half a and b:  Similarly, we can write  x = 10n 2 · a + b.  y = 10n 2 · c + d.  To compute the product of x and y, let’s use the two expressions above and multiply out:  x · y =  10n 2 · a + b  ·  10n 2 · c + d   = 10n ·  a · c  + 10n 2 ·  a · d + b · c  + b · d.   1.1   6I’m assuming you’ve heard of recursion as part of your programming back- ground. A recursive procedure is one that invokes itself as a subroutine with a smaller input, until a base case is reached.   1.3 Karatsuba Multiplication  9  Note that all of the multiplications in  1.1  are either between pairs of n 2-digit numbers or involve a power of 10.7  The expression  1.1  suggests a recursive approach to multiplying two numbers. To compute the product x · y, we compute the expres- sion  1.1 . The four relevant products  a · c, a · d, b · c, and b · d  all concern numbers with fewer than n digits, so we can compute each of them recursively. Once our four recursive calls come back to us with their answers, we can compute the expression  1.1  in the obvious way: tack on n trailing zeroes to a· c, add a· d and b· c  using grade-school addition  and tack on n 2 trailing zeroes to the result, and ﬁnally add these two expressions to b · d.8 We summarize this algorithm, which we’ll call RecIntMult, in the following pseudocode.9  RecIntMult  Input: two n-digit positive integers x and y. Output: the product x · y. Assumption: n is a power of 2.  if n = 1 then  else     base case     recursive case  compute x · y in one step and return the result a, b := ﬁrst and second halves of x c, d := ﬁrst and second halves of y recursively compute ac := a · c, ad := a · d, bc := b · c, and bd := b · d compute 10n · ac + 10n 2 ·  ad + bc  + bd using grade-school addition and return the result  Is the RecIntMult algorithm faster or slower than the grade-school 7For simplicity, we are assuming that n is a power of 2. A simple hack for enforcing this assumption is to add an appropriate number of leading zeroes to x and y, which at most doubles their lengths. Alternatively, when n is odd, it’s also ﬁne to break x and y into two numbers with almost equal lengths.  8Recursive algorithms also need one or more base cases, so that they don’t keep calling themselves until the rest of time. Here, the base case is: if x and y are 1-digit numbers, just multiply them in one primitive operation and return the result.  9In pseudocode, we use “=” to denote an equality test, and “:=” to denote a  variable assignment.   10  Introduction  algorithm? You shouldn’t necessarily have any intuition about this question, and the answer will have to wait until Chapter 4.  1.3.3 Karatsuba Multiplication  Karatsuba multiplication is an optimized version of the RecIntMult algorithm. We again start from the expansion  1.1  of x · y in terms of a, b, c, and d. The RecIntMult algorithm uses four recursive calls, one for each of the products in  1.1  between n 2-digit numbers. But we don’t really care about a · d or b · c, except inasmuch as we care about their sum a · d + b · c. With only three quantities that we care about—a · c, a · d + b · c, and b · d—can we get away with only three recursive calls? To see that we can, ﬁrst use two recursive calls to compute a · c  and b · d, as before. Step 1: Recursively compute a · c. Step 2: Recursively compute b · d.  Instead of recursively computing a·d or b·c, we recursively compute  the product of a + b and c + d.10 Step 3: Compute a + b and c + d  using grade-school addition , and recursively compute  a + b  ·  c + d .  The key trick in Karatsuba multiplication goes back to the early 19th-century mathematician Carl Friedrich Gauss, who was thinking about multiplying complex numbers. Subtracting the results of the ﬁrst two steps from the result of the third step gives exactly what we want, the middle coeﬃcient in  1.1  of a · d + b · c:   a · c   b · d = a · d + b · c.   a + b  ·  c + d  =a·c+a·d+b·c+b·d  {z    }  Step 4: Subtract the results of the ﬁrst two steps from the result of the third step to obtain a · d + b · c.  The ﬁnal step computes  1.1 , as in the RecIntMult algorithm. 10The numbers a + b and c + d might have as many as  n 2  + 1 digits, but  the algorithm still works ﬁne.   1.3 Karatsuba Multiplication  11  Step 5: Compute  1.1  by adding up the results of steps 1, 2, and 4, after adding 10n trailing zeroes to the answer in step 1 and 10n 2 trailing zeroes to the answer in step 4.  Karatsuba  Input: two n-digit positive integers x and y. Output: the product x · y. Assumption: n is a power of 2.  if n = 1 then  else     base case     recursive case  compute x · y in one step and return the result a, b := ﬁrst and second halves of x c, d := ﬁrst and second halves of y compute p := a + b and q := c + d using grade-school addition recursively compute ac := a · c, bd := b · d, and pq := p · q compute adbc := pq   ac   bd using grade-school addition compute 10n · ac + 10n 2 · adbc + bd using grade-school addition and return the result  Thus Karatsuba multiplication makes only three recursive calls! Sav- ing a recursive call should save on the overall running time, but by how much? Is the Karatsuba algorithm faster than the grade-school multiplication algorithm? The answer is far from obvious, but it is an easy application of the tools you’ll acquire in Chapter 4 for analyzing the running time of such “divide-and-conquer” algorithms.  On Pseudocode  This book explains algorithms using a mixture of high-level pseudocode and English  as in this section . I’m assuming that you have the skills to translate such high-level descriptions into working code in your favorite programming language. Several other books   12  Introduction  and resources on the Web oﬀer concrete implementa- tions of various algorithms in speciﬁc programming languages.  The ﬁrst beneﬁt of emphasizing high-level descrip- tions over language-speciﬁc implementations is ﬂexi- bility: while I assume familiarity with some program- ming language, I don’t care which one. Second, this approach promotes the understanding of algorithms at a deep and conceptual level, unencumbered by low- level details. Seasoned programmers and computer scientists generally think and communicate about al- gorithms at a similarly high level.  Still, there is no substitute for the detailed under- standing of an algorithm that comes from providing your own working implementation of it. I strongly encourage you to implement as many of the algo- rithms in this book as you have time for.  It’s also a great excuse to pick up a new programming language!  For guidance, see the end-of-chapter Programming Problems and supporting test cases.  1.4  MergeSort: The Algorithm  This section provides our ﬁrst taste of analyzing the running time of a non-trivial algorithm—the famous MergeSort algorithm.  1.4.1 Motivation  MergeSort is a relatively ancient algorithm, and was certainly known to John von Neumann as early as 1945. Why begin a modern course on algorithms with such an old example?  Oldie but a goodie. Despite being over 70 years old, MergeSort is still one of the methods of choice for sorting. It’s used all the time in practice, and is the standard sorting algorithm in a number of programming libraries.   1.4  MergeSort: The Algorithm  13  Canonical divide-and-conquer algorithm. The “divide-and- conquer” algorithm design paradigm is a general approach to solving problems, with applications in many diﬀerent domains. The basic idea is to break your problem into smaller subproblems, solve the subproblems recursively, and ﬁnally combine the solutions to the subproblems into one for the original problem. MergeSort is an ideal introduction to the divide-and-conquer paradigm, the beneﬁts it oﬀers, and the analysis challenges it presents.  Calibrate your preparation. Our MergeSort discussion will give you a good indication of whether your current skill set is a good match for this book. My assumption is that you have the programming and mathematical backgrounds to  with some work  translate the high- level idea of MergeSort into a working program in your favorite programming language and to follow our running time analysis of the algorithm. If this and the next section make sense, then you are in good shape for the rest of the book.  Motivates guiding principles for algorithm analysis. Our run- ning time analysis of MergeSort exposes a number of more general guiding principles, such as the quest for running time bounds that hold for every input of a given size, and the importance of the rate of growth of an algorithm’s running time  as a function of the input size .  Warm-up for the master method. We’ll analyze MergeSort us- ing the “recursion tree method,” which is a way of tallying up the operations performed by a recursive algorithm. Chapter 4 builds on these ideas and culminates with the “master method,” a powerful and easy-to-use tool for bounding the running time of many diﬀer- ent divide-and-conquer algorithms, including the RecIntMult and Karatsuba algorithms of Section 1.3.  1.4.2 Sorting  You probably already know the sorting problem and some algorithms that solve it, but just so we’re all on the same page:   14  Introduction  Problem: Sorting  Input: An array of n numbers, in arbitrary order.  Output: An array of the same numbers, sorted from small- est to largest.  For example, given the input array  5  4  1  8  7  2  6  3   the desired output array is  1  2  3  4  5  6  7  8   In the example above, the eight numbers in the input array are distinct. Sorting isn’t really any harder when there are duplicates, and it can even be easier. But to keep the discussion as simple as possible, let’s assume—among friends—that the numbers in the input array are always distinct. I strongly encourage you to think about how our sorting algorithms need to be modiﬁed  if at all  to handle duplicates.11  If you don’t care about optimizing the running time, it’s not too diﬃcult to come up with a correct sorting algorithm. Perhaps the simplest approach is to ﬁrst scan through the input array to identify the minimum element and copy it over to the ﬁrst element of the output array; then do another scan to identify and copy over the second-smallest element; and so on. This algorithm is called SelectionSort. You may have heard of InsertionSort, which can be viewed as a slicker implementation of the same idea of iteratively growing a preﬁx of the sorted output array. You might also know BubbleSort, in which you identify adjacent pairs of elements that 11In practice, there is often data  called the value  associated with each number  which is called the key . For example, you might want to sort employee records  with the name, salary, etc. , using social security numbers as keys. We focus on sorting the keys, with the understanding that each key retains its associated data.   1.4  MergeSort: The Algorithm  15  are out of order, and perform repeated swaps until the entire array is sorted. All of these algorithms have quadratic running times, meaning that the number of operations performed on arrays of length n scales with n2, the square of the input length. Can we do better? By using the divide-and-conquer paradigm, the MergeSort algorithm improves dramatically over these more straightforward sorting algorithms.12  1.4.3 An Example  The easiest way to understand MergeSort is through a picture of a concrete example  Figure 1.3 . We’ll use the input array from Section 1.4.2.  5  4  1  8  7  2  6  3   divide   5  4  1  8   7  2  6  3   .  .  .  .    recursive calls   .  .  .  .    1  4  5  8   2  3  6  7   merge   1  2  3  4  5  6  7  8   Figure 1.3: A bird’s-eye view of MergeSort on a concrete example.  As a recursive divide-and-conquer algorithm, MergeSort calls itself on smaller arrays. The simplest way to decompose a sorting problem into smaller sorting problems is to break the input array in half. The ﬁrst and second halves are each sorted recursively. For example, in  12While generally dominated by MergeSort, InsertionSort is still useful in  practice in certain cases, especially for small input sizes.   16  Introduction  Figure 1.3, the ﬁrst and second halves of the input array are {5, 4, 1, 8} and {7, 2, 6, 3}. By the magic of recursion  or induction, if you prefer , the ﬁrst recursive call correctly sorts the ﬁrst half, returning the array {1, 4, 5, 8}. The second recursive call returns the array {2, 3, 6, 7}. The ﬁnal “merge” step combines these two sorted arrays of length 4 into a single sorted array of all 8 numbers. Details of this step are given below, but the idea is to walk indices down each of the sorted subarrays, populating the output array from left to right in sorted order.  1.4.4 Pseudocode  The picture in Figure 1.3 suggests the following pseudocode, with two recursive calls and a merge step, for the general problem. As usual, our description cannot necessarily be translated line by line into working code  though it’s pretty close .  MergeSort  Input: array A of n distinct integers. Output: array with the same integers, sorted from smallest to largest.     ignoring base cases C := recursively sort ﬁrst half of A D := recursively sort second half of A return Merge  C,D   There are several omissions from the pseudocode that deserve comment. As a recursive algorithm, there should also be one or more base cases, where there is no further recursion and the answer is returned directly. So if the input array A contains only 0 or 1 elements, MergeSort returns it  it is already sorted . The pseudocode does not detail what “ﬁrst half” and “second half” mean when n is odd, but the obvious interpretation  with one “half” having one more element than the other  works ﬁne. Finally, the pseudocode ignores the implementation details of how to actually pass the two subarrays to their respective recursive calls. These details depend somewhat on the programming language. The point of high-level pseudocode is   1.4  MergeSort: The Algorithm  17  to ignore such details and focus on the concepts that transcend any particular programming language.  1.4.5 The Merge Subroutine  How should we implement the Merge step? At this point, the two recursive calls have done their work and we have in our possession two sorted subarrays C and D of length n 2. The idea is to traverse both the sorted subarrays in order and populate the output array from left to right in sorted order.13  Merge  Input: sorted arrays C and D  length n 2 each . Output: sorted array B  length n . Simplifying assumption: n is even.  if C[i] < D[j] then  1 i := 1 2 j := 1 3 for k := 1 to n do 4 5 6 7 8 9  B[k] := C[i] i := i + 1  B[k] := D[j] j := j + 1  else     populate output array    increment i    D[j] < C[i]  We traverse the output array using the index k, and the sorted subarrays with the indices i and j. All three arrays are traversed from left to right. The for loop in line 3 implements the pass over the output array. In the ﬁrst iteration, the subroutine identiﬁes the minimum element in either C or D and copies it over to the ﬁrst position of the output array B. The minimum element overall is either in C  in which case it’s C[1], since C is sorted  or in D  in which case it’s D[1], since D is sorted . Advancing the appropriate index  i or j   13We number our array entries beginning with 1  rather than 0 , and use the syntax “A[i]” for the ith entry of an array A. These details vary across programming languages.   18  Introduction  eﬀectively removes from further consideration the element just copied, and the process is then repeated to identify the smallest element remaining in C or D  the second-smallest overall . In general, the smallest element not yet copied over to B is either C[i] or D[j]; the subroutine explicitly checks to see which one is smaller and proceeds accordingly. Since every iteration copies over the smallest element still under consideration in C or D, the output array is indeed populated in sorted order.  As usual, our pseudocode is intentionally a bit sloppy, to emphasize the forest over the trees. A full implementation should also keep track of when the traversal of C or D falls oﬀ the end, at which point the remaining elements of the other array are copied into the ﬁnal entries of B  in order . Now is a good time to work through your own implementation of the MergeSort algorithm.  1.5  MergeSort: The Analysis  What’s the running time of the MergeSort algorithm, as a function of the length n of the input array? Is it faster than more straightforward methods of sorting, such as SelectionSort, InsertionSort, and BubbleSort? By “running time,” we mean the number of lines of code executed in a concrete implementation of the algorithm. Think of walking line by line through this implementation using a debugger, one “primitive operation” at a time We’re interested in the number of steps the debugger takes before the program completes.  1.5.1 Running Time of Merge  Analyzing the running time of the MergeSort algorithm is an intim- idating task, as it’s a recursive algorithm that calls itself over and over. So let’s warm up with the simpler task of understanding the number of operations performed by a single invocation of the Merge subroutine when called on two sorted arrays of length ` 2 each. We can do this directly, by inspecting the code in Section 1.4.5  where n corresponds to ` . First, lines 1 and 2 each perform a initialization, and we’ll count this as two operations. Then, we have a for loop that executes a total of ` times. Each iteration of the loop performs a comparison in line 4, an assignment in either line 5 or line 8, and an increment in either line 6 or line 9. The loop index k also needs   1.5  MergeSort: The Analysis  19  to get incremented each loop iteration. This means that 4 primitive operations are performed for each of the ` iterations of the loop.14 Totaling up, we conclude that the Merge subroutine performs at most 4` + 2 operations to merge two sorted arrays of length ` 2 each. Let me abuse our friendship further with a true but sloppy inequality that will make our lives easier: for `   1, 4` + 2  6`. That is, 6` is also a valid upper bound on the number of operations performed by the Merge subroutine.  Lemma 1.1  Running Time of Merge  For every pair of sorted input arrays C, D of length ` 2, the Merge subroutine performs at most 6` operations.  On Lemmas, Theorems, and the Like  In mathematical writing, the most important techni- cal statements are labeled theorems. A lemma is a technical statement that assists with the proof of a theorem  much as Merge assists with the implementa- tion of MergeSort . A corollary is a statement that follows immediately from an already-proved result, such as a special case of a theorem. We use the term proposition for stand-alone technical statements that are not particularly important in their own right.  1.5.2 Running Time of MergeSort  How can we go from the straightforward analysis of the Merge subrou- tine to an analysis of MergeSort, a recursive algorithm that spawns further invocations of itself? Especially terrifying is the rapid prolifera- tion of recursive calls, the number of which is blowing up exponentially with the depth of the recursion. The one thing we have going for us is the fact that every recursive call is passed an input substantially smaller than the one we started with. There’s a tension between two 14One could quibble with the choice of 4. Does comparing the loop index k to its upper bound also count as an additional operation each iteration, for a total of 5? Section 1.6 explains why such diﬀerences in accounting don’t really matter. So let’s agree, among friends, that it’s 4 primitive operations per iteration.   20  Introduction  competing forces: on the one hand, the explosion of diﬀerent sub- problems that need to be solved; and on the other, the ever-shrinking inputs for which these subproblems are responsible. Reconciling these two forces will drive our analysis of MergeSort. In the end, we’ll prove the following concrete and useful upper bound on the number of operations performed by MergeSort  across all its recursive calls .  Theorem 1.2  Running Time of MergeSort  For every input ar- ray of length n   1, the MergeSort algorithm performs at most  operations, where log2 denotes the base-2 logarithm.  6n log2 n + 6n  On Logarithms  Some students are unnecessarily frightened by the appearance of a logarithm, which is actually a very down-to-earth concept. For a positive integer n, log2 n just means the following: type n into a calculator, and count the number of times you need to divide it by 2 before the result is 1 or less.a For example, it takes ﬁve divide-by-twos to bring 32 down to 1, so log2 32 = 5. Ten divide-by-twos bring 1024 down to 1, so log2 1024 = 10. These examples make it intuitively clear that log2 n is much less than n  compare 10 vs. 1024 , especially as n grows large. A plot conﬁrms this intuition  Figure 1.4 .  aTo be pedantic, log2 n is not an integer if n is not a power of 2, and what we have described is really log2 n rounded up to the nearest integer. We can ignore this minor distinction.  Theorem 1.2 is a win for the MergeSort algorithm and showcases the beneﬁts of the divide-and-conquer algorithm design paradigm. We mentioned that the running times of simpler sorting algorithms, like SelectionSort, InsertionSort, and BubbleSort, depend quadrat- ically on the input size n, meaning that the number of operations required scales as a constant times n2. In Theorem 1.2, one of these   1.5  MergeSort: The Analysis  21  f n =n f n =log n    n   f  20  40  35  30  25  15  10  5  0  0  5  10  15  25  30  35  40  20 n  Figure 1.4: The logarithm function grows much more slowly than the identity function. The base of the logarithm is 2; other bases lead to qualitatively similar pictures.  factors of n is replaced by log2 n. As suggested by Figure 1.4, this means that MergeSort typically runs much faster than the simpler sorting algorithms, especially as n grows large.15  1.5.3 Proof of Theorem 1.2  We now do a full running time analysis of MergeSort, thereby substan- tiating the claim that a recursive divide-and-conquer approach results in a faster sorting algorithm than more straightforward methods. For simplicity, we assume that the input array length n is a power of 2. This assumption can be removed with minor additional work.  The plan for proving the running time bound in Theorem 1.2 is to use a recursion tree; see Figure 1.5.16 The idea of the recursion tree method is to write out all the work done by a recursive algorithm in a tree structure, with nodes of the tree corresponding to recursive calls, and the children of a node corresponding to the recursive calls made  15See Section 1.6.3 for further discussion of this point. 16For some reason, computer scientists seem to think that trees grow downward.   22  Introduction  by that node. This tree structure provides us with a principled way to tally up all the work done by MergeSort across all its recursive calls.  level 0    outermost call     ﬁrst recursive   level 1   calls    level 2   .  .  .  .   .  .  .  .   entire input   left half   right half   .  .  .  .   .  .  .  .   .  .  .  .   leaves  single-element arrays    Figure 1.5: A recursion tree for MergeSort. Nodes correspond to recursive calls. Level 0 corresponds to the outermost call to MergeSort, level 1 to its recursive calls, and so on.  The root of the recursion tree corresponds to the outermost call to MergeSort, where the input is the original input array. We’ll call this level 0 of the tree. Since each invocation of MergeSort spawns two recursive calls, the tree will be binary  that is, with two children per node . Level 1 of the tree has two nodes, corresponding to the two recursive calls made by the outermost call, one for the left half of the input array and one for the right half. Each of the level-1 recursive calls will itself make two recursive calls, each operating on a particular quarter of the original input array. This process continues until eventually the recursion bottoms out with arrays of size 0 or 1  the base cases .  Quiz 1.1  Roughly how many levels does this recursion tree have, as a function of the length n of the input array?   1.5  MergeSort: The Analysis  23  a  A constant number  independent of n   b  log2 n c  pn d  n   See Section 1.5.4 for the solution and discussion.   This recursion tree suggests a particularly convenient way to account for the work done by MergeSort, which is level by level. To implement this idea, we need to understand two things: the number of distinct subproblems at a given recursion level j, and the length of the input to each of these subproblems.  Quiz 1.2  What is the pattern? Fill in the blanks in the following statement: at each level j = 0, 1, 2, . . . of the recursion tree, there are [blank] subproblems, each operating on a subarray of length [blank].  a  2j and 2j, respectively  b  n 2j and n 2j, respectively  c  2j and n 2j, respectively  d  n 2j and 2j, respectively   See Section 1.5.4 for the solution and discussion.   Let’s now put this pattern to use and tally all the operations that MergeSort performs. We proceed level by level, so ﬁx a level j of the recursion tree. How much work is done by the level-j recursive calls, not counting the work done by their recursive calls at later levels? Inspecting the MergeSort code, we see that it does only three things: make two recursive calls and invoke the Merge subroutine on the results. Thus ignoring the work done by later recursive calls, the work done by a level-j subproblem is just the work done by Merge.   24  Introduction  This we already understand from Lemma 1.1: at most 6` operations, where ` is the length of the input array to this subproblem.  To put everything together, we can express the total work done     of level-j subproblems  by level-j recursive calls  not counting later recursive calls  as ⇥ work per level-j subproblem }   Using the solution to Quiz 1.2, we know that the ﬁrst term equals 2j, and the input length to each such subproblem is n 2j. Taking ` = n 2j, Lemma 1.1 implies that each level-j subproblem performs at most 6n 2j operations. We conclude that at most  {z  {z  =6n 2j  }  =2j  .  2j ·  6n 2j = 6n  operations are performed across all the recursive calls at the jth recursion level.  Remarkably, our bound on the work done at a given level j is independent of j! That is, each level of the recursion tree contributes the same number of operations to the analysis. The reason for this is a perfect equilibrium between two competing forces—the number of subproblems doubles every level, while the amount of work performed per subproblem halves every level.  We’re interested in the number of operations performed across all levels of the recursion tree. By the solution to Quiz 1.1, the recursion tree has log2 n + 1 levels  levels 0 through log2 n, inclusive . Using our bound of 6n operations per level, we can bound the total number of operations by  number of levels  =log2 n+1  {z    ⇥ work per level  }  {z  6n  }  matching the bound claimed in Theorem 1.2. QE D17   6n log2 n + 6n,  17“Q.e.d.” is an abbreviation for quod erat demonstrandum, and means “that which was to be demonstrated.” In mathematical writing, it is used at the end of a proof to mark its completion.   1.5  MergeSort: The Analysis  25  On Primitive Operations  We measure the running time of an algorithm like MergeSort in terms of the number of “primitive oper- ations” performed. Intuitively, a primitive operation performs a simple task  like adding, comparing, or copying  while touching a small number of simple variables  like 32-bit integers .18 Warning: in some high-level programming languages, a single line of code can mask a large number of primitive operations. For example, a line of code that touches every element of a long array translates to a number of primitive operations proportional to the array’s length.  1.5.4 Solutions to Quizzes 1.1–1.2  Solution to Quiz 1.1  Correct answer:  b . The correct answer is ⇡ log2 n. The reason is that the input size decreases by a factor of two with each level of the recursion. If the input length in level 0 is n, the level-1 recursive calls operate on arrays of length n 2, the level-2 recursive calls on arrays of length n 4, and so on. The recursion bottoms out at the base cases, with input arrays of length at most one, where there are no more recursive calls. How many levels of recursion are required? The number of times you need to divide n by 2 before obtaining a number that is at most 1. For n a power of 2, this is precisely the deﬁnition of log2 n.  More generally, it is log2 n rounded up to the nearest integer.   Solution to Quiz 1.2  Correct answer:  c . The correct answer is that there are 2j distinct subproblems at recursion level j, and each operates on a subarray of length n 2j. For the ﬁrst point, start with level 0, where there is one recursive call. There are two recursive calls as level 1, and more  18More precise deﬁnitions are possible, but we won’t need them.   26  Introduction  generally, since MergeSort calls itself twice, the number of recursive calls at each level is double the number at the previous level. This successive doubling implies that there are 2j subproblems at each level j of the recursion tree. Similarly, since every recursive call gets only half the input of the previous one, after j levels of recursion the input length has dropped to n 2j. Or for a diﬀerent argument, we already know that there are 2j subproblems at level j, and the original input array  of length n  is equally partitioned among these—exactly n 2j elements per subproblem.  1.6 Guiding Principles for the Analysis of  Algorithms  With our ﬁrst algorithm analysis under our belt  MergeSort, in The- orem 1.2 , it’s the right time to take a step back and make explicit three assumptions that informed our running time analysis and in- terpretation of it. We will adopt these three assumptions as guiding principles for how to reason about algorithms, and use them to deﬁne what we actually mean by a “fast algorithm.”  The goal of these principles is to identify a sweet spot for the analysis of algorithms, one that balances accuracy with tractability. Exact running time analysis is possible only for the simplest algorithms; more generally, compromises are required. On the other hand, we don’t want to throw out the baby with the bathwater—we still want our mathematical analysis to have predictive power about whether an algorithm will be fast or slow in practice. Once we ﬁnd the right balance, we’ll be able to prove good running time guarantees for dozens of fundamental algorithms, and these guarantees will paint an accurate picture of which algorithms tend to run faster than others.  1.6.1 Principle 1: Worst-Case Analysis  Our running time bound of 6n log2 n + 6n in Theorem 1.2 holds for every input array of length n, no matter what its contents. We made no assumptions about the input beyond its length n. Hypothetically, if there was an adversary whose sole purpose in life was to concoct a malevolent input designed to make MergeSort run as slow as possible, the 6n log2 n + 6n bound would still apply. This type of analysis is   1.6 Guiding Principles for the Analysis of Algorithms  27  called worst-case analysis, since it gives a running time bound that is valid even for the “worst” inputs.  Given how naturally worst-case analysis fell out of our analysis of MergeSort, you might well wonder what else we could do. One alternative approach is “average-case analysis,” which analyzes the average running time of an algorithm under some assumption about the relative frequencies of diﬀerent inputs. For example, in the sorting problem, we could assume that all input arrays are equally likely and then study the average running time of diﬀerent sorting algorithms. A second alternative is to look only at the performance of an algorithm on a small collection of “benchmark instances” that are thought to be representative of “typical” or “real-world” inputs.  Both average-case analysis and the analysis of benchmark instances can be useful when you have domain knowledge about your problem, and some understanding of which inputs are more representative than others. Worst-case analysis, in which you make absolutely no assumptions about the input, is particularly appropriate for general- purpose subroutines designed to work well across a range of application domains. To be useful to as many people as possible, these books focus on such general-purpose subroutines and, accordingly, use worst-case analysis to judge algorithm performance.  As a bonus, worst-case analysis is usually much more tractable mathematically than its alternatives. This is one reason why worst- case analysis naturally popped out of our MergeSort analysis, even though we had no a priori focus on worst-case inputs.  1.6.2 Principle 2: Big-Picture Analysis  The second and third guiding principles are closely related. Let’s call the second one big-picture analysis  warning: this is not a stan- dard term . This principle states that we should not worry unduly about small constant factors or lower-order terms in running time bounds. We’ve already seen this philosophy at work in our analysis of MergeSort: when analyzing the running time of the Merge subroutine  Lemma 1.1 , we ﬁrst proved an upper bound of 4` + 2 on the number of operations  where ` is the length of the output array  and then settled for the simpler upper bound of 6`, even though it suﬀers from a larger constant factor. How do we justify being so fast and loose with constant factors?   28  Introduction  Mathematical tractability. The ﬁrst reason for big-picture anal- ysis is that it’s way easier mathematically than the alternative of pinning down precise constant factors or lower-order terms. This point was already evident in our analysis of the running time of MergeSort. Constants depend on environment-speciﬁc factors. The sec- ond justiﬁcation is less obvious but extremely important. At the level of granularity we’ll use to describe algorithms, as with the MergeSort algorithm, it would be totally misguided to obsess over exactly what the constant factors are. For example, during our analysis of the Merge subroutine, there was ambiguity about exactly how many “primitive operations” are performed each loop iteration  4, 5, or something else? . Thus diﬀerent interpretations of the same pseudocode can lead to diﬀerent constant factors. The ambiguity only increases once pseudocode gets translated into a concrete implementation in some high-level programming language, and then translated further into machine code—the constant factors will inevitably depend on the pro- gramming language used, the speciﬁc implementation, and the details of the compiler and processor. Our goal is to focus on properties of algorithms that transcend the details of the programming language and machine architecture, and these properties should be independent of small constant-factor changes in a running time bound. Lose little predictive power. The third justiﬁcation is simply that we’re going to be able to get away with it. You might be concerned that ignoring constant factors would lead us astray, tricking us into thinking that an algorithm is fast when it is actually slow in practice, or vice versa. Happily, this won’t happen for the algorithms discussed in these books.19 Even though we won’t be keeping track of lower-order terms and constant factors, the qualitative predictions of our mathematical analysis will be highly accurate—when analysis suggests that an algorithm should be fast, it will in fact be fast in practice, and conversely. So while big-picture analysis does discard some information, it preserves what we really care about: accurate guidance about which algorithms tend to be faster than others.20  19With one possible exception, the deterministic linear-time selection algorithm  in the optional Section 6.3.  20It’s still useful to have a general sense of the relevant constant factors, however. For example, in the highly tuned versions of MergeSort that you’ll   1.6 Guiding Principles for the Analysis of Algorithms  29  1.6.3 Principle 3: Asymptotic Analysis  Our third and ﬁnal guiding principle is to use asymptotic analysis and focus on the rate of growth of an algorithm’s running time, as the input size n grows large. This bias toward large inputs was already evident when we interpreted our running time bound for MergeSort  Theorem 1.2 , of 6n log2 n + 6n operations. We then cavalierly declared that MergeSort is “better than” simpler sorting methods with running time quadratic in the input size, such as InsertionSort. But is this really true?  For concreteness, suppose we have a sorting algorithm that per- 2 n2 operations when sorting an array of length n, and  forms at most 1 consider the comparison  6n log2 n + 6n vs.  n2.  1 2  Looking at the behavior of these two functions in Figure 1.6 a , we see that 1 2 n2 is the smaller expression when n is small  at most 90 or so , while 6n log2 n + 6n is smaller for all larger n. So when we say that MergeSort is faster than simpler sorting methods, what we really mean is that it is faster on suﬃciently large instances.  Why should we care more about large instances than small ones? Because large problems are the only ones that require algorithmic ingenuity. Almost any sorting method you can think of would sort an array of length 1000 instantaneously on a modern computer—there’s no need to learn about divide-and-conquer algorithms.  Given that computers are constantly getting faster, you might wonder if all computational problems will eventually become trivial to solve. In fact, the faster computers get, the more relevant asymptotic analysis becomes. Our computational ambitions have always grown with our computational power, so as time goes on, we will consider larger and larger problem sizes. And the gulf in performance between algorithms with diﬀerent asymptotic running times only becomes wider as inputs grow larger. For example, Figure 1.6 b  shows the diﬀerence between the functions 6n log2 n + 6n and 1 2 n2 for larger  but still modest  values of n, and by the time n = 1500 there is roughly a  ﬁnd in many programming libraries, the algorithm switches from MergeSort over to InsertionSort  for its better constant factor  once the input array length becomes small  for example, at most seven elements .   30  12000  10000  8000    n   f  6000  4000  2000  0  0  Introduction  105  12  f n =n2 2 f n =6n log n + 6n  f n =n2 2 f n =6n log n + 6n    n   f  10  8  6  4  2  50  n  100   a  Small values of n  150  0  0  500  1000  1500  n   b  Medium values of n  Figure 1.6: The function 1 2 n2 grows much more quickly than 6n log2 n+6n as n grows large. The scales of the x- and y-axes in  b  are one and two orders of magnitude, respectively, bigger than those in  a .  factor-10 diﬀerence between them. If we scaled n up by another factor of 10, or 100, or 1000 to start reaching interesting problem sizes, the diﬀerence between the two functions would be huge.  For a diﬀerent way to think about asymptotic analysis, suppose you have a ﬁxed time budget, like an hour or a day. How does the solvable problem size scale with additional computing power? With an algorithm that runs in time proportional to the input size, a four-fold increase in computing power lets you solve problems four times as large as before. With an algorithm that runs in time proportional to the square of the input size, you would be able to solve problems that are only twice as large as before.  1.6.4 What Is a “Fast” Algorithm?  Our three guiding principles lead us to the following deﬁnition of a “fast algorithm:”  A “fast algorithm” is an algorithm whose worst-case running time grows slowly with the input size.  Our ﬁrst guiding principle, that we want running time guarantees that do not assume any domain knowledge, is the reason why we focus on the worst-case running time of an algorithm. Our second and third guiding principles, that constant factors are language- and   1.6 Guiding Principles for the Analysis of Algorithms  31  machine-dependent and that large problems are the interesting ones, are the reasons why we focus on the rate of growth of the running time of an algorithm.  What do we mean that the running time of an algorithm “grows slowly?” For almost all of problems we’ll discuss, the holy grail is a linear-time algorithm, meaning an algorithm with running time proportional to the input size. Linear time is even better than our bound on the running time of MergeSort, which is proportional to n log n and hence modestly super-linear. We will succeed in designing linear-time algorithms for some problems but not for others. In any case, it is the best-case scenario to which we will aspire.  For-Free Primitives  We can think of an algorithm with linear or near- linear running time as a primitive that we can use essentially “for free,” since the amount of computation used is barely more than what is required just to read the input. Sorting is a canonical example of a for-free primitive, and we will also learn several others. When you have a primitive relevant for your problem that is so blazingly fast, why not use it? For example, you can always sort your data in a preprocessing step, even if you’re not quite sure how it’s going to be helpful later. One of the goals of this book series is to stock your algorithmic toolbox with as many for-free primitives as possible, ready to be applied at will.  The Upshot  P An algorithm is a set of well-deﬁned rules for  solving some computational problem.  P The number of primitive operations performed by the algorithm you learned in grade school to multiply two n-digit integers scales as a quadratic function of the number n.   32  Introduction  P Karatsuba multiplication is a recursive algo- rithm for integer multiplication, and it uses Gauss’s trick to save one recursive call over a more straightforward recursive algorithm.  P Seasoned programmers and computer scientists generally think and communicate about algo- rithms using high-level descriptions rather than detailed implementations.  P The MergeSort algorithm is a “divide-and- conquer” algorithm that splits the input array into two halves, recursively sorts each half, and combines the results using the Merge subrou- tine.  P Ignoring constant  factors and lower-order terms, the number of operations performed by MergeSort to sort n elements grows like the function n log2 n. The analysis uses a recursion tree to conveniently organize the work done by all the recursive calls.  P Because the function log2 n grows slowly with n, MergeSort is typically faster than simpler sort- ing algorithms, which all require a quadratic number of operations. For large n, the improve- ment is dramatic.  P Three guiding principles for the analysis of algo- rithms are:  i  worst-case analysis, to promote general-purpose algorithms that work well with no assumptions about the input;  ii  big-picture analysis, which balances predictive power with mathematical tractability by ignoring constant factors and lower-order terms; and  iii  asymp- totic analysis, which is a bias toward large in- puts, which are the inputs that require algorith- mic ingenuity.   Problems  33  P A “fast algorithm” is an algorithm whose worst- case running time grows slowly with the input size.  P A “for-free primitive” is an algorithm that runs in linear or near-linear time, barely more than what is required to read the input.  Test Your Understanding  Problem 1.1 Suppose we run MergeSort on the following input array:  5  3  8  9  1  7  0  2  6  4   Fast forward to the moment after the two outermost recursive calls complete, but before the ﬁnal Merge step. Thinking of the two 5-element output arrays of the recursive calls as a glued-together 10-element array, which number is in the 7th position?  Problem 1.2 Consider the following modiﬁcation to the MergeSort algorithm: divide the input array into thirds  rather than halves , recursively sort each third, and ﬁnally combine the results using a three-way Merge subroutine. What is the running time of this algorithm as a function of the length n of the input array, ignoring constant factors and lower-order terms? [Hint: Note that the Merge subroutine can still be implemented so that the number of operations is only linear in the sum of the input array lengths.]  a  n  b  n log n  c  n log n 2  d  n2 log n  Problem 1.3 Suppose you are given k sorted arrays, each with n elements, and you want to combine them into a single array of kn   34  Introduction  elements. One approach is to use the Merge subroutine from Sec- tion 1.4.5 repeatedly, ﬁrst merging the ﬁrst two arrays, then merging the result with the third array, then with the fourth array, and so on until you merge in the kth and ﬁnal input array. What is the running time taken by this successive merging algorithm, as a function of k and n, ignoring constant factors and lower-order terms?  Problem 1.4 Consider again the problem of merging k sorted length- n arrays into a single sorted length-kn array. Consider the algorithm that ﬁrst divides the k arrays into k 2 pairs of arrays, and uses the Merge subroutine to combine each pair, resulting in k 2 sorted length- 2n arrays. The algorithm repeats this step until there is only one length-kn sorted array. What is the running time of this procedure, as a function of k and n, ignoring constant factors and lower-order terms?  a  n log k  b  nk  c  nk log k  d  nk log n  e  nk2  f  n2k  a  n log k  b  nk  c  nk log k  d  nk log n  e  nk2  f  n2k   Problems  35  Challenge Problems  Problem 1.5 You are given as input an unsorted array of n distinct numbers, where n is a power of 2. Give an algorithm that identiﬁes the second-largest number in the array, and that uses at most n+log2 n 2 comparisons. [Hint: What information do you have left over after computing the largest number?]  Programming Problems  Problem 1.6 Implement Karatsuba’s integer multiplication algo- rithm in your favorite programming language.21 To get the most out of this problem, your program should invoke the language’s multipli- cation operator only on pairs of single-digit numbers.  For a concrete challenge, what’s the product of the following two  64-digit numbers?22  3141592653589793238462643383279502884197169399375105820974944592  2718281828459045235360287471352662497757247093699959574966967627  21Food for thought: does it make your life easier if the number of digits of each  integer is a power of 2?  22If you need help or want to compare notes with other readers, visit the  discussion forums at www.algorithmsilluminated.org.   Chapter 2  Asymptotic Notation  This chapter develops the mathematical formalism that encodes our guiding principles for the analysis of algorithms  Section 1.6 . The goal is to identify a sweet spot of granularity for reasoning about algorithms—we want to suppress second-order details like constant factors and lower-order terms, and focus on how the running time of an algorithm scales as the input size grows. This is done formally through big-O notation and its relatives—concepts that belong in the vocabulary of every serious programmer and computer scientist.  2.1 The Gist  Before getting into the mathematical formalism of asymptotic notation, let’s make sure the topic is well motivated, that you have a strong sense of what it’s trying to accomplish, and that you’ve seen a couple of simple and intuitive examples.  2.1.1 Motivation  Asymptotic notation provides the basic vocabulary for discussing the design and analysis of algorithms. It’s important that you know what programmers mean when they say that one piece of code runs in “big-O of n time,” while another runs in “big-O of n-squared time.”  This vocabulary is so ubiquitous because it identiﬁes the right “sweet spot” for reasoning about algorithms. Asymptotic notation is coarse enough to suppress all the details you want to ignore—details that depend on the choice of architecture, the choice of programming language, the choice of compiler, and so on. On the other hand, it’s precise enough to make useful comparisons between diﬀerent high- level algorithmic approaches to solving a problem, especially on larger inputs  the inputs that require algorithmic ingenuity . For example,  36   2.1 The Gist  37  asymptotic analysis helps us diﬀerentiate between better and worse approaches to sorting, better and worse approaches to multiplying two integers, and so on.  2.1.2 The High-Level Idea  If you ask a practicing programmer to explain the point of asymptotic notation, he or she is likely to say something like the following:  Asymptotic Notation in Seven Words  suppress constant factors too system-dependent  and lower-order terms irrelevant for large inputs    {z  }    {z  }  We’ll see that there’s more to asymptotic notation than just these seven words, but ten years from now, if you remember only seven words about it, these are good ones.  When analyzing the running time of an algorithm, why would we want to throw away information like constant factors and lower- order terms? Lower-order terms, by deﬁnition, become increasingly irrelevant as you focus on large inputs, which are the inputs that require algorithmic ingenuity. Meanwhile, the constant factors are generally highly dependent on the details of the environment. If we don’t want to commit to a speciﬁc programming language, architecture, or compiler when analyzing an algorithm, it makes sense to use a formalism that does not focus on constant factors.  For example, remember when we analyzed MergeSort  Sec-  tion 1.4 ? We gave an upper bound on its running time of  6n log2 n + 6n  primitive operations, where n is the length of the input array. The lower-order term here is the 6n, as n grows more slowly than n log2 n, so it will be suppressed in asymptotic notation. The leading constant factor of 6 also gets suppressed, leaving us with the much simpler expression of n log n. We would then say that the running time of MergeSort is “big-O of n log n,” written O n log n , or that MergeSort   38  Asymptotic Notation  is an “O n log n -time algorithm.”1 Intuitively, saying that something is O f  n   for a function f  n  means that f  n  is what you’re left with after suppressing constant factors and lower-order terms.2 This “big-O notation” buckets algorithms into groups according to their asymptotic worst-case running times—the linear  O n  -time algorithms, the O n log n -time algorithms, the quadratic  O n2  -time algorithms, the constant  O 1  -time algorithms, and so on.  To be clear, I’m certainly not claiming that constant factors never matter in algorithm design. Rather, when you want to make a comparison between fundamentally diﬀerent ways of solving a problem, asymptotic analysis is often the right tool for understanding which one is going to perform better, especially on reasonably large inputs. Once you’ve ﬁgured out the best high-level algorithmic approach to a problem, you might well want to work harder to improve the leading constant factor, and perhaps even the lower-order terms. By all means, if the future of your start-up depends on how eﬃciently you implement a particular piece of code, have at it and make it as fast as you can.  2.1.3 Four Examples  We conclude this section with four very simple examples. They are so simple that, if you have any prior experience with big-O notation, you should probably just skip straight to Section 2.2 to start learning the mathematical formalism. But if these concepts are completely new to you, these simple examples should get you properly oriented. Consider ﬁrst the problem of searching an array for a given integer t. Let’s analyze the straightforward algorithm that performs a linear scan through the array, checking each entry to see if it is the desired integer t.  1When ignoring constant factors, we don’t even need to specify the base of the logarithm  as diﬀerent logarithmic functions diﬀer only by a constant factor . See Section 4.2.2 for further discussion.  2For example, even the function 10100 · n is technically O n . In these books, we will only study running time bounds where the suppressed constant factor is reasonably small.   2.1 The Gist  39  Searching One Array  Input: array A of n integers, and an integer t. Output: Whether or not A contains t.  for i := 1 to n do if A[i] = t then return TRUE  return FALSE  This code just checks each array entry in turn. If it ever ﬁnds the integer t it returns true, and if it falls oﬀ the end of the array without ﬁnding t it returns false.  We haven’t formally deﬁned what big-O notation means yet, but from our intuitive discussion so far you might be able to guess the asymptotic running time of the code above.  Quiz 2.1  What is the asymptotic running time of the code above for searching one array, as a function of the array length n?  a  O 1   b  O log n   c  O n   d  O n2    See Section 2.1.4 for the solution and discussion.   Our last three examples concern diﬀerent ways of combining two loops. First, let’s think about one loop followed by another. Suppose we’re now given two integer arrays A and B, both of length n, and we want to know whether a target integer t is in either one. Let’s again consider the straightforward algorithm, where we just search through A, and if we fail to ﬁnd t in A, we then search through B. If we don’t ﬁnd t in B either, we return false.   40  Asymptotic Notation  Searching Two Arrays  Input: arrays A and B of n integers each, and an integer t. Output: Whether or not A or B contains t.  for i := 1 to n do if A[i] = t then return TRUE  for i := 1 to n do  if B[i] = t then return TRUE  return FALSE  a  O 1   b  O log n   c  O n   d  O n2   What, in big-O notation, is the running time of this longer piece of code?  Quiz 2.2  What is the asymptotic running time of the code above for searching two arrays, as a function of the array lengths n?   See Section 2.1.4 for the solution and discussion.   Next let’s look at a more interesting example of two loops that are nested, rather than in sequence. Suppose we want to check whether or not two given arrays of length n have a number in common. The simplest solution is to check all possibilities. That is, for each index i into the array A and each index j into the array B, we check if A[i] is the same number as B[j]. If it is, we return true. If we exhaust all the possibilities without ever ﬁnding equal elements, we can safely return false.   2.1 The Gist  41  Checking for a Common Element  Input: arrays A and B of n integers each. Output: Whether or not there is an integer t contained in both A and B.  for i := 1 to n do  for j := 1 to n do  if A[i] = B[j] then  return TRUE  return FALSE  The question is the usual one: in big-O notation, what is the running time of this piece of code?  Quiz 2.3  What is the asymptotic running time of the code above for checking for a common element, as a function of the array lengths n?  a  O 1   b  O log n   c  O n   d  O n2    See Section 2.1.4 for the solution and discussion.   Our ﬁnal example again involves nested loops, but this time we’re looking for duplicate entries in a single array A, rather than in two diﬀerent arrays. Here’s the piece of code we’re going to analyze.   42  Asymptotic Notation  Checking for Duplicates  Input: array A of n integers. Output: Whether or not A contains an integer more than once.  for i := 1 to n do  for j := i + 1 to n do if A[i] = A[j] then  return TRUE  return FALSE  There are two small diﬀerences between this piece of code and the previous one. The ﬁrst and more obvious change is that we’re com- paring the ith element of A to the jth element of A, rather than to the jth element of some other array B. The second and more subtle change is that the inner loop now begins at the index i + 1 rather than the index 1. Why not start at 1, like before? Because then it would also return true in the very ﬁrst iteration  since clearly A[1] = A[1] , whether or not the array has any duplicate entries! Correctness could be salvaged by skipping all the iterations where i and j are equal, but this would still be wasteful: each pair of elements A[h] and A[k] of A would be compared to each other twice  once when i = h and j = k and once when i = k and j = h , while the code above compares them only once.  The question is the usual one: running time of this piece of code?  in big-O notation, what is the  Quiz 2.4  What is the asymptotic running time of the code above for checking for duplicates, as a function of the array length n?  a  O 1   b  O log n   c  O n    2.1 The Gist  d  O n2   43   See Section 2.1.4 for the solution and discussion.   These basic examples should have given you a strong intuitive sense of how big-O notation is deﬁned and what it is trying to accomplish. Next we move on to both the mathematical development of asymptotic notation and some more interesting algorithms.  2.1.4 Solutions to Quizzes 2.1–2.4  Solution to Quiz 2.1  Correct answer:  c . The correct answer is O n . Equivalently, we say that the algorithm has running time linear in n. Why is that true? The exact number of operations performed depends on the input—whether or not the target t is contained in the array A and, if so, where in the array it lies. In the worst case, when t is not in the array, the code will do an unsuccessful search, scanning through the entire array  over n loop iterations  and returning false. The key observation is that the code performs a constant number of operations for each entry of the array  comparing A[i] with t, incrementing the loop index i, etc. . Here “constant” means some number independent of n, like 2 or 3. We could argue about exactly what this constant is in the code above, but whatever it is, it is conveniently suppressed in the big-O notation. Similarly, the code does a constant number of operations before the loop begins and after it ends, and whatever the exact constant may be, it constitutes a lower-order term that is suppressed in the big-O notation. Since ignoring constant factors and lower-order terms leaves us with a bound of n on the total number of operations, the asymptotic running time of this code is O n .  Solution to Quiz 2.2  Correct answer:  c . The answer is the same as before, O n . The reason is that the worst-case number of operations performed  in an unsuccessful search  is twice that of the previous piece of code—ﬁrst we search the ﬁrst array, and then the second array. This extra factor of 2 contributes only to the leading constant in the running time   44  Asymptotic Notation  bound and is therefore suppressed when we use big-O notation. So this algorithm, like the previous one, is a linear-time algorithm.  Solution to Quiz 2.3  Correct answer:  d . This time, the answer has changed. For this piece of code, the running time is not O n , but is O n2 .  “Big-O of n squared,” also called a “quadratic-time algorithm.”  So with this algorithm, if you multiply the lengths of the input arrays by 10, the running time will go up by a factor of 100  rather than a factor of 10 for a linear-time algorithm .  Why does this code have a running time of O n2 ? The code again does a constant number of operations for each loop iteration  that is, for each choice of the indices i and j  and a constant number of operations outside the loops. What’s diﬀerent is that there’s now a total of n2 iterations of this double for loop—one for each choice of i 2 {1, 2, . . . , n} and j 2 {1, 2, . . . , n}. In our ﬁrst example, there were only n iterations of a single for loop. In our second example, because the ﬁrst for loop completed before the second one began, we had only 2n iterations overall. Here, for each of the n iterations of the outer for loop, the code performs n iterations of the inner for loop. This gives n ⇥ n = n2 iterations in all.  Solution to Quiz 2.4  Correct answer:  d . The answer to this question is the same as the last one, O n2 . The running time is again proportional to the number of iterations of the double for loop  with a constant number of operations per iteration . So how many iterations are there? The answer is roughly n2 2 . One way to see this is to remember that this piece of code does roughly half the work of the previous one  since the inner for loop starts at j = i + 1 rather than j = 1 . A second way is to observe that there is exactly one iteration for each subset {i, j} of two distinct indices from {1, 2, . . . , n}, and there are precisely 2  = n n 1   n 3 n 2  is pronounced “n choose 2,” and is also sometimes referred to as a  “binomial coeﬃcient.” See also the solution to Quiz 3.1.  such subsets.3  2   2.2 Big-O Notation  45  2.2 Big-O Notation  This section presents the formal deﬁnition of big-O notation. We begin with a deﬁnition in plain English, illustrate it pictorially, and ﬁnally give the mathematical deﬁnition.  2.2.1 English Deﬁnition  Big-O notation concerns functions T  n  deﬁned on the positive integers n = 1, 2, . . .. For us, T  n  will almost always denote a bound on the worst-case running time of an algorithm, as a function of the size n of the input. What does it mean to say that T  n  = O f  n  , for some “canonical” function f  n , like n, n log n, or n2? Here’s the deﬁnition in English.  Big-O Notation  English Version   T  n  = O f  n   if and only if T  n  is eventually bounded above by a constant multiple of f  n .  2.2.2 Pictorial Deﬁnition  See Figure 2.1 for a pictorial illustration of the deﬁnition of big-O notation. The x-axis corresponds to the parameter n, the y-axis to the value of a function. Let T  n  be the function corresponding to the solid line, and f  n  the lower dashed line. T  n  is not bounded above by f  n , but multiplying f  n  by 3 results in the upper dashed line, which does lie above T  n  once we go far enough to the right on the graph, after the “crossover point” at n0. Since f  n  is indeed eventually bounded above by a constant multiple of f  n , we can say that T  n  = O f  n  .  2.2.3 Mathematical Deﬁnition  Here is the mathematical deﬁnition of big-O notation, the deﬁnition you should use in formal proofs.   46  Asymptotic Notation  c 3⋅ f  n  T n   f  n   n0 n → ∞  Figure 2.1: A picture illustrating when T  n  = O f  n  . The constant c quantiﬁes the “constant multiple” of f  n , and the constant n0 quantiﬁes “eventually.”  Big-O Notation  Mathematical Version   T  n  = O f  n   if and only if there exist positive constants c and n0 such that  T  n   c · f  n    2.1   for all n   n0.  This is a direct translation of the English deﬁnition in Section 2.2.1. The inequality in  2.1  expresses that T  n  should be bounded above by a multiple of f  n   with the constant c specifying the multiple . The “for all n   n0” expresses that the inequality only needs to hold eventually, once n is suﬃciently large  with the constant n0 specifying how large . For example, in Figure 2.1, the constant c corresponds to 3, while n0 corresponds to the crossover point between the functions T  n  and c · f  n . If you want to prove that T  n  = A game-theoretic view. O f  n  , for example to prove that the asymptotic running time   2.3 Two Basic Examples  47  of an algorithm is linear in the input size  corresponding to f  n  = n , then your task is to choose the constants c and n0 so that  2.1  holds whenever n   n0. One way to think about this is game-theoretically, as a contest between you and an opponent. You go ﬁrst, and have to commit to constants c and n0. Your opponent goes second and can choose any integer n that is at least n0. You win if  2.1  holds, your opponent wins if the opposite inequality T  n  > c · f  n  holds. If T  n  = O f  n  , then there are constants c and n0 such that  2.1  holds for all n   n0, and you have a winning strategy in this game. Otherwise, no matter how you choose c and n0, your opponent can choose a large enough n   n0 to ﬂip the inequality and win the game.  A Word of Caution  When we say that c and n0 are constants, we mean they cannot depend on n. For example, in Figure 2.1, c and n0 were ﬁxed numbers  like 3 or 1000 , and we then considered the inequality  2.1  as n grows arbi- trarily large  looking rightward on the graph toward inﬁnity . If you ever ﬁnd yourself saying “take n0 = n” or “take c = log2 n” in an alleged big-O proof, you need to start over with choices of c and n0 that are independent of n.  2.3 Two Basic Examples  Having slogged through the formal deﬁnition of big-O notation, let’s look at a couple of examples. These examples won’t provide us with any insights we don’t already have, but they serve as an important sanity check that big-O notation is achieving its intended goal, of suppressing constant factors and lower-order terms. They are also a good warm-up for the less obvious examples we will encounter later.  2.3.1 Degree-k Polynomials are O nk   Our ﬁrst formal claim is that if T  n  is a polynomial with some degree k, then T  n  = O nk .   48  Asymptotic Notation  Proposition 2.1 Suppose  T  n  = aknk + ··· a1n + a0,  where k   0 is a nonnegative integer and the ai’s are real numbers  positive or negative . Then T  n  = O nk .  Proposition 2.1 says that with a polynomial, in big-O notation, all you need to worry about is the highest degree that appears in the polynomial. Thus, big-O notation really is suppressing constant factors and lower-order terms. Proof of Proposition 2.1: To prove this proposition, we need to use the mathematical deﬁnition of big-O notation  Section 2.2.3 . To satisfy the deﬁnition, it’s our job to ﬁnd a pair of positive constants c and n0  each independent of n , with c quantifying the constant multiple of nk and n0 quantifying “suﬃciently large n.” To keep things easy to follow but admittedly mysterious, let’s pull values for these constants out of a hat: n0 = 1 and c equal to the sum of absolute values of the coeﬃcients:4  c = ak + ··· + a1 + a0.  Both of these numbers are independent of n. We now need to show that these choices of constants satisfy the deﬁnition, meaning that T  n   c ˙nk for all n   n0 = 1. To verify this inequality, ﬁx an arbitrary positive integer n   n0 = 1. We need a sequence of upper bounds on T  n , culminating in an upper bound of c · nk. First let’s apply the deﬁnition of T  n :  T  n  = aknk + ··· + a1n + a0.  If we take the absolute value of each coeﬃcient ai on the right-hand side, the expression only becomes larger.  ai can only be bigger than ai, and since ni is positive, aini can only be bigger than aini.  This means that  T  n   aknk + ··· + a1n + a0.  Why is this step useful? Now that the coeﬃcients are nonnegative, we can use a similar trick to turn the diﬀerent powers of n into a 4Recall that the absolute value x of a real number x equals x when x   0,  and  x when x  0. In particular, x is always nonnegative.   2.3 Two Basic Examples  49  common power of n. Since n   1, nk is only bigger than ni for every i 2 {0, 1, 2, . . . , k}. Since ai is nonnegative, aink is only bigger than aini. This means that T  n   aknk + ··· + a1nk + a0nk =  ak + ··· + a1 + a0  }  This inequality holds for every n   n0 = 1, which is exactly what we wanted to prove. QE D  {z  ·nk.    =c  How do you know how to choose the constants c and n0? The usual approach is to reverse engineer them. This involves going through a derivation like the one above and ﬁguring out on-the-ﬂy the choices of constants that let you push the proof through. We’ll see some examples of this method in Section 2.5.  2.3.2 Degree-k Polynomials Are Not O nk 1   Our second example is really a non-example: a degree-k polynomial is O nk , but is not generally O nk 1 .  Proposition 2.2 Let k   1 be a positive integer and deﬁne T  n  = nk. Then T  n  is not O nk 1 .  Proposition 2.2 implies that polynomials with distinct degrees are distinct with respect to big-O notation.  If this weren’t true, something would be wrong with our deﬁnition of big-O notation!  Proof of Proposition 2.2: The best way to prove that one function is not big-O of another is usually with a proof by contradiction. In this type of proof, you assume the opposite of what you want to prove, and then build on this assumption with a sequence of logically correct steps that culminate in a patently false statement. Such a contradiction implies that the assumption can’t be true, and this proves the desired statement.  So, assume that nk is in fact O nk 1 ; we proceed to derive a contradiction. What does it mean if nk = O nk 1 ? That nk is eventually bounded by a constant multiple of nk 1. That is, there are positive constants c and n0 such that nk  c · nk 1   50  Asymptotic Notation  for all n   n0. Since n is a positive number, we can cancel nk 1 from both sides of this inequality to derive  n  c  for all n   n0. This inequality asserts that the constant c is bigger than every positive integer, a patently false statement  for a counterexample, take c + 1, rounded up to the nearest integer . This shows that our original assumption that nk = O nk 1  cannot be correct, and we can conclude that nk is not O nk 1 . QE D  2.4 Big-Omega and Big-Theta Notation  Big-O notation is by far the most important and ubiquitous concept for discussing the asymptotic running time of algorithms. A couple of its close relatives, the big-omega and big-theta notations, are also worth knowing. If big-O is analogous to “less than or equal to   ,” then big-omega and big-theta are analogous to “greater than or equal to    ,” and “equal to  = ,” respectively. Let’s now treat them a little more precisely.  2.4.1 Big-Omega Notation  The formal deﬁnition of big-omega notation parallels that of big-O notation. In English, we say that one function T  n  is big-omega of another function f  n  if and only if T  n  is eventually bounded below by a constant multiple of f  n . In this case, we write T  n  = ⌦ f  n  . As before, we use two constants c and n0 to quantify “constant multiple” and “eventually.”  Big-Omega Notation  Mathematical Version   T  n  = ⌦ f  n   if and only if there exist positive constants c and n0 such that  T  n    c · f  n   for all n   n0. You can imagine what the corresponding picture looks like:   2.4 Big-Omega and Big-Theta Notation  51  f  n   T n   1 4 ⋅ f  n  c  n0  n → ∞  T  n  again corresponds to the function with the solid line. The function f  n  is the upper dashed line. This function does not bound T  n  from below, but if we multiply it by the constant c = 1 4, the result  the lower dashed line  does bound T  n  from below for all n past the crossover point at n0. Thus T  n  = ⌦ f  n  .  2.4.2 Big-Theta Notation  Big-theta notation, or simply theta notation, is analogous to “equal to.” Saying that T  n  = ⇥ f  n   just means that both T  n  = ⌦ f  n   and T  n  = O f  n  . Equivalently, T  n  is eventually sandwiched between two diﬀerent constant multiples of f  n .5  Big-Theta Notation  Mathematical Version   T  n  = ⇥ f  n   if and only if there exist positive constants c1, c2, and n0 such that  c1 · f  n   T  n   c2 · f  n   for all n   n0. 5Proving this equivalence amounts to showing that one version of the deﬁnition is satisﬁed if and only if the other one is. If T  n  = ⇥ f  n   according to the second deﬁnition, then the constants c2 and n0 prove that T  n  = O f  n  , while the constants c1 and n0 prove that T  n  = ⌦ f  n  . In the other direction, suppose you can prove that T  n  = O f  n   using the constants c2 and n00 and T  n  = ⌦ f  n   using the constants c1 and n000 . Then T  n  = ⇥ f  n   in the sense of the second deﬁnition, with constants c1, c2, and n0 = max{n00, n000}.   52  Asymptotic Notation  A Word of Caution  Algorithm designers often use big-O notation even when big-theta notation would be more accurate. This book will follow that tradition. For example, consider a subroutine that scans an array of length n, perform- ing a constant number of operations per entry  like the Merge subroutine in Section 1.4.5 . The running time of such a subroutine is obviously ⇥ n , but it’s common to only mention that it is O n . This is because, as algorithm designers, we generally focus on upper bounds—guarantees about how long our algorithms could possibly run.  The next quiz checks your understanding of big-O, big-omega, and  big-theta notation.  Quiz 2.5  Let T  n  = 1 2 n2 + 3n. Which of the following statements are true?  There might be more than one correct answer.   a  T  n  = O n   b  T  n  = ⌦ n   c  T  n  = ⇥ n2   d  T  n  = O n3    See Section 2.4.5 for the solution and discussion.   2.4.3 Little-O Notation  There’s one ﬁnal piece of asymptotic notation, “little-o notation,” that you see from time to time. If big-O notation is analogous to “less than or equal to,” little-o notation is analogous to “strictly less than.”6  6Similarly, there is a “little-omega” notation that corresponds to “strictly greater than,” but we won’t have occasion to use it. There is no “little-theta”   2.4 Big-Omega and Big-Theta Notation  53  Little-O Notation  Mathematical Version   T  n  = o f  n   if and only if for every positive constant c > 0, there exists a choice of n0 such that  T  n   c · f  n    2.2   for all n   n0.  Proving that one function is big-O of another requires only two constants c and n0, chosen once and for all. To prove that one function is little-o of another, we have to prove something stronger, that for every constant c, no matter how small, T  n  is eventually bounded above by the constant multiple c · f  n . Note that the constant n0 chosen to quantify “eventually” can depend on c  but not n! , with smaller constants c generally requiring bigger constants n0. For example, for every positive integer k, nk 1 = o nk .7  2.4.4 Where Does Notation Come From?  Asymptotic notation was not invented by computer scientists—it has been used in number theory since around the turn of the 20th century. Donald E. Knuth, the grandfather of the formal analysis of algorithms, proposed using it as the standard language for discussing rates of growth, and in particular for algorithm running times.  “On the basis of the issues discussed here, I propose that members of SIGACT,8 and editors of computer science and mathematics journals, adopt the O, ⌦, and ⇥ notations as deﬁned above, unless a better alternative can be found reasonably soon.”9  notation.  7Here’s the proof. Fix an arbitrary constant c > 0. In response, choose n0 to c , rounded up to the nearest integer. Then for all n   n0, n0 · nk 1  nk and be 1 hence nk 1  1 8SIGACT is the special interest group of the ACM  Association for Computing Machinery  that concerns theoretical computer science, and in particular the analysis of algorithms.  n0 · nk  c · nk, as required.  9Donald E. Knuth, “Big Omicron and Big Omega and Big Theta,” SIGACT News, Apr.-June 1976, page 23. Reprinted in Selected Papers on Analysis of Algorithms  Center for the Study of Language and Information, 2000 .   54  Asymptotic Notation  2.4.5 Solution to Quiz 2.5  Correct answers:  b , c , d . The ﬁnal three responses are all correct, and hopefully the intuition for why is clear. T  n  is a quadratic function. The linear term 3n doesn’t matter for large n, so we should expect that T  n  = ⇥ n2   answer  c  . This automatically implies that T  n  = ⌦ n2  and hence T  n  = ⌦ n  also  answer  b  . Note that ⌦ n  is not a particularly impressive lower bound on T  n , but it is a legitimate one nonetheless. Similarly, T  n  = ⇥ n2  implies that T  n  = O n2  and hence also T  n  = O n3   answer  d  . Proving these statements formally boils down to exhibiting appropriate constants to satisfy the deﬁnitions. For example, taking n0 = 1 and 2 proves  b . Taking n0 = 1 and c = 4 proves  d . Combining c = 1 these constants  n0 = 1, c1 = 1 2, c2 = 4  proves  c . The argument in the proof of Proposition 2.2 can be used to prove formally that  a  is not a correct answer.  2.5 Additional Examples  This section is for readers who want additional practice with asymp- totic notation. Other readers can skip the three additional examples here and proceed straight to Chapter 3.  2.5.1 Adding a Constant to an Exponent  First we have another example of a proof that one function is big-O of another.  Proposition 2.3 If  then T  n  = O 2n .  T  n  = 2n+10,  That is, adding a constant to the exponent of an exponential  function does not change its asymptotic rate of growth. Proof of Proposition 2.3: To satisfy the mathematical deﬁnition of big-O notation  Section 2.2.3 , we just need to exhibit a suitable pair of positive constants c and n0  each independent of n , such that T  n  is at most c · 2n for all n   n0. In the proof of Proposition 2.1 we just   2.5 Additional Examples  55  pulled these two constants out of a hat; here, let’s reverse engineer them.  We’re looking for a derivation that begins with T  n  on the left- hand side, followed by a sequence of only-larger numbers, culminating in a constant multiple of 2n. How would such a derivation begin? The “10” in the exponent is annoying, so a natural ﬁrst step is to separate it out:  T  n  = 2n+10 = 210 · 2n = 1024 · 2n.  Now we’re in good shape; the right-hand side is a constant multiple of 2n, and the derivation suggests that we should take c = 1024. Given this choice of c, we have T  n   c · 2n for all n   1, so we just take n0 = 1. This pair of constants certiﬁes that T  n  is indeed O 2n . QE D  2.5.2 Multiplying an Exponent by a Constant  Next is another non-example, showing that one function is not big-O of another.  Proposition 2.4 If  then T  n  is not O 2n .  T  n  = 210n,  That is, multiplying the exponent of an exponential function by a  constant changes its asymptotic rate of growth. Proof of Proposition 2.4: As with Proposition 2.2, the usual way to prove that one function is not big-O of another is by contradiction. So assume the opposite of the statement in the proposition, that T  n  is in fact O 2n . By the deﬁnition of big-O notation, this means there are positive constants c and n0 such that 210n  c · 2n  for all n   n0. Since 2n is a positive number, we can cancel it from both sides of this inequality to derive 29n  c  for all n   n0. But this inequality is patently false: the right-hand side is a ﬁxed constant  independent of n , while the left-hand side   56  Asymptotic Notation  goes to inﬁnity as n grows large. This shows that our assumption that T  n  = O 2n  cannot be correct, and we can conclude that 210n is not O 2n . QE D  2.5.3 Maximum vs. Sum  Our ﬁnal example uses big-theta notation  Section 2.4.2 , the asymp- totic version of “equal to.” This example shows that, asymptotically, there’s no diﬀerence between taking the pointwise maximum of two nonnegative functions and taking their sum.  Proposition 2.5 Let f and g denote functions from the positive integers to the nonnegative real numbers, and deﬁne  T  n  = max{f  n , g n } for each n   1. Then T  n  = ⇥ f  n  + g n  .  One consequence of Proposition 2.5 is that an algorithm that performs a constant number  meaning independent of n  of O f  n  - time subroutines runs in O f  n   time.  Proof of Proposition 2.5: Recall that T  n  = ⇥ f  n   means that T  n  is eventually sandwiched between two diﬀerent constant multiples of f  n . To make this precise, we need to exhibit three constants: the usual constant n0, and the constants c1 and c2 corresponding to the smaller and larger multiples of f  n . Let’s reverse engineer values for these constants.  Consider an arbitrary positive integer n. We have  max{f  n , g n }  f  n  + g n ,  since the right-hand side is just the left-hand side plus a nonnegative number  f  n  or g n , whichever is smaller . Similarly,  2 · max{f  n , g n }   f  n  + g n ,  since the left-hand side is two copies of the larger of f  n , g n  and the right-hand side is one copy of each. Putting these two inequalities together, we see that  1 2   f  n  + g n    max{f  n , g n }  f  n  + g n    2.3    Problems  57  for every n   1. Thus max{f  n , g n } is indeed wedged between two diﬀerent multiples of f  n  + g n . Formally, choosing n0 = 1, c1 = 1 2, and c2 = 1 shows  by  2.3   that max{f  n , g n } = ⇥ f  n  + g n  . QE D  The Upshot  P The purpose of asymptotic notation is to sup- press constant factors  which are too system- dependent  and lower-order terms  which are irrelevant for large inputs .  P A function T  n  is said to be “big-O of f  n ,” written “T  n  = O f  n  ,” if it is eventually  for suﬃciently n  bounded above by a constant multiple of f  n . That is, there are positive constants c and n0 such that T  n   c· f  n  for all n   n0.  P A function T  n  is “big-omega of f  n ,” written “T  n  = ⌦ f  n  ,” if it is eventually bounded below by a constant multiple of f  n .  P A function T  n  is “big-theta of f  n ,” written “T  n  = ⇥ f  n  ,” if both T  n  = O f  n   and T  n  = ⌦ f  n  .  P A big-O statement is analogous to “less than or equal to,” big-omega to “greater than or equal to,” and big-theta to “equal to.”  Test Your Understanding  Problem 2.1 Let f and g be non-decreasing real-valued functions deﬁned on the positive integers, with f  n  and g n  at least 1 for all n   1. Assume that f  n  = O g n  , and let c be a positive constant. Is f  n  · log2 f  n c  = O g n  · log2 g n   ?  a  Yes, for all such f, g, and c   58  Asymptotic Notation  b  Never, no matter what f, g, and c are  c  Sometimes yes, sometimes no, depending on the constant c  d  Sometimes yes, sometimes no, depending on the functions f  and g  and g  Problem 2.2 Assume again two positive non-decreasing functions f and g such that f  n  = O g n  . Is 2f  n  = O 2g n   ?  Multiple answers may be correct; choose all that apply.   a  Yes, for all such f and g  b  Never, no matter what f and g are  c  Sometimes yes, sometimes no, depending on the functions f  d  Yes whenever f  n   g n  for all suﬃciently large n  Problem 2.3 Arrange the following functions in order of increasing growth rate, with g n  following f  n  in your list if and only if f  n  = O g n  .  Problem 2.4 Arrange the following functions in order of increasing growth rate, with g n  following f  n  in your list if and only if f  n  = O g n  .  a  pn b  10n  c  n1.5 d  2plog2 n e  n5 3  a  n2 log2 n b  2n  c  22n   59  Problem 2.5 Arrange the following functions in order of increasing growth rate, with g n  following f  n  in your list if and only if f  n  = O g n  .  Problems  d  nlog2 n  e  n2  a  2log2 n  b  22log2 n  c  n5 2 d  2n2  e  n2 log2 n   Chapter 3  Divide-and-Conquer Algorithms  This chapter provides practice with the divide-and-conquer algorithm design paradigm through applications to three basic problems. Our ﬁrst example is an algorithm for counting the number of inversions in an array  Section 3.2 . This problem is related to measuring similarity between two ranked lists, which is relevant for making good recommendations to someone based on your knowledge of their and others’ preferences  called “collaborative ﬁltering” . Our second divide-and-conquer algorithm is Strassen’s mind-blowing recursive algorithm for matrix multiplication, which improves over the obvi- ous iterative method  Section 3.3 . The third algorithm, which is advanced and optional material, is for a fundamental problem in computational geometry: computing the closest pair of points in the plane  Section 3.4 .1  3.1 The Divide-and-Conquer Paradigm  You’ve seen the canonical example of a divide-and-conquer algorithm, MergeSort  Section 1.4 . More generally, the divide-and-conquer algorithm design paradigm has three conceptual steps.  The Divide-and-Conquer Paradigm 1. Divide the input into smaller subproblems.  2. Conquer the subproblems recursively.  3. Combine the solutions for the subproblems into a  solution for the original problem.  1The presentation in Sections 3.2 and 3.4 draws inspiration from Chapter 5 of  Algorithm Design, by Jon Kleinberg and Éva Tardos  Pearson, 2005 .  60   3.2 Counting Inversions in O n log n  Time  61  For example, in MergeSort, the “divide” step splits the input array into its left and right halves, the “conquer” step uses two recursive calls to sort the left and right subarrays, and the “combine” step is implemented by the Merge subroutine  Section 1.4.5 . In MergeSort and many other algorithms, it is the last step that requires the most ingenuity. There are also divide-and-conquer algorithms in which the cleverness is in the ﬁrst step  see QuickSort in Chapter 5  or in the speciﬁcation of the recursive calls  see Section 3.2 .  3.2 Counting Inversions in O n log n  Time  3.2.1 The Problem  This section studies the problem of computing the number of inversions in an array. An inversion of an array is a pair of elements that are “out of order,” meaning that the element that occurs earlier in the array is bigger than the one that occurs later.  Problem: Counting Inversions  Input: An array A of distinct integers.  Output: The number of inversions of A—the number of pairs  i, j  of array indices with i   A[j].  For example, an array A that is in sorted order has no inversions. You should convince yourself that the converse is also true: every array that is not in sorted order has at least one inversion.  3.2.2 An Example  Consider the following array of length 6:  1  3  5  2  4  6   How many inversions does this array have? One that jumps out is the 5 and 2  corresponding to i = 3 and j = 4 . There are exactly two other out-of-order pairs: the 3 and the 2, and the 5 and the 4.   62  Divide-and-Conquer Algorithms  Quiz 3.1  What is the largest-possible number of inversions a 6-element array can have?  a  15  b  21  c  36  d  64   See Section 3.2.13 for the solution and discussion.   3.2.3 Collaborative Filtering  Why would you want to count the number of inversions in an ar- ray? One reason is to compute a numerical similarity measure that quantiﬁes how close two ranked lists are to each other. For example, suppose I ask you and a friend to rank, from favorite to least favorite, ten movies that you have both seen. Are your tastes “similar” or “diﬀerent?” One way to answer this question quantitatively is through the following 10-element array A: A[1] contains your friend’s ranking of your favorite movie in their list, A[2] your friend’s personal ranking of your second-favorite movie, . . . , and A[10] your friend’s personal ranking of your least favorite movie. So if your favorite movie is Star Wars but your friend has it only ﬁfth in their list, then A[1] = 5. If your rankings are identical, this array will be sorted and have no inversions. The more inversions the array has, the more pairs of movies on which you disagree about their relative merits, and the more diﬀerent your preferences.  One reason you might want a similarity measure between rankings is to do collaborative ﬁltering, a technique for generating recommen- dations. How do Web sites come up with suggestions for products, movies, songs, news stories, and so on? In collaborative ﬁltering, the idea is to identify other users who have similar preferences, and to recommend to you things that have been popular with them. Thus collaborative ﬁltering algorithms require a formal notion of “similarity” between users, and the problem of computing inversions captures   3.2 Counting Inversions in O n log n  Time  63  some of the essence of this problem.  3.2.4 Brute-Force Search  How quickly can we compute the number of inversions in an ar- ray? If we’re feeling unimaginative, there’s always brute-force search.  Brute-Force Search for Counting Inversions  Input: array A of n distinct integers. Output: the number of inversions of A.  numInv := 0 for i := 1 to n   1 do  for j := i + 1 to n do if A[i] > A[j] then  numInv := numInv + 1  return numInv  This is certainly a correct algorithm. What about its running time? From the solution to Quiz 3.1, we know that the number of loop iterations grows quadratically with the length n of the input array. Since the algorithm does a constant number of operations in each loop iteration, its asymptotic running time is ⇥ n2 . Remember the mantra of a seasoned algorithm designer: can we do better?  3.2.5 A Divide-and-Conquer Approach  The answer is yes, and the solution will be a divide-and-conquer algorithm that runs in O n log n  time, a big improvement over the brute-force search algorithm. The “divide” step will be exactly as in the MergeSort algorithm, with one recursive call for the left half of the array and one for the right half. To understand the residual work that needs to be done outside the two recursive calls, let’s classify the inversions  i, j  of an array A of length n into one of three types:  1. left inversion: an inversion with i, j both in the ﬁrst half of the  array  i.e., i, j  n  2  ;2  2The abbreviation “i.e.” stands for id est, and means “that is.”   64  Divide-and-Conquer Algorithms  2. right inversion: an inversion with i, j both in the second half of  the array  i.e., i, j > n  2  ;  right half  i.e., i  n  2 < j .  3. split inversion: an inversion with i in the left half and j in the  For example, in the six-element array in Section 3.2.2, all three of the inversions are split inversions.  The ﬁrst recursive call, on the ﬁrst half of the input array, re- cursively counts all the left inversions  and nothing else . Similarly, the second recursive call counts all the right inversions. The remain- ing task is to count the inversions not counted by either recursive call—the split inversions. This is the “combine” step of the algorithm, and we will need to implement a special linear-time subroutine for it, analogous to the Merge subroutine in the MergeSort algorithm.  3.2.6 High-Level Algorithm  Our divide-and-conquer approach translates to the following pseu- docode; the subroutine CountSplitInv is, as of now, unimplemented.  CountInv  Input: array A of n distinct integers. Output: the number of inversions of A.  if n = 0 or n = 1 then  return 0  else  lef tInv := CountInv ﬁrst half of A  rightInv := CountInv second half of A  splitInv := CountSplitInv A  return lef tInv + rightInv + splitInv     base cases  The ﬁrst and second recursive calls count the number of left and right inversions. Provided the subroutine CountSplitInv correctly computes the number of split inversions, CountInv correctly computes the total number of inversions.   3.2 Counting Inversions in O n log n  Time  65  3.2.7 Key Idea: Piggyback on MergeSort  Counting the number of split inversions of an array in linear time is an ambitious goal. There can be a lot of split inversions: if A consists of the numbers n 2 + 1, . . . , n in order, followed by the numbers 1, 2, . . . , n 2 in order, there are n2 4 split inversions. How could we ever count a quadratic number of things with only a linear amount of work?  The inspired idea is to design our recursive inversion-counting algorithm so that it piggybacks on the MergeSort algorithm. This involves demanding more from our recursive calls, in service of making it easier to count the number of split inversions.3 Each recursive call will be responsible not only for counting the number of inversions in the array that it is given, but also for returning a sorted version of the array. We already know  from Theorem 1.2  that sorting is a for-free primitive  see page 31 , running in O n log n  time, so if we’re shooting for a running time bound of O n log n , there’s no reason not to sort. And we’ll see that the task of merging two sorted subarrays is tailor-made for uncovering all the split inversions of an array.  Here is the revised version of the pseudocode in Section 3.2.6,  which counts inversions while also sorting the input array.  Sort-and-CountInv  Input: array A of n distinct integers. Output: sorted array B with the same integers, and the number of inversions of A.  if n = 0 or n = 1 then  return  A, 0   else     base cases   C, lef tInv  := Sort-and-CountInv ﬁrst half of A   D, rightInv  := Sort-and-CountInv second half of A   B, splitInv  := Merge-and-CountSplitInv C, D  return  B, lef tInv + rightInv + splitInv   3Similarly, sometimes a proof by induction becomes easier to push through  after strengthening your inductive hypothesis.   66  Divide-and-Conquer Algorithms  We still need to implement the Merge-and-CountSplitInv subroutine. We know how to merge two sorted lists in linear time, but how can we piggyback on this work to also count the number of split inversions?  3.2.8  Merge Revisited  To see why merging sorted subarrays naturally uncovers split inver- sions, let’s revisit the pseudocode for the Merge subroutine.  Merge  Input: sorted arrays C and D  length n 2 each . Output: sorted array B  length n . Simplifying assumption: n is even.  i := 1, j := 1 for k := 1 to n do  if C[i] < D[j] then  B[k] := C[i], i := i + 1  else  B[k] := D[j], j := j + 1     D[j] < C[i]  To review, the Merge subroutine walks one index down each of the sorted subarrays in parallel  i for C and j for D , populating the output array  B  from left to right in sorted order  using the index k . At each iteration of the loop, the subroutine identiﬁes the smallest element that it hasn’t yet copied over to B. Since C and D are sorted, and all the elements before C[i] and D[j] have already been copied over to B, the only two candidates are C[i] and D[j]. The subroutine determines which of the two is smaller and then copies it over to the next position of the output array.  What does the Merge subroutine have to do with counting the number of split inversions? Let’s start with the special case of an array A that has no split inversions at all—every inversion of A is either a left or a right inversion.  Quiz 3.2  Suppose the input array A has no split inversions. What is   3.2 Counting Inversions in O n log n  Time  67  the relationship between the sorted subarrays C and D?  a  C has the smallest element of A, D the second-smallest,  C the third-smallest, and so on.  b  All elements of C are less than all elements of D.  c  All elements of C are greater than all elements of D.  d  There is not enough information to answer this ques-  tion.   See Section 3.2.13 for the solution and discussion.   After solving Quiz 3.2, you can see that Merge has an particularly boring execution on an array with no split inversions. Since every element of C is smaller than every element of D, the smallest remaining element is always in C  until no elements of C remain . Thus the Merge subroutine just concatenates C and D—it will ﬁrst copy over all of C, and then all of D. This suggests that, perhaps, split inversions have something to do with the number of elements remaining in C when an element of D is copied over to the output array.  3.2.9  Merge and Split Inversions  To build our intuition further, let’s think about running the MergeSort algorithm on the six-element array A = {1, 3, 5, 2, 4, 6} from Sec- tion 3.2.2; see also Figure 3.1. The left and right halves of this array are already sorted, so there are no left inversions or right inversions, and the two recursive calls return 0. In the ﬁrst iteration of the Merge subroutine, the ﬁrst element of C  the 1  is copied over to B. This tells us nothing about any split inversions, and indeed there are no split inversions that involve this element. In the second iteration, however, the 2 is copied over to the output array, even though C still contains the elements 3 and 5. This exposes two of the split inversions of A—the two such inversions that involve the 2. In the third iteration, the 3 is copied over from C and there are no further split inversions that involve this element. When the 4 is copied over from D, the array C still contains a 5, and this copy exposes the third and ﬁnal split inversion of A  involving the 5 and the 2 .   68  Divide-and-Conquer Algorithms  C   1  4  5   i   2  4  6   D   j   B   1  2  3  4  5  6   k   Figure 3.1: The fourth iteration of the Merge subroutine given the sorted subarrays {1, 3, 5} and {2, 4, 6}. Copying the 4 over from D, with the 5 still in C, exposes the split inversion involving these two elements.  The following lemma states that the pattern in the example above holds in general: the number of split inversions that involve an ele- ment y of the second subarray D is precisely the number of elements remaining in C in the iteration of the Merge subroutine in which y is copied to the output array.  Lemma 3.1 Let A be an array, and C and D sorted versions of the ﬁrst and second halves of A. An element x from the ﬁrst half of A and y from the second half of A form a split inversion if and only if, in the Merge subroutine with inputs C and D, y is copied to the output array before x.  Proof: Since the output array is populated from left to right in sorted order, the smaller of x or y is copied over ﬁrst. Since x is in the ﬁrst half of A and y in the second half, x and y form a split inversion if and only if x > y, and this is true if and only if y is copied over to the output array before x. QE D  3.2.10  Merge-and-CountSplitInv  With the insight provided by Lemma 3.1, we can extend the implemen- tation of Merge to an implementation of Merge-and-CountSplitInv. We maintain a running count of the split inversions, and every time an element is copied over from the second subarray D to the output   3.2 Counting Inversions in O n log n  Time  69  array B, we increment the running count by the number of elements remaining in the ﬁrst subarray C.  Merge-and-CountSplitInv  Input: sorted arrays C and D  length n 2 each . Output: sorted array B  length n  and the number of split inversions. Simplifying assumption: n is even.  i := 1, j := 1, splitInv := 0 for k := 1 to n do  if C[i] < D[j] then  B[k] := C[i], i := i + 1  else  B[k] := D[j], j := j + 1 splitInv := splitInv +   n  return  B, splitInv   2   i + 1   left in C  {z  }       D[j] < C[i]  3.2.11 Correctness  Correctness of Merge-and-CountSplitInv follows from Lemma 3.1. Every split inversion involves exactly one element y from the second subarray, and this inversion is counted exactly once, when y is copied over to the output array. Correctness of the entire Sort-and-CountInv algorithm  Section 3.2.7  follows: the ﬁrst recursive call correctly computes the number of left inversions, the second recursive call the number of right inversions, and Merge-and-CountSplitInv the remaining  split  inversions.  3.2.12 Running Time  We can also analyze the running time of the Sort-and-CountInv algorithm by piggybacking on the analysis we already did for the MergeSort algorithm. First consider the running time of a single invocation of the Merge-and-CountSplitInv subroutine, given two subarrays of length ` 2 each. Like the Merge subroutine, it does a   70  Divide-and-Conquer Algorithms  constant number of operations per loop iteration, plus a constant number of additional operations, for a running time of O ` .  Looking back at our running time analysis of the MergeSort algorithm in Section 1.5, we can see that there were three important properties of the algorithm that led to the running time bound of O n log n . First, each invocation of the algorithm makes two recursive calls. Second, the length of the input is divided in half with each level of recursion. Third, the amount of work done in a recursive call, not counting work done by later recursive calls, is linear in the input size. Since the Sort-and-CountInv algorithm shares these three properties, the analysis in Section 1.5 carries over, again giving a running time bound of O n log n .  Theorem 3.2  Counting Inversions  For every input array A of length n   1, the Sort-and-CountInv algorithm computes the number of inversions of A and runs in O n log n  time.  3.2.13 Solutions to Quizzes 3.1–3.2  Solution to Quiz 3.1  Correct answer:  a . The correct answer to this question is 15. The maximum-possible number of inversions is at most the number of ways of choosing i, j 2 {1, 2, . . . , 6} with i < j. The latter quantity is denoted  6 , and so  6 2  = 15.4 In a six-element array sorted in reverse order  6, 5, . . . , 1 ,  every pair of elements is out of order, and so this array achieves 15 inversions.  2 , for “6 choose 2.”  In general,  n  2  = n n 1   2  Solution to Quiz 3.2  Correct answer:  b . In an array with no split inversions, everything in the ﬁrst half is less than everything in the second half. If there was an element A[i] in the ﬁrst half  with i 2 {1, 2, . . . , n 2}  that is greater than an element A[j] in the second half  with j 2 { n 2 +2, . . . , n} , 2 +1, n then  i, j  would constitute a split inversion.  4There are n n   1  ways to choose  i, j  so that i 6= j  n choices for i, then  n   1 for j . By symmetry, i < j in exactly half of these.   3.3  Strassen’s Matrix Multiplication Algorithm  71  3.3  Strassen’s Matrix Multiplication Algorithm  This section applies the divide-and-conquer algorithm design paradigm to the problem of multiplying matrices, culminating in Strassen’s amaz- ing subcubic-time matrix multiplication algorithm. This algorithm is a canonical example of the magic and power of clever algorithm design—of how algorithmic ingenuity can improve over straightforward solutions, even for extremely fundamental problems.  3.3.1 Matrix Multiplication  Suppose X and Y are n ⇥ n matrices of integers—n2 entries in each. In the product Z = X· Y, the entry zij in the ith row and jth column of Z is deﬁned as the dot product of the ith row of X and the jth column of Y  Figure 3.2 .5 That is,  zij =  xikykj.  nXk=1   3.1   X   ith row   !     n m u l o c   h t j  Y   =   Z   zij   Figure 3.2: The  i, j  entry of the matrix product X· Y is the dot product of the ith row of X and the jth column of Y.  3.3.2 Example  n = 2   Let’s drill down on the n = 2 case. We can describe two 2⇥ 2 matrices using eight parameters:  and  ✓ a b c d ◆ } {z   X  .  ✓ e f g h ◆ } {z   Y  5To compute the dot product of two length-n vectors a =  a1, . . . , an  and b = i=1 aibi.   b1, . . . , bn , add up the results of multiplying componentwise: a · b =Pn   72  Divide-and-Conquer Algorithms  In the matrix product X · Y, the upper-left entry is the dot product of the ﬁrst row of X and the ﬁrst column of Y, or ae + bg. In general, for X and Y as above,  X · Y =✓ ae + bg af + bh cf + dh ◆ .  ce + dg   3.2   3.3.3 The Straightforward Algorithm  Now let’s think about algorithms for computing the product of two matrices.  Problem: Matrix Multiplication Input: Two n ⇥ n integer matrices, X and Y.6 Output: The matrix product X · Y.  The input size is proportional to n2, the number of entries in each of X and Y. Since we presumably have to read the input and write out the output, the best we can hope for is an algorithm with running time O n2 —linear in the input size, and quadratic in the dimension. How close can we get to this best-case scenario?  There is a straightforward algorithm for matrix multiplication,  which just translates the mathematical deﬁnition into code.  Straightforward Matrix Multiplication  Input: n ⇥ n integer matrices X and Y. Output: Z = X · Y. for i := 1 to n do  for j := 1 to n do  Z[i][j] := 0 for k := 1 to n do  Z[i][j] := Z[i][j] + X[i][k] · Y[k][j]  return Z  6The algorithms we discuss can also be extended to multiply non-square  matrices, but we’ll stick with the square case for simplicity.   3.3  Strassen’s Matrix Multiplication Algorithm  73  What is the running time of this algorithm?  Quiz 3.3  What is the asymptotic running time of the straightforward algorithm for matrix multiplication, as a function of the matrix dimension n? Assume that the addition or multipli- cation of two matrix entries is a constant-time operation.  a  ⇥ n log n   b  ⇥ n2   c  ⇥ n3   d  ⇥ n4    See Section 3.3.7 for the solution and discussion.   3.3.4 A Divide-and-Conquer Approach  The question is, as always, can we do better? Everyone’s ﬁrst reaction is that matrix multiplication should, essentially by deﬁnition, require ⌦ n3  time. But perhaps we’re emboldened by the success of the Karatsuba algorithm for integer multiplication  Section 1.3 , where a clever divide-and-conquer algorithm improves over the straightfor- ward grade-school algorithm.7 Could a similar approach work for multiplying matrices?  To apply the divide-and-conquer paradigm  Section 3.1 , we need to ﬁgure out how to divide the input into smaller subproblems and how to combine the solutions of these subproblems into a solution for the original problem. The simplest way to divide a square matrix into smaller square submatrices is to slice it in half, both vertically and horizontally. In other words, write  X =✓ A B  C D ◆ and Y =✓ E F G H ◆ ,   3.3   where A, B, . . . , H are all n  2 ⇥ n  2 matrices.8  7We haven’t actually proved this yet, but we will in Section 4.3. 8As usual, we’re assuming that n is even for convenience. And as usual, it  doesn’t really matter.   74  Divide-and-Conquer Algorithms  One cool thing about matrix multiplication is that equal-size blocks behave just like individual entries. That is, for X and Y as above, we have  X · Y =✓ A · E + B · G A · F + B · H C · E + D · G C · F + D · H ◆ ,   3.4   completely analogous to the equation  3.2  for the n = 2 case.  This follows from the deﬁnition of matrix multiplication, as you should check.  In  3.4 , adding two matrices just means adding them entrywise—the  i, j  entry of K + L is the sum of the  i, j  entries of K and L. The decomposition and computation in  3.4  translates nat- urally to a recursive algorithm for matrix multiplication, RecMatMult.  RecMatMult  Input: n ⇥ n integer matrices X and Y. Output: Z = X · Y. Assumption: n is a power of 2.  else  if n = 1 then     base case return the 1 ⇥ 1 matrix with entry X[1][1] · Y[1][1]    recursive case A, B, C, D := submatrices of X as in  3.3  E, F, G, H := submatrices of Y as in  3.3  recursively compute the eight matrix products that appear in  3.4  return the result of the computation in  3.4   The running time of the RecMatMult algorithm is not immediately obvious. What is clear is that there are eight recursive calls, each on an input of half the dimension. Other than making these recursive calls, the only work required is the matrix additions in  3.4 . Since an n ⇥ n matrix has n2 entries, and the number of operations needed to add two matrices is proportional to the number of entries, a recursive call on a pair of `⇥` matrices performs ⇥ `2  operations, not counting the work done by its own recursive calls. Disappointingly, this recursive algorithm turns out to have a running time of ⇥ n3 , the same as the straightforward algorithm.   3.3  Strassen’s Matrix Multiplication Algorithm  75   This follows from the “master method,” explained in the next chapter.  Has all our work been for naught? Remember that in the integer multiplication problem, the key to beating the grade-school algorithm was Gauss’s trick, which reduced the number of recursive calls from four to three  Section 1.3.3 . Is there an analog of Gauss’s trick for matrix multiplication, one that allows us to reduce the number of recursive calls from eight to seven?  3.3.5 Saving a Recursive Call  The high-level plan of the Strassen algorithm is to save one recursive call relative to the RecMatMult algorithm, in exchange for a constant number of additional matrix additions and subtractions.  Strassen  Very High-Level Description   Input: n ⇥ n integer matrices X and Y. Output: Z = X · Y. Assumption: n is a power of 2.  if n = 1 then  else     base case return the 1 ⇥ 1 matrix with entry X[1][1] · Y[1][1]    recursive case A, B, C, D := submatrices of X as in  3.3  E, F, G, H := submatrices of Y as in  3.3  recursively compute seven  cleverly chosen  products involving A, B, . . . , H return the appropriate  cleverly chosen  additions and subtractions of the matrices computed in the previous step  Saving one of the eight recursive calls is a big win. It doesn’t merely reduce the running time of the algorithm by 12.5%. The recursive call is saved over and over again, so the savings are compounded and—spoiler alert!—this results in an asymptotically superior running time. We’ll see the exact running time bound in Section 4.3, but for now the important thing to know is that saving a recursive call yields an algorithm with subcubic running time.   76  Divide-and-Conquer Algorithms  This concludes all the high-level points you should know about Strassen’s matrix multiplication algorithm. Are you in disbelief that it’s possible to improve over the obvious algorithm? Or curious about exactly how the products and additions are actually chosen? If so, the next section is for you.  3.3.6 The Details  Let X and Y denote the two n ⇥ n input matrices, and deﬁne A, B, . . . , H as in  3.3 . Here are the seven recursive matrix mul- tiplications performed by Strassen’s algorithm:  P1 = A ·  F   H  P2 =  A + B  · H P3 =  C + D  · E P4 = D ·  G   E  P5 =  A + D  ·  E + H  P6 =  B   D  ·  G + H  P7 =  A   C  ·  E + F .  After spending ⇥ n2  time performing the necessary matrix additions and subtractions, P1, . . . , P7 can be computed using seven recursive calls on pairs of n 2 matrices. But is this really enough information to reconstruct the matrix product of X and Y in ⇥ n2  time? The following amazing equation gives an aﬃrmative answer:  2 ⇥ n  X · Y =✓ A · E + B · G A · F + B · H C · E + D · G C · F + D · H ◆  =✓ P5 + P4   P2 + P6  P3 + P4  P1 + P2  P1 + P5   P3   P7 ◆ .  The ﬁrst equation is copied from  3.4 . For the second equation, we need to check that the equality holds in each of the four quadrants. To quell your disbelief, check out the crazy cancellations in the upper-left   *3.4 An O n log n -Time Algorithm for the Closest Pair  77  quadrant:  P5 + P4   P2 + P6 =  A + D  ·  E + H  + D ·  G   E      A + B  · H +  B   D  ·  G + H  = A · E + A · H + D · E + D · H + D · G    D · E   A · H   B · H + B · G + B · H   D · G   D · H  = A · E + B · G.  The computation for the lower-right quadrant is similar, and equality is easy to see in the other two quadrants. So the Strassen algorithm really can multiply matrices with only seven recursive calls and ⇥ n2  additional work!9  3.3.7 Solution to Quiz 3.3  Correct answer:  c . The correct answer is ⇥ n3 . There are three nested for loops. This results in n3 inner loop iterations  one for each choice of i, j, k 2 {1, 2, . . . , n} , and the algorithm performs a constant number of operations in each iteration  one multiplication and one addition . Alternatively, for each of the n2 entries of Z, the algorithm spends ⇥ n  time evaluating  3.1 .  *3.4 An O n log n -Time Algorithm for the Closest Pair  Our ﬁnal example of a divide-and-conquer algorithm is a very cool algorithm for the closest pair problem, in which you’re given n points in the plane and want to ﬁgure out the pair of points that are closest to each other. This is our ﬁrst taste of an application in computational geometry, an area that studies algorithms for reasoning about and  9Of course, checking that the algorithm works is a lot easier than coming up with it in the ﬁrst place. And how did Volker Strassen ever come up with it, back in 1969? Here’s what he said  in a personal communication, June 2017 : “The way I remember it, I had realized that a faster noncommutative algorithm for some small case would give a better exponent. I tried to prove that the straightforward algorithm is optimal for 2 ⇥ 2 matrices. To simplify matters I worked modulo 2, and then discovered the faster algorithm combinatorially.”   78  Divide-and-Conquer Algorithms  manipulating geometric objects, and that has applications in robotics, computer vision, and computer graphics.10  3.4.1 The Problem  The closest pair problem concerns points  x, y  2 R2 in the plane. To measure the distance between two points p1 =  x1, y1  and p2 =  x2, y2 , we use the usual Euclidean  straight-line  distance:  d p1, p2  =p x1   x2 2 +  y1   y2 2.   3.5   Problem: Closest Pair  Input: the plane.  n   2 points p1 =  x1, y1 , . . . , pn =  xn, yn  in  Output: The pair pi, pj of points with smallest Euclidean distance d pi, pj .  For convenience, we’ll assume that no two points have the same x- coordinate or the same y-coordinate. You should think about how to extend the algorithm from this section to accommodate ties.11  The closest pair problem can be solved in quadratic time using brute-force search—just compute the distance between each of the ⇥ n2  pairs of points one-by-one, and return the closest of them. For the counting inversions problem  Section 3.2 , we were able to improve over the quadratic-time brute-force search algorithm with a divide-and-conquer algorithm. Can we also do better here?  3.4.2 Warm-Up: The 1-D Case  Let’s ﬁrst consider the simpler one-dimensional version of the problem: given n points p1, . . . , pn 2 R in arbitrary order, identify a pair that 10Starred sections like this one are the more diﬃcult sections, and they can be  skipped on a ﬁrst reading.  11In a real-world implementation, a closest pair algorithm will not bother to compute the square root in  3.5 —the pair of points with the smallest Euclidean distance is the same as the one with the smallest squared Euclidean distance, and the latter distance is easier to compute.   *3.4 An O n log n -Time Algorithm for the Closest Pair  79  minimizes the distance pi   pj. This special case is easy to solve in O n log n  time using the tools that are already in our toolbox. The key observation is that, whatever the closest pair is, the two points must appear consecutively in the sorted version of the point set  Figure 3.3 .  1-D Closest Pair  sort the points use a linear scan through the sorted points to identify the closest pair  The ﬁrst and second steps of the algorithm can be implemented in O n log n  time  using MergeSort  and O n  time  straightforwardly , respectively, for an overall running time of O n log n . Thus in the one- dimensional case, there is indeed an algorithm better than brute-force search.  closest pair   Figure 3.3: In one dimension, the points in the closest pair appear consec- utively in the sorted version of the point set.  3.4.3 Preprocessing  Can sorting help solve the two-dimensional version of the closest pair problem in O n log n  time? An immediate issue is that there are two diﬀerent coordinates you can use to sort the points. But since sorting is a for-free primitive  see page 31 , why not just do it  twice ? That is, in a preprocessing step, our algorithm makes two copies of the input point set: a copy Px with the points sorted by x-coordinate, and a copy Py sorted by y-coordinate. This takes O n log n  time, which is within the time bound we’re shooting for.  How can we put the sorted versions Px and Py to use? Unfortu- nately, the closest pair of points need not appear consecutively in   80  Divide-and-Conquer Algorithms  either Px or Py  Figure 3.4 . We will have to do something more clever than a simple linear scan.  closest in x-coordinate   closest pair   closest in y-coordinate   Figure 3.4: In two dimensions, the points in the closest pair need not appear consecutively when the points are sorted by x- or y-coordinate.  3.4.4 A Divide-and-Conquer Approach  We can do better with a divide-and-conquer approach.12 How should we divide the input into smaller subproblems, and how can we then combine the solutions of these subproblems into one for the original problem? For the ﬁrst question, we use the ﬁrst sorted array Px to divide the input into its left and right halves. Call a pair of points a left pair if both belong to the left half of the point set, a right pair if both belong to the right half, and a split pair if the points belong to diﬀerent halves. For example, in the point set in Figure 3.4, the closest pair is a split pair, and the pair of points closest in x-coordinate is a left pair.  If the closest pair is a left pair or a right pair, it will be recursively identiﬁed by one of the two recursive calls. We’ll need a special- purpose subroutine for the remaining case, when the closest pair is a split pair. This subroutine plays a similar role to the CountSplitInv subroutine in Section 3.2.  The following pseudocode summarizes these ideas; the subroutine  ClosestSplitPair is, as of now, unimplemented.  12Thus the divide-and-conquer paradigm is used in both the preprocessing step,  to implement MergeSort, and again in the main algorithm.   *3.4 An O n log n -Time Algorithm for the Closest Pair  81  ClosestPair  Preliminary Version   Input: two copies Px and Py of n   2 points in the plane, sorted by x- and y-coordinate, respectively. Output: the pair pi, pj of distinct points with smallest Euclidean distance between them.     base case of <= 3 points omitted  1 Lx := ﬁrst half of Px, sorted by x-coordinate 2 Ly := ﬁrst half of Px, sorted by y-coordinate 3 Rx := second half of Px, sorted by x-coordinate 4 Ry := second half of Px, sorted by y-coordinate 5  l1, l2  := ClosestPair Lx, Ly     best left pair 6  r1, r2  := ClosestPair Rx, Ry     best right pair 7  s1, s2  := ClosestSplitPair Px, Py     best split  pair  8 return best of  l1, l2 ,  r1, r2 ,  s1, s2   In the omitted base case, when there are two or three input points, the algorithm computes the closest pair directly in constant  O 1   time. Deriving Lx and Rx from Px is easy  just split Px in half . To compute Ly and Ry, the algorithm can perform a linear scan over Py, putting each point at the end of either Ly or Ry, according to the point’s x-coordinate. We conclude that lines 1–4 can be implemented in O n  time.  Provided we implement the ClosestSplitPair subroutine cor- rectly, the algorithm is guaranteed to compute the closest pair of points—the three subroutine calls in lines 5–7 cover all possibilities for where the closest pair might be.  Quiz 3.4  Suppose that we correctly implement the ClosestSplitPair subroutine in O n  time. What will be the overall running time of the ClosestPair algorithm?  Choose the smallest upper bound that applies.    82  Divide-and-Conquer Algorithms  a  O n   b  O n log n   c  O n log n 2   d  O n2    See Section 3.4.10 for the solution and discussion.   3.4.5 A Subtle Tweak  The solution to Quiz 3.4 makes our goal clear: we want an O n -time implementation of the ClosestSplitPair subroutine, leading to an overall running time bound of O n log n  and matching the running time of our algorithm for the one-dimensional special case.  We’ll design a slightly weaker subroutine that is adequate for our purposes. Here’s the key observation: we need the ClosestSplitPair subroutine to identify the closest split pair only when it is the closest pair overall. If the closest pair is a left or right pair, ClosestSplitPair might as well return garbage—line 8 of the pseudocode in Section 3.4.4 will ignore its suggested point pair anyway, in favor of the actual closest pair computed by one of the recursive calls. Our algorithm will make crucial use of this relaxed correctness requirement.  To  this  implement  idea, we’ll  explicitly pass  the ClosestSplitPair subroutine the distance   between the closest pair that is a left or right pair; the subroutine then knows that it has to worry only about split pairs with interpoint distance less than  . In other words, we replace lines 7–8 of the pseudocode in Section 3.4.4 with the following.  to  ClosestPair  Addendum   7   := min{d l1, l2 , d r1, r2 } 8  s1, s2  := ClosestSplitPair Px, Py,    9 return best of  l1, l2 ,  r1, r2 ,  s1, s2    *3.4 An O n log n -Time Algorithm for the Closest Pair  83  3.4.6  ClosestSplitPair  We now provide an implementation of the ClosestSplitPair subrou- tine that runs in linear time and correctly computes the closest pair whenever it is a split pair. You may not believe that the following pseudocode satisﬁes these requirements, but it does. The high-level idea is to do brute-force search over a cleverly restricted set of point pairs.  ClosestSplitPair  Input: two copies Px and Py of n   2 points in the plane, sorted by x- and y-coordinate, and a parameter  . Output: the closest pair, provided it is a split pair.     median  between ¯x     and ¯x +  , sorted by y-coordinate}  1 ¯x := largest x-coordinate in left half  x-coordinate 2 Sy := {points q1, q2, . . . , q` with x-coordinate 3 best :=   4 bestP air := N U LL 5 for i := 1 to `   1 do 6 7 8 9 10 return bestP air  for j := 1 to min{7, `   i} do if d qi, qi+j  < best then  best := d qi, qi+j  bestP air :=  qi, qi+j   The subroutine begins in line 1 by identifying the rightmost point in the left half of the point set, which deﬁnes the “median x-coordinate” ¯x. A pair of points is a split pair if and only if one point has x- coordinate at most ¯x and the other greater than ¯x. Computing ¯x takes constant  O 1   time because Px stores the points sorted by x- coordinate  the median is the n 2 th array entry . In line 2 the subroutine performs a ﬁltering step, discarding all points except those lying in the vertical strip of width 2  centered at ¯x  Figure 3.5 . The set Sy can be computed in linear time by scanning through Py and removing any   84  Divide-and-Conquer Algorithms  points with an x-coordinate outside the range of interest.13 Lines 5–9 perform brute-force search over the pairs of points of Sy that have at most 6 points in between them  in the ordering of Sy by y-coordinates , and computes the closest such pair of points.14 You can think of this as an extension of our algorithm for the one-dimensional case, in which we examine all “nearly consecutive” pairs of points. The total number of loop iterations is less than 7`  7n = O n , and the algorithm performs a constant number of primitive operations in each iteration. We conclude that the ClosestSplitPair subroutine runs in O n  time, as desired. But why on Earth should it ever ﬁnd the closest pair?  δ   2δ  x  Figure 3.5: The ClosestSplitPair subroutine. Sy is the set of points enclosed by the vertical strip.   is the smallest distance between a left pair or a right pair of points. The split point pairs have one point on either side of the dotted line.  13This step is the reason why we sorted the point set by y-coordinate once and for all in the initial preprocessing step. Since we’re shooting for a linear-time subroutine, there’s no time to sort them now!  14If there is no such pair of points at distance less than  , then the subroutine returns NULL. In this case, in ClosestPair, this NULL pair is ignored and the ﬁnal comparison is between only the point pairs returned by the two recursive calls.   *3.4 An O n log n -Time Algorithm for the Closest Pair  85  3.4.7 Correctness  The ClosestSplitPair subroutine runs in linear time because, out of the quadratic number of possible point pairs, it searches over only a linear number of them. How do we know it didn’t miss out on the true closest pair? The following lemma, which is a bit shocking, guarantees that when the closest pair is a split pair, its points appear nearly consecutively in the ﬁltered set Sy.  Lemma 3.3 In the ClosestSplitPair subroutine, suppose  p, q  is a split pair with d p, q  <  , where   is the smallest distance between a left pair or right pair of points. Then:   a  p and q will be included in the set Sy;   b  at most six points of Sy have a y-coordinate in between those of  This lemma is far from obvious, and we prove it in the next section. Lemma 3.3 implies that the ClosestSplitPair subroutine does  p and q.  its job.  Corollary 3.4 When the closest pair ClosestSplitPair subroutine returns it.  is a split pair,  the  Proof: Assume that the closest pair  p, q  is a split pair, and so d p, q  <  , where   is the minimum distance between a left or right pair. Then, Lemma 3.3 ensures that both p and q belong to the set Sy in the ClosestSplitPair subroutine, and that there are at most six points of Sy between them in y-coordinate. Since ClosestSplitPair exhaustively searches over all pairs of points that satisfy these two properties, it will compute the closest such pair, which must be the actual closest pair  p, q . QE D  Pending the proof of Lemma 3.3, we now have a correct and  blazingly fast algorithm for the closest pair problem.  Theorem 3.5  Computing the Closest Pair  For every set P of n   2 points in the plane, the ClosestPair algorithm correctly com- putes the closest pair of P and runs in O n log n  time.   86  Divide-and-Conquer Algorithms  Proof: We have already argued the running time bound: the algorithm spends O n log n  time in its preprocessing step, and the rest of the algorithm has the same asymptotic running time as MergeSort  with two recursive calls each on half the input, plus linear additional work , which is also O n log n .  For correctness, if the closest pair is a left pair, it is returned by the ﬁrst recursive call  line 5 in Section 3.4.4 ; if it is a right pair, it is returned by the second recursive call  line 6 . If it is a split pair, then Corollary 3.4 guarantees that it is returned by the ClosestSplitPair subroutine. In all cases, the closest pair is among the three candidates examined by the algorithm  line 9 in Section 3.4.5 , and will be returned as the ﬁnal answer. QE D  3.4.8 Proof of Lemma 3.3 a   Part  a  of Lemma 3.3 is the easier part. Assume that there is a split pair  p, q , with p in the left half of the point set and q in the right half, such that d p, q  <  , where   is the minimum distance between a left or right pair. Write p =  x1, y1  and q =  x2, y2 , and let ¯x denote the x-coordinate of the rightmost point of the left half. Since p and q are in the left and right halves, respectively, we have  x1  ¯x < x2.  At the same time, x1 and x2 cannot be very diﬀerent. Formally, remembering the deﬁnition of Euclidean distance  3.5 , we can write    > d p, q   =p x1   x2 2 +  y1   y2 2  pmax{ x1   x2 2,  y1   y2 2}  = max{x1   x2,y1   y2}.  This means that p and q diﬀer by less than   in both their x- and y-coordinates:  x1   x2,y1   y2 <  .   3.6  Since x1  ¯x and x2 is at most   larger than x1, we have x2  ¯x +  .15 Since x2   ¯x and x1 is at most   smaller than x2, x1   ¯x   . 15Imagine that p and q are people tied at the waist by a rope of length  . The point p can travel only as far rightward as ¯x, which limits q’s travels to ¯x +    Figure 3.6 .   *3.4 An O n log n -Time Algorithm for the Closest Pair  87  In particular, p and q both have x-coordinates that are wedged in between ¯x     and ¯x +  . All such points, including p and q, belong to the set Sy.  ≤ δ   x −δ  x1  x  x2  x +δ  Figure 3.6: Proof of Lemma 3.3 a . Both p and q have x-coordinates between ¯x     and ¯x +  .  3.4.9 Proof of Lemma 3.3 b   Recall our standing assumptions: there is a split pair  p, q , with p =  x1, y1  in the left half of the point set and q =  x2, y2  in the right half, such that d p, q  <  , where   is the minimum distance between a left or right pair. Lemma 3.3 b  asserts that p and q not only appear in the set Sy  as proved in part  a  , but that they are nearly consecutive, with at most six other points of Sy possessing y-coordinates between y1 and y2.  For the proof, we draw eight boxes in the plane in a 2 ⇥ 4 pattern, where each box has side length   2  Figure 3.7 . There are two columns of boxes on either side of ¯x, the median x-coordinate. The bottom of the boxes is aligned with the lower of the points p and q, at the y-coordinate min{y1, y2}.16 From part  a , we know that both p and q have x-coordinates between ¯x     and ¯x +  . For concreteness, suppose q has the smaller y-coordinate; the other case is analogous. Thus, q appears at the bottom of some box on the bottom row  in the right half . Since p’s y-coordinate can only be   larger than q’s  see  3.6  , p also appears in one of the boxes  in the left half . Every point of Sy with y- coordinate between p and q has x-coordinate between ¯x    and ¯x +   16Don’t forget: these boxes are purely for the sake of reasoning about why the ClosestPair algorithm is correct. The algorithm itself knows nothing about these boxes, and remains just the pseudocode in Sections 3.4.4–3.4.6.   88  Divide-and-Conquer Algorithms  p  δ 2  x −δ  b  a  q  x  x +δ  Figure 3.7: Proof of Lemma 3.3 b . The points p and q inhabit two of these eight boxes, and there is at most one point in each box.   the requirement for membership in Sy  and y-coordinate between y2 and y1 < y2 +  , and hence lies in one of the eight boxes.  The worry is that there are lots of points in these boxes that have y-coordinate between y1 and y2. To show that this can’t happen, let’s prove that each box contains at most one point. Then, the eight boxes contain at most eight points  including p and q , and there can only be six points of Sy in between p and q in y-coordinate.17  Why does each box have at most one point? This is the part of the argument that uses our observation in Section 3.4.5 and the fact that   is the smallest distance between a left pair or a right pair. To derive a contradiction, suppose that some box has two points, a and b  one of which might be p or q . This point pair is either a left pair  if the points are in the ﬁrst two columns  or a right pair  if they are in the last two . The farthest apart that a and b can be is at opposite corners of the box  Figure 3.7 , in which case, by the Pythagorean theorem18, the distance between a and b is p2 ·   2 <  . But this contradicts the assumption that there is no left or right pair at a distance less than  ! This contradiction implies that each of the eight boxes in Figure 3.7 has at most one point; hence, at most six points of Sy have a y-coordinate between those of p and q. QE D  17If a point has x-coordinate exactly ¯x, count it toward the box to its left. Other points on the boundary of multiple boxes can be assigned arbitrarily to one of them.  18For a right triangle, the sum of the squares of the sides equals the square of  the hypotenuse.   *3.4 An O n log n -Time Algorithm for the Closest Pair  89  3.4.10 Solution to Quiz 3.4  Correct answer:  b . The correct answer is O n log n . O n  is not correct because, among other reasons, the ClosestPair algorithm already spends ⇥ n log n  time in its preprocessing step creating the sorted lists Px and Py. The upper bound of O n log n  follows from the exact same argument as for MergeSort: the ClosestPair algorithm makes two recursive calls, each on an input of half the size, and performs O n  work outside its recursive calls.  Recall that lines 1–4 and 8 can be implemented in O n  time, and for this quiz we are assuming that ClosestSplitPair also runs in linear time.  This pattern perfectly matches the one we already analyzed for MergeSort in Section 1.5, so we know that the total number of operations performed is O n log n . Since the preprocessing step also runs in O n log n  time, the ﬁnal running time bound is O n log n .  The Upshot  P A divide-and-conquer algorithm divides the in- put into smaller subproblems, conquers the sub- problems recursively, and combines the subprob- lem solutions into a solution for the original problem.  P Computing the number of inversions in an array is relevant for measuring similarity between two ranked lists. The brute-force search algorithm for the problem runs in ⇥ n2  time for arrays of length n.  P There is a divide-and-conquer algorithm that piggybacks on MergeSort and computes the number of inversions in O n log n  time.  P Strassen’s subcubic-time divide-and-conquer al- gorithm for matrix multiplication is a mind- blowing example of how algorithmic ingenuity can improve over straightforward solutions. The key idea is to save a recursive call over a sim-   90  Divide-and-Conquer Algorithms  pler divide-and-conquer algorithm, analogous to Karatsuba multiplication.  P In the closest pair problem, the input is n points in the plane, and the goal is to compute the pair of points with smallest Euclidean distance between them. The brute-force search algorithm runs in ⇥ n2  time.  P There is a sophisticated divide-and-conquer al- gorithm that solves the closest pair problem in O n log n  time.  Test Your Understanding  Problem 3.1 Consider the following pseudocode for calculating ab, where a and b are positive integers:19  FastPower  Input: positive integers a and b. Output: ab.  if b = 1 then  return a  else  c := b · b ans := FastPower c,bb 2c  if b is odd then return a · ans return ans  else  Assume for this problem that each multiplication and division can be performed in constant time. What is the asymptotic running time of this algorithm, as a function of b?  19The notation bxc denotes the “ﬂoor” function, which rounds its argument  down to the nearest integer.   Problems  a  ⇥ log b  b  ⇥ pb  c  ⇥ b   d  ⇥ b log b   91  Challenge Problems  Problem 3.2 You are given a unimodal array of n distinct elements, meaning that its entries are in increasing order up until its maximum element, after which its elements are in decreasing order. Give an algorithm to compute the maximum element of a unimodal array that runs in O log n  time.  Problem 3.3 You are given a sorted  from smallest to largest  array A of n distinct integers which can be positive, negative, or zero. You want to decide whether or not there is an index i such that A[i] = i. Design the fastest algorithm you can for solving this problem.  Problem 3.4  Diﬃcult.  You are given an n-by-n grid of distinct numbers. A number is a local minimum if it is smaller than all its neighbors.  A neighbor of a number is one immediately above, below, to the left, or to the right. Most numbers have four neighbors; numbers on the side have three; the four corners have two.  Use the divide-and-conquer algorithm design paradigm to compute a local minimum with only O n  comparisons between pairs of numbers.  Note: since there are n2 numbers in the input, you cannot aﬀord to look at all of them.  [Hint: Figure out how to recurse on an n O n  work.]  2 grid after doing only  2 -by- n  Programming Problems  Problem 3.5 Implement in your favorite programming language the CountInv algorithm from Section 3.2 for counting the number of inversions of an array.  See www.algorithmsilluminated.org for test cases and challenge data sets.    Chapter 4  The Master Method  This chapter presents a “black-box” method for determining the run- ning time of recursive algorithms—plug in a few key characteristics of the algorithm, and out pops an upper bound on the algorithm’s running time. This “master method” applies to most of the divide- and-conquer algorithms you’ll ever see, including Karatsuba’s integer multiplication algorithm  Section 1.3  and Strassen’s matrix multipli- cation algorithm  Section 3.3 .1 This chapter also illustrates a more general theme in the study of algorithms: properly evaluating novel algorithmic ideas often requires non-obvious mathematical analysis. After introducing recurrences in Section 4.1, we give a formal statement of the master method  Section 4.2  and look at six example applications  Section 4.3 . Section 4.4 covers the proof of the master method, with an emphasis on the meaning behind its famous three cases. The proof builds nicely on our analysis of the MergeSort algorithm in Section 1.5.  4.1  Integer Multiplication Revisited  To motivate the master method, let’s recall the main points of our integer multiplication discussion  Sections 1.2–1.3 . The problem is to multiply two n-digit numbers, where the primitive operations are the addition or multiplication of two single-digit numbers. The iterative grade-school algorithm requires ⇥ n2  operations to multiply two n-digit numbers. Can we do better with a divide-and-conquer approach?  1The master method is also called the “master theorem.”  92   4.1  Integer Multiplication Revisited  93  4.1.1 The RecIntMult Algorithm  The RecIntMult algorithm from Section 1.3 creates smaller subprob- lems by breaking the given n-digit numbers x and y into their ﬁrst and second halves: x = 10n 2 · a + b and y = 10n 2 · c + d, where a, b, c, d are n 2-digit numbers  assuming n is even, for simplicity . For example, if x = 1234, then a = 12 and b = 34. Then x · y = 10n ·  a · c  + 10n 2 ·  a · d + b · c  + b · d,   4.1  which shows that multiplying two n-digit numbers reduces to multi- ply four pairs of n 2-digit numbers, plus O n  additional work  for appending zeroes appropriately and grade-school addition .  The way to describe this formally is by a recurrence. Let T  n  denote the maximum number of operations used by this recursive algorithm to multiply two n-digit numbers—this is the quantity we want to bound from above. A recurrence expresses a running time bound T  n  in terms of the number of operations performed by recursive calls. The recurrence for the RecIntMult algorithm is  2⌘ 4 · T⇣ n  {z }  T  n    +  .  work done by recursive calls  work done outside recursive calls  Like a recursive algorithm, a recurrence also needs a base case, which states what T  n  is for values of n that are too small to trigger any recursive calls. Here, the base case is when n = 1, and the algorithm just performs a single multiplication, so T  1  = 1.  O n   {z}  4.1.2 The Karatsuba Algorithm  Karatsuba’s recursive algorithm for integer multiplication uses a trick due to Gauss to save one recursive call. The trick is to recursively compute the products of a and c, b and d, and a + b and c + d, and extract the middle coeﬃcient a · d + b · c via  a + b  c + d    ac   bd. This is enough information to compute the right-hand side of  4.1  with O n  additional primitive operations.  Quiz 4.1  Which recurrence best describes the running time of the Karatsuba algorithm for integer multiplication?   94  The Master Method  2  + O n2   a  T  n   2 · T  n b  3 · T  n 2  + O n  2  + O n2  c  3 · T  n d  4 · T  n 2  + O n    See below for the solution and discussion.   Correct answer:  b . The only change from the RecIntMult algo- rithm is that the number of recursive calls has dropped by one. It’s true that the amount of work done outside the recursive calls is larger in the Karatsuba algorithm, but only by a constant factor that gets suppressed in the big-O notation. The appropriate recurrence for the Karatsuba algorithm is therefore  T  n    3 · T⇣ n 2⌘ } {z   work done by recursive calls again with the base case T  1  = 1.2  work done outside recursive calls  4.1.3 Comparing the Recurrences  At the moment, we don’t know the running time of RecIntMult or Karatsuba, but inspecting their recurrences suggests that the latter can only be faster than the former. Another point of comparison is the MergeSort algorithm, where our analysis in Section 1.5 leads to the recurrence  T  n    2⌘ 2 · T⇣ n  {z }  work done by recursive calls  work done outside recursive calls  where n is the length of the array to be sorted. This suggests that our running time bounds for both the RecIntMult and Karatsuba algorithms cannot be better than our bound for MergeSort, which is O n log n . Beyond these clues, we really have no idea what the running time of either algorithm is. Enlightenment awaits with the master method, discussed next.  2Technically, the recursive call on a + b and c + d might involve   n  2 + 1 -digit numbers. Among friends, let’s ignore this—it doesn’t matter in the ﬁnal analysis.  O n   {z}  O n   {z}  +  +  ,  ,   4.2  Formal Statement  95  4.2 Formal Statement  The master method is exactly what you’d want for analyzing recursive algorithms. It takes as input the recurrence for the algorithm and— boom—spits out as output an upper bound on the running time of the algorithm.  4.2.1 Standard Recurrences  We’ll discuss a version of the master method that handles what we’ll call “standard recurrences,” which have three free parameters and the following form.3  Standard Recurrence Format  Base case: T  n  is at most a constant for all suﬃciently small n.4  General case: for larger values of n,  Parameters:  b⌘ + O nd .  T  n   a · T⇣ n   a = number of recursive calls   b = input size shrinkage factor   d = exponent in running time of the “combine step”  The base case of a standard recurrence asserts that once the input size is so small that no recursive calls are needed, the problem can be solved in O 1  time. This will be the case for all the applications we consider. The general case assumes that the algorithm makes a recursive calls, each on a subproblem with size a b factor smaller than its input, and does O nd  work outside these recursive calls. For 3This presentation of the master method draws inspiration from Chapter 2 of Algorithms, by Sanjoy Dasgupta, Christos Papadimitriou, and Umesh Vazirani  McGraw-Hill, 2006 .  4Formally, there exist positive integers n0 and c, independent of n, such that  T  n   c for all n  n0.   96  The Master Method  example, in the MergeSort algorithm, there are two recursive calls  a = 2 , each on an array of half the size of the input  b = 2 , and O n  work is done outside the recursive calls  d = 1 . In general, a can be any positive integer, b can be any real number bigger than 1  if b  1 then the algorithm won’t terminate , and d can be any nonnegative real number, with d = 0 indicating only constant  O 1   work beyond the recursive calls. As usual, we ignore the detail that b might need to be rounded up or down to an integer—and as usual, n it doesn’t aﬀect our ﬁnal conclusions. Never forget that a, b, and d should be constants—numbers that are independent of the input size n.5 Typical values for these parameters are 1  for a and d , 2, 3, and 4. If you ever ﬁnd yourself saying something like “apply the master method with a = n or b = n n 1,” you’re using it incorrectly. One restriction in standard recurrences is that every recursive call is on a subproblem of the same size. For example, an algorithm that recurses once on the ﬁrst third of an input array and once on the rest would lead to a non-standard recurrence. Most  but not all  natural divide-and-conquer algorithms lead to standard recurrences. For example, in the MergeSort algorithm, both recursive calls operate on problems with size half that of the input array. In our recursive integer multiplication algorithms, recursive calls are always given numbers with half as many digits.6  4.2.2 Statement and Discussion of the Master Method  We can now state the master method, which provides an upper bound on a standard recurrence as a function of the key parameters a, b, and d.  Theorem 4.1  Master Method  If T  n  is deﬁned by a standard recurrence, with parameters a   1, b > 1, and d   0, then  O nd log n  O nd  O nlogb a   if a = bd if a < bd if a > bd  [Case 1] [Case 2] [Case 3].   4.2   5There are also the constants suppressed in the base case and in the “O nd ” term, but the conclusion of the master method does not depend on their values. 6There are more general versions of the master method that accommodate a wider family of recurrences, but the simple version here is suﬃcient for almost any divide-and-conquer algorithm you’re likely to encounter.  T  n  =8<:   4.3  Six Examples  97  What’s up with the three cases, and why are the relative values of a and bd so important? In the second case, could the running time of the whole algorithm really be only O nd , when the outermost recursive call already does O nd  work? And what’s the deal with the exotic-looking running time bound in the third case? By the end of this chapter we’ll learn satisfying answers to all of these questions, and the statement of the master method will seem like the most natural thing in the world.7  More On Logarithms  Another puzzling aspect of Theorem 4.1 concerns the inconsistent use of logarithms. The third case carefully states that the logarithm in question is base- b—the number of times you can divide n by b before the result is at most 1. Meanwhile, the ﬁrst case does not specify the base of the logarithm at all. The reason is that any two logarithmic functions diﬀer only by a constant factor. For example, the base- 2 logarithm always exceeds the natural logarithm  i.e., the base-e logarithm, where e = 2.718 . . .  by a factor of 1  ln 2 ⇡ 1.44. In the ﬁrst case of the master method, changing the base of the logarithm only changes the constant factor that is conveniently suppressed in the big-O notation. In the third case, the logarithm appears in the exponent, where diﬀerent constant factors translate to very diﬀerent running time bounds  like n2 vs. n100 !  4.3  Six Examples  The master method  Theorem 4.1  is hard to get your head around the ﬁrst time you see it. Let’s instantiate it in six diﬀerent examples. 7The bounds in Theorem 4.1 have the form O f  n   rather than ⇥ f  n   because in our recurrence we only assume an upper bound on T  n . If we replace “” with “=” and O nd  with ⇥ nd  in the deﬁnition of a standard recurrence, the bounds in Theorem 4.1 hold with O ·  replaced by ⇥ · . Verifying this is a good way to check your understanding of the proof in Section 4.4.   98  The Master Method  4.3.1  MergeSort Revisited  As a sanity check, let’s revisit an algorithm whose running time we already know, MergeSort. To apply the master method, all we need to do is identify the values of the three free parameters: a, the number of recursive calls; b, the factor by which the input size shrinks prior to the recursive calls; and d, the exponent in the bound on the amount of work done outside the recursive calls.8 In MergeSort, there are two recursive calls, so a = 2. Each recursive call receives half of the input array, so b = 2 as well. The work done outside these recursive calls is dominated by the Merge subroutine, which runs in linear time  Section 1.5.1 , and so d = 1. Thus  a = 2 = 21 = bd,  putting us in the ﬁrst case of the master method. Plugging in the parameters, Theorem 4.1 tells us that the running time of MergeSort is O nd log n  = O n log n , thereby replicating our analysis in Sec- tion 1.5.  4.3.2 Binary Search  For our second example, we consider the problem of searching a sorted array for a given element. Think, for example, of searching for your own name in an alphabetical list in a large book.9 You could search linearly starting from the beginning, but this would squander the advantage that the list is in alphabetical order. A smarter approach is to look in the middle of the book and recurse on either its ﬁrst half  if the name in the middle comes after your own  or its second half  otherwise . This algorithm, translated to the problem of searching a sorted array, is known as binary search.10  What’s the running time of binary search? This question is easy to answer directly, but let’s see how the master method handles it.  8All of the recurrences we consider have a base case in the form required for  standard recurrences, and we won’t discuss them from here on out.  9Readers of at least a certain age should be reminded of a phone book. 10If you haven’t walked through the code of this algorithm before, look it up  in your favorite introductory programming book or tutorial.   4.3  Six Examples  99  Quiz 4.2  What are the respective values of a, b, and d for the binary search algorithm?  a  1, 2, 0 [case 1]  b  1, 2, 1 [case 2]  c  2, 2, 0 [case 3]  d  2, 2, 1 [case 1]   See Section 4.3.7 for the solution and discussion.   4.3.3 Recursive Integer Multiplication  Now we get to the good stuﬀ, divide-and-conquer algorithms for which we don’t yet know a running time bound. Let’s begin with the RecIntMult algorithm for integer multiplication. We saw in Section 4.1 that the appropriate recurrence for this algorithm is  and so a = 4, b = 2, and d = 1. Thus  T  n   4 · T⇣ n  2⌘ + O n ,  a = 4 > 2 = 21 = bd,  putting us in the third case of the master method. In this case, we obtain the exotic-looking running time bound of O nlogb a . For our parameter values, this is O nlog2 4  = O n2 . Thus the RecIntMult algorithm matches but does not outperform the iterative grade-school algorithm for integer multiplication  which uses ⇥ n2  operations .  4.3.4 Karatsuba Multiplication  A divide-and-conquer approach to integer multiplication pays oﬀ only once Gauss’s trick is used to save a recursive call. As we saw in Section 4.1, the running time of the Karatsuba algorithm is governed by the recurrence  T  n   3 · T⇣ n  2⌘ + O n ,   100  The Master Method  which diﬀers from the previous recurrence only in that a has dropped from 4 to 3  b is still 2, d is still 1 . We expect the running time to be somewhere between O n log n   the bound when a = 2, as in MergeSort  and O n2   the bound when a = 4, as in RecIntMult . If the suspense is killing you, the master method oﬀers a quick resolution: we have  a = 3 > 2 = 21 = bd,  and so we are still in the third case of the master method, but with an improved running time bound: O nlogb a  = O nlog2 3  = O n1.59 . Thus saving a recursive call leads to a fundamentally better running time, and the integer multiplication algorithm that you learned in the third grade is not the fastest possible!11  4.3.5 Matrix Multiplication  Section 3.3 considered the problem of multiplying two n ⇥ n matri- ces. As with integer multiplication, we discussed three algorithms— a straightforward iterative algorithm, the straightforward recursive RecMatMult algorithm, and the ingenious Strassen algorithm. The iterative algorithm uses ⇥ n3  operations  Quiz 3.3 . The RecMatMult algorithm breaks each of the two input matrices into four n 2 matri- ces  one for each quadrant , performs the corresponding eight recur- sive calls on smaller matrices, and combines the results appropriately  using straightforward matrix addition . The Strassen algorithm cleverly identiﬁes seven pairs of n 2 matrices whose products suﬃce to reconstruct the product of the original input matrices.  2 ⇥ n  2 ⇥ n  Quiz 4.3  What running time bounds does the master method provide for the RecMatMult and Strassen algorithms, respectively?  a  O n3  and O n2   b  O n3  and O nlog2 7   11Fun fact: in the Python programming language, the built-in subroutine for multiplying integer objects uses the grade-school algorithm for integers with at most 70 digits, and the Karatsuba algorithm otherwise.   4.3  Six Examples  101  c  O n3  and O n3   d  O n3 log n  and O n3    See Section 4.3.7 for the solution and discussion.   4.3.6 A Fictitious Recurrence  In our ﬁve examples thus far, two recurrences have fallen in the ﬁrst case of the master method, and the rest in the third case. There are also naturally occurring recurrences that fall in the second case. For example, suppose we have a divide-and-conquer algorithm that operates like MergeSort, except that the algorithm works harder outside the recursive calls, doing a quadratic rather than linear amount of work. That is, consider the recurrence  Here, we have  T  n   2 · T⇣ n  2⌘ + O n2 .  a = 2 < 4 = 22 = bd,  putting us squarely in the second case of the master method, for a run- ning time bound of O nd  = O n2 . This might seem counterintuitive; given that the MergeSort algorithm does linear work outside the two recursive calls and has a running time of O n log n , you might expect that a quadratic-time combine step would lead to a running time of O n2 log n . The master method shows this to be an overestimate, and provides the better upper bound of O n2 . Remarkably, this means that the total running time of the algorithm is dominated by the work done in the outermost call—all subsequent recursive calls only increase the total number of operations performed by a constant factor.12  4.3.7 Solutions to Quizzes 4.2–4.3  Solution to Quiz 4.2  Correct answer:  a . Binary search recurses on either the left half of the input array or the right half  never both , so there is only one 12We’ll see another example of case 2 of the master method when we discuss  linear-time selection in Chapter 6.   102  The Master Method  recursive call  a = 1 . This recursive call is on half of the input array, so b is again equal to 2. Outside the recursive call, all binary search does is a single comparison  between the middle element of the array and the element being searched for  to determine whether to recurse on the left or the right half of the array. This translates to O 1  work outside the recursive call, so d = 0. Since a = 1 = 20 = bd, we are again in the ﬁrst case of the master method, and we get a running time bound of O nd log n  = O log n .  Solution to Quiz 4.3  Correct answer:  b . Let’s start with the RecMatMult algorithm  Section 3.3.4 . Let T  n  denote the maximum number of primitive operations that the algorithm uses to multiply two n ⇥ n matrices. The number a of recursive calls is 8. Each of these calls is on a pair of 2 matrices, so b = 2. The work done outside the recursive calls 2 ⇥ n n involves a constant number of matrix additions, and these require O n2  time  constant time for each of the n2 matrix entries . Thus the recurrence is  and since  T  n   8 · T⇣ n  2⌘ + O n2 ,  a = 8 > 4 = 22 = bd,  we are in the third case of the master method, which gives a running time bound of O nlogb a  = O nlog2 8  = O n3 .  The only diﬀerence between the recurrence for the Strassen algo- rithm and the recurrence above is that the number a of recursive calls drops from 8 to 7. It’s true that the Strassen algorithm does more matrix additions than RecMatMult, but only by a constant factor, and hence d is still equal to 2. Thus  a = 7 > 4 = 22 = bd.  We are still in the third case of the master method, but with an improved running time bound: O nlogb a  = O nlog2 7  = O n2.81 . Thus the Strassen algorithm really is asymptotically superior to the   *4.4 Proof of the Master Method  103  straightforward iterative algorithm!13  *4.4 Proof of the Master Method  This section proves the master method  Theorem 4.1 : governed by a standard recurrence, of the form  if T  n  is  T  n   a · T⇣ n  b⌘ + O nd ,  O nd log n  O nd  O nlogb a   if a = bd if a < bd if a > bd  [Case 1] [Case 2] [Case 3].  then  T  n  =8<:  It’s important to remember the meanings of the three free parameters:  Parameter  Meaning  a b d  number of recursive calls factor by which input size shrinks in recursive call exponent of work done outside recursive calls  4.4.1 Preamble  The proof of the master method is important not because we care about formality for its own sake, but because it provides the fundamental explanation for why things are the way they are—for example, why the master method has three cases. With this in mind, you should distinguish between two types of content in the proof. At a couple points we will resort to algebraic computations to understand what’s going on. These calculations are worth seeing once in your life, but they are not particularly important to remember in the long term. What is worth remembering is the conceptual meaning of the three cases of the master method. The proof will use the recursion tree approach that served us so well for analyzing the MergeSort algorithm  Section 1.5 , and the three cases correspond to three diﬀerent types 13There is a long line of research papers that devise increasingly sophisticated matrix multiplication algorithms with ever-better worst-case asymptotic running times  albeit with large constant factors that preclude practical implementations . The current world record is a running time bound of roughly O n2.3729 , and for all we know there could be an O n2 -time algorithm waiting to be discovered.   104  The Master Method  of recursion trees. If you can remember the meaning of the three cases, there is no need to memorize the running times in the master method—you will be able to reverse engineer them as needed from your conceptual understanding of it.  For the formal proof, we should explicitly write out all the constant  factors in the recurrence:  Base case: T  1   c. General case: for n > 1,  T  n   a · T⇣ n  b⌘ + cnd.   4.3   For simplicity we’re assuming that the constant n0 specifying when the base case kicks in is 1; the proof for a diﬀerent constant n0 is pretty much the same. We can assume that the suppressed constants in the base case and the O nd  term in the general case are equal to the same number c; if they were diﬀerent constants, we could just work with the larger of the two. Finally, let’s focus on the case in which n is a power of b. The proof for the general case is similar, with no additional conceptual content, but is more tedious.  4.4.2 Recursion Trees Revisited  The high-level plan for the proof is as natural as could be: generalize the recursion tree argument for MergeSort  Section 1.5  so that it accommodates other values of the key parameters a, b, and d. Recall that a recursion tree provides a principled way to keep track of all the work done by a recursive algorithm, across all its recursive calls. Nodes of the tree correspond to recursive calls, and the children of a node correspond to the recursive calls made by that node  Figure 4.1 . Thus the root  level 0  of the recursion tree corresponds to the outermost call to the algorithm, level 1 has a nodes corresponding to its recursive calls, and so on. The leaves at the bottom of the tree correspond to recursive calls where the base case is triggered.  As in our analysis of MergeSort, we’d like to account level-by-level for the work performed by a recursive algorithm. This plan requires understanding two things: the number of distinct subproblems at a given recursion level j, and the length of the input to each of these subproblems.   *4.4 Proof of the Master Method  105  level 0    outermost call     ﬁrst recursive   level 1   calls    original input   branching factor = a   subproblem 1   ......   subproblem a   level 2   ......   ......   .  .  .  .   .  .  .  .   .  .  .  .   .  .  .  .   .  .  .  .   level logb n: leaves  base cases    Figure 4.1: The recursion tree corresponding to a standard recurrence. Nodes correspond to recursive calls. Level 0 corresponds to the outermost call, level 1 to its recursive calls, and so on.  Quiz 4.4  What is the pattern? Fill in the blanks in the following statement: at each level j = 0, 1, 2, . . . of the recursion tree, there are [blank] subproblems, each operating on a subarray of length [blank].  a  aj and n aj, respectively  b  aj and n bj, respectively  c  bj and n aj, respectively  d  bj and n bj, respectively   See Section 4.4.10 for the solution and discussion.    106  The Master Method  4.4.3 Work Performed at a Single Level  Inspired by our MergeSort analysis, the plan is to count the total number of operations performed by the level-j subproblems in a divide- and-conquer algorithm, and then add up over all the levels. So zoom in on a recursion level j. By the solution to Quiz 4.4, there are aj diﬀerent subproblems at level j, each with an input with size n bj. We care only about the size of a subproblem inasmuch as it determines the amount of work the recursive call performs. Our recurrence  4.3  asserts that the work performed in a level-j subproblem, not counting the work performed in its recursive calls, is at most a constant times the input size raised to the d power: c n bj d. Adding up over all aj of the level-j subproblems gives an upper bound on the amount of work performed at level j of the recursion tree:  work at level j    of subproblems  ·  aj  {z}  work per subproblem  .  d  {  } z c ·h n bji {z}  input size  Let’s simplify this expression by separating out the parts that depend on the level j and the parts that don’t:  work at level j  cnd ·h a bdij  .  The right-hand side marks the grand entrance of the critical ratio a bd. Given that the value of a versus bd is exactly what dictates the relevant case of the master method, we shouldn’t be surprised that this ratio shows up explicitly in the analysis.  4.4.4 Summing over the Levels  How many levels are there? The input size is initially n and drops by a factor of b with each level. Since we’re assuming that n is a power of b and that the base case kicks in when the input size is 1, the number of levels is exactly the number of times you need to divide n by b to reach 1, also known as logb n. Summing over all the levels j = 0, 1, 2, . . . , logb n we obtain the following inscrutable upper bound on the running time  using that nd is independent of j : and can be   107   4.4   *4.4 Proof of the Master Method  yanked out front :  total work  cnd ·  logb nXj=0 h a bdij  .  Believe it or not, we’ve reached an important milestone in the proof of the master method. The right-hand side of  4.4  probably looks like alphabet soup, but with the proper interpretation, it holds the keys that unlock a deep understanding of the master method.  4.4.5 Good vs. Evil: The Need for Three Cases  Next we’ll attach some semantics to the running time bound in  4.4  and develop some intuition about why the running time bounds in the master method are what they are.  Why is the ratio of a vs. bd so important? Fundamentally, this comparison represents a tug-of-war between the forces of good and the forces of evil. Evil is represented by a, the rate of subproblem proliferation  RSP —with every level of recursion, the number of subproblems explodes by an a factor, and this is a little scary. Good takes the form of bd, the rate of work shrinkage  RWS —the good news is that with every level of recursion, the amount of work per subproblem decreases by a factor of bd.14 The key question then is: which side wins, the forces of good or the forces of evil? The three cases of the master method correspond exactly to the three possible outcomes of this tug-of-war: a draw  RSP = RW S , a victory for good  RSP   RW S .  To understand this better, spend some time thinking about the amount of work done at each level of a recursion tree  as in Figure 4.1 . When is the amount of work performed increasing with the recursion tree level j? When is it decreasing? Is it ever the same at every level?  14Why bd instead of b? Because b is the rate at which the input size shrinks, and we care about input size only inasmuch as it determines the amount of work performed. For example, in a divide-and-conquer algorithm with a quadratic-time combine step  d = 2 , when the input size is cut in half  b = 2 , only 25% as much work is needed to solve each smaller subproblem  since bd = 4 .   108  The Master Method  Quiz 4.5  Which of the following statements are true?  Choose all that apply.   a  If RSP < RW S then the amount of work performed  is decreasing with the recursion level j.  b  If RSP > RW S then the amount of work performed  is increasing with the recursion level j.  c  No conclusions can be drawn about how the amount of work varies with the recursion level j unless RSP = RW S.  d  If RSP = RW S then the amount of work performed  is the same at every recursion level.   See Section 4.4.10 for the solution and discussion.   4.4.6 Forecasting the Running Time Bounds  We now understand why the master method has three cases. There are three fundamentally diﬀerent types of recursion trees—with the work- per-level staying the same, decreasing, or increasing—and the relative sizes of a  the RSP   and bd  the RW S  determine the recursion tree type of a divide-and-conquer algorithm.  Even better, we now have enough intuition to accurately forecast the running time bounds that appear in the master method. Consider the ﬁrst case, when a = bd and the algorithm performs the same amount of work at every level of its recursion tree. We certainly know how much work is done at the root, in level 0—O nd , as explicitly speciﬁed in the recurrence. With O nd  work per level, and with 1 + logb n = O log n  levels, we should expect a running time bound of O nd log n  in this case  cf., case 1 of Theorem 4.1 .15  In the second case, a < bd and the forces of good are victorious— the amount of work performed is decreasing with the level. Thus more work is done at level 0 than at any other level. The simplest and best outcome we could hope for is that the work done at the root  15The abbreviation “cf.” stands for confer and means “compare to.”   *4.4 Proof of the Master Method  109  dominates the running time of the algorithm. Since O nd  work is done at the root, this best-case scenario would translate to an overall running time of O nd   cf., case 2 of Theorem 4.1 .  In the third case, when subproblems proliferate even faster than the work-per-subproblem shrinks, the amount of work performed is increasing with the recursion level, with the most work being done at the leaves of the tree. Again, the simplest- and best-case scenario would be that the running time is dominated by the work done at the leaves. A leaf corresponds to a recursive call where the base case is triggered, so the algorithm performs only O 1  operations per leaf. How many leaves are there? From the solution to Quiz 4.4, we know that there are aj nodes at each level j. The leaves are at the last level j = logb n, so there are alogb n leaves. Thus, the best-case scenario translates to a running time bound of O alogb n .  The remaining mystery is the connection between our forecasted running time bound for the third case of the master method  O alogb n   and the actual bound that appears in Theorem 4.1  O nlogb a  . The connection is. . . they are exactly the same! The identity  more intuitive  easier to apply  alogb n  {z}  = nlogb a  {z}  probably looks like a rookie mistake made by a freshman algebra student, but it’s actually true.16 Thus the running time bound of O nlogb a  just says that the work performed at the leaves of the recursion tree dominates the computation, with the bound stated in a form convenient for plugging in parameters  as for the integer and matrix multiplication algorithms analyzed in Section 4.3 .  4.4.7 The Final Calculations: Case 1  We still need to check that our intuition in the previous section is actually correct, and the way to do this is through a formal proof. The culmination of our previous calculations was the following scary- looking upper bound on the running time of a divide-and-conquer  16To verify it, just take the logarithm base-b of both sides:  logb alogb n  = logb n·logb a = logb a·logb n = logb nlogb a .  And since logb is a strictly increasing function, the only way logb x and logb y can be equal is if x and y are equal.    110  The Master Method  algorithm, as a function of the parameters a, b, and d:  total work  cnd ·  logb nXj=0 h a bdij  .   4.5   We obtained this bound by zooming in on a particular level j of the recursion tree  with its aj subproblems and c n bj d work-per- subproblem  and then summing up over the levels.  When the forces of good and evil are in perfect equilibrium  i.e., a = bd  and the algorithm performs the same amount of work at every level, the right-hand side of  4.5  simpliﬁes dramatically:  cnd ·  logb nXj=0  j  a bd  264 375 {z}=1  {z }  =1 for each j  which is O nd log n .17  = cnd ·  1 + 1 + ··· + 1  } 1 + logb n times  {z    ,  4.4.8 Detour: Geometric Series  Our hope is that for the second and third types of recursion trees  decreasing- and increasing-work-per-level, respectively , the overall running time is dominated by the work performed at the most diﬃcult level  the root and the leaves, respectively . Making this hope real requires understanding geometric series, which are expressions of the form 1 + r + r2 + ··· + rk for some real number r and nonnegative integer k.  For us, r will be the critical ratio a bd.  Whenever you see a parameterized expression like this, it’s a good idea to keep a couple of canonical parameter values in mind. For example, if r = 2, it’s a sum of positive powers of 2: 1 + 2 + 4 + 8 + ··· + 2k. When 2k . 2, it’s a sum of negative powers of 2: 1 + 1 8 + ··· + 1 r = 1 When r 6= 1, there is a useful closed-form formula for a geometric  4.6   series:18  4 + 1  2 + 1  1 + r + r2 + ··· + rk =  .  17Remember that since diﬀerent logarithmic functions diﬀer by a constant  1   rk+1 1   r  factor, there is no need to specify the base of the logarithm.  18To verify this identity, just multiply both sides by 1   r:  1   r  1 + r + r2 +  ··· + rk  = 1   r + r   r2 + r2   r3 + r3   ···   rk+1 = 1   rk+1.   *4.4 Proof of the Master Method  111  Two consequences of this formula are important for us. First, when r < 1,  1 + r + r2 + ··· + rk   1 1   r  = a constant  independent of k .  Thus every geometric series with r < 1 is dominated by its ﬁrst term— the ﬁrst term is 1 and the sum is only O 1 . For example, it doesn’t matter how many powers of 1 2 you add up, the resulting sum is never more than 2.  Second, when r > 1,  1 + r + r2 + ··· + rk =  rk+1   1 r   1   rk+1 r   1  = rk ·  r r   1  .  Thus every geometric series with r > 1 is dominated by its last term— the last term is rk while the sum is at most a constant factor  r  r 1   times this. For example, if you sum up the powers of 2 up to 1024, the resulting sum is less than 2048.  4.4.9 The Final Calculations: Cases 2 and 3  Returning to our analysis of  4.5 , suppose that a < bd. In this case, the proliferation in subproblems is drowned out by the savings in work-per-subproblem, and the number of operations performed is decreasing with the recursion tree level. Set r = a bd; since a, b, and d are constants  independent of the input size n , so is r. Since r < 1, the geometric series in  4.5  is at most the constant 1  1   r , and the bound in  4.5  becomes  cnd ·  = O nd ,  rj  logb nXj=0  {z }  =O 1   where the big-O expression suppresses the constants c and 1  1   r . This conﬁrms our hope that, with the second type of recursion tree, the total amount of work performed is dominated by the work done at the root.  For the ﬁnal case, suppose that a > bd, with the proliferation of subproblems outpacing the rate of work shrinkage per-subproblem.   112  The Master Method  Set r = a bd. Since r is now greater than 1, the last term of the geometric series dominates and the bound in  4.5  becomes  = O nd · rlogb n  = O⇣nd ·  a  bd logb n⌘ .   4.7   cnd ·  rj  logb nXj=0  {z }  =O rlogb n   This looks messy until we notice some remarkable cancellations. Since exponentiation by b and the logarithm base-b are inverse operations, we can write   b d logb n = b d logb n =  blogb n  d = n d.  Thus the  1 bd logb n term in  4.7  cancels out the nd term, leaving us with an upper bound of O alogb n . This conﬁrms our hope that the total running time in this case is dominated by the work done at the leaves of the recursion tree. Since alogb n is the same as nlogb a, we have completed the proof of the master method. QE D  4.4.10 Solutions to Quizzes 4.4–4.5  Solution to Quiz 4.4  Correct answer:  b . First, by deﬁnition, the “branching factor” of the recursion tree is a—every recursive call that doesn’t trigger the base case makes a new recursive calls. This means the number of distinct subproblems gets multiplied by a with each level. Since there is 1 subproblem at level 0, there are aj subproblems at level j.  For the second part of the solution, again by deﬁnition, the sub- problem size decreases by a factor of b with each level. Since the problem size is n at level 0, all subproblems at level j have size n bj.19  19Unlike in our MergeSort analysis, the fact that the number of subproblems at level j is aj does not imply that the size of each subproblem is n aj. In MergeSort, the inputs to the level-j subproblems form a partition of the original input. This is not the case in many of our other divide-and-conquer algorithms. For example, in our recursive integer and matrix multiplication algorithms, parts of the original input are reused across diﬀerent recursive calls.   *4.4 Proof of the Master Method  113  Solution to Quiz 4.5  Correct answers:  a , b , d . First suppose that RSP < RW S, and so the forces of good are more powerful than the forces of evil—the shrinkage in work done per subproblem more than makes up for the increase in the number of subproblems. In this case, the algorithm does less work with each successive recursion level. Thus the ﬁrst statement is true  and the third statement is false . The second statement is true for similar reasons—if subproblems grow so rapidly that they outpace the savings-per-subproblem, then each recursion level requires more work than the previous one. In the ﬁnal statement, when RSP = RW S, there is a perfect equilibrium between the forces of good and evil. Subproblems are proliferating, but our savings in work-per-subproblem are increasing at exactly the same rate. The two forces cancel out, and the work done at each level of the recursion tree remains the same.  The Upshot  P A recurrence expresses a running time bound T  n  in terms of the number of operations per- formed by recursive calls.  P A standard recurrence T  n   aT   n  b   + O nd  is deﬁned by three parameters: the number a of recursive calls, the input size shrinkage factor b, and the exponent d in the running time of the combine step.  P The master method provides an asymptotic up- per bound for every standard recurrence, as a function of a, b, and d: O nd log n  if a = bd, O nd  if a   bd.  P Special cases include an O n log n  time bound for MergeSort, an O n1.59  time bound for Karatsuba, and an O n2.81  time bound for Strassen.  P The proof of the master method generalizes   114  The Master Method  the recursion tree argument used to analyze MergeSort.  P The quantities a and bd represent the forces of evil  the rate of subproblem proliferation  and the forces of good  the rate of work shrinkage .  P The three cases of the master method corre- spond to three diﬀerent types of recursion trees: those with the per-level work performed the same at each level  a tie between good and evil , decreasing with the level  when good wins , and increasing with the level  when evil wins .  P Properties of geometric series imply that the work done at the root of the recursion tree  which is O nd   dominates the overall running time in the second case, while the work done at the leaves  which is O alogb n  = O nlogb a   dominates in the third case.  Test Your Understanding  Problem 4.1 Recall the master method  Theorem 4.1  and its three parameters a, b, and d. Which of the following is the best interpreta- tion of bd?  a  The rate at which the total work is growing  per level of recur-  sion .  level of recursion .  recursion .  level of recursion .  b  The rate at which the number of subproblems is growing  per  c  The rate at which the subproblem size is shrinking  per level of  d  The rate at which the work-per-subproblem is shrinking  per  Problem 4.2 This and the next two questions will give you further practice with the master method. Suppose the running time T  n    Problems  115  of an algorithm is bounded by a standard recurrence with T  n   3   + O n2 . Which of the following is the smallest correct upper 7 · T   n bound on the asymptotic running time of the algorithm?  Problem 4.3 Suppose the running time T  n  of an algorithm is bounded by a standard recurrence with T  n   9 · T   n 3   + O n2 . Which of the following is the smallest correct upper bound on the asymptotic running time of the algorithm?  Problem 4.4 Suppose the running time T  n  of an algorithm is bounded by a standard recurrence with T  n   5 · T   n 3   + O n . Which of the following is the smallest correct upper bound on the asymptotic running time of the algorithm?  a  O n log n   b  O n2   c  O n2 log n   d  O n2.81   a  O n log n   b  O n2   c  O n2 log n   d  O n3.17   a  O nlog5 3   b  O n log n   c  O nlog3 5   d  O n5 3   e  O n2   f  O n2.59    116  The Master Method  Challenge Problems  Problem 4.5 Suppose the running time T  n  of an algorithm is bounded by the  non-standard!  recurrence with T  1  = 1 and T  n   T  bpnc  + 1 for n > 1.20 Which of the following is the smallest correct upper bound on the asymptotic running time of the algorithm?  Note that the master method does not apply!   a  O 1   b  O log log n   c  O log n  d  O pn   20Here bxc denotes the “ﬂoor” function, which rounds its argument down to  the nearest integer.   Chapter 5  QuickSort  This chapter covers QuickSort, a ﬁrst-ballot hall-of-fame algorithm. After giving a high-level overview of how the algorithm works  Sec- tion 5.1 , we discuss how to partition an array around a “pivot element” in linear time  Section 5.2  and how to choose a good pivot element  Section 5.3 . Section 5.4 introduces randomized QuickSort, and Sec- tion 5.5 proves that its asymptotic average running time is O n log n  for n-element arrays. Section 5.6 wraps up our sorting discussion with a proof that no “comparison-based” sorting algorithm can be faster than O n log n .  5.1 Overview  Ask a professional computer scientist or programmer to list their top 10 algorithms, and you’ll ﬁnd QuickSort on many lists  including mine . Why is this? We already know one blazingly fast sorting algorithm  MergeSort —why do we need another?  On the practical side, QuickSort is competitive with and often superior to MergeSort, and for this reason is the default sorting method in many programming libraries. The big win for QuickSort over MergeSort is that it runs in place—it operates on the input array only through repeated swaps of pairs of elements, and for this reason needs to allocate only a minuscule amount of additional memory for intermediate computations. On the aesthetic side, QuickSort is just a remarkably beautiful algorithm, with an equally beautiful running time analysis.  The QuickSort algorithm solves the problem of sorting an array, the same problem we tackled in Section 1.4.  5.1.1 Sorting  117   118  QuickSort  Problem: Sorting  Input: An array of n numbers, in arbitrary order.  Output: An array of the same numbers, sorted from small- est to largest.  So if the input array is  3  8  2  5  1  4  7  6   then the correct output array is  1  2  3  4  5  6  7  8   As in our MergeSort discussion, for simplicity let’s assume that the input array has distinct elements, with no duplicates.1  5.1.2 Partitioning Around a Pivot  QuickSort is built around a fast subroutine for “partial sorting,” whose responsibility is to partition an array around a “pivot element.”  Step 1: Choose a pivot element. First, choose one element of the array to act as a pivot element. Section 5.3 will obsess over exactly how this should be done. For now, let’s be naive and just use the ﬁrst element of the array  above, the “3” .  Step 2: Rearrange the input array around the pivot. Given the pivot element p, the next task is to arrange the elements of the array so that everything before p in the array is less than p, and everything after p is greater than p. For example, with the input array above, here’s one legitimate way of rearranging the elements: 1In the unlikely event that you need to implement QuickSort yourself, be warned that handling ties correctly and eﬃciently is a bit tricky, more so than in MergeSort. For a detailed discussion, see Section 2.3 of Algorithms  Fourth Edition , by Robert Sedgewick and Kevin Wayne  Addison-Wesley, 2011 .   5.1 Overview  119  3  8  2  5  1  4  7  6   2  1  3  6  7  4  5  8   pivot element   less than pivot  greater than pivot   This example makes clear that the elements before the pivot do not need to be placed in the correct relative order  the “1” and “2” are reversed , and similarly for the elements after the pivot. This partitioning subroutine places the  non-pivot  elements of the array into two buckets, one for the elements smaller than the pivot and the other for those greater than the pivot.  Here are the two key facts about this partition subroutine.  Fast. The partition subroutine has a blazingly fast implementation, running in linear  O n   time. Even better, and key to the practical utility of QuickSort, the subroutine can be implemented in place, with next to no memory beyond that occupied by the input array.2 Section 5.2 describes this implementation in detail.  Signiﬁcant progress. Partitioning an array around a pivot element makes progress toward sorting the array. First, the pivot element winds up in its rightful position, meaning the same position as in the sorted version of the input array  with all smaller elements before it and all larger elements after it . Second, partitioning reduces the sorting problem to two smaller sorting problems: sorting the elements less than the pivot  which conveniently occupy their own subarray  and the elements greater than the pivot  also in their own subarray . After recursively sorting the elements in each of these two subarrays, the algorithm is done!3  5.1.3 High-Level Description  In the following high-level description of the QuickSort algorithm, the “ﬁrst part” and “second part” of the array refer to the elements less than and greater than the pivot element, respectively:  2This contrasts with MergeSort  Section 1.4 , which repeatedly copies elements  over from one array to another.  3One of the subproblems might be empty, if the minimum or maximum element is chosen as the pivot. In this case, the corresponding recursive call can be skipped.   120  QuickSort  < p  p   > p   ﬁrst part   second part   QuickSort  High-Level Description   Input: array A of n distinct integers. Postcondition: elements of A are sorted from smallest to largest.  return  if n  1 then choose a pivot element p partition A around p recursively sort ﬁrst part of A recursively sort second part of A     base case-already sorted     to-be-implemented    to-be-implemented  While both MergeSort and QuickSort are divide-and-conquer algorithms, the order of operations is diﬀerent. In MergeSort, the recursive calls are performed ﬁrst, followed by the combine step, Merge. In QuickSort, the recursive calls occur after partitioning, and their results don’t need to be combined at all!4  5.1.4 Looking Ahead  Our remaining to-do list is:  1.  Section 5.2  How do we implement the partitioning subroutine?  2.  Section 5.3  How should we choose the pivot element?  3.  Sections 5.4 and 5.5  What’s the running time of QuickSort?  Another question is: “are we really sure that QuickSort always cor- rectly sorts the input array?” I’ve been giving short shrift to formal 4QuickSort was invented by Tony Hoare, in 1959, when he was just 25 years old. Hoare went on to make numerous fundamental contributions in programming languages and was awarded the ACM Turing Award—the equivalent of the Nobel Prize in computer science—in 1980.   5.2 Partitioning Around a Pivot Element  121  correctness arguments thus far because students generally have strong and accurate intuition about why divide-and-conquer algorithms are correct.  Compare this to understanding the running times of divide- and-conquer algorithms, which are usually far from obvious!  If you have any lingering concerns, it is straightforward to formally argue the correctness of QuickSort using a proof by induction.5  5.2 Partitioning Around a Pivot Element  Next we ﬁll in the details about how to partition an array around a pivot element p, meaning rearranging the array so that it looks like this:  < p  p   > p   5.2.1 The Easy Way Out  It’s easy to come up with a linear-time partitioning subroutine if we don’t care about allocating additional memory. One approach is to do a single scan over the input array A and copy over its non-pivot elements one by one into a new array B of the same length, populating B both from its front  for elements less than p  and its back  for elements bigger than p . The pivot element can be copied into the remaining entry of B after all the non-pivot elements have been  5Following the template for induction proofs reviewed in Appendix A, let P  n  denote the statement “for every input array of length n, QuickSort correctly sorts it.” The base case  n = 1  is uninteresting: an array with 1 element is necessarily sorted, and so QuickSort is automatically correct in this case. For the inductive step, ﬁx an arbitrary positive integer n   2. We’re allowed to assume the inductive hypothesis  i.e., P  k  is true for all k < n , meaning that QuickSort correctly sorts every array with fewer than n elements.  After the partitioning step, the pivot element p is in the same position as it is in the sorted version of the input array. The elements before p are exactly the same as those before p in the sorted version of the input array  possibly in the wrong relative order , and similarly for the elements after p. Thus the only remaining tasks are to reorganize the elements before p in sorted order, and similarly for the elements after p. Since both recursive calls are on subarrays of length at most n   1  if nothing else, p is excluded , the inductive hypothesis implies that both calls sort their subarrays correctly. This concludes the inductive step and the formal proof of correctness for the QuickSort algorithm.   122  QuickSort  processed. For our running example input array, here’s a snapshot from the middle of this computation:  next element to copy   next element   3   3  8  2  5  1  4  7  6   2   5  8   pivot element   less than pivot   greater than pivot   Since this subroutine does only O 1  work for each of the n elements in the input array, its running time is O n .  5.2.2  In-Place Implementation: The High-Level Plan  How do we partition an array around a pivot element while allocating almost no additional memory? Our high-level approach will be to do a single scan through the array, swapping pairs of elements as needed so that the array is properly partitioned by the end of the pass.  Assume that the pivot element is the ﬁrst element of the array; this can always be enforced  in O 1  time  by swapping the pivot element with the ﬁrst element of the array in a preprocessing step. As we scan and transform the input array, we will take care to ensure that it has the following form:  p   < p   > p   ?   already partitioned   unpartitioned   That is, the subroutine maintains the following invariant:6 the ﬁrst element is the pivot element; next are the non-pivot elements that have already been processed, with all such elements less than the pivot preceding all such elements greater than the pivot; followed by the not-yet-processed non-pivot elements, in arbitrary order.  If we succeed with this plan, then at the conclusion of the linear  scan we will have transformed the array so that it looks like this:  6An invariant of an algorithm is a property that is always true at prescribed  points of its execution  like at the end of every loop iteration .   5.2 Partitioning Around a Pivot Element  123  p   < p   > p   To complete the partitioning, we can swap the pivot element with the last element less than it:  p   < p   > p   < p   p   > p   swap   5.2.3 Example  Next we’ll step through the in-place partitioning subroutine on a concrete example. It may seem weird to go through an example of a program before you’ve seen its code, but trust me: this is the shortest path to understanding the subroutine.  Based on our high-level plan, we expect to keep track of two boundaries: the boundary between the non-pivot elements we’ve already looked at and those we haven’t, and within the ﬁrst group, the boundary between the elements less than the pivot and those greater than the pivot. We’ll use the indices j and i, respectively, to keep track of these two boundaries. Our desired invariant can then be rephrased as:  Invariant: all elements between the pivot and i are less than the pivot, and all elements between i and j are greater than the pivot.  Both i and j are initialized to the boundary between the pivot element and the rest. There are then no elements between the pivot and j, and the invariant holds vacuously:  i,j   3  8  2  5  1  4  7  6   pivot   unpartitioned    124  QuickSort  Each iteration, the subroutine looks at one new element, and incre- ments j. Additional work may or may not be required to maintain the invariant. The ﬁrst time we increment j in our example, we get:  i   j   3  8  2  5  1  4  7  6   partitioned   unpartitioned   There are no elements between the pivot and i, and the only element between i and j  the “8”  is greater than the pivot, so the invariant still holds.  Now the plot thickens. After incrementing j a second time, there is an element between i and j that is less than the pivot  the “2” , a violation of the invariant. To restore the invariant, we swap the “8” with the “2,” and also increment i, so that it is wedged between the “2” and the “8” and again delineates the boundary between processed elements less than and greater than the pivot:  i   j   swap   i   j   3  8  2  5  1  4  7  6   3  2  8  5  1  4  7  6   not partitioned   unpartitioned   partitioned   unpartitioned   The third iteration is similar to the ﬁrst. We process the next element  the “5”  and increment j. Because the new element is greater than the pivot, the invariant continues to hold and there is nothing more to do:  i   j   3  2  8  5  1  4  7  6   partitioned   unpartitioned    5.2 Partitioning Around a Pivot Element  125  The fourth iteration is similar to the second.  Incrementing j ushers in an element less than the pivot  the “1”  between i and j, which violates the invariant. But restoring the invariant is easy enough—just swap the “1” with the ﬁrst element greater than the pivot  the “8” , and increment i to reﬂect the new boundary between processed elements less than and greater than the pivot: j   i   j   i   swap   3  2  8  5  1  4  7  6   3  2  1  5  8  4  7  6   not partitioned   unpartitioned   partitioned   unpartitioned   The last three iterations process elements that are larger than the pivot, so nothing needs to be done beyond incrementing j. After all the elements have been processed and everything after the pivot has been partitioned, we conclude with the ﬁnal swap of the pivot element and the last element smaller than it:  i   swap   j   3  2  1  5  8  4  7  6   1  2  3  5  8  4  7  6   pivot   partitioned   partitioned   As required, in the ﬁnal array, all the elements less than pivot come before it, and all the elements greater than the pivot come after it. It is a coincidence that the “1” and “2” are in sorted order. The elements after the pivot are obviously not in sorted order.  5.2.4 Pseudocode for Partition  The pseudocode for the Partition subroutine is exactly what you’d expect after the example.7  7If you look at other textbooks or on the Web, you’ll see a number of variants of this subroutine that diﬀer in the details.  There’s even a version performed by Hungarian folk dancers! See https:  www.youtube.com watch?v=ywWBy6J5gz8.  These variants are equally suitable for our purposes.   126  QuickSort  Partition  Input: array A of n distinct integers, left and right endpoints `, r 2 {1, 2, . . . , n} with l  r. Postcondition: elements of the subarray A[`], A[` + 1], . . . , A[r] are partitioned around A[`]. Output: ﬁnal position of pivot element.  p := A[`] i := ` + 1 for j := ` + 1 to r do  if A[j] < p then  swap A[j] and A[i] i := i + 1  swap A[`] and A[i   1] return i   1     if A[j] > p do nothing     restores invariant    place pivot correctly    report final pivot position  The Partition subroutine takes as input an array A but operates only on the subarray of elements A[`], . . . , A[r], where ` and r are given parameters. Looking ahead, each recursive call to QuickSort will be responsible for a speciﬁc contiguous subset of the original input array, and the parameters ` and r specify the corresponding endpoints.  As in the example, the index j keeps track of which elements have been processed, while i keeps track of the boundary between processed elements that are less than and greater than the pivot  with A[i] the leftmost processed element greater than the pivot, if any . Each iteration of the for loop processes a new element. Like in the example, when the new element A[j] is greater than the pivot, the invariant holds automatically and there’s nothing to do. Otherwise, the subroutine restores the invariant by swapping A[j], the new element, and A[i], the leftmost element greater than the pivot, and incrementing i to update the boundary between elements less than and greater than the pivot.8,9 The ﬁnal step, as previously advertised, swaps the pivot 8No swap is necessary if no elements greater than the pivot have yet been encountered—the subarray of processed elements is trivially partitioned. But the extra swap is harmless  as you should verify , so we’ll stick with our simple pseudocode.  9Why does this swap and increment always restore the invariant? The invariant   5.2 Partitioning Around a Pivot Element  127  element into its rightful position, displacing the rightmost element less than it. The Partition subroutine concludes by reporting this position back to the invocation of QuickSort that called it.  This implementation is blazingly fast. It performs only a constant number of operations for each element A[`], . . . , A[r] of the relevant subarray, and so runs in time linear in the length of this subarray. Importantly, the subroutine operates on this subarray in place, without allocating any additional memory beyond the O 1  amount needed to keep track of variables like i and j.  5.2.5 Pseudocode for QuickSort  We now have a full description of the QuickSort algorithm, modulo the subroutine ChoosePivot that chooses a pivot element.  QuickSort  Input: array A of n distinct integers, left and right endpoints `, r 2 {1, 2, . . . , n}. Postcondition: elements of the subarray A[`], A[` + 1], . . . , A[r] are sorted from smallest to largest.  return  if `   r then i := ChoosePivot A, `, r  swap A[`] and A[i] j := Partition A, `, r  QuickSort A, `, j   1  QuickSort A, j + 1, r      0- or 1-element subarray     to-be-implemented    make pivot first    j =new pivot position    recurse on first part    recurse on second part  held before the most recent increment of j  by induction, if you want to be formal about it . This means that all the elements A[` + 1], . . . , A[i   1] are less than the pivot and all the elements A[i], . . . , A[j   1] are greater than the pivot. The only problem is that A[j] is less than the pivot. After swapping A[i] with A[j], the elements A[` + 1], . . . , A[i] and A[i + 1], . . . , A[j] are less than and greater than the pivot, respectively. After incrementing i, A[` + 1], . . . , A[i   1] and A[i], . . . , A[j] are less than and greater than the pivot, respectively, which restores the invariant.   128  QuickSort  Sorting an n-element array A reduces QuickSort A, 1, n .10  to the function call  5.3 The Importance of Good Pivots  Is QuickSort a fast algorithm? The bar is high: simple sorting algorithms like InsertionSort run in quadratic  O n2   time, and we already know one sorting algorithm  MergeSort  that runs in O n log n  time. The answer to this question depends on how we implement the ChoosePivot subroutine, which chooses one element from a designated subarray. For QuickSort to be quick, it’s important that “good” pivot elements are chosen, meaning pivot elements that result in two subproblems of roughly the same size.  5.3.1 Naive Implementation of ChoosePivot  In our overview of QuickSort we mentioned a naive implementation, which always picks the ﬁrst element.  ChoosePivot  Naive Implementation   Input: array A of n distinct integers, left and right endpoints `, r 2 {1, 2, . . . , n}. Output: an index i 2 {`, ` + 1, . . . , r}. return `  Is this naive implementation already good enough?  Quiz 5.1  What is the running time of the QuickSort algorithm, with the naive implementation of ChoosePivot, when the n- element input array is already sorted?  a  ⇥ n   10The array A is always passed by reference, meaning that all function calls  operate directly on the original copy of the input array.   5.3 The Importance of Good Pivots  129  b  ⇥ n log n   c  ⇥ n2   d  ⇥ n3    See Section 5.3.3 for the solution and discussion.   5.3.2 Overkill Implementation of ChoosePivot  Quiz 5.1 paints a worst-case picture of what can happen in QuickSort, with only one element removed per recursive call. What would be the best-case scenario? The most perfectly balanced split is achieved by the median element of the array, meaning the element for which the same number of other elements are less than it and greater than it.11 So if we want to work really hard for our pivot element, we can compute the median element of the given subarray.  ChoosePivot  Overkill Implementation   Input: array A of n distinct integers, left and right endpoints `, r 2 {1, 2, . . . , n}. Output: an index i 2 {`, ` + 1, . . . , r}. return position of the median element of {A[`], . . . , A[r]}  We’ll see in the next chapter that the median element of an array can be computed in time linear in the array length; let’s take this fact on faith for the following quiz.12 Is there any reward for working hard to compute an ideal pivot element?  Quiz 5.2  What is the running time of the QuickSort algorithm, with  11For example, the median of an array containing {1, 2, 3, . . . , 9} would be 5. For an even-length array, there are two legitimate choices for the median, and either is ﬁne for our purposes. So in an array that contains {1, 2, 3, . . . , 10}, either 5 or 6 can be considered the median element. 12You do at this point know an O n log n -time algorithm for computing the  median of an array.  Hint: Sort!    130  QuickSort  the overkill implementation of ChoosePivot, on an arbi- trary n-element input array? Assume that the ChoosePivot subroutine runs in ⇥ n  time.  a  Insuﬃcient information to answer  b  ⇥ n   c  ⇥ n log n   d  ⇥ n2    See Section 5.3.3 for the solution and discussion.   5.3.3 Solutions to Quizzes 5.1–5.2  Solution to Quiz 5.1  Correct answer:  c . The combination of naively chosen pivots and an already-sorted input array causes QuickSort to run in ⇥ n2  time, which is much worse than MergeSort and no better than simple algo- rithms such as InsertionSort. What goes wrong? The Partition subroutine in the outermost call to QuickSort, with the ﬁrst  smallest  element as the pivot, does nothing: it sweeps over the array, and since it only encounters elements greater than the pivot, it never swaps any pair of elements. After this call to Partition completes, the picture is:  recurse on these   < p  p   > p   empty   n-1 elements   still sorted    In the non-empty recursive call, the pattern recurs: the subarray is already sorted, the ﬁrst  smallest  element is chosen as the pivot, and there is one empty recursive call and one recursive call that is passed a subarray of n   2 elements. And so on. In the end, the Partition subroutine is invoked on subarrays of length n, n   1, n   2, . . . , 2. Since the work done in one call to   5.4 Randomized QuickSort  131  Partition is proportional to the length of the call’s subarray, the total amount of work done by QuickSort in this case is proportional to  n +  n   1  +  n   2  + ··· + 1    =⇥ n2   {z  }  and hence is quadratic in the input length n.13  Solution to Quiz 5.2  Correct answer:  c . In this best-case scenario, QuickSort runs in ⇥ n log n  time. The reason is that its running time is governed by the exact same recurrence that governs the running time of MergeSort. That is, if T  n  denotes the running time of this implementation of QuickSort on arrays of length n, then  T  n  =  +  .  since pivot = median  ChoosePivot & Partition  ⇥ n   {z}  2 · T⇣ n 2⌘ } {z   The primary work done by a call to QuickSort outside its recursive calls occurs in its ChoosePivot and Partition subroutines. We’re assuming that the former is ⇥ n , and Section 5.2 proves that the latter is also ⇥ n . Since we’re using the median element as the pivot element, we get a perfect split of the input array and each recursive call gets a subarray with at most n  2 elements:  < p   p   > p    cid:1  50% of array   cid:1  50% of array   Applying the master method  Theorem 4.1  with a = b = 2 and d = 1 then gives T  n  = ⇥ n log n .14  13A quick way to see that n +  n   1  +  n   2  + ··· + 1 = ⇥ n2  is to note that it is at most n2  each of the n terms is at most n  and at least n2 4  each of the ﬁrst n 2 terms is at least n 2 .  14Technically, we’re using here a variant of the master method that works with  ⇥-notation rather than O-notation, but otherwise is the same as Theorem 4.1.   132  QuickSort  5.4 Randomized QuickSort  Choosing the ﬁrst element of a subarray as the pivot takes only O 1  time but can cause QuickSort to run in ⇥ n2  time. Choosing the median element as the pivot guarantees an overall running time of ⇥ n log n  but is much more time-consuming  if still linear-time . Can we have the best of both worlds? Is there a simple and lightweight way to choose a pivot element that leads to a roughly balanced split of the array? The answer is yes, and the key idea is to use randomization.  5.4.1 Randomized Implementation of ChoosePivot  A randomized algorithm is one that “ﬂips coins” as it proceeds, and can make decisions based on the outcomes of these coin ﬂips. If you run a randomized algorithm on the same input over and over, you will see diﬀerent behavior on diﬀerent runs. All major programming languages include libraries that make it easy to pick random numbers at will, and randomization is a tool that should be in the toolbox of every serious algorithm designer.  Why on earth would you want to inject randomness into your algorithm? Aren’t algorithms just about the most deterministic thing you can think of? As it turns out, there are hundreds of computational problems for which randomized algorithms are faster, more eﬀective, or easier to code than their deterministic counterparts.15  The simplest way to incorporate randomness into QuickSort, which turns out to be extremely eﬀective, is to always choose pivot elements uniformly at random.  ChoosePivot  Randomized Implementation   Input: array A of n distinct integers, left and right endpoints `, r 2 {1, 2, . . . , n}. Output: an index i 2 {`, ` + 1, . . . , r}. return an element of {`, ` + 1, . . . , r}, chosen uniformly at random  15It took computer scientists a while to ﬁgure this out, with the ﬂoodgates opening in the mid-1970s with fast randomized algorithms for testing whether or not an integer is prime.   5.4 Randomized QuickSort  133  For example, if ` = 41 and r = 50, then each of the 10 elements A[41], . . . , A[50] has a 10% chance of being chosen as the pivot ele- ment.16  5.4.2 Running Time of Randomized QuickSort  The running time of randomized QuickSort, with pivot elements chosen at random, is not always the same. There is always some chance, however remote, that the algorithm always picks the minimum element of the remaining subarray as the pivot element, leading to the ⇥ n2  running time observed in Quiz 5.1.17 There’s a similarly remote chance that the algorithm gets incredibly lucky and always selects the median element of a subarray as the pivot, resulting in the ⇥ n log n  running time seen in Quiz 5.2. So the algorithm’s running time ﬂuctuates between ⇥ n log n  and ⇥ n2 —which occurs more frequently, the best-case scenario or the worst-case scenario? Amazingly, the performance of QuickSort is almost always close to its best-case performance.  Theorem 5.1  Running Time of Randomized QuickSort  For every input array of length n   1, the average running time of randomized QuickSort is O n log n .  The word “average” in the theorem statement refers to the randomness in the QuickSort algorithm itself. Theorem 5.1 does not assume that the input array is random. Randomized QuickSort is a general- purpose algorithm  cf., Section 1.6.1 : no matter what your input array is, if you run the algorithm on it over and over again, the average running time will be O n log n , good enough to qualify as a for-free primitive. In principle randomized QuickSort can run in ⇥ n2  time, but you will almost always observe a running time of O n log n  in practice. Two added bonuses: the constant hidden in the big-O notation in Theorem 5.1 is reasonably small  like in MergeSort , and the algorithm doesn’t spend time allocating and managing additional memory  unlike MergeSort .  16Another equally useful way is to randomly shuﬄe the input array in a  preprocessing step and then run the naive implementation of QuickSort.  17For even modest values of n, there’s a bigger probability that you’ll be struck  by a meteor while reading this!   134  QuickSort  5.4.3  Intuition: Why Are Random Pivots Good?  To understand deeply why QuickSort is so quick, there’s no substitute for studying the proof of Theorem 5.1, which is explained in Section 5.5. In preparation for that proof, and also as a consolation prize for the reader who is too time-limited to absorb Section 5.5, we next develop intuition about why Theorem 5.1 should be true.  The ﬁrst insight is that, to achieve a running time of O n log n  as in the best-case scenario of Quiz 5.2, it’s overkill to use the median element as the pivot element. Suppose we instead use an “approximate median,” meaning some element that gives us a 25%-75% split or better. Equivalently, this is an element that is greater than at least 25% of the other elements and also less than at least 25% of the other elements. The picture after partitioning around such a pivot element is:  approximate median   < p  p   > p   25-75% of array   25-75% of array   If every recursive call chooses a pivot element that is an approxi- mate median in this sense, the running time of QuickSort is still O n log n . We cannot derive this fact directly from the master method  Theorem 4.1 , because using a non-median results in subproblems with diﬀerent sizes. But it is not hard to generalize the analysis of MergeSort  Section 1.5  so that it also applies here.18  18Draw out the recursion tree of the algorithm. Whenever QuickSort calls itself recursively on two subproblems, the subproblems involve diﬀerent elements  those less than the pivot, and those greater than it . This means that, for every recursion level j, there are no overlaps between the subarrays of diﬀerent level-j subproblems, and so the sum of subarray lengths of level-j subproblems is at most n. The total work done at this level  by calls to Partition  is linear in the sum of the subarray lengths. Thus, like MergeSort, the algorithm does O n  work per recursion level. How many levels are there? With pivot elements that are approximate medians, at most 75% of the elements are passed to the same recursive call, and so the subproblem size drops by at least a factor of 4 3 with each level. This means there are at most log4 3 n = O log n  levels in the recursion tree, and so O n log n  work is done in all.   *5.5 Analysis of Randomized QuickSort  135  The second insight is that while you’d have to get incredibly lucky to choose the median element in randomized QuickSort  only a 1 in n chance , you have to be only slightly lucky to choose an approximate median. For example, consider an array that contains the elements {1, 2, 3, . . . , 100}. Any number between 26 and 75, inclusive, is an approximate median, with at least 25 elements less than it and 25 elements greater than it. This is 50% of the numbers in the array! So QuickSort has a 50-50 chance of randomly choosing an approximate median, as if it were trying to guess the outcome of a fair coin ﬂip. This means we expect roughly 50% of the calls to QuickSort to use approximate medians, and we can hope that the O n log n  running time analysis in the previous paragraph continues to hold, perhaps with twice as many levels as before.  Make no mistake: this is not a formal proof, just a heuristic argu- ment that Theorem 5.1 might plausibly be true. If I were you, given the central position of QuickSort in the design and analysis of algo- rithms, I would demand an indisputable argument that Theorem 5.1 really is true.  *5.5 Analysis of Randomized QuickSort  Randomized QuickSort seems like a great idea, but how do we really know it will work well? More generally, when you come up with a new algorithm in your own work, how do you know whether it’s brilliant or whether it stinks? One useful but ad hoc approach is to code up the algorithm and try it on a bunch of diﬀerent inputs. Another approach is to develop intuition about why the algorithm should work well, as in Section 5.4.3 for randomized QuickSort. But thoroughly understanding what makes an algorithm good or bad often requires mathematical analysis. This section will give you such an understanding of why QuickSort is so quick.  This section assumes familiarity with the concepts from discrete probability that are reviewed in Appendix B: sample spaces, events, random variables, expectation, and linearity of expectation.  5.5.1 Preliminaries  Theorem 5.1 asserts that for every input array of length n   1, the average running time of randomized QuickSort  with pivot elements   136  QuickSort  chosen uniformly at random  is O n log n . Let’s begin by translating this assertion into a formal statement in the language of discrete probability.  Fix for the rest of the analysis an arbitrary input array A of length n. Recall that a sample space is the set of all possible out- comes of some random process. In randomized QuickSort, all the randomness is in the random choices of pivot elements in the diﬀerent recursive calls. Thus we take the sample space ⌦ as the set of all possible outcomes of random choices in QuickSort  i.e., all pivot sequences .  Recall that a random variable is a numerical measurement of the outcome of a random process—a real-valued function deﬁned on ⌦. The random variable we care about is the number RT of primitive operations  i.e., lines of code  performed by randomized QuickSort. This is a well-deﬁned random variable because, when- ever all the pivot element choices are pre-determined  i.e., ! 2 ⌦ is ﬁxed , QuickSort has some ﬁxed running time RT  ! . Ranging over all possible choices !, RT  !  ranges from ⇥ n log n  to ⇥ n2   see Section 5.3 .  We can get away with analyzing a simpler random variable that counts only comparisons and ignores the other types of primitive operations performed. Let C denote the random variable equal to the number of comparisons between pairs of input elements performed by QuickSort with a given sequence of pivot choices. Looking back over the pseudocode, we see that these comparisons occur in exactly one place: the line “if A[j] < p” in the Partition subroutine  Sec- tion 5.2.4 , which compares the current pivot element to some other element of the input subarray.  The following lemma shows that comparisons dominate the overall running time of QuickSort, meaning that the latter is larger than the former only by a constant factor. This implies that, to prove an upper bound of O n log n  on the expected running time of QuickSort, we only need to prove an upper bound of O n log n  on the expected number of comparisons made.  Lemma 5.2 There is a constant a > 0 such that, for every input array A of length at least 2 and every pivot sequence !, RT  !   a · C ! .   *5.5 Analysis of Randomized QuickSort  137  We include the proof for the skeptics; skip it if you ﬁnd Lemma 5.2  intuitively obvious.  Proof of Lemma 5.2: First, in every call to Partition, the pivot element is compared exactly once to every other element in the given subarray. Thus the number of comparisons in the call is linear in the subarray length and, by inspection of the pseudocode in Section 5.2.4, the total number of operations in the call is at most a constant times this. By inspection of the pseudocode in Section 5.2.5, randomized QuickSort performs only a constant number of operations in each recursive call outside the Partition subroutine.19 There are at most n recursive calls to QuickSort in all—each input array element can be chosen as the pivot only once before being excluded from all future recursive calls—and so the total work outside calls to Partition is O n . Summing over all the recursive calls, the total number RT  !  of operations is at most a constant times the number C !  of com- parisons, plus O n . Since C !  is always at least proportional to n  or to n log n, even , the additional O n  work can be absorbed into the constant factor a of the lemma statement, and this completes the proof. QE D  The rest of this section concentrates on bounding the expected  number of comparisons.  Theorem 5.3  Comparisons in Randomized QuickSort  For every input array of length n   1, the expected number of comparisons between input array elements in randomized QuickSort is at most 2 n   1  ln n = O n log n . By Lemma 5.2, Theorem 5.3 implies Theorem 5.1, with a diﬀerent constant factor hidden in the big-O notation.  5.5.2 A Decomposition Blueprint  The master method  Theorem 4.1  resolved the running time of every divide-and-conquer algorithm we’ve studied up to this point, but 19This statement assumes that choosing a random pivot element counts as one primitive operation. The proof remains valid even if choosing a random pivot requires ⇥ log n  primitive operations  as you should check , and this covers typical practical implementations of random number generators.   138  QuickSort  there are two reasons why it doesn’t apply to randomized QuickSort. First, the running time of the algorithm corresponds to a random recurrence or a random recursion tree, and the master method works with deterministic recurrences. Second, the two subproblems that are solved recursively  elements less than the pivot and elements greater than the pivot  do not generally have the same size. We need a new idea.20  To prove Theorem 5.3, we’ll follow a decomposition blueprint that is useful for analyzing the expectation of complicated random variables. The ﬁrst step is to identify the  possibly complicated  random variable Y that you care about; for us, this is the number C of comparisons between input array elements made by randomized QuickSort, as in Theorem 5.3. The second step is to express Y as the sum of simpler random variables, ideally indicator  i.e., 0-1  random variables X1, . . . , Xm;  Y =  X`.  mX`=1  We are now in the wheelhouse of linearity of expectation, which states that the expectation of a sum of random variables equals the sum of their expectations  Theorem B.1 . The third step of the blueprint uses this property to reduce the computation of the expectation of Y to that of the simple random variables:  E[Y ] = E" mX`=1  X` =  mX`=1  E[X`] .  When the X`’s are indicator random variables, their expectations are particularly easy to compute via the deﬁnition  B.1 :  E[X`] = 0 · Pr[X` = 0]  +1 · Pr[X` = 1] = Pr[X` = 1] .    =0  {z  }  The ﬁnal step computes the expectations of the simple random vari- ables and adds up the results.21  20There are generalizations of the master method that address both these  issues, but they are somewhat complicated and outside the scope of this book.  21The randomized load-balancing analysis in Section B.6 is a simple example of this blueprint in action. We’ll also reuse this blueprint when we talk about hash tables in Part 2.   *5.5 Analysis of Randomized QuickSort  139  A Decomposition Blueprint  1. Identify the random variable Y that you care about.  2. Express Y as a sum of indicator  i.e., 0-1  random  variables X1, . . . , Xm:  3. Apply linearity of expectation:  Y =  X`.  mX`=1  E[Y ] =  Pr[X` = 1] .  mX`=1  4. Compute each of the Pr[X` = 1]’s and add up the  results to obtain E[Y ].  5.5.3 Applying the Blueprint  To apply the decomposition blueprint to the analysis of randomized QuickSort, we need to decompose the random variable C that we really care about into simpler  ideally 0-1  random variables. The key idea is to break down the total comparison count according to the pair of input array elements getting compared.  To make this precise, let zi denote the ith-smallest element in the input array, also known as the ith order statistic. For example, in the array  6  8  9  2   z1 refers to the “2,” z2 the “6,” z3 the “8,” and z4 the “9.” Note that zi does not denote the element in the ith position of the  unsorted  input array, but rather the element in this position of the sorted version of the input array.  For every pair of array indices i, j 2 {1, 2, . . . , n} with i < j, we  deﬁne a random variable Xij as follows:   140  QuickSort  for every ﬁxed choice of pivots !, Xij !  is the number of times the elements zi and zj get compared in QuickSort when the pivots are speciﬁed by !.  For the input array above, for example, X1,3 is the number of times the QuickSort algorithm compares the “2” with the “8.” We don’t care about the Xij’s per se, except inasmuch as they add up to the random variable C that we do care about.  The point of this deﬁnition is to implement the second step of the decomposition blueprint. Since each comparison involves exactly one pair of input array elements,  C !  =  Xij !   n 1Xi=1  nXj=i+1  for every ! 2 ⌦. The fancy-looking double sum on the right-hand side is just iterating over all pairs  i, j  with i < j, and this equation just says that the Xij’s account for all the comparisons made by the QuickSort algorithm.  Quiz 5.3  Fix two diﬀerent elements of the input array, say zi and zj. How many times might zi and zj be compared with each other during the execution of QuickSort?  a  exactly once  b  0 or 1 times  c  0, 1, or 2 times d  any number between 0 and n   1 is possible  See Section 5.5.6 for the solution and discussion.   The solution to Quiz 5.3 shows that all of the Xij’s are indicator random variables. We can therefore apply the third step of our decomposition blueprint to obtain  E[C] =  E[Xij] =  Pr[Xij = 1] .   5.1   n 1Xi=1  nXj=i+1  n 1Xi=1  nXj=i+1   *5.5 Analysis of Randomized QuickSort  141  To compute what we really care about, the expected number E[C] of comparisons, all we need to do is understand the Pr[Xij = 1]’s! Each of these numbers is the probability that some zi and zj are compared to each other at some point in randomized QuickSort, and the next order of business is to nail down these numbers.22  5.5.4 Computing Comparison Probabilities  There is a satisfying formula for the probability that two input array elements get compared in randomized QuickSort.  Lemma 5.4  Comparison Probability  If zi and zj denote the ith and jth smallest elements of the input array, with i < j, then  Pr[zi, zj get compared in randomized QuickSort] =  2  .  j   i + 1  For example, if zi and zj are the minimum and maximum elements  i = 1 and j = n , then they are compared with probability only 2 n. If there are no elements with value between zi and zj  j = i + 1 , then zi and zj are always compared to each other.  Fix zi and zj with i < j, and consider the pivot zk chosen in the  ﬁrst call to QuickSort. What are the diﬀerent scenarios?  Four QuickSort Scenarios  1. The chosen pivot is smaller than both zi and zj  k < i . Both zi and zj are passed to the second recursive call.  2. The chosen pivot is greater than both zi and zj  k > j .  Both zi and zj are passed to the ﬁrst recursive call.  3. The chosen pivot is between zi and zj  i < k < j . zi is passed to the ﬁrst recursive call, and zj to the second one.  22Section B.5 makes a big deal of the fact that linearity of expectation applies even to random variables that are not independent  where knowledge of one random variable tells you something about the others . This fact is crucial for us here, since the Xij’s are not independent. For example, if I tell you that X1n = 1, you know that either z1 or zn was chosen as the pivot element in the outermost call to QuickSort  why? , and this in turn makes it much more likely that a random variable of the form X1j or Xjn also equals 1.   142  QuickSort  4. The chosen pivot is either zi or zj  k 2 {i, j} . The pivot is excluded from both recursive calls; the other element is passed to the ﬁrst  if k = j  or second  if k = i  recursive call.  We have two things going for us. First, remember that every comparison involves the current pivot element. Thus zi and zj are compared in the outermost call to QuickSort if and only if one of them is chosen as the pivot element  scenario 4 . Second, in scenario 3, not only will zi and zj not be compared now, but they will never again appear together in the same recursive call and so cannot be compared in the future. For example, in the array  8  3  2  5  1  4  7  6   with zi = 3 and zj = 7, if any of the elements {4, 5, 6} are chosen as the pivot element, then zi and zj are sent to diﬀerent recursive calls and never get compared. For example, if the “6” is chosen, the picture is:  zi  3   8   zj   2  5  1  4  7  6   2  3  5  4  1  6  8  7   split into diﬀerent recursive calls   pivot element   1st recursive call  2nd recursive call   Scenarios 1 and 2 are a holding pattern: zi and zj haven’t been compared yet, but it’s still possible they will be compared in the future. During this holding pattern, zi and zj, and all of the elements zi+1, . . . , zj 1 with values in between zi and zj, lead parallel lives and keep getting passed to the same recursive call. Eventually, their collective journey is interrupted by a recursive call to QuickSort in which one of the elements zi, zi+1, . . . , zj 1, zj is chosen as the pivot element, triggering either scenario 3 or scenario 4.23 Fast forwarding to this recursive call, which is where the action is, scenario 4  and a comparison between zi and zj  is triggered if zi 23If nothing else, previous recursive calls eventually whittle the subarray down  to just the elements {zi, zi+1, . . . , zj 1, zj}.   *5.5 Analysis of Randomized QuickSort  143  or zj is the chosen pivot, while scenario 3  and no such comparison, ever  is triggered if any one of zi+1, . . . , zj 1 is chosen as the pivot. So there are two bad cases  zi and zj  out of the j   i + 1 options  zi, zi+1, . . . , zj 1, zj . Because randomized QuickSort always chooses pivot elements uniformly at random, by symmetry, each element of {zi, zi+1, . . . , zj 1, zj} is equally likely to be the ﬁrst pivot element chosen from the set. Putting everything together,  Pr[zi, zj get compared at some point in randomized QuickSort]  is the same as  which is  Pr[zi or zj is chosen as a pivot before any of zi+1, . . . , zj 1] ,  number of bad cases total number of options =  2  .  j   i + 1  This completes the proof of Lemma 5.4. QE D  Returning to our formula  5.1  for the expected number of compar- isons made by randomized QuickSort, we obtain a shockingly exact expression:  E[C] =  Pr[Xij = 1] =  n 1Xi=1  nXj=i+1  n 1Xi=1  nXj=i+1  2  .  j   i + 1   5.2   To prove Theorem 5.3, all that’s left to show is that the right-hand side of  5.2  is in fact O n log n .  5.5.5 Final Calculations  It’s easy to prove an upper bound of O n2  on the right-hand side of  5.2 : there are at most n2 terms in the double sum, and each of these has value at most 1 2  achieved when j = i + 1 . But we’re after a much better upper bound of O n log n , and we’ll have to be smarter to get it, by exploiting the fact that most of the quadratically many terms are much smaller than 1 2.  Consider one of the inner sums in  5.2 , for a ﬁxed value of i:  nXj=i+1  2  j   i + 1  2  = 2 ·✓ 1   +  1 3  1  n   i + 1◆ }  + ··· + n   i terms  {z  .   144  QuickSort  We can bound each of these sums from above by the largest such sum, which occurs when i = 1: j   i + 1   = 2 n   1  ·   5.3   1 j  2  .  n 1Xi=1  nXj=2  2 j  nXj=2  {z }  independent of i  j ? Let’s look at a picture. 1  j=2  n 1Xi=1 nXj=i+1 How big isPn  f x  = 1 x   1  1 2  1 3 1 4  0  area = ½   area = ⅓   area = ¼   1  2  3  4  Figure 5.1: Each term of the sum Pn j=2 1 j can be identiﬁed with a rectangle of width 1  between x-coordinates j   1 and j  and height 1 j  between y-coordinates 0 and 1 j . The graph of the function f  x  = 1 x kisses the northeastern corner of each of these rectangles, and so the area under the curve  i.e., the integral  is an upper bound on the area of the rectangles.  j=2  j as rectangles in the plane as 1 in Figure 5.1, we see that we can bound this sum from above by the area under the curve f  x  = 1 x between the points 1 and n, also known x . If you remember a little bit of calculus, you’ll dx recognize the solution to this integral as the natural logarithm ln x  i.e., ln x is the function whose derivative is 1  Viewing the terms of the sumPn as the integralR n nXj=2  j Z n  = ln n.   5.4   x :  1 x  1  n  1  1  1  dx = ln x     = ln n   ln 1{z}=0   *5.6  Sorting Requires ⌦ n log n  Comparisons  145  Chaining together the equations and inequalities in  5.2 – 5.4 , we have  E[C] =  n 1Xi=1  nXj=i+1  2  j   i + 1  2 n   1  ·  nXj=2  1 j  2 n   1  ln n.  Thus the expected number of comparisons made by randomized QuickSort—and also its expected running time, by Lemma 5.2— really is O n log n ! QE D  5.5.6 Solution to Quiz 5.3  Correct answer:  b . If either zi or zj is chosen as the pivot element in the outermost call to QuickSort, then zi and zj will get compared in the ﬁrst call to Partition.  Remember that the pivot element is compared to every other element in the subarray.  If i and j diﬀer by more than 1, it is also possible that zi and zj never get compared at all  see also Section 5.5.4 . For example, the minimum and maximum elements will not be compared to each other unless one of them is chosen as the pivot element in the outermost recursive call  do you see why? .  Finally, as one would expect from a good sorting algorithm, zi and zj will never be compared to each other more than once  which would be redundant . Every comparison involves the current pivot element, so the ﬁrst time zi and zj are compared in some call  if ever , one of them must be the pivot element. Since the pivot element is excluded from all future recursive calls, zi and zj never again appear together in the same recursive call  let alone get compared to each other .  *5.6  Sorting Requires ⌦ n log n  Comparisons  Is there a sorting algorithm faster than MergeSort and QuickSort, with running time better than ⇥ n log n ? It’s intuitively clear that an algorithm has to look at every input element once, but this implies only a linear lower bound of ⌦ n . This optional section shows that we can’t do better for sorting—the MergeSort and QuickSort algorithms achieve the best-possible asymptotic running time.   146  QuickSort  5.6.1 Comparison-Based Sorting Algorithms  Here’s the formal statement of the ⌦ n log n  lower bound.  Theorem 5.5  Lower Bound for Sorting  There is a constant c > 0 such that, for every n   1, every comparison-based sorting algorithm performs at least c · n log2 n operations on some length-n input array.  By a “comparison-based sorting algorithm,” we mean an algorithm that accesses the input array only via comparisons between pairs of elements, and never directly accesses the value of an element. Comparison-based sorting algorithms are general-purpose, and make no assumptions about the input elements other than that they belong to some totally ordered set. You can think of a comparison-based sorting algorithm as interacting with the input array through an API that supports only one operation: given two indices i and j  between 1 and the array length n , the operation returns 1 if the ith element is smaller than the jth element and 0 otherwise.24  For example, the MergeSort algorithm is a comparison-based sorting algorithm—it doesn’t care if it’s sorting integers or fruits  assuming we agreed on a total ordering of all possible fruits, like alphabetical .25 So are SelectionSort, InsertionSort, BubbleSort, and QuickSort.  5.6.2 Faster Sorting Under Stronger Assumptions  The best way to understand comparison-based sorting is to look at some non-examples. Here are three sorting algorithms that make assumptions about the input but in exchange beat the ⌦ n log n  lower bound in Theorem 5.5.26  24For example, the default sorting routine in the Unix operating system works this way. The only requirement is a user-deﬁned function for comparing pairs of input array elements.  25For an analogy, compare Sudoku and KenKen puzzles. Sudoku puzzles need only a notion of equality between diﬀerent objects, and would make perfect sense with the digits 1–9 replaced by nine diﬀerent fruits. KenKen puzzles involve arithmetic and hence need numbers—what would be the sum of a pluot and a mangosteen?  26For a more thorough treatment see, for example, Introduction to Algorithms  Third Edition , by Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Cliﬀord Stein  MIT Press, 2009 .   *5.6  Sorting Requires ⌦ n log n  Comparisons  147  BucketSort. The BucketSort algorithm is useful in practice for numerical data, especially when it is spread out uniformly over a known range. For example, suppose the input array has n elements between 0 and 1 that are roughly evenly spread out. In our minds, we divide the interval [0, 1] into n “buckets,” the ﬁrst reserved for input elements between 0 and 1 n, the second for elements between 1 n n, and so on. The ﬁrst step of the BucketSort algorithm does a and 2 single linear-time pass over the input array and places each element in its bucket. This is not a comparison-based step—the BucketSort algorithm looks at the actual value of an input element to identify which bucket it belongs to. It matters whether the value of an input element is .17 or .27, even if we hold the relative ordering of the elements ﬁxed.  If the elements are roughly evenly spread out, the population of ev- ery bucket is small. The second step of the algorithm sorts the elements inside each bucket separately  for example, using InsertionSort . Provided there are few elements in every bucket, this step also runs in linear time  with a constant number of operations performed per bucket . Finally, the sorted lists of the diﬀerent buckets are con- catenated, from the ﬁrst to the last. This step also runs in linear time. We conclude that linear-time sorting is possible under a strong assumption on the input data. CountingSort. The CountingSort algorithm is a variation on the same idea. Here, we assume that there are only k diﬀerent possi- ble values of each input element  known in advance , such as the integers {1, 2, . . . , k}. The algorithm sets up k buckets, one for each possible value, and in a single pass through the input array places each element in the appropriate bucket. The output array is simply the concatenation of these buckets  in order . CountingSort runs in linear time when k = O n , where n is the length of the input array. Like BucketSort, it is not a comparison-based algorithm, an extension of RadixSort. The CountingSort that gracefully handles n-element integer input arrays with reasonably large numbers represented in binary  a string of 0s and 1s, or “bits” . The ﬁrst step of RadixSort considers only the block of the log2 n least signiﬁcant bits of the input numbers, and sorts them accordingly. Because log2 n bits can encode only n diﬀerent values—corresponding to the numbers 0, 1, 2, . . . , n   1,  algorithm is  RadixSort   148  QuickSort  written in binary—the CountingSort algorithm can be used to implement this step in linear time. The RadixSort algorithm then re-sorts all the elements using the block of the next-least signiﬁcant log2 n bits, and so on until all the bits of the input have been processed. For this algorithm to sort correctly, it’s important to implement the CountingSort subroutine so that it is stable, meaning that it preserves the relative order of diﬀerent elements with the same value.27 The RadixSort algorithm runs in linear time provided the input array contains only integers between 0 and nk for some constant k.  These three sorting algorithms demonstrate how additional as- sumptions about the input data  like being not-too-large integers  enable techniques beyond comparisons  like bucketing  and algorithms that are faster than ⇥ n log n  time. Theorem 5.5 states that such improvements are impossible for general-purpose comparison-based sorting algorithms. Let’s see why.  5.6.3 Proof of Theorem 5.5  Fix an arbitrary deterministic comparison-based sorting algorithm.28 We can think of the output of the algorithm as a permutation  i.e., reordering  of the numbers 1, 2, . . . , n, with the ith element of the output indicating the position of the ith-smallest element in the input array. For example, if the input array is  6  8  9  2   4  1  2  3   then the output of a correct sorting algorithm can be interpreted as the array of indices  27Not all sorting algorithms are stable. For example, QuickSort is not a stable  sorting algorithm  do you see why? .  28Similar arguments apply to randomized comparison-based sorting algorithms,  and no such algorithm has expected running time better than ⇥ n log n .   *5.6  Sorting Requires ⌦ n log n  Comparisons  149  There are n! = n ·  n   1 ··· 2 · 1 possibilities for the correct output array.29 For every input array, there is a unique correct output array.  Lemma 5.6 If a comparison-based sorting algorithm never makes more than k comparisons for any length-n input array, then it gener- ates at most 2k distinct output arrays.  Proof: We can break the operations performed by the algorithm into phases, where phase i comprises the work done by the algorithm after its  i   1 th comparison and up to and including its ith comparison.  The algorithm can do whatever it wants in between comparisons— bookkeeping, ﬁguring out the next comparison to ask, etc.—as long as it doesn’t access the input array.  The speciﬁc operations performed in phase i can depend only on the results of the ﬁrst i  1 comparisons, since this is the only input-speciﬁc information possessed by the algorithm. These operations do not, for example, depend on the actual value of an element involved in one of these comparisons. The output array at the end of the algorithm depends only on the results of all the comparisons. If the algorithm never makes more than k comparisons, then there are at most 2k diﬀerent executions of the algorithm and hence at most 2k diﬀerent output arrays.30 QE D  A correct sorting algorithm must be capable of producing any of the n! possible correct output arrays. By Lemma 5.6, if k is the maximum number of comparisons made on n-element input arrays, then  2k    n 2  k    ,  n· n 1 ···2·1   ⇣ n 2⌘n 2  n!{z} 2⌘ = ⌦ n log n . log2⇣ n  where we have used the fact that the ﬁrst n 2 terms of n· n 1 ··· 2·1 are all at least n 2 . Taking the logarithm base-2 of both sides shows that  This lower bound applies to arbitrary comparison-based sorting algo- rithms, completing the proof of Theorem 5.5. QE D  29There are n choices for the position of the smallest element in the input array, n  1 remaining choices for the position of the second-smallest element, and so on. 30For the ﬁrst comparison, there are two possible outcomes; whatever the result and consequent second comparison, it also has two possible outcomes; and so on.   150  QuickSort  The Upshot  P The famous QuickSort algorithm has three high-level steps: ﬁrst, it chooses one element p of the input array to act as a “pivot element”; second, its Partition subroutine rearranges the array so that elements smaller than and greater than p come before it and after it, respectively; third, it recursively sorts the two subarrays on either side of the pivot.  P The Partition subroutine can be implemented to run in linear time and in place, meaning with negligible additional memory. As a consequence, QuickSort also runs in place.  P The correctness of the QuickSort algorithm does not depend on how pivot elements are chosen, but its running time does.  P The worst-case scenario is a running time of ⇥ n2 , where n is the length of the input array. This occurs when the input array is already sorted and the ﬁrst element is always used as the pivot element. The best-case scenario is a running time of ⇥ n log n . This occurs when the median element is always used as the pivot.  P In randomized QuickSort, the pivot element is always chosen uniformly at random. Its running time can be anywhere from ⇥ n log n  to ⇥ n2 , depending on its random coin ﬂips.  P The average running time of randomized QuickSort is ⇥ n log n , only a small constant factor worse than its best-case running time.  P Intuitively, choosing a random pivot is a good idea because there’s a 50% chance of getting a 25%-75% or better split of the input array.   Problems  151  P The formal analysis uses a decomposition blueprint to express a complicated random vari- able as a sum of 0-1 random variables and then apply linearity of expectation.  P The key insight is that the ith- and jth-smallest elements of the input array get compared in QuickSort if and only if one of them is chosen as a pivot before an element with value strictly in between them is chosen as a pivot.  P A comparison-based sorting algorithm is a general-purpose algorithm that accesses the in- put array only by comparing pairs of elements, and never directly uses the value of an element.  P No comparison-based sorting algorithm has a worst-case asymptotic running time better than O n log n .  Test Your Understanding  Problem 5.1 Recall the Partition subroutine employed by QuickSort  Section 5.2 . You are told that the following array has just been partitioned around some pivot element:  3  1  2  4  5  8  7  6  9   Which of the elements could have been the pivot element?  List all that apply; there could be more than one possibility.   Problem 5.2 Let ↵ be some constant, independent of the input array length n, strictly between 0 and 1 2. What is the probability that, with a randomly chosen pivot element, the Partition subroutine produces a split in which the size of both the resulting subproblems is at least ↵ times the size of the original array?  a  ↵   152  b  1   ↵ c  1   2↵ d  2   2↵  QuickSort  Problem 5.3 Let ↵ be some constant, independent of the input array length n, strictly between 0 and 1 2. Assume you achieve the approximately balanced splits from the preceding problem in every recursive call—so whenever a recursive call is given an array of length k, each of its two recursive calls is passed a subarray with length between ↵k and  1 ↵ k. How many successive recursive calls can occur before triggering the base case? Equivalently, which levels of the algorithm’s recursion tree can contain leaves? Express your answer as a range of possible numbers d, from the minimum to the maximum number of recursive calls that might be needed. [Hint: The formula that relates logarithmic functions with diﬀerent bases is logb n = ln n ln b .]  ln ↵  a  0  d    ln n b    ln n c    ln n d    ln n  ln ↵  d    ln n ln 1 ↵  ln 1 ↵   d    ln n ln 1 2↵   d    ln n ln 1 ↵   ln ↵  Problem 5.4 Deﬁne the recursion depth of QuickSort as the max- imum number of successive recursive calls it makes before hitting the base case—equivalently, the largest level of its recursion tree. In randomized QuickSort, the recursion depth is a random vari- able, depending on the pivots chosen. What is the minimum- and maximum-possible recursion depth of randomized QuickSort?  a  minimum: ⇥ 1 ; maximum: ⇥ n   b  minimum: ⇥ log n ; maximum: ⇥ n   c  minimum: ⇥ log n ; maximum: ⇥ n log n  d  minimum: ⇥ pn ; maximum: ⇥ n    Problems  153  Challenge Problems  Problem 5.5 Extend the ⌦ n log n  lower bound in Section 5.6 to apply also to the expected running time of randomized comparison- based sorting algorithms.  Programming Problems  Problem 5.6 Implement the QuickSort algorithm in your favorite programming language. Experiment with the performance of diﬀerent ways of choosing the pivot element.  One approach is to keep track of the number of comparisons between input array elements made by QuickSort.31 For several diﬀerent input arrays, determine the number of comparisons made with the following implementations of the ChoosePivot subroutine:  1. Always use the ﬁrst element as the pivot.  2. Always use the last element as the pivot.  3. Use a random element as the pivot.  In this case you should run the algorithm 10 times on a given input array and average the results.   4. Use the median-of-three as the pivot element. The goal of this rule is to do a little extra work to get much better performance on input arrays that are nearly sorted or reverse sorted. In more detail, this implementation of ChoosePivot considers the ﬁrst, middle, and ﬁnal elements of the given array.  For an array with even length 2k, use the kth element for the “middle” one.  It then identiﬁes which of these three elements is the median  i.e., the one whose value is in between the other two , and returns this as the pivot.32 For example, with the input array  31There’s no need to count the comparisons one by one. When there is a recursive call on a subarray of length m, you can simply add m  1 to your running total of comparisons.  Recall that the pivot element is compared to each of the other m   1 elements in the subarray in this recursive call.  32A careful analysis would keep track of the comparisons made in identifying the median of the three candidate elements, in addition to the comparisons made in calls to Partition.   154  QuickSort  8  3  2  5  1  4  7  6   the subroutine would consider the ﬁrst  8 , middle  5 , and last  6  elements. It would return 6, the median of the set {5, 6, 8}, as the pivot element.  See www.algorithmsilluminated.org for test cases and challenge  data sets.   Chapter 6  Linear-Time Selection  This chapter studies the selection problem, where the goal is to iden- tify the ith-smallest element of an unsorted array. It’s easy to solve this problem in O n log n  time using sorting, but we can do better. Section 6.1 describes an extremely practical randomized algorithm, very much in the spirit of randomized QuickSort, that runs in linear time on average. Section 6.2 provides the elegant analysis of this algorithm—there’s a cool way to think about the progress the algo- rithm makes in terms of a simple coin-ﬂipping experiment, and then linearity of expectation  yes, it’s back. . .   seals the deal.  Theoretically inclined readers might wonder whether the selection problem can be solved in linear time without resorting to randomiza- tion. Section 6.3 describes a famous deterministic algorithm for the problem, one that has more Turing Award-winning authors than any other algorithm I know of. It is deterministic  i.e., no randomization allowed  and based on an ingenious “median-of-medians” idea for guaranteeing good pivot choices. Section 6.4 proves the linear running time bound, which is not so easy!  This chapter assumes that you remember the Partition subrou- tine from Section 5.2 that partitions an array around a pivot element in linear time, as well as the intuition for what makes a pivot element good or bad  Section 5.3 .  6.1 The RSelect Algorithm  6.1.1 The Selection Problem  In the selection problem, the input is the same as for the sort- ing problem—an array of n numbers—along with an integer i 2 {1, 2, . . . , n}. The goal is to identify the ith order statistic—the ith-smallest entry in the array.  155   156  Linear-Time Selection  Problem: Selection  Input: An array of n numbers, in arbitrary order, and an integer i 2 {1, 2, . . . , n}. Output: The ith-smallest element of A.  As usual, we assume for simplicity that the input array has distinct elements, with no duplicates.  For example, if the input array is  6  8  9  2   and the value of i is 2, the correct output is 6. If i were 3, the correct output would be 8, and so on.  When i = 1, the selection problem is just the problem of computing the minimum element of an array. This is easy to do in linear time— make one pass through the array and remember the smallest element seen. Similarly, the case of ﬁnding the maximum element  i = n  is easy. But what about for values of i in the middle? For example, what if we want to compute the middle element—the median—of an array?  To be precise, for an array with odd length n, the median is the ith order statistic with i =  n + 1  2. For an array with even length n, let’s agree to deﬁne the median as the smaller of the two possibilities, 2 .1 which corresponds to i = n  6.1.2 Reduction to Sorting  We already know a fast algorithm for the selection problem, which piggybacks on our fast sorting algorithms.  1Why would you want to compute the median element of an array? After all, the mean  i.e., average  is easy enough to compute in linear time—just sum up all the array elements in a single pass and divide by n. One reason is to compute a summary statistic of an array that is more robust than the mean. For example, one badly corrupted element, such as a data entry error, can totally screw up the mean of an array, but generally has little eﬀect on the median.   6.1 The RSelect Algorithm  157  Reducing Selection to Sorting  Input: array A of n distinct numbers, and an integer i 2 {1, 2, . . . , n}. Output: the ith order statistic of A.  B := MergeSort A  return B[i]  After sorting the input array, we certainly know where to ﬁnd the ith smallest element—it’s hanging out in the ith position of the sorted array. Because MergeSort runs in O n log n  time  Theorem 1.2 , so does this two-step algorithm.2  But remember the mantra of any algorithm designer worth their salt: can we do better? Can we design an algorithm for the selection problem that is even faster than O n log n  time? The best we can hope for is linear time  O n  —if we don’t even take the time to look at each element in the array, there’s no hope of always cor- rectly identifying, say, the minimum element. We also know from Theorem 5.5 that any algorithm that uses a sorting subroutine is stuck with a worst-case running time of ⌦ n log n .3 So if we can get a running time better than O n log n  for the selection problem, we’ll have proved that selection is fundamentally easier than sorting. Accomplishing this requires ingenuity—piggybacking on our sorting algorithms won’t cut it.  6.1.3 A Divide-and-Conquer Approach  The randomized linear-time selection algorithm RSelect follows the template that proved so successful in randomized QuickSort: choose a random pivot element, partition the input array around the pivot,  2A computer scientist would call this a reduction from the selection problem to the sorting problem. A reduction absolves you from developing a new algo- rithm from scratch, and instead allows you to stand on the shoulders of existing algorithms. In addition to their practical utility, reductions are an extremely fundamental concept in computer science, and we will discuss them at length in Part 4.  3Assuming that we restrict ourselves to comparison-based sorting algorithms,  as in Section 5.6.   158  Linear-Time Selection  and recurse appropriately. The next order of business is to understand the appropriate recursion for the selection problem.  Recall what the Partition subroutine in Section 5.2 does: given an array and a choice of pivot element, it rearranges the elements of the array so that everything less than and greater than the pivot appears before and after the pivot, respectively.  3  8  2  5  1  4  7  6   2  1  3  6  7  4  5  8   pivot element   less than pivot  greater than pivot   Thus the pivot element ends up in its rightful position, after all the elements less than it and before all the elements greater than it.  QuickSort recursively sorted the subarray of elements less than the pivot element, and also the subarray of elements greater than the pivot. What is the analog for the selection problem?  Quiz 6.1  Suppose we are looking for the 5th order statistic in an input array of 10 elements. Suppose that after partitioning the array, the pivot element ends up in the third position. On which side of the pivot element should we recurse, and what order statistic should we look for?  a  The 3rd order statistic on the left side of the pivot.  b  The 2nd order statistic on the right side of the pivot.  c  The 5th order statistic on the right side of the pivot.  d  We might need to recurse on both the left and the  right sides of the pivot.   See Section 6.1.6 for the solution and discussion.   6.1.4 Pseudocode for RSelect  Our pseudocode for the RSelect algorithm follows the high-level description of QuickSort in Section 5.1, with two changes. First, we   6.1 The RSelect Algorithm  159  commit to using random pivot elements rather than having a generic ChoosePivot subroutine. Second, RSelect makes only one recursive call, while QuickSort makes two. This diﬀerence is the primary reason to hope that RSelect might be even faster than randomized QuickSort.  RSelect  Input: array A of n   1 distinct numbers, and an integer i 2 {1, 2, . . . , n}. Output: the ith order statistic of A.  if n = 1 then return A[1]     base case  choose pivot element p uniformly at random from A partition A around p j := p’s position in partitioned array if j = i then     you got lucky!  return p  else if j > i then  else  return RSelect ﬁrst part of A, i   return RSelect second part of A, i   j      j < i  Partitioning the input array around the pivot element p splits the array into three pieces, leading to three cases in the RSelect algorithm:  < p  p   > p   ﬁrst part    j-1 elements    jth position   second part   n-j elements    Because the pivot element p assumes its rightful position in the partitioned array, if it’s in the jth position, it must be the jth order statistic. If by dumb luck the algorithm was looking for the jth order statistic  i.e., i = j , it’s done. If the algorithm is searching for a smaller number  i.e., i < j , it must belong to the ﬁrst part of the partitioned array. In this case, recursing only throws out   160  Linear-Time Selection  elements bigger than the jth  and hence the ith  order statistic, so the algorithm is still looking for the ith-smallest element among those in the ﬁrst subarray. In the ﬁnal case  i > j , the algorithm is looking for a number larger than the pivot element, and the recursion mimics the solution to Quiz 6.1. The algorithm recurses on the second part of the partitioned array, throwing out the pivot element and the j   1 elements smaller than it from further consideration. Since the algorithm was originally looking for the ith-smallest element, it’s now looking for the  i   j th-smallest element among those that remain.  6.1.5 Running Time of RSelect  Like randomized QuickSort, the running time of the RSelect algo- rithm depends on the pivots it chooses. What’s the worst that could happen?  Quiz 6.2  What is the running time of the RSelect algorithm if pivot elements are always chosen in the worst possible way?  a  ⇥ n   b  ⇥ n log n   c  ⇥ n2   d  ⇥ 2n    See Section 6.1.6 for the solution and discussion.   We now know that the RSelect algorithm does not run in linear time for all possible choices of pivot elements, but could it run in linear time on average over its random choices of pivots? Let’s start with a more modest goal: are there any choices of pivots for which RSelect runs in linear time?  What makes a good pivot? The answer is the same as for QuickSort  see Section 5.3 : good pivots guarantee that recursive calls receive signiﬁcantly smaller subproblems. The worst-case sce- nario is a pivot element that gives the most unbalanced split possible, with one empty subarray and the other subarray having everything   6.1 The RSelect Algorithm  161  save for the pivot element  as in Quiz 6.2 . This scenario occurs when the minimum or maximum element is chosen as the pivot. The best-case scenario is a pivot element that gives the most balanced split possible, with two subarrays of equal length.4 This scenario occurs when the median element is chosen as the pivot. It may seem circular to explore this scenario, as we might well be trying to compute the median in the ﬁrst place! But it’s still a useful thought experiment to understand the best-possible running time that RSelect can have  which had better be linear! .  Let T  n  denote the running time of RSelect on arrays of length n. If RSelect magically chooses the median element of the given subarray in every recursive call, then every recursive call does work linear in its subarray  mostly in the Partition subroutine  and makes one recursive call on a subarray of half the size:  T  n    T⇣ n 2⌘  {z }  since pivot = median  +  .  Partition, etc.  O n   {z}  This recurrence is right in the wheelhouse of the master method  Theorem 4.1 : since there is one recursive call  a = 1 , the subproblem size drops by a factor of 2  b = 2 , and linear work is done outside the recursive call  d = 1 , 1 = a < bd = 2 and the second case of the master method tells us that T  n  = O n . This is an important sanity check: if RSelect gets suﬃciently lucky, it runs in linear time. So is the running time of RSelect typically closer to its best-case performance of ⇥ n  or its worst-case performance of ⇥ n2 ? With the success of randomized QuickSort under our belt, we might hope that typical executions of RSelect have performance close to the best-case scenario. And indeed, while in principle RSelect can run in ⇥ n2  time, you will almost always observe a running time of O n  in practice.  Theorem 6.1  Running Time of RSelect  For every input array of length n   1, the average running time of RSelect is O n . Section 6.2 provides the proof of Theorem 6.1.  4We’re ignoring the lucky case in which the chosen pivot is exactly the order statistic being searched for—this is unlikely to happen before the last few recursive calls of the algorithm.   162  Linear-Time Selection  Amazingly, the average running time of RSelect is only a con- stant factor larger than the time needed to read the input! Since sorting requires ⌦ n log n  time  Section 5.6 , Theorem 6.1 shows that selection is fundamentally easier than sorting.  The same comments about the average running time of randomized QuickSort  Theorem 5.1  apply here. The RSelect algorithm is general-purpose in that the running time bound is for arbitrary inputs and the “average” refers only to the random pivot elements chosen by the algorithm. Like with QuickSort, the constant hidden in the big-O notation in Theorem 6.1 is reasonably small, and the RSelect algorithm can be implemented to work in place, without allocating signiﬁcant additional memory.5  6.1.6 Solution to Quizzes 6.1–6.2  Solution to Quiz 6.1  Correct answer:  b . After partitioning the array, we know that the pivot element is in its rightful position, with all smaller numbers before it and larger numbers after it. Since the pivot element wound up in the third position of the array, it is the third-smallest element. We’re looking for the ﬁfth-smallest element, which is larger. We can therefore be sure that the 5th order statistic is in the second subarray, and we need to recurse only once. What order statistic are we looking for in the recursive call? Originally we were looking for the ﬁfth-smallest, but now we’ve thrown out the pivot element and the two elements smaller than it. Since 5   3 = 2, we’re now looking for the second-smallest element among those passed to the recursive call.  Solution to Quiz 6.2  Correct answer:  c . The worst-case running time of RSelect is the same as for randomized QuickSort. The bad example is the same as in Quiz 5.1: suppose the input array is already sorted, and the algorithm repeatedly picks the ﬁrst element as the pivot. In every recursive call, the ﬁrst part of the subarray is empty while the second 5The in-place implementation uses left and right endpoints to keep track of the current subarray, like in the pseudocode for QuickSort in Section 5.2.5. See also Programming Problem 6.5.   *6.2 Analysis of RSelect  163  part has everything save for the current pivot. Thus the subarray length of each recursive call is only one less than the previous one. The work done in each recursive call  mostly by the Partition subroutine  is linear in its subarray length. When computing the median element, there are ⇡ n 2 recursive calls, each with a subarray of length at least 2 , and so the overall running time is ⌦ n2 . n  *6.2 Analysis of RSelect  One way to prove the linear expected running time bound for the RSelect algorithm  Theorem 6.1  is to follow the same decomposition blueprint that worked so well for analyzing randomized QuickSort  Section 5.5 , with indicator random variables that track comparisons. For RSelect, we can also get away with a simpler instantiation of the decomposition blueprint that formalizes the intuition from Sec- tion 5.4.3:  i  random pivots are likely to be pretty good; and  ii  pretty good pivots make rapid progress.  6.2.1 Tracking Progress via Phases  We’ve already noted that a call to RSelect does O n  work outside of its recursive call, primarily in its call to Partition. That is, there is a constant c > 0 such that   *  for every input array of length n, RSelect performs at most cn operations outside of its recursive call.  Because RSelect always makes only one recursive call, we can track its progress by the length of the subarray that it is currently working on, which only gets smaller over time. For simplicity, we’ll use a coarser version of this progress measure.6 Suppose the outer call to RSelect is given an array of length n. For an integer j   0, we say that a recursive call to RSelect is in phase j if the length of its subarray is between  ✓ 3 4◆j+1  · n and ✓ 3 4◆j  · n.  6A more reﬁned analysis can be done and results in a better constant factor  in the running time bound.   164  Linear-Time Selection  For example, the outermost call to RSelect is always in phase 0, as are any subsequent recursive calls that operate on at least 75% of the original input array. Recursive calls on subarrays that contain 4  2 ⇡ 56% and 75% of the original elements belong to between   3 phase 1, and so on. By phase j ⇡ log4 3 n, the subarray has size at most 1 and there are no further recursive calls. For each integer j   0, let Xj denote the random variable equal to the number of phase-j recursive calls. Xj can be as small as 0, since a phase might get skipped entirely, and certainly can’t be bigger than n, the maximum number of recursive calls made by RSelect. By  * , RSelect performs at most  c ·  ✓ 3 4◆j {z   · n  }  max subarray length   phase j   operations in each phase-j recursive call. We can then decompose the running time of RSelect across the diﬀerent phases:  running time of RSelect Xj 0  · c✓ 3 4◆j Xj{z} calls  {z = cnXj 0✓ 3 4◆j   phase j   Xj.   phase j   n  }  work per call  This upper bound on the running time of RSelect is a complicated random variable, but it is a weighted sum of simpler random variables  the Xj’s . Your automatic response at this point should be to apply linearity of expectation  Theorem B.1 , to reduce the computation of the complicated random variable to those of the simpler ones:  E[running time of RSelect]  cnXj 0✓ 3 4◆j  E[Xj] .   6.1   So what is E[Xj]?  6.2.2 Reduction to Coin Flipping  We have two things going for us in bounding the expected num- ber E[Xj] of phase-j recursive calls. First, whenever we pick a pretty   *6.2 Analysis of RSelect  165  good pivot, we proceed to a later phase. As in Section 5.4.3, deﬁne an approximate median of a subarray as an element that is greater than at least 25% of the other elements in the subarray and also less than at least 25% of the other elements. The picture after partitioning around such a pivot element is:  approximate median   < p  p   > p   25-75% of array   25-75% of array   No matter which case is triggered in RSelect, the recursive call gets a subarray of length at most 3 4 times that of the previous call, and therefore belongs to a later phase. This argument proves the following proposition.  Proposition 6.2  Approximate Medians Make Progress  If a phase-j recursive call chooses an approximate median, then the next recursive call belongs to phase j + 1 or later.  Second, as proved in Section 5.4.3, a recursive call has a decent  chance of picking an approximate median.  Proposition 6.3  Approximate Medians Are Abundant  A call to RSelect chooses an approximate median with probability at least 50%. For example, in an array that contains the elements {1, 2, . . . , 100}, each of the ﬁfty elements between 26 and 75, inclusive, is an approxi- mate median.  Propositions 6.2 and 6.3 let us substitute a simple coin-ﬂipping experiment for the number of phase-j recursive calls. Suppose you have a fair coin, equally likely to be heads or tails. Flip the coin repeatedly, stopping the ﬁrst time you get “heads,” and let N be the number of coin ﬂips performed  including the last ﬂip . Think of “heads” as corresponding to choosing an approximate median  and ending the experiment .   166  Linear-Time Selection  Proposition 6.4  Reduction to Coin Flipping  For each phase j, E[Xj]  E[N ]. Proof: All the diﬀerences between the deﬁnitions of Xj and N are such that the expected value of the former can only be smaller:  1. There might be no phase-j recursive calls  if the phase is skipped entirely , while there is always at least one coin ﬂip  the ﬁrst one .  2. Each coin ﬂip has exactly a 50% chance of prolonging the experiment  if it comes up tails . Propositions 6.2 and 6.3 imply that each phase-j recursive call has at most a 50% chance of prolonging the phase—a necessary condition is that it fails to pick an approximate median.  QE D  The random variable N is a geometric random variable with pa- 2. Looking up its expectation in a textbook or on the Web, rameter 1 we ﬁnd that E[N ] = 2. Alternatively, a sneaky way to see this is to write the expected value of N in terms of itself. The key idea is to exploit the fact that the random experiment is memoryless: if the ﬁrst coin ﬂip comes up “tails,” the rest of the experiment is a copy of the original one. In math, whatever the expected value of N might be, it must satisfy the relationship  E[N ] = 1{z}  ﬁrst ﬂip  1  +  ·  2{z}Pr[tails]  further coin ﬂips  .  E[N ]  {z}  The unique value for E[N ] that satisﬁes this equation is 2.7  Proposition 6.4 implies that this value is an upper bound on what  we care about, the expected number of phase-j recursive calls.  Corollary 6.5  Two Calls Per Phase  For every j, E[Xj]  2. 7Strictly speaking, we should also rule out the possibility that E[N ] = +1   which is not hard to do .   *6.3 The DSelect Algorithm  167  6.2.3 Putting It All Together  We can now use the upper bound in Corollary 6.5 on the E[Xj]’s to simplify our upper bound  6.1  on the expected running time of RSelect:  E[running time of RSelect]  cnXj 0✓ 3 4◆j The sumPj 0  3  E[Xj]  2cnXj 0✓ 3 4◆j 4 j looks messy, but it’s a beast we’ve already tamed.  When proving the master method  Section 4.4 , we took a detour to discuss geometric series  Section 4.4.8 , and derived the exact formula  4.6 :  .  1 + r + r2 + ··· + rk =  1   rk+1 1   r  1  4, we have  for every real number r 6= 1 and nonnegative integer k. When r < 1, 1 r, no matter how big k is. Plugging in this quantity is at most r = 3 Xj 0✓ 3 4◆j  1 1   3  = 4,    4  and so  E[running time of RSelect]  8cn = O n .  This completes the analysis of RSelect and the proof of Theorem 6.1. QE D  *6.3 The DSelect Algorithm  The RSelect algorithm runs in expected linear time for every input, where the expectation is over the random choices made by the al- gorithm. Is randomization required for linear-time selection?8 This section and the next resolve this question with a deterministic linear- time algorithm for the selection problem.  For the sorting problem, the O n log n  average running time of randomized QuickSort is matched by that of the deterministic 8Understanding the power of randomness in computation more generally is a deep question and continues to be a topic of active research in theoretical computer science.   168  Linear-Time Selection  MergeSort algorithm, and both QuickSort and MergeSort are useful algorithms in practice. In contrast, while the deterministic linear-time selection algorithm described in this section works OK in practice, it is not competitive with the RSelect algorithm. The two reasons for this are larger constant factors in the running time and the work performed by DSelect allocating and managing additional memory. Still, the ideas in the algorithm are so cool that I can’t help but tell you about them.  6.3.1 The Big Idea: Median-of-Medians  The RSelect algorithm is fast because random pivots are likely to be pretty good, yielding a roughly balanced split of the input array after partitioning, and pretty good pivots make rapid progress. If we’re not allowed to use randomization, how can we compute a pretty good pivot without doing too much work?  The big idea in deterministic linear-time selection is to use the “median-of-medians” as a proxy for the true median. The algorithm treats the input array elements like sports teams and runs a two-round knockout tournament, the champion of which is the pivot element; see also Figure 6.1.  median-of-medians    champion    group medians    round 1 winners    9   10  7   9   input array    contestants    11  6  10  2  15   8   1   7  14  3   9  12  4   5  13   group 1   group 2   group 3   Figure 6.1: Computing a pivot element with a two-round knockout tour- nament. In this example, the chosen pivot is not the median of the input array, but it is pretty close.   *6.3 The DSelect Algorithm  169  The ﬁrst round is the group stage, with the elements in positions 1– 5 of the input array the ﬁrst group, the elements in positions 6–10 the second group, and so on. The ﬁrst-round winner of a group of 5 is deﬁned as the median element  i.e., the third-smallest . Since there are ⇡ n 5 ﬁrst-round winners.  As usual, we ignore fractions for simplicity.  The tournament champion is then deﬁned as the median of the ﬁrst-round winners.  5 groups of 5, there are ⇡ n  6.3.2 Pseudocode for DSelect  How do we actually compute the median-of-medians? Implementing the ﬁrst stage of the knockout tournament is easy, since each median computation involves only 5 elements. For example, each such com- putation can be done by brute force,  for each of the 5 possibilities, explicitly check if it’s the middle element , or by using our reduction to sorting  Section 6.1.2 . To implement the second stage, we compute the median of the ⇡ n  5 ﬁrst-round winners recursively.  DSelect  Input: array A of n   1 distinct numbers, and an integer i 2 {1, 2, . . . , n}. Output: the ith order statistic of A.     base case  C[h] := middle element from the hth group of 5     first-round winners     median-of-medians  5 do  1 if n = 1 then return A[1] 2 3 for h := 1 to n 4 5 p := DSelect C, n 10  6 partition A around p 7 j := p’s position in partitioned array 8 if j = i then 9 10 else if j > i then 11 12 else 13  return DSelect ﬁrst part of A, i   return p  return DSelect second part of A, i   j      you got lucky!     j < i   170  Linear-Time Selection  Lines 1–2 and 6–13 are identical to RSelect. Lines 3–5 are the only new part of the algorithm; they compute the median-of-medians of the input array, replacing the line in RSelect that chooses a pivot element at random.  Lines 3 and 4 compute the ﬁrst-round winners of the knockout tournament, with the middle element of each group of 5 computed using brute force or a sorting algorithm, and copy these winners over into a new array C.9 Line 5 computes the tournament champion by recursively computing the median of C; since C has length  roughly  n 5 , this is the n 10th order statistic of C. No randomization is used in any step of the algorithm.  6.3.3 Understanding DSelect  It may seem dangerously circular to recursively call DSelect while computing the pivot element. To understand what’s going on, let’s ﬁrst be clear on the total number of recursive calls.  Quiz 6.3  How many recursive calls does a single call to DSelect typically make?  a  0  b  1  c  2  d  3   See below for the solution and discussion.   Correct answer:  c . Putting aside the base case and the lucky case in which the pivot element happens to be the desired order statistic, the DSelect algorithm makes two recursive calls. To see why, don’t overthink it; just inspect the pseudocode for DSelect line by line. There is one recursive call on line 5, and one more on either line 11 or 13.  9This auxiliary array is why DSelect, unlike RSelect, fails to run in place.   *6.3 The DSelect Algorithm  171  There are two common points of confusion about these two re- cursive calls. First, isn’t the fact that the RSelect algorithm makes only one recursive call the reason it runs faster than our sorting al- gorithms? Isn’t the DSelect algorithm giving up this improvement by making two recursive calls? Section 6.4 shows that, because the extra recursive call in line 5 needs to solve only a relatively small subproblem  with 20% of the elements of the original array , we can still rescue the linear-time analysis.  Second, the two recursive calls play fundamentally diﬀerent roles. The goal of the recursive call in line 5 is to identify a good pivot element for the current recursive call. The goal of the recursive call in line 11 or 13 is the usual one, to recursively solve a smaller residual problem left by the current recursive call. Nevertheless, the recursive structure in DSelect is squarely in the tradition of all the other divide-and-conquer algorithms we’ve studied: each recursive call makes a small number of further recursive calls on strictly smaller subproblems, and does some amount of additional work. If we weren’t worried about an algorithm like MergeSort or QuickSort running forever, we shouldn’t be worried about DSelect either.  6.3.4 Running Time of DSelect  The DSelect algorithm is not just a well-deﬁned program that com- pletes in a ﬁnite amount of time—it runs in linear time, performing only a constant factor more work than necessary to read the input.  Theorem 6.6  Running Time of DSelect  For every input array of length n   1, the running time of DSelect is O n . Unlike the running time of RSelect, which can in principle be as bad as ⇥ n2 , the running time of DSelect is always O n . Still, you should prefer RSelect to DSelect in practice, because the former runs in place and the constant hidden in the “O n ” average running time in Theorem 6.1 is smaller than the constant hidden in Theorem 6.6.  A Computer Science Superteam  One of the goals of this book series is to make famous algorithms seem so simple  at least in hindsight  that you feel like you could have come up with them your-   172  Linear-Time Selection  self, had you been in the right place at the right time. Almost nobody feels this way about the DSelect al- gorithm, which was devised by a computer science superteam of ﬁve researchers, four of whom have been recognized with the ACM Turing Award  all for dif- ferent things! , the equivalent of the Nobel Prize for computer science.10 So don’t despair if it’s hard to imagine coming up with the DSelect algorithm, even on your most creative days—it’s also hard to imagine beating Roger Federer  let alone ﬁve of him  on the tennis court!  *6.4 Analysis of DSelect  Could the DSelect algorithm really run in linear time? It seems to do an extravagant amount of work, with two recursive calls and signiﬁcant extra work outside the recursive calls. Every other algorithm we’ve seen with two or more recursive calls has running time ⇥ n log n  or worse.  6.4.1 Work Outside Recursive Calls  Let’s start by understanding the number of operations performed by a call to DSelect outside its recursive calls. The two steps that require signiﬁcant work are computing the ﬁrst-round winners  lines 3–4  and partitioning the input array around the median-of-medians  line 6 .  10The algorithm and its analysis were published in the paper “Times Bounds for Selection,” by Manuel Blum, Robert W. Floyd, Vaughan Pratt, Ronald L. Rivest, and Robert E. Tarjan  Journal of Computer and System Sciences, 1973 .  It was very unusual to see papers with ﬁve authors back then.  In chronological order: Floyd won the Turing Award in 1978 for contributions to algorithms and also programming languages and compilers; Tarjan was recognized in 1986  along with John E. Hopcroft  for his work on algorithms and data structures, which we’ll discuss further in later parts of Algorithms Illuminated; Blum was awarded it in 1995, largely for his contributions to cryptography; and Rivest, whom you may know as the “R” in the RSA cryptosystem, won it in 2002  with Leonard Adleman and Adi Shamir  for his work on public-key cryptography. Meanwhile, Pratt is famous for accomplishments that run the gamut from primality testing algorithms to the co-founding of Sun Microsystems!   *6.4 Analysis of DSelect  173  As in QuickSort or RSelect, the second step runs in linear time. What about the ﬁrst step?  Focus on a particular group of 5 elements. Because this is only a constant number of elements  independent of the input array length n , computing the median takes constant time. For example, suppose we do this computation by reducing to sorting  Section 6.1.2 , say using MergeSort. We understand well the amount of work done by MergeSort  Theorem 1.2 : at most  6m log2 m + 1   operations to sort an array of length m. You might be worried about the fact that the MergeSort algorithm does not run in linear time. But we’re invoking it only for constant-size subarrays  m = 5 , and as a result it performs a constant number of operations  at most 6 · 5 ·  log2 5 + 1   120  per subarray. Summing over the n 5 groups of 5 that need to be sorted, this is at most 120 · n 5 = 24n = O n  operations in all. We conclude that, outside its recursive calls, the DSelect algorithm does only linear work.  6.4.2 A Rough Recurrence  In Chapter 4 we analyzed divide-and-conquer algorithms using re- currences, which express a running time bound T  n  in terms of the number of operations performed by recursive calls. Let’s try the same approach here, letting T  n  denote the maximum number of operations that the DSelect algorithm performs on an input array of length n. When n = 1 the DSelect algorithm just returns the sole ar- ray element, so T  1  = 1. For larger n, the DSelect algorithm makes one recursive call in line 5, another recursive call in line 11 or 13, and performs O n  additional work  for partitioning, and computing and copying over the ﬁrst-round winners . This translates to a recurrence of the form  T  n   T  size of subproblem 1    + T  size of subproblem 2    + O n .    =n 5  {z  }    =?  {z  To evaluate the running time of DSelect, we need to understand the sizes of the subproblems solved by its two recursive calls. The size of the ﬁrst subproblem  line 5  is n 5 , the number of ﬁrst-round winners.  }   174  Linear-Time Selection  We don’t know the size of the second subproblem—it depends on which element ends up being the pivot, and on whether the order statistic sought is less than or greater than this pivot. This indeterminacy in subproblem size is why we didn’t use recurrences to analyze the QuickSort and RSelect algorithms.  In the special case in which the true median element of the input array is chosen as the pivot, the second subproblem is guaranteed to comprise at most n 2 elements. The median-of-medians is generally not the true median  Figure 6.1 . Is it close enough to guarantee an approximately balanced split of the input array, and hence a not-too-big subproblem in line 11 or 13?  6.4.3 The 30-70 Lemma  The heart of the analysis of DSelect is the following lemma, which quantiﬁes the payoﬀ of the hard work done to compute the median-of- medians: this pivot element guarantees a split of 30%-70% or better of the input array.  Lemma 6.7  30-70 Lemma  For every input array of length n   2, the subarray passed to the recursive call in line 11 or 13 of DSelect has length at most 7  10 n.11  The 30-70 Lemma lets us substitute “ 7  10 n” for “?” in the rough  recurrence above: for every n   2,  T  n   T✓ 1  5 · n◆ + T✓ 7  10 · n◆ + O n .   6.2   We ﬁrst prove the 30-70 Lemma, and then prove that the recur- rence  6.2  implies that DSelect is a linear-time algorithm.  Proof of Lemma 6.7: Let k = n 5 denote the number of groups of 5, and hence the number of ﬁrst-round winners. Deﬁne xi as the ith- smallest of the ﬁrst-round winners. Equivalently, x1, . . . , xk are the  11Strictly speaking, because one of the “groups of 5” could have fewer than ﬁve elements  if n is not a multiple of 5 , the 7 10 n + 2, rounded up to the nearest integer. We’ll ignore the “+2” for the same reason we ignore fractions—it is a detail that complicates the analysis in an uninteresting way and has no real eﬀect on the bottom line.  10 n should be 7   *6.4 Analysis of DSelect  175  ﬁrst-round winners listed in sorted order. The tournament champion, the median-of-medians, is xk 2  or xdk 2e, if k is odd .12 The plan is to argue that xk 2 is no smaller than at least 60% of the elements in at least 50% of the groups of 5, and is no larger than at least 60% of the elements in at least 50% of the groups. Then at least 60% · 50% = 30% of the input array elements would be no larger than the median-of-medians, and at least 30% would be no smaller:  median-of-medians   < p  p   > p   30-70% of array   30-70% of array   To implement this plan, consider the following thought experiment. In our minds  not in the actual algorithm! , we lay out all the input array elements in a two-dimensional grid format. There are ﬁve rows, and each of the n 5 columns corresponds to one of the groups of 5. Within each column, we lay out the 5 elements in sorted order from bottom to top. Finally, we lay out the columns so that the ﬁrst-round winners  i.e., the elements in the middle row  are in sorted order from left to right. For example, if the input array is  11  6  10  2  15   8   1   7  14  3   9  12  4   5  13   then the corresponding grid is  14   cid:1   8   cid:1   7   cid:1   3   cid:1   1   pivot  <   <   13   cid:1   12   cid:1   9   cid:1   5   cid:1   4   15   cid:1   11   cid:1   10   cid:1   6   cid:1   2   12The notation dxe denotes the “ceiling” function, which rounds its argument  up to the nearest integer.   176  Linear-Time Selection  with the pivot element, the median-of-medians, in the center posi- tion.  Key Observation  Because the middle row is sorted from left to right, and each column is sorted from bottom to top, all the elements to the left and down from the pivot are less than the pivot, and all the elements to the right and up from the pivot are greater than the pivot.13  In our example, the pivot is the “9,” the elements to the left and down are {1, 3, 4, 5, 7}, and the elements to the right and up are {10, 11, 12, 13, 15}. Thus at least 6 elements will be excluded from the subarray passed to the next recursive call—the pivot element 9 and either {10, 11, 12, 13, 15}  in line 11  or {1, 3, 4, 5, 7}  in line 13 . Either way, the next recursive call receives at most 9 elements, and 9 is less than 70% of 15.  The argument for the general case is the same. Figure 6.2 depicts what the grid looks like for an arbitrary input array. Because the pivot element is the median of the elements in the middle row, at least 50% of the columns are to the left of the one that contains the pivot  counting also the pivot’s own column . In each of these columns, at least 60% of the elements  the three smallest of the 5  are no larger than the column’s median and hence no larger than the pivot element. Thus at least 30% of the input array elements are no larger than the pivot element, and all of these would be excluded from the recursive call in line 13. Similarly, at least 30% of the elements are no smaller than the pivot, and these would be excluded from the recursive call in line 11. This completes the proof of the 30-70 lemma. QE D  6.4.4 Solving the Recurrence  The 30-70 Lemma implies that the input size shrinks by a constant factor with every recursive call of DSelect, and this bodes well for a linear running time. But is it a Pyrrhic victory? Does the cost  13Elements to the left and up or to the right and down could be either less  than or greater than the pivot.   *6.4 Analysis of DSelect   cid:1     cid:1     cid:1   x1  <   cid:1     cid:1   x2  <   cid:1     cid:1     cid:1     cid:1     cid:1   x3   cid:1     cid:1    60% of  rows   no smaller than pivot    cid:2  50% of columns   <   .........   <   .........   <   pivot  <   .........   .........   .........   .........    cid:1     cid:1   xk 2   cid:1     cid:1    .........   .........   .........   .........    cid:1     cid:1   xk   cid:1     cid:1    177  60% of  rows    cid:2  50% of columns   no larger than pivot   Figure 6.2: Proof of the 30-70 Lemma. Imagine the input array elements laid out in a grid format. Each column corresponds to a group of 5, sorted from bottom to top. Columns are sorted in order of their middle elements. The picture assumes that k is even; for k odd, the “xk 2” is instead “xdk 2e.” Elements to the southwest of the median-of-medians can only be less than it; those to the northeast can only be greater than it. As a result, at least 60%· 50% = 30% of the elements are excluded from each of the two possible recursive calls.  of computing the median-of-medians outweigh the beneﬁts of par- titioning around a pretty good pivot? Answering these questions, and completing the proof of Theorem 6.6, requires ﬁguring out the solution to the recurrence in  6.2 .  Since the DSelect algorithm does O n  work outside its recursive calls  computing ﬁrst-round winners, partitioning the array, etc. , there is a constant c > 0 such that, for every n   2, 10 · n◆ + cn,  5 · n◆ + T✓ 7  T  n   T✓ 1   6.3   where T  n  is an upper bound on the running time of DSelect on length-n arrays. We can assume that c   1  as increasing c cannot invalidate the inequality  6.3  . Also, T  1  = 1. As we’ll see, the crucial property of this recurrence is that 1  5 + 7  10 < 1.   178  Linear-Time Selection  We leaned on the master method  Chapter 4  to evaluate all the recurrences we’ve encountered so far—for MergeSort, Karatsuba, Strassen, and more, we just plugged in the three relevant parameters  a, b, and d  and out popped an upper bound on the algorithm’s running time. Unfortunately, the two recursive calls in DSelect have diﬀerent input sizes, and this rules out applying Theorem 4.1. It is possible to generalize the recursion tree argument in Theorem 4.1 to accommodate the recurrence in  6.3 .14 For variety’s sake, and to add another tool to your toolbox, we proceed instead with a diﬀerent method.  6.4.5 The Guess-and-Check Method  The guess-and-check method for evaluating recurrences is just as ad hoc as it sounds, but it’s also extremely ﬂexible and applies to arbitrarily crazy recurrences.  Step 1: Guess. Guess a function f  n  which you suspect satisﬁes T  n  = O f  n  .  Step 2: Check. Prove by induction on n that T  n  really is O f  n  .  In general, the guessing step is a bit of a dark art. In our case, since we’re trying to prove a linear running time bound, we’ll guess that T  n  = O n .15 That is, we guess that there is a constant ` > 0  independent of n  such that  T  n   ` · n   6.4   for every positive integer n. If true, since ` is a constant, this would imply our hope that T  n  = O n .  14For a heuristic argument, think about the ﬁrst pair of recursive calls to DSelect—the two nodes in level 1 of the algorithm’s recursion tree. One has 20% of the input array elements, the other at most 70%, and the work done at this level is linear in the sum of the two subproblem sizes. Thus the amount of work done at level 1 is at most 90% of that done at level 0, and so on at subsequent levels. This resembles the second case of the master method, in which the work-per-level drops by a constant factor each level. This analogy suggests that the O n  work performed at the root should dominate the running time  cf., Section 4.4.6 .  15“Hope and check” might be a more apt description for us!   *6.4 Analysis of DSelect  179  When verifying  6.4 , we are free to choose ` however we want, as long as it is independent of n. Similar to asymptotic notation proofs, the usual way to ﬁgure out the appropriate constant is to reverse engineer it  cf., Section 2.5 . Here, we’ll take ` = 10c, where c is the constant factor in the recurrence  6.3 .  Since c is a constant, so is `.  Where did this number come from? It’s the smallest constant for which the inequality  6.5  below is valid.  We prove  6.4  by induction. In the language of Appendix A, P  n  is the assertion that T  n   ` · n = 10c · n. For the base case, we need to prove directly that P  1  is true, meaning that T  1   10c. The recurrence explicitly says that T  1  = 1 and c   1, so certainly T  1   10c. For the inductive step, ﬁx an arbitrary positive integer n   2. We need to prove that T  n   ` · n. The inductive hypothesis states that P  1 , . . . , P  n   1  are all true, meaning that T  k   ` · k for all k < n. To prove P  n , let’s just follow our noses.  First, the recurrence  6.3  decomposes T  n  into three terms:  5 · n◆ T  n   T✓ 1 {z  }  `· n   ind. hyp.   5  10 · n◆ + T✓ 7 {z  }  `· 7n   ind. hyp.   10  +cn.  We can’t directly manipulate any of these terms, but we can apply the inductive hypothesis, once with k = n  5 and once with k = 7n 10 :  Grouping terms,  T  n   ` ·  n 5  + ` ·  7n 10  + cn.  T  n   n✓ 9   10  ` + c◆ } {z  =`   as `=10c   = ` · n.   6.5   This proves the inductive step, which veriﬁes that T  n   `· n = O n  and completes the proof that the ingenious DSelect algorithm runs in linear time  Theorem 6.6 . QE D   180  Linear-Time Selection  The Upshot  P The goal in the selection problem is to compute the ith-smallest element of an unsorted array.  P The selection problem can be solved in O n log n  time, where n is the length of the input array, by sorting the array and then re- turning the ith element.  P The problem can also be solved by partitioning the input array around a pivot element, as in QuickSort, and recursing once on the relevant side. The RSelect algorithm always chooses the pivot element uniformly at random.  P The running time of RSelect varies from ⇥ n   to ⇥ n2 , depending on the pivots chosen.  P The average running time of RSelect is ⇥ n . The proof uses a reduction to a coin-ﬂipping experiment.  P The big idea in the deterministic DSelect algo- rithm is to use the “median-of-medians” as the pivot element: break the input array into groups of 5, directly compute the median of each group, and recursively compute the median of these n 5 ﬁrst-round winners.  P The 30-70 Lemma shows that the median-of- medians guarantees a 30%-70% or better split of the input array.  P The analysis of DSelect shows that the work spent in the recursive call to compute the median-of-medians is outweighed by the beneﬁt of a 30%-70% split, resulting in a linear running time.   Problems  181  Test Your Understanding  Problem 6.1 Let ↵ be some constant, independent of the input array length n, strictly between 1 2 and 1. Suppose you are using the RSelect algorithm to compute the median element of a length-n array. What is the probability that the ﬁrst recursive call is passed a subarray of length at most ↵ · n?  Problem 6.2 Let ↵ be some constant, independent of the input array length n, strictly between 1 2 and 1. Assume that every recursive call to RSelect makes progress as in the preceding problem—so whenever a recursive call is given an array of length k, its recursive call is passed a subarray with length at most ↵k. What is the maximum number of successive recursive calls that can occur before triggering the base case?  2  a  1   ↵ b  ↵   1 c  1   ↵ d  2↵   1  2  ↵  ln ↵  a    ln n b    ln n c    ln n ln 1 ↵  d    ln n 1 2 +↵   ln   Challenge Problems  Problem 6.3 In this problem, the input is an unsorted array of n distinct elements x1, x2, . . . , xn with positive weights w1, w2, . . . , wn. i=1 wi of the weights. Deﬁne a weighted median as an element xk for which the total weight of all elements with wi  is at most W 2, and also the total wi  is at most W 2. Observe that there are at most two weighted medians. Give  Let W denote the sum Pn value less than xk  i.e.,Pxi<xk weight of elements with value larger than xk  i.e.,Pxi>xk   182  Linear-Time Selection  a deterministic linear-time algorithm for computing all the weighted medians in the input array. [Hint: Use DSelect as a subroutine.]  Problem 6.4 Suppose we modify the DSelect algorithm by breaking the elements into groups of 7, rather than groups of 5.  Use the median- of-medians as the pivot element, as before.  Does this modiﬁed algorithm also run in O n  time? What if we use groups of 3?16  Programming Problems  Problem 6.5 Implement in your favorite programming language the RSelect algorithm from Section 6.1. Your implementation should operate in place, using an in-place implementation of Partition  which you might have implemented for Problem 5.6  and passing indices through the recursion to keep track of the still-relevant portion of the original input array.  See www.algorithmsilluminated.org for test cases and challenge data sets.   16For a deep dive on this question, see the paper “Select with Groups of 3 or 4 Takes Linear Time,” by Ke Chen and Adrian Dumitrescu  arXiv:1409.3600, 2014 .   Appendix A  Quick Review of Proofs By Induction  Proofs by induction come up all the time in computer science. For example, in Section 5.1, we use a proof by induction to argue that the QuickSort algorithm always correctly sorts its input array. In Section 6.4, we use induction to prove that the DSelect algorithm runs in linear time.  Proofs by induction can be unintuitive, at least at ﬁrst sight. The good news is that they follow a fairly rigid template, and become nearly automatic with a little practice. This appendix explains the template and provides two short examples. If you’ve never seen proofs by induction before, you should supplement this appendix with another source that has many more examples.1  A.1 A Template for Proofs by Induction  For our purposes, a proof by induction establishes an assertion P  n  for every positive integer n. For example, when proving the correctness of the QuickSort algorithm in Section 5.1, we can deﬁne P  n  as the statement: “for every input array of length n, QuickSort correctly sorts it.” When analyzing the running time of the DSelect algorithm in Section 6.4, we can deﬁne P  n  as “for every input array of length n, DSelect halts after performing at most 100n operations.” Induction allows us to prove a property of an algorithm, like correctness or a running time bound, by establishing the property for each input length in turn.  Analogous to a recursive algorithm, a proof by induction has two parts: a base case and an inductive step. The base case proves that P  n  is true for all suﬃciently small values of n  typically just  1For instance, see Chapter 2 of the freely available lecture notes by Eric Lehman and Tom Leighton  http:  www.boazbarak.org cs121 LehmanLeighton.pdf .  183   184  Quick Review of Proofs By Induction  n = 1 . In the inductive step, you assume that P  1 , . . . , P  n   1  are all true and prove that P  n  is consequently true as well. Base case: Prove directly that P  1  is true. Inductive step: Prove that, for every integer n   2,  if P  1 , P  2 , . . . , P  n   1  are true }   inductive hypothesis  {z  then P  n  is true.  In the inductive step, you get to assume that P  k  has already been es- tablished for all values of k smaller than n—this is called the inductive hypothesis—and should use this assumption to establish P  n .  If you prove both the base case and the inductive step, then P  n  is indeed true for every positive integer n. P  1  is true by the base case, and applying the inductive step over and over again shows that P  n  is true for arbitrarily large values of n.  A.2 Example: A Closed-Form Formula  We can use induction to derive a closed-form formula for the sum of the ﬁrst n positive integers. Let P  n  denote the assertion that  1 + 2 + 3 + ··· + n =   n + 1 n  .  2  When n = 1, the left-hand side is 1 and the right-hand side is 2·1 2 = 1. This shows that P  1  is true and completes the base case. For the inductive step, we pick an arbitrary integer n   2 and assume that P  1 , P  2 , . . . , P  n   1  are all true. In particular, we can assume P  n   1 , which is the assertion  1 + 2 + 3 + ··· +  n   1  = Now we can add n to both sides to derive  n n   1   .  2  1 + 2 + 3 + ··· + n =  n n   1   2  + n =  n2   n + 2n  =   n + 1 n  ,  2  2  which proves P  n . Since we’ve established both the base case and the inductive step, we can conclude that P  n  is true for every positive integer n.   A.3 Example: The Size of a Complete Binary Tree  185  A.3 Example: The Size of a Complete Binary Tree  Next, let’s count the number of nodes in a complete binary tree with n levels. In Figure A.1, we see that with n = 4 levels, the number of nodes is 15 = 24   1. Could this pattern be true in general?  Figure A.1: A complete binary tree with 4 levels and 24   1 = 15 nodes.  For each positive integer n, let P  n  be the statement “a complete binary tree with n levels has 2n   1 nodes.” For the base case, note that a complete binary tree with 1 level has exactly one node. Since 21   1 = 1, this proves that P  1  is true. For the inductive step, ﬁx a positive integer n   2 and assume that P  1 ,. . . ,P  n   1  are all true. The nodes of the complete binary tree with n levels can be divided into three groups:  i  the root;  ii  the nodes in the left subtree of the root; and  iii  the nodes in the right subtree of the root. The left and right subtrees of the root are themselves complete binary trees, each with n   1 levels. Since we are assuming that P  n   1  is true, there are exactly 2n 1   1 nodes in each of the left and right subtrees. Adding up the nodes in the three groups, we get a total of  1{z}root  + 2n 1   1  {z }  left subtree  + 2n 1   1  {z }  right subtree  = 2n   1  nodes in the tree. This proves the statement P  n  and, since n   2 was arbitrary, completes the inductive step. We conclude that P  n  is true for every positive integer n.   Appendix B  Quick Review of Discrete Probability  This appendix reviews the concepts from discrete probability that are necessary for our analysis of randomized QuickSort  Theorem 5.1 and Section 5.5 : sample spaces, events, random variables, expectation, and linearity of expectation. Section B.6 concludes with a load- balancing example that ties all these concepts together. We will also use these concepts in future parts of this book series, in the contexts of data structures, graph algorithms, and local search algorithms. If you’re seeing this material for the ﬁrst time, you probably want to supplement this appendix with a more thorough treatment.1 If you have seen it before, don’t feel compelled to read this appendix from front to back—dip in as needed wherever you need a refresher.  B.1 Sample Spaces  We’re interested in random processes, in which any number of diﬀerent things might happen. The sample space is the set ⌦ of all the diﬀerent things that could happen—the universe in which we’re going to assign probabilities, take average values, and so on. For example, if our random process is the throw of a six-sided die, then ⌦ = {1, 2, 3, 4, 5, 6}. Happily, in the analysis of randomized algorithms, we can almost always take ⌦ to be a ﬁnite set and work only with discrete probability, which is much more elementary than general probability theory.  Each element i of a sample space ⌦ comes with a nonnegative probability p i , which can be thought of as the frequency with which the outcome of the random process is i. For example, if a six-sided die is fair, then p i  is 1 6 for each i = 1, 2, 3, 4, 5, 6. In general, since ⌦ is supposed to be everything that could be possibly happen, the  1In addition to the Lehman-Leighton lecture notes mentioned in Appendix A, there is a free Wikibook on discrete probability  https:  en.wikibooks.org  wiki High_School_Mathematics_Extensions Discrete_Probability .  186   B.2 Events  187  probabilities should sum to 1:  p i  = 1.  Xi2⌦  A common special case is when every element of ⌦ is equally likely— known as the uniform distribution—in which case p i  = 1 for every ⌦ i 2 ⌦.2 This may seem like a pretty abstract concept, so let’s introduce two running examples. In the ﬁrst example, the random process is a throw of two standard  six-sided  dice. The sample space is the set of 36 diﬀerent things that could happen:  ⌦ = { 1, 1 ,  2, 1 ,  3, 1 , . . . ,  5, 6 ,  6, 6 }  .  36 ordered pairs  {z  }    36 for every i 2 ⌦.  Assuming the dice are fair, each of these outcomes is equally likely: p i  = 1  The second example, more germane to algorithms, is the choice of the pivot element in the outermost call to randomized QuickSort  Section 5.4 . Any element of the input array can be chosen as the pivot, so  where n is the length of the input array. By deﬁnition, in randomized QuickSort each element is equally likely to be chosen as the pivot element, and so p i  = 1  ⌦ =  ,  possible positions of pivot element  }  {1, 2, 3, . . . , n}    {z n for every i 2 ⌦. B.2 Events  An event is a subset S ✓ ⌦ of the sample space—a collection of possible outcomes of a random process. The probability Pr[S] of an event S is deﬁned as you would expect, as the probability that one of the outcomes of S occurs:  Pr[S] =Xi2S  p i .  Let’s get some practice with this concept using our two running examples.  2For a ﬁnite set S, S denotes the number of elements in S.   188  Quick Review of Discrete Probability  Quiz B.1  Let S denote the set of outcomes for which the sum of two standard dice equals 7. What is the probability of the event S?3  a   1 36  b   1 12 c  1 6 d  1 2  b  1 4 c  1 2 d  3 4   See Section B.2.1 for the solution and discussion.   The second quiz concerns the choice of the random pivot element in the outermost call to QuickSort. We say that a pivot element is an “approximate median” if at least 25% of the array elements are less than the pivot, and at least 25% of the elements are greater than the pivot.  Quiz B.2  Let S denote the event that the chosen pivot element in the outermost call to QuickSort is an approximate median. What is the probability of the event S?  a  1  n, where n is the length of the array   See Section B.2.2 for the solution and discussion.   3A useful fact to know when playing the dice game craps. . .   B.3 Random Variables  189  B.2.1 Solution to Quiz B.1  Correct answer:  c . There are six outcomes in which the sum of the two dice equals 7:  S = { 6, 1 ,  5, 2 ,  4, 3 ,  3, 4 ,  2, 5 ,  1, 6 }.  Since every outcome of ⌦ is equally likely, p i  = 1 and so  36 for every i 2 S  Pr[S] = S · 1  36 = 6  36 = 1 6 .  B.2.2 Solution to Quiz B.2  4 elements, the next-smallest n  Correct answer:  c . As a thought experiment, imagine dividing the elements in the input array into four groups: the smallest n 4 elements, the next-smallest n 4 elements, and ﬁnally the largest n 4 elements.  As usual, we’re ignoring fractions for simplicity.  Every element of the second and third groups is an approximate median: all the n 4 elements from the ﬁrst and last groups are less than and greater than the pivot, respectively. Conversely, if the algorithm picks a pivot element from either the ﬁrst or the last group, either the elements less than the pivot comprise only a strict subset of the ﬁrst group, or the elements greater than the pivot are only a strict subset of the last group. In this case, the pivot element is not an approximate median. Thus the event S corresponds to the n 2 elements in the second and third groups; since each element is equally likely to be chosen as the pivot element,  Pr[S] = S · 1  n = n  2 · 1  n = 1 2 .  B.3 Random Variables  A random variable is a numerical measurement of the outcome of a random process. Formally, it is a real-valued function X : ⌦ ! R deﬁned on the sample space ⌦—the input i 2 ⌦ to X is an outcome of the random process, and the output X i  is a numerical value. In our ﬁrst running example, we can deﬁne a random variable that is the sum of the two dice. This random variable maps outcomes  pairs  i, j  with i, j 2 {1, 2, . . . , 6}  to real numbers according to the   190  Quick Review of Discrete Probability  map  i, j  7! i + j. In our second running example, we can deﬁne a random variable that is the length of the subarray passed to the ﬁrst recursive call to QuickSort. This random variable maps each outcome  that is, each choice of a pivot element  to an integer between 0  if the chosen pivot is the minimum element  and n  1, where n is the length of the input array  if the chosen pivot is the maximum element . Section 5.5 studies the random variable X that is the running time of randomized QuickSort on a given input array. Here, the state space ⌦ is all possible sequences of pivot elements the algorithm might choose, and X i  is the number of operations performed by the algorithm for a particular sequence i 2 ⌦ of pivot choices.4  B.4 Expectation  The expectation or expected value E[X] of a random variable X is its average value over everything that could happen, weighted appro- priately with the probabilities of diﬀerent outcomes. Intuitively, if a random process is repeated over and over again, E[X] is the long-run average value of the random variable X. For example, if X is the value of a fair six-sided die, then E[X] = 3.5.  In math, if X : ⌦ ! R is a random variable and p i  denotes the  probability of outcome i 2 ⌦,  E[X] =Xi2⌦  p i  · X i .   B.1   The next two quizzes ask you to compute the expectation of the two random variables deﬁned in the preceding section.  Quiz B.3  What is the expectation of the sum of two dice?  a  6.5  b  7  c  7.5  4Since the only randomness in randomized QuickSort is in the choice of pivot elements, once we ﬁx these choices, QuickSort has some well-deﬁned running time.   B.4 Expectation  d  8  191   See Section B.4.1 for the solution and discussion.   Returning to randomized QuickSort, how big, on average, is the length of the subarray passed to the ﬁrst recursive call? Equivalently, how many elements are less than a randomly chosen pivot on average?  Quiz B.4  Which of the following is closest to the expectation of the size of the subarray passed to the ﬁrst recursive call in QuickSort?  a  n 4 b  n 3 c  n 2 d  3n 4   See Section B.4.2 for the solution and discussion.   B.4.1 Solution to Quiz B.3  Correct answer:  b . There are several ways to see why the expec- tation is 7. The ﬁrst way is to compute it by brute force, using the deﬁning equation  B.1 . With 36 possible outcomes, this is doable but tedious. A slicker way is to pair up the possible values of the sum and use symmetry. The sum is equally likely to be 2 or 12, equally likely to be 3 or 11, and so on. In each of these pairs the average value is 7, so this is also the average value overall. The third and best way is to use linearity of expectation, as covered in the next section.  B.4.2 Solution to Quiz B.4  Correct answer:  c . The exact value of the expectation is  n 1  2. There is a 1 n chance that the subarray has length 0  if the pivot element is the smallest element , a 1 n chance that it has length 1  if the pivot element is the second-smallest element , and so on, up   192  Quick Review of Discrete Probability  to a 1 n chance that it has length n   1  if the pivot element is the largest element . By the deﬁnition  B.1  of expectation, and recalling the identityPn 1  i=1 i = n n 1   ,5 we have  2  1 n · 0 +  1 n · 1 + ··· +  1 n ·  n   1  =  1 n ·  1 + 2 + ··· +  n   1    E[X] =    =  n   1  .  2  n n 1   =  {z  2  }  B.5 Linearity of Expectation  B.5.1 Formal Statement and Use Cases  Our ﬁnal concept is a mathematical property, not a deﬁnition. Lin- earity of expectation is the property that the expectation of a sum of random variables is equal to the sum of their individual expectations. It is incredibly useful for computing the expectation of a complex random variable, like the running time of randomized QuickSort, when the random variable can be expressed as a weighted sum of simpler random variables.  Theorem B.1  Linearity of Expectation  Let X1, . . . , Xn be random variables deﬁned on the same sample space ⌦, and let a1, . . . , an be real numbers. Then  E24  nXj=1  aj · Xj35 =  nXj=1  aj · E[Xj] .   B.2   That is, you can take the sum and the expectation in either order j=1 ajXj is a complex random variable  like the running time of randomized  and get the same thing. The common use case is whenPn  5One way to see that 1 + 2 + ··· +  n   1  = n n 1   is to use induction on n  see Section A.2 . For a sneakier proof, take two copies of the left-hand side and pair up the “1” from the ﬁrst copy with the “n   1” from the second copy, the “2” from the ﬁrst copy with the “n   2” from the second copy, and so on. This gives n   1 pairs with value n each. Since double the sum equals n n   1 , the original sum equals n n 1   .  2  2   B.5  Linearity of Expectation  193  QuickSort  and the Xj’s are simple random variables  like 0-1 random variables .6  For example, let X be the sum of two standard dice. We can write X as the sum of two random variables X1 and X2, which are the values of the ﬁrst and second die, respectively. The expectation of X1 or X2 is easy to compute using the deﬁnition  B.1  as 1 6  1+2+3+4+5+6  = 3.5. Linearity of expectation then gives  E[X] = E[X1] + E[X2] = 3.5 + 3.5 = 7,  replicating our answer in Section B.4.1 with less work.  An extremely important point is that linearity of expectation holds even for random variables that are not independent. We won’t need to formally deﬁne independence in this book, but you probably have good intuition about what it means: knowing something about the value of one random variable doesn’t tell you anything new about the values of the others. For example, the random variables X1 and X2 above are independent because the two dice are assumed to be thrown independently.  For an example of dependent random variables, consider a pair of magnetically linked dice, where the second die always comes up with value one larger than that of the ﬁrst  or 1, if the ﬁrst die comes up 6 . Now, knowing the value of either die tells you exactly what the value of the other die is. But we can still write the sum X of the two dice as X1 + X2, where X1 and X2 are the values of the two dice. It is still the case that X1, viewed in isolation, is equally likely to be each of {1, 2, 3, 4, 5, 6}, and the same is true for X2. Thus we still have E[X1] = E[X2] = 3.5 and by linearity of expectation we still have E[X] = 7.  Why should you be surprised? Superﬁcially, the identity in  B.2  might look like a tautology. But if we switch from sums to products of random variables, the analog of Theorem B.1 no longer holds for dependent random variables.7 So linearity of expectation really is a special property about sums of random variables.  6In the Stanford version of this course, over ten weeks of blackboard lectures, I draw a box around exactly one mathematical identity—linearity of expectation. 7The magnetically linked dice provide one counterexample. For an even simpler counterexample, suppose X1 and X2 are either equal to 0 and 1, or to 1 and 0, with each outcome having 50% probability. Then E[X1 · X2] = 0 while 4 . E[X1] · E[X2] = 1   194  Quick Review of Discrete Probability  B.5.2 The Proof  The utility of linearity of expectation is matched only by the simplicity of its proof.8 Proof of Theorem B.1: Starting with the right-hand side of  B.2  and expanding using the deﬁnition  B.1  of expectation gives   B.3   Reversing the order of summation, we have  nXj=1  =  aj · E[Xj] =  aj · Xi2⌦ p i  · Xj i ! nXj=1 aj · p i  · Xj i ! . nXj=1 Xi2⌦ aj · p i  · Xj i 1A . 0@ aj · p i  · Xj i ! =Xi2⌦ nXj=1 aj · Xj i 1A . p i  ·0@ aj · p i  · Xj i 1A =Xi2⌦ nXj=1 aj · Xj35 . aj · Xj i 1A = E24 p i  ·0@  nXj=1  nXj=1  nXj=1  Xi2⌦  nXj=1 Xi2⌦ 0@  Xi2⌦  Finally, using again the deﬁnition  B.1  of expectation, we obtain the left-hand side of  B.2 :  Since p i  is independent of j = 1, 2, . . . , n, we can pull it out of the inner sum:  QE D  double summation.  That’s it! Linearity of expectation is really just a reversal of a  Speaking of double summations, equation  B.3  might seem opaque if you’re rusty on these kinds of algebraic manipulations. For a down- to-earth way to think about it, arrange the ajp i Xj i ’s in a grid, 8The ﬁrst time you read this proof, you should assume for simplicity that  a1 = a2 = ··· = an = 1.   B.6 Example: Load Balancing  195  with rows indexed by i 2 ⌦, columns indexed by j 2 {1, 2, . . . , n}, and the number aj · p i  · Xj i  in the cell in the ith row and jth column:  " i   8>>>>>>>> >>>>>>>:  . . . . . . ··· . . . . . .  ... ...  . . . . . . ··· ajp i Xj i  . . . . . .  ... ...  . . . . . . ··· . . . . . .  . . . . . . ··· . . . . . .      j !  {z  }  The left-hand side of  B.3  ﬁrst sums up each of the columns, and then adds up these column sums. The right-hand side ﬁrst sums up the rows and then sums up these row sums. Either way, you get the sum of all the entries in the grid.  B.6 Example: Load Balancing  To tie together all the preceding concepts, let’s study an example about load balancing. Suppose we need an algorithm that assigns processes to servers, but we’re feeling super-lazy. One easy solution is to just assign each process to a random server, with each server equally likely. How well does this work?9  For concreteness, assume there are n processes and also n servers, where n is some positive integer. First, let’s be clear on the sample space: the set ⌦ is all nn possible ways of assigning the processes to the servers, with n choices for each of the n processes. By the deﬁnition of our lazy algorithm, each of these nn outcomes is equally likely.  Now that we have a sample space, we’re in a position to deﬁne random variables. One interesting quantity is server load, so let’s deﬁne Y as the random variable equal to the number of processes that get assigned to the ﬁrst server.  The story is the same for all the servers by symmetry, so we may as well focus on the ﬁrst one.  What is the expectation of Y ?  In principle, we can compute E[Y ] by brute-force evaluation of the deﬁning equation  B.1 , but this is impractical for all but the  9This example is also relevant to our discussion of hashing in Part 2.   196  Quick Review of Discrete Probability  smallest values of n. Fortunately, since Y can be expressed as a sum of simple random variables, linearity of expectation can save the day. Formally, for j = 1, 2, . . . , n, deﬁne  Xj =⇢ 1 if the jth process gets assigned to the ﬁrst server  0 otherwise.  Random variables that only take on the values 0 and 1 are often called indicator random variables because they indicate whether some event occurs  like the event that process j gets assigned to the ﬁrst server .  From the deﬁnitions, we can express Y as the sum of the Xj’s:  Y =  Xj.  nXj=1  By linearity of expectation  Theorem B.1 , the expectation of Y is then the sum of the expectations of the Xj’s:  E[Y ] = E24  nXj=1  Xj35 =  nXj=1  E[Xj] .  Because each random variable Xj is so simple, it’s easy to compute its expectation directly:  E[Xj] = 0 · Pr[Xj = 0]  +1 · Pr[Xj = 1] = Pr[Xj = 1] .  Since the jth process is equally likely to be assigned to each of the n servers, Pr[Xj = 1] = 1  n. Putting it all together, we have    =0  {z  }  nXj=1  E[Y ] =  E[Xj] = n ·  1 n  = 1.  So if we care only about average server loads, our super-lazy algo- rithm works just ﬁne! This example and randomized QuickSort are characteristic of the role that randomization plays in algorithm design: we can often get away with really simple heuristics if we make random choices along the way.   B.6 Example: Load Balancing  197  Quiz B.5  Consider a group of k people. Assume that each person’s birthday is drawn uniformly at random from the 365 pos- sibilities.  And ignore leap years.  What is the smallest value of k such that the expected number of pairs of distinct people with the same birthday is at least one? [Hint: Deﬁne an indicator random variable for each pair of people. Use linearity of expectation.]  a  20  b  23  c  27  d  28  e  366   See below for the solution and discussion.   Correct answer:  d . Fix a positive integer k, and denote the set of people by {1, 2, . . . , k}. Let Y denote the number of pairs of people with the same birthday. As suggested by the hint, deﬁne one random variable Xij for every choice i, j 2 {1, 2, . . . , k} of people with i < j. Deﬁne Xij as 1 if i and j have the same birthday and 0 otherwise. Thus, the Xij’s are indicator random variables, and  By linearity of expectation  Theorem B.1 ,  Y =  Xij.  k 1Xi=1 kXj=i+1 Xij35 =  E[Y ] = E24  k 1Xi=1  kXj=i+1  k 1Xi=1  kXj=i+1  E[Xij] .   B.4   Since Xij is an indicator random variable, E[Xij] = Pr[Xij = 1]. There are  365 2 possibilities for the birthdays of the people i and j, and in 365 of these possibilities i and j have the same birthday.   198  Quick Review of Discrete Probability  Assuming that all birthday combinations are equally likely,  Pr[Xij = 1] =  365  365 2 =  1 365  .  Plugging this in to  B.4 , we have  E[Y ] =  k 1Xi=1  kXj=i+1  1 365  =  1  365 ·✓k 2◆ =  k k   1   ,  730  2  denotes the binomial coeﬃcient “k choose 2”  as in the where  k solution to Quiz 3.1 . The smallest value of k for which k k 1  730   1 is 28.   Index  70   x  absolute value , 48  n 2   binomial coeﬃcient , 44, dxe  ceiling , 175 n!  factorial , 149 bxc  ﬂoor , 90  S  set size , 187 = vs. :=, 9  ACM, 53 Adleman, Leonard, 172 algorithm, 1  constant-time, 38 fast, 30 linear-time, 31 mind-blowing, 60 quadratic-time, 38  among friends, 14, 19, 94 applications, 2 asymptotic analysis, 29, 57 asymptotic notation, 36 as a sweet spot, 36 big-O notation, see big-O  big-O vs. big-theta nota-  notation  tion, 52  big-omega notation, 50 big-theta notation, 51 history, 53 in seven words, 37  little-o notation, 52 average-case analysis, 27  base case  induction , 183 base case  recursion , 8 big-O notation, 45–47  as a game, 47 English deﬁnition, 45 high-level idea, 38 mathematical deﬁnition,  45  pictorial deﬁnition, 45  big-omega notation, 50 big-picture analysis, 27 big-theta notation, 51 binary search, 98 birthday paradox, 196 bit, 147 blazingly fast, viii, 31, 85, 117,  119, 127  Blum, Manuel, 172 brute-force search  for closest pair, 78 for counting inversions, 63  BubbleSort, 14, 146 BucketSort, 146  can we do better?, 6 cf., 108 Chen, Ke, 182 ChoosePivot  199   200  Index  median-of-three implemen-  decomposition blueprint, 137,  closest pair  tation, 153  naive implementation, 128,  130 overkill  implementation,  randomized implementa-  129, 131  tion, 132  brute-force search, 78 correctness, 86–88 left vs. right vs. split, 80 one-dimensional case, 78 problem deﬁnition, 78 pseudocode, 80, 82, 83 running time, 81  cocktail party, ix coin ﬂipping, 165 collaborative ﬁltering, 62–63 computational geometry, 77 computational lens, 2 constant, 47, 96  reverse engineering,  49,  179  constant factors, 28, 37, 38 Cormen, Thomas H., 146 corollary, 19 counting inversions  correctness, 64, 69 implementation, 91 problem deﬁnition, 61 pseudocode, 64, 65 running time, 69 split inversions, 68  CountingSort, 147  stable implementation, 148  Coursera, x  Dasgupta, Sanjoy, 95  design patterns, ix discussion forum, xi divide-and-conquer, 11, 12, 60–  for closest pair, 80 for counting inversions, 63 for matrix multiplication,  for sorting, 61 proofs of correctness, 121  double summation, 194 DSelect  30-70 Lemma, 174–176 as a knockout tournament,  does not run in place, 170 heuristic analysis, 178 history, 171 pseudocode, 169 running time, 171 running time analysis, 172–  164  61  73  169  179  vs. RSelect, 168, 171 with groups of 3 or 7, 182  Dumitrescu, Adrian, 182  Euclidean distance, 78 event, 187 exhaustive search, see brute-  force search  expectation, 190  linearity of, 192 expected value, 190  fast algorithm, 30 Federer, Roger, 172 Floyd, Robert W., 172   for-free primitive, viii, 31, 79,  Karatsuba multiplication, 6–11  Index  133  Gauss’s trick, 10, 75 Gauss, Carl Friedrich, 10 geometric series, 110–111, 167 good vs. evil, 107 Google, 2 googol, 38 greatest hits, ix guess-and-check method, 178 guiding principles, 26–31  hall of fame, 117 Hoare, Tony, 120 Hopcroft, John E., 172  i.e., 63 in-place algorithm, 117 induction, see proofs, by induc-  inductive hypothesis, 184 inductive step, 183 InsertionSort, 14, 28, 146 integer multiplication, 3–11, 92–  grade-school algorithm, 4 Karatsuba’s algorithm, 11 simple recursive algorithm,  tion  94  9  interview questions, ix invariant, 122 inversion, 61  left vs. right vs. split, 63  IQ points, 3  Karatsuba, 11  implementation, 35 recurrence, 93 running time, 99  201  in Python, 100  Karatsuba, Anatoly, 6 key-value pair, 14 Kleinberg, Jon, 60 Knuth, Donald E., 53  Lehman, Eric, x, 183 Leighton, Tom, x, 183 Leiserson, Charles E., 146 lemma, 19 linear-time algorithm, 31 linearity of expectation, 192  doesn’t need independence,  141, 193  little-o notation, 52 ln x, 97 logarithms, 20, 97 lower-order terms, 37  mangosteen, 146 mantra, 6 master method  a, b, and d, 95, 103 applied to RecIntMult, 99 applied to Karatsuba, 99 applied to MergeSort, 98 applied to Strassen, 100,  applied to binary search,  big-theta vs. big-O, 97 does not apply, 134, 138,  formal statement, 96 meaning of the three cases,  107–109  more general versions, 96 proof, 104–112  102  98, 101  178   202  Index  master theorem, see master  mathematical background, x,  method  182–196  O f  n  , see big-O notation o f  n  , see little-o notation ⌦ f  n  , see big-omega nota-  tion  matrix multiplication  order statistic, 139, 155  deﬁnition, 71 exponent, 103 iterative algorithm, 72 simple recursive algorithm,  Strassen’s algorithm, 75–  74  77  median, 129, 156  the 2 ⇥ 2 case, 71 approximate, 134, 165, 188 vs. mean, 156 weighted, 181  median-of-medians,  see  DSelect  Merge, 17–18  for counting inversions, 66 running time, 18–19  MergeSort, 12–26 analysis, 21–24 as a divide-and-conquer al-  gorithm, 61  implementation, 91 is comparison-based, 146 motivation, 12 pseudocode, 16 recurrence, 94 running time, 20, 98  QuickSort  does not run in place, 117  Moore’s Law, 2, 29  n log n vs. n2, 21, 29 Nobel Prize, see Turing Award  Papadimitriou, Christos, 95 Partition, 125  proof of correctness, 126 runs in place, 127  pivot element, 118 Pratt, Vaughan, 172 prime number, 132 primitive operation, 4, 18, 24,  28  probability, 186  of an event, 187  problems vs. solutions, 3 programming, x, 11 programming problems, xi proofs, x  by contradiction, 49 by induction, 65, 121, 179,  183–184  of correctness, 121  proposition, 19 pseudocode, 11, 16 Pyrrhic victory, 176 Pythagorean theorem, 88  QE D  q.e.d. , 24 QuickSort  best-case scenario, 129 handling ties, 118 high-level description, 120 history, 120 implementation, 153 is comparison-based, 146 is not stable, 148   Index  203  median-of-three, 153 partitioning  around pivot, 118, 121–127  a  pivot element, 118 proof of correctness, 121 pseudocode, 127 random shuﬄe, 133 randomized, 132 running time, 133 running time  intuition ,  running time  proof , 135–  134–135  145  runs in place, 117 worst-case scenario, 128  quizzes, xi  RadixSort, 147 random variable, 189  geometric, 166 independent, 193 indicator, 196  randomized algorithms, 132,  167, 196  analysis  rate of growth, see asymptotic  RecIntMult, 9  recurrence, 93 running time, 99  RecMatMult, 74 recommendation system, 62–63 recurrence, 93  standard, 95  recursion, 8 recursion tree, 21, 104 reduction, 157 Rivest, Ronald L., 146, 172 rookie mistake, 109 RSelect  best-case scenario, 161 expected running time, 161 implementation, 182 pseudocode, 159 running time analysis, 163–  167  runs in place, 162 worst-case scenario, 160  RSP  rate of subproblem pro- liferation , see master method, meaning of the three cases running time, 18, 24, 38 RWS  rate of work shrinkage , see master method, meaning of the three cases  sample space, 186 Sedgewick, Robert, 118 selection  DSelect, see DSelect RSelect, see RSelect problem deﬁnition, 155 reduces to sorting, 156  SelectionSort, 14, 146 Shamir, Adi, 172 SIGACT, 53 solutions, xi sorting  MergeSort, see MergeSort MergeSort vs. QuickSort,  117  QuickSort, see QuickSort associated data, 14 by key, 14 comparison-based, 146 in place, 117 in Unix, 146   204  Index  lower bound, 146, 148 non-comparison-based,  146–148  problem deﬁnition, 13 randomized, 132, 148 simple algorithms, 14–15 stable, 148 with duplicates, 14 with folk dancers, 125  Stanford Lagunita, x starred sections, x, 78 Stein, Cliﬀord, 146 Strassen, 75–77  running time, 100, 102  Strassen, Volker, 77 Sudoku vs. KenKen, 146 superteam, 171 Tardos, Éva, 60 Tarjan, Robert E., 172 test cases, xi theorem, 19 ⇥ f  n  , see big-theta notation theta notation, see big-theta  notation  tug-of-war, 107 Turing Award, 120, 172 uniform distribution, 187 upshot, viii Vazirani, Umesh, 95 videos, xi von Neumann, John, 12 Wayne, Kevin, 118 why bother?, viii, 1 work, see running time worst-case analysis, 26 YouTube, xi

@highlight

Algorithms are the heart and soul of computer science. Their applications range from network routing and computational genomics to public-key cryptography and machine learning. Studying algorithms can make you a better programmer, a clearer thinker, and a master of technical interviews. Algorithms Illuminated is an accessible introduction to the subject for anyone with at least a little programming experience. The exposition emphasizes the big picture and conceptual understanding over low-level implementation and mathematical details---like a transcript of what an expert algorithms tutor would say over a series of one-on-one lessons. Part 1 covers asymptotic analysis and big-O notation, divide-and-conquer algorithms and the master method, randomized algorithms, and several famous algorithms for sorting and selection