Neural  Network  Learning: Theoretical  Foundations  This  book describes recent  theoretical  advances  in the  study of  artificial neural  networks.  It  explores  probabilistic  models of supervised  learning  problems,  and  addresses  the  key statistical  and  computational questions. Research  on  pattern  classification  with  binary-output networks  is surveyed,  including  a discussion  of the  relevance of  the Vapnik-Chervonenkis  dimension.  Estimates  of this  dimension  are  calculated  for  several neural network  models. A model of classification  by  real-output  networks  is developed,  and  the  usefulness  of  classification with  a  large margin  is demonstrated.  The  authors  explain  the  role of scale-sensitive versions of the Vapnik-Chervonenkis  dimension  in  large  margin  classification,  and  in real estimation.  They  also discuss  the computational  complexity  of neural  network  learning, describing  a variety  of hardness  results, and  outlining  two efficient  constructive learning  algorithms.  The  book  is self-contained  and  is intended  to  be accessible to  researchers  and  graduate  students  in computer  science,  engineering,  and  mathematics.  Martin  Anthony  is Reader  in  Mathematics  and  Executive  Director of the  Centre  for  Discrete and  Applicable  Mathematics  at  the  London  School of Economics and  Political  Science.  Peter  Bartlett  is a  Senior  Fellow st  the  Research  School of  Information  Sciences and  Engineering  at  the  Australian  National  University.    Neural Network  Learning: Theoretical  Foundations  Martin Anthony and Peter L. Bartlett  CAMBRIDGE UNIVERSITY  PRESS   Cambridge, New York, Melbourne, Madrid, Cape Town, Singapore, Sao Paulo, Delhi  CAMBRIDGE UNIVERSITY PRESS  Cambridge University Press  The Edinburgh Building, Cambridge CB2 8RU, UK  Published in the United States of America by Cambridge University Press, New York  Information on this title: www.cambridge.org 9780521118620  www. Cambridge. org    Cambridge University Press 1999  This publication is in copyright. Subject to statutory exception and to the provisions of relevant collective licensing agreements, no reproduction of any part may take place without the written  permission of Cambridge University Press.  First published 1999 Reprinted 2001, 2002  This digitally printed version 2009  A catalogue record for  this publication is available from the British Library  Library of Congress Cataloguing in Publication data  Anthony, Martin.  Learning in neural networks : theoretical foundations    Martin Anthony and Peter L. Bartlett.  1. Neural networks  Computer science .  I. Bartlett, Peter L.,  p.  cm.  Includes bibliographical references. ISBN 0 521 57353 X  hardcover   1966-   .  II. Title.  QA76.87.A58  1999  006.3'2-dc21  98-53260  CIP  ISBN 978-0-521-57353-5 hardback ISBN 978-0-521-11862-0 paperback   To Colleen, Selena and James.    Contents  face   -re, 1 1.1 1.2 1.3 1.4  2 2.1 2.2 2.3 2.4 2.5 2.6 3 3.1 3.2 3.3 3.4 4 4.1 4.2 4.3 4.4 4.5 4.6 4.7  page xui 1 1 2 7 9  Introduction Supervised  learning Artificial  neural networks Outline of the book Bibliographical notes Part  one:  Pattern  Classification  with  Binary-Output Neural Networks The Pattern  Classification  Problem The learning problem Learning finite function  classes Applications to  perceptrons Restricted model Remarks Bibliographical  notes The Growth Function and VC-Dimension Introduction The growth  function The Vapnik-Chervonenkis dimension Bibliographical notes General Upper  Bounds on Sample Complexity Learning by minimizing sample error Uniform  convergence and  learnability Proof of uniform  convergence result Application to the  perceptron The restricted  model Remarks Bibliographical notes  11 13 13 19 22 23 25 27 29 29 29 35 41 42 42 43 45 50 52 53 58  Vll   viii   Contents  5 5.1 5.2 5.3 5.4 5.5 5.6 6 6.1 6.2 6.3 6.4 6.5 7 7.1 7.2 7.3 7.4 7.5 7.6 8 8.1 8.2 8.3 8.4 8.5 8.6  9 9.1 9.2 9.3 9.4 10 10.1 10.2 10.3 10.4 10.5  General Lower Bounds on Sample Complexity 59 Introduction 59 A lower bound for  learning 59 The restricted  model 65 VC-dimension quantifies  sample complexity 69 Remarks 71 Bibliographical  notes 72 The VC-Dimension of Linear Threshold  Networks 74 Feed-forward  neural networks 74 Upper  bound 77 Lower bounds 80 Sigmoid networks 83 Bibliographical  notes 85 Bounding the VC-Dimension using Geometric Techniques 86 Introduction 86 The need for conditions on the activation  functions 86 A bound on the growth  function 89 92 Proof of the growth function  bound More on solution set components bounds 102 Bibliographical notes 106 Vapnik-Chervonenkis Dimension Bounds for Neural Networks 108 Introduction 108 Function classes that  are polynomial in their  parameters 108 112 Piecewise-polynomial  networks 122 Standard  sigmoid  networks Remarks 128 Bibliographical notes 129 Part  two:  Pattern  Classification  with  Real-Output Networks Classification  with Real-Valued  Functions Introduction Large margin  classifiers Remarks Bibliographical notes Covering Numbers and Uniform  Convergence Introduction Covering numbers A uniform  convergence result Covering numbers in general Remarks  131 133 133 135 138 138 140 140 140 143 147 149   Contents   The Sample Complexity of Classification  Learning   The Pseudo-Dimension  and Fat-Shattering  Dimension   10.6  Bibliographical notes  11  11.1  Introduction  11.2  The pseudo-dimension  11.3  The fat-shattering  dimension  11.4  Bibliographical notes  12  Bounding Covering Numbers with Dimensions  12.1  Introduction  12.2  Packing numbers  12.3  Bounding with the pseudo-dimension  12.4  Bounding with the fat-shattering  dimension  12.5  Comparing the two approaches  12.6  Remarks  12.7  Bibliographical notes  13  13.1  Large margin SEM algorithms  13.2  Large margin SEM algorithms as learning algorithms  13.3  Lower bounds for certain function  classes  13.4  Using the pseudo-dimension  13.5  Remarks  13.6  Bibliographical notes  14  14.1  Introduction  14.2  Pseudo-dimension of neural networks  14.3  Fat-shattering dimension bounds:  number of parameters  14.4  Fat-shattering dimension bounds:  size of parameters  14.5  Remarks  14.6  Bibliographical notes  15  Model Selection  15.1  Introduction  15.2  Model selection results  15.3  Proofs of the results  15.4  Remarks  15.5  Bibliographical notes   The Dimensions of Neural Networks   ix  150 151 151 151 159 163 165 165 165 167 174 181 182 183 184 184 185 188 191 191 192 193 193 194 196 203 213 216 218 218 220 223 225 227   x   Contents  finiteness   Part  three:  Learning  Real-Valued  Functions  Learning Classes of Real Functions   Sample Complexity of Learning Real Function  Classes   16  16.1  Introduction  16.2  The learning framework  for real estimation  16.3  Learning finite classes of real functions  16.4  A substitute for  16.5  Remarks  16.6  Bibliographical notes  17  Uniform  Convergence Results for  Real Function  Classes  17.1  Uniform  convergence for real functions  17.2  Remarks  17.3  Bibliographical notes  18  Bounding Covering Numbers  18.1  Introduction  18.2  Bounding with the fat-shattering  dimension  18.3  Bounding with the pseudo-dimension  18.4  Comparing the different  approaches  18.5  Remarks  18.6  Bibliographical notes  19  19.1  Introduction  19.2  Classes with finite fat-shattering  dimension  19.3  Classes with finite pseudo-dimension  19.4  Results for neural networks  19.5  Lower bounds  19.6  Remarks  19.7  Bibliographical notes  20  20.1  Introduction  20.2  Lower bounds for non-convex classes  20.3  Upper bounds for  convex classes  20.4  Remarks  20.5  Bibliographical notes  21  Other Learning Problems  21.1  Loss functions  in general  21.2  Convergence for general loss functions  21.3  Learning in multiple-output  networks  21.4  Interpolation  models  21.5  Remarks  21.6  Bibliographical notes   Convex Classes   229 231 231 232 234 236 239 240 241 241 245 246 247 247 247 250 254 255 256 258 258 258 260 261 262 265 267 269 269 270 277 280 282 284 284 285 286 289 295 296   Contents   xi  The Boolean Perceptron   Learning as Optimization   Part  four:  Algorithmics  Efficient  Learning   297 299 299 299 301 302 305 306 307 307 307 311 312 312 314 315 316 316 316 319 322 328 329 331 331 331 335 337 338 339 Constructive Learning Algorithms for  Two-Layer Networks  342 342 342 351 355 357 365 379 382  22  22.1  Introduction  22.2  Graded function  classes  22.3  Efficient  learning  22.4  General classes of efficient  learning algorithms  22.5  Efficient  learning in the restricted  model  22.6  Bibliographical notes  23  23.1  Introduction  23.2  Randomized  algorithms  23.3  Learning as randomized optimization  23.4  A characterization of efficient  learning  23.5  The hardness of learning  23.6  Remarks  23.7  Bibliographical notes  24  24.1  Introduction  24.2  Learning is hard for the simple perceptron  24.3  Learning is easy for fixed  fan-in  perceptrons  24.4  Perceptron  learning in the restricted  model  24.5  Remarks  24.6  Bibliographical notes  25  Hardness Results for  Feed-Forward Networks  25.1  Introduction  25.2  Linear threshold networks with binary inputs  25.3  Linear threshold  networks with real inputs  25.4  Sigmoid networks  25.5  Remarks  25.6  Bibliographical notes  26  26.1  Introduction  26.2  Real estimation with convex combinations  26.3  Classification  learning using boosting  26.4  Bibliographical notes  Appendix 1  Useful  Results  Bibliography  Author index  Subject index     Preface  Results from  computational  learning theory  are important  in many as- pects  of  machine  learning  practice.  Understanding  the  behaviour  of systems  that  learn  to  solve information  processing problems   like  pat- tern recognition and prediction  is crucial for the design of effective sys- tems.  In  recent  years, ideas  and  techniques  in  computational  learning theory  have matured  to  the  point  where  theoretical  advances  are now contributing  to  machine  learning  applications,  both  through  increased understanding and through the development of new practical algorithms. In  this  book,  we concentrate  on  statistical  and  computational  ques- tions  associated  with  the  use of rich function  classes, such  as  artificial neural networks, for pattern recognition and prediction problems.  These issues are of fundamental  importance in machine learning, and we have seen several significant advances in this area in the last decade. The book focuses on three specific models of learning, although the techniques, re- sults, and intuitions we obtain from  studying these formal models carry over to many other situations.  The book is aimed at  researchers and graduate students in computer science, engineering,  and  mathematics.  The reader  is assumed  to have some familiarity  with analysis, probability, calculus, and linear  algebra, to  the  level  of  an  early  undergraduate  course.  We remind  the  reader of  most  definitions,  so  it  should  suffice  just  to  have  met  the  concepts before.  Most chapters have a 'Remarks' section near the end, containing ma- terial  that  is  somewhat  tangential  to  the  main flow of  the  text.  All chapters finish with a 'Bibliographical Notes' section giving pointers to the literature, both  for  the  material  in the  chapter  and  related  results. However these sections are not exhaustive.  It is a pleasure to thank  many colleagues and friends  for their contri-  Xlll   xiv   Preface  butions  to  this  book.  Thanks, in particular,  to  Phil  Long for  carefully and thoroughly reading the book, and making many helpful suggestions, and to Ron Meir for making many thoughtful  comments on large sections of the book.  Jon Baxter considerably improved the results in Chapter 5, and  made several useful  suggestions  that  improved  the  presentation of topics in Chapter 7. Gabor Lugosi suggested significant improvements to the results in Chapter 4.  Thanks also to James Ashton, Shai Ben-David, Graham Brightwell, Mostefa  Golea, Ying Guo, Ralf Herbrich, Wee Sun Lee,  Frederic  Maire,  Shie  Mannor,  Llew  Mason,  Michael  Schmitt  and Ben Veal for  comments, corrections, and  suggestions.  It  is also a plea- sure to thank the many collaborators and colleagues who have influenced the  way we think  about  the  topics covered in this  book:  Andrew  Bar- ron, Jon Baxter, Shai Ben-David, Norman Biggs, Soura Dasgupta, Tom Downs, Paul Fischer, Marcus Frean, Yoav Freund, Mostefa Golea, Dave Helmbold, Klaus Hoffgen,  Adam Kowalczyk, Sanjeev Kulkarni, Wee Sun Lee, Tamas  Linder,  Phil  Long,  David  Lovell, Gabor  Lugosi,  Wolfgang Maass,  Llew  Mason,  Ron  Meir,  Eli  Posner,  Rob  Schapire,  Bernhard Scholkopf,  John  Shawe-Taylor,  Alex Smola,  and  Bob Williamson.  We also thank  Roger Astley of Cambridge University  Press for  his support and for his efficient  handling of this  project.  Parts of the book were written while Martin Anthony was visiting the Australian National University, supported by the Royal Society and the Australian  Telecommunications  and  Electronics  Research  Board,  and while Peter Bartlett was visiting the London School of Economics.  Mar- tin Anthony's research has also been supported by the European Union  through  the  'Neurocolt'  and  'Neurocolt  2'  ESPRIT  projects   and  the Engineering  and  Physical  Sciences  Research  Council.  Peter  Bartlett's research  has  been  supported  by  the  Australian  Research  Council  and the  Department  of Industry,  Science  and  Tourism.  We are  grateful  to these funding  bodies and to our respective institutions for providing the opportunities for  us to work on this book.  We thank our families, particularly Colleen and Selena, for their help,  encouragement and tolerance over the years.  Martin Anthony and Peter  Bartlett London and  Canberra March 1999.   1  Introduction  1.1  Supervised  Learning  This  book  is about  the  use of artificial  neural  networks for  supervised learning problems.  Many such problems occur in practical  applications of artificial  neural  networks.  For  example,  a neural  network  might  be used  as a  component  of a face  recognition  system  for  a security  appli- cation.  After  seeing a number  of images of legitimate  users' faces,  the network needs to determine accurately whether a new image corresponds to  the  face  of a legitimate  user  or  an  imposter.  In  other  applications, such  as the  prediction  of future  price of shares on the  stock exchange, we may  require  a neural  network  to  model  the  relationship  between  a pattern  and a real-valued  quantity.  In general, in a supervised learning problem, the learning system must predict  the  labels  of patterns, where the label might be a class label or a  real  number.  During  training, it  receives  some  partial  information about  the  true  relationship  between  patterns  and  their  labels  in  the form  of  a  number  of  correctly  labelled  patterns.  For  example,  in  the face  recognition  application,  the  learning  system  receives a  number of images, each labelled  as either a legitimate user or an imposter.  Learn- ing to accurately  label patterns from  training data in this way has two major  advantages over designing a hard-wired system to solve the same problem:  it can save an enormous amount of design effort,  and it can be used for  problems that  cannot  easily be specified  precisely  in advance, perhaps because the environment is changing.  In designing a learning system for a supervised learning problem, there are three key questions that  must be considered.  The first of these con- cerns  approximation,  or  representational,  properties:  we can  associate with a learning system the class of mappings between patterns and labels   2   Introduction  that it can produce, but is this class sufficiently  powerful to approximate accurately enough the true relationship between the patterns and  their labels?  The second key issue is a statistical  one concerning estimation: since we do not  know the  true relationship  between patterns  and  their labels,  and  instead  receive only  a finite  amount  of data  about  this re- lationship,  how much  data  suffices  to  model the  relationship  with  the desired accuracy?  The third key question is concerned with the compu- tational  efficiency  of learning  algorithms:  how can  we  efficiently make use of the training data to choose an accurate model of the relationship? In  this  book,  we concentrate  mainly  on the  estimation  question,  al- though  we also  investigate  the  issues  of  computation  and,  to  a  lesser extent,  approximation.  Many  of  the  results  are  applicable  to  a  large family  of function  classes, but  we focus on artificial  neural networks.  1.2  Artificial  Neural  Networks  Artificial  neural networks have become popular  over the  last  ten  years for diverse applications from financial prediction to machine vision.  Al- though these networks were originally proposed  as simplified  models of biological neural networks, we are concerned here with their  application to supervised learning problems.  Consequently, we omit the word 'arti- ficial,' and we consider a neural network as nothing more than a certain type of nonlinear function.  In this section we introduce two of the neu- ral network classes that  are discussed later in the book and use them to illustrate the key issues of approximation, estimation, and  computation described above.  The  simple  perceptron  First  we consider the  simple  real-input  perceptron,  which  computes  a function  from  Rn  to  {0,1}.  Networks  such  as  this,  whose  output  is either 0 or 1, are potentially suitable for pattern  classification  problems in which we wish to divide the patterns into two classes, labelled '0' and '1'.  A simple perceptron  computes a function     of the form  f x   =  sgn wx-0 ,  for  input  vector   x  G Mn,  where  w  =   itfi,... ,wn   €  W 1  and  S GM   are adjustable  parameters,   or weights   the  particular  weight   0 being  known   1.2  Artificial neural networks  Fig. 1.1. The decision boundary in R2 computed by a simple perceptron with parameters w, 0.  as the  threshold . Here, w   x  denotes the inner product  Yl7=i wixt>  and  S g n   a   =  1  ifa>0 0  other~wise.  Clearly, the  decision  boundary  of this  function   that  is, the  boundary between the set of points classified  as 0 and those classified  as 1  is the affine  subspace of Rn  defined  by the equation w   x — 6 =  0.  Figure  1.1 shows  an  example  of  such  a  decision  boundary.  Notice  that  the  vec- tor  w determines the orientation  of the boundary, and the ratio  0 H determines its distance from  the origin   where w =   X^=1 wf     .  Suppose we wish to use a simple perceptron for a pattern  classification problem, and that we are given a collection of labelled data   , y  pairs  that  we want  to  use  to  find  good  values  of  the  parameters  w  and  0. The  perceptron  algorithm is a  suitable  method.  This  algorithm  starts with arbitrary values of the parameters, and cycles through the training data, updating the parameters whenever the perceptron misclassifies an example.  If  the  current  function     misclassifies  the  pair   re, y    with   Introduction  Fig. 1.2.  The perceptron algorithm updates the parameters to move the deci- sion boundary towards a misclassified example.  x  €  W 1  and  y  €  {0,1} ,  the  algorithm  adds  rj y   —  f x  x  to  w  and rj f x   — y   to 0, where rj is a   prescribed  fixed positive  constant.  This update  has  the  effect  of  moving  the  decision  boundary  closer  to  the misclassified  point  x   see Figure  1.2 .  As we shall  see in  Chapter  24, after  a finite number of iterations  this algorithm finds values of the parameters that  correctly classify  all of the training examples, provided such parameters  exist.  It  is  instructive  to  consider  the  key  issues  of  approximation,  estima- tion, and computation for the simple perceptron.  Although we shall not study  its  approximation  capabilities  in  this  book,  we  mention  that  the representational capabilities of the simple perceptron are rather limited. This is demonstrated, for instance, by the fact that for binary input vari- ables   x  €  {0, l} n ,  the  class  of  functions  computed  by  the  perceptron forms a tiny fraction of the total number of boolean functions.  Results in the first two parts of this  book  provide answers to the  estimation  ques- tion  for  classes  of  functions  such  as  simple  perceptrons.  It  might  not suffice  simply  to  find  parameter  values  that  give  correct  classifications   1.2  Artificial neural networks   5  for all of the training examples, since we would also like the  perceptron to perform  well on subsequent   as yet  unseen   data.  We are led to the problem of generalization,  in which we ask how the performance  on the training  data  relates  to  subsequent  performance.  In  the  next  chapter, we describe  some assumptions  about  the  process  generating  the  train- ing  data   and  subsequent  patterns ,  which  will  allow  us  to  pose  such questions  more  precisely.  In  the  last  part  of  the  book,  we study  the computation  question for  a number  of neural  network function  classes. Whenever it is possible, the perceptron algorithm is guaranteed to find, in a finite number of iterations, parameters that  correctly classify  all of the training examples.  However, it is desirable that  the number of iter- ations required  does not  grow too rapidly  as a function  of the  problem complexity  measured by the input dimension and the training set size . Additionally,  if  there  are  no  parameter  values  that  classify  all  of  the training set correctly, we should like a learning algorithm to find a func- tion that  minimizes the number of mistakes made on the training  data. In general the perceptron algorithm will not converge to such a function. Indeed, as we shall see, it is known that no algorithm can efficiently  solve this problem   given standard  complexity theoretic assumptions .  The two-layer  real-output sigmoid network  As a second example, we now consider the  two-layer real-output sigmoid network. This network computes a function     from  Rn  to R of the form  '  x  where x  G  W1 is the input  vector, w%  €  E   i =  0,..., k   are the  output weights,  V{ €  En  and  v^o   i  =  0,..., k   are  the  input  weights,  and a  : R  ->  R,  the  activation function,  is the  standard sigmoid function, given by  a{a   =  —   .    1.1   This function  is illustrated  in Figure 1.3.  Each of the  functions  x  H-> a  vi   x  +  can be thought  of as a smoothed  version of the  function  computed  by a  simple  perceptron.  Thus,  the  two-layer  sigmoid  network  computes an affine  combination of these  'squashed'  affine  functions.  It  should be   Introduction  Fig. 1.3.  The graph of the function  a -  defined in Equation  1.1 .  a  noted that the output of this network is a real number, and is not simply either 0 or 1 as for the simple perceptron.  To use a network of this kind for  a  supervised  learning  problem,  a  learning  algorithm  would  receive a  set  of labelled  examples   {x,y   pairs,  with  x  €  W 1 and  y  G E   and attempt  to find  parameters that  minimize some measure of the error of the  network  output  over  the  training  data.  One  popular  technique  is to start  with small initial values for the parameters and use a  gradient descent' procedure to adjust  the parameters in such a way as to locally minimize  the  sum  over  the  training  examples   i,2 i   of  the  squared errors   f{xi   -  yi 2.  In general, however, this approach  leads only to  a local minimum of the squared  error.  We  can  consider  the  key  issues  of  approximation,  estimation,  and computation  for  this  network  also.  The  approximation  question  has  a more positive  answer  in  this  case.  It  is known  that  two-layer  sigmoid networks  axe  'universal  approximators',  in  the  sense  that,  given  any continuous  function     defined  on  some  compact  subset  5  of  En,  and any desired accuracy e, there is a two-layer sigmoid network computing a  function  that  is  within  e of     at  each  point  of  5.  Of  course,  even though such a network exists, a limited  amount  of training  data might not  provide  enough  information  to  specify  it  accurately.  How  much   1.3  Outline of the book   7  data  will suffice  depends  on the  complexity  of the function      or more precisely  on the  complexity—number  of computation  units  and  size of parameters—of  a network that  accurately  approximates     .  Results in Part  3 address these questions, and Part  4 considers the  computational complexity of finding a suitable network.  General neural networks  Quite  generally,  a  neural  network  N  may  be  regarded  as  a  machine capable  of  taking  on  a  number  of  * states',  each  of  which  represents  a function  computable  by  the  machine.  These  functions  map  from  an input space X   the set of all possible patterns  to an output space Y.  For neural networks, inputs are typically encoded as vectors of real numbers  so I C En  for some n , and these real numbers often  lie in a bounded range.  In Part  1, we consider  binary  output  networks for  classification problems, so, there, we have Y  =  {0,1}.  In  Parts  2 and  3 we consider networks with real outputs.  Formalizing mathematically, we may regard a neural network as being characterized by a set fi of states, a set X  of inputs, a set Y  of outputs, and  a  parameterized  function  F  : ft  x X  -*  Y.  For  any  u>  £  fi,  the function  represented by state u  is h^  : X  -> Y  given by  The  function  F  describes  the  functionality  of  the  network:  when  the network is in state u  it computes the function  h^.  The set of  functions computable  by N  is  {h^  :  u  £  fi},  and  this  is  denoted  by  HN.  AS a  concrete  example  of  this,  consider  the  simple  perceptron.  Here,  a typical state is u  =   w\, 1U2,      , wn, 0 , and the function  it represents is  ,  xx, x2,...,  xn    1.3  Outline of the Book  The first three parts of the book define  three supervised  learning prob- lems  and  study  how the  accuracy  of  a  model  depends  on  the  amount   8   Introduction  of training data and the model complexity.  Results are generally of the form  error <   estimate of error  +   complexity penalty ,  where the complexity penalty increases with some measure of the com- plexity of the class of models used by the learning system, and decreases as the  amount  of data  increases.  How  'complexity'  is defined  here de- pends both on the definition  of error and on how the error is estimated. The  three  different  learning  problems  are  distinguished  by  the  types of labels  that  must  be  predicted  and  by  how the  network  outputs  are interpreted.  In  Part  1,  we study  the  binary classification  problem, in  which we want to predict  a binary-valued  quantity  using a class of binary-valued functions.  The correct  measure of complexity in this  context  is a com- binatorial  quantity  known as the  Vapnik-Chervonenkis  dimension.  Es- timates  of this  dimension  for  simple perceptrons  and  networks of per- ceptrons have been known for  some time.  Part  1 reviews these results, and presents some more recent results, including estimates for the more commonly used sigmoid networks. In all cases, the complexity of a neu- ral network is closely related  to its size, as measured by the number of parameters in the network.  In Part  2, we study  the  real classification problem,  in which we again  want  to  predict  a binary-valued  quantity,  but  by  using  a  class of real- valued  functions.  Learning  algorithms  that  can  be  used  for  classes of real-valued  functions  are  quite  different  from  those  used  for  binary- valued  classes, and  this  leads to  some anomalies between  experimental experience and the VC theory described in Part  1. Part 2 presents some recent  advances  in  the  area of  large  margin classifiers,  which  are clas- sifiers  based  on  real-valued  functions  whose output  is interpreted  as  a measure  of the  confidence  in  a  classification.  In  this  case, the  correct measure of complexity  is a scale-sensitive version of the  VC-dimension known as the fat-shattering dimension.  We shall see that  this  analysis can  lead  to  more  precise  estimates  of the  misclassification  probability  that  is, better  answers to  the  estimation  question ,  and  that  the  size of a neural  network  is not  always the  most  appropriate  measure of its complexity, particularly if the parameters are constrained to be small.  In Part  3, we study the  real prediction problem. Here, the problem is to predict a real-valued quantity   using a class of real-valued  functions . Once again, the fat-shattering dimension emerges as the correct measure of complexity.  This part  also features  some recent results on the use of   1.4  Bibliographical  notes   9  convex function  classes for real prediction problems.  For instance, these results suggest that  for a simple function  class, using the convex hull of the class  that  is, forming  a two-layer neural network of functions  from the  class,  with  a  constraint  on  the  output  weights   has  considerable benefits and little cost, in terms of the rate at which the error decreases. Part  4 concerns the  algorithmics  of supervised  learning,  considering the computational limitations on learning with neural networks and in- vestigating the performance  of particular  learning algorithms   the  per- ceptron  algorithm  and  two  constructive  algorithms  for  two-layer  net- works .  1.4  Bibliographical  Notes  There axe many good introductory books on the topic of artificial  neural networks;  see,  for  example,   Hertz,  Krogh  and  Palmer,  1991; Haykin, 1994; Bishop, 1995; Ripley, 1996; Anderson and Rosenfeld,  1988 . There are also a number of books on the estimation  questions associated with general learning systems, and many of these include a chapter on neural networks.  See, for  example,  the  books  by  Anthony  and  Biggs   1992 , Kearns and Vazirani  1995 , Natarajan   1991a , Vidyasagar  1997 , and Vapnik   1982; 1995 .  The notion of segmenting the analysis of learning systems into the key questions  of approximation,  estimation  and  computation  is popular  in learning theory research   see, for instance,  Barron, 1994  .  The  simple  perceptron  and  perceptron  learning  algorithm  were first discussed by Rosenblatt   1958 . The notion of adjusting the strengths of connections in biological neurons on the basis of correlations between in- puts and outputs was earlier articulated  by Hebb  1949  who, in trying to  explain  how  a  network  of  living  brain  cells  could  adapt  to  differ- ent stimuli, suggested that  connections that  were used frequently  would gradually  become stronger,  while those that  were not  used  would  fade away.  A  classic  work  concerning  the  power   and  limitations   of  sim- ple perceptrons is the book by Minsky and  Papert   1969 .  Around  the time of the publication of this book, interest in artificial neural networks waned, but  was restored in the early 1980's, as computational resources became more abundant   and with the popularization of the observation that gradient computations in a multi-layer sigmoid network could share intermediate  calculations .  See, for  example,   Rumelhart,  Hinton  and Williams,  1986a; Rumelhart,  Hinton  and  Williams,  1986b .  Since  this   10   Introduction  time,  there  have been  many  international  conferences  concentrating  on neural networks research.  The  'universal  approximation'  property  of  neural  networks  has  been proved under many different  conditions and in many different  ways; see  Cybenko,  1989;  Hornik,  Stinchcombe  and  White,  1990;  Leshno,  Lin, Pinkus  and Schocken,  1993; Mhaskar,  1993 .   Part  one  Pattern  Classification  with  Binary-Output  Neural Networks    2  The Pattern Classification  Problem  2.1  The Learning Problem  Introduction  In this  section  we describe  the  basic  model  of  learning  we use  in  this part  of the book.  This model is applicable to neural networks with one output unit that computes either the value 0 or 1; that is, it concerns the types of neural  network  used  for  binary classification  problems.  Later in  the  book  we develop more general  models  of learning  applicable  to many  other  types  of neural  network,  such  as  those  with  a  real-valued output.  The definition  of learning we use is formally  described  using the lan- guage of probability theory.  For the moment, however, we move towards the definition  in a fairly non-technical manner, providing some informal motivation for the technical definitions  that  will follow.  In  very  general  terms,  in  a  supervised  learning  environment,  neural network 'learning' is the adjustment  of the network's state in response to data generated by the environment.  We assume this data is generated by some random  mechanism,  which is, for  many  applications,  reasonable. The method by which the state of the network is adjusted  in response to the data constitutes a learning  algorithm. That  is, a learning algorithm describes  how  to  change  the  state  in  response  to  training  data.  We assume  that  the  'learner'f  knows  little  about  the  process  generating the  data.  This  is  a  reasonable  assumption  for  many  applications  of neural networks: if it is known that  the data is generated according to a particular type of statistical process, then in practice it might be better to take advantage of this information  by using a more restricted class of functions  rather than  a neural network.  t  The learner' in this context  is simply the learning algorithm.  13   14   The Pattern  Classification Problem  Towards  a formal   framework  In  our  learning  framework,  the  learner  receives  a  sequence  of  training data,  consisting of ordered pairs of the form   x,y , where x  is an input to  the  neural  network   x  £  X   and  y  is  an  output   y  €  Y .  We call such  pairs  labelled  examples.  In  this  part  of the  book,  and  in  Part  2, we consider  classification  problems,  in  which  Y  =  {0,1}.  It  is  helpful to think of the label y as the 'correct output' of the network on input  x  although this interpretation is not entirely valid, as we shall see below . We assume that  each such  pair  is chosen, independently  of the  others, according  to  a fixed probability  distribution  on  the  set  Z  =  X  x  Y. This probability  distribution  reflects  the  relative frequency  of  different patterns  in  the  environment  of  the  learner,  and  the  probability  that the  patterns  will be  labelled  in a particular  way.  Note that  we do not necessarily  regard  there  to  be  some  Correct'  classification  function  t : X  -»  {0,1}:  for a given x  € X,  both  , 0  and  x, 1  hiay have a positive probability  of being presented  to  the  learner,  so neither  0 nor  1 is the 'correct' label.  Even when there is some correct classification function     : X  ->  {0,1}  that is,    is such that the probability of the set { x,   &   : x  €  X}  is one , we do not  assume that  the  neural  network  is  capable of computing  the  function   .  This  is a very general  model  of  training data  generation  and  it  can  model, among other  things,  a  classification problem in which some inputs are ambiguous, or in which there is some 'noise' corrupting the patterns or labels.  The aim of successful  learning is that,  after  training on a large enough sequence of labelled examples, the neural network computes a function  that  matches, almost as closely as  it  can,  the  process  generating  the  data;  that  is,  we hope  that  the classification  of  subsequent  examples  is  close  to  the  best  performance that  the network can possibly manage.  It  is  clear  that  we have  to  make  the  above  notions  mathematically precise.  We  first  discuss  the  formal  expression  of  the  statement  that the training data is randomly generated.  We assume that  there is some probability  distribution  P  defined  on  Z.  The  probability  distribution P  is fixed for  a given learning  problem,  but  it  is  unknown.  The  infor- mation presented to the neural network during training consists only of a  sequence  of  labelled  examples, each  of the  form   x,y .  Formally,  for some positive integer m, the network is given during training a training sample  The labelled examples Z{  —  xi,yi   are drawn independently,  according   2.1  The learning problem   15  to  the  probability  distribution  P.  In  other  words,  a random  training sample  of length  m is an  element  of Zm  distributed  according  to the product  probability  distribution  Pm.  We  now  turn  our  attention  to measuring  how  well  a given  function computed  by the  network  'approximates'  the  process  generating  the data.  Let us denote the set of all functions  the network can compute by H  rather  than  HN   to  keep  the  notation  simple,  but  also  because  the model of learning to be defined can apply to learning systems other than neural networks .  Given a function  h € H,  the  error  ofh  with  respect to P   called  simply  the  error  of h when P is clear  is  defined  as  followsrf  erP h =P{ x,y eZ:h x ^y}.  This is  the  probability,  for   x, y   drawn randomly  according  to P, that h is  'wrong' in the  sense  that  h x   ^ y.  The  error of  h is a measure of how  accurately  h approximates  the  relationship  between  patterns  and labels  generated  by P.  A related  quantity  is the  sample  error  of h on the sample z   sometimes  called the  observed  error ,  defined  to  be  erz h  = — \{i:  1 < i < m and h xi     yi}\,  the  proportion  of labelled  examples   xi,j f   in the  training  sample z on which h is 'wrong'.  The  sample  error is a useful  quantity,  since it can easily  be  determined  from  the  training  data  and  it provides  a simple estimate  of the true error erp h .  It is to T>e hoped  that,  after  training,  the  error of  the  function  com- puted  by the  network is close to the  minimum value it can be.  In other words, if h is the function  computed  by the network after  training   that is,  h is the  hypothesis  returned  by the  learning  algorithm ,  then we should like to  have erp ft   close to the  quantity  optp iJ   =  inf  erp g .  This quantity  can be thought  of as the  approximation  error  of the  class H,  since  it describes  how  accurately  the  best  function  in H  can  ap- proximate  the  relationship  between x and y that  is determined  by  the probability  distribution P.   Note  that  we take  an infimum  rather  than simply  a minimum  here  because  the  set  of  values  that  erp  ranges  over  t  The  functions  in H have to be  measurable,  and  they  also  have to satisfy  some additional,  fairly weak, measurability  conditions for the subsequent  quantities to be well-defined.  These conditions are satisfied  by all function  classes discussed in this book.   16   The Pattern  Classification Problem  may be infinite.   More precisely, a positive real number  e is prescribed in advance, and the aim is to produce h £ H  such that  evP h   < optp    + e.  We say that  such an h is e-good   for P .  The number e  which we may take  to  belong  to  the  interval   0,1   of  positive  numbers  less  than  1 , is  known  as  the  accuracy  parameter.  Given  the  probabilistic  manner in  which  the  training  sample  is  generated,  it  is  possible  that  a  large 'unrepresentative' training sample will be presented  that  will mislead  a learning algorithm.  It cannot, therefore, be guaranteed that the hypoth- esis will always be e-good.  Nevertheless, we can at least hope to ensure that  it  will be e-good  with high probability—specifically,  with probabil- ity at  least  1 — 5, where 5, again prescribed  in advance, is a  confidence parameter.   Again, we may assume that  S €   0,1 .   Formal  definition  of  learning  We are now in a position to say what we mean by a learning  algorithm. Informally,  a learning  algorithm  takes  random  training  samples  and acts on these to produce a hypothesis h £ H  that,  provided the sample is large enough, is, with  probability  at  least  1 —  5, e-good for  P.  Fur- thermore, it  can do this for each choice of e and 6 and regardless of the distribution  P.  We have the following formal  definition.  Definition  2.1  Suppose  that H  is a class of functions  that map from a set X  to {0,1}.  A  learning algorithm  L for H  is a function  oo  L :  J Zm -> H  m=l  from  the set of all training samples to H,  with the following property:     given any e G   0,1 ,    given any S G   0,1 ,  there is an integer mo e, 8  such that ifm>  mo e,5   then,     for  any probability  distribution P o nZ  =   I x { 0 , l },  if z  is a training sample of length m,  drawn  randomly according  to the product probability  distribution Pm,  then, with probability  at least 1 — 5, the hypothesis L z   output by L  is such that  evP L z    < optP i7  + e.   2.1  The  learning  problem   17  More compactly, for m  >  mo e,5 ,  Pm  {eTP L z       1 -  5.  We say that H  is learnable  if there is a learning algorithm for  H.  Equivalently, a function  L is a learning algorithm if there is a function eo m,6   such that,  for  all m,  5, and  P,  with  probability  at  least  1 — 5 over z  G  Zm  chosen according to  P m ,  erP L z    < optp iJ   + eo m, S ,  and  for  all  5 G  0,1 ,  eo ra, 5   approaches  zero as m  tends  to  infinity. We refer  to eo m,S   as an  estimation error bound  for  the algorithm  L. In  analysing learning algorithms, we often  present  results either  in  the form  of  sample  complexity  bounds   by  providing  a  suitable  mo e,5   or estimation  error  bounds.  It  is usually  straightforward  to  transform between the two.  For a neural network  N,  we sometimes refer  to  a learning  algorithm  for  JHJV more simply as a learning algorithm for iV.  There  are  some  aspects  of  Definition  2.1  that  are  worth  stressing. Note that  the learning algorithm  L must  'succeed' for  all choices of the accuracy  and  confidence  parameters  e and  5.  Naturally,  the  quantity mo e,<$ ,  known  as  a  sufficient  sample  size  for   e, 6  -learning  H  by  L, is allowed  to  vary  with  c and  5.  This  is to  be  expected  since decreas- ing the value of either e or S makes the learning problem more  difficult  and hence we should be prepared  to use a larger sample .  Note, how- ever, that rao e,<S  depends in no way on P\  that  is, a sufficient  sample size can  be  given  that  will work for  any fixed distribution  P.  This  is desirable because P  is unknown; it is not given as an input to the learn- ing  problem  in  the  way  that  e and  6 are.  We could  have  defined  the learning  problem  so  that  the  sample  size  is  allowed  to  vary  with  the distribution.  However, if the sample complexity depends on the proba- bility  distribution,  we would need  to have some information  about  the distribution  in  order  to  get  sample  size bounds.  Many  simple  pattern classification  techniques   such  as certain  nearest  neighbour  algorithms and kernel methods   are known to give predictions that  are asymptoti- cally optimal, in the sense that for any probability distribution, the error approaches that of the best deterministic classifier   the 'Bayes optimal' , as the amount of training data increases.  On the other hand, the rate at which the error converges can be made arbitrarily slow by suitable choice of the probability  distribution.  This is one of the main motivations  for   18   The Pattern  Classification  Problem  considering classes like neural networks. We shall see that, because these classes are not too complex, we can prove estimation error convergence rates that  apply to every probability  distribution.  Learnability  may  appear  difficult  to  achieve,  particularly  in  view of the 'distribution-independence'   that is, the need for a sufficient  sample size that  is independent of P .  However, we show in this chapter that if the class H  consists of a finite number of functions  then H  is learnable, and in subsequent  chapters we establish the learnability of many other classes.  It  is  of  central  importance  to  determine  whether  a  given  set  H  of functions  is learnable  and,  if so, to  design  a learning  algorithm  for  H. One measure  of the  efficiency  of  a learning  algorithm  is the  minimum sample size mo c, J  sufficient  for learning to the levels of accuracy and confidence  prescribed  by  e  and  S.  We  define  the  sample  complexity function  mi e, 5   of L  as the  smallest  integer  that  can  be taken  to  be mo c, J  in Definition  2.1; that is,  mL e, S   =  min{m : m is a sufficient  sample size  for   e,  J -learning H  by L}.  Similarly, we define the estimation error e£, m, 6  of L to be the smallest possible estimation  error bound.  It  is also useful  to define  the inherent sample  complexity  mjj e, S   of  the  learning  problem  for  H:  mH e,S   =  minmL e, J ,  Jb  where the  minimum  is taken  over  all  learning  algorithms  for  H.  The inherent sample complexity ra e, $  provides an absolute lower bound on the size of sample needed to   e,  J -learn if,  no matter what  learning algorithm is being used.  The inherent estimation error € j m, 8  may be defined  similarly.  Notice  that  these  definitions  allow  us  to  separate  the  problems  of approximation  and  estimation  introduced  in  Chapter  1.  The  approxi- mation error is measured by optP if ,  and a quantitative answer to the estimation  question is provided by €i  m, J    and mi  e, S  .  Although  we have  used  the  term  'algorithm',  we have  not  been  at all specific  about  the  computational  or algorithmic  aspects of learning. Later  in the  book,  we investigate  the  important  issue of the computa- tional complexity of learning.  For the moment, we simply regard a learn- ing algorithm  as a function  and  concentrate on quantifying  the sample complexity of learning.   2.2  Learning finite function  classes   19  2.2  Learning  Finite  Function  Classes  In this section we show that  there are particularly simple learning algo- rithms for finite classes of functions.  These  algorithms  use the  sample error as an estimate for the true error and choose as output  hypothesis a function  in the class with minimal sample error.  This works since the sample  errors  converge in  a  certain  manner  to  the  true  errors,  as  the sample length is increased.  For many neural networks AT, the  class H^  of functions  computable by the network is finite. For example, any neural network with a binary output  defined  on a finite input set   such as a neural network accepting only  binary-valued  inputs   will  compute  a finite number  of  functions. Furthermore, neural networks defined on real inputs but having weights that  can take only a finite number of values also have finite  A  'uniform  convergence9  result,  and  learning  using  sample  error  The  aim  of  learning  is  to  produce  a  function  h  in  H  that  has  near- minimal error erp ft .  Given that  the true errors of the functions  in H are  unknown,  it  seems  natural  to  use  the  sample  errors  as  estimates. We might  believe that  if a function  has small sample error then  it  has small true error.  In this vein, suppose that  an algorithm L chooses  L z  having minimal sample error on z\ that is,  z       ez h   In  the  remainder  of this  section  we show that  such  an  L  is a  learning algorithm whenever H  is finite.  The following  result  will be useful.  It  shows that,  given any  h  €  H  and given a large enough random sample, the sample error of h is close to the true error of h.  This is unsurprising, as it is just  a  lLaw of Large Numbers' result from probability theory, telling us how rapidly the tails of a  binomial  distribution  approach  zero.   If  we toss  a  coin  a  number of times, this theorem describes the rate at which the relative  frequency of  heads  approaches  the  probability  of  a  head.  In  this  analogy,  the probability  of a  head  is erp  i ,  and  the  relative frequency of heads is  Theorem 2.2 Suppose that h is a function from a set X  to {0,1}.  Then  Pm  {\erz h  -  eiP h \  >e}<  2exp -2e2m ,   20  The Pattern  Classification  Problem  L z   h*  H  Fig. 2.1.  If every h in function class H has eip h   close to erz h ,  minimizing erz h   will approximately minimize evp h .  for  any probability  distribution  P,  any e,  and  any positive  integer  m.  Proof  The theorem is a restatement  of Hoeffding's  inequality   Inequal- ity   1.16   in  Appendix  1 .  To  see  this,  let  Xi,X2,..-,Xm  be  a  se- quence  of TO {0, l}-vaJued  random  variables,  where  X%  is  1  on  Z{  =  xuVi   €  Z  if  and  only  if  h xi   ^  j j.  Then  the  sample  error of  h on  z is   1 ra   Xi  +  X2  H  P{h x   ^  y},  which is erP  i .   h -X"m , and  the  expectation  of each  X{  equals D  We  wish  to  show  that  any  algorithm  L  that  minimizes  sample  error on  z  is  a learning  algorithm  when  H  is  finite.  Theorem  2.2  shows  that for any particular  h G  if,  the  sample error and error of h are close with high probability.  But  this is not  quite sufficient  to ensure that  L learns. Rather,  since  L  examines  the  sample  error of  every  h €  H  and  chooses the one that  gives minimal sample error, we would like the sample error of every  h €  H  to be close to its true error.  Figure 2.1 illustrates why, if this  is  so,  then  minimizing  erz ti   will  approximately  minimize  erp  i . The following  result  is of the  required type.   2.2  Learning finite function  classes   21  Theorem  2.3  Suppose  that H  is a finite set of functions from  a set X to {0,1}.  Then  Pm   maxer2  i   -  erP ft   > e\  <  2\Hexp -2e2m ,  for  any probability  distribution P,  any e, and any positive integer m.  Proof  We use Theorem 2.2, as follows.  =  Pm \J{zeZm:   \evz h  -  erP ft  > e}    \heH      heH  2exp -2e2m  ,  <   as  required.   The  first  inequality—that  the  probability  of  a  union  of events is no more than  the sum of their  probabilities—is  known as the union bound.      Theorem  2.3 is an example of a  uniform convergence result;  it shows that  the sample errors converge  in probability   to the true errors, uni- formly  over H,  asm  tends to infinity.  We apply this theorem to obtain our first learnability  result.  Theorem  2.4  Suppose that H  is  a  finite  set  of functions  from  a set X  to  {0,1}.  Let L  : Um=i Zm  -> H  be such that for  any m  and any zezm,  Then L  is a learning  algorithm for H,  with estimation  error  and sample  complexity  Proof We must show that erp L ^   is not much bigger than optP H   =   22   The Pattern  Classification Problem  infheH erp  i .  Since H  is finite, this infimum  is attained; that  is, there is h* e  H  such that  erp  i*  = optP  .  Theorem 2.3 states that  Pm   max\Az h   -  erP{h \ > e\  < 2\H\ exp  -2e2m ,  {heH      and this is no more than  S if  In this case, with probability  at  least  1-5,  for  every  heH,  erP h   -  €   < erz h   < erP h   + e,  and so,  evP L z    <  evz L z    + e =  minerz fc   + e <  etz h* +e  =  optp ff   + 2e.  Hence, with probability  at  least  1 -  5, L  returns a function  h with  erp ft   <optP  +     —In  1 2  V s  Solving for m gives the sample complexity bound.      2.3  Applications to  Perceptrons  We have already mentioned  that  for  many neural networks N  the class HN  of computable functions is finite. We now give two specific examples, both  of which  are types of perceptron. The perceptron  was introduced in  Section  1.2.  Recall  that,  for  an  input  x  €  R n,  the  parameterized function  describing the mapping computed  by a perceptron is  ,a;2,...   ,xn    V=i  In  order  to  ensure  that  the  function  class  is  finite,  we  now  consider perceptrons in which there are restrictions on the allowable values of the   2.4  Restricted model   23  weights and threshold.  First, we consider what we call the binary-weight perceptron, where each non-threshold weight can be only 0 or 1, and the threshold 6 can only be an integer in the range 0 to n  where n, as usual, is the number of inputs .  Since there are 2 choices for each of the n non- threshold weights and n + 1 choices for the threshold, the total number of states  is   n +  l 2n.  Hence, for  this  network  N,  \HN\  <  n  +  l 2 n, using  the  fact  that  the  number  of  functions  computable  by  a  neural network is bounded by the number of states.  Secondly, we consider the k-bit perceptron, in which the states are those for which the weights and threshold  are  expressible  in  binary  as  integers  of  length  k   where  one of the  bits  encodes  the  sign of the  weight  or threshold .  There  are 2k possibilities for  each weight  and  threshold  and so, for  this network JV7, \HN>\  <  2k n+l  =2* n+1 .  We may apply Theorem 2.4 directly.   We state only sample complex- ity  bounds  in  the  following  result,  but  estimation  error  bounds  follow immmediately.   Theorem  2.5  Let N  be the binary-weight perceptron  on n  inputs, and let Z  =  En  x  {0,1}.  Suppose  that L  : \J£=i  zm  "> HN  ™  such that for any m  and any z €  Z m,  evz L z    =  min  evz h .  Then L  is a learning algorithm for HN  and its sample complexity satis- fies the  inequality  ,<$  = ^   nln2  + ln n + 1  + In f   J J ,  for  all e, $G   0,1 .  The same statement  holds for  the k-bit perceptron N1,  with mo e,S   replaced  by  2.4  Restricted  Model  Definitions  As we emphasized, in the model of learning we have considered, it is not assumed  that  there  is a  target  function'  computable  by  the  network. But  if this  is the  case, one can obtain  learnability  results  with  smaller sufficient  sample sizes.   24   The Pattern  Classification  Problem  To describe this  restricted model of learning, let H be a set of {0,1}-  valued functions  on a set X  and, rather than having an arbitrary distri- bution  P on Z  = X  x {0,1}, let us imagine that  we have some target function  t, belonging to if,  and a probability  distribution \i on the set X  of inputs.  We define the error of h G if  with respect to £ and  x to be  In this context, the training samples  rather than being arbitrary mem- bers of Z  are of the form    xi,* zi  ,  x 2,t x 2  ,...,   z m,* zm   ,  which we call the  training sample corresponding  to x  and t.  A learning algorithm maps from such samples to H and satisfies the following: given any  e,   mo e,S ,  for  any t  G H and  any probability distribution   zonX,  with  ^-probability at least  1 — 5, a random x G X m is such that if  Z  =  is the corresponding  training  sample,  then  er^Z^z  ,  the error  of the output  hypothesis, is less than e.   In this case, since t G H, we can aim for arbitrarily small error, since the optimal error is 0.   The  restricted  model  may  be regarded  as a straightforward  special case of the model of learning described in this  chapter.  For, given any t  G H and a distribution   x on X, there is a corresponding  distribution P  on Z, in the sense that for any measurable subset A of X,  P{ x,t x  :xeA}  =   i A ,  P{{x,y :x£A,  y?t x }  =  0,  and, furthermore, erp h  = erM h,^ . Thus the restricted learning prob- lem corresponds to considering only a subset of all possible distributions ? on   Z.  Consistent  learning algorithms  Given  any  training  sample z  corresponding to a function  t  G if,  there is always at least  one function  in H that  matches t on the sample, in the sense that  its values on the Xi agree with the labels t xi .   Indeed, the  target  function  t  itself  obviously  has this  property, and there  may be other  such functions.   It turns out that  any L returning a  function   2.5  Remarks   25  ft £ H  that  matches the sample is a learning algorithm for a finite  class H.   Such a learning algorithm is said to be  consistent   Theorem 2.6 Suppose that H  is a finite set of functions from a set X  to {0,1}.  Let L  be such that for  any m  and for  any t € H,  if x  € Xm  and z  is  the training sample corresponding  to x  and t,  then  the  hypothesis ft =  L z   satisfies h xi   =  t xi   for i =  1,2,...,  ra.  Then L  is a learning algorithm for H  in the restricted model,  with sample  complexity  f\H\\ rriL{e,S   < mo e,S  =  -In  —r1  \  S     1  e   .  Proof  Let t € H  and suppose ft E H  is such that  Then the probability   with respect to the product  distribution  xm   that ft agrees with t on a random sample of length m  is clearly at  most   1  - c m.  This is at most exp —em , using a standard approximation.  Thus, since there  are certainly  at  most  \H\  such functions  ft,  the  probability that some function in H has error at least e and matches t on a randomly chosen training  sample of length  m  is at  most  ffexp -em .  For any fixed positive 5, this probability is less than  6 provided  ra :  as required.   The main difference  between this sample complexity bound  and  that given in Theorem 2.4 is the presence of 1 e rather than the larger  1 e2. We shall see this difference  arise in a number of contexts.  The intuitive explanation  of this improvement  is that  less data  is needed  to form  an accurate estimate of a random quantity if its variance is smaller.     2.5  Remarks  Learning with respect to  a touchstone class  It  is often  useful  to weaken the requirement  of a learning algorithm by asking only that   with high probability   eiP L z    < optp T  + e =  inf erP *  + c   26   The Pattern  Classification Problem  where  T,  called  the  touchstone class, is  a  subset  of  H.  We  call  this learning with  respect to  a  touchstone class.  What  we require  in  this modified  framework  is that  L  outputs  a  function  whose  error  is  close to  the  smallest  error  achievable by a  function  in  the  class T   and  not necessarily to  the smallest  error of a function  in the full  class H .  For example,  in  learning  with  a  multi-layer  neural  network,  we might  be satisfied if, after training, the network computes a function that is almost as good as any function  a simple perceptron could compute.  The model of  learning  we  have  considered  may  be  viewed  as  the  case  in  which T  =  H.  Sometimes it is computationally easier to learn with respect  to a touchstone class when T  is a strict  subset of  H.  Bounding  the  expected  value  of  error  In our definition  of learning, we demand that if m  > rao e, 5 , then with probability  at  least  1 —  <5, erp L z    <  optp iif   +  c.  An  alternative approach  would  be  to  require  a  bound  on  the  expected  value  of  the random  variable  eip L z  .  Explicitly,  we could  ask  that,  given  a  €  0,1 , there is  ra^a   such that  E  erP L z     < opt P    +  a  for m  > rao a , where E  denotes the expectation over Zm  with respect to  Pm.  This  model  and  the  one of  this  chapter  are  easily  seen  to  be related.  By Markov's inequality   see Appendix 1 ,  E erP L z   <optP H     E  erP L z   -  optP ff    < tS =*  P m  {evP L z   -  optp H  >e}<j  = 6.  Therefore,  learnability  in  the  alternative  model  implies  learnability  in our model, and we may take mo e,<5  =  m'0 e5 . Conversely, suppose  Then, since erp L z    — optP H   < 1 for all z,  we have  E  eiP L z   -  optP H    <    p m  {eiP L z    -  optP H   <  a.   2.6  Bibliographical  notes   27  It follows that learnability in the sense of this chapter implies learnability in the alternative model, and we may take m^a   = mo  a 2, a 2 .  2.6  Bibliographical  Notes  For an introduction to measure theory and probability, see, for example,  Billingsley,  1986; Feller, 1971 .  The problem of supervised learning for pattern classification  is an old one, and there are many books discussing it.  See, for instance, the books of Duda and Hart   1973 , and Devroye, Gyorfi  and Lugosi  1996 .  The probablistic  models of learning discussed  in this  chapter  derive from  a model due to Valiant  1984b .   Valiant, however, emphasizes the compu- tational complexity of learning, something we do not address until later in  this  book.   In  fact,  the  'Probably  Approximately  Correct'   PAC  model as originally developed is precisely our restricted model of learn- ing;  our  main  model  of learning  is a  form  of  'agnostic  PAC  learning', studied  by Haussler   1992 , Kearns, Schapire and  Sellie   1994 , Maass  1995 , and others.  The existence  of universal  asymptotically  optimal  pattern  classifica- tion  schemes was first shown by Stone   1977 .  Devroye   1982  showed that  the rate at  which the error of such schemes converges to the Bayes optimal can be arbitrarily slow.  The fact  that finite hypothesis  spaces are learnable  in the  restricted model  was shown  by  Valiant   1984b .  Blumer,  Ehrenfeucht,  Haussler and Warmuth   1989  highlighted the important  role of consistent learn- ing algorithms in learning, building on work of Vapnik and Chervonenkis  1971 .  The  notion  of  a  touchstone  class  has  been  studied  by  Kearns  et  al.  1994 .  Later in the book, we shall see an example of a neural network class that  can be learnt efficiently  using a larger class of  functions.  The  learning  model  that  focuses  on  the  expected  value of error  has been investigated by Haussler, Littlestone and Warmuth   1994 ; see also  Devroye and  Lugosi, 1995 .  The relationship between  this model,  the restricted model, and many other learning models is studied in  Haussler, Kearns, Littlestone and Warmuth, 1991 .  There  has  been  a  great  deal  of  work  on  learning  models  that  re- lax  Valiant's  PAC  model   the  restricted  model .  For  instance,  these models  allow  noisy  labels   Angluin  and  Laird,  1988;  Kearns  and  Li, 1993; Sloan, 1995; Cesa-Bianchi, Fischer, Shamir and  Simon, 1997 , re- lax the  assumptions  of the  independence  of training  examples   Aldous   28   The Pattern  Classification  Problem  and  Vazirani,  1990; Bartlett,  Fischer  and  Hoffgen,  1994 , relax the as- sumption  that  the  distribution  generating  training  examples  is fixed  Bartlett,  1992;  Bartlett,  Ben-David  and  Kulkarni,  1996;  Barve  and Long, 1997; Freund and Mansour,  1997 , relax the assumption that  the relationship between the examples and their labels is fixed  Kuh, Petsche and Rivest, 1991; Blum and Chalasani, 1992; Helmbold and Long, 1994 , relax the requirement that  the sample size bounds apply to all distribu- tions   Benedek and Itai,  1991  and functions   Benedek  and Itai, 1988 , and restrict  consideration to some restricted  set of probability  distribu- tions  Li and Vitanyi, 1991; Li and Vitanyi, 1993 . In other models, the training examples are not randomly chosen; they may be chosen by the learner, and labelled by an oracle  Angluin, 1988; Angluin, 1992 , or they may be chosen by a helpful  teacher  Shinohara and Miyano, 1991; Gold- man and Kearns, 1991; Jackson and Tomkins, 1992; Anthony, Brightwell and Shawe-Taylor, 1995 .   The  Growth  Function  and  VC-dimension  3.1  Introduction  The previous chapter  gave a formal  definition  of the learning problem, and  showed that  it  can be solved if the  class HN  of functions  is finite. However, many interesting function  classes are not finite. For example, the  number  of functions  computed  by  the  percofctron  with  real-valued weights and inputs  is infinite.  Many other  neural networks can also be represented as a parameterized function  class with an infinite  parameter set.  We shall see that learning is possible for many  but not all   function classes like this, provided the function  class is not too complex.  In this chapter, we examine two measures of the complexity of a function  class, the growth function  and the VC-dimension, and we show that  these are intimately related.  In the next two chapters, we shall see that the growth function  and  VC-dimension  of  a function  class determine  the  inherent sample complexity of the learning problem.  3.2  The Growth Function  Consider a finite subset 5 of the input space X.  For a function  class  if, the restriction of H  to the set  S   that  is, the set of restrictions to 5 of all functions  in H   is denoted  by H\s.  If H\s  is the set of all  functions from S to {0,1}, then clearly, if  is as powerful as it can be in  classifying the points in S.  We can view the  cardinality of H\s   and in  particular how it compares with 2^   as a measure of the classification  complexity of H  with respect to the set  5.  The growth function  of if,  UH : N -> N, is defined  as  Iltf m   = max{ifs  : 5  C X  and  5 =  m}  .  Notice that  UH m   < 2m for all m.  If H is finite, then clearly UH m   <  3  29   30   The Growth Function and VC-Dimension  \H\ for  all ra, and  II m   =  \H\ for  sufficiently  large ra, so the growth function  can be considered to be a refinement of the notion of cardinality that  is applicable to infinite  sets of  functions.  As an example, we shall calculate the growth function  for the percep- tron.  By  a  dichotomy  by  H   of  a finite subset  5  of the  input  space, we mean one of the UH S   ways in which the set S  can be classified  by functions in H into positive and negative examples; that  is, a dichotomy is one of the functions     : 5  ->  {0,1} in H\s.  To count  the  number of dichotomies of a subset  of the  input  space, it  is convenient  to  consider the  parameter  space.  If we divide the  parameter  space into  a  number of regions, so that  in each region all parameters correspond to the same dichotomy of the set, we can then count these regions to obtain an upper bound on the number of dichotomies.  We shall see in later chapters that this approach is also useful  for more complex function  classes.  Theorem  3.1  Let N  be the real-weight  simple perceptron  with n  G  N real inputs and H  the set of functions  it  computes. Then  Here,  Jb=O  a\   a a  for any a > 0 and b > 0.  By convention, we define   £  =  1 for any a > 0. Notice that   £  =  0 for  b > a, and it is easy to see that  ££=o   T   = 2™ for n >  ra.  The proof of Theorem 3.1 involves three steps.  We first show that  the number  of dichotomies of a set  of m  points is the  same as the  number of  cells  in  a  certain  partition  of  the  parameter  space   defined  by  the points .  Then we count the number of these cells when the points are in general position.   A set of points in En  is in general position if no subset of k +1 points lies on a  k — l -plane, for k =  1,..., n.   Finally, we show that  we can always assume that the points lie in general position, in the sense  that  if  they  are  not,  then  this  can  only  decrease the  number of dichotomies.  The set  of parameters for  a real-weight  simple perceptron  with n  in- puts is En  x K, which we identify  with Rn+1.  For a subset  5  C Mn+1 of this space, we let CC 5  denote the number of connected components of 5.   A connected  component of 5  is a maximal nonempty subset  A CS   3.2  The  growth function   31  such that  any two points  of A are connected  by a continuous  curve  lying in A   L e m ma  3.2  For  a set S = { z i , . . . , zm}  C Rn,  let P i , P2, . .. ,Pm  be the hyperplanes  given by  Pi = { w,0   € Mn+1  : wTXi  -6  = 0}.  Then  Proof  Clearly,  \H\S\ is the number  of nonempty  subsets  of parameter space  Rn +1  of the  form  { w,0   E  Mn+1  : sgn wTXi  -0 =bi   for i =  1 , . .. ,ra} ,    3.1   where   61,62?     >&m   runs  through  all 2m  {0,1}-vectors.  Let C  = Rn +1  — UHi  Pi-  In ev^ry  connected component of C, the sign of  wTXi—0 is  fixed, for i = 1 , . . ., m.  Hence  each  distinct  connected  component of C  is contained  in a distinct  set of the  form   3.1 , and so  \Hls\>CC C .  To prove the reverse inequality,  we show that  every set of the  form   3.1  intersects  exactly  one  connected  component  of C.  First,  if a set  3.1  contains   w,0  for which  wTX{  — 6 ^  0 for all i,  then  it intersects  ex- actly  one  connected  component  of C,  as desired.  But  every  set of the form   3.1   contains  such  a  point.  To see  this,  suppose   w,0   satisfies sgn wTXi  — 0  = b{ for  i = 1 , . . ., m.  Define  S = min{\wTXi   -  0\ : wTXi  -  6 ^  0} .  Then   w,6 —  8 2   also  satisfies  sgn wTXi  —  6   =  bi for all i,  but  in addition  wTxi  — 0 ^  0 for all i. It follows  that    Figure  3.1  shows an example of an  arrangement  of three  hyperplanes in  R2, defined  by three  points  in R.  It  turns  out that  the number of cells does  not  depend  on  the  choice of the  planes Pi when  the  points in 5  are in general  position,  as the  following  lemma  shows.  Before  stating   32  The Growth Function and VC-Dimension  A.  Fig.  3.1.  The  planes  Pi,  P2,  and  P3   defined  by  points  xi,X2,3  €  R   divide R2  into six  cells.  this lemma, we note that  the planes in Lemma 3.2 may be expressed in the form  Pi = {ve  Rn+1  : vTZi  =  0} ,  for  i  =  l,...,ra,  where  zf  =   xj,—l .  When  the  X{ are  in  general position, every subset of up to n 4-1 points in {zi, 22,..., zm}  is linearly independent.  To apply the lemma, we shall set d =  n + 1.  Lemma  3.3  For m,d  G N,  suppose  T  =  {21,...,2m}  Q Kd  has every subset of no more than d points linearly independent. Let Pi =  {v € R d : vTZi  =  0} for i =  1,..., m,  and  define  C T   = CC I Rd  -  J  Pi  1  .  Then C T   depends  only on m  and dy so we can write C T   =  C m,d ,   3.2  The  growth  function  33  P2  Fig. 3.2.  Planes  Pi,  P2, and  P  in R3.  The intersections  of Pi  and  P2  with  P are shown as bold lines.  and for  all ra, d >  1,  we  have  k=0   3.2   Proof  First  notice that  linear  independence  of every subset  of up to d points  of T  is equivalent  to  the  condition  that  the  intersection  of  any 1  <  k  < d linear  subspaces Pi is a   d — fc -dimensional linear  subspace  a ' d -  A; -plane' .  With this condition,  it  is clear that  C l,d   =  2 for d > 1, and C ra, 1  =  2 for ra >  1, so  3.2  holds in these cases.   Recall that   m^"1  =  1 for any positive ra.   We shall  prove  the  lemma  by  induction.  Assume  that  the  claim  is true for all T  C W  with  \T\ < m and j  < d.  Then suppose that  we have m planes P i , . . ., P m  satisfying  the independence condition, and that we introduce another plane P  so that  the linear independence condition for the corresponding m +  1 points is satisfied.   See Figure  3.2.   Consider the m intersections of the new plane P  with each of the pre- vious planes.  By the linear independence condition, each intersection is a   d — 2 -plane  in  the   d —  l -plane  P,  and  all  of these   d — 2 -planes satisfy  the independence condition in P   that  is, the intersection of any l < f c < d - l of   them  is a   d  —  1 — fc -plane . Clearly,  having  inter-   The Growth  Function and VC-Dimension  34  sected  Pi,... , Pm,  the  number  of  new  components  of  Rd  obtained  by then introducing P is exactly the number of connected components in P defined by the m   d—2 -planes in P.   For every connected component of P — [JiLi Pi  there are two connected components of Rd —  j£Li Pt> one to either side of P, that were a single component before P  was added.  Con- versely, every new component  created  by the  addition  of P  is a  subset of some component  C of Rd  — U£Li Pi, and must  have a  corresponding new component  on  the  'other  side'  of  P.  Since  C  is  connected,  there must be a connecting point in P, but  C C Rd — UHi Pi>  so  this point is in P  —  J™^ Pi.   The inductive hypothesis then shows that  the number of connected components in our arrangement depends only on m and d, and is given by  -     It follows that   3.2  is true for  all m, d > 1.      Proo    of  Theorem  3.1   Let  5  =  {xi,X2,... ,xm}  be  an  arbitrary subset  of  X  =  En.  Applying  Lemmas  3.2,  3.3,  and  the  observations before Lemma 3.3, we see that  if 5  is in general position  then  If  5  is not  in  general  position,  then  suppose  that  H\s  =  { i,...,   <-}, and  that,  for  i  =  1,2,..., m,  {WJ,0J   corresponds to the function  fj  as follows:  k=0  _  J  1   if wjzi  -  Oj  > 0  J   \  0  otherwise.  Then let  r  __   .  r   T   _  ^1    I  <  7  <  rn  w?T-  — 9    -£  0\  and J =  min^ Sj.  Now if we replace each Oj  by 0' =  0j -   5 2, we obtain   3.3  The  Vapnik-Chervonenkis  dimension   35  a set of parameters   WJJO'J   corresponding to the functions  in H\s,  with the additional 'separation' property that  \wjxi  -6j\  > 5 2 > 0 for all i and   j.  Clearly, it is possible to perturb the points in 5  so that for any set S  within some sufficiently  small ball,   If we define  W  =  maxj  \\WJ\\, then  any point  in  5  can be moved any distance less than S  2W   without altering the classifications of the point by the functions  fj.   Now, general position  is a generic property  of  a set  of points  in  En,  in  the  sense that  the  set  of m-tuples  of points in Rn  that  are not  in general position  has Lebesgue measure zerof  when regarded as a subset of Em n.  As a result, within the ball of perturbed sets S  satisfying   3.3 , we can always find  some set in general position, so that  which,  together  with   3.3 ,  shows  that  the  number  of  dichotomies  is maximal for points in general position.     3.3  The  Vapnik-Chervonenkis  Dimension  For a function  class H  and  a set  5  of m  points in the input  space  X, if H  can compute all dichotomies of 5   in our notation, if  if51 = 2m , we  say  that  H  shatters  5.  The  Vapnik-Chervonenkis  dimension   or VC-dimension  of H  is the size of the largest shattered subset of X   or infinity, if the maximum does not exist .  Equivalently, the VC-dimension of  H  is the  largest  value of m  for  which  the  growth  function  UH  rri  equals  2m.  We  shall  see  that  the  behaviour  of  the  growth  function is strongly  constrained  by the  value of the  VC-dimension, so the VC- dimension can be viewed as a 'single-integer summary' of the behaviour of the growth function.  f  If 5 is not in general position, some subset of S of size A; +1  lies on a  k — l -plane, for some  1 <  A;  <  n.  This means that  the determinant  of some   k +  1  x   k +  1  matrix  constructed  from  an  axis-orthogonal  projection  of  the  elements  of  this subset  is  zero.  However,  there  is  a  finite  number  of  these  matrices,  and  their determinants are analytic  polynomial  functions of the m points.  Clearly, each of these functions is not identically zero, so the set of points that  are not in general position has Lebesgue measure no more than the sum of the measures of the zero sets of these analytic functions, which is zero.   36   The Growth Function and VC-Dimension  For the perceptron, we have  UH{m    =  if n  > m -  1 otherwise,  and this is less than 2m exactly when m  > n + 2, so VCdim Jff   =  n + 1. As  an  illustration  of  the  notion  of  VC-dimension,  the  proof  of  the following  theorem  gives an  alternative  derivation  of the  VC-dimension of the  perceptron.  Theorem  3.4  Let N  be the real-weight  simple perceptron  with n  G  N real inputs.  Then  a set  S  =  {i,...,£m}  C  En  is  shattered by H  if and  only  if  S  is  affinely independent; that  is,  if  and  only  if  the  set {{xj,  —1 ,...,  x^, —1 } is linearly independent in En + 1.  It follows that VCdim fT   = n +  l.  Proof  We first  show that  if 5  is shattered by H  then it must be  affinely independent.  Suppose, to the contrary, that  5  is shattered by H,  but is affinely  dependent.  Then for any b G  {0, l }m  there is a weight vector w in Rn,  threshold  0 in E,  and vector v € Em  such that  I  x\  T  Xn   -1  \  i —1  with  V{ >  0  if  and  only  if  6» =  1.  Let  { wi,i ,... ,  w2™>02"» } be a  representative  set  of  weights  satisfying  these  constraints  for  binary vectors 61,62,      , hm,  the 2m possible values of 6. Then there are vectors vi,..., V2"»  whose components have the appropriate  signs, determined by the b{   such that  -1  \ -1  T  - 1      1^1  W2  \  0\  02   W2m  \ 02m       .  But since we have assumed that  S  is affinely  dependent, without loss of   3.3  The  Vapnik-Chervonenkis  dimension  37  generality we can write  m -l  for some OL\ ..., a m_i.  It  follows that  all column vectors V{  have vmi  = If  we   choose i such that  otjVji  > 0 for  all  1 < j  < m  — 1, X ^ i*   ajvji-  then, necessarily, vmi  > 0, which contradicts our assumption that the V{ together take on all 2m sign patterns.  It follows that VCdim iJ   < n + 1. For the second part  of the proof, suppose that  5  is affinely  indepen-  dent.  Then the matrix     x\  xT   -1  \ -1  has row-rank m.  So for  any vector v  E Km  there is a solution   w, 6  to the equation  x\   -1  xT  m   -1     from  which  it  immediately  follows  that  5  can  be  shattered.  Clearly, VCdim iJ >n  + l.     This result can be generalized to any function  class H whose members are  thresholded,  shifted  versions  of  elements  of  a  vector  space  of  real functions; that is, to a class of the form H  =  {sgn   + g    f  E F},  where g is some fixed real function  and F  satisfies  the linearity condition:  for all  i, 2  £ F  and  0:1,0:2 G  K, the function  a\f\  + 02 2  also belongs to F.  Recall that  the linear dimension  dim F   of a vector  space F  is the size of a basis, that  is, of a linearly independent  subset  { 1,..., fd   Q F for which    £ ?=1  aifi  : a{  G  R}  =  F.  Theorem 3.5 Suppose F  is a vector space of real-valued functions, g is a real-valued function,  andH  =  {sgn   + g  :  f  G  F}.  Then VCdim if   = dim F .  Proof  The proof is similar to that  of Theorem  3.4.  Let  { 1,..., fd}  be a  basis  for  F.  Then,  if  {a?i,X2,...,a:m}  is  shattered  by  if,  there  are   38  The  Growth  Function  and  VC-Dimension  vectors  i>i, v2,...,  v2m  taking   as in the  previous  proof   all possible  sign patterns,  and  corresponding  wi,w 2,       , w2m  e  Rd  such  that  M W\  ' '   W2m    =  V2m     —  9 xi   9{x2    -      \  9 x2   9{x2   \  9 xm   g xm          3.4   where  M  =  [Si       \  fd xx   fd x2   fl Xm   fd Xm   J   The  proof  for  the  simple perceptron  effectively  uses the  basis  functions fi  : x  H> Xi for  i  =  1 , . . ., n  and    n +i  : x  h-> - 1, it  has  g  : x   H+ 0,  and  it denotes  the  last  entry  of Wj by  6j.   If  m  >  d  then  the  matrix  M  on  the  left  of  Equation   3.4   is  not  of row-rank  m,  so as  in  the  proof  of Theorem  3.4,  we may  assume  that  its last  row can  be  written  as a linear  combination  of the  other  rows.  With Vji  and  ct{ defined  as  in  the  previous  proof,  we then  have  m—1  m—1  ^  0> choose i  such  that  a ^i  >  0 for  all   If g{xm   -  Z j aj9 xj   l<j< m  — 1, and  we see that  v mi  > 0.  Otherwise, choose i  such that  a^Vji  <  0 for  all  j,   and  we see that  v mi  < 0.  In  either  case, the  Vi do  not  take  all 2 m  sign  patterns,  and  so VCdim H   <  d.  Conversely,  since  { i, 2,.  -, <*}  is  a  basis  of  F,  there  is  a  d-set {ij2 -.. ,Xd}  such  that  the  linear  system  of  equations   3.4   is  full- rank.  To  see  why,  suppose  that  for  any  d-set  { x i , . ..  ,£<*},  the  rows  of the  matrix  M  form  a  linearly  dependent  set.  Then  the  vector  space  V spanned  by  all  vectors  of  the  form    i x ,   2   x   , . . ., fd x    as  x  ranges through  all of the  domain of F,  has linear  dimension  at  most  d— 1, since it  has  no  basis  of  length  d.  But  this  would  mean  that    1 ,..., <*  were linearly  dependent,  which is not  the case.  It  now follows,  as in the  proof of Theorem  3.4, that  VCdim if   >  d.      As mentioned  above, we shall see that  the growth function  determines the  inherent  sample  complexity  of the  learning  problem.  The  following   3.3  The  Vapnik-Chervonenkis  dimension   39  useful result shows that the behaviour of the growth function  is strongly constrained by the VC-dimension.  Theorem  3.6  For a function  class H  with VCdin^ff   =  d,  IItf m <£ ™Y    3.5   for  all positive  integers  m.  Proof  For m  < d, inequality   3.5  is trivially true since in that  case the sum is 2m.  Assume, then, that  m  > d and fix a set S  =  {i,..., xm}  C X.  We  will  make  use  of  the  natural  correspondence  between  {0,1}- valued  functions  on  a  set  and  subsets  of  that  set  by  defining  the  set system   or family  of sets   The  proof  proceeds by  creating  a transformed  version T*  of T  that  is an  ideal and  has the same cardinality  as T.   A set  J7* of subsets is an ideal if each set in T*  has all of its subsets also in J7*.   We then  prove that  i=0  which will yield the result, since 5  was chosen  arbitrarily.  For an element x of 5, let Tx  denote the operator that, acting on a set system, removes the element  x  from  all sets in the system,  unless  that would give a set that  is already in the system:  Tx{?   = {A-  {x} : A e F}U {A e F : A-  {x} e F} .  Consider now T*  = TXl TX2 - -TXm{T          .  Clearly,  \T*\ =  \T\.  Fur- thermore, for all x  in 5, TX T*  = T*,  so F* is an ideal.  To prove the bound on the cardinality of T*,  it will be useful to define the notion of shattering for  a family  of subsets, in the same way as for a family  of {0, l}-valued functions.  For R  C 5,  we say that  T  shatters R if Tr\  R  =  {A fl R  : A  e F   is the set of all subsets of  R.  Suppose we can show that any subset shattered by T* is also shattered by T.  This would imply that  T*  can only shatter  sets of cardinality  at most d.  Since T*  is an ideal, this would mean that  the largest set in F*   40   The Growth Function and VC-Dimension  has cardinality no more than d.  It  would then follow  that  <£ ? ,  i=0   since this expression is the number of subsets of 5  containing no more than  d elements , and hence that  \T\  is bounded  as required.  It suffices  therefore to show that, whenever T*  shatters a set, so does T.  For x  in 5, and RC  S,  suppose, then, that TX{T   shatters R.  If x is not  in  ?, then, trivially, T  shatters R.  If x  is in R,  then for  all A  C  R with  x  not  in  A, since TX !F   shatters  R  we have A  G TX T   f  R  and A U {x}  G TX T   fl R.  By the definition  of the operator Tx, this implies A  £ T  fl R  and  A U {x}  e  T  D R.  This shows that  T  shatters R.  The result follows.     Theorem 3.6 has the following corollary, which makes it explicit that a function  class of finite VC-dimension has polynomially-bounded  growth function.  Theorem  3.7  For m  > d > 1,  £  ?  < = '.   «=0  x   '   3.6,  Hence, for  a function  class H  with VCdim ff   =  d,  and, for m  > 1, UH m   <md  +  l.  Proof  For 0   d,   m d d d mY  > 1. Hence,  i=o  ^  i=o  ^ * ^  where the second inequality follows from  the Binomial Theorem   Equa- tion   1.6   in  Appendix  1 , and  the  last  inequality  follows from  Euler's Inequality   Inequality   1.4  in Appendix 1 .  The bound  UH m   <md  + l  follows from  Theorem 3.6.  This result, together  with the definition  of the VC-dimension, imme- diately  gives  the  following  corollary,  which  shows  that  the  log  of  the growth function  is within a log factor  of the VC-dimension.      34  Bibliographical  notes   41  Corollary  3.8  For a function  class  H  with VCdim if   =  d,  if m  > d then    <  dlog2 em d .  3.4  Bibliographical  Notes  The  notion  of  VC-dimension  was  introduced  by  Vapnik  and  Chervo- nenkis   1971 .  It  has subsequently  been investigated  by many authors. See, for example,  Bollobas, 1986, Chapter  17 , in which it is referred  to as the trace number of a set system.  Wenocur and Dudley  1981  named VCdim H   +1  the  VC-number of the class H.  It seems that  it was first called  the  VC-dimension  by  Haussler  and  Welzl   1987 .  A number of other  notions  of shattering  and  dimension  have been  studied   see,  for example,   Cover, 1965; Sontag,  1992; Kowalczyk,  1997; Sontag, 1997  , but  we shall  see that  the  VC-dimension  is the  crucial  quantity  for  the learning  problem  that  we  study  here.  The  VC-dimension  has  found application  in  other  areas  of  mathematics  and  computer  science,  in- cluding logic  Shelah, 1972  and computational geometry   Haussler and Welzl, 1987; Matousek, 1995 .  The inductive argument to count the number of cells in a hyperplane arrangement  was apparently first discovered by Schlafli  in the last cen- tury   see   Schlafli,  1950  .  A number  of  authors  have  presented  this argument; see  Cover, 1965; Makhoul, El-Jaroudi and Schwartz, 1991 . The linear algebraic proof of the VC-dimension of a thresholded vector space of real functions   Theorem 3.5 and its corollary, Theorem 3.4  are due to Dudley   1978    see also  Wenocur and Dudley, 1981  .  The  question  of  the  possible  rates  of  growth  of  n^ m   was  posed by  Erdos  in  1970,  and  the  answer   Theorem  3.6   was  independently discovered  by a number  of authors   Sauer,  1972; Shelah,  1972; Vapnik and  Chervonenkis,  1971 ; see   Assouad,  1983 .  This theorem  is widely known as Sauer ys Lemma.  The proof presented here was first presented by  Steele   1978 ;  see  also   Frankl,  1983; Alon,  1983; Bollobas,  1986 . The theorem can also be proved using an inductive argument that is very similar to the argument used to prove the bound on the growth  function of  the  real-weight  simple  perceptron   Theorem  3.1 .  In  addition,  we shall encounter a linear algebraic proof in Chapter 12.  The  proof  of  Theorem  3.7  is  due  to  Chari,  Rohatgi  and  Srinivasan   1994 .   General  Upper  Bounds on Sample  Complexity  4.1  Learning  by  Minimizing  Sample  Error  In  Chapter  2, we showed that  if a set  H  of functions  is finite then  it is learnable, by a particularly  simple type  of learning algorithm.  Specifi- cally, if, given a training sample z, L returns a hypothesis L z   such that L z   has minimal sample error on z,  then  L  is a learning algorithm  for H.  Generally,  for  any  set  H  of  {0, l}-valued  functions   that  need  not be  finite ,  we define  a  sample error minimization  algorithm]  for  H— or  SEM  algorithm—to be  any  function  L  :  Jm=i zm  ~* H  w i th  property that  for any m  and any z G  Z m,  t he  erz L z    =  miner2  i .  Thus,  a  SEM algorithm  will produce a hypothesis  that,  among  all hy- potheses in H,  has the fewest  disagreements with the labelled examples it has seen.  Using this terminology, the learnability result of Chapter 2  Theorem 2.4  has the following consequence.  Theorem  4.1  Suppose  that H  is a finite set of {0,1}-valued  functions. Then any SEM  algorithm for H  is a learning  algorithm for  H.  Our  main  aim  in this  chapter  to  show that  the  conclusion  of Theo- rem 4.1 also holds for many infinite function  classes.  Explicitly, we shall show that if H has finite Vapnik-Chervonenkis dimension then any SEM algorithm for  if  is a learning algorithm.  Theorem  2.4 provides bounds on the  estimation  error  and  sample complexity  of SEM algorithms  for finite  function  classes. But as these bounds involve the cardinality of the  f  We are not yet explicitly concerned  with questions of computability or computa- tional complexity; thus, for the moment, we are content to use the term 'algorithm' when speaking simply of a function.  42   4.2  Uniform convergence  and learnability   43  function  class, they are clearly inapplicable when H is infinite.  We shall see, however,  that,  for  H  of finite VC-dimension,  the  estimation  error and sample complexity of any SEM algorithm can be bounded in terms of the VC-dimension of H.   To a first approximation, In \H\  is replaced by VCdim ff .   Moreover, we shall see that, for some finite classes, the new bounds are better  than those given earlier.  The main theorem is the following.  Theorem  4.2  Suppose that H  is  a set  of functions  from  a set  X  to {0,1}  and  that  H  has  finite  Vapnik-Chervonenkis  dimension] d  >  1. Let L  be any sample error minimization  algorithm for  H.  Then L  is a learning algorithm for H.  In particular,  ifm>  d 2  then the estimation error of L satisfies  and its sample complexity satisfies the  inequality  mL{t,S  < mo e,S  = ^   id In {^j  +ln        .  This is a very general result:  the bound applies to all function  classes H with finite VC-dimension.  It may seem surprising that  such a simple learning algorithm should suffice.  In fact, we shall see in the next chapter that  the  sample  complexity  bound  applying  to  the  SEM  algorithm  is tight  in the  rather  strong  sense that  no learning  algorithm  can have  a significantly  smaller  sample  complexity.   In  Part  4,  we shall  also see that  the  computational  complexity  of  learning  cannot  be  significantly less than that  of minimizing sample error.   4.2  Uniform  Convergence  and  Learnability  As with  the  learnability  result  of  Chapter  2,  the  crucial  step  towards proving learnability is to obtain a result on the  uniform convergence  of sample errors to true  errors.  The use of a SEM algorithm  for  learning is motivated  by  the  assumption  that  the  sample  errors  axe good  indi- cators  of  the  true  errors;  for,  if  they  are,  then  choosing  a  hypothesis with minimal error is clearly a good strategy   as indicated in Chapter 2 by Figure  2.1 .  The following  result  shows that,  given  a  large enough  t  The restriction d >  1 is just for convenience.  In any case, classes of VC-dimension  0 are uninteresting, since they consist only of one function.   44   General Upper Bounds on Sample  Complexity  random sample, then  with  high probability, for  every h G  H,  the sam- ple error  of  h  and  the  true  error  of  h are close.  It  is a  counterpart  to Theorem 2.2.  Theorem 4.3  Suppose that H  is a set o {0,1}-valued functions defined on a set X  and that P  is a probability  distribution on Z  =  X  x  {0,1}. For 0 < e <  1 and m  a positive integer,  we have  Pm  {\erP h  -  &s h \  > e for some h G H}  < 4IlH 2m exp     ~^  The proof of this uniform  convergence result is rather involved and is deferred  until  the next  section.  However, notice that  if  UH  2m   grows exponentially quickly in m then the bound is trivial  it never drops below 1 .  On the  other  hand,  if IIj  2m   grows only  polynomially  quickly in m,  the  bound  goes to  zero exponentially  fast.  So Theorem  4.2  follows fairly  directly from  this result, as we now show.  Proof   of Theorem  4.2   We first  show that if  \erP h   -  erz h \  <e   for all h G JT,    4.1   then  erp L z    is close to optp if .  We then  use Theorem 4.3 to show that  the condition on e suffices   and we solve for m .  Suppose that   4.1  holds.  Then we have  erP L *    <  erz L z    + e  =  minevz h   + e.    4.2   Now,  since  H  might  be  infinite,  we cannot  be  sure  that  the  infimum optp if   =  infheH erp  i  is attained; we can, however, assert  since the infimum is a greatest lower bound  that for any a  > 0 there is an h* G H with erP  i*   < optP    + a.  It follows from  4.1  and   4.2   that  eip L z    <  erz h*  + e  <  erP h*   + 2e  Since this is true for all a  > 0, we must have  erp L 2    < optp ff   + 2c.  Now, Theorem 4.3 shows that   4.1  holds with probability at least 1 — S   4-3 Proof of uniform convergence  result   45  provided  that  is, provided  4II f 2m exp -e2m 8   < 6;  m  So, applying Theorem 3.7, we have that, with probability at least 1 — 5,  \ 1  2 etP L z    < optp H  +  I —  dln 2em d  + ln 4 «   J   f\1   .  For the second part of the theorem, we need to show that m > mo e, S  ensures that  erp L z    < optP H   -f e.  Clearly, by the above, it  suffices if  m>^    dlnm + ciln 2e d  + ln 4 J  .  Now,  since  lnx     0   Inequality   1.2   in Appendix  1 , we have  32d  Therefore, it  suffices  to have  m  m   32  so  suffices.      4.3  Proof of Uniform  Convergence  Result  We now embark on the  proof  of Theorem  4.3.  This is rather  long and can, at first sight, seem mysterious.  However, we shall try to present it in digestible morsels.   46   General Upper Bounds on Sample  Complexity  High-level  view  First,  we give a  high-level  indication  of the  basic  thinking  behind  the proof.  Our  aim  is  to  bound  the  probability  that  a  given  sample  z  of length  m  is  'bad',  in the  sense that  there  is some function  h  in  H  for which \erp h  -  erz h \  > e. We transform  this problem into one involv- ing samples  z  =  rs  of length  2m.  For such a sample, the sub-sample  r comprising  the first half  of the  sample  may be thought  of as the  orig- inal randomly  drawn  sample of length  m,  while the  second  half  s may be thought  of as  a  'testing'  sample  which  we use to  estimate  the  true error of a function.  This allows us to replace eip h   by a sample-based estimate ers ft ,  which is crucial for the rest of the proof.  Next we need to  bound  the  probability  that  some function  h  has err h   significantly different from er8 h .  Since the labelled examples in the sample are cho- sen independently  at  random   according to the distribution  P , a given labelled  example is just  as likely to occur in the first half  of a  random 2m-sample  as in  the  second  half.  Thus,  if we randomly  swap pairs of examples  between  the  first  and  second  halves  of  the  sample,  this  will not affect  the probability that  the two half-samples  have different  error estimates.  We can then bound the probability of a bad sample in terms of probabilities  over a  set  of permutations  of the  double  sample.  This allows us to consider the restriction of the function  class to a fixed dou- ble sample  and  hence,  for  classes with finite VC-dimension,  it  reduces the  problem  to  one  involving  a finite function  class.  As  in  the  proof for the finite case  Theorem 2.2 , we can then use the union bound  and Hoeffding's  inequality.  Symmetrization  As indicated  above, the first step  of the  proof  is to  bound  the  desired probability in terms of the probability of an event based on two samples. This  technique  is known  as  symmetrization.  In  what  follows,  we shall often  write  a  vector  in  Z2m  in  the  form  rs,  where  r,s  €  Zm.  The symmetrization  result is as follows.  Lemma  4.4  With the notation as above,  let  Q = {zeZm   : \evP h  -  erz h \  > e for some h G H}  and  R =    r, s  G  Zm  x Zm  : err ft   -  &. h \  >   for some h G }  .   4.3  Proof of uniform convergence  result   47  Then, for m > 2 e2,  Pm Q <2P2m R .  Proof  We prove that  P2m R   > Pm Q  2.  By the triangle inequality, if  \erP h  -  err  i  > e and  erP  i  -  ers  i  < e 2,  then erP ft  -  er, ft  > e 2, so  P2m R   >  P2m  {3h e H, \erP h  -  err ft  > e and  \eip h -exa h \<e 2}  =   f  Pm{s:3heH,  JQ  \erP h  -  err ft  > e and  \eiP h   -  ers ft  < e 2} dPm r .   4.3   Now, for r  G  Q fix an h G  i? with  erp ft   — err  i  > e.  For this  i, we shall show that  Pm  {erP ft   -  et.{h \    1 2.    4.4   It follows that, for any r G Q we have  Pm  {s : 3ft G ,  erp fc  -  err  i  > e and  \erP h   -  er,  i  < e 2} >l 2,  and combining this result with   4.3  shows that  P2m R   >  Pm Q  2.  To  complete  the proof,  we show  that   4.4  holds  for any h  G H. For a fixed ft, notice  that  mer5  i   is a binomial  random variable, with expectation  merp ft   and variance erp ft  l  -  erp  i  ra.  Chebyshev's inequality  Inequality   1.11  in Appendix 1  bounds the probability that erp  i -er,  i >€ 2by  erp  i  l  — erp  i  m '   em 2 2   which is less  than  l  e2m    using  the fact  that  x l  — x   <  1 4 for  x between 0 and 1 . This is at most 1 2 for m > 2 e2, which implies  4.4 .    Permutations  The  next  step of the proof is to bound the probability  of the set R of Lemma 4.4 in terms of a probability  involving a set of permutations on   48   General Upper Bounds on Sample  Complexity  the labels of the double sample, exploiting the fact  that  a given labelled example  is  as  likely  to  occur  among  the  first  m  entries  of  a  random z 6 Z2m  as it  is to occur among the second m entries.  Let  Fm  be  the  set  of  all  permutations  of  {1,2,..., 2m}  that  swap i and m + i, for all i in some subset of {1,..., m}.   That is, for all a  G F m and i  G {1,...,m},  either  a i   =  i, in which case a m  + i   =  m + z, or o- t  =  m-H, in which case cr m+i   =  i.  Then we can regard a as acting on coordinates, so that  it  swaps some entries  z%  in the first half  of the sample with the corresponding entries in the second half.  For instance, a typical member a of F3  might give    ^  -  The following result shows that  by randomly choosing a permutation a  € F m  and calculating the probability that  a permuted  sample falls in the bad set R,  we can eliminate the dependence on the distribution  P.  Lemma  4.5  Let R  be any subset of Z 2m  and P  any probability  distri- bution on Z.  Then  P2m R   = EPr <rz € R   <  max^  Pr <j* €  R ,  where  the expectation  is over z  chosen according  to P2m ability is over a  chosen uniformly from F m,  the set of swapping permu- tations on {1,..., 2m}  described  above.  f  and the prob-  Proof  First  notice that  for any o in Fm,  P2m{R   = P2m{z  : az  G  R},  since  coordinate  permutations  preserve the  product  distribution  P2m. Since  Fm  is  finite  we  can  interchange  summation  and  integration  as follows.   Here,  1R Z  is the  indicator  function  of  R,  taking*value  1 if z  G R  and 0 otherwise.   P2™ R   =   lR z dP2m z   [  Jz2m   4-3 Proof of uniform convergence  result   49  Pv azeR dP2m z   <  max  Pr  az  G R .  Notice that the maximum exists, since there is only a finite set of values that  the probability under a random permutation  can take.     Reduction  to  a  finite  class  In order to bound P2m  2 , we see now that it suffices to obtain an upper bound  on the  maximum  over  z  G Z2m  of  the  probability  Px az  G R  under random  a.  Lemma  4.6  For the set R  C Z2m  defined in Lemma 4-4>  and  permuta- tion a  chosen uniformly at random from F m,  max  Pr az  G  R   < 2II   2m exp  Proof  Suppose  that  z  =   21,22*    ,22m   €  Z 2m,  where  the  labelled example zi equals  xi,yi ,  and let 5  =  {xi,X2,... ,a?2m}-  ^ et   * =  l^lsl> which is at  most  11   2m .  Then  there are functions fti,  12,..., ht  G  -ff such that  for any h G  if,  there is some i between  1 and t with  hi xk   = h xk   for  1 <  k < 2m.  Recalling that  evz h  = i  {1 < t < m : ftfo    w } ,  m  we see that  az  G R  if and only if some h in H  satisfies  — {1 < i < m  : h xa{i      ya i }\  771  -  ^  {m +  1 < i < 2m :  ft ^ i    Hence, if we define  for  1 < i  < 2m and  1 < j  < t,  we have that  az  G  R  if and only if some j in  {l,...,t}  satisfies  0  otherwise  m  ~  2L, vcr{m+i    50   General Upper Bounds on Sample  Complexity  Then the union bound for probabilities gives  <  UH 2m   max  Pr  Given the distribution of the permutations a, for each i, v3,^ — equals  ±\v{  — v^+ i,  with  each of these two possibilities  equally likely. Thus,  Pr  >e 2  where the probability on the right is over the  %, which are independently and  uniformly  chosen  from  {—1,1}.  Hoeffding's  inequality  shows  that this probability is no more than  2exp —me2 8 , which gives the result.    Combining Lemmas 4.4, 4.5, and 4.6 shows that, for m  > 2 e2,  Pm  {3h e  H,  \erP h   -  erz{h \  > e}  <  2P2m R   < 4UH 2m   exp -me2 8 .  The  same bound  holds for  m  < 2 e2  since in that  case the  right-hand side is greater than one.  Theorem 4.3 is now established.  4.4  Application to the  Perceptron  Perhaps the primary motivation of the work in this chapter is to obtain for infinite  function  classes the type of learnability result we earlier ob- tained  for finite classes.  For example, although  we were able earlier  to prove learnability  for  the   finite   classes of functions  computed  by  the binary-weight perceptron and the fc-bit perceptron, we could not obtain any such result for the general perceptron, as this is capable of comput- ing an infinite  number of functions  on Rn.  However, the theory of this chapter applies, since the n-input  perceptron has a finite VC-dimension of n +  1, as  shown  in  Chapter  3.  We immediately  have the  following result.   4*4  Application  to  the perceptron   51 Theorem  4.7  Let N  be the perceptron  on n  inputs,  and let Z  =  Rn  x {0,1}.  Suppose that L  : Um=i %m  ~* HN  *5 such  that for  any m  and any z G  Zm,  eiz L{z    =  min  erz h .  h€H   That  is,  L  is  a SEM  algorithm.   Then L  is  a learning algorithm for HN,  with estimation  error   W   \ 1  2  eL m,S  <  f ^  n + l ln 2em  n + l   + ln 4 *  J  form  >  n+l  2  andO < S < 1.  Furthermore,  L has sample complexity  mL eyS  < $J  2 n + 1  In ^fj  4- In Q    ,  for  all e,6 € 0,1 .  It is worth noting how Theorem 4.2 compares with the corresponding result  for  finite  function  classes, Theorem  2.4.  We start  by comparing the two sample complexity  results for  the case of the fe-bit perceptron. As we saw in Chapter  2, Theorem 2.4 gives an upper bound of  on  the  sample  complexity  of  a  SEM  learning  algorithm  for  the fe-bit, n-input  perceptron.  Since a fc-bit perceptron  is certainly  a  perceptron, Theorem 4.7 applies to give a sample complexity bound of  for such an algorithm.  For many values of e and 6, this is worse  that is, it  is larger , but  it  should be noted  that  it  has no explicit  dependence on k,  and  so for  large enough  A;, there are ranges of e and  S for  which the new bound is better.  A more striking example of the use of the new bound for finite function classes may be given by considering the  boolean perceptron, the percep- tron restricted  to binary-valued inputs.  Here, the perceptron  functions as usual, but  the relevant  domain is X  =  {0, l } n rather than  X  =  W1. It  is clear that,  since X  is finite, so is the set of functions  H  computed by the boolean perceptron.  It  can be shown that   if n is large enough , \H\ > 2<n2-n  2.  It  is clear that  the VC-dimension  of the boolean  per- ceptron is no more than that of the perceptron—namely, n +1.   In fact,   52   General Upper Bounds on Sample  Complexity  the VC-dimension is precisely n +1.   Suppose that  L is a SEM learning algorithm for the boolean perceptron.  The results of Chapter 2 yield an upper bound of  its sample complexity, which, given that  \H\ > 2^n  ~n  2,  is at  least  on  2   {n2-n ,   n   ,   f2\\  By contrast,  the sample complexity  bound  obtained  from  Theorem 4.2 is  64  This latter bound has worse constants, but note that it depends linearly on n,  whereas the first  bound  is quadratic  in n.  Thus,  in  a  sense, the bound of this chapter can be markedly better for the boolean perceptron than the simple bound of Chapter 2.  4.5  The Restricted Model  We now briefly consider the restricted model of learning, in which there is a target function  in H  and a probability distribution [i on the domain X  of H.   See Chapter  2.   Here, any  algorithm  returning  a  consistent hypothesis is a learning algorithm, a result that  follows from the theory of this  chapter,  since such  an  algorithm  constitutes  a  SEM  algorithm. Moreover, it  is possible to obtain  a better  upper  bound  on the  sample complexity of such learning algorithms than that  obtained above for the general  learning  model.  To  do so,  instead  of using  a  general  uniform convergence result,  we use the  following  bound   where the  notation  is as usual :  for any t and any m  > S e let Pbad be the probability  fim  {for some h £ H,  h xi   =  t xi ,   l<i<m    and erM  i,t   > e} .  Then   The  proof  is similar  to  that  of Theorem  4.3, except  that  Hoeffding's inequality  is replaced  by  a  simple  counting  argument:  the  probability of a permutation  that  gives no mistakes on the first half sample but  at least  cm 2  on  the  second  half  sample  is  no  more  than  2~€m 2.  This   4-6 Remarks   53 replaces the larger  factor  of exp -e2ra 8   that  arose from  Hoeffding's inequality.   This bound can be used to obtain the following result. The proof is similar to that  of Theorem 4.2.  Theorem  4.8  Suppose that H  is a set of functions from  a set X  to {0,1} and that H  has finite Vapnik-Chervonenkis  dimension d > 1. Let L  be a consistent algorithm;  that is, for  any m  and for  any t  G H, if x G Xm  and z  is the training sample corresponding to x  and t,  then the hypothesis  h  =  L z   satisfies h{xi  =  t xi   for i  =  1,2,...,ra.  Then L  is  a learning algorithm  for H  in  the restricted model, with sample complexity  and with estimation  error  The constants in this result can be improved, but that  need not con- cern us here.  What  is most  important  in the sample complexity bound is the dependence on e: for the general model, the upper bound  we ob- tained  involved a 1 e2  factor,  whereas for the restricted  model, we can obtain  an upper  bound  involving the much smaller  1 e factor.  Equiv- alently,  the error  of the hypothesis  returned  by a SEM algorithm ap- proaches the optimum at rate  In m  m  in the restricted model, whereas the corresponding rate in the general model is y  lnm  m.  The intuitive explanation of this improvement  is that  less data is needed to form an accurate estimate of a random quantity if its variance is lower.  4.6  Remarks  A  better uniform convergence  result  Theorem  4.3 is not the best  uniform  convergence  result  that  can be obtained.  It is possible to prove the following result, although the proof is a little more involved.  Theorem  4.9  There are positive constants c\, c^, and c$ such that the following holds. Suppose that H is a set o {0,1}-valued functions defined on a domain X  and that H  has finite Vapnik-Chervonenkis  dimension d.  Let P  be a probability  distribution on Z  =  X  x  {0,1},  e any  real   54   General Upper Bounds on Sample  Complexity  number between 0 and 1,  and m  any positive integer. Then  Pm  {\evP h  -  &z h \  > e  for  some heH}<   cxc\e-c^m\  This leads to the following learnability  result.  Theorem  4.10  There is  a positive constant c  such that  the following holds.  Suppose that  H  is  a  set  of functions  from  a  set  X  to  {0,1} and that H  has finite Vapnik-Chervonenkis  dimension d > 1.  Let L be any sample error minimization  algorithm for  H.  Then L  is  a  learning algorithm for H  and its sample complexity satisfies the inequality  mL e,S  < m'0 e,5  = ^   d + \n Q  The sample complexity bound m'0 e,5  should be compared with  the bound rao e, J  of  Theorem  4.2,  which  contains  an  additional  ln l e  term multiplying the VC-dimension.  The proof of Theorem 4.9 is similar to that of Theorem 4.3, except that we use  the  following  improvement  of  Lemma  4.6.  Ignoring  constants, the growth function  of H  in Lemma 4.6 is replaced by an expression of V C d i mW?  and  this  leads  to  the  improvement  in  the  sample the  form  c complexity bound by a factor  of lnra.  Lemma  4.11  For the set R  C Z2m  defined in Lemma 4-4>  and  permu- tation a  chosen uniformly at random from Fm,  ifm>  400 VCdim if  + l  e2,  then  max  Pr  az  G  R     exp  The proof of Lemma 4.11 involves the following result.  In this lemma, the  VC-dimension  of  a  subset  of  {0, l }m  is  defined  by  interpreting  a vector in  {0, l }m as a function  mapping from  {1,... ,m} to  {0,1}.  We omit the proof   see the Bibliographical Notes .  Lemma 4.12  For any G C {0, l }m,  if all distinct g,g'  eG  satisfy  then  XVCdim G     < T   4.6 Remarks  55  This  lemma  shows  that  the  size  of  any  set  of  pairwise  €-separated points  in  {0, l }m  is  bounded  by  an  exponential  function  of  its  VC- dimension.  In  the  lemma,  the  separation  of  two  points  is  measured using the distance d\  defined  for two vectors g,g'  in Em  as  In  later  chapters,  we  introduce  the  notion  of  a  packing number   see Section  12.2 ; we shall see that  Lemma 4.12 corresponds to a bound on the d\  packing number of a class with finite VC-dimension.  The proof  of Lemma 4.11 uses a  chaining argument.  The  idea is to split  each  function  of  interest  in  the  original  class  H\z  into  a  sum of functions  chosen from  a  sequence  of classes of progressively  increasing complexity.  Then the desired uniform convergence result for the original class is obtained by combining uniform convergence results for all of these classes. The classes are constructed  carefully  to balance the complexity of  the  class  and  the  magnitude  of  functions  in  the  class.  For  classes that  are more complex,  although  the  uniform  convergence result  must apply  to  a  richer  class,  that  class  contains  smaller  functions,  so  the variables of interest have smaller variance.  This delicate balance leads to a slight improvement over Lemma 4.6, in which we considered the whole class H\z  at once by directly applying the union bound and  Hoeffding's inequality.  Proof   of Lemma 4.11  Notice that  maxzez2m  Pr <72 G R   is equal  to the maximum over z G  Z2m  of  Pr  [3h  G  H,  where the  probability  is over  fa  chosen  uniformly  from  {±1}, and  the function  ^ f at   >! *  takes value 1 if h xi   ^  y%  and 0 otherwise.  Fix z =    1,2 1 ,...,  x2miy2m    G  Z2m  and  define  G =   {{e h{xl ,y1 ,...ie h x2m ,y2m  :heH}.  Let  p  =  Pr 3ftG,   56  General Upper Bounds on Sample  Complexity  Let d = VCdim  .  It is easy to verify  that  VCdim G   < d.  Let  n  =  _lo62mJ  + 2f«  We define  a  sequence  of sets  Go,...,Gn as  follows,  so that  as i  increases  it  constitutes  a  progressively  better approximation  of G.  Fix Go as a set containing  a  single   arbitrary  element of G.  Then, for each j  = 1,..., n, first set Gj = Gj-i, then add to Gj an element g G G for which d\ g,gt   > 2~i for all g1 G Gj. Keep adding  these  distinct  elements  until  it  is no longer  possible  to do  so. Then it is easy to verify  that  the sets Gj have the following properties, forj  =  l,...,n.     for all g G G, some fa G Gj has di gj,g   < 2~j,    for all distinct g,g' G Gj, di{g,g'   > 2~',    Gn = G.  We now define  a sequence of sets  Vo,..., Vn  that  contain  differences between vectors in the sets Gj. Let VQ = Go. For j  = 1,...,n,  define  where for each g e G and j  = 0,..., n, £  denotes an element of Gj  that has d\ gj,g   < 2~J.  It is easy to see that  these difference  sets Vj have the following properties, for j  = 0,..., n.     for all v e Vj,     for all g  G Gj,  there  are vectors  vo G Vo,vi  G V\,...,Vj  G Vj, In particular,  for all g  G G, there are  such  that  p =  Jjl-0Vi.  ^o £ Vo,..., vn  G Vn with g = X^_o ^.  Hence,  p < Pr I 3v0 G  Vo,..., vn  G Vn,  y   m   n  t=i   j=o  where «j- =  v ,i,..., Vj^m -  By the triangle inequality,  p < P r   3 v0  G  V o,..., vn  G Vn, ]T  t  Recall that the floor function,  [-J, is defined  as the largest integer no larger than its  real  argument,  and that  the ceiling  function,  [*",  is defined  as the smallest integer no smaller than its argument.   and  if we choose  Co, ...,€„  such  that  4.6 Remarks  j =0  then  57   4.5   We shall  use  HoefFding's  inequality   Inequality   1.16   in  Appendix  1  to give a bound  on each of these probabilities, exploiting the  fact  that ]C£Li vt -vm+i 2  gets progressively smaller as i increases. Indeed, since Z fc  M   m  and vt 6 {-l,0,l}2m,  we have  tf  =  4 It'  2m  Applying  HoefFding's  inequality  shows that  for  each  j  2m  Pr  \3v  e  Vh  Now  \Vj\  <  \Gj\,  points  in  Gj  are  2~J-separated,  and  VCdim Gj   < VCdim G   < d, so Lemma 4.12 implies that  Hence,  p < 2 - 41d Y^, exP  i  i=o  If we choose tj  =  c-y  j + 1 2^ 12, then it is easy to verify  that   4.5  is satisfied.  Substituting shows that  p  <  ~    58   General Upper Bounds on Sample  Complexity  For m > 400 d + l  e2, the denominator is at least  1 2,  which gives the result.     4.7  Bibliographical  Notes  The results presented in this chapter are to a great extent derived from the work of Vapnik and  Chervonenkis   1971  in probability theory   see also  the  book  of  Vapnik   1982  .  In  particular,  Theorem  4.3  is from Vapnik and Chervonenkis  1971 . Instead of using the 'swapping group' Fm,  however, their original proof involved the use of the full  symmetric group   all  permutations  of  {1,2,..., 2m} .  It  was subsequently  noted that  the former  resulted  in easier proofs of this and similar results; see Pollard   1984 , for example.   See also  Dudley,  1978 .   Our use of Ho- effding's  inequality follows Haussler   1992 .  The use of Inequality   1.2  from  Appendix 1 in the proof of Theorem 4.2 follows Anthony, Biggs and Shawe-Taylor  1990 . That paper gave sample complexity bounds for the restricted model with improved constants; see also  Lugosi, 1995 . In our discussion of sample complexity bounds for the boolean perceptron, we noted that the number of such functions is at least 2<n ~n  2.  This is due to Muroga   1965; 1971 .  Results of Chapter  3 lead to an upper  bound 2n2 i+o i    jjj f g^  a  r e c e nt  paper of Zuev  1989  shows that  if N n   is the number of functions  computable by the n-input boolean perceptron, then, as n -* oo, log2 N n   ~  n2.  The results on the complexity of learn- ing in the restricted model are from Blumer et al.  1989 . Theorem 4.9 is  due  to  Talagrand   1994 .   See also   Alexander,  1984 .   The  proof of Lemma 4.11 is due to Long  1998a .  Although this lemma does not have the best constant in the exponent, it has a simple proof. For related chaining arguments, see   Pollard,  1984; Pollard,  1990 .  Lemma 4.12 is due  to  Haussler   1995 .  For  large  values  of the  VC-dimension,  it  im- proves Theorem  18.4 for the special case of {0, l}-valued  functions.   General  Lower Bounds on Sample  Complexity  5.1  Introduction  In the previous chapters we showed that a class of functions of finite VC- dimension is learnable by the fairly natural class of SEM algorithms, and we provided  bounds on the estimation  error  and  sample  complexity of these learning algorithms in terms of the VC-dimension of the class.  In this chapter we provide lower bounds on the estimation error and sample complexity  of any  learning  algorithm.  These  lower bounds  are  also in terms of the VC-dimension, and are not vastly different  from the upper bounds  of  the  previous  chapter.  We shall  see,  as  a  consequence,  that the VC-dimension not only characterizes learnability, in the sense that  a function  class is learnable if and only if it has finite  VC-dimension, but it provides precise information  about  the number of examples required.  5.2  A  Lower  Bound  for  Learning  A  technical lemma  The first  step towards a general lower bound  on the sample complexity is the  following  technical  lemma,  which  will  also prove  useful  in  later chapters. It concerns the problem of estimating the parameter describing a Bernoulli random variable.  Lemma  5.1  Suppose  that a  is a random variable  uniformly distributed on  {a_,a+},  where  a_  =  1 2  -  e 2  and a+  =  1 2  +  e 2,  with 0  < e  <  1.  Suppose that £i,...,£m  we  i.i.d.  independent and  identically distributed  {0,1}-valued  random variables  with Pr &  =  1  =  a  for all  59   General Lower Bounds on Sample  Complexity  60  i.  Let f  be a function from  {0,l}m  to {a_,a+}.  Then  .    5.1     5- 2   Hence, if this probability  is no more than 6, where 0 < 6 < 1 4,  then  In this lemma,    can be viewed as a decision rule.  That  is, based on the observations &,     & , . .. ,f m   represents a guess of whether  a =  a_ or  a  =  a+.  The  lemma  shows  that  for  every  decision  rule  there  is a  limitation  on  its  accuracy  that  depends  on  the  similarity  of the  two choices  e  and the amount of data   m .  Proof  For a random sequence £ =   £i,..   ,£m > define N £   =  {i: & = 1}.  We first show  that  the  maximum  likelihood  decision  rule,  which returns a  =  a_  if and only if N £   < ra 2, is optimal, in the sense that for any decision rule  ,  the probability of guessing a incorrectly  satisfies  >  i m   2 a  = a  +  .    5.3   To see this, fix a decision rule  .  Clearly,  \  Pr  f O  = a-  and N O >m 2\a  =    Pr    O  = a_  and JV fl < m 2 a  = a+  +   p r      O  = a+ and AT O > m 2a = a.   +  i  Pr      0  = a +  and AT ^  < m 2 a  =  a_05.4   But the probability of a particular sequence f  is equal to   5.2  A  lower bound for  learning   61  so if N 0  > m 2, Pr £a  =  a+   > Pr £a  =  a_ .  Hence,  Pr      0  =  a_  and  N £   > m 2\a  =  a+   >  Pr    £   = a_  and  N Q  > m 2 a =  a_ .  Similarly,  =  a+  and JV fl     =  a +  and JV O < m 2 a =  Substituting  into   5.4 ,  and  using  the  fact  that  either    £   =  a_  or   ^   = a+, gives Inequality   5.3 .  Now, we assume that m is even  for, if it is not, we may replace m by m +1,  which can only decrease the probability , and discard the second term to show that  Pr   O   a   >  \Pv N O  > m   2 a  =  a_ ,  which is the probability that a binomial  m, 1 2—e 2  random variable is at least m 2.  Slud's Inequality   Inequality   1.22  in Appendix  1  shows that  where  Z  is a  normal   0,1   random  variable.  Standard  tail  bounds  for the normal distribution   see Inequality   1.23  in Appendix  1  show that  for any fi > 0.  Hence,  Pr    O * a  > I   l  -  It follows that  Pr   £     a   > <J when  provided  0  <  <5  <  1 4.  Recall  that  if m  was odd,  we replaced  it  with m + 1, which gives  5.1  and   5.2 .      62   General Lower Bounds on Sample  Complexity  The  general  lower  bound  Using Lemma 5.1, we can now obtain the following general lower bound on the sample complexity of a learning algorithm.  Theorem  5.2  Suppose  that H  is a class of {0,1}-valued functions and that H  has  Vapnik-Chervonenkis  dimension d.  For any learning  algo- rithm L for H,  the sample complexity raz, e, S  of L satisfies  mL e,S   >  320e2  for  all 0  <  e,S  <  1 64.  Furthermore,  if  H  contains  at  least  two  func- tions,  we  have  for  allO<e<l   andO<5  <  1 4.  Proof  At  the  heart  of  the  proof  of  these  inequalities  is  a  simple  ap- plication of the probabilistic  method,  a useful  technique for  proving the existence of objects  with  certain  properties.  To apply  the  method, we assume  that  the  underlying  probability  distribution  P  is itself  chosen uniformly  at  random  from  a  specified  finite  class of distributions.  We then  show that,  for  any  learning  algorithm,  the  expectation   over  this random  choice  of  P   of  the  probability  of  failure  is  at  least  S, which implies that for some distribution P, the probability of failure is at least 8.  Notice  that  we can  prove  that,  for  each  algorithm,  there  is  a  dis- tribution  that  leads to failure  of the algorithm, even though  we do not explicitly construct a distribution that is problematic for that algorithm. The main  idea behind  showing that  the expected  probability  of  failure is at  least S for the first  inequality of the theorem is to concentrate the distribution on a shattered set, and then set the conditional probability Pv y =  l\x   near  1 2   actually   1 ± ce  2, for some constant  c  for each point  x  in the  shattered  set.  To get  near-optimal  error,  the  algorithm must estimate these conditional probabilities to accuracy ce for a signif- icant  proportion  of the  points,  but  Lemma  5.1 shows that  this  means that  it  must  see at  least  of order  1 e2  examples of each point.  Having given this overview, we now proceed with the technical details.  Since H  has VC-dimension d, there is a set 5  =  {x\,  2,      , d} of d examples that  is shattered  by H.   We may assume d > 1; the theorem   5.2  A  lower bound for  learning   63  clearly holds if d = 0.   Let V  be the class of all distributions P with the following properties:     P  assigns zero probability to all sets not intersecting 5  x  {0,1},    for each i =  1,2,..., d, either  -  P Xi,  1  =   1 + a   2d   and P x,,0   =   1 -  o   2d ,  or -  Pfo,  1  =   1 -  a   2d   and P s«,0   =   1 + a   2d , where 0 < a  <  1.   The parameter  a  will be chosen later.   First,  we note  that  for  a  given  P  £  V,  the  optimal  error  optP if  is achieved  by  any  function  h* €  H  for  which  h* x{  =  1 if  and  only if  P xi,l   =   1 4- a   2d .   The  class  H  contains  such  functions  h* because it shatters 5.   The optimal error is given as follows:  d  1 -   2d   1 2   2'  Furthermore, for  any h G  H  we have  d  erP{h   ^  I  f  a       5.5   For any sample z  €  Z m,  let  N{z   =   Ni ^ ,...,Nd z  >  where iV< z  is the number of occurrences of either   xi,0   or   xi, 1  in z.  Then for  any fo  =  L z   we have  TV  i =l  where N  =   iVi,..., Nd  ranges over the set of d-tuples of positive inte- gers with Y^i=i Ni  = m.  From Lemma 5.1,   64   General Lower Bounds on Sample  Complexity  It  is easy to check that  this is a convex function  of iV*, and so  E  [d^lfl  by Jensen's inequality   see Appendix  1 .  Let B  denote the quantity on the right hand side of the last inequality.  Using the fact  that  any [0, l]-valued random variable Z  satisfies  for 0 < 7 <  1, we have  d    i   for any 0 < 7 <  1. Hence,  E Pm  {erp L *   -  optP if   >  ^Ba   where the expectation  is over the random choice of the probability dis- tribution  P  from  V.  It follows that  some P  has  Pm  {erp L ^   -  optp ff   > 7J3a} >   1 - <  Then  and  together imply  B  > j^    5.6   c < jBa   Pm  {erP L z    -  optP H   > e} > 6.    5.7    5.8    5.3  The restricted model   65  Now, to satisfy   5.6   and   5.7 , choose 7  =  1 -  8S.  Then   5.7   follows from  Setting a  =  8e  l — 85  implies e = 7a 8, which together with  5.6  and the choice of 7 implies   5.7 , since B  > 1 8  in that  case. Hence,  m  m  implies  5.8 .  Using the fact  that  0 < e,5 < 1 64  shows that  will suffice,  which gives the first inequality of the theorem.  The  proof  of  the  second  inequality  is  similar  but  simpler.  Since H contains  at  least  two functions,  there  is a point  x  e  X  such that  two functions  hi, hz e  H have h\ x   ^  h,2{x .  Consider the distributions P_ and  P+  that  are concentrated  on the  labelled  examples   x,h\ x    and  x,h,2 x  ,  and  satisfy  P± x,hi x    =  a±  and  P±{x, i2 x    =  1 — a±, with a±  =   1 ± e  2 as in Lemma 5.1.  If P  is one of these  distributions and the learning algorithm chooses the 'wrong' function,  then  evp L z    -  optp il   =   1 + e  2 -   1 -  e  2 = e.  Hence, learning to accuracy e is equivalent to guessing which distribution generated the examples.  Now, if we choose a probability  distribution  P  uniformly  at  random from the  set  {P-1P+}J  then  Lemma 5.1 shows that,  for  any learner  L, the expectation   over the choice of P   of the probability   over z  G  Zm  that  the learner has erp L z   -  opt P if   > e is at least  5 if  provided 0 < S < 1 4.      As an important consequence of this theorem, we see that  if a class of  functions  is learnable then it necessarily has finite VC-dimension.  5.3  The Restricted Model  It  is natural  to  ask  whether  finite  VC-dimension  is  also necessary  for learnability  in  the restricted  model,  and  if so to  seek  lower bounds on   66   General Lower Bounds on Sample  Complexity  the  sample  complexity  and  estimation  error  of  learning  in  this  model. Theorem 5.2 tells us nothing about the restricted model since the prob- ability distributions used in the proof of that theorem do not correspond to  target  functions  combined  with  probability  distributions  on the do- main.  However,  it  is  still  possible  to  use  the  probabilistic  method  to obtain lower bounds in this model.  Theorem  5.3  Suppose  that H  is a class of {0,1}-valued functions and that H  has  Vapnik-Chervonenkis  dimension d.  For any learning  algo- rithm L  for  H  in  the restricted model, the sample complexity rriL e,5  of L satisfies  for  all 0 <  e <  1 8  and 0 <  S < 1 100.  Furthermore, if H  contains at least three functions,  we have  mL e,S >  — l  forO<e<  3 4  and 0 < 5 < 1.  Proof  Suppose that  S  =  {a?o,£i,... ,xr}  is shattered  by H,  where r  = d - 1.    We assume that d > 2.   Let P be the probability distribution on the domain X  of H  such that  P x   =  0 if x  £ 5, P x0   =  1 -  8e, and for i  =  1,2,...,d,  P x{   =  8e r.  With  probability  one, for  any  ra,  a  Pm - random sample lies in 5m , so henceforth,  to make the analysis simpler, we assume without  loss of generality  that  X  =  S  and  that  H  consists precisely  of all  2d  functions  from  5  to  {0,1}.  For convenience, and  to be explicit, if a training sample z  corresponds to a sample x  £ Xm  and a function  t 6 H,  we shall denote L z   by  L x,t .  Let S'  =  {x\,  2 ?      »xr} and let H' be the set of all 2r functions h € H such that h xo  =  0. We shall make use of the probabilistic method, with target functions t drawn at random according to the uniform distribution U  on  H1.  Let  L  be  any  learning  algorithm  for  H.  We obtain  a lower bound  on  the  sample  complexity  of  L  under  the  assumption  that  L always returns a function in H;;  that is, we assume that whatever sample z is given, L z   classifies xo correctly.   This assumption causes no loss of generality:  if the output  hypothesis of L  does not always belong to  if', we can  consider  the  'better'  learning  algorithm  derived  from  L  whose output  hypotheses  are  forced  to  classify  x0  correctly.  Clearly  a lower bound on the sample complexity of this latter  algorithm is also a lower   5.3  The restricted model   67  bound  on  the  sample  complexity  of  L.   Let  m  be  any fixed positive integer and, for x  £ 5m, denote by l x   the number of distinct  elements of S" occurring in the sample x.  It  is clear that  for  any x  € S", exactly half of the functions  h1 in H1 satisfy  h! x  —  1  and exactly half  satisfy h' x   = 0 . It follows that  for any fixed x  G Sm,  ^1,    5.9   where E^i  .   denotes expected  value when t is drawn according to  U, the uniform  distribution  on H1.  We now focus on a special subset S  of 5m,  consisting of all x for which Z z  < r 2.  If a: € <S then, by  5.9 ,  Et~u  erp L x, *   > 2e.    5.10  Now, let  Q denote the restriction of Pm  to <S, so that  for  any A  C 5m, Q J4  = Pm{AnS  Pm S .  Then  Ex~QEt~c erp Z, :r,£    > 2e,  since  5.10  holds for every x € 5.   Here, EX^Q -   denotes the expected value when x  is drawn  according to  Q.   By Fubini's  theorem,  the two expectations operators may be interchanged.  In other words,  But this implies that  for  some t1 G H',  ^U  ecP{L x,t    > 2c.    > 2e.  Let  pc  be  the  probability   with  respect  to  Q   that  erp L x,t'    >  e. Given  our  assumption  that  L  returns  a  function  in  H1,  the  error  of L x, t'   with  respect  to  P  is never  more than  8e  the  P-probability  of 5; .  Hence we must have  2e < Ea.^gerP L a;,t     < Sep€ +   1 -pc e,  from  which we obtain p€  > 1 7.  It  now follows   from  the  definition  of Q  that  P™{erp L s,O >e}  >  Q{erP  L a,f    >  e}Pm 5   =  PePm 5  >  yPm 5 .  Now, Pm S   is the probability that a Pm-random sample z has no more than r 2  distinct entries from 5'.  But this is at least  1 -  GE 8e, m, r 2    68   General Lower Bounds on Sample  Complexity   in the notation of Appendix 1 . If ra < r  32e  then, using the Chernoff bound   see Inequality   1.14   in  Appendix  1 , it  can  be  seen  that  this probability is at  least  7 100.  Therefore, if ra < r  32e   and 5 < 1 100,  and the first  part  of the result follows.  To prove the second part of the theorem, notice that  if H  contains at  least  three functions,  there  are examples o,6  and functions   ii, i2  € H such  that  hi a   =  Ii2 a  and  hi b   =  l, i2 &   =  0.  Without  loss of generality,  we  shall  assume  that  hi  a   =   i2 a   =  1.  Let  P  be  the probability  distribution for which P a   =  1 -  c and P b  =  e  and such that  P  is zero elsewhere on the example set X ,  The probability that  a sample x  € Xm  has all its entries equal to a is  1—e m. Now,  1—e m > S if and only if  ra  Further,  -  ln l  -  e   <  2e for  e <  3 4.  It  follows  that  if m  is no more than   l  2e   ln l <J   then,  with  probability  greater  than  5,  a  sample x £ Xm  has all its entries equal to a.  Let a1  denote the training sample a1  =    a, 1 ,...,  a, 1   of length  ra.  Note that  a1  is a training  sample corresponding  both  to  fti  and  to   12.  Suppose  that  L  is  a  learning  al- gorithm for  H  and  let  La  denote the output  L{al   of L  on the  sample a1.  If La 6   =  1 then  La  has error at  least  c  the probability  of b  with respect to  &2, while if Lo &  =  0 then it has error at least e with respect to hi.  It follows that  if m  <  l  2e   In  1 6  then either  or  Pm  {evP  L z, ii    > c} > ^ { a1}  >  5  Pm  {erP  L z, h2    > e} > Pm{a1}  > 6.  We therefore  deduce that  the learning algorithm  'fails' for  some t  € H if m  is this small.      As in  the  corresponding  upper  bounds,  the  key difference  to  be ob- served  between  the  sample  complexity  lower  bound  for  the  restricted model and that  given in Theorem  5.2 for  the general model is that  the former  is proportional to  1 e  rather than  1 e2.   5.4  VC-dimension quantifies sample complexity   69  5.4  VC-Dimension  Quantifies  Sample  Complexity  and  Estimation  Error  Combining  Theorem  5.2  and  Theorem  4.10,  we  obtain  the  following result.  Theorem  5.4  Suppose that H  is  a set  of functions  that  map from  a set X  to {0,1}.  Then H  is learnable  if and only if it has finite Vapnik- Chervonenkis dimension.  Furthermore, there are constants c\, c^  > 0 such that the inherent sample complexity  of the learning problem for H satisfies  ~   vCdim   + In  ^JJ  < mH{e,6  < ^f   vCdim H  + In QJ  J .  for  allO<e<   1 40  and  0 <  6 <  1 20.  In  particular,  since Theorem  4.10 applies to  sample error  minimiza- tion  algorithms, if  L  is a  SEM algorithm  for  if,  then  its  sample com- plexity  satisfies  these inequalities,  and  so its estimation  error grows as y  VCdim H   + ln l <J    m.  This result shows that the VC-dimension of a function class determines its  statistical  properties  in  a  rather  strong  sense.  It  also  shows  that the simple SEM algorithms have a nearly optimal estimation rate.  The results presented in this chapter and the previous chapter together imply the following theorem.  In this theorem, we use the      notationf, which indicates that  the functions  are asymptotically  within a constant  factor of each other.  Theorem  5.5  For a  class H  of functions  mapping from  a set  X  to {0,1},  the following statements are  equivalent.   i   H  is  learnable.  ii   The inherent sample complexity  of H, ra f €, S , satisfies  mH e1S   = Q  -olnf  7  f  Recall  that,  for  functions  f,g  : Nk  —>  N,     =  O g   means  that  there  are pos- l,...,fc ,  itive  numbers  c  and  &!,...,&*.  such  that  for  any  a%  >  b{   for  i  =    oi,...,o k   <  C0 ai,...,afc .  Similarly,     =  Q g   means  that  there  are pos- itive  numbers  c  and  bi,...,t>k  such  that  for  any  a»  >  6»   for  i  =  l,...,fc ,   oi,...,afc   >  C9 ai,...,a* .  Also,     =  Q g   means that  both     =  O g   and    =  Q g .  For convenience, we extend this notation in the obvious way to functions that  increase as some real argument gets small.   70   General Lower Bounds on Sample  Complexity   iii   The inherent estimation error of H,  e^ m,5 , satisfies   iv   VCdim fT   < <»   v   The growth function  of H,  11    m ,  is  bounded  by a polynomial  in  m.   vi   H  has  the following uniform  convergence  property:  There is a  function  eo m> 5   satisfying    for  every probability  distribution P  on X  x  {0,1},  Pm   sup  erP ft   -  or* ft   > eo fM l  < 5,  eo fM l J J  ihH   60 m, J   =  0  This theorem shows that the learning behaviour of a function  class if and  its  uniform  convergence properties  are strongly  constrained  by  its VC-dimension.  Recall  that,  in  order  to  be  able to  apply  Theorem  4.3  or  its  improvement,  Theorem  4.9   in  the  previous  chapter,  we  only need  the  growth  function  11   m   to  grow  more  slowly  with  m  than ec2fn.  In  fact,  Theorem  3.7  shows  that  it  either  grows  as  2m   if  the VC-dimension  is infinite   or  as md   if the  VC-dimension  is  finite .  So Theorem  4.3 can only be used to show that  estimation  error  decreases as 1 y rfi  equivalently, that sample complexity grows as 1 e2 .  Now the lower bounds in this chapter  show that  this is essentially the only  rate possible:  while the constants are different  for  different  function  classes, if the VC-dimension  is finite, we have this rate, and if it  is infinite,  the class is not learnable.  The same characterization is of course also possible for the restricted model, by making use of Theorems 5.3 and 4.8.  The following  theorem implies  that  we can  add  the  property  lH  is learnable in the  restricted model' to the list of equivalent  statements in Theorem 5.5.  Theorem  5.6  Suppose that H  is  a set  of functions  from  a set  X  to {0,1}.  Then H  is learnable  in the restricted model if and only if H has finite  Vapnik-Chervonenkis dimension. Furthermore,  there are constants ci,C2  >  0  such  that  the  inherent sample complexity of  the  restricted learning problem for H satisfies   5.5  Remarks   71  <   rnL eyS   5.5  Remarks  Relative uniform convergence results  The  results  in  this  chapter  show that  the  rate  of uniform  convergence of erp  i   to erz h   can be no faster  than  1 y m   if we require an upper bound that  is valid for all probability distributions .  In fact,  the rate of uniform  convergence of eip h   to a slightly larger value is considerably faster; the following theorem shows that if the VC-dimension of the class is finite, erp h   decreases to  1 + a erz ft   at least as quickly as Inra ra, for any fixed  a  > 0.  Theorem 5.7  Suppose that H  is a set o {0,1}-valued functions defined on a set X  and that P  is a probability  distribution on Z  = X  x  {0,1}. For 0   0,  and m  a positive integer,  we have  Pm  {3ft  G if  : evP h   >   1 + a eiz h   +  ?}  The theorem follows immediately from the following theorem, on set-  ting v to 2 3 a  and e to a  2  + a .  Theorem  5.8  For H  and P  as in  Theorem  5.7,  and 0 < e, v < 1,  : * 'W   + erz h  + v   J  > e} < 4II* 2m exp  Proof  The theorem follows from  the inequality  pmLheH:   evP h -ev h   >   V \  <  4UH{2m exp   5.11  We omit the proof of this inequality, but note that it uses similar ideas to the proof of Theorem 4.3.  To see that the inequality implies the theorem, suppose that  erp  i  — erz h     0 we have two cases:   i   If evP h   <  1 + 1 a V,  then erP ft   < evz h  + r 2 l +  I a .   72   General Lower Bounds on Sample  Complexity   ii   IferP  i   >   1 + I   a   V,  then evP h   <  &, fc +a  l+a erP fc ,  and so erp  i   <   1 +  a e"r2 ft .  In either case, erp  i   <   1 + a erz h   + ry2 l + I a .  Hence,  Pm  {3heH:   erP h   >   1 + a er,  i  + r 2 l +  I a }  Choosing a  =  2e  l -  e  and ry2 =  2i e2  l  -  e2  gives the result.      The inequality in Theorem  5.8 can be made two-sided; the  argument is similar.  That  theorem  also  implies  a  version  of  Theorem  4.3, with different  constants.  To see this, notice that  erp ft   <  1 and etz h   < 1, so  Pm  {3heH:   erp ft   -  evz h   > rj}  h e  H :  5.6  Bibliographical  Notes  The second part  of the proof of Lemma 5.1 was suggested by Jon Bax- ter   see Theorem  12 in   Baxter,  1998  .  The  constants  in that  lemma improve  on  those  in  many  similar  results   see,  for  example,   Simon, 1996; Ben-David  and  Lindenbaum,  1997  .  Lower bound  results of the form of Theorem 5.2 have appeared in a number of papers; see  Vapnik and  Chervonenkis, 1974; Devroye and Lugosi, 1995; Simon, 1996 .  The bounds in Theorem 5.2 improve  by constants  all previous bounds that we are aware of.  The proof technique for the first  inequality of the theo- rem uses ideas of Ehrenfeucht, Haussler, Kearns and Valiant  1989 , who used  a  similar  approach  to  give lower bounds  for  the  restricted  model  Theorem 5.6 .  The constants in the first  inequality of Theorem 5.6 can be  improved   from  32 to  12 and  100 to  20   at  the  expense  of a  more complicated proof; see  Devroye and Lugosi, 1995, Theorem 2 .  Theorem  5.6 shows that,  in the restricted model of learning, the  rate at  which  the  estimation  error  decreases as the  sample size increases is essentially  the  same  for  every  learnable  function  class.  This  does  not imply that,  for  every target  function,  the estimation  error decreases  at this  rate.  In  fact,  it  is possible  for  the  rate  to  be  considerably  faster   5.6  Bibliographical  notes   73  for  every  function  in  the  class,  but  with  different  constants.  See,  for example,  Schuurmans, 1995 .  Inequality   5.11 , which  we used  to  derive the  relative  uniform  con- vergence results   Theorems  5.7 and  5.8   is an improvement  on a result of Vapnik   1982  due to Anthony  and Shawe-Taylor   1993b .   The VC-Dimension  of Linear  Threshold  6  Networks  6.1  Feed-Forward  Neural  Networks  In this  chapter,  and  many  subsequent  ones, we deal with  feed-forward neural networks.  Initially, we shall be particularly concerned with feed- forward linear threshold networks, which can be thought of as combina- tions of perceptrons.  To define  a neural network class, we need to specify  the  architecture of the network and the parameterized functions computed by its compo- nents.  In general, a feed-forward  neural network has as its main compo- nents a set of computation  units, a set of input units, and a set of con- nections from  input  or computation  units to computation  units.  These connections  are  directed;  that  is, each  connection  is from a  particular unit  to a particular computation unit.  The key structural property of a feed-forward  network—the feed-forward condition—is that these connec- tions do not form any loops.  This means that  the units can be labelled with  integers in such a way that  if there is a connection  from  the  unit labelled i to the computation  unit  labelled j  then i  < j.  Associated  with  each  unit  is  a  real  number  called  its  output  The output  of a computation  unit  is a particular  function  of the outputs of units  that  are  connected  to  it.  The  feed-forward  condition  guarantees that the outputs of all units in the network can be written as an explicit function  of the network inputs.  pften  we will be concerned with  multi-layer networks.  For such net- works, the computation units of the network may be grouped into layers, labelled 1,2,..., I, in such a way that the input units feed into the com- putation  units, and if there is a connection from  a computation unit  in layer i to a computation unit in layer j,   then we must have j  > i.  Note, in  particular,  that  there  are  no connections  between  any  two units  in  74   6A  Feed-forward neural networks  75  input units   [  1  J   12   «  layer 1  layer 2  layer 3  Fig. 6.1.  A feed-forward  network.  a given layer.  Figure 6.1 shows a multi-layer network with three layers of computation  units.  This figure also illustrates  the  convention  used to  number  the layers of computation  units.  Consistent  with  this  num- bering scheme, an  '£-layer network'  denotes  a network  with  I  layers of computation units.  A feed-forward  network is said to be fully  connected between  adjacent  layers if it  contains all possible connections between  consecutive layers of computation  units, and all possible connections from  the input  units to  the  first  layer  of  computation  units.  For  our  purposes,  one  of  the computation units in the final  highest  layer is designated as an output unit.   More generally, there may be more than  one output  unit.   Associated with each computation unit is a fixed real function  known   76   The  VC-Dimension  of  Linear  Threshold  Networks  as  the  unit's  activation function.  Usually  we  shall  assume  that  this function  is  the  same  for  each  computation  unit   or  at  least  for  each computation  unit  other than  the output  unit .  We shall assume in this part of the book that the activation function of the output unit is binary- valued   so that  the  network  can be  used for  classification,  and  so that the  theory  of the  previous  chapters  applies .  The  functionality  of  the network  is determined  by these  activation  functions  and  by  a  number of adjustable  parameters,  known  as  the  weights and  thresholds.  Each connection  has a  weight—which  is simply some real  number—assigned to  it,  and  each  computation  unit  is  assigned  a  threshold  value,  again some real number.  All of the weights and thresholds together  constitute the  state of the network.  We shall usually use the symbol W  to denote the total number of weights and thresholds; thus, W  is the total number of  adjustable  parameters  in  the  network.   Recall  that  the  activation functions  are fixed.   The input  patterns  are applied  to the input  units.  If there are n  in- puts  then  each  input  pattern  is  some element  of  W1 and  the  network computes some function  on the domain W1.  The computation  units re- ceive and  transmit  along the relevant  connections of the network.  The action  of  a  computation  unit  may  be  described  as  follows.  First,  the inputs into the unit   some of which may be from  input  units and some from  computation  units   are  aggregated  by taking  their  weighted  sum according to the weights on the connections into the unit, and then sub- tracting  the  threshold.  Then  the  activation  function  of the  unit  takes this aggregation as its argument, the value computed  being the  output of the computation unit.  Explicitly, suppose that the computation units and  inputs  are labelled  with  the integers  1,2,..., fc, and that  the com- putation  unit  labelled  r  has  activation  function   r.  Suppose  that  this unit  receives inputs  z\, 22,..., z* from  d units, and that  the weights on the  corresponding  connections  are   respectively   tt>i, W2>      >w<j.  Then the output  of r  is  where 0 is the threshold assigned to unit  r.  A feed-forward  network is said to be a linear threshold network if each  activation function  is the step  function,  tt  \  f y  = mill   = {  0  othe;wise.  if 2  > 0     1      \    6.2  Upper bound   77  The  simplest  type  of  linear  threshold  network  is  the  perceptron,  dis- cussed  in  earlier  chapters.  Later  we  shall  look  at  sigmoid  networks, which make use of the activation function  f y   =  1  1 + e"y ,  and also at networks with piecewise-polynomial activation  functions.  6.2  Upper Bound  In this section  we present  upper  bounds on the  VC-dimension  of  feed- forward  linear  threshold  networks   by  which,  to  be  precise,  we mean the VC-dimension  of the class of functions  computed  by the network . We already  know one  such  result,  from  Chapter  3:  the  VC-dimension of a perceptron on n   real or binary   input  units is n + 1, which equals W,  the  total  number  of  weights  and  thresholds.  The  following  result gives a general upper  bound  on the  VC-dimension  of any  feed-forward linear threshold network, in terms of the total number W  of weights and thresholds.  Theorem 6.1 Suppose that N  is a feed-forward linear threshold network having a total of W  variable  weights  and thresholds,  and k computation units.  Let H  be the class of functions  computable  by N  on real  inputs. Then for m>W   the growth function  of H satisfies  W  and hence VCdim ff   <  2Wlog2 2Jfe ln2 .  Proof  Let  k  denote  the  number  of computation  units  in  the  network. Since the network is a feed-forward  network, we may label the compu- tation units with the integers 1,2,..., k so that  if the output  of compu- tation  unit  i  is fed  into unit  j  then  i  < j.  We shall bound  the  growth function  in an iterative manner, by considering in turn the action of each computation  unit.  Recall that by a state of the network we mean an assignment of weights to the  connections, and  thresholds  to the  computation  units.  Suppose now  that  S  is  any  set  of  m  input  patterns.  We  say  that  two  states u,u '  of N  compute different functions  on S  up to unit I if there is some input  pattern  x  in  5  such  that,  when  x  is  input,  the  output  of some computation unit labelled from  1 to I differs  in the two states.   In other words, if one has access to the signals transmitted  by units  1 to I only, then, using input patterns from 5, one can differentiate  between the two   78   The  VC-Dimension of Linear Threshold Networks  states.   We shall denote by Di S   the maximum  cardinality of a set of states that  compute different  functions  on S  up to unit I.  Note that  the number  of functions  computed  by N  on the set  5  is certainly  bounded above by  Dk S .   Two states  that  do not  compute  different  functions up  to  unit  k—the  output  unit—certainly  yield  the  same  function  on 5.   For   between  2 and fc, we let ul  denote the vector  of weights and thresholds at units 1,2,..., .  Thus ul  describes the state of the network up to  computation  unit  I.  Crucial to the proof  is the  observation  that the  output  of computation  unit  I depends only on the network  inputs, the outputs  of the  computation  units  that  feed  into Z, and the  weights and  thresholds  at  unit  L  To exploit  this,  we 'decompose' ul  into two parts, a;'""1 and Q.  The first  of these describes the state of the network up to unit  I -  1  and hence determines the outputs of the  computation units  1 to  I —  1 ,  while  the  second,  0,  denotes  the  threshold  on  unit    and  the  weights  on  the  connections  leading  into  I   from  input  units or  previous  computation  units .  Since  computation  unit  I is  a  linear threshold unit, the set of functions computable by that unit  in isolation  has VC-dimension  d ,  where di is the number  of parameters  associated with  unit      that  is, the  number  of  connections  terminating  at  I, plus one for its threshold .  Consider first computation unit 1. Two states compute different  func- tions up to unit  1 if and  only if they result  in different  outputs  at  unit 1.  Therefore,  the  number  of such mutually  different  states  is bounded simply by the  number  of dichotomies  achievable by the  perceptron de- termined  by unit  1, on the  sample  5.  The  perceptron  in  question  has VC-dimension  d\  and  so, by Theorem  3.7,  the  number  of  dichotomies is  no  more  than   em di dl,  since  m  >  W  >  d\.  In  other  words,  We now consider a unit   , where 2 <   <  k.  The decomposition  of  ul into a;'"1  and Q shows that  if two states compute different  functions  on 5  up to unit  Z, but  do not compute different  functions  up to unit  I — 1, then these states must be distinguished by the action of the unit  . Now, by Theorem 3.1, and the fact that I computes linear threshold  functions of di — 1 inputs, if T  is any set of m points of Rdl ~*, then the number of ways in which unit  I can classify  T, as the weight vector Q varies, is at most  {em diY1.  Therefore,  for  each of the  Di-i S   different  states  up to unit  I — 1, there are at  most   em dt dl  states that  compute  different functions  up to unit   .  Hence   6.2  Upper bound   79  It follows, by induction,  that  As  mentioned  earlier,  II f ra   is bounded  by the maximum  of  Dk S  over all 5 of cardinality m, so  6.1  implies  The expression  on the  right is reminiscent  of the  entropy of a discrete random variable.  We may write  — lnlljf ro   + In  —  W   v  '   \emj  ~~ frfW   <  V  -£ In  —  + In  —  \dij   \emj  noting that  ]C =i d\jW =  1.  Since di W > 0, we see that  the  bound can  indeed be expressed  as an entropy.  It is well known   and  easy to show,  using  the convexity  of the  logarithm  function   that  entropy is maximized  when the distribution is uniform,  that  is when di W =  l k for    =  1,2,..., fc. The  fact  that  di is restricted  to integer  values can only decrease the sum.  Hence,  Rearranging, we have  1  -lnn* m  + ln^-j<ln .   W\  n* m < 9r   .  To see that  this gives the required bound  on VCdim iJ , notice  that then 2m > IIJJ m , which implies VCdim   < if m > Wlog2 emk W   m.  Inequality   1.2  in Appendix  1 shows that  for any a,x > 0, lnx  < ax  — In a —  1, with  equality  only if ax =  1.  Applying  this  inequality with  x  =  emh W  and a  =  In2  2efc   shows  that  it  suffices  to take m = 2Wlog2 2fc ln2 .      80   The  VC-Dimension of Linear Threshold Networks  6.3  Lower  Bounds  The  following  result  shows  that  the  VC-dimension  of  two-layer  linear threshold  networks  is  bounded  below  by  a  quantity  of  order  W,  the total number of weights.  Theorem  6.2  Let N  be  a two-layer linear threshold network, fully con- nected between  adjacent layers, with n  > 3  input units,  k computation units in the first layer  and one output unit in the second layer . Suppose that k  < 2n+1  n2  + n + 2 .  Then the class H  of functions  computable by N  on binary inputs is such that  VCdim ff   > nk + 1 > 3W 5,  where W  =  nk  4- 2k + 1 is the total number of weights  and thresholds.  Proof  We prove the result  by constructing a shattered  set of size nk  + 1.  Recall  that  the  decision  boundary  of  a  linear  threshold  unit  is  a hyperplane,  so  that  for  all  points  on  one  side  of  the  hyperplane,  the unit outputs 0 and for all points on the other side, or on the hyperplane itself, it outputs  1. The idea of the proof is to choose appropriate values for the parameters of the network and then, for each of the k first layer units,  to  include  in  the  shattered  set  n  points  that  lie  on  its  decision boundary.  By adjusting  the parameters of a first layer unit  slightly, we can adjust  the classification  of each of the associated n points, without affecting  the classification  of the other points.  Now, a  three-packing  of {0, l } n is a subset T  of {0, l } n such that  for any two members of T, their Hamming distance   the number of entries on which they  differ   is at  least  three.  If we construct  a  three-packing in a greedy way, by iteratively  adding to T  some point  that  is at  least Hamming distance three from all points in T, each new point added  to the  packing  eliminates  no  more  than  N  =   £   +   ?   +  1 points  from consideration.  It follows that  some three-packing T  has  \T\  *  Jf  =  n* + n + 2'  So let T =  {*!, *2,..., t k}  be a three-packing  recall that k < 2n+1  ri2 + n +  2  .  For i  between  1 and fc, let  Si  be the  set  of points  in  {0, l } n whose Hamming distance from U is 1.   Thus, Si  consists of all n  points of  {0, l } n differing  from  U in exactly  one entry.   There is a single hy- perplane passing through every point of 5».   Without  loss of generality, suppose U is the all-0 element  of  {0, l } n,  then  Si  consists of all points   6.3  Lower bounds   81  with  exactly  one  entry  equal  to  1,  and  the  appropriate  hyperplane  is defined  by the equation x\  + X2 + ... + xn  =  1.   Furthermore, because T  is a three-packing  no two of these  k  hyperplanes  intersect  in  [0, l]n. Let  us set  the  weights and  thresholds  of the  computation  units  in  the first layer so that  the  decision  boundary  of the  zth unit  in the  layer is the  hyperplane  passing through  Si,  and  for  input  U the  output  of the unit  is 0.  Assign weight  1 to each connection into the output  unit,  and assign threshold  k to the output, so that  the output  of the network is 1 precisely when the output  of all units in the first layer is 1.  Since  the  points  of  S{  sit  on  the  hyperplanes  described  above,  the weights  and  threshold  corresponding  to  unit  i  may  be  perturbed—in other  words,  the  planes  moved  slightly—so  that  for  any  given  subset S^ of Si,  unit  i  outputs  0 on inputs  in  S^ and  1 on inputs  in  Si  — S^. Furthermore,  because  the  hyperplanes  do  not  intersect  in  the  region [0, l]n,  such perturbations  can be carried out  independently  for each of the  A;  units  in  the  first  layer.  The  network  can  therefore  achieve  any desired classification  of the points in 5  =  J*=1 Sj.  In other words, this set 5  is shattered.  Furthermore, by negating the weights and thresholds of the first layer units, and  changing the threshold  at  the output  unit  to  1, the network can still shatter the set 5 by perturbing the first layer parameters. How- ever, it  now classifies  each  U as  1, where before  they  were classified  as 0.  So the set 5 U {h}  is shattered, and hence VCdim J¥   > nk + 1.  The second inequality of the theorem follows from the fact that n  > 3,    which implies W  < nk + 2nfc 3 + 1<  5 nk + l  3.   The  lower  bound  just  given  is  linear  in  the  number  W  of  weights and thresholds, while the upper bounds of the previous section indicate that  the  VC-dimension  of  a  feed-forward  linear  threshold  network  is of order  at  most  Wlog2 W.  Moreover,  we have already  seen  that  the perceptron  has  VC-dimension  W,  so it  is natural  to  ask  whether  the bound of Theorem 6.1 is of the best possible order or whether one should be able to prove that in this case the VC-dimension is really of order  W. In other  words, can  it  be true  that  some feed-forward  linear  threshold networks  have  VC-dimension  significantly larger  than  W,  the  number of variable parameters?  The answer is 'yes'>  as shown by the  following results, which  we state  without  proof.   See the  Bibliographical  Notes section at the end of the  chapter.    82   The  VC-Dimension of Linear Threshold Networks  Theorem 6.3  Let W  be any positive integer greater than 32.  Then there is a three-layer feed-forward linear threshold network N\y  with at most W weights and thresholds,  for  which the following  holds.  If  H  is the  class of functions  computable  by Nw  on  binary inputs,  then VCdim if   >  1 132 Wlog2  fc 16 ,  where k  is the number of computation units.  Theorem  6.3 refers  to  networks  taking  binary  inputs.  It  is  perhaps surprising  that,  even  with  this  restriction,  a  network  may  have  a *su- perlinear' VC-dimension.  The result shows that  no upper bound  better than  order  W log2 k  can  be given:  to  within  a  constant,  the  bound of Theorem 6.1 is tight.  The networks of Theorem 6.3 have three layers.  The following  result shows that  there  are  two-layer  feed-forward  linear  threshold  networks having superlinear  VC-dimension  on  real inputs.  These networks have fewer layers—and hence in a sense are less complex—than those of Theo- rem 6.3, but the result concerns real inputs, not binary inputs and hence is not immediately comparable with Theorem 6.3. For the same reason, the result is not directly comparable with Theorem 6.2.  Theorem  6.4  Let N  be a two-layer feed-forward  linear threshold  net- work, fully  connected between adjacent layers,  having k  computation units and n > 3 inputs, where k < 2n 2~2.  Let H  be the set of functions computable  by N  on Rn.  Then  where W  =  nk -f 2k + 1 is the total number of weights  and thresholds.  We omit the proof.   See the Bibliographical Notes section at  the end of the chapter.   Theorem 6.4 should be compared to the upper bound of Theorem 6.1. The upper  and  lower bounds are within constant  factors of each other.  Notice  that  Theorems  6.3  and  6.4  show  that  there  are  certain  neu- ral  networks with  VC-dimension  growing at  least  as  WlogW.  Recall that  the upper  bound  of Theorem  6.1 applies to feed-forward  networks with an arbitrary number of layers.  By embedding two- and three-layer networks in a network of any fixed depth, it  is easy to show that  there is a sequence of networks of that  depth  with  VC-dimension  increasing as Wlog W.  However this does not  imply a similar  result  for  arbitrary architectures.  Given an arbitrary sequence of linear threshold  networks of fixed depth  with  increasing  W,  it  is  clear  that  the  VC-dimension   64  Sigmoid networks   83  cannot be forced  to grow as W log W  without  some constraints on how the  weights  are  distributed  among  the  layers.  A trivial  example  is  a three-layer  network  with  hi  units  in the first layer and  A?2  >  2*1  units in the second layer.  In this case, any weights associated with additional computation units in the second layer cannot lead to an increase in VC- dimension, since it is already possible to compute all boolean  functions of the k\  first  layer outputs.  However, in this case it  is known that  the VC-dimension is larger than some universal constant times W,  provided that &2 is smaller than a fixed exponential function of k\.  It is not known whether  this bound  can be improved without  a stronger  constraint  on the number of second layer units.  6.4  Sigmoid Networks  Feed-forward  sigmoid networks form an important  and much-used class of neural network.  In such networks, the output  unit has the step func- tion as its activation function,  but the activation function  of every other computation unit  is the standard sigmoid function,  cr, given by   W = 1+7=?-   A computation  unit  of this  type  is often  called  a  sigmoid unit.   The graph of the function  a may be found  in Chapter  1 as Figure 1.3.  Note that the standard sigmoid network just defined has a binary-valued out- put, in contrast to the two-layer real-output  sigmoid network discussed in Chapter 1.  The  sigmoid function  is, in  a sense, a  'smoothed-out'  version  of  the step function,  sgn, since a  maps from R  into the  interval   0,1   and  it has limits  lim  a a   =  0,   lim  a a   =  1.  As M  increases, the graph of the function  a i-t a Ma   becomes increas- ingly like that  of the linear threshold step function  sgn a .  The VC-dimension  upper  bound  results obtained  in this chapter  are specifically  for  linear threshold  networks and  cannot  be applied  to sig- moid networks.   We shall derive upper bounds on sigmoid networks in Chapter 8.   However, it is possible to use the lower bound results on the VC-dimension  of multi-layer  linear  threshold  networks to  obtain  lower bounds on the VC-dimension of multi-layer sigmoid networks, by means of the following  observation.   84   The  VC-Dimension of Linear Threshold Networks  Theorem  6.5  Suppose s  :  R  -*  R  satisfies lima-+oo s a   =  1 and lima-*_oo s a   =  0.  Let N  be a feed-forward  linear threshold  network, and N1  a network with the same structure as N,  but with the  threshold activation functions  replaced  by  the  activation function  s  in  all non- output computation units.  Suppose  that S  is any finite set of input pat- terns.  Then, any function  computable  by N  on S  is also computable  by N1.  It  is easy to  see that  the  limits  1 and  0 can be replaced  by any two  distinct  numbers.  Proof  Consider  a function  h  computable by  N  on 5.  Label the  com- putation  units with  integers  1,2,..., k in such a way that  unit  j  takes input from unit i only if i  < j,   and so that unit A; is the output unit.  Let V{ x  denote  the  net  input  to  computation  unit  i  in  response to  input pattern  x  G S.   That  is, if unit  i  has input  vector  2, weight  vector  w, and  threshold  wo, Vi x  =  wTz  4- wo.   The proof  uses the fact  that  we can multiply the argument of s -  by a large constant and, provided the argument  is not  zero, the resulting function  accurately  approximates  a threshold  function.  First, define c = minjmina:Gs fi x .  Suppose that  e > 0.   Otherwise we can change the thresholds to ensure this, while keeping the  function computed  on  S  unchanged.   Now,  we step  through  the  network,  re- placing each threshold  activation  function  v  H> sgn v   by the  function v  H> s Mv ,  where M  is a positive real number.  Let V^M0*0  denote the net input to computation unit i in response to x G 5 when the activation functions  of units  1,..., i -  1 have been changed in this way. Since 5  is finite and e > 0, the limiting property of s implies that  lim  max\s Mvi x    -  sgn vi x   = 0. M—+00  xQ.S  Since the  net  input  to  a  computation  unit  is a  continuous  function  of the outputs of previous units, this implies that  lim  maxv2,M ^  —1*2 0*01 =  0,  and so  lim  max\s{Mv2,M x    -  sgn v2 a0  = 0.  Proceeding in this way, we conclude that  lim  max \vkMx   M—too  ajfco  -  *>* *  I  = 0>   6.5  Bibliographical notes   85  which shows that,  for sufficiently  large  M,     =  h x   for  all  x  e  5.  Now,  by  scaling  the  weights  and  thresholds  by  M  and replacing the  activation  function  v  H> S{MV   by the  function  v  H* S V , we see that  the function  h on S  is computable  by N'.  D  It  follows  immediately  that  any  set  of input  patterns  shattered  by  a network of linear threshold units is also shattered by a network of units each with an activation function s of the type described.  Hence the lower bound results Theorem 6.2, Theorem 6.3 and Theorem 6.4 also hold for such networks, and in particular for standard sigmoid  networks.  6.5  Bibliographical  Notes  The proof of the upper bound of Theorem 6.1 is due to Baum and Haus- sler  1989 .   For more on properties of the entropy function,  which were used  in that  proof,  see, for example,   Cover  and  Thomas,  1991 .   This result  was  originally  due  to  Cover   1968 .  A  lower  bound  on  the  VC- dimension of two-layer networks that  is linear in the number of  weights was also presented  in   Baum  and Haussler,  1989 .  Theorem 6.2  gives  a slight  improvement   by  a constant  factor   of this  result,  with  a simpler proof; see  Bartlett, 1993a .  The corresponding result for real inputs  re- lying  on  the  inputs  being  in  general  position,  which  is  not  the  case  for binary inputs  appears in  Baum, 1988 , using a technique that appeared in   Nilsson,  1965 .  Lower bounds for  networks with  binary  weights  are given in  Ji and Psaltis, 1991 .  Theorem 6.3 is due to Maass  1994 , and Theorem  6.4  is  due  to  Sakurai   1993 .  General  lower  bounds  for  any smoothly parameterized function  class are given in  Erlich, Chazan, Pe- track and Levy,  1997    see also   Lee, Bartlett  and Williamson,  1995a  . The Q W   bound for arbitrary three-layer linear threshold networks with not  too  many  computation  units  in  the  second  layer  was  presented  in  Bartlett,  1993a; Bartlett,  1993b .  The fact that lower bounds for linear threshold  networks imply  lower bounds  for sigmoid  networks is proved, for example, in   Sontag,  1992; Koiran and Sontag,  1997 .   Bounding the VC-Dimension  using  Geometric  Techniques  7.1  Introduction  Results in the previous chapter show that the VC-dimension of the class of functions computed by a network of linear threshold units with W  pa- rameters is no larger than a constant times W log W.  These results can- not immediately be extended to networks of sigmoid units  with continu- ous activation functions , since the proofs involve counting the number of distinct outputs of all linear threshold units in the network as the input varies over m patterns, and a single sigmoid unit has an infinite number of output  values.  In this chapter and the next we derive bounds on the VC-dimension of certain sigmoid networks, including networks of units having  the  standard  sigmoid  activation  function  a a   =  1  1  +  e~a . Before  we begin  this  derivation,  we study  an example that  shows  that the form  of the activation function  is crucial.  7.2  The  Need  for  Conditions  on  the  Activation  Functions  One might  suspect  that  if we construct  networks of sigmoid units with a well-behaved  activation  function,  they  will have finite VC-dimension. For instance, perhaps it  suffices  if the activation  function  is  sufficiently smooth, bounded, and monotonically increasing.  Unfortunately,  the sit- uation  is  not  so  simple.  The  following  result  shows  that  there  is  an activation  function  that  has  all  of  these  properties,  and  even  has  its derivative  monotonically  increasing  to  the  left  of  zero  and  decreasing to  the  right   so it  is convex and  concave in  those  regions , and  yet  is such that  a two-layer network having only two computation units in the first layer, each with this activation function,  has infinite VC-dimension. What  is more, the  activation function  can be made arbitrarily  close to  86   7.2  The  need for  conditions  on  the  activation  functions   87   b   s x    ^^  a x   0.88  0.86  0.84  0.82  0.8  0.78  0.76  0.74  0.72  i.i   1^  Fig.  7.1.  The  graphs  of  the  functions  s -    defined  in  Equation   7.1 ,  with c =  0.05  and the  standard sigmoid  a -    defined  in Equation   1.1  ,   a   in  the interval  [-10,10]  and   b   in  the  interval  [1,2].  the  standard  sigmoid,  a a   =  1  1  + e  a .  Clearly, then, finiteness of the VC-dimension of neural networks depends on more than simply the smoothness of the activation  function.  Theorem  7.1 Define  s x   =  +  cx3e   x  sin a:   7.1   for  c > 0.  Then  s -   is  analytic,  and for  any  sufficiently  small  c > 0,  we have  lim  s x   =  1,  3»OO  lim  s x   = 0, x->-  oo  <  0  >0   ifx>0 ifx<0.  Let N  be  a two-layer network with one real input, two first-layer compu- tation units using this activation function,  and one output unit,  so that functions  in HN  are of the form  X H-» Sgn   WQ  with x,Wo1wi>W2,ai,a2  G  E.  Then VCdim iljv   =  oo.  Figure  7.1 compares the graphs of s -   and the standard sigmoid. The proof of Theorem  7.1 relies on the following  lemma.   88   Bounding  the  VC-Dimension  using  Geometric  Techniques  Lemma  7.2  The  class  F  =  {x   -> sgn sin ax    : a €  R+}  of  functions defined on N  has VCdim F   =  oo.  Proof  For any d  G  N,  choose  Xi =  2*"1 for i  =  1 , . . ., d.  We shall  show that  the set  {x\,...  ,<*} is shattered  by F,  and since d is arbitrary  this establishes  that  VCdim F   =  oo.  For   bu...,  bd   €  {0, l }d, let   so  the  binary  representation  of  c  is  0   6162       W «  Then  by  setting a  =  2?rc,  we  can  use  the  function  sgn sin oxi    to  extract  the  bits bi from c.  That  is,  sgn sin axi    =   sgn  [ sin  [ 2TT [ V  2~jbj  +  2"  d + 1   fi-i  =   sgn j sin  and  the  argument  of  sin »   in  the  last  expression  lies  strictly  between hit  and   bi +  1 TT,  SO sgn sin ai    =  1 — bi.  Since  this  is  true  for  any i  =  l , . . . , d,  and  any  choice  of  the  6j,  it  follows  that  {a?i,...,£d}  is   shattered.   It is now easy to verify  the claims made in Theorem  7.1.  The proper- ties of s '   are easy to check  although checking the properties of convex- ity  to  the  left  of zero and concavity  to  the  right  of zero is tedious .  To show  that  the  class  HN  has infinite  VC-dimension,  we use the  network N  to  compute  sgn sin ox  .  Specifically,  for  a  €  M, set  the  weights  so that  the network  computes sgn  io x  ,  where  ha x   =  s ax   +  s -ax    -  1 =  2c ax 3e" 2x2 sin ax .  For a  >  0  and x  >  0,  sgn ho x    =  sgn sin aa;  ,  so Lemma  7.2  implies that VCdim HN   = 00.   7.3  A  bound on the growth function   89  7.3  A Bound on the Growth Function  In  the  remainder  of this  chapter,  we consider  classes of  binary-valued functions that are obtained from parameterized real-valued functions by 'thresholding'.  Classes defined  in this  way include the  perceptron  and the class of functions  computed  by thresholding the output  of a multi- layer  network  of  units  having  either  the  standard  sigmoid  activation function or a piecewise-polynomial activation function.  In this definition, and  in  the  remainder  of this  chapter,  we assume  that  there  are d real parameters; we use a to denote the vector of these parameters.  Definition  7.3  Let H  be a class  of {0,1}-valued functions  defined on a set X,  and F  a class of real-valued functions  defined onRd  x X.  We say that H  is  a fc-combination of sgn F   if  there is  a boolean function g : {0,1}* -4 {0,1} and functions   i, ..., *  in F  so that for  all h  in H there is a parameter vector a G ld  such that  h x   =  p sgn  i  a, &  ,..., sgn  * a,  x     for  all x  in  X.  We say  that  a function  f  in F  is continuous  in  its  parameters   Cp  in its parameters^   if, for  all x  in X,  f{-,x   is continuous   respectively, &> .  In this chapter we develop a technique for bounding the growth func- tion  of  a  class  H  of  functions  expressible  as  boolean  combinations of parameterized  real-valued  functions  in  this  way.  Theorem  7.6  below provides a  bound  in terms  of the  number  of connected  components of the  solution  set  in parameter  space of certain  systems of equations in- volving the real-valued functions that define H.   Recall that a connected component of a subset S of Rd  is a maximal nonempty subset A C 5  for which there is a continuous curve connecting any two points in A.   We can think  of this as a generalization of the notion of the number of so- lutions of a system of equations.  It turns out that we need only concern ourselves with systems of equations that are not degenerate in the follow- ing sense.   Here, for a function     : Rd  -> if,  if f a   =    i a ,...,    a  , then the Jacobian of    at a E Rd, denoted  ' a , is the d x I matrix with entry  i, j  equal to  Difj a ,  the  partial  derivative of fj a   with  respect to the ith  component of a =   a\,...,  a^ .   t  that is, the first p derivatives of    are defined  and are continuous functions.   90   Bounding  the  VC-Dimension  using  Geometric  Techniques  Fig. 7.2.  An example illustrating Definition  7.4.  The set { i, 2, 3}  does not have regular zero-set intersections, since the Jacobian of the function   i,  i  : R2->R*  has rank 1 at a*.  Definition  7.4  A  set  {   i , . . . ,   *}  of  differentiate  functions  mapping from  Rd  to  E  is  said  to  have  regular  zero-set  intersections  if, for all nonempty  subsets {»i,... ,i } C {l,...,fc},  theJacobianof fa,...yfa   : Erf  -> IRf   ms ran ; I at every  point  a  of the solution  set  This  definition  forbids  degenerate  intersections  of the zero-sets  of the functions.  For instance,  if two zero-sets  'touch' at a point,  so that the hyperplanes tangential  to them at that  point  coincide, the functions do not  have regular zero-set intersections   see Figure 7.2 . More generally, when the zero-sets of more than two functions  intersect  at a point and the  intersection  of  the  tangent  hyperplanes  at  that  point  has higher dimension  than  expected,  the  functions  do not  have  regular  zero-set intersections.  The main result of this chapter gives a growth function bound in terms of a solution  set components  bound. As in Chapter 3, we use the notation CC A   to denote the number of connected components of a set A C R*.   7.5  A  bound on the growth function   91 Definition  7.5  Let G  be a set  of real-valued functions  defined on  Rd. We say that G has solution set components bound B  if for any 1 <  k < d and any { i,..., jk}CC  that has regular zero-set intersections,  we have  Notice that  the  intersection  of any  k  > d zero-sets of functions  with regular  zero-set  intersections  must  be  empty   otherwise  the  rank  con- dition  in  Definition  7.4  could  not  be  satisfied .  Hence  we  need  only consider k < d in the definition  of the solution set components bound.  We shall always be concerned with  classes F  of real-valued  functions defined on Rd  x X,  and with the solution set components bound for the class G =  {ai4    a, x   :     G  F, x  G X}.  We say that  F  has solution set components bound  B  when this is the  case for  the corresponding class G.  Furthermore, we say that  F  is closed under addition of constants if, for any c G  R, whenever    G F, the function   a,x    *-> f a,x   + c is also i n F.  With these definitions, we can present the main theorem of this chap-  ter.  Theorem  7.6  Suppose that F  is a class  of real-valued functions defined on Rd  x X,  and that H  is  a k-combination o sgn F .  If  F  is  closed under addition of constants, has solution set components  bound B,  and functions  in F  are Cd  in their parameters,  then  for m  >  d k.  As an example, suppose H  is the class of functions  computed  by the simple perceptron on Rd.  Then the parameter space is Rd+1  and we can define F  as the class of functions     satisfying  d    a, x   =  ^2   xiai  + ao + c>  for some c in R, where a =   ao,ai,..  ,a<f .   We include the  redundant constant c so that F  is closed under addition of constants.   In this case, F  has solution set  components bound  B  =  1.  Also, functions  in F  are   92   Bounding the  VC-Dimension using Geometric  Techniques  C   in their parameters and H  is a 1-combination  of sgn F , so  Notice that  E  7   -  i=0  i=0  J   =  2§   ,-i  so  this  bound  is  larger  than  the  correct  value   see  Theorem  3.1   by  7+11 -  However,  the  bound  on  the  VC-dimension  that  is  implied  by Theorem  7.6 is tight.  7.4  Proof of the Growth Function Bound  The remainder  of this  chapter  is more technical than  most other  parts of the book.  The reader who is happy to accept  Theorem  7.6 on  trust can proceed  to  Chapter  8  perhaps after  reading the first paragraph of Section 7.5 , without  suffering  much loss of continuity.  Growth function  and connected components in parameter  space  Recall that in the proof of the growth function bound for the perceptron, we first related the number of dichotomies of a set of input points x» to the  number  of cells in the  partition  of the  parameter  space defined  by the equations wTX{ — 0 =  0  Lemma 3.2 .  The following lemma shows that we can do this more generally, for any class that is a  fc-combination of thresholded  real-valued functions.  In this case, we relate the growth function  to the number of connected  components of the complement of certain  zero-sets  of  functions  that  have  regular  zero-set  intersections.  For the simple perceptron, this is equivalent  to the condition  that  the examples are in general position.   We first need the following lemma, which shows that  almost all shifts of a set of functions  result in a set that has regular zero-set intersections. We use this to show that we can always perturb the problem to give a new collection of zero-sets that have regular intersections, without decreasing the number of dichotomies.   74  Proof of the growth function  bound   93 Lemma 7.7  Given a set { i,...,  *} ofCd  functions  that map from Rd to R,  the set  5  =  {A G R* : { i -  Ai,..., fk  -  A*} does no*  lave  regular zero-set intersections]  has measure^ 0.  Proo   Consider  a subset  A of { i, . . . ,   * };  without  loss of generality suppose it is { i,... 9 j}.  If we define    : Rd -> Hf  as    =    i,...,   , Sard's Theorem   see Appendix  1  implies that  the set  5A  = {2  G H? : 3x e Rd s.t.   x  = y and rank ' re   <  }  has  measure  0.  Let TA =   Rf -  5A   X B*""1.  Clearly,  the comple- ment  of TA has measure  0.  We can construct  the corresponding set TA  C R* of 'regular  values' for any  subset  A.  It is easy to see  that, if we choose A from  the intersection  over all subsets A of the  TA,  then { 1 ~ Ai,..., fk  -  A*} has  regular  zero-set  intersections, so 5  C R*  - f]A  TA-  But  we can write  which is a finite union of measure 0 sets, and  hence has measure 0. So 5  has measure 0.     Lemma 7.8  Let F be a class of real-valued functions  defined on Rd x X that is closed  under addition of constants.  Suppose that  the functions in F  are continuous in their parameters and let H  be a  k-combination of sgn F C  Then for  some functions  i ,..., *  in F and some  examples a?i,...,xm  in X,  the set  {a K> U{a, Xj  : i = 1,..., fc, j  = 1,..., m}  has regular  zero-set intersections and the number of connected  compo- nents of the set  is at least  t  See Section A1.3.   94   Bounding the VC-Dimension using Geometric  Techniques  Proof  Since  if  is  a fc-combination of  sgn F ,  we can  fix  functions   l , . . . ,  *  in  F  and g  :  {0,l}k  ->  {0,1} that  give the  parameterized representation  h{x  = s sgn  i o,a?  ,... ,sgn  * a,z      for functions  h in H.  Fix arbitrary x\,...,  xm  in X.  For each dichotomy computed by some function  h in iJ, there must be a corresponding a in Rd  satisfying  h xj   = g  sgn  i a, z,  ,..., sgn  * a, Xj     for j  =  1,..., m.  We want to relate the number of these dichotomies to the number of connected  components of a certain set in the parameter space.  To this  end,  consider  the  zero-sets  in  parameter  space  of the functions  a i-»  fi{a,Xj :  for  j  =  1,2,..., m  and i  =  1,2,..., ?.  These  sets  split  the parameter space into a number of cells, each of which is a connected component of the set  S = Rd -\J\J   {a6Rd  : fi a,Xj   = 0} .    7.2   t=ij=i  Figure 7.3 shows an example of the cells defined by these zero-sets, with k = m = 2.  If two parameters a\  and a^ in 5  give distinct  dichotomies of the set {xi,...,zm},  then  a\  and 02 lie in  distinct  cells  of 5.   This  is  true because  if a\  and a2  give  distinct  dichotomies,  there  must  be some i and j  such that  one of  i ai,rcj   and fi a2,Xj   is positive and the other negative. Then the continuity of  » with respect to its parameters implies that,  for any continuous  curve  connecting  a\  and 02, there  must  be a point on that  curve where fi a,Xj   = 0.   It is possible that  we may be forced to consider parameters that lie on one of the zero-sets.  In the case of the perceptron, we could adjust  the offset  parameter 6 to ensure that  any dichotomy can be computed with parameters that do not lie on any boundary set  where wTX{ — 0 = 0 for some Xi , so we needed only to count the number of  n + l -dimensional cells in parameter space. In the more general case we consider here, there might  be dichotomies  that  can  only be computed  by parameters  lying on some zero-set.  In this  case, we perturb  the zero-sets by considering fi a,Xj   -  Xij  =  0 for some small  Xij,  instead  of fi{a,Xj   =  0.  This   7.4  Proof  of  the  growth function  bound  95  Fig. 7.3. The connected components of the set S of Equation  7.2 .  will ensure that  dichotomies that  previously could only be computed by parameters  lying on the  zero-set  can  be  computed  by  parameters  that lie strictly  inside a distinct  cell in the  new   perturbed   partition  of the parameter space.  For  the  example  illustrated  in  Figure  7.3, the  zero-sets  of   i a, i ,  2 a,i ,  and   2 a, 2   intersect  at  a  single  point.  Suppose  that  the signs  of  these  functions  are  such  that  the  point  a*  shown  in  the fig- ure  satisfies   i a*,i   >  0,   2 a*,i   <  0,  and   2 a*,£2   <  0.  Then the only  parameter  for  which  we have sgn  i -,xi    =  sgn  2 -,xi    = sgn  2 -,z2    =  1—which corresponds  to   i a,zi   >  0,   2 a,zi   >  0, and   2 a,^2   >  0—is the intersection  point  of the  three zero-sets.  Fig- ure  7.4 shows the  situation  when  we replace   2 a, x\   by   2 a,i   -+  e; the shaded region in the figure marks parameters that  do not lie on any of the zero-sets, but  ensure that  the three functions  are nonnegative.  Now, suppose that  H\{   xm}  =  N,  and  choose parameter  vectors a\,...,  ax  from  Rd  so that  for  each  distinct  dichotomy  there  is a cor- responding  a .   Some of the  a   might  lie in  the  zero-set  of one  of  the   96   Bounding  the  VC-Dimension  using  Geometric  Techniques  Fig.  7.4.  A perturbed version of the arrangement  of zero-sets in Figure 7.3. The shaded region is a new connected component that results from the per- turbation.  functions  f% ',Xj .   Choose e strictly  between  0 and  min {1 4 01,^ 1  : fifaxj    <  0,  1 <  i  <  *,1  <  j  <  m,  1 <     <  N}   and  choose  any  e  >  0  if  this  set  is  empty .  Then  for  any  sequence  Ai,i,...,\k,m   from   0,e * m,  consider the  sets  for i  =  1 , . . ., k and j  =  1 , . . ., m,  and the complement  of their union,  k  m  R = Rd  -   J  J  t=i   i=i  xj   =  -A,,,-}.  Clearly, the  choice of e implies that  all of the  aj's lie in R.  In fact,  each a\  must  lie in  a distinct  connected  component  of  R.  To see this,  notice that since a\  and a^ give rise to distinct dichotomies, there is some i and j  such that  sgn  i ai,x J    ^  sgn fi a2,Xj  .  Without  loss of generality, assume  fi ai,Xj   >  0  and  fi a,2,Xj     -\ij   74  Proof of the growth function  bound   97  and,  by the  choice of e, fi a>2,Xj   < —e  < —\ij,  which implies  a\  and a2 are in distinct  connected components of  R.  It follows that,  whatever the choice of the Af j   subject  to 0 <  Xij  < e ,  for  each  dichotomy  of  {xi,...,xm}  there  corresponds  at  least  one distinct  connected component  of  R.  By Lemma  7.7, we can  choose suitable  values of  Aifi,..., Xkym  such  that  the set of  functions  { »  > Xj  =   *  > Xj  -  Aij  : i =  1,..., fc, j  =  1,...,  mj  both has regular zero-set intersections and  satisfies     km  ** - U U {   This is because the set of suitable Xij  contains the intersection of a set of positive measure with the complement  of a set  of zero measure, and so is nonempty.  The  functions  fi  are  in  F  because  it  is closed  under   addition of constants.  The result follows.   Bounding  the  number  of  connected components:  the regular  case  In  the  proof  of the  growth  function  bound  for  the  simple  perceptron, we used  an  inductive  argument  to  count  the  number  of cells in  an  ar- rangement  of hyperplanes   Lemma 3.3 .  In this section, we use a very similar inductive argument to give a bound on the number of connected components of the set described in Lemma 7.8, in terms of the number of  connected  components  of  the  solution  set  of  a  system  of  equations involving the functions   i -,Xj .   In the case of the simple  perceptron, the number  of connected  components of the  solution  set  is never more than  one.   In  this lemma,  and  in what  follows,  we use the  convention that  ni €0 Si =  Rd  for subsets Si C  Rd.  Lemma 7.9 Let { i,..., fk}  be a set of differentiable functions that map from Rd  to R,  with regular zero-set intersections. For each i,  define Zi to be the zero-set of fi:  Zi =  {a € Rd  :  » a  =  0}.  Then  cc i®>-\Jz <   98   Bounding the VC-Dimension using  Geometric  Techniques  The  proof  requires  two  lemmas.  The  first  shows  that  if  we take  a connected  component  of  a  set  defined  by  some  of  the  zero-sets,  and remove a  connected  component  of its  intersection  with  another  of  the zero-sets,  the  set  is split  into  no  more  than  two  pieces.  We omit  the proof, which uses ideas from  point  set topology.   See the  Bibliographic Notes section.   Lemma  7.10  Define a set  of functions  { i,..., fc}  a$  tn  Lemma  7.9, and define sets 5i,...,S*_i  so  that fori  =  1,..., k -  1,  either Si  = {a  6  Rd  : fi a   =  0}  or S{  =  {a  G Rd  : fi a   ^  0}.  Let C  be a and let C" be a connected component of connected component ofn^Si,  CD {a 6 Rd  :  fc a  = 0}.  Then C — C*  has no more than two  connected components.  The second lemma we need gives a result  analogous to the induction step in the  argument  used to bound  the growth function  of the  simple perceptron.  Lemma  7.11  Define a set  of functions  {   I , . . . ,   *}  and the  zero-sets as  in  Lemma  7.9.  Let  I  C  {l,...,fc}  and  define M  = Zi,...,Zk  f ieIZi.  Define b  =  jfc -     and let  {Mu...,Mb}  =  {Z{  : i  £    }. Then  <CC   M-  [JMj ] +CC [MnM6-  J M,  6 -1  Figure  7.5  illustrates  the  case  in  which  M  =  R3   7  =  0 ,  and  Mi, M2,  and  Ms  are  planes.  Here,  M  —  Uj=i ^i  consists  of  eight  cells, bounded  by the  three  planes,  M  -   J^=1 Mj  consists of four  cells, and M  fl Ms — Uj=i -Wj consists of four  cells, bounded  by the bold lines in the figure.  Compare Figure 7.5 with Figure 3.2.   Proof  Let  5  =  M  -   jjlj  Mj-  Suppose  that  CC 5   =  iV.  We wish to  show that  removing  Mb from  5  increases  the  number  of  connected components from N  to no more than  iV + CC  Let  {Ci,..., CN}  be the  connected  components of 5.  Consider one of these components,  Cj.  Let  A\  be a connected  component  of Cj f! M\>.   7.4  Proof  of the growth function  bound  99  Mi  Fig.  7.5.  An  example  of  zero-sets  Mi,  M2,  and  M3  in  M  =  R3.   See Lemma  7.11.   The  intersections  of  Ms   =  M 0  M3   with  M\  and  M2 are shown as bold lines.   See Figure 7.6.   By Lemma 7.10  with C  =  Cj  and C" =  Ai ,  removing Ai  splits  Cj  into  no  more  than  two  connected  components.  Consider a  second  component  A2  of  Cj  fl  M&.  Since  it  is  disjoint  from  A\,  it lies  entirely  within  one  connected  component  of  Cj  —  A\.  We  have established  that  Cj  — A\  has at  most  two components.  Let  us  suppose that  its  components  are  Di  and  D2   where  we  take  D\  =  0  if  in  fact Cj  — A\  has just  one  component ,  and  choose the  labels  of  D\  and D2 so that  A2  n Di  =  0.   See  Figure  7.6.   By  Lemma  7.10   with  C  =  D2 and C" =  A2 ,  CC Z?2 -  ^2   <  2, and  so  U  D2  -  A2    CC £>2 -  A2   <  3.  Continuing in this way, considering in turn further  components  of Cj  fl Mb, we obtain  -  Mb   <  CC Cj  H Mb    100  Bounding the  VC-Dimension using Geometric  Techniques  Fig.  7.6.  A  connected  component  Cj  of  5,  as  described  in  the  proof  of Lemma 7.11.  The bold lines, A\  and A2, are connected components of Cj  M The  shaded  regions,  D\  and  D2,  are the  two components  of  Cj  -  A\.  Hence, writing  5  as a  disjoint   and  disconnected   union  of the  Cj,  we have  CC  =  C C   S - M6   =  CC    j CrM6     N  AT  =  cc  1   which is what  we require.   74  Proof of the growth function  bound   101  Proof   of Lemma 7.9  We  shall  prove  that  the following  proposition,   P& , is true  for any b e  N U {0}.  The lemma will follow from the case b = k.   Pa   Suppose  A; >  6.  Define  functions  { i,..., fc}  and  their  zero- sets  Zi,..., Zk as in Lemma 7.9.  Let    C {1,..., k} such  that    =  k -  6.  Define  M  =  f]ieI  Z{   and recall  that  we define  i = Rd , and let {Mi,..., M b}  = {Z{ : i <? I}.  Then  SC{1,...,6}  where,  if  b =  0,  the  right-hand  side  is to  be  interpreted  as CC Af .  Clearly,   Po  is true, since it states  that  CC M  < CC M  for M  = nt=i %i-  Suppose  that   Pb  is true,  and consider  k  > b +  1 and    C {1,..., A;}  with    = k -   b + 1 . Let M = fi€   2i, and define the zero sets {Mi,... ,M&+i} =  {Ziii  &   }.   Then Lemma 7.11 and  Pb  imply that      6+i   \  CC I M -  J  Mi  1  Notice that the first sum in this expression includes all subsets of the set {1,..., b + 1} that  do not contain 6 + 1,  and the second can be written as a sum of the same form over all subsets that  contain 6 -fl.  It follows that  cc   M -   J Mi  <  \   i=l      and hence   Pfc+i   is true.       102  Bounding the VC-Dimension using Geometric  Techniques  Proof  of  Theorem  7.6  The proof of Theorem 7.6 is now immediate.  The first inequality follows from  Lemmas 7.8, and 7.9, taking the zero-sets Z{ in Lemma 7.9 to be those of the mk functions  a H->  ^ a, Xj  defined  on the parameter  space Rd, for i = 1,2,..., k and j  = 1,2,...,m.  This gives the bound  ILH m   <  where the maximum is over the zero-sets Z* that  have regular intersec- tions, and the second inequality follows from the definition of the solution set components bound and the fact  that the intersection of more than d such zero-sets is always empty.  The second inequality of Theorem 7.6 follows from Theorem 3.7, pro-  vided m > d k.  7.5  More  on Solution  Set Components  Bounds  In finding a solution set components bound for a set G of functions,  it is often  convenient  to express the functions  in G as functions  of addi- tional variables representing the result of intermediate calculations. For example, if,  while computing    a , we calculate  &i, then  62> and so on up to frn, and we use these to calculate   a , then we can write    in the form  where each  bi is a function  only of a and &i,..., 6i—i-  The theorem in this section shows that  we can consider the variables 61,..., bn as extra parameters, and that  we can obtain  a solution  set components  bound for  G from  a bound for the class of functions  involving these extra pa- rameters.  The situation is, however, not quite so simple:  in calculating solution set components bounds for a function  class, we need to consider simultaneously k < d functions     of this form, and we must expect each to compute its own independent  intermediate variables bi. Hence, using n intermediate variables to compute each function     corresponds to the addition  of dn new parameters,  n  for each of the d functions.   Typi- cally, more parameters corresponds to a larger solution set components bound.   To simplify  notation in the theorem and proof, when we write   7.5 More on solution set components  bounds   103  a function  of these dn + d arguments as a function  of fewer  arguments, it indicates that  the function  does not depend on the other  arguments. First,  we formalize  the  notion  of using  additional,  or  'intermediate', variables.  In this definition,  an intermediate  calculation  is expressed  as b =   j>{a   for some function   . To ensure that  the intermediate variables take the appropriate values, we use the trick of defining  a function  g for which the constraint b =  0 a  is satisfied  when g a, b  = 0.  This implicit definition  of the intermediate  variables is convenient, since we are only interested in the behaviour of functions  around their zero-sets.  Definition  7.12  For a set G  of differentiable  real-valued functions de- fined  on Rd  and a set G  of differentiable  real-valued functions defined on Kd n+1 ,  we say that G computes G with n intermediate variables  if, for  any 1 <  k < d  and { i,..., fk}  Q G,  there is a set  {fl,9lA,--,9l,n,---,fk,9kA,--,9kynj   Q G  that satisfies the following  conditions.   i   For i =  1,..., kj  there are differentiable functions  iyn  :  can be  written   i,j a, b   =   i,j a, 6i,i,..., 6ij-i   for  j  =  2 , . . . ,n  where a e  Rd,  and b =    &M, . . ., &i,n,..., bdyn   G Rdn.  tion    >ij  defines  the  intermediate  variable  &ij,  and  the  faj  are ordered so  that  their  values  depend  only  on  previously  computed intermediate  variables.    The  func-   ii   For  i  =  l,...,fc,   the  functions  0t,i,...,ff»,n   can  be  written  as gij a,b   =  gitj a,bitu...,bitj   l , . . . , n.  That  is,  the  function  gij  depends  only  on  previously  computed intermediate  variables,  and  on  bij.   for  all  a,  6,  and  j  =    iii   For i  =  1 , . . ., fc, and  I =  1,...,n,  if  bij  =   ij a, b  for  all a,  6,  and j  <l,   then  for  all a  and  b we  have  9iAa>b   =  0  if  and  only  ifb^i  =  0t,  a,6   and  Dbitl9i,i   a, &,i a, 6   , . . ., &,, a, 6   ^  0.   That  is,  the function  gij  implicitly  defines  the  intermediate  vari- able bij.  The  derivative  condition  ensures  that  the  zero-set  in- tersections  remain  regular.    104  Bounding  the  VC-Dimension  using  Geometric Techniques   iv   For  all a  e  Rd  and b e  Rdn,  if  bij  =   ij{a,b   for  i  =  1,...,k  and j  =  l , . . . ,n   then  Theorem  7.13  For function  classes G  and G  and a positive  integer  n, if  G  computes  G  with  n  intermediate  variables,  then  any  solution  set components  bound for  G  is  also a solution  set  components bound for  G.  The  following  lemma  will  be  used  in  the  inductive  proof  of  Theo- rem  7.13.  It  shows that  adding  a single intermediate  variable  preserves both  the  number  of  connected  components  and  the  regularity  of  the zero-set  intersections.  As explained  above,  we specify  the  value of  the intermediate  variable b €  E  implicitly  through  the equation   g{a>b   =  0, which implies b =   j  a   in the  zero-set  intersection.  Lemma  7.14  Suppose   f u . . .,   fk  : Ed  -> E,  f u . . .,   fk  : E d +1  -» R,  and  : Ed  -> R  are differentiate  functions  satisfying  fi a   =   i a,0 a    for all a  G  Erf.  Let g : E d +1  ->  E  be a differentiate  function  such  that,  for allaeRd,  g a, b   =  0   if  and only if   b = <£ a ,  and  Define  k  Z  =  Then  we have:  Z  =  { a,b :g a,b  = 0}nf]{ a,b :fi a,b   =  0}.   i   CC Z   =  CC Z ,  and  ii   for a €  Z,  the Jacobian of    i,...,  fk   at a has rank k  if and only  if  the  Jacobian of     i , . . ., fk, g   at  a,   a    has rank k + 1.  Proof 1.  The  transformation  a ^   a,  a    is a  one-to-one function  mapping   7.5  More on solution set components  bounds   105  from  Z to Z.  Clearly,  this  transformation  preserves connectedness, so CC Z  = CC Z . 2.  Define    =   i, ..., *   and    =   i,...,  * .  Fix a point   a,   a    in Z.  In what  follows, we consider the Jacobians of    and   , g  at a and  a, l> a    respectively, but to simplify  notation we write these as  ' and   , ',  dropping the a and   a,  {a  .  We can write the Jacobian of   , g  as  Let p  J   denote the rank of a matrix J.  Then, since Dbg ^  0 and  >9 ={Dbf   Dbg '     h   0\  is a full  rank   k + 1  x  k 4-1   matrix   where  7* is the k x k  identity matrix , we have  Daf  Dag\    h   0 1  . ,.   -  „.._»-  Now,  since  g{a,  > a    =  0 for all a,  Dag +  -D^p^'  =  0.  It  follows  that  '  =  -Dag Dbg,   and hence  f'  = Daf  + Dbf4t  =  Daf-  so p   ,p '   = p  ;  + 1, as required.   We can now prove Theorem 7.13.     Proof   of Theorem  7.13   Suppose Si = { i,..., *} C G has regular zero-set intersections in Ed. Let  S2 =   106  Bounding the  VC-Dimension using Geometric  Techniques  be the corresponding functions in G satisfying Definition  7.12. We prove by induction that  S\  has regular zero-set intersections if and only if 52 does,  and  that  the  numbers of connected  components in their  zero-set intersections are the same, from which the theorem follows.  The induction  proceeds over a sequence of function  classes, involving successively more of the  additional  variables 6jj.  To start  with,  define Go  =  { ?,..., £},  with  ff  =  fc.   So Go  =  Si.   Now, define  the  set G\  of functions  mapping from  Rd+1  to R to be Gi  =  j  f \, . . .,  f\,  i,i  j, with  for i =  2,..., k and all o, and  for  all  a.  Notice  that  we  can  think  of  g\,i  as  a  function  defined  on Rd+1.  Lemma 7.14 implies that  Go has regular zero-set intersections if and  only  if the  same is true  of Gi,  and  that  the  zero-set  intersections have  the  same  number  of  connected  components.  We proceed  in  this fashion, iteratively adding a parameter   &ij , modifying  a function    '  to depend on the new parameter, and adding another function   gij   that implicitly defines the value of the new parameter in its zero-set.  Finally, we obtain the set Gfcn, which is the set of restrictions of functions  in 52 to Rd+kn,  so the result follows.     7.6  Bibliographical  Notes  Several examples are known of well-behaved activation functions  giving rise to small networks with infinite VC-dimension.  The first was given by Sontag  1992 . See also  Macintyre and Sontag, 1993 . Sontag's example has the added feature  that,  for  any set of points in R,  and  any desired dichotomy,  it  is easy  to find parameters  for  a  two-layer  network  with two computation  units in the first layer, so that  the network  computes the  desired  dichotomy  of the  points.   This  implies that  proving lower bounds on the computational  complexity of learning two-layer sigmoid networks requires some strong conditions on the sigmoid  functions—see Chapter  25.  Of course, results from  Chapter  5 show that  such compu- tational difficulties  are irrelevant in this case, since the VC-dimension of the function  class is infinite.   Krzyzak, Linder and Lugosi  1996  give a similar example for radial basis function  networks.  Devroye et al.  1996  give an example of a sigmoid network in which the  activation  function   7.6  Bibliographical notes   107  is  again  analytic  and  monotonically  increasing,  but  also  convex  to  the left  of zero and concave to the right.  They  showed that  a two-layer net- work with eight first-layer units of this  kind has infinite  VC-dimension. The  example  we  give  in  Section  7.2  is  simpler.  The  elegant  proof  of Lemma 7.2  was suggested  by Jon Baxter  and Phil  Long.  The  techniques  for  bounding  the  growth  function  that  are  described in  the  remainder  of  the  chapter  and  in  Chapter  8  are  mainly  due  to Goldberg  and  Jerrum   1995 .  These  ideas  have  a  long  history.  The idea of  counting  the  number  of  connected  components  in  the  partition of parameter space defined  by the input  points goes back to the  growth function  calculations  for  the  simple  perceptron.   See  the  bibliographi- cal notes  at  the end  of  Chapter  3.   Goldberg  and  Jerrum gave  bounds on  the  VC-dimension  of  function  classes  that  are  polynomial  in  their parameters;  the  idea  of  Lemma  7.8  arose  in  the  proof  of  their  result. A  similar  idea  was independently  suggested  at  the  same  time   and  re- ported  at  the  same  conference,  COLT'93   by  Ben-David  and  Linden- baum   1993 .  Both  Ben-David  and  Lindenbaum's  paper  and  an  early version of  Goldberg  and  Jerrum's  paper  used  a result  of  Milnor   1964  that  implies  a  bound  on  the  number  of  connected  components  of  the complement  of the  solution of a system  of polynomial  equations.  Later versions  of  the  Goldberg  and  Jerrum  paper  used  the  slightly  stronger results  of  Warren   1968 ,  who  was  studying  approximation  properties of  polynomials,  and  apparently  was  unaware  of  Milnor's  result.  Fol- lowing  Goldberg  and  Jerrum,  Karpinski  and  Macintyre   1997   noticed that  a  result  developed  by  Warren  as  a  tool  could  be  used  more  gen- erally:  Lemma  7.9  was  essentially  in   Warren,  1968 ,  and  the  proof  of Lemma 7.10 is in  Warren, 1968 .  Karpinski and Macintyre showed how to tie these results together and use Sard's Theorem to reduce the prob- lem to  the  regular case for  C    functions.  They  then  used  solution  set components  bounds  for  sets  of functions  involving  exponentials  to  give VC-dimension  bounds  for  sigmoid  networks   we  shall  encounter  these results  in the  next  chapter .  The  idea of representing  the  computation of intermediate  variables as the  solution of another equation in an aug- mented set  of parameters is also due to  Karpinski  and Macintyre.   8  Vapnik-Chervonenkis  Dimension  Bounds for  Neural  Networks  8.1  Introduction  In  this  chapter  we apply  the  theory  developed  in the  previous  chapter to derive bounds on the VC-dimension for a number of specific types of neural network classes, including standard sigmoid networks.  8.2  Function Classes that  are Polynomial in their Parameters We first consider  classes of functions  that  can be expressed as boolean combinations of thresholded real-valued functions, each of which is poly- nomial in  its  parameters.  To apply  Theorem  7.6 to  obtain  bounds on the VC-dimension for these classes, we need a solution set  components bound   that  is,  a  bound  on  the  number  of  connected  components  in the  intersection  of zero-sets of polynomials .  The following  result  will be  useful  for  this.  It  follows from Bezout's  Theorem   which  describes the number of solutions of a non-degenerate system of polynomial equa- tions .  Here, the  degree of  a  polynomial  f  of d variables is the  maxi- mum,  over monomials appearing  in   ,  of the  sum  of the  exponents  in the monomial.  Lemma 8.1  Suppose f  : Rd  ->  R is a polynomial of degree I.  Then the number of connected components of {a G Ed  : f a   = 0} is no more than  Corollary  8.2  For I € N,  the set of  degree I polynomials  defined on Rd has solution set components  bound B  = 2 2  d.  Proof  Suppose  1 < k  <d  and that    i , . . .,   *  are degree I polynomials defined  on R4.  Then the polynomial X f=i fi  has degree no more than  108   8.2  Function  classes  that  are polynomial  in  their  parameters  109  2Z,  and  its zero-set  is the intersection  of the  zero-sets  of { i,.   . . ,   * }  Applying  Lemma  8.1  gives  the  result.      Theorem  8.3  Let F  bea  class of functions  mapping from  R d  x X  to R  so that,  for  all x  €  X  and  f  €  F,  the function  a   »-     a,x   is  a  polynomial on  Rd  of  degree no  more  than  1.  Suppose  that  H  is  a It-combination  of sgn F .  Then  ifm>   d k,  IMm <2f——J  ,  and hence VCdim ff   <  2dlog2 12JW .  Proof  Prom Theorem  7.6  and Corollary 8.2, we have  and if mk  >  d, Theorem  3.7 shows that  !*  2 2Q',  i=0  To  prove  the  bound  on  the  VC-dimension,  recall  that  if  11^  m   < 2 m  then  we  have  VCdim    <  m.  Now,  2 2emkl d d  <  2m  if  and only  if  d log2m  +  Iog2 2efc  d    <  m  -  1.  But  log2m  <  m  2d   + Iog2 2d  eln2     see   1.2   in  Appendix  1 ,  so  it  suffices  to  have  m  > 2 dlog2 4W  In 2   +  1 , which is implied  by m  >  2dlog2 12W .     Theorem 8.3 is a powerful result; it can be used to give bounds on the VC-dimension  of  a function  class in terms  of the  number of  arithmetic operations  required  to  compute  the  functions,  as  the  following  result demonstrates.  Theorem  8.4  Suppose  h  is  a function  from  R d  xW1   to  {0,1}  and  let  be  the  class  determined  by  h.  Suppose  that  h  can  be  computed  by  an algorithm  that  takes  as  input  the  pair   a,x   €  R d  x  Rn  and  returns  i a, x   after  no  more  than  t  operations  of  the following  types:     the  arithmetic  operations  +,  -,  x,  and     on  real  numbers,    jumps  conditioned  on  >,>,<,<,=,   and  ^  comparisons  of real  numbers,  and   110  Vapnik-Chervonenkis  Dimension Bounds for  Neural Networks     output 0 or 1.  Then VCdim    < 4d t + 2 .  Let  A  denote  the  algorithm  that  computes  h.  We  first  show that  any  comparison  that  A  makes  can  be expressed  as  a  comparison of  bounded  degree  polynomials  in  the  parameters  a.  Then  we show that  the  output  of  A  can  be  expressed  as  a  boolean  function  of  the results of comparisons involving a bounded number of these polynomials. Theorem 8.3 then gives the result.  The degree of a rational function   a ratio of polynomials  is the sum of the degrees of the numerator  and denominator polynomials.  The result of an  arithmetic  operation  on two rational  functions  can  be  expressed as a rational function  with degree no more than the sum of the degrees. Furthermore, a comparison of two rational functions  is equivalent  to  a comparison  of two polynomials,  with  degree no more than  the  sum of the  degrees  of  the  rational  functions.  It  follows  that  any  comparison performed  by  A  can  be  expressed  as  a  comparison  of  polynomials of degree no more than 2*.  The algorithm A  can be expressed as a computation tree of depth no more  than  £, with  each  node  corresponding  to  a  comparison  between polynomials, and each leaf corresponding to an output operation.  There can  be  no  more  than  2 t -1   —  1 comparison  nodes  in  this  tree,  so  the number  of distinct  polynomials  that  are examined  by the  algorithm  is no more than  2*"1 — 1.  To invoke Theorem  8.3, we must  express ft as  a fixed boolean  com- bination of functions  of the form  sgn pi a     where pi is a polynomial . This involves only comparisons of the form pi a   > 0, so we may need to use a negated copy of each polynomial to allow the computation of the > , < , =,  and  ^  comparisons using just  the  sgn -   function.  It  follows that  we can express h as a   2* — 2 -combination  of sgn F , where F  is the class of polynomials of degree no more than  2*. Theorem 8.3 shows that  VCdim iJ   < 2d 2* + log2 12  < Adt + 8d.     This  theorem  has  interesting  consequences.  If  we consider  a  model of computing  that  only  allows the  standard  arithmetic  operations  and comparisons  with  real  numbers,  then  any  class consisting  of  functions that are specified by a finite number of parameters and can be computed in finite time on such a computer has finite VC-dimension.  Furthermore, if we consider a sequence of function  classes computed in this way, with increasing  dimension  n  of the  input  domain   X  =  Rn ,  and  with  the   8.2  Function classes  that are polynomial  in their parameters  111  number of parameters and  the computation  time growing only polyno- miaJly with n, then the VC-dimension  also grows only polynomially.  It follows that  any class containing functions  computable in time polyno- mial in the input  dimension n has VC-dimension polynomial in n.  The main  result  in  Chapter  4   Theorem  4.2   shows  that  this  implies  the sample complexity grows only polynomially with the input  dimension. The following result shows that  the bound of Theorem 8.4 cannot be  improved by more than  a constant  factor.  Theorem  8.5  For  all d^t  >  1,  there is  a  class H  of functions,  pa- rameterized  by d  real  numbers, that can be computed  in  time O i    us- ing  the  model of  computation defined in  Theorem 8.4 ,  and  that has VCdim ff   >  dt.  Proof  The idea of the proof is to show that  we can extract one of up to t  bits from  any of the d parameters in time O t    with the input  value specifying  which  parameter  and  which  bit  in  that  parameter .  This means that  we can shatter  a set of dt input  points.  More specifically,  the algorithm we consider  computes a function  h : K ^ x i 2  4 { 0 , l }.   We shall define this function  only for  certain  values of a €  R d  and  x  €  K 2;  it  is easy to ensure that  the  algorithm  halts in time O t  and outputs something for other parameter and input values. Consider x  =   Z,ra  6  {1,... ,d} x {1,...,t}.   Let a =   ai,... ,a<i  be a sequence oft-bit  numbers in [0,1 ; explicitly, suppose a* =  ]C*=1  H,j2~* for a^,..., a^t  €    {0,1}.  Then  define  Clearly, h can be computed  in time O i   by iteratively  doubling, com- paring  with  1,  and  conditionally  subtracting  1 from  a .   We  assume that t =  £2 d , since otherwise no algorithm can read the d parameters.  Since we can  choose the  aij  arbitrarily,  the  set  {1,..., d}  x  {1,...,   t}  of size dt  is shattered  by H.     It is interesting to observe that  Theorem 8.4 would no longer be true if we allowed computation of the floor function,  [-J, in unit time.  In that case, there would be an  algorithm  to  compute the function  h of Theo- rem 8.5 in timef  O log2 £  , which would imply that  there is a class H  f  In fact, this observation and Theorem 8.4 show that in the model of computation defined  in  Theorem  8.4,  taking  the floor function  of  t-bit  numbers  takes  time n t iog2t .   112  Vapnik-Chervonenkis  Dimension Bounds for  Neural Networks  of functions  that  can be computed  in time T  and involving d parame- ters with VCdim if   =  Q 2Td .  Similarly, if we allowed computation of the sin -  function  in unit time, a constant time program could compute a  class  of  functions  defined  by  a  single  parameter  but  having  infinite VC-dimension   see Lemma 7.2 .  8.3  Piecewise-Polynomial  Networks  As  an  easy  example  of  the  application  of  Theorem  8.4,  we may  con- sider  the  class  of  feed-forward  linear  threshold  networks.  Recall  that Theorem  6.1  implies  that  the  VC-dimension  of  the  class  of  functions computed  by  a  linear  threshold  network  with  W  parameters   weights and thresholds   is O WlnW .  Since computing the output  of a linear threshold network takes time O W ,  Theorem 8.4 immediately gives the following   slightly worse  bound.  Theorem 8.6 Suppose N  is a feed-forward linear threshold network with a total of W  weights, and let H  be the class of functions  computed  by this network.  Then VCdim ff   =  O W2 .  This  theorem  can  easily  be  generalized  to  networks  with  activation functions  that  are piecewise-polynomial.  A piecewise-polynomial  func- tion     : R  ->  R  can  be  written  as  f a   =  Y%=i l;4 t     where A l ,..., A p  are disjoint real intervals whose union is R, and   i , . . .,   fp are polynomials.  We say that     has p pieces, and  we define  the degree of     as  the  largest  degree  of  the  polynomials   *.  Figure  8.1  shows  a piecewise-polynomial  activation  function,  which  has  three  pieces  and degree  one.   In  fact,  Figure  1.3,  which  we described  as  the  graph of the  standard  sigmoid  function,  illustrates  a piecewise-polynomial  acti- vation  function  with  100 pieces  and  degree one, because  this  function was plotted  using 100 line segments.   Theorem  8.7  Suppose N  is a feed-forward  network with a total of W weights and  k  computation units,  in  which the  output  unit  is  a  lin- ear threshold unit  and  every other  computation unit  has  a  piecewise- polynomial activation function  with p  pieces and degree  no  more than I.  Then,  if H  is  the class of functions  computed by N,  VCdim if   =  kllog2p  .   8.3  Piecewise-polynomial  networks  113    <*   1   -  -1  Fig.  8.1.  A  piecewise-linear  activation  function.  Proof  To  compute  an  activation  function,  we can  determine  the  ap- propriate piece with  flog2 pi  comparisons.  Computing the value of the function  takes  an  additional  0 2   steps.  Hence, the total  computation time is 0 W  + Mlog2p , and Theorem 8.4 implies the result.     If the number of layers in the network is restricted, the following bound  is better.  Theorem  8.8  Suppose  N  is  a feed-forward  network  of  the  form  de- scribed  in  Theorem  8.7,  with  W  weights,  k  computation  units,  and  all non-output  units  having  piecewise-polynomial  activation  functions  with p pieces  and  degree no  more  than  I.  Suppose  in  addition  that  the  com- putation  units  in  the  network  are arranged in  L  layers,  so  that  each  unit has  connections  only from  units  in  earlier  layers.  Then  if H  is  the  class of functions  computed  by  N,  < 2L  {2emkp l +   l L~l   L~l WL  and  VCdim if   < 2WL\og2{4WLpk \xi2   + 2WL2log2 Z + 1  4- 2L.  For fixed pandl,  VCdim if   =  0{WL  log2 W  +  WL2 .   114  Vapnik-Chervonenkis  Dimension  Bounds for  Neural  Networks  Proof  Without  loss of generality, suppose that  in each non-output  unit the activation function  is a fixed  piecewise-polynomial  function  a  : E  -> R,  of the  form  p  where  A l   =   -00, *i ,  A i   =  [<i-i,t{   for  i  =  2 , . . . , p,  h  <   t<i < ...  <  t p  =  00,  and  each  <^i  is  a  polynomial  of  degree  no  more  than 2.  Let fct denote  the  number  of  computational  units  in  layer i,  so  that f- * L -I   +1-  For input x  and parameter vector a €  R w, k =  ki  + fo H  let    x, a   denote the  argument  of the  threshold function  in the  output unit,  so that  the  network  computes  sgn   x,a  .  Let  F  be the  class of functions  {x  H->     X,  a   :ae  R w  }, so that  sgn F   is the set of  functions computed  by  N.  Before  presenting  the  details,  we outline  the  main  idea  of  the  proof. For any  fixed  input  x,  the  function    x, a   corresponds  to  a  piecewise- polynomial  function  in the  parameters a,  of bounded  degree.  Thus,  the parameter  domain  Rw  can  be  split  into  regions,  and  in  each  of  these regions the function    x,    is polynomial.  Theorem 8.3 shows that  each region does not contribute much to the growth function.  Taking the sum of these bounds over all regions gives the  result.  Now, fix  arbitrary x\,  X2,..., x m  in X.  We wish to  bound  K  =  { sgn   x 1  o  ,...,8gn   * m,a     : a  €  R w}\.  Consider a partition S  =  { S I , 5 2 , . . . , 5 J V}   of the parameter domain  So,  Ujli  Si  =  R w  and 5< n Ss  =  0 if i     j.   Clearly  K  <  S   { sgn   x!, a     , . . ., sgn   x m, a     : a  €  We  choose  the  partition  so  that  within  each  region  5»,  the  functions     x i,   >   >     x m ,     are  all  fixed  polynomials  of  degree  no  more  than     -f  1 L~~1.  Then,  by  Theorem  8.3,  each  term  in  this  sum  is  no  more than  In  the  remainder  of  the  proof,  we  define  the  partition  and  show  that it  is  not  too  large.  The  partition  is  constructed  recursively,  through a  sequence   <SL-I  of successive  refinements.  These  partitions are constructed  so that  for any parameter  value within  a fixed  element   8.3  Piecewise-polynomial  networks   115  of partition Sn  and for any unit in the first n layers, the net input to the unit  lies within  some fixed  piece of the  piecewise-polynomial  activation function.  In this way, we recursively construct the partition <SL_I, which defines  regions  of  parameter  space  where  the  function     is  polynomial in the  parameters.  Let  <Si  be  a  partition  of  Rw  such  that,  for  all  5  £  «Si,  there  are  constants  bh,ij  €  {0,1}  for which  sgntPM,.  a  -  U  =  bhiij   for all a  G  5,    8.2     { l , . . . , m },   h  €  {l,...,fci}  and  i  €    { l , . . . , p },   and  where where  j  € Phtxj  is  the  affine  function  describing  the  net  input  to  the  A-th  first layer unit,  in response to  Xj.  Recall  that  U are the  break-points  of  the piecewise-polynomial  activation  functions.  Clearly, for any fixed 5,  any first layer  unit,  and  any  a?j, as  a  ranges  over  5  the  output  of  the  first layer  unit  in  response  to  Xj  is  a  fixed  polynomial  in  o.  Note  that  the partition <Si only distinguishes  weights in the first layer of  computation units.  If we define  H\  as the set of  functions  {   M , j    *-> sgn p M i a   -  U   : a €  R w}   ,  then  we can  choose S\  so that  its  size is  no larger than  Il j^mfcip .  If W\  is  the  number  of weights in  the first layer,  the  functions  in  H\  are thresholded versions of functions that are affine in these W\  parameters, and so Theorem 8.3 implies  that   Notice that we can obtain a better bound from Theorem 3.1, but  using the  weaker  consequence  of  Theorem  8.3  simplifies  subsequent  calcula- tions,  and only  affects  the final result  by a constant  factor.   Now,  let  W i , . . ., WL  be  the  number  of  variables  used  in  computing the unit outputs up to layer 1 , . . ., L  respectively   so WL =  W ,  and let fci,...,  hi, be the  number of computation  units in layer  1 , . . ., L  respec- tively   recall  that  fee  =  1 .  Define  Sn   for  n  >  1   as  follows.  Assume that  for all  5  in <Sn_i  and  all Xj,  the  net  input  of every unit  in  layer  n in  response  to  Xj  is  a fixed polynomial  function  of  a  €  5,  of  degree  no more than     -I- l   n - 1 .  Let Sn  be a partition of A  that  is a refinement  of <Sn_i   that  is, for all  5  €  <S n, there is an  S* €  <S n-i  with  5  C 5' ,  such that  for all 5  €  S n  there are constants  bhtij  G {0,1}  such that  sgn p Mi  ^  -  U  =  bhtij   for all a €  5,    8.3    116  Vapnik'Chervonenkis  Dimension Bounds for  Neural Networks  where ph,Xj  is the  polynomial  function  describing  the net input  of the ft-th unit in the n-th layer, in response to Xj, when a € S.  Since S C S1 for some 5' G    8.3  implies that, within 5, the output of each nth layer unit in response to an Xj is a fixed  polynomial in a of degree no more than 1 1 + I ""1.  If we define Hn as the set of  functions  { h,ij   H> sgn pM. a    -U :aeRw},  then  we can choose Sn  such  that,  for all S1 €  <Sn-i  the number of subsets  \{S € Sn  : S  C S'}\ is no more  than  IIj n mfcnp .  Since Hn contains  thresholded  functions  that  are polynomial in Wn parameters, with degree no more than     + I 11""1, Theorem 8.3 implies that  for 5' £  Notice also that  the net input of every unit in layer n +1  in response to Xj is a fixed polynomial function  of a € S € Sn  of degree no more than  Proceeding in this way we obtain a partition SL-I of A such that  for S  G SL-I the network output in response to any Xj is a fixed polynomial of a G 5 of degree no more than 1 1 + 1 L~2.  Furthermore,  r11   -  l\2\  wt  1-1 \W«  Multiplying by the bound   8.1  shows that  Since the points  x\,...,  xm  were  chosen  arbitrarily,  this  gives the  re- quired upper bound on the growth function  of H.  Taking  logarithms  to base  two  of this  bound  shows  that  the VC-  dimension d of H satisfies   2edpfei f  +  l  i-1\  <  L [1 +  L -  1  W log2  I +1  + W log2  2edpk ].   8.3  Piecewise-polynomial  networks   117  Applying Inequality   1.2 , Appendix  1, shows that  VCdim F   < 2L [ L -  1 Wlog2 Z +  1  + Wlog2  4WLpk   In2  + 1].  The second statement in the theorem follows from the fact that L  <k  < W.  D  This  theorem  implies  that  if we approximate  an  activation  function with a piecewise-polynomial function, the resulting network has bounded VC-dimension.  Suppose the  activation  function     increases monotoni- cally  and  takes  values  between  0  and  1.  Then  there  is  a  piecewise- constant function     with  0 1 A   pieces that  approximates     within A everywhere.   Using higher order  polynomials instead  of constants  also allows  the  approximating  function  to  match  derivatives  of   .   Then a  feed-forward  network  with  W  parameters,  L  layers,  and  computa- tion  units  with  activation  function     has  VC-dimension  O WL L  + log2 WL A   .  So even  though  Theorem  7.1  shows  that,  for  certain sigmoidal  activation  functions,  small  networks  of  units  with  these  ac- tivation  functions  have infinite  VC-dimension,  there are many sigmoid functions   including the function  defined in Theorem 7.1  for which net- works  of  units  with  activation  functions  that  accurately  approximate these  sigmoid  functions  have  small  VC-dimension.  The  class  of  func- tions  that  can  be  approximated  accurately  using  piecewise-polynomial functions  is large, and includes, for example, functions  of bounded vari- ation.  The bounds given by Theorems 8.7 and  8.8 are nearly  optimal.  For networks with  a fixed  number of layers and  a piecewise-polynomial ac- tivation  function  with  a  fixed  number  of pieces  of fixed  degree, Theo- rem 6.3 shows that, in the case of binary inputs and at least three layers, the  VC-dimension  grows as  Wlog2 W,  and  so Theorem  8.8  cannot  be improved by more than a constant factor.  Similarly, Theorem 6.4 shows this in the case of real inputs and at least two layers. If we also consider the number of layers, Theorem 8.7 gives better bounds than Theorem 8.8 when W  =  o L2 .  The following theorem shows that  the O W2   bound of Theorem 8.7 cannot be improved by more than a constant factor if we allow an arbitrary number of layers. In fact, the result applies to a rich class of activation functions, including the standard sigmoid function,  as well as piecewise-polynomial  functions.  Theorem  8.9  Suppose s : E -> R has the following properties:   i   lima_Kx> s a   =  1  and  lima_>_oo s a   =  0,  and   118  Vapnik-Chervonenkis  Dimension Bounds for Neural Networks   ii   s  is differentiate  at some point cto  G E,  with s' ao  ^  0.  For any L>1  and W  > 10L — 14, there is a feed-forward network with L layers and a total of W  parameters,  where every computation unit but the output unit has activation function  s,  the output unit  being  a  linear threshold unit,  and for  which the set  H  of functions  computed by the network has  VCdim ff   >  ~   y  Clearly, the constants  1 and 0 in this theorem can be replaced by any  two distinct  numbers.  In the proof of Theorem 8.9, we construct  a network of linear thresh- old units  and linear units   that  is, computation  units with the  identity activation  function   that  exhibits  the  lower  bound,  and  then  use  the following  result  to  show that  we can  replace these  units  by units  with activation function  s.  Lemma  8.10  Let N  be a feed-forward  network of linear units  and lin- ear threshold units,  with  the  output  unit  a  linear threshold  unit.  Let N1  be a feed-forward  network identical to N,  but with all  computation units  except the output unit  replaced  by computation units  with  activa- tion function s : E -* E,  where s has properties 1 and 2 of Theorem 8.9. Then for any finite set S  of input patterns, every function on S computed by N  can be computed by N1.  Proof  The proof  is similar to the proof of Theorem  6.5:  we show  that we can accurately approximate a threshold function  by multiplying the argument  of  s -   by  a  large  constant,  and  we can  accurately  approxi- mate the identity function  by concentrating on a neighbourhood  of the differentiate  point  ao of 5.  Consider a function  g computed by AT on 5.  Suppose that  N  contains k  computation  units,  and  label  these  units  so that  unit  j  takes  input from unit i only if i < j.   Let V{ x   denote the net input to computation unit  i in response to input  pattern  x  G S,  for  1 < i < k.  Defining  e =  min   ,  we can  assume  without  loss  of  generality  that  e  >  0.  Now,  we  step through  the  network, replacing each threshold  activation  function  v t->   8.3  Piecewise-polynomial  networks   119  sgn v  by the  function  v^s Mv ,   and replacing each identity  activation function  by  v  H>  s v M  + a0   -  s a0    M s' a0 ,    8.4    8.5   where M  is a positive real number.  For 1 < i < fc, let V^M^   denote the net input to computation unit i in response to x G S when the activation functions  of units  1,..., i — 1 have been  changed in this way.  If unit  1 is  a  linear  threshold  unit,  the finiteness of  5  and  the  fact  that  e >  0 implies that  lim  max\s Mvi x    -sga vi x  \  = 0. M-»oo  x€S  If unit  1 is a linear unit, we have  lim  max\ s vi x  M  + ao  -  s ao   M s' ao  -  vi x \ = 0.  In either case, we have that  lim  maxI>2,M    — V2 x \  =  0. M—Kx>  x£S  Proceeding in this way, we conclude that  lim  max \v lim  max \vk,M x  -  Vk{x \ = 0, M-»oo  xGS M-»oo  xGS  and so, for sufficiently  large M,    =  g x .  By scaling the weights and adjusting  the thresholds of units in the net- work, we can replace the activation functions   8.4  and  8.5  by v H* S V , which shows that  the function  g on 5  is computable by N'.  D  Proof   of  Theorem  8.9   The proof  follows  that  of Theorem  8.5; we show how the functions  described there can be computed by a network, and keep track of the number  of parameters and layers required.   It  is not surprising that the dependence on time in the proof of Theorem 8.5 corresponds to the dependence on depth in this proof.   Fix  M,7V E N.  Let  a{ =  £ ^i  a iyj2^   for  aitj  G {0,1}, so at  G [0,1   for i =  1,..., N.  We will consider inputs in B^  x BM, where  BN  =  {ei : 1 < i <  N},  e* G  {0,1} N  has ith bit 1 and all other bits 0, and BM is defined similarly.   120  Vapnik-Chervonenkis  Dimension  Bounds for  Neural  Networks  U2  V2  at  = ci  CM-I  a ,i  A vi  ,2 A V2  Fig.  8.2.  The  network  of linear  threshold  units and linear units  constructed to prove the VC-dimension lower bound of Theorem 8.9.  As in the  proof of Theorem  8.5,  we show how to  extract  the  bits of  the a», so that for input  x  =   e , em   the network outputs a ,m.   We use this encoding  of  the  integers  Z,m  to  simplify  the  network  computation  and reduce the  number of layers required.   The construction is illustrated in Figure 8.2.  Suppose the network in- put is x  =    u\,...,  UN ,   V\,...,  VM    =   ci, e m .  Then using one linear unit  we  can  compute  ^2i=1  Uidi  =  aj.  This  involves  N  +  1 parameters   8.3  Piecewise-polynomial  networks   121  and one computation unit in one layer.   In fact,  we only need N  param- eters, but  we need  the extra parameter  when we apply  Lemma  8.10.   Define  M  for fc =  1 , . . . , M.  Then  ai,k  =  sgn c* -  1 2   for all  k.  Also, c\  =  a   and  for  k =  2 , . . .,  M.  Clearly, we can compute  the bits  a^\,...,  a ,M-i  and the numbers  C2,...,CM-I  in another  2 M  — 2  4-1  layers, using  5 M — 2  +  2 parameters in  2 M  -  2  +1  computation  units   see Figure  8.2 .  Now  set  If m  =  M  then 6 =  ajtM, otherwise 6 =  0.  This, and the computation of  b  =  sgn  I 2CM-I  — Q>I,M-I ~~  M -l  aj,m =  bV  \J   aiti  A v»   involves another 5M  parameters in M  +  1 computation units, and adds another 2 layers.   Here, we are using the standard notation for describ- ing  boolean  functions  in  terms  of  the  OR  connective  V  and  the  AND connective  A, and we interpret  0 as FALSE and  1 as TRUE.   In total,  there  are 2M  layers and  10M +  N  — 7 parameters,  and  the network shatters  a set  of size NM.  Notice that  we can add  parameters and  layers  without  affecting  the  function  of  the  network.  So  for  any L,W  €  N,  we can  set  M  =  [L 2\  and  N  =  W  +  7 -  10M,  which  is  at least  [W 2\  provided  W  >  10L -  14.  In that  case, the VC-dimension  is at  least  IS]  Lemma 8.10 implies  the  result.       122  Vapnik-Chervonenkis Dimension Bounds for  Neural Networks  8.4  Standard  Sigmoid Networks  Discrete  inputs  and bounded fan-in  In this section we consider networks with the standard sigmoid activation function,  a a   =  1  1 + e~a .  The first  result is for two-layer networks with discrete inputs.  We define the fan-in  of a computation  unit to be the number of input  units or computation  units that  feed into it.  Theorem  8.11  Consider a two-layer feed-forward  network with input domain X  =  {-£>,  -D  + 1 , . . .,  D}n   for D £ N   and k first-layer com- putation  units,  each  with the standard sigmoid activation function   the output unit being a linear threshold unit .  Let W  be  the total number of parameters in the network,  and suppose that the fan-in of each  first-layer unit is no more than N.  Then the class H  of functions  computed by this network has VCdim iJ   <  2Wlog2 6 WD .  This  theorem  implies  that  if  the  fan-in  of  the  first-layer  units  in  a two-layer network is bounded  by a fixed  number iV, and the inputs are restricted  to  a fixed discrete  set  of  this  kind,  then  the  VC-dimension increases  only  linearly  with  the  number  of  parameters.  It  is  easy  to see that,  even  for  binary  inputs,  the  VC-dimension  of  these  networks is  Q W .   The  lower bound  is exhibited  by  a  two-layer  linear  thresh- old  network  with  k  first-layer  units  and  Nk  inputs,  with  each  input connected to only one first-layer unit.  The argument used to prove The- orem 6.2 easily extends to show that  this network shatters a set of size Nk  =  il W .  Theorem  6.5  shows  that  this  lower  bound  also  applies to  sigmoid  networks.   In  contrast,  Theorem  8.9  shows that  standard sigmoid networks with discrete inputs   \X\  =  O W2    but fan-in  Q W  and depth il W   have VC-dimension Q W2 .  Theorem 8.11 implies that there is a similar gap for linear threshold networks, because we can apply Theorem 6.5  and the observation above  to give the following result.  Theorem  8.12  Consider a two-layer feed-forward linear threshold net- work that has W  parameters and whose first-layer units have fan-in  no more than N.  If H  is the set of functions  computed by this network on binary inputs,  then VCdim iJ   < 2Wlog2 60iV .  Furthermore,  there is a constant c such that for  all W  there is a network with W parameters that has VCdim H   >  cW.  This  result  shows that  there  is a  gap  between  the  VC-dimension of these networks and the fi W log2 W   VC-dimension for both deeper net-   8.4  Standard sigmoid networks   123  works with arbitrary fan-in   Theorem 6.3  and two-layer networks  even with constant fan-in   with richer input  domains   Theorem 6.4 .  Proof   of  Theorem  8.11   The  proof  involves  a  simple  transforma- tion of the parameters in such a way that,  for each input  vector x,  the network computes the sgn function  of a polynomial in the  transformed parameters.  We can then apply Theorem 8.3.  Consider  a first-layer unit,  and  suppose   without  loss of  generality  that  it has connections from  the first N  input  components, a?i,..., XN. Let W\,..., WN be the corresponding weights, and let 6 be the threshold, so the unit  computes  f x   =   1  1 + exp  -  Y,"=i  WjXj +  Now, if we set a,j  =  e~Wi  for j  =  1,..., iV, and ao =  ee,  we have  which, for Xj G {—D,...,  D}, is the ratio of a polynomial of degree  ND and a polynomial of degree no more than 2ND  + 1.    Notice that  such a transformation  cannot decrease the VC-dimension.   Since the network has a linear threshold output,  it  computes the sgn function  of an affine  combination of k of these rational functions,  or of inputs.  For a fixed input vector, this is equivalent to the sgn function of a polynomial of degree no more than  3ND  -I- 2 in the transformed  first layer  weights  and  the  second  layer  weights.  Theorem  8.3  shows  that VCdim H   < 2W log2  36iVD + 24  < 2W log2  60ND .     Notice that the proof technique of Theorem 8.11 is specific to the par- ticular form  of the standard  sigmoid activation  function.  It  apparently cannot be extended to arbitrary finite domains, since the proof requires elements of X  to be small integral multiples of some number.  General  standard  sigmoid  networks  The following  result  provides  a  general VC-dimension  bound  for  stan- dard sigmoid networks.   124  Vapnik-Chervonenkis  Dimension Bounds for  Neural Networks  Theorem 8.13  Let H  be the set of functions computed by a feed-forward network with W  parameters and k  computation units,  in  which  each computation unit  other than  the  output unit  has the standard  sigmoid activation function   the output unit being a linear threshold unit .  Then  provided m  > W,  and  VCdim H   <   Wk 2  +  UWklog2 lSWk2 .  There  is  a  considerable  gap  between  this  O  kW 2   bound  and  the best  known  lower  bound  of  fi W2 ,  which  is  exhibited  by  a  network with  k =  Q W   computation  units   see Theorem 8.9 .  The following result is analogous to Theorem 8.4, which gives bounds on the VC-dimension of a function  class in terms of the number of arith- metic  operations  required  to  compute  the  functions.  In  this  case, we also allow the computation of the exponential function  to be one of the basic operations.  Theorem 8.14  Let h  be a function from Rd x Rn  to {0,1}, determining the  class  H=  {x^h{a,x    : a € E d }.  Suppose that h  can be computed by an algorithm  that takes as input the pair  a, x   €  K d  x  En  and returns ft a, x   after no  more than t  of the following  operations:     the exponential function Q 4 e   on  real numbers,    the arithmetic  operations  +,  —, x,  and    on real  numbers,    jumps  conditioned  on >,>,<,<,=,   and ^  comparisons  of real  numbers,  and    output 0 or 1.  Then VCdim ff   <  t2d d+  191og2 9d  .  Furthermore, if  the t  steps include no more than q in  which the exponential function  is  evaluated, then  and hence VCdim iJ   <   d q + I  2 +  Ud q + l  t + Iog2 9d g + 1   .   8.4 Standard sigmoid networks   125  This  result  immediately  implies  a  bound  on the  VC-dimension  for feed-forward  standard  sigmoid  networks that  is only  a constant  factor worse than  the bound  of Theorem 8.13.  Notice that,  for q =  0  which corresponds to the function  classes described  by Theorem  8.4,  defined in terms of arithmetic  operations , this result  is only a constant  factor worse than  Theorem  8.4, since  O d d + t + log2 d    =  O dt  when we assume that  the program makes use of all d parameters.  Proof  of  VC- dimension  bounds for  sigmoid  networks  and  algorithms  The  proofs  of  Theorems  8.13 and  8.14 use the  following  solution  set components bound for polynomials of certain exponential  functions.  Lemma  8.15  Let   i , . . ., fq  be fixed affine functions  of a\,...,  a<*,  and let G be the class of polynomials  in a i , . . ., a^, e^a\...,  e^    of degree no more than I.  Then G has solution set components  bound  Because the affine functions  *in this lemma must be fixed, we cannot apply Theorem 7.6 directly  not even for the case of two-layer networks . The  following  lemma  shows  how we can  apply  Theorem  7.13 to get around  this  problem.  Recall  that  the degree of a rational  function   a ratio of two polynomials  is the sum of the degrees of the numerator and denominator polynomials.  Lemma  8.16  Suppose  G is the class of functions  defined  on Rd com- puted by a circuit satisfying the following conditions:  the circuit contains q gates, the output gate computes  a rational function  of degree  no  more than I > 1, each non-output gate computes the exponential function  of a rational function  of degree no more than  ,  and the denominator of each rational function  is  never zero.  Then G  has solution set  components bound   2  Proof The idea of the proof is to include extra variables to represent the value of the rational function  computed at each gate and the value of the output  of each gate.  This shows that  the class defined  in Lemma 8.15 computes G with intermediate variables, so we can apply Theorem 7.13. Fix  a  set  of  k  functions    I , . . . ,  *  from  G,  where  k  <  d.  Define   126  Vapnik-Chervonenkis  Dimension Bounds for Neural Networks  polynomials riij  and dij   for i = 1,..., k and j  =  1,..., q  so that each fi  can be defined  recursively in terms of V{ti,...,  i>i 9_i, by  and, for j  =  l , . . . , g - l,  t;- -M -  exD   That is, fi a  is the output of circuit z, and Vi,j a  is the output  of gate j in circuit i.   Now, consider the functions  for i = 1,..., k and j  = 1,..., q, and  hij a,b,c  = exp c^   -  for z = 1,..., k and j  = l,...,g—  1.  Let F be the set of polynomials in the variables  a, 6, c  and  for  i  =  1,..., k  and j  =  1,..., q —  1, of degree  no more  than  I + 1. Clearly, the functions  fi, gij,  and hij  are in F. Lemma 8.15 shows that F  has solution set components bound  B   =          It  is easy to check  that  F  computes  G with  2q -  1 intermediate  vari- ables  the derivative condition is satisfied  because the denominators of the rational functions  are never zero , so Theorem  7.13 shows that  this implies the same solution set components bound for G.     Proof  of  Theorem  8.13  For a  standard  sigmoid  network  with W parameters and k computation  units, and a fixed input  vector x, there is an equivalent  circuit of the kind defined  in Lemma 8.16.  To see this, notice that  we can distribute the computation of the standard  sigmoid function,  so that  the exponential  function  is computed  at the output of one gate, and the function  a i-> 1  1 + a  is computed  by the gates to  which  this  gate is connected.  In this  way, every  gate in the circuit   84  Standard sigmoid networks   127   except the output  gate  computes a function  of the form  exp   -  where  vi  is  the  output  of  gate  i  and  Xj  is  a  component  of  the  input vector.  Since there are no more than k variables vi in the denominators, we can  compute  a  common  denominator  and  express the  argument  of the exponential  function  as a rational function   of the  weights Wj and gate outputs Vi   of degree no more than  2k.  Lemma 8.16   with d =  W, q = fc, and   =  2k , together with Theorem 7.6 shows that  nH m   <  for m  > W.  For this number to be less than  2m  we require  m >   Wk 2 2  + 5Wk\og2 18Wk2   +  Wlog2 em W .  Inequality   1.2  in Appendix  1 implies that  Wlog2  m < m 2 +  Wlog2 2W  eIn2  ,  so it suffices if  m >   Wk 2  + 10Wklog2 18Wk2   + 2Wlog2 2 ln2 ,  and this is implied by m >   Wk 2  + UWk\og2 18Wk2 .   D  Proof  of  Theorem  8.14   We start  with  the  growth  function  bound. The proof closely follows that  of Theorem 8.4.  First,  by  the  same  argument  as  in  the  proof  of  Theorem  8.4,  any rational  function  computed  by the  algorithm  has degree no more  than 2*. Furthermore, the number of distinct circuits of the kind described in Lemma 8.16  call these rational exponential circuits  that  are examined by the  algorithm  can be no more than  2*"1 -  1, but  to express this as a boolean combination  of sgn functions,  we may need to use a negated copy of each.  Also, each rational exponential  circuit  contains no more than q+1 gates. It follows that we can express ft as a  2*—2 -combination of sgn F ,  where F  is the  class of rational exponential  circuits  of size q + 1. Lemma 8.16 and Theorem 7.6 imply  that  HH m   <   128  Vapnik-Chervonenkis Dimension Bounds for  Neural Networks  provided m > d  2t  — 2  , and this is less than  2m when  m >  M? + *    +5d q+l {t+\og 2 9d{q+l   +dlog2 e d +dt+d\og2  m.  + *    Applying Inequality   1.2  from  Appendix  1 shows that m >   d 9 + I  2 + 10d q +1  * + Iog2 9d 9 + 1    + <f t + Iog2 2  In2  , will  suffice,  and  this  is  implied  by  m  >   d{q + I  2  +  Ud q  + l  t  + Iog2 9d g +  1   .  It  follows  that  VCdim    <   d q + I  2 +  lld g  +  If we allow up to q =  t — 1 exponential functions  in the  computation,  we have  VCdim ff   <    dt 2 + Udt t  + Iog2 9dt    <  ^2d d+191og2 9d  ,  which is the first inequality of the theorem.      8.5  Remarks  The techniques used in this chapter  to establish  VC-dimension  bounds for sigmoid networks are considerably more generally applicable.  For in- stance, in radial basis function networks, the computation units compute a function  of their input  vector x  G Kn  of the form    x — c TH x  — c  , where c £  En  and  S  is an  n  x n  matrix.  A common choice for        is the function  a  H^ e" a.  In this case, Theorem 8.14 immediately  implies bounds on the VC-dimension of these radial basis function  networks, or even networks that  combine piecewise-polynomial  activation  functions, the  standard  sigmoid,  and  radial  basis functions.  Other  choices for    include various rational functions,  such as the function  a  H> 1  1 + a . In this case, the more specific result, Theorem 8.4, implies slightly better bounds on the VC-dimension.  Another  example  of  a function  class for  which  the  bounds  of Theo- rem 8.14 are applicable is the  mixture of experts model.  In this model, the network computes the  function        where  i , . . . ,  *  and  Q\,...,  * are functions  computed  by sigmoid net- works.  The  idea of the  model  is that  each  of the  functions   * gives  a   8.6  Bibliographical  notes   129  good approximation to the desired mapping in some region of the input space   fi  is an  'expert',  with  specialized  expertise in  that  region , and the value at some x of each corresponding function  gi indicates the con- fidence  in that  expert's accuracy at the point x.  Clearly, Theorem  8.14 gives bounds  on  the  VC-dimension  of the  class of functions  computed by these networks.  8.6  Bibliographical  Notes  Bezout's Theorem is given, for example, in  Benedetti and Risler, 1990, Theorem  4.4.1,  p.  206 .  The  upper  bounds  on  the  VC-dimension  of polynomially  parameterized  function  classes,  and  of  classes  computed by bounded time algorithms  Theorems 8.3 and 8.4  are slight improve- ments  by constant factors   of the main results of Goldberg and Jerrum  1995 .  The  lower  bound   Theorem  8.5   is also  due  to  Goldberg  and Jerrum.  Koiran  and  Sontag   1997   showed that  the  functions  that  Goldberg and  Jerrum  used  to  establish  the  lower bound  of Theorem  8.5  can  be computed  by  sigmoid  networks,  which  shows that  there  is  a  net  with VC-dimension  Q W2 .  Lemma 8.10, which shows that  linear units and linear threshold units can be simulated  by sigmoids satisfying  two mild conditions,  is also due  to  Koiran  and  Sontag.  They  also showed  that the  ft W2   lower bound applies to any activation  function  s that  is C2 but  not  essentially  a linear  function.  The construction  of Theorem  8.9  which is from   Bartlett, Maiorov and Meir, 1998  , shows how the VC- dimension depends on the number of layers in the network  and also im- proves the constants from  Koiran and Sontag's bound .  Theorem 8.8— the upper bound for shallow networks of units with piecewise-polynomial activation functions—is  also from   Bartlett et al., 1998 . Sakurai  1999  independently  obtained related   and in some cases improved  results.  The  observation  that  Goldberg  and  Jerrum's  bound  for  polynomial functions  can be applied  to give a bound  on the VC-dimension  of two- layer sigmoid networks with  discrete inputs  was made in   Bartlett  and Williamson,  1996 .  Theorems  8.11  and  8.12  improve  their  result  by showing  the  dependence  of  the  bound  on  the  fan-in  of  the first-layer units.  The solution set components bound of Lemma 8.15, for functions  that are  polynomial  in  the  parameters  and  in  exponentials  of  fixed  affine functions  of the parameters, is an immediate consequence of a result of Khovanskii   1991    see also   Benedetti  and  Risler,  1990  .  Khovanskii   130  Vapnik-Chervonenkis  Dimension  Bounds for  Neural  Networks  gives  a  general  approach  which  is  applicable  to  any  functions  that  are solutions  of  certain  first  order  differential  equations.  These  functions are known as Pfaffian functions]  the exponential function is an example. Lemma 8.15  can be expressed in this more general form, in terms of an arbitrary fixed Pfaffian  function,  but  we do not  know of any  commonly used neural network classes for which this generalization would be useful. On  the  other  hand,  it  would  be  interesting  to  establish  conditions  on activation  functions  that  are  necessary  and  sufficient  for  finiteness  of the  VC-dimension  of  the  network.  The  Pfaffian  condition  is  certainly not  necessary.  Macintyre and Sontag   1993   used results from model theory to show that  the VC-dimension  of feed-forward  sigmoid networks with the stan- dard  sigmoid  activation  function  is  finite.  Karpinski  and  Macintyre  1997   showed  how  the  techniques  of  Goldberg  and  Jerrum  could  be combined  with  the  results  of  Khovanskii   through  the  application  of Sard's  Theorem  described  in  the  last  chapter   to  give  a  bound  on  the VC-dimension  of  feed-forward  sigmoid  networks  with  activation  func- tions  that  are rational  functions  of Pfaffian  functions.  Theorem  8.13  is a special case of this result for the standard sigmoid.  Theorem 8.14 uses the same techniques, and the ideas of Theorem 8.4 to give an analogous result  for computations  involving  exponentials.  The mixture of experts model is described in  Jacobs, Jordan, Nowlan and  Hinton,  1991 ,  and  radial  basis  function  networks  are described  in  Powell, 1987 .  See also  Krzyzak et al., 1996 .  The techniques presented in  this  chapter  can  also  be  used  to  give  bounds  for  the  VC-dimension of  certain  classes  of  functions  computed  by  recurrent  neural  networks  with  feedback  connections ;  see   DasGupta  and  Sontag,  1996; Koiran and Sontag,  1998 .   Pattern  Classification  with  Real-Output  Part  two  Networks    9  Classification  with  Real-Valued  Functions  9.1  Introduction  The general upper and lower bounds on sample complexity described in Chapters  4 and  5 show that  the  VC-dimension  determines  the  sample complexity of the learning problem for  a function  class H.  The results of  Chapters  6 and  8 show  that,  for  a  variety  of  neural  networks,  the VC-dimension grows with the number of parameters.  In particular,  the lower bounds on the VC-dimension of neural networks described in Sec- tion 6.3, together with Theorem 6.5, show that  with mild conditions on the  architecture  of  a  multi-layer  network  and  the  activation  functions of its computation  units, the VC-dimension grows at  least linearly with the number of parameters.  These results do not, however, provide a complete explanation of the sample  size  requirements  of  neural  networks  for  pattern  classification problems.  In  many  applications  of  neural  networks  the  network  pa- rameters are adjusted  on the basis of a small training set, sometimes an order of magnitude smaller than the number of parameters.  In this case, we might  expect  the  network  to  'overfit',  that  is, to  accurately  match the  training  data,  but  predict  poorly  on subsequent  data.  Indeed,  the results from  Part  1 based on the VC-dimension suggest that the estima- tion  error could be large, because VCdim if   ra  is large.  Nonetheless, in many such situations these networks seem to avoid overfitting, in that the training set error is a reliable estimate of the error on subsequent ex- amples.  Furthermore, Theorem 7.1 shows that an arbitrarily small mod- ification  to the activation function  can make the VC-dimension  infinite, and it  seems unnatural  that  such a change should  affect  the  statistical behaviour of networks in applications.  One  possible  explanation  of  such  discrepancies  between  theory  and  133   134   Classification  with  Real- Valued  Functions  practice  is that  in the  theoretical  model we have discussed,  a  learning algorithm is required to perform well for any probability distribution.  In particular, in proving lower bounds on sample complexity, we used care- fully  constructed  probability  distributions,  and  it  seems  unlikely  that these  distributions  are  accurate  models for  applications.  It  could  well be the case that  learning problems that  occur commonly in applications are easier than the 'distribution-free' theory indicates.  However, it is not clear how we can take advantage of this, since, as argued in Section 2.1, in many applications of neural networks we have little knowledge of the process generating the  data.  Another  inadequacy  of the  learning  model  investigated  in  Part  1 is that  it  is not appropriate for  many neural network learning algorithms. For instance, suppose we wish to use a real-output  sigmoid network for binary  classification.  It  is natural  to  threshold  the  real  output  of  the network  to  obtain  a  binary  classification  label.  This  is  the  approach taken  implicitly  in Part  1 of this  book,  and  the  theoretical  results ob- tained  there apply.  However, many learning algorithms for  real-output sigmoid networks take into account the value of the real number that is computed by the network, and not simply its binary thresholded value. Such algorithms typically do not minimize the number of  misclassifica- tions over the training examples, but instead minimize the squared error of the  real-valued  outputs,  a procedure  that  can  lead  to  very  different behaviour.  An  algorithm  of  this  type  is unlikely  to  return  a  function whose thresholded  version makes a small number of classification  errors on a sample but  has its function  values all close to the threshold.   Such a function  would have real output  values that,  on the training sample, mostly fall on the correct side of the threshold, but  not by much. Algo- rithms that minimize squared error, as we shall see, tend not to produce such 'indecisive' functions.   Therefore, we cannot expect that algorithms of this type will produce a function  whose thresholded version has near- minimal  sample error.  For this  reason, the theory  of Part  1 cannot  be readily used to guarantee a nearly optimal misclassification  probability. In  the  next  seven  chapters,  we  investigate  an  alternative  model  of learning that  is more applicable to  situations where a real-output  neu- ral  network  is  used  for  binary  classification,  but  in  a  way  other  than by  simple  thresholding.  As  described  above,  if  one  simply  thresholds real-valued  functions  in  some  class  and  applies  the  theory  of  Part  1, then  all that  matters is whether  a function  gives a real number  output lying on the correct side of the threshold.  In the modified  approach, we take into consideration the  'margin' by which the function  value lies on   9.2  Large  margin classifiers   135  the correct side; that  is, we take into account the distance between the threshold  value and  the  real number  output  by the  network.  Such  an approach  enables  us to  exploit  the  fact  that  learning  algorithms  com- monly  used  for  neural  networks   such  as  gradient  descent  algorithms that  locally minimize squared  error   tend  to  produce classifiers  having a large margin on most  of the training examples.  In the new model of learning,  we shall  require that  the  output  hypothesis  of the  algorithm has misclassification  probability nearly as small as the smallest error of a  'large margin'  classifier  in the  class.  We shall see that  this  approach eliminates some of the peculiarities of the results obtained  in Part  1.  The new model is, in effect, a 'relaxation' of the learning model studied in Part  I, in the sense that  we weaken the  requirement  on the misclas- sification  error of the output  hypothesis.  By measuring error using the margins, we obtain bounds on misclassification  probability of the form  error <   estimate of error  +   complexity penalty ,  where the  error  estimate  is measured  using margins  and  the  complex- ity penalty  depends, for  instance, on the  size of the  parameters  in  the network, rather than  on the number of parameters.  In many cases, this new  model  explains  how  overfitting  may  be  avoided  even  with  train- ing sets that  are considerably  smaller  than  the  number  of  parameters. This does not contradict the lower bounds of Chapter 5: the complexity penalty  can  be  smaller  in the  new model,  because  in situations  where overfitting  might occur in the old model, the new error estimate would be large.  Loosely  speaking,  the  limitations  of the  learning  algorithms  that  they  cannot find functions  with  few  errors on the  training  data, since those functions  do not have large margins  effectively  counter  the deficiency  in  the  amount  of training  data.  Since the  new model  more accurately describes the types of learning algorithm used in practice, the results in this part  are likely to give a more accurate explanation of the performance  of these learning algorithms.  9.2  Large  Margin  Classifiers  Suppose  F  is  a  class  of  functions  defined  on  the  set  X  and  mapping to the interval  [0,1], such as the class of all functions  computable by a real-output  sigmoid network, for  example.  One way to use such a class for binary  classification  is to use 'thresholded'  versions of its  functions; that  is, to  work  with  the  class  of binary-valued  functions  of the  form h x   =  sgn   x   —  1 2 ,  and then  to apply the theory  of Part  1.  Here,   136   Classification  with Real- Valued Functions  however,  we take  a  different  approach,  in  which  we are  interested  in functions  that  classify  examples correctly, but  with  a large margin.  In this definition,     can be viewed as a function  that  maps to the interval [0,1]; we define  the notion  of margin  more generally because it  will be useful  later.  Definition  9.1  Let Z  =  X  x {0,1}.  If f  is a real-valued function  in  F, the margin  of f  on  x, y  € Z  is  Suppose 7  is  a nonnegative  real number and P  is a probability  distribu- tion on Z.  We define the error erp     of f  with respect  to P  and 7 as the probability  and the misclassification probability  of f  as  evp f   = P{sga f x -1 2    ^y}.  ,2   < 7 },  Clearly, if sgn   x   —  1 2   correctly  classifies  y,  the  margin  is nonneg- ative.  We can interpret  a large positive margin  as a 'confident'  correct classification.  Notice  that  erp        0,  and  that inequality can hold.  Recall that  a learning algorithm for  a class H  of binary-valued  func- tions  is  a  function  taking  a  training  sample  z  €  Z m  and  returning  a function  h in H  that,  with high probability  over Zm,  has error  satisfy- ing  erP h   <  inf  erP g   + e.  9&H  We can  define  a  classification  learning algorithm for  a class F  of real- valued functions  in an analogous way.  Definition  9.2  Suppose  that F  is a class  of functions  mapping from X into R and that Z  denotes X  x {0,1}.  A classification learning algorithm L  for  F  takes as input  a margin parameter 7  >  0  and a sample z G U £i   Z*>  and  returns a function  in F  such that, for  any e, 5 G   0,1  and any 7  >  0,  there is  an integer mo e,S,y   such that  if m  > mo e,<$,7  then, for  any probability  distribution P  on Z  =  X  x  {0,1},  Pm   erP L 7,*      1 -  5.   9.2  Large  margin classifiers   137  We define the sample complexity function  mi, e,<$,7  of L  as the small- est integer that  can be taken to be rao e,<J,7  in this definition,  and the inherent sample complexity m,F{t, S, 7  of the classification learning prob- lem for F  as the minimum over all learning algorithms L of m^fe, <$, 7 . As in the definition  of the binary classification  problem, we say that  an algorithm L has estimation error bound e m, 5,7  if, for all m, 5, 7, and P,  with probability at  least  1 — S over z € Z m  chosen according to  P m ,  erP L 7,*    <  inf  ery  p g  + e ra,<J,7 .  9€r  The estimation  error of L,  denoted  ex, m, J,7 , is the  smallest  possible estimation error bound.  Clearly,  if  F  is a  class of  {0, l}-valued  functions,  Definition  9.2  and the analogous Definition  2.1 for  learning binary-valued  function  classes are equivalent.  In deriving the results for  neural network classes in the previous chapters, we have, in effect,  considered the binary-valued class H =  {sgn  —1 2   :    € F}.  By considering directly the real-valued class F,  Definition  9.2 gives a potentially  easier learning problem, since now the learning algorithm needs only to find a function with misclassification probability close to the best error 'at  margin 7'.  This  definition  of learning is well-motivated  for  neural  networks.  In particular,  learning  algorithms  for  real-output  sigmoid  networks  typi- cally attempt  to find a function  in F  that  approximately minimizes the squared error on the training sample.  The following  proposition shows that  when there is a function  with small squared error, that  function  is also a large margin  classifier.  In order to  state the result,  we need one further  definition:  for  a function  f  in F  and 7 > 0, the  sample error of f  with respect to 7 on the sample z is  eilif   =  — {i: margin   xi ,2 i   < 7}!.  Proposition  9.3  For any function  f  : X  -»  E  and  any sequence  of labelled examples    zi,2 i ,...,{x m,ym    in  X  x {0,l}  m, if  1  m  mi=i  er2   <€  l 2- 7  2  then  for all 0 < 7 <  1 2.   138   Classification with Real- Valued Functions  Proof If marginC fo ,^      1 2-7.  So e  l 2 -  7 2 implies  It is possible to obtain a result analogous to Theorem 2.4 that provides a bound on the sample complexity of classification  learning with a finite class of real-valued functions.  However, in this case there is no advantage to  considering the  real-valued  nature  of the  class.  We shall  see in  the following  chapters  that  there  can  be  a  considerable  advantage  if  the function  class is infinite.  9.3  Remarks  When using a [0, l]-valued function  for pattern  classification,  we can in- terpret  the value of the function  as an estimate of the probability  that the  correct  label  is  1.  This  provides  extra  information  that  would  be ignored  if  we  simply  thresholded  the  function,  and  this  is  frequently valuable in applications.  For instance, in predicting outcomes of a med- ical procedure, it  seems sensible to  allow a classifier  to return  a  'don't know' value, rather  than  forcing it  to predict either  a 0 or a  1.  In such cases, we can consider two kinds of errors: those where the classifier says 'don't  know',  and  those  in  which  it  makes  a prediction,  but  is wrong. For a class of [0, l]-valued functions,  it is reasonable to interpret a value near the threshold   say, within 7  as a 'don't  know' value. In that case, erj,  measures the sum of the probabilities of these two kinds of errors. Although the results described in the following chapters give bounds on erp in terms of the infimum  over F  of er£   , it is easy to extend these results to obtain bounds on er£.  9.4  Bibliographical  Notes  The  idea  that  studying  margins  might  provide  an  explanation  of  the surprisingly  good  generalization  performance  of neural  network  classi- fiers  was first described in  Bartlett, 1998 , but the definition of learning presented in this chapter is new.  This work builds on a large collection of results and intuition  for linear classifiers.  Many authors   see, for ex- ample,  Duda and Hart, 1973   have suggested building linear  classifiers   9.4  Bibliographical  notes   139  with  large margins; part  of the  motivation  seems to  have been  robust- ness to noise in the input variables.  Vapnik  1982  studied combinatorial properties of large margin classifiers.  Boser, Guyon  and Vapnik   1992  proposed  algorithms  for  large  margin  linear  classifiers.  Shawe-Taylor, Bartlett, Williamson and Anthony  1996  gave an analysis of linear clas- sifiers with large margins. The results in  Bartlett, 1998  also have tech- nical  ancestors  in   Lee,  Bartlett  and  Williamson,  1996; Lee,  Bartlett and Williamson,  1995b    see also  Koiran,  1994   and in   Anthony  and Bartlett,  1995 , as well as in  Shawe-Taylor et aL, 1996 —we shall meet some of these results in later chapters.  The observation  Proposition 9.3  that minimizing squared error gives good margins was made in  Bartlett,  1998 .   But see  Sontag and Suss- mann, 1989  for an illustration that minimizing squared error using gra- dient descent can be problematic, even for simple problems.   The model described in this chapter is related to the problem of learn-  ing probabilistic  concepts   see   Kearns and  Schapire,  1994  .  A proba- bilistic  concept  is  a  function  defined  on  an  input  domain  X  mapping to  the  interval  [0,1].  The  value  of  the  function  at  a  point  a;  6  I  is an estimate of the probability  that  the label associated  with  x  is 1.  In this model, a learning algorithm aims to produce a probabilistic concept from  some  class  that  approximately  minimizes  the  expected  value of some loss function.  Typical loss functions  studied include the  quadratic loss function,   f x ,y   H>  f x   -  y 2,  and  the  absolute  loss  function    x ,j    H-> \f x   -  y\.  The quantity  er£  defined  in this  chapter  is the expected value of the loss function   f x ,y   H-»   l\f x -y\>i 2-^,y .   10  Covering Numbers and  Uniform  Convergence  10.1  Introduction  Just  as  in  Chapter  4,  we shall  show that  in  many  cases  an  algorithm returning  a function  with  small sample error will constitute  a  learning algorithm   where, in the present  context, the sample error is measured with  respect  to  a  certain  margin .  The  details  will be  given  in  Chap- ter  13.  As  in  the  theory  developed  earlier  for  the  case  of  classes of functions  mapping  into  {0,1},  we first  obtain  a  'uniform  convergence' result,  analogous to Theorem 4.3.  The notion of  covering  numbers will prove crucial in our analysis.  10,2  Covering Numbers  Measuring the extent  of a function  class  Theorem  4.3 concerns  a  class of functions  that  map  from  some set  X into {0,1}, and it involves the growth function  IIJJ TO .  Recall that  if  5 is a finite subset  of X,  then  ifs  denotes the restriction of H  to 5,  and that  the growth function  UH is given by  nH m   = max{ifs  : 5  C X  and  5 = m} .  Since H  maps into {0,1}, H\s  is finite for every finite 5.  However, if we consider  a  class  F  of  real-valued  functions,  we encounter  the  problem that  even for finite 5,  F\s  may be infinite.  Thus  we cannot  make use of the  cardinality  of F\s  in obtaining  a uniform  convergence result  for real function  classes.  In fact,  we only need the  set  F\s  to have not  too many elements that  are very different;  instead of cardinality, we use the notion of covers  to measure the 'extent' of  F\s.  140   10.2  Covering  numbers  141  ' : f t ^ '^  Fig.  10.1.  The centres of the doo balls form an e-cover of the shaded region, F U C R 2  Covering  numbers  for  subsets  of  Euclidean  space  Given  W  C R*  and  a positive real number  e, we say that  C  C Rk  is a doo e-cover for  W  if C CW  and for every w G W there is a v G C7 such that  {\wi-Vi\   :i  =  l,...,fc}  < e.  In  other  words,  given  any vector  w  in  W,  there  is  some  vector  in C whose  distance from w is less than e, where distance is measured  using the metric  doo on E*, defined by  x 2,...,a;* ,  2 1,2 2,.  ., Vk   =  max \xi -  y*.  For t; G  K* and c>  0,  define the open set  ball at v of radius e to be the  {ueRk  Then we could also define an c-cover for W  C Rk  as a subset C of W for which W is contained in the union of the set of open  do  balls of radius e  centred  at the points  in C.  Figure  10.1 illustrates  this  idea.  We  say   142   Covering Numbers and Uniform  Convergence  that  W  is totally bounded if for each e > 0 there is a finite e-cover for  W. In  this  case, we definef  the  doo e-covering  number of W,  Af e, W,doo , to be the minimum cardinality of a doo e-cover for  W.  Uniform  covering numbers for  a function  class  Suppose that  F  is a class of functions  from  X  to  R.  Given a sequence x =   xi, x2,...,Xk   £ Xk,  we let Fa  be the subset of R* given by   If  the  X{ are  distinct,  there  is  a  clear  one-to-one  correspondence  be- tween F\x  and the restriction Fs  of F  to the set 5  =  {xu  x2,      , ^n}  Q X.   For  a  positive  number  e,  we define  the  uniform  covering  number  Afoo  e, F, fc  to be the maximum, over all x  € Xk,  of the covering num- ber Af e,F\m,doo   and we take it  to be infinite  if there is no bound on these covering numbers ; that is,  JVOO  e, F, k  = max {Af e, Fu,dec   : x  G Xk   .  The covering number A Jx>  e, F, k   can be interpreted  as a measure of the richness of the class F  at the scale e.  For a fixed value of e, this cov- ering number—and in particular how rapidly it grows with k—indicates how much the set F\m  'fills up' Rk,  when we examine it at the scale e. For example, suppose that  functions  in F  map to the interval  [0,1],  If there is a vector  x  in  Xk  with  F\a  =  [0,1]*, then Afoo  e,F, k   grows roughly as   1 e *,  and  this  is  the  fastest  growth  possible.  However,  we shall see that  if functions  in F  are bounded  and F\x  is always contained  in a linear subspace of Rk  of dimension d, then .Afc  c, F, k  grows roughly as  fc c d,  which is polynomial in  k.  We shall see in the next  section  that the  rate  of growth  of these covering numbers  with  k  is crucial, just  as the rate of growth of Ilif  A;  with k was crucial for binary  classification. In fact, the uniform covering number is a generalization of the growth function.  To see this, suppose that functions in H map into {0,1}.  Then, for all x  e  Xk,  H\m  is finite and, for all e <  1, Af e,H\x,d^   =  \H\m\,  so  t  We explicitly  write  doo  in the  definition  since we shall  see that  it  is useful  to  be  able to define  covering numbers in terms of other notions of distance.   10.3 A  uniform convergence  result   143  10.3  A  Uniform  Convergence  Result  In this section we present  and prove a uniform  convergence result for  a class of real-valued functions.   Strictly speaking, it is not a uniform con- vergence result,  since it  gives uniform  one-sided  bounds on error prob- abilities.   The proof of this result  is in many ways similar to the proof of  the  corresponding  result  in  Chapter  4,  Theorem  4.3, although  the technical details are slightly more intricate.  Theorem  10.1  Suppose that F  is a set of real-valued functions defined on the  domain X.  Let P  be any probability  distribution on Z  =  X  x {0,1},  e any real number between 0  and 1, 7  any positive real  number, and m  any positive integer. Then  Pm  {eip f   > er*    + e for  some f  in  F}  Notice how this compares with Theorem 4.3:  that bound involves the growth function,  whereas this involves the covering number.  Since the covering number is a generalization of the growth function, this result is, in a sense, a generalization of a one-sided version of Theorem 4.3.  Recall that Theorem 4.3 is useful when II ra   increases polynomially with  TO, and this is guaranteed when the VC-dimension of H is finite.  In the same way, Theorem 10.1 shows that the probability that the sample error does not provide a uniformly  accurate error estimate decreases to zero as the training  sample  size  m  increases,  provided  that  the  covering  number Afoo  7 2, F, 2m  does not grow too rapidly with m.  In particular, if the covering number grows only polynomially with m, this probability goes to zero exponentially quickly as a function  of m.  We now embark  on the  proof  of Theorem  10.1.  This  uses the  same key  techniques  as  the  proof  of  Theorem  4.3, namely  symmetrization, permutation,  and  reduction  to  a  finite  class.  Having  presented  some high-level justification  for  these  techniques  in  Chapter  4,  and  having given many  of the  essential  definitions,  we proceed  a little  faster  with the present proof.  Symmetrization  First, we have the following result, which allows us to consider a sample- based estimate of  eip f .   144   Covering Numbers and Uniform  Convergence  Lemma  10.2  With the notation as above,  let  Q = {zeZm:   some f  in F has erP{f   > erj    + e}  and  Then for m > 2 e2,  R = { r, s  e Zm x Zm : some f  in F has &,     > &?    + e 2}.  Pm{Q   <  2P2m R .  Proof The proof closely follows the proof of Lemma 4.4.  If some    e F satisfies erP    > &?    + e and er5    > erP    -  e 2,  then it satisfies ev8 f  > erj f   + e 2, so  P2m R   >  P 2 m{3 €F,er P   >er7   -heand  * .      >  er P   -c 2}  Pm{s:3feF,eip f >eiJ f   +  [  Q  * .        > erp    -  e 2} dPm r .    10.1   As in the  proof of Lemma 4.4,  we can use  Chebyshev's  inequality to show that  P"1 {*.     > C T P       - C   2 }>  1 2  for all    € F, shows that P2m R   > Pm Q  2.    provided that m > 2 e2,  and this, together with  10.1 ,    Permutations  As in Chapter 4, we now bound the probability  P2m R   in teri^is of a probability involving permutations of the sample labels.  Recall that Fm is the set of permutations a on {1,2,..., 2m} that switch elements from the first and second  halves   that is, {cr i ,a i + m }  =  {i,i + rri   for i = 1,..., m .  Recall also that we can regard a permutation a as acting on an element of £T2m, so that  Lemma 4.5 shows that  P2m R   = EPr az  G R  <  ma^ Pr az 6 R ,    10.2    10.3 A  uniform convergence  result   145 where the  expectation  is over z  in  Z2m  chosen according to  P 2 m,  and the  probability  is over  permutations  chosen  uniformly  in  Fm.  So  the problem reduces to estimating this latter  probability.  Reduction to  a finite class  In  the  analogous step  of the  proof  of Theorem  4.3, we used  the  union bound  to  show that  the  probability  that  az  €  R  is no more than  the sum  over all elements of F\g of some probability  involving an  element. As noted  earlier  in  this  chapter,  the  corresponding  restriction  here  is typically infinite,  so instead we approximate F\9 by using a cover.  Lemma  10.3  For the set R  C Z2m  defined in Lemma 10.2, and for a permutation a  chosen uniformly at random from F m,  max  Pr <7* eR <Afoo   7 2, F, 2m  exp   -^     .  Proof  Fix  z  =   2i,...,22m   in  Z 2 m,  where  Zi  =   xi,yi .  Let  x  =  xi,...,X2m   and  fix  a  minimal  7 2-cover  T  of  F^.  Clearly,  \T\  = Mr j 2,F\9,doo   <  jVoo 7 2,F,2m .  Then  for  all     in  F  there  is  an f  in  T  with    xi   -   i  <  7 2  for  1  <  i  <  2m.  Now,  if  az  €  R then, by definition,  some f  in F  has er*     >  evj f   + e 2, where r  =  Mi '       >Mm    and  s  =   Mm+i >       >M2m  -  By  the  definition  of er,     and e r ^    ,   this    satisfies      ^  tfirW }  ~  I{m + 1 < i < 2m : sgn       ^   Q   -  1 2   ^  tfirW }   g     ^ Q    I{   >  — {1 < i < m  : margin {f xa{i  ,ya{i    < *y}\ +  -.  Now, if sgn   x  -  1 2   ^  y then margin   x ,y   < 0, so this implies  — \{m  +  1  »* *    ^   }  >  — {1  »*«      +  -   10-3   Choose     €  r  such  that,  for  all  i,  \f {  -    a?i   <  7 2.  It  follows  that if  margin  i,2 i   <  7 2,  then  margin   a;j ,2 j   <  7.  Furthermore,  if margin   xi ,2 i   <  0 then  we have margin  i,2 i   <  7 2.  Combining these facts, we see that  if az  e  R  then there is some    G T such that  — {m + 1 < i < 2m : margin   „ « ,y^    < 7 2}   146   Covering Numbers and Uniform  Convergence  >  — {l  < i  < m  : margin  jaii ,V*li    < 1f 2}\  +  -  If we define  v   f  f-\ =  J  !  U'  then it follows that  ;   Pr az  e  R   if  margin  i, yt   < 7 2  \  0  otherwise,     , a i     > « 2 J .  The union bound then implies that  this probability is no more than  \T\ ou» Pr f 1  f;   «  ,  r m + i   - v f,  = \T\ DMKPr f i  f;  «  ,i  -  t»  ,n» + i \ Hi > e 2J ,  where the final probability is over the  ft,  which are independently  and uniformly  chosen from  {—1,1}.  Hoeffding's  inequality   see Appendix 1  shows that this probability is no more than exp —e2m 8 , which implies D the result.   Theorem  10.1  now  follows  from  Lemma  10.2,  Inequality   10.2 ,  and Lemma  10.3.  Indeed,  it  is trivially  true for  m  < 2 e2,  because in  that case the right-hand side of the bound is greater than  1, so we can apply Lemma 10.2.  In fact,  Theorem  10.1 can be improved  in the case that  functions  in F  take values that  are far  from  the threshold value of 1 2.  Clearly, the behaviour  of  a  function  in  F  is  irrelevant  in  a  region  where  its  value is  far  from  1 2.  If  functions  in  F  are  very  complex  in  such  regions, the  covering  number  of  F  might  be  an  overly  pessimistic  measure of the  complexity  of  F.  Instead  of  F,  consider  the  class  TT7 F ,  where TT7 : R -> [1 2 -  7,1 2 + 7]  satisfies  { I 2 + 7   I   2 -7   i f a>  I 2 + 7 if a  <  1   2 -7   a   otherwise,   10.4    see Figure  10.2 , and  TT7 F   =  {TT7 O    :    £  F}, where   g o f  x   = g f x  .  It is clear that the analysis carried out in the above proof is still true when    is replaced by ?r7 o  ,  and hence we can use a cover of TT7 F    10.4  Covering numbers in general  147  2  i -7  1  2   , _. 1  2 +7  Fig.  10.2.  The graph of the function  ?r7 : R -> [1 2 -  7,1 2  + 7].  rather  than  F.  Clearly,  the  covering numbers  of  TT7 F   are  no  larger than those of F, and can be considerably smaller.  For example, if there is a point  x  €  X  for  which the set  {f x   : f  €  F}  C R is  unbounded, then jVoo  7 2, F, 1  =  00 for all 7 > 0, whereas A ^  7 2, TT7 F , 1  < 4. Thus, the following theorem is an improvement of Theorem 10.1.  Theorem  10.4  Suppose that F  is a set of real-valued functions defined on a domain X.  Let P  be any probability distribution onZ  =  Xx{0,l}, e  any real number between 0  and 1, 7  any positive real number, and m any positive integer. Then  Pm  {erp f   > erj     + e for  some f  in  F}  Q,7r7 F ,2m   exp   -  10.4  Covering  Numbers  in  General  The notion  of covering numbers can be defined  quite generally,  and  in subsequent  chapters  we shall  use  several  notions  of  covering  numbers other  than  those  discussed  earlier  in  this  chapter.  We  now  give  the general  definition.  Recall that  a metric space  consists of a set A together with a  metric,   148   Covering Numbers and Uniform  Convergence  d, a mapping  from  A x A to the nonnegative  reals  with  the following properties, for all x,y,z  € A:   i   d x, y  = 0 if and only if x = y,  ii   d x,y   = d{y,x ,  and  iii   d z,2 <d x,2  For e > 0, and for W C A, we say that  C C A is an e-cover of W with respect to d if C C W  and for every w G  W there is a v G  C such  that d it;, v   < e.  In other  words, the union of the set of d balls of radius e centred at the points in C contains W.  We say that W is totally bounded if for each e > 0 there is a finite e-cover for W.  In this case, we define the e-covering number of W, Af e, W, d , to be the minimum cardinality of an e-cover for W  with respect to the metric d.  Of course, the definition just given subsumes the one we used earlier in the chapter for d = doo. There are many other metrics—and hence, types of  covering  number—on  Rk.  Of particular  relevance  are the covering numbers corresponding to the metrics d\ and d2, given by  and  In  Section  10.2 we defined  the uniform  covering numbers .A ^  e,F,  fc , for  a  function  class  F.  In an analogous  way, we may define  uniform covering numbers corresponding to the d\ and d%  metrics: we define  Mp  e, F, k  = max {^ e, F\.,dp   : x €  Xk}  for p =  1,2, oo.  Jensen's inequality   see Appendix  1  shows that  and it is immediate  that  * *,»   < *   * , »  ,   d2{x,y <doo x,y .    10.5    10.6   Hence, we have the following  result.   10.5 Remarks   149  Lemma  10.5  For any  class F  of real-valued functions  defined on  X, any e > 0,  and any  i G N,  10.5  Remarks  Pseudo-metric  spaces  It is possible to define general covering numbers with respect to a pseudo- metric rather than a metric.  A pseudo-metric d satisfies the second and third conditions in the definition of a metric, but the first condition does not  necessarily  hold.  Instead,  d xyy   > 0 for  all x,2 , and  d x,x   =  0, but  we can have x ^  y and d x, y  = 0.  Improper coverings  Recall that,  if   A, d  is a metric space and  W  C A, then, for  e > 0, we say  that  C  C  A is  an  e-cover  of  W  if  C  C  W  and  for  every  w  G W there  is  a  v  E  C  such  that  d iu, v   <  e.  If  we drop  the  requirement that  C  C  W  then  we say  that  C  is an  improper cover.   The  type of cover we defined earlier is sometimes called a proper cover.   We use this definition  of cover because it is often  more convenient, but the following lemma  shows that  the  corresponding  definition  of  covering  number  is closely related to the one used in this  chapter.  Lemma  10.6  Suppose that W  is  a totally bounded  subset of a metric space   A, d .  For e > 0,  let Af' e, W, d   be  the minimum  cardinality  of a finite  improper e-cover for W.  Then,  e, W, d  < Af' e,  W, d  < N e, W, d   for  all e > 0.  Proof  The  second  inequality  is  immediate  since  any  proper  cover  is also an  improper  cover.  For the first inequality,  suppose that  C  is an improper c-cover for W of mimimum cardinality.  Then, since no element of C  can be removed  since it is minimal  without leaving some element of  W  'uncovered',  it  must  be  the  case  that  for  each  v1  G C  there  is c v'   e  W  with  d c v' ,vf   < e.  Consider the set  C =  {c{v'  : v1 G  C"}. We claim C is a proper 2e-cover for W.  Certainly, C C W.  Furthermore, for  w G  W, there is v1 G  C  such that  d w,v'   < e, and by the  triangle   Covering Numbers and Uniform  Convergence  150  inequality, d iu,c v;    < d w, v' +d t ,c t     < e+e.  The result follows.    10.6  Bibliographical  Notes  Theorems  10.1 and  10.4 are from   Bartlett,  1998 .  The proofs of these theorems  use  ideas  due  to  Vapnik  and  Chervonenkis   1971    see  also Section  4.7   and  Pollard   1984 .  Kolmogorov  and  Tihomirov   1961  give many useful  properties of covers, including Lemma 10.6.   The Pseudo-Dimension  and  Fat-Shattering  11  Dimension  11.1  Introduction  Chapters 4 and 5 show that  the Vapnik-Chervonenkis dimension is cru- cial in characterizing  learnability  by binary-output  networks, and  that it  can  be  used  to  bound  the  growth  function.  Chapter  10 shows  that covering numbers are a generalization of the growth function  useful  for analysing  classification  by  real-output  neural  networks   or,  more  gen- erally,  by real-valued  function  classes .  We see later  in the  book  that covering numbers are also important in analysing other models of learn- ing.  It  is  natural  to  ask  whether  there  is  a  Combinatorial'  measure analogous to  the  VC-dimension  that  can  be  used  to  bound  the  cover- ing numbers  of a  class of real-valued  functions,  and  hence to  quantify the  sample  complexity  of  classification  learning.  This  is  largely  true, although  the  definitions  and  proofs  are more complicated  than  for  the binary  classification  case.  In this chapter  we introduce the key 'dimen- sions' that  we use in our analysis of learning with  real function  classes and establish some associated basic results and useful properties.  In the next  chapter we show how these dimensions may be used to bound  the covering numbers.  11.2  The  Pseudo-Dimension  The definition of the pseudo-dimension  To introduce the first of the new dimensions, we first present a slightly different  formulation  of the  definition  of the  VC-dimension.  For  a  set of  functions  H  mapping  from  X  to  {0,1},  recall  that  a  subset  5  = {xi,X2,.. .xm}  of X  is shattered  by H  if H\s  has cardinality  2m.  This means that  for  any binary vector 6 =   6i, 62,..., bm   G  {0, l } m ,  there is  151   152   The Pseudo-Dimension and Fat-Shattering Dimension   n  00  Fig. 11.1. The set {xi,^} C R is shattered by the class F of affine  functions on R, F = {x H> ax + 6 : a, 6 € R}.  The points n, r2 witness the shattering.  some corresponding function  hi, in H  such that    , . . ., hb xm    =  ft,  or, in other words, such that   i& xt  =  &* for all  i.  For a  class F  of real-valued  function,  we may modify  this  definition of shattering as follows.   Recall that  sgn a   takes value 1 if a  >  0 and 0 otherwise.   Definition  11.1  Let F  be a set of functions mapping from a domain X to R  and suppose  that S  =  {xi,X2,...,a:m}  C X.  Then S  is pseudo- shattered  by F  if there are real numbers r\, r2,..., rm  such that for each b e  {0, l }m  there is  a function  fa  in F  with sgn fb xi   -  r<   =  6* for 1 < i < m.  We say that r =   ri, r2,..., rm   witnesses  the shattering.  Thus,  5  is pseudo-shattered  if  there  are  numbers  ri,r2,...,rm  and 2m  functions  fb  that  achieve  all  possible  'above below'  combinations with  respect  to  the  r,.   See Figure  11.1.   Often  we will simply  write 'shattered'  when  we  mean  'pseudo-shattered'.  Corresponding  to  this notion of shattering, we have the pseudo-dimension.  Definition  11.2  Suppose that F  is  a set  of functions from  a domain X  to  R.  Then  F  has pseudo-dimension  d  if  d  is  the  maximum car- dinality of  a subset S  of X  that  is pseudo-shattered  by F.  If  no such   11.2  The pseudo-dimension   153  maximum  exists,  we  say  that  F  has  infinite  pseudo-dimension.  The pseudo-dimension of F  is denoted Pdim F .  Other interpretations  of pseudo-dimension  There  are  at  least  two other  useful  ways of defining  pseudo-shattering and the  pseudo-dimension.  First,  for  any     G F,  let  Bf  be the  indicator  function  of the  region  below or on the graph of  ;  that is,  Then  the  pseudo-dimension  of  F  is precisely  the  VC-dimension  of  the subgraph  class BF  =  {Bf  :    G F}.  Second, for any x € X m,  define  *.  =  {   *i ,  *2 ,       , f xm     :f€F}CW n.  Then the set  {x\,  X2,..., xm} is pseudo-shattered if this subset of Em  is sufficiently  'extensive' in the following geometrical sense.  Suppose that for a given r  G  Em  and W  CW1,  denotes the  translate of W  by r,  and  define  the  2m  orthants of Em  to be the sets of points   yi,2 2>       >2 m  defined  by m  inequalities, with in- equality i  being either  yi  >  0 or yi  < 0.  Then  the set  {xi,X2,... ,xm} is pseudo-shattered by F  if and only if some translate F\m  + r  of F\m  in- tersects all 2m orthants of Rm.  This interpretation of pseudo-dimension is useful  in some proofs.  Pseudo-dimension  and compositions with non-decreasing  functions  The following theorem shows that composing the function  computed by a neural network with  a non-decreasing function   such as the  standard sigmoid, or the step function   does not increase its pseudo-dimension.  Theorem  11.3  Suppose F  is  a class of real-valued functions  and a  : E -4 R is a non-decreasing function.  Let  J F   denote the class {a o f  : feF}.   Then Pdim a F   < Pdim F .   154   The Pseudo-Dimension and Fat-Shattering Dimension  Proof  For d < Pdim a F  ,  suppose  {vofb:be{0,l}d}Ca F   pseudo-shatters a set 5 =  {zi,...,xj}   C X, witnessed  by  ri,...,r^ . Then, by suitably relabelling the  &, for all b G  {0, l}d and i G {1,..., d}, we have sgn   ? fb xi   -  r$  = 6*.  Take  Vi = min { 6 xi   : a fb xi    > r<, 6 €   {0,l}d}  for  i  =  l,...,d.   Notice  that  the minimum  exists,  since the set is fi- nite.   Then,  since a is non-decreasing,  it is straightforward  to verify that  sgn  fb xi   -  yi  = 6« for b G  {0, l}d and i G {1,...,  d .     Pseudo-dimension  and linear dimension  We now present  a quite general result  that  is useful  in estimating the pseudo-dimension  of many  function  classes.  Recall  that  a class F  of real-valued  functions  is a vector  space if for all  , g  G F and any two real numbers A and  x, the function  Xf +  ig belongs to F.  The following result links the pseudo-dimension to the linear dimension.  Theorem  11.4 If F  is a vector space of real-valued  functions  then Pdim F  = dim F .  Proof  For the class BF of 'below-the-graph' indicator functions  defined above, Pdim F  = VCdim J5F .  But  BF  = { *,»  *  sgn   aO -  y  :    G F},  so the functions  in Bp are of the form  sgn   + p , where    is a func- tion  from  the vector  space  and g is the fixed function  g x,y   =  — y. Theorem 3.5 now shows that  VCdim £F  = dim F .     Many of the function  classes discussed in this book contain  functions that map into some bounded range, and hence cannot be a vector space. However, if the class is a subset of a vector space, the following imme- diate corollary can be applied.  Corollary  11.5 If F  is a subset of a vector space F1 of  real-valued functions  then Pdim F  < dim F' .   11.2  The pseudo-dimension   155  Linear computation units  and polynomial  transformations  Theorem 11.4 immediately enables us to calculate the pseudo-dimension of many function  classes. In this section, we consider linear functions of certain fixed  transformations  of the inputs.  Suppose that  F  is the class of affine combinations of n  real inputs of  the form  where Wi eR  and x  =   a?i,... ,xn   6  W1 is the input  pattern.  We can think of F  as the class of functions  computable by a linear computation unit, which has the identity function  as its activation  function.  Theorem  11.6  Let  F  be the  class of real functions  computable  by a linear computation  unit  on En.  Then Pdim F   = n + 1.  Proof  It  is  easy  to  check  that  F  is  a  vector  space.  Furthermore,  if 1  denotes  the  identically-1 function  and,   * is the  ith  co-ordinate pro- jection,  fi x   =  Xi, for  1  <  i  <  n,  then  B  =  { i, 2,... ,  n, l}  is  a basis of F.  To see this, notice that  any function  in  F  is a linear  com- bination  of  the  elements  of  B,  so it  remains  only  to  show  that  these functions  are linearly  independent.  Suppose, then,  that  the  constants « i , a2, . .. , an +i  are such that     =  a ji  + a2 2  +        + an n  +  an+il is  the  identically-0  function.  Then,    0   =  0 implies  that  an+i  =  0 and    ei   =    e2   =        =  f en   =  0 imply  that  a\  =        =  an  =  0, where e%  has entry  i  equal to  1 and  all other  entries 0.  It  follows  that dim F   =  n +  1 and hence, by Theorem  11.4, Pdim F   = n + 1.     We saw earlier in the book that the VC-dimension of the class of  binary- valued   functions  computed  by the  boolean  perceptron  is the  same  as that  computed  on  real  inputs.  The  following  theorem  shows that  the corresponding statement  is true of pseudo-dimension.  Theorem  11.7  Let  F  be the  class of  real functions  computable  by a linear computation  unit  on {0, l }n.  Then Pdim F   =  n + 1.  Proof  We need only note that in the proof of the previous theorem, the proof of the linear independence of the functions   i,  2, . . .,  n, 1 involved values of x   the  all-0 vector  and  the  e»  that  are  in  {0, l }n.  Thus  the functions  form  a basis of the space of functions  computed  by the linear   The Pseudo-Dimension and Fat-Shattering Dimension  156  computation  unit  on {0, l } n.  This space therefore  has linear dimension and pseudo-dimension  n +  1.     As a  generalization  of the  class of functions  computable  by  a  linear computation  unit,  we can consider the  class of polynomial  transforma- tions.  A polynomial transformation  of En  is a function  of the form  f x   =wo  + w\ \{x  + w2 2 x  +       + wi i x >  for  x  6  Rn,  where I is some integer and for each i between  1 and  Z, the function  fa  is defined  on W1 by  for some nonnegative integers r^-.  The degree of fa is rn + U2 H  h rin. We  say  that  the  polynomial  transformation     is  of  degree  at  most  k when  the  largest  degree of  any  fa  is at  most  k.  Thus  the  polynomial transformations  of degree at  most  one are precisely the functions  com- puted  by  a  linear  computation  unit  and,  for  example,  the  polynomial transformations  of  degree  at  most  two on  R3  are  the  functions  of  the form  f x   =  Wo + WiX\ + W2X2 + W3X3 + WAX\ + W5X2 +   I  W7XiX2 + WsXiXs + W9X2X3.  Theorem  11.8 Let F  be the class  of all polynomial transformations on Kn  of degree  at most k.  Then  Proof  It  is easy to see that  F  is a vector space, so to prove the  result we exhibit  a basis for F  of size   n£* .  Let us denote the set {1,2,..., n} by [n], and denote by [n]k the set of all selections of at  most  A: objects from [n] where repetition  is allowed. Thus, [n]k may be thought of as a collection of 'multisets'.  For example, [3]2 consists of the multisets  0, {1}, {2}, {3}, {1,1}, {2,2}, {3,3}, {1,2}, {1,3}, {2,3}.  For each T  e  [n]k, and  for  any x  =   £i,a;2,       ,^n   €  K n,   T  denotes the  function   11.2  The pseudo-dimension   157  with repetitions as required, and with the convention that  f l i ^ ^i  =  1- For example, 0{1 2»3} x  =  £1X2X3, <^ljl'2* x   =  xfx2,  and <ffi x   =  1. It is clear that the functions fa used in the definition of the polynomial  transformations  may be written  in the form  fa  =   T  for  some multiset T.  Therefore  a function  defined  on En  is a polynomial  transformation of degree at  most  k if and  only if there  are constants  WT,  one for  each T  G  [n]*, such that    x  =  £  wT T x .  It  follows  that  F  is  a  vector  space  of  functions,  spanned  by  the  set £ n,fc   =  { T  : T  G  [n]k}.  We prove  by  induction  that  this  set  is linearly independent.  The base case  n =  1  follows from  the fact  that the functions  I,x,x2,x3,...  ,x* are linearly independent.  Suppose now that  the assertion is true for  a value of n  > 1 and let  k be any positive integer.  By  the  inductive  assumption,  the  set  {<j T  :  T  e  [n]k}  is  a linearly  independent  set.  For  0 <  j  < fc, let  Tj  C  [n +  1]* be  the  set of selections  containing  n +  1 exactly  j  times.  Suppose  that  for  some constants ar,  for all x  G En + 1,  Then,  TG[n+l]fc  i=o  T€T,  for  all x, where, for  T  G  J ,  <f  2   is 0T X   w^h  the j  factors  equal  to xn+i  deleted.   So,  J  G B n,fc .   It  follows,  from  the  linear  indepen- dence of the functions  1, xn+i, x%+1,...,  x*+1, that for all x\,  X2,...,  xn, we have  for each j.  But the inductive assumption then implies that for all j  and for all T  G Tj, ar  =  0; that is, all the coefficients  ar  are zero.  Hence the functions  are linearly independent.  It follows that dim F   =  \B n, k \  = Ml- It  remains to show that  [n]k consists of   n£     multisets.  To see this, let  us  represent  a  multiset  having  k{ copies  of  element  i  as  a  binary vector  consisting  of  k\  ones,  then  a  zero,  then  &2 ones,  then  a  zero,   The Pseudo-Dimension and Fat-Shattering Dimension  158  ...,  then  kn  ones, then  a zero, then   k -  Y%=i fe  o n e s-   For example, {1,1,2,4,4,5} £  [5]6 is encoded as 11010011010.   This defines a one-to- one correspondence between  [n]k and the subset  of vectors in  {0,  l}n+k with exactly  k ones. It  follows  that    The following result  shows that  for  k > 1, the pseudo-dimension  of the class  of  polynomial  transformations  defined  on  {0, l } n  is  strictly  less than  that  of the  class defined  on En.   We have seen  above that  these dimensions are equal when k =  1.   Theorem  11.9  Let F  be  the class  of all polynomial transformations on {0, l } n of degree  at most k.  Then  Pdim F   =  Proof The proof involves showing that a subset of the set JB n, k  defined in  the  proof  of  the  previous  theorem  is  a  basis  for  F.  If  we  restrict attention  to  binary  inputs,  then  any  terms   j>T  in  which  T  contains  a repetition  are redundant,  simply because for  x  =  0 or  1, xr  =  x  for  all r.  We shall denote the  set  of all subsets of at  most  k  objects  from  [n] by  [ri\ k\  Any  T  £  [n]W contains  no repetitions.  For  example, consists of the sets  Clearly,  [n]W consists of ]C*=0  ?   s e t s-  ^n  v*ew   f  *^e  redundancy of repetitions in the binary-input case, the class F  of polynomial  transfor- mations on  {0, l } n is generated by the set  To show that this is a basis, suppose that for some constants ar  and for a l l x € { 0 , l } n ,  A x =   ]T   aT<£T z  = 0. T6[n] fc>  Set  x  to  be  the  all-0  vector  to  deduce  that  ctQ  =  0.  Let  1  <  I  <  k and  assume, inductively,  that  ar  =  0 for  all T  C [n]  with  \T\  < I.  Let   11.3  The fat-shattering dimension   159  T  C [n]  with  \T\ =  I.  Setting x{  equal to  1 if i  G  T  and  0 if i  & T, we deduce  that  A x   =  OLT  =  0.  Thus  for  all T  of cardinality   ,  ar  =  0.   Hence ar  =  0 for all T, and the functions  are linearly independent.   11.3  The Fat-Shattering  Dimension  Definition  of  the fat-shattering  dimension  Recall that  a set  5  =  {xi,X2>       >m} is shattered  by a function  class F  if  there  are  numbers  ri,r2,...,rm  such  that  all  2m  'above below' combinations  with  respect  to  the  r»  are  possible.  We  can  make  this condition more stringent by requiring not merely that the function values on the X{  be either above r* or below r», but  above or below by at  least a certain  'clearance'.  The following definition  formalizes this.  Definition  11.10  Let F  be a set  of functions  mapping from  a domain X  to R  and suppose  that S  =  {xi,X2>       >^m} Q X.  Suppose also  that 7  is a positive real number. Then S  is 7-shattered  by F  if there are real numbers r\, r2,..., rm  such that for  each b G  {0, l }m  there is a function fb  in F  with  fb xi   > n  + 7 if bi =  1, and fb{xi   < u  -  7 if 6* =  0,  for 1 < i <  m.  We say that r =   ri,r2,... ,rm   witnesses  the  shattering.  Thus, 5  is 7-shattered  if it  is shattered  with  a  'width  of shattering' of at  least  7.   See  Figure  11.2.   This  notion  of  shattering  leads  to  the following dimension.  Definition  11.11  Suppose  that F  is a set  of functions from  a domain X  to R  and that 7 > 0.  Then F  has 7-dimension  d if d is the maximum cardinality of a subset S  of X  that is ^-shattered by F.  If no such maxi- mum exists, we say that F  has infinite ^-dimension.  The ^-dimension of F  is denoted fatjr   7 .  This defines a function fatF  : K+  -» N U {0,00}, which we call the fat-shattering  dimension  of F.  We say  that  F  has finite fat-shattering  dimension  whenever it is the case that for  all 7 > 0,   7   is finite.  Since  this  dimension  depends  on  the  scale  7,  it  is  often  described  as a  scale-sensitive  dimension.  To illustrate the fat-shattering  dimension, we consider  classes of functions  of bounded  variation.  We say  that  a function     : [0,1]  -»  R is of  bounded  variation if there  is  V  such  that   160   The Pseudo-Dimension and Fat-Shattering Dimension  T2+7   n   oi  X\  n -l  t=l  Fig. 11.2.  The set {xi, £2} C R is 7-shattered by the class F of affine functions on R, F  =  {a; H> ax + 6 : a,6 € R}.  The points n,r2  witness the shattering.  for  every  integer  n  and  every  sequence  2 i>2 2>       >2 n of  numbers  with 0  <  2 1 <  2 2 <        <  Vn  <  1, we have  In this case, we say that     has total variation at  most  V.  Theorem  11.12  Let  F  be  the  set  of  all functions  mapping from  the interval  [0,1]  to  the  interval  [0,1]  and  having total variation  at  most  V. Then  Proof  Suppose  that  7  >  0  and  that  the  set  5  =  {xi,X2,...,xm}  is 7-shattered,  witnessed  by r  =   ri,r2,... ,rm .  Assume,  without  loss of generality,  that  x\  <  2  <        <  xm.   Clearly,  no two of the  Xi can  be equal.   Since  5  is 7-shattered,  there  are two functions    i ,  2  6  F  with the following  properties:  fi{xi   >  r« -I- 7  fi{xi   < n  -  7   for i  odd, for i even,   11.3  The fat-shattering  dimension  161  and   2 i   < n  -  7   2 £t   > ^  + 7   for i odd, for i even.  Suppose that m is odd.  Then the total variation V\ of  i  satisfies  m -1  Similarly, the total variation V2  of  2  satisfies  Clearly,  max Vi,V2   >  2 m —  1 7.  If  m  is even,  the  same  argument shows that  max VuV2  m 2-l  1=1  >  2 m -  1 7.  4- 2 m -  1 7  But,  since   1  and   2  have  total  variation  at  most  V,  it  follows  that m  < 1 + 17 27 , and so  f*,<7  <!+[£]   Now, let d =  IJV27J  and let  5  =  {i d  : i =  0,1,2,..., d}, which has cardinality  d+  1.  Let  G consist  of 2d+1  functions  g  : [0,1] ->  {0,27} that  are piecewise-constant on the intervals  rj_  _ 3\  '  \2d'2d ''"']   F2d-3  2 d -n  F2d^l Id  '  2d   2d    '   ]   162   The Pseudo-Dimension and Fat-Shattering Dimension  Then it  is clear that  S  is shattered  by G, witnessed by the all-7 vector r  =   7>7J--->7 -  Furthermore,  G  C  F,  since  each  g  G G  has  total variation at  most 27c? =  27 y 27j  <  V.  It  follows that  F  7-shatters  5 and hence  as required.      Relating fat-shattering  dimension  and  pseudo-dimension  Given that  the notion of 7-shattering  is a refinement  of that  of pseudo- shattering, some relationships between the fat-shattering  dimension and the pseudo-dimension  can be given.  Theorem  11.13 Suppose that F  is a set of real-valued functions.  Then:   i   For all 7 > 0, fatF   7  <  Pdim F .  ii      a finite set S  is pseudo-shattered then there is 70 such that for  all 7 < 70, 5  is 7-shattered.   iii   The function fatjr  is non-increasing with 7.  iv   Pdim F   = Iim74,o fat^  7    where  both  sides may be infinite .  Proof  For   i ,  we need  only  note  that  if  a finite set  5  is  7-shattered for  some 7  >  0 then  it  is pseudo-shattered.  For   ii , suppose the finite set  S  =  {xi,X2,...,xm}  is  pseudo-shattered  by  F,  witnessed  by  r  =  ri, r2,..., r m .  With the functions fb  £ F as in the definition of pseudo- shattering, let  70 =  ^ min {n  -  fb Xi    : 1 < i < m, b €    {0, l } m ,  fb x{   < rj  .  Then  for  all  7  <  70,  5  is  7-shattered,  witnessed  by  r',  where  r\  = U -  7o 2.  To prove  iii , we observe that  if 7 < 7' then any 7;-shattered set is also 7-shattered.  Part   iv  follows immediately.     It  should  be  noted  that  it  is  possible  for  the  pseudo-dimension  to  be infinite,  even when the fat-shattering  dimension is finite for  all positive 7.  Indeed,  by  Theorem  11.12 and  part   iv   of Theorem  11.13, this  is true for the class of functions  on  [0,1] of total  variation  at  most  V   for any  V .  We say that a function  class is closed under scalar multiplication if for  all     e  F  and  all real numbers A, the  function  A   belongs to  F.  Any   11.4 Bibliographical  notes   163  vector space of functions  is certainly closed under scalar  multiplication, so the class of functions  computable by the linear computation unit, for instance,  has  this  property.   Note,  however,  that  any  set  of  functions mapping into a bounded  range does not have this property.   Theorem  11.14  Suppose  that a set F  of real-valued functions  is  closed under scalar multiplication. Then, for  all positive 7,  fatF   7  =  Pdim F .  In particular,  F  has finite fat-shattering dimension if and only if it has finite  pseudo-dimension.  Proof Suppose that a finite set 5  =  {a?i, X2,..., xm} is pseudo-shattered and that 7 is some arbitrary positive number.  Part  ii  of Theorem 11.13 shows that  there is a 70 >  0 such that  5  is 70-shattered.  Suppose  the shattering is witnessed by  ri,r2,... ,rm , and let the functions  { &} be as in the  definition  of 70-shattering.  Then  the functions  { 7 7o  &} 7- shatter  5,  witnessed  by    7 70 ^1        >  7 7o ^m «  But  these  functions are also in  F,  since F  is closed under  scalar  multiplication,  and  so for all 7  >  0, fatF  7   >  Pdim F .  Combining this  with  part   i  of Theo-   rem 11.13 gives the result.   11.4  Bibliographical  Notes  The pseudo-dimension was introduced by Pollard  1984; 1990 , who also observed  that  Theorem  11.4 follows  from  the  corresponding  result  for VC-dimension.  Several  closely  related  dimensions  have  been  consid- ered;  see, for  example,  the  definition  of  VC-major  classes  in   Dudley, 1987 .  Theorem  11.3 is from  Haussler,  1992 ; see also Proposition  4.2 in  Dudley, 1987  . The results on classes of polynomial  transformations may be found  in   Anthony,  1995; Anthony  and Holden,  1993; Anthony and Holden, 1994 .  The fat-shattering  dimension was introduced by Kearns and Schapire  1994 , who used it to prove lower bounds in a related learning problem  that of learning probabilistic concepts—see Section 9.4 .  Closely related quantities  were  used  earlier  in  approximation  theory   Lorentz,  1986, p.  113 , and attributed  in  Tikhomirov,  1969  to Kolmogorov.  The cal- culation of the fat-shattering  dimension of bounded  variation  functions   164   The Pseudo-Dimension  and Fat-Shattering  Dimension   Theorem  11.12   is  due  to  Simon   1997 .   In  fact,  Simon  calculates  a slightly different  dimension, but the  calculation is essentially  identical.    12  Bounding  Covering Numbers with  Dimensions  12.1  Introduction  Having introduced  the pseudo-dimension  and fat-shattering  dimension, we now show how these can be used to bound  the covering numbers of a function  class.  Given that  these dimensions are generalizations of the VC-dimension and that  the covering numbers are generalizations of the growth function,  this  is analogous to  bounding  the  growth  function  in terms of the VC-dimension.  The details are, as one might expect, rather more complicated.  12.2  Packing  Numbers  In  computing  upper  bounds  on covering numbers,  it  is often  useful  to bound  related  measures of 'richness' known as packing  numbers.  Since several different  packing numbers are also useful  later  in  the book, we define them for a general metric space.  Suppose that  A is a set  and that  d is a metric on A.  Given  W  C  A and a positive number c, a subset P  C W  is said to be e-separated, or to be an e-packing of W, if for all distinct x,y  e  P, d x,y   > e.  Figure 12.1 shows a subset of R2  that  is e-separated  with respect to the efe metric.  We define the e-packing number ofW,  M e,  W, d , to be the maximum cardinality  of an c-separated  subset  of W.   If there is no upper  bound on the cardinality of €-separated subsets of W, we say that  the packing number is infinite.   We are particularly interested  in the cases where  A is Rk  for some k and the metric d is either the doo  metric, the d\ metric, or the cfe metric.  As for covering numbers, we are concerned with cases where W  =  H\9  for  a function  class H  and  a sample x  of length  k.  We define the  uniform packing  numbers  as  Mp e,H,k   =  mxK{M{e,H\m,dp    :x€Xk}  165   166   Bounding  Covering  Numbers  with  Dimensions  Fig. 12.1.  The crosses form an e-separated subset of R2 with respect to the efe metric.  for p —  l,2,oo.  It  turns  out  that  packing numbers are intimately  related  to  covering  numbers, as the following result shows.  Theorem  12.1  Let   A,d   be a metric  space.  Then for  all positive e, and for  every subset W  of A,  the covering numbers and packing numbers satisfy  M{2e,WJd <SS e W,d <M e,W,d .    12.1   Proof  For the first  inequality in  12.1 , suppose that  C is an e-cover for W  and that P is a 2e-packing of W of maximum cardinality, M  2e, W, d . We show that  \P\  <  C,  from  which  the  inequality  follows.  Now,  for each  w  €  P,  since  C  is  an  e-cover  for  W,  there  is  some  v  G C  such that  d w,v      C,  then   by  a  simple  application  of  the 'pigeonholef  principle'   there  must  be  some  v  G C  such  that  for  two points  wi,W2  of Pj  d wi,v   < e and  d u 2,v  < e.  Then,  since d is  a metric,  d wi,W2  < d wi,v   +d u 2,v   < 2e.  f  The pigeonhole  principle states that  if n  pigeons are distributed  over fewer than  n pigeonholes, some pigeonhole must contain more than one pigeon.   12.3 Bounding with the pseudo-dimension   167  But this contradicts the fact  that  the points of P are 2e-separated, and we obtain the desired result.  To  prove  the second  inequality,  suppose  that  P  is an  e-packing of maximal cardinality, M{e, W,d .  Then for any w G W, there must be a v G P with d v, w  < e; otherwise w is not an element of P and PU{w} is an e-packing, which contradicts the assumption that  P is a maximal e-packing.  It follows that  any maximal e-packing is an e-cover.     12.3  Bounding with the Pseudo-Dimension  In this section we bound the doo-covering numbers by a quantity involv- ing the pseudo-dimension.  Theorem  12.2 Let F  be a set of real functions from a domain X  to the bounded interval [0, JB]. Let e > 0 and suppose that the pseudo-dimension of F  is d.  Then  which is less than  emB  ed     for  m>d.  Quantization  Integral to the proof of Theorem  12.2 is the technique of quantizing the function  class.  This  is a method  that  we employ  several  times  in the book.  Suppose that F is a class of functions mapping from a domain X  into a bounded interval in R. We assume that  this interval is [0,1]; by shifting and  scaling,  everything  that  follows  can be modified  to deal  with the case where functions  in F map into some other bounded  interval.  Let a be any positive real number.  For any real number n, the  quan-  tized version of u,  with quantization  width a is  In other  words, Qa u   is the largest  integer  multiple of a  less than or equal to u.   See Figure 12.2.   For a function     E F, the function  Qa f  is defined  on X  as   Qa f   x =Qa f x  .   168   Bounding Covering Numbers with Dimensions  Qaiu   2a   -  a   -  a   2a  r—** u  1  Fig.  12.2.  The  quantization  function  Qa  : K -+ R.  This function  maps from X  into the finite subset  {0, a, 2a,...,  [1 aJ  a} of [0,1].  We denote by QQ{F   the function  class {QQ f   : f  €  F}.  Proof  of  Theorem  12.2  Packing numbers and the quantized class  To derive upper bounds on covering numbers, we bound the correspond- ing packing  numbers  and  appeal  to  Theorem  12.1.   This  approach  is typical;  it  seems to  be  easier  to  show that  there  cannot  be  too  many well-separated  functions  than  to  show  directly  that  a  small  cover  ex- ists.   The first step in bounding the packing numbers of a function  class is to  reduce the  problem to one involving a quantized  version of F,  as expressed by the following lemma.  Lemma 12,3 For a set F  of real-valued functions mapping from X  into the interval [0,1],  we have   a [£J ,   12.3  Bounding with the pseudo-dimension   169  for  all positive  integers m,  all e > 0,  and all 0 < a < e.  Hence,  max  Proof  We use the following inequality, which holds for any real numbers a, 6.  \Q« a -Qa b \>Qa \a-b\ .  To see this, notice that  \\a-b\\   =  + - - I - I - I - - I-  *MIJ-=-  = ]   but the second term is zero since it is the floor function of the  difference between two numbers in the interval  [0,1 . Now, fix e, ra, and  x =   a?i,...,xm   €   X m.  Consider  any  two functions f,g G  F, and let  a denote     xi ,  x2 ,...,  rcm  .  Then doo  .>ff.   > c implies that some 1   e.  By the inequality  above, this implies that  \Qaf Xi -Qa9 Xi \>a[^\.  Hence, the quantized versions of distinct  elements of an e-separated  set are a [c aj-separated.  For a < e, this  gives  the first inequality  of  the theorem.  The second inequality follows on substituting a = c, since  M eiQ€  F \m,doo =    Now,  since  Qe '   is non-decreasing,  Theorem  11.3  shows  that the  pseudo-dimension  of the quantized class is no more than that  of  F,  Pdim Qc F  <Pdim F    12.2   for all e > 0.  A  combinatorial result  We now see that to bound the covering number, it is enough to bound the  X m,  we may  regard the  class cardinality  of  QC F ,  .  Fixing  some x € Qe F i  as a set of functions mapping from the finite set 5 consisting of the entries of x to the finite range Y.   170   Bounding  Covering  Numbers  with  Dimensions  It  is  possible  to  bound  the  size  of  Q€  F i  using  Theorem  3.6, by  see Section  11.2 .  considering the corresponding subgraph class  BQ€  F   To see this, notice  that   Indeed, if we were to add a function  to F  so that  Qe F \s  contained a new vector, this would lead to a new vector in BQ€  F   .   Theorem 3.6 shows that  .     i=0  where  N  =  \Y\  =  [1 cJ +  1 and d  =  VCdim  BQ^F  .  But then  d  = Pdim Q€  F    <  Pdim F ,  so we have a bound  in terms of the pseudo- dimension  of  F.  However,  to  prove  Theorem  12.2 we need  a  slightly better  bound.  The following  result is  sufficient.  Theorem  12.4 Suppose  that H  is a set of functions  from  a finite set X of  cardinality  m  to  a  finite set Y  C R of cardinality  N  and that  H has pseudo-dimension  at most  d.  Then  i* i < E  ?W - u*.  *=o  v   '  Notice that substituting N  =  1 into this theorem gives Sauer's Lemma  Theorem 3.6 . Theorem  12.4 follows from a more general result,  which also applies to other notions of dimension.  Before presenting this result, we require some  definitions.  We can think of a subset T of {0, l } m as a set of {0, l}-valued functions defined on { 1 , . . .,  m}, each of which maps % to U for some  £i,..., tm   € T.  Using this interpretation, we can define the VC-dimension of T as the VC-dimension of the class {i  H- U :  *i,..., t m   e  T}  of functions  defined on  { 1 , . . .,  m}.  For any finite subset Y  C R, a set 5  C Ym  of m-vectors, and a set $  of functions from Y  to {0,1},  we define the $-dimension  of 5,  denoted  $dim 5 ,   as the maximum over  i, fa,...,  m  €  *  of  VCdimiUMyi ,---, m ym    :  2 i,...,S m  € 5} .  To see how this general framework applies to the problem considered in Theorem  12.4, suppose that X  =  {xu  $2,...,xm}>  Y  =  where j i <  2 2 <      *  <  2 N,  and that  we take  {yuy2,...,2  5 = Hu  = { ft ari ,h{x2\...,h xm     :heH}   12.3 Bounding with the pseudo-dimension   171  and  $  =  $5  =  {si  : 1 < i  < N},  where s* a   =  sgn a — yi .  Then it is straightforward  to see that  $dim S   is precisely Pdim   H .  The following theorem uses the notion of a spanning set.  Recall that, for a subset  {v\,...,  vpj}  of a vector space V, we define the span of the subset as  span{vi,..., vN}  =  and  we say  that  the  subset  spans V  if  span{vi,..., VN}  =  V.  For  a finite set  Y  =  {2 1,2 2?-- ->2 N}  the  set  of real-valued  functions  defined on Y   denoted  Ry   is a vector space of dimension  N.  In the  following theorem, the condition that  $ y  spans the vector space Ry  ensures that the set $  is rich enough to capture the complexity of the set  5.  Theorem  12.5  For a finite set Y  of cardinality N,  and a set S  C  Ym with  spans Ry  then  <  d,  t=o  To see that  this  theorem  implies  Theorem  12.4,  suppose  that  Y  =  {2 1»     ,2 N}  where 2 1   &&&  that  s\,52,...,SN  defined  above. Then, as we have noted, $dim 5   =  Pdim if .  Further- more, $ y  consists of the N  vectors  are as   1,1,1,...,1 ,   0,1,1,...,1 ,   0,0,l,...,l ,  ..., 0,0,...,0,1 .  These vectors span RN  and hence $y  spans  Ry.  The  proof  of  Theorem  12.5 uses  the  following  lemma,  which  shows that  a  certain  class $<d   has  &d\s  spanning  R5.  The  proof  also uses the  observation  that  any  such  class must  have  5  <  dim span $ d   . For  a  set  of  functions  $  as  above,  $W  denotes  the  following  set  of functions  defined  on  Ym.   These functions  are  'monomials'  evaluated by forming the products of the values of some of the functions  in  $  at up to d of the 2fr.   =    fiflMyii    : * > 0, ^  €  *, 1 < J< < m I.  {   i=u=i   J   172   Bounding  Covering  Numbers  with  Dimensions  We use the  convention that  n*€0  a*  =: *   Lemma  12.6  Suppose  Y  is  a  finite  set,  $  is  a  set  of  {0,  \}-valued functions  defined  on  Y,  and  S  C  Ym  satisfies  $dim S   =  d.  If  $\Y spans  R y,  then  &d\s   spans  Rs.  Proof  The  proof  is  in  two  steps.  First,  we show  that  if  $ y  spans  Ry then  $  m  ym  spans R y m.  Then we show that  this implies  &d\s  spans R5.  If $  y  spans R y,  where Y  =  {yi, y2,...,  VN}, then there are functions   \ >       >  N  in  $  for which the  matrix  is of full rank.  The Kronecker product of two NxN  matrices, A  = and By is given by  ^2 2 2   \  0iv y2    '     N VN               ai  KTB   \  a\2B  CL22B  aN2B           To  see  this,  notice  that If  A  and  B  are  of  full  rank,  so  is  A 0  B.   A 0  JB  C 0  D   =   AC   0  CRD ,  so we  have   A 0  J3  A"1  0  B"1   = IN  0-TAT =  -TAT2* where   v  is the NxN  identity matrix.   Therefore,  the m-fold  Kronecker product,  M  0  M  0        0  M  is of full  rank.  Now, it  is easy to  see that  this matrix is of the  form  m    \  where the  iVm  functions  ^t     Ym  R are all the functions  of the  form  2,...   ,wm    =  where  1 <  ji, j 2 ,. . ., Ym  of  the  form   y^jj ta,..   ,y*m   for  1 <  ii,i2,  that  $   m    ym   spans  R y m.    j m  <  N,  and  where  the  Zi are  all  the  elements  of . . . , i m  <  JV.  It  follows   12.3 Bounding with the pseudo-dimension   173  For the second part of the proof, we show that this implies that   \ s spans  R5.  Consider  a  monomial  in  $ m   involving  k  > d  components of  y.  We show  that,  for  all  y  €  5,  this  is  equivalent  to  some  linear combination  of monomials  each  of which  involves no more than  k —  1 components.  Without loss of generality, we shall consider the monomial Yli-i   i yi ,  with  k  > d.  Since  VCdim{ fcfa ,..., k Vk     : y e  Y}  < d,  there  is  some fc-vector  &i,...,fr*   G {0,1}*  such  that,  for  all  y  G 5, some 1  i yi     &». If we define  otherwise,  then  n*=i z* y   =  0  for  all  y  €  5.  Expanding  the  product  in  this equation  to  a  sum,  and  noticing that  n£=i & 2 i   appears  in  only one term shows that  we can write  for all y E 5, where p is a polynomial involving only monomials with no more than  k — 1 of the variables  i yi .  Applying this result  iteratively to  every  monomial involving more than  d components  shows that  any monomial in  $ m   is equivalent  on S  to a linear  combination  of mono- mials in  &d\  Combining this  with  the  first  part  of the  proof,  we see that  $W S  spans R 5:     Proof   of Theorem  12.5  Since $  spans R y,  Lemma 12.6 implies that any function on 5 can be represented as a linear combination of elements of &dK  But  any function     : Y  -+ R—and,  in particular,  any  function in $—can be represented as a polynomial of degree no more than N — 1,  It  follows that  any function  on 5  can be represented  as a linear combi- nation of elements of the set M^"1  of monomials with up to d variables, each with exponent no more than  N  — 1,  M?-1  = I y H> JI $  : 0 <   < d, 1 < ji < m, 0 < k{ < N -  11,   174   Bounding Covering Numbers with Dimensions  where,  as  above,  we use  the  convention  that  Ylie^Vj!  ~  *   That ls> spans  Rs,  and  so  5  <  dim  span  M^"1  .  The  proof  of M""1^  Theorem  11.8 shows that  the set of all monomials of bounded  degree is linearly independent, which implies that  i=0  Obtaining Theorem  12.2     Theorem 12.2 follows immediately.  Fix any x and let H be the restriction of Qe F  to the set consisting of the m entries of X.  Then H  maps into the  finite  set  Y  of  cardinality  N  =  1 +  [l e\  and,  by  Lemma  12.3, Theorem  12.1, and Theorem 12.4,  Afoo  e,F,m   <  max  QAF *  =  Iff I  <  V  d  where  d  =  Pdim if   <  Pdim F .  The first inequality  of  the  theorem now follows, on rescaling F and e by a factor of B.  The second inequality follows from  Theorem 3.7.  12.4  Bounding with the Fat-Shattering  Dimension  A  general  upper  bound  We now present one of the key results of this chapter, in which we bound the packing numbers  and hence the covering numbers  by an expression involving the fat-shattering  dimension.  Theorem  12.7  Suppose that F  is a set of real functions from a domain X  to the bounded interval [0,£]  and that e > 0.  Then  Moo  e,F,m <  2  m62 n g22 1,  where b =  [2B e\,  and, with d = fatp  e 4 ,  The  proof  of  this  result  is  given  below,  but  first  we state  a  useful corollary, which follows from the relationships between covering numbers and packing numbers expressed by Theorem 12.1.   12.4  Bounding  with  the fat-shattering  dimension   175  Theorem  12.8  Let F  be a set  of  real functions  from  a domain  X  to  the bounded interval  [0,B].  Let  e  >  0  and  let  d  =  fatF  e 4 .  Then for  all m>d,  dlog2 4eBm  dc    Proof  Theorem  12.1 and Theorem  12.7 imply  that  By  Theorem 3.7, for m  >  d >  1,  and hence  \\og2y]<\dlog2 2eBm  de  }.  If d  >  1, this  is no more than  dlog2 4ei?ra  de  ,  from which the  result D follows.  If d =  0, it  is easy to see that jVoo  e, F% m   <  1.   Proof  of  Theorem  12.7  As in the proof of Theorem 12.2, Theorem 12.7 is proved by relating the packing numbers and dimension of F  to those of Qa F ,  for appropriate a.  In  this  case  we  bound  the  packing  numbers  using  combinatorial techniques,  much as we bounded  the growth function  in Theorem  3.6.  Relating  packing  numbers  and  dimensions  to  those  of the quantized  class  For a function  class F,  if e >  0, Lemma  12.3 shows that  Moo   c, F, m   <  Moo   e,  Q €   2 f ,m .  If a  <  2e then  which  implies  ktQa F   to  <  fetF   c -  a 2 ,    12.4    176   Bounding Covering Numbers with Dimensions  To  see   12.4 ,  suppose  that  Qa F   C  X,  wit- nessed  by  the  vector   n  . . ., rd   G Kd.  Then  for  all  b =    6 1 , . . ., bd   G {0, l}d  there  is a  function   & G F  with  €-shatters  {xi,...,Xd}   It follows  that  Qafb{xi -ri  Q« d *< -r«  <   >  e  -e   if 6i = 1 if 6i = 0.  fb xi   -  n  >  e   & zi  -  n  <   -e + a   if 6*  =  1 if 6^  = 0.  So F   e -  a 2 -shatters  {i,... ,<*}  C  X,  witnessed  by  the  sequence  ri  + a 2,... ,rrf  + a 2 .  This implies  12.4 .  A finite combinatorial problem  Fix x  e  Xm,  let  H  denote QC 2 F U, and let d = fatge 2 F    e 2 .  By a simple rescaling, the following lemma shows that  where b =  [2B e\  and y is as defined  in the lemma.  This implies Theo- rem 12.7.  Lemma  12.9  Let Y  =  {0,1,..., 6},  and suppose  \X\  =  m  ond H  CYX has fatj*   1  =  d.  7%en  2   =  2 ^ i =i  \ i  b  -  As in the proof of Theorem 3.6, the proof of this lemma uses a count- ing argument,  although  the proof here is more complicated.  The proof hinges on using an inductive argument to bound, for 2-separated classes of functions,  the number of distinct pairs consisting of a 1-shattered set and a witness for the shattering.  Proof  We can  assume that  b > 3.   The  result  holds trivially  if  b < 2 since in that  case a maximal 2-packing of H  has cardinality  1 or 2.   For k > 2 and m  > 1, define £ fc,ra   as  min   \{ A,r   : G 1-shatters A C X,  witnessed by r  : A -> Y,  A  ^  0} :  X  = m, G C yx,  \G\  = fc, and G is 2-separatedj ,  or take t k, m   to be infinite  if the minimum is over the empty  set.   12.4  Bounding  with  the fat-shattering  dimension   177  Notice that  the  number of pairs   A,r   with A^Q  and  \A\ <  d is less  than  d   £ ? *   since  for  each  fixed  A  of  positive  cardinality  i  <  d,  there  are  at  most  6 -  1 *  <  6* possible  functions  r  that  witness  the  1-shattering  of  A.  The  definition  of  1-shattering  shows  that  we  cannot  have r x   =  0 or r x   =  6.   It  follows  from  the  definition  of  t  that  if  t k,  m   >  y,  then every  2-separated  set  G  C Yx  of  cardinality  k  1-shatters  some  A  with \A\  >  d.  But  fatjy  1   =  d,  so  if  t k,m   >  y  then  M 2,H,doo   <  k. Hence, it  suffices  to show  that  fa*10**^,™   >y    12.5   for all d >  1 and m  >  1.  Now,  if  there  is  no  2-separated  subset  of  Yx  of  cardinality  k  = 2 mb2yi tev\  then t 2 mb2Ylo^y\m   =  oo and we axe done.  So as- sume that  such a set  G of functions  exists.  Split  G  arbitrarily into  k 2 pairs.   Note  that  k  is  divisible  by  2.   Since  the  functions  in  G  are  2- separated,  for  each  such  pair   01, 72 ,  we  can  choose  some  x  G X  such that  \gi x   — g2 x \  >  2.  By the pigeonhole principle, there is an XQ  G X such that  we choose x  =  o  for  at  least  k  2m   of these  pairs.   Other- wise,  the  number of  functions  in  G  would  be  less  than  2mk  2m   = fc, a  contradiction.   The  number  of  possible  {i,j}  C  Y  with  j  >  i +  2  is Ct*   ~~ &  ^ e  can  apply  the  pigeonhole  principle  again to  deduce  that there exist  i, j  G Y  with j  >  i +  2 such that  for at  least  pairs   <7i,< 2 ,  we  have  {pi a?o ,p2 ^o }  =  {hj}-  Thus,  there  are  two subsets  C?i,Cx2  of  G  such  that  Gi  =  IG2I  >  k  mb2   and,  for  some xo  G X  and  some  i,j  G F  with  j  >  i -f  2,  ^i xo   =  i  for  ^i  G G\  and ^2 ^0   =  i  for 02 G  C?2.  Clearly  the functions  in  G\  are 2-separated  on X  -  {xo}, since they are 2-separated on X  but equal on xo.  The same is true of G2.  Hence, by the definition  of the function  t,  there are at  least t   [fc ra&2J  ,m  -  1   pairs   A,r   such that  G\  1-shatters  ACX-  {xo}, witnessed  by r  : A  ->  F,  and the  same is true of  ?2-  Moreover, if  both G\  and  G2  1-shatter  A  witnessed  by  r,  then  G  1-shatters  A U {xo}, witnessed  by  r',  where  r'  equals  r  on  A  and  r' xo   =  L * + i  2j-  It   178   Bounding Covering Numbers with Dimensions  follows  that  Q ^ J      12.6  Since  the  total  number  of  functions  in  H  is  bounded  by   6 4- l  m, we have  \G\  =  2 m62 rio«2i>l  <   b +  l m,  from  which  it  follows  that m >  flog22 l.  Applying  12.6   flog2 2 1  times shows that  2t  but  the definition  of t shows that  £ 2,ra  =  1 for all ra >  1.  Combining this with   12.7  gives  12.5 , and this completes the proof.     Recall that  the d\-metric on R* is defined  as follows:  A  general  lower  bound  1  Recall  also  that,  for  all  x,y  G R*,  di x,y      so  that  for any  function  class  F,  any  c,  and  any  ra,  A i  c, F, ra  <  Afoo  e, F, ra   see  Lemma  10.5 .  The  following  result  gives  a  lower  bound  on  the di-covering numbers   and hence on the doo-covering numbers .  Theorem  12.10  Let F  be a set of real functions  and let e > 0.  Then,  AT   f  F m   >  AA  f  F rri\ > pfatF l6c  8  J M QQ    \ C« X  «  lib    ^   » "1  V ^l -^  »    llvl   *^  O   orra  >fatF 16c .  Proo   We first show that if d = fatF   16e  then M   2c, F, d   > ed 8.  Fix a sample x  of length d that  is 16e-shattered.  Then, by the definition of 16c-shattering,  there  is a  vector  r  G Rd  such that  the  following  holds: for  any b G  {0, l} d, there is fb  G F  such that  fb xi   > r* +  16e if 6* =  1 {0, l}d} C F  be such and  fb xi   < r{ -  16c if 6*  =  0.  Let G = {fb:be  a  set  of  2d  functions.  For  b,c  e  {0,l}d,  let  A 6,c   be  the  number  of entries on which 6 and c differ   that  is, the Hamming distance between them .  For g eG,  denote by g\m  the element  p «i ,gfa ,...  ,g xd    of R*.  Then it is clear that  for any 6,c G  {0, l} d,  32eA 6,c    12.4  Bounding  with  the fat-shattering  dimension   179  Therefore,  if dx   fb\9Jc\m   <  4e then  A 6,c   <  d 8.  It follows  that,  for any fixed ft  G  G, the number of functions  fc  G  G for which the  distance d\  [fb\m > fc\m   is  less  than 4c is no more than  1<* 8J   =0  But, by a Chernoff  bound   see Appendix  1 , this  sum of binomial  coef- ficients is at  most  That  is,  every  function  in  G  has  no  more  than  2de"d 8  functions  from G  at  d\  distance less than 4c.  Suppose C  is a 2e-cover for F\m. For each element c G  C, if some g G  G  satisfies  di    c ^ , ^   <  2c, we have  'eG:  dx  p'U,cu   < 2e} C {p' G G : dx  ^,.^1.   < 4c} ,  by  the  triangle  inequality.  But  this  set  has  cardinality  no  more  than 2de~"d 8,  so  every  element  of  the  cover  C  accounts  for  no  more  than 2<*e-d 8  eie m e nts  of G.  Since C  covers G,  we must  have  This shows that  M   2c, F, d  >  exp  fatF   16c  8 .  Now  we  bound  M\   e, -F, m   for  m  >  d.  Suppose  that  m  >  d  and  let m  = fcd 4- r  where  fc,r  are integers,  with fc >  1 and  0  <  r  <  d.  Let  z be the sample of length m  obtained by  concatenating fc copies of x  and adjoining the first r entries, xi, X2,...,  xr  of x.  Now, for   ,  G F,   180   Bounding Covering Numbers with Dimensions  ra — r       x  Since  ra  >  d,  we have  ra  = fed + r  <   A; +  l d  <  2fcd and  hence  r  = ra - fcd <  ra 2.  Hence,  if d\  {f\t,g\x   < e then  d\  f\x,g\9   < 2e.  As a result, given any e-cover of Fz, there is a 2e-cover of F\m  of the same cardinality.  Therefore  M   c, F, ra  > Mi  2c, F, d  > exp  fatF   16c  8 ,  as required.      Fat-shattering  dimension  characterizes  covering  numbers  The  upper  and  lower bound  results  Theorem  12.8 and  Theorem  12.10 show  that  the  fat-shattering  dimension  determines  in  a  fairly  precise manner the covering numbers of a function  class. The following theorem  which follows immediately from the preceding ones  makes this clear.  Theorem  12.11 If F  is a class of functions mapping from a set X  into the interval [0, B],  then for  any c, ifm>  fatj?  e 4  > 1,  <   log2 JVoo  e, F, ra  < 3fat F  e  This  shows,  in  particular,  that  if  a  class  has  finite  fat-shattering  di- mension,  then  the  covering  number  .Afc  e, F, ra  is  a  sub-exponential function  of ra.  An  example  To illustrate the application of these results, consider again the class  F of functions  from  [0,1] to  [0,1] of total  variation  at  most  V.  We have seen  Theorem 11.12  that fatF   7  =  1+ [V  2j \.  Prom Theorem 12.8, we obtain  the  following  upper  bound  on  the  covering numbers  of  this class.   12,5  Comparing  the two approaches   181  Theorem  12.12  Let  F  be the  class of functions  of  total variation at most V,  mapping from the interval [0,1] into [0,1].  Then, for  any e > 0,  <4m\ l+2V c log2 2ern V   for  all TO.  In  Chapter  10 it  was noted  that  the  appropriate  covering  numbers for analysing classification  by real function  classes are those of the class TT7 F ,  which  maps  into  a  bounded  interval  of  length  27.  Indeed,  in Theorem  10.4, the  key  covering number  is .Afc  7 2,TT 7 F ,2TO .  The following result applies.  Theorem  12.13  Suppose that  F  is  a  set  of  real functions  and that 7  > 0.     d = fat F   7 8   then  JVOO   7 2,TT 7 F ,2TO   <  2 128m dlog2 32em d .  Proof  By a trivial translation, we may assume  TT7 F  maps into [0,27]. Then, by Theorem  12.8, taking B  =  27 and e = 7 2,  we have  AToo  7 2,7r7 F ,m   <  from  which the result follows.      12.5  Comparing  the  Two  Approaches  We have  seen  that  if  F  is  uniformly  bounded  and  has  finite  pseudo- dimension  then  there is a  constant  c\  such that  the  covering numbers satisfy  where d — Pdim F .  More generally, if F  is uniformly  bounded and has finite fat-shattering  dimension, then there are constants 02,03  such that  where d =  fatF   c 4 .  Certainly,  d =  fat r  e 4   <  Pdim F ,  and  if the two are equal then the first bound is better.  However, it is possible for fatj?  c 4   to  be  significantly less than  Pdim F .  For example, for  the class  F  of  bounded  variation  functions  considered  in  Theorem  12.12, Pdim F   is  infinite,  but  fat r  e 4   is  finite  for  all  e  >  0.  It  follows   182   Bounding Covering Numbers with Dimensions  that  there can  be no characterization  of the  rate  of growth of the cov- ering numbers   as in Theorem  12.11  in terms of the  pseudo-dimension. Nonetheless, the first bound is sometimes useful.  For instance, if a func- tion  class F  has finite  pseudo-dimension  then,  for  all sufficiently  small e, the fat-shattering  dimension fat^  e 4   equals the  pseudo-dimension, and  then  the  covering  number  bound  of  Theorem  12.2  is  better  than that  of Theorem 12.8.  12.6  Remarks  A large family  of dimensions, defined  in terms of a set  of  {0, l}-valued functions  $,  satisfy  the  conditions  of Theorem  12.5.  It  can  be  shown that  all of these dimensions are closely related.  It  is possible to extend the  definition  of  these  dimensions  to  include  sets  $  of  functions  that map  from  Y  to  {0, *,1},  and  the  fat-shattering  dimension  emerges  as a  special  case.  Then  a  generalization  of Lemma  12.9 can  be  obtained in  terms  of  any  such  dimension,  provided  the  set  $  satisfies  a  weaker version of the spanning condition.  This weaker condition is that, for any two elements of Y  that  are separated by a gap of a certain size, there is a function  in $  that  labels one of these elements as a 0 and the other as a  1.   This is weaker than  the condition  that  $y  spans E y ,  since  that condition is equivalent to the condition that, for any two elements of  Y, there is a function  in $  that  labels those elements distinctly.   Theorem  12.11 cannot  be significantly  improved:  there are examples of function  classes that  show that  the lower bound  cannot be improved by  more than  a  constant  factor,  and  that  the  upper  bound  cannot  be improved  by  more  than  a  log factor.  However,  there  is one  aspect  of this theorem that  can be loose. For any non-decreasing function     from R+  to  N U {oo}, it  is  easy  to  construct  a  function  class  F  that  has fati?  7  =    7 .  In particular, the fat-shattering  dimension can increase at an arbitrary rate as its argument approaches zero. It follows that  the constant  in the  argument  of the  fat-shattering  function  can  be  impor- tant.  It  is known  that  if fat ?  is finite at  a  scale slightly  smaller  than e 2,  then  the  logarithms  of  these  e-covering  numbers   lnA i  e,F,m , lnjVoo  e,F,m    grow  'acceptably  slowly' with  m   that  is, slower  than linearly , whereas they grow unacceptably quickly if fatj?  is infinite  at  a scale slightly larger than c.   See the Bibliographical Notes.   Notice that there is still an unexplained  gap of a factor  of two in our knowledge of the appropriate scale at  which the fat-shattering  dimension  determines these covering numbers.  It may be that this gap is inevitable.  The intu-   12.7 Bibliographical  notes   183  ition behind this conjecture is as follows.  The proofs of upper bounds on these quantities naturally  involve packing numbers, whereas the  proofs of lower bounds involve covering numbers.  Theorem  12.1 shows that  Moo   2c,  F,m   <  JVOO  C, F,  m   <  Moo   c,  F,m .  There is an upper bound on Moo  e, F, m  in terms of some fat-shattering dimension at a scale of roughly c. To avoid violating this upper bound, if it is possible that jVoo  e, F, m  is close to Moo  2e, F, m  in the inequality above,  any  lower  bound  on  JVOO  e, F, m   must  be  in  terms  of  the  fat- shattering dimension at  a scale of roughly 2e.  12.7  Bibliographical  Notes  Theorem  12.1, relating coverings to packings, is due to Kolmogorov and Tihomirov   1961 .  Theorems  12.2   the  bound  on  covering  numbers in  terms  of  pseudo-dimension   and  12.4   the  generalization  of  Sauer's Lemma—Theorem  3.6—to classes of functions  that  map to a finite do- main   are due to Haussler and Long  1995 .  The latter  result  improves on a result due to Natarajan   1991b  which uses a slightly different  no- tion of dimension.  Theorem  12.5, the generalization 6f Sauer's Lemma in terms  of  ^-dimensions,  is due to  Gurvits   1997a .  The  linear  alge- braic proof technique appears in  Babai and Prankl, 1992 , and the idea of ^-dimensions is due to Ben-David, Cesa-Bianchi, Haussler and Long  1995 . That  paper studied the problem of learning classes of  functions that take values in a finite set.  It also showed that  a large family  of the ^-dimensions  are close to each other   see Section  12.6 .  The extension of these results to  {0, *, l}-valued families  $,  described in Section 12.6, was in  Anthony and Bartlett,  1995 . See also  Cesa-Bianchi and Haus- sler, 1998 .  Theorem  12.7, giving  covering  number  bounds  in  terms  of  the  fat- shattering  dimension,  is  due  to  Alon,  Ben-David,  Cesa-Bianchi  and Haussler   1993 .  The  corresponding  lower  bound,  Theorem  12.10,  is from   Bartlett,  Kulkarni  and  Posner,  1997 .   That  paper  also  gave tighter  bounds  than  Theorem  12.12 for  covering  numbers  of  the  class of functions  of bounded  variation.   The calculation of covering number bounds for classes TT7 F    Theorem  12.13  is from   Bartlett,  1998 . The investigation  of the  scales  at  which finiteness of the  fat-shattering  di- mension is necessary and sufficient  for learning is described in   Bartlett and Long, 1995; Bartlett  and Long, 1998 . The conjecture that  the gap in these scales is inevitable is due to Phil Long.   The Sample Complexity  of  Classification  13  Learning  13.1  Large  Margin  SEM  Algorithms  After  our  digression into covering numbers, pseudo-dimension  and  fat- shattering dimension, we now return to the central learning problem of this part  of the book.  In this chapter, we use the results of the last few chapters to construct classification learning algorithms and to determine bounds on the sample complexity of these algorithms.  In Chapter 4 we saw that  for binary classification,  it was appropriate to  use  an  algorithm  that,  given  a  training  sample,  returns  as  output hypothesis some function  which minimizes sample error; that is, we con- sidered SEM algorithms L, which have the property that  for all  z,  In  analysing  classification  learning  algorithms  for  real-valued  function classes, it  is useful  to  consider  algorithms  that,  given  a  sample  and  a parameter 7, return hypotheses minimizing the sample error with respect to 7.  Recall  that,  for  a  sample  z  =    a?i,j i ,..., xm,j m    G Zm y  a function     : X  -> K, and 7 > 0, the sample error of    with respect to  7 is defined  as  —   See Definition  9.1.   Definition  13.1  Suppose that F  is a set of real-valued functions defined on the domain X.  Then a large margin sample error minimization algo- rithm   or large margin SEM algorithm  L for F  takes as input a margin parameter 7  >  0  and a sample z  €  Um=i Zm>  o.nd  returns a function  184   13.2 Large  margin SEM algorithms  as learning  algorithms  185  from F  such that for  all 7 > 0,  all m,  and all z€  Z m,  13.2  Large  Margin  SEM  Algorithms  as  Classification  Learning  Algorithms  Chapter  4 establishes that  SEM algorithms are learning algorithms for binary-valued  classification  when the  class H  has finite  VC-dimension. We show now that,  analogously, the  large margin  SEM algorithms  for a  function  class  F  are  classification  learning  algorithms  when  F  has finite fat-shattering  dimension.  It  is  useful  to  recall  the  definition  of a  classification  learning  algorithm  for  a real function  class F.  For  any probability  distribution  P  on  Z,  such an  algorithm  L  takes as input  a number 7 £  0,1 2] and a random binary-labelled sample z> and returns a function  f  € F.  If m >   TUL  e, <S, 7 , then with probability at least  1-5,    satisfies  er P   <opt£ F   + €,  where optp F   =  inf 6irer£   .   Here, rai, e, $,7  denotes the  sample complexity of L.   In  Chapter  4,  we made  use of  a  uniform  convergence  result   Theo- rem 4.3   which tells  us that,  provided  H  has finite VC-dimension  and m  is large, then with high probability  evz h   -  e < erP  i   < erz h   + e  for all h € H.  In this chapter, we make use of Theorem  10.4, which, as we shall see, establishes that  for many function  classes F,  er P   <er?     + e  for  all f  e  F  with  high  probability   provided  m  is large enough .  We use this to obtain the following result.  Theorem  13.2  Suppose that F  is a set of real-valued functions  defined on X  and that L  is a large  margin SEM  algorithm for  F.  Suppose  that e e   0,1   and 7  >  0.  Then, given any probability  distribution P  on  Z, for  all m,  we have  , z    > opt£ F  + e} < 2Afoo  7 2,7T7 F ,2m    186   The Sample Complexity of Classification Learning  It  should  be noted  that  Theorem  10.4 is not  'two-sided'  in  the  way that Theorem 4.3 is, for it does not bound the probability that erp f   > erj     — c.  For this reason, we prove Theorem  13.2 using a slightly  dif- ferent  approach from that  taken in Chapter 4. In proving Theorem 4.2, we used the uniform lower bound on erp f   to show that a near-optimal function  in the class is likely to have small sample error.  However, since we need  only show this  for  a single such near-optimal  function,  we do not  need  a  uniform lower  bound.  In  the  analogous  step  in  the  proof of  Theorem  13.2, we use  instead  the  following  simple  consequence of HoefFding's  inequality   see Appendix 1 .  Lemma  13.3  Suppose  that f  is a real-valued function  defined on X,  P is a probability  distribution on Z,  e, 7  are real numbers with e > 0 and 7  > 0,  and m  is a positive integer. Then  Proof   of Theorem 13.2  Let  *  € F satisfy er£  *   < opt£ F +e 3. By Lemma 13.3,  e r :   r   < o p t ^   F   +      13.1   with probability at least l-e""2*2™ 9.  By Theorem 10.4, with probability at least  1 -  2N<»  7 2,7r7 F ,2m e"-€2m  72,  er P     < erj     + c 3 for all    € F.    13.2   Thus, with probability at least  1 _  e-2e3m 9 _  2-Voo  7 2, 7T7 F , 2m    e^m'12  we have both   13.1  and   13.2 , and in that  case  erp L 7,*    <     e.   The  second  inequality  follows from the  fact  that  L  is a  large  margin SEM algorithm, and so erJ L 7, z    =  min   €F  AJ     < erj  * .      Combining this result with Theorem 12.8, which bounds the covering numbers  in  terms  of  the  fat-shattering  dimension,  gives  the  following result.   13.2 Large  margin SEM algorithms  as learning  algorithms  187  Theorem  13.4  Suppose that F  is  a set  of  real-valued functions  with finite fat-shattering dimension, and that L  is a large  margin SEM algo- rithm for F.  Then L  is a classification learning algorithm for F.  Given 8 G  0,1   and 7  >  0,  suppose d =  fat^ ?    7 8   >  1.  Then the estima- tion error of L satisfies  tLimAi   <  ^   dlog 2   ^ p   ln 128m  +  In   j  Furthermore,  the sample  complexity  of L  satisfies  mUeAl  < moM,7  = f  ^dln*    ^    + In    for  die   € 0,1 .  Proof  By Theorem  13.2 and Theorem  12.13, Pm{eiP L 'y,z }>oi>tl F   + e   <  2 Afoo  7 2, TT7 F , 2m  e"'2"1 72  +  e"24'"1 9 <  3 max  Afoo  7 2, * 7 F , 2m ,  1   <  6 128m dlog2 32em <'  e-f2m n,   e^m'12   13.3   for d = fat^j?    7 8   >  1.  Clearly, this last quantity is no more than S when  We now consider the sample complexity.  The expression   13.3  is no  more than <5 if  € 2m 72  ~  f  + 7 dln f    +dlog   f  and for this it suffices if  m>  -j   in    -1  To proceed from here, we bound  In m and In2 m from above by expres- sions that are linear in m.  For the lnm term, we use Inequality   1.2   in Appendix  1 , which implies  72   4032d\ ,    lnm  <  --  +  —r— In  —^  \  ee2     ^m  4   1008^,   e2    The Sample Complexity of Classification Learning  188  For  the  ln2ra  term  we make  use  of  Inequality   1.3    in  Appendix  1 . Taking b =  1728d  e2ln2   and supposing that  m >  1 6,  this inequality shows that  72d  1  2   m     216d  ^  ^1728d^  so it suffices  to have m 2  at  least  r  UJ+7dln ITJ+ 1 4 d ln  r^"J+i^ln  Some straightforward  approximations yield the result.      13.3  Lower  Bounds  for  Certain  Function  Classes  Theorem  13.4 provides  an  upper  bound  on  the  sample  complexity  of any large margin SEM classification  learning algorithm  in terms of the fat-shattering  dimension.  For  many  function  classes, it  can  be  shown that  the  sample  complexity  of  any classification  learning  algorithm  is bounded below by a quantity involving the fat-shattering  dimension.  Recall that  a function  class F  is closed under addition of constants if for every    6 F  and every real number c, the function  x  H* f x   + c also belongs to  F.  For instance, if F  is the set  of functions  computable by a linear computation unit, then F  is closed under addition of constants: we simply adjust  the threshold.  Theorem  13.5  Suppose that F  is a set  of functions  mapping into the interval [0,1]  and that F  is dosed under addition of constants.  Then, if L  is  any classification  learning algorithm for  F,  the sample  complexity of L satisfies  max  for 0   0,  where d = fatW47 F   27  >  1.   Recall  that  7r7  : R  ->  [1 2 — 7,1 2  +  7]  is the  squashing  function  defined  in   10.4 .   Proof  The idea of the  proof  is to  construct  a class H  of  {0, l}-valued functions   the  definition  of  which  depends  on  the  parameter  7 ,  and show  that  a  classification  learning  algorithm  for  F  can  be  used  as  a   13.3  Lower  bounds for  certain  function  classes  189  5+47  "  -  1  2  2  ~ 27  r  L.  L.  II     r  -r.  1  Fig.  13.1.  Points  { x i , . . .,  X3} in the  set  T,  with  their  associated  witnesses  r». The  dotted  lines  extend  2y  above  and  below  each  r».  learning  algorithm  for  if,  and  that  the  estimation  errors   and  hence sample complexities  of both  are closely related.  The first step is to  construct  the  class H.  Fix 7  >  0.  Choose a  set so that  \S\ =  d = f a t ^   F     27  and 5 is 27-shattered by TT47 F , SCX  witnessed by r  £ [1 2-27, l 2+27]d.  By the pigeonhole principle, either at least d 2  of the n  fall in the interval  [1 2 -  27,1 2], or at  least  that many fall  in  [1 2,1 2 +  27].  Let  T  C S  contain  the  corresponding *, so \T\  > d 2.  Clearly, T  is 7-shattered  by  TT27 F , witnessed  by either   1   2 - 7 , 1   2 - 7 , - . - , 1   2 - 7   <»  1 2 + 7,1 2  + 7,--.,1 2 + 7    see Figure 13.1 . Because F  is closed under addition of constants, it follows also that T is 7-shattered,  witnessed by  1 2,..., 1 2 ;  we simply  ^hift' all the  relevant  functions  as necessary.  Now let  FQ C  F  be  the  set of functions  f  in F  such that  for all x  e  T,  \f x   -  1 2  > 7.  Clearly, the set H  of {0, l}-valued functions  on T  defined  by  ff  =  {w  sgn   x   -  1 2    :feF 0}  is the  set  of all  {0, l}-valued  functions  on T,  and  hence VCdim if   > d 2.  The second step of the proof is to relate the problem of  classification learning F to that of learning H, and to appeal to the lower bound results   190   The Sample  Complexity  of Classification Learning  from  Chapter  5.  Suppose that  L  is a  classification  learning  algorithm for  F,  with  sample  complexity rax,  €,£,7 .  Then  for  any  probability distribution  F o n l x { 0 , l },  the probability  of a random  sample z of length TO > raz, e, $,7  satisfying  is at  least  1 -  6. Notice that  opt£ F   =  mf  <  -  =   inf feF0 inf  erp ft ,  since erp     =  erp     for  all     E Fo.  It  follows  that  z  *->  sgn L 7,z   is a learning algorithm  for  H  with  sample complexity TO£, e, J,7 . But Theorem 5.2 shows that, for 0 < e,5 <  1 64,  max  Theorems 13.4 and 13.5 show how the inherent sample complexity of a function  class grows with the fat-shattering  dimension and the accuracy parameter  e.  In particular,  they  show that  there  are constants  c\  and C2 for  which     7 8   for  suitably  small e and  5, provided  that  F  is closed under  addition of constants.  Clearly, it  is only the  behaviour  of functions  in  F  near  the threshold  value of  1 2  that  influences  the  complexity  of F  for  classifi- cation  learning,  whereas  the  fat-shattering  dimension  in  these  bounds measures the  complexity  of functions  in  TT7 F   over the  whole of their [1 2 -  7,1 2 + 7] range.  The condition that  F  is closed under  addition of constants ensures that the complexity of functions  in F  is, in a sense, uniform  over  this  range.  For  example,  consider  the  following  class of functions   which is not closed under addition of constants .  Let a  be a positive constant, and define F  as the set of all functions  mapping from N to  [1 2 +  a, 00 .  Then  fat7r7 F    7 8   is infinite  for  7  >  4a 3,  but there is a classification  learning algorithm for  F.   Indeed, since sgn    is identically  1 for  all     in  F,  classification  learning with  F  is trivial.  The class F  is certainly complex, but the complexity of the functions  in   13.4  Using the pseudo-dimension   191  F is restricted to a range that does not include the threshold, and hence this complexity is irrelevant for classification  learning.  13.4  Using the  Pseudo-Dimension  The  analysis  in the  previous  section  uses an  upper  bound  on  covering numbers in terms of the fat-shattering dimension.  However, we have also derived  an upper  bound   Theorem  12.2  on covering numbers in terms of the pseudo-dimension,  and  it  is natural  to investigate what  this will tell us about the sample complexity of classification learning algorithms. Using Theorem 12.2 as it applies to TT7 F , the following result can easily be obtained.  Theorem  13.6  If  F  is  a  set  of  real  functions  having finite pseudo- dimension, and L  is a large  margin SEM  algorithm for  F,  then L  is a classification  learning algorithm  for  F.  For all 5  €   0,1 ,  all m,  and 7  > 0,  its estimation error satisfies  eL m, 6,7  < *o m, 6,7  =   — \d In I —  J  + In f - JJ  1   ,  where d =  Pdim F .  This  result  is,  however,  weaker  than  the  VC-dimension  results  of Chapter  4.  To  see this,  let  if  =  {x  H> sgn   z   -  1 2   :   6 f },  and notice  that  VCdim iJ   <  Pdim F   and  optP iJ   <  optJ, F .  It  fol- lows that Theorem 4.2 implies a stronger version of Theorem 13.6  with smaller constants .  Thus, using the pseudo-dimension to analyse classifi- cation learning for classes of real-valued functions  gives no improvement over the results of Chapter 4.  However, using the fat-shattering  dimen- sion  in  a  'scale-sensitive'  analysis  of  classification  learning  can  give  a significant  improvement over the VC-dimension results of Part  1. In the next chapter, we see examples of neural network classes that  have finite fat-shattering  dimension,  but  whose thresholded  versions  have  infinite VC-dimension.  13.5  Remarks  Relative uniform convergence  results  Theorem  13.5 implies that,  for  function  classes satisfying  a mild  "self- similarity"  condition   the  class is closed  under  addition  of  constants ,   192   The  Sample  Complexity  of  Classification  Learning  the  rate  of  uniform  convergence  of  erp     to  evj f   can  be  no  faster than  1 y m.  However, just  as the relative uniform  convergence results of Section 5.5 demonstrate for the learning model of Part  1, it turns out that erp f   converges more quickly to  1 + a erj     for any fixed a  > 0, as the following result shows.  Theorem  13.7  Suppose that F  is a set of real-valued functions  defined on X,  P  is a probability  distribution on Z,  7 > 0,  and a,  3 > 0,  Pm  {3  € F  : erp     >  1 + « &J     +  0}  < 4  ^    7 2, n, F ,  2m  exp  The theorem follows from  the inequality   .[ .,...«* -*!   .  .  J  {   Verp f   <  4A -OO  7 2,7r7 F ,2m exp  in the  same way that  Theorem  5.7 follows  from  Inequality   5.11 .  We omit  the  proof  of  this  inequality;  it  is  closely  related  to  the  proof  of Theorem 10.1.  13.6  Bibliographical  Notes  Most of the results in this chapter have their origins in  Bartlett, 1998 . Theorem  13.2 follows   using Hoeffding's  inequality,  as in  Lemma 13.3  from a corresponding result in   Bartlett,  1998 .  Clearly, the proof uses ideas of Vapnik and Chervonenkis  1971  and Pollard  1984 , but  work- ing with the doo metric, as in   Anthony and Bartlett,  1995 . The proof technique  used  for  the  lower bound,  Theorem  13.5, is  from   Bartlett, 1998 .  Similar  techniques  are  used  by  Simon   1996   to  give  a  lower bound for the sample complexity of learning probabilistic concepts  see Section  9.4 ,  in  terms  of  a  different  scale-sensitive  dimension   that  is smaller  than  the  fat-shattering  dimension .  Inequality   13.4 ,  and  a slightly weaker version of Theorem 13.7 are also given in  Bartlett, 1998 . Independent related work appears in  Horv&th and Lugosi, 1998 , relat- ing the classification  performance of a class of real-valued functions  to a certain scale-sensitive dimension.   14  The  Dimensions of Neural  Networks  14.1  Introduction  In this  chapter  we  bound  the  pseudo-dimension  and  the  fat-shattering dimension of the function  classes computed  by certain neural networks. The pseudo-dimension  bounds follow easily from VC-dimension  bounds obtained  earlier,  so these  shall  not  detain  us for  long.  Of more  impor- tance  are  the  bounds  we  obtain  on  the  fat-shattering  dimension.  We derive these bounds by bounding certain covering numbers.  Later in the book,  we shall use these covering number bounds  directly.  We bound the covering numbers and fat-shattering dimensions for net- works that  are fully  connected  between  adjacent  layers, that  have units with  a  bounded  activation  function  satisfying  a  Lipschitz  constraint, and  that  have  all  weights   or  all  weights  in  certain  layers   constrained to  be  small.  We  give  two  main  results  on  the  covering  numbers  and fat-shattering  dimensions  of  networks  of  this  type.  In  Section  14.3  we give  bounds  in  terms  of  the  number  of  parameters  in  the  network.  In contrast, Section  14.4 gives bounds on the fat-shattering  dimension  that instead  grow  with  the  bound  on the  size  of the  parameters  and,  some- what  surprisingly,  are independent  of the  number  of  parameters  in  the network.  This result is consistent with the intuition we obtain by study- ing  networks  of  linear  units   units  with  the  identity  function  as  their activation  function .  For a  network  of  this  kind,  no  matter  how  large, the  function  computed  by  the  network  is  a  linear  combination  of  the input  variables, and so its pseudo-dimension  does not  increase with  the number of parameters.  In a network  of computation  units in which  the activation  function  satisfies  a  Lipschitz  constraint,  if  the  weights  are constrained  to  be  small  then  the  function  computed  by  the  network  is 'approximately linear' in the parameters.  It makes sense, then, that  the  193   194  The Dimensions of Neural Networks  Xn   I   -  T   Xl   I  -   Xn   !  V  Fig. 14.1. The networks N  and N1 of Theorem 14.1; N  computes    € F and N'  computes h €  H'.  h x,y   fat-shattering  dimension  in  this  case does  not  increase  with  the  num- ber of parameters.  On the other  hand, the closeness to linearity of the network function  depends on the magnitude of the weights, the number of layers, and the scale of interest,  and  we shall see that  the bound on fatjr   7  increases with 1 7, with the weight bound, and with the number of layers.  These results suggest that  there are two distinct  notions of the com- plexity of a multi-layer network:  we can restrict a network's complexity either by restricting the number of parameters or by restricting the size of the parameters.  14.2  Pseudo-Dimension of Neural Networks  We spent  much effort  in Part  1 in bounding the VC-dimension  of vari- ous types of neural network.  Fortunately, we can use our VC-dimension bounds  to  obtain  bounds  on the  pseudo-dimension  of real-output  net- works.  The following  result  relates the  pseudo-dimension  of the  set of functions  computed  by  a  real-output  network  to  the  VC-dimension of an  augmented  version of the same network.  Figure  14.1 illustrates  the networks described in the theorem.   14-2  Pseudo-dimension  of neural networks   195  Theorem  14.1  Let  N  be any  neural  network  with  a  single real-valued output  unit,  and form  a neural network  N'  as follows.  The network  N1 has one extra input  unit  and one extra computation  unit  This  additional computation  unit  is  a linear  threshold unit  receiving input  only from  the output unit  ofN  and from  the new input  unit,  and it  is the output unit of N1.  If H'  is  the set  o {0,1}-valued  functions  computed by N1  and F  the set  of real-valued functions  computed by N  then Pdim F   <  VCdim if; .  Proof  Any  state  of  N* is of the  form u* =   a;, w, 0 ,  where a; is  a  state of Ny  w  is the weight on the connection from the  new input  to the  new output  unit,  and 9 is the  threshold  on the  new  computation  unit.  Any input  pattern  to  N'  is  of  the  form   x, y   where  x  is  an  input  pattern to  N  and  y  is  the  new  input.  If  h^  is  the  function  computed  by  Nr when in state a   then  h^ x,  y   =  sgn  fu> x   +wy  -0 ,  where  a,  is the function  N  computes in state a;.  Thus, if a state u   of N'  has 0 =  0 and w  =  —1,  then  K>{x,y   =  sgn  „ a;   -  y .    14.1   But then the VC-dimension of the subset of H1  that  corresponds to this choice of w  and 6 is the VC-dimension  of the sub-graph class,  BF  =  { «,»   H> sgn   x   -  y   : f  G  F},  which  is  precisely  the  pseudo-dimension  of  F   see  Section  11.2 .  The result  follows.     It is  clear that,  in the  notation  of Theorem  14.1, the  network  N1  has precisely two more parameters than iV, one more computation unit, and one  more input.  In fact,  if  the  activation  function  s  of the  output  unit in the network N  is non-decreasing and continuous from the rightf, then we can write  sgn  s g x    -  y   =  sgn  g x   -  s"1^   >  where  5""1 y   =  inf{o  :  s a   >  y}.  It  follows  that  we  can  compute the  functions  described  in  the  proof  of  Theorem  14.1  using  a  network N'  with  the  same  number  of  computation  units  as  N.  Notice  that  the standard  sigmoid  function,  the  identity  function,  and  the  sgn  function are all non-decreasing  and continuous from the  right.  t  A function  s  : R  -¥  R  is continuous  from the  right  if,  for all  XQ G  R,  \\mx±xo  s x   —  s x0 .   196   The Dimensions of Neural Networks  We could give many specific  pseudo-dimension  bounds now by using earlier VC-dimension bounds.  We give just  one example, which follows from Theorem 8.13.  Theorem  14.2 Let F  be the set of functions computed by a feed-forward network with W parameters and k computation units, in which each com- putation unit   including the output  has the standard sigmoid activation function.  Then  Pdim F   <  { W + 2 k 2  + U W  + 2 Jfclog2 18 W + 2 Jfc2 .  14.3  Bounds on the Fat-Shattering  Dimension in terms of  Number of Parameters  Covering numbers of compositions  of function  classes  In  this  section  we bound  the  covering numbers  and  fat-shattering  di- mension of multilayer networks by quantities depending on the number of adjustable  parameters.  To obtain these bounds, we use the fact  that the  functions  computed  by  all  units  beyond  the  first  layer  can  be  ap- proximated  accurately  by  a  finite  set  of  such  functions.  The  precise details of this follow, but  the basic idea is quite simple:  we form  a grid in  parameter  space of the  required fineness  depending  on the  level of approximation required  and we use functions  corresponding to the grid points.  In the analysis of this section, we split the network into two parts:  the first layer, and later layers. We bound  certain covering numbers for  the classes of functions  computable by these parts, and then  combine these covering number bounds.  In what follows, X  is the input  space Rn,  Y\  is the output  set of the first layer   so that,  for  example, Y\  =  [0, l]k  if there are k  units  in  the first layer, each of which maps to [0,1] , Fi  is the class of vector-valued functions  that  can be computed by the first layer, and G is the class of functions  that  can be computed by the remainder of the network.  Then F\ is a class of functions from X  to Y\, G is a class of functions from Y\ to R, and the set of functions computable by the network is the composition of these classes, G o JFi, given by G o F\  =  {go f  : g G G,    € F\}  where  W  s   =      *  .  We define the uniform, or Loo, distance between functions  f,g  : Y\  ->   14-3 Fat-shattering dimension bounds:  number of parameters 197  Eby  The following lemma bounds the doo covering number of the composition of two function  classes in terms of a doo covering number  of one class and  an Loo covering  number  of the other.  Recall,  from  Chapter 10, that .Af  e, F, d  denotes the e-covering number of a function  class F with respect to the metric d, and that, for a positive integer ra,  JV00  e,F,ra  = max {7V e,F\m,doo : x G Xm   .  Lemma  14.3 Let X  be a set and  Yi,p   a metric space. Suppose  that L>0,F\  is a class of functions mapping from X  toY\y  and G is a class of real-valued functions  defined on Y\  satisfying the following  Lipschitz condition: for all g in G and ally,z  in Y\,  \g v -g z \<Lp y,z .  For y =   yi,... ,ym   and z =  *i,... ,zm   from Yf", let  dZo y>Z   =  BMX^PiViiZi .  Then  Proof  Fix x  e  Xm.  Suppose  that  F\ is an e  2L -cover  of Fi\x  with respect to d£, and G is an e 2-cover of G with respect to  11^  . Let  We show that G\fi  is an e-cover of  G   Fi \m  with respect to doo. Since G\p  has cardinality no more than  Af e  2L ,Fi\m,d00 Af e 21G,dL00 , the  result  follows.  To see that  G\p  is a  cover,  choose     €  F\ and g € G, and  pick    € F x and g € G  such that  d^if^J   < e  2L  and dLoofa.d     w h e re    .  =    *i i       >   **»  .  Then  max  p f xi Ji <e  2L\ \<%<m  and so  nut.  \g f xi   -  g{fi \ < L  e  2L   = e 2, Kt<m   198   The Dimensions of Neural Networks  which implies  max  \g{f xi  -g fi \<e. l<i<m  The result follows.      It  is clear  that  the existence of a finite  Loo-cover of a function  class is a much  stronger  requirement  than  the existence of any of the other covers  we considered  in  Chapter  12, because  it  implies  that  a finite set of functions  approximates the function  class accurately at  all points of the input  space.  Even the class of functions  computed  by the real weight  perceptron  with  a single input  cannot  be approximated  in this strong sense, as the following  proposition  shows.  In contrast,  however, Theorem  3.1  together  with the observation  that  the covering number equals the growth function for {0, l}-valued functions—see  Section 10.2  shows that we can find a doo-cover for this class whose cardinality grows linearly with m.  Proposition  14.4 Let F  be the class of functions  computed by the real weight perceptron  with a single  real input.  Then  fore  < 1.  Proof  Suppose there is a finite e-cover  F.  Then  each element  of F is a function   a  : E -> {0,1} corresponding to a certain  'transition  point' a,  so that  fa x   =  0 if and only if x  < a.  Let S C E be  the set of all transition  points of functions  from  the cover.  Since 5 is finite, we can choose a function  f  in F that has a transition point not in 5, and hence for all elements    of the cover there is some point in E at which    differs   from     by 1. Hence e > 1, and the result follows.   Even though the existence of a finite Loo cover for functions  computed by all but the first layer of the network is a stringent requirement, if we bound  the parameters  and allow  only  smooth  activation  functions  in that  part  of the network,  we can approximate the class in this  strong sense, as we shall see.   14- 3  Fat-shattering  dimension bounds:  number of parameters  199  Covering numbers for  neural network classes  For the  remainder  of this  section, we consider  the  class F  of  functions computed by a feed-forward real-output multi-layer network.  We assume that  this network has the following properties:     The network has £ > 2 layers, with connections only between  adjacent  layers.     There are W  weights in the network.    For some 6, each computation unit maps into the interval  [—6,6], and each computation unit in the first layer has a non-decreasing activation function.     There are constants  V  > 0 and  L  > 1 V,  so that  for  each unit  in all but  the first  layer of the network, the vector w of weights associated with  that  unit  has  \\w\\i < V,  and  the  unit's  activation  function  s  : E ->- [—b,b]  satisfies the Lipschitz condition  \s ai   — s ct2 \  < L\a\  — c*2 for  all ai,c*2  in  R   Here,  itfi  is the  sum  of absolute values of the entries of w.   For convenience, we also assume that  the computation  units have no thresholds.  It is straightforward  to transform  a network with thresholds to  one  without:  we add  a  new  input  unit  with  a  constant  value,  and a new computation  unit  in each non-output  layer, with one connection from the new unit in the previous layer, and we replace the threshold in a computation unit with a connection from the new unit in the previous layer.  Theorem  14.5  For the class  F  of functions  computed  by the  network described  above,  if e < 26, then  The  proof  of  this  theorem  uses  Lemma  14.3.  Suppose  the  network has k units in the first  layer.  We split the network so that  F  =  G o F\, where Fi  is the class of functions  from  X  to Y\  =  [—6, b]k computed by the  first  layer  units,  and  G is the  class of functions  from  Y\  to  [—6, b] computed by the later layers.  As a first  step, we show that  functions  in G satisfy  a Lipschitz condi- tion.  For a =   01,... ,a*   € R*, let  aoo =  max* ai.   We will use the corresponding metric on Yi, given by p a,b  =  \\a — 6oo for a, b E Y\.    200   The Dimensions of Neural Networks  Lemma  14.6  For the class G  of functions  computed by all but the first layer, as defined above,  every g in G  and 2 1,2 2 in Y\  satisfy  Proof  We decompose the function  class G into functions  computed  by each  layer  of  units.  Let  Y\ be  the  output  space of units  in  layer  i,  so if there  are  ki  units  in layer i,  then  Y{ =  [—6,6]**.  Then  we can  write G =  Gi o Gt-i  o       o G2, where G% is the class of functions  from  Yi_i  to Yi computed by layer i.  Then for any gi e  Gi and 2 1,2 2 € l i - i,  by the Lipschitz condition on s,  \\9i{yi   -  0t 2 2 oo  <  Lmaxu;T 2 i  -  2 2 !,  where the maximum is over all weight vectors w associated with a unit in layer i.  Clearly   see Inequality   1.8  in Appendix  1 , \wT{y\  — 2 2 ! < IMI1II2 1 ~ 2 2II00  -  The result  now follows by induction on i.     The  second  step  of the  proof  of  Theorem  14.5 is  to  bound  the  d^- covering numbers  of F\\9,  where p is the  absolute  difference  metric on  Lemma  14.7  For the class Fi  of functions  computed  by the first layer defined as above,  and for x  €  Xm,  where WQ is the number of weights  in all but the first layer.  Proof  Fix  x  e  Xm.  For     6  Fu   noting that  Fx  is a  class of  vector- valued functions ,  write f x   =    i x ,... , fc x    € [—6,6]*, and  define  for  i  =  l,...,fe.  Clearly,  F\\9  C  F14.  x       x  -Fi.jfei  »  and  so  we  can construct  a  cover  of  Fia  with  respect  to  d^  by  taking  the  product  of covers of  JFi,i,  with  respect  to  doo.  This  implies   14-3 Fat-shattering  dimension bounds:  number of parameters  201  Suppose I C f1.  Since the activation function  of each first-layer com- putation unit is a non-decreasing function,  we can apply Theorems 11.3, 11.6 and  12.2 to show that  for any z. Since kn =  W  -  WG,  the result follows.      The final step in the proof of Theorem  14.5 is to show that  there is a  small  LQO cover of G.  Lemma  14.8  With notation  as defined above,  if e   1 then  Proof  We construct  a fine grid of points in parameter  space, and show that  the set of functions  corresponding to these parameter values forms an e-cover of the class G with respect to d ^.  To this end, let A =  2V N for some integer N   which will be chosen later , and consider the set  regarded as a subset of the parameter space.  Clearly  \S\ =   N  +  1 WG. Let SG  be the subset consisting of the points of 5 that satisfy the weight constraints described in the definition  of the network.  Consider a func- tion g in G, and its corresponding vector w of parameter values.  Clearly, there is a point  w in  S   corresponding to  a function  g in G   such  that every component  of w  is within  A  of the  corresponding  component of w.  Now, as in the proof of Lemma 14.6, write G =  Gi o Gt-\  o       o G2. Define g<i  G G2,...,gt  G Gi so that g = gto---og2,  and define  fa,...,gt similarly.  Consider j i € Yi, and let 2 i+i  =  gi+i yi  for i =  1 , . . . ,  -  1. Similarly,  let  yi  =  yi,  and  define  &+1 =  &+i 2 i -  Now, we prove by induction  that  \\Vi - foil*, < LAWGb{L^~\    14.2   First, notice that this is trivially true for i =  1. Next, assume that   14.2  is true for i > 1. We have  HtfH-l -  ft+llloo  =   ll&   202   The Dimensions of Neural Networks  By the same argument as in the proof of Lemma 14.6, the first term in this  expression  is no more than  LVj < — J too-  To bound  the  second term, consider a unit in layer  i +1  of the network, and let the relevant components of w and w be v and v respectively, so the unit  computes s vTyi   in calculating <ft+i 2 t  and s vTyi   in calculating &+i yi .  Then it is clear that  \s vTHi  -  s vTyi \  < L\\v - fllliHfolloo <  LAWGb.  It follows that  Hence,  and so SQ  corresponds to a di^  e-cover of G if this quantity is less than e. For this, it suffices  if we choose  2LVWGb  LVy-i  c LV-l    -   -  1   + 1>  and we have jt e,G,dLoo   <  N + 1 ^ , which implies the result.      Theorem  14.5  follows  immediately  from  Lemmas  14.3,  14.6,  14.7,  and  14.8  with the L of Lemma 14.3 taken to be   LV l~l .  A  bound on  the fat-shattering   dimension  As  a  corollary  of  Theorem  14.5, we can  obtain  a  bound  on  the  fat- shattering dimension of the neural networks under  consideration.  Theorem  14.9  For the class F  of functions computed  by the  network described above,  fatF  e  < 16W  ein LV  + 21n 32W  + In f   ^   jj  .  Proof  Theorem  14.5 establishes that  M    e F  X    4embW LV iy  W   14 *4 Fat-shattering  dimension bounds:  size of parameters  203  By Theorem 12.10,  for  m  >  fatir 16e .  Setting  d  =  fatj? e ,  replacing  e by  e 16,  taking m  =  d in the lower bound just  stated,  and  using Inequality   1.2   from Appendix  1 shows that  \  <   \n Afoo e W,F,d    V  e LV-l   {  <LV-1      from which the result follows.      For fixed depth, the bound of Theorem 14.9 is O W ln VW e  .  Thus, the rate of growth with the number of parameters W  is essentially  the same as the corresponding results for linear threshold networks and for piecewise-polynomial networks  Theorems 6.1 and 8.8 .  Both the bound V  on the size of the parameters and the scale e enter only logarithmically.  14.4  Bounds  on the  Fat-Shattering  Dimension  in  terms  of  Size  of  Parameters  In this  section,  we bound  the  fat-shattering  dimension  of sigmoid  net- works by quantities that do not depend on the number of parameters in the  network.  The  key idea is to  approximate  a network with  bounded weights by one with few weights. There are two steps in the derivation, outlined in the next two sections: the first presents an approximation re- sult for convex combinations of bounded functions,  and the second uses this to bound the covering numbers of the class of convex combinations of  basis  functions  in  terms  of  covering  numbers  of  the  basis  function  An  approximation result  The following result shows that  a convex combination of bounded  func- tions  can  be  approximated  by  some  small  convex  combination.  The   204  The Dimensions  of Neural  Networks  measure of approximation error is defined  in terms of a scalar  productf. For a subset  5  of a vector space H,  the  convex hull of 5,  co 5   C H,  is defined  as  N   \  { N   ^aiSi:  N e N,Si  e S,cti > 0,]Tai  = * \    Theorem  14.10  Let  F  be a  vector  space  with  a scalar product  and  let 11 11 =  y UTF   be the  induced norm  on F.  Suppose G  C F  and  that,  for Then for  all f  e  co G ,  all  keN, some  B>0,  and  all c>  B2  — \\f\\2,  there  are  elements  & , . . .,   <fc  of G  satisfying  \\g\\ <  B  for  all geG.   2  Proof  The proof uses the probabilistic method.  The idea is to show that, for  a  suitable  random  choice  of  the  g^  the  expectation  of  the  squared norm is bounded by c fc, from which it follows that there must exist some pi's for which the squared norm is no larger than this.  Write    €  co G  f r   *  €  G  and  suppose the  <ft's are chosen  independently as  ]T iIi  aifi  such that  Pr j =  fj   =  aj.  Then we have  * =i  The  second  term  inside  the  sum  is  zero,  since  the  independence  of and gj  implies  Efo - f,9j -    = E f  f  -   ,ft  -   j  = E  0,9j -    = 0.  For the first term, notice  that  <  c.  t  If F is a vector space, a scalar product on F is a function defined for all  i, fi  in F  and denoted   i, 2  with the properties:  1    i, i   > 0 for h  ^ 0,  2    i, 2   =   2, i ,   3   A i, 2  = A  i, 2 , and  4  {fuh  + A  =  fi,h   +  Uuh -   144  Fat-shattering  dimension bounds:  size of parameters  205  Combining shows that  *  i 1  2  *  i 1  D  Relating  covering numbers  Recall  the  definition  of  the  metric  cfe:  for  any  x  =   a?i,...,a?n   and  <*2 z,2  =  [  -  Notice  that  d2  is  the  metric  induced  by  the  scalar  product   a, b  —  1 n  22=ia*^»  on  ^n*  ^e  f U wing  theorem  uses the previous result to bound covering numbers  with respect to d2  of bounded linear com- binations of functions.  Theorem  14.11  Suppose  b > 0  and that F  is a class of  [-6^-valued functions  defined on  a set X,  and Af2  e,F,m   is finite for  all m  €  N and e > 0.  Then provided e\ +  €2  < e,  log2M   e,co F ,m   <   -J  log2N2   c2,  F,m . I €i I  Proo   Suppose A 2   e2,  F,  m   = iV. Then for any « =   »!,..., xm   G Xm there is an €2-cover 5  of Fa  such that  5 =  N.  We use Theorem  14.10 to show that  we can approximate vectors in the restriction  of co F   to x using small convex combinations of functions  in 5.  To this end, define Tk  C Rm  as  Clearly  \Tk\  < Nk.  Choose any    6 co F , and suppose    =  £i=i <*ifi with  oti >  0,  2i=i  ai  — 1> a n  i   i  G F.  Using  the  notation  g\9  =  ^i ,.-.,p ^m  ,  let    iU  =    t xi ,..., i xm    €  Em.  Since 5  is an e2-cover of F\9  with respect to the cfe metric, there are vectors    1 , . . . ,    in S  such that   206   The Dimensions of Neural Networks  It follows  that  But Theorem  14.10 shows that  there are gi,...   ,gk in 5  such that  and the triangle inequality for cfe then implies that  <€2-  b  Since this is true for any    € co F ,  T* is an  e2+b Vk -covei  of co F a. Choosing k =  f62 ef"  gives the desired result.     We use the following two simple results in deriving the covering num- ber and fat-shattering  dimension bounds for  neural networks.  The first concerns  covering  numbers  of  scaled  versions  of  a  function  class.  For a  real  number  a  and  a  subset  F  of  a  normed  vector  space,  we  define  = {af:f€    F}.  Lemma  14.12 IfG  is a normed vector space with induced metric d and F  is a subset ofG,  then N e,F,d   = N \a\e,aF,d ,   for  any e >  0 and  Proof  If  F  is  a  minimal  e-cover  of  F,  then  for  all     G F  there  is  an f  in F  with  d  ,     <  e.  Since d is induced  by a norm,  it  satisfies  the property  d af,af   =  \a\d f,f .  It  follows  that  aF  is an  ae-cover of aF,  which implies AT ae, aF, d  < jV e, F, d .  The reverse inequality is proved similarly.     Finally, the following lemma shows that  taking the composition  of a fixed function  with functions  from a class does not significantly  increase the  covering  number,  provided  the  fixed  function  satisfies  a  Lipschitz condition.  Lemma  14.13  Suppose F  is a class of real-valued functions  defined  on a set  X,  and the function     : R ->  R  satisfies the Lipschitz condition \4> x   ~   y \  < L\x  "  2 1  for  allx,y€R.  Then N2  e,   o F, m   < M2  e L, F, m .  Let    o F  =  {  o    :    G  F}.   14-4 Fat-shattering  dimension bounds:  size of parameters  207  The  proof  is similar  to the proof  of Lemma  14.12, but uses the fact  that  \ 4>of {x -i4>og  x \<L\f x -g x \.  Covering  numbers  for  two-layer  networks  We first  consider two-layer networks in which the first-layer  units com- pute arbitrary functions  from  some set.  The following theorem  follows easily  from  Theorem  14.11.  In  the  theorem  statement,  —F\ denotes  Theorem  14.14 Suppose b > 0 and that F\  is a class of [—b,b]-valued functions  defined on a set X  and satisfying the following  conditions:   i   Fx =  -Ft,  ii   F\  contains the identically  zero function, and  iii   jV2  e,Fi,m   is finite for allmeN  and e > 0.  For V>1,  define  F = I Y,Wifr  = ^  €   N, * €   F!,]T K < V \ .  N   i=i   }  J     N   I i=i   Then for e\ + e2 < e,  Proof  The conditions on Fi  imply that  F  =  Vco Fi .  Theorem 14.11 and Lemma 14.12 give the result.     We can apply  this  result  to  give  covering  number  bounds  for two- layer networks.  The following  corollary gives a bound  that  depends on the input dimension, and is applicable to a network with non-decreasing activation functions  in the first  layer units.  Corollary  14.15  Suppose that b > 0  and s  : R  -»  [-6,6]  is  a non- decreasing function.  Let V  > 1 and suppose  that F  is the class of func- tions from En  to R given  by   The Dimensions of Neural Networks  N ^2  W{s vfx  + Vio  + wo : N  G N, vi  G Rn, vw G R, t=i  Tften forO<e<b   and m  > n + 1,  i=0   e,F,m  <   ^   ^log2  ^^H  J    Proof  Define Fi  as the class of functions  computed by units in the first layer,  =  {x  x  Theorem  11.4  shows  that  the  pseudo-dimension  of  the  class  of  affine combinations is n + 1.  Theorem  11.3 shows that this is not increased by composing with the activation function,  and Theorem  12.2 shows that  for m>d.  Lemma 14.12 shows that N%  e, —Fi,m  = M* fa Fi,m , and so the class F\ U — F\ U {0,1}  where 0 and 1 are the identically zero and identically one functions   has covering number  .A a  c,Fi U - Fi  U {0, l},m   < 2M  faFum   + 2.  Theorem  14.14 shows that  Substituting  €i =  e2 =  e 2 gives the result.   D  The second  corollary gives a bound  on the logarithm of the covering number  that  increases  only  logarithmically  with  the  input  dimension, but it requires smooth activation functions  in the first layer units and a bound on the first layer weights.  Corollary  14.16  Suppose that 6,L  >  0  and s  : R  -*  [-6,6] satisfies \s ai   -  s a2 \    1 and B  > 1, let  =  \  N  Y,u>ifi + wo : N U=l   N  i=0   144  Fat-shattering  dimension  bounds: size  of parameters   209  where  Fi  =  \  x  i-> s  I Y^ViXi +  v0    :VieR,xe   [-B,B]n  9jr,\vi\   <  V  1.  Then fore<V  min{£L, &},  log2M2   e, F, m   <  50 f   c2   j  Iog2 2n +  2 .  Applying  Theorem  14.14   as  in  the  proof  of  Corollary  14.15   as  well as Lemma  14.13 shows that  eRm    <   r FW  V2B2L2'  provided  B  >  1 and  e\  +  e2  >  e,  where  G  =  {x  i-> x%  : i  € Clearly,  for  all  e  >  0,  Af2  e,G,m   <  \G\  =  n.  Since  [y 2B 2L 2 cf]  < 2V2B2L2 e2,  we can choose ei  <  e such  that    { 1 , . . .,  n}}.  F 2 B2 L 2 -    2y 2J3 2L 2  Hence,  Similarly, if 6 >  1,  log2 M2  c, Fi, m   <   5   log 2  2n 4- 2 .   €,F,m   <  1 ^ -1  log 2  2JV2  ~,Fi,m   +2   for ci  +  e2  <  c.  Setting  e\  =  c2  =  c 2  gives  W2b2L2      8 ^ B £,     n   2 + I o g 2   2n  + 2   50V6L4b2B2,   3   tn  log2 2n  and the result  follows.   210   The Dimensions  of Neural Networks  Covering  numbers  for  deeper  networks  We can use a similar approach to give bounds for deeper networks.  For convenience, we assume that the bound B on the magnitude of the input values is equal to the maximum magnitude  b of the activation  function. Define  Fo  =  {x  H> Xi : x  =   xi,...,x n   6  [-6,b]n,  i  €  {1,...,n}}  U  {0,1},  where  0  and  1  are  the  identically  zero  and  identically  one  functions, respectively.  For i  >  1, define  Thus,  Ft  is  the  class  of  functions  that  can  be  computed  by  an  £-layer feed-forward  network in which each unit has the sum of the  magnitudes of its  weights  bounded  by  V.  We assume  as  above  that  the  activation function s  : R ->- [—6,6] satisfies the Lipschitz condition s ai —s c*2   < L\ot\ -  c*2 for all ai,a2  € M.  Theorem  14.17  For I >  1,  the class Ft  defined above satisfies  log2 N2  c, Ft,m <\    ?pj    2VL '«+1> log2 2n + 2 ,  provided  b>i,V>   1  2L ,  and e <  VbL.  Proof  As in the proof of Corollary  14.16,  log2 Af2  e, Fx,m   <  —^    lo82 2n +  2 .    14.4   2V2  2L2  Theorem  14.14 and Lemma  14.13 imply  that  2     ^ Z     14.5   for i  >  1, where Gi-i  =   J*~o  ft-  Clearly, d  =  Gi_i  U Fu  so for i  >  2,  loga-A ^  e,Gi,m    5V26 2L2 c2   +  — p — I o g 2 ^2     ^ . G i - i.   14-4  Fat-shattering  dimension  bounds:  size  of parameters   211  We now  prove by  induction  that  this  implies  Clearly,  this  is trae for i  =  1.  If it is true  for  1 <  t  <  j  -1,    14.6   implies  e,Gj,m  6VWL2,   ..     c  _  .i-1  as desired.  Now,  Inequality   14.4   shows  that  log2 AT2  e, Gi, m   <   log 2    2n +  2 < 2V' t2i2A2   +  2 n  +  2   log2 2n +  2 .   e2   14.8   Combining   14.5 ,   14.7 ,  and   14.8   shows  that  log2A 2 c,F ,m  „  5V 26 2L 2,   k,      c   ^  e2   \   e2  e2   log2 2n  +  2   which implies  the  result.       212   The Dimensions of Neural Networks  A  bound  on  the  fat-shattering   dimension  Using  the  covering  number  bounds  of  Corollary  14.15,  together  with Theorem  12.10,  straightforward  calculations   similar  to  those  used  to prove Theorem  14.9  give the following bound on the fat-shattering  di- mension of two-layer networks with bounded weights.  Theorem  14.18  Suppose that  6  >  1  and s  :  E  -»  [—6,6] is  a  non- decreasing function.  Let V  > 1 and suppose  that F  is the class of func- tions from En  to E  given  by  N  {  x H> Y]wis vfx  + vi0   + w0  : N  € N,v{  G Rn  1vi0  € E,  ^  N   i=0  Then for 0 < e < 6,  Using the  proof  techniques from  Theorem  14.17, this  result  can eas- ily  be  extended  to  networks  of  any  fixed  depth,  with  bounded  non- decreasing activation  functions  in the first layer  and  bounded,  smooth  Lipschitz   activation functions  in later layers. As mentioned at the be- ginning of this chapter, these bounds show that  when the parameters in all except  the first layer are not  too large relative to the  relevant  scale  that  is,  V  is small  relative  to  e ,  the  fat-shattering  dimension  of  the network  grows linearly  with  the  input  dimension  n.  Hence,  when  the computation units in all except the first layer are operating in their 'ap- proximately  linear'  region, the  dimension  grows just  as it  would  if  the network contained only linear computation units.  When the units in the first layer also have smooth activation  functions and  small  weights,  Theorems  14.17 and  12.10  immediately  imply  the following result.  In this case, when the weights are small relative to the relevant scale, the fat-shattering dimension can be even smaller than the dimension of a network of linear units.  Theorem  14.19  Suppose that Ft  is defined recursively by  14-3 .  Then  €  c  < 4  ^\    2VL '«+1> ln 2n + 2 ,   H.5  Remarks   213  provided b > 1, V  >  1  2L ,  and e < 16VbL.  Note that  both this bound and that  of Theorem  14.18 depend on the bound  V  on the size of the weights, but  do not  involve W,  the number of weights.  For a fixed number of layers £, both bounds are independent of the size of the network.  14.5  Remarks  Because the  pseudo-dimension  gives a  bound  on  the  fat-shattering  di- mension,  the  VC-dimension  bounds  of  Chapter  8,  together  with  The- orem  14.1,  imply  bounds  for  several  networks,  including  those  with piecewise-polynomiaJ activation functions and the standard sigmoid acti- vation function.  These bounds are in terms of the number of parameters in the network, and have the advantage over Theorem  14.5 that  they do not  require a bound on the magnitude of the parameters.  However, for networks of units  with  the standard  sigmoid  activation  function,  these bounds grow more quickly with the size of the network.  The  techniques  used  in  Section  14.4 can  be  applied  to  any  function classes that  can be expressed as compositions of bounded linear combi- nations  and  bounded  scalar  functions  satisfying  a  Lipschitz  condition. For  example,  there  are  straightforward  analogues  of  Theorems  14.18 and  14.19 for radial basis function  networks with  bounded  parameters.  See Section  8.5.   Linear classifiers  Techniques  from  Section  14.4 give the  following  result  for  the  class of linear  functions  with  bounded  inputs  and  bounded  weights.  We as- sume that  the  inputs  have bounded  infinity-norm,  defined  by  xoo  = max* \xi\, for x =   xi,... ,xn   G E n, and that the weights have bounded 1-norm, defined  by  Hi  =  ££=1 w», for w =   wi,... ,wn   G  En.  The proof follows from the bound on the covering numbers of Fi  in the proof of Corollary  14.16, together with Theorem 12.10.  Theorem 14.20 For the class Fv  =  {x  *-> wTx  : a:oo <  1, IMIi  <  we  have  v}  fetjv   c  < 212   --    ln 2n + 2 .   214   The Dimensions of Neural Networks  Since the  1-norm and  the  infinity-norm  are  dual  norms, the  bounds on these norms ensure that functions  in F are bounded.  A similar result to  Theorem  14.20 can be obtained  for  another  pair  of dual  norms, the 2-norm of x  and the 2-norm of w.  Theorem  14.21  For the class Fv  =  {x  ^  wTx  : \\x\\2 < 1, \\w\\2  <  V} we  have  These results are especially interesting because of the attractive com- putational properties of these function classes. A problem closely related to large margin sample error minimization for the class defined  in The- orem  14.21  namely, the  problem of maximizing 7  so that  eY*     =  0  can be expressed as a quadratic optimization  problem with convex con- straints.  This problem  can be solved in polynomial time using interior point  methods.  As  a  result,  these  function  classes  have  been  studied extensively.  The  bounds  of  Theorems  14.20 and  14.21,  together  with the  results  of  Chapter  13, explain  why the  generalization  performance of these classifiers  does not  have a significant  dependence on the  input dimension  n.  The  proof  of  Theorem  14.21 involves  two  lemmas.  The  first  shows that  the sum of any subset of a shattered set is far from  the sum of the remainder of that  set.  The second shows that  these sums cannot be too far  apart  when the  2-norms of the input  vectors are small.  Comparing the results gives the bound on the fat-shattering  dimension.  Lemma  14.22  Let  Fv  =  {x  i-> wTx  : x2  <  1, IHI2  <  V}.  e-shattered by Fv,  then every subset So C S satisfies  If  S  is   Here, for a finite set T,^T   denotes the sum of the members of T.   Proof  Suppose  that  S  =  {xi,...,a?d}  is e-shattered  by Fy,  witnessed by  r i , . . . , rd  €  K.  Then  for  all  b =   &i,...,bd   €  { - l , l }d  there  is  a Wb  with  \\wb\\    e.  Fix a  subset So C 5.  We consider two cases. If  n  - *i € So} > Y,  in   : xi  e  S  ~  5o} ,    14.9    H.5  Remarks   215  then fix bi =  1 if and only if Xi G So. In that case we have wjxi  > r» + e if Xi G So, and w^Xi  < r* -  e otherwise.  It follows  that  Similarly,  Hence,  -\S-   S0\e.  But since iu2 <  V, the Cauchy-Schwarz inequality  Inequality   1.7  in Appendix  1  implies that  In the other  case  if   14.9   is not  satisfied ,  we fix bi =  1 if and only  if Xi G 5 — So, and use an identical argument.      Lemma  14.23  For all S  C Rn  with \\x\\2 <lforxeS,  satisfies  some So  C S  Proof  The  proof  uses  the  probabilistic  method.  Fix  any  finite  set S  =  {a;i,... ,Xd] satisfying  the conditions of the lemma.  We choose So randomly, by defining So =  {xi  G S : bi =  1}, where 6i, ...,&<< E {-1,1} are independent  and uniform  random variables.  Then  d  =  E  \  ^ biXi  2  2   216   The Dimensions of Neural Networks  where the last equality follows from the fact that the 6$'s have zero mean and  are independent.  Since the expected  value of this squared  norm is no more than  £, there must  be a set  So for  which this  quantity  is no more than  \S\.     Theorem 14.21 follows immediately:  if 5 is c-shattered, then 5e V  <  , so \S\  <  14.6  Bibliographical  Notes  Theorem  14.1, the  observation  that  the  pseudo-dimension  of  a  neural network  can  be  bounded  by  the  VC-dimension  of  a  related  network, appears in   Vidyasagar, 1997 .  The techniques of Theorem  14.5—relating covering numbers of com- positions  of  function  classes  to  covering  numbers  of  the  classes—have been used by a number of authors; see, for example,  White, 1990; Haus- sler, 1992; Barron, 1994; McCaffrey  and Gallant, 1994 .  Theorem  14.10, the approximation result on which the results of Sec- tion  14.4  are  based,  is  attributed  in   Dudley,  1987   to  Maurey   see  Pisier,  1981; Carl,  1982  ,  and  was  used  by  Jones   1992 ,  and  Bar- ron   1993  to obtain approximation rates for two layer neural networks. Makovoz  1996  gives an improvement of this result that  is significantly better when the subset G has small covering numbers, and shows that his result is optimal  see also  Dudley, 1987, Theorem 5.1  .  If, in deriving a bound on covering numbers for two-layer networks  like Corollary 14.15 , we use  Makovoz' result  in  place of Theorem  14.10, we obtain  a  slight improvement:  for input dimension n, the new bound on the log covering number grows as €-2»  1+*l   ignoring log factors.  Comparing this with the  old  bound,  which is roughly of the  form  1 c2,  shows that  the  im- provement is significant for n =  1, but as the input dimension n becomes large, the new bound approaches the old.  Typically, we are interested in problems with large input dimension, where the improvement is not sig- nificant.  However, the difference  is significant  if we are interested  in the asymptotic distribution of the largest difference  between expected error and  sample error  of functions  in the  class.  See, for  example,   van  der Vaart and Wellner, 1996 .  Theorem  14.11 and  its  proof  were in   Lee et  al.,  1996 , and  Corol- lary 14.15 is a more general version of a result there.  See  Dudley, 1987, Theorem  5.1   for  a similar, but  weaker, result.  Using uniform  approxi- mation techniques due to Barron  1992 , Gurvits and Koiran  1997  give   14-6 Bibliographical  notes   217  a bound on the fat-shattering  dimension for these networks that  implies a weaker result than Corollary 14.15. Corollary 14.16 and Theorem 14.17 are from   Bartlett,  1998 , as are the  corresponding  bounds on the  fat- shattering dimension   Theorems  14.18 and  14.19 .  Theorem  14.20, the fat-shattering  dimension bound for convex combinations of points in an Zoo ball in En,  is an immediate consequence of these results.  A shorter proof of the uniform  convergence implications  of that  theorem  is given in   Schapire, Freund,  Bartlett  and  Lee, 1998 .  A similar  result for  this class—with  an  even  shorter  proof—is  given  in   Mason,  Bartlett  and Baxter, 1999 .  Vapnik and  Chervonenkis  see  Vapnik,  1982   gave a result that  can be used  to give a weaker version of Theorem  14.21  see   Shawe-Taylor et  al.,  1996; Shawe-Taylor,  Bartlett,  Williamson  and  Anthony,  1998  . For  details  of  the  quadratic  optimization  problem  for  support  vector machines mentioned  after  that  theorem, see  Boser et  al.,  1992; Cortes and  Vapnik,  1995; Wahba,  1990; Vapnik,  1995 .   See also   Scholkopf, Burges and  Smola,  1999; Smola,  Bartlett,  Scholkopf  and  Schuurmans, 1999 .   Gurvits   1997b   gave  a  simple  proof  of  a  weaker  version  of Theorem 14.21, using random approximation ideas similar to those used for  Theorem  14.10.  The  proof  given  here  uses  the  same  ideas,  but  is simpler  again,  and  gives  a  better  constant.  See   Bartlett  and  Shawe- Taylor, 1999 .  That  theorem  can be significantly  improved for  support vector  machines, where the  variables x  are themselves fixed non-linear functions of input variables; see  Williamson, Smola and Scholkopf, 1999; Guo, Bartlett,  Shawe-Taylor and Williamson, 1999 .   15  Model Selection  15.1  Introduction  The first two parts of this book have considered the following three step approach to solving a pattern  classification  problem.   i   Choose a suitable class of functions,  ii   Gather  data,  iii   Choose a function  from the class.  In  this formulation,  the  class is fixed  before  the  data  is gathered,  and the aim is to select  a function  that  is nearly optimal in the class.  The results presented  so far  show how the estimation  error  depends on  the complexity  of  this fixed class  and  the  amount  of  data.  For  example, consider  a  two-layer  network  N\y  with  input  set  X,  W  parameters,  a linear threshold output  unit, and first-layer units with a fixed bounded piecewise-linear  activation  function   such  as the  activation  function  il- lustrated  in  Figure 8.1 .  Theorem  8.8,  Theorem  4.3, and  the  proof of Theorem  4.2  together  imply  the  following  result.  For  convenience  in what follows, we split the result into the key results that  imply  it.f.  Theorem  15.1  There are constants Ci,C2,C3  such  that  the  following holds.  For any  W,  let H\y  be the  class of functions  computed by the two-layer network Nw  with piecewise-linear first-layer units,  as  defined above. Suppose P  is a probability distribution onXx  {0,1}, and z  €  Zm is chosen according to Pm.  Then with probability at least 1 — 6,  every h  t  In fact, we did not use Inequality  15.1  to derive Theorem 4.2.  We include it here for uniformity.  It  is easy  to  obtain,  using an identical  argument  to  the  proof of Lemma 13.3.  218   15.1  Introduction   219  in  Hw  satisfies  evP h   < evz h   +   ^    wln{Wm   For  any  fixed  h*   in  particular,  for  h*  €  Hw  satisfying  erp h*   <  +  1 m ,  with  probability  at  least  1 — 6  we  have  *, fc*  < evP h*  +  jln  Q      .    15.1   Hence,  if  Lw  is  a SEM  algorithm  for  Hw   so  that  Lw z   has erz  min- imal  over  Hw ,   then  with  probability  at  least  1 — 8  we  have  erp Lw{z   <  inf  eip h  +    ~  1 2  Unfortunately,  this  result  is  applicable  only  if  we  fix  the  complex- ity  of  our  class   that  is,  the  number  of first-layer units,  and  hence  the number  W  of parameters   before seeing any data.  This is rather  unsat- isfactory; we would prefer that,  after  seeing the data,  a learning  system could choose automatically  a suitable level of complexity  that  gives the smallest  possible error.  This problem  is known as  model  selection.  As a second example, Theorem  14.18, Theorem  13.2, and the proof of Theorem  13.4 together imply the following result.  Here, the class Fy  of functions  computed  by the  two-layer network  Ny  is defined  by  Fy  =  I x  H> ]T  Wis vf   x  + vi 0  + wo  : k G N, ^   I   i=i   \wi\   ,  J  i=o    15.2   where  V  is  a  positive  constant,  x  G En,  and  the  activation  function s  : E  ->- [— 1,1]  associated  with  each first-layer unit  is a  non-decreasing function.  T h e o r em  15.2  There  are  constants  ci,C2,C3  such  that  the  following holds.  For  any  V  and  n,  let  Fy  be the function  class  defined  by   15.2 . Fix  7  G  0,1]  and  suppose  that  P  is  a probability  distribution  on  X  x {0,1},  and that z  G Zm  is chosen  according to Pm.  Then  with  probability at  least  1 — 8,  every  f  G Fy  satisfies   220   Model  Selection  For  any  fixed   *,  with  probability  at  least  1 — S  we  have  In particular, this is true for f*  6 Fy  with erp  *   < inf e.pv  erp     + 1 ra.  Hence, if Ly  is a large margin sample error minimization al- gorithm for  Fy   so  that  evJ Ly z1j    = miiif^Fv e rI   A  ^ en   with probability  at least 1 — S we have  erp Ly z,j    \m  In this case, there are two complexity  parameters, the weight bound V  and the margin 7, that  must be fixed in advance.  Again, it would be preferable to choose these parameters  automatically.  15*2  Model  Selection  Results  The  lower  bound  results  of  Chapter  5  show  that  we  cannot  hope  to find a function  whose error  is nearly minimal  over the  set  of networks with  any  number  and  size of parameters.  However,  in  both  cases  the above results  show how to  trade  complexity  for  estimation  error.  For instance, increasing 7 in  15.3  decreases the estimation error term, but may  increase  the  error  term.  An  obvious  approach  here  is  to  choose the complexity parameters  and hence function  class  to minimize these upper  bounds  on misclassification  probability.  However the  bounds of Theorems 15.1 and 15.2 are applicable only if the complexity parameters are fixed in advance.  The following results are versions of these theorems that  allow  the  complexity  parameters  to  be  chosen  after  the  data  has been seen.  The estimation error bounds increase by only a log factor.  In the first of these theorems, let  Hw  be the class of functions  com- puted  by two-layer networks with  W  parameters,  as in  Theorem 15.1. Let Lc  be a learning algorithm that  returns ft € \J W  Hw  corresponding to a pair   ft, W   with ft € Hw  that  minimizes  fW\\\1'2 c     — I W ln Wm   + In  ^7-  I 1    15.4  over all values of W  G  N and  h €  Hw-   Since in   15.4   we can always restrict  our consideration to a finite set of values of W, the quantity  to be  minimized  takes values in a finite set,  and  so the  minimum  always  ,    15.2 Model selection  results   221  exists.   Notice that, if Lw  is a sample error minimization algorithm for Hw,  then a suitable algorithm Lc  is the one which returns the  function Lw z   corresponding to the value of W  for which  evz Lw z   +  ±   win Wm  + \n   y       is minimized.  Here, c E E  is a  parameter  of the  algorithm.  Theorem  15.3  There  are constants  c,ci  for  which  the following  holds. Suppose  P  is  a probability  distribution  on  X  x  {0,1},  and  z  G Zm  is chosen  according  to  Pm.  For  the  two-layer  network  learning  algorithm Lc  described  above,  with probability  at  least  1-5  we  have  evP Lc z    Define  H  =  \JW  Hw  to  be the  class of functions  computed  by two- layer networks with  any  number  of parameters.  Then  we can think of the algorithm  Lc  as minimizing an expression of the form   sample error of h  4-  complexity penalty for  h   over H, where the complexity penalty involves the number W  of param- eters in the  network  that  computes  the  function  ft.  This  optimization problem involves an explicit trade-off between the number of parameters and the sample error.  The results of Chapter 5 show that, if we minimize only the  sample error over the  class if,  we cannot  always hope to ob- tain small error.  Theorem  15.3 shows that  if we include the complexity penalty  term,  we can  be confident  that  we will obtain  a function  with small error.  Furthermore,  this approach is nearly optimal  in the sense that the resulting classification function  minimizes the sum of error and a complexity penalty term.  For  the  second  theorem,  let  ify  be  the  class of functions  computed by the two-layer network with  bounded  output  weights as described in Theorem  15.2, for  any  positive  real  number  V.  Let  Lc  be  a  learning algorithm  that  returns     €  \JV  Fy  corresponding  to  a  triple    , V, 7  that  has    G Fy  and  within  1 m  of  its  infimum  over  all  values  of  7  €   0,1],  V  E M+,  and   222   Model  Selection  f  e  Fy.   We need  to  consider  an  algorithm  that  approximately  min- imizes this  error  criterion,  since we cannot  be  sure that  the  minimum always exists.   Notice that,  if Ly  is a large margin  sample error mini- mization algorithm for Fy,  a suitable Lc  is that which returns the func- tion Ly z,rf   corresponding to the values of V  and 7 for which  is within  1 m  of its infimum  over all values of 7 E  0,1] and  V  £ It*".  Theorem  15.4  There are constants c,ci  such that the following  holds. Suppose P  is  a probability  distribution on X  x  {0,1},  and z  €  Z m  is chosen according  to Pm.  For the two-layer  network learning  algorithm Lc  described  above,  with probability  at least 1 — S we  have      »   to   Y.  +ta  a 2   \7    \7*  In the same way as for  Theorem  15.3, if we define  F  =  IJv Fv  to   be the class of functions  computed by two-layer networks with  unbounded parameters,  we can  think  of the  algorithm  described  by  Theorem  15.4 as minimizing over F  an expression of the form   sample error of     +   complexity penalty for     ,  where the  complexity  penalty  involves both  the  scale parameter  7  and the size V  of the parameters in the network that  computes the  function  .  The theorem shows that including the complexity penalty term in the optimization  will probably  result  in  a function  that  has error  no more than  the  minimum  of the  sum  of the  error  er'p f    with  respect  to  7  and  a penalty  term  that  depends on the  complexity  of the  network  at scale  7.  The  quantity  erp     can  be  larger  than  the  misclassification probability  erp sgn      of  the  thresholded  function.  This  is  the  cost incurred by measuring complexity in terms of the size of the parameters rather  than  the  number  of parameters.  On the other  hand,  this  result and  Theorem  15.3 are  incomparable,  since  there  are  situations  where each approach results in a smaller error than the other.  Theorems  15.3 and  15.4  are  examples  of  more  general  results  that bound the error of algorithms that  minimize a sum of sample error and a complexity penalty term.  We shall discuss these more general results in Section 15.4.   15.3 Proofs of the results   223  15.3  Proofs of the  Results  The proofs of these theorems use the following lemma, which shows how to  extend  a probability  statement  that  is true  for  any fixed value of a parameter  a  £   0,1] to one that  is uniform  over all values of a.  In the application  to  Theorem  15.1 above, for  example, a  corresponds to  the reciprocal of the complexity parameter  W.  Lemma  15.5  Suppose P  is a probability  distribution  and  {E{aua2,6    :0  < aua2,6  <1}  is a set of events such  that:   i   For all 0 < a  < 1 and 0 < S < 1,   ii   For all 0 < ax  < a  < a2  <  1 and 0 < Sx < S < 1,  P E a,a,6    <S.  E aua2,Si CE a,a,6 .  Then for 0<a,5  < 1,  P  I   J  E aa, a, 6a{l -  a   j  < 6.  \«G O,1]      The idea of the proof is to split the interval of values of the parameter a into a countable collection of intervals that get exponentially smaller as a gets close to zero, and allocate an exponentially decreasing proportion of the probability S to these sets. In a sense, we are penalizing small values of  a,  since  the  conditions  of  the  theorem  imply  that  the  probability statements we make get weaker as a gets close to zero. In the application to Theorems 15.1 and 15.2 above, this implies that we make progressively weaker statements as the complexity parameters W, V, and 1 7 increase. However, in this  case it  is not  a significant  penalty,  since it  introduces only an extra logarithmic factor  into the error bounds.  Proof  We assume, of course, that  for  all 0 <  a < 1 and  0 <  S < 1, the event Ua€ o,i] E aa,a,6a l   -  a   is measurable.  Then we have   224   Model  Selection  =  P  Q  {E{aa,aM{l~a     : a G  a*1,a']}  \i=0  \i=0  <  6 1  - a   *=o  D  Proof   of  Theorem  15.3   We  combine  the  first  inequality  of  Theo- rem  15.1 with  Lemma  15.5, taking i? ai,a2, J   to be the set  of z  G  Zm for which some h in Hw2  has   ~   Wi lnm +  ln  where W\  =  [1 aiJ  and W2 =  [l «2j     In this proof, we do not take the trouble to evaluate  any universal  constants, but  it  is clear that  suitable constants  can be found.   Lemma  15.5 implies   on taking  a  =  1 2,  say  that,  with  probability at  least  1 —  5, every h G  IJ^ Hw  satisfies  &M h  +  ^   wln Wm  + In    ^\\\  for  some constant  c,  where  W  is the  smallest  value for which  h G  Hw- In particular, if Lc z   €  Hw,  we have  1 2  erp Lc *    c     —  W*]n W*m  + lnl-^- jj    W\\\1 2   15.6   for  any  W*  and  ft*  G Hw*,  from  the  definition  of  the  algorithm  Lc. Now, let  W*  and ft* G w"  be such that  is  within  1 m  of  its  infimum  over  all  W  and  h  G .ffw-  Then  combin- ing   15.6   with the second inequality  of Theorem  15.1 shows that   15.4 Remarks   225  eTP L  z    The  proof  of  Theorem  15.4  is  similar,  except  that  Lemma  15.5  is applied twice.  For the first application, we fix 7 and define E ai,  ai, 5  to be the set of z € Z m  for  which some    in i<V2 has  where V\ =  1 ai  and  V2  =  I 0C2.  For the second application, we define E ai1a2,6   Fy  satisfy  to  be the  set  of z  e  Zm  for  which  some V  and  some    in  where 71 =  ai  and 72 = c*2.  15.4  Remarks  The model selection methods described  in this chapter  are similar to  a number of techniques that  are commonly used by neural network prac- titioners.  The  learning algorithm  Lc,  described  in  Theorem  15.4, that approximately minimizes the quantity   15.5 , is qualitatively similar  to a popular technique known as weight  decay. In this technique, the cost function   typically,  total  squared  error  on  the  training  data   is  aug- mented  with  a  penalty  term  involving  the  sum  of  the  squares  of  the parameters, and gradient descent is used to locally minimize this penal- ized cost function.  In the same way,  15.5  is a combination of a sample error term  and  a penalty term  that  increases as the size of the weights increase.  'Early stopping' is a related  approach.  Here, the gradient de- scent  procedure  is  initialized  near  the  origin  of  parameter  space,  and stopped  after  only  a  small  number  of  steps  have  been  taken.  In  this case, the final parameters are guaranteed to be small.  It is clear that  the proof of the first main result of this chapter   The- orem  15.3   extends  to  give  a  similar  result  for  any  indexed  family  of function  classes  {Ht  : t  G N} that  have  increasing  VC-dimension.  In this case, the penalty term  involves the VC-dimension  of the class con- taining the function.  Similarly, Theorem  15.4 extends to give a similar   226   Model  Selection  result  for  any  indexed  family  of function  classes  {Ft  : t  €  E+ }  with  in- creasing  fat-shattering  dimension,  and  the  corresponding  penalty  term involves the fat-shattering dimension.  One interpretation of these results is  that  in  balancing  the  performance  of  a  function  on  the  sample  and the  complexity  of  the  function,  we should  aim  to  minimize  a  combina- tion  of the error on the  training data  and the  number of bits  needed  to accurately  describe  the  function's  behaviour  on the  training  data   that is, the  logarithm  of the growth function,  or the logarithm  of the appro- priate  covering  number.   This  is  similar  to  the  philosophy  behind  the minimum  description  length principle,  a model selection technique  that advocates  choosing  a function  providing the  shortest  description  of  the labels  of  the  training  points.  Here, the  length  of the  description  is  the total  number of bits needed to specify  both the identity  of the  function and the identity  of the points on which it errs.  It is possible to derive slightly different  versions of these results using the relative uniform convergence results of Theorems 5.7 and 13.7.  These lead to error bounds  of the  form  evP Lc z   < a  inf Qnf^ erP ft  + i   win Wm   + In  and  <  d  inf    inf  erUf   + -   ^   In2 m  In  -}  + In  ^]         for  the  two  neural  network  classes  considered  here.   Of  course,  more general results  are possible for indexed  families  of function  classes  with increasing  VC-dimension  or  fat-shattering  dimension.   These  results have the  attractive  property  that  if  some function  in  one of the  classes has zero error, then  the  error of the  algorithm's  hypothesis  approaches zero at essentially the optimal rate  see, for instance, Section 5.3 .  That is,  for  every  algorithm,  the  estimation  error could  not  decrease  signifi- cantly faster, even if we knew in advance which function class contained the target function.  This property of an algorithm is known as 'adaptiv- ity':  the algorithm automatically adapts to the complexity of the  target function.  Although the rate of convergence of these methods  is nearly  optimal, they  suffer  from  the  drawback  that  they  are based  on  upper  bounds on error.  While  the  upper  bounds  are  tight  in  general   that  is,  there  are probability distributions that illustrate that they cannot be improved , if   15.5  Bibliographical notes   227  those upper bounds are loose for a particular problem, and in particular if the  complexity  that  minimizes  the  upper bound  does not  correspond to  that  which  minimizes  the  error,  then  these  methods  may  not  give optimal  results.  15.5  Bibliographical  Notes  A  general  result  of  the  form  of  Theorem  15.3  was  given  by  Vapnik  1982 ,  who named  the  approach  structural  risk  minimization.  Similar results,  for  a  sequence  of  classes  of  bounded  VC-dimension,  have  been presented by a number of authors, including Linial, Mansour and Rivest  1991 , Lugosi   1995 , Lugosi and Zeger  1996 , and Shawe-Taylor et al.  1996; 1998 .  Related results are given by Farag6 and Lugosi   1993  for linear threshold  networks, and by  Krzyzak  et  al.   1996   for radial  basis function  networks.  Barron   1991    see  also   Barron,  1994    has  inves- tigated  the  more  general  notion  of  complexity  regularization,  and  gave general  results  of the flavour of Theorems  15.3.  These  ideas  have  been applied  in a neural network  context  by Lugosi and  Zeger   1996 .  A  large  number  of  related  model  selection  methods  have  been  pro- posed.  Rissanen's  minimum  description  length   Rissanen,  1978    see also   Rissanen,  1986;  Rissanen,  1989  ,  Wallace's  minimum  message length   Wallace and Boulton,  1968 , and Akaike's information  criterion  Akaike, 1974  are some of the better known.  Kearns, Mansour, Ng and Ron   1997   give  a  critique  of  a number  of these  methods,  as well  as of structural  risk  minimization.  The  approach  formalized  in  Lemma  15.5  is  known  as  the  method of sieves;  see   Grenander,  1981 .  That  lemma,  as  well  as  Theorem  15.4, appeared in   Bartlett,  1998 .  Heuristic techniques that  aim to keep the weights of a neural network small  during  training,  such  as  weight  decay,  are  described  in   Hertz et  al.,  1991 .    Part  three  Learning Real-Valued  Functions    16  Learning Classes of Real  Functions  16.1  Introduction  This part  of the book examines supervised  learning problems in which we require a learning system to model the relationship between a pattern and  a real-valued  quantity.  For example, in using a neural  network  to predict  the future  price of shares on the stock exchange, or to estimate the probability that a particular patient will experience problems during a surgical procedure, the predictions are represented  by the real-valued output  of the network.  In  the  pattern  classification  problems  studied  in  Parts  1 and  2,  the  a;, y   pairs are generated  by a probability  distribution  on the  product space X  x  {0,1}.  In a similar way, we assume in this part  of the book that  the  data  is generated  by  a  probability  distribution  P  on  X  x R. This is a generalization of the pattern classification  model, and includes a  number  of other  data-generating  processes  as  special  cases.  For ex- ample, it  can model a  deterministic  relationship  between  patterns  and their  labels, where each   x, y   pair  satisfies  y =  f x   for  some  function  .  It  can model a deterministic relationship with  additive  independent observation  noise,  where  y\  =  f{x{   + 7 t,  and  the  rji are  independent and identically distributed  random variables.  It  can also model a noisy relationship  in  which  the  observation  noise  variables  r\{  are  mutually independent, but the distribution  of rji  depends on the pattern X{.  In the pattern classification  problem, we aim to find a function     that has Pr   x   ^  y  nearly minimal over a certain class of functions.  When f x   and y are real-valued, this aim is typically too ambitious.  Instead, it  is more appropriate to  take  account  of  'how far'  f x   is from  y  and not simply whether or not it equals y.  To measure how accurately  f x  approximates y, we use the  quadratic  loss: the aim is to find a  function  231   Learning  Classes  of Real  Functions  232  f for which the expected quadratic loss, E   x  — y 2, is nearly minimal over some function  class. This is called the  real prediction problem.  Many  other  choices  of  loss  function  are  possible   and  we  examine some of these  in  Chapter  21 , but  quadratic  loss has  several  desirable properties.  In particular, for random   x, y   € X  x R, it  is easy to show that  E   x   -  y 2  =  E  E »*  -  f x  2  + E  E y\x   -  y 2 ,    16.1   which implies that  choosing a function     to minimize quadratic  loss is equivalent to finding the best approximation of the conditional expecta- tion of y given x.  Since quadratic loss is not bounded, we need to make some additional assumptions  about  the  probability  distribution  P  that  generates  the  x, y  pairs.  To see why this is necessary, consider a distribution  P  that allocates  a  small  probability   say,  a   to  some pair   xo,2 o >  where  the magnitude  of  yo is  large.  Then  unless  a  learning  algorithm  sees  the pair  o, Vo    and this can be made arbitrarily unlikely , we cannot hope that it will choose a function that gives a near-optimal prediction for the point xo.  Even though the probability of this point is small, the cost of an inaccurate prediction can be arbitrarily large. To avoid this  difficulty, we assume that  the random variable y falls in a bounded interval.  This assumption is quite natural if y represents a physical quantity.  We shall insist, however, that  a learning algorithm  can cope with  any bound on the  length  of the interval.  One reason for  this  requirement  is that  the range of y values may not be known in advance.  It  is easy to construct  a similar example to that  above, showing that we also need some boundedness assumption on the real-valued  functions used by a learning algorithm.  Consequently, we assume that these func- tions map to a bounded interval.  We concentrate on functions  computed by  a  variety  of  neural  networks,  typically  with  a  bounded  activation function  at  the  output  or  a  bound  on  the  magnitudes  of  the  output parameters, so this constraint  is not too restrictive.  16.2  The  Learning  Framework  for  Real  Estimation  In  the  real  prediction  problem,  we assume  that  training  examples  are generated  at  random according to some probability distribution  on the set  X  x R  We define  the  error of a function     : X  -¥  E  with  respect to  P  to be the expected  value of     x   — y 2, where the expectation  is   16.2  The learning framework  for  real estimation   233  taken  with  respect  to  z  =    a;, y   drawn  according to  P.  We write this  as  where the  loss function  If  : X  x  E  ->  R+  is given  by  £f x,y   =     x   - 2  2.  Of  course,  if  P  is  concentrated  on  X  x  {0,1}  and  if     maps  to {0,1} then  the present  definition  of error coincides with  that  introduced in  Chapter  2.  We  shall  assume  that  the  range  of  the  functions  in  F  is the  bounded  interval   [0,1], and  that  P  is such  that  y  falls  in a  bounded interval,  so that  there  is a  bound  B  >  1 for  which   y -  y  2  < B2  for  all y  G [0,1].  That  is,  P  is  such  that  1 -  B  <  y  <  B  with  probability  1. Obviously,  the  particular  intervals  chosen  are  not  crucial,  since  we  can transform  the  problem  by shifting  and  rescaling the  functions  and  the  y values.  A  learning  algorithm  in  the  present  context  will  take  as  input  some sample  z  €   X  x  R  m  and  return  some function  in  F.  In  learning a  real function  class,  the  aim  is  to  produce,  with  high  probability,  a  function whose error  is close to  the  optimal  error,  opt P F   =  inf  G jperp   .  We formalize  this  as  follows.  Definition  16.1  Suppose  that  F  is  a set  of functions  mapping from  a domain  X  into  the  real interval  [0,1].  A  learning  algorithm  L  for  F  is a  function  L :  J  X x R  m -> H  oo  m=l  with  the following  property:    given  any e e   0,1 ,    given  any 5 £   0,1 ,    given  any B  >  1,  there  is  an integer  mo e, 5, B   such that  ifm>  mo e,<J,B   then,     for  any probability  distribution  P  on X  x  [1 —  B,B],  if z  is  a  training  sample  of length m,  drawn  randomly  according to  the product probability  distribution  Pm,  then,  with probability  at  least  1 — 6, the function  L z   output  by L  is  such that  where  erP L z   <optP F  + e  optP F  = jijf  erP   .   234   Learning Classes  of Real Functions  That is, for m  > mo e, <J, B ,  Pm  {evP{L z       1 -  6.  We say that F  is learnable  if there is a learning  algorithm for  F.  The sample complexity of a learning algorithm for F  is defined  in the obvious manner:  for  each e, <$, and  J3, m,L e,5,B   is the  least  possible value rao e, S,2?  can take in Definition  16.1. It  is easy to see that  L is a  learning algorithm  if and  only if there  is a function  e , m,<5,iJ   such that, for all S G   0,1   and B  >  1, 6L{m,6,B   -+ 0 as m  -> oo, and such that  for all m, <J, B, and P, with probability at least  1 — 8 over z  £  Zm chosen according to  Pm,  erp L z    < optp if   +eL m,8yB .  In  this  case,  the  least  possible  €L jn,6,B   is  the  estimation  error of the  algorithm  L.  It  is, as in  the  previous two learning models, a sim- ple matter  to interchange between estimation error bounds and sample complexity bounds.  We shall find it convenient to ignore the dependence on B  in the sam- ple complexity  and  estimation  error  bounds.  In proving upper  bounds on sample complexity in this and later chapters, we assume that  2? =  1, define  Z  =  X  x  [0,1], and  assume that  the  support  of the  probability distribution  is  a  subset  of  Z.  Hence,  the  loss  function  if  maps  from Z  to  [0,1].  This  simplifies  the  statements  and  proofs  of  the  results. Also  in  the  interests  of  conciseness,  we define  mi, e, $   as rai, €, J, 1 , and similarly for  CL* It is always straightforward  to extend these proofs to  arbitrary  B  > 1, without  changing any of the  algorithms.  We shall indicate these extensions in the  "Remarks" sections.  16.3  Learning Finite Classes of Real Functions  SEM algorithms learn finite classes  As with earlier learning problems, the 'empirical5 error  sample error  of a function  on the training sample is useful  as an indication of its error. Given a function     € F  and sample z =    xi,j i ,  x2,2 2 , - - - > 0&m,2 m   in Zm, we define the sample error er^     of    on z to be   16.3 Learning finite classes  of real functions   235  With this definition  of sample error, we can define a sample error min- imization   SEM  algorithm for a finite class F  to be a function  L from Zm  to F  with the property  that  Theorem  16.2  Let F  be a finite class of functions  mapping from  a set X  into the interval [0,1] and suppose  that L  is a SEM algorithm for  F. Then L  is a learning  algorithm for F,  whose estimation error satisfies  ex, m,<$   <   eo m,S   =  I —In  I ——  I ]     \  6   \m   Proof  Suppose  c =  eo{m^S .  By  Hoeffding's  inequality,  for  any fixed  >  Hence  Pm {3  G F,  er,    - erP    > }  <  W ^2,  which is 6, since c =  co e, 6 .  Denote by  *  any function  in F  for which  erp  *   = optp F   =  minerp F .   Note that  such an   *  exists, since F  is finite.  Then, with  probability at least  1 — S,    erP L z    <  er* L 2   +  <  &,  *  + § <   «*  *  +5 + f =  o p t p^  + e.  D   236   Learning Classes  of Real Functions  An  application to  neural networks  As a straightforward  application of Theorem 16.2, we have the following learnability result for neural networks in which we restrict the allowable states  to  be  those in  which the  weights  and  thresholds  are  expressible as a sequence of k binary  digits.   We assume that  for  each weight  and threshold,  one of the  k  bits  indicates  whether  the  weight  or  threshold value is positive or negative.   Theorem  16.3  Suppose that N  is a neural network with arbitrary  ac- tivation functions  and  an  output  that  takes values in  [0,1].  Let  F  be the set of functions  computable  by N  when each  weight and threshold is represented using k  bits.  Then any SEM  algorithm for  F  is a  learning algorithm for  F  and the sample complexity of any such algorithm  L  is bounded as follows:  where W  is the total number of adjustable  weights  and thresholds.  Proof  We need only note that  since there are 2* numbers expressible in the given form, the total number of possible states—that is, assignments of weights and  thresholds—is   2* ^  since there  are 2* possibilities  for each weight  and threshold.  Therefore,  F  is finite and  F  <  2kW.  The result follows immediately from Theorem  16.2.     16.4  A Substitute for Finiteness  When we studied classification by real function classes, we found it useful to  use  covering numbers,  and  to  bound  these  using  the  fat-shattering dimension and pseudo-dimension.  We shall carry out  a similar  analysis for the present framework in the next two chapters, but first we present a  slightly  less  sophisticated  way  of  extending  to  some  infinite  classes the type of analysis we have just  carried out for finite function  classes. This  approach  will,  in  a  sense,  be  superseded  by  results  in  the  next few chapters, but  is worth pursuing at  this stage since it  enables us to introduce the notion of an  approximate-SEM algorithm.  Suppose that  F  is some set  of functions  mapping from a  domain  X into the interval [0,1],  Recall from Chapter  14 that  the LQQ  metric  d^^   164  A  substitute for   finiteness   237  on F  is given by  xex  for f,g  € F.   We can obtain a result similar to Theorem 16.2, for function classes that  are not necessarily finite but that  are totally bounded with respect to the Loo metric.  In this result, the learning algorithm makes use  of  what  we  shall  call  an  approximate-SEM algorithm.  Although total  boundedness  with  respect  to the  LQO metric is a rather  stringent condition, we shall nevertheless find approximate-SEM algorithms useful for real prediction  when the function  classes satisfy  weaker conditions.  Definition  16.4  An approximate-SEM  algorithm A for a function class F  takes as input  a sample z  in Um=i  ^   x  K m  and an error bound e in 1+,  and returns a function  in F  such that  eiz A z,e    «  for  all z  and all e.  Note that, since F may be infinite, then, unlike the error measure used in  Part  1 of the  book, the set of values of erz f ,  as    ranges  through F,  may be infinite.  Thus, we use inf € Ferz     above, since there may be no minimum value of erz f .  For example, consider the class  of functions  defined  on R, and suppose that the training data z  satisfies Xi > 0 and  y% =  1 for all i.  Then inf € jperz f   =  0, since  for all i, but  every f  € F  has er z f   > 0.  The next  theorem  shows that  if  F  has finite covering numbers  with respect to the Loo metric, then  any approximate-SEM  algorithm  for  F can be used to construct  a learning algorithm.  Theorem  16.5  Suppose that F  is a class of functions  mapping to the interval [0,1]  that is totally bounded with respect  to d^^.  Suppose  that A  is  an  approximate-SEM algorithm  for  F  and  let  L  satisfy  L z   = .A z, eo 12   for  z  e  Zm,  where c0  =  y 72 m.  Then  L  is  a  learning   238   Learning  Classes  of Real Functions  algorithm for F  and  mL c,8   < mo €, J   = ^-ln f   j   j    Proof  Suppose  that  m  > rao e,<S , and  that  z  €  Z m.  Then,  since €o  = y 72 m,  and m > rao e, 5  > 72 e2, we have eo < e. It follows that  er,  L{z   = er*  A z,eo 12    <  inf er,    +   <  inf  er,    + -^.   Note  that,  since  e is not given as part  of the  input  to the learning algorithm, we cannot simply ask that the algorithm returns a hypothesis with sample error within e 12 of optimal.  Instead, we use Co, determined from  the length m of the sample,  and which turns out to be no more than e provided m > mo e, 6 .  This idea will be useful in later chapters.  We now note that if  , € F  are such that dLoo f,g   <a  < 1,  then  for any probability distribution P on Z = X x [0,1],  er P   -er P p <2a.  To prove this, we first observe that for any x € X,   f * -y 2    f x -g{x +9 x -y 2  =  =     *  -  g x   f x  + g x  -  2y  +  g x  -  yf <  2a +  g x -y \    16.2   where we have used the fact that    x  -  g x \ < a.  Therefore,  erP     = E    *   -  j  2 < E  2a +  g x  -  y 2   =2a + evP g ,  and since the same argument can be repeated with    and g interchanged, erp     — erp      <  2a.  Since P is arbitrary,  we also  have  erz     — erz g \  < 2a for all z.  For e > 0, let C€  \2 be some fixed  e 12-cover  for F with  respect to dLoo, of cardinality jV e 12, F^L^ -  Now, for any function    € F  there is    €   C€  i2 such that ofo^   ,    <  e 12 and so  and   e r p       - e r p        <!    e r *       - e r z        <   erp  *   < optp F  + £-.   16.3    16.4   for all z.  Let P be any distribution on Z and let  * be such that   16.5 Remarks   239  Denote by  *  a function  in Cc i2  such that  Prom the proof of Theorem 16.2, since m  > rao e, 5 , we have that  with probability at  least  1 — <S, for  all g G  Cc i2,  \&z 9  -  erP   <  —.    16.5   For convenience, we denote L z   by fz.  Choose fz  G  Cc i2 such  that dLooifzifz   < e 12.  Then the following chain of inequalities holds with probability at  least  1 - 5:  evP L z    =   evP fz   by  16.3   rz  z  +  ^    by  16.5   by  16.4   <   -  :    by   16.4   4    by  16.5   by   16.3   =  optP F   + e.  The result follows.      16.5  Remarks  It is trivial to extend Theorem  16.2 to the case of an arbitrary bound  B on \f x -y\;  the bound on the random variables in Hoeffding's  inequality is increased by a factor  of i?2, which gives   240   Learning Classes  of Real Functions  Similarly,  we can  extend  Theorem  16.5 to this  case.  Here,  Inequal- ity   16.2   relating   f x  -  y 2  to  g x  -  y 2  when  \f x  -  g x \  <  a becomes     *   ~ V?  < 25a  +  g x  -  y 2.  Hence, as  well as  changing the  constants in Hoeffding's  inequality, we need to adjust  the scale of the cover, which gives  TOD4  litf  -  <2   O  A * ^-  IOD\  U 1  Ji   ,  f 2AI   \\  € [I2n ,r,aL oo \ ;      —*  Learning  with  respect  to a touchstone  class  We mentioned  in Chapter  2 that  the  learning  model of binary  classi- fication may  be  weakened  by  asking only  that  the  algorithm  return a function  with  error  close to the  optimal  error in a 'touchstone  class'. The  same  modification  may  be made for real  prediction.  Explicitly, given the  touchstone  class T of real functions,  we may  modify  Defini- tion  16.1 by allowing the algorithm  to return a function  in a larger set F DT, and requiring only that, with high probability,  erp L *   < optp T  + c = inf  erp    + c.  Allowing an algorithm to choose a function  from the larger set F in this way can make learning computationally easier.  Furthermore, it can pro- vide a significant reduction in the sample complexity, compared to learn- ing the class T in the usual sense.  Chapter  20 gives an example of this kind: if the function  class T is finite, and F is the set of convex combina- tions of functions from T, the estimation error erp L z    -inf GT  &p f  of an approximate-SEM algorithm L decreases more quickly as a function of m  than  the  estimation  error  of any learning algorithm  that  returns functions from T.  16.6  Bibliographical  Notes  The  model  presented  in this  chapter  is a special  case of models  that have been  proposed  by  Vapnik   1982   and  Haussler   1992 .  We shall encounter these more general models in Chapter 21.  Theorem  16.5 is an easy extension  of Theorem  16.2.  This result  can be improved, to give a better convergence rate  Barron, 1994; McCaffrey and  Gallant,  1994 . In  Chapter  20, we shall prove a result that  implies this better  result   Theorem 20.10 .   17  Uniform  Convergence  Results for  Real  Function  Classes  17.1  Uniform  Convergence  for  Real  Functions  In Chapter  19, we shall show that  in many cases an  approximate-SEM algorithm  constitutes  a  learning  algorithm.  As in  the  development  of the earlier learning models, we first derive a uniform  convergence result for  classes of real-valued  functions.  The  following  result  and  its  proof are similar to those of Theorems 4.3 and 10.1.  Theorem 17.1 Suppose that F  is a set of functions defined on a domain X  and mapping into  the  real interval [0,1].  Let  P  be any  probability distribution on Z  =  X  x  [0,1], e any real number between 0 and 1, and m  any positive integer. Then  Pm  {some f  inF  has \erP f   -  erz f \  > e}  <  AMi  c 16, F,2m exp   -e 2m 32 .  Notice how this compares with Theorem 4.3: that bound involves the growth function,  whereas this involves the covering number.  As we have seen, the notion of error used here reduces to that of Chapter 4 when the functions  are {0, l}-valued, and the covering number is a generalization of  the  growth  function,  so in  a  sense this  result  is  a  generalization of Theorem  4.3.  Theorem  17.1 is also similar  to  Theorem  10.1, although  as noted  in  Chapter  10  that  result  is one-sided.   Another  difference is that  the present bound involves d\  covering numbers rather than  the larger doo covering numbers.   The proof of Theorem 17.1 uses the same key techniques as the proofs of Theorems 4.3 and  Theorem  10.1, namely symmetrization,  permuta- tion, and reduction to a finite class.  241   242   Uniform  Convergence  Results  for  Real  Function  Classes  Symmetrization  Lemma  17.2  With the notation as above,  let  Q = {zeZm:   some f  in F  has \erP f   -  erz f \  > e}  and  R =   r,5   e  Zm  x Zm  : some f  in F  has err     -  er,    >     Then, for m  > 4 c2,  Pm{Q   <  2P2m R .  Proof  The proof is similar to the proofs of Lemmas 4.4 and 10.2.  If some f  in F satisfies  erp     — err     > c and erp     — er,     <  e 2, then it also satisfies  err     -  er*    > e 2, so  P2m{R   >   P2 m{ 3   € F :  e r p       - e rr       > e a nd  =   [  Pm{s:3feF,  JQ  erP   -er5   <6 2}  erP     -  err    > c and  erP     -  er,     < e 2}  dP™ r .    17.1   Hoeffding's  inequality  shows that  Pm{erp   -ers    l 2  for all    €  F,  provided that  m >  4 c2,  and this, together  with   17.1 , shows that  P2m R   > Pm Q  2.  D  Permutation  and reduction to  a finite class  Recall that  Tm  is the set of permutations on  { 1 , 2 , . . ., 2m} that  switch elements i and m 4-i, for i in some subset of { 1 , . . ., m}.  By Lemma 4.5,  P2m R   =  EPr az  € R   <  max  Pv az G iJ ,  z€Z2m  where the expectation is over z in Z2m  chosen according to P2 m,  and the probability  is  over permutations  a  chosen uniformly  in  Fm.  To bound this  latter  probability,  we  again  use  the  notion  of  covering  numbers. We first require two technical  results concerning the  covering numbers of  IF  and  of  F.  The  first  concerns covers of   £F \X>  and  is  the  key  to the  reduction to  a  finite  class.  The  second  shows how  a bound  on  the cardinality of such covers can be given in terms of the covering numbers of the class F  itself.   17.1  Uniform  convergence for  real  functions  243  Lemma  17.3  Suppose that F is a set  of functions  mapping from  a set  X into  [0,1],  and for f  eFletif.Z  ->> [0,1] be given by lf x,  y   =     x   - y 2.  Denote  by lF  the collection {£f  :    eF}.  Letze  Z2m  and suppose that  {1>G \X  W an e 8-cover  with  respect to d\ for  ^F I  ,  where G C F. ,sm   E*Zm  50 tta^ z = Define r =    n , . .. ,rm   G Zm  and s =   su...    n , . ..  , rm, « i , . ..  ,5m .  Then,  if f  £ F  satisfies  err     -  er,     >  f,  g € G such  £fta£ err p   — era ^  >  .  Proof  Fix  2 =    n , . . . , rm, « i , . . . , sm   =    € F  satisfies  Z2m-  Suppose  that  and let g e G be such that  -  2m  Then  \err g   -  ei8 g \  m  t =l  i   2m  -4 E  i=m+l  2m  t=m+l  2m  >   err   -er,   -- m  >  e 4.  i  2m    -£ E -«  ««    i=m+l  Lemma  17.4  For all positive  integers m,  for all positive  numbers e,  and for  all z G Zm,  M e,   tF u,di   < M    e 2tF,m .  D   Uniform  Convergence Results for Real Function  Classes  244  Proof  Fix z  =    si,j i , x2,2 2 ,     » xm,l m    and  ,  € F.  Then the di-distance between the vectors   ^  =   ^  ^i ,... ,^  ^m    and £p,  is  t =i -  m  =  £ E  K  x«  " * *<     *<  + 9 *i  - 23 01  where  we have  used  the  fact  that,  since     and  g  map  into  [0,1]  and yi e  [0,1], \f{xi   + g xi  -  2yi\ < 2. It follows that, given any e 2-cover for Fx, there is an e-cover for   ^F Z > of the same cardinality.  The result follows.     We now use these results to bound maxzez2m  Pr <72 €  ? .  Lemma  17.5  For the set R  C Z2m  defined in Lemma 17.2, and for a permutation a  chosen  uniformly at random from  Tm,  max  Pr crz G R   < 2M   e 16,F,2m exp  Proof Suppose that z G  Z2m, where z\ =   xi, yi , and let T be a minimal e 8-cover for   ^F I  with respect  to the d\  metric.  Lemma  17.4 shows that  Pick G  C  F  such that  T  =   £G ,  and  G  =  \T\.  Lemma  17.3 shows that if az  = rs G i? then there is some f  £G  such that   e r r       - e r .        > e   4.  Thus,  <  Pr  3   € G:  >€ 4   17.2 Remarks   245  <   ITImaxPr  =   TmaxPr   tea   \  t=l TO  >e 4  >e 4  where each  % is independently  and uniformly  drawn from  {—1,1}.  By Hoeffding's  inequality,  the  probability  in the  last  line is no more  than 2exp -c2m 32 , which implies the result.     Theorem  17.1 now follows.  It  is trivially true for  m  < 4 e2,  because the  right-hand  side  of  the  bound  is  greater  than  1 in  that  case.  For m  > 4 e2,  we have  Pm Q   <2P2m R   < 4M   e 16,F,2m e-c2m 32,  c2  as required.  17.2  Remarks  The  results  in  this  chapter  do  not  depend  significantly  on  the  choice of  the  quadratic  loss  function.  In  particular,  except  in  the  proof  of Lemma 17.4, boundedness was the only property of this function  that we used.  An examination of the proof of that lemma reveals that we only use the fact that the loss function  £ f{x ,y   varies slowly when the  function value f x   varies. Indeed, it is easy to prove the following generalization of Lemma 17.4, which requires that the loss function  satisfies a Lipschitz condition.   To see that  it  is a generalization, notice that  the  quadratic loss function  satisfies  this condition with L  =  2 when the ranges of the functions  and of y are restricted to the interval  [0,1].   Lemma  17.6  Let Y  be a real interval and B  > 1, and suppose  that the loss function £ : [0,1] x Y  -* [0, B] satisfies the Lipschitz condition,  K 2 i,2 o  ~  %2,2 o   <  L\yx  -  y2\  for  all  2 o>2 i,2 2-  Define  £f x,y   =  £ f x ,y .  [0,1]-valued   functions,  Then  for  a  class  F  of  max   ^   e , ^F U, d i   < M   e   L , F , m  .  This  implies  the  following  generalization  of  Theorem  17.1.  Its  proof is  essentially  identical  to  that  of  Theorem  17.1, except  that  we need   246   Uniform Convergence Results for Real Function  Classes  to take account  of the changed bound  on the loss function  in  applying Hoeffding's  inequality.  Theorem 17.7 Suppose that F  is a set of functions defined on a domain X  and  mapping into  the  real interval [0,1],  Y  C  E,  and B  >  1.  Let P  be any probability  distribution on  Z  =  X  x  Y,  e  any  real  number between 0  and 1,  and m  any positive integer.  Then for  a loss function £ : [0,1] x Y  -4 [0, B] satisfying the Lipschitz condition of Lemma 17.6,  Pm  < some f  in F has  <  4M   e  8L ,F,2m exp   -e 2m  32B4  .  Notice that  if £ is the quadratic loss and Y  is such that £ maps to [0, £], the Lipschitz condition is satisfied  with L = 22?.  17.3  Bibliographical  Notes  The  techniques  of  this  chapter  go  back  to  Vapnik  and  Chervonenkis  1971 , and  Pollard   1984    see also   Haussler,  1992  .  Lemma  17.6 is due to Natarajan   1993   see also  Bartlett, Long and Williamson, 1996; Vidyasagar, 1997  .   18  Bounding  Covering  Numbers  18.1  Introduction  We have seen that,  for the real prediction  model of learning considered in this part of the book, the d\ -covering numbers are crucial.  These cov- ering numbers are always bounded  above by the doo-covering numbers, the  main  object  of  study  in  Chapter  12, so the  upper  bounds  for  the doo-covering numbers  obtained  in  that  chapter  are  also upper  bounds on  the  di-covering  numbers.  However,  it  is  possible  to  obtain  better bounds on the di-covering numbers using more direct arguments.  As in Chapter  12, we present  two bounds, one in terms of the  fat-shattering dimension, and one in terms of the  pseudo-dimension.  18.2  Bounding with the Fat-Shattering  Dimension  This section gives a bound on the d\ -packing numbers in terms of the fat- shattering dimension.  This bound, presented  in the following  theorem, uses a similar proof to that  of Theorem  12.7, so some parts of the proof are only sketched.  Theorem  18.1  Suppose that F  is a set of real functions from a domain X  to the bounded interval [0,1]  and that 0 < e < 1.  Then  where b =  [4 eJ  and, with d = fatF   e 8  > 1,  We therefore obtain the following bound on the covering numbers.  d  ' £ " '   247   248   Bounding Covering Numbers  Theorem  18.2  Let F  be a set of real functions from a domain X  to the bounded  interval [0,1].  Let 0 <  e <  1 and let d =  fatF   e 8 .  Then for m>d> 1,  v  3dlog2 16em  de    Proof By Theorem 18.1 and the relationship between covering and pack- ing numbers,  Mi {e,F,m  < Mi   e,F,m  < 2   j   By Theorem 3.7, for m  > d > 1,  d    rn\    A\  *    pm\d    A\  ^  and hence   I + 1 < dlog2    -^    + 2 < dlog2  The result follows.      Proof  of  Theorem  18.1  As  in  the  proof  of  Theorem  12.7,  we first  relate  the  packing  number and dimension of F  to the quantized  version Qa F , for  appropriate  a.  Recall that  Qa F   =  {QQ f   : f  e  F],  where Qa    *  =  aL  x  aJ.  Fix e, m,  0 <  a  <  e, and  a; =   i,... ,xm   G Xm.  Consider   ,p  G F, and let  f\m  denote   f xi ,...,   f xm  .  Then, using the fact  that  \Qa{a -Qa{b \>Qa{\a~b\   for  all a, 6 € E   see the proof of Lemma 12.3 , we have   18.2 Bounding with the fat-shattering dimension   249  Hence, given any e-separated subset of Fa, there is an  e -  a -separated subset of Qa{F -  It follows that  Mi e,F,m <Mi{e-a,Qa F ,m .  The proof of Theorem  12.7 also showed that  fat Q a   F  c <fat F e-a 2   for a < 2e  see  12.4  .  lemma shows that  Now, let H denote Q€  4 F , and let d = ^Q€ A  F     €  4 .  The following  where b =  [4 eJ  and y is as defined  in the  lemma.  This implies Theo- rem 18.1.  Lemma  18.3  Let Y = {0,1,..., b} with b > 3,  and suppose  that \X\ = m  andH  C Yx  has fatjj  1  = d > 1.  Then  with  Proof By analogy with the proof of Theorem 12.7, for k > 2 and m > 1, define t\  fc, m  as  min I { A,r   : G 1-shatters ACX,  witnessed by r : A -> y, A ^   0} :  \X\=m,GCYx,\G\   = Jfc, and G is 3-separated} ,  or  take  ti fc,m   to be  infinite  if the  minimum  is over  the  empty  set. Here,  *3-separated'  means  with  respect  to di,  and  we regard  d\ as  a metric on Yx  by defining  di f,g   =  1 ra  £*€*  l    x   ~  x l-  By the  same argument  as in the  proof  of Lemma  12.9, it suffices to  show that  for  all d > 1 and m > 1.  Also as in the proof  of Lemma  12.9, we shall demonstrate this by proving a lower bound on t using induction.  Choose a set  G of 2b3^log2V^1^  pairwise 3-separated  functions,  and   250   Bounding Covering Numbers  split  G  arbitrarily  into fc 2 pairs.  Consider  any  pair   1,02   and  let I <mbe   the number of points x  for which  \gi x  -  ^   ^   > 2.  Then  =  ^  £   l* *  -  82 x \ < 1   U 4- 2 m  -  from  which we obtain Z  >  ra 6.   Note that  6 > 2.   It  follows that  there are kl 2  > km  2b  triples  gi,g2,x  with  \gi x  -  g2 z \ > 2.  By the pigeonhole principle there is some x0  € X  such that for more than  k  26  of the  pairs   pi,    we have i :co  ~* 02 o  >  2.  By the  pigeonhole principle again, there are i, j  6 F  with j  > i + 2 such that  for at  least  of ^e^epairs, wehave{pi xo ,p2 ^o } =  {*»i}- If we let Gi denote these functions  with p a?o  =  i and G2 denote the functions  with g xo  =  j,   it is easy to see that, for any g,g'  e  Gi,  and a similar statement is true for G2. It follows that G has two subsets, each of size at  least fc 63, and each 3-separated  on X  — {o}.  Thus,  An  easy  inductive  proof,  using  the  fact  that  £i 2,m   >  1 for  all  m, together with the observation that  m >  flog2 y]   obtained  in a manner similar to the corresponding observation in the proof of Theorem 12.7 , yields  completing the proof.      18.3  Bounding with the  Pseudo-Dimension  Since the fat-shattering  dimension is always no more than  the pseudo- dimension, if a function  class F has finite pseudo-dimension, then Theo- rem 18.2 trivially yields an upper bound on covering numbers in terms of the pseudo-dimension.  However, for  classes of finite  pseudo-dimension, a quite different  bound  can be obtained.   18.3 Bounding with the pseudo-dimension   251  Theorem  18.4  Let F  be a nonempty  set of real functions mapping from  a domain X  into  the real interval [0,1] and suppose that F has finite pseudo-dimension d.  Then   €,  F,m   < Mi  e, F, m   < e d + 1   {^\  for  all €  > 0.  We omit the proof of this theorem, but prove the slightly weaker result,   18.1   The result is obtained as a corollary of a more general result on packing numbers of F.  For a probability  distribution  P on X,  we define the pseudo-metric d^p   on the function  class F by  dLl P  f,9  = E    x  - g x \  = j  \f x  -  g x \ dP.  The  packing  numbers  of F  with  respect  to d^^p   are then  denoted M e,  F,d^i P  «  We shall prove the following  result.  Theorem  18.5  Let F  be a nonempty  set of real functions mapping from a domain X  into the real  interval [0,1],  and suppose F has  finite pseudo-dimension d.  Then  M e,F,dLl{P   < 2 f^ln  \^jj\  for  any probability  distribution P on X,  and for all 0 < e < 1.  To obtain  Inequality   18.1 , we can simply note that  if x € X m  and if we take Px to be the distribution  that  is uniform  on the entries of x and vanishes elsewhere, then  At e,F\x,di   — M{e,F,dpx ,  and so  Mi e,F,ra   =  max{A^ c,FB,di   : x €  X m  =  max{M e,F,dPx :xeXm}  <   2   — l nf   —  for all e > 0.  To proceed with the proof of Theorem 18.5, we first need a key lemma, and for this we shall need some additional notation.  For any z eRk  ,we   252   Bounding Covering Numbers  define the element sgn z  of {0,1}* to  be  sgn z  =  sgn 2i ,sgn 22 ,... ,  and  for x G Xk  and r G Rk let  s g n   F  . - r   = { s g n     u - r   :   € F },  where, as usual, f\x =    xi ,   x 2 ,...,  Lemma  18.6  Let F be a set of functions from X  to [0,1],  and suppose that P  is a probability distribution on X.  Let x  G Xk  be drawn according to the probability distribution Pk, and let r  be a random element of [0,1]* where  each entry  of r  is independently drawn according  to the uniform probability  distribution on [0,1].  Then for 0 < e < 1,  Esgn Fu-r >M €,F,d Jrl p   l-^ 6,F,dLl P  e-^ ,  where  the expectation  is over the random choice  of x  and r.  Proof  Let  G be an e-separated  subset  of F,  with  respect  to dz^p , and suppose that  G has maximum possible cardinality  M{e^I Then we have  E  sgn Fu  - r    >  E  s g n   G u - r   >  E \{g G G : sgn  g\m -  r   ^ sgn  h\m -  r   Vft G G, h ^  g}\ =  ^ ^ Pr  sgn \g\x — rj  ^ sgn  h\x — r   Vft € G,h   ^£ g   g€G    ^  l — Pr  3h  E   J ,   I^   ,sgn  ^^  — rj  = sgn {h\x — rjjj  =   >  V^  I 1 — G  max  Pr  sgn  px — r   = sgn   i^  — r  g£G  ^  Now,  Pr  sgn  g{x -  r  = sgn  ftu  -  r    A;  =  TT  1 — Pr  ri is between  i xi   and  g xi     l-E\h xi -g xi \    18.3 Bounding with the pseudo-dimension   253  where the second  last  inequality  follows  from  Inequality   1.4  in  Ap- pendix 1, and the last  inequality follows from  the fact  that  h and g are e-separated.  The result follows, since we now have    We also have the following bound on sgn  FB — r   for any x £  Xk.  Lemma  18.7 Let F  be a set of functions  mapping from  X  to R and having pseudo-dimension  d > 1.  Then, for all k > d, e > 0, x  €  X k, and r e R*,  d  Proof  Let iJ be the set of {0, l}-valued functions  defined  on X  x R as follows.  The definition of pseudo-dimension implies that Pdim F  =  VCdim if , and it is easy to see that  The result follows immediately from  Theorem 3.7.      Theorem  18.5 now follows after  some manipulation.  Proof  of Theorem  18.5  Clearly, if d = Pdim F  = 0, Mi   e,F,ra  = 1,  so the result  holds  in  that  case.  Assume  then  that  d  >  1.  Let M  = M^^F^di^p  -  By the lemmas just  presented,  M l-Me-€k     <E  sgn F je - r    <   ^j\   ,    18.2       \  d  for k>d.  We want to establish that  M<2 -\n[-   254   Bounding Covering Numbers  for  0 <  e <  1.  If   l c ln 2M   <  d and  0 <  e <  1 then  the  right-hand side of this bound is  so the theorem holds in this case.  Suppose now that  d <  1 e  ln 2X . Taking  k  =  f l e ln 2.M l,  we have k  >  l e ln 2M ,  Me~€k  <  1 2, and k > d.  Thus,  Now,  and so  <  In  ^e   + U  Applying Inequality   1.2  from  Appendix  1 implies  which gives the result.      18.4  Comparing  the  Different  Approaches  It  is useful  to  see how the  bounds  of this  chapter  compare with  those implied by the results of Chapter 12.  Suppressing all multiplicative and additive constants   including those in the exponent  in order to emphasize the dependence on TO and e, the bound resulting from  Theorem  12.8 takes the form  fatF € 4 loga m  efatF e 4     whereas the bound of Theorem  18.2 has the form  ,jv  fatF c 8 log2 m  €fatF € 8     For a fixed value of e, the second bound grows more slowly with m than the first.  In this sense, the second bound  is often  better.   18.5 Remarks   255  Consider now the bounds involving the pseudo-dimension.  Suppress- ing again all multiplicative and additive constants, Theorem 12.2 implies a bound of the form  ^xPdim F   whereas the bound of Theorem  18.4 has the form  M c,F,m <   ^  Pdim F   The latter bound removes the dependence on m and hence is significantly better.  There are two bounds derived in this chapter:  one involving the  fat- shattering  dimension  and  one  involving  the  pseudo-dimension.  Con- cerning which of these is more useful,  we may make comments  similar to those made in Section  12.5.  Generally  speaking, if a class has finite pseudo-dimension,  then  for  small  e it  is  better  to  bound  Mi  e, F,  ra  using Theorem  18.4 rather  than  Theorem  18.2.  For  larger  values of e, or  for  classes  with  infinite  pseudo-dimension,  Theorem  18.2  may  give better results.  18.5  Remarks  All of the covering number bounds presented in this chapter assume that functions  in  the  class  map  to  the  interval  [0,1].  Lemma  14.12 shows that  we can easily  use these results to obtain  covering number  bounds for  classes  of  functions  that  map  to  any  bounded  interval,  simply  by scaling and shifting the functions.  A similar comment applies to packing numbers.  L<2{P   covering  numbers  The covering numbers discussed so far  in this book are perhaps not the most  natural.  For  example,  if  P  is  a  probability  distribution  on  X, consider the pseudo-metric di2{p   defined  by  dL2 P  f,9    =      J2   For the problem of learning to minimize squared error, this metric mea- sures the distance between functions  in some class F  in the most  direct way.  If  we have  access  to  a  small  cover  of  F  with  respect  to  dz,2 P    256   Bounding Covering Numbers  for every P,  then  we can use it  to construct  a learning algorithm.  The uniform  convergence result  for finite classes   that  follows  from  Hoeffd- ing's inequality  and  the  union bound—see  the  proof  of Theorem  16.2  shows that the element of the cover with smallest sample error will have expected  error  that  is  near-minimal  over  the  cover,  and  the  fact  that every function  in F  is approximated  by an element of the cover implies that  this error is also nearly minimal over all of the class F.  Of course, the fact  that  the learning algorithm does not know the probability dis- tribution  P—in general the cover will depend  on the distribution—is  a drawback to this approach.  In fact, it is possible to generate such a cover using training data, and its  size can  be bounded  in  terms  of the  fat-shattering  dimension.  For a sample  xi,...  ,xm   chosen according to a probability  distribution  F, choose a subset of F  whose restriction to the sample forms a cover of F with respect  to the  metric cfo.  It  is straightforward  to apply a  uniform convergence result like Theorem  17.1 to show that, for sufficiently  large m,  with  high  probability  this  subset  constitutes  a  cover  with  respect to the metric di,2 p .  Strictly  speaking, instead  of uniform  convergence over all functions in F, we need that, for all pairs of functions in F,  their distances under the metric Gfe  and under the metric dx,2 p  are close, but this follows easily from  Theorem  17.1. Thus, it  is possible to prove the following result.  Theorem  18.8  There are constants ci,C2,C3 such that, for  all probabil- ity distributions P  on  X,  In  Af e, F,dz,2 P     < cifatF   c2e  ln:2  i fatF   c3e2   A converse result  is immediate from  the lower bound on d\  covers in terms of the fat-shattering  dimension  Theorem 12.10 . This shows that  maxln  Af €,F,d MP     > maxln  Af e,F,dLl{P     > fatF  16e  8.  18.6  Bibliographical  Notes  Theorem 18.1 is from  Bartlett and Long, 1995 , and its proof uses ideas from   Alon et  al.,  1993 .  Theorem  18.4 is due to Haussler   1995 .  The proof  we give for  the  weaker  Inequality   18.1   comes  from   Haussler, 1992 , using techniques due to Pollard   1984  and  Dudley  1978 .   18.6 Bibliographical  notes   257  For a proof of a result analogous to Theorem  18.8, but for L\{P   cov- ers, see  Bartlett  et  al,  1997 .  Buescher and  Kumar   1991; 1992  used the approach of constructing an empirical cover to design powerful  gen- eral learning algorithms.  These algorithms can be successful  in certain situations   that  is, for  certain function  classes and probability  distribu- tions  in which approximate-SEM algorithms fail.  As we shall see in the next chapter, this difference  cannot occur in the learning problem stud- ied here, since approximate-SEM algorithms lead to learning algorithms for every learnable function  class.   19  The Sample Complexity of Learning Real  Function  Classes  19.1  Introduction  In  Chapter  16, we encountered  the  notion  of an  approximate-SEM  al- gorithm  for  a  function  class  F,  which  takes  as  input  a  sample  z  and a  positive  real number  e and  returns  a function  A z,  e   6  F  such  that erz {A z,e    < inf gj?erz     + e. We proved  that  if  a function  class is totally  bounded  with  respect  to the Loo metric, then  it  is learnable by an algorithm  derived, as in Theorem  16.5, from any  approximate-SEM algorithm.  In  this  chapter  we use the  uniform  convergence  results of Chapter  17 and  the  covering number  bounds  of  Chapter  18 to  extend this  result  to  other  function  classes  by  showing  that,  provided  F  has finite fat-shattering  dimension,  then  any  approximate-SEM  algorithm can be used to construct a learning algorithm for F.  We also give lower bounds on the sample complexity of any learning algorithm, in terms of the  fat-shattering  dimension  of the  function  class.  These  results show that  a function  class is learnable if and only if it has finite fat-shattering dimension.  19.2  Classes  with  Finite  Fat-Shattering  Dimension  The  main  result  The main result of this chapter, which makes use of the uniform conver- gence result obtained in Chapter  17 and the covering number bounds of Chapter  18, is as follows.  Theorem  19.1  Suppose that F  is a class of functions  mapping from a domain X  into the real interval [0,1],  and suppose also  that F  has  finite fat-shattering dimension. Let A  be  any approximate-SEM algorithm for  258   19.2  Classes  with finite fat-shattering  dimension   259  F  and define, for z  G  Zm,  L z   =  -4 2,eo 6 ,  where e0 =  16 y m.  Then L  is a learning  algorithm for F,  and its sample  complexity  satisfies  mL e,S  < mo e,<J  = ^    l8fatF  c 256  In2 H^\  + In  Hf  for  alle,6>0.  We  say that  the learning  algorithm  L  described  in Theorem  19.1 is  based on the approximateSEM  algorithm  A.  Proof  By Theorem 17.1, Pm  { 3   G F,  er P     -  er 2     > e 2} < 4M    e 32,F,2m e~€2m  128.  We claim that  this  quantity is at most 6 2,  provided m > mo e, 5 . Let d =  fatF   e 256 .  If d =  0, it is clear that M   e 32, F, 2m   <  1, and the claim follows, so assume that  d >  1.  Theorem  18.2  then  shows  that  4M   e 32,F, 2m e-Vi28  <   8   li£J   e - c  2      1 9 *\   3dlog2 512em  dc    For this to be at most <5 2, we need  >ln  which is equivalent to  .  A 6\   e2nx  _  > ln ^_J   + 3dln {—j log2 ^     1 2 8V   . ..    512e\   „,.     1 2 8V  j  + 3dln  ^—j log2 m.  We now use Inequality  1.2  of Appendix  1, ln m  < am — ln a — 1, choos- ing a  appropriately,  to verify  the claim.   The details of the  calculation are omitted.   Suppose  then  that  m  > rao c,<S , and that  z  G Zm.  Then,  with  probability at least  1 — 6 2,  a Pm-random  z  G Zm  is such  that  erP     -  er,     <  ,  for all    € F.      19.1   Furthermore,  since eo =  \ 256 m,  and m >  mo e,   256 e2,  we have Co < €, so it  follows  that  er*  L z   = er,  A z,e  6    <  mf e r ,       +  <  mf  er,    + .   19.2   Suppose that  *  G F  satisfies  <  optp F  +  .    19.3    260   Sample Complexity of Learning Real Function  Classes  Hoeffding's  inequality   Inequality   1.16   in Appendix  1  implies  that, with probability at least  1 — J 2,  &*  *  < erp f*   + -,    19-4   provided m >  18 e2 ln 2 5 .  Thus, for m > mo e,<S , with  probability at  least  1 — S we have  erP L *    <  erz L z   +     by  19.1   <  mf,er2     + !    by  19.2   +  j  <  optP F  + e,    by 19.4    by  19.3   and it follows that  L is a learning algorithm  whose sample  complexity is bounded above by mo e, 6 .  O  19.3  Classes with Finite  Pseudo-Dimension  Bounding sample complexity with pseudo-dimension  If  a function  class has finite pseudo-dimension,  then  it  also has  finite fat-shattering  dimension.  Hence, by the results of the previous section, it  is clear  that  F  is learnable  by an algorithm  derived  as above  from an approximate-SEM algorithm.  We can, however, use Theorem 18.4 to obtain an upper bound on the sample complexity of such algorithms in terms of the  pseudo-dimension.  Theorem  19.2  Suppose that F  is a class of functions  mapping from a  domain X  into  the interval [0,1]  of real numbers, and that F has finite pseudo-dimension.  Let A  be any approximate-SEM algorithm for F  and let L  be as described in the statement  of Theorem  19.1  that is, the learning algorithm  based on A .  Then L is a learning  algorithm for F  and its sample complexity is bounded as follows:  mL e,6   < mo e,5  = ^    2Pdim F   In   ^   + In  forall0<e,6<  1.   19.4 Results for  neural networks   261  Proof  By Theorem 17.1 and Theorem 18.4, if d = Pdim F   and m >  d,  Pm{3f€F,\ev P h -evz h \>e 2}  This is easily checked to be at most 5 2 when m > rao e, S . The rest of the proof proceeds as in the proof of Theorem  19.1.     For a function  class of finite pseudo-dimension, the sample complexity bound of Theorem  19.2 is better than that  implied by Theorem  19.1 for small values of e. However, Theorem 19.1 is more widely applicable, since not  all  classes  with  finite  fat-shattering  dimension  have finite  pseudo- dimension.  19.4  Results for Neural Networks  Combining  Theorems  19.1  and  19.2  with  upper  bounds  on  the  fat- shattering dimension or pseudo-dimension  of a neural network  function class  immediately  gives  sample  complexity  bounds  for  learning  algo- rithms  for  that  class.  The  following  two  corollaries  describe  two such bounds.  Similar  results  can  be  obtained  for  other  classes  of  networks considered in Chapters 6, 8 and 14.  The  first  corollary  concerns  feed-forward  networks  with  a  bounded number  of  computation  units,  each  with  a  piecewise-polynomial  acti- vation  function.  Theorem  8.8  gives  a  bound  on  the  VC-dimension  of this  function  class, and  Theorem  14.1 extends  this  to  a  bound  on  the pseudo-dimension, which implies the following result.  Corollary  19.3  Suppose  that a feed-forward network N  has W weights and k  computation units  arranged  in L  layers. Suppose that each  com- putation  unit  has a fixed piecewise-polynomial  activation function  with p pieces  and degree no more than I.  Let F  be the class  of functions com- puted by N.  Then  any approximate-SEM  algorithm for  F  can be  used to define a learning  algorithm for  F,  and for fixed p  and I,  the  sample complexity  of this learning  algorithm  is  O  1    WLlnW  +  WL* \n   i   +ln   I  The second  corollary  concerns two-layer  networks with  bounded  pa- rameters, but  an  arbitrary  number  of first-layer  units.  Theorem  14.19   262   Sample  Complexity  of Learning Real Function  Classes  gives a bound  on the fat-shattering  dimension  of the  class of  functions computed  by  such  a  network.  Rather  than  combine  this  with  Theo- rem 19.1, we use the covering number bounds of Corollary 14.16 directly, giving an improvement  of log factors.  Corollary  19.4  Consider  the  class  of  two-layer  networks  defined  in Corollary  14-16,  with  inputs  in  [—A, A]n,  where  each computation  unit has  a  bound V  on  the  sum  of the  magnitudes  of the  associated  parame- ters,  and  an activation  function  that  is  bounded and satisfies  a  Lipschitz constraint  Let  F  be  the  class  of functions  computed  by  this  network. Any  approximate-SEM  algorithm  can  be used  to  define  a  learning algo- rithm  L  for  F  that  has sample  complexity  satisfying  19.5  Lower  Bounds  The following theorem gives a lower bound on the sample complexity of any learning algorithm,  in terms of the fat-shattering  dimension  of the function  class.  Together  with  Theorem  19.1, this  shows that finiteness of  the  fat-shattering  dimension  of  a  function  class  is  a  necessary  and sufficient  condition for the existence of a learning algorithm for the class.  Theorem  19.5  Suppose  that F  is  a class  of functions  mapping from  X to  [0,1].  Then for  B  >  2,  0  <  c <  1  and 0  <  S <  1 100,  any  learning algorithm  L  for  F  has sample  complexity  satisfying  for  any  0 <  a  <  1 4.  By  suitable  choice of  a,  this  theorem  implies  that  the  sample  com- plexity  of a  learning  algorithm  for  any  function  class  F  is n l c   and ft fatF   4c  .  There is a considerable gap between this result, which shows that  the  sample complexity of any learning algorithm for a class F  satisfies  ra e, <S, B   =  Q   -  +  fatF   4c  j  ,   19.5    19.5 Lower bounds   263  and  Theorem  19.1, which  describes  a  learning  algorithm  with  sample complexity   for fixed  B  and 8    e,69B  = O  ±   fatF  6 256  In2   7  m   For instance, suppose that F is a class with limc_^o fat^   e  =  Pdim F   < 00.  Then  the upper  bound  shows that  the sample complexity  grows as 1 e2   ignoring log factors , whereas the lower bound shows that it grows at  least  as  1 e.  This  gap  is inevitable;  we shall  see in  the  next  chap- ter  that  the  sample complexity  of a convex class F  with finite  pseudo- dimension grows as 1 e,  ignoring log factors, but for a non-convex class sample  complexity  grows as  1 e2.  There  are  also examples  that  illus- trate  that  the  second  term  in   19.5   cannot  be  improved  in  general. In  particular,  in  Chapter  26, we describe  a  learning  algorithm  for  the class of two-layer networks with an arbitrary number of first-layer  units and constraints on the size of the parameters.  This algorithm is an ap- proximate sample error minimization  algorithm that  returns a  function from  a certain  restricted  subset  of the  class.  It  has sample  complexity O fatjr   e  , ignoring log factors.  Proof  of Theorem 19.5  The idea of the proof is to reduce the learning problem  to  a related  learning problem  in the  restricted  model for  pat- tern classification.  Consider the class Hd of all functions  mapping from the finite set  {zi,... ,£<*} C X  to  {0,1}.  Theorem  5.3 shows that  any learning algorithm for  Hd has sample complexity  at  least   d —  l   32e  for suitably small e and S. We shall show that, for any fixed a between 0 and  1 4,  any learning algorithm for the class F  that  learns to accuracy e can  be  used  to  construct  a learning  algorithm  for  Hd that  learns  to accuracy a 2,  where d = fat^   e a .  Fix  0  <  a  <  1 4  and  0  <  e  <  1,  and  let  d  =  fat^   e a .  We can assume that  d  >  1, because the  theorem  is trivial  otherwise.  Suppose {!,...,£<*}  is  e a-shattered  by  F,  witnessed  by  ri,...,rd.  Without loss of generality, we suppose that  X  =  {x\,...  ,<*}. Suppose that  L is a learning algorithm for F.  Then we can construct  a learning algorithm for  Hd  as  follows.  For  each  labelled  example   i,2 i ,  the  algorithm passes to  L  the  labelled  example   xi,2 i ,  where  yi  =  2 if  y» =  1 and yi  =  -1  if  yi  =  0.  Let  P  be  the  original  distribution  on  X  x  {0,1}, and P  the resulting distribution on X  x {-1,2}.  Then given a  function    : X  -» [0,1] produced by L, the algorithm for Hd outputs the  function   264   Sample  Complexity  of Learning  Real  Function  Classes  h : X -* {0,1}, where h xi  = 1 if and only if f xi   > r*.  We claim that if erp f   -  optp F   < e then erP  i  < a 2.  To see this, notice that  optp F   =   inf  erp g   =  ME g x -y 2  gEr  <  Emm  { y-y 2:y€{r x ±  e a}},  where r xi   = r<.  The inequality follows from the fact  that  the  distri- bution is concentrated on a shattered set. It follows that  >  E [   *  -  yf  -  min { y -  yf  : y 6 {r{x  ± e a}}] .  Consider the quantity inside the square brackets. For x = Xi with yt = 0, y~i = -1  and this quantity is  £ ' + 2  Hence for y = 0, if     *  - V? - mTn { »-y  2:»6  {r x  ± e a}}  < ,  then f x  < r x ,  and hence h x  = j .  A similar argument applies when y = \.  Thus,   *  -  y 2 -  min { j  -  j  2  : y 6 {r x  ± e a}}  >     <   ^E   [   *   - yf  -  min  {{y -y 2:y6   {r x  ±  e a}}]  where the second last inequality is Markov's inequality  Inequality  1.10  in Appendix  1 . This implies the result.      19.6 Remarks   19.6  Remarks  265  It  is possible to improve on the  upper  bound  of Theorem  19.1 for spe- cific classes with infinite  pseudo-dimension,  using a chaining technique.  We have  seen  an  example  of  this  technique  in  Section  4.6;  it  led  to an  improved  sample  complexity  result  for  the  classification  problem.  However,  the  interesting  cases  of  neural  network  classes  with  infinite pseudo-dimension  are networks with  constraints  on the  size of the  pa- rameters but no constraint on the number of parameters.  In Chapter 20 we show that  all convex classes like this exhibit  faster  rates of  uniform convergence than one would expect from  Theorem  19.1. In Chapter 26, we use this result  to show that  a learning algorithm for these networks has a sample complexity that matches the lower bound of Theorem 19.5. It  is easy  to  extend  Theorems  19.1 and  19.2 to  the  case  where  the bound  B  >  1.  Using  the  result  from  Section  17.2  in  place  of  Theo- rem 17.1 gives the following sample complexity bounds, in terms of the fat-shattering  dimension and pseudo-dimension  respectively.  m i M,B    <  Restricted  model  Just  as in previous parts of the book, we can define a restricted version of the learning framework  for  real prediction,  in which the labelled ex- amples presented to the learning algorithm are of the form   x, f x    for some    €  F,  However such a model can have some unnatural  features that  arise  because  the  value  f x   can  encode  an  arbitrary  amount  of information  about   .   In previous models, the label could provide only one bit.   The following example describes an extreme case of this phe- nomenon, in which a single labelled example  x, f x    serves to uniquely identify  the function   .  In this case, the fat-shattering  dimension of F is not an appropriate measure of its complexity.  Example  19.6  For  a positive  integer d,  let  So,---,Sd-i  be disjoint subsets of X  with \Jj Sj  =  X.  Define the class of [0, l]-valued functions   266   Sample Complexity  of Learning Real Function  Classes  where  j=o   ife=o  and Is;  is the indicator function for Sj.  That is, the labels bj determine the two most significant bits of the binary representation  of the value  of the function  in  Sj,  and the d  least significant bits of  its  value at  any   Clearly,  for  any 7  <  1 4, x  €  X  encode the identity  of the function. fatFd   7   =  d.  Hence,  the  union  of  these  classes, F  =  IJdLi Fd has fat r  7  =  00 for 7 <  1 4,  but any f  in F  can be identified from a single example  x,  f x  .  There  are less restricted  models that  avoid  these  pathological  cases. For instance,  if the  labels  are noisy versions of the  function  values   so that  labelled  examples  are  of  the  form   x,     +  77 , for  some  noise variable 77   or if the labels are quantized versions of the function  values, the  amount  of information  passed  to  a  learner  in  a  label  is  restricted. Typically,  the  fat-shattering  dimension  is  the  appropriate  measure of complexity in such cases, in the sense that  a function  class is learnable in  the  appropriate  sense  if  and  only  if  its  fat-shattering  dimension  is finite.  We shall consider one such model at the end of the next  chapter: in that  case the labels are noisy versions of the function  values, and the noise is such that  the conditional expectation  function  E yx   is in the class  F.  Relative  uniform convergence  results  We shall see in the  next  chapter  that  the  rate  of uniform  convergence of erp f   to erz     for non-convex classes cannot be faster  than  l y m. However,  as  in  Parts  1 and  2,  it  is possible  to  obtain  a  faster  rate of uniform  convergence  of  eip f   to  a  value  slightly  larger  than  er*   . The following theorem is analogous to Theorems 5.7 and 13.7.  Theorem  19,7 Suppose that F  is a set o [0, l]-valued functions defined on a set X  and that P  is  a probability  distribution on Z  =  X  x  [0,1], For a, e > 0 and m  a positive integer,  we have  Pm  {SfeF:   evp f  >  1 + a «9r,    + e}   19.7 Bibliographical  notes   267  The theorem is a consequence of the following  result, since erp f   >   1 + a erz    + e if and only if  erP   -«M     ^  a  erp    + erz f   + 2e a   2 + a'  Theorem 19.8 ForF  and P  as in Theorem 19.7, v > 0 and 0 <  3 <  1,  S<M &,,,*. «P .«£ .  We omit the proof.  Model selection  It  is possible to derive model selection results for learning real  function classes that  are analogous to those presented in Chapter  15 for the case of pattern  classification.  Consider an indexed family  of function  classes {Ft : t G E+ } with increasing fat-shattering  dimension.  The estimation error bounds of this chapter, together with the techniques of Chapter 15 give error bounds for algorithms that choose a function from these classes to minimize a sum of the sample error for a function     and a complexity penalty for  .  19.7  Bibliographical  Notes  Alon, Ben-David,  Cesa-Bianchi and  Haussler   1997  gave the first  uni- form  convergence  result  in terms  of the  fat-shattering  dimension.  A learning result  like Theorem  19.1  but in terms of absolute loss, rather than  quadratic loss  is given in  Bartlett,  Long and  Williamson,  1996  using the results of Alon et a .,  together  with a result  like Lemma 17.6 relating covering numbers of a class to those of the loss function  class. For a description of the chaining technique, see, for example,  Pollard, 1984; Pollard, 1990 . Long  1998b  uses this technique to analyse a class with infinite pseudo-dimension that improves on the results presented in this chapter.  Birge and Massart  1993  give examples of function  classes and learning algorithms fair these classes that have faster  convergence of estimation error than sample error minimization  algorithms.  Bartlett,  Long and  Williamson   1996   showed  that  the  finiteness of the fat-shattering  dimension of a class F characterizes  the  learnability   268   Sample Complexity of Learning Real Function  Classes  of F  when the labels are noisy versions of the value of a target  function from the  class F.  Anthony  and  Bartlett   1995  showed that  the same property characterizes learnability for  a deterministic noise model, and when the labels are quantized versions of a target function's  value.   See also   Bartlett,  1994 .   Simon   1997   gave lower  bounds  in  terms  of a different  scale-sensitive dimension.  The  lower  bound  result,  Theorem  19.5, is  new.  The  example  that illustrates  the  problem  with  obtaining  lower  bounds  in  the  restricted model appeared in  Bartlett, Long and Williamson, 1996 . Theorem 19.8 is due to Haussler; it is an improvement of a result due to Pollard  1984 . A similar inequality, expressed in terms of doo covering numbers, can also be derived from a stronger result that is similar to Inequality  5.11 , but for real-valued functions   see  Bartlett  and Lugosi, 1999  .  Model selection results for neural networks and related classes of real- valued functions  have been presented by a number of authors, including Barron  1991; 1994 , Krzyzak et al.  1996 , and Lugosi and Zeger  1995 .   20  Convex Classes  20.1  Introduction  We have seen in the previous chapter that finiteness of the  fat-shattering dimension is necessary and  sufficient  for  learning.  Unfortunately,  there is a  considerable  gap  between  our  lower  and  upper  bounds  on  sample complexity.  Even for  a function  class with finite pseudo-dimension,  the bounds show only that the sample complexity is n l e   and O l e2 .  In this chapter, we show that this gap is not just a consequence of our lack of skill in proving sample complexity bounds: there are function  classes demonstrating that  both rates are possible.  More surprisingly, we show that  the  sample  complexity  or,  equivalently,  the  estimation  error  rate is determined  by the  'closure convexity' of the function  class.   Closure convexity  is  a  slightly  weaker  condition  than  convexity.   Specifically, for  function  classes with finite pseudo-dimension,  if the  class is closure convex, the sample complexity grows roughly as 1 e;  if it is not  closure convex, the sample complexity grows roughly as 1 e2,  and no other rates are possible  ignoring log factors .  To understand  the intuition  behind  these results, consider  a  domain X  of  cardinality  one.  In  this  case,  a  function  class  is equivalent  to  a bounded subset of the real numbers, and the learning problem is equiv- alent to finding the best approximation from that subset to the expecta- tion of a bounded random variable.  It is a standard result of probability theory that  the squared difference  between the sample average and  the expectation of such a random variable decreases as 1 ra.  If the  function class is a closed interval in R, then  clearly the excess error of the value returned  by a sample error minimization  algorithm   which  returns  the best  approximation  in  the  class to  the  sample  average   also  decreases as  1 ra,  and  so the  sample  complexity  is 0 1 e .  On the  other  hand,  269   270   Convex  Classes  if the function  class consists of two disjoint  closed intervals and the ex- pectation  of the random  variable falls  between the intervals, a learning algorithm  is forced  to  choose an  estimate  from  one or  other  of the in- tervals.  If the expectation  is c away from  the centre of the gap between the two intervals, the additional squared error that  arises from choosing the wrong interval is linear in e. For a suitably chosen probability distri- bution, this learning problem is equivalent to the problem of estimating the  expectation  of  a  Bernoulli  random  variable,  and  the  lower  bound result of Lemma 5.1 shows that  fi l e2   examples are necessary.  The  results in this  chapter  generalize these observations.  The single closed interval is an example of a convex function  class, and the sample complexity of this class is of order  1 e.   Recall that  a function  class F is convex if,  for  all  i, 2  in F  and  0 <  a  <  1, the  convex combination a i  +  1 — a  2  is also in F.   The union of two disjoint  closed intervals is an example of a non-convex function  class, and the sample complexity in this case is of order at  least  1 e2.  20.2  Lower  Bounds  for  Non-Convex  Classes  For the  sample complexity  results  in this  chapter,  we make use of the notion of 'closure convexity', a slightly weaker condition than convexity. To understand why a modification  of convexity will be relevant, consider again  a function  class F  defined  on a domain  X  of cardinality  one   so that  F  is equivalent  to  a  subset  of  R .  If  the  class  F  is the  union of the  intervals  [0,1 2   and   1 2,1],  then  it  is not  convex, but  it  is clear that the learning problem is equivalent to that involving the convex class [0,1],  since  F  contains  arbitrarily  good  approximations  to  the  missing point.  This  example  suggests  that  we do not  need  convexity  in  order to  obtain  fast  estimation  rates;  rather,  convexity  of the  closure of  the class  might  suffice.  The  appropriate  topology  in  which  to  define  the closure depends on the probability distribution on the domain X,  as the following definition  demonstrates.  Definition  20.1  For  a probability  distribution Px  on  X,  define the norm induced by Px  on the set of functions f  : X  -» R as  1 2  For a class F  of real-valued functions  defined on a set X  and a probability distribution  Px  on X,  let  F  denote  the  closure  of F  with  respect  to  this  a   20.2  Lower bounds for  non-convex classes   271  norm.  We say that such a class F  is closure convex if, for all probability distributions Px  on X,  F  is  convex.  The following theorem is the main result of this section.  It shows that the sample complexity of a class that  is not closure convex is  fi l e2 .  Theorem  20.2  For every class F  that  is not  closure  convex, there is a positive constant k  and a bound  B'  such that for  all 0  <  8 <  1, all sufficiently small e > 0,  all B  > B',  and all learning  algorithms  L  for F,  the sample complexity satisfies .  m^   kln l S   ,   mL e,8,B   >   \L-L.  The  proof  of the  theorem  involves two lemmas.  In  the  first  lemma, we think  of  the  function  class  F  as  a  subset  of  the  Hilbert  space of real-valued functions  equipped with the scalar product    ,<? =  f  Jx  f{x g{x dPx x ,  where Px  is the probability distribution  on X.  This lemma shows that we may  assume  that  the  closure of  F  is compact   that  is, every  open cover of F has a finite subcover . The lemma follows from Theorem 19.5, the fact   Theorem  18.8  that finiteness of the fat-shattering  dimension implies total boundedness in Z>2 P , and the fact  that  every closed, to- tally bounded subset of a Hilbert space is compact.  Lemma 20.3    a class F  of real-valued functions is learnable,  then for any probability  distribution Px  on X,  F  is  compact.  The next  lemma shows that  if F  is not  closure convex, then  we can find an open ball in Hilbert space that  does not intersect the class, and whose surface intersects F  in at least two points.  Figure 20.1 illustrates the functions  defined  in the lemma.  Lemma  20.4     F  is compact  and not convex,  then there is a  bounded interval Y  C R,  a function  c : X  ->  Y,  and two functions   i, 2  £  F, such that for  all f  in F,  \\c -     >  \\c -  A  =  \\c -   2.  Proof Since F is not convex, there are functions g,h  G F and a constant a  G  0,1   for  which  the  function     =  ag +   1 — a h  is not  in  F.   See Figure 20.1.   To show that  the desired functions  c,   i, and   2  exist, we consider the smallest ball around     that  touches a point  in F.   This is   272  Convex  Classes  Fig. 20.1.  A two-dimensional slice through Hilbert space, illustrating the func- tion class F,  the functions  i,   a, and c defined in Lemma 20.4, and the con- struction used in the proof of that lemma.   Notice that g and h need not lie in the same plane as  ,   i,  and  2.    Notice  that  the  minimum  exists  represented by the small circle in Figure 20.1.   Thus, the radius of this ball  is  S =  min{ ' -      :  '  €  F}.  \\f  -     =  5} and is positive because F  is compact.   If the set  {f'eF:  contains  more than  one  function,  the  lemma  is  proved,  with  c  =   .  If the set  contains only one function   1, we consider the smallest  ball  that touches  F  at  f\  and  some  other  point,  and  has  its  centre  on  the  ray from   1  that  passes  through   .  That  is,  we  set  c  =   1 -I-  ?    -   ? i with  the  smallest  ft  >  0 such that  some   '  in  F  has    ;  -   i  >  0 and  '  -  c\\ =  I i  -  c.   The  set  of suitable  points  c is represented  by  the dotted  line  in  Figure  20.1.   We  show  that  such  a   3  exists  by  showing that,  for sufficiently  large ft, the  ball includes either g or  h.  Now,  A similar calculation with h replacing g shows that  a   ll i  "  c2  -  \\g -  c2   +   1 -  a    HA -  c2  -  \\h -  c2    20.2  Lower  bounds for  non-convex  classes   273  =  =   ll i -   ll2 -  a3 -   2 -   1 -  a \\h -   2 + 2 A -   ,    -  c  ll i "  IP -  "IIS -   ll2 -   1 -  a \\h -   2 + 2 ? ! -   2.  For sufficiently  large  ?, this is greater than zero, so one of the terms on the left  hand side is greater than zero. In that  case, c is closer to either g or h than  to f\.     These lemmas establish  that  for  every non-convex class, we can find a  ball in  Hilbert  space whose interior  does not  intersect  the  class, but whose surface  touches the  class in at  least  two distinct  places.  This is enough to  show that,  by positioning  the  conditional  expectation  func- tion  E yx   inside  the  ball  approximately  equidistant  from  these  two functions,  we can make the learning problem as difficult  as the problem of estimating  the  probability  of  a  Bernoulli  random  variable.  That  is the idea behind the proof of Theorem 20.2. We omit some details of the proof; they are tedious but  straightforward  to  verify.  Proof  of Theorem  20.2  We show that the problem can be reduced to the Bernoulli random variable decision problem described in Lemma 5.1. Let   £i,... ,£m   in  {0, l }m  be  a  sequence of m  i.i.d. random  variables with  Pr &  =  1   =  a  and  a  €  {a~,a+},  where  a_  =  1 2  -  7 2  and a+  =  1 2 4- 7 2.  A learning algorithm for F  can be used to construct  a randomized decision rule  that is, a function  A mapping from  sequences in {0, l }m  to random variables in {a_,a+}   for this problem.  The idea is to pick two uniformly  bounded functions   { and f'2 with certain prop- erties.  For  an  input  sequence   £1,..., £m   €  {0, l }m,  A{£\,...,  fm   is computed by first choosing xi,... ,xm  E X  i.i.d. according to the prob- ability distribution  Px-  Then, for each i, if & =  1, A passes   xi,f[ xi   to the learning algorithm, otherwise it passes  xi,  Jfo  .  If the learning algorithm returns    € F  with    -   i <    -   2, A returns  1 2 + 7 2, otherwise it returns  1 2  -  7 2.  The functions  we construct are illustrated in Figure 20.2. The details are omitted, but  it is possible to choose functions   {  and  fy  so that  the following conditions are  satisfied.   i   Define  the  functions   j* and   I  as  the  conditional  expectation of the label passed  to the learning algorithm  when  a  =  a+  and when a  =  a-  respectively, ft  =   274  Convex  Classes h  fi      A   n  c  Y\  Fig. 20.2.  A two-dimensional slice through Hilbert space, illustrating the func- tions  used  in the  proof of Theorem  20.2.   ii   We can write  fi  =  for  some  constant  0  <  p  <  1,  and  hence  when  a  =  a+,   i  is the  function  in  F  that  gives  the  minimal  error,   i  —   £ 2  = inf € F   -   ill2,  and  similarly  when  a  =  a_,   2  -    2  =   iii   For e =  c\j   where c\  depends only on the class F , every f  6 F  satisfies  ll - 2l 2        -    2   <      -   l  .  So,  if  the  learning  algorithm  returns  a  function  f  e  F  with erp f   < optp F +e, then the decision rule A returns the correct answer for the Bernoulli problem.  These conditions, together  with the lower bound of Lemma 5.1, show that  to find a  function  in  F  with  evp f   <  optP F   +  e,  a  learning algorithm requires fi ln l <J  e2  examples.  To see that the labels passed to the learning algorithm lie in a bounded interval in E, first observe that the functions  1,  2  and c are uniformly   20.2  Lower  bounds for  non-convex classes   275  bounded,  and  the  bound  depends  only  on  the  function  class  F.  Fur- thermore,  we  can  write  f[  and  f2  as  convex  combinations   where  the coefficients  depend  on  e   of  fixed  linear  combinations  of  the  functions   i ,  2,  andc.     As  an  example,  consider  the  class  Fk of  two-layer  networks,  with  a linear  output  unit  and  A; first-layer computation  units,  each  with  the standard sigmoid activation function,  cr a  =  1  1  +  e~~a .  Theorem  20.5  For any k  €  N,  the  class Fk  is  not  convex,  even  if the input  space is X  =  R.  As a result, if the parameters are restricted  to  any compact  set,  it  is immediate that the class Fk is not closure convex.  Prom Theorems  8.13 and 14.1, the pseudo-dimension of Fk  is finite, so the sample complexity of this class grows as ln l <J  e2,  ignoring log factors.  The proof of Theorem 20.5 uses the following result, which shows that the functions computed by the first-layer units are linearly independent. The proof uses ideas from complex analysis, and can be skipped without loss of continuity.  Lemma  20.6  Let a  : R -> R  be the standard  sigmoid function.  Choose k  €  N  and  WI,WI,O,W2JW2,O>---,Wk,Wk,o   wj,Wjto   T£  ± wj>,Wj'to   for  all j  and all f     j.  Then the  set  €  R,  such  that  Wj  ^  0  and  {x  *-> a wjX  + wji0   : 1   1}  of real functions  defined on R is  linearly  independent.  Proof  We use i  to denote the square root of  — 1.  Consider the  complex function defined by a z   =  1  1 -f e"z   for z  € C.  This function has poles at  ±nm  for all odd n,  and is analytic  in the strip  {zed  \$t z \  <  TT},  where 9 z   denotes the imaginary part of z .  Hence, for a sequence of values of z contained in this strip and converging to iri, the product   z — ni a z   converges to a constant, the residue of the pole at ni.  The proof uses  this  fact  to  "isolate"  functions  and  show  that  the  corresponding coefficient  is zero.  Now,  suppose  that  <*o +  £jLi  <XJ<T WJX +  Wjto   =  0  for  all  x  e  R. Without  loss  of generality,  assume  that  0  <  iui  <  tt^  <        <  t0jb. Then  if  we  consider  the  analytic  continuation  of  this  function  in  the complex plane, it is identically zero on the strip {x  €  C : 9 wjfex   < TT}.   276   Convex  Classes  It follows that  the limit as x  approaches   iri - wjfe,o  w* of   x "  is  0.  Now, \WJ\ < \wk\ for  all j  < fc, and if some j  < k  has  \WJ\ =  \wk\ then Wjyo ^ Wk,o, so the limit of WjX + Wj,o is in the analytic region of a.  This implies  lim  X —*    7T«—ttffc ,0     tUfc        _ 7rt-t t;M \ \  Wk J  =  o,  and  hence  a*  = 0.  A  similar  argument  shows  that  ctj = 0 for j = 1,..., fc — 1, and so the set is linearly independent.      Proof   of  Theorem  20.5   Suppose that  the class Fk of functions  de- fined  on X  =  R is convex. Then choose two functions   i, fa G Fk defined by  and  suppose  that  Vj ^ 0,  t^j  0  and   itfj,tty,o     ± ti;j ,^ ,o   for all j  and  all  j1  ^ j.  Since  Fjt  is  convex,  there  must  be  parameters  and vi,...,Vfc  such that  is identically zero. An easy inductive argument using Lemma 20.6 shows that  for  all j  €   {1,..., k}  either  Wj =  0 or there  is a j'  G  {1,...,  2k} such that   wj,Wjto  =  ± itfj',i^',o .  It follows that there are  coefficients V'Q, ..., v'2k,  not all zero, for which  2*  2k  vo  +   20.3  Upper bounds for  convex classes   277  is identically zero, which contradicts Lemma 20.6. Thus F& is not convex.    20.3  Upper Bounds for Convex  Classes  The following  theorem  is a strong converse to Theorem  20.2.  It  shows that  closure convexity leads to a smaller sample complexity.  Theorem  20.7  Suppose  F  is  a closure convex class of functions  that map to  the interval [0,1],  A  is  an  approximate-SEM  algorithm for  F, and L z   =  A z,  1 m   for  z  €  Zm.  Suppose that the distribution P  on X  xR  is such that \f x   -  y\ < B  almost surely. Then  Hence, if  F  has finite fat-shattering  dimension,  then  L  is  a  learning algorithm  with  where d =  fatF   e  768J53  .  Furthermore,  if d =  Pdim F   is  finite,  L is a learning  algorithm,  and  The classes of functions defined in Section 14.4  see, for example, The- orems 14.14 and  14.17  are examples of closure convex function  classes. In  fact,  each  of these  classes can be  represented  as the  convex combi- nation  of  an  arbitrary  number  of  functions  from  a  certain  class.  For example, a two-layer network with a bound V  on the sum of the magni- tudes of the output  weights can be thought of as a convex combination of an arbitrary number of first-layer unit functions   scaled by the bound V .  Clearly, this class is convex, since a convex combination of two such functions  is also in the class.  The remainder  of this  section  gives an outline  of the  proof  of Theo- rem  20.7.  The following  lemma is the  key uniform  convergence  result. It uses information  on the variances of the random variables it considers to give a faster  convergence rate than earlier results.   278   Convex Classes  Lemma  20,8  Fix constants Ki  >  0  and K2  >  1.  Consider  a class  G of real functions defined  on a set Z,  and suppose  that for  every g  G G and every z  £  Z,  \g z \  <  K\.  Let P  be a probability  distribution  on Z for which Eg z   > 0 and E g z  2  <  K2Eg z   for all g  in G.  Then for e > 0, 0   max{4 i  +  K2   a2e ,K^  a2e }f  where  Ezg  =  ^  Y H = IP   ^    for  Z =     Z I , . . . , z m  .  The  proof  of this  lemma  is  similar  to  that  of Theorem  19.8, except that  Bernstein's inequality is used in the place of Hoeffding's  inequality  see   1.21   in  Appendix  1   to  give  tighter  bounds  when  the  random variables  have  small  variance.  To prove  Theorem  20.7,  we apply  this lemma to the class G of excess quadratic loss, G =  {£   — £fa  :    € F}, where fa  is the best  approximation  in F  to the conditional  expectation E y\x ,  and  £  x,y   =   y -  f x  2  is the  quadratic  loss function.   We need  to  allow  fa  €  F  in  case the  infimum  of the  quadratic  loss is not achieved  in  F.   The  following  lemma  shows that  the  class  G  satisfies the variance condition of Lemma 20.8 if F  is closure convex.  Lemma  20.9  Suppose  that  a  class  F  of functions  mapping  from  X  to [0,1]  is  closure  convex.  Iff  is  in  F  and  P  is  such  that  \y  — f x \  <  B almost  surely,  then for  fa  e  F  satisfying  E y  — fa x  2  =  inf €j p  E y  — f x  2,  we  have  <  4B 2E   x -  o x   2  Proof  It  is easy to show that  E [   y -     x     2 -   y -   a   x     2 ] 2  =  E [ y -   o x   +y-  <  4B 2E   o x -  x   2,    x   2  o x   -    x   2]   20.3  Upper bounds for  convex  classes  279  r  Fig.  20.3.  A two-dimensional  slice through Hilbert space,  illustrating  why  the inner  product  between  the  vectors   *  -  f a  and  fa  -     is  always  nonnegative for  a convex  class  F.  See  Lemma 20.9.  which is the first inequality.  To see the second inequality, notice that  =   E  2 2  -  fa x   fa x    -  where f* x   = E y\x   and,  as always, the  inner  product  is defined  by   >   =     f x 9{x   dP x ,  Hence, we need to show that  the inner prod- uct   * -  fa,  fa  -      is nonnegative.  It is clear from Figure 20.3 why this should be true.  By definition of  o, for all g e  F,   * -    a2  <  \\f*  -g\\2. In  particular,  since  F  is convex,  we can  take g  =  af  +  1 -  a fa  for a  € [0,1] to obtain  from  which it follows that  and hence  But for this to hold for all a  > 0,      -   o, fa  -      must be nonnegative. The second inequality follows.      280   Convex  Classes  Proof   of Theorem  20.7   We use Lemma  17.6 to give  bounds on the  covering  numbers of the  class G =  {£  -  £fa  :    €   F}, in terms of  the  covering  numbers  of F.  The  definitions  imply  that  K\  <  S 2 , and  Lemma  20.9  shows  that  K<i  < 4B2.  It is easy  to see that the function     =  L z  returned  by the approximate-SEM  algorithm has Ez tf-tfa <l m,  since  Now, either m is so small that the upper bound of the theorem is larger than  one, or 1 ra < c and so E2  if -  £fa   < e.  Setting a =  1 2 and scaling e appropriately  in Lemma  20.8 gives the first inequality of the theorem.   If the upper bound of the theorem is less than one, the con- dition  on m in Lemma  20.8 is satisfied.   The  pseudo-dimension and fat-shattering  dimension  results  follow  immediately  from  the covering number bounds of Theorems 18.4 and  18.2.  D  20.4  Remarks  Uniqueness of parameterization  For  any  parameterized  function  class, it is natural to ask  whether the parameterization  is unique;  that  is,  whether  for every  distinct  pair of parameter settings, the corresponding functions are distinct.   This ques- tion is important  in the design of gradient-based optimization schemes, since non-uniqueness can lead to numerical difficulties.   Lemma 20.6 was used to show that  the class of functions  computed by two-layer sigmoid networks with a fixed number of first-layer units,  Fk =  ^r,Vior wi-x + wito +vo : vuwiyj  €R>  ,  I   *=i   J  is not convex. In fact, it also tells us something about the uniqueness of the  parameterization  of this function  class.  Clearly, the  parameteriza- tion is not unique: if the output weight V{ of first-layer unit i is zero, we can arbitrarily change the values of the input weights w;, Wito associated with first-layer unit i, without  changing the function  computed.  Simi- larly, if we swap the labels of the input  weights and output  weights as- sociated with two first-layer units, the function  computed is unchanged. Also, by exploiting the fact  that  <r a  = 1 — or —a ,  we can negate the input  weights and output  weight  of a first-layer unit  and add one to the output  bias VQ, and the function  computed is unchanged.  However,   204  Remarks   281  Lemma 20.6 can be used in an easy inductive proof to show that,  apart from these  obvious invariances,  the  parameterization  is unique.  More precisely, these invariances define equivalence classes over the set of net- work states, and  any two states that  are in distinct  equivalence classes correspond to different  functions.  Restricted model  Section 19.6 discussed the restricted version of the real prediction prob- lem.  In that  section, we observed that  unnatural  phenomena can occur unless  the  amount  of  information  passed  to  the  learning  algorithm  is limited,  for  example  by observation  noise in  the  labels.  The  following theorem shows that, for any function  class F, if the noise has zero mean, so that  the conditional expectation  "E y\x   is in F,  the rate of  uniform convergence is the same as the fast  rate achieved by convex classes  see Theorem 20.7 .  Theorem  20.10  Suppose that F  is  a  class of functions  that  map  to the interval [0,1],  A  is  an approximate-SEM  algorithm for  F,  L z   = A z,  1 ra   for  z  6  Zm,  and the distribution P  on X  xR  is such that 1 0*0  -  y\ <  B  almost surely and E y\x   is in F.  Then  Clearly, the sample complexity bounds in Theorem 20.7 that  are ex- pressed  in  terms  of  pseudo-dimension  and  fat-shattering  function  are also valid in this case.  The proof is essentially identical to the proof of Theorem 20.7, except that  the last step of the proof of Lemma 20.9, where we used convexity to  show  that    *  —   o, o  —      >  0,  is  immediate  in  this  case,  since  Subsets of a closure convex set  The proof of Theorem 20.7 implies the following slightly stronger result. This  shows  that  we can  achieve  the  same  fast  rate  of  convergence of empirical averages to expected values in the class G =  {If—£fa  : f  €  F}, even when the class F  is not closure convex, as long as F  is contained in   282   Convex  Classes  a closure convex class F', and the best approximation fa  is chosen from F'.  Lemma  20.11  Suppose that F1  is  a closure  convex class of functions that  map from  X  to  the  interval [0,1],  and  that  F  C  F'.  Let  P  be a probability distribution  on  X  x  R  for  which almost  surely  \f x  — y\     0,  0    max{5B2  a2e ,B*  a2e },  where Qj =  If-  fix  .  tfa,  and fa  € F'   satisfies E y -  fa x    = i  20.5  Bibliographical  Notes  Theorem  20.2 is due to  Lee, Bartlett  and  Williamson   1998    see also Lee's PhD thesis  1996  . For further  information on the definitions  and results from topology used in the proof of that theorem  including a proof of the fact that every closed, totally bounded subset of a Hilbert space is compact , see, for example,  Kolmogorov and Fomin, 1975, Chapter 3 . The linear independence argument   Lemma 20.6  used to prove that two-layer sigmoid  networks are not  convex is due to  Albertihi,  Sontag and  Maillot   1993 .  They were interested  in the uniqueness of param- eterization  of  sigmoid  networks.  There  are  many  related  results  that apply  to  a  broad  family  of  activation  functions  of the first-layer units  see  Sussmann,  1992; Fefferman,  1994  .  Theorem  20.7 is  also  due  to  Lee et  al.   1996    Lee,  1996 .  Theo- rem 20.10 is also in  Lee et al., 1996 . This theorem generalizes several earlier results.  In particular,  it  was already known that  the faster  con- vergence  rate  can  be  achieved  in  two  situations:  when  the  labels  are generated  by  a  function  in  the  class  and  the  class  has  finite  pseudo- dimension   see   Haussler,  1992; Pollard,  1995  ,  and  when  the  condi- tional  expectation  is in  the  class and  the  class has finite L  covering numbers   see   Barron,  1994; McCaffrey  and  Gallant,  1994  .  In  fact,   20.5  Bibliographical  notes   283  the  proof  of  Lemma  20.8 extends  ideas  from  these  proofs,  which  also used  Bernstein's  inequality  to  take  advantage  of the  small  variance of the estimates.   21  Other  Learning  Problems  21.1  Loss  Functions   in  General  In Chapter  16, we made the observation that in the context of learning real  function  classes,  it  is  inappropriate  simply  to  define  the error of a  function     as the probability  that  f x ^y  for a  randomly  drawn  x,y .  Instead,  we defined  the error of    to be the expected  value of  f x   — y 2.  The quantity  f x   — y 2  may be thought of as the 'loss' of    on   x,y .  It might  have occurred to the reader that  there would be other approaches, based on different  methods of measuring such a loss. One straightforward variant comes, for instance, when we instead define the error of    to be the expected value of \f x   — y\.  In order to discuss the  use of  different  'loss  functions'  in  some  more  generality,  we shall assume that  our function  class F  maps from a set X  into the interval [0,1],  and that  the label  y  is chosen from some  set Y  C M.   Usually, as  before,  Y  will  be the interval  [0,1].   A  bounded loss function  is a function  I  :  [0,1] x  Y  ->  Rt,  for which  there  is  a  B  >  0  such  that l Pi>tfe   <  B  for all 2 1 e  [0,1] and y2  €  Y.  In  what  follows,  we shall assume that I maps to the interval [0,1].  Two examples of loss functions are the quadratic loss given by t{y^y'   =  {y — 2 ' 2> and the absolute loss given by l{y,y'   =  y -  y'\.  Given a particular loss function  £, we define, for    € F, the  function      : X  x Y  -> [0,1] by     *,y   =      * ,» ,  and  we let IF  =  {if       €  F}  be the  corresponding  loss  class.  The l-error  of    € F   with respect to a distribution P  on Z =  X  x Y  is the expected value of £   with respect to P,  284   21.2  Convergence for general loss functions   285  and, for z £ Zm,  the ^-sample error er*    is  - 1 71  21.2  Convergence  for General  Loss  Functions  As in Chapter 17, we can bound the rate at which sample errors converge to errors in terms of covering numbers.  As is clear from  its proof, the symmetrization result of Lemma 17.2 still holds when we replace the er- ror erp f   by erj>     and the sample error er*    by er*   .  Lemma 17.3 and Lemma 17.4 relate the covering numbers of the loss class Up to those of  F  itself.  The proof  of Theorem  17.1 contains  the following  result, which holds for any loss  function.  Theorem  21.1 Suppose that F  is a class  of functions mapping into the interval [0,1],  and that £ : [0,1] x Y  -> [0,1] is a loss function.  Let P be any probability  distribution on Z  = X  xY,  0 <  c <  1,  and m any positive integer. Then  Pm  {\evl  P h  -  ere  z h \ > e for some h €  To move from this general 'uniform convergence' result to more useful results,  we might  want  to  bound  the  rate  of convergence  in  terms of the  pseudo-dimension  or fat-shattering  dimension  of the class F.  One approach would be to use the results of previous chapters to bound the covering numbers of the loss class in terms of its pseudo-dimension or fat-shattering  dimension,  and then  to relate these dimensions to those of F itself.  Another approach is to obtain a direct relationship between the  covering numbers  of Up  and the covering numbers  of F  and then apply  the  results  of  earlier  chapters.  This  is the  approach  that  was taken in Chapter  17 for the quadratic loss function.  Lemma 17.6 shows that  this  approach  is suitable for any bounded  loss function  satisfying a  Lipschitz  condition.  The triangle  inequality  implies the  appropriate Lipschitz  condition  for the  absolute  loss, which  leads  to the  following result.   286   Other Learning Problems  Corollary  21.2  Let £ denote the absolute  loss function.  Then, for all positive integers k  and for  all positive numbers e,  Having related the covering numbers of £F  to those of F,  the rate of convergence of sample errors to errors may be bounded  in terms of the dimensions  of F.  One can,  in  a  manner  similar  to  Chapter  19, devise learning algorithms derived from approximate sample error minimization algorithms   where  the  sample  error  is  that  corresponding  to  the  loss function .  The details are omitted in this discussion.  21.3  Learning  in Multiple-Output  Networks  A  general  approach  The types of problems considered so far—and  the loss functions  used— deal with the case in which the set of functions under consideration maps into some subset of the real numbers.  But the context of loss  functions is  more  general  than  this  and,  in  particular,  will  allow  us  to  analyse learnability  for  function  classes  that  map  into  a  subset  of  R8  where s  > 1.  This encompasses, for  instance, the  sets of functions  computed by neural networks with s output  units.  Suppose that  F  maps from a set X  into R8.  How should we define  a suitable loss function  on F?  If we were to use the bounded loss function £ for a class mapping into R, then it would seem appropriate to use the loss function  £8 : R8  x R8  -> [0,1], defined  as follows:  For  instance,  if the  class F  is the  set  of functions  computed  by  an  s- output  neural  network,  and  £ is  the  quadratic  loss  function,  then £8 measures the loss as the average quadratic loss over the outputs,  t =i  t =l  For a given i between  1 and s and    € F, let  » denote the projection  of    on the ith  co-ordinate, defined  by  fi{x   =     *  «,   21.3  Learning in multiple-output networks  287  the ith entry of f{x   G  W,  and let F* =  { ,  :    G  F}.  For    G  F, we define ^  : l8x RM  [0,1] by  and  we let  £Fi  =  {£ <  :     €  F}.  Then  we have  the  following  useful relationship between the covering numbers for the loss class t8 F and the covering numbers for the loss classes £F. .  Theorem  21.3  With the above  notations,   or  all positive integers k  and all e > 0.  Proo   Let  z  €  Zk  be given, and  suppose that  for  each i between  1 and s, C{ is an e-cover for ^  i   Suppose that     G F  and that, for 1 < j  <  s, Vj G  Cj  is such that  d\{lf. z  Vj   < e.   Such a Vj exists since Cj  is an e-cover for lFi\x-   Then  It follows  that  <  e.  is an  e-cover  for  t8 follows.  F.  Observing that  \C\  <  \C1\\C2\  \C8\, the  result     288   Other Learning Problems  Relating the  covering numbers  of the  loss spaces  lFi  to  those of  we obtain the following  corollary.  Corollary  21.4  If I  is the quadratic loss function  then  for  all positive integers k  and all e > 0.  If I  is the absolute loss function, then  for  all €   and k.  Application  to  specific neural networks  We can define the sample complexity of a learning algorithm for a class of  vector-valued  functions  in  the  obvious  way,  with  the  loss  function I8  replacing the  quadratic  loss I  in  Definition  16.1.  We can  define  an approximate-SEM algorithm for such a class by extending Definition 16.4 in the same way. Then combining Theorem 21.1, Corollary 21.4, and the bounds  on  covering numbers for  neural  networks given in  Chapter  18, and using the same arguments as in the proof of Theorem 19.1 gives up- per bounds on the sample complexity of learning algorithms for  neural networks with 5 outputs.  These bounds are no more than s times the cor- responding upper  bounds for  a network with one output.  For instance, the following results give bounds for networks with few parameters and networks with small parameters.  These bounds are a factor  of s larger than the corresponding bounds described in Corollaries 19.3 and 19.4.  Theorem  21.5  Suppose that a feed-forward  network N  has W weights and k  computation units arranged in L  layers, where s  of these  compu- tation units are output units.  Suppose that each  computation unit has a fixed piecewise-polynomial  activation function  with p  pieces and  degree no more than 1.  Let F  be the class of functions  computed  by N.  Then any approximate-SEM algorithm for F  can be used to define a  learning algorithm  for  F,  and for fixed p  and I,  the sample complexity of this   214  Interpolation models   289  algorithm  is  0   ^   s {WLlnW + WL2  In Cjj  + In    ±\\Y  Theorem  21.6  Consider the  class of  two-layer networks  defined in Corollary  14-16, but with s  output units.  These networks have inputs in  [-A,A]n,  and each computation unit  has a bound V  on the sum of the magnitudes of the associated parameters,  and an activation function that is  bounded  and satisfies a Lipschitz constraint  Let F  be the  class of vector-valued functions  computed by this network. Any  approximate- SEM  algorithm  can be  used to define a learning  algorithm L for F  that has sample complexity satisfying  21.4  Interpolation Models  Learning and approximate interpolation  In  this  section,  we  take  a  fresh  approach  to  the  question  of  how  to extend  the  basic  learning  model  of  Part  I  for  binary  classification  to models of learning applicable to real-valued function  classes.  Specifically, we  extend  in  two  different  ways  the  restricted  model  of  learning  for {0, l}-classes.  Recall that  in this restricted  model of learning, we have a class of functions  H mapping from X  to {0,1}, a fixed   but unknown  probability distribution \i on the set X  of all possible examples, and some target function t G H.  The error of h € H  is then the  x-probability that h x   ^  t x ,  and the aim is to guarantee that  with high probability   at least  1 — <$,  in the usual notation , the error is small   at  most  e .  This could be extended  to  classes of real-valued  functions  using absolute or quadratic loss functions  in the obvious manner, but a different  approach can  be  taken,  as  follows.  Suppose  F  is  a  class  of  functions  mapping from  X  to the interval  [0,1], that  t is any function  from  X  to  [0,1]  not necessarily in the class F , and that \i is a probability distribution on  X. Let us suppose that  we have a learning algorithm  that,  given a sample x  =   zi,£2,...,zm ,  produces  a  function  f  €  F  that approximately interpolates the target function t on the sample, in the sense that   ar» — t xi \  < r\ for i =  1,2,..., m, where r\ is a small positive number  perhaps prescribed in advance . What performance might we reasonably demand of such an output hypothesis?  It is clear that since the values of    match   290   Other  Learning  Problems  those of t very closely on each example in the sample, the sample error with respect to either the absolute or quadratic loss function is small and therefore we might expect that our algorithm produces hypotheses that, with  high probability,  have small absolute or quadratic loss.  However, we might be able to ask for more. To see why, let us concentrate on the absolute loss. The fact that  \f xi   -  t x{ \  < 77 for i =  1,2,...,m means that the absolute loss on the sample is the average of m numbers, each of which is less than 77, and hence is itself less than  77. However, one could not, conversely, deduce from the fact that the sample error is small that all the differences  \f x{   -  t xi \  were very small: indeed, if all we knew was that  the sample error was at most  77, then the only general bound we could place on each of these differences  would be 77177. In this sense, therefore,  the condition  that  \f{xi   -  t xi \  < rj for each i  is stronger than the condition that the sample error is less than 77. Consequently, we might be able to demand more of such functions  .   We shall ask, roughly speaking,  that,  with  high  probability  the output  hypothesis     is such that  the values f x   and t x   are close almost  everywhere  rather  than simply close 'on average', which would be what  the usual  approach—a bound  on the error  with  respect  to  absolute  loss—would  entail .  We shall  take  two distinct  approaches  to  what  we mean  by  'close  almost everywhere'.  First,  we shall  ask that     approximates  to  t  as well as it  does on the sample,  almost  everywhere, in the sense that,  for all x, except for those from  a set of probability  at most  c, \f x   — t x \  < 77. This  is analogous  to the restricted  model  of learning  a  binary-valued class: there, from the fact  that  h equals t on a sample, we demand  that  with  high probability   h equals t  almost  everywhere on X.  To obtain the present model, we simply replace 'equals' by 'is within 77 of.  Definition  21.7  Suppose that F  is a class of functions  mapping from a set X  to the interval [0,1].  Then F  strongly generalizes from  approx- imate  interpolation  if for  any c, 5,77 €    0,1 ,  there is mo e,<J,77   such that for  m  >  mo  e, £,77 , for  any probability  distribution  1 in X  and any function  t:  X  -¥ [0,1],  the following holds: with probability  at least 1 -  5,  if x  =   a?i,X2,...,xm   €  X m,  \f xi   -  t x{ \  < j] for i = 1,2,... ,ra, we have  then for  any f  €  F   satisfying  »{x:\f x -t x \ l-e.  We now turn to our second way of thinking about what  'close almost everywhere' should mean.  It might at first  appear that  strong general- ization from interpolation is a rather demanding condition:  simply from   214  Interpolation models   291  the fact  that    XJ   -  t x{ \  < 77 for each X{  in the sample, we want that  for most samples x ,  f x   is at least as accurate an estimate of t x   on almost all of the examples X.  That  is, we expect the same high level of approximation almost everywhere.  Clearly—and as might have been an- ticipated from  the use of the word 'strongly' in Definition  21.7—we can weaken the definition by requiring a less accurate level of approximation. Specifically,  instead  of requiring  \f x   — t x \  < rj for  almost  all x,  we might instead relax the condition a little and ask that  \f{x —t x \  < 77+7 for almost all re, where 7 is a small, positive, number.  In this new model, therefore,  with  high probability, from  approximate interpolation  to ac- curacy  77  on  a  sample,  we expect  to  deduce  approximation  within  the slightly larger  'margin'  of 77 + 7  on almost  all of X   that  is, on all but a  subset  of  X  of  probability  less  than  e .  We arrive  at  the  following definition.  Definition  21.8  Suppose  that F  is a class of functions  mapping from a  set  X  to  the  interval [0,1].  Then F  generalizes  from  approximate interpolation  if for  any €,£,77,7 6   0,1 ,  there is mo e, 5,77,7  such that for  m  > rao c, $,77,7 , for  any probability  distribution \i  in X  and any function  t  : X  ->  [0,1],  the following holds:  with probability  at least 1 -  S,  if  x  =   xi,x2,...,a:m   e  Xm,  \f xi   — t x{ \  < 7  for i =  1,2,...,m,  we have  then for  any f  G F  satisfying  fi{x:\f x -t x \<r]  Characterizing strong generalization from  interpolation  The  problem  of  strong  generalization  from  approximate  interpolation may be considered within the loss functions  framework.  To see this, let us fix 7] €   0,1   and take C1  to be the loss function  given by  1  otherwise.  Then, for fixed 77, strong generalization from  interpolation  follows  from convergence of sample errors to errors for the loss function  P1. We know from previous results that we have such convergence if the  fat-shattering dimension of the loss class is finite.  In this case, the loss class lPF is {0,1}- valued,  so  its  fat-shattering  dimension  is  precisely  its  VC-dimension, which depends on 77. We therefore  find  the following  scale-sensitive di- mension relevant.   292   Other Learning Problems  Definition  21.9  For a class of functions  mapping from  a set X  into [0,1],  and for 77 £  0,1 ,  let the band  dimension be  The following  characterization  of strong generalization  from  approx- imate  interpolation  can  be  obtained.  The  proof  is omitted   but  note that  the  upper  bound  on sample complexity  follows  immediately  from Theorem 4.8 .  Theorem  21.10  Let  F  be a set  of functions  from  X  to  [0,1].  Then F  strongly generalizes  from  approximate interpolation if  and  only if band F,r    is finite  for  all rj €   0,1 .  When  this  condition holds, a suitable rao e, <5, rj   is  Furthermore, we must have  moM,77   > max   ^ ln  Q   , ^    band F,ij   -  when 5   4.  This theorem provides a characterization of strong generalization form approximate interpolation in terms of the the scale-sensitive band dimen- sion, and furthermore it shows that the sample size mo is quantified  fairly precisely by this dimension.  However, the band dimension is not one of the  standard  dimensions  we have encountered  thus  far.  The  following result relates it to the more familiar pseudo-dimension.  The  non-trivial, technical  proof is omitted.  Theorem  21.11  Suppose that F  is  a class of functions  mapping into [0,1].  Then  F  has  finite  band dimension  if  and  only  if  it  has finite pseudo-dimension. Furthermore, for  all 7] G   0,1 ,  We therefore  have the following,  more appealing, characterization of  strong generalization from  approximate  interpolation.  Theorem 21.12  Suppose that F  is  a set  of functions from  a set X  to  [0,1].  Then F   21.4  Interpolation models   293  strongly generalizes from  approximate  interpolation  if and only if F has finite pseudo-dimension. Furthermore,  if F  has finite pseudo-dimension Pdim F   then a sufficient sample length function for generalization from approximate  interpolation  is  «,ij   =  \  fl5Pdim F ln   {^  and any suitable sample length function  must satisfy  for  allr >O,ee    0,1 2   and  6 £   0,1 .  Thus, although it looks like a very difficult  definition to satisfy,  strong generalization  from  approximate  interpolation  holds  for  all  classes of finite pseudo-dimension.  The fact  that finite pseudo-dimension is neces- sary is in contrast to the learnability results of previous chapters, where it is enough to have finite fat-shattering  dimension.  Characterizing generalization from  interpolation  Since finite pseudo-dimension  is a sufficient  condition  for  strong gener- alization  from  approximate  interpolation,  it  is  also  a  sufficient  condi- tion for   the weaker  generalization  from  approximate interpolation.  It should be noted that  in analysing generalization from  interpolation, we cannot  express the  problem  within  the  loss functions  framework,  since the  'sample-based'  condition  \f xi   -  t xi \  < 77  is  a  statement  about the  sample  error  for  the  loss  function  tV   explicitly,  the  sample  error with  respect  to F1 is zero , whereas the  required  approximation  condi- tion, fj, {x  : \f x   — t x \  < 77 + 7} is a statement  about the different loss function  ^ + 7 .  It  is, however, possible to obtain  the following  'conver- gence result'.  Theorem  21.13  Suppose that F  is a class  of functions mapping from a domain X  to the real interval [0,1]  and that F  has finite fat-shattering dimension.  Let t  be any function from X  to R  and let 7,77,6  G  0,1 . Let \i  be any probability  distribution on X  and m  any positive integer. Define Pbad  to be the probability ofx€  X m  for  which  there exists f  € F with  »{xeX:\f x -t x \>   294   and  Then  Other Learning Problems  \f xi -t xi \<ri, l<i<  We  then  have  the following  characterization  of generalization  from approximate interpolation.  The sufficiency  follows from Theorem 21.13; the proof of necessity is omitted.  Theorem  21.14 Suppose that F  is a class of functions  mapping into [0,1].  Then F generalizes from  approximate  interpolation if and only  if F  has finite fat-shattering dimension.  Furthermore, there is a  constant c  such  that  if F  has finite fat-shattering  dimension,  then  a sufficient sample length for generalization from  approximate  interpolation is  It should be noted that for generalization from interpolation, it suffices to  have finite fat-shattering  dimension,  whereas the stronger  condition of finite pseudo-dimension must hold for strong generalization.  The two models therefore  turn out to be quite  different.  A result on large margin classification  It  is useful  to compare  some earlier  results  with  those just  given.  In particular,  it is possible to use our results  on generalization  from ap- proximate interpolation to derive a result useful for a restricted form of the  classification  learning  model of Part  2.  Recall  that  in this  frame- work, we consider a real function  class F that  is used for classification. For a probability distribution P on X x {0,1}, a positive number 7, and    € F,   we define  evl f   = P { *,»   : margin   a: ,y   < 7} .  In  Chapter  10, we proved  the following  convergence  result:  with the usual notation,  P m  {some    in F has erP     > erj     + e}   21.5  Remarks   295  The  following  result  is  similar  to  this,  but  is  more  specialized  in  two ways: first, it concerns only distributions P  that  correspond to a target function  t  : X  -»  {0,1} together  with  a  probability  distribution  \i on X   rather than  some arbitrary  distribution  on X  x {0,1} ; secondly, it bounds  the  probability  that  erp     >  e  and erj f   =  0,  rather  than providing a bound on the probability that  these two quantities differ  by a certain amount   where the latter  can be non-zero .  Theorem  21.15  Suppose  that F  is a set  of functions  mapping from a set X  to [0,1], that t:  X  ->  {0,1}, and that \x is a probability distribution on X.  Let 7  6   0,1 2]  and e €   0,1 .  For f  € F,   define erM  ,*   to  be fi{x  : sgn   x  -  1 2   ^  t x },  the error incurred in using the function f  for  binary classification. Let Pbad  be the probability  of x  6  Xm  for which some f  €  F  has margin f xi ,t xi    > 7  for  i  =   l,...,ra,  but erM  ,t   > e.  Then Pbad  < 2 j^oo  7 2,F, 2m  2" €m  2.  Proo   To prove this, we use the  result  of Theorem  21.13, which  states that  for  a, 7  G  0,1 , if  Q C Xm  is the  set  of x  for  which there  exists feF  with   x{x € X  :   x   -   * z  > a  + 7} > c  and  then   xm Q   <  2^   7 2,F,2m 2~ cm 2.  For  a fixed 7,  we take  a  = 1 2  -  7.  Then  a  + 7  =  1 2  and  \f x   -  * z   <  a  + 7  if  and  only if sgn   x  -  1 2   =  t x .  Therefore,  li{x  G X  :   x   -  t s  > a + 7} = <*„  ,* .  Furthermore,    a;j  ~ i xi  < a  =  1 2  -  7 is equivalent  to  margin   xt ,* a;i    > 7,  so it follows that Q is the set of x for which there is    € F with er M   , t   > e and such that  margin   i , t xi    > 7 for  i =  1,2,..., m.  The result follows immediately from this.     21.5  Remarks  Theorem 21.1 gives an upper  bound on estimation error that  decreases no faster  than  1 y m,  with  any  suitable  loss function.  In  Chapter  20,   296   Other Learning  Problems  we  proved  faster  convergence  rates  for  convex  function  classes  with quadratic loss.  Unfortunately,  this result relied on properties of Hilbert spaces that  do not extend to arbitrary loss functions.  The following ex- ample shows that simple convex classes can exhibit this slow convergence rate with the absolute loss function.  Example  21.16  Consider  the  problem  of  learning  the   convex   class of  all  [0, l]-valued  functions  defined on  a single point  x,  with  respect  to the  absolute  loss.  This  problem  is  equivalent  to  estimating  a  random y  €  [0,1]  by  choosing a  number y  G [0,1]  that  approximately  minimizes the  expected absolute difference E\y  — y\.  Let  the probability  distribution of y  satisfy  Pr y  =  0   =  a  and Pr y  =  1   =  1 — a,  where a  is  chosen uniformly  at  random from  the set  {1 2 ±  e}.  It  is  easy  to see  that  unless y is to the same  side  of1 2  as a,  then E\y—y\  is at least e bigger than  its optimal  value.  But  Lemma  5.1 shows that  this requires Q  l e2   \n l S   examples.  21.6  Bibliographical  Notes  The powerful  loss function framework described  in Section  21.1 is pre- sented,  for  example,  in   Vapnik,  1989 .  Haussler   1992   gives a  lucid decision-theoretic description of this approach.  The results on interpolation  and approximate interpolation  are  from  Anthony  and  Shawe-Taylor,  1994;  Anthony  and  Bartlett,  1995; An- thony, Bartlett, Ishai and Shawe-Taylor, 1996 . The connection between approximate interpolation  and  real classification  learning was observed in   Shawe-Taylor et al., 1998 .  We conjecture  that  Theorem  21.5, the  sample complexity  bound  for multiple output  networks, can be improved, since the proof of that the- orem ignores the fact  that  the functions  computed  by different  network outputs  share  common  computation  units.  Certainly  the  result  can- not  be  improved  significantly  for  the  case of  a  single  network  output. However  it  seems  likely  that  the  dependence  on  s  given  in  that  theo- rem is too strong.  This conjecture  is motivated  by an analogous result when the  network  output  units  are linear  threshold  units  and  the  loss function  is the  discrete loss  for  which ^ 2 1,2 2  takes the  value 0 when 2 i =2 2, and  1 otherwise ; see  Natarajan,  1989; Shawe-Taylor and An- thony, 1991; Anthony and Shawe-Taylor, 1993b .   Part  four Algorithmics    22  Efficient  Learning  22.1  Introduction  In  this  part  of the  book,  we turn  our  attention  to  aspects  of the  time complexity, or computational complexity of learning.  Until now we have discussed only the sample complexity of learning, and we have been using the  phrase  'learning  algorithm'  without  any  reference  to  algorithmics. But  issues of running time are crucial.  If a learning algorithm  is to be of practical  value, it  must, first,  be possible to  implement  the  learning algorithm on a computer; that  is, it must be  computable  and  therefore, in  a  real  sense,  an  algorithm, not  merely  a  function.  Furthermore,  it should be possible to produce a good output  hypothesis  'quickly'.  One subtlety  that  we have not  so far  explicitly  dealt  with  is that  a practical learning algorithm does not really output a hypothesis; rather, it  outputs  a  representation of  a  hypothesis.  In  the  context  of  neural networks, such a representation consists of a state of the network; that is, an assignment of weights and thresholds. In studying the computational complexity of a learning algorithm, one therefore might take into account the 'complexity' of the representation output  by the learning algorithm. However,  this  will  not  be  necessary  in  the  approach  taken  here.  For convenience, we shall continue to use notation suggesting that the output of a learning algorithm is a function  from  a class of hypotheses, but  the reader should be aware that, formally, the output  is a representation of such a  function.  22.2  Graded Function  Classes  Clearly, we have to be more precise about  what  we mean by a learning algorithm  working 'quickly'.  Suppose we have a learning algorithm  for a general class of neural networks, such as, say, the class of perceptrons. If  the  algorithm  is  to  be  useful,  then  the  time  taken  to  learn  using  299   300   Efficient  Learning  the  algorithm  should scale fairly  modestly  with the size of the  network, which in this case can be measured by the number of inputs.  Otherwise the  algorithm  may  work  quickly  enough  on  the  smallest  networks  but impossibly  slowly on the larger networks and would, arguably, be  useful only for  'toy  problems'.  To  formalize  this  notion  of  'scaling'  with  respect  to  the  number  of inputs,  we  use  the  idea  of  a  graded  function  class.  First,  we  assume that  Xn  is  a  subset  of  Rn:  usually,  Xn  will  equal  En  or  {0,1 }n.   In the  above example,  Xn  would  be  the  set  of all  possible  inputs  x  to  the n-input  network in the class.   Furthermore, Fn  will denote a set of real functions defined on Xn.   In the above example, this would be the set of functions  computable by the network with n inputs.   In many cases,  Fn will map just into {0,1},  in which case we call it a binary function class.  We shall often  use the notation  Hn  for such classes.   Given a  function class  Fn  for  each  positive  integer  n,  we  say  that  the  union  F  =  \jFn is  a  graded  function  class.  Using  the  graded  function  class  framework will enable us to introduce some generality in our description of learning algorithms and, furthermore,  it will allow us to address issues of scaling for learning algorithms.  Suppose that  we have a family  of neural networks  {iVn},  one for each positive integer n,  where Nn  is a network on n inputs.  Here, the appro- priate graded function class is F  =  \J Fn  where Fn  is the set of functions computable  by  network  iVn.  We  may  want  to  consider  what  might  be termed a 'general' learning algorithm for the class.  This is an algorithm that  works in essentially  the same manner  on each of the  networks  Nn. For  example,  as  we  shall  demonstrate  shortly,  there  is  a  learning  al- gorithm  for  the  perceptron  that  operates  by  solving  a  linear  program. The only essential  difference  between its actions on, say, the  perceptron with  17 inputs  and  the  one  with  25  inputs  lies  in  the  size  of  the  linear program; the  method  is fundamentally  the  same.  To  formalize  these  notions,  we  need  to  define  what  we  mean  by  a learning  algorithm  for  a  graded  function  class.  It  is  easy  to  modify the  definitions  of a learning algorithm for a binary function  class in the model  of  Part  1,  and  of  a  learning  algorithm  for  a  real  function  class in the model  of Part  3.   We shall not  discuss the  classification  learning model  of Part  2.   For a graded binary function  class H  =   J Hn,  we let Zn  =  Xn  x  {0,1}.  Then  a learning algorithm for  if  is a  mapping  n=l m=l   n=l   22.3  Efficient  learning   301  such  that  if  z  G Z™,  then  L z   G Hn,  and  for  each  n,  L  is  a  learning algorithm for ifn,  in the sense of Definition  2.1.  The only  difference  between  this  definition  and the  basic  notion  of  a learning  algorithm for  an  ungraded  class  is that  we have now  encapsu- lated  some  sense  of  the  'generality'  of  the  algorithm  in  its  action  over all  the  Hn.  We  define  a  learning  algorithm  for  a  graded  real  function class  F  =  J Fn  in  the  same  way.  It  is  now  possible  to  study  how  the 'complexity' of a learning algorithm scales with n,  and this is one of the topics of the next  section.  22.3  Efficient  Learning  We now  assume  that  learning  algorithms  are  algorithms  in  the  proper sense   that  is, that  they  are computable  functions .  We shall  implicitly use  the  'bit  cost'  model  of  computation,  in  which  it  is  assumed  that operations  are performed  on a fixed  number of bits in one unit  of time, and that  a single bit  can  be  read or written  in one  unit  of time.  Thus, even  though  we shall  consider  real-valued  inputs  and  outputs,  it  is  as- sumed  that  those  values  are  represented  using  a  finite  number  of  bits. We would like the computation time of learning algorithms to grow only slowly  as  the  difficulty  of  the  problem  is  increased.  For  instance,  we shall require that  the computation  time is polynomial  in the input  size. By  this  we  mean  that  as  the  number  of  input  bits  increases,  the  com- putation  time  is  no  more than  some  fixed  polynomial  function  of  that number.  For convenience, we shall assume that  the number of bits used to  represent  each  real  number  in  the  input  is  fixed,  and  consider  how the computation  time grows with the number of real number inputs.  Suppose that L is a learning algorithm for a graded real function  class F  =  U^n-   The  same  comments  apply  in  the  special  case  that  F  is a  graded  binary  function  class.   An  input  to  L  is  a  training  sample, which consists of m labelled real vectors of length n   that is, m elements of  Rn .  It  would  be  possible  to  use  the  total  number  of  binary  digits in  the  input  as the  measure  of  input  size,  but  there  is  some  advantage in  keeping  track  of  m  and  n  separately  and  assuming  that  each  real number  is  represented  with  a  constant  number  of  bits.  We  shall  use the notation  £, ra,n   to denote the worst-case running time of L  on  a training sample of m labelled examples, each of 'size' n.  Thus, i?jr, ra, n  is the maximum number of computation steps required for L to produce an  output  hypothesis  in  Fn  given  a  sample  in  Z™.   Notice  that  this notation  hides  the  dependence  of  running  time  on  the  precision  of  the   302   Efficient Learning  real number inputs.   Clearly, n is not the only parameter with which the running time of the learning procedure as a whole should be allowed to vary, since decreasing either the confidence parameter 8 or the accuracy parameter e makes the learning task more difficult.  The total time taken to produce an output hypothesis of accuracy e, with confidence given by 8, should vary in some appropriate way as 1 8 and  1 e  increase, but we should not want a decrease in either e or 8 to result  in too dramatic an increase in  the  time taken  for  the  learning task.  We could  simply  ask that  the running time increases polynomially with 1 8 and 1 e,  but  it is reasonable for us to be more stringent about the dependence on 1 8 for the following reason.  If the length of training sample input to an  efficient learning algorithm is doubled, we might expect the probability that  the output  hypothesis is 'bad5 to be approximately  squared.  Motivated  by this,  we shall  ask  that  the  running  time  of  a  learning  algorithm  L  be polynomial  in  m,  and  that  the  sample  complexity  m£, n,5,e   depend polynomially on In  1 8 ,  In the case of the accuracy parameter, we shall ask that  the running time of L be polynomial in m, and that  its sample complexity depend  polynomially on  1 e.  If both  these conditions  hold, then  the  running  time  required  to  produce  a  'good'  output  hypothesis is polynomial in n, ln l <J  and  1 e.  Combining these three notions of efficiency  together, we can now for- mally  define  what  we  mean  by  an  efficient  learning algorithm for  a graded function  class.  Definition  22.1  Let F  =  \JFn  be a graded  class of functions  and sup- pose that  L  is  a  learning algorithm  for  F.  We say  that L  is  efficient if:   i   the worst-case  running time RL m,n   of L  on samples z  G  Z™  is polynomial in m  and n, and   ii   the  sample complexity  mL n,e,<£   of L  on Fn  is polynomial in  n,l e  and  ln l 8 .  22 A  General  Classes  of Efficient  Learning  Algorithms  In  earlier  chapters,  much  emphasis  was placed  on  SEM   sample  error minimization  and approximate-SEM algorithms.  One can define a SEM algorithm  for  a  graded  binary  class  H  in  the  obvious  manner,  as  an algorithm that given any sample z G  Z™, returns a function  h G Hn  that has minimal sample error erz h   on z.  An approximate-SEM  algorithm   22.4  General classes  of efficient learning algorithms   303  for  a graded  real  function  class F  =  J Fn  takes  as input  z  e  Z™ and e £   0,1  and returns f  € Fn  such4;hat eArz    < infp6Fn  &*  _+£.  Aawe noted earlier, it will not be possible in general to have a SEM algorithm for  a  real  function  class,  as  the  infimum  may  not  be  a  minimum.  It is  quite  possible  to  consider  approximate-SEM  algorithms  for  binary function  classes, but given an approximate-SEM algorithm A  for such a class, we can readily construct a SEM algorithm.  To see this, suppose we are given a sample z  £ Z™  of length ra, and that  we take e in the input to  the  approximate-SEM  algorithm  to  be  1 ra.  Then,  the  algorithm returns h £ Hn  such that  erz{h  <  mfjrz g   + —.  But, since evz h   is  1 ra  times the number of z\  on which h is 'wrong', this means that  &,{h   =  inf  4t, g .  In  other  words,  given  a  sample  z,  the  algorithm  L  given  by  L z   = A z,  1 ra , for  z  of length  ra,  is a SEM algorithm.  Thus the  notion of an approximate-SEM algorithm is crucial only for real function  classes. The following results show that  the rate of growth with n of the 'ex- pressive  power'—which,  for  a  binary  class,  is  the  VC-dimension  and, for  a real class is the fat-shattering  dimension—determines  the  sample complexity of learning algorithms.  These theorems are immediate conse- quences of the corresponding results for each subclass, Hn   Theorems 4.2 and 5.2  or Fn   Theorems  19.1 and 19.5 .  Theorem  22.2  Let H  =  [jHn  be a graded binary function class.   i      VCdim ifn   is polynomial in n,  then any SEM algorithm for H is  a learning algorithm  with sample complexity raL n,e, J  poly- nomial in n, 1 e  and  \n l S .   ii      there is an efficient learning algorithm for H,  then VCdim ifn   is polynomial in  n.  Theorem  22.3  Let F  =  [ Fn  be a graded real function class.   i   1  the fat-shattering dimension fat rn   a   is polynomial  in n and  I a,  and L  is the learning  algorithm  based  on any  approximate- SEM algorithm A   as in  Theorem 19.1 , then L  has sample  com- plexity  raL n,c,5   polynomial in n,  1 e  and  ln l S .   304   Efficient  Learning   ii       there  is  an efficient  learning algorithm for  F,  then fat ? n  a   is  polynomial  in n  and  I a.  We now turn our attention to the running time of SEM algorithms and approximate-SEM algorithms. Having seen that, in many circumstances, such  algorithms  yield  learning  algorithms, we now investigate the  effi- ciency of these derived learning algorithms.  Theorem 22.5 below shows that the following definition  of efficiency  of SEM and  approximate-SEM algorithms is  sufficient.  Definition 22.4 An efficient approximate-SEM algorithm for the graded real function  class F  =  J Fn  is  an algorithm  that  takes  as input  z  G  Z™ and  e  G   0,1   and,  in  time  polynomial  in  m,n  and  1 e,  produces  an output  hypothesis  f  G  Fn  such  that  An  efficient  SEM  algorithm  for  the  graded  binary  function  class  H  = J Hn  is an algorithm that takes as input z  €  Z™ and, in time  polynomial in m  and n,  returns  h G  Hn  such  that  eiz h   =  mm  erz g .  Theorem  22.5    %   Suppose  that H  =  \JHn  class  and that VCdim ifn   is polynomial  in n.  Then,  any  efficient  SEM algorithm for  H  is  an  efficient  learning  algorithm for  H.  is  a graded binary  function   ii   Suppose  that  F  =  J Fn  is  a  graded  real function  class  and  that fatiTn  a   is polynomial  in n  and  I a.  Then  any  learning  algorithm  for F  based on  an  efficient  approximate-SEM  algorithm  is  efficient  Proof  We prove the  second part  of the theorem,  the proof  of the first part being similar  and more straightforward .  Suppose A  is an  efficient approximate-SEM algorithm for the graded real class F  =   J Fn  and that fatFn   a   is polynomial  in n  and  I a.  Let  L  be the  learning  algorithm based on A   as described  in Theorem  19.1 .  Theorem  22.3 shows that the  sample  complexity  mi n,c, 5   is polynomial  in  n, 1 e  and  ln l 5 . Given  z  G Z£*, L  computes  >i z,eo ,  where  eo =  16 y m.  Since  A  is efficient,  the time taken to produce A zyeo   is polynomial in m,n  and l e0  =  -y m 16; thus, RL{m,ri   is polynomial in m  and n.      22.5  Efficient learning in the restricted model   305  22.5  Efficient  Learning  in  the  Restricted  Model  We now briefly describe how the preceding approach can be modified  to enable us to discuss efficient  learnability of binary classes in the restricted model of  learning.  Suppose  that  we have,  as  usual,  a  graded  binary function  class  H  =  \JHni  where  Hn  is  defined  on  some subset  Xn  of Rn.  Recall that,  in the restricted  model of learning, rather than  having labelled examples generated according to some distribution on one of the sets Zn  =  Xn  x {0,1}, we instead have some target function t, belonging to Hn,  together  with  a probability  distribution   i on Xn.  The  error of h G Hn  with respect to t and \i is then defined  to be  erM fc, *  = p {x  e  Xn  : h x     t x }  .  The training samples  rather than being arbitrary members of Z™   are of the form  z =    zi,* zi  ,  x2, t{x2  ,...,    xm,t xm   ,  which we call the training sample corresponding to x  and t.  A learning algorithm  L  for. the  graded  class   in  the  restricted  model   maps from such  training  samples  to  if,  and  satisfies  the  following  conditions  for   i   L z   e  Hni  and  ii   L is a learning algorithm for  Hn   in the restricted model .  As described earlier, the restricted model may be regarded as a straight- forward  special case of the standard model of learning of Part  1. In the restricted model, a learning algorithm is said to be efficient if its sample complexity rriL n,c, 8   the least sufficient  value of mo n, e, 5  for learn- ing  is polynomial in n, 1 e  and In 1 5 , and its worst-case running time -RL WI> n  on training samples of length m for target functions  in Hni  is polynomial in m  and n.  Given  that  the  training  samples  arise from  functions  in  H,  for  any training sample z in the restricted model, there will always be a function in  H  with  zero  sample  error  on  z\  in  other  words,  there  will  always be some function  h  consistent with  z.  Efficient  consistent-hypothesis- finders are the natural counterpart  in the restricted  model to the SEM algorithms for general learning of binary classes.  Definition  22.6  An  algorithm  L  is  an efficient  consistent-hypothesis- finder  for the graded binary class H  =  J Hn  if, given any training sample   306   Efficient Learning  z  of length m for  a target function  in Hn,  L  halts in time polynomial in m  and n  and returns h =  L z   € H n  such that erz h   =  0.  Theorem  22.7  Suppose that H  =  \JHn  is  a  binary graded function class and that VCdim ifn   is polynomial in n.  Then any algorithm  that is an efficient consistent-hypothesis-finder for H  is an efficient learning algorithm for  H.  Proof Suppose L is an efficient  consistent-hypothesis-finder  for H.  Then, by Theorem  4.8, L  is a  learning algorithm  for  H  and  its  sample com- plexity on Hn  is bounded by  which is polynomial in n, 1 e  and  ln l  J .  Furthermore, from  the  defi- nition, its worst-case running time .Rjr, m,n   is polynomial.     22.6  Bibliographical  Notes  Valiant   1984b;  1984a , in  describing  the  model  of learning  that  came to be known as the 'probably approximately correct' model  and which we have  termed  the  restricted  model ,  placed  great  emphasis  on  con- siderations  of  computational  complexity.  The  efficiency  of learning  in the  restricted  model was further  investigated  by Blumer  et  al.   1989 .  See  also   Haussler  et  al.,  1991 .   The  definitions  we give  of  efficient learning  algorithms,  where  the  running  time  of  the  algorithm  and  its sample  complexity  are  separated,  are  standard  or  based  on  standard definitions:  other  definitions  are possible,  but  they  are  all,  in  a sense, equivalent; see  Haussler et al., 1991  for the case of the restricted model. For a detailed  treatment  of the bit  cost model of computing, see  Aho and Ullman, 1992 .   23  Learning as Optimization  23.1  Introduction  The previous chapter demonstrated that efficient  SEM and approximate- SEM algorithms for graded classes F  =   J Fn  give rise to efficient  learn- ing algorithms, provided the expressive power of Fn  grows polynomially with  n   in,  respectively,  the  binary  classification  and  real  prediction learning  models .  In  this  chapter  we show that  randomized SEM  and approximate-SEM  algorithms  suffice,  and  that  a  converse  result  then holds:  if efficient  learning is possible then  there must  exist  an  efficient randomized  approximate-SEM  algorithm.   Hence, for  the  case of a bi- nary  function  class, there  must  be  an  efficient  randomized  SEM algo- rithm.   This  will  establish  that,  in  both  models  of  learning,  efficient learning is intimately  related  to  the  optimization  problem  of finding a hypothesis with small sample error.  23.2  Randomized  Algorithms  For our purposes, a  randomized  algorithm has available to it  a random number  generator  that  produces  a sequence of independent,  uniformly distributed bits.  We shall assume that examining one bit of this random sequence takes one unit of time.   It  is sometimes convenient to assume that  the  algorithm  has  access to  a  sequence  of independent  uniformly distributed  integers in the set  {0,1,..., },  for some    >  1; it is easy to construct  such  a sequence from  a sequence of random  bits.   The  ran- domized algorithm A uses these random bits as part of its input, but it is useful to think of this input as somehow 'internal' to the algorithm, and to think of the algorithm as defining a mapping from an 'external' input to a probability distribution over outputs.  The computation carried out  307   308   Learning as  Optimization  by the algorithm is, of course, determined  by its input, so that,  in  par- ticular, it depends on the particular  sequence produced  by the  random number generator, as well as on the 'external' input.  When we speak of the  'probability'  that A has a given outcome on an   external   input x, we mean the probability that  the stream of random  numbers gives rise to that  outcome when the external input  to the algorithm  is x.  The following  definition  is really two definitions  in one.  It describes both what is meant by an efficient  randomized learning algorithm for a graded binary class in the model of Part 1, and for a graded real function class in the model of Part  3. In order to simplify  the notation, we shall let  Zn = Xn  x {0,1} if H = [ Hn is a graded  binary  function  class, and  Zn = Xn  x [0,1] if F =  \J Fn  is a graded  class of real  functions. Generally, it will be clear from  the  context  which of these is  intended. A training sample of length m is then  an element  of  Z™ for  some pos- itive integer n.  Notice that  the meaning of the input  space Zn and  the measure of error are different  in the two cases.  Definition  23.1  With the above  notation, a randomized learning algo- rithm for the graded class F = \J Fn  is a  mapping  L:{0,iyx\J   jZ-->jFn  n=l m=l   n=l  such that ifzeZ™,   then L b,z   e Fn, and:     given  any e €  0,1 ,    given  any 5 G   0,1 ,    for  any positive  integer n,  there is an integer mo n,e, 6   such that  ifm>  mo n,e, J   then     for any probability  distribution P on Zn,  if z is a training  sample  of length m,  drawn randomly  according to the product probability  distribution  Pm, and b is a sequence of independent, uniformly  chosen bits,  then with probability at least 1 — 6,  the hypothesis L z   output  by L  satisfies  erP L M  <optP Fn   + e.  That is, for m > mo n,c,<5 ,  EPm  {erP L 6, *     1 -  6.  We say  that F =  J Fn  is learnable  if there is a learning  algorithm for F.   23.2 Randomized algorithms   309  We  shall  be  interested  in  randomized  SEM  and  approximate-SEM  algorithms.  Definition  23.2   %   A  randomized algorithm A  is an efficient  random- ized SEM algorithm for  the graded  binary function  class  H  =   J Hn  if given any z  G  Z™, A  halts in time polynomial in n  and m  and outputs h G Hn  which, with probability  at least 1 2, satisfies  evz h   =  mm  erz g .  9£H   ii  A  randomized algorithm A  is an efficient  randomized  approximate- SEM algorithm for  the graded function  class  F  =   J Fn  if the following holds:  given any z  G  Z™,  and any e G   0,1 , A  halts in time polynomial in n,  m  and 1 e  and outputs f  G Fn  which,  with probability at least 1 2, satisfies  g€Fn  Suppose  we run  a  randomized  approximate-SEM  algorithm  k  times on  a fixed input   £,e ,  keeping  the  output  hypothesis   W  with  mini- mal sample error among all the k hypotheses returned.  In other words, we take  the  best of k  iterations of the  algorithm.  Then  the  probabil- ity  that    *   has  error  that  is  not  within  e of  the  optimal  is  at  most  1 2 *, since the algorithm will have bailed'  k times to find a good hy- pothesis.  This is the basis of the following result, which shows that,  as far as its applications to learning are concerned, an efficient  randomized approximate-SEM algorithm is as useful as its deterministic counterpart.  Theorem  23.3   i  Suppose that H  =   J Hn  is a graded binary function class and that VCdim J?n   is polynomial in n.      there is  an efficient randomized SEM algorithm A  for H,  then there is an efficient learning algorithm for H  that uses A  as a subroutine.   ii  Suppose that F  =   J Fn  is a graded real function  class with fat^n   a  polynomial in n  and I a.  If there is an efficient randomized approximate SEM algorithm A for H,  then there is an efficient learning algorithm for F  that uses A  as a subroutine.  Proof  We first prove   i .  The  key idea  is that  we take  the  best  of k iterations of A  for a suitable fc, absorbing the randomness in the action of A  into the '<$' of learning.  Suppose then that  A  is a randomized SEM   310  Learning  as  Optimization  algorithm.  Theorem 4.3 shows that,  for  any probability  distribution  P on Zn,  we have  erp fc   -  erz h \  < e    23.1  with  probability  at  least  1 -  4II^ 2m exp  ~e2m 8 .  If   23.1   holds and  h £ Hn  has  for  all h € Hn   erz h   =  min  &x g ,  9£H  then, as in the proof of Theorem 4.2, we have erp fc   < optP iJn   4- 2e. Now,  given  a  random  sample  z  G Z™, we take  z  as  the  input  to  the randomized  SEM  algorithm  *4, and  run  A  a  total  of  k  times  on  this input,  taking  h^  to be the  best  of these  k  iterations.  Because A  is a randomized  SEM algorithm, the probability that  h^  satisfies  *, &   =  min  evz g   is at  least  1 -  1 2*.  Hence, EPm  {erP  »   > optP n   <  E Pm  2e  <  4UH 2m exp   -e2m 8   If we choose k such that  1 2*  < 5 2, we have  =  min  eiz g   9€H  +1 2*  EPm  >  optP ifn   +  W^  In 8nH 2m  «   1  < S.  Define  mo n,e,<S  =  -y   VCdim n  ln 128 €2   and choose k =  m 32+1.  Then for m > mo n,  e, 5 ,  we  have 1 2*  <  5 2, and so  EPm  Thus,  the  algorithm  L  that  iterates  the  randomized  SEM  algorithm k  = m 32 +  1 times and  takes the best  of k iterations is a learning al- gorithm.  Its sample complexity is no more than mo.  Since VCdim iIn  is polynomial in n, mo n,e,<5  is polynomial in n, 1 e  and  ln l <J .  Fur- thermore, its worst-case running time on a sample in  Z™  is at  most  k   23.3  Learning  as randomized  optimization   311  times the worst-case running time of A.  Since k is linear in ra, and since the running time of A  is polynomial in ra and n, the running time of L is polynomial in ra and  n.  The proof of  ii  is similar, and makes use of Theorem  19.1.      23.3  Learning  as Randomized  Optimization  We have seen that efficient  approximate-SEM and SEM algorithms  both deterministic  and  randomized   can in many cases be used to  construct efficient  learning  algorithms.  The  next  result  proves,  as  a  converse, that  if  there  is  an  efficient  learning  algorithm  for  a  graded  class  then necessarily  there  is an  efficient  randomized  SEM or  approximate-SEM algorithm   depending on whether the class is real or binary .  Theorem  23.4  graded  binary class H  =  \jHn,  SEM algorithm.   i   If  there is  an  efficient learning algorithm for  the then  there is  an  efficient  randomized   ii  If there is an efficient learning algorithm for the graded real function class F  = [jFn  then there is an efficient randomized  approximate-SEM algorithm.  Proof   i   is implied  by   ii   and  the  observation  that  the  existence of an  efficient  approximate-SEM  algorithm  for  a binary  class implies  the existence  of an  efficient  SEM algorithm  for  the  class.  Hence, we need only prove  ii .  Suppose that  L is an efficient  learning algorithm for the real class F  =  \JFn.  We construct  the following randomized  algorithm A,  which  we shall  prove  is  an  efficient  randomized  approximate-SEM algorithm.  Given  as  input  to  A  the  sample  z  G Z™ and  the  number e  €   0,1 ,  we use  the  randomization  allowed  in  A  to  form  a  sample of length ra* = rax, n,e, 1 2 ,  in which each labelled  example is drawn according to the distribution P that is uniform on the labelled examples in z and zero elsewhere on Xn  x [0,1],   This probability is defined  with multiplicity; that  is, for instance, if there are two labelled examples in z each equal to z, we assign the labelled example z probability 2 ra  rather than  1 m.   Let  z*  denote  the  resulting  sample.  Feeding  z*  into  the learning algorithm, we receive as output   *  =  L z*   and we take this to be the output  of the algorithm  A] that  is, A z,e   =   *  =  L z* .  With probability  at  least  1 2   since we took  CS  =  1 2'  when determining  the   312   Learning  as Optimization  length ra* of z* ,  erp  * <opt P F +e.  But  for  any  ,  by the definition  of P,  erp f   =  erz f .  So with  proba- bility at  least  1 2,     <  optP F   + e  =   inf  erp g  + e 9&F  This means that  A  is a randomized  approximate-SEM  algorithm.  Be- cause L is efficient, ra* =  rriL n, e, 1 2   is polynomial in n and 1 c.  Since the sample z* has length ra*, and since L is efficient,  the time taken by L to produce  *  is polynomial in  ra*,n  and  1 e,  and hence A  has running time polynomial in n, ra and  1 e,  as required.     23.4  A  Characterization  of Efficient  Learning  We may summarize the  results of this  and  the  previous chapter  in  the following theorem.  Theorem 23.5 Suppose that F  =  J Fn  is a graded function  class. Then F  is efficiently learnable  if and only if fatj?n   a   is polynomial  in n and I a  and  there is  an  efficient randomized approximate-SEM  algorithm forF.  A special case of this result  applies to binary classes.  Theorem  23.6  Suppose that H  =   J Hn  is  a graded  binary function class.  Then H  is efficiently learnable  if and only if VCdim Hn   is poly- nomial  in  n  and  there is  an  efficient randomized SEM  algorithm for H.  23.5  The  Hardness  of  Learning  We concentrate now on graded binary function  classes.  In order to dis- cuss further  the computational  complexity of learning, we shall assume that the reader has a rudimentary understanding of the basic concepts of computational  complexity theory—in  particular,  the notion of an 4NP- hard5 problem.   Loosely speaking, a problem is NP-hard if it is at  least as hard as one of a number of standard problems that are thought not to   23.5  The hardness  of learning   313  be solvable by a polynomial-time algorithm.   We have seen that  H  can be efficiently  learned only if there is an efficient  randomized  SEM algo- rithm for if.  In order to prove that  it is unlikely that  such an algorithm exists, it  is enough to  show that  a certain  decision problem  associated with H  is NP-hard.  To this end, we define the following  decision prob- lems.  The  first  asks  if  there  is  a  function  h  in  H  achieving  a  certain fit  to  the  sample—that  is,  disagreeing  on  at  most  a  specified  number of entries  of  the  sample.  The  second  asks  if  there  is  a  function  in  H consistent  with the sample.  if-FIT Instance: z €  Rn  x {0, l} m  and an integer k between 1 and m. Question: Is there h € Hn  such that erz h  <  k mi  if-CONSISTENCY Instance:  z G  Rn  x {0,l} m. Question:  Is there h € Hn  such that eiz h  = 0?  Clearly  if-CONSISTENCY is a sub-problem of if-FIT, obtained by set- ting  k  =  0.  Thus,  any  algorithm  for  if-FIT  can  be used  also to  solve if-CONSISTENCY.  To obtain hardness results for learning, we need some more standard notions from computational complexity theory.  We say that  a random- ized  algorithm  A  solves a  decision  problem  II  if the  algorithm  always halts and produces an output—either  'yes' or 'no'—such that  if the an- swer to II on the given instance is 'no', the output  of A  is 'no', and if the answer to II on the given instance is  cyes' then, with probability  at least  1 2,  the  output  of A  is 'yes'.  A randomized  algorithm  is said  to be polynomial-time  if its worst-case running time   over all instances  is polynomial  in  the  size of its  input.   In  both  of the  decision  problems defined  above, the  size of the  input  is m.   The  class of decision  prob- lems II that  can be solved by a polynomial-time randomized  algorithm is denoted by RP.  Theorem  23.7  Let H  =   J Hn  be a graded  binary function  class.  If there is an efficient learning algorithm for H  then there is a polynomial- time randomized algorithm for if-FIT;  in  other words,  if-FIT  is in RP.  Proof If H is efficiently  learnable then, by Theorem 23.4, there exists an efficient  randomized  SEM algorithm A  for  H.  Using A,  we construct  a polynomial-time randomized algorithm B for if-FIT as follows.  Suppose that  z 6   Rx  {0, l} m  and  k together  constitute  an instance of  if-FIT,   314   Learning as Optimization  and hence an input to B.  The first step of the algorithm B is to compute h =  A{z ,  the output  of A  on z.  This function  belongs to Hn  and, with probability  at  least  1 2,  erz h   is minimal  among  all functions  in  Hn. The next  step  in  B  is to check whether  erz h   < k m.  If so, then  the output  of B is  cyes* and, if not, the output  is 'no'.  It  is clear that  B is a  randomized  algorithm  for  H-FIT.  Furthermore,  since A  runs in time polynomial in m and n, and since the time taken for B to calculate  erz h  is linear in the size of z,  B is a polynomial-time algorithm.     Of course, this result also shows that  if H  is efficiently  learnable then  there is a polynomial-time  randomized  algorithm  for  ff-CONSlSTENCY. If we believe that  RP  ^  NP—and  this  is widely held  to be the case— then the following two immediate results will enable us to prove that  in some cases efficient  learning  is impossible,  as  we shall  see in  the  next two chapters.  Theorem  23.8  Suppose RP  ^  NP  and  that  H  is  a graded class  of binary functions.  If H-FIT is NP-hard then there is no efficient learning algorithm for  H.  Corollary  23.9  Suppose RP ^  NP  and that H  is a graded  class of bi- nary functions.  If ff-CONSlSTENCY is NP- wrtf  then there is no efficient learning algorithm for  H.  23.6  Remarks  For a class F of real-valued functions, it is easy to show that the following 'multiple choice' decision problem can be efficiently  solved if and only if the class is efficiently  learnable.  APPROX-F-FIT Instance:  z €  R n  x {0, l} m, and a, € Question:  Choose one of  a inf € Fer*   >a,  b inf €F erz     <a  + e.   €   [0,1].  We require that  a randomized  algorithm  produces a correct  response with  probability  at  least  1 2  in  time  polynomial  in  1 e  and  the  input size.  Notice that, in contrast with ff-FiT  the corresponding problem for binary  classes , sometimes  both  responses to  the  decision  problem  are correct.  This arises because in this case we only need to solve an approx- imate  SEM problem.  We do not  consider  the  APPROX-F-FIT  problem   23.7  Bibliographical  notes   315  further;  in what  follows, it  is convenient  to deal with the  approximate- SEM problem directly.  The restricted model  Just as efficient  learning of binary classes is linked to the existence of an efficient  SEM algorithm,  it  can  be  shown  that  in the  restricted model of learning binary classes, efficient  learning is related to the existence of an  efficient  consistent-hypothesis-finder,  as  the  following  result  shows. Here, the notion of an efficient  randomized consistent-hypothesis-finder is  the  obvious  one,  obtained  from  the  definition  of  a   deterministic  consistent-hypothesis-finder  in  the  same  way that  the  definition  of  an efficient  randomized SEM algorithm is obtained from that of an  efficient SEM algorithm.  Also, the definition  of a randomized learning algorithm is the  obvious  generalization  of  the  definition  in  Section  2.4.  The  re- sult  can be proved  in a way analogous to the  proofs  of Theorems  23.3 and 23.4.  Theorem  23.10  Suppose that H  =  \JHn  is  a graded  binary function class.  Then H  is efficiently learnable  in the restricted model if and only if VCdim  fn   is polynomial in n  and  there is  an  efficient  randomized consistent-hypothesis-finder for  H.  In particular,  therefore,  if the  if-CONSiSTENCY  problem  is  NP-hard, and RP ^  NP, then  H  is not efficiently  learnable  even in the  restricted model,  an observation that  strengthens Corollary 23.9.  23.7  Bibliographical  Notes  The importance of consistent-hypothesis-finders  in the restricted  model of  learning  was  demonstrated  by  Blumer  et  al.   1989   and  Pitt  and Valiant   1988 .  The link between the consistency problem  and  efficient learning in the restricted model was also discussed in  Natarajan,  1989 . The  classic book  on  complexity  theory  is   Garey  and  Johnson,  1979 ; there  have  been  many  more  recent  books,  including  that  of  Cormen, Leiserson and Rivest  1990 .   24  The Boolean Perceptron  24.1  Introduction  In  this  chapter,  we consider  the  computational  complexity  of  learning the class of functions  computed  by the boolean perceptron   the simple perceptron  with  binary  inputs .  Section  24.2 shows that  learning with this class is difficult,  and so we consider two relaxations of this learning problem: learning the subclass of simple perceptrons that have fixed fan- in  that is, those in which the number of non-zero weights is constrained , and  learning  in the  restricted  model.  In  both  cases, there  are  efficient learning algorithms.  24.2  Learning  is Hard  for  the  Simple  Perceptron  Let BPn  denote the set of boolean functions  from  {0, l }n  to {0,1} com- puted  by the  boolean  perceptron,  and  let  BP  =  J BPn  be the  corre- sponding graded function  class. The  B P - F IT  problem is as follows.  JBP-FIT Instance:  z  6   {0, l }n x {0, l} m  and  an integer  k between  1 and  m. Questicm:  Is there h € BPn  such that  evz h   <  k ml  In this section, we show that  this problem is NP-hard by establishing that  it  is at  least  as hard  as  a  well-known  NP-hard  problem  in  graph theory.  A  graph G  =   F, E   consists  of  a  set  V  of  vertices  and  a  set  E  of unordered  pairs of vertices.  Thus, if the  vertices  are labelled  with  the numbers 1,2,..., n, then a typical edge is a 2-set {z, j}.  For convenience, we denote this simply by ij   which, it should be realized, means the same as ji .  A  vertex cover of the  graph  is a set  U of vertices such that  for  316   24-2 Learning is hard for  the simple perceptron   317  each edge ij  of the graph, at least one of the vertices i, j  belongs to  U. The following decision problem is known to be NP-hard.  VERTEX  COVER Instance:  A graph G =  V, E  and an integer k < \V\. Question: Is there a vertex cover U CV  such that \U\  <k?  Note that if the graph has r edges and the vertex set V has cardinality  n, then the size of an instance of this problem is  O rn .  Our proof that  BP-FIT is NP-hard is a standard reduction  argument: we show that, given an instance of the VERTEX COVER problem, we can construct   in polynomial time  an instance of B P - F IT   of a size polyno- mially related to that  of the instance of the vertex covering problem  in such a way that  the  answer to  B P - F IT  on this  constructed  instance is the same as the answer to VERTEX  COVER on the original instance.  This establishes that,  if there was a polynomial-time  algorithm  for  BP-FIT, then we could solve VERTEX COVER in polynomial time by transforming any instance  of this  problem  to  one of BP-FIT  and  applying the algo- rithm for BP-FIT.  The details of this transformation  are given below.  A typical instance of VERTEX COVER is a graph  G =   V, E   together with  an  integer  k  <  \V\.  We shall  assume,  for  simplicity,  that  V  = {1,2,..., n} and we shall denote the number of edges, 25, by r.  Notice that the size of an instance of VERTEX  COVER is Q r + n .  We construct z  =  z G   G  {0, l}2n  x  {0, l} 2r+n  as  follows.  For  any  two  distinct integers ij  between 1 and 2n, let e^  denote the binary vector of length 2n with ones in positions i and j  and zeroes elsewhere. The sample z G  consists of the  labelled  examples   e^n+i, 1  for  i  =  1,2,... ,n  and,  for each edge ij  G E,  the labelled examples  eij,0   and   en+ijTl+j,O .  Note that the 'size' of z is  2r + n  2n 4-1 , which is polynomial in the size of the original instance of VERTEX  COVER, and that z G  can be computed in polynomial time.  For example, if a graph G has vertex set V  =  {1,2,3,4} and edge set E  =  {12,23,14,13}, then  the  sample z G   consists of the following  12 labelled examples:   10001000,1 ,  01000100,1 ,  00100010,1 ,  00010001,1 ,  11000000,0 ,  00001100,0 ,  01100000,0 ,  00000110,0 ,  10100000,0 ,  00001010,0 ,  10010000,0 ,  00001001,0 .  Lemma  24.1  Given any graph  G  =  {V,E   with n  vertices, and any integer k<n,letz  = z G   be as defined above. Then, there is h G   318   The  Boolean  Perceptron  such  that  erz h   <  k  2ri   if  and  only  if  there  is  a  vertex  cover  of  G  of cardinality  at  most  k.  Proof  Suppose  first  that  there is such  an  h  and  that  this  is  represented by  the  state  u  =   ttfi,  W2,..   ,W2n,0   of  the  boolean  perceptron  on  2n inputs.   Thus  w  =   itfi, W2>       ,W2n   €  B& 2n  is  a  weight  vector  and  0  a threshold.   We  construct  a  subset  U  of  V  as  follows.  If   i ei,n+i   =  0, then  we include  i  in  U] if,  for  i  ^  j,   h{e^j   =  1 or   i en+jjn+<7   =  1 then we include  either  one  of i, j  in  U.  Because  h  is  'wrong'  on  at  most  k  of the  examples  in  2,  the  set  U  consists  of  at  most  k  vertices.  We  claim that  U  is  a  vertex  cover.  To  show  this,  we  need  to  verify  that  given yj  belongs  to  U.  It  is  clear  from  the any  edge  ij  €  E,  at  least  one  of  i manner  in which  U is constructed  that  this is true  if either   i ei, n+i   =  0 or  h ejifl+j   =  0, so suppose  that  neither  of these  holds; in  other  words, suppose  that  h{e^ n^i   =  1 =   i ej,n+j .  Then  we may  deduce  that  and so  that is,  Wi +  Wn+i > 0,  Wj +  Wn+j >  0,  Wi +  Wj + wn+i  + wn+j  >  20]   wi + Wj  +  wn+i  +  wn+j   >  20.  Prom this, we see that  either   Wi + Wj >0  or wn+i  + wn+j  > 0  or  both ; thus,  h eij   =  1  or  h en+t>n+j   =  1,  or  both.  Because  of  the  way  in which  U  is  constructed,  it  follows  that  at  least  one  of  the  vertices  i,j belongs  to  U.  Since  ij  was  an  arbitrary  edge  of  the  graph,  this  shows that  U  is indeed  a  vertex  cover.  We now show, conversely, that  if there is a vertex cover of G  consisting of at  most  k  vertices, then  there is a function  in  BP<in  with  sample  error at most  k  2n   on z{G .  Suppose U is a vertex cover and  \U\ < k.  Define a  state u  =   w\,  W2,...,  W2n> 0  of the boolean  perceptron  as follows:  let 0  =  1 and,  for  i  =  1,2,..., n,  Wi  =  wn+i  =  {  x   f  -1    i f i eU [{.^u  We  claim  that  er z h   <  k  2n .  Observe  that  if  ij  €  E,  then,  since U is  a  vertex  cover,  at  least  one  of  i,j  belongs  to  U  and  hence  the  inner products  wTeij  and  wTen+iyTl+j  are  both  either  0  or  —2, less  than  0, so  h eij   =  ft en+i>n+<7   =  0.  The  function  h  is therefore  correct  on  all   24-3 Learning  is easy for fixed fan-in perceptrons   319  the examples in z G   arising from the edges of G.  We now consider the other  types  of labelled  example  in  z G :  those  of the  form   ei,n+i,l . Now, wTeiyn+i  is -2  if i G  U and is 2 otherwise, so h eiyn+i  =  0 if i G  U and  h ei,n+i   =  1 otherwise.  It  follows  that  h  is  'wrong'  only  on  the examples ei,n+i for i G  U and hence  as claimed.      This result shows that the answer to BP-FIT  on the instance  z G , k  is the same as the answer to  VERTEX  COVER on instance   G,fc .  Given that  z G   can be computed from G in time polynomial in the size of G, we therefore establish the following hardness result.  Theorem  24.2  BP-FIT  is NP-hard.  Corollary  24.3   RP  ^  NP then  there is no efficient learning algorithm  24.3  Learning  is  Easy  for  Fixed  Fan-In  Perceptrons  Since Theorem  24.2 shows that  learning the  simple perceptron  is  diffi- cult, it is natural to seek easier versions of this learning problem, so that we can determine what features of the problem make it difficult.  In this section, we consider simple perceptrons in which the number of non-zero weights is constrained.  We say that  a simple perceptron  with weights w G  En  and  threshold 0 G R has fan-in k if the number of non-zero components of w is no more than fc. An easy VC-dimension argument shows that such functions  can compute no more than   em  k  + 1  *+  dichotomies of m  > k points in Rn.  For fixed fc, this is polynomial in m and n, so an obvious candidate for  an efficient  learning algorithm is an algorithm that enumerates all dichotomies that fixed fan-in  perceptrons can compute, and returns the one with minimal  sample error.  It  turns out that, for fixed fc, there is such an efficient  learning algorithm.  Figure  24.1 shows  pseudocode  for  the  procedure  Splitting  which,  given a training set  5  of size m, returns a set of dichotomies of 5.   320  The Boolean  Perceptron argument:  Training  set,  5 = {xi,..., x m}  C Rn returns:   Set  of  weights  and  thresholds,  W = { w,6 }  function  Splitting  S   P : =0 for  all  ti  <       < tk  from  {1,..., n}  for  all     from  {l,...,fc + l}  for  all  n  <       < ri  from  {1,..., rn   for  all  ai,     ,aj  from  {±1}  if  there  is  a  solution   w,9  to  the  system  of  linear  equations xTi  w + Q = aj  satisfying {f:ti;<0} = {*i,*2,...,**}  i =  1,...,   then  S' := {x € S  : w   x -  6 < 0} S"  := {x e S : w   x -  0 > 0} if  { S 1 , ^}   JfP then  endif  endif  endfor  endfor  endfor  endfor return  W  end  Fig. 24.1. Pseudocode for the Splitting  procedure.  Theorem  24.4  The procedure Splitting  returns all dichotomies of its argument S c E n  that can be computed  by some simple perceptron  with fan-in  no more than k.  For \S\ = m,  it takes time  O n2k2km2k+3 .  Proof  We first  show that  the procedure returns all dichotomies.  Notice that  any dichotomy  computed  by a perceptron  with fewer  than  k non- zero weights can be computed by one with exactly k non-zero weights. Since Splitting  enumerates all possible A;-subsets of the n weights, we need only show that, for any dichotomy  {5;, 5"} of m points in 5 C Rk that  can be computed  by a  simple  perceptron   that  is, a  perceptron with k inputs , there is a subset of the m points {x r i,..., xri}  of size no more than k + 1, and a sequence  ai,...,a*   €  {±1}', so that for every   24.3  Learning is easy for fixed fan-in perceptrons   321  solution   w,6   to the equations  xri  -w + 9 = cti   i =  1,...,   the  simple  perceptron  with  weights  w  and  threshold  9  computes  the dichotomy.  To see this,  consider  a  dichotomy  {S',Sn}  computed  by  a simple  perceptron.  By  suitably  shifting  and  scaling  9 and  w,  we can obtain a pair   w,6   for which  These m  inequalities define a closed nonempty polyhedron in the space R*+1  of parameters   w, 0 .  It is well known that  every closed nonempty polyhedron in Rfc+1  contains the nonempty intersection of some number s < k +  1 of the hyperplanes that  make up its surfaces.  Hence, we can choose some set of s < k + 1 equations of the form  w   xi + 9 = oti   for  ai  G  {±1} ,  and  for  all  solutions   w,9 ,  the  simple  perceptron with  those  parameters  computes  the  dichotomy  {S',S"}  of 5.  Hence, Splitting  enumerates all possible dichotomies.  We now prove the bound  on the computation  time of the  algorithm. Clearly,  the  innermost  loop   searching  for  a  solution  to  the  system of linear equations   is executed  times.  Each iteration  involves the  solution  of no more than  k + 1 lin- ear equations in no more than  k +  1 variables, and so takes time  O k3   using, for instance, Gaussian elimination .  Since P contains distinct di- chotomies of 5, it has size O ra*+1n* , and so checking whether {5',  S"} is in P  takes time O \P\m   =  O mk+2nk .  So the total time is  O n*2*+1m*+1 Jfe3 + mMnk     =  O n2*2*m2*+3 .     Corollary 24.5 For fixed k,  define the graded class Hk  = J n Hk,  where Hk  is the class  of simple perceptrons defined on Rn  with fan-in  no more than k.  The class Hk  is efficiently  learnable.   322   The  Boolean  Perceptron  24.4  Perceptron  Learning  in the  Restricted  Model  We now turn our attention to the learnability of the boolean perceptron in the restricted  model of learning.  As we shall see, there is an  efficient learning algorithm in the restricted  model, in contrast  to the results of Section  24.2.  By Theorem  22.7, since VCdim £Pn   =  n +  1, which is certainly  polynomial  in n,  any efficient  consistent-hypothesis-finder  for BPn  will  constitute  an  efficient  learning  algorithm.  We  discuss  here two distinct  consistent-hypothesis-finders  for  BPn;  one based on  linear programming  and  one based  on  the  perceptron  learning  algorithm   as described in Chapter  1 .  Using  linear  programming  Suppose that  z  =    i,2 i ,...,  xm,ym    function  in BPn.  For a weight vector w €R n,  number 7, define the column vector  is a training sample for  some threshold  8 G R and real  and the row vectors  Vi  =      2 ^ - 1   2 ^ , 1 - 2 ^ , - 1  ,  for i =  1,..., m.  Then ViW  > 0 is equivalent  to  xjw  -  0 > 7  xjw  -  0 < -7   if  yi =  1,  and  if  yi =  0.    24.1   24.2   Hence, a solution w to the linear program  maximize 7 subject  to  w  >0  that  has  7  >  0  corresponds  to  a  consistent  hypothesis.  Clearly,  the feasible region is nonempty, since w = 0 satisfies the constraints.   Notice that, if there is a solution with 7 > 0, the feasible region is unbounded. This  is  easy  to  remedy,  by  adding  the  condition  Yl?=i  \w*\ +  1^1  ^  1> which can be represented  with the addition of n +  1 slack variables and another  2n + 3 linear  constraints.    244  Perceptron learning in the restricted model   323  It  is  known  that  there  are  algorithms  for  linear  programming  that are efficient.  For instance, interior  point  methods  such as  Karmarkar's algorithm have polynomial running time.  We have therefore  established the following  result.  Theorem  24.6  The consistent-hypothesis-finder described above,  using any efficient algorithm for linear programming   such as Karmarkar's al- gorithm , constitutes an efficient consistent-hypothesis-finder  and hence is an efficient learning algorithm in the restricted model  for  the boolean perceptron.  Using  the  perceptron  learning  algorithm  The well-known perceptron  learning  algorithm   discussed in Chapter  1  was originally motivated  by biological considerations and  is 'incremen- tal',  in  the  sense that  small  changes are made to  the  weight  vector  in response  to  each  labelled  example  in  turn.  For  any  learning  constant rj > 0, the perceptron learning algorithm Lv  acts sequentially as follows. Suppose that  z G  Z™  is a training sample.  The algorithm Lv  maintains a  current state, u  =   w,0 , where w eRn  and 0 G  R, which initially has the all-0 vector as weight vector, and threshold 0.  The state is updated sequentially as the algorithm cycles through each of the labelled exam- ples  in  the  training  sample.  When  the  algorithm  considers  a  labelled example   x,y ,  where  x  G W1  and  y  G {0,1},  it  calculates  the  label h x   =  sgn tu   x -  0  assigned to x by the perceptron in state u;  where, as before,  w   x  =  wTx,  the  inner  product  of w  and  x   and  it  updates the state to the new state, u   =  {w',01  given by  w'  =  w + 7]  y -  h x   x, 0>   =  Q  Notice  that  the  state  u  is  only  updated  on  a  labelled  example  if  the perceptron in state CJ misclassifies the example.  It is convenient to think of the algorithm L^  as maintaining the hypothesis ft, which is updated each time it misclassifies an example.  The algorithm operates on a train- ing sample by repeatedly cycling through the m  examples, and when it has  completed  a  cycle through  the  training  data  without  updating  its hypothesis, it returns that hypothesis.  The following result, the Percep- tron Convergence  Theorem shows that if the training sample is consistent with some simple perceptron, then this algorithm converges after a finite   The Boolean Perceptron  324  number  of iterations.  Since the  theorem  allows examples in  En,  it  im- plies the corresponding result for examples in  {0,  l } n that are consistent with some t €  BP n.  Theorem  24.7  Define Zn  =  Rn  x  {0,1},  and  fix  a  training sample z  =    xi,j i ,..., xm,2 m    £  Z™.  Suppose that there is  a weight  vec- tor  w*  and  threshold  0*  satisfying  \\w*\\2  +  0*2  =  1 for  which y*  = sgn  w*  - Xi — 0*   for  i  =  1 , . . ., m.  Define  7  =  min{t ;*  - Xi — 0*  : i  =  l , . . . , m}  ,  and suppose  7  >  0.  Then for  all 77 > 0,  the hypothesis maintained by the perceptron  algorithm  Lv  converges  after no more than  R2  +  l  7 2 updates, where R  — max; xi,  and the limiting hypothesis  is consistent with the training data z.  Proof  Let   wi,0i   be  the  state  maintained  by  L^  immediately  before the zth update.  Suppose that the ith update occurs on example   xj,yj . This example must have been misclassified  by the perceptron with state  wi>0i j so sgn ttfj    Xj -  0i  =  1 -  yj,  and hence  =  Wi  4- 7] {yj  -  sgn w*   Xj -  0,   Xj =  u;*  Similarly, 0W  =  0< -   2yi -  l r .  Now, to measure the progress of the  algorithm, we consider the evo- lution of the squared norm,  wi2 + 02, and observe that  it grows with each mistake.  In  particular,  w*   wi+i  + 0*0i+i  =  w* -Wi  + 0*0f +   2yj  -  l rj w*    Xj -  0*   >  w*  -Wi+  0*0;  +  777,  and so tu* -tUi+i +0*0»+i  > ^ 7.  Since iy*2 + 0*2  =  1, this implies—by the  Cauchy-Schwarz  inequality   Inequality   1.7   in  Appendix  1 —that  On the other hand, this squared norm cannot be too large, since  + rf  HxJ2 + 1  + 2 2y, -  lfo  to,   Xj  -  Bt  + »?2 ll^ll2 + i   and so ti;t+i2 + 02 < irj2 R2 + 1 .  Combining these inequalities shows   244  Perceptron learning in the restricted model   325  that  and so i <  R2  + l  7 2  as required.      Since the perceptron  algorithm makes an update  at  least once in ev- ery  cycle through  the  training  data,  and  each  iteration  involves  O n  computation  steps,  this  theorem  implies  that  Ln  has  time  complexity O  R2  +  l mn  7  2 .  A natural  question  is whether  this  consistent-hypothesis-finder  is  ef- ficient. Clearly, if the margin 7 is sufficiently  large  for instance, if  I 7 is no larger  than  some polynomial  in n ,  then  the  algorithm  would  be efficient.  However, for samples of size polynomial in n, the margin 7 can be exponentially  small in n.  We shall show that,  in fact,  the  algorithm is not efficient  by proving that  for a sample of size polynomial in n, the number of complete cycles required can be exponential in n.  In  order  to  show  that  this  procedure  is  inefficient,  we consider  the  boolean function  f2n  of 2n variables that  has  formula  Hn  =  U2n A  U2n-l  V {u2n-2  A  u 2 n-3  V  . . .  ti 2  A U\    . . . .  Here,  we  use  the  standard  notation  for  describing  boolean  functions in  terms  of  the  literals  tii, u2,...,  the  OR  connective  V and  the  AND connective A. We shall use the following two lemmas.  Lemma  24.8  Let the set S n  C {0, l } 2n   of cardinality 2n + 1 be  defined for  each positive integer n  as follows. S\  =  {01,10,11}, and, for n > 1,  Sn+i  =  {zOl: x € S n} U {11... 10,00... 011}.  Then the only function h G  BP2n  consistent with f2n  on Sn  is f2n  itself.  Proof  We prove by induction  that  the  only  h  €  BP 2n  consistent  with f2n  on Sn  is f2n  itself and that, if f2n  is represented  by the state   w,0  of the perceptron, then each weight Wi is positive.  This is easily seen to be true for n =  1. For, suppose that  h € BP 2  is such that  h agrees with   2  on Si.  If ft is represented  by weight vector   wi,w2   and threshold 0 then, since ft 10  =   2 10   =  0 and ft 01  =   2 01   =  0, we have wx  < 6 and w2    6. It  follows  that  wi,w2  >  0 and  hence  0  >  0.  Therefore  ft 00   =  0  =  2 00 .  Since ft x  =  f2 x   for  all four  elements of  {0,1}2, ft equals   2, as required.  Let us now make the inductive hypothesis that  n  >  1 and that  if ft € BP 2n  agrees with  f2n  on Sn,  then ft =  f2n   in other words,   326   The Boolean  Perceptron  Sn  specifies  f2n   and,  furthermore,  that  any weight  vector  representing J2n has all its weights  positive.  Now  suppose  that  h G BP2n+2  agrees with  2n+2 on Sn+i.  Suppose also that h is represented by weight vector  tui,W2,...,  W2n> W2n+i, W2n+2  anc^ threshold 0. The examples rcOl, for x  €  Sn,  are  in £n+i  and so h agrees  with   2n+2  on all such  examples. But,  since   2n+2 y01   =  J2n{y  for all y, and  since Sn  specifies  J2n->  the function  g defined  by g y  =   i yOl   must  equal  J2n- Therefore,  for  all 2 €{O,l}2",  ft yOl  =  f2n y   =   2n+2 y01 .  Now, for y G  {0,  l }2 n,  '2n     2n  \  wiyi  + w2n+2 -  0 I ,  and so   2n is represented by weight vector  w\, W2,...,  ^2n   and thresh- old 0 -  W2n+2-  Given the  inductive  hypothesis,  we deduce  that  Wl,W2,...,W2n  >  0.  This,  coupled  with  the fact  that  h 11...10   =   2 n+2 H...10   =  0, implies that for all y G {0, l }2 n,  because for any y G {0, l }2 n,  we have    y l 0   - w<   11       10    n; < tf.  Similarly,  since   i 00... 011  =   2 n+2 00... 011  = 1,  %11   =  1 =   2 n + 2  p l l   for all y G {0, l }2 n.  Let z be any element of {0, l }2n such that   2 n z   = 0.  Prom the  fact   established  above   that  ft yOl  =  J2n{y  for all y,  we have h z01   =  f2n z   =  0; furthermore, h zll   =  1, as just shown.  Since changing  xn +i  from  0 to 1 changes  h zxn+\l   from  0 to 1, W2n+i > 0 and hence,  since  ft ll...  10  = 0, we have that  for all y G  {0, l }2 n,  ft y00  =  0 =   2 n + 2 y00 .  We  have  now  shown  that  h x  =   2n+2    for all x  G {0, l }2 n + 2, and hence  h  =   2n+2>  and we have  also  established  that  W{ >  0 for i  = 1,2,...,  2n +  1.  Since    i   l l . ..  10  = 0 and  ft 00...  011  =  1, we have  In  0 >   244  Perceptron learning in the restricted model   327  and hence  W2n+2 >  J^  Wi  > 0,  2n  t =l  completing the proof.      Lemma  24.9  Let n  be any positive integer and suppose  J2n is  repre- sented by the state u  =   w, 9  of the boolean perceptron,  where w G  E2n and 9 € E.  Then w2n  >  v^ *'1 min wi, w2 .  Proof  We prove the  result  by induction  on n.  It  is clearly  true  when n  =  1.  For  n  =  2, since   4  0011    =  1 and   4  1110    =  0,  we have W4 + W3> 0 and w3 + w2 4- w\  < 0.  Combining these inequalities gives  W4  >  W2  +  wi  >  2min wi,K;2 -  Suppose the  result  is true  for  n  =  s  and  consider  n  =  s +  1.  Now, if  2S+2 is represented  by   w, 9  then   2«  is represented  by weight  vector  itfi,..., W28   and threshold 9 — W2«+i- To see this, observe that  28   28  ^  0  ~"  W2s+1  and this last condition holds precisely when  2» a?i,..., xn   =  1. So, by the  induction  hypothesis,  for  each  k     A 3  min ^i,it;2 . Now, for each j  < 5, there are x,y  such that   2«+2 z  =  0, f2s-\-2 y   = 1 and such that x, y differ only in entries 2j and 2j +1, with  2O2J+1 =  0 =  y 2j, and   x 2j =  1 =   2  2j+i-  Hence iy-aj  ti^i.  Also, since  2,+2   110101... 101   =  1 and  2 5 +2    111... 10   = 0, we have  W\  -h W2  +  Ws  H   h W28+I <  9  <  Wi  +  W2  +  W4  H  so that  W28+2  >  W3+Ws-\   h W28-l  this  last  inequality  since W2J+1  > W2j. Consider  the  case s  =  2:  here, using the fact  that  W4  > 2min wi,W2 , we obtain  2  = V5   328   The Boolean  Perceptron  as required.  For general s > 2, by the inductive hypothesis,  w2s+2  >  and the result follows from  the fact  that  for  s > 3.   D  Combining  these  two  results,  we obtain  the  following  result,  which  shows that  the consistent-hypothesis-finder  is  inefficient.  Theorem  24.10  For any fixed rj > 0,  the  consistent-hypothesis-finder arising from  the perceptron learning  algorithm Ln  is not efficient.  Proof Suppose we take the target t to be  2n and we take as the input to the consistent-hypothesis-finder  a sample z^  consisting of the members of  Sn,  labelled  by  their  classifications  according  to  f2n     Suppose  the initial state of the perceptron is   00,..., 0 , 0 .  Let u be the number of updates  made  before  a function  ft,  consistent  with  the  sample,  is pro- duced.  By  Lemma  24.8, this  consistent  function  ft  must  be   2n  itself. Therefore,  if  it  is represented  by  the  state   w,0 ,  itfi,W2  >  0 and,  by Lemma 24.9, W2n >   \ 3   min K;i,it;2 -  After  u  updates,  the maxi- mum entry in the new weight vector w9 is at most urj and the minimum non-zero entry is certainly at least 77. Hence the ratio of maximum entry to minimum non-zero entry is at  most u.  But, since in the final output , it  fol- weight vector this ratio is at  least  w<inlmin^i,^   , which is exponential in n, and hence in the size lows that  u >   \ 3     of the sample z.   >   A 3    This result  also holds if 77 = r} n   is any function  of n, and also if the  initial state of the perceptron is chosen  differently.  24.5  Remarks  The perceptron convergence theorem   Theorem 24.7  tells us something about  the  computational  cost  of  learning  simple  perceptrons  in  a  re- stricted  version of the real classification  problem  defined  in  Chapter  9.  In this restricted  model, we assume that  there is some target  function in the  class of simple perceptrons that  classifies  all examples correctly,   24-6 Bibliographical  notes   329  with  some positive margin  7.   We can define  efficient  learning in this model in the obvious way.  Since a simple  perceptron  is a  thresholded linear function, the graded class BP is equivalent to the family of thresh- olded versions of functions  in F = \JnFn,  where the linear functions in Fn  satisfy  for some w eRn  and 9 e R and any x G  {0, l} n, and  f x   =  w-x-6  a   2 =  l.    24.3   The perceptron convergence theorem implies that there is a polynomial- time classification  learning algorithm for F   in this restricted sense .  Instead of restricting the functions  in Fn  with   24.3  so that  the Eu- clidean norm of the augmented weight vector is bounded by 1, we could insist that  the 1-norm is bounded by 1, that is,  In  this  case,  the  linear  programming  approach  shows  that  there  is a polynomial-time classification learning algorithm for F in this restricted sense.  24.6  Bibliographical  Notes  The problem of finding  a half-space  that  best  matches an arbitrary di- chotomy of a set of points in  {0, l } n is listed  in   Garey  and  Johnson, 1979  as an NP-complete  problem;  see  Johnson  and Preparata, 1978  and   Hoffgen,  Simon and Horn,  1995 .   However, modified  versions of the  perceptron  algorithm  perform  reasonably  well in practice;  see, for example,  Gallant, 1990 .   The procedure Splitting  for enumerating dichotomies computed by perceptrons with bounded fan-in  was presented in  Lee et al., 1996 . It is similar to an algorithm earlier proposed by Farago and Lugosi  1993 . For  details  on Gaussian  elimination,  see  Watkins,  1991 .  The simple general approach of constructing learning algorithms that enumerate all dichotomies  of the  training  examples  was discussed  by  Blumer  et  al.  1989 .  The linear programming approach to finding a consistent  perceptron is well-known.  Polynomial-time algorithms for linear programming are described  in   Karmarkar,  1984 .  In fact,  the time  grows polynomially   330   The Boolean Perceptron  in  the  number  of  bits  required  to  represent  the  solution.  Thus,  the boolean perceptron is efficiently  learnable in the restricted model only if the weights and threshold  of some perceptron  consistent  with the  data can  be  represented  in  only  polynomially  many  bits.  Fortunately,  any boolean perceptron can be represented by another with integer weights in which the ratio of the largest to the smallest weight is no more than 2o ninn     S ee   Muroga, 1965; Muroga, 1971 .   Pitt  and Valiant   1988  showed that, if the weights of a boolean per- ceptron are restricted to the set  {0,1}, then even in the restricted case, learning is hard.  Rosenblatt   1958  gives a proof  of the  perceptron  convergence theo-  rem; see also  Block, 1962; Nilsson, 1965 .  The fact  that  the perceptron  algorithm  can be inefficient  is shown in  Anthony and Shawe-Taylor, 1993c; Anthony and Shawe-Taylor, 1993a   see  also   Anthony  et  al.,  1995  .  Baum   1990a   has  shown  that  the algorithm  is efficient  in the  restricted  model when the  training  data is uniformly  distributed  on the unit  sphere in En.  Bylander   1994; 1997  shows that modifications of the algorithm can be efficient  for certain dis- tributions, even in the presence of certain types of random  classification noise.   25  Hardness  Results for  Feed-Forward  Networks  25.1  Introduction  In this  chapter  we show that  the  consistency  problem  can be hard  for some very simple feed-forward  neural networks. In Section 25.2, we show that, for certain graded spaces of feed-forward  linear threshold networks with  binary  inputs,  the  consistency  problem  is  NP-hard.  This  shows that for each such family of networks, unless RP = NP, there can be no efficient  learning algorithm in the restricted  learning model and hence, in  particular,  no efficient  learning  algorithm  in  the  standard  model of Part  1.  These networks are somewhat unusual  in that  the output  unit is constrained to compute a conjunction.  In Section 25.3, we extend the hardness  result  to  networks  with  an  arbitrary  linear  threshold  output unit,  but  with  real inputs.  In  Section  25.4, we describe similar  results for  graded  classes of feed-forward  sigmoid networks with  linear  output units, showing that  approximately minimizing sample error is NP-hard for  these  classes.  Unless  RP  =  NP,  this  shows that  there  can  be  no efficient  learning algorithm in the restricted learning model of Part 3.  25.2  Linear  Threshold  Networks  with  Binary  Inputs  For each positive integer n,  we define  a neural network  on n  inputs  as follows.  The network has n binary inputs and fc+1 linear threshold units  k > 1 . It has two layers of computation units, the first consisting of k linear threshold  units, each connected  to all of the inputs.  The  output unit  is also  a  linear  threshold  unit,  with  a  connection  of  fixed  weight 1 from each of the  other  k  threshold  units.  The output  unit  has  fixed threshold  k.  The  effect  of  this  arrangement  is  that  the  output  unit computes  the  conjunction  of the first-layer linear  threshold  units.  We  331   332  Hardness  Results for Feed-Forward Networks  n  input  units  k  first-layer  units  1  output  unit  Fig.  25.1.  The feed-forward  two-layer linear threshold  network  that  computes the  function  class NJ\iH.  shall refer to this network  and the set of functions it computes  as Nj^ n. The network is illustrated in Figure 25.1.  The  consistency  problem  for the graded  space  N% =  \JnN%n  is as  follows.  i\T*-CONSISTENCY Instance:  z G   {0,l} n  * {0,l} m. Question:  Is there h € N^ n  such that erz h   = 0?  We shall prove that  N^-CONSISTENCY is NP-hard  provided k >  3 , by relating the problem to a well-known NP-hard problem in graph theory. Let  G  be a graph  with  vertex  set V  and edge  set E.  For a  positive integer fc, a  k-colouring  of G  is a function  x  '   V  ~*  { 1 , 2 , . . .,  fc} with the  property  that,  whenever  ij  €  E,   then  x i   ifi  xU -  The following decision problem for fc-colourings is known to be NP-hard for each k > 3.  ^-COLOURING Instance:  A graph G. Question:  Does G have a ^-colouring?   25.2  Linear threshold networks with binary inputs   333  Note  that  the integer  k  is not  part  of the instance.  The assertion that fc-COLOURlNG is NP-hard for k > 3 therefore  means that, for each fixed  k  > 3, it  is NP-hard  to determine  whether  a given  graph  has a fc-colouring.  Thus, for example, it is NP-hard  to determine  whether  a given graph is 3-colourable.  Let G be a graph  with  vertex set V  =  {1,2,..., n}  and edge set  25,  with r =  \E\. We construct a training sample  z =   z G e {o,i}nx{o,i} n+r+1  as follows.  Denoting by e% the vector in {0, l }n with a 1 in position i and 0 elsewhere, the sample z consists of the following labelled examples:      ei,0 , fori  =  1,2,...,n;     e* + ej, 1  for each edge ij  e E;     0,1 , where 0 is the all-0 vector.  For example, suppose that  G is the graph on vertex set  {1,2,3,4,5}, with  edge set  {12,14,15,23,34,45}.  Then  the corresponding  training sample z G  consists of the  12 labelled examples   10000,0 ,  01000,0 ,  00100,0 ,  00010,0 ,  00001,0 ,   11000,1 ,  10010,1 ,  10001,1 ,  01100,1 ,  00110,1 ,  00011,1 ,   00000,1 .  The following result establishes the link between the consistency prob-  lem for the neural network and the graph colouring problem.  Theorem  25.1  There is a function in N*n  that is consistent with z G  if and only if the graph G is k-colourable.  Proof  A state u  of N*n  is described  completely  by the thresholds on each of the first-layer units and by the weights on the connections be- tween each of these and each of the input  units.  We shall denote by wi and 0i the weight vector and threshold of the Ith unit in the first layer, for   = 1,2,..., fe, so that wij is the weight on the connection from input unit i to first-layer unit I.  Consider  a function  h  E N*n,  and let  ii, i2>--->ftfc    Kn  -* {0>l} be  the A;  functions  computed  by the units  in the  first  layer.  By the construction of the network, h is the conjunction  h = hi A  12 A       A hk of these functions.  Suppose that h is consistent with the training sample. This means that  there are weight vectors wi, w<i,..., Wk  and thresholds   334   Hardness Results for  Feed-Forward Networks  0i, 02, -.  , Ok  such that, for all I between  1 and fc, hi x   =  1 if and only if the inner product  wjx  is at least 0 .  Note that,  since 0 is labelled  as a positive example, we have 0  < 0 for each I between  1 and k.  For each vertex i  in G,  h e{  =  0, and  so there is at  least  one function  hm   1 < m    {1,2,..., k} by  X i  = min{m : Am e0  = 0}.  It  remains  to  prove  that  \  X j   =  m, so that  ftm ei   =   imfo   = 0.  Then,  ls  a  colouring  of  G.  Suppose  that  x 0  =  and so, recalling that  0m  < 0, we have  = ™mei + wmeJ  < Sm + 0m  < 0  It follows that hm ei  + ej   = 0 and, hence, h ei + ej   = 0.  Now if ij  were an edge of <?, then  we should have h{ei + ej   =  1, because we assumed that  h is consistent with the training sample.  Thus ij  is not an edge of G, and x  ls  a  colouring, as claimed.  Conversely,  suppose  we are given  a  colouring x  :  V  -> {1,2,..., :}.  For   between  1 and fc, define the weight vector wi as follows:  1,  otherwise,  and  take the  threshold  0   to  be  -1 2.  Let  hi, 12,...,ft*  be the  corre- sponding  linear  threshold  functions,  and  h their  conjunction   which  is computable  by AT*n .  We claim that  h is consistent  with  z G .  Since 0 >  61  =  -1 2  it  follows that ft  0  =  1 for  each  , and  so fo O  =  1.  In order to evaluate ft ei , note that  if x i   =  ^  th en  so hm{ei   =  0 and  hfe   =  0, as required.  Finally, for  any I between  1 and  k,  and for any edge ij,  we know that  at  least one of x 0  and  x j  is not   .  Therefore, for any  ,  where either  both  of the  terms  on the  right-hand  side are  1, or one is 1  and  the  other  is  —1.  In  either  case,  the  inner  product  exceeds  the threshold -1 2, and fc  e* + ej   =  1 for each  .  Thus h{ei + ej   =  1.      25.3  Linear threshold networks with real inputs   335  Given the NP-hardness of ^-COLOURING, and the fact  that  z G   can be computed in polynomial time, this reduction establishes the hardness of the consistency problem.  Corollary  25.2  Let k > 3 be any fixed integer.  Then N%-CONSISTENCY is NP -hard.  Having established  that  the consistency problem is NP-hard for each of the classes of networks, we now have the following hardness result for learning.  Corollary  25.3  Let k  be any fixed integer, k > 3.  Suppose that iV*  n  is the network described above and let Hn  be the set of functions computable by iV*n.  Then, unlessRP  =  NP, there is no efficient learning algorithm for  the graded class H  =  [jHn.  25.3  Linear Threshold Networks with Real Inputs  The result of the previous section is limited, since it shows that  learning is difficult  for  a  rather  unusual  network  class, that  of two-layer  linear threshold networks in which the output  unit  is constrained  to  compute a  conjunction.  In this section, we extend  the result  to linear  threshold networks with an arbitrary linear threshold output  unit.  For  each  k  >  3 and  n  >  1,  define  the  class  iV* of  two-layer  linear threshold  networks  as  follows.  A network  in  N%  has  n  real  inputs,  k linear threshold units in the first layer, and one linear threshold  output unit.  The following result  extends Corollary 25.3 from  the class N*  to the class Nk  =  [ nN*.  Theorem  25.4  Let k  be any fixed integer that  is  at  least 3.  Suppose that N%  is the network described above and let Hn  be the set of functions computable  by  N%.  Then, unless RP = NP, there is no efficient learning algorithm for  the graded class H  = J  Hn.  Proof  We use the  reduction  of Theorem  25.1, and  augment  the  inputs with two extra  real  components, which we use to force the output unit to compute a  conjunction.  Specifically,  given a graph G with n  > 3 vertices, we show that  there is  a  training  sample  z G   that  is  consistent  with  a  function  in  N*  if and  only  if G is fc-colourable. Furthermore,  z G   can  be  computed  in   336  Hardness  Results for  Feed-Forward Networks  Fig.  25.2.  The sets  Sin  and SOut used in the proof of Theorem  25.4, for the case k =  5.  The points in £jn axe marked as crosses; those in Sout  are marked as circles.  polynomial time.  The sample z  =  z G   consists of the following labelled examples:         0 , 0 , ^ , 0  ,  for i  =  1,2,...,n;      0,0, e» +  ej ,  1  for each edge ij  G E\     0,1 ,  where 0 is the  all-0 vector;      s, 0 , 1 , for s  €  S in  C E2; and      s,0 ,0 ,  for s  e  Sout C E 2,  where Sin  and 50Ut  will be defined  shortly.  Here, the first three types of labelled examples are those used in the reduction described in the proof of Theorem  25.1, augmented  with  two real inputs  set  to zero.  The  sets S'in and  iSout both  have cardinality  3k.  Each point  in  S\n  is paired  with a  point  in  SOut> and  this  pair  straddles  some  edge  of  a  regular fc-sided polygon  in E2  that  has vertices on the  unit  circle centred  at  the  origin, as shown in Figure 25.2.   We call this pair of points a 'straddling pair'.  The  midpoint  of  each  pair  lies  on  some  edge  of  the  polygon,  and  the line  passing  through  the  pair is perpendicular  to  that  edge.  The  set  of 3k  midpoints   one  for  each pair   and  the  k  vertices  of  the  polygon  are equally  spaced  around the  polygon.  Let  a  denote the  distance  between a  point  in  Sin  U 50Ut  and  its  associated  edge.  Clearly,  since  the  points   5out} in  { 5,0   :  5  €  Sin}  are  labelled  1  and  those  in  { s,0   :  s  € are labelled  0, for every straddling pair described  above, any  consistent function in N%  has some hidden unit whose decision boundary  separates the pair.  It  is easy to show using elementary  trigonometry  that  there is   254  Sigmoid networks   337 a constant c such that, if a  < c k,  no line in R2  can pass between more than three of these pairs, and no line can pass between three unless they all straddle the  same edge of the polygon.  Since k  lines must  separate 3k straddling pairs, and the origin must be classified  as 1, any  function in iV* that  is consistent  with  z  can  be represented  as a conjunction  of k  linear  threshold  functions.  Clearly,  the  extra  input  components  do not  affect  the  reduction  from  the  proof  of Theorem  25.1.  Thus,  there is a function  in  N*  consistent  with  z G   if and  only  if the  graph  G is fc-colourable.  One can  show fairly  directly  that  this  is also true  if  the components of vectors in 5in and 50Ut must be ratios of integers, provided the integers are allowed to be as large as cfc2, for  some constant  c  the proof is omitted .  Hence, for each fc, the number of bits in z G   is linear in the size of the graph G.     It is clear from the proof that the same result is true if the output unit is chosen from  any class of boolean functions  that  contains  conjunction.  25.4  Sigmoid Networks  Fix a positive constant K  and let a : E -* [0,1] be a monotone  function satisfying  the Lipschitz  constraint  \cr ai   -  <j a2 \  <  ai  - a 2   for  all ai,a2  €  R-  Then  define  iV 2 >n  as the  class of two-layer  sigmoid network with n real inputs, two units in the first layer, each with the acti- vation function  a,  and a single linear output unit, with weights bounded by K  and no threshold.  That  is, AT2   n  computes the class  F  =   {x  H>  W\ J  V\    X -f  Vi,o   +  W2 T  V2  ' X  n  Consider the following approximate-SEM  problem for the graded space N2  =  I I  N2  iV2-APPROX-SEM Instance:  z =    *i,yi ,..., *«,*»    €   R n  x  {0,l} m. Problem:  Find     € N^ n  such  that   338   Hardness  Results  for  Feed-Forward  Networks  The following theorem implies that  the class N%  is not efficiently  learn- able in  the  restricted  model of Part  3, unless  RP  =  NP.  We omit  the proof.  Theorem  25.5  The problem iV*-APPROX-SEM  is NP-hard,  A similar  result  applies  to  larger  networks.  Let  a  : R  ->  [0,1] be  a monotone  function,  and  define  iV* n  as  the  class  of  two-layer  sigmoid network with n real inputs, k units in the first layer, each with the acti- vation function  a,  and a single linear output  unit, with positive weights summing to one, and no threshold.  That  is, iV* n  computes the class  f I  ia   v*'   x  i=i  Vi e  R n, vit0  e  R, Wi > 0,  k  Consider  the following  approximate-SEM  problem  for the graded  space Ng  =   J n Wfl%n\  where p is some  polynomial.  iVJ-APPROX-SEM Instance:  z =    a?i,yi ,... , x m,y m    €  R n  x{0,l}  m. Question:  Find    €   iV?,^ such  that  The following theorem shows that,  unless  RP =  NP, Ng is not   efficiently learnable in the restricted  model of Part  3. We omit  the proof.  Theorem  25.6  There  is a polynomial  p  such  that  iVJ-APPROX-SEM  is NP-hard.  25.5  Remarks  Two hidden unit linear threshold networks  Corollary  25.2 shows  that  iV*-CONSlSTENCY  is NP-hard  for  k  > 3.  In fact,  this  problem  is also  NP-hard  for  k  =  2, but the   graph-colouring reduction  does not establish  this   since 2-colouring is easy .  Instead, we show  that  iV2-C0NSiSTENCY  is at  least  as hard  as the   SET-SPLITTING decision  problem,  defined  as follows:   25.6  Bibliographical  notes   339  SET-SPLITTING Instance:  A set  S  =  {x\,  £2,..., xn]  and  a collection  A  =  {Ai, A2,...,  Ai  of subsets of S. Question:  Are there subsets Si, 52 of S such that  S  — S1US2 and such that, for each i between  1 and  , Ai  does not lie entirely within  Si  or S2?  In  fact,  the  set-splitting  problem  is  a  generalization  of  2-colouring. The  pair   S,A   is  a  hypergraph—a  graph  in  which  edges  are  sets of vertices of arbitrary size—and the set-splitting problem is equivalent  to finding a  2-colouring  of this  hypergraph  so that  no  edge  in  A  has  all vertices of the same colour.  For a given instance of SET-SPLITTING, let us define a training sample z A   e   {0, l } n x {0, l} n +   as follows.  The negative examples in  z A  are vi,V2,... ,v n,  where, for  i  between  1 and  n,  V{  has a  1 in  position i  and  0  in  every  other  position.  The  positive  examples  in  z A   are ai,O2,...,a   where,  for  i  between  1 and  Z, a* has  a  1 in  position  j  if Xj  e  Ai  and  a  0 in  position  j  if  Xj  & A{.  It  can  be  shown  that  the answer to the  SET-SPLITTING problem in instance A  is 'yes' if and only if  the  answer  to  JV2-CONSISTENCY  on  instance  sample  z A   is  'yes>- Prom this, the NP-hardness of ^-CONSISTENCY  follows.  25.6  Bibliographical  Notes  The reduction used to prove Theorem 25.1 and the corresponding result for networks with two units in the first layer  described in Section 25.5  are due to Blum and Rivest  1992   see also  Anthony and Biggs, 1992  . The proof is based on a proof due to Pitt and Valiant  1988  that learning fc-term  DNF is hard in the restricted model.  Blum and Rivest also gave an extension to networks with two hidden units and an arbitrary  linear threshold unit at the output.  The extension of Theorem 25.4, using real inputs, is from   Bartlett  and Ben-David, 1999 .  Earlier, Judd   1990  proved NP-hardness results for an approximate- SEM problem for  certain linear threshold  networks.  DasGupta, Siegel- mann  and  Sontag   1995   proved  NP-hardness  of  binary  classification learning with two layer networks of units in which the first-layer units' activation function  is a certain piecewise-linear  function.  It  has long been known that  gradient  descent  algorithms for  sigmoid networks can fail because of local minima  see, for example,  Sontag and Sussmann,  1989  .  In  fact,  Auer,  Herbster  and  Warmuth   1996   have shown that  the  number  of local  minima  can  be  exponentially  large in   340   Hardness  Results for  Feed-Forward Networks  the number of network parameters.  See also  Sontag,  1995 , which gives upper bounds on the number of such local minima.  The results of Section 25.4, and the proofs of Theorems 25.5 and  25.6 are due to Jones   1997 .  Vu  1998  presents stronger results for sigmoid networks with an additional condition on the rate at which the activation function a  approaches its asymptotic values.  In that case, he shows that approximate  sample  error  minimization  over  N%  is  hard  for  all  k  >  3, provided the approximation error is smaller than some threshold that de- pends only on k and n, and not on the sample size m.  Vu also gives hard- ness  results  for  approximate  sample  error  minimization  over  two-layer networks  with  linear  threshold  hidden  units  and  a  linear  output  unit. Related  results  are  given  in   Bartlett  and  Ben-David,  1999 .  Bartlett and  Ben-David   1999   also  show  that  approximate  sample  error  mini- mization  over certain  classes  of linear  threshold  network  is  hard.   The approximate  sample  error  minimization  problem  is  defined  for  binary- valued  function  classes  in  the  obvious  way, although  it  is  not  sufficient for the  pattern classification  problem studied  in Part  1.   The  results  of  this  chapter  do  not  rule  out  the  possibility  that  we can efficiently  choose a function  with error near-minimal  over a  certain neural  network  class,  provided  that  the  function  is  not  restricted  to the  same  class.  For  instance,  if  we  use  the  class  of  linear  threshold networks with two units in the first hidden layer as our touchstone  class  see  Section  2.5 ,  it  may  be  possible  to  efficiently  choose  a  function that  has  error  not  much  worse  than  that  of  the  best  network  in  the touchstone  class.  Baum   1990b   has shown that  this  is possible  with  a fixed input dimension, and with arbitrary input  dimension provided the distribution  is spherically  symmetric.  Another  approach is to  allow  the training  algorithm  to  also determine  the  size of the  network;  a  number of algorithms of this kind have been proposed   see, for example,   Prean, 1990;  Brent,  1991  ,  and  we  shall  see  two  such  algorithms  in  the  next chapter.  Another way to make the learning problem easier is to give the learn- ing  algorithm  more  power.  For  instance,  if  we  allow  the  algorithm  to ask an expert for the labels associated with points in the input  space of its choosing, this can significantly  simplify  the learning problem.  There are a number  of positive  results  for  neural  networks  in  models  that  al- low membership  queries   see  Angluin,  1988; Angluin,  1992; Kearns and Vazirani,  1995    of this  kind.  For instance,  Hancock,  Golea  and  Marc- hand   1994   have  proved  such  a  result  for  two-layer  linear  threshold networks  in  which  each  input  unit  is  connected  to  no  more  than  one   25.6  Bibliographical notes   341  first-layer unit, and Baum   1990c; 1991  has proved a result of this kind for linear threshold  networks with  up to four hidden units.   26  Constructive  Learning Algorithms  for  Two-Layer  Networks  26.1  Introduction  In  this  chapter,  we consider  learning  algorithms  for  classes  F  of  real- valued functions  that  can be expressed as convex combinations of func- tions from some class G of basis functions.  The key example of such a class is that of feed-forward  networks with a linear output unit in which the  sum  of the  magnitudes  of the  output  weights is bounded  by some constant B.  In this case, the basis function  class G is the set of functions that  can be computed by any non-output unit in the network, and their negations,  scaled  by  B.  We investigate  two  algorithms.  Section  26.2 describes Construct, an algorithm for the real prediction problem, and Section 26.3 describes Adaboost, an algorithm for the restricted version of the real classification  problem.  Both  algorithms use a learning algo- rithm for the basis function  class to iteratively add basis functions  to a convex combination, leaving previous basis functions fixed.  26.2  Real Estimation with Convex Combinations of Basis  Functions  Theorem  14.10   Section  14.4   shows  that  any  convex  combination  of bounded basis functions can be accurately approximated  with respect to the distance di2 p ,  for instance  using a small convex combination.  This shows that  the  approximate-SEM  problem  for  the  class  co G   c&n be solved by considering only small convex combinations of functions  from G.  In  fact,  the  problem  can be simplified  even further.  The  following theorem  shows  that  we can  construct  a  small  convex  combination  in an iterative way, by greedily minimizing error as each basis function  is added.  342   26.2  Real estimation with convex combinations   343  Theorem  26.1 Let V be a vector space  with an inner product, and let 11 11 =  y UTJ   be the induced norm  on V.  Suppose that G C V  and that, for some B > 0,  \\g\\  < B for all g eG.  Fix f  eV,  k £ N,  and c>  B2,  and define  o = 0.  Then for i = 1,..., k,  choose gi E G  such that    -  Ml2 < inf 11  -    1 -  <*i  i-i + ai9 \\2  + c,  fftCr  where on = 2  i + 1 , et < 4 c -  B2   i  + I 2,  and we  define  Then  fi  =  l-  <Xi fi-i  +  atgi.   -A2 <  inf   -  2 + ^.  Proof  Let d  = infy€co  Gs    -   .  Given S > 0, let  ,? be a point in the convex hull of G with \\fs -  f\\ <df  + 6. Thus fs = Y$=i  H9i  w i th Pj e G, 7j > 0 and Y^=i li  = 1 f r s o me   sufficiently  large N.  Then for all 1 < i < k and a* G  [0,1],  -  fs, fs -    .  Thus,  =  =   =    l-a i   i_ 1+a i 5-  2-    5-  2   1 -  cn fi-x  + <*i9 ~ Ml2 + 2  1 -  ai fi-x  + c*i9 -  fs, h  ~ f      1-   Oi  Ji-i  -fs   +  <*i g-fs \\2  + 2  1 -  ajfi-x  + atg -  fs, fs -  f    l-a i  2  i_ 1-  l 5 2 + a2<7- i2 fs   + 2 1 -  ai ai{fi-i  -fs,9~   -  ai  i_i  + atg -  fs, fs -      Let g be independently  drawn  from  the set  {gi,-.-,9N}  with  Pr <7 = gj   =  7_,-.  Recall  that  fs  =  £ j Li   7j0j-  Hence,  the average  value of   344  Constructive  Learning  Algorithms  for  Two-Layer  Networks      -  «i 2ll i-i - ht  + o$\\9j - ht  2ll ii - ht  + o$\\9j -  ht  +2 1 -  <Xi ai{fi-i -  fs,9j  -  fs   -i ~ fit  + a? 5>llfc -  all2  t  -a,  i_i  +atfs-fi,ft-f   + 2 1 -a*   *-i   -fsJs-f   + 2 1-o«   i_i- ,, ,-    <    1 - a, 2!! *-! - Ml2 + a?B2 + 2 1 -  ai   i-i - fsjs  - f   Since  the  average  is bounded  in this  way,  there  must  be some g {9I,---,9N}   such that  <  =   <    1 - aifWh-x - h\?  + a?B2 + 2 1 - ai   i_1  -   , ,   , -     1 - a,    1 - adWfi-i - fs\\2 + 2  i_i -  ,,  , -      + a}B  1 - a,   ll i-i - Ml2 + 2  «_1 -   , ,   !-       + a?B2,   26  where the last inequality holds since aj > 0. Noting that  ll i-i " f\?  = ll i-i " h\? + Wfs - ft  + 2  i-i   -fs,fs-f ,  we get  ll i-i -  ll2 - Wfs - ft  = ll i-i - Ml2 + 2  ,_i   -fs,fs-f .   26.2 Real estimation with convex combinations   345  Substituting  into Inequality   26.1  and taking the  limit  as S goes to 0, we get mf      l - a i     i - i + a ^ -     2 -4   <  1-a,    \\f^  -    2 -  d} +a2  {B2.  26.2   Setting i = 1, ai = 1 and  o = 0, we see that  mf5- 2-4<B2.  Hence the theorem is true for k = 1. Assume, as an inductive hypothesis, that  Then   26.2  implies that  Setting at = 2  i + 1  gives  inf        l -   4£ 2   « + 1    4   c - B 2   * + 1   Ac     \ -  1 \   4c   2  \   Z + 1    4c " + 7^"  i + 1 2   t + 1   4c i2 + 2i   i2 + 2t + l i Ac i '  _   The theorem follows.   D  As always, we are concerned with a vector space of functions,  and we interpret the squared norm   —  &2 as the error of fk  in approximating a target function  .  Theorem 26.1 improves Theorem 14.10 in two ways. First, it shows how to iteratively  construct  an  approximating  function fk,  whereas Theorem 14.10 proved only the existence of such a  function. Second, it is concerned  with  approximating  any function   ,  not just a function in the convex hull of G. It gives the same rate as Theorem 14.10. That  is,  the  error  of a convex  combination  of k functions  from  G  in approximating  any  function     decreases  to the error  of an arbitrary convex combination at the same rate as it would if    were itself a convex combination.   346  Constructive Learning Algorithms for  Two-Layer Networks  The  algorithm  Construct  At step i of the construction described by Theorem 26.1, the basis func- tion Qi  is chosen to approximately minimize   -  1- *  <_!+ai<7 2.  Notice that we can express this as approximately minimizing of \\fi—g\\2, where  This observation suggests a constructive algorithm that uses an approxi- mate-SEM  algorithm  for  a  class G to  approximately  minimize  sample error over the class co G .  To apply Theorem  26.1, we define the inner product  of functions   i  and   2 to be  For  z  =    ari,2 i ,..., xm,2 m  ,  we define   *  as  the  empirical  condi- tional expectation of y given some  xu  Then we have  ii  - r  ii 2+1  Since the choice of    cannot  affect  the second term, choosing    to min- imize the first term corresponds to minimizing er*   .  Figure 26.1 shows pseudocode for  the  algorithm  Construct^,  which makes use of  an  approximate-SEM  algorithm  L  for  the  basis  function class G.  Corollary  26.2  Suppose that  G  =  \JnGn  is  a  graded class of  real- valued functions that map to some bounded interval, andL  is an efficient   26.2  Real  estimation  with  convex combinations   347  arguments:  Training  set,  S =  { xi,yi ,... , zm,ym }  C  X  x R m  Number  of  iterations,  k Bound,  B,  on  range  of  functions  in  G Convex  combination  of  functions  from  G,  fk =  £?=!  returns:   function  Construct  S, A;, B    o:=0 for  t := 1  to  k  o<:= 2  t +  l  for  j  := 1  torn  fe  =    l   a   end for  endfor return  fk  end  Fig. 26.1. Pseudocode for the ConstructL  algorithm.   L is an  approximate- SEM algorithm for G C [-£,  B]x.   approximate-SEM algorithm  for  G,  with running time  O p m,n,  1 e   for  some polynomial p.  Then  the  algorithm ConstructL  can be  used as  the  basis of  an  efficient  approximate-SEM algorithm  for  co G   = Unco Gn ,  and this algorithm has running time O p m,n, l e2  e .  Proof  Suppose that  functions  in  G map to  [—B}B],  Given  a  training set  5  and  desired error c, if we set  k =  \&B2 e\  and call the  algorithm C o n s t r u c t s, fc,B , then  Theorem  26.1 implies that  the function    G co G  returned  by the algorithm  satisfies  *,     <  m inf  &,    + 5£L <   inf  erz    + e,   €co G     Gco G   OD2  *   so this is an approximate-SEM algorithm for  co G .  It  is easy to check that  the running time of the algorithm is O p ra, n, l e2  e .     Sample complexity  We have seen  that,  if there  is a  learning  algorithm  L  for  a  class G of bounded  real-valued functions,  then  ConstructL can be used to give a learning algorithm for the convex hull co G  of the class. If we appeal to   348  Constructive Learning Algorithms for  Two-Layer Networks  the error bounds for convex classes given in Theorem 20.7, together with the covering number bounds for  convex combinations of functions  that follow from Theorems  14.11 and  12.8, we obtain  the  following  sample complexity bound for this learning algorithm.  Theorem  26.3  There is  a  constant c  such  that for  any  class G  of [—B,B]-valued functions  with finite fat-shattering  dimension,  if  there is an approximate-SEM algorithm for G then it can be used to construct a learning algorithm L for co G  that has sample  complexity  mL e, S  = O  ^   ^fat G   ce B*  In2   f   + In Q      .  In  some  cases,  it  is  possible  to  obtain  a  better  sample  complexity bound by using the approximation result of Theorem 26.1 more directly.  Theorem 26.4  LetG  be a class of [—B,B]-valued functions,  and define the set co* G   of convex combinations  of k functions from  G,  =   I  : 9i  G  G,a*  >     there is a learning algorithm L for G,  then the algorithm ConstructL can be used as the basis of a learning algorithm for  co G .  There is a constant c such that the sample complexity m  of this learning  algorithm satisfies  {tHMB> ,cot G ,2m \  '       where k=   \16B2 e].  Proof  Fix  a  probability  distribution  P  and  suppose  that  fa  €  co G  satisfies E y — fa x    = inf ej?   E y — f x  .  Lemma 20.11 shows that  '   2 6 3    26.2  Real estimation with convex combinations   349  where g   =  if  — £fa.  The inequality inside the probability  statement  is true for some    G co* G   precisely when  E  y -  f{x  2  -{y-   fa{x  2   > 2E2  y -  f x  2  -   y -  fa{x f   + e 2.  Now, the algorithm ensures that  Ez y -  h{x  2  < My  -   fa x  2  so the probability that some    G co* G  has  E  2 -  x  2- y- o x  2 >  A;   A  is no more than the quantity on the right hand side of Inequality  26.3 . Choosing k =  \16B2 e]  and rearranging gives the result.     Neural  networks  Theorem 26.4 has an immediate corollary for two-layer neural networks. Let G be the class of scaled and possibly negated linear threshold  func- tions defined  on Rn,  G =  BH  U -BH,  with  H  =  {x  H-> sgn tuTx 4- wo  : w G  Kn,  WQ  G  R} ,  and  BH  =  {Bh  : h  e  H}.  Then  F  =  co G   is the  class of  two-layer networks with linear threshold units in the first layer and a linear output unit with the constraint that  the magnitudes of the output  weights sum to JB.  Theorem  26.5  There is a learning  algorithm L for  the class F  of two- layer networks defined above,  and L  has sample  complexity  Proof  The  algorithm  Splitting  described  in  Figure  24.1 enumerates all restrictions to the training sample of linear threshold functions  with fan-in  bounded  by  k.  Clearly, this  can be used  as the  basis of a SEM algorithm  for  this  class.  If  k  =  n,  it  leads  to  a  SEM  algorithm  for the class H  of linear threshold functions   although the algorithm  takes time  exponential  in  n  in  that  case .  By  separately  enumerating  the restriction to the sample of all functions  in BH  and — BH  and returning the function  with minimum sample error, we can use Splitting  as the   350  Constructive Learning Algorithms for  Two-Layer Networks  basis of a SEM algorithm, and hence learning algorithm L for G.  Then the  algorithm  Constructs  is  an  approximate-SEM  algorithm  for  the class  F.  To prove the sample complexity bound, first notice that Theorems 3.4  and 3.7 imply n  m    n.  Hence we have  x max{M  e,co 5 ,m   : 5 C G, \S\ = k}  n   2eB   em\2n\k      f  where  the  second  inequality  follows  from  Theorem  18.4.  Substituting into Theorem 26.4 and applying Inequality   1.2  from  Appendix  1 gives the result.     It is interesting that  the algorithm in Theorem 26.5, which minimizes sample error over some restricted  subset of the class, has a better  sam- ple complexity  bound  than  that  implied  by Theorem  26.3 for  a simple SEM algorithm.  In contrast, we saw in Section 5.4 that  in the  pattern classification  problem, sample error minimization  gives optimal  sample complexity.  The  learning  algorithm  L  that  features  in  Theorem  26.5 is  the  al- gorithm  Construct,  based on a learning algorithm for  linear  threshold functions.  Unfortunately,  results in  Chapter  24 show that  the  problem of  learning  linear  threshold  functions  is  computationally  difficult.  If, instead  of this  class, we consider the class Hk  of linear  threshold  func- tions  with  fan-in  bounded  by  k   see Section  24.3 , then  the  algorithm Splitting  can be used as the basis of an efficient  learning algorithm for this class. Construct, based on this algorithm, then leads to an  efficient learning algorithm  for the  class of two-layer networks containing linear threshold units with bounded fan-in in the first layer and a linear output unit  with bounded weights.  Theorem  26.6  Let  H%  be the set  of  bounded fan-in  linear  threshold functions,  Hn  =  {x   -* sgn iuTx +  WQ  : w G  Rn,  \{i:  wi ^  0} < fc, w0  € E} ,   26.3  Classification  learning  using  boosting   351  and let F* =  co B* U -BH% .  Then the algorithm Construct,  based on the  algorithm  Splitting,  is  an  efficient learning algorithm for  the graded class Fk  =  [JnF*.  Clearly, the sample complexity bound of Theorem 26.5 is also valid in this case, since F%  is a subset of the class F  considered in that  theorem.  26.3  Classification  Learning  using  Boosting  This section describes a learning algorithm for a convex combination of binary-valued functions from some set  H.  We show that,  provided  the algorithm  has  access to  a  learning  algorithm  for  H,  it  is successful  in the restricted model of classification learning with real functions.  In this model, we assume that  for some 7 > 0 the distribution  P  on X  x {0,1} is such that  for some function     € F  we have margin   x ,2    > 7 with probability  1.   Recall  that  margin   a; ,y   is f x   —  1 2  if y  =  1 and 1 2  -  f x   otherwise.   If F  =  co if ,  the  following  lemma shows  that this condition  implies that  some h € H  is a weak predictor  of the label y,  in the sense that  h x   disagrees with y slightly less frequently  than  a random guess.  Lemma  26.7  Suppose  that the sample z =    1,2 1 ,...,  x m,ym    and the function  f  G co iJ   are such  that margin   xi ,2 i   >  7  for  i  = 1,..., m.  Then for  all distributions D  on { xi,  2 1 ,...,  xm, ym },  some he  H  has  evD h <  - - 7.  Proof  We use the  probabilistic method.  Suppose that     =  X «=i a»ht with  hi  e  H,  on  >  0  and  E i l i a *  =  1-  Choose  h  €  {h u...yhN} randomly with Pr  i =  hi  = a*.  Then  N   m  EerD h   =  ^ ^ S ^ fa  m   N  Consider  the  inner  sum,  and  suppose  first  that  yj  =  0.  Clearly,  the indicator  function  1^ 3, ^  =  hi xj   in that  case, so the  inner  sum is f xj .  Since margin   xj ,2 j   > 7, we have f xj   < 1 2 —7.  Similarly,   352  Constructive Learning Algorithms for Two-Layer Networks  if yj  = 1, the inner sum is l-f xj   <  I   2 - 7.  Hence, Eern h   < I   2 - 7, and the result follows.     This lemma shows that in the restricted  model of classification  learn- ing for a class F = co if , some function  in the basis class H can provide useful  predictions.  A converse result  is also true:  if, for any distribu- tion  on a training  set there  is a function  in H  that  has error  slightly better  than  random  guessing, then  some    G  co H   has large margins. The proof of this is constructive.  We show that  we can use a SEM al- gorithm  for H to construct  a large margin  SEM algorithm  for  co if . Actually,  we need  something  slightly  stronger  than  a  SEM  algorithm for  H:  we need  an algorithm  that  can minimize  weighted sample er- ror.  By this,  we mean  that  for any  distribution  D o na  training  sam- ple  { xi,2 1 ,..., xm,2 m }, the algorithm  returns  h  €  H   minimizing er£>  i .  The algorithm Adaboost, illustrated in Figure 26.2, iteratively combines basis functions  returned by a weighted SEM algorithm L. The idea  behind  Adaboost  is to start  with  a  uniform  weighting  over the training sample, and progressively adjust  the weights to emphasize the examples that  have been frequently  misclassified  by the basis functions. The  basis  functions  returned  by the algorithm  L  are combined,  with convex coefficients  that depend on their respective weighted errors.  The following  theorem  shows  that  Adaboost  is a  large  margin  SEM  algo- rithm.  Recall  that  the sample  error of a function     with  respect  to 7 on a  sample z is  erj     = —  {*: marginC ^ ,^   <  7 } ,  where the margin is the amount by which   »  is to the correct side of the threshold  value of 1 2.  Theorem  26.8 If L  is a weighted sample error minimization  algorithm for H,  then Adaboost  returns a function f  that satisfies  t=i  In particular, if et < 1 2 -  4ry,  then  er2   < l-72 T 2, and this is less than eforT>   2 72  ln l e .   353 argument:  Training  sample,  z =   zi,yi ,...,  xm,ym    G    I x { 0,  l} m  26.3  Classification  learning  using  boosting   Number  of  iterations,  T Convex  combination  of  functions  from  H,     = ^ C^ 7t *i    returns:  function  Adaboost  2, T   for all i  from  i = 1,..., m            endfor for  all t  from  {1,...,T}  ht:=L ZiDt   * =i  zt for all  i  from  i =  l , . . . ,m  A+i 0 :=    A 0e '  endfor  endfor  T  X <3 UUXXi  end  Fig. 26.2.  Pseudocode for the Adaboost  algorithm.   L is a weighted sample error minimization algorithm.   Proof  Although the algorithm  Adaboost  specifies  the choice of the co- efficients  at, for now we leave them unspecified,  and at each step choose Zt as an overall normalization factor  so that  I?t+i  is always a distribu- tion.  Later, we shall show that the choice of at   and Zt   specified  in the algorithm is optimal.  It is convenient to scale the variables y\,...,  ym and the basis functions  ii,...,hm  so that  they  take  values  in {—1,1}.  Define  yi =  2yi —  1, ht  = 2ht — 1, and    = 2  — 1. Then it is clear that margin   a;i , yi   < 7 if and only if yif x{   < 27, which is equivalent to  t=i   354  Constructive  Learning Algorithms  for  Two-Layer  Networks  and this  implies     exp  T   T   \  It follows  that  -&  53  atht xi +27  53   1.  1  m       T  - «53 =  mexp   2 7 S a * 153exp  -w53a***^*       T \m    T   j       \  Now,  and so       T -2 i 53 t=i -2 i 53 atht{xi    =   \   T  IJ  e xp    -  It follows  that      T   \  m  T   =  exp  27 2^ a t         2753a*   a2*' I J Z t.  t=l t=i  \  T  t=l  t=i   \       26.4   To minimize  this,  we  choose  at  to  minimize,  at  each  step,  the  normal- ization factor  Zt.  We have  Zt  =   m ^   264  Bibliographical  notes   355  Differentiating  with respect to a*, setting to zero, and solving shows that at  =   1 2  ln  l  -  et  et   gives the minimum  value  Zt  =  2-^  1 -e* et, and these values of at  and Zt are used in the Adaboost algorithm.  Sub- stituting into  26.4  gives the first inequality of the theorem.     Theorem 23.4 shows that if there is an efficient  learning algorithm for a binary class, then there is an efficient  randomized  SEM algorithm.  It is  clear  that  the  proof  of  this  theorem  can  be  extended  to  show  that in that  case there is also an efficient  randomized  weighted sample error minimization algorithm.  This, together with Theorem 26.8, shows that if a class H of binary-valued functions  is learnable, then co H  is learnable in the restricted real classification model.  It is clear that  Adaboost calls the learning algorithm  L for  H  only T  =  O  l 72 ln l e    times, so if L is an efficient  learning algorithm for if,  the Adaboost algorithm based on  L  is an  efficient  learning  algorithm  for  co H   in  the  restricted  real classification  model.  26.4  Bibliographical  Notes  The iterative approximation result described in Theorem 26.1 is an im- provement,  due  to  Lee et  al.   1996 , of  a  result  of  Barron   1993 .  A weaker result  with a slower rate  was independently obtained by Koiran  1994 . Darken, Donahue, Gurvits and Sontag  1997  extend the approx- imation result to non-Hilbert spaces. The algorithm Construct  and the sample complexity bound Theorem 26.5 are also from   Lee et al., 1996   see also  Lee et  al., 1995b  .  A similar result follows from  more recent work of Auer, Kwek, Maass and Warmuth  1996 , who consider learning two-layer networks in an online model.  Boosting algorithms  were first proposed  by  Schapire   1990   to illus- trate the equivalence of weak binary classification  learning  producing a hypothesis  with  error slightly  better  than  random  guessing   and prob- ably  approximately  correct  learning   learning  a binary  class in the re- stricted  model,  using  some  richer  class  of  hypotheses .  There  were  a number of improvements of this result  see, for example,  Preund, 1995  . Subsequently, experimental machine learning researchers observed  that this approach can give improvements in pattern  classification  problems  see, for instance,   Drucker,  Cortes, Jackel, LeCun and Vapnik, 1994  . The algorithm Adaboost was introduced by Freund and Schapire  1997 , and its experimental properties studied in  Preund and Schapire, 1996 . A number of researchers noticed that  this algorithm gave reductions in   356  Constructive Learning Algorithms for  Two-Layer Networks  error when more basis functions  were introduced, even after  the sample error of the convex combination had been reduced to zero  Drucker and Cortes,  1996; Quinlan,  1996; Breiman,  1998 .  Theorem  26.8  suggests that  in these cases Adaboost is increasing the margins of training exam- ples, and the results of Part  2 show why the estimation  error decreases. Theorem 26.8 is due to Schapire et al.  1998 , extending in a  straightfor- ward way a similar result  from   Preund  and  Schapire,  1997  for  binary sample error.  The  derivation  of  the  optimal  values  of  at  and  Zt  in  the  proof  of Theorem  2&8 can  be  extended  to  show  that  the  Adaboost  algorithm chooses the basis functions  ht  and their  coefficients  at  to minimize in a greedy way the  sample  average of an  exponential  function  of the  mar- gin  of  the  linear  combination  ^2t=i at^t   see   Breiman,  1999;  Prean and  Downs,  1998  .  Friedman,  Hastie  and  Tibshirani   1998   inter- pret  Adaboost  as an  approximate  stagewise maximum  likelihood  algo- rithm, by viewing the exponential function  as an approximate likelihood cost  function.  Mason,  Bartlett  and  Baxter   1999   and  Mason,  Baxter, Bartlett  and  Prean   1999   have shown that  minimizing  the  sample av- erage of other functions  of the margin   that  do not penalize mistakes so fiercely   can give significant  advantages.  Grove and Schuurmans  1998  give  experimental  evidence  suggesting  that  maximizing  the  minimum margin does not give optimal estimation  error.   Appendix 1 Useful  Results  This appendix is a collection of various inequalities and technical results that  are useful  in a number of places throughout  the book.  Al.l  Inequalities  Estimates for  natural logarithms  Elementary calculus shows that  l  + x<ex   for all x  € K.    1.1   Setting x  =  ab — 1 and rearranging shows that  i -l   for all a,b > 0,    1.2   with equality only if ab =  1.  with b   1,  Another  useful  inequality  is the following.  For positive numbers a, b  31n2 l 6 .    1.3   To prove this, we make use of two facts:  first, for x  > 1, In2 x  < 4x and, secondly, for a, b > 0,  e use of two fa  The first of these can be shown easily, and the second follows from  -  In  ln l b  -ln l bY  357   358  Useful Results So, since ab > 1, we have In2 a6   < 4a6, and  In2 a  <  4o6-ln 26-21naln6  <  4ab -  In2 b + 2ab + 2 ln l 6    in   ^  J + In In   j  <  4ab -  In2 b + 2ab + 4 In2   j  31n2 l 6 .  Eider's   inequality  Elementary calculus shows that   1 + l x x  < e and  1 -  l x x  < e"1  for all x  > 0.  This implies that, for all x > 0 and a e  K, if o ^  0,  Stirling's   approximation  For any n  G N,  n ne- nv^i^e 1   1 2 n + 1   < n! < n ne-f lv  2^e 1   1 2 n .    1.5   For a proof, see  Feller,  1968 , for example.  It is easy to prove by induction that, for all a G E and d € N,  Binomial   theorem  For a vector space X, a convex function     : X  -> E is one for which  Jensen's   inequality    cm +   1 -  a 6   < a  a   +   1 -  a   6 ,  for  all  o, 6 €  X  and  0 <  a  <  1.  If     is a  convex function,  and  x  is  a random variable in X,  then  E    *    >     E   *    .   A 1.1  Inequalities   359  For  a  proof,  see, for  example,   Kolmogorov  and  Fomin,  1975 .  Cauchy-Schwarz inequality  For  any  elements  a, b of an  inner  product  space,   <*,& < IMPII,    1.7   where  o2  =   a,a .  To see  why  this  is true,  notice  that  the  inequality is trivially  true  if  6 =  0, and  if  b   0 we can  write  W-feff-1.-'-""  \\b\\2  >o.  Holder   inequalities  the  standard  inner  product   a, b   may  be  bounded  as  For  a, 6  €  Rk,  follows:  =   aiboo-    1.8   This inequality and the Cauchy-Schwarz inequality are both special cases of Holder's  inequality.  This  states  that  if p,q  are such  that  1 p-h  1 q  = 1   which  is  interpreted  in  the  obvious  way  if  one  of  these  numbers  is infinite ,  then  for  a, 6 €  R*,     &} = V   b  <   f   ?]   [  V j 'l  =   laP6,-   '    1-9    See,  for  example,   Kolmogorov  and  Fomin,  1975 .   The  p-norm  and the  g-norm  are  said  to  be  dual.   360   Useful Results  A1.2  Tails  of  Distributions  Markov's   inequality  If a random variable x is almost  surely nonnegative, then  for  all a > 0.  To see this, notice that  E x  > E xx >  o  Pr x  > a   >  Pr x > a  < ^   a   1.10    x > a .  For a random variable x with mean    and variance a2,  Chebyshev   inequality  -   i  > a   <^    1.11   for all a > 0. This follows immediately from Markov's inequality applied to the random variable  x —    2.  Chernoff  bounds  Suppose m €  N and  x\,...  variables with Pr x* =  1  = pi, where 0 < pi < 1 for i =  1,..., m. Let  ,x m  are independent  {0, l}-valued  random  Then for e > 0, we have  ^   \ m   i=i   j      P r f - V x ^a  -€      <   exp  -€ 2 xm 2 .    1.13    ,    1.12   Proofs of these bounds may be found in the original paper of Chernoff   1952 , and in  Hagerup and Rub, 1990 .  The special cases of the Chernoff bounds in which all the probabilities Pi are equal is frequently  useful.  Suppose that  each pi equals p.  Then  i = p and the probability  on the left-hand-side  of Inequality   1.12  is the probability that the number of X{  equal to 1 is at least m l 4- c  x = m l  +  e p.  The probability  of obtaining  at  least  k  'successes'  in an experiment  repeated  m independent  times, in which the probability of   A1.2  Tails of distributions   361  'success' is p, is denoted  GE p,ra, fc , and  the probability  of at  most  k successes is similarly denoted  LE p,ra, k .  By the Chernoff  bounds, we immediately have  GE p,m, 1 + e mp  =   £    ™ p' l-p ™-'  < exp   -e 2pm 3 ,  L l-c mpJ   £  »=o   .v m  W  l  -  p «-<  < exp   -e 2pm 2 . ^  ^  GE p,m, fe   <  exp - ik-pm 2 3pm     1.14   LE  p, m,  1 -  e mp  =   Generally, it follows  that  for k > pm  and  LE p,m,Jfc   <  exp - pm-ifc 2 2pm     1.15   for  k < pm.  See also  Angluin and Valiant, 1979 .  Hoeffding f8  inequality  Let X  be a set, D  a probability distribution on X,  and    i , . . .,    m  real- valued  functions  defined  on X,  with   »  : X  ->  [ai,fy]  for  i  =  1,... ,ra, where a* and  b{  are real numbers satisfying  a* <  &,.  Then  we have the following inequality   due to Hoeffding   1963  .  Pr >SH- =!    >€  where the probability is over the random sequence   a?i,... ,arm   chosen according to Z?m.  Inequality  1.16  follows immediately from the two 'one-sided' bounds  Pr  —     < exp I   -2e2m2  m  \22 b   -^      2    \  I ,   .,  _  1.17    362   Useful Results  To prove Inequality   1.17 , we follow the proof in   Devroye et al., 1996; Lugosi,  1998 .   Inequality   1.18   is proved  similarly.   Let 2 1,2 2,     ,2 m be  independent  random  variables,  each  with  mean  0,  such  that  2 1 € [cj,di].  We first note  that  for any s > 0,  Pr   £  Vi > a J  =  PrUxp  s £  w j  > e"M  m  Vi ,    1.19   where  we have  used  Markov's  inequality  and  the  independence  of  the random  variables  e8yi.  Now  choose  any  i  between  1 and  m  and  let us  denote  2 i,Ci,d»  simply  by  y^c^d.  By the  convexity  of the  function x »-» e8X  for  s > 0, we have  e 5 e + ^ e, d-c  ~" d-c   and so  taking expectations and using the fact  that  Ey = 0 ,  Ee8y < -f^-e — c   d  e  +   re  d — c  where p =  —c  d — c ,u  =  s d — c  and g u   =  —pw -f In  1 — p + peu . It  is easy to verify  that  g 0  =  g' 0  =  0 and g" u     0.  By Taylor's theorem, for some f  G [0,u],  Thus E esy   < e^u   < es2 d-c 2 s^ 3nd  substituting  into   1.19  gives  Pr   f> > « J < e""5a fl etf2 di-c^2 8.   \i=i       i=i   1.20   To apply this result, let   i,x»  be as above and define, for each i,  vi =  -    < *«  -  E     ,     =  ^   A1.2  Tails of distributions   363  Then  Eyi  =  0 and,  taking  d{ =   bi -  E i   m  and  c* =   a* -  E i    m, we obtain from   1.20  that  Pr  —  <  exp  -se + 2]       V    2  '  for any s > 0.  Choosing 5 = 4m2e   ES=i fri  -  ai 2   y i e l ds   1-17 .  Bernstein's  inequality  Bernstein's  inequality  is more useful  than  Hoeffding's  inequality  when functions    i , . . . ,   m  ^nd  i.i.d.  random  variables  £ i , . . . , x m  are  such that  the  variances  var  i   =  E  i x   — E i x  2  are  small.  It  states that  if  \fi{x   -  Efi x \  <  M  for  all  i  with  probability  1 and  a2  =  \m   See  Bernstein, 1946 .   1.21   Slud's inequality  Let B  be a binomial   m,p   random variable with p < 1 2,  and suppose that  mp < k < m l  -  p .  Then  Pr B  > k  > Pr  z  >     " mP    mp lp J  .   ,   \    1-22   where Z  is a normal   0,1   random  variable.   See  Slud,  1977; Devroye et al.> 1996 .    364   Useful Results  Normal  tail  bounds  If Z  is a normal   0,1   random variable and x  > 0, then  Pr £  > re  >  \   l  -  \ A - e ~ * 2    .    1.23    See  Patel and Read,  1982, p64 ,  Tate, 1953 .   A1.3  Sard's  Theorem  We say that  a set 5  C Rn  has  outer  measure zero if for all e > 0 there is a sequence Bi, 2?2,... of balls in Rn  with  and  where vol Bj   is the volume  Lebesgue measure  of the ball B{.  For  a  C1  function     : Rn  -»  Rm,  we say  that  x  €  IR n  is  a critical point  of     if  rank ' x     Rm,  if    x   =    i x ,...,  m x  ,  then  the  Jacobian of     at  a: €  R n, denoted   ' a ,  is  the  n  x  m  matrix  with  entry  i,j  equal  to  Difj x , the  partial  derivative  of  fj x   with  respect  to  the  zth  component  of x  =   i,... ,x n .   We say that  y £  Rm  is a  critical value of    if some x € R n  satisfying    x   =  y is a critical point of   .  Theorem  1.1   Sard's  Theorem   For n,m  >  1,  if f  : Rn  ->  Rm  is C*  and fe >  max l,n  -  m +  1 ,  £Aen ^Ae 5ei o  critical values of f  has measure zero.  For a proof, see  Sternberg, 1964, Chapter II .   Bibliography  Aho, A. V. and Ullman, J.  D.  1992 .  Foundations of computer science,  Principles of computer science, Computer  Science Press. [306]  Akaike, H.  1974 .  A new look at  the statistical  model identification,  IEEE  Transactions on Automatic  Control AC-19: 716-723. [227]  Albertini, F., Sontag, E. D. and  Maillot,  V.  1993 .  Uniqueness of weights for neural networks, in R. J.  Mammone  ed. , Artificial Neural Networks for Speech  and  Vision, Chapman  & Hall, London, pp.  115-125. [282] Aldous, D. and Vazirani, U.  1990 .  A Markovian  extension of Valiant's  learning model, Proceedings  of the 31st Symposium on the  Foundations of Computer Science, IEEE Computer  Society  Press, Los Alamitos, CA, pp. 392-396. [28]  Alexander, K. S.  1984 .  Probability  inequalities for  empirical processes and  a law of the iterated  logarithm,  Annals  of Probability 12: 1041-1067. [58]  Alon, N.  1983 .  On the density of sets of vectors, Discrete Mathematics  46:  199-202. [41]  Alon, N., Ben-David, S., Cesa-Bianchi, N. and Haussler, D.  1993 .  Scale-sensitive dimensions, uniform  convergence, and  learnability, Proceedings  of the SJ^rd Annual Symposium on Foundations of Computer Science, IEEE Computer  Society Press, Los Alamitos, CA, pp. 292-301. [183, 256]  Alon, N., Ben-David, S., Cesa-Bianchi, N. and Haussler, D.  1997 .  Scale-sensitive dimensions, uniform  convergence, and  learnability, Journal of the A CM 44 4 : 616-631. [267]  Anderson, J.  A. and Rosenfeld,  E.  eds    1988 .  Neurocomputing:  Foundations  Angluin, D.  1988 .  Queries and concept  learning,  Machine  Learning  of Research,  MIT Press. [9]  2 4 : 319-342.  [28, 340]  Angluin, D.  1992 .  Computational  learning theory:  survey and  selected bibliography,  Proceedings  of the 24th Annual ACM  Symposium on Theory of Computing, ACM Press, New York, NY, pp. 351-369. [28, 340]  Angluin, D. and Laird, P.  1988 .  Learning from noisy examples, Machine  Learning 2 4 : 343-370. [27]  Angluin, D. and Valiant, L. G.   1979 .  Fast  probabilistic  algorithms for  Hamiltonian  circuits and matchings,  Journal of Computer and System Sciences 18 2 : 155-193. [361]  Anthony, M.  1995 .  Classification  by polynomial surfaces,  Discrete  Applied  365   366   Bibliography  Mathematics  61: 91-103.  [163]  Anthony,  M.  and  Bartlett,  P.  L.   1995 .  Function  learning  from  interpolation,  Computational  Learning  Theory:  Eurocolt  '95, Springer-Verlag, pp.  211-221.  [139,  183,  192, 268,  296]  Anthony,  M.,  Bartlett,  P.  L., Ishai,  Y.  and Shawe-Taylor,  J.   1996 .  Valid  generalisation  from  approximate  interpolation,  Combinatorics, Probability,  and  Computing  5:  191-214.  [296]  Anthony,  M.  and  Biggs,  N.   1992 .  Computational  Learning  Theory,  Cambridge  Tracts  in Theoretical  Computer  Science   30 ,  Cambridge University  Press.  [9, 339]  Anthony,  M.,  Biggs,  N.  and  Shawe-Taylor,  J.   1990 .  The  learnability  of  formal  concepts,  Proceedings  of  the  3rd  Annual  Workshop  on Computational  Learning  Theory,  Morgan Kaufmann,  San  Mateo,  CA, pp.  246-257.  [58]  Anthony,  M.,  Brightwell,  G.  and  Shawe-Taylor,  J.   1995 .  On  specifying  Boolean  functions  by labelled  examples,  Discrete  Applied  Mathematics 61:  1-25.  [28,  330]  Anthony,  M.  and Holden,  S.  B.   1993 .  On the  power of  polynomial  discriminators  and radial  basis function  networks,  Proceedings  of  the  6th Annual  Workshop  on  Computational  Learning  Theory,  ACM  Press,  New York,  NY,  pp.  158-164.  [163]  Anthony,  M.  and Holden,  S.  B.   1994 .  Quantifying  generalisation  in  linearly-weighted  neural  networks,  Complex  Systems  8: 91-114.  [163]  Anthony,  M.  and  Shawe-Taylor,  J.   1993a .  Bounds  on the  complexity  of  testing  and  loading  neurons,  ICANN'93:  Proceedings  of  the International  Conference  on  Artificial  Neural  Networks,  1993, Springer-Verlag.  [330]  Anthony,  M.  and  Shawe-Taylor,  J.   1993b .  A  result  of Vapnik  with  applications,  Discrete  Applied  Mathematics  47 3 :  207-217.  See  also erratum   1994   52:  211.  [73, 296]  Anthony,  M.  and  Shawe-Taylor,  J.   1993c .  Using the  perceptron  algorithm  to  find  consistent  hypotheses,  Combinatorics,  Probability  and  Computing 4 2 :  385-387.  [330]  Anthony,  M.  and  Shawe-Taylor,  J.   1994 .  Valid generalisation  of  functions  from close  approximations  on  a sample,  Computational  Learning Theory:  Eurocolt  '93, Vol.  53 of  The  Institute  of  Mathematics  and  its Applications  Conference  Series,  Oxford  University  Press,  Oxford, pp.  94-108.  [296]  Assouad,  P.   1983 .  Density et  dimension,  Ann.   Inst.  Fourier  pp. 233-282.  [41] Auer,  P.,  Herbster,  M. and Warmuth,  M. K.   1996 .  Exponentially  many  local  minima for  single  neurons,  in D.  S.  Touretzky,  M.  C.  Mozer  and  M.  E. Hasselmo   eds ,  Advances  in  Neural  Information  Processing  Systems  8, MIT  Press,  pp. 316-322.  [339]  Auer,  P.,  Kwek,  S.,  Maass,  W.  and Warmuth,  M. K.   1996 .  Learning  of  depth  two neural  networks with  constant  fan-in  at  the  hidden  nodes, Proceedings  of  the  9th  Annual  Conference  on  Computational  Learning Theory,  ACM  Press, New York, NY,  pp. 333-343.  [355]  Babai,  L.  and  Frankl,  P.   1992 .  Linear  Algebra  Methods  in  Combinatorics,  with  Applications  to  Geometry  and  Computer  Science,  The  University  of Chicago,  Department  of Computer  Science.  [183]  Barron,  A.  R.   1991 .  Complexity  regularization  with  application  to  artificial   Bibliography   367  neural  networks,  in G.  Roussas   ed. ,  Nonparametric  Functional Estimation  and  Related  Topics,  Kluwer  Academic,  pp.  561-576.  [227, 268]  Barron,  A.  R.   1992 .  Neural  net  approximation,  Proceedings  of  the  7th  Yale  Workshop  on  Adaptive  and  Learning  Systems.  [216]  Barron,  A.  R.   1993 .  Universal  approximation  bounds  for  superpositions  of  a  sigmoidal  function,  IEEE  Transactions  on  Information  Theory 39:  930-945.  [216,  355]  Barron,  A.  R.   1994 .  Approximation  and estimation  bounds  for  artificial neural  networks,  Machine  Learning  14 1 :  115-133.  [9, 216,  227,  240, 268,  282]  B art let t,  P.  L.   1992 .  Learning with  a slowly  changing  distribution,  Proceedings  of  the  5th  Annual  Workshop  on  Computational  Learning Theory,  ACM  Press,  New  York,  NY,  pp.  243-252.  [28]  B art let t,  P.  L.   1993a .  Lower bounds on the  Vapnik-Chervonenkis  dimension  of multi-layer  threshold  networks,  Proceedings  of the  6th  Annual Workshop  on  Computational  Learning  Theory,  ACM  Press,  New  York, NY,  pp.  144-150.  [85]  Bartlett,  P.  L.   1993b .  Vapnik-Chervonenkis  dimension  bounds for  two-  and  three-layer  networks,  Neural  Computation  5 3 :  371-373.  [85]  Bartlett,  P.  L.   1994 .  Learning  quantized  real-valued  functions,  Proceedings  of  Computing:  the  Australian  Theory  Seminar,  University  of  Technology Sydney,  pp. 24-35.  [268]  Bartlett,  P.  L.   1998 .  The  sample  complexity  of pattern  classification  with  neural  networks:  the size  of the weights  is more important  than  the  size of the  network,  IEEE  Transactions  on  Information  Theory 44 2 :  525-536.  [138,  139,  150,  183,  192,  217,  227]  Bartlett,  P.  L. and  Ben-David,  S.   1999 .  Hardness results  for  neural  network approximation  problems,  in P.  Fischer  and H.  Simon   eds ,  Proceedings of the  4th  European  Conference  on  Computational  Learning  Theory, Springer.  [339,  340]  Bartlett,  P.  L.,  Ben-David,  S.  and Kulkarni,  S.  R.   1996 .  Learning  changing  concepts  by exploiting  the  structure  of change,  Proceedings  of  the  9th Annual  Conference  on  Computational  Learning  Theory,  ACM  Press, New  York,  NY,  pp.  131-139.  [28]  Bartlett,  P.  L.,  Fischer,  P.  and  Hoffgen,  K.-U.   1994 .  Exploiting  random  walks for  learning,  Proceedings  of the  1th  Annual  ACM  Workshop  on Computational  Learning  Theory,  ACM  Press,  New  York,  NY, pp.  318-327.  [28]  Bartlett,  P.  L., Kulkarni,  S.  R.  and  Posner,  S.  E.   1997 .  Covering  numbers  for real-valued  function  classes,  IEEE  Transactions  on  Information Theory  43 5 :  1721-1724.  [183,  256]  Bartlett,  P.  L. and Long,  P.  M.   1995 .  More theorems  about  scale-sensitive dimensions  and learning,  Proceedings  of  the  8th  Annual  Conference  on Computational  Learning  Theory,  ACM  Press,  New  York,  NY, pp.  392-401.  [183,  256]  Bartlett,  P.  L. and  Long,  P.  M.   1998 .  Prediction,  learning,  uniform  convergence,  and scale-sensitive  dimensions,  Journal  of  Computer  and System  Sciences  56:  174-190.  [183]  Bartlett,  P.  L.,  Long,  P.  M.  and Williamson,  R.  C.   1996 .  Fat-shattering  and  the  learnability  of real-valued  functions,  Journal  of  Computer  and   368   Bibliography  System  Sciences  52 3 :  434-452.  [246,  267,  268]  Baxtlett,  P.  L.  and  Lugosi,  G.   1999 .  An  inequality  for  uniform  deviations  of  sample  averages from their  means,  Statistics  and  Probability  Letters 44:  55-62.  [268]  Bartlett,  P.  L.,  Maiorov,  V.  and  Meir,  R.   1998 .  Almost  linear  VC-dimension  bounds for  piecewise  polynomial  networks,  Neural  Computation 10:  2159-2173.  [129]  Bartlett,  P.  L.  and  Shawe-Taylor,  J.   1999 .  Generalization  performance  of support  vector  machines  and other  pattern  classifiers,  in B.  Scholkopf, C.  J.  C.  Burges  and  A.  J.  Smola   eds ,  Advances  in  Kernel  Methods: Support  Vector Learning,  MIT  Press,  pp. 43-54.  [217]  Bartlett,  P.  L.  and Williamson,  R.  C.   1996 .  The  Vapnik-Chervonenkis dimension  and pseudodimension  of two-layer  neural  networks  with discrete  inputs,  Neural  Computation  8:  653-656.  [129]  Barve,  R.  D.  and  Long,  P.  M.   1997 .  On  the  complexity  of learning  from  drifting  distributions,  Information  and  Computation  138 2 :  101-123. [23]  Baum,  E.  B.   1988 .  On the  capabilities  of  multilayer  perceptrons,  Journal  of  Complexity  4:  193-215.  [85]  Baum,  E.  B.   1990a .  The  perceptron  algorithm  is fast  for  nonmalicious  distributions,  Neural  Computation  2:  248-260.  [330]  Baum,  E.  B.   1990b .  A  polynomial  time  algorithm  that  learns  two  hidden  net  units,  Neural  Computation  2: 510-522.  [340]  Baum,  E.  B.   1990c .  Polynomial  time  algorithms  for  learning  neural  nets, Proceedings  of the  3rd  Annual  Workshop  on  Computational  Learning Theory,  Morgan  Kaufmann,  San  Mateo,  CA,  pp.  258-272.  [341]  Baum,  E.  B.   1991 .  Neural  net  algorithms  that  learn  in polynomial  time from examples  and  queries,  IEEE  Transactions  on  Neural  Networks 2:  5-19.  [341]  Baum,  E.  B.  and Haussler,  D.   1989 .  What  size  net  gives  valid  generalization?,  Neural  Computation  1 1 :  151-160.  [85]  Baxter,  J.   1998 .  A  model  of inductive  bias learning,  Technical  report,  Department  of Systems Engineering,  Australian  National  University.  [72]  Ben-David,  S.,  Cesa-Bianchi,  N.,  Haussler,  D.  and  Long,  P.  M.   1995 . Characterizations  of learnability  for  classes  of  { 0 , . . .,   n}-valued functions,  Journal  of  Computer  and  System  Sciences  50 1 :  74-86.  [183]  Ben-David,  S.  and  Lindenbaum,  M.   1993 .  Localization  vs.  identification  of  semi-algebraic  sets,  Proceedings  of  the  6th  Annual  Workshop  on Computational  Learning  Theory,  ACM  Press,  New  York,  NY, pp.  327-336.  [107]  Ben-David,  S.  and  Lindenbaum,  M.   1997 .  Learning  distributions  by  their density  levels:  A  paradigm  for  learning  without  a teacher,  Journal  of Computer  and  System  Sciences  55 1 :  171-182.  [72]  Benedek,  G.  M. and Itai,  A.   1988 .  Nonuniform  learnability,  Proceedings  of  ICALP,  pp. 82-92.  [28]  Benedek,  G.  M.  and Itai,  A.   1991 .  Learnability  with  respect  to  fixed distributions,  Theoretical  Computer  Science  86 2 :  377-389.  [28]  Benedetti,  R.  and Risler,  J.-J.   1990 .  Real  algebraic  and  semi-algebraic  sets,  Bernstein,  S.  N.   1946 .  The  Theory  of Probabilities,  Gastehizdat  Publishing  Hermann.  [129]  House.  [363]   Bibliography   369  Billingsley,  P.   1986 .  Probability  and  Measure,  Wiley,  New  York,  NY.  [27] Birge,  L. and  Massart,  P.   1993 .  Rates  of convergence  for  minimum  contrast  estimators,  Probability  Theory  and  Related  Fields  97:  113-150.  [267]  Bishop,  C.  M.   1995 .  Neural  Networks  for  Pattern  Recognition,  Oxford  University  Press,  Oxford.  [9]  Block,  H.  D.   1962 .  The  perceptron:  a model  for  brain functioning,  Reviews  of Modern  Physics  34:  123-135.  [330]  Blum,  A.  and  Chalasani,  P.   1992 .  Learning  switching  concepts,  Proceedings of the  5th  Annual  Workshop  on  Computational  Learning  Theory,  ACM Press,  New  York,  NY,  pp.  231-242.  [28]  Blum,  A.  and Rivest,  R.  L.   1992 .  Training  a 3-node  neural  network  is  NP-complete,  Neural  Networks  5 1 :  117-127.  [339]  Blumer,  A.,  Ehrenfeucht,  A.,  Haussler,  D.  and Warmuth,  M.  K.   1989 .  Learnability  and  the  Vapnik-Chervonenkis  dimension,  Journal  of the A CM 36 4 :  929-965.  [27,  58,  306,  315,  329]  Bollobas,  B.   1986 .  Combinatorics:  Set  systems,  hypergraphs, families  of vectors,  and  combinatorial  probability,  Cambridge  University  Press, Cambridge.  [41]  Boser,  B.  E.,  Guyon, I.  M.  and Vapnik,  V.  N.   1992 .  A training  algorithm  for  optimal  margin classifiers,  Proceedings  of  the  5th  Annual  Workshop  on Computational  Learning  Theory,  ACM  Press,  New  York,  NY, pp.  144-152.  [139,  217]  Breiman,  L.   1998 .  Arcing classifiers,  Annals  of Statistics  26 3 :  801-849.  [356]  Breiman,  L.   1999 .  Prediction  games  and  arcing  algorithms,  Neural  Computation  11 7 :  1493-1517.  [356]  Brent,  R.  P.   1991 .  Fast  training  algorithms  for  multilayer  neural  nets,  IEEE  Transactions  on  Neural  Networks  2 3 :  346-354.  [340]  Buescher,  K.  L.  and Kumar,  P.  R.   1991 .  Simultaneous  learning  of  concepts  and simultaneous  estimation  of probabilities,  Proceedings  of  the  4th Annual  Workshop  on  Computational  Learning  Theory,  Morgan Kaufmann,  San  Mateo,  CA,  pp.  33-42.  [257]  Buescher,  K.  L. and Kumar,  P.  R.   1992 .  Learning stochastic  functions  by  smooth simultaneous  estimation,  Proceedings  of  the  5th  Annual Workshop  on  Computational  Learning  Theory,  ACM  Press,  New  York, NY,  pp.  272-279.  [257]  Bylander,  T.   1994 .  Learning linear  threshold  functions  in the  presence  of classification  noise,  Proceedings  of  the  7th Annual  ACM  Workshop  on Computational  Learning  Theory,  ACM  Press,  New  York,  NY, pp.  340-347.  [330]  Bylander,  T.   1997 .  Learning  probabilistically  consistent  linear  threshold  functions,  Proceedings  of  the  10th  Annual  Conference  on  Computational Learning  Theory,  ACM  Press,  New  York,  NY,  pp.  62-71.  [330]  Carl,  B.   1982 .  On  a characterization  of operators from lq  into  a  Banach  space of type p  with  some  applications  to  eigenvalue  problems,  Journal of Functional  Analysis  48:  394-407.  [216]  Cesa-Bianchi,  N.,  Fischer,  P.,  Shamir,  E.  and  Simon,  H.  U.   1997 .  Randomized  hypotheses  and  minimum  disagreement  hypotheses  for learning  with  noise,  Computational  Learning  Theory:  Eurocolt  '97, Springer-Verlag,  pp.  119-133.  [27]  Cesa-Bianchi,  N.  and Haussler,  D.   1998 .  A  graph-theoretic  generalization  of   370   Bibliography  the Sauer-Shelah  lemma,  Discrete Applied Mathematics 86 1 : 27-35. [183]  Chari, S., Rohatgi, P. and  Srinivasan,  A.  1994 .  Improved algorithms via  approximations  of probability  distributions,  Proceedings  of the Twenty-Sixth  Annual ACM Symposium  on the Theory  of Computing, pp. 584-592. [41]  Chernoff,  H.   1952 .  A measure of asymptotic efficiency  for  tests of a  hypothesis based on the sum of observations,  Annals  of Mathematical Statistics  23: 493-509. [360]  Cormen, T.,  Leiserson,  C. and Rivest, R. L.  1990 .  Introduction to  Cortes, C. and Vapnik, V. N.  1995 .  Support-vector  networks, Machine  Algorithms, MIT Press. [315]  Learning 20 3 : 273-297. [217]  Cover, T.  M.  1965 .  Geometrical and statistical  properties of systems of  linear  inequalities with  applications  in pattern  recognition,  IEEE Transactions  on Electronic  Computers EC-14: 326-334.  [41]  Cover, T.  M.  1968 .  Capacity  problems for  linear  machines,  in L. Kanal   ed. ,  Pattern  Recognition,  Thompson, pp. 283-289. [85]  Cover, T.  M. and Thomas, J.   1991 .  Elements  of Information  Theory,  Wiley.  [85]  Cybenko, G.   1989 .  Approximation  by superpositions  of a sigmoidal  function,  Mathematics of Control,  Signals  and Systems 2 4 : 303-314.  Correction  in vol. 5, p455, 1992 . [10]  Darken, C,  Donahue, M., Gurvits, L. and  Sontag, E. D.  1997 .  Rates of  convex approximation  in non-Hilbert  spaces, Constructive Approximation 13: 187-220. [355]  DasGupta,  B., Siegelmann, H. T. and  Sontag, E. D.   1995 .  On  the  complexity of training neural networks with continuous  activation functions,  IEEE Transactions  on Neural Networks 6 6 : 1490-1504. [339]  DasGupta,  B. and  Sontag, E. D.  1996 .  Sample complexity  for  learning  recurrent perceptron mappings, IEEE Transactions  on  Information Theory 42: 1479-1487. [130]  Devroye, L.   1982 .  Any discrimination  rule can have an  arbitrarily  bad  probability  of error for  finite sample size, IEEE  Transactions on Pattern Analysis and Machine  Intelligence 4: 154-157.  [27]  Devroye, L., Gyorfi,  L. and Lugosi, G.  1996 .  A probabilistic  theory of pattern  recognition,  Applications of Mathematics:  Stochastic  Modelling  and Applied Probability   31 , Springer.  [27, 106, 362, 363]  Devroye, L. and Lugosi, G.   1995 .  Lower bounds in pattern  recognition  and  learning,  Pattern  Recognition 28 7 : 1011-1018.  [27, 72]  Drucker, H. and  Cortes, C.   1996 .  Boosting decision  trees,  Advances in  Neural Information Processing  Systems, Vol. 8, pp. 479-485.  [356]  Drucker, H., Cortes, C,  Jackel, L. D., LeCun, Y. and  Vapnik, V. N.  1994 .  Boosting and other  machine learning algorithms, Proceedings  of the 11th International Conference  on Machine Learning,  Morgan Kaufmann, pp. 53-61.  [355]  Duda, R. O. and Hart, P. E.   1973 .  Pattern  Classification  and Scene  Analysis, Wiley. [27, 138]  Dudley, R.  M.  1978 .  Central  limit  theorems for  empirical measures, Annals  of Probability 6 6 : 899-929.  [41, 58, 256]  Dudley, R.  M.  1987 .  Universal  Donsker classes and  metric entropy,  Annals   Bibliography   371  of Probability  15 4 :  1306-1326.  [163,  216]  Ehrenfeucht,  A.,  Haussler,  D.,  Kearns,  M.  J.  and Valiant,  L.  G.   1989 .  A general  lower  bound on  the  number  of examples  needed  for  learning, Information  and  Computation  82:  247-261.  [72]  Erlich,  Y.,  Chazan,  D.,  Petrack,  S.  and  Levy,  A.   1997 .  Lower bound  on  VC-dimension  by local  shattering,  Neural  Computation  9 4 :  771-776. [85]  Farago,  A.  and  Lugosi,  G.   1993 .  Strong  universal  consistency  of  neural  network  classifiers,  IEEE  Transactions  on  Information  Theory 39 4 :  1146-1151.  [227,  329]  Fefferman,  C.   1994 .  Reconstructing  a neural  network  from  its  output,  Rev.  Mat.  Iberoamericana  10:  507-555.  [282]  Feller,  W.   1968 .  An  Introduction  to  Probability  Theory  and  its  Applications,  Feller,  W.   1971 .  An  Introduction  to  Probability  and  its  Applications,  Vol.  2,  Vol.  1, John  Wiley  and  Sons.  [358]  John  Wiley  and  Sons.  [27]  Theory,  Series  A  34: 41-45.  [41]  Frankl,  P.   1983 .  On the  trace  of  finite  sets,  Journal  of  Combinatorial  Frean,  M.   1990 .  The  upstart  algorithm:  a method  for constructing  and  training  feedforward  neural  networks,  Neural  Computation  2:  198-209. [340]  Frean,  M.  and  Downs,  T.   1998 .  A  simple  cost  function  for  boosting,  Technical report,  Department  of  Computer  Science  and  Electrical  and Computer  Engineering,  University  of  Queensland.  [356]  Freund, Y.   1995 .  Boosting  a weak  learning  algorithm  by  majority,  Information  and  Computation  121 2 :  256-285.  [355]  Freund,  Y.  and  Mansour,  Y.   1997 .  Learning  under  persistent  drift, Computational  Learning  Theory:  Eurocolt  '97, Springer-Verlag, pp.  109-118.  [28]  Freund, Y.  and  Schapire,  R.  E.   1996 .  Experiments  with  a new  boosting  algorithm,  Proceedings  of  the  13th  International  Conference  on  Machine Learning,  Morgan  Kaufmann,  pp.  148-156.  [355]  Freund, Y.  and  Schapire,  R.  E.   1997 .  A  decision-theoretic  generalization  of  on-line  learning  and  an application  to  boosting,  Journal  of  Computer and  System  Sciences  55 1 :  119-139.  [355,  356]  Friedman,  J.,  Hastie,  T.  and Tibshirani,  R.   1998 .  Additive  logistic  regression  :  A statistical  view  of boosting,  Technical report,  Stanford University.  [356]  Gallant,  S.   1990 .  Perceptron-based  learning  algorithms,  IEEE  Transactions  on Neural  Networks  1 2 :  179-191.  [329]  Garey,  M.  R.  and  Johnson,  D.  S.   1979 .  Computers  and  intractability:  A  guide  to  the  theory  of NP-completeness,  W.  H.  Freeman  and  Company. [315,  329]  Goldberg,  P. W.  and  Jerrum,  M.  R.   1995 .  Bounding  the  Vapnik-Chervonenkis  dimension  of concept  classes  parametrized  by real numbers,  Machine  Learning  18 2 3 :  131-148.  [107,  129]  Goldman,  S.  A.  and Kearns,  M.  J.   1991 .  On the  complexity  of  teaching, Proceedings  of  the  4th  Annual  Workshop  on  Computational  Learning Theory,  Morgan  Kaufmann,  San  Mateo,  CA,  pp.  303-314.  [28]  Grenander,  U.   1981 .  Abstract  Inference,  Wiley.  [227] Grove,  A.  J.  and  Schuurmans,  D.   1998 .  Boosting  in the  limit:  Maximizing   372   Bibliography  the  margin  of learned  ensembles,  Proceedings  of  the  Fifteenth  National Conference  on  Artificial  Intelligence,  pp.  692-699.  [356]  Guo,  Y.,  Bartlett,  P.  L.,  Shawe-Taylor,  J.  and Williamson,  R.  C.   1999 . Covering  numbers for  support  vector  machines,  Proc.  12th  Annual Workshop  on  Computational  Learning  Theory,  pp.  267-277.  [217]  Gurvits,  L.   1997a .  Linear  algebraic  proofs  of VC-dimension  based  inequalities,  Computational  Learning  Theory:  Eurocolt  '97, Springer-Verlag,  pp.  238-250.  [183]  Gurvits,  L.   1997b .  A  note  on  a scale-sensitive  dimension  of linear  bounded functional  in  Banach  spaces,  Technical  report,  NEC  Research  Institute. [217]  Gurvits,  L. and  Koiran,  P.   1997 .  Approximation  and  learning  of  convex  superpositions,  Journal  of  Computer  and  System  Sciences 55 1 :  161-170.  [216]  Hagerup,  T.  and  Rub,  C.   1990 .  A  guided tour  of Chernov  bounds,  Information  Processing  Letters  33: 305-308.  [360]  Hancock.,  T.  R.,  Golea,  M.  and  Marchand,  M.   1994 .  Learning  nonoverlapping  perceptron  networks from examples  and  membership queries,  Machine  Learning  16 3 :  161-184.  [340]  Haussler,  D.   1992 .  Decision  theoretic  generalizations  of the  PAC model  for  neural  net  and  other  learning  applications,  Information  and Computation  100 1 :  78-150.  [27,  58,  163,  216,  240,  246,  256,  282,  296]  Haussler,  D.   1995 .  Sphere packing numbers for subsets  of the  Boolean n-cube  with  bounded  Vapnik-Chervonenkis  dimension,  Journal  of Combinatorial  Theory,  Series  A  69 2 :  217-232.  [58,  256]  Haussler,  D.,  Kearns,  M.  J.,  Littlestone,  N.  and  Warmuth,  M.  K.   1991 . Equivalence  of models  for  polynomial  learnability,  Information  and Computation  95 2 :  129-161.  [27,  306]  Haussler,  D.,  Littlestone,  N.  and  Warmuth,  M.  K.   1994 .  Predicting  {0,1}  functions  on randomly  drawn  points,  Information  and  Computation 115 2 :  284-293.  [27]  Haussler,  D.  and  Long,  P.  M.   1995 .  A  generalization  of  Sauer's  lemma,  Journal  of  Combinatorial  Theory,  Series  A  71 2 :  219-240.  [183]  Haussler,  D.  and  Welzl,  E.   1987 .  Epsilon-nets  and simplex  range  queries,  Discrete  Computational  Geometry  2:  127-151.  [41]  Haykin,  S.   1994 .  Neural  Networks:  a  Comprehensive  Foundation,  Macmillan,  New  York,  NY.  [9]  Hebb,  D.   1949 .  The  Organization  of Behavior,  Wiley,  New  York.  [9] Helmbold,  D.  P. and  Long,  P.  M.   1994 .  Tracking  drifting  concepts  by  minimizing  disagreements,  Machine  Learning  14 1 :  27-45.  [28]  Hertz,  J.,  Krogh,  A.  and  Palmer,  R.  G.   1991 .  Introduction  to  the  Theory  of Neural  Computation,  Addison-Wesley,  Redwood  City,  California.  [9, 227]  Hoeffding,  W.   1963 .  Probability  inequalities  for  sums of  bounded  random  variables,  Journal  of  the  American  Statistical  Association 58 301 :  13-30.  [361]  Hoffgen,  K.-U.,  Simon,  H.  U.  and  Horn,  K.  S.  V.   1995 .  Robust  trainability  of single  neurons,  Journal  of  Computer  and  System  Sciences 50 1 :  114-125.  [329]  Hornik,  K.,  Stinchcombe,  M.  and  White,  H.   1990 .  Universal  approximation of  an unknown  mapping  and  its  derivatives  using  multilayer  feedforward networks,  Neural  Networks  3: 551-560.  [10]   Bibliography   373  Horvath,  M.  and Lugosi,  G.   1998 .  A  data-dependent  skeleton  estimate  and  a  scale-sensitive  dimension  for  classification,  Discrete  Applied Mathematics  86 1 :  37-61.  [192]  Jackson,  J.  and Tomkins,  A.   1992 .  A computational  model  of  teaching,  Proceedings  of the  5th  Annual  Workshop  on  Computational  Learning Theory,  ACM  Press,  New  York,  NY,  pp.  319-326.  [28]  Jacobs,  R.  A.,  Jordan,  M.  I.,  Nowlan,  S.  J.  and  Hinton,  G.  E.   1991 .  Adaptive  mixtures of  local  experts,  Neural  Computation  3: 79-87.  [130]  Ji,  C.  and  Psaltis,  D.   1991 .  The  VC-dimension  vs.  the  statistical  capacity  for  two  layer networks with  binary  weights,  Proceedings  of  the  4th Annual  Workshop  on  Computational  Learning  Theory,  Morgan Kaufmann,  San  Mateo,  CA,  pp.  250-256.  [85]  Johnson,  D.  S.  and  Preparata,  F.  P.   1978 .  The  densest  hemisphere  problem,  Theoretical  Computer  Science  6:  93-107.  [329]  Jones,  L. K.   1992 .  A simple  lemma on  greedy  approximation  in  Hilbert  space  and convergence  rates  for  projection  pursuit  regression  and  neural network  training,  Annals  of Statistics  20 1 :  608-613.  [216]  Jones,  L. K.   1997 .  The  computational  intractability  of training  sigmoidal  neural  networks,  IEEE  Transactions  on  Information  Theory 43 1 :  167-173.  [340]  Judd,  J.  S.   1990 .  Neural  Network  Design  and  the  Complexity  of  Learning,  MIT  Press.  [339]  Karmarkar,  N.   1984 .  A  new  polynomial-time  algorithm  for  linear  programming,  Combinatorica  4 4 : 373-395.  [329]  Karpinski,  M.  and  Macintyre,  A.  J.   1997 .  Polynomial  bounds  for  VC  dimension  of sigmoidal  and general  Pfaffian  neural  networks,  Journal  of Computer  and  System  Sciences  54:  169-176.  [107,  130]  Kearns,  M.  J.  and Li,  M.   1993 .  Learning  in the  presence  of  malicious  errors,  SIAM  Journal  of  Computing  22: 807-837.  [27]  Kearns,  M.  J.,  Mansour,  Y.,  Ng,  A.  Y.  and  Ron,  D.   1997 .  An  experimental  and theoretical  comparison  of model  selection  methods,  Machine Learning  27:  7-50.  [227]  Kearns,  M.  J.  and  Schapire,  R.  E.   1994 .  Efficient  distribution-free  learning  of probabilistic  concepts,  Journal  of  Computer  and  System  Sciences 48 3 :  464-497.  [139,  163]  Kearns,  M.  J.,  Schapire,  R.  E.  and  Sellie,  L.  M.   1994 .  Toward  efficient  agnostic  learning,  Machine  Learning  17 2 3 :  115-142.  [27]  Kearns,  M.  J.  and Vazirani,  U.   1995 .  Introduction  to  Computational  Learning  Theory,  MIT  Press,  Cambridge,  MA.  [9, 340]  Khovanskii,  A.  G.   1991 .  Fewnomials,  Vol.  88  of  Translations  of  Mathematical  Monographs,  American  Mathematical  Society.  [129]  Koiran,  P.   1994 .  Efficient  learning  of continuous  neural  networks,  Proceedings  of the  7th Annual  ACM  Workshop  on  Computational Learning  Theory,  ACM  Press,  New  York,  NY,  pp.  348-355.  [139,  355]  Koiran,  P.  and  Sontag,  E.  D.   1997 .  Neural  networks  with  quadratic  VC  dimension,  Journal  of  Computer  and  System  Sciences  54 1 :  190-198. [85,  129]  Koiran,  P.  and  Sontag,  E.  D.   1998 .  Vapnik-Chervonenkis  dimension  of recurrent  neural  networks,  Discrete  Applied  Math  86: 63-79.  [130] Kolmogorov,  A.  N.  and  Fomin,  S.  V.   1975 .  Introductory  Real  Analysis,  Dover.  [282, 359]   374   Bibliography  Kolmogorov,  A.  N.  and  Tihomirov,  V.  M.   1961 .  e-entropy  and  e-capacity  of sets  in  function  spaces,  American  Mathematics  Society  Translations   2  17:  277-364.  [150,  183]  Kowalczyk,  A.   1997 .  Dense  shattering  and  teaching  dimensions  for  differentiable  families,  Proceedings  of  the  10th  Annual  Conference  on Computational  Learning  Theory,  ACM  Press,  New  York,  NY, pp.  143-151.  [41]  Krzyzak,  A.,  Linder,  T.  and  Lugosi,  G.   1996 .  Nonparametric  estimation  and  classification  using radial  basis  function  nets  and  empirical  risk minimization,  IEEE  Transactions  on  Neural  Networks  7 2 :  475-487. [106,  130,  227,  268]  Kuh,  A.,  Petsche,  T.  and  Rivest,  R.  L.   1991 .  Learning  time-varying concepts,  Advances  in  Neural  Information  Processing  Systems  3, Morgan  Kaufmann,  pp.  183-189.  [28]  Lee, W.  S.   1996 .  Agnostic  learning  and  single  hidden  layer  neural  networks,  PhD  thesis,  Department  of Systems  Engineering,  Research  School  of Information  Sciences  and  Engineering,  Australian  National  University, Canberra,  Australia.  [282]  Lee,  W.  S.,  Bartlett,  P.  L.  and Williamson,  R.  C.   1995a .  Lower  bounds  on the  VC-dimension  of smoothly  parameterized  function  classes,  Neural Computation  7 5 :  1040-1053.   Correction  in  Neural  Computation, vol.  9 4 ,  pp.  765-769,  1997 .  [85]  Lee,  W.  S.,  Bartlett,  P.  L.  and Williamson,  R.  C.   1995b .  On  efficient  agnostic  learning  of linear  combinations  of basis  functions,  Proceedings of  the  8th  Annual  Conference  on  Computational  Learning  Theory,  ACM Press,  New  York,  NY,  pp. 369-376.  [139,  355]  Lee, W.  S.,  Bartlett,  P.  L.  and Williamson,  R.  C.   1996 .  Efficient  agnostic  learning  of neural  networks  with  bounded fan-in,  IEEE  Transactions  on Information  Theory  42 6 :  2118-2132.  [139,  216,  282,  329,  355]  Lee,  W.  S.,  Bartlett,  P.  L. and Williamson,  R.  C.   1998 .  The  importance  of  convexity  in  learning  with  squared  loss,  IEEE  Transactions  on Information  Theory  44 5 :  1974-1980.  [282]  Leshno,  M.,  Lin,  V.,  Pinkus,  A.  and  Schocken,  S.   1993 .  Multilayer  feedforward  networks  with  a nonpolynomial  activation  function  can approximate  any  function,  Neural  Networks  6:  861-867.  [10]  Li,  M.  and  Vitanyi,  P.  M.  B.   1991 .  Learning  simple  concepts  under  simple  distributions,  SI AM  Journal  of  Computing  20:  911-935.  [28]  Li,  M.  and  Vitanyi,  P.  M.  B.   1993 .  An  Introduction  to  Kolmogorov  Complexity  and  Its  Applications,  Text  and  Monographs  in  Computer Science,  Springer-Verlag.  [28]  Linial,  N.,  Mansour,  Y.  and  Rivest,  R.  L.   1991 .  Results  on  learnability  and  the  Vapnik-Chervonenkis  dimension,  Information  and  Computation 90 1 :  33-49.  [227]  Long,  P.  M.   1998a .  The  complexity  of learning  according  to  two models  of  a  drifting  environment,  Proceedings  of  the  11th  Annual  Conference  on Computational  Learning  Theory,  ACM  Press,  pp.  116-125.  [58]  Long,  P.  M.   1998b .  On the  sample  complexity  of learning  functions  with  bounded  variation,  Proceedings  of  the  11th  Annual  Conference  on Computational  Learning  Theory,  ACM  Press,  pp.  126-133.  [267]  Lorentz,  G.  G.   1986 .  Approximation  of Functions,  Chelsea.  [163] Lugosi,  G.   1995 .  Improved  upper  bounds  for  probabilities  of  uniform   Bibliography   375  deviations,  Statistics  and  Probability  Letters  25:  71-77.  [58,  227]  Lugosi,  G.   1998 .  On concentration-of-measure  inequalities.   Seminar  notes,  Pompeu  Fabra University,  Barcelona .  [362]  Lugosi,  G.  and  Zeger,  K.   1995 .  Nonparametric  estimation  via empirical  risk  minimization,  IEEE  Transactions  on  Information  Theory 41 3 :  677-687.  [268]  Lugosi,  G.  and  Zeger,  K.   1996 .  Concept  learning  using  complexity  regularization,  IEEE  Transactions  on  Information  Theory  42 1 :  48-54. [227]  Maass,  W.   1994 .  Neural  nets  with  superlinear  VC-dimension,  Neural  Computation  6 5 :  877-884.  [85]  Maass,  W.   1995 .  Agnostic  PAC-learning  of functions  on  analog  neural  networks,  Neural  Computation  7 5 :  1054-1078.  [27]  Macintyre,  A.  J.  and  Sontag,  E.  D.   1993 .  Finiteness  results for  sigmoidal  "neural"  networks,  Proceedings  of  the  Twenty-Fifth  Annual  ACM Symposium  on  the  Theory  of  Computing,  pp.  325-334.  [106,  130]  Makhoul,  J.,  El-Jaroudi,  A.  and  Schwartz,  R.   1991 .  Partitioning  capabilities  of two-layer  neural  networks,  IEEE  Transactions  on  Signal  Processing 39 6 :  1435-1440.  [41]  Makovoz,  Y.   1996 .  Random  approximants  and  neural  networks,  Journal  of  Approximation  Theory  85:  98-109.  [216]  Mason,  L.,  Bart let t,  P.  L.  and  Baxter,  J.   1999 .  Direct  optimization  of  margins  improves  generalization  in combined classifiers,  Advances  in Neural  Information  Processing  Systems,  Vol.  11, pp.  288-294.  [217,  356]  Mason,  L.,  Baxter,  J.,  Bartlett,  P.  L. and  Frean,  M.   1999 .  Functional  gradient  techniques  for  combining  hypotheses,  Technical  report, Research  School  of  Information  Sciences  and  Engineering,  Australian National  University.  [356]  Matousek,  J.   1995 .  Tight  upper  bounds  for  the  discrepancy  of  half-spaces,  Discrete  Computational  Geometry  13: 593-601.  [41]  McCaffrey,  D.  F.  and  Gallant,  A.  R.   1994 .  Convergence  rates  for  single  hidden layer feedforward  networks,  Neural  Networks  7 1 :  147-158.  [216, 240,  282]  Mhaskar,  H.   1993 .  Approximation  properties  of a multilayered  feedforward  artificial  neural  network,  Advances  in  Computational  Mathematics 1: 61-80.  [10]  Milnor,  J.   1964 .  On the  Betti  numbers of real  varieties,  Proceedings  of the  AMS  15: 275-280.  [107]  [9]  Minsky,  M.  and  Papert,  S.   1969 .  Perceptrons,  MIT  Press,  Cambridge,  MA.  Muroga,  S.   1965 .  Lower  bounds  of the  number of threshold  functions  and  a  maximum weight,  IEEE  Transactions  on  Electronic  Computers 14:  136-148.  [58,  330]  Muroga,  S.   ed.    1971 .  Threshold  logic  and  its  applications,  Wiley.  [58, 330] Natarajan,  B.  K.   1989 .  On  learning  sets  and functions,  Machine  Learning  4 1 :  67-97.  [296,  315]  Natarajan,  B.  K.   1991a .  Machine  Learning:  A  Theoretical  Approach,  Morgan  Kaufmann,  San  Mateo,  CA.  [9]  Natarajan,  B.  K.   1991b .  Probably  approximate  learning  of sets  and  functions,  SI AM  Journal  of  Computing  20 2 :  328-351.  [183]  Natarajan,  B.  K.   1993 .  Occam's  razor for  functions,  Proceedings  of  the  6th   376   Bibliography  Annual Workshop  on Computational Learning  Theory,  ACM Press, New York, NY, pp. 370-376. [246]  Nilsson, N. J.   1965 .  Learning  Machines:  foundations of trainable  pattern-classifying  systems, McGraw-Hill.  [85, 330]  Patel, J. K. and Read, C. B.  1982 .  Handbook of the normal distribution,  Marcel Dekker. [364]  Pisier,  G.   1981 .  Remarques sur un result at  non publie de B. Maurey,  Seminaire d'Analyse Fonctionelle  1980-1981, Ecole Polytechnique, Centre de Mathematiques,  Palaiseau,  pp. V.1-V.12. [216]  Pitt,  L. and Valiant,  L. G.   1988 .  Computational  limitations on learning  from examples, Journal of the ACM Zb: 965-984.  [315, 330, 339]  Pollard,  D.  1984 .  Convergence  of Stochastic Processes,  Springer-Verlag. [58,  150, 163, 192, 246, 256, 267, 268]  Pollard, D.   1990 .  Empirical Processes: Theory and Applications,  Vol. 2,  Institute of Mathematical  Statistics.  [58, 163, 267]  Pollard,  D.  1995 .  Uniform  ratio limit  theorems for  empirical processes,  Scandinavian  Journal of Statistics 22: 271-278. [282]  Powell, M. J.  D.  1987 .  Radial basis functions  for  multivariable  interpolation:  A review,  Algorithms for  Approximation, Clarendon  Press,  Oxford, pp.  143-167. [130]  Quinlan, J.  R.   1996 .  Bagging, boosting, and  C4.5, Proceedings  of the  Thirteenth National Conference  on Artificial Intelligence,  pp. 725-730. [356]  Ripley, B. D.  1996 .  Pattern Recognition  and Neural Networks,  Cambridge  University  Press. [9]  14: 465-471. [227]  14 3 :  1080-1100. [227]  Rissanen,  J.   1978 .  Modeling by shortest  data  description,  Automatica  Rissanen,  J.   1986 .  Stochastic complexity and modeling,  Annals  of Statistics  Rissanen, J.   1989 .  Stochastic Complexity  in Statistical Inquiry,  Vol. 15 of  Series in  Computer Science,  World Scientific.  [227]  Rosenblatt,  F.   1958 .  The perceptron:  A probabilistic  model for  information  storage and organization  in the brain, Psychological Review 65: 386-407.  Reprinted in Neurocomputing   MIT Press,  1988  . [9, 330]  Rumelhart,  D. E., Hinton, G. E. and Williams, R. J.   1986a .  Learning internal representations  by error  propagation,  Parallel Distributed Processing  -  Explorations  in the Micro structure of Cognition,  MIT Press, chapter  8, pp. 318-362. [9]  Rumelhart,  D. E., Hinton, G. E. and Williams, R. J.   1986b .  Learning  representations  by back-propagating  errors,  Nature 323: 533-536. [9]  Sakurai, A.  1993 .  Tighter  bounds of the VC-dimension  of three-layer  networks, Proceedings  of the 1993 World Congress  on Neural Networks, Vol. 3, Erlbaum, Hillsdale, NJ, pp. 540-543. [85]  Sakurai,  A.  1999 .  Tight  bounds for  the VC-dimension of piecewise  polynomial  networks,  in M. S. Kearns, S. A. Solla and D. A. Cohn  eds ,  Advances in NIPS  11, MIT Press, Cambridge, MA. [129]  Sauer, N.  1972 .  On the density of families  of sets,  Journal of Combinatorial  Theory, Series A  13: 145-147. [41]  Schapire, R. E.   1990 .  The strength  of weak learnability,  Machine Learning  5 2 :  197-227. [355]  Schapire, R. E.,  Preund,'Y.,  Bartlett,  P. L. and  Lee, W. S.   1998 .  Boosting   Bibliography   377  the margin:  a new explanation  for  the effectiveness  of voting methods, Annals of Statistics 26 5 : 1651-1686.  [217, 356]  Schlafli,  L.  1950 .  Gesammelte  Mathematische  Abhandlungen  J, Birkhauser,  Basel. [41]  Scholkopf, B., Burges, C. J.  C. and  Smola, A. J.   1999 .  Advances in Kernel  Methods:  Support Vector Learning,  MIT Press. [217]  Schuurmans, D.  1995 .  Characterizing  rational  versus exponential  learning  curves,  Computational Learning  Theory:  Eurocolt  '95,  Springer-Verlag, pp. 272-286. [73]  Shawe-Taylor,  J.  and  Anthony, M.  1991 .  Sample sizes for  multiple-output  threshold  networks, Network 2: 107-117. [296]  Shawe-Taylor, J.,  Bartlett,  P. L., Williamson,  R. C. and Anthony, M.  1996 .  A framework  for  structural  risk minimization,  Proceedings  of the 9th Annual Conference  on Computational Learning  Theory,  ACM Press, New York, NY, pp. 68-76.  [139, 217, 227]  Shawe-Taylor,  J.,  Bartlett,  P. L., Williamson,  R. C. and  Anthony, M.  1998 .  Structural  risk minimisation  over data-dependent  hierarchies,  IEEE Transactions  on Information  Theory 44 5 : 1926-1940.  [217, 227, 296]  Shelah, S.  1972 .  A combinatorial  problem:  Stability  and order for  models  and theories in infinity  languages,  Pacific Journal of Mathematics 41:  247-261. [41]  Shinohara,  A. and  Miyano, S.  1991 .  Teachability  in computational  learning,  New Generation  Computing 8: 337-347. [28]  Simon, H. U.  1996 .  General bounds on the number of examples needed  for  learning probabilistic  concepts,  Journal of Computer and System Sciences 52 2 : 239-254.  [72, 192]  Simon, H. U.  1997 .  Bounds on the number of examples needed for  learning  functions,  SIAM  Journal of Computing 26 3 : 751-763. [164, 268]  Sloan, R. H.  1995 .  Four types of noise in data for  PAC learning,  Information Processing Letters 54: 157-162. [27]  Slud, E.  1977 .  Distribution  inequalities for  the binomial law,  Annals of  Probability 5: 404-412. [363]  Smola, A. J.,  Bartlett,  P. L., Scholkopf, B. and  Schuurmans, D.  1999 .  Advances  in Large  Margin  Classifiers,  MIT Press.   To appear . [217]  Sontag, E. D.  1992 .  Feedforward  nets for  interpolation  and  classification, Journal  of Computer and System Sciences 45: 20-48.  [41, 85, 106] Sontag, E. D.  1995 .  Critical  points for  least-squares  problems involving  certain  analytic functions,  with  applications to sigmoidal nets, Advances in Computational Mathematics 5: 245-268. [340]  Sontag, E. D.  1997 .  Shattering all sets of k points in  "general  position"  requires   k -  l  2  parameters,  Neural Computation 9 2 : 337-348. [41] Sontag, E. D. and Sussmann, H. J.   1989 .  Backpropagation  can give rise to  spurious local minima even for  networks without  hidden layers, Complex Systems 3 1 : 91-106.  [139, 339]  Steele, J.  M.  1978 .  Existence of submatrices with all possible columns,  Journal  of Combinatorial  Theory,  Series A 24: 84-88. [41]  Sternberg,  S.  1964 .  Lectures on Differential Geometry,  Prentice Hall. [364] Stone, C.  1977 .  Consistent  nonparameteric  regression,  Annals  of Statistics  8: 1348-1360. [27]  Sussmann, H. J.   1992 .  Uniqueness of the weights for  minimal  feedforward nets with  a given input-output  map, Neural Networks 5: 589-593. [282]   378   Bibliography  Talagrand,  M.   1994 .  Sharper  bounds  for  Gaussian  and empirical  processes,  Annals  of  Probability  22:  28-76.  [58]  Tate,  R.  F.   1953 .  On  a double  inequality  of  the  normal  distribution,  Annals  of  Mathematical  Statistics  24:  132-134.  [364]  Tikhomirov,  V.  M.   1969 .  Diameters  of sets  in  function  spaces  and  the  theory  of  best  approximations,  Russian  Mathematical  Surveys 15 3 :  75-111.  [163]  Valiant,  L.  G.   1984a .  Deductive  learning,  Philosophical  Transactions  of the  Royal  Society  of  London,  Series  A  312:  441-446.  [306]  Valiant,  L.  G.   1984b .  A  theory  of  the  learnable,  Communications  of  the  ACM  27 11 :  1134-1142.  [27,  306]  van  der Vaart,  A.  W.  and Wellner,  J.  A.   1996 .  Weak  Convergence  and  Empirical  Processes,  Springer.  [216]  Vapnik,  V.  and  Chervonenkis,  A.   1974 .  Theory  of Pattern  Recognition,  Nauka,  Moscow,   in Russian .  [72]  Vapnik,  V.  N.   1982 .  Estimation  of  Dependences  Based  on  Empirical  Data,  Springer-Verlag,  New  York.  [9,  58,  73,  139,  217,  227,  240]  Vapnik,  V.  N.   1989 .  Inductive  principles  of the  search  for  empirical  dependences   methods  based  on weak  convergence  of  probability measures ,  Proceedings  of  the  2nd  Annual  Workshop  on  Computational Learning  Theory,  Morgan  Kaufmann,  San  Mateo,  CA,  pp.  3-21.  [296]  Vapnik,  V.  N.   1995 .  The  Nature  of  Statistical  Learning  Theory,  Springer-Verlag,  New  York.  [9,  217]  Vapnik,  V.  N.  and  Chervonenkis,  A.  Y.   1971 .  On the  uniform  convergence  of relative  frequencies  of events  to  their  probabilities,  Theory  of  Probability and  its  Applications  16 2 :  264-280.  [27,  41, 58,  150,  192,  246]  Vidyasagar,  M.   1997 .  A  Theory  of  Learning  and  Generalization,  with  to  Neural  Networks  and  Control  Systems,  Springer,  New  Applications  York.  [9,  216,  246]  Vu,  V.  H.   1998 .  On  the  infeasibility  of training  neural  networks  with  small  squared errors,  in  M.  I.  Jordan,  M.  J.  Kearns  and  S.  A.  Solla   eds , Advances  in  Neural  Information  Processing  Systems  10,  MIT  Press, pp.  371-377.  [340]  Wahba,  G.   1990 .  Spline  models  for  observational  data,  CBMS-NSF  Regional  Conference  Series in  Applied  Mathematics,  SI AM,  Philadelphia.  [217]  Wallace,  C.  S.  and  Boulton,  D.  M.   1968 .  An  information  measure  for  classification,  The  Computer  Journal  11 2 :  185-194.  [227]  Warren,  H.  E.   1968 .  Lower  bounds  for  approximation  by  nonlinear  manifolds,  Transactions  of  the  AMS  133:  167-178.  [107]  Watkins,  D.  S.   1991 .  Fundamentals  of  Matrix  Computations,  Wiley.  [329] Wenocur,  R.  S.  and  Dudley,  R.  M.   1981 .  Some  special  Vapnik-Chervonenkis  classes,  Discrete  Mathematics  33: 313-318.  [41]  White,  H.   1990 .  Connectionist  nonparametric  regression:  Multilayer  feedforward  networks  can  learn  arbitrary  mappings,  Neural  Networks 3:  535-549.  [216]  Williamson,  R.  C,  Smola,  A.  J.  and  Scholkopf,  B.   1999 .  Entropy  numbers,  operators  and support  vector  kernels,  Advances  in  Kernel  Methods: Support  Vector  Learning,  MIT  Press,  chapter  9,  pp.  127-144.  [217]  Zuev,  Y.  A.   1989 .  Asymptotics  of  the  logarithm  of  the  number  of  threshold  functions  of  the  algebra  of  logic,  Soviet  Mathematics  Doklady 39:  512-513.  [58]   Author index  Aho, A. V.,  306, 365 Akaike, H., 227, 365 Albertini, F.,  282, 365 Aldous, D., 28, 365 Alexander, K. S.,  58, 365 Alon, N., 41, 183, 256, 267, 365 Anderson, J.  A.,  9, 365 Angluin, D., 27, 28, 340, 361, 365 Anthony, M.,  9, 28, 58, 73, 139, 163,  183, 192, 217, 227, 268, 296, 330, 339, 365,  366,  377  Assouad, P., 41, 366 Auer, P., 339, 355, 366  Babai, L.,  183, 366 Barron, A. R.,  9, 216, 227, 240, 268,  282, 355, 366,  367  Bartlett,  P. L.,  28, 85, 129, 138, 139, 150, 183, 192, 216, 217, 227, 246, 256, 257, 267, 268, 282, 296, 329, 339, 340, 355, 356, 366-368,  372, 374-377  Barve, R.  D.,  28, 368 Baum, E. B., 85, 330, 340,  341, 368 Baxter, J.,  72, 107, 217, 356, 368,  375 Ben-David, S.,  28, 72, 107, 183, 256,  267, 339, 340, 365, 367,  368  Benedek, G.  M., 28, 368 Benedetti,  R.,  129, 368 Bernstein, S. N., 363, 368 Biggs, N., 9, 58, 339, 366 Billingsley, P., 27, 369 Birg<§, L.,  267, 369 Bishop, C. M., 9, 369 Block, H. D.,  330, 369 Blum, A.,  28, 339, 369 Blumer, A.,  27, 58, 306, 315, 329, 369 Bollobas, B., 41, 369 Boser, B. E.,  139, 217, 369 Boulton, D. M., 227, 378  Breiman, L.,  356, 369 Brent,  R. P.,  340, 369 Brightwell, G.,  28, 330, 366 Buescher, K. L.,  257, 369 Burges, C. J.  C,  217, 377 Bylander, T.,  330, 369  Carl,  B., 216, 369 Cesa-Bianchi, N., 27, 183, 256, 267,  365,  368,  369  Chalasani,  P., 28, 369 Chari, S., 41, 370 Chazan,  D.,  85, 371 Chernoff,  H.,  360, 370 Chervonenkis,  A.,  72, 378 Chervonenkis,  A. Y.,  27, 41,  58, 150,  192, 246, 378  Cormen, T.,  315, 370 Cortes, C,  217, 355, 356, 370 Cover, T.  M., 41, 85, 370 Cybenko,  G.,  10, 370  Darken, C,  355, 370 DasGupta,  B.,  130, 339, 370 Devroye, L.,  27, 72, 106, 362, 363, 370 Donahue,  M.,  355, 370 Downs, T.,  356, 371 Drucker, H.,  355,  356, 370 Duda,  R.  O.,  27, 138, 370 Dudley, R.  M., 41, 58, 163, 216, 256,  370,  378  Ehrenfeucht,  A.,  27, 58, 72, 306, 315,  329, 369,  371  El-Jaroudi,  A., 41, 375 Erlich, Y.,  85, 371  Farag6, A.,  227, 329, 371 Fefferman,  C,  282, 371 Feller, W.,  27, 358, 371  379   380  Author  index  Fischer, P., 27,  28, 367,  369 Fomin, S. V.,  282, 359, 373 Frankl, P., 41,  183, 366,  371 Frean, M., 340, 356, 371, 375 Freund, Y.,  28, 217, 355,  356, 371,  376 Friedman, J.,  356, 371  Gallant, A. R.,  216, 240, 282, 375 Gallant, S.,  329, 371 Garey, M. R.,  315, 329, 371 Goldberg, P. W.,  107, 129, 371 Goldman, S. A.,  28, 371 Golea, M., 340, 372 Grenander,  U., 227, 371 Grove, A. J.,  356, 371 Guo, Y.,  217, 372 Gurvits, L.,  183, 216,  217, 355, 370,  372  Guyon, I.  M.,  139, 217, 369 Gyorfi,  L.,  27, 106, 362, 363, 370  Hagerup, T.,  360, 372 Hancock, T.  R.,  340, 372 Hart, P. E.,  27, 138, 370 Hastie, T.,  356, 371 Haussler, D., 27, 41,  58, 72, 85, 163,  183, 216, 240, 246, 256, 267, 282, 296, 306, 315, 329, 365, 368,  369, 371,  372  Haykin, S.,  9, 372 Hebb, D.,  9, 372 Helmbold, D. P., 28, 372 Herbster, M., 339, 366 Hertz, J.,  9, 227, 372 Hinton, G. E.,  9, 130, 373,  376 Hoeffding,  W.,  361, 372 Holden, S. B., 163, 366 Horn, K. S. V.,  329, 372 Hornik, K.,  10, 372 Horvath, M.,  192, 373 Hoffgen,  K.-U., 28, 329, 367,  372  Ishai, Y.,  296, 366 Itai, A.,  28, 368  Jackel, L. D., 355, 370 Jackson, J.,  28, 373 Jacobs, R. A.,  130, 373 Jerrum, M. R.,  107, 129, 371 Ji, C.,  85, 373 Johnson, D. S.,  315, 329, 371, 373 Jones, L. K.,  216, 340, 373 Jordan,  M. I.,  130, 373 Judd, J.  S.,  339, 373  Kearns, M. J.,  9, 27,  28, 72, 139, 163,  227, 306, 340, 371-373 Khovanskii, A. G.,  129, 373 Koiran, P.,  85, 129,  130, 139, 216, 355,  Kolmogorov, A. N.,  150, 183,  282,359,  372,  373  373,  374  Kowalczyk, A., 41,  374 Krogh, A.,  9, 227, 372 Krzyzak, A.,  106, 130, 227, 268, 374 Kuh, A.,  28, 374 Kulkarni,  S. R.,  28, 183, 257, 367 Kumar, P. R.,  257, 369 Kwek, S.,  355, 366  Laird, P., 27, 365 Lee, W.  S.,  85, 139, 216,  217, 282, 329,  355, 356, 374,  376 Leiserson, C.,  315, 370 Leshno, M.,  10, 374 Levy, A.,  85, 371 LeCun, Y.,  355, 370 Li, M.,  27,  28, 373,  374 Lin, V.,  10, 374 Lindenbaum, M.,  72, 107, 368 Linder, T.,  106, 130, 227, 268, 374 Linial, N., 227, 374 Littlestone,  N., 27, 306, 372 Long, P. M., 28, 58, 107, 183, 246, 256,  267, 268, 367,  368, 372,  374  Lorentz,  G. G.,  163, 374 Lugosi, G.,  27, 58, 72, 106, 130, 192,  227, 268, 329, 362,  363, 368,  370, 371,  373-375  Maass, W.,  27, 85, 355, 366,  375 Macintyre, A. J.,  106,  107, 130, 373,  375  Maillot, V.,  282, 365 Maiorov, V.,  129, 368 Makhoul, J.,  41,  375 Makovoz, Y.,  216, 375 Mansour, Y.,  28, 227, 371, 373,  374 Marchand, M.,  340, 372 Mason, L.,  217, 356, 375 Massart, P., 267, 369 Matousek, J.,  41, 375 McCaffrey,  D. F.,  216, 240, 282, 375 Meir, R.,  129, 368 Mhaskar,  H.,  10, 375 Milnor, J.,  107, 375 Minsky, M.,  9, 375 Miyano, S.,  28, 377 Muroga, S.,  58, 330, 375  Karmarkar, N., 329, 373 Karpinski, M., 107, 130, 373  Natarajan,  B. K.,  9, 183, 246, 296, 315,  375   Author index  381  139, 282, 339, 340, 355, 365, 370, 373,  375,  377  Srinivasan, A., ^i,  370 Steele, J.  M., 41, 377 Sternberg,  S.,  364, 377 Stinchcombe,  M.,  10, 372 Stone, C,  27, 377 Sussmann, H. J.,  139, 282, 339, 377  Talagrand, M.,  58, 378 Tate,  R. F.,  364, 378 Thomas, J.,  85, 370 Tibshirani, R.,  356, 371 Tihomirov, V. M.,  150, 183, 374 Tikhomirov, V. M.,  163, 378 Tomkins, A., 28, 373  Ullman, J.  D., 306, 365  Valiant, L. G.,  27, 72, 306, 315, 330,  339, 361, 365, 371, 376,  378  van der Vaart, A. W.,  216, 378 Vapnik, V.,  72, 378 Vapnik, V. N., 9, 27, 41, 58, 73, 139, 150, 192, 217, 227, 240, 246, 296, 355, 369,  370,  378  Vazirani, U.,  9, 28, 340, 365,  373 Vidyasagar, M.,  9, 216, 246, 378 Vitanyi, P. M. B., 28, 374 Vu, V. H.,  340, 378  Wahba,  G.,  217, 378 Wallace, C. S.,  227, 378 Warmuth,  M. K.,  27, 58, 306, 315, 329,  339, 355, 366, 369,  372  Warren, H. E.,  107, 378 Watkins, D. S.,  329, 378 Wellner, J.  A.,  216, 378 Welzl, E.,  41, 372 Wenocur, R. S., 41, 378 White,  H.,  10, 216, 372,  378 Williams, R. J.,  9, 376 Williamson, R.  C,  85, 129, 139, 216,  217, 227, 246, 267, 268, 282, 296, 329, 355, 367,  368, 372, 374, 377,  378  Zeger, K.,  227, 268, 375 Zuev, Y. A.,  58, 378  Ng, A. Y.,  227, 373 Nilsson, N. J.,  85, 330, 376 Nowlan, S. J.,  130, 373  Palmer, R.  G.,  9, 227, 372 Papert, S.,  9, 375 Patel, J.  K.,  364, 376 Petrack, S.,  85, 371 Petsche, T.,  28, 374 Pinkus, A.,  10, 374 Pisier, G.,  216, 376 Pitt,  L.,  315, 330, 339, 376 Pollard, D., 58, 150, 163, 192, 246, 256,  267, 268, 282, 376  Posner, S. E.,  183, 256, 367 Powell, M. J.  D., 130, 376 Preparata, F. P., 329, 373 Psaltis, D., 85, 373  Quinlan, J.  R.,  356, 376  Read, C. B., 364, 376 Ripley, B. D., 9, 376 Risler, J.-J.,  129, 368 Rissanen, J.,  227, 376 Rivest, R. L.,  28, 227, 315, 339, 369,  370,  374  Rohatgi, P., 41, 370 Ron, D., 227, 373 Rosenblatt, F.,  9, 330, 376 Rosenfeld, E.,  9, 365 Rub, C,  360, 372 Rumelhart, D. E.,  9, 376  Sakurai, A.,  85, 129, 376 Sauer, N., 41, 376 Schapire, R. E.,  27, 139, 163, 217, 355,  356, 371, 373, 376  Schlafli,  L., 41, 377 Schocken, S.,  10, 374 Schuurmans, D.,  73, 217, 356, 371, 377 Schwartz, R.,  41, 375 Scholkopf, B., 217, 377,  378 Sellie, L. M., 27, 373 Shamir, E.,  27, 369 Shawe-Taylor, J.,  28, 58, 7 ,  139, 217, 227, 296, 330, 366, 368, 372,  377  Shelah, S., 41, 377 Shinohara, A., 28, 377 Siegelmann, H. T.,  339, 370 Simon, H. U., 27, 72, 164, 192, 268,  329, 369, 372,  377  Sloan, R. H.,  27, 377 Slud, E.,  363, 377 Smola, A. J.,  217, 377,  378 Sontag, E. D., 4U  85, 106, 129, 130,   Subject  index     inner product , 3 [n]  the set  {l,2,...,n} ,  156 \-]   ceiling function , 56 L-J  floor function , 56 ®   Kronecker product , 172  A z,e    approximate-SEM  algorithm ,  237  absolute loss, 267, 284  average over multiple outputs, 288  accuracy parameter, 16 activation function, 5, 76  satisfying  a Lipschitz condition, 199  Adaboost, 352, 355  and weighted sample error  minimization, 353  learning in the restricted real classification  model, 355  adaptivity, 226 affine  subspace, 3 agnostic pac learning, 27 Akaike's information criterion, 227 algorithmics of supervised  learning, 9,  299-356  analytic functions, 35 APPROX-F-FIT decision problem, 314 approximate interpolation, 289, 296  generalization from, 291  and fat-shattering  dimension, 294 and pseudo-dimension, 293  strong generalization from, 290  and band dimension, 292 and pseudo-dimension, 292  approximate-SEM  algorithm, 236, 237,  258  efficient,  304 efficient  randomized, 309  for a class of vector-valued  functions,  for a graded function class, 302 for convex combinations, 342, 346 sample complexity, 258  approximation  error of a function class,  288  15  approximation  issues in supervised  learning,  1, 9, 18  artificial  neural networks, see neural  networks  asymptotic optimality of pattern classification  techniques, 17  B   bound on \y — y\ in real prediction  problem , 233  band F,7     band dimension , 292 band dimension, 292  and pseudo-dimension, 292  Bayes optimal classifier,  17 Bernoulli random variable  estimating the probability of,  59, 273  Bernstein's inequality, 278, 283, 363 BF   subgraph class ,  153, 154 Bf   indicator function  of region below  graph of    ,  153  binary classification,  8, 13  restricted model, 23, 52, 263, 289  and existence of weak predictors,  351  and probably  approximately  correct model, 306  efficient  learning, 305, 306, 315 estimation error, 53, 58 sample complexity, 52, 58 sample complexity,  184-192  binary input, 4 binomial distribution, 19 binomial theorem, 358 boolean perceptron,  316-330  382   Subject  index  383  efficient  consistent-hypothesis-finder,  efficient,  305  323  sample complexity, 52 boosting algorithms, 355 bounded variation functions,  159  covering numbers, 180, 183 fat-shattering  dimension, 160, 163  BP,  BPn   functions computed by the  boolean perceptron , 316  consistent-hypothesis-finders,  322 not efficiently  learnable, 319  BP-FIT  Construct   learning algorithm for  convex combinations , 346, 355  sample complexity, 348  constructive  learning algorithms, 340,  342-356  continuous from the right, 195 convex class  sample complexity, 263  convex classes, 269-283  sample complexity, 277 slow convergence with absolute loss,  decision problem, 316 reduction  to  VERTEX  COVER,  317  296  convex  combinations  Cauchy-Schwarz inequality, 359 CC -   number of connected  components , 30  chaining, 55-58, 265, 267 Chebyshev inequality, 360 Chernoff  bounds, 360 classification,  14 classification  learning, see binary  classification,  real  classification  closed under addition of constants,  188 closed under scalar multiplication, 162 closure convexity, 269  and sample complexity, 271, 277 and variances, 278 definition, 271  closure of a function  class, 270 co 5    convex hull of 5 , 204 coin toss, 19 compactness, 271, 282 complexity  automatic choice of,  219-227 of a network, 8 penalty, 8, 221, 222 regularization, 227 theory, 312, 315 computability, 299 computation  bit cost model of,  301 issues in supervised learning, 9 units, 74  computational complexity, 2, 299, 306,  312  conditional  expectation  approximating, 232 best approximation, 278 empirical, 346  confidence  parameter, 16 connected components, 30 connections, 74 consistent  hypotheses, 305 consistent  learning algorithms, 24, 27 consistent-hypothesis-finders,  52, 315  approximation rate, 203, 216 constructive approximation, 342, 355 constructive learning algorithms,  342-351  covering number bounds, 205 large margin SEM algorithm, 352  convex  function  Jensen's inequality, 358  convex hull, 204 covering numbers  and dimensions,  165-183 and uniform convergence,  140-150 as generalization of growth function,  241  bounds, 247-257  pseudo-dimension versus  fat-shattering  dimension, 181  bounds in terms of fat-shattering  dimension,  175, 248  di,  241, 247  bounds in terms of  pseudo-dimension, 251  doo, 241, 247, 268 effects  of composition with a  Lipschitz function, 206  effects  of scaling, 206 lower bound in terms of fat-shattering  dimension, 178  of composition of function classes, 197 of the loss function class, 242 relationship with packing numbers,  166, 183  critical point, 364 critical value, 364  d\  covering numbers, see covering  d\  packing numbers, see packing  numbers, d\  numbers, d\  numbers, doo  doo  covering numbers, see covering  ki -P   pseudometric , 251   384  Subject index  packing numbers, see packing  numbers, d£,i P   dL2 P    L* p   metric , 255 dLoo   ^oo metric ,  197, 236 doo *»    distance  between  vectors of elements of a metric space , 197  data generation  model, 231 decision boundary, 3, 4 decision problem, 313 decision rule, 60  randomized, 273  decision-theoretic  learning models, 296 6   confidence  parameter , 16 dichotomies  counting in parameter space, 30  dim »    linear dimension , 37 distribution  changing, 28  distribution-independence, 18  early stopping, 225 efficient  learning, 299-306  characterization, 312  efficient  learning algorithm, 306  and fat-shattering  dimension, 303 and  J-T-FIT,  313 and NP-hardness of the    -CONSISTENCY  decision problem, 314  and NP-hardness of the  if-FIT  decision problem, 314  and VC-dimension, 303 definition, 302 efficient  randomized SEM algorithm  is necessary, 311  is sufficient, 309  empirical cover, 255 empirical error, 234 entropy, 79, 85 e  accuracy  parameter , 16 e-good hypothesis, 16 e-packing, 165 e-separated, 55,  165 eo m>6   estimation error , 17 cx, m,<J,B    estimation  error , 234 €£, m, £,7    estimation error , 137 erp *    error of a real-valued  function  with respect to P  and 7 , 136  erl -    sample error of a real-valued function  with respect to 7 ,  184 &Z  '    ^-sample error of a real-valued  €TZ       sample error of a binary-valued  function , 285  function , 15  function , 234  erp -    £-error of a real-valued  function , 284  erM -,t    error of a binary-valued  function  with respect  to target £ , 24  erp -    error of a binary-valued  function , 15  erp '    error of a real-valued  function ,  233  error  and error estimate, 8 convergence  rate, 17, 27 expected  value of, 26  relationship with the restricted  model, 27  in restricted  model of binary  classification, 24  of a binary-valued function, 15 of a real-valued function, 232  estimation error  convergence  rate, 18 definition,  17, 18, 137, 234 for classes with finite VC-dimension,  43  for finite classes, 21, 235 inherent, 18  estimation  issues in supervised learning,  2,  4, 9,  18  Euler's inequality, 358  F   closure of F , 270 Fn   function  class with  complexity  parameter n , 300   *   conditional  expectation , 279 fa   best approximation to conditional  factorial  Stirling's approximation, 358  fat-shattering  dimension, 8, 159-163  and covering numbers, 174, 180,  182,  183  and learnability of real function  classes, 258  and packing numbers, 174 arbitrary rate, 182 characterization  of learnability,  262-267  definition, 159 finite, 159  but pseudo-dimension  infinite, 162  of parameterized  classes  with bounded number of  parameters, 196  with bounded  parameters, 203  relationship with  pseudo-dimension,  fatF  7   fat-shattering  dimension , 159  efficient  randomized SEM algorithm  expectation , 278  erz -    sample error of a real-valued  162,  163   Subject  index  385  feed-forward  networks, 74 finite function  classes, 19-22, 234-236 fully connected  between adjacent layers,  75  7-dimension, 159 7-shattered,  159 Gaussian elimination, 329 GE p,m,fc    Chernoff  bound , 361 general position, 30  and linear independence, 32  generalization, 5 graded function class, 299 gradient descent, 6, 9, 225, 339 graph colouring, 332 graph theory, 316 growth function,  29-35  and VC-dimension, 39  H  binary-valued  function  class , 15 Hk   functions  computed by simple  perceptrons with fan-in no more than fc , 321  Hn   binary function class with  complexity parameter n , 300    -CONSISTENCY decision problem, 313 H'FYT decision problem, 313 Hamming distance, 80, 178 Hilbert space, 271, 282 Hoeffding's  inequality, 361  bounds on tails of binomial  distribution, 20  in uniform convergence proof, 50, 55  Holder inequalities, 359 hyperplanes, 31 hypothesis, 15  representation, 299  $$ z   imaginary part of z y  275 independence of training examples, 27 inner product, 3, 343 input, 14 space, 7 units, 74 vector, 2 weights, 5  interior point  methods, 214  for linear programming, 323  interpolation, 296 interpolation models, 289-295  Jacobian, 364 Jensen's inequality, 358  A:-COLOURING decision problem, 332 fc-plane,  33 Karmarkar's algorithm, 323  Kronecker product, 172  L  learning algorithm , 16 Li P   pseudometric, 251 L2 P  covering numbers, 255 Loo covering numbers  sample complexity bounds in terms  of,  237  Loo metric, 196, 236 If   loss function  for    , 233 IF   loss class , 284 £v   loss function  for approximate  interpolation , 291  Is   loss function  for 5 outputs , 286  covering numbers, 287  terror, 284 flayer  network, 75 ^-sample error, 285 labelled examples, 6, 14 labels, 1, 14 noisy, 27  large margin classification,  8, 135  and generalization from approximate  interpolation, 294 law of large numbers, 19 layers, 74 LE p,ra,fc    Chernoff  bound , 361 learnability,  17, 234  efficient,  308  learner, 13 learning  as optimization, 307-315 pattern classification,  see binary  classification,  real classification  real-valued functions,  see real  prediction  learning algorithms  based on approximate-SEM  algorithm, 259  definition,  13, 16, 136, 233 enumerative, 329  linear computation units  fat-shattering  dimension, 213 pseudo-dimension, 155  linear programming  efficient  algorithms, 323 for boolean  perceptron learning  algorithms, 322  for BPn  learning algorithms, 329 for classification  learning with linear  functions, 329  linear subspace, 33 linear threshold networks, 76 approximate sample error  minimization, 340  feed-forward, 74 growth function, 77   386  Subject  index  hardness of learning, 331, 335, 338,  simulation with sigmoid network, 84,  339  85  VC-dimension, 74-85  lower bounds, 80, 82, 85 upper bounds, 77, 85  with bounded fan-in  efficient  learning algorithm, 350  Lipschitz condition, 199 local minima, 6, 339 logarithm  inequalities, 357  loss class, 284  covering numbers, 285  loss functions, 233, 284-289, 296  bounded, 284 satisfying  a Lipschitz condition, 245,  285  mo e,<J    sample complexity , 16 WIL C»£    sample complexity , 234 TOL C,£,7    sample complexity , 137 A4 e, Wjd    c-packing number of W  with respect  to d , 165  .Mi  €jH fk    uniform packing number  with respect to di , 165  A^2  c, H,k    uniform packing number  with respect  to cfe , 165  margins, 291, 351 Markov's inequality, 360 matrix  determinant, 35 row-rank, 37  measurability conditions,  15 measure  Lebesgue, 35, 364 outer, 364 theory, 27  method of sieves, 227 minimum description length principle,  226,  227  minimum message length, 227 misclassification  probability, 8 model selection, 218-227 monomials, 171 ft  probability distribution , 24 multi-layer networks, see neural  networks, multi-layer  multisets, 156  N%7  7v£n   two-layer sigmoid networks  with two first-layer units , 337  TV^-APPROX-SEM problem, 337 Af\  N*   two-layer linear threshold  networks with k first-layer units , 335  and graph colouring, 335  ^A»  ^A,n   conjunctions of A; linear  threshold units on binary inputs , 332  and graph colouring, 333, 339  N^-CONSISTENCY decision problem, 332  NP-hardness, 335  N%,  N%n   two-layer sigmoid networks  with k first-layer units , 338  A-APPROX-SEM  problem, 338 net input, 84 neural networks  architecture, 74 biological, 9 classes with  infinite  pseudo-dimension, 265  dimensions,  193-217 general, 7 hardness of learning, 331-341 Lipschitz in parameters  covering number bounds, 199 fat-shattering  dimension bounds,  202  multi-layer, 74 sample complexity bounds, 261 state,  13, 76, 299 two-layer  approximate-SEM  algorithm, 350 as convex combinations of  functions, 342  VC-dimension,  108-130 with bounded output  weights as convex combination, 277  with bounded  parameters  covering number bounds, 207-212 fat-shattering  dimension bounds,  212-213  sample complexity,  262, 349  with finite weight set  sample complexity, 236  with multiple outputs,  286-289 sample complexity, 288, 296 with piecewise-linear activation  hardness of learning, 339  with piecewise-polynomial  activation  functions  functions  sample complexity, 261  with real-valued output, 231  noise, 14, 231, 266, 268 non-convex classes  sample complexity, 263, 270  normal distribution tail bounds, 364  norms  dual, 359   Subject  index  387  induced by probability distributions,  270  NP-hardness, 312  optp  F    approximation error of the  class F  of real functions , 233  optp H    approximation error of the  class H of binary-valued  functions , 15  optJ F    optimal  large margin error of  the class F , 185 orthantsof  Rm,  153 output  space, 7 unit, 75 weights, 5  P   probability distribution , 14 PAC learning, see probably  approximately correct learning  packing, 80 packing numbers, 55, 165-167 and quantized  classes, 168 bounds in terms of fat-shattering  dimension, 174  bound in terms of fat-shattering  dimension, 247  di  dLx P   bounds in terms of  pseudo-dimension, 251  uniform, 165 parameter space  counting cells, 30, 32, 41  parameterization  uniqueness of,  280  pattern classification,  13  with binary-output  networks,  11-130 with real-output  networks,  131-227  patterns, 1 Pdim -    pseudo-dimension ,  153 perceptron, 2, 9, 22, 74, 77  binary-weight, 23  sample complexity, 23  convergence  theorem, 323, 330  and real classification,  328  enumerative learning algorithm, 319 estimation error, 51 fan-in, 319 functions computable  by, 7  b-bit, 23  sample complexity, 23  learning algorithm, 3, 9, 323, 329  and classification  noise, 330 is not efficient,  328, 330  learning in the restricted model,  322-328  representational  capabilities, 4 sample complexity, 51 shattering and affine  independence,  36, 41  242  with binary inputs, 316  permutations on a double sample, 47,  swapping group, 58 symmetric group, 58 ^-dimension,  170, 183 $dim -    ^-dimension ,  170, 182 pigeonhole principle, 166 n      growth function , 29 Pm   product  probability distribution ,  15  polynomial transformation,  156, 163  pseudo-dimension, 156  polynomial-time algorithm, 313 predicting a real-valued quantity, see  learning real-valued  functions  probabilistic concepts, 192  and fat-shattering  dimension, 163  probability distribution, 14  product, 15  probability estimation, 231 probability theory,  13, 27 probably approximately correct  learning, 27, 306  and weak learning, 355  pseudo-dimension,  151-159, 163  and compositions with non-decreasing  functions,  153  and d -covering numbers, 167, 183 and linear dimension, 154 from VC-dimension bounds,  194, 216 infinite,  265, 267  pseudo-shattered,  152  versus shattered, 152  Qa -    quantization  operator , 167 Qa{F    quantized  versions of functions  quadratic loss, 231, 245, 267, 278, 284 average over multiple outputs, 286,  in F , 248  288  quadratic optimization, 214, 217 quantization,  167, 248 queries, 28, 340  radial basis function  networks, 213 random number generator, 307 randomized  algorithm, 307  polynomial-time, 313  rank  of a linear system of equations, 38 of a matrix, 37  real classification,  8,  131-227   Subject  index  388  real labels  encoding information in, 265 noisy, 266 quantized, 266, 268  real prediction, 8, 231-296  restricted model, 265, 268, 281 sample complexity, 258-268  regularization  complexity, 227 weight decay, 225  RL m,n    worst-case running time ,  301  RP   decision problems solvable in  polynomial time using a randomized algorithm , 313  sample complexity, 17  bounding with pseudo-dimension, 260 definition,  18, 137, 234 for a class of vector-valued  functions,  288  gap between upper and lower bounds,  262,  269 inherent, 18 lower bounds in terms of  fat-shattering  dimension, 188, 262  lower bounds in terms of VC-dimension, 59-73  of a closure convex class, 269 of a finite binary-valued class, 21, 235  restricted model, 25  upper bounds in terms of  fat-shattering  dimension, 265  upper bounds in terms of  pseudo-dimension, 265  upper bounds in terms of VC-dimension, 42-58  sample error  as estimate of error, 19 definition,  15, 234 weighted, 352  sample error minimization algorithms,  19,42  efficient,  304 efficient  randomized, 309 for a graded function  class, 302 for finite class of real-valued  functions, 235  estimation error, 235  large margin, 184  estimation error, 187 sample complexity,  187 sample complexity bound involving  pseudo-dimension, 191  sample complexity, 54 weighted, 352  Sard's Theorem, 364 Sauer's Lemma, 41  generalization  involving  pseudo-dimension,  170, 183  linear algebraic proof, 183  scalar product, 204, 271 scale-sensitive dimension,  159, 291 SEM algorithm, see sample error  minimization algorithms  SET-SPLITTING decision problem, 339 sgn »   threshold function , 3, 76 shattering,  35, 41, 151  width of,  159 witness of,  152, 159  cr '    standard sigmoid function , 83 sigmoid functions  linear independence, 275 standard, 5, 83  sigmoid networks, 83  approximation, estimation,  computation  properties, 6  hardness of learning, 337-338, 340 invariances, 281 non-convexity, 275 two-layer, 5 uniqueness of parameterization, 280,  282  VC-dimension lower bounds, 83 VC-dimension upper bounds,  122-128  sigmoid unit, 83 simple perceptron,  see perceptron Slud's inequality, 363 span . , 171 spanning set of a vector space, 171 Splitting   enumerative algorithm for simple perceptrons , 319, 329, 349  squared error, see quadratic loss squashing function, 5 states, 7 step function, 76  versus sigmoid function, 83  structural risk minimization, 227 subgraph class, 153 supervised learning, 1  applications, 1 definition,  13  support vector machines, 217 symmetrization, 46, 242, 285  tails of distributions, 360 target function, 23 testing sample, 46 6   threshold of simple perceptron , 2 thresholds, 3, 76 time complexity, 299 topology  of function  classes, 270, 282 total variation, 160   Subject index   389  Y   output  space , 7 y   label , 14  Z   = X  x y  , 14 z   training sample , 14 Zn   = XnxY >  300  touchstone  class, 25, 27   and computational  complexity, 340  and sample complexity, 240 of real functions,  240   trace number of a set system, 41  training,  1  data, 14 samples, 14, 24  uniform convergence  and fat-shattering  dimension, 267 for classes with finite VC-dimension,  43,53  for finite classes, 19 for real functions,  241-246 rate, 266, 282  improved using variance  information, 277  restricted  model with zero mean  noise, 281, 282  relative results, 71-72,  191, 266 with general  loss functions, 285  uniform distance, 196 union bound, 21  in uniform convergence  proof, 50, 55  universal  approximation, 6, 10  Vapnik-Chervonenkis  dimension, 8,  35-41  lower bounds for smoothly  parameterized  function  classes, 85  of a vector space of real functions, 37  VC-dimension, see  Vapnik-Chervonenkis  dimension  VC-major classes, 163 VC-number, 41 vector space basis, 37 dimension, 37 pseudo-dimension, 154 spanning set,  171  VERTEX  COVER decision  problem,  317 vertex cover of a graph, 316  W   number of network  parameters , 76 w   weights of simple perceptron , 2 weak binary classification  learning, 355 weight decay, 225 weights, 76  of a simple perceptron, 2  worst-case running time, 301  X   input  space , 7 x   input  vector , 2, 14 Xn   input  space with  complexity  parameter n , 300

@highlight

This important work describes recent theoretical advances in the study of artificial neural networks. It explores probabilistic models of supervised learning problems, and addresses the key statistical and computational questions. Chapters survey research on pattern classification with binary-output networks, including a discussion of the relevance of the Vapnik Chervonenkis dimension, and of estimates of the dimension for several neural network models. In addition, Anthony and Bartlett develop a model of classification by real-output networks, and demonstrate the usefulness of classification with a "large margin." The authors explain the role of scale-sensitive versions of the Vapnik Chervonenkis dimension in large margin classification, and in real prediction. Key chapters also discuss the computational complexity of neural network learning, describing a variety of hardness results, and outlining two efficient, constructive learning algorithms. The book is self-contained and accessible to researchers and graduate students in computer science, engineering, and mathematics.