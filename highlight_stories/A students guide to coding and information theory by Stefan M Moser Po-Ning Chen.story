A Student’s Guide to Coding and Information Theory  This easy-to-read guide provides a concise introduction to the engineering background of modern communication systems, from mobile phones to data compression and storage. Background mathematics and speciﬁc engineering techniques are kept to a minimum, so that only a basic knowledge of high-school mathematics is needed to understand the material covered. The authors begin with many practical applications in coding, includ- ing the repetition code, the Hamming code, and the Huffman code. They then explain the corresponding information theory, from entropy and mutual information to channel capacity and the information transmission theorem. Finally, they provide insights into the connections between coding theory and other ﬁelds. Many worked examples are given throughout the book, using practical applications to illustrate theoretical deﬁni- tions. Exercises are also included, enabling readers to double-check what they have learned and gain glimpses into more advanced topics, making this perfect for anyone who needs a quick introduction to the subject.  stef a n m. mos er is an Associate Professor in the Department of Electrical Engi- neering at the National Chiao Tung University  NCTU , Hsinchu, Taiwan, where he has worked since 2005. He has received many awards for his work and teaching, including the Best Paper Award for Young Scholars by the IEEE Communications Society and IT Society  Taipei Tainan Chapters  in 2009, the NCTU Excellent Teaching Award, and the NCTU Outstanding Mentoring Award  both in 2007 .  po-ning chen is a Professor in the Department of Electrical Engineering at the National Chiao Tung University  NCTU . Amongst his awards, he has received the 2000 Young Scholar Paper Award from Academia Sinica. He was also selected as the Outstanding Tutor Teacher of NCTU in 2002, and he received the Distinguished Teaching Award from the College of Electrical and Computer Engineering in 2003.    A Student’s Guide to Coding and  Information Theory  STEFAN M. MOSER  PO-NING CHEN  National Chiao Tung University  NCTU ,  Hsinchu, Taiwan   cambridge university press  Cambridge, New York, Melbourne, Madrid, Cape Town,  Singapore, S˜ao Paulo, Delhi, Tokyo, Mexico City  Cambridge University Press  The Edinburgh Building, Cambridge CB2 8RU, UK  Published in the United States of America by Cambridge University Press, New York  Information on this title: www.cambridge.org 9781107015838  www.cambridge.org  C cid:1  Cambridge University Press 2012  This publication is in copyright. Subject to statutory exception and to the provisions of relevant collective licensing agreements, no reproduction of any part may take place without the written  permission of Cambridge University Press.  First published 2012  Printed in the United Kingdom at the University Press, Cambridge  A catalog record for this publication is available from the British Library  ISBN 978-1-107-01583-8 Hardback ISBN 978-1-107-60196-3 Paperback  Additional resources for this publication at www.cambridge.org moser  Cambridge University Press has no responsibility for the persistence or accuracy of URLs for external or third-party internet websites referred to  in this publication, and does not guarantee that any content on such  websites is, or will remain, accurate or appropriate.   Contents  1  2  3  List of contributors Preface Introduction 1.1 1.2 Model and basic operations of information processing  Information theory versus coding theory  page ix xi 1 1  1.3 1.4 1.5 1.6  systems Information source Encoding a source alphabet Octal and hexadecimal codes Outline of the book References  Error-detecting codes 2.1 2.2 2.3 2.4 2.5 2.6 2.7  Review of modular arithmetic Independent errors – white noise Single parity-check code The ASCII code Simple burst error-detecting code Alphabet plus number codes – weighted codes Trade-off between redundancy and error-detecting capability Further reading References  2.8  Repetition and Hamming codes 3.1 3.2  Arithmetics in the binary ﬁeld Three-times repetition code  2 4 5 8 9 11  13 13 15 17 19 21 22  27 30 30  31 33 34   vi  4  5  Contents  3.3  3.4  Hamming code 3.3.1 3.3.2  Some historical background Encoding and error correction of the  7,4  Hamming code  3.3.3 Hamming bound: sphere packing Further reading References  Data compression: efﬁcient coding of a random message 4.1 4.2 4.3 4.4 4.5 4.6 4.7 4.8 4.9  A motivating example Preﬁx-free or instantaneous codes Trees and codes The Kraft Inequality Trees with probabilities Optimal codes: Huffman code Types of codes Some historical background Further reading References  Entropy and Shannon’s Source Coding Theorem 5.1 Motivation 5.2  Binary entropy function The Information Theory Inequality Bounds on the entropy  Uncertainty or entropy 5.2.1 Deﬁnition 5.2.2 5.2.3 5.2.4 Trees revisited Bounds on the efﬁciency of codes 5.4.1 What we cannot do: fundamental limitations  of source coding  Coding Theorem for a Single Random Message  5.4.2 What we can do: analysis of the best codes 5.4.3 Coding of an information source Some historical background Further reading Appendix: Uniqueness of the deﬁnition of entropy References  5.3 5.4  5.5 5.6 5.7 5.8  40 40  42 48 52 53  55 55 57 58 62 65 66 73 78 78 79  81 81 86 86 88 89 90 92 95  95 97 101 103 108 110 111 112   6  7  8  Contents  Introduction The channel The channel relationships The binary symmetric channel System entropies  Mutual information and channel capacity 6.1 6.2 6.3 6.4 6.5 6.6 Mutual information 6.7 6.8 6.9 6.10 Characterization of the capacity-achieving input distri-  Deﬁnition of channel capacity Capacity of the binary symmetric channel Uniformly dispersive channel  bution  6.11 Shannon’s Channel Coding Theorem 6.12 Some historical background 6.13 Further reading  References  Approaching the Shannon limit by turbo coding 7.1 7.2 7.3 7.4 7.5 7.6 7.7  Information Transmission Theorem The Gaussian channel Transmission at a rate below capacity Transmission at a rate above capacity Turbo coding: an introduction Further reading Appendix: Why we assume uniform and independent data at the encoder Appendix: Deﬁnition of concavity References  7.8  Other aspects of coding theory 8.1 8.2 8.3  Hamming code and projective geometry Coding and game theory Further reading References  References Index  vii  115 115 116 118 119 122 126 130 131 134  136 138 140 141 141  143 143 145 146 147 155 159  160 164 165  167 167 175 180 182  183 187    Contributors  Po-Ning Chen   Chapter 7   Francis Lu   Chapter 3 and 8   Stefan M. Moser   Chapter 4 and 5   Chung-Hsuan Wang   Chapter 1 and 2   Jwo-Yuh Wu   Chapter 6     Preface  Most of the books on coding and information theory are prepared for those who already have good background knowledge in probability and random pro- cesses. It is therefore hard to ﬁnd a ready-to-use textbook in these two subjects suitable for engineering students at the freshmen level, or for non-engineering major students who are interested in knowing, at least conceptually, how in- formation is encoded and decoded in practice and the theories behind it. Since communications has become a part of modern life, such knowledge is more and more of practical signiﬁcance. For this reason, when our school requested us to offer a preliminary course in coding and information theory for students who do not have any engineering background, we saw this as an opportunity and initiated the plan to write a textbook.  In preparing this material, we hope that, in addition to the aforementioned purpose, the book can also serve as a beginner’s guide that inspires and at- tracts students to enter this interesting area. The material covered in this book has been carefully selected to keep the amount of background mathematics and electrical engineering to a minimum. At most, simple calculus plus a lit- tle probability theory are used here, and anything beyond that is developed as needed. Its ﬁrst version has been used as a textbook in the 2009 summer freshmen course Conversion Between Information and Codes: A Historical View at National Chiao Tung University, Taiwan. The course was attended by 47 students, including 12 from departments other than electrical engineering. Encouraged by the positive feedback from the students, the book went into a round of revision that took many of the students’ comments into account. A preliminary version of this revision was again the basis of the correspond- ing 2010 summer freshmen course, which this time was attended by 51 stu- dents from ten different departments. Speciﬁc credit must be given to Professor Chung-Hsuan Wang, who volunteered to teach these 2009 and 2010 courses and whose input considerably improved the ﬁrst version, to Ms. Hui-Ting   xii  Preface  Chang  a graduate student in our institute , who has redrawn all the ﬁgures and brought them into shape, and to Pei-Yu Shih  a post-doc in our institute  and Ting-Yi Wu  a second-year Ph.D. student in our institute , who checked the readability and feasibility of all exercises. The authors also gratefully ac- knowledge the support from our department, which continues to promote this course.  Among the eight chapters in this book, Chapters 1 to 4 discuss coding tech- niques  including error-detecting and error-correcting codes , followed by a brieﬁng in information theory in Chapters 5 and 6. By adopting this arrange- ment, students can build up some background knowledge on coding through concrete examples before plunging into information theory. Chapter 7 con- cludes the quest on information theory by introducing the Information Trans- mission Theorem. It attempts to explain the practical meaning of the so-called Shannon limit in communications, and reviews the historical breakthrough of turbo coding, which, after 50 years of research efforts, ﬁnally managed to ap- proach this limit. The ﬁnal chapter takes a few glances at unexpected relations between coding theory and other ﬁelds. This chapter is less important for an understanding of the basic principles, and is more an attempt to broaden the view on coding and information theory.  In summary, Chapter 1 gives an overview of this book, including the system model, some basic operations of information processing, and illustrations of how an information source is encoded.  Chapter 2 looks at ways of encoding source symbols such that any errors, up to a given level, can be detected at the receiver end. Basics of modular arithmetic that will be used in the analysis of the error-detecting capability are also included and discussed.  Chapter 3 introduces the fundamental concepts of error-correcting codes us- ing the three-times repetition code and the Hamming code as starting exam- ples. The error-detecting and -correcting capabilities of general linear block codes are also discussed.  Chapter 4 looks at data compression. It shows how source codes represent the output of an information source efﬁciently. The chapter uses Professor James L. Massey’s beautifully simple and elegant approach based on trees. By this means it is possible to prove all main results in an intuitive fashion that relies on graphical explanations and requires no abstract math.  Chapter 5 presents a basic introduction to information theory and its main quantity entropy, and then demonstrates its relation to the source coding of Chapter 4. Since the basic deﬁnition of entropy and some of its properties are rather dry mathematical derivations, some time is spent on motivating the deﬁnitions. The proofs of the fundamental source coding results are then again   Preface  xiii  based on trees and are therefore scarcely abstract in spite of their theoretical importance.  Chapter 6 addresses how to convey information reliably over a noisy com- munication channel. The mutual information between channel input and output is deﬁned and then used to quantify the maximal amount of information that can get through a channel  the so-called channel capacity . The issue of how to achieve channel capacity via proper selection of the input is also discussed. Chapter 7 begins with the introduction of the Information Transmission Theorem over communication channels corrupted by additive white Gaussian noise. The optimal error rate that has been proven to be attainable by Claude E. Shannon  baptized the Shannon limit  is then addressed, particularly for the situation when the amount of transmitted information is above the channel ca- pacity. The chapter ends with a simple illustration of turbo coding, which is considered the ﬁrst practical design approaching the Shannon limit.  Chapter 8 describes two particularly interesting connections between coding theory and seemingly unrelated ﬁelds: ﬁrstly the relation of the Hamming code to projective geometry is discussed, and secondly an application of codes to game theory is given.  The title, A Student’s Guide to Coding and Information Theory, expresses our hope that this book is suitable as a beginner’s guide, giving an overview to anyone who wishes to enter this area. In order not to scare the students  espe- cially those without an engineering background , no problems are given at the end of each chapter as usual textbooks do. Instead, the problems are incorpo- rated into the main text in the form of Exercises. The readers are encouraged to work them out. They are very helpful in understanding the concepts and are motivating examples for the theories covered in this book at a more advanced level.  The book will undergo further revisions as long as the course continues to be delivered. If a reader would like to provide comments or correct typos and errors, please email any of the authors. We will appreciate it very much!    1  Introduction  Systems dedicated to the communication or storage of information are com- monplace in everyday life. Generally speaking, a communication system is a system which sends information from one place to another. Examples include telephone networks, computer networks, audio video broadcasting, etc. Stor- age systems, e.g. magnetic and optical disk drives, are systems for storage and later retrieval of information. In a sense, such systems may be regarded as com- munication systems which transmit information from now  the present  to then  the future . Whenever or wherever problems of information processing arise, there is a need to know how to compress the textual material and how to protect it against possible corruption. This book is to cover the fundamentals of infor- mation theory and coding theory, to solve the above main problems, and to give related examples in practice. The amount of background mathematics and electrical engineering is kept to a minimum. At most, simple results of calculus and probability theory are used here, and anything beyond that is developed as needed.  1.1 Information theory versus coding theory  Information theory is a branch of probability theory with extensive applica- tions to communication systems. Like several other branches of mathematics, information theory has a physical origin. It was initiated by communication scientists who were studying the statistical structure of electrical communica- tion equipment and was principally founded by Claude E. Shannon through the landmark contribution [Sha48] on the mathematical theory of communications. In this paper, Shannon developed the fundamental limits on data compression and reliable transmission over noisy channels. Since its inception, information theory has attracted a tremendous amount of research effort and provided lots   2  Introduction  of inspiring insights into many research ﬁelds, not only communication and signal processing in electrical engineering, but also statistics, physics, com- puter science, economics, biology, etc.  Coding theory is mainly concerned with explicit methods for efﬁcient and reliable data transmission or storage, which can be roughly divided into data compression and error-control techniques. Of the two, the former attempts to compress the data from a source in order to transmit or store them more efﬁ- ciently. This practice is found every day on the Internet where data are usually transformed into the ZIP format to make ﬁles smaller and reduce the network load.  The latter adds extra data bits to make the transmission of data more robust to channel disturbances. Although people may not be aware of its existence in many applications, its impact has been crucial to the development of the Inter- net, the popularity of compact discs  CD , the feasibility of mobile phones, the success of the deep space missions, etc.  Logically speaking, coding theory leads to information theory, and informa- tion theory provides the performance limits on what can be done by suitable encoding of the information. Thus the two theories are intimately related, al- though in the past they have been developed to a great extent quite separately. One of the main purposes of this book is to show their mutual relationships.  1.2 Model and basic operations of information  processing systems  Communication and storage systems can be regarded as examples of informa- tion processing systems and may be represented abstractly by the block dia- gram in Figure 1.1. In all cases, there is a source from which the information originates. The information source may be many things; for example, a book, music, or video are all information sources in daily life.  Figure 1.1 Basic information processing system.  The source output is processed by an encoder to facilitate the transmission  or storage  of the information. In communication systems, this function is often called a transmitter, while in storage systems we usually speak of a  Information  source  Encoder  Channel  Decoder   1.2 Model and basic operations  3  recorder. In general, three basic operations can be executed in the encoder: source coding, channel coding, and modulation. For source coding, the en- coder maps the source output into digital format. The mapping is one-to-one, and the objective is to eliminate or reduce the redundancy, i.e. that part of the data which can be removed from the source output without harm to the infor- mation to be transmitted. So, source coding provides an efﬁcient representation of the source output. For channel coding, the encoder introduces extra redun- dant data in a prescribed fashion so as to combat the noisy environment in which the information must be transmitted or stored. Discrete symbols may not be suitable for transmission over a physical channel or recording on a digi- tal storage medium. Therefore, we need proper modulation to convert the data after source and channel coding to waveforms that are suitable for transmission or recording.  The output of the encoder is then transmitted through some physical com- munication channel  in the case of a communication system  or stored in some physical storage medium  in the case of a storage system . As examples of the former we mention wireless radio transmission based on electromagnetic waves, telephone communication through copper cables, and wired high-speed transmission through ﬁber optic cables. As examples of the latter we indicate magnetic storage media, such as those used by a magnetic tape, a hard-drive, or a ﬂoppy disk drive, and optical storage disks, such as a CD-ROM1 or a DVD.2 Each of these examples is subject to various types of noise disturbances. On a telephone line, the disturbance may come from thermal noise, switching noise, or crosstalk from other lines. On magnetic disks, surface defects and dust par- ticles are regarded as noise disturbances. Regardless of the explicit form of the medium, we shall refer to it as the channel.  Information conveyed through  or stored in  the channel must be recovered at the destination and processed to restore its original form. This is the task of the decoder. In the case of a communication system, this device is often referred to as the receiver. In the case of a storage system, this block is often called the playback system. The signal processing performed by the decoder can be viewed as the inverse of the function performed by the encoder. The output of the decoder is then presented to the ﬁnal user, which we call the information sink.  The physical channel usually produces a received signal which differs from the original input signal. This is because of signal distortion and noise intro- duced by the channel. Consequently, the decoder can only produce an estimate  1 CD-ROM stands for compact disc read-only memory. 2 DVD stands for digital video disc or digital versatile disc.   4  Introduction  of the original information message. All well-designed systems aim at repro- ducing as reliably as possible while sending as much information as possible per unit time  for communication systems  or per unit storage  for storage sys- tems .  1.3 Information source  Nature usually supplies information in continuous forms like, e.g., a beauti- ful mountain scene or the lovely chirping of birds. However, digital signals in which both amplitude and time take on discrete values are preferred in modern communication systems. Part of the reason for this use of digital signals is that they can be transmitted more reliably than analog signals. When the inevitable corruption of the transmission system begins to degrade the signal, the digital pulses can be detected, reshaped, and ampliﬁed to standard form before relay- ing them to their ﬁnal destination. Figure 1.2 illustrates an ideal binary digital pulse propagating along a transmission line, where the pulse shape is degraded as a function of line length. At a propagation distance where the transmitted pulse can still be reliably identiﬁed  before it is degraded to an ambiguous state , the pulse is ampliﬁed by a digital ampliﬁer that recovers its original ideal shape. The pulse is thus regenerated. On the other hand, analog signals cannot be so reshaped since they take an inﬁnite variety of shapes. Hence the farther the signal is sent and the more it is processed, the more degradation it suffers from small errors.  Figure 1.2 Pulse degradation and regeneration.  Modern practice for transforming analog signals into digital form is to sam- ple the continuous signal at equally spaced intervals of time, and then to quan- tize the observed value, i.e. each sample value is approximated by the nearest  Original signal  Regenerated signal  Propagation distance   1.4 Encoding a source alphabet  5  level in a ﬁnite set of discrete levels. By mapping each quantized sample to a codeword consisting of a prescribed number of code elements, the information is then sent as a stream of digits. The conversion process is illustrated in Fig- ure 1.3. Figure 1.3 a  shows a segment of an analog waveform. Figure 1.3 b  shows the corresponding digital waveform based on the binary code in Ta- ble 1.1. In this example, symbols 0 and 1 of the binary code are represented by zero and one volt, respectively. Each sampled value is quantized into four bi- nary digits  bits  with the last bit called sign bit indicating whether the sample value is positive or negative. The remaining three bits are chosen to represent the absolute value of a sample in accordance with Table 1.1.  Table 1.1 Binary representation of quantized levels  Index of  Binary  quantization level  representation  Index expressed as sum of powers of 2  0 1 2 3 4 5 6 7  000 001 010 011 100 101 110 111  20  21 21 + 20  + 20  22 22 22 + 21 22 + 21 + 20  As a result of the sampling and quantizing operations, errors are introduced into the digital signal. These errors are nonreversible in that it is not possible to produce an exact replica of the original analog signal from its digital represen- tation. However, the errors are under the designer’s control. Indeed, by proper selection of the sampling rate and number of the quantization levels, the errors due to the sampling and quantizing can be made so small that the difference between the analog signal and its digital reconstruction is not discernible by a human observer.  1.4 Encoding a source alphabet  Based on the discussion in Section 1.3, we can assume without loss of gener- ality that an information source generates a ﬁnite  but possibly large  number of messages. This is undoubtedly true for a digital source. As for an analog   6  Introduction   a    b   Figure 1.3  a  Analog waveform.  b  Digital representation.  Voltage  5  2  1  −2  −6  1.0 0.0  1  1  000  1  0  0  1  0  11  0  1  0  Sign bit  negative   Sign bit  negative   Sign bit  positive   Sign bit  positive    1.4 Encoding a source alphabet  7  source, the analog-to-digital conversion process mentioned above also makes the assumption feasible. However, even though speciﬁc messages are actually sent, the system designer has no idea in advance which message will be chosen for transmission. We thus need to think of the source as a random  or stochas- tic  source of information, and ask how we may encode, transmit, and recover the original information.  An information source’s output alphabet is deﬁned as the collection of all possible messages. Denote by U a source alphabet which consists of r mes- sages, say u1,u2, . . . ,ur, with probabilities p1, p2, . . . , pr satisfying  pi ≥ 0, ∀i, and  r  ∑  i=1  pi = 1.   1.1   Here the notation ∀ means “for all” or “for every.” We can always represent each message by a sequence of bits, which provides for easier processing by computer systems. For instance, if we toss a fair dice to see which number faces up, only six possible outputs are available with U = {1,2,3,4,5,6} and pi = 1 6, ∀ 1≤ i≤ 6. The following shows a straightforward binary description of these messages:  1 ↔ 001, 2 ↔ 010, 3 ↔ 011, 4 ↔ 100, 5 ↔ 101, 6 ↔ 110,   1.2   where each decimal number is encoded as its binary expression. Obviously, there exist many other ways of encoding. For example, consider the two map- pings listed below:  1 ↔ 00, 2 ↔ 01, 3 ↔ 100, 4 ↔ 101, 5 ↔ 110, 6 ↔ 111   1.3   and  1 ↔ 1100, 2 ↔ 1010, 3 ↔ 0110, 4 ↔ 1001, 5 ↔ 0101, 6 ↔ 0011.   1.4   Note that all the messages are one-to-one mapped to the binary sequences, no matter which of the above encoding methods is employed. The original message can always be recovered from the binary sequence.  Given an encoding method, let li denote the length of the output sequence, called the codeword, corresponding to ui, ∀ 1 ≤ i ≤ r. From the viewpoint of source coding for data compression, an optimal encoding should minimize the average length of codewords deﬁned by Lav  cid:44  r ∑   1.5   pili.  i=1   8  Introduction  By  1.5 , the average lengths of codewords in  1.2 ,  1.3 , and  1.4  are, re- spectively,  L 1.2  av =  L 1.3  av =  L 1.4  av =  1 6 1 6 1 6  3 +  3 +  3 +  3 +  3 +  2 +  2 +  3 +  3 +  3 +  3 = 3, 8 3  cid:39  2.667,  3 =  4 +  4 +  4 +  4 +  4 +  4 = 4.  1 6 1 6 1 6  1 6 1 6 1 6  1 6 1 6 1 6  1 6 1 6 1 6  1 6 1 6 1 6   1.6    1.7    1.8   The encoding method in  1.3  thus provides a more efﬁcient way for the rep- resentation of these source messages.  As for channel coding, a good encoding method should be able to protect the source messages against the inevitable noise corruption. Suppose 3 is to be transmitted and an error occurs in the least signiﬁcant bit  LSB , namely the ﬁrst bit counted from the right-hand side of the associated codeword. In the case of code  1.2  we now receive 010 instead of 011, and in the case of code  1.3  we receive 101 instead of 100. In both cases, the decoder will retrieve a wrong message  2 and 4, respectively . However, 0111 will be received if 3 is encoded by  1.4 . Since 0111 is different from all the codewords in  1.4 , we can be aware of the occurrence of an error, i.e. the error is detected, and possible retransmission of the message can be requested. Not just the error in the LSB, but any single error can be detected by this encoding method. The code  1.4  is therefore a better choice from the viewpoint of channel coding.  Typically, for channel coding, the encoding of the message to be transmitted over the channel adds redundancy to combat channel noise. On the other hand, the source encoding usually removes redundancy contained in the message to be compressed. A more detailed discussion on channel and source coding will be shown in Chapters 2 and 3 and in Chapters 4 and 5, respectively.  1.5 Octal and hexadecimal codes  Although the messages of an information source are usually encoded as bi- nary sequences, the binary code is sometimes inconvenient for humans to use. People usually prefer to make a single discrimination among many things. Ev- idence for this is the size of the common alphabets. For example, the English alphabet has 26 letters, the Chinese “alphabet”  bopomofo  has 37 letters, the Phoenician alphabet has 22 letters, the Greek alphabet has 24 letters, the Rus- sian alphabet 33, the Cyrillic alphabet has 44 letters, etc. Thus, for human use, it is often convenient to group the bits into groups of three at a time and call them the octal code  base 8 . This code is given in Table 1.2.   1.6 Outline of the book  9  Table 1.2 Octal code  Binary  Octal  000 001 010 011 100 101 110 111  0 1 2 3 4 5 6 7  When using the octal representation, numbers are often enclosed in paren- theses with a following subscript 8. For example, the decimal number 81 is written in octal as  121 8 since 81 = “1”×82 +“2”×81 +“1”×80. The trans- lation from octal to binary is so immediate that there is little trouble in going either way.  The binary digits are sometimes grouped in fours to make the hexadecimal code  Table 1.3 . For instance, to translate the binary sequence 101011000111 to the octal form, we ﬁrst partition these bits into groups of three:  101   cid:124  cid:123  cid:122  cid:125   011   cid:124  cid:123  cid:122  cid:125   000   cid:124  cid:123  cid:122  cid:125   111   cid:124  cid:123  cid:122  cid:125  .  1010   cid:124  cid:123  cid:122  cid:125   1100   cid:124  cid:123  cid:122  cid:125   0111   cid:124  cid:123  cid:122  cid:125 ,  Each group of bits is then mapped to an octal number by Table 1.2, hence resulting in the octal representation  5307 8. If we partition the bits into groups of four, i.e.  we can get the hexadecimal representation  AC7 16 by Table 1.3. Since com- puters usually work in bytes, which are 8 bits each, the hexadecimal code ﬁts into the machine architecture better than the octal code. However, the octal code seems to ﬁt better into the human’s psychology. Thus, in practice, neither code has a clear victory over the other.   1.9    1.10   1.6 Outline of the book  After the introduction of the above main topics, we now have a basis for dis- cussing the material the book is to cover.   10  Introduction  Table 1.3 Hexadecimal code  Binary  Hexadecimal  0000 0001 0010 0011 0100 0101 0110 0111 1000 1001 1010 1011 1100 1101 1110 1111  0 1 2 3 4 5 6 7 8 9 A B C D E F  In general, the error-detecting capability will be accomplished by adding some digits to the message, thus making the message slightly longer. The main problem is to achieve a required protection against the inevitable channel er- rors without too much cost in adding extra digits. Chapter 2 will look at ways of encoding source symbols so that any errors, up to a given level, may be detected at the terminal end. For a detected error, we might call for a repeat transmission of the message, hoping to get it right the next time.  In contrast to error-detecting codes, error-correcting codes are able to cor- rect some detected errors directly without having to retransmit the message a second time. In Chapter 3, we will discuss two kinds of error-correcting codes, the repetition code and the Hamming code, as well as their encoding and de- coding methods.  In Chapter 4, we consider ways of representing information in an efﬁcient way. The typical example will be an information source that can take on r different possible values. We will represent each of these r values by a string of 0s and 1s with varying length. The question is how to design these strings such that the average length is minimized, but such that we are still able to recover the original data from it. So, in contrast to Chapters 2 and 3, here we try to shorten the codewords.   References  11  While in Chapters 2 to 4 we are concerned with coding theory, Chapter 5 in- troduces information theory. We deﬁne some way of measuring “information” and then apply it to the codes introduced in Chapter 4. By doing so we can not only compare different codes but also derive some fundamental limits of what is possible and what not. So Chapter 5 provides the information theory related to the coding theory introduced in Chapter 4.  In Chapter 6, we continue on the path of information theory and develop the relation to the coding theory of Chapters 2 and 3. Prior to the mid 1940s people believed that transmitted data subject to noise corruption can never be perfectly recovered unless the transmission rate approaches zero. Shannon’s landmark work in 1948 [Sha48] disproved this thinking and established a fundamental result for modern communication: as long as the transmission rate is below a certain threshold  the so-called channel capacity , errorless data transmis- sion can be realized by some properly designed coding scheme. Chapter 6 will highlight the essentials regarding the channel capacity. We shall ﬁrst introduce a communication channel model from the general probabilistic setting. Based on the results of Chapter 5, we then go on to specify the mutual information, which provides a natural way of characterizing the channel capacity.  In Chapter 7, we build further on the ideas introduced in Chapters 2, 3, and 6. We will cover the basic concept of the theory of reliable transmission of in- formation bearing signals over a noisy communication channel. In particular, we will discuss the additive white Gaussian noise  AWGN  channel and intro- duce the famous turbo code that is the ﬁrst code that can approach the Shannon limit of the AWGN channel up to less than 1 dB at a bit error rate  BER  of 10−5.  Finally, in Chapter 8, we try to broaden the view by showing two relations of coding theory to quite unexpected ﬁelds. Firstly we explain a connection of projective geometry to the Hamming code of Chapter 3. Secondly we show how codes  in particular the three-times repetition code and the Hamming code  can be applied to game theory.  References  [Sha48] Claude E. Shannon, “A mathematical theory of communication,” Bell System Technical Journal, vol. 27, pp. 379–423 and 623–656, July and October 1948. Available: http:  moser.cm.nctu.edu.tw nctu doc shannon1948.pdf    2  Error-detecting codes  When a message is transmitted, the inevitable noise disturbance usually de- grades the quality of communication. Whenever repetition is possible, it is sufﬁcient to detect the occurrence of an error. When an error is detected, we simply repeat the message, and it may be correct the second time or even pos- sibly the third time.  It is not possible to detect an error if every possible symbol, or set of sym- bols, that can be received is a legitimate message. It is only possible to catch errors if there are some restrictions on what a proper message is. The prob- lem is to keep these restrictions on the possible messages down to ones that are simple. In practice, “simple” means “easily computable.” In this chapter, we will mainly investigate the problem of designing codes such that any sin- gle error can be detected at the receiver. In Chapter 3, we will then consider correcting the errors that occur during the transmission.  2.1 Review of modular arithmetic  We ﬁrst give a quick review of the basic arithmetic which is extensively used in the following sections. For binary digits, which take values of only 0 and 1, the rules for addition and multiplication are deﬁned by 0× 0 = 0 0× 1 = 0 1× 0 = 0 1× 1 = 1,  0 + 0 = 0 0 + 1 = 1 1 + 0 = 1 1 + 1 = 0   2.1   and  respectively. For example, by  2.1 , we have  1 + 1× 0 + 0 + 1× 1 = 1 + 0 + 0 + 1 = 0.   2.2    14  Error-detecting codes  If we choose to work in the decimal arithmetic, the binary arithmetic in  2.1  can be obtained by dividing the result in decimal by 2 and taking the remainder. For example,  2.2  yields  1 + 0 + 0 + 1 = 2 ≡ 0 mod 2.   2.3   Occasionally, we may work modulo some number other than 2 for the case of a nonbinary source. Given a positive integer m, for the addition and multi- plication mod m  “mod” is an abbreviation for “modulo” , we merely divide the result in decimal by m and take the nonnegative remainder. For instance, consider an information source with ﬁve distinct outputs 0, 1, 2, 3, 4. It follows that  2 + 4 = 1× 5 + “1” 3× 4 = 2× 5 + “2”  ⇐⇒ ⇐⇒  2 + 4 ≡ 1 mod 5, 3× 4 ≡ 2 mod 5.   2.4   2.5   Other cases for the modulo 5 addition and multiplication can be referred to in Table 2.1.  Table 2.1 Addition and multiplication modulo 5  + mod 5  0 1 2 3 4  0  0 1 2 3 4  1  1 2 3 4 0  2  2 3 4 0 1  3  3 4 0 1 2  4  4 0 1 2 3  × mod 5  0 1 2 3 4  0  0 0 0 0 0  1  0 1 2 3 4  2  0 2 4 1 3  3  0 3 1 4 2  4  0 4 3 2 1  For multiplication mod m, we have to be more careful if m is not a prime. Suppose that we have the numbers a and b congruent to a cid:48  and b cid:48  modulo the modulus m. This means that  a ≡ a cid:48  mod m and b ≡ b cid:48  mod m  a = a cid:48  + k1m and b = b cid:48  + k2m for some integers k1 and k2. For the product ab, we have ab = a cid:48 b cid:48  + a cid:48 k1m + b cid:48 k2m + k1k2m2  or  and hence  ab ≡ a cid:48 b cid:48  mod m.   2.6    2.7    2.8    2.9    2.2 Independent errors – white noise  15  Now consider the particular case  a = 15,  b = 12, m = 10.   2.10  We have a cid:48  = 5 and b cid:48  = 2 by  2.7  and ab ≡ a cid:48 b cid:48  ≡ 0 mod 10 by  2.9 . But neither a nor b is zero! Only for a prime modulus do we have the important property that if a product is zero, then at least one factor must be zero. Exercise 2.1 check out the following problems:  In order to become more familiar with the modular operation  and  3× 6 + 7 ≡ ? mod 11  5− 4× 2 ≡ ? mod 7.  More on modular arithmetic can be found in Section 3.1.   2.11    2.12  ♦  2.2 Independent errors – white noise  To simplify the analysis of noise behavior, we assume that errors in a message satisfy the following constraints:   1  the probability of an error in any binary position is assumed to be a ﬁxed  number p, and   2  errors in different positions are assumed to be independent.1  Such noise is called “white noise” in analogy with white light, which is sup- posed to contain uniformly all the frequencies detected by the human eye. However, in practice, there are often reasons for errors to be more common in some positions in the message than in others, and it is often true that errors tend to occur in bursts and not to be independent.We assume white noise in the very beginning because this is the simplest case, and it is better to start from the simplest case and move on to more complex situations after we have built up a solid knowledge on the simple case.  Consider a message consisting of n digits for transmission. For white noise,  the probability of no error in any position is given by   1− p n.   2.13   1 Given events A cid:96 , they are said to be independent if Pr   cid:84 n  cid:96 =1 Pr  A cid:96  . Here “ cid:84  cid:96 ” denotes set-intersection, i.e. cid:84  cid:96  A cid:96  is the set of elements that are members of all sets A cid:96 . Hence, Pr  cid:84 n   cid:96 =1 A cid:96   is the event that all events A cid:96  occur at the same time. The notation ∏ cid:96  is a short-  hand for multiplication: ∏n   cid:96 =1 A cid:96   = ∏n   cid:96 =1 a cid:96   cid:44  a1 · a2 ···an.   16  Error-detecting codes  The probability of a single error in the message is given by  np 1− p n−1.  The probability of  cid:96  errors is given by the   cid:96  + 1 th term in the binomial ex- pansion:  1 cid:19 p 1− p n−1 + cid:18 n  2 cid:19 p2 1− p n−2  1 = cid:0  1− p  + p cid:1 n 0 cid:19  1− p n + cid:18 n = cid:18 n n cid:19 pn +··· + cid:18 n =  1− p n + np 1− p n−1 +  For example, the probability of exactly two errors is given by  n n− 1   2  p2 1− p n−2 +··· + pn.  n n− 1   2  p2 1− p n−2.  We can obtain the probability of an even number of errors  0,2,4, . . .  by  adding the following two binomial expansions and dividing by 2:  1 = cid:0  1− p  + p cid:1 n  1− 2p n = cid:0  1− p − p cid:1 n  =  =  Denote by  cid:98 ξ cid:99  the greatest integer not larger than ξ . We have2  Pr An even number of errors  =  n  n   cid:96 =0   cid:96 =0 cid:18 n  cid:96  cid:19 p cid:96  1− p n− cid:96 , ∑  −1  cid:96  cid:18 n ∑  cid:96 =0 cid:18  n  cid:98 n 2 cid:99 ∑ 1 +  1− 2p n   cid:96  cid:19 p cid:96  1− p n− cid:96 . 2 cid:96  cid:19 p2 cid:96  1− p n−2 cid:96   =  .  2  The probability of an odd number of errors is 1 minus this number. Exercise 2.2 Actually, this is a good chance to practice your basic skills on the method of induction: can you show that  and  2 cid:96  cid:19 p2 cid:96  1− p n−2 cid:96  =  cid:96 =0 cid:18  n  cid:98 n 2 cid:99 ∑ 2 cid:96  + 1 cid:19 p2 cid:96 +1 1− p n−2 cid:96 −1 =  cid:96 =0  cid:18  n  cid:98  n−1  2 cid:99 ∑  1 +  1− 2p n  2  1−  1− 2p n  2  2 Note that zero errors also counts as an even number of errors here.   2.14    2.15    2.16    2.17    2.18    2.19    2.20    2.21    2.22    2.23    2.24     2.25   ♦   2.26    2.27   2.3 Single parity-check code  17  by induction on n? Hint: Note that  for n,k ≥ 1.   cid:18 n + 1 k  cid:19  = cid:18 n  k cid:19  + cid:18  n k− 1 cid:19   2.3 Single parity-check code  The simplest way of encoding a binary message to make it error-detectable is to count the number of 1s in the message, and then append a ﬁnal binary digit chosen so that the entire message has an even number of 1s in it. The entire message is therefore of even parity. Thus to  n− 1  message positions we append an nth parity-check position. Denote by x cid:96  the original bit in the  cid:96 th message position, ∀1 ≤  cid:96  ≤ n− 1, and let xn be the parity-check bit. The constraint of even parity implies that  xn =  n−1 ∑  cid:96 =1  x cid:96   n  ∑   cid:96 =1  y cid:96   cid:54 = 0,  by  2.1 . Note that here  and for the remainder of this book  we omit “mod 2” and implicitly assume it everywhere. Let y cid:96  be the channel output correspond- ing to x cid:96 , ∀1 ≤  cid:96  ≤ n. At the receiver, we ﬁrstly count the number of 1s in the received sequence y. If the even-parity constraint is violated for the received vector, i.e.  this indicates that at least one error has occurred.  For example, given a message  x1,x2,x3,x4  =  0111 , the parity-check bit  is obtained by  x5 = 0 + 1 + 1 + 1 = 1,   2.28   and hence the resulting even-parity codeword  x1,x2,x3,x4,x5  is  01111 . Suppose the codeword is transmitted, but a vector y =  00111  is received. In this case, an error in the second position is met. We have  y1 + y2 + y3 + y4 + y5 = 0 + 0 + 1 + 1 + 1 = 1   cid:54 = 0 ;   2.29   thereby the error is detected. However, if another vector of  00110  is re- ceived, where two errors  in the second and the last position  have occurred,   18  Error-detecting codes  no error will be detected since  y1 + y2 + y3 + y4 + y5 = 0 + 0 + 1 + 1 + 0 = 0.   2.30   Evidently in this code any odd number of errors can be detected. But any even number of errors cannot be detected.  For channels with white noise,  2.22  gives the probability of any even num- ber of errors in the message. Dropping the ﬁrst term of  2.21 , which corre- sponds to the probability of no error, we have the following probability of undetectable errors for the single parity-check code introduced here:  Pr Undetectable errors  =   cid:96 =1 cid:18  n  cid:98 n 2 cid:99 ∑ 1 +  1− 2p n  2 cid:96  cid:19 p2 cid:96  1− p n−2 cid:96  −  1− p n.  2  =   2.31    2.32   The probability of detectable errors, i.e. all the odd-number errors, is then ob- tained by  Pr Detectable errors  = 1−  2  1 +  1− 2p n  1−  1− 2p n  .  =   2.33   2  Obviously, we should have that  Pr Detectable errors   cid:29  Pr Undetectable errors  .  For p very small, we have  Pr Undetectable errors  =  =  =  2  1  1 +  1− 2p n −  1− p n 2 cid:19  2p 2 −··· cid:21  1 cid:19  2p  + cid:18 n 0 cid:19 − cid:18 n 2 cid:20  cid:18 n 1 2 + 2 cid:19 p2 −··· cid:21  1 cid:19 p + cid:18 n 0 cid:19 − cid:18 n − cid:20  cid:18 n 4p2 −··· cid:21  2 cid:20 1− 2np + n n− 1  p2 −··· cid:21  − cid:20 1− np + n n− 1  n n− 1   1 2 +  p2  2  1  2   cid:39   2  2  1−  1− 2p n 2 cid:20  cid:18 n 1 2 −  1  =  0 cid:19 − cid:18 n  1 cid:19  2p  +··· cid:21    2.34    2.35    2.36    2.37    2.38    2.39    2.40   and  Pr Detectable errors  =   In the above approximations, we only retain the leading term that dominates the sum.  Hence,  2.34  requires  and implies that the shorter the message, the better the detecting performance. In practice, it is common to break up a long message in the binary alphabet into blocks of  n− 1  digits and to append one binary digit to each block. This produces the redundancy of  2.4 The ASCII code  1 2 [1− 2np +··· ]  1 2 − =  cid:39  np.  np  cid:29   n n− 1   p2,  2  n  n− 1 = 1 +  1 n− 1 ,  19   2.41   2.42    2.43    2.44   where the redundancy is deﬁned as the total number of binary digits divided by the minimum necessary. The excess redundancy is 1  n− 1 . Clearly, for low redundancy we want to use long messages, but for high reliability short messages are better. Thus the choice of the length n for the blocks to be sent is a compromise between the two opposing forces.  2.4 The ASCII code  Here we introduce an example of a single parity-check code, called the Amer- ican Standard Code for Information Interchange  ASCII , which was the ﬁrst code developed speciﬁcally for computer communications. Each character in ASCII is represented by seven data bits constituting a unique binary sequence. Thus a total of 128  = 27  different characters can be represented in ASCII. The characters are various commonly used letters, numbers, special control symbols, and punctuation symbols, e.g. $, %, and @. Some of the special con- trol symbols, e.g. ENQ  enquiry  and ETB  end of transmission block , are used for communication purposes. Other symbols, e.g. BS  back space  and CR  carriage return , are used to control the printing of characters on a page. A complete listing of ASCII characters is given in Table 2.2.  Since computers work in bytes which are blocks of 8 bits, a single ASCII symbol often uses 8 bits. The eighth bit is set so that the total number of 1s in the eight positions is an even number. For example, consider “K” in Table 2.2 encoded as  113 8, which can be transformed into binary form as follows:   113 8 = 1001011   2.45    20  Error-detecting codes  Table 2.2 Seven-bit ASCII code  Octal Char. code  Octal Char. code  Char.  Octal code  Octal Char. code  000 001 002 003 004 005 006 007 010 011 012 013 014 015 016 017 020 021 022 023 024 025 026 027 030 031 032 033 034 035 036 037  NUL SOH STX ETX EOT ENQ ACK BEL BS HT LF VT FF CR SO SI DLE DC1 DC2 DC3 DC4 NAK SYN ETB CAN EM SUB ESC FS GS RS US  040 041 042 043 044 045 046 047 050 051 052 053 054 055 056 057 060 061 062 063 064 065 066 067 070 071 072 073 074 075 076 077  SP ! ”  $ % & ’     * + , - .   0 1 2 3 4 5 6 7 8 9 : ; < = > ?  @ A B C D E F G H I J K L M N O P Q R S T U V W X Y Z [ \ ] ˆ  100 101 102 103 104 105 106 107 110 111 112 113 114 115 116 117 120 121 122 123 124 125 126 127 130 131 132 133 134 135 136 137  140 141 142 143 144 145 146 147 150 151 152 153 154 155 156 157 160 161 162 163 164 165 166 167 170 171 172 173 174 175 176 177  ‘ a b c d e f g h i j k l m n o p q r s t u v w x y z {  } ˜  DEL   2.5 Simple burst error-detecting code  21   where we have dropped the ﬁrst 2 bits of the ﬁrst octal symbol . In this case, the parity-check bit is 0; “K” is thus encoded as 10010110 for even parity. You are encouraged to encode the remaining characters in Table 2.2.  By the constraint of even parity, any single error, a 0 changed into a 1 or a 1 changed into a 0, will be detected3 since after the change there will be an odd number of 1s in the eight positions. Thus, we have an error-detecting code that helps to combat channel noise. Perhaps more importantly, the code makes it much easier to maintain the communication quality since the machine can detect the occurrence of errors by itself.  2.5 Simple burst error-detecting code  In some situations, errors occur in bursts rather than in isolated positions in the received message. For instance, lightning strikes, power-supply ﬂuctuations, loose ﬂakes on a magnetic surface are all typical causes of a burst of noise. Suppose that the maximum length of any error burst4 that we are to detect is L. To protect data against the burst errors, we ﬁrst divide the original message into a sequence of words consisting of L bits. Aided with a pre-selected error- detecting code, parity checks are then computed over the corresponding word positions, instead of the bit positions.  Based on the above scenario, if an error burst occurs within one word, in effect only a single word error is observed. If an error burst covers the end of one word and the beginning of another, still no two errors corresponding to the same position of words will be met, since we assumed that any burst length l satisﬁes 0 ≤ l ≤ L. Consider the following example for illustration. Example 2.3  If the message is  and the maximum burst error length L is 8, we can use the 7-bit ASCII code in Table 2.2 and protect the message against burst errors as shown in Table 2.3.  Here no parity check is used within the ASCII symbols.  The encoded mes- sage is therefore  Hello NCTU  Hello NCTUn  3 Actually, to be precise, every odd number of errors is detected. 4 An error burst is said to have length L if errors are conﬁned to L consecutive positions. By this deﬁnition, the error patterns 0111110, 0101010, and 0100010 are all classiﬁed as bursts of length 5. Note that a 0 in an error pattern denotes that no error has happened in that position, while a 1 denotes an error. See also  3.34  in Section 3.3.2.   22  Error-detecting codes  Table 2.3 Special type of parity check to protect against burst errors of  maximum length L = 8  H =  110 8 = e =  145 8 = l =  154 8 = l =  154 8 = o =  157 8 = SP=  040 8 = N =  116 8 = C =  103 8 = T =  124 8 = U =  125 8 = Check sum =  01001000 01100101 01101100 01101100 01101111 00100000 01001110 01000011 01010100 01010101 01101110 = n  where n is the parity-check symbol.  Suppose a burst error of length 5, as shown in Table 2.4, is met during the transmission of the above message, where the bold-face positions are in error. In this case, the burst error is successfully detected since the check sum is not 00000000. However, if the burst error of length 16 shown in Table 2.5 occurs, ♦ the error will not be detected due to the all-zero check sum.  Exercise 2.4 Could you repeat the above process of encoding for the case of L = 16? Also, show that the resulting code can detect all the bursts of length ♦ at most 16.  Exercise 2.5 Can you show that the error might not be detected if there is ♦ more than one burst, even if each burst is of length no larger than L?  2.6 Alphabet plus number codes – weighted codes  The codes we have discussed so far were all designed with respect to a simple form of “white noise” that causes some bits to be ﬂipped. This is very suit- able for many types of machines. However, in some systems, where people are involved, other types of noise are more appropriate. The ﬁrst common human error is to interchange adjacent digits of numbers; for example, 38 becomes 83. A second common error is to double the wrong one of a triple of digits, where two adjacent digits are the same; for example, 338 becomes 388. In addition, the confusion of O  “oh”  and 0  “zero”  is also very common.   2.6 Alphabet plus number codes – weighted codes  23  Table 2.4 A burst error of length 5 has occurred during transmission and is detected because the check sum is not 0000000; bold-face positions denote  positions in error  H ⇒ K e ⇒ ENQ  l l o SP N C T U n  Check sum =  H ⇒ K e ⇒ J l ⇒ @  l o SP N C T U n  Check sum =  0 0 0 0 0 0 0 0 0 0 0  0  0 0 0 0 0 0 0 0 0 0 0  0  1 0 1 1 1 0 1 1 1 1 1  1  1 1 1 1 1 0 1 1 1 1 1  0  0 0 1 1 1 1 0 0 0 0 1  1  0 0 0 1 1 1 0 0 0 0 1  0  0 0 0 0 0 0 0 0 1 1 0  0  0 0 0 0 0 0 0 0 1 1 0  0  1 0 1 1 1 0 1 0 0 0 1  0  1 1 0 1 1 0 1 0 0 0 1  0  0 1 1 1 1 0 1 0 1 1 1  0  0 0 0 1 1 0 1 0 1 1 1  0  1 0 0 0 1 0 1 1 0 0 1  1  1 1 0 0 1 0 1 1 0 0 1  0  1 1 0 0 1 0 0 1 0 1 0  1  1 0 0 0 1 0 0 1 0 1 0  0  Table 2.5 A burst error of length 16 has occurred during transmission, but it  is not detected; bold-face positions denote positions in error   24  Error-detecting codes  Table 2.6 Weighted sum: progressive digiting  Message  w x y z  Sum  w  w + x  Sum of sum  w  2w + x  w + x + y  3w + 2x + y  w + x + y + z  4w + 3x + 2y + z  In English text-based systems, it is quite common to have a source alphabet consisting of the 26 letters, space, and the 10 decimal digits. Since the size of this source alphabet, 37  = 26 + 1 + 10 , is a prime number, we can use the following method to detect the presence of the above described typical errors. Firstly, each symbol in the source alphabet is mapped to a distinct number in {0,1,2, . . . ,36}. Given a message for encoding, we weight the symbols with weights 1,2,3, . . ., beginning with the check digit of the message. Then, the weighted digits are summed together and reduced to the remainder after divid- ing by 37. Finally, a check symbol is selected such that the sum of the check symbol and the remainder obtained above is congruent to 0 modulo 37.  To calculate this sum of weighted digits easily, a technique called progres- sive digiting, illustrated in Table 2.6, has been developed. In Table 2.6, it is supposed that we want to compute the weighted sum for a message wxyz, i.e. 4w + 3x + 2y + 1z. For each symbol in the message, we ﬁrst compute the run- ning sum from w to the symbol in question, thereby obtaining the second col- umn in Table 2.6. We can sum these sums again in the same way to obtain the desired weighted sum.  Example 2.6 We assign a distinct number from {0,1,2, . . . ,36} to each symbol in the combined alphabet number set in the following way: “0” = 0, “1” = 1, “2” = 2, . . . , “9” = 9, “A” = 10, “B” = 11, “C” = 12, . . . , “Z” = 35, and “space” = 36. Then we encode  We proceed with the progressive digiting as shown in Table 2.7 and obtain a weighted sum of 183. Since 183 mod 37 = 35 and 35 + 2 is divisible by 37, it follows that the appended check digit should be  3B 8.  “2” = 2.   2.6 Alphabet plus number codes – weighted codes  25  Table 2.7 Progressive digiting for the example of “3B 8”: we need to add “2” = 2 as a check-digit to make sure that the weighted sum divides 37  Sum Sum of sum  “3” = 3 “B” = 11 “space” = 36 “8” = 8 Check-digit = ??  3 14 50 58 58  3 17 67 125 183  37     4 183 148 35  Table 2.8 Checking the encoded message “3B 82”  “space”  3 B  8 2  3× 5 = 15 11× 4 = 44 36× 3 = 108 8× 2 = 16 2× 1 = 2 Sum = 185 = 37× 5 ≡ 0 mod 37  The encoded message is therefore given by  3B 82.  To check whether this is a legitimate message at the receiver, we proceed as shown in Table 2.8.  Now suppose “space” is lost during the transmission such that only “3B82” is received. Such an error can be detected since the weighted sum is now not congruent to 0 mod 37; see Table 2.9. Similarly, the interchange from “82” to  Table 2.9 Checking the corrupted message “3B82”  3× 4 = 12 3 B 11× 3 = 33 8 8× 2 = 16 2× 1 = 2 2 Sum = 63   cid:54 ≡ 0 mod 37   26  Error-detecting codes  Table 2.10 Checking the corrupted message “3B 28”  “space”  3 B  2 8  3× 5 = 15 11× 4 = 44 36× 3 = 108 2× 2 = 4 8× 1 = 8 Sum = 179   cid:54 ≡ 0 mod 37  “28” can also be detected; see Table 2.10.  ♦  In the following we give another two examples of error-detecting codes that  are based on modular arithmetic and are widely used in daily commerce.  Example 2.7 The International Standard Book Number  ISBN  is usually a 10-digit code used to identify a book uniquely. A typical example of the ISBN is as follows:  0  country   cid:124  cid:123  cid:122  cid:125   ID  – 52  18 – 4868  – 7  publisher   cid:124  cid:123  cid:122  cid:125   ID   cid:124   book number   cid:123  cid:122    cid:125   check digit   cid:124  cid:123  cid:122  cid:125   where the hyphens do not matter and may appear in different positions. The ﬁrst digit stands for the country, with 0 meaning the United States and some other English-speaking countries. The next two digits are the publisher ID; here 52 means Cambridge University Press. The next six digits, 18 – 4868, are the publisher-assigned book number. The last digit is the weighted check sum modulo 11 and is represented by “X” if the required check digit is 10.  To conﬁrm that this number is a legitimate ISBN number we proceed as ♦  shown in Table 2.11. It checks!  Exercise 2.8 Check whether 0 – 8044 – 2957 – X is a valid ISBN number. ♦  Example 2.9 The Universal Product Code  UPC  is a 12-digit single parity- check code employed on the bar codes of most merchandise to ensure reliabil- ity in scanning. A typical example of UPC takes the form  0 36000 manufacturer   cid:124   cid:123  cid:122   cid:125   ID  29145 item   cid:124  cid:123  cid:122  cid:125   number  2  parity check   cid:124  cid:123  cid:122  cid:125    2.7 Redundancy versus error-detecting capability  27  Table 2.11 Checking the ISBN number 0 – 5218 – 4868 – 7  Sum Sum of sum  0 5 2 1 8 4 8 6 8 7  0 5 7 8 16 20 28 34 42 49  0 5 12 20 36 56 84 118 160 209 = 11× 19 ≡ 0 mod 11  where the last digit is the parity-check digit. Denote the digits as x1,x2, . . . ,x12. The parity digit x12 is determined such that  3 x1 + x3 + x5 + x7 + x9 + x11  +  x2 + x4 + x6 + x8 + x10 + x12    2.46   is a multiple5 of 10. In this case,  3 0 + 6 + 0 + 2 + 1 + 5  +  3 + 0 + 0 + 9 + 4 + 2  = 60.   2.47  ♦  2.7 Trade-off between redundancy and  error-detecting capability  As discussed in the previous sections, a single parity check to make the whole message even-parity can help the detection of any single error  or even any odd number of errors . However, if we want to detect the occurrence of more errors in a noisy channel, what can we do for the design of error-detecting codes? Can such a goal be achieved by increasing the number of parity checks, i.e. at the cost of extra redundancy? Fortunately, the answer is positive. Let us consider the following illustrative example.  5 Note that in this example the modulus 10 is used although this is not a prime. The slightly unusual summation  2.46 , however, makes sure that every single error can still be detected. The reason why UPC chooses 10 as the modulus is that the check digit should also range from 0 to 9 so that it can easily be encoded by the bar code.   28  Error-detecting codes  Example 2.10 For an information source of eight possible outputs, obviously each output can be represented by a binary 3-tuple, say  x1,x2,x3 . Suppose three parity checks x4, x5, x6 are now appended to the original message by the following equations:  to form a legitimate codeword  x1,x2,x3,x4,x5,x6 . Compared with the sin- gle parity-check code, this code increases the excess redundancy from 1 3 to 3 3. Let  y1,y2,y3,y4,y5,y6  be the received vector as  x1,x2,x3,x4,x5,x6  is transmitted. If at least one of the following parity-check equations is violated:  x4 = x1 + x2, x5 = x1 + x3, x6 = x2 + x3,  y4 = y1 + y2, y5 = y1 + y3, y6 = y2 + y3,      the occurrence of an error is detected.  For instance, consider the case of a single error in the ith position such that  yi = xi + 1 and  y cid:96  = x cid:96 , ∀  cid:96  ∈ {1,2, . . . ,6}\{i}.   2.50   It follows that  y4  cid:54 = y1 + y2, y5  cid:54 = y1 + y3 y4  cid:54 = y1 + y2, y6  cid:54 = y2 + y3 y5  cid:54 = y1 + y3, y6  cid:54 = y2 + y3 y4  cid:54 = y1 + y2 y5  cid:54 = y1 + y3 y6  cid:54 = y2 + y3  if i = 1, if i = 2, if i = 3, if i = 4, if i = 5, if i = 6.    Therefore, all single errors can be successfully detected. In addition, consider the case of a double error in the ith and jth positions, respectively, such that  yi = xi +1,  y j = x j +1,  and y cid:96  = x cid:96 , ∀  cid:96  ∈ {1,2, . . . ,6}\{i, j}.  2.52    2.48    2.49    2.51    2.7 Redundancy versus error-detecting capability  29  We then have  y5  cid:54 = y1 + y3, y6  cid:54 = y2 + y3 y4  cid:54 = y1 + y2, y6  cid:54 = y2 + y3 y5  cid:54 = y1 + y3 y4  cid:54 = y1 + y2 y4  cid:54 = y1 + y2, y5  cid:54 = y1 + y3, y6  cid:54 = y2 + y3 y4  cid:54 = y1 + y2, y5  cid:54 = y1 + y3 y6  cid:54 = y2 + y3 y4  cid:54 = y1 + y2, y5  cid:54 = y1 + y3, y6  cid:54 = y2 + y3 y4  cid:54 = y1 + y2 y4  cid:54 = y1 + y2, y5  cid:54 = y1 + y3, y6  cid:54 = y2 + y3 y6  cid:54 = y2 + y3 y5  cid:54 = y1 + y3 y4  cid:54 = y1 + y2, y5  cid:54 = y1 + y3 y4  cid:54 = y1 + y2, y6  cid:54 = y2 + y3 y5  cid:54 = y1 + y3, y6  cid:54 = y2 + y3    if  i, j  =  1,2 , if  i, j  =  1,3 , if  i, j  =  1,4 , if  i, j  =  1,5 , if  i, j  =  1,6 , if  i, j  =  2,3 , if  i, j  =  2,4 , if  i, j  =  2,5 , if  i, j  =  2,6 , if  i, j  =  3,4 , if  i, j  =  3,5 , if  i, j  =  3,6 , if  i, j  =  4,5 , if  i, j  =  4,6 , if  i, j  =  5,6 .  Hence, this code can detect any pattern of double errors.  ♦  Exercise 2.11 Unfortunately, not all triple errors may be caught by the code ♦ of Example 2.10. Can you give an example for veriﬁcation?  Without a proper design, however, increasing the number of parity checks may not always improve the error-detecting capability. For example, consider another code which appends the parity checks by   2.53    2.54   x4 = x1 + x2 + x3, x5 = x1 + x2 + x3, x6 = x1 + x2 + x3.    In this case, x5 and x6 are simply repetitions of x4. Following a similar discus- sion as in Example 2.10, we can show that all single errors are still detectable. But if the following double error occurs during the transmission: and y cid:96  = x cid:96 , ∀3 ≤  cid:96  ≤ 6,  y2 = x2 + 1,  y1 = x1 + 1,   2.55   none of the three parity-check equations corresponding to  2.54  will be vi- olated. This code thus is not double-error-detecting even though the same amount of redundancy is required as in the code  2.48 .   30  Error-detecting codes  2.8 Further reading  In this chapter simple coding schemes, e.g. single parity-check codes, burst error-detecting codes, and weighted codes, have been introduced to detect the presence of channel errors. However, there exists a class of linear block codes, called cyclic codes, which are probably the most widely used form of error- detecting codes. The popularity of cyclic codes arises primarily from the fact that these codes can be implemented with extremely cost-effective electronic circuits. The codes themselves also possess a high degree of structure and reg- ularity  which gives rise to the promising advantage mentioned above , and there is a certain beauty and elegance in the corresponding theory. Interested readers are referred to [MS77], [Wic94], and [LC04] for more details of cyclic codes.  References  [LC04] Shu Lin and Daniel J. Costello, Jr., Error Control Coding, 2nd edn. Prentice  Hall, Upper Saddle River, NJ, 2004.  [MS77] F. Jessy MacWilliams and Neil J. A. Sloane, The Theory of Error-Correcting  Codes. North-Holland, Amsterdam, 1977.  [Wic94] Stephen B. Wicker, Error Control Systems for Digital Communication and  Storage. Prentice Hall, Englewood Cliffs, NJ, 1994.   3  Repetition and Hamming codes  The theory of error-correcting codes comes from the need to protect informa- tion from corruption during transmission or storage. Take your CD or DVD as an example. Usually, you might convert your music into MP3 ﬁles1 for stor- age. The reason for such a conversion is that MP3 ﬁles are more compact and take less storage space, i.e. they use fewer binary digits  bits  compared with the original format on CD. Certainly, the price to pay for a smaller ﬁle size is that you will suffer some kind of distortion, or, equivalently, losses in audio quality or ﬁdelity. However, such loss is in general indiscernible to human au- dio perception, and you can hardly notice the subtle differences between the uncompressed and compressed audio signals. The compression of digital data streams such as audio music streams is commonly referred to as source coding. We will consider it in more detail in Chapters 4 and 5.  What we are going to discuss in this chapter is the opposite of compression. After converting the music into MP3 ﬁles, you might want to store these ﬁles on a CD or a DVD for later use. While burning the digital data onto a CD, there is a special mechanism called error control coding behind the CD burning pro- cess. Why do we need it? Well, the reason is simple. Storing CDs and DVDs in- evitably causes small scratches on the disk surface. These scratches impair the disk surface and create some kind of lens effect so that the laser reader might not be able to retrieve the original information correctly. When this happens, the stored ﬁles are corrupted and can no longer be used. Since the scratches are inevitable, it makes no sense to ask the users to keep the disks in per- fect condition, or discard them once a perfect read-out from the disk becomes impossible. Therefore, it would be better to have some kind of engineering mechanism to protect the data from being compromised by minor scratches.  1 MP3 stands for MPEG-2 audio layer 3, where MPEG is the abbreviation for moving picture  experts group.   32  Repetition and Hamming codes  We use error-correcting codes to accomplish this task. Error-correcting codes are also referred to as channel coding in general.  First of all, you should note that it is impossible to protect the stored MP3 ﬁles from impairment without increasing the ﬁle size. To see this, say you have a binary data stream s of length k bits. If the protection mechanism were not allowed to increase the length, after endowing s with some protection capa- bility, the resulting stream x is at best still of length k bits. Then the whole protection process is nothing but a mapping from a k-bit stream to another k- bit stream. Such mapping is, at its best, one-to-one and onto, i.e. a bijection, since if it were not a bijection, it would not be possible to recover the original data. On the other hand, because of the bijection, when the stored data stream x is corrupted, it is impossible to recover the original s. Therefore, we see that the protection process  henceforth we will refer to it as an encoding process  must be an injection, meaning x must have length larger than k, say n, so that when x is corrupted, there is a chance that s may be recovered by using the extra  n− k  bits we have used for storing extra information.  How to encode efﬁciently a binary stream of length k with minimum  n− k  extra bits added so that the length k stream s is well protected from corrup- tion is the major concern of error-correcting codes. In this chapter, we will brieﬂy introduce two kinds of error-correcting codes: the repetition code and the Hamming code. The repetition code, as its name suggests, simply repeats information and is the simplest error-protecting correcting scheme. The Ham- ming code, developed by Richard Hamming when he worked at Bell Labs in the late 1940s  we will come back to this story in Section 3.3.1 , on the other hand, is a bit more sophisticated than the repetition code. While the original Hamming code is actually not that much more complicated than the repeti- tion code, it turns out to be optimal in terms of sphere packing in some high- dimensional space. Speciﬁcally, this means that for certain code length and error-correction capability, the Hamming code actually achieves the maximal possible rate, or, equivalently, it requires the fewest possible extra bits.  Besides error correction and data protection, the Hamming code is also good in many other areas. Readers who wish to know more about these subjects are referred to Chapter 8, where we will brieﬂy discuss two other uses of the Hamming code. We will show in Section 8.1 how the Hamming code relates to a geometric subject called projective geometry, and in Section 8.2 how the Hamming code can be used in some mathematical games.   3.1 Arithmetics in the binary ﬁeld  33  3.1 Arithmetics in the binary ﬁeld  Prior to introducing the codes, let us ﬁrst study the arithmetics of binary oper- ations  see also Section 2.1 . These are very important because the digital data is binary, i.e. each binary digit is either of value 0 or 1, and the data will be processed in a binary fashion. By binary operations we mean binary addition, subtraction, multiplication, and division. The binary addition is a modulo-2 addition, i.e.  0 + 0 = 0, 1 + 0 = 1, 0 + 1 = 1, 1 + 1 = 0.  0 = 0− 0, 1 = 1− 0, 0 = 1− 1, 1 = 0− 1.  The only difference between binary and usual additions is the case of 1 + 1. Usual addition would say 1 + 1 = 2. But since we are working with modulo- 2 addition, meaning the sum is taken as the remainder when divided by 2, the remainder of 2 divided by 2 equals 0, hence we have 1 + 1 = 0 in binary arithmetics.  By moving the second operand to the right of these equations, we obtain  subtractions:  Further, it is interesting to note that the above equalities also hold if we replace “−” by “+”. Then we realize that, in binary, subtraction is the same as addition. This is because the remainder of −1 divided by 2 equals 1, meaning −1 is considered the same as 1 in binary. In other words,  a− b = a +  −1 × b = a +  1 × b = a + b.  Also, it should be noted that the above implies  a− b = b− a = a + b  in binary, while this is certainly false for real numbers.   3.1    3.2    3.3    3.4    34  Repetition and Hamming codes  Multiplication in binary is the same as usual, and we have  0× 0 = 0, 1× 0 = 0, 0× 1 = 0, 1× 1 = 1.   3.5   Show that the laws of association and distribution hold for  The same holds also for division. Exercise 3.1 binary arithmetics. That is, show that for any a,b,c ∈ {0,1} we have  additive associative law ,  multiplicative associative law ,  distributive law . In this chapter, we will use the notation ?  a + b + c =  a + b  + c = a +  b + c  a× b× c =  a× b × c = a×  b× c  a×  b + c  =  a× b  +  a× c  Exercise 3.2 = to denote a con- ditional equality, by which we mean that we are unsure whether the equality = 0. ♦ holds. Show that the condition of a ?  = b in binary is the same as a + b ?  ♦  3.2 Three-times repetition code  A binary digit  or bit in short  s is to be stored on CD, but it could be corrupted for some reason during read-out. To recover the corrupted data, a straight- forward means of protection is to store as many copies of s as possible. For simplicity, say we store three copies. Such a scheme is called the three-times repetition code. Thus, instead of simply storing s, we store  s,s,s . To distin- guish them, let us denote the ﬁrst s as x1 and the others as x2 and x3. In other words, we have   cid:40 x2 = x3 = 0 if x1 = 0,  x2 = x3 = 1 if x1 = 1,   3.6   and the possible values of  x1,x2,x3  are  000  and  111 .  When you read out the stream  x1,x2,x3  from a CD, you must check wheth- er x1 = x2 and x1 = x3 in order to detect if there was a data corruption. From Exercise 3.2, this can be achieved by the following computation:   cid:40 data clean  data corrupted  if x1 + x2 = 0 and x1 + x3 = 0, otherwise.   3.7   For example, if the read-out is  x1,x2,x3  =  000 , then you might say the data   3.2 Three-times repetition code  35  is clean. Otherwise, if the read-out shows  x1,x2,x3  =  001  you immediately ﬁnd x1 + x3 = 1 and the data is corrupted.  Now say that the probability of writing in 0 and reading out 1 is p, and the same for writing in 1 and reading out 0. You see that a bit is corrupted with probability p and remains clean with probability  1− p . Usually we can assume p < 1 2, meaning the data is more likely to be clean than corrupted. In the case of p > 1 2, a simple bit-ﬂipping technique of treating the read-out of 1 as 0 and 0 as 1 would do the trick.  Thus, when p < 1 2, the only possibilities for data corruption going unde- tected are the cases when the read-out shows  111  given writing in was  000  and when the read-out shows  000  given writing in was  111 . Each occurs with probability2 p3 < 1 8. Compared with the case when the data is unpro- tected, the probability of undetectable corruption drops from p to p3. It means that when the read-out shows either  000  or  111 , we are more conﬁdent that such a read-out is clean.  The above scheme is commonly referred to as error detection  see also Chapter 2 , by which we mean we only detect whether the data is corrupted, but we do not attempt to correct the errors. However, our goal was to correct the corrupted data, not just detect it. This can be easily achieved with the rep- etition code. Consider the case of a read-out  001 : you would immediately guess that the original data is more likely to be  000 , which corresponds to the binary bit s = 0. On the other hand, if the read-out shows  101 , you would guess the second bit is corrupted and the data is likely to be  111  and hence determine the original s = 1.  There is a good reason for such a guess. Again let us denote by p the proba- bility of a read-out bit being corrupted, and let us assume3 that the probability of s being 0 is 1 2  and of course the same probability for s being 1 . Then, given the read-out  001 , the probability of the original data being  000  can be computed as follows. Here again we assume that the read-out bits are cor- rupted independently. Assuming Pr[s = 0] = Pr[s = 1] = 1 2, it is clear that  Pr Writing in  000   = Pr Writing in  111   =   3.8   1 2 .  It is also easy to see that  2 Here we assume each read-out bit is corrupted independently, meaning whether one bit is cor- rupted or not has no effect on the other bits being corrupted or not. With this independence assumption, the probability of having three corrupted bits is p· p· p = p3. 3 Why do we need this assumption? Would the situation be different without this assumption? Take the case of Pr[s = 0] = 0 and Pr[s = 1] = 1 as an example.   36  Repetition and Hamming codes  Pr Writing in  000  and reading out  001    = Pr Writing in  000  · Pr Reading out  001  Writing in  000    3.9   3.10  = Pr Writing in  000  · Pr 0 → 0 · Pr 0 → 0 · Pr 0 → 1  =   3.11   1 2 ·  1− p ·  1− p · p  1− p 2 p Similarly, we have  =  2  .  Pr Writing in  111  and reading out  001   =   1− p p2  .  2  These together show that  Pr Reading out  001    = Pr Writing in  000  and reading out  001    + Pr Writing in  111  and reading out  001    1− p p  .  =  2  Thus  Pr Writing in  000  Reading out  001    Pr Writing in  000  and reading out  001    Pr Reading out  001    = = 1− p.  Similarly, it can be shown that  Pr Writing in  111  Reading out  001   = p.  As p < 1 2 by assumption, we immediately see that   3.12    3.13    3.14    3.15    3.16    3.17    3.18    3.19   1− p > p,  and, given that the read-out is  001 , the case of writing in  111  is less likely. Hence we would guess the original data is more likely to be  000  due to its higher probability. Arguing in a similar manner, we can construct a table for decoding, shown in Table 3.1.  From Table 3.1, we see that given the original data being  000 , the cor- rectable error events are the ones when the read-outs are  100 ,  010 , and  001 , i.e. the ones when only one bit is in error. The same holds for the other write-in of  111 . Thus we say that the three-times repetition code is a single- error-correcting code, meaning the code is able to correct all possible one-bit errors. If there are at least two out of the three bits in error during read-out,   3.2 Three-times repetition code  37  Table 3.1 Decoding table for the repetition code based on probability  Read-outs  Likely original Decoded output   000 ,  100 ,  010 ,  001   111 ,  011 ,  101 ,  110    000   111   s = 0 s = 1  then this code is bound to make an erroneous decision as shown in Table 3.1. The probability of having an erroneous decoded output is given by  Pr Uncorrectable error  = 3p2 1− p  + p3   3.20   that is smaller than the original p. Exercise 3.3 Prove 3p2 1− p  + p3 < p for p ∈  0,1 2 . Exercise 3.4 It should be noted that Table 3.1 is obtained under the assump- tion of Pr[s = 0] = Pr[s = 1] = 1 2. What if Pr[s = 0] = 0 and Pr[s = 1] = 1? Re- construct the table for this case and conclude that Pr Uncorrectable error  = 0. Then rethink whether you need error protection and correction in this case. ♦ To summarize this section, below we give a formal deﬁnition of an error-  ♦  correcting code. Deﬁnition 3.5 A code C is said to be an  n,k  error-correcting code if it is a scheme of mapping k bits into n bits, and we say C has code rate R = k n. We say C is a t-error-correcting code if C is able to correct any t or fewer errors in the received n-vector. Similarly, we say C is an e-error-detecting code if C is able to detect any e or fewer errors in the received n-vector.  With the above deﬁnition, we see that the three-times repetition code is a  3,1  error-correcting code with code rate R = 1 3, and it is a 1-error-cor- recting code. When being used purely for error detection, it is also a 2-error- detecting code. Moreover, in terms of the error correction or error detection capability, we have the following two theorems. The proofs are left as an exer- cise. Theorem 3.6 Let C be an  n,k  binary error-correcting code that is t-error- correcting. Then assuming a raw bit error probability of p, we have  t + 1 cid:19 pt+1 1− p n−t−1 Pr Uncorrectable error  ≤ cid:18  n t + 2 cid:19 pt+2 1− p n−t−2 +··· + cid:18 n + cid:18  n  n cid:19 pn,  3.21     3.22   38  where cid:0 n  Repetition and Hamming codes   cid:96  cid:1  is the binomial coefﬁcient deﬁned as  cid:96 ! n−  cid:96  ! .   cid:96  cid:19   cid:44   cid:18 n  n!  Theorem 3.7 Let C be an  n,k  binary error-correcting code that is e-error- detecting. Then assuming a raw bit error probability of p, we have  Pr Undetectable error  ≤ cid:18  n e + 1 cid:19 pe+1 1− p n−e−1 + cid:18  n e + 2 cid:19 pe+2 1− p n−e−2 +··· + cid:18 n  Exercise 3.8 Prove Theorems 3.6 and 3.7.  n cid:19 pn.  3.23   Hint: For the situation of Theorem 3.6, if a code is t-error-correcting, we know that it can correctly deal with all error patterns of t or fewer errors. For error patterns with more than t errors, we do not know: some of them might be corrected; some not. Hence, as an upper bound to the error probability, assume that any error pattern with more than t errors cannot be corrected. ♦ The same type of thinking also works for Theorem 3.7.  Recall that in  3.6  and  3.7 , given the binary bit s, we use x1 = x2 = x3 = s ? to generate the length-3 binary stream  x1,x2,x3 , and use x1 +x2 = 0 and x1 + ? x3 = 0 to determine whether the read-out has been corrupted. In general, it is easier to rewrite the two processes using matrices; i.e., we have the following:  1 1 cid:17   x2   cid:16 x1 1 0 1 cid:33   cid:32 1 1 0  x3 cid:17  = s cid:16 1  = cid:32 0 0 cid:33   x1 x2 x3  ?   generating equation ,   3.24    check equations .   3.25   The two matrix equations above mean that we use the matrix  to generate the length-3 binary stream and use the matrix  G = cid:16 1 1 H = cid:32 1 1  1 0  1 cid:17  1 cid:33   0   3.26    3.27   to check whether the data is corrupted. Thus, the matrix G is often called the generator matrix and H is called the parity-check matrix. We have the follow- ing deﬁnition.   3.2 Three-times repetition code  39  Deﬁnition 3.9 Let C be an  n,k  error-correcting code that maps length-k binary streams s into length-n binary streams x. We say C is a linear code if there exist a binary matrix G of size  k × n  and a binary matrix H of size   n− k × n  such that the mapping from s to x is given by  x =  x1 ··· xn  =  s1 ··· sk   cid:125   cid:123  cid:122   =s  G  and the check equations are formed by   3.28    3.29    cid:124   ,  0 ... 0  =  HxT ?  where by xT we mean the transpose of vector x  rewriting horizontal rows as vertical columns and vice versa . The vector x is called a codeword associated with the binary message s.  Exercise 3.10 With linear codes, the detection of corrupted read-outs is ex- tremely easy. Let C be an  n,k  binary linear code with a parity-check matrix H of size   n− k × n . Given any read-out y =  y1, . . . ,yn , show that y is cor- rupted if HyT  cid:54 = 0T, i.e. if at least one parity-check equation is unsatisﬁed. It ♦ should be noted that the converse is false in general.4  Exercise 3.11 Let C be an  n,k  binary linear code with generator matrix G of size  k × n  and parity-check matrix H of size   n− k × n . Show that the product matrix HGT must be a matrix whose entries are either 0 or multiples of 2; hence, after taking modulo reduction by 2, we have HGT = 0, an all-zero ♦ matrix.  Exercise 3.12  Dual code  In Deﬁnition 3.9, we used matrix G to generate the codeword x given the binary message s and used the matrix H to check the integrity of read-outs of x for an  n,k  linear error-correcting code C . On the other hand, it is possible to reverse the roles of G and H. The process is detailed as follows. Given C , G, and H, we deﬁne the dual code C ⊥ of C by encoding the length- n− k  binary message s cid:48  as x cid:48  = s cid:48 H and check the integrity of x cid:48  using Gx cid:48 T ?  a  The dual code C ⊥ of the three-times repetition code is a  3,2  linear code  = 0T. Based on the above, verify the following:  with rate R cid:48  = 2 3, and   b  C ⊥ is a 1-error-detecting code. 4 For the correction of corrupted read-outs of linear codewords, see the discussion around  3.34 .   40  Repetition and Hamming codes  This code is called the single parity-check code; see Chapter 2.  ♦  In the above exercise, we have introduced the concept of a dual code. The dual code is useful in the sense that once you have an  n,k  linear code with generator matrix G and parity-check matrix H, you immediately get another  n,n− k  linear code for free, simply by reversing the roles of G and H. How- ever, readers should be warned that this is very often not for the purpose of error correction. Speciﬁcally, throughout the studies of various kinds of linear codes, it is often found that if the linear code C has a very strong error correc- tion capability, then its dual code C ⊥ is highly likely to be weak. Conversely, if C is very weak, then its dual C ⊥ might be strong. Nevertheless, the duality between C and C ⊥ can be extremely useful when studying the combinatorial properties of a code, such as packing  see Section 3.3.3 , covering  see p. 179 , weight enumerations, etc.  3.3 Hamming code  In Section 3.2, we discussed the  3,1  three-times repetition code that is capa- ble of correcting all 1-bit errors or detecting all 2-bit errors. The price for such a capability is that we have to increase the ﬁle size by a factor of 3. For exam- ple, if you have a ﬁle of size 700 MB to be stored on CD and, in order to keep the ﬁle from corruption, you use a  3,1  three-times repetition code, a space of 2100 MB, i.e. 2.1 GB, is needed to store the encoded data. This corresponds to almost half of the storage capacity of a DVD!  Therefore, we see that while the three-times repetition code is able to pro- vide some error protection, it is highly inefﬁcient in terms of rate. In general, we would like the rate R = k n to be as close to 1 as possible so that wastage of storage space is kept to a minimum.  3.3.1 Some historical background  The problem of ﬁnding efﬁcient error-correcting schemes, but of a much smal- ler scale, bothered Richard Wesley Hamming  1915–1998  while he was em- ployed by Bell Telephone Laboratory  Bell Labs  in the late 1940s. Hamming was a mathematician with a Ph.D. degree from the University of Illinois at Urbana-Champaign in 1942. He was a professor at the University of Louisville during World War II, and left to work on the Manhattan Project in 1945, pro- gramming a computer to solve the problem of whether the detonation of an atomic bomb would ignite the atmosphere. In 1946, Hamming went to Bell   3.3 Hamming code  41  Labs and worked on the Bell Model V computer, an electromechanical relay- based machine. At that time, inputs to computers were fed in on punch cards, which would invariably have read errors  the same as CDs or DVDs in com- puters nowadays . Prior to executing the program on the punch cards, a special device in the Bell Model V computer would check and detect errors. During weekdays, when errors were found, the computer would ﬂash lights so the op- erators could correct the problem. During after-hours periods and at weekends, when there were no operators, the machine simply terminated the program and moved on to the next job. Thus, during the weekends, Hamming grew increas- ingly frustrated with having to restart his programs from scratch due to the unreliable card reader.  Over the next few years he worked on the problem of error correction, de- veloping an increasingly powerful array of algorithms. In 1950 he published what is now known as the Hamming code, which remains in use in some ap- plications today.  Hamming is best known for the Hamming code he developed in 1950, as well as the Hamming window5 used in designing digital ﬁlters, the Hamming bound related to sphere packing theory  see Section 3.3.3 , and the Ham- ming distance as a measure of distortion in digital signals  see Exercise 3.16 . Hamming received the Turing award in 1968 and was elected to the National Academy of Engineering in 1980.  Exercise 3.13 The error-correcting mechanism used in CDs6 for data pro- tection is another type of error-correcting code called Reed–Solomon  R-S  code, developed by Irving S. Reed and Gustave Solomon in 1960. The use of R-S codes as a means of error correction for CDs was suggested by Jack van Lint  1932–2004  while he was employed at Philips Labs in 1979. Two con- secutive R-S codes are used in serial in a CD. These two R-S codes operate in bytes  B  instead of bits  1 B = 8 bits . The ﬁrst R-S code takes in 24 B of raw data and encodes them into a codeword of length 28 B. After this, another mechanism called interleaver would take 28 such encoded codewords, each 28 B long, and then permute the overall 282 = 784 B of data symbols. Finally, the second R-S code will take blocks of 28 B and encode them into blocks of  5 The Hamming window was actually not due to Hamming, but to John Tukey  1915–2000 , who also rediscovered with James Cooley the famous algorithm of the fast Fourier transform  FFT  that was originally invented by Carl Friedrich Gauss in 1805, but whose importance to modern engineering was not realized by the researchers until 160 years later. The wonderful Cooley–Tukey FFT algorithm is one of the key ingredients of your MP3 players, DVD players, and mobile phones. So, you actually have Gauss to thank for it. Amazing, isn’t it?  6 A similar mechanism is also used in DVDs. We encourage you to visit the webpage of Professor Tom Høholdt at http:  www2.mat.dtu.dk people T.Hoeholdt DVD index.html for an extremely stimulating demonstration.   42  Repetition and Hamming codes  32 B. Thus, the ﬁrst R-S code can be regarded as a  28 B, 24 B  linear code and the second as a  32 B, 28 B  linear code. Based on the above, determine the actual size  in megabytes  MB   of digital information that is stored on a CD if a storage capacity of 720 MB is claimed. Also, what is the overall code ♦ rate used on a CD?  3.3.2 Encoding and error correction of the  7,4  Hamming code The original Hamming code is a  7,4  binary linear code with the following generator and parity-check matrices:  G =  1 1 0 0 1 1 1 1 1 1 0 1  1 0 0 0 1 0 0 0 1 0 0 0  0 0 0 1    and H =  1 0 0  0 1 0  0 0 1  1 1 0  0 1 1  1 1 1   3.30  Speciﬁcally, the encoder of the  7,4  Hamming code takes in a message of four bits, say s =  s1,s2,s3,s4  and encodes them as a codeword of seven bits, say x =  p1, p2, p3,s1,s2,s3,s4 , using the following generating equations:  1 0 1   .   3.31   p1 = s1 + s3 + s4, p2 = s1 + s2 + s3, p3 = s2 + s3 + s4. Mappings from s to x are tabulated in Table 3.2.    Table 3.2 Codewords of the  7,4  Hamming code  Message  Codeword  Message  Codeword  0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1 1 1  0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1 0 1 1 1  1 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 1  1 1 0 1 0 0 0 0 1 1 1 0 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1  There are several ways to memorize the  7,4  Hamming code. The simplest   3.3 Hamming code  43  Figure 3.1 Venn diagram of the  7,4  Hamming code.  way is perhaps to use the Venn diagram pointed out by Robert J. McEliece [McE85] and shown in Figure 3.1. There are three overlapping circles: circles I, II, and III. Each circle represents one generating equation as well as one parity- check equation of the  7,4  Hamming code  see  3.31  . For example, circle I corresponds to the ﬁrst generating equation of p1 = s1 + s3 + s4 and the parity- check equation p1 + s1 + s3 + s4 = 0 since bits s1,s3,s4, and p1 are included in this circle. Similarly, the second circle, circle II, is for the check equation of p2 + s1 + s2 + s3 = 0, and the third circle III is for the check equation of p3 + s2 + s3 + s4 = 0. Note that each check equation is satisﬁed if, and only if, there is an even number of 1s in the corresponding circle. Hence, the p1, p2, and p3 are also known as even parities.  Using the Venn diagram, error correction of the Hamming code is easy. For example, assume codeword x =  0110100  was written onto the CD, but due to an unknown one-bit error the read-out shows  0111100 . We put the read- out into the Venn diagram shown in Figure 3.2. Because of the unknown one- bit error, we see that  is shown;    the number of 1s in circle I is 1, an odd number, and a warning  bold circle    the number of 1s in circle II is 3, an odd number, and a warning  bold circle    the number of 1s in circle III is 2, an even number, so no warning  normal  is shown;  circle  is given.  From these three circles we can conclude that the error must not lie in circle III, but lies in both circles I and II. This leaves s1 as the only possibility, since s1  II  p2  s3  s4  s1  p1  I   44  Repetition and Hamming codes  Figure 3.2 Venn diagram used for decoding  0111100 .  is the only point lying in circles I and II but not in III. Hence s1 must be wrong and should be corrected to a 0 so that both warnings are cleared and all three circles show no warning. This then corrects the erroneous bit as expected.  Let us try another example. What if the read-out is  1110100 ? The corre- sponding Venn diagram is shown in Figure 3.3. Following the same reasoning  Figure 3.3 Venn diagram used for decoding  1110100 .  as before, we see that the error must lie in circle I, but cannot be in circles II and III. Hence the only possible erroneous bit is p1. Changing the read-out of p1 from 1 to 0 will correct the one-bit error.  Now let us revisit the above two error-correcting examples and try to for- mulate a more systematic approach. In the ﬁrst example, the read-out was y =  0111100 . Using the parity-check matrix H deﬁned in  3.30  we see  1  0  0  1  0  I  0  1  0  0  1  I     0 1 1 1 1 0 0          1 1 1 0 1 0 0    3.3 Hamming code  45  the following connection:  HyT =  1 0 0 0 1 0 0 0 1  1 1 0  0 1 1 1 1 0 1 1 1  =  1 1 0   ⇐⇒   I II III   .   3.32   This corresponds exactly to the Venn diagram of Figure 3.2. Note that the ﬁrst entry of  110  corresponds to the ﬁrst parity-check equation of p1 + s1 + s3 + ? s4 = 0 as well as to circle I in the Venn diagram. Since it is unsatisﬁed, a warning is shown. Similarly for the second and third entries of  110 . Now we ask the question: which column of H has the value  110 T? It is the fourth column, which corresponds to the fourth bit of y, i.e. s1 in x. Then we conclude that s1 is in error and should be corrected to a 0. The corrected read-out is therefore  0110100 .  For the second example of a read-out being y =  1110100 , carrying out  the same operations gives  HyT =  1 0 0 1 0 0  0 1 0 1 1 0  0 1 1  1 1 1 0 1 1  =  1 0 0   ⇐⇒   I II III     3.33   corresponding to the Venn diagram of Figure 3.3. Since  100 T is the ﬁrst column of H, it means the ﬁrst entry of y is in error, and the corrected read-out should be  0110100 .  There is a simple reason why the above error correction technique works. For brevity, let us focus on the ﬁrst example of y =  0111100 . This is the case when the fourth bit of y, i.e. s1, is in error. We can write y as follows:  y =  0111100  =  0110100   +  0001000  ,   3.34   where x is the original codeword written into a CD and e is the error pattern.   cid:124   = x   cid:123  cid:122    cid:125    cid:124   = e   cid:123  cid:122    cid:125    46  Repetition and Hamming codes  Recall that x is a codeword of the  7,4  Hamming code; we must have HxT = 0T from Deﬁnition 3.9. Thus from the distributive law veriﬁed in Exercise 3.1 we see that  HyT = H  xT + eT  = HxT + HeT = 0T + HeT = HeT.   3.35   Since the only nonzero entry of e is the fourth entry, left-multiplying eT by H gives the fourth column of H. Thus, HeT =  110 T. In other words, we have the following logic deductions: without knowing the error pattern e in the ﬁrst place, to correct the one-bit error,  HyT =  110 T HeT =  110 T  e =  0001000 .  =⇒ =⇒   3.36   3.37   3.38   The last logic deduction relies on the following two facts:   1  we assume there is only one bit in error, and  2  all columns of H are distinct.  With the above arguments, we get the following result.  Theorem 3.14 The  7,4  Hamming code can be classiﬁed as one of the fol- lowing:   1  a single-error-correcting code,  2  a double-error-detecting code.  Proof The proof of the Hamming code being able to correct all one-bit errors follows from the same logic deduction given above. To establish the second claim, simply note that when two errors occur, there are two 1s in the error pattern e, for example e =  1010000 . Calculating the parity-check equations shows HyT = HeT. Note that no two distinct columns of H can be summed to yield  000 T. This means any double-error will give HyT  cid:54 = 0T and hence can be detected.  From Theorem 3.14 we see that the  7,4  Hamming code is able to correct all one-bit errors. Thus, assuming each bit is in error with probability p, the probability of erroneous correction is given by  Pr Uncorrectable error  ≤ cid:18 7  2 cid:19 p2 1− p 5 + cid:18 7  3 cid:19 p3 1− p 4 +··· + cid:18 7  7 cid:19 p7.   3.39   It should be noted that in  3.39  we actually have an equality. This follows from   3.3 Hamming code  47  the fact that the Hamming code cannot correct any read-outs having more than one bit in error.  Exercise 3.15 For error detection of the  7,4  Hamming code, recall the check equation HyT = HeT ? = 0T. Using this relation, ﬁrst show that an error pattern e is undetectable if, and only if, e is a nonzero codeword. Thus the  7,4  Hamming code can detect some error patterns that have more than two errors. Use this fact to show that the probability of a detection error of the  7,4  Hamming code is  Pr Undetectable error  = 7p3 1− p 4 + 7p4 1− p 3 + p7,   3.40   which is better than what has been claimed by Theorem 3.7.  Hint: Note from Table 3.2 that, apart from the all-zero codeword, there are seven codewords containing three 1s, another seven codewords containing four ♦ 1s, and one codeword consisting of seven 1s.  Next we compare the performance of the three-times repetition code with the performance of the  7,4  Hamming code. First, note that both codes are able to correct all one-bit errors or detect all double-bit errors. Yet, the repetition code requires to triple the size of the original ﬁle for storage, while the  7,4  Hamming code only needs 7 4 = 1.75 times the original space. Therefore, the  7,4  Hamming code is more efﬁcient than the three-times repetition code in terms of required storage space.  Before concluding this section, we use the following exercise problem as a quick introduction to another contribution of Hamming. It is related to the topic of sphere packing, which will be discussed in Section 3.3.3.  Exercise 3.16  Hamming distance  Another contribution of Richard Ham- ming is the notion of Hamming distance. Given any two codewords x =  x1, x2, . . . ,x7  and x cid:48  =  x cid:48 1,x cid:48 2, . . . ,x cid:48 7 , the Hamming distance between x and x cid:48  is the number of places x differs from x cid:48 . For example, the Hamming dis- tance between  1011100  and  0111001  is 4 since x differs from x cid:48  in the ﬁrst, second, ﬁfth, and seventh positions. Equivalently, you can compute  1011100  +  0111001  =  1100101 . Then the condition given in Exer- ? = 0 for  cid:96  = 1,2, . . . ,7, shows cise 3.2, namely, x cid:96  the distance is 4 since there are four 1s appearing in the sum  1100101 . Note that the number of ones in a binary vector is called the Hamming weight of the vector.  ? = x cid:48  cid:96  is equivalent to x cid:96  + x cid:48  cid:96   Now, using Table 3.2, show that every two distinct codewords of a  7,4  ♦  Hamming code are separated by Hamming distance ≥ 3.   48  Repetition and Hamming codes  Some of you might wonder why not simply stick to the general deﬁnition of Euclidean distance and try to avoid the need for this new deﬁnition of Ham- ming distance. There is a good reason for this. Recall that the Euclidean dis- tance between two distinct points  x,y  and  x cid:48 ,y cid:48   is deﬁned as follows:  d  cid:44  cid:113  x− x cid:48  2 +  y− y cid:48  2.  However, this deﬁnition will not work in the binary space. To see this, con- sider the example of  x,y  =  00  and  x cid:48 ,y cid:48   =  11 . The Euclidean distance between these two points is given by  d = cid:113  0− 1 2 +  0− 1 2 = √1 + 1 = √0 = 0,  where you should note that 1 + 1 = 0 in binary. Thus, the Euclidean distance fails in the binary space.   3.41    3.42   3.3.3 Hamming bound: sphere packing  In Exercise 3.16 we have seen that every distinct pair of codewords of the  7,4  Hamming code is separated by Hamming distance at least d = 3. Thus in terms of geometry we have a picture as shown in Figure 3.4 a . Now if we draw two spheres as shown in Figure 3.4 b   you might want to think of them as high- dimensional balls , each with radius R = 1, centered at x and x cid:48 , respectively, these two spheres would not overlap and must be well-separated. Points within the x-sphere represent the read-outs that are at a distance of at most 1 from x. In other words, the points within the x-sphere are either x or x with a one-bit error.   a    b   Figure 3.4 Geometry of x  cid:54 = x cid:48  in the  7,4  Hamming code with Hamming distance 3.  If we draw a sphere with radius R = 1 centered at each codeword of the  7,4  Hamming code, there will be 16 nonoverlapping spheres since there are 16 codewords and every pair of distinct codewords is separated by a distance of at least 3. Given that a codeword x was written into the CD, for example,  x  x  d = 3  R = 1   3.3 Hamming code  49  the one-bit error read-out must be at distance 1 from x and therefore must lie within the radius-1 x-sphere centered at x. It cannot lie in other spheres since the spheres are well-separated. This shows that this one-bit error read-out is closer to x than to any other codewords of the  7,4  Hamming code.  Thus, correcting a one-bit error is always possible for the  7,4  Hamming code. This can be seen as a geometrical explanation of the single-error-cor- rection capability of the  7,4  Hamming code. We may generalize the above argument slightly and give the following theorem.  Theorem 3.17 Let C be an  n,k  error-correcting code  not necessarily lin- ear, i.e. it does not necessarily have the generator and parity-check matrices G and H . Assume that every distinct pair of codewords in C is separated by Hamming distance at least d; then C is a t-error-correcting code with  2  cid:23  , t = cid:22  d − 1   3.43   where by  cid:98 ξ cid:99  we mean7 the largest integer not larger than ξ . Also, if C is used only for error detection, then C is an  d − 1 -error-detecting code. Proof Given d, we can draw spheres with radius t centered at the codewords of C . Since 2t < d, the spheres must be nonoverlapping. Extending the proof to error detection is obvious.  The above theorem says that in order to correct more errors, the codewords should be placed as far apart as possible. But this is not what we are interested in here. Instead, we are interested in the reverse direction. We ask the following question.  Consider a t-error-correcting code C that maps input messages to a binary stream of length n. So, we draw spheres of radius t centered at the codewords of C . The spheres do not overlap with each other. What is the maximal number of codewords C can have? In other words, we are interested in knowing how many nonoverlapping radius-t spheres can be packed into an n-dimensional binary space.  This is the sphere packing problem in discrete mathematics. Let us work through some examples in order to understand the question better.  Example 3.18 The  7,4  Hamming code has 16 codewords, hence 16 spheres with radius 1, since the code is 1-error-correcting.   The codewords have length 7, with a binary value in each coordinate. So, 7 For example,  cid:98 1.1 cid:99  =  cid:98 1.9 cid:99  = 1.   50  Repetition and Hamming codes  the number of possible length-7 binary tuples is 27 = 128, meaning there are 128 points in this 7-dimensional binary space.  1 from the center, i.e. one-bit errors from the codeword.    Each codeword of the Hamming code is surrounded by a sphere with radius 1 cid:1  = 8 points in each sphere. This ﬁrst “1” corresponds to 1. There are 1 + cid:0 7 1 cid:1  points are the ones at distance the center, i.e. distance 0. The remaining cid:0 7 Thus, the 16 nonoverlapping spheres actually cover 16×8 = 128 points, which are all the points in the 7-dimensional binary space. We see that the  7,4  Hamming code has the tightest possible packing of radius-1 spheres in the ♦ 7-dimensional binary space.  Example 3.19 Let us consider the dual of the  7,4  Hamming code C in this example. Recall from Exercise 3.12 that the dual code C ⊥ is obtained by reversing the roles of the generating matrix G and the parity-check matrix H of C . That is, we use the parity-check matrix H for encoding and the generator matrix G for checking. Thus the generator matrix G⊥ of C ⊥ is given by  G⊥ = H =  1 0 0  0 0 1 1 0 1 0 1 0  0 1 1  1 1 1  1 0 1     3.44   and it maps binary messages of length 3 to codewords of length 7. All the eight possible codewords are tabulated in Table 3.3.  Table 3.3 Codewords of the dual code of the  7,4  Hamming code  Message  Codeword  Message  Codeword  0 0 0 0 0 1 0 1 0 0 1 1  0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1  1 0 0 1 0 1 1 1 0 1 1 1  1 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0  You can check that the codewords are separated by Hamming distance 4  exactly. Hence C ⊥ is able to correct errors up to t = cid:4  4−1  same as C does. Thus, it is clear that C ⊥ is not a good packing of radius-1 spheres in the 7-dimensional binary space since it packs only eight spheres, ♦ while C can pack 16 spheres into the same space.  2  cid:5  = 1, which is the  Why are we interested in the packing of radius-t spheres in an n-dimensional   3.3 Hamming code  51  space? The reason is simple. Without knowing the parameter k in the ﬁrst place, i.e. without knowing how many distinct 2k binary messages can be encoded, by ﬁxing t we make sure that the codewords are immune to errors with at most t error bits. Choosing n means the codewords will be stored in n bits. Being able to pack more radius-t spheres into the n-dimensional spaces means we can have more codewords, hence larger k. This gives a general bound on k, known as the sphere bound, and stated in the following theorem. Theorem 3.20  Sphere bound  Let n, k, and t be deﬁned as above. Then we have  2k ≤  2n   cid:0 n 0 cid:1  + cid:0 n  1 cid:1  +··· + cid:0 n t cid:1  .   3.45   Codes with parameter n, k, and t that achieve equality in  3.45  are called perfect, meaning a perfect packing. Proof Note that 2k is the number of codewords, while 2n is the number of points in an n-dimensional binary space, i.e. the number of distinct bi- nary n-tuples. The denominator shows the number of points within a radius-t sphere. The inequality follows from the fact that for t-error-correcting codes the spheres must be nonoverlapping.  Finally we conclude this section with the following very deep result.  Theorem 3.21 The only parameters satisfying the bound  3.45  with equality are  n = 2u − 1, n = 23, n = 2u + 1,  k = 2u − u− 1, k = 12, k = 1,  t = 1, t = 3; t = u,  for any positive integer u;  for any positive integer u.   3.46   This theorem was proven by Aimo Tiet¨av¨ainen [Tie73] in 1973 after much work by Jack van Lint. One code satisfying the second case of n = 23, k = 12, and t = 3 is the Golay code, hand-constructed by Marcel J. E. Golay in 1949 [Gol49].8 Vera Pless [Ple68] later proved that the Golay code is the only code with these parameters that satisﬁes  3.45  with equality. The ﬁrst case is a general Hamming code of order u  see Exercise 3.22 below , and the last case is the  2u +1 -times repetition code, i.e. repeating the message  2u +1  times. 8 This paper is only half a page long, but belongs to the most important paper in information theory ever written! Not only did it present the perfect Golay code, but it also gave the gen- eralization of the Hamming code and the ﬁrst publication of a parity-check matrix. And even though it took over 20 years to prove it, Golay already claimed in that paper that there were no other perfect codes. For more details on the life of Marcel Golay, see http:  www.isiweb.ee.eth z.ch archive massey pub pdf BI953.pdf.     52  Repetition and Hamming codes  Exercise 3.22  Hamming code of order u  Recall that the  7,4  Hamming code is deﬁned by its parity-check matrix  H =  1 0 0 1 0 0  0 0 1  1 0 1 1 1 1 1 0 0 1 1 1   .   3.47   Note that the columns of the above  3× 7  matrix consist of all possible non- zero length-3 binary vectors. From this, we can easily deﬁne a general Ham- ming code Cu of order u. Let Hu be the matrix whose columns consist of all possible nonzero length-u binary vectors; Hu is of size  u×  2u − 1  . Then a general Hamming code Cu is the code deﬁned by the parity-check matrix Hu with n = 2u − 1 and k = 2u − u− 1. Show that  a  Cu is 2-error-detecting  Hint: Show that HueT  cid:54 = 0T for all nonzero vectors  b  Cu is 1-error-correcting  Hint: Show that HuyT = 0T for some nonzero vector y if, and only if, y has at least three nonzero entries. Then use this to conclude that every pair of distinct codewords is separated by Hamming ♦ distance at least 3 .  e that have at most two nonzero entries ;  3.4 Further reading  In this chapter we have brieﬂy introduced the theory of error-correcting codes and have carefully studied two example codes: the three-times repetition code and the  7,4  Hamming code. Besides their error correction capabilities, we have also brieﬂy studied the connections of these codes to sphere packing in high-dimensional spaces.  For readers who are interested in learning more about other kinds of error- correcting codes and their practical uses, [Wic94] is an easy place to start, where you can learn about a more general treatment of the Hamming codes. Another book, by Shu Lin and Daniel Costello [LC04], is a comprehensive collection of all modern coding schemes. An old book by Jessy MacWilliams and Neil Sloane [MS77] is the most authentic source for learning the theory of error-correcting codes, but it requires a solid background in mathematics at graduate level.  The Hamming codes are closely related to combinatorial designs, differ- ence sets, and Steiner systems. All are extremely fascinating objects in com- binatorics. The interested readers are referred to [vLW01] by Jack van Lint and Richard Wilson for further reading on these subjects. These combinatorial   References  53  objects are also used in the designs of radar systems, spread spectrum-based cellular communications, and optical ﬁber communication systems.  The topic of sphere packing is always hard, yet fascinating. Problems therein have been investigated for more than 2000 years, and many remain open. A general discussion of this topic can be found in [CS99]. As already seen in Theorem 3.21, the sphere packing bound is not achievable in almost all cases. Some bounds that are tighter than the sphere packing bound, such as the Gilbert–Varshamov bound, the Plotkin bound, etc., can be found in [MS77] and [Wic94]. In [MS77] a table is provided that lists all known best packings in various dimensions. An updated version can be found in [HP98]. So far, the tightest lower bound on the existence of the densest possible packings is the Tsfasman–Vl˘adut¸–Zink  TVZ  bound, and there are algebraic geometry codes constructed from function ﬁelds deﬁned by the Garcia–Stichtentoth curve that perform better than the TVZ bound, i.e. much denser sphere packings. A good overview of this subject can be found in [HP98].  References  [CS99]  John Conway and Neil J. A. Sloane, Sphere Packings, Lattices and Groups, 3rd edn. Springer Verlag, New York, 1999.  [Gol49] Marcel J. E. Golay, “Notes on digital coding,” Proceedings of the IRE,  vol. 37, p. 657, June 1949.  [HP98] W. Cary Huffman and Vera Pless, eds., Handbook of Coding Theory. North-  [LC04]  Holland, Amsterdam, 1998. Shu Lin and Daniel J. Costello, Jr., Error Control Coding, 2nd edn. Prentice Hall, Upper Saddle River, NJ, 2004.  [McE85] Robert J. McEliece, “The reliability of computer memories,” Scientiﬁc Amer-  ican, vol. 252, no. 1, pp. 68–73, 1985.  [MS77] F. Jessy MacWilliams and Neil J. A. Sloane, The Theory of Error-Correcting  Codes. North-Holland, Amsterdam, 1977.  [Ple68] Vera Pless, “On the uniqueness of the Golay codes,” Journal on Combination  Theory, vol. 5, pp. 215–228, 1968.  [Tie73] Aimo Tiet¨av¨ainen, “On the nonexistence of perfect codes over ﬁnite ﬁelds,” SIAM Journal on Applied Mathematics, vol. 24, no. 1, pp. 88–96, January 1973.  [vLW01] Jacobus H. van Lint and Richard M. Wilson, A Course in Combinatorics,  2nd edn. Cambridge University Press, Cambridge, 2001.  [Wic94] Stephen B. Wicker, Error Control Systems for Digital Communication and  Storage. Prentice Hall, Englewood Cliffs, NJ, 1994.    4  Data compression: efﬁcient coding of a  random message  In this chapter we will consider a new type of coding. So far we have con- centrated on codes that can help detect or even correct errors; we now would like to use codes to represent some information more efﬁciently, i.e. we try to represent the same information using fewer digits on average. Hence, instead of protecting data from errors, we try to compress it such as to use less storage space.  To achieve such a compression, we will assume that we know the probabil- ity distribution of the messages being sent. If some symbols are more probable than others, we can then take advantage of this by assigning shorter code- words to the more frequent symbols and longer codewords to the rare symbols. Hence, we see that such a code has codewords that are not of ﬁxed length.  Unfortunately, variable-length codes bring with them a fundamental prob- lem: at the receiving end, how do you recognize the end of one codeword and the beginning of the next? To attain a better understanding of this question and to learn more about how to design a good code with a short average codeword length, we start with a motivating example.  4.1 A motivating example  You would like to set up your own telephone system that connects you to your three best friends. The question is how to design efﬁcient binary phone num- bers. In Table 4.1 you ﬁnd six different ways of how you could choose them. Note that in this example the phone number is a codeword for the person we want to talk to. The set of all phone numbers is called code. We also assume that you have different probabilities when calling your friends: Bob is your best friend whom you will call in 50% of the times. Alice and Carol are contacted with a frequency of 25% each.   56  Efﬁcient coding of a random message  Table 4.1 Binary phone numbers for a telephone system with three friends  Friend  Probability  Phone number  Alice Bob Carol  1 4 1 2 1 4  0011 0011 1100  i   001101 001110 110000   ii   0 1 10  iii   00 11 10  iv   0 11 10  v   10 0 11  vi   Let us discuss the different designs in Table 4.1.   i  In this design, Alice and Bob have the same phone number. The system  obviously will not be able to connect properly.   ii  This is much better, i.e. the code will actually work. However, the phone  numbers are quite long and therefore the design is rather inefﬁcient.   iii  Now we have a code that is much shorter and, at the same time, we have made sure that we do not use the same codeword twice. However, a closer look reveals that the system will not work. The problem here is that this code is not uniquely decodable: if you dial 10 this could mean “Carol” or also “Bob, Alice.” Or, in other words, the telephone system will never connect you to Carol, because once you dial 1, it will immediately connect you to Bob.   iv  This is the ﬁrst quite efﬁcient code that is functional. But we note some- thing: when calling Alice, why do we have to dial two zeros? After the ﬁrst zero it is already clear to whom we would like to be connected! Let us ﬁx that in design  v .   v  This is still uniquely decodable and obviously more efﬁcient than  iv . Is it the most efﬁcient code? No! Since Bob is called most often, he should be assigned the shortest codeword!   vi  This is the optimal code. Note one interesting property: even though the numbers do not all have the same length, once you ﬁnish dialing any of the three numbers, the system immediately knows that you have ﬁnished dialing. This is because no codeword is the preﬁx1 of any other codeword, i.e. it never happens that the ﬁrst few digits of one codeword are identical to another codeword. Such a code is called preﬁx-free  see Section 4.2 . Note that  iii  was not preﬁx-free: 1 is a preﬁx of 10.  1 According to the Oxford English Dictionary, a preﬁx is a word, letter, or number placed before  another.   4.2 Preﬁx-free or instantaneous codes  57  From this example we learn the following requirements that we impose on  our code design.   A code needs to be uniquely decodable.   A code should be short; i.e., we want to minimize the average codeword length Lav, which is deﬁned as follows: Lav  cid:44  r ∑   4.1   pili.  i=1  Here pi denotes the probability that the source emits the ith symbol, i.e. the probability that the ith codeword ci is selected; li is the length of the ith codeword ci; and r is the number of codewords.    We additionally require the code to be preﬁx-free. Note that this requirement is not necessary, but only convenient. However, we will later see that we lose nothing by asking for it.  Note that any preﬁx-free code is implicitly uniquely decodable, but not vice versa. We will discuss this issue in more detail in Section 4.7.  4.2 Preﬁx-free or instantaneous codes  Consider the following code with four codewords:  c1 = 0 c2 = 10 c3 = 110 c4 = 111   4.2   Note that the zero serves as a kind of “comma”: whenever we receive a zero  or the code has reached length 3 , we know that the codeword has ﬁnished. However, this comma still contains useful information about the message as there is still one codeword without it! This is another example of a preﬁx-free code. We recall the following deﬁnition.  Deﬁnition 4.1 A code is called preﬁx-free  sometimes also called instanta- neous  if no codeword is the preﬁx of another codeword.  The name instantaneous is motivated by the fact that for a preﬁx-free code we can decode instantaneously once we have received a codeword and do not need to wait for later until the decoding becomes unique. Unfortunately, in the literature one also ﬁnds that people call a preﬁx-free code a preﬁx code. This   58  Efﬁcient coding of a random message  name is confusing because rather than having preﬁxes it is the point of the code to have no preﬁx! We will stick to the name of preﬁx-free codes.  Consider next the following example:  c1 = 0 c2 = 01 c3 = 011 c4 = 111   4.3   This code is not preﬁx-free  0 is a preﬁx of 01 and 011; 01 is a preﬁx of 011 , but it is still uniquely decodable.  Exercise 4.2 Given the code in  4.3 , split the sequence 0011011110 into ♦ codewords.  Note the drawback of the code design in  4.3 : the receiver needs to wait and see how the sequence continues before it can make a unique decision about the decoding. The code is not instantaneously decodable.  Apart from the fact that they can be decoded instantaneously, another nice property of preﬁx-free codes is that they can very easily be represented by leaves of decision trees. To understand this we will next make a small detour and talk about trees and their relation to codes.  4.3 Trees and codes  The following deﬁnition is quite straightforward.  Deﬁnition 4.3  Trees  A rooted tree consists of a root with some branches, nodes, and leaves, as shown in Figure 4.1. A binary tree is a rooted tree in which each node  hence also the root  has exactly two children,2 i.e. two branches stemming forward.  The clue to this section is to note that any binary code can be represented as a binary tree. Simply take any codeword and regard each bit as a decision  2 The alert reader might wonder why we put so much emphasis on having exactly two children. It is quite obvious that if a parent node only had one child, then this node would be useless and the child could be moved back and replace its parent. The reason for our deﬁnition, however, has nothing to do with efﬁciency, but is related to the generalization to D-ary trees where every node has exactly D children. We do not cover such trees in this book; the interested reader is referred to, e.g., [Mas96].   4.3 Trees and codes  59  Figure 4.1 A rooted tree  in this case a binary tree  with a root  the node that is grounded , four nodes  including the root , and ﬁve leaves. Note that in this book we will always clearly distinguish between nodes and leaves: a node always has children, while a leaf always is an “end-point” in the tree.  whether to go up  “0”  or down3  “1” . Hence, every codeword can be rep- resented by a particular path traversing through the tree. As an example, Fig- ure 4.2 shows the binary tree of a binary code with ﬁve codewords. Note that on purpose we also keep branches that are not used in order to make sure that the tree is binary.  In Figure 4.3, we show the tree describing the preﬁx-free code given in  4.2 .  Note that here every codeword is a leaf. This is no accident. Lemma 4.4 A binary code {c1, . . . ,cr} is preﬁx-free if, and only if, in its binary tree every codeword is a leaf.  But not every leaf necessarily is a code- word; see, e.g., code  iv  in Figure 4.4.   Exercise 4.5 Prove Lemma 4.4.  Hint: Think carefully about the deﬁnition of preﬁx-free codes  see Deﬁni- ♦  tion 4.1 .  As mentioned, the binary tree of a preﬁx-free code might contain leaves that  are not codewords. Such leaves are called unused leaves.  Some more examples of trees of preﬁx-free and non-preﬁx-free codes are  shown in Figure 4.4.  3 It actually does not matter whether 1 means up and 0 down, or vice versa.  parent node  with  two children  root  branch  nodes  forward   60  Efﬁcient coding of a random message  Figure 4.2 An example of a binary tree with ﬁve codewords: 110, 0010, 001101, 001110, and 110000. At every node, going upwards corresponds to a 0, and going downwards corresponds to a 1. The node with the ground symbol is the root of the tree indicating the starting point.  Figure 4.3 Decision tree corresponding to the preﬁx-free code given in  4.2 .  Figure 4.4 Examples of codes and their corresponding trees. The examples are taken from Table 4.1. The preﬁx-free code  iv  has one unused leaf.  0  1  0010  110  0  1  0   iii    iv   00   v – vi   0  1  10  not preﬁx-free  preﬁx-free  10  11   4.3 Trees and codes  61  An important concept of trees is the depths of their leaves.  Deﬁnition 4.6 The depth of a leaf in a binary tree is the number of steps it takes when walking from the root forward to the leaf.  As an example, consider again Figure 4.4. Tree  iv  has four leaves, all of them at depth 2. Both tree  iii  and tree  v – vi  have three leaves, one at depth 1 and two at depth 2.  We now will derive some interesting properties of trees. Since codes can be represented as trees, we will then be able to apply these properties directly to codes.  Lemma 4.7  Leaf-Counting and Leaf-Depth Lemma  leaves n and their depths l1,l2, . . . ,ln in a binary tree satisfy:  The number of  n = 1 + N,  n  ∑  i=1  2−li = 1,   4.4    4.5   where N is the number of nodes  including the root .  Proof By extending a leaf we mean changing a leaf into a node by adding two branches that stem forward. In that process   we reduce the number of leaves by 1,   we increase the number of nodes by 1, and   we increase the number of leaves by 2, i.e. in total we gain one node and one leaf. This process is depicted graphically in Figure 4.5.  Figure 4.5 Extending a leaf: both the total number of nodes and the total number of leaves is increased by 1.  =⇒  −1 leaf  +1 node   62  Efﬁcient coding of a random message  To prove the ﬁrst statement  4.4 , we start with the extended root; i.e., at the beginning we have the root and n = 2 leaves. In this case we have N = 1 and  4.4  is satisﬁed. Now we can grow any tree by continuously extending some leaf, every time increasing the number of leaves and nodes by one each. We see that  4.4  remains valid. By induction this proves the ﬁrst statement.  We will prove the second statement  4.5  also by induction. We again start  with the extended root.   1  An extended root has two leaves, all at depth 1: li = 1. Hence,  n  ∑  i=1  2−li =  2  ∑  i=1  2−1 = 2· 2−1 = 1;  i.e., for the extended root,  4.5  is satisﬁed.   2  Suppose ∑n  i=1 2−li = 1 holds for an arbitrary binary tree with n leaves. Now we extend one leaf, say the nth leaf.4 We get a new tree with n cid:48  = n + 1 leaves, where  n cid:48 ∑  i=1  2−li =  + 2· 2− ln+1   cid:124   cid:123  cid:122   cid:125   new leaves at depth ln + 1  unchanged  2−li  n−1 ∑ i=1  cid:124   cid:123  cid:122   cid:125  leaves n−1 ∑ i=1 n  ∑  i=1  =  =  2−li + 2−ln  2−li = 1.  Here the last equality follows from our assumption that ∑n Hence, by extending one leaf, the second statement continues to hold.  i=1 2−li = 1.   3  Since any tree can be grown by continuously extending some leaves, the  proof follows by induction.  We are now ready to apply our ﬁrst insights about trees to codes.  4.4 The Kraft Inequality  The following theorem is very useful because it gives us a way of ﬁnding out whether a preﬁx-free code exists or not.  4 Since the tree is arbitrary, it does not matter how we number the leaves!   4.6    4.7    4.8    4.9    4.4 The Kraft Inequality  63  Theorem 4.8  Kraft Inequality  There exists a binary preﬁx-free code with r codewords of lengths l1,l2, . . . ,lr if, and only if,  r  ∑  i=1  2−li ≤ 1.  If  4.10  is satisﬁed with equality, then there are no unused leaves in the tree.  Example 4.9 Let l1 = 3, l2 = 4, l3 = 4, l4 = 4, l5 = 4. Then  2−3 + 4· 2−4 =  1 8 +  4 16 =  3 8 ≤ 1;  i.e., there exists a binary preﬁx-free code consisting of ﬁve codewords with the given codeword lengths.  On the other hand, we cannot ﬁnd any preﬁx-free code with ﬁve codewords  of lengths l1 = 1, l2 = 2, l3 = 3, l4 = 3, and l5 = 4 because  17 16 > 1. These two examples are shown graphically in Figure 4.6.  2−1 + 2−2 + 2· 2−3 + 2−4 =   4.10    4.11    4.12  ♦  Figure 4.6 Examples of the Kraft Inequality.  Proof of the Kraft Inequality We prove the two directions separately. =⇒: Suppose that there exists a binary preﬁx-free code with the given code- word lengths. From Lemma 4.4 we know that all r codewords of a binary preﬁx-free code are leaves in a binary tree. The total number n of  used and unused  leaves in this tree can therefore not be smaller than r, i.e.  r ≤ n.   4.13   3  4  4  4  4   64  Efﬁcient coding of a random message  Hence,  r  ∑  i=1  2−li ≤  n  ∑  i=1  2−li = 1,   4.14   where the last equality follows from the Leaf-Depth Lemma  Lem- ma 4.7 .  ⇐=: Suppose that ∑r  i=1 2−li ≤ 1. We now can construct a preﬁx-free code as  follows:  Step 1 Start with the extended root, i.e. a tree with two leaves, set i = 1,  and assume, without loss of generality, that l1 ≤ l2 ≤ ··· ≤ lr. Step 2 If there is an unused leaf at depth li, put the ith codeword there. Note that there could be none because li can be strictly larger than the current depth of the tree. In this case, extend any unused leaf to depth li, and put the ith codeword to one of the new leaves.  Step 3 If i = r, stop. Otherwise i → i + 1 and go to Step 2. We only need to check that Step 2 is always possible, i.e. that there is always some unused leaf available. To that goal, note that if we get to Step 2, we have already put i−1 codewords into the tree. From the Leaf- Depth Lemma  Lemma 4.7  we know that  n  n  +  j=1  1 =  ∑  2−l j  2−˜l j  2−˜l j =  ∑  cid:124   cid:123  cid:122   cid:125  where ˜l j are the depths of the leaves in the tree at that moment; i.e.,  ˜l1, . . . , ˜li−1  =  l1, . . . ,li−1  and ˜li, . . . , ˜ln are the depths of the  so far  unused leaves. Now note that in our algorithm i ≤ r, i.e.  i−1 ∑ j=1  cid:124   cid:123  cid:122   cid:125   unused leaves  used leaves   4.15   j=i  ,  i−1 ∑ j=1  2−l j <  r  ∑  j=1  2−l j ≤ 1,  where the last inequality follows by assumption. Hence,  +  n  ∑  j=i  2−˜l j = 1 =⇒  n  ∑  j=i  2−˜l j > 0  2−l j  i−1 ∑ j=1  cid:124   cid:123  cid:122   cid:125 <1  and there still must be some unused leaves available!   4.16    4.17    4.5 Trees with probabilities  65  Figure 4.7 Rooted tree with probabilities.  4.5 Trees with probabilities  We have seen already in Section 4.1 that for codes it is important to consider the probabilities of the codewords. We therefore now introduce probabilities in our trees.  Deﬁnition 4.10 A rooted tree with probabilities is a ﬁnite rooted tree with probabilities assigned to each node and leaf such that    the probability of a node is the sum of the probabilities of its children, and   the root has probability 1.  An example of a rooted tree with probabilities is given in Figure 4.7. Note that the probabilities can be seen as the overall probability of passing through a particular node  or reaching a particular leaf  when making a random walk from the root to a leaf. Since we start at the root, the probability that our path goes through the root is always 1. Then, in the example of Figure 4.7, we have an 80% chance that our path will go through node 2 and a 10% chance to end up in leaf 4.  Since in a preﬁx-free code all codewords are leaves and we are particularly interested in the average codeword length, we are very much interested in the average depth of the leaves in a tree  where for the averaging operation we use the probabilities in the tree . Luckily, there is an elegant way to compute this average depth, as shown in the following lemma.  Lemma 4.11  Path Length Lemma   In a rooted tree with probabilities, the  node 1  1  leaf 1  0.2  node 2  0.8   66  Efﬁcient coding of a random message  average depth Lav of the leaves is equal to the sum of the probabilities of all nodes  including the root .  To clarify our notation we refer to leaf probabilities by small pi while node  probabilities are denoted by capital P cid:96 . Example 4.12 Consider the tree of Figure 4.7. We have four leaves: one at depth l1 = 1 with a probability p1 = 0.2, one at depth l2 = 2 with a probability p2 = 0.5, and two at depth l3 = l4 = 3 with probabilities p3 = 0.2 and p4 = 0.1, respectively. Hence, the average depth of the leaves is given by  4  ∑  i=1  Lav =  pili = 0.2· 1 + 0.5· 2 + 0.2· 3 + 0.1· 3 = 2.1.   4.18   According to Lemma 4.11, this must be equal to the sum of the node probabil- ities:  Lav = P1 + P2 + P3 = 1 + 0.8 + 0.3 = 2.1.  Proof of Lemma 4.11 The lemma is easiest understood when looking at a par- ticular example. Let us again consider the tree of Figure 4.7: the probability p1 = 0.2 of leaf 1 needs to be counted once only, which is the case as it is only part of the probability of the root P1 = 1. The probability p2 = 0.5 must be counted twice. This is also the case because it is contained in the root proba- bility P1 = 1 and also in the probability of the second node P2 = 0.8. Finally, the probabilities of leaf 3 and leaf 4, p3 = 0.2 and p4 = 0.1, are counted three times: they are part of P1, P2, and P3:  Lav = 2.1  = 1· 0.2 + 2· 0.5 + 3· 0.2 + 3· 0.1 = 1·  0.2 + 0.5 + 0.2 + 0.1  + 1·  0.5 + 0.2 + 0.1   + 1·  0.2 + 0.1   = 1· P1 + 1· P2 + 1· P3 = P1 + P2 + P3.   4.19  ♦   4.20   4.21    4.22   4.23   4.24   4.6 Optimal codes: Huffman code  Let us now connect the probabilities in the tree with the probabilities of the code, or actually, more precisely, the probabilities of the random messages that   4.6 Optimal codes: Huffman code  67  shall be represented by the code. We assume that we have in total r different message symbols. Let the probability of the ith message symbol be pi and let the length of the corresponding codeword representing this symbol be li. Then the average length of the code is given by r  With no loss in generality, the pi may be taken in nonincreasing order. If the lengths li are then not in the opposite order, i.e. we do not have both  Lav =  pili.  ∑  i=1  p1 ≥ p2 ≥ p3 ≥ ··· ≥ pr  l1 ≤ l2 ≤ l3 ≤ ··· ≤ lr,  and  then the code is not optimal in the sense that we could have a shorter average length by reassigning the codewords to different symbols. To prove this claim, suppose that for some i and j with i < j we have both  pi > p j  and  li > l j.  In computing the average length, originally, the sum in  4.25  contains, among others, the two terms  By interchanging the codewords for the ith and jth symbols, we get the terms  old:  pili + p jl j.  new:  pil j + p jli,  while the remaining terms are unchanged. Subtracting the old from the new we see that new− old:   pil j + p jli −  pili + p jl j  = pi l j − li  + p j li − l j   =  pi − p j  l j − li  < 0.  From  4.28  this is a negative number, i.e. we can decrease the average code- word length by interchanging the codewords for the ith and jth symbols. Hence the new code with exchanged codewords for the ith and jth symbols is better than the original code – which therefore cannot have been optimal.  We will now examine the optimal binary code which is called the Huffman code due to its discoverer. The trick of the derivation of the optimal code is the insight that the corresponding code tree has to be grown backwards, starting   4.25    4.26    4.27    4.28    4.29    4.30    4.31   4.32   4.33    68  Efﬁcient coding of a random message  Figure 4.8 Code performance and unused leaves: by deleting the unused leaf and moving its sibling to the parent, we can improve on the code’s perfor- mance.  Figure 4.9 Improving a code by removing an unused leaf.  from the leaves  and not, as might be intuitive at a ﬁrst glance, starting from the root .  The clue of binary Huffman coding lies in two basic observations. The ﬁrst  observation is as follows.  Lemma 4.13 no unused leaf.  In a binary tree of an optimal binary preﬁx-free code, there is  Proof Suppose that the tree of an optimal code has an unused leaf. Then we can delete this leaf and move its sibling to the parent node; see Figure 4.8. By doing so we reduce Lav, which contradicts our assumption that the original code was optimal.  tree  rest of tree  unused leaf  0.3  0.2  0.5   i    4.6 Optimal codes: Huffman code  69  Example 4.14 As an example consider the two codes given in Figure 4.9, both of which have three codewords. Code  i  has an average length5 of Lav = 2, and code  ii  has an average length of Lav = 1.5. Obviously, code  ii  per- ♦ forms better.  The second observation basically says that the two most unlikely symbols  must have the longest codewords.  Lemma 4.15 There exists an optimal binary preﬁx-free code such that the two least likely codewords only differ in the last digit, i.e. the two most unlikely codewords are siblings.  Proof Since we consider an optimal code, the codewords that correspond to the two least likely symbols must be the longest codewords  see our discussion after  4.27  . If they have the same parent node, we are done. If they do not have the same parent node, this means that there exist other codewords of the same length  because we know from Lemma 4.13 that there are no unused leaves . In this case, we can simply swap two codewords of equal maximum length in such a way that the two least likely codewords have the same parent, and we are done.  Because of Lemma 4.13 and the Path Length Lemma  Lemma 4.11 , we see that the construction of an optimal binary preﬁx-free code for an r-ary random message U is equivalent to constructing a binary tree with r leaves such that the sum of the probabilities of the nodes is minimum when the leaves are assigned the probabilities pi for i = 1,2, . . . ,r:  Lav =  .  P cid:96    cid:96 =1  N∑  cid:124  cid:123  cid:122  cid:125   −→minimize!  PN = pr−1 + pr.  But Lemma 4.15 tells us how we may choose one node in an optimal code tree, namely as the parent of the two least likely leaves pr−1 and pr:  So we have ﬁxed one P cid:96  in  4.34  already. But, if we now pruned our binary tree at this node to make it a leaf with probability p = pr−1 + pr, it would become one of  r− 1  leaves in a new tree. Completing the construction of the optimal code would then be equivalent to constructing a binary tree with these  5 Remember Lemma 4.11 to compute the average codeword length: summing the node probabil- ities. In code  i  we have P1 = 1 and P2 = P3 = 0.5  note that the unused leaf has, by deﬁnition, zero probability , and in code  ii  P1 = 1 and P2 = 0.5.   4.34    4.35    Efﬁcient coding of a random message  70  r− 1  leaves such that the sum of the probabilities of the nodes is minimum:  4.36   Lav =  P cid:96   N−1 ∑  cid:96 =1  cid:124   cid:123  cid:122   cid:125   −→minimize!  .  + PN cid:124  cid:123  cid:122  cid:125   optimally chosen  Again Lemma 4.15 tells us how to choose one node in this new tree, and so on. We have thus proven the validity of the following algorithm.  Example 4.16 In Figure 4.10 we show the procedure of producing a Huffman code for the example of a random message with four possible symbols with probabilities p1 = 0.4, p2 = 0.3, p3 = 0.2, p4 = 0.1. We see that the average codeword length of this Huffman code is  Lav = 0.4· 1 + 0.3· 2 + 0.2· 3 + 0.1· 3 = 1.9.  Using Lemma 4.11 this can be computed much easier as follows:  Lav = P1 + P2 + P3 = 1 + 0.6 + 0.3 = 1.9.   4.37    4.38  ♦  Note that the code design process is not unique in several respects. Firstly, the assignment of the 0 or 1 digits to the codewords at each forking stage is arbitrary, but this produces only trivial differences. Usually, we will stick to the convention that going upwards corresponds to 0 and downwards to 1. Secondly, when there are more than two least likely  active  nodes leaves, it does not matter which we choose to combine. The resulting codes can have codewords of different lengths; however, the average codeword length will always be the same. Example 4.17 As an example of different Huffman encodings of the same random message, let p1 = 0.4, p2 = 0.2, p3 = 0.2, p4 = 0.1, p5 = 0.1. Fig- ure 4.11 shows three different Huffman codes for this message: the list of  Huffman’s Algorithm for Optimal Binary Codes  Step 1 Create r leaves corresponding to the r possible symbols and assign their probabilities p1, . . . , pr. Mark these leaves as active. Step 2 Create a new node that has the two least likely active leaves or nodes as children. Activate this new node and deactivate its children.  Step 3 If there is only one active node left, root it. Otherwise, go to  Step 2.   4.6 Optimal codes: Huffman code  71  Figure 4.10 Creation of a binary Huffman code. Active nodes and leaves are shaded.  0.1  Step 1  0.4  0.3  0.2  0.4  0.3  0.2  0.1  1  0.3  Step 2, second time  0  1  1  0.6  0.6  0.6  Step 3  0.3   72  Efﬁcient coding of a random message  Figure 4.11 Different binary Huffman codes for the same random message.  0  1  1  0.6  0.4  0.2  1  1  0.6  0.4  0.2  0.6  0.2   4.7 Types of codes  73  codeword lengths are  1,2,3,4,4 ,  1,3,3,3,3 , and  2,2,2,3,3 , respectively. ♦ But all of these codes have the same performance, Lav = 2.2. Exercise 4.18 ure 4.11  yourself.  Try to generate all three codes of Example 4.17  see Fig- ♦  4.7 Types of codes  Note that in Section 4.1 we have restricted ourselves to preﬁx-free codes. So, up to now we have only proven that Huffman codes are the optimal codes under the assumption that we restrict ourselves to preﬁx-free codes. We would now like to show that Huffman codes are actually optimal among all useful codes. To reach that goal, we need to come back to a more precise deﬁnition of “useful codes,” i.e. we continue the discussion that we started in Section 4.1. Let us consider an example with a random message U with four different sym- bols and let us design various codes for this message as shown in Table 4.2.  Table 4.2 Various codes for a random message with four possible values  U Code  i  Code  ii   Code  iii  Code  iv   a b c d  0 0 1 1  0 010 01 10  10 00 11 110  0 10 110 111  We discuss these different codes.  Code  i  is useless because some codewords are used for more than one sym-  bol. Such a code is called singular.  Code  ii  is nonsingular. But we have another problem: if we receive 010 we have three different possibilities how to decode it: it could be  010  giving us b, or it could be  0  10  leading to ad, or it could be  01  0  corresponding to ca. Even though nonsingular, this code is not uniquely decodable and therefore in practice is as useless as code  i .6  6 Note that adding a comma between the codewords is not allowed because in this case we change the code to be ternary, i.e. the codewords contain three different letters “0”, “1”, and “,” instead of only two “0” and “1”. By the way, it is not very difﬁcult to generalize all results given in this chapter to D-ary codes. See, for example, [Mas96]. In this book, we will stick to binary codes.   74  Efﬁcient coding of a random message  Figure 4.12 Set of all codes.  Code  iii  is uniquely decodable, even though it is not preﬁx-free! To see this, note that in order to distinguish between c and d we only need to wait for the next 1 to show up: if the number of 0s in between is even, we decode 11, otherwise we decode 110. Example:  11000010 =  11  00  00  10  110000010 =  110  00  00  10   =⇒ cbba, =⇒ dbba.   4.39   4.40   So in a uniquely decodable but not preﬁx-free code we may have to delay the decoding until later.  Code  iv  is preﬁx-free and therefore trivially uniquely decodable.  We see that the set of all possible codes can be grouped as shown in Fig- ure 4.12. We are only interested in the uniquely decodable codes. But so far we have restricted ourselves to preﬁx-free codes. So the following question arises: is there a uniquely decodable code that is not preﬁx-free, but that has a better performance than the best preﬁx-free code  i.e. the corresponding Huff- man code ?  Luckily the answer to this question is No, i.e. the Huffman codes are the best uniquely decodable codes. This can be seen from the following theorem.  Theorem 4.19  McMillan’s Theorem   The codeword lengths li of any  all codes  uniquely decodable codes  nonsingular codes   uniquely decodable code must satisfy the Kraft Inequality  4.7 Types of codes  r  ∑  i=1  2−li ≤ 1.  75   4.41   Why does this help to answer our question about the most efﬁcient uniquely decodable code? Well, note that we know from Theorem 4.8 that every preﬁx- free code also satisﬁes  4.41 . So, for any uniquely decodable, but non-preﬁx- free code with given codeword lengths, one can ﬁnd another code with the same codeword lengths that is preﬁx-free. But if the codeword lengths are the same, the performance is identical! Hence, there is no gain in designing a non- preﬁx-free code.  Proof of Theorem 4.19 Suppose we are given a random message U that takes on r possible values u ∈ U  here the set U denotes the message alphabet . Suppose further that we have a uniquely decodable code that assigns to every possible symbol u ∈ U a certain codeword of length l u . Now choose an arbitrary positive integer ν and design a new code for a vector of ν symbols u =  u1,u2, . . . ,uν   ∈ Uν = U×···× U by simply con- catenating the original codewords. Example 4.20 Consider a ternary message with the possible values u = a, u = b, or u = c, i.e. U = {a,b,c}. If the probabilities of these possible values are  Pr[U = a] =  1 2 , Pr[U = b] = Pr[U = c] =  1 4 ,   4.42   a binary  single-letter  Huffman code would map   4.43  If we now choose ν = 3, we get a new source with 33 = 27 possible symbols, namely  b  cid:55 → 10,  c  cid:55 → 11.  a  cid:55 → 0,  U3 = {aaa,aab,aac,aba,abb,abc,aca,acb,acc, baa,bab,bac,bba,bbb,bbc,bca,bcb,bcc, caa,cab,cac,cba,cbb,cbc,cca,ccb,ccc}.  The corresponding 27 codewords are then as follows  given in the same order :  {000,0010,0011,0100,01010,01011,0110,01110,01111, 1000,10010,10011,10100,101010,101011,10110,101110,101111, 1100,11010,11011,11100,111010,111011,11110,111110,111111}.   4.44    4.45  ♦   76  Efﬁcient coding of a random message  The clue observation now is that because the original code was uniquely decodable, it immediately follows that this new concatenated code also must be uniquely decodable.  Exercise 4.21 Explain this clue observation, i.e. explain why the new con- catenated code is also uniquely decodable.  Hint: Note that the codewords of the new code consist of a sequence of ♦  uniquely decodable codewords.  The lengths of the new codewords are given by  ˜l u  =  l u j .  ν ∑  j=1  ˜lmax = νlmax.  Let lmax be the maximal codeword length of the original code. Then the new code has a maximal codeword length ˜lmax satisfying  We now compute the following:   cid:32  ∑  u∈U  2−l u  cid:33 ν  2−l u1  cid:33  cid:32  ∑ u1∈U u2∈U··· ∑ ∑ 2−l u1 −l u2 −···−l uν    2−l u2  cid:33 ··· cid:32  ∑ uν∈U u2∈U 2−l u1 2−l u2 ···2−l uν    uν∈U  2−l uν   cid:33   4.48   = cid:32  ∑ = ∑ u1∈U = ∑ u∈Uν = ∑ u∈Uν = ∑ u∈Uν  2−∑ν  j=1 l u j   2−˜l u .  Here  4.48  follows by writing the exponentiated sum as a product of ν sums; in  4.50  we combine the ν sums over u1, . . . ,uν into one huge sum over the ν-vector u; and  4.52  follows from  4.46 .  Next we will rearrange the order of the terms by collecting all terms with  the same exponent together:  2−˜l u  =  w m 2−m,  ˜lmax∑  m=1  ∑ u∈Uν  where w m  counts the number of such terms with equal exponent, i.e. w m  denotes the number of codewords of length m in the new code.   4.46    4.47    4.49    4.50    4.51    4.52    4.53    4.7 Types of codes  77  Example 4.22  Continuation from Example 4.20  We see from  4.45  that the new concatenated code has one codeword of length 3, six codewords of length 4, twelve codewords of length 5, and eight codewords of length 6. Hence,  2−˜l u  = 1· 2−3 + 6· 2−4 + 12· 2−5 + 8· 2−6,   4.54   ∑ u∈Uν  i.e.  w m  =  for m = 3, 1 for m = 4, 6 12 for m = 5, for m = 6, 8 otherwise. 0 Also note that ˜lmax = 6 = ν · lmax = 3· 2 in this case. We combine  4.53  and  4.52  and use  4.47  to write     cid:32  ∑  u∈U  2−l u  cid:33 ν  =  νlmax∑  m=1  w m 2−m.   4.56   Note that since the new concatenated code is uniquely decodable, every codeword of length m is used at most once. But in total there are only 2m different sequences of length m, i.e. we know that  w m  ≤ 2m.  Thus,  or   cid:32  ∑  u∈U  2−l u  cid:33 ν  =  νlmax∑  m=1  w m 2−m ≤  νlmax∑  m=1  2m2−m = νlmax   4.58   2−l u  ≤  νlmax   1 ν  .  ∑ u∈U  At this stage we are back to an expression that depends only on the original uniquely decodable code. So forget about the trick with the new concatenated code, but simply note that we have shown that for any uniquely decodable code and any positive integer ν, expression  4.59  must hold! Also note that we can choose ν freely here.  Note further that for any ﬁnite value of lmax one can show that   νlmax   1 ν  = 1.  lim ν→∞   4.55   ♦   4.57    4.59    4.60    78  Efﬁcient coding of a random message  Hence, by choosing ν extremely large  i.e. we let ν tend to inﬁnity  we have  2−l u  ≤ 1  ∑ u∈U   4.61   as we wanted to prove.  4.8 Some historical background  David A. Huffman had ﬁnished his B.S. and M.S. in electrical engineering and also served in the U.S. Navy before he became a Ph.D. student at the Massachusetts Institute of Technology  MIT . There, in 1951, he attended an information theory class taught by Professor Robert M. Fano who was working at that time, together with Claude E. Shannon, on ﬁnding the most efﬁcient code, but could not solve the problem. So Fano assigned the question to his students in the information theory class as a term paper. Huffman tried for a long time to ﬁnd a solution and was about to give up when he had the sudden inspiration to start building the tree backwards from leaves to root instead from root to leaves. Once he had understood this, he was quickly able to prove that his code was the most efﬁcient one. Naturally, Huffman’s term paper was later published.  Huffman became a faculty member of MIT in 1953, and later, in 1967, he moved to the University of California, Santa Cruz, where he stayed until his retirement in 1994. He won many awards for his accomplishments, e.g. in 1988 the Richard Hamming Medal from the Institute of Electrical and Electronics Engineers  IEEE . Huffman died in 1998. See [Sti91] and [Nor89].  4.9 Further reading  For an easy-to-read, but precise, introduction to coding and trees, the lecture notes [Mas96] of Professor James L. Massey from ETH Zurich are highly rec- ommended. There the interested reader will ﬁnd a straightforward way to gen- eralize the concept of binary codes to general D-ary codes using D-ary trees. Moreover, in [Mas96] one also ﬁnds the concept of block codes, i.e. codes with a ﬁxed codeword length. Some of the best such codes are called Tunstall codes [Tun67].   References References  79  [Mas96] James L. Massey, Applied Digital Information Theory I and II, Lecture notes, Signal and Information Processing Laboratory, ETH Zurich, 1995 1996. Available: http:  www.isiweb.ee.ethz.ch archive massey scr   [Nor89] Arthur L. Norberg, “An interview with Robert M. Fano,” Charles Babbage  Institute, Center for the History of Information Processing, April 1989.  [Sti91] Gary Stix, “Proﬁle: Information theorist David A. Huffman,” Scientiﬁc Amer- ican  Special Issue on Communications, Computers, and Networks , vol. 265, no. 3, September 1991.  [Tun67] Brian P. Tunstall, “Synthesis of noiseless compression codes,” Ph.D. disserta-  tion, Georgia Institute of Technology, September 1967.    5  Entropy and Shannon’s Source Coding Theorem  Up to this point we have been concerned with coding theory. We have de- scribed codes and given algorithms of how to design them. And we have eval- uated the performance of some particular codes. Now we begin with informa- tion theory, which will enable us to learn more about the fundamental proper- ties of general codes without having actually to design them.  Basically, information theory is a part of physics and tries to describe what information is and how we can work with it. Like all theories in physics it is a model of the real world that is accepted as true as long as it predicts how nature behaves accurately enough.  In the following we will start by giving some suggestive examples to mo- tivate the deﬁnitions that follow. However, note that these examples are not a justiﬁcation for the deﬁnitions; they just try to shed some light on the reason why we will deﬁne these quantities in the way we do. The real justiﬁcation of all deﬁnitions in information theory  or any other physical theory  is the fact that they turn out to be useful.  5.1 Motivation  We start by asking the question: what is information?  Let us consider some examples of sentences that contain some “informa-  tion.”   The weather will be good tomorrow.   The weather was bad last Sunday.   The president of Taiwan will come to you tomorrow and will give you one  million dollars.  The second statement seems not very interesting as you might already know   82  Entropy and Shannon’s Source Coding Theorem  what the weather was like last Sunday. The last statement is much more excit- ing than the ﬁrst two and therefore seems to contain much more information. But, on the other hand, do you actually believe it? Do you think it is likely that you will receive one million dollars tomorrow?  Let us consider some easier examples.  This question has only two possible answers: “yes” or “no.”    You ask: “Is the temperature in Taiwan currently above 30 degrees?”   You ask: “The president of Taiwan has spoken with a certain person from Here, the question has about 400 000 possible answers  since Hsinchu has  Hsinchu today. With whom?”  about 400 000 inhabitants .  Obviously the second answer provides you with a much bigger amount of in- formation than the ﬁrst one. We learn the following.  Here is another example.    You observe a gambler throwing a fair dice. There are six possible outcomes {1,2,3,4,5,6}. You note the outcome and then tell it to a friend. By doing so you give your friend a certain amount of information.   Next you observe the gambler throwing the dice three times. Again, you note the three outcomes and tell them to your friend. Obviously, the amount of information that you give to your friend this time is three times as much as the ﬁrst time.  So we learn the following.  Now we face a new problem: regarding the example of the gambler above we see that in the ﬁrst case we have r = 6 possible answers, while in the second case we have r = 63 = 216 possible answers. Hence in the second experiment there are 36 times more possible outcomes than in the ﬁrst experiment! But we would like to have only a three times larger amount of information. So how do we solve this?  Idea: use a logarithm. Then the exponent 3 will become a factor exactly as we wish: logb 63 = 3· logb 6.  The number of possible answers r should be linked to “information.”  “Information” should be additive in some sense.   Exactly these observations have been made by the researcher Ralph Hartley  in 1928 in Bell Labs [Har28]. He gave the following deﬁnition. Deﬁnition 5.1 We deﬁne the following measure of information:  5.1 Motivation  ˜I U   cid:44  logb r,  83   5.1   where r is the number of all possible outcomes of a random message U.  Using this deﬁnition we can conﬁrm that it has the wanted property of addi-  tivity:  ˜I U1,U2, . . . ,Un  = logb rn = n· logb r = n· ˜I U .   5.2   Hartley also correctly noted that the basis b of the logarithm is not really impor- tant for this measure. It only decides on the unit of information. So, similarly to the fact that 1 km is the same distance as 1000 m, b is only a change of units without actually changing the amount of information it describes.  For two important and one unimportant special cases of b it has been agreed  to use the following names for these units:  b = 2  log2 : b = e  ln : b = 10  log10 :  bit, nat  natural logarithm , Hartley.  Note that the unit Hartley has been chosen in honor of the ﬁrst researcher who made the ﬁrst  partially correct  attempt at deﬁning information. However, as nobody in the world ever uses the basis b = 10 for measuring information, this honor is questionable.  The measure ˜I U  is the right answer to many technical problems.  Example 5.2 A village has eight telephones. How long must the phone num- ber be? Or, asked differently: how many bits of information do we need to send to the central ofﬁce so that we are connected to a particular phone?  8 phones =⇒ log2 8 = 3 bits.  We choose the following phone numbers:  {000, 001, 010, 011, 100, 101, 110, 111}.   5.3    5.4  ♦  In spite of its usefulness, Hartley’s deﬁnition had no effect whatsoever in the world. That’s life. . . On the other hand, it must be admitted that Hartley’s deﬁnition has a fundamental ﬂaw. To realize that something must be wrong, note that according to  5.1  the smallest nonzero amount of information is   84  Entropy and Shannon’s Source Coding Theorem  Figure 5.1 Two hats with four balls each.  log2 2 = 1 bit. This might sound like only a small amount of information, but actually 1 bit can be a lot of information! As an example, consider the 1-bit  yes or no  answer if a man asks a woman whether she wants to marry him. If you still do not believe that one bit is a huge amount of information, consider the following example.  Example 5.3 Currently there are 6 902 106 897 persons living on our planet  U.S. Census Bureau, 25 February 2011, 13:43 Taiwan time . How long must a binary telephone number U be if we want to be able to connect to every person? According to Hartley we need ˜I U  = log2 6902106897   cid:39  32.7 bits. So with only 33 bits we can address every single person on this planet. Or, in other words, we only need 33 times 1 bit in order to distinguish every human ♦ being alive.  We see that 1 bit is a lot of information and it cannot be that this is the  smallest amount of  nonzero  information.  To understand more deeply what is wrong, consider the two hats shown in Figure 5.1. Each hat contains four balls, where the balls can be either white or black. Let us draw one ball at random and let U be the color of the ball. In hat A we have r = 2 colors: black and white, i.e. ˜I UA  = log2 2 = 1 bit. In hat B we also have r = 2 colors and hence also ˜I UB  = 1 bit. But obviously, we get less information if in hat B black shows up, since we somehow expect black to show up in the ﬁrst place. Black is much more likely!  We realize the following.  This was observed for the ﬁrst time by Claude Elwood Shannon in 1948 in  black and white balls  A:  A proper measure of information needs to take into account the proba- bilities of the various possible events.   5.1 Motivation  85  his landmark paper “A mathematical theory of communication” [Sha48]. This paper has been like an explosion in the research community!1  Before 1948, the engineering community was mainly interested in the be- havior of a sinusoidal waveform that is passed through a communication sys- tem. Shannon, however, asked why we want to transmit a deterministic sinu- soidal signal. The receiver already knows in advance that it will be a sinus, so it is much simpler to generate one at the receiver directly rather than to trans- mit it over a channel! In other words, Shannon had the fundamental insight that we need to consider random messages rather than deterministic messages whenever we deal with information.  Let us go back to the example of the hats in Figure 5.1 and have a closer  look at hat B.    There is one chance out of four possibilities that we draw a white ball.  Since we would like to use Hartley’s measure here, we recall that the quantity r inside the logarithm in  5.1  is “the number of all possible out- comes of a random message.” Hence, from Hartley’s point of view, we will see one realization out of r possible realizations. Translated to the case of the white ball, we see that we have one realization out of four possible real- izations, i.e.  log2 4 = 2 bits   5.5   of information.  ball.    On the other hand, there are three chances out of four that we draw a black Here we cannot use Hartley’s measure directly. But it is possible to trans- late the problem into a form that makes it somehow accessible to Hartley: we need to “normalize” the statement into a form that gives us one realization out of r. This can be done if we divide everything by 3, the number of black balls: we have one chance out of 4 3 possibilities  whatever this means , or, stated differently, we have one realization out of 4 3 possible “realizations,” i.e.  log2  4 3 = 0.415 bits   5.6   of information.  1 Besides the amazing accomplishment of inventing information theory, at the age of 21 Shan- non also “invented” the computer in his Master thesis [Sha37]! He proved that electrical circuits can be used to perform logical and mathematical operations, which was the foundation of dig- ital computer and digital circuit theory. It is probably the most important Master thesis of the twentieth century! Incredible, isn’t it?   86  Entropy and Shannon’s Source Coding Theorem  So now we have two different values depending on what color we get. How shall we combine them to one value that represents the information? The most obvious choice is to average it, i.e. we weight the different information values according to their probabilities of occurrence:  1 4 · 2 bits +  3 4 · 0.415 bits = 0.811 bits  1 4  log2 4 +  log2  3 4  4 3 = 0.811 bits.   5.7    5.8   or  We see the following.  We end this introductory section by pointing out that the given three moti-  vating ideas, i.e.   1  the number of possible answers r should be linked to “information”;  2  “information” should be additive in some sense; and  3  a proper measure of information needs to take into account the probabili-  ties of the various possible events,  are not sufﬁcient to exclusively specify  5.9 . The interested reader can ﬁnd in Appendix 5.8 some more information on why Shannon’s measure should be deﬁned like  5.9  and not differently.  5.2 Uncertainty or entropy  5.2.1 Deﬁnition  We now formally deﬁne the Shannon measure of “self-information of a source.” Due to its relationship with a corresponding concept in different areas of phys- ics, Shannon called his measure entropy. We will stick to this name as it is standard in the whole literature. However, note that uncertainty would be a far more precise description.  Shannon’s measure of information is an “average Hartley information”:  r  ∑ i=1  pi log2  1 pi  = −  r  ∑ i=1  pi log2 pi,   5.9   where pi denotes the probability of the ith possible outcome.   5.2 Uncertainty or entropy  87  Deﬁnition 5.4  Entropy  The uncertainty or entropy of a random message U that takes on r different values with probability pi, i = 1, . . . ,r, is deﬁned as  H U   cid:44  −  r  ∑  i=1  pi logb pi.  Remark 5.5 What happens if pi = 0? Remember that logb 0 = −∞. However, also note that pi = 0 means that the symbol i never shows up. It therefore should not contribute to the uncertainty. Luckily this is the case:  t logb t = 0,  lim t→0  i.e. we do not need to worry about this case.  So we note the following.  As in the case of the Hartley measure of information, b denotes the unit of  uncertainty:  bit, nat,  b = 2 : b = e : b = 10 : Hartley.  logb ξ =  log2 ξ log2 b ,  If the base of the logarithm is not speciﬁed, then we can choose it freely. How- ever, note that the units are very important. A statement “H U  = 0.43” is completely meaningless: since  0.43 could mean anything as, e.g.,  if b = 2 : H U  = 0.43 bits, if b = e : H U  = 0.43 nats  cid:39  0.620 bits, if b = 256 = 28 : H U  = 0.43 “bytes” = 3.44 bits.  Note that the term bits is used in two ways: its ﬁrst meaning is the unit of entropy when the base of the logarithm is chosen to be 2; its second meaning is binary digits, i.e. in particular the number of digits of a binary codeword.   5.10    5.11    5.12   5.13   5.14    5.15    5.16   5.17   5.18   Whenever we sum over pi logb pi, we implicitly assume that we exclude all indices i with pi = 0.   88  Entropy and Shannon’s Source Coding Theorem  Remark 5.6 Shannon’s deﬁnition of entropy reduces to Hartley’s measure:  It is worth mentioning that if all r events are equally likely,  pi =  1 r , ∀i : H U  = −  r  ∑  i=1  1 r  logb  1 r =  1 r  logb r·  = logb r.   5.19   Remark 5.7 Be careful not to confuse uncertainty with information. For motivation purposes, in Section 5.1 we talked a lot about “information.” How- ever, what we actually meant there is “self-information” or, more nicely put, “uncertainty.” You will learn in Chapter 6 that information is what you get by reducing uncertainty and see a formal deﬁnition of information there.  Another important observation is that the entropy of U does not depend on the different possible values that U can take on, but only on the probabilities of these values. Hence,  r  1  i=1  ∑  cid:124  cid:123  cid:122  cid:125 =r  and  prob. 1 2  U ∈ cid:8  1  cid:124  cid:123  cid:122  cid:125 with V ∈ cid:8  34  cid:124  cid:123  cid:122  cid:125 with  , 2   cid:124  cid:123  cid:122  cid:125 with  prob. 1 3  , 3   cid:124  cid:123  cid:122  cid:125 with  prob. 1 6  , 512   cid:124  cid:123  cid:122  cid:125 with  prob. 1 3  , 981   cid:124  cid:123  cid:122  cid:125 with  prob. 1 6   cid:9    cid:9    5.20    5.21   prob. 1 2 have both the same entropy, which is 1 3  H U  = H V   = −  1 2 −  log2  1 2  log2  1 3 −  1 6  log2  1 6  cid:39  1.46 bits.   5.22   5.2.2 Binary entropy function  One special case of entropy is so important that we introduce a speciﬁc name. Deﬁnition 5.8  Binary entropy function  values u1 and u2 such that Pr[U = u1] = p and Pr[U = u2] = 1− p, then  If U is binary with two possible  H U  = Hb p ,  where Hb ·  is called the binary entropy function and is deﬁned as p ∈ [0,1].  Hb p   cid:44  −plog2 p−  1− p log2 1− p ,  The function Hb ·  is shown in Figure 5.2.  Exercise 5.9 for p = 1 2.  Show that the maximal value of Hb p  is 1 bit and is taken on ♦   5.23    5.24    5.2 Uncertainty or entropy  89  Figure 5.2 Binary entropy function Hb p  as a function of the probability p.  5.2.3 The Information Theory Inequality  The following inequality does not really have a name, but since it is so impor- tant in information theory, we will follow James Massey, retired professor at ETH in Zurich, and call it the Information Theory Inequality or the IT Inequal- ity. Lemma 5.10  IT Inequality  For any base b > 0 and any ξ > 0,   cid:18 1−  1  ξ cid:19 logb e ≤ logb ξ ≤  ξ − 1 logb e  with equalities on both sides if, and only if, ξ = 1.  Proof Actually, Figure 5.3 can be regarded as a proof. For those readers who would like a formal proof, we provide a mathematical derivation. We start with the upper bound. First note that  Then have a look at the derivatives:  logb ξ cid:12  cid:12 ξ =1 = 0 =  ξ − 1 logb e cid:12  cid:12 ξ =1.  d dξ  logb ξ =  logb e cid:40 > logb e  < logb e  1 ξ  if 0 < ξ < 1, if ξ > 1,   5.25    5.26    5.27   ] s t i b [    p   b H  1  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  0  0  0.1  0.2  0.3  0.4  0.6  0.7  0.8  0.5 p   90  Entropy and Shannon’s Source Coding Theorem  Figure 5.3 Illustration of the IT Inequality.  and  d dξ  ξ − 1 logb e = logb e.  Hence, the two functions coincide at ξ = 1, and the linear function is above the logarithm for all other values.  To prove the lower bound again note that   cid:18 1− ξ cid:19 logb e =  1  1  ξ cid:19 logb e cid:12  cid:12  cid:12  cid:12 ξ =1 ξ 2 logb e cid:40 > d  < d  1  and  d  dξ cid:18 1−  similarly to above.  = 0 = logb ξ cid:12  cid:12 ξ =1  dξ logb ξ = 1 dξ logb ξ = 1  ξ logb e ξ logb e  if 0 < ξ < 1, if ξ > 1,  5.2.4 Bounds on the entropy  Lemma 5.11  If U has r possible values, then  0 ≤ H U  ≤ log2 r bits,   5.28    5.29    5.30    5.31   4  3  2  1  0  −1 −2 −3 −4 −5     0   ξ−1  logb e logb ξ  1−1 ξ  logb e  0.5  1  1.5  2  3  3.5  2.5 ξ   5.2 Uncertainty or entropy  where  H U  = 0 H U  = log2 r bits  if, and only if, pi = 1 for some i, if, and only if, pi =  1 r ∀i.  Proof Since 0 ≤ pi ≤ 1, we have  −pi log2 pi  cid:40 = 0  > 0  if pi = 1, if 0 < pi < 1.  Hence, H U  ≥ 0. Equality can only be achieved if −pi log2 pi = 0 for all i, i.e. pi = 1 for one i and pi = 0 for the rest. To derive the upper bound, we use a trick that is quite common in informa- tion theory: we take the difference and try to show that it must be nonpositive. In the following we arrange the probabilities in descending order and assume that r cid:48   r cid:48  ≤ r  of the r values of the probabilities pi are strictly positive, i.e. pi > 0 for all i = 1, . . . ,r cid:48 , and pi = 0 for i = r cid:48  + 1, . . . ,r. Then  H U − log2 r = −  pi log2 pi − log2 r  r  i=1  ∑ r cid:48 ∑  i=1  pi  i=1  r cid:48 ∑  cid:124  cid:123  cid:122  cid:125 =1  pi log2 r  = −  pi log2 pi − log2 r·  =  i=1  i=1  i=1  i=1  r cid:48 ∑  = −  pi log2 pi −  r cid:48 ∑ r cid:48 ∑ pi log2 pi · r  = − pi log2 cid:18  1  cid:19  r cid:48 ∑ pi · r  cid:124  cid:123  cid:122  cid:125  cid:44 ξ pi · r − 1 cid:19 · log2 e pi cid:18  1 r cid:48 ∑  cid:33 · log2 e = cid:32  r cid:48 ∑ r cid:48 ∑  cid:124  cid:123  cid:122  cid:125 =1 = cid:18  r cid:48  r − 1 cid:19 · log2 e ≤  1− 1 · log2 e = 0.  1 r −  ≤  i=1  i=1  i=1  pi  91   5.32    5.33    5.34    5.35    5.36    5.37    5.38    5.39    5.40    5.41    5.42    5.43    92  Entropy and Shannon’s Source Coding Theorem  Here,  5.40  follows from the IT Inequality  Lemma 5.10 , and  5.43  follows because r cid:48  ≤ r. Hence, H U  ≤ log2 r. Equality can only be achieved if both   1  in the IT Inequality ξ = 1, i.e. if 1 pir = 1 for all i, i.e. if pi = 1 r for all  i; and  2  r cid:48  = r.  Note that if the ﬁrst condition is satisﬁed, then the second condition is auto- matically satisﬁed.  5.3 Trees revisited  The most elegant way to connect our new deﬁnition of entropy with the codes introduced in Chapter 4 is to rely again on trees with probabilities.  Consider a binary tree with probabilities. We remind the reader of our nota-  tion:   n denotes the total number of leaves;   pi, i = 1, . . . ,n, denote the probabilities of the leaves;   N denotes the number of nodes  including the root, but excluding the leaves ;   P cid:96 ,  cid:96  = 1, . . . , N, denote the probabilities of the nodes, where by deﬁnition  and  P1 = 1 is the root probability.  Moreover, we will use q cid:96 , j to denote the probability of the jth node leaf that is one step forward from node  cid:96   the jth child of node  cid:96  , where j = 0,1. That is, we have  q cid:96 ,0 + q cid:96 ,1 = P cid:96 .  Now we give the following deﬁnitions.  Deﬁnition 5.12 The leaf entropy is deﬁned as  Hleaf  cid:44  −  n  ∑  i=1  pi log2 pi.  Deﬁnition 5.13 Denoting by P1,P2, . . . ,PN the probabilities of all nodes  in- cluding the root  and by q cid:96 , j the probability of the nodes and leaves one step forward from node  cid:96 , we deﬁne the branching entropy H cid:96  of node  cid:96  as  H cid:96   cid:44  −  q cid:96 ,0 P cid:96   log2  q cid:96 ,0 P cid:96  −  q cid:96 ,1 P cid:96   log2  q cid:96 ,1 P cid:96   .   5.44    5.45    5.46    5.3 Trees revisited  93  Figure 5.4 An example of a binary tree with probabilities to illustrate the calculations of the leaf entropy and the branching entropies.  Note that following Remark 5.5 we implicitly assume that the sum is only over those j for which q cid:96 , j > 0, i.e. we have H cid:96  = 0 if one of the q cid:96 , j is zero. Note further that q cid:96 , j P cid:96  is the conditional probability of going along the jth branch given that we are at node  cid:96   normalization! . Example 5.14 As an example consider the tree in Figure 5.4. We have  log2  Hleaf = −0.4log2 0.4− 0.1log2 0.1− 0.5log2 0.5  cid:39  1.361 bits; H1 = − H2 = − H3 = −  0.4 1 − 0.1 log2 0.6 − 0.1 0.1 = 0 bits.  0.6 1  cid:39  0.971 bits; 0.5 0.6  cid:39  0.650 bits;  0.4 1 0.1 0.6 0.1 0.1  0.6 1 0.5 0.6  log2  log2  log2  We will next prove a very interesting relationship between the leaf entropy and the branching entropy that will turn out to be fundamental for the under- standing of codes. Theorem 5.15  Leaf Entropy Theorem  have that  In any tree with probabilities we   5.47    5.48    5.49    5.50  ♦   5.51   Hleaf =  P cid:96 H cid:96 .  N∑   cid:96 =1  H1  1  H2  0.6   94  Entropy and Shannon’s Source Coding Theorem  Proof Recall that by the deﬁnition of trees and trees with probabilities we have, for every node  cid:96 ,  P cid:96  = q cid:96 ,0 + q cid:96 ,1.  Using the deﬁnition of branching entropy, we obtain  q cid:96 ,1  P cid:96   cid:19   log2 q cid:96 ,1 P cid:96   q cid:96 ,0 q cid:96 ,1 log2 P cid:96  − P cid:96  q cid:96 ,0 P cid:96  − q cid:96 ,1 log2  P cid:96 H cid:96  = P cid:96  · cid:18 − q cid:96 ,0 P cid:96  = −q cid:96 ,0 log2 = −q cid:96 ,0 log2 q cid:96 ,0 − q cid:96 ,1 log2 q cid:96 ,1 + q cid:96 ,0 log2 P cid:96  + q cid:96 ,1 log2 P cid:96  = −q cid:96 ,0 log2 q cid:96 ,0 − q cid:96 ,1 log2 q cid:96 ,1 + cid:0 q cid:96 ,0 + q cid:96 ,1 cid:1   cid:123  cid:122   cid:125  =P cid:96  = −q cid:96 ,0 log2 q cid:96 ,0 − q cid:96 ,1 log2 q cid:96 ,1 + P cid:96  log2 P cid:96 ,  log2 P cid:96    cid:124   where the last equality follows from  5.52 .   5.52    5.53    5.54    5.55   5.56    5.57   Figure 5.5 Graphical proof of the Leaf Entropy Theorem. There are three nodes: we see that all contributions cancel apart from the root node  whose contribution is 0  and the leaves.  Hence, for every node ˜ cid:96 , we see that it will contribute to ∑N  cid:96 =1 P cid:96 H cid:96  twice:   ﬁrstly it will add P˜ cid:96  log2 P˜ cid:96  when the node counter  cid:96  passes through ˜ cid:96 ; but   secondly it will subtract q cid:96 , j log2 q cid:96 , j = P˜ cid:96  log2 P˜ cid:96  when the node counter  cid:96  points to the parent node of ˜ cid:96 .  0.4  −0.4 log2 0.4  1  +1 log2 1 = 0  −0.6 log2 0.6  0.1  −0.05 log2 0.05 +0.1 log2 0.1  −0.05 log2 0.05  −0.1 log2 0.1  0.6  +0.6 log2 0.6  −0.5 log2 0.5  contribution of P1H1  contribution of P2H2  contribution of P3H3  0.5   5.4 Bounds on the efﬁciency of codes  95  Hence, the contributions of all nodes will be canceled out – apart from the root that does not have a parent! The root only contributes P1 log2 P1 for  cid:96  = 1. However, since P1 = 1, we have P1 log2 P1 = 1log2 1 = 0. So the root does not contribute either.  It only remains to consider the leaves. Note that the node counter  cid:96  will not pass through leaves by deﬁnition. Hence, a leaf only contributes when the node counter points to its parent node and its contribution is −q cid:96 , j log2 q cid:96 , j = −pi log2 pi. Since the sum of all −pi log2 pi equals the leaf entropy by deﬁni- tion, this proves the claim.  In Figure 5.5 we have tried to depict this proof graphically.  Example 5.16  Continuation from Example 5.14  Using the values from  5.47 – 5.50  we obtain  P1H1 + P2H2 + P3H3 = 1· 0.971 + 0.6· 0.650 + 0.1· 0 bits  = 1.361 bits = Hleaf   5.58   5.59  ♦  as expected.  5.4 Bounds on the efﬁciency of codes  The main strength of information theory is that it can provide some fundamen- tal statements about what is possible and what is not possible to achieve. So a typical information theoretic result will consist of an upper bound and a lower bound or, in general, an achievability part and a converse part. The achievabil- ity part of a theorem tells us what we can do, and the converse part tells us what we cannot do.  Sometimes, the theorem will also tell us how to do it, but usually the result is theoretic in the sense that it only proves what is possible without actually saying how it could be done. To put it pointedly: information theory tells us what is possible; coding theory tells us how to do it.  5.4.1 What we cannot do: fundamental limitations of  source coding  Let us quickly summarize what we know about codes and their corresponding trees.   The most efﬁcient codes can always be chosen to be preﬁx-free  Theo-  rem 4.19 .   corresponds to a leaf in the tree.  Entropy and Shannon’s Source Coding Theorem  96   Every preﬁx-free code can be represented by a tree where every codeword   Every codeword has a certain probability corresponding to the probability   Unused leaves can be regarded as symbols that never occur, i.e. we assign  of the symbol it represents.  probability zero to them.  Hence, from these observations we immediately see that the entropy H U  of a random message U with probabilities p1, . . . , pr and the leaf entropy of the corresponding tree are the same:   5.60  Note that the unused leaves do not contribute to Hleaf since they have zero probability.  Hleaf = H U .  Moreover, the average codeword length Lav is equivalent to the average depth of the leaves.  Again we can ignore the unused leaves, since they have probability zero and therefore do not contribute to the average.   Now note that since we consider binary trees where each node branches into two different children, we know from Lemma 5.11 that the branching entropy can be upper-bounded as follows:  H cid:96  ≤ log2 2 = 1 bit.   5.61   Hence, using this together with the Leaf Entropy Theorem  Theorem 5.15  and the Path Length Lemma  Lemma 4.11  we obtain the following:  H U  = Hleaf =  N∑   cid:96 =1  P cid:96 H cid:96  ≤  N∑   cid:96 =1  P cid:96  · 1 bit =  N∑   cid:96 =1  P cid:96  = Lav bits.   5.62   In other words,  Lav ≥ H U  bits.   5.63   This is the converse part of the Coding Theorem for a Single Random Mes- sage. It says that whatever code you try to design, the average codeword length of any binary code for an r-ary random message U cannot be smaller than the entropy of U  using the correct unit of bits !  Note that to prove this statement we have not designed any code, but instead  we have been able to prove something that holds for every code that exists!  When do we have equality? From the above derivation we see that we have equality if the branching entropy is always 1 bit, H cid:96  = 1 bit, i.e. the branching probabilities are all uniform. This is only possible if pi is a negative integer power of 2 for all i:  pi = 2−νi   5.64    5.4 Bounds on the efﬁciency of codes  97  with νi a natural number  and, of course, if we design an optimal code .  5.4.2 What we can do: analysis of the best codes  In practice, it is not only important to know where the limitations are, but also perhaps even more so to know how close we can get to these limitations. So as a next step we would like to analyze the best codes  i.e. the Huffman codes derived in Section 4.6  and see how close they get to the limitations shown in Section 5.4.1.  Unfortunately, it is rather difﬁcult to analyze Huffman codes. To circumvent this problem, we will design a new code, called the Fano code, and analyze its performance instead. Fano codes are not optimal in general, i.e. their per- formance is worse than the performance of Huffman codes. Therefore any up- per bound on Lav that can be achieved by a Fano code can deﬁnitely also be achieved by a Huffman code. Deﬁnition 5.17  Fano code  The Fano code2 is generated according to the following algorithm: Step 1 Arrange the symbols in order of nonincreasing probability. Step 2 Divide the list of ordered symbols into two parts, with the total proba- bility of the left part being as close to the total probability of the right part as possible.  Step 3 Assign the binary digit 0 to the left part of the list, and the digit 1 to the right part. This means that the codewords for the symbols in the ﬁrst part will all start with 0, and the codewords for the symbols in the second part will all start with 1.  Step 4 Recursively apply Step 2 and Step 3 to each of the two parts, subdi- viding into further parts and adding bits to the codewords until each symbol is the single member of a part.  Note that effectively this algorithm constructs a tree. Hence, the Fano code is preﬁx-free. Example 5.18 Let us generate the Fano code for a random message with ﬁve symbols having probabilities  p1 = 0.35, p4 = 0.15,  p2 = 0.25, p5 = 0.1.  p3 = 0.15,   5.65   2 Note that this code is usually known as the Shannon–Fano code. However, this is a misnaming because it was Fano’s invention. Shannon proposed a slightly different code, which unfortu- nately is also known as the Shannon–Fano code. For more details on this confusion, we refer to the discussion in Section 5.6.   98  Entropy and Shannon’s Source Coding Theorem  Since the symbols are already ordered in decreasing order of probability, Step 1 can be omitted. We hence want to split the list into two parts, both having as similar total probability as possible. If we split {1} and {2,3,4,5}, we have a total probability 0.35 on the left and 0.65 on the right; the split {1,2} and {3,4,5} yields 0.6 and 0.4; and {1,2,3} and {4,5} gives 0.75 and 0.25. We see that the second split is best. So we assign 0 as a ﬁrst digit to {1,2} and 1 to {3,4,5}. Now we repeat the procedure with both subgroups. Firstly, we split {1,2} into {1} and {2}. This is trivial. Secondly, we split {3,4,5} into {3} and {4,5} because 0.15 and 0.25 is closer than 0.3 and 0.1 that we would have obtained by dividing into {3,4} and {5}. Again we assign the corresponding second digits. Finally, we split the last group {4,5} into {4} and {5}. We end up with the ﬁve codewords {00,01,10,110,111}. This whole procedure is shown in ♦ Figure 5.6.  p1 0.35  p2 0.25  p3 0.15  0.6 0  0.35  0.25  0  1  0.15 0.15  0  0.25  1  p4 0.15 0.4 1 0.15  0.15  0 110  p5 0.1  0.1  0.1 1 111  00  01  10  Figure 5.6 Construction of the Fano code of Example 5.18.  Exercise 5.19 Construct the Fano code for the random message U of Exam- ple 4.16 with four symbols having probabilities  p1 = 0.4,  p2 = 0.3,  p3 = 0.2,  p4 = 0.1,  and show that it is identical to the corresponding Huffman code.   5.66  ♦  Remark 5.20 We would like to point out that there are cases where the algo- rithm given in Deﬁnition 5.17 does not lead to a unique design: there might be two different ways of dividing the list into two parts such that the total proba- bilities are as similar as possible. Since the algorithm does not specify what to   5.4 Bounds on the efﬁciency of codes  99  do in such a case, you are free to choose any possible way. Unfortunately, how- ever, these different choices can lead to codes with different performance.3 As an example, consider a random message U with seven possible symbols having the following probabilities:  p1 = 0.35, p5 = 0.05,  p2 = 0.3, p6 = 0.05,  p3 = 0.15, p7 = 0.05.  p4 = 0.05,   5.67   Figures 5.7 and 5.8 show two different possible Fano codes for this random message. The ﬁrst has an average codeword length of Lav = 2.45, while the latter’s performance is better with Lav = 2.4.  p5 0.05 0.35  1  0.05  p1 0.35  p2 0.3  p3 0.15  p4 0.05  p6 0.05  p7 0.05  0.65  0  0.35  0.3  0.15  0.05  0  1  0.2 0  0.05 0.15  1  0.05  0.05  0.05 0.05  1  0.15  0.05  0.05  0  1  0.1 0  0.05  0.05  0  1  00  01  100  101  1100  1101  111  Figure 5.7 One possible Fano code for the random message given in  5.67 .  Exercise 5.21 In total there are six different possible designs of a Fano code for the random message given in Remark 5.20. Design all of them and compare ♦ their performances.  We next prove a simple property of the Fano code.  Lemma 5.22 The codeword lengths li of a Fano code satisfy the following:  li ≤ cid:24 log2  1  pi cid:25  ,   5.68   where  cid:100 ξ cid:101  denotes the smallest integer not smaller than ξ . 3 This cannot happen in the case of a Huffman code! Even though the algorithm of the Huffman code is not unique either, it always will result in codes of equal  optimal  performance. The reason for this is clear: we have proven that the Huffman algorithm results in an optimal code.   100  Entropy and Shannon’s Source Coding Theorem  p1 0.35 0.35  0  p2 0.3  p3 0.15  p4 0.05  p5 0.05  p6 0.05  p7 0.05  0.3 0.3 0  0.15 0.15  0  0.15  0.05  0.05  0.05  0.65  1  0.05 0.35  1  0.05  0.2 1  0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.1 0  0.1 1  0.05  0.05  0.05  0.05  0  1  0  1  0  10  110  11100  11101  11110  11111  Figure 5.8 A second possible Fano code for the random message given in  5.67 .  Proof By construction, any symbol with probability pi ≥ 1 2 will be alone in one part in the ﬁrst round of the algorithm. Hence,  If 1 4 ≤ pi < 1 2, then at the latest in the second round of the algorithm the symbol will occupy one partition.  Note that it is possible that the symbol is already the single element of one partition in the ﬁrst round. For example, for p1 = 3 4 and p2 = 1 4, p2 will have l2 = 1.  Hence, we have  In the same fashion we show that for 1 8 ≤ pi < 1 4,  li = 1 = cid:24 log2  1  pi cid:25  .  li ≤ 2 = cid:24 log2 li ≤ 3 = cid:24 log2 li ≤ 4 = cid:24 log2  1  1  pi cid:25  . pi cid:25 ; pi cid:25 ;  1   5.69    5.70    5.71    5.72   for 1 16 ≤ pi < 1 8,  etc.   5.4 Bounds on the efﬁciency of codes  101  Next, let us see how efﬁcient the Fano code is. To that goal, we note that  from  5.68  we have  We get  1  pi cid:25  < log2  1 pi  + 1.  li ≤ cid:24 log2 ∑  Lav =  r  pili  i=1 r  ∑  i=1 r  ∑  i=1  <  =  pi cid:18 log2  1 pi  1 pi  +  pi log2 r  + 1 cid:19  ∑  pi  r  i=1  ∑  pi log2 pi + 1  = − = H U  + 1 bits,  i=1   5.73    5.74    5.75    5.76    5.77    5.78   where the entropy is based on the binary logarithm, i.e. it is measured in bits. Hence, the Fano code  even though it is not an optimal code  approaches the ultimate lower bound  5.63  by less than 1 bit! A Huffman code will be even better than that.  5.4.3 Coding Theorem for a Single Random Message  We summarize this so far most important result of this chapter.  Example 5.24 We consider a random message U with seven symbols having probabilities  p1 = 0.4, p5 = 0.1,  p2 = 0.1, p6 = 0.1,  p3 = 0.1, p7 = 0.1,  p4 = 0.1,   5.80   Theorem 5.23  Coding Theorem for a Single Random Message  For an optimal binary preﬁx-free code  i.e. a binary Huffman code  for an r-ary random message U , the average codeword length Lav satisﬁes  H U  bits ≤ Lav < H U  + 1 bits   5.79    where the entropy is measured in bits . We have equality on the left if, and only if, pi is a negative integer power of 2, ∀ i. Moreover, this statement also holds true for Fano coding.   Entropy and Shannon’s Source Coding Theorem  102 i.e. H U   cid:39  2.52 bits. We ﬁrstly design a Fano code; see Figure 5.9. The cor- responding tree is shown in Figure 5.10. Note that the construction algorithm is not unique in this case: in the second round we could split the second group either to {3,4} and {5,6,7} or {3,4,5} and {6,7}. In this case, both ways will result in a code of identical performance. The same situation occurs in the third round.  p1 0.4  p2 0.1  p3 0.1  p4 0.1  p6 0.1  p7 0.1  0.5 0  0.4  0.1  0.1  0.1  0  1  0.2 0  0.1  0.1  0.1  0  1  00  01  100  101  p5 0.1 0.5 1 0.1  0.1 0.3 1 0.1  0.2 0  0.1 0 1100  0.1 1 1101  0.1  0.1 0.1 1  111  Figure 5.9 Construction of the Fano code of Example 5.24.  The efﬁciency of this Fano code is given by  Lav = 1 + 0.5 + 0.5 + 0.2 + 0.3 + 0.2 = 2.7 bits,   5.81   which satisﬁes, as predicted,  2.52 bits ≤ 2.7 bits < 3.52 bits.  A corresponding Huffman code for U is shown in Figure 5.11. Its perfor- mance is Lav = 2.6 bits, i.e. it is better than the Fano code, but of course it still holds that  2.52 bits ≤ 2.6 bits < 3.52 bits.  Exercise 5.25 Design a Huffman code and a Fano code for the random mes- sage U with probabilities  p1 = 0.25, p5 = 0.1,  p2 = 0.2, p6 = 0.1, and compare their performances.  p3 = 0.2, p7 = 0.05,  p4 = 0.1,   5.82    5.83  ♦   5.84   ♦   5.5 Coding of an information source  103  Figure 5.10 A Fano code for the message U of Example 5.24.  We have seen in the above examples and exercises that, even though the Fano code is not optimal, its performance is usually very similar to the optimal Huffman code. In particular, we know from Theorem 5.23 that the performance gap is less than one binary digit.  We are going to see next that once we start encoding not a single random message, but a sequence of such messages emitted by a random source, this difference becomes negligible.  5.5 Coding of an information source  So far we have only considered a single random message, but in reality we are much more likely to encounter a situation where we have a stream of messages  0  1  1  0.5  0.5  00  0.4  01  0.1  0.2  0.3  0.1  0.1  0.2  0.1   104  Entropy and Shannon’s Source Coding Theorem  Figure 5.11 A Huffman code for the message U of Example 5.24.  that should be encoded continuously. Luckily we have prepared ourselves for this situation already by considering preﬁx-free codes only, which make sure that a sequence of codewords can be separated easily into the individual code- words.  In the following we will consider only the simplest case where the random source is memoryless, i.e. each symbol that is emitted by the source is inde- pendent of all past symbols. A formal deﬁnition is given as follows.  Deﬁnition 5.26  DMS  An r-ary discrete memoryless source  DMS  is a de- vice whose output is a sequence of random messages U1,U2,U3, . . ., where   each U cid:96  can take on r different values with probability p1, . . . , pr, and   the different messages U cid:96  are independent of each other.  The obvious way of designing a compression system for such a source is to design a Huffman code for U, continuously use it for each message U cid:96 , and concatenate the codewords together. The receiver can easily separate the code-  0  1  1  0.6  0.4  0.2  0.2  0.2   5.5 Coding of an information source  105  words  because the Huffman code is preﬁx-free  and decode them to recover the sequence of messages {U cid:96 }. that it is also possible to combine two or more messages  However, the question is whether this is the most efﬁcient approach. Note   U cid:96 ,U cid:96 +1, . . . ,U cid:96 +ν    together and design a Huffman code for these combined messages! We will show below that this latter approach is actually more efﬁcient. But before doing so, we need to think about such random vector messages.  Remark 5.27 Note that a random vector message V =  U1, . . . ,Uν   is, from the mathematical point of view, no different from any other random message: it takes on a certain ﬁnite number of different values with certain probabilities. If U cid:96  is r-ary, then V is rν-ary, but otherwise there is no fundamental difference. We can even express the entropy of V as a function of the entropy of U. Let q j denote the probability of the jth symbol of V. Since the different messages U cid:96  are independent, we have  where pi cid:96  denotes the probability of the i cid:96 th symbol of U cid:96 . Hence,  q j = pi1 · pi2 ··· piν ,   5.85    5.86   H V  = −  q j log2 q j  rν ∑  j=1 r  r  r  r  r  r  r   5.87    5.88   = −  = −  = −   pi1 · pi2 ··· piν  log2 pi1 · pi2 ··· piν    pi1 · pi2 ··· piν   cid:0 log2 pi1 +··· + log2 piν cid:1  pi1 · pi2 ··· piν · log2 pi1 ∑ iν =1  ∑ ∑ i1=1··· iν =1 ∑ ∑ i1=1··· iν =1 ∑ ∑ i1=1··· iν =1 r ∑ pi1 · pi2 ··· piν · log2 piν i1=1··· −···− ··· cid:32  r pi1 log2 pi1 cid:33 · cid:32  r pi2 cid:33  piν cid:33  ∑ ∑ iν =1 i2=1  cid:123  cid:122   cid:125   cid:124   cid:124   cid:123  cid:122   cid:125  · cid:32  r ··· cid:32  r piν log2 piν cid:33   5.90  piν−1 cid:33  ∑ ∑ iν =1 iν−1=1  cid:124   cid:123  cid:122   cid:125  =1  = − cid:32  r ∑ i1=1 −···− cid:32  r pi1 cid:33  ∑ i1=1  cid:124   cid:123  cid:122   cid:125    5.89   =1  =1  =1   106  Entropy and Shannon’s Source Coding Theorem  pi1 log2 pi1 cid:33 −···− cid:32  r ∑ iν =1  = − cid:32  r ∑ i1=1 = H U1  +··· + H Uν   = νH U .  piν log2 piν cid:33    5.91    5.92   5.93   Here the last equality follows because the entropy of all U cid:96  is identical.  In other words, since V consists of ν independent random messages U, its  uncertainty is simply ν times the uncertainty of U.  Figure 5.12 A coding scheme for an information source: the source parser groups the source output sequence {U cid:96 } into messages {V cid:96  cid:48 }. The message encoder then assigns a codeword C cid:96  cid:48  to each possible message V cid:96  cid:48 .  Now our compression system looks as shown in Figure 5.12. The source parser is a device that groups ν incoming source symbols  U1, . . . ,Uν   together to a new message V. Note that because the source {U cid:96 } is memoryless, the different messages {V cid:96  cid:48 } are independent. Therefore we only need to look at one such message V  where we omit the time index  cid:96  cid:48  . So let us now use an optimal code  i.e. a Huffman code  or at least a good code  e.g. a Fano code  for the message V. Then from the Coding Theorem for a Single Random Message  Theorem 5.23  we know that H V  bits ≤ Lav < H V  + 1 bits,   5.94   where Lav denotes the average codeword length for the codewords describing the vector messages V.  Next note that it is not really fair to compare different Lav because, for larger ν, Lav also will be larger. So, to be correct we should compute the average codeword length necessary to describe one source symbol. Since V contains ν source symbols U cid:96 , the correct measure of performance is Lav ν.  Hence, we divide the whole expression  5.94  by ν:  ν and make use of  5.93 ,  H V   bits ≤  Lav ν <  H V   ν +  1 ν  bits,  νH U   ν  bits ≤  Lav ν <  νH U   ν +  1 ν  bits;   5.95    5.96   r-ary  DMS  Uℓ  symbols  Source  parser  Vℓ′  messages  Message  encoder   i.e.,  5.5 Coding of an information source  H U  bits ≤  Lav ν < H U  +  1 ν  bits.  107   5.97   Note that again we assume that the entropies are measured in bits.  We immediately get the following main result, also known as Shannon’s  Source Coding Theorem.  Note that everywhere we need to choose the units of the entropy to be in  bits.  We would like to discuss this result brieﬂy. The main point to note here is that by choosing ν large enough, we can approach the ultimate limit of compression H U  arbitrarily closely when using a Huffman or a Fano code. Hence, the entropy H U  is the amount of information that is packed in the output of the discrete memoryless source U! In other words, we can compress any DMS to H U  bits on average, but not less. This is the ﬁrst real justiﬁcation of the usefulness of Deﬁnition 5.4.  We also see that in the end it does not make much difference whether we use a Huffman code or a suboptimal Fano code as both approach the ultimate limit for ν large enough.  On the other hand, note the price we have to pay: by making ν large, we not only increase the number of possible messages, and thereby make the code complicated, but also we introduce delay into the system as the encoder can only encode the message after it has received a complete block of ν source symbols! Basically, the more closely we want to approach the ultimate limit of entropy, the larger is our potential delay in the system.  We would like to mention that our choice of a source parser that splits the  Theorem 5.28  Coding Theorem for a DMS  There exists a binary preﬁx-free code of a ν-block message from a dis- crete memoryless source  DMS  such that the average number Lav ν of binary code digits per source letter satisﬁes  Lav ν  1 ν  < H U  +  bits,   5.98   where H U  is the entropy of a single source letter measured in bits. Conversely, for every binary code of a ν-block message,  Lav ν ≥ H U  bits.   5.99    108  Entropy and Shannon’s Source Coding Theorem  source sequence into blocks of equal length is not the only choice. It is actually possible to design source parsers that will choose blocks of varying length depending on the arriving source symbols and their probabilities. By trying to combine more likely source symbols to larger blocks, while less likely symbols are grouped to smaller blocks, we can further improve on the compression rate of our system. A parser that is optimal in a speciﬁc sense is the so-called Tunstall source parser [Tun67], [Mas96]. The details are outside the scope of this introduction. However, note that whatever source parser and whatever message encoder we choose, we can never beat the lower bound in  5.99 .  All the systems we have discussed here contain one common drawback: we always have assumed that the probability statistics of the source is known in advance when designing the system. In a practical situation this is often not the case. What is the probability distribution of a digitized speech in a telephone system? Or of English ASCII text in comparison to French ASCII text?4 Or of different types of music? A really practical system should work independently of the source; i.e., it should estimate the probabilities of the source symbols on the ﬂy and adapt to it automatically. Such a system is called a universal compression scheme. Again, the details are outside of the scope of this introduction, but we would like to mention that such schemes exist and that commonly used compression algorithms like, e.g., ZIP successfully implement such schemes.  5.6 Some historical background  The Fano code is in the literature usually known as the Shannon–Fano code, even though it is an invention of Professor Robert Fano from MIT [Fan49] and not of Shannon. To make things even worse, there exists another code that is also known as the Shannon–Fano code, but actually should be called the Shannon code because it was proposed by Shannon [Sha48, Sec. 9]: the construction of the Shannon code also starts with the ordering of the symbols according to decreasing probability. The ith codeword with probability pi is then obtained by writing the cumulative probability  Fi  cid:44  i−1 ∑  j=1  p j   5.100   in binary form. For example, Fi = 0.625 in binary form is .101 yielding a codeword 101, or Fi = 0.3125 is written in binary as .0101, which then results  4 Recall the deﬁnition of ASCII given in Table 2.2.   5.6 Some historical background  109  in a codeword 0101. Since in general this binary expansion might be inﬁnitely long, Shannon gave the additional rule that the expansion shall be carried out to exactly li positions, where  li  cid:44  cid:24 log2  1  pi cid:25  .  .10011001100 . . . ,   5.101    5.102    5.103   So if Fi = 0.625  with binary form .101  and pi is such that li = 5, then the resulting codeword is 10100, or if Fi = 0.6, which in binary form is  and pi is such that li = 3, then the resulting codeword is 100.  It is not difﬁcult to show that this code is preﬁx-free. In particular it is  straightforward to show that the Kraft Inequality  Theorem 4.8  is satisﬁed:  r  ∑  i=1  2−li =  2− cid:108 log2  1  pi cid:109  ≤  r  ∑  i=1  r  ∑  i=1  2−log2  1 pi =  pi = 1,  r  ∑  i=1  where we have used that  li = cid:24 log2  1  pi cid:25  ≥ log2  1 pi  .  Shannon’s code performs similarly to the Fano code of Deﬁnition 5.17, but Fano’s code is in general slightly better, as can be seen by the fact that in  5.68  we have an inequality while in  5.101  we have, by deﬁnition, equal- ity always. However – and that is probably one of the reasons5 why the two codes are mixed up and both are known under the same name Shannon–Fano code – both codes satisfy the Coding Theorem for a Single Random Message  Theorem 5.23 .  Actually, one also ﬁnds that any code that satisﬁes  5.101  is called a Shan- non–Fano code! And to complete the confusion, sometimes the Shannon–Fano code is also known as Shannon–Fano–Elias code [CT06, Sec. 5.9]. The rea- son is that the Shannon code was the origin of arithmetic coding, which is an elegant and efﬁcient extension of Shannon’s idea, applied to the compression of the output sequence of a random source. It is based on the insight that it is not necessary to order the output sequences u according to their probabili- ties, but that it is sufﬁcient to have them ordered lexicographically  according to the alphabet . The codeword for u is then the truncated binary form of the  5 Another reason is that Shannon, when introducing his code in [Sha48, Sec. 9], also refers to  Fano’s code construction.   110  Entropy and Shannon’s Source Coding Theorem  cumulative probability  where p˜u is the probability of ˜u. However, in order to guarantee that the code is preﬁx-free and because the output sequences are ordered lexicographically and not according to probability, it is necessary to increase the codeword length by 1; i.e., for arithmetic coding we have the rule that the codeword length is  Fu  cid:44   ∑  p˜u,  all sequences ˜u  that are alphabetically  before u  lu  cid:44  cid:24 log2  1  pu cid:25  + 1.   5.104    5.105   Note further that, since the sequences are ordered lexicographically, it is also possible to compute the cumulative probability  5.104  of a particular source output sequence u iteratively without having to know the probabilities of all other source sequences. These ideas have been credited to the late Professor Peter Elias from MIT  hence the name Shannon–Fano–Elias coding , but ac- tually Elias denied this. The concept has probably come from Shannon himself during a talk that he gave at MIT.  For an easy-to-read introduction to arithmetic coding including its history,  the introductory chapter of [Say99] is highly recommended.  5.7 Further reading  For more information about data compression, the lecture notes [Mas96] of Professor James L. Massey from ETH, Zurich, are highly recommended. They read very easily and are very precise. The presentation of the material in Chap- ters 4 and 5 is strongly inspired by these notes. Besides the generalization of Huffman codes to D-ary alphabets and the variable-length–to–block Tunstall codes, one also ﬁnds there details of a simple, but powerful, universal data compression scheme called Elias–Willems coding.  For the more commonly used Lempel–Ziv universal compression scheme we refer to [CT06]. This is also a good place to learn more about entropy and its properties.  One of the best books on information theory is by Robert Gallager [Gal68]; however, it is written at an advanced level. It is fairly old and therefore does not cover more recent discoveries, but it gives a very deep treatment of the foundations of information theory.   5.8 Appendix: Uniqueness of the deﬁnition of entropy  111  5.8 Appendix: Uniqueness of the deﬁnition of entropy  In Section 5.1 we tried to motivate the deﬁnition of entropy. Even though we partially succeeded, we were not able to provide a full justiﬁcation of Def- inition 5.4. While Shannon did provide a mathematical justiﬁcation [Sha48, Sec. 6], he did not consider it very important. We omit Shannon’s argument, but instead we will now quickly summarize a slightly different result that was presented in 1956 by Aleksandr Khinchin. Khinchin speciﬁed four properties that entropy is supposed to have and then proved that, given these four proper- ties,  5.10  is the only possible deﬁnition.  We deﬁne Hr p1, . . . , pr  to be a function of r probabilities p1, . . . , pr that  sum up to 1:  r  ∑  i=1  pi = 1.   5.106   We ask this function to satisfy the following four properties.   1  For any r, Hr p1, . . . , pr  is continuous  i.e. a slight change to the values of pi will only cause a slight change to Hr  and symmetric in p1, . . . , pr  i.e. changing the order of the probabilities does not affect the value of Hr .   2  Any event of probability zero does not contribute to Hr:  Hr+1 p1, . . . , pr,0  = Hr p1, . . . , pr .   5.107    3  Hr is maximized by the uniform distribution:  Hr p1, . . . , pr  ≤ Hr cid:18 1  r , . . . ,  1  r cid:19  .   5.108    4  If we partition the m· r possible outcomes of a random experiment into m groups, each group containing r elements, then we can do the experiment in two steps:  i  determine the group to which the actual outcome belongs,  ii  ﬁnd the outcome in this group. Let p j,i, 1 ≤ j ≤ m, 1 ≤ i ≤ r, be the probabilities of the outcomes in this random experiment. Then the total probability of all outcomes in group j is given by  and the conditional probability of outcome i from group j is then given by  q j =  p j,i,  r  ∑  i=1  p j,i q j  .   5.109    5.110    112  Entropy and Shannon’s Source Coding Theorem  Now Hm·r can be written as follows: Hm·r p1,1, p1,2, . . . , pm,r  = Hm q1, . . . ,qm  +  m  ∑  j=1  q jHr cid:18  p j,1  q j  , . . . ,  p j,r  q j  cid:19 ;   5.111   i.e., the uncertainty can be split into the uncertainty of choosing a group and the uncertainty of choosing one particular outcome of the chosen group, averaged over all groups.  Theorem 5.29 The only functions Hr that satisfy the above four conditions are of the form  Hr p1, . . . , pr  = −c  pi log2 pi,  r  ∑  i=1   5.112   where the constant c > 0 decides about the units of Hr.  Proof This theorem was proven by Aleksandr Khinchin in 1956, i.e. af- ter Shannon had deﬁned entropy. The article was ﬁrst published in Russian [Khi56], and then in 1957 it was translated into English [Khi57]. We omit the details.  References  [CT06] Thomas M. Cover and Joy A. Thomas, Elements of Information Theory,  2nd edn.  John Wiley & Sons, Hoboken, NJ, 2006.  [Fan49] Robert M. Fano, “The transmission of information,” Research Laboratory of Electronics, Massachusetts Institute of Technology  MIT , Technical Report No. 65, March 17, 1949.  [Gal68] Robert G. Gallager, Information Theory and Reliable Communication.  John  [Har28] Ralph Hartley, “Transmission of information,” Bell System Technical Journal,  Wiley & Sons, New York, 1968.  vol. 7, no. 3, pp. 535–563, July 1928.  [Khi56] Aleksandr Y. Khinchin, “On the fundamental theorems of information theory,”   in Russian , Uspekhi Matematicheskikh Nauk XI, vol. 1, pp. 17–75, 1956.  [Khi57] Aleksandr Y. Khinchin, Mathematical Foundations of Information Theory.  Dover Publications, New York, 1957.  [Mas96] James L. Massey, Applied Digital Information Theory I and II, Lecture notes, Signal and Information Processing Laboratory, ETH Zurich, 1995 1996. Available: http:  www.isiweb.ee.ethz.ch archive massey scr   [Say99] Jossy Sayir, “On coding by probability transformation,” Ph.D. dissertation, ETH Zurich, 1999, Diss. ETH No. 13099. Available: http:  e-collection.ethbi b.ethz.ch view eth:23000   References  113  [Sha37] Claude E. Shannon, “A symbolic analysis of relay and switching circuits,” Master’s thesis, Massachusetts Institute of Technology  MIT , August 1937. [Sha48] Claude E. Shannon, “A mathematical theory of communication,” Bell System Technical Journal, vol. 27, pp. 379–423 and 623–656, July and October 1948. Available: http:  moser.cm.nctu.edu.tw nctu doc shannon1948.pdf  [Tun67] Brian P. Tunstall, “Synthesis of noiseless compression codes,” Ph.D. disserta-  tion, Georgia Institute of Technology, September 1967.    6  Mutual information and channel capacity  6.1 Introduction  We go on to take a closer look at a typical problem in communications: how to send information reliably over a noisy communication channel. A commu- nication channel can be thought of as the medium through which the message signal propagates from the transmit end  the source  to the receive end  the destination . A channel is said to be noisy if the data read at the channel output is not necessarily the same as the input  due to, e.g., perturbation caused by the ambient noise . Consider for example that Alice writes down a “7” on a paper with a small font size, and uses a fax machine to transfer this page to Bob. Due to limited resolution of electronic cable data transfer, Bob sees a “distorted 7” on the faxed page and could decode it incorrectly as, say, “9”  see Figure 6.1 .  Figure 6.1 Cable data transfer as a channel.  In this example the route of data transfer through the cable acts as the chan- nel, which is noisy since it distorts the input alphabet and, in turn, leads to possibly incorrect message decoding at the destination. It is still probable that Bob reads the message correctly as “7.” The higher the probability of correct message decoding is, the more reliable the communication will be.   116  Mutual information and channel capacity  Figure 6.2 Signaling model.  The general block diagram of a typical communication system is depicted  in Figure 6.2.  For a probabilistic message source, we are now able to quantify the amount of its information content in terms of the entropy deﬁned in Chapter 5. We implicitly assume that the message has been compressed in order to remove the inherent redundancy, if any; this can be done via data compression as in- troduced in Chapter 4  see also the discussion in Appendix 7.7 . To combat the detrimental effect induced by the channel, the source message is further encoded with certain channel coding schemes, like the Hamming code intro- duced in Chapter 3. The encoded data stream is then sent over the channel. Message decoding is performed at the receiver based on the channel output. We examine each of the following problems.   How should we measure the amount of information that can get through the   How can we use the channel to convey information reliably? Note that if the channel is noiseless, i.e. the input is always reproduced at the output without errors, the answers to the aforementioned problems are simple: the maximal amount of information that can be conveyed over the channel equals the source entropy, and this can be done without any data protection mechanisms such as channel coding. If the channel is noisy, the answers turn out to be rather nontrivial. Let us begin the discussions with the mathematical model of a noisy communication channel.  channel, and what is the maximal amount?  6.2 The channel  Recall that in the example depicted in Figure 6.1, the input letter “7” can be ei- ther correctly decoded or mis-recognized as some other letter. The uncertainty in source symbol recovery naturally suggests a probabilistic characterization of the input–output relation of a noisy channel; such a mathematical channel  Noise  Source  Encoder  Channel   6.2 The channel  117  model is needed in order to pin down various intrinsic properties of a channel, e.g. how much information can go through a channel.  letters;  letters; and  Below is the formal deﬁnition for a channel. Deﬁnition 6.1  Channel  A channel  X,PYX  y jxi , Y  is given by  1  an input alphabet X  cid:44  {x1, . . . ,xs}, where s denotes the number of input  2  an output alphabet Y  cid:44  {y1, . . . ,yt}, where t denotes the number of output  3  a conditional probability distribution PYX  y jxi , which speciﬁes the prob- ability of observing Y = y j at the output given that X = xi is sent, 1 ≤ i ≤ s, 1 ≤ j ≤ t. Hence a channel with input X ∈ X and output Y ∈ Y is entirely speciﬁed by a set of conditional probabilities PYX  y jxi . The size of the input and output alphabets, namely s and t, need not be the same. A schematic description of the channel is shown Figure 6.3.  Figure 6.3 Channel model.  In this model the channel is completely described by the matrix of condi-  PYX  y1x1  PYX  y2x1  PYX  y1x2  PYX  y2x2   tional probabilities, the so-called channel transition matrix: . . . PYX  ytx1  . . . PYX  ytx2  ... . . . PYX  ytxs  The channel transition matrix has the following properties.  PYX  y1xs  PYX  y2xs     ...  ...  ...    .   6.1    1  The entries on the ith row consist of the probabilities of observing output  letters y1, . . . ,yt given that the ith input symbol xi is sent.   2  The entries on the jth column consist of the probabilities of observing the jth output letter y j given, respectively, the ith input symbols xi are sent, i = 1, . . . ,s.  X =  X  PYX  y jxi   x1 x2 x3 ... xs       118  Mutual information and channel capacity   3  The sum of the entries in a row is always 1, i.e.  t  ∑  j=1  PYX  y jxi  = 1.  This merely means that for each input xi we are certain that something will come out, and that the PYX  y jxi  give the distribution of these probabili- ties.   4  If PX  xi  is the probability of the input symbol xi, then  s  ∑  i=1  t  ∑  j=1  PYX  y jxi PX  xi  = 1,  meaning that when something is put into the system, then certainly some- thing comes out. The probabilities PYX  y jxi , 1 ≤ i ≤ s, 1 ≤ j ≤ t, characterize the channel completely. We assume that the channel is stationary, i.e. the probabilities do not change with time. We note that X is not a source but is an information- carrying channel input, which is typically a stream of encoded data  see Fig- ure 6.2; see Chapters 3 and 7 for more details .  6.3 The channel relationships  At the transmit end we have s possible input symbols {x1, . . . ,xs}. If the ith symbol xi is selected and sent over the channel, the probability of observing the jth channel output letter y j is given by the conditional probability PYX  y jxi . This means that the probability that the input–output pair  xi,y j  simultane- ously occurs, i.e. the joint probability of X = xi and Y = y j, is given by  PX,Y  xi,y j   cid:44  PYX  y jxi PX  xi .  Let us go one step further by asking the question of how to determine the probability that the jth letter y j will occur at the channel output, hereafter denoted by PY  y j . A simple argument, taking into account that each input symbol occurs with probability PX  xi , yields  PY  y j  = PYX  y jx1 PX  x1  +··· + PYX  y jxs PX  xs   =  s  ∑  i=1  PYX  y jxi PX  xi ,  1 ≤ j ≤ t.  The above “channel equation” characterizes the input–output relation of a chan- nel. Note that in terms of the joint probability PX,Y  xi,y j  in  6.4 , we can   6.2    6.3    6.4    6.5    6.6    6.4 The binary symmetric channel  rewrite  6.6  in a more compact form:  PY  y j  =  PX,Y  xi,y j ,  1 ≤ j ≤ t.  s  ∑  i=1  Now take a further look at  6.4 , which relates the probability of a joint oc- currence of the symbol pair  xi,y j  with the input distribution via the forward conditional probability PYX  y jxi   starting from the input front with xi given and expressing the probability that y j is the resultant output . We can alterna- tively write PX,Y  xi,y j  as  PX,Y  xi,y j  = PXY  xiy j PY  y j ,  which evaluates the joint probability PX,Y  xi,y j  based on the output distri- bution and the backward conditional probability PXY  xiy j   given that y j is received, the probability that xi is sent . Equating  6.4  with  6.8  yields  PXY  xiy j  =  PYX  y jxi PX  xi   ,  PY  y j   which is the well known Bayes’ Theorem on conditional probabilities [BT02]. In the Bayes’ formula  6.9  we can write PY  y j  in the denominator in terms  of  6.6  to get the equivalent expression  PXY  xiy j  =  PYX  y jxi PX  xi  ∑s i cid:48 =1 PYX  y jxi cid:48   PX  xi cid:48     .  Summing  6.10  over all the xi clearly gives  s  ∑  i=1  PXY  xiy j  =  s  PYX  y jxi PX  xi  ∑ ∑s i cid:48 =1 PYX  y jxi cid:48  PX  xi cid:48   i=1 ∑s i=1 PYX  y jxi PX  xi  ∑s i cid:48 =1 PYX  y jxi cid:48  PX  xi cid:48   = 1,  =  119   6.7    6.8    6.9    6.10    6.11    6.12    6.13   which means that, given output y j, some xi was certainly put into the channel.  6.4 The binary symmetric channel  A simple special case of a channel is the binary channel, which has two input symbols, 0 and 1, and two output symbols, 0 and 1; a schematic description is depicted in Figure 6.4.  The binary channel is said to be symmetric if  PYX  00  = PYX  11 ,  PYX  01  = PYX  10 .   6.14    120  Mutual information and channel capacity  Figure 6.4 The binary channel.  Usually we abbreviate binary symmetric channel to BSC.  Let the probabilities of the input symbols be  and let the BSC probabilities be  The channel matrix is therefore  PX  0  = δ , PX  1  = 1− δ ,  PYX  00  = PYX  11  = 1− ε, PYX  10  = PYX  01  = ε.   cid:32 1− ε  ε  ε  1− ε cid:33   PY  0  =  1− ε δ + ε 1− δ  , PY  1  = εδ +  1− ε  1− δ  .  and the channel relationships  6.6  become   6.15   6.16    6.17   6.18    6.19    6.20   6.21   Note that these equations can be simply checked by computing their sum:  PY  0  + PY  1  =  1− ε + ε δ +  1− ε + ε  1− δ   = δ + 1− δ = 1. Given that we know what symbol we received, what are the probabilities for the various symbols that might have been sent?   6.22   x1 = 0  PYX  00   x2 = 1  PYX  11    6.4 The binary symmetric channel  121  We ﬁrst compute the two denominators in Equation  6.10 :  2  ∑  i=1 2  ∑  i=1  PYX  y1xi PX  xi  =  1− ε δ + ε 1− δ  ,  PYX  y2xi PX  xi  = εδ +  1− ε  1− δ  ,  which of course are the same as  6.20  and  6.21 . We then have  PXY  00  = PXY  10  = PXY  01  = PXY  11  =   1− ε δ ε 1− δ     1− ε δ + ε 1− δ    1− ε δ + ε 1− δ   εδ +  1− ε  1− δ   εδ +  1− ε  1− δ     1− ε  1− δ    εδ  ,  ,  ,  .  Note that this involves the choice of the probabilities of the channel input.  In the special case of equally likely input symbols  δ = 1 2  we have the  very simple equations  PXY  00  = PXY  11  = 1− ε, PXY  10  = PXY  01  = ε.   6.29   6.30  As a more peculiar example, suppose that 1− ε = 9 10 and ε = 1 10 for the BSC, but suppose also that the probability of the input x = 0 being sent is δ = 19 20 and x = 1 being sent is 1− δ = 1 20. We then have   6.23    6.24    6.25    6.26    6.27    6.28    6.31    6.32    6.33    6.34   PXY  00  = PXY  10  = PXY  01  = PXY  11  =  171 172 , 1 172 , 19 28 , 9 28 .  Thus if we receive y = 0, it is more likely that x = 0 was sent because 171 172 is much larger than 1 172. If, however, y = 1 is received, we still have 19 28 > 9 28, and hence x = 0 has a higher probability of being the one that has been sent. Therefore, x = 0 is always claimed regardless of the symbol received. As a result, if a stream of n binary bits is generated according to the probability law PX  0  = δ = 19 20, then there are about n 1− δ   = n 20 1s in the input sequence that will be decoded incorrectly as 0s at the channel output. Hence,   122  Mutual information and channel capacity  the above transmission scheme does not use the channel properly, since irre- spective of n it will incur an average decoding error of about 1 20, signiﬁcantly away from zero.  This situation arises whenever both following conditions are valid:  From  6.25 – 6.28  we see that for the binary symmetric channel these condi- tions are  PXY  00  > PXY  10 , PXY  01  > PXY  11 .   1− ε δ > ε 1− δ  ,  εδ >  1− ε  1− δ  ,  δ > ε, δ > 1− ε;   6.35   6.36    6.37   6.38    6.39   6.40   or equivalently  i.e., the bias in the choice of input symbols is greater than the bias of the chan- nel. This discussion shows that it is possible, for given values of the channel transition probabilities, to come up with values for the channel input probabil- ities that do not make much sense in practice. As will be shown below, we can improve this if we can learn more about the fundamental characteristics of the channel and then use the channel properly through a better assignment of the input distribution.1 To this end, we leverage the entropy deﬁned in Chapter 5 to deﬁne the notion of “capability of a channel for conveying information” in a precise fashion.  6.5 System entropies  We can regard the action of a channel as “transferring” the probabilistic in- formation-carrying message X into the output Y by following the conditional probability law PYX  y jxi . Both the input and output ends of the channel are thus uncertain in nature: we know neither exactly which input symbol will be selected nor which output letter will be certainly seen at the output  rather, only probabilistic characterizations of various input–output events in terms of PX  ·  and PYX  ··  are available . One immediate question to ask is: how much 1 Note that, whereas the source is assumed to be given to us and therefore cannot be modiﬁed, we can freely choose the channel input probabilities by properly designing the channel encoder  see Figure 6.2 .   6.5 System entropies  123  “aggregate” information, or amount of uncertainty, is contained in the overall channel system? From Chapter 5, we know that the average amount of uncer- tainty of the input is quantiﬁed by the entropy as  We have shown that H X  ≥ 0, and H X  = 0 if the input is certain; also, H X  is maximized when all xi are equally likely.2 We can also likewise deﬁne the entropy of the output as  H X  =  s  ∑  i=1  PX  xi log2 cid:18  1  PX  xi  cid:19  .  H Y   =  t  ∑  j=1  PY  y j log2 cid:18  1  PY  y j  cid:19  ,   6.41    6.42   which, as expected, measures the uncertainty of the channel output. If we look at both the input and the output, the probability of the event that X = xi and Y = y j simultaneously occur is given by the joint probability PX,Y  xi,y j   see  6.4  . Analogous to the entropy of X  or Y  , we have the following deﬁnition of the entropy when both X and Y are simultaneously taken into account.  Deﬁnition 6.2 The joint entropy of X and Y , deﬁned as  H X,Y    cid:44  s ∑  i=1  t  ∑  j=1  PX,Y  xi,y j log2 cid:18   1  PX,Y  xi,y j  cid:19  ,   6.43   measures the total amount of uncertainty contained in the channel input and output, hence the overall channel system.  One might immediately ask about the relation between H X,Y   and the in- dividual entropies, in particular whether H X,Y   just equals the sum of H X  and H Y  . This is in general not true, unless X and Y are statistically indepen- dent, meaning that what comes out does not depend on what goes in. More precisely, independence among X and Y is characterized by [BT02]  PX,Y  xi,y j  = PX  xi PY  y j .   6.44   Based on  6.43  and  6.44 , we have the following proposition.  Proposition 6.3  If X and Y are statistically independent, then  H X,Y   = H X  + H Y  .   6.45   2 As noted in Lemma 5.11, H X  ≤ log2 s bits, with equality if PX  xi  = 1 s for all i.    6.46   PY  y j  cid:19  cid:19   6.47   124  Mutual information and channel capacity  Proof With  6.44 , we have  H X,Y   =  =  =  s  ∑  i=1 s  ∑  i=1 s  ∑  i=1  t  ∑  j=1 t  ∑  j=1 t  ∑  j=1 s  +  ∑  i=1  t  ∑  j=1  +  s  ∑  i=1  t  1  PX  xi PY  y j log2 cid:18  PX  xi PY  y j  cid:19  PX  xi PY  y j  cid:18 log2 cid:18  1 PX  xi  cid:19  + log2 cid:18  1 PX  xi PY  y j log2 cid:18  1 PX  xi  cid:19  PX  xi PY  y j log2 cid:18  1 PY  y j  cid:19  ∑ PX  xi log2 cid:18  1 PX  xi  cid:19  ∑  cid:125   cid:124  PY  y j log2 cid:18  1 PY  y j  cid:19  ∑  cid:125   cid:124    cid:123  cid:122   cid:123  cid:122   H X   H Y    j=1  i=1  j=1  PX  xi   s  t  = H X  + H Y  ,  =  PY  y j   where the last equality follows since  t  ∑  j=1  s  ∑  i=1  PY  y j  =  PX  xi  = 1.  In Proposition 6.4 we derive the relation that links the joint entropy H X,Y   with the individual H X   or H Y    when X and Y are dependent, which is typically true since the channel output depends at least partly on the channel input  otherwise no information can be conveyed through the channel .  Proposition 6.4  Chain rule  The following result holds:  where  H X,Y   = H X  + H YX , PX,Y  xi,y j log2 cid:18   ∑  j=1  i=1  t  1  PYX  y jxi  cid:19   H YX   cid:44  s ∑  is the conditional entropy associated with Y given X. Proof By means of the relation PX,Y  xi,y j  = PYX  y jxi PX  xi   see  6.4   it   6.48    6.49    6.50    6.51    6.52    6.53    follows that  H X,Y   =  6.5 System entropies  =  =  =  s  ∑  i=1 s  ∑  i=1  +  s  ∑  i=1  +  s  ∑  i=1  +  i=1  t  t  1  1  j=1  ∑  ∑  j=1 t  j=1 s  PYX  y jxi PX  xi  cid:19  PX  xi  cid:19  PYX  y jxi  cid:19   PX,Y  xi,y j log2 cid:18  PX,Y  xi,y j log2 cid:18  1 PX,Y  xi,y j log2 cid:18  ∑ ∑ log2 cid:18  1 PX  xi  cid:19  t ∑  cid:123  cid:122   cid:124  PX,Y  xi,y j log2 cid:18  PYX  y jxi  cid:19  ∑ ∑ PX  xi log2 cid:18  1 PX  xi  cid:19  PX,Y  xi,y j log2 cid:18  ∑ ∑  PYX  y jxi  cid:19   PX,Y  xi,y j   PX  xi    cid:125   j=1  j=1  j=1  1  1  s  s  t  t  i=1  i=1  = H X  + H YX .  The joint entropy H X,Y   is thus the sum of the input entropy H X  and the conditional entropy H YX , which measures the uncertainty remaining in Y , given that X is known. Note that if X and Y are independent, i.e. one can infer nothing about Y even if X is already known, we have H YX  = H Y   and Proposition 6.4 reduces to Proposition 6.3. Another interpretation of H YX  is that it represents how much must be added to the input entropy to obtain the joint entropy; in this regard, H YX  is called the equivocation of the channel. We can again use  6.4  to rewrite H YX  as  H YX  =  t  j=1  ∑  PYX  y jxi PX  xi log2 cid:18  PX  xi H Yxi ,  s  ∑  i=1 s  ∑  i=1  =  1  PYX  y jxi  cid:19   where  H Yxi   cid:44  t ∑  j=1  PYX  y jxi log2 cid:18   1  PYX  y jxi  cid:19   125   6.54    6.55    6.56    6.57    6.58    6.59    6.60    6.61    126  Mutual information and channel capacity  is the conditional entropy of Y given a particular X = xi. Finally, we remark that, starting from the alternative expression for PX,Y  xi,y j  given in  6.8 , H X,Y   can be accordingly expressed as   6.62  Exercise 6.5 Let  X,Y   have the joint distribution given in Table 6.1. Com- ♦ pute H X , H X,Y  , and H XY  .  H X,Y   = H Y   + H XY  .  Table 6.1 A joint distribution of  X,Y    1  1 8 1 16 1 16 1 4  Y  1 2 3 4  X  2  1 16 1 8 1 16 0  3  1 32 1 32 1 16 0  4  1 32 1 32 1 16 0  Exercise 6.6 Verify that H YX  = H Y   if X and Y are independent.  ♦  6.6 Mutual information  Consider again the transmission system shown in Figure 6.3. We wish to de- termine how much information about the input can be gained based on some particular received output letter Y = y j; this is the ﬁrst step toward quantifying the amount of information that can get through the channel.  At the transmit side, the probability that the ith input symbol xi occurs is PX  xi , which is called the a priori3 probability of xi. Upon receiving Y = y j, one can try to infer which symbol probably has been sent based on the information carried by y j. In particular, given y j is received, the probability that xi has been sent is given by the backward conditional probability PXY  xiy j , which is commonly termed the a posteriori4 probability of xi. The change of probability  from a priori to a posteriori  is closely related to how much information one can learn about xi from the reception of y j. Speciﬁcally, the difference between the uncertainty before and after receiving y j measures the  3 From the Latin, meaning “from what comes ﬁrst” or “before.” 4 From the Latin, meaning “from what comes after” or “afterwards.”   6.6 Mutual information  127  gain in information due to the reception of y j. Such an information gain is called the mutual information and is naturally deﬁned to be   6.63    6.64    6.65   I xi;y j    cid:124   cid:123  cid:122   cid:125   information gain or uncertainty loss after receiving y j  −log2 cid:18   cid:44  log2 cid:18  1 PX  xi  cid:19   cid:123  cid:122   cid:125   cid:124   cid:124  = log2 cid:18  PXY  xiy j  PX  xi   cid:19  .  before receiving y j  uncertainty  1  PXY  xiy j  cid:19   cid:123  cid:122   cid:125   uncertainty  after receiving y j  Note that if the two events X = xi and Y = y j are independent, thereby  PXY  xiy j  = PX  xi ,  we have I xi;y j  = 0, i.e. no information about xi is gained once y j is received. For the noiseless channel, thus y j = xi, we have PXY  xiy j  = 1 since, based on what is received, we are completely certain about which input symbol has been sent. In this case, the mutual information attains the maximum value log2 1 PX  xi  ; this means that all information about xi is conveyed without any loss over the channel.  Since  we have  PXY  xiy j PY  y j  = PX,Y  xi,y j  = PYX  y jxi PX  xi , I xi;y j  = log2 cid:18  PX,Y  xi,y j  PX  xi PY  y j  cid:19  = I y j;xi .   6.66    6.67   Hence, we see that xi provides the same amount of information about y j as y j does about xi. This is why I xi;y j  has been coined “mutual information.”  We have now characterized the mutual information with respect to a partic- ular input–output event. Owing to the random nature of the source and channel output, the mutual information should be averaged with respect to both the input and output in order to account for the true statistical behavior of the channel. This motivates the following deﬁnition.  Deﬁnition 6.7 The system mutual information, or average mutual informa- tion, is deﬁned as  I X;Y    cid:44  s ∑  t  ∑  j=1 t  ∑  j=1  i=1 s  ∑  i=1  =  PX,Y  xi,y j I xi;y j   PX,Y  xi,y j log2 cid:18  PX,Y  xi,y j  PX  xi PY  y j  cid:19  .   6.68    6.69    128  Mutual information and channel capacity  The average mutual information I X;Y   has the following properties  the  proofs are left as exercises .  Lemma 6.8 The system mutual information has the following properties:  1  I X;Y   ≥ 0;  2  I X;Y   = 0 if, and only if, X and Y are independent;  3  I X;Y   = I Y ;X .  Exercise 6.9 Prove Lemma 6.8.  Hint: For the ﬁrst and second property use the IT Inequality  Lemma 5.10 . You may proceed similarly to the proof of Lemma 5.11. The third property can ♦ be proven based on the formula of the mutual information, i.e.  6.69 .  Starting from  6.68 , we have ∑  I X;Y   =  s  measures the information about the entire input X provided by the reception of the particular y j. In an analogous way we can obtain  t  ∑  j=1 t  ∑  j=1  i=1 s  ∑  i=1 t  ∑  j=1  =  =  PX,Y  xi,y j I xi;y j   PXY  xiy j PY  y j I xi;y j   PY  y j I X;y j ,  I X;y j   cid:44  s ∑  i=1  PXY  xiy j I xi;y j   I X;Y   =  PX,Y  xi,y j I xi;y j   t  ∑  j=1 t  ∑  j=1  s  ∑  i=1 s  ∑  i=1 s  ∑  i=1  =  =  PYX  y jxi PX  xi I xi;y j   PX  xi I xi;Y  ,  I xi;Y    cid:44  t ∑  j=1  PYX  y jxi I xi;y j    6.70    6.71    6.72    6.73    6.74    6.75    6.76    6.77   where  where  represents the information about the output Y given that we know the input letter xi is sent.   6.6 Mutual information  129  Let us end this section by specifying the relation between the average mutual information I X;Y   and various information quantities introduced thus far, e.g. input entropy H X , output entropy H Y  , joint entropy H X,Y  , and condi- tional entropies H XY   and H YX . To proceed, let us use  6.69  to express I X;Y   as  I X;Y   =  t  s  =  i=1  j=1  ∑  ∑  ∑  ∑  i=1 s  j=1 t  PX  xi PY  y j  cid:19  PX,Y  xi,y j log2 cid:18  PX,Y  xi,y j  PX,Y  xi,y j  cid:16 log2 PX,Y  xi,y j − log2 PX  xi  −log2 PY  y j  cid:17  PX,Y  xi,y j log2 cid:18  ∑ PX  xi log2 cid:18  1 PX  xi  cid:19  + = H X  + H Y  − H X,Y   ≥ 0.  PX,Y  xi,y j  cid:19  ∑  PY  y j log2 cid:18  1  = −  i=1 s  ∑  ∑  j=1  j=1  i=1  +  1  s  t  t  PY  y j  cid:19   6.80    6.81   Since  we also have  H X,Y   = H X  + H YX  = H Y   + H XY  ,  I X;Y   = H X − H XY   = H Y  − H YX .  We have the following corollary  the proof is left as an exercise . Corollary 6.10  Conditioning reduces entropy  The following inequalities hold:   a    b   0 ≤ H XY   ≤ H X , 0 ≤ H YX  ≤ H Y  ;  H X,Y   ≤ H X  + H Y  .  Part  a  of Corollary 6.10 asserts that conditioning cannot increase entropy, whereas Part  b  shows that the joint entropy H X,Y   is maximized when X and Y are independent. Exercise 6.11 Prove Corollary 6.10.  Hint: Use known properties of I X;Y   and H X .  ♦   6.78    6.79    6.82   6.83    6.84   6.85    6.86   6.87   6.88    130  Mutual information and channel capacity  To summarize: the average mutual information is given by  the equivocation is given by  the joint entropy is given by  I X;Y   =  H X  + H Y  − H X,Y  , H X − H XY  , H Y  − H YX ;  H XY   = H X − I X;Y  , H YX  = H Y  − I X;Y  ;  H X,Y   =  H X  + H Y  − I X;Y  , H X  + H YX , H Y   + H XY  .  A schematic description of the relations between various information quanti- ties is given by the Venn diagram in Figure 6.5.   6.89    6.90   6.91    6.92   Figure 6.5 Relation between entropy, conditional entropy, and mutual infor- mation.  6.7 Deﬁnition of channel capacity  Given the conditional probabilities PYX  y jxi , which deﬁne a channel, what is the maximum amount of information we can send through the channel? This is the main question attacked in the rest of this chapter.  H X,Y    H X   I X ;Y    H XY    H YX    6.8 Capacity of the binary symmetric channel  131  The mutual information connects the two ends of the channel together. It is  deﬁned by  6.84  as  I X;Y   = H X − H XY  ,   6.93   where the entropy H X  is the uncertainty of the channel input before the re- ception of Y , and H XY   is the uncertainty that remains after the reception of Y . Thus I X;Y   is the change in the uncertainty. An alternative expression for I X;Y   is  I X;Y   =  s  ∑  i=1  t  ∑  j=1  PX  xi PYX  y jxi log2 cid:18   i cid:48 =1 PX  xi cid:48  PYX  y jxi cid:48   cid:19  . PYX  y jxi   ∑s   6.94   This formula involves the input symbol frequencies PX  xi ; in particular, for a given channel law PYX  y jxi , I X;Y   depends completely on PX  xi . We saw in the example of a binary symmetric channel  Section 6.4  how a poor match of PX  xi  to the channel can ruin a channel. Indeed, we know that if the probability of one symbol is PX  xi  = 1, then all the others must be zero and the constant signal contains no information.  How can we best choose the PX  xi  to get the most through the channel, and  what is that amount?  Deﬁnition 6.12  Capacity  For a given channel, the channel capacity, de- noted by C, is deﬁned to be the maximal achievable system mutual information I X;Y   among all possible input distributions PX  · :  C  cid:44  max PX  ·   I X;Y  .   6.95   Finding a closed-form solution to the channel capacity is in general difﬁcult, except for some simple channels, e.g. the binary symmetric channel deﬁned in Section 6.4  see also Section 6.8 below .  We would like to point out that even though this deﬁnition of capacity is intuitively quite pleasing, at this stage it is a mere mathematical quantity, i.e. a number that is the result of a maximization problem. However, we will see in Section 6.11 that it really is the capacity of a channel in the sense that it is only possible to transmit signals reliably  i.e. with very small error probability  through the channel as long as the transmission rate is below the capacity.  6.8 Capacity of the binary symmetric channel  Consider again the binary symmetric channel  BSC , with the probability of transmission error equal to ε, as depicted in Figure 6.6.   132  Mutual information and channel capacity  Figure 6.6 Binary symmetric channel  BSC .  The channel matrix is given by   cid:32 1− ε  ε  ε  1− ε cid:33  .  Exercise 6.13 For the BSC, show that  H YX = 0  = H YX = 1  = Hb ε , where Hb ·  is the binary entropy function deﬁned in  5.24 .  We start from the deﬁnition of mutual information  6.85  to obtain the fol-  lowing set of relations:  I X;Y   = H Y  − H YX   PX  x H YX = x   = H Y  − ∑ x∈X = H Y  − cid:0 PX  0 H YX = 0  + PX  1 H YX = 1  cid:1  = H Y  − Hb ε  ≤ 1− Hb ε  bits,  where  6.102  follows since Y is a binary random variable  see Lemma 5.11 . Since equality in  6.102  is attained if Y is uniform, which will hold if input X is uniform, we conclude that the capacity of the BSC is given by  C = 1− Hb ε  bits,  and that the achieving input distribution is PX  0  = PX  1  = 1 2. Alternatively, we can ﬁnd the capacity of a BSC by starting from PX  0  = δ = 1− PX  1  and   6.96    6.97  ♦   6.98   6.99    6.100   6.101   6.102    6.103   PX  0   0  1−ε  PX  1   1  1−ε  ε  ε   6.8 Capacity of the binary symmetric channel  133  expressing I X;Y   as   6.104   I X;Y   = H Y  − H YX   = − cid:0 δ  1− ε  +  1− δ  ε cid:1 log2 cid:0 δ  1− ε  +  1− δ  ε cid:1  − cid:0 δε +  1− δ   1− ε  cid:1 log2 cid:0 δε +  1− δ   1− ε  cid:1  +  1− ε log2 1− ε  + ε log2 ε.   6.105  If we now maximize the above quantity over δ ∈ [0,1], we ﬁnd that the optimal δ is δ = 1 2, which immediately yields  6.103 . Figure 6.7 depicts the mutual information in  6.105  versus δ with respect to three different choices of the error probability ε. As can be seen from the ﬁgure, the peak value of each curve is indeed attained by δ = 1 2.  Figure 6.7 Mutual information over the binary symmetric channel  BSC :  6.105  as a function of δ , for various values of ε.  Figure 6.8 plots the capacity C in  6.103  versus the cross-over probability ε. We see from the ﬁgure that C attains the maximum value 1 bit when ε = 0 or ε = 1, and attains the minimal value 0 when ε = 1 2.  When ε = 0, it is easy to see that C = 1 bit is the maximum rate at which information can be communicated through the channel reliably. This can be achieved simply by transmitting uncoded bits through the channel, and no de- coding is necessary because the bits are received unchanged. When ε = 1 the  ] s t i b [    Y  ;  X   I  0.7  0.6  0.5  0.4  0.3  0.2  0.1     0  0  0.1  0.2  0.3  0.4  0.6  0.7  0.5 δ   134  Mutual information and channel capacity  Figure 6.8 Capacity of the binary symmetric channel  BSC .  same can be achieved with the additional decoding step which complements all the received bits. By doing so, the bits transmitted through the channel can be recovered without error. Thus from a communications point of view, for binary channels, a channel which never makes an error and a channel which always makes an error are equally good.  When ε = 1 2, the channel output is independent of the channel input. Therefore, no information can possibly be communicated through the chan- nel.  6.9 Uniformly dispersive channel  Recall that the channel transition matrix for the BSC is   cid:32 1− ε  ε  ε  1− ε cid:33  ,   6.106   in which the second row is a permutation of the ﬁrst row. In fact, the BSC belongs to the class of uniformly dispersive channels. Deﬁnition 6.14 A channel is said to be uniformly dispersive if the set  A x   cid:44  {PYX  y1x , . . . ,PYX  ytx }   6.107   is identical for each input symbol x. Hence a uniformly dispersive channel has  1  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  0 0  ] s t i b [  C  0.1  0.2  0.3  0.4  0.6  0.7  0.5 ε   6.9 Uniformly dispersive channel  135  a channel matrix  P11 P12 P21 P22 ... ... Ps1 Ps2    ··· P1t ··· P2t ... ... ··· Pst     6.108    6.109    6.110   6.111    6.112    6.113   such that each row is a permutation of the ﬁrst row.  According to the deﬁnition, for a uniformly dispersive channel the entropy  of the output conditioned on a particular input alphabet x being sent, namely  H YX = x  = ∑ y∈Y  PYX  yx log2 cid:18   1  PYX  yx  cid:19  ,  is thus identical for all x. By means of  6.109 , the mutual information between the channel input and output reads  I X;Y   = H Y  − H YX   PX  x H YX = x  PX  x   = H Y  − ∑ x∈X = H Y  − H YX = x  ∑ x∈X  cid:124  = H Y  − H YX = x .   cid:123  cid:122    cid:125   = 1  Equations  6.110 – 6.113  should be reminiscent of  6.98 – 6.101 . This is no surprise since the BSC is uniformly dispersive.  Recall also from  6.102  that the capacity of the BSC is attained with a uni- form output, which can be achieved by a uniform input. At a ﬁrst glance one might expect that a similar argument can be directly applied to the uniformly dispersive case to ﬁnd the capacity of an arbitrary uniformly dispersive chan- nel. However, for a general uniformly dispersive channel, the uniform input does not necessarily result in the uniform output, and neither can the capacity necessarily be achieved with the uniform output. For example, consider the bi- nary erasure channel  BEC  depicted in Figure 6.9. In this channel, the input alphabet is X = {0,1}, while the output alphabet is Y = {0,1,?}. With proba- bility γ, the erasure symbol ? is produced at the output, which means that the input bit is lost; otherwise the input bit is reproduced at the output without er- ror. The parameter γ is thus called the erasure probability. The BEC has the channel transition matrix   cid:32 1− γ  0  γ γ  0  1− γ cid:33    6.114    136  Mutual information and channel capacity  and is thus, by deﬁnition, uniformly dispersive  the second row is a permuta- tion of the ﬁrst row . However, with the input distribution PX  0  = PX  1  = 1 2, the output is, in general, not uniform  PY  ?  = γ and PY  0  = PY  1  =  1− γ  2 . Despite this, the uniform input remains as the capacity-achieving input distribution for the BEC.  Figure 6.9 Binary erasure channel  BEC .  Exercise 6.15 Based on  6.113 , show that the capacity of the BEC is  C = 1− γ bits,   6.115   which is attained with PX  0  = PX  1  = 1 2. The result is intuitively reason- able: since a proportion γ of the bits are lost in the channel, we can recover  at most  a proportion  1− γ  of the bits, and hence the capacity is  1− γ . ♦  6.10 Characterization of the capacity-achieving  input distribution  Even though it is, in general, difﬁcult to ﬁnd the closed-form capacity formula and the associated capacity-achieving input distribution, it is nonetheless pos- sible to specify some underlying properties of the optimal PX  · . The following theorem, which is stated without proof, provides one such characterization in terms of I x;Y  , i.e. the information gain about Y given that X = x is sent  see  6.77  .  0  1  γ  γ  1−γ  1−γ   6.10 Capacity-achieving input distribution  137  Theorem 6.16  Karush–Kuhn–Tucker  KKT  conditions  An input distri- bution PX  ·  achieves the channel capacity C if, and only if, I x;Y   cid:40 = C for all x with PX  x  > 0; ≤ C for all x with PX  x  = 0.   6.116   Remark 6.17 The KKT conditions were originally named after Harold W. Kuhn and Albert W. Tucker, who ﬁrst published the conditions in 1951 [KT51]. Later, however, it was discovered that the necessary conditions for this problem had already been stated by William Karush in his master’s thesis [Kar39].  The assertion of Theorem 6.16 is rather intuitive: if PX  ·  is the capacity- achieving input distribution and PX  x  > 0, i.e. the particular letter x will be used with a nonvanishing probability to convey information over the channel, then the contribution of the mutual information due to this x must attain the capacity; otherwise there will exist another PX cid:48  ·  capable of achieving the capacity by just disregarding this x  thus, PX cid:48  x  = 0  and using more often input letters other than x.  Theorem 6.16 can also be exploited for ﬁnding the capacity of some chan- nels. Consider again the BSC case; the capacity should satisfy one of the fol- lowing three cases:  C = I 0;Y   = I 1;Y    for PX  0  > 0 and PX  1  > 0   6.117   or  or  C = I 0;Y   ≥ I 1;Y    for PX  0  = 1 and PX  1  = 0   6.118   C = I 1;Y   ≥ I 0;Y    for PX  0  = 0 and PX  1  = 1.   6.119   Since  6.118  and  6.119  only yield uninteresting zero capacity, it remains to verify whether or not  6.117  can give a positive capacity. By rearrangement  6.117  implies  C = I 0;Y    =  1  ∑  y=0  PYX  y0 log2 ∑  1  PYX  y0  PY  y   y=0  PYX  y0 log2 PY  y  +  = − = − 1− ε log2 PY  0 − ε log2 PY  1 − Hb ε   y=0  PYX  y0 log2 PYX  y0   1  ∑   6.120    6.121    6.122    6.123    138  and  Mutual information and channel capacity  C = I 1;Y    =  1  ∑  y=0  PYX  y1 log2 ∑  1  PYX  y1  PY  y   y=0  PYX  y1 log2 PY  y  +  = − = −ε log2 PY  0 −  1− ε log2 PY  1 − Hb ε ,  y=0  PYX  y1 log2 PYX  y1   1  ∑  which yields  This can only be satisﬁed if PY  0  = PY  1   = 1 2 . Thus  − 1− ε · log2 PY  0 − ε · log2 PY  1 − Hb ε   = −ε · log2 PY  0 −  1− ε · log2 PY  1 − Hb ε . C = −ε · log2 cid:18 1 2 cid:19 − Hb ε  = 1− Hb ε  bits.  2 cid:19 −  1− ε · log2 cid:18 1   6.124    6.125    6.126    6.127    6.128    6.129    6.130   Exercise 6.18 Repeat the above arguments to derive the capacity of the BEC. ♦  6.11 Shannon’s Channel Coding Theorem  The channel capacity measures the amount of information that can be carried over the channel; in fact, it characterizes the maximal amount of transmis- sion rate for reliable communication. Prior to the mid 1940s people believed that transmitted data subject to noise corruption can never be perfectly recov- ered unless the transmission rate approaches zero [Gal01]. Shannon’s land- mark work [Sha48] in 1948 disproved this thinking and established the well known Channel Coding Theorem: as long as the transmission rate in the same units as the channel capacity, e.g. information bits per channel use, is below  but can be arbitrarily close to  the channel capacity, the error can be made smaller than any given number  which we term arbitrarily small  by some properly designed coding scheme.  In what follows are some deﬁnitions that are required to state the theorem  formally; detailed mathematical proofs can be found in [CT06] and [Gal68]. Deﬁnition 6.19 An  M,n  coding scheme for the channel  X,PYX  yx , Y  consists of the following.   6.11 Shannon’s Channel Coding Theorem  139   1  A message set {1,2, . . . , M}.  2  An encoding function φ : {1,2, . . . , M} → Xn, which is a rule that asso- ciates message m with a channel input sequence of length n, called the mth codeword and denoted by xn m . The set of all codewords  {xn 1 ,xn 2 , . . . ,xn M }  is called the codebook  or simply the code .   3  A decoding function ψ : Yn → {1,2, . . . , M}, which is a deterministic rule  that assigns a guess to each possible received vector.  Deﬁnition 6.20  Rate  The rate R of an  M,n  coding scheme is deﬁned to be  R  cid:44  log2 M  n  bits per transmission.   6.131   In  6.131 , log2 M describes the number of digits needed to write the num- bers 0, . . . , M− 1 in binary form. For example, for M = 8 we need three bi- nary digits  or bits : 000, . . . ,111. The denominator n tells how many times the channel is used for the total transmission of a codeword  recall that n is the codeword length . Hence the rate describes how many bits are transmitted on average in each channel use.  Deﬁnition 6.21 Let  λm  cid:44  Pr[ψ Yn   cid:54 = m Xn = xn m ]   6.132   be the conditional probability that the receiver makes a wrong guess given that the mth codeword is sent. The average error probability λ  n  for an  M,n  coding scheme is deﬁned as  λ  n   cid:44  1 M  M∑  m=1  λm.   6.133   Now we are ready for the famous Channel Coding Theorem due to Shannon.   140  Mutual information and channel capacity  Take the BSC for example. If the cross-over probability is ε = 0.1, the result- ing capacity is C = 0.531 bits per channel use. Hence reliable communication is only possible for coding schemes with a rate smaller than 0.531 bits per channel use.  Although the theorem shows that there exist good coding schemes with arbi- trarily small error probability for long blocklength n, it does not provide a way of constructing the best coding schemes. Actually, the only knowledge we can infer from the theorem is perhaps “a good code favors a large blocklength.” Ever since Shannon’s original ﬁndings, researchers have tried to develop prac- tical coding schemes that are easy to encode and decode; the Hamming code we discussed in Chapter 3 is the simplest of a class of algebraic error-correcting codes that can correct one error in a block of bits. Many other techniques have also been proposed to construct error-correcting codes, among which the turbo code – to be discussed in Chapter 7 – has come close to achieving the so-called Shannon limit for channels contaminated by Gaussian noise.  6.12 Some historical background  In his landmark paper [Sha48], Shannon only used H, R, and C to denote en- tropy, rate, and capacity, respectively. The ﬁrst to use I for information were  5 In Theorem 6.22, 2nR is a convenient expression for the code size and should be understood as either the smallest integer no less than its value or the largest integer no greater than its value. Researchers tend to drop the ceiling or ﬂooring function applying to it, because the ratio of 2nR, against the integer it is understood to be, will be very close to unity as n is large. Since the theorem actually deals with very large codeword lengths n  note that λ  n  approaches zero only when n is very large , the slack use of 2nR as an integer is somehow justiﬁed in concept.  Theorem 6.22  Shannon’s Channel Coding Theorem  For a discrete-time information channel, it is possible to transmit mes- sages with an arbitrarily small error probability  i.e. we have so-called reliable communication , if the communication rate R is below the channel capacity C. Speciﬁcally, for every rate R < C, there exists a sequence of  2nR, n  coding schemes5 with average error probability λ n  → 0 as n → ∞. Conversely, any sequence of  2nR, n  coding schemes with λ n  → 0 must have R ≤ C. Hence, any attempt of transmitting at a rate larger than capacity will for sure fail in the sense that the average error prob- ability is strictly larger than zero.   6.13 Further reading  141  Philip M. Woodward and Ian L. Davies in [WD52]. This paper is a very good read and gives an astoundingly clear overview of the fundamentals of informa- tion theory only four years after the theory had been established by Shannon. The authors give a slightly different interpretation of Shannon’s theory and redevelop it using two additivity axioms. However, they did not yet use the name “mutual information.” The name only starts to appear between 1954 and 1956. In 1954, Mark Pinsker published a paper in Russian [Pin54] with the title “Mutual information between a pair of stationary Gaussian random processes.” However, depending on the translation, the title also might read “The quantity of information about a Gaussian random stationary process, contained in a sec- ond process connected with it in a stationary manner.” Shannon certainly used the term “mutual information” in a paper about the zero-error capacity in 1956 [Sha56].  By the way, Woodward is also a main pioneer in modern radar theory. He had the insight to apply probability theory and statistics to the problem of re- covering data from noisy samples. Besides this, he is a huge clock fan and made many contributions to horology; in particular, he built the world’s most precise mechanical clock, the Clock W5, inventing a completely new mecha- nism.6  6.13 Further reading  Full discussions of the mutual information, channel capacity, and Shannon’s Channel Coding Theorem in terms of probability theory can be found in many textbooks, see, e.g., [CT06] and [Gal68]. A uniﬁed discussion of the capacity results of the uniformly dispersive channel is given in [Mas96]. Further gener- alizations of uniformly dispersive channels are quasi-symmetric channels, dis- cussed in [CA05, Chap. 4], and T-symmetric channels, described in [RG04]. The proof of Theorem 6.16 is closely related to the subject of constrained op- timization, which is a standard technique for ﬁnding the channel capacity; see, e.g., [Gal68] and [CT06]. In addition to the Source Coding Theorem  Theo- rem 5.28 introduced in Chapter 5  and the Channel Coding Theorem  The- orem 6.22 , Shannon’s third landmark contribution is the development of the so-called rate distortion theory, which describes how to represent a continuous source with good ﬁdelity using only a ﬁnite number of “representation levels”; for more details, please also refer to [CT06] and [Gal68].  6 To see some amazing videos of this, search on http:  www.youtube.com for “clock W5.”   142  Mutual information and channel capacity  References  [BT02] Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability.  Athena Scientiﬁc, Belmont, MA, 2002.  [CA05] Po-Ning Chen and Fady Alajaji, Lecture Notes on Information Theory, vol. 1, Department of Electrical Engineering, National Chiao Tung Univer- sity, Hsinchu, Taiwan, and Department of Mathematics & Statistics, Queen’s University, Kingston, Canada, August 2005. Available: http:  shannon.cm.nc tu.edu.tw it itvol12004.pdf  [CT06] Thomas M. Cover and Joy A. Thomas, Elements of Information Theory,  2nd edn.  John Wiley & Sons, Hoboken, NJ, 2006.  [Gal68] Robert G. Gallager, Information Theory and Reliable Communication.  John  Wiley & Sons, New York, 1968.  [Gal01] Robert G. Gallager, “Claude E. Shannon: a retrospective on his life, work, and impact,” IEEE Transactions on Information Theory, vol. 47, no. 7, pp. 2681–2695, November 2001.  [Kar39] William Karush, “Minima of functions of several variables with inequalities as side constraints,” Master’s thesis, Department of Mathematics, University of Chicago, Chicago, IL, 1939.  [KT51] Harold W. Kuhn and Albert W. Tucker, “Nonlinear programming,” in Pro- ceedings of Second Berkeley Symposium on Mathematical Statistics and Probability, J. Neyman, ed. University of California Press, Berkeley, CA, 1951, pp. 481–492.  [Mas96] James L. Massey, Applied Digital Information Theory I and II, Lecture notes, Signal and Information Processing Laboratory, ETH Zurich, 1995 1996. Available: http:  www.isiweb.ee.ethz.ch archive massey scr   [Pin54] Mark S. Pinsker, “Mutual information between a pair of stationary Gaus- sian random processes,”  in Russian , Doklady Akademii Nauk SSSR, vol. 99, no. 2, pp. 213–216, 1954, also known as “The quantity of information about a Gaussian random stationary process, contained in a second process connected with it in a stationary manner.”  [RG04] Mohammad Rezaeian and Alex Grant, “Computation of total capacity for discrete memoryless multiple-access channels,” IEEE Transactions on Infor- mation Theory, vol. 50, no. 11, pp. 2779–2784, November 2004.  [Sha48] Claude E. Shannon, “A mathematical theory of communication,” Bell System Technical Journal, vol. 27, pp. 379–423 and 623–656, July and October 1948. Available: http:  moser.cm.nctu.edu.tw nctu doc shannon1948.pdf  [Sha56] Claude E. Shannon, “The zero error capacity of a noisy channel,” IRE Trans-  actions on Information Theory, vol. 2, no. 3, pp. 8–19, September 1956.  [WD52] Philip M. Woodward and Ian L. Davies, “Information theory and inverse probability in telecommunication,” Proceedings of the IEE, vol. 99, no. 58, pp. 37–44, March 1952.   7  Approaching the Shannon limit by turbo coding  7.1 Information Transmission Theorem  The reliable transmission of information-bearing signals over a noisy commu- nication channel is at the heart of what we call communication. Information theory, founded by Claude E. Shannon in 1948 [Sha48], provides a mathemat- ical framework for the theory of communication. It describes the fundamental limits to how efﬁciently one can encode information and still be able to recover it with negligible loss.  At its inception, the main role of information theory was to provide the en- gineering and scientiﬁc communities with a mathematical framework for the theory of communication by establishing the fundamental limits on the per- formance of various communication systems. Its birth was initiated with the publication of the works of Claude E. Shannon, who stated that it is possible to send information-bearing signals at a ﬁxed code rate through a noisy commu- nication channel with an arbitrarily small error probability as long as the code rate is below a certain ﬁxed quantity that depends on the channel character- istics [Sha48]; he “baptized” this quantity with the name of channel capacity  see the discussion in Chapter 6 . He further proclaimed that random sources – such as speech, music, or image signals – possess an irreducible complexity beyond which they cannot be compressed distortion-free. He called this com- plexity the source entropy  see the discussion in Chapter 5 . He went on to assert that if a source has an entropy that is less than the capacity of a com- munication channel, then asymptotically error-free transmission of the source over the channel can be achieved. This result is usually referred to as the Infor- mation Transmission Theorem or the Joint Source–Channel Coding Theorem.   144  Approaching the Shannon limit by turbo coding  Recall from Deﬁnition 6.20 that the rate of a code is deﬁned as  R =  log2 M  n  bits per transmission.  From Figure 7.1 we now see that here we have1 M = 2k, i.e.  R =  log2 2k   n  =  k n .  kTs = nTc.  On the other hand, it is also apparent from Figure 7.1 that, due to timing rea- sons, we must have  1 The number of codewords M is given by the number of different source sequences Uk of length k. For simplicity we assume here that the source is binary, i.e. U = 2. In general we have M = Uk and R =  k n log2 U.   7.3    7.4    7.5   Theorem 7.1  Information Transmission Theorem  Consider the transmission of a source Uk =  U1,U2, . . . ,Uk  through a channel with input Xn =  X1, X2, . . . , Xn  and output Yn =  Y1,Y2, . . . ,Yn  as shown in Figure 7.1. Assume that both the source sequence U1,U2, . . . ,Uk and the noise sequence N1, N2, . . . , Nn are inde- pendent and identically distributed. Then, subject to a ﬁxed code rate R = k n, there exists a sequence of encoder–decoder pairs such that  the decoding error, i.e. Pr cid:2  ˆUk 6= Uk cid:3 , can be made arbitrarily small   i.e. arbitrarily close to zero  by taking n sufﬁciently large if  H U  bits second <  I X ;Y   bits second,   7.1   where the base-2 logarithm is adopted in the calculation of entropy and mutual information  so they are in units of bits , and Ts and Tc are, respectively, the time  in units of second  to generate one source symbol Uℓ and the time to transmit one channel symbol Xℓ. On the other hand, if  H U  bits second >  I X ;Y   bits second,   7.2   then Pr cid:2  ˆUk 6= Uk cid:3  has a universal positive lower bound for all coding  schemes of any length k; hence, the error cannot be made arbitrarily small.  1 Tc  max PX  1 Tc  max PX  1 Ts  1 Ts   7.2 The Gaussian channel  145  Transmitter Uk, . . . ,U2,U1 Encoder   cid:45   Channel  cid:45  ⊕  cid:54   Nn, . . . ,N2,N1  Xn, . . . ,X1  Yn, . . . ,Y1  cid:45   ˆUk, . . . , ˆU2, ˆU1  cid:45   Receiver  Decoder  Figure 7.1 Noisy information transmission system.  Combined with  7.4  we thus see that the code rate can also be represented as  R =  Tc Ts  .   7.6   In Sections 7.3 and 7.4, we will further explore the two situations corre-  sponding to whether condition  7.1  or  7.2  is valid.  7.2 The Gaussian channel  Figure 7.1 considers the situation of a binary source U1,U2, . . . ,Uk being trans- mitted through a noisy channel that is characterized by  Y cid:96  = X cid:96  + N cid:96 ,   cid:96  = 1,2, . . . ,n,   7.7   where X1,X2, . . . ,Xn and Y1,Y2, . . . ,Yn are, respectively, the channel input and channel output sequences, and N1,N2, . . . ,Nn is the noise sequence.  Assume that U cid:96  is either 0 or 1, and Pr[U cid:96  = 0] = Pr[U cid:96  = 1] = 1 2. Also assume that U1,U2, . . . ,Uk are all independent.2 Hence, its average entropy is equal to 1 k  Pr[Uk = uk] cid:19  bits source symbol  H Uk  =  1 k ∑ uk∈{0,1}k  1  Pr cid:104 Uk = uk cid:105 log2 cid:18  2−k log2 cid:18  1  2−k cid:19  bits source symbol  =  1 k ∑ uk∈{0,1}k  = 1 bit source symbol,   7.8    7.9    7.10   where we abbreviate  U1,U2, . . . ,Uk  as Uk.  In a practical communication system, there usually exists a certain constraint  2 This assumption is well justiﬁed in practice: were U cid:96  not uniform and independent, then any good data compression scheme could make them so. For more details on this, we refer to Appendix 7.7.   146  Approaching the Shannon limit by turbo coding  E on the transmission power  for example, in units of joule per transmission . This power constraint can be mathematically modeled as  1 + x2 x2  2 +··· + x2 n  n  ≤ E for all n.   7.11    7.12   When being transformed into an equivalent statistical constraint, one can re- place  7.11  by  E cid:20 X 2  1 + X 2  2 +··· + X 2 n  n   cid:21  = E cid:2 X 2 1 cid:3  ≤ E, 2 cid:3  = ··· = E cid:2 X 2  1 cid:3  = E cid:2 X 2  where E[·] denotes the expected value of the target random variable, and equal- n cid:3 , i.e. the channel ity holds because we assume E cid:2 X 2 encoder is expected to assign, on average, an equal transmission power to each channel input. Note that the channel inputs are in general strongly dependent so as to combat interference; what we assume here is that they have, on average, equal marginal power. We further assume that the noise samples N1,N2, . . . ,Nn are independent in statistics and that the probability density function3 of each N cid:96  is given by  fN cid:96  t  =  1  √2πσ 2  exp cid:18 −  t2  2σ 2 cid:19  ,  t ∈ ℜ.   7.13   This is usually termed the zero-mean Gaussian distribution, and the corre- sponding channel  7.7  is therefore called the Gaussian channel.  7.3 Transmission at a rate below capacity  It can be shown that the channel capacity  i.e. the largest code rate below which arbitrarily small error probability can be obtained  of a Gaussian channel as deﬁned in Section 7.2 is  C E  = max  I X;Y    fX :E[X2]≤E 1 2  log2 cid:18 1 +  =  E  σ 2 cid:19  bits channel use,   7.14    7.15   3 A probability density function is the density function for probability. Similar to the fact that the density of a material describes its mass per unit volume, the probability density function gives the probability of occurrence per unit point. Hence, integration of the material density over a volume leads to the mass conﬁned within it, and integration of the probability density function over a range tells us the probability that one will observe a value in this range. The Gaussian density function in  7.13  is named after the famous mathematician Carl Friedrich Gauss, who used it to analyze astronomical data. Since it is quite commonly seen in practice, it is sometimes named the normal distribution.   1 Ts  1 Ts  7.4 Transmission at a rate above capacity  147  where the details can be found in [CT06, Eq.  9.16 ]. Recall from the Informa- tion Transmission Theorem  Theorem 7.1  that if the source entropy  namely, 1 bit source symbol  is less than the capacity of a communication channel  i.e.  1 2 log2 1+E σ 2  bits per channel use , then reliable transmission becomes feasible. Hence, in equation form, we can present the condition for reliable transmission as follows:  bits second <  1 2Tc  log2 cid:18 1 +  E  σ 2 cid:19  bits second.   7.16   Note that when comparing we have to represent the average source entropy and channel capacity by the same units  here, bits second . This is the reason why we have introduced Ts and Tc.  7.4 Transmission at a rate above capacity  Now the question is what if  bits second >  1 2Tc  log2 cid:18 1 +  E  σ 2 cid:19  bits second.   7.17   In such a case, we know from the Information Transmission Theorem  Theo- rem 7.1  that an arbitrarily small error probability cannot be achieved. How- ever, can we identify the smallest error rate that can be possibly obtained?  Uk, . . . ,U1 Com- pressor   cid:45   Transmitter  Encoder   cid:54   Vk, . . . ,V1  Channel Xn, . . . ,X1 ⊕  cid:45   cid:54  Nn, . . . ,N1  Yn, . . . ,Y1  cid:45   Receiver  Decoder  ˆVk, . . . , ˆV1   cid:45   Figure 7.2 Noisy information transmission system with incorporated com- pressor.  A straightforward system design is to map each source sequence u1, . . . ,uk into a compressed binary sequence vk  cid:44  g uk  for transmission  see Fig- ure 7.2 , where the compressor function g ·  is chosen such that the resulting average entropy is less than the channel capacity, i.e.  1 Ts  1 k  H Vk  bits second <  1 2Tc  log2 cid:18 1 +  E  σ 2 cid:19  bits second.   7.18   Then, from Theorem 7.1 we know that V1,V2, . . . ,Vk can be transmitted through the additive Gaussian noise channel with arbitrarily small error. This is to say,   148  Approaching the Shannon limit by turbo coding  there exists a transmission scheme such that the decision at the channel out- put ˆv1, ˆv2, ˆv3, . . . is the same as the compressed channel input v1,v2,v3, . . . with probability arbitrarily close to unity.4 As a consequence, the error that is intro- duced in this straightforward system design occurs only at those instances  i.e.  cid:96 s  where the compression is not reversible, i.e. u cid:96  cannot be recovered from vk.  In the following, we will determine a minimum error probability that the straightforward system in Figure 7.2 cannot beat even if we optimize over all possible compressor designs subject to the average entropy of the compressor output being smaller than the channel capacity. Before we come to this analy- sis, we give several examples of how a compressor works and what is its error. Example 7.2 For example, let g ·  map u1,u2,u3, . . . into v1,v2,v3, . . . in a fashion that   v1,v2,v3,v4, . . .  = g u1,u2,u3,u4, . . .  =  u1,u1,u3,u3, . . . ,   7.19  i.e. v2 cid:96 −1 = v2 cid:96  = u2 cid:96 −1  see Table 7.1 . Since V2 cid:96 −1 = V2 cid:96  for every  cid:96 , no new information is provided by V2 cid:96  given V2 cid:96 −1. The average entropy of V1,V2, . . . , Vk, with k = 2m even, is then given by  1 k  H Vk  =  H V1,V2, . . . ,V2m   1 2m 1 2m 1 2m 1 2  =  =  =  H V1,V3, . . . ,V2m−1  H U1,U3,U5, . . . ,U2m−1  bits source symbol,  1 Ts  1 k  H Vk  =  bits second.  1 2Ts   7.20    7.21    7.22    7.23    7.24   4 What Shannon targeted in his theorem is the block error rate, not the bit error rate. Hence, his  theorem actually concludes that Pr cid:2  V1,V2, . . . ,Vk  =   ˆV1, ˆV2, . . . , ˆVk  cid:3   cid:39  1 when k is sufﬁciently  large since  i.e.  is less than  1 Ts  1 k  H Vk  bits second  1 2Tc  log2 cid:18 1 +  E  σ 2 cid:19  bits second.  This is a very strong statement because, for example, v cid:96  = ˆv cid:96  for 1 ≤  cid:96  ≤ k− 1 and vk  cid:54 = ˆvk will be counted as one block error even though there is only one bit error among these k bits. Note that Pr cid:2  V1,V2, . . . ,Vk  =   ˆV1, ˆV2, . . . , ˆVk  cid:3   cid:39  1 surely implies that Pr cid:2 V cid:96  = ˆV cid:96  cid:3   cid:39  1 for most  cid:96 ,  but not vice versa.   7.4 Transmission at a rate above capacity  149  Table 7.1 The mapping g ·  from uk to vk deﬁned in  7.19  in Example 7.2  u1,u2,u3,u4, . . .  v1,v2,v3,v4, . . .  0000. . . 0001. . . 0010. . . 0011. . . 0100. . . 0101. . . 0110. . . 0111. . . 1000. . . 1001. . . 1010. . . 1011. . . 1000. . . 1101. . . 1110. . . 1111. . .  0000. . . 0000. . . 0011. . . 0011. . . 0000. . . 0000. . . 0011. . . 0011. . . 1100. . . 1100. . . 1111. . . 1111. . . 1100. . . 1100. . . 1111. . . 1111. . .  Under the premise that  1 2Ts  bits second <  1 2Tc  log2 cid:18 1 +  E  σ 2 cid:19  bits second,   7.25   the average “bit” error rate of the system is given by  1  1  =  +··· + Pr[U2m  cid:54 = V2m] cid:17   k cid:16 Pr[U1  cid:54 = V1] + Pr[U2  cid:54 = V2] + Pr[U3  cid:54 = V3] + Pr[U4  cid:54 = V4] +··· + Pr[Uk  cid:54 = Vk] cid:17  2m cid:16 Pr[U1  cid:54 = V1] + Pr[U2  cid:54 = V2] + Pr[U3  cid:54 = V3] + Pr[U4  cid:54 = V4] 2m cid:16 0 + Pr[U2  cid:54 = V2] + 0 + Pr[U4  cid:54 = V4] +··· + Pr[U2m  cid:54 = V2m] cid:17   7.27  2m cid:16 Pr[U2  cid:54 = U1] + Pr[U4  cid:54 = U3] +··· + Pr[U2m  cid:54 = U2m−1] cid:17  Pr[U2  cid:54 = U1] 2 cid:16 Pr[ U1,U2  =  01 ] + Pr[ U1,U2  =  10 ] cid:17    7.26    7.28    7.29    7.30   1 2 1  =  =  =  =  1  1   150  Approaching the Shannon limit by turbo coding  =  1  2 cid:18 1  4 +  1  4 cid:19  =  1 4 .   7.31   In fact, the error rate of 1 4 can be obtained directly without computation by observing that all the odd-indexed bits U1,U3,U5, . . . can be correctly recovered from V1,V3,V5, . . .; however, the sequence Vk provides no information for all the even-indexed bits U2,U2,U6, . . . Thus, we can infer that a zero error rate for odd-indexed bits and a 1 2 error rate based on a pure guess for even-indexed ♦ bits combine to a 1 4 error rate.  Example 7.2 provides a compressor with average bit error rate  BER  of 1 4 between input U1,U2, . . . ,Uk and output V1,V2, . . . ,Vk subject to its aver- age output entropy 1 2Ts bits second being smaller than the channel capac- ity C E  =  1 2Tc log2 1 + E σ 2 . However, dropping all even-indexed bits may not be a good compressor design because it is possible that half of the bits in V1,V2, . . . ,Vk are different from U1,U2, . . . ,Uk; i.e., in the worst case, the difference between compressor input U1,U2, . . . ,Uk and compressor output V1,V2, . . . ,Vk will result in a large distortion of k 2 bits.  In Section 3.3.3 we saw that the  7,4  Hamming code is a perfect packing of radius-one spheres in the 7-dimensional binary space. Using this property, we can provide in Example 7.3 an alternative compressor design such that the input and output are different in at most 1 bit with a  smaller  average BER of 1 8 and a  slightly larger  average output entropy of 4 7Ts bits second.  Example 7.3 A compressor g is deﬁned based on the  7,4  Hamming code- words listed in Table 3.2 as follows: g u7  = v7 if v7 is a  7,4  Hamming code- word and u7 is at Hamming distance at most one from v7. The perfect packing of the 16 nonoverlapping radius-one spheres centered at the codewords for the  7,4  Hamming code guarantees the existence and uniqueness of such a v7; hence, the compressor function mapping is well deﬁned.  The probability of each  7,4  Hamming codeword appearing at the output is 8· 2−7 = 2−4  since there are eight u7 mapped to the same v7 . Hence,  1 7  H V7  =  2−4 log2 cid:18  1 2−4 cid:19   1 7 ∑ v7∈CH 4 bits source symbol, 7  =  1 7Ts  H V7  =  bits second.  4 7Ts   7.32    7.33    7.34   where CH denotes the set of the 16 codewords of the  7,4  Hamming code. Hence,   7.4 Transmission at a rate above capacity  151 Next, we note that Pr[U1  cid:54 = V1] = 1 8 because only one of the eight u7 that are mapped to the same v7 results in a different ﬁrst bit. Similarly, we can obtain   7.35    7.36   Hence, the average BER is given by  Pr[U2  cid:54 = V2] = Pr[U3  cid:54 = V3] = ··· = Pr[U7  cid:54 = V7] = 7 cid:0 Pr[U1  cid:54 = V1] + Pr[U2  cid:54 = V2] +···Pr[U7  cid:54 = V7] cid:1  =  1  1 8 .  1 8 .  BER =  This example again shows that data compression can be regarded as the oppo- site operation of error correction coding, where the former removes the redun- dancy  or even some information such as in this example  while the latter adds ♦ controlled redundancy to combat the channel noise effect. Exercise 7.4 Design a compressor mapping by reversing the roles of encoder and decoder of the three-times repetition code. Prove that the average BER is ♦ 1 4 and the average output entropy equals 1 3 bits per source symbol.  Readers may infer that one needs to know the best compressor design, which minimizes the BER subject to the average output entropy less than C E , in or- der to know what will be the minimum BER attainable for a given channel  or more speciﬁcally for a given C E  . Ingeniously, Shannon identiﬁes this minimum BER without specifying how it can be achieved. We will next de- scribe his idea of a converse proof that shows that the minimum BER cannot be smaller than some quantity, but that does not specify g · . U1,U2, . . . ,Uk and output V1,V2, . . . ,Vk. Then, by  We can conceptually treat the compressor system as a channel with input  H VkUk  = H g Uk Uk  = 0,  we derive from  6.85  that  I Uk;Vk  = H Uk − H UkVk  = H Vk − H VkUk  = H Vk .   7.38   This implies that the average entropy of the compressor output is equal to  H UkVk  = 1−  H UkVk  bits.  1 k  1 k  1 k  H Vk  =  1 H Uk − k By the chain rule for entropy,5 H UkVk   H Vk  = 1−  1 k  1 k  5 The chain rule for entropy is   7.37    7.39    7.40   H Uk  = H U1  + H U2U1  + H U3U1,U2  +··· + H UkU1, . . . ,Uk−1 .  This is a generalized form of Proposition 6.4 and can be proven similarly.   152  Approaching the Shannon limit by turbo coding  = 1−  ≥ 1−  1  k cid:0 H U1Vk  + H U2U1,Vk  + H U3U1,U2,Vk  +··· + H UkU1, . . . ,Uk−1,Vk  cid:1  k cid:0 H U1V1  + H U2V2  + H U3V3  +··· + H UkVk  cid:1 ,  1   7.41    7.42   where  7.42  holds because additional information always helps to decrease entropy; i.e., H U cid:96 U1, . . . ,U cid:96 −1,V1, . . . ,Vk  ≤ H U cid:96 V cid:96   since the former has additional information  U1, . . . ,U cid:96 −1,V1, . . . ,V cid:96 −1,V cid:96 +1, . . . ,Vk   see Corollar- y 6.10 .  We proceed with the derivation by pointing out that H U cid:96 V cid:96   = Pr[V cid:96  = 0]H U cid:96 V cid:96  = 0  + Pr[V cid:96  = 1]H U cid:96 V cid:96  = 1   = Pr[V cid:96  = 0]Hb cid:0 Pr[U cid:96  = 1 V cid:96  = 0] cid:1  + Pr[V cid:96  = 1]Hb cid:0 Pr[U cid:96  = 0 V cid:96  = 1] cid:1  ≤ Hb cid:16 Pr[V cid:96  = 0]Pr[U cid:96  = 1 V cid:96  = 0] + Pr[V cid:96  = 1]Pr[U cid:96  = 0 V cid:96  = 1] cid:17   = Hb BER cid:96  , where BER cid:96   cid:44  Pr[U cid:96   cid:54 = V cid:96 ];   7.43    7.44    7.45   7.46   Hb p   cid:44  plog2  1 p +  1− p log2  1 1− p ,  for 0 ≤ p ≤ 1,   7.47   is the so-called binary entropy function  see Section 5.2.2 ; and  7.45  follows from the concavity6 of the function Hb · . We then obtain the ﬁnal lower bound of the output average entropy:  1 k  1  k  1 k  Hb BER cid:96    k cid:0 Hb BER1  + Hb BER2  +··· + Hb BERk  cid:1  H Vk  ≥ 1− ∑ = 1− ≥ 1− Hb cid:32 1 ∑ = 1− Hb BER .  BER cid:96  cid:33    cid:96 =1   cid:96 =1  k  k   7.48    7.49    7.50    7.51   Here,  7.50  follows again from concavity.  In conclusion, the Information Transmission Theorem  Theorem 7.1  iden- tiﬁes the achievable bit error rate  BER  for the target additive Gaussian noise  6 For a deﬁnition of concavity see Appendix 7.8.   7.4 Transmission at a rate above capacity  153  channel as follows:  1 Ts   1− Hb BER   ≤  1 Ts  1 k  H Vk  <  1 2Tc  log2 cid:18 1 +  E  σ 2 cid:19  ,   7.52   where the ﬁrst inequality follows from  7.51  and the second follows from our assumption  7.18 . In usual communication terminologies, people denote R = Tc Ts = k n  information bit carried per channel use  as the channel code rate; N0  cid:44  2σ 2  joule  as the noise energy level; Eb  cid:44  ETs Tc  joule  as the equivalent transmitted energy per information bit; and γb  cid:44  Eb N0 as the signal-to-noise power ratio per information bit. This transforms the above in- equality to  Equation  7.53  clearly indicates that the BER cannot be made smaller than  Hb BER  > 1− b  cid:18 1−  1 2R  H−1  1 2R  log2  1 + 2Rγb  .  log2  1 + 2Rγb  cid:19  ,   7.53    7.54   where H−1 b  ·  is the inverse function of the binary entropy function Hb ξ    see Section 5.2.2  for ξ ∈ [0,1 2]. Shannon also proved the  asymptotic  achiev- ability of this lower bound. Hence,  7.53  provides the exact margin on what we can do and what we cannot do when the amount of information to be trans- mitted is above the capacity.  We plot the curves corresponding to R = 1 2 and R = 1 3 in Figure 7.3. The ﬁgure indicates that there exists a rate-1 2 system design that can achieve BER = 10−5 at γb,dB  cid:44  10log10 Eb N0  close to 0 dB, i.e. for Eb  cid:39  N0. On the other hand, no system with a rate R = 1 2 can yield a BER less than 10−5 if the signal energy per information bit Eb is less than the noise energy level N0. Information theorists therefore call this threshold the Shannon limit.  For decades  ever since Shannon ingeniously drew such a sacred line in 1948 simply by analysis , researchers have tried to ﬁnd a good design that can achieve the Shannon limit. Over the years, the gap between the real transmis- sion scheme and this theoretical limit has been gradually closed. For example, a concatenated code [For66] proposed by David Forney can reach BER = 10−5 at about γb,dB  cid:39  2 dB. However, no schemes could push their performance curves within 1 dB of the Shannon limit until the invention of turbo codes in 1993 [BGT93]. Motivated by the turbo coding idea, the low-density parity- check  LDPC  codes were subsequently rediscovered7 in 1998; these could  7 We use “rediscover” here because the LDPC code was originally proposed by Robert G. Gal- lager in 1962 [Gal62]. However, due to its high complexity, computers at that time could not perform any simulations on the code; hence, nobody realized the potential of LDPC codes. It   154  Approaching the Shannon limit by turbo coding  Figure 7.3 The Shannon limits for rates 1 2 and 1 3 codes on continuous- input AWGN channels. Decibel  abbreviated as dB  is a logarithmic scaling of a given quantity; i.e., we ﬁrst take the base-10 logarithm and then multiply by 10. So, e.g., γb,dB  cid:44  10log10 γb  = 10log10 Eb N0 .  reduce the performance gap  between the LDPC codes and the Shannon limit  within, e.g., 0.1 dB. This counts 50 years of efforts  from 1948 to 1998  until we ﬁnally caught up with the pioneering prediction of Shannon in the classical additive Gaussian noise channel.  With excitement, we should realize that this is just the beginning of closing the gap, not the end of it. Nowadays, the channels we face in reality are much more complicated than the simple additive Gaussian noise channel. Multipath and fading effects, as well as channel nonlinearities, make the Shannon-limit approaching mission in these channels much more difﬁcult. New ideas other than turbo and LDPC coding will perhaps be required in the future. So we are waiting for some new exciting results, similar to the discovery of turbo codes in 1993.  was Matthew Davey and David MacKay who rediscovered and examined the code, and con- ﬁrmed its superb performance in 1998 [DM98].  Shannon limit  R = 1 2 R = 1 3  R E B  10−3  1  10−1  10−2  10−4  10−5  10−6  −6  −5  −4  −3  −2  γb,dB [dB]  −1 −0.5 0  1  2   7.5 Turbo coding: an introduction  155  7.5 Turbo coding: an introduction  Of all error-correcting codes to date, the turbo code was the ﬁrst that could approach the Shannon limit within 1 dB, at BER = 10−5, over the additive Gaussian noise channel. It is named the turbo code because the decoder func- tions iteratively like a turbo machine, where two turbo engines take turns to reﬁne the previous output of the other until a certain number of iterations is reached.  Figure 7.4 Exempliﬁed turbo encoder from [BGT93]. An example of how a length-5 input sequence passes through this encoder is depicted in Figure 7.5. The complete list of all length-5 input sequences with their corresponding codewords is given in Table 7.2.  As an example, an information sequence  s1,s2,s3,s4,s5  =  10100  is fed into the turbo encoder shown in Figure 7.4, where s1 is inserted ﬁrst. In this ﬁgure, the squares marked with t1, t2, t3, and t4 are clocked memory elements, usually named ﬂip-ﬂops or simply registers, which store the coming input binary data and, at the same time, output its current content according to a clocked timer. The square marked with π is the interleaver that permutes the input sequence into its interleaved counterpart. The notation “⊕” denotes the modulo-2 addition. This ﬁgure then indicates that the output sequence from node x1 will be the original information sequence  s1,s2,s3,s4,s5  =  10100 . Since the contents of all four registers are initially zero, and since  t1  cid:96  + 1  = s cid:96  ⊕t1  cid:96  ⊕t2  cid:96  ⊕t3  cid:96  ⊕t4  cid:96  ,   7.55   information seq.  --  W cid:15  cid:11  cid:9  ⊕  t1  6 -  t2  6 -  t3  ? π  ?  interleaved seq.  - -  W cid:15  cid:11  cid:9  ⊕  t1  6 -  t2  6 -  t3  6 -  6 -  t4  t4  - x1  6  ?- -  x2  ⊕ 6  ?- -  ⊕  x3   156  Approaching the Shannon limit by turbo coding  register contents  output sequence  t4 → x2 5 x2 4 x2 3 x2 2 x2 1  0  input  sequence s5s4s3s2s1 → t1 0  00101  t2 0  0  1  1  1  1  t3 0  0  0  1  1  1  0010  001  00  0  1  1  1  1  0  0  0  0  1  1  1  11  111  1111  11111  Figure 7.5 The snap show of the input and output sequences of the turbo encoder from Figure 7.4 at node x2. Note that we have mirrored the sequences to match the direction of the register placement in Figure 7.4.  t2  cid:96  + 1  = t1  cid:96  , t3  cid:96  + 1  = t2  cid:96  , t4  cid:96  + 1  = t3  cid:96  ,   7.56   7.57   7.58   7.59   x2  cid:96   = t1  cid:96  + 1 ⊕t4  cid:96   = s cid:96  ⊕t1  cid:96  ⊕t2  cid:96  ⊕t3  cid:96  ,  where  cid:96  represents the clocked time instance, we obtain the output sequence from node x2 as 11111  see Figure 7.5 . Note that in order to start indexing all sequences from 1, we re-adjust the index at the output such that x2  cid:96   is actually outputted at clocked time instance  cid:96  + 1.  An important feature of the turbo code design is the incorporation of an  interleaver π that permutes the input sequence. For example,  π s1s2s3s4s5  = s4s2s1s5s3.   7.60   In concept, the purpose of adding an interleaver is to introduce distant depen- dencies into the codewords. Notably, a strong dependency among the code bits can greatly enhance their capability against local noisy disturbance. A good example is a code of two codewords, i.e.  000000000000000  and  111111111111111,  where the code bits are all the same and hence are strongly dependent. Since the receiver knows all code bits should be the same, the local noisy distur- bances that alter, for example, code bits 3, 7, and 10, yielding  001000100100000,   7.5 Turbo coding: an introduction  157  Table 7.2 The information sequences of length 5 and their respective turbo  codewords of length 15 for the turbo code in Figure 7.4 with interleaver  π s1s2s3s4s5  = s4s2s1s5s3  Codewords  Information sequences  s  s1s2s3s4s5  x1 = s  x2 1 x2 2 x2 3 x2 4 x2 5   x3 1 x3 2 x3 3 x3 4 x3 5   x1  — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —  00000 00001 00010 00011 00100 00101 00110 00111 01000 01001 01010 01011 01100 01101 01110 01111 10000 10001 10010 10011 10100 10101 10110 10111 11000 11001 11010 11011 11100 11101 11110 11111  x2  00000 00001 00011 00010 00110 00111 00101 00100 01100 01101 01111 01110 01010 01011 01001 01000 11001 11000 11010 11011 11111 11110 11100 11101 10101 10100 10110 10111 10011 10010 10000 10001  x3  00000 00011 11001 11010 00001 00010 11000 11011 01100 01111 10101 10110 01101 01110 10100 10111 00110 00101 11111 11100 00111 00100 11110 11101 01010 01001 10011 10000 01011 01000 10010 10001   158  Approaching the Shannon limit by turbo coding  can be easily recovered back to the transmitted codeword  000000000000000.  By interleaving the information bits, s1 may now affect distant code bits such as x3  cid:96  , where  cid:96  can now be much larger than the number of registers, 4. This is contrary to the conventional coding scheme for which the code bit is only a function of several recent information bits. For example, without interleaving, s1 can only affect x2 4  but not any x2  cid:96   with  cid:96  > 4 according to  7.55 – 7.59 . We can of course purposely design a code such that each code bit is a function of more distant information bits, but the main problem here is that the strong dependency of code bits on distant information bits will make the decoder infeasibly complex.  This leads to another merit of using the interleaver: it helps structure a fea- sible decoding scheme, i.e. turbo decoding. In short, the sub-decoder based on x1 and x2 will deal with a code that only has local dependencies as each code bit only depends on the previous four information bits. The second sub- decoder based on interleaved x1 and x3 similarly handles a code with only local dependencies. By this design, the task of decoding the code with distant depen- dencies can be accomplished by the cooperation of two feasible sub-decoders. To be speciﬁc, the practice behind turbo decoding is to ﬁrst decode the in- formation sequence based on the noisy receptions due to the transmission of sequences x1 and x2  in terms of the above example, 10100 and 11111 . Since the code bits generated at node x2 depend on previous information bits only through the contents of four registers, the sub-decoding procedure is feasi- ble. The decoding output sequence, however, is not the ﬁnal estimate about the information sequence 10100, but a sequence of real numbers that repre- sent the probability for each bit to be, e.g. 1, calculated based on the noisy receptions due to the transmission of x1 and x2. Continuing with the exam- ple of the simple 5-bit input sequence, the decoding output sequence might be  0.8,0.2,0.7,0.1,0.1 . Based on these numbers, we know that with 80% probability the ﬁrst bit is 1. Also, we assert with only 20% conﬁdence that the second information bit is 1. Please note that if there is no noise during the transmission, the ﬁve real numbers in the decoding output sequence should be  1.0,0.0,1.0,0.0,0.0 . It is due to the noise that the receiver can only approxi- mate the sequence that the transmitter sends. In terminology, we call these real numbers the soft decoding outputs in contrast to the conventional zero–one hard decoding outputs.  After obtaining the real-valued decoding output based on the noisy recep- tions due to the transmission of x1 and x2, one can proceed to reﬁne these numbers by performing a similar decoding procedure based on the noisy re-   7.6 Further reading  159  ceptions due to the transmission of x1 and x3 as well as the interleaved soft de- coding output from the previous step, e.g.  0.1,0.2,0.8,0.1,0.7  subject to the interleaver π s1s2s3s4s5  = s4s2s1s5s3. With the additional knowledge from the noisy receptions due to the transmission of x3, these numbers may be reﬁned to, e.g.,  0.05,0.1,0.9,0.05,0.8 ; hence, the receiver is more certain  here, 90% sure  that s1 should be 1.  By performing the decoding procedure based on the noisy receptions due to x1 and x2 as well as the de-interleaved soft decoding output  e.g.  0.9, 0.1, 0.8, 0.05, 0.05  , these numbers are reﬁned again. Then, in terms of these re-reﬁned numbers, the decoding procedure based on the noisy receptions due to x1 and x3 is re-performed.  Because the repetitive decoding procedures are similar to running two turbo pistons alternatively, it is named turbo coding. Simulations show that after 18 iterations we can make the ﬁnal hard decisions  i.e. 0 and 1 on each bit  based on the repeatedly reﬁned soft decoding outputs yielding a bit error rate almost achieving the Shannon limit.  Ever since the publication of turbo coding, iterative decoding schemes have become a new research trend, and codes similar to the rediscovered LDPC code have subsequently been proposed. In this way the Shannon limit ﬁnally has become achievable after 50 years of research efforts!  7.6 Further reading  In this chapter we introduced the Information Transmission Theorem  Theo- rem 7.1  as a summary of what we have learned in this book. In order to appre- ciate the beauty of the theorem, we then examined it under a speciﬁc case when the coded information is corrupted by additive Gaussian noise. Two scenarios followed in a straightforward manner: transmission at a rate below capacity and transmission at a rate above capacity. The former directed us to reliable trans- mission where decoding errors can be made arbitrarily small, while the latter gave the  in principle  required Eb N0 to achieve an acceptable BER. Infor- mation theorists have baptized this minimum Eb N0 the Shannon limit. Since this is the center of information theory, most advanced textbooks in this area cover the subject extensively. For readers who are interested in learning more about the theorem, [CT06] could be a good place to start. The long-standing bible-like textbook [Gal68] by Robert G. Gallager, who was also the inventor of the Shannon-limit-achieving low-density parity-check  LDPC  codes, can also serve as a good reference. Some advanced topics in information theory, such as the channel reliability function, can be found in [Bla88].   160  Approaching the Shannon limit by turbo coding  As previously mentioned, the turbo code was the ﬁrst empirically conﬁrmed near-Shannon-limit error-correcting code. It is for this reason that the turbo code was introduced brieﬂy at the end of this chapter. The two books [HW99] and [VY00] may be useful for those who are speciﬁcally interested in the prac- tice and principle of turbo codes. Due to their signiﬁcance, Shu Lin and Daniel Costello also devote one chapter for each of turbo coding and LDPC coding in their book in the new 2004 edition [LC04]. For general readers, the material in these two chapters should sufﬁce for a comprehensive understanding of the related subjects.  7.7 Appendix: Why we assume uniform and independent  data at the encoder  It is very common to assume that the input S cid:96  of a channel encoder comprise independent binary data bits of uniform distribution 1 2 .  Pr[S cid:96  = 0] = Pr[S cid:96  = 1] =   7.61   The reason for this lies in the observation that, under the assumption of in- dependence with uniform marginal distribution, no data compression can be performed further on the sequence S1,S2, . . . ,Sk since every binary combina- tion of length k has the same probability 2−k  or speciﬁcally,  1 k H Sk  = 1 bit per source symbol . Note that any source that is compressed by an opti- mal data compressor should in principle produce a source output of this kind, and we can regard this assumption as that S1,S2, . . . ,Sk are the output from an optimal data compressor.  For a better understanding of this notion, consider the following example. Assume that we wish to compress the sequence U1,U2,U3, . . . to the binary sequence S1,S2,S3, . . ., and that each source output U cid:96  is independent of all others and has the probability distribution  Pr[U cid:96  = a] =  1 2 , Pr[U cid:96  = b] = Pr[U cid:96  = c] =  1 4 .  We then use the single-letter  i.e. ν = 1  Huffman code that maps as follows:  a  cid:55 → 0,  b  cid:55 → 10,  c  cid:55 → 11.  Note that  7.63  is also a Fano code  see Deﬁnition 5.17 ; in fact, when the source probabilities are reciprocals of integer powers of 2, both the Huffman code and the Fano code are optimal compressors with average codeword length   7.62    7.63    7.7 Appendix: Uniform and independent data at the encoder  161  Lav equal to the source entropy, H U  bits. This then results in  Pr cid:104 S cid:96 +1 = 0 cid:12  cid:12  cid:12 S cid:96  = s cid:96  cid:105  Pr cid:104 U cid:96  cid:48 +1 = a cid:12  cid:12  cid:12 U cid:96  cid:48  = u cid:96  cid:48  cid:105  = Pr cid:104 U cid:96  cid:48 +1 = b cid:12  cid:12  cid:12 U cid:96  cid:48  = u cid:96  cid:48  and U cid:96  cid:48 +1  cid:54 = a cid:105   if code u cid:96  cid:48   = s cid:96 , if code u cid:96  cid:48   = s cid:96 −1 and s cid:96  = 1.   7.64   Here  cid:96  cid:48  denotes the symbol-timing at the input of the Huffman encoder  or, equivalently, the Fano encoder , and  cid:96  is the corresponding timing of the output of the Huffman encoder  equivalently, the Fano encoder .  We can now conclude by the independence of the sequence U1,U2,U3, . . .  that  Pr cid:104 S cid:96 +1 = 0 cid:12  cid:12  cid:12 S cid:96  = s cid:96  cid:105  = cid:40 Pr[U cid:96  cid:48 +1 = a] Pr[U cid:96  cid:48 +1 = b U cid:96  cid:48 +1  cid:54 = a] 1 2 .  =  if code u cid:96  cid:48   = s cid:96 , if code u cid:96  cid:48   = s cid:96 −1 and s cid:96  = 1  Since the resultant quantity 1 2 is irrespective of the s cid:96  given, we must have  Pr cid:104 S cid:96 +1 = 0 cid:12  cid:12  cid:12 S cid:96  = s cid:96  cid:105  = Pr[S cid:96 +1 = 0] =  1 2 .  Hence, S cid:96 +1 is independent of S cid:96  and is uniform in its statistics. Since this is true for every positive integer  cid:96 , S1,S2,S3, . . . is an independent sequence with uniform marginal distribution.  Sometimes, the output from an optimal compressor can only approach as- ymptotic independence with asymptotic uniform marginal distribution. This occurs when the probabilities of U are not reciprocals of powers of 2, i.e. dif- ferent from what we have assumed in the previous derivation. For example, assume  Pr[U = a] =  and Pr[U = b] = Pr[U = c] =   7.68   1 6 .  2 3  Then S1,S2,S3, . . . can only be made asymptotically independent with asymp- totic uniform marginal distribution in the sense that a multiple-letter code  i.e. a code that encodes several input letters at once; see Figure 5.12 in Section 5.5  needs to be used with the number of letters per compression growing to inﬁnity. For example, the double-letter Huffman code in Table 7.3 gives  Pr[S1 = 0] = Pr cid:2 U2 = aa cid:3  = cid:18 2 3 cid:19 2  =  4 9   7.69    7.65    7.66    7.67    162  Approaching the Shannon limit by turbo coding  Table 7.3 Double-letter and triple-letter Huffman codes for source statistics  Pr[U = a] = 2 3 and Pr[U = b] = Pr[U = c] = 1 6  Letters Code  Letters Code  Letters Code  Letters Code  aa ab ac bb ba bc ca cb cc  0 100 110 11100 1010 11101 1011 11110 11111  aaa aab aac aba abb abc aca acb acc  00 1100 0100 0101 10000 10001 0110 10010 10011  baa bab bac bba bbb bbc bca bcb bcc  0111 10100 10101 110100 1111000 1111001 110101 1111010 1111011  caa cab cac cba cbb cbc cca ccb ccc  1110 10110 10111 110110 1111100 1111101 110111 1111110 1111111  and  and   7.70    7.71    7.72    7.73    7.74   Pr[S2 = 0] = Pr cid:2  cid:0 U2 = ab or ba or ca cid:1  or U4 = aaaa cid:3   2  1 6 +  1 6 ·  2 3 +  1 6 ·  3 + cid:18 2 3 cid:19 4  =  =  2 3 · 43 81 .  These two numbers are closer to 1 2 than those from the single-letter Huffman code that maps a,b,c to 0,10,11, respectively, which gives  Pr[S1 = 0] = Pr[U1  cid:54 = a] =  1 3  Pr[S2 = 0] = Pr cid:2  U1 = b  or  U2 = aa  cid:3  =  33 54 .  Note that the approximation from the triple-letter Huffman code may be tran- siently less “accurate” to the uniform distribution than the double-letter Huff- man code  in the current example we have Pr[S1 = 0] = 16 27, which is less close to 1 2 than Pr[S1 = 0] = 4 9 from  7.69  . This complements what has been pointed out in Section 5.4.2, namely that it is rather difﬁcult to analyze  and also rather operationally intensive to examine numerically8  the average  8 As an example, when ν  the number of source letters per compression  is only moderately large, such as 20, you can try to construct a Huffman code with source alphabet of size {a,b,c}20 = 320 using Huffman’s Algorithm from Section 4.6. Check how many iterations are required to root a tree with 320 leaves.   7.7 Appendix: Uniform and independent data at the encoder  163  Table 7.4 Double-letter and triple-letter Fano codes for source statistics  Pr[U = a] = 2 3 and Pr[U = b] = Pr[U = c] = 1 6  Letters Code  aa ab ac bb ba bc ca cb cc  00 01 100 11100 101 11101 110 11110 11111  Letters Code  Letters Code  Letters Code  aaa aab aac aba abb abc aca acb acc  00 0100 0111 011 1011 110000 1000 110001 110010  baa bab bac bba bbb bbc bca bcb bcc  1001 110011 110100 111000 1111010 1111011 111001 11111000 11111001  caa cab cac cba cbb cbc cca ccb ccc  1010 110101 11011 11101 1111101 11111100 111100 11111101 11111111  codeword length of Huffman codes. However, we can anticipate that better ap- proximations to the uniform distribution can be achieved by Huffman codes if the number of letters per compression further increases.  In comparison with the Huffman code, the Fano code is easier in both analysis and implementation. As can be seen from Table 7.4 and Figure 7.6, Pr[S1 = 0] quickly converges to 1 2, and the assumption that the compressor output S1,S2,S3, . . . is independent with uniform marginal distribution can be acceptable when ν is moderately large.   164  Approaching the Shannon limit by turbo coding  Figure 7.6 Asymptotics of ν-letter Fano codes.  7.8 Appendix: Deﬁnition of concavity  Deﬁnition 7.5 A real-valued function h ·  is concave if  h cid:0 λ p1 +  1− λ  p2 cid:1  ≥ λ h p1  +  1− λ  h p2   for all real numbers p1 and p2, and all 0 ≤ λ ≤ 1. Geometrically, this means that the line segment that connects two points of the curve h ·  will always lie below the curve; see Figure 7.7 for an illustration.   7.75   Figure 7.7 Example of a concave function.  ] 0 =  1 S [ r P  0.7  0.6  0.5  0.4  0.3  0.2  0.1  0  1  2  3  4  5  6  7  8  9  10  ν  h p   h p1   h cid:0 λp1 +  1−λ p2 cid:1   λh p1  +  1−λ h p2   p1   References  165  The concavity of a function can be veriﬁed by showing that its second derivative is nonpositive. By this approach, we can prove that the binary en- tropy function is concave, as this can also be observed from Figure 5.2. By induction, a concave function satisﬁes  h cid:32 1  k  a cid:96  cid:33  ≥  k  ∑   cid:96 =1  1 k  k  ∑   cid:96 =1  h a cid:96  ;   7.76   hence,  7.50  is also conﬁrmed.  References  [BGT93] Claude Berrou, Alain Glavieux, and Punya Thitimajshima, “Near Shannon limit error-correcting coding and decoding: turbo-codes,” in Proceedings of IEEE International Conference on Communications  ICC , Geneva, Switzer- land, May 23–26, 1993, pp. 1064–1070.  [Bla88] Richard E. Blahut, Principles and Practice of Information Theory. Addi-  [CT06]  son-Wesley, Reading, MA, 1988. Thomas M. Cover and Joy A. Thomas, Elements of Information Theory, 2nd edn.  John Wiley & Sons, Hoboken, NJ, 2006.  [DM98] Matthew C. Davey and David MacKay, “Low-density parity check codes over GF q ,” IEEE Communications Letters, vol. 2, no. 6, pp. 165–167, June 1998.  [For66] G. David Forney, Jr., Concatenated Codes. MIT Press, Cambridge, MA,  [Gal62] Robert G. Gallager, Low Density Parity Check Codes. MIT Press, Cam-  1966.  bridge, MA, 1962.  [Gal68] Robert G. Gallager, Information Theory and Reliable Communication. John  Wiley & Sons, New York, 1968.  [HW99] Chris Heegard and Stephen B. Wicker, Turbo Coding. Kluwer Academic  [LC04]  Publishers, Dordrecht, 1999. Shu Lin and Daniel J. Costello, Jr., Error Control Coding, 2nd edn. Prentice Hall, Upper Saddle River, NJ, 2004.  [Sha48] Claude E. Shannon, “A mathematical theory of communication,” Bell Sys- tem Technical Journal, vol. 27, pp. 379–423 and 623–656, July and October 1948. Available: http:  moser.cm.nctu.edu.tw nctu doc shannon1948.pdf  [VY00] Branka Vucetic and Jinhong Yuan, Turbo Codes: Principles and Applica-  tions. Kluwer Academic Publishers, Dordrecht, 2000.    8  Other aspects of coding theory  We end this introduction to coding and information theory by giving two exam- ples of how coding theory relates to quite unexpected other ﬁelds. Firstly we give a very brief introduction to the relation between Hamming codes and pro- jective geometry. Secondly we show a very interesting application of coding to game theory.  8.1 Hamming code and projective geometry  Though not entirely correct, the concept of projective geometry was ﬁrst de- veloped by Gerard Desargues in the sixteenth century for art paintings and for architectural drawings. The actual development of this theory dated way back to the third century to Pappus of Alexandria. They were all puzzled by the axioms of Euclidean geometry given by Euclid in 300 BC who stated the following.   1  Given any distinct two points in space, there is a unique line connecting  these two points.   2  Given any two nonparallel1 lines in space, they intersect at a unique point.  3  Given any two distinct parallel lines in space, they never intersect.  The confusion comes from the third statement, in particular from the concept of parallelism. How can two lines never intersect? Even to the end of universe?  1 Note that in some Euclidean spaces we have three ways of how two lines can be arranged: they can intersect, they can be skew, or they can be parallel. For both skew and parallel lines, the lines never intersect, but in the latter situation we additionally have that they maintain a con- stant separation between points closest to each other on the two lines. However, the distinction between skew and parallel relies on the deﬁnition of a norm. If such a norm is not deﬁned, “distance” is not properly deﬁned either and, therefore, we cannot distinguish between skew and parallel. We then simply call both types to be “parallel.”   168  Other aspects of coding theory  Figure 8.1 Two-dimensional binary Euclidean space.  In your daily life, the two sides of a road are parallel to each other, yet you do see them intersect at a distant point. So, this is somewhat confusing and makes people very uncomfortable. Revising the above statements gives rise to the theory of projective geometry.  Deﬁnition 8.1  Axioms of projective geometry    1  Given any two distinct points in space, there is a unique line connecting   2  Given any two distinct lines in space, these two lines intersect at a unique  these two points.  point.  So, all lines intersect with each other in projective geometry. For parallel lines, they will intersect at a point at inﬁnity. Sounds quite logical, doesn’t it? Having solved our worries, let us now focus on what the projective geometry looks like. We will be particularly working over the binary space.  Consider a two-dimensional binary Euclidean space2 as shown in Figure 8.1. Do not worry about the fact that one line is curved and is not a straight line. We did this on purpose, and the reason will become clear later. Here we have four points, and, by the ﬁrst axiom in Euclidean geometry, there can be at most  2 cid:1  = 6 lines.  cid:0 4  Exercise 8.2 Ask yourself: why at most six? Can there be fewer than six lines ♦ given four points in space?  We use [XY ] to denote the four points in space. Consider, for example, the dash–single-dotted line [00][10] and the dash–double-dotted line [01][11]. The  2 Note that, as mentioned in Section 3.3.2, the Euclidean distance fails to work in the binary  Euclidean space.  [00]  [01]   8.1 Hamming code and projective geometry  169  Figure 8.2 Two-dimensional binary Euclidean space with a point at inﬁnity.  dash–single-dotted line [00][10] represents the line Y = 0 and the dash–double- dotted line [01][11] is the line of Y = 1. In Euclidean geometry, these two lines never intersect, hence it worries people. Now we introduce the concept of a “point at inﬁnity” and make these two lines intersect as shown in Figure 8.2.  To distinguish the points at inﬁnity from the original points, we add another coordinate Z in front of the coordinates [XY ]. The points in the new plots are read as [ZXY ]. The points with coordinates [1XY ] are the original points and the ones with [0XY ] are the ones at inﬁnity. But why do we label this new point at inﬁnity with coordinate [010] and not something else? This is because points lying on the same line are co-linear:  [101] + [111] = [010],   8.1   i.e. we simply add the coordinates. Note that the same holds for [100]+[110] = [010]. Having the same result for these two sums means that the lines of [100][110] and [101][111] intersect at the same point, [010].  Repeating the above process gives the geometry shown in Figure 8.3. Fi-  nally, noting that the points at inﬁnity satisfy  [001] + [011] = [010],   8.2   we see that these three newly added points are co-linear as well. So we can add another line connecting these three points and obtain the ﬁnal geometry given in Figure 8.4. This is the famous Fano plane for the two-dimensional projective binary plane. There are seven lines and seven points in this plane.  Note that the number of lines and the number of points in the projective geometry are the same, and this is no coincidence. Recall the original deﬁnition of projective geometry.  [110]  [100]  [010]  [101]   170  Other aspects of coding theory  Figure 8.3 Two-dimensional binary Euclidean space with many points at in- ﬁnity.   1  Given any two distinct points in space, a unique line lies on these two  points.   2  Given any two distinct lines in space, a unique point lies on these two lines.  For the moment, forget about the literal meanings of “lines” and “points.” Rewrite the above as follows.   1  Given any two distinct  cid:3  in space, a unique  cid:13  lies on these two  cid:3 .  2  Given any two distinct  cid:13  in space, a unique  cid:3  lies on these two  cid:13 . So you see a symmetry between these two deﬁnitions, and this means the  cid:3 s  lines  are just like the  cid:13 s  points  and vice versa. In other words, if we label the points and lines as in Figure 8.5, we immediately discover the symmetry between the two. We only rename the L by P and the P by L in Figure 8.5 a  and Figure 8.5 b . In particular, the patterns of the lines in Figure 8.5 a  are matched to the patterns of the points in Figure 8.5 b  to signify such a duality. To understand more about the symmetry, consider for example the follow-  ing.   Point P1 is intersected by lines L2, L3, and L5 in Figure 8.5 a . In Fig-  [001]  [100]  [011]  [110]  [010]  [101]   8.1 Hamming code and projective geometry  171  Figure 8.4 Final projective geometry of the two-dimensional binary Eu- clidean space with points at inﬁnity.  ure 8.5 b , we see exactly the same relation between the lines L3, L5, and L2 and the point P1.    Line L1 is related to points P2, P3, and P5 in Figure 8.5 b . In Figure 8.5 a ,  we see that L1 passes through all these three points.  Also note that, in terms of the [ZXY ] axes, the lines are deﬁned by the following functions:  Z = 0, L1 : X = 0, L2 : Y = 0, L3 : Z + X = 0, L4 : X +Y = 0, L5 : L6 : Z + X +Y = 0, Z +Y = 0. L7 :   8.3   Note that the above is quite different from what you learned in high school mathematics. For example, the function Z = 0 does not give a plane in projec- tive geometry as it does in Euclidean geometry.  [001]  [100]  [011]  [110]  [010]  [101]   172  Other aspects of coding theory   a    b   Figure 8.5 Symmetry between two deﬁnitions of projective geometry.  L4  L1  L6  P3  L2  P1  L3  L5  P5  P4  P2  P7  L7  P4  P1  P6  L3  P2  L1  P3  P5  L5  L4  L2  L7  P7   8.1 Hamming code and projective geometry  173  In general, the connection between lines in the two-dimensional Euclidean plane and the lines in the two-dimensional projective plane can be easily ob- tained through the following. For simplicity, let E2 denote the two-dimensional binary Euclidean plane, and let P2 denote the two-dimensional binary projec- tive plane. Then the connection between lines in E2 and P2 is given by L : aX + bY + c = 0 in E2 ⇐⇒ L : aX + bY + cZ = 0 in P2,   8.4   where not all a, b, and c equal zero.  Example 8.3 We now apply  8.4  in order to study a solid example so that we can understand more about the two geometries from the algebraic point of view. Consider, for example, lines L3 and L7, which represent the functions Y = 0 and Y = 1 in E2, respectively. Of course, L3 and L7 are parallel in E2 and do not intersect. On the other hand, lifting these two functions from E2 to P2  by setting  a,b,c  =  010  and  011  in  8.4   gives Y = 0 and Y + Z = 0, respectively. It then follows that these two functions do intersect in P2 at [ZXY ] = [010], a point satisfying these two equations. It justiﬁes the fact that any two distinct lines always intersect with each other. An equivalent view from algebra says that every system of linear equations is always solvable in ♦ the projective sense.  To relate the Fano plane to the Hamming code, we simply construct Ta- ble 8.1. A “1” means the point lies on the line, or, equivalently, that the line passes through the point.  Table 8.1 Relation of Fano plane to Hamming code  P1  P2  P3  P4  P5  P6  P7  L1 L2 L3 L4 L5 L6 L7  0 1 1 0 1 0 0  1 0 1 0 0 0 1  1 1 0 1 0 0 0  0 0 1 1 0 1 0  1 0 0 0 1 1 0  0 0 0 1 1 0 1  0 1 0 0 0 1 1  On reading Table 8.1 row-wise and comparing these rows with codewords  in Table 3.2, we see that   line L1 deﬁnes the codeword  0110100  associated with message  0100 ,   Other aspects of coding theory  174   line L2 deﬁnes the codeword  1010001  associated with message  0001 ,   line L3 deﬁnes the codeword  1101000  associated with message  1000 ,   line L4 deﬁnes the codeword  0011010  associated with message  1010 ,   line L5 deﬁnes the codeword  1000110  associated with message  0110 ,   line L6 deﬁnes the codeword  0001101  associated with message  1101 ,   line L7 deﬁnes the codeword  0100011  associated with message  0011 . So, the seven lines deﬁne seven codewords of a  7,4  Hamming code. What about the remaining 16− 7 = 9 codewords? Well, if we add an empty line, L0, to deﬁne the codeword  00000000 , then the remaining eight codewords are just the binary complement of these eight codewords. For example, the binary complement of  00000000  is  11111111 , and the binary complement of  0110100  deﬁned by L1 is  1001011   simply replace 0 by 1 and 1 by 0 . This way you recover all the 16 codewords of the  7,4  Hamming code.  While all the above seems tricky and was purposely done, it presents a means of generalization of the Hamming code. In particular, extending the two-dimensional Fano plane to higher-dimensional binary projective spaces, say  u− 1  dimensions,  1  we could construct the  2u − 1,2u − u− 1  Hamming codes deﬁned by the lines in the  u− 1 -dimensional binary projective space,3 and  2  we could construct other codes deﬁned by the s-dimensional subspaces  called s-ﬂats or s-hyperplanes in ﬁnite geometry  in the  u− 1 -dimen- sional binary projective space. These codes are therefore coined projective geometry codes.  Exercise 8.4 Identify all the 16 Hamming codewords on the Fano plane plus an empty line. The points associated with each codeword form either a line or a complement of a line. Can you use this geometrical fact to decode read-outs with one-bit error? We can give you some hints for this.   1  Read-outs with only one nonzero position are decoded to the empty line.  2  Read-outs with two nonzero positions are decoded to a line. Two nonzero positions mean two points in the Fano plane. The line obtained by joining these two points gives the decoded codeword. For example, if the nonzero positions are P2 and P4, then they form the line L3, and the corrected output should be the codeword associated with L3.   3  Read-outs with three nonzero positions correspond to either a line or a tri- angle in the Fano plane. If it is a line, then the line gives the decoded code- word. Otherwise, the triangle can be made into a quadrangle by adding an  3 An alternative way of getting this is given in Exercise 3.22.   8.2 Coding and game theory  175  extra point. Then note that the quadrangle is a complement of a line, which is a codeword. So the codeword associated with this quadrangle is the de- coded output. For example, assume the nonzero positions of the read-out are P1, P2, and P3, which form a triangle. To make the triangle into a quad- rangle, we should add point P6  note that adding either P4, P5, or P7 would not work: it would still be a triangle . Then the quadrangle P1P2P3P6 is the complement of the projective line L6, and hence it corresponds to a valid codeword.  Complete the decodings of read-outs with more than three nonzero positions. ♦  8.2 Coding and game theory  The Hamming code can be used to solve many problems in combinatorial de- signs as well as in game theory. One of the most famous and most interesting problems is the hat game. On April 10, 2001, the New York Times published an article entitled “Why mathematicians now care about their hat color.” The game has the following setup.   A team of n players enters a room, whereupon they each receive a hat with a color randomly selected from r equally probable possibilities. Each player can see everyone else’s hat, but not his own.  is allowed once they are inside the room.    The players must simultaneously guess the color of their own hat, or pass.   The team loses if any player guesses wrong or if all players pass.   The players can meet beforehand to devise a strategy, but no communication   The goal is to devise a strategy that gives the highest probability of winning. Example 8.5 Let n = 3 and r = 2 with the colors Red and Blue. Let us number the three players by 1, 2, and 3, and denote their hats by H1, H2, and H3, respectively. If the three players receive  H1,H2,H3  =  Red,Red,Blue  and they guess  Red,Pass,Pass , then they win the game. Otherwise, for example, if they guess  Pass,Blue,Blue , then they lose the game due to the wrong guess ♦ of the second player. They also lose for the guess of  Pass,Pass,Pass .  Random strategy What if the players guess at random? Say, guessing with probability 1  r + 1  for each color and probability 1  r + 1  for pass. With this random strategy, the probability of winning is given by  Pr Win by using “random strategy”  = cid:18  2  r + 1 cid:19 n  1  −   r + 1 n .   8.5    176  Other aspects of coding theory  So, in Example 8.5 the random strategy will yield a probability of winning  Pr Win by using “random strategy”  =   8.6   7 27  cid:39  26%,  i.e. the odds are not good.  Exercise 8.6 Prove  8.5 .  1 r ,  1 2 = 50%,  Hint: the ﬁrst term of  8.5  describes the probability of the correct color or ♦  a pass and that the second term is the probability of all passing.  One-player-guess strategy Another simple strategy is to let only one of the players, say the ﬁrst player, guess and let the others always pass. It is clear that if the ﬁrst player passes, then the team loses. So he must make a choice. In this case, the probability of winning the game is given by  Pr Win by using “one-player-guess strategy”  =   8.7   i.e. for Example 8.5  r = 2   Pr Win by using “one-player-guess strategy”  =   8.8   and the ﬁrst player simply guesses the color to be either Red or Blue, each with probability 1 2. This strategy is a lot better than the random guess strategy. Now the question is, can we do better than 1 2? Actually we can, with the help of the three-times repetition code and the Hamming code we learned in Chapter 3.  Repetition code strategy For simplicity, let us focus on the case of n = 3 and r = 2 with colors being Red  denoted as binary 0  and Blue  denoted as binary 1 . Recall that the three-times repetition code Crep has two codewords  000  and  111 . Using the repetition code, we formulate the following strategy.   For the ﬁrst player, let  ?,H2,H3  be a vector where H2,H3 ∈ {0,1} are the hat colors of the second and the third players, respectively. The question mark symbol “?” means that the color of the hat is unknown. The colors H2 and H3 are known to the ﬁrst player according to the setup. Then the ﬁrst player makes a guess using the following rule:  0 1 pass  if  1,H2,H3  is a codeword in Crep, if  0,H2,H3  is a codeword in Crep, otherwise.   8.9   ? =   177   The same applies to the second and the third players. For example, the strat-  8.2 Coding and game theory  egy of the second player is  ? =  0 1 pass  if  H1,1,H3  is a codeword in Crep, if  H1,0,H3  is a codeword in Crep, otherwise,   8.10   where H1 is the color of the ﬁrst player’s hat known to the second player.  Crep, so he passes;  If the three players receive  Example 8.7  Continuation from Example 8.5   H1,H2,H3  =  Red,Red,Blue  =  001 , then   the ﬁrst player sees  ?01 , and neither  001  nor  101  is a codeword in   the second player sees  0?1 , and neither  001  nor  011  is a codeword in   the third player sees  00? . He notices that  000  is a codeword in Crep, so ♦  Crep, so he passes, too;  he guesses 1.  Hence, the team wins.  Exercise 8.8 code strategy gives a probability of winning  Show by listing all possibilities that the three-times repetition  Pr cid:0 Win by using “Crep code strategy” cid:1  =  3 4   8.11  ♦  when n = 3 and r = 2.  It turns out that for n = 3 and r = 2, the three-times repetition code Crep is the best possible strategy for this game. Also, by carrying out Exercise 8.8 you will see that the only cases for the Crep strategy to fail are the ones when the players are given hats as  000  and  111 , which are exactly the two codewords in Crep.   7,4  Hamming code strategy The  7,4  Hamming code strategy is the best strategy when n = 7 and r = 2. But, prior to handing over the strategy, we quickly review what happened in the three-times repetition code case. In the previous example of n = 3 and r = 2, the ith player, given the observation  H1, . . . ,Hi−1,?,Hi+1, . . . ,H3 , makes the following guess:  if  H1, . . . ,Hi−1,1,Hi+1, . . . ,H3  is a codeword in Crep, if  H1, . . . ,Hi−1,0,Hi+1, . . . ,H3  is a codeword in Crep,  0 1 pass otherwise.   8.12   ? =   178  Other aspects of coding theory  So, for the case of n = 7 and r = 2, let CH be the  7,4  Hamming code with 16 codewords given in Table 3.2. Then we use the following similar strategy.   The ith player, given the observation  H1, . . . ,Hi−1,?,Hi+1, . . . ,H7 , makes  the following guess:  ? =  if  H1, . . . ,Hi−1,1,Hi+1, . . . ,H7  is a codeword in CH, if  H1, . . . ,Hi−1,0,Hi+1, . . . ,H7  is a codeword in CH,  0 1 pass otherwise.   8.13   Example 8.9 For example, the seven players are given hats according to   Blue,Red,Blue,Red,Blue,Blue,Blue  =  1010111 .   8.14   word. So he guesses 1, i.e. Blue.  Based on the strategy in  8.13  and the codewords of CH in Table 3.2, the players make the following guesses.   The ﬁrst player observes  ?010111  and notices that  0010111  is a code-   The second player observes  1?10111  and notices that neither  101011   You can check the remaining cases and show that they all pass. Since the ﬁrst player makes the right guess and the others pass, the team wins. ♦  1  nor  1110111  are codewords. So he passes.  We can show the following theorem.  Theorem 8.10 For the case of n = 7 and r = 2, the  7,4  Hamming code strategy given as in  8.13  yields the probability of winning 16 27 =  Pr Win by using “CH code strategy”  = 1−   8.15   7 8 .  Proof Note that from the sphere bound of Theorems 3.20 and 3.21, we see that the  7,4  Hamming code CH is a perfect packing of 16 spheres of radius 1 in the seven-dimensional binary space. Hence, given any combination of hats H =  H1,H2, . . . ,H7 , H must lie in one of the 16 spheres. In other words, there must exist a codeword x =  x1, . . . ,x7  of CH that is at Hamming distance at most 1 from H. We distinguish the following cases. Case I: If H ∈ CH, then according to the strategy  8.13 , the  cid:96 th player for every 1 ≤  cid:96  ≤ 7 would notice that  H1, . . . , H cid:96 −1,x cid:96 ,H cid:96 +1, . . . ,H7  is a codeword in CH, hence he will guess ¯x cid:96 , the binary complement of x cid:96 . The team always loses in this case.   8.2 Coding and game theory  179 Case II: If H  cid:54 ∈ CH, then H is at Hamming distance 1 from some codeword x. Say the difference is at the jth place, for some j, i.e. the hat color Hj of the jth player equals ¯x j.   For the  cid:96 th player,  cid:96   cid:54 = j, we see from strategy  8.13  that  H1, . . . ,H cid:96 −1, x cid:96 ,H cid:96 +1, . . . ,H7  is at Hamming distance 1 from x and  H1, . . . ,H cid:96 −1, ¯x cid:96 , H cid:96 +1, . . . ,H7  is at Hamming distance 2 from x. Both cannot be codewords because the codewords of Hamming code are separated by a distance of at least 3. Thus, the  cid:96 th player always passes in this case.   The jth player observes that  H1, . . . ,Hj−1,x j,Hj+1, . . . ,H7  = x is a codeword, hence he guesses ¯x j, which is the correct guess.  Thus, the team wins.  From the above analysis we see that the team loses if, and only if, H is a codeword in CH. Since there are 16 such possibilities, we conclude that  Pr Lose by using “CH code strategy”  =   8.16   16 27  and the theorem is proven.  The strategy we have devised above is related to the covering of error- correcting codes. The concept of covering is the opposite of that of sphere packing: the problem of covering asks what the minimum number of t is such that the radius-t spheres centered at the 2k codewords of a code ﬁll up the com- plete n-dimensional binary space. Here the spheres are allowed to overlap with each other. The three-times repetition code Crep and the Hamming code CH are both 1-covering codes because radius-1 spheres centered at their codewords completely cover the three-dimensional and seven-dimensional binary space, respectively.  In general, we can show the following theorem. Theorem 8.11 Let C be a length-n 1-covering error-correcting code with C codewords. Then for the hat game with n players and r = 2 colors, following the strategy deﬁned by C as in  8.13 , yields a winning probability of  Pr Win by using “C code strategy”  = 1− C 2n .   8.17  Exercise 8.12 Prove Theorem 8.11 by showing that  Case I  if H ∈ C , the team always loses, and  Case II  if H  cid:54 ∈ C , the team always wins even if the codewords of the 1-covering code are not separated by a distance of at least 3. ♦ Hint:  H1, . . . ,H cid:96 −1, ¯x cid:96 ,H cid:96 +1, . . . ,Hn  could be a codeword.   180  Other aspects of coding theory  Finally we remark that both Crep and CH are optimal 1-covering codes be- cause they have the smallest possible code size among all 1-covering codes of length 3 and length 7, respectively. The fact that the three-times repetition code Crep is an optimal length-3 1-covering code follows from the third case in Theorem 3.21 with u = 1.  8.3 Further reading  In this chapter we have brieﬂy discussed two different aspects of coding theory. Using the  7,4  Hamming code as a starting example, we have shown how the error-correcting codes can be used in the study of ﬁnite geometry as well as game theory. To encourage further investigations in this direction, we provide a short list of other research ﬁelds that are closely related to coding theory.  Cryptography One aim in cryptography is message encryption so that eaves- droppers cannot learn the true meaning of an encrypted message. The encryption device has a key, which is known to the sender and the re- cipient, but not to the eavesdropper. Given the key K, the encryption device encrypts plaintext S into ciphertext C. It is hoped that without the key the eavesdropper cannot easily recover the plaintext S from C. In 1949 Shannon [Sha49] ﬁrst applied information theory to the study of cryptography and deﬁned the notion of perfect secrecy. We say that the communication is perfectly secure if the mutual informa- tion between S and C is zero, i.e. I S;C  = 0. Noting that  I S;C  = H S − H SC ,   8.18   a perfectly secure communication means that the eavesdropper can never learn any information about S from the observation of C. While none of the currently used cryptosystems can offer such perfect se- crecy, in 1978 Robert J. McEliece proposed a highly secure cryptosys- tem based on the use of  n,k  binary linear error-correcting codes. McEliece’s cryptosystem with large n is immune to all known attacks, including those made by quantum computers. Readers interested in this line of research are referred to [McE78] and [Sti05] for further reading.  Design of pseudo-random sequences The pseudo-random number generator is perhaps one of the most important devices in modern comput- ing. A possible implementation of such a device is through the use   8.3 Further reading  181  of maximum-length sequences, also known as m-sequences. The m- sequence is a binary pseudo-random sequence in which the binary values 0 and 1 appear almost statistically independent, each with prob- ability 1 2. Given the initial seed, the m-sequence can be easily gen- erated by feedback shift registers. It is also one of the key components in modern cellular communication systems that are built upon code- division multiple-access  CDMA  technology. The m-sequence and the Hamming code are closely connected. In fact, the m-sequence is always a codeword in the dual of the Hamming code. Readers are re- ferred to [McE87] and [Gol82] for more details about this connection and about the design of pseudo-random sequences.  Latin square and Sudoku puzzle The Latin square is a special kind of com- binatorial object which many people have seen in some mathematical puzzles. Speciﬁcally, a Latin square is an  n× n  array in which each row and each column consist of the same set of elements without rep- etition. For example, the following is a  3× 3  Latin square.  3 1 1 2 2 3  2 3 1       8.19   The famous game of Sudoku can also be regarded as a special kind of  9× 9  Latin square. Sudoku puzzles are probably the most pop- ular among all Latin squares. Another interesting extension is called the orthogonal array, which has very useful applications in software testing. Two  n×n  Latin squares A and B are said to be orthogonal if all the n2 pairs  [A]i, j, [B]i, j  are distinct. For example, the following  4× 4  Latin squares are orthogonal to each other: 2 1 3 4  2 3 4 1 4 3 4 1 2 3 2 1  1 2 3 4  3 4 2 1  4 3 1 2  1 2 4 3   8.20   and  .  While there are many ways to construct mutually orthogonal arrays, one of the most notable constructions is from the ﬁnite projective plane we studied in Section 8.1. A famous theorem in this area states that there exists  n− 1  mutually orthogonal  n× n  Latin squares if, and only if, there exists a ﬁnite projective plane in which every projec- tive line has  n− 1  points. Again, the ﬁnite projective planes are tied           182  Other aspects of coding theory  closely to the Hamming codes. Please refer to [Bry92] and [vLW01] for a deeper discussion.  Balanced incomplete block designs The problem with block design is as fol- lows: ν players form t teams with m members in each team. Two con- ditions are required:  a  each player is in precisely µ teams, and  b  every pair of players is in precisely λ teams. Conﬁgurations meet- ing the above requirements are termed  ν,t, µ,m,λ   block designs. It should be noted that these parameters are not all independent. The main challenge is, given a set of parameters, to ﬁnd out whether the design exists, and, if the answer is yes, how to construct it. For many parameters these questions are still unanswered. The  ν,t, µ,m,λ   block designs have many applications to experimental designs, cryp- tography, and optical ﬁber communications. Moreover, block designs can be transformed into a class of error-correcting codes, termed con- stant-weight codes. Certain block-designs with λ = 1 can be obtained from ﬁnite projective planes. For more details please refer to [HP03].  References  [Bry92] Victor Bryant, Aspects of Combinatorics: A Wide-Ranging Introduction.  Cambridge University Press, Cambridge, 1992.  [Gol82] Solomon W. Golomb, Shift Register Sequences, 2nd edn.  Aegean Park  Press, Laguna Hills, CA, 1982.  [HP03] W. Cary Huffman and Vera Pless, eds., Fundamentals of Error-Correcting  Codes. Cambridge University Press, Cambridge, 2003.  [McE78] Robert J. McEliece, “A public-key cryptosystem based on algebraic coding theory,” DSN Progress Report 42-44, Technical Report, January and Febru- ary 1978.  [McE87] Robert J. McEliece, Finite Field for Scientists and Engineers, Kluwer Inter- national Series in Engineering and Computer Science. Kluwer Academic Publishers, Norwell, MA, 1987.  [Sha49] Claude E. Shannon, “Communication theory of secrecy systems,” Bell Sys-  tem Technical Journal, vol. 28, no. 4, pp. 656–715, October 1949.  [Sti05] Douglas R. Stinson, Cryptography: Theory and Practice, 3rd edn. Chapman  & Hall CRC Press, Boca Raton, FL, 2005.  [vLW01] Jacobus H. van Lint and Richard M. Wilson, A Course in Combinatorics,  2nd edn. Cambridge University Press, Cambridge, 2001.   References  [BGT93] Claude Berrou, Alain Glavieux, and Punya Thitimajshima, “Near Shannon limit error-correcting coding and decoding: turbo-codes,” in Proceedings of IEEE International Conference on Communications  ICC , Geneva, Switzer- land, May 23–26, 1993, pp. 1064–1070.  [Bla88] Richard E. Blahut, Principles and Practice of Information Theory. Addi-  son-Wesley, Reading, MA, 1988.  [Bry92] Victor Bryant, Aspects of Combinatorics: A Wide-Ranging Introduction.  Cambridge University Press, Cambridge, 1992.  [BT02] Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability.  [CA05]  [CS99]  [CT06]  Athena Scientiﬁc, Belmont, MA, 2002. Po-Ning Chen and Fady Alajaji, Lecture Notes on Information Theory, vol. 1, Department of Electrical Engineering, National Chiao Tung Univer- sity, Hsinchu, Taiwan, and Department of Mathematics & Statistics, Queen’s University, Kingston, Canada, August 2005. Available: http:  shannon.cm.n ctu.edu.tw it itvol12004.pdf John Conway and Neil J. A. Sloane, Sphere Packings, Lattices and Groups, 3rd edn. Springer Verlag, New York, 1999. Thomas M. Cover and Joy A. Thomas, Elements of Information Theory, 2nd edn.  John Wiley & Sons, Hoboken, NJ, 2006.  [DM98] Matthew C. Davey and David MacKay, “Low-density parity check codes over GF q ,” IEEE Communications Letters, vol. 2, no. 6, pp. 165–167, June 1998.  [Fan49] Robert M. Fano, “The transmission of information,” Research Laboratory of Electronics, Massachusetts Institute of Technology  MIT , Technical Report No. 65, March 17, 1949.  [For66] G. David Forney, Jr., Concatenated Codes. MIT Press, Cambridge, MA,  1966.  bridge, MA, 1962.  [Gal62] Robert G. Gallager, Low Density Parity Check Codes. MIT Press, Cam-  [Gal68] Robert G. Gallager, Information Theory and Reliable Communication. John  Wiley & Sons, New York, 1968.   184  References  [Gal01] Robert G. Gallager, “Claude E. Shannon: a retrospective on his life, work, and impact,” IEEE Transactions on Information Theory, vol. 47, no. 7, pp. 2681–2695, November 2001.  [Gol49] Marcel J. E. Golay, “Notes on digital coding,” Proceedings of the IRE,  [Gol82] Solomon W. Golomb, Shift Register Sequences, 2nd edn.  Aegean Park  vol. 37, p. 657, June 1949.  Press, Laguna Hills, CA, 1982.  [Har28] Ralph Hartley, “Transmission of information,” Bell System Technical Jour-  nal, vol. 7, no. 3, pp. 535–563, July 1928.  [HP98] W. Cary Huffman and Vera Pless, eds., Handbook of Coding Theory. North-  Holland, Amsterdam, 1998.  [HP03] W. Cary Huffman and Vera Pless, eds., Fundamentals of Error-Correcting  Codes. Cambridge University Press, Cambridge, 2003.  [HW99] Chris Heegard and Stephen B. Wicker, Turbo Coding. Kluwer Academic  Publishers, Dordrecht, 1999.  [Kar39] William Karush, “Minima of functions of several variables with inequalities as side constraints,” Master’s thesis, Department of Mathematics, University of Chicago, Chicago, IL, 1939.  [Khi56] Aleksandr Y. Khinchin, “On the fundamental theorems of information the- ory,”  in Russian , Uspekhi Matematicheskikh Nauk XI, vol. 1, pp. 17–75, 1956.  [Khi57] Aleksandr Y. Khinchin, Mathematical Foundations of Information Theory.  Dover Publications, New York, 1957.  [KT51] Harold W. Kuhn and Albert W. Tucker, “Nonlinear programming,” in Pro- ceedings of Second Berkeley Symposium on Mathematical Statistics and Probability, J. Neyman, ed. University of California Press, Berkeley, CA, 1951, pp. 481–492. Shu Lin and Daniel J. Costello, Jr., Error Control Coding, 2nd edn. Prentice Hall, Upper Saddle River, NJ, 2004. James L. Massey, Applied Digital Information Theory I and II, Lecture notes, Signal and Information Processing Laboratory, ETH Zurich, 1995 1996. Available: http:  www.isiweb.ee.ethz.ch archive massey scr   [Mas96]  [LC04]  [McE78] Robert J. McEliece, “A public-key cryptosystem based on algebraic coding theory,” DSN Progress Report 42-44, Technical Report, January and Febru- ary 1978.  [McE85] Robert J. McEliece, “The reliability of computer memories,” Scientiﬁc Amer-  ican, vol. 252, no. 1, pp. 68–73, 1985.  [McE87] Robert J. McEliece, Finite Field for Scientists and Engineers, Kluwer Inter- national Series in Engineering and Computer Science. Kluwer Academic Publishers, Norwell, MA, 1987.  [MS77] F. Jessy MacWilliams and Neil J. A. Sloane, The Theory of Error-Correcting  Codes. North-Holland, Amsterdam, 1977.  [Nor89] Arthur L. Norberg, “An interview with Robert M. Fano,” Charles Babbage  Institute, Center for the History of Information Processing, April 1989.  [Pin54] Mark S. Pinsker, “Mutual information between a pair of stationary Gaus- sian random processes,”  in Russian , Doklady Akademii Nauk SSSR, vol. 99, no. 2, pp. 213–216, 1954, also known as “The quantity of information about   References  185  a Gaussian random stationary process, contained in a second process con- nected with it in a stationary manner.”  [Ple68] Vera Pless, “On the uniqueness of the Golay codes,” Journal on Combination  Theory, vol. 5, pp. 215–228, 1968.  [RG04] Mohammad Rezaeian and Alex Grant, “Computation of total capacity for discrete memoryless multiple-access channels,” IEEE Transactions on In- formation Theory, vol. 50, no. 11, pp. 2779–2784, November 2004. Jossy Sayir, “On coding by probability transformation,” Ph.D. dissertation, ETH Zurich, 1999, Diss. ETH No. 13099. Available: http:  e-collection.ethb ib.ethz.ch view eth:23000  [Say99]  [Sha37] Claude E. Shannon, “A symbolic analysis of relay and switching circuits,” Master’s thesis, Massachusetts Institute of Technology  MIT , August 1937. [Sha48] Claude E. Shannon, “A mathematical theory of communication,” Bell Sys- tem Technical Journal, vol. 27, pp. 379–423 and 623–656, July and October 1948. Available: http:  moser.cm.nctu.edu.tw nctu doc shannon1948.pdf  [Sha49] Claude E. Shannon, “Communication theory of secrecy systems,” Bell Sys-  tem Technical Journal, vol. 28, no. 4, pp. 656–715, October 1949.  [Sha56] Claude E. Shannon, “The zero error capacity of a noisy channel,” IRE Trans-  [Sti91]  [Sti05]  actions on Information Theory, vol. 2, no. 3, pp. 8–19, September 1956. Gary Stix, “Proﬁle: Information theorist David A. Huffman,” Scientiﬁc American  Special Issue on Communications, Computers, and Networks , vol. 265, no. 3, September 1991. Douglas R. Stinson, Cryptography: Theory and Practice, 3rd edn. Chapman & Hall CRC Press, Boca Raton, FL, 2005.  [Tie73] Aimo Tiet¨av¨ainen, “On the nonexistence of perfect codes over ﬁnite ﬁelds,” SIAM Journal on Applied Mathematics, vol. 24, no. 1, pp. 88–96, January 1973.  [Tun67] Brian P. Tunstall, “Synthesis of noiseless compression codes,” Ph.D. disser-  tation, Georgia Institute of Technology, September 1967.  [vLW01] Jacobus H. van Lint and Richard M. Wilson, A Course in Combinatorics,  2nd edn. Cambridge University Press, Cambridge, 2001.  [VY00] Branka Vucetic and Jinhong Yuan, Turbo Codes: Principles and Applica-  tions. Kluwer Academic Publishers, Dordrecht, 2000.  [WD52] Philip M. Woodward and Ian L. Davies, “Information theory and inverse probability in telecommunication,” Proceedings of the IEE, vol. 99, no. 58, pp. 37–44, March 1952.  [Wic94] Stephen B. Wicker, Error Control Systems for Digital Communication and  Storage. Prentice Hall, Englewood Cliffs, NJ, 1994.    Index  Italic entries are to names.  ∀, 7  cid:100 ξ cid:101 , 99  cid:98 ξ cid:99 , 16, 49 ? =, 34 xT, 39  a posteriori probability, 126 a priori probability, 126 additive white Gaussian noise channel, see  AWGN channel  Alajaji, Fady, 141 arithmetic coding, 109 arithmetics in the binary ﬁeld, see modular  arithmetic ASCII code, 19 average codeword length, 57, 65, 96 AWGN channel, 11, 145, 154  Bayes’ Theorem, 119 BEC, 135 BER, see bit error rate Berrou, Claude, 153, 155 Bertsekas, Dimitri P., 119, 123 binary digit, 87 binary tree, see tree bit, 5, 87 bit error rate, 148, 150, 152 Blahut, Richard E., 159 block design, 182 block error rate, 148 bound  on average codeword length, 96, 101 on BER, 153 on entropy, 90 Gilbert–Varshamov, 53  Hamming, 51 Plotkin, 53 sphere, 51, 53 TVZ, 53  Bryant, Victor, 182 BSC, 120 byte, 9, 19, 41, 87  CD, 3, 31, 41 CDMA, 181 channel, 2, 117  AWGN, 11, 145, 154 binary, 119 binary erasure, 135 binary symmetric, 119, 131 channel transition matrix, 117 Gaussian, 145 quasi-symmetric, 141 stationary, 118 uniformly dispersive, 134  channel capacity, 131, 139, 143 of binary erasure channel, 136 of binary symmetric channel, 132 of Gaussian channel, 146 KKT conditions, 136 power constraint, 146 of uniformly dispersive channel, 135 channel coding, 2, 31, 138, 143, see also  under code  Channel Coding Theorem, 139 Chen, Po-Ning, 141 code, 5, 55, 139  ASCII, 19 average codeword length, 57, 65, 96 block, 78, 110 concatenated, 153   188  Index  constant-weight, 182 cyclic, 30 dual, 39, 40 Elias–Willems, 110 error-correcting, 37  single-error-correcting, 36, 46 t-error-correcting, 37 error-detecting, 13, 37  double-error-detecting, 29, 46 e-error-detecting, 37  Fano, 97, 108, 160 generator matrix, 38 Hamming, 42 general, 52 strategy, 177 Venn diagram, 43  hexadecimal, 10 Huffman, 70, 160 instantaneous, 57 ISBN, 26 LDPC, 153 Lempel–Ziv, 110 linear, 39 octal, 9 parity-check matrix, 38 perfect, 51 preﬁx-free, 57, 74 rate, 37, 139, 144 Reed–Solomon, 41 repetition, 34 Shannon, 108 Shannon–Fano, see code, Fano single parity-check, 17, 40 singular, 73 Tunstall, 78, 108, 110 turbo, 11, 155 decoder, 158 encoder, 155 interleaver, 156  uniquely decodable, 56, 73 universal product, 26 variable-length, 55 weighted, 22 codebook, 139 codeword, 7, 17, 39, 55, 139 coding scheme, 138 Coding Theorem  Information Transmission Theorem, 143 Joint Source–Channel Coding Theorem,  143  for a Single Random Message, 101  coding theory, 1 compression, see data compression compressor, 147 concatenation of codewords, 75, 104 concavity, 164 Conway, John, 53 Cooley, James, 41 Costello, Jr., Daniel J., 30, 52, 160 Cover, Thomas M., 109, 110, 138, 141, 147,  159  covering, 179 cryptography, 180  data compression, 55, 160  universal, 108  Davey, Matthew C., 154 Davies, Ian L., 141 decoder, 2, 116, 145, 147 of Hamming code, 42 of turbo code, 158  depth of leaf, 61 Desargues, Gerard, 167 discrete memoryless source, see DMS DMS, 104, 107 dual code, 39 DVD, 3, 31, 41  Elias, Peter, 110 encoder, 2, 106, 116, 145, 147  of Hamming code, 42 of turbo code, 155  entropy, 87  binary entropy function, 88, 132, 152 branching, 92 chain rule, 123, 124, 151 conditional, 124 conditioning reduces entropy, 129, 152 joint, 105, 123, 130 leaf, 92 properties, 90 source, 107, 143 uniqueness, 111 unit, 87  achievability part, 97 Channel Coding Theorem, 139 converse part, 95 for a Discrete Memoryless Source, 107  equivocation, 125, 130 error  arbitrarily small, 138, 139, 143 bit, 148   block, 148 burst, 21 detectable, 18, 27 independent, 15 uncorrectable, 37 undetectable, 18, 38  error pattern, 21, 45 error probability, 139 error-correcting code, see code,  error-correcting  error-detecting code, see code, error-detecting Euclid, 167 Euclidean geometry, 167 extended root, 62 extending a leaf, 61  Fano, Robert M., 78, 108 Fano code, 97, 108, 160 Fano plane, 169 Forney, Jr., G. David, 153  Gallager, Robert G., 110, 138, 141, 153, 159 game theory, 175  Hamming code strategy, 177 one-player-guess strategy, 176 random strategy, 175 repetition code strategy, 176  Gauss, Carl F., 41, 146 Gaussian channel, 145 Gaussian distribution, 146 generator matrix, 38 geometry  Euclidean, 167 Fano plane, 169 projective, 167–169  Gilbert–Varshamov bound, 53 Glavieux, Alain, 153, 155 Golay, Marcel J. E., 51 Golomb, Solomon W., 181 Grant, Alex, 141  Hamming, Richard W., 32, 40, 47 Hamming bound, 51 Hamming code, 42  general, 52 strategy, 177 Venn diagram, 43  Hamming distance, 47 Hamming weight, 47 hard decoding, 158  Index  189  Hartley, 87 Hartley, Ralph, 83 hat game, 175 Heegard, Chris, 160 Høholdt, Tom, 41 Huffman, David A., 67, 78 Huffman, W. Cary, 53, 182 Huffman code, 70, 160  IEEE, 78 independent random variables, 15, 106, 123,  125, 128, 145, 160  information, 88, see also mutual information information theory, 1 Information Theory Inequality, see IT  Inequality  Information Transmission Theorem, 143 instantaneous code, 57 ISBN, 26 IT Inequality, 89  Joint Source–Channel Coding Theorem, 143  Karush, William, 137 Karush–Kuhn–Tucker conditions, see KKT  conditions  Khinchin, Aleksandr Y., 111, 112 KKT conditions, 136 Kraft Inequality, 63 Kuhn, Harold W., 137 Kuhn–Tucker conditions, see KKT conditions  Latin square, 181 LDPC code, 153 Leaf Entropy Theorem, 93 Leaf-Counting Lemma, 61 Leaf-Depth Lemma, 61 Lin, Shu, 30, 52, 160 linear code, 39 LSB, 8  m-sequences, 181 McEliece, Robert J., 43, 180, 181 MacKay, David, 154 McMillan’s Theorem, 74 MacWilliams, F. Jessy, 30, 52, 53 Massey, James L., 58, 73, 78, 89, 108, 110, 141 MIT, 78 modular arithmetic, 13, 33 modulation, 3   190  Index  modulo-2, see modular arithmetic mutual information, 127 average, 127, 130, 131 properties, 128 system, 127 Venn diagram, 130  nat, 87 Norberg, Arthur L., 78 normal distribution, 146  packing, 48, 51, 53 Pappus of Alexandria, 167 parity check  ASCII code, 19 LDPC code, 153 matrix, 38  of Hamming code, 42  over words, 21 single parity-check code, 17, 40 universal product code, 26  Path Length Lemma, 65 perfect code, 51 perfect secrecy, 180 Pinsker, Mark S., 141 Pless, Vera, 51, 53, 182 Plotkin bound, 53 power constraint, 146 preﬁx-free code, 57, 74 probability  rooted tree, see tree  Sayir, Jossy, 110 Shannon, Claude E., 1, 11, 78, 84, 85, 108,  109, 111, 138, 140, 141, 143, 180  Shannon code, 108 Shannon limit, 153, 155, 159 Shannon–Fano code, see Fano code sign bit, 5 singular code, 73 Sloane, Neil J. A., 30, 52, 53 soft decoding, 158 Solomon, Gustave, 41 source coding, 2, 55, 143, see also under code Source Coding Theorem, 107 source entropy, 107, 143 sphere bound, 51 sphere packing, 48, 51, 53 stationarity, 118 statistical independence, 15, 106, 123, 125,  128, 145, 160  Stinson, Douglas R., 180 Stix, Gary, 78 strategy  Hamming code, 177 one-player-guess, 176 random, 175 repetition code, 176  Sudoku, 181  a posteriori, 126 a priori, 126 backward conditional, 119 conditional, 118, 119 conditional probability distribution, 117 forward conditional, 119 joint, 118  probability density function, 146  Gaussian, 146  progressive digiting, 24 projective geometry, 167–169 pseudo-random sequences, 180  rate, 37, 139, 144 rate distortion theory, 141 receiver, see decoder redundancy, 19, 27 Reed, Irving S., 41 reliable communication, 131, 139 Rezaeian, Mohammad, 141  Thitimajshima, Punya, 153, 155 Thomas, Joy A., 109, 110, 138, 141, 147, 159 Tiet¨av¨ainen, Aimo, 51 transition matrix, 117 transmitter, see encoder tree, 58, 78  Tsfasman–Vl˘adut¸–Zink bound, see TVZ  depth of leaf, 61 extended root, 62 extending a leaf, 61 with probabilities, 65, 92 unused leaf, 59, 63, 68  bound  Tsitsiklis, John N., 119, 123 Tucker, Albert W., 137 Tukey, John, 41 Tunstall, Brian P., 78, 108 turbo code, 155 decoder, 158 encoder, 155 interleaver, 156   Index  191  TVZ bound, 53  uncertainty, see entropy uniquely decodable code, 56, 73 unused leaf, 59, 63, 68 UPC, 26  van Lint, Jacobus H., 41, 51, 52, 182 vector message, 105 Venn diagram  Hamming code, 43 mutual information, 130  Vucetic, Branka, 160  white noise, 15 Gaussian, 145  Wicker, Stephen B., 30, 52, 53, 160 Wilson, Richard M., 52, 182 Woodward, Philip M., 141  Yuan, Jinhong, 160

@highlight

This easy-to-read guide provides a concise introduction to the engineering background of modern communication systems, from mobile phones to data compression and storage. Background mathematics and specific engineering techniques are kept to a minimum so that only a basic knowledge of high-school mathematics is needed to understand the material covered. The authors begin with many practical applications in coding, including the repetition code, the Hamming code and the Huffman code. They then explain the corresponding information theory, from entropy and mutual information to channel capacity and the information transmission theorem. Finally, they provide insights into the connections between coding theory and other fields. Many worked examples are given throughout the book, using practical applications to illustrate theoretical definitions. Exercises are also included, enabling readers to double-check what they have learned and gain glimpses into more advanced topics, making this perfect for anyone who needs a quick introduction to the subject, Introduction -- Error-detecting codes -- Repetition and hamming codes -- Data compression: efficient coding of a random message -- Entropy and Shannon's, source coding theorem -- Mutual information and channel capacity -- Approaching the Shannon limit by turbo coding -- Other aspects of coding theory