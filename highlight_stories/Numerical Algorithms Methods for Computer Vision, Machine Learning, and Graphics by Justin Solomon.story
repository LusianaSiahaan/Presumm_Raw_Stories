COMPUTER SCIENCE  Numerical Algorithms  Methods for Computer Vision, Machine Learning, and Graphics  “This book covers an impressive array of topics, many of which are paired with a real-world  application. Its conversational style and relatively few theorem-proofs make it well suited for  computer science students as well as professionals looking for a refresher.”  —Dianne Hansford, FarinHansford.com  Numerical Algorithms: Methods for Computer Vision, Machine Learning, and Graphics  presents a new approach to numerical analysis for modern computer scientists. Using ex- amples from a broad base of computational tasks, including data processing, computation- al photography, and animation, the book introduces numerical modeling and algorithmic  design from a practical standpoint and provides insight into the theoretical tools needed to  support these skills. The book covers a wide range of topics—from numerical linear algebra to optimization and  differential equations—focusing on real-world motivation and unifying themes. It incorpo- rates cases from computer science research and practice, accompanied by highlights from  in-depth  literature  on  each  subtopic.  Comprehensive  end-of-chapter  exercises  encourage  critical thinking and build your intuition while introducing extensions of the basic material. Features     Introduces themes common to nearly all classes of numerical algorithms    Covers algorithms for solving linear and nonlinear problems, including popular tech-  niques recently introduced in the research community     Includes comprehensive end-of-chapter exercises that push you to derive, extend, and   analyze numerical algorithms     Access online or download to your smartphone, tablet or PC Mac    Search the full text of this and other titles you own    Make and share notes and highlights    Copy and paste text and figures for use in your own documents    Customize your view by changing font size and layout  an informa business w w w . c r c p r e s s . c o m  6000 Broken Sound Parkway, NW  Suite 300, Boca Raton, FL 33487 711 Third Avenue  New York, NY 10017 2 Park Square, Milton Park  Abingdon, Oxon OX14 4RN, UK  K23847  ISBN: 978-1-4822-5188-3 90000  9 781482 251883 w w w . c r c p r e s s . c o m  WITH VITALSOURCE®  EBOOK  N u m e r i c a l    A l g o r i t h m  s  Solomon  Numerical Algorithms  Methods for Computer Vision, Machine Learning, and Graphics        Justin Solomon  A N   A   K   P E T E R S   B O O K   Accessing the E-book edition    Using the VitalSource® ebook Access to the VitalBookTM ebook accompanying this book is  via VitalSource® Bookshelf – an ebook reader which allows  you to make and share notes and highlights on your ebooks  and search across all of the ebooks that you hold on your  VitalSource Bookshelf. You can access the ebook online or  offline on your smartphone, tablet or PC Mac and your notes  and highlights will automatically stay in sync no matter where  you make them.  1.  Create a VitalSource Bookshelf account at   https:  online.vitalsource.com user new or log into  your existing account if you already have one.  2.  Redeem the code provided in the panel below   to get online access to the ebook. Log in to  Bookshelf and click the Account menu at the top right  of the screen. Select Redeem and enter the redemption  code shown on the scratch-off panel below in the Code  To Redeem box. Press Redeem. Once the code has  been redeemed your ebook will download and appear in  your library.  DOWNLOAD AND READ OFFLINE  To use your ebook offline, download BookShelf to your PC,  Mac, iOS device, Android device or Kindle Fire, and log in to  your Bookshelf account to access your ebook:  On your PC Mac Go to http:  bookshelf.vitalsource.com  and follow the  instructions to download the free VitalSource Bookshelf   app to your PC or Mac and log into your Bookshelf account.  On your iPhone iPod Touch iPad  Download the free VitalSource Bookshelf App available  via the iTunes App Store and log into your Bookshelf  account. You can find more information at https:  support. vitalsource.com hc en-us categories 200134217- Bookshelf-for-iOS  On your Android™ smartphone or tablet Download the free VitalSource Bookshelf App available  via Google Play and log into your Bookshelf account. You can  find more information at https:  support.vitalsource.com  hc en-us categories 200139976-Bookshelf-for-Android- and-Kindle-Fire  On your Kindle Fire Download the free VitalSource Bookshelf App available  from Amazon and log into your Bookshelf account. You can  find more information at https:  support.vitalsource.com  hc en-us categories 200139976-Bookshelf-for-Android- and-Kindle-Fire  N.B. The code in the scratch-off panel can only be used once.  When you have created a Bookshelf account and redeemed  the code you will be able to access the ebook online or offline  on your smartphone, tablet or PC Mac.  SUPPORT If you have any questions about downloading Bookshelf,  creating your account, or accessing and using your ebook  edition, please visit http:  support.vitalsource.com    Numerical Algorithms    Numerical Algorithms  Methods for Computer Vision, Machine Learning, and Graphics  Justin Solomon  Boca Raton  London  New York  CRC Press is an imprint of the Taylor & Francis Group, an informa business A N   A   K   P E T E R S   B O O K   CRC Press Taylor & Francis Group 6000 Broken Sound Parkway NW, Suite 300 Boca Raton, FL 33487-2742   2015 by Taylor & Francis Group, LLC CRC Press is an imprint of Taylor & Francis Group, an Informa business No claim to original U.S. Government works Version Date: 20150105 International Standard Book Number-13: 978-1-4822-5189-0  eBook - PDF  This book contains information obtained from authentic and highly regarded sources. Reasonable efforts have been  made to publish reliable data and information, but the author and publisher cannot assume responsibility for the valid- ity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright  holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this  form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may  rectify in any future reprint. Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, transmitted, or uti- lized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopy- ing, microfilming, and recording, or in any information storage or retrieval system, without written permission from the  publishers. For permission to photocopy or use material electronically from this work, please access www.copyright.com  http:   www.copyright.com   or contact the Copyright Clearance Center, Inc.  CCC , 222 Rosewood Drive, Danvers, MA 01923,  978-750-8400. CCC is a not-for-profit organization that provides licenses and registration for a variety of users. For  organizations that have been granted a photocopy license by the CCC, a separate system of payment has been arranged. Trademark Notice: Product or corporate names may be trademarks or registered trademarks, and are used only for  identification and explanation without intent to infringe. Visit the Taylor & Francis Web site at http:  www.taylorandfrancis.com and the CRC Press Web site at http:  www.crcpress.com   In memory of Cliﬀord Nass   1958–2013     Contents  PREFACE ACKNOWLEDGMENTS Section I Preliminaries Chapter 1  cid:4  Mathematics Review  1.3  1.5  2.1  PRELIMINARIES: NUMBERS AND SETS  1.1 1.2 VECTOR SPACES  Span, Linear Independence, and Bases  1.2.1 Deﬁning Vector Spaces 1.2.2 1.2.3 Our Focus: Rn LINEARITY 1.3.1 Matrices 1.3.2 1.3.3 Matrix Storage and Multiplication Methods 1.3.4 Model Problem: A cid:126 x =  cid:126 b  Scalars, Vectors, and Matrices  1.4 NON-LINEARITY: DIFFERENTIAL CALCULUS  1.4.1 Diﬀerentiation in One Variable 1.4.2 Diﬀerentiation in Multiple Variables 1.4.3 Optimization EXERCISES  Chapter 2  cid:4  Numerics and Error Analysis  STORING NUMBERS WITH FRACTIONAL PARTS 2.1.1 Fixed-Point Representations 2.1.2 Floating-Point Representations 2.1.3 More Exotic Options  2.2 UNDERSTANDING ERROR 2.2.1 Classifying Error 2.2.2 Conditioning, Stability, and Accuracy PRACTICAL ASPECTS 2.3.1 Computing Vector Norms  2.3  xv xix  3 3 4 4 5 7 9 10 12 13 14 15 16 17 20 23 27 27 28 29 31 32 33 35 36 37  vii   viii  cid:4  Contents  2.4  2.3.2 Larger-Scale Example: Summation EXERCISES  Section II Linear Algebra Chapter 3  cid:4  Linear Systems and the LU Decomposition  SOLVABILITY OF LINEAR SYSTEMS AD-HOC SOLUTION STRATEGIES ENCODING ROW OPERATIONS 3.3.1 Permutation 3.3.2 Row Scaling 3.3.3 Elimination  3.4 GAUSSIAN ELIMINATION  3.4.1 Forward-Substitution 3.4.2 Back-Substitution 3.4.3 Analysis of Gaussian Elimination LU FACTORIZATION 3.5.1 Constructing the Factorization 3.5.2 Using the Factorization 3.5.3 EXERCISES  Implementing LU  Chapter 4  cid:4  Designing and Analyzing Linear Systems  Image Alignment  SOLUTION OF SQUARE SYSTEMS 4.1.1 Regression 4.1.2 Least-Squares 4.1.3 Tikhonov Regularization 4.1.4 4.1.5 Deconvolution 4.1.6 Harmonic Parameterization SPECIAL PROPERTIES OF LINEAR SYSTEMS 4.2.1 Positive Deﬁnite Matrices and the Cholesky Factorization 4.2.2 4.2.3 Additional Special Structures SENSITIVITY ANALYSIS 4.3.1 Matrix and Vector Norms 4.3.2 Condition Numbers EXERCISES  Sparsity  3.1 3.2 3.3  3.5  3.6  4.1  4.2  4.3  4.4  38 39  47 47 49 51 51 52 52 54 55 56 56 58 59 60 61 61 65 65 66 68 70 71 73 74 75 75 79 80 81 81 84 86   Chapter 5  cid:4  Column Spaces and QR  THE STRUCTURE OF THE NORMAL EQUATIONS STRATEGY FOR NON-ORTHOGONAL MATRICES  5.1 5.2 ORTHOGONALITY 5.3 5.4 GRAM-SCHMIDT ORTHOGONALIZATION  5.4.1 Projections 5.4.2 Gram-Schmidt Algorithm REDUCED QR FACTORIZATION EXERCISES  5.5 HOUSEHOLDER TRANSFORMATIONS 5.6 5.7  Chapter 6  cid:4  Eigenvectors  6.1 MOTIVATION  Statistics  Spectral Embedding  6.1.1 6.1.2 Diﬀerential Equations 6.1.3 PROPERTIES OF EIGENVECTORS 6.2.1 6.2.2  Symmetric and Positive Deﬁnite Matrices Specialized Properties 6.2.2.1 Characteristic Polynomial 6.2.2.2  Jordan Normal Form 6.3 COMPUTING A SINGLE EIGENVALUE  6.3.1 Power Iteration 6.3.2 Inverse Iteration 6.3.3 Shifting FINDING MULTIPLE EIGENVALUES 6.4.1 Deﬂation 6.4.2 QR Iteration 6.4.3 Krylov Subspace Methods SENSITIVITY AND CONDITIONING EXERCISES  6.2  6.4  6.5 6.6  Chapter 7  cid:4  Singular Value Decomposition  7.1 DERIVING THE SVD 7.2  7.1.1 Computing the SVD APPLICATIONS OF THE SVD 7.2.1  Solving Linear Systems and the Pseudoinverse  Contents  cid:4  ix 91 91 92 93 94 94 96 99 103 103 107 107 108 109 110 112 114 116 116 116 117 117 118 119 120 120 121 126 126 127 131 131 133 134 134   x  cid:4  Contents  7.2.2 Decomposition into Outer Products and Low-Rank Approxi-  mations  7.2.3 Matrix Norms 7.2.4 The Procrustes Problem and Point Cloud Alignment 7.2.5 Principal Component Analysis  PCA  7.2.6 Eigenfaces EXERCISES  7.3  Section III Nonlinear Techniques Chapter 8  cid:4  Nonlinear Systems  8.1  ROOT-FINDING IN A SINGLE VARIABLE 8.1.1 Characterizing Problems 8.1.2 Continuity and Bisection 8.1.3 Fixed Point Iteration 8.1.4 Newton’s Method 8.1.5 8.1.6 Hybrid Techniques 8.1.7 8.2 MULTIVARIABLE PROBLEMS  Single-Variable Case: Summary  Secant Method  8.2.1 Newton’s Method 8.2.2 Making Newton Faster: Quasi-Newton and Broyden EXERCISES  8.3 CONDITIONING 8.4  Chapter 9  cid:4  Unconstrained Optimization  9.1 UNCONSTRAINED OPTIMIZATION: MOTIVATION 9.2 OPTIMALITY  9.2.1 Diﬀerential Optimality 9.2.2 Alternative Conditions for Optimality  9.3 ONE-DIMENSIONAL STRATEGIES  9.3.1 Newton’s Method 9.3.2 Golden Section Search 9.4 MULTIVARIABLE STRATEGIES  9.4.1 Gradient Descent 9.4.2 Newton’s Method in Multiple Variables 9.4.3 Optimization without Hessians: BFGS EXERCISES APPENDIX: DERIVATION OF BFGS UPDATE  9.5 9.6  135 136 137 139 140 141  147 147 147 148 149 151 153 155 155 156 156 156 158 158 163 163 165 166 168 169 170 170 173 173 174 175 178 182   Chapter 10  cid:4  Constrained Optimization  10.1 MOTIVATION 10.2  THEORY OF CONSTRAINED OPTIMIZATION 10.2.1 Optimality 10.2.2 KKT Conditions  10.3 OPTIMIZATION ALGORITHMS  10.3.1  Sequential Quadratic Programming  SQP   10.3.1.1 Equality Constraints 10.3.1.2  Inequality Constraints  10.3.2 Barrier Methods 10.4 CONVEX PROGRAMMING 10.4.1 Linear Programming 10.4.2 10.4.3 10.4.4 EXERCISES  10.5  Second-Order Cone Programming Semideﬁnite Programming Integer Programs and Relaxations  Chapter 11  cid:4  Iterative Linear Solvers  11.1 GRADIENT DESCENT  11.1.1 Gradient Descent for Linear Systems 11.1.2 Convergence  11.2 CONJUGATE GRADIENTS  11.2.1 Motivation Suboptimality of Gradient Descent 11.2.2 11.2.3 Generating A-Conjugate Directions 11.2.4 Formulating the Conjugate Gradients Algorithm 11.2.5 Convergence and Stopping Conditions PRECONDITIONING 11.3.1 CG with Preconditioning 11.3.2 Common Preconditioners 11.4 OTHER ITERATIVE ALGORITHMS EXERCISES 11.5  11.3  Chapter 12  cid:4  Specialized Optimization Methods  12.1 NONLINEAR LEAST-SQUARES  12.1.1 Gauss-Newton 12.1.2 Levenberg-Marquardt ITERATIVELY REWEIGHTED LEAST-SQUARES  12.2  Contents  cid:4  xi 185 186 189 189 189 192 193 193 193 194 194 196 197 199 200 201 207 208 208 209 211 212 214 215 217 219 219 220 221 222 223 227 227 228 229 230   xii  cid:4  Contents  12.3 COORDINATE DESCENT AND ALTERNATION Identifying Candidates for Alternation  12.3.1 12.3.2 Augmented Lagrangians and ADMM  12.4 GLOBAL OPTIMIZATION  12.4.1 Graduated Optimization 12.4.2 Randomized Global Optimization EXERCISES  12.5 ONLINE OPTIMIZATION 12.6  Section IV Functions, Derivatives, and Integrals Chapter 13  cid:4  Interpolation  13.1  INTERPOLATION IN A SINGLE VARIABLE 13.1.1 Polynomial Interpolation 13.1.2 Alternative Bases 13.1.3 Piecewise Interpolation  13.2 MULTIVARIABLE INTERPOLATION  13.2.1 Nearest-Neighbor Interpolation 13.2.2 Barycentric Interpolation 13.2.3 Grid-Based Interpolation THEORY OF INTERPOLATION 13.3.1 Linear Algebra of Functions 13.3.2 Approximation via Piecewise Polynomials EXERCISES  13.3  13.4  Chapter 14  cid:4  Integration and Differentiation  14.1 MOTIVATION 14.2 QUADRATURE  Interpolatory Quadrature  14.2.1 14.2.2 Quadrature Rules 14.2.3 Newton-Cotes Quadrature 14.2.4 Gaussian Quadrature 14.2.5 Adaptive Quadrature 14.2.6 Multiple Variables 14.2.7 Conditioning 14.3 DIFFERENTIATION  14.3.1 Diﬀerentiating Basis Functions 14.3.2 Finite Diﬀerences 14.3.3 Richardson Extrapolation  231 231 235 240 241 243 244 248  257 258 258 262 263 265 265 266 268 269 269 272 272 277 278 279 280 281 282 286 287 289 290 290 291 291 293   Contents  cid:4  xiii  14.3.4 Choosing the Step Size 14.3.5 Automatic Diﬀerentiation 14.3.6 EXERCISES  14.4  Chapter 15  cid:4  Ordinary Differential Equations  Integrated Quantities and Structure Preservation  15.1 MOTIVATION 15.2  THEORY OF ODES 15.2.1 Basic Notions 15.2.2 Existence and Uniqueness 15.2.3 Model Equations TIME-STEPPING SCHEMES 15.3.1 Forward Euler 15.3.2 Backward Euler 15.3.3 Trapezoidal Method 15.3.4 Runge-Kutta Methods 15.3.5 Exponential Integrators  15.3  15.4 MULTIVALUE METHODS  15.4.1 Newmark Integrators 15.4.2 EXERCISES  15.5 COMPARISON OF INTEGRATORS 15.6  Staggered Grid and Leapfrog  Chapter 16  cid:4  Partial Differential Equations  16.1 MOTIVATION 16.2  STATEMENT AND STRUCTURE OF PDES 16.2.1 Properties of PDEs 16.2.2 Boundary Conditions  16.4  16.3 MODEL EQUATIONS 16.3.1 Elliptic PDEs 16.3.2 Parabolic PDEs 16.3.3 Hyperbolic PDEs REPRESENTING DERIVATIVE OPERATORS 16.4.1 Finite Diﬀerences 16.4.2 Collocation 16.4.3 Finite Elements 16.4.4 Finite Volumes 16.4.5 Other Methods SOLVING PARABOLIC AND HYPERBOLIC EQUATIONS  16.5  294 295 296 298 303 304 305 305 307 309 311 311 313 314 315 316 318 318 321 322 324 329 330 335 335 336 338 338 339 340 341 342 346 347 350 351 352   Semidiscrete Methods  16.5.1 16.5.2 Fully Discrete Methods 16.6 NUMERICAL CONSIDERATIONS  16.6.1 Consistency, Convergence, and Stability 16.6.2 Linear Solvers for PDE EXERCISES  16.7  xiv  cid:4  Contents  Bibliography  Index  352 353 354 354 354 355 361  369   Preface  C OMPUTER science is experiencing a fundamental shift in its approach to modeling  and problem solving. Early computer scientists primarily studied discrete mathematics, focusing on structures like graphs, trees, and arrays composed of a ﬁnite number of distinct pieces. With the introduction of fast ﬂoating-point processing alongside “big data,” three- dimensional scanning, and other sources of noisy input, modern practitioners of computer science must design robust methods for processing and understanding real-valued data. Now, alongside discrete mathematics computer scientists must be equally ﬂuent in the languages of multivariable calculus and linear algebra.  Numerical Algorithms introduces the skills necessary to be both clients and designers of numerical methods for computer science applications. This text is designed for advanced undergraduate and early graduate students who are comfortable with mathematical nota- tion and formality but need to review continuous concepts alongside the algorithms under consideration. It covers a broad base of topics, from numerical linear algebra to optimization and diﬀerential equations, with the goal of deriving standard approaches while developing the intuition and comfort needed to understand more extensive literature in each subtopic. Thus, each chapter gently but rigorously introduces numerical methods alongside mathe- matical background and motivating examples from modern computer science.  Nearly every section considers real-world use cases for a given class of numerical algo- rithms. For example, the singular value decomposition is introduced alongside statistical methods, point cloud alignment, and low-rank approximation, and the discussion of least- squares includes concepts from machine learning like kernelization and regularization. The goal of this presentation of theory and application in parallel is to improve intuition for the design of numerical methods and the application of each method to practical situations.  Special care has been taken to provide unifying threads from chapter to chapter. This strategy helps relate discussions of seemingly independent problems, reinforcing skills while presenting increasingly complex algorithms. In particular, starting with a chapter on math- ematical preliminaries, methods are introduced with variational principles in mind, e.g., solving the linear system A cid:126 x =  cid:126 b by minimizing the energy  cid:107 A cid:126 x −  cid:126 b cid:107 2 2 or ﬁnding eigenvec- tors as critical points of the Rayleigh quotient. The book is organized into sections covering a few large-scale topics:  I. Preliminaries covers themes that appear in all branches of numerical algorithms. We start with a review of relevant notions from continuous mathematics, designed as a refresher for students who have not made extensive use of calculus or linear algebra since their introductory math classes. This chapter can be skipped if students are conﬁdent in their mathematical abilities, but even advanced readers may consider taking a look to understand notation and basic constructions that will be used re- peatedly later on. Then, we proceed with a chapter on numerics and error analysis, the basic tools of numerical analysis for representing real numbers and understanding the quality of numerical algorithms. In many ways, this chapter explicitly covers the high-level themes that make numerical algorithms diﬀerent from discrete algorithms: In this domain, we rarely expect to recover exact solutions to computational problems but rather approximate them.  xv   xvi  cid:4  Preface  II. Linear Algebra covers the algorithms needed to solve and analyze linear systems of equations. This section is designed not only to cover the algorithms found in any treatment of numerical linear algebra—including Gaussian elimination, matrix fac- torization, and eigenvalue computation—but also to motivate why these tools are useful for computer scientists. To this end, we will explore wide-ranging applications in data analysis, image processing, and even face recognition, showing how each can be reduced to an appropriate matrix problem. This discussion will reveal that numerical linear algebra is far from an exercise in abstract algorithmics; rather, it is a tool that can be applied to countless computational models.  III. Nonlinear Techniques explores the structure of problems that do not reduce to linear systems of equations. Two key tasks arise in this section, root-ﬁnding and opti- mization, which are related by Lagrange multipliers and other optimality conditions. Nearly any modern algorithm for machine learning involves optimization of some ob- jective, so we will ﬁnd no shortage of examples from recent research and engineering. After developing basic iterative methods for constrained and unconstrained optimiza- tion, we will return to the linear system A cid:126 x =  cid:126 b, developing the conjugate gradients algorithm for approximating  cid:126 x using optimization tools. We conclude this section with a discussion of “specialized” optimization algorithms, which are gaining popularity in recent research. This chapter, whose content does not appear in classical texts, covers strategies for developing algorithms speciﬁcally to minimize a single energy functional. This approach contrasts with our earlier treatment of generic approaches for minimiza- tion that work for broad classes of objectives, presenting computational challenges on paper with the reward of increased optimization eﬃciency.  IV. Functions, Derivatives, and Integrals concludes our consideration of numerical algorithms by examining problems in which an entire function rather than a sin- gle value or point is the unknown. Example tasks in this class include interpolation, approximation of derivatives and integrals of a function from samples, and solution of diﬀerential equations. In addition to classical applications in computational physics, we will show how these tools are relevant to a wide range of problems including ren- dering of three-dimensional shapes, x-ray scanning, and geometry processing.  Individual chapters are designed to be fairly independent, but of course it is impossible to orthogonalize the content completely. For example, iterative methods for optimization and root-ﬁnding must solve linear systems of equations in each iteration, and some inter- polation methods can be posed as optimization problems. In general, Parts III  Nonlinear Techniques  and IV  Functions, Derivatives, and Integrals  are largely independent of one another but both depend on matrix algorithms developed in Part II  Linear Algebra . In each part, the chapters are presented in order of importance. Initial chapters introduce key themes in the subﬁeld of numerical algorithms under consideration, while later chapters focus on advanced algorithms adjacent to new research; sections within each chapter are organized in a similar fashion.  Numerical algorithms are very diﬀerent from algorithms approached in most other branches of computer science, and students should expect to be challenged the ﬁrst time they study this material. With practice, however, it can be easy to build up intuition for this unique and widely applicable ﬁeld. To support this goal, each chapter concludes with a set of problems designed to encourage critical thinking about the material at hand.  Simple computational problems in large part are omitted from the text, under the expec- tation that active readers approach the book with pen and paper in hand. Some suggestions of exercises that can help readers as they peruse the material, but are not explicitly included in the end-of-chapter problems, include the following:   Preface  cid:4  xvii  1. Try each algorithm by hand. For instance, after reading the discussion of algorithms for solving the linear system A cid:126 x =  cid:126 b, write down a small matrix A and corresponding vector  cid:126 b, and make sure you can recover  cid:126 x by following the steps the algorithm. After reading the treatment of optimization, write down a speciﬁc function f   cid:126 x  and a few iterates  cid:126 x1,  cid:126 x2,  cid:126 x3, . . . of an optimization method to make sure f   cid:126 x1  ≥ f   cid:126 x2  ≥ f   cid:126 x3  > ··· .  2. Implement the algorithms in software and experiment with their behavior. Many nu- merical algorithms take on beautifully succinct—and completely abstruse—forms that must be unraveled when they are implemented in code. Plus, nothing is more reward- ing than the moment when a piece of numerical code begins functioning properly, transitioning from an abstract sequence of mathematical statements to a piece of machinery systematically solving equations or decreasing optimization objectives.  3. Attempt to derive algorithms by hand without referring to the discussion in the book. The best way to become an expert in numerical analysis is to be able to reconstruct the basic algorithms by hand, an exercise that supports intuition for the existing methods and will help suggest extensions to other problems you may encounter.  Any large-scale treatment of a ﬁeld as diverse and classical as numerical algorithms is bound to omit certain topics, and inevitably decisions of this nature may be controversial to readers with diﬀerent backgrounds. This book is designed for a one- to two-semester course in numerical algorithms, for computer scientists rather than mathematicians or engineers in scientiﬁc computing. This target audience has led to a focus on modeling and applications rather than on general proofs of convergence, error bounds, and the like; the discussion includes references to more specialized or advanced literature when possible. Some topics, including the fast Fourier transform, algorithms for sparse linear systems, Monte Carlo methods, adaptivity in solving diﬀerential equations, and multigrid methods, are mentioned only in passing or in exercises in favor of explaining modern developments in optimization and other algorithms that have gained recent popularity. Future editions of this textbook may incorporate these or other topics depending on feedback from instructors and readers. The reﬁnement of course notes and other materials leading to this textbook beneﬁted from the generous input of my students and colleagues. In the interests of maintaining these materials and responding to the needs of students and instructors, please do not hesitate to contact me with questions, comments, concerns, or ideas for potential changes.  Justin Solomon    Acknowledgments  P REPARATION of this textbook would not have been possible without the support  of countless individuals and organizations. I have attempted to acknowledge some of the many contributors and supporters below. I cannot thank these colleagues and friends enough for their patience and attention throughout this undertaking.  The book is dedicated to the memory of Professor Cliﬀord Nass, whose guidance funda- mentally shaped my early academic career. His wisdom, teaching, encouragement, enthusi- asm, and unique sense of style all will be missed on the Stanford campus and in the larger community.  My mother, Nancy Griesemer, was the ﬁrst to suggest expanding my teaching materials into a text. I would not have been able to ﬁnd the time or energy to prepare this work without her support or that from my father Rod Solomon; my sister Julia Solomon Ensor, her husband Jeﬀ Ensor, and their daughter Caroline Ensor; and my grandmothers Juddy Solomon and Dolores Griesemer. My uncle Peter Silberman and aunt Dena Silberman have supported my academic career from its inception. Many other family members also should be thanked including Archa and Joseph Emerson; Jerry, Jinny, Kate, Bonnie, and Jeremiah Griesemer; Jim, Marge, Paul, Laura, Jarrett, Liza, Jiana, Lana, Jahson, Jaime, Gabriel, and Jesse Solomon; Chuck and Louise Silverberg; and Barbara, Kerry, Greg, and Amy Schaner. My career at Stanford has been guided primarily by my advisor Leonidas Guibas and co-advisor Adrian Butscher. The approaches I take to many of the problems in the book undoubtedly imitate the problem-solving strategies they have taught me. Ron Fedkiw sug- gested I teach the course leading to this text and provided advice on preparing the material. My collaborators in the Geometric Computing Group and elsewhere on campus—including Panagiotis Achlioptas, Roland Angst, Mirela Ben-Chen, Daniel Chen, Takuya Funatomi, Tanya Glozman, Jonathan Huang, Qixing Huang, Michael Kerber, Vladimir Kim, Young Min Kim, Yang Li, Yangyan Li, Andy Nguyen, Maks Ovsjanikov, Franco Pestilli, Chris Piech, Raif Rustamov, Hao Su, Minhyuk Sung, Fan Wang, and Eric Yi—kindly have allowed me to use some research time to complete this text and have helped reﬁne the discussion at many points. Staﬀ in the Stanford computer science department, including Meredith Hutchin, Claire Stager, and Steven Magness, made it possible to organize my numerical algorithms course and many others.  I owe many thanks to the students of Stanford’s CS 205A course  fall 2013  for catch- ing numerous typos and mistakes in an early draft of this book; students in CS 205A  spring 2015  also identiﬁed some subtle typos and mathematical issues. The following is a no-doubt incomplete list of students and course assistants who contributed to this ef- fort: Abraham Botros, Paulo Camasmie, Scott Chung, James Cranston, Deepyaman Datta, Tao Du, Lennart Jansson, Miles Johnson, David Hyde, Luke Knepper, Warner Krause, Ilya Kudryavtsev, Minjae Lee, Nisha Masharani, David McLaren, Sid Mittal, J. Eduardo Mu- cino, Catherine Mullings, John Reyna, Blue Sheﬀer, William Song, Ben-Han Sung, Martina Troesch, Ozhan Turgut, Blanca Isabel Villanueva, Jon Walker, Patrick Ward, Joongyeub Yeo, and Yang Zhao.  xix   xx  cid:4  Acknowledgments  David Hyde and Scott Chung continued to provide detailed feedback in winter and spring 2014. In addition, they helped prepare ﬁgures and end-of-chapter problems. Problems that they drafted are marked DH and SC, respectively.  I leaned upon several colleagues and friends to help edit the text. In addition to those mentioned above, additional contributors include: Nick Alger, George Anderson, Rahil Baber, Nicolas Bonneel, Chen Chen, Matthew Cong, Roy Frostig, Jessica Hwang, Howon Lee, Julian Kates-Harbeck, Jonathan Lee, Niru Maheswaranathan, Mark Pauly, Dan Robin- son, and Hao Zhuang.  Special thanks to Jan Heiland and Tao Du for helping clarify the derivation of the BFGS  algorithm.  Charlotte Byrnes, Sarah Chow, Rebecca Condit, Randi Cohen, Kate Gallo, and Hayley Ruggieri at Taylor & Francis guided me through the publication process and answered countless questions as I prepared this work for print.  The Hertz Foundation provided a valuable network of experienced and knowledgeable members of the academic community. In particular, Louis Lerman provided career advice throughout my PhD that shaped my approach to research and navigating academia. Other members of the Hertz community who provided guidance include Diann Callaghan, Wendy Cieslak, Jay Davis, Philip Eckhoﬀ, Linda Kubiak, Amanda O’Connor, Linda Souza, Thomas Weaver, and Katherine Young. I should also acknowledge the NSF GRFP and NDSEG fellowships for their support.  A multitude of friends supported this work in assorted stages of its development. Addi- tional collaborators and mentors in the research community who have discussed and encour- aged this work include Keenan Crane, Fernando de Goes, Michael Eichmair, Hao Li, Niloy Mitra, Helmut Pottmann, Fei Sha, Olga Sorkine-Hornung, Amir Vaxman, Etienne Vouga, Brian Wandell, and Chris Wojtan. The ﬁrst several chapters of this book were drafted on tour with the Stanford Symphony Orchestra on their European tour “In Beethoven’s Foot- steps”  summer 2013 . Beyond this tour, Geri Actor, Susan Bratman, Debra Fong, Stephen Harrison, Patrick Kim, Mindy Perkins, Thomas Shoebotham, and Lowry Yankwich all sup- ported musical breaks during the drafting of this book. Prometheus Athletics provided an unexpected outlet, and I should thank Archie de Torres, Amy Giver, Lori Giver, Troy Obrero, and Ben Priestley for allowing me to be an enthusiastic if clumsy participant.  Additional friends who have lent advice, assistance, and time to this eﬀort include: Chris Aakre, Katy Ashe, Katya Avagian, Kyle Barrett, Noelle Beegle, Gilbert Bernstein, Elizabeth Blaber, Lia Bonamassa, Eric Boromisa, Katherine Breeden, Karen Budd, Lindsay Burdette, Avery Bustamante, Rose Casey, Arun Chaganty, Phil Chen, Andrew Chou, Bernie Chu, Cindy Chu, Victor Cruz, Elan Dagenais, Abe Davis, Matthew Decker, Bailin Deng, Martin Duncan, Eric Ellenoﬀ, James Estrella, Alyson Falwell, Anna French, Adair Gerke, Christina Goeders, Gabrielle Gulo, Nathan Hall-Snyder, Logan Hehn, Jo Jaﬀe, Dustin Janatpour, Brandon Johnson, Victoria Johnson, Jeﬀ Gilbert, Stephanie Go, Alex Godofsky, Alan Guo, Randy Hernando, Petr Johanes, Maria Judnick, Ken Kao, Jonathan Kass, Gavin Kho, Hyungbin Kim, Sarah Kongpachith, Jim Lalonde, Lauren Lax, Atticus Lee, Eric Lee, Jonathan Lee, Menyoung Lee, Letitia Lew, Siyang Li, Adrian Lim, Yongwhan Lim, Alex Louie, Lily Louie, Kate Lowry, Cleo Messinger, Courtney Meyer, Daniel Meyer, Lisa New- man, Logan Obrero, Pualani Obrero, Thomas Obrero, Molly Pam, David Parker, Madeline Paymer, Cuauhtemoc Peranda, Fabianna Perez, Bharath Ramsundar, Arty Rivera, Daniel Rosenfeld, Te Rutherford, Ravi Sankar, Aaron Sarnoﬀ, Amanda Schloss, Keith Schwarz, Steve Sellers, Phaedon Sinis, Charlton Soesanto, Mark Smitt, Jacob Steinhardt, Char- lie Syms, Andrea Tagliasacchi, Michael Tamkin, Sumil Thapa, David Tobin, Herb Tyson, Katie Tyson, Madeleine Udell, Greg Valdespino, Walter Vulej, Thomas Waggoner, Frank Wang, Sydney Wang, Susanna Wen, Genevieve Williams, Molby Wong, Eddy Wu, Kelima Yakupova, Winston Yan, and Evan Young.   I  Preliminaries  1    C H A P T E R1  Mathematics Review  CONTENTS  1.1 1.2  1.3  1.4  Preliminaries: Numbers and Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.2.1 Deﬁning Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.2.2 Span, Linear Independence, and Bases . . . . . . . . . . . . . . . . . . . . . . . . . 1.2.3 Our Focus: Rn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Linearity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.3.1 Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.3.2 Scalars, Vectors, and Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.3.3 Matrix Storage and Multiplication Methods . . . . . . . . . . . . . . . . . . . 1.3.4 Model Problem: A cid:126 x =  cid:126 b . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Non-Linearity: Diﬀerential Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.4.1 Diﬀerentiation in One Variable . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.4.2 Diﬀerentiation in Multiple Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.4.3 Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  3 4 4 5 7 9 10 12 13 14 15 16 17 20  I N this chapter, we will outline notions from linear algebra and multivariable calculus that  will be relevant to our discussion of computational techniques. It is intended as a review of background material with a bias toward ideas and interpretations commonly encountered in practice; the chapter can be safely skipped or used as reference by students with stronger background in mathematics.  1.1 PRELIMINARIES: NUMBERS AND SETS  Rather than considering algebraic  and at times philosophical  discussions like “What is a number?,” we will rely on intuition and mathematical common sense to deﬁne a few sets:    The natural numbers N = {1, 2, 3, . . .}   The integers Z = {. . . ,−2,−1, 0, 1, 2, . . .}   The rational numbers Q = {a b : a, b ∈ Z, b  cid:54 = 0}   The real numbers R encompassing Q as well as irrational numbers like π and √2   The complex numbers C = {a + bi : a, b ∈ R}, where i ≡ √−1.  The deﬁnition of Q is the ﬁrst of many times that we will use the notation {A : B}; the braces denote a set and the colon can be read as “such that.” For instance, the deﬁnition of Q can be read as “the set of fractions a b such that a and b are integers.” As a second example, we could write N = {n ∈ Z : n > 0}. It is worth acknowledging that our deﬁnition 3   4  cid:4  Numerical Algorithms  of R is far from rigorous. The construction of the real numbers can be an important topic for practitioners of cryptography techniques that make use of alternative number systems, but these intricacies are irrelevant for the discussion at hand.  N, Z, Q, R, and C can be manipulated using generic operations to generate new sets of  numbers. In particular, we deﬁne the “Euclidean product” of two sets A and B as  We can take powers of sets by writing  A × B = { a, b  : a ∈ A and b ∈ B}.  ·„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„‚„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„¶  An = A × A × ··· × A  .  n times  Rn = { a1, a2, . . . , an  : ai ∈ R for all i}.  1.2 VECTOR SPACES  This construction yields what will become our favorite set of numbers in chapters to come:  Introductory linear algebra courses easily could be titled “Introduction to Finite- Dimensional Vector Spaces.” Although the deﬁnition of a vector space might appear ab- stract, we will ﬁnd many concrete applications expressible in vector space language that can beneﬁt from the machinery we will develop.  1.2.1 Defining Vector Spaces  We begin by deﬁning a vector space and providing a number of examples: Deﬁnition 1.1  Vector space over R . A vector space over R is a set V closed under addition and scalar multiplication satisfying the following axioms:    Additive commutativity and associativity: For all  cid:126 u,  cid:126 v,  cid:126 w ∈ V,  cid:126 v +  cid:126 w =  cid:126 w +  cid:126 v and    cid:126 u +  cid:126 v  +  cid:126 w =  cid:126 u +   cid:126 v +  cid:126 w .    Distributivity: For all  cid:126 v,  cid:126 w ∈ V and a, b ∈ R, a  cid:126 v+  cid:126 w  = a cid:126 v+a  cid:126 w and  a+b  cid:126 v = a cid:126 v+b cid:126 v.   Additive identity: There exists  cid:126 0 ∈ V with  cid:126 0 +  cid:126 v =  cid:126 v for all  cid:126 v ∈ V.   Additive inverse: For all  cid:126 v ∈ V, there exists  cid:126 w ∈ V with  cid:126 v +  cid:126 w =  cid:126 0.   Multiplicative identity: For all  cid:126 v ∈ V, 1 ·  cid:126 v =  cid:126 v.   Multiplicative compatibility: For all  cid:126 v ∈ V and a, b ∈ R,  ab  cid:126 v = a b cid:126 v .  A member  cid:126 v ∈ V is known as a vector ; arrows will be used to indicate vector variables.  For our purposes, a scalar is a number in R; a complex vector space satisﬁes the same deﬁnition with R replaced by C. It is usually straightforward to spot vector spaces in the wild, including the following examples:  Example 1.1  Rn as a vector space . The most common example of a vector space is Rn. Here, addition and scalar multiplication happen component-by-component:   1, 2  +  −3, 4  =  1 − 3, 2 + 4  =  −2, 6   10 ·  −1, 1  =  10 · −1, 10 · 1  =  −10, 10 .   Mathematics Review  cid:4  5  Figure 1.1  a  Vectors  cid:126 v1,  cid:126 v2 ∈ R2; the plane R2; span{ cid:126 v1,  cid:126 v2,  cid:126 v3} = span{ cid:126 v1,  cid:126 v2} because  cid:126 v3 is a linear combination of  cid:126 v1 and  cid:126 v2.  span is  their   b    c   Example 1.2  Polynomials . A second example of a vector space is the ring of polynomials with real-valued coeﬃcients, denoted R[x]. A polynomial p ∈ R[x] is a function p : R → R taking the form∗  p x  = cid:88 k  akxk.  Addition and scalar multiplication are carried out in the usual way, e.g., if p x  = x2+2x−1 and q x  = x3, then 3p x  + 5q x  = 5x3 + 3x2 + 6x − 3, which is another polynomial. As an aside, for future examples note that functions like p x  =  x − 1  x + 1  + x2 x3 − 5  are still polynomials even though they are not explicitly written in the form above.  ﬁnite numbers of terms, so this sequence eventually will end in a string of zeros.   cid:126 vi’s. In the second example, the “vectors” are polynomials, although we do not normally use this language to discuss R[x]; unless otherwise noted, we will assume variables notated with arrows  cid:126 v are members of Rn for some n. One way to link these two viewpoints would  A weighted sum cid:80 i ai cid:126 vi, where ai ∈ R and  cid:126 vi ∈ V, is known as a linear combination of the be to identify the polynomial cid:80 k akxk with the sequence  a0, a1, a2, . . . ; polynomials have 1.2.2 Span, Linear Independence, and Bases Suppose we start with vectors  cid:126 v1, . . . ,  cid:126 vk ∈ V in vector space V. By Deﬁnition 1.1, we have two ways to start with these vectors and construct new elements of V: addition and scalar multiplication. Span describes all of the vectors you can reach via these two operations:  Deﬁnition 1.2  Span . The span of a set S ⊆ V of vectors is the set  span S ≡ {a1 cid:126 v1 + ··· + ak cid:126 vk :  cid:126 vi ∈ S and ai ∈ R for all i}.  Figure 1.1 a-b  illustrates the span of two vectors. By deﬁnition, span S is a subspace of V, that is, a subset of V that is itself a vector space. We provide a few examples: Example 1.3  Mixology . The typical well at a cocktail bar contains at least four ingre- dients at the bartender’s disposal: vodka, tequila, orange juice, and grenadine. Assuming we have this well, we can represent drinks as points in R4, with one element for each in- gredient. For instance, a tequila sunrise can be represented using the point  0, 1.5, 6, 0.75 ,  ∗The notation f : A → B means f is a function that takes as input an element of set A and outputs an element of set B. For instance, f : R → Z takes as input a real number in R and outputs an integer Z, as might be the case for f  x  =  cid:98 x cid:99 , the “round down” function.   cid:2 v2  R2  a   cid:2 v1,  cid:2 v2 ∈ R2   cid:2 v1  span{ cid:2 v1,  cid:2 v2}   b  span{ cid:2 v1,  cid:2 v2}   c  span{ cid:2 v1,  cid:2 v2,  cid:2 v3}   cid:2 v3   6  cid:4  Numerical Algorithms  representing amounts of vodka, tequila, orange juice, and grenadine  in ounces , respec- tively.  The set of drinks that can be made with our well is contained in  span{ 1, 0, 0, 0 ,  0, 1, 0, 0 ,  0, 0, 1, 0 ,  0, 0, 0, 1 },  that is, all combinations of the four basic ingredients. A bartender looking to save time, however, might notice that many drinks have the same orange juice-to-grenadine ratio and mix the bottles. The new simpliﬁed well may be easier for pouring but can make fundamentally fewer drinks:  span{ 1, 0, 0, 0 ,  0, 1, 0, 0 ,  0, 0, 6, 0.75 }.  For example, this reduced well cannot fulﬁll orders for a screwdriver, which contains orange juice but not grenadine.  Example 1.4  Cubic polynomials . Deﬁne pk x  ≡ xk. With this notation, the set of cubic polynomials can be written in two equivalent ways  {ax3 + bx2 + cx + d ∈ R[x] : a, b, c, d ∈ R} = span{p0, p1, p2, p3}.  Adding another item to a set of vectors does not always increase the size of its span, as  illustrated in Figure 1.1 c . For instance, in R2,  span{ 1, 0 ,  0, 1 } = span{ 1, 0 ,  0, 1 ,  1, 1 }.  In this case, we say that the set { 1, 0 ,  0, 1 ,  1, 1 } is linearly dependent: Deﬁnition 1.3  Linear dependence . We provide three equivalent deﬁnitions. A set S ⊆ V of vectors is linearly dependent if:  1. One of the elements of S can be written as a linear combination of the other elements,  or S contains zero.   cid:80 m  2. There exists a non-empty linear combination of elements  cid:126 vk ∈ S yielding  k=1 ck cid:126 vk = 0 where ck  cid:54 = 0 for all k.  3. There exists  cid:126 v ∈ S such that span S = span S\{ cid:126 v}. That is, we can remove a vector  from S without aﬀecting its span.  If S is not linearly dependent, then we say it is linearly independent.  Providing proof or informal evidence that each deﬁnition is equivalent to its counterparts  in an “if and only if” fashion  is a worthwhile exercise for students less comfortable with notation and abstract mathematics.  The concept of linear dependence provides an idea of “redundancy” in a set of vectors. In this sense, it is natural to ask how large a set we can construct before adding another vector cannot possibly increase the span. More speciﬁcally, suppose we have a linearly independent set S ⊆ V, and now we choose an additional vector  cid:126 v ∈ V. Adding  cid:126 v to S has one of two possible outcomes:  1. The span of S ∪ { cid:126 v} is larger than the span of S. 2. Adding  cid:126 v to S has no eﬀect on its span.   Mathematics Review  cid:4  7  The dimension of V counts the number of times we can get the ﬁrst outcome while building up a set of vectors:  Deﬁnition 1.4  Dimension and basis . The dimension of V is the maximal size S of a linearly independent set S ⊂ V such that span S = V. Any set S satisfying this property is called a basis for V. Example 1.5  Rn . The standard basis for Rn is the set of vectors of the form  ·„„„„„„„„„„„„„‚„„„„„„„„„„„„„¶   cid:126 ek ≡   0, . . . , 0, k−1 elements  1, 0, . . . , 0   .  ·„„„„„„„„„„„‚„„„„„„„„„„¶  n−k elements  That is,  cid:126 ek has all zeros except for a single one in the k-th position. These vectors are linearly independent and form a basis for Rn; for example in R3 any vector  a, b, c  can be written as a cid:126 e1 + b cid:126 e2 + c cid:126 e3. Thus, the dimension of Rn is n, as expected.  Example 1.6  Polynomials . The set of monomials {1, x, x2, x3, . . .} is a linearly inde- pendent subset of R[x]. It is inﬁnitely large, and thus the dimension of R[x] is ∞. 1.2.3 Our Focus: Rn Of particular importance for our purposes is the vector space Rn, the so-called n-dimensional Euclidean space. This is nothing more than the set of coordinate axes encountered in high school math classes:    R1 ≡ R is the number line.   R2 is the two-dimensional plane with coordinates  x, y .   R3 represents three-dimensional space with coordinates  x, y, z .  Nearly all methods in this book will deal with transformations of and functions on Rn.  For convenience, we usually write vectors in Rn in “column form,” as follows:  This notation will include vectors as special cases of matrices discussed below.  Unlike some vector spaces, Rn has not only a vector space structure, but also one  additional construction that makes all the diﬀerence: the dot product.  Deﬁnition 1.5  Dot product . The dot product of two vectors  cid:126 a =  a1, . . . , an  and  cid:126 b =  b1, . . . , bn  in Rn is given by   a1, . . . , an  ≡    a1 a2 ... an  .     cid:126 a ·  cid:126 b ≡  akbk.  n cid:88 k=1   8  cid:4  Numerical Algorithms  Example 1.7  R2 . The dot product of  1, 2  and  −2, 6  is 1·−2 + 2· 6 = −2 + 12 = 10. The dot product is an example of a metric, and its existence gives a notion of geometry to Rn. For instance, we can use the Pythagorean theorem to deﬁne the norm or length of a vector  cid:126 a as the square root   cid:107  cid:126 a cid:107 2 ≡ cid:113 a2  n = √ cid:126 a ·  cid:126 a. Then, the distance between two points  cid:126 a, cid:126 b ∈ Rn is  cid:107  cid:126 b −  cid:126 a cid:107 2. trigonometric identity holds for  cid:126 a, cid:126 b ∈ R3:  1 + ··· + a2  Dot products provide not only lengths and distances but also angles. The following  where θ is the angle between  cid:126 a and  cid:126 b. When n ≥ 4, however, the notion of “angle” is much harder to visualize in Rn. We might deﬁne the angle θ between  cid:126 a and  cid:126 b to be   cid:126 a ·  cid:126 b =  cid:107  cid:126 a cid:107 2 cid:107  cid:126 b cid:107 2 cos θ,  θ ≡ arccos   cid:126 a ·  cid:126 b   cid:107  cid:126 a cid:107 2 cid:107  cid:126 b cid:107 2  .  We must do our homework before making such a deﬁnition! In particular, cosine outputs values in the interval [−1, 1], so we must check that the input to arc cosine  also notated cos−1  is in this interval; thankfully, the well-known Cauchy-Schwarz inequality  cid:126 a ·  cid:126 b ≤  cid:107  cid:126 a cid:107 2 cid:107  cid:126 b cid:107 2 guarantees exactly this property. When  cid:126 a = c cid:126 b for some c ∈ R, we have θ = arccos 1 = 0, as we would expect: The angle between parallel vectors is zero. What does it mean for  nonzero  vectors to be perpendic- ular? Let’s substitute θ = 90◦. Then, we have  0 = cos 90◦ =   cid:126 a ·  cid:126 b   cid:107  cid:126 a cid:107 2 cid:107  cid:126 b cid:107 2  .  Multiplying both sides by  cid:107  cid:126 a cid:107 2 cid:107  cid:126 b cid:107 2 motivates the deﬁnition: Deﬁnition 1.6  Orthogonality . Two vectors  cid:126 a, cid:126 b ∈ Rn are perpendicular, or orthogonal, when  cid:126 a ·  cid:126 b = 0. This deﬁnition is somewhat surprising from a geometric standpoint. We have managed to deﬁne what it means to be perpendicular without any explicit use of angles.  Aside 1.1. There are many theoretical questions to ponder here, some of which we will address in future chapters:    Do all vector spaces admit dot products or similar structures?   Do all ﬁnite-dimensional vector spaces admit dot products?   What might be a reasonable dot product between elements of R[x]?  Intrigued students can consult texts on real and functional analysis.   Mathematics Review  cid:4  9  1.3 LINEARITY  A function from one vector space to another that preserves linear structure is known as a linear function: Deﬁnition 1.7  Linearity . Suppose V and V cid:48  are vector spaces. Then, L : V → V cid:48  is linear if it satisﬁes the following two criteria for all  cid:126 v,  cid:126 v1,  cid:126 v2 ∈ V and c ∈ R:    L preserves sums: L[ cid:126 v1 +  cid:126 v2] = L[ cid:126 v1] + L[ cid:126 v2]   L preserves scalar products: L[c cid:126 v] = cL[ cid:126 v]  It is easy to express linear maps between vector spaces, as we can see in the following examples: Example 1.8  Linearity in Rn . The following map f : R2 → R3 is linear:  f  x, y  =  3x, 2x + y,−y .  We can check linearity as follows:    Sum preservation:  f  x1 + x2, y1 + y2  =  3 x1 + x2 , 2 x1 + x2  +  y1 + y2 ,− y1 + y2    =  3x1, 2x1 + y1,−y1  +  3x2, 2x2 + y2,−y2  = f  x1, y1  + f  x2, y2     Scalar product preservation:  f  cx, cy  =  3cx, 2cx + cy,−cy   = c 3x, 2x + y,−y  = cf  x, y   Contrastingly, g x, y  ≡ xy2 is not linear. For instance, g 1, 1  = 1, but g 2, 2  = 8  cid:54 = 2 · g 1, 1 , so g does not preserve scalar products. Example 1.9  Integration . The following “functional” L from R[x] to R is linear:  L[p x ] ≡ cid:90  1  0  p x  dx.  This more abstract example maps polynomials p x  ∈ R[x] to real numbers L[p x ] ∈ R. For example, we can write  Linearity of L is a result of the following well-known identities from calculus:  0  L[3x2 + x − 1] = cid:90  1  cid:90  1 c · f  x  dx = c cid:90  1 [f  x  + g x ] dx = cid:90  1  0  0  0   cid:90  1  0   3x2 + x − 1  dx =  1 2  .  f  x  dx  f  x  dx + cid:90  1  0  g x  dx.   10  cid:4  Numerical Algorithms  We can write a particularly nice form for linear maps on Rn. The vector  cid:126 a =  a1, . . . , an   is equal to the sum cid:80 k ak cid:126 ek, where  cid:126 ek is the k-th standard basis vector from Example 1.5.  Then, if L is linear we can expand:  L[ cid:126 a] = L cid:34  cid:88 k = cid:88 k = cid:88 k  ak cid:126 ek cid:35  for the standard basis  cid:126 ek  L [ak cid:126 ek] by sum preservation  akL [ cid:126 ek] by scalar product preservation.  This derivation shows:  A linear operator L on Rn is completely determined by its action  on the standard basis vectors  cid:126 ek.  That is, for any vector  cid:126 a ∈ Rn, we can use the sum above to determine L[ cid:126 a] by linearly combining L[ cid:126 e1], . . . ,L[ cid:126 en]. Example 1.10  Expanding a linear map . Recall the map in Example 1.8 given by f  x, y  =  3x, 2x + y,−y . We have f   cid:126 e1  = f  1, 0  =  3, 2, 0  and f   cid:126 e2  = f  0, 1  =  0, 1,−1 . Thus, the formula above shows:  f  x, y  = xf   cid:126 e1  + yf   cid:126 e2  = x  3 2  0  + y  0 1  −1  .  1.3.1 Matrices  The expansion of linear maps above suggests a context in which it is useful to store multiple vectors in the same structure. More generally, say we have n vectors  cid:126 v1, . . . ,  cid:126 vn ∈ Rm. We can write each as a column vector:  Carrying these vectors around separately can be cumbersome notationally, so to simplify matters we combine them into a single m × n matrix:   cid:126 v1 =     cid:126 v1   v11 v21 ... vm1    v12 v22 ... vm2  ,  cid:126 v2 =  =    cid:126 v2    ···  cid:126 vn     ,··· ,  cid:126 vn =  v1n v2n ... vmn  v11 v21 ... vm1  v12 v22 ... vm2  ··· ··· ... ···  v1n v2n ... vmn  .  .     We will call the space of such matrices Rm×n.   Mathematics Review  cid:4  11  Example 1.11  Identity matrix . We can store the standard basis for Rn in the n × n “identity matrix” In×n given by:  Since we constructed matrices as convenient ways to store sets of vectors, we can use multiplication to express how they can be combined linearly. In particular, a matrix in Rm×n can be multiplied by a column vector in Rn as follows:  In×n ≡    cid:126 e1     cid:126 e2    ···  cid:126 en       cid:126 v1     v11 v21 ... vm1  v12 v22 ... vm2    c1 c2 ... cn      ···  cid:126 vn   v1n v2n ... vmn    c1 c2 ... cn      cid:126 v2   ··· ··· ... ···  .  1 0 ... 0 0  0 0 ... 1 0  0 0 ... 0 1   =  0 ··· 1 ··· ... ... 0 ··· 0 ···     ≡ c1 cid:126 v1 + c2 cid:126 v2 + ··· + cn cid:126 vn. =  ...  c1v11 + c2v12 + ··· + cnv1n c1v21 + c2v22 + ··· + cnv2n  c1vm1 + c2vm2 + ··· + cnvmn  .    Expanding this sum yields the following explicit formula for matrix-vector products:  Example 1.12  Identity matrix multiplication . For any  cid:126 x ∈ Rn, we can write  cid:126 x = In×n cid:126 x, where In×n is the identity matrix from Example 1.11. Example 1.13  Linear map . We return once again to the function f  x, y  from Exam- ple 1.8 to show one more alternative form:  3 2  0 1  0 −1  cid:18  x f  x, y  = y  cid:19  .  ≡  We similarly deﬁne a product between a matrix M ∈ Rm×n and another matrix in Rn×p  with columns  cid:126 ci by concatenating individual matrix-vector products:  ··· M cid:126 cp     M cid:126 c1 M cid:126 c2     ···  cid:126 cp      cid:126 c1  cid:126 c2    M   .  Example 1.14  Mixology . Continuing Example 1.3, suppose we make a tequila sunrise and second concoction with equal parts of the two liquors in our simpliﬁed well. To ﬁnd out how much of the basic ingredients are contained in each order, we could combine the recipes for each column-wise and use matrix multiplication:  Well 1 Well 2 Well 3  Drink 1 Drink 2    Vodka Tequila OJ Grenadine  1 0 0 0  0 1 0 0    cid:32   0 0 6  0.75   cid:33     =  Drink 1 Drink 2  0 1.5 1  0.75 0.75  2  0 1.5 6  0.75  0.75 0.75 12 1.5  Vodka Tequila OJ Grenadine     12  cid:4  Numerical Algorithms  We will use capital letters to represent matrices, like A ∈ Rm×n. We will use the notation Aij ∈ R to denote the element of A at row i and column j. 1.3.2 Scalars, Vectors, and Matrices If we wish to unify notation completely, we can write a scalar as a 1 × 1 vector c ∈ R1×1. Similarly, as suggested in §1.2.3, if we write vectors in Rn in column form, they can be considered n × 1 matrices  cid:126 v ∈ Rn×1. Matrix-vector products also can be interpreted in this context. For example, if A ∈ Rm×n,  cid:126 x ∈ Rn, and  cid:126 b ∈ Rm, then we can write expressions like  We will introduce one additional operator on matrices that is useful in this context:  Deﬁnition 1.8  Transpose . The transpose of a matrix A ∈ Rm×n is a matrix A cid:62  ∈ Rn×m with elements  A cid:62  ij = Aji.  Example 1.15  Transposition . The transpose of the matrix  Aﬁ  ·  cid:126 xﬁ  =  cid:126 bﬁ  m×n  n×1  m×1  .  1 3 5  A = A cid:62  = cid:18  1  2  2 4  6  6  cid:19  .  5  3 4  is given by  Geometrically, we can think of transposition as ﬂipping a matrix over its diagonal.  This uniﬁed notation combined with operations like transposition and multiplication yields slick expressions and derivations of well-known identities. For instance, we can com- pute the dot products of vectors  cid:126 a, cid:126 b ∈ Rn via the following sequence of equalities:   cid:126 a ·  cid:126 b =  n cid:88 k=1  akbk = cid:0  a1 a2  ··· an  cid:1     b1 b2 ... bn    =  cid:126 a cid:62  cid:126 b.  Identities from linear algebra can be derived by chaining together these operations with a few rules:   A cid:62   cid:62  = A,   A + B  cid:62  = A cid:62  + B cid:62 ,  and   AB  cid:62  = B cid:62 A cid:62 .  Example 1.16  Residual norm . Suppose we have a matrix A and two vectors  cid:126 x and  cid:126 b. If we wish to know how well A cid:126 x approximates  cid:126 b, we might deﬁne a residual  cid:126 r ≡  cid:126 b − A cid:126 x; this residual is zero exactly when A cid:126 x =  cid:126 b. Otherwise, we can use the norm  cid:107  cid:126 r cid:107 2 as a proxy for the similarity of A cid:126 x and  cid:126 b. We can use the identities above to simplify:   cid:107  cid:126 r cid:107 2  2 =  cid:107  cid:126 b − A cid:126 x cid:107 2 =   cid:126 b − A cid:126 x  ·   cid:126 b − A cid:126 x  as explained in §1.2.3  2   Mathematics Review  cid:4  13  Figure 1.2 Two implementations of matrix-vector multiplication with diﬀerent loop ordering.  =   cid:126 b − A cid:126 x  cid:62   cid:126 b − A cid:126 x  by our expression for the dot product above =   cid:126 b cid:62  −  cid:126 x cid:62 A cid:62    cid:126 b − A cid:126 x  by properties of transposition =  cid:126 b cid:62  cid:126 b −  cid:126 b cid:62 A cid:126 x −  cid:126 x cid:62 A cid:62  cid:126 b +  cid:126 x cid:62 A cid:62 A cid:126 x after multiplication  All four terms on the right-hand side are scalars, or equivalently 1 × 1 matrices. Scalars thought of as matrices enjoy one additional nice property c cid:62  = c, since there is nothing to transpose! Thus,   cid:126 x cid:62 A cid:62  cid:126 b =   cid:126 x cid:62 A cid:62  cid:126 b  cid:62  =  cid:126 b cid:62 A cid:126 x.  This allows us to simplify even more:   cid:107  cid:126 r cid:107 2  2 =  cid:126 b cid:62  cid:126 b − 2 cid:126 b cid:62 A cid:126 x +  cid:126 x cid:62 A cid:62 A cid:126 x 2 − 2 cid:126 b cid:62 A cid:126 x +  cid:107  cid:126 b cid:107 2 =  cid:107 A cid:126 x cid:107 2 2.  We could have derived this expression using dot product identities, but the intermediate steps above will prove useful in later discussion.  1.3.3 Matrix Storage and Multiplication Methods  In this section, we take a brief detour from mathematical theory to consider practical aspects of implementing linear algebra operations in computer software. Our discussion considers not only faithfulness to the theory we have constructed but also the speed with which we can carry out each operation. This is one of relatively few points at which we will consider computer architecture and other engineering aspects of how computers are designed. This consideration is necessary given the sheer number of times typical numerical algorithms call down to linear algebra routines; a seemingly small improvement in imple- menting matrix-vector or matrix-matrix multiplication has the potential to increase the eﬃciency of numerical routines by a large factor.  Figure 1.2 shows two possible implementations of matrix-vector multiplication. The diﬀerence between these two algorithms is subtle and seemingly unimportant: The order of the two loops has been switched. Rounding error aside, these two methods generate the same output and do the same number of arithmetic operations; classical “big-O” analysis from computer science would ﬁnd these two methods indistinguishable. Surprisingly, however,  function Multiply A,  cid:2 x   cid:3  Returns  cid:2 b = A cid:2 x, where  cid:3  A ∈ Rm×n and  cid:2 x ∈ Rn  cid:2 b ←  cid:2 0 for i ← 1, 2, . . . , m for j ← 1, 2, . . . , n bi ← bi + aijxj  return  cid:2 b   a   function Multiply A,  cid:2 x   cid:3  Returns  cid:2 b = A cid:2 x, where  cid:3  A ∈ Rm×n and  cid:2 x ∈ Rn  cid:2 b ←  cid:2 0 for j ← 1, 2, . . . , n for i ← 1, 2, . . . , m bi ← bi + aijxj  return  cid:2 b   b    14  cid:4  Numerical Algorithms  Figure1.3 Two possible ways to store  a  a matrix in memory:  b  row-major ordering and  c  column-major ordering.  considerations related to computer architecture can make one of these options much faster than the other!  A reasonable model for the memory or RAM in a computer is as a long line of data. For this reason, we must ﬁnd ways to “unroll” data from matrix form to something that could be written completely horizontally. Two common patterns are illustrated in Figure 1.3:    A row-major ordering stores the data row-by-row; that is, the ﬁrst row appears in a  contiguous block of memory, then the second, and so on.    A column-major ordering stores the data column-by-column, moving vertically ﬁrst  rather than horizontally.  Consider the matrix multiplication method in Figure 1.2 a . This algorithm computes all of b1 before moving to b2, b3, and so on. In doing so, the code moves along the elements of A row-by-row. If A is stored in row-major order, then the algorithm in Figure 1.2 a  proceeds linearly across its representation in memory  Figure 1.3 b  , whereas if A is stored in column-major order  Figure 1.3 c  , the algorithm eﬀectively jumps around between elements in A. The opposite is true for the algorithm in Figure 1.2 b , which moves linearly through the column-major ordering.  In many hardware implementations, loading data from memory will retrieve not just the single requested value but instead a block of data near the request. The philosophy here is that common algorithms move linearly though data, processing it one element at a time, and anticipating future requests can reduce the communication load between the main processor and the RAM. By pairing, e.g., the algorithm in Figure 1.2 a  with the row-major ordering in Figure 1.3 b , we can take advantage of this optimization by moving linearly through the storage of the matrix A; the extra loaded data anticipates what will be needed in the next iteration. If we take a nonlinear traversal through A in memory, this situation is less likely, leading to a signiﬁcant loss in speed.  1.3.4 Model Problem: A cid:126 x =  cid:126 b  In introductory algebra class, students spend considerable time solving linear systems such as the following for triplets  x, y, z :  Our constructions in §1.3.1 allows us to encode such systems in a cleaner fashion:  3x + 2y + 5z = 0 −4x + 9y − 3z = −7 2x − 3y − 3z = 1.    3 −4  2 5 9 −3  2 −3 −3   x y  z  =  0 −7  1  .  A =⎛⎝  1 3 5  a   2 4  6 ⎞⎠  1 2 3 4 5 6  1 3 5 2 4 6   b  Row-major   c  Column-major   Mathematics Review  cid:4  15  More generally, we can write any linear system of equations in the form A cid:126 x =  cid:126 b by fol- lowing the same pattern above; here, the vector  cid:126 x is unknown while A and  cid:126 b are known. Such a system of equations is not always guaranteed to have a solution. For instance, if A contains only zeros, then no  cid:126 x will satisfy A cid:126 x =  cid:126 b whenever  cid:126 b  cid:54 =  cid:126 0. We will defer a general consideration of when a solution exists to our discussion of linear solvers in future chapters.  A key interpretation of the system A cid:126 x =  cid:126 b is that it addresses the task:  Write  cid:126 b as a linear combination of the columns of A.  Why? Recall from §1.3.1 that the product A cid:126 x encodes a linear combination of the columns of A with weights contained in elements of  cid:126 x. So, the equation A cid:126 x =  cid:126 b sets the linear combination A cid:126 x equal to the given vector  cid:126 b. Given this interpretation, we deﬁne the column space of A to be the space of right-hand sides  cid:126 b for which the system A cid:126 x =  cid:126 b has a solution:  Deﬁnition 1.9  Column space and rank . The column space of a matrix A ∈ Rm×n is the span of the columns of A. It can be written as  col A ≡ {A cid:126 x :  cid:126 x ∈ Rn}.  The rank of A is the dimension of col A. A cid:126 x =  cid:126 b is solvable exactly when  cid:126 b ∈ col A. One case will dominate our discussion in future chapters. Suppose A is square, so we can write A ∈ Rn×n. Furthermore, suppose that the system A cid:126 x =  cid:126 b has a solution for all choices of  cid:126 b, so by our interpretation above the columns of A must span Rn. In this case, we can substitute the standard basis  cid:126 e1, . . . ,  cid:126 en to solve equations of the form A cid:126 xi =  cid:126 ei, yielding vectors  cid:126 x1, . . . ,  cid:126 xn. Combining these  cid:126 xi’s horizontally into a matrix shows:  A    cid:126 x1     cid:126 x2    ···  cid:126 xn    = =    A cid:126 x1 A cid:126 x2      cid:126 e1     cid:126 e2    ··· A cid:126 xn     = In×n,   ···  cid:126 en   where In×n is the identity matrix from Example 1.11. We will call the matrix with columns  cid:126 xk the inverse A−1, which satisﬁes  AA−1 = A−1A = In×n.  By construction,  A−1 −1 = A. If we can ﬁnd such an inverse, solving any linear system A cid:126 x =  cid:126 b reduces to matrix multiplication, since:   cid:126 x = In×n cid:126 x =  A−1A  cid:126 x = A−1 A cid:126 x  = A−1 cid:126 b.  1.4 NON-LINEARITY: DIFFERENTIAL CALCULUS  While the beauty and applicability of linear algebra makes it a key target for study, non- linearities abound in nature, and we must design machinery that can deal with this reality.   16  cid:4  Numerical Algorithms  Figure 1.4 The closer we zoom into f  x  = x3 + x2 − 8x + 4, the more it looks like a  line. 1.4.1 Differentiation in One Variable  While many functions are globally nonlinear, locally they exhibit linear behavior. This idea of “local linearity” is one of the main motivators behind diﬀerential calculus. Figure 1.4 shows that if you zoom in close enough to a smooth function, eventually it looks like a line. The derivative f cid:48  x  of a function f  x  : R → R is the slope of the approximating line, computed by ﬁnding the slope of lines through closer and closer points to x:  f cid:48  x  = lim y→x  f  y  − f  x   .  y − x  In reality, taking limits as y → x may not be possible on a computer, so a reasonable question to ask is how well a function f  x  is approximated by a line through points that are a ﬁnite distance apart. We can answer these types of questions using inﬁnitesimal analysis. Take x, y ∈ R. Then, we can expand:  f  y  − f  x  = cid:90  y  x  f cid:48  t  dt by the Fundamental Theorem of Calculus  x  x  x  tf cid:48  cid:48  t  dt  tf cid:48  cid:48  t  dt  tf cid:48  cid:48  t  dt, after integrating by parts  again by the Fundamental Theorem of Calculus  = yf cid:48  y  − xf cid:48  x  − cid:90  y =  y − x f cid:48  x  + y f cid:48  y  − f cid:48  x   − cid:90  y =  y − x f cid:48  x  + y cid:90  y f cid:48  cid:48  t  dt − cid:90  y =  y − x f cid:48  x  + cid:90  y  y − t f cid:48  cid:48  t  dt. f cid:48  x ∆x − [f  y  − f  x ] = cid:12  cid:12  cid:12  cid:12  cid:90  y ≤ ∆x cid:90  y ≤ D∆x2, assuming f cid:48  cid:48  t    0. We can introduce some notation to help express the relationship we have written:   y − t f cid:48  cid:48  t  dt cid:12  cid:12  cid:12  cid:12  from the relationship above  Rearranging terms and deﬁning ∆x ≡ y − x shows:  x  x  x  x f cid:48  cid:48  t  dt, by the Cauchy-Schwarz inequality    x   f  100  50  0 −50  15 10 5 0  10  5  −4−2 0 2 4  x  −2 −1 0 x  1  2  0 −1−0.5 0 0.5 1  x   Mathematics Review  cid:4  17  Figure 1.5 Big-O notation; in the ε neighborhood of the origin, f  x  is dominated by Cg x ; outside this neighborhood, Cg x  can dip back down.  Deﬁnition 1.10  Inﬁnitesimal big-O . We will say f  x  = O g x   if there exists a constant C > 0 and some ε > 0 such that f  x  ≤ Cg x  for all x with x < ε. This deﬁnition is illustrated in Figure 1.5. Computer scientists may be surprised to see that we are deﬁning “big-O notation” by taking limits as x → 0 rather than x → ∞, but since we are concerned with inﬁnitesimal approximation quality, this deﬁnition will be more relevant to the discussion at hand.  Our derivation above shows the following relationship for smooth functions f : R → R:  f  x + ∆x  = f  x  + f cid:48  x ∆x + O ∆x2 .  This is an instance of Taylor’s theorem, which we will apply copiously when developing strategies for integrating ordinary diﬀerential equations. More generally, this theorem shows how to approximate diﬀerentiable functions with polynomials:  f  x + ∆x  = f  x  + f cid:48  x ∆x + f cid:48  cid:48  x   ∆x2 2!  + ··· + f  k  x   ∆xk k!  + O ∆xk+1 .  1.4.2 Differentiation in Multiple Variables If a function f takes multiple inputs, then it can be written f   cid:126 x  : Rn → R for  cid:126 x ∈ Rn. In other words, to each point  cid:126 x =  x1, . . . , xn  in n-dimensional space, f assigns a single number f  x1, . . . , xn .  The idea of local linearity must be repaired in this case, because lines are one- rather than n-dimensional objects. Fixing all but one variable, however, brings a return to single- variable calculus. For instance, we could isolate x1 by studying g t  ≡ f  t, x2, . . . , xn , where we think of x2, . . . , xn as constants. Then, g t  is a diﬀerentiable function of a single variable that we can characterize using the machinery in §1.4.1. We can do the same for any of the xk’s, so in general we make the following deﬁnition of the partial derivative of f :  Deﬁnition 1.11  Partial derivative . The k-th partial derivative of f , notated ∂f ∂xk given by diﬀerentiating f in its k-th input variable:  , is  ∂f ∂xk   x1, . . . , xn  ≡  f  x1, . . . , xk−1, t, xk+1, . . . , xn t=xk .  d dt  f  x   Cg x   ε  ε  x   18  cid:4  Numerical Algorithms  Figure 1.6 We can visualize a function f  x1, x2  as a three-dimensional graph; then ∇f   cid:126 x  is the direction on the  x1, x2  plane corresponding to the steepest ascent of f . Alternatively, we can think of f  x1, x2  as the brightness at  x1, x2   dark indicates a low value of f  , in which case ∇f points perpendicular to level sets f   cid:126 x  = c in the direction where f is increasing and the image gets lighter.  The notation used in this deﬁnition and elsewhere in our discussion “t=xk ” should be read as “evaluated at t = xk.”  Example 1.17  Relativity . The relationship E = mc2 can be thought of as a function mapping pairs  m, c  to a scalar E. Thus, we could write E m, c  = mc2, yielding the partial derivatives  ∂E ∂m  = c2  ∂E ∂c  = 2mc.  Using single-variable calculus, for a function f : Rn → R,  f   cid:126 x + ∆ cid:126 x  = f  x1 + ∆x1, x2 + ∆x2, . . . , xn + ∆xn  ∂f ∂x1 by single-variable calculus in x1  = f  x1, x2 + ∆x2, . . . , xn + ∆xn  +  ∆x1 + O ∆x2 1   = f  x1, . . . , xn  +  ∆xk + O ∆x2  n cid:88 k=1 cid:20  ∂f  ∂xk  k  cid:21   by repeating this n − 1 times in x2, . . . , xn  = f   cid:126 x  + ∇f   cid:126 x  · ∆ cid:126 x + O  cid:107 ∆ cid:126 x cid:107 2 2 , ∇f   cid:126 x  ≡ cid:18  ∂f    cid:126 x ,··· ,  ∂f ∂xn  ∂f ∂x2  ∂x1    cid:126 x ,    cid:126 x  cid:19  ∈ Rn.  where we deﬁne the gradient of f as  Figure 1.6 illustrates interpretations of the gradient of a function, which we will reconsider in our discussion of optimization in future chapters.  We can diﬀerentiate f in any direction  cid:126 v via the directional derivative D cid:126 vf :  D cid:126 vf   cid:126 x  ≡  f   cid:126 x + t cid:126 v t=0 = ∇f   cid:126 x  ·  cid:126 v.  d dt  We allow  cid:126 v to have any length, with the property Dc cid:126 vf   cid:126 x  = cD cid:126 vf   cid:126 x .  f  x1, x2   x2 x1    cid:2 x, f   cid:2 x     cid:2 x ∇f   cid:2 x   Graph of f   cid:2 x   Steepest ascent  Level sets of f   cid:2 x   f   cid:2 x  = c  ∇f   cid:2 x   x2  x1   Mathematics Review  cid:4  19  Example 1.18  R2 . Take f  x, y  = x2y3. Then,  ∂f ∂x  = 2xy3  ∂f ∂y  = 3x2y2.  Equivalently, ∇f  x, y  =  2xy3, 3x2y2 . So, the derivative of f at  x, y  =  1, 2  in the direction  −1, 4  is given by  −1, 4  · ∇f  1, 2  =  −1, 4  ·  16, 12  = 32.  There are a few derivatives that we will use many times. These formulae will appear  repeatedly in future chapters and are worth studying independently:  Example 1.19  Linear functions . It is obvious but worth noting that the gradient of f   cid:126 x  ≡  cid:126 a ·  cid:126 x +  cid:126 c =  a1x1 + c1, . . . , anxn + cn  is  cid:126 a. Example 1.20  Quadratic forms . Take any matrix A ∈ Rn×n, and deﬁne f   cid:126 x  ≡  cid:126 x cid:62 A cid:126 x. Writing this function element-by-element shows  f   cid:126 x  = cid:88 ij  Aijxixj.  Expanding f and checking this relationship explicitly is worthwhile. Take some k ∈ {1, . . . , n}. Then, we can separate out all terms containing xk:  With this factorization,  f   cid:126 x  = Akkx2  k + xk cid:88 i cid:54 =k = 2Akkxk + cid:88 i cid:54 =k  ∂f ∂xk  Aikxi + cid:88 j cid:54 =k  Akjxj +  cid:88 i,j cid:54 =k Akjxj = n cid:88 i=1  Aikxi + cid:88 j cid:54 =k  Aijxixj.   Aik + Aki xi.  This sum looks a lot like the deﬁnition of matrix-vector multiplication! Combining these partial derivatives into a single vector shows ∇f   cid:126 x  =  A + A cid:62   cid:126 x. In the special case when A is symmetric, that is, when A cid:62  = A, we have the well-known formula ∇f   cid:126 x  = 2A cid:126 x.  We generalized diﬀerentiation from f : R → R to f : Rn → R. To reach full generality, we should consider f : Rn → Rm. That is, f inputs n numbers and outputs m numbers. This extension is straightforward, because we can think of f as a collection of single-valued functions f1, . . . , fm : Rn → R smashed into a single vector. Symbolically,  f   cid:126 x  =  f1  cid:126 x  f2  cid:126 x   ...  fm  cid:126 x   .    Each fk can be diﬀerentiated as before, so in the end we get a matrix of partial derivatives called the Jacobian of f :   20  cid:4  Numerical Algorithms  Deﬁnition 1.12  Jacobian . The Jacobian of f : Rn → Rm is the matrix Df   cid:126 x  ∈ Rm×n with entries  Example 1.21  Jacobian computation . Suppose f  x, y  =  3x,−xy2, x + y . Then,   Df  ij ≡  ∂fi ∂xj  .  Df  x, y  =  3 0 −y2 −2xy 1  1  .  Example 1.22  Matrix multiplication . Unsurprisingly, the Jacobian of f   cid:126 x  = A cid:126 x for matrix A is given by Df   cid:126 x  = A.  Here, we encounter a common point of confusion. Suppose a function has vector input and scalar output, that is, f : Rn → R. We deﬁned the gradient of f as a column vector, so to align this deﬁnition with that of the Jacobian we must write Df = ∇f cid:62 . 1.4.3 Optimization  A key problem in the study of numerical algorithms is optimization, which involves ﬁnding points at which a function f   cid:126 x  is maximized or minimized. A wide variety of computational challenges can be posed as optimization problems, also known as variational problems, and hence this language will permeate our derivation of numerical algorithms. Generally speaking, optimization problems involve ﬁnding extrema of a function f   cid:126 x , possibly subject to constraints specifying which points  cid:126 x ∈ Rn are feasible. Recalling physical systems that naturally seek low- or high-energy conﬁgurations, f   cid:126 x  is sometimes referred to as an energy or objective.  From single-variable calculus, the minima and maxima of f : R → R must occur at points x satisfying f cid:48  x  = 0. This condition is necessary rather than suﬃcient: there may exist saddle points x with f cid:48  x  = 0 that are not maxima or minima. That said, ﬁnding such critical points of f can be part of a function minimization algorithm, so long as a subsequent step ensures that the resulting x is actually a minimum maximum.  If f : Rn → R is minimized or maximized at  cid:126 x, we have to ensure that there does not exist a single direction ∆x from  cid:126 x in which f decreases or increases, respectively. By the discussion in §1.4.1, this means we must ﬁnd points for which ∇f = 0. Example 1.23  Critical points . Suppose f  x, y  = x2 + 2xy + 4y2. Then, ∂f and ∂f  ∂y = 2x + 8y. Thus, critical points of f satisfy:  ∂x = 2x + 2y  2x + 2y = 0  and  2x + 8y = 0.  This system is solved by taking  x, y  =  0, 0 . Indeed, this is the minimum of f , as can be seen by writing f  x, y  =  x + y 2 + 3y2 ≥ 0 = f  0, 0 . Example 1.24  Quadratic functions . Suppose f   cid:126 x  =  cid:126 x cid:62 A cid:126 x +  cid:126 b cid:62  cid:126 x + c. Then, from Examples 1.19 and 1.20 we can write ∇f   cid:126 x  =  A cid:62  + A  cid:126 x +  cid:126 b. Thus, critical points  cid:126 x of f satisfy  A cid:62  + A  cid:126 x +  cid:126 b = 0.   Mathematics Review  cid:4  21  Figure 1.7 Three rectangles with the same perimeter 2w + 2h but unequal areas wh; the square on the right with w = h maximizes wh over all possible choices with prescribed 2w + 2h = 1.  Unlike single-variable calculus, on Rn we can add nontrivial constraints to our optimiza-  tion. For now, we will consider the equality-constrained case, given by  When we add the constraint g  cid:126 x  = 0, we no longer expect that minimizers  cid:126 x satisfy ∇f   cid:126 x  = 0, since these points might not satisfy g  cid:126 x  =  cid:126 0. Example 1.25  Rectangle areas . Suppose a rectangle has width w and height h. A classic geometry problem is to maximize area with a ﬁxed perimeter 1:  minimize f   cid:126 x  subject to g  cid:126 x  =  cid:126 0.  maximize wh subject to 2w + 2h − 1 = 0.  This problem is illustrated in Figure 1.7.  For now, suppose g : Rn → R, so we only have one equality constraint; an example for n = 2 is shown in Figure 1.8. We deﬁne the set of points satisfying the equality constraint as S0 ≡ { cid:126 x : g  cid:126 x  = 0}. Any two  cid:126 x,  cid:126 y ∈ S0 satisfy the relationship g  cid:126 y  − g  cid:126 x  = 0 − 0 = 0. Applying Taylor’s theorem, if  cid:126 y =  cid:126 x + ∆ cid:126 x for small ∆ cid:126 x, then g  cid:126 y  − g  cid:126 x  = ∇g  cid:126 x  · ∆ cid:126 x + O  cid:107 ∆ cid:126 x cid:107 2 2 .  In other words, if g  cid:126 x  = 0 and ∇g  cid:126 x  · ∆ cid:126 x = 0, then g  cid:126 x + ∆ cid:126 x  ≈ 0. If  cid:126 x is a minimum of the constrained optimization problem above, then any small dis- placement  cid:126 x to  cid:126 x +  cid:126 v still satisfying the constraints should cause an increase from f   cid:126 x  to f   cid:126 x +  cid:126 v . On the inﬁnitesimal scale, since we only care about displacements  cid:126 v preserving the g  cid:126 x + cid:126 v  = c constraint, from our argument above we want ∇f · cid:126 v = 0 for all  cid:126 v satisfying ∇g  cid:126 x  ·  cid:126 v = 0. In other words, ∇f and ∇g must be parallel, a condition we can write as ∇f = λ∇g for some λ ∈ R, illustrated in Figure 1.8 c . Λ  cid:126 x, λ  ≡ f   cid:126 x  − λg  cid:126 x .  Deﬁne  Then, critical points of Λ without constraints satisfy:  ∂Λ = −g  cid:126 x  = 0, by the constraint g  cid:126 x  = 0. ∂λ ∇ cid:126 xΛ = ∇f   cid:126 x  − λ∇g  cid:126 x  = 0, as argued above.  In other words, critical points of Λ with respect to both λ and  cid:126 x satisfy g  cid:126 x  = 0 and ∇f   cid:126 x  = λ∇g  cid:126 x , exactly the optimality conditions we derived!  h  w  h  h  w  w   22  cid:4  Numerical Algorithms  Figure 1.8  a  An equality-constrained optimization. Without constraints, f   cid:126 x  is minimized at the star; solid lines show isocontours f   cid:126 x  = c for increasing c. Mini- mizing f   cid:126 x  subject to g  cid:126 x  = 0 forces  cid:126 x to be on the dashed curve.  b  The point  cid:126 x is suboptimal since moving in the ∆ cid:126 x direction decreases f   cid:126 x  while maintaining g  cid:126 x  = 0.  c  The point  cid:126 q is optimal since decreasing f from f   cid:126 q  would require moving in the −∇f direction, which is perpendicular to the curve g  cid:126 x  = 0.  Extending our argument to g : Rn → Rk yields the following theorem:  Theorem 1.1  Method of Lagrange multipliers . Critical points of the equality- constrained optimization problem above are  unconstrained  critical points of the Lagrange multiplier function  with respect to both  cid:126 x and  cid:126 λ.  Λ  cid:126 x,  cid:126 λ  ≡ f   cid:126 x  −  cid:126 λ · g  cid:126 x ,  Some treatments of Lagrange multipliers equivalently use the opposite sign for  cid:126 λ; considering ¯Λ  cid:126 x,  cid:126 λ  ≡ f   cid:126 x  +  cid:126 λ · g  cid:126 x  leads to an analogous result above. This theorem provides an analog of the condition ∇f   cid:126 x  =  cid:126 0 when equality constraints g  cid:126 x  =  cid:126 0 are added to an optimization problem and is a cornerstone of variational algo- rithms we will consider. We conclude with a number of examples applying this theorem; understanding these examples is crucial to our development of numerical methods in future chapters.  Example 1.26  Maximizing area . Continuing Example 1.25, we deﬁne the Lagrange multiplier function Λ w, h, λ  = wh − λ 2w + 2h − 1 . Diﬀerentiating Λ with respect to w, h, and λ provides the following optimality conditions:  0 =  ∂Λ ∂w  = h − 2λ  0 =  ∂Λ ∂h  = w − 2λ  0 =  ∂Λ ∂λ  = 1 − 2w − 2h.  So, critical points of the area wh under the constraint 2w + 2h = 1 satisfy    0 1 −2 1 0 −2 2 2  0   w h  λ  =  0 0  1  .  Solving the system shows w = h = 1 4  and λ = 1 8 . In other words, for a ﬁxed amount of perimeter, the rectangle with maximal area is a square.  g  cid:2 x  = 0   cid:2 x  Δ cid:2 x  f  cid:2 x =c   cid:2 q  ∇f   a  Constrained optimization   b  Suboptimal  cid:2 x   c  Optimal  cid:2 q   Mathematics Review  cid:4  23  Example 1.27  Eigenproblems . Suppose that A is a symmetric positive deﬁnite matrix, meaning A cid:62  = A  symmetric  and  cid:126 x cid:62 A cid:126 x > 0 for all  cid:126 x ∈ Rn\{ cid:126 0}  positive deﬁnite . We 2 = 1 for a given matrix A ∈ Rn×n; without the may wish to minimize  cid:126 x cid:62 A cid:126 x subject to  cid:107  cid:126 x cid:107 2 constraint the function is minimized at  cid:126 x =  cid:126 0. We deﬁne the Lagrange multiplier function  Λ  cid:126 x, λ  =  cid:126 x cid:62 A cid:126 x − λ  cid:107  cid:126 x cid:107 2  2 − 1  =  cid:126 x cid:62 A cid:126 x − λ  cid:126 x cid:62  cid:126 x − 1 .  Diﬀerentiating with respect to  cid:126 x, we ﬁnd 0 = ∇ cid:126 xΛ = 2A cid:126 x − 2λ cid:126 x. In other words, critical points of  cid:126 x are exactly the eigenvectors of the matrix A:  A cid:126 x = λ cid:126 x, with  cid:107  cid:126 x cid:107 2  2 = 1.  At these critical points, we can evaluate the objective function as  cid:126 x cid:62 A cid:126 x =  cid:126 x cid:62 λ cid:126 x = λ cid:107  cid:126 x cid:107 2 2 = λ. Hence, the minimizer of  cid:126 x cid:62 A cid:126 x subject to  cid:107  cid:126 x cid:107 2 2 = 1 is the eigenvector  cid:126 x with minimum eigenvalue λ; we will provide practical applications and solution techniques for this opti- mization problem in detail in Chapter 6.  1.5 EXERCISES SC 1.1 Illustrate the gradients of f  x, y  = x2 + y2 and g x, y  = cid:112 x2 + y2 on the plane, and  show that  cid:107 ∇g x, y  cid:107 2 is constant away from the origin.  DH 1.2 Compute the dimensions of each of the following sets:  span{ 1, 1, 1 ,  1,−1, 1 ,  −1, 1, 1 ,  1, 1,−1 } span{ 2, 7, 9 ,  3, 5, 1 ,  0, 1, 0 }  1.3 Which of the following functions is linear? Why?   b    a  col  d  col   c   1 0 0 0 1 0  0 0 0  0 0 1   1 1 0 1 1 0   a  f  x, y, z  = 0   b  f  x, y, z  = 1   c  f  x, y, z  =  1 + x, 2z    d  f  x  =  x, 2x    e  f  x, y  =  2x + 3y, x, 0   1.4 Suppose that U1 and U2 are subspaces of vector space V. Show that U1 ∩ U2 is a  subspace of V. Is U1 ∪ U2 always a subspace of V?  1.5 Suppose A, B ∈ Rn×n and  cid:126 a, cid:126 b ∈ Rn. Find a  nontrivial  linear system of equations  satisﬁed by any  cid:126 x minimizing the energy  cid:107 A cid:126 x −  cid:126 a cid:107 2  2 +  cid:107 B cid:126 x −  cid:126 b cid:107 2 2.   24  cid:4  Numerical Algorithms  1.6 Take C 1 R  to be the set of continuously diﬀerentiable functions f : R → R. Why is  C 1 R  a vector space? Show that C 1 R  has dimension ∞.  1.7 Suppose the rows of A ∈ Rm×n are given by the transposes of  cid:126 r1, . . . ,  cid:126 rm ∈ Rn and  the columns of A ∈ Rm×n are given by  cid:126 c1, . . . ,  cid:126 cn ∈ Rm. That is,  A =  −  cid:126 r cid:62 1 − −  cid:126 r cid:62 2 −... −  cid:126 r cid:62 m −    =     cid:126 c1  cid:126 c2     ···  cid:126 cn    .  Give expressions for the elements of A cid:62 A and AA cid:62  in terms of these vectors.  1.8 Give a linear system of equations satisﬁed by minima of the energy f   cid:126 x  =  cid:107 A cid:126 x− cid:126 b cid:107 2  with respect to  cid:126 x, for  cid:126 x ∈ Rn, A ∈ Rm×n, and  cid:126 b ∈ Rm.  1.9 Suppose A, B ∈ Rn×n. Formulate a condition for vectors  cid:126 x ∈ Rn to be critical points of  cid:107 A cid:126 x cid:107 2 subject to  cid:107 B cid:126 x cid:107 2 = 1. Also, give an alternative expression for the value of  cid:107 A cid:126 x cid:107 2 at these critical points, in terms a Lagrange multiplier for this optimization problem.  1.10 Fix some vector  cid:126 a ∈ Rn\{ cid:126 0} and deﬁne f   cid:126 x  =  cid:126 a ·  cid:126 x. Give an expression for the  maximum of f   cid:126 x  subject to  cid:107  cid:126 x cid:107 2 = 1.  1.11 Suppose A ∈ Rn×n is symmetric, and deﬁne the Rayleigh quotient function R  cid:126 x  as  R  cid:126 x  ≡   cid:126 x cid:62 A cid:126 x  cid:107  cid:126 x cid:107 2  2  .  Show that minimizers of R  cid:126 x  subject to  cid:126 x  cid:54 =  cid:126 0 are eigenvectors of A.  1.12 Show that  A cid:62  −1 =  A−1  cid:62  when A ∈ Rn×n is invertible. If B ∈ Rn×n is also  invertible, show  AB −1 = B−1A−1.  1.13 Suppose A t  is a function taking a parameter t and returning an invertible square matrix A t  ∈ Rn×n; we can write A : R → Rn×n. Assuming each element aij t  of A t  is a diﬀerentiable function of t, deﬁne the derivative matrix dA dt  t  as the matrix whose elements are daij  dt  t . Verify the following identity:  = −A−1 dA Hint: Start from the identity A−1 t  · A t  = In×n.  dt  dt  d A−1   A−1.  1.14 Derive the following relationship stated in §1.4.2:  d dt  f   cid:126 x + t cid:126 v t=0 = ∇f   cid:126 x  ·  cid:126 v.  1.15 A matrix A ∈ Rn×n is idempotent if it satisﬁes A2 = A.   a  Suppose B ∈ Rm×k is constructed so that B cid:62 B is invertible. Show that the  matrix B B cid:62 B −1B cid:62  is idempotent.   Mathematics Review  cid:4  25   b    c   If A is idempotent, show that In×n − A is also idempotent. If A is idempotent, show that 1 its inverse.  2 In×n − A is invertible and give an expression for   d  Suppose A is idempotent and that we are given  cid:126 x  cid:54 =  cid:126 0 and λ ∈ R satisfying  A cid:126 x = λ cid:126 x. Show that λ ∈ {0, 1}.  1.16 Show that it takes at least O n2  time to ﬁnd the product AB of two matrices A, B ∈ Rn×n. What is the runtime of the algorithms in Figure 1.2? Is there room for improvement?  1.17  “Laplace approximation,” [13]  Suppose p  cid:126 x  : Rn → [0, 1] is a probability distribu-  tion, meaning that p  cid:126 x  ≥ 0 for all  cid:126 x ∈ Rn and   cid:90 Rn  p  cid:126 x  d cid:126 x = 1.  In this problem, you can assume p  cid:126 x  is inﬁnitely diﬀerentiable.  One important type of probability distribution is the Gaussian distribution, also known as the normal distribution, which takes the form  GΣ, cid:126 µ  cid:126 x  ∝ e− 1  2   cid:126 x− cid:126 µ  cid:62 Σ−1  cid:126 x− cid:126 µ .  Here, f   cid:126 x  ∝ g  cid:126 x  denotes that there exists some c ∈ R such that f   cid:126 x  = cg  cid:126 x  for all  cid:126 x ∈ Rn. The covariance matrix Σ ∈ Rn×n and mean  cid:126 µ ∈ Rn determine the particular bell shape of the Gaussian distribution. Suppose  cid:126 x∗ ∈ Rn is a mode, or local maximum, of p  cid:126 x . Propose a Gaussian approxi- mation of p  cid:126 x  in a neighborhood of  cid:126 x∗.  Hint: Consider the negative log likelihood function, given by  cid:96   cid:126 x  ≡ − ln p  cid:126 x .    C H A P T E R2 Numerics and Error Analysis  CONTENTS  2.1  2.2  2.3  Storing Numbers with Fractional Parts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.1.1 Fixed-Point Representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.1.2 Floating-Point Representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.1.3 More Exotic Options . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Understanding Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2.1 Classifying Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2.2 Conditioning, Stability, and Accuracy . . . . . . . . . . . . . . . . . . . . . . . . . . Practical Aspects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3.1 Computing Vector Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3.2 Larger-Scale Example: Summation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  27 28 29 31 32 33 35 36 37 38  N umerical analysis introduces a shift from working with ints and longs to floats and  doubles. This seemingly innocent transition shatters intuition from integer arithmetic, requiring adjustment of how we must think about basic algorithmic design and implemen- tation. Unlike discrete algorithms, numerical algorithms cannot always yield exact solutions even to well-studied and well-posed problems. Operation counting no longer reigns supreme; instead, even basic techniques require careful analysis of the trade-oﬀs among timing, ap- proximation error, and other considerations. In this chapter, we will explore the typical factors aﬀecting the quality of a numerical algorithm. These factors set numerical algo- rithms apart from their discrete counterparts.  2.1 STORING NUMBERS WITH FRACTIONAL PARTS  Most computers store data in binary format. In binary, integers are decomposed into powers of two. For instance, we can convert 463 to binary using the following table:  This table illustrates the fact that 463 has a unique decomposition into powers of two as:  1 28  1 27  1 26  0 25  0 24  1 23  1 22  1 21  1 20  463 = 256 + 128 + 64 + 8 + 4 + 2 + 1  = 28 + 27 + 26 + 23 + 22 + 21 + 20.  All positive integers can be written in this form. Negative numbers also can be represented either by introducing a leading sign bit  e.g., 1 for “positive” and 0 for “negative”  or by using a “two’s complement” trick.  The binary system admits an extension to numbers with fractional parts by including  negative powers of two. For instance, 463.25 can be decomposed by adding two slots:  27   28  cid:4  Numerical Algorithms  1 28  1 27  1 26  0 25  0 24  1 23  1 22  1 21  1. 20  0 2−1  1 2−2  Representing fractional parts of numbers this way, however, is not nearly as well-behaved as representing integers. For instance, writing the fraction 1 3 in binary requires inﬁnitely many digits:  1 3  = 0.0101010101···2 .  There exist numbers at all scales that cannot be represented using a ﬁnite binary string. In fact, all irrational numbers, like π = 11.00100100001 . . .2, have inﬁnitely long expansions regardless of which  integer  base you use!  Since computers have a ﬁnite amount of storage capacity, systems processing values in R instead of Z are forced to approximate or restrict values that can be processed. This leads to many points of confusion while coding, as in the following snippet of C++ code:  double x = 1.0; double y = x   3.0; if   x == y *3.0  cout << " They are equal ! " ; else cout << " They are NOT equal . " ;  Contrary to intuition, this program prints "They are NOT equal." Why? Since 1 3 cannot be written as a ﬁnite-length binary string, the deﬁnition of y makes an approximation, rounding to the nearest number representable in the double data type. Thus, y*3.0 is close to but not exactly 1. One way to ﬁx this issue is to allow for some tolerance:  double x = 1.0; double y = x   3.0; if   fabs  x - y *3.0   :: epsilon    cout << " They are equal ! " ;  else cout << " They are NOT equal . " ;  Here, we check that x and y*3.0 are near enough to each other to be reasonably considered identical rather than whether they are exactly equal. The tolerance epsilon expresses how far apart values should be before we are conﬁdent they are diﬀerent. It may need to be adjusted depending on context. This example raises a crucial point:  Rarely if ever should the operator == and its equivalents be used on fractional values. Instead, some tolerance should be used to  check if they are equal.  There is a trade-oﬀ here: the size of the tolerance deﬁnes a line between equality and “close- but-not-the-same,” which must be chosen carefully for a given application.  The error generated by a numerical algorithm depends on the choice of representations for real numbers. Each representation has its own compromise among speed, accuracy, range of representable values, and so on. Keeping the example above and its resolution in mind, we now consider a few options for representing numbers discretely.  2.1.1 Fixed-Point Representations  The most straightforward way to store fractions is to use a ﬁxed decimal point. That is, as in the example above, we represent values by storing 0-or-1 coeﬃcients in front of powers of two that range from 2−k to 2 cid:96  for some k,  cid:96  ∈ Z. For instance, representing all nonnegative values between 0 and 127.75 in increments of 1 4 can be accomplished by taking k = 2 and  cid:96  = 7; in this case, we use 10 binary digits total, of which two occur after the decimal point.   Numerics and Error Analysis  cid:4  29  The primary advantage of this representation is that many arithmetic operations can be carried out using the same machinery already in place for integers. For example, if a and b are written in ﬁxed-point format, we can write:  a + b =  a · 2k + b · 2k  · 2−k.  The values a·2k and b·2k are integers, so the summation on the right-hand side is an integer operation. This observation essentially shows that ﬁxed-point addition can be carried out using integer addition essentially by “ignoring” the decimal point. In this way, rather than needing specialized hardware, the preexisting integer arithmetic logic unit  ALU  can carry out ﬁxed-point mathematics quickly.  Fixed-point arithmetic may be fast, but it suﬀers from serious precision issues. In partic- ular, it is often the case that the output of a binary operation like multiplication or division can require more bits than the operands. For instance, suppose we include one decimal point of precision and wish to carry out the product 1 2 · 1 2 = 1 4. We write 0.12 × 0.12 = 0.012, which gets truncated to 0. More broadly, it is straightforward to combine ﬁxed-point num- bers in a reasonable way and get an unreasonable result.  Due to these drawbacks, most major programming languages do not by default include a ﬁxed-point data type. The speed and regularity of ﬁxed-point arithmetic, however, can be a considerable advantage for systems that favor timing over accuracy. Some lower-end graphics processing units  GPU  implement only ﬁxed-point operations since a few decimal points of precision are suﬃcient for many graphical applications.  2.1.2 Floating-Point Representations  One of many numerical challenges in scientiﬁc computing is the extreme range of scales that can appear. For example, chemists deal with values anywhere between 9.11 × 10−31  the mass of an electron in kilograms  and 6.022 × 1023  the Avogadro constant . An operation as innocent as a change of units can cause a sudden transition between scales: The same observation written in kilograms per lightyear will look considerably diﬀerent in megatons per mile. As numerical analysts, we are charged with writing software that can transition gracefully between these scales without imposing unnatural restrictions on the client.  Scientists deal with similar issues when recording experimental measurements, and their methods can motivate our formats for storing real numbers on a computer. Most promi- nently, one of the following representations is more compact than the other:  6.022 × 1023 = 602, 200, 000, 000, 000, 000, 000, 000.  Not only does the representation on the left avoid writing an unreasonable number of zeros, but it also reﬂects the fact that we may not know Avogadro’s constant beyond the second 2.  In the absence of exceptional scientiﬁc equipment, the diﬀerence between 6.022 × 1023 and 6.022 × 1023 + 9.11 × 10−31 likely is negligible, in the sense that this tiny perturbation is dwarfed by the error of truncating 6.022 to three decimal points. More formally, we say that 6.022 × 1023 has only four digits of precision and probably represents some range of possible measurements [6.022 × 1023 − ε, 6.022 × 1023 + ε] for some ε ≈ 0.001 × 1023. Our ﬁrst observation allowed us to shorten the representation of 6.022× 1023 by writing it in scientiﬁc notation. This number system separates the “interesting” digits of a number from its order of magnitude by writing it in the form a × 10e for some a ∼ 1 and e ∈ Z. We call this format the ﬂoating-point form of a number, because unlike the ﬁxed-point setup in   30  cid:4  Numerical Algorithms  Figure2.1 The values from Example 2.1 plotted on a number line; typical for ﬂoating- point number systems, they are unevenly spaced between the minimum  0.5  and the maximum  3.5 .  §2.1.1, the decimal point “ﬂoats” so that a is on a reasonable scale. Usually a is called the signiﬁcand and e is called the exponent.  Floating-point systems are deﬁned using three parameters:   The base or radix b ∈ N. For scientiﬁc notation explained above, the base is b = 10;  for binary systems the base is b = 2.    The precision p ∈ N representing the number of digits used to store the signiﬁcand.   The range of exponents [L, U ] representing the allowable values for e.  The expansion looks like:  ±ﬁ  sign  ·„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„‚„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„¶   d0 + d1 · b−1 + d2 · b−2 + ··· + dp−1 · b1−p   signiﬁcand  × beﬁ  exponent  ,  where each digit dk is in the range [0, b − 1] and e ∈ [L, U ]. When b = 2, an extra bit of precision can be gained by normalizing ﬂoating-point values and assuming the most signiﬁcant digit d0 is one; this change, however, requires special treatment of the value 0.  Floating-point representations have a curious property that can aﬀect software in un- expected ways: Their spacing is uneven. For example, the number of values representable between b and b2 is the same as that between b2 and b3 even though usually b3− b2 > b2− b. To understand the precision possible with a given number system, we will deﬁne the ma- chine precision εm as the smallest εm > 0 such that 1 + εm is representable. Numbers like b + εm are not expressible in the number system because εm is too small. Example 2.1  Floating-point . Suppose we choose b = 2, L = −1, and U = 1. If we choose to use three digits of precision, we might choose to write numbers in the form  1. cid:3  cid:3  × 2   cid:3   .  Notice this number system does not include 0. The possible signiﬁcands are 1.002 = 110, 1.012 = 1.2510, 1.102 = 1.510, and 1.112 = 1.7510. Since L = −1 and U = 1, these signiﬁcands can be scaled by 2−1 = 0.510, 20 = 110, and 21 = 210. With this information in hand, we can list all the possible values in our number system:  Signiﬁcand  1.0010 1.2510 1.5010 1.7510  ×2−1 0.50010 0.62510 0.75010 0.87510  ×20 1.00010 1.25010 1.50010 1.75010  ×21 2.00010 2.50010 3.00010 3.50010  These values are plotted in Figure 2.1; as expected, they are unevenly spaced and bunch toward zero. Also, notice the gap between 0 and 0.5 in this sampling of values; some  0  0.5  1  1.5  2  2.5  3  3.5  0.625 0.75 0.875  1.25  1.75   Numerics and Error Analysis  cid:4  31  number systems introduce evenly spaced subnormal values to ﬁll in this gap, albeit with less precision. Machine precision for this number system is εm = 0.25, the smallest displacement possible above 1.  By far the most common format for storing ﬂoating-point numbers is provided by the IEEE 754 standard. This standard speciﬁes several classes of ﬂoating-point numbers. For instance, a double-precision ﬂoating-point number is written in base b = 2  as are all numbers in this format , with a single ± sign bit, 52 digits for d, and a range of exponents between −1022 and 1023. The standard also speciﬁes how to store ±∞ and values like NaN, or “not-a-number,” reserved for the results of computations like 10 0. IEEE 754 also includes agreed-upon conventions for rounding when an operation results in a number not represented in the standard. For instance, a common unbiased strategy for rounding computations is round to nearest, ties to even, which breaks equidistant ties by rounding to the nearest ﬂoating-point value with an even least-signiﬁcant  rightmost  bit. There are many equally legitimate strategies for rounding; agreeing upon a single one guarantees that scientiﬁc software will work identically on all client machines regardless of their particular processor or compiler.  2.1.3 More Exotic Options  For most of this book, we will assume that fractional values are stored in ﬂoating-point format unless otherwise noted. This, however, is not to say that other numerical systems do not exist, and for speciﬁc applications an alternative choice might be necessary. We acknowledge some of those situations here.  The headache of inexact arithmetic to account for rounding errors might be unacceptable for some applications. This situation appears in computational geometry, e.g., when the diﬀerence between nearly and completely parallel lines may be a diﬃcult distinction to make. One solution might be to use arbitrary-precision arithmetic, that is, to implement fractional arithmetic without rounding or error of any sort.  Arbitrary-precision arithmetic requires a specialized implementation and careful consid- eration for what types of values you need to represent. For instance, it might be the case that rational numbers Q, which can be written as ratios a b for a, b ∈ Z, are suﬃcient for a given application. Basic arithmetic can be carried out in Q without any loss in precision, as follows:  a b ×  c d  =  ac bd  a b ÷  c d  =  ad bc  .  Arithmetic in the rationals precludes the existence of a square root operator, since values like √2 are irrational. Also, this representation is nonunique since, e.g., a b = 5a 5b, and thus certain operations may require additional routines for simplifying fractions. Even after simplifying, after a few multiplies and adds, the numerator and denominator may require many digits of storage, as in the following sum:  1 100  +  1 101  +  1 102  +  1 103  +  1 104  +  1 105  =  188463347 3218688200  .  In other situations, it may be useful to bracket error by representing values alongside error estimates as a pair a, ε ∈ R; we think of the pair  a, ε  as the range a ± ε. Then, arithmetic operations also update not only the value but also the error estimate, as in   x ± ε1  +  y ± ε2  =  x + y  ±  ε1 + ε2 + error x + y  ,   32  cid:4  Numerical Algorithms  where the ﬁnal term represents an estimate of the error induced by adding x and y. Main- taining error bars in this fashion keeps track of conﬁdence in a given value, which can be informative for scientiﬁc calculations.  2.2 UNDERSTANDING ERROR With the exception of the arbitrary-precision systems described in §2.1.3, nearly every com- puterized representation of real numbers with fractional parts is forced to employ rounding and other approximations. Rounding, however, represents one of many sources of error typically encountered in numerical systems:    Rounding or truncation error comes from rounding and other approximations used to deal with the fact that we can only represent a ﬁnite set of values using most computational number systems. For example, it is impossible to write π exactly as an IEEE 754 ﬂoating-point value, so in practice its value is truncated after a ﬁnite number of digits.    Discretization error comes from our computerized adaptations of calculus, physics, and other aspects of continuous mathematics. For instance, a numerical system might attempt to approximate the derivative of a function f  t  using divided diﬀerences:  f cid:48  t  ≈  f  t + ε  − f  t   ε  for some ﬁxed choice of ε > 0. This approximation is a legitimate and useful one that we will study in Chapter 14, but since we must use a ﬁnite ε > 0 rather than taking a limit as ε → 0, the resulting value for f cid:48  t  is only accurate to some number of digits.   Modeling error comes from having incomplete or inaccurate descriptions of the prob- lems we wish to solve. For instance, a simulation predicting weather in Germany may choose to neglect the collective ﬂapping of butterﬂy wings in Malaysia, although the displacement of air by these butterﬂies might perturb the weather patterns elsewhere. Furthermore, constants such as the speed of light or acceleration due to gravity might be provided to the system with a limited degree of accuracy.    Input error can come from user-generated approximations of parameters of a given system  and from typos! . Simulation and numerical techniques can help answer “what if” questions, in which exploratory choices of input setups are chosen just to get some idea of how a system behaves. In this case, a highly accurate simulation might be a waste of computational time, since the inputs to the simulation were so rough.  Example 2.2  Computational physics . Suppose we are designing a system for simulating planets as they revolve around the sun. The system essentially solves Newton’s equation F = ma by integrating forces forward in time. Examples of error sources in this system might include:    Rounding error: Rounding the product ma to IEEE ﬂoating-point precision   Discretization error: Using divided diﬀerences as above to approximate the velocity  and acceleration of each planet    Modeling error: Neglecting to simulate the moon’s eﬀects on the earth’s motion  within the planetary system   Numerics and Error Analysis  cid:4  33    Input error: Evaluating the cost of sending garbage into space rather than risking a Wall-E style accumulation on Earth, but only guessing the total amount of garbage to jettison monthly  2.2.1 Classifying Error  Given our previous discussion, the following two numbers might be regarded as having the same amount of error:  1 ± 0.01 105 ± 0.01.  Both intervals [1 − 0.01, 1 + 0.01] and [105 − 0.01, 105 + 0.01] have the same width, but the latter appears to encode a more conﬁdent measurement because the error 0.01 is much smaller relative to 105 than to 1.  The distinction between these two classes of error is described by distinguishing between  absolute error and relative error:  Deﬁnition 2.1  Absolute error . The absolute error of a measurement is the diﬀerence between the approximate value and its underlying true value.  Deﬁnition 2.2  Relative error . The relative error of a measurement is the absolute error divided by the true value.  Absolute error is measured in input units, while relative error is measured as a percentage.  Example 2.3  Absolute and relative error . Absolute and relative error can be used to express uncertainty in a measurement as follows:  Absolute: 2 in ± 0.02 in Relative: 2 in ± 1%  Example 2.4  Catastrophic cancellation . Suppose we wish to compute the diﬀerence d ≡ 1− 0.99 = 0.01. Thanks to an inaccurate representation, we may only know these two values up to ±0.004. Assuming that we can carry out the subtraction step without error, we are left with the following expression for absolute error:  In other words, we know d is somewhere in the range [0.002, 0.018]. From an absolute perspective, this error may be fairly small. Suppose we attempt to calculate relative error:  d = 0.01 ± 0.008.  0.002 − 0.01  = 0.018 − 0.01  = 80%.  0.01  0.01  Thus, although 1 and 0.99 are known with relatively small error, the diﬀerence has enor- mous relative error of 80%. This phenomenon, known as catastrophic cancellation, is a danger associated with subtracting two nearby values, yielding a result close to zero.   34  cid:4  Numerical Algorithms  Figure 2.2 Values of f  x  from Example 2.5, computed using IEEE ﬂoating-point arithmetic.  Example 2.5  Loss of precision in practice . Figure 2.2 plots the function  f  x  ≡  ex − 1  x − 1,  for evenly spaced inputs x ∈ [−10−8, 10−8], computed using IEEE ﬂoating-point arith- metic. The numerator and denominator approach 0 at approximately the same rate, re- sulting in loss of precision and vertical jumps up and down near x = 0. As x → 0, in theory f  x  → 0, and hence the relative error of these approximate values blows up.  In most applications, the true value is unknown; after all, if it were known, there would be no need for an approximation in the ﬁrst place. Thus, it is diﬃcult to compute relative error in closed form. One possible resolution is to be conservative when carrying out computations: At each step take the largest possible error estimate and propagate these estimates forward as necessary. Such conservative estimates are powerful in that when they are small we can be very conﬁdent in our output.  An alternative resolution is to acknowledge what you can measure; this resolution re- quires somewhat more intricate arguments but will appear as a theme in future chapters. For instance, suppose we wish to solve the equation f  x  = 0 for x given a function f : R → R. Our computational system may yield some xest satisfying f  xest  = ε for some ε with ε  cid:28  1. If x0 is the true root satisfying f  x0  = 0, we may not be able to evaluate the dif- ference x0 − xest since x0 is unknown. On the other hand, by evaluating f we can compute f  xest  − f  x0  ≡ f  xest  since f  x0  = 0 by deﬁnition. This diﬀerence of f values gives a proxy for error that still is zero exactly when xest = x0. This example illustrates the distinction between forward and backward error. Forward error is the most direct deﬁnition of error as the diﬀerence between the approximated and actual solution, but as we have discussed it is not always computable. Contrastingly, backward error is a calculable proxy for error correlated with forward error. We can adjust the deﬁnition and interpretation of backward error as we consider diﬀerent problems, but one suitable—if vague—deﬁnition is as follows:  Deﬁnition 2.3  Backward error . The backward error of an approximate solution to a numerical problem is the amount by which the problem statement would have to change to make the approximate solution exact.  10−7  0.2 × 10−8  x   Numerics and Error Analysis  cid:4  35  This deﬁnition is somewhat obtuse, so we illustrate its application to a few scenarios.  Example 2.6  Linear systems . Suppose we wish to solve the n× n linear system A cid:126 x =  cid:126 b for  cid:126 x ∈ Rn. Label the true solution as  cid:126 x0 ≡ A−1 cid:126 b. In reality, due to rounding error and other issues, our system yields a near-solution  cid:126 xest. The forward error of this approximation is the diﬀerence  cid:126 xest −  cid:126 x0; in practice, this diﬀerence is impossible to compute since we do not know  cid:126 x0. In reality,  cid:126 xest is the exact solution to a modiﬁed system A cid:126 x =  cid:126 best for  cid:126 best ≡ A cid:126 xest; thus, we might measure backward error in terms of the diﬀerence  cid:126 b −  cid:126 best. Unlike the forward error, this error is easily computable without inverting A, and  cid:126 xest is a solution to the problem exactly when backward  or forward  error is zero.  Example 2.7  Solving equations, from [58], Example 1.5 . Suppose we write a function  for ﬁnding square roots of positive numbers that outputs √2 ≈ 1.4. The forward error is 1.4 − √2 ≈ 0.0142. The backward error is 1.42 − 2 = 0.04.  These examples demonstrate that backward error can be much easier to compute than forward error. For example, evaluating forward error in Example 2.6 required inverting a matrix A while evaluating backward error required only multiplication by A. Similarly, in Example 2.7, transitioning from forward error to backward error replaced square root computation with multiplication.  2.2.2 Conditioning, Stability, and Accuracy  In nearly any numerical problem, zero backward error implies zero forward error and vice versa. A piece of software designed to solve such a problem surely can terminate if it ﬁnds that a candidate solution has zero backward error. But what if backward error is small but nonzero? Does this condition necessarily imply small forward error? We must address such questions to justify replacing forward error with backward error for evaluating the success of a numerical algorithm.  The relationship between forward and backward error can be diﬀerent for each problem  we wish to solve, so in the end we make the following rough classiﬁcation:    A problem is insensitive or well-conditioned when small amounts of backward error imply small amounts of forward error. In other words, a small perturbation to the statement of a well-conditioned problem yields only a small perturbation of the true solution.    A problem is sensitive, poorly conditioned, or stiﬀ when this is not the case.  Example 2.8  ax = b . Suppose as a toy example that we want to ﬁnd the solution x0 ≡ b a to the linear equation ax = b for a, x, b ∈ R. Forward error of a potential solution x is given by x−x0 while backward error is given by b−ax = a x−x0 . So, when a  cid:29  1, the problem is well-conditioned since small values of backward error a x − x0  imply even smaller values of x − x0; contrastingly, when a  cid:28  1 the problem is ill-conditioned, since even if a x− x0  is small, the forward error x− x0 ≡ 1 a· a x− x0  may be large given the 1 a factor.  We deﬁne the condition number to be a measure of a problem’s sensitivity:   36  cid:4  Numerical Algorithms  Deﬁnition 2.4  Condition number . The condition number of a problem is the ratio of how much its solution changes to the amount its statement changes under small pertur- bations. Alternatively, it is the ratio of forward to backward error for small changes in the problem statement.  Problems with small condition numbers are well-conditioned, and thus backward error can be used safely to judge success of approximate solution techniques. Contrastingly, much smaller backward error is needed to justify the quality of a candidate solution to a problem with a large condition number.  Example 2.9  ax = b, continued . Continuing Example 2.8, we can compute the condition number exactly:  c =  forward error backward error  = x − x0 a x − x0  ≡  .  1 a  Computing condition numbers usually is nearly as hard as computing forward error, and thus their exact computation is likely impossible. Even so, many times it is possible to bound or approximate condition numbers to help evaluate how much a solution can be trusted.  Example 2.10  Root-ﬁnding . Suppose that we are given a smooth function f : R → R and want to ﬁnd roots x with f  x  = 0. By Taylor’s theorem, f  x + ε  ≈ f  x  + εf cid:48  x  when ε is small. Thus, an approximation of the condition number for ﬁnding the root x is given by  forward error backward error  f  x + ε  − f  x  ≈ ε =  x + ε  − x εf cid:48  x   =  1  .  f cid:48  x   This approximation generalizes the one in Example 2.9. If we do not know x, we cannot evaluate f cid:48  x , but if we can examine the form of f and bound f cid:48  near x, we have an idea of the worst-case situation.  Forward and backward error measure the accuracy of a solution. For the sake of scientiﬁc repeatability, we also wish to derive stable algorithms that produce self-consistent solutions to a class of problems. For instance, an algorithm that generates accurate solutions only one ﬁfth of the time might not be worth implementing, even if we can use the techniques above to check whether a candidate solution is good. Other numerical methods require the client to tune several unintuitive parameters before they generate usable output and may be unstable or sensitive to changes to any of these options.  2.3 PRACTICAL ASPECTS The theory of error analysis introduced in §2.2 will allow us to bound the quality of numerical techniques we introduce in future chapters. Before we proceed, however, it is worth noting some more practical oversights and “gotchas” that pervade implementations of numerical methods.  We purposefully introduced the largest oﬀender early in §2.1, which we repeat in a larger  font for well-deserved emphasis:  Rarely if ever should the operator == and its equivalents  be used on fractional values. Instead, some tolerance  should be used to check if numbers are equal.   Numerics and Error Analysis  cid:4  37  Finding a suitable replacement for == depends on particulars of the situation. Example 2.6 shows that a method for solving A cid:126 x =  cid:126 b can terminate when the residual  cid:126 b − A cid:126 x is zero; since we do not want to check if A*x==b explicitly, in practice implementations will check norm A*x-b <epsilon. This example demonstrates two techniques:    the use of backward error  cid:126 b − A cid:126 x rather than forward error to determine when to  terminate, and    checking whether backward error is less than epsilon to avoid the forbidden ==0  predicate.  The parameter epsilon depends on how accurate the desired solution must be as well as the quality of the discrete numerical system.  Based on our discussion of relative error, we can isolate another common cause of bugs  in numerical software:  Beware of operations that transition between orders of magnitude, like division by small values and subtraction of similar quantities.  Catastrophic cancellation as in Example 2.4 can cause relative error to explode even if the inputs to an operation are known with near-complete certainty.  2.3.1 Computing Vector Norms  A programmer using ﬂoating-point data types and operations must be vigilant when it comes to detecting and preventing poor numerical operations. For example, consider the following code snippet for computing the norm  cid:107  cid:126 x cid:107 2 for a vector  cid:126 x ∈ Rn represented as a 1D array x[]:  double n o r m S q u a r e d = 0; for   int i = 0; i < n ; i ++   n o r m S q u a r e d += x [ i ]* x [ i ];  return sqrt   n o r m S q u a r e d  ;  In theory, mini xi ≤  cid:107  cid:126 x cid:107 2 √n ≤ maxi xi, that is, the norm of  cid:126 x is on the order of the values of elements contained in  cid:126 x. Hidden in the computation of  cid:107  cid:126 x cid:107 2, however, is the expression x[i]*x[i]. If there exists i such that x[i] is near DOUBLE_MAX, the product x[i]*x[i] will overﬂow even though  cid:107  cid:126 x cid:107 2 is still within the range of the doubles. Such overﬂow is preventable by dividing  cid:126 x by its maximum value, computing the norm, and multiplying back:  double m a x E l e m e n t = epsilon ;    don ’ t want to d i v i d e by zero ! for   int i = 0; i < n ; i ++   m a x E l e m e n t = max   maxElement , fabs   x [ i ]  ;  for   int i = 0; i < n ; i ++  {  double scaled = x [ i ]   m a x E l e m e n t ; n o r m S q u a r e d += scaled * scaled ;  } return sqrt   n o r m S q u a r e d   * m a x E l e m e n t ;  The scaling factor alleviates the overﬂow problem by ensuring that elements being summed are no larger than 1, at the cost of additional computation time.  This small example shows one of many circumstances in which a single character of code can lead to a non-obvious numerical issue, in this case the product *. While our intuition from continuous mathematics is suﬃcient to formulate many numerical methods, we must always double-check that the operations we employ are valid when transitioning from theory to ﬁnite-precision arithmetic.   38  cid:4  Numerical Algorithms  Figure 2.3  a  A simplistic method for summing the elements of a vector  cid:126 x;  b  the Kahan summation algorithm. 2.3.2 Larger-Scale Example: Summation  We now provide an example of a numerical issue caused by ﬁnite-precision arithmetic whose resolution involves a more subtle algorithmic trick. Suppose that we wish to sum a list of ﬂoating-point values stored in a vector  cid:126 x ∈ Rn, a task required by systems in accounting, machine learning, graphics, and nearly any other ﬁeld. A simple strategy, iterating over the elements of  cid:126 x and incrementally adding each value, is detailed in Figure 2.3 a . For the vast majority of applications, this method is stable and mathematically valid, but in challenging cases it can fail.  What can go wrong? Consider the case where n is large and most of the values xi are small and positive. Then, as i progresses, the current sum s will become large relative to xi. Eventually, s could be so large that adding xi would change only the lowest-order bits of s, and in the extreme case s could be large enough that adding xi has no eﬀect whatsoever. Put more simply, adding a long list of small numbers can result in a large sum, even if any single term of the sum appears insigniﬁcant.  To understand this eﬀect mathematically, suppose that computing a sum a + b can be oﬀ by as much as a factor of ε > 0. Then, the method in Figure 2.3 a  can induce error on the order of nε, which grows linearly with n. If most elements xi are on the order of ε, then the sum cannot be trusted whatsoever ! This is a disappointing result: The error can be as large as the sum itself.  Fortunately, there are many ways to do better. For example, adding the smallest values ﬁrst might make sure they are not deemed insigniﬁcant. Methods recursively adding pairs of values from  cid:126 x and building up a sum also are more stable, but they can be diﬃcult to implement as eﬃciently as the for loop above. Thankfully, an algorithm by Kahan provides an easily implemented “compensated summation” method that is nearly as fast as iterating over the array [69].  function Simple-Sum  cid:2 x   s ← 0 for i ← 1, 2, . . . , n : s ← s + xi return s   cid:3  Current total  function Kahan-Sum  cid:2 x   s, c ← 0 for i ← 1, 2, . . . , n   cid:3  Current total and compensation  v ← xi + c  cid:3  Try to add xi and compensation c to the sum snext ← s + v  cid:3  Compute the summation result of this iteration c ← v −  snext − s   cid:3  Compute compensation using the Kahan error estimate s ← snext  cid:3  Update sum  return s   a    b    Numerics and Error Analysis  cid:4  39  The useful observation to make is that we can approximate the inaccuracy of s as it  changes from iteration to iteration. To do so, consider the expression    a + b  − a  − b.  Algebraically, this expression equals zero. Numerically, however, this may not be the case. In particular, the sum  a + b  may be rounded to ﬂoating-point precision. Subtracting a and b one at a time then yields an approximation of the error of approximating a + b. Removing a and b from a + b intuitively transitions from large orders of magnitude to smaller ones rather than vice versa and hence is less likely to induce rounding error than evaluating the sum a + b; this observation explains why the error estimate is not itself as prone to rounding issues as the original operation.  With this observation in mind, the Kahan technique proceeds as in Figure 2.3 b . In addition to maintaining the sum s, now we keep track of a compensation value c approxi- mating the diﬀerence between s and the true sum at each iteration i. During each iteration, we attempt to add this compensation to s in addition to the current element xi of  cid:126 x; then we recompute c to account for the latest error.  Analyzing the Kahan algorithm requires more careful bookkeeping than analyzing the incremental technique in Figure 2.3 a . Although constructing a formal mathematical argu- ment is outside the scope of our discussion, the ﬁnal mathematical result is that error is on the order O ε + nε2 , a considerable improvement over O nε  when 0 ≤ ε  cid:28  1. Intuitively, it makes sense that the O nε  term from Figure 2.3 a  is reduced, since the compensation attempts to represent the small values that were otherwise neglected. Formal arguments for the ε2 bound are surprisingly involved; one detailed derivation can be found in [49].  Implementing Kahan summation is straightforward but more than doubles the operation count of the resulting program. In this way, there is an implicit trade-oﬀ between speed and accuracy that software engineers must make when deciding which technique is most appropriate. More broadly, Kahan’s algorithm is one of several methods that bypass the accumulation of numerical error during the course of a computation consisting of more than one operation. Another representative example from the ﬁeld of computer graphics is Bresenham’s algorithm for rasterizing lines [18], which uses only integer arithmetic to draw lines even when they intersect rows and columns of pixels at non-integer locations.  2.4 EXERCISES  2.1 When might it be preferable to use a ﬁxed-point representation of real numbers over ﬂoating-point? When might it be preferable to use a ﬂoating-point representation of real numbers over ﬁxed-point?  DH 2.2  “Extraterrestrial chemistry”  Suppose we are programming a planetary rover to an- alyze the chemicals in a gas found on a neighboring planet. Our rover is equipped with a ﬂask of volume 0.5 m3 and also has pressure and temperature sensors. Using the sensor readouts from a given sample, we would like our rover to determine the amount of gas our ﬂask contains.  One of the fundamental physical equations describing a gas is the Ideal Gas Law P V = nRT , which states:   P  ressure ·  V  olume = amou n t of gas · R ·  T  emperature,  where R is the ideal gas constant, approximately equal to 8.31 J · mol−1 · K−1. Here, P is in pascals, V is in cubic meters, n is in moles, and T is in Kelvin. We will use this equation to approximate n given the other variables.   40  cid:4  Numerical Algorithms   a  Describe any forms of rounding, discretization, modeling, and input error that  can occur when solving this problem.   b  Our rover’s pressure and temperature sensors do not have perfect accuracy. Sup- pose the pressure and temperature sensor measurements are accurate to within ±εP and ±εT , respectively. Assuming V , R, and fundamental arithmetic opera- tions like + and × induce no errors, bound the relative forward error in computing n, when 0 < εP  cid:28  P and 0 < εT  cid:28  T.   c  Continuing the previous part, suppose P = 100 Pa, T = 300 K, εP = 1 Pa, and εT = 0.5 K. Derive upper bounds for the worst absolute and relative errors that we could obtain from a computation of n.   d  Experiment with perturbing the variables P and T . Based on how much your estimate of n changes between the experiments, suggest when this problem is well-conditioned or ill-conditioned.  DH 2.3 In contrast to the “absolute” condition number introduced in this chapter, we can  deﬁne the “relative” condition number of a problem to be  κrel ≡  relative forward error relative backward error  .  In some cases, the relative condition number of a problem can yield better insights into its sensitivity. Suppose we wish to evaluate a function f : R → R at a point x ∈ R, obtaining y ≡ f  x . Assuming f is smooth, compare the absolute and relative condition numbers of computing y at x. Additionally, provide examples of functions f with large and small relative condition numbers for this problem near x = 1. Hint: Start with the relationship y + ∆y = f  x + ∆x , and use Taylor’s theorem to write the condition numbers in terms of x, f  x , and f cid:48  x .  2.4 Suppose f : R → R is inﬁnitely diﬀerentiable, and we wish to write algorithms for ﬁnding x∗ minimizing f  x . Our algorithm outputs xest, an approximation of x∗. Assuming that in our context this problem is equivalent to ﬁnding roots of f cid:48  x , write expressions for:   a  Forward error of the approximation   b  Backward error of the approximation   c  Conditioning of this minimization problem near x∗  2.5 Suppose we are given a list of ﬂoating-point values x1, x2, . . . , xn. The following quan-  tity, known as their “log-sum-exp,” appears in many machine learning algorithms:   cid:96  x1, . . . , xn  ≡ ln cid:34  n cid:88 k=1  exk cid:35  .   a  The value pk ≡ exk often represents a probability pk ∈  0, 1]. In this case, what  is the range of possible xk’s?   b  Suppose many of the xk’s are very negative  xk  cid:28  0 . Explain why evaluating the log-sum-exp formula as written above may cause numerical error in this case.   Numerics and Error Analysis  cid:4  41  Figure 2.4 z-ﬁghting, for Exercise 2.6; the overlap region is zoomed on the right.   c  Show that for any a ∈ R,   cid:96  x1, . . . , xn  = a + ln cid:34  n cid:88 k=1  exk−a cid:35  .  To avoid the issues you explained in 2.5b, suggest a value of a that may improve the stability of computing  cid:96  x1, . . . , xn .  2.6  “z-ﬁghting”  A typical pipeline in computer graphics draws three-dimensional sur- faces on the screen, one at a time. To avoid rendering a far-away surface on top of a close one, most implementations use a z-buﬀer, which maintains a double-precision depth value z x, y  ≥ 0 representing the depth of the closest object to the camera at each screen coordinate  x, y . A new object is rendered at  x, y  only when its z value is smaller than the one currently in the z-buﬀer.  A common artifact when rendering using z-buﬀering known as z-ﬁghting is shown in Figure 2.4. Here, two surfaces overlap at some visible points. Why are there rendering artifacts in this region? Propose a strategy for avoiding this artifact; there are many possible resolutions.  2.7  Adapted from Stanford CS 205A, 2012  Thanks to ﬂoating-point arithmetic, in most implementations of numerical algorithms we cannot expect that computations involv- ing fractional values can be carried out with 100% precision. Instead, every time we do a numerical operation we induce the potential for error. Many models exist for studying how this error aﬀects the quality of a numerical operation; in this problem, we will explore one common model. Suppose we care about an operation  cid:5  between two scalars x and y; here  cid:5  might stand for +, −, ×, ÷, and so on. As a model for the error that occurs when computing x cid:5  y, we will say that evaluating x  cid:5  y on the computer yields a number  1 + ε  x  cid:5  y  for some number ε satisfying 0 ≤ ε < εmax  cid:28  1; we will assume ε can depend on  cid:5 , x, and y.   a  Why is this a reasonable model for modeling numerical issues in ﬂoating-point arithmetic? For example, why does this make more sense than assuming that the output of evaluating x  cid:5  y is  x  cid:5  y  + ε?   42  cid:4  Numerical Algorithms   b    Revised by B. Jo  Suppose we are given two vectors  cid:126 x,  cid:126 y ∈ Rn and compute their dot product as sn via the recurrence:  s0 ≡ 0 sk ≡ sk−1 + xkyk.  In practice, both the addition and multiplication steps of computing sk from sk−1 induce numerical error. Use ˆsk to denote the actual value computed incorporating numerical error, and denote ek ≡ ˆsk − sk. Show that max¯sn ,  en ≤ nεmax¯sn + O nε2  where ¯sn ≡ cid:80 n  k=1 xkyk. You can assume that adding x1y1 to zero incurs no error, so ˆs1 =  1 + ε× x1y1, where ε× encodes the error induced by multiplying x1 and y1. You also can assume that nεmax < 1.  2.8 Argue using the error model from the previous problem that the relative error of com- puting x− y for x, y > 0 can be unbounded; assume that there is error in representing x and y in addition to error computing the diﬀerence. This phenomenon is known as “catastrophic cancellation” and can cause serious numerical issues.  2.9 In this problem, we continue to explore the conditioning of root-ﬁnding. Suppose f  x   and p x  are smooth functions of x ∈ R.  a  Thanks to inaccuracies in how we evaluate or express f  x , we might accidentally compute roots of a perturbation f  x  + εp x . Take x∗ to be a root of f, so f  x∗  = 0. If f cid:48  x∗   cid:54 = 0, for small ε we can write a function x ε  such that f  x ε   + εp x ε   = 0, with x 0  = x∗. Assuming such a function exists and is diﬀerentiable, show:   b  Assume f  x  is given by Wilkinson’s polynomial [131]:  f  x  ≡  x − 1  ·  x − 2  ·  x − 3  · ··· ·  x − 20 .  We could have expanded f  x  in the monomial basis as f  x  = a0 + a1x + a2x2 + ··· + a20x20, for appropriate choices of a0, . . . , a20. If we express the coeﬃcient a19 inaccurately, we could use the model from Exercise 2.9a with p x  ≡ x19 to predict how much root-ﬁnding will suﬀer. For these choices of f  x  and p x , show:   c  Compare dx  dε from the previous part for x∗ = 1 and x∗ = 20. Which root is more  stable to this perturbation?  2.10 The roots of the quadratic function ax2 + bx + c are given by the quadratic equation  p x∗  f cid:48  x∗   .  = −  dx  dε cid:12  cid:12  cid:12  cid:12 ε=0  dx  dε cid:12  cid:12  cid:12  cid:12 ε=0,x∗=j  = − cid:89 k cid:54 =j  j  .  j − k  x∗ ∈ −b ± √b2 − 4ac  2a  .   Numerics and Error Analysis  cid:4  43   a  Prove the alternative formula  x∗ ∈  −2c  b ± √b2 − 4ac  .   b  Propose a numerically stable algorithm for solving the quadratic equation.  2.11 One technique for tracking uncertainty in a calculation is the use of interval arithmetic. In this system, an uncertain value for a variable x is represented as the interval [x] ≡ [x, x] representing the range of possible values for x, from x to x. Assuming inﬁnite-precision arithmetic, give update rules for the following in terms of x, x, y, and y:    [x] + [y]   [x] − [y]   [x] × [y]    [x] ÷ [y]   [x]1 2  Additionally, propose a conservative modiﬁcation for ﬁnite-precision arithmetic.  2.12 Algorithms for dealing with geometric primitives such as line segments and triangles are notoriously diﬃcult to implement in a numerically stable fashion. Here, we high- light a few ideas from “ε-geometry,” a technique built to deal with these issues [55].   a  Take  cid:126 p,  cid:126 q,  cid:126 r ∈ R2. Why might it be diﬃcult to determine whether  cid:126 p,  cid:126 q, and  cid:126 r are  collinear using ﬁnite-precision arithmetic?   b  We will say  cid:126 p,  cid:126 q, and  cid:126 r are ε-collinear if there exist  cid:126 p cid:48  with  cid:107  cid:126 p −  cid:126 p cid:48  cid:107 2 ≤ ε,  cid:126 q cid:48  with  cid:107  cid:126 q −  cid:126 q cid:48  cid:107 2 ≤ ε, and  cid:126 r cid:48  with  cid:107  cid:126 r −  cid:126 r cid:48  cid:107 2 ≤ ε such that  cid:126 p cid:48 ,  cid:126 q cid:48 , and  cid:126 r cid:48  are exactly collinear. For ﬁxed  cid:126 p and  cid:126 q, sketch the region { cid:126 r ∈ R2 :  cid:126 p,  cid:126 q,  cid:126 r are ε-collinear}. This region is known as the ε-butterﬂy of  cid:126 p and  cid:126 q.   c  An ordered triplet   cid:126 p,  cid:126 q,  cid:126 r  ∈ R2 × R2 × R2 is ε-clockwise if the three points can be perturbed by at most distance ε so that they form a triangle whose vertices are in clockwise order; we will consider collinear triplets to be both clockwise and counterclockwise. For ﬁxed  cid:126 p and  cid:126 q, sketch the region { cid:126 r ∈ R2 :   cid:126 p,  cid:126 q,  cid:126 r  is ε-clockwise}.   d  Show a triplet is ε-collinear if and only if  it is both ε-clockwise and ε-  counterclockwise.   e  A point  cid:126 x ∈ R2 is ε-inside the triangle   cid:126 p,  cid:126 q,  cid:126 r  if and only if  cid:126 p,  cid:126 q,  cid:126 r, and  cid:126 x can be moved by at most distance ε such that the perturbed  cid:126 x cid:48  is exactly inside the perturbed triangle   cid:126 p cid:48 ,  cid:126 q cid:48 ,  cid:126 r cid:48  . Show that when  cid:126 p,  cid:126 q, and  cid:126 r are in  exactly  clockwise order,  cid:126 x is inside   cid:126 p,  cid:126 q,  cid:126 r  if and only if   cid:126 p,  cid:126 q,  cid:126 x ,   cid:126 q,  cid:126 r,  cid:126 x , and   cid:126 r,  cid:126 p,  cid:126 x  are all clockwise. Is the same statement true if we relax to ε-inside and ε-clockwise?    II  Linear Algebra  45    C H A P T E R3  Linear Systems and the LU Decomposition  CONTENTS  3.1 3.2 3.3  Solvability of Linear Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Ad-Hoc Solution Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Encoding Row Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.1 Permutation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.2 Row Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.3 Elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4 Gaussian Elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4.1 Forward-Substitution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4.2 Back-Substitution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4.3 Analysis of Gaussian Elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . LU Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.5.1 Constructing the Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.5.2 Using the Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.5.3 Implementing LU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  3.5  47 49 51 51 52 52 54 55 56 56 58 59 60 61  W E commence our discussion of numerical algorithms by deriving ways to solve the  linear system of equations A cid:126 x =  cid:126 b. We will explore applications of these systems in Chapter 4, showing a variety of computational problems that can be approached by constructing appropriate A and  cid:126 b and solving for  cid:126 x. Furthermore, solving a linear system will serve as a basic step in larger methods for optimization, simulation, and other numerical tasks considered in almost all future chapters. For these reasons, a thorough treatment and understanding of linear systems is critical.  3.1 SOLVABILITY OF LINEAR SYSTEMS As introduced in §1.3.4, systems of linear equations like  can be written in matrix form as in  3x + 2y = 6 −4x + y = 7   cid:18  3 −4 1  cid:19  cid:18  x  y  cid:19  = cid:18  6 7  cid:19  .  2  47   48  cid:4  Numerical Algorithms  More generally, we can write linear systems in the form A cid:126 x =  cid:126 b for A ∈ Rm×n,  cid:126 x ∈ Rn, and  cid:126 b ∈ Rm.  The solvability of A cid:126 x =  cid:126 b must fall into one of three cases:  1. The system may not admit any solutions, as in:   cid:18  1  1  0  0  cid:19  cid:18  x  y  cid:19  = cid:18  −1 1  cid:19  .  This system enforces two incompatible conditions simultaneously: x = −1 and x = 1. 2. The system may admit a single solution; for instance, the system at the beginning of  this section is solved by  x, y  =  −8 11, 45 11 .  3. The system may admit inﬁnitely many solutions, e.g., 0 cid:126 x =  cid:126 0. If a system A cid:126 x =  cid:126 b admits two distinct solutions  cid:126 x0 and  cid:126 x1, then it automatically has inﬁnitely many solutions of the form t cid:126 x0 +  1 − t  cid:126 x1 for all t ∈ R, since  A t cid:126 x0 +  1 − t  cid:126 x1  = tA cid:126 x0 +  1 − t A cid:126 x1 = t cid:126 b +  1 − t  cid:126 b =  cid:126 b.  Because it has multiple solutions, this linear system is labeled underdetermined.  The solvability of the system A cid:126 x =  cid:126 b depends both on A and on  cid:126 b. For instance, if we  modify the unsolvable system above to   cid:18  1 0 1 0  cid:19  cid:18  x  y  cid:19  = cid:18  1 1  cid:19  ,  then the system changes from having no solutions to inﬁnitely many of the form  1, y . Every matrix A admits a right-hand side  cid:126 b such that A cid:126 x =  cid:126 b is solvable, since A cid:126 x =  cid:126 0 always can be solved by  cid:126 x =  cid:126 0 regardless of A.  For alternative intuition about the solvability of linear systems, recall from §1.3.1 that the matrix-vector product A cid:126 x can be viewed as a linear combination of the columns of A with weights from  cid:126 x. Thus, as mentioned in §1.3.4, A cid:126 x =  cid:126 b is solvable exactly when  cid:126 b is in the column space of A. In a broad way, the shape of the matrix A ∈ Rm×n has considerable bearing on the solvability of A cid:126 x =  cid:126 b. First, consider the case when A is “wide,” that is, when it has more columns than rows  n > m . Each column is a vector in Rm, so at most the column space can have dimension m. Since n > m, the n columns of A must be linearly dependent; this implies that there exists a set of weights  cid:126 x0  cid:54 =  cid:126 0 such that A cid:126 x0 =  cid:126 0. If we can solve A cid:126 x =  cid:126 b for  cid:126 x, then A  cid:126 x + α cid:126 x0  = A cid:126 x + αA cid:126 x0 =  cid:126 b +  cid:126 0 =  cid:126 b, showing that there are actually inﬁnitely many solutions  cid:126 x to A cid:126 x =  cid:126 b. In other words:  No wide matrix system admits a unique solution.  When A is “tall,” that is, when it has more rows than columns  m > n , then its n columns cannot possibly span the larger-dimensional Rm. For this reason, there exists some vector  cid:126 b0 ∈ Rm\col A. By deﬁnition, this  cid:126 b0 cannot satisfy A cid:126 x =  cid:126 b0 for any  cid:126 x. That is:  For every tall matrix A, there exists a  cid:126 b0 such that A cid:126 x =  cid:126 b0 is not  solvable.   Linear Systems and the LU Decomposition  cid:4  49  The situations above are far from favorable for designing numerical algorithms. In the wide case, if a linear system admits many solutions, we must specify which solution is desired by the user. After all, the solution  cid:126 x + 1031 cid:126 x0 might not be as meaningful as  cid:126 x − 0.1 cid:126 x0. In the tall case, even if A cid:126 x =  cid:126 b is solvable for a particular  cid:126 b, a small perturbation A cid:126 x =  cid:126 b + ε cid:126 b0 may not be solvable. The rounding procedures discussed in the last chapter easily can move a tall system from solvable to unsolvable.  Given these complications, in this chapter we will make some simplifying assumptions:   We will consider only square A ∈ Rn×n.   We will assume that A is nonsingular, that is, that A cid:126 x =  cid:126 b is solvable for any  cid:126 b.  From §1.3.4, the nonsingularity condition ensures that the columns of A span Rn and implies the existence of a matrix A−1 satisfying A−1A = AA−1 = In×n. We will relax these conditions in subsequent chapters. A misleading observation is to think that solving A cid:126 x =  cid:126 b is equivalent to computing the matrix A−1 explicitly and then multiplying to ﬁnd  cid:126 x ≡ A−1 cid:126 b. While this formula is valid mathematically, it can represent a considerable amount of overkill and potential for numerical instability for several reasons:    The matrix A−1 may contain values that are diﬃcult to express in ﬂoating-point  precision, in the same way that 1 ε → ∞ as ε → 0.    It may be possible to tune the solution strategy both to A and to  cid:126 b, e.g., by working with the columns of A that are the closest to  cid:126 b ﬁrst. Strategies like these can provide higher numerical stability.    Even if A is sparse, meaning it contains many zero values that do not need to be stored explicitly, or has other special structure, the same may not be true for A−1.  We highlight this point as a common source of error and ineﬃciency in numerical software:  Avoid computing A−1 explicitly unless you have a strong  justiﬁcation for doing so.  3.2 AD-HOC SOLUTION STRATEGIES  In introductory algebra, we often approach the problem of solving a linear system of equa- tions as a puzzle rather than as a mechanical exercise. The strategy is to “isolate” variables, iteratively simplifying individual equalities until each is of the form x = const. To formulate step-by-step algorithms for solving linear systems, it is instructive to carry out an example of this methodology with an eye for aspects that can be fashioned into a general technique.  We will consider the following system:  Alongside each simpliﬁcation step, we will maintain a matrix system encoding the current state. Rather than writing out A cid:126 x =  cid:126 b explicitly, we save space using the augmented matrix  y − z = −1 3x − y + z = 4 x + y − 2z = −3.    0 3 −1 1  1 −1 −1 4  1 −2 −3  .  1   x + y − 2z = −3 y − z = −1 3x − y + z = 4  x + y − 2z = −3 y − z = −1 −4y + 7z = 13  x + y − 2z = −3 y − z = −1 3z = 9  x + y − 2z = −3 y − z = −1 z = 3  x + y = 3 y = 2 z = 3  x = 1 y = 2 z = 3         1 0 0  1 −2 −3 1 −1 −1  1  1 0 3 −1  1 −2 −3 1 −1 −1  7  1 0 0 −4  1 0 0  1 0 0  1 −2 −3 1 −1 −1 0  3  1 −2 −3 1 −1 −1 0  1  4  13  9  3   1 0 0  1 1 0  0 0 1  0 1 0  0 0 1  3 2  3  3   1 2  50  cid:4  Numerical Algorithms  We can write linear systems this way so long as we agree that variable coeﬃcients remain on the left of the line and the constants on the right.  Perhaps we wish to deal with the variable x ﬁrst. For convenience, we can permute the  rows of the system so that the third equation appears ﬁrst:  We then substitute the ﬁrst equation into the third to eliminate the 3x term. This is the same as scaling the relationship x + y − 2z = −3 by −3 and adding the result to the third equation:  Similarly, to eliminate y from the third equation, we scale the second equation by 4 and add the result to the third:  We have now isolated z! We scale the third row by 1 3 to yield an expression for z:  Now, we substitute z = 3 into the other two equations to remove z from all but the ﬁnal row:  Finally, we make a similar substitution for y to reveal the solution:  Revisiting the steps above yields a few observations about how to solve linear systems:   We wrote successive systems Ai cid:126 x =  cid:126 bi that can be viewed as simpliﬁcations of the  original A cid:126 x =  cid:126 b.    We solved the system without ever writing down A−1.   We repeatedly used a few elementary operations: scaling, adding, and permuting rows.   The same operations were applied to A and  cid:126 b. If we scaled the k-th row of A, we also scaled the k-th row of  cid:126 b. If we added rows k and  cid:96  of A, we added rows k and  cid:96  of  cid:126 b.   The steps did not depend on  cid:126 b. That is, all of our decisions were motivated by elimi-  nating nonzero values in A;  cid:126 b just came along for the ride.    We terminated when we reached the simpliﬁed system In×n cid:126 x =  cid:126 b.  We will use all of these general observations about solving linear systems to our advantage.   Linear Systems and the LU Decomposition  cid:4  51  3.3 ENCODING ROW OPERATIONS Looking back at the example in §3.2, we see that solving A cid:126 x =  cid:126 b only involved three operations: permutation, row scaling, and adding a multiple of one row to another. We can solve any linear system this way, so it is worth exploring these operations in more detail.  A pattern we will see for the remainder of this chapter is the use of matrices to express row operations. For example, the following two descriptions of an operation on a matrix A are equivalent:  1. Scale the ﬁrst row of A by 2.  2. Replace A with S2A, where S2 is deﬁned by:  S2 ≡  2 0 0 ... 0    0 1 0 ... 0  0 ··· 0 ··· 1 ··· ... . . . 0 ···  0 0 0 ... 1  .    When presenting the theory of matrix simpliﬁcation, it is cumbersome to use words to describe each operation, so when possible we will encode matrix algorithms as a series of pre- and post-multiplications by specially designed matrices like S2 above.  This description in terms of matrices, however, is a theoretical construction. Implementa- tions of algorithms for solving linear systems should not construct matrices like S2 explicitly. For example, if A ∈ Rn×n, it should take n steps to scale the ﬁrst row of A by 2, but ex- plicitly constructing S2 ∈ Rn×n and applying it to A takes n3 steps! That is, we will show for notational convenience that row operations can be encoded using matrix multiplication, but they do not have to be encoded this way.  3.3.1 Permutation Our ﬁrst step in §3.2 was to swap two of the rows. More generally, we might index the rows of a matrix using the integers 1, . . . , m. A permutation of those rows can be written as a function σ : {1, . . . , m} → {1, . . . , m} such that {σ 1 , . . . , σ m } = {1, . . . , m}, that is, σ maps every index to a diﬀerent target. If  cid:126 ek is the k-th standard basis vector, the product  cid:126 e cid:62 k A is the k-th row of the matrix A. We can “stack” or concatenate these row vectors vertically to yield a matrix permuting the rows according to σ:  The product PσA is the matrix A with rows permuted according to σ.  Example 3.1  Permutation matrices . Suppose we wish to permute rows of a matrix in R3×3 with σ 1  = 2, σ 2  = 3, and σ 3  = 1. According to our formula we have  Pσ ≡  −  cid:126 e cid:62 σ 1  − −  cid:126 e cid:62 σ 2  −  ...  −  cid:126 e cid:62 σ m  −   Pσ =  0 0 1  1 0 0  .   0  .  0 1   52  cid:4  Numerical Algorithms  From Example 3.1, Pσ has ones in positions indexed  k, σ k   and zeros elsewhere. Reversing the order of each pair, that is, putting ones in positions indexed  σ k , k  and zeros elsewhere, undoes the eﬀect of the permutation. Hence, the inverse of Pσ must be its transpose P  cid:62 σ . Symbolically, we write P  cid:62 σ Pσ = Im×m, or equivalently P −1 3.3.2 Row Scaling  σ = P  cid:62 σ .  Suppose we write down a list of constants a1, . . . , am and seek to scale the k-th row of A by ak for each k. This task is accomplished by applying the scaling matrix Sa:  Assuming that all the ak’s satisfy ak  cid:54 = 0, it is easy to invert Sa by scaling back:  Sa ≡    a1 0 ... 0  0 a2 ... 0  ··· 0 ··· 0 ... . . . ··· am  .    S−1 a = S1 a ≡    1 a1 0 ... 0  0 1 a2 ... 0  ··· ··· ... 1 am  0 0 . . . ···  .    If any ak equals zero, Sa is not invertible.  3.3.3 Elimination  Finally, suppose we wish to scale row k by a constant c and add the result to row  cid:96 ; we will assume k  cid:54 =  cid:96 . This operation may seem less natural than the previous two, but actually it is quite practical. In particular, it is the only one we need to combine equations from diﬀerent rows of the linear system! We will realize this operation using an elimination matrix M , such that the product M A is the result of applying this operation to matrix A.  The product  cid:126 e cid:62 k A picks out the k-th row of A. Pre-multiplying the result by  cid:126 e cid:96  yields a  matrix  cid:126 e cid:96  cid:126 e cid:62 k A that is zero except on its  cid:96 -th row, which is equal to the k-th row of A. Example 3.2  Elimination matrix construction . Take  Suppose we wish to isolate the third row of A ∈ R3×3 and move it to row two. As discussed above, this operation is accomplished by writing:  3 6  9  . 1  cid:1  9  cid:1   0 1  1 4 7  2 5 8  A = 0  cid:0  0 0  cid:0  7 0  .  0 7 0  0 8 0  0 1  0 9  8  0   cid:126 e2 cid:126 e cid:62 3 A = = =  1 4 7  2 5 8  3 6  9    Linear Systems and the LU Decomposition  cid:4  53  We multiplied right to left above but just as easily could have grouped the product as   cid:126 e2 cid:126 e cid:62 3  A. Grouping this way involves application of the matrix   cid:126 e2 cid:126 e cid:62 3 =  0 1  0  cid:0  0  0  1  cid:1  =  0 0 0  0 0 0  0 1  0  .  We have succeeded in isolating row k and moving it to row  cid:96 . Our original elimination operation was to add c times row k to row  cid:96 , which we can now carry out using the sum A + c cid:126 e cid:96  cid:126 e cid:62 k A =  In×n + c cid:126 e cid:96  cid:126 e cid:62 k  A. Isolating the coeﬃcient of A, the desired elimination matrix is M ≡ In×n + c cid:126 e cid:96  cid:126 e cid:62 k . The action of M can be reversed: Scale row k by c and subtract the result from row  cid:96 . We can check this formally:   In×n − c cid:126 e cid:96  cid:126 e cid:62 k   In×n + c cid:126 e cid:96  cid:126 e cid:62 k   = In×n +  −c cid:126 e cid:96  cid:126 e cid:62 k + c cid:126 e cid:96  cid:126 e cid:62 k   − c2 cid:126 e cid:96  cid:126 e cid:62 k  cid:126 e cid:96  cid:126 e cid:62 k  = In×n − c2 cid:126 e cid:96   cid:126 e cid:62 k  cid:126 e cid:96   cid:126 e cid:62 k = In×n since  cid:126 e cid:62 k  cid:126 e cid:96  =  cid:126 ek ·  cid:126 e cid:96 , and k  cid:54 =  cid:96 .  That is, M−1 = In×n − c cid:126 e cid:96  cid:126 e cid:62 k . Example 3.3  Solving a system . We can now encode each of our operations from Sec- tion 3.2 using the matrices we have constructed above:  1. Permute the rows to move the third equation to the ﬁrst row:  2. Scale row one by −3 and add the result to row three: 0 1 0  1 0 −3 3. Scale row two by 4 and add the result to row three:  P =  0 1 0  0 0 1  1 0  0  .  E1 = I3×3 − 3 cid:126 e3 cid:126 e cid:62 1 = E2 = I3×3 + 4 cid:126 e3 cid:126 e cid:62 2 = S = diag 1, 1, 1 3  = E3 = I3×3 + 2 cid:126 e1 cid:126 e cid:62 3 =  0 0  0 0  1  . 1  .  . 1  .  2 0  1 0 0  0 0 1 0 0 1 3  1 0 0  0 1 4  1 0 0  0 1 0  4. Scale row three by 1 3:  5. Scale row three by 2 and add it to row one:   54  cid:4  Numerical Algorithms  6. Add row three to row two:  1 0 0  0 1 0  E4 = I3×3 +  cid:126 e2 cid:126 e cid:62 3 = E5 = I3×3 −  cid:126 e1 cid:126 e cid:62 3 =  7. Scale row two by −1 and add the result to row one: 1 −1 1 0 0 0  0 1  1  . 1  .  0 0  Thus, the inverse of A in Section 3.2 satisﬁes  A−1 = E5E4E3SE2E1P  1 0 0  1 0  =  =  1 −1 0 0 0 0  1 0 0 0 1 0  1  0 4 1  1 3 −1  .  1 3 0 1 3 −1  1 3 7 3 4 3  1 0 −3  1 0 0  0 0 1 0 0 1 3    0 1 0  0 1 0  1 0 0  0 1  1  1   0 0  0 1 0  0 1 0  0 0 1  2 0  1  0   1 0  Make sure you understand why these matrices appear in reverse order! As a reminder, we would not normally construct A−1 by multiplying the matrices above, since these operations can be implemented more eﬃciently than generic matrix multiplication. Even so, it is valuable to check that the theoretical operations we have deﬁned are equivalent to the ones we have written in words.  3.4 GAUSSIAN ELIMINATION  The sequence of steps chosen in Section 3.2 was by no means unique: There are many diﬀerent paths that can lead to the solution of A cid:126 x =  cid:126 b. Our steps, however, used Gaussian elimination, a famous algorithm for solving linear systems of equations.  To introduce this algorithm, let’s say our system has the following generic “shape”:   cid:16  A  cid:126 b  cid:17  =  × × × × × × × × × × × × × × × × × × × ×   .  Here, an × denotes a potentially nonzero value. Gaussian elimination proceeds in phases described below.   Linear Systems and the LU Decomposition  cid:4  55  3.4.1 Forward-Substitution  Consider the upper-left element of the matrix:   cid:16  A  cid:126 b  cid:17  =  × × × × × × × × × × × × × × × × × × × ×   .  We will call this element the ﬁrst pivot and will assume it is nonzero; if it is zero we can permute rows so that this is not the case. We ﬁrst scale the ﬁrst row by the reciprocal of the pivot so that the value in the pivot position is one:  Now, we use the row containing the pivot to eliminate all other values underneath in the same column using the strategy in §3.3.3:  At this point, the entire ﬁrst column is zero below the pivot. We change the pivot label to the element in position  2, 2  and repeat a similar series of operations to rescale the pivot row and use it to cancel the values underneath:  Now, our matrix begins to gain some structure. After the ﬁrst pivot has been eliminated from all other rows, the ﬁrst column is zero except for the leading one. Thus, any row operation involving rows two to m will not aﬀect the zeros in column one. Similarly, after the second pivot has been processed, operations on rows three to m will not remove the zeros in columns one and two.  We repeat this process until the matrix becomes upper triangular :         1 × × × × × × × × × × × × × × × × × × ×  1 × × × × 0 × × × × 0 × × × × 0 × × × ×  1 × × × × 1 × × × 0 0 × × × 0 0 × × × 0  1 × × × × 1 × × × 0 1 × × 0 0 1 × 0 0 0   .  .   .   .  The method above of making a matrix upper triangular is known as forward-substitution and is detailed in Figure 3.1.   56  cid:4  Numerical Algorithms  Figure 3.1 Forward-substitution without pivoting; see §3.4.3 for pivoting options. 3.4.2 Back-Substitution Eliminating the remaining ×’s from the remaining upper-triangular system is an equally straightforward process proceeding in reverse order of rows and eliminating backward. After the ﬁrst set of back-substitution steps, we are left with the following shape:  Similarly, the second iteration yields:  After our ﬁnal elimination step, we are left with our desired form:      1 × × 0 × 1 × 0 × 0 0 × 0 0 1 × 0 0  1 0  1 × 0 0 0 1 0 0 0  1 0 0  1 0 0 0  0 1 0 0  0 0 1 0  0 × 0 × 0 × 1 ×  0 × 0 × 0 × 1 ×   .  .  .  The right-hand side now is the solution to the linear system A cid:126 x =  cid:126 b. Figure 3.2 implements this method of back-substitution in more detail.  3.4.3 Analysis of Gaussian Elimination  Each row operation in Gaussian elimination—scaling, elimination, and swapping two rows— takes O n  time to complete, since they iterate over all n elements of a row  or two  of A.  function Forward-Substitution A, cid:2 b    cid:3  Converts a system A cid:2 x =  cid:2 b to an upper-triangular system U cid:2 x =  cid:2 y.  cid:3  Assumes invertible A ∈ Rn×n and  cid:2 b ∈ Rn. U,  cid:2 y ← A, cid:2  b U cid:3  for p ← 1, 2  n,...,  will be upper triangular at completion Iterate over current pivot row p   cid:3    cid:3  Scale row p to make element at  p, p  equal one   cid:3  Optionally insert pivoting code here s ← 1 upp yp ← s · yp for c ← p, . . . , n : upc ← s · upc for r ←  p + 1   n,...,  s ← −urp yr ← yr + s · yp for c ← p, . . . , n : urc ← urc + s · upc  return U,  cid:2 y   cid:3   Eliminate from future rows  cid:3  Scale row p by s and add to row r   Linear Systems and the LU Decomposition  cid:4  57  Figure 3.2 Back-substitution for solving upper-triangular systems; this implementa- tion returns the solution  cid:126 x to the system without modifying U .  Once we choose a pivot, we have to do n forward- or back-substitutions into the rows below or above that pivot, respectively; this means the work for a single pivot in total is O n2 . In total, we choose one pivot per row, adding a ﬁnal factor of n. Combining these counts, Gaussian elimination runs in O n3  time.  One decision that takes place during Gaussian elimination meriting more discussion is the choice of pivots. We can permute rows of the linear system as we see ﬁt before performing forward-substitution. This operation, called pivoting, is necessary to be able to deal with all possible matrices A. For example, consider what would happen if we did not use pivoting on the following matrix:  The circled element is exactly zero, so we cannot scale row one by any value to replace that 0 with a 1. This does not mean the system is not solvable—although singular matrices are guaranteed to have this issue—but rather it means we must pivot by swapping the ﬁrst and second rows.  To highlight a related issue, suppose A looks like:  where 0 < ε  cid:28  1. If we do not pivot, then the ﬁrst iteration of Gaussian elimination yields:  We have transformed a matrix A that looks nearly like a permutation matrix  A−1 ≈ A cid:62 , a very easy way to solve the system!  into a system with potentially huge values of the fraction 1 ε. This example is one of many instances in which we should try to avoid dividing by vanishingly small numbers. In this way, there are cases when we may wish to pivot even when Gaussian elimination theoretically could proceed without such a step.  Since Gaussian elimination scales by the reciprocal of the pivot, the most numerically stable option is to have a large pivot. Small pivots have large reciprocals, which scale matrix elements to regimes that may lose precision. There are two well-known pivoting strategies:  1. Partial pivoting looks through the current column and permutes rows of the matrix so that the element in that column with the largest absolute value appears on the diagonal.  A = cid:18  0  1  1  0  cid:19  .  1  1  A = cid:18  ε ˜A = cid:18  1  0  cid:19  , 0 −1 ε  cid:19  .  1 ε  function Back-Substitution U,  cid:2 y    cid:3  Solves upper-triangular systems U cid:2 x =  cid:2 y for  cid:2 x.  cid:2 x ←  cid:2 y for p ← n, n − 1, . . . , 1 for r ← 1, 2, . . . , p − 1 xr ← xr − urpxp upp  return  cid:2 x   cid:3  We will start from U cid:2 x =  cid:2 y and simplify to In×n cid:2 x =  cid:2 x  cid:3  Iterate backward over pivots  cid:3  Eliminate values above upp   58  cid:4  Numerical Algorithms  2. Full pivoting iterates over the entire matrix and permutes rows and columns to place the largest possible value on the diagonal. Permuting columns of a matrix is a valid operation after some added bookkeeping: it corresponds to changing the labeling of the variables in the system, or post-multiplying A by a permutation.  Full pivoting is more expensive computationally than partial pivoting since it requires iter- ating over the entire matrix  or using a priority queue data structure  to ﬁnd the largest absolute value, but it results in enhanced numerical stability. Full pivoting is rarely neces- sary, and it is not enabled by default in common implementations of Gaussian elimination.  Example 3.4  Pivoting . Suppose after the ﬁrst iteration of Gaussian elimination we are left with the following matrix:  If we implement partial pivoting, then we will look only in the second column and will swap the second and third rows; we leave the 10 in the ﬁrst row since that row already has been visited during forward-substitution:  If we implement full pivoting, then we will move the 9:  1  0  0    10 −10 9 0.1  4  6.2   .     1 0 0  10 −10 6.2 4 0.1 9  1 −10 9 0 0 6.2  10 0.1 4   .  .  3.5 LU FACTORIZATION There are many times when we wish to solve a sequence of problems A cid:126 x1 =  cid:126 b1, A cid:126 x2 =  cid:126 b2, . . . , where in each system the matrix A is the same. For example, in image processing we may apply the same ﬁlter encoded in A to a set of images encoded as  cid:126 b1, cid:126 b2, . . .. As we already have discussed, the steps of Gaussian elimination for solving A cid:126 xk =  cid:126 bk depend mainly on the structure of A rather than the values in a particular  cid:126 bk. Since A is kept constant here, we may wish to cache the steps we took to solve the system so that each time we are presented with a new  cid:126 bk we do not have to start from scratch. Such a caching strategy compromises between restarting Gaussian elimination for each  cid:126 bi and computing the potentially numerically unstable inverse matrix A−1.  Solidifying this suspicion that we can move some of the O n3  expense for Gaussian elimination into precomputation time if we wish to reuse A, recall the upper-triangular system appearing after forward-substitution:    1 × × × × 1 × × × 0 1 × × 0 0 1 × 0 0 0   .   Linear Systems and the LU Decomposition  cid:4  59  Unlike forward-substitution, solving this system by back-substitution only takes O n2  time! Why? As implemented in Figure 3.2, back-substitution can take advantage of the structure of the zeros in the system. For example, consider the circled elements of the initial upper- triangular system:  Since we know that the  circled  values to the left of the pivot are zero by deﬁnition of an upper-triangular matrix, we do not need to scale them or copy them upward explicitly. If we ignore these zeros completely, this step of backward-substitution only takes n operations rather than the n2 taken by the corresponding step of forward-substitution.  The next pivot beneﬁts from a similar structure:      1 0 0 0  1 0 0 0  × 1 0 0  × × × × × × 1 × × 1 × 0  × × 0 × 1 × 0 × 0 × 0 1 × 0  1 0   .   .  Again, the zeros on both sides of the one do not need to be copied explicitly.  A nearly identical method can be used to solve lower -triangular systems of equations  via forward-substitution. Combining these observations, we have shown:  While Gaussian elimination takes O n3  time, solving triangular  systems takes O n2  time.  We will revisit the steps of Gaussian elimination to show that they can be used to factorize the matrix A as A = LU , where L is lower triangular and U is upper triangular, so long as pivoting is not needed to solve A cid:126 x =  cid:126 b. Once the matrices L and U are obtained, solving A cid:126 x =  cid:126 b can be carried out by instead solving LU cid:126 x =  cid:126 b using forward-substitution followed by backward-substitution; these two steps combined take O n2  time rather than the O n3  time needed for full Gaussian elimination. This factorization also can be extended to a related and equally useful decomposition when pivoting is desired or necessary.  3.5.1 Constructing the Factorization Other than full pivoting, from §3.3 we know that all the operations in Gaussian elimination can be thought of as pre-multiplying A cid:126 x =  cid:126 b by diﬀerent matrices M to obtain easier systems  M A  cid:126 x = M cid:126 b. As demonstrated in Example 3.3, from this standpoint, each step of Gaussian elimination brings a new system  Mk ··· M2M1A  cid:126 x = Mk ··· M2M1 cid:126 b . Explicitly storing these matrices Mk as n × n objects is overkill, but keeping this interpretation in mind from a theoretical perspective simpliﬁes many of our calculations. After the forward-substitution phase of Gaussian elimination, we are left with an upper- triangular matrix, which we call U ∈ Rn×n. From the matrix multiplication perspective,  Mk ··· M1A = U  =⇒ A =  Mk ··· M1 −1U  1 M−1  =  M−1 ≡ LU , if we make the deﬁnition L ≡ M−1  ··· M−1  2  k  U from the fact  AB −1 = B−1A−1  ··· M−1 k .  1 M−1  2   60  cid:4  Numerical Algorithms  U is upper triangular by design, but we have not characterized the structure of L; our remaining task is to show that L is lower triangular. To do so, recall that in the absence of pivoting, each matrix Mi is either a scaling matrix or has the structure Mi = In×n + c cid:126 e cid:96  cid:126 e cid:62 k , from §3.3.3, where  cid:96  > k since we carried out forward-substitution to obtain U . So, L is the product of scaling matrices and matrices of the form M−1 i = In×n − c cid:126 e cid:96  cid:126 e cid:62 k ; these matrices are lower triangular since  cid:96  > k. Since scaling matrices are diagonal, L is lower triangular by the following proposition:  Proposition 3.1. The product of two or more upper-triangular matrices is upper trian- gular, and the product of two or more lower-triangular matrices is lower triangular.  Proof. Suppose A and B are upper triangular, and deﬁne C ≡ AB. By deﬁnition of upper- triangular matrices, aij = 0 and bij = 0 when i > j. Fix two indices i and j with i > j. Then,  aikbkj by deﬁnition of matrix multiplication  cij = cid:88 k  = ai1b1j + ai2b2j + ··· + ainbnj.  The ﬁrst i − 1 terms of the sum are zero because A is upper triangular, and the last n − j terms are zero because B is upper triangular. Since i > j,  i − 1  +  n − j  > n − 1 and hence all n terms of the sum over k are zero, as needed. If A and B are lower triangular, then A cid:62  and B cid:62  are upper triangular. By our proof  above, B cid:62 A cid:62  =  AB  cid:62  is upper triangular, showing that AB is again lower triangular.  3.5.2 Using the Factorization Having factored A = LU , we can solve A cid:126 x =  cid:126 b in two steps, by writing  LU   cid:126 x =  cid:126 b, or equivalently  cid:126 x = U−1L−1 cid:126 b:  1. Solve L cid:126 y =  cid:126 b for  cid:126 y, yielding  cid:126 y = L−1 cid:126 b.  2. With  cid:126 y now ﬁxed, solve U cid:126 x =  cid:126 y for  cid:126 x.  Checking the validity of  cid:126 x as a solution of the system A cid:126 x =  cid:126 b comes from the following chain of equalities:   cid:126 x = U−1 cid:126 y from the second step  = U−1 L−1 cid:126 b  from the ﬁrst step =  LU  −1 cid:126 b since  AB −1 = B−1A−1 = A−1 cid:126 b since we factored A = LU.  Forward- and back-substitution to carry out the two steps above each take O n2  time. So, given the LU factorization of A, solving A cid:126 x =  cid:126 b can be carried out faster than full O n3  Gaussian elimination. When pivoting is necessary, we will modify our factorization to include a permutation matrix P to account for the swapped rows and or columns, e.g., A = P LU  see Exercise 3.12 . This minor change does not aﬀect the asymptotic timing beneﬁts of LU factorization, since P −1 = P  cid:62 .   Linear Systems and the LU Decomposition  cid:4  61  3.5.3 Implementing LU  The implementation of Gaussian elimination suggested in Figures 3.1 and 3.2 constructs U but not L. We can make some adjustments to factor A = LU rather than solving a single system A cid:126 x =  cid:126 b.  Let’s examine what happens when we multiply two elimination matrices:  In×n − c cid:96  cid:126 e cid:96  cid:126 e cid:62 k   In×n − cp cid:126 ep cid:126 e cid:62 k   = In×n − c cid:96  cid:126 e cid:96  cid:126 e cid:62 k − cp cid:126 ep cid:126 e cid:62 k .  As in the construction of the inverse of an elimination matrix in §3.5.1, the remaining term vanishes by orthogonality of the standard basis vectors  cid:126 ei since k  cid:54 = p. This formula shows that the product of elimination matrices used to forward-substitute a single pivot after it is scaled to 1 has the form:  M =  0 1  1 0 0 0 0 × 1 0 × 0  0 0 0 1   ,  where the values × are those used for forward-substitutions of the circled pivot. Products of matrices of this form performed in forward-substitution order combine the values below the diagonal, as demonstrated in the following example:    1 0 0 0 2 1 0 0 3 0 1 0 4 0 0 1      1 0 0 0 1 0 0 5 1 0 6 0  0 0 0 1      1 0 0 0  0 1 0 0  0 0 1 7  0 0 0 1   =  1 2 3 4  0 1 5 6  0 0 1 7  0 0 0 1   .  We constructed U by pre-multiplying A with a sequence of elimination and scaling matrices. We can construct L simultaneously via a sequence of post-multiplies by their inverses, starting from the identity matrix. These post-multiplies can be computed eﬃciently using the above observations about products of elimination matrices.  For any invertible diagonal matrix D,  LD  D−1U   provides an alternative factorization of A = LU into lower- and upper-triangular matrices. Thus, by rescaling we can decide to keep the elements along the diagonal of L in the LU factorization equal to 1. With this decision in place, we can compress our storage of both L and U into a single n × n matrix whose upper triangle is U and which is equal to L beneath the diagonal; the missing diagonal elements of L are all 1.  We are now ready to write pseudocode for LU factorization without pivoting, illustrated in Figure 3.3. This method extends the algorithm for forward-substitution by storing the corresponding elements of L under the diagonal rather than zeros. This method has three nested loops and runs in O n3  ≈ 2 3 n3 time. After precomputing this factorization, however, solving A cid:126 x =  cid:126 b only takes O n2  time using forward- and backward-substitution. 3.6 EXERCISES 3.1 Can all matrices A ∈ Rn×n be factored A = LU ? Why or why not? 3.2 Solve the following system of equations using Gaussian elimination, writing the cor-  responding elimination matrix of each step:   cid:18  2  3  4  5  cid:19  cid:18  x  y  cid:19  = cid:18  2 4  cid:19  .  Factor the matrix on the left-hand side as a product A = LU.   62  cid:4  Numerical Algorithms  Figure 3.3 Pseudocode for computing the LU factorization of A ∈ Rn×n, stored in the compact n × n format described in §3.5.3. This algorithm will fail if pivoting is needed.  DH 3.3 Factor the following matrix A as a product A = LU :  1 3 6    2 7 5 −1 1  4  .  3.4 Modify the code in Figure 3.1 to include partial pivoting.  3.5 The discussion in §3.4.3 includes an example of a 2 × 2 matrix A for which Gaussian elimination without pivoting fails. In this case, the issue was resolved by introducing partial pivoting. If exact arithmetic is implemented to alleviate rounding error, does there exist a matrix for which Gaussian elimination fails unless full rather than partial pivoting is implemented? Why or why not?  3.6 Numerical algorithms appear in many components of simulation software for quantum physics. The Schr¨odinger equation and others involve complex numbers in C, however, so we must extend the machinery we have developed for solving linear systems of equations to this case. Recall that a complex number x ∈ C can be written as x = a + bi, where a, b ∈ R and i = √−1. Suppose we wish to solve A cid:126 x =  cid:126 b, but now A ∈ Cn×n and  cid:126 x, cid:126 b ∈ Cn. Explain how a linear solver that takes only real-valued systems can be used to solve this equation. Hint: Write A = A1 + A2i, where A1, A2 ∈ Rn×n. Similarly decompose  cid:126 x and  cid:126 b. In the end you will solve a 2n × 2n real-valued system.  3.7 Suppose A ∈ Rn×n is invertible. Show that A−1 can be obtained via Gaussian elimi-  nation on augmented matrix   cid:0  A In×n  cid:1  .  3.8 Show that if L is an invertible lower-triangular matrix, none of its diagonal elements  can be zero. How does this lemma aﬀect the construction in §3.5.3?  3.9 Show that the inverse of an  invertible  lower-triangular matrix is lower triangular. 3.10 Show that any invertible matrix A ∈ Rn×n with a11 = 0 cannot have a factorization  A = LU for lower-triangular L and upper-triangular U .  function LU-Factorization-Compact A   n,...,   cid:2  Factors A ∈ Rn×n to A = LU in compact format. for p ← 1, 2, . . . , n for r ← p + 1 s ← −arp app arp ← −s for c ← p + 1   cid:2  Choose pivots like in forward-substitution Forward-substitution row  cid:2  Amount to scale row p for forward-substitution  cid:2  L contains −s because it reverses the forward-substitution n,..., Perform forward-substitution arc ← arc + sapc   cid:2    cid:2   return A   Linear Systems and the LU Decomposition  cid:4  63  3.11 Show how the LU factorization of A ∈ Rn×n can be used to compute the determinant  of A.  3.12 For numerical stability and generality, we incorporated pivoting into our methods for Gaussian elimination. We can modify our construction of the LU factorization somewhat to incorporate pivoting as well.  a  Argue that following the steps of Gaussian elimination on a matrix A ∈ Rn×n with partial pivoting can be used to write U = Ln−1Pn−1 ··· L2P2L1P1A, where the Pi’s are permutation matrices, the Li’s are lower triangular, and U is upper triangular.   b  Show that Pi is a permutation matrix that swaps rows i and j for some j ≥ i. Also, argue that Li is the product of matrices of the form In×n + c cid:126 ek cid:126 e cid:62 i where k > i.   c  Suppose j, k > i. Show Pjk In×n + c cid:126 ek cid:126 e cid:62 i   =  In×n + c cid:126 ej cid:126 e cid:62 i  Pjk, where Pjk is a  permutation matrix swapping rows j and k.   d  Combine the previous two parts to show that  Ln−1Pn−1 ··· L2P2L1P1 = Ln−1L cid:48 n−2L cid:48 n−3 ··· L cid:48 1Pn−1 ··· P2P1,  where L cid:48 1, . . . , L cid:48 n−2 are lower triangular.   e  Conclude that A = P LU , where P is a permutation matrix, L is lower triangular,  and U is upper triangular.   f  Extend the method from §3.5.2 for solving A cid:126 x =  cid:126 b when we have factored A = P LU , without aﬀecting the time complexity compared to factorizations A = LU .  3.13  “Block LU decomposition”  Suppose a square matrix M ∈ Rn×n is written in block  form as  M = cid:18  A B C D  cid:19  ,  where A ∈ Rk×k is square and invertible.  a  Show that we can decompose M as the product I  cid:19  cid:18  A  M = cid:18   CA−1  0  I  0 D − CA−1B  cid:19  cid:18  I A−1B  0  0  I   cid:19  .  Here, I denotes an identity matrix of appropriate size.   b  Suppose we decompose A = L1U1 and D − CA−1B = L2U2. Show how to  construct an LU factorization of M given these additional matrices.   c  Use this structure to deﬁne a recursive algorithm for LU factorization; you can assume n = 2 cid:96  for some  cid:96  > 0. How does the eﬃciency of your method compare with that of the LU algorithm introduced in this chapter?  3.14 Suppose A ∈ Rn×n is columnwise diagonally dominant, meaning that for all i,  cid:80 j cid:54 =i aji < aii. Show that Gaussian elimination on A can be carried out with- 3.15 Suppose A ∈ Rn×n is invertible and admits a factorization A = LU with ones along  out pivoting. Is this necessarily a good idea from a numerical standpoint?  the diagonal of L. Show that such a decomposition of A is unique.    C H A P T E R4  Designing and Analyzing Linear Systems  CONTENTS  4.1  Solution of Square Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.1 Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.2 Least-Squares . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.3 Tikhonov Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.4 Image Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.5 Deconvolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.6 Harmonic Parameterization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Special Properties of Linear Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2.1 Positive Deﬁnite Matrices and the Cholesky Factorization . . . . . 4.2.2 Sparsity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2.3 Additional Special Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Sensitivity Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3.1 Matrix and Vector Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3.2 Condition Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  65 66 68 70 71 73 74 75 75 79 80 81 81 84  4.2  4.3  N OW that we can solve linear systems of equations, we will show how to apply this  machinery to several practical problems. The algorithms introduced in the previous  chapter can be applied directly to produce the desired output in each case.  While LU factorization and Gaussian elimination are guaranteed to solve each of these problems in polynomial time, a natural question is whether there exist more eﬃcient or stable algorithms if we know more about the structure of a particular linear system. Thus, we will examine the matrices constructed in the initial examples to reveal special properties that some of them have in common. Designing algorithms speciﬁcally for these classes of matrices will provide speed and numerical advantages, at the cost of generality.  Finally, we will return to concepts from Chapter 2 to design heuristics evaluating how much we can trust the solution  cid:126 x to a linear system A cid:126 x =  cid:126 b, in the presence of rounding and other sources of error. This aspect of analyzing linear systems must be considered when designing reliable and consistent implementations of numerical algorithms.  4.1 SOLUTION OF SQUARE SYSTEMS  In the previous chapter, we only considered square, invertible matrices A when solving A cid:126 x =  cid:126 b. While this restriction does preclude some important cases, many if not most  65   66  cid:4  Numerical Algorithms  Figure 4.1  a  The input for regression, a set of  x k , y k   pairs;  b  a set of basis functions {f1, f2, f3, f4};  c  the output of regression, a set of coeﬃcients c1, . . . , c4  k=1 ckfk x  goes through the data points.  such that the linear combination cid:80 4  applications of linear systems can be posed in terms of square, invertible matrices. We explore a few of these applications below.  4.1.1 Regression  We start with an application from data analysis known as regression. Suppose we carry out a scientiﬁc experiment and wish to understand the structure of the experimental results. One way to model these results is to write the independent variables of a given trial in a vector  cid:126 x ∈ Rn and to think of the dependent variable as a function f   cid:126 x  : Rn → R. Given a few   cid:126 x, f   cid:126 x   pairs, our goal is to predict the output of f   cid:126 x  for a new  cid:126 x without carrying out the full experiment.  Example 4.1  Biological experiment . Suppose we wish to measure the eﬀects of fertilizer, sunlight, and water on plant growth. We could do a number of experiments applying diﬀerent amounts of fertilizer  in cm3 , sunlight  in watts , and water  in ml  to a plant and measuring the height of the plant after a few days. Assuming plant height is a direct function of these variables, we can model our observations as samples from a function f : R3 → R that takes the three parameters we wish to test and outputs the height of the plant at the end of the experimental trial.  In parametric regression, we additionally assume that we know the structure of f ahead  of time. For example, suppose we assume that f is linear:  f   cid:126 x  = a1x1 + a2x2 + ··· + anxn.  Then, our goal becomes more concrete: to estimate the coeﬃcients a1, . . . , an.  We can carry out n experiments to reveal y k  ≡ f   cid:126 x k   for samples  cid:126 x k , where k ∈ {1, . . . , n}. For the linear example, plugging into the formula for f shows a set of statements:  y 1  = f   cid:126 x 1   = a1x 1  y 2  = f   cid:126 x 2   = a1x 2   1 + a2x 1  1 + a2x 2   2 + ··· + anx 1  2 + ··· + anx 2   n  n  ...  y  x  f1  f3  f4  f2   a    b    c    Designing and Analyzing Linear Systems  cid:4  67  Contrary to our earlier notation A cid:126 x =  cid:126 b, the unknowns here are the ai’s, not the  cid:126 x k ’s. With this notational diﬀerence in mind, if we make exactly n observations we can write    −  cid:126 x 1  cid:62  − −  cid:126 x 2  cid:62  −  ...  −  cid:126 x n  cid:62  −      a1 a2 ... an    =  y 1  y 2  ... y n   .    In other words, if we carry out n trials of our experiment and write the independent variables in the columns of a matrix X ∈ Rn×n and the dependent variables in a vector  cid:126 y ∈ Rn, then the coeﬃcients  cid:126 a can be recovered by solving the linear system X cid:62  cid:126 a =  cid:126 y. We can generalize this method to certain nonlinear forms for the function f using an approach illustrated in Figure 4.1. The key is to write f as a linear combination of basis functions. Suppose f   cid:126 x  takes the form  f   cid:126 x  = a1f1  cid:126 x  + a2f2  cid:126 x  + ··· + amfm  cid:126 x ,  where fk : Rn → R and we wish to estimate the parameters ak. Then, by a parallel derivation given m observations of the form  cid:126 x k   cid:55 → y k  we can ﬁnd the parameters by solving:  f1  cid:126 x 1   f1  cid:126 x 2    f2  cid:126 x 1   f2  cid:126 x 2    ...  ...    fm  cid:126 x 1   fm  cid:126 x 2    ...  ··· ··· ··· ···      a1 a2 ... am    =  y 1  y 2  ... y m   .    f1  cid:126 x m    f2  cid:126 x m    fm  cid:126 x m    That is, even if the f ’s are nonlinear, we can learn weights ak using purely linear techniques.  Example 4.2  Linear regression . The system X cid:62  cid:126 a =  cid:126 y from our initial example can be recovered from the general formulation by taking fk  cid:126 x  = xk.  Example 4.3  Polynomial regression . As in Figure 4.1, suppose that we observe a func- tion of a single variable f  x  and wish to write it as an  n − 1 -st degree polynomial  f  x  ≡ a0 + a1x + a2x2 + ··· + an−1xn−1.  Given n pairs x k   cid:55 → y k , we can solve for the parameters  cid:126 a via the system  1 x 1  1 x 2  ... ... 1 x n      x 1  2  x 2  2  ...  ··· ··· ··· ···   x 1  n−1  x 2  n−1  ...      a0 a1 ... an−1    =  y 1  y 2  ... y n   .     x n  2   x n  n−1  In other words, we take fk x  = xk−1 in the general form above. Incidentally, the matrix on the left-hand side of this relationship is known as a Vandermonde matrix.  As an example, suppose we wish to ﬁnd a parabola y = ax2 + bx + c going through     −1, 1 ,  0,−1 , and  2, 7 . We can write the Vandermonde system in two ways: 1 −1  a −1 2 + b −1  + c = 1 a 0 2 + b 0  + c = −1 a 2 2 + b 2  + c = 7  1 −1 1 0 2 1   −1 2 02  22   c b  a  =  7  .   ⇐⇒   Gaussian elimination on this system shows  a, b, c  =  2, 0,−1 , corresponding to the polynomial y = 2x2 − 1.   68  cid:4  Numerical Algorithms  Figure 4.2 Drawbacks of ﬁtting function values exactly:  a  noisy data might be better represented by a simple function rather than a complex curve that touches every data point and  b  the basis functions might not be tuned to the function being sampled. In  b , we ﬁt a polynomial of degree eight to nine samples from f  x  = x but would have been more successful using a basis of line segments.  Example 4.4  Oscillation . A foundational notion from signal processing for audio and images is the decomposition of a function into a linear combination of cosine or sine waves at diﬀerent frequencies. This decomposition of a function deﬁnes its Fourier transform.  As the simplest possible case, we can try to recover the parameters of a single-frequency wave. Suppose we wish to ﬁnd parameters a and φ of a function f  x  = a cos x + φ  given two  x, y  samples satisfying y 1  = f  x 1   and y 2  = f  x 2  . Although this setup as we have written it is nonlinear, we can recover a and φ using a linear system after some mathematical transformations.  From trigonometry, any function of the form g x  = a1 cos x + a2 sin x can be written  g x  = a cos x + φ  after applying the formulae  a = cid:113 a2  1 + a2 2  φ = − arctan  a2 a1  .  We can ﬁnd f  x  by applying the linear method to compute the coeﬃcients a1 and a2 in g x  and then using these formulas to ﬁnd a and φ. This construction can be extended  discrete Fourier transform of f , explored in Exercise 4.15.  to ﬁtting functions of the form f  x  = cid:80 k ak cos x + φk , giving one way to motivate the 4.1.2 Least-Squares The techniques in §4.1.1 provide valuable methods for ﬁnding a continuous f matching a set of data pairs  cid:126 xk  cid:55 → yk exactly. For this reason, they are called interpolation schemes, which we will explore in detail in Chapter 13. They have two related drawbacks, illustrated in Figure 4.2:    There might be some error in measuring the values  cid:126 xk and yk. In this case, a simpler f   cid:126 x  satisfying the approximate relationship f   cid:126 xk  ≈ yk may be acceptable or even preferable to an exact f   cid:126 xk  = yk that goes through each data point.    If there are m functions f1, . . . , fm, then we use exactly m observations  cid:126 xk  cid:55 → yk. Additional observations have to be thrown out, or we have to introduce more fk’s, which can make the resulting function f   cid:126 x  increasingly complicated.   a  Overﬁtting   b  Wrong basis   Designing and Analyzing Linear Systems  cid:4  69  Both of these issues are related to the larger problem of over-ﬁtting: Fitting a function with n degrees of freedom to n data points leaves no room for measurement error.  More broadly, suppose we wish to solve the linear system A cid:126 x =  cid:126 b for  cid:126 x. If we denote  row k of A as  cid:126 r cid:62 k , then the system looks like      x1 x2 ... xn      b1 b2 ... bn    = =  ...  −  cid:126 r cid:62 1 − −  cid:126 r cid:62 2 − ... ... −  cid:126 r cid:62 n −  cid:126 r1 ·  cid:126 x  cid:126 r2 ·  cid:126 x  ...   cid:126 rn ·  cid:126 x    by expanding A cid:126 x  by deﬁnition of matrix multiplication.  From this perspective, each row of the system corresponds to a separate observation of the form  cid:126 rk ·  cid:126 x = bk. That is, an alternative way to interpret the linear system A cid:126 x =  cid:126 b is that it encodes n statements of the form, “The dot product of  cid:126 x with  cid:126 rk is bk.” A tall system A cid:126 x =  cid:126 b where A ∈ Rm×n and m > n encodes more than n of these dot product observations. When we make more than n observations, however, they may be incompatible; as explained §3.1, tall systems do not have to admit a solution. When we cannot solve A cid:126 x =  cid:126 b exactly, we can relax the problem and try to ﬁnd an approximate solution  cid:126 x satisfying A cid:126 x ≈  cid:126 b. One of the most common ways to solve this problem, known as least-squares, is to ask that the residual  cid:126 b − A cid:126 x be as small as possible by minimizing the norm  cid:107  cid:126 b− A cid:126 x cid:107 2. If there is an exact solution  cid:126 x satisfying the tall system A cid:126 x =  cid:126 b, then the minimum of this energy is zero, since norms are nonnegative and in this case  cid:107  cid:126 b − A cid:126 x cid:107 2 =  cid:107  cid:126 b −  cid:126 b cid:107 2 = 0. Example 1.16 to:  Minimizing  cid:107  cid:126 b − A cid:126 x cid:107 2 is the same as minimizing  cid:107  cid:126 b − A cid:126 x cid:107 2 2 =  cid:126 x cid:62 A cid:62 A cid:126 x − 2 cid:126 b cid:62 A cid:126 x +  cid:107  cid:126 b cid:107 2 2.∗  2, which we expanded in   cid:107  cid:126 b − A cid:126 x cid:107 2  The gradient of this expression with respect to  cid:126 x must be zero at its minimum, yielding the following system:  or equivalently,  A cid:62 A cid:126 x = A cid:62  cid:126 b.   cid:126 0 = 2A cid:62 A cid:126 x − 2A cid:62  cid:126 b,  This famous relationship is worthy of a theorem: Theorem 4.1  Normal equations . Minima of the residual norm  cid:107  cid:126 b− A cid:126 x cid:107 2 for A ∈ Rm×n  with no restriction on m or n  satisfy A cid:62 A cid:126 x = A cid:62  cid:126 b.  The matrix A cid:62 A is sometimes called a Gram matrix. If at least n rows of A are linearly independent, then A cid:62 A ∈ Rn×n is invertible. In this case, the minimum residual occurs uniquely at  A cid:62 A −1A cid:62  cid:126 b. Put another way:  In the overdetermined case, solving the least-squares problem A cid:126 x ≈  cid:126 b is equivalent to solving the square system A cid:62 A cid:126 x = A cid:62  cid:126 b.  Via the normal equations, we can solve tall systems with A ∈ Rm×n, m ≥ n, using algo- rithms for square matrices.  ∗If this result is not familiar, it may be valuable to return to the material in §1.4 at this point for review.   70  cid:4  Numerical Algorithms 4.1.3 Tikhonov Regularization  When solving linear systems, the underdetermined case m < n is considerably more diﬃcult to handle due to increased ambiguity. As discussed in §3.1, in this case we lose the possibility of a unique solution to A cid:126 x =  cid:126 b. To choose between the possible solutions, we must make an additional assumption on  cid:126 x to obtain a unique solution, e.g., that it has a small norm or that it contains many zeros. Each such regularizing assumption leads to a diﬀerent solution algorithm. The particular choice of a regularizer may be application-dependent, but here we outline a general approach commonly applied in statistics and machine learning; we will introduce an alternative in §7.2.1 after introducing the singular value decomposition  SVD  of a matrix. 2, the least-squares energy function is insuﬃcient to isolate a single output. For this reason, for ﬁxed α > 0, we might introduce an additional term to the minimization problem:  When there are multiple vectors  cid:126 x that minimize  cid:107 A cid:126 x −  cid:126 b cid:107 2   cid:126 x  cid:107 A cid:126 x −  cid:126 b cid:107 2 min  2 + α cid:107  cid:126 x cid:107 2 2.  This second term is known as a Tikhonov regularizer. When 0 < α  cid:28  1, this optimization eﬀectively asks that among the minimizers of  cid:107 A cid:126 x −  cid:126 b cid:107 2 we would prefer those with small norm  cid:107  cid:126 x cid:107 2; as α increases, we prioritize the norm of  cid:126 x more. This energy is the product of an “Occam’s razor” philosophy: In the absence of more information about  cid:126 x, we might as well choose an  cid:126 x with small entries.  To minimize this new objective, we take the derivative with respect to  cid:126 x and set it equal  to zero:  or equivalently   cid:126 0 = 2A cid:62 A cid:126 x − 2A cid:62  cid:126 b + 2α cid:126 x,  A cid:62 A + αIn×n  cid:126 x = A cid:62  cid:126 b.  So, if we wish to introduce Tikhonov regularization to a linear problem, all we have to do is add α down the diagonal of the Gram matrix A cid:62 A.  When A cid:126 x =  cid:126 b is underdetermined, the matrix A cid:62 A is not invertible. The new Tikhonov  term resolves this issue, since for  cid:126 x  cid:54 =  cid:126 0,   cid:126 x cid:62  A cid:62 A + αIn×n  cid:126 x =  cid:107 A cid:126 x cid:107 2  2 + α cid:107  cid:126 x cid:107 2  2 > 0.  The strict > holds because  cid:126 x  cid:54 =  cid:126 0; it implies that A cid:62 A + αIn×n cannot have a null space vector  cid:126 x. Hence, regardless of A, the Tikhonov-regularized system of equations is invertible. In the language we will introduce in §4.2.1, it is positive deﬁnite. Tikhonov regularization is eﬀective for dealing with null spaces and numerical issues. When A is poorly conditioned, adding this type of regularization can improve conditioning even when the original system was solvable. We acknowledge two drawbacks, however, that can require more advanced algorithms when they are relevant:    The solution  cid:126 x of the Tikhonov-regularized system no longer satisﬁes A cid:126 x =  cid:126 b exactly.   When α is small, the matrix A cid:62 A+αIn×n is invertible but may be poorly conditioned.  Increasing α solves this problem at the cost of less accurate solutions to A cid:126 x =  cid:126 b.  When the columns of A span Rm, an alternative to Tikhonov regularization is to minimize  cid:107  cid:126 x cid:107 2 with the “hard” constraint A cid:126 x =  cid:126 b. Exercise 4.7 shows that this least-norm solution is given by  cid:126 x = A cid:62  AA cid:62  −1 cid:126 b, a similar formula to the normal equations for least-squares.   Designing and Analyzing Linear Systems  cid:4  71  Example 4.5  Tikhonov regularization . Suppose we pose the following linear system:   cid:18  1 1 1.00001  cid:19   cid:126 x = cid:18  1  0.99  cid:19  .  1  This system is solved by  cid:126 x =  1001,−1000 . The scale of this  cid:126 x ∈ R2, however, is much larger than that of any values in the original problem. We can use Tikhonov regularization to encourage smaller values in  cid:126 x that still solve the linear system approximately. In this case, the Tikhonov system is   cid:34  cid:18  1 1 1.00001  cid:19  cid:62  cid:18  1  1 1.00001  cid:19  + αI2×2 cid:35   cid:126 x = cid:18  1  1  1  1  1  1.00001  cid:19  cid:62  cid:18  1  0.99  cid:19  ,  or equivalently,   cid:18  2 + α 2.00001 2.0000200001 + α  cid:19   cid:126 x = cid:18   2.00001  1.99  1.9900099  cid:19  .  As α increases, the regularizer becomes stronger. Some example solutions computed nu- merically are below:  α = 0.00001 −→  cid:126 x ≈  0.499998, 0.494998  α = 0.001 −→  cid:126 x ≈  0.497398, 0.497351  α = 0.1 −→  cid:126 x ≈  0.485364, 0.485366 .  Even with a tiny amount of regularization, these solutions approximate the symmetric near-solution  cid:126 x ≈  0.5, 0.5 , which has much smaller magnitude. If α becomes too large, regularization overtakes the system and  cid:126 x →  0, 0 . 4.1.4 Image Alignment  Suppose we take two photographs of the same scene from diﬀerent positions. One common task in computer vision and graphics is to stitch them together to make a single larger image. To do so, the user  or an automatic system  marks p pairs of points  cid:126 xk,  cid:126 yk ∈ R2 such that for each k the location  cid:126 xk in image one corresponds to the location  cid:126 yk in image two. Then, the software automatically warps the second image onto the ﬁrst or vice versa such that the pairs of points are aligned.  When the camera makes a small motion, a reasonable assumption is that there exists some transformation matrix A ∈ R2×2 and a translation vector  cid:126 b ∈ R2 such that for all k,   cid:126 yk ≈ A cid:126 xk +  cid:126 b.  That is, position  cid:126 x on image one should correspond to position A cid:126 x +  cid:126 b on image two. Figure 4.3 a  illustrates this notation. With this assumption, given a set of corresponding pairs   cid:126 x1,  cid:126 y1 , . . . ,   cid:126 xp,  cid:126 yp , our goal is to compute the A and  cid:126 b matching these points as closely as possible.  Beyond numerical issues, mistakes may have been made while locating the corresponding points, and we must account for approximation error due to the slightly nonlinear camera projection of real-world lenses. To address this potential for misalignment, rather than   72  cid:4  Numerical Algorithms  Figure 4.3  a  The image alignment problem attempts to ﬁnd the parameters A and  cid:126 b of a transformation from one image of a scene to another using labeled keypoints  cid:126 x on the ﬁrst image paired with points  cid:126 y on the second. As an example, keypoints marked in white on the two images in  b  are used to create  c  the aligned image.  requiring that the marked points match exactly, we can ask that they are matched in a least-squares sense. To do so, we solve the following minimization problem:  min A, cid:126 b  p cid:88 k=1   cid:107  A cid:126 xk +  cid:126 b  −  cid:126 yk cid:107 2 2.  This problem has six unknowns total, the four elements of A and the two elements of  cid:126 b. Figure 4.3 b,c  shows typical output for this method; ﬁve keypoints rather than the required three are used to stabilize the output transformation using least-squares.  This objective is a sum of squared linear expressions in the unknowns A and  cid:126 b, and we  will show that it can be minimized using a linear system. Deﬁne  f  A, cid:126 b  ≡ cid:88 k   cid:107  A cid:126 xk +  cid:126 b  −  cid:126 yk cid:107 2 2.  We can simplify f as follows:  f  A, cid:126 b  = cid:88 k   A cid:126 xk +  cid:126 b −  cid:126 yk  cid:62  A cid:126 xk +  cid:126 b −  cid:126 yk  since  cid:107  cid:126 v cid:107 2  2 =  cid:126 v cid:62  cid:126 v  = cid:88 k  cid:104  cid:126 x cid:62 k A cid:62 A cid:126 xk + 2 cid:126 x cid:62 k A cid:62  cid:126 b − 2 cid:126 x cid:62 k A cid:62  cid:126 yk +  cid:126 b cid:62  cid:126 b − 2 cid:126 b cid:62  cid:126 yk +  cid:126 y cid:62 k  cid:126 yk cid:105   where terms with leading 2 apply the fact  cid:126 a cid:62  cid:126 b =  cid:126 b cid:62  cid:126 a.  To ﬁnd where f is minimized, we diﬀerentiate it with respect to  cid:126 b and with respect to the elements of A, and set these derivatives equal to zero. This leads to the following system:  0 = ∇ cid:126 bf  A, cid:126 b  = cid:88 k  cid:104 2A cid:126 xk + 2 cid:126 b − 2 cid:126 yk cid:105  0 = ∇Af  A, cid:126 b  = cid:88 k  cid:104 2A cid:126 xk cid:126 x cid:62 k + 2 cid:126 b cid:126 x cid:62 k − 2 cid:126 yk cid:126 x cid:62 k cid:105  by the identities in Exercise 4.3.  In the second equation, we use the gradient ∇Af to denote the matrix whose entries are   ∇Af  ij ≡ ∂f ∂Aij. Simplifying somewhat, if we deﬁne X ≡  cid:80 k  cid:126 xk cid:126 x cid:62 k ,  cid:126 xsum ≡  cid:80 k  cid:126 xk,   cid:2 x   cid:2 x  cid:2 →  cid:2 y = A cid:2 x +  cid:2 b   cid:2 y   a    b  Input images with keypoints   c  Aligned images   Designing and Analyzing Linear Systems  cid:4  73  Figure4.4 Suppose rather than taking  a  the sharp image, we accidentally take  b  a blurry photo; then, deconvolution can be used to recover  c  a sharp approximation of the original image. The diﬀerence between  a  and  c  is shown in  d ; only high-frequency detail is diﬀerent between the two images.   cid:126 ysum ≡  cid:80 k  cid:126 yk, and C ≡  cid:80 k  cid:126 yk cid:126 x cid:62 k , then the optimal A and  cid:126 b satisfy the following linear  system of equations:  A cid:126 xsum + p cid:126 b =  cid:126 ysum AX +  cid:126 b cid:126 x cid:62 sum = C.  This system is linear in the unknowns A and  cid:126 b; Exercise 4.4 expands it explicitly using a 6 × 6 matrix. This example illustrates a larger pattern in modeling using least-squares. We started by deﬁning a desirable relationship between the unknowns, namely  A cid:126 x +  cid:126 b  −  cid:126 y ≈  cid:126 0. Given a number of data points   cid:126 xk,  cid:126 yk , we designed an objective function f measuring the quality of potential values for the unknowns A and  cid:126 b by summing up the squared norms of expressions we wished to equal zero:  cid:80 k  cid:107  A cid:126 xk +  cid:126 b  −  cid:126 yk cid:107 2 2. Diﬀerentiating this sum gave a linear system of equations to solve for the best possible choice. This pattern is a common source of optimization problems that can be solved linearly and essentially is a subtle application of the normal equations.  4.1.5 Deconvolution  An artist hastily taking pictures of a scene may accidentally take photographs that are slightly out of focus. While a photo that is completely blurred may be a lost cause, if there is only localized or small-scale blurring, we may be able to recover a sharper image using computational techniques. One strategy is deconvolution, explained below; an example test case of the method outlined below is shown in Figure 4.4.  We can think of a grayscale photograph as a point in Rp, where p is the number of pixels it contains; each pixel’s intensity is stored in a diﬀerent dimension. If the photo is in color, we may need red, green, and blue intensities per pixel, yielding a similar representation in R3p. Regardless, most image blurs are linear, including Gaussian convolution or operations averaging a pixel’s intensity with those of its neighbors. In image processing, these operators can be encoded using a matrix G taking a sharp image  cid:126 x to its blurred counterpart G cid:126 x.  Suppose we take a blurry photo  cid:126 x0 ∈ Rp. Then, we could try to recover the underlying  sharp image  cid:126 x ∈ Rp by solving the least-squares problem  min   cid:126 x∈Rp  cid:107  cid:126 x0 − G cid:126 x cid:107 2 2.   a  Sharp   b  Blurry   c  Deconvolved   d  Diﬀerence   74  cid:4  Numerical Algorithms  Figure 4.5  a  An example of a triangle mesh, the typical structure used to represent three-dimensional shapes in computer graphics.  b  In mesh parameterization, we seek a map from a three-dimensional mesh  left  to the two-dimensional image plane  right ; the right-hand side shown here was computed using the method suggested in §4.1.6.  c  The harmonic condition is that the position of vertex v is the average of the positions of its neighbors w1, . . . , w5.  This model assumes that when you blur  cid:126 x with G, you get the observed photo  cid:126 x0. By the same construction as previous sections, if we know G, then this problem can be solved using linear methods.  In practice, this optimization might be unstable since it is solving a diﬃcult inverse problem. In particular, many pairs of distinct images look very similar after they are blurred, making the reverse operation challenging. One way to stabilize the output of deconvolution is to use Tikhonov regularization, from §4.1.3:  cid:126 x∈Rp  cid:107  cid:126 x0 − G cid:126 x cid:107 2  2 + α cid:107  cid:126 x cid:107 2 2.  min  More complex versions may constrain  cid:126 x ≥ 0, since negative intensities are not reasonable, but adding such a constraint makes the optimization nonlinear and better solved by the methods we will introduce starting in Chapter 10.  4.1.6 Harmonic Parameterization  Systems for animation often represent geometric objects in a scene using triangle meshes, sets of points linked together into triangles as in Figure 4.5 a . To give these meshes ﬁne textures and visual detail, a common practice is to store a detailed color texture as an image or photograph, and to map this texture onto the geometry. Each vertex of the mesh then carries not only its geometric location in space but also texture coordinates representing its position on the texture plane.  Mathematically, a mesh can be represented as a collection of n vertices V ≡ {v1, . . . , vn} linked in pairs by edges E ⊆ V × V . Geometrically, each vertex v ∈ V is associated with a location  cid:126 x v  in three-dimensional space R3. Additionally, we will decorate each vertex with a texture coordinate  cid:126 t v  ∈ R2 describing its location in the image plane. It is desirable for these positions to be laid out smoothly to avoid squeezing or stretching the texture relative to the geometry of the surface. With this criterion in mind, the problem of parameterization is to ﬁll in the positions  cid:126 t v  for all the vertices v ∈ V given a few positions laid out manually; desirable mesh parameterizations minimize the geometric distortion of the mesh from its conﬁguration in three-dimensional space to the plane. Surprisingly, many state-of-the-art parameterization algorithms involve little more than a linear solve; we will outline one method originally proposed in [123].  w4  v  w3  w5  w2  w1   a  Triangle mesh   b  Parameterization   c  Harmonic condition   Designing and Analyzing Linear Systems  cid:4  75  For simplicity, suppose that the mesh has disk topology, meaning that it can be mapped to the interior of a circle in the plane, and that we have ﬁxed the location of each vertex on its boundary B ⊆ V . The job of the parameterization algorithm then is to ﬁll in positions for the interior vertices of the mesh. This setup and the output of the algorithm outlined below are shown in Figure 4.5 b .  For a vertex v ∈ V , take N  v  to be the set of neighbors of v on the mesh, given by  Then, for each vertex v ∈ V \B, a reasonable criterion for parameterization quality is that v should be located at the center of its neighbors, illustrated in Figure 4.5 c . Mathematically, this condition is written  N  v  ≡ {w ∈ V :  v, w  ∈ E}.   cid:126 t v  =  1  N  v   cid:88 w∈N  v    cid:126 t w .  Using this expression, we can associate each v ∈ V with a linear condition either ﬁxing its position on the boundary or asking that its assigned position equals the average of its neighbors’ positions. This V ×V  system of equations deﬁnes a harmonic parameterization. The ﬁnal output in Figure 4.5 b  is laid out elastically, evenly distributing vertices on the image plane. Harmonic parameterization has been extended in countless ways to enhance the quality of this result, most prominently by accounting for the lengths of the edges in E as they are realized in three-dimensional space.  4.2 SPECIAL PROPERTIES OF LINEAR SYSTEMS  The examples above provide several contexts in which linear systems of equations are used to model practical computing problems. As derived in the previous chapter, Gaussian elim- ination solves all of these problems in polynomial time, but it remains to be seen whether this is the fastest or most stable technique. With this question in mind, here we look more closely at the matrices from §4.1 to reveal that they have many properties in common. By deriving solution techniques speciﬁc to these classes of matrices, we will design specialized algorithms with better speed and numerical quality.  4.2.1 Positive Definite Matrices and the Cholesky Factorization As shown in Theorem 4.1, solving the least-squares problem A cid:126 x ≈  cid:126 b yields a solution  cid:126 x satisfying the square linear system  A cid:62 A  cid:126 x = A cid:62  cid:126 b. Regardless of A, the matrix A cid:62 A has a few special properties that distinguish it from arbitrary matrices.  First, A cid:62 A is symmetric, since by the identities  AB  cid:62  = B cid:62 A cid:62  and  A cid:62   cid:62  = A,   A cid:62 A  cid:62  = A cid:62  A cid:62   cid:62  = A cid:62 A.  We can express this symmetry index-wise by writing  A cid:62 A ij =  A cid:62 A ji for all indices i, j. This property implies that it is suﬃcient to store only the values of A cid:62 A on or above the diagonal, since the rest of the elements can be obtained by symmetry. Furthermore, A cid:62 A is a positive semideﬁnite matrix, deﬁned below:  Deﬁnition 4.1  Positive  Semi- Deﬁnite . A matrix B ∈ Rn×n is positive semideﬁnite if for all  cid:126 x ∈ Rn,  cid:126 x cid:62 B cid:126 x ≥ 0. B is positive deﬁnite if  cid:126 x cid:62 B cid:126 x > 0 whenever  cid:126 x  cid:54 =  cid:126 0. The following proposition relates this deﬁnition to the matrix A cid:62 A:   76  cid:4  Numerical Algorithms  Proposition 4.1. For any A ∈ Rm×n, the matrix A cid:62 A is positive semideﬁnite. Further- more, A cid:62 A is positive deﬁnite exactly when the columns of A are linearly independent. Proof. We ﬁrst check that A cid:62 A is always positive semideﬁnite. Take any  cid:126 x ∈ Rn. Then,   cid:126 x cid:62  A cid:62 A  cid:126 x =  A cid:126 x  cid:62  A cid:126 x  =  A cid:126 x  ·  A cid:126 x  =  cid:107 A cid:126 x cid:107 2  2 ≥ 0.  To prove the second statement, ﬁrst suppose the columns of A are linearly independent. If A were only semideﬁnite, then there would be an  cid:126 x  cid:54 =  cid:126 0 with  cid:126 x cid:62 A cid:62 A cid:126 x = 0, but as shown above, this would imply  cid:107 A cid:126 x cid:107 2 = 0, or equivalently A cid:126 x =  cid:126 0, contradicting the independence of the columns of A. Conversely, if A has linearly dependent columns, then there exists a  cid:126 y  cid:54 =  cid:126 0 with A cid:126 y =  cid:126 0. In this case,  cid:126 y cid:62 A cid:62 A cid:126 y =  cid:126 0 cid:62  cid:126 0 = 0, and hence A is not positive deﬁnite.  As a corollary, A cid:62 A is invertible exactly when A has linearly independent columns, provid- ing a condition to check whether a least-squares problem admits a unique solution.  Given the prevalence of the least-squares system A cid:62 A cid:126 x = A cid:62  cid:126 b, it is worth considering the possibility of writing faster linear solvers specially designed for this case. In particular, suppose we wish to solve a symmetric positive deﬁnite  SPD  system C cid:126 x =  cid:126 d. For least- squares, we could take C = A cid:62 A and  cid:126 d = A cid:62  cid:126 b, but there also exist many systems that naturally are symmetric and positive deﬁnite without explicitly coming from a least-squares model. We could solve the system using Gaussian elimination or LU factorization, but given the additional structure on C we can do somewhat better.  Aside 4.1  Block matrix notation . Our construction in this section will rely on block matrix notation. This notation builds larger matrices out of smaller ones. For example, suppose A ∈ Rm×n, B ∈ Rm×k, C ∈ Rp×n, and D ∈ Rp×k. Then, we could construct a larger matrix by writing:   cid:18  A B C D  cid:19  ∈ R m+p × n+k .  This “block matrix” is constructed by concatenation. Block matrix notation is convenient, but we must be careful to concatenate matrices with dimensions that match. The mecha- nisms of matrix algebra generally extend to this case, e.g.,   cid:18  A B C D  cid:19  cid:18  E F  G H  cid:19  = cid:18  AE + BG AF + BH CE + DG CF + DH  cid:19  .  We will proceed without checking these identities explicitly, but as an exercise it is worth double-checking that they are true.  We can deconstruct the symmetric positive-deﬁnite matrix C ∈ Rn×n as a block matrix:  C = cid:18  c11   cid:126 v   cid:126 v cid:62   ˜C  cid:19   where c11 ∈ R,  cid:126 v ∈ Rn−1, and ˜C ∈ R n−1 × n−1 . The SPD structure of C provides the following observation:  0 <  cid:126 e cid:62 1 C cid:126 e1 since C is positive deﬁnite and  cid:126 e1  cid:54 =  cid:126 0 = cid:0  1 0 ···  0  cid:1  cid:18  c11   cid:126 v cid:62    cid:126 v  1 0 ... 0  ˜C  cid:19      Designing and Analyzing Linear Systems  cid:4  77  = cid:0  1 0 ···  = c11.  0  cid:1  cid:18  c11  cid:126 v  cid:19   By the strict inequality in the ﬁrst line, we do not have to use pivoting to guarantee that c11  cid:54 = 0 in the ﬁrst step of Gaussian elimination. the form  Continuing with Gaussian elimination, we can apply a forward-substitution matrix E of  E = cid:18  1 √c11   cid:126 r   cid:126 0 cid:62   I n−1 × n−1   cid:19  .  Here, the vector  cid:126 r ∈ Rn−1 contains forward-substitution scaling factors satisfying ri−1c11 = −ci1. Unlike our original construction of Gaussian elimination, we scale row 1 by 1 √c11 for reasons that will become apparent shortly.  By design, after forward-substitution, the form of the product EC is:  EC = cid:18  √c11   cid:126 0   cid:126 v cid:62  √c11  D  cid:19  ,  for some D ∈ R n−1 × n−1 . to the second row, to maintain symmetry, we post-multiply by E cid:62  to obtain ECE cid:62 :  Now, we diverge from the derivation of Gaussian elimination. Rather than moving on  ECE cid:62  =  EC E cid:62    cid:126 0  = cid:18  √c11 = cid:18  1  cid:126 0 cid:62   cid:126 0 D  cid:19  .   cid:126 v cid:62  √c11  D  cid:19  cid:18  1 √c11   cid:126 0   cid:126 r cid:62   I n−1 × n−1   cid:19   The  cid:126 0 cid:62  in the upper right follows from the construction of E as an elimination matrix. Alternatively, an easier if less direct argument is that ECE cid:62  is symmetric, and the lower- left element of the block form for ECE cid:62  is  cid:126 0 by block matrix multiplication. Regardless, we have eliminated the ﬁrst row and the ﬁrst column of C! Furthermore, the remaining submatrix D is also symmetric and positive deﬁnite, as suggested in Exercise 4.2.  Example 4.6  Cholesky factorization, initial step . As a concrete example, consider the following symmetric, positive deﬁnite matrix  We can eliminate the ﬁrst column of C using the elimination matrix E1 deﬁned as:  0 0 1 0  4 5 −4  4 −2 −2 4 −4  14  . C = 1  −→ E1C = E1CE cid:62 1 = 10  .  0 0 4 −2  E1 =  1 2 1 2 −1 0  2 4 −2  2 −1 0 0 −2  10  .  We chose the upper left element of E1 to be 1 2 = 1 √4 = 1 √c11. Following the construction above, we can post-multiply by E cid:62 1 to obtain: 1 0 0 −2  The ﬁrst row and column of this product equal the standard basis vector  cid:126 e1 =  1, 0, 0 .   78  cid:4  Numerical Algorithms  We can repeat this process to eliminate all the rows and columns of C symmetrically.  This method is speciﬁc to symmetric positive-deﬁnite matrices, since    symmetry allowed us to apply the same E to both sides, and   positive deﬁniteness guaranteed that c11 > 0, thus implying that 1 √c11 exists.  Similar to LU factorization, we now obtain a factorization C = LL cid:62  for a lower-triangular matrix L. This factorization is constructed by applying elimination matrices symmetrically using the process above, until we reach  Then, like our construction in §3.5.1, we deﬁne L as a product of lower-triangular matrices:  Ek ··· E2E1CE cid:62 1 E cid:62 2 ··· E cid:62 k = In×n.  L ≡ E−1  1 E−1  2  ··· E−1 k .  The product C = LL cid:62  is known as the Cholesky factorization of C. If taking the square roots causes numerical issues, a related LDL cid:62  factorization, where D is a diagonal matrix, avoids this issue and can be derived from the discussion above; see Exercise 4.6.  Example 4.7  Cholesky factorization, remaining steps . Continuing Example 4.6, we can eliminate the second row and column as follows:  Rescaling brings the symmetric product to the identity matrix I3×3: 1 0 0  0 1 0 0 1 0 0 0 1 3  E2 = E3 = 3 =  0  0 0  1 0 0  0 1 0  1 0 0 1 2 0  0 1 2 1  −→ E2 E1CE cid:62 1  E cid:62 2 =  −→ E3 E2E1CE cid:62 1 E cid:62 2  E cid:62 3 = 0 1   1   2 0 0 −1 1 0 2  1 0 0 2 0 −1  9  . 1  . 3  =  0 1 0  1 0 0  0 1 0  0 0  0 0  0 0  Hence, we have shown E3E2E1CE cid:62 1 E cid:62 2 E cid:62 3 = I3×3. As above, deﬁne:  L = E−1  1 E−1  2 E−1  This matrix L satisﬁes LL cid:62  = C.  2 0 −1 2 2 −1  0 0  3  .  The Cholesky factorization has many practical properties. It takes half the memory to store L from the Cholesky factorization rather than the LU factorization of C. Speciﬁcally, L has n n+1  2 nonzero elements, while the compressed storage of LU factorizations explained in §3.5.3 requires n2 nonzeros. Furthermore, as with the LU decomposition, solving C cid:126 x =  cid:126 d can be accomplished using fast forward- and back-substitution. Finally, the product LL cid:62  is symmetric and positive semideﬁnite regardless of L; if we factored C = LU but made rounding and other mistakes, in degenerate cases the computed product C cid:48  ≈ LU may no longer satisfy these criteria exactly. Code for Cholesky factorization can be very succinct. To derive a particularly compact form, we can work backward from the factorization C = LL cid:62  now that we know such an   Designing and Analyzing Linear Systems  cid:4  79  object exists. Suppose we choose an arbitrary k ∈ {1, . . . , n} and write L in block form isolating the k-th row and column:  Here, since L is lower triangular, L11 and L33 are both lower-triangular square matrices. Applying block matrix algebra to the product C = LL cid:62  shows:  L =  L11  cid:126  cid:96  cid:62 k L31   cid:126 0 0  cid:126 0 cid:62   cid:96 kk  cid:126  cid:96  cid:48 k L33   .  C = LL cid:62  = =  L11  cid:126  cid:96  cid:62 k L31 × ×   cid:126  cid:96  cid:62 k L cid:62 11   cid:126 0 0  cid:126 0 cid:62   cid:96 kk  cid:126  cid:96  cid:48 k L33 ×  cid:126  cid:96 k +  cid:96 2 ×     cid:126  cid:96 k L cid:62 11  cid:126 0 cid:62   cid:96 kk  cid:126 0 0 × kk × ×   .   cid:126  cid:96  cid:62 k  L cid:62 31   cid:126  cid:96  cid:48 k  cid:62  L cid:62 33    We leave out values of the product that are not necessary for our derivation.  Since C = LL cid:62 , from the product above we now have ckk =  cid:126  cid:96  cid:62 k   cid:126  cid:96 k +  cid:96 2  kk, or equivalently   cid:96 kk = cid:113 ckk −  cid:107  cid:126  cid:96 k cid:107 2  2,  where  cid:126  cid:96 k ∈ Rk−1 contains the elements of the k-th row of L to the left of the diagonal. We can choose  cid:96 kk ≥ 0, since scaling columns of L by −1 has no eﬀect on C = LL cid:62 . Furthermore, applying C = LL cid:62  to the middle left element of the product shows L11 cid:126  cid:96 k =  cid:126 ck, where  cid:126 ck contains the elements of C in the same position as  cid:126  cid:96 k. Since L11 is lower triangular, this system can be solved by forward-substitution for  cid:126  cid:96 k!  Synthesizing the formulas above reveals an algorithm for computing the Cholesky factor- ization by iterating k = 1, 2, . . . , n. L11 will already be computed by the time we reach row k, so  cid:126  cid:96 k can be found using forward-substitution. Then,  cid:96 kk is computed directly using the square root formula. We provide pseudocode in Figure 4.6. As with LU factorization, this algorithm runs in O n3  time; more speciﬁcally, Cholesky factorization takes approximately 1 3 n3 operations, half the work needed for LU. 4.2.2 Sparsity  We set out in this section to identify properties of speciﬁc linear systems that can make them solvable using more eﬃcient techniques than Gaussian elimination. In addition to positive deﬁniteness, many linear systems of equations naturally enjoy sparsity, meaning that most of the entries of A in the system A cid:126 x =  cid:126 b are exactly zero. Sparsity can reﬂect particular structure in a given problem, including the following use cases:    In image processing  e.g., §4.1.5 , systems for photo editing express relationships be- tween the values of pixels and those of their neighbors on the image grid. An image may be a point in Rp for p pixels, but when solving A cid:126 x =  cid:126 b for a new size-p image, A ∈ Rp×p may have only O p  rather than O p2  nonzeros since each row only involves a single pixel and its up down left right neighbors.    In computational geometry  e.g., §4.1.6 , shapes are often expressed using collections of triangles linked together into a mesh. Equations for surface smoothing, parameter- ization, and other tasks link values associated with given vertex with only those at their neighbors in the mesh.   80  cid:4  Numerical Algorithms  Figure 4.6 Cholesky factorization for writing C = LL cid:62 , where the input C is sym- metric and positive-deﬁnite and the output L is lower triangular.    In machine learning, a graphical model uses a graph G =  V, E  to express probability distributions over several variables. Each variable corresponds to a node v ∈ V , and edges e ∈ E represent probabilistic dependences. Linear systems in this context often have one row per v ∈ V with nonzeros in columns involving v and its neighbors.  If A ∈ Rn×n is sparse to the point that it contains O n  rather than O n2  nonzero values, there is no reason to store A with n2 values. Instead, sparse matrix storage tech- niques only store the O n  nonzeros in a more reasonable data structure, e.g., a list of row column value triplets. The choice of a matrix data structure involves considering the likely operations that will occur on the matrix, possibly including multiplication, iteration over nonzeros, or iterating over individual rows or columns.  Unfortunately, the LU  and Cholesky  factorizations of a sparse matrix A may not result in sparse L and U matrices; this loss of structure severely limits the applicability of using these methods to solve A cid:126 x =  cid:126 b when A is large but sparse. Thankfully, there are many direct sparse solvers that produce an LU-like factorization without inducing much ﬁll, or addi- tional nonzeros; discussion of these techniques can be found in [32]. Alternatively, iterative techniques can obtain approximate solutions to linear systems using only multiplication by A and A cid:62 . We will derive some of these methods in Chapter 11. 4.2.3 Additional Special Structures  Certain matrices are not only sparse but also structured. For instance, a tridiagonal system of linear equations has the following pattern of nonzero values:    × × × × ×  × × ×  × × × × ×  .    This algorithm destructively replaces C with L  function Cholesky-Factorization C   C   cid:2  Factors C = LLT , assuming C is symmetric and positive deﬁnite L ← for k ← 1, 2, . . . , n   cid:2  Back-substitute to place  cid:3  cid:4  cid:2 k at the beginning of row k for i ← 1, . . . , k − 1   cid:2   s ← 0  cid:2  Iterate over L11; j < i, so the iteration maintains Lkj =   cid:3  cid:4 k j. for j ← 1, . . . , i − 1 : s ← s + LijLkj Lki ←  Lki−s  Lii   cid:2  Current element i of  cid:3  cid:4 k   cid:2  For computing  cid:3  cid:3  cid:4 k cid:3 2  2   cid:2  Apply the formula for  cid:4 kk v ← 0 for j ← 1, . . . , k − 1 : v ← v + L2 Lkk ← √Lkk − v  kj  return L   Designing and Analyzing Linear Systems  cid:4  81  In Exercise 4.8, you will derive a special version of Gaussian elimination for dealing with this banded structure.  In other cases, matrices may not be sparse but might admit a sparse representation. For  example, consider the circulant matrix:    a b c d a b c b  d c d a b d a c   .  This matrix can be stored using only the values a, b, c, d. Specialized techniques for solving systems involving this and other classes of matrices are well-studied and often are more eﬃcient than generic Gaussian elimination.  Broadly speaking, once a problem has been reduced to a linear system A cid:126 x =  cid:126 b, Gaussian elimination provides only one option for how to ﬁnd  cid:126 x. It may be possible to show that the matrix A for the given problem can be solved more easily by identifying special properties like symmetry, positive-deﬁniteness, and sparsity. Interested readers should refer to the discussion in [50] for consideration of numerous cases like the ones above.  4.3 SENSITIVITY ANALYSIS  It is important to examine the matrix of a linear system to ﬁnd out if it has special properties that can simplify the solution process. Sparsity, positive deﬁniteness, symmetry, and so on provide clues to the proper algorithm to use for a particular problem. Even if a given solution strategy might work in theory, however, it is important to understand how well we can trust the output. For instance, due to rounding and other discrete eﬀects, it might be the case that an implementation of Gaussian elimination for solving A cid:126 x =  cid:126 b yields a solution  cid:126 x0 such that 0 <  cid:107 A cid:126 x0 −  cid:126 b cid:107 2  cid:28  1; in other words,  cid:126 x0 only solves the system approximately. One general way to understand the likelihood of error is through sensitivity analysis. To measure sensitivity, we ask what might happen to  cid:126 x if instead of solving A cid:126 x =  cid:126 b, in reality we solve a perturbed system of equations  A + δA  cid:126 x =  cid:126 b + δ cid:126 b. There are two ways of viewing conclusions made by this type of analysis:  1. We may represent A and  cid:126 b inexactly thanks to rounding and other eﬀects. This analysis then shows the best possible accuracy we can expect for  cid:126 x given the mistakes made representing the problem.  2. Suppose our solver generates an inexact approximation  cid:126 x0 to the solution  cid:126 x of A cid:126 x =  cid:126 b. This vector  cid:126 x0 itself is the exact solution of a diﬀerent system A cid:126 x0 =  cid:126 b0 if we deﬁne  cid:126 b0 ≡ A cid:126 x0  be sure you understand why this sentence is not a tautology! . Understanding how changes in  cid:126 x0 aﬀect changes in  cid:126 b0 show how sensitive the system is to slightly incorrect answers.  The discussion here is motivated by the deﬁnitions of forward and backward error in §2.2.1. 4.3.1 Matrix and Vector Norms  Before we can discuss the sensitivity of a linear system, we have to be somewhat careful to deﬁne what it means for a change δ cid:126 x to be “small.” Generally, we wish to measure the length, or norm, of a vector  cid:126 x. We have already encountered the two-norm of a vector:   cid:107  cid:126 x cid:107 2 ≡ cid:113 x2  1 + x2  2 + ··· + x2  n   82  cid:4  Numerical Algorithms  Figure 4.7 The set { cid:126 x ∈ R2 :  cid:107  cid:126 x cid:107  = 1} for diﬀerent vector norms  cid:107  ·  cid:107 . for  cid:126 x ∈ Rn. This norm is popular thanks to its connection to Euclidean geometry, but it is by no means the only norm on Rn. Most generally, we deﬁne a norm as follows: Deﬁnition 4.2  Vector norm . A vector norm is a function  cid:107  ·  cid:107  : Rn → [0,∞  satisfying the following conditions:     cid:107  cid:126 x cid:107  = 0 if and only if  cid:126 x =  cid:126 0  “ cid:107  ·  cid:107  separates points” .    cid:107 c cid:126 x cid:107  = c cid:107  cid:126 x cid:107  for all scalars c ∈ R and vectors  cid:126 x ∈ Rn  “absolute scalability” .    cid:107  cid:126 x +  cid:126 y cid:107  ≤  cid:107  cid:126 x cid:107  +  cid:107  cid:126 y cid:107  for all  cid:126 x,  cid:126 y ∈ Rn  “triangle inequality” .  Other than  cid:107  ·  cid:107 2, there are many examples of norms:    The p-norm  cid:107  cid:126 x cid:107 p, for p ≥ 1, is given by   cid:107  cid:126 x cid:107 p ≡  x1p + x2p + ··· + xnp   1 p .  Of particular importance is the 1-norm, also known as the “Manhattan” or “taxicab” norm:   cid:107  cid:126 x cid:107 1 ≡  xk.  n cid:88 k=1  This norm receives its nickname because it represents the distance a taxicab drives between two points in a city where the roads only run north south and east west.    The ∞-norm  cid:107  cid:126 x cid:107 ∞ is given by   cid:107  cid:126 x cid:107 ∞ ≡ max x1,x2,··· ,xn .  These norms are illustrated in Figure 4.7 by showing the “unit circle” { cid:126 x ∈ R2 :  cid:107  cid:126 x cid:107  = 1} for diﬀerent choices of norm  cid:107  ·  cid:107 ; this visualization shows that  cid:107  cid:126 v cid:107 p ≤  cid:107  cid:126 v cid:107 q when p > q. Despite these geometric diﬀerences, many norms on Rn have similar behavior. In par- ticular, suppose we say two norms are equivalent when they satisfy the following property:  Deﬁnition 4.3  Equivalent norms . Two norms  cid:107 · cid:107  and  cid:107 · cid:107  cid:48  are equivalent if there exist constants clow and chigh such that clow cid:107  cid:126 x cid:107  ≤  cid:107  cid:126 x cid:107  cid:48  ≤ chigh cid:107  cid:126 x cid:107  for all  cid:126 x ∈ Rn.  This condition guarantees that up to some constant factors, all norms agree on which vectors are “small” and “large.” We will state without proof a famous theorem from analysis:   cid:2  ·  cid:2 1   cid:2  ·  cid:2 1.5   cid:2  ·  cid:2 2   cid:2  ·  cid:2 3   cid:2  ·  cid:2 ∞   Designing and Analyzing Linear Systems  cid:4  83  Theorem 4.2  Equivalence of norms on Rn . All norms on Rn are equivalent.  This somewhat surprising result implies that all vector norms have the same rough be- havior, but the choice of a norm for analyzing or stating a particular problem still can make a huge diﬀerence. For instance, on R3 the ∞-norm considers the vector  1000, 1000, 1000  to have the same norm as  1000, 0, 0 , whereas the 2-norm certainly is aﬀected by the additional nonzero values.  Since we perturb not only vectors but also matrices, we must also be able to take the norm of a matrix. The deﬁnition of a matrix norm is nothing more than Deﬁnition 4.2 with matrices in place of vectors. For this reason, we can “unroll” any matrix in Rm×n to a vector in Rnm to adapt any vector norm to matrices. One such norm is the Frobenius norm   cid:107 A cid:107 Fro ≡ cid:115  cid:88 i,j  a2 ij.  Such adaptations of vector norms, however, are not always meaningful. In particular, norms on matrices A constructed this way may not have a clear connection to the action of A on vectors. Since we usually use matrices to encode linear transformations, we would prefer a norm that helps us understand what happens when A is multiplied by diﬀerent vectors  cid:126 x. With this motivation, we can deﬁne the matrix norm induced by a vector norm as follows:  Deﬁnition 4.4  Induced norm . The matrix norm on Rm×n induced by a vector norm  cid:107  ·  cid:107  is given by   cid:107 A cid:107  ≡ max{ cid:107 A cid:126 x cid:107  :  cid:107  cid:126 x cid:107  = 1}.  That is, the induced norm is the maximum length of the image of a unit vector multiplied by A.  This deﬁnition in the case  cid:107 · cid:107  =  cid:107 · cid:107 2 is illustrated in Figure 4.8. Since vector norms satisfy  cid:107 c cid:126 x cid:107  = c cid:107  cid:126 x cid:107 , this deﬁnition is equivalent to requiring  cid:107 A cid:126 x cid:107   cid:107  cid:126 x cid:107    cid:107 A cid:107  ≡ max   cid:126 x∈Rn\{0}  .  From this standpoint, the norm of A induced by  cid:107  ·  cid:107  is the largest achievable ratio of the norm of A cid:126 x relative to that of the input  cid:126 x. This deﬁnition in terms of a maximization problem makes it somewhat complicated to compute the norm  cid:107 A cid:107  given a matrix A and a choice of  cid:107 · cid:107 . Fortunately, the matrix norms induced by many popular vector norms can be simpliﬁed. Some well-known formulae for matrix norms include the following:    The induced one-norm of A is the maximum absolute column sum of A:    The induced ∞-norm of A is the maximum absolute row sum of A:   cid:107 A cid:107 1 = max 1≤j≤n  aij.   cid:107 A cid:107 ∞ = max 1≤i≤m  aij.  m cid:88 i=1  n cid:88 j=1   84  cid:4  Numerical Algorithms  Figure 4.8 The norm  cid:107  ·  cid:107 2 induces a matrix norm measuring the largest distortion  of any point on the unit circle after applying A.    The induced two-norm, or spectral norm, of A ∈ Rn×n is the square root of the largest  eigenvalue of A cid:62 A. That is,   cid:107 A cid:107 2  2 = max{λ : there exists  cid:126 x ∈ Rn with A cid:62 A cid:126 x = λ cid:126 x}.  The ﬁrst two norms are computable directly from the elements of A; the third will require machinery from Chapter 7.  4.3.2 Condition Numbers  Now that we have tools for measuring the action of a matrix, we can deﬁne the condition number of a linear system by adapting our generic deﬁnition of condition numbers from Chapter 2. In this section, we will follow the development presented in [50].  Suppose we are given a perturbation δA of a matrix A and a perturbation δ cid:126 b of the right-hand side of the linear system A cid:126 x =  cid:126 b. For small values of ε, ignoring invertibility technicalities we can write a vector-valued function  cid:126 x ε  as the solution to  Diﬀerentiating both sides with respect to ε and applying the product rule shows:  In particular, when ε = 0 we ﬁnd   A + ε · δA  cid:126 x ε  =  cid:126 b + ε · δ cid:126 b.  δA ·  cid:126 x ε  +  A + ε · δA   dε  d cid:126 x ε   = δ cid:126 b.  δA ·  cid:126 x 0  + A  = δ cid:126 b  d cid:126 x  dε cid:12  cid:12  cid:12 ε=0  or, equivalently,  Using the Taylor expansion, we can write  where we deﬁne  cid:126 x cid:48  0  = d cid:126 x the perturbed system:  cid:107  cid:126 x ε  −  cid:126 x 0  cid:107   =  cid:107 ε cid:126 x cid:48  0  + O ε2  cid:107    cid:107  cid:126 x 0  cid:107    cid:107  cid:126 x 0  cid:107   d cid:126 x  = A−1 δ cid:126 b − δA ·  cid:126 x 0  .  dε cid:12  cid:12  cid:12 ε=0 dε cid:12  cid:12 ε=0. Thus, we can expand the relative error made by solving   cid:126 x ε  =  cid:126 x 0  + ε cid:126 x cid:48  0  + O ε2 ,  by the Taylor expansion above   cid:2 x  A  A cid:2 x   cid:2  cid:2 x cid:2 2 = 1   Designing and Analyzing Linear Systems  cid:4  85  by the derivative we computed    cid:107 A−1δ cid:126 b cid:107  +  cid:107 A−1δA ·  cid:126 x 0   cid:107   + O ε2  by the triangle inequality  cid:107 A + B cid:107  ≤  cid:107 A cid:107  +  cid:107 B cid:107    cid:107  cid:126 x 0  cid:107   =  cid:107 εA−1 δ cid:126 b − δA ·  cid:126 x 0   + O ε2  cid:107  ≤ ε  cid:107  cid:126 x 0  cid:107  ≤ ε cid:107 A−1 cid:107  cid:32   cid:107 δ cid:126 b cid:107  = ε cid:107 A−1 cid:107  cid:107 A cid:107  cid:32   cid:107 δ cid:126 b cid:107  ≤ ε cid:107 A−1 cid:107  cid:107 A cid:107  cid:32   cid:107 δ cid:126 b cid:107  = ε cid:107 A−1 cid:107  cid:107 A cid:107  cid:32  cid:107 δ cid:126 b cid:107   cid:107  cid:126 b cid:107    cid:107 A cid:107  cid:107  cid:126 x 0  cid:107    cid:107 A cid:126 x 0  cid:107   +  cid:107 δA cid:107    cid:107  cid:126 x 0  cid:107   +  cid:107 δA cid:107  cid:33  + O ε2  by the identity  cid:107 AB cid:107  ≤  cid:107 A cid:107  cid:107 B cid:107   +  cid:107 δA cid:107   +  cid:107 δA cid:107    cid:107 A cid:107  cid:33  + O ε2   cid:107 A cid:107  cid:33  + O ε2  since  cid:107 A cid:126 x 0  cid:107  ≤  cid:107 A cid:107  cid:107  cid:126 x 0  cid:107   cid:107 A cid:107  cid:33  + O ε2  since by deﬁnition A cid:126 x 0  =  cid:126 b.  Here we have applied some properties of induced matrix norms which follow from corre- sponding properties for vectors; you will check them explicitly in Exercise 4.12.  The sum D ≡  cid:107 δ cid:126 b cid:107   cid:107  cid:126 b cid:107 + cid:107 δA cid:107   cid:107 A cid:107  appearing in the last equality above encodes the magni- tudes of the perturbations of δA and δ cid:126 b relative to the magnitudes of A and  cid:126 b, respectively. From this standpoint, to ﬁrst order we have bounded the relative error of perturbing the system by ε in terms of the factor κ ≡  cid:107 A cid:107  cid:107 A−1 cid:107 :   cid:107  cid:126 x ε  −  cid:126 x 0  cid:107    cid:107  cid:126 x 0  cid:107   ≤ ε · D · κ + O ε2   Hence, the quantity κ bounds the conditioning of linear systems involving A, inspiring the following deﬁnition:  Deﬁnition 4.5  Matrix condition number . The condition number of A ∈ Rn×n with respect to a given matrix norm  cid:107  ·  cid:107  is  cond A ≡  cid:107 A cid:107  cid:107 A−1 cid:107 .  If A is not invertible, we take cond A ≡ ∞. For nearly any matrix norm, cond A ≥ 1 for all A. Scaling A has no eﬀect on its condition number. Large condition numbers indicate that solutions to A cid:126 x =  cid:126 b are unstable under perturbations of A or  cid:126 b.  If  cid:107  ·  cid:107  is induced by a vector norm and A is invertible, then we have   cid:107 A−1 cid:107  = max  cid:126 x cid:54 = cid:126 0 = max  cid:126 y cid:54 = cid:126 0  = cid:18 min   cid:126 y cid:54 = cid:126 0   cid:107 A−1 cid:126 x cid:107   cid:107  cid:126 x cid:107   cid:107  cid:126 y cid:107   cid:107 A cid:126 y cid:107   cid:107 A cid:126 y cid:107    cid:107  cid:126 y cid:107   cid:19 −1  by deﬁnition  by substituting  cid:126 y = A−1 cid:126 x  by taking the reciprocal.   86  cid:4  Numerical Algorithms  Figure 4.9 The condition number of A measures the ratio of the largest to smallest distortion of any two points on the unit circle mapped under A.  In this case, the condition number of A is given by:  cond A = cid:18 max   cid:126 x cid:54 = cid:126 0   cid:107 A cid:126 x cid:107    cid:107  cid:126 x cid:107   cid:19  cid:18 min   cid:126 y cid:54 = cid:126 0   cid:107 A cid:126 y cid:107    cid:107  cid:126 y cid:107   cid:19 −1  .  In other words, cond A measures the ratio of the maximum to the minimum possible stretch of a vector  cid:126 x under A; this interpretation is illustrated in Figure 4.9.  A desirable stability property of a system A cid:126 x =  cid:126 b is that if A or  cid:126 b is perturbed, the solution  cid:126 x does not change considerably. Our motivation for cond A shows that when the condition number is small, the change in  cid:126 x is small relative to the change in A or  cid:126 b. Oth- erwise, a small change in the parameters of the linear system can cause large deviations in  cid:126 x; this instability can cause linear solvers to make large mistakes in  cid:126 x due to rounding and other approximations during the solution process.  In practice, we might wish to evaluate cond A before solving A cid:126 x =  cid:126 b to see how successful we can expect to be in this process. Taking the norm  cid:107 A−1 cid:107 , however, can be as diﬃcult as computing the full inverse A−1. A subtle “chicken-and-egg problem” exists here: Do we need to compute the condition number of computing matrix condition numbers? A common way out is to bound or approximate cond A using expressions that are easier to evaluate. Lower bounds on the condition number represent optimistic bounds that can be used to cull out particularly bad matrices A, while upper bounds guarantee behavior in the worst case. Condition number estimation is itself an area of active research in numerical analysis. For example, one way to lower-bound the condition number is to apply the identity  cid:107 A−1 cid:126 x cid:107  ≤  cid:107 A−1 cid:107  cid:107  cid:126 x cid:107  as in Exercise 4.12. Then, for any  cid:126 x  cid:54 =  cid:126 0 we can write  cid:107 A−1 cid:107  ≥  cid:107 A−1 cid:126 x cid:107   cid:107  cid:126 x cid:107 . Thus,  cond A =  cid:107 A cid:107  cid:107 A−1 cid:107  ≥  cid:107 A cid:107  cid:107 A−1 cid:126 x cid:107   .   cid:107  cid:126 x cid:107   So, we can bound the condition number by solving A−1 cid:126 x for some vectors  cid:126 x. The necessity of a linear solver to ﬁnd A−1 cid:126 x again creates a circular dependence on the condition number to evaluate the quality of the estimate! After considering eigenvalue problems, in future chapters we will provide more reliable estimates when  cid:107  ·  cid:107  is induced by the two-norm. 4.4 EXERCISES  4.1 Give an example of a sparse matrix whose inverse is dense.  A   Designing and Analyzing Linear Systems  cid:4  87  4.2 Show that the matrix D introduced in §4.2.1 is symmetric and positive deﬁnite. 4.3  “Matrix calculus”  The optimization problem we posed for A ∈ R2×2 in §4.1.4 is an example of a problem where the unknown is a matrix rather than a vector. These prob- lems appear frequently in machine learning and have inspired an alternative notation for diﬀerential calculus better suited to calculations of this sort.   a  Suppose f : Rn×m → R is a smooth function. Justify why the gradient of f can ∂A to notate the  be thought of as an n × m matrix. We will use the notation ∂f gradient of f  A  with respect to A.   b  Take the gradient ∂ ∂A of the following functions, assuming  cid:126 x and  cid:126 y are constant  vectors:   i   cid:126 x cid:62 A cid:126 y  ii   cid:126 x cid:62 A cid:62 A cid:126 x  iii     cid:126 x − A cid:126 y  cid:62 W   cid:126 x − A cid:126 y  for a constant, symmetric matrix W   c  Now, suppose X ∈ Rm×n is a smooth function of a scalar variable X t  : R → Rm×n. We can notate the diﬀerential ∂X ≡ X cid:48  t . For matrix functions X t  and Y  t , justify the following identities:  i  ∂ X + Y   = ∂X + ∂Y  ii  ∂ X cid:62   =  ∂X  cid:62   iii  ∂ XY   =  ∂X Y + X ∂Y    iv  ∂ X−1  = −X−1 ∂X X−1  see Exercise 1.13   After establishing a dictionary of identities like the ones above, taking the derivatives of functions involving matrices becomes a far less cumbersome task. See [99] for a comprehensive reference of identities and formulas in matrix calculus.  4.4 The system of equations for A and  cid:126 b in §4.1.4 must be “unrolled” if we wish to use standard software for solving linear systems of equations to recover the image transformation. Deﬁne  A ≡ cid:18  a11 a12 a21 a22  cid:19   and   cid:126 b ≡ cid:18  b1 b2  cid:19  .  We can combine all our unknowns into a vector  cid:126 u as follows:   cid:126 u ≡    a11 a12 a21 a22 b1 b2  .    Write a matrix M ∈ R6×6 and vector  cid:126 d ∈ R6 so that  cid:126 u—and hence A and  cid:126 b—can be recovered by solving the system M cid:126 u =  cid:126 d for  cid:126 u; you can use any computable temporary variables to simplify your notation, including  cid:126 xsum,  cid:126 ysum, X, and C.   88  cid:4  Numerical Algorithms  4.5 There are many ways to motivate the harmonic parameterization technique from  §4.1.6. One alternative is to consider the Dirichlet energy of a parameterization  ED[ cid:126 t · ] ≡  cid:88  v,w ∈E   cid:107  cid:126 t v  −  cid:126 t w  cid:107 2 2.  minimize ED[ cid:126 t · ] subject to   cid:126 t v  =  cid:126 t0 v  ∀v ∈ B.  Then, we can write an optimization problem given boundary vertex positions  cid:126 t0 ·  : B → R2:  This optimization minimizes the Dirichlet energy ED[·] over all possible parameter- izations  cid:126 t ·  with the constraint that the positions of boundary vertices v ∈ B are ﬁxed. Show that after minimizing this energy, interior vertices v ∈ V \B satisfy the barycenter property introduced in §4.1.6: 1   cid:126 t v  =  N  v   cid:88 w∈N  v    cid:126 t w .  This variational formulation connects the technique to the diﬀerential geometry of smooth maps into the plane.  4.6 A more general version of the Cholesky decomposition that does not require the  computation of square roots is the LDLT decomposition.  a  Suppose A ∈ Rn×n is symmetric and admits an LU factorization  without piv- oting . Show that A can be factored A = LDL cid:62 , where D is diagonal and L is lower triangular. Hint: Take D ≡ U L− cid:62 ; you must show that this matrix is diagonal.   b  Modify the construction of the Cholesky decomposition from §4.2.1 to show how a symmetric, positive-deﬁnite matrix A can be factored A = LDL cid:62  without using any square root operations. Does your algorithm only work when A is positive deﬁnite?  4.7 Suppose A ∈ Rm×n has full rank, where m < n. Show that taking  cid:126 x = A cid:62  AA cid:62  −1 cid:126 b  solves the following optimization problem:  min cid:126 x   cid:107  cid:126 x cid:107 2  subject to A cid:126 x =  cid:126 b.  Furthermore, show that taking α → 0 in the Tikhonov-regularized system from §4.1.3 recovers this choice of  cid:126 x.  4.8 Suppose A ∈ Rn×n is tridiagonal, meaning it can be written:  v1 w1 u2  v2 w2 v3 u3 . . .  A =    w3 . . . un−1  . . . vn−1 wn−1 vn un  .    Show that in this case the system A cid:126 x =  cid:126 b can be solved in O n  time. You can assume that A is diagonally dominant, meaning vi > ui + wi for all i. Hint: Start from Gaussian elimination. This algorithm usually is attributed to [118].   Designing and Analyzing Linear Systems  cid:4  89  4.9 Show how linear techniques can be used to solve the following optimization problem  for A ∈ Rm×n, B ∈ Rk×n,  cid:126 c ∈ Rk:  minimize cid:126 x∈Rn cid:107 A cid:126 x cid:107 2 subject to B cid:126 x =  cid:126 c.  2  4.10 Suppose A ∈ Rn×n admits a Cholesky factorization A = LL cid:62 .   a  Show that A must be positive semideﬁnite.   b  Use this observation to suggest an algorithm for checking if a matrix is positive  semideﬁnite.  4.11 Are all matrix norms on Rm×n equivalent? Why or why not? 4.12 For this problem, assume that the matrix norm  cid:107 A cid:107  for A ∈ Rn×n is induced by a  vector norm  cid:107  cid:126 v cid:107  for  cid:126 v ∈ Rn  but it may be the case that  cid:107  ·  cid:107   cid:54 =  cid:107  ·  cid:107 2 .  a  For A, B ∈ Rn×n, show  cid:107 A + B cid:107  ≤  cid:107 A cid:107  +  cid:107 B cid:107 .  b  For A, B ∈ Rn×n and  cid:126 v ∈ Rn, show  cid:107 A cid:126 v cid:107  ≤  cid:107 A cid:107  cid:107  cid:126 v cid:107  and  cid:107 AB cid:107  ≤  cid:107 A cid:107  cid:107 B cid:107 .  c  For k > 0 and A ∈ Rn×n, show  cid:107 Ak cid:107 1 k ≥ λ for any real eigenvalue λ of A.  d  For A ∈ Rn×n and  cid:107  cid:126 v cid:107 1 ≡ cid:80 i vi, show  cid:107 A cid:107 1 = maxj cid:80 i aij.   e  Prove Gelfand’s formula: ρ A  = limk→∞  cid:107 Ak cid:107 1 k, where ρ A  ≡ max{λi} for eigenvalues λ1, . . . , λm of A. In fact, this formula holds for any matrix norm  cid:107 · cid:107 . 4.13  “Screened Poisson smoothing”  Suppose we sample a function f  x  at n positions x1, x2, . . . , xn, yielding a point  cid:126 y ≡  f  x1 , f  x2 , . . . , f  xn   ∈ Rn. Our measurements might be noisy, however, so a common task in graphics and statistics is to smooth these values to obtain a new vector  cid:126 z ∈ Rn.  a  Provide least-squares energy terms measuring the following:   i  The similarity of  cid:126 y and  cid:126 z.  ii  The smoothness of  cid:126 z.  Hint: We expect f  xi+1  − f  xi  to be small for smooth f .   b  Propose an optimization problem for smoothing  cid:126 y using the terms above to obtain   cid:126 z, and argue that it can be solved using linear techniques.   c  Suppose n is very large. What properties of the matrix in 4.13b might be relevant  in choosing an eﬀective algorithm to solve the linear system?  4.14  “Kernel trick”  In this chapter, we covered techniques for linear and nonlinear para- metric regression. Now, we will develop a least-squares technique for nonparametic regression that is used commonly in machine learning and vision.   a  You can think of the least-squares problem as learning the vector  cid:126 a in a function f   cid:126 x  =  cid:126 a ·  cid:126 x given a number of examples  cid:126 x 1   cid:55 → y 1 , . . . ,  cid:126 x k   cid:55 → y k  and the assumption f   cid:126 x i   ≈ y i . Suppose the columns of X are the vectors  cid:126 x i  and that  cid:126 y is the vector of values y i . Provide the normal equations for recovering  cid:126 a with Tikhonov regularization.   90  cid:4  Numerical Algorithms   b  Show that  cid:126 a ∈ span{ cid:126 x 1 , . . . ,  cid:126 x k } in the Tikhonov-regularized system.  c  Thus, we can write  cid:126 a = c1 cid:126 x 1  + ··· + ck cid:126 x k . Give a k × k linear system of  equations satisﬁed by  cid:126 c assuming X cid:62 X is invertible.   d  One way to do nonlinear regression might be to write a function φ : Rn → Rm and learn fφ  cid:126 x  =  cid:126 a · φ  cid:126 x , where φ may be nonlinear. Deﬁne K  cid:126 x,  cid:126 y  = φ  cid:126 x  · φ  cid:126 y . Assuming we continue to use regularized least-squares as in 4.14a, give an alternative form of fφ that can be computed by evaluating K rather than φ. Hint: What are the elements of X cid:62 X?   e  Consider the following formula from the Fourier transform of the Gaussian:  e−π s−t 2  e−πx2   sin 2πsx  sin 2πtx  + cos 2πsx  cos 2πtx   dx.  = cid:90  ∞  −∞  Suppose we wrote K x, y  = e−π x−y 2 . Explain how this “looks like” φ x  · φ y  for some φ. How does this suggest that the technique from 4.14d can be generalized?  4.15  “Discrete Fourier transform”  This problem deals with complex numbers, so we will  take i ≡ √−1.  a  Suppose θ ∈ R and n ∈ N. Derive de Moivre’s formula by induction on n:   cos θ + i sin θ n = cos nθ + i sin nθ.   b  Euler’s formula uses “complex exponentials” to deﬁne eiθ ≡ cos θ + i sin θ. Write  de Moivre’s formula in this notation.   c  Deﬁne the primitive n-th root of unity as ωn ≡ e−2πi n. The discrete Fourier  transform matrix can be written  Wn ≡  1 √n    1 ωn ω2 n ω3 n ...  1 1 1 1 ... 1 ωn−1  n  1 ω2 n ω4 n ω6 n ...  1 ω3 n ω6 n ω9 n ...  ω2 n−1  n  ω3 n−1  n  n  1  ωn−1 ω2 n−1  n ω3 n−1  n  ··· ··· ··· ··· . . . ··· ω n−1  n−1   ...  n  .    Show that Wn can be written in terms of a Vandermonde matrix, as deﬁned in Example 4.3.   d  The complex conjugate of a + bi ∈ C, where a, b ∈ R, is a + bi ≡ a − bi. Show  that W −1  n = W ∗n , where W ∗n ≡ W  cid:62  n .   e  Suppose n = 2k. In this case, show how Wn can be applied to a vector  cid:126 x ∈ Cn via two applications of Wn 2 and post-processing that takes O n  time. Note: The fast Fourier transform essentially uses this technique recursively to apply Wn in O n log n  time.   f  Suppose that A is circulant, as described in §4.2.3. Show that W ∗n AWn is diagonal.   C H A P T E R5  Column Spaces and QR  CONTENTS  5.1 The Structure of the Normal Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Orthogonality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3 Strategy for Non-orthogonal Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4 Gram-Schmidt Orthogonalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.1 Projections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.2 Gram-Schmidt Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Householder Transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Reduced QR Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  5.5 5.6  91 92 93 94 94 96 99 103  O NE way to interpret the linear problem A cid:126 x =  cid:126 b for  cid:126 x is that we wish to write  cid:126 b as a linear combination of the columns of A with weights given in  cid:126 x. This perspective does not change when we allow A ∈ Rm×n to be non-square, but the solution may not exist or be unique depending on the structure of the column space of A. For these reasons, some techniques for factoring matrices and analyzing linear systems seek simpler representations of the column space of A to address questions regarding solvability and span more explicitly than row-based factorizations like LU.  5.1 THE STRUCTURE OF THE NORMAL EQUATIONS As shown in §4.1.2, a necessary and suﬃcient condition for  cid:126 x to be a solution of the least- squares problem A cid:126 x ≈  cid:126 b is that  cid:126 x must satisfy the normal equations  A cid:62 A  cid:126 x = A cid:62  cid:126 b. This equation shows that least-squares problems can be solved using linear techniques on the matrix A cid:62 A. Methods like Cholesky factorization use the special structure of this matrix to the solver’s advantage.  There is one large problem limiting the use of the normal equations, however. For now,  suppose A is square; then we can write:  cond A cid:62 A =  cid:107 A cid:62 A cid:107  cid:107  A cid:62 A −1 cid:107   ≈  cid:107 A cid:62  cid:107  cid:107 A cid:107  cid:107 A−1 cid:107  cid:107  A cid:62  −1 cid:107  for many choices of  cid:107  ·  cid:107  =  cid:107 A cid:107 2 cid:107 A−1 cid:107 2 =  cond A 2.  That is, the condition number of A cid:62 A is approximately the square of the condition number of A! Thus, while generic linear strategies might work on A cid:62 A when the least-squares problem is “easy,” when the columns of A are nearly linearly dependent these strategies are likely to exhibit considerable error since they do not deal with A directly.  91   92  cid:4  Numerical Algorithms  Figure 5.1 The vectors  cid:126 a1 and  cid:126 a2 nearly coincide; hence, writing  cid:126 b in the span of these vectors is diﬃcult since  cid:126 v1 can be replaced with  cid:126 v2 or vice versa in a linear combination without incurring much error.  Intuitively, a primary reason that cond A cid:62 A can be large is that columns of A might look “similar,” as illustrated in Figure 5.1. Think of each column of A as a vector in Rm. If two columns  cid:126 ai and  cid:126 aj satisfy  cid:126 ai ≈  cid:126 aj, then the least-squares residual length  cid:107  cid:126 b− A cid:126 x cid:107 2 will not suﬀer much if we replace multiples of  cid:126 ai with multiples of  cid:126 aj or vice versa. This wide range of nearly—but not completely—equivalent solutions yields poor conditioning. While the resulting vector  cid:126 x is unstable, however, the product A cid:126 x remains nearly unchanged. If our goal is to write  cid:126 b in the column space of A, either approximate solution suﬃces. In other words, the backward error of multiple near-optimal  cid:126 x’s is similar.  To solve such poorly conditioned problems, we will employ an alternative technique with closer attention to the column space of A rather than employing row operations as in Gaussian elimination. This strategy identiﬁes and deals with such near-dependencies explicitly, bringing about greater numerical stability.  5.2 ORTHOGONALITY  We have identiﬁed why a least-squares problem might be diﬃcult, but we might also ask when it is possible to perform least-squares without suﬀering from conditioning issues. If we can reduce a system to the straightforward case without inducing conditioning problems along the way, we will have found a stable way around the drawbacks explained in §5.1. The easiest linear system to solve is In×n cid:126 x =  cid:126 b, where In×n is the n× n identity matrix: The solution is  cid:126 x =  cid:126 b! We are unlikely to bother using a linear solver to invert this particular linear system on purpose, but we may do so accidentally while solving least-squares. Even when A  cid:54 = In×n—A may not even be square—we may, in particularly lucky circumstances, ﬁnd that the Gram matrix A cid:62 A satisﬁes A cid:62 A = In×n, making least-squares trivial. To avoid confusion with the general case, we will use the variable Q to represent such a matrix satisfying Q cid:62 Q = In×n. While simply praying that Q cid:62 Q = In×n unlikely will yield a useful algorithm, we can examine this case to see how it becomes so favorable. Write the columns of Q as vectors  cid:126 q1,··· ,  cid:126 qn ∈ Rm. Then, the product Q cid:62 Q has the following structure:  cid:126 q1 ·  cid:126 q2  cid:126 q2 ·  cid:126 q2  ··· ··· ··· ···  cid:126 qn ·  cid:126 qn Setting the expression on the right equal to In×n yields the following relationship:  Q cid:62 Q =  −  cid:126 q cid:62 1 − −  cid:126 q cid:62 2 −... −  cid:126 q cid:62 n   cid:126 q1 ·  cid:126 qn  cid:126 q2 ·  cid:126 qn   cid:126 q1 ·  cid:126 q1  cid:126 q2 ·  cid:126 q1       ···  cid:126 qn    =  cid:126 qi ·  cid:126 qj = cid:26  1 when i = j  0 when i  cid:54 = j.     cid:126 qn ·  cid:126 q1   cid:126 qn ·  cid:126 q2    cid:126 q2     cid:126 q1   ...  ...  ...  .   cid:2 a1   cid:2 a2   cid:2 b   Column Spaces and QR  cid:4  93  Figure 5.2 Isometries can  a  rotate and ﬂip vectors but cannot  b  stretch or shear them.  In other words, the columns of Q are unit-length and orthogonal to one another. We say that they form an orthonormal basis for the column space of Q:  Deﬁnition 5.1  Orthonormal; orthogonal matrix . A set of vectors { cid:126 v1,··· ,  cid:126 vk} is or- thonormal if  cid:107  cid:126 vi cid:107 2 = 1 for all i and  cid:126 vi· cid:126 vj = 0 for all i  cid:54 = j. A square matrix whose columns are orthonormal is called an orthogonal matrix.  The standard basis { cid:126 e1,  cid:126 e2, . . . ,  cid:126 en} is an example of an orthonormal basis, and since the columns of the identity matrix In×n are these vectors, In×n is an orthogonal matrix. We motivated our discussion by asking when we can expect Q cid:62 Q = In×n. Now we know that this condition occurs exactly when the columns of Q are orthonormal. Furthermore, if Q is square and invertible with Q cid:62 Q = In×n, then by multiplying both sides of the expression Q cid:62 Q = In×n by Q−1 shows Q−1 = Q cid:62 . Hence, Q cid:126 x =  cid:126 b is equivalent to  cid:126 x = Q cid:62  cid:126 b after multiplying both sides by the transpose Q cid:62 . Orthonormality has a strong geometric interpretation. Recall from Chapter 1 that we can regard two orthogonal vectors  cid:126 a and  cid:126 b as being perpendicular. So, an orthonormal set of vectors is a set of mutually perpendicular unit vectors in Rn. Furthermore, if Q is orthogonal, then its action does not aﬀect the length of vectors:   cid:107 Q cid:126 x cid:107 2  2 =  cid:126 x cid:62 Q cid:62 Q cid:126 x =  cid:126 x cid:62 In×n cid:126 x =  cid:126 x ·  cid:126 x =  cid:107  cid:126 x cid:107 2 2.  Similarly, Q cannot aﬀect the angle between two vectors, since:   Q cid:126 x  ·  Q cid:126 y  =  cid:126 x cid:62 Q cid:62 Q cid:126 y =  cid:126 x cid:62 In×n cid:126 y =  cid:126 x ·  cid:126 y.  From this standpoint, if Q is orthogonal, then the operation  cid:126 x  cid:55 → Q cid:126 x is an isometry of Rn, that is, it preserves lengths and angles. As illustrated in Figure 5.2, Q can rotate or reﬂect vectors but cannot scale or shear them. From a high level, the linear algebra of orthogonal matrices is easier because their actions do not aﬀect the geometry of the underlying space.  5.3 STRATEGY FOR NON-ORTHOGONAL MATRICES Most matrices A encountered when solving A cid:126 x =  cid:126 b or the least-squares problem A cid:126 x ≈  cid:126 b will not be orthogonal, so the machinery of §5.2 does not apply directly. For this reason, we must do some additional computations to connect the general case to the orthogonal one. To this end, we will derive an alternative to LU factorization using orthogonal rather than substitution matrices.  Take a matrix A ∈ Rm×n, and denote its column space as col A; col A is the span of the columns of A. Now, suppose a matrix B ∈ Rn×n is invertible. We make the following observation about the column space of AB relative to that of A:   94  cid:4  Numerical Algorithms  Proposition 5.1  Column space invariance . For any A ∈ Rm×n and invertible B ∈ Rn×n, col A = col AB. Proof. Suppose  cid:126 b ∈ col A. By deﬁnition, there exists  cid:126 x with A cid:126 x =  cid:126 b. If we take  cid:126 y = B−1 cid:126 x, then AB cid:126 y =  AB  ·  B−1 cid:126 x  = A cid:126 x =  cid:126 b, so  cid:126 b ∈ col AB. Conversely, take  cid:126 c ∈ col AB, so there exists  cid:126 y with  AB  cid:126 y =  cid:126 c. In this case, A ·  B cid:126 y  =  cid:126 c, showing that  cid:126 c ∈ col A.  Recall the “elimination matrix” description of Gaussian elimination: We started with a matrix A and applied row operation matrices Ei such that the sequence A, E1A, E2E1A, . . . eventually reduced to more easily solved triangular systems. The proposition above suggests an alternative strategy for situations like least-squares in which we care about the column space of A: Apply column operations to A by post-multiplication until the columns are orthonormal. So long as these operations are invertible, Proposition 5.1 shows that the column spaces of the modiﬁed matrices will be the same as the column space of A.  In the end, we will attempt to ﬁnd a product Q = AE1E2 ··· Ek starting from A and applying invertible operation matrices Ei such that Q is orthonormal. As we have argued above, the proposition shows that col Q = col A. Inverting these operations yields a fac- torization A = QR for R = E−1 1 . The columns of the matrix Q contain an orthonormal basis for the column space of A, and with careful design we can once again make R upper triangular.  k−1 ··· E−1  k E−1  When A = QR, by orthogonality of Q we have A cid:62 A = R cid:62 Q cid:62 QR = R cid:62 R. Making this substitution, the normal equations A cid:62 A cid:126 x = A cid:62  cid:126 b imply R cid:62 R cid:126 x = R cid:62 Q cid:62  cid:126 b, or equivalently R cid:126 x = Q cid:62  cid:126 b. If we design R to be a triangular matrix, then solving the least-squares system A cid:62 A cid:126 x = A cid:62  cid:126 b can be carried out eﬃciently by back-substitution via R cid:126 x = Q cid:62  cid:126 b. 5.4 GRAM-SCHMIDT ORTHOGONALIZATION  Our ﬁrst algorithm for QR factorization follows naturally from our discussion above but may suﬀer from numerical issues. We use it here as an initial example of orthogonalization and then will improve upon it with better operations.  5.4.1 Projections Suppose we have two vectors  cid:126 a and  cid:126 b, with  cid:126 a  cid:54 =  cid:126 0. Then, we could easily ask, “Which multiple of  cid:126 a is closest to  cid:126 b?” Mathematically, this task is equivalent to minimizing  cid:107 c cid:126 a− cid:126 b cid:107 2 2 over all possible c ∈ R. If we think of  cid:126 a and  cid:126 b as n × 1 matrices and c as a 1 × 1 matrix, then this is nothing more than an unconventional least-squares problem  cid:126 a · c ≈  cid:126 b. In this formulation, the normal equations show  cid:126 a cid:62  cid:126 a · c =  cid:126 a cid:62  cid:126 b, or  cid:126 a ·  cid:126 b  cid:126 a ·  cid:126 a   cid:126 a ·  cid:126 b  cid:107  cid:126 a cid:107 2  c =  =  2  .  We denote the resulting projection of  cid:126 b onto  cid:126 a as:  cid:126 a ·  cid:126 b  cid:126 a ·  cid:126 a   cid:126 b ≡ c cid:126 a =  proj cid:126 a   cid:126 a =   cid:126 a ·  cid:126 b  cid:107  cid:126 a cid:107 2  2   cid:126 a.  By design, proj cid:126 a following computation to ﬁnd out:   cid:126 b is parallel to  cid:126 a. What about the remainder  cid:126 b − proj cid:126 a  cid:126 b  =  cid:126 a ·  cid:126 b −  cid:126 a · cid:32   cid:126 a ·  cid:126 b   cid:126 a cid:33  by deﬁnition of proj cid:126 a   cid:126 b   cid:107  cid:126 a cid:107 2  2   cid:126 a ·   cid:126 b − proj cid:126 a   cid:126 b? We can do the   Column Spaces and QR  cid:4  95  Figure 5.3 The projection proj cid:126 a perpendicular to  cid:126 a.   cid:126 b is parallel to  cid:126 a, while the remainder  cid:126 b − proj cid:126 a   cid:126 b is    cid:126 a ·  cid:126 a  by moving the constant outside the dot product   cid:126 a ·  cid:126 b  cid:107  cid:126 a cid:107 2  =  cid:126 a ·  cid:126 b − =  cid:126 a ·  cid:126 b −  cid:126 a ·  cid:126 b since  cid:126 a ·  cid:126 a =  cid:107  cid:126 a cid:107 2 = 0.  2  2  This simpliﬁcation shows we have decomposed  cid:126 b into a component proj cid:126 a another component  cid:126 b − proj cid:126 a hats over vectors with unit length. For any single i, by the projection formula above  Now, suppose that ˆa1, ˆa2,··· , ˆak are orthonormal; for clarity, in this section we will put   cid:126 b orthogonal to  cid:126 a, as illustrated in Figure 5.3.   cid:126 b parallel to  cid:126 a and  projˆai   cid:126 b =  ˆai ·  cid:126 b ˆai.  The denominator does not appear because  cid:107 ˆai cid:107 2 = 1 by deﬁnition. More generally, however, we can project  cid:126 b onto span {ˆa1,··· , ˆak} by minimizing the following energy function E over c1, . . . , ck ∈ R:  E c1, c2, . . . , ck  ≡  cid:107 c1ˆa1 + c2ˆa2 + ··· + ckˆak −  cid:126 b cid:107 2  2  cicj ˆai · ˆaj  − 2 cid:126 b · cid:32  k cid:88 i=1 = k cid:88 j=1 k cid:88 i=1 by applying and expanding  cid:107  cid:126 v cid:107 2 k cid:88 i=1 cid:16 c2 i − 2ci cid:126 b · ˆai cid:17  +  cid:107  cid:126 b cid:107 2  =  ciˆai cid:33  +  cid:126 b ·  cid:126 b  2 =  cid:126 v ·  cid:126 v  2 since the ˆai’s are orthonormal.  The second step here is only valid because of orthonormality of the ˆai’s. At a minimum, the derivative of this energy with respect to ci is zero for every i, yielding the relationship  0 =  ∂E ∂ci  = 2ci − 2 cid:126 b · ˆai =⇒ ci = ˆai ·  cid:126 b.  This argument shows that when ˆa1,··· , ˆak are orthonormal,  projspan {ˆa1,··· ,ˆak}   cid:126 b =  ˆa1 ·  cid:126 b ˆa1 + ··· +  ˆak ·  cid:126 b ˆak.  This formula extends the formula for proj cid:126 a single-vector projections, we must have   cid:126 b, and by a proof identical to the one above for  ˆai ·   cid:126 b − projspan {ˆa1,··· ,ˆak}   cid:126 b  = 0.   cid:2 b   cid:2 b − proj cid:2 a   cid:2 b   cid:2 a  proj cid:2 a   cid:2 b   96  cid:4  Numerical Algorithms  Figure 5.4 The Gram-Schmidt algorithm for orthogonalization. This implementation assumes that the input vectors are linearly independent; in practice, linear depen- dence can be detected by checking for division by zero.  Figure 5.5 Steps of the Gram-Schmidt algorithm on  a  two vectors  cid:126 v1 and  cid:126 v2:  b  ˆa1 is a rescaled version of  cid:126 v1;  c   cid:126 v2 is decomposed into a parallel component  cid:126 p and a residual  cid:126 r;  d   cid:126 r is normalized to obtain ˆa2.  Once again, we separated  cid:126 b into a component parallel to the span of the ˆai’s and a perpen- dicular residual.  5.4.2 Gram-Schmidt Algorithm  Our observations above lead to an algorithm for orthogonalization, or building an orthogonal basis {ˆa1,··· , ˆak} whose span is the same as that of a set of linearly independent but not necessarily orthogonal input vectors { cid:126 v1,··· ,  cid:126 vk}. We add one vector at a time to the basis, starting with  cid:126 v1, then  cid:126 v2, and so on. When  cid:126 vi is added to the current basis {ˆa1, . . . , ˆai−1}, we project out the span of ˆa1, . . . , ˆai−1. By the discussion in §5.4.1 the remaining residual must be orthogonal to the current basis, so we divide this residual by its norm to make it unit-length and add it to the basis. This technique, known as Gram-Schmidt orthogonalization, is detailed in Figure 5.4 and illustrated in Figure 5.5.  Example 5.1  Gram-Schmidt orthogonalization . Suppose we are given  cid:126 v1 =  1, 0, 0 ,  cid:126 v2 =  1, 1, 1 , and  cid:126 v3 =  1, 1, 0 . The Gram-Schmidt algorithm proceeds as follows:  function Gram-Schmidt  cid:2 v1,  cid:2 v2, . . . ,  cid:2 vk    cid:3  Computes an orthonormal basis ˆa1, . . . , ˆak for span{ cid:2 v1, . . . ,  cid:2 vk}  cid:3  Assumes  cid:2 v1, . . . ,  cid:2 vk are linearly independent. ˆa1 ←  cid:2 v1  cid:2  cid:2 v1 cid:2 2 for i ← 2, 3, . . . , k   cid:3  Nothing to project out of the ﬁrst vector   cid:3  Projection of  cid:2 vi onto span{ˆa1, . . . , ˆai−1}  cid:3  Projecting onto orthonormal basis Residual is orthogonal to current basis  cid:3  Normalize this residual and add it to the basis   cid:3   ←  cid:2 0  cid:2 p for j ← 1, 2, . . . , i − 1 ← +   cid:2 vi · ˆaj ˆaj  cid:2 p   cid:2 p  cid:2 r ←  cid:2 vi − p cid:2  ˆai ←  cid:2 r  cid:2  cid:2 r cid:2 2  return {ˆa1, . . . , ˆak}  ˆa1   cid:2 v2   cid:2 v1   cid:2 r   cid:2 p  ˆa2   a  Input   b  Rescaling   c  Projection   d  Normalization   Column Spaces and QR  cid:4  97  1. The ﬁrst vector  cid:126 v1 is already unit-length, so we take ˆa1 =  cid:126 v1 =  1, 0, 0 .  2. Now, we remove the span of ˆa1 from the second vector  cid:126 v2:   cid:126 v2 − projˆa1  cid:126 v2 =  1 1  1  −  1 0  0  ·  1 1  1   1 0  0  =  0 1  1  .  Dividing this vector by its norm, we take ˆa2 =  0, 1 √2, 1 √2 .  3. Finally, we remove span{ˆa1, ˆa2} from  cid:126 v3:  0  ·  1 1  0   1 0  0  −  0 1 √2 1 √2   ·  1 1  0   0 1 √2 1 √2     cid:126 v3 − projspan {ˆa1,ˆa2}  cid:126 v3 1 0  = =  1 1  0  −  .  0 1 2 −1 2  Normalizing this vector yields ˆa3 =  0, 1 √2,−1 √2 .  If we start with a matrix A ∈ Rm×n whose columns are  cid:126 v1,··· ,  cid:126 vk, then we can imple- ment Gram-Schmidt using a series of column operations on A. Dividing column i of A by its norm is equivalent to post-multiplying A by a k×k diagonal matrix. The projection step for column i involves subtracting only multiples of columns j with j < i, and thus this opera- tion can be implemented with an upper-triangular elimination matrix. Thus, our discussion in §5.3 applies, and we can use Gram-Schmidt to obtain a factorization A = QR. When the columns of A are linearly independent, one way to ﬁnd R is as a product R = Q cid:62 A; a more stable approach is to keep track of operations as we did for Gaussian elimination.  Example 5.2  QR factorization . Suppose we construct a matrix whose columns are  cid:126 v1,  cid:126 v2, and  cid:126 v3 from Example 5.1:  The output of Gram-Schmidt orthogonalization can be encoded in the matrix  A ≡ Q ≡  1 0 0  1 1 1  1 1  0  .  1 0 0 0 1 √2 1 √2 0 1 √2 −1 √2   . 0  =  1 1  We can obtain the upper-triangular matrix R in the QR factorization two diﬀerent ways. First, we can compute R after the fact using a product:  R = Q cid:62 A =  1 0 0 0 1 √2 1 √2 0 1 √2 −1 √2    cid:62   1 0 0  1 1 1  As expected, R is upper triangular.  1  1 1 0 √2 1 √2 1 √2 0  0   .   98  cid:4  Numerical Algorithms  Figure 5.6 The modiﬁed Gram-Schmidt algorithm.  We can also return to the steps of Gram-Schmidt orthogonalization to obtain R from the sequence of elimination matrices. A compact way to write the steps of Gram-Schmidt from Example 5.1 is as follows:  0 0  1 1 1 0 1 1  0 1 0  Step 1: Q0 = Step 2: Q1 = Q0E1 = Step 3: Q2 = Q1E2 = 1 =  R = E−1  2 E−1  1 1 1 0 1 1  1 1 0 1 √2 1  0 1 0  0 1 √2 0    1 0 1 0 1 1 √2 0 0 1 √2  0  1 −1 √2 0  1 √2 0 0 −√2 1 −1 0 √2  1 0 0  1 1 0 √2 0 0  0 0 1  0  0  1 0 0 0 1 √2 1 √2 0 1 √2 −1 √2  1 1 0 1 √2 1  0 1 √2 0  1  =  =  =  1 1 0 √2 1 √2 1 √2 0  2 E−1   .  0  1   .  These steps show Q = AE1E2, or equivalently A = QE−1 to compute R:  1 . This gives a second way  The Gram-Schmidt algorithm is well known to be numerically unstable. There are many reasons for this instability that may or may not appear depending on the particular appli- cation. For instance, thanks to rounding and other issues, it might be the case that the ˆai’s are not completely orthogonal after the projection step. Our projection formula for ﬁnding  cid:126 p within the algorithm in Figure 5.4, however, only works when the ˆai’s are orthogonal. For this reason, in the presence of rounding, the projection  cid:126 p of  cid:126 vi becomes less accurate.  One way around this issue is the “modiﬁed Gram-Schmidt”  MGS  algorithm in Fig- ure 5.6, which has similar running time but makes a subtle change in the way projec- tions are computed. Rather than computing the projection  cid:126 p in each iteration i onto span{ˆa1, . . . , ˆai−1}, as soon as ˆai is computed it is projected out of  cid:126 vi+1, . . . ,  cid:126 vk; subse- quently we never have to consider ˆai again. This way, even if the basis globally is not completely orthogonal due to rounding, the projection step is valid since it only projects onto one ˆai at a time. In the absence of rounding, modiﬁed Gram-Schmidt and classical Gram-Schmidt generate identical output.  function Modified-Gram-Schmidt  cid:2 v1,  cid:2 v2, . . . ,  cid:2 vk    cid:3  Computes an orthonormal basis ˆa1, . . . , ˆak for span{ cid:2 v1, . . . ,  cid:2 vk}  cid:3  Assumes  cid:2 v1, . . . ,  cid:2 vk are linearly independent. for i ← 1, 2, . . . , k   cid:3  Normalize the current vector and store in the basis   cid:3  Project ˆai out of the remaining vectors  ˆai ←  cid:2 vi  cid:2  cid:2 vi cid:2 2 for j ← i + 1, i + 2, . . . , k   cid:2 vj ←  cid:2 vj −   cid:2 vj · ˆai ˆai  return {ˆa1, . . . , ˆak}   Column Spaces and QR  cid:4  99  Figure 5.7 A failure mode of the basic and modiﬁed Gram-Schmidt algorithms; here ˆa1 is nearly parallel to  cid:126 v2 and hence the residual  cid:126 r is vanishingly small.  A more subtle instability in the Gram-Schmidt algorithm is not resolved by MGS and can introduce serious numerical instabilities during the subtraction step. Suppose we provide the vectors  cid:126 v1 =  1, 1  and  cid:126 v2 =  1 + ε, 1  as input to Gram-Schmidt for some 0 < ε  cid:28  1. A reasonable basis for span { cid:126 v1,  cid:126 v2} might be { 1, 0 ,  0, 1 }. But, if we apply Gram-Schmidt, we obtain:  1  =   cid:126 p =  ˆa1 =   cid:126 v1  cid:107  cid:126 v1 cid:107  2 + ε  √2 cid:18  1 1  cid:19  2  cid:18  1 1  cid:19   cid:126 r =  cid:126 v2 −  cid:126 p = cid:18  1 + ε 1  cid:19  − −ε  cid:19  . 2 cid:18  ε  2 + ε  =  1  2  cid:18  1 1  cid:19   Taking the norm,  cid:107  cid:126 v2 −  cid:126 p cid:107 2 =  √2 2  · ε, so computing ˆa2 =  1 √2,−1 √2  will require  division by a scalar on the order of ε. Division by small numbers is an unstable numerical operation that generally should be avoided. A geometric interpretation of this case is shown in Figure 5.7.  5.5 HOUSEHOLDER TRANSFORMATIONS In §5.3, we motivated the construction of QR factorization through the use of column operations. This construction is reasonable in the context of analyzing column spaces, but as we saw in our derivation of the Gram-Schmidt algorithm, the resulting numerical techniques can be unstable.  Rather than starting with A and post-multiplying by column operations to obtain Q = AE1 ··· Ek, however, we can also start with A and pre-multiply by orthogonal matrices Qi to obtain Qk ··· Q1A = R. These Q’s will act like row operations, eliminating elements of A until the resulting product R is upper triangular. Thanks to orthogonality of the Q’s, we can write A =  Q cid:62 1 ··· Q cid:62 k  R, obtaining the QR factorization since products and transposes of orthogonal matrices are orthogonal. The row operation matrices we used in Gaussian elimination and LU will not suﬃce for QR factorization since they are not orthogonal. Several alternatives have been suggested; we will introduce a common orthogonal row operation introduced in 1958 by Alston Scott Householder [65].  The space of orthogonal n × n matrices is very large, so we seek a smaller set of pos- sible Qi’s that is easier to work with while still powerful enough to implement elimination   cid:2  cid:2 r cid:2 2 ≈ 0   cid:2 v2  ˆa1   100  cid:4  Numerical Algorithms  Figure 5.8 Reﬂecting  cid:126 b over  cid:126 v.  operations. To develop some intuition, from our geometric discussions in §5.2 we know that orthogonal matrices must preserve angles and lengths, so intuitively they only can rotate and reﬂect vectors. Householder proposed using only reﬂection operations to reduce A to upper-triangular form. A well-known alternative by Givens uses only rotations to accomplish the same task [48] and is explored in Exercise 5.11.  One way to write an orthogonal reﬂection matrix is in terms of projections, as illustrated in Figure 5.8. Suppose we have a vector  cid:126 b that we wish to reﬂect over a vector  cid:126 v. We have  cid:126 b is perpendicular to  cid:126 v. Following the reverse of this shown that the residual  cid:126 r ≡  cid:126 b − proj cid:126 v direction twice shows that the diﬀerence 2proj cid:126 v   cid:126 b −  cid:126 b reﬂects  cid:126 b over  cid:126 v.  We can expand our reﬂection formula as follows:  2proj cid:126 v   cid:126 b −  cid:126 b = 2   cid:126 v ·  cid:126 b  cid:126 v ·  cid:126 v = 2 cid:126 v ·   cid:126 v −  cid:126 b by deﬁnition of projection  cid:126 v cid:62  cid:126 b  cid:126 v cid:62  cid:126 v −  cid:126 b using matrix notation  cid:126 v cid:62  cid:126 v − In×n cid:19  cid:126 b  = cid:18  2 cid:126 v cid:126 v cid:62  ≡ −H cid:126 v cid:126 b, where we deﬁne H cid:126 v ≡ In×n −  2 cid:126 v cid:126 v cid:62   cid:126 v cid:62  cid:126 v  .  By this factorization, we can think of reﬂecting  cid:126 b over  cid:126 v as applying a matrix −H cid:126 v to  cid:126 b; −H cid:126 v has no dependence on  cid:126 b. H cid:126 v without the negative is still orthogonal, and by convention we will use it from now on. Our derivation will parallel that in [58]. Like in forward-substitution, in our ﬁrst step we wish to pre-multiply A by a matrix that takes the ﬁrst column of A, which we will denote  cid:126 a, to some multiple of the ﬁrst identity vector  cid:126 e1. Using reﬂections rather than forward-substitutions, however, we now need to ﬁnd some  cid:126 v, c such that H cid:126 v cid:126 a = c cid:126 e1. Expanding this relationship,  c cid:126 e1 = H cid:126 v cid:126 a, as explained above  = cid:18 In×n −  2 cid:126 v cid:126 v cid:62    cid:126 v cid:62  cid:126 v  cid:19   cid:126 a, by deﬁnition of H cid:126 v  =  cid:126 a − 2 cid:126 v   cid:126 v cid:62  cid:126 a  cid:126 v cid:62  cid:126 v  .  Moving terms around shows   cid:126 v =   cid:126 a − c cid:126 e1  ·   cid:126 v cid:62  cid:126 v 2 cid:126 v cid:62  cid:126 a  .   proj cid:2 v   cid:2 b  −  cid:2 b  cid:2 v   proj cid:2 v   cid:2 b  −  cid:2 b   cid:2 b   cid:2 b  p r o j  cid:2 v  2 proj cid:2 v   cid:2 b  −  cid:2 b   Column Spaces and QR  cid:4  101  In other words, if H cid:126 v accomplishes the desired reﬂection, then  cid:126 v must be parallel to the diﬀerence  cid:126 a − c cid:126 e1. Scaling  cid:126 v does not aﬀect the formula for H cid:126 v, so for now assuming such an H cid:126 v exists we can attempt to choose  cid:126 v =  cid:126 a − c cid:126 e1. If this choice is valid, then substituting  cid:126 v =  cid:126 a − c cid:126 e1 into the simpliﬁed expression shows  Assuming  cid:126 v  cid:54 =  cid:126 0, the coeﬃcient next to  cid:126 v on the right-hand side must be 1, showing:   cid:126 v =  cid:126 v ·   cid:126 v cid:62  cid:126 v 2 cid:126 v cid:62  cid:126 a  .   cid:126 v cid:62  cid:126 v 1 = 2 cid:126 v cid:62  cid:126 a =  cid:107  cid:126 a cid:107 2  2 − 2c cid:126 e1 ·  cid:126 a + c2 2  cid:126 a ·  cid:126 a − c cid:126 e1 ·  cid:126 a  2 − c2 =⇒ c = ± cid:107  cid:126 a cid:107 2.  or, 0 =  cid:107  cid:126 a cid:107 2  After choosing c = ± cid:107  cid:126 a cid:107 2, our steps above are all reversible. We originally set out to ﬁnd  cid:126 v such that H cid:126 v cid:126 a = c cid:126 e1. By taking  cid:126 v =  cid:126 a − c cid:126 e1 with c = ± cid:107  cid:126 a cid:107 2, the steps above show:  We have just accomplished a step similar to forward elimination using orthogonal matrices!  Example 5.3  Householder transformation . Suppose  The ﬁrst column of A has norm √22 + 22 + 12 = 3, so if we take c = 3 we can write:  .  ...  ...  5 2  2 −1 1 2 1  c × × × 0 × × × ... ... 0 × × ×   H cid:126 vA = 0 −2  . A = 1  . 0  = 1  − 3  cid:126 v =  cid:126 a − c cid:126 e1 = 3 2  . 0 −2  = 0 −1 −1  .  2 1 2 −1 −2 1 −2  2 1 2 −1 −2 1 −2  2   H cid:126 v = I3×3 −  2 −1 1 2 1  3 0 0 −1  2 cid:126 v cid:126 v cid:62   cid:126 v cid:62  cid:126 v  −1 2  4 4  5 2  2 2  1 0  =  2  2  1  H cid:126 vA =  1  3  As expected, H cid:62  cid:126 v H cid:126 v = I3×3. Furthermore, H cid:126 v eliminates the ﬁrst column of A:  This choice of  cid:126 v gives elimination matrix  To fully reduce A to upper-triangular form, we must repeat the steps above to eliminate all elements of A below the diagonal. During the k-th step of triangularization, we can take   102  cid:4  Numerical Algorithms  Figure 5.9 Householder QR factorization; the products with H cid:126 v can be carried out in quadratic time after expanding the formula for H cid:126 v in terms of  cid:126 v  see Exercise 5.2 .   cid:126 a to be the k-th column of Qk−1Qk−2 ··· Q1A, where the Qi’s are reﬂection matrices like the one derived above. We can split  cid:126 a into two components:  Here,  cid:126 a1 ∈ Rk−1 and  cid:126 a2 ∈ Rm−k+1. We wish to ﬁnd  cid:126 v such that  Following a parallel derivation to the one above for the case k = 1 shows that   cid:126 a = cid:18   cid:126 a1  cid:126 a2  cid:19  . H cid:126 v cid:126 a =  .  cid:126 a2  cid:19  − c cid:126 ek  cid:126 v = cid:18   cid:126 0   cid:126 a1 c  cid:126 0  accomplishes exactly this transformation when c = ± cid:107  cid:126 a2 cid:107 2. The algorithm for Householder QR, illustrated in Figure 5.9, applies these formulas iteratively, reducing to triangular form in a manner similar to Gaussian elimination. For each column of A, we compute  cid:126 v annihilating the bottom elements of the column and apply H cid:126 v to A. The end result is an upper-triangular matrix R = H cid:126 vn ··· H cid:126 v1 A. Q is given by the product H cid:62  cid:126 v1 ··· H cid:62  cid:126 vn . When m < n, it may be preferable to store Q implicitly as a list of vectors  cid:126 v, which ﬁts in the lower triangle that otherwise would be empty in R.  Example 5.4  Householder QR . Continuing Example 5.3, we split the second column  of H cid:126 vA as  cid:126 a1 =  0  ∈ R1 and  cid:126 a2 =  −1,−1  ∈ R2. We now take c cid:48  = − cid:107  cid:126 a2 cid:107 2 = −√2,  yielding   cid:126 v cid:48  = cid:18   cid:126 0   cid:126 a2  cid:19  − c cid:48  cid:126 e2 =  0 −1  −1  + √2  0 1  0  =  0  −1 + √2 −1    function Householder-QR A   cid:2  Factors A ∈ Rm×n as A = QR.  cid:2  Q ∈ Rm×m is orthogonal and R ∈ Rm×n is upper triangular Q ← Im×m R ← A for k ← 1, 2, . . . , m   cid:3 a ← R cid:3 ek   cid:3 a1,  cid:3 a2  ← Split  cid:3 a,k − 1  c ←  cid:4  cid:3 a2 cid:4 2   cid:2  Isolate column k of R and store it in  cid:3 a  cid:2  Separate oﬀ the ﬁrst k − 1 elements of  cid:3 a  cid:2  Find reﬂection vector  cid:3 v for the Householder matrix H cid:2 v   cid:2  Eliminate elements below the diagonal of the k-th column   cid:3 a2  cid:3  − c cid:3 ek   cid:3 v ← cid:2   cid:3 0 R ← H cid:2 vR Q ← QH cid:2  cid:2 v return Q, R   Column Spaces and QR  cid:4  103  =⇒ H cid:126 v cid:48  =  1 0 0 0 1 √2 1 √2 0 1 √2 −1 √2   .  Applying the two Householder steps reveals an upper-triangular matrix:  R = H cid:126 v cid:48  H cid:126 vA =  4  3  0  0 −√2 3 √2 5 √2  0  0   .  The corresponding Q is given by Q = H cid:62  cid:126 v cid:48  H cid:62  cid:126 v .  5.6 REDUCED QR FACTORIZATION We conclude our discussion by returning to the least-squares problem A cid:126 x ≈  cid:126 b when A ∈ Rm×n is not square. Both algorithms we have discussed in this chapter can factor non- square matrices A into products QR, but the sizes of Q and R are diﬀerent depending on the approach:    When applying Gram-Schmidt, we do column operations on A to obtain Q by orthog- onalization. For this reason, the dimension of A is that of Q, yielding Q ∈ Rm×n and R ∈ Rn×n as a product of elimination matrices.    When using Householder reﬂections, we obtain Q as the product of m × m reﬂection  matrices, leaving R ∈ Rm×n.  Suppose we are in the typical case for least-squares, for which m  cid:29  n. We still prefer to use the Householder method due to its numerical stability, but now the m× m matrix Q might be too large to store. To save space, we can use the upper-triangular structure of R to our advantage. For instance, consider the structure of a 5 × 3 matrix R:  .  R =  × × × 0 × × 0 × 0 0 0 0 0 0 0   A = QR = cid:0  Q1 Q2  cid:1  cid:18  R1  0  cid:19  = Q1R1.  Anything below the upper n × n square of R must be zero, yielding a simpliﬁcation:  Here, Q1 ∈ Rm×n and R1 ∈ Rn×n still contains the upper triangle of R. This is called the “reduced” QR factorization of A, since the columns of Q1 contain a basis for the column space of A rather than for all of Rm; it takes up far less space. The discussion in §5.3 still applies, so the reduced QR factorization can be used for least-squares in a similar fashion. 5.7 EXERCISES  5.1 Use Householder reﬂections to obtain a QR factorization of the matrix A from Exam-  ple 5.2. Do you obtain the same QR factorization as the Gram-Schmidt approach?   104  cid:4  Numerical Algorithms  5.2 Suppose A ∈ Rn×n and  cid:126 v ∈ Rn. Provide pseudocode for computing the product H cid:126 vA in O n2  time. Explain where this method might be used in implementations of Householder QR factorization.  5.3  Adapted from Stanford CS 205A, 2012  Suppose A ∈ Rm×n is factored A = QR.  Show that P0 = Im×m − QQ cid:62  is the projection matrix onto the null space of A cid:62 .  5.4  Adapted from Stanford CS 205A, 2012  Suppose we consider  cid:126 a ∈ Rn as an n × 1  matrix. Write out its “reduced” QR factorization explicitly.  5.5 Show that the Householder matrix H cid:126 v is involutary, meaning H 2   cid:126 v = In×n. What are  the eigenvalues of H cid:126 v?  5.6 Propose a method for ﬁnding the least-norm projection of a vector  cid:126 v onto the column  space of A ∈ Rm×n with m > n.  5.7 Alternatives to the QR factorization:   a  Can a matrix A ∈ Rm×n be factored into A = RQ where R is upper triangular  and Q is orthogonal? How?   b  Can a matrix A ∈ Rm×n be factored into A = QL where L is lower triangular?  5.8 Relating QR and Cholesky factorizations:   a  Take A ∈ Rm×n and suppose we apply the Cholesky factorization to obtain A cid:62 A = LL cid:62 . Deﬁne Q ≡ A L cid:62  −1. Show that the columns of Q are orthogonal.  b  Based on the previous part, suggest a relationship between the Cholesky factor-  ization of A cid:62 A and QR factorization of A.  5.9 Suppose A ∈ Rm×n is rank m with m < n. Suppose we factor  A cid:62  = Q cid:18  R1 0  cid:19  .  Provide a solution  cid:126 x to the underdetermined system A cid:126 x =  cid:126 b in terms of Q and R1.  Hint: Try the square case A ∈ Rn×n ﬁrst, and use the result to guess a form for  cid:126 x. Be careful that you multiply matrices of proper size.  5.10  “Generalized QR,” [2]  One way to generalize the QR factorization of a matrix is to  consider the possibility of factorizing multiple matrices simultaneously.   a  Suppose A ∈ Rn×m and B ∈ Rn×p, with m ≤ n ≤ p. Show that there are orthogonal matrices Q ∈ Rn×n and V ∈ Rp×p as well as a matrix R ∈ Rn×m such that the following conditions hold:   Q cid:62 A = R.   Q cid:62 BV = S, where S can be written S = cid:0  0  ¯S  cid:1  ,  for upper-triangular ¯S ∈ Rn×n.   Column Spaces and QR  cid:4  105    R can be written  R = cid:18  ¯R 0  cid:19  ,  for upper-triangular ¯R ∈ Rm×m.  Hint: Take ¯R to be R1 from the reduced QR factorization of A. Apply RQ factorization to Q cid:62 B; see Exercise 5.7a.   b  Show how to solve the following optimization problem for  cid:126 x and  cid:126 u using the  generalized QR factorization:  min cid:126 x, cid:126 u   cid:107  cid:126 u cid:107 2  subject to A cid:126 x + B cid:126 u =  cid:126 c.  You can assume ¯S and ¯R are invertible.  5.11 An alternative algorithm for QR factorization uses Givens rotations rather than  Householder reﬂections.   a  The 2 × 2 rotation matrix by angle θ ∈ [0, 2π  is given by  Rθ ≡ cid:18  cos θ  − sin θ  sin θ  cos θ  cid:19  .  Show that for a given  cid:126 x ∈ R2, a θ always exists such that Rθ cid:126 x = r cid:126 e1, where r ∈ R and  cid:126 e1 =  1, 0 . Give formulas for cos θ and sin θ that do not require trigonometric functions.   b  The Givens rotation matrix of rows i and j about angle θ is given by  G i, j, θ  ≡    1 ··· 0 ... ... . . . 0 ··· c ... ... 0 ··· −s ... ... 0 ··· 0  ···  ··· . . . ···  ···  ···  0 ··· ... s ... ··· c ... . . . 0 ···  0 ... 0 ... 0 ... 1  ,    where c ≡ cos θ and s ≡ sin θ. In this formula, the c’s appear in positions  i, i  and  j, j  while the s’s appear in positions  i, j  and  j, i . Provide an O n  method for ﬁnding the product G i, j, θ A for A ∈ Rn×n; the matrix A can be modiﬁed in the process.   c  Give an O n3  time algorithm for overwriting A ∈ Rn×n with Q cid:62 A = R, where Q ∈ Rn×n is orthogonal and R ∈ Rn×n is upper triangular. You do not need to store Q.   d  Suggest how you might store Q implicitly if you use the QR method you developed  in the previous part.   e  Suggest an O n3  method for recovering the matrix Q given A and R.   106  cid:4  Numerical Algorithms  5.12  Adapted from [50], §5.1  If  cid:126 x,  cid:126 y ∈ Rm with  cid:107  cid:126 x cid:107 2 =  cid:107  cid:126 y cid:107 2, write an algorithm for  ﬁnding an orthogonal matrix Q such that Q cid:126 x =  cid:126 y.  5.13  “TSQR,” [28]  The QR factorization algorithms we considered can be challenging to extend to parallel architectures like MapReduce. Here, we consider QR factorization of A ∈ Rm×n where m  cid:29  n.  a  Suppose A ∈ R8n×n. Show how to factor A = Q ¯R, where Q ∈ R8n×4n has orthogonal columns and ¯R ∈ R4n×n contains four n × n upper-triangular blocks. Hint: Write   b  Recursively apply your answer from 5.13a to generate a QR factorization of A.   c  Suppose we make the following factorizations:  A =  A1 A2 A3 A4   .  A1 = Q1R1   cid:18  R1 A2  cid:19  = Q2R2  cid:18  R2 A3  cid:19  = Q3R3  cid:18  R3 A4  cid:19  = Q4R4,  where each of the Ri’s are square. Use these matrices to factor A = QR.   d  Suppose we read A row-by-row. Why might the simpliﬁcation in 5.13c be useful for QR factorization of A in this case? You can assume we only need R from the QR factorization.   C H A P T E R6  Eigenvectors  CONTENTS  6.2  6.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.1.1 Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.1.2 Diﬀerential Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.1.3 Spectral Embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Properties of Eigenvectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Symmetric and Positive Deﬁnite Matrices . . . . . . . . . . . . . . . . . . . . . . 6.2.1 Specialized Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2.2 Characteristic Polynomial 6.2.2.1 . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2.2.2 Jordan Normal Form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Computing a Single Eigenvalue . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.3.1 Power Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Inverse Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.3.2 6.3.3 Shifting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Finding Multiple Eigenvalues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.4.1 Deﬂation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.4.2 QR Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.4.3 Krylov Subspace Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Sensitivity and Conditioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  6.4  6.5  6.3  107 108 109 110 112 114 116 116 116 117 117 118 119 120 120 121 126 126  W E turn our attention now to a nonlinear problem about matrices: Finding their  eigenvalues and eigenvectors. Eigenvectors  cid:126 x and corresponding eigenvalues λ of a square matrix A are determined by the equation A cid:126 x = λ cid:126 x. There are many ways to see that the eigenvalue problem is nonlinear. For instance, there is a product of unknowns λ and  cid:126 x. Furthermore, to avoid the trivial solution  cid:126 x =  cid:126 0, we constrain  cid:107  cid:126 x cid:107 2 = 1; this constraint keeps  cid:126 x on the unit sphere, which is not a vector space. Thanks to this structure, algorithms for ﬁnding eigenspaces will be considerably diﬀerent from techniques for solving and analyzing linear systems of equations.  6.1 MOTIVATION  Despite the arbitrary-looking form of the equation A cid:126 x = λ cid:126 x, the problem of ﬁnding eigenvec- tors and eigenvalues arises naturally in many circumstances. To illustrate this point, before presenting algorithms for ﬁnding eigenvectors and eigenvalues we motivate our discussion with a few examples.  It is worth reminding ourselves of one source of eigenvalue problems already considered in Chapter 1. As explained in Example 1.27, the following fact will guide many of our examples:  107   108  cid:4  Numerical Algorithms  Figure 6.1  a  A dataset with correlation between the horizontal and vertical axes;  b  we seek the unit vector ˆv such that all data points are well-approximated by some point along span{ˆv};  c  to ﬁnd ˆv, we can minimize the sum of squared  2 with the constraint that  cid:107 ˆv cid:107 2 = 1.  residual norms cid:80 i  cid:107  cid:126 xi − projˆv  cid:126 xi cid:107 2  When A is symmetric, the eigenvectors of A are the critical points  Many eigenvalue problems are constructed using this fact as a starting point.  of  cid:126 x cid:62 A cid:126 x under the constraint  cid:107  cid:126 x cid:107 2 = 1.  6.1.1 Statistics  Suppose we have machinery for collecting statistical observations about a collection of items. For instance, in a medical study we may collect the age, weight, blood pressure, and heart rate of every patient in a hospital. Each patient i can be represented by a point  cid:126 xi ∈ R4 storing these four values. These statistics may exhibit strong correlations between the diﬀerent dimensions, as in Figure 6.1 a . For instance, patients with higher blood pressures may be likely to have higher weights or heart rates. For this reason, although we collected our data in R4, in reality it may—to some approximate degree—live in a lower-dimensional space capturing the relationships between the diﬀerent dimensions.  For now, suppose that there exists a one-dimensional space approximating our dataset, illustrated in Figure 6.1 b . Then, we expect that there exists some vector  cid:126 v such that each data point  cid:126 xi can be written as  cid:126 xi ≈ ci cid:126 v for a diﬀerent ci ∈ R. From before, we know that the best approximation of  cid:126 xi parallel to  cid:126 v is proj cid:126 v  cid:126 xi. Deﬁning ˆv ≡  cid:126 v  cid:107  cid:126 v cid:107 2, we can write  The magnitude of  cid:126 v does not matter for the problem at hand, since the projection of  cid:126 xi onto any nonzero multiple of ˆv is the same, so it is reasonable to restrict our search to the space of unit vectors ˆv.  Following the pattern of least-squares, we have a new optimization problem:  proj cid:126 v  cid:126 xi =   cid:126 v by deﬁnition   cid:126 xi ·  cid:126 v  cid:126 v ·  cid:126 v  =   cid:126 xi · ˆv ˆv since  cid:126 v ·  cid:126 v =  cid:107  cid:126 v cid:107 2 2.  minimizeˆv  cid:88 i  subject to  cid:107 ˆv cid:107 2 = 1.   cid:107  cid:126 xi − projˆv  cid:126 xi cid:107 2  2   cid:2 xi  ˆv   cid:2 xi − projˆv  cid:2 xi   a  Input data   c  Projection error  {cˆv : c ∈ R}  b  Principal axis   Eigenvectors  cid:4  109  This problem minimizes the sum of squared diﬀerences between the data points  cid:126 xi and their best approximation as a multiple of ˆv, as in Figure 6.1 c . We can simplify our optimization objective using the observations we already have made and some linear algebra:   cid:88 i   cid:107  cid:126 xi − projˆv  cid:126 xi cid:107 2  2 as explained above   cid:107  cid:126 xi −   cid:126 xi · ˆv ˆv cid:107 2  2 = cid:88 i = cid:88 i  cid:0  cid:107  cid:126 xi cid:107 2 = cid:88 i  cid:0  cid:107  cid:126 xi cid:107 2 = const. − cid:88 i = const. −  cid:107 X cid:62 ˆv cid:107 2  2 cid:1  since  cid:107   cid:126 w cid:107 2  2 =  cid:126 w ·  cid:126 w  2 − 2  cid:126 xi · ˆv   cid:126 xi · ˆv  +   cid:126 xi · ˆv 2 cid:107 ˆv cid:107 2 2 −   cid:126 xi · ˆv 2 cid:1  since  cid:107 ˆv cid:107 2 = 1   cid:126 xi · ˆv 2 since the unknown here is ˆv  2, where the columns of X are the vectors  cid:126 xi.  After removing the negative sign, this derivation shows that we can solve an equivalent maximization problem:  maximize  cid:107 X cid:62 ˆv cid:107 2 2 subject to  cid:107 ˆv cid:107 2 2 = 1.  Statisticians may recognize this equivalence as maximizing variance rather than minimizing approximation error. We know  cid:107 X cid:62 ˆv cid:107 2  2 = ˆv cid:62 XX cid:62 ˆv, so by Example 1.27, ˆv is the eigenvector of XX cid:62  with the highest eigenvalue. The vector ˆv is known as the ﬁrst principal component of the dataset.  6.1.2 Differential Equations  Many physical forces can be written as functions of position. For instance, the force exerted by a spring connecting two particles at positions  cid:126 x,  cid:126 y ∈ R3 is k  cid:126 x −  cid:126 y  by Hooke’s Law; such spring forces are used to approximate forces holding cloth together in many simula- tion systems for computer graphics. Even when forces are not linear in position, we often approximate them in a linear fashion. In particular, in a physical system with n particles, we can encode the positions of all the particles simultaneously in a vector  cid:126 X ∈ R3n. Then, the forces in the system might be approximated as  cid:126 F ≈ A  cid:126 X for some matrix A ∈ R3n×3n. Newton’s second law of motion states F = ma, or force equals mass times acceleration. In our context, we can write a diagonal mass matrix M ∈ R3n×3n containing the mass of each particle in the system. Then, the second law can be written as  cid:126 F = M  cid:126 X cid:48  cid:48 , where prime denotes diﬀerentiation in time. By deﬁnition,  cid:126 X cid:48  cid:48  =    cid:126 X cid:48   cid:48 , so after deﬁning  cid:126 V ≡  cid:126 X cid:48  we have a ﬁrst-order system of equations:  d  dt cid:18   cid:126 X   cid:126 V  cid:19  = cid:18   0  M−1A  I3n×3n  0   cid:19  cid:18   cid:126 X  cid:126 V  cid:19  .  Here, we simultaneously compute both positions in  cid:126 X ∈ R3n and velocities  cid:126 V ∈ R3n of all n particles as functions of time; we will explore this reduction in more detail in Chapter 15. Beyond this reduction, diﬀerential equations of the form  cid:126 y cid:48  = B cid:126 y for an unknown func- tion  cid:126 y t  and ﬁxed matrix B appear in simulation of cloth, springs, heat, waves, and other   110  cid:4  Numerical Algorithms  Figure 6.2 Suppose we are given  a  an unsorted database of photographs with some matrix W measuring the similarity between image i and image j.  b  The one- dimensional spectral embedding assigns each photograph i a value xi so that if images i and j are similar, then xi will be close to xj  ﬁgure generated by D. Hyde .  phenomena. Suppose we know eigenvectors  cid:126 y1, . . . ,  cid:126 yk of B satisfying B cid:126 yi = λi cid:126 yi. If we write the initial condition of the diﬀerential equation in terms of the eigenvectors as  then the solution of the diﬀerential equation can be written in closed form:   cid:126 y 0  = c1 cid:126 y1 + ··· + ck cid:126 yk,   cid:126 y t  = c1eλ1t cid:126 y1 + ··· + ckeλkt cid:126 yk.  That is, if we expand the initial conditions of this diﬀerential equation in the eigenvector basis, then we know the solution for all times t ≥ 0 for free; you will check this formula in Exercise 6.1. This formula is not the end of the story for simulation: Finding the complete set of eigenvectors of B is expensive, and B may evolve over time.  6.1.3 Spectral Embedding Suppose we have a collection of n items in a dataset and a measure wij ≥ 0 of how similar elements i and j are; we will assume wij = wji. For instance, maybe we are given a collection of photographs as in Figure 6.2 a  and take wij to be a measure of the amount of overlap between the distributions of colors in photo i and in photo j.  Given the matrix W of wij values, we might wish to sort the photographs based on their similarity to simplify viewing and exploring the collection. That is, we could lay them out on a line so that the pair of photos i and j is close when wij is large, as in Figure 6.2 b . The measurements in wij may be noisy or inconsistent, however, so it may not be obvious how to sort the n photos directly using the n2 values in W .  One way to order the collection would be to assign a number xi to each item i such that similar objects are assigned similar numbers; we can then sort the collection based on the values in  cid:126 x. We can measure how well an assignment of values in  cid:126 x groups similar objects by using the energy function  E  cid:126 x  ≡ cid:88 ij  wij xi − xj 2.  The diﬀerence  xi − xj 2 is small when xi and xj are assigned similar values. Given the weighting wij next to  xi−xj 2, minimizing E  cid:126 x  asks that items i and j with high similarity scores wij get mapped the closest.  x1  xn   a  Database of photos   b  Spectral embedding   Minimizing E  cid:126 x  with no constraints gives a minimum  cid:126 x with E  cid:126 x  = 0: xi = const. for all i. Furthermore, adding a constraint  cid:107  cid:126 x cid:107 2 = 1 does not remove this constant solution: Taking xi = 1 √n for all i gives  cid:107  cid:126 x cid:107 2 = 1 and E  cid:126 x  = 0. To obtain a nontrivial output, we must remove this case as well:  Eigenvectors  cid:4  111  minimize E  cid:126 x  subject to  cid:107  cid:126 x cid:107 2  2 = 1  cid:126 1 ·  cid:126 x = 0.  Our second constraint requires that the sum of elements in  cid:126 x is zero, preventing the choice x1 = x2 = ··· = xn when combined with the  cid:107  cid:126 x cid:107 2 = 1 constraint.  We can simplify the energy in a few steps: wij xi − xj 2 by deﬁnition  E  cid:126 x  = cid:88 ij = cid:88 ij = cid:88 i = 2 cid:126 x cid:62  A − W   cid:126 x where A ≡ diag  cid:126 a .  i − 2xixj + x2 j   i − 2 cid:88 ij  wijxixj + cid:88 j  wij x2  aix2  ajx2  j where  cid:126 a ≡ W cid:126 1, since W  cid:62  = W  We can check that  cid:126 1 is an eigenvector of A − W with eigenvalue 0:   A − W   cid:126 1 = A cid:126 1 − W cid:126 1 =  cid:126 a −  cid:126 a =  cid:126 0.  More interestingly, the eigenvector corresponding to the second -smallest eigenvalue is the minimizer for our constrained problem above! One way to see this fact is to write the Lagrange multiplier function corresponding to this optimization:  Applying Theorem 1.1, at the optimal point we must have:  Λ ≡ 2 cid:126 x cid:62  A − W   cid:126 x − λ 1 −  cid:107  cid:126 x cid:107 2  2  − µ  cid:126 1 ·  cid:126 x .  If we take the dot product of both sides of the ﬁrst expression with  cid:126 1 shows  0 = ∇ cid:126 xΛ = 4 A − W   cid:126 x + 2λ cid:126 x − µ cid:126 1 1 =  cid:107  cid:126 x cid:107 2 0 =  cid:126 1 ·  cid:126 x.  2  0 =  cid:126 1 · [4 A − W   cid:126 x + 2λ cid:126 x − µ cid:126 1] = 4 cid:126 1 cid:62  A − W   cid:126 x − µn since  cid:126 1 ·  cid:126 x = 0 = −µn since A cid:126 1 = W cid:126 1 =  cid:126 a  =⇒ µ = 0.  2 W − A  cid:126 x = λ cid:126 x.  Substituting this new observation into the Lagrange multiplier condition, we ﬁnd:  We explicitly ignore the eigenvalue λ = 0 of W − A corresponding to the eigenvector  cid:126 1, so  cid:126 x must be the eigenvector with the second -smallest eigenvalue. The resulting  cid:126 x is the “spectral embedding” of W onto one dimension, referring to the fact that we call the set of eigenvalues of a matrix its spectrum. Taking more eigenvectors of A − W provides embeddings into higher dimensions.   112  cid:4  Numerical Algorithms 6.2 PROPERTIES OF EIGENVECTORS  We have established a variety of applications in need of eigenspace computation. Before we can explore algorithms for this purpose, however, we will more closely examine the structure of the eigenvalue problem.  We can begin with a few deﬁnitions that likely are evident at this point:  Deﬁnition 6.1  Eigenvalue and eigenvector . An eigenvector  cid:126 x  cid:54 =  cid:126 0 of a matrix A ∈ Rn×n is any vector satisfying A cid:126 x = λ cid:126 x for some λ ∈ R; the corresponding λ is known as an eigenvalue. Complex eigenvalues and eigenvectors satisfy the same relationships with λ ∈ C and  cid:126 x ∈ Cn. Deﬁnition 6.2  Spectrum and spectral radius . The spectrum of A is the set of eigenvalues of A. The spectral radius ρ A  is the maximum value λ over all eigenvalues λ of A.  The scale of an eigenvector is not important. In particular, A c cid:126 x  = cA cid:126 x = cλ cid:126 x = λ c cid:126 x , so c cid:126 x is an eigenvector with the same eigenvalue. For this reason, we can restrict our search to those eigenvectors  cid:126 x with  cid:107  cid:126 x cid:107 2 = 1 without losing any nontrivial structure. Adding this constraint does not completely relieve ambiguity, since ± cid:126 x are both eigenvectors with the same eigenvalue, but this case is easier to detect. The algebraic properties of eigenvectors and eigenvalues are the subject of many mathe- matical studies in themselves. Some basic properties will suﬃce for the discussion at hand, and hence we will mention just a few theorems aﬀecting the design of numerical algorithms. The proofs here parallel the development in [4].  First, we should check that every matrix has at least one eigenvector, so that our search for eigenvectors is not in vain. Our strategy for this and other related problems is to notice that λ is an eigenvalue such that A cid:126 x = λ cid:126 x if and only if  A − λIn×n  cid:126 x =  cid:126 0. That is, λ is an eigenvalue of A exactly when the matrix A − λIn×n is not full-rank. Proposition 6.1  [4], Theorem 2.1 . Every matrix A ∈ Rn×n has at least one  potentially complex  eigenvector.  Proof. Take any vector  cid:126 x ∈ Rn\{ cid:126 0}, and assume A  cid:54 = 0 since this matrix trivially has eigenvalue 0. The set { cid:126 x, A cid:126 x, A2 cid:126 x,··· , An cid:126 x} must be linearly dependent because it contains n + 1 vectors in n dimensions. So, there exist constants c0, . . . , cn ∈ R not all zero such that  cid:126 0 = c0 cid:126 x + c1A cid:126 x + ··· + cnAn cid:126 x. Deﬁne a polynomial  By the Fundamental Theorem of Algebra, there exist m ≥ 1 roots zi ∈ C and c  cid:54 = 0 such that  Applying this factorization,  f  z  ≡ c0 + c1z + ··· + cnzn.  f  z  = c z − z1  z − z2 ···  z − zm .   cid:126 0 = c0 cid:126 x + c1A cid:126 x + ··· + cnAn cid:126 x =  c0In×n + c1A + ··· + cnAn  cid:126 x = c A − z1In×n ···  A − zmIn×n  cid:126 x.  In this form, at least one A − ziIn×n has a null space, since otherwise each term would be invertible, forcing  cid:126 x =  cid:126 0. If we take  cid:126 v to be a nonzero vector in the null space of A− ziIn×n, then by construction A cid:126 v = zi cid:126 v, as needed.   Eigenvectors  cid:4  113  There is one additional fact worth checking to motivate our discussion of eigenvector computation. While it can be the case that a single eigenvalue admits more than one cor- responding eigenvector, when two eigenvectors have diﬀerent eigenvalues they cannot be related in the following sense:  Proposition 6.2  [4], Proposition 2.2 . Eigenvectors corresponding to diﬀerent eigenval- ues must be linearly independent.  Proof. Suppose this is not the case. Then there exist eigenvectors  cid:126 x1, . . . ,  cid:126 xk with distinct eigenvalues λ1, . . . , λk that are linearly dependent. This implies that there are coeﬃcients c1, . . . , ck not all zero with  cid:126 0 = c1 cid:126 x1 + ··· + ck cid:126 xk.  For any two indices i and j, since A cid:126 xj = λj cid:126 xj, we can simplify the product   A − λiIn×n  cid:126 xj = A cid:126 xj − λi cid:126 xj = λj cid:126 xj − λi cid:126 xj =  λj − λi  cid:126 xj.  Pre-multiplying the relationship  cid:126 0 = c1 cid:126 x1 + ··· + ck cid:126 xk by the matrix  A − λ2In×n ···  A − λkIn×n  shows:   cid:126 0 =  A − λ2In×n ···  A − λkIn×n  c1 cid:126 x1 + ··· + ck cid:126 xk  = c1 λ1 − λ2 ···  λ1 − λk  cid:126 x1.  Since all the λi’s are distinct, this shows c1 = 0. The same argument shows that the rest of the ci’s have to be zero, contradicting linear dependence.  This proposition shows that an n×n matrix can have at most n distinct eigenvalues, since a set of n eigenvalues yields n linearly independent vectors. The maximum number of linearly independent eigenvectors corresponding to an eigenvalue λ is the geometric multiplicity of λ. It is not true, however, that a matrix has to have exactly n linearly independent eigenvectors. This is the case for many matrices, which we will call nondefective:  Deﬁnition 6.3  Nondefective . A matrix A ∈ Rn×n is nondefective or diagonalizable if its eigenvectors span Rn.  Example 6.1  Defective matrix . The matrix   cid:18  5  0  2  5  cid:19   has only one linearly independent eigenvector  1, 0 .  We call nondefective matrices diagonalizable for the following reason: If a matrix is nondefective, then it has n eigenvectors  cid:126 x1, . . . ,  cid:126 xn ∈ Rn with corresponding  possibly non- unique  eigenvalues λ1, . . . , λn. Take the columns of X to be the vectors  cid:126 xi, and deﬁne D to be the diagonal matrix with λ1, . . . , λn along the diagonal. Then, we have AX = XD; this relationship is a “stacked” version of A cid:126 xi = λi cid:126 xi. Applying X−1 to both sides, D = X−1AX, meaning A is diagonalized by a similarity transformation A  cid:55 → X−1AX: Deﬁnition 6.4  Similar matrices . Two matrices A and B are similar if there exists T with B = T −1AT.  Similar matrices have the same eigenvalues, since if B cid:126 x = λx, by substituting B = T −1AT we know T −1AT  cid:126 x = λ cid:126 x. Hence, A T  cid:126 x  = λ T  cid:126 x , showing T  cid:126 x is an eigenvector of A with eigenvalue λ. In other words:   114  cid:4  Numerical Algorithms  We can apply all the similarity transformations we want to a  matrix without modifying its set of eigenvalues.  This observation is the foundation of many eigenvector computation methods, which start with a general matrix A and reduce it to a matrix whose eigenvalues are more obvious by applying similarity transformations. This procedure is analogous to applying row operations to reduce a matrix to triangular form for use in solving linear systems of equations.  6.2.1 Symmetric and Positive Definite Matrices Unsurprisingly given our special consideration of Gram matrices A cid:62 A in previous chapters, symmetric and or positive deﬁnite matrices enjoy special eigenvector structure. If we can verify a priori that a matrix is symmetric or positive deﬁnite, specialized algorithms can be used to extract its eigenvectors more quickly.  Our original deﬁnition of eigenvalues allows them to be complex values in C even if A is a real matrix. We can prove, however, that in the symmetric case we do not need complex arithmetic. To do so, we will generalize symmetric matrices to matrices in Cn×n by introducing the set of Hermitian matrices:  Deﬁnition 6.5  Complex conjugate . The complex conjugate of a number z = a + bi ∈ C, where a, b ∈ R, is ¯z ≡ a − bi. The complex conjugate ¯A of a matrix A ∈ Cm×n is the matrix with elements ¯aij.  Deﬁnition 6.6  Conjugate transpose . The conjugate transpose of A ∈ Cm×n is AH ≡ ¯A cid:62 .  Deﬁnition 6.7  Hermitian matrix . A matrix A ∈ Cn×n is Hermitian if A = AH . A symmetric matrix A ∈ Rn×n is automatically Hermitian because it has no complex part. We also can generalize the notion of a dot product to complex vectors by deﬁning an inner product as follows:   cid:104  cid:126 x,  cid:126 y cid:105  ≡ cid:88 i  xi ¯yi,  where  cid:126 x,  cid:126 y ∈ Cn. Once again, this deﬁnition coincides with  cid:126 x ·  cid:126 y when  cid:126 x,  cid:126 y ∈ Rn; in the complex case, however, dot product symmetry is replaced by the condition  cid:104  cid:126 v,  cid:126 w cid:105  =  cid:104   cid:126 w,  cid:126 v cid:105 . We now can prove that it is not necessary to search for complex eigenvalues of symmetric or Hermitian matrices:  Proposition 6.3. All eigenvalues of Hermitian matrices are real.  Proof. Suppose A ∈ Cn×n is Hermitian with A cid:126 x = λ cid:126 x. By scaling, we can assume  cid:107  cid:126 x cid:107 2  cid:104  cid:126 x,  cid:126 x cid:105  = 1. Then:  2 =  λ = λ cid:104  cid:126 x,  cid:126 x cid:105  since  cid:126 x has norm 1 =  cid:104 λ cid:126 x,  cid:126 x cid:105  by linearity of  cid:104 ·,· cid:105  =  cid:104 A cid:126 x,  cid:126 x cid:105  since A cid:126 x = λ cid:126 x =  A cid:126 x  cid:62  cid:126 ¯x by deﬁnition of  cid:104 ·,· cid:105  =  cid:126 x cid:62   ¯A cid:62  cid:126 x  by expanding the product and applying the identity ab = ¯a¯b =  cid:104  cid:126 x, AH cid:126 x cid:105  by deﬁnition of AH and  cid:104 ·,· cid:105    Eigenvectors  cid:4  115  =  cid:104  cid:126 x, A cid:126 x cid:105  since A = AH = ¯λ cid:104  cid:126 x,  cid:126 x cid:105  since A cid:126 x = λ cid:126 x = ¯λ since  cid:126 x has norm 1.  Thus λ = ¯λ, which can happen only if λ ∈ R, as needed. Not only are the eigenvalues of Hermitian  and symmetric  matrices real, but also their eigenvectors must be orthogonal:  Proposition 6.4. Eigenvectors corresponding to distinct eigenvalues of Hermitian matri- ces must be orthogonal. Proof. Suppose A ∈ Cn×n is Hermitian, and suppose λ  cid:54 = µ with A cid:126 x = λ cid:126 x and A cid:126 y = µ cid:126 y. By the previous proposition we know λ, µ ∈ R. Then,  cid:104 A cid:126 x,  cid:126 y cid:105  = λ cid:104  cid:126 x,  cid:126 y cid:105 . But since A is Hermitian we can also write  cid:104 A cid:126 x,  cid:126 y cid:105  =  cid:104  cid:126 x, AH cid:126 y cid:105  =  cid:104  cid:126 x, A cid:126 y cid:105  = µ cid:104  cid:126 x,  cid:126 y cid:105 . Thus, λ cid:104  cid:126 x,  cid:126 y cid:105  = µ cid:104  cid:126 x,  cid:126 y cid:105 . Since λ  cid:54 = µ, we must have  cid:104  cid:126 x,  cid:126 y cid:105  = 0.  Finally, we state  without proof  a crowning result of linear algebra, the Spectral The- orem. This theorem states that all symmetric or Hermitian matrices are non-defective and therefore must have exactly n orthogonal eigenvectors. Theorem 6.1  Spectral Theorem . Suppose A ∈ Cn×n is Hermitian  if A ∈ Rn×n, suppose it is symmetric . Then, A has exactly n orthonormal eigenvectors  cid:126 x1,··· ,  cid:126 xn with  possibly repeated  eigenvalues λ1, . . . , λn. In other words, there exists an orthogonal matrix X of eigenvectors and diagonal matrix D of eigenvalues such that D = X cid:62 AX. This theorem implies that any  cid:126 y ∈ Rn can be decomposed into a linear combination of the eigenvectors of a Hermitian A. Many calculations are easier in this basis, as shown below: Example 6.2  Computation using eigenvectors . Take  cid:126 x1, . . . ,  cid:126 xn ∈ Rn to be the unit- length eigenvectors of a symmetric invertible matrix A ∈ Rn×n with corresponding eigen- values λ1, . . . , λn ∈ R. Suppose we wish to solve A cid:126 y =  cid:126 b. By the Spectral Theorem, we can decompose  cid:126 b = c1 cid:126 x1 + ··· + cn cid:126 xn, where ci =  cid:126 b ·  cid:126 xi by orthonormality. Then,   cid:126 y =  c1 λ1   cid:126 x1 + ··· +  cn λn   cid:126 xn.  The fastest way to check this formula is to multiply  cid:126 y by A and make sure we recover  cid:126 b:  A cid:126 y = A cid:18  c1   cid:126 x1 + ··· + cn λn  λ1 A cid:126 x1 + ··· +  =  c1 λ1  cn λn   cid:126 xn cid:19   A cid:126 xn  = c1 cid:126 x1 + ··· + cn cid:126 xn since A cid:126 xk = λk cid:126 xk for all k =  cid:126 b, as desired.  The calculation above has both positive and negative implications. It shows that given the eigenvectors and eigenvalues of symmetric matrix A, operations like inversion become straightforward. On the ﬂip side, this means that ﬁnding the full set of eigenvectors of a symmetric matrix A is “at least” as diﬃcult as solving A cid:126 x =  cid:126 b.  Returning from our foray into the complex numbers, we revisit the real numbers to prove  one ﬁnal useful fact about positive deﬁnite matrices:   116  cid:4  Numerical Algorithms  Proposition 6.5. All eigenvalues of positive deﬁnite matrices are positive.  Proof. Take A ∈ Rn×n positive deﬁnite, and suppose A cid:126 x = λ cid:126 x with  cid:107  cid:126 x cid:107 2 = 1. By positive deﬁniteness, we know  cid:126 x cid:62 A cid:126 x > 0. But,  cid:126 x cid:62 A cid:126 x =  cid:126 x cid:62  λ cid:126 x  = λ cid:107  cid:126 x cid:107 2  2 = λ, as needed.  This property is not nearly as remarkable as those associated with symmetric or Her- mitian matrices, but it helps order the eigenvalues of A. Positive deﬁnite matrices enjoy the property that the eigenvalue with smallest absolute value is also the eigenvalue closest to zero, and the eigenvalue with largest absolute value is the one farthest from zero. This property inﬂuences methods that seek only a subset of the eigenvalues of a matrix, usually at one of the two ends of its spectrum.  6.2.2 Specialized Properties  We mention some specialized properties of eigenvectors and eigenvalues that inﬂuence more advanced methods for their computation. They largely will not ﬁgure into our subsequent discussion, so this section can be skipped if readers lack suﬃcient background.  6.2.2.1 Characteristic Polynomial The determinant of a matrix det A satisﬁes det A  cid:54 = 0 if and only if A is invertible. Thus, one way to ﬁnd eigenvalues of a matrix is to ﬁnd roots of the characteristic polynomial  pA λ  = det A − λIn×n .  We have chosen to avoid determinants in our discussion of linear algebra, but simplifying pA reveals that it is an n-th degree polynomial in λ.  From this construction, we can deﬁne the algebraic multiplicity of an eigenvalue λ as its multiplicity as a root of pA. The algebraic multiplicity of any eigenvalue is at least as large as its geometric multiplicity. If the algebraic multiplicity is 1, the root is called simple, because it corresponds to a single eigenvector that is linearly independent from any others. Eigenvalues for which the algebraic and geometric multiplicities are not equal are called defective, since the corresponding matrix must also be defective in the sense of Deﬁnition 6.3.  In numerical analysis, it is common to avoid using the determinant of a matrix. While it is a convenient theoretical construction, its practical applicability is limited. Determinants are diﬃcult to compute. In fact, most eigenvalue algorithms do not attempt to ﬁnd roots of pA directly, since doing so would require evaluation of a determinant. Furthermore, the determinant det A has nothing to do with the conditioning of A, so a near-but-not-exactly zero determinant of det A − λIn×n  might not show that λ is nearly an eigenvalue of A. 6.2.2.2 Jordan Normal Form  We can only diagonalize a matrix when it has a full eigenspace. All matrices, however, are similar to a matrix in Jordan normal form, a general layout satisfying the following criteria:    Nonzero values are on the diagonal entries aii and on the “superdiagonal” ai i+1 .   Diagonal values are eigenvalues repeated as many times as their algebraic multiplicity;  the matrix is block diagonal about these clusters.    Oﬀ-diagonal values are 1 or 0.   Eigenvectors  cid:4  117  Thus, the shape looks something like the following:  λ1  1 λ1  1 λ1    .    λ2  1 λ2  λ3  . . .  Jordan normal form is attractive theoretically because it always exists, but the 1 0 structure is discrete and unstable under numerical perturbation.  6.3 COMPUTING A SINGLE EIGENVALUE  Computing the eigenvalues of a matrix is a well-studied problem with many potential al- gorithmic approaches. Each is tuned for a diﬀerent situation, and achieving near-optimal conditioning or speed requires experimentation with several techniques. Here, we cover a few popular algorithms for the eigenvalue problem encountered in practice.  6.3.1 Power Iteration Assume that A ∈ Rn×n is non-defective and nonzero with all real eigenvalues, e.g., A is symmetric. By deﬁnition, A has a full set of eigenvectors  cid:126 x1, . . . ,  cid:126 xn ∈ Rn; we sort them such that their corresponding eigenvalues satisfy λ1 ≥ λ2 ≥ ··· ≥ λn. Take an arbitrary vector  cid:126 v ∈ Rn. Since the eigenvectors of A span Rn, we can write  cid:126 v in the  cid:126 xi basis as  cid:126 v = c1 cid:126 x1 + ··· + cn cid:126 xn. Applying A to both sides,  A cid:126 v = c1A cid:126 x1 + ··· + cnA cid:126 xn  = c1λ1 cid:126 x1 + ··· + cnλn cid:126 xn since A cid:126 xi = λi cid:126 xi  A2 cid:126 v = λ2  λ2 λ1  = λ1 cid:18 c1 cid:126 x1 + 1 cid:32 c1 cid:126 x1 + cid:18  λ2 λ1 cid:19 2 1 cid:32 c1 cid:126 x1 + cid:18  λ2 λ1 cid:19 k  ...  Ak cid:126 v = λk  c2 cid:126 x2 + ··· +  λn λ1  cn cid:126 xn cid:19  c2 cid:126 x2 + ··· + cid:18  λn λ1 cid:19 2  cn cid:126 xn cid:33   c2 cid:126 x2 + ··· + cid:18  λn λ1 cid:19 k  cn cid:126 xn cid:33  .  As k → ∞, the ratio  λi λ1 k → 0 unless λi = ±λ1, since λ1 has the largest magnitude of any eigenvalue by construction. If  cid:126 x is the projection of  cid:126 v onto the space of eigenvectors with eigenvalues λ1, then—at least when the absolute values λi are unique—as k → ∞ the following approximation begins to dominate: Ak cid:126 v ≈ λk 1 cid:126 x. This argument leads to an exceedingly simple algorithm for computing a single eigen- vector  cid:126 x1 of A corresponding to its largest-magnitude eigenvalue λ1:  1. Take  cid:126 v1 ∈ Rn to be an arbitrary nonzero vector. 2. Iterate until convergence for increasing k:  cid:126 vk = A cid:126 vk−1   118  cid:4  Numerical Algorithms  Figure 6.3 Power iteration  a  without and  b  with normalization for ﬁnding the largest eigenvalue of a matrix.  Figure 6.4 Inverse iteration  a  without and  b  with LU factorization.  This algorithm, known as power iteration and detailed in Figure 6.3 a , produces vectors  cid:126 vk more and more parallel to the desired  cid:126 x1 as k → ∞. Although we have not considered the defective case here, it is still guaranteed to converge; see [98] for a more advanced discussion. One time that this technique may fail is if we accidentally choose  cid:126 v1 such that c1 = 0, but the odds of this peculiarity occurring are vanishingly small. Such a failure mode only occurs when the initial guess has no component parallel to  cid:126 x1. Also, while power iteration can succeed in the presence of repeated eigenvalues, it can fail if λ and −λ are both eigenvalues of A with the largest magnitude. In the absence of these degeneracies, the rate of convergence for power iteration depends on the decay rate of terms 2 to n in the sum above for Ak cid:126 v and hence is determined by the ratio of the second-largest-magnitude eigenvalue of A to the largest.  If λ1 > 1, however, then  cid:107  cid:126 vk cid:107  → ∞ as k → ∞, an undesirable property for ﬂoating- point arithmetic. We only care about the direction of the eigenvector rather than its mag- nitude, so scaling has no eﬀect on the quality of our solution. To avoid dealing with large- magnitude vectors, we can normalize  cid:126 vk at each step, producing the normalized power iteration algorithm in Figure 6.3 b . In the algorithm listing, we purposely do not decorate the norm  cid:107  ·  cid:107  with a particular subscript. Mathematically, any norm will suﬃce for pre- venting  cid:126 vk from going to inﬁnity, since we have shown that all norms on Rn are equivalent. In practice, we often use the inﬁnity norm  cid:107  ·  cid:107 ∞; this choice has the convenient property that during iteration  cid:107 A cid:126 vk cid:107 ∞ → λ1. 6.3.2 Inverse Iteration We now have an iterative algorithm for approximating the largest-magnitude eigenvalue λ1 of a matrix A. Suppose A is invertible, so that we can evaluate  cid:126 y = A−1 cid:126 v by solving A cid:126 y =  cid:126 v  function Power-Iteration A   function Normalized-Iteration A    cid:2 v ← Arbitrary n  for k ← 1, 2, 3, . . .  cid:2 v ← A cid:2 v return  cid:2 v   a    cid:2 v ← Arbitrary n  for k ← 1, 2, 3, . . .   cid:2 w ← A cid:2 v  cid:2 v ←  cid:2 w  cid:2  cid:2 w cid:2   return  cid:2 v   b   function Inverse-Iteration A    cid:2 v ← Arbitrary n  for k ← 1, 2, 3, . . .  ← A−1 cid:2 v  cid:2 w  cid:2 v ←  cid:2 w  cid:2  cid:2 w cid:2   return  cid:2 v  function Inverse-Iteration-LU A    cid:2 v ← Arbitrary n  L, U ← LU-Factorize A  for k ← 1, 2, 3, . . .   cid:2 y ← Forward-Substitute L,  cid:2 v   cid:2 w  cid:2 v ←  cid:2 w  cid:2  cid:2 w cid:2   ← Back-Substitute U,  cid:2 y   return  cid:2 v   a    b    Eigenvectors  cid:4  119  Figure 6.5 Rayleigh quotient iteration for ﬁnding an eigenvalue close to an initial guess σ.  using techniques covered in previous chapters. If A cid:126 x = λ cid:126 x, then  cid:126 x = λA−1 cid:126 x, or equivalently A−1 cid:126 x = 1  λ  cid:126 x. Thus, 1 λ is an eigenvalue of A−1 with eigenvector  cid:126 x.  If a ≥ b then b−1 ≥ a−1, so the smallest-magnitude eigenvalue of A is the largest- magnitude eigenvector of A−1. This construction yields an algorithm for ﬁnding λn rather than λ1 called inverse power iteration, in Figure 6.4 a . This iterative scheme is nothing more than the power method from §6.3.1 applied to A−1. We repeatedly are solving systems of equations using the same matrix A but diﬀerent right-hand sides, a perfect application of factorization techniques from previous chapters. For instance, if we write A = LU , then we could formulate an equivalent but considerably more eﬃcient version of inverse power iteration illustrated in Figure 6.4 b . With this sim- pliﬁcation, each solve for A−1 cid:126 v is carried out in two steps, ﬁrst by solving L cid:126 y =  cid:126 v and then by solving U  cid:126 w =  cid:126 y as suggested in §3.5.1. 6.3.3 Shifting  Suppose λ2 is the eigenvalue of A with second-largest magnitude. Power iteration converges fastest when λ2 λ1 is small, since in this case the power  λ2 λ1 k decays quickly. If this ratio is nearly 1, it may take many iterations before a single eigenvector is isolated. If the eigenvalues of A are λ1, . . . , λn with corresponding eigenvectors  cid:126 x1, . . . ,  cid:126 xn, then  the eigenvalues of A − σIn×n are λ1 − σ, . . . , λn − σ, since:   A − σIn×n  cid:126 xi = A cid:126 xi − σ cid:126 xi = λi cid:126 xi − σ cid:126 xi =  λi − σ  cid:126 xi.  With this idea in mind, one way to make power iteration converge quickly is to choose σ such that:   cid:12  cid:12  cid:12  cid:12   λ2 − σ  λ1 − σ cid:12  cid:12  cid:12  cid:12  < cid:12  cid:12  cid:12  cid:12   λ2  λ1 cid:12  cid:12  cid:12  cid:12  .  That is, we ﬁnd eigenvectors of A − σIn×n rather than A itself, choosing σ to widen the gap between the ﬁrst and second eigenvalue to improve convergence rates. Guessing a good σ, however, can be an art, since we do not know the eigenvalues of A a priori.  More generally, if we think that σ is near an eigenvalue of A, then A − σIn×n has an eigenvalue close to 0 that we can reveal by inverse iteration. In other words, to use power iteration to target a particular eigenvalue of A rather than its largest or smallest eigenvalue as in previous sections, we shift A so that the eigenvalue we want is close to zero and then can apply inverse iteration to the result.  If our initial guess of σ is inaccurate, we could try to update it from iteration to iteration of the power method. For example, if we have a ﬁxed guess of an eigenvector  cid:126 x of A, then  function Rayleigh-Quotient-Iteration A, σ    cid:3 v ← Arbitrary n  for k ← 1, 2, 3, . . .  ←  A − σIn×n −1 cid:3 v   cid:3 w  cid:3 v ←  cid:2 w  cid:2  cid:2 w cid:2  σ ←  cid:2 v cid:2 A cid:2 v  cid:2  cid:2 v cid:2 2  2  return  cid:3 v   120  cid:4  Numerical Algorithms  by the normal equations the least-squares approximation of the corresponding eigenvalue σ is given by  σ ≈   cid:126 x cid:62 A cid:126 x  cid:107  cid:126 x cid:107 2  2  .  This fraction is known as a Rayleigh quotient. Thus, we can attempt to increase convergence by using Rayleigh quotient iteration, in Figure 6.5, which uses this approximation for σ to update the shift in each step.  Rayleigh quotient iteration usually takes fewer steps to converge than power iteration given a good starting guess σ, but the matrix A − σkIn×n is diﬀerent each iteration and cannot be prefactored as in Figure 6.4 b . In other words, fewer iterations are necessary but each iteration takes more time. This trade-oﬀ makes the Rayleigh method more or less preferable to power iteration with a ﬁxed shift depending on the particular choice and size of A. As an additional caveat, if σk is too good an estimate of an eigenvalue, the matrix A − σkIn×n can become near-singular, causing conditioning issues during inverse iteration; that said, depending on the linear solver, this ill-conditioning may not be a concern because it occurs in the direction of the eigenvector being computed. In the opposite case, it can be diﬃcult to control which eigenvalue is isolated by Rayleigh quotient iteration, especially if the initial guess is inaccurate.  6.4 FINDING MULTIPLE EIGENVALUES  So far, we have described techniques for ﬁnding a single eigenvalue eigenvector pair: power iteration to ﬁnd the largest eigenvalue, inverse iteration to ﬁnd the smallest, and shifting to target values in between. For many applications, however, a single eigenvalue will not suﬃce. Thankfully, we can modify these techniques to handle this case as well.  6.4.1 Deflation  Recall the high-level structure of power iteration: Choose an arbitrary  cid:126 v1, and iteratively multiply it by A until only the largest eigenvalue λ1 survives. Take  cid:126 x1 to be the correspond- ing eigenvector.  We were quick to dismiss an unlikely failure mode of this algorithm when  cid:126 v1 ·  cid:126 x1 = 0, that is, when the initial eigenvector guess has no component parallel to  cid:126 x1. In this case, no matter how many times we apply A, the result will never have a component parallel to  cid:126 x1. The probability of choosing such a  cid:126 v1 randomly is vanishingly small, so in all but the most pernicious of cases power iteration is a stable technique.  We can turn this drawback on its head to formulate a method for ﬁnding more than one eigenvalue of a symmetric matrix A. Suppose we ﬁnd  cid:126 x1 and λ1 via power iteration as before. After convergence, we can restart power iteration after projecting  cid:126 x1 out of the initial guess  cid:126 v1. Since the eigenvectors of A are orthogonal, by the argument in §6.3.1, power iteration after this projection will recover its second -largest eigenvalue! Due to ﬁnite-precision arithmetic, applying A to a vector may inadvertently introduce a small component parallel to  cid:126 x1. We can avoid this eﬀect by projecting in each iteration. This change yields the algorithm in Figure 6.6 for computing the eigenvalues in order of descending magnitude.  The inner loop of projected iteration is equivalent to power iteration on the matrix AP ,  where P projects out  cid:126 v1, . . . ,  cid:126 v cid:96 −1:  P  cid:126 x =  cid:126 x − projspan { cid:126 v1,..., cid:126 v cid:96 −1}  cid:126 x.   Eigenvectors  cid:4  121  Figure 6.6 Projection for ﬁnding k eigenvectors of a symmetric matrix A with the largest eigenvalues. If  cid:126 u =  cid:126 0 at any point, the remaining eigenvalues of A are all zero.  AP has the same eigenvectors as A with eigenvalues 0, . . . , 0, λ cid:96 , . . . , λn. More generally, the method of deﬂation involves modifying the matrix A so that power iteration reveals an eigenvector that has not already been computed. For instance, AP is a modiﬁcation of A so that the large eigenvalues we already have computed are zeroed out.  Projection can fail if A is asymmetric. Other deﬂation formulas, however, can work in its place with similar eﬃciency. For instance, suppose A cid:126 x1 = λ1 cid:126 x1 with  cid:107  cid:126 x1 cid:107 2 = 1. Take H to be the Householder matrix  see §5.5  such that H cid:126 x1 =  cid:126 e1, the ﬁrst standard basis vector. From our discussion in §6.2, similarity transforms do not aﬀect the set of eigenvalues, so we safely can conjugate A by H without changing A’s eigenvalues. Consider what happens when we multiply HAH cid:62  by  cid:126 e1:  HAH cid:62  cid:126 e1 = HAH cid:126 e1 since H is symmetric  = HA cid:126 x1 since H cid:126 x1 =  cid:126 e1 and H 2 = In×n = λ1H cid:126 x1 since A cid:126 x1 = λ1 cid:126 x1 = λ1 cid:126 e1 by deﬁnition of H.  From this chain of equalities, the ﬁrst column of HAH cid:62  is λ1 cid:126 e1, showing that HAH cid:62  has the following structure [58]:   cid:126 0 B  cid:19  . HAH cid:62  = cid:18  λ1  cid:126 b cid:62   The matrix B ∈ R n−1 × n−1  has eigenvalues λ2, . . . , λn. Recursively applying this observa- tion, another algorithm for deﬂation successively generates smaller and smaller B matrices, with each eigenvalue computed using power iteration.  6.4.2 QR Iteration  Deﬂation has the drawback that each eigenvector must be computed separately, which can be slow and can accumulate error if individual eigenvalues are not accurate. Our remaining algorithms attempt to ﬁnd more than one eigenvector simultaneously.  Recall that similar matrices A and B = T −1AT have the same eigenvalues for any in- vertible T . An algorithm seeking the eigenvalues of A can apply similarity transformations to A with abandon in the same way that Gaussian elimination premultiplies by row oper- ations. Applying T −1 may be diﬃcult, however, since it would require inverting T , so to make such a strategy practical we seek T ’s whose inverses are known.  function Projected-Iteration symmetric A,k   for  cid:2  ← 1, 2, . . . , k   cid:3 v cid:2  ← Arbitrary n  for p ← 1, 2, 3, . . .   cid:3 u ←  cid:3 v cid:2  − projspan{ cid:3 v1,..., cid:3 v cid:2 −1}  cid:3 v cid:2   cid:3 w ← A cid:3 v  cid:3 v cid:2  ←   cid:3 w  cid:2  cid:3 w cid:2  return  cid:3 v1, . . . ,  cid:3 vk   122  cid:4  Numerical Algorithms  Figure 6.7 QR iteration for ﬁnding all the eigenvalues of A in the non-repeated eigenvalue case.  One of our motivators for the QR factorization in Chapter 5 was that the matrix Q is orthogonal, satisfying Q−1 = Q cid:62 . Because of this formula, Q and Q−1 are equally straight- forward to apply, making orthogonal matrices strong choices for similarity transformations. We already applied this observation in §6.4.1 when we deﬂated using Householder matrices. Conjugating by orthogonal matrices also does not aﬀect the conditioning of the eigenvalue problem.  But if we do not know any eigenvectors of A, which orthogonal matrix Q should we choose? Ideally, Q should involve the structure of A while being straightforward to compute. It is less clear how to apply Householder matrices strategically to reveal multiple eigenvalues in parallel  some advanced techniques do exactly this! , but we do know how to generate one orthogonal Q from A by factoring A = QR. Then, experimentally we might conjugate A by Q to ﬁnd:  Q−1AQ = Q cid:62 AQ = Q cid:62  QR Q =  Q cid:62 Q RQ = RQ.  Amazingly, conjugating A = QR by the orthogonal matrix Q is identical to writing the product RQ!  This matrix A2 ≡ RQ is not equal to A = QR, but it has the same eigenvalues. Hence, we can factor A2 = Q2R2 to get a new orthogonal matrix Q2 and once again conjugate to deﬁne A3 ≡ R2Q2. Repeating this process indeﬁnitely generates a sequence of similar matrices A, A2, A3, . . . with the same eigenvalues. Curiously, for many choices of A, as k → ∞, one can check numerically that while iterating QR factorization in this manner, Rk becomes an upper-triangular matrix containing the eigenvalues of A along its diagonal. Based on this elegant observation, in the 1950s multiple groups of European mathemati- cians studied the same iterative algorithm for ﬁnding the eigenvalues of a matrix A, shown in Figure 6.7:  Repeatedly factorize A = QR and replace A with RQ.  Take Ak to be A after the k-th iteration of this method; that is A1 = A = Q1R1, A2 = R1Q1 = Q2R2, A3 = R2Q2 = Q3R3, and so on. Since they are related via conjugation by a sequence of Q matrices, the matrices Ak all have the same eigenvalues as A. So, our analysis must show  1  when we expect this technique to converge and  2  if and how the limit point reveals eigenvalues of A. We will answer these questions in reverse order, for the case when A is symmetric and invertible with no repeated eigenvalues up to sign; so, if λ  cid:54 = 0 is an eigenvalue of A, then −λ is not an eigenvalue of A. More advanced analysis and application to asymmetric or defective matrices can be found in [50] and elsewhere. We begin by proving a proposition that will help characterize limit behavior of the QR  iteration algorithm:∗  ∗The conditions of this proposition can be relaxed but are suﬃcient for the discussion at hand.  function QR-Iteration A ∈ Rn×n   for k ← 1, 2, 3, . . .  Q, R ← QR-Factorize A  A ← RQ  return diag R    Eigenvectors  cid:4  123  Proposition 6.6. Take A, B ∈ Rn×n. Suppose that the eigenvectors of A span Rn and have distinct eigenvalues. Then, AB = BA if and only if A and B have the same set of eigenvectors  with possibly diﬀerent eigenvalues .  Proof. Suppose A and B have eigenvectors  cid:126 x1, . . . ,  cid:126 xn with eigenvalues λA and eigenvalues λB  n for B. Any  cid:126 y ∈ Rn can be decomposed as  cid:126 y = cid:80 i ai cid:126 xi, showing:  1 , . . . , λB  1 , . . . , λA  n for A  BA cid:126 y = BA cid:88 i AB cid:126 y = AB cid:88 i  i  cid:126 xi = cid:88 i i  cid:126 xi = cid:88 i So, AB cid:126 y = BA cid:126 y for all  cid:126 y ∈ Rn, or equivalently AB = BA. A B cid:126 x  =  AB  cid:126 x =  BA  cid:126 x = B A cid:126 x  = λ B cid:126 x . We have two cases:  ai cid:126 xi = B cid:88 i ai cid:126 xi = A cid:88 i  λB  λA  λA i λB  i  cid:126 xi  λA i λB  i  cid:126 xi.  Now, suppose AB = BA, and take  cid:126 x to be any eigenvector of A with A cid:126 x = λ cid:126 x. Then,    If B cid:126 x  cid:54 =  cid:126 0, then B cid:126 x is an eigenvector of A with eigenvalue λ. Since A has no repeated eigenvalues and  cid:126 x is also an eigenvector of A with eigenvalue λ, we must have B cid:126 x = c cid:126 x for some c  cid:54 = 0. In other words,  cid:126 x is also an eigenvector of B with eigenvalue c.    If B cid:126 x =  cid:126 0, then  cid:126 x is an eigenvector of B with eigenvalue 0.  Hence, all of the eigenvectors of A are eigenvectors of B. Since the eigenvectors of A span Rn, A and B have exactly the same set of eigenvectors.  Returning to QR iteration, suppose Ak → A∞ as k → ∞. If we factor A∞ = Q∞R∞, then since QR iteration converged, A∞ = Q∞R∞ = R∞Q∞. By the conjugation property, Q cid:62  ∞A∞Q∞ = R∞Q∞ = A∞, or equivalently A∞Q∞ = Q∞A∞. Since A∞ has a full set of distinct eigenvalues, by Proposition 6.6, Q∞ has the same eigenvectors as A∞. The eigenvalues of Q∞ are ±1 by orthogonality. Suppose A∞ cid:126 x = λ cid:126 x. In this case,  λ cid:126 x = A∞ cid:126 x = Q∞R∞ cid:126 x = R∞Q∞ cid:126 x = ±R∞ cid:126 x,  so R∞ cid:126 x = ±λ cid:126 x. Since R∞ is upper triangular, we now know  Exercise 6.3 :  The eigenvalues of A∞—and hence the eigenvalues of A—equal the  diagonal elements of R∞ up to sign.  We can remove the sign caveat by computing QR using rotations rather than reﬂections. The derivation above assumes that there exists A∞ with Ak → A∞ as k → ∞. Although we have not shown it yet, QR iteration is a stable method guaranteed to converge in many situations, and even when it does not converge, the relevant eigenstructure of A often can be computed from Rk as k → ∞ regardless. We will not derive exact convergence conditions here but will provide some intuition for why we might expect this method to converge, at least given our restrictions on A.  To help motivate when we expect QR iteration to converge and yield eigenvalues along the diagonal of R∞, suppose the columns of A are given by  cid:126 a1, . . . ,  cid:126 an, and consider the matrix Ak for large k. We can write:  Ak = Ak−1 · A =        Ak−1 cid:126 a1 Ak−1 cid:126 a2  ··· Ak−1 cid:126 an      .  By our derivation of power iteration, in the absence of degeneracies, the ﬁrst column of Ak will become more and more parallel to the eigenvector  cid:126 x1 of A with largest magnitude λ1 as k → ∞, since we took a vector  cid:126 a1 and multiplied it by A many times.   124  cid:4  Numerical Algorithms  Applying intuition from deﬂation, suppose we project  cid:126 x1, which is approximately par- allel to the ﬁrst column of Ak, out of the second column of Ak. By orthogonality of the eigenvectors of A, we equivalently could have projected  cid:126 x1 out of  cid:126 a2 initially and then ap- plied Ak−1. For this reason, as in §6.4.1, thanks to the removal of  cid:126 x1 the result of either process must be nearly parallel to  cid:126 x2, the vector with the second -most dominant eigenvalue! Proceeding inductively, when A is symmetric and thus has a full set of orthogonal eigenvec- tors, factoring Ak = QR yields a set of near-eigenvectors of A in the columns of Q, in order of decreasing eigenvalue magnitude, with the corresponding eigenvalues along the diagonal of R.  Multiplying to ﬁnd Ak for large k approximately takes the condition number of A to the k-th power, so computing the QR decomposition of Ak explicitly is likely to lead to numerical problems. Since decomposing Ak would reveal the eigenvector structure of A, however, we use this fact to our advantage without paying numerically. To do so, we make the following observation about QR iteration:  Grouping the Qi variables and the Ri variables separately provides a QR factorization of Ak. In other words, we can use the Qk’s and Rk’s constructed during each step of QR iteration to construct a factorization of Ak, and thus we expect the columns of the product Q1 ··· Qk to converge to the eigenvectors of A. iteration. Since Ak = QkRk, we substitute Rk = Q cid:62 k Ak inductively to show:  By a similar argument, we show a related fact about the iterates A1, A2, . . . from QR  A = Q1R1 by deﬁnition of QR iteration A2 =  Q1R1  Q1R1   = Q1 R1Q1 R1 by regrouping = Q1Q2R2R1 since A2 = R1Q1 = Q2R2  ...  Ak = Q1Q2 ··· QkRkRk−1 ··· R1 by induction.  A1 = A A2 = R1Q1 by our construction of QR iteration  = Q cid:62 1 AQ1 since R1 = Q cid:62 1 A1  A3 = R2Q2  = Q cid:62 2 A2Q2 = Q cid:62 2 Q cid:62 1 AQ1Q2 from the previous step ...  Ak+1 = Q cid:62 k ··· Q cid:62 1 AQ1 ··· Qk inductively  =  Q1 ··· Qk  cid:62 A Q1 ··· Qk ,  where Ak is the k-th matrix from QR iteration. Thus, Ak+1 is the matrix A conjugated by the product ¯Qk ≡ Q1 ··· Qk. We argued earlier that the columns of ¯Qk converge to the eigenvectors of A. Since conjugating by the matrix of eigenvectors yields a diagonal matrix of eigenvalues, we know Ak+1 = ¯Q cid:62 k A ¯Q will have approximate eigenvalues of A along its diagonal as k → ∞, at least when eigenvalues are not repeated. In the case of symmetric matrices without repeated eigenvalues, we have shown that both Ak and Rk will converge unconditionally to diagonal matrices containing the eigenvalues of   Eigenvectors  cid:4  125  A, while the product of the Qk’s will converge to a matrix of the corresponding eigenvectors. This case is but one example of the power of QR iteration, which is applied to many problems in which more than a few eigenvectors are needed of a given matrix A.  In practice, a few simplifying steps are usually applied before commencing QR iteration. QR factorization of a full matrix is relatively expensive computationally, so each iteration of the algorithm as we have described it is costly for large matrices. One way to avoid this cost for symmetric A is ﬁrst to tridiagonalize A, systematically conjugating it by orthogonal matrices until entries not on or immediately adjacent to the diagonal are zero; tridiagonal- ization can be carried out using Householder matrices in O n3  time for A ∈ Rn×n [22]. QR factorization of symmetric tridiagonal matrices is much more eﬃcient than the general case [92].  Example 6.3  QR iteration . To illustrate typical behavior of QR iteration, we apply the algorithm to the matrix  The ﬁrst few iterations, computed numerically, are shown below:   cid:18  2.000  3.000  A1 =  3.000 2.000  =⇒ A2 = R1Q1 =   cid:18  4.769  −1.154 −1.154 −0.769  A2 =   cid:18  4.990  =⇒ A3 = R2Q2 = 0.240 0.240 −0.990  A3 =  =⇒ A4 = R3Q3 =   cid:18  5.000  −0.048 −0.048 −1.000  A4 =   cid:18  5.000  =⇒ A5 = R4Q4 = 0.010 0.010 −1.000  A5 =  =⇒ A6 = R5Q5 =   cid:18  5.000  −0.002 −0.002 −1.000  A6 =  3  3  =  =  R2  R1  Q2   cid:19    cid:19   0.941 1.019  −4.907 0.000  −3.606 −3.328 1.387 0.000  −0.972 −0.235 −0.972 0.235  −0.555 0.832 −0.832 −0.555  Q1 −1.154 −1.154 −0.769  0.240 0.240 −0.990 −0.999 0.048 = −0.048 −0.999  A = cid:18  2 2  cid:19  .  cid:18   cid:19   cid:19   cid:18   cid:19  ·„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„‚„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„¶ ·„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„‚„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„¶  cid:18  4.769  cid:19   cid:18   cid:19   cid:18   cid:19  ·„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„‚„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„¶ ·„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„‚„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„¶  cid:18  4.990  cid:18   cid:19   cid:19   cid:19   cid:18  ·„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„‚„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„¶ ·„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„‚„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„¶  cid:18  5.000  cid:18   cid:19   cid:18   cid:19   cid:19  ·„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„‚„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„¶ ·„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„‚„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„¶  cid:18  5.000  cid:18   cid:19   cid:18   cid:19   cid:19  ·„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„‚„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„¶ ·„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„‚„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„¶  cid:18  5.000  cid:18   cid:19   cid:18   cid:19   cid:19  ·„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„‚„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„¶ ·„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„‚„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„¶  cid:18  5.000  0.010 0.010 −1.000 −1.000 0.002 = −0.002 −1.000  Q3 −0.048 −0.048 −1.000  Q5 −0.002 −0.002 −1.000  −1.000 −0.000 −1.000 0.000  −1.000 −0.010 −1.000 0.010  −5.000 −0.008 1.000 0.000  −4.996 −0.192 1.001 0.000  −5.000 0.000  −5.000 0.000  0.002 1.000  0.038 1.000   cid:19    cid:19    cid:19    cid:19   Q4  Q6  R5  R3  R4  R6  =  =  =⇒ A7 = R6Q6 =  0.000 0.000 −1.000  The diagonal elements of Ak converge to the eigenvalues 5 and −1 of A, as expected.   126  cid:4  Numerical Algorithms 6.4.3 Krylov Subspace Methods Our justiﬁcation for QR iteration involved analyzing the columns of Ak as k → ∞, applying observations we already made about power iteration in §6.3.1. More generally, for a vector  cid:126 b ∈ Rn, we can examine the so-called Krylov matrix  Kk ≡     cid:126 b A cid:126 b A2 cid:126 b       ··· Ak−1 cid:126 b      .  Methods analyzing Kk to ﬁnd eigenvectors and eigenvalues generally are classiﬁed as Krylov subspace methods. For instance, the Arnoldi iteration algorithm uses Gram-Schmidt orthogonalization to maintain an orthogonal basis { cid:126 q1, . . . ,  cid:126 qk} for the column space of Kk:  1. Begin by taking  cid:126 q1 to be an arbitrary unit-norm vector.  2. For k = 2, 3, . . .   a  Take  cid:126 ak = A cid:126 qk−1.  b  Project out the  cid:126 q’s you already have computed:   cid:126 bk =  cid:126 ak − projspan { cid:126 q1,..., cid:126 qk−1} cid:126 ak.   c  Renormalize to ﬁnd the next  cid:126 qk =  cid:126 bk  cid:107  cid:126 bk cid:107 2.  The matrix Qk whose columns are the vectors found above is an orthogonal matrix with the same column space as Kk, and eigenvalue estimates can be recovered from the structure of Q cid:62 k AQk.  The use of Gram-Schmidt makes this technique unstable, and its timing gets progres- sively worse as k increases. So, extensions are needed to make it feasible. For instance, one approach involves running some iterations of the Arnoldi algorithm, using the output to generate a better guess for the initial  cid:126 q1, and restarting [80]. Methods in this class are suited for problems requiring multiple eigenvectors at the ends of the spectrum of A without computing the complete set. They also can be applied to designing iterative methods for solving linear systems of equations, as we will explore in Chapter 11.  6.5 SENSITIVITY AND CONDITIONING  We have only outlined a few eigenvalue techniques out of a rich and long-standing literature. Almost any algorithmic technique has been experimented with for ﬁnding spectra, from iterative methods to root-ﬁnding on the characteristic polynomial to methods that divide matrices into blocks for parallel processing.  As with linear solvers, we can evaluate the conditioning of an eigenvalue problem inde- pendently of the solution technique. This analysis can help understand whether a simplistic iterative algorithm will be successful at ﬁnding the eigenvectors of a given matrix or if more complex stabilized methods are necessary. To do so, we will derive a condition number for the problem of ﬁnding eigenvalues for a given matrix A. Before proceeding, we should highlight that the conditioning of an eigenvalue problem is not the same as the condition number of the matrix for solving linear systems.  Suppose a matrix A has an eigenvector  cid:126 x with eigenvalue λ. Analyzing the conditioning of the eigenvalue problem involves analyzing the stability of  cid:126 x and λ to perturbations in A. To this end, we might perturb A by a small matrix δA, thus changing the set of eigenvectors.   Eigenvectors  cid:4  127  We can write eigenvectors of A + δA as perturbations of eigenvectors of A by solving the problem   A + δA   cid:126 x + δ cid:126 x  =  λ + δλ   cid:126 x + δ cid:126 x .  Expanding both sides yields:  A cid:126 x + Aδ cid:126 x + δA ·  cid:126 x + δA · δ cid:126 x = λ cid:126 x + λδ cid:126 x + δλ ·  cid:126 x + δλ · δ cid:126 x.  Since δA is small, we will assume that δ cid:126 x and δλ also are small  this assumption should be checked in a more rigorous treatment! . Products between these variables then are negligible, yielding the following approximation:  Since A cid:126 x = λ cid:126 x, we can subtract this vector from both sides to ﬁnd:  A cid:126 x + Aδ cid:126 x + δA ·  cid:126 x ≈ λ cid:126 x + λδ cid:126 x + δλ ·  cid:126 x.  Aδ cid:126 x + δA ·  cid:126 x ≈ λδ cid:126 x + δλ ·  cid:126 x.  We now apply an analytical trick to complete our derivation. Since A cid:126 x = λ cid:126 x, we know  A− λIn×n  cid:126 x =  cid:126 0, so A− λIn×n is not full rank. The transpose of a matrix is full-rank only if the matrix is full-rank, so we know  A − λIn×n  cid:62  = A cid:62  − λIn×n also has a null space vector  cid:126 y, with A cid:62  cid:126 y = λ cid:126 y. We call  cid:126 y a left eigenvector corresponding to  cid:126 x. Left-multiplying our perturbation estimate above by  cid:126 y cid:62  shows  Since A cid:62  cid:126 y = λ cid:126 y, we can simplify:   cid:126 y cid:62  Aδ cid:126 x + δA ·  cid:126 x  ≈  cid:126 y cid:62  λδ cid:126 x + δλ ·  cid:126 x .  Rearranging yields:  Finally, assume  cid:107  cid:126 x cid:107 2 = 1 and  cid:107  cid:126 y cid:107 2 = 1. Then, taking norms on both sides shows:   cid:126 y cid:62 δA ·  cid:126 x ≈ δλ cid:126 y cid:62  cid:126 x.  δλ ≈   cid:126 y cid:62  δA  cid:126 x  .   cid:126 y cid:62  cid:126 x  δλ  cid:47   cid:107 δA cid:107 2  cid:126 y ·  cid:126 x  .  This expression shows that conditioning of the eigenvalue problem roughly depends directly on the size of the perturbation δA and inversely on the angle between the left and right eigenvectors  cid:126 x and  cid:126 y.  Based on this derivation, we can use 1  cid:126 x· cid:126 y as an approximate condition number for ﬁnding the eigenvalue λ corresponding to eigenvector  cid:126 x of A. Symmetric matrices have the same left and right eigenvectors, so  cid:126 x =  cid:126 y, yielding a condition number of 1. This strong conditioning reﬂects the fact that the eigenvectors of symmetric matrices are orthogonal and thus maximally separated.  6.6 EXERCISES 6.1 Verify the solution  cid:126 y t  given in §6.1.2 to the ODE  cid:126 y cid:48  = B cid:126 y. 6.2 Deﬁne  A ≡ cid:18  0  1  1  0  cid:19  .  Can power iteration ﬁnd eigenvalues of this matrix? Why or why not?   128  cid:4  Numerical Algorithms  6.3 Show that the eigenvalues of upper-triangular matrices U ∈ Rn×n are exactly their  diagonal elements.  6.4 Extending Exercise 6.3, if we assume that the eigenvectors of U are  cid:126 vk satisfying U cid:126 vk = ukk cid:126 vk, characterize span{ cid:126 v1, . . . ,  cid:126 vk} for 1 ≤ k ≤ n when the diagonal values ukk of U are distinct.  6.5 We showed that the Rayleigh quotient iteration method can converge more quickly than power iteration. Why, however, might it still be more eﬃcient to use the power method in some cases?  6.6  Suggested by J. Yeo  Suppose  cid:126 u and  cid:126 v are vectors in Rn such that  cid:126 u cid:62  cid:126 v = 1, and  deﬁne A ≡  cid:126 u cid:126 v cid:62 .  a  What are the eigenvalues of A?   b  How many iterations does power iteration take to converge to the dominant  eigenvalue of A?  6.7  Suggested by J. Yeo  Suppose B ∈ Rn×n is diagonalizable with eigenvalues λi sat- isfying 0 < λ1 = λ2 < λ3 < ··· < λn. Let  cid:126 vi be the eigenvector corresponding to λi. Show that the inverse power method applied to B converges to a linear combination of  cid:126 v1 and  cid:126 v2.  6.8  “Mini-Riesz Representation Theorem”  We will say  cid:104 ·,· cid:105  is an inner product on Rn  if it satisﬁes:  1.  2.  3.  4.   cid:104  cid:126 x,  cid:126 y cid:105  =  cid:104  cid:126 y,  cid:126 x cid:105 ∀ cid:126 x,  cid:126 y ∈ Rn,  cid:104 α cid:126 x,  cid:126 y cid:105  = α cid:104  cid:126 x,  cid:126 y cid:105 ∀ cid:126 x,  cid:126 y ∈ Rn, α ∈ R,  cid:104  cid:126 x +  cid:126 y,  cid:126 z cid:105  =  cid:104  cid:126 x,  cid:126 z cid:105  +  cid:104  cid:126 y,  cid:126 z cid:105 ∀ cid:126 x,  cid:126 y,  cid:126 z ∈ Rn, and  cid:104  cid:126 x,  cid:126 x cid:105  ≥ 0 with equality if and only if  cid:126 x =  cid:126 0.   a  Given an inner product  cid:104 ·,· cid:105 , show that there exists a matrix A ∈ Rn×n  depend- ing on  cid:104 ·,· cid:105   such that  cid:104  cid:126 x,  cid:126 y cid:105  =  cid:126 x cid:62 A cid:126 y for all  cid:126 x,  cid:126 y ∈ Rn. Also, show that there exists a matrix M ∈ Rn×n such that  cid:104  cid:126 x,  cid:126 y cid:105  =  M cid:126 x ·  M  cid:126 y  for all  cid:126 x,  cid:126 y ∈ Rn. [This shows that all inner products are dot products after suitable rotation, stretching, and shearing of Rn!]   b  A Mahalanobis metric on Rn is a distance function of the form d  cid:126 x,  cid:126 y  =  halanobis metrics in terms of matrices M , and show that d  cid:126 x,  cid:126 y 2 is a quadratic function in  cid:126 x and  cid:126 y jointly.   cid:112  cid:104  cid:126 x −  cid:126 y,  cid:126 x −  cid:126 y cid:105  for a ﬁxed inner product  cid:104 ·,· cid:105 . Use Exercise 6.8a to write Ma-  c  Suppose we are given several pairs   cid:126 xi,  cid:126 yi  ∈ Rn×Rn. A typical “metric learning” problem involves ﬁnding a nontrivial Mahalanobis metric such that each  cid:126 xi is close to each  cid:126 yi with respect to that metric. Propose an optimization problem for this task that can be solved using eigenvector computation. Note: Make sure that your optimal Mahalanobis distance is not identically zero, but it is acceptable if your optimization allows pseudometrics; that is, there can exist some  cid:126 x  cid:54 =  cid:126 y with d  cid:126 x,  cid:126 y  = 0.   6.9  “Shifted QR iteration”  A widely used generalization of the QR iteration algorithm  for ﬁnding eigenvectors and eigenvalues of A ∈ Rn×n uses a shift in each iteration:  Eigenvectors  cid:4  129  A0 = A  Ak − σkIn×n = QkRk  Ak+1 = RkQk + σkIn×n.  Uniformly choosing σk ≡ 0 recovers basic QR iteration. Diﬀerent variants of this method propose heuristics for choosing σk  cid:54 = 0 to encourage convergence or numerical stability.   a  Show that Ak is similar to A for all k ≥ 0.  b  Propose a heuristic for choosing σk based on the construction of Rayleigh quotient iteration. Explain when you expect your method to converge faster than basic QR iteration.  6.10 Suppose A, B ∈ Rn×n are symmetric and positive deﬁnite.   a  Deﬁne a matrix √A ∈ Rn×n and show that  √A 2 = A. Generally speaking, √A  is not the same as L in the Cholesky factorization A = LL cid:62 .   b  Do most matrices have unique square roots? Why or why not?   c  We can deﬁne the exponential of A as eA ≡ cid:80 ∞k=0  k! Ak; this sum is uncondition- ally convergent  you do not have to prove this! . Write an alternative expression for eA in terms of the eigenvectors and eigenvalues of A.  1  If AB = BA, show eA+B = eAeB.   d   e  Show that the ordinary diﬀerential equation  cid:126 y cid:48  t  = −A cid:126 y with  cid:126 y 0  =  cid:126 y0 for some   cid:126 y0 ∈ Rn is solved by  cid:126 y t  = e−At cid:126 y0. What happens as t → ∞?  6.11  “Epidemiology”  Suppose  cid:126 x0 ∈ Rn contains sizes of diﬀerent populations carrying a particular infection in year 0; for example, when tracking malaria we might take x01 to be the number of humans with malaria and x02 to be the number of mosquitoes carrying the disease. By writing relationships like “The average mosquito infects two humans,” we can write a matrix M such that  cid:126 x1 ≡ M cid:126 x0 predicts populations in year 1,  cid:126 x2 ≡ M 2 cid:126 x0 predicts populations in year 2, and so on.  a  The spectral radius ρ M   is given by maxi λi, where the eigenvalues of M are λ1, . . . , λk. Epidemiologists call this number the “reproduction number” R0 of M . Explain the diﬀerence between the cases R0   1 in terms of the spread of disease. Which case is more dangerous?   b  Suppose we only care about proportions. For instance, we might use M ∈ R50×50 to model transmission of diseases between residents in each of the 50 states of the USA, and we only care about the fraction of the total people with a disease who live in each state. If  cid:126 y0 holds these proportions in year 0, give an iterative scheme to predict proportions in future years. Characterize behavior as time goes to inﬁnity.   130  cid:4  Numerical Algorithms  Note: Those readers concerned about computer graphics applications of this mate- rial should know that the reproduction number R0 is referenced in the 2011 thriller Contagion.  6.12  “Normalized cuts,” [110]  Similar to spectral embedding  §6.1.3 , suppose we have a collection of n objects and a symmetric matrix W ∈  R+ n×n whose entries wij measure the similarity between object i and object j. Rather than computing an embedding, however, now we would like to cluster the objects into two groups. This machinery is used to mark photos as day or night and to classify pixels in an image as foreground or background.   a  Suppose we cluster {1, . . . , n} into two disjoint sets A and B; this clustering  deﬁnes a cut of the collection. We deﬁne the cut score of  A, B  as follows:  C A, B  ≡ cid:88 i∈A  j∈B  wij.  This score is large if objects in A and B are similar. Eﬃciency aside, why is it inadvisable to minimize C A, B  with respect to A and B?   b  Deﬁne the volume of a set A as V  A  ≡  cid:80 i∈A cid:80 n  j=1 wij. To alleviate issues with minimizing the cut score directly, instead we will attempt minimize the normalized cut score N  A, B  ≡ C A, B  V  A −1 + V  B −1 . What does this score measure?   c  For a ﬁxed choice of A and B, deﬁne  cid:126 x ∈ Rn as follows:  Explain how to construct matrices L and D from W such that  if i ∈ A if i ∈ B.  xi ≡ cid:26  V  A −1 −V  B −1  cid:126 x cid:62 L cid:126 x = cid:88 i∈A  j∈B   cid:126 x cid:62 D cid:126 x = V  A −1 + V  B −1.  wij cid:0 V  A −1 + V  B −1 cid:1 2  Conclude that N  A, B  =  cid:126 x cid:62 L cid:126 x  cid:126 x cid:62 D cid:126 x .   d  Show that  cid:126 x cid:62 D cid:126 1 = 0.   e  The normalized cuts algorithm computes A and B by optimizing for  cid:126 x. Argue that the result of the following optimization lower-bounds the minimum normalized cut score of any partition  A, B  :  min cid:126 x subject to   cid:126 x cid:62 L cid:126 x  cid:126 x cid:62 D cid:126 x  cid:126 x cid:62 D cid:126 1 = 0.  Assuming D is invertible, show that this relaxed  cid:126 x can be computed using an eigenvalue problem.   C H A P T E R7  Singular Value Decomposition  CONTENTS  7.1  7.2  Deriving the SVD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.1.1 Computing the SVD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Applications of the SVD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.2.1 Solving Linear Systems and the Pseudoinverse . . . . . . . . . . . . . . . . . 7.2.2 Decomposition into Outer Products and Low-Rank  131 133 134 134  Approximations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135 136 137 139 140  7.2.3 Matrix Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.2.4 The Procrustes Problem and Point Cloud Alignment . . . . . . . . . . 7.2.5 Principal Component Analysis  PCA  . . . . . . . . . . . . . . . . . . . . . . . . . . 7.2.6 Eigenfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  C HAPTER 6 derived a number of algorithms for computing the eigenvalues and eigen- vectors of matrices A ∈ Rn×n. Using this machinery, we complete our initial discussion of numerical linear algebra by deriving and making use of one ﬁnal matrix factorization that exists for any matrix A ∈ Rm×n, even if it is not symmetric or square: the singular value decomposition  SVD . 7.1 DERIVING THE SVD For A ∈ Rm×n, we can think of the function  cid:126 v  cid:55 → A cid:126 v as a map taking points  cid:126 v ∈ Rn to points A cid:126 v ∈ Rm. From this perspective, we might ask what happens to the geometry of Rn in the process, and in particular the eﬀect A has on lengths of and angles between vectors. Applying our usual starting point for eigenvalue problems, we examine the eﬀect that  A has on the lengths of vectors by examining critical points of the ratio  R  cid:126 v  =  cid:107 A cid:126 v cid:107 2  cid:107  cid:126 v cid:107 2  over various vectors  cid:126 v ∈ Rn\{ cid:126 0}. This quotient measures relative shrinkage or growth of  cid:126 v under the action of A. Scaling  cid:126 v does not matter, since  R α cid:126 v  =  cid:107 A · α cid:126 v cid:107 2  cid:107 α cid:126 v cid:107 2  = α α ·  cid:107 A cid:126 v cid:107 2  cid:107  cid:126 v cid:107 2  =  cid:107 A cid:126 v cid:107 2  cid:107  cid:126 v cid:107 2  = R  cid:126 v .  Thus, we can restrict our search to  cid:126 v with  cid:107  cid:126 v cid:107 2 = 1. Furthermore, since R  cid:126 v  ≥ 0, we can 2 =  cid:126 v cid:62 A cid:62 A cid:126 v. As we have shown in previous instead ﬁnd critical points of [R  cid:126 v ]2 =  cid:107 A cid:126 v cid:107 2  131   132  cid:4  Numerical Algorithms  chapters, critical points of  cid:126 v cid:62 A cid:62 A cid:126 v subject to  cid:107  cid:126 v cid:107 2 = 1 are exactly the eigenvectors  cid:126 vi satisfying A cid:62 A cid:126 vi = λi cid:126 vi; we know λi ≥ 0 and  cid:126 vi· cid:126 vj = 0 when i  cid:54 = j since A cid:62 A is symmetric and positive semideﬁnite. Based on our use of the function R, the { cid:126 vi} basis is a reasonable one for studying the eﬀects of A on Rn. Returning to the original goal of characterizing the action of A from a geometric standpoint, deﬁne  cid:126 ui ≡ A cid:126 vi. We can make an observation about  cid:126 ui revealing a second eigenvalue structure:  λi cid:126 ui = λi · A cid:126 vi by deﬁnition of  cid:126 ui  = A λi cid:126 vi  = A A cid:62 A cid:126 vi  since  cid:126 vi is an eigenvector of A cid:62 A =  AA cid:62   A cid:126 vi  by associativity =  AA cid:62   cid:126 ui.  leads to one of two conclusions:  2 = cid:112  cid:126 v cid:62 i A cid:62 A cid:126 vi = √λi cid:107  cid:126 vi cid:107 2. This formula Taking norms shows  cid:107  cid:126 ui cid:107 2 =  cid:107 A cid:126 vi cid:107 2 = cid:112  cid:107 A cid:126 vi cid:107 2 1. Suppose  cid:126 ui  cid:54 =  cid:126 0. In this case,  cid:126 ui = A cid:126 vi is a corresponding eigenvector of AA cid:62  with   cid:107  cid:126 ui cid:107 2 = √λi cid:107  cid:126 vi cid:107 2. 2. Otherwise,  cid:126 ui =  cid:126 0.  An identical proof shows that if  cid:126 u is an eigenvector of AA cid:62 , then  cid:126 v ≡ A cid:62  cid:126 u is either zero or an eigenvector of A cid:62 A with the same eigenvalue. Take k to be the number of strictly positive eigenvalues λi > 0 for i ∈ {1, . . . , k}. By our construction above, we can take  cid:126 v1, . . . ,  cid:126 vk ∈ Rn to be eigenvectors of A cid:62 A and corresponding eigenvectors  cid:126 u1, . . . ,  cid:126 uk ∈ Rm of AA cid:62  such that  A cid:62 A cid:126 vi = λi cid:126 vi AA cid:62  cid:126 ui = λi cid:126 ui  for eigenvalues λi > 0; here, we normalize such that  cid:107  cid:126 vi cid:107 2 =  cid:107  cid:126 ui cid:107 2 = 1 for all i. Deﬁne matrices ¯V ∈ Rn×k and ¯U ∈ Rm×k whose columns are  cid:126 vi’s and  cid:126 ui’s, respectively. By construction, ¯U contains an orthogonal basis for the column space of A, and ¯V contains an orthogonal basis for the row space of A.  We can examine the eﬀect of these new basis matrices on A. Take  cid:126 ei to be the i-th  standard basis vector. Then,  ¯U cid:62 A ¯V  cid:126 ei = ¯U cid:62 A cid:126 vi by deﬁnition of ¯V  ¯U cid:62 A A cid:62 A cid:126 vi  since  cid:126 vi is an eigenvector of A cid:62 A  ¯U cid:62 A λi cid:126 vi  since we assumed λi > 0  =  =  =  ¯U cid:62  AA cid:62  A cid:126 vi by associativity  1 λi 1 λi 1 λi 1 √λi = cid:112 λi ¯U cid:62  cid:126 ui since AA cid:62  cid:126 ui = λi cid:126 ui = cid:112 λi cid:126 ei.  =  ¯U cid:62  AA cid:62   cid:126 ui since we rescaled so that  cid:107  cid:126 ui cid:107 2 = 1  Take ¯Σ = diag √λ1, . . . ,√λk . Then, the derivation above shows that ¯U cid:62 A ¯V = ¯Σ.   Singular Value Decomposition  cid:4  133  Figure 7.1 Geometric interpretation for the singular value decomposition A = U ΣV  cid:62 . The matrices U and V  cid:62  are orthogonal and hence preserve lengths and angles. The diagonal matrix Σ scales the horizontal and vertical axes independently.  Complete the columns of ¯U and ¯V to U ∈ Rm×m and V ∈ Rn×n by adding orthonormal null space vectors  cid:126 vi and  cid:126 ui with A cid:62 A cid:126 vi =  cid:126 0 and AA cid:62  cid:126 ui =  cid:126 0, respectively. After this extension, U cid:62 AV  cid:126 ei =  cid:126 0 and or  cid:126 e cid:62 i U cid:62 AV =  cid:126 0 cid:62  for i > k. If we take  Σij ≡ cid:26  √λi  0  i = j and i ≤ k  otherwise,  then we can extend our previous relationship to show U cid:62 AV = Σ, or, by orthogonality of U and V ,  A = U ΣV  cid:62 .  This factorization is the singular value decomposition  SVD  of A. The columns of U are called the left singular vectors, and the columns of V are called the right singular vectors. The diagonal elements σi of Σ are the singular values of A; usually they are sorted such that σ1 ≥ σ2 ≥ ··· ≥ 0. Both U and V are orthogonal matrices; the columns of U and V corresponding to σi  cid:54 = 0 span the column and row spaces of A, respectively. The SVD provides a complete geometric characterization of the action of A. Since U and V are orthogonal, they have no eﬀect on lengths and angles; as a diagonal matrix, Σ scales individual coordinate axes. Since the SVD always exists, all matrices A ∈ Rm×n are a composition of an isometry, a scale in each coordinate, and a second isometry. This sequence of operations is illustrated in Figure 7.1.  7.1.1 Computing the SVD The columns of V are the eigenvectors of A cid:62 A, so they can be computed using algorithms discussed in the previous chapter. Rewriting A = U ΣV  cid:62  as AV = U Σ, the columns of U corresponding to nonzero singular values in Σ are normalized columns of AV . The remaining columns satisfy AA cid:62  cid:126 ui =  cid:126 0 and can be computed using the LU factorization.  This is by no means the most eﬃcient or stable way to compute the SVD, but it works reasonably well for many applications. We omit more specialized algorithms for ﬁnding the SVD, but many of them are extensions of power iteration and other algorithms we already have covered that avoid forming A cid:62 A or AA cid:62  explicitly.  V  cid:2   Σ =  σ1 0  0 σ2  U   cid:2 x  V  cid:2  cid:2 x  ΣV  cid:2  cid:2 x  A cid:2 x   134  cid:4  Numerical Algorithms 7.2 APPLICATIONS OF THE SVD  We devote the remainder of this chapter to introducing applications of the SVD. The SVD appears countless times in both the theory and practice of numerical linear algebra, and its importance hardly can be exaggerated.  7.2.1 Solving Linear Systems and the Pseudoinverse In the special case where A ∈ Rn×n is square and invertible, the SVD can be used to solve the linear problem A cid:126 x =  cid:126 b. By substituting A = U ΣV  cid:62 , we have U ΣV  cid:62  cid:126 x =  cid:126 b, or by orthogonality of U and V ,   cid:126 x = V Σ−1U cid:62  cid:126 b.  Σ is a square diagonal matrix, so Σ−1 is the matrix with diagonal entries 1 σi.  Computing the SVD is far more expensive than most of the linear solution techniques we introduced in Chapter 3, so this initial observation mostly is of theoretical rather than practical interest. More generally, however, suppose we wish to ﬁnd a least-squares solution to A cid:126 x ≈  cid:126 b, where A ∈ Rm×n is not necessarily square. From our discussion of the normal equations, we know that  cid:126 x must satisfy A cid:62 A cid:126 x = A cid:62  cid:126 b. But when A is “short” or “underde- termined,” that is, when A has more columns than rows  m < n  or has linearly dependent columns, the solution to the normal equations might not be unique.  To cover the under-, completely-, and overdetermined cases simultaneously without re- sorting to regularization  see §4.1.3 , we can solve an optimization problem of the following form:  minimize subject to A cid:62 A cid:126 x = A cid:62  cid:126 b.   cid:107  cid:126 x cid:107 2  2  This optimization chooses the vector  cid:126 x ∈ Rn with least norm that satisﬁes the normal equations A cid:62 A cid:126 x = A cid:62  cid:126 b. When A cid:62 A is invertible, meaning the least-squares problem is completely- or overdetermined, there is only one  cid:126 x satisfying the constraint. Otherwise, of all the feasible vectors  cid:126 x, we choose the one with minimal  cid:107  cid:126 x cid:107 2. That is, we seek the smallest possible least-square solution of A cid:126 x ≈  cid:126 b, when multiple  cid:126 x’s minimize  cid:107 A cid:126 x −  cid:126 b cid:107 2.  Write A = U ΣV  cid:62 . Then,  A cid:62 A =  U ΣV  cid:62   cid:62  U ΣV  cid:62    = V Σ cid:62 U cid:62 U ΣV  cid:62  since  AB  cid:62  = B cid:62 A cid:62  = V Σ cid:62 ΣV  cid:62  since U is orthogonal.  Using this expression, the constraint A cid:62 A cid:126 x = A cid:62  cid:126 b can be written  V Σ cid:62 ΣV  cid:62  cid:126 x = V Σ cid:62 U cid:62  cid:126 b,  or equivalently, Σ cid:62 Σ cid:126 y = Σ cid:62   cid:126 d,  after taking  cid:126 d ≡ U cid:62  cid:126 b and  cid:126 y ≡ V  cid:62  cid:126 x.  Since V is orthogonal,  cid:107  cid:126 y cid:107 2 =  cid:107  cid:126 x cid:107 2 and our optimization becomes:  minimize subject to Σ cid:62 Σ cid:126 y = Σ cid:62   cid:126 d.   cid:107  cid:126 y cid:107 2  2  Since Σ is diagonal, the condition Σ cid:62 Σ cid:126 y = Σ cid:62   cid:126 d can be written σ2 i yi = σidi. So, whenever σi  cid:54 = 0 we must have yi = di σi. When σi = 0, there is no constraint on yi. Since we   Singular Value Decomposition  cid:4  135  2 we might as well take yi = 0. In other words, the solution to this  are minimizing  cid:107  cid:126 y cid:107 2 optimization is  cid:126 y = Σ+  cid:126 d, where Σ+ ∈ Rn×m has the form: i = j and σi  cid:54 = 0 otherwise.  Σ+  ij ≡ cid:26  1 σi  0  Undoing the change of variables, this result in turn yields  cid:126 x = V  cid:126 y = V Σ+  cid:126 d = V Σ+U cid:62  cid:126 b.  With this motivation, we make the following deﬁnition:  Deﬁnition 7.1  Pseudoinverse . The pseudoinverse of A = U ΣV  cid:62  ∈ Rm×n is A+ ≡ V Σ+U cid:62  ∈ Rn×m. Our derivation above shows that the pseudoinverse of A enjoys the following properties:    When A is square and invertible, A+ = A−1.   When A is overdetermined, A+ cid:126 b gives the least-squares solution to A cid:126 x ≈  cid:126 b.   When A is underdetermined, A+ cid:126 b gives the least-squares solution to A cid:126 x ≈  cid:126 b with  minimal  Euclidean  norm.  This construction from the SVD uniﬁes solutions of the underdetermined, fully determined, and overdetermined cases of A cid:126 x ≈  cid:126 b. 7.2.2 Decomposition into Outer Products and Low-Rank Approximations If we expand the product A = U ΣV  cid:62  column by column, an equivalent formula is the following:  A =  σi cid:126 ui cid:126 v cid:62 i ,   cid:96  cid:88 i=1  where  cid:96  ≡ min{m, n} and  cid:126 ui and  cid:126 vi are the i-th columns of U and V , respectively. The sum only goes to  cid:96  since the remaining columns of U or V will be zeroed out by Σ. This expression shows that any matrix can be decomposed as the sum of outer products  of vectors: Deﬁnition 7.2  Outer product . The outer product of  cid:126 u ∈ Rm and  cid:126 v ∈ Rn is the matrix  cid:126 u ⊗  cid:126 v ≡  cid:126 u cid:126 v cid:62  ∈ Rm×n. This alternative formula for the SVD provides a new way to compute the product A cid:126 x :  A cid:126 x = cid:32   cid:96  cid:88 i=1  σi cid:126 ui cid:126 v cid:62 i  cid:33   cid:126 x =   cid:96  cid:88 i=1   cid:96  cid:88 i=1  σi cid:126 ui  cid:126 v cid:62 i  cid:126 x  =  σi  cid:126 vi ·  cid:126 x  cid:126 ui, since  cid:126 x ·  cid:126 y =  cid:126 x cid:62  cid:126 y.  In words, applying A to  cid:126 x is the same as linearly combining the  cid:126 ui vectors with weights σi  cid:126 vi ·  cid:126 x . This formula provides savings when the number of nonzero σi values is relatively small. More importantly, we can round small values of σi to zero, truncating this sum to approximate A cid:126 x with fewer terms.  Similarly, from §7.2.1 we can write the pseudoinverse of A as:  A+ =  cid:88 σi cid:54 =0   cid:126 vi cid:126 u cid:62 i σi  .  With this formula, we can apply the same truncation trick to compute A+ cid:126 x and can ap- proximate A+ cid:126 x by only evaluating those terms in the sum for which σi is relatively small.   136  cid:4  Numerical Algorithms  In practice, we compute the singular values σi as square roots of eigenvalues of A cid:62 A or AA cid:62 , and methods like power iteration can be used to reveal a partial rather than full set of eigenvalues. If we are satisﬁed with approximating A+ cid:126 x, we can compute a few of the smallest σi values and truncate the formula above rather than ﬁnding A+ completely. This also avoids ever having to compute or store the full A+ matrix and can be accurate when A has a wide range of singular values.  Returning to our original notation A = U ΣV  cid:62 , our argument above shows that a useful approximation of A is ˜A ≡ U ˜ΣV  cid:62 , where ˜Σ rounds small values of Σ to zero. The column space of ˜A has dimension equal to the number of nonzero values on the diagonal of ˜Σ. This approximation is not an ad hoc estimate but rather solves a diﬃcult optimization problem posed by the following famous theorem  stated without proof :  Theorem 7.1  Eckart-Young, 1936 . Suppose ˜A is obtained from A = U ΣV  cid:62  by truncat- ing all but the k largest singular values σi of A to zero. Then, ˜A minimizes both  cid:107 A− ˜A cid:107 Fro and  cid:107 A− ˜A cid:107 2 subject to the constraint that the column space of ˜A has at most dimension k.  7.2.3 Matrix Norms  Constructing the SVD also enables us to return to our discussion of matrix norms from §4.3.1. For example, recall that the Frobenius norm of A is  Fro ≡ cid:88 ij  cid:107 A cid:107 2  a2 ij.  If we write A = U ΣV  cid:62 , we can simplify this expression:  Fro = cid:88 j  cid:107 A cid:107 2 = cid:88 j = cid:88 j =  cid:107 ΣV  cid:62  cid:107 2 =  cid:107 V Σ cid:107 2 = cid:88 j = cid:88 j  2 since the product A cid:126 ej is the j-th column of A   cid:107 A cid:126 ej cid:107 2  cid:107 U ΣV  cid:62  cid:126 ej cid:107 2  cid:126 e cid:62 j V Σ2V  cid:62  cid:126 ej since  cid:107  cid:126 x cid:107 2  2, substituting the SVD  Fro by reversing the steps above  2 =  cid:126 x cid:62  cid:126 x and U is orthogonal  Fro since a matrix and its transpose have the same Frobenius norm  cid:107 V Σ cid:126 ej cid:107 2  2 since Σ is a diagonal matrix  σ2 j cid:107 V  cid:126 ej cid:107 2  2 = cid:88 j  σ2 j since V is orthogonal.  Thus, the squared Frobenius norm of A ∈ Rm×n is the sum of the squares of its singular values. This result is of theoretical interest, but it is easier to evaluate the Frobenius norm of A by summing the squares of its elements rather than ﬁnding its SVD. More interestingly, recall that the induced two-norm of A is given by   cid:107 A cid:107 2  2 = max{λ : there exists  cid:126 x ∈ Rn with A cid:62 A cid:126 x = λ cid:126 x}.   Singular Value Decomposition  cid:4  137  Figure 7.2 If we scan a three-dimensional object from two angles, the end result is two point clouds that are not aligned. The approach explained in §7.2.4 aligns the two clouds, serving as the ﬁrst step in combining the scans.  Figure generated by S. Chung.   In the language of the SVD, this value is the square root of the largest eigenvalue of A cid:62 A, or equivalently  In other words, the induced two-norm of A can be read directly from its singular values.  Similarly, recall that the condition number of an invertible matrix A is given by cond A =  cid:107 A cid:107 2 cid:107 A−1 cid:107 2. By our derivation of A+, the singular values of A−1 must be the reciprocals of the singular values of A. Combining this with the formula above for  cid:107 A cid:107 2 yields:   cid:107 A cid:107 2 = max{σi}.  cond A =  σmax σmin  .  This expression provides a new formula for evaluating the conditioning of A.  There is one caveat that prevents this formula for the condition number from being used universally. In some cases, algorithms for computing σmin may involve solving systems A cid:126 x =  cid:126 b, a process which in itself may suﬀer from poor conditioning of A. Hence, we cannot always trust values of σmin. If this is an issue, condition numbers can be bounded or ap- proximated using various inequalities involving the singular values of A. Also, alternative iterative algorithms similar to QR iteration can be applied to computing σmin.  7.2.4 The Procrustes Problem and Point Cloud Alignment  Many techniques in computer vision involve the alignment of three-dimensional shapes. For instance, suppose we have a laser scanner that collects two point clouds of the same rigid object from diﬀerent views. A typical task is to align these two point clouds into a single coordinate frame, as illustrated in Figure 7.2.  Since the object is rigid, we expect there to be some orthogonal matrix R and translation  cid:126 t ∈ R3 such that rotating the ﬁrst point cloud by R and then translating by  cid:126 t aligns the two data sets. Our job is to estimate  cid:126 t and R. If the two scans overlap, the user or an automated system may mark n points that correspond between the two scans; we can store these in two matrices X1, X2 ∈ R3×n. Then, for each column  cid:126 x1i of X1 and  cid:126 x2i of X2, we expect R cid:126 x1i +  cid:126 t =  cid:126 x2i. To account for  Point cloud 1  Point cloud 2  Initial alignment  Final alignment   138  cid:4  Numerical Algorithms  error in measuring X1 and X2, rather than expecting exact equality, we will minimize an objective function that measures how much this relationship holds true:  E R,  cid:126 t  ≡ cid:88 i   cid:107 R cid:126 x1i +  cid:126 t −  cid:126 x2i cid:107 2 2.  If we ﬁx R and only consider  cid:126 t, minimizing E becomes a least-squares problem. On the other hand, optimizing for R with  cid:126 t ﬁxed is the same as minimizing  cid:107 RX1 − X t Fro, where 2 are those of X2 translated by  cid:126 t. This second optimization is subject to the columns of X t the constraint that R is a 3 × 3 orthogonal matrix, that is, that R cid:62 R = I3×3. It is known as the orthogonal Procrustes problem. To solve this problem using the SVD, we will introduce the trace of a square matrix as  2 cid:107 2  follows:  Deﬁnition 7.3  Trace . The trace of A ∈ Rn×n is the sum of its diagonal elements:  tr A  ≡ cid:88 i  aii.  In Exercise 7.2, you will check that  cid:107 A cid:107 2  can be simpliﬁed as follows:  Fro = tr A cid:62 A . Starting from this identity, E   cid:107 RX1 − X t  2 cid:107 2 Fro = tr  RX1 − X t  2  cid:62  RX1 − X t 2    = tr X cid:62 1 X1 − X cid:62 1 R cid:62 X t = const. − 2tr X t cid:62 2 RX1 ,  2 − X t cid:62 2 RX1 + X t cid:62 2 X2   since tr A + B  = tr A + tr B and tr A cid:62   = tr A .  This argument shows that we wish to maximize tr X t cid:62 2 RX1  with R cid:62 R = I3×3. From Exercise 7.2, tr AB  = tr BA . Applying this identity, the objective simpliﬁes to tr RC  with C ≡ X1X t cid:62 2 . If we decompose C = U ΣV  cid:62  then:  tr RC  = tr RU ΣV  cid:62   by deﬁnition  = tr  V  cid:62 RU  Σ  since tr AB  = tr BA  = tr  ˜RΣ  if we deﬁne ˜R = V  cid:62 RU , which is orthogonal  = cid:88 i  σi˜rii since Σ is diagonal.  Since ˜R is orthogonal, its columns all have unit length. This implies that ˜rii ≤ 1 for all i, since otherwise the norm of column i would be too big. Since σi ≥ 0 for all i, this argument shows that tr RC  is maximized by taking ˜R = I3×3, which achieves that upper bound. Undoing our substitutions shows R = V ˜RU cid:62  = V U cid:62 .  Changing notation slightly, we have derived the following fact:  Theorem 7.2  Orthogonal Procrustes . The orthogonal matrix R minimizing  cid:107 RX − Y  cid:107 2  Fro is given by V U cid:62 , where SVD is applied to factor XY  cid:62  = U ΣV  cid:62 .  Returning to the alignment problem, one typical strategy employs alternation:  1. Fix R and minimize E with respect to  cid:126 t.   Singular Value Decomposition  cid:4  139  2. Fix the resulting  cid:126 t and minimize E with respect to R subject to R cid:62 R = I3×3. 3. Return to step 1.  The energy E decreases with each step and thus converges to a local minimum. Since we never optimize  cid:126 t and R simultaneously, we cannot guarantee that the result is the smallest possible value of E, but in practice this method works well. Alternatively, in some cases it is possible to work out an explicit formula for  cid:126 t, circumventing the least-squares step. 7.2.5 Principal Component Analysis  PCA  Recall the setup from §6.1.1: We wish to ﬁnd a low-dimensional approximation of a set of data points stored in the columns of a matrix X ∈ Rn×k, for k observations in n dimensions. Previously, we showed that if we wish to project onto a single dimension, the best possible axis is given by the dominant eigenvector of XX cid:62 . With the SVD in hand, we can consider more complicated datasets that need more than one projection axis.  Suppose that we wish to choose d vectors whose span best contains the data points in X  we considered d = 1 in §6.1.1 ; we will assume d ≤ min{k, n}. These vectors can be written in the columns of an n × d matrix C. The column space of C is preserved when we orthogonalize its columns. Rather than orthogonalizing a posteriori, however, we can safely restrict our search to matrices C whose columns are orthonormal, satisfying C cid:62 C = Id×d. Then, the projection of X onto the column space of C is given by CC cid:62 X. Paralleling our earlier development, we will minimize  cid:107 X−CC cid:62 X cid:107 Fro subject to C cid:62 C =  Id×d. The objective can be simpliﬁed using trace identities:   cid:107 X − CC cid:62 X cid:107 2  Fro = tr  X − CC cid:62 X  cid:62  X − CC cid:62 X   since  cid:107 A cid:107 2 = tr X cid:62 X − 2X cid:62 CC cid:62 X + X cid:62 CC cid:62 CC cid:62 X  = const. − tr X cid:62 CC cid:62 X  since C cid:62 C = Id×d = − cid:107 C cid:62 X cid:107 2  Fro + const.  Fro = tr A cid:62 A   By this chain of equalities, an equivalent problem to the minimization posed above is to maximize  cid:107 C cid:62 X cid:107 2 Fro. For statisticians, when the rows of X have mean zero, this shows that we wish to maximize the variance of the projection C cid:62 X. Now, introduce the SVD to factor X = U ΣV  cid:62 . Taking ˜C ≡ U cid:62 C, we are maximizing  cid:107 C cid:62 U ΣV  cid:62  cid:107 Fro =  cid:107 Σ cid:62  ˜C cid:107 Fro by orthogonality of V . If the elements of ˜C are ˜cij, then expanding the formula for the Frobenius norm shows  σ2   cid:107 Σ cid:62  ˜C cid:107 2  Fro = cid:88 i By orthogonality of the columns of ˜C,  cid:80 i ˜c2 fact that ˜C may have fewer than n columns, cid:80 j ˜c2  ij = 1 for all j, and, taking into account the i is at most 1 in the sum above, and if we sort such that σ1 ≥ σ2 ≥ ··· , then the maximum is achieved by taking the columns of ˜C to be  cid:126 e1, . . . ,  cid:126 ed. Undoing our change of coordinates, we see that our choice of C should be the ﬁrst d columns of U .  ij ≤ 1. Hence, the coeﬃcient next to σ2  i cid:88 j  ˜c2 ij.  We have shown that the SVD of X can be used to solve such a principal component analysis  PCA  problem. In practice, the rows of X usually are shifted to have mean zero before carrying out the SVD.   140  cid:4  Numerical Algorithms  Figure 7.3 The “eigenface” technique [122] performs PCA on  a  a database of face images to extract  b  their most common modes of variation. For clustering, recog- nition, and other tasks, face images are written as  c  linear combinations of the eigenfaces, and the resulting coeﬃcients are compared.  Figure generated by D. Hyde; images from the AT&T Database of Faces, AT&T Laboratories Cambridge.   7.2.6 Eigenfaces∗  One application of PCA in computer vision is the eigenfaces technique for face recognition, originally introduced in [122]. This popular method works by applying PCA to the images in a database of faces. Projecting new input faces onto the small PCA basis encodes a face image using just a few basis coeﬃcients without sacriﬁcing too much accuracy, a beneﬁt that the method inherits from PCA.  For simplicity, suppose we have a set of k photographs of faces with similar lighting and alignment, as in Figure 7.3 a . After resizing, we can assume the photos are all of size m×n, so they are representable as vectors in Rmn containing one pixel intensity per dimension. As in §7.2.5, we will store our entire database of faces in a “training matrix” X ∈ Rmn×k. By convention, we subtract the average face image from each column, so X cid:126 1 =  cid:126 0. Applying PCA to X, as explained in the previous section, yields a set of “eigenface” images in the basis matrix C representing the common modes of variation between faces. One set of eigenfaces ordered by decreasing singular value is shown in Figure 7.3 b ; the ﬁrst few eigenfaces capture common changes in face shape, prominent features, and so on. Intuitively, PCA in this context searches for the most common distinguishing features that make a given face diﬀerent from average.  The eigenface basis C ∈ Rmn×d can be applied to face recognition. Suppose we take a new photo  cid:126 x ∈ Rmn and wish to ﬁnd the closest match in the database of faces. The  ∗Written with assistance by D. Hyde.  secaftupnI a   secafnegiE b   = −13.1×  +5.3×  −2.4×  −7.1×  +···   c  Projection   Singular Value Decomposition  cid:4  141  projection of  cid:126 x onto the eigenface basis is  cid:126 y ≡ C cid:62  cid:126 x. The best matching face is then the closest column of C cid:62 X to  cid:126 y. There are two primary advantages of eigenfaces for practical face recognition. First, usually d  cid:28  mn, reducing the dimensionality of the search problem. More importantly, PCA helps separate the relevant modes of variation between faces from noise. Diﬀerencing the mn pixels of face images independently does not search for important facial features, while the PCA axes in C are tuned to the diﬀerences observed in the columns of X.  Many modiﬁcations, improvements, and extensions have been proposed to augment the original eigenfaces technique. For example, we can set a minimum threshold so that if the weights of a new image do not closely match any of the database weights, we report that no match was found. PCA also can be modiﬁed to be more sensitive to diﬀerences between identity rather than between lighting or pose. Even so, a rudimentary implementation is surprisingly eﬀective. In our example, we train eigenfaces using photos of 40 subjects and then test using 40 diﬀerent photos of the same subjects; the basic method described achieves 80% recognition accuracy.  7.3 EXERCISES 7.1 Suppose A ∈ Rn×n. Show that condition number of A cid:62 A with respect to  cid:107  ·  cid:107 2 is the  square of the condition number of A  7.2 Suppose A ∈ Rm×n and B ∈ Rn×m. Show  cid:107 A cid:107 2 Fro = tr A cid:62 A  and tr AB  = tr BA . 7.3 Provide the SVD and condition number with respect to  cid:107 · cid:107 2 of the following matrices.  0  0 1 0 √2 0 √3 0  0   a    b   cid:18  −5 3  cid:19     7.4  Suggested by Y. Zhao.  Show that  cid:107 A cid:107 2 =  cid:107 Σ cid:107 2, where A = U ΣV T is the singular  value decomposition of A.  7.5 Show that adding a row to a matrix cannot decrease its largest singular value. 7.6  Suggested by Y. Zhao.  Show that the null space of a matrix A ∈ Rn×n is spanned by columns of V corresponding to zero singular values, where A = U ΣV  cid:62  is the singular value decomposition of A.  7.7 Take σi A  to be the i-th singular value of the square matrix A ∈ Rn×n. Deﬁne the  nuclear norm of A to be   cid:107 A cid:107 ∗ ≡  σi A .  n cid:88 i=1  Note: What follows is a tricky problem. Apply the mantra from this chapter: “If a linear algebra problem is hard, substitute the SVD.”   a  Show  cid:107 A cid:107 ∗ = tr √A cid:62 A , where trace of a matrix tr A  is the sum cid:80 i aii of its ric, positive semideﬁnite matrix M to be √M ≡ XD1 2X cid:62 , where D1 2 is the  diagonal elements. For this problem, we will deﬁne the square root of a symmet-   142  cid:4  Numerical Algorithms  diagonal matrix containing  nonnegative  square roots of the eigenvalues of M and X contains the eigenvectors of M = XDX cid:62 . Hint  to get started : Write A = U ΣV  cid:62  and argue Σ cid:62  = Σ in this case.   b  Show  cid:107 A cid:107 ∗ = maxC cid:62 C=I tr AC .  Hint: Substitute the SVD of A and apply Exercise 7.2.   c  Show that  cid:107 A + B cid:107 ∗ ≤  cid:107 A cid:107 ∗ +  cid:107 B cid:107 ∗.  Hint: Use Exercise 7.7b.   d  Minimizing  cid:107 A cid:126 x − cid:126 b cid:107 2  2 +  cid:107  cid:126 x cid:107 1 provides an alternative to Tikhonov regularization that can yield sparse vectors  cid:126 x under certain conditions. Assuming this is the case, explain informally why minimizing  cid:107 A − A0 cid:107 2 Fro +  cid:107 A cid:107 ∗ over A for a ﬁxed A0 ∈ Rn×n might yield a low-rank approximation of A0.   e  Provide an application of solutions to the “low-rank matrix completion” prob-  lem; 7.7d provides an optimization approach to this problem.  7.8  “Polar decomposition”  In this problem we will add one more matrix factorization to our linear algebra toolbox and derive an algorithm by N. Higham for its com- putation [61]. The decomposition has been used in animation applications interpo- lating between motions of a rigid object while projecting out undesirable shearing artifacts [111].  a  Show that any matrix A ∈ Rn×n can be factored A = W P, where W is orthogonal and P is symmetric and positive semideﬁnite. This factorization is known as the polar decomposition. Hint: Write A = U ΣV  cid:62  and show V ΣV  cid:62  is positive semideﬁnite.   b  The polar decomposition of an invertible A ∈ Rn×n can be computed using an  iterative scheme:  X0 ≡ A  Xk+1 =   Xk +  X−1  k   cid:62    1 2  We will prove this in a few steps:   i  Use the SVD to write A = U ΣV  cid:62 , and deﬁne Dk = U cid:62 XkV. Show D0 = Σ   ii  From  i , each Dk is diagonal. If dki is the i-th diagonal element of Dk,  and Dk+1 = 1  2  Dk +  D−1  k   cid:62  .  show  d k+1 i =  1  2 cid:18 dki +  1  dki cid:19  .   iii  Assume dki → ci as k → ∞  this convergence assumption requires proof! .  iv  Use 7.8 b iii to show Xk → U V  cid:62 .  Show ci = 1.  7.9  “Derivative of SVD,” [95]  In this problem, we will continue to use the notation of Exercise 4.3. Our goal is to diﬀerentiate the SVD of a matrix A with respect to changes in A. Such derivatives are used to simulate the dynamics of elastic objects; see [6] for one application.  a  Suppose Q t  is an orthogonal matrix for all t ∈ R. If we deﬁne ΩQ ≡ Q cid:62 ∂Q, show that ΩQ is antisymmetric, that is, Ω cid:62 Q = −ΩQ. What are the diagonal elements of ΩQ?   Singular Value Decomposition  cid:4  143   b  Suppose for a matrix-valued function A t  we use SVD to decompose A t  =  U  t Σ t V  t  cid:62 . Derive the following formula:  U cid:62  ∂A V = ΩU Σ + ∂Σ − ΣΩV .   c  Show how to compute ∂Σ directly from ∂A and the SVD of A.   d  Provide a method for ﬁnding ΩU and ΩV from ∂A and the SVD of A using a sequence of 2 × 2 solves. Conclude with formulas for ∂U and ∂V in terms of the Ω’s. Hint: It is suﬃcient to compute the elements of ΩU and ΩV above the diagonal.  7.10  “Latent semantic analysis,” [35]  In this problem, we explore the basics of latent semantic analysis, used in natural language processing to analyze collections of doc- uments.   a  Suppose we have a dictionary of m words and a collection of n documents. We can write an occurrence matrix X ∈ Rm×n whose entries xij are equal to the number of times word i appears in document j. Propose interpretations of the entries of XX cid:62  and X cid:62 X.   b  Each document in X is represented using a point in Rm, where m is potentially large. Suppose for eﬃciency and robustness to noise, we would prefer to use representations in Rk, for some k  cid:28  min{m, n}. Apply Theorem 7.1 to propose a set of k vectors in Rm that best approximates the full space of documents with respect to the Frobenius norm.   c   In cross-language applications, we might have a collection of n documents trans- lated into two diﬀerent languages, with m1 and m2 words, respectively. Then, we can write two occurrence matrices X1 ∈ Rm1×n and X2 ∈ Rm2×n. Since we do not know which words in the ﬁrst language correspond to which words in the second, the columns of these matrices are in correspondence but the rows are not. One way to ﬁnd similar phrases in the two languages is to ﬁnd vectors  cid:126 v1 ∈ Rm1 and  cid:126 v2 ∈ Rm2 such that X cid:62 1  cid:126 v1 and X cid:62 2  cid:126 v2 are similar. To do so, we can solve a canonical correlation problem:  Show how this maximization can be solved using SVD machinery.  7.11  “Stable rank,” [121]  The stable rank of A ∈ Rn×n is deﬁned as  max  cid:126 v1, cid:126 v2   X cid:62 1  cid:126 v1  ·  X cid:62 2  cid:126 v2   .   cid:107  cid:126 v1 cid:107 2 cid:107  cid:126 v2 cid:107 2  stable-rank A  ≡  cid:107 A cid:107 2  cid:107 A cid:107 2  2  Fro  .  It is used in research on low-rank matrix factorization as a proxy for the rank  di- mension of the column space  of A.   a  Show that if all n columns of A are the same vector  cid:126 v ∈ Rn\{ cid:126 0}, then  stable-rank A  = 1.   144  cid:4  Numerical Algorithms   b  Show that when the columns of A are orthonormal, stable-rank A  = n.   c  More generally, show 1 ≤ stable-rank A  ≤ n.  d  Show stable-rank A  ≤ rank A .   III  Nonlinear Techniques  145    C H A P T E R8  Nonlinear Systems  CONTENTS  8.1  Root-Finding in a Single Variable . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.1.1 Characterizing Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.1.2 Continuity and Bisection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.1.3 Fixed Point Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.1.4 Newton’s Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.1.5 Secant Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.1.6 Hybrid Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.1.7 Single-Variable Case: Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.2 Multivariable Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.2.1 Newton’s Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.2.2 Making Newton Faster: Quasi-Newton and Broyden . . . . . . . . . . . Conditioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  8.3  147 147 148 149 151 153 155 155 156 156 156 158  T RY as we might, it is not possible to express all systems of equations in the linear  framework we have developed over the last several chapters. Logarithms, exponentials, trigonometric functions, absolute values, polynomials, and so on are commonplace in prac- tical problems, but none of these functions is linear. When these functions appear, we must employ a more general—but often less eﬃcient—toolbox for nonlinear problems.  8.1 ROOT-FINDING IN A SINGLE VARIABLE  We begin by considering methods for root-ﬁnding in a single scalar variable. Given a function f  x  : R → R, we wish to develop algorithms for ﬁnding points x∗ ∈ R subject to f  x∗  = 0; we call x∗ a root or zero of f . Single-variable problems in linear algebra are not particularly interesting; after all we can solve the equation ax − b = 0 in closed form as x∗ = b a. Roots of a nonlinear equation like y2 + ecos y − 3 = 0, however, are less easily calculated. 8.1.1 Characterizing Problems  We no longer assume f is linear, but without any information about its structure, we are unlikely to make headway on ﬁnding its roots. For instance, root-ﬁnding is guaranteed to fail on  or even more deviously  recall Q denotes the set of rational numbers :  1 x > 0  f  x  = cid:26  −1 x ≤ 0 f  x  = cid:26  −1 x ∈ Q  1  otherwise.  147   148  cid:4  Numerical Algorithms  These examples are trivial in the sense that any reasonable client of root-ﬁnding software would be unlikely to expect it to succeed in this case, but more subtle examples are not much more diﬃcult to construct.  For this reason, we must add some “regularizing” assumptions about f to make the  root-ﬁnding problem well-posed. Typical assumptions include the following:    Continuity: A function f is continuous if it can be drawn without lifting up a pen;  more formally, f is continuous if the diﬀerence f  x  − f  y  vanishes as x → y.    Lipschitz: A function f is Lipschitz continuous if there exists a constant c such that f  x  − f  y  ≤ cx − y; Lipschitz functions need not be diﬀerentiable but are limited in their rates of change.    Diﬀerentiability: A function f is diﬀerentiable if its derivative f cid:48  exists for all x.   C k: A function is C k if it is diﬀerentiable k times and each of those k derivatives is  continuous; C∞ indicates that all derivatives of f exist and are continuous.  Example 8.1  Classifying functions . The function f  x  = cos x is C∞ and Lipschitz on R. The function g x  = x2 as a function on R is C∞ but not Lipschitz. In particular, g x − g 0  = x2, which cannot be bounded by any linear function of x as x → ∞. When restricted to the unit interval [0, 1], however, g x  = x2 can be considered Lipschitz since its slope is bounded by 2 in this interval; we say f is “locally Lipschitz” since this property holds on any interval [a, b]. The function h x  = x is continuous—or C 0—and Lipschitz but not diﬀerentiable thanks to its singularity at x = 0.  When our assumptions about f are stronger, we can design more eﬀective algorithms to solve f  x∗  = 0. We will illustrate the spectrum trading oﬀ between generality and eﬃciency by considering a few algorithms below.  8.1.2 Continuity and Bisection  Suppose that all we know about f is that it is continuous. This is enough to state an intuitive theorem from single-variable calculus:  Theorem 8.1  Intermediate Value Theorem . Suppose that f : [a, b] → R is continuous and that f  a  < u < f  b  or f  b  < u < f  a . Then, there exists z ∈  a, b  such that f  z  = u.  In other words, in the space between a and b, the function f must achieve every value between f  a  and f  b .  Suppose we are given as input a continuous function f  x  as well as two values  cid:96  and r such that f   cid:96  ·f  r  < 0; that is, f   cid:96   and f  r  have opposite sign. Then, by the Intermediate Value Theorem, somewhere between  cid:96  and r there is a root of f . Similar to binary search, this property suggests a bisection algorithm for ﬁnding x∗, shown in Figure 8.1. This algorithm divides the interval [ cid:96 , r] in half recursively, each time keeping the side in which a root is known to exist by the Intermediate Value Theorem. It converges unconditionally, in the sense that  cid:96  and r are guaranteed to become arbitrarily close to one another and converge to a root x∗ of f  x .  Bisection is the simplest but not necessarily the fastest technique for root-ﬁnding. As with eigenvalue methods, bisection inherently is iterative and may never provide an exact solution x∗; this property is true for nearly any root-ﬁnding algorithm unless we put strong assumptions on the class of f . We can ask, however, how close the value ck of the center   Nonlinear Systems  cid:4  149  Figure 8.1  a  Pseudocode and  b  an illustration of the bisection algorithm for ﬁnd- ing roots of continuous f  x  given endpoints  cid:96 , r ∈ R with f   cid:96   · f  r  < 0. The interval [c, r] contains a root x∗ because f  c  and f  r  have opposite sign.  point c between  cid:96 k and rk in the k-th iteration is to the root x∗ that we hope to compute. This analysis will provide a baseline for comparison to other methods.  More broadly, suppose we can establish an error bound Ek such that the estimate xk of the root x∗ during the k-th iteration of root-ﬁnding satisﬁes xk − x∗ < Ek. Any algorithm with Ek → 0 is convergent. Assuming a root-ﬁnding algorithm is convergent, however, the primary property of interest is the convergence rate, characterizing the rate at which Ek shrinks.  For bisection, since during each iteration ck and x∗ are in the interval [ cid:96 k, rk], an upper bound of error is given by Ek ≡ rk −  cid:96 k. Since we divide the interval in half each iteration, we can reduce our error bound by half in each iteration: Ek+1 = 1 2Ek. Since Ek+1 is linear in Ek, we say that bisection exhibits linear convergence.  In exchange for unconditional linear convergence, bisection requires initial estimates of  cid:96  and r bracketing a root. While some heuristic search methods exist for ﬁnding a bracketing interval, unless more is known about the form of f , ﬁnding this pair may be nearly as diﬃcult as computing a root! In this case, bisection might be thought of as a method for reﬁning a root estimate rather than for global search.  8.1.3 Fixed Point Iteration  Bisection is guaranteed to converge to a root of any continuous function f , but if we know more about f we can formulate algorithms that converge more quickly.  As an example, suppose we wish to ﬁnd x∗ satisfying g x∗  = x∗; this setup is equivalent to root-ﬁnding since solving g x∗  = x∗ is the same as solving g x∗  − x∗ = 0. As an addi- tional piece of information, however, we also might know that g is Lipschitz with constant 0 ≤ c < 1  see §8.1.1 . This condition deﬁnes g as a contraction, since g x − g y  < x− y for any x, y.  The system g x  = x suggests a potential solution method:  1. Take x0 to be an initial guess of x∗.  2. Iterate xk = g xk−1 .  If this iteration converges, the result is a ﬁxed point of g satisfying the criteria above.  function Bisection f  x ,  cid:2 , r   for k ← 1, 2, 3, . . .  c ←  cid:2 +r 2 if f  c  < εf or r −  cid:2  < εx then else if f   cid:2   · f  c  < 0 then else  return x∗ ≈ c r ← c  cid:2  ← c   a   f  x   f  x  > 0  x∗  r  x   cid:2   c  f  x  < 0   b    150  cid:4  Numerical Algorithms  Figure 8.2 Convergence of ﬁxed point iteration. Fixed point iteration searches for the intersection of g x  with the line y = x by iterating xk = g xk−1 . One way to visualize this method on the graph of g x  visualized above is that it alternates between moving horizontally to the line y = x and vertically to the position g x . Fixed point iteration  a  converges when the slope of g x  is small and  b  diverges otherwise.  When c < 1, the Lipschitz property ensures convergence to a root if one exists. To verify  this statement, if Ek = xk − x∗, then we have the following property:  Ek = xk − x∗  = g xk−1  − g x∗  by design of the iterative scheme and deﬁnition of x∗ ≤ cxk−1 − x∗ since g is Lipschitz = cEk−1.  Applying this statement inductively shows Ek ≤ ckE0 → 0 as k → ∞. If g is Lipschitz with constant c < 1 in a neighborhood [x∗ − δ, x∗ + δ], then so long as x0 is chosen in this interval, ﬁxed point iteration will converge. This is true since our expression for Ek above shows that it shrinks each iteration. When the Lipschitz constant is too large—or equivalently, when g has large slope—ﬁxed point iteration diverges. Figure 8.2 visualizes the two possibilities.  One important case occurs when g is C 1 and g cid:48  x∗  < 1. By continuity of g cid:48  in this case, there are values ε, δ > 0 such that g cid:48  x  < 1 − ε for any x ∈  x∗ − δ, x∗ + δ .  This statement is hard to parse: Make sure you understand it!  Take any x, y ∈  x∗ − δ, x∗ + δ . Then,  g x  − g y  = g cid:48  θ  · x − y by the Mean Value Theorem, for some θ ∈ [x, y]  <  1 − ε x − y.  This argument shows that g is Lipschitz with constant 1−ε < 1 in the interval  x∗−δ, x∗+δ . Applying our earlier discussion, when g is continuously diﬀerentiable and g cid:48  x∗  < 1, ﬁxed point iteration will converge to x∗ when the initial guess x0 is close by.  So far, we have little reason to use ﬁxed point iteration: We have shown it is guaranteed to converge only when g is Lipschitz, and our argument about the Ek’s shows linear con- vergence, like bisection. There is one case, however, in which ﬁxed point iteration provides an advantage.  g x   x  =  y  g x   x  =  y  x0  x∗  x2  x1  x  x1  x0  x  ecnegrevnoC a   ecnegreviD b    Nonlinear Systems  cid:4  151  Suppose g is diﬀerentiable with g cid:48  x∗  = 0. Then, the ﬁrst-order term vanishes in the  Taylor series for g, leaving behind:  g xk  = g x∗  +  1 2  g cid:48  cid:48  x∗  xk − x∗ 2 + O cid:0  xk − x∗ 3 cid:1  .  In this case,  Ek = xk − x∗  = g xk−1  − g x∗  as before =  1 2g cid:48  cid:48  x∗  xk−1 − x∗ 2 + O  xk−1 − x∗ 3  from the Taylor argument 1  g cid:48  cid:48  x∗  + ε  xk−1 − x∗ 2 for some ε so long as xk−1 is close to x∗ 2 1  g cid:48  cid:48  x∗  + ε E2 2  k−1.  ≤ =  By this chain of inequalities, in this case Ek is quadratic in Ek−1, so we say ﬁxed point iteration can have quadratic convergence. This implies that Ek → 0 much faster, needing fewer iterations to reach a reasonable root approximation.  Example 8.2  Fixed point iteration . We can apply ﬁxed point iteration to solving x = cos x by iterating xk+1 = cos xk. A numerical example starting from x0 = 0 proceeds as follows:  k xk  0 0  1  2  3  4  5  6  7  8  9  1.000  0.540  0.858  0.654  0.793  0.701  0.764  0.722  0.750  In this case, ﬁxed point iteration converges linearly to the root x∗ ≈ 0.739085. near x∗ = 0. For this reason, ﬁxed point iteration xk+1 = sin x2 converges more quickly to the root:  The root-ﬁnding problem x = sin x2 satisﬁes the condition for quadratic convergence k starting at x0 = 1  k xk  0 1  1  2  3  4  5  6  7  8  9  0.841  0.650  0.410  0.168  0.028  0.001  0.000  0.000  0.000  Finally, the roots of x = ex + e−x − 5 do not satisfy convergence criteria for ﬁxed point iteration. Iterates of the failed ﬁxed point scheme xk+1 = exk + e−xk − 5 starting at x0 = 1 are shown below:  k xk  1  0 1 −1.914  2  3  4  5  6  1.927  2.012  2.609  8.660  5760.375  7 ···  8.1.4 Newton’s Method  We tighten our class of functions once more to derive a root-ﬁnding algorithm based more fundamentally on a diﬀerentiability assumption, this time with consistent quadratic con- vergence. We will attempt to solve f  x∗  = 0 rather than ﬁnding ﬁxed points, with the assumption that f ∈ C 1—a slightly tighter condition than Lipschitz.  Since f is diﬀerentiable, it can be approximated near xk ∈ R using a tangent line:  f  x  ≈ f  xk  + f cid:48  xk  x − xk .   152  cid:4  Numerical Algorithms  Figure 8.3 Newton’s method iteratively approximates f  x  with tangent lines to ﬁnd roots of a diﬀerentiable function f  x .  Setting the expression on the right equal to zero and solving for x provides an approximation xk+1 of the root:  In reality, xk+1 may not satisfy f  xk+1  = 0, but since it is the root of an approximation of f we might hope that it is closer to x∗ than xk. If this is true, then iterating this formula should give xk’s that get closer and closer to x∗. This technique is known as Newton’s method for root-ﬁnding, and it amounts to repeatedly solving linear approximations of the original nonlinear problem. It is illustrated in Figure 8.3.  If we deﬁne  xk+1 = xk −  f  xk  f cid:48  xk   .  g x  = x −  f  x  f cid:48  x   ,  then Newton’s method amounts to ﬁxed point iteration on g. Diﬀerentiating,  g cid:48  x  = 1 −  f cid:48  x 2 − f  x f cid:48  cid:48  x   f cid:48  x 2  by the quotient rule  =  f  x f cid:48  cid:48  x   f cid:48  x 2  after simpliﬁcation.  Suppose x∗ is a simple root of f  x , meaning f cid:48  x∗   cid:54 = 0. Using this formula, g cid:48  x∗  = 0, and by our analysis of ﬁxed point iteration in §8.1.3, Newton’s method must converge quadrat- ically to x∗ when starting from a suﬃciently close x0. When x∗ is not simple, however, convergence of Newton’s method can be linear or worse.  The derivation of Newton’s method via linear approximation suggests other methods using more terms in the Taylor series. For instance, “Halley’s method” also makes use of f cid:48  cid:48  via quadratic approximation, and more general “Householder methods” can include an ar- bitrary number of derivatives. These techniques oﬀer higher-order convergence at the cost of having to evaluate many derivatives and the possibility of more exotic failure modes. Other methods replace Taylor series with alternative approximations; for example, “lin- ear fractional interpolation” uses rational functions to better approximate functions with asymptotes.  f  x   x0  x2  x1  x   Nonlinear Systems  cid:4  153  Example 8.3  Newton’s method . The last part of Example 8.2 can be expressed as a root-ﬁnding problem on f  x  = ex + e−x − 5 − x. The derivative of f  x  in this case is f cid:48  x  = ex − e−x − 1, so Newton’s method can be written  xk+1 = xk −  exk + e−xk − 5 − xk  exk − e−xk − 1  .  This iteration quickly converges to a root starting from x0 = 2:  k xk  0 2  1  2  3  4  1.9161473  1.9115868  1.9115740  1.9115740  Example 8.4  Newton’s method failure . Suppose f  x  = x5−3x4 +25. Newton’s method applied to this function gives the iteration  xk+1 = xk −  x5 k − 3x4 5x4  k + 25 k − 12x3  .  These iterations converge when x0 is suﬃciently close to the root x∗ ≈ −1.5325. For instance, the iterates starting from x0 = −2 are shown below:  0  k xk −2 −1.687500 −1.555013 −1.533047 −1.532501  4  3  2  1  Farther away from this root, however, Newton’s method can fail. For instance, starting from x0 = 0.25 gives a divergent set of iterates:  k xk  0  1  2  3  4  0.25  149.023256  119.340569  95.594918  76.599025  8.1.5 Secant Method One concern about Newton’s method is the cost of evaluating f and its derivative f cid:48 . If f is complicated, we may wish to minimize the number of times we have to evaluate either of these functions. Higher orders of convergence for root-ﬁnding alleviate this problem by reducing the number of iterations needed to approximate x∗, but we also can design numerical methods that explicitly avoid evaluating costly derivatives.  Example 8.5  Rocket design . Suppose we are designing a rocket and wish to know how much fuel to add to the engine. For a given number of gallons x, we can write a function f  x  giving the maximum height of the rocket during ﬂight; our engineers have speciﬁed that the rocket should reach a height h, so we need to solve f  x  = h. Evaluating f  x  involves simulating a rocket as it takes oﬀ and monitoring its fuel consumption, which is an expensive proposition. Even if f is diﬀerentiable, we might not be able to evaluate f cid:48  in a practical amount of time.  One strategy for designing lower-impact methods is to reuse data as much as possible. For instance, we could approximate the derivative f cid:48  appearing in Newton’s method as follows:  f cid:48  xk  ≈  f  xk  − f  xk−1   .  xk − xk−1   154  cid:4  Numerical Algorithms  Figure 8.4 The secant method is similar to Newton’s method  Figure 8.3  but ap- proximates tangents to f  x  as the lines through previous iterates. It requires both x0 and x1 for initialization.  Since we had to compute f  xk−1  in the previous iteration anyway, we reuse this value to approximate the derivative for the next one. This approximation works well when xk’s are near convergence and close to one another. Plugging it into Newton’s method results in a new scheme known as the secant method, illustrated in Figure 8.4:  xk+1 = xk −  f  xk  xk − xk−1  f  xk  − f  xk−1   .  The user must provide two initial guesses x0 and x1 or can run a single iteration of Newton to get it started.  Analyzing the secant method is more involved than the other methods we have consid- ered because it uses both f  xk  and f  xk−1 ; proof of its convergence is outside the scope of our discussion. Error analysis reveals that the secant method decreases error at a rate of  1+√5  2  the “Golden Ratio” , which is between linear and quadratic. Since convergence is close to that of Newton’s method without the need for evaluating f cid:48 , the secant method is a strong alternative. Example 8.6  Secant method . Suppose f  x  = x4−2x2−4. Iterates of Newton’s method for this function are given by  xk+1 = xk −  x4 k − 2x2 4x3  k − 4 k − 4xk  .  Contrastingly, iterates of the secant method for the same function are given by  xk+1 = xk −   x4   x4 k − 2x2  k − 2x2 k − 4  −  x4  k − 4  xk − xk−1   k−1 − 2x2  k−1 − 4   .  By construction, a less expensive way to compute these iterates is to save and reuse f  xk−1  from the previous iteration. We can compare the two methods starting from x0 = 3; for the secant method we also choose x−1 = 2:  k  xk  Newton  xk  secant   0 3 3  1  2  3  4  5  6  2.385417 1.927273  2.005592 1.882421  1.835058 1.809063  1.800257 1.799771  1.798909 1.798917  1.798907 1.798907  The two methods exhibit similar convergence on this example.  f  x   x0  x1  x2  x4  x  x3   Nonlinear Systems  cid:4  155  8.1.6 Hybrid Techniques  With additional engineering, we can combine the advantages of diﬀerent root-ﬁnding algo- rithms. For instance, we might make the following observations:    Bisection is guaranteed to converge, but only at a linear rate.   The secant method has a faster rate of convergence, but it may not converge at all if  the initial guess x0 is far from the root x∗.  Suppose we have bracketed a root of f  x  in the interval [ cid:96 k, rk]. Given the iterates xk and xk−1, we could take the next estimate xk+1 to be either of the following:    The next secant method iterate, if it is contained in   cid:96 k, rk .   The midpoint  cid:96 k+rk 2 otherwise.  This combination of the secant method and bisection guarantees that xk+1 ∈   cid:96 k, rk . Regardless of the choice above, we can update the bracket containing the root to [ cid:96 k+1, rk+1] by examining the sign of f  xk+1 .  The algorithm above, called “Dekker’s method,” attempts to combine the unconditional convergence of bisection with the stronger root estimates of the secant method. In many cases it is successful, but its convergence rate is somewhat diﬃcult to analyze. Specialized failure modes can reduce this method to linear convergence or worse: In some cases, bisection can converge more quickly! Other techniques, e.g., “Brent’s method,” make bisection steps more often to strengthen convergence and can exhibit guaranteed behavior at the cost of a more complex implementation.  8.1.7 Single-Variable Case: Summary  We only have scratched the surface of the one-dimensional root-ﬁnding problem. Many other iterative schemes for root-ﬁnding exist, with diﬀerent guarantees, convergence rates, and caveats. Starting from the methods above, we can make a number of broader observations:   To support arbitrary functions f that may not have closed-form solutions to f  x∗  = 0, we use iterative algorithms generating approximations that get closer and closer to the desired root.    We wish for the sequence xk of root estimates to reach x∗ as quickly as possible. If Ek is an error bound with Ek → 0 as k → ∞, then we can characterize the order of convergence using classiﬁcations like the following:  1. Linear convergence: Ek+1 ≤ CEk for some C < 1. 2. Superlinear convergence: Ek+1 ≤ CEr  if Ek is small enough, the r-th power of Ek can cancel the eﬀects of C.  k for r > 1; we do not require C < 1 since  3. Quadratic convergence: Ek+1 ≤ CE2 k. 4. Cubic convergence: Ek+1 ≤ CE3  k  and so on .    A method might converge quickly, needing fewer iterations to get suﬃciently close to x∗, but each individual iteration may require additional computation time. In this case, it may be preferable to do more iterations of a simpler method than fewer iterations of a more complex one. This idea is further explored in Exercise 8.1.   156  cid:4  Numerical Algorithms 8.2 MULTIVARIABLE PROBLEMS Some applications may require solving the multivariable problem f   cid:126 x  =  cid:126 0 given a function f : Rn → Rm. We have already seen one instance of this problem when solving A cid:126 x =  cid:126 b, which is equivalent to ﬁnding roots of f   cid:126 x  ≡ A cid:126 x −  cid:126 b, but the general case is considerably more diﬃcult. Strategies like bisection are challenging to extend since we now must guarantee that m diﬀerent functions all equal zero simultaneously.  8.2.1 Newton’s Method One of our single-variable strategies extends in a straightforward way. Recall from §1.4.2 that for a diﬀerentiable function f : Rn → Rm we can deﬁne the Jacobian matrix giving the derivative of each component of f in each of the coordinate directions:   Df  ij ≡  ∂fi ∂xj  .  We can use the Jacobian of f to extend our derivation of Newton’s method to multiple dimensions. In more than one dimension, a ﬁrst-order approximation of f is given by  Substituting the desired condition f   cid:126 x  =  cid:126 0 yields the following linear system determining the next iterate  cid:126 xk+1:  When Df is square and invertible, requiring n = m, we obtain the iterative formula for a multidimensional version of Newton’s method:  f   cid:126 x  ≈ f   cid:126 xk  + Df   cid:126 xk  ·   cid:126 x −  cid:126 xk .  Df   cid:126 xk  ·   cid:126 xk+1 −  cid:126 xk  = −f   cid:126 xk .   cid:126 xk+1 =  cid:126 xk − [Df   cid:126 xk ]−1f   cid:126 xk ,  where as always we do not explicitly compute the matrix [Df   cid:126 xk ]−1 but rather solve a linear system, e.g., using the techniques from Chapter 3. When m < n, this equation can be solved using the pseudoinverse to ﬁnd one of potentially many roots of f ; when m > n, one can attempt least-squares, but the existence of a root and convergence of this technique are both unlikely.  An analogous multidimensional argument to that in §8.1.3 shows that ﬁxed-point meth- ods like Newton’s method iterating  cid:126 xk+1 = g  cid:126 xk  converge when the largest-magnitude eigenvalue of Dg has absolute value less than 1  Exercise 8.2 . A derivation identical to the one-dimensional case in §8.1.4 then shows that Newton’s method in multiple variables can have quadratic convergence near roots  cid:126 x∗ for which Df   cid:126 x∗  is nonsingular. 8.2.2 Making Newton Faster: Quasi-Newton and Broyden  As m and n increase, Newton’s method becomes very expensive. For each iteration, a diﬀerent matrix Df   cid:126 xk  must be inverted. Since it changes in each iteration, factoring Df   cid:126 xk  = LkUk does not help.  Quasi-Newton algorithms apply various approximations to reduce the cost of individual iterations. One approach extends the secant method beyond one dimension. Just as the secant method contains the same division operation as Newton’s method, such secant-like approximations will not necessarily alleviate the need to invert a matrix. Instead, they make it possible to carry out root-ﬁnding without explicitly calculating the Jacobian Df .   Nonlinear Systems  cid:4  157  An extension of the secant method to multiple dimensions will require careful adjustment, however, since divided diﬀerences yield a single value rather than a full approximate Jaco- bian matrix.  The directional derivative of f in the direction  cid:126 v is given by D cid:126 vf = Df ·  cid:126 v. To imitate the secant method, we can use this scalar value to our advantage by requiring that the Jacobian approximation J satisﬁes  Jk ·   cid:126 xk −  cid:126 xk−1  = f   cid:126 xk  − f   cid:126 xk−1 .  This formula does not determine the action of J on any vector perpendicular to  cid:126 xk −  cid:126 xk−1, so we need additional approximation assumptions to describe a complete root-ﬁnding algorithm. One algorithm using the approximation above is Broyden’s method, which maintains not only an estimate  cid:126 xk of  cid:126 x∗ but also a full matrix Jk estimating a Jacobian at  cid:126 xk that satisﬁes the condition above. Initial estimates J0 and  cid:126 x0 both must be supplied by the user; commonly, we approximate J0 = In×n in the absence of more information.  Suppose we have an estimate Jk−1 of the Jacobian at  cid:126 xk−1 left over from the previous iteration. We now have a new data point  cid:126 xk at which we have evaluated f   cid:126 xk , so we would like to update Jk−1 to a new matrix Jk taking into account this new piece of information. Broyden’s method applies the directional derivative approximation above to ﬁnding Jk while keeping it as similar as possible to Jk−1 by solving the following optimization problem:  minimizeJk subject to   cid:107 Jk − Jk−1 cid:107 2 Jk ·   cid:126 xk −  cid:126 xk−1  = f   cid:126 xk  − f   cid:126 xk−1 .  Fro  To solve this problem, deﬁne ∆J ≡ Jk − Jk−1, ∆ cid:126 x ≡  cid:126 xk −  cid:126 xk−1, and  cid:126 d ≡ f   cid:126 xk − f   cid:126 xk−1 − Jk−1 · ∆ cid:126 x. Making these substitutions provides an alternative optimization problem:  minimize∆J subject to ∆J · ∆ cid:126 x =  cid:126 d.   cid:107 ∆J cid:107 2  Fro  If we take  cid:126 λ to be a Lagrange multiplier, this minimization is equivalent to ﬁnding critical points of the Lagrangian Λ:  Diﬀerentiating with respect to an unknown element  ∆J ij shows:  Λ =  cid:107 ∆J cid:107 2  Fro +  cid:126 λ cid:62  ∆J · ∆ cid:126 x −  cid:126 d .  0 =  ∂Λ  ∂ ∆J ij  = 2 ∆J ij + λi ∆ cid:126 x j =⇒ ∆J = −   cid:126 λ ∆ cid:126 x  cid:62 .  1 2  Substituting into ∆J · ∆ cid:126 x =  cid:126 d shows  cid:126 λ ∆ cid:126 x  cid:62  ∆ cid:126 x  = −2 cid:126 d, or equivalently  cid:126 λ = −2  cid:126 d  cid:107 ∆ cid:126 x cid:107 2 2. Finally, we substitute into the Lagrange multiplier expression to ﬁnd:  Expanding back to the original notation shows:  ∆J = −  1 2   cid:126 λ ∆ cid:126 x  cid:62  =   cid:126 d ∆ cid:126 x  cid:62   cid:107 ∆ cid:126 x cid:107 2  2  .  Jk = Jk−1 + ∆J  = Jk−1 +  = Jk−1 +   cid:126 d ∆ cid:126 x  cid:62   cid:107 ∆ cid:126 x cid:107 2  f   cid:126 xk  − f   cid:126 xk−1  − Jk−1 · ∆ cid:126 x   2   cid:107  cid:126 xk −  cid:126 xk−1 cid:107 2  2    cid:126 xk −  cid:126 xk−1  cid:62 .   158  cid:4  Numerical Algorithms  Figure 8.5  a  Broyden’s method as described in §8.2.2 requires solving a linear sys- tem of equations, but the formula from Exercise 8.7 yields  b  an equivalent method using only matrix multiplication by updating the inverse matrix J−1 directly in- stead of J.  k f   cid:126 xk .  Broyden’s method alternates between this update and the corresponding Newton step  cid:126 xk+1 =  cid:126 xk − J−1 Additional eﬃciency in some cases can be gained by keeping track of the matrix J−1 explicitly rather than the matrix Jk, which can be updated using a similar formula and avoids the need to solve any linear systems of equations. This possibility is explored via the Sherman-Morrison update formula in Exercise 8.7. Both versions of the algorithm are shown in Figure 8.5.  k  8.3 CONDITIONING  We already showed in Example 2.10 that the condition number of root-ﬁnding in a single variable is:  condx∗ f =  1  .  f cid:48  x∗   As shown in Figure 8.6, this condition number shows that the best possible situation for root-ﬁnding occurs when f is changing rapidly near x∗, since in this case perturbing x∗ will make f take values far from 0.  Applying an identical argument when f is multidimensional gives a condition number of  cid:107 Df   cid:126 x∗  cid:107 −1. When Df is not invertible, the condition number is inﬁnite. This degeneracy occurs because perturbing  cid:126 x∗ preserves f   cid:126 x  =  cid:126 0 to ﬁrst order, and indeed such a condition can create challenging root-ﬁnding cases similar to that shown in Figure 8.6 b .  8.4 EXERCISES 8.1 Suppose it takes processor time t to evaluate f  x  or f cid:48  x  given x ∈ R. So, com- puting the pair  f  x , f cid:48  x   takes time 2t. For this problem, assume that individual arithmetic operations take negligible amounts of processor time compared to t.   a  Approximately how much time does it take to carry out k iterations of Newton’s method on f  x ? Approximately how much time does it take to carry out k iterations of the secant method on f  x ?  function Broyden f   cid:2 x ,  cid:2 x0, J0  J ← J0  cid:3  Can default to In×n  cid:2 x ←  cid:2 x0 for k ← 1, 2, 3, . . . Δ cid:2 x ← −J−1f   cid:2 x  Δf ← f   cid:2 x + Δx  − f   cid:2 x   cid:2 x ←  cid:2 x + Δ cid:2 x J ← J +  Δf−JΔ cid:2 x   Δx  cid:3   cid:2 Δ cid:2 x cid:2 2   cid:3  Linear  2  return  cid:2 x   a   function Broyden-Inverted f   cid:2 x ,  cid:2 x0, J−1 0    cid:3  Can default to In×n  0  J−1 ← J−1  cid:2 x ←  cid:2 x0 for k ← 1, 2, 3, . . . Δ cid:2 x ← −J−1f   cid:2 x  Δf ← f   cid:2 x + Δx  − f   cid:2 x   cid:2 x ←  cid:2 x + Δ cid:2 x J−1 ← J−1 + Δ cid:2 x−J−1Δf  return  cid:2 x   b   Δ cid:2 x cid:2 J−1Δf Δ cid:2 x cid:3 J−1   cid:3  Matrix multiply   Nonlinear Systems  cid:4  159  Figure 8.6 Intuition for the conditioning of ﬁnding roots of a function f  x .  a  When the slope at the root x∗ is large, the problem is well-conditioned because moving a small distance δ away from x∗ makes the value of f change by a large amount.  b  When the slope at x∗ is smaller, values of f  x  remain close to zero as we move away from the root, making it harder to pinpoint the exact location of x∗.   b  Why might the secant method be preferable in this case?  DH 8.2 Recall from §8.1.3 the proof of conditions under which single-variable ﬁxed point iteration converges. Consider now the multivariable ﬁxed point iteration scheme  cid:126 xk+1 ≡ g  cid:126 xk  for g : Rn → Rn.  a  Suppose that g ∈ C 1 and that  cid:126 xk is within a small neighborhood of a ﬁxed point  cid:126 x∗ of g. Suggest a condition on the Jacobian Dg of g that guarantees g is Lipschitz in this neighborhood.   b  Using the previous result, derive a bound for the error of  cid:126 xk+1 in terms of the  error of  cid:126 xk and the Jacobian of g.   c  Show a condition on the eigenvalues of Dg that guarantees convergence of mul-  tivariable ﬁxed point iteration.   d  How does the rate of convergence change if Dg  cid:126 x∗  = 0?  DH 8.3 Which method would you recommend for ﬁnding the root of f : R → R if all you  know about f is that:   a  f ∈ C 1 and f cid:48  is inexpensive to evaluate;  b  f is Lipschitz with constant c satisfying 0 ≤ c ≤ 1;  c  f ∈ C 1 and f cid:48  is costly to evaluate; or  d  f ∈ C 0\C 1, the continuous but non-diﬀerentiable functions.  DH 8.4 Provide an example of root-ﬁnding problems that satisfy the following criteria:   a  Can be solved by bisection but not by ﬁxed point iteration   b  Can be solved using ﬁxed point iteration, but not using Newton’s method  f  x   f  x∗ − δ   f  x   x∗  δ  x  f  x∗ − δ   x∗  δ  x   a  Good conditioning   b  Poor conditioning   160  cid:4  Numerical Algorithms  8.5 Is Newton’s method guaranteed to have quadratic convergence? Why?  DH 8.6 Suppose we wish to compute n√y for a given y > 0. Using the techniques from this chapter, derive a quadratically convergent iterative method that ﬁnds this root given a suﬃciently close initial guess.  8.7 In this problem, we show how to carry out Broyden’s method for ﬁnding roots without  solving linear systems of equations.   a  Verify the Sherman-Morrison formula, for invertible A ∈ Rn×n and vectors  cid:126 u,  cid:126 v ∈  Rn:   A +  cid:126 u cid:126 v cid:62  −1 = A−1 −  A−1 cid:126 u cid:126 v cid:62 A−1 1 +  cid:126 v cid:62 A−1 cid:126 u  .   b  Use this formula to show that the algorithm in Figure 8.5 b  is equivalent to  Broyden’s method as described in §8.2.2.  8.8 In this problem, we will derive a technique known as Newton-Raphson division. Thanks to its fast convergence, it is often implemented in hardware for IEEE-754 ﬂoating-point arithmetic.   a  Show how the reciprocal 1  a of a ∈ R can be computed iteratively using New- ton’s method. Write your iterative formula in a way that requires at most two multiplications, one addition or subtraction, and no divisions.   b  Take xk to be the estimate of 1  a during the k-th iteration of Newton’s method.  If we deﬁne εk ≡ axk − 1, show that εk+1 = −ε2 k.   c  Approximately how many iterations of Newton’s method are needed to compute 1 a within d binary decimal points? Write your answer in terms of ε0 and d, and assume ε0 < 1. Is this method always convergent regardless of the initial guess of 1 a ?   d   8.9  LSQI, [50]  In this problem, we will develop a method for solving least-squares with  a quadratic inequality constraint:  min   cid:107  cid:126 x cid:107 2≤1 cid:107 A cid:126 x −  cid:126 b cid:107 2.  You can assume the least-squares system A cid:126 x ≈  cid:126 b, where A ∈ Rm×n with m > n, is overdetermined.   a  The optimal  cid:126 x either satisﬁes  cid:107  cid:126 x cid:107 2 < 1 or  cid:107  cid:126 x cid:107 2 = 1. Explain how to distinguish  between the two cases, and give a formula for  cid:126 x when  cid:107  cid:126 x cid:107 2 < 1.   b  Suppose we are in the  cid:107  cid:126 x cid:107 2 = 1 case. Show that there exists λ ∈ R such that   A cid:62 A + λIn×n  cid:126 x = A cid:62  cid:126 b.  c  Deﬁne f  λ  ≡  cid:107  cid:126 x λ  cid:107 2  2 − 1, where  cid:126 x λ  is the solution to the system  A cid:62 A + λIn×n  cid:126 x = A cid:62  cid:126 b for ﬁxed λ ≥ 0. Assuming that the optimal  cid:126 x for the original optimization problem satisﬁes  cid:107  cid:126 x cid:107 2 = 1, show f  0  ≥ 0 and that f  λ  < 0 for suﬃciently large λ > 0.   d  Propose a strategy for the  cid:107  cid:126 x cid:107 2 = 1 case using root-ﬁnding.   Nonlinear Systems  cid:4  161  8.10  Proposed by A. Nguyen.  Suppose we have a polynomial p x  = akxk +···+a1x+a0.  You can assume ak  cid:54 = 0 and k ≥ 1.  a  Suppose the derivative p cid:48  x  has no roots in  a, b . How many roots can p x  have  in this interval?   b  Using the result of Exercise 8.10a, propose a recursive algorithm for estimating all the real roots of p x . Assume we know that the roots of p x  are at least ε apart and that they are contained with an interval [a, b].  8.11 Root-ﬁnding for complex- or real-valued polynomials is closely linked to the eigenvalue  problem considered in Chapter 6.   a  Give a matrix A whose eigenvalues are the roots of a given polynomial p x  =  akxk + ··· + a1x + a0; you can assume p x  has no repeated roots.   b  Show that the eigenvalues of a matrix A ∈ Rn×n are the roots of a polynomial function. Is it advisable to use root-ﬁnding algorithms from this chapter for the eigenvalue problem?    C H A P T E R9 Unconstrained Optimization  CONTENTS  9.1 Unconstrained Optimization: Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9.2 Optimality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9.2.1 Diﬀerential Optimality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9.2.2 Alternative Conditions for Optimality . . . . . . . . . . . . . . . . . . . . . . . . . . 9.3 One-Dimensional Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9.3.1 Newton’s Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9.3.2 Golden Section Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9.4 Multivariable Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9.4.1 Gradient Descent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9.4.2 Newton’s Method in Multiple Variables . . . . . . . . . . . . . . . . . . . . . . . . 9.4.3 Optimization without Hessians: BFGS . . . . . . . . . . . . . . . . . . . . . . . . .  163 165 166 168 169 170 170 173 173 174 175  P REVIOUS chapters have taken a largely variational approach to deriving numerical  algorithms. That is, we deﬁne an objective function or energy E  cid:126 x , possibly with constraints, and pose our algorithms as approaches to a corresponding minimization or maximization problem. A sampling of problems that we solved this way is listed below:  Problem Least-squares Project  cid:126 b onto  cid:126 a Eigenvectors of symmetric A 6.1 Pseudoinverse Principal component analysis Broyden step  2  Objective  § Constraints 4.1.2 E  cid:126 x  =  cid:107 A cid:126 x −  cid:126 b cid:107 2 None 5.4.1 E c  =  cid:107 c cid:126 a −  cid:126 b cid:107 2 None E  cid:126 x  =  cid:126 x cid:62 A cid:126 x  cid:107  cid:126 x cid:107 2 = 1 A cid:62 A cid:126 x = A cid:62  cid:126 b 7.2.1 E  cid:126 x  =  cid:107  cid:126 x cid:107 2 7.2.5 E C  =  cid:107 X − CC cid:62 X cid:107 Fro C cid:62 C = Id×d 8.2.2 E Jk  =  cid:107 Jk − Jk−1 cid:107 2  Jk · ∆ cid:126 xk = ∆fk  Fro  2  2  The formulation of numerical problems in variational language is a powerful and general technique. To make it applicable to a larger class of nonlinear problems, we will design algorithms that can perform minimization or maximization in the absence of a special form for the energy E.  9.1 UNCONSTRAINED OPTIMIZATION: MOTIVATION  In this chapter, we will consider unconstrained problems, that is, problems that can be posed as minimizing or maximizing a function f : Rn → R without any constraints on the input  cid:126 x. It is not diﬃcult to encounter such problems in practice; we explore a few examples below.  163   164  cid:4  Numerical Algorithms  Figure 9.1 Illustration for Example 9.2. Given the heights h1, h2, . . . , hn of students in a class, we may wish to estimate the mean µ and standard deviation σ of the most likely normal distribution explaining the observed heights.  Example 9.1  Nonlinear least-squares . Suppose we are given a number of pairs  xi, yi  such that f  xi  ≈ yi and wish to ﬁnd the best approximating f within a particular class. For instance, if we expect that f is exponential, we should be able to write f  x  = ceax for some c, a ∈ R; our job is to ﬁnd the parameters a and c that best ﬁt the data. One strategy we already developed in Chapter 4 is to minimize the following energy function:  E a, c  = cid:88 i   yi − ceaxi 2.  This form for E is not quadratic in a, so the linear least-squares methods from §4.1.2 do not apply to this minimization problem. Hence, we must employ alternative methods to minimize E.  Example 9.2  Maximum likelihood estimation . In machine learning, the problem of pa- rameter estimation involves examining the results of a randomized experiment and trying to summarize them using a probability distribution of a particular form. For example, we might measure the height of every student in a class to obtain a list of heights hi for each student i. If we have a lot of students, we can model the distribution of student heights using a normal distribution:  g h; µ, σ  =  1  σ√2π  e− h−µ 2 2σ2,  where µ is the mean of the distribution and σ is the standard deviation of the standard “bell curve” shape. This notation is illustrated in Figure 9.1.  Under this normal distribution, the likelihood that we observe height hi for student i is given by g hi; µ, σ , and under the  reasonable  assumption that the height of student i is probabilistically independent of that of student j, the likelihood of observing the entire set of heights observed is proportional to the product  P  {h1, . . . , hn}; µ, σ  = cid:89 i  g hi; µ, σ .  A common method for estimating the parameters µ and σ of g is to maximize P viewed as a function of µ and σ with {hi} ﬁxed; this is called the maximum-likelihood estimate of µ and  σ  μ  h1  h2  hn   Unconstrained Optimization  cid:4  165  Figure 9.2 The geometric median problem seeks a point  cid:126 x minimizing the total  non- squared  distance to a set of data points  cid:126 x1, . . . ,  cid:126 xk.  σ. In practice, we usually optimize the log likelihood  cid:96  µ, σ  ≡ log P  {h1, . . . , hn}; µ, σ . This function has the same maxima but enjoys better numerical and mathematical prop- erties.  Example 9.3  Geometric problems . Many geometric problems encountered in computer graphics and vision do not reduce to least-squares energies. For instance, suppose we have a number of points  cid:126 x1, . . . ,  cid:126 xk ∈ Rn. If we wish to cluster these points, we might wish to summarize them with a single  cid:126 x minimizing  E  cid:126 x  ≡ cid:88 i   cid:107  cid:126 x −  cid:126 xi cid:107 2.  The  cid:126 x minimizing E is known as the geometric median of { cid:126 x1, . . . ,  cid:126 xk}, as illustrated in Figure 9.2. Since the norm of the diﬀerence  cid:126 x −  cid:126 xi in E is not squared, the energy is no longer quadratic in the components of  cid:126 x.  Example 9.4  Physical equilibria, adapted from [58] . Suppose we attach an object to a set of springs; each spring is anchored at point  cid:126 xi ∈ R3 with natural length Li and constant ki. In the absence of gravity, if our object is located at position  cid:126 p ∈ R3, the network of springs has potential energy  E  cid:126 p  =  ki   cid:107  cid:126 p −  cid:126 xi cid:107 2 − Li 2 .  1  2 cid:88 i  Equilibria of this system are given by local minima of E and represent points  cid:126 p at which the spring forces are all balanced. Extensions of this problem are used to visualize graphs G =  V, E , by attaching vertices in V with springs for each pair in E.  9.2 OPTIMALITY  Before discussing how to minimize or maximize a function, we should characterize properties of the maxima and minima we are seeking. With this goal in mind, for a particular f : Rn → R and  cid:126 x∗ ∈ Rn, we will derive optimality conditions that verify whether  cid:126 x∗ has the optimal   cid:2 x5   cid:2 x4   cid:2 x   cid:2 x6   cid:2 x7   cid:2 x3   cid:2 x1   cid:2 x2   166  cid:4  Numerical Algorithms  Figure 9.3 A function f  x  with two local minima but only one global minimum.  value f   cid:126 x∗ . Maximizing f is the same as minimizing −f , so from this section onward the minimization problem is suﬃcient for our consideration.  In most situations, we ideally would like to ﬁnd global minima of f :  Deﬁnition 9.1  Global minimum . The point  cid:126 x∗ ∈ Rn is a global minimum of f : Rn → R if f   cid:126 x∗  ≤ f   cid:126 x  for all  cid:126 x ∈ Rn.  Finding a global minimum of f   cid:126 x  without any bounds on  cid:126 x or information about the structure of f eﬀectively requires searching in the dark. For instance, suppose an optimiza- tion algorithm identiﬁes the left local minimum in the function in Figure 9.3. It is nearly impossible to realize that there is a second, lower minimum by guessing x values—and for all we know, there may be a third even lower minimum of f miles to the right!  To relax these diﬃculties, in many cases we are satisﬁed if we can ﬁnd a local minimum:  Deﬁnition 9.2  Local minimum . The point  cid:126 x∗ ∈ Rn is a local minimum of f : Rn → R if there exists some ε > 0 such that f   cid:126 x∗  ≤ f   cid:126 x  for all  cid:126 x ∈ Rn satisfying  cid:107  cid:126 x −  cid:126 x∗ cid:107 2 < ε. This deﬁnition requires that  cid:126 x∗ attains the smallest value in some neighborhood deﬁned by the radius ε. Local optimization algorithms have the severe limitation that they may not ﬁnd the lowest possible value of f , as in the left local minimum in Figure 9.3. To mitigate these issues, many strategies, heuristic and otherwise, are applied to explore the landscape of possible  cid:126 x’s to help gain conﬁdence that a local minimum has the best possible value.  9.2.1 Differential Optimality  A familiar story from single- and multi-variable calculus is that ﬁnding potential minima and maxima of a function f : Rn → R is more straightforward when f is diﬀerentiable. In this case, the gradient vector ∇f =  ∂f ∂x1, . . . , ∂f ∂xn  at  cid:126 x points in the direction moving from  cid:126 x in which f increases at the fastest rate; the vector −∇f points in the direction of greatest decrease. One way to see this is to approximate f   cid:126 x  linearly near a point  cid:126 x0 ∈ Rn:  f   cid:126 x  ≈ f   cid:126 x0  + ∇f   cid:126 x0  ·   cid:126 x −  cid:126 x0 .  f  x   Local minimum  Global minimum  x   Unconstrained Optimization  cid:4  167  Figure 9.4 Diﬀerent types of critical points.  Figure 9.5 A function with many stationary points.  If we take  cid:126 x −  cid:126 x0 = α∇f   cid:126 x0 , then  f   cid:126 x0 + α∇f   cid:126 x0   ≈ f   cid:126 x0  + α cid:107 ∇f   cid:126 x0  cid:107 2 2.  The value  cid:107 ∇f   cid:126 x0  cid:107 2 mines whether f increases or decreases locally.  2 is always nonnegative, so when  cid:107 ∇f   cid:126 x0  cid:107 2 > 0, the sign of α deter- By the above argument, if  cid:126 x0 is a local minimum, then ∇f   cid:126 x0  =  cid:126 0. This condition is necessary but not suﬃcient: Maxima and saddle points also have ∇f   cid:126 x0  =  cid:126 0 as shown in Figure 9.4. Even so, this observation about minima of diﬀerentiable functions yields a high-level approach to optimization:  1. Find points  cid:126 xi satisfying ∇f   cid:126 xi  =  cid:126 0. 2. Check which of these points is a local minimum as opposed to a maximum or saddle  point.  Given their role in optimization, we give the points  cid:126 xi a special name: Deﬁnition 9.3  Stationary point . A stationary point of f : Rn → R is a point  cid:126 x ∈ Rn satisfying ∇f   cid:126 x  =  cid:126 0. Our methods for minimization mostly will ﬁnd stationary points of f and subsequently eliminate those that are not minima.  It is imperative to keep in mind when we can expect minimization algorithms to succeed. In most cases, such as those in Figure 9.4, the stationary points of f are isolated, meaning we can write them in a discrete list { cid:126 x0,  cid:126 x1, . . .}. A degenerate case, however, is shown in Figure 9.5; here, an entire interval of values x is composed of stationary points, making it    x   f  x  local minimum   x  saddle point   x  local maximum     x   f  x   168  cid:4  Numerical Algorithms  impossible to consider them individually. For the most part, we will ignore such issues as unlikely, poorly conditioned degeneracies.  Suppose we identify a point  cid:126 x ∈ R as a stationary point of f and wish to check if it is a  local minimum. If f is twice-diﬀerentiable, we can use its Hessian matrix  Hf   cid:126 x  =  ∂x2∂x1    ∂2f ∂x2 1 ∂2f  ...  ∂2f  ∂2f  ∂x1∂x2  ∂2f ∂2x2  ...  ∂2f  ∂xn∂x1  ∂xn∂x2  ∂x1∂xn  ∂2f  ∂2f  ∂x2∂xn  ...  ∂2f ∂2xn  .    ··· ··· ··· ···  1 2  Adding a term to the linearization of f reveals the role of Hf :  f   cid:126 x  ≈ f   cid:126 x0  + ∇f   cid:126 x0  ·   cid:126 x −  cid:126 x0  +    cid:126 x −  cid:126 x0  cid:62 Hf   cid:126 x −  cid:126 x0 .  If we substitute a stationary point  cid:126 x∗, then since ∇f   cid:126 x∗  =  cid:126 0,  f   cid:126 x  ≈ f   cid:126 x∗  +    cid:126 x −  cid:126 x∗  cid:62 Hf   cid:126 x −  cid:126 x∗ .  1 2  If Hf is positive deﬁnite, then this expression shows f   cid:126 x  ≥ f   cid:126 x∗  near  cid:126 x∗, and thus  cid:126 x∗ is a local minimum. More generally, a few situations can occur:    If Hf is positive deﬁnite, then  cid:126 x∗ is a local minimum of f .   If Hf is negative deﬁnite, then  cid:126 x∗ is a local maximum of f .   If Hf is indeﬁnite, then  cid:126 x∗ is a saddle point of f .   If Hf is not invertible, then oddities such as the function in Figure 9.5 can occur; this  includes the case where Hf is semideﬁnite.  Checking if a Hessian matrix is positive deﬁnite can be accomplished by checking if its Cholesky factorization exists or—more slowly—by verifying that all its eigenvalues are pos- itive. So, when f is suﬃciently smooth and the Hessian of f is known, we can check sta- tionary points for optimality using the list above. Many optimization algorithms including the ones we will discuss ignore the non-invertible case and notify the user, since again it is relatively unlikely.  9.2.2 Alternative Conditions for Optimality If we know more information about f : Rn → R, we can provide optimality conditions that are stronger or easier to check than the ones above. These conditions also can help when f is not diﬀerentiable but has other geometric properties that make it possible to ﬁnd a minimum.  One property of f that has strong implications for optimization is convexity, illustrated  in Figure 9.6 a : Deﬁnition 9.4  Convex . A function f : Rn → R is convex when for all  cid:126 x,  cid:126 y ∈ Rn and α ∈  0, 1  the following relationship holds:  f   1 − α  cid:126 x + α cid:126 y  ≤  1 − α f   cid:126 x  + αf   cid:126 y .  When the inequality is strict  replace ≤ with < , the function is strictly convex.   Unconstrained Optimization  cid:4  169  Figure 9.6  a  Convex functions must be bowl-shaped, while  b  quasiconvex func- tions can have more complicated features.  Convexity implies that if you connect two points in Rn with a line, the values of f along the line are less than or equal to those you would obtain by linear interpolation.  Convex functions enjoy many strong properties, the most basic of which is the following: Proposition 9.1. A local minimum of a convex function f : Rn → R is necessarily a global minimum.  Proof. Take  cid:126 x to be such a local minimum and suppose there exists  cid:126 x∗ with f   cid:126 x∗  < f   cid:126 x . Then, for suﬃciently small α ∈  0, 1 ,  f   cid:126 x  ≤ f   cid:126 x + α  cid:126 x∗ −  cid:126 x   since  cid:126 x is a local minimum  ≤  1 − α f   cid:126 x  + αf   cid:126 x∗  by convexity.  Moving terms in the inequality f   cid:126 x  ≤  1 − α f   cid:126 x  + αf   cid:126 x∗  shows f   cid:126 x  ≤ f   cid:126 x∗ . This contradicts our assumption that f   cid:126 x∗  < f   cid:126 x , so  cid:126 x must minimize f globally.  This proposition and related observations show that it is possible to check if you have reached a global minimum of a convex function by applying ﬁrst-order optimality. Thus, it is valuable to check by hand if a function being optimized happens to be convex, a situation occurring surprisingly often in scientiﬁc computing; one suﬃcient condition that can be easier to check when f is twice-diﬀerentiable is that Hf is positive deﬁnite everywhere.  Other optimization techniques have guarantees under weaker assumptions about f . For  example, one relaxation of convexity is quasi -convexity, achieved when  f   1 − α  cid:126 x + α cid:126 y  ≤ max f   cid:126 x , f   cid:126 y  .  An example of a quasiconvex function is shown in Figure 9.6 b . Although it does not have the characteristic “bowl” shape of a convex function, its local minimizers are necessarily global minimizers.  9.3 ONE-DIMENSIONAL STRATEGIES As in the last chapter, we will start by studying optimization for functions f : R → R of one variable and then expand to more general functions f : Rn → R.  x   1 − α x + αy  y  x   1 − α x + αy  y  xevnoC a   xevnocisauQ b    170  cid:4  Numerical Algorithms 9.3.1 Newton’s Method Our principal strategy for minimizing diﬀerentiable functions f : Rn → R will be to ﬁnd stationary points  cid:126 x∗ satisfying ∇f   cid:126 x∗  = 0. Assuming we can check whether stationary points are maxima, minima, or saddle points as a post-processing step, we will focus on the problem of ﬁnding the stationary points  cid:126 x∗.  To this end, suppose f : R → R is twice-diﬀerentiable. Then, following our derivation of  Newton’s method for root-ﬁnding in §8.1.4, we can approximate:  f  x  ≈ f  xk  + f cid:48  xk  x − xk  +  f cid:48  cid:48  xk  x − xk 2.  1 2  We need to include second-order terms since linear functions have no nontrivial minima or maxima. The approximation on the right-hand side is a parabola whose vertex is located at xk − f cid:48  xk  f cid:48  cid:48  xk . In reality, f may not be a parabola, so its vertex will not necessarily give a critical point of f directly. So, Newton’s method for minimization iteratively minimizes and adjusts the parabolic approximation:  xk+1 = xk −  f cid:48  xk  f cid:48  cid:48  xk   .  This technique is easily analyzed given the work we put into understanding Newton’s method in the previous chapter. Speciﬁcally, an alternative way to derive the iterative formula above comes from applying Newton’s method for root-ﬁnding to f cid:48  x , since stationary points x of f  x  satisfy f cid:48  x  = 0. Applying results about convergence to a root, in most cases Newton’s method for optimization exhibits quadratic convergence, provided the initial guess x0 is suﬃciently close to x∗.  A natural question is whether the secant method can be similarly adapted to minimiza- tion. Our derivation of Newton’s method above ﬁnds roots of f cid:48 , so the secant method could be used to eliminate f cid:48  cid:48  but not f cid:48  from the optimization formula. One-dimensional situations in which f cid:48  is known but not f cid:48  cid:48  are relatively rare. A more suitable parallel is to replace line segments through the last two iterates, used to approximate f in the se- cant method for root-ﬁnding, with parabolas through the last three iterates. The resulting algorithm, known as successive parabolic interpolation, also minimizes a quadratic approxi- mation of f at each iteration, but rather than using f  xk , f cid:48  xk , and f cid:48  cid:48  xk  to construct the approximation, it uses f  xk , f  xk−1 , and f  xk−2 . This technique can converge su- perlinearly; in practice, however, it can have drawbacks that make other methods discussed in this chapter more preferable. We explore its design in Exercise 9.3.  9.3.2 Golden Section Search  Since Newton’s method for optimization is so closely linked to root-ﬁnding, we might ask whether a similar adaptation can be applied to bisection. Unfortunately, this transition is not obvious. A primary reason for using bisection is that it employs the weakest assumption on f needed to ﬁnd roots: continuity. Continuity is enough to prove the Intermediate Value Theorem, which justiﬁes convergence of bisection. The Intermediate Value Theorem does not apply to extrema of a function in any intuitive way, so it appears that directly using bisection to minimize a function is not so straightforward.  It is valuable, however, to have at least one minimization algorithm available that does not require diﬀerentiability of f as an underlying assumption. After all, there are non- diﬀerentiable functions that have clear minima, like f  x  ≡ x at x = 0. To this end, one alternative assumption might be that f is unimodular :   Unconstrained Optimization  cid:4  171  Figure9.7 The golden section search algorithm ﬁnds minima of unimodular functions f  x  on the interval [a, b] even if they are not diﬀerentiable.  Deﬁnition 9.5  Unimodular . A function f : [a, b] → R is unimodular if there exists x∗ ∈ [a, b] such that f is decreasing  or non-increasing  for x ∈ [a, x∗] and increasing  or non-decreasing  for x ∈ [x∗, b]. In other words, a unimodular function decreases for some time, and then begins increas- ing; no localized minima are allowed. Functions like x are not diﬀerentiable but still are unimodular. Suppose we have two values x0 and x1 such that a < x0 < x1 < b. We can make two observations that will help us formulate an optimization technique for a unimodular function f  x :    If f  x0  ≥ f  x1 , then f  x  ≥ f  x1  for all x ∈ [a, x0]. Thus, the interval [a, x0] can  be discarded in a search for the minimum of f .    If f  x1  ≥ f  x0 , then f  x  ≥ f  x0  for all x ∈ [x1, b], and we can discard the interval  [x1, b].  This structure suggests a bisection-like minimization algorithm beginning with the interval [a, b] and iteratively removing pieces according to the rules above. In such an algorithm, we could remove a third of the interval each iteration. This requires two evaluations of f , at x0 = 2a 3 + b 3 and x1 = a 3 + 2b 3. If evaluating f is expensive, however, we may attempt to reduce the number of evaluations per iteration to one.  To design such a method reducing the computational load, we will focus on the case when a = 0 and b = 1; the strategies we derive below eventually will work more generally by shifting and scaling. In the absence of more information about f , we will make a symmetric  function Golden-Section-Search f  x , a, b   2  √5 − 1  τ ← 1 x0 ← a +  1 − τ   b − a  x1 ← a + τ  b − a  f0 ← f  x0  f1 ← f  x1  for k ← 1, 2, 3, . . . if b − a < ε then return x∗ = 1 else if f0 ≥ f1 then  2  a + b   a ← x0 x0 ← x1 f0 ← f1 x1 ← a + τ  b − a  f1 ← f  x1  else if f1 > f0 then b ← x1 x1 ← x0 f1 ← f0 x0 ← a +  1 − τ   b − a  f0 ← f  x0    cid:3  Initial division of interval a < x0 < x1 < b   cid:3  Function values at x0 and x1   cid:3  Golden section search converged   cid:3  Remove the interval [a, x0]  cid:3  Move left side  cid:3  Reuse previous iteration   cid:3  Generate new sample   cid:3  Remove the interval [x1, b]  cid:3  Move right side  cid:3  Reuse previous iteration   cid:3  Generate new sample   172  cid:4  Numerical Algorithms  Figure 9.8 Iterations of golden section search on unimodular f  x  shrink the interval [a, b] by eliminating the left segment [a, x0] or the right segment [x1, b]; each iteration reuses either f  x0  or f  x1  via the construction in §9.3.2. In this illustration, each horizontal line represents an iteration of golden section search, with the values a, x0, x1, and b labeled in the circles.  choice x0 = α and x1 = 1 − α for some α ∈  0, 1 2 ; taking α = 1 3 recovers the evenly divided technique suggested above. Now, suppose that during minimization we can eliminate the rightmost interval [x1, b] by the rules listed above. In the next iteration, the search interval shrinks to [0, 1− α], with x0 = α 1 − α  and x1 =  1 − α 2. To reuse f  α , we could set  1 − α 2 = α, yielding:  α =  1 − α =  1 2 1 2  √5   3 −  √5 − 1 .  The value 1 − α ≡ τ above is the golden ratio! A symmetric argument shows that the same choice of α works if we had removed the left interval instead of the right one. In short, “trisection” algorithms minimizing unimodular functions f  x  dividing intervals into segments with length determined using this ratio can reuse a function evaluation from one iteration to the next.  The golden section search algorithm, documented in Figure 9.7 and illustrated in Fig- ure 9.8, makes use of this construction to minimize a unimodular function f  x  on the interval [a, b] via subdivision with one evaluation of f  x  per iteration. It converges uncon- ditionally and linearly, since a fraction α of the interval [a, b] bracketing the minimum is removed in each step.  f  x   x  x1  b  Iteration 1  b  Iteration 2  a  a  a  x0  x1  b  b  x0  x1  x0  a  Iteration 3  x0 x1  Iteration 4   Unconstrained Optimization  cid:4  173  Figure 9.9 The gradient descent algorithm iteratively minimizes f : Rn → R by solv- ing one-dimensional minimizations through the gradient direction. Line-Search can be one of the methods from §9.3 for minimization in one dimension. In faster, more advanced techniques, this method can ﬁnd suboptimal t∗ > 0 that still de- creases g t  suﬃciently to make sure the optimization does not get stuck.  When f is not globally unimodular, golden section search does not apply unless we can ﬁnd some [a, b] such that f is unimodular on that interval. In some cases, [a, b] can be guessed by attempting to bracket a local minimum of f . For example, [101] suggests stepping farther and farther away from some starting point x0 ∈ R, moving downhill from f  x0  until f increases again, indicating the presence of a local minimum. 9.4 MULTIVARIABLE STRATEGIES  We continue to parallel our discussion of root-ﬁnding by expanding from single-variable to multivariable problems. As with root-ﬁnding, multivariable optimization problems are considerably more diﬃcult than optimization in a single variable, but they appear so many times in practice that they are worth careful consideration.  Here, we will consider only the case that f : Rn → R is twice-diﬀerentiable. Optimization methods similar to golden section search for non-diﬀerentiable functions are less common and are diﬃcult to formulate. See, e.g., [74, 17] for consideration of non-diﬀerentiable opti- mization, subgradients, and related concepts.  9.4.1 Gradient Descent From our previous discussion, ∇f   cid:126 x  points in the direction of “steepest ascent” of f at  cid:126 x and −∇f   cid:126 x  points in the direction of “steepest descent.” If nothing else, these properties suggest that when ∇f   cid:126 x   cid:54 =  cid:126 0, for small α > 0, f   cid:126 x − α∇f   cid:126 x   ≤ f   cid:126 x . Suppose our current estimate of the minimizer of f is  cid:126 xk. A reasonable iterative mini- mization strategy should seek the next iterate  cid:126 xk+1 so that f   cid:126 xk+1  < f   cid:126 xk . Since we do not expect to ﬁnd a global minimum in one shot, we can make restrictions to simplify the search for  cid:126 xk+1. A typical simpliﬁcation is to use a one-variable algorithm from §9.3 on f restricted to a line through  cid:126 xk; once we solve the one-dimensional problem for  cid:126 xk+1, we choose a new line through  cid:126 xk+1 and repeat.  Consider the function gk t  ≡ f   cid:126 xk − t∇f   cid:126 xk  , which restricts f to the line through  cid:126 xk parallel to −∇f   cid:126 xk . We have shown that when ∇f   cid:126 xk   cid:54 =  cid:126 0, g t  < g 0  for small t > 0. Hence, this is a reasonable direction for a restricted search for the new iterate. The resulting gradient descent algorithm shown in Figure 9.9 and illustrated in Figure 9.10 iteratively solves one-dimensional problems to improve  cid:126 xk.  function Gradient-Descent f   cid:2 x ,  cid:2 x0    cid:2 x ←  cid:2 x0 for k ← 1, 2, 3, . . .  Define-Function g t  ≡ f   cid:2 x − t∇f   cid:2 x    t∗ ← Line-Search g t , t ≥ 0   cid:2 x ←  cid:2 x − t∗∇f   cid:2 x  if  cid:6 ∇f   cid:2 x  cid:6 2 < ε then return x∗ =  cid:2 x   cid:3  Update estimate of minimum   174  cid:4  Numerical Algorithms  Figure 9.10 Gradient descent on a function f : R2 → R, whose level sets are shown in gray. The gradient ∇f   cid:126 x  points perpendicular to the level sets of f , as in Figure 1.6; gradient descent iteratively minimizes f along the line through this direction.  Each iteration of gradient descent decreases f   cid:126 xk , so these values converge assuming they are bounded below. The approximations  cid:126 xk only stop changing when ∇f   cid:126 xk  ≈  cid:126 0, showing that gradient descent must at least reach a local minimum; convergence can be slow for some functions f , however.  Rather than solving the one-variable problem exactly in each step, line search can be replaced by a method that ﬁnds points along the line that decrease the objective a non- negligible if suboptimal amount. It is more diﬃcult to guarantee convergence in this case, since each step may not reach a local minimum on the line, but the computational savings can be considerable since full one-dimensional minimization is avoided; see [90] for details. Taking the more limited line search strategy to an extreme, sometimes a ﬁxed t > 0 is used for all iterations to avoid line search altogether. This choice of t is known in machine learning as the learning rate and trades oﬀ between taking large minimization steps and potentially skipping over a minimum. Gradient descent with a constant step is unlikely to converge to a minimum in this case, but depending on f it may settle in some neighborhood of the optimal point; see Exercise 9.7 for an error bound of this method in one case.  9.4.2 Newton’s Method in Multiple Variables Paralleling our derivation of the single-variable case in §9.3.1, we can write a Taylor series approximation of f : Rn → R using its Hessian matrix Hf :  f   cid:126 x  ≈ f   cid:126 xk  + ∇f   cid:126 xk  cid:62  ·   cid:126 x −  cid:126 xk  +    cid:126 x −  cid:126 xk  cid:62  · Hf   cid:126 xk  ·   cid:126 x −  cid:126 xk .  1 2  Diﬀerentiating with respect to  cid:126 x and setting the result equal to zero yields the following iterative scheme:   cid:126 xk+1 =  cid:126 xk − [Hf   cid:126 xk ]−1∇f   cid:126 xk .  This expression generalizes Newton’s method from §9.3.1, and once again it converges quadratically when  cid:126 x0 is near a minimum. Newton’s method can be more eﬃcient than gradient descent depending on the objective f since it makes use of both ﬁrst- and second-order information. Gradient descent has no knowledge of Hf ; it proceeds analogously to walking downhill by looking only at your feet. By using Hf , Newton’s method has a larger picture of the shape of f nearby.  Each iteration of gradient descent potentially requires many evaluations of f during line search. On the other hand, we must evaluate and invert the Hessian Hf during each   cid:2 x4   cid:2 x3   cid:2 x2   cid:2 x1   cid:2 x0   Unconstrained Optimization  cid:4  175  iteration of Newton’s method. These implementation diﬀerences do not aﬀect the number of iterations to convergence but do aﬀect the computational time taken per iteration of the two methods.  When Hf is nearly singular, Newton’s method can take very large steps away from the current estimate of the minimum. These large steps are a good idea if the second- order approximation of f is accurate, but as the step becomes large the quality of this approximation can degenerate. One way to take more conservative steps is to “dampen” the change in  cid:126 x using a small multiplier γ > 0:   cid:126 xk+1 =  cid:126 xk − γ[Hf   cid:126 xk ]−1∇f   cid:126 xk .  A more expensive but safer strategy is to do line search from  cid:126 xk along the direction −[Hf   cid:126 xk ]−1∇f   cid:126 xk . When Hf is not positive deﬁnite, the objective locally might look like a saddle or peak rather than a bowl. In this case, jumping to an approximate stationary point might not make sense. To address this issue, adaptive techniques check if Hf is positive deﬁnite before applying a Newton step; if it is not positive deﬁnite, the methods revert to gradient descent to ﬁnd a better approximation of the minimum. Alternatively, they can modify Hf , for example, by projecting onto the closest positive deﬁnite matrix  see Exercise 9.8 .  9.4.3 Optimization without Hessians: BFGS  Newton’s method can be diﬃcult to apply to complicated or high-dimensional functions f : Rn → R. The Hessian of f is often more expensive to evaluate than f or ∇f , and each Hessian Hf is used to solve only one linear system of equations, eliminating potential savings from LU or QR factorization. Additionally, Hf has size n× n, requiring O n2  space, which might be too large. Since Newton’s method deals with approximations of f in each iteration anyway, we might attempt to formulate less expensive second-order approximations that still outperform gradient descent.  As in our discussion of root-ﬁnding in §8.2.2, techniques for minimization that imitate Newton’s method but use approximate derivatives are called quasi-Newton methods. They can have similarly strong convergence properties without the need for explicit re-evaluation and even inversion of the Hessian at each iteration. Here, we will follow the development of [90] to motivate one modern technique for quasi-Newton optimization.  Suppose we wish to minimize f : Rn → R iteratively. Near the current estimate  cid:126 xk of  the minimizer, we might estimate f with a quadratic function:  f   cid:126 xk + δ cid:126 x  ≈ f   cid:126 xk  + ∇f   cid:126 xk  · δ cid:126 x +  1 2   δ cid:126 x  cid:62 Bk δ cid:126 x .  Here, we require that our approximation agrees with f to ﬁrst order at  cid:126 xk, but we will allow the estimate of the Hessian Bk to diﬀer from the actual Hessian of f .  Slightly generalizing Newton’s method in §9.4.2, this quadratic approximation is mini- mized by taking δ cid:126 x = −B−1 k ∇f   cid:126 xk . In case  cid:107 δ cid:126 x cid:107 2 is large and we do not wish to take such a large step, we will allow ourselves to scale this diﬀerence by a step size αk determined, e.g., using a line search procedure, yielding the iteration  Our goal is to estimate Bk+1 by updating Bk, so that we can repeat this process.   cid:126 xk+1 =  cid:126 xk − αkB−1  k ∇f   cid:126 xk .   176  cid:4  Numerical Algorithms  Figure 9.11 The BFGS algorithm for ﬁnding a local minimum of diﬀerentiable f   cid:126 x  without its Hessian. The function Compute-Alpha ﬁnds large α > 0 satisfying  cid:126 y ·  cid:126 s > 0, where  cid:126 y = ∇f   cid:126 x +  cid:126 s  − ∇f   cid:126 x  and  cid:126 s = α cid:126 p.  The Hessian of f is nothing more than the derivative of ∇f , so like Broyden’s method  we can use previous iterates to impose a secant-style condition on Bk+1:  Bk+1  cid:126 xk+1 −  cid:126 xk  = ∇f   cid:126 xk+1  − ∇f   cid:126 xk .  For convenience of notation, we will deﬁne  cid:126 sk ≡  cid:126 xk+1 −  cid:126 xk and  cid:126 yk ≡ ∇f   cid:126 xk+1  − ∇f   cid:126 xk , simplifying this condition to Bk+1 cid:126 sk =  cid:126 yk.  Given the optimization at hand, we wish for Bk to have two properties:    Bk should be a symmetric matrix, like the Hessian Hf .   Bk should be positive  semi- deﬁnite, so that we are seeking minima of f rather than  maxima or saddle points.  These conditions eliminate the possibility of using the Broyden estimate we developed in the previous chapter.  The positive deﬁnite constraint implicitly puts a condition on the relationship between  cid:126 sk and  cid:126 yk. Pre-multiplying the relationship Bk+1 cid:126 sk =  cid:126 yk by  cid:126 s cid:62 k shows  cid:126 s cid:62 k Bk+1 cid:126 sk =  cid:126 s cid:62 k  cid:126 yk. For Bk+1 to be positive deﬁnite, we must then have  cid:126 sk ·  cid:126 yk > 0. This observation can guide our choice of αk; it must hold for suﬃciently small αk > 0. Assume that  cid:126 sk and  cid:126 yk satisfy the positive deﬁnite compatibility condition. Then, we can write down a Broyden-style optimization problem leading to an updated Hessian ap- proximation Bk+1:  minimizeBk+1   cid:107 Bk+1 − Bk cid:107  subject to B cid:62 k+1 = Bk+1 Bk+1 cid:126 sk =  cid:126 yk.  For appropriate choice of norms  cid:107 · cid:107 , this optimization yields the well-known DFP  Davidon- Fletcher-Powell  iterative scheme. Rather than working out the details of the DFP scheme, we derive a more popular method known as the BFGS  Broyden-Fletcher-Goldfarb-Shanno  algorithm, in Figure 9.11. The BFGS algorithm is motivated by reconsidering the construction of Bk+1 in DFP. We  function BFGS f  x , x0   if  f  x  < ε then  H ← In×n x ← x0 for k ← 1, 2, 3, . . . return x∗ = x p ← −Hk∇f  x  α ← Compute-Alpha  s ← pα x ← x + s y ← ∇f  x + s  − ∇f  x  ρ ← 1 y·s H ←  In×n − ρsy  H In×n − ρys   + ρss  y,x,p,f     Next search direction Satisfy positive deﬁnite condition Displacement of x Update estimate Change in gradient  Apply BFGS update to inverse Hessian approximation   Unconstrained Optimization  cid:4  177  k  k+1!  and B−1  use Bk when minimizing the second-order approximation, taking δ cid:126 x = −B−1 k ∇f   cid:126 xk . Based on this formula, the behavior of our iterative minimizer is dictated by the inverse matrix B−1 k . Asking that  cid:107 Bk+1 − Bk cid:107  is small can still imply relatively large diﬀerences between B−1 With this observation in mind, BFGS makes a small alteration to the optimization for Bk. Rather than updating Bk in each iteration, we can compute its inverse Hk ≡ B−1 directly. We choose to use standard notation for BFGS in this section, but a common point of confusion is that H now represents an approximate inverse Hessian; this is the not the same as the Hessian Hf in §9.4.2 and elsewhere. Now, the condition Bk+1 cid:126 sk =  cid:126 yk gets reversed to  cid:126 sk = Hk+1 cid:126 yk; the condition that Bk is symmetric is the same as the condition that Hk is symmetric. After these changes, the BFGS algorithm updates Hk by solving an optimization problem  k  minimizeHk+1   cid:107 Hk+1 − Hk cid:107  subject to H cid:62 k+1 = Hk+1  cid:126 sk = Hk+1 cid:126 yk.  This construction has the convenient side beneﬁt of not requiring matrix inversion to com- pute δ cid:126 x = −Hk∇f   cid:126 xk . To derive a formula for Hk+1, we must decide on a matrix norm  cid:107 · cid:107 . The Frobenius norm looks closest to least-squares optimization, making it likely we can generate a closed-form expression for Hk+1. This norm, however, has one serious drawback for modeling Hessian matrices and their inverses. The Hessian matrix has entries  Hf  ij = ∂2f ∂xi∂xj. Often, the quantities xi for diﬀerent i can have diﬀerent units. Consider maximizing the proﬁt  in dollars  made by selling a cheeseburger of radius r  in inches  and price p  in dollars , a function f :  inches, dollars  → dollars. Squaring quantities in diﬀerent units and adding them up does not make sense. Suppose we ﬁnd a symmetric positive deﬁnite matrix W so that W  cid:126 sk =  cid:126 yk; we will check in the exercises that such a matrix exists. This matrix takes the units of  cid:126 sk =  cid:126 xk+1 −  cid:126 xk to those of  cid:126 yk = ∇f   cid:126 xk+1  − ∇f   cid:126 xk . Taking inspiration from the expression  cid:107 A cid:107 2 Fro = Tr A cid:62 A , we can deﬁne a weighted Frobenius norm of a matrix A as  Unlike the Frobenius norm of Hk+1, this expression has consistent units when applied to the optimization for Hk+1.  When both W and A are symmetric with columns  cid:126 wi and  cid:126 ai, respectively, expanding  the expression above shows:   cid:107 A cid:107 2  W ≡ Tr A cid:62 W  cid:62 AW  .  W = cid:88 ij  cid:107 A cid:107 2     cid:126 wi ·  cid:126 aj    cid:126 wj ·  cid:126 ai .  This choice of norm combined with the choice of W yields a particularly clean formula for Hk+1 given Hk,  cid:126 sk, and  cid:126 yk:  Hk+1 =  In×n − ρk cid:126 sk cid:126 y cid:62 k  Hk In×n − ρk cid:126 yk cid:126 s cid:62 k   + ρk cid:126 sk cid:126 s cid:62 k ,  where ρk ≡ 1  cid:126 yk· cid:126 sk. We show in the appendix to this chapter how to derive this formula, which remarkably has no W dependence. The proof requires a number of algebraic steps but conceptually is no more diﬃcult than direct application of Lagrange multipliers for constrained optimization  see Theorem 1.1 .   178  cid:4  Numerical Algorithms  The BFGS algorithm avoids the need to compute and invert a Hessian matrix for f , but it still requires O n2  storage for Hk. The L-BFGS  “Limited-Memory BFGS”  variant avoids this issue by keeping a limited history of vectors  cid:126 yk and  cid:126 sk and using these to apply Hk by expanding its formula recursively. L-BFGS can have better convergence than BFGS despite its compact use of space, since old vectors  cid:126 yk and  cid:126 sk may no longer be relevant and should be ignored. Exercise 9.11 derives this technique.  9.5 EXERCISES 9.1 Suppose A ∈ Rn×n. Show that f   cid:126 x  =  cid:107 A cid:126 x −  cid:126 b cid:107 2  g  cid:126 x  =  cid:126 x cid:62 A cid:126 x +  cid:126 b cid:62  cid:126 x + c convex?  9.2 Some observations about convex and quasiconvex functions:  2 is a convex function. When is   a  Show that every convex function is quasiconvex, but that some quasiconvex func-  tions are not convex.   b  Show that any local minimum of a continuous, strictly quasiconvex function f : Rn → R is also a global minimum of f . Here, strict quasiconvexity replaces the ≤ in the deﬁnition of quasiconvex functions with <.   c  Show that the sum of two convex functions is convex, but give a counterexample  showing that the sum of two quasiconvex functions may not be quasiconvex.   d  Suppose f  x  and g x  are quasiconvex. Show that h x  = max f  x , g x   is  quasiconvex.  9.3 In §9.3.1, we suggested the possibility of using parabolas rather than secants to min- imize a function f : R → R without knowing any of its derivatives. Here, we outline the design of such an algorithm:   a  Suppose we are given three points  x1, y1 ,  x2, y2 ,  x3, y3  with distinct x values. Show that the vertex of the parabola y = ax2 + bx + c through these points is given by:  x = x2 −   x2 − x1 2 y2 − y3  −  x2 − x3 2 y2 − y1  2 x2 − x1  y2 − y3  −  x2 − x3  y2 − y1   .   b  Use this formula to propose an iterative technique for minimizing a function of  one variable without using any of its derivatives.   c  What happens when the three points in Exercise 9.3a are collinear? Does this  suggest a failure mode of successive parabolic interpolation?   d  Does the formula in Exercise 9.3a distinguish between maxima and minima of  parabolas? Does this suggest a second failure mode?  9.4 Show that a strictly convex function f : [a, b] → R is unimodular. 9.5 We might ask how well we can expect methods like golden section search can work after introducing ﬁnite precision arithmetic. We step through a few analytical steps from [101]:   Unconstrained Optimization  cid:4  179   a  Suppose we have bracketed a local minimum x∗ of diﬀerentiable f  x  in a small  interval. Justify the following approximation in this interval:  f  x  ≈ f  x∗  +  f cid:48  cid:48  x∗  x − x∗ 2.  1 2   b  Suppose we wish to reﬁne the interval containing the minimum until the second term in this approximation is negligible. Show that if we wish to upper-bound the absolute value of the ratio of the two terms in Exercise 9.5a by ε, we should enforce  x − x∗ < cid:115  2εf  x∗   f cid:48  cid:48  x∗   .   c  By taking ε to be machine precision as in §2.1.2, conclude that the size of the interval in which f  x  and f  x∗  are indistinguishable numerically grows like √ε. Based on this observation, can golden section search bracket a minimizer within machine precision?  Hint: For small ε > 0, √ε  cid:29  ε.  DH 9.6 For a convex function f : U → Rn, where U ⊆ Rn is convex and open, deﬁne a  subgradient of f at  cid:126 x0 ∈ U to be any vector  cid:126 s ∈ Rn such that  f   cid:126 x  − f   cid:126 x0  ≥  cid:126 s ·   cid:126 x −  cid:126 x0   for all  cid:126 x ∈ U [112]. The subgradient is a plausible choice for generalizing the notion of a gradient at a point where f is not diﬀerentiable. The subdiﬀerential ∂f   cid:126 x0  is the set of all subgradients of f at  cid:126 x0. For the remainder of this question, assume that f is convex and continuous:   a  What is ∂f  0  for the function f  x  = x?  b  Suppose we wish to minimize  convex and continuous  f : Rn → R, which may not be diﬀerentiable everywhere. Propose an optimality condition involving sub- diﬀerentials for a point  cid:126 x∗ to be a minimizer of f . Show that your condition holds if and only if  cid:126 x∗ globally minimizes f .  DH 9.7 Continuing the previous problem, the subgradient method extends gradient descent to a wider class of functions. Analogously to gradient descent, the subgradient method performs the iteration   cid:126 xk+1 ≡  cid:126 xk − αk cid:126 gk,  where αk is a step size and gk is any subgradient of f at  cid:126 xk. This method might not decrease f in each iteration, so instead we keep track of the best iterate we have seen so far,  cid:126 xbest  . We will use  cid:126 x∗ to denote the minimizer of f on U .  k  In the following parts, assume that we ﬁx α > 0 to be a constant with no dependence on k, that f is Lipschitz continuous with constant C > 0, and that  cid:107  cid:126 x1 −  cid:126 x∗ cid:107 2 ≤ B for some B > 0. Under these assumptions, we will show that  lim k→∞  f   cid:126 xbest  k    ≤ f   cid:126 x∗  +  C 2 2  α,  a bound characterizing convergence of the subgradient method.   180  cid:4  Numerical Algorithms   a  Derive an upper bound for the error  cid:107  cid:126 xk+1 −  cid:126 x∗ cid:107 2  2 of  cid:126 xk+1 in terms of  cid:107  cid:126 xk −  cid:126 x∗ cid:107 2 2,   cid:126 gk, α, f   cid:126 xk , and f   cid:126 x∗ .   b  By recursively applying the result from Exercise 9.7a, provide an upper bound for the squared error of  cid:126 xk+1 in terms of B, α, the subgradients, and evaluations of f .   c   Incorporate f   cid:126 xbest your result, and take a limit as k → ∞ to obtain the desired conclusion.    and the bounds given at the beginning of the problem into  k   d  Suppose we are willing to run subgradient descent for exactly k steps. Suggest a  choice of α for this case; your formula for α can and should involve k.  SC 9.8 This problem will demonstrate how to project a Hessian onto the nearest positive deﬁnite matrix. Some optimization techniques use this operation to avoid attempting to minimize in directions where a function is not bowl-shaped.   a  Suppose M, U ∈ Rn×n, where M is symmetric and U is orthogonal. Show that   cid:107 U M U cid:62  cid:107 Fro =  cid:107 M cid:107 Fro.   b  Decompose M = QΛQ cid:62 , where Λ is a diagonal matrix of eigenvalues and Q is an orthogonal matrix of eigenvectors. Using the result of the previous part, explain how the positive semideﬁnite matrix ¯M closest to M with respect to the Frobenius norm can be constructed by clamping the negative eigenvalues in Λ to zero.  9.9 Our derivation of the BFGS algorithm in §9.4.3 depended on the existence of a sym- metric positive deﬁnite matrix W satisfying W  cid:126 sk =  cid:126 yk. Show that one such matrix is W ≡ ¯Gk, where ¯Gk is the average Hessian [90]:  ¯Gk ≡ cid:90  1  0  Hf   cid:126 xk + τ cid:126 sk  dτ.  Do we ever have to compute W in the course of running BFGS?  9.10 Derive an explicit update formula for obtaining Bk+1 from Bk in the Davidon- Fletcher-Powell scheme mentioned in §9.4.3. Use the  cid:107  ·  cid:107 W norm introduced in the derivation of BFGS, but with the reversed assumption W  cid:126 yk =  cid:126 sk.  9.11 The matrix H used in the BFGS algorithm generally is dense, requiring O n2  storage  for f : Rn → R. This scaling may be infeasible for large n.  a  Provide an alternative approach to storing H requiring O nk  storage in iteration  k of BFGS. Hint: Your algorithm may have to “remember” data from previous iterations.   b   If we need to run for many iterations, the storage from the previous part can exceed the O n2  limit we were attempting to avoid. Propose an approximation to H that uses no more than O nkmax  storage, for a user-speciﬁed constant kmax.  9.12 The BFGS and DFP algorithms update  inverse  Hessian approximations using matri- ces of rank two. For simplicity, the symmetric-rank-1  SR1  update restricts changes to be rank one instead [90].   Unconstrained Optimization  cid:4  181   a  Suppose Bk+1 = Bk + σ cid:126 v cid:126 v cid:62 , where σ = 1 and  cid:126 yk = Bk+1 cid:126 sk. Show that under  these conditions we must have   b  Suppose Hk ≡ B−1    cid:126 yk − Bk cid:126 sk   cid:126 yk − Bk cid:126 sk  cid:62   .  Bk+1 = Bk +    cid:126 yk − Bk cid:126 sk  cid:62  cid:126 sk k . Show that Hk can be updated as  Hk+1 = Hk +    cid:126 sk − Hk cid:126 yk   cid:126 sk − Hk cid:126 yk  cid:62   .    cid:126 sk − Hk cid:126 yk  cid:62  cid:126 yk  Hint: Use the result of Exercise 8.7.  9.13 Here we examine some changes to the gradient descent algorithm for unconstrained  optimization on a function f .   a   In machine learning, the stochastic gradient descent algorithm can be used to optimize many common objective functions:  taking the form f   cid:126 x  = 1   i  Give an example of a practical optimization problem with an objective i=1 g  cid:126 xi −  cid:126 x  for some function g : Rn → R.  ii  Propose a randomized approximation of ∇f summing no more than k terms  for some k  cid:28  N   assuming the  cid:126 xi’s are similar to one another. Discuss advantages and drawbacks of using such an approximation.  N cid:80 N   b  The “line search” part of gradient descent must be considered carefully:   i  Suppose an iterative optimization routine gives a sequence of estimates  cid:126 x1,  cid:126 x2, . . . of the position  cid:126 x∗ of the minimum of f . Is it enough to assume f   cid:126 xk  < f   cid:126 xk−1  to guarantee that the  cid:126 xk’s converge to a local minimum? Why?   ii  Suppose we run gradient descent. If we suppose f   cid:126 x  ≥ 0 for all  cid:126 x and that we are able to ﬁnd t∗ exactly in each iteration, show that f   cid:126 xk  converges as k → ∞.  iii  Explain how the optimization in 9.13 b ii for t∗ can be overkill. In partic- ular, explain how the Wolfe conditions  you will have to look these up!  relax the assumption that we can ﬁnd t∗.  9.14 Sometimes we are greedy and wish to optimize multiple objectives simultaneously. For example, we might want to ﬁre a rocket to reach an optimal point in time and space. It may not be possible to carry out both tasks simultaneously, but some theories attempt to reconcile multiple optimization objectives. Suppose we are given functions f1  cid:126 x , f2  cid:126 x , . . . , fk  cid:126 x . A point  cid:126 y is said to Pareto dominate another point  cid:126 x if fi  cid:126 y  ≤ fi  cid:126 x  for all i and fj  cid:126 y  < fj  cid:126 x  for some j ∈ {1, . . . , k}. A point  cid:126 x∗ is Pareto optimal if it is not dominated by any point  cid:126 y. Assume f1, . . . , fk are strictly convex functions on a closed, convex set S ⊂ Rn  in particular, assume each fi is minimized at a unique point  cid:126 x∗i  .   a  Show that the set of Pareto optimal points is nonempty in this case.   b  Suppose  cid:80 i γi = 1 and γi > 0 for all i. Show that the minimizer  cid:126 x∗ of g  cid:126 x  ≡  Note: One strategy for multi-objective optimization is to promote  cid:126 γ to a variable   cid:80 i γifi  cid:126 x  is Pareto optimal. with constraints  cid:126 γ ≥  cid:126 0 and cid:80 i γi = 1.   182  cid:4  Numerical Algorithms   c  Suppose  cid:126 x∗i minimizes fi  cid:126 x  over all possible  cid:126 x. Write vector  cid:126 z ∈ Rk with com- ponents zi = fi  cid:126 x∗i  . Show that the minimizer  cid:126 x∗ of h  cid:126 x  ≡  cid:80 i fi  cid:126 x  − zi 2 is  Pareto optimal. Note: This part and the previous part represent two possible scalarizations of the multi-objective optimization problem that can be used to ﬁnd Pareto optimal points.  9.6 APPENDIX: DERIVATION OF BFGS UPDATE In this optional appendix, we derive in detail the BFGS update from §9.4.3.∗ Our optimiza- tion for Hk+1 has the following Lagrange multiplier expression  for ease of notation we take Hk+1 ≡ H and Hk = H∗ : Λ ≡ cid:88 ij = cid:88 ij     cid:126 wi ·   cid:126 hj −  cid:126 h∗j      cid:126 wj ·   cid:126 hi −  cid:126 h∗i    − cid:88 i<j    cid:126 wi ·   cid:126 hj −  cid:126 h∗j      cid:126 wj ·   cid:126 hi −  cid:126 h∗i    − cid:88 ij  αij Hij − Hji  −  cid:126 λ cid:62  H cid:126 yk −  cid:126 sk  αijHij −  cid:126 λ cid:62  H cid:126 yk −  cid:126 sk  if we deﬁne αij = −αji  Taking derivatives to ﬁnd critical points shows  for  cid:126 y ≡  cid:126 yk,  cid:126 s ≡  cid:126 sk :  0 =  ∂Λ ∂Hij  2wi cid:96    cid:126 wj ·   cid:126 h cid:96  −  cid:126 h∗ cid:96     − αij − λiyj wi cid:96  W  cid:62  H − H∗  j cid:96  − αij − λiyj  W  cid:62  H − H∗  j cid:96 w cid:96 i − αij − λiyj by symmetry of W  = cid:88  cid:96  = 2 cid:88  cid:96  = 2 cid:88  cid:96  = 2 W  cid:62  H − H∗ W  ji − αij − λiyj = 2 W  H − H∗ W  ij − αij − λiyj by symmetry of W and H.  So, in matrix form we have the following list of facts:  0 = 2W  H − H∗ W − A −  cid:126 λ cid:126 y cid:62 , where Aij = αij A cid:62  = −A W  cid:62  = W H cid:62  = H  H∗  cid:62  = H∗  H cid:126 y =  cid:126 s W  cid:126 s =  cid:126 y.  We can achieve a pair of relationships using transposition combined with symmetry of H and W and asymmetry of A:  0 = 2W  H − H∗ W − A −  cid:126 λ cid:126 y cid:62  0 = 2W  H − H∗ W + A −  cid:126 y cid:126 λ cid:62   =⇒ 0 = 4W  H − H∗ W −  cid:126 λ cid:126 y cid:62  −  cid:126 y cid:126 λ cid:62 . ∗Special thanks to Tao Du for debugging several parts of this derivation.   Unconstrained Optimization  cid:4  183  Post-multiplying this relationship by  cid:126 s shows:  Now, take the dot product with  cid:126 s:   cid:126 0 = 4  cid:126 y − W H∗ cid:126 y  −  cid:126 λ  cid:126 y ·  cid:126 s  −  cid:126 y  cid:126 λ ·  cid:126 s .  0 = 4  cid:126 y ·  cid:126 s  − 4  cid:126 y cid:62 H∗ cid:126 y  − 2  cid:126 y ·  cid:126 s   cid:126 λ ·  cid:126 s .   cid:126 λ ·  cid:126 s = 2ρ cid:126 y cid:62   cid:126 s − H∗ cid:126 y , for ρ ≡ 1  cid:126 y· cid:126 s.  This shows:  Now, we substitute this into our vector equality:   cid:126 0 = 4  cid:126 y − W H∗ cid:126 y  −  cid:126 λ  cid:126 y ·  cid:126 s  −  cid:126 y  cid:126 λ ·  cid:126 s  from before = 4  cid:126 y − W H∗ cid:126 y  −  cid:126 λ  cid:126 y ·  cid:126 s  −  cid:126 y[2ρ cid:126 y cid:62   cid:126 s − H∗ cid:126 y ] from our simpliﬁcation  =⇒  cid:126 λ = 4ρ  cid:126 y − W H∗ cid:126 y  − 2ρ2[ cid:126 y cid:62   cid:126 s − H∗ cid:126 y ] cid:126 y.  Post-multiplying by  cid:126 y cid:62  shows:  Taking the transpose,   cid:126 λ cid:126 y cid:62  = 4ρ  cid:126 y − W H∗ cid:126 y  cid:126 y cid:62  − 2ρ2[ cid:126 y cid:62   cid:126 s − H∗ cid:126 y ] cid:126 y cid:126 y cid:62 .   cid:126 y cid:126 λ cid:62  = 4ρ cid:126 y  cid:126 y cid:62  −  cid:126 y cid:62 H∗W   − 2ρ2[ cid:126 y cid:62   cid:126 s − H∗ cid:126 y ] cid:126 y cid:126 y cid:62 .  Combining these results and dividing by four shows:  1 4    cid:126 λ cid:126 y cid:62  +  cid:126 y cid:126 λ cid:62   = ρ 2 cid:126 y cid:126 y cid:62  − W H∗ cid:126 y cid:126 y cid:62  −  cid:126 y cid:126 y cid:62 H∗W   − ρ2[ cid:126 y cid:62   cid:126 s − H∗ cid:126 y ] cid:126 y cid:126 y cid:62 .  Now, we will pre- and post-multiply by W −1. Since W  cid:126 s =  cid:126 y, we can equivalently write  cid:126 s = W −1 cid:126 y. Furthermore, by symmetry of W we then know  cid:126 y cid:62 W −1 =  cid:126 s cid:62 . Applying these identities to the expression above shows:  1 4  W −1  cid:126 λ cid:126 y cid:62  +  cid:126 y cid:126 λ cid:62  W −1 = 2ρ cid:126 s cid:126 s cid:62  − ρH∗ cid:126 y cid:126 s cid:62  − ρ cid:126 s cid:126 y cid:62 H∗ − ρ2  cid:126 y cid:62  cid:126 s  cid:126 s cid:126 s cid:62  + ρ2  cid:126 y cid:62 H∗ cid:126 y  cid:126 s cid:126 s cid:62   = 2ρ cid:126 s cid:126 s cid:62  − ρH∗ cid:126 y cid:126 s cid:62  − ρ cid:126 s cid:126 y cid:62 H∗ − ρ cid:126 s cid:126 s cid:62  +  cid:126 sρ2  cid:126 y cid:62 H∗ cid:126 y  cid:126 s cid:62   by deﬁnition of ρ  = ρ cid:126 s cid:126 s cid:62  − ρH∗ cid:126 y cid:126 s cid:62  − ρ cid:126 s cid:126 y cid:62 H∗ +  cid:126 sρ2  cid:126 y cid:62 H∗ cid:126 y  cid:126 s cid:62 .  Finally, we can conclude our derivation of the BFGS step as follows:  0 = 4W  H − H∗ W −  cid:126 λ cid:126 y cid:62  −  cid:126 y cid:126 λ cid:62  from before  W −1  cid:126 λ cid:126 y cid:62  +  cid:126 y cid:126 λ cid:62  W −1 + H∗  =⇒ H =  1 4  = ρ cid:126 s cid:126 s cid:62  − ρH∗ cid:126 y cid:126 s cid:62  − ρ cid:126 s cid:126 y cid:62 H∗ +  cid:126 sρ2  cid:126 y cid:62 H∗ cid:126 y  cid:126 s cid:62  + H∗ from the last paragraph = H∗ I − ρ cid:126 y cid:126 s cid:62   + ρ cid:126 s cid:126 s cid:62  − ρ cid:126 s cid:126 y cid:62 H∗ +  ρ cid:126 s cid:126 y cid:62  H∗ ρ cid:126 y cid:126 s cid:62   = H∗ I − ρ cid:126 y cid:126 s cid:62   + ρ cid:126 s cid:126 s cid:62  − ρ cid:126 s cid:126 y cid:62 H∗ I − ρ cid:126 y cid:126 s cid:62   = ρ cid:126 s cid:126 s cid:62  +  I − ρ cid:126 s cid:126 y cid:62  H∗ I − ρ cid:126 y cid:126 s cid:62  .  This ﬁnal expression is exactly the BFGS step introduced in the chapter.    C H A P T E R10  Constrained Optimization  CONTENTS  10.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.2 Theory of Constrained Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.2.1 Optimality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.2.2 KKT Conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.3 Optimization Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.3.1 Sequential Quadratic Programming  SQP  . . . . . . . . . . . . . . . . . . . . . Equality Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Inequality Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.3.2 Barrier Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.4 Convex Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.4.1 Linear Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.4.2 Second-Order Cone Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.4.3 Semideﬁnite Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.4.4 Integer Programs and Relaxations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  10.3.1.1 10.3.1.2  186 189 189 189 192 193 193 193 194 194 196 197 199 200  W E continue our consideration of optimization problems by studying the constrained  case. These problems take the following general form:  minimize f   cid:126 x  subject to g  cid:126 x  =  cid:126 0 h  cid:126 x  ≥  cid:126 0.  Here, f : Rn → R, g : Rn → Rm, and h : Rn → Rp; we call f the objective function and the expressions g  cid:126 x  =  cid:126 0, h  cid:126 x  ≥  cid:126 0 the constraints. This form is extremely generic, so algorithms for solving such problems in the absence of additional assumptions on f , g, or h are subject to degeneracies such as local minima and lack of convergence. In fact, this general problem encodes other problems we already have considered. If we take f   cid:126 x  = h  cid:126 x  ≡ 0, then this constrained optimization becomes root-ﬁnding on g  Chapter 8 , while if we take g  cid:126 x  = h  cid:126 x  ≡  cid:126 0, it reduces to unconstrained optimization on f  Chapter 9 . Despite this bleak outlook, optimization methods handling the general constrained prob- lem can be valuable even when f , g, and h do not have strong structure. In many cases, especially when f is heuristic anyway, ﬁnding a feasible  cid:126 x for which f   cid:126 x  < f   cid:126 x0  starting from an initial guess  cid:126 x0 still represents an improvement from the starting point. One appli- cation of this philosophy would be an economic system in which f measures costs; since we wish to minimize costs, any  cid:126 x decreasing f is a useful—and proﬁtable—output.  185   186  cid:4  Numerical Algorithms  Figure 10.1 “Blobby” shapes are constructed as level sets of a linear combination of functions.  10.1 MOTIVATION  Constrained optimization problems appear in nearly any area of applied math, engineering, and computer science. We already listed many applications of constrained optimization when we discussed eigenvectors and eigenvalues in Chapter 6, since this problem for symmetric matrices A ∈ Rn×n can be posed as ﬁnding critical points of  cid:126 x cid:62 A cid:126 x subject to  cid:107  cid:126 x cid:107 2 = 1. The particular case of eigenvalue computation admits special algorithms that make it a simpler problem. Here, however, we list other optimization problems that do not enjoy the unique structure of eigenvalue problems:  Example 10.1  Geometric projection . Many shapes S in Rn can be written implicitly in the form g  cid:126 x  = 0 for some g. For example, the unit sphere results from taking g  cid:126 x  ≡  cid:107  cid:126 x cid:107 2 2 − 1, while a cube can be constructed by taking g  cid:126 x  =  cid:107  cid:126 x cid:107 1 − 1. Some 3D modeling environments allow users to specify “blobby” objects, as in Figure 10.1, as zero-value level sets of g  cid:126 x  given by  Suppose we are given a point  cid:126 y ∈ R3 and wish to ﬁnd the closest point  cid:126 x ∈ S to  cid:126 y. This problem is solved by using the following constrained minimization:  g  cid:126 x  ≡ c + cid:88 i  aie−bi cid:107  cid:126 x− cid:126 xi cid:107 2 2.  minimize cid:126 x  cid:107  cid:126 x −  cid:126 y cid:107 2 subject to g  cid:126 x  = 0.  Example 10.2  Manufacturing . Suppose you have m diﬀerent materials; you have si units of each material i in stock. You can manufacture k diﬀerent products; product j gives you proﬁt pj and uses cij of material i to make. To maximize proﬁts, you can solve the following optimization for the amount xj you should manufacture of each item j:  maximize cid:126 x  pjxj  subject to xj ≥ 0∀j ∈ {1, . . . , k}  cijxj ≤ si ∀i ∈ {1, . . . , m}.  k cid:88 j=1 k cid:88 j=1  g1  cid:2 x  = c1  g1  cid:2 x  + g2  cid:2 x  = c3  g2  cid:2 x  = c2   Constrained Optimization  cid:4  187  Figure 10.2 Notation for bundle adjustment with two images. Given corresponding points  cid:126 xij marked on images, bundle adjustment simultaneously optimizes for cam- era parameters encoded in Pi and three-dimensional positions  cid:126 yj.  The ﬁrst constraint ensures that you do not make negative amounts of any product, and the second ensures that you do not use more than your stock of each material. This optimization is an example of a linear program, because the objective and constraints are all linear functions. Linear programs allow for inequality constraints, so they cannot always be solved using Gaussian elimination.  Example 10.3  Nonnegative least-squares . We already have seen numerous examples of least-squares problems, but sometimes negative values in the solution vector might not make sense. For example, in computer graphics, an animated model can be expressed as a deforming bone structure plus a meshed “skin”; for each point on the skin a list of weights can be computed to approximate the inﬂuence of the positions of the bone joints on the position of the skin vertices [67]. Such weights should be constrained to be nonnegative to avoid degenerate behavior while the surface deforms. In such a case, we can solve the “nonnegative least-squares” problem:  minimize cid:126 x  cid:107 A cid:126 x −  cid:126 b cid:107 2 subject to xi ≥ 0 ∀i.  Some machine learning methods leverage the sparsity of nonnegative least-squares solu- tions, which often lead to optimal vectors  cid:126 x with xi = 0 for many indices i [113].  Example 10.4  Bundle adjustment . In computer vision, suppose we take pictures of an object from several angles. A natural task is to reconstruct the three-dimensional shape of the object from these pictures. To do so, we might mark a corresponding set of points on each image; we can take  cid:126 xij ∈ R2 to be the position of feature point j on image i, as in Figure 10.2. In reality, each feature point has a position  cid:126 yj ∈ R3 in space, which we would like to compute. Additionally, we must ﬁnd the positions of the cameras themselves, which we can represent as unknown projection matrices Pi.  R2   cid:2 y1   cid:2 x11  P1  R3  R2   cid:2 x21  P2   188  cid:4  Numerical Algorithms  Figure 10.3 As-rigid-as-possible  ARAP  optimization generates the deformed mesh on the right from the original mesh on the left given target positions for a few points on the head, feet, and torso.  The problem of estimating the  cid:126 yj’s and Pi’s, known as bundle adjustment, can be posed  as an optimization:  minimize cid:126 yj ,Pi cid:88 ij   cid:107 Pi cid:126 yj −  cid:126 xij cid:107 2 such that Pi is orthogonal ∀i.  2  The orthogonality constraint ensures that the camera transformations could have come from a typical lens.  Example 10.5  As-rigid-as-possible deformation . The “as-rigid-as-possible”  ARAP  modeling technique is used in computer graphics to deform two- and three-dimensional shapes in real time for modeling and animation software [116]. In the planar setting, suppose we are given a two-dimensional triangle mesh, as in Figure 10.3 a . This mesh consists of a collection of vertices V connected into triangles by edges E ⊆ V × V ; we will assume each vertex v ∈ V is associated with a position  cid:126 xv ∈ R2. Furthermore, assume the user manually moves a subset of vertices V0 ⊂ V to target positions  cid:126 yv ∈ R2 for v ∈ V0 to specify a potential deformation of the shape. The goal of ARAP is to deform the remainder V \V0 of the mesh vertices elastically, as in Figure 10.3 b , yielding a set of new positions  cid:126 yv ∈ R2 for each v ∈ V with  cid:126 yv ﬁxed by the user when v ∈ V0. The least-distorting deformation of the mesh is a rigid motion, meaning it rotates and translates but does not stretch or shear. In this case, there exists an orthogonal matrix R ∈ R2×2 so that the deformation satisﬁes  cid:126 yv −  cid:126 yw = R  cid:126 xv −  cid:126 xw  for any edge  v, w  ∈ E. But, if the user wishes to stretch or bend part of the shape, there might not exist a single R rotating the entire mesh to satisfy the position constraints in V0.  To loosen the single-rotation assumption, ARAP asks that a deformation is approxi- mately or locally rigid. Speciﬁcally, no single vertex on the mesh should experience more than a little stretch or shear, so in a neighborhood of each vertex v ∈ V there should exist an orthogonal matrix Rv satisfying  cid:126 yv −  cid:126 yw ≈ Rv  cid:126 xv −  cid:126 xw  for any  v, w  ∈ E. Once again applying least-squares, we deﬁne the as-rigid-as-possible deformation of the mesh to be  lanigirO a   demrofeD b    Constrained Optimization  cid:4  189  the one mapping  cid:126 xv  cid:55 →  cid:126 yv for all v ∈ V by solving the following optimization problem:  minimizeRv, cid:126 yv cid:88 v∈V  cid:88  v,w ∈E subject to R cid:62 v Rv = I2×2 ∀v ∈ V   cid:126 yv ﬁxed ∀v ∈ V0.   cid:107 Rv  cid:126 xv −  cid:126 xw  −   cid:126 yv −  cid:126 yw  cid:107 2  2  We will suggest one way to solve this optimization problem in Example 12.5.  10.2 THEORY OF CONSTRAINED OPTIMIZATION  In our discussion, we will assume that f , g, and h are diﬀerentiable. Some methods exist that only make weak continuity or Lipschitz assumptions, but these techniques are quite specialized and require advanced analytical consideration.  10.2.1 Optimality  Although we have not yet developed algorithms for general constrained optimization, we have made use of the theory of these problems. Speciﬁcally, recall the method of Lagrange multipliers, introduced in Theorem 1.1. In this technique, critical points of f   cid:126 x  subject to g  cid:126 x  =  cid:126 0 are given by critical points of the unconstrained Lagrange multiplier function  Λ  cid:126 x,  cid:126 λ  ≡ f   cid:126 x  −  cid:126 λ ·  cid:126 g  cid:126 x   with respect to both  cid:126 λ and  cid:126 x simultaneously. This theorem allowed us to provide variational interpretations of eigenvalue problems; more generally, it gives an alternative criterion for  cid:126 x to be a critical point of an equality-constrained optimization.  As we saw in Chapter 8, even ﬁnding a feasible  cid:126 x satisfying the constraint g  cid:126 x  =  cid:126 0 can be a considerable challenge even before attempting to minimize f   cid:126 x . We can separate these issues by making a few deﬁnitions:  Deﬁnition 10.1  Feasible point and feasible set . A feasible point of a constrained opti- mization problem is any point  cid:126 x satisfying g  cid:126 x  =  cid:126 0 and h  cid:126 x  ≥  cid:126 0. The feasible set is the set of all points  cid:126 x satisfying these constraints.  Deﬁnition 10.2  Critical point of constrained optimization . A critical point of a con- strained optimization satisﬁes the constraints and is also a local maximum, minimum, or saddle point of f within the feasible set.  10.2.2 KKT Conditions  Constrained optimizations are diﬃcult because they simultaneously solve root-ﬁnding prob- lems  the g  cid:126 x  =  cid:126 0 constraint , satisﬁability problems  the h  cid:126 x  ≥  cid:126 0 constraint , and min- imization  on the function f  . As stated in Theorem 1.1, Lagrange multipliers allow us to turn equality-constrained minimization problems into root-ﬁnding problems on Λ. To push our diﬀerential techniques to complete generality, we must ﬁnd a way to add inequality constraints h  cid:126 x  ≥  cid:126 0 to the Lagrange multiplier system. each inequality constraint hi  cid:126 x∗  ≥ 0, we have two options:  Suppose we have found a local minimum subject to the constraints, denoted  cid:126 x∗. For   190  cid:4  Numerical Algorithms  Figure 10.4 Active and inactive constraints h  cid:126 x  ≥ 0 for minimizing a function whose level sets are shown in black; the region h  cid:126 x  ≥ 0 is shown in gray. When the h  cid:126 x  ≥ 0 constraint is active, the optimal point  cid:126 x∗ is on the border of the feasible domain and would move if the constraint were removed. When the constraint is inactive,  cid:126 x∗ is in the interior of the feasible set, so the constraint h  cid:126 x  ≥ 0 has no eﬀect on the position of the  cid:126 x∗ locally.    hi  cid:126 x∗  = 0: Such a constraint is active, likely indicating that if the constraint were  removed  cid:126 x∗ would no longer be optimal.    hi  cid:126 x∗  > 0: Such a constraint is inactive, meaning in a neighborhood of  cid:126 x∗ if we had  removed this constraint we still would have reached the same minimum.  These two cases are illustrated in Figure 10.4. While this classiﬁcation will prove valuable, we do not know a priori which constraints will be active or inactive at  cid:126 x∗ until we solve the optimization problem and ﬁnd  cid:126 x∗.  If all of our constraints were active, then we could change the constraint h  cid:126 x  ≥  cid:126 0 to an equality constraint h  cid:126 x  =  cid:126 0 without aﬀecting the outcome of the optimization. Then, ap- plying the equality-constrained Lagrange multiplier conditions, we could ﬁnd critical points of the following Lagrange multiplier expression:  Λ  cid:126 x,  cid:126 λ,  cid:126 µ  ≡ f   cid:126 x  −  cid:126 λ · g  cid:126 x  −  cid:126 µ · h x .  In reality, we no longer can say that  cid:126 x∗ is a critical point of Λ, because inactive inequality constraints would remove terms above. Ignoring this  important!  issue for the time being, we could proceed blindly and ask for critical points of this new Λ with respect to  cid:126 x, which satisfy the following:   cid:126 0 = ∇f   cid:126 x  − cid:88 i  λi∇gi  cid:126 x  − cid:88 j  µj∇hj  cid:126 x .  Here, we have separated out the individual components of g and h and treated them as scalar functions to avoid complex notation.  h  cid:2 x  > 0   cid:2 x∗  h  cid:2 x  > 0   cid:2 x∗  h  cid:2 x  = 0  h  cid:2 x  = 0  tniartsnocevitcA  h  cid:2 x∗  = 0  tniartsnocevitcanI  h  cid:2 x∗  > 0   Constrained Optimization  cid:4  191  A clever trick can extend this  currently incorrect  optimality condition to include in- equality constraints. If we deﬁne µj ≡ 0 whenever hj is inactive, then the irrelevant terms are removed from the optimality conditions. In other words, we can add a constraint on the Lagrange multiplier above:  µjhj  cid:126 x  = 0.  With this constraint in place, we know that at least one of µj and hj  cid:126 x  must be zero; when the constraint hj  cid:126 x  ≥ 0 is inactive, then µj must equal zero to compensate. Our ﬁrst-order optimality condition still holds at critical points of the inequality-constrained problem—after adding this extra constraint.  So far, our construction has not distinguished between the constraint hj  cid:126 x  ≥ 0 and the constraint hj  cid:126 x  ≤ 0. If the constraint is inactive, it could have been dropped without aﬀecting the outcome of the optimization locally, so we consider the case when the constraint is active. Intuitively,∗ in this case we expect there to be a way to decrease f by violating the constraint. Locally, the direction in which f decreases is −∇f   cid:126 x∗  and the direction in which hj decreases is −∇hj  cid:126 x∗ . Thus, starting at  cid:126 x∗ we can decrease f even more by violating the constraint hj  cid:126 x  ≥ 0 when ∇f   cid:126 x∗  · ∇hj  cid:126 x∗  > 0. Products of gradients of f and hj are diﬃcult to manipulate. At  cid:126 x∗, however, our ﬁrst- order optimality condition tells us:  ∇f   cid:126 x∗  = cid:88 i  λ∗i ∇gi  cid:126 x∗  +  cid:88 j active  µ∗j∇hj  cid:126 x∗ .  The inactive µj values are zero and can be removed. We removed the g  cid:126 x  = 0 constraints by adding inequality constraints g  cid:126 x  ≥  cid:126 0 and g  cid:126 x  ≤  cid:126 0 to h; this is a mathematical convenience rather than a numerically wise maneuver.  Taking dot products with ∇hk for any ﬁxed k shows:   cid:88 j active  µ∗j∇hj  cid:126 x∗  · ∇hk  cid:126 x∗  = ∇f   cid:126 x∗  · ∇hk  cid:126 x∗  ≥ 0.  Vectorizing this expression shows Dh  cid:126 x∗ Dh  cid:126 x∗  cid:62  cid:126 µ∗ ≥  cid:126 0. Since Dh  cid:126 x∗ Dh x∗  cid:62  is positive semideﬁnite, this implies  cid:126 µ∗ ≥  cid:126 0. Thus, the ∇f   cid:126 x∗  · ∇hj  cid:126 x∗  ≥ 0 observation is equivalent to the much easier condition µj ≥ 0. These observations can be combined and formalized to prove a ﬁrst-order optimality condition for inequality-constrained minimization problems: Theorem 10.1  Karush-Kuhn-Tucker  KKT  conditions . The vector  cid:126 x∗ ∈ Rn is a critical point for minimizing f subject to g  cid:126 x  =  cid:126 0 and h  cid:126 x  ≥  cid:126 0 when there exists  cid:126 λ ∈ Rm and  cid:126 µ ∈ Rp such that:    cid:126 0 = ∇f   cid:126 x∗  − cid:80 i λi∇gi  cid:126 x∗  − cid:80 j µj∇hj  cid:126 x∗   “stationarity”    g  cid:126 x∗  =  cid:126 0 and h  cid:126 x∗  ≥  cid:126 0  “primal feasibility”    µjhj  cid:126 x∗  = 0 for all j  “complementary slackness”    µj ≥ 0 for all j  “dual feasibility”   When h is removed, this theorem reduces to the Lagrange multiplier criterion.  ∗You should not consider this discussion a formal proof, since we do not consider many boundary cases.   192  cid:4  Numerical Algorithms  Example 10.6  KKT conditions . Suppose we wish to solve the following optimization  proposed by R. Israel, UBC Math 340, Fall 2006 :  In this case we will have no λ’s and three µ’s. We take f  x, y  = −xy, h1 x, y  ≡ 2−x−y2, h2 x, y  = x, and h3 x, y  = y. The KKT conditions are:  maximize xy subject to x + y2 ≤ 2 x, y ≥ 0.  Stationarity: 0 = −y + µ1 − µ2  0 = −x + 2µ1y − µ3  Primal feasibility: x + y2 ≤ 2  Complementary slackness: µ1 2 − x − y2  = 0  x, y ≥ 0  µ2x = 0 µ3y = 0  Dual feasibility: µ1, µ2, µ3 ≥ 0  Example 10.7  Linear programming . Consider the optimization:  minimize cid:126 x  cid:126 b ·  cid:126 x subject to A cid:126 x ≥  cid:126 c.  Example 10.2 can be written this way. The KKT conditions for this problem are:  Stationarity: A cid:62  cid:126 µ =  cid:126 b Primal feasibility: A cid:126 x ≥  cid:126 c  Dual feasibility:  cid:126 µ ≥  cid:126 0  Complementary slackness: µi  cid:126 ai ·  cid:126 x − ci  = 0 ∀i, where  cid:126 a cid:62 i  is row i of A  As with Lagrange multipliers, we cannot assume that any  cid:126 x∗ satisfying the KKT condi- tions automatically minimizes f subject to the constraints, even locally. One way to check for local optimality is to examine the Hessian of f restricted to the subspace of Rn in which  cid:126 x can move without violating the constraints. If this “reduced” Hessian is positive deﬁnite, then the optimization has reached a local minimum.  10.3 OPTIMIZATION ALGORITHMS  A careful consideration of algorithms for constrained optimization is out of the scope of our discussion. Thankfully, many stable implementations of these techniques exist, and much can be accomplished as a “client” of this software rather than rewriting it from scratch. Even so, it is useful to sketch common approaches to gain some intuition for how these libraries work.   Constrained Optimization  cid:4  193  10.3.1 Sequential Quadratic Programming  SQP   Similar to BFGS and other methods we considered in Chapter 9, one typical strategy for constrained optimization is to approximate f , g, and h with simpler functions, solve the approximate optimization, adjust the approximation based on the latest function evaluation, and repeat.  Suppose we have a guess  cid:126 xk of the solution to the constrained optimization problem. We could apply a second-order Taylor expansion to f and ﬁrst-order approximation to g and h to deﬁne a next iterate as the following:   cid:126 xk+1 ≡  cid:126 xk+ arg min  cid:126 d  2   cid:126 d cid:62 Hf   cid:126 xk   cid:126 d + ∇f   cid:126 xk  ·  cid:126 d + f   cid:126 xk  cid:21   cid:20  1 subject to gi  cid:126 xk  + ∇gi  cid:126 xk  ·  cid:126 d = 0 hi  cid:126 xk  + ∇hi  cid:126 xk  ·  cid:126 d ≥ 0.  The optimization to ﬁnd  cid:126 d has a quadratic objective with linear constraints, which can be solved using one of many specialized algorithms; it is known as a quadratic program. This Taylor approximation, however, only works in a neighborhood of the optimal point. When a good initial guess  cid:126 x0 is unavailable, these strategies may fail. 10.3.1.1 Equality Constraints When the only constraints are equalities and h is removed, the quadratic program for  cid:126 d has Lagrange multiplier optimality conditions derived as follows:  Λ  cid:126 d,  cid:126 λ  ≡   cid:126 d cid:62 Hf   cid:126 xk  cid:126 d + ∇f   cid:126 xk  ·  cid:126 d + f   cid:126 xk  +  cid:126 λ cid:62  g  cid:126 xk  + Dg  cid:126 xk   cid:126 d   1 2  =⇒  cid:126 0 = ∇  cid:126 dΛ = Hf   cid:126 xk   cid:126 d + ∇f   cid:126 xk  + [Dg  cid:126 xk ] cid:62  cid:126 λ.  Combining this expression with the linearized equality constraint yields a symmetric linear system for  cid:126 d and  cid:126 λ:   cid:18  Hf   cid:126 xk   Dg  cid:126 xk   [Dg  cid:126 xk ] cid:62   0   cid:19  cid:32   cid:126 d   cid:126 λ  cid:33  = cid:18  −∇f   cid:126 xk  −g  cid:126 xk   cid:19  .  Each iteration of sequential quadratic programming in the presence of only equality con- straints can be implemented by solving this linear system to get  cid:126 xk+1 ≡  cid:126 xk +  cid:126 d. This linear system is not positive deﬁnite, so on a large scale it can be diﬃcult to solve. Extensions op- erate like BFGS for unconstrained optimization by approximating the Hessian Hf . Stability also can be improved by limiting the distance that  cid:126 x can move during any single iteration.  10.3.1.2 Inequality Constraints  Specialized algorithms exist for solving quadratic programs rather than general nonlinear programs that can be used for steps of SQP. One notable strategy is to keep an “active set” of constraints that are active at the minimum with respect to  cid:126 d. The equality-constrained methods above can be applied by ignoring inactive constraints. Iterations of active-set opti- mization update the active set by adding violated constraints and removing those inequality constraints hj for which ∇f · ∇hj ≤ 0 as in §10.2.2.   194  cid:4  Numerical Algorithms  Figure 10.5 Convex and nonconvex shapes on the plane. 10.3.2 Barrier Methods  Another option for constrained minimization is to change the constraints to energy terms. For example, in the equality constrained case we could minimize an “augmented” objective as follows:  fρ  cid:126 x  = f   cid:126 x  + ρ cid:107 g  cid:126 x  cid:107 2 2.  Taking ρ → ∞ will force  cid:107 g  cid:126 x  cid:107 2 to be as small as possible, eventually reaching g  cid:126 x  ≈  cid:126 0. Barrier methods for constrained optimization apply iterative unconstrained optimization to fρ and check how well the constraints are satisﬁed; if they are not within a given tolerance, ρ is increased and the optimization continues using the previous iterate as a starting point. Barrier methods are simple to implement and use, but they can exhibit some pernicious failure modes. In particular, as ρ increases, the inﬂuence of f on the objective function diminishes and the Hessian of fρ becomes more and more poorly conditioned.  Barrier methods be constructed for inequality constraints as well as equality constraints. In this case, we must ensure that hi  cid:126 x  ≥ 0 for all i. Typical choices of barrier functions for inequality constraints include 1 hi  cid:126 x   the “inverse barrier”  and − log hi  cid:126 x   the “logarithmic barrier” . 10.4 CONVEX PROGRAMMING  The methods we have described for constrained optimization come with few guarantees on the quality of the output. Certainly they are unable to obtain global minima without a good initial guess  cid:126 x0, and in some cases, e.g., when Hessians near  cid:126 x∗ are not positive deﬁnite, they may not converge at all.  There is a notable exception to this rule, which appears in many well-known optimization problems: convex programming. The idea here is that when f is a convex function and the feasible set itself is convex, then the optimization problem possesses a unique minimum. We considered convex functions in Deﬁnition 9.4 and now expand the class of convex problems to those containing convex constraint sets: Deﬁnition 10.3  Convex set . A set S ⊆ Rn is convex if for any  cid:126 x,  cid:126 y ∈ S, the point t cid:126 x +  1 − t  cid:126 y is also in S for any t ∈ [0, 1]. Intuitively, a set is convex if its boundary does not bend inward, as shown in Figure 10.5.   cid:2 y   cid:2 y  t cid:2 x +  1 − t  cid:2 y  t cid:2 x +  1 − t  cid:2 y   cid:2 x   cid:2 x  xevnoC  xevnocnoN   Constrained Optimization  cid:4  195  Example 10.8  Circles . The disc { cid:126 x ∈ Rn :  cid:107  cid:126 x cid:107 2 ≤ 1} is convex, while the unit circle { cid:126 x ∈ Rn :  cid:107  cid:126 x cid:107 2 = 1} is not. A nearly identical proof to that of Proposition 9.1 shows:  A convex function cannot have suboptimal local minima even  when it is restricted to a convex domain.  If a convex objective function has two local minima, then the line of points between those minima must yield objective values less than or equal to those on the endpoints; by Deﬁni- tion 10.3 this entire line is feasible, completing the proof.  Strong convergence guarantees are available for convex optimization methods that guar- antee ﬁnding a global minimum so long as f is convex and the constraints on g and h make a convex feasible set. A valuable exercise for any optimization problem is to check if it is convex, since this property can increase conﬁdence in the output quality and the chances of success by a large factor.  A new ﬁeld called disciplined convex programming attempts to chain together rules about convexity to generate convex optimization problems. The end user is allowed to combine convex energy terms and constraints so long as they do not violate the convexity of the ﬁnal problem; the resulting objective and constraints are then provided automatically to an appropriate solver. Useful statements about convexity that can be used to construct convex programs from smaller convex building blocks include the following:    The intersection of convex sets is convex; thus, enforcing more than one convex con-  straint is allowable.    The sum of convex functions is convex.   If f and g are convex, so is h  cid:126 x  ≡ max{f   cid:126 x , g  cid:126 x }.   If f is a convex function, the set { cid:126 x : f   cid:126 x  ≤ c} is convex for ﬁxed c ∈ R.  Tools such as the CVX library help separate implementation of convex programs from the mechanics of minimization algorithms [51, 52].  Example 10.9  Convex programming .    The nonnegative least-squares problem in Example 10.3 is convex because  cid:107 A cid:126 x− cid:126 b cid:107 2  is a convex function of  cid:126 x and the set { cid:126 x ∈ Rn :  cid:126 x ≥  cid:126 0} is convex.    Linear programs, introduced in Example 10.7, are convex because they have linear  objectives and linear constraints.    We can include  cid:107  cid:126 x cid:107 1 in a convex optimization objective, if  cid:126 x is an optimization variable. To do so, introduce a variable  cid:126 y and add constraints yi ≥ xi and yi ≥ −xi for since we have constrained yi ≥ xi and might as well minimize the elements of  cid:126 y. “Disciplined” convex libraries do such operations behind the scenes without exposing substitutions and helper variables to the end user.  each i. Then,  cid:107  cid:126 x cid:107 1 can be written as cid:80 i yi. At the minimum, we must have yi = xi  Convex programming has much in common with areas of computer science theory involving reductions of algorithmic problems to one another. Rather than verifying NP- completeness, however, in this context we wish to use a generic solver to optimize a given objective, just like we reduced assorted problems to a linear solve in Chapter 4. There is a   196  cid:4  Numerical Algorithms  Figure 10.6 On the  x, y  plane, the optimization minimizing  cid:107  x, y  cid:107 p subject to ax + by = c has considerably diﬀerent output depending on whether we choose  a  p = 2 or  b  p = 1. Level sets { x, y  :  cid:107  x, y  cid:107 p = c} are shown in gray. formidable pantheon of industrial-scale convex programming tools that can handle diﬀerent classes of problems with varying eﬃciency and generality; below, we discuss some common classes. See [15, 84] for larger discussions of related topics.  10.4.1 Linear Programming  A well-studied example of convex optimization is linear programing, introduced in Exam- ple 10.7. Exercise 10.4 will walk through the derivation of some properties making linear programs attractive both theoretically and from an algorithmic design standpoint.  The famous simplex algorithm, which can be considered an active set method as in §10.3.1.2, updates the estimate of  cid:126 x∗ using a linear solve, and checks if the active set must be updated. No Taylor approximations are needed because the objective and constraints are linear. Interior point linear programming algorithms such as the barrier method in §10.3.2 also are successful for these problems. Linear programs can be solved on a huge scale—up to millions or billions of variables!—and often appear in problems like scheduling or pricing. One popular application of linear programming inspired by Example 10.9 provides an alternative to using pseudoinverse for underdetermined linear systems  §7.2.1 . When a matrix A is underdetermined, there are many vectors  cid:126 x that satisfy A cid:126 x =  cid:126 b for a given vector  cid:126 b. In this case, the pseudoinverse A+ applied to  cid:126 b solves the following problem:  Using linear programs, we can solve a slightly diﬀerent system:  cid:107  cid:126 x cid:107 1  subject to A cid:126 x =  cid:126 b.  subject to A cid:126 x =  cid:126 b.   cid:107  cid:126 x cid:107 2  Pseudoinverse cid:26  minimize cid:126 x L1 minimization cid:26  minimize cid:126 x  All we have done here is replace the norm  cid:107  ·  cid:107 2 with a diﬀerent norm  cid:107  ·  cid:107 1. Why does this one-character change make a signiﬁcant diﬀerence in the output  cid:126 x? Con- sider the two-dimensional instance of this problem shown in Figure 10.6, which minimizes  y  ax+by=c   x∗, y∗   y  ax+by=c   x∗, y∗   x  x   a  p  2=   b   p = 1   Constrained Optimization  cid:4  197   cid:107  x, y  cid:107 p for p = 2  pseudoinverse  and p = 1  linear program . In the p = 2 case  a , we are minimizing x2 + y2, which has circular level sets; the optimal  x∗, y∗  subject to the constraints is in the interior of the ﬁrst quadrant. In the p = 1 case  b , we are minimizing x + y, which has diamond-shaped level sets; this makes x∗ = 0 since the outer points of the diamond align with the x and y axes, a more sparse solution. More generally, the use of the norm  cid:107  cid:126 x cid:107 2 indicates that no single element xi of  cid:126 x should have a large value; this regularization tends to favor vectors  cid:126 x with lots of small nonzero values. On the other hand,  cid:107  cid:126 x cid:107 1 does not care if a single element of  cid:126 x has a large value so long as the sum of all the elements’ absolute values is small. As we have illustrated in the two-dimensional case, this type of regularization can produce sparse vectors  cid:126 x, with elements that are exactly zero.  This type of regularization using  cid:107 · cid:107 1 is fundamental in the ﬁeld of compressed sensing, which solves underdetermined signal processing problems with the additional assumption that the output should be sparse. This assumption makes sense in many contexts where sparse solutions of A cid:126 x =  cid:126 b imply that many columns of A are irrelevant [37].  A minor extension of linear programming is to keep using linear inequality constraints but introduce convex quadratic terms to the objective, changing the optimization in Exam- ple 10.7 to:  minimize cid:126 x  cid:126 b ·  cid:126 x +  cid:126 x cid:62 M cid:126 x subject to A cid:126 x ≥  cid:126 c.   cid:126 x  cid:107 A cid:126 x −  cid:126 b cid:107 2 min  2 + α cid:107  cid:126 x cid:107 1.  Here, M is an n × n positive semideﬁnite matrix. With this machinery, we can provide an alternative to Tikhonov regularization from §4.1.3:  This “lasso” regularizer also promotes sparsity in  cid:126 x while solving A cid:126 x ≈  cid:126 b, but it does not enforce A cid:126 x =  cid:126 b exactly. It is useful when A or  cid:126 b is noisy and we prefer sparsity of  cid:126 x over solving the system exactly [119].  10.4.2 Second-Order Cone Programming  A second-order cone program  SOCP  is a convex optimization problem taking the following form [15]:  minimize cid:126 x  cid:126 b ·  cid:126 x subject to  cid:107 Ai cid:126 x −  cid:126 bi cid:107 2 ≤ di +  cid:126 ci ·  cid:126 x for all i = 1, . . . , k.  Here, we use matrices A1, . . . , Ak, vectors  cid:126 b1, . . . , cid:126 bk, vectors  cid:126 c1, . . . ,  cid:126 ck, and scalars d1, . . . , dk to specify the k constraints. These “cone constraints” will allow us to pose a broader set of convex optimization problems.  One non-obvious application of second-order cone programming explained in [83] appears when we wish to solve the least-squares problem A cid:126 x ≈  cid:126 b, but we do not know the elements of A exactly. For instance, A might have been constructed from data we have measured experimentally  see §4.1.2 for an example in least-squares regression .  to be the i-th row of A. Then, the least-squares problem A cid:126 x ≈  cid:126 b can be understood as minimizing cid:80 i  cid:126 ai ·  cid:126 x− bi 2 over  cid:126 x. If we do not know A exactly, however, we might allow each  cid:126 ai to vary somewhat before solving least-squares. In particular, maybe we  Take  cid:126 a cid:62 i   198  cid:4  Numerical Algorithms  think that  cid:126 ai is an approximation of some unknown  cid:126 a0 ﬁxed ε > 0.  i −  cid:126 ai cid:107 2 ≤ ε for some To make least-squares robust to this model of error, we can choose  cid:126 x to thwart an ad- i . Formally, we solve the following “minimax” problem:  versary picking the worst possible  cid:126 a0  i satisfying  cid:107  cid:126 a0  minimize cid:126 x cid:20  max{ cid:126 a0  subject to  cid:107  cid:126 a0  i }  cid:80 i  cid:126 a0  i ·  cid:126 x − bi 2  i −  cid:126 ai cid:107 2 ≤ ε for all i  cid:21  .  That is, we want to choose  cid:126 x so that the least-squares energy with the worst possible un- knowns  cid:126 a0 i − cid:126 ai cid:107 2 ≤ ε still is small. It is far from evident that this complicated optimization problem is solvable using SOCP machinery, but after some simpliﬁcation we will manage to write it in the standard SOCP form above.  i satisfying  cid:107  cid:126 a0  If we deﬁne δ cid:126 ai ≡  cid:126 ai −  cid:126 a0  i , then our optimization becomes:  minimize cid:126 x cid:20  max{δ cid:126 ai}  cid:80 i  cid:126 ai ·  cid:126 x + δ cid:126 ai ·  cid:126 x − bi 2  subject to  cid:107 δ cid:126 ai cid:107 2 ≤ ε for all i   cid:21  .  When maximizing over δ cid:126 ai, each term of the sum over i is independent. Hence, we can solve the inner maximization for one δ cid:126 ai at a time. Peculiarly, if we maximize an absolute value rather than a sum  usually we go in the other direction! , we can ﬁnd a closed-form solution to the optimization for δ cid:126 ai for a single ﬁxed i:  max   cid:107 δ cid:126 ai cid:107 2≤ε cid:126 ai ·  cid:126 x + δ cid:126 ai ·  cid:126 x − bi = max  cid:107 δ cid:126 ai cid:107 2≤ε since x = max{x,−x}  max{ cid:126 ai ·  cid:126 x + δ cid:126 ai ·  cid:126 x − bi,− cid:126 ai ·  cid:126 x − δ cid:126 ai ·  cid:126 x + bi}  = max cid:26  max   cid:107 δ cid:126 ai cid:107 2≤ε  [ cid:126 ai ·  cid:126 x + δ cid:126 ai ·  cid:126 x − bi] ,  [− cid:126 ai ·  cid:126 x − δ cid:126 ai ·  cid:126 x + bi] cid:27   max   cid:107 δ cid:126 ai cid:107 2≤ε  after changing the order of the maxima  = max{ cid:126 ai ·  cid:126 x + ε cid:107  cid:126 x cid:107 2 − bi,− cid:126 ai ·  cid:126 x + ε cid:107  cid:126 x cid:107 2 + bi} =  cid:126 ai ·  cid:126 x − bi + ε cid:107  cid:126 x cid:107 2.  After this simpliﬁcation, our optimization for  cid:126 x becomes:  minimize cid:126 x cid:88 i    cid:126 ai ·  cid:126 x − bi + ε cid:107  cid:126 x cid:107 2 2.  This minimization can be written as a second-order cone problem:  minimizes, cid:126 t, cid:126 x  s  subject to  cid:107  cid:126 t cid:107 2 ≤ s    cid:126 ai ·  cid:126 x − bi  + ε cid:107  cid:126 x cid:107 2 ≤ ti ∀i −  cid:126 ai ·  cid:126 x − bi  + ε cid:107  cid:126 x cid:107 2 ≤ ti ∀i.  In this optimization, we have introduced two extra variables s and  cid:126 t. Since we wish to minimize s with the constraint  cid:107  cid:126 t cid:107 2 ≤ s, we are eﬀectively minimizing the norm of  cid:126 t. The last two constraints ensure that each element of  cid:126 t satisﬁes ti =  cid:126 ai ·  cid:126 x − bi + ε cid:107  cid:126 x cid:107 2. This type of regularization provides yet another variant of least-squares. In this case, rather than being robust to near-singularity of A, we have incorporated an error model directly into our formulation allowing for mistakes in measuring rows of A. The parameter ε controls sensitivity to the elements of A in a similar fashion to the weight α of Tikhonov or L1 regularization.   Constrained Optimization  cid:4  199  Figure 10.7 Examples of graphs laid out via semideﬁnite embedding. 10.4.3 Semidefinite Programming Suppose A and B are n × n positive semideﬁnite matrices; we will notate this as A, B  cid:23  0. Take t ∈ [0, 1]. Then, for any  cid:126 x ∈ Rn we have:   cid:126 x cid:62  tA +  1 − t B  cid:126 x = t cid:126 x cid:62 A cid:126 x +  1 − t  cid:126 x cid:62 B cid:126 x ≥ 0,  where the inequality holds by semideﬁniteness of A and B. This proof veriﬁes a surprisingly useful fact:  The set of positive semideﬁnite matrices is convex.  Hence, if we are solving optimization problems for a matrix A, we safely can add constraints A  cid:23  0 without aﬀecting convexity. Algorithms for semideﬁnite programming optimize convex objectives with the ability to add constraints that matrix-valued variables must be positive  or negative  semideﬁnite. More generally, semideﬁnite programming machinery can include linear matrix inequality  LMI  constraints of the form:  x1A1 + x2A2 + ··· + xkAk  cid:23  0,  where  cid:126 x ∈ Rk is an optimization variable and the matrices Ai are ﬁxed. As an example of semideﬁnite programming, we will sketch a technique known as semideﬁnite embedding from graph layout and manifold learning [130]. Suppose we are given a graph  V, E  consisting of a set of vertices V = {v1, . . . , vk} and a set of edges E ⊆ V × V. For some ﬁxed n, the semideﬁnite embedding method computes positions  cid:126 x1, . . . ,  cid:126 xk ∈ Rn for the vertices, so that vertices connected by edges are nearby in the embedding with respect to Euclidean distance  cid:107  ·  cid:107 2; some examples are shown in Figure 10.7. If we already have computed  cid:126 x1, . . . ,  cid:126 xk, we can construct a Gram matrix G ∈ Rk×k satisfying Gij =  cid:126 xi ·  cid:126 xj. G is a matrix of inner products and hence is symmetric and positive semideﬁnite. We can measure the squared distance from  cid:126 xi to  cid:126 xj using G:   cid:107  cid:126 xi −  cid:126 xj cid:107 2  2 =   cid:126 xi −  cid:126 xj  ·   cid:126 xi −  cid:126 xj  =  cid:107  cid:126 xi cid:107 2 = Gii − 2Gij + Gjj.  Similarly, suppose we wish the center of mass 1 of the graph does not have a signiﬁcant eﬀect on its layout. We alternatively can write  2 = 0 and can express this condition in terms of G:   cid:107  cid:80 i  cid:126 xi cid:107 2  2  2 − 2 cid:126 xi ·  cid:126 xj +  cid:107  cid:126 xj cid:107 2 k cid:80 i  cid:126 xi to be  cid:126 0, since shifting the embedding  cid:126 xi cid:33  = cid:88 ij   cid:126 xi ·  cid:126 xj = cid:88 ij  Gij.  0 = cid:13  cid:13  cid:13  cid:13  cid:13  cid:88 i   cid:126 xi cid:13  cid:13  cid:13  cid:13  cid:13   2  2  = cid:32  cid:88 i   cid:126 xi cid:33  · cid:32  cid:88 i   200  cid:4  Numerical Algorithms  Finally, we might wish that our embedding of the graph is relatively compact or small. One  way to do this would be to minimize cid:80 i  cid:107  cid:126 xi cid:107 2  2 = cid:80 i Gii = Tr G .  The semideﬁnite embedding technique turns these observations on their head, optimizing for the Gram matrix G directly rather than the positions  cid:126 xi of the vertices. Making use of the observations above, semideﬁnite embedding solves the following optimization problem:  subject to G = G cid:62   minimizeG∈Rk×k Tr G  G  cid:23  0 Gii − 2Gij + Gjj = 1 ∀ vi, vj  ∈ E   cid:80 ij Gij = 0.  This optimization for G is motivated as follows:    The objective asks that the embedding of the graph is compact by minimizing the  sum of squared norms cid:80 i  cid:107  cid:126 xi cid:107 2  2.  deﬁnite.    The ﬁrst two constraints require that the Gram matrix is symmetric and positive    The third constraint requires that the embeddings of any two adjacent vertices in the  graph have distance one.    The ﬁnal constraint centers the full embedding about the origin.  We can use semideﬁnite programming to solve this optimization problem for G. Then, since G is symmetric and positive semideﬁnite, we can use the Cholesky factorization  §4.2.1  or the eigenvector decomposition  §6.2  of G to write G = X cid:62 X for some matrix X ∈ Rk×k. Based on the discussion above, the columns of X are an embedding of the vertices of the graph into Rk where all the edges in the graph have length one, the center of mass is the origin, and the total square norm of the positions is minimized.  We set out to embed the graph into Rn rather than Rk, and generally n ≤ k. To compute a lower-dimensional embedding that approximately satisﬁes the constraints, we can decompose G = X cid:62 X using its eigenvectors; then, we remove k − n eigenvectors with eigenvalues closest to zero. This operation is exactly the low-rank approximation of G via SVD given in §7.2.2. This ﬁnal step provides an embedding of the graph into Rn. A legitimate question about the semideﬁnite embedding is how the optimization for G interacts with the low-rank eigenvector approximation applied in post-processing. In many well-known cases, the solution of semideﬁnite optimizations like the one above yield low- rank or nearly low-rank matrices whose lower-dimensional approximations are close to the original; a formalized version of this observation justiﬁes the approximation. We already explored such a justiﬁcation in Exercise 7.7, since the nuclear norm of a symmetric positive semideﬁnite matrix is its trace.  10.4.4 Integer Programs and Relaxations  Our ﬁnal application of convex optimization is—surprisingly—to a class of highly non- convex problems: Ones with integer variables. In particular, an integer program is an opti- mization in which one or more variables is constrained to be an integer rather than a real number. Within this class, two well-known subproblems are mixed-integer programming, in which some variables are continuous while others are integers, and zero-one programming, where the variables take Boolean values in {0, 1}.   Constrained Optimization  cid:4  201  Example 10.10  3-SAT . We can deﬁne the following operations from Boolean algebra for binary variables U, V ∈ {0, 1}:  U V ¬U  “not U ”  ¬V  “not V ”  U ∧ V  “U and V ”  U ∨ V  “U or V ”  0 0 1 1  1 1 0 0  0 1 0 1  0 1 1 1  0 0 0 1  1 0 1 0  We can convert Boolean satisﬁability problems into integer programs using a few steps. For example, we can express the “not” operation algebraically using ¬U = 1−U. Similarly, suppose we wish to ﬁnd U, V satisfying  U ∨ ¬V   ∧  ¬U ∨ V  . Then, U and V as integers satisfy the following constraints:  U +  1 − V   ≥ 1  1 − U   + V ≥ 1 U, V ∈ Z 0 ≤ U, V ≤ 1   U ∨ ¬V    ¬U ∨ V    integer constraint   Boolean variables   As demonstrated in Example 10.10, integer programs encode a wide class of discrete problems, including many that are known to be NP-hard. For this reason, we cannot expect to solve them exactly with convex optimization; doing so would settle a long-standing question of theoretical computer science by showing “P = N P.” We can, however, use convex optimization to ﬁnd approximate solutions to integer programs.  If we write a discrete problem like Example 10.10 as an optimization, we can relax the constraint keeping variables in Z and allow them to be in R instead. Such a relaxation can yield invalid solutions, e.g., Boolean variables that take on values like 0.75. So, after solving the relaxed problem, one of many strategies can be used to generate an integer approxi- mation of the solution. For example, non-integral variables can be rounded to the closest integer, at the risk of generating outputs that are suboptimal or violate the constraints. Al- ternatively, a slower but potentially more eﬀective method iteratively rounds one variable at a time, adds a constraint ﬁxing the value of that variable, and re-optimizes the objective subject to the new constraint.  Many diﬃcult discrete problems can be reduced to integer programs, from satisﬁability problems like the one in Example 10.10 to the traveling salesman problem. These reductions should indicate that the design of eﬀective integer programming algorithms is challenging even in the approximate case. State-of-the-art convex relaxation methods for integer pro- gramming, however, are fairly eﬀective for a large class of problems, providing a remarkably general piece of machinery for approximating solutions to problems for which it may be diﬃcult or impossible to design a discrete algorithm. Many open research problems involve designing eﬀective integer programming methods and understanding potential relaxations; this work provides a valuable and attractive link between continuous and discrete mathe- matics.  10.5 EXERCISES 10.1 Prove the following statement from §10.4: If f is a convex function, the set { cid:126 x : f   cid:126 x  ≤  c} is convex.   202  cid:4  Numerical Algorithms  10.2 The standard deviation of k values x1, . . . , xk is  σ x1, . . . , xk  ≡ cid:118  cid:117  cid:117  cid:116  1  k  k cid:88 i=1   xi − µ 2,  10.3 Some properties of second-order cone programming:  where µ ≡ 1  k cid:80 i xi. Show that σ is a convex function of x1, . . . , xk.  a  Show that the Lorentz cone { cid:126 x ∈ Rn, c ∈ R :  cid:107  cid:126 x cid:107 2 ≤ c} is convex.  b  Use this fact to show that the second-order cone program in §10.4.2 is convex.  c  Show that second-order cone programming can be used to solve linear programs.  10.4 In this problem we will study linear programming in more detail.   a  A linear program in “standard form” is given by:  Here, the optimization is over  cid:126 x ∈ Rn; the remaining variables are constants A ∈ Rm×n,  cid:126 b ∈ Rm, and  cid:126 c ∈ Rn. Find the KKT conditions of this system.   b  Suppose we add a constraint of the form  cid:126 v cid:62  cid:126 x ≤ d for some ﬁxed  cid:126 v ∈ Rn and d ∈ R. Explain how such a constraint can be added while keeping a linear program in standard form.   c  The “dual” of this linear program is another optimization:  minimize cid:126 x  cid:126 c cid:62  cid:126 x subject to A cid:126 x =  cid:126 b  cid:126 x ≥  cid:126 0.  maximize cid:126 y  cid:126 b cid:62  cid:126 y subject to A cid:62  cid:126 y ≤  cid:126 c.  Assuming that the primal and dual have exactly one stationary point, show that the optimal value of the primal and dual objectives coincide. Hint: Show that the KKT multipliers of one problem can be used to solve the other. Note: This property is called “strict duality.” The famous simplex algorithm for solving linear programs maintains estimates of  cid:126 x and  cid:126 y, terminating when  cid:126 c cid:62  cid:126 x∗ −  cid:126 b cid:62  cid:126 y∗ = 0.  10.5 Suppose we take a grayscale photograph of size n × m and represent it as a vector  cid:126 v ∈ Rnm of values in [0, 1]. We used the wrong lens, however, and our photo is blurry! We wish to use deconvolution machinery to undo this eﬀect.   a  Find the KKT conditions for the following optimization problem:  minimize cid:126 x∈Rnm  cid:107 A cid:126 x −  cid:126 b cid:107 2  2  subject to  0 ≤ xi ≤ 1 ∀i ∈ {1, . . . , nm}.   Constrained Optimization  cid:4  203   b  Suppose we are given a matrix G ∈ Rnm×nm taking sharp images to blurry ones. Propose an optimization in the form of  a  for recovering a sharp image from our blurry  cid:126 v.   c  We do not know the operator G, making the model in  b  diﬃcult to use. Suppose, however, that for each r ≥ 0 we can write a matrix Gr ∈ Rnm×nm approximating a blur with radius r. Using the same camera, we now take k pairs of photos   cid:126 v1,  cid:126 w1 , . . . ,   cid:126 vk,  cid:126 wk , where  cid:126 vi and  cid:126 wi are of the same scene but  cid:126 vi is blurry  taken using the same lens as our original bad photo  and  cid:126 wi is sharp. Propose a nonlinear optimization for approximating r using this data.  DH 10.6  “Fenchel duality,” adapted from [10]  Let f   cid:126 x  be a convex function on Rn that is proper. This means that f accepts vectors from Rn or whose coordinates may  individually  be ±∞ and returns a real scalar in R ∪ {∞} with at least one f   cid:126 x0  taking a non-inﬁnite value. Under these assumptions, the Fenchel dual of f at  cid:126 y ∈ Rn is deﬁned to be the function  f∗  cid:126 y  ≡ sup  cid:126 x∈Rn    cid:126 x ·  cid:126 y − f   cid:126 x  .  Fenchel duals are used to study properties of convex optimization problems in theory and practice.   a  Show that f∗ is convex.   b  Derive the Fenchel-Young inequality:   c  The indicator function of a subset A ∈ Rn is given by  f   cid:126 x  + f∗  cid:126 y  ≥  cid:126 x ·  cid:126 y. χA  cid:126 x  ≡ cid:26  0  if  cid:126 x ∈ A  ∞ otherwise.  With this deﬁnition in mind, determine the Fenchel dual of f   cid:126 x  =  cid:126 c ·  cid:126 x, where  cid:126 c ∈ Rn.   d  What is the Fenchel dual of the linear function f  x  = ax + b?  2 is self-dual, meaning f = f∗.   e  Show that f   cid:126 x  = 1  f  Suppose p, q ∈  1,∞  satisfy 1  2 cid:107  cid:126 x cid:107 2  1  pxp is f∗ y  = 1 to derive H¨older’s inequality  p + 1  q = 1. Show that the Fenchel dual of f  x  = qyq. Use this result along with previous parts of this problem  ukvk ≤ cid:32  cid:88 k  ukp cid:33 1 p cid:32  cid:88 k   cid:88 k  vkq cid:33 1 q  ,  for all  cid:126 u,  cid:126 v ∈ Rn.   204  cid:4  Numerical Algorithms  Figure 10.8 Notation for Exercise 10.7.  SC 10.7 A monomial is a function of the form f   cid:126 x  = cxa1  1 xa2  2 ··· xan  n , where each ai ∈ R and  c > 0. We deﬁne a posynomial as a sum of one or more monomials:  Geometric programs are optimization problems taking the following form:  f   cid:126 x  =  ckx  ak1 1 x  ak2 2  ··· xakn n .  K cid:88 k=1  f0  cid:126 x   minimize cid:126 x subject to fi  cid:126 x  ≤ 1∀i ∈ {1, . . . , m} gi  cid:126 x  = 1∀i ∈ {1, . . . , p},  where the functions fi are posynomials and the functions gi are monomials.   a  Suppose you are designing a slow-dissolving medicinal capsule. The capsule looks like a cylinder with hemispherical ends, illustrated in Figure 10.8. To ensure that the capsule dissolves slowly, you need to minimize its surface area. The cylindrical portion of the capsule must have volume larger than or equal to V to ensure that it can hold the proper amount of medicine. Also, because the capsule is manufactured as two halves that slide together, to ensure that the capsule will not break, the length  cid:96  of its cylindrical portion must be at least  cid:96 min. Finally, due to packaging limitations, the total length of the capsule must be no larger than C. Write the corresponding minimization problem and argue that it is a geometric program.   b  Transform the problem from Exercise 10.7a into a convex programming problem.  Hint: Consider the substitution yi = log xi.  10.8 The cardinality function  cid:107  ·  cid:107 0 computes the number of nonzero elements of  cid:126 x ∈ Rn:   a  Show that  cid:107  ·  cid:107 0 is not a norm on Rn, but that it is connected to Lp norms by  the relationship   cid:107  cid:126 x cid:107 0 =  n cid:88 i=1 cid:26  1 xi  cid:54 = 0  0  otherwise.   cid:107  cid:126 x cid:107 0 = lim p→0+  n cid:88 i=1  xip.  r   cid:2    Constrained Optimization  cid:4  205   b  Suppose we wish to solve an underdetermined system of equations A cid:126 x =  cid:126 b. One alternative to SVD-based approaches or Tikhonov regularizations is cardinality minimization:  Rewrite this optimization in the form  min cid:126 x∈Rn  subject to A cid:126 x =  cid:126 b   cid:107  cid:126 x cid:107 0  cid:107  cid:126 x cid:107 ∞ ≤ R.  min cid:126 x, cid:126 z subject to   cid:107  cid:126 z cid:107 1  cid:126 z ∈ {0, 1}n  cid:126 x,  cid:126 z ∈ C,  where C is some convex set [15].   c  Show that relaxing the constraint  cid:126 z ∈ {0, 1}n to  cid:126 z ∈ [0, 1]n lower-bounds the original problem. Propose a heuristic for the {0, 1} problem based on this relax- ation.  10.9  “Grasping force optimization;” adapted from [83]  Suppose we are writing code to control a robot hand with n ﬁngers grasping a rigid object. Each ﬁnger i is controlled by a motor that outputs nonnegative torque ti. The force  cid:126 Fi imparted by each ﬁnger onto the object can be decomposed into two orthogonal parts as  cid:126 Fi =  cid:126 Fni +  cid:126 Fsi, a normal force  cid:126 Fni and a tangential friction force  cid:126 Fsi:  Normal force:  cid:126 Fni = citi cid:126 vi =   cid:126 v cid:62 i Friction force:  cid:126 Fsi =  I3×3 −  cid:126 vi cid:126 v cid:62 i    cid:126 Fi, where  cid:107   cid:126 Fsi cid:107 2 ≤ µ cid:107 Fni cid:107 2   cid:126 Fi  cid:126 vi  Here,  cid:126 vi is a  ﬁxed  unit vector normal to the surface at the point of contact of ﬁnger i. The value ci is a constant associated with ﬁnger i. Additionally, the object experiences a gravitational force in the downward direction given by  cid:126 Fg = m cid:126 g. For the object to be grasped ﬁrmly in place, the sum of the forces exerted by all ﬁngers must be  cid:126 0. Show how to minimize the total torque outputted by the motors while ﬁrmly grasping the object using a second-order cone program.  10.10 Show that when  cid:126 ci =  cid:126 0 for all i in the second-order cone program of §10.4.2, the optimization problem can be solved as a convex quadratic program with quadratic constraints.  10.11  Suggested by Q. Huang  Suppose we know    1 1  1 1 1 x  1 x 1   cid:23  0.  SC 10.12 We can modify the gradient descent algorithm for minimizing f   cid:126 x  to account for  What can we say about x?  linear equality constraints A cid:126 x =  cid:126 b.   206  cid:4  Numerical Algorithms   a  Assuming we choose  cid:126 x0 satisfying the equality constraint, propose a modiﬁcation  to gradient descent so that each iterate  cid:126 xk satisﬁes A cid:126 xk =  cid:126 b. Hint: The gradient ∇f   cid:126 x  may point in a direction that could violate the con- straint.   b  Brieﬂy justify why the modiﬁed gradient descent algorithm should reach a local  minimum of the constrained optimization problem.   c  Suppose rather than A cid:126 x =  cid:126 b we have a nonlinear constraint g  cid:126 x  =  cid:126 0. Propose a modiﬁcation of your strategy from Exercise 10.12a maintaining this new con- straint approximately. How is the modiﬁcation aﬀected by the choice of step sizes?  10.13 Show that linear programming and second-order cone programming are special cases  of semideﬁnite programming.   C H A P T E R11  Iterative Linear Solvers  CONTENTS  11.1 Gradient Descent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11.1.1 Gradient Descent for Linear Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . 11.1.2 Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11.2 Conjugate Gradients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11.2.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11.2.2 Suboptimality of Gradient Descent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11.2.3 Generating A-Conjugate Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11.2.4 Formulating the Conjugate Gradients Algorithm . . . . . . . . . . . . . . 11.2.5 Convergence and Stopping Conditions . . . . . . . . . . . . . . . . . . . . . . . . . 11.3 Preconditioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11.3.1 CG with Preconditioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11.3.2 Common Preconditioners . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11.4 Other Iterative Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  208 208 209 211 212 214 215 217 219 219 220 221 222  I N the previous two chapters, we developed general algorithms for minimizing a function  f   cid:126 x  with or without constraints on  cid:126 x. In doing so, we relaxed our viewpoint from nu- merical linear algebra that we must ﬁnd an exact solution to a system of equations and instead designed iterative methods that successively produce better approximations of the minimizer. Even if we never ﬁnd the position  cid:126 x∗ of a local minimum exactly, such methods generate  cid:126 xk with smaller and smaller f   cid:126 xk , in many cases getting arbitrarily close to the desired optimum.  We now revisit our favorite problem from numerical linear algebra, solving A cid:126 x =  cid:126 b for  cid:126 x, but apply an iterative approach rather than seeking a solution in closed form. This adjustment reveals a new class of linear solvers that can ﬁnd reliable approximations of  cid:126 x in remarkably few iterations. To formulate these methods, we will view solving A cid:126 x =  cid:126 b not as a system of equations but rather as a minimization problem, e.g., on energies like  cid:107 A cid:126 x− cid:126 b cid:107 2 2. Why bother deriving yet another class of linear solvers? So far, most of our direct solvers require us to represent A as a full n × n matrix, and algorithms such as LU, QR, and Cholesky factorization all take around O n3  time. Two cases motivate the need for iterative methods:    When A is sparse, Gaussian elimination tends to induce ﬁll, meaning that even if A contains O n  nonzero values, intermediate steps of elimination may ﬁll in the remaining O n2  empty positions. Storing a matrix in sparse format dramatically reduces the space it takes in memory, but ﬁll during elimination can rapidly undo these savings. Contrastingly, the algorithms in this chapter require only application A to vectors  that is, computation of the product A cid:126 v for any  cid:126 v , which does not induce ﬁll and can be carried out in time proportional to the number of nonzeros.  207   208  cid:4  Numerical Algorithms    We may wish to defeat the O n3  runtime of standard matrix factorization techniques. If an iterative scheme can uncover a fairly, if not completely, accurate solution to A cid:126 x =  cid:126 b in a few steps, we may halt the method early in favor of speed over accuracy of the output.  Newton’s method and other nonlinear optimization algorithms solve a linear system in each iteration. Formulating the fastest possible solver can make a huge diﬀerence in eﬃciency when implementing these methods for large-scale problems. An inaccurate but fast linear solve may be suﬃcient, since it feeds into a larger iterative technique anyway.  Although our discussion in this chapter beneﬁts from intuition and formalism developed in previous chapters, our approach to deriving iterative linear methods owes much to the classic extended treatment in [109].  11.1 GRADIENT DESCENT We will focus our discussion on solving A cid:126 x =  cid:126 b where A has three properties:  1. A ∈ Rn×n is square. 2. A is symmetric, that is, A cid:62  = A. 3. A is positive deﬁnite, that is, for all  cid:126 x  cid:54 =  cid:126 0,  cid:126 x cid:62 A cid:126 x > 0.  Toward the end of this chapter we will relax these assumptions. Of course, we always can replace A cid:126 x =  cid:126 b—at least when A is invertible or overdetermined—with the normal equations A cid:62 A cid:126 x = A cid:62  cid:126 b to satisfy these criteria, although as discussed in §5.1, this substitution can create conditioning issues.  11.1.1 Gradient Descent for Linear Systems Under the restrictions above, solutions of A cid:126 x =  cid:126 b are minima of the function f   cid:126 x  given by the quadratic form  f   cid:126 x  ≡   cid:126 x cid:62 A cid:126 x −  cid:126 b cid:62  cid:126 x + c  1 2  ∇f   cid:126 x  = A cid:126 x −  cid:126 b,  for any c ∈ R. To see this connection, when A is symmetric, taking the derivative of f shows  and setting ∇f   cid:126 x  =  cid:126 0 yields the desired result. Solving ∇f   cid:126 x  =  cid:126 0 directly amounts to performing Gaussian elimination on A. Instead, suppose we apply gradient descent to this minimization problem. Recall the basic gradient descent algorithm:  1. Compute the search direction  cid:126 dk ≡ −∇f   cid:126 xk−1  =  cid:126 b − A cid:126 xk−1. 2. Deﬁne  cid:126 xk ≡  cid:126 xk−1 + αk  cid:126 dk, where αk is chosen such that f   cid:126 xk  < f   cid:126 xk−1 . 3. Repeat.  For a generic function f , deciding on the value of αk can be a diﬃcult one-dimensional “line search” problem, boiling down to minimizing f   cid:126 xk−1 + αk  cid:126 dk  as a function of a single   Iterative Linear Solvers  cid:4  209  Figure11.1 Gradient descent algorithm for solving A cid:126 x =  cid:126 b for symmetric and positive deﬁnite A, by iteratively decreasing the energy f   cid:126 x  = 1  2 cid:126 x cid:62 A cid:126 x −  cid:126 b cid:62  cid:126 x + c.  variable αk ≥ 0. For the quadratic form f   cid:126 x  = 1 αk optimally using a closed-form formula. To do so, deﬁne  2  cid:126 x cid:62 A cid:126 x −  cid:126 b cid:62  cid:126 x + c, however, we can choose  g α  ≡ f   cid:126 x + α  cid:126 d   =  =  =  1 2 1 2  1 2    cid:126 x + α  cid:126 d  cid:62 A  cid:126 x + α  cid:126 d  −  cid:126 b cid:62   cid:126 x + α  cid:126 d  + c by deﬁnition of f   cid:126 x cid:62 A cid:126 x + 2α cid:126 x cid:62 A  cid:126 d + α2  cid:126 d cid:62 A cid:126 d  −  cid:126 b cid:62  cid:126 x − α cid:126 b cid:62   cid:126 d + c after expanding the product α2  cid:126 d cid:62 A  cid:126 d + α  cid:126 x cid:62 A  cid:126 d −  cid:126 b cid:62   cid:126 d  + const.  =⇒  dg dα   α  = α  cid:126 d cid:62 A cid:126 d +  cid:126 d cid:62  A cid:126 x −  cid:126 b  by symmetry of A.  With this simpliﬁcation, to minimize g with respect to α, we solve dg dα = 0 to ﬁnd  For gradient descent, we chose  cid:126 dk =  cid:126 b − A cid:126 xk, so αk takes the form  α =   cid:126 d cid:62   cid:126 b − A cid:126 x   .   cid:126 d cid:62 A  cid:126 d  αk =  cid:107  cid:126 dk cid:107 2  cid:126 d cid:62 k A cid:126 dk  2  .  Since A is positive deﬁnite, αk > 0 by deﬁnition. This formula leads to the iterative gradient descent algorithm for solving A cid:126 x =  cid:126 b shown in Figure 11.1. Unlike generic line search, for this problem the choice of α in each iteration is optimal.  11.1.2 Convergence  By construction, gradient descent decreases f   cid:126 xk  in each step. Even so, we have not shown that the algorithm approaches the minimum possible f   cid:126 xk , nor we have been able to characterize how many iterations we should run to reach a reasonable level of conﬁdence that A cid:126 xk ≈  cid:126 b. One way to understand the convergence of the gradient descent algorithm for our choice of f is to examine the change in backward error from iteration to iteration; we will follow the argument in [38] and elsewhere.  function Linear-Gradient-Descent A, cid:2 b    cid:2 x ←  cid:2 0 for k ← 1, 2, 3, . . .  cid:2 d ←  cid:2 b − A cid:2 x α ←  cid:2   cid:2 d cid:2 2  cid:2 x ←  cid:2 x + α  cid:2 d  2  cid:2 d cid:2 A  cid:2 d   cid:3  Search direction is residual  cid:3  Line search formula   cid:3  Update solution vector  cid:2 x   210  cid:4  Numerical Algorithms  Suppose  cid:126 x∗ satisﬁes A cid:126 x∗ =  cid:126 b exactly. Then, the change in backward error in iteration k  is given by  Rk ≡  f   cid:126 xk  − f   cid:126 x∗  f   cid:126 xk−1  − f   cid:126 x∗   .  Bounding Rk < β < 1 for some ﬁxed β  possibly depending on A  would imply f   cid:126 xk  − f   cid:126 x∗  → 0 as k → ∞, showing that the gradient descent algorithm converges.  For convenience, we can expand f   cid:126 xk :  f   cid:126 xk  = f   cid:126 xk−1 + αk  cid:126 dk  by our iterative scheme  =  1 2    cid:126 xk−1 + αk  cid:126 dk  cid:62 A  cid:126 xk−1 + αk  cid:126 dk  −  cid:126 b cid:62   cid:126 xk−1 + αk  cid:126 dk  + c  1 = f   cid:126 xk−1  + αk  cid:126 d cid:62 k A cid:126 xk−1 + 2 = f   cid:126 xk−1  + αk  cid:126 d cid:62 k   cid:126 b −  cid:126 dk  + = f   cid:126 xk−1  − αk  cid:126 d cid:62 k α2 k  cid:126 d cid:62 k  cid:126 dk  cid:126 d cid:62 k A  cid:126 dk   cid:126 d cid:62 k  cid:126 dk 2 2  cid:126 d cid:62 k A cid:126 dk  = f   cid:126 xk−1  −  = f   cid:126 xk−1  −   cid:126 dk +     cid:126 d cid:62 k  1 2   cid:126 dk  +  .  α2 k   cid:126 d cid:62 k A  cid:126 dk − αk cid:126 b cid:62   cid:126 dk by deﬁnition of f  cid:126 d cid:62 k A cid:126 dk − αk cid:126 b cid:62   cid:126 dk since  cid:126 dk =  cid:126 b − A cid:126 xk−1 α2 k  1 2  cid:126 d cid:62 k A cid:126 dk since the remaining terms cancel  1  2 cid:32   cid:126 d cid:62 k  cid:126 d cid:62 k A  cid:126 dk cid:33 2   cid:126 dk   cid:126 d cid:62 k A cid:126 dk by deﬁnition of αk  We can use this formula to ﬁnd an alternative expression for the backward error Rk:  f   cid:126 xk−1  −    cid:126 d cid:62 k  2  cid:126 d cid:62 k A  cid:126 dk − f   cid:126 x∗    cid:126 dk 2  Rk =  f   cid:126 xk−1  − f   cid:126 x∗   cid:126 dk 2     cid:126 d cid:62 k  = 1 −  2 cid:126 d cid:62 k A cid:126 dk f   cid:126 xk−1  − f   cid:126 x∗    .  by the expansion of f   cid:126 xk   To simplify the diﬀerence in the denominator, we can use  cid:126 x∗ = A−1 cid:126 b to write:  f   cid:126 xk−1  − f   cid:126 x∗  = cid:20  1  2    cid:126 x∗  cid:62  cid:126 b −  cid:126 b cid:62  cid:126 x∗ + c cid:21    cid:126 x cid:62 k−1A cid:126 xk−1 −  cid:126 b cid:62  cid:126 xk−1 + c cid:21  − cid:20  1 2  cid:126 x cid:62 k−1A cid:126 xk−1 −  cid:126 b cid:62  cid:126 xk−1 +  A cid:126 xk−1 −  cid:126 b  cid:62 A−1 A cid:126 xk−1 −  cid:126 b  by symmetry of A  cid:126 d cid:62 k A−1  cid:126 dk by deﬁnition of  cid:126 dk.  1 2   cid:126 b cid:62 A−1 cid:126 b again since  cid:126 x∗ = A−1 cid:126 b  =  =  =  1 2 1 2 1 2  Plugging this expression into our simpliﬁed formula for Rk shows:  Rk = 1 −  = 1 −     cid:126 d cid:62 k   cid:126 dk 2   cid:126 d cid:62 k A cid:126 dk ·  cid:126 d cid:62 k A−1  cid:126 dk  cid:126 dk  cid:126 d cid:62 k  cid:126 d cid:62 k A cid:126 dk ·   cid:126 d cid:62 k A−1  cid:126 dk   cid:126 d cid:62 k   cid:126 dk   Iterative Linear Solvers  cid:4  211  1  Figure 11.2 Gradient descent starting from the origin  cid:126 0  at the center  on f   cid:126 x  = 2 cid:126 x cid:62 A cid:126 x −  cid:126 b cid:62  cid:126 x + c for two choices of A. Each ﬁgure shows level sets of f   cid:126 x  as well as iterates of gradient descent connected by line segments.  cid:126 d cid:62 A−1  cid:126 d cid:33  since this makes the second term smaller  cid:126 d cid:62 A−1  cid:126 d cid:33 −1   cid:126 d cid:62 A cid:126 d cid:33  cid:32  min  cid:126 d cid:62 A  cid:126 d cid:33 −1 cid:32  max  ≤ 1 − cid:32  min = 1 − cid:32  max   cid:107   cid:126 d cid:107 =1   cid:107   cid:126 d cid:107 =1  1  1   cid:107   cid:126 d cid:107 =1  where σmin, σmax are the minimum maximum singular values of A   cid:107   cid:126 d cid:107 =1 σmin σmax 1  cond A  .  = 1 − = 1 −  Here, we assume the condition number cond A is computed with respect to the two-norm of A. It took a considerable amount of algebra, but we proved an important fact:  Convergence of gradient descent on f depends on the  conditioning of A.  That is, the better conditioned A is, the faster gradient descent will converge. Additionally, since cond A ≥ 1, we know that gradient descent converges unconditionally to  cid:126 x∗, although convergence can be slow when A is poorly conditioned. Figure 11.2 illustrates the behavior of gradient descent for well and poorly conditioned matrices A. When the eigenvalues of A have a wide spread, A is poorly conditioned and gradient descent struggles to ﬁnd the minimum of our quadratic function f , zig-zagging along the energy landscape.  11.2 CONJUGATE GRADIENTS Solving A cid:126 x =  cid:126 b for dense A ∈ Rn×n takes O n3  time using Gaussian elimination. Reexam- ining gradient descent from §11.1.1 above, we see that in the dense case each iteration takes O n2  time, since we must compute matrix-vector products between A and  cid:126 xk−1,  cid:126 dk. So, if gradient descent takes more than n iterations, from a timing standpoint we might as well have used Gaussian elimination, which would have recovered the exact solution in the same amount of time. Unfortunately, gradient descent may never reach the exact solution  cid:126 x∗ in  Well conditioned A  Poorly conditioned A   212  cid:4  Numerical Algorithms  a ﬁnite number of iterations, and in poorly conditioned cases it can take a huge number of iterations to approximate  cid:126 x∗ well.  For this reason, we will design the conjugate gradients  CG  algorithm, which is guar- anteed to converge in at most n steps, preserving O n3  worst-case timing for solving linear systems. We also will ﬁnd that this algorithm exhibits better convergence properties overall, often making it preferable to gradient descent even if we do not run it to completion.  11.2.1 Motivation  Our derivation of the conjugate gradients algorithm is motivated by writing the energy functional f   cid:126 x  in an alternative form. Suppose we knew the solution  cid:126 x∗ to A cid:126 x∗ =  cid:126 b. Then, we could write:  f   cid:126 x  =   cid:126 x cid:62 A cid:126 x −  cid:126 b cid:62  cid:126 x + c by deﬁnition   cid:126 x −  cid:126 x∗  cid:62 A  cid:126 x −  cid:126 x∗  +  cid:126 x cid:62 A cid:126 x∗ −  1 2    cid:126 x∗  cid:62 A cid:126 x∗ −  cid:126 b cid:62  cid:126 x + c  by adding and subtracting the same terms    cid:126 x −  cid:126 x∗  cid:62 A  cid:126 x −  cid:126 x∗  +  cid:126 x cid:62  cid:126 b −   cid:126 x −  cid:126 x∗  cid:62 A  cid:126 x −  cid:126 x∗  + const. since the  cid:126 x cid:62  cid:126 b terms cancel.    cid:126 x∗  cid:62  cid:126 b −  cid:126 b cid:62  cid:126 x + c since A cid:126 x∗ =  cid:126 b  1 2  1 2 1 2  1 2 1 2  =  =  =  Thus, up to a constant shift, f is the same as the product 1 2   cid:126 x−  cid:126 x∗  cid:62 A  cid:126 x−  cid:126 x∗ . In practice, we do not know  cid:126 x∗, but this observation shows us the nature of f : It measures the distance from  cid:126 x to  cid:126 x∗ with respect to the “A-norm”  cid:107  cid:126 v cid:107 2 Since A is symmetric and positive deﬁnite, even if it might be slow to compute algorith- mically, we know from §4.2.1 that A admits a Cholesky factorization A = LL cid:62 . With this factorization, f takes a nicer form:  A ≡  cid:126 v cid:62 A cid:126 v.  f   cid:126 x  =  1 2 cid:107 L cid:62   cid:126 x −  cid:126 x∗  cid:107 2  2 + const.  From this form of f   cid:126 x , we now know that the A-norm truly measures a distance between  cid:126 x and  cid:126 x∗.  Deﬁne  cid:126 y ≡ L cid:62  cid:126 x and  cid:126 y∗ ≡ L cid:62  cid:126 x∗. After this change of variables, we are minimizing ¯f   cid:126 y  ≡  cid:107  cid:126 y −  cid:126 y∗ cid:107 2 2. Optimizing ¯f would be easy if we knew L and  cid:126 y∗  take  cid:126 y =  cid:126 y∗ , but to eventually remove the need for L we consider the possibility of minimizing ¯f using only line searches derived in §11.1.1; from this point on, we will assume that we use the optimal step α for this search rather than any other procedure. We make an observation about minimizing our simpliﬁed function ¯f using line searches,  illustrated in Figure 11.3: Proposition 11.1. Suppose {  cid:126 w1, . . . ,  cid:126 wn} are orthogonal in Rn. Then, ¯f is minimized in at most n steps by line searching in direction  cid:126 w1, then direction  cid:126 w2, and so on. Proof. Take the columns of Q ∈ Rn×n to be the vectors  cid:126 wi; Q is an orthogonal matrix. Since Q is orthogonal, we can write ¯f   cid:126 y  =  cid:107  cid:126 y −  cid:126 y∗ cid:107 2 2; in other words, we rotate so that  cid:126 w1 is the ﬁrst standard basis vector,  cid:126 w2 is the second, and so on. If we write  cid:126 z ≡ Q cid:62  cid:126 y and  cid:126 z∗ ≡ Q cid:62  cid:126 y∗, then after the ﬁrst iteration we must have z1 = z∗1 , after the second iteration z2 = z∗2 , and so on. After n steps we reach zn = z∗n, yielding the desired result.  2 =  cid:107 Q cid:62  cid:126 y − Q cid:62  cid:126 y∗ cid:107 2   Iterative Linear Solvers  cid:4  213  Figure 11.3 Searching along any two orthogonal directions minimizes ¯f   cid:126 y  =  cid:107  cid:126 y − 2 over  cid:126 y ∈ R2. Each example in this ﬁgure has the same starting point but  cid:126 y∗ cid:107 2 searches along a diﬀerent pair of orthogonal directions; in the end they all reach the same optimal point.  So, optimizing ¯f can be accomplished via n line searches so long as those searches are in orthogonal directions.  All we did to pass from f to ¯f is change coordinates using L cid:62 . Linear transformations take straight lines to straight lines, so line search on ¯f along some vector  cid:126 w is equivalent to line search along  L cid:62  −1  cid:126 w on the original quadratic function f . Conversely, if we do n line searches on f in directions  cid:126 vi such that L cid:62  cid:126 vi ≡  cid:126 wi are orthogonal, then by Proposition 11.1 we must have found  cid:126 x∗. The condition  cid:126 wi ·  cid:126 wj = 0 can be simpliﬁed:  0 =  cid:126 wi ·  cid:126 wj =  L cid:62  cid:126 vi  cid:62  L cid:62  cid:126 vj  =  cid:126 v cid:62 i  LL cid:62   cid:126 vj =  cid:126 v cid:62 i A cid:126 vj.  We have just argued a corollary to Proposition 11.1. Deﬁne conjugate vectors as follows:  Deﬁnition 11.1  A-conjugate vectors . Two vectors  cid:126 v,  cid:126 w are A-conjugate if  cid:126 v cid:62 A  cid:126 w = 0.  Then, we have shown how to use Proposition 11.1 to optimize f rather than ¯f :  Proposition 11.2. Suppose { cid:126 v1, . . . ,  cid:126 vn} are A-conjugate. Then, f is minimized in at most n steps by line search in direction  cid:126 v1, then direction  cid:126 v2, and so on.  Inspired by this proposition, the conjugate gradients algorithm generates and searches along A-conjugate directions rather than moving along −∇f . This change might appear somewhat counterintuitive: Conjugate gradients does not necessarily move along the steep- est descent direction in each iteration, but rather constructs a set of search directions satisfying a global criterion to avoid repeating work. This setup guarantees convergence in a ﬁnite number of iterations and acknowledges the structure of f in terms of ¯f discussed above.  We motivated the use of A-conjugate directions by their orthogonality after applying L cid:62  from the factorization A = LL cid:62 . From this standpoint, we are dealing with two dot products:  cid:126 xi· cid:126 xj and  cid:126 yi· cid:126 yj ≡  L cid:62  cid:126 xi · L cid:62  cid:126 xj  = x cid:62 i LL cid:62  cid:126 xj =  cid:126 x cid:62 i A cid:126 xj. These two products will ﬁgure into our subsequent discussion, so for clarity we will denote the “A-inner product” as   cid:104  cid:126 u,  cid:126 v cid:105 A ≡  L cid:62  cid:126 u  ·  L cid:62  cid:126 v  =  cid:126 u cid:62 A cid:126 v.   214  cid:4  Numerical Algorithms 11.2.2 Suboptimality of Gradient Descent If we can ﬁnd n A-conjugate search directions, then we can solve A cid:126 x =  cid:126 b in n steps via line searches along these directions. What remains is to uncover a formula for ﬁnding these directions eﬃciently. To do so, we will examine one more property of gradient descent that will inspire a more reﬁned algorithm.  Suppose we are at  cid:126 xk during an iterative line search method on f   cid:126 x ; we will call the direction of steepest descent of f at  cid:126 xk the residual  cid:126 rk ≡  cid:126 b − A cid:126 xk. We may not decide to do a line search along  cid:126 rk as in gradient descent, since the gradient directions are not necessarily A-conjugate. So, generalizing slightly, we will ﬁnd  cid:126 xk+1 via line search along a yet-undetermined direction  cid:126 vk+1.  From our derivation of gradient descent in §11.1.1, even if  cid:126 vk+1  cid:54 =  cid:126 rk, we should choose   cid:126 xk+1 =  cid:126 xk + αk+1 cid:126 vk+1, where  αk+1 =   cid:126 v cid:62 k+1 cid:126 rk   cid:126 v cid:62 k+1A cid:126 vk+1  .  Applying this expansion of  cid:126 xk+1, we can write an update formula for the residual:   cid:126 rk+1 =  cid:126 b − A cid:126 xk+1  =  cid:126 b − A  cid:126 xk + αk+1 cid:126 vk+1  by deﬁnition of  cid:126 xk+1 =   cid:126 b − A cid:126 xk  − αk+1A cid:126 vk+1 =  cid:126 rk − αk+1A cid:126 vk+1 by deﬁnition of  cid:126 rk.  This formula holds regardless of our choice of  cid:126 vk+1 and can be applied to any iterative line search method on f .  In the case of gradient descent, we chose  cid:126 vk+1 ≡  cid:126 rk, giving a recurrence relation  cid:126 rk+1 =   cid:126 rk − αk+1A cid:126 rk. This formula inspires an instructive proposition: Proposition 11.3. When performing gradient descent on f , span{ cid:126 r0, . . . ,  cid:126 rk} = span{ cid:126 r0, A cid:126 r0, . . . , Ak cid:126 r0}. Proof. This statement follows inductively from our formula for  cid:126 rk+1 above.  The structure we are uncovering is beginning to look a lot like the Krylov subspace methods mentioned in Chapter 6: This is not a coincidence!  Gradient descent gets to  cid:126 xk by moving along  cid:126 r0, then  cid:126 r1, and so on through  cid:126 rk. In the end we know that the iterate  cid:126 xk of gradient descent on f lies somewhere in the plane  cid:126 x0 + span{ cid:126 r0,  cid:126 r1, . . . ,  cid:126 rk−1} =  cid:126 x0 + span{ cid:126 r0, A cid:126 r0, . . . , Ak−1 cid:126 r0}, by Proposition 11.3. Unfor- tunately, it is not true that if we run gradient descent, the iterate  cid:126 xk is optimal in this subspace. In other words, it can be the case that   cid:126 xk −  cid:126 x0  cid:54 =  arg min  f   cid:126 x0 +  cid:126 v .   cid:126 v∈span { cid:126 r0,A cid:126 r0,...,Ak−1 cid:126 r0}  Ideally, switching this inequality to an equality would make sure that generating  cid:126 xk+1 from  cid:126 xk does not “cancel out” any work done during iterations 1 to k − 1. If we reexamine our proof of Proposition 11.1 from this perspective, we can make an observation suggesting how we might use conjugacy to improve gradient descent. Once zi switches to z∗i , it never changes in a future iteration. After rotating back from  cid:126 z to  cid:126 x the following proposition holds:   Iterative Linear Solvers  cid:4  215  Proposition 11.4. Take  cid:126 xk to be the k-th iterate of the process from Proposition 11.1 after searching along  cid:126 vk. Then,   cid:126 xk −  cid:126 x0 =  arg min  f   cid:126 x0 +  cid:126 v .   cid:126 v∈span { cid:126 v1,..., cid:126 vk}  In the best of all possible worlds and in an attempt to outdo gradient descent, we might hope to ﬁnd A-conjugate directions { cid:126 v1, . . . ,  cid:126 vn} such that span{ cid:126 v1, . . . ,  cid:126 vk} = span{ cid:126 r0, A cid:126 r0, . . . , Ak−1 cid:126 r0} for each k. By the previous two propositions, the resulting iter- ative scheme would be guaranteed to do no worse than gradient descent even if it is halted early. But, we wish to do so without incurring signiﬁcant memory demand or computation time. Amazingly, the conjugate gradient algorithm satisﬁes all these criteria.  11.2.3 Generating A-Conjugate Directions Given any set of directions spanning Rn, we can make them A-orthogonal using Gram- Schmidt orthogonalization. Explicitly orthogonalizing { cid:126 r0, A cid:126 r0, A2 cid:126 r0, . . .} to ﬁnd the set of search directions, however, is expensive and would require us to maintain a complete list of directions in memory; this construction likely would exceed the time and memory requirements even of Gaussian elimination. Alternatively, we will reveal one ﬁnal observation about Gram-Schmidt that makes conjugate gradients tractable by generating conjugate directions without an expensive orthogonalization process.  To start, we might write a “method of conjugate directions” using the following itera-  tions:   cid:126 vk ← Ak−1 cid:126 r0 − cid:80 i<k  cid:104 Ak−1 cid:126 r0, cid:126 vi cid:105 A αk ←  cid:126 v cid:62 k  cid:126 rk−1  cid:126 xk ←  cid:126 xk−1 + αk cid:126 vk  cid:126 rk ←  cid:126 rk−1 − αkA cid:126 vk   cid:104  cid:126 vi, cid:126 vi cid:105 A   cid:126 v cid:62 k A cid:126 vk   cid:126 vi   cid:46  Explicit Gram-Schmidt   cid:46  Line search  cid:46  Update estimate  cid:46  Update residual  Here, we compute the k-th search direction  cid:126 vk by projecting  cid:126 v1, . . . ,  cid:126 vk−1 out of the vector Ak−1 cid:126 r0 using the Gram-Schmidt algorithm. This algorithm has the property span{ cid:126 v1, . . . ,  cid:126 vk} = span{ cid:126 r0, A cid:126 r0, . . . , Ak−1 cid:126 r0} suggested in §11.2.2, but it has two issues: 1. Similar to power iteration for eigenvectors, the power Ak−1 cid:126 r0 is likely to look mostly like the ﬁrst eigenvector of A, making projection poorly conditioned when k is large.  2. We have to store  cid:126 v1, . . . ,  cid:126 vk−1 to compute  cid:126 vk, so each iteration needs more memory  and time than the last.  We can ﬁx the ﬁrst issue in a relatively straightforward manner. Right now, we project the previous search directions out of Ak−1 cid:126 r0, but in reality we can project out previous directions from any vector  cid:126 w so long as   cid:126 w ∈ span{ cid:126 r0, A cid:126 r0, . . . , Ak−1 cid:126 r0}\span{ cid:126 r0, A cid:126 r0, . . . , Ak−2 cid:126 r0},  that is, as long as  cid:126 w has some component in the new part of the space.  An alternative choice of  cid:126 w in this span is the residual  cid:126 rk−1. We can check this using the residual update  cid:126 rk =  cid:126 rk−1 − αkA cid:126 vk; in this expression, we multiply  cid:126 vk by A, introducing the new power of A that we need. This choice also more closely mimics the gradient descent algorithm, which took  cid:126 vk =  cid:126 rk−1. We can update our algorithm to use this improved choice:   216  cid:4  Numerical Algorithms   cid:126 vi   cid:104  cid:126 vi, cid:126 vi cid:105 A   cid:126 vk ←  cid:126 rk−1 − cid:80 i<k  cid:104  cid:126 rk−1, cid:126 vi cid:105 A αk ←  cid:126 v cid:62 k  cid:126 rk−1  cid:126 xk ←  cid:126 xk−1 + αk cid:126 vk  cid:126 rk ←  cid:126 rk−1 − αkA cid:126 vk   cid:126 v cid:62 k A cid:126 vk   cid:46  Gram-Schmidt on residual   cid:46  Line search  cid:46  Update estimate  cid:46  Update residual  Now we do not do arithmetic with the poorly conditioned vector Ak−1 cid:126 r0 but still have the “memory” problem above since the sum in the ﬁrst step is over k − 1 vectors. A surprising observation about the residual Gram-Schmidt projection above is that most terms in the sum are exactly zero! This observation allows each iteration of conjugate gradients to be carried out without increasing memory requirements. We memorialize this result in a proposition:  Proposition 11.5. In the second “conjugate direction” method above,  cid:104  cid:126 rk,  cid:126 v cid:96  cid:105 A = 0 for all  cid:96  < k.  Proof. We proceed inductively. There is nothing to prove for the base case k = 1, so assume k > 1 and that the result holds for all k cid:48  < k. By the residual update formula,   cid:104  cid:126 rk,  cid:126 v cid:96  cid:105 A =  cid:104  cid:126 rk−1,  cid:126 v cid:96  cid:105 A − αk cid:104 A cid:126 vk,  cid:126 v cid:96  cid:105 A =  cid:104  cid:126 rk−1,  cid:126 v cid:96  cid:105 A − αk cid:104  cid:126 vk, A cid:126 v cid:96  cid:105 A,  where the second equality follows from symmetry of A.  First, suppose  cid:96  < k− 1. Then the ﬁrst term of the diﬀerence above is zero by induction. Furthermore, by construction A cid:126 v cid:96  ∈ span{ cid:126 v1, . . . ,  cid:126 v cid:96 +1}, so since we have constructed our search directions to be A-conjugate, the second term must be zero as well. To conclude the proof, we consider the case  cid:96  = k − 1. By the residual update formula,  Pre-multiplying by  cid:126 r cid:62 k shows  A cid:126 vk−1 =    cid:126 rk−2 −  cid:126 rk−1 .  1 αk−1   cid:104  cid:126 rk,  cid:126 vk−1 cid:105 A =   cid:126 r cid:62 k   cid:126 rk−2 −  cid:126 rk−1 .  1 αk−1  The diﬀerence  cid:126 rk−2 −  cid:126 rk−1 is in the subspace span{ cid:126 r0, A cid:126 r0, . . . , Ak−1 cid:126 r0}, by the residual update formula. Proposition 11.4 shows that  cid:126 xk is optimal in this subspace. Since  cid:126 rk = −∇f   cid:126 xk , this implies that we must have  cid:126 rk ⊥ span{ cid:126 r0, A cid:126 r0, . . . , Ak−1 cid:126 r0}, since otherwise there would exist a direction in the subspace to move from  cid:126 xk to decrease f . In particular, this shows the inner product above  cid:104  cid:126 rk,  cid:126 vk−1 cid:105 A = 0, as desired.  Our proof above shows that we can ﬁnd a new direction  cid:126 vk as follows:   cid:126 vk =  cid:126 rk−1 − cid:88 i<k  cid:104  cid:126 rk−1,  cid:126 vi cid:105 A  cid:104  cid:126 vi,  cid:126 vi cid:105 A =  cid:126 rk−1 −  cid:104  cid:126 rk−1,  cid:126 vk−1 cid:105 A  cid:104  cid:126 vk−1,  cid:126 vk−1 cid:105 A   cid:126 vi by the Gram-Schmidt formula   cid:126 vk−1 because the remaining terms vanish.  Since the summation over i disappears, the cost of computing  cid:126 vk has no dependence on k.   Iterative Linear Solvers  cid:4  217  11.2.4 Formulating the Conjugate Gradients Algorithm  Now that we can obtain A-conjugate search directions with relatively little computational eﬀort, we apply this strategy to formulate the conjugate gradients algorithm, with full pseudocode in Figure 11.4 a :   cid:126 vk−1   cid:126 vk ←  cid:126 rk−1 −  cid:104  cid:126 rk−1, cid:126 vk−1 cid:105 A  cid:104  cid:126 vk−1, cid:126 vk−1 cid:105 A αk ←  cid:126 v cid:62 k  cid:126 rk−1  cid:126 xk ←  cid:126 xk−1 + αk cid:126 vk  cid:126 rk ←  cid:126 rk−1 − αkA cid:126 vk   cid:126 v cid:62 k A cid:126 vk   cid:46  Update search direction   cid:46  Line search  cid:46  Update estimate  cid:46  Update residual  This iterative scheme is only a minor adjustment to the gradient descent algorithm but has many desirable properties by construction:    f   cid:126 xk  is upper-bounded by that of the k-th iterate of gradient descent.   The algorithm converges to  cid:126 x∗ in at most n steps, as illustrated in Figure 11.5.   At each step, the iterate  cid:126 xk is optimal in the subspace spanned by the ﬁrst k search  directions.  In the interests of squeezing maximal numerical quality out of conjugate gradients, we can simplify the numerics of the formulation in Figure 11.4 a . For instance, if we plug the search direction update into the formula for αk, by orthogonality we know  The numerator of this fraction now is guaranteed to be nonnegative even when using ﬁnite- precision arithmetic.  Similarly, we can deﬁne a constant βk to split the search direction update into two steps:  αk =   cid:126 r cid:62 k−1 cid:126 rk−1  cid:126 v cid:62 k A cid:126 vk  .  βk ≡ − cid:104  cid:126 rk−1,  cid:126 vk−1 cid:105 A  cid:104  cid:126 vk−1,  cid:126 vk−1 cid:105 A  cid:126 vk =  cid:126 rk−1 + βk cid:126 vk−1.  We can simplify the formula for βk:  by deﬁnition of  cid:104 ·,· cid:105 A  since  cid:126 rk =  cid:126 rk−1 − αkA cid:126 vk  by a calculation below  βk = −  = −   cid:126 rk−1A cid:126 vk−1  cid:126 v cid:62 k−1A cid:126 vk−1  cid:126 r cid:62 k−1  cid:126 rk−2 −  cid:126 rk−1  αk−1 cid:126 v cid:62 k−1A cid:126 vk−1  cid:126 r cid:62 k−1 cid:126 rk−1  =  =  αk−1 cid:126 v cid:62 k−1A cid:126 vk−1  cid:126 r cid:62 k−1 cid:126 rk−1  cid:126 r cid:62 k−2 cid:126 rk−2  by our last formula for αk.  This expression guarantees that βk ≥ 0, a property that might not have held after rounding using the original formula. We have one remaining calculation below:   cid:126 r cid:62 k−2 cid:126 rk−1 =  cid:126 r cid:62 k−2  cid:126 rk−2 − αk−1A cid:126 vk−1  by the residual update formula   218  cid:4  Numerical Algorithms  Figure 11.4 Two equivalent formulations of the conjugate gradients algorithm for solving A cid:126 x =  cid:126 b when A is symmetric and positive deﬁnite. The initial guess  cid:126 x0 can be  cid:126 0 in the absence of a better estimate.  Figure 11.5 The conjugate gradients algorithm solves both linear systems in Fig- ure 11.2 in two steps.  function Conjugate-Grad-1 A, cid:2 b,  cid:2 x0    cid:2 x ←  cid:2 x0  cid:2 r ←  cid:2 b − A cid:2 x  cid:2 v ←  cid:2 r for k ← 1, 2, 3, . . .  α ←  cid:2 v cid:2  cid:2 r  cid:2 v cid:2 A cid:2 v  cid:2 x ←  cid:2 x + α cid:2 v  cid:2 r ←  cid:2 r − αA cid:2 v 2 < ε cid:3  cid:2 r0 cid:3 2 if  cid:3  cid:2 r cid:3 2 return x∗ =  cid:2 x  cid:2 v ←  cid:2 r −  cid:3  cid:2 r, cid:2 v cid:4 A  cid:2 v  cid:3  cid:2 v, cid:2 v cid:4 A   cid:4  Line search  cid:4  Update estimate  cid:4  Update residual  2 then   cid:4  Search direction  function Conjugate-Grad-2 A, cid:2 b,  cid:2 x0    cid:2 x ←  cid:2 x0  cid:2 r ←  cid:2 b − A cid:2 x  cid:2 v ←  cid:2 r β ← 0 for k ← 1, 2, 3, . . .   cid:2 v ←  cid:2 r + β cid:2 v α ←  cid:5  cid:2 r cid:5 2 2  cid:2 v cid:2 A cid:2 v  cid:2 x ←  cid:2 x + α cid:2 v  cid:2 rold ←  cid:2 r  cid:2 r ←  cid:2 r − αA cid:2 v if  cid:3  cid:2 r cid:3 2 β ←  cid:5  cid:2 r cid:5 2  2 < ε cid:3  cid:2 r0 cid:3 2 return x∗ =  cid:2 x 2  cid:5  cid:2 rold cid:5 2  2   cid:4  Search direction  cid:4  Line search  cid:4  Update estimate  cid:4  Save old residual  cid:4  Update residual  2 then   cid:4  Direction step  Well conditioned A  Poorly conditioned A   Iterative Linear Solvers  cid:4  219   cid:126 r cid:62 k−2A cid:126 vk−1 by our formula for αk  =  cid:126 r cid:62 k−2 cid:126 rk−2 −  =  cid:126 r cid:62 k−2 cid:126 rk−2 −   cid:126 r cid:62 k−2 cid:126 rk−2  cid:126 v cid:62 k−1A cid:126 vk−1  cid:126 r cid:62 k−2 cid:126 rk−2  cid:126 v cid:62 k−1A cid:126 vk−1  = 0, as needed.   cid:126 v cid:62 k−1A cid:126 vk−1  by the update for  cid:126 vk and A-conjugacy of the  cid:126 vk’s  Our new observations about the iterates of CG provide an alternative but equivalent for- mulation that can have better numerical properties; it is shown in Figure 11.4 b . Also for numerical reasons, occasionally rather than using the update formula for  cid:126 rk it is advisable to use the residual formula  cid:126 rk =  cid:126 b − A cid:126 xk. This requires an extra matrix-vector multiply but repairs numerical “drift” caused by ﬁnite-precision rounding. There is no need to store a long list of previous residuals or search directions; conjugate gradients takes a constant amount of space from iteration to iteration.  11.2.5 Convergence and Stopping Conditions  By construction, the conjugate gradients  CG  algorithm is guaranteed to converge as fast as gradient descent on f , while being no harder to implement and having a number of other favorable properties. A detailed discussion of CG convergence is out of the scope of our treatment, but in general the algorithm behaves best on matrices with eigenvalues evenly distributed over a small range.  One rough bound paralleling the estimate in §11.1.2 shows that the CG algorithm sat-  isﬁes:  f   cid:126 xk  − f   cid:126 x∗   f   cid:126 x0  − f   cid:126 x∗  ≤ 2 cid:18 √κ − 1 √κ + 1 cid:19 k  where κ ≡ cond A. Broadly speaking, the number of iterations needed for conjugate gradient to reach a given error level usually can be bounded by a function of √κ, whereas bounds for convergence of gradient descent are proportional to κ.  Conjugate gradients is guaranteed to converge to  cid:126 x∗ exactly in n steps—m steps if A has m < n unique eigenvalues—but when n is large it may be preferable to stop earlier. The formula for βk will divide by zero when the residual gets very short, which can cause numerical precision issues near the minimum of f . Thus, in practice CG usually is halted when the ratio  cid:107  cid:126 rk cid:107   cid:107  cid:126 r0 cid:107  is suﬃciently small. 11.3 PRECONDITIONING We now have two powerful iterative algorithms for solving A cid:126 x =  cid:126 b when A is symmetric and positive deﬁnite: gradient descent and conjugate gradients. Both converge uncondition- ally, meaning that regardless of the initial guess  cid:126 x0, with enough iterations they will get arbitrarily close to the true solution  cid:126 x∗; conjugate gradients reaches  cid:126 x∗ exactly in a ﬁnite number of iterations. The “clock time” taken to solve A cid:126 x =  cid:126 b for both of these methods is proportional to the number of iterations needed to reach  cid:126 x∗ within an acceptable tolerance, so it makes sense to minimize the number of iterations until convergence.  We characterized the convergence rates of both algorithms in terms of the condition number cond A. The smaller the value of cond A, the less time it should take to solve A cid:126 x =  cid:126 b. This situation contrasts with Gaussian elimination, which takes the same number of steps regardless of A; what is new here is that the conditioning of A aﬀects not only the quality of the output of iterative methods but also the speed at which  cid:126 x∗ is approached.   220  cid:4  Numerical Algorithms  For any invertible matrix P , solving P A cid:126 x = P cid:126 b is equivalent to solving A cid:126 x =  cid:126 b. The condition number of P A, however, does not need to be the same as that of A. In the extreme, if we took P = A−1, then conditioning issues would be removed altogether! More generally, suppose P ≈ A−1. Then, we expect cond P A  cid:28  cond A, making it advisable to apply P before solving the linear system using iterative methods. In this case, we will call P a preconditioner.  While the idea of preconditioning appears attractive, two issues remain:  1. While A may be symmetric and positive deﬁnite, the product P A in general will not  enjoy these properties.  2. We need to ﬁnd P ≈ A−1 that is easier to compute than A−1 itself.  We address these issues in the sections below.  11.3.1 CG with Preconditioning  We will focus our discussion of preconditioning on conjugate gradients since it has better convergence properties than gradient descent, although most of our constructions can be paralleled to precondition other iterative linear methods.  Starting from the steps in §11.2.1, the construction of CG fundamentally depended on both the symmetry and positive deﬁniteness of A. Hence, running CG on P A usually will not converge, since it may violate these assumptions. Suppose, however, that the preconditioner P is itself symmetric and positive deﬁnite. This is a reasonable assumption since the inverse A−1 of a symmetric, positive deﬁnite matrix A is itself symmetric and positive deﬁnite. Under this assumption, we can write a Cholesky factorization of the inverse P −1 = EE cid:62 . Then, E−1AE− cid:62  ≈ E−1P −1E− cid:62  = E−1EE cid:62 E− cid:62  = In×n. In words, we expect E−1AE− cid:62  to be well-conditioned when P A is well-conditioned. This intuition is partially conﬁrmed by the following observation:  Proposition 11.6. P A and E−1AE− cid:62  have the same eigenvalues.  Proof. Suppose E−1AE− cid:62  cid:126 x = λ cid:126 x; notice the vectors  cid:126 x span Rn because E−1AE− cid:62  is symmetric. By construction, P −1 = EE cid:62 , so P = E− cid:62 E−1. If we pre-multiply both sides of the eigenvector expression by E− cid:62 , we ﬁnd P AE− cid:62  cid:126 x = λE− cid:62  cid:126 x. Deﬁning  cid:126 y ≡ E− cid:62  cid:126 x shows P A cid:126 y = λ cid:126 y. Hence, each eigenvector  cid:126 x of E−1AE− cid:62  provides a corresponding eigenvector  cid:126 y of P A, showing that P A and E−1AE− cid:62  both have full eigenspaces and identical eigenvalues.  This proposition implies that if we do CG on the symmetric positive deﬁnite matrix E−1AE− cid:62 , we will receive similar conditioning beneﬁts enjoyed by P A. Imitating the con- struction in Proposition 11.6 above, we can carry out our new solve for  cid:126 y = E cid:62  cid:126 x in two steps:  1. Solve E−1AE− cid:62  cid:126 y = E−1 cid:126 b for  cid:126 y using the CG algorithm.  2. Multiply to ﬁnd  cid:126 x = E− cid:62  cid:126 y.  Evaluating E and its inverse would be integral to this strategy, but doing so can induce ﬁll and take too much time. By modifying the steps of CG for the ﬁrst step above, however, we can make this factorization unnecessary.  If we had computed E, we could perform step 1 using CG as follows:   Iterative Linear Solvers  cid:4  221  βk ←  cid:126 r cid:62 k−1 cid:126 rk−1  cid:126 r cid:62 k−2 cid:126 rk−2  cid:126 vk ←  cid:126 rk−1 + βk cid:126 vk−1 αk ←  cid:126 r cid:62 k−1 cid:126 rk−1  cid:126 v cid:62 k E−1AE− cid:62  cid:126 vk  cid:126 yk ←  cid:126 yk−1 + αk cid:126 vk  cid:126 rk ←  cid:126 rk−1 − αkE−1AE− cid:62  cid:126 vk   cid:46  Update search direction   cid:46  Line search  cid:46  Update estimate  cid:46  Update residual  This iteration will converge according to the conditioning of E−1AE− cid:62 .  Deﬁne ˜rk ≡ E cid:126 rk, ˜vk ≡ E− cid:62  cid:126 vk, and  cid:126 xk ≡ E− cid:62  cid:126 yk. By the relationship P = E− cid:62 E−1, we can rewrite our preconditioned conjugate gradients iteration completely in terms of these new variables:  βk ← ˜r cid:62 k−1P ˜rk−1 ˜r cid:62 k−2P ˜rk−2 ˜vk ← P ˜rk−1 + βk ˜vk−1 αk ← ˜r cid:62 k−1P ˜rk−1  cid:126 xk ←  cid:126 xk−1 + αk ˜vk ˜rk ← ˜rk−1 − αkA˜vk  ˜v cid:62 k A˜vk   cid:46  Update search direction   cid:46  Line search  cid:46  Update estimate  cid:46  Update residual  This iteration does not depend on the Cholesky factorization of P −1, but instead can be carried out using only P and A. By the substitutions above,  cid:126 xk →  cid:126 x∗, and this scheme enjoys the beneﬁts of preconditioning without needing to compute the Cholesky factorization of P .  As a side note, more general preconditioning can be carried out by replacing A with P AQ for a second matrix Q, although this second matrix will require additional computations to apply. This extension presents a common trade-oﬀ: If a preconditioner takes too long to apply in each iteration of CG, it may not be worth the reduced number of iterations.  11.3.2 Common Preconditioners  Finding good preconditioners in practice is as much an art as it is a science. Finding an eﬀective approximation P of A−1 depends on the structure of A, the particular application at hand, and so on. Even rough approximations, however, can help convergence, so rarely do applications of CG appear that do not use a preconditioner.  The best strategy for ﬁnding P often is application-speciﬁc, and generally it is necessary to test a few possibilities for P before settling on the most eﬀective option. A few common generic preconditioners include the following:    A diagonal  or “Jacobi ”  preconditioner takes P to be the matrix obtained by inverting diagonal elements of A; that is, P is the diagonal matrix with entries 1 aii. This preconditioner can alleviate nonuniform scaling from row to row, which is a common cause of poor conditioning.    The sparse approximate inverse preconditioner is formulated by solving a subproblem minP∈S  cid:107 AP − I cid:107 Fro, where P is restricted to be in a set S of matrices over which it is less diﬃcult to optimize such an objective. For instance, a common constraint is to prescribe a sparsity pattern for P , e.g., that it only has nonzeros on its diagonal or where A has nonzeros.   222  cid:4  Numerical Algorithms    The incomplete Cholesky preconditioner factors A ≈ L∗L cid:62   ∗ and then approximates A−1 by carrying out forward- and back-substitution. For instance, a popular heuristic involves going through the steps of Cholesky factorization but only saving the parts of L in positions  i, j  where aij  cid:54 = 0.    The nonzero values in A can be used to construct a graph with edge  i, j  whenever aij  cid:54 = 0. Removing edges in the graph or grouping nodes may disconnect assorted components; the resulting system is block-diagonal after permuting rows and columns and thus can be solved using a sequence of smaller solves. Such a domain decompo- sition can be eﬀective for linear systems arising from diﬀerential equations like those considered in Chapter 16.  Some preconditioners come with bounds describing changes to the conditioning of A after replacing it with P A, but for the most part these are heuristic strategies that should be tested and reﬁned.  11.4 OTHER ITERATIVE ALGORITHMS The algorithms we have developed in this chapter apply to solving A cid:126 x =  cid:126 b when A is square, symmetric, and positive deﬁnite. We have focused on this case because it appears so often in practice, but there are cases when A is asymmetric, indeﬁnite, or even rectangular. It is out of the scope of our discussion to derive iterative algorithms in each case, since many require some specialized analysis or advanced development  see, e.g., [7, 50, 56, 105] , but we summarize some techniques here:    Splitting methods decompose A = M − N and use the fact that A cid:126 x =  cid:126 b is equivalent to M cid:126 x = N cid:126 x +  cid:126 b. If M is easy to invert, then a ﬁxed-point scheme can be derived by writing M cid:126 xk = N cid:126 xk−1 +  cid:126 b; these techniques are easy to implement but have convergence depending on the spectrum of the matrix G = M−1N and in particular can diverge when the spectral radius of G is greater than one. One popular choice of M is the diagonal of A. Methods such as successive over-relaxation  SOR  weight these two terms for better convergence.    The conjugate gradient normal equation residual  CGNR  method applies the CG al- gorithm to the normal equations A cid:62 A cid:126 x = A cid:62  cid:126 b. This method is guaranteed to converge so long as A is full-rank, but convergence can be slow thanks to poor conditioning of A cid:62 A as in §5.1.    The conjugate gradient normal equation error  CGNE  method similarly solves  AA cid:62  cid:126 y =  cid:126 b; then, the solution of A cid:126 x =  cid:126 b is A cid:62  cid:126 y.    Methods such as MINRES and SYMMLQ apply to all symmetric matrices A by 2 [93]; this function g is  replacing the quadratic form f   cid:126 x  with g  cid:126 x  ≡  cid:107  cid:126 b − A cid:126 x cid:107 2 minimized at solutions to A cid:126 x =  cid:126 b regardless of the deﬁniteness of A.    Given the poor conditioning of CGNR and CGNE, the LSQR and LSMR algorithms also minimize g  cid:126 x  with fewer assumptions on A, in particular allowing for solution of least-squares systems [94, 42].    Generalized methods including GMRES, QMR, BiCG, CGS, and BiCGStab solve A cid:126 x =  cid:126 b with the only caveat that A is square and invertible [106, 44, 40, 115, 126]. They optimize similar energies but often have to store more information about previous   Iterative Linear Solvers  cid:4  223  iterations and may have to factor intermediate matrices to guarantee convergence with such generality.    Finally, methods like the Fletcher-Reeves, Hestenes-Stiefel, Polak-Ribi`ere, and Dai- Yuan algorithms return to the more general problem of minimizing a non-quadratic function f , applying conjugate gradient steps to ﬁnding new line search directions [30, 41, 59, 100]. Functions f that are well-approximated by quadratics can be minimized very eﬀectively using these strategies, even though they do not necessarily make use of the Hessian. For instance, the Fletcher-Reeves method replaces the residual in CG iterations with the negative gradient −∇f .  Most of these algorithms are nearly as easy to implement as CG or gradient descent. Pre- packaged implementations are readily available that only require A and  cid:126 b as input; they typically require the end user to implement subroutines for multiplying vectors by A and by A cid:62 , which can be a technical challenge in some cases when A is only known implicitly. As a rule of thumb, the more general a method is—that is, the fewer the assumptions a method makes on the structure of the matrix A—the more iterations it is likely to need to compensate for this lack of assumptions. This said, there are no hard-and-fast rules that can be applied by examining the elements of A for guessing the most successful iterative scheme.  11.5 EXERCISES  11.1 If we use inﬁnite-precision arithmetic  so rounding is not an issue , can the conjugate gradients algorithm be used to recover exact solutions to A cid:126 x =  cid:126 b for symmetric positive deﬁnite matrices A? Why or why not?  11.2 Suppose A ∈ Rn×n is invertible but not symmetric or positive deﬁnite.   a  Show that A cid:62 A is symmetric and positive deﬁnite.   b  Propose a strategy for solving A cid:126 x =  cid:126 b using the conjugate gradients algorithm  based on your observation in  a .   c  How quickly do you expect conjugate gradients to converge in this case? Why?  11.3 Propose a method for preconditioning the gradient descent algorithm from §11.1.1,  paralleling the derivation in §11.3.  11.4 In this problem we will derive an iterative method of solving A cid:126 x =  cid:126 b via splitting [50].   a  Suppose we decompose A = M − N , where M is invertible. Show that the iterative scheme  cid:126 xk = M−1 N cid:126 xk−1 +  cid:126 b  converges to A−1 cid:126 b when max{λ : λ is an eigenvalue of M−1N} < 1. Hint: Deﬁne  cid:126 x∗ = A−1 cid:126 b and take  cid:126 ek =  cid:126 xk −  cid:126 x∗. Show that  cid:126 ek = Gk cid:126 e0, where G = M−1N. For this problem, you can assume that the eigenvectors of G span Rn  it is possible to prove this statement without the assumption but doing so requires more analysis than we have covered .   224  cid:4  Numerical Algorithms   b  Suppose A is strictly diagonally dominant, that is, for each i it satisﬁes   cid:88 j cid:54 =i  aij < aii.  Suppose we deﬁne M to be the diagonal part of A and N = M − A. Show that the iterative scheme from Exercise 11.4a converges in this case. You can assume the statement from Exercise 11.4a holds regardless of the eigenspace of G.  11.5 As introduced in §10.4.3, a graph is a data structure G =  V, E  consisting of n vertices in a set V = {1, . . . , n} and a set of edges E ⊆ V × V. A common problem is graph layout, where we choose positions of the vertices in V on the plane R2 respecting the connectivity of G. For this problem we will assume  i, i   cid:54 ∈ E for all i ∈ V .  a  Take  cid:126 v1, . . . ,  cid:126 vn ∈ R2 to be the positions of the vertices in V ; these are the  unknowns in graph layout. The Dirichlet energy of a layout is  E  cid:126 v1, . . . ,  cid:126 vn  =  cid:88  i,j ∈E   cid:107  cid:126 vi −  cid:126 vj cid:107 2 2.  Suppose an artist speciﬁes positions of vertices in a nonempty subset V0 ⊆ V . We will label these positions as  cid:126 v0 k for k ∈ V0. Derive two  n − V0  ×  n − V0  linear systems of equations satisﬁed by the x and y components of the unknown  cid:126 vi’s solving the following minimization problem:  minimize E  cid:126 v1, . . . ,  cid:126 vn  subject to  cid:126 vk =  cid:126 v0  k ∀k ∈ V0.  Hint: Your answer can be written as two independent linear systems A cid:126 x =  cid:126 bx and A cid:126 y =  cid:126 by.   b  Show that your systems from the previous part are symmetric and positive deﬁ-  nite.   c   Implement both gradient descent and conjugate gradients for solving this system, updating a display of the graph layout after each iteration. Compare the number of iterations needed to reach a reasonable solution using both strategies.   d   Implement preconditioned conjugate gradients using a preconditioner of your choice. How much does convergence improve?  DH 11.6 The successive over-relaxation  SOR  method is an example of an iterative splitting method for solving A cid:126 x =  cid:126 b, for A ∈ Rn×n. Suppose we decompose A = D + L + U , where D, L, and U are the diagonal, strictly lower-triangular, and strictly upper- triangular parts of A, respectively. Then, the SOR iteration is given by:   ω−1D + L  cid:126 xk+1 =   ω−1 − 1 D − U   cid:126 xk +  cid:126 b,  for some constant ω ∈ R. We will show that if A is symmetric and positive deﬁnite and ω ∈  0, 2 , then the SOR method converges.  a  Show how SOR is an instance of the splitting method in Exercise 11.4 by deﬁning matrices M and N appropriately. Hence, using this problem we now only need to show that ρ G  < 1 for G = M−1N to establish convergence of SOR.   Iterative Linear Solvers  cid:4  225   b  Deﬁne Q ≡  ω−1D + L  and let  cid:126 y =  In×n − G  cid:126 x for an arbitrary eigenvector  cid:126 x ∈ Cn of G with corresponding eigenvalue λ ∈ C. Derive expressions for Q cid:126 y and  Q − A  cid:126 y in terms of A,  cid:126 x, and λ.   c  Show that dii > 0 for all i. This expression shows that all the possibly nonzero  elements of the diagonal matrix D are positive.   d  Substitute the deﬁnition of Q into your relationships from Exercise 11.6b and  simplify to show that:  ω−1 cid:104  cid:126 y,  cid:126 y cid:105 D +  cid:104  cid:126 y,  cid:126 y cid:105 L =  1 − ¯λ  cid:104  cid:126 x,  cid:126 x cid:105 A  ω−1 − 1  cid:104  cid:126 y,  cid:126 y cid:105 D −  cid:104  cid:126 y,  cid:126 y cid:105 U cid:62  =  1 − λ ¯λ cid:104  cid:126 x,  cid:126 x cid:105 A.  Note: We are dealing with complex values here, so inner products in this problem are given by  cid:104  cid:126 x,  cid:126 y cid:105 A ≡  A cid:126 x  cid:62 conjugate  cid:126 y .   e  Recalling our assumptions on A, write a relationship between L and U . Use this  and the previous part to conclude that   2ω−1 − 1  cid:104  cid:126 y,  cid:126 y cid:105 D =  1 − λ2  cid:104  cid:126 x,  cid:126 x cid:105 A.   f  Justify why, under the given assumptions and results of the previous parts, each of  2ω−1 − 1 ,  cid:104  cid:126 y,  cid:126 y cid:105 D, and  cid:104  cid:126 x,  cid:126 x cid:105 A must be positive. What does this imply about λ? Conclude that the SOR method converges under our assumptions.  DH 11.7  “Gradient domain painting,” [86]  Let I : S → R be a monochromatic image, where  S ⊂ R2 is a rectangle. We know I on a collection of square pixels tiling S. Suppose an artist is editing I in the gradient domain. This means the artist edits the x and y derivatives gx and gy of I rather than values in I. After editing gx and gy, we need to recover a new image ˜I that has the edited gradients, at least approximately.   a  For the artist to paint in the gradient domain, we ﬁrst have to calculate discrete approximations of gx and gy using the values of I on diﬀerent pixels. How might you estimate the derivatives of I in the x and y directions from a pixel using the values of I at one or both of the two horizontally adjacent pixels?   b  Describe matrices Ax and Ay such that AxI = gx and AyI = gy, where in this case we have written I as a vector I = [I1,1, I1,2, ..., I1,n, I2,1, ..., Im,n]T and Ii,j is the value of I at pixel  i, j . Assume the image I is m pixels tall and n pixels wide.   d    c  Give an example of a function g : R2 → R2 that is not a gradient, that is, g  admits no f such that ∇f = g. Justify your answer. In light of the fact that ∇ ˜I = g may not be solvable exactly, propose an opti- mization problem whose solution is the “best” approximate solution  in the L2 norm  to this equation. Describe the advantage of using conjugate gradients to solve such a system.  11.8 The locally optimal block preconditioned conjugate gradient  LOBPCG  algorithm applies conjugate gradients to ﬁnding generalized eigenvectors  cid:126 x of matrices A and B satisfying A cid:126 x = λB cid:126 x [75, 76]. Assume A, B ∈ Rn×n are symmetric and positive deﬁnite.   226  cid:4  Numerical Algorithms   a  Deﬁne the generalized Rayleigh quotient ρ  cid:126 x  as the function  ρ  cid:126 x  ≡   cid:126 x cid:62 A cid:126 x  cid:126 x cid:62 B cid:126 x  .  Show that ∇ρ is parallel to A cid:126 x − ρ  cid:126 x B cid:126 x.   b  Show that critical points of ρ  cid:126 x  with  cid:126 x  cid:54 =  cid:126 0 are the generalized eigenvectors of  A, B . Argue that the largest and smallest generalized eigenvalues come from maximizing and minimizing ρ  cid:126 x , respectively.   c  Suppose we wish to ﬁnd the generalized eigenvector with the largest eigenvalue. If we search in the gradient direction from the current iterate  cid:126 x, we must solve the following line search problem:  max  α∈R ρ  cid:126 x + α cid:126 r  cid:126 x  ,  where r  cid:126 x  ≡ A cid:126 x − ρ  cid:126 x B cid:126 x. Show that α can be found by computing roots of a low-degree polynomial.   d  Based on our construction above, propose an iteration for ﬁnding  cid:126 x. When B =  In×n, is this method the same as the power method?   C H A P T E R12  Specialized Optimization Methods  CONTENTS  12.1 Nonlinear Least-Squares . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12.1.1 Gauss-Newton . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12.1.2 Levenberg-Marquardt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12.2 Iteratively Reweighted Least-Squares . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12.3 Coordinate Descent and Alternation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12.3.1 Identifying Candidates for Alternation . . . . . . . . . . . . . . . . . . . . . . . . . 12.3.2 Augmented Lagrangians and ADMM . . . . . . . . . . . . . . . . . . . . . . . . . . . 12.4 Global Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12.4.1 Graduated Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12.4.2 Randomized Global Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12.5 Online Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  227 228 229 230 231 231 235 240 241 243 244  O PTIMIZATION algorithms like Newton’s method are completely generic approaches to minimizing a function f   cid:126 x , with or without constraints on  cid:126 x. These algorithms make few assumptions about the form of f or the constraints. Contrastingly, by designing the 2  cid:126 x cid:62 A cid:126 x− cid:126 b cid:62  cid:126 x+ conjugate gradient algorithm speciﬁcally for minimizing the objective f   cid:126 x  ≡ 1 c, we were able to guarantee more reliable and eﬃcient behavior than general algorithms. In this chapter, we continue to exploit special structure to solve optimization problems, this time for more complex nonlinear objectives. Replacing monolithic generic algorithms with ones tailored to a given problem can make optimization faster and easier to trou- bleshoot, although doing so requires more implementation eﬀort than calling a pre-packaged solver.  12.1 NONLINEAR LEAST-SQUARES  Recall the nonlinear regression problem posed in Example 9.1. If we wish to ﬁt a function y = ceax to a set of data points  x1, y1 , . . . ,  xk, yk , an optimization mimicking linear least-squares is to minimize the function  E a, c  ≡ cid:88 i   yi − ceaxi 2.  This energy reﬂects the fact that we wish yi − ceaxi ≈ 0 for all i.  227   228  cid:4  Numerical Algorithms  More generally, suppose we are given a set of functions f1  cid:126 x , . . . , fk  cid:126 x  for  cid:126 x ∈ Rn. If we want fi  cid:126 x  ≈ 0 for all i, then a reasonable objective trading oﬀ between these terms is  ENLS  cid:126 x  ≡  [fi  cid:126 x ]2.  1  2 cid:88 i  Objective functions of this form are known as nonlinear least-squares problems. For the exponential regression problem above, we would take fi a, c  ≡ yi − ceaxi. 12.1.1 Gauss-Newton  When we run Newton’s method to minimize a function f   cid:126 x , we must know the gradient and Hessian of f . Knowing only the gradient of f is not enough, since approximating functions with planes provides no information about their extrema. The BFGS algorithm carries out optimization without Hessians, but its approximate Hessians depend on the sequence of iterations and hence are not local to the current iterate.  Contrastingly, the Gauss-Newton algorithm for nonlinear least-squares makes the obser- vation that approximating each fi with a linear function yields a nontrivial curved approx- imation of ENLS since each term in the sum is squared. The main feature of this approach is that it requires only ﬁrst-order approximation of the fi’s rather than Hessians.  Suppose we write  Then, we can approximate ENLS with E0  NLS given by  fi  cid:126 x  ≈ fi  cid:126 x0  + [∇fi  cid:126 x0 ] ·   cid:126 x −  cid:126 x0 .  Deﬁne F   cid:126 x  ≡  f1  cid:126 x , f2  cid:126 x , . . . , fk  cid:126 x   by stacking the fi’s into a column vector. Then,  E0  NLS  cid:126 x  =   fi  cid:126 x0  + [∇fi  cid:126 x0 ] ·   cid:126 x −  cid:126 x0  2 .  1  2 cid:88 i  E0  NLS  cid:126 x  =  1 2 cid:107 F   cid:126 x0  + DF   cid:126 x0   cid:126 x −  cid:126 x0  cid:107 2 2,  where DF is the Jacobian of F . Minimizing E0 −F   cid:126 x0  ≈ DF   cid:126 x0   cid:126 x −  cid:126 x0  that can be solved via the normal equations:  NLS  cid:126 x  is a linear least-squares problem   cid:126 x =  cid:126 x0 −  DF   cid:126 x0  cid:62 DF   cid:126 x0  −1DF   cid:126 x0  cid:62 F   cid:126 x0 .  More practically, as we have discussed, the system can be solved using the QR factorization of DF   cid:126 x0  or—in higher dimensions—using conjugate gradients and related methods.  We can view  cid:126 x from minimizing E0  NLS  cid:126 x  as an improved approximation of the minimum of ENLS  cid:126 x  starting from  cid:126 x0. The Gauss-Newton algorithm iterates this formula to solve nonlinear least-squares:   cid:126 xk+1 =  cid:126 xk −  DF   cid:126 xk  cid:62 DF   cid:126 xk  −1DF   cid:126 xk  cid:62 F   cid:126 xk .  This iteration is not guaranteed to converge in all situations. Given an initial guess suf- ﬁciently close to the minimum of the nonlinear least-squares problem, however, the ap- proximation above behaves similarly to Newton’s method and even can have quadratic convergence. Given the nature of the Gauss-Newton approximation, the algorithm works best when the optimal objective value ENLS  cid:126 x∗  is small; convergence can suﬀer when the optimal value is relatively large.   Specialized Optimization Methods  cid:4  229  12.1.2 Levenberg-Marquardt The Gauss-Newton algorithm uses an approximation E0 NLS  cid:126 x  of the nonlinear least-squares energy as a proxy for ENLS  cid:126 x  that is easier to minimize. In practice, this approximation is likely to fail as  cid:126 x moves farther from  cid:126 x0, so we might modify the Gauss-Newton step to include a step size limitation:  min cid:126 x E0  NLS  cid:126 x  subject to  cid:107  cid:126 x −  cid:126 x0 cid:107 2  2 ≤ ∆.  That is, we now restrict our change in  cid:126 x to have norm less than some user-provided value ∆; the ∆ neighborhood about  cid:126 x0 is called a trust region. Denote H ≡ DF   cid:126 x0  cid:62 DF   cid:126 x0  and δ cid:126 x ≡  cid:126 x −  cid:126 x0. Then, we can solve: minδ cid:126 x  2 δ cid:126 x cid:62 Hδ cid:126 x + F   cid:126 x0  cid:62 DF   cid:126 x0 δ cid:126 x  1  subject to  cid:107 δ cid:126 x cid:107 2  2 ≤ ∆.  That is, we displace  cid:126 x by minimizing the Gauss-Newton approximation after imposing the step size restriction. This problem has the following KKT conditions  see §10.2.2 :  Stationarity:  cid:126 0 = Hδ cid:126 x + DF   cid:126 x0  cid:62 F   cid:126 x0  + 2µδ cid:126 x  Primal feasibility:  cid:107 δ cid:126 x cid:107 2  2 ≤ ∆ Complementary slackness: µ ∆ −  cid:107 δ cid:126 x cid:107 2  2  = 0  Dual feasibility: µ ≥ 0.  Deﬁne λ ≡ 2µ. Then, the stationarity condition can be written as follows:   H + λIn×n δ cid:126 x = −DF   cid:126 x0  cid:62 F   cid:126 x0 . 2 ≤ ∆ is active, that is,  cid:107 δ cid:126 x cid:107 2  Assume the constraint  cid:107 δ cid:126 x cid:107 2 2 = ∆. Then, except in degenerate cases λ > 0; combining this inequality with the fact that H is positive semideﬁnite, H + λIn×n must be positive deﬁnite.  The Levenberg-Marquardt algorithm starts from this stationarity formula, taking the  following step derived from a user-supplied parameter λ > 0 [82, 85]:   cid:126 x =  cid:126 x0 −  DF   cid:126 x0  cid:62 DF   cid:126 x0  + λIn×n −1DF   cid:126 x0  cid:62 F   cid:126 x0 .  This linear system also can be derived by applying Tikhonov regularization to the Gauss- Newton linear system. When λ is small, it behaves similarly to the Gauss-Newton algorithm, while large λ results in a gradient descent step for ENLS.  Rather than specifying ∆ as introduced above, Levenberg-Marquardt steps ﬁx λ > 0 directly. By the KKT conditions, a posteriori we know this choice corresponds to having taken ∆ =  cid:107  cid:126 x− cid:126 x0 cid:107 2 2. As λ → ∞, the step from Levenberg-Marquardt satisﬁes  cid:107  cid:126 x− cid:126 x0 cid:107 2 → 0; so, we can regard ∆ and λ as approximately inversely proportional. Typical approaches adaptively adjust the damping parameter λ during each iteration:   cid:126 xk+1 =  cid:126 xk −  DF   cid:126 xk  cid:62 DF   cid:126 xk  + λkIn×n −1DF   cid:126 x0  cid:62 F   cid:126 xk .  For instance, we can scale up λk when the step in ENLS  cid:126 x  agrees well with the approximate value predicted by E0 NLS  cid:126 x , since this corresponds to increasing the size of the neighborhood in which the Gauss-Newton approximation is eﬀective.   230  cid:4  Numerical Algorithms 12.2 ITERATIVELY REWEIGHTED LEAST-SQUARES  Continuing in our consideration of least-squares problems, suppose we wish to minimize a function of the form:  EIRLS  cid:126 x  ≡ cid:88 i  fi  cid:126 x [gi  cid:126 x ]2.  We can think of fi  cid:126 x  as a weight on the least-squares term gi  cid:126 x .  Example 12.1  Lp optimization . Similar to the compressed sensing problems in §10.4.1, given A ∈ Rm×n and  cid:126 b ∈ Rm we can generalize least-squares by minimizing  Ep  cid:126 x  ≡  cid:107 A cid:126 x −  cid:126 b cid:107 p p.  Choosing p = 1 can promote sparsity in the residual  cid:126 b − A cid:126 x. We can write this function in an alternative form:  Ep  cid:126 x  = cid:88 i    cid:126 ai ·  cid:126 x − bi p−2  cid:126 ai ·  cid:126 x − bi 2.  Here, we denote the rows of A as  cid:126 a cid:62 i . Then, Ep = EIRLS after deﬁning:  fi  cid:126 x  =   cid:126 ai ·  cid:126 x − bi p−2 gi  cid:126 x  =  cid:126 ai ·  cid:126 x − bi.   cid:126 xk+1 = min  fi  cid:126 xk [gi  cid:126 xk+1 ]2.   cid:126 xk+1 cid:88 i  The iteratively reweighted least-squares  IRLS  algorithm makes use of the following  ﬁxed point iteration:  In the minimization,  cid:126 xk is ﬁxed, so the optimization is a least-squares problem over the gi’s. When gi is linear, the minimization can be carried out via linear least-squares; otherwise we can use the nonlinear least-squares techniques in §12.1. Example 12.2  L1 optimization . Continuing Example 12.1, suppose we take p = 1. Then,  E1  cid:126 x  = cid:88 i   cid:126 ai ·  cid:126 x − bi = cid:88 i  1   cid:126 ai ·  cid:126 x − bi    cid:126 ai ·  cid:126 x − bi 2.  This functional leads to the following IRLS iteration, after adjustment for numerical issues:  wi ← [max  cid:126 ai ·  cid:126 x − bi, δ ]−1  cid:126 x ← min cid:126 x cid:80 i wi  cid:126 ai ·  cid:126 x − bi 2   cid:46  Recompute weights  cid:46  Linear least-squares  The parameter δ > 0 avoids division by zero; large values of δ make better-conditioned linear systems but worse approximations of the original  cid:107  ·  cid:107 1 problem. Example 12.3  Weiszfeld algorithm . Recall the geometric median problem from Exam- ple 9.3. In this problem, given  cid:126 x1, . . . ,  cid:126 xk ∈ Rn, we wish to minimize  E  cid:126 x  ≡ cid:88 i   cid:107  cid:126 x −  cid:126 xi cid:107 2.   Specialized Optimization Methods  cid:4  231  Similar to the L1 problem in Example 12.2, we can write this function like a weighted least-squares problem:  E  cid:126 x  ≡ cid:88 i  1   cid:107  cid:126 x −  cid:126 xi cid:107 2 cid:107  cid:126 x −  cid:126 xi cid:107 2  2.  Then, IRLS provides the Weiszfeld algorithm for geometric median problems:  wi ← [max  cid:107  cid:126 x −  cid:126 xi cid:107 2, δ ]−1  cid:126 x ← min cid:126 x cid:80 i wi  cid:126 x −  cid:126 xi 2   cid:46  Recompute weights  cid:46  Linear least-squares  We can solve for the second step of the Weiszfeld algorithm in closed form. Diﬀerentiating the objective with respect to  cid:126 x shows   cid:126 0 = cid:88 i  2wi  cid:126 x −  cid:126 xi  =⇒  cid:126 x =  cid:80 i wi cid:126 xi cid:80 i wi  .  Thus, the two alternating steps of Weiszfeld’s algorithm can be carried out eﬃciently as:   cid:80  i wi cid:126 xi cid:80   wi ← [max  cid:107  cid:126 x −  cid:126 xi cid:107 2, δ ]−1  cid:126 x ←  i wi   cid:46  Recompute weights  cid:46  Weighted centroid  IRLS algorithms are straightforward to formulate, so they are worth trying if an opti- mization can be written in the form of EIRLS. When gi is linear for all i as in Example 12.2, each iteration of IRLS can be carried out quickly using Cholesky factorization, QR, conju- gate gradients, and so on, avoiding line search and other more generic strategies.  It is diﬃcult to formulate general conditions under which IRLS will reach the minimum of EIRLS. Often, iterates must be approximated somewhat as in the introduction of δ to Ex- ample 12.2 to avoid division by zero and other degeneracies. In the case of L1 optimization, however, IRLS can be shown with small modiﬁcation to converge to the optimal point [31].  12.3 COORDINATE DESCENT AND ALTERNATION Suppose we wish to minimize a function f : Rn+m → R. Rather than viewing the input as a single variable  cid:126 x ∈ Rn+m, we might write f in an alternative form as f   cid:126 x,  cid:126 y , for  cid:126 x ∈ Rn and  cid:126 y ∈ Rm. One strategy for optimization is to ﬁx  cid:126 y and minimize f with respect to  cid:126 x, ﬁx  cid:126 x and minimize f with respect to  cid:126 y, and repeat:  for i ← 1, 2, . . .   cid:126 xi+1 ← min cid:126 x f   cid:126 x,  cid:126 yi   cid:126 yi+1 ← min cid:126 y f   cid:126 xi+1,  cid:126 y    cid:46  Optimize  cid:126 x with  cid:126 y ﬁxed  cid:46  Optimize  cid:126 y with  cid:126 x ﬁxed  In this alternating approach, the value of f   cid:126 xi,  cid:126 yi  decreases monotonically as i increases since a minimization is carried out at each step. We cannot prove that alternation always reaches a global or even local minimum, but in many cases it can be an eﬃcient option for otherwise challenging problems.  12.3.1 Identifying Candidates for Alternation  There are a few reasons why we might wish to perform alternating optimization:   232  cid:4  Numerical Algorithms    The individual problems over  cid:126 x and  cid:126 y are optimizations in a lower dimension and may  converge more quickly.    We may be able to split the variables in such a way that the individual  cid:126 x and  cid:126 y steps  are far more eﬃcient than optimizing both variables jointly.  Below we provide a few examples of alternating optimization in practice. Example 12.4  Generalized PCA . In the PCA problem from §7.2.5, we are given a data matrix X ∈ Rn×k whose columns are k data points in Rn. We seek a basis in Rn of size d such that the projection of the data points onto the basis introduces minimal approximation error; we will store this basis in the columns of C ∈ Rn×d. Classical PCA Fro over both C and Y , where the columns of Y ∈ Rd×k are the minimizes  cid:107 X − CY  cid:107 2 coeﬃcients of the data points in the C basis. If C is constrained to be orthogonal, then Y = C cid:62 X, recovering the formula in our previous discussion.  The Frobenius norm in PCA is somewhat arbitrary: The relevant relationship is X − CY ≈ 0. Alternative PCA models minimize µ X − CY   over C and Y , for some other energy function µ : Rn×k → R favoring matrices with entries near zero; µ can provide enhanced robustness to noise or encode application-speciﬁc assumptions. Taking µ M   ≡  cid:107 M cid:107 2 Fro recovers classical PCA; another popular choice is robust PCA [71], which takes µ M   ≡ cid:80 ij Mij.  The product CY in µ X − CY   makes the energy nonlinear and nonconvex. A typical minimization routine for this problem uses alternation: First optimize C with Y ﬁxed, then optimize Y with C ﬁxed, and repeat. Whereas optimizing the energy with respect to C and Y jointly might require a generic large-scale method, the individual alternating C and Y steps can be easier:   When µ M   =  cid:107 M cid:107 2  Fro, both the Y and C alternations are least-squares problems,  leading to the alternating least-squares  ALS  algorithm for classical PCA.    When µ M   ≡ cid:80 ij Mij, the Y and C alternations are linear programs, which can  be optimized using the techniques mentioned in §10.4.1.  Example 12.5  ARAP . Recall the planar “as-rigid-as-possible”  ARAP  problem intro- duced in Example 10.5:  minimizeRv, cid:126 yv cid:88 v∈V  cid:88  v,w ∈E subject to R cid:62 v Rv = I2×2 ∀v ∈ V   cid:126 yv ﬁxed ∀v ∈ V0.   cid:107 Rv  cid:126 xv −  cid:126 xw  −   cid:126 yv −  cid:126 yw  cid:107 2  2  Solving for the matrices Rv ∈ R2×2 and vertex positions  cid:126 yv ∈ R2 simultaneously is a highly nonlinear and nonconvex task, especially given the orthogonality constraint R cid:62 v Rv = I2×2. There is one  cid:126 yv and one Rv for each vertex v of a triangle mesh with potentially thousands or even millions of vertices, so such a direct optimization using quasi-Newton methods requires a large-scale linear solve per iteration and still is prone to ﬁnding local minima.  Instead, [116] suggests alternating between the following two steps:  1. Fixing the Rv matrices and optimizing only for the positions  cid:126 yv:  minimize cid:126 yv cid:88 v∈V  cid:88  v,w ∈E  subject to  cid:126 yv ﬁxed ∀v ∈ V0.   cid:107 Rv  cid:126 xv −  cid:126 xw  −   cid:126 yv −  cid:126 yw  cid:107 2  2   Specialized Optimization Methods  cid:4  233  Figure 12.1 Coordinate descent in two dimensions alternates between minimizing in the horizontal and vertical axis directions.  This least-squares problem can be solved using a sparse, positive-deﬁnite linear sys- tem of equations.  2. Fixing the  cid:126 yv’s and optimizing for the Rv’s. No energy terms or constraints couple any pair Rv, Rw for v, w ∈ V , so we can solve for each matrix Rv independently. That is, rather than solving for 4V  unknowns simultaneously, we loop over v ∈ V , solving the following optimization for each Rv ∈ R2×2:  minimizeRv  cid:88  v,w ∈E  subject to R cid:62 v Rv = I2×2.   cid:107 Rv  cid:126 xv −  cid:126 xw  −   cid:126 yv −  cid:126 yw  cid:107 2  2  This optimization problem is an instance of the Procrustes problem from §7.2.4 and can be solved in closed-form using a 2 × 2 SVD. We have replaced a large-scale minimization with the application of a formula that can be evaluated in parallel for each vertex, a massive computational savings.  Alternating between optimizing for the  cid:126 yv’s with the Rv’s ﬁxed and vice versa decreases the energy using two eﬃcient pieces of machinery, sparse linear solvers and 2 × 2 SVD factorization. This can be far more eﬃcient than considering the  cid:126 yv’s and Rv’s simulta- neously, and in practice a few iterations can be suﬃcient to generate elastic deformations like the one shown in Figure 10.3. Extensions of ARAP even run in real time, optimizing fast enough to provide interactive feedback to artists editing two- and three-dimensional shapes.  Example 12.6  Coordinate descent . Taking the philosophy of alternating optimization to an extreme, rather than splitting the inputs of f : Rn → R into two variables, we could view f as a function of several variables f  x1, x2, . . . , xn . Then, we could cycle through each input xi, performing a one-dimensional optimization in each step. This lightweight algorithm, illustrated in Figure 12.1, is known as coordinate descent.  For instance, suppose we wish to solve the least-squares problem A cid:126 x ≈  cid:126 b by minimizing  cid:107 A cid:126 x −  cid:126 b cid:107 2 2. As in Chapter 11, line search over any single xi can be solved in closed form. If the columns of A are vectors  cid:126 a1, . . . ,  cid:126 an, then as shown in §1.3.1 we can write A cid:126 x − cid:126 b =   234  cid:4  Numerical Algorithms  Figure 12.2 The k-means algorithm seeks cluster centers  cid:126 yi that partition a set of data points  cid:126 x1, . . . ,  cid:126 xm based on their closest center.  x1 cid:126 a1 + ··· + xn cid:126 an −  cid:126 b. By this expansion,  ∂  0 =  ∂xi cid:107 x1 cid:126 a1 + ··· + xn cid:126 an −  cid:126 b cid:107 2  Solving this equation for xi yields the following coordinate descent update for xi:  ajiajkxk cid:33  − ajibj cid:35  .  2 = 2 A cid:126 x −  cid:126 b  ·  cid:126 ai = cid:88 j  cid:34  cid:32  cid:88 k  cid:126 ai ·  cid:126 b − cid:80 k cid:54 =i xk  cid:126 ai ·  cid:126 ak    cid:107  cid:126 ai cid:107 2  2  .  xi ←  Coordinate descent for least-squares iterates this formula over i = 1, 2, . . . , n repeatedly until convergence. This approach has eﬃcient localized updates and appears in machine learning methods where A has many more rows than columns, sampled from a data dis- tribution. We have traded a global method for one that locally updates the solution  cid:126 x by solving extremely simple subproblems.  Example 12.7  k-means clustering . Suppose we are given a set of data points  cid:126 x1, . . . ,  cid:126 xm ∈ Rn and wish to group these points into k clusters based on distance, as in Figure 12.2. Take  cid:126 y1, . . . ,  cid:126 yk ∈ Rn to be the centers of clusters 1, . . . , k, respectively. To cluster the data by assigning each point  cid:126 xi to a single cluster centered at  cid:126 yc, the k-means technique optimizes the following energy:  E  cid:126 y1, . . . ,  cid:126 yk  ≡  min  c∈{1,...,k} cid:107  cid:126 xi −  cid:126 yc cid:107 2 2.  m cid:88 i=1  In words, E measures the total squared distance of the data points  cid:126 xi to their closest cluster center  cid:126 yc.  Deﬁne ci ≡ arg minc∈{1,...,k}  cid:107  cid:126 xi −  cid:126 yc cid:107 2  2; that is, ci is the index of the cluster center  cid:126 yci closest to  cid:126 xi. Using this substitution, we can write an expanded formulation of the k-means objective as follows:  E  cid:126 y1, . . . ,  cid:126 yk; c1, . . . , cm  ≡   cid:107  cid:126 xi −  cid:126 yci cid:107 2 2.  m cid:88 i=1  The variables ci are integers, but we can optimize them jointly with the  cid:126 y’s using alterna- tion:   cid:31 y1   cid:31 y2   cid:31 y3   Specialized Optimization Methods  cid:4  235    When the ci’s are ﬁxed, the optimization for the  cid:126 yj’s is a least-squares problem whose  solution can be written in closed form as   cid:126 yj =  cid:80 ci=j  cid:126 xi  {ci = j}  .  That is,  cid:126 yj is the average of the points  cid:126 xi assigned to cluster j.    The optimization for ci also can be carried out in closed form using the expression 2 by iterating from 1 to k for each i. This iteration  ci ≡ arg minc∈{1,...,k}  cid:107  cid:126 xi −  cid:126 yc cid:107 2 just assigns each  cid:126 xi to its closest cluster center.  This alternation is known as the k-means algorithm and is a popular method for clustering. One drawback of this method is that it is sensitive to the initial guesses of  cid:126 y1, . . . ,  cid:126 yk. In practice, k-means is often run several times with diﬀerent initial guesses, and only the best output is preserved. Alternatively, methods like “k-means++” speciﬁcally design initial guesses of the  cid:126 yi’s to encourage convergence to a better local minimum [3].  12.3.2 Augmented Lagrangians and ADMM  Nonlinear constrained problems are often the most challenging optimization tasks. While the general algorithms in §10.3 are applicable, they can be sensitive to the initial guess of the minimizer, slow to iterate due to large linear solves, and slow to converge in the absence of more information about the problems at hand. Using these methods is easy from an engineering perspective since they require providing only a function and its derivatives, but with some additional work on paper, certain objective functions can be tackled us- ing faster techniques, many of which can be parallelized on multiprocessor machines. It is worth checking if a problem can be solved via one of these strategies, especially when the dimensionality is high or the objective has a number of similar or repeated terms.  In this section, we consider an alternating approach to equality-constrained optimization that has gained considerable attention in recent literature. While it can be used out-of-the- box as yet another generic optimization algorithm, its primary value appears to be in the decomposition of complex minimization problems into simpler steps that can be iterated, often in parallel. In large part we will follow the development of [14], which contains many examples of applications of this class of techniques.  As considered in Chapter 10, the equality-constrained optimization problem can be  stated as follows:  minimize f   cid:126 x  subject to g  cid:126 x  =  cid:126 0.  fρ  cid:126 x  = f   cid:126 x  +  1 2  ρ cid:107 g  cid:126 x  cid:107 2 2.  One incarnation of the barrier method suggested in §10.3.2 optimizes an unconstrained objective with a quadratic penalty:  As ρ → ∞, critical points of fρ satisfy the g  cid:126 x  =  cid:126 0 constraint more and more closely. The trade-oﬀ for this method, however, is that the optimization becomes poorly conditioned as   236  cid:4  Numerical Algorithms  Figure 12.3 We can optimize f  x, y  ≡ xy subject to x + y = 1 approximately by minimizing the penalized version fρ x, y  = xy + ρ x + y − 1 2. As ρ increases, however, level sets of xy get obscured in favor of enforcing the constraint.  ρ becomes large. This eﬀect is illustrated in Figure 12.3; when ρ is large, the level sets of fρ mostly are dedicated to enforcing the constraint rather than minimizing the objective f   cid:126 x , making it diﬃcult to distinguish between  cid:126 x’s that all satisfy the constraint.  Alternatively, by the method of Lagrange multipliers  Theorem 1.1 , we can seek ﬁrst-  order optima of this problem as the critical points of Λ  cid:126 x,  cid:126 λ  given by  Λ  cid:126 x,  cid:126 λ  ≡ f   cid:126 x  −  cid:126 λ cid:62 g  cid:126 x .  This Lagrangian does not suﬀer from conditioning issues that aﬀect the quadratic penalty method. On the other hand, it replaces a minimization problem—which can be solved by moving “downhill”—with a more challenging saddle point problem in which critical points should be minima of Λ with respect to  cid:126 x and maxima of Λ with respect to  cid:126 λ. Optimizing by alternatively minimizing with respect to  cid:126 x and maximizing with respect to  cid:126 λ can be unstable; intuitively this makes some sense since it is unclear whether Λ should be small or large.  The augmented Lagrangian method for equality-constrained optimization combines the quadratic penalty and Lagrangian strategies, using the penalty to “soften” individual iter- ations of the alternation for optimizing Λ described above. It replaces the original equality- constrained optimization problem with the following equivalent augmented problem:  minimize f   cid:126 x  +  1 2 subject to g  cid:126 x  =  cid:126 0.  ρ cid:107 g  cid:126 x  cid:107 2  2  Any  cid:126 x satisfying the g  cid:126 x  =  cid:126 0 constraint makes the second objective term vanish. But, when the constraint is not exactly satisﬁed, the second energy term biases the objective toward points  cid:126 x that approximately satisfy the equality constraint. In other words, during iterations of augmented Lagrangian optimization, the ρ cid:107 g  cid:126 x  cid:107 2 2 acts like a rubber band pulling  cid:126 x closer to the constraint set even during the minimization step.  This modiﬁed problem has a new Lagrangian given by  Λρ  cid:126 x,  cid:126 λ  ≡ f   cid:126 x  +  ρ cid:107 g  cid:126 x  cid:107 2  2 −  cid:126 λ cid:62 g  cid:126 x .  1 2  Hence, the augmented Lagrangian method optimizes this objective by alternating as follows:  for i ← 1, 2, . . .   cid:126 λi+1 ←  cid:126 λi − ρg  cid:126 xi   cid:126 xi ← min cid:126 x Λρ  cid:126 x,  cid:126 λi+1    cid:46  Dual update  cid:46  Primal update  ρ = 0  ρ = 0.01  ρ = 0.1  ρ = 1  ρ = 10   Specialized Optimization Methods  cid:4  237  The dual update step can be thought of as a gradient ascent step for  cid:126 λ. The parameter ρ here no longer has to approach inﬁnity for exact constraint satisfaction, since the Lagrange multiplier enforces the constraint regardless. Instead, the quadratic penalty serves to make sure the output of the  cid:126 x iteration does not violate the constraints too strongly.  Augmented Lagrangian optimization has the advantage that it alternates between ap- plying a formula to update  cid:126 λ and solving an unconstrained minimization problem for  cid:126 x. For many optimization problems, however, the unconstrained objective still may be non- diﬀerentiable or diﬃcult to optimize. A few special cases, e.g., Uzawa iteration for dual decomposition [124], can be eﬀective for optimization but in many circumstances quasi- Newton algorithms outperform this approach with respect to speed and convergence.  A small alteration to general augmented Lagrangian minimization, however, yields the alternating direction method of multipliers  ADMM  for optimizing slightly more speciﬁc objectives of the form  minimize f   cid:126 x  + h  cid:126 z  subject to A cid:126 x + B cid:126 z =  cid:126 c.  Here, the optimization variables are both  cid:126 x and  cid:126 z, where f, h : Rn → R are given functions and the equality constraint is linear. As we will show, this form encapsulates many important optimization problems. We will design an algorithm that carries out alternation between the two primal variables  cid:126 x and  cid:126 z, as well as between primal and dual optimization.  The augmented Lagrangian in this case is  Λρ  cid:126 x,  cid:126 z,  cid:126 λ  ≡ f   cid:126 x  + h  cid:126 z  +  ρ cid:107 A cid:126 x + B cid:126 z −  cid:126 c cid:107 2  2 +  cid:126 λ cid:62  A cid:126 x + B cid:126 z −  cid:126 c .  1 2  Alternating in three steps between optimizing  cid:126 x,  cid:126 z, and  cid:126 λ suggests a modiﬁcation of the augmented Lagrangian method:  for i ← 1, 2, . . .   cid:126 xi+1 ← arg min cid:126 x Λρ  cid:126 x,  cid:126 zi,  cid:126 λi   cid:126 zi+1 ← arg min cid:126 z Λρ  cid:126 xi+1,  cid:126 z,  cid:126 λi   cid:126 λi+1 ←  cid:126 λi + ρ A cid:126 xi+1 + B cid:126 zi+1 −  cid:126 c    cid:46   cid:126 x update  cid:46   cid:126 z update  cid:46  Dual update  In this algorithm,  cid:126 x and  cid:126 z are optimized one at a time; the augmented Lagrangian method would optimize them jointly. Although this splitting can require more iterations for con- vergence, clever choices of  cid:126 x and  cid:126 z lead to powerful division-of-labor strategies for breaking down diﬃcult problems. Each individual iteration will take far less time, even though more iterations may be needed for convergence. In a sense, ADMM is a “meta-algorithm” used to design optimization techniques. Rather than calling a generic package to minimize Λρ with respect to  cid:126 x and  cid:126 z, we will ﬁnd choices of  cid:126 x and  cid:126 z that make individual steps fast.  Before working out examples of ADMM in action, it is worth noting that it is guaranteed to converge to a critical point of the objective under fairly weak conditions. For instance, ADMM reaches a global minimum when f and h are convex and Λρ has a saddle point. ADMM has also been observed to converge even for nonconvex problems, although current theoretical understanding in this case is limited. In practice, ADMM tends to be quick to generate approximate minima of the objective but can require a long tail of iterations to squeeze out the last decimal points of accuracy; for this reason, some systems use ADMM to do initial large-scale steps and transition to other algorithms for localized optimization. We dedicate the remainder of this section to working out examples of ADMM in practice. The general pattern is to split the optimization variables into  cid:126 x and  cid:126 z in such a way that   238  cid:4  Numerical Algorithms  the two primal update steps each can be carried out eﬃciently, preferably in closed form or decoupling so that parallelized computations can be used to solve many subproblems at once. This makes individual iterations of ADMM inexpensive. Example 12.8  Nonnegative least-squares . Suppose we wish to minimize  cid:107 A cid:126 x− cid:126 b cid:107 2 2 with respect to  cid:126 x subject to the constraint  cid:126 x ≥  cid:126 0. The  cid:126 x ≥ 0 constraint rules out using Gaussian elimination, but ADMM provides one way to bypass this issue.  Consider solving the following equivalent problem:  Here, we deﬁne the new function h  cid:126 z  as follows:  minimize  cid:107 A cid:126 x −  cid:126 b cid:107 2 subject to  cid:126 x =  cid:126 z.  2 + h  cid:126 z   h  cid:126 z  = cid:26  0   cid:126 z ≥  cid:126 0  ∞ otherwise.  The function h  cid:126 z  is discontinuous, but it is convex. This equivalent form of nonnegative least-squares may be harder to read, but it provides an eﬀective ADMM splitting.  For this optimization, the augmented Lagrangian is  Λρ  cid:126 x,  cid:126 z,  cid:126 λ  =  cid:107 A cid:126 x −  cid:126 b cid:107 2  2 + h  cid:126 z  +  ρ cid:107  cid:126 x −  cid:126 z cid:107 2  2 +  cid:126 λ cid:62   cid:126 x −  cid:126 z .  1 2  For ﬁxed  cid:126 z with zi  cid:54 = ∞ for all i, then Λρ is diﬀerentiable with respect to  cid:126 x. Hence, we can carry out the  cid:126 x step of ADMM by setting the gradient with respect to  cid:126 x equal to  cid:126 0:   cid:126 0 = ∇ cid:126 xΛρ  cid:126 x,  cid:126 z,  cid:126 λ  = 2A cid:62 A cid:126 x − 2A cid:62  cid:126 b + ρ  cid:126 x −  cid:126 z  +  cid:126 λ =  2A cid:62 A + ρIn×n  cid:126 x +   cid:126 λ − 2A cid:62  cid:126 b − ρ cid:126 z  =⇒  cid:126 x =  2A cid:62 A + ρIn×n −1 2A cid:62  cid:126 b + ρ cid:126 z −  cid:126 λ .  This linear solve is a Tikhonov-regularized least-squares problem. For extra speed, the Cholesky factorization of 2A cid:62 A + ρIn×n can be computed before commencing ADMM and used to ﬁnd  cid:126 x in each iteration. Minimizing Λρ with respect to  cid:126 z can be carried out in closed form. Any objective function involving h eﬀectively constrains each component of  cid:126 z to be nonnegative, so we can ﬁnd  cid:126 z using the following optimization:  The  cid:107 A cid:126 x − cid:126 b cid:107 2 2 term in the full objective is removed because it has no  cid:126 z dependence. This problem decouples over the components of  cid:126 z since no energy terms involve more than one dimension of  cid:126 z at a time. So, we can solve many instances of the following one-dimensional problem:  1 2  minimize cid:126 z subject to  cid:126 z ≥  cid:126 0.  ρ cid:107  cid:126 x −  cid:126 z cid:107 2  2 +  cid:126 λ cid:62   cid:126 x −  cid:126 z   1 2  minimizezi subject to zi ≥ 0.  ρ xi − zi 2 + λi xi − zi   In the absence of the zi ≥ 0 constraint, the objective is minimized when 0 = ρ zi − xi  − λi =⇒ zi = xi + λi ρ; when this value is negative, we ﬁx zi = 0. Hence, the ADMM algorithm for nonnegative least-squares is:   Specialized Optimization Methods  cid:4  239  for i ← 1, 2, . . .   cid:126 xi+1 ←  2A cid:62 A + ρIn×n −1 2A cid:62  cid:126 b + ρ cid:126 zi −  cid:126 λi   cid:46   cid:126 x update; least-squares  cid:126 z0 ←  cid:126 λi ρ +  cid:126 xi+1  cid:46  Unconstrained  cid:126 z formula  cid:126 zi+1 ← Elementwise-Max  cid:126 z0, cid:126 0   cid:46  Enforce  cid:126 z ≥  cid:126 0  cid:126 λi+1 ←  cid:126 λi + ρ  cid:126 xi+1 −  cid:126 zi+1   cid:46  Dual update  This algorithm for nonnegative least-squares took our original problem—a quadratic pro- gram that could require diﬃcult constrained optimization techniques—and replaced it with an alternation between a linear solve for  cid:126 x, a formula for  cid:126 z, and a formula for  cid:126 λ. These individual steps are straightforward to implement and eﬃcient computationally.  Example 12.9  ADMM for geometric median . Returning to Example 12.3, we can recon- sider the energy E  cid:126 x  for the geometric median problem using the machinery of ADMM:  This time, we will split the problem into two unknowns  cid:126 zi,  cid:126 x:   cid:107  cid:126 x −  cid:126 xi cid:107 2.  E  cid:126 x  ≡  N cid:88 i=1 minimize  cid:88 i   cid:107  cid:126 zi cid:107 2  subject to  cid:126 zi +  cid:126 x =  cid:126 xi ∀i.  The augmented Lagrangian for this problem is:  Λρ = cid:88 i  cid:20  cid:107  cid:126 zi cid:107 2 +  1 2  ρ cid:107  cid:126 zi +  cid:126 x −  cid:126 xi cid:107 2  2 +  cid:126 λ cid:62 i   cid:126 zi +  cid:126 x −  cid:126 xi  cid:21  .  As a function of  cid:126 x, the augmented Lagrangian is diﬀerentiable and hence to ﬁnd the  cid:126 x iteration we write:   cid:126 0 = ∇ cid:126 xΛρ = cid:88 i  cid:104 ρ  cid:126 x −  cid:126 xi +  cid:126 zi  +  cid:126 λi cid:105  N  cid:88 i  cid:20  cid:126 xi −  cid:126 zi −   cid:126 λi cid:21  .  1 ρ  1  =⇒  cid:126 x =  The optimization for the  cid:126 zi’s decouples over i when  cid:126 x is ﬁxed, so after removing constant 2 +  cid:126 λ cid:62 i  cid:126 zi for each  cid:126 zi separately. We can combine terms we minimize  cid:107  cid:126 zi cid:107 2 + 1 the second and third terms by “completing the square” as follows:  2 ρ cid:107  cid:126 zi +  cid:126 x−  cid:126 xi cid:107 2  1 2  ρ cid:107  cid:126 zi +  cid:126 x −  cid:126 xi cid:107 2  2 +  cid:126 λ cid:62 i  cid:126 zi =   cid:126 λi +  cid:126 x −  cid:126 xi cid:19  + const.  + const.  ρ  =  1 2  1 2  1 ρ  ρ cid:107  cid:126 zi cid:107 2  2 + ρ cid:126 z cid:62 i  cid:18  1 ρ cid:13  cid:13  cid:13  cid:13  cid:126 zi +  cid:126 λi +  cid:126 x −  cid:126 xi cid:13  cid:13  cid:13  cid:13  2 cid:21  .  cid:126 zi  cid:20  cid:107  cid:126 zi cid:107 2 + ρ cid:107  cid:126 zi −  cid:126 z0 cid:107 2  1 2  2  2  min  The constant terms can have  cid:126 x dependence since it is ﬁxed in the  cid:126 zi iteration. Deﬁning  cid:126 z0 ≡ − 1   cid:126 λi −  cid:126 x +  cid:126 xi, in the  cid:126 zi iteration we have shown that we can solve:  ρ   240  cid:4  Numerical Algorithms  Written in this form, it is clear that the optimal  cid:126 zi satisﬁes  cid:126 zi = t cid:126 z0 for some t ∈ [0, 1], since the two terms of the objective balance the distance of  cid:126 zi to  cid:126 0 and to  cid:126 z0. After dividing by  cid:107  cid:126 z0 cid:107 2, we can solve:  Using elementary calculus techniques we ﬁnd:  t≥0 cid:20 t +  min  1 2  ρ cid:107  cid:126 z0 cid:107 2 t − 1 2 cid:21  .  t = cid:26  1 − 1 ρ cid:107  cid:126 z0 cid:107 2 when ρ cid:107  cid:126 z0 cid:107 2 ≥ 1  otherwise.  0  Taking  cid:126 zi = t cid:126 z0 ﬁnishes the  cid:126 z iteration of ADMM.  In summary, the ADMM algorithm for geometric medians is as follows:  for i ← 1, 2, . . .  ρ   cid:126 λi cid:105   N cid:80 i cid:104  cid:126 xi −  cid:126 zi − 1  cid:126 x ← 1 for j ← 1, 2, . . . , N  cid:126 z0 ← − 1 t ← cid:26  1 − 1 ρ cid:107  cid:126 z0 cid:107 2 when ρ cid:107  cid:126 z0 cid:107 2 ≥ 1  cid:126 zj ← t cid:126 z0  cid:126 λj ←  cid:126 λj + ρ  cid:126 zi +  cid:126 x −  cid:126 xi    cid:126 λi −  cid:126 x +  cid:126 xi 0  otherwise  ρ   cid:46   cid:126 x update  cid:46  Can parallelize   cid:46   cid:126 z update  cid:46  Dual update  The examples above show the typical ADMM strategy, in which a diﬃcult nonlinear problem is split into two subproblems that can be carried out in closed form or via more eﬃcient operations. The art of posing a problem in terms of  cid:126 x and  cid:126 z to get these savings requires practice and careful study of individual problems.  The parameter ρ > 0 often does not aﬀect whether or not ADMM will eventually converge, but an intelligent choice of ρ can help this technique reach the optimal point faster. Some experimentation can be required, or ρ can be adjusted from iteration to iteration depending on whether the primal or dual variables are converging more quickly [127]. In some cases, ADMM provably converges faster when ρ → ∞ as the iterations proceed [104]. 12.4 GLOBAL OPTIMIZATION  Nonlinear least-squares, IRLS, and alternation are lightweight approaches for nonlinear ob- jectives that can be optimized quickly after simpliﬁcation. On the other side of the spectrum, some minimization problems not only do not readily admit fast specialized algorithms but also are failure modes for Newton’s method and other generic solvers. Convergence guaran- tees for Newton’s method and other algorithms based on the Taylor approximation assume that we have a strong initial guess of the minimum that we wish to reﬁne. When we lack such an initial guess or a simplifying assumption like convexity, we must solve a global optimization problem searching over the entire space of feasible output.  As discussed brieﬂy in §9.2, global optimization is a challenging, nearly ill-posed problem. For example, in the unconstrained case it is diﬃcult to know whether  cid:126 x∗ yields the minimum possible f   cid:126 x  anywhere, since this is a statement over an inﬁnitude of points  cid:126 x. Hence, global optimization methods use one or more strategies to improve the odds of ﬁnding a minimum:    Initially approximate the objective f   cid:126 x  with an easier function to minimize to get a  better starting point for the original problem.   Specialized Optimization Methods  cid:4  241  Figure 12.4 Newton’s method can get caught in any number of local minima in the function on the left; smoothing this function, however, can generate a stronger initial guess of the global optimum.    Sample the space of possible inputs  cid:126 x to get a better idea of the behavior of f over a  large domain.  These and other strategies are heuristic, meaning that they usually cannot be used to guarantee that the output of such a minimization is globally optimal. In this section, we mention a few common techniques for global optimization as pointers to more specialized literature.  12.4.1 Graduated Optimization  Consider the optimization objective illustrated in Figure 12.4. Locally, this objective wiggles up and down, but at a larger scale, a more global pattern emerges. Newton’s method seeks any critical point of f  x  and easily can get caught in one of its local minima. To avoid this suboptimal output, we might attempt to minimize a smoothed version of f  x  to generate an initial guess for the minimum of the more involved optimization problem.  Graduated optimization techniques solve progressively harder optimization problems with the hope that the coarse initial iterations will generate better initial guesses for the more accurate but sensitive later steps. In particular, suppose we wish to minimize some function f   cid:126 x  over  cid:126 x ∈ Rn with many local optima as in Figure 12.4. Graduated methods generate a sequence of functions f1  cid:126 x , f2  cid:126 x , . . . , fk  cid:126 x  with fk  cid:126 x  = f   cid:126 x , using critical points of fi as initial guesses for minima of fi+1.  Example 12.10  Image alignment . A common task making use of graduated opti- mization is photograph alignment as introduced in §4.1.4. Consider the images in Fig- ure 12.5. Aligning the original two images can be challenging because they have lots of high-frequency detail; for instance, the stones on the wall all look similar and easily could be misidentiﬁed. By blurring the input images, a better initial guess of the alignment can be obtained, because high-frequency details are suppressed.  The art of graduated optimization lies in ﬁnding an appropriate sequence of fi’s to help reach a global optimum. In signal and image processing, like in Example 12.10, a typical approach is to use the same optimization objective in each iteration but blur the underlying data to reveal larger-scale patterns. Scale space methods like [81] blur the objective itself,   242  cid:4  Numerical Algorithms  Figure 12.5 The photos on the left can be hard to align using automatic methods because they have lots of high-frequency detail that can obscure larger alignment patterns; by blurring the photos we can align larger features before reﬁning the alignment using texture and other detail.  for instance by deﬁning fi to be f   cid:126 x  ∗ gσi   cid:126 x , the result of blurring f   cid:126 x  using a Gaussian of width σi, with σi → 0 as i → ∞. A related set of algorithms known as homotopy continuation methods continuously changes the optimization objective by leveraging intuition from topology. These algorithms make use of the following notion from classical mathematics:  Deﬁnition 12.1  Homotopic functions . Two continuous functions f   cid:126 x  and g  cid:126 x  are homotopic if there exists continuous function H  cid:126 x, s  with  H  cid:126 x, 0  = f   cid:126 x   and H  cid:126 x, 1  = g  cid:126 x   for all  cid:126 x.  The idea of homotopy is illustrated in Figure 12.6.  Similar to graduated methods, homotopy optimizations minimize f   cid:126 x  by deﬁning a new function H  cid:126 x, s  where H  cid:126 x, 0  is easy to optimize and H  cid:126 x, 1  = f   cid:126 x . Taking  cid:126 x∗0 to be the minimum of H  cid:126 x, 0  with respect to  cid:126 x, basic homotopy methods incrementally increase s, each time updating to a new  cid:126 x∗s. Assuming H is continuous, we expect the minimum  cid:126 x∗s to trace a continuous path in Rn as s increases; hence, the solve for each  cid:126 x∗s after increasing s diﬀerentially has a strong initial guess from the previous iteration.  Example 12.11  Homotopy methods, [45] . Homotopy methods also apply to root- ﬁnding. As a small example, suppose we wish to ﬁnd points x satisfying arctan x  = 0. Applying the formula from §8.1.4, Newton’s method for ﬁnding such a root iterates  xk+1 = xk −  1 + x2  k  arctan x .  If we provide an initial guess x0 = 4, however, this iteration diverges. Instead, we can deﬁne a homotopy function as  H x, s  ≡ arctan x  +  s − 1  arctan 4 .  We know H x, 0  = arctan x  − arctan 4  has a root at the initial guess x0 = 4. Stepping s by increments of 1 10 from 0 to 1, each time minimizing H x, si  with initial guess x∗i−1 via Newton’s method yields a sequence of convergent problems reaching x∗ = 0.  lanigirO  derrulB   Specialized Optimization Methods  cid:4  243  Figure 12.6 The curves γ0 t  and γ1 t  are homotopic because there exists a contin- uously varying set of curves γs t  for s ∈ [0, 1] coinciding with γ0 at s = 0 and γ1 at s = 1.  More generally, we can think of a solution path as a curve of points   cid:126 x t , s t   such that s 0  = 0, s 1  = 1, and at each time t,  cid:126 x t  is a local minimizer of H  cid:126 x, s t   over  cid:126 x. Our initial description of homotopy optimization would take s t  = t, but now we can allow s t  to be non-monotonic as a function of t as long as it eventually reaches s = 1. Advanced homotopy continuation methods view   cid:126 x t , s t   as a curve satisfying certain ordinary diﬀerential equations, which you will derive in Exercise 12.6; these equations can be solved using the techniques we will deﬁne in Chapter 15.  12.4.2 Randomized Global Optimization  When smoothing the objective function is impractical or fails to remove local minima from f   cid:126 x , it makes sense to sample the space of possible inputs  cid:126 x to get some idea of the energy landscape. Newton’s method, gradient descent, and others all have strong dependence on the initial guess of the location of the minimum, so trying more than one starting point increases the chances of success.  If the objective f is suﬃciently noisy, we may wish to remove dependence on diﬀerential estimates altogether. Without gradients, we do not know which directions locally point downhill, but via sampling we can ﬁnd such patterns on a larger scale. Heuristics for global optimization at this scale commonly draw inspiration from the natural world and the idea of swarm intelligence, that complex natural processes can arise from individual actors following simple rules, often in the presence of stochasticity, or randomness. For instance, optimization routines have been designed to mimic ant colonies transporting food [26], thermodynamic energy in “annealing” processes [73], and evolution of DNA and genetic material [87]. These methods usually are considered heuristics without convergence guarantees but can help guide a large-scale search for optima.  As one example of a method well-tuned to continuous problems, we consider the particle swarm method introduced in [72] as an optimization technique inspired by social behavior in bird ﬂocks and ﬁsh schools. Many variations of this technique have been proposed, but we explore one of the original versions introduced in [36].  t = 0  +s  +t  γ1 t   γs t   γ0 t   s = 1  t = 1  s = 0   244  cid:4  Numerical Algorithms  Suppose we have a set of candidate minima  cid:126 x1, . . . ,  cid:126 xk. We will think of these points as particles moving around the possible space of  cid:126 x values, and hence they will also be assigned velocities  cid:126 v1, . . . ,  cid:126 vk. The particle swarm method maintains a few additional variables:     cid:126 p1, . . . ,  cid:126 pk, the position over all iterations so far of the lowest value f   cid:126 pi  observed by  each particle i.    The position  cid:126 g ∈ { cid:126 p1, . . . ,  cid:126 pk} with the smallest objective value; this position is the  globally best solution observed so far.  This notation is illustrated in Figure 12.7.  In each iteration of particle swarm optimization, the velocities of the particles are up- dated to guide them toward likely minima. Each particle is attracted to its own best observed minimum as well as to the global best position so far:   cid:126 vi ←  cid:126 vi + α  cid:126 pi −  cid:126 xi  + β  cid:126 g −  cid:126 xi .  The parameters α, β ≥ 0 determine the amount of force felt from  cid:126 xi to move toward these two positions; larger α, β values will push particles toward minima faster at the cost of more limited exploration of the space of possible minima. Once velocities have been updated, the particles move along their velocity vectors:  Then, the process repeats. This algorithm is not guaranteed to converge, but it can be termi- nated at any point, with  cid:126 g as the best observed minimum. The ﬁnal method is documented in Figure 12.8.   cid:126 xi ←  cid:126 xi +  cid:126 vi.  12.5 ONLINE OPTIMIZATION  We brieﬂy consider a class of optimization problems from machine learning, game theory, and related ﬁelds in which the objective itself is allowed to change from iteration to iteration. These problems, known as online optimization problems, reﬂect a world in which evolving input parameters, priorities, and desired outcomes can make the output of an optimization irrelevant soon after it is generated. Hence, techniques in this domain must adaptively react to the changing objective in the presence of noise. Our discussion will introduce a few basic ideas from [107]; we refer the reader to that survey article for a more detailed treatment.  Example 12.12  Stock market . Suppose we run a ﬁnancial institution and wish to main- tain an optimal portfolio of investments. On the morning of day t, in a highly simpliﬁed model we might choose how much of each stock 1, . . . , n to buy, represented by a vector  cid:126 xt ∈  R+ n. At the end of the day, based on ﬂuctuations of the market, we will know a function ft so that ft  cid:126 x  gives us our total proﬁt or loss based on the decision  cid:126 x made in the morning. The function ft can be diﬀerent every day, so we must attempt to design a policy that predicts the objective function and or its optimal point every day.  Problems in this class often can be formalized as online convex optimization problems. In the unconstrained case, online convex optimization algorithms are designed for the following feedback loop:  for t = 1, 2, . . .   cid:46  At each time t   cid:46  Predict  cid:126 xt ∈ U  cid:46  Receive loss function ft : U → R  cid:46  Suﬀer loss ft  cid:126 xt    Specialized Optimization Methods  cid:4  245  Figure 12.7 The particle swarm navigates the landscape of f   cid:126 x  by maintaining po- sitions and velocities for a set of potential minima  cid:126 xi; each  cid:126 xi is attracted to the position  cid:126 pi at which it has observed the smallest value of f   cid:126 xi  as well as to the minimum  cid:126 g observed thus far by any particle.  Figure 12.8 Particle swarm optimization attempts to minimize f   cid:126 x  by simulating a collection of particles  cid:126 x1, . . . ,  cid:126 xk moving in the space of potential inputs  cid:126 x.   cid:2 pi   cid:2 g   cid:2 xi   cid:2 vi  function Particle-Swarm f   cid:2 x , k, α, β,  cid:2 xmin,  cid:2 xmax,  cid:2 vmin,  cid:2 vmax   fmin ← ∞ for i ← 1, 2, . . . , k   cid:2 xi ← Random-Position  cid:2 xmin,  cid:2 xmax   cid:2 vi ← Random-Velocity  cid:2 vmin,  cid:2 vmax  fi ← f   cid:2 xi  i ←  cid:2 xi  cid:2 p if fi < fmin then fmin ← fi  cid:2 g ←  cid:2 xi for j ← 1, 2  ...,  for i ← 1, 2, . . . , k   cid:2 vi ←  cid:2 vi + α  cid:2 pi −  cid:2 xi  + β  cid:2 g −  cid:2 xi   cid:2 xi ←  cid:2 xi +  cid:2 vi  for i ← 1, 2, . . . , k  if f   cid:2 xi  < fi then  i ←  cid:2 xi  cid:2 p fi ← f   cid:2 xi  if fi < fmin then fmin ← fi  cid:2 g ←  cid:2 xi   cid:5  Initialize positions randomly  cid:5  Initialize velocities randomly  cid:5  Evaluate f  cid:5  Current particle optimum  cid:5  Check if it is global optimum  cid:5  Update optimal value  cid:5  Set global optimum Stop when satisﬁed with  cid:2 g   cid:5    cid:5  Update velocity  cid:5  Update position   cid:5  Better minimum for particle i  cid:5  Update particle optimum  cid:5  Store objective value  cid:5  Check if it is a global optimum  cid:5  Update optimal value  cid:5  Global optimum   246  cid:4  Numerical Algorithms  We will assume the ft’s are convex and that U ⊆ Rn is a convex set. There are a few features of this setup worth highlighting:    To stay consistent with our discussion of optimization in previous chapters, we phrase  the problem as minimizing loss rather than, e.g., maximizing proﬁt.    The optimization objective can change at each time t, and we do not get to know the objective ft before choosing  cid:126 xt. In the stock market example, this feature reﬂects the fact that we do not know the price of a stock on day t until the day is over, and we must decide how much to buy before getting to that point.    The online convex optimization algorithm can choose to store f1, . . . , ft−1 to inform its choice of  cid:126 xt. For stock investment, we can use the stock prices on previous days to predict them for the future.  Since online convex optimization algorithms do not know ft before predicting  cid:126 xt, we cannot expect them to perform perfectly. An “adversarial” client might wait for  cid:126 xt and purposefully choose a loss function ft to make  cid:126 xt look bad! For this reason, metrics like t=1 ft  cid:126 xt  are unfair measures for the quality of an online optimization  method at time T . In some sense, we must lower our standards for success.  cumulative loss  cid:80 T  One model for online convex optimization is minimization of regret, which compares  performance to that of a ﬁxed expert beneﬁting from hindsight:  Deﬁnition 12.2  Regret . The regret of an online optimization algorithm at time T over a set U is given by  RT ≡ max   cid:126 u∈U cid:34  T cid:88 t=1   ft  cid:126 xt  − ft  cid:126 u   cid:35  .  The regret RT measures the diﬀerence between how well our algorithm has performed over time—as measured by summing ft  cid:126 xt  over t—and the performance of any constant point  cid:126 u that must remain the same over all t. For the stock example, regret compares the proﬁts lost by using our algorithm and the loss of using any single stock portfolio over all time. Ideally, the ratio RT T measuring average regret over time should decrease as T → ∞. strategy, which chooses  cid:126 xt based on how it would have performed at times 1, . . . , t − 1 :  The most obvious approach to online optimization is the “follow the leader”  FTL   Follow the leader:  cid:126 xt ≡ arg min  fs  cid:126 x .  t−1 cid:88 s=1   cid:126 x∈U  FTL is a reasonable heuristic if we assume past performance has some bearing on future results. After all, if we do not know ft we might as well hope that it is similar to the objectives f1, . . . , ft−1 we have observed in the past.  For many classes of functions ft, FTL is an eﬀective approach that makes increasingly well-informed choices of  cid:126 xt as t progresses. It can experience some serious drawbacks, how- ever, as illustrated in the following example:  Example 12.13  Failure of FTL, [107] §2.2 . Suppose U = [0, 1] and we generate a sequence of functions as follows:  ft x  =  −x 2 x −x  if t = 1 if t is even otherwise.   Specialized Optimization Methods  cid:4  247  FTL minimizes the sum over all previous objective functions, giving the following series of outputs:  t = 1 : t = 2 : t = 3 : t = 4 : t = 5 :  ...  x arbitrary ∈ [0, 1] x2 = arg minx∈[0,1] −x 2 = 1 x 2 = 0 x3 = arg minx∈[0,1] x4 = arg minx∈[0,1] −x 2 = 1 x5 = arg minx∈[0,1] x 2 = 0  ...  From the above calculation, we ﬁnd that in every iteration except t = 1, FTL incurs loss 1, while ﬁxing x = 0 for all time would incur zero loss. For this example, FTL has regret growing proportionally to t.  This example illustrates the type of analysis and reasoning typically needed to design online learning methods. To bound regret, we must consider the worst possible adversary, who generates functions ft speciﬁcally designed to take advantage of the weaknesses of a given technique.  FTL failed because it was too strongly sensitive to the ﬂuctuations of ft from iteration to iteration. To resolve this issue, we can take inspiration from Tikhonov regularization  §4.1.3 , L1 regularization  §10.4.1 , and other methods that dampen the output of numer- ical methods by adding an energy term punishing irregular or large output vectors. To do so, we deﬁne the “follow the regularized leader”  FTRL  strategy:  Follow the regularized leader:  cid:126 xt ≡ arg min   cid:126 x∈U  cid:34 r  cid:126 x  +  fs  cid:126 x  cid:35  .  t−1 cid:88 s=1  Here, r  cid:126 x  is a convex regularization function, such as  cid:107  cid:126 x cid:107 2   L1 regularization , or cid:80 i xi log xi when U includes only  cid:126 x ≥  cid:126 0  entropic regularization .  2  Tikhonov regularization ,  cid:107  cid:126 x cid:107 1 Just as regularization improves the conditioning of a linear problem when it is close to singular, in this case the change from FTL to FTRL avoids ﬂuctuation issues illustrated in Example 12.13. For instance, suppose r  cid:126 x  is strongly convex as deﬁned below for diﬀeren- tiable r:  Deﬁnition 12.3  Strongly convex . A diﬀerentiable regularizer r  cid:126 x  is σ-strongly convex with respect to a norm  cid:107  ·  cid:107  if for any  cid:126 x,  cid:126 y the following relationship holds:   ∇r  cid:126 x  − ∇r  cid:126 y   ·   cid:126 x −  cid:126 y  ≥ σ cid:107  cid:126 x −  cid:126 y cid:107 2 2.  Intuitively, a strongly convex regularizer not only is bowl-shaped but has a lower bound for the curvature of that bowl. Then, we can prove the following statement:  Proposition 12.1  [107], Theorem 2.11 . Assume r  cid:126 x  is σ-strongly convex and that each ft is convex and L-Lipschitz  see §8.1.1 . Then, the regret is bounded as follows:  RT ≤ cid:20 max   cid:126 u∈U  r  cid:126 u  cid:21  − cid:20 min   cid:126 v∈U  r  cid:126 v  cid:21  +  T L2  .  σ  The proof of this proposition uses techniques well within the scope of this book but due to its length is omitted from our discussion.  Proposition 12.1 can be somewhat hard to interpret, but it is a strong result about the eﬀectiveness of the FTRL technique given an appropriate choice of r. In particular, the max   248  cid:4  Numerical Algorithms  and min terms as well as σ are properties of r  cid:126 x  that should guide which regularizer to use for a particular problem. The value σ contributes to both terms in competing ways:    The diﬀerence between the maximum and minimum values of r is its range of possible outputs. Increasing σ has the potential to increase this diﬀerence, since it is bounded below by a “steeper” bowl. So, minimizing this term in our regret bound prefers small σ.    Minimizing T L2 σ prefers large σ.  Practically speaking, we can decide what range of T we care about and choose a regularizer accordingly: Example 12.14  FTRL choice of regularizers . Consider the regularizer rσ  cid:126 x  ≡ 1 2 σ cid:107  cid:126 x cid:107 2 2. It has gradient ∇rσ  cid:126 x  = σ cid:126 x, so by direct application of Deﬁnition 12.3, it is σ-strongly convex. Suppose U = { cid:126 x ∈ Rn :  cid:107  cid:126 x cid:107 2 ≤ 1} and that we expect to run our optimization for T time steps. If we take σ = √T , then the regret bound from Proposition 12.1 shows:  RT ≤  1 + L2 √T .  For large T , this value is small relative to T , compared to the linear growth for FTL in Example 12.13.  Online optimization is a rich area of research that continues to be explored. Beyond FTRL, we can deﬁne algorithms with better or more usable regret bounds, especially if we know more about the class of functions ft we expect to observe. FTRL also has the drawback that it has to solve a potentially complex optimization problem at each iteration, which may not be practical for systems that have to make decisions quickly. Surprisingly, even easy-to-solve linearizations can behave fairly well for convex objectives, as illustrated in Exercise 12.14. Popular online optimization techniques like [34] have been applied to a variety of learning problems in the presence of huge amounts of noisy data.  12.6 EXERCISES  12.1 An alternative derivation of the Gauss-Newton algorithm shows that it can be thought  of as an approximation of Newton’s method for unconstrained optimization.   a  Write an expression for the Hessian of ENLS  cid:126 x   deﬁned in §12.1  in terms of the  derivatives of the fi’s.   b  Show that the Gauss-Newton algorithm on ENLS is equivalent to Newton’s  method  §9.4.2  after removing second derivative terms from the Hessian.   c  When is such an approximation of the Hessian reasonable?  12.2 Motivate the Levenberg-Marquardt algorithm by applying Tikhonov regularization to  the Gauss-Newton algorithm.  12.3 Derive steps of an alternating least-squares  ALS  iterative algorithm for minimizing  cid:107 X−CY  cid:107 Fro with respect to C ∈ Rn×d and Y ∈ Rd×k, given a ﬁxed matrix X ∈ Rn×k. Explain how the output of your algorithm depends on the initial guesses of C and Y . Provide an extension of your algorithm that orthogonalizes the columns of C in each iteration using its reduced QR factorization, and argue why the energy still decreases in each iteration.   Specialized Optimization Methods  cid:4  249  12.4 Incorporate matrix factorization into the nonnegative least-squares algorithm in Ex- ample 12.8 to make the  cid:126 x step more eﬃcient. When do you expect this modiﬁcation to improve the speed of the algorithm?  12.5 For a ﬁxed parameter δ > 0, the Huber loss function Lδ x  is deﬁned as:  Lδ x  ≡ cid:26   x2 2, when x ≤ δ  otherwise.  δ x − δ 2 ,  Illustrate the eﬀect of choosing diﬀerent values of δ on the shape of Lδ x .  This function “softens” the non-diﬀerentiable singularity of x at x = 0.  a   b  Recall that we can ﬁnd an  cid:126 x nearly satisfying the overdetermined system A cid:126 x ≈  cid:126 b by minimizing  cid:107 A cid:126 x −  cid:126 b cid:107 2  least-squares  or  cid:107 A cid:126 x −  cid:126 b cid:107 1  compressive sensing . Propose a similar optimization compromising between these two methods using Lδ.   c  Propose an IRLS algorithm for optimizing your objective from Exercise 12.5b.  You can assume A cid:62 A is invertible.   d  Propose an ADMM algorithm for optimizing your objective from Exercise 12.5b.  Again, assume A cid:62 A is invertible. Hint: Introduce a variable  cid:126 z = A cid:126 x −  cid:126 b.  DH 12.6  From notes by P. Blomgren  In §12.4.1, we introduced homotopy continuation methods for optimization. These methods begin by minimizing a simple objective H  cid:126 x, 0  = f0  cid:126 x  and then smoothly modify the objective and minimizer simultane- ously until a minimum of H  cid:126 x, 1  = f   cid:126 x —the original objective—is found. Suppose that s t  is a function of t ≥ 0 such that s 0  = 0; we will assume that s t  ≥ 0 for all t ≥ 0 and that s t  eventually reaches s t  = 1. Our goal is to produce a path  cid:126 x t  such that each  cid:126 x t  minimizes H  cid:126 x, s t   with respect to  cid:126 x.   a  To maintain optimality of  cid:126 x t , what relationship does ∇ cid:126 xH  cid:126 x, s  satisfy for all  t ≥ 0 at points   cid:126 x t , s t   on the solution path?   b  Diﬀerentiate this equation with respect to t. Write one side as a matrix-vector  product.   c  Provide a geometric interpretation of the vector  cid:126 g t  ≡   cid:126 x cid:48  t , s cid:48  t   in terms of  the solution path   cid:126 x t , s t  .   d  We will impose the restriction that  cid:126 g t 2 = 1 ∀t ≥ 0, i.e., that  cid:126 g t  has unit length. In this case, what is the geometric interpretation of t in terms of the solution path?   e  Combine Exercises 12.6b and 12.6d to propose an ordinary diﬀerential equation  ODE  for computing   cid:126 x cid:48  t , s cid:48  t   from   cid:126 x t , s t  , so that the resulting solution path maintains our design constraints. Note: Using this formula, numerical ODE solvers like the ones we will propose in Chapter 15 can calculate a solution path for homotopy continuation optimiza- tion. This derivation provides a connection between topology, optimization, and diﬀerential equations.   250  cid:4  Numerical Algorithms  12.7  “Least absolute deviations”  Instead of solving least-squares, to take advantage of methods from compressive sensing we might wish to minimize  cid:107 A cid:126 x −  cid:126 b cid:107 1 with  cid:126 x unconstrained. Propose an ADMM-style splitting of this optimization and give the alternating steps of the optimization technique in this case.  DH 12.8 Suppose we have two convex sets S, T ⊆ Rn. The alternating projection method discussed in [9] and elsewhere is used to ﬁnd a point  cid:126 x ∈ S ∩ T . For any initial guess  cid:126 x0, alternating projection performs the iteration   cid:126 xk+1 = PS  PT   cid:126 xk   ,  where PS and PT are operators that project onto the nearest point in S or T with respect to  cid:107 · cid:107 2, respectively. As long as S∩T  cid:54 = ∅, this iterative procedure is guaranteed to converge to an  cid:126 x ∈ S ∩ T , though this convergence may be impractically slow [23]. Instead of this algorithm, we will consider ﬁnding a point in the intersection of convex sets using ADMM.   a  Propose an unconstrained optimization problem whose solution is a point  cid:126 x ∈  S ∩ T , assuming S ∩ T  cid:54 = ∅. Hint: Use indicator functions.   b  Write this problem in a form that is amenable to ADMM, using  cid:126 x and  cid:126 z as your  variables.   c  Explicitly write the ADMM iterations for updating  cid:126 x,  cid:126 z, and any dual variables.  Your expressions can use PS and PT .  DH 12.9 A popular technique for global optimization is simulated annealing [73], a method motivated by ideas from statistical physics. The term annealing refers to the process in metallurgy whereby a metal is heated and then cooled so its constituent particles arrange in a minimum energy state. In this thermodynamic process, atoms may move considerably at higher temperatures but become restricted in motion as the temper- ature decreases. Borrowing from this analogy in the context of global optimization, we could let a potential optimal point take large, random steps early on in a search to explore the space of outputs, eventually taking smaller steps as the number of iterations gets large, to obtain a more reﬁned output. Pseudocode for the resulting simulated annealing algorithm is provided in the following box.  function Simulated-Annealing f   cid:126 x ,  cid:126 x0   T0 ← High temperature Ti ← Cooling schedule, e.g., Ti = αTi−1 for some α < 1  cid:126 x ←  cid:126 x0 for i ← 1, 2, 3, . . .   cid:46  Current model initialized to the input  cid:126 x0   cid:126 y ← Random-Model ∆f ← f   cid:126 y  − f   cid:126 x  if ∆f < 0 then   cid:46  Random guess of output  cid:46  Compute change in objective  cid:46  Objective improved at  cid:126 y   cid:126 x ←  cid:126 y  cid:126 x ←  cid:126 y  else if Uniform 0,1 < e−∆f  Ti then   cid:46  True with probability e−∆f  Ti  cid:46  Randomly keep suboptimal output  Simulated annealing randomly guesses a solution to the optimization problem in each iteration. If the new solution achieves a lower objective value than the current solution,   Specialized Optimization Methods  cid:4  251  the algorithm keeps the new solution. If the new solution is less optimal, however, it is not necessarily rejected. Instead, the suboptimal point is accepted with exponentially small probability as temperature decreases. The hope of this construction is that local minima will be avoided early on in favor of global minima due to the signiﬁcant amount of exploration during the ﬁrst few iterations, while some form of convergence is still obtained as the iterates stabilize at lower temperatures.  Consider the Euclidean traveling salesman problem  TSP : Given a set of points  cid:126 x1, . . . ,  cid:126 xn ∈ R2 representing the positions of cities on a map, we wish to visit each city exactly once while minimizing the total distance traveled. While Euclidean TSP is NP-hard, simulated annealing provides a practical way to approximate its solution.   a  Phrase Euclidean TSP as a global optimization problem. It is acceptable to have  variables that are discrete rather than continuous.   b  Propose a method for generating random tours that reach each city exactly once.  What f should you use to evaluate the quality of a tour?   c   Implement your simulated annealing solution to Euclidean TSP and explore the trade-oﬀ between solution quality and runtime when the initial temperature T0 is changed. Also, experiment with diﬀerent cooling schedules, either by varying α in the example Ti or by proposing your own cooling schedule.   d  Choose another global optimization algorithm and explain how to use it to solve Euclidean TSP. Analyze how its eﬃciency compares to that of simulated anneal- ing.   e  Rather than generating a completely new tour in each iteration of simulated annealing, propose a method that perturbs tours slightly to generate new ones. What would be the advantages and or disadvantages of using this technique in place of totally random models?  SC 12.10 Recall the setup from Exercise 10.7 for designing a slow-dissolving medicinal capsule  shaped as a cylinder with hemispherical ends.   a  Suppose we are unhappy with the results of the optimization proposed in Exer- cise 10.7 and want to ensure that the volume of the entire capsule  including the ends  is at least V . Explain why the resulting problem cannot be solved using geometric programming methods.   b  Propose an alternating optimization method for this problem. Is it necessary to  solve a geometric program in either alternation?  12.11 The mean shift algorithm, originally proposed in [27], is an iterative clustering tech- nique appearing in literature on nonparametric machine learning and image process- ing. Given n data points  cid:126 xi ∈ Rd, the algorithm groups points together based on their closest maxima in a smoothed density function approximating the distribution of data points.  a  Take k x  : R → R+ to be a nonnegative function. For a ﬁxed bandwidth param-  eter h > 0, deﬁne the kernel density estimator ˆf   cid:126 x  to be  ˆfk  cid:126 x  ≡  ck,d nhd   cid:126 x −  cid:126 xi  h  n cid:88 i=1  k cid:32  cid:13  cid:13  cid:13  cid:13   2  2 cid:33  .  cid:13  cid:13  cid:13  cid:13    252  cid:4  Numerical Algorithms  If k x  is peaked at x = 0, explain how ˆfk  cid:126 x  encodes the density of data points  cid:126 xi. What is the eﬀect of increasing the parameter h?  ˆf   cid:126 x  d cid:126 x = 1. Choosing k x  ≡ e−x 2   b  Deﬁne g x  ≡ −k cid:48  x  and take m  cid:126 x  to be the mean shift vector given by  makes ˆf a sum of Gaussians.  Note: The constant ck,d is chosen so that cid:82 Rd 2 cid:17  m  cid:126 x  ≡  cid:80 i  cid:126 xig cid:16  cid:13  cid:13   cid:126 x− cid:126 xi h  cid:13  cid:13 2  cid:80 i g cid:16  cid:13  cid:13   cid:126 x− cid:126 xi 2 cid:17  −  cid:126 x. h  cid:13  cid:13 2  Show that ∇ ˆfk  cid:126 x  can be factored as follows:  ∇ ˆfk  cid:126 x  =  α h2 · ˆfg  cid:126 x  · m  cid:126 x ,  for some constant α.   c  Suppose  cid:126 y0 is a guess of the location of a peak of ˆfk. Using your answer from Exercise 12.11b, motivate the mean shift algorithm for ﬁnding a peak of ˆfk  cid:126 x , which iterates the formula  2   cid:126 yk+1 ≡  cid:80 i  cid:126 xig cid:18  cid:13  cid:13  cid:13   cid:126 yk− cid:126 xi 2 cid:19  h  cid:13  cid:13  cid:13   cid:80 i g cid:18  cid:13  cid:13  cid:13   cid:126 yk− cid:126 xi 2 cid:19  . h  cid:13  cid:13  cid:13   2  Note: This algorithm is guaranteed to converge under mild conditions on k. Mean shift clustering runs this method to convergence starting from  cid:126 y0 =  cid:126 xi for each i in parallel;  cid:126 xi and  cid:126 xj are assigned to the same cluster if mean shift iteration yields the same output  within some tolerance  for starting points  cid:126 y0 =  cid:126 xi and  cid:126 y0 =  cid:126 xj.   d  Suppose we represent a grayscale image as a set of pairs   cid:126 pi, qi , where  cid:126 pi is the center of pixel i  typically laid out on a grid , and qi ∈ [0, 1] is the intensity of pixel i. The bilateral ﬁlter [120] for blurring images while preserving their sharp edges is given by:  ˆqi ≡  cid:80 j qjk1  cid:107  cid:126 pj −  cid:126 pi cid:107 2 k2 qj − qi   cid:80 j k1  cid:107  cid:126 pj −  cid:126 pi cid:107 2 k2 qj − qi  where k1, k2 are Gaussian kernels given by ki x  ≡ e−aix2 . Fast algorithms have been developed in the computer graphics community for evaluating the bilateral ﬁlter and its variants [97]. Propose an algorithm for clustering the pixels in an image using iterated calls to a modiﬁed version of the bilateral ﬁlter; the resulting method is called the “local mode ﬁlter” [125, 96].  ,  12.12 The iterative shrinkage-thresholding algorithm  ISTA  is another technique relevant to large-scale optimization applicable to common objectives from machine learning. Extensions such as [11] have led to renewed interest in this technique. We follow the development of [20].   Specialized Optimization Methods  cid:4  253   a  Show that the iteration from gradient descent  can be rewritten in proximal form as   cid:126 xk+1 =  cid:126 xk − α∇f   cid:126 xk    cid:126 xk+1 = arg min   cid:126 x   cid:20 f   cid:126 xk  + ∇f   cid:126 xk  cid:62   cid:126 x −  cid:126 xk  +  2 cid:21  . 1 2α cid:107  cid:126 x −  cid:126 xk cid:107 2   b  Suppose we wish to minimize a sum f   cid:126 x  + g  cid:126 x . Based on the previous part, ISTA attempts to combine exact optimization for g with gradient descent on f :   cid:126 xk+1 ≡ arg min   cid:126 x   cid:20 f   cid:126 xk  + ∇f   cid:126 xk  cid:62   cid:126 x −  cid:126 xk  +  Derive the alternative form   cid:126 xk+1 = arg min   cid:126 x   cid:20 g  cid:126 x  +  1 2α cid:107  cid:126 x −  cid:126 xk cid:107 2  2 + g  cid:126 x  cid:21  . 2 cid:21  . 1 2α cid:107  cid:126 x −   cid:126 xk − α∇f   cid:126 xk   cid:107 2   c  Derive a formula for ISTA iterations when g  cid:126 x  = λ cid:107  cid:126 x cid:107 1, where λ > 0.  Hint: This case reduces to solving a set of single-variable problems.  12.13 Suppose D is a bounded, convex, and closed domain in Rn and f   cid:126 x  is a convex, diﬀer- entiable objective function. The Frank-Wolfe algorithm for minimizing f   cid:126 x  subject to  cid:126 x ∈ D is as follows [43]:   cid:126 sk ← arg min  cid:126 s∈D 2  [ cid:126 s · ∇f   cid:126 xk−1 ]  k + 2  γk ←  cid:126 xk ←  1 − γk  cid:126 xk−1 + γk cid:126 sk.  A starting point  cid:126 x0 ∈ D must be provided. This algorithm has gained renewed atten- tion for large-scale optimization in machine learning in the presence of sparsity and other specialized structure [66].   a  Argue that  cid:126 sk minimizes a linearized version of f subject to the constraints. Also, show that if D = { cid:126 x : A cid:126 x ≤  cid:126 b} for ﬁxed A ∈ Rm×n and  cid:126 b ∈ Rm, then each iteration of the Frank-Wolfe algorithm solves a linear program.   b  Show that  cid:126 xk ∈ D for all k > 0.  c  Assume ∇f   cid:126 x  is L-Lipschitz on D, meaning  cid:107 ∇f   cid:126 x  − ∇f   cid:126 y  cid:107 2 ≤ L cid:107  cid:126 x −  cid:126 y cid:107 2,  for all  cid:126 x,  cid:126 y ∈ D. Derive the bound  proposed in [88] : L 2  cid:107  cid:126 y −  cid:126 x cid:107 2 2. Hint: By the Fundamental Theorem of Calculus, f   cid:126 y  = f   cid:126 x + cid:82  1  f   cid:126 y  − f   cid:126 x  −   cid:126 y −  cid:126 x  · ∇f   cid:126 x  ≤  τ   cid:126 y −  cid:126 x   dτ.  0   cid:126 y− cid:126 x ·∇f   cid:126 x+   254  cid:4  Numerical Algorithms   d  Deﬁne the diameter of D to be d ≡ max cid:126 x, cid:126 y∈D  cid:107  cid:126 x −  cid:126 y cid:107 2. Furthermore, assume  ∇f   cid:126 x  is L-Lipschitz on D. Show that  2 γ2  f   cid:126 y  − f   cid:126 x  −   cid:126 y −  cid:126 x  · ∇f   cid:126 x   ≤ d2L,  for all  cid:126 x,  cid:126 y,  cid:126 s ∈ D with  cid:126 y =  cid:126 x + γ  cid:126 s −  cid:126 x  and γ ∈ [0, 1]. Conclude that  f   cid:126 y  ≤ f   cid:126 x  + γ  cid:126 s −  cid:126 x  · ∇f   cid:126 x  +  γ2d2L  .  2   e  Deﬁne the duality gap g  cid:126 x  ≡ max cid:126 s∈D  cid:126 x −  cid:126 s  · ∇f   cid:126 x . For the Frank-Wolfe algo-  rithm, show that  f   cid:126 xk  ≤ f   cid:126 xk−1  − γg  cid:126 xk−1  +  γ2 kd2L 2  .   f  Take  cid:126 x∗ to be the location of the minimum for the optimization problem, and deﬁne h  cid:126 x  ≡ f   cid:126 x  − f   cid:126 x∗ . Show g  cid:126 x  ≥ h  cid:126 x , and using the previous part conclude  h  cid:126 xk  ≤  1 − γk h  cid:126 xk−1  +  γ2 kd2L 2  .   g  Conclude h  cid:126 xk  → 0 as k → ∞. What does this imply about the Frank-Wolfe  algorithm?  12.14 The FTRL algorithm from §12.5 can be expensive when the ft’s are diﬃcult to min- imize. In this problem, we derive a linearized alternative with similar performance guarantees.   a  Suppose we make the following assumptions about an instance of FTRL:    U = { cid:126 x ∈ Rn :  cid:107  cid:126 x cid:107 2 ≤ 1}.   All of the objectives ft provided to FTRL are of the form ft  cid:126 x  =  cid:126 zt ·  cid:126 x for  cid:107  cid:126 zt cid:107 2 ≤ 1.   r  cid:126 x  ≡ 1 Provide an explicit formula for the iterates  cid:126 xt in this case, and specialize the bound from Proposition 12.1.  2 σ cid:107  cid:126 x cid:107 2 2.   b  We wish to apply the bound from 12.14a to more general ft’s. To do so, suppose  we replace FTRL with a linearized objective for  cid:126 xt:   cid:126 x∈U  cid:34 r  cid:126 x  +   cid:126 xt ≡ arg min   fs  cid:126 xs  + ∇fs  cid:126 xs  ·   cid:126 x −  cid:126 xs   cid:35  . t−1 cid:88 s=1  Provide an explicit formula for  cid:126 xt in this case, assuming the same choice of U and r.   c  Propose a regret bound for the linearized method in 12.14b.  Hint: Apply convexity of the ft’s and the result of 12.14a.   IV  Functions, Derivatives, and Integrals  255    C H A P T E R13  Interpolation  CONTENTS  13.1  Interpolation in a Single Variable . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13.1.1 Polynomial Interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13.1.2 Alternative Bases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13.1.3 Piecewise Interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13.2 Multivariable Interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13.2.1 Nearest-Neighbor Interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13.2.2 Barycentric Interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13.2.3 Grid-Based Interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13.3 Theory of Interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13.3.1 Linear Algebra of Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13.3.2 Approximation via Piecewise Polynomials . . . . . . . . . . . . . . . . . . . . . .  258 258 262 263 265 265 266 268 269 269 272  S O far we have derived methods for analyzing functions f , e.g., ﬁnding their minima and roots. Evaluating f   cid:126 x  at a particular  cid:126 x ∈ Rn might be expensive, but a fundamental assumption of the methods we developed in previous chapters is that we can obtain f   cid:126 x  when we want it, regardless of  cid:126 x.  There are many contexts in which this assumption is unrealistic. For instance, if we take a photograph with a digital camera, we receive an n× m grid of pixel color values sampling the continuum of light coming into the camera lens. We might think of a photograph as a continuous function from image position  x, y  to color  r, g, b , but in reality we only know the image value at nm separated locations on the image plane. Similarly, in machine learning and statistics, often we only are given samples of a function at points where we collected data, and we must interpolate to have values elsewhere; in a medical setting we may monitor a patient’s response to diﬀerent dosages of a drug but must predict what will happen at a dosage we have not tried explicitly.  In these cases, before we can minimize a function, ﬁnd its roots, or even compute val- ues f   cid:126 x  at arbitrary locations  cid:126 x, we need a model for interpolating f   cid:126 x  to all of Rn  or some subset thereof  given a collection of samples f   cid:126 xi . Techniques for this interpolation problem are inherently approximate, since we do not know the true values of f , so instead we seek for the interpolated function to be smooth and serve as a reasonable prediction of function values. Mathematically, the deﬁnition of “reasonable” will depend on the par- ticular application. If we want to evaluate f   cid:126 x  directly, we may choose an interpolant and sample positions  cid:126 xi so that the distance of the interpolated f   cid:126 x  from the true values can be bounded above given smoothness assumptions on f ; future chapters will estimate derivatives, integrals, and other properties of f from samples and may choose an interpolant designed to make these approximations accurate or stable.  257   258  cid:4  Numerical Algorithms  In this chapter, we will assume that the values f   cid:126 xi  are known with complete certainty; in this case, we can think of the problem as extending f to the remainder of the domain without perturbing the value at any of the input locations. To contrast, the regression problem considered in §4.1.1 and elsewhere may forgo matching f   cid:126 xi  exactly in favor of making f more smooth. 13.1 INTERPOLATION IN A SINGLE VARIABLE  Before considering the general case, we will design methods for interpolating functions of a single variable f : R → R. As input, we will take a set of k pairs  xi, yi  with the assumption f  xi  = yi; our job is to predict f  x  for x  cid:54 ∈ {x1, . . . , xk}. Desirable interpolants f  x  should be smooth and should interpolate the data points faithfully without adding extra features like spurious local minima and maxima.  We will take inspiration from linear algebra by writing f  x  in a basis. The set of all possible functions f : R → R is far too large to work with and includes many functions that are not practical in a computational setting. Thus, we simplify the search space by forcing f to be written as a linear combination of building block basis functions. This formulation is familiar from calculus: The Taylor expansion writes functions in the basis of polynomials, while Fourier series use sine and cosine.  The construction and analysis of interpolation bases is a classical topic that has been studied for centuries. We will focus on practical aspects of choosing and using interpolation bases, with a brief consideration of theoretical aspects in §13.3. Detailed aspects of error analysis can be found in [117] and other advanced texts.  13.1.1 Polynomial Interpolation  Perhaps the most straightforward class of interpolation formulas assumes that f  x  is in R[x], the set of polynomials. Polynomials are smooth, and we already have explored linear methods for ﬁnding a degree k − 1 polynomial through k sample points in Chapter 4. Example 4.3 worked out the details of such an interpolation technique. As a reminder, suppose we wish to ﬁnd f  x  ≡ a0 + a1x + a2x2 + ··· + ak−1xk−1 through the points  x1, y1 , . . . ,  xk, yk ; here our unknowns are the values a0, . . . , ak−1. Plugging in the ex- pression yi = f  xi  for each i shows that the vector  cid:126 a satisﬁes the k × k Vandermonde system:    1 x1 x2 1 1 x2 x2 2 ... ... 1 xk x2 k  ...  1  2  ··· xk−1 ··· xk−1 ... ··· ··· xk−1  k      a0 a1 ... ak−1    =  y0 y1 ... yk  .    By this construction, degree k − 1 polynomial interpolation can be accomplished using a k × k linear solve for  cid:126 a using the linear algorithms in Chapter 3. This method, however, is far from optimal for many applications. As mentioned above, one way to think about the space of polynomials is that it can be spanned by a basis of functions. Just like writing vectors in Rn as linear combinations of linearly independent vectors  cid:126 v1, . . . ,  cid:126 vn ∈ Rn, in our derivation of the Vandermonde matrix, we wrote polynomials in the monomial basis {1, x, x2, . . . , xk−1} for polynomials of degree k − 1. Although monomials may be an obvious basis for R[x], they have limited properties useful for simplifying the polynomial interpolation problem. One way to visualize this issue is to plot the sequence of functions 1, x, x2, x3, . . . for x ∈ [0, 1]; in this interval, as shown in   Interpolation  cid:4  259  Figure 13.1 As k increases, the monomials xk on [0, 1] begin to look more and more similar. This similarity creates poor conditioning for monomial basis problems like solving the Vandermonde system.  Figure 13.1, the functions xk all start looking similar as k increases. As we know from our consideration of projection problems in Chapter 5, projection onto a set of similar-looking basis vectors can be unstable.  We may choose to write polynomials in a basis that is better suited to the problem at hand. Recall that we are given k pairs  x1, y1 , . . . ,  xk, yk . We can use these  ﬁxed  points to deﬁne the Lagrange interpolation basis φ1, . . . , φk by writing:  φi x  ≡  cid:81 j cid:54 =i x − xj   cid:81 j cid:54 =i xi − xj   .  Example 13.1  Lagrange basis . Suppose x1 = 0, x2 = 2, x3 = 3, and x4 = 4. The Lagrange basis for this set of xi’s is:  =  1 24   −x3 + 9x2 − 26x + 24   φ1 x  =  φ2 x  =  φ3 x  =  φ4 x  =  1 4  =   x − 2  x − 3  x − 4  −2 · −3 · −4 x x − 3  x − 4  2 ·  2 − 3  2 − 4  x x − 2  x − 4  3 ·  3 − 2  ·  3 − 4  x x − 2  x − 3  4 ·  4 − 2  ·  4 − 3   =  =   x3 − 7x2 + 12x  1  −x3 + 6x2 − 8x  3 1  x3 − 5x2 + 6x . 8  This basis is shown in Figure 13.2.  As shown in this example, although we did not deﬁne it explicitly in the monomial basis {1, x, x2, . . . , xk−1}, each φi is still a polynomial of degree k− 1. Furthermore, the Lagrange basis has the following desirable property:  φi x cid:96   = cid:26  1 when  cid:96  = i  otherwise.  0   260  cid:4  Numerical Algorithms  Figure 13.2 The Lagrange basis for x1 = 0, x2 = 2, x3 = 3, x4 = 4. Each φi satisﬁes φi xi  = 1 and φi xj  = 0 for all i  cid:54 = j.  Using this formula, ﬁnding the unique degree k − 1 polynomial ﬁtting our  xi, yi  pairs is formulaic in the Lagrange basis:  To check, if we substitute x = xj we ﬁnd:  f  x  ≡ cid:88 i  yiφi x .  f  xj  = cid:88 i  yiφi xj   = yj since φi xj  = 0 when i  cid:54 = j.  We have shown that in the Lagrange basis we can write a closed formula for f  x  that does not require solving the Vandermonde system; in other words, we have replaced the Vandermonde matrix with the identity matrix. The drawback, however, is that each φi x  takes O k  time to evaluate using the formula above, so computing f  x  takes O k2  time total; contrastingly, if we ﬁnd the coeﬃcients ai from the Vandermonde system explicitly, the evaluation time for interpolation subsequently becomes O k .  Computation time aside, the Lagrange basis has an additional numerical drawback, in that the denominator is the product of a potentially large number of terms. If the xi’s are close together, then this product may include many terms close to zero; the end result is division by a small number when evaluating φi x . As we have seen, this operation can create numerical instabilities that we wish to avoid.  A third basis for polynomials of degree k − 1 that attempts to compromise between the numerical quality of the monomials and the eﬃciency of the Lagrange basis is the Newton basis, deﬁned as  ψi x  =   x − xj .  i−1 cid:89 j=1  This product has no terms when i = 1, so we deﬁne ψ1 x  ≡ 1. Then, for all indices i, the function ψi x  is a degree i − 1 polynomial.  φ1  1  0  φ2  φ3  φ4  2  3  4   Interpolation  cid:4  261  Figure 13.3 The Newton basis for x1 = 0, x2 = 2, x3 = 3, x4 = 4. Each ψi satisﬁes ψi xj  = 0 when j < i.  Example 13.2  Newton basis . Continuing from Example 13.1, again suppose x1 = 0, x2 = 2, x3 = 3, and x4 = 4. The corresponding Newton basis is:  This basis is illustrated in Figure 13.3.  By deﬁnition of ψi, ψi x cid:96   = 0 for all  cid:96  < i. If we wish to write f  x  = cid:80 i ciψi x  and  write out this observation more explicitly, we ﬁnd:  ψ1 x  = 1 ψ2 x  = x ψ3 x  = x x − 2  = x2 − 2x ψ4 x  = x x − 2  x − 3  = x3 − 5x2 + 6x.  f  x1  = c1ψ1 x1  f  x2  = c1ψ1 x2  + c2ψ2 x2  f  x3  = c1ψ1 x3  + c2ψ2 x3  + c3ψ3 x3   ...  ...  These expressions provide the following lower-triangular system for  cid:126 c:    0  ψ1 x1  ψ1 x2  ψ2 x2  ψ1 x3  ψ2 x3  ψ3 x3   0 0  ...  ...  ...  ψ1 xk  ψ2 xk  ψ3 xk   0 0 0 ...  ··· ··· ··· ··· ··· ψk xk       c1 c2 ... ck    =  y1 y2 ... yk  .    This system can be solved in O k2  time using forward-substitution, rather than the O k3  time needed to solve the Vandermonde system using Gaussian elimination.∗ Evaluation time  ∗For completeness, we should mention that O k2  Vandermonde solvers can be formulated; see [62] for  discussion of these specialized techniques.  10  0  2  3  ψ3  ψ4  ψ2  ψ1  4   262  cid:4  Numerical Algorithms  is similar to that of the Lagrange basis, but since there is no denominator, numerical issues are less likely to appear.  We now have three strategies of interpolating k data points using a degree k − 1 poly- nomial by writing it in the monomial, Lagrange, and Newton bases. All three represent diﬀerent compromises between numerical quality and speed, but the resulting interpolated function f  x  is the same in each case. More explicitly, there is exactly one polynomial of degree k − 1 going through a set of k points, so since all our interpolants are degree k − 1 they must have the same output.  13.1.2 Alternative Bases  Although polynomial functions are particularly amenable to mathematical analysis, there is no fundamental reason why an interpolation basis cannot consist of diﬀerent types of functions. For example, a crowning result of Fourier analysis implies that many functions are well-approximated by linear combinations of trigonometric functions cos kx  and sin kx  for k ∈ N. A construction like the Vandermonde matrix still applies in this case, and the fast Fourier transform algorithm  which merits a larger discussion  solves the resulting linear system with remarkable eﬃciency.  A smaller extension of the development in §13.1.1 is to rational functions of the form:  f  x  ≡  p0 + p1x + p2x2 + ··· + pmxm q0 + q1x + q2x2 + ··· + qnxn .  If we are given k pairs  xi, yi , then we will need m + n + 1 = k for this function to be well-deﬁned. One degree of freedom must be ﬁxed to account for the fact that the same rational function can be expressed multiple ways by simultaneously scaling the numerator and the denominator.  Rational functions can have asymptotes and other features not achievable using only polynomials, so they can be desirable interpolants for functions that change quickly or have poles. Once m and n are ﬁxed, the coeﬃcients pi and qi still can be found using linear techniques by multiplying both sides by the denominator:  yi q0 + q1xi + q2x2  i + ··· + qnxn  i   = p0 + p1xi + p2x2  i + ··· + pmxm i .  For interpolation, the unknowns in this expression are the p’s and q’s.  The ﬂexibility of rational functions, however, can cause some issues. For instance, con-  sider the following example:  Example 13.3  Failure of rational interpolation, [117] §2.2 . Suppose we wish to ﬁnd a rational function f  x  interpolating the following data points:  0, 1 ,  1, 2 ,  2, 2 . If we choose m = n = 1, then the linear system for ﬁnding the unknown coeﬃcients is:  One nontrivial solution to this system is:  q0 = p0  2 q0 + q1  = p0 + p1 2 q0 + 2q1  = p0 + 2p1.  p0 = 0 p1 = 2  q0 = 0 q1 = 1.   Interpolation  cid:4  263  Figure 13.4 Interpolating eight samples of the function f  x  ≡ 1 2 using a seventh-  degree polynomial yields a straight line, but perturbing a single data point at x = 3 creates an interpolant that oscillates far away from the inﬁnitesimal vertical displacement.  This implies the following form for f  x :  f  x  =  2x x  .  This function has a degeneracy at x = 0, and canceling the x in the numerator and denominator does not yield f  0  = 1 as we might desire.  This example illustrates a larger phenomenon. The linear system for ﬁnding the p’s and  q’s can run into issues when the resulting denominator  cid:80  cid:96  p cid:96 x cid:96  has a root at any of the  ﬁxed xi’s. It can be shown that when this is the case, no rational function exists with the ﬁxed choice of m and n interpolating the given values. A typical partial resolution in this case is presented in [117], which suggests incrementing m and n alternatively until a nontrivial solution exists. From a practical standpoint, however, the specialized nature of these methods indicates that alternative interpolation strategies may be preferable when the basic rational methods fail.  13.1.3 Piecewise Interpolation  So far, we have constructed interpolation bases out of elementary functions deﬁned on all of R. When the number k of data points becomes high, however, many degeneracies become apparent. For example, Figure 13.4 illustrates how polynomial interpolation is nonlocal, meaning that changing any single value yi in the input data can change the behavior of f for all x, even those that are far away from xi. This property is undesirable for most applications: We usually expect only the input data near a given x to aﬀect the value of the interpolated function f  x , especially when there is a large cloud of input points. While the Weierstrass Approximation Theorem from real analysis guarantees that any smooth function f  x  on an interval x ∈ [a, b] can be approximated arbitrarily well using polynomials, achieving a quality interpolation in practice requires choosing many carefully placed sample points. As an alternative to global interpolation bases, when we design a set of basis functions  φ1, . . . , φk, a desirable property we have not yet considered is compact support:  Deﬁnition 13.1  Compact support . A function g  cid:126 x  has compact support if there exists C ∈ R such that g  cid:126 x  = 0 for any  cid:126 x with  cid:107  cid:126 x cid:107 2 > C.  1 2  1   264  cid:4  Numerical Algorithms  Figure 13.5 Two piecewise interpolation strategies.  That is, compactly supported functions only have a ﬁnite range of points in which they can take nonzero values.  Piecewise formulas provide one technique for constructing interpolatory bases with com- pact support. Most prominently, methods in computer graphics and many other ﬁelds make use of piecewise polynomials, which are deﬁned by breaking R into a set of intervals and writing a diﬀerent polynomial in each interval. To do so, we will order the data points so that x1 < x2 < ··· < xk. Then, two examples of piecewise interpolants are the following, illustrated in Figure 13.5:    Piecewise constant interpolation: For a given x ∈ R, ﬁnd the data point xi minimizing  x − xi and deﬁne f  x  = yi.    Piecewise linear interpolation: If x   xk take f  x  = yk.  Otherwise, ﬁnd the interval with x ∈ [xi, xi+1] and deﬁne  f  x  = yi+1 ·  x − xi xi+1 − xi  + yi · cid:18 1 −  x − xi  xi+1 − xi cid:19  .  Notice our pattern so far: Piecewise constant polynomials are discontinuous, while piecewise linear functions are continuous. Piecewise quadratics can be C 1, piecewise cubics can be C 2, and so on. This increased continuity and diﬀerentiability occurs even though each yi has local support; this theory is worked out in detail in constructing “splines,” or curves interpolating between points given function values and tangents.  Increased continuity, however, has its drawbacks. With each additional degree of diﬀer- entiability, we put a stronger smoothness assumption on f . This assumption can be unreal- istic: Many physical phenomena truly are noisy or discontinuous, and increased smoothness can negatively aﬀect interpolatory results. One domain in which this eﬀect is particularly clear is when interpolation is used in conjunction with physical simulation algorithms. Sim- ulating turbulent ﬂuid ﬂows with excessively smooth functions inadvertently can remove discontinuous phenomena like shock waves.  These issues aside, piecewise polynomials still can be written as linear combinations of basis functions. For instance, the following functions serve as a basis for the piecewise constant functions:  φi x  = cid:26  1 when xi−1+xi  2 otherwise.  0  ≤ x < xi+xi+1  2  This basis puts the constant 1 near xi and 0 elsewhere; the piecewise constant interpolation  of a set of points  xi, yi  is written as f  x  = cid:80 i yiφi x . Similarly, the so-called “hat” basis  tnatsnocesiweceiP  raenilesiweceiP   Interpolation  cid:4  265  Figure 13.6 Basis functions corresponding to the piecewise interpolation strategies in Figure 13.5.  spans the set of piecewise linear functions with sharp edges at the data points xi:  ψi x  =  x−xi−1 xi−xi−1 xi+1−x xi+1−xi  0  when xi−1 < x ≤ xi when xi < x ≤ xi+1 otherwise.  Once again, by construction, the piecewise linear interpolation of the given data points is  f  x  = cid:80 i yiψi x . Examples of both bases are shown in Figure 13.6. 13.2 MULTIVARIABLE INTERPOLATION  It is possible to extend the strategies above to the case of interpolating a function given data points   cid:126 xi, yi  where  cid:126 xi ∈ Rn now can be multidimensional. Interpolation algorithms in this general case are challenging to formulate, however, because it is less obvious how to partition Rn into a small number of regions around the source points  cid:126 xi. 13.2.1 Nearest-Neighbor Interpolation Given the complication of interpolation on Rn, a common pattern is to interpolate using many low-order functions rather than fewer smooth functions, that is, to favor simplistic and eﬃcient interpolants over ones that output C∞ functions. For example, if all we are given is a set of pairs   cid:126 xi, yi , then one piecewise constant strategy for interpolation is to use nearest-neighbor interpolation. In this case, f   cid:126 x  takes the value yi corresponding to  cid:126 xi minimizing  cid:107  cid:126 x −  cid:126 xi cid:107 2. Simple implementations iterate over all i to ﬁnd the closest  cid:126 xi to  cid:126 x, and data structures like k-d trees can ﬁnd nearest neighbors more quickly. Just as piecewise constant interpolants on R take constant values on intervals about the data points xi, nearest-neighbor interpolation yields a function that is piecewise-constant on Voronoi cells:  Deﬁnition 13.2  Voronoi cell . Given a set of points S = { cid:126 x1,  cid:126 x2, . . . ,  cid:126 xk} ⊆ Rn, the Voronoi cell corresponding to a speciﬁc  cid:126 xi ∈ S is the set Vi ≡ { cid:126 x :  cid:107  cid:126 x −  cid:126 xi cid:107 2 <  cid:107  cid:126 x −  cid:126 xj cid:107 2 for all j  cid:54 = i}. That is, Vi is the set of points closer to  cid:126 xi than to any other  cid:126 xj in S. Figure 13.7 shows an example of the Voronoi cells about a set of points in R2. These cells have many favorable properties; for example, they are convex polygons and are localized  1  φi x   1  ψi x   xi  xi  Piecewise constant basis  Piecewise linear basis  hat function    266  cid:4  Numerical Algorithms  Figure 13.7 Voronoi cells associated with ten points in a rectangle.  about each  cid:126 xi. The adjacency of Voronoi cells is a well-studied problem in computational geometry leading to the construction of the celebrated Delaunay triangulation [33].  In many cases, however, it is desirable for the interpolant f   cid:126 x  to be continuous or diﬀerentiable. There are many options for continuous interpolation in Rn, each with its own advantages and disadvantages. If we wish to extend the nearest-neighbor formula, we could compute multiple nearest neighbors  cid:126 x1, . . . ,  cid:126 xk of  cid:126 x and interpolate f   cid:126 x  by averaging the corresponding y1, . . . , yk with distance-based weights; Exercise 13.4 explores one such weighting. Certain “k-nearest neighbor” data structures also can accelerate queries searching for multiple points in a dataset closest to a given  cid:126 x.  13.2.2 Barycentric Interpolation  Another continuous multi-dimensional interpolant appearing frequently in the computer graphics literature is barycentric interpolation. Suppose we have exactly n+1 sample points   cid:126 x1, y1 , . . . ,   cid:126 xn+1, yn+1 , where  cid:126 xi ∈ Rn, and we wish to interpolate the yi’s to all of Rn; on the plane R2, we would be given three values associated with the vertices of a triangle. In the absence of degeneracies  e.g., three of the  cid:126 xi’s coinciding on the same line , any  cid:126 x ∈ Rn can be written uniquely as a linear combination  cid:126 x = cid:80 n+1 i=1 ai cid:126 xi where cid:80 i ai = 1. This formula expresses  cid:126 x as a weighted average of the  cid:126 xi’s with weights ai. For ﬁxed  cid:126 x1, . . . ,  cid:126 xn+1, the weights ai can be thought of as components of a function  cid:126 a  cid:126 x  taking points  cid:126 x to their corresponding coeﬃcients. Barycentric interpolation then deﬁnes f   cid:126 x  ≡ cid:80 i ai  cid:126 x yi. On the plane, barycentric interpolation has a straightforward geometric interpretation involving triangle areas, illustrated in Figure 13.8 a . Regardless of dimension, however, the barycentric interpolant f   cid:126 x  is aﬃne, meaning it can be written f   cid:126 x  = c +  cid:126 d · x for some c ∈ R and  cid:126 d ∈ Rn. Counting degrees of freedom, the n + 1 sample points are accounted for via n unknowns in  cid:126 d and one unknown in c.  The system of equations to ﬁnd  cid:126 a  cid:126 x  corresponding to some  cid:126 x ∈ Rn is:  ai cid:126 xi =  cid:126 x   cid:88 i  ai = 1   cid:88 i  This system usually is invertible when there are n+1 points  cid:126 xi. In the presence of additional  cid:126 xi’s, however, it becomes underdetermined. This implies that there are multiple ways of writing  cid:126 x as a weighted average of the  cid:126 xi’s, making room for additional design decisions during barycentric interpolation, encoded in the particular choice of  cid:126 a  cid:126 x .  One resolution of this non-uniqueness is to add more linear or nonlinear constraints on the weights  cid:126 a. These yield diﬀerent generalized barycentric coordinates. Typical constraints   Interpolation  cid:4  267  Figure 13.8  a  The barycentric coordinates of  cid:126 p ∈ R2 relative to the points  cid:126 p1,  cid:126 p2, and  cid:126 p3, respectively, are  A1 A, A2 A, A3 A , where A ≡ A1 + A2 + A3 and Ai is the area of triangle i;  b  the barycentric deformation method [129] uses a generalized version of barycentric coordinates to deform planar shapes according to motions of a polygon with more than three vertices.  Figure 13.9  a  A collection of points on R2 can be triangulated into a triangle mesh;  b  using this mesh, a per-point function can be interpolated to the interior using per-triangle barycentric interpolation;  c  a single “hat” basis function takes value one on a single vertex and is interpolated using barycentric coordinates to the remainder of the domain.  on  cid:126 a ask that it is smooth as a function of  cid:126 x on Rn and nonnegative on the interior of the polygon or polyhedron bordered by the  cid:126 xi’s. Figure 13.8 b  shows an example of image deformation using a recent generalized barycentric coordinates algorithm; the particular method shown makes use of complex-valued coordinates to take advantage of geometric properties of the complex plane [129].  Another way to carry out barycentric interpolation with more than n + 1 data points employs piecewise aﬃne functions for interpolation; we will restrict our discussion to  cid:126 xi ∈ R2 for simplicity, although extensions to higher dimensions are possible. Suppose we are given not only a set of points  cid:126 xi ∈ R2 but also a triangulation linking those points together, as in Figure 13.9 a . If the triangulation is not known a priori, it can be computed using well-known geometric techniques [33]. Then, we can interpolate values from the vertices of each triangle to its interior using barycentric interpolation.  Example 13.4  Shading . A typical representation of three-dimensional shapes in com- puter graphics is a set of triangles linked into a mesh. In the per-vertex shading model, one color is computed for each vertex on the mesh using lighting of the scene, mate- rial properties, and so on. Then, to render the shape on-screen, those per-vertex colors are interpolated using barycentric interpolation to the interiors of the triangles. Similar  A3  A1   cid:31 p A2   cid:31 p 3   cid:31 p 2   a    cid:31 p 1   b    a  Triangle mesh   b  Barycentric interpolation   c  Hat function   268  cid:4  Numerical Algorithms  strategies are used for texturing and other common tasks. Figure 13.9 b  shows an example of this technique.  As an aside, one pertinent issue speciﬁc to computer graphics is the interplay between perspective transformations and interpolation. Barycentric interpolation of color along a triangulated 3D surface and then projection of that color onto the image plane is not the same as projecting triangles to the image plane and subsequently interpolating color along the projected two-dimensional triangles. Algorithms in this domain must use perspective- corrected interpolation strategies to account for this discrepancy during the rendering process.  Interpolation using a triangulation parallels the use of a piecewise-linear hat basis for one-dimensional functions, introduced in §13.1.3. Now, we can think of f   cid:126 x  as a linear com- a 1 on  cid:126 xi and 0 everywhere else, as in Figure 13.9 c .  bination cid:80 i yiφi  cid:126 x , where each φi  cid:126 x  is the piecewise aﬃne function obtained by putting  Given a set of points in R2, the problem of triangulation is far from trivial, and analogous constructions in higher dimensions can scale poorly. When n > 3, methods that do not require explicitly partitioning the domain usually are preferable.  13.2.3 Grid-Based Interpolation  Rather than using triangles, an alternative decomposition of the domain of f occurs when the points  cid:126 xi occur on a regular grid. The following examples illustrate situations when this is the case:  Example 13.5  Image processing . A typical digital photograph is represented as an m × n grid of red, green, and blue color intensities. We can think of these values as living on the lattice Z× Z ⊂ R× R. Suppose we wish to rotate the image by an angle that is not a multiple of 90◦. Then, we must look up color values at potentially non-integer positions, requiring the interpolation of the image to R × R. Example 13.6  Medical imaging . The output of a magnetic resonance imaging  MRI  device is an m × n × p grid of values representing the density of tissue at diﬀerent points; a theoretical model for this data is as a function f : R3 → R. We can extract the outer surface of a particular organ by ﬁnding the level set { cid:126 x : f   cid:126 x  = c} for some c. Finding this level set requires us to extend f to the entire voxel grid to ﬁnd exactly where it crosses c.  Grid-based interpolation applies the one-dimensional formulae from §13.1.3 one dimen- sion at a time. For example, bilinear interpolation in R2 applies linear interpolation in x1 and then x2  or vice versa :  Example 13.7  Bilinear interpolation . Suppose f takes on the following values:  f  0, 0  = 1  f  0, 1  = −3  f  1, 0  = 5  f  1, 1  = −11  and that in between f is obtained by bilinear interpolation. To ﬁnd f   1 interpolate in x1 to ﬁnd:  4 , 1  2  , we ﬁrst  4  f cid:18  1 f cid:18  1  4  , 0 cid:19  = , 1 cid:19  =  3 4 3 4  f  0, 0  +  f  1, 0  = 2  f  0, 1  +  f  1, 1  = −5.  1 4 1 4   Interpolation  cid:4  269  Next, we interpolate in x2:  f cid:18  1  4  ,  1  2 cid:19  =  1 2  f cid:18  1  4  , 0 cid:19  +  1 2  f cid:18  1  4  , 1 cid:19  = −  3 2  .  We receive the same output interpolating ﬁrst in x2 and second in x1.  Higher-order methods like bicubic and Lanczos interpolation use more polynomial terms but are slower to evaluate. For example, bicubic interpolation requires values from more grid points than just the four closest to  cid:126 x needed for bilinear interpolation. This additional expense can slow down image processing tools for which every lookup in memory incurs signiﬁcant computation time.  13.3 THEORY OF INTERPOLATION  Our treatment of interpolation has been fairly heuristic. While relying on our intuition for what a “reasonable” interpolation for a set of function values for the most part is acceptable, subtle issues can arise with diﬀerent interpolation methods that should be acknowledged.  13.3.1 Linear Algebra of Functions  We began our discussion by posing interpolation strategies using diﬀerent bases for the set of functions f : R → R. This analogy to vector spaces extends to a complete linear-algebraic theory of functions, and in many ways the ﬁeld of functional analysis essentially extends the geometry of Rn to sets of functions. Here, we will discuss functions of one variable, although many aspects of the extension to more general functions are easy to carry out.  Just as we can deﬁne notions of span and linear combination for functions, for ﬁxed  a, b ∈ R we can deﬁne an inner product of functions f  x  and g x  as follows:   cid:104 f, g cid:105  ≡ cid:90  b  a  f  x g x  dx.  We then can deﬁne the norm of a function f  x  to be  cid:107 f cid:107 2 ≡ cid:112  cid:104 f, f cid:105 . These constructions parallel the corresponding constructions on Rn; both the dot product  cid:126 x ·  cid:126 y and the inner product  cid:104 f, g cid:105  are obtained by multiplying the “elements” of the two multiplicands and summing—or integrating.  Example 13.8  Functional inner product . Take pn x  = xn to be the n-th monomial. Then, for a = 0 and b = 1,  This shows:  xn+m dx =  1  n + m + 1  .  0   cid:104 pn, pm cid:105  = cid:90  1  cid:28  pn   cid:107 pn cid:107   0  xn · xm dx = cid:90  1  cid:107 pm cid:107  cid:29  =  cid:104 pn, pm cid:105    cid:107 pn cid:107  cid:107 pm cid:107   pm  ,  =  cid:112  2n + 1  2m + 1   n + m + 1  .  This value is approximately 1 when n ≈ m but n  cid:54 = m, substantiating our earlier claim illustrated in Figure 13.1 that the monomials “overlap” considerably on [0, 1].   270  cid:4  Numerical Algorithms  Figure 13.10 The ﬁrst ﬁve Legendre polynomials, notated P0 x , . . . , P4 x .  Given this inner product, we can apply the Gram-Schmidt algorithm to ﬁnd an orthogo- nal basis for the set of polynomials, as we did in §5.4 to orthogonalize a set of vectors. If we take a = −1 and b = 1, applying Gram-Schmidt to the monomial basis yields the Legendre polynomials, plotted in Figure 13.10:  P2 x  =  P0 x  = 1 P1 x  = x 1 2 1 2 1 8  P3 x  =  P4 x  =  ...   3x2 − 1   5x3 − 3x   35x4 − 30x2 + 3  ...  ai =  cid:104 f, Pi cid:105   cid:104 Pi, Pi cid:105   .   cid:104 f, g cid:105 w ≡ cid:90  b  a  w x f  x g x  dx.  These polynomials have many useful properties thanks to their orthogonality. For example,  suppose we wish to approximate f  x  with a sum  cid:80 i aiPi x . If we wish to minimize  cid:107 f − cid:80 i aiPi cid:107 2 in the functional norm, this is a least-squares problem! By orthogonality of  the Legendre basis for R[x], our formula from Chapter 5 for projection onto an orthogonal basis shows:  Thus, approximating f using polynomials can be accomplished by integrating f against the members of the Legendre basis. In the next chapter, we will learn how this integral can be carried out numerically.  Given a positive function w x , we can deﬁne a more general inner product  cid:104 ·,· cid:105 w as  1  P0 x   P3 x   P4 x   1  x  P2 x   P1 x    Interpolation  cid:4  271  Figure 13.11 The ﬁrst ﬁve Chebyshev polynomials, notated T0 x , . . . , T4 x .  If we take w x  = 1√1−x2 with a = −1 and b = 1, then Gram-Schmidt on the monomials yields the Chebyshev polynomials, shown in Figure 13.11:  T0 x  = 1 T1 x  = x T2 x  = 2x2 − 1 T3 x  = 4x3 − 3x T4 x  = 8x4 − 8x2 + 1  ...  ...  Tk x  = cos k arccos x  .  A surprising identity holds for these polynomials:  This formula can be checked by explicitly verifying it for T0 and T1, and then inductively applying the observation:  Tk+1 x  = cos  k + 1  arccos x    = 2x cos k arccos x   − cos  k − 1  arccos x   by the identity  cos  k + 1 θ  = 2 cos kθ  cos θ  − cos  k − 1 θ   = 2xTk x  − Tk−1 x .  This three-term recurrence formula also gives a way to generate explicit expressions for the Chebyshev polynomials in the monomial basis.  Thanks to this trigonometric characterization of the Chebyshev polynomials, the minima and maxima of Tk oscillate between +1 and −1. Furthermore, these extrema are located at x = cos iπ k   the Chebyshev points  for i from 0 to k. This even distribution of extrema avoids oscillatory phenomena like that shown in Figure 13.4 when using a ﬁnite number of polynomial terms to approximate a function. More technical treatments of polynomial interpolation recommend placing samples xi for interpolation near Chebyshev points to obtain smooth output.   272  cid:4  Numerical Algorithms 13.3.2 Approximation via Piecewise Polynomials  Suppose we wish to approximate a function f  x  with a polynomial of degree n on an interval [a, b]. Deﬁne ∆x to be the spacing b−a. One measure of the error of an approximation is as a function of ∆x. If we approximate f with piecewise polynomials, this type of analysis tells us how far apart we should space the sample points to achieve a desired level of approximation. 2  , as in piecewise constant  Suppose we approximate f  x  with a constant c = f   a+b  interpolation. If we assume f cid:48  x  < M for all x ∈ [a, b], we have:  max  x∈[a,b]f  x  − c ≤ ∆x max x∈[a,b] ≤ M ∆x.  M by the mean value theorem  Thus, we expect O ∆x  error when using piecewise constant interpolation.  Suppose instead we approximate f using piecewise linear interpolation, that is, by taking  We can use the Taylor expansion about x to write expressions for f  a  and f  b :  ˜f  x  =  f  a  +  f  b .  b − x b − a  x − a b − a  f  a  = f  x  +  a − x f cid:48  x  + f  b  = f  x  +  b − x f cid:48  x  +   a − x 2f cid:48  cid:48  x  + O ∆x3   b − x 2f cid:48  cid:48  x  + O ∆x3 .  1 2 1 2  Substituting these expansions into the formula for ˜f  x  shows  ˜f  x  = f  x  +  1  = f  x  +    x − a  b − x 2 +  b − x  x − a 2 f cid:48  cid:48  x  + O ∆x3   2∆x 1  x − a  x − b f cid:48  cid:48  x  + O ∆x3  after simpliﬁcation. 2  This expression shows that linear interpolation holds up to O ∆x2 , assuming f cid:48  cid:48  is bounded. Furthermore, for all x ∈ [a, b] we have the bound x − ax − b ≤ ∆x2 4, implying an error bound proportional to ∆x2 8 for the second term. Generalizing this argument shows that approximation with a degree-n polynomial gener- ates O ∆xn+1  error. In particular, if f  x  is sampled at x0, x1, . . . , xn to generate a degree-n polynomial pn, then assuming x0 < x1 < ··· < xn, the error of such an approximation can be bounded as  f  x  − pn x  ≤  1   n + 1 ! cid:34  max  x∈[x0,xn] cid:89 k  x − xk cid:35  · cid:20  max  x∈[x0,xn]f  n+1  x  cid:21  ,  for any x ∈ [x0, xn]. 13.4 EXERCISES 13.1 Write the degree-three polynomial interpolating between the data points  −2, 15 ,   0,−1 ,  1, 0 , and  3,−2 . Hint: Your answer does not have to be written in the monomial basis.  13.2 Show that the interpolation from Example 13.7 yields the same result regardless of  whether x1 or x2 is interpolated ﬁrst.   Interpolation  cid:4  273  13.3  “Runge function”  Consider the function  1  f  x  ≡  1 + 25x2 .  Suppose we approximate f  x  using a degree-k polynomial pk x  through k + 1 points x0, . . . , xk with xi = 2i k − 1.  a  Plot pk x  for a few samples of k. Does increasing k improve the quality of the  approximation?   b  Specialize the bound at the end of §13.3.2 to show  max  x∈[−1,1]f  x  − pk x  ≤  1   k + 1 ! cid:34  max  x∈[−1,1] cid:89 i  x − xi cid:35  · cid:20  max  x∈[−1,1]f  k+1  x  cid:21  .  Does this bound get tighter as k increases?   c  Suggest a way to ﬁx this problem assuming we cannot move the xi’s.   d  Suggest an alternative way to ﬁx this problem by moving the xi’s.  13.4  “Inverse distance weighting”  Suppose we are given a set of distinct points  cid:126 x1, . . . ,  cid:126 xk ∈ Rn with labels y1, . . . , yk ∈ R. Then, one interpolation strategy deﬁnes an interpolant f   cid:126 x  as follows [108]:   cid:80  f   cid:126 x  ≡ cid:40  yi  cid:80   i wi  cid:126 x yi i wi  cid:126 x   if  cid:126 x =  cid:126 xi for some i otherwise,  where wi  cid:126 x  ≡  cid:107  cid:126 x −  cid:126 xi cid:107 −p  a  Argue that as p → ∞, the interpolant f   cid:126 x  becomes piecewise constant on the  for some ﬁxed p ≥ 1.  2  Voronoi cells of the  cid:126 xi’s.   b  Deﬁne the function  φ  cid:126 x, y  ≡ cid:32  cid:88 i  2 cid:33 1 p  .   y − yi 2  cid:107  cid:126 x −  cid:126 xi cid:107 p  Show that for ﬁxed  cid:126 x ∈ Rn\{ cid:126 x1, . . . ,  cid:126 xk}, the value f   cid:126 x  is the minimum of φ  cid:126 x, y  over all y.   c  Evaluating the sum in this formula can be expensive when k is large. Propose a modiﬁcation to the wi’s that avoids this issue; there are many possible techniques here.  13.5  “Barycentric Lagrange interpolation,” [12]  Suppose we are given k pairs   x1, y1 , . . . ,  xk, yk .   a  Deﬁne  cid:96  x  ≡ cid:81 k  j=1 x − xj . Show that the Lagrange basis satisﬁes  φi x  =  wi cid:96  x  x − xi  ,  for some weight wi depending on x1, . . . , xn. The value wi is known as the barycen- tric weight of xi.   274  cid:4  Numerical Algorithms   b  Suppose f  x  is the degree k − 1 polynomial through the given  xi, yi  pairs. Assuming you have precomputed the wi’s, use the result of the previous part to give a formula for Lagrange interpolation that takes O k  time to evaluate.  c  Use the result of 13.5b to write a formula for the constant function g x  ≡ 1.  d  Combine the results of the previous two parts to provide a third formula for f  x   that does not involve  cid:96  x . Hint: f  x  1 = f  x .  13.6  “Cubic Hermite interpolation”  In computer graphics, a common approach to draw- ing curves is to use cubic interpolation. Typically, artists design curves by specifying their endpoints as well as the tangents to the curves at the endpoints.   a  Suppose P  t  is the cubic polynomial:  P  t  = at3 + bt2 + ct + d.  Write a set of linear conditions on a, b, c, and d such that P  t  satisﬁes the following conditions for ﬁxed values of h0, h1, h2, and h3:  P  0  = h0 P  1  = h1  P  cid:48  0  = h2 P  cid:48  1  = h3.   b  Write the cubic Hermite basis for cubic polynomials {φ0 t , φ1 t , φ2 t , φ3 t }  such that P  t  satisfying the conditions from 13.6a can be written  P  t  = h0φ0 t  + h1φ1 t  + h2φ2 t  + h3φ3 t .  13.7  “Cubic blossom”  We continue to explore interpolation techniques suggested in the  previous problem.   a  Given P  t  = at3 + bt2 + ct + d, deﬁne a cubic blossom function F  t1, t2, t3  in  terms of {a, b, c, d} satisfying the following properties [102]: Symmetric: F  t1, t2, t3  = F  ti, tj, tk   Aﬃne: F  αu +  1 − α v, t2, t3  = αF  u, t2, t3  +  1 − α F  v, t2, t3   for any permutation  i, j, k  of {1, 2, 3} f  t  = F  t, t, t   Diagonal:   b  Now, deﬁne  p = F  0, 0, 0  r = F  0, 1, 1   q = F  0, 0, 1  s = F  1, 1, 1 .  Write expressions for f  0 , f  1 , f cid:48  0 , and f cid:48  1  in terms of p, q, r, and s.   c  Write a basis {B0 t , B1 t , B2 t , B3 t } for cubic polynomials such that given  a cubic blossom F  t1, t2, t3  of f  t  we can write  f  t  = F  0, 0, 0 B0 t  + F  0, 0, 1 B1 t  + F  0, 1, 1 B2 t  + F  1, 1, 1 B3 t .  The functions Bi t  are known as the cubic Bernstein basis.   d  Suppose F1 t1, t2, t3  and F2 t1, t2, t3  are the cubic blossoms of functions f1 t  and f2 t , respectively, and deﬁne  cid:126 F  t1, t2, t3  ≡  F1 t1, t2, t3 , F2 t1, t2, t3  . Consider the four points shown in Figure 13.12. By bisecting line segments and drawing new ones, show how to construct  cid:126 F  1 2, 1 2, 1 2 .   Interpolation  cid:4  275  Figure 13.12 Diagram for Exercise 13.7d.  DH 13.8 Consider the polynomial p x  = a0 + a1x + a2x2 + ··· + an−1xn−1. Alternatively, we  can write p x  in the Newton basis relative to x1, . . . , xn as  p x  = c1 + c2  x − x1  + c3  x − x1   x − x2  + ··· + cn   x − xi  ,  n−1 cid:89 i=1  where x1, . . . , xn are ﬁxed constants.   a  Argue why we can write any  n − 1 -st degree p x  in this form.  b  Find explicit expressions for c1, c2, and c3 in terms of x1, x2, and evaluations of p · . Based on these expressions  and computing more terms if needed , propose a pattern for ﬁnding ck.   c  Use function evaluation to deﬁne the zeroth divided diﬀerence of p as p [x1] =  p  x1 . Furthermore, deﬁne the ﬁrst divided diﬀerence of p as  p [x1, x2] =  p [x1] − p [x2]  .  x1 − x2  Finally, deﬁne the second divided diﬀerence as  p [x1, x2, x3] =  p [x1, x2] − p [x2, x3]  .  x1 − x3  Based on this pattern and the pattern you observed in the previous part, deﬁne p[xi, xi+1, . . . , xj] and use it to provide a formula for the coeﬃcients ck.   d  Suppose we add another point  xn+1, yn+1  and wish to recompute the Newton  interpolant. How many Newton coeﬃcients need to be recomputed? Why?  13.9  “Horner’s rule”  Consider the polynomial p x  ≡ a0 + a1x + a2x2 + ··· + akxk. For  ﬁxed x0 ∈ R, deﬁne c0, . . . , ck ∈ R recursively as follows:  ck ≡ ak ci ≡ ai + ci+1x0 ∀i < k.  Show c0 = p x0 , and compare the number of multiplication and addition operations needed to compute p x0  using this method versus the formula in terms of the ai’s.   cid:2 F  0, 0, 1    cid:2 F  0, 1, 1    cid:2 F  1, 1, 1    cid:2 F  0, 0, 0    276  cid:4  Numerical Algorithms  DH 13.10 Consider the L2 distance between polynomials f, g on [−1, 1], given by  f − g2 ≡ cid:20  cid:90  1  −1   f  x  − g x  2 dx cid:21 1 2  ,  which arises from the inner product  cid:104 f, g cid:105  =  cid:82  1 −1 f  x g x  dx. Let Pn be the vector space of polynomials of degree no more than n, endowed with this inner product. As we have discussed, polynomials {pi}m i=1 are orthogonal with respect to this inner product if for all i  cid:54 = j,  cid:104 pi, pj cid:105  = 0; we can systematically obtain a set of orthonormal polynomials using the Gram-Schmidt process.   a  Derive constant multiples of the ﬁrst three Legendre polynomials via Gram-  Schmidt orthogonalization on the monomials 1, x, and x2.   b  Suppose we wish to approximate a function f with a degree-n polynomial g. To do so, we can ﬁnd the g ∈ Pn that is the best least-squares ﬁt for f in the norm above. Write an optimization problem for ﬁnding g.   c  Suppose we construct the Gram matrix G with entries gij ≡  cid:104 pi, pj cid:105  for a basis of polynomials p1, . . . , pn ∈ Pn. How is G involved in solving Exercise 13.10b? What is the structure of G when p1, . . . , pn are the ﬁrst n Legendre polynomials?  DH 13.11 For a given n, the Chebyshev points are given by xk = cos cid:0  kπ  n cid:1  , where k ∈ {0, . . . , n}.   a  Show that the Chebyshev points are the projections onto the x axis of n evenly  spaced points on the upper half of the unit circle.   b  Suppose that we deﬁne the Chebyshev polynomials using the expression Tk x  ≡ cos k arccos x  . Starting from this expression, compute the ﬁrst four Chebyshev polynomials in the monomial basis.   c  Show that the Chebyshev polynomials you computed in the previous part are  orthogonal with respect to the inner product  cid:104 f, g cid:105  ≡ cid:82  1   d  Show that the extrema of Tn are located at Chebyshev points xk.  √1−x2 dx.  f  x g x   −1  13.12 We can use interpolation strategies to formulate methods for root-ﬁnding in one or  more variables.   a  Show how to recover the parameters a, b, c of the linear fractional transformation  f  x  ≡  x + a bx + c  going through the points  x0, y0 ,  x1, y1 , and  x2, y2 , either in closed form or by posing a 3 × 3 linear system of equations.   b  Find x4 such that f  x4  = 0.   c  Suppose we are given a function f  x  and wish to ﬁnd a root x∗ with f  x∗  = 0. Suggest an algorithm for root-ﬁnding using the construction in Exercise 13.12b.   C H A P T E R14  Integration and Differentiation  CONTENTS  14.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14.2 Quadrature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14.2.1 Interpolatory Quadrature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14.2.2 Quadrature Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14.2.3 Newton-Cotes Quadrature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14.2.4 Gaussian Quadrature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14.2.5 Adaptive Quadrature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14.2.6 Multiple Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14.2.7 Conditioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14.3 Diﬀerentiation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14.3.1 Diﬀerentiating Basis Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14.3.2 Finite Diﬀerences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14.3.3 Richardson Extrapolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14.3.4 Choosing the Step Size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14.3.5 Automatic Diﬀerentiation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14.3.6 Integrated Quantities and Structure Preservation . . . . . . . . . . . . . .  278 279 280 281 282 286 287 289 290 290 291 291 293 294 295 296  T HE previous chapter developed tools for predicting values of a function f   cid:126 x  given a  sampling of points   cid:126 xi, f   cid:126 xi   in the domain of f . Such methods are useful in themselves for completing functions that are known to be continuous or diﬀerentiable but whose values only are sampled at a set of isolated points, but in some cases we instead wish to compute “derived quantities” from the sampled function. Most commonly, many applications must approximate the integral or derivatives of f rather than its values.  There are many applications in which numerical integration and diﬀerentiation play key roles for computation. In the most straightforward instance, some well-known functions are deﬁned as integrals. For instance, the “error function” given by the cumulative distribution of a bell curve is deﬁned as:  erf x  ≡  2  √π cid:90  x  0  e−t2  dt.  Approximations of erf x  are needed in statistical methods, and one reasonable approach to ﬁnding these values is to compute the integral above numerically.  Other times, numerical approximations of derivatives and integrals are part of a larger system. For example, methods we will develop in future chapters for approximating so- lutions to diﬀerential equations will depend strongly on discretizations of derivatives. In  277   278  cid:4  Numerical Algorithms  computational electrodynamics, integral equations for an unknown function φ  cid:126 y  given a kernel K  cid:126 x,  cid:126 y  and function f   cid:126 x  are expressed as the relationship  f   cid:126 x  = cid:90 Rn  K  cid:126 x,  cid:126 y φ  cid:126 y  d cid:126 y.  Equations in this form are solved for φ to estimate electric and magnetic ﬁelds, but unless the φ and K are very special we cannot hope to work with such an integral in closed form. Hence, these methods typically discretize φ and the integral using a set of samples and then solve the resulting discrete system of equations.  In this chapter, we will develop methods for numerical integration and diﬀerentiation given a sampling of function values. We also will suggest strategies to evaluate how well we can expect approximations of derivatives and integrals to perform, helping formalize intuition for their relative quality and eﬃciency in diﬀerent circumstances or applications.  14.1 MOTIVATION  It is not hard to encounter applications of numerical integration and diﬀerentiation, given how often the tools of calculus appear in physics, statistics, and other ﬁelds. Well-known formulas aside, here we suggest a few less obvious places requiring algorithms for integration and diﬀerentiation.  Example 14.1  Sampling from a distribution . Suppose we are given a probability dis- tribution p t  on the interval [0, 1]; that is, if we randomly sample values according to this distribution, we expect p t  to be proportional to the number of times we draw a value near t. A common task is to generate random numbers distributed like p t .  Rather than develop a specialized sampling method every time we receive a new p t , it is possible to leverage a single uniform sampling tool to sample from nearly any distribution on [0, 1]. We deﬁne the cumulative distribution function  CDF  of p to be  F  t  = cid:90  t  0  p x  dx.  If X is a random number distributed evenly in [0, 1], one can show that F −1 X  is dis- tributed like p, where F −1 is the inverse of F . That is, if we can approximate F or F −1, we can generate random numbers according to an arbitrary distribution p.  Example 14.2  Optimization . Most of our methods for minimizing and ﬁnding roots of a function f   cid:126 x  require computing not only values f   cid:126 x  but also gradients ∇f   cid:126 x  and even Hessians Hf   cid:126 x . BFGS and Broyden’s method build up rough approximations of derivatives of f during optimization. When f changes rapidly in small neighborhoods, however, it may be better to approximate ∇f directly near the current iterate  cid:126 xk rather than using values from potentially far-away iterates  cid:126 x cid:96  for  cid:96  < k, which can happen as BFGS or Broyden slowly build up derivative matrices.  Example 14.3  Rendering . The rendering equation from computer graphics and ray tracing is an integral equation expressing conservation of light energy [70]. As it was originally presented, the rendering equation states:  I  cid:126 x,  cid:126 y  = g  cid:126 x,  cid:126 y  cid:20 ε  cid:126 x,  cid:126 y  + cid:90 S  ρ  cid:126 x,  cid:126 y,  cid:126 z I  cid:126 y,  cid:126 z  d cid:126 z cid:21  .   Integration and Diﬀerentiation  cid:4  279  Here I  cid:126 x,  cid:126 y  is proportional to the intensity of light going from point  cid:126 y to point  cid:126 x in a scene. The functions on the right-hand side are:  g  cid:126 x,  cid:126 y  A geometry term accounting, e.g., for objects occluding the  path from  cid:126 x to  cid:126 y  ε  cid:126 x,  cid:126 y  The light emitted directly from  cid:126 x to  cid:126 y  ρ  cid:126 x,  cid:126 y,  cid:126 z  A scattering term giving the amount of light scattered to point  cid:126 x by a patch of surface at location  cid:126 z from light lo- cated at  cid:126 z  S = ∪iSi The set of surfaces Si in the scene  Many rendering algorithms can be described as approximate strategies for solving this integral equation.  Example 14.4  Image processing . Suppose we think of an image or photograph as a function of two variables I x, y  giving the brightness of the image at each position  x, y . Many classical image processing ﬁlters can be thought of as convolutions, given by   I ∗ g  x, y  = cid:90  cid:90 R2  I u, v g x − u, y − v  du dv.  For example, to blur an image we can take g to be a Gaussian or bell curve; in this case  I∗g  x, y  is a weighted average of the colors of I near the point  x, y . In practice, images are sampled on discrete grids of pixels, so this integral must be approximated.  Example 14.5  Bayes’ Rule . Suppose X and Y are continuously valued random vari- ables; we can use P  X  and P  Y   to express the probabilities that X and Y take particular values. Sometimes, knowing X may aﬀect our knowledge of Y . For instance, if X is a pa- tient’s blood pressure and Y is a patient’s weight, then knowing a patient has high weight may suggest that he or she also has high blood pressure. In this situation, we can write conditional probability distributions P  XY    read “the probability of X given Y ”  ex- pressing such relationships. A foundation of modern probability theory states that P  XY   and P  Y X  are related  by Bayes’ rule  P  XY   =  P  Y X P  X    cid:82  P  Y X P  X  dX  .  Estimating the integral in the denominator can be a serious problem in machine learning algorithms where the probability distributions take complex forms. Approximate and often randomized integration schemes are needed for algorithms in parameter selection that use this value as part of a larger optimization technique [63].  14.2 QUADRATURE  We will begin by considering the problem of numerical integration, or quadrature. This problem—in a single variable—can be expressed as: “Given a sampling of n points from some a f  x  dx.” In the previous section, we presented  function f  x , ﬁnd an approximation of cid:82  b  some applications that reduce to exactly this problem.  There are a few variations of this setup that require slightly diﬀerent treatment or  adaptation:   280  cid:4  Numerical Algorithms    The endpoints a and b may be ﬁxed, or we may wish to ﬁnd a quadrature scheme  that eﬃciently can approximate integrals for many  a, b  pairs.    We may be able to query f  x  at any x but wish to approximate the integral using relatively few samples, or we may be given a list of precomputed pairs  xi, f  xi   and are constrained to using these data points in our approximation.  These considerations should be kept in mind as we design assorted quadrature techniques.  14.2.1 Interpolatory Quadrature  Many of the interpolation strategies developed in the previous chapter can be extended to methods for quadrature. Suppose we write a function f  x  in terms of a set of basis functions φi x :  f  x  = cid:88 i  aiφi x .  Then, we can ﬁnd the integral of f as follows:   cid:90  b  a  aiφi x  cid:35  dx by deﬁnition of f φi x  dx cid:35  by swapping the sum and the integral  a  cid:34  cid:88 i f  x  dx = cid:90  b ai cid:34  cid:90  b = cid:88 i ciai if we make the deﬁnition ci ≡ cid:90  b = cid:88 i  φi x  dx.  a  a  In other words, the integral of f  x  written in a basis is a weighted sum of the integrals of the basis functions making up f .  Example 14.6  Monomials . Suppose we write f  x  = cid:80 k akxk. We know  Applying the formula above, we can write  0   cid:90  1  cid:90  1  0  xk dx =  1  .  k + 1  f  x  dx = cid:88 k  ak  .  k + 1  In the more general notation above, we have taken ck = 1 k+1 . This formula shows that the integral of f  x  in the monomial basis can be computed directly via a weighted sum of the coeﬃcients ak.  Integration schemes derived using interpolatory basis functions are known as interpola- tory quadrature rules; nearly all the methods we will present below can be written this way.  We can encounter a chicken-and-egg problem if the integral cid:82  φi x  dx itself is not known  in closed form. Certain methods in higher-order ﬁnite elements deal with this problem by putting extra computational time into making a high-quality numerical approximation of the integral of a single φi. Then, since all the φ’s have similar form, these methods ap- ply change-of-coordinates formulas to compute integrals of the remaining basis functions. The canonical integral can be approximated oﬄine using a high-accuracy scheme and then reused during computations where timing matters.   Integration and Diﬀerentiation  cid:4  281  14.2.2 Quadrature Rules  Our discussion above suggests the following form for a quadrature rule approximating the integral of f on some interval given a set of sample locations xi:  Q[f ] ≡ cid:88 i  wif  xi .  Diﬀerent weights wi yield diﬀerent approximations of the integral, which we hope become increasingly similar as the xi’s are sampled more densely. From this perspective, the choices of {xi} and {wi} determine a quadrature rule. The classical theory of integration suggests that this formula is a reasonable starting point. For example, the Riemann integral presented in introductory calculus takes the form   cid:90  b  a  f  x  dx ≡ lim  f  ˜xk  xk+1 − xk .  ∆xk→0 cid:88 k  Here, the interval [a, b] is partitioned into pieces a = x1 < x2 < ··· < xn = b, where ∆xk = xk+1 − xk, and ˜xk is any point in [xk, xk+1]. For a ﬁxed set of xk’s before taking the limit, this integral is in the Q[f ] form above. There are many ways to choose the form of Q[·], as we will see in the coming section and as we already have seen for interpolatory quadrature. If we can query f for its values anywhere, then the xi’s and wi’s can be chosen strategically to sample f in a near-optimal way, but even if the xi’s are ﬁxed, there exist many ways to choose the weights wi with diﬀerent advantages and disadvantages.  Example 14.7  Method of undetermined coeﬃcients . Suppose we ﬁx x1, . . . , xn and wish  to ﬁnd a reasonable set of weights wi so that cid:80 i wif  xi  approximates the integral of f for reasonably smooth f : [a, b] → R. An alternative to interpolatory quadrature is the method of undetermined coeﬃcients. In this strategy, we choose n functions f1 x , . . . , fn x  whose integrals are known, and require that the quadrature rule recovers the integrals of these functions exactly:  a  a   cid:90  b  cid:90  b  cid:90  b  a  f1 x  dx = w1f1 x1  + w2f1 x2  + ··· + wnf1 xn   f2 x  dx = w1f2 x1  + w2f2 x2  + ··· + wnf2 xn   ...  ...  fn x  dx = w1fn x1  + w2fn x2  + ··· + wnfn xn .  The n expressions above create an n × n linear system of equations for the unknown wi’s. One common choice is to take fk x  ≡ xk−1, that is, to make sure that the quadrature scheme recovers the integrals of low-order polynomials. As in Example 14.6,   cid:90  b  a  xk dx =  bk+1 − ak+1  .  k + 1   282  cid:4  Numerical Algorithms  Figure 14.1 Closed and open Newton-Cotes quadrature schemes diﬀer by where they place the samples xi on the interval [a, b]; here we show the two samplings for n = 8.  Thus, we solve the following linear system of equations for the wi’s:  w1 + w2 + ··· + wn = b − a  x1w1 + x2w2 + ··· + xnwn = x2 1w1 + x2 nwn =  2w2 + ··· + x2  b2 − a2 b3 − a3  2  3 ...  n  bn − an  .  ...  xn−1 1 w1 + xn−1  2 w2 + ··· + xn−1  n wn =  In matrix form, this system is    1 x1 x2 1 ... xn−1 1  1 x2 x2 2 ... xn−1 2  ··· 1 ··· xn x2 ··· n ... . . . ··· xn−1  n      w1 w2 ... wn    =    b − a  1  1  2  b2 − a2  3  b3 − a3   ...  .    1  n  bn − an  This is the transpose of the Vandermonde system discussed in §13.1.1. 14.2.3 Newton-Cotes Quadrature Quadrature rules that integrate the result of polynomial interpolation when the x cid:48 is are evenly spaced in [a, b] are known as Newton-Cotes quadrature rules. As illustrated in Fig- ure 14.1, there are two reasonable choices of evenly spaced samples:    Closed Newton-Cotes quadrature places xi’s at a and b. In particular, for k ∈  {1, . . . , n} we take    Open Newton-Cotes quadrature does not place an xi at a or b:  xk ≡ a +   k − 1  b − a   .  n − 1  xk ≡ a +  k b − a  n + 1  .  The Newton-Cotes formulae compute the integral of the polynomial interpolant approxi- mating the function on a to b through these points; the degree of the polynomial must be n − 1 to keep the quadrature rule well-deﬁned. There is no inherent advantage to using  x1  x2  x3  x4  x5  x6  x7  x8  x1  x2  x3  x4  x5  x6  x7  x8  Closed  Open   Integration and Diﬀerentiation  cid:4  283  Figure 14.2 Newton-Cotes quadrature schemes; the approximated integral based on the  xi, f  xi   pairs shown is given by the area of the gray region.  closed versus open Newton-Cotes rules; the choice between these options generally depends on which set of samples is available.  We illustrate the integration rules below in Figure 14.2. We will keep n relatively small to avoid oscillation and noise sensitivity that occur when ﬁtting high-degree polynomials to a set of data points. Then, as in piecewise polynomial interpolation, we will then chain together small pieces into composite rules when integrating over a large interval [a, b].  Closed rules. Closed Newton-Cotes quadrature strategies require n ≥ 2 to avoid dividing by zero. The two lowest-order closed integrators are the most common:    The trapezoidal rule for n = 2  so x1 = a and x2 = b  is constructed by linearly interpolating from f  a  to f  b . It eﬀectively computes the area of a trapezoid via the formula  2   Simpson’s rule is used for n = 3, with sample points  f  x  dx ≈  b − a   f  a  + f  b   .   cid:90  b  a  x1 = a  x2 =  a + b  2  x3 = b.  Integrating the parabola that goes through these three points yields   cid:90  b  a  f  x  dx ≈  b − a  6  cid:18 f  a  + 4f cid:18  a + b  2  cid:19  + f  b  cid:19  .  Open rules. By far the most common rule for open quadrature is the midpoint rule, which takes n = 1 and approximates an integral with the signed area of a rectangle through the midpoint of the integration interval [a, b]:   cid:90  b  a  2  cid:19  . f  x  dx ≈  b − a f cid:18  a + b  Larger values of n yield formulas similar to Simpson’s rule and the trapezoidal rule.  f  x2   f  x3   f  x1   f  x1   f  x2   f  x1   x1  x2  x1  x2  x3  x1  Trapezoidal rule  Simpson’s rule  Midpoint rule   284  cid:4  Numerical Algorithms  Composite integration. We usually wish to integrate f  x  with more than one, two, or three sample points xi. To do so, we can construct a composite rule out of the midpoint or trapezoidal rules, as illustrated in Figure 14.3, by summing up smaller pieces along each interval. For example, if we subdivide [a, b] into k intervals, then we can take ∆x ≡ b−a and xi ≡ a +  i − 1 ∆x. Then, the composite midpoint rule is  k  Similarly, the composite trapezoidal rule is  f cid:18  xi+1 + xi  2   cid:19  ∆x.  a   cid:90  b  f  x  dx ≈  k cid:88 i=1 k cid:88 i=1 cid:18  f  xi  + f  xi+1  = ∆x cid:18  1  2  2   cid:19  ∆x  f  a  + f  x2  + f  x3  + ··· + f  xk  + after reorganizing the sum.  1 2  f  b  cid:19    cid:90  b  a  f  x  dx ≈  An alternative derivation of the composite midpoint rule applies the interpolatory quadra- ture formula from §14.2.1 to piecewise constant interpolation; the composite version of the trapezoidal rule comes from piecewise linear interpolation. The composite version of Simpson’s rule, also illustrated in Figure 14.3, chains together three points at a time to make parabolic approximations. Adjacent parabolas meet at every other xi and may not share tangents. After combining terms, this quadrature rule becomes:   cid:90  b  a  f  x  dx ≈  [f  a  + 4f  x2  + 2f  x3  + 4f  x4  + 2f  x5  + ··· + 4f  xk  + f  b ] .  ∆x 6  Accuracy. We have developed a number of quadrature rules that combine the same set of f  xi ’s with diﬀerent weights to obtain potentially unequal approximations of the integral of f . Each approximation is based on a diﬀerent interpolatory construction, so it is unclear that any of these rules is better than any other. Thus, we need to compute error estimates characterizing their respective behavior. We will study the basic Newton-Cotes integrators above to show how such comparisons might be carried out.  First, consider the midpoint quadrature rule on a single interval [a, b]. Deﬁne c ≡ 1  2  a+b .  The Taylor series of f about c is:  f  x  = f  c  + f cid:48  c  x − c  + After integration, by symmetry about c, the odd-numbered derivatives drop out:  f cid:48  cid:48  cid:48  c  x − c 3 +  f cid:48  cid:48  c  x − c 2 +  f cid:48  cid:48  cid:48  cid:48  c  x − c 4 + ···  1 24  1 6  1 2   cid:90  b  a  f  x  dx =  b − a f  c  +  f cid:48  cid:48  c  b − a 3 + The ﬁrst term of this sum is exactly the estimate of  cid:82  b  1 24  rule, so based on this formula we can conclude that this rule is accurate up to O ∆x3 .  a f  x  dx provided by the midpoint  1  f cid:48  cid:48  cid:48  cid:48  c  b − a 5 + ···  1920   Integration and Diﬀerentiation  cid:4  285  Figure 14.3 Composite Newton-Cotes quadrature rules; each rule is marked with the number of samples  xi, f  xi   used to approximate the integral over six subintervals.  a  a  a  a  Actual integral  Composite midpoint rule  6 samples   Composite trapezoidal rule  7 samples   Composite Simpson’s rule  7 samples   f  x   f  x   f  x   f  x   x  x  x  x  b  b  b  b   286  cid:4  Numerical Algorithms  Continuing, plugging a and b into the Taylor series for f  x  about c shows:  f  a  = f  c  + f cid:48  c  a − c  + f  b  = f  c  + f cid:48  c  b − c  +  1 2 1 2  f cid:48  cid:48  c  a − c 2 + f cid:48  cid:48  c  b − c 2 +  1 f cid:48  cid:48  cid:48  c  a − c 3 + ··· 6 1 f cid:48  cid:48  cid:48  c  b − c 3 + ··· 6  Adding these together and multiplying both sides by 1  2  b − a  shows:  f  a  + f  b    b − a   2  = f  c  b − a  + = f  c  b − a  +  f cid:48  cid:48  c  b − a   a − c 2 +  b − c 2  + ··· f cid:48  cid:48  c  b − a 3 + ··· by deﬁnition of c.  1 4 1 8  The f cid:48  c  term vanishes for the ﬁrst line by substituting c = 1 2  a + b . Now, the left- hand side is the trapezoidal rule integral estimate, and the right-hand side agrees with the a f  x  dx up to the cubic term. Hence, the trapezoidal rule is also O ∆x3  accurate in a single interval. A similar argument provides error estimate for Simpson’s rule; after somewhat more involved algebra, one can show Simpson’s rule has error scaling like O ∆x5 .  Taylor series for cid:82  b  We pause here to highlight a surprising result: The trapezoidal and midpoint rules have the same order of accuracy! Examining the third-order term shows that the midpoint rule is approximately two times more accurate than the trapezoidal rule, making it marginally preferable for many calculations. This observation seems counterintuitive, since the trape- zoidal rule uses a linear approximation while the midpoint rule uses a constant approxima- tion. As you will see in Exercise 14.1, however, the midpoint rule recovers the integrals of linear functions, explaining its extra degree of accuracy.  A notable caveat applies to this sort of analysis. Taylor’s theorem only applies when ∆x is small ; otherwise, the analysis above is meaningless. When a and b are far apart, to return to the case of small ∆x, we can divide [a, b] into many intervals of width ∆x and apply the composite quadrature rules. The total number of intervals is b−a ∆x, so we must multiply error estimates by 1 ∆x in this case. Hence, the following orders of accuracy hold:    Composite midpoint: O ∆x2    Composite trapezoid: O ∆x2    Composite Simpson: O ∆x4   14.2.4 Gaussian Quadrature  In some applications, we can choose the locations xi where f is sampled. In this case, we can optimize not only the weights for the quadrature rule but also the locations xi to get the highest quality. This observation leads to challenging but theoretically appealing quadrature rules, such as the Gaussian quadrature technique explored below.  The details of this technique are outside the scope of our discussion, but we provide one path to its derivation. Generalizing Example 14.7, suppose that we wish to optimize x1, . . . , xn and w1, . . . , wn simultaneously to increase the order of an integration scheme. Now we have 2n instead of n unknowns, so we can enforce equality for 2n examples:   cid:90  b  a  f1 x  dx = w1f1 x1  + w2f1 x2  + ··· + wnf1 xn    Integration and Diﬀerentiation  cid:4  287  a   cid:90  b  cid:90  b  a  f2 x  dx = w1f2 x1  + w2f2 x2  + ··· + wnf2 xn   ...  ...  f2n x  dx = w1f2n x1  + w2f2n x2  + ··· + wnf2n xn .  Since both the xi’s and the wi’s are unknown, this system of equations is not linear and must be solved using more involved methods.  Example 14.8  Gaussian quadrature . If we wish to optimize weights and sample loca- tions for polynomials on the interval [−1, 1], we would have to solve the following system of polynomials [58]:  −1  w1 + w2 = cid:90  1 w1x1 + w2x2 = cid:90  1 2 = cid:90  1 2 = cid:90  1  1 + w2x3  1 + w2x2  w1x2  w1x3  −1  −1  −1  1 dx = 2  x dx = 0  x2 dx =  2 3  x3 dx = 0.  Systems like this can have multiple roots and other degeneracies that depend not only on the fi’s  typically polynomials  but also on the interval over which the integral is ap- proximated. These rules are not progressive, in that the xi’s chosen to integrate using n data points have little in common with those used to integrate using k data points when k  cid:54 = n. So, it is diﬃcult to reuse data to achieve a better estimate with this quadrature rule. On the other hand, Gaussian quadrature has the highest possible degree of accuracy for ﬁxed n. Kronrod quadrature rules adapt Gaussian points to the progressive case but no longer have the highest possible order of accuracy.  14.2.5 Adaptive Quadrature  Our discussion of Gaussian quadrature suggests that the placement of the xi’s can aﬀect the quality of a quadrature scheme. There still is one piece of information we have not used, however: the function values f  xi . That is, diﬀerent classes or shapes of functions may require diﬀerent integration methods, but so far our algorithms have not attempted to detect this structure into account in any serious way.  With this situation in mind, adaptive quadrature strategies examine the current esti- mate of an integral and generate new xi’s where the integrand appears to be undersampled. Strategies for adaptive integration often compare the output of multiple quadrature tech- niques, e.g., trapezoid and midpoint, with the assumption that they agree where sampling of f is suﬃcient, as illustrated in Figure 14.4. If they do not agree to some tolerance on a given interval, an additional sample point is generated and the integral estimates are updated.  Figure 14.5 outlines a bisection technique for adaptive quadrature. The idea is to sub- divide intervals in which the integral estimate appears to be inaccurate recursively. This method must be accompanied with special consideration when the level of recursion is too deep, accounting for the case of a function f  x  that is noisy even at tiny scale.   288  cid:4  Numerical Algorithms  Figure 14.4 The trapezoidal and midpoint rules disagree considerably on the left subinterval  top , so adaptive quadrature methods subdivide in that region to get better accuracy  bottom .  Figure 14.5 An outline for recursive quadrature via bisection. This method can use any of the quadrature rules discussed in this chapter; error estimates can be con- structed, e.g., by evaluating the diﬀerence between using diﬀerent quadrature rules for the same interval. The parameter ε0 is a tolerance for the quality of the quadra- ture rule.  e r o f e B  r e t f A  a  a  f  x   x  f  x   x  b  b  a  a  f  x   x  f  x   x  b  b  elurtniopdiM  elurladiozeparT  function Recursive-Quadrature f  x , a, b, ε0   I ← Quadrature-Rule f  x , a, b  E ← Error-Estimate f  x , I, a, b  if E < ε0 then  return I  else  2  a + b   c ← 1 I1 ← Recursive-Quadrature f  x , a, c, ε0  I2 ← Recursive-Quadrature f  x , c, b, ε0  return I1 + I2   Integration and Diﬀerentiation  cid:4  289  Figure 14.6 Pseudocode for Monte Carlo integration of a function f   cid:126 x  : Ω → R. 14.2.6 Multiple Variables Many times we wish to integrate functions f   cid:126 x  where  cid:126 x ∈ Rn. For example, when n = 2 we might integrate over a rectangle by computing   cid:90  b a  cid:90  d  c  f  x, y  dx dy.  More generally, we might wish to ﬁnd an integral cid:82 Ω f   cid:126 x  d cid:126 x, where Ω is some subset of Rn.  A “curse of dimensionality” makes integration more diﬃcult as the dimension increases. The number of sample locations  cid:126 xi of f   cid:126 x  needed to achieve comparable quadrature accu- racy for an integral in Rn increases exponentially in n. This observation may be disheart- ening but is somewhat reasonable: The more input dimensions for f , the more samples are needed to understand its behavior in all dimensions.  This issue aside, one way to extend single-variable integration to Rk is via the iterated integral. For example, if f  x, y  is a function of two variables, suppose we wish to ﬁnd c f  x, y  dx dy. For ﬁxed y, we can approximate the inner integral over x using a one- dimensional quadrature rule; then, we integrate these values over y using another quadrature rule. The inner and outer integrals both induce some error, so we may need to sample the  cid:126 xi’s more densely than in one dimension to achieve desired output quality.   cid:82  b a cid:82  d  Alternatively, just as we subdivided [a, b] into intervals, we can subdivide Ω into triangles and rectangles in 2D, polyhedra or boxes in 3D, and so on and use interpolatory quadrature rules in each piece. For instance, one popular option is to integrate barycentric interpolants  §13.2.2 , since this integral is known in closed form. When n is high, however, it is not practical to divide the domain as suggested. In this case, we can use the randomized Monte Carlo method. In the most basic version of this method, we generate k random points  cid:126 xi ∈ Ω with uniform probability. Averaging the values  f   cid:126 xi  and scaling the result by the volume Ω of Ω yields an approximation of cid:82 Ω f   cid:126 x  d cid:126 x:   cid:90 Ω  f   cid:126 x  d cid:126 x ≈ Ω  k  f   cid:126 xi .  k cid:88 i=1  This approximation converges like 1 √k as more sample points are added—independent of the dimension of Ω! So, in large dimensions the Monte Carlo estimate is preferable to the deterministic quadrature methods above. A proof of convergence requires some notions from probability theory, so we refer the reader to [103] or a similar reference for discussion.  One advantage of Monte Carlo techniques is that they are easily implemented and ex- tended. Figure 14.6 provides a pseudocode implementation of Monte Carlo integration over  function Monte-Carlo-Integral f   cid:2 x , Ω ⊆ [a, b]n, p    cid:3  Number of points inside Ω and average value Sample p points   cid:3   p,...,  c, d ← 0 for k ← 1, 2   cid:2 x ← Uniform-Random [a, b]n  if Inside  cid:2 x, Ω  then   cid:3  Otherwise reject  c ← c + 1 d ← d + f   cid:2 x  p  b − a n  v ← c y ← d return vy  c   cid:3  Estimate of Ω  cid:3  Average observed f   cid:2 x    290  cid:4  Numerical Algorithms  a region Ω ⊆ [a, b]n. Even if we do not have a method for producing uniform samples in Ω directly, the more general integral can be carried out by sampling in the box [a, b]n and rejecting those samples outside Ω. This sampling is inappropriate when Ω is small relative to the bounding box [a, b]n, since the odds of randomly drawing a point in Ω decrease in this case. To improve conditioning of this case, more advanced techniques bias their samples of [a, b]n based on evidence of where Ω takes the most space and where f   cid:126 x  is nontrivial.  Iterated integration can be eﬀective for low-dimensional problems, and Monte Carlo methods show the greatest advantage in high dimensions. In between these two regimes, the choice of integrators is less clear. One compromise that samples less densely than iterated integration without resorting to randomization is the sparse grid or Smolyak grid method, designed to reduce the eﬀect of the curse of dimensionality on numerical quadrature. We refer the reader to [114, 47] for discussion of this advanced technique.  14.2.7 Conditioning We have evaluated the quality of a quadrature method by bounding its accuracy like O ∆xk  for small ∆x. By this metric, a set of quadrature weights with large k is preferable. Another measure discussed in [58] and elsewhere, however, balances out the accuracy measurements obtained using Taylor arguments by considering the stability of a quadrature method under perturbations of the function being integrated.  Consider the quadrature rule Q[f ] ≡ cid:80 i wif  xi . Suppose we perturb f to some other ˆf . Deﬁne  cid:107 f − ˆf cid:107 ∞ ≡ maxx∈[a,b] f  x  − ˆf  x . Then, =  cid:80 i wi f  xi  − ˆf  xi   ≤  cid:80 i wif  xi  − ˆf  xi  ≤  cid:107   cid:126 w cid:107 ∞ since f  xi  − ˆf  xi  ≤  cid:107 f − ˆf cid:107 ∞ by deﬁnition.  Q[f ] − Q[ ˆf ]  cid:107 f − ˆf cid:107 ∞  by the triangle inequality   cid:107 f − ˆf cid:107 ∞   cid:107 f − ˆf cid:107 ∞  According to this bound, the most stable quadrature rules are those with small weights  cid:126 w. If we increase the order of quadrature accuracy by augmenting the degree of the poly- nomial used in Newton-Cotes quadrature, the conditioning bound  cid:107   cid:126 w cid:107 ∞ generally becomes less favorable. In degenerate circumstances, the wi’s even can take negative values, echo- ing the degeneracies of high-order polynomial interpolation. Thus, in practice we usually prefer composite quadrature rules summing simple estimates from many small subinter- vals to quadrature from higher-order interpolants, which can be unstable under numerical perturbation.  14.3 DIFFERENTIATION f  x  on cid:82  b perspective, one can show that the integral cid:82  f  x  dx generally has lower frequencies than  Numerical integration is a relatively stable problem, in that the inﬂuence of any single value a f  x  dx shrinks to zero as a and b become far apart. Approximating the derivative of a function f cid:48  x , on the other hand, has no such property. From the Fourier analysis  f , while diﬀerentiating to produce f cid:48  ampliﬁes the frequency content of f , making sampling constraints, conditioning, and stability particularly challenging for approximating f cid:48 .  Despite the challenging circumstances, approximations of derivatives usually are rela- tively easy to implement and can be stable under suﬃcient smoothness assumptions. For   Integration and Diﬀerentiation  cid:4  291  Figure 14.7 If a function is written in the basis of piecewise-linear “hat” functions ψi x , then its derivative can be written in the basis of piecewise constant functions ψ cid:48 i x .  example, while developing the secant rule, Broyden’s method, and so on we used approxi- mations of derivatives and gradients to help guide optimization routines with success on a variety of objectives.  Here, we will focus on approximating f cid:48  for f : R → R. Finding gradients and Jacobians usually is carried out by diﬀerentiating in one dimension at a time, eﬀectively reducing to the one-dimensional problem.  14.3.1 Differentiating Basis Functions  From a mathematical perspective, perhaps the simplest use case for numerical diﬀerentiation involves functions that are constructed using interpolation formulas. As in §14.2.1, if f  x  =   cid:80 i aiφi x , then by linearity  f cid:48  x  = cid:88 i  aiφ cid:48 i x .  In other words, the functions φ cid:48 i form a basis for derivatives of functions written in the φi basis!  This phenomenon often connects diﬀerent interpolatory schemes, as in Figure 14.7. For example, piecewise linear functions have piecewise constant derivatives, polynomial func- tions have polynomial derivatives of lower degree, and so on. In future chapters, we will see that this structure strongly inﬂuences discretizations of diﬀerential equations.  14.3.2 Finite Differences  A more common situation is that we have a function f  x  that we can query but whose derivatives are unknown. This often happens when f takes on a complex form or when a user provides f  x  as a subroutine without analytical information about its structure.  The deﬁnition of the derivative suggests a reasonable approximation  f cid:48  x  ≡ lim h→0  f  x + h  − f  x   .  h  As we might expect, for a ﬁnite h > 0 with small h the expression in the limit provides an approximation of f cid:48  x .  To substantiate this intuition, use Taylor series to write  f  x + h  = f  x  + f cid:48  x h +  1 2  f cid:48  cid:48  x h2 + ··· .  ψi x   xi  ψ cid:2 i x   xi   292  cid:4  Numerical Algorithms  Rearranging this expression shows  Thus, the following forward diﬀerence approximation of f cid:48  has linear convergence:  f cid:48  x  =  f  x + h  − f  x   + O h .  h  f cid:48  x  ≈  f  x + h  − f  x   .  f cid:48  x  ≈  f  x  − f  x − h   .  h  h  Similarly, ﬂipping the sign of h shows that backward diﬀerences also have linear convergence:  We can improve this approximation by combining the forward and backward estimates.  By Taylor’s theorem,  f  x + h  = f  x  + f cid:48  x h +  f cid:48  cid:48  x h2 +  1 2 1 2  1 6 1 6  f cid:48  cid:48  cid:48  x h3 + ··· f cid:48  cid:48  cid:48  x h3 + ···  f  x − h  = f  x  − f cid:48  x h +  f cid:48  cid:48  x h2 −  =⇒ f  x + h  − f  x − h  = 2f cid:48  x h + =⇒  f  x + h  − f  x − h   2h  = f cid:48  x  + O h2 .  1 3  f cid:48  cid:48  cid:48  x h3 + ···  Hence, centered diﬀerences approximate f cid:48  x  with quadratic convergence; this is the highest order of convergence we can expect to achieve with a single divided diﬀerence. We can, however, achieve more accuracy by evaluating f at other points, e.g., x + 2h, at the cost of additional computation time, as explored in §14.3.3. we add together the Taylor expansions of f  x + h  and f  x − h , then  Approximations of higher-order derivatives can be derived via similar constructions. If  f  x + h  + f  x − h  = 2f  x  + f cid:48  cid:48  x h2 + O h3   f  x + h  − 2f  x  + f  x − h   = f cid:48  cid:48  x  + O h .  =⇒  h2  To construct similar combinations for higher derivatives, one trick is to notice that our  second derivative formula can be factored diﬀerently:  f  x + h  − 2f  x  + f  x − h   =  h2  f  x+h −f  x   h  − f  x −f  x−h  h  h  .  That is, the second derivative approximation is a “ﬁnite diﬀerence of ﬁnite diﬀerences.” One way to interpret this formula is shown in Figure 14.8. When we compute the forward diﬀerence approximation of f cid:48  between x and x + h, we can think of this slope as living at x + h 2; we similarly can use backward diﬀerences to place a slope at x − h 2. Finding the slope between these values puts the approximation back on x.   Integration and Diﬀerentiation  cid:4  293  Figure 14.8 Computing the second derivative f cid:48  cid:48  x  by divided diﬀerences can be thought of as applying the same divided diﬀerence rule once to approximate f cid:48  and a second time to approximate f cid:48  cid:48 . 14.3.3 Richardson Extrapolation  One way to improve convergence of the approximations above is Richardson extrapolation. As an example of a more general pattern, suppose we wish to use forward diﬀerences to approximate f cid:48  x . For ﬁxed x ∈ R, deﬁne D h  ≡  f  x + h  − f  x   h  .  We have argued that D h  approaches f cid:48  x  as h → 0. Furthermore, the diﬀerence between D h  and f cid:48  x  scales like O h .  More speciﬁcally, from our discussion in §14.3.2, D h  takes the form  D h  = f cid:48  x  +  f cid:48  cid:48  x h + O h2 .  1 2  1 2  Suppose we know D h  and D αh  for some 0 < α < 1. Then,  D αh  = f cid:48  x  +  f cid:48  cid:48  x αh + O h2 .  We can combine these two relationships in matrix form as  Applying the inverse of the 2 × 2 matrix on the left,  1 2 h 1  2 αh  cid:19  cid:18  f cid:48  x   1   cid:18  1  cid:18  f cid:48  x  f cid:48  cid:48  x   cid:19  = cid:18  1  1  1 2 h 1  f cid:48  cid:48  x   cid:19  = cid:18  D h  2 αh  cid:19 −1 cid:20  cid:18  D h  1 h − 2 1 h − 2  D αh   cid:19  + O h2 . D αh   cid:19  + O h2  cid:21  h  cid:19  cid:20  cid:18  D h  h  cid:19  cid:18  D h   D αh   cid:19  + O h2  cid:21  D αh   cid:19  + cid:18  O h2  O h   cid:19  .  2  2  =  =  1  1 − α cid:18  −α 1 − α cid:18  −α  1  Focusing on the ﬁrst row, we took two O h  approximations of f cid:48  x  using D h  and com- bined them to make an O h2  approximation! This clever technique is a method for se- quence acceleration, improving the order of convergence of the approximation D h . The  f  x − h   f  x   f  x + h   f cid:2  x − h 2   f cid:2  x + h 2   f cid:2  cid:2  x    294  cid:4  Numerical Algorithms  same method is applicable to many other problems including numerical integration, as ex- plored in Exercise 14.9. Richardson extrapolation even can be applied recursively to make higher and higher order approximations of the same quantity.  Example 14.9  Richardson extrapolation . Suppose we wish to approximate f cid:48  1  for f  x  = sin x2. To carry out Richardson extrapolation, we will use the function  If we take h = 0.1 and α = 0.5, then  D h  =  sin 1 + h 2 − sin 12  .  h  D 0.1  = 0.941450167 . . . D 0.1 · 0.5  = 1.017351587 . . .  These approximations both hold up to O h . The O h2  Richardson approximation is  1  1 − 0.5   −0.5D 0.5  + D 0.1 · 0.5   = 1.0932530067 . . .  This approximation is a closer match to the ground truth value f cid:48  1  ≈ 1.0806046117 . . . . 14.3.4 Choosing the Step Size We showed that the error of Richardson extrapolation shrinks more quickly as h → 0 than the error of divided diﬀerences. We have not justiﬁed, however, why this scaling matters. The Richardson extrapolation derivative formula requires more arithmetic than divided diﬀerences, so at ﬁrst glance it may seem to be of limited interest. That is, in theory we can avoid depleting a ﬁxed error budget in computing numerical derivatives equally well with both formulas, even though divided diﬀerences will need a far smaller h.  More broadly, unlike quadrature, numerical diﬀerentiation has a curious property. It appears that any formula above can be arbitrarily accurate without extra computational cost by choosing a suﬃciently small h. This observation is appealing from the perspective that we can achieve higher-quality approximations without additional computation time.  The catch, however, is that implementations of arithmetic operations usually are inexact. The smaller the value of h, the more similar the values f  x  and f  x + h  become, to the point that they are indistinguishable in ﬁnite-precision arithmetic. Dividing by very small h > 0 induces additional numerical instability. Thus, there is a range of h values that are not large enough to induce signiﬁcant discretization error and not small enough to generate numerical problems. Figure 14.9 shows an example for diﬀerentiating a simple function in IEEE ﬂoating-point arithmetic.  Similarly, suppose as in §14.2.7 that due to noise, rather than evaluating f  x , we receive perturbed values from a function ˆf  x  satisfying  cid:107 f − ˆf cid:107 ∞ ≤ ε. Then, we can bound the error of computing a diﬀerence quotient:  ˆf  x + h  − ˆf  x   h  ˆf  x + h  − ˆf  x   h  −  f  x + h  − f  x   + O h   h  by our previous bound   ˆf  x + h  − f  x + h   −   ˆf  x  − f  x    h  + O h    cid:12  cid:12  cid:12  cid:12  cid:12   − f cid:48  x  cid:12  cid:12  cid:12  cid:12  cid:12  ≤ cid:12  cid:12  cid:12  cid:12  cid:12  ≤ cid:12  cid:12  cid:12  cid:12  cid:12    cid:12  cid:12  cid:12  cid:12  cid:12    cid:12  cid:12  cid:12  cid:12  cid:12    Integration and Diﬀerentiation  cid:4  295  Figure14.9 The ﬁnite diﬀerence 1 h f  x+h −f  x   as a function of h for f  x  = x2 2,  computed using IEEE ﬂoating-point arithmetic; when h is too small, the approxi- mation suﬀers from numerical issues, while large h yields discretization error. The horizontal axis is on a logarithmic scale, and the vertical axis scales linearly.  2ε h  ≤  + O h  since  cid:107 f − ˆf cid:107 ∞ ≤ ε.  For ﬁxed ε > 0, this bound degrades if we take h → 0. Instead, we should choose h to balance the 2ε h and O h  terms to get minimal error. That is, if we cannot compute values of f  x  exactly, taking larger h > 0 can actually improve the quality of the estimate of f cid:48  x . Exercise 14.6f has a similar conclusion about a method for numerical integration. 14.3.5 Automatic Differentiation  As we have seen, typical algorithms for numerical diﬀerentiation are relatively fast since they involve little more than computing a diﬀerence quotient. Their main drawback is numerical, in that ﬁnite-precision arithmetic and or inexact evaluation of functions fundamentally limit the quality of the output. Noisy or rapidly varying functions are thus diﬃcult to diﬀerentiate numerically with any conﬁdence.  On the other end of the spectrum between computational eﬃciency and numerical qual- ity lies the technique of automatic diﬀerentiation  “autodiﬀ” , which is not subject to any discretization error [8]. Instead, this technique takes advantage of the chain rule and other properties of derivatives to compute them exactly.  “Forward” automatic diﬀerentiation is particularly straightforward to implement. Sup- pose we have two variables u and v, stored using ﬂoating-point values. We store alongside these variables additional values u cid:48  ≡ du dt and v cid:48  ≡ dv dt for some independent variable t; in some programming languages, we alternatively can deﬁne a new data type holding pairs of values [u, u cid:48 ] and [v, v cid:48 ]. We can deﬁne an algebra on these pairs that encodes typical operations:  [u, u cid:48 ] + [v, v cid:48 ] ≡ [u + v, u cid:48  + v cid:48 ]  c[u, u cid:48 ] ≡ [cu, cu cid:48 ]  [u, u cid:48 ] · [v, v cid:48 ] ≡ [uv, uv cid:48  + u cid:48 v] [u, u cid:48 ] ÷ [v, v cid:48 ] ≡ cid:20  u  cid:21  vu cid:48  − uv cid:48  v2 exp [u, u cid:48 ]  ≡ [eu, u cid:48 eu]  v  ,  1 + 10−6  1  Numerical error  Discretization error  h  10−9  10−8  10−7   296  cid:4  Numerical Algorithms  u cid:48   ln [u, u cid:48 ]  ≡ cid:20 ln u, cos [u, u cid:48 ]  ≡ [cos u,−u cid:48  sin u]  u cid:21   ...  ...  Starting with the pair t ≡ [t0, 1]—since dt dt = 1—we can evaluate a function f  t  and its derivative f cid:48  t  simultaneously using these rules. If they are implemented in a programming language supporting operator overloading, the additional derivative computations can be completely transparent to the implementer.  The method we just described builds up the derivative f cid:48  t  in parallel with building y = f  t . “Backward” automatic diﬀerentiation is an alternative algorithm that can require fewer function evaluations in exchange for more memory usage and a more complex im- plementation. This technique constructs a graph representing the steps of computing f  t  as a sequence of elementary operations. Then, rather than starting from the fact dt dt = 1 and working forward to dy dt, backward automatic diﬀerentiation starts with dy dy = 1 and works backward from the same rules to replace the denominator with dt. Backward auto- matic diﬀerentiation can avoid unnecessary computations, particularly when y is a function of multiple variables. For instance, suppose we can write f  t1, t2  = f1 t1  + f2 t2 ; in this case, backward automatic diﬀerentiation does not need to diﬀerentiate f1 with respect to t2 or f2 with respect to t1. The backpropagation method for neural networks in machine learning is a special case of backward automatic diﬀerentiation.  Automatic diﬀerentiation is widely regarded as an under-appreciated numerical tech- nique, yielding exact derivatives of functions with minimal implementation eﬀort. It is par- ticularly valuable when prototyping software making use of optimization methods requiring derivatives or Hessians, avoiding having to recompute derivatives by hand every time an objective function is adjusted. The cost of this convenience, however, is computational ef- ﬁciency, since in eﬀect automatic diﬀerentiation methods do not simplify expressions for derivatives but rather apply the most obvious rules.  14.3.6 Integrated Quantities and Structure Preservation  Continuing in our consideration of alternatives to numerical diﬀerentiation, we outline an approach that has gained popularity in the geometry and computer graphics communities for dealing with curvature and other diﬀerential measures of shape.  As we have seen, a typical pattern from numerical analysis is to prove that properties of approximated derivatives hold as ∆x → 0 for some measure of spacing ∆x. While this type of analysis provides intuition relating discrete computations to continuous notions from calculus, it neglects a key fact: In reality, we must ﬁx ∆x > 0. Understanding what happens in the ∆x > 0 regime can be equally important to the ∆x → 0 limit, especially when taking coarse approximations. For example, in computational geometry, it may be desirable to link measures like curvature of smooth shape directly to discrete values like lengths and angles that can be computed on complexes of polygons.  With this new view, some techniques involving derivatives, integrals, and other quan- tities are designed with structure preservation in mind, yielding “discrete” rather than “discretized” analogs of continuous quantities [53]. That is, rather than asking that struc- ture from continuous calculus emerges as ∆x → 0, we design diﬀerentiators and integrators for which certain theorems from continuous mathematics hold exactly. One central technique in this domain is the use of integrated quantities to encode derivatives. As a basic example, suppose we are sampling f  t  and have computed   Integration and Diﬀerentiation  cid:4  297  Figure 14.10 Notation for Example 14.10; each curve segment Γi is the union of the two half-segments adjacent to  cid:126 vi, bounded by the marked midpoints.  f  t1 , f  t2 , . . . , f  tk  for some discrete set of times t1 < t2 < ··· < tk. Rather than using divided diﬀerences to approximate the derivative f cid:48 , we can use the Fundamental Theorem of Calculus to show   cid:90  ti+1  ti  f cid:48  t  dt = f  ti+1  − f  ti .  This formula may not appear remarkable beyond ﬁrst-year calculus, but it encodes a deep idea. The diﬀerence f  ti+1 − f  ti  on the right side is computable exactly from the samples f  t1 , f  t2 , . . . , f  tk , while the quantity on the left is an averaged version of the derivative f cid:48 . By substituting integrated versions of f cid:48  into computations whenever possible, we can carry out discrete analogs of continuous calculus for which certain theorems and properties hold exactly rather than in the limit.  Example 14.10  Curvature of a 2D curve, [53] . In the continuous theory of diﬀerential geometry, a smooth curve Γ on the two-dimensional plane can be parameterized as a function γ s  : R → R2 satisfying γ cid:48  s   cid:54 =  cid:126 0 for all s. Assume that  cid:107 γ cid:48  s  cid:107 2 = 1 for all s; such an arc length parameterization is always possible by moving along the curve with constant speed. Then, Γ has unit tangent vector  cid:126 T  s  ≡ γ cid:48  s . If we write  cid:126 T  s  ≡  cos θ s , sin θ s   for angle θ s , then the curvature of γ s  is given by the derivative κ s  ≡ θ cid:48  s . This notation is illustrated in Figure 14.10 alongside notation for the discretization below.  Suppose Γ is closed, that is, γ s0  = γ s1  for some s0, s1 ∈ R. In this case, the turning  number theorem from topology states   cid:90  s1  s0  κ s  ds = 2πk,  for some integer k. Intuitively, this theorem represents the fact that  cid:126 T  s0  =  cid:126 T  s1 , and hence θ took some number of loops around the full circle.  A typical discretization of a two-dimensional curve is as a sequence of line segments  cid:126 vi ↔  cid:126 vi+1. Approximating κ s  on such a curve can be challenging, since κ is related to the second derivative γ cid:48  cid:48 . Instead, suppose at each joint  cid:126 vi we deﬁne the integrated curvature over the two half-segments around  cid:126 vi to be the turning angle θi given by the π minus the angle between the two segments adjacent to  cid:126 vi.   298  cid:4  Numerical Algorithms  Partition the discretization of Γ into pairs of half-segments Γi. Then, if Γ is closed,   cid:90 Γ  κ ds = cid:88 i  cid:90 Γi = cid:88 i  = 2πk,  κ ds by breaking into individual terms  θi by deﬁnition of integrated curvature  where the ﬁnal equality comes from the fact that the discrete Γ is a polygon, and we are summing its exterior angles. That is, for this choice of discrete curvature, the turning number theorem holds exactly even for coarse approximations of Γ, rather than becoming closer and closer to true as the lengths Γi → 0. In this sense, the integrated turning-angle curvature has more properties in common with the continuous curvature of a curve γ s  than an inexact but convergent discretization coming from divided diﬀerences.  The example above shows a typical structure-preserving treatment of a derivative quan- tity, in this case the curvature of a two-dimensional curve, accompanied by a discrete structure—the turning number theorem—holding without taking any limit as ∆x → 0. We have not shown, however, that the value θi—or more precisely some non-integrated pointwise approximation like θi Γi—actually converges to the curvature of Γ. This type of convergence does not always hold, and in some cases it is impossible to preserve structure exactly and converge as ∆x → 0 simultaneously [128]. Such issues are the topic of active research at the intersection of numerical methods and geometry processing. 14.4 EXERCISES  14.1 Show that the midpoint rule is exact for the function f  x  = mx+c along any interval  x ∈ [a, b].  14.2  Suggested by Y. Zhao  Derive α, β, and x1 such that the following quadrature rule  holds exactly for polynomials of degree ≤ 2 :   cid:90  2  0  f  x  dx ≈ αf  0  + βf  x1 .  14.3 Suppose we are given a quadrature rule of the form  cid:82  1  some a, b ∈ R. Propose a corresponding composite rule for approximating cid:82  1  given n + 1 closed sample points y0 ≡ f  0 , y1 ≡ f  1 n , y2 ≡ f  2 n , . . . , yn ≡ f  1 . 14.4 Some quadrature problems can be solved by applying a suitable change of variables:  0 f  x  dx ≈ af  0  + bf  1  for 0 f  x  dx   a  Our strategies for quadrature break down when the interval of integration is not  of ﬁnite length. Derive the following relationships for f : R → R:  −∞   cid:90  ∞  cid:90  ∞  cid:90  ∞  0  c  −1  f  x  dx = cid:90  1 f  x  dx = cid:90  1 f  x  dx = cid:90  1  0  0  1 − t2 cid:19  1 + t2 f cid:18  t  1 − t2 2 dt  f  − ln t   dt  t  f cid:18 c +  t  1 − t cid:19  ·  1   1 − t 2 dt.   Integration and Diﬀerentiation  cid:4  299  How can these formulas be used to integrate over intervals of inﬁnite length? What might be a drawback of evenly spacing t samples?   b  Suppose f : [−1, 1] → R can be written: a0 2  f  cos θ  =  +  ak cos kθ .  Then, show:   cid:90  1  −1  f  x  dx = a0 +  2a2k  1 −  2k 2 .  ∞ cid:88 k=1 ∞ cid:88 k=1  This formula provides a way to integrate a function given its Fourier series [25].  14.5 The methods in this chapter for diﬀerentiation were limited to single-valued functions f : R → R. Suppose g : Rn → Rm. How would you use these techniques to approx- imate the Jacobian Dg? How does the timing of your approach scale with m and n?  14.6  “Lanczos diﬀerentiator,” [77]  Suppose f  t  is a smooth function.   a  Suppose we sample f  t  at t = kh for k ∈ {−n,−n + 1, . . . , 0, . . . , n}, yielding samples y−n = f  −nh , y−n+1 = f   −n + 1 h , . . . , yn = f  nh . Show that the parabola p t  = at2 + bt + c optimally ﬁtting these data points via least-squares satisﬁes   b  Use this formula to propose approximations of f cid:48  0  when n = 1, 2, 3.   c  Motivate the following formula for “diﬀerentiation by integration”:  p cid:48  0  =  cid:80 k kyk h cid:80 k k2 .  f cid:48  0  = lim h→0  3  2h3 cid:90  h  −h  tf  t  dt.  This formula provides one connection between numerical methods for integration and diﬀerentiation.   d  Show that when h > 0,  3  tf  t  dt = f cid:48  0  + O h2 .  2h3 cid:90  h −h tf  t  dt. Suppose thanks to noise we actually observe f ε t   −h  2h3 cid:82  h  e  Denote Dhf ≡ 3  satisfying f  t  − f ε t  ≤ ε for all t. Show the following relationship:  Dhf ε − f cid:48  0  ≤  3ε 2h  + O h2 .   f  Suppose the second term in Exercise 14.6e is bounded above by M h2 10; this is the case when f cid:48  cid:48  cid:48  t  ≤ M everywhere [54]. Show that with the right choice of h, the integral approximation from Exercise 14.6e is within O ε2 3  of f cid:48  0 . Note: Your choice of h eﬀectively trades oﬀ between numerical approximation error from using the “diﬀerentiation by integration” formula and noise approxi- mating f with f ε. This property makes the Lanczos approximation eﬀective for certain noisy functions.   300  cid:4  Numerical Algorithms  14.7 Propose an extension of forward automatic diﬀerentiation to maintaining ﬁrst and second derivatives in triplets [u, u cid:48 , u cid:48  cid:48 ]. Provide analogous formulas for the operations listed in §14.3.5 given [u, u cid:48 , u cid:48  cid:48 ] and [v, v cid:48 , v cid:48  cid:48 ].  14.8 The problem of numerical diﬀerentiation is challenging for noisy functions. One way to stabilize such a calculation is to consider multiple samples simultaneously [1]. For this problem, assume f : [0, 1] → R is diﬀerentiable.  a  By the Fundamental Theorem of Calculus, there exists c ∈ R such that f  x  = c + cid:90  x  f cid:48  ¯x  d¯x.  0  Suppose we sample f  x  at evenly spaced points x0 = 0, x1 = h, x2 = 2h, . . . , xn = 1 and wish to approximate the ﬁrst derivative f cid:48  x  at x1− h 2, x2− h 2, . . . , xn − h 2. If we label our samples of f cid:48  x  as a1, . . . , an, write a least- squares problem in the ai’s and an additional unknown c approximating this integral relationship.   b  Propose a Tikhonov regularizer for this problem.   c  We also could have written  f  x  = ˜c − cid:90  1  x  f cid:48  ¯x  d¯x.  Does your approximation of f cid:48  ¯x  change if you use this formula?  14.9 The Romberg quadrature rules are derived by applying Richardson extrapolation integration. Here, we will derive Romberg integration for   §14.3.3  to numerical f : [a, b] → R.  a  Suppose we divide [a, b] into 2k subintervals for k ≥ 0. Denote by Tk,0 the result of applying the composite trapezoidal rule to f  x  to this subdivision. Show that there exists a constant C dependent on f but not k such that:   cid:90  b  a  f  x  dx = Tk,0 + Ch2 + O h4 ,  where h k  =  b−a  2k. For this problem, you may assume that f is inﬁnitely diﬀerentiable and that the Taylor series for f centered at any c ∈ [a, b] is conver- gent.   b  Use Richardson extrapolation to derive an estimate Tk,1 of the integral that is  accurate up to O h4 . Hint: Combine Tk,0 and Tk−1,0.   c  Assume that the error expansion for the trapezoidal rule continues in a similar  fashion:   cid:90  b  a  f  x  dx = Tk,0 + C2h2 + C4h4 + C6h6 + ··· .  By iteratively applying Richardson extrapolation, propose values Tk,j for j ≤ k that can be used to achieve arbitrarily high-order estimates of the desired integral. Hint: You should be able to deﬁne Tk,j as a linear combination of Tk,j−1 and Tk−1,j−1.   Integration and Diﬀerentiation  cid:4  301  14.10 Give examples of closed and open Newton-Cotes quadrature rules with negative coef- ﬁcients for integrating f  x  on [0, 1]. What unnatural properties can be exhibited by these approximations?  14.11 Provide a sequence of diﬀerentiable functions fk : [0, 1] → R and a function f : [0, 1] → R such that maxx∈[0,1] fk x −f  x  → 0 as k → ∞ but maxx∈[0,1] f cid:48 k x −f cid:48  x  → ∞. What does this example imply about numerical diﬀerentiation when function values are noisy? Is a similar counterexample possible for integration when f and the fk’s are diﬀerentiable?    C H A P T E R15  Ordinary Differential Equations  CONTENTS  15.3 Time-Stepping Schemes  15.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15.2 Theory of ODEs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15.2.1 Basic Notions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15.2.2 Existence and Uniqueness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15.2.3 Model Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15.3.1 Forward Euler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15.3.2 Backward Euler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15.3.3 Trapezoidal Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15.3.4 Runge-Kutta Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15.3.5 Exponential Integrators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15.4.1 Newmark Integrators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15.4.2 Staggered Grid and Leapfrog . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15.5 Comparison of Integrators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  15.4 Multivalue Methods  304 305 305 307 309 311 311 313 314 315 316 318 318 321 322  C HAPTER 13 motivated the problem of interpolation by transitioning from analyzing  functions to ﬁnding functions. In problems like interpolation and regression, the un- known is a entire function f   cid:126 x , and the job of the algorithm is to ﬁll in f   cid:126 x  at positions  cid:126 x where it is unknown.  In this chapter and the next, our unknown will continue to be a function f , but rather than ﬁlling in missing values we will solve more complex design problems like the following:    Find f approximating some other function f0 but satisfying additional criteria   smoothness, continuity, boundedness, etc. .    Simulate some dynamical or physical relationship as f  t  where t is time.   Find f with similar values to f0 but certain properties in common with a diﬀerent  function g0.  In each of these cases, our unknown is a function f , but our criterion for success is more involved than “matches a given set of data points.”  The theories of ordinary diﬀerential equations  ODEs  and partial diﬀerential equations  PDEs  involve the case where we wish to ﬁnd a function f   cid:126 x  based on information about  303   304  cid:4  Numerical Algorithms  or relationships between its derivatives. We inadvertently solved one problem in this class while studying quadrature: Given f cid:48  t , quadrature approximates f  t  using integration.  In this chapter, we will consider ordinary diﬀerential equations and in particular initial value problems. In these problems, the unknown is a function f  t  : R → Rn, given f  0  and an equation satisﬁed by f and its derivatives. Our goal is to predict f  t  for t > 0. We will provide examples of ODEs appearing in practice and then will describe common solution techniques.  15.1 MOTIVATION  ODEs appear in nearly every branch of science, and hence it is not diﬃcult to identify target applications of solution techniques. We choose a few representative examples both from the computational and scientiﬁc literatures: Example 15.1  Newton’s Second Law . Continuing from §6.1.2, recall that Newton’s Second Law of Motion states  cid:126 F = m cid:126 a, that is, the total force on an object is equal to its mass times its acceleration. If we simulate n particles simultaneously as they move in three-dimensional space, we can combine all their positions into a single vector  cid:126 x t  ∈ R3n. Similarly, we can write a function  cid:126 F  t,  cid:126 x,  cid:126 x cid:48   ∈ R3n taking the current time, the positions of the particles, and their velocities and returning the total force on each particle divided by its mass. This function can take into account interrelationships between particles  e.g., gravitational forces, springs, or intermolecular bonds , external eﬀects like wind resistance  which depends on  cid:126 x cid:48  , external forces varying with time t, and so on. To ﬁnd the positions of all the particles as functions of time, we can integrate Newton’s second law forward in time by solving the equation  cid:126 x cid:48  cid:48  =  cid:126 F  t,  cid:126 x,  cid:126 x cid:48  . We usually are given the positions and velocities of all the particles at time t = 0 as a starting condition.  Example 15.2  Protein folding . On a small scale, the equations governing motions of molecules stem from Newton’s laws or—at even smaller scales—the Schr¨odinger equation of quantum mechanics. One challenging case is that of protein folding, in which the geometric structure of a protein is predicted by simulating intermolecular forces over time. These forces take many nonlinear forms that continue to challenge researchers in computational biology due in large part to a variety of time scales: The same forces that cause protein folding and related phenomena also can make molecules vibrate rapidly, and the disparate time scales of these two diﬀerent behaviors makes them diﬃcult to capture simultaneously.  Example 15.3  Gradient descent . Suppose we wish to minimize an objective function E  cid:126 x  over all  cid:126 x. Especially if E is a convex function, the most straightforward option for minimization from Chapter 9 is gradient descent with a constant step size or “learning rate.” Since −∇E  cid:126 x  points in the direction along which E decreases the most from a given  cid:126 x, we can iterate:  for ﬁxed h > 0. We can rewrite this relationship as  In the style of §14.3, we might think of  cid:126 xk as a sample of a function  cid:126 x t  at t = hk. Heuristically, taking h → 0 motivates an ordinary diﬀerential equation   cid:126 xi+i ≡  cid:126 xi − h∇E  cid:126 xi ,   cid:126 xi+1 −  cid:126 xi  h  = −∇E  cid:126 xi .   cid:126 x cid:48  t  = −∇E  cid:126 x .   Ordinary Diﬀerential Equations  cid:4  305  If we take  cid:126 x 0  to be an initial guess of the location where E  cid:126 x  is minimized, then this ODE is a continuous model of gradient descent. It can be thought of as the equation of a path smoothly walking “downhill” along a landscape provided by E.  For example, suppose we wish to solve A cid:126 x =  cid:126 b for symmetric positive deﬁnite A. From 2  cid:126 x cid:62 A cid:126 x −  cid:126 b cid:62  cid:126 x + c. Using the continuous §11.1.1, this is equivalent to minimizing E  cid:126 x  ≡ 1 model of gradient descent, we can instead solve the ODE  cid:126 x cid:48  = −∇E  cid:126 x  =  cid:126 b − A cid:126 x. As t → ∞, we expect  cid:126 x t  to better and better satisfy the linear system. Example 15.4  Crowd simulation . Suppose we are writing video game software requiring realistic simulation of virtual crowds of humans, animals, spaceships, and the like. One way to generate plausible motion is to use diﬀerential equations. In this technique, the velocity of a member of the crowd is determined as a function of its environment; for example, in human crowds, the proximity of other humans, distance to obstacles, and so on can aﬀect the direction a given agent is moving. These rules can be simple, but in the aggregate their interaction becomes complex. Stable integrators for diﬀerential equations underlie crowd simulation to avoid noticeably unrealistic or unphysical behavior.  15.2 THEORY OF ODES  A full treatment of the theory of ordinary diﬀerential equations is outside the scope of our discussion, and we refer the reader to [64] or any other basic text for details from this classical theory. We highlight relevant results here for development in future sections.  15.2.1 Basic Notions  The most general initial value problem takes the following form:  satisfying F [t, f  t , f cid:48  t , f cid:48  cid:48  t , . . . , f  k  t ] =  cid:126 0  Find f  t  : R+ → Rn given f  0 , f cid:48  0 , f cid:48  cid:48  0 , . . . , f  k−1  0 .  Here, F is some relationship between f and all its derivatives; we use f   cid:96   to denote the  cid:96 -th derivative of f . The functions f and F can be multidimensional, taking on values in Rn rather than R, but by convention and for convenience of notation we will omit the vector sign. We also will use the notation  cid:126 y ≡ f  t  as an alternative to writing f  t  when the t dependence is implicit; in this case, derivatives will be notated  cid:126 y cid:48  ≡ f cid:48  t ,  cid:126 y cid:48  cid:48  ≡ f cid:48  cid:48  t , and so on.  Example 15.5  Canonical ODE form . Suppose we wish to solve the ODE y cid:48  cid:48  = ty cid:48  cos y. In the general form above, the ODE can be written F [t, y, y cid:48 , y cid:48  cid:48 ] = 0, where F [t, a, b, c] ≡ tb cos a − c.  ODEs determine the evolution of f over time t; we know f and its derivatives at time t = 0 and wish to predict these quantities moving forward. They can take many forms even in a single variable. For instance, denote y = f  t  for y ∈ R1. Then, examples of ODEs include the following:   306  cid:4  Numerical Algorithms  Example ODE Distinguishing properties y cid:48  = 1 + cos t  y cid:48  = ay y cid:48  = ay + et y cid:48  cid:48  + 3y cid:48  − y = t y cid:48  cid:48  sin y = ety cid:48   Can be solved by integrating both sides with respect to t; can be solved discretely using quadrature Linear in y, no dependence on time t Time- and value-dependent Involves multiple derivatives of y Nonlinear in y and t  We will restrict most of our discussion to the case of explicit ODEs, in which the highest-  order derivative can be isolated:  Deﬁnition 15.1  Explicit ODE . An ODE is explicit if can be written in the form  f  k  t  = F [t, f  t , f cid:48  t , f cid:48  cid:48  t , . . . , f  k−1  t ].  Certain implicit ODEs can be converted to explicit form by solving a root-ﬁnding problem, for example, using the machinery introduced in Chapter 8, but this approach can fail in the presence of multiple roots.  Generalizing a trick ﬁrst introduced in §6.1.2, any explicit ODE can be converted to a ﬁrst-order equation f cid:48  t  = F [t, f  t ] by adding to the dimensionality of f . This construction implies that it will be enough for us to consider algorithms for solving  multivariable  ODEs containing only a single time derivative. As a reminder of this construction for the second- order ODE y cid:48  cid:48  = F [t, y, y cid:48 ], recall that  d2y dt2 =  d  dt cid:18  dy dt cid:19  . F [t, y, z]  cid:19  .  z  d  dt cid:18  y  z  cid:19  = cid:18   Deﬁning an intermediate variable z ≡ dy dt, we can expand to the following ﬁrst-order system:  More generally, if we wish to solve the explicit problem  f  k  t  = F [t, f  t , f cid:48  t , f cid:48  cid:48  t , . . . , f  k−1  t ]  for f : R+ → Rn, then instead we can solve the ﬁrst-order ODE in dimension n k + 1 :  d dt    f0 t  f1 t  f2 t  ...  fk−1 t     =    f1 t  f2 t  f3 t  ...  .    F [t, f0 t , f1 t , . . . , fk−1 t ]  Here, we denote fi t  : R → Rn as the i-th derivative of f0 t , which satisﬁes the original ODE. To check, our expanded system above implies f1 t  = f cid:48 0 t , f2 t  = f cid:48 1 t  = f cid:48  cid:48 0  t , and so on; the ﬁnal row encodes the original ODE.  This trick simpliﬁes notation and allows us to emphasize ﬁrst-order ODEs, but some care should be taken to understand that it does come with a cost. The expansion above replaces ODEs with potentially many derivatives with ODEs containing just one derivative but with much higher dimensionality. We will return to this trade-oﬀ between dimensionality and number of derivatives when designing methods speciﬁcally for second-order ODEs in §15.4.2.   Ordinary Diﬀerential Equations  cid:4  307  Figure 15.1 First-order ODEs in one variable y cid:48  = F [t, y] can be visualized using slope ﬁelds on the  t, y  plane. Here, short line segments show the slope F [t, y] at each sampled point; solution curves y t  shown as dotted lines start at  0, y 0   and follow the slope ﬁeld as their tangents. We show an example of a time-independent  “autonomous”  ODE y cid:48  = F [y] and an example of a time-dependent ODE y cid:48  = F [t, y].  Example 15.6  ODE expansion . Suppose we wish to solve y cid:48  cid:48  cid:48  = 3y cid:48  cid:48  − 2y cid:48  + y where y t  : R+ → R. This equation is equivalent to:  d  dt  y z  w  =  0 1 0 0 1 −2  0 1  3   y z  w  .  In the interests of making our canonical ODE problem as simple as possible, we can further restrict our consideration to autonomous ODEs. These equations are of the form f cid:48  t  = F [f  t ], that is, F has no dependence on t  or on higher-order derivatives of f , removed above . To reduce an ODE to this form, we use the fact d dt t  = 1. After deﬁning a trivial function g t  = t, the ODE f cid:48  t  = F [t, f  t ] can be rewritten as the autonomous equation  d  dt cid:18  g t   f  t   cid:19  = cid:18   1  F [g t , f  t ]  cid:19  ,  with an additional initial condition g 0  = 0.  It is possible to visualize the behavior and classiﬁcation of low-dimensional ODEs in many ways. If the unknown f  t  is a function of a single variable, then F [f  t ] provides the slope of f  t , as shown in Figure 15.1. For higher-order ODEs, it can be useful to plot f  t  and its derivatives, shown for the equation of motion for a pendulum in Figure 15.2. In higher dimensions, it may be possible only to show example solution paths, as in Figure 15.3.  15.2.2 Existence and Uniqueness  Before we discretize the initial value ODE problem, we should acknowledge that not all diﬀerential equations are solvable, while others admit inﬁnitely many solutions. Existence and uniqueness of ODE solutions can be challenging to prove, but without these properties  y  y  t  t  tnednepedni-emiT  tnedneped-emiT   308  cid:4  Numerical Algorithms  Figure 15.2 The phase space diagram of a pendulum, which satisﬁes the ODE θ cid:48  cid:48  = − sin θ. Here, the horizontal axis shows position θ of the pendulum as it swings  as an angle from vertical , and the vertical axis shows the angular velocity θ cid:48 . Each path represents the motion of a pendulum with diﬀerent starting conditions; the time t is not depicted. Rings indicate a swinging pendulum, while waves indicate that the pendulum is doing complete revolutions.  Figure 15.3 The trace of an ODE solution  x t , y t , z t   shows typical behavior without showing the velocity of the path or dependence on time t; here we show a solution to the Lorenz equations  known as a “Lorenz attractor”  x cid:48  = σ y−x , y cid:48  = x ρ − z  − y, z cid:48  = xy − βz integrated numerically  ρ = 28, σ = 10, β = 8 3 .  θ cid:31  t   θ t   z  y  x   Ordinary Diﬀerential Equations  cid:4  309  we cannot hold numerical methods responsible for failure to recover a reasonable solution. Numerical ODE solvers can be thought of as ﬁlling the gap between knowing that a solution to a diﬀerential equation exists and being able to write this solution in closed form; checking existence and uniqueness is largely a function of how an ODE is written before discretization and usually is checked theoretically rather than algorithmically.  Example 15.7  Unsolvable ODE . Consider the equation y cid:48  = 2y t, with y 0   cid:54 = 0 given; the 1 t factor does not divide by zero because the ODE only has to hold for t > 0. Rewriting as  and integrating with respect to t on both sides shows  1 y  dy dt  =  2 t  lny = 2 ln t + c.  Exponentiating both sides shows y = Ct2 for some C ∈ R. In this expression, y 0  = 0, contradicting the initial conditions. Thus, this ODE has no solution with the given initial conditions.  Example 15.8  Nonunique solutions . Now, consider the same ODE with y 0  = 0. Consider y t  given by y t  = Ct2 for any C ∈ R. Then, y cid:48  t  = 2Ct and  2y t  =  2Ct2  t  = 2Ct = y cid:48  t ,  showing that the ODE is solved by this function regardless of C. Thus, solutions of this equation with the new initial conditions are nonunique.  There is a rich theory characterizing behavior and stability of solutions to ordinary diﬀerential equations. Under weak conditions on F , it is possible to show that an ODE f cid:48  t  = F [f  t ] has a solution; in the next chapter, we will see that showing existence and or uniqueness for PDEs rather than ODEs does not beneﬁt from this structure. One such theorem guarantees existence of a solution when F is not sharply sloped:  Theorem 15.1  ODE existence and uniqueness . Suppose F is continuous and Lipschitz, that is,  cid:107 F [ cid:126 y] − F [ cid:126 x] cid:107 2 ≤ L cid:107  cid:126 y −  cid:126 x cid:107 2 for some ﬁxed L ≥ 0. Then, the ODE f cid:48  t  = F [f  t ] admits exactly one solution for all t ≥ 0 regardless of initial conditions. In our subsequent development, we will assume that the ODE we are attempting to solve satisﬁes the conditions of such a theorem. This assumption is realistic since the conditions guaranteeing existence and uniqueness are relatively weak.  15.2.3 Model Equations  One way to understand computational methods for integrating ODEs is to examine their behavior on well-understood model equations. Many ODEs locally can be approximated by these model equations, motivating our detailed examination of these simplistic test cases. We start by introducing a model equation for ODEs with a single dependent variable. Given our simpliﬁcations in §15.2.1, we consider equations of the form y cid:48  = F [y], where y t  : [0,∞  → R. Taking a linear approximation of F , we might deﬁne y cid:48  = ay + b to be   310  cid:4  Numerical Algorithms  Figure 15.4 Three cases of the linear model equation y cid:48  = ay.  the model ODE, but we actually can ﬁx b = 0. To justify using just one degree of freedom, deﬁne ¯y ≡ y + b a. Then,  ¯y cid:48  = cid:18 y +  b  a cid:19  cid:48   by deﬁnition of ¯y  = y cid:48  since the second term is constant with respect to t = ay + b from the linearization = a ¯y − b a  + b by inverting the deﬁnition of ¯y = a¯y.  This substitution satisﬁes ¯y cid:48  = a¯y, showing that the constant b does not aﬀect the qualitative behavior of the ODE. Hence, in the phenomenological study of model equations we safely take b = 0.  By the argument above, we locally can understand behavior of y cid:48  = F [y] by studying the linear equation y cid:48  = ay. While the original ODE may not be solvable in closed form, applying standard arguments from calculus shows that the model equation is solved by the formula  y t  = Ceat.  Qualitatively, this formula splits into three cases, illustrated in Figure 15.4:  1. a > 0: Solutions get larger and larger; if y t  and ˆy t  both satisfy the ODE with  slightly diﬀerent starting conditions, as t → ∞ they diverge.  2. a = 0: This system is solved by constant functions; solutions with diﬀerent starting  points stay the same distance apart.  3. a < 0: All solutions approach 0 as t → ∞.  We say cases 2 and 3 are stable, in the sense that perturbing y 0  yields solutions that do not diverge from each other over time; case 1 is unstable, since a small mistake in specifying the initial condition y 0  will be ampliﬁed as time t advances.  Unstable ODEs generate ill-posed computational problems. Without careful considera- tion, we cannot expect numerical methods to generate usable solutions in this case, since theoretical solutions are already sensitive to perturbations of the input. On the other hand, stable problems are well-posed since small mistakes in y 0  get diminished over time. Both cases are shown in Figure 15.5.  Extending to multiple dimensions, we study the linearized equation  cid:126 y cid:48  = A cid:126 y; for simplic- ity, we will assume A is symmetric. As explained in §6.1.2, if  cid:126 y1,··· ,  cid:126 yk are eigenvectors of A   Ordinary Diﬀerential Equations  cid:4  311  Figure 15.5 A stable ODE diminishes the diﬀerence between solutions over time t if y 0  is perturbed, while an unstable ODE ampliﬁes this diﬀerence.  with eigenvalues λ1, . . . , λk and  cid:126 y 0  = c1 cid:126 y1 +···+ck cid:126 yk, then  cid:126 y t  = c1eλ1t cid:126 y1 +···+ckeλkt cid:126 yk. Based on this formula, the eigenvalues of A take the place of a in the one-dimensional model equation. From this result, it is not hard to intuit that a multivariable solution to  cid:126 y cid:48  = A cid:126 y is stable exactly when the eigenvalues of A are bounded above by zero.  As in the single-variable case, in reality we need to solve  cid:126 y cid:48  = F [ cid:126 y] for general functions F . Assuming F is diﬀerentiable, we can approximate F [ cid:126 y] ≈ F [ cid:126 y0] + JF   cid:126 y0   cid:126 y− cid:126 y0 , yielding the model equation above after a shift. This argument shows that for short periods of time we expect behavior similar to the model equation with A = JF   cid:126 y0 , the Jacobian at  cid:126 y0.  15.3 TIME-STEPPING SCHEMES We now describe several methods for solving the nonlinear ODE  cid:126 y cid:48  = F [ cid:126 y] given potentially nonlinear functions F . Given a “time step” h, our methods will generate estimates of  cid:126 y t+h  given  cid:126 y t  and F . Applying these methods iteratively generates estimates  cid:126 y0 ≡  cid:126 y t ,  cid:126 y1 ≈  cid:126 y t + h ,  cid:126 y2 ≈  cid:126 y t + 2h ,  cid:126 y3 ≈  cid:126 y t + 3h , and so on. We call algorithms for generating approximations of  cid:126 y t  time-stepping schemes or integrators, reﬂecting the fact that they are integrating out the derivatives in the input equation.  Of key importance to our consideration is the idea of stability. Even if an ODE theoreti- cally is stable using the deﬁnition from §15.2.3, the integrator may produce approximations that diverge at an exponential rate. Stability usually depends on the time step h; when h is too large, diﬀerential estimates of the quality of an integrator fail to hold, yielding unpredictable output. Stability, however, can compete with accuracy. Stable schemes may generate bad approximations of  cid:126 y t , even if they are guaranteed not to have wild behavior. ODE integrators that are both stable and accurate tend to require excessive computation time, indicating that we must compromise between these two properties.  15.3.1 Forward Euler  Our ﬁrst ODE integrator comes from our construction of the forward diﬀerencing scheme in §14.3.2:  F [ cid:126 yk] =  cid:126 y cid:48  t  =   cid:126 yk+1 −  cid:126 yk  h  + O h .   312  cid:4  Numerical Algorithms  Figure 15.6 Unstable and stable cases of forward Euler integration for the model equation y cid:48  = ay with h = 1.  Solving this relationship for  cid:126 yk+1 shows   cid:126 yk+1 =  cid:126 yk + hF [ cid:126 yk] + O h2  ≈  cid:126 yk + hF [ cid:126 yk].  This forward Euler scheme applies the approximation on the right to estimate  cid:126 yk+1 from  cid:126 yk. It is one of the most computationally eﬃcient strategies for time-stepping. Forward Euler is an explicit integrator, since there is an explicit formula for  cid:126 yk+1 in terms of  cid:126 yk and F .  The forward Euler approximation of  cid:126 yk+1 holds to O h2 , so each step induces quadratic error. We call this the localized truncation error because it is the error induced by a single time step. The word “truncation” refers to the fact that we truncated a Taylor series to obtain the integrator. The iterate  cid:126 yk, however, already may be inaccurate thanks to accumu- lated truncation errors from previous iterations. If we integrate from t0 to t with k = O 1 h  steps, then the total error looks like O h . This estimate quantiﬁes global truncation error, and thus we usually say that the forward Euler scheme is “ﬁrst-order accurate.”  The stability of forward Euler can be motivated by studying the model equation. We will work out the stability of methods in the one-variable case y cid:48  = ay, with the intuition that similar statements carry over to multidimensional equations by replacing a with a spectral radius. Substituting the one-variable model equation into the forward Euler scheme gives  yk+1 = yk + ahyk =  1 + ah yk.  Expanding recursively shows yk =  1+ah ky0. By this explicit formula for yk in terms of y0, the integrator is stable when 1+ah ≤ 1, since otherwise yk → ∞ exponentially. Assuming a < 0  otherwise the theoretical problem is ill-posed , this condition takes a simpler form:  1 + ah ≤ 1 ⇐⇒ −1 ≤ 1 + ah ≤ 1 by expanding the absolute value  ⇐⇒ 0 ≤ h ≤  , since a < 0.  2 a  This derivation shows that forward Euler admits a time step restriction. That is, the output of forward Euler integration can explode even if y cid:48  = ay is stable, when h is too large.  Figure 15.6 illustrates what happens when the stability condition is obeyed or violated. When time steps are too large—or equivalently when a is too large—the forward Euler method is not only inaccurate but also has very diﬀerent qualitative behavior. For nonlinear ODEs this formula gives a guide for stability at least locally in time; globally h may have to be adjusted if the Jacobian of F becomes worse conditioned.  Stable  a = −0.4   Unstable  a = −2.3    Ordinary Diﬀerential Equations  cid:4  313  Certain well-posed ODEs require tiny time steps h for forward Euler to be stable. In this case, even though the forward Euler formula is computationally inexpensive for a single step, integrating to some ﬁxed time t may be infeasible because so many steps are needed. Such ODEs are called stiﬀ, inspired by stiﬀ springs requiring tiny time steps to capture their rapid oscillations. One text deﬁnes stiﬀ problems slightly diﬀerently  via [60] : “Stiﬀ equations are problems for which explicit methods don’t work” [57]. With this deﬁnition in mind, in the next section we consider an implicit method with no stability time step restriction, making it more suitable for stiﬀ problems.  15.3.2 Backward Euler  We could have applied backward diﬀerencing at  cid:126 yk+1 to design an ODE integrator:  F [ cid:126 yk+1] =  cid:126 y cid:48  t + h  =   cid:126 yk+1 −  cid:126 yk  h  + O h .  Isolating  cid:126 yk shows that this integrator requires solving the following potentially nonlinear system of equations for  cid:126 yk+1:   cid:126 yk+1 =  cid:126 yk + hF [ cid:126 yk+1].  This equation diﬀers from forward Euler integration by the evaluation of F at  cid:126 yk+1 rather than at  cid:126 yk. Because we have to solve this equation for  cid:126 yk+1, this technique, known as backward Euler integration, is an implicit integrator.  Example 15.9  Backward Euler . Suppose we wish to generate time steps for the ODE  cid:126 y cid:48  = A cid:126 y, with ﬁxed A ∈ Rn×n. To ﬁnd  cid:126 yk+1 we solve the following system:   cid:126 yk =  cid:126 yk+1 − hA cid:126 yk+1 =⇒  cid:126 yk+1 =  In×n − hA −1 cid:126 yk.  Backward Euler is ﬁrst-order accurate like forward Euler by an identical argument. Its stability, however, contrasts considerably with that of forward Euler. Once again considering the model equation y cid:48  = ay, we write:  yk = yk+1 − hayk+1 =⇒ yk+1 =  yk  .  1 − ha  To prevent exponential blowup, we enforce the following condition:  1  1 − ha ≤ 1 ⇐⇒ h ≤  2 a  or h ≥ 0, for a < 0.  We always choose h ≥ 0, so backward Euler is unconditionally stable, as in Figure 15.7. Even if backward Euler is stable, however, it may not be accurate. If h is too large,  cid:126 yk will approach zero at the wrong rate. When simulating cloth and other physical materials that require lots of high-frequency detail to be realistic, backward Euler may exhibit undesirable dampening. Furthermore, we have to invert F [·] to solve for  cid:126 yk+1.   314  cid:4  Numerical Algorithms  Figure 15.7 Backward Euler integration is unconditionally stable, so no matter how large a time step h with the same initial condition, the resulting approximate so- lution of y cid:48  = ay does not diverge. While the output is stable, when h is large the result does not approximate the continuous solution y = Ceat eﬀectively. 15.3.3 Trapezoidal Method Suppose that in addition to having  cid:126 yk at time t and  cid:126 yk+1 at time t + h, we also know  cid:126 yk+1 2 at the halfway point in time t + h 2. Then, by our derivation of centered diﬀerencing   cid:126 yk+1 =  cid:126 yk + hF [ cid:126 yk+1 2] + O h3 .  In our derivation of error bounds for the trapezoidal rule in §14.2.3, we derived the following relationship via Taylor’s theorem:  Substituting this equality into the expression for  cid:126 yk+1 yields a second-order ODE integrator, the trapezoid method :  F [ cid:126 yk+1] + F [ cid:126 yk]  2  = F [ cid:126 yk+1 2] + O h2 .   cid:126 yk+1 =  cid:126 yk + h  F [ cid:126 yk+1] + F [ cid:126 yk]  2  Like backward Euler, this method is implicit since we must solve this equation for  cid:126 yk+1.  Example 15.10  Trapezoidal integrator . Returning to the ODE  cid:126 y cid:48  = A cid:126 y from Exam- ple 15.9, trapezoidal integration solves the system   cid:126 yk+1 =  cid:126 yk + h  A cid:126 yk+1 + A cid:126 yk  2  =⇒  cid:126 yk+1 = cid:18 In×n −  hA  2  cid:19 −1 cid:18 In×n +  hA  2  cid:19   cid:126 yk.  To carry out stability analysis on y cid:48  = ay, the example above shows time steps of the  trapezoidal method satisfy  Consequently, the method is stable when  y0.  2 ha  yk = cid:18  1 + 1 2 ha cid:19 k 1 − 1  cid:12  cid:12  cid:12  cid:12  2 ha cid:12  cid:12  cid:12  cid:12  < 1.  1 + 1 1 − 1  2 ha  This inequality holds whenever a   0, showing that the trapezoid method is unconditionally stable.   Ordinary Diﬀerential Equations  cid:4  315  Figure 15.8 The trapezoidal method is unconditionally stable, so regardless of the step size h the solution curves always approach y = 0; when h is large, however, the output oscillates about zero as it decays.  Despite its higher order of accuracy with maintained stability, the trapezoid method has some drawbacks that make it less popular than backward Euler for large time steps. In particular, consider the ratio  R ≡  yk+1 yk  =  1 + 1 1 − 1  2 ha 2 ha  .  When a < 0, for large enough h this ratio eventually becomes negative; as h → ∞, we have R → −1. As illustrated in Figure 15.8, this observation shows that if time steps h are too large, the trapezoidal method of integration tends to introduce undesirable oscillatory behavior not present in theoretical solutions Ceat of y cid:48  = ay. 15.3.4 Runge-Kutta Methods  A class of integrators can be derived by making the following observation:   cid:126 yk+1 =  cid:126 yk + cid:90  tk+h =  cid:126 yk + cid:90  tk+h  tk  tk   cid:126 y cid:48  t  dt by the Fundamental Theorem of Calculus  F [ cid:126 y t ] dt since  cid:126 y satisﬁes  cid:126 y cid:48  t  = F [ cid:126 y t ].  Using this formula outright does not help design a method for time-stepping, since we do not know  cid:126 y t  a priori. Approximating the integral using quadrature rules from the previous chapter, however, produces a class of well-known strategies for ODE integration.  Suppose we apply the trapezoidal quadrature rule to the integral for  cid:126 yk+1. Then,   cid:126 yk+1 =  cid:126 yk +   F [ cid:126 yk] + F [ cid:126 yk+1]  + O h3 .  h 2  This is the formula we wrote for the trapezoidal method in §15.3.3. If we wish to ﬁnd an explicit rather than implicit method with the accuracy of the trapezoidal time-stepping, however, we must replace F [ cid:126 yk+1] with a high-accuracy approximation that is easier to evaluate:  F [ cid:126 yk+1] = F [ cid:126 yk + hF [ cid:126 yk] + O h2 ] by the forward Euler order of accuracy  = F [ cid:126 yk + hF [ cid:126 yk]] + O h2  by Taylor’s theorem.   316  cid:4  Numerical Algorithms  Since it gets scaled by h, making this substitution for  cid:126 yk+1 does not aﬀect the order of approximation of the trapezoidal time step. This change results in a new approximation:  h 2  1 2   cid:126 yk+1 =  cid:126 yk +   F [ cid:126 yk] + F [ cid:126 yk + hF [ cid:126 yk]]  + O h3 .  Ignoring the O h3  terms yields a new integrator known as Heun’s method, which is second- order accurate and explicit.  To evaluate the stability of Heun’s method for y cid:48  = ay with a < 0, we expand  yk+1 = yk +  h 2   ayk + a yk + hayk   = cid:18  1  2  h2a2 + ha + 1 cid:19  yk.  From this substitution, the method is stable when  −1 ≤ 1 + ha +  h2a2 ≤ 1 ⇐⇒ −4 ≤ 2ha + h2a2 ≤ 0.  The inequality on the right is equivalent to writing h ≤ 2 , and the inequality on the left a is always true for h > 0 and a < 0. Hence, the stability condition for Heun’s method can be written h ≤ 2 a Heun’s method is an example of a Runge-Kutta integrator derived by starting from a quadrature rule and substituting Euler steps to approximate F [ cid:126 yk+ cid:96 ], for  cid:96  > 0. Forward Euler is a ﬁrst-order accurate Runge-Kutta method, and Heun’s method is second-order. A popular fourth-order Runge-Kutta method  abbreviated “RK4”  is given by:  , the same as the stability condition for forward Euler.   cid:126 yk+1 =  cid:126 yk +  h 6 where  cid:126 k1 = F [ cid:126 yk]    cid:126 k1 + 2 cid:126 k2 + 2 cid:126 k3 +  cid:126 k4   1 2 1 2   cid:126 k2 = F cid:20  cid:126 yk + h cid:126 k1 cid:21   cid:126 k3 = F cid:20  cid:126 yk + h cid:126 k2 cid:21   cid:126 k4 = F cid:104  cid:126 yk + h cid:126 k3 cid:105   This formula is constructed from Simpson’s quadrature rule.  Runge-Kutta methods are popular because they are explicit but provide high degrees of accuracy. The cost of this accuracy, however, is that F [·] must be evaluated more times to carry out a single time step. Implicit Runge-Kutta integrators also have been constructed, for poorly conditioned ODEs.  15.3.5 Exponential Integrators We have focused our stability and accuracy analyses on the model equation y cid:48  = ay. If this ODE is truly an inﬂuential test case, however, we have neglected a key piece of information: We know the solution of y cid:48  = ay in closed form as y = Ceat! We might as well incorporate this formula into an integration scheme to achieve 100% accuracy on the model equation. That is, we can design a class of integrators that achieves strong accuracy when F [·] is nearly linear, potentially at the cost of computational eﬃciency.   Ordinary Diﬀerential Equations  cid:4  317  Assuming A is symmetric, using the eigenvector method from §15.2.3 we can write the solution of the ODE  cid:126 y cid:48  = A cid:126 y as  cid:126 y t  = eAt cid:126 y 0 , where eAt is a matrix encoding the transformation from  cid:126 y 0  to  cid:126 y t   see Exercise 6.10 . Starting from this formula, integrating in time by writing  cid:126 yk+1 = eAh cid:126 yk achieves perfect accuracy on the linear model equation; our strategy is to use this formula to construct integrators for the nonlinear case.  When F is smooth, we can attempt to factor the ODE  cid:126 y cid:48  = F [ cid:126 y] as   cid:126 y cid:48  = A cid:126 y + G[ cid:126 y],  where G is a nonlinear but small function and A ∈ Rn×n. Taking A to be the Jacobian of F makes this factorization agree with its ﬁrst-order Taylor expansion. Exponential integrators integrate the A cid:126 y part using the exponential formula and approximate the eﬀect of the nonlinear G part separately.  We start by deriving a “variation of parameters” formula from the classical theory of ODEs. Rewriting the original ODE as  cid:126 y cid:48  − A cid:126 y = G[ cid:126 y], suppose we multiply both sides by e−At to obtain e−At  cid:126 y cid:48  − A cid:126 y  = e−AtG[ cid:126 y]. The left-hand side satisﬁes  after applying the identity AeAt = eAtA  see Exercise 15.2 , implying d dt e−At cid:126 y t   = e−AtG[ cid:126 y]. Integrating this expression from 0 to t shows  e−At  cid:126 y cid:48  − A cid:126 y  =  d  dt cid:0 e−At cid:126 y t  cid:1  ,  e−Aτ G[ cid:126 y τ  ] dτ,  e−Aτ G[ cid:126 y τ  ] dτ  eA t−τ  G[ cid:126 y τ  ] dτ.  0  e−At cid:126 y t  −  cid:126 y 0  = cid:90  t  cid:126 y t  = eAt cid:126 y 0  + eAt cid:90  t = eAt cid:126 y 0  + cid:90  t  cid:126 yk+1 = eAh cid:126 yk + cid:90  tk+h  tk  0  0  eA tk+h−t G[ cid:126 y t ] dt.   cid:126 yk+1 ≈ eAh cid:126 yk + cid:34  cid:90  h  0  eA h−t  dt cid:35  G[ cid:126 yk].   cid:126 yk+1 = eAh cid:126 yk + A−1 eAh − In×n G[ cid:126 yk].  Similar to the Runge-Kutta methods, exponential integrators apply quadrature to the in- tegral on the right-hand side to approximate the time step to  cid:126 yk+1.  For example, the ﬁrst-order exponential integrator applies forward Euler to the nonlinear  G term by making a constant approximation G[ cid:126 y t ] ≈ G[ cid:126 yk], yielding  As shown in Exercise 15.5, the integral can be taken in closed form, leading to the expression  Analyzing exponential integrators like this one requires techniques beyond using the linear model equation, since they are designed to integrate linear ODEs exactly. Intuitively, expo- nential integrators behave best when G ≈ 0, but the cost of this high numerical performance is the use of a matrix exponential, which is diﬃcult to compute or apply eﬃciently.  or equivalently,  Slightly generalizing this formula shows   318  cid:4  Numerical Algorithms 15.4 MULTIVALUE METHODS The transformations in §15.2.1 reduced all explicit ODEs to the form  cid:126 y cid:48  = F [ cid:126 y], which can be integrated using the methods introduced in the previous section. While all explicit ODEs can be written this way, however, it is not clear that they always should be when designing a high-accuracy integrator.  When we reduced k-th order ODEs to ﬁrst order, we introduced new variables repre- senting the ﬁrst through  k − 1 -st derivatives of the desired output function  cid:126 y t . The integrators in the previous section then approximate  cid:126 y t  and these k − 1 derivatives with equal accuracy, since in some sense they are treated “democratically” in ﬁrst-order form. A natural question is whether we can relax the accuracy of the approximated derivatives of  cid:126 y t  without aﬀecting the quality of the  cid:126 y t  estimate itself.  To support this perspective, consider the Taylor series   cid:126 y tk + h  =  cid:126 y tk  + h cid:126 y cid:48  tk  +   cid:126 y cid:48  cid:48  tk  + O h3 .  h2 2  If we perturb  cid:126 y cid:48  tk  by some value on the order O h2 , the quality of this Taylor series approximation does not change, since  Perturbing  cid:126 y cid:48  cid:48  tk  by a value on the order O h  has a similar eﬀect, since  h · [ cid:126 y cid:48  tk  + O h2 ] = h cid:126 y cid:48  tk  + O h3 .  h2 2 · [ cid:126 y cid:48  cid:48  tk  + O h ] =  h2 2   cid:126 y cid:48  cid:48  tk  + O h3 .  Based on this argument, multivalue methods integrate  cid:126 y k  t  = F [t,  cid:126 y cid:48  t ,  cid:126 y cid:48  cid:48  t , . . . ,  cid:126 y k−1  t ] using less accurate estimates of the higher-order derivatives of  cid:126 y t .  We will restrict our discussion to the second-order case  cid:126 y cid:48  cid:48  t  = F [t,  cid:126 y,  cid:126 y cid:48 ], the most common case for ODE integration thanks to Newton’s second law F = ma. Extending the methods we consider to higher order, however, follows similar if notationally more complex arguments. For the remainder of this section, we will deﬁne a “velocity” vector  cid:126 v t  ≡  cid:126 y cid:48  t  and an “acceleration” vector  cid:126 a ≡  cid:126 y cid:48  cid:48  t . By the reduction to ﬁrst order, we wish to solve the following order system:   cid:126 y cid:48  t  =  cid:126 v t   cid:126 v cid:48  t  =  cid:126 a t   cid:126 a t  = F [t,  cid:126 y t ,  cid:126 v t ].  Our goal is to derive integrators tailored to this system, evaluated based on the accuracy of estimating  cid:126 y t  rather than  cid:126 v t  or  cid:126 a t .  15.4.1 Newmark Integrators  We begin by deriving the class of Newmark integrators following the development in [46]. Denote  cid:126 yk,  cid:126 vk, and  cid:126 ak as position, velocity, and acceleration vectors at time tk; our goal is to advance to time tk+1 ≡ tk + h.  By the Fundamental Theorem of Calculus,   cid:126 vk+1 =  cid:126 vk + cid:90  tk+1  tk   cid:126 a t  dt.   Ordinary Diﬀerential Equations  cid:4  319  We also can write  cid:126 yk+1 as an integral involving  cid:126 a t  by deriving an error estimate developed in some proofs of Taylor’s theorem:   cid:126 yk+1 =  cid:126 yk + cid:90  tk+1  tk   cid:126 v t  dt by the Fundamental Theorem of Calculus  =  cid:126 yk + [t cid:126 v t ]tk+1  t cid:126 a t  dt after integration by parts  tk − cid:90  tk+1  tk  tk  =  cid:126 yk + tk+1 cid:126 vk+1 − tk cid:126 vk − cid:90  tk+1 =  cid:126 yk + h cid:126 vk + tk+1 cid:126 vk+1 − tk+1 cid:126 vk − cid:90  tk+1 =  cid:126 yk + h cid:126 vk + tk+1  cid:126 vk+1 −  cid:126 vk  − cid:90  tk+1 =  cid:126 yk + h cid:126 vk + tk+1 cid:90  tk+1  cid:126 a t  dt − cid:90  tk+1 =  cid:126 yk + h cid:126 vk + cid:90  tk+1   tk+1 − t  cid:126 a t  dt.  tk  tk  tk  tk  tk  t cid:126 a t  dt by expanding the diﬀerence term  t cid:126 a t  dt by adding and subtracting h cid:126 vk  t cid:126 a t  dt after factoring out tk+1  t cid:126 a t  dt since  cid:126 v cid:48  t  =  cid:126 a t   Fix a constant τ ∈ [tk, tk+1]. Then, we can write expressions for  cid:126 ak and  cid:126 ak+1 using the Taylor series about τ :   cid:126 ak =  cid:126 a τ   +  cid:126 a cid:48  τ   tk − τ   + O h2    cid:126 ak+1 =  cid:126 a τ   +  cid:126 a cid:48  τ   tk+1 − τ   + O h2 .  For any constant γ ∈ R, scaling the expression for  cid:126 ak by 1 − γ, scaling the expression for  cid:126 ak+1 by γ, and summing shows  cid:126 a τ   =  1 − γ  cid:126 ak + γ cid:126 ak+1 +  cid:126 a cid:48  τ    γ − 1  tk − τ   − γ tk+1 − τ    + O h2   =  1 − γ  cid:126 ak + γ cid:126 ak+1 +  cid:126 a cid:48  τ   τ − hγ − tk  + O h2  after substituting tk+1 = tk + h. Integrating  cid:126 a t  from tk to tk+1 yields the change in velocity. Substituting the approximation above shows:   cid:126 vk+1 −  cid:126 vk = cid:90  tk+1  tk   cid:126 a τ   dτ =  1 − γ h cid:126 ak + γh cid:126 ak+1 + cid:90  tk+1 =  1 − γ h cid:126 ak + γh cid:126 ak+1 + O h2 ,  tk   cid:126 a cid:48  τ   τ − hγ − tk  dτ + O h3   where the second step holds because  τ − tk − hγ = O h  for τ ∈ [tk, tk+1] and the interval of integration is of width h. Rearranging shows   cid:126 vk+1 =  cid:126 vk +  1 − γ h cid:126 ak + γh cid:126 ak+1 + O h2 .  Starting again from the approximation we wrote for  cid:126 a τ  —this time using a new constant β rather than γ—we can also develop an approximation for  cid:126 yk+1. To do so, we will work with the integrand in the Taylor estimate for  cid:126 yk+1:   cid:90  tk+1  tk   tk+1 − t  cid:126 a t  dt = cid:90  tk+1   tk+1 − τ    1 − β  cid:126 ak + β cid:126 ak+1 +  cid:126 a cid:48  τ   τ − hβ − tk   dτ  tk + O h3   1 − β h2 cid:126 ak +  1 2  1 2  =  βh2 cid:126 ak+1 + O h2  by a similar simpliﬁcation.   320  cid:4  Numerical Algorithms  We can use this observation to write the Taylor series error estimate for  cid:126 yk+1 in a diﬀerent form:  Summarizing this technical argument, we have derived the class of Newmark schemes,  each characterized by the two ﬁxed parameters γ and β:   cid:126 yk+1 =  cid:126 yk + h cid:126 vk + cid:90  tk+1 =  cid:126 yk + h cid:126 vk + cid:18  1  tk   tk+1 − t  cid:126 a t  dt from before  2 − β cid:19  h2 cid:126 ak + βh2 cid:126 ak+1 + O h2 .   cid:126 yk+1 =  cid:126 yk + h cid:126 vk + cid:18  1  2 − β cid:19  h2 cid:126 ak + βh2 cid:126 ak+1   cid:126 vk+1 =  cid:126 vk +  1 − γ h cid:126 ak + γh cid:126 ak+1   cid:126 ak = F [tk,  cid:126 yk,  cid:126 vk]  This integrator is accurate up to O h2  in each time step, making it globally ﬁrst-order accurate. Depending on γ and β, the integrator can be implicit, since  cid:126 ak+1 appears in the expressions for  cid:126 yk+1 and  cid:126 vk+1.  Speciﬁc choices of β and γ yield integrators with additional properties:    β = γ = 0 gives the constant acceleration integrator:   cid:126 yk+1 =  cid:126 yk + h cid:126 vk +   cid:126 vk+1 =  cid:126 vk + h cid:126 ak.  1 2  h2 cid:126 ak  This integrator is explicit and holds exactly when the acceleration is a constant func- tion of time.    β = 1 2, γ = 1 gives the constant implicit acceleration integrator:  The velocity is stepped implicitly using backward Euler, giving ﬁrst-order accuracy. The  cid:126 y update, however, can be written  which coincides with the trapezoidal rule. Hence, this is our ﬁrst example of a scheme where the velocity and position updates have diﬀerent orders of accuracy. This tech- nique, however, is still only globally ﬁrst-order accurate in  cid:126 y.    β = 1 4, γ = 1 2 gives the following second-order trapezoidal scheme after some algebra:   cid:126 yk+1 =  cid:126 yk + h cid:126 vk +  h2 cid:126 ak+1  1 2   cid:126 vk+1 =  cid:126 vk + h cid:126 ak+1.   cid:126 yk+1 =  cid:126 yk +  h  cid:126 vk +  cid:126 vk+1 ,   cid:126 yk+1 =  cid:126 yk +  h  cid:126 vk +  cid:126 vk+1    cid:126 vk+1 =  cid:126 vk +  h  cid:126 ak +  cid:126 ak+1 .  1 2  1 2 1 2   Ordinary Diﬀerential Equations  cid:4  321    β = 0, γ = 1 2 gives a second-order accurate central diﬀerencing scheme. In the canon-  ical form, it is written  The method earns its name because simplifying the equations above leads to the alternative form:   cid:126 yk+1 =  cid:126 yk + h cid:126 vk +  1 2  h2 cid:126 ak   cid:126 vk+1 =  cid:126 vk +  h  cid:126 ak +  cid:126 ak+1 .  1 2   cid:126 vk+1 =   cid:126 ak+1 =   cid:126 yk+2 −  cid:126 yk  cid:126 yk+2 − 2 cid:126 yk+1 +  cid:126 yk  2h  .  h2  Newmark integrators are unconditionally stable when 4β > 2γ > 1, with second-order accuracy exactly when γ = 1 2.  15.4.2 Staggered Grid and Leapfrog  A diﬀerent way to achieve second-order accuracy in stepping  cid:126 y is to use centered diﬀerences about tk+1 2 ≡ tk + h 2:   cid:126 yk+1 =  cid:126 yk + h cid:126 vk+1 2.  Rather than attempting to approximate  cid:126 vk+1 2 from  cid:126 vk and or  cid:126 vk+1, we can process veloc- ities  cid:126 v directly at half points on the grid of time steps.  A similar update steps forward the velocities with the same accuracy:   cid:126 vk+3 2 =  cid:126 vk+1 2 + h cid:126 ak+1.  A lower-order approximation suﬃces for the acceleration term since it is a higher-order derivative:   cid:126 ak+1 = F cid:20 tk+1,  cid:126 xk+1,  1 2    cid:126 vk+1 2 +  cid:126 vk+3 2  cid:21  .  This expression can be substituted into the equation for  cid:126 vk+3 2.  When F [·] has no dependence on  cid:126 v, e.g., when simulating particles without wind resis-  tance, the method is fully explicit:   cid:126 yk+1 =  cid:126 yk + h cid:126 vk+1 2  cid:126 ak+1 = F [tk+1,  cid:126 yk+1]  cid:126 vk+3 2 =  cid:126 vk+1 2 + h cid:126 ak+1  This is known as the leapfrog integrator, thanks to the staggered grid of times and the fact that each midpoint is used to update the next velocity or position.  A distinguishing property of the leapfrog scheme is its time reversibility.∗ Assume we have used the leapfrog integrator to generate   cid:126 yk+1,  cid:126 vk+3 2,  cid:126 ak+1 . Starting at tk+1, we might reverse the direction of time to step backward. The leapfrog equations give   cid:126 vk+1 2 =  cid:126 vk+3 2 +  −h  cid:126 ak+1   cid:126 yk =  cid:126 yk+1 − h cid:126 vk+1 2.  These formulas invert the forward time step exactly. That is, if we run the leapfrog in reverse, we trace the solution back to where it started exactly, up to rounding error.  ∗Discussion of time reversibility contributed by Julian Kates-Harbeck.   322  cid:4  Numerical Algorithms  Figure 15.9 Leapfrog integration computes velocities at half time steps; here arrows denote dependencies between computed values. If the initial conditions specify  cid:126 v at t = 0, an initial half time step must be carried out to approximate  cid:126 v1 2.  A consequence of reversibility is that errors in position, energy, and angular momen- tum tend to cancel out over time as opposed to accumulating. For instance, for problems where the acceleration only depends on position, angular momentum is conserved exactly by leapfrog integration, and energy remains stable over time, whereas other even higher- order schemes can induce signiﬁcant “drift” of these quantities. Symmetry, second-order accuracy for “ﬁrst-order work”  i.e., the same amount of computation as forward Euler integration , and conservation properties make leapfrog integration a popular method for physical simulation. These properties classify the leapfrog method as a symplectic integra- tor, constructed to conserve the continuous structure of ODEs coming from Hamiltonian dynamics and related physical systems.  If F [·] has dependence on  cid:126 v, then this “staggered grid” method becomes implicit. Such dependence on velocity often is symmetric. For instance, wind resistance changes sign if you reverse the direction in which you are moving. This property makes the matrices symmetric in the implicit step for updating velocities, enabling the application of conjugate gradients and related iterative methods.  15.5 COMPARISON OF INTEGRATORS  This chapter has introduced a sampling from the remarkably large pantheon of ODE in- tegrators. Choosing the right ODE integrator for a given problem is a challenging task representing a careful balancing act between accuracy, stability, computational eﬃciency, and assorted special properties like reversibility. The table in Figure 15.10 compares the basic properties of the methods we considered.  In practice, it may require some experimentation to determine the proper integrator given an ODE problem; thankfully, most of the integrators we have introduced are relatively easy to implement. In addition to the generic considerations we have discussed in this chapter, additional “domain-speciﬁc” concerns also inﬂuence the choice of ODE integrators, including the following:    In computer graphics and other ﬁelds prioritizing visual eﬀect over reproducibility in the real world, it may be more important that a time-stepping method looks right   cid:30 y0   cid:30 y1   cid:30 y2   cid:30 y3   cid:30 y4   cid:30 v0   cid:30 v1 2   cid:30 v3 2   cid:30 v5 2   cid:30 v7 2   cid:30 a0   cid:30 a1   cid:30 a2   cid:30 a3   cid:30 a4  +t   Ordinary Diﬀerential Equations  cid:4  323  Figure 15.10 Comparison of ODE integrators.  than whether the numerical output is perfect. For instance, simulation tools for visual eﬀects need to produce ﬂuids, gases, and cloth that exhibit high-frequency swirls, vortices, and folds. These features may be dampened by a backward Euler integrator, even if it is more likely to be stable than other alternatives.    Most of our analysis used Taylor series and other localized arguments, but long-term behavior of certain integrators can be favorable even if individual time steps are sub- optimal. For instance, forward Euler integration tends to add energy to oscillatory ODEs, while backward Euler removes it. If we wish to simulate a pendulum swinging in perpetuity, neither of these techniques will suﬃce.    Some ODEs operate in the presence of constraints. For instance, if we simulate a ball attached to a string, we may not wish for the string to stretch beyond its natural length. Methods like forward Euler and leapfrog integration can overshoot such con- straints, so an additional projection step may be needed to enforce the constraints more exactly.    A degree of adaptivity is needed for applications in which discrete events can happen during the course of solving an ODE. For instance, when simulating the dynamics of a piece of cloth, typically parts of the cloth can run into each other or into objects in their surroundings. These collision events can occur at fractional time steps and must be handled separately to avoid interpenetration of objects in a scene [5].    For higher-quality animation and physical predictions, some ODE integrators can output not only the conﬁguration at discrete time steps but also some indicator  e.g., an interpolatory formula  approximating continuous behavior between the time steps.   If the function F in  cid:126 y cid:48  = F [ cid:126 y] is smooth and diﬀerentiable, the derivatives of F can  be used to improve the quality of time-stepping methods.  Many of these problems are diﬃcult to handle eﬃciently in large-scale simulations and in cases where computational power is relatively limited.  Integrator  Section Accuracy  Stability  Notes  Implicit or explicit? Explicit Implicit Implicit Explicit Explicit Explicit  First First Second Second Fourth First  Conditional Unconditional Unconditional Large steps oscillate Conditional Conditional Conditional  First  Implicit  Conditional  Forward Euler Backward Euler Trapezoidal Heun RK4 1st-order expo- nential Newmark  §15.3.1 §15.3.2 §15.3.3 §15.3.4 §15.3.4 §15.3.5 §15.4.1  §15.4.2 §15.4.2  Staggered Leapfrog  Second Second  Implicit Explicit  Conditional Conditional  Needs matrix expo- nential For 2nd-order ODE; 2nd-order accurate when γ = 1 2; explicit when β = γ = 0 For 2nd-order ODE For 2nd-order ODE; reversible; F [·] must not depend on  cid:27 v   324  cid:4  Numerical Algorithms 15.6 EXERCISES  15.1 Some practice discretizing an ODE:   a  Suppose we wish to solve the ODE dy dt = − sin y numerically. For time step h > 0, write the implicit backward Euler equation for approximating yk+1 at t =  k + 1 h given yk at t = kh.   b  Write the Newton iteration for solving the equation from Exercise 15.1a for yk+1.  15.2 We continue our discussion of the matrix exponential introduced in Exercise 6.10 and used in our discussion of exponential integrators. For this problem, assume A ∈ Rn×n is a symmetric matrix.   a  Show that A commutes with eAt for any t ≥ 0. That is, justify the formula  AeAt = eAtA.   b  Recall that we can write  eAt = In×n + At +   At 2   At 3  +  + ··· .  3!  2!  For suﬃciently small h ≥ 0, prove a similar formula for matrix inverses:   In×n − hA −1 = In×n + hA +  hA 2 +  hA 3 + ···   c  Which of the two series from Exercise 15.2b should converge faster? Based on this observation, compare the computational cost of a single backward Euler iteration  see Example 15.9  versus that of an iteration of the exponential integrator from §15.3.5 using these formulas.  15.3 Suppose we are solving a second-order ODE using the leapfrog integrator. We are given initial conditions  cid:126 y 0  and  cid:126 v 0 , the position and velocity vectors at time t = 0. But, the leapfrog scheme maintains velocities at the half time steps. Propose a way to initialize  cid:126 v1 2 at time t = h 2, and argue that your initialization does not aﬀect the order of accuracy of the leapfrog integrator if it is run for suﬃciently many time steps.  15.4 Suppose we wish to approximate solutions to  cid:126 y cid:48  cid:48  = F [ cid:126 y]. Add together Taylor expan- sions for  cid:126 y t + h  and  cid:126 y t − h  to derive the Verlet algorithm for predicting  cid:126 yk+1 from  cid:126 yk and  cid:126 yk−1, which induces O h4  integration error in a single time step.  15.5 Verify the following formula used in §15.3.5 for symmetric A ∈ Rn×n:   cid:90  h  0  eA h−t  dt = A−1 eAh − In×n .  Also, derive a global order of accuracy in the form O hk  for some k ∈ N for the ﬁrst-order exponential integrator.  15.6 In this problem, we will motivate an ODE used in computer graphics applications that does not come from Newton’s laws. Throughout this problem, assume f, g : [0, 1] → R are diﬀerentiable functions with g 0  = g 1  = 0. We will derive continuous and discrete versions of the screened Poisson equation, used for smoothing  see, e.g., [24] .   Ordinary Diﬀerential Equations  cid:4  325   a  So far our optimization problems have been to ﬁnd points  cid:126 x∗ ∈ Rn minimizing some function h  cid:126 x , but sometimes our unknown is an entire function. Thankfully, the “variational” approach still is valid in this case. Explain in words what the following energies, which take a function f as input, measure about f :  0  f  t  − f0 t  2 dt for some ﬁxed function f0 : [0, 1] → R. 0  f cid:48  t  2 dt.   b  For an energy functional E[·] like the two above, explain how the following ex- pression for dE f ; g   the Gˆateaux derivative of E  can be thought of as the “directional derivative of E at f in the g direction”:   i  E1[f ] ≡ cid:82  1  ii  E2[f ] ≡ cid:82  1  dE f ; g  =  d dε  E[f + εg] cid:12  cid:12 ε=0.   c  Again assuming g 0  = g 1  = 0, derive the following formulae:   i  dE1 f, g  = cid:82  1  ii  dE2 f, g  = cid:82  1  0 2 f  t  − f0 t  g t  dt. 0 −2f cid:48  cid:48  t g t  dt.  Hint: Apply integration by parts to get rid of g cid:48  t ; recall our assumption g 0  = g 1  = 0.   d  Suppose we wish to approximate f0 with a smoother function f . One reasonable model for doing so is to minimize E[f ] ≡ E1[f ]+αE2[f ] for some α > 0 controlling the trade-oﬀ between similarity to f0 and smoothness. Using the result of 15.6c, argue informally that an f minimizing this energy should satisfy the diﬀerential equation f  t  − f0 t  = αf cid:48  cid:48  t  for t ∈  0, 1 .   e  Now, suppose we discretize f on [0, 1] using n evenly spaced samples f 1, f 2, . . . , f n ∈ R and f0 using samples f 1 0 . Devise a discrete ana- log of E[f ] as a quadratic energy in the f k’s. For k  cid:54 ∈ {1, n}, does diﬀerentiating E with respect to fk yield a result analogous to Exercise 15.6d?  0 , . . . , f n  0 , f 2  15.7  Adapted from [21]  The swing angle θ of a pendulum under gravity satisﬁes the  following ODE:  θ cid:48  cid:48  = − sin θ,  where θ 0  < π and θ cid:48  0  = 0.  a  Suppose θ t  solves the ODE. Show that the following value  representing the  energy of the system  is constant as a function of t:  E t  ≡   θ cid:48  2 − cos θ.  1 2   b  Many ODE integrators drift away from the desired output as time progresses over larger periods. For instance, forward Euler can add energy to a system by overshooting, while backward Euler tends to damp out motion and remove en- ergy. In many computer graphics applications, quality long-term behavior can be prioritized, since large-scale issues cause visual artifacts. The class of symplectic integrators is designed to avoid this issue.   326  cid:4  Numerical Algorithms  Figure 15.11 Three simulations of an undamped oscillator.  Denote ω ≡ θ cid:48 . The symplectic Euler scheme makes a series of estimates θ0, θ1, θ2, θ3, . . . and ω0, ω1, ω2, ω3, . . . at time t = 0, h, 2h, 3h, . . . using the fol- lowing iteration:   c  Suppose we make the small-angle approximation sin θ ≈ θ and decide to solve the  linear ODE θ cid:48  cid:48  = −θ instead. Now, symplectic Euler takes the following form:  θk+1 = θk + hωk ωk+1 = ωk − h sin θk+1.  Ek ≡  ω2 k − cos θk.  1 2  Deﬁne  Show that Ek+1 = Ek + O h2 .  Write a 2 × 2 matrix A such that  θk+1 = θk + hωk ωk+1 = ωk − hθk+1.   cid:18  θk+1 ωk+1  cid:19  = A cid:18  θk ωk  cid:19  .   d   If we deﬁne Ek ≡ ω2 from 15.7c. In other words, Ek is constant from time step to time step.  k, show that Ek+1 = Ek in the iteration  k + hωkθk + θ2  15.8 Suppose we simulate a spring by solving the ODE y cid:48  cid:48  = −y with y 0  = 0 and y cid:48  0  = 1. We obtain the three plots of y t  in Figure 15.11 by using forward Euler, backward Euler, and symplectic Euler time integration. Determine which plot is which, and justify your answers using properties of the three integrators.  15.9 Suppose we discretize Schr¨odinger’s equation for a particular quantum simulation yielding an ODE  cid:126 x cid:48  = A cid:126 x, for  cid:126 x t  ∈ Cn and A ∈ Cn×n. Furthermore, suppose that A is self-adjoint and negative deﬁnite, that is, A satisﬁes the following properties:    Self-adjoint: aij = ¯aji, where a + bi = a − bi.  x  t   Ordinary Diﬀerential Equations  cid:4  327    Negative deﬁnite:  cid:126 ¯x cid:62 A cid:126 x ≤ 0  and is real  for all  cid:126 x ∈ Cn\{ cid:126 0}. Here we deﬁne    cid:126 ¯x i ≡ ¯xi.  Derive a backward Euler formula for solving this ODE and show that each step can be carried out using conjugate gradients. Hint: Before discretizing, convert the ODE to a real-valued system by separating imaginary and real parts of the variables and constants.  15.10  “Phi functions,” [89]  Exponential integrators made use of ODEs with known solu- tions to boost numerical quality of time integration. This strategy can be extended using additional closed-form solutions.   a  Deﬁne ϕk x  recursively by deﬁning ϕ0 x  ≡ ex and recursively writing  ϕk+1 x  ≡  1  x cid:18 ϕk x  −  1  k! cid:19  .  Write the Taylor expansions of ϕ0 x , ϕ1 x , ϕ2 x , and ϕ3 x  about x = 0.   b  Show that for k ≥ 1,  ϕk x  =  e 1−θ xθk−1 dθ.  1   k − 1 ! cid:90  1  0  Hint: Use integration by parts to show that the recursive relationship from Ex- ercise 15.10a holds.   c  Check the following formula for ϕ cid:48 k x  when k ≥ 1: x cid:20 ϕk x  x − k  +  ϕ cid:48 k x  =  1  1   k − 1 ! cid:21  .   d  Show that the ODE   cid:126 u cid:48  t  = L cid:126 u t  +  cid:126 v0 +  t cid:126 v1 +  t2 cid:126 v2 +  1 1!  1 2!  1 3!  t3 cid:126 v3 + ···  subject to  cid:126 u 0  =  cid:126 u0 is solved by   cid:126 u t  = ϕ0 tL  cid:126 u0 + tϕ1 tL  cid:126 v0 + t2ϕ2 tL  cid:126 v1 + t3ϕ3 tL  cid:126 v2 + ··· .  When L is a matrix, assume ϕk tL  is evaluated using the formula from Exer- cise 15.10b.  ODE  cid:126 y cid:48  = A cid:126 y + cid:80  cid:96    e  Use the closed-form solution from Exercise 15.10d to propose an integrator for the k!  cid:126 vk + G[ cid:126 y] that provides exact solutions when G[ cid:126 y] ≡  cid:126 0. 15.11  “Fehlberg’s method,” [39] via notes by J. Feldman  We can approximate the error of an ODE integrator to help choose appropriate step sizes given a desired level of accuracy.  k=1  tk   328  cid:4  Numerical Algorithms   a  Suppose we carry out a single time step of  cid:126 y cid:48  = F [ cid:126 y] with size h starting from   cid:126 y 0  =  cid:126 y0. Make the following deﬁnitions:  We can write two estimates of  cid:126 y h :   cid:126 v1 ≡ F [ cid:126 y0]  cid:126 v2 ≡ F [ cid:126 y0 + h cid:126 v1] h 4   cid:126 v3 ≡ F cid:20  cid:126 y0 +    cid:126 v1 +  cid:126 v2  cid:21  .   cid:126 y 1  ≡  cid:126 y0 +  cid:126 y 2  ≡  cid:126 y0 +  h 2 h 6    cid:126 v1 +  cid:126 v2     cid:126 v1 +  cid:126 v2 + 4 cid:126 v3 .  Show that there is some K ∈ R such that  cid:126 y 1  =  cid:126 y h  + Kh3 + O h4  and  cid:126 y 2  =  cid:126 y h  + O h4 .   b  Use this relationship to derive an approximation of the amount of error intro- duced per unit increase of time t if we use  cid:126 y 1  as an integrator. If this value is too large, adaptive integrators reject the step and try again with a smaller h.   C H A P T E R16 Partial Differential Equations  CONTENTS  16.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16.2 Statement and Structure of PDEs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16.2.1 Properties of PDEs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16.2.2 Boundary Conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16.3 Model Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16.3.1 Elliptic PDEs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16.3.2 Parabolic PDEs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16.3.3 Hyperbolic PDEs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16.4 Representing Derivative Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16.4.1 Finite Diﬀerences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16.4.2 Collocation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16.4.3 Finite Elements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16.4.4 Finite Volumes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16.4.5 Other Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16.5 Solving Parabolic and Hyperbolic Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . 16.5.1 Semidiscrete Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16.5.2 Fully Discrete Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16.6 Numerical Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16.6.1 Consistency, Convergence, and Stability . . . . . . . . . . . . . . . . . . . . . . . . 16.6.2 Linear Solvers for PDE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  330 335 335 336 338 338 339 340 341 342 346 347 350 351 352 352 353 354 354 354  I NTUITION for ordinary diﬀerential equations largely stems from the time evolution of  physical systems. Equations like Newton’s second law, determining the motion of physical objects over time, dominate the literature on ODE problems; additional examples come from chemical concentrations reacting over time, populations of predators and prey interacting from season to season, and so on. In each case, the initial conﬁguration—e.g., the positions and velocities of particles in a system at time zero—is known, and the task is to predict behavior as time progresses. Derivatives only appear in a single time variable.  In this chapter, we entertain the possibility of coupling relationships between diﬀerent derivatives of a function. It is not diﬃcult to ﬁnd examples where this coupling is necessary. When simulating gases or ﬂuids, quantities like “pressure gradients,” which encode the derivatives of pressure in space, ﬁgure into how material moves over time. These gradients appear since gases and ﬂuids naturally move from high-pressure regions to low-pressure regions. In image processing, coupling the horizontal and vertical partial derivatives of an image can be used to describe its edges, characterize its texture, and so on.  Equations coupling together derivatives of functions in more than one variable are known as partial diﬀerential equations. They are the subject of a rich, nuanced theory worthy of  329   330  cid:4  Numerical Algorithms  Figure 16.1 Vector calculus notation. On the left, we show a function f   cid:126 x  for  cid:126 x ∈ R2 colored from black to white, its gradient ∇f , and its Laplacian ∇2f ; on the right are vector ﬁelds  cid:126 v  cid:126 x  with diﬀerent balances between divergence and curl.  larger-scale treatment, so we simply will summarize key ideas and provide suﬃcient material to approach problems commonly appearing in practice.  16.1 MOTIVATION  Partial diﬀerential equations  PDEs  provide one or more relationships between the partial derivatives of a function f : Rn → Rm; the goal is to ﬁnd an f satisfying the criteria. PDEs appear in nearly any branch of applied mathematics, and we list just a few below. Unlike in previous chapters, the algorithms in this chapter will be far from optimal with respect to accuracy or speed when applied to many of the examples. Our goals are to explore the vast space of problems that can be expressed as PDEs, to introduce the language needed to determine necessary numerical machinery, and to highlight key challenges and techniques for diﬀerent classes of PDEs.  There are a few combinations of partial derivatives that appear often in the world of PDEs. If f : R3 → R is a function and  cid:126 v : R3 → R3 is a vector ﬁeld, then the following operators from vector calculus, illustrated in Figure 16.1, are worth remembering:  Name  Notation Deﬁnition ∇f Gradient Divergence ∇ ·  cid:126 v ∇ ×  cid:126 v Curl Laplacian ∇2f   cid:16  ∂f , ∂f ∂x2 + ∂v2  cid:16  ∂v3 ∂x2 ∂x2 − ∂v2 ∂x3 + ∂2f ∂x2 2  ∂x3 cid:17  , ∂f + ∂v3 ∂x3 , ∂v1 ∂x3 − ∂v3 + ∂2f ∂x2 3  ∂2f ∂x2 1  ∂v1 ∂x1  ∂x1  ∂x1  ∂x2 cid:17  ∂x1 − ∂v1 , ∂v2  For PDEs involving ﬂuids, electrodynamics, and other physical quantities, by convention we think of the derivatives above as acting on the spatial variables  x, y, z  rather than the time variable t. For instance, the gradient of a function f :  x, y, z; t  → R will be written ∇f ≡  ∂f ∂x, ∂f ∂y, ∂f ∂z ; the partial derivative in time ∂f ∂t is treated separately. Example 16.1  Fluid simulation . The ﬂow of ﬂuids and smoke is governed by the Navier- Stokes equations, a system of PDEs in many variables. Suppose a ﬂuid is moving in a region Ω ⊆ R3. We deﬁne the following quantities: Time  t ∈ [0,∞   cid:126 v t  : Ω → R3 Velocity p t  : Ω → R Pressure  cid:126 f  t  : Ω → R3 External forces  e.g., gravity   f   cid:30 x   ∇f   cid:30 x   ∆f   cid:30 x   ∇ ·  cid:30 v large ∇ ×  cid:30 v small  ∇ ·  cid:30 v small ∇ ×  cid:30 v large   Partial Diﬀerential Equations  cid:4  331  Figure 16.2 Laplace’s equation takes a function on the boundary ∂Ω of a domain Ω ⊆ R2  left  and interpolates it to the interior of Ω as smoothly as possible  right .  If the ﬂuid has ﬁxed viscosity µ and density ρ, then the  incompressible  Navier-Stokes equations state  ρ · cid:18  ∂ cid:126 v  ∂t  +  cid:126 v · ∇ cid:126 v cid:19  = −∇p + µ∇2 cid:126 v +  cid:126 f  with  ∇ ·  cid:126 v = 0.  This system of equations determines the time dynamics of ﬂuid motion and can be con- structed by applying Newton’s second law to tracking “particles” of ﬂuid. Its statement involves derivatives in time ∂ ∂t and derivatives in space ∇, making it a PDE. Example 16.2  Maxwell’s equations . Maxwell’s equations determine the interaction between electric ﬁelds  cid:126 E and magnetic ﬁelds  cid:126 B over time. As with the Navier-Stokes equa- tions, we think of the gradient, divergence, and curl operators as taking partial derivatives in space  x, y, z  and not time t. In a vacuum, Maxwell’s system  in “strong” form  can be written:  ρ Gauss’s law for electric ﬁelds: ∇ ·  cid:126 E = ε0 Gauss’s law for magnetism: ∇ ·  cid:126 B = 0 Faraday’s law: ∇ ×  cid:126 E = − Amp`ere’s law: ∇ ×  cid:126 B = µ0 cid:32   cid:126 J + ε0  ∂  cid:126 B ∂t  ∂  cid:126 E  ∂t cid:33   Here, ε0 and µ0 are physical constants and  cid:126 J encodes the density of electrical current. Just like the Navier-Stokes equations, Maxwell’s equations relate derivatives of physical quantities in time t to their derivatives in space  given by curl and divergence terms .  Example 16.3  Laplace’s equation . Suppose Ω is a domain in R2 with boundary ∂Ω and that we are given a function g : ∂Ω → R, illustrated in Figure 16.2. We may wish to interpolate g to the interior of Ω as smoothly as possible. When Ω is an irregular shape, however, our strategies for interpolation from Chapter 13 can break down.  Boundary conditions  on ∂Ω   Laplace solution  on Ω    332  cid:4  Numerical Algorithms  Take f   cid:126 x  : Ω → R to be an interpolating function satisfying f   cid:126 x  = g  cid:126 x  for all  cid:126 x ∈ ∂Ω. One metric for evaluating the quality of f as a smooth interpolant is to deﬁne an energy functional:  E[f ] = cid:90 Ω  cid:107 ∇f   cid:126 x  cid:107 2  2 d cid:126 x.  Here, the notation E[·] does not stand for “expectation” as it might in probability theory, but rather is an “energy” functional; it is standard notation in variational analysis. E[f ] measures the “total derivative” of f measured by taking the norm of its gradient and integrating this quantity over all of Ω. Wildly ﬂuctuating functions f will have high values of E[f ] since the slope ∇f will be large in many places; smooth functions f , on the other hand, will have small E[f ] since their slope will be small everywhere. We could ask that f interpolates g while being as smooth as possible in the interior of  Ω using the following optimization:  minimizef E[f ] subject to f   cid:126 x  = g  cid:126 x ∀x ∈ ∂Ω.  This setup looks like optimizations we have solved elsewhere, but now our unknown is a function f rather than a point in Rn.  If f minimizes E subject to the boundary conditions, then E[f + h] ≥ E[f ] for all functions h  cid:126 x  with h  cid:126 x  = 0 for all  cid:126 x ∈ ∂Ω. This statement is true even for small per- turbations E[f + εh] as ε → 0. Subtracting E[f ], dividing by ε, and taking the limit as ε → 0, we must have d dε E[f + εh]ε=0 = 0; this expression is akin to setting directional derivatives of a function equal to zero to ﬁnd its minima. We can simplify:    cid:107 ∇f   cid:126 x  cid:107 2 Diﬀerentiating with respect to ε shows  2 + 2ε∇f   cid:126 x  · ∇h  cid:126 x  + ε2 cid:107 ∇h  cid:126 x  cid:107 2  2  d cid:126 x.  E[f + εh] = cid:90 Ω  cid:107 ∇f   cid:126 x  + ε∇h  cid:126 x  cid:107 2  2 d cid:126 x  = cid:90 Ω E[f + εh] = cid:90 Ω E[f + εh]ε=0 = 2 cid:90 Ω  d dε  =⇒  d dε   2∇f   cid:126 x  · ∇h  cid:126 x  + 2ε cid:107 ∇h  cid:126 x  cid:107 2 [∇f   cid:126 x  · ∇h  cid:126 x ] d cid:126 x.  2  d cid:126 x  Applying integration by parts and recalling that h is zero on ∂Ω,  d dε  E[f + εh]ε=0 = −2 cid:90 Ω  h  cid:126 x ∇2f   cid:126 x  d cid:126 x.  This expression must equal zero for all perturbations h that are zero on ∂Ω. Hence, ∇2f   cid:126 x  = 0 for all  cid:126 x ∈ Ω\∂Ω  a formal proof is outside of the scope of our discussion . following PDE:  We have shown that the boundary interpolation problem above amounts to solving the  ∇2f   cid:126 x  = 0 ∀ cid:126 x ∈ Ω\∂Ω f   cid:126 x  = g  cid:126 x  ∀ cid:126 x ∈ ∂Ω.  This PDE is known as Laplace’s equation.   Partial Diﬀerential Equations  cid:4  333  Figure 16.3 A CT scanner passes x-rays through an object; sensors on the other side collect the energy that made it through, giving the integrated density of the object along the x-ray path. Placing the source and sensor in diﬀerent rotated poses allows for reconstruction of the pointwise density function.  Example 16.4  X-ray computerized tomography . Computerized tomography  CT  tech- nology uses x-rays to see inside an object without cutting through it. The basic model is shown in Figure 16.3. Essentially, by passing x-rays through an object, the density of the object integrated along the x-ray path can be sensed by collecting the proportion that makes it through to the other side.  Suppose the density of an object is given by a function ρ : R3 → R+. For any two points  cid:126 x,  cid:126 y ∈ R3, we can think of a CT scanner abstractly as a device that can sense the integral u of ρ along the line connecting  cid:126 x and  cid:126 y:  u  cid:126 x,  cid:126 y  ≡ cid:90  ∞  −∞  ρ t cid:126 x +  1 − t  cid:126 y  dt.  The function u : R3 × R3 → R+ is known as the Radon transform of ρ.  Suppose we take a second derivative of u in an  cid:126 x and then a  cid:126 y coordinate:  ∂ ∂xi  −∞  u  cid:126 x,  cid:126 y  = cid:90  ∞ = cid:90  ∞ u  cid:126 x,  cid:126 y  = cid:90  ∞ = cid:90  ∞  −∞  −∞  −∞  =⇒  ∂2  ∂yj∂xi  ∂ ∂xi  ρ t cid:126 x +  1 − t  cid:126 y  dt by deﬁnition of u  t cid:126 ei · ∇ρ t cid:126 x +  1 − t  cid:126 y  dt ∂ ∂yj t 1 − t  cid:126 e cid:62 i Hρ t cid:126 x +  1 − t  cid:126 y  cid:126 ej dt for Hessian Hρ of ρ.  t cid:126 ei · ∇ρ t cid:126 x +  1 − t  cid:126 y  dt  An identical set of steps shows that the derivative applying symmetry of Hρ. That is, u satisﬁes the following relationship:  ∂xj ∂yi  equals the same expression after  ∂2u  ∂2u  ∂2u  .  =  ∂yj∂xi  ∂xj∂yi  This equality, known as the Fritz John equation [68], gives information about u without involving the unknown density function ρ. In a computational context, it can be used to ﬁll in data missing from incomplete x-ray scans or to smooth data from a potentially noisy x-ray sensor before reconstructing ρ.  X  -r  a  y  s  o  u  r  c  e  X  -r a  y  s  e  n  s  o  r   334  cid:4  Numerical Algorithms  Figure 16.4 Shortest-path distances constrained to move within the interior of a non- convex shape have to wrap around corners; level sets of the distance function  shown as black lines  are no longer circles beyond these corner points.  Example 16.5  Eikonal equation . Suppose Ω is a closed region in Rn. For a ﬁxed point  cid:126 x0 ∈ Ω, we might wish to ﬁnd a function d  cid:126 x  : Ω → R+ measuring the length of the shortest path from  cid:126 x0 to  cid:126 x restricted to move only within Ω. When Ω is convex, we can write d in closed form as  d  cid:126 x  =  cid:107  cid:126 x −  cid:126 x0 cid:107 2.  As illustrated in Figure 16.4, however, if Ω is non-convex or is a complicated domain like a surface, these distance functions become more challenging to compute. Solving for d, however, is a critical step for tasks like planning paths of robots by minimizing the distance they travel while avoiding obstacles marked on a map.  If Ω is non-convex, away from singularities, the function d  cid:126 x  still satisﬁes a derivative  condition known as the eikonal equation:   cid:107 ∇d cid:107 2 = 1.  Intuitively, this PDE states that a distance function should have unit rate of change every- where. As a sanity check, this relationship is certainly true for the absolute value function x− x0 in one dimension, which measures the distance along the real line between x0 and x. This equation is nonlinear in the derivative ∇d, making it a particularly challenging problem to solve for d  cid:126 x . Specialized algorithms known as fast marching methods and fast sweeping methods estimate d  cid:126 x  over all of Ω by integrating the eikonal equation. Many algorithms for ap- proximating solutions to the eikonal equation have structure similar to Dijkstra’s algorithm for computing shortest paths along graphs; see Exercise 16.8 for one example.  Example 16.6  Harmonic analysis . Diﬀerent objects respond diﬀerently to vibrations, and in large part these responses are functions of the geometry of the objects. For example, cellos and pianos can play the same note, but even an inexperienced listener can distinguish between the sounds they make.  From a mathematical standpoint, we can take Ω ⊆ R3 to be a shape represented either as a surface or a volume. If we clamp the edges of the shape, then its frequency spectrum is given by eigenvalues coming from the following problem:  ∇2φ = λφ φ  cid:126 x  = 0 ∀ cid:126 x ∈ ∂Ω,   Partial Diﬀerential Equations  cid:4  335  Figure 16.5 The ﬁrst eight eigenfunctions φi of the Laplacian operator of the domain Ω from Figure 16.2, which satisfy ∇2φi = λiφi in order of increasing frequency; we omit φ1, which is the constant function with λ = 0.  where ∇2 is the Laplacian of Ω and ∂Ω is the boundary of Ω. Figure 16.5 shows examples of these functions on a two-dimensional domain Ω. Relating to the one-dimensional theory of waves, sin kx solves this problem when Ω is the interval [0, 2π] and k ∈ Z. To check, the Laplacian in one dimension is ∂2 ∂x2, and thus  ∂2 ∂x2 sin kx =  ∂ ∂x  k cos kx = −k2 sin kx  sin k · 0  = 0 sin k · 2π  = 0.  That is, the eigenfunctions are sin kx with eigenvalues −k2. 16.2 STATEMENT AND STRUCTURE OF PDES  Vocabulary used to describe PDEs is extensive, and each class of PDEs has substantially diﬀerent properties from the others in terms of solvability, theoretical understanding of solutions, and discretization challenges. Our main focus eventually will be on developing algorithms for a few common tasks rather than introducing the general theory of continu- ous or discretized PDE, but it is worth acknowledging the rich expressive possibilities—and accompanying theoretical challenges—that come with using PDE language to describe nu- merical problems.  Following standard notation, in our subsequent development we will assume that our unknown is some function u  cid:126 x . For ease of notation, we will use subscript notation to denote partial derivatives:  ux ≡  ∂u ∂x  ,  uy ≡  ∂u ∂y  ,  uxy ≡  ∂2u ∂x∂y  ,  and so on.  16.2.1 Properties of PDEs  Just as ODEs couple the time derivatives of a function, PDEs typically are stated as rela- tionships between two or more partial derivatives of u. By examining the algebraic form of a PDE, we can check if it has any of a number of properties, including the following:    Homogeneous  e.g., x2uxx + uxy − uy + u = 0 : The PDE can be written using linear combinations of u and its derivatives; the coeﬃcients can be scalar values or func-  φ2  φ3  φ4  φ5  φ6  φ7  φ8  φ9   336  cid:4  Numerical Algorithms  Figure 16.6 Dirichlet boundary conditions prescribe the values of the unknown func- tion u on the boundary ∂Ω of the domain Ω, while Neumann conditions prescribe the derivative of u orthogonal to ∂Ω.  tions of the independent variables. The equation can be nonlinear in the independent variables  x and y in our example .    Linear  e.g., uxx − yuyy + u = xy2 : Similar to homogeneous PDE, but potentially with a nonzero  inhomogeneous  right-hand side built from scalars or the dependent variables. PDEs like the eikonal equation  or u2 xx = uxy  are considered nonlinear because they are nonlinear in u.    Quasi-linear  e.g., uxy + 2uxx + u2  order derivatives of u.  y + u2  x = y : The statement is linear in the highest-    Constant-coeﬃcient  e.g., uxx + 3uy = uz : The coeﬃcients of u and its derivatives  are not functions of the independent variables.  One potentially surprising observation about the properties above is that they are more concerned with the role of u than those of the independent variables like x, y, and z. For instance, the deﬁnition of a “linear” PDE allows u to have coeﬃcients that are nonlinear functions of these variables. While this may make the PDE appear nonlinear, it is still linear in the unknowns, which is the distinguishing factor.  The order of a PDE is the order of its highest derivative. Most of the PDEs we consider in this chapter are second-order and already present considerable numerical challenges. Methods analogous to reduction of ODEs to ﬁrst order  §15.2.1  can be carried out but do not provide as much beneﬁt for solving PDEs.  16.2.2 Boundary Conditions  ODEs typically are considered initial-value problems, because given a conﬁguration that is known at the initial time t = 0, they evolve the state forward indeﬁnitely. With few exceptions, the user does not have to provide information about the state for t > 0.  PDE problems also can be boundary-value problems rather than or in addition to being initial value problems. Most PDEs require information about behavior at the boundary of the domain of all the variables. For instance, Laplace’s equation, introduced in Exam- ple 16.3, requires ﬁxed values on the boundary ∂Ω of Ω. Similarly, the heat equation used to  ∂Ω  Ω  R  telhciriD  ∂Ω  Ω  nnamueN   Partial Diﬀerential Equations  cid:4  337  Figure 16.7 Boundary conditions for the PDE utt = 0 from Example 16.7.  simulate conductive material like metals admits a number of possible boundary conditions, corresponding to whether the material is attached to a heat source or dispersing heat energy into the surrounding space.  If the unknown of a PDE is a function u : Ω → R for some domain Ω ⊆ Rn, typical  boundary conditions include the following:    Dirichlet conditions directly specify the values of u  cid:126 x  for all  cid:126 x ∈ ∂Ω.   Neumann conditions specify the derivative of u  cid:126 x  in the direction orthogonal to ∂Ω.   Mixed or Robin conditions specify a relationship between the value and normal deriva-  tives of u  cid:126 x  on ∂Ω.  The ﬁrst two choices are illustrated in Figure 16.6.  Improperly encoding boundary conditions is a subtle oversight that creeps into count- less discretizations of PDEs. There are many sources of confusion that explain this common issue. Diﬀerent discretizations of the same boundary conditions can yield qualitatively dif- ferent outputs from a PDE solver if they are expressed improperly. Indeed, some boundary conditions are not realizable even in theory, as illustrated in the example below.  Example 16.7  Boundary conditions in one dimension . Suppose we are solving the following PDE  more precisely an ODE, although the distinction here is not relevant  in one variable t over the interval Ω = [a, b]:  utt = 0.  From one-variable calculus, that solutions must take the form u t  = αt + β.  Consider the eﬀects of assorted choices of boundary conditions on ∂Ω = {a, b}, illus-  trated in Figure 16.7:    Dirichlet conditions specify the values u a  and u b  directly. There is a unique line that goes through any pair of points  a, u a   and  b, u b  , so a solution to the PDE always exists and is unique in this case.    Neumann conditions specify u cid:48  a  and u cid:48  b . From the general form of u t , u cid:48  t  = α, reﬂecting the fact that lines have constant slope. Neumann conditions specifying diﬀerent values for u cid:48  a  and u cid:48  b  are incompatible with the PDE itself. Compatible Neumann conditions, on the other hand, specify u cid:48  a  = u cid:48  b  = α but are satisﬁed for any choice of β.  u t   u t   u cid:31  a   u cid:31  b   u t   u cid:31  a   u cid:31  b    b, u b     a, u a    a  t  b  a  t  b  a  t  b  Dirichlet  Neumann  compatible   Neumann  incompatible    338  cid:4  Numerical Algorithms 16.3 MODEL EQUATIONS In §15.2.3, we studied properties of ODEs and their integrators by examining the model equation y cid:48  = ay. We can pursue a similar analytical technique for PDEs, although we will have to separate into multiple special cases to cover the qualitative phenomena of interest. We will focus on the linear, constant-coeﬃcient, homogeneous case. As mentioned in §16.2.1, the non-constant coeﬃcient and inhomogeneous cases often have similar qualitative behavior, and nonlinear PDEs require special consideration beyond the scope of our discus- sion. We furthermore will study second-order systems, that is, systems containing at most the second derivative of u. While the model ODE y cid:48  = ay is ﬁrst-order, a reasonable model PDE needs at least two derivatives to show how derivatives in diﬀerent directions interact. Linear, constant-coeﬃcient, homogeneous second-order PDEs have the following general  form, for unknown function u : Rn → R: ∂u   cid:88 ij  aij  ∂xi∂xj  + cid:88 i  bi  ∂u ∂xi  + cu = 0.  To simplify notation, we can deﬁne a formal “gradient operator” as the vector of derivatives  ∇ ≡ cid:18  ∂  ∂x1  ,  ∂ ∂x2  , . . . ,  ∂  ∂xn cid:19  .   ∇ cid:62 A∇ + ∇ ·  cid:126 b + c u = 0.  Expressions like ∇f , ∇ ·  cid:126 v, and ∇ ×  cid:126 v agree with the deﬁnitions of gradients, divergence, and curl on R3 using this formal deﬁnition of ∇. In this notation, the model PDE takes a matrix-like form:  The operator ∇ cid:62 A∇ + ∇ ·  cid:126 b + c acting on u abstractly looks like a quadratic form in ∇ as a vector; since partial derivatives commute, we can assume A is symmetric. The deﬁniteness of A determines the class of the model PDE, just as the deﬁniteness of a matrix determines the convexity of its associated quadratic form. Four cases bring about qualitatively diﬀerent behavior for u:    If A is positive or negative deﬁnite, the system is elliptic.   If A is positive or negative semideﬁnite, the system is parabolic.   If A has only one eigenvalue of diﬀerent sign from the rest, the system is hyperbolic.   If A satisﬁes none of these criteria, the system is ultrahyperbolic.  These criteria are listed approximately in order of the diﬃculty level of solving each type of equation. We consider the ﬁrst three cases below and provide examples of corresponding behavior by specifying diﬀerent matrices A; ultrahyperbolic equations do not appear as often in practice and require highly specialized solution techniques.  16.3.1 Elliptic PDEs  Positive deﬁnite linear systems can be solved using eﬃcient algorithms like Cholesky de- composition and conjugate gradients that do not necessarily work for indeﬁnite matrices. Similarly, elliptic PDEs, for which A is positive deﬁnite, have strong structure that makes them the most straightforward equations to characterize and solve, both theoretically and computationally.   Partial Diﬀerential Equations  cid:4  339  Figure 16.8 The heat equation in one variable ut = αuxx decreases u over time where it is curved down and increases u over time where u is curved up, as measured using the second derivative in space uxx. Here, we show a solution of the heat equation u x, t  at a ﬁxed time t0; the arrows indicate how values of u will change as t advances.  The model elliptic PDE is the Laplace equation, given by ∇2u = 0, as in Example 16.3.  In two variables, the Laplace equation is written  uxx + uyy = 0.  Figure 16.2 illustrated a solution of the Laplace equation, which essentially interpolates information from the boundary of the domain of u to its interior.  Elliptic equations are well-understood theoretically and come with strong properties characterizing their behavior. Of particular importance is elliptic regularity, which states that solutions of elliptic PDEs automatically are diﬀerentiable to higher order than their building blocks. Physically, elliptic equations characterize stable equilbria like the rest pose of a stretched rubber sheet, which naturally resists kinks and other irregularities.  16.3.2 Parabolic PDEs  Positive semi deﬁnite linear systems are only marginally more diﬃcult to deal with than positive deﬁnite ones, at least if their null spaces are known and relatively small. Positive semideﬁnite matrices have null spaces that prevent them from being invertible, but orthog- onally to the null space they behave identically to deﬁnite matrices. In PDE, these systems correspond to parabolic equations, for which A is positive semideﬁnite.  The heat equation is the model parabolic PDE. Suppose u0 x, y  is a ﬁxed distribution of temperature in some region Ω ⊆ R2 at time t = 0. Then, the heat equation determines how heat diﬀuses over time t > 0 as a function u t; x, y :  ut = α uxx + uyy ,  where α > 0. If ∇ =  ∂ ∂x, ∂ ∂y , the heat equation can be written ut = α∇2u. There is no second derivative in time t, making the equation parabolic rather than elliptic. Figure 16.8 provides a phenomenological interpretation of the heat equation in one vari- able ut = αuxx. The second derivative ∇2u measures the convexity of u. The heat equation increases u with time when its value is “cupped” upward, and decreases u otherwise. This  u x, t0   uxx < 0  x  uxx > 0   340  cid:4  Numerical Algorithms  Figure 16.9 Solution to the heat equation ut = uxx + uyy on the unit circle with Dirichlet  top  and Neumann  bottom  boundary conditions. Solutions are colored from −1  black  to 1  white .  negative feedback is stable and leads to equilibrium as t → ∞. Example solutions to the heat equation with diﬀerent boundary conditions are shown in Figure 16.9. The corresponding second-order term matrix A for the heat equation is:  A =   t x y  t x y 0 0 0 0 0  0 1 0  1.  The heat equation is parabolic since this matrix has eigenvalues 0, 1, and 1.  There are two boundary conditions needed for the heat equation, both of which have  physical interpretations:    The distribution of heat u 0; x, y  ≡ u0 x, y  at time t = 0 at all points  x, y  ∈ Ω.   Behavior of u when t > 0 at boundary points  x, y  ∈ ∂Ω. Dirichlet conditions ﬁx u t; x, y  for all t ≥ 0 and  x, y  ∈ ∂Ω, e.g., if Ω is a piece of foil sitting next to a heat source like an oven whose temperature is controlled externally. Neumann conditions specify the derivative of f in the direction normal to the boundary ∂Ω; they correspond to ﬁxing the ﬂux of heat out of Ω caused by diﬀerent types of insulation.  16.3.3 Hyperbolic PDEs  The ﬁnal model equation is the wave equation, corresponding to the indeﬁnite matrix case:  utt = c2 uxx + uyy .  The wave equation is hyperbolic because the second derivative in time t has opposite sign from the two spatial derivatives when all terms involving u are isolated on the same side. This equation determines the motion of waves across an elastic medium like a rubber sheet. It can be derived by applying Newton’s second law to points on a piece of elastic, where x and y are positions on the sheet and u t; x, y  is the height of the piece of elastic at time t. Figure 16.10 illustrates a solution of the wave equation with Dirichlet boundary con- ditions; these boundary conditions correspond to the vibrations of a drum whose outer boundary is ﬁxed. As illustrated in the example, wave behavior contrasts considerably with  t = 0  t = 2.5 · 10−4 t = 5 · 10−4  t = 0.001  t = 0.002  t = 0.004  t = 0.008  t = 0.016   Partial Diﬀerential Equations  cid:4  341  Figure16.10 The wave equation on a square with Dirichlet boundary conditions; time is sampled evenly and progresses left to right. Color is proportional to the height of the wave, from −1  black  to 1  white .  heat diﬀusion in that as t → ∞ the energy of the system does not disperse; waves can bounce back and forth across a domain indeﬁnitely. For this reason, implicit integration strategies may not be appropriate for integrating hyperbolic PDEs because they tend to damp out motion.  Boundary conditions for the wave equation are similar to those of the heat equation,  but now we must specify both u 0; x, y  and ut 0; x, y  at time zero:    The conditions at t = 0 specify the position and velocity of the wave at the start time.   Boundary conditions on ∂Ω determine what happens at the ends of the material. Dirichlet conditions correspond to ﬁxing the sides of the wave, e.g., plucking a cello string that is held ﬂat at its two ends on the instrument. Neumann conditions corre- spond to leaving the ends of the wave untouched, like the end of a whip.  16.4 REPRESENTING DERIVATIVE OPERATORS  A key intuition that underlies many numerical techniques for PDEs is the following:  Derivatives act on functions in the same way that sparse matrices  act on vectors.  Our choice of notation reﬂects this parallel: The derivative d dx[f  x ] looks like the product of an operator d dx and a function f .  Formally, diﬀerentiation is a linear operator like matrix multiplication, since for all  smooth functions f, g : R → R and scalars a, b ∈ R, d dx   af  x  + bg x   = a  d dx  f  x  + b  g x .  d dx  The derivatives act on functions, which can be thought of as points in an inﬁnite-dimensional vector space. Many arguments from Chapter 1 and elsewhere regarding the linear algebra of matrices extend to this case, providing conditions for invertibility, symmetry, and so on of these abstract operators.  Nearly all techniques for solving linear PDEs make this analogy concrete. For example, recall the model equation  ∇ cid:62 A∇ +∇· cid:126 b + c u = 0 subject to Dirichlet boundary conditions u∂Ω = u0 for some ﬁxed function u0. We can deﬁne an operator R∂Ω : C∞ Ω  → C∞ ∂Ω , that is, an operator taking functions on Ω and returning functions on its boundary ∂Ω,  −−−−−−−−−−−−−−−→  +t   342  cid:4  Numerical Algorithms  Figure 16.11 The one-dimensional ﬁnite diﬀerence Laplacian operator L takes sam- ples ui of a function u x  and returns an approximation of u cid:48  cid:48  at the same grid points by combining neighboring values using weights  1 — −2 — 1 ; here u x  is approximated using nine samples u0, . . . , u8. Boundary conditions are needed to deal with the unrepresented quantities at the white endpoints.  by restriction: [R∂Ωu]  cid:126 x  ≡ u  cid:126 x  for all  cid:126 x ∈ ∂Ω. Then, the model PDE and its boundary conditions can be combined in matrix-like notation:  In this sense, we wish to solve M u = w where M is a linear operator. If we discretize M as a matrix, then recovering the solution u of the original equation is as easy as writing   cid:18   ∇ cid:62 A∇ + ∇ ·  cid:126 b + c   R∂Ω   cid:19  u = cid:18  0  u0  cid:19  .  “u = M−1w.”  Many discretizations exist for M and u, often derived from the discretizations of deriva- tives introduced in §14.3. While each has subtle advantages, disadvantages, and conditions for eﬀectiveness or convergence, in this section we provide constructions and high-level themes from a few popular techniques. Realistically, a legitimate and often-applied tech- nique for ﬁnding the best discretization for a given application is to try a few and check empirically which is the most eﬀective.  16.4.1 Finite Differences  Consider a function u x  on [0, 1]. Using the methods from Chapter 14, we can approximate the second derivative u cid:48  cid:48  x  as  u cid:48  cid:48  x  =  u x + h  − 2u x  + u x − h   + O h2 .  h2  In the course of solving a PDE in u, assume u x  is discretized using n + 1 evenly spaced samples u0, u1, . . . , un, as in Figure 16.11, and take h to be the spacing between samples, satisfying h = 1 n. Applying the formula above provides an approximation of u cid:48  cid:48  at each grid point:  u cid:48  cid:48 k ≈  uk+1 − 2uk + uk−1  .  h2  That is, the second derivative of a function on a grid of points can be estimated using the  1 — −2 — 1  stencil illustrated in Figure 16.12. Boundary conditions are needed to compute u cid:48  cid:48 0 and u cid:48  cid:48 n since we have not included u−1 or un+1 in our discretization. Keeping in mind that u0 = u 0  and un = u 1 , we can incorporate them as follows:  u−1  u0  u1  u2  u3  u4  u5  u6  u7  u8  u9  h  1  1 −2  u cid:2  cid:2 0  u cid:2  cid:2 1  u cid:2  cid:2 2  u cid:2  cid:2 3  u cid:2  cid:2 4  u cid:2  cid:2 5  u cid:2  cid:2 6  u cid:2  cid:2 7  u cid:2  cid:2 8   Partial Diﬀerential Equations  cid:4  343  Figure 16.12 The one-dimensional ﬁnite diﬀerence Laplacian can be thought of as dragging a  1 — −2 — 1  stencil across the domain.    Dirichlet: u−1 ≡ un+1 = 0, that is, ﬁx the value of u beyond the endpoints to be zero.   Neumann: u−1 = u0 and un+1 = un, encoding the condition u cid:48  0  = u cid:48  1  = 0.   Periodic: u−1 = un and un+1 = u0, making the identiﬁcation u 0  = u 1 . Suppose we stack the samples uk into a vector  cid:126 u ∈ Rn+1 and the samples u cid:48  cid:48 k into a second vector  cid:126 w ∈ Rn+1. The construction above shows that h2  cid:126 w = L cid:126 u, where L is one of the choices below:  Dirichlet 1 −2 . . .  1 . . . . . . 1 −2 1  1 −2      −1 1  Neumann 1 −2 . . .  1 . . . . . . 1 −2 1      1 −1  Periodic  1 −2 . . .  −2 1  1  1 . . . . . . 1 −2 1    1  1 −2    −2 1  The matrix L can be thought of as a discretized version of the operator d2  cid:126 u ∈ Rn+1 rather than functions u : [0, 1] → R. In two dimensions, we can use a similar approximation of the Laplacian ∇2u of u : [0, 1] × [0, 1] → R. Now, we sample using a grid of values shown in Figure 16.13. In this case, ∇2u = uxx + uyy, so we sum up x and y second derivatives constructed in the one- dimensional example above. If we number our samples as uk, cid:96  ≡ u kh,  cid:96 h , then the formula for the Laplacian of u becomes  dx2 acting on   ∇2u k, cid:96  ≈  u k−1 , cid:96  + uk,  cid:96 −1  + u k+1 , cid:96  + uk,  cid:96 +1  − 4uk, cid:96   .  h2  This approximation implies a  1 — −4 — 1  stencil over a 3 × 3 box. If we once again combine our samples of u and ∇u into  cid:126 u and  cid:126 w, respectively, then h2  cid:126 w = L2 cid:126 y where L2 comes from the stencil we derived. This two-dimensional grid Laplacian L2 appears in many image processing applications, where  k,  cid:96   is used to index pixels on an image.  Regardless of dimension, given a discretization of the domain and a Laplacian matrix L, we can approximate solutions of elliptic PDEs using linear systems of equations. Consider the Poisson equation ∇2u = w. After discretization, given a sampling  cid:126 w of w  cid:126 x , we can obtain an approximation  cid:126 u of the solution by solving L cid:126 u = h2  cid:126 w.   344  cid:4  Numerical Algorithms  Figure 16.13 For functions u x, y  discretized on a two-dimensional grid  left , the Laplacian L2 has a  1 — −4 — 1  stencil.  This approach can be extended to inhomogeneous boundary conditions. For example, if we wish to solve ∇2u = w on a two-dimensional grid subject to Dirichlet conditions prescribed by a function u0, we can solve the following linear system of equations for  cid:126 u:  u k−1 , cid:96  + uk,  cid:96 −1  + u k+1 , cid:96  + uk,  cid:96 +1  − 4uk, cid:96  = 0 otherwise.  uk, cid:96  = u0 kh, lh  when k ∈ {0, n} or  cid:96  ∈ {0, n}  This system of equations uses the 3× 3 Laplacian stencil for vertices in the interior of [0, 1]2 while explicitly ﬁxing the values of u on the boundary. These discretizations exemplify the ﬁnite diﬀerences method of discretizing PDEs, usu- ally applied when the domain can be approximated using a grid. The ﬁnite diﬀerence method essentially treats the divided diﬀerence approximations from Chapter 14 as linear operators on grids of function values and then solves the resulting discrete system of equations.  Quoting results from Chapter 14 directly, however, comprises a serious breach of nota- tion. When we write that an approximation of u cid:48  x  or u cid:48  cid:48  x  holds to O hk , we implicitly assume that u x  is suﬃciently diﬀerentiable. Hence, what we need to show is that the result of solving systems like L cid:126 u = h2  cid:126 w produces a  cid:126 u that actually approximates samples from a smooth function u x  rather than oscillating crazily. The following example shows that this issue is practical rather than theoretical, and that reasonable but non-convergent discretizations can fail catastrophically.  Example 16.8  Lack of convergence . Suppose we again sample a function u x  of one variable and wish to solve an equation that involves a ﬁrst-order u cid:48  term. Interestingly, this task can be more challenging than solving second-order equations.  First, if we deﬁne u cid:48 k as the forward diﬀerence 1  h  uk+1 − uk , then we will be in the unnaturally asymmetric position of needing a boundary condition at un but not at u0 as shown in Figure 16.14. Backward diﬀerences suﬀer from the reverse problem.  We might attempt to solve this problem and simultaneously gain an order of accuracy 2h  uk+1 − uk−1 , but this discretization suﬀers  by using the symmetric diﬀerence u cid:48 k ≈ 1   Partial Diﬀerential Equations  cid:4  345  Figure 16.14 Forward diﬀerencing to approximate u cid:48  x  asymmetrically requires boundary conditions on the right but not the left.  Figure 16.15 Centered diﬀerencing yields a symmetric approximation of u cid:48  x , but u cid:48 k is not aﬀected by the value of uk using this formula.  Figure 16.16 Solving u cid:48  x  = w x  for u x  using a centered diﬀerence discretization suﬀers from the fencepost problem; odd- and even-indexed values of u have com- pletely separate behavior. As more gridpoints are added in x, the resulting u x  does not converge to a smooth function, so O hk  estimates of derivative quality do not apply.  1  w x   u x   1  x   346  cid:4  Numerical Algorithms  from a more subtle fencepost problem illustrated in Figure 16.15. In particular, this version of u cid:48 k ignores the value of uk itself and only looks at its neighbors uk−1 and uk+1. This oversight means that uk and u cid:96  are treated diﬀerently depending on whether k and  cid:96  are even or odd. Figure 16.16 shows the result of attempting to solve a numerical problem with this discretization; the result is not diﬀerentiable.  As with the leapfrog integration algorithm in §15.4.2, one way to avoid these issues is to think of the derivatives as living on half gridpoints. In the one-dimensional case, this change corresponds to labeling the diﬀerence 1 h  yk+1 − yk  as y cid:48 k+1 2. This technique of placing diﬀerent derivatives on vertices, edges, and centers of grid cells is particularly common in ﬂuid simulation, which often maintains pressures, ﬂuid velocities, and other physical quantities at locations suggested by the discretization.  16.4.2 Collocation  A challenge when working with ﬁnite diﬀerences is that we must justify that the end result “looks like” the theoretical solution we are seeking to approximate. That is, we have replaced a continuous unknown u  cid:126 x  with a sampled proxy on a grid but may inadvertently lose the connection to continuous mathematics in the process; Example 16.8 showed one example where a discretization is not convergent and hence yields unusable output. To avoid these issues, many numerical PDE methods attempt to make the connection between continuous and discrete less subtle.  One way to link continuous and discrete models is to write u  cid:126 x  in a basis φ1, . . . , φk:  u  cid:126 x  ≈  aiφi  cid:126 x .  k cid:88 i=1  This strategy should be familiar, as it underlies machinery for interpolation, quadrature, and diﬀerentiation. The philosophy here is to ﬁnd coeﬃcients a1, . . . , ak providing the best possible approximation of the solution to the continuous problem in the φi basis. As we add more functions φi to the basis, in many cases the approximation will converge to the theoretical solution, so long as the φi’s eventually cover the relevant part of function space. Perhaps the simplest method making use of this new construction is the collocation method. In the presence of k basis functions, this method samples k points  cid:126 x1, . . . ,  cid:126 xk ∈ Ω and requires that the PDE holds exactly at these locations. For example, if we wish to solve the Poisson equation ∇2u = w, then for each i ∈ {1, . . . , k} we write  w  cid:126 xi  = ∇2u  cid:126 xi  =  aj∇2φj  cid:126 xi .  k cid:88 j=1  The only unknown quantities in this expression are the aj’s, so it can be used to write a square linear system for the vector  cid:126 a ∈ Rk of coeﬃcients. It can be replaced with a least-squares problem if more than k points are sampled in Ω. Collocation requires a choice of basis functions φ1, . . . , φk and a choice of collocation points  cid:126 x1, . . . ,  cid:126 xk. Typical basis functions include full or piecewise polynomial functions and trigonometric functions. When the φi’s are compactly supported, that is, when φi  cid:126 x  = 0 for most  cid:126 x ∈ Ω, the resulting system of equations is sparse. Collocation outputs a set of coeﬃcients rather than a set of function values as in ﬁnite diﬀerences. Since the basis functions do not have to have any sort of grid structure, it is well-suited to non-rectangular domains, which can provide some challenge for ﬁnite diﬀerencing.   Partial Diﬀerential Equations  cid:4  347  A drawback of collocation is that it does not regularize the behavior of the approximation u  cid:126 x  between the collocation points. Just as interpolating a polynomial through a set of sample points can lead to degenerate and in some cases highly oscillatory behavior between the samples, the collocation method must be used with caution to avoid degeneracies, for instance by optimizing the choice of basis functions and collocation points. Another option is to use a method like ﬁnite elements, considered below, which integrates behavior of an approximation over more than one sample point at a time.  16.4.3 Finite Elements  Finite element discretizations also make use of basis functions but do so by examining integrated quantities rather than pointwise values of the unknown function u  cid:126 x . This type of discretization is relevant to simulating a wide variety of phenomena and remains a popular choice in a diverse set of ﬁelds including mechanical engineering, digital geometry processing, and cloth simulation.  As an example, suppose that Ω ⊆ R2 is a region on the plane and that we wish to solve the Dirichlet equation ∇2u = 0 in its interior. Take any other function v  cid:126 x  satisfying v  cid:126 y  = 0 for all  cid:126 y ∈ ∂Ω. If we solve the PDE for u successfully, then the function u  cid:126 x  will satisfy the relationship   cid:90 Ω  v  cid:126 x ∇2u  cid:126 x  d cid:126 x = cid:90 Ω  v  cid:126 x  · 0 d cid:126 x = 0,  regardless of the choice of v  cid:126 x .  We can deﬁne a bilinear operator  cid:104 u, v cid:105 ∇2 as the integral v  cid:126 x ∇2u  cid:126 x  d cid:126 x.   cid:104 u, v cid:105 ∇2 ≡ cid:90 Ω  Any function u  cid:126 x  for which  cid:104 u, v cid:105 ∇2 = 0 for all reasonable v : Ω → R deﬁned above is called a weak solution to the Dirichlet equation. The functions v are known as test functions. A remarkable observation suggests that weak solutions to PDEs may exist even when a strong solution does not. When v  cid:126 x  vanishes on ∂Ω, the divergence theorem from multi- variable calculus implies the following alternative form for  cid:104 u, v cid:105 ∇2 :   cid:104 u, v cid:105 ∇2 = − cid:90 Ω ∇u  cid:126 x  · ∇v  cid:126 x  d cid:126 x.  We used a similar step in Example 16.3 to derive Laplace’s equation. Whereas the Laplacian ∇2 in the Dirichlet equation requires the second derivative of u, this expression only requires u to be once diﬀerentiable. In other words, we have expressed a second-order PDE in ﬁrst- order language. Furthermore, this form of  cid:104 ·,· cid:105 ∇2 is symmetric and negative semideﬁnite, in the sense that   cid:104 u, u cid:105 ∇2 = − cid:90 Ω  cid:107 ∇u  cid:126 x  cid:107 2  2 d cid:126 x ≤ 0.  Our deﬁnition of weak PDE solutions above is far from formal, since we were somewhat cavalier about the space of functions we should consider for u and v. Asking that  cid:104 u, v cid:105 ∇2 = 0 for all possible functions v  cid:126 x  is an unreasonable condition, since the space of all functions includes many degenerate functions that may not even be integrable. For the theoretical study of PDEs, it is usually suﬃcient to assume v is suﬃciently smooth and has small support. Even with this restriction, however, the space of functions is far too large to be discretized in any reasonable way.   348  cid:4  Numerical Algorithms  The ﬁnite elements method  FEM , however, makes the construction above tractable by restricting functions to a ﬁnite basis. Suppose we approximate u in a basis φ1  cid:126 x , . . . , φk  cid:126 x  i=1 aiφi  cid:126 x  for unknown coeﬃcients a1, . . . , ak. Since the actual solution  by writing u  cid:126 x  ≈ cid:80 k u  cid:126 x  of the PDE is unlikely to be expressible in this form, we cannot expect  cid:104  cid:80 i aiφi, v cid:105 ∇2 =  0 for all test functions v  cid:126 x . Hence, we not only approximate u  cid:126 x  but also restrict the class of test functions v  cid:126 x  to one in which we are more likely to be successful.  The best-known ﬁnite element approximation is the Galerkin method. In this method, we require that  cid:104 u, v cid:105 ∇2 = 0 for all test functions v that also can be written in the φi basis. By linearity of  cid:104 ·,· cid:105 ∇2 , this method amounts to requiring that  cid:104 u, φi cid:105 ∇2 = 0 for all i ∈ {1, . . . , k}. Expanding this relationship shows  ajφj, φi cid:43   ∇2   cid:104 u, φi cid:105 ∇2 = cid:42  cid:88 j = cid:88 j  by our approximation of u  aj cid:104 φi, φj cid:105 ∇2 by linearity and symmetry of  cid:104 ·,· cid:105 ∇2.  Using this ﬁnal expression, we can recover the vector  cid:126 a ∈ Rk of coeﬃcients by solving the following linear system of equations:     cid:104 φ1, φ1 cid:105 ∇2  cid:104 φ2, φ1 cid:105 ∇2   cid:104 φ1, φ2 cid:105 ∇2  cid:104 φ2, φ2 cid:105 ∇2  ...  ...   cid:104 φk, φ1 cid:105 ∇2   cid:104 φk, φ2 cid:105 ∇2  ··· ··· . . . ···   cid:104 φ1, φk cid:105 ∇2  cid:104 φ2, φk cid:105 ∇2  ...   cid:104 φk, φk cid:105 ∇2   cid:126 a =  cid:126 0,    subject to the proper boundary conditions. For example, to impose nonzero Dirichlet bound- ary conditions, we can ﬁx those values ai corresponding to elements on the boundary ∂Ω. Approximating solutions to the Poisson equation ∇2u = w can be carried out in a slightly modiﬁed linear system of equations. The weak form of the Poisson equation has the same left-hand side but now has a nonzero right-hand side:  similar fashion. If we write w =  cid:80 i biφi, then Galerkin’s method amounts to writing a   cid:90 Ω  v  cid:126 x ∇2u  cid:126 x  d cid:126 x = cid:90 Ω  v  cid:126 x w  cid:126 x  d cid:126 x,  solving:   cid:104 φ1, φ1 cid:105 ∇2  cid:104 φ2, φ1 cid:105 ∇2  for all test functions v  cid:126 x . To apply Galerkin’s method in this case, we not only ap-  proximate u  cid:126 x  =  cid:80 i aiφi  cid:126 x  but also assume the right-hand side w  cid:126 x  can be written w  cid:126 x  =  cid:80 i biφi  cid:126 x . Then, solving the weak Poisson equation in the φi basis amounts to   where  cid:104 f, g cid:105  ≡ cid:82 Ω f   cid:126 x g  cid:126 x  d cid:126 x, the usual inner product of functions. The matrix next to  cid:126 a is  known as the stiﬀness matrix, and the matrix next to  cid:126 b is known as the mass matrix. This is still a linear system of equations, since  cid:126 b is a ﬁxed input to the Poisson equation.   cid:126 a =   cid:104 φ1, φk cid:105 ∇2  cid:104 φ2, φk cid:105 ∇2   cid:104 φ1, φ2 cid:105 ∇2  cid:104 φ2, φ2 cid:105 ∇2   cid:104 φ1, φk cid:105   cid:104 φ2, φk cid:105    cid:104 φ1, φ2 cid:105   cid:104 φ2, φ2 cid:105    cid:104 φ1, φ1 cid:105   cid:104 φ2, φ1 cid:105    cid:104 φk, φk cid:105 ∇2   cid:104 φk, φ2 cid:105 ∇2   cid:104 φk, φ1 cid:105 ∇2  ··· ··· . . . ···  ··· ··· . . . ···     cid:104 φk, φk cid:105    cid:104 φk, φ2 cid:105    cid:104 φk, φ1 cid:105   ...  ...  ...  ...  ...  ...  Finite element discretizations like Galerkin’s method boil down to choosing appropriate spaces for approximation solutions u and test functions v. Once these spaces are chosen,   cid:126 b,   Partial Diﬀerential Equations  cid:4  349  the mass and stiﬀness matrices can be worked out oﬄine, either in closed form or by using a quadrature method as explained in Chapter 14. These matrices are computable from the choice of basis functions. A few common choices are documented below:    In two dimensions, the most typical use case for FEM makes use of a triangulation of the domain Ω ⊂ R2 and takes the φi basis to be localized small neighborhoods of triangles. For example, for the Poisson equation it is suﬃcient to use piecewise- linear “hat” basis functions as discussed in §13.2.2 and illustrated in Figure 13.9. In this case, the mass and stiﬀness matrices are very sparse, because most of the basis functions φi have no overlap. Exercise 16.2 works out the details of one such approach on the plane. Volumes in R3 admit similar formulations with triangles replaced by tetrahedra.    Spectral methods use bases constructed out of cosine and sine, which have the advan- tage of being orthogonal with respect to  cid:104 ·,· cid:105 ; in particularly favorable situations, this orthogonality can make the mass or stiﬀness matrices diagonal. Furthermore, the fast Fourier transform and related algorithms accelerate computations in this case.    Adaptive ﬁnite element methods analyze the output of a FEM solver to identify regions of Ω in which the solution has poor quality. Additional basis functions φi are added to reﬁne the output in those regions.  Example 16.9  Piecewise-linear FEM . Suppose we wish to solve the Poisson equation u cid:48  cid:48  x  = w x  for u x  on the unit interval x ∈ [0, 1] subject to Dirichlet boundary condi- tions u 0  = c and u 1  = d. We will use the piecewise linear basis functions introduced in §13.1.3. Deﬁne  1 + x when x ∈ [−1, 0] 1 − x when x ∈ [0, 1] 0  otherwise.  We deﬁne k + 1 basis elements using the formula φi x  ≡ φ kx − i  for i ∈ {0, . . . , k}.  For convenience, we begin by computing the following integrals:   cid:90  1  −1  −1  −1  φ x  ≡  cid:90  1 φ x 2 dx = cid:90  0 φ x φ x − 1  dx = cid:90  1 6k · φ cid:48  x  ≡   cid:104 φi, φj cid:105  =  1  0   1 + x 2 dx + cid:90  1  x 1 − x  dx =  0 1 6  .   1 − x 2 dx =  2 3  4 when i = j 1 when i − j = 1 0  otherwise.  when x ∈ [−1, 0] 1 −1 when x ∈ [0, 1] otherwise. 0  After applying change of coordinates, these integrals show  Furthermore, the derivative φ cid:48  x  satisﬁes  Hence, after change-of-variables we have   cid:104 φi, φj cid:105 d2 dx2 = − cid:104 φ cid:48 i, φ cid:48 j cid:105 2 = k ·  −2 when i = j when i − j = 1 1 otherwise. 0   350  cid:4  Numerical Algorithms  Figure 16.17 Approximated piecewise linear solutions of u cid:48  cid:48  x  = w x  computed using ﬁnite elements as derived in Example 16.9; in these examples, we take c = −1, d = 1, and k ∈ {5, 15, 100}. Up to the constant k, these values coincide with the divided diﬀerence second-derivative from §16.4.1. bi = w i k . Then, based on the integrals above, we should solve:  We will apply the Galerkin method to discretize u x  ≈ cid:80 i aiφi x . Assume we sample  1 k 1 −2  1 1 −2 . . .  k     cid:126 a =  1 6k  1 . . . . . . 1 −2  1 1 k    6k 1  4 1  1 4 . . .    1 . . . 1  . . . 4  1 6k      c b1 ... bk−1 d  .    The ﬁrst and last rows of this equation encode the boundary conditions, and the remaining rows come from the ﬁnite elements discretization. Figure 16.17 shows an example of this discretization in practice.  16.4.4 Finite Volumes  The ﬁnite volume method might be considered somewhere on the spectrum between ﬁnite elements and collocation. Like collocation, this method starts from the pointwise formulation of a PDE. Rather than asking that the PDE holds at a particular set of points in the domain Ω, however, ﬁnite volumes requires that the PDE is satisﬁed on average by integrating within the cells of a partition of Ω.  Suppose Γ ⊆ Ω is a region contained within the domain Ω and that we once again wish to solve the Laplace equation ∇2u = 0. A key tool for the ﬁnite volume method is the divergence theorem, which states that the divergence of a smooth vector ﬁeld  cid:126 v x  can be integrated over Γ two diﬀerent ways:   cid:90 Γ ∇ ·  cid:126 v  cid:126 x  d cid:126 x = cid:90 ∂Γ   cid:126 v  cid:126 x  ·  cid:126 n  cid:126 x  d cid:126 x.  Here,  cid:126 n is the normal to the boundary ∂Γ. In words, the divergence theorem states that the total divergence of a vector ﬁeld  cid:126 v x  in the interior of Γ is the same as summing the amount of  cid:126 v “leaving” the boundary ∂Γ.  100  1  1  1  w x   u x   approx.    Partial Diﬀerential Equations  cid:4  351   cid:90 Γ  Suppose we solve the Poisson equation ∇2u = w in Ω. Integrating over Γ shows  w  cid:126 x  d cid:126 x = cid:90 Γ ∇2u  cid:126 x  d cid:126 x since we solved the Poisson equation = cid:90 Γ ∇ ·  ∇u  cid:126 x   d cid:126 x since the Laplacian is the divergence of the gradient = cid:90 ∂Γ ∇u  cid:126 x  ·  cid:126 n  cid:126 x  d cid:126 x by the divergence theorem.  This ﬁnal expression characterizes solutions to the Poisson equation when they are averaged over Γ.  divide Ω into k regions Ω = ∪k  To derive a ﬁnite-volume approximation, again write u  cid:126 x  ≈  cid:80 k w  cid:126 x  d cid:126 x = cid:90 ∂Ωi ∇  cid:90 Ωi  ajφj  cid:126 x  ·  cid:126 n  cid:126 x  d cid:126 x =  i=1Ωi. For each Ωi,  k cid:88 j=1  k cid:88 j=1  aj cid:20  cid:90 ∂Ωi ∇φj  cid:126 x  ·  cid:126 n  cid:126 x  d cid:126 x cid:21  .  i=1 aiφi  cid:126 x  and now  This is a linear system of equations for the ai’s. A typical discretization in this case might take the φi’s to be piecewise-linear hat functions and the Ωi’s to be the Voronoi cells associated with the triangle centers  see §13.2.1 . 16.4.5 Other Methods  Countless techniques exist for discretizing PDEs, and we have only scraped the surface of a few common methods in our discussion. Texts such as [78] are dedicated to developing the theoretical and practical aspects of these tools. Brieﬂy, a few other notable methods for discretization include the following:    Domain decomposition methods solve small versions of a PDE in diﬀerent subregions of the domain Ω, iterating from one to the next until a solution to the global problem is reached. The subproblems can be made independent, in which case they are solvable via parallel processors. A single iteration of these methods can be used to approximate the global solution of a PDE to precondition iterative solvers like conjugate gradients.    The boundary element and analytic element methods solve certain PDEs using ba- sis functions associated with points on the boundary ∂Ω, reducing dependence on a triangulation or other discretization of the interior of Ω.    Mesh-free methods simulate dynamical phenomena by tracking particles rather than meshing the domain. For example, the smoothed-particle hydrodynamics  SPH  tech- nique in ﬂuid simulation approximates a ﬂuid as a collection of particles moving in space; particles can be added where additional detail is needed, and relatively few particles can be used to get realistic eﬀects with limited computational capacity.    Level set methods, used in image processing and ﬂuid simulation, discretize PDEs governing the evolution and construction of curves and surfaces by representing those objects as level sets { cid:126 x ∈ Rn : ψ  cid:126 x  = 0}. Geometric changes are represented by evolution of the level set function ψ.   352  cid:4  Numerical Algorithms 16.5 SOLVING PARABOLIC AND HYPERBOLIC EQUATIONS  In the previous section, we mostly dealt with the Poisson equation, which is an elliptic PDE. Parabolic and hyperbolic equations generally introduce a time variable into the formulation, which also is diﬀerentiated but potentially to lower order or with a diﬀerent sign. Discretizing time in the same fashion as space may not make sense for a given problem, since the two play fundamentally diﬀerent roles in most physical phenomena. In this section, we consider options for discretizing this variable independently of the others.  16.5.1 Semidiscrete Methods Semidiscrete methods apply the discretizations from §16.4 to the spatial domain but not to time, leading to an ODE with a continuous time variable that can be solved using the methods of Chapter 15. This strategy is also known as the method of lines.  Example 16.10  Semidiscrete heat equation . Consider the heat equation in one variable, given by ut = uxx, where u t; x  represents the heat of a wire at position x ∈ [0, 1] and time t. As boundary data, the user provides a function u0 x  such that u 0; x  ≡ u0 x ; we also attach the boundary x ∈ {0, 1} to a refrigerator and enforce Dirichlet conditions u t; 0  = u t; 1  = 0. Suppose we discretize x using evenly spaced samples but leave t as a continuous vari- able. If we use the ﬁnite diﬀerences technique from §16.4.1, this discretization results in functions u0 t , u1 t , . . . , un t , where ui t  represents the heat at position i as a function of time. Take L to be the corresponding second derivative matrix in the x samples with Dirichlet conditions. Then, the semidiscrete heat equation can be written h2 cid:126 u cid:48  t  = L cid:126 u t , where h = 1 n is the spacing between samples. This is an ODE for  cid:126 u t  that could be time-stepped using backward Euler integration:   cid:126 u tk+1  ≈ cid:18 I n+1 × n+1  −  1 h  L cid:19 −1   cid:126 u tk .  The previous example is an instance of a general pattern for parabolic equations. PDEs for diﬀusive phenomena like heat moving across a domain or chemicals moving through a membrane usually have one lower-order time variable and several spatial variables that are diﬀerentiated in an elliptic way. When we discretize the spatial variables using ﬁnite diﬀerences, ﬁnite elements, or another technique, the resulting semidiscrete formulation  cid:126 u cid:48  = A cid:126 u usually contains a negative deﬁnite matrix A. This makes the resulting ODE unconditionally stable.  As outlined in the previous chapter, there are many choices for solving the ODE after spatial discretization. If time steps are small, explicit methods may be acceptable. Implicit solvers, however, often are applied to solving parabolic PDEs; diﬀusive behavior of implicit Euler agrees behaviorally with diﬀusion from the heat equation and may be acceptable even with fairly large time steps. Hyperbolic PDEs, on the other hand, may require implicit steps for stability, but advanced integrators can combine implicit and explicit terms to prevent oversmoothing of non-diﬀusive phenomena.  When A does not change with time, one contrasting approach is to write solutions of semidiscrete systems  cid:126 u cid:48  = A cid:126 u in terms of eigenvectors of A. Suppose  cid:126 v1, . . . ,  cid:126 vn are eigenvectors of A with eigenvalues λ1, . . . , λn and that  cid:126 u 0  = c1 cid:126 v1 + ··· + cn cid:126 vn. As we showed in §6.1.2, the solution of  cid:126 u cid:48  = A cid:126 u is  cid:126 u t  = cid:88 i  cieλit cid:126 vi.   Partial Diﬀerential Equations  cid:4  353  The eigenvectors and eigenvalues of A may have physical interpretations in the case of a semidiscrete PDE. Most commonly, the eigenvalues of the Laplacian ∇2 on a domain Ω correspond to resonant frequencies of a domain, that is, the frequencies that sound when hitting the domain with a hammer. The eigenvectors provide closed-form “low-frequency approximations” of solutions to common PDEs after truncating the sum above.  16.5.2 Fully Discrete Methods  Rather than discretizing time and then space, we might treat the space and time variables more democratically and discretize them both simultaneously. This one-shot discretization is in some sense a more direct application of the methods we considered in §16.4, just by including t as a dimension in the domain Ω under consideration. Because we now multiply the number of variables needed to represent Ω by the number of time steps, the resulting linear systems of equations can be large if dependence between time steps has global reach.  Example 16.11  Fully discrete heat diﬀusion, [58] . Consider the heat equation ut = uxx. Discretizing x and t simultaneously via ﬁnite diﬀerences yields a matrix of u values, which we can index as uj i , representing the heat at position i and time j. Take ∆x and ∆t to be the spacing of x and t in the grid, respectively. Choosing where to evaluate the diﬀerent derivatives leads to diﬀerent discretization schemes stepping from time j to time j + 1.  For example, evaluating the x derivative at time j produces an explicit formula:  Isolating uj+1  i  gives a formula for obtaining u at time j + 1 without a linear solve.  Alternatively, we can evaluate the x derivative at time j + 1 for an implicit integrator:  uj+1 i − uj ∆t  i  =  uj i+1 − 2uj  ∆x 2  i + uj  i−1  .  uj+1 i − uj ∆t  i  =  uj+1 i+1 − 2uj+1  ∆x 2  i + uj+1 i−1  .  This integrator is unconditionally stable but requires a linear solve to obtain the u values at time j + 1 from those at time j.  These implicit and explicit integrators inherit their accuracy from the quality of the ﬁnite diﬀerence formulas, and hence—stability aside—both are ﬁrst-order accurate in time and second-order accurate in space. To improve the accuracy of the time discretization, we can use the Crank-Nicolson method, which applies a trapezoidal time integrator:  uj+1 i − uj ∆t  i  =  i + uj  i+1 − 2uj  ∆x 2  i−1  +  uj+1 i+1 − 2uj+1  ∆x 2  i + uj+1 i−1  1  2 cid:34  uj   cid:35  .  This method inherits the unconditional stability of trapezoidal integration and is second- order accurate in time and space. Despite this stability, however, as explained in §15.3.3, taking time steps that are too large can produce unrealistic oscillatory behavior.  In the end, even semidiscrete methods can be considered fully discrete in that the time- stepping ODE method still discretizes the t variable; the diﬀerence between semidiscrete and fully discrete is mostly for classiﬁcation of how methods were derived. One advantage of semidiscrete techniques, however, is that they can adjust the time step for t depending on the current iterate, e.g., if objects are moving quickly in a physical simulation, it might make sense to take more dense time steps and resolve this motion. Some methods also   354  cid:4  Numerical Algorithms  adjust the discretization of the domain of x values in case more resolution is needed near local discontinuities such as shock waves.  16.6 NUMERICAL CONSIDERATIONS  We have considered several options for discretizing PDEs. As with choosing time integra- tors for ODEs, the trade-oﬀs between these options are intricate, representing diﬀerent compromises between computational eﬃciency, numerical quality, stability, and so on. We conclude our treatment of numerical methods for PDE by outlining a few considerations when choosing a PDE discretization.  16.6.1 Consistency, Convergence, and Stability  A key consideration when choosing ODE integrators was stability, which guaranteed that errors in specifying initial conditions would not be ampliﬁed over time. Stability remains a consideration in PDE integration, but it also can interact with other key properties:    A method is convergent if solutions to the discretized problem converge to the theo-  retical solution of the PDE as spacing between discrete samples approaches zero.    A method is consistent if the accompanying discretization of the diﬀerential operators  better approximates the derivatives taken in the PDE as spacing approaches zero.  For ﬁnite diﬀerencing schemes, the Lax-Richtmyer Equivalence Theorem states that if a linear problem is well-posed, consistency and stability together are necessary and suﬃcient for convergence [79]. Consistency and stability tend to be easier to check than convergence. Consistency arguments usually come from Taylor series. A number of well-established meth- ods establish stability or lack thereof; for example, the well-known CFL condition states that the ratio of time spacing to spatial spacing of samples should exceed the speed at which waves propagate in the case of hyperbolic PDE [29]. Even more caution must be taken when simulating advective phenomena and PDEs that can develop fronts and shocks; specialized upwinding schemes attempt to detect the formation of these features to ensure that they move in the right direction and at the proper speed.  Even when a time variable is not involved, some care must be taken to ensure that a PDE approximation scheme reduces error as sampling becomes more dense. For example, in elliptic PDE, convergence of ﬁnite elements methods depends on the choice of basis functions, which must be suﬃciently smooth to represent the theoretical solution and must span the function space in the limit [16].  The subtleties of consistency, convergence, and stability underlie the theory of numeri- cal PDE, and the importance of these concepts cannot be overstated. Without convergence guarantees, the output of a numerical PDE solver cannot be trusted. Standard PDE in- tegration packages often incorporate checks for assorted stability conditions or degenerate behavior to guide clients whose expertise is in modeling rather than numerics.  16.6.2 Linear Solvers for PDE  The matrices resulting from PDE discretizations have many favorable properties that make them ideal inputs for the methods we have considered in previous chapters. For instance, as motivated in §16.3.1, elliptic PDEs are closely related to positive deﬁnite matrices, and typ- ical discretizations require solution of a positive deﬁnite linear system. The same derivative operators appear in parabolic PDEs, which hence have well-posed semidiscretizations. For   Partial Diﬀerential Equations  cid:4  355  this reason, methods like Cholesky decomposition and conjugate gradients can be applied to these problems. Furthermore, derivative matrices tend to be sparse, inducing additional memory and time savings. Any reasonable implementation of a PDE solver should include these sorts of optimizations, which make them scalable to large problems.  Example 16.12  Elliptic operators as matrices . Consider the one-dimensional second derivative matrix L with Dirichlet boundary conditions from §16.4.1. L is sparse and negative deﬁnite. To show the latter property, we can write L = −D cid:62 D for the matrix D ∈ R n+1 ×n given by  1 −1  1 −1  1 . . .  D =    .    . . . −1  1 −1  This matrix is a ﬁnite-diﬀerenced ﬁrst derivative, so this observation parallels the fact that d2y dx2 = d dx dy dx . For any  cid:126 x ∈ Rn,  cid:126 x cid:62 L cid:126 x = − cid:126 x cid:62 D cid:62 D cid:126 x = − cid:107 D cid:126 x cid:107 2 2 ≤ 0, showing L is negative semideﬁnite. Furthermore, D cid:126 x = 0 only when  cid:126 x = 0, completing the proof that L is negative deﬁnite.  Example 16.13  Stiﬀness matrix is positive semideﬁnite . Regardless of the basis φ1, . . . , φk, the stiﬀness matrix from discretizing the Poisson equation via ﬁnite elements  see §16.4.3  is negative semideﬁnite. Taking M∇2 to be the stiﬀness matrix and  cid:126 a ∈ Rk,  aiaj cid:104 φi, φj cid:105 ∇2 by deﬁnition of M∇2   cid:126 a cid:62 M∇2 cid:126 a = cid:88 ij = cid:42  cid:88 i aiφi, cid:88 j =  cid:104 ψ, ψ cid:105 ∇2 if we deﬁne ψ ≡ cid:88 i = − cid:90 Ω  cid:107 ∇ψ  cid:126 x  cid:107 2  ajφj cid:43   ∇2  2 d cid:126 x by deﬁnition of  cid:104 ·,· cid:105 ∇2  ≤ 0 since the integrand is nonnegative.  by bilinearity of  cid:104 ·,· cid:105 ∇2  aiφi  16.7 EXERCISES  16.1  “Shooting method,” [58]  The two-point boundary value problem inherits some struc- ture from ODE and PDE problems alike. In this problem, we wish to solve the ODE  cid:126 y cid:48  = F [ cid:126 y] for a function  cid:126 y t  : [0, 1] → Rn. Rather than specifying initial conditions, however, we specify some relationship g  cid:126 y 0 ,  cid:126 y 1   =  cid:126 0.   a  Give an example of a two-point boundary value problem that does not admit a  solution.   b  Assume we have checked the conditions of an existence uniqueness theorem, so given  cid:126 y0 =  cid:126 y 0  we can generate  cid:126 y t  for all t > 0 satisfying  cid:126 y cid:48  t  = F [ cid:126 y t ].   356  cid:4  Numerical Algorithms  Figure 16.18 Notation for Exercise 16.2.  Denote  cid:126 y t;  cid:126 y0  : R+ × Rn → R as the function returning  cid:126 y at time t given  cid:126 y 0  =  cid:126 y0. In this notation, pose the two-point boundary value problem as a root-ﬁnding problem.   c  Use the ODE integration methods from Chapter 15 to propose a computationally feasible root-ﬁnding problem for approximating a solution  cid:126 y t  of the two-point boundary value problem.   d  As discussed in Chapter 8, most root-ﬁnding algorithms require the Jacobian of the objective function. Suggest a technique for ﬁnding the Jacobian of your objective from Exercise 16.1c.  16.2 In this problem, we use ﬁrst-order ﬁnite elements to derive the famous cotangent  Laplacian formula used in geometry processing. Refer to Figure 16.18 for notation.   a  Suppose we construct a planar triangle T with vertices  cid:126 v1,  cid:126 v2,  cid:126 v3 ∈ R2 in coun- terclockwise order. Take f1  cid:126 x  to be the aﬃne hat function f1  cid:126 x  ≡ c +  cid:126 d ·  cid:126 x satisfying f1  cid:126 v1  = 1, f1  cid:126 v2  = 0, and f1  cid:126 v3  = 0. Show that ∇f1 is a constant vector satisfying:  ∇f1 ·   cid:126 v1 −  cid:126 v2  = 1 ∇f1 ·   cid:126 v1 −  cid:126 v3  = 1 ∇f1 ·   cid:126 v2 −  cid:126 v3  = 0.  The third relationship shows that ∇f1 is perpendicular to the edge from  cid:126 v2 to  cid:126 v3.   b  Show that  cid:107 ∇f1 cid:107 2 = 1  Figure 16.18  left .  Hint: Start by showing ∇f1 ·   cid:126 v1 −  cid:126 v3  =  cid:107 ∇f1 cid:107 2 cid:96 3 cos cid:0  π  Integrate over the triangle T to show  2 − β cid:1 .   c   h , where h is the height of the triangle as marked in   cid:90 T  cid:107 ∇f1 cid:107 2  2 dA =   cot α + cot β .  1 2  Hint: Since ∇f1 is a constant vector, the integral equals  cid:107 ∇f1 cid:107 2 the area of T . From basic geometry, A = 1  2  cid:96 1h.  2A, where A is   cid:4 v1  h   cid:5 1   cid:5 2   cid:5 3  α   cid:4 v2  β   cid:4 v3  p  p  βi  αi  θ2  q  θ1  Triangle T  One ring  Adjacent vertices   Partial Diﬀerential Equations  cid:4  357   d  Deﬁne θ ≡ π − α− β, and take f2 and f3 to be the hat functions associated with   cid:126 v2 and  cid:126 v3, respectively. Show that   cid:90 T ∇f2 · ∇f3 dA = −  1 2  cot θ.   e  Now, consider a vertex p of a triangle mesh  Figure 16.18, middle , and deﬁne fp : R2 → [0, 1] to be the piecewise linear hat function associated with p  see §13.2.2 and Figure 13.9 . That is, restricted to any triangle adjacent to p, the function fp behaves as constructed in Exercise 16.2a; fp ≡ 0 outside the triangles adjacent to p. Based on the results you already have constructed, show:   cid:90 R2  cid:107 ∇fp cid:107 2  1  2 cid:88 i  2 dA =   cot αi + cot βi ,  where {αi} and {βi} are the angles opposite p in its neighboring triangles.   f  Suppose p and q are adjacent vertices on the same mesh, and deﬁne θ1 and θ2 as  shown in Figure 16.18  right . Show   g  Conclude that in the basis of hat functions on a triangle mesh, the stiﬀness matrix  for the Poisson equation has the following form:   cot θ1 + cot θ2 .  1 2   cid:90 R2 ∇fp · ∇fq dA = − 2  1   cid:80 i∼j cot αj + cot βj   − cot αj + cot βj  0  Lij ≡ −  if i = j if i ∼ j otherwise.  Here, i ∼ j denotes that vertices i and j are adjacent.   h  Write a formula for the entries of the corresponding mass matrix, whose entries  are   cid:90 R2  fpfq dA.  Hint: This matrix can be written completely in terms of triangle areas. Divide into cases:  1  p = q,  2  p and q are adjacent vertices, and  3  p and q are not adjacent.  16.3 Suppose we wish to approximate Laplacian eigenfunctions f   cid:126 x , satisfying ∇2f = λf. Show that discretizing such a problem using FEM results in a generalized eigenvalue problem A cid:126 x = λB cid:126 x.  16.4 Propose a semidiscrete form for the one-dimensional wave equation utt = uxx, similar  to the construction in Example 16.10. Is the resulting ODE well-posed  §15.2.3 ?  16.5 Graph-based semi-supervised learning algorithms attempt to predict a quantity or la- bel associated with the nodes of a graph given labels on a few of its vertices. For instance, under the  dubious  assumption that friends are likely to have similar in- comes, it could be used to predict the annual incomes of all members of a social network given the incomes of a few of its members. We will focus on a variation of the method proposed in [132].   358  cid:4  Numerical Algorithms   a  Take G =  V, E  to be a connected graph, and deﬁne f0 : V0 → R to be a set of scalar-valued labels associated with the nodes of a subset V0 ⊆ V . The Dirichlet energy of a full assignment of labels f : V → R is given by  E[f ] ≡  cid:88  v1,v2 ∈E   f  v2  − f  v1  2.  Explain why E[f ] can be minimized over f satisfying f  v0  = f0 v0  for all v0 ∈ V0 using a linear solve.   b  Explain the connection between the linear system from Exercise 16.5a and the   c  Suppose f is the result of the optimization from Exercise 16.5a. Prove the discrete  3 × 3 Laplacian stencil from §16.4.1.  maximum principle:  Relate this result to a physical interpretation of Laplace’s equation.  max v∈V  f  v  = max v0∈V0  f0 v0 .  16.6 Give an example where discretization of the Poisson equation via ﬁnite diﬀerences  and via collocation lead to the same system of equations.  16.7  “Von Neumann stability analysis,” based on notes by D. Levy  Suppose we wish to approximate solutions to the PDE ut = aux for some ﬁxed a ∈ R. We will use initial conditions u x, 0  = f  x  for some f ∈ C∞ [0, 2π]  and periodic boundary conditions u 0, t  = u 2π, t .   a  What is the order of this PDE? Give conditions on a for it to be elliptic, hyper-  bolic, or parabolic.   b  Show that the PDE is solved by u x, t  = f  x + at .   c  The Fourier transform of u x, t  in x is  [Fxu] ω, t  ≡  1  √2π cid:90  2π  0  u x, t e−iωx dx,  where i = √−1  see Exercise 4.15 . It measures the frequency content of u ·, t . Deﬁne v x, t  ≡ u x + ∆x, t . If u satisﬁes the stated boundary conditions, show that [Fxv] ω, t  = eiω∆x[Fxu] ω, t .   d  Suppose we use a forward Euler discretization:  u x, t + ∆t  − u x, t   u x + ∆x, t  − u x − ∆x, t   .  = a  ∆t  2∆x  Show that this discretization satisﬁes  [Fxu] ω, t + ∆t  = cid:18 1 +  ai∆t ∆x  sin ω∆x  cid:19  [Fxu] ω, t .   e  Deﬁne the ampliﬁcation factor  ˆQ ≡ 1 +  ai∆t ∆x  sin ω∆x .  Show that  ˆQ > 1 for almost any choice of ω. This shows that the discretization ampliﬁes frequency content over time and is unconditionally unstable.   Partial Diﬀerential Equations  cid:4  359   f  Carry out a similar analysis for the alternative discretization  u x, t+∆t  =   u x − ∆x, t  + u x + ∆x, t  +  [u x + ∆x, t  − u x − ∆x, t ] .  a∆t 2∆x  1 2  Derive an upper bound on the ratio ∆t ∆x for this discretization to be stable.  16.8  “Fast marching,” [19]  Nonlinear PDEs require specialized treatment. One nonlin- ear PDE relevant to computer graphics and medical imaging is the eikonal equation  cid:107 ∇d cid:107 2 = 1 considered in §16.5. Here, we outline some aspects of the fast marching method for solving this equation on a triangulated domain Ω ⊂ R2  see Figure 13.9 .  a  We might approximate solutions of the eikonal equation as shortest-path dis- tances along the edges of the triangulation. Provide a way to triangulate the unit square [0, 1]× [0, 1] with arbitrarily small triangle edge lengths and areas for which this approximation gives distance 2 rather than √2 from  0, 0  to  1, 1 . Hence, can the edge-based approximation be considered convergent?   b  Suppose we approximate d  cid:126 x  with a linear function d  cid:126 x  ≈  cid:126 n cid:62  cid:126 x + p, where  cid:107  cid:126 n cid:107 2 = 1 by the eikonal equation. Given d1 = d  cid:126 x1  and d2 = d  cid:126 x2 , show that p can be recovered by solving a quadratic equation and provide a geometric interpretation of the two roots. You can assume that  cid:126 x1 and  cid:126 x2 are linearly independent.   c  What geometric assumption does the approximation in Exercise 16.8b make about the shape of the level sets { cid:126 x ∈ R2 : d  cid:126 x  = c}? Does this approxima- tion make sense when d is large or small? See [91] for a contrasting circular approximation.   d  Extend Dijkstra’s algorithm for graph-based shortest paths to triangulated shapes using the approximation in Exercise 16.8b. What can go wrong with this approach? Hint: Dijkstra’s algorithm starts at the center vertex and builds the shortest path in breadth-ﬁrst fashion. Change the update to use Exercise 16.8b, and consider when the approximation will make distances decrease unnaturally.  16.9 Constructing higher-order elements can be necessary for solving certain diﬀerential  equations.   a  Show that the parameters a0, . . . , a5 of a function f  x, y  = a0 + a1x + a2y + a3x2 + a4y2 + a5xy are uniquely determined by its values on the three vertices and three edge midpoints of a triangle.   b  Show that if  x, y  is on an edge of the triangle, then f  x, y  can be computed  knowing only the values of f at the endpoints and midpoint of that edge.   c  Use these facts to construct a basis of continuous, piecewise-quadratic functions on a triangle mesh, and explain why it may be useful for solving higher-order PDEs.  16.10 For matrices A, B ∈ Rn×n, the Lie-Trotter-Kato formula states  eA+B = lim n→∞   eA neB n n,  where eM denotes the matrix exponential of M ∈ Rn×n  see §15.3.5 .   360  cid:4  Numerical Algorithms  Suppose we wish to solve a PDE ut = Lu, where L is some diﬀerential operator that admits a splitting L = L1 + L2. How can the Lie-Trotter-Kato formula be applied to designing PDE time-stepping machinery in this case?  Note: Such splittings are useful for breaking up integrators for complex PDEs like the Navier-Stokes equations into simpler steps.   Bibliography  [1] S. Ahn, U. J. Choi, and A. G. Ramm. A scheme for stable numerical diﬀerentiation.  Journal of Computational and Applied Mathematics, 186 2 :325–334, 2006.  [2] E. Anderson, Z. Bai, and J. Dongarra. Generalized QR factorization and its applica-  tions. Linear Algebra and Its Applications, 162–164 0 :243–271, 1992.  [3] D. Arthur and S. Vassilvitskii. K-means++: The advantages of careful seeding. In Proceedings of the Symposium on Discrete Algorithms, pages 1027–1035. Society for Industrial and Applied Mathematics, 2007.  [4] S. Axler. Down with determinants! American Mathematical Monthly, 102:139–154,  1995.  22 3 :862–870, July 2003.  [5] D. Baraﬀ, A. Witkin, and M. Kass. Untangling cloth. ACM Transactions on Graphics,  [6] J. Barbiˇc and Y. Zhao. Real-time large-deformation substructuring. ACM Transac-  tions on Graphics, 30 4 :91:1–91:8, July 2011.  [7] R. Barrett, M. Berry, T. Chan, J. Demmel, J. Donato, J. Dongarra, V. Eijkhout, R. Pozo, C. Romine, and H. van der Vorst. Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods. Society for Industrial and Applied Mathematics, 1994.  [8] M. Bartholomew-Biggs, S. Brown, B. Christianson, and L. Dixon. Automatic dif- Journal of Computational and Applied Mathematics,  ferentiation of algorithms. 124 12 :171–190, 2000.  [9] H. Bauschke and J. Borwein. On projection algorithms for solving convex feasibility  problems. SIAM Review, 38 3 :367–426, 1996.  [10] H. H. Bauschke and Y. Lucet. What is a Fenchel conjugate? Notices of the American  Mathematical Society, 59 1 , 2012.  [11] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear  inverse problems. SIAM Journal on Imaging Sciences, 2 1 :183–202, Mar. 2009.  [12] J.-P. Berrut and L. Trefethen. Barycentric Lagrange interpolation. SIAM Review,  46 3 :501–517, 2004.  Statistics. Springer, 2006.  [13] C. Bishop. Pattern Recognition and Machine Learning.  Information Science and  [14] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 3 1 :1–122, Jan. 2011.  361   362  cid:4  Bibliography  2004.  [15] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press,  [16] S. Brenner and R. Scott. The Mathematical Theory of Finite Element Methods. Texts  in Applied Mathematics. Springer, 2008.  [17] R. Brent. Algorithms for Minimization without Derivatives. Dover Books on Mathe-  matics. Dover, 2013.  Journal, 4 1 :25–30, 1965.  [18] J. E. Bresenham. Algorithm for computer control of a digital plotter. IBM Systems  [19] A. Bronstein, M. Bronstein, and R. Kimmel. Numerical Geometry of Non-Rigid  Shapes. Monographs in Computer Science. Springer, 2008.  [20] S. Bubeck. Theory of convex optimization for machine learning. arXiv preprint  arXiv:1405.4980, 2014.  [21] C. Budd. Advanced numerical methods  MA50174 : Assignment 3, initial value ordi-  nary diﬀerential equations. University Lecture, 2006.  [22] R. Burden and J. Faires. Numerical Analysis. Cengage Learning, 2010.  [23] W. Cheney and A. A. Goldstein. Proximity maps for convex sets. Proceedings of the  American Mathematical Society, 10 3 :448–450, 1959.  [24] M. Chuang and M. Kazhdan. Interactive and anisotropic geometry processing using the screened Poisson equation. ACM Transactions on Graphics, 30 4 :57:1–57:10, July 2011.  [25] C. Clenshaw and A. Curtis. A method for numerical integration on an automatic  computer. Numerische Mathematik, 2 1 :197–205, 1960.  [26] A. Colorni, M. Dorigo, and V. Maniezzo. Distributed optimization by ant colonies.  In Proceedings of the European Conference on Artiﬁcial Life, pages 134–142, 1991.  [27] D. Comaniciu and P. Meer. Mean shift: A robust approach toward feature space analysis. Transactions on Pattern Analysis and Machine Intelligence, 24 5 :603–619, May 2002.  [28] P. G. Constantine and D. F. Gleich. Tall and skinny QR factorizations in MapReduce architectures. In Proceedings of the Second International Workshop on MapReduce and Its Applications, pages 43–50. ACM, 2011.  [29] R. Courant, K. Friedrichs, and H. Lewy. ¨Uber die partiellen diﬀerenzengleichungen  der mathematischen physik. Mathematische Annalen, 100 1 :32–74, 1928.  [30] Y. H. Dai and Y. Yuan. A nonlinear conjugate gradient method with a strong global  convergence property. SIAM Journal on Optimization, 10 1 :177–182, May 1999.  [31] I. Daubechies, R. DeVore, M. Fornasier, and C. S. G¨unt¨urk. Iteratively reweighted least squares minimization for sparse recovery. Communications on Pure and Applied Mathematics, 63 1 :1–38, 2010.  [32] T. Davis. Direct Methods for Sparse Linear Systems. Fundamentals of Algorithms.  Society for Industrial and Applied Mathematics, 2006.   Bibliography  cid:4  363  [33] M. de Berg. Computational Geometry: Algorithms and Applications. Springer, 2000.  [34] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121–2159, July 2011.  [35] S. T. Dumais. Latent semantic analysis. Annual Review of Information Science and  Technology, 38 1 :188–230, 2004.  [36] R. Eberhart and J. Kennedy. A new optimizer using particle swarm theory. In Micro  Machine and Human Science, pages 39–43, Oct 1995.  [37] M. Elad. Sparse and Redundant Representations: From Theory to Applications in  Signal and Image Processing. Springer, 2010.  [38] M. A. Epelman. Continuous optimization methods  IOE 511 : Rate of convergence  of the steepest descent algorithm. University Lecture, 2007.  [39] E. Fehlberg. Low-order classical Runge-Kutta formulas with stepsize control and their application to some heat transfer problems. NASA technical report. National Aero- nautics and Space Administration, 1969.  [40] R. Fletcher. Conjugate gradient methods for indeﬁnite systems. In G. A. Watson, editor, Numerical Analysis, volume 506 of Lecture Notes in Mathematics, pages 73–89. Springer, 1976.  [41] R. Fletcher and C. M. Reeves. Function minimization by conjugate gradients. The  Computer Journal, 7 2 :149–154, 1964.  [42] D. C.-L. Fong and M. Saunders. LSMR: An iterative algorithm for sparse least-squares  problems. SIAM Journal on Scientiﬁc Computing, 33 5 :2950–2971, Oct. 2011.  [43] M. Frank and P. Wolfe. An algorithm for quadratic programming. Naval Research  Logistics Quarterly, 3 1–2 :95–110, 1956.  [44] R. W. Freund and N. M. Nachtigal. QMR: A quasi-minimal residual method for  non-Hermitian linear systems. Numerische Mathematik, 60 1 :315–339, 1991.  [45] C. F¨uhrer. Numerical methods in mechanics  FMN 081 : Homotopy method. Univer-  sity Lecture, 2006.  tural Dynamics. Wiley, 1997.  [46] M. G´eradin and D. Rixen. Mechanical Vibrations: Theory and Application to Struc-  [47] T. Gerstner and M. Griebel. Numerical integration using sparse grids. Numerical  Algorithms, 18 3–4 :209–232, 1998.  [48] W. Givens. Computation of plane unitary rotations transforming a general matrix to triangular form. Journal of the Society for Industrial and Applied Mathematics, 6 1 :26–50, 1958.  [49] D. Goldberg. What every computer scientist should know about ﬂoating-point arith-  metic. ACM Computing Surveys, 23 1 :5–48, Mar. 1991.  [50] G. Golub and C. Van Loan. Matrix Computations. Johns Hopkins Studies in the  Mathematical Sciences. Johns Hopkins University Press, 2012.   364  cid:4  Bibliography  version 2.1.  [51] M. Grant and S. Boyd. CVX: MATLAB software for disciplined convex programming,  [52] M. Grant and S. Boyd. Graph implementations for nonsmooth convex programs. In V. Blondel, S. Boyd, and H. Kimura, editors, Recent Advances in Learning and Control, Lecture Notes in Control and Information Sciences, pages 95–110. Springer, 2008.  [53] E. Grinspun and M. Wardetzky. Discrete diﬀerential geometry: An applied introduc-  tion. In SIGGRAPH Asia Courses, 2008.  [54] C. W. Groetsch. Lanczos’ generalized derivative. American Mathematical Monthly,  105 4 :320–326, 1998.  [55] L. Guibas, D. Salesin, and J. Stolﬁ. Epsilon geometry: Building robust algorithms from imprecise computations. In Proceedings of the Fifth Annual Symposium on Com- putational Geometry, pages 208–217. ACM, 1989.  [56] W. Hackbusch.  Iterative Solution of Large Sparse Systems of Equations. Applied  Mathematical Sciences. Springer, 1993.  [57] G. Hairer. Solving Ordinary Diﬀerential Equations II: Stiﬀ and Diﬀerential-Algebraic  Problems. Springer, 2010.  [58] M. Heath. Scientiﬁc Computing: An Introductory Survey. McGraw-Hill, 2005.  [59] M. R. Hestenes and E. Stiefel. Methods of conjugate gradients for solving linear systems. Journal of Research of the National Bureau of Standards, 49 6 :409–436, Dec. 1952.  [60] D. J. Higham and L. N. Trefethen. Stiﬀness of ODEs. BIT Numerical Mathematics,  33 2 :285–303, 1993.  [61] N. Higham. Computing the polar decomposition with applications. SIAM Journal on  Scientiﬁc and Statistical Computing, 7 4 :1160–1174, Oct. 1986.  [62] N. Higham. Accuracy and Stability of Numerical Algorithms. Society for Industrial  and Applied Mathematics, 2 edition, 2002.  [63] G. E. Hinton. Training products of experts by minimizing contrastive divergence.  Neural Computation, 14 8 :1771–1800, Aug. 2002.  [64] M. Hirsch, S. Smale, and R. Devaney. Diﬀerential Equations, Dynamical Systems,  and an Introduction to Chaos. Academic Press, 3rd edition, 2012.  [65] A. S. Householder. Unitary triangularization of a nonsymmetric matrix. Journal of  the ACM, 5 4 :339–342, Oct. 1958.  [66] M. Jaggi. Revisiting Frank-Wolfe: Projection-free sparse convex optimization. Jour- nal of Machine Learning Research: Proceedings of the International Conference on Machine Learning, 28 1 :427–435, 2013.  [67] D. L. James and C. D. Twigg. Skinning mesh animations. ACM Transactions on  Graphics, 24 3 :399–407, July 2005.  [68] F. John. The ultrahyperbolic diﬀerential equation with four independent variables.  Duke Mathematical Journal, 4 2 :300–322, 6 1938.   Bibliography  cid:4  365  [69] W. Kahan. Pracniques: Further remarks on reducing truncation errors. Communica-  tions of the ACM, 8 1 :47–48, Jan. 1965.  [70] J. T. Kajiya. The rendering equation.  In Proceedings of SIGGRAPH, volume 20,  pages 143–150, 1986.  [71] Q. Ke and T. Kanade. Robust L1 norm factorization in the presence of outliers and missing data by alternative convex programming. In Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 739–746. IEEE, 2005.  [72] J. Kennedy and R. Eberhart. Particle swarm optimization.  In Proceedings of the International Conference on Neural Networks, volume 4, pages 1942–1948. IEEE, 1995.  [73] S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. Optimization by simulated annealing.  Science, 220 4598 :671–680, 1983.  [74] K. Kiwiel. Methods of Descent for Nondiﬀerentiable Optimization. Lecture Notes in  Mathematics. Springer, 1985.  [75] A. Knyazev. A preconditioned conjugate gradient method for eigenvalue problems and its implementation in a subspace. In Numerical Treatment of Eigenvalue Problems, volume 5, pages 143–154. Springer, 1991.  [76] A. Knyazev. Toward the optimal preconditioned eigensolver: Locally optimal block preconditioned conjugate gradient method. SIAM Journal on Scientiﬁc Computing, 23 2 :517–541, 2001.  [77] C. Lanczos. Applied Analysis. Dover Books on Mathematics. Dover Publications,  1988.  [78] S. Larsson and V. Thom´ee. Partial Diﬀerential Equations with Numerical Methods.  Texts in Applied Mathematics. Springer, 2008.  [79] P. D. Lax and R. D. Richtmyer. Survey of the stability of linear ﬁnite diﬀerence equations. Communications on Pure and Applied Mathematics, 9 2 :267–293, 1956.  [80] R. B. Lehoucq and D. C. Sorensen. Deﬂation techniques for an implicitly restarted Arnoldi iteration. SIAM Journal on Matrix Analysis and Applications, 17 4 :789–821, Oct. 1996.  [81] M. Leordeanu and M. Hebert. Smoothing-based optimization. In Proceedings of the  Conference on Computer Vision and Pattern Recognition. IEEE, June 2008.  [82] K. Levenberg. A method for the solution of certain non-linear problems in least-  squares. Quarterly of Applied Mathematics, 2 2 :164–168, July 1944.  [83] M. S. Lobo, L. Vandenberghe, S. Boyd, and H. Lebret. Applications of second-order  cone programming. Linear Algebra and Its Applications, 284 13 :193–228, 1998.  [84] D. Luenberger and Y. Ye. Linear and Nonlinear Programming. International Series  in Operations Research & Management Science. Springer, 2008.  [85] D. W. Marquardt. An algorithm for least-squares estimation of nonlinear parameters. Journal of the Society for Industrial and Applied Mathematics, 11 2 :431–441, 1963.   366  cid:4  Bibliography  [86] J. McCann and N. S. Pollard. Real-time gradient-domain painting. ACM Transactions  on Graphics, 27 3 :93:1–93:7, Aug. 2008.  [87] M. Mitchell. An Introduction to Genetic Algorithms. MIT Press, 1998.  [88] Y. Nesterov and I. Nesterov. Introductory Lectures on Convex Optimization: A Basic  Course. Applied Optimization. Springer, 2004.  [89] J. Niesen and W. M. Wright. Algorithm 919: A Krylov subspace algorithm for eval- uating the ϕ-functions appearing in exponential integrators. ACM Transactions on Mathematical Software, 38 3 :22:1–22:19, Apr. 2012.  [90] J. Nocedal and S. Wright. Numerical Optimization. Series in Operations Research  and Financial Engineering. Springer, 2006.  [91] M. Novotni and R. Klein. Computing geodesic distances on triangular meshes. Journal  of the Winter School of Computer Graphics  WSCG , 11 1–3 :341–347, Feb. 2002.  [92] J. M. Ortega and H. F. Kaiser. The LLT and QR methods for symmetric tridiagonal  matrices. The Computer Journal, 6 1 :99–101, 1963.  [93] C. Paige and M. Saunders. Solution of sparse indeﬁnite systems of linear equations.  SIAM Journal on Numerical Analysis, 12 4 :617–629, 1975.  [94] C. C. Paige and M. A. Saunders. LSQR: An algorithm for sparse linear equations and sparse least squares. ACM Transactions on Mathematical Software, 8 1 :43–71, Mar. 1982.  [95] T. Papadopoulo and M. I. A. Lourakis. Estimating the Jacobian of the singular value decomposition: Theory and applications. In Proceedings of the European Conference on Computer Vision, pages 554–570. Springer, 2000.  [96] S. Paris, P. Kornprobst, and J. Tumblin. Bilateral Filtering: Theory and Applications.  Foundations and Trends in Computer Graphics and Vision. Now Publishers, 2009.  [97] S. Paris, P. Kornprobst, J. Tumblin, and F. Durand. A gentle introduction to bilateral  ﬁltering and its applications. In ACM SIGGRAPH 2007 Courses, 2007.  [98] B. N. Parlett and J. Poole, W. G. A geometric theory for the QR, LU and power  iterations. SIAM Journal on Numerical Analysis, 10 2 :389–412, 1973.  [99] K. Petersen and M. Pedersen. The Matrix Cookbook. Technical University of Denmark,  November 2012.  [100] E. Polak and G. Ribi`ere. Note sur la convergence de m´ethodes de directions con-  jugu´ees. Mod´elisation Math´ematique et Analyse Num´erique, 3 R1 :35–43, 1969.  [101] W. Press. Numerical Recipes in C++: The Art of Scientiﬁc Computing. Cambridge  University Press, 2002.  [102] L. Ramshaw. Blossoming: A Connect-the-Dots Approach to Splines. Number 19 in  SRC Reports. Digital Equipment Corporation, 1987.  [103] C. P. Robert and G. Casella. Monte Carlo Statistical Methods. Springer Texts in  Statistics. Springer, 2005.  [104] R. Rockafellar. Monotone operators and the proximal point algorithm. SIAM Journal  on Control and Optimization, 14 5 :877–898, 1976.   Bibliography  cid:4  367  [105] Y. Saad. Iterative Methods for Sparse Linear Systems. Society for Industrial and  Applied Mathematics, 2nd edition, 2003.  [106] Y. Saad and M. H. Schultz. GMRES: A generalized minimal residual algorithm for solving nonsymmetric linear systems. SIAM Journal on Scientiﬁc and Statistical Computing, 7 3 :856–869, July 1986.  [107] S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and  Trends in Machine Learning, 4 2 :107–194, 2012.  [108] D. Shepard. A two-dimensional interpolation function for irregularly-spaced data. In Proceedings of the 1968 23rd ACM National Conference, pages 517–524. ACM, 1968.  [109] J. R. Shewchuk. An introduction to the conjugate gradient method without the  agonizing pain. Technical report, Carnegie Mellon University, 1994.  [110] J. Shi and J. Malik. Normalized cuts and image segmentation. Transactions on  Pattern Analysis and Machine Intelligence, 22 8 :888–905, Aug 2000.  [111] K. Shoemake and T. Duﬀ. Matrix animation and polar decomposition. In Proceedings  of the Conference on Graphics Interface, pages 258–264. Morgan Kaufmann, 1992.  [112] N. Z. Shor, K. C. Kiwiel, and A. Ruszcay`nski. Minimization Methods for Non-  diﬀerentiable Functions. Springer, 1985.  [113] M. Slawski and M. Hein. Sparse recovery by thresholded non-negative least squares.  In Advances in Neural Information Processing Systems, pages 1926–1934, 2011.  [114] S. Smolyak. Quadrature and interpolation formulas for tensor products of certain  classes of functions. Soviet Mathematics, Doklady, 4:240–243, 1963.  [115] P. Sonneveld. CGS: A fast Lanczos-type solver for nonsymmetric linear systems.  SIAM Journal on Scientiﬁc and Statistical Computing, 10 1 :36–52, 1989.  [116] O. Sorkine and M. Alexa. As-rigid-as-possible surface modeling. In Proceedings of the Symposium on Geometry Processing, pages 109–116. Eurographics Association, 2007.  [117] J. Stoer and R. Bulirsch.  Introduction to Numerical Analysis. Texts in Applied  Mathematics. Springer, 2002.  [118] L. H. Thomas. Elliptic problems in linear diﬀerential equations over a network. Tech-  nical report, Columbia University, 1949.  [119] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal  Statistical Society, Series B, 58:267–288, 1994.  [120] C. Tomasi and R. Manduchi. Bilateral ﬁltering for gray and color images. In Pro- ceedings of the Sixth International Conference on Computer Vision, pages 839–846. IEEE, 1998.  [121] J. A. Tropp. Column subset selection, matrix factorization, and eigenvalue optimiza- tion. In Proceedings of the Symposium on Discrete Algorithms, pages 978–986. Society for Industrial and Applied Mathematics, 2009.  [122] M. Turk and A. Pentland. Eigenfaces for recognition. Journal of Cognitive Neuro-  science, 3 1 :71–86, Jan. 1991.   368  cid:4  Bibliography  13 1 :743–767, 1963.  University Press, 1989.  [123] W. T. Tutte. How to draw a graph. Proceedings of the London Mathematical Society,  [124] H. Uzawa and K. Arrow. Iterative Methods for Concave Programming. Cambridge  [125] J. van de Weijer and R. van den Boomgaard. Local mode ﬁltering. In Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 428–433. IEEE, 2001.  [126] H. A. van der Vorst. Bi-CGSTAB: A fast and smoothly converging variant of BI-CG for the solution of nonsymmetric linear systems. SIAM Journal on Scientiﬁc and Statistical Computing, 13 2 :631–644, Mar. 1992.  [127] S. Wang and L. Liao. Decomposition method with a variable parameter for a class of monotone variational inequality problems. Journal of Optimization Theory and Applications, 109 2 :415–429, 2001.  [128] M. Wardetzky, S. Mathur, F. K¨alberer, and E. Grinspun. Discrete Laplace operators: In Proceedings of the Fifth Eurographics Symposium on Geometry  No free lunch. Processing, pages 33–37. Eurographics Association, 2007.  [129] O. Weber, M. Ben-Chen, and C. Gotsman. Complex barycentric coordinates with  applications to planar shape deformation. Computer Graphics Forum, 28 2 , 2009.  [130] K. Q. Weinberger and L. K. Saul. Unsupervised learning of image manifolds by semideﬁnite programming. International Journal of Computer Vision, 70 1 :77–90, Oct. 2006.  [131] J. H. Wilkinson. The perﬁdious polynomial. Mathematical Association of America,  1984.  [132] X. Zhu, Z. Ghahramani, J. Laﬀerty, et al. Semi-supervised learning using Gaussian In Proceedings of the International Conference on  ﬁelds and harmonic functions. Machine Learning, volume 3, pages 912–919. MIT Press, 2003.    COMPUTER SCIENCE  Numerical Algorithms  Methods for Computer Vision, Machine Learning, and Graphics  “This book covers an impressive array of topics, many of which are paired with a real-world  application. Its conversational style and relatively few theorem-proofs make it well suited for  computer science students as well as professionals looking for a refresher.”  —Dianne Hansford, FarinHansford.com  Numerical Algorithms: Methods for Computer Vision, Machine Learning, and Graphics  presents a new approach to numerical analysis for modern computer scientists. Using ex- amples from a broad base of computational tasks, including data processing, computation- al photography, and animation, the book introduces numerical modeling and algorithmic  design from a practical standpoint and provides insight into the theoretical tools needed to  support these skills. The book covers a wide range of topics—from numerical linear algebra to optimization and  differential equations—focusing on real-world motivation and unifying themes. It incorpo- rates cases from computer science research and practice, accompanied by highlights from  in-depth  literature  on  each  subtopic.  Comprehensive  end-of-chapter  exercises  encourage  critical thinking and build your intuition while introducing extensions of the basic material. Features     Introduces themes common to nearly all classes of numerical algorithms    Covers algorithms for solving linear and nonlinear problems, including popular tech-  niques recently introduced in the research community     Includes comprehensive end-of-chapter exercises that push you to derive, extend, and   analyze numerical algorithms     Access online or download to your smartphone, tablet or PC Mac    Search the full text of this and other titles you own    Make and share notes and highlights    Copy and paste text and figures for use in your own documents    Customize your view by changing font size and layout  an informa business w w w . c r c p r e s s . c o m  6000 Broken Sound Parkway, NW  Suite 300, Boca Raton, FL 33487 711 Third Avenue  New York, NY 10017 2 Park Square, Milton Park  Abingdon, Oxon OX14 4RN, UK  K23847  ISBN: 978-1-4822-5188-3 90000  9 781482 251883 w w w . c r c p r e s s . c o m  WITH VITALSOURCE®  EBOOK  N u m e r i c a l    A l g o r i t h m  s  Solomon  Numerical Algorithms  Methods for Computer Vision, Machine Learning, and Graphics        Justin Solomon  A N   A   K   P E T E R S   B O O K

@highlight

Numerical Algorithms: Methods for Computer Vision, Machine Learning, and Graphics presents a new approach to numerical analysis for modern computer scientists. Using examples from a broad base of computational tasks, including data processing, computational photography, and animation, the textbook introduces numerical modeling and algorithmic design from a practical standpoint and provides insight into the theoretical tools needed to support these skills.

The book covers a wide range of topics―from numerical linear algebra to optimization and differential equations―focusing on real-world motivation and unifying themes. It incorporates cases from computer science research and practice, accompanied by highlights from in-depth literature on each subtopic. Comprehensive end-of-chapter exercises encourage critical thinking and build students’ intuition while introducing extensions of the basic material.

The text is designed for advanced undergraduate and beginning graduate students in computer science and related fields with experience in calculus and linear algebra. For students with a background in discrete mathematics, the book includes some reminders of relevant continuous mathematical background.