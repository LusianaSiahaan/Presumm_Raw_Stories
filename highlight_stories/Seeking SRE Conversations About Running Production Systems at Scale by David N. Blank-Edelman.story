Seeking    SRE  CONVERSATIONS ABOUT RUNNING PRODUCTION SYSTEMS AT SCALE  Curated and edited by   David N. Blank-Edelman   Praise for Seeking SRE  “Reading this book is like being a fly on the wall as SREs discuss the challenges and successes they’ve had implementing SRE strategies outside of Google. A must-read for everyone in tech!” —Thomas A. Limoncelli  SRE Manager, Stack Overflow, Inc. Google SRE Alum  “A fantastic collection of SRE insights and principles from engineers at Google, Netflix, Dropbox, SoundCloud, Spotify, Amazon, and more. Seeking SRE shares the secrets to high availability and durability for many of the most popular products we all know and use.” —Tammy Butow  Principle SRE, Gremlin  “Imagine you invited all your favorite SREs to a big dinner party where you just walked around all night quietly eavesdropping. What would you hear? This book is that. These are the conversations that happen between the sessions at conferences or over lunch. These are the  sometimes animated, but always principled  debates we have among ourselves. This book is your seat at the SRE family kitchen table.” —Dave Rensin  Director of Google CRE   “Although Google’s two SRE books have been a force for good in the industry, they primarily frame the SRE narrative in the context of the solutions Google decided upon, and those may or may not work for every organization. Seeking SRE does an excellent job of demonstrating how SRE tenets can be adopted  or adapted  in various contexts across different organizations, while still staying true to the core principles championed by Google. In addition to providing the rationale and technical underpinning behind several of the infrastructural paradigms du jour that are required to build resilient systems, Seeking SRE also underscores the cultural scaffolding needed to ensure their successful implementation. The result is an actionable blueprint that the reader can use to make informed choices about when, why, and how to introduce these changes into existing infrastructures and organizations.” —Cindy Sridharan  Distributed Systems Engineer   Seeking SRE Conversations About Running Production Systems at Scale  Curated and edited by David N. Blank-Edelman  Beijing Beijing  Boston Boston  Farnham Sebastopol Farnham Sebastopol  Tokyo Tokyo   Seeking SRE Curated and edited by David N. Blank-Edelman Copyright   2018 David N. Blank-Edelman. All rights reserved. Printed in the United States of America. Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472. O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are also available for most titles  http:  oreilly.com safari . For more information, contact our corporate insti‐ tutional sales department: 800-998-9938 or corporate@oreilly.com.  Indexer: WordCo Indexing Services, Inc. Interior Designer: David Futato Cover Designer: Karen Montgomery Illustrator: Rebecca Demarest  Editor: Virginia Wilson Acquisitions Editor: Nikki McDonald Proofreader: Rachel Monaghan Copyeditor: Octal Publishing Services, Inc. Production Editors: Kristen Brown and Melanie Yarbrough  September 2018:   First Edition  Revision History for the First Edition 2018-08-21:  First Release  See http:  oreilly.com catalog errata.csp?isbn=9781491978863 for release details.  The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Seeking SRE, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc. The views expressed in this work are those of the authors, and do not represent the publisher’s views. While  the  publisher  and  the  authors  have  used  good  faith  efforts  to  ensure  that  the  information  and instructions contained in this work are accurate, the publisher and the authors disclaim all responsibility for errors or omissions, including without limitation responsibility for damages resulting from the use of or reliance on this work. Use of the information and instructions contained in this work is at your own risk. If any code samples or other technology this work contains or describes is subject to open source licenses  or  the  intellectual  property  rights  of  others,  it  is  your  responsibility  to  ensure  that  your  use thereof complies with such licenses and or rights.  978-1-491-97886-3 [GP]   Table of Contents  Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ix  Part I.   SRE Implementation  1. Context Versus Control in SRE. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   3  2.  Interviewing Site Reliability Engineers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   15  3. So, You Want to Build an SRE Team?. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    25  4. Using Incident Metrics to Improve SRE at Scale. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    33  5. Working with Third Parties Shouldn’t Suck. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   43  6. How to Apply SRE Principles Without Dedicated SRE Teams. . . . . . . . . . . . . . . . . . . . . .  65  7. SRE Without SRE: The Spotify Case Study. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  81  8.  Introducing SRE in Large Enterprises. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   111  9. From SysAdmin to SRE in 8,963 Words. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  123  10. Clearing the Way for SRE in the Enterprise. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   147  11. SRE Patterns Loved by DevOps People Everywhere. . . . . . . . . . . . . . . . . . . . . . . . . . . .   177  12. DevOps and SRE: Voices from the Community. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  187  v   13. Production Engineering at Facebook. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   207  Part II.  Near Edge SRE  14.  In the Beginning, There Was Chaos. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   233  15. The Intersection of Reliability and Privacy. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   245  16. Database Reliability Engineering. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   257  17. Engineering for Data Durability. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   275  18.  Introduction to Machine Learning for SRE. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  293  Part III.   SRE Best Practices and Technologies  19. Do Docs Better: Integrating Documentation into the Engineering Workflow. . . . . .   325  20. Active Teaching and Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   343  21. The Art and Science of the Service-Level Objective. . . . . . . . . . . . . . . . . . . . . . . . . . . .    355  22. SRE as a Success Culture. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   365  23. SRE Antipatterns. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   379  24.  Immutable Infrastructure and SRE. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  405  25. Scriptable Load Balancers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   413  26. The Service Mesh: Wrangler of Your Microservices?. . . . . . . . . . . . . . . . . . . . . . . . . . . .  431  Part IV.   The Human Side of SRE  27. Psychological Safety in SRE. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   451  28. SRE Cognitive Work. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  461  29. Beyond Burnout. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    487  vi      Table of Contents   30. Against On-Call: A Polemic. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  507  31. Elegy for Complex Systems. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   529  32.  Intersections Between Operations and Social Activism. . . . . . . . . . . . . . . . . . . . . . . . .  537  33. Conclusion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   555  Index. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   557  Table of Contents      vii    Introduction  David N. Blank-Edelman, curator editor  And So It Begins... Conversations. That’s the most important word in the title of this book, so pardon the  lack  of  subtlety  I’m  demonstrating  by  making  it  the  first  and  last  word  of  this book. Why is it so important? That’s where the “Seeking” part of Seeking SRE comes in. The people I respect in the Site Reliability Engineering  SRE  field all believe that the field itself is still evolving, expanding, changing, and being discovered. We are all in some sense still seeking SRE. In my experience, fields like ours grow best when the people in that field—the actual practitioners—talk to one another. Bring people together, let them talk, argue, laugh, share their experiences  success and failures  and their unsolved problems. A smart, kind,  diverse,  inclusive,  and  respectful  community  in  conversation  can  catalyze  a field like nothing else. Origin Story It was at SREcon16 Europe, one of the gatherings of the SRE community, that this book was born.  Full disclosure: I’m one of the cofounders of SREcon.  Brian Ander‐ son, the original O’Reilly editor for this book, was on the hunt. The splendid book by Google called Site Reliability Engineering had recently met with much-deserved com‐ mercial success and the publisher was on the lookout for more SRE content to pub‐ lish. He and I were talking about the possibilities during a break when I realized what didn’t  exist  for  SRE.  There  was  no  volume  I  knew  of  that  could  bring  people  into some  of  the  more  interesting  conversations  that  were  happening  in  the  field   like those that were happening at SREcon . I was seeing people discuss subjects like these:  Introduction      ix     New implementations of SRE that didn’t have a book yet. SRE has blossomed in new and exciting ways as it has taken root in different  sometimes brownfield  contexts.    Innovative ways to learn how to practice SRE.   What gets in the way of adopting SRE.   The best practices people had discovered as they adopted, adapted, and lived it.   Where the field was going next, including the subjects that are new now in the  field but will be commonplace in short order.    Finally—and  maybe  most  important—what  about  the  humans  in  the  picture? What is SRE doing for them? What is SRE doing to them? Are humans really the problem in operations  that need to be automated away  or is that short sighted? Can SRE improve more than just operations?  And, so, the idea for Seeking SRE was born. Much to my surprise and delight, close to 40 authors from all over the field and all over the world liked the idea and decided to join me on this little project. I can’t thank them enough. Voices Besides a wee bit of meta matter like what you are reading now, I’ve tried to keep my voice soft in the book so that we could stand together and hear what its amazing con‐ tributors have to say. You’ll likely notice that this book doesn’t have a single consis‐ tent textual voice  mine or any other editor’s . There’s no attempt made to put the material  into  a  blender  and  homogenize  the  chapters  into  a  beige  “technical  book register.” I intentionally wanted you to hear the different voices from the different contributors just the way they talk. The only instruction they were given on tone was this:  Pretend you are at lunch at a conference like SREcon. You are sitting with a bunch of smart SREs at lunch you don’t know, and one of them says to you, “So, what are you working on? What’s interesting to you these days?” You begin to answer the question… Now write that down.  Even  further  in  this  direction  is  the  crowdsourced  chapter  on  the  relationship between DevOps and SRE  Chapter 12 . When I realized that there are likely many answers to this question and that no one I knew had the only right answer, I put the question  out  to  my  social  networks   and  people  were  kind  enough  to  broadcast  it even further . I’m very grateful for everyone who answered the call. And just for fun, there are also little “You Know You’re an SRE” tidbits scattered throughout the book from anonymous contributors  I promised anonymity when I asked the internet for them, but thanks to everyone who provided one or several!   x      Introduction   In addition to a variety of voices, there’s also a variety of opinions and viewpoints. Are you going to agree with everything you read in this volume? Gosh, I hope not. I don’t agree with everything, so I don’t see why you should. That would make for a really boring set of conversations, no? I strongly recommend that you remember that there are one or more humans behind every chapter who were brave enough to put their opinions out there, so we all could have a good confab. The SRE community’s capacity to engage with respect is something I’ve come to appreciate over the years, and I know you’ll follow in that tradition. As the editor curator of this book, which I’m very proud of, I do want to mention one  regret  up  front.  Our  field  has  a  real  problem  with  diversity  and  inclusion  of underrepresented minorities. We can’t have all of the important conversations if not everyone is in the room. Despite my attempts to address this lack of representation, this book doesn’t go far enough to work against this situation. I take full responsibil‐ ity for that failure. Forward in All Directions!1 Are these the only interesting conversations to be had in SRE? Not by a long shot. These are just the ones that could be assembled into a book in finite time. By the time you are done reading, I am sure you will have your own list of omissions  don’t hesi‐ tate to send them to me . There are definitely subjects that didn’t make it into the book for a whole host of reasons  time, author availability, no one thought of it in time, etc. . I’m keenly interested in hearing what you think should be in a book like this that isn’t already. If this book is a hit and there are future editions volumes, your suggestions will be a great start for future conversations. And finally, if this book is just a good passive read for you, dear reader, it is either a partial success or a partial failure depending on your disposition. At the very best, it hasn’t met its Service-Level Objective. If you take it as the invitation it is meant to be and join the conversation—and by doing so you help us push the field forward—it will have met its objective. Welcome. Acknowledgments Drop a stone into a pond and you get these lovely ripples undulating out and back into the center. Drop a book idea into the world and the same thing happens.  1 Apologies to 3 Mustaphas 3.  Introduction      xi   This means there are these concentric circles of people I need to thank. In the middle are all of the contributors who were willing to bring their time, energy and brilliance to the project. Thank you authors, one and all. The authors created fabulous content, and then the tech reviewers told us all how to make it even better. Thank you to Patrick Cable, Susan J. Fowler, Thomas A. Limon‐ celli,  James  Meickle,  Niall  Richard  Murphy,  Amy  Nguyen,  Grace  Petegorsky,  and Yonatan Zunger. The next ring are the O’Reilly editors, past and present who were willing to take this book in, tolerate the chaotic deadline-defying maelstrom inherent in a contributed volume  of  brilliant  people,  and  refine  its  contents  into  the  splendid  shape  you  see now.  Thank  you  Brian  Anderson,  Virginia  Wilson,  Kristen  Brown,  Melanie  Yar‐ brough, Nick Adams, and Nikki McDonald. Thanks to the proofreaders Bob Russell and  Rachel  Monaghan.  Also,  thank  you  to  Karen  Montgomery  and  the  O’Reilly design department for providing the cuddly animal for the cover. Now the sea otter on my bookshelf has a friend. And finally, thank you to Cindy and Elijah, my family, and all my friends who were willing to tolerate yet another book project with good cheer. They are the people that provided the support I needed to drop the stone into the pond in the first place.  xii      Introduction   PART I SRE Implementation    Can you build an SRE practice by giving people the context they need?   How do you find and hire good SREs?   How do you build an SRE team?   Is it possible to improve a team itself through metrics?   How do you do SRE when you don’t own your systems?   Is it possible to implement SRE principles at your organization without a dedica‐  ted SRE team?    How does an SRE implementation unlike any other evolve over time?   What does it take to grow SRE in a brownfield setting at a large enterprise?   How do you transition from sysadmin to SRE?   What do you need to eliminate in your organization to be able to do SRE?   What does the DevOps world think about SRE?   What is the relationship between DevOps and SRE?   What is SRE like at other organizations that serve billions of people daily?  Discuss.   You Know You’re an SRE When…  …your children tell you to stop bugging them about doing their homework because, “I’m within SLO!” …you have to justify the difference between ops, SRE, DevOps, and so on, and you confuse even yourself sometimes. …your first question is “how are you measuring that?” …you erase the word “blame” from your vocabulary.   CHAPTER 1 Context Versus Control in SRE  A discussion with Coburn Watson, Microsoft  formerly Netflix  and David N. Blank-Edelman  David:  We’ve  had  the  pleasure  of  talking  about  a  lot  of  things  in  the  time  we’ve known each other. One of the most interesting things I’ve heard you speak about is a way of doing SRE that focuses on providing context instead of using processes that are centered around control  the more common way SRE is practiced . Can we dig some more into this? Can you explain what you mean by context versus control and what a good example of each would be? Coburn:  I  think  of  context  as  providing  additional,  pertinent  information,  which allows someone to better understand the rationale behind a given request or state‐ ment.  At  the  highest  level,  availability-related  context  as  shared  at  Netflix  with  an engineering team would be the trended availability of their microservice[s] and how that relates to the desired goal, including availability of downstream dependencies. With this domain-specific context, an engineering team has the responsibility  and context  to take the necessary steps to improve their availability. In  a  control-based  model  a  team  will  be  aware  of  their  microservice[s]  availability goal, but if they fail to achieve that goal there might be a punitive action. This action might involve removing their ability to push code to production. At Netflix, we err toward  the  former  model,  sharing  context  on  microservice-level  availability,  then working with teams when needed to help improve availability. The challenge is making sure sufficient context is provided to teams. When someone makes a nonideal operational decision at Netflix, the first question to ask is whether that  person  had  sufficient  context  to  make  a  better  decision.  Quite  often,  the  SRE team will find that a hit to availability is a result of insufficient context being passed to the team, in particular context related to reliability. Those are the gaps we seek to close as an SRE team to improve overall availability.  3   In a very large organization, it can be challenging to provide enough context such that, based on context alone, people can achieve the desired availability goals for their services. In organizations of this scale, you often have to fall back on more processes to achieve your availability goals. One example is the Google error budget model.1 Another case for a more control-based model is when lives are on the line. If some‐ one frequently writes unsafe software for an airplane autopilot system, that person  and  company   probably  has  a  very  low  tolerance  for  a  primarily  context-based approach. They don’t want to get together and figure out how to improve their avail‐ ability through additional context if planes are falling out of the sky. It’s up to each SRE organization to determine how much risk they can assume as one factor in find‐ ing the split between a context- versus control-based model. I believe there is a difference between information and context. In systems monitor‐ ing, information could just be a bunch of availability metrics I jam into a dashboard and email to the team. A typical engineer receiving such an email would ignore it because, 1  they are in charge of writing business logic for a service, and 2  they lack the expertise to digest and understand resource and availability metrics presented as time series. At Netflix we have hundreds of thousands of operational metrics available to us. In order  to  support  a  context-driven  model  to  improve  availability,  we  have  to  bring specific domain knowledge to bear on the data. This requires taking the information and massaging it into a format that tells a story about availability. By applying such a transformation, we are able to push this context to teams on an as-needed basis so they can measure if availability improves for the given microservice. As an example, one key availability metric is the trended success rate from dependent services on a given microservice  as measured from the client side and breaking down failure rates based on cause . My  team  doesn’t  own  availability,  but  our  job  is  to  improve  it  over  time.  Why? Because someone can always blow a tire off the car. Oftentimes teams reach out and say, “I’m not quite sure why my availability dropped; can we talk about that?” While investigating  the  situation,  it  could  be  discovered  that  someone  modified  a  client library  or  changed  a  timeout  setting.  As  mentioned  earlier,  it  is  important  to  start from the principle that people are not operating in a negligent manner; they’re just lacking the context to make the better decisions. We also don’t forget that the systems can be overly complex and the operational bar required to avoid incidents is both too high or unnecessary. An example in this latter category is the tuning of static time‐ outs in a dynamic system.  1 See Chapter 4, “Service Level Objectives”, from Google’s first Site Reliability Engineering book.  4      Chapter 1: Context Versus Control in SRE   Though  a  context-driven  model  is  ideal,  you  can  drift  toward  the  needed  control when you see a team having repeated incidents. They have been provided the same effective availability context as other teams, yet their availability continues to suffer. In some companies, control takes the form of prohibiting code from being pushed to production, and then of course engineers come to management because they need to push code to production. At Netflix I’ll lead with the “you’re on my list” discussion with  an  engineering  manager,  which  involves  in-person  additional  level-setting around expectations of service availability. If there is a common view, then I’ll men‐ tion  that  they  are  developing  a  lot  of  features,  but  not  addressing  the  necessary changes required to improve availability. I end by asking, “How do we get those on your list?” In my mind, that’s about the extent of control I really want. I am lucky to work in a company with a mature set of individuals who recognize the importance of availability to the business and prioritize efforts appropriately. I find this type of dis‐ cussion typically gets the needle pointing in the right direction. Mentioned  previously,  but  worth  calling  out  again,  before  I  make  context-versus- control seem like a utopia that everyone can achieve, just remember that we run a business where if we have a failure, maybe streams don’t get delivered. We don’t have planes falling out of the sky or people’s heart machines stopping. That gives us more flexibility about where we fall on the context-versus-control spectrum. David: Does this mean that a context-based approach works at smaller scales but not nearly as well at larger scales? Coburn: I believe the “scale” at which context over control works has little to do with the size of the production environment or customer base. It’s more about the organi‐ zation size and how they operate between teams. As you grow an organization, the effective use of context for availability can be challenging. A key factor may be less one-on-one communication or face-to-face time as the company grows. At Netflix, I have the luxury of all the engineering teams I engage with residing on one campus. I can have coffee with any manager at Netflix. I’ve worked at large com‐ panies like HP with teams sitting on the other side of the globe. In that global situa‐ tion, I still get them appropriate context, but it requires more work to give effective context. My assumption is if a company leans in and starts with control it’s primarily because   regardless  of  scale   they’re  much  more  comfortable  leading  with  process and control. David: Is there a way that infrastructure plays into reliability as related to context? Coburn: We consider ourselves in our push-model perspective to have great reliabil‐ ity because we’re immutable in nature. I think we’ve all worked at companies where we upgrade something, and it turns out it was bad and we spend the night firefighting it because we’re trying to get the site back up because the patch didn’t work. When we go to put new code into production, we never upgrade anything, we just push a  Context Versus Control in SRE      5   new version alongside the current code base. It’s that red black or blue green—what‐ ever people call it at their company. The new version of code goes in, and if some‐ thing breaks in the canary, we just take it back out and we immediately roll back, or we don’t even go through with the full deployment. This brings recovery down from possibly hours to minutes. Although we have immutable code deployments, we also have something called Fast Properties. This means we can go into production with a later ability to dynamically update an application property. Since this breaks our immutable model but is often required, we see this capability being overused and leading to production incidents. Much like other common problems in production, if we see a team or collection of teams  stumbling  over  the  problem  of  dynamic  property  management,  we  look  for ways to remove the risk through improved tooling. In our production environment, we now use Spinnaker, our continuous delivery platform, to manage staggered roll‐ outs  of  dynamic  properties  to  minimize  blast  radius  and  identify  issues  early.  We refer to this general strategy as a “guardrail.” Even  once  we’ve  done  that,  sometimes  teams  still  go  change  something  manually instead of going through a fast property pipeline and break production. If it happens again, we’re like, “OK, they clearly haven’t gotten the message. We need to go meet with them and say, ‘please don’t push this button’.” We try at all costs to not take away the button, but at some point, we probably will. David: This brings up a question about feedback loops. It sounds like some of your feedback signals are “did the site go down?”, “did something trend positive?”, or in terms of money “did it trend negative in the way you wanted it to?” Is there a more direct way to get an understanding from the humans whether they got the context they needed besides observing the system in a black box fashion and seeing whether the indicators you were looking at changed? Coburn: That’s one of the evolutions we’ve made as our company’s grown. At Netflix we probably have 2,000 engineers, and that covers all different product domains. I could be building an internal tools UI, an encoding engine, standing up an infrastruc‐ ture stack, or something to give you better recommendations. Some of those efforts have a more significant impact on customer-facing availability than others. From a numbers perspective, probably 50% of our engineering teams on a given day can do whatever they want in production and it has no possible risk to our service availabil‐ ity in any way whatsoever. Initially we used to walk around the hall banging the virtual pan and yelling, “Hey, our availability is at three-nines; this is a problem!” The problem is it’s a very diffuse message and 50% of the people are like, “OK, great, um, you’re saying availability is not good, but what can I do about it as my services can’t impact availability?”  6      Chapter 1: Context Versus Control in SRE   Then, we changed our messaging to focus the banging of the pan on the teams that actually affect availability, which might be the other 50% of the population. So, now, at least our message about service availability is hitting the right audience. The next step is to funnel context, which is specific to each engineering team and allows them to evaluate if they have substandard availability. But  even  figuring  out  who  to  provide  this  information  to  isn’t  always  easy.  In  a microservice architecture, there might be 40-plus teams that actually have microser‐ vices running at any given time, which can impact availability on the service critical path.  If  we’re  measuring  availability  at  the  edge  of  our  service  and  determine  that one-tenth of 1% of users couldn’t play movies in a given hour, it’s quite challenging to identify which team might have actually driven that outage. Even more challenging is that, in many cases, it’s externally driven. As one example, think of an online gam‐ ing platform that needs to be available to play a title on the service. In that case per‐ haps I have to go to the UI team at Netflix that depends on that service and see if they can build resiliency to the external vendor service failure  which we have done . I  find  it  helpful  to  be  clear  within  your  organization  when  referring  to  the  terms “availability” and “reliability.” As an analogy to how we use these terms at Netflix in the context of our streaming service, I refer to a disk array which might be part of a Storage-Area Network. If the disk array represents the service, the underlying disk drives in the array would represent microservices. The design of the disk array takes into account that individual drives might fail, but the array should still function to both serve and persist data. Using this model, you would calculate availability as the percentage of time the disk array  the Netflix streaming service, in my world  is able to provide service to a client. The rate at which drives fail in the array represents the reliability of the drive  in my case, a microservice . Much as with disk array configurations  RAID configuration, caching, etc. , you can apply operational patterns in a microservice architecture to improve overall service availability in light of possible microservice failures. One such example in use at Net‐ flix is the Hystrix framework, based on the bulkhead pattern, which opens up “cir‐ cuits” between microservices when a downstream microservice fails. The framework serves a fallback experience when possible to still provide service to an end user. In  conclusion,  setting  and  measuring  reliability  goals  at  the  microservice  level  lets you  move  toward  a  desired  aggregate  service-level  availability.  Different  organiza‐ tions might use the terms availability and reliability differently, but this is how we define them in light of our operational model. David: So, what sort of info can you give to that team that would be actionable? Coburn: Another company I talked to had a microservice availability report. They looked at the rate at which services were providing successful calls to their neighbor. If I’m a service that handles subscriber or member information and I have 30 services  Context Versus Control in SRE      7   talking to me, my primary measure of availability should be the rate of success my dependencies have. Many times, teams will focus on their service-side view of being up and running. This doesn’t guarantee you are servicing requests successfully at the desired rate, but it all comes down to the measures. We liked the model from the other company and looked to see how we could apply it to our own infrastructure. Luckily, we have a common IPC [interprocess communi‐ cation] framework and we have metrics around Hystrix and other commands that record the rate at which a client  dependent service  is having successful calls. We aggregate this information for all our critical microservices and compare the trend to the previous 30 days. It isn’t meant to be near real time, it’s meant to be more opera‐ tional—are you getting better or worse. Let’s say a team owns three microservices, and the goal is for the microservices to have  four-nines  availability  for  calls  from  clients.  If  these  microservices  stay  above four-nines, the team will never receive an email  availability report . If the last 7 days compared to the previous 21 have a deviation in a negative manner, or if the micro‐ services are failing to achieve the four-nines, a report will be sent to the team. The report shows week over week as a bar chart. Green indicates you’re achieving the goal for  a  given  week.  Red  or  yellow  means  you’re  failing  to  achieve  the  goal.  Yellow means improvement over the previous week; red indicates degradation. Clicking on a bar in the graph will deliver a detailed report that shows for the entire window call rates of upstream  client  services talking to the microservice and the availability to those calls. On the right panel is a view of downstream dependency call and success rates,  which  is  helpful  as  the  microservice  availability  might  be  reduced  by  down‐ stream dependency. We call this internal implementation the “microservice availabil‐ ity scorecard framework.” At this point, the SRE team  working with Data Analytics  has provided information to a team about the microservices they own, and in partic‐ ular their microservice availability to dependent client services, independent of Net‐ flix  service  availability.  This  should  be  very  actionable  information.  When dashboards are continually pushed to engineers, they can stop looking at them rela‐ tively quickly. The SRE solution at Netflix is to only push a scorecard when some‐ thing changes that a team might need to pay attention to. David:  So,  this  sounds  pretty  reactive,  right?  You  send  them  a  scorecard  because you’d like to see something that already happened be different in the future. What’s the proactive side of this? How do you help someone who is trying to make a good decision well before they would get a scorecard? Coburn: A good analogy for the scorecard is a message on your car dash indicating “your right front tire is losing pressure.” It doesn’t tell you what to do about it, just provides  information  that  a  trend  is  going  in  the  wrong  direction.  When  thinking about how to best inform people, I leveraged experience from past work in the per‐ formance  domain,  where  we’re  not  as  worried  about  catching  large  performance  8      Chapter 1: Context Versus Control in SRE   deviations. We have canaries that evaluate a system looking for significant deviations, so if CPU demand happens to jump 20% on a push, alarms start going off. What ends up getting you in the long run is the five-millisecond increase week over week for a six-month period. At the end of which you say, “Wow, I’m suddenly running with three times as much capacity for that service; what happened?” The role of the score‐ card is to catch smaller deviations as well to avoid that slow and dangerous drift. I think the analogous situation for reliability is if you ignore the small deviations and fail to proactively address them, you’re just waiting for the big outage. David: OK, so what do you do from a contextual perspective to allow people to make the right decisions in the moment? In theory, the error budget concept means that at any time T, I have a way to determine at that moment whether I should do or not do something like launch a new version. What is the contextual version of that? Is the theory that people look at a scorecard synchronously and then make the right deci‐ sion? Coburn: If someone is having a problem where they’re significantly impacting actual production availability, such as consistently having production incidents that result in  customers  not  being  able  to  stream  content,  then  the  microservice  availability scorecard context is probably not the way to solve that problem. In that case, they have most likely been receiving the report but not acting on it. The way to solve that problem is to gather in a room and figure a path forward. It still doesn’t mean they should  stop  pushing  code,  because  they  have  a  lot  of  other  services  dependent  on them. To your question about more real-time inputs to help engineers make better deploy‐ ment decisions in the moment, we strive to put the information inside of the tools they are working with. Spinnaker now exposes availability trends at the cluster  the AWS term is Autoscaling Group  and it is right in front of the engineer if they are making a change via the UI. When we look at continually improving availability, the target falls into two primary categories:  avoiding  all  incidents,  regardless  of  type,  and  a  specific  failure  pattern someone has that’s causing an incident or repeated outage. From a context perspective, if they’re having a problem where they’re making a set of decisions over and over that result in production-impacting incidents, that requires a human-delivered  versus  a  system-delivered  report.  At  Netflix,  the  Core  SRE  team doesn’t have the responsibility to step in and troubleshoot and resolve microservice- specific issues, but the team is instead considered the “central nervous system of Net‐ flix.” Taking the central nervous system analogy a bit further, this is the only team at Netflix that sees all the failures, whether external, internal, CDN [Content Delivery Network], network, etc., with the resultant responsibility to determine which teams to reach out to and engage.  Context Versus Control in SRE      9   David: Viewing your group as the nervous system of Netflix, what process do you have for propagating information throughout the org so that people in one part can learn from the other parts  either a good way to do something or not to repeat a bad experience ? Coburn:  Based  on  the  specific  shortcoming  in  operational  risk  we  would  like  to address, we have a couple of channels to get the best practice applied: drive to get the necessary enhancements into our operational tools  for example, Spinnaker, canary analysis  to seamlessly expose it to all teams or socialize the suggested change via an availability newsletter which is published monthly. In some cases, the suggested best practice is one of the tooling extensions previously incorporated into tooling. Both  of  these  are  methods  to  help  move  the  needle  on  availability  in  an  aggregate manner. Some changes that we incorporate into tooling might be referred to as a guardrail. A concrete example is an additional step in deployment added to Spinnaker that detects when someone attempts to remove all cluster capacity while still taking significant amounts of traffic. This guardrail helps call attention to a possibly dangerous action, which the engineer was unaware was risky. This is a method of “just in time” context. David: What about cases where somebody uses one of your tools like Hystrix in a way  that  is  a  perfectly  valid  way  to  use  it,  but  experience  has  shown  that  this  way yields negative results? It’s not the case that you want to change the tool because the tool  is  just  fine.  How  do  you  propagate  experience  like  this  through  the  org  once you’ve learned that lesson? It sounds like the availability reports would be one of the examples of that? Coburn: That’s right. The availability report is actually challenging to interpret. Once you receive it, you need to click in three to four strategic  and often hidden  places on the report just to get to actionable data. This was a fairly significant barrier to adop‐ tion, and so we created a four-minute video to function as an onboarding tutorial for report usage. When people get the report, it says please watch this four-minute video to understand how to use this report. It didn’t move the needle fully but definitely improved the actionability of the report for those who took the time to use it. David: Let’s talk about the limitations of context versus control. Do you think that a context-based  system  works  as  well  in  an  environment  where  the  product  and  the metrics for it are more complex than Netflix? Is it the simplicity of the Netflix prod‐ uct that makes it work so well for you? Coburn: First I would point out that in aggregate the Netflix service  in terms of its interacting parts  is as complex as you might find anywhere else. There are a number of factors that allow context to be more broadly effective at Netflix versus other com‐ panies. It ties directly to a lack of complexity in the engineering organization and the structure of our service. The following are some of the key factors:  10      Chapter 1: Context Versus Control in SRE     We have all the engineers sitting in one location. Common engineering princi‐ ples  are  easily  socialized  and  additional  tribal  knowledge  of  best  practices  is implicitly socialized and discussed.    We have one dominant version of the software running in a given region at a given  time.  We  additionally  have  control  over  that  software  stack  and  we  can change  it  whenever  we  want   not  shipped  to  customers,  with  the  exception  of device applications .    We have a service where people don’t die if it breaks.   We have a little bit of runway and we have an SRE team who is willing to take a  bunch of bullets if availability gets worse and say “my job is to fix it.”  For this last bullet point, it can be somewhat of a difficult situation for the reliability organization  to  say  they’re  responsible  for  improving  availability,  but  at  the  same time don’t have total control over the factors that impact availability. A lot of people aren’t  comfortable  with  saying  they’re  responsible  for  improving  something  they don’t necessarily have absolute control over. At a company where you ship a product to the field  for example, self-driving car software ,  this  model  might  not  work.  You  wouldn’t  want  to  wait  and  say,  “Wow, look, that car drove off the road. That was an error. Who wrote that? Well, let’s get it right next year.” In contrast, the space we operate in allows a fair amount of room to leverage  context  over  control,  as  we  don’t  put  lives  at  risk  and  can  make  changes quickly to the aggregate product environment. That said, I think you can see situa‐ tions when companies start to scale where they started with context but move over time to have a bit more control as warranted by the needs of the customer and relia‐ bility. You could generalize this perspective in that you will eventually move more toward the other end of the spectrum from which you started, if even only a little bit. David: In the past we’ve talked a bit about context versus control from an engineer‐ ing investment perspective. Can you say a bit more about this? Coburn: I think about four primary dimensions: rate of innovation, security, reliabil‐ ity, and efficiency  infrastructure operations . Depending on how your organization is structured, you could own one or more of the dimensions. Even if you don’t own a given  dimension,  the  decisions  you  make  in  your  own  space  can  have  significant impacts on the others. I’m in a situation where I own at least two, reliability and effi‐ ciency, because I also own capacity planning. When I think about how hard I want to push  on  teams  to  improve  those  dimensions  I  want  to  try  as  little  as  possible  to reduce drag on the others  rate of innovation, security . Keeping this model top-of- mind allows us as an SRE organization to be more thoughtful in our asks of other engineering teams.  Context Versus Control in SRE      11   Somewhat related, we also make explicit trade-offs in order to improve one dimen‐ sion  at  the  cost  of  another.  We  have  cases  where  we  run  significantly  inefficient  over-provisioned   for  a  given  microservice  for  the  purposes  of  increased  rate  of innovation or reliability. If we have a service that needs to run with twice as much headroom because of bursty traffic, I don’t care if that costs me an additional tens of thousands of dollars a year, because all I need is one big outage to quickly burn that amount of engineering effort time. David: OK, so how do you connect that back to context versus control? Coburn:  With  control  as  an  example,  let’s  say  I  actually  take  away  the  ability  for someone to push code because I need to increase this dimension of reliability. On the one hand it stops the near-term breakage, but it slows down innovation. The goal is to try to provide more context to avoid forcibly slowing down other dimensions and let the owner of a given service determine which dimension to optimize for, taking into  account  their  current  demands.  In  contrast,  control  has  a  much  more  binary effect on other dimensions. David: Does context have a positive or negative effect on any of those dimensions? Control clearly does, as you just mentioned, but what about context? Coburn: Context does have a strong upside and track record, at least in our case. In the past five years, our customer population has grown six times, our streaming has grown six times, our availability has improved every year, our number of teams has increased  every  year,  and  our  rate  of  innovation  has  gone  up  every  year.  That’s without  applying  control  to  improve  availability,  but  rather  predominantly  by improving  through  context  combined  with  improvement  of  the  platform  so  engi‐ neers spend less time working on operational aspects that shouldn’t be required in their role  such as determining how to configure pipelines to have staggered pushes across regions . David:  If  control  can  have  a  negative  effect  on  those  dimensions,  can  context  can have a similar adverse effect? Coburn: It can, but I have yet to see where the context we provide to teams results in an undesired behavior that actually harms availability. One area in which this can be a  risk  is  infrastructure  efficiency.  Although  we  provide  detailed  infrastructure  cost context  to  teams,  we  don’t  have  or  enforce  cloud  cost  budgets  on  teams.  Case  in point, engineers simply deploy whatever infrastructure they need and receive a cost report each month that provides them context about their cost. With this context one team  chose  to  try  and  optimize  efficiency  by  staying  on  an  older  and  less  reliable instance type, which was of course much less expensive. We then had an outage due to poorly performing and less reliable instances, often running underprovisioned. In this case, we provided cost context, the decision was to overindex on efficiency, and as a result reliability took a hit. We decided to socialize with teams that we would  12      Chapter 1: Context Versus Control in SRE   rather run less efficient than compromise reliability, aligned with our explicit prioriti‐ zation of domain concerns.  Coburn  Watson  is  currently  a  partner  in  the  Production  Infrastructure  Engineering organization at Microsoft. He recently concluded a six year adventure at Netflix where he  led  the  organization  responsible  for  site  reliability,  performance  engineering,  and cloud infrastructure. His 20+ years of experience in the technology domain spans sys‐ tems management through application development and large-scale site reliability and performance.  Context Versus Control in SRE      13    CHAPTER 2 Interviewing Site Reliability Engineers  Andrew Fong, Dropbox, Inc.  There are two phrases you continually hear about why projects are successful: “My engineers made it possible” or “My engineers did the impossible.” Much of this book is about technology and how to apply it to problems. That tech‐ nology is completely useless without the right people. This chapter is dedicated to the softer side of our jobs: hiring those right people. Every one of these hires will operate and build the technology that is used to solve problems, disrupt industries, and ach‐ ieve feats that no one thought were possible. It all begins with the engineer. Interviewing 101 Before we get into the specifics of SRE hiring, it is useful to build a common under‐ standing  of  how  interviewing  typically  works.  This  is  a  general  overview  based  on experiences working at companies such as AOL, Google, YouTube, and Dropbox. Who Is Involved Generally, there is a minimum of three parties involved: the candidate, the recruiter, and the hiring manager. Industry Versus University Usually, there are two types of candidate profiles that hiring managers recruit: indus‐ try and university  which includes masters and PhDs . Industry candidates are engi‐ neers  who  have  worked  in  similar  positions  at  other  companies.  University candidates  are  exactly  what  it  sounds  like—students.  Compared  to  recruiting  soft‐  15   ware  engineers,  recruiting  SREs  from  universities  is  significantly  harder  and  more specialized. The major reason for this is that you will first need to educate university students  on  what  SRE  is  before  they  will  even  apply  for  the  role.  In  this  book,  we focus exclusively on industry recruiting. Biases Before we go too deep into the interviewing content, it is important to recognize that we  are  all  humans  and  thus  have  biases.  We  do  not  cover  how  to  avoid  all  biases within this chapter. However, we do call out two specific ways to combat biases that organizations starting to build out SRE teams should seek to implement. The first is to do a blind résumé review, and the second is to standardize your hiring processes. Both of these are simple enough tweaks to existing processes or are simply best prac‐ tices for building a new interview process. Blind  résumé  review  means  that  you  are  attempting  to  remove  biases  from  the résumé  review  by  obfuscating  identifying  information  within  the  résumé.  You  will need to understand what matters to your organization and calibrate the blind review process accordingly. Helping you to standardize the hiring process is a goal of this chapter. By the end of this chapter you will have learned a systematic way of evaluating candidates, both at scale as well as for a limited number of positions. At the end of this chapter, there are links to further reading on the topic of conscious and unconscious biases. The Funnel Most companies think of hiring as a funnel that has stages through which candidates pass until they either fall out at a given stage or receive an offer. Figure 2-1 illustrates what a typical funnel looks like.  16      Chapter 2: Interviewing Site Reliability Engineers   Figure 2-1. The hiring funnel  Let’s take a look at the individual steps involved:  1. Pre-interview chats 2. Recruiter screen 3. Phone screen 4. Onsite interview 5. Evaluation 6. Take-home questions 7. Additional evaluation 8. Reference checks 9. Selling candidates 10. Offer out 11. Offer accepted  Each of these stages serves a unique purpose. Measuring the stages of the funnel will help you to refine the process and hire world-class candidates. What and how you measure is a topic for another book. What’s important here is that you know the fun‐ nel exists and some of its basic stages.  Interviewing 101      17   SRE Funnels Now  that  you  know  what  a  funnel  is,  there  are  three  major  areas  that  are  crafted specifically for hiring SREs: the phone screen, onsite interviews, and take-home ques‐ tions. This section lays out the purpose of each area and what technical and cultural areas it covers. The content of the interview and stages will differ between companies and is not meant to be a prescriptive formula to recruiting. As with all areas related to talent  within  your  organization,  you  should  apply  the  lens  that  your  organization requires. Phone Screens Before  you  bring  a  candidate  on  site,  it  is  important  to  get  some  basic  signal  with respect to motivation, technical aptitude, and general experience. The phone screen is done at the top of the funnel because it can help filter out candidates who will not make it through the interview. This prevents incurring a high expense at later stages, in the candidate’s time as well as your engineering team’s time. Also, you can use the phone screen to touch on a variety of topics to help form a hypothesis of where this engineer would fit or would not fit into your organization. If you are unable to for‐ mulate this hypothesis, you might want to walk away after the phone screen.  Conducting a phone screen A good format that has been tested at multiple companies is to have phone-screen questions that are specific and concrete enough that the candidate is able to get to a solution within 20 minutes. For example, having a coding and a process or trouble‐ shooting question allows you to determine someone’s technical breadth and experi‐ ence. Coding, which is pragmatic and simple enough to do in a shared online coding tool, works well and usually results in reasonable signal regarding the candidate’s technical depth. Troubleshooting or process questions should aim at understanding how the candi‐ date reasons through problems that do not involve code. A simple rule of thumb can be  aiming  for  two  20-minute  questions  along  with  a  10-  to  15-minute  discussion about the role, company, and expectations; this combination typically fills a one-hour phone screen. You will want to have a few phone-screen questions in your tool chest and be willing to  pivot  depending  on  the  candidate’s  experience.  Having  an  easy  question  that allows you to keep the conversation going is perfectly fine at this stage. It’s important that the candidate walks away from the conversation with a positive view of your organization, even if you are not moving on to the next stage.  18      Chapter 2: Interviewing Site Reliability Engineers   The Onsite Interview Before you bring a candidate into your office to interview, you should formulate an explicit hypothesis around the candidate. This allows you to do the following:    Set expectations for the interview panel   Fully understand what your organization is looking to hire   Craft an interview loop that will fairly evaluate the candidate  A good hypothesis will include the expected seniority, growth path, and leadership skill set, which you can validate via the interview process. For example:  The systems engineering team, which works on Linux automation, is looking to hire a senior engineer with strong distributed systems knowledge that will grow into a tech‐ nical leader, either formal or informal, of a team of mostly junior engineers.  Teasing this apart, you can identify what the team is looking for in a candidate:    Senior engineer   Systems knowledge   Distributed systems   Technical leadership   Mentorship  Taking this data and combining it with other baseline requirements that you might have, such as coding, you can create an interview loop that accurately evaluates the candidate for the role. In this case, to evaluate these skills, you want to understand the  candidate’s  working  knowledge  of  Linux,  how  that  person  operates  distributed systems, and how well they can mentor and set technical direction on a team as well as whether the candidate possesses the baseline coding requirement. A good loop for a candidate that is being evaluated for that hypothesis would include the following:    Two medium-to-difficult coding questions that involve understanding of system  designs    Deep dive or architecture   A focused interview on leadership, mentorship, and working with others  SRE Funnels      19   Coding and system questions Good questions that can test coding and systems tend to be those that are real world and are bounded by your organization’s size against a scaling factor of 10. For exam‐ ple, consider the following question:  Design and code a system that can distribute a package in parallel to N servers.  You want to select an N that is realistic for the scenario that you expect the candidate to  face.  If  your  organization  has  100  servers,  it’s  realistic  to  expect  it  will  grow  to 1,000 but not 100,000. You will expect the SRE to solve the problem at hand when on the job, so ask the question in a way that is realistic for the working environment.  Deep dives and architecture questions Deep dives and architecture test similar attributes of a candidate—namely, their abil‐ ity to reason about technical trade-offs. However, you will want to set up the inter‐ views in a slightly different way. Deep dives tend to work better when you ask the candidate to select the space ahead of time. This allows you to place an engineer with the candidate who has thought about the problem ahead of time. Architecture ques‐ tions do not require this advance preparation. However, similar to the coding ques‐ tion, you want to set bounding parameters. An example of a bounding parameter can be to specify an initial scale and then increase the scale at various stages. By starting small, you can assess attributes of the candidate, such as their bias toward simple ver‐ sus complex solutions as well as how they think about trade-offs at each stage of the problem.  Cultural interviews All cultural fit interviews are a way of ascertaining how well a candidate will fit into your organization’s cultural value system. This is not a “would I like to hang out with this person?” interview. Rather, it is a properly crafted set of questions that dig into the way an engineer works with others, whether the person focuses on the right areas, and whether they are able to articulate problems. Cultural fit can even offset technical weaknesses depending on the size and structure of your organization. For example, an extremely talented engineer who cannot oper‐ ate in a large company might be a pass, whereas someone who understands organiza‐ tional leverage but has small technical gaps might be much more valuable at scale. You can determine this fit by posing questions that dig into the candidate’s working style, such as asking for examples of how the person approached a certain project and what reflections and learnings they took from the experience. Good candidates will typically be able to articulate the “why’s” behind their thought process and what they would do differently given your environment.  20      Chapter 2: Interviewing Site Reliability Engineers   Take-Home Questions Occasionally, you will have a scenario in which you want to get additional technical signal after you have looked at the interview feedback. You can choose to have the candidate return for another technical interview or use a take-home question. Take- home  questions  have  positives  and  negatives.  Let’s  explore  both  before  examining what makes a good take-home question. On the positive side, take-home questions allow a candidate to fully showcase their ability to write software in a more real-world scenario. The candidate has access to their own development environment, access to their favorite search engine, and the ability to pace themselves. All of this results in much higher signal, including aspects around collaboration via clarifying questions, if needed, with the proctor. However, take-home questions are not without drawbacks. The major disadvantages of take-home questions involve the time commitment required of both the candidate and an engineer from your company. A typical scenario requires a three-day window for both parties to be available and will delay the potential offer. You should be very judicious in the use of a take-home question. Good take-home questions should be built in a way that can provide signal for the area or areas for which you lacked signal from the initial interviews. At this stage of the funnel, you will have very few candidates, and it will also be hard to calibrate your organization on the questions. To counteract this, having another hypothesis on what the take-home will validate is very important. It is equally important to not discount the onsite interview if parts of the take-home are not on par with the onsite. Remem‐ ber, you are looking to validate something you did not get signal on during the initial onsite interview. Advice for Hiring Managers The hiring manager, or the person responsible for staffing the role, has three major responsibilities during the interview process:    Building the hypothesis  covered in “The Onsite Interview” on page 19    Convincing or selling the candidate to join your organization   Knowing when to walk away from a candidate  Selling candidates From the first minute you meet and talk to a candidate, you should be selling them on your organization. How you do this will vary from person to person. You want to know the answer to three questions in order to to effectively sell a candidate:  SRE Funnels      21     What motivates the candidate?   What is their value system?   What type of environment brings out their strengths?  By understanding these dimensions, you can craft a compelling narrative for a candi‐ date to join your organization.  Walking away Walking away from a candidate is one of the hardest things to do during the process. You, and potentially your team and organization, will have made a time and emo‐ tional investment into a candidate. Walking away is also something that can occur at any stage, not just after you decide to pass because of interview performance. The core reason for walking away from a candidate comes down to organizational fit. Each time you learn something new about a candidate is an opportunity to reevalu‐ ate. Just as in engineering, the hiring process is about trade-offs. A perfect candidate might demand a salary that you cannot afford, or they might have the perfect techni‐ cal skills but none of the leadership skills you are looking for. Walking away is hard, but know that making the wrong hire is almost always worse in the long run. Opti‐ mize your pipeline to avoid false positives. If you are willing to take risks, be explicit and have a strong performance management program in place. Final Thoughts on Interviewing SREs People form the backbone of your organization, and you should use your interview‐ ing process to ensure that you have found the very best people for your organization. Be  prepared  to  take  risks  and  place  bets  on  people  just  as  you  would  if  they  were internal to your organization, because you will never know everything you want to know in such a short time with a candidate. This is an imperfect process attempting to get a perfect output, and thus you will need to iterate and be open-minded as well as take risks. At the end of the day, even if you decide to not make an offer based on the interview performance, make sure that the candidate walks away feeling the process was fair and they had a good experience. Wouldn’t that be what you would want if you were in their shoes?  22      Chapter 2: Interviewing Site Reliability Engineers   Further Reading   Hiring Site Reliability Engineers   7 Practical Ways to Reduce Bias in Your Hiring Process   Hiring Your First SRE  Andrew Fong is an engineering director at Dropbox. He is also one of the first members of the SRECon steering committee and helped cochair the inaugural conferences. He has spent his career in infrastructure at companies such as AOL, YouTube Google, and Dropbox.  Further Reading      23    CHAPTER 3 So, You Want to Build an SRE Team?  Luke Stone, Google  This  chapter  is  for  leaders  in  organizations  that  are  not  practicing  Site  Reliability Engineering   SRE .  It’s  for  the  IT  managers  who  are  facing  disruption  from  the cloud. It’s for operations directors who are dealing with more and more complexity every day. It’s for CTOs who are building new technical capabilities. You might be thinking about building an SRE team. Is it a good fit for your organization? I want to help you decide. Since 2014, I’ve worked in Google Cloud Platform, and I’ve met at least a hundred leaders in this situation. My first job at Google was before SRE existed, and today I work with cloud customers who are applying SRE in their organizations. I’ve seen SRE teams grow from seeds in many environments. I’ve also seen teams that strug‐ gled to find energy and nutrition. So, let’s assume that you want to build an SRE team. Why? I’ve heard some themes that come up over and over. These made-up quotes illustrate the themes:    “My friends think SRE is cool and it would look nifty on my résumé.”   “I got yelled at the last time we had an outage.”   “My organization wants more predictable reliability and is willing to pay for it.”  They  are  cheeky  distillations  of  more  nuanced  situations,  but  they  might  resonate with you. Each one points to several opportunities and pitfalls. Understanding them will help you to figure out how SRE would fit into your organization. Let’s take a look at the real meaning of each one.  25   Choose SRE for the Right Reasons  “My friends think SRE is cool and it would look nifty on my résumé.”  The first point is about avoiding misconceptions: your organization must understand SRE at a deeper level. If SRE seems cool to you, congratulations on your excellent taste! It is pretty cool to take something as theoretical as an error budget and apply it to a real-world prob‐ lem…and to see results. It’s science! It’s engineering! However,  in  the  long  run,  SRE  is  not  going  to  thrive  in  your  organization  based purely on its current popularity. Every outage will be a test of the team’s ability to deliver real value. The bad news is that it takes a lot of work to build trust around a team that is highly visible when things go disastrously wrong. The good news is that SRE is well designed, detailed, and tested in the real world. It can be the backbone of your plan to achieve the reliability that you need. So, you can’t count on the coolness to take you very far. A well-functioning SRE team has a culture that develops over time, and it’s going to be a long-term, high-effort project to start a team in your organization. We’re talking months to years. If this is not your idea of fun, you can still get some quick results by adopting a few SRE prac‐ tices. For example, you can practice consistently writing postmortems and reviewing them with your team. This can be useful, but don’t expect it to have the same impact as adopting the full set of SRE principles. Similarly, don’t expect to have impact from your SRE team if it is not practicing the SRE approach to production engineering all day, every day. It’s tempting to slap the SRE label on an existing team, change a few job titles, and claim that you’ve taken the first steps. You can probably guess that this is unlikely to succeed because it doesn’t actually change anything about the systems themselves. An SRE team is fundamen‐ tally  a  team  of  software  engineers  who  are  pointed  at  the  reliability  problem,  so  if your  organization  doesn’t  sustain  software  engineers,  it’s  not  right  for  SRE.  If  you have software engineers but can’t focus their energy on an SRE approach to produc‐ tion reliability, you’ll have another flavor of “SRE in name only.” An effective SRE team requires a certain environment. There must be support from the ground up, also from the sides and above. You’ll need to get buy-in from the SRE team  members  themselves,  their  peers   development  teams  and  other  SRE  teams , and management. The team will not thrive in isolation. It needs to be integrated into your company’s technical core. Ease of communication matters, so consider physical colocation with development teams or investing in high-quality video conferencing. SRE needs to be at the table for high-level decisions about reliability needs and levels of investment in people and redundant infrastructure.  26      Chapter 3: So, You Want to Build an SRE Team?   The members of the team must be dedicated and trustworthy to one another. They must be skilled at automation and technical troubleshooting. They must possess the generosity necessary to continually build one another up because SRE is really a team effort  and  there  is  a  saying:  “No  heroes.”  Avoid  teams  that  have  been  scraped together  from  far-flung  individuals.  It  might  be  better  to  repurpose  a  well- functioning team to bootstrap your SRE team. SRE teams interact with other teams, particularly development teams, so these need to be bought in, too. For example, it’s hard to create a blameless postmortem culture if  developers  or  managers  are  pointing  fingers.  When  paired  with  an  SRE  team,  a development  team  should  put  something  of  value  into  the  partnership.  Here  are some examples:    Fund the team   Agree to take some on-call shifts   Give SRE a mechanism to prioritize SRE needs   Dedicate the best software engineers to the SRE team  Here’s a recipe for disaster: when the dev team expects the SRE team to “just fix it.” SRE should be empowered to ensure that the software can be operated reliably, and the dev team should be interested in collaborating toward that goal. The developers should be expected to actively participate in production operations—for example, by pairing with SREs or taking a small slice of the SRE team’s operational duties. This will make them aware of the value of the SRE team and also gives some firsthand data about what it’s like to operate the system. It’s also very important for business con‐ tinuity in case the SRE team needs to “hand back the pager” and stop supporting the system. If the developers cannot or will not take responsibility for some portion of the production engineering tasks, SRE is not going to work. SRE thrives in an environment of open, effective communication. It will be very diffi‐ cult if the SRE team is excluded from the conversations about the future of the prod‐ uct or service. This can be unintentional friction, for example if the SRE team is in a remote location, works in a different time zone, or uses another language  spoken or programming language, that is . It’s even worse if the SRE team is local but excluded from planning or social events. But wait, I thought SRE was obviously cool! I mean, look at SREcon attendance grow‐ ing  every  year,  and  the  SRE  book’s  a  best-seller.  At  Google,  SREs  wear  awesome bomber jackets and they have a logo with a dragon on it. Super cool! If you want to make SRE successful in your organization, it definitely helps to make it cool. That doesn’t mean it needs to be elitist or exclusive. There will be a tendency for other teams to reject SRE if they don’t understand it, so it’s very important to get all technical  leaders  to  introduce  the  concepts  and  the  people  into  their  teams.  You  Choose SRE for the Right Reasons      27   might be willing to try a rotation program between dev and SRE, or a policy whereby people can freely transfer between dev and SRE teams. It’s about building personal relationships and treating SRE as equals. At a higher level, the executive leadership must be willing to back up the SRE team when it is acting according to its principles, even when the world appears to be on fire. Leadership must defer to SREs who are managing a live incident. They must be willing to sign off on Service-Level Objectives  SLOs , meaning that they are taking responsibility for defending some amount of downtime as acceptable. They must be willing to enforce error budgets, meaning enforcing consequences like feature freezes when budgets are exceeded. Orienting to a Data-Driven Approach  “I got yelled at the last time we had an outage.”  This point is about recognizing that an organization is compatible with SRE only if it’s driven by facts and data. My grandfather was not an SRE. He was a truck driver and an entrepreneur. At one point in the 1950s he invested in a novel gadget: an alarm clock that turns off when the user screams. Very effective for getting everyone up in the morning. It’s the only machine I’ve ever seen that does the right thing when you yell at it. If  your  organization  reacts  to  engineering  problems  by  applying  emotion,  it’s  not ready for SRE. People will need to keep cool heads in order to get to the bottom of increasingly  difficult  reliability  problems.  When  people  feel  threatened,  they  are motivated to hide information, and SRE requires an open and collaborative atmos‐ phere. For example, some teams give awards, not punishment, to people who take responsibility for causing an outage, if they follow up with an appropriate incident response and postmortem. Be alert for signs of maturity like focus on long-term benefits, incremental improve‐ ment,  and  acknowledgment  of  existing  problems.  If  people  can  face  problems,  a blameless  postmortem  culture  can  take  root.  If  they  want  to  measure  the  pace  of improvement, SLOs and error budgets will work. If they are willing to invest, auto‐ mation will thrive and toil melts away. A good sign is the use of objective metrics to drive business discussions. People agree ahead of time on the desired, measurable outcomes, and then periodic reviews check the numbers and adjust as necessary. If you have seen a scenario in which someone’s review was full of bad news, and the group’s reaction was to offer help, the culture might be right for SRE. Another good sign of maturity is when the organization can reward “landings”  e.g., automation in production providing measurable savings  instead of “launches”  like  28      Chapter 3: So, You Want to Build an SRE Team?   a  new,  shiny  tool .  Do  you  reward  significant  clean-ups  and  grunge  work  that improves maintainability and operational effectiveness? SRE relies on a fact-based approach, like hard science. It can take time to get some difficult  facts,  like  the  root  cause  of  an  outage  or  an  analysis  of  when  the  current capacity will be inadequate. It’s very important that these facts get the respect they deserve. Look for people who are patient about gathering the facts. Look for people who question assumptions and demand hard evidence. Avoid people who don’t have the focus or the interest to get to the bottom of a complex technical situation. People  across  your  organization  will  need  to  put  their  trust  in  the  SRE  process  as their chosen tool for managing reliability. They will need to stick to the process and live  with  the  reliability  goals  that  they  originally  agreed  to.  An  SLO  and  an  error budget  are  like  a  contract  with  performance  targets  and  consequences.  Sticking  to this contract in the face of an incident requires a cool head. It needs to be OK for SRE to say, “errors increased, but stayed within SLO, so this is not an emergency.” Commitment to SRE  “My organization wants more predictable reliability and is willing to pay for it.”  This brings us to the last point: your organization must be willing to invest the atten‐ tion and resources required to sustain an SRE team. SRE can be expensive. You’ll want to be sure about what you’re getting from it. A sustainable  SRE  team  needs  enough  people  to  cover  on-call  duties  without  getting burned out, and enough people to have cycles for automation, tooling, and other pro‐ duction engineering projects. They will also need enough people to respond to the service’s myriad operational needs that come up. If the team is understaffed, even by a little, the most talented people will leave and you won’t achieve your goal. There is a minimum team size necessary to carry SRE responsibilities for a system. This doesn’t make sense for smaller systems, so some things will be out of scope for the SRE team. They are not expected to solve everyone’s problems. Many  people  think  that  SRE  is  about  continually  increasing  reliability,  forever, asymptotically approaching 100%. That could be your goal, but at some point, you’ll get diminishing returns on your resources. SRE is really about managing your relia‐ bility so that it doesn’t get in the way of other business goals. The organization must be  able  to  accept  reliability  under  100%—because  no  systems  are  perfect.  Some amount of downtime must be anticipated, accepted, and even embraced. Reluctance to face this reality is the single most significant barrier to adopting SRE. What goals would be more important than reliability? When technology is a differen‐ tiator to the business. Where innovation is important. In these cases, you might want a product development team going as fast as possible—and it would be worthwhile to  Commitment to SRE      29   have an SRE team providing some reliability guardrails. For example, SRE could pro‐ vide a deployment system with canary servers and easy rollbacks. SRE can fit into your organization if you have enough discipline to trust the process. At first, it seems like SRE establishes another set of constraints. But, actually, people gain freedom when they adopt practices like SLOs and blameless postmortems. An SLO is a codification of clear expectations. It allows local optimizations and empow‐ ers people to reason about the reliability impacts of their decisions. A blameless post‐ mortem is explicit permission to point out things that went wrong. It allows you to begin to address the root cause of your reliability issues. These are powerful tools. Like  all  good  tools,  they  require  training  and  maintenance.  And,  of  course,  they should be used according to their design and best purpose. Making a Decision About SRE I hope this chapter has given you some perspective on whether SRE is a good fit for your organization. The steps for actually starting the team can be covered in another chapter, or perhaps another book! If you’re still not sure whether SRE is right for you, here are some ideas for gathering more information:    Find  some  influential  people  in  your  organization   leaders  and  old-timers,  for example  and have them read the first few chapters of the original SRE book. Do the ideas resonate? Does SRE seem realistic?    Brainstorm  some  SLOs  for  your  service.  Test  them  with  the  people  who  are responsible  for  the  user  experience.  Play  “what  if”  with  a  few  scenarios,  like unplanned outages of various durations, planned maintenance for a feature, and degraded service.    Try to think of a mature, important system with which to start. Are there clear  reliability metrics? Is it clear who would decide on the SLOs and error budget?    Imagine  that  you’ve  built  an  SRE  team.  How  do  you  tell  if  it’s  working  well? More reliability with fewer people? At greater release velocity? Something that’s measurable.  You are now well armed with questions to help you evaluate the possibility of SRE in your organization. Be sure to include your colleagues and stakeholders while you are gathering data and deliberating. This decision is too big to be taken by one person. Working together, you can make a wise decision on starting your SRE team. Good luck!  30      Chapter 3: So, You Want to Build an SRE Team?   Luke Stone joined Google in 2002 as the first technical support engineer for AdSense. He witnessed SRE’s impact on Google from the beginning and joined Google SRE as a founding member of the Customer Reliability Engineering team. Before Google, Luke was a system administrator and developer in academic and nonprofit organizations, and studied computer science at Stanford.  Making a Decision About SRE      31    CHAPTER 4 Using Incident Metrics to Improve SRE at Scale  Martin Check, Microsoft  Whether your service is looking to add its next dozen users or its next billion users, sooner or later you’ll end up in a conversation about how much to invest in which areas to stay reliable as the service scales up. In this chapter, we take a look how to use incident metrics to focus investments by way of a case study from Microsoft Azure. It applies lessons we’ve learned working on service reliability on a variety of services, ranging from startups to enterprise services all the way to cloud scale. Azure makes a particularly good case study, as the tremendous scale, growth, and diversity of prod‐ uct  offerings  amplify  the  typical  reliability  themes.  We  show  how  using  data  and some innovative techniques to analyze and report on these themes helped us to drive improvements. The Virtuous Cycle to the Rescue: If You Don’t Measure It… As with any problem management effort, we began by looking at the data. However, when we went to do so, it turned out that we had thousands of data sources, service telemetry, incident management metrics, deployment metrics, and on and on. In fact, we had so many data sources to look at that it made deciding what data to look at and in which order to tackle the problems tricky. After looking at the best practices in the industry and consulting with experts, we ultimately landed on a system called the vir‐ tuous cycle, shown in Figure 4-1, to underpin our improvement efforts. The virtuous cycle created a framework that we could use to see how effective our monitoring was by  how  quickly  we  detected  outages,  how  well  we  were  learning  from  outages  by measuring the root-cause analysis  RCA  process and repairs, and how quickly those  33   bugs  were  getting  fixed.  We  could  then  look  at  our  code  quality  and  deployment velocity to see how quickly we’d run through the full cycle.  Figure 4-1. The virtuous cycle  As SREs, we know that every minute of downtime matters, so we began by finding the  key  metrics  that  told  us  how  effective  we  were  at  responding  to  and  repairing incidents. This meant that first we had to define the metrics that were representative and then get agreement on definitions and start end times. Let’s dive into the metrics that we picked and why we thought they were so important: Time to Detect  TTD   Time to Detect is the time from the impact start to the time that an incident is visible to an operator. We start the counter when impact first becomes visible to the customer, even if our monitoring didn’t detect it. This is often the same as the time that the Service-Level Agreement  SLA  is breached. Believe  it  or  not,  TTD  is  the  most  important  metric  for  incidents  that  require manual mitigation action. This measure determines the quality and accuracy of your monitoring. If you don’t know about your customer pain, you can’t begin the process of recovery, and you certainly can’t actuate automation to respond or mitigate. Maybe even more important, you can’t communicate to your customers that you know about the problem and are working on it. The challenge with TTD is  balancing  the  monitoring  sensitivity  such  that  you  find  all  of  the  customer issues quickly and accurately, without constantly interrupting your engineers for issues that don’t impact customers.  Time to Engage  TTE   This is the time from detection until the appropriate engineer is engaged. This can be difficult to determine during the event, and sometimes even afterward. It  34      Chapter 4: Using Incident Metrics to Improve SRE at Scale   can be hard to look back through the fog of war to pin this down on a single engineer,  so  it’s  OK  to  approximate  with  the  first  engineer  on  the  scene.  This metric  is  very  important  to  look  at  how  effectively  we’re  able  to  mobilize  the response, and accounts for both the triage time  determining severity and owner‐ ship  as well as the time to escalate and mobilize the responders. There are a lot of  ways  in  which  you  can  improve  this;  automated  escalation  and  paging  sys‐ tems,  clear  expectations  for  on-call,  follow-the-sun  support  models,  and  even improved monitoring can help ensure the alert goes to the right on-call engineer the first time. Time to Fix  TTF   This is the time that it takes the responder to mitigate the issue. All of these metrics, when added together  TTD + TTE + TTF  are represented by  Time  to  Mitigate   TTM ,  the  full-cycle  time  through  the  outage,  as  repre‐ sented in Figure 4-2.  Figure 4-2. Example of an outage mitigation time breakdown  You might have different metrics, definitions, or thresholds, but all that matters is that your group agrees on the common taxonomy and measures. The agreement on taxonomy  is  particularly  important  because  if  the  mitigation  event  is  not  agreed upon, we can have disconnects, as some teams could try to disengage before the inci‐ dent is fully resolved. These become especially critical after the incident to ensure a common taxonomy during the incident review meetings to talk about where there are opportunities for improvements.  The Virtuous Cycle to the Rescue: If You Don’t Measure It…      35   Metrics Review: If a Metric Falls in the Forest… After we had these metrics defined, we brought our engineering leaders together to look at the key metrics that we identified as crucial to drive the virtuous cycle. We then could track how we’re progressing and derive insights and create action plans in areas where we weren’t hitting our goals. After defining and agreeing on metrics, we began  collecting  and  reporting  on  per-service  aggregates  of  the  data  to  determine how we’re doing, find areas and common themes for improvement, and measure the impact of improvements we made. Figure 4-3 shows an example of a dashboard to measure incident and deployment metrics. This allowed us to track metrics trends for our incident response cycle and engineer improvements just the way we engineer fea‐ tures into the product.  Figure 4-3. SRE metrics dashboard  Notice that the incident response metrics we discussed earlier show up here: TTD, TTE, TTF, and TTM, trending by time period, and measured against goals that we had set and agreed on with service owners. If we found data was sparse, had high var‐ iability, or had major outliers, we would apply percentiles to the data to normalize it enough. We then could look at the outliers to understand them better and drive the percentiles toward 100%. Surrogate Metrics If  you  look  closely  at  the  SRE  metrics  dashboard,  you’ll  notice  a  few  metrics  like Directly Responsible Individual  DRI  hops  how many on-call engineers are needed to resolve an incident  and autodetection  how many incidents are detected via mon‐  36      Chapter 4: Using Incident Metrics to Improve SRE at Scale   itoring . These are surrogate metrics, which are submetrics related to the top-level “Time to” metric that are more specific and actionable than the high-level metrics, but don’t by themselves indicate success. We found that using surrogate metrics led to faster, more durable improvements because giving engineering managers specific action items and submetrics was a more effective way to drive action than just telling teams to “do better” and “try harder.” Exploring your data is a great way to find surrogate metrics. Looking at TTE as an example, as we investigated incidents with long engagement times, we found factors that correlated with or led to high engagement times, such as engaging many engi‐ neers to resolve a single incident. This would occur due to knowledge gaps, instru‐ mentation gaps, or even inconsistent expectations for on-call response. To address this,  we  added  the  “DRIs  engaged  per  bridge”  submetric,  which  lets  us  see  how many DRIs are engaged on any given incident. Having fewer DRIs engaged could still result  in  a  long  response  time,  especially  if  they  don’t  engage  additional  resources when they should. However, when taken together with the TTD and TTE, this is a good indication of how effective our monitoring and diagnostics are at getting the alert to the right responder early. Similarly,  while  working  to  improve  TTD,  we  noticed  that  it  was  10  times  higher when the outage was detected by our customers instead of being caught by monitor‐ ing. To measure this, we instrumented the autodetect rate as a surrogate metric for TTD. This doesn’t mean that all autodetect incidents have good TTD, but automated detection is required to get good TTD. As is typical for surrogate metrics, automated detection is necessary, but not sufficient to achieve world-class TTD. This is not a complete list of surrogate metrics, but just a few examples to get you started. Repair Debt Some of the best insight we derived during the metrics review came from the post- incident review process. There is a lot of great material out there already, so I’m not going to go deep into how to do a good postmortem  see “Further Reading” on page 41 for a few examples . I’ll skip to what matters most to our metric reviews: every time we  identify  a  bug  or  improvement  opportunity,  we  log  it  and  track  it  as  a  repair. Repair items are technology or process fixes that either prevent an outage from recur‐ ring or reduce its duration. Typically, these are broken down into short-term items and long-term items. Short-term items should be rolled out quickly  within a week  and might be a process, a script, or a hotfix. Long-term items are more durable fixes, such as more thorough code fixes  i.e., fixing a class of issue versus an instance of an issue, or fixing a class of issue across multiple product lines , creating broader process change  i.e., building and delivering incident management training across multiple organizations , or building tooling like chat bots or autoescalation mitigation. Repair  Repair Debt      37   items are typically tracked in the same work management system you use for tracking product work items, but what matters is that they are recorded and reportable and distinguishable from standard product backlog. Tracking repair items lets us incorporate operational debt into the standard engineer‐ ing process and treat it just like feature work. The diagram in Figure 4-4 is a good example of what happens when we first began tracking repairs. There is usually an initial spike in debt as we start to expose previously unknown or unfocused repair actions. Then, the team adjusts its practices to incorporate repair debt into its normal rhythm of business, and the repair debt comes down. Tracking repair debt is impor‐ tant because these problems always existed, but until they were tracked, measured, and shared, the engineering team couldn’t take action to improve the service. This helps to provide a signal for the team, together with the error budget, for how to pri‐ oritize service reliability work against feature work.  Figure 4-4. An example repair debt graph Virtual Repair Debt: Exorcising the Ghost in the Machine Not every movie has a Hollywood ending, and we found that not every service saw the fruits of repair item rigor. In some cases, even though repair debt was stable, the reliability month over month wasn’t necessarily getting better. For these services that had stable repair debt but weren’t seeing reliability improvements, we were baffled. Why weren’t they improving if the repair debt was stable and appeared well atten‐ ded? We dug deep into some of the data that we hadn’t yet curated, did our best Sher‐ lock Holmes impression, and found a surprising insight. Some services weren’t doing a thorough RCA, and as a result they had either low RCA completion rates or RCAs without enough repairs. This meant that the repair items never made it into the back‐ log and the service didn’t have the opportunity to improve.  38      Chapter 4: Using Incident Metrics to Improve SRE at Scale   This brought about a new challenge: how do we measure the quality of a postmor‐ tem? Not only do we need to measure whether repair items were getting fixed, we also need to measure whether they were getting created in the first place. Most post‐ mortems include a lot of text describing the problem and what was done to mitigate. We  could  certainly  apply  some  machine  learning  to  the  text  and  try  to  parse  out intent, but that would take a significant investment and would not be deterministic. The  simplest  solution  was  sitting  right  in  front  of  us  all  along  in  the  form  of  the “Time to” metrics that we attach to every incident and every postmortem. Any inci‐ dent that missed a time to detect, engage, or mitigate goal should have a correspond‐ ing repair item. This meant that we had to have a taxonomy in the repair process to attach engagement, diagnosis, and detection flags so that we could programmatically extract “missed repair items.” We then used all of the missed repair items to measure what we called “virtual repair debt.” Visualizing  virtual  repair  debt  became  a  powerful  tool  to  drive  improvements.  As seen in Figure 4-5, the gray line representing repair debt makes it look like the team is keeping up with the repair debt, but when you add the dotted red line that represents missed repairs, the dark-matter repair items that form virtual debt become glaringly obvious.  Virtual  debt  is  particularly  important  because  it  represents  the  corpus  of repair  items  that  were  never  logged  and  will  end  up  hurting  the  service  down  the road. When there is virtual debt, the specific TTD and TTM misses will repeat over and over until they are logged and fixed.  Virtual Repair Debt: Exorcising the Ghost in the Machine      39   Figure 4-5. Repair virtual debt graph Real-Time Dashboards: The Bread and Butter of SRE Possibly  the  most  important  part  of  metrics  review  is  bringing  the  metrics  and insights  into  real-time  dashboards.  Looking  at  the  data  monthly  or  even  weekly doesn’t help drive change quickly enough. Every service, every component needs to be able to see in real time where they have work to do, where they are doing well, and where they can improve. This meant creating dashboards that can pivot by service, by manager, even down to the individual engineer who owns the work item. Learnings: TL;DR If you want to simplify everything from this chapter into a sentence, here it is: meas‐ ure everything, be relentlessly curious, and don’t be afraid to get dirty and wallow in your  data  to  find  the  right  actions  to  take.  In  many  cases,  getting  these  insights required hand curating a fair bit of data, but after we understood which metrics mat‐ tered, we could then instrument and automate them and help bring visibility to met‐ rics that could help the services get better.  40      Chapter 4: Using Incident Metrics to Improve SRE at Scale   Further Reading Blameless postmortems:    “Blameless PostMortems and a Just Culture”: John Allspaw, Etsy   “Postmortem Action Items: Plan the Work and Work the Plan”: Sue Lueder and  Betsy Beyer, Google    Beyond Blame—Learning from Failure and Success: Dave Zwieback  Using data to derive operational insights:    “Improving  Operations  Using  Data  Analytics”:  Parviz  Deyhim  and  Arti  Garg,    “Incident Analysis”: Sue Lueder, Google   “Measuring  the  Success  of  Incident  Management  at  Atlassian”:  Gerry  Millar,  Datapipe  Atlassian    “PDA: A Tool for Automated Problem Determination”: Hai Huang, Raymond Jennings III, Yaoping Ruan, Ramendra Sahoo, Sambit Sahu, and Anees Shaikh, IBM T.J. Watson Research Center  Martin Check is a site reliability engineering manager on the Microsoft Azure team. He has worked on large-scale services at Microsoft for 14 years in a variety of roles, includ‐ ing service design and implementation, crisis response, problem management, and even leading teams through the DevOps SRE transition. Martin is currently working as an SRE  manager  for  global  SRE  teams,  where  he  continues  to  leverage  data  insights  to drive SRE improvements.  Further Reading      41    CHAPTER 5 Working with Third Parties Shouldn’t Suck  Jonathan Mercereau, traffiq corp.  formerly LinkedIn   Over the years, the definition of Site Reliability Engineering  SRE  has evolved, but the  easiest  to  digest  is  subjectively  “what  happens  when  software  engineering  is tasked with what used to be called ‘operations.’”1 Most Site Reliability teams consider operations as the applications running on their own infrastructure. These days, more and more companies rely on third parties to serve a very specific function in which they  specialize.  This  includes  things  like  Domain  Name  System   DNS ,  Content Delivery  Network   CDN ,  Application  Performance  Management   APM ,  Storage, Payments, Email, Messaging  SMS , Security  such as Single Sign-On [SSO] or Two- Factor Authentication [2FA] , Log Processing, and more. Any one of these resources, if  not  implemented  properly,  is  a  dependency  that  has  the  capacity  to  bring  down your site. Are  vendors  black  boxes  that  we  don’t  control?  Not  necessarily.  As  we  approach working with vendors, it’s important that we apply the same suite of SRE disciplines to working with third parties in an effort to make it suck less. Build, Buy, or Adopt? Before we dive into the topic of working with vendors, we should discuss the deci‐ sions that would lead us to buy over build or adopt. Our level of involvement in this process will depend on the combination of importance and stakeholders. Determin‐ ing importance is the first step in this entire process and will dictate the significance of other deciding factors, such as weight of cost, the weight of support, influencers, Service-Level Agreements  SLAs , and more.  1 Site Reliability Engineering, Introduction.  43   Establish Importance Determining importance can be challenging for an SRE leading a project or integra‐ tion.  For  instance,  if  the  decision  is  on  which  JavaScript  framework  to  use  for  the next version of the website, it’s clear that there are going to be many stakeholders involved in the decision-making process and many more impacted, such as Data Sci‐ ence, Quality Assurance, Tools, and more. However, the decision over which certificate authority  CA  to select is another story altogether. For some, the choice of certificate is as simple as saying, “Let’s just use Let’s  Encrypt,”  and  move  ahead.  There  are,  however,  a  handful  of  companies  that must make considerations that go beyond the choice between a single name certifi‐ cate and a multiname certificate. If you’re scratching your head right now, good. Let’s explore this a bit more. Depending on the SRE team, certificates might simply imply security. To other SRE teams,  certificates  might  elicit  concerns  of  impact  on  performance.  Yet  other  SRE teams  might  question  the  impact  to  protocols  and  cipher  suites.  And  beyond  SRE teams  comes  the  question  of  long-term  ownership,  integration,  changes  to  request process, workflow, access control, revocation process, certificate rotation, and more. Determining the importance of an integration early on will help avoid technical debt and scaling limitations down the road. Identify Stakeholders The  smaller  the  company,  the  easier  the  decision-making  process  is.  However,  as companies  grow,  it  is  important  to  at  least  consider  factors  that  will  impact  scale, slow  growth,  reduce  productivity,  and  cause  headaches  down  the  road.  Further, understanding importance also allows us to identify who might be impacted by our decisions so that they can be brought into the fold. As SREs, we know firsthand the role collaboration plays in site reliability. The earliest SRE teams at Google were embedded with software engineering teams, provided val‐ uable  insights  into  scaling,  and  took  an  operational  approach  to  building  reliable services. It would only make sense that as we consider the impact of integration, we identify the stakeholders and influencers who will prove key to our project’s overall success. The sense of ownership and importance play a significant role in a project’s success. It  goes  without  saying  that  people  don’t  like  unplanned  work  and  they  don’t  like being told what to do. The earlier stakeholders are identified and consulted, the easier it will be to apply ownership and have additional people to work toward the project’s success. Nothing is worse than starting a project and committing time and energy, only to have it shot down by a stakeholder whose role was not considered.  44      Chapter 5: Working with Third Parties Shouldn’t Suck   Make a Decision After you have established importance and stakeholders, we can begin evaluating the decision to build or buy. Quite often, the build discussion is heavily dependent on the incorporation of an open source solution, which I propose is an altogether new cate‐ gory to be considered: adopt. Let’s break down the differences between each option. Build  developed in-house from scratch   Your needs are so custom and innovative that it cannot be done by anyone other than your company. Often, these solutions are open-sourced after battle harden‐ ing. Such examples include Kafka by LinkedIn, React by Facebook, and Envoy by Lyft.  Buy  SaaS, hosted, or licensed solution   Your needs are common and a paid solution exists to solve that problem.  Adopt  integration of an open source solution   Your needs are common and open source solutions exist.  As we deliberate whether to build, buy, or adopt, we might note that some key differ‐ ences exist in when and where the money is spent. As we consider adopting an open source solution, the capital expenditure  CapEx  and operational expense  OpEx  can be difficult to predict. Evaluating and researching an open source project for a Proof- of-Concept  PoC  is relatively inexpensive; however, as the project is utilized at scale, issues  such  as  memory  utilization,  computational  constraints,  and  storage  require‐ ments  can  quickly  exceed  benchmarks  captured  during  R&D.  Additionally,  skilled personnel  are  necessary  not  only  for  initial  integration,  but  also  for  the  continued operation,  enhancements,  and  inevitable  upgrades.  Open  source  projects  will  have bugs that can lead to exposure to significant security vulnerabilities, memory leaks, and  loss  of  data.  In  some  cases,  full  version  upgrades  create  incompatibilities  with other  integrations  as  a  result  of  feature  deprecation.  Last,  but  not  least,  migrating between open source versions introduces breaking changes, often causing full config‐ uration  rewrites,  loss  of  backward  compatibility,  and  operational  instability.  These sorts of issues are generally covered in a single line item in the CapEx table, but that line item should be expanded into its own project operating expense  PrOpEx  table. As a result, our predictions often aim low if we don’t consult with an experienced project manager. In the buy case, CapEx is generally low with little integration cost, while OpEx is generally perceived as high—perceived because we often do not con‐ sider the personnel hours we would otherwise have to spend to produce a similarly operationally viable product. Many of the considerations involved in an open source adoption are resolved by a third-party solution, and they are on the hook for main‐ taining that operational stability. Neither  adopt  nor  buy  solutions  invalidate  the  need  for  documentation,  mainte‐ nance, monitoring, logging, tooling, automation, and all other aspects that define a  Build, Buy, or Adopt?      45   service.  The  difference  between  a  PoC,  a  functional  prototype,  a  Minimum  Viable Product  MVP , and a first-class service with high operational integrity resembles the distance between the sun and each planet. Toward the end of this chapter, we address some of the critical facets that define a first-class service. However, now that we’re at least considering the difference between build, buy, and adopt, we can address some additional points of consideration. Acknowledge Reality You’re a fantastic SRE. You produce eloquent code. You’re a natural leader. Scalabil‐ ity comes naturally to you. But you aren’t good at everything. It’s likely that you’ve found  a  niche  that  allows  you  to  excel.  The  next  interesting  project  might  not  be something that should be built from scratch. Not because you can’t, but because busi‐ ness decisions are rife with justifications that go beyond your personal interests. As a responsible SRE, prioritizing business objectives over personal interests can lead to success.  The  unfortunate  reality  is  that  working  on  something  cool  is  not  always what’s best for business. As we let that reality sink in, let’s take a look at some other project considerations that we need to assess:    What problem is being solved?   How does this impact the bottom line?   Will this impact the critical path?   Is this a core competency?   Maturity of a solution?   Nice to have or need to have?   Will this have continued adoption?   What vulnerabilities are exposed?   What is our CapEx?   What are our OpExes?   What are our PrOpExes?   What are our abandonment expenses?   Who are our customers?   To whom are we a customer?   Did we identify ancillary benefits?   What is the delta of inception to production?   What is the integration timeline?  46      Chapter 5: Working with Third Parties Shouldn’t Suck     How will this be monitored?   Who’s responsible long term?   What does escalation look like?   What are the SLAs? Is that critical?   What are the risks benefits?   What is our fallback?   How to measure success?   How to tolerate failure?   What happens if I get hit by a truck?  These  points  are  only  a  handful  of  the  considerations  to  keep  in  the  back  of  your mind  when  debating  whether  to  buy  or  adopt.  Going  through  each  consideration with your team depends on the significance of the integration. As we consider buy options, it’s worth expanding on a few of these points to provoke additional thoughts that you might not have considered.  Is this a core competency? Always weigh whether a solution integration under consideration is both your core competency as well as the company’s core competency. Third  parties  offer  specialized  solutions  and  have  teams  of  engineers  working  to expand their value proposition by building additional features and services. A Soft‐ ware as a Service  SaaS  solution for log aggregation might be more effective in the long  run  than  trying  to  adopt  an  open  source  solution  such  as  Elasticsearch.  You have  to  decide  whether  Elasticsearch  is  really  something  that  you,  your  team,  and your company need to focus on.  Integration timeline? However long you think it’s going to take to complete an integration, double it. Then add a little more. A PoC for an open source solution might be easy to implement, but this does not mean that the project is production ready. The definition of production readiness for SREs should encompass testing staging, monitoring, logging, configura‐ tion management, deployment procedure, disaster recovery, and beyond. With things like CDN, DNS, monitoring, CA, and messaging solutions, implementa‐ tion of these production readiness factors can prove to be incredibly challenging, and in  some  cases,  impossible.  Fortunately,  with  third-party  integrations,  especially  at larger companies, a variety of teams will evaluate the solution as we work on the inte‐  Build, Buy, or Adopt?      47   gration. You should expect that Legal, Security, and Procurement2 teams will play a role  during  the  purchase  process.  Each  of  these  teams  will  introduce  its  own  pro‐ cesses, which can delay getting the third-party solution into production.  Project Operating Expense and Abandonment Expense Beyond consideration for the initial cost to purchase goods and services  CapEx, such as hardware, software, and one-time licenses  and the ongoing costs to run and main‐ tain  OpEx, such as monthly server prices, recurring service fees , additional expense categories should be a part of the consideration process. Project Operating Expense  PrOpEx   Implementation  costs  for  a  solution.  Typically  a  single  line  item  in  the  CapEx table, implementation costs can prove to be a more significant factor in the deci‐ sion over build, buy, or adopt choices. Implementation costs to consider include services that will be utilized as the solution is implemented, such as legal fees, vulnerability  probing,  additional  monitoring  and  performance  infrastructure, and professional or consulting services. These costs can also expose overlooked OpEx costs.  Abandonment expense  AbEx   This is the cost to abandon a solution to implement another altogether. This is sometimes  referred  to  by  the  softer  term  migration  cost.  There  is  always  the potential that adopting an open source solution might result in abandonment in order to buy a fleshed-out SaaS solution, or vice versa. Costs will differ between abandonment scenarios. In the buy-to-adopt scenario, you might be required to pay out contractual obligations such as monthly usage commitments in addition to the incurred costs of restarting a project. In the adopt-to-buy scenario, cost is less of an issue; however, time is a much larger factor. In the previous section, we saw that Legal, Security, and Procurement processes can prolong time to produc‐ tion. It’s in our best interest to plan accordingly and abandon early. You should consider AbEx throughout the lifetime of a project. Working with third parties means that we should also consider the buy-to-buy abandonment scenario, as well—that is, leaving one vendor to use a cheaper, more robust ven‐ dor’s solution. If you’re not careful, one vendor’s custom solution could cause  vendor lock-in, ultimately increasing abandonment expense. Vendor lock-in can also tip the scales in the vendor’s favor when it comes time to renegotiate con‐ tracts.  2 Procurement teams are the experts in handling of purchase orders and subcontracts. They purchase the goods  and services required for the project and company initiatives. They pay the bills to keep the website on, but also ensure that legal and business obligations are met.  48      Chapter 5: Working with Third Parties Shouldn’t Suck   Case Study  Although LinkedIn worked with a variety of third parties for DNS, CDNs, monitor‐ ing, CAs, and the like, we always considered whether a vendor’s solution would cause us to be locked in to a solution. To a less experienced engineer, DNS and CDN appear to be straightforward solutions for steering your users to your home page and accel‐ erating  your  content,  respectively.  However,  sales  engineers  at  third  parties  often push features that make their product unique and cool. Even though these are often great features, they are not feasible with competing vendors. Some generic examples of this include Geo-steering provided by a DNS provider or Edge Side Includes  ESI  by CDN Dynamic Site Acceleration  DSA  providers. Let’s dive deeper into an example. ESI allows for caching of reusable portions of a page—such  as  title  bars,  menus,  footers,  and  so  on—to  be  stitched  together  in  the final  delivery  at  the  edge.  Although  this  solution  can  marginally  reduce  page  load times or improve load on origin servers, ESI implementation is inconsistently imple‐ mented across providers.3 After you begin using one solution and its unique features, your cost to undo new and evolving workflows begins to outweigh the cost to imple‐ ment a competing third party or implement alternate solutions such as server-side includes, alternate compression techniques, or client-side includes scripting.  After deliberating the risks and benefits to every solution, you may reach the decision to buy. Therefore, it should come as no surprise to the higher-ups that there’s work involved  in  setting  up  the  third  party  beyond  simple  configuration.  SREs  have  an obligation to build world-class solutions, and to that end, we should explore some best practices to move third parties from being ancillary to your tech stack to being an extension of it. Third Parties as First-Class Citizens In many organizations, you hear the term “first-class citizen” being thrown around in reference to some mission-critical service—for example, auth login or ads. Rarely will you  hear  “third  party”  and  “first-class  citizen”  in  the  same  sentence.  By  and  large, third parties are treated as ancillary to the tech stack. That sentiment is to be expected —if you’re paying another company for its products, features, and services, it’s that company’s responsibility to manage its SLAs. However, your end users  your custom‐ ers  do not necessarily share that sentiment, nor are they as forgiving. For all intents and purposes, your third-party integrations are an extension of your company.  3 Several third-party CDN providers offer ESI solutions. Some providers offer basic ESI support based on Spec‐  ification 1.0, while others offer extensions and custom solutions.  Third Parties as First-Class Citizens      49   When They’re Down, You’re Down Third-party integrations take many forms. As we integrate with third-party solutions, it’s  important  to  define  how  these  can  have  an  impact  on  our  end  users.  In Figure 5-1, there are a few third-party integration points that have direct and indirect impacts to end-user experience.  Figure 5-1. Third parties can fit anywhere around the edge of your tech stack  Direct impact Quite often, the third-party providers that we work with have a direct impact on site availability, site reliability, and performance. In fact, these third parties are obviously impactful to the overall reliability. Here is a short list of providers with direct impact:    DNS is the first system that end users experience. When DNS is down, you’re  down.    Edge products, such as site accelerators, wide-area firewall, forward proxies load balancers,  and  so  on,  can  suffer  congestion  issues,  human  configuration  error, and routing failures.    CDNs  for  static  objects,  such  as  JavaScript  and  stylesheets,  play  a  role  in  per‐ ceived user experience; improper configuration can lead to ineffective cacheabil‐ ity  and  high  page  load  times,  leading  to  increased  bounce  rates.  The  ability  to  50      Chapter 5: Working with Third Parties Shouldn’t Suck   diagnose  and  triage  performance  has  become  increasingly  complicated  with  Single-Page Application  SPA  frameworks.4  Indirect impact Some  providers  impact  site  reliability  in  less  obvious  ways.  Based  on  experience, these  are  third  parties  that  process  transactions  on  the  backend  of  the  technology stack. The following are examples of providers with indirect impact:    Payment processors outages—for instance, failed API key rotation or incorrect billing codes—should not delay user experience. An optimistic approach might use  previous  billing  history  to  indicate  future  payment  processing  success  and will simply queue the transaction.    Synthetic  and  Real-User  Monitoring   RUM   are  necessary  for  telemetry;  how‐  ever, use caution when utilizing either for automation.5    SMS and email integrations generally have no direct touchpoints between your end users and your tech stack. However, end users have come to expect instant gratification; therefore, undelivered payment receipts or delayed two-factor veri‐ fication codes will lead to a negative perception of your company.  SREs  are  not  only  seeking  excellent  uptimes,  but  also  a  consistent  user  experience overall.  A  theoretically  simple  mitigation  strategy  for  mission-critical  services  is  to establish  redundancy.  Whether  direct  or  indirect,  these  mission-critical  third-party integrations should be treated as first-class citizens and run like any other service in your tech stack. But if third parties are categorized as black boxes, we shouldn’t expect much from them in terms of data, right? That’s simply not true in modern day. More and more third parties are building APIs that allow us to take advantage of running their infra‐  4 SPA frameworks dynamically rewrite the current page rather than reloading the page from the server; that is, SPAs use client-side rendering instead of server-side rendering. They do so by loading HTML, JavaScript, and stylesheets in a single page load and caching those objects locally in the browser. Content is then loaded dynamically via WebSockets, polling over AJAX, server-sent events, or event-triggered AJAX calls. For a cached SPA, traditional page load timing using Navigation Timing API is incredibly low; however, the page is likely unusable without the dynamic content. Triggering a Resource Timing API in modern browsers well after expected page load times can help with the ability to diagnose and triage user experience issues.  5 Use of monitoring-based automation is growing among large companies. Synthetic monitoring solutions uti‐  lize a wide dispersion of nodes running tests on set intervals. Automation relying on consecutive event trig‐ gers from synthetic test nodes can be unwieldy and inconsistent, especially between geographies. Instead, triggers based on event consensus among test nodes can yield better results. RUM can be just as unwieldy for larger sites because the amount of data is much larger. A more surgical approach to automation is required for working with RUM-based automation.  Third Parties as First-Class Citizens      51   structure as code, which means we are able to manage configuration, reporting, and a suite of other features to run our third-party products more like services. Running the Black Box Like a Service Third-party solutions are not like the services you run in your tech stack; therefore, the  consensus  view  is  that  they’re  black  boxes—you  put  stuff  in  and  get  stuff  out. Each third-party vendor offers a set of products and features that provide some value propositions. Depending on the vendor, its solution could very well be a black box. However, many modern vendors have come to understand the need for transparency and have built APIs to facilitate integrations. Thus, rather than stamping a vendor as a black box, we should consider a vendor as being on a spectrum. We can use Figure 5-2 as a way to determine where on the spectrum a third-party vendor  might  land.  Let’s  take  two  vendors.  Vendor  A  might  have  great  reporting, tons of monitoring capabilities, simple configurations, access to a robust set of APIs, and high-caliber documentation to maximize self-service. Vendor B could have the best  solution  with  simple  configurations  through  an  intuitive  portal,  but  if  we’re looking for the ability to utilize Vendor B to help automate processes, limited API access will prevent us from fully implementing its solution. In this example, Vendor A is a very open box, and Vendor B is a more closed solution, perhaps closer to a black box.  Figure 5-2. Not all third-party solutions are black box  It’s often hard to determine which part of a third-party integration could possibly be treated like a custom-built service. To that end, we need to define the top-level func‐ tionality  that  we  are  attempting  to  achieve.  After  we  understand  that  functionality well, we should refine the activities needed to manage that integration to yield a set of predictable and actionable states. Some of these states might be the following: Fully operational  All vendor operations working as expected.  52      Chapter 5: Working with Third Parties Shouldn’t Suck   Operational but degraded performance  Vendor solution is operational but performance is degraded; for example, CDN is delivering content, but objects are being delivered slowly.  Operational but limited visibility  Vendor solution is operational but telemetry does not reflect operational state. An example of this with a third-party integration could be reporting API or Log Delivery Service outage if you rely on third-party data to flag service state.  Operational but API down  Vendor solution is operational but its configuration API and or portal are down. Vendors  typically  segregate  configuration  from  operation.  Fortunately,  we  can still be up, operationally, but this might create a bottleneck.  Hard down  Vendor solution is not operational.  With third parties, some additional states might include portal outages, processing delays,  data  consistency  issues,  regional  performance  degradation,  regional  service outages, and so on. The level of observability we have will dictate our success when we’re dealing with these states. Let’s dig into what it takes to help manage these states. Service-Level Indicators, Service-Level Objectives, and SLAs We can’t talk about running a service without talking about Service-Level Indicators  SLI ,  Service-Level  Objectives   SLO ,  and  SLAs.  But  how  do  you  capture  SLIs  on services that run outside of your tech stack that are seemingly out of your control? This is where we need to be creative.  SLIs on black boxes SLIs for services within our own tech stack are objectively easy to collect. In many cases, it’s a matter of running a daemon to collect operating system virtual machine  container stats or a library SDK that we import to emit metrics to our metrics collec‐ tion  cluster.  Easy  enough.  Vendors  run  their  services  within  their  own  tech  stack. They will not typically emit server-level metrics; however, they do offer a wealth of data in a couple of different forms.  Polling API informs SLIs.     If  your  vendor  does  not  offer  a  robust  reporting  API,  walk away. Reporting APIs will be a first step toward building your SLIs. You should not, however, trust these nor use them as if they were real time. Many vendors will try to keep their reporting APIs as close to “real time” as possible; however, there is always a processing delay. Any provider running a distributed network will likely suffer from some reporting latency—they will need to aggregate, process, and provide a flexible  Third Parties as First-Class Citizens      53   API—and this all comes at a cost that is not necessarily beneficial to their bottom line.  Real-time data informs SLIs.    As  technology  has  progressed,  vendors  are  beginning  to offer real-time logs delivered via Syslog, HTTP POST, or SFTP upload. In addition, log processing companies have partnered with these vendors to facilitate integration and dashboards. With these integrations in place, we can see the paradigm shift from the vendor is a black box to the vendor is a partner. With the data on hand, we are allowed  to  lift  the  weight  of  vendor  dependency  to  becoming  more  self-reliant.  If real-time data is offered as a paid service, ask that it be free. Ultimately, you, as an SRE, will utilize this to troubleshoot issues, thus alleviating their customer support.  Synthetic monitoring informs SLIs.    Trust but verify. Our providers can give us a wealth of data, and it’s unlikely that our vendors would hide anything. But for services that we run that have a very broad scope, such as CDN or DNS, it can often be extremely difficult to triage and diagnose an issue. Synthetic monitoring  APM solutions such as Catchpoint and Keynote  provides an additional layer of detail that could not be seen  through  service  metrics  alone.  Synthetic  tests  will  often  reveal  configuration issues, delivery latency, cache efficiency, compression handling, and corner cases that would be difficult to find with log data alone. For SREs that own CDN and DNS, this is becoming critical. And if you’re wondering, synthetic monitoring providers offer both real-time and polling APIs, so you can do more cool stuff with test results.  RUM informs SLIs.    Nothing is better than real-user data. With advancements made in modern browsers, there is a wealth of information that we can garner from naviga‐ tion  timing  and  resource  timing  APIs   client  side .  RUM  can  tell  you  all  sorts  of information  about  how  users  are  experiencing  your  third-party  services,  which includes CDN, DNS, CA, payment processors, ads, and more.  SLOs SREs typically focus on meeting their SLOs—the agreement between SRE and prod‐ uct owners for how often a service’s SLIs can be out of spec. As an SRE working with a provider, SLOs can be difficult to calculate and even more difficult to define. It’s not simply  a  matter  of  carrying  over  the  SLA  from  the  vendor.  SREs  responsible  for third-party integrations might be dealing with multiple vendors simultaneously and support a broad range of product teams, as is the case with payments, ads, CDN, and DNS. In this scenario, the product owners end up being your sibling SRE teams. The SLOs we define in this scenario allow our sibling SRE teams to calculate risk and for‐ mulate error budgets.  Negotiating SLAs with vendors.    When you first sign up for service with your vendor, you sign a lot of things, including a Master Service Agreement  MSA , which covers  54      Chapter 5: Working with Third Parties Shouldn’t Suck   all of the vendor’s products and services, and an SLA. Historically, SREs haven’t had to work with MSAs or SLAs, but with the evolution of the SRE role and broadened scope of work, these agreements are more commonplace for the SRE. That said, if you’re big enough to have a legal team and a procurement team, be sure they are in the loop before signing anything. Often, vendor SLAs provide credits for service when they fail to meet their end of the agreement.  Most  SLAs  include  99.5%  uptime.  Unless  there’s  a  catastrophic  event, these SLA targets are easily met. Things get tricky when we begin to consider SLAs that  meet  specific  needs,  such  as  throughput  for  large  object  delivery  on  a  CDN, indexing latency for our log processing companies, or metric accuracy for our syn‐ thetic monitoring providers. Use the same SLIs and SLOs to inform your SLAs to hold vendors accountable for their  service  availability  and  performance.  Your  procurement  team  will  appreciate the transparency and validation of their hard work. Playbook: From Staging to Production With many of the services we run, we have to adhere to some sort of playbook—a means for running our service in production. This involves a detailed explanation of what the service is intended to do, how to test it, how to deploy it, and how to deal with it should things go sideways.  Testing and staging Quarterly releases are ancient history. These days, Continuous Integration  CI  and Continuous Deployment  CD  are standard. We’ve become accustomed to maintain‐ ing a deployment pipeline similar to one that is triggered by committing code to mas‐ ter and allowing our automation to run a suite of unit tests, followed by deployment to a staging environment, followed by Selenium testing, followed by canary deploy‐ ment, and finally production deployment. Third-party integrations can pose a different set of challenges with regard to properly testing functionality within staging environments. It’s unlikely that CDN, DNS, cer‐ tificates, and edge proxy configurations are considered as part of a CI CD pipeline. It’s even less likely that your staging environment resembles production, if that stag‐ ing  environment  even  exists.  Unfortunately,  as  these  products  have  been  seen  as ancillary, little attention has been given to how these technologies play a role in the deployment process. With  the  emergence  of  new  players  in  the  CDN,  DNS,  certificate,  edge,  and  APM space, progress is being made as to how we can configure and manage their respec‐ tive products via code. As a result, replication of environments has become more fea‐ sible.  Additionally,  some  vendors  offer  some  form  of  staging  environment.  For  Third Parties as First-Class Citizens      55   example, Akamai offers configuration staging as a standard part of its configuration deployment procedure; the staged configuration is triggered by modifying hostname resolution to target the staging hosts. Staging environments ultimately encourage the creation of unit, regression, and Selenium test regimens.  Case Study  For LinkedIn’s CDN integrations, we maintained a suite of regression tests that were executed before and after every configuration change and mapped to a set of content profiles  static images versus JavaScript versus stylesheets . Even though a set of con‐ tent  might  have  a  shared  origin  server6  and  edge  hostname configuration,7  the context-path mapped to different underlying services with a variety of caching head‐ ers. Sprites and logos rarely change and are highly cacheable; therefore, query param‐ eters can be ignored, implement long expiries, and disable edge compression. On the contrary,  to  optimize  delivery  performance,  JavaScript  was  minified  and  concaten‐ ated  to  take  advantage  of  a  single  connection.  Further,  JavaScript  concatenation required query parameters included as part of the cache key, a long expiry, and sup‐ port for edge compression. The  suite  of  regression  tests  validated  cacheability,  retention  rules,  negative  testing  what  happens  when  something  goes  wrong;  e.g.,  HTTP  404  or  HTTP  500 ,  error handling, compression, and protocol because they vary between each content profile. In addition to running regression tests before and after configuration changes, these tests were also executed periodically. This allowed us to detect modifications to the underlying services acting as origins, thus allowing us to get ahead of any major per‐ formance or reliability issues.  Monitoring There are two categories of monitoring that we should consider with third-party sol‐ utions:  integration  monitoring   is  the  integration  working?   and  impact  monitoring  what impact does the solution have on our users? . Obviously, we want to ensure that the integrated service  CDN, message processor, etc.  is stable; hence, we should be looking at whether the third party is up or down. Additionally, we want to know when the integrated service is dropping connections, becoming overloaded, or intro‐ ducing processing delays. Some third-party integrations rely entirely on APIs; thus, monitoring the integration is a bit more straightforward, as is the case with an SMS or email provider.  6 An origin server in the context of CDNs is simply the source of the original content. 7 A CDN edge hostname generally maps to a single configuration protocol combination; that configuration  contains a mapping to the origin server and content rules. Conditional rules  such as request headers or context-path  can trigger alternate origin server selection, caching rules, header modification, and so on.  56      Chapter 5: Working with Third Parties Shouldn’t Suck   Impact monitoring is a bit different. When we relinquish some control of our site to third parties—for example, content delivery by CDN or steering to data centers by DNS providers—monitoring a third party’s impact on our users becomes a bit more challenging. To understand how end users are impacted by these critical path inte‐ grations, we should rely on synthetic monitoring or RUM.  Uses for synthetic monitoring.    Synthetic monitoring is a dimension of end-user experi‐ ence testing that relies on probes or nodes that run canned tests at regular intervals. Many third parties specialize in this type of APM solution; these include Catchpoint, Keynote, Gomez, Pingdom, and Thousand Eyes. This type of monitoring is great for things as simple as testing whether an endpoint is up or down, or as complicated as transactional workflows. Here are some things to do:    Utilize real-time push APIs rather than polling for aggregated test data. Polling APIs will cause delayed Mean Time to Detect  MTTD  if you’re using synthetic tests  to  identify  failures  due  to  reporting  aggregation  delays.  Push  APIs  can ensure that you get the raw results.    Create multiple synthetic tests for each CDN edge configuration. Each CDN can behave differently within the same region, and synthetic tests can possibly reflect regional performance and reliability issues.    Monitor  for  content  errors  such  as  HTTP  error  codes  or  connection  resets  or timeouts on child assets  not just base page  to better understand how a CDN provider is handling your content.    Run DNS tests against authoritative name servers so that you remain aware of issues related to your traffic steering; testing against default resolvers will not iso‐ late issues to your DNS provider s .    Ensure good test node coverage over your highest-traffic geographies and main‐ tain low test intervals to ensure maximum test data. This will allow you to flag for errors from a consensus of servers over a smaller rolling window.  And here are some pitfalls to avoid:    Don’t rely on polling of third-party reporting APIs. This data is generally aggre‐  gated.    Don’t configure long test intervals to save money. Long test execution intervals will  limit  granularity.  Because  each  node  will  generally  run  a  test  only  once within a given interval, it’s unlikely that monitoring consecutive events from a single node will flag failures. Instead, consensus for a test over a given interval might prove more fruitful.  Third Parties as First-Class Citizens      57     Don’t run static page tests to monitor your CDNs. Synthetic test nodes will likely hit  the  same  CDN  edge  Point-of-Presence   PoP   repeatedly  and  artificially improve cache efficiency anyway. Therefore, static page testing will tell you only how well a CDN serves cached content; this does not represent how well a CDN serves content from origin to end user.    Don’t pick nodes for the sake of picking nodes. Some nodes might not be on eye‐ ball networks  such as Comcast or Time Warner ; instead, nodes sit on Backbone networks in data centers or peering facilities. Performance data from backbone networks might not be a reflection of what real eyeball networks experience.    Don’t select nodes that are regionally disparate from your actual users. Selecting a node in Timbuktu when most of your customers are in North America is sim‐ ply illogical.  Uses for RUM.     RUM  also  falls  within  the  dimension  of  end-user  experience  testing. The key distinction is that RUM is executed on the client side. The improvements in browser APIs have allowed for enhanced reporting data. We now have access to navi‐ gation timing APIs—page load time—and more interestingly, resource timing APIs —per object load times—across most modern browsers. The same third parties that offer a synthetic monitoring solution often offer a complementary RUM solution in the form of a JavaScript beacon. Here are some things to do:    Take advantage of both navigation and resource timing APIs to pinpoint content  delivery issues.    If  using  vendor-agnostic  hostnames   hostnames  that  are  not  vendor  specific , implement a custom header to identify which third-party vendor was responsible for object delivery.    If your company has a Data Science Performance team, it is likely that real-user data is already being collected and processed through a data pipeline. They might be  enticed  to  enhance  that  data  pipeline  to  include  enhanced  delivery  details from resource timing APIs.    Ensure Security and Privacy teams evaluate the RUM beacons that you are con‐ sidering using. This is especially important because RUM can expose personally identifiable information  PII , cookies, and IP details. This might run afoul of the  58      Chapter 5: Working with Third Parties Shouldn’t Suck   EU’s General Data Protection Regulation  GDPR 8 compliance rules, which went into effect on May 25, 2018.  Again, here are some pitfalls to avoid:    Don’t reinvent the wheel; if you have a Data Science or other team concerned about RUM, it might already have a data pipeline9 that we can utilize. Similarly, Google,  Amazon  Web  Services   AWS ,  and  others  offer  solutions  for  message queueing and streaming.    Don’t rely on third-party APIs as a conduit for monitoring third-party health. Third parties generally do an excellent job of performing what they’re paid to do; however, their reporting APIs are not necessarily their core competency. If you rely on third-party reporting APIs, you are likely relying on data that is stale or inaccurate.  This  is  because  of  data  aggregation  delays,  processing  delays,  data pruning, and repackaging of that data for your consumption. Instead, we want to rely on as much real data as possible, such as data that can be pulled from logs in real time.  Case Study  Many large organization use a combination of synthetic and RUM to diagnose issues. While at LinkedIn, my team was responsible for synthetic monitoring, and we grew the suite of tests 15-fold to cover all products and detect issues. Synthetic monitoring was utilized to identify content delivery issues in a variety of ways for CDN and DNS and was a great secondary resource for identifying issues with configuration changes.  Tooling Tooling is a critical part of our third-party integration, especially in a larger organiza‐ tion. As an SRE responsible for third-party integrations, we want to avoid being a  choke  point  as  much  as  possible.  To  that  end,  we  should  build  systems  that  take advantage of everything the third party has to offer via APIs. These APIs facilitate integrations  by  allowing  you  to  build  the  tools  needed  to  effectively  manage  your third-party integrations. As these APIs are employed, we will want to consider AbEx and avoid the use of custom features that could create vendor lock-in. We can see an  8 GDPR applies to all foreign companies and extends current data protection regulations for all EU residents.  You can find more information at https:  www.eugdpr.org.  9 Many companies use a data pipeline for interapplication messaging following a publish–subscribe  pub–sub   architecture to trigger events. These pipelines are often also utilized for streaming data, such as page views, application logs, or real-user data.  Third Parties as First-Class Citizens      59   example of this in CDN purge APIs—some vendors utilize surrogate keys, some uti‐ lize some form of regular expression, some utilize wildcards, and some utilize batch‐ ing. Any of these implementations into our regular workflow could cause an issue if we need to replace or add another vendor’s solution. Further removing us from being a choke point, we want to avoid any integration that requires we play a role in access control. Managing access to vendor portals for every engineer is overly cumbersome and has the potential to lead to security issues and technical debt. Finally, as we create tooling around our vendor’s API, it’s critical that we consider abandonment. Try to maintain a modular architecture to support a variety of third parties.  This  layer  of  abstraction  allows  for  additional  providers  and  swapability should a third-party vendor need to be replaced.  Case Study  For years, LinkedIn has worked with upward of six CDNs and three DNS providers. To prevent becoming a choke point, we implemented tools that allowed for tedious, manual processes to be implemented across all providers uniformly. Tasks such as content purge, DNS record changes, and reporting data are handled via centralized systems.  Requests  are  abstracted  and  data  is  normalized  to  maximize  implementa‐ tions across each provider.  Automation As an extension of tooling, we want to be able to automate some of the functionality of  our  third-party  solutions  as  much  as  possible.  The  possibilities  here  are  endless and fully depend on how you make use of your third party’s APIs. To give you an idea of what we can do, let’s look at the following examples: Automating content removal on account closure  When  a  user  closes  their  account,  certain  privacy  laws  apply  and  require  that user PII is deleted. As a result, it’s possible to utilize your company’s data pipe‐ line   or  similar  message  queue   to  identify  such  events  to  automate  the  image removal process. This will be necessary for GDPR compliance.  Automating data center endpoint selection  In the event that a critical service is down behind an endpoint, we can use DNS APIs to steer traffic away from the affected endpoint.  Automating certificate renewal  In the event that certificates are ready to expire, you could easily use your CA’s API  to  reinstall  certificates  to  your  endpoints  rather  than  relying  on  manual intervention or outage.  60      Chapter 5: Working with Third Parties Shouldn’t Suck   Logging Third-party  reporting  APIs  are  generally  not  great.  Often,  the  reporting  data  we receive via pull APIs are aggregated, processed, and scrubbed. Aggregation delays are common with pull API and should not necessarily be used for raising alerts. As an SRE, we need to know what’s happening now, not what happened 15 minutes ago. To that end, third-party integrations have often been looked at as black boxes with little to no visibility into their operations. But times have changed. Many third parties offer real-time data feeds that include all sorts of useful data; CDNs are offering delivery receipts for every object; DNS provid‐ ers  are  offering  query  data;  and  synthetic  monitoring  providers  offer  granular  test data. The benefit to all of this available data is that we’re able to build with it as we see fit. However,  this  is  also  a  risk.  Just  because  we  have  the  data  does  not  mean  that  it’s entirely useful. As we utilize real-time logging data, we need to be sure we’re utilizing what we need to supplement our monitoring as well as provide a way to track down issues. For example, CDN logs can include a browser type, extension details, client IP details,  cookies,  and  other  request  headers.  Much  of  this  is  unnecessary  for  most applications, such as regional isolation of HTTP response codes. Finally, logging can be absurdly expensive. Not only do some vendors charge for the delivery of this data, but you’re also responsible for consuming, indexing, processing, and  retaining  it.  This  can  be  a  large  undertaking  in  and  of  itself,  and  you  should approach it with caution.  Disaster planning Although we don’t like to think about it, working with a third-party vendor incurs some risk. As part of our playbook, we will need to consider what to do when the vendor  is  hard  down.  Disaster  planning  should  include  considerations  for  the removal of the third party altogether. You will likely have to consider the following:    Maintaining an additional  as active or standby  provider   Having capacity to serve requests ourselves   Queueing requests to be handled later  Removal  of  a  CDN  might  require  that  we  accept  a  certain  amount  of  latency. Removal of a payment processor might require an optimistic approach to ensure a good customer experience. Some delay in a welcome message or payment receipt via email might not be entirely noticeable to an end user. Remember, slow is better than down.  Third Parties as First-Class Citizens      61   Communication With third-party integrations, it’s important to not live in a silo. Talking to a solu‐ tions  or  sales  engineer  actually  plays  to  your  advantage  for  discussing  product enhancements, discovering unknown or underutilized features, understanding their product roadmap, and preparing third parties for future events such as product relea‐ ses. Additionally, it pays to maintain bidirectional communication to align priorities. Third-party  vendors  should  be  under  NDA;  therefore,  it’s  helpful  to  communicate any upcoming projects that might alter the relationship. Establishing a regular call cadence can be useful but might not be ideal for all SREs. Often, these calls are tedious are time-consuming, and can conflict with production issues.  Additionally,  there  is  a  bit  of  organization  required,  especially  when  you’re working with multiple vendors. To that end, it’s often a good idea to work with a project manager or someone similar who can facilitate these calls and keep detailed notes. Support from a project manager will help you keep your vendors aligned with your team’s and company’s priorities.  Decommissioning When it comes time to decommission a service, we consider all of our dependencies. The same is true of dealing with third parties. We obviously need to consider removal of tooling, service monitoring, alerts, and data collection services endpoints. In addi‐ tion, we are obliged to consider a few other dependencies that play a role in the inte‐ gration. Sometimes, termination is not as simple as stating “you’re fired.” Terms of the termi‐ nation are generally stated in your service agreements. If you’re deciding to terminate a contract for financial reasons, you might be able to negotiate in advance of termina‐ tion. In these cases, use caution when evaluating contracts that include Most Favored Customer   MFC   clauses  because  these  often  make  it  harder  to  negotiate  a  better deal. Some agreements also allow for autorenewal months prior to a contract’s end date. Your legal and procurement teams might have prevented this, but it’s important to look at the contract and discuss termination with them. Communication obviously played a role in the purchase and ongoing relationship of your third-party integration. Communication is even more critical during contract termination. If, for instance, the third party was not meeting SLAs but that was never communicated, it’s not necessarily fair to that vendor and could prove costly for the third-party sales teams. The tech industry is small, and stories of a bad breakup will haunt you and your com‐ pany. These stories could play a role in your future integration attempts. Sales organi‐ zations remember which customers were most difficult to work with, and salespeople move between companies, especially within the same industry. Failure to communi‐  62      Chapter 5: Working with Third Parties Shouldn’t Suck   cate  during  the  decommissioning  process  can  tarnish  reputations  and  complicate future relationships. The best way to avoid this is to communicate your intent to terminate a third party well in advance of the end date—at least a quarter ahead. This gives the third-party sales team an opportunity to rectify any problems that might have persisted during the integration life cycle. We vote with our dollars. Losing a large enough customer can incentivize a third party to prove themselves. Closing Thoughts Many SREs consider working with third-party solutions to be a disappointment from both an intellectual-challenge and career-path perspective. Not only would we rather solve large problems by building our own custom, in-house solutions, but we’re often encouraged to do so to advance our professional trajectory. However, custom solu‐ tions are not necessarily in the best interest of our company or our end users. Quite often, buy options offer as much of a challenge as the build or adopt options. We should shift our mindset away from third-party solutions getting in the way of our creativity and instead consider how a third-party solution plays a role in end-user experience and site reliability. As we close out, you should take the following points with you:    Third parties are an extension of your stack, not ancillary.   If it’s critical path, treat it like a service.   Consider abandonment during the life cycle of an integration.   The quality of your third-party integration depends on good communication.  Jonathan Mercereau has spent his career leading teams and architecting resilient, fault tolerant, and performant solutions working with the biggest players in DNS, CDN, Cer‐ tificate  Authority,  and  Synthetic  Monitoring.  Chances  are,  you’ve  experienced  the results of his work, from multi-CDN and optimizing streaming algorithms at Netflix to all multi-vendor solutions and performance enhancements at LinkedIn. In 2016, Jona‐ than cofounded a SaaS startup, traffiq corp, to bring big company traffic engineering best practices, orchestration, and automation to the broader web.  Closing Thoughts      63    CHAPTER 6 How to Apply SRE Principles Without Dedicated SRE Teams  Björn Rabenstein and Matthias Rampke, SoundCloud Ltd.  Often,  mid-sized  organizations  find  themselves  in  a  position  in  which  a  relatively small number of engineers must develop and run a relatively large number of diverse features. SoundCloud has grown into exactly that situation. With each new feature added to the  original  monolithic  Ruby  on  Rails  code  base,  adding  the  next  feature  became more difficult. So around 2012, we began a gradual move to a microservices architec‐ ture.  SoundCloud  engineers  have  talked  a  lot  about  the  various  challenges  that needed to be tackled for such a move to succeed.1 In this chapter, we explore lessons learned  from  reliably  running  hundreds  of  services  at  SoundCloud  with  a  much smaller number of engineers. SREs to the Rescue!  and How They Failed  In 2012, SoundCloud happened to hire a couple of former Google SREs. Although dramatically smaller in scale, SoundCloud was moving toward technological patterns not so different from what larger internet companies had been doing for a while. By extension, it was an obvious move to also run those systems in the same way Google does. We tried “SRE by the book,” except that back then there was no actual book.  1 The SoundCloud backstage blog is a good starter to learn more.  65   A Matter of Scale in Terms of Headcount What is the smallest reasonable size of an SRE team? Because SREs ought to be on- call, the team needs to be large enough for at least one on-call rotation. Following the best practices for on-call rotations,2 we end up with a minimum team size of eight. For organizations of a similar size as SoundCloud—that is, roughly 100 engineers— even that one SRE team of minimal size would already comprise 5 to 10 percent of the total engineering population. Doubling or tripling this percentage would not be feasible, which limits us to at most one SRE team. For an even smaller organization size, the headcount requirement of a dedicated SRE team appears to be prohibitive. At Google, with two orders of magnitude more engineers, there are obviously many SRE teams dedicated to certain areas, like Search or Ads, supporting a limited num‐ ber  of  development  teams.3  The  one  SRE  team  SoundCloud  could  accommodate would  be  in  charge  of  everything!  However,  SoundCloud  needs  to  serve  a  very diverse set of features to different groups of users: listeners want to stream music; cre‐ ators want to upload and manage their tracks; rights holders want reports and ulti‐ mately  money;  advertisers  want  to  manage  their  campaigns.  In  fact,  SoundCloud already had a catch-all on-call rotation, staffed by a team called SysOps. Even with fairly stable, mature services  and SoundCloud’s services at that time were far from that , the large number of moving parts would have led to an unsustainable pager load, violating the goal of keeping toil below 50% of each engineer’s time.4 On top of that, the wide diversity of involved services made it unlikely that the on-call systems engineer could react competently to a given page. The usual reaction to a page was to identify the right team to which to escalate. We had to conclude that, despite being large enough to sustain a dedicated SRE team, our feature diversity effectively put us into the same situation as smaller organizations: we could not just copy Google SRE verbatim, we had to adjust the approach to our circumstances. The Embedded SRE In a first attempt to apply SRE principles and reduce the pressure on the SysOps team on  call  for  everything,  we  embedded  SREs  into  the  backend  development  teams. SoundCloud  has  always  had  a  strong  culture  of  software  engineers  controlling  the deployment  of  their  work.  This  was  partially  a  holdover  from  the  very  early  days when there was neither an operations team nor an on-call rotation. Later, it was per‐ petuated  as  part  of  a  deliberate  management  strategy  that  optimizes  for  long-term velocity in delivering features.  2 Site Reliability Engineering: How Google Runs Production Systems, Chapter 11. 3 Ibid., Introduction  p. 125 . 4 Ibid., Chapter 5  66      Chapter 6: How to Apply SRE Principles Without Dedicated SRE Teams   The  approach  of  embedding  SREs  was  doomed  by  the  numbers:  because  develop‐ ment  teams  consisted  of  no  more  than  10  engineers,  there  was  at  most  one  SRE assigned  to  each  of  them,  following  the  assumption  that  about  10%  of  engineers would be SREs. In this environment, individual SREs had very little leverage with which to influence the design and technical direction of new services. Even though some of them had soaked up the concept at Google, most of the other engineers and managers had not, and did not know what to do with that engineer who showed up one day and pro‐ claimed to be “their SRE now.” In most cases, they were either ignored or treated as yet another backend engineer, but working with them was not seen as an opportunity to improve reliability or productivity. Being embedded, they also could not work together as a team on the kind of impact‐ ful,  strategic  projects  that  would  later  provide  a  framework  for  empowering  the development teams. You Build It, You Run It At this point, we had to try something new. We embraced what was already present in our cultural DNA: you build it, you run it. Or, as we sometimes like to put it: true DevOps, where there are no separate dev and ops teams anymore and not even desig‐ nated dev or ops roles within a team. Arguably, this is contrary to fundamentals of SRE. After all, an SRE team is an operations team on some level, albeit with the prem‐ ise of solving ops concerns with the mindset and the toolbox of software engineers. To  successfully  apply  SRE  principles  in  our  scenario,  we  needed  to  instill  the  SRE mindset into everybody. The Deployment Platform Very early in our migration to microservices, we learned that our processes and infra‐ structure  were  not  ready  for  the  resulting  large  number  of  services.  Each  service required dedicated servers, the configuration management code to support them, and a deploy procedure to actually get code changes out. Application developers would deploy the code but did not control the infrastructure. Servers for new or existing services were provisioned exclusively by the SysOps team. For each new service, we needed to write configuration management and deployment scripts.5 This led to friction and long cycle times for bringing up new services or for scaling out existing ones. For example, adding more servers to the Rails monolith, at  5 Because of our background in Ruby, we chose Chef for configuration management early on. Ruby services were usually deployed using Capistrano. As we began experimenting with alternatives to the Ruby on Rails monolith, many more technologies and methods came into play.  You Build It, You Run It      67   the time still the biggest single service by far, required multiple hand-offs between SysOps and the backend development team to provision the servers, add an initial set of configuration management to prepare for the deployment, deploy the code, and then change configuration management to actually create the processes. To  deal  with  this  situation,  a  handful  of  engineers  had  already  begun  to  create  a container-based  deployment  platform  that  would  empower  all  developers  to  build, deploy, and operate their services. It was, by design, superficially similar to the popu‐ lar Heroku Platform as a Service. It supported 12-factor applications, which was luck‐ ily not a big step for many existing services. To build a new version and make it ready to deploy, developers would push to a Git repository. This made the deployment sys‐ tem  immediately  accessible  to  application  developers  without  the  steep  learning curve of configuration management. Its constraints shaped the design of applications and made it easy to do the right thing without stifling innovation. The engineering management realized that a dedicated team was needed to support and further develop the new deployment platform as well as other “platform services” like monitoring. This was the birth of the Platforms team. Besides the developers of the  deployment  platform,  the  new  team  drew  heavily  on  the  formerly  embedded SREs, resulting in a mix of engineers with very solid software engineering skills but also a good understanding of systems and operational concerns. Almost accidentally, the Platforms team became the role model for a self-sufficient, autonomous develop‐ ment team: from the beginning, it was on call for all the services it provided, includ‐ ing the supporting infrastructure.  Historical note: our deployment platform was called Bazooka. You might still find references to it in talks and posts on the internet. At its  time,  it  was  a  highly  innovative  container  orchestration  plat‐ form.  Back  then,  there  was  no  Docker,  containers  were  fairly arcane stuff to most, and there was obviously no established con‐ tainer  orchestration  platform  available  in  the  open  source  space. Hence, building Bazooka was a necessity and not a result of a not- invented-here syndrome. Later, we migrated completely to Kuber‐ netes. Bazooka and the Platforms team are history now.  Closing the Loop: Take Your Own Pager Now that it was easy and quick to deploy changes and create new services, a new bot‐ tleneck became apparent: even though developers could deploy at any time, they had to be mindful and coordinate with those carrying the pager for their systems. From the earliest days of SoundCloud, the decision of when to deploy changes had been with the developers making the change. On the other hand, the SysOps team, built from experienced systems and network engineers, had installed the first moni‐ toring and, by default, received all of the notifications and pages.  68      Chapter 6: How to Apply SRE Principles Without Dedicated SRE Teams   This situation caused a large amount of friction: SysOps were regularly dealing with the consequences of code deploys over which they had no control. They were unfa‐ miliar with the deploy mechanisms for those applications not running on Bazooka, and  they  did  not  know  to  which  versions  they  could  safely  roll  back.  Because  the global traffic peak usually happened just after regular work hours ended in Europe, performance  regressions  often  hit  in  the  off-hours.  For  most  issues,  the  on-call SysOps engineer could do only one thing: find an application developer to whom to escalate. Because there was no organized developer rotation, this could at times be difficult, and the load of escalations was very unevenly distributed among developers. It was clear that we could not have a sustainable single on-call rotation while main‐ taining the independence of development teams to deploy features as quickly as pos‐ sible. Adding more gatekeeping was out of the question. Instead, we went the other way:  spearheaded  by  the  Platforms  team,  more  and  more  teams  took  over  on-call duties for their own services. Starting  with  the  health  of  the  service  itself,  developers  gradually  extended  their responsibility to all direct dependencies such as database clusters. The main incentive for teams to take up the operational burden was autonomy: they could now build and deploy new features without having to wait for any other teams. To ease the transition for engineers who did not have on-call experience, we devel‐ oped a series of interactive learning workshops. To help all engineers brush up on their  operational  skills,  these  workshops  covered  topics  like  monitoring,  incident response, and conducting postmortems. The  resources  freed  up  in  SysOps  were  reinvested  in  continuously  improving  the tooling to support the development teams in their journey so that more ownership and  alerts  could  be  handed  away.  We  had  created  a  virtuous  circle:  the  more  we improved our systems and automation, the faster we could do so. Introducing Production Engineering As each team works within the boundaries of its own domain, overall availability suf‐ fers. Decoupling systems to such a degree that an outage in one does not influence any others can reduce this issue, but not completely eliminate it—at some point the user experience comes together to form a whole, and this is where cross-domain fail‐ ures happen. The Production Engineering team’s function is to counter this, cham‐ pion holistic system stability, and foster exchange of knowledge and good practices throughout the organization. As the SysOps team succeeded in handing away much of the daily operations of serv‐ ices and their direct dependencies, it remained primarily responsible for the buildout and maintenance of the physical layer—servers and networks. However, this position also caused it to loosely interact with almost all of the other teams. Teams would use  You Build It, You Run It      69   this  opportunity  to  gather  input  on  system  design,  help  with  database  issues,  or become connected with other teams solving similar challenges. As  the  SysOps  team  worked  toward  its  own  obsolescence,  Production  Engineering  or ProdEng for short  was formed by merging some of the former SysOps engineers with the aforementioned Platforms team, which had shrunk significantly after its big projects had been mostly completed. Thus, it again had the SRE-typical mix of sys‐ tems and software engineers. The goal here was to increase focus by separating the consulting and data center operations functions. A significant difference from both traditional operations teams and Google-style SRE teams is that ProdEng at SoundCloud does not have formal power to prevent feature releases, let alone any more sophisticated safeguards like error budgets.  In fact, in our  early  days,  we  only  learned  about  our  user-facing  error  rate  from  Twitter.   In contrast  to  a  Google-style  SRE  team,  ProdEng  cannot  even  threaten  to  return  the pager,  because  they  don’t  carry  the  pager  for  user-facing  applications  in  the  first place. Although this was a challenge, it proved possible to overcome. As we will see in more  detail  later,  there  was  no  “decree  from  heaven”  to  follow  ProdEng’s  advice, especially as our engineering culture placed a heavy focus on autonomy of teams. From the Platforms team, ProdEng inherited ownership of the deployment platform. By providing this fundamental service that every production deployment needs, they maintain contact with all engineering teams without needing to become a gatekeeper. Working up from the bottom of Mikey Dickerson’s Hierarchy of Service Reliability,6 ProdEng offers assistance and common technologies for monitoring  so that we can detect outages before our users tweet about them , guide incident response, and facil‐ itate the postmortem process, all with well-known SRE practices in mind. Because of the wide variety of experiences and exposure to past choices, ProdEng is also often called upon to help with the design of new systems. This step is, however, completely  optional—development  teams  are  free  to  choose  whether  they  want  to engage consultations at any step or leave it be. These consultations are often informal —for example, a hallway conversation might turn into a one-hour brainstorming in the early conception of a new feature. Formal reviews also happen, but only if the team owning a service feels the need. They are a desirable service that ProdEng pro‐ vides, not a requirement for launch. Reviews can happen at any point in the service life cycle, with changes in service ownership being a common reason. It is up to development teams whether, and in what form, they want to engage with ProdEng. This allows them to pace feature development as it suits their needs.  6 Site Reliability Engineering: How Google Runs Production Systems, Figure 3-1.  70      Chapter 6: How to Apply SRE Principles Without Dedicated SRE Teams   Keeping  development  and  production  support  within  the  same  group  creates  self- regulation: a team that haphazardly ships half-baked features to production will be slowed down as a consequence of their choices, delaying future features. A team that overthinks and overdesigns will similarly be slow to deliver, but each team finds its own sweet spot on this continuum—and temporarily deviates from it if the circum‐ stances demand it. It is important to not damage the user experience too much while exercising this con‐ trol loop. Quantifying and setting limits for the impact on the user experience is not trivial. We have set a few clearly defined high-level availability targets but rely other‐ wise  on  our  common  postmortem  process  to  hold  each  team  accountable  for  the impact of its choices, an approach mostly enabled by the limited size of our engineer‐ ing organization and certainly helped by a strong sense of empathy with our users. Some Implementation Details This  section  provides  concrete  details  of  how  we  applied  SRE  principles  in  spirit while adjusting the implementation to our given situation in which a relatively small number of engineers run a relatively diverse set of features. We picked examples we deem most interesting and most suitable to illustrate how the adjustment worked out. Developers’ Productivity and Health Versus the Pager Having a group of people on call for too many different moving parts is not only very prone to pager fatigue, it also makes it difficult to react in a competent way and thus leads to “pager monkey” devolution. However, the opposite end of the spectrum is equally dangerous. With our move to developers being on call for the systems they built,  there  was  a  natural  tendency  to  have  very  small,  expert-only  rotations.  This sounds great at first but falls into a number of traps:    “Experts only” usually means very few engineers in the rotation, which leads to far too much time on call for a healthy work–life balance  especially if follow- the-sun is not an option and on-call times regularly span the night .    Being on call for very few things creates a larger tolerance toward noisy pages, which  would  be  unbearable  in  a  rotation  with  more  responsibilities.  It  is  thus very easy to fall into bad habits and procrastinate improvements of the situation, given that there are usually more pressing things to work on.    As there are only experts in the rotation, “tribal knowledge” is another bad habit to easily slip into. The incentive to write proper runbooks or even set up alerts in an easy-to-understand way becomes fairly low.  We fell into all of these traps and had countless two- or three-person rotations that were paged all the time by completely undocumented, often meaningless alerts. Get‐  Some Implementation Details      71   ting out of that trough was not only a matter of discipline and strict guidelines; suit‐ able technology  like Prometheus for monitoring and alerting  played an important part in enabling meaningful alerting and facilitating incident response even for non- experts. Following our strategy of making it easy to use the right solutions, without mandat‐ ing them, we added extensive Prometheus instrumentation to the preferred applica‐ tion framework, which wraps Finagle. Most of our microservices work in a network- heavy  scatter-gather  fashion  with  only  limited  internal  logic.  By  exposing  metrics about incoming and outgoing network calls, the users of this framework get insight into the health of their services without the usually required boilerplate code. This, in turn, makes it more attractive to use the framework in the first place. ProdEng made it easy for teams to improve their own monitoring, and additionally provided some generic high-level alerting. With these as a starting point, it is now easier for development teams to have decent monitoring than no monitoring at all—a profound shift that removes the perception of monitoring as a burden. With the tools in place, we were ready to address the issue of small, noisy, expert- only on-call rotations. To limit the time being on call, we needed more engineers in the rotation, not only experts. A few smaller teams began merging their on-call rotations into one. This meant that even without a dedicated SRE or Ops team, engineers were again on call for services for which they did not have a hand in writing and did not own. This was made possi‐ ble by following SRE practices. This  group  of  teams  documented  and  publicized  their  experiences  as  well  as  the ground rules they had developed for any other teams joining this rotation:    Any team that has any service covered by the consolidated on-call rotation must have at least one engineer on the rotation. This way, teams could not hand off all operational responsibility without contributing back.    All operational tasks for incident handling must be documented in a runbook. There  must  be  a  runbook  entry  at  least  giving  context  and  an  explanation  for every alert.    Noisy pages must be eliminated—every page to the consolidated rotation should have meaning. Unless there is a strong argument that future pages will be action‐ able, unactionable pages usually cause the alert to be eliminated or made less sen‐ sitive.  This  tends  to  eliminate  cause-based  alerts  unless  they  are  very  strongly correlated with real and immediate issues.    Mechanistic remediation actions should be automated whenever they can be.   If  the  documentation  is  insufficient  or  the  problem  complex,  incidents  can  be escalated to the team that owns the service. This happens infrequently enough  72      Chapter 6: How to Apply SRE Principles Without Dedicated SRE Teams   that these teams usually do not have a separate rotation, but just spray the notifi‐ cation to everyone on the team.    During the workday, the on-call engineer can delegate dealing with any pages to the team that owns the service. This relieves that engineer from figuring out what changes are being made to this service at the moment.    It is the responsibility of the team that owns the service to prevent problems from  occurring in the first place.  All of those together not only keep the shared on-call rotation viable, they also help the productivity and health of the developers who own the service. Having developers on call imposes an even stricter requirement to follow the best practices than dedica‐ ted SRE support. An SRE is at least a full-time SRE, whereas a developer with SRE responsibilities can wear their SRE hat for only small fractions of their time. Through  the  framing  and  incentive  of  reduced  on-call  load,  it  was  attractive  for teams to invest into their documentation and monitoring. Soon, more consolidated on-call rotations formed, often by groups of teams with related features and similar engineering cultures such as language and framework choice. That the team owning a feature is ultimately responsible for the operation of all of the services related to it provides an important safety valve in this setup. Although teams have  the  freedom  to  weigh  short-term  feature  delivery  against  long-term  mainte‐ nance and tech debt, they cannot neglect the latter forever. As services reach the lim‐ its of their design, or as badly designed services are created, the operational load on the team increases. This slows down future feature delivery—drastically so if the sit‐ uation becomes so bad that the services are removed from the consolidated rotation and on-call is again the responsibility of one team alone. Resolving Cross-Team Reliability Issues by Using Postmortems Even with the best of intentions, implicit instrumentation, and generic alerts, things do go wrong between systems owned by different teams. Maintenance has unexpec‐ ted  side  effects,  alerts  are  missing,  or  a  new  feature  critically  depends  on  a  system whose owners are unaware of the new dependency. A company-wide, firm but lightweight postmortem process7 is the basis for address‐ ing these issues. All teams document serious incidents, which helps them to prioritize and justify efforts to improve their own systems and their interactions with others. Integrated engineering teams that are responsible for maintaining their own systems have a strong incentive to address recurring causes of incidents, especially when esti‐ mating and reporting the time lost to incident response or toil that could have been  7 Site Reliability Engineering: How Google Runs Production Systems, Chapter 15.  Some Implementation Details      73   used for feature work. At our size, it is possible for ProdEng to review all incidents company-wide and identify issues that occur across teams or that might be solved by a cross-team effort. Postmortem  documentation  follows  a  specific  format  guided  by  a  template  in  the internal wiki. We prioritize ease of use for humans over machine readability, but do also extract some automated reports. A weekly meeting is the forum for discussing recent incidents, resolutions, and future improvements. The meetings are attended by at least one representative for each inci‐ dent on the agenda, ProdEng, and anyone else who wants to join. The majority of attendees are engineers directly involved in the development of the affected services. The wider perspectives of the attendees often bring new insights into the incidents at hand, or make connections between past and current incidents. Postmortem meetings are an efficient way to spread knowledge about the patterns and practices that cause or avoid incidents, help to resolve them, and mitigate their impact. Here, smaller organizations have a big advantage over large corporations: if there is one meeting for the entire organization, such knowledge can reach into all teams within a very short period of time, without the need for elaborate processes or enforcement through management. If  there  are  too  many  incidents  to  discuss  in  a  reasonable  time,  ProdEng  curates based on noteworthiness or impact. Incidents with a very high impact are usually dis‐ cussed, but this is not absolute—often, knowledge about a close miss and why it did not  become  a  catastrophe  is  more  important  to  share  than  a  well-understood  and accepted  failure  scenario  with  a  clear  resolution.  In  the  end,  all  incident  reports undergo a review of one sort or another, even if not all of them are subject to a meet‐ ing. Meetings dedicated to a single incident are held only in extraordinary circum‐ stances and feed into the general postmortem meeting. In  general,  and  especially  for  cross-team  issues,  it  can  be  easy  to  become  stuck  on assigning blame. The postmortem guidelines take this into account by focusing on the systemic issues that led to the event, finding ways to avoid the class of problems altogether, and attempting to understand the originating context and how it might be improved. The engineers who attend the meeting take these learnings back into their teams. The meeting often inspires additional means of knowledge sharing, such as internal tech talks, documentation, or pairing sessions. Uniform Infrastructure and Tooling Versus Autonomy and Innovation Historically, SoundCloud’s technology stack has been quite diverse, especially when it comes to languages and runtimes. This has its advantages. Freedom of choice keeps developers happy and makes recruiting easier. Tools can be picked very specifically for the project at hand.  74      Chapter 6: How to Apply SRE Principles Without Dedicated SRE Teams   However, in the long run there is a price to pay. Transitioning systems between own‐ ers often means that the new owners need to learn yet another new set of technolo‐ gies, and a lot of wheels need to be reinvented several times. Systems in less common languages  will  often  lack  the  more  sophisticated  instrumentation  and  client-side load-balancing capabilities that we need to maintain a fleet of microservices. At SoundCloud, technical direction more often emerges from the bottom up than it is decided from the top down. In the transition away from being a “Ruby shop,” we had  opened  up  to  and  recruited  from  many  different  language  and  platform  com‐ munities. In this culture, it was unlikely that simply declaring a set of allowed tech‐ nologies  would  be  accepted.  However,  the  Cambrian  explosion  of  technologies caused an unsustainable fragmentation of efforts around tooling and frameworks. To rein this in, management began favoring the Java Virtual Machine  JVM , Scala, and Finagle stack over others by allocating resources to maintaining this ecosystem. How‐ ever, we chose JSON over HTTP as the primary communications protocol over more efficient contenders such as ThriftMux. This lingua franca of service-to-service com‐ munication allowed us to go without immediately rewriting all the nonconforming services, and leaves room for experimentation with other languages and technologies. As a direct consequence, Go has gained a foothold for certain use cases for which its simplicity and performance outweigh the disadvantage of not benefiting from all the effort that went into the Scala framework. This approach also extends beyond language choice. By providing an easy path for anyone following the common practices, development teams have a strong incentive not to introduce new technologies—but they can if the benefits outweigh the costs of blazing a new trail. This also extends into the domain of data stores. Certain database and cache technol‐ ogies   Percona  Server  for  MySQL,  Cassandra,  and  Memcached   are  maintained  as easily reusable, shared components. They allow any team to stand up a cluster very quickly and benefit from all the automation, documentation, and support of a widely used and well-understood tech stack. As  mentioned  earlier,  although  the  setup  is  provided  for  them,  teams  are  wholly responsible for the day-to-day on-call and maintenance of their dedicated infrastruc‐ ture dependencies. They do not need to ask for permission or coordinate with any‐ one for changing a schema or creating a new feature. Consultation for schema and capacity planning is voluntary and often not necessary. In part, this is possible by vir‐ tue of physical separation. Dedicated database clusters for each system limit the blast radius  of  an  ill-advised  change  or  a  lack  of  capacity.  The  increase  in  development velocity  and  the  more  predictable  planning  that  results  from  this  autonomy  more than make up for the slightly reduced utilization of single-tenant clusters. Sometimes, though, a new technology can really solve a problem so much better that it is worth taking up. This is a decision that each team makes for itself. As an exam‐  Some Implementation Details      75   ple, the Data Platform team adopted Apache Kafka to power the central event bus, replacing  the  well-established  RabbitMQ.  Soon,  Kafka  gained  traction  with  other teams as well, who could build on the prior work of the original explorers. However, maintaining a generic solution that works for many teams can become a strain on the resources of a team that should be focusing on the actual product. At this  time,  it  makes  sense  to  hand  over  the  technology  to  a  team  oriented  toward maintaining the common technological basis. A dedicated team was founded to sup‐ port the common backend framework for JVM-based languages. Monitoring is one of the main “products” of the ProdEng team, mostly in the form of templates and automation for running Prometheus. Shared components can go a long way and make teams autonomous in areas that are often guarded by dedicated teams providing them as a service. How far you want to go in this is up to you. For SoundCloud, ProdEng provides the deployment platform to all teams as a service. It is, however, not mandatory to use it—certain applications live outside of it by choice of the teams that maintain them. Getting Buy-In There is a common theme in the way in which Google ensures SRE practices are fol‐ lowed. Rather than enforcing “golden rules,” the preferred way is to create an incen‐ tive  structure  that  aligns  following  best  practices  with  the  natural  desire  of  an engineer  to  increase  their  own  efficiency  and  productivity.  The  result  is  a  low- overhead self-regulation. The proverbial example is the introduction of error budg‐ ets8 rather than SRE-mandated stop gaps for releases. Where the self-regulation is not sufficient, a Google SRE team does not need intervention of higher management, but can simply threaten to return the pager. The privilege of SRE support outweighs the burden of following rules. Does that mean management support is not needed at all? On the contrary. Although error  budgets  or  the  “nuclear  option”  of  returning  the  pager  foster  self-regulation, putting those mechanisms in place and making them work requires extraordinarily strong management support, capable of mandating top-down decisions if need be.9 Introducing SRE principles into an established organization will first face the chal‐ lenge of how to get buy-in by management on the various levels before even getting to the problem of how to incentivize individual engineers. At SoundCloud, the situation was, again, a bit special. Autonomy of individual teams has been deeply rooted in our engineering culture from the beginning. Conway’s law  8 Site Reliability Engineering: How Google Runs Production Systems, Chapters 1 and 3. 9 Site Reliability Engineering: How Google Runs Production Systems, Chapter 1  p. 7 .  76      Chapter 6: How to Apply SRE Principles Without Dedicated SRE Teams   was often quoted when it came to rationalizing our move to a microservices architec‐ ture, but it was also used in reverse to justify the team autonomy as a good fit for our engineering approach. This aspect of our engineering culture was both good and bad for the introduction of SRE principles. The good part was that there was little resist‐ ance to overcome. Senior management was, in general, very open to experiments and welcomed any attempt to tackle our increasing reliability problems. The bad part was that  centrally  introducing  and  enforcing  any  rules  or  even  a  framework  like  error budgets  was  difficult.  Getting  buy-in  from  team  leads,  who  cherished  their autonomy, was difficult, not to mention individual engineers. This problem was not limited to SRE practices. Enforcing anything that is contrary to your engineering culture is usually a bad idea, not only because you  hopefully  like your own culture. It’s much better if you can alter your approaches so that they play well with the existing culture or even gently encourage an organic cultural evolution toward a more mature organization. The  episode  of  embedded  SREs  that  we  described  earlier  is  a  perfect  case  study. Nobody  really  opposed  the  idea.  However,  there  was  an  implicit  expectation  that every individual team would independently change its proven habits to make good use of its embedded SRE. In hindsight, that was overly optimistic. Even during the era of the Platforms team, a frequent complaint by engineers on the team was that their advice was ignored or not even requested in the first place. Both cases prove that change is impossible if there is no incentive structure in place to drive it. Similar to Google SRE, we needed to create an incentive structure and self-regulation rather than mandating strict rules. However, for doing so, we could not rely on cen‐ tralized decisions and guidance; rather, we had to embrace a cultural environment that is very open to experiments and innovation and highly values team autonomy. In the success stories that we’ve related in this chapter, you can see the repeating pat‐ tern of teams gaining more autonomy by following the new ways: by taking their own pager, they took things into their own hands. Subsequently, they could make better trade-offs between features and stability. By adopting the common deployment plat‐ form, they could push changes more quickly and reduce operational overhead at the same time. Introducing modern monitoring practices not only helped to detect and resolve  outages,  but  enabled  entirely  new  ways  of  debugging  and  optimizing  code. Monitoring is seen as a necessary evil in many places. At SoundCloud, it is by now almost seen as a privilege to have tools like Prometheus well integrated in the infra‐ structure, with direct support by domain experts in the ProdEng team. Some other helpful aspects of our engineering culture have evolved over time:    There is a strong notion of “learning from one another,” which sometimes coun‐ teracts or even supersedes the desire for autonomy. Soliciting and receiving help from others has always been an important part of the SoundCloud experience,  Some Implementation Details      77   but  has  become  even  more  important  over  time.  Opportunities  to  learn,  like internal  tech  talks  or  the  postmortem  reviews,  are  well  received,  even  without coercion by management.    Perhaps  related  to  the  previous  point,  “leading  by  example”  often  works  well after success has been proven. For example, the adoption of Prometheus initially required a lot of hard persuasion. It only gained traction after monitoring as the base of Dickerson’s Hierarchy of Reliability was sufficiently established to visibly improve our availability.    The lack of alignment and cooperation between teams became a constant topic of complaint during retrospectives. At some point, the collective consciousness of the  company  gained  the  insight  that  some  agreement  to  common  practices  is strictly necessary. The stronger alignment was certainly helping our SRE efforts but had a much broader impact on many aspects of engineering at SoundCloud.  Conclusion For developers at Google, one of the most attractive aspects of getting SRE support is freedom  from  the  operational  burden.  At  first  glance,  we  did  exactly  the  opposite: instead of relieving developers of the operational burden, we put them in charge of it. However,  while  doing  so,  we  embraced  the  fundamental  principle  of  SRE  all  the more: “SRE is what happens when you ask a software engineer to design an opera‐ tions team.”10 The bottom line is that, in SoundCloud’s scenario of a mid-sized engineering organi‐ zation in charge of a relatively broad and diverse spectrum of features, dedicated SRE teams might not work out as intended. But even without dedicated SRE teams, you can, and in fact must, apply SRE principles, precisely because your software engineers must wear an SRE hat during a good chunk of their work. Comprehensive monitor‐ ing, low-noise alerting, and decent operational documentation reduce the workload so that features can be shipped without offloading operations to another team; pre‐ ferring  certain  platforms  and  a  common  postmortem  process  foster  cohesion  and exchange of knowledge; and a well-placed team in a consultative role does not need veto powers to maintain availability.  10 Ben Treynor in Site Reliability Engineering: How Google Runs Production Systems, Chapter 1  78      Chapter 6: How to Apply SRE Principles Without Dedicated SRE Teams   Further Reading   Some of the initial impulses for these developments came from former Googlers, many of them SREs. The context from which they came from is well explained in Site Reliability Engineering: How Google Runs Production Systems.    Our postmortem process is based on the one developed at Etsy in the early 2010s. A good starting point is the blog post that popularized it. More in-depth, Etsy has also published a “Debriefing Facilitation Guide”. A notable differences in our version of this process is that we do not hold separate meetings for every inci‐ dent.    Decoupling production readiness reviews from the initial release of a feature was inspired  by  Susan  Fowler’s  Production-Ready  Microservices   O’Reilly,  2016 . Until its release, we had struggled to come up with a definition of “good” services that would not be seen as a burden. Although we place much less emphasis on standardization, the book provides a good starting point for questions to con‐ sider when building or owning a service.  Björn Rabenstein is a production engineer at SoundCloud and a Prometheus developer. Previously, Björn was a site reliability engineer at Google and a number cruncher for science. Matthias Rampke joined SoundCloud in 2011 to help with internal IT. As part of the Systems and Production Engineering teams, he has been operating, debugging, and fre‐ quently restarting the glue that holds SoundCloud together since 2013.  Further Reading      79    CHAPTER 7 SRE Without SRE: The Spotify Case Study  Daniel Prata Almeida, Saunak Jai Chakrabarti, Jeff Eklund, David Poblador i Garcia, Niklas Gustavsson, Mattias Jansson, Drew Michel, Lynn Root, Johannes Russek, Spotify  Many people are surprised that Spotify does not actually have an SRE organization. We don’t have a central SRE team or even SRE-only teams, yet our ability to scale over time has been dependent on our ability to apply SRE principles in everything we do. Given this unusual setup, other companies have approached us to learn how our model  “Ops-in-Squads”  works. Some have adopted a similar model. Let us tell you a bit about how we came to this model and how it works for us so that you can see if a similar idea would work for you. First, we need to share a little context about our engineering culture: at Spotify, we organize into small, autonomous teams. The idea is that every team owns a certain feature or user experience from front to back. In practice, this means a single engi‐ neering team consists of a cross-functional set of developers—from designer to back‐ end  developer  to  data  scientist—working  together  on  the  various  Spotify  clients, backend services, and data pipelines. To  support  our  feature  teams,  we  created  groups  centered  around  infrastructure. These infrastructure teams in turn also became small, cross-functional, and autono‐ mous  to  provide  self-service  infrastructure  products.  Some  examples  of  these  are continuous  integration,  deployment,  monitoring,  software  frameworks,  and  guide‐ lines. The vast majority of the SREs at Spotify work in these teams, using their skills and experience to make the production environment reliable, scalable, and easy to work with for our feature teams. However, some concerns of SRE are cross-cutting and can be addressed only from a central perspective. This includes outages caused by large cascading failures, teaching best  practices  for  deployments,  incident  management,  or  postmortems.  Our  SREs  81   organize  themselves  in  working  groups  across  the  company,  but  these  working groups are not exclusively engineers with the SRE title. Only half of the engineers on our central escalation on-call rotation  internally referred to as Incident Manager On Call, or IMOC , for instance, are SREs; the rest are engineers with various roles. Why have we organized ourselves in such a way? What are the benefits of doing so, and what are the trade-offs? In the following sections, we discuss how Spotify built its SRE organization from a tiny startup with servers in a Stockholm apartment to the large global company that it is today. We highlight how Spotify has made operations the default among all engineers by providing a frictionless development environment and a culture of trust and knowledge sharing. Tabula Rasa: 2006–2007    One ops engineer   Seven developers, approximately nine backend systems at the time of the invite-  only beta release  Prelude We’ll talk a bit about how we began to incorporate an operations focus in our early history, including: Ops by default  Unintentionally  bringing  in  an  operations  focus  from  the  beginning  has  influ‐ enced our engineering culture, proving to be beneficial in the future.  Learning to iterate over failure  Although we might have had the foresight with operations, we were not impervi‐ ous to the common traps that startups fall into.  One of the curious aspects about the Spotify story of operations and SRE is how the initial staffing of the six-person company included an operations engineer. Many startups add an operations-minded person only after the first customers begin using the service. The unfortunate ops engineer might then discover a backend hob‐ bled  by  undocumented  scripts;  services  running  inside  of  screen  sessions;  lack  of backups; single points of failure; and layers of unfinished, good intentions. From this point on, the ops engineer might constantly be playing catch-up, trying to fight fires while keeping abreast of new developments. Including an ops engineer from the beginning, ensured that operational soundness, by extension, was included in discussions as an equal partner, not left until the last  82      Chapter 7: SRE Without SRE: The Spotify Case Study   minute.  From  the  get-go,  dev  and  ops  worked  side-by-side  toward  our  common vision of streaming music to the entire world. This initial way of working led to a cul‐ ture of collaboration and trust, which continues to thrive today. Our  backend,  as  originally  envisioned,  consists  of  many  small  services—a  pattern now called microservices—which together provide the content for the Spotify clients. The microservices themselves are programs that do one thing and do it well. During these first two years, work was distributed between dev and ops as follows: developers took care of the business logic in the form of client features or backend services, whereas ops owned everything else, including detecting problems in produc‐ tion as well as taking care of office technology. This was a startup, and as you might imagine, almost everything was done manually, like deployment of backend services and desktop client rollouts. As Spotify grew in the beginning, tools were added to lessen the load, but these tools only helped with manual tasks; humans still needed to make all the critical decisions. Key Learnings Some of our key learnings from this early period were:    Including  an  ops  engineer  from  the  get-go  influenced  the  engineering  culture  and proved highly beneficial as we grew.    You  should  introduce  an  operational  mindset  into  the  engineering  culture  as  early as possible.    Aside from business logic architecture, ensure that an infrastructural architecture  is in place, as early as possible.  Beta and Release: 2008–2009    Three ops engineers   ~10 backend engineers   1 data center   10–20 backend services   4 clients: Windows, Mac, iPhone, libspotify  Beta and Release: 2008–2009      83   Prelude In this section, we’ll talk about how our “Ops by default” philosophy shifted and how that links up with our core engineering values: Ops by default  Bringing in operations into regular discussions of scalability and reliability with developers from the start was pivotal to creating the foundation of our engineer‐ ing approach. Operations was a cornerstone in every engineer’s work, whether it was  in  developing  and  maintaining  services,  or  in  the  intrinsic  urge  to  help improve our infrastructure.  Core engineering values  Innately trusting engineers set the foundation for one of the most prevalent prin‐ ciples we continue to have in the tech organization: autonomy.  Spotify went into “invite-only beta” in May 2007, and a premium  and “freemium”  option was released in 2008. During this period, Spotify experienced its first real dose of the dreaded problems that come with rapid user growth. At scale, pain points that are rare and mostly theoretical become visible. They applied not only to the backend services, but all the way down the stack. Here are a few examples:    The backend services had capacity-related outages during peak times.   There were disk I O performance issues when the battery of a RAID controller  failed.    A ZFS bug caused the servers responsible for critical data to become unrespon‐  sive under high utilization.    Racking and stacking servers was painfully slow.  Adding to the problem, the number and complexity of the backend services rose as Spotify developed new features. With time and a lot of effort, each of these technical issues was solved, and the backend continued to scale with the number of users. However,  the  way  in  which  ops  engineers  initially  worked  did  not  scale  as  well  as Spotify did. There were three ops engineers at the time who owned all of the follow‐ ing:    Keeping an eye on all the services that were handed over to ops from dev   Maintaining the underlying Linux server configuration   Responding to service disruptions   Ensuring incident remediations were prioritized   Keeping up with system security   Maintaining networking equipment and configuration  84      Chapter 7: SRE Without SRE: The Spotify Case Study     Releasing new Spotify desktop clients and monitoring for deployment failures   Data center management including procurement, vendor relationships, and pro‐  visioning    Maintaining and expanding storage   Maintaining our data infrastructure and pipelines   Owning  office  IT   network,  firewall,  staff  computers  and  peripherals,  Google  apps, LDAP, Samba, Kerberos, AFS, wiki, printers, etc.     Serving as a support center to all of our colleagues; for example, helping with lap‐ top  setup,  explaining  networking  configurations  and  how  TCP IP  works,  or helping our peers work with system monitoring graphs  There was quite a lot to do, but, at least initially, operations still had a little time left over for deep discussions like the future of IPv6, or bickering over the principles and pragma  of  the  different  free  software  licenses.  However,  as  the  different  scalability issues hit us in increasingly large waves, it became clear that operations would fail spectacularly at keeping Spotify running if something wasn’t done. Bringing Scalability and Reliability to the Forefront Until this point, there were many people at Spotify who wanted to help with scalabil‐ ity and reliability-type problems, but only the ops staff was ultimately accountable for the work. But the ops team had too many responsibilities to juggle and needed to dis‐ tribute those to the rest of the organization. A weekly meeting was introduced during which all relevant backend developers and at least one ops engineer discussed services that were dealing with scalability issues. Every week, one or more services were discussed, and developer stories to scale up a system would usually trump feature work. Through this meeting, the ops engineers got some indication of which types of servers  high in disk space, CPU, or memory  was needed for the next procurement purchasing cycle. The flavor of this meeting changed over time as dev and ops worked together. A sin‐ gle service’s scalability and reliability issues involved more and more dependencies on other services, so looking at each service in isolation was not enough; the focus of the meetings began to shift to the backend ecosystem as a whole. Although we didn’t know this at the time, a transformation was taking place in our organization.  Gradually,  the  sense  of  responsibility  for  ensuring  system  health  was moving from being an ops-only job to something shared by all engineers working in the backend, regardless of role. At this point in the company’s history, some of the original backend developers had root  access.  Seeing  how  useful  this  was,  we  gave  root  access  to  all  developers  who  Beta and Release: 2008–2009      85   needed it. This level of access was unheard of in most traditional companies. Since the advent of DevOps, this is now common practice, but it wasn’t at the time. Yet it never occurred to us to do it differently; we innately trusted our engineers. The com‐ pany was young, and we were united in the belief that we would take over the world of music. By the time Spotify went out of beta, a new service would go from idea to production in the following way:  1. A developer would write and locally test the service. 2. The developer would then ask ops for one or more production servers. 3. After the servers were provisioned, the developer would log in to the host and  configure the service and its dependencies.  An unintended benefit of this new flow was that developers could now understand how  their  services  behaved  in  the  wild  better  than  before.  The  entire  problem  of, “Well,  it  worked  on  my  computer”  was  bypassed.  Developers  could  now  see  how their  code  behaved  in  production,  view  logs  as  they  were  produced,  trace  running processes, and even modify the operating system if necessary. Looking back, we were unintentionally disassociating the specialized role of the SRE from the individuals in ops into a set of skills and responsibilities that could be trans‐ ferred  onto  anyone  who  was  capable  and  motivated.  Over  the  years  to  come,  we reused this strategy multiple times with increasing scope and depth. And this worked—for a time. Key Learnings Our key learnings from this period of high growth were:    Make operations part of the solution life cycle. Scalability, reliability, maintaina‐ bility, and other operational qualities should be discussed and addressed as early as possible.    Granting privileged access  e.g., root  to everyone who needed it removed fric‐ tion, unblocking and speeding up our iteration cycles. Reliability is about achiev‐ ing trust: start by trusting your own.  86      Chapter 7: SRE Without SRE: The Spotify Case Study   The Curse of Success: 2010    Five ops engineers   <25 backend engineers   2 data centers   A few hundred machines   ~20 backend services   7 clients  Windows, Mac, iPhone, Android, BlackBerry, S60, libspotify   Prelude In this section, we’ll talk a bit about how we had to shift our operations’ mentality as we grew: Ops by default  Introducing  the  roles  of  dev  and  ops  owner  helped  us  perpetuate  our  natural inclination to be mindful of operations in the context of feature development.  Iterating over failure  We could not scale ourselves as an operations team fast enough, and therefore had to divest some of our responsibilities.  Spotify continued to grow in popularity during 2010. This was reflected externally in an increasing number of registered users and concurrent active users, and internally in the number of features and corresponding backend services as well as more Spotify staff. This twin growth conundrum—a sharp upturn in both users and staff—hit the opera‐ tions team very hard. Despite growing from three to five people, ops was facing a per‐ fect storm. The increase in users led to more pressure on the backend, magnifying the risk of edge-case failures, and ultimately leading to more incidents and cross-service failures. At this time, the increase in Spotify staff was evenly split between technical and nontechnical, resulting in a higher demand for quality support with shorter lead times. In  addition  to  these  factors,  with  more  skilled  developers,  features  were  being churned out much more quickly. In 2009 and 2010, Spotify released five more clients: our users could now access Spotify on an iPhone, Android, BlackBerry, S60 Nokia phones, and libspotify  a library enabling third-party developers to access the Spotify backend  through  their  own  applications .  The  backend  saw  more  feature  work  as  The Curse of Success: 2010      87   well, but the primary focus of 2010 was scalability and stability. Every system needed some sort of revamp, be it modification to enable easy horizontal scaling, a caching layer, or even complete rewrites. Each frontend and backend change to the Spotify ecosystem introduced new bugs, revealed new bottlenecks, and changed how the system as a whole behaved. Adding a new feature in the client caused unmodified and historically stable services to experi‐ ence sudden pressure due to changing user behavior patterns. This in turn often led yet other systems to topple in a domino-like fashion. Complex systems are unpredict‐ able and difficult to manage, and, in our case, only a handful of people knew how the entire ecosystem fit together. Responding to unforeseen incidents became more and more of a full-time job. When Spotify experienced periods of backend instability, developers would often show up at the desk of an ops person, needing critical support. This prevented ops from creat‐ ing tools to simplify and automate work. In addition, our model of enabling the back‐ end developers with maintenance wasn’t working as well as it should have. Although developers  continued  to  maintain  their  systems,  pressure  was  steadily  building  to deliver features, and the informal and best-effort agreement of upkeeping their serv‐ ices suffered, often resulting in out-of-date components, bitrot, and security issues. In  the  meantime,  our  developer  colleagues  were  dealing  with  their  own  organiza‐ tional problems. What was originally a small team of developers with a shared under‐ standing on how to collaborate to deliver code grew to become multiple development teams.  In  this  new  setting,  developers  found  that  collaboration  between  teams  was difficult without agreeing on things like sprint commitments. As a result, teams felt a greater  sense  of  urgency  to  deliver  what  they  committed  to.  An  unfortunate  side effect of this was that many developers became frustrated by not being able to fulfill their informal agreement with us to maintain their services. At this point, operations became increasingly aware of how the success of the product translated into countless nights by the laptop, manually catching up with failing sys‐ tems struggling with the load. All of this led to experimenting and, ultimately, adapt‐ ing to a new reality. A New Ownership Model We needed to clarify responsibilities across our operations and developer roles.  The dev owner role In early 2010, we formalized the unofficial, best-effort agreement between ops and dev by introducing the dev owner role. Each service had an owner who worked in the feature team itself. Here’s what the dev owner’s responsibilities were:    Keep operating systems up-to-date with latest updates  88      Chapter 7: SRE Without SRE: The Spotify Case Study     Think about scalability of the service on our ecosystem   Ensuring development would be scheduled to keep up with growth  Feature  developers  were  also  allocated  one  day  during  a  sprint—a  “system  owner day”—when they were given dedicated time to maintain, upgrade, and generally nur‐ ture their services. The dev owner for each service was expected to call on us for help if  needed.  This  eased  some  of  the  pressure  on  ops.  This  was  not  a  controversial change, because many backend developers were already motivated to maintain their own services; this now ensured that they had the dedicated time to do so.  The ops owner role The dev owner role helped us to spread the responsibilities of basic maintenance onto the  largest  group  of  engineers  at  the  company,  but  if  something  went  seriously wrong, ops was still accountable. However, there were many systems and still only five in ops. Because some of the more critical services needed more attention than others, we assigned an owner from ops to each of these important services. This way, even though every service had a dev owner, the significant ones had both a dev and an ops owner, enabling two peo‐ ple to focus on the services’ health. Within the ops team, everyone was an owner of at least one service. Ops was expected to know the idiosyncrasies and common failure modes of any of our services and be ready to be called at any point, day or night, to restore the service back to health. When redesigning a service, the dev and ops owner would often work together. Formalizing Core Services Because  our  backend  services  were  deeply  interconnected  with  one  another,  most incidents affected multiple services; as a consequence, often one system needed to be sacrificed to save another. One of the hardest things to do was make a decision in the middle of the night, alone and sleep deprived, that might adversely impact the entire backend  ecosystem.  Each  of  us  asked  ourselves  multiple  times  every  week:  “Am  I making the right decision? What if I don’t have the complete picture?” This made work during the night very draining. When  we  finally  identified  this  as  a  problem,  a  hierarchy  of  services  was  defined: those that were critical, those that were important, and those that could wait for free cycles. Out of this process came the “core services” concept. Because Spotify’s pur‐ pose was to enable people to listen to music, a core service was defined as any back‐ end service that was in the critical path from a user logging in to client-side audio playback.  This  amounted  to  a  handful  of  services,  residing  on  a  small  cluster  of servers. In addition to the core user-facing services, the infrastructural systems, such  The Curse of Success: 2010      89   as our border routers, backend network switching, firewall, and DNS, were deemed critical. By making these designations, we lessened the burden of nighttime work, and could deal with decision-making in a healthier way. Blessed Deployment Time Slots Because ops had yet to formalize an on-call procedure, we were careful with making deployments during off hours. The norm was to make sensitive deployments during work  hours  when  many  people  were  available  to  help  out  if  and  when  something went terribly wrong. This applied to all of our deploys with the exception of changes to network infrastructure, which was done when most of our users and employees were asleep. Feature teams were encouraged to avoid any deployments on Fridays. This allowed us to have an honest chance of resting during the weekends. On-Call and Alerting Up to this point, everyone and no one was on call. Ops had no system in place to automatically send alerts on system failure. If something went down in the middle of the night, it was left broken until the morning when someone checked the graphs and reacted. In the event of an outage we hadn’t noticed ourselves, our colleagues would call one of us and we would start the work of identifying the problem. As a result, going to sleep was difficult because we didn’t know if we would be called. The stress affected our decision-making abilities at night as well as our capacity dur‐ ing work hours to operate well—or at all. When ops grew to five people in early 2010, we could finally do what we should have done a long time prior to that but couldn’t due to workload: install a proper alerting system.  This  solved  one  of  our  key  issues:  it  didn’t  leave  the  detection  of  system anomalies and outages to chance. We could now proceed with the next step in our plan, starting with defining a weekly on-call rotation. With one person on call for all backend service disruptions, the rest of us in ops were finally able to sleep, with the caveat that we might be looped in if the incident involved a core system for which we were the ops owner. Any incident con‐ cerning a noncore system was not a priority, and there was no need to fix the prob‐ lem  in  the  middle  of  the  night.  Instead,  we  did  some  basic  troubleshooting  to understand the problem; then, we let the service be until the following workday to be dealt with by the appropriate people. Finally, the on-call person had the mandate to call the CTO, who in turn could call anyone in the company should the need arise.  90      Chapter 7: SRE Without SRE: The Spotify Case Study   Not completely pain-free Even though an on-call rotation was defined and alerting set up, every day was full of surprises with unplanned work, issues, and outages. Firefighting and troubleshooting took precedence over any other work. Being on call meant carrying the pager for the whole of Spotify, alerting whenever some metric went over or under statically defined thresholds. If we weren’t paged while on call, it could only have meant one of two things: either we’ve run out of SMS credits, or our cell phone provider blocked our texts due to a high rate of alerts that we were sending. On-call fatigue was constant, but was alleviated by the company culture and camar‐ aderie: anyone was willing to step in if the on-call engineer was tired and needed time off after hours of firefighting. It was also fun to troubleshoot outages happening in the most unpredictable ways. Learning through failure is an essential part of a healthy engineering culture. Spawning Off Internal Office Support As mentioned earlier, a large part of our workload in ops until this point had been internal  office  IT  systems  and  support.  Due  to  our  growth,  some  weeks  we  would have four new colleagues, each eager to get started, who needed a computer, a phone, LDAP credentials, email, an introduction to our wiki, and the usual password secu‐ rity lecture. On top of this, our office network and shared filesystems needed mainte‐ nance and polishing. Supporting nontechnical colleagues took time and patience and took focus away from our never-ending efforts at keeping backend entropy at bay. To better support our use cases, Spotify finally split the ops team into two: produc‐ tion operations and internal IT operations. This enabled each team to concentrate on its respective work. Addressing the Remaining Top Concerns We’ll talk a bit about some of the issues we faced at this stage and how we improved.  Long lead times Despite splitting our internal IT support to another team, we still had a large number of requests every week, including developers needing help with the maintenance of their services, addressing network issues, coordinating with external parties of how to integrate with the Spotify backend, and maintaining communication with data center and hardware vendors. All of these issues were dealt with in a best-effort way, which led, unsurprisingly, to very long lead times, and dissatisfied colleagues and customers.  The Curse of Success: 2010      91   Unintentional specialization and misalignment We unwittingly become domain specialists by chance. If one of us solved a specific problem, the person who asked for help would inevitably return to the same person the next time they encountered a similar situation. Others would hear about this, and when they had questions on the topic, they would turn to this same person. Not only did  we  become  knowledge  silos,  but  because  our  respective  approaches  were  not aligned  with  others’  outside  of  SRE,  our  solution  space  become  siloed  as  well.  For example, we had multiple deployment tools that were being used outside of SRE; the authors of one set of tools didn’t always know about the others’ work.  Interruptions Finally, there was a constant flow of interruptions, making our day-to-day work— sometimes requiring extended periods of time for analysis, planning, and implemen‐ tation—very difficult. Although we were becoming a larger group of engineers, the close-knit relationship between the dev teams and what was now production operations was still alive and kicking. Many tasks and requests were handled as if we were one small team: some‐ one looking for help, seeking guidance, or needing work done would walk over to our desks and ask for anyone around. Often, this would result in more than one person interrupting their current work, listening in, and joining the discussion. It was a fan‐ tastic way of collaborating, but the amount of context switches meant that we were much slower to make progress in our improvement work than desired.  Introducing the goalie role Our approach to solve these three problems was to introduce a new role: the goalie. The  goalie  was  rotated  weekly  and  served  as  the  designated  lightning  rod  for  all incoming  requests  during  office  hours  to  the  ops  team.  If  the  incoming  requests queue was particularly low, the goalie would attempt to solve all issues on their own, occasionally asking for help from the other ops engineers. If the queue was overflow‐ ing,  the  goalie  would  then  only  triage,  dropping  some  requests  and  passing  the remaining ones to appropriate people in the team. Rotating the goalie role minimized knowledge siloing because everyone was exposed to the most common problems. Creating Detectives One of the more creative and rewarding aspects of our work as SREs is when we roll up our sleeves and do forensics during and after a system anomaly. This detective work requires knowledge of every service in the ecosystem and how they fit together. A typical investigation could begin with peculiar behavior in one of our Spotify cli‐ ents, leading us to study a flailing backend service, which, when stabilized, proved to  92      Chapter 7: SRE Without SRE: The Spotify Case Study   be innocent. We’d then find a downstream system, which initially had seemed to be a model citizen, but in reality had triggered a chain of failures. Even  though  we  gained  more  and  more  knowledge  and  experience  from  jumping down rabbit holes and fighting complex and interesting incidents, we couldn’t keep up with the ever-growing backlog of remediations, leaving us exposed to repeating the same incidents again and again. To make matters worse, the influx of new devel‐ opers and services was adding to the rate at which incidents occurred. And though we  very  much  wanted  to  ask  for  help  from  our  developer  counterparts,  given  that they were experts on their respective services, most of them lacked the big picture. There were only a handful of SRE detectives, and an increasing number of incidents to study and resolve. We needed more detectives. A solution emerged: we could make more detectives by teaching developers how the backend  system  worked.  Out  of  this  insight  sparked  a  rather  popular  lecture  that came to be known as “Click-to-Play.” Initially, we explained the backend and how it worked to a few interested developers. We noticed that it was easier both for us to teach and them to learn if we followed a scenario that touched upon all the backend systems involved, starting when a user logged in and ending with when a song began to play. Eventually, this became a stan‐ dard component in the onboarding process for engineers at Spotify, and a shorter, nontechnical version is now taught to all Spotifiers around the globe. Key Learnings Our key takeaways from this period of high growth were:    Deployment windows work to protect ops, so leverage them wisely.   Alerting and on-call need to have processes and expectations in place. Fail fast  and learn through failure.    Teaching skills and responsibilities is an essential part of operations. Hold talks  on teaching how the entire system works.    It’s tempting to have a single team handle production and IT in an early stage,  but to be a productive SRE team, you should split it.    A “goalie” role or another formal way to handle interruptive work helps the team  focus on proactive work.  The Curse of Success: 2010      93   Pets and Cattle, and Agile: 2011    Eight ops engineers   <25 backend engineers   2 data centers   ~30 backend services   7 clients  Prelude In this section, we talk about how we needed to become more agile in the way we approached operations and how our values informed that growth: Agile ops  We needed to move from a mindset of treating servers like “pets” to one in which hardware clusters were “cattle”; this radically changed how we approached tool‐ ing and operations processes.  Core engineering values  Our inclination toward autonomy and trust from the start informed our ways of working. However, as we became comfortable with our operational processes, a larger  shift  was  underway  in  the  tech  organization,  which  would  cause  us  to again reevaluate our approach.  In the past, when we talked about services, we often talked about individual servers: “Server X’s disks are full”; “We need to add another CPU-heavy server to lighten the load of the other login servers”; and so on. This was important to us because each server  had  its  own  character  and  personality,  and  only  by  knowing  this  could  we really  optimize  the  usage  of  these  servers.  Furthermore,  each  healthy  server  was someone’s pet. You could tell which services were healthy by looking at how well the server  was  taken  care  of:  for  example,  whether  the   home  directory  was  regularly cleaned or the service-specific logs were nice and ordered. Our world was server-centric rather than service-centric, and this showed in our con‐ versations, our prioritizations, and our tools. Forming Bad Habits The tools initially built and used at Spotify to manage our nascent fleet assumed that a server, after it was installed and live, would continue to work in that corner of the  94      Chapter 7: SRE Without SRE: The Spotify Case Study   backend until it was decommissioned. It was also assumed that provisioning a server itself  would  be  an  unusual  event  in  the  grand  scheme  of  things—something  done only a few times a month. Therefore, it was acceptable that the tool that installed a server took a few minutes to run, with frequent manual interventions. From begin‐ ning to end, the installation of a batch of servers could take anywhere between an hour to a full workday. By 2011, we were installing new servers far more often than anticipated. Repurposing servers was a known process, but there were often gotchas that forced operations to step in and fix things manually. This wasn’t strange or wrong for a small company with a handful of servers, but at this point in our journey, Spotify already had two data centers with live traffic and a third on the way. With far more than a handful of servers, this was becoming increas‐ ingly difficult to do. Breaking Those Bad Habits There was a paradigm shift in the air, and though some of us saw it coming, none of us really knew what this would entail for us and the availability of our services. This paradigm shift, when it finally hit us, was simple and obvious: instead of adapting our intent to the hardware, we needed to adapt our hardware to our intent. This shift was difficult for many of us to relate to because we were too comfortable in our  routines.  In  fact,  it  took  several  years  for  this  mindset  to  change  completely, probably  because  it  took  that  long  for  us  to  create  the  tools  to  make  this  shift  in mindset possible in the first place. Toward  the  end  of  2011,  another  largely  unnoticed  shift  began:  feature  developers were beginning to organize differently. Over lunches, we heard the devs talk about autonomous  self-organizing  teams  and  about  which  chapter  they  belonged  to.  We could still talk with them about services and users, but conversations now included a focus on products and stakeholders. The entire dev department was slowly making the transition into a scalable Agile organization, and we in ops observed this from the sidelines. From the perspective of those of us in ops, what was most jarring about this organiza‐ tional change was the ancillary effect it had on our role. The shift in nomenclature from  “dev  teams”  to  “squads”  with  the  added  emphasis  on  autonomy  and  self- organizing challenged the centrality to which we’d become accustomed. Squads were in turn grouped into so-called “tribes,” which also had explicit intent in being auton‐ omous and self-organizing. Instead of one organization to work with, ops was now faced with multiple tribes, each structured slightly differently, with worryingly few indications of staying homo‐ genous.  Pets and Cattle, and Agile: 2011      95   Key Learnings Some of our key learnings from this period were:    Switch the mindset from server-centric to service-centric and make the tooling    The introduction of the Agile matrix organizational model forced us to rethink  reflect that.  operations.  A System That Didn’t Scale: 2012    <25 ops engineers   <70 backend engineers   Three data centers, a few thousand servers   <50 backend services  Prelude In this section we’ll talk about our challenges scaling the ops team with the organiza‐ tion: “Iterating over failure”  Scaling effectively continued to prove difficult, and divesting our responsibilities was not enough. We needed to revisit what operations at Spotify meant.  “Ops by default”  A central ops team doing most of the operational work doesn’t scale. We needed to make ops truly default by completely shifting the operational responsibilities closer to developers.  It was now 2012 and the Spotify user base continued to grow, introducing new scala‐ bility and stability problems for us. The ops team was composed of fewer than 10 SREs  it was, in fact, during this year that the term “SRE” was adopted , unevenly split between Stockholm and New York. As an ops owner, each SRE was operationally responsible for dozens of backend serv‐ ices:  handling  deployments  of  new  versions,  capacity  planning,  system  design reviews,  configuration  management  or  code  reviews,  and  maintaining  operational handbooks, among several other day-to-day operational duties.  96      Chapter 7: SRE Without SRE: The Spotify Case Study   Our backend was now running in three data centers, with a fourth on the horizon, which we had to operate and maintain. This meant that we were responsible for con‐ figuring  rack  switches,  ordering  hardware,  remote  hands  cabling,  server  ingestion, host provisioning, packaging services, configuration management, and deployment— the whole chain from physical space through the application environment. As a result of this widening responsibility, we formed another team to develop tooling automa‐ tion,  which  worked  closely  with  ops.  One  of  the  early  products  developed  by  this team  was  a  configuration  management  database   CMDB   for  hardware  inventory and capacity provisioning. With  tailored  configurations  and  nonuniformity,  predictability  was  hard.  An  ops owner worked closely with the dev owner of a service in efforts to improve quality, follow  production  readiness  practices,  and  run  through  a  now-formalized  deploy‐ ment checklist to ensure that operational standards were present even during the ini‐ tial service design. Concerns we regularly brought up included the following:    Is the service packaged and built on our build system?   Does the service produce logs?   Is there graphing, monitoring, and alerting?   Are backups set up and restore tests defined?   Was there a security review?   Who owns this service? What are its dependencies?   Any potential scalability concerns? Are there any single points of failure?  Manual Work Hits a Cliff Even though we deployed constantly, either manually or via configuration manage‐ ment, continuous delivery was not in place. We were still delivering at a good pace, continuously, at the expense of work done by hand. Service  discovery  consisted  of  static  DNS  records  with  manually  edited  and  main‐ tained  zone  files.  DNS  changes  were  usually  reviewed  and  deployed  by  ops.  We achieved  mutual  exclusion  during  DNS  deploys  by  shouting  “DNS  DEPLOY!”  on IRC and manually executing a script. The ops owner and dev owner regularly went through a capacity planning spread‐ sheet to ensure that the service had enough capacity to sustain the current increase in usage.  Access  patterns  and  resource  utilization  were  collected  and  used  to  predict capacity needs according to the current growth. For an ops owner of dozens of serv‐ ices, that meant doing a lot of capacity planning.  A System That Didn’t Scale: 2012      97   During the second half of 2012, we hit one million concurrent users. That meant a million people listening to music connecting directly to one of our three data centers. A pretty huge feat. Looking back, that should be attributed to some early decisions like Spotify backend architecture being designed to scale, early client protocol backend optimizations, and good implementation that paid off. Also, with no multitenancy, most failures could be  easily  pinpointed  and  isolated.  Every  backend  service  did  one  thing  and  did  it right. We kept it simple. As the number of users streaming music continued to increase exponentially, the ops team couldn’t do the same. We found that the centralized ops SRE team was a system that could not scale. Key Learnings Our key learnings from this period were:    Make ops truly default from the get-go; if you build it, you run and operate it.   Shift operational responsibilities closer to the know-how: the developers.   As your service ecosystem scales, make sure to revisit how operational work is  scaling. What worked well yesterday might not work well today.  Introducing Ops-in-Squads: 2013–2015    <50 ops engineers   <150 backend engineers   3 data centers   60 backend services   1 cloud provider for staging environment  Prelude In this section we talk about how a new approach to operations reduced bottlenecks and allowed the tech organization to grow more rapidly: “Iterating over failure”  Adopting the new model of Ops-in-Squads freed us up to focus on minimizing our amount of manual work.  98      Chapter 7: SRE Without SRE: The Spotify Case Study   Core engineering values  Squads owning their own operations unwittingly helped us to maintain our val‐ ues  in  autonomy  and  trust.  Engineers  had  the  potential  to  inflict  widespread damage but were given the tools and processes to avoid it.  At this point, the engineering organization had become too large to operate as a sin‐ gle  team.  The  Infrastructure  and  Operations   IO   tribe  was  formed,  the  home  of teams focusing on delivering infrastructure for our backend developers and tackling the problems that came with operating at scale. One part of this tribe was called Ser‐ vice Availability  SA , mostly consisting of our engineers previously working in pro‐ duction operations. By 2013, SA consisted of four squads: security, monitoring, and two squads working on any other infrastructure tooling needed to provision and run servers. The rate at which we hired new developers and started new dev teams was too high for those four SA teams to keep up. More and more, the ability to deliver new or improved features slowed down due to the pace at which we could buy and rack  new  servers,  review  and  merge  Puppet  changes,  add  DNS  records,  or  set  up alerting for a service. Dozens of feature teams were impatiently waiting to get their changes out to our users but had to wait on us for code reviews or provision hard‐ ware for their services. We were also still on call for core services, and the operational quality and stability of those services suffered as we tried to keep up with our accumulation of tasks. The global “operations backlog” just kept on growing and growing.  Lightening the manual load After  looking  at  data  of  how  our  backend  engineers  worked  and  where  they  were most often blocked, we decided to focus on removing those blockers wherever possi‐ ble.  It  was  time  to  bring  back  the  small  startup  collaboration  with  mutual  trust between dev and ops and enable our feature teams to iterate fast while still keeping production reliable. One of the first areas to improve was the provisioning of servers. When servers were available in our data centers, we still had to bootstrap the operating system and con‐ figure  the  basic  environment.  We  had  some  basic  automation,  but  to  kick  off  that process,  the  backend  engineers  had  to  create  a  ticket  for  the  ops  team,  specifying which data center they needed, how many servers, and for what service. The goalie would then pick this up and use our set of duct-taped databases and command-line tools to kick off the provisioning process. After about 40 minutes of automated tasks, the servers were often ready for backend developers. Our first attempt at improving this process was a tool called provgun, short for “pro‐ visioning gun.” Instead of sending tickets to the ops team or hunting down an opera‐ tions  person,  teams  could  now  open  a  JIRA  ticket  to  kick  off  the  provisioning themselves. After a while, a cronjob would scan those open tickets and automatically  Introducing Ops-in-Squads: 2013–2015      99   kick off all the steps we previously did manually and then report back to the ticket when servers were successfully provisioned. This freed up time for the ops team to focus on other things like working on the next iteration  of  this  system:  a  custom  web  interface  with  hardware  configurability  and available  stock  exposed.  This  web  interface  would  show  the  queue  of  outstanding requests,  the  locality  of  servers,  rack  diversity,  and  the  current  stock  of  available servers in every cage of the data centers, giving developers more choice and a better understanding of how we built our hardware ecosystem. The  next  thing  that  was  tackled  was  the  DNS  infrastructure.  At  the  time,  DNS records  for  servers  were  manually  added  and  deployed  by  the  ops  team.  The  first thing the ops team did was to use a CMDB as the source of truth for determining what  server  records  should  be  automatically  added  to  the  zone  files.  This  helped reduce the number of mistakes made, like forgetting to add the trailing dot. When enough confidence was built that these zone files were accurate, they were automati‐ cally deployed to the authoritative DNS servers. This, again, freed up much of our time, and developer satisfaction increased. Services in our backend discover one another using DNS SRV records. These, plus any user-friendly CNAMEs, had to be manually added to the zone files—a tedious and error-prone task that still required an operations engineer to review and deploy the changes. To  remove  this  bottleneck,  we  considered  ways  in  which  we  could  automate  the review and deploy process. A basic testing framework was introduced in which one could express things that would be looked for in a review, such as “is the playlist ser‐ vice discoverable in our data center?” We also created a bot that allowed anyone to merge a change as long as the tests passed and it had been peer reviewed. This was a scary addition: a simple change in one file could impact an entire data center. There were some initial incidents in which a bad push by a team took down much  more  than  expected.  But  our  backend  developers  quickly  learned  about  the power and responsibilities that came with these changes; soon the number of mis‐ takes dropped to almost zero. Again, this reduced the time they had to wait for the ops team from days to minutes. After the success with DNS and server provisioning, the next piece of the puzzle was Puppet, the configuration management system that installed all the software on our servers  and  deployed  our  applications.  The  ops  team  had  long  before  accepted patches to Puppet but reviewed every commit before merging anything. For larger commits or complex systems, this meant that they could be stuck in review limbo for days until someone found enough time to review it. We tried the approach taken with DNS: merging your own change as long as your patch had a positive review from someone else. For the first few weeks, we were all  100      Chapter 7: SRE Without SRE: The Spotify Case Study   pretty anxious and monitored almost every commit that was merged. We soon found that we had little reason to. Giving backend developers a feedback cycle—of making a change,  merge,  deploy,  find  problem,  and  go  back  and  do  it  all  over  again—gave them that much more insight into many common problems we had and overall led to improved code quality and fewer mistakes. These  first  few  steps  in  making  the  teams  operationally  responsible  was  a  success. They could now get servers, add DNS records for those servers, and deploy configu‐ ration and applications to the newly provisioned servers by themselves. Not only did we remove major friction points and idle time for our developers, but the stability of the services increased as a result of this first shift from “build it” to “run it.” Building on Trust However, as we removed these “sanity checks” previously done by the ops team, we faced a new problem. Without the guidance of the ops team, the entropy in our back‐ end exploded. Poorly tested services and simple experimentations made it into the production  environment,  sometimes  causing  outages  and  paging  our  ops  on-call engineers who now lacked the required understanding of what was running in pro‐ duction. We needed to figure out how to get the knowledge and operational responsi‐ bilities back into one place, paging the right people to troubleshoot an incident. The traditional ops-owner approach worked well when Spotify was a small organiza‐ tion, but it had complexity and scalability issues, both technical and organizational. It became impossible to rely on a single or a pair of systems owners to figure them all out.  We  also  realized  that  acting  as  a  gatekeeper  to  operations  meant  we  kept  a monopoly on operational learning opportunities. Handing this responsibility over to the backend teams seemed like the logical next step. The next question was how to proceed. We needed to actively engage the teams and figure out a way forward with the rest of the tech organization. We  started  rolling  out  a  new  way  of  working—dubbed  Ops-in-Squads—which  in essence included everything needed to hand over on-call and operational responsibil‐ ity for services to the teams that developed them. We needed to write tooling to do many of the things that were done by hand before; we needed better documentation and training so that developers would be able to troubleshoot production issues; and we needed buy-in from developers to drive this change. A guild created for developers and ops-people to share knowledge among everyone and open a bidirectional communication channel helped define some key things we needed:  Introducing Ops-in-Squads: 2013–2015      101     A standardized way of defining Service-Level Agreement  SLAs; we did not use the  Service-Level  Indicator  [SLI] Service-Level  Objectives  [SLO]  concepts  back then     A  how-to  crash  course  on  being  on  call,  handling  incidents,  performing  root-  cause analysis, and holding postmortems    Guidance on capacity planning   Best  practices  for  setting  up  monitoring  and  alerting  as  well  as  instruction  on  interpreting monitoring data    Training in troubleshooting, system interactions, and infrastructure tooling  Handing all of this responsibility over to the teams would of course be a big under‐ taking over a long period of time, and the infrastructure team still remained to sup‐ port and build upon the core platform. We struggled with defining exactly what fell into the core platform; some things were obvious, like networking, monitoring, and provisioning,  but  other  systems  were  harder  to  classify.  Critical  parts  of  our  infra‐ structure, like the user login service, the playlist system, or the song encryption key system, needed to be evaluated: are they core infrastructure, or should they be han‐ dled like any other backend system? We wanted to make a deal: we remove gatekeeping and friction points in exchange for shifting operational responsibilities into feature teams. If a team needs to deploy a change on a Friday, it shouldn’t be prevented from doing so. We assumed that the knowledge for operating services is held closest to where it is developed. This was not met with cheers and optimism from everyone in the organization; there were  concerns  about  negatively  impacting  feature  teams’  speed  of  iteration.  How would teams have sufficient bandwidth to deliver on features and growth if they also handled the maintenance and operational duties of their systems? The teams consis‐ ted of developers, not SREs; how much of their time would they have to devote to learning  and  practicing  operations?  Preparation,  process,  and  tooling  would  be essential to convince people to take on this responsibility. We  continued  investing  heavily  in  picking  blessed  tooling,  writing  and  promoting frameworks,  and  writing  documentation  to  help  make  the  transition  as  smooth  as possible for most teams. Driving the Paradigm Shift  We aim to make mistakes faster than anyone else.  —Daniel Ek, Spotify founder and CEO  To reach all the teams with the information and tools needed, we took several differ‐ ent approaches. We held presentations for developers, discussing how the systems in  102      Chapter 7: SRE Without SRE: The Spotify Case Study   our  backend  fit  together  and  how  to  use  the  shared  infrastructure.  Our  principal architect  internally  held  postmortems  for  dozens  of  developers,  walking  them through a large incident that had resulted from a cascading failure. We made it easier for teams to find documentation and ownership for infrastructure systems by having a  central  entry  point.  We  developed  an  Ops-in-Squads  handbook  that  became  the standard document to which to point new developers. It contained most of the infor‐ mation  that  teams  needed  to  get  started;  pointers  on  things  to  do;  where  to  read more; and checklists of processes, technical features, or processes to implement. We embedded in backend teams for short periods to transition the host team to being on-call for their services; helped teach operations; and worked directly with improv‐ ing the systems, deployment procedures, on call handbooks, and so on for teams that requested or needed it. Again, we made a deal: we’ll work with the team for a few sprints and “clean house” before handing over the on-call responsibilities to them. In 2014, this approach—embedding engineers in teams—was expanded into an Ops- in-Squads tour where some parts of our operations team went week by week to dif‐ ferent  teams,  engaging  with  their  daily  work  and  trying  to  help  them  on  anything with which they struggled. During these embeds, we reviewed the architecture of the host teams and looked at alerting and monitoring, helping to improve these when needed. We also discussed how to do on-call and shared best practices around schedule rotation, how to escalate problems, and how to hold postmortems. Postmortems, in particular, had a reputa‐ tion of being time-sink ceremonies. Teams that had a lot of incidents often skipped postmortems  because  the  work  to  establish  timelines,  find  root  causes,  and  define remediations for 10 or 20 incidents per week seemed like too much overhead. Find‐ ing  different  ways  of  approaching  postmortems  proved  useful  in  these  cases,  like clustering incidents after a theme, or doing short postmortems for multiple incidents at  once.  Often  many  incidents  had  similar  root  causes;  exact  timelines  weren’t  as important  as  identifying  the  top  remediations  that  would  reduce  the  likelihood  of that  type  of  incident  by  90%  in  the  future.  Throughout  this  shift,  we  retained  the principle  of  blameless  postmortems,  emphasizing  the  importance  of  learning  from our mistakes, and ensuring that no one was thrown under the bus. Key Learnings Some of our key learnings from this period were:    Strive  to  automate  everything.  Removing  manual  steps,  friction,  and  blockers  improves your ability to iterate on product.    Ensure  that  self-service  operational  tooling  has  adequate  protection  and  safety  nets in place.  Introducing Ops-in-Squads: 2013–2015      103     Teaching ops through collaboration is vital to making ops “default” in the orga‐  nization.  Autonomy Versus Consistency: 2015–2017    <100 ops engineers   <200 backend engineers   4 data centers   120 services   Multiple cloud providers  Prelude In this section we’ll talk a bit about how we tried to balance autonomy for squads with consistency in the tech stack: Iterating over failure  Our  first  approach  at  introducing  consistency  in  the  technology  stack  caused unintentional  further  fragmentation.  Although  we  continued  to  reiterate  and address these newly exposed concerns, we found ourselves blocking teams once again.  Core engineering values  Focusing on unblocking feature teams allowed us to maintain squad independ‐ ence and freedom while introducing much-needed infrastructure consistency.  In moving forward with the Ops-in-Squads model, we shifted our focus to standard‐ izing our technology stack. With this decentralized operations model, we needed to reduce  the  cost  of  operations  for  teams  by  providing  consistency  across  our  infra‐ structure. High entropy meant expensive context switches and unnecessary overhead. However,  striking  a  balance  between  consistency  and  our  penchant  for  autonomy took thoughtfulness and foresight. Bringing  uniformity  to  Spotify’s  stack  came  in  a  few  forms.  Although  teams  were now in charge of operating their services, we needed to get out of their way by build‐ ing abstraction layers in the right places. The first levels of abstraction layers were tools for ourselves, iterating on our early successes of removing friction points with provgun and DNS. Out of this work came a number of tools: moob, for out-of-band management of hardware; neep, a job dispatcher to install, recycle, and restart hard‐  104      Chapter 7: SRE Without SRE: The Spotify Case Study   ware; and zonextgen, a batch job to create DNS records for all servers in use, to name a few. Building off of the earlier legwork, in 2015 through 2016 we concentrated on creating and  iterating  on  self-service  tooling  and  products  for  feature  developers  around capacity  management,  Dockerized  deployments,  monitoring,  and  SLA  definitions. No  longer  did  developers  need  to  plan  capacity  with  spreadsheets  or  deploy  via SSH’ing  into  servers  to  do  an  apt-get  install.  A  few  simple  clicks  of  a  button  and squads got the capacity they needed with their service deployed. We  established  a  blessed  stack  in  2015,  one  that  is  explicitly  supported  and  main‐ tained by the infrastructure team, that became the “Golden Path” for how a backend service is developed, deployed, and monitored at Spotify. We began using the term Golden Path to describe a series of steps that was supported, maintained, and opti‐ mized by our infrastructure team. Instead of dictating a “blessed stack” as a manda‐ tory  solution,  we  wanted  to  make  the  Golden  Path  so  easy  to  use  that  there  was almost no reason to use anything else. The Golden Path consisted of step-by-step guide for our developers. It included how to set up their environment; create a simple, Dockerized service; manage secrets; add storage;  prepare  for  on-call;  and,  finally,  properly  deprecate  a  service.  We  built Apollo, our Java microservice framework, which would give a developer many fea‐ tures  for  free,  like  metrics  instrumentation,  logging,  and  service  discovery.  Heroic and Alien, our time series database  TSDB  and frontend, allowed engineers to create prepackaged dashboards and alerts of their instrumented-by-default Apollo services. Pairing with Helios then provided a supported way to roll out Docker deployments in a controlled manner with zero downtime. Benefits Squads were now able to focus even more on building features because operational tasks were continuously being minimized. But developer speed was not the only ben‐ efit to improving the consistency within our infrastructure. It allowed our migration from our physical data centers to Google Cloud Platform to be fairly seamless. Most of the work happened behind the scenes from the developer point of view. Creating capacity in Google Cloud Platform was no different than on bare metal. Neither was deploying or monitoring a service. From the developer point of view, moving to a new squad or doing a temporary embed became frictionless, given a consistent set of tools to work with. Another benefit to a consistent, blessed stack is that it prevents us from falling into potential traps of trendy technologies that come and go. We also have insights and understanding  about  what  engineers  are  using,  informing  us  how  to  improve  our products.  There  is  less  operational  complexity  and  fragmentation,  which  can  be  a nightmare during incidents. We’re able to prescribe best practices for operating a ser‐  Autonomy Versus Consistency: 2015–2017      105   vice,  and  provide  ongoing  support.  As  an  infrastructure  team,  we  decided  to  have fewer responsibilities, but strived to handle them well. Trade-Offs It was not all easy sailing, though. Standardizing a technology stack still had its draw‐ backs. Too restrictive, and we risk losing squad autonomy and experimentation, hin‐ dering  development  when  not  all  use  cases  are  addressed.  We  defined  the  Golden Path to be an Apollo service, but hadn’t yet provided explicit support for the legacy and internal Python services, frontend applications, or data pipelines and analytics. We provided the ability to easily create and destroy capacity, but we didn’t yet gener‐ ally support autoscaling. These unaddressed use cases indirectly contributed to the fragmentation  that  we  were  confronting  as  some  teams  created  their  own  bespoke tools for workarounds. In  some  cases,  supported  tools  were  not  used.  JetBrain’s  TeamCity  used  to  be  the only Continuous Integration  CI  system used. It became too cumbersome for back‐ end service developers, so squads spun up their own Jenkins instances. As evidence of team  autonomy  and  experimentation,  the  use  of  Jenkins  spread  so  quickly  among squads that it soon became the de facto backend service CI tool. It made sense with the Ops-in-Squads model we adopted, except that teams lacked good habits to main‐ tain their own Jenkins setup, leaving them out of date, vulnerable, and inadequately secured. It forced us to rethink our build pipeline support, where we ultimately devel‐ oped an explicitly supported managed Jenkins service. Similarly, we found developers were not maintaining their Cassandra clusters, despite tools provided by our team. Fearing potential data loss and realizing maintenance was too much overhead for a feature team, we brought this, too, back into our operational ownership by offering a managed Cassandra service and related support. Even though we had many self-service tools to unblock feature teams, the IO tribe still got in the way. In 2017, our focus shifted again to strengthening our overall infra‐ structure by prioritizing ephemerality, security, and reliability. We built tools for ini‐ tiating controlled rolling reboots of our entire fleet, which encouraged developers to write  resilient  services.  We  automated  regional  failover  exercises,  bringing  to  light inconsistencies that teams have in their services’ capacity across regions. As we make progress  in  taking  advantage  of  products  and  services  that  Google  Cloud  Platform offers, teams are now needing to take time away from feature development to, for instance, migrate to a new cloud-native storage solution such as Bigtable but reap the benefits when they no longer need to maintain any infrastructure. Although we succeeded in shifting to the Ops-in-Squads model and struck a balance between autonomy and consistency, we are now focusing on removing friction from operations—but we still have a long road ahead.  106      Chapter 7: SRE Without SRE: The Spotify Case Study   Key Learnings Some of our key learnings from this period were:    Golden paths provide a low-friction way of getting from code to production fast.   Support one blessed stack and support it well.   Giving clear incentives is essential for a blessed stack adoption  e.g., operational  monitoring, continuous deployment pipelines .  The Future: Speed at Scale, Safely When thinking about the future, we can imagine a landscape where the operational burden for feature teams has been safely reduced to nearly zero. The infrastructure supports  continuous  deployment  and  rapid  experimentation  across  hundreds  of teams, and there is little to no cost for the majority of developers in operating their services at scale. This is the dream, but we’re not fully there yet. There are a number of technology shifts being made as part of this zero ops dream. The first part of this strategy for us is the migration to the cloud. Instead of spending time on tasks that don’t give us a competitive advantage, like data center manage‐ ment and hardware configuration, we can shift that problem to cloud providers and benefit from their economies of scale. The second part is around adopting cloud primitives, shifting from bespoke solutions to open source products with vibrant communities. An example of this is our plan‐ ned move from our homegrown container orchestration system, Helios, to a man‐ aged  Kubernetes  services   Google  Kubernetes  Engine .  In  adopting  Kubernetes instead of further investing in our own container orchestration system, we can bene‐ fit from the many contributions of the open source community. Making these shifts allows  the  ops  teams  to  focus  on  higher-level  problems  facing  the  organization, thereby delivering more value. Even with the abstractions of the cloud, ops teams still own the uptime of the plat‐ form. Toward this end, we are adopting a mantra, speed at scale, safely, or s3 for short. We want to enable Spotify to iterate as fast as possible but to do so in a way that is reliable  and  secure.  Our  move  to  the  cloud  is  consistent  with  this  message,  but  as infrastructure and operations engineers, we also face a more nuanced problem space. Initially,  services,  data  centers,  network,  and  hardware  were  all  architected,  provi‐ sioned, and managed by us; we understood the intricacies of operating and support‐ ing these systems. With the cloud and, moreover, our ever-increasing scale, we need better insights, automation, and communication channels to ensure that we can meet our internal availability SLO of 99.95%. Therefore, we’ve invested in reliability as a  The Future: Speed at Scale, Safely      107   product, which includes domains ranging from chaos engineering to black-box mon‐ itoring of services. Another part of the challenge of speed at scale, safely involves how we guide feature teams through the myriad technology product offerings. We can’t serve as roadblocks to innovation, but we also need to ensure the reliability of the platform, which means we need to guarantee consistency in those cases for which reliability is a core con‐ cern. This involves building the right tooling, conformity engineering, a robust devel‐ oper  platform  that  engineers  use  to  find  the  right  product  for  their  needs,  and  a concerted teaching and advocacy effort. Toward this end, we will also need to reevaluate our Ops-in-Squads model. Although this structure has served us well, we’ll need to consider how we can further reduce the operational burden that feature squads face today. In the few years since this change was put in place, we have grown and learned as a company, and the same metrics for success that applied in the past might not be as applicable in the future. We don’t know exactly what this will look like, but we know that we’ll be agile and continue to experiment  to  find  the  best  solution.  For  example,  consider  our  incident  manage‐ ment process. A team’s on-call engineer will triage incidents affecting its services and, if needed, escalate to the IMOC for assistance with overall incident coordination and communication  during  high-impact  situations.  Although  the  IMOC  structure  has been  an  important  mechanism  to  quickly  swarm  on  high-impact  issues,  with  our increasing workforce spread across the globe, it’s not always well understood when to page IMOC and, more generally, how to handle team on-call. In such cases, we will need to improve our teaching of operational best practices throughout the organiza‐ tion, and how we do so—how we advocate for operational quality— might require adjustments to our original thinking behind Ops-in-Squads. Finally, as the technology landscape changes, so do our site reliability needs. One area that we’re exploring is machine learning. With thousands of virtual machine instan‐ ces  and  systems,  and  more  than  a  hundred  cross-functional  teams,  we  might  find machine learning an effective way to make sense of an ever-growing, complex ecosys‐ tem  of  microservices.  Furthermore,  we  also  have  the  opportunity  to  revolutionize what has largely been manual tunables in the past, whether it be “right-sizing” our cloud capacity needs or detecting the next big incident before it occurs; the opportu‐ nities here are manifold. We might not only predict incidents, but also mitigate them with self-healing services or by providing automatic failbacks to a trusted state. There are other infrastructural trends to consider; for instance, adopting serverless patterns imposes a set of challenges for our service infrastructure, from how we mon‐ itor to how we deploy and operate. We also see a shift that’s both organizational and technical; our developers are increasingly working across the landscape of backend, data, mobile, and machine learning systems. To support their end-to-end delivery, we need touchpoints that are seamless across these areas. Reliability can no longer be  108      Chapter 7: SRE Without SRE: The Spotify Case Study   focused on the robustness of backend-only systems but must also consider the net‐ work of data and machine learning engines that provide increasing value to our eco‐ system. We  hope  you’ve  enjoyed  learning  about  our  successes  and  failures  in  building  a global operations and infrastructure organization. Although we’ve experimented with many  different  flavors  of  SRE  and  now  function  in  this  Ops-in-Squads  model,  we believe that what has made us successful is not the model itself but our willingness to embrace change while keeping core SRE principles in mind. As the technology land‐ scape changes, we’ll also need to keep evolving to ensure that the music never stops streaming.  Daniel Prata Almeida is an infrastructure and operations product manager for reliabil‐ ity at Spotify. In a previous life as an SRE, he carried the pager and nurtured complex distributed systems. He’s addicted to uptime. Saunak Jai Chakrabarti leads infrastructure and operations in the US for Spotify. He’s passionate  about  distributed  systems  and  loves  to  study  all  the  different  ways  they break or work unexpectedly. Jeff Eklund, formerly at Spotify, is an SRE and technology historian with a passion for the arcane and quirky. He believes every computer problem can be solved by tenacity, camaraderie, and ramen. David Poblador i Garcia, ex-SRE and product lead, now engineering lead for the tech‐ nology  platform  at  Spotify,  is  rumored  to  be  the  friendly  face  of  deep  technological knowledge and strategy. Niklas Gustavsson is the former owner of Spotify’s worst-behaving service and currently serves as Spotify’s Principal Architect. Mattias  Jansson   Spotify   is  an  ex-{Dev,Teacher,SRE,Manager},  current  Agile  coach. He’s a history geek with a passion for systems—both silicon- and carbon-based. Drew Michel is an SRE at Spotify trying to keep the lights on in the face of chaos. In his spare time, he enjoys running long distances and walking his Australian Shepherd dog. Lynn Root is an SRE at Spotify with historical issues of using her last name as her user‐ name, and the resident FOSS evangelist. She is also a global leader of PyLadies and for‐ mer  Vice  Chair  of  the  Python  Software  Foundation  Board  of  Directors.  When  her hands are not on a keyboard, they are usually holding a bass guitar. Johannes Russek, officially former SRE and technical product manager at Spotify, unof‐ ficially dropped-ball finder and software systems archeologist. Favorite tool is a white‐ board.  The Future: Speed at Scale, Safely      109    CHAPTER 8 Introducing SRE in Large Enterprises  Sriram Gollapalli, Agilent Technologies  This  chapter  describes  my  story  of  how  an  acquired  Software  as  a  Service   SaaS  startup  founded in 2006  introduced SRE into a large enterprise  originally founded in  the  1930s ,  including  the  challenges  and  opportunities  of  traditional  IT,  opera‐ tions, support quality teams, and distinct product engineering divisions working in concert with one another. This is intended for managers who understand that SRE is what their organization and products need but they are struggling to determine how to implement these ideas. This was adapted from a talk I gave at SRECon17 Europe in the summer of 2017 in Dublin, Ireland.  Quality means doing it right when no one is looking.  —Henry Ford Background As a cofounder of a small SaaS startup, joining a large organization was a thrilling experience for me. Our team was excited to share our knowledge and take advantage of the resources and infrastructure this larger company could provide to expand our business. Our startup learned that the matrixed structure—in which groups have dual reporting relationships, by function and also to the product lines—can be both flexi‐ ble as well as challenging when it comes to introducing new paradigms. Our organization, Agilent Technologies, has deep Silicon Valley roots from the late 1930s. As a large instrument manufacturer with primarily shrink-wrapped software products, we traditionally have delivered software by burning CDs and DVDs and mailing them, with major releases every 12 to 24 months. As our customers are start‐ ing to accept more electronic delivery of their products, we’ve introduced download‐ able patches and upgrades, but size limitations still make shipping sometimes more efficient.  111   As Agilent acquires SaaS companies and introduces SaaS products, continuous inte‐ gration  and  deployment  pipelines  offer  evolving  conduits  for  delivery,  which  chal‐ lenge  traditional  processes  and  organizational  structures  and  roles.  Organizations such as the United States Postal Service and FedEx used to be the sole source for the reliability of our software product releases. As cloud delivery becomes more ubiqui‐ tous,  we  have  to  be  that  source.  It  becomes  an  integral  component  of  our  brand promise and customer experience. Introducing SRE It’s  important  to  understand  that  fundamentally  SRE  is  a  vehicle  to  maintain  and improve the health of customer-facing products while increasing software engineer‐ ing velocity. This message resonates with executives and helps to advocate for dedica‐ ted SRE resources. In order to introduce SRE, we used the following approach, which you can see in Figure 8-1.  Figure 8-1. Steps to implementing SRE in an enterprise. Defining Current State To determine where SRE will fit in the future and how to present the business case to your  leadership,  you  have  to  start  by  understanding  the  organizational  structure. There  are  many  components  to  consider  when  evaluating  current  capabilities  and potential gaps.  Start by defining the roles and responsibilities of traditional functions in the organization to understand the landscape Typical enterprises will have central functions such as the following:    Information technology central IT information systems help desk   Product support and services   Global operations  Given  the  unique  combination  of  skill  sets  needed  that  span  multiple  groups,  the cross-functional nature of SRE will present an initial challenge to determining where to position SRE in the organization. We conducted informal surveys with our peers at similar large enterprises—there isn’t always a clear home for the group. To deter‐ mine the best organizational structure, identify leaders and or executives in leadership  112      Chapter 8: Introducing SRE in Large Enterprises   positions  to  gauge  their  operational  experience  and  appetite  for  embracing  a  new approach to operations. This will later be important for SRE education and sharing the business case. Example roles could include the CIO, VP of operations, VP of sup‐ port, VP of cloud, or VP of engineering. It’s possible that one of these groups will be a logical fit under which your SRE team can incubate.  To help us identify who to work with, we looked at an established SaaS product. We imagined it becoming suddenly unavailable and followed an incident to see which roles and groups would be affec‐ ted. This took us to traditional product support, and IT support, and,  more  importantly,  revealed  gaps  where  there  was  neither  a clear  owner  nor  established  process  to  comprehensively  manage and monitor the product.  Prepare the business case: personalize and evaluate the cost of having engineering resources responsible for reliability Imagine this scenario: your sprint has well-planned stories with consistent definitions of done, groomed backlog, acceptance tests approved—it’s a sprint planned to within striking distance of completion with ample buffer for unplanned incidents. It’s Tues‐ day afternoon, you’re getting ready to dive into another focused coding session, and you get pinged from your support team: “Hey, I’m getting some customer tickets that they can’t add items to their shopping cart, could you take a look?” Poof! Any hope of hitting your sprint goals and velocity is out the window while you focus on trying to figure out what happened. Sound familiar? Context switching increasingly reduces the efficiency in task delivery. The first ele‐ ment of your business case can focus on the fact that without SRE resources, tradi‐ tional  engineering  resources  cannot  focus  on  the  business-prioritized  product enhancements and improvements. Gather stories from your support, sales, marketing, and other customer-facing teams of how engineering could have completed products more efficiently and high-level estimates of time wasted looking at outages or incidents.  Prepare the business case: calculate cost of similar resources doing duplicate work In large organizations with multiple products  cloud and otherwise , there will likely be  multiple  resources  or  even  teams  with  SRE-like  responsibilities.  Collaborating across these resources can result in knowledge sharing and cost savings by focusing these  efforts  in  a  centralized  manner.  Disparate  resources  across  the  organization could be defining and developing similar processes and tools. Identify these resources and estimate the cost of redundant work efforts.  Introducing SRE      113   To establish a roadmap for what products SRE will be responsible for, survey the current infrastructure landscape As  we  started  to  identify  the  kinds  of  products  that  would  benefit  from  an  SRE engagement, we gathered information about the technology footprints the products were built on. We saw a wide variety, from a ready-to-release-to-cloud product with snowflake servers spread out, to a product with a complex Windows architecture and dependencies that had a history of failures and fragile ecosystem  unsupported oper‐ ating  system  versions,  limited  vendor  support,  etc. .  Don’t  underestimate  the  com‐ plexity  of  operationalizing  an  existing  environment  and  include  that  in  your assessment. Identifying and Educating Stakeholders Once you understand the current environment, start to have conversations and build the story of what SRE will mean to your organization and the impact it can have.  Start having conversations with leaders and champions in the organization As identified in the previous section, leaders can include the CIO, VP of operations, VP of support, and VP of engineering. We started with one-on-one conversations to understand current responsibilities and pain points that each group was experienc‐ ing. We used this time to identify and tailor the story of how SRE could improve each of their functional areas. Because  we  did  our  homework  and  understood  our  product  footprint,  we  started these conversations sharing a high-level analysis of our current state and disparate product  delivery  models   see  Figure  8-2 .  We  used  this  opportunity  to  introduce cloud  delivery  concepts  and  how  they  differ  from  the  traditional  shrink-wrapped software.  Figure 8-2. Kinds of software products and their delivery method  the dots represent a unique product .  114      Chapter 8: Introducing SRE in Large Enterprises   With that context, we discussed the evolution of the delivery and operating models, which started with traditional system administrators that were usually in a central IT department. Even though our organization wasn’t aspiring to be the next SaaS uni‐ corn or hyped social media product, using examples from Google, Netflix, and so on helped to provide the background to show how the delivery models in the industry were  shifting  to  being  more  integrated  with  the  engineering  organizations   see Figure  8-3 .  We  used  this  to  highlight  the  differences  among  traditional  operating paradigms and the lack of an obvious home in an organization.  Figure 8-3. Evolution of software delivery models.  Defining SRE This brought us to defining the key tenets of SRE as adapted for Agilent:    The  SRE  team  runs  production  services—a  set  of  related  systems,  operated  for our  customers,  who  can  be  internal  or  external—and  this  team  is  ultimately responsible for the health of these services.    Successfully operating a service entails a wide range of activities:  — Developing  and  implementing  monitoring  systems  for  performance,  availa‐  bility, latency, and efficiency  — Capacity planning, server management, disaster recovery — Emergency  incident  response,  ensuring  the  root  causes  of  outages  are  addressed  — Working closely with the product team on release management — Setting and meeting availability targets  Introducing SRE      115     When SREs engage with a service, the aim is to improve the service along these elements   improved  incident  response  resolution  time,  increased  availability, etc. , which makes managing production environments for the service easier.    SREs  represent  the  user’s  interest  that  the  software  product  is  always  available    SRE’s goal is to let traditional engineering resources focus on the product and  and running.  increase velocity.  At  first,  some  of  these  seemed  counterintuitive  and  overlapping  with  our  existing functions  i.e., IT infrastructure support or product support . However, upon further conversations, it became clearer that SRE is a unique function that interacts with soft‐ ware engineering groups and resources much more closely than traditional enterprise support groups have done in the past.  During  these  conversations,  everyone  agreed  that  engineering resources were filling this role to keep the products afloat. We also agreed that was a distraction for those resources and also prevented them  from  working  on  business-prioritized  features.  The  initial inclination  was  to  expand  the  scope  of  the  operations  team  to include  these  SRE  responsibilities.  However,  as  we  defined  the technical skills required and the emerging similar needs for other products in the portfolio, it became clearer that this needed dedica‐ ted  product  engineering  experience  with  an  operations  mindset, while staying close to the product groups.  In addition, current roles did not have the capacity or technical knowledge to take on disparate products with the level of detail necessary to support a healthy experience. Arming  ourselves  with  these  definitions  and  summary,  we  scheduled  one-on-one meetings with each of the previously identified leaders and walked them through the SRE concepts and fundamentals. This education proved to be key to allowing us to present a business case to fund a team and move forward. Presenting the Business Case Because we had spent a few months preparing with stakeholders, we concluded this phase by having a large group meeting during which we summarized the opportunity and presented it as a business case. In that meeting, we presented the following story:    The current challenges business problem we were facing   Current effort costs spent in similar but noncohesive functions   Description  of  the  approach  of  incubating  SRE  function  under  our  technical  leadership  116      Chapter 8: Introducing SRE in Large Enterprises     Potential positive impact on operations reliability   Proposed timeline and milestones to achieve return on the investment  Implementing the SRE Team By this point, the business case and the “why” we are doing SRE should be answered. Now  comes  the  “how.”  Start  with  setting  goals  and  staffing  the  team.  Identify  the kinds of resources that are doing this kind of work already for the product s  you’re responsible for, and try to separate them into a distinct group apart from their engi‐ neering obligations so that they can now focus on SRE responsibilities.  Setting goals and defining metrics of success Given this will likely be a unique team in the organization, consistent metrics and updates  are  important  to  define,  measure,  and  report  out  to  your  stakeholders.  A good starting point to measure the success of your SRE implementation is Google’s Four Golden Signals: traffic  number of requests per minute , errors  number of inci‐ dents errors per day, per product , saturation  number of web workers active , and latency  number of timeouts or page-load duration . Monitoring these can influence an overall product uptime metric.  Growing the team: insource or outsource? It  could  be  tempting  to  hire  contractors  to  focus  on  your  product’s  reliability  and delivery  this might be the case in your organization already, in fact  to add more global  coverage,  redundant  knowledge,  support,  and  so  on.  This  is  a  “supporting role” after all, right? Generally speaking, this wouldn’t be prudent. Although it might seem expedient and lower risk at first, we found that one of our products became too reliant on contractors for a similar supporting function. One day,  the  contractor  had  some  internal  turnover  and  it  tried  to  manage  knowledge transfer among its staff with a replacement. The contractor’s transition was challeng‐ ing for us because it was out of our control and the replacement didn’t have the in- depth knowledge that the prior resource had about our products. More important, we  realized  that  we  didn’t  even  have  the  details  to  take  over  the  support—our product-specific knowledge had been completely outsourced. When you’re just starting to implement the SRE function, investing in a set of dedica‐ ted  employees  with  specific  product  domain  expertise  will  prove  to  be  invaluable. Contractor turnover, and time for knowledge transfer for production systems are two reasons that using contractors as key members of your team can be challenging in the long term. When you’re just starting out, contractors could add cost and time to have a consistent, reliable SRE function. You want to focus your efforts on building in-  Introducing SRE      117   house capabilities and expertise, not worrying about managing an outsourcing opera‐ tion. Instances for which contractors could work include when you’re looking to scale up the SRE operations, after processes are well documented and you need to add first- level triage for additional capacity.  Insourcing experienced talent: rotating engineering team members One attribute of successful SRE team members is that they’re closely tied to the engi‐ neering  and  development  process.  This  kind  of  experience  and  in-depth  product architecture knowledge makes current engineering team members a great funnel for resources. One way to softly recruit while spreading the SRE mindset throughout the organization  is  to  work  with  engineering  management  to  rotate  engineering  team members onto the SRE team. Find a timeframe that works for your organization; two to  three  months  generally  works  well.  You’ll  be  surprised  to  find  that  sometimes resources already exist for doing this work in the product software divisions. When you bring them onto a central team, they become more exposed to the broader prod‐ uct portfolio and will benefit from all of the knowledge sharing.  SRE throughout the development cycle After the team is established, it’s important to start engaging with product teams dur‐ ing all parts of the product development life cycle, as described in Figure 8-4. The SRE  function  will  be  more  effective  the  earlier  you  can  involve  it  in  each  product. Informing  and  educating  engineering,  product  marketing,  and  support  teams  will result in more successful relationships and implementation of the SRE function.  Figure 8-4. Responsibilities, sample activities, and benefits of engaging SRE at each stage of the product development life cycle  118      Chapter 8: Introducing SRE in Large Enterprises   As Matt Klein, software engineer at Lyft, says, “SREs should be embedded into prod‐ uct teams, while not reporting to the product team engineering manager. This allows the  SREs  to  scrum  with  their  team,  gain  mutual  trust,  and  still  have  appropriate checks  and  balances  in  place  such  that  a  real  conversation  can  take  place  when attempting to weigh reliability versus feature.”1  Defining the role of supporting divisions A final component of the successful implementation will be to understand how SRE will leverage, depend on, and enhance existing divisions and functions in the enter‐ prise. Here are some example interactions and relationships to build that could work:    Information technology central IT information systems help desk  — These  groups  are  likely  responsible  for  infrastructure  layers   i.e.,  network, data centers, corporate security  as well as running enterprise security opera‐ tions centers and network operation centers. Ensure that you have key points of contacts and responsibilities defined to avoid duplicative effort.    Product support and services  — This group will usually have the front line of user-facing support, calls, and defined-incident management processes. Similarly, invest time in introducing the SRE team and its capabilities. Use it when setting up an escalation process and seamless handoff to minimize confusion. This group might also be great for handling customer-facing communications in the middle of incidents. It will be a strong partner in your successful implementation.    Global operations  — This  group  could  have  centralized  dashboards  that  executives  already  are attuned to reviewing. Partner with them to incorporate SRE metrics and sta‐ tistics of the products you’re monitoring into their communications.  You will constantly be interacting with these kinds of divisions; don’t replicate effort and  institutional  knowledge.  By  leveraging  these  existing  resources  and  functions, you’ll have an opportunity to jumpstart your SRE function. Take the time to under‐ stand these capabilities and look for the intersection points.  1 Matt Klein, “The human scalability of DevOps”.  Introducing SRE      119   Lessons Learned Following are some lessons we learned during our process of introducing SRE into our organization: Challenge the status quo  It might not be immediately apparent that SRE is the right answer for your orga‐ nization. Don’t be afraid to introduce SRE as a methodology to tackle the prob‐ lems you’re facing. The unique blend that combines engineering, operations, IT, and support skills into a cohesive offering for next-generation products can be complex to communicate. We were fortunate to engage with leadership that had a forward-looking mindset that embraced new models of product delivery. Don’t be complacent and try to slot SRE responsibilities across existing functions—use industry examples and continue to make your case.  Invest time in understanding your organizational structures  Whether you’re in a small division or sit next to the CIO, take a moment to place yourself  in  leadership’s  shoes  and  define  where  you  believe  an  SRE  function would fit. If it’s not immediately apparent, work with leadership to introduce the concept as described. Change is possible; after you align executive support and prove  the  business  case,  you  should  have  the  green  light  to  incubate  an  SRE group. In addition, network with key leadership roles. You will need their sup‐ port to introduce SRE.  Frame the SRE introduction around a business case structure  The business case will resonate with executives and provide a natural approach to the conversation with the organization’s leadership. It imperative for you to have  the  technical  knowledge  to  support  why  SRE  is  what  your  organization needs, but distilling into a business case can be effective.  Setting boundaries and focus  After  you’ve  established  your  group,  it’s  imperative  to  set  limits  of  what  your group can deliver. Start small and focus on the reliability of a few production sys‐ tems. After you’re successful, you can scale and build a shared center of excel‐ lence. We were thought of as the “new kid on the block,” and different groups came to us asking for assistance in improving the reliability of their systems. It was tempting to add more products to our scope; however, we realized that we needed to focus on our original set of priorities first. After you have a good infra‐ structure  and  proven  set  of  processes  in  place,  the  next  natural  step  will  be  to scale up and add more products to the SRE group’s responsibilities.  120      Chapter 8: Introducing SRE in Large Enterprises   Sample Implementation Roadmap After you have established and funded your team, here are some sample activities for starting with a single product:    Quarter 1    Quarter 2  — Plan overall dashboard — Decide on availability monitoring and error monitoring tools — Decide on incident management tools — Decide on customer-facing status site — Identify on-call rotation plan — Reduce noise from any existing monitoring tools in place  N A if no monitor‐  ing currently in place   — Define the SLOs that you want to monitor — Investigate Continuous Improvement Continuous Deployment tools — Implement 12×5 on-call rotation  — Build go live with dashboard — Implement availability monitoring tool in production environments — Implement 18×5 on-call rotation — Standardize environments configurations — Autodeploy to environments — Self-service deploy of production systems via user interface — Implement error monitoring across all environments — Monitor  and  manage  error  budgets—coordinate  with  product  marketing  — Integrate customer-facing status site to monitoring and internal chat  Slack,  teams  Stride, etc.     Quarter 3  — Conduct Disaster Recovery Business Continuity Planning test  make sure to document and test Recovery Time Objectives and Recovery Point Objectives  — Experiment  with  container  management  and  container  orchestration   as products become more complex with microservices, containers will be impor‐ tant to understand as a delivery vehicle   — Research technologies and prepare for microservice deployments by piloting  test application deployments  Introducing SRE      121   — Confirm  the  environment  has  met  resiliency  and  high-availability  goals  for  key infrastructure components  — Formalize sign-off in product development life cycle process as described in  Figure 8-4    Quarter 4  — Investigate and test tools for predictive analytics from logging analysis — Implement 24 7 on-call rotation  Closing Thoughts Good luck on introducing SRE into your enterprise. SRE has an exciting future in small and large organizations alike. Further Reading 1. Accelerate: The Science of Lean Software and DevOps: Building and Scaling High Performing Technology Organizations, by Nicole Forsgren, Jez Humble, and Gene Kim  IT Revolution Press, 2018   2. Increment On-Call 3. Starting and Scaling DevOps in the Enterprise, by Gary Gruver  BookBaby, 2016  4. Ideas on metrics 5. Skill sets for successful SRE team members 6. Templates for incident management  Sriram  Gollapalli  has  spent  over  15  years  designing,  developing,  and  implementing enterprise SaaS product and solutions. He was the cofounder and COO CTO of iLab Solutions, an enterprise SaaS focused on the healthcare market that Agilent acquired in the summer of 2016. Prior to that, he spent several years as a consultant at Deloitte Consulting focusing on strategic technology enterprise solutions and at Intel as a system engineer. Sriram has a B.S. in Computer Science and masters in information systems management from Carnegie Mellon University.  122      Chapter 8: Introducing SRE in Large Enterprises   CHAPTER 9 From SysAdmin to SRE in 8,963 Words  Vladimir Legeza, Amazon Japan  If you cannot measure it, you cannot improve it.  —William Thomson, Lord Kelvin  Over the past 10 years, Site Reliability Engineering has become a well-recognized term among  many  tech  companies  and  the  SysAdmins  community.  In  many  cases,  it stands  as  a  synonym  for  a  new,  advanced  way  of  computer  systems  management tightly coupled with such keywords as distributed systems and containerization, rep‐ resenting a set of practices that allow a variety of companies to run and support sys‐ tems at large scale efficiently and cost effectively. The  fundamental  property  that  differentiates  Site  Reliability  Engineers   SREs   and traditional System Administrators is the point of view. The conventional approach is to make sure that the system does not produce errors or become overloaded. SRE, on the other hand, defines the desired system state in terms of business needs. Both  approaches  use  myriad  metrics  that  monitor  service  from  every  angle,  from individual CPU core temperature to the stack traces of a high-level application. How‐ ever,  the  same  metrics  will  lead  the  two  approaches  to  very  different  conclusions. From the SysAdmin point of view, latency growth of a couple of milliseconds might not  seem  significant  compared  to  a  large  number  of  errors.  An  SRE,  on  the  other hand, might be led to an entirely opposite conclusion: an error might happen, but if the end users have not been affected, the service is fine. Of course, if even a negligible latency increase causes difficulties for customers, now we are dealing with a severe issue. Let’s take a closer look at where this difference is getting its roots through the follow‐ ing short example.  123   Suppose that a manager asked us to create a small new service: “A sort of a simplified web crawler. It has to receive a base URL, download its content, and find and return a list  of  all  URLs  retrieved  from  that  page  with  the  status  of  whether  it  is  valid  and accessible  or  not.”  This  task  is  more  or  less  straightforward.  An  average  software developer can immediately begin the implementation using a small number of lines of  high-level  code.  An  experienced  SysAdmin  given  the  same  task  will,  with  high probability, try to understand the technical aspects of the project. For instance, they might ask questions like, “Does the project have an SLA [Service-Level Agreement]?” and “What load should we expect, and what kind of outages do we need to survive?” At that point, prerequisites might be as simple as, “The load will be no more than 10 requests per second, and we expect that responses will take no longer than 10 seconds for a single URL request.” Now let’s invite an SRE to the conversation. One of their first questions would be something like, “Who are our customers? And why is getting the response in 10 sec‐ onds important for them?” Despite the fact that these questions came primarily from the business perspective, the information questions like these reveal can change the game dramatically. What if this service is for an “information retrieval” development team whose purpose is to address the necessity of content validation on the search engine results page, to make sure that the new index serves only live links? And what if we download a page with a million links on it? Now we can see the conflict between the priorities in the SLA and those of the ser‐ vice’s purposes. The SLA stated that the response time is crucial, but the service is intended to verify data, with accuracy as the most vital aspect of the service for the end user. We therefore need to adjust project requirements to meet business necessi‐ ties.  There  are  lots  of  ways  to  solve  this  difficulty:  wait  until  all  million  links  are checked,  check  only  the  first  hundred  links,  or  architect  our  service  so  that  it  can handle  a  large  number  of  URLs  in  a  reasonable  time.  The  last  solution  is  highly unlikely, and the SLA should therefore be modified to reflect real demands. What we’ve just done is to raise the discussion to a new level—the business level. We started  with  the  customer  and  worked  backward.  We  understood  the  service’s  use cases, identified its key aspects, and established and adjusted the SLA. Only now can we begin to architect the solution. This is the exact meaning of the first of Amazon’s leadership  principles:  “Customer  Obsession—Leaders  start  with  the  customer  and work  backwards.”  Absolutely  the  same  idea  appears  in  the  first  of  Google’s  “Ten Things” philosophy: “Focus on the user and all else will follow.” Clarifying Terminology At this point, I want to present a short, three-character terminology clarification to avoid confusion or uncertainty. We will use these terms in the same way as they were introduced in Chapter 4 of Google’s Site Reliability Engineering book.  124      Chapter 9: From SysAdmin to SRE in 8,963 Words   Service-Level Indicator Service-Level Indicator  SLI  is a single, measurable metric related to service behavior that is carefully chosen with a deep understanding of its value’s meaning. Every indi‐ cator covers one specific aspect of the service. We can measure every aspect on its own terms and conditions; however, the rule of thumb is that every indicator must be meaningful. Example list of SLIs:    Availability  % of requests per calendar year 1   Response time  milliseconds   SLA SLA is a combined set of SLIs that defines the overall behavior of what users should expect from the service. Every indicator gets its own particular value or a range of values. Every possible value should be clearly defined as “good” or “bad.” Also, all barrier values that turn “good” to “bad,” and vice versa, should be precisely specified. A good SLA not only represents a list of guarantees, but also contains all possible lim‐ itations  and  actions  that  might  take  place  in  specific  circumstances;  for  instance, graceful degradation in a case of primary data center outage, or what happens if a certain limit is exhausted. Example SLA: “99% of all requests per one calendar year should be served in 200 ms A request should contain 10 chunks at maximum with no more than 2 megabytes of payload per chunk. All requests that exceed the limit will be served on a best-effort basis or entirely rejected.” This agreement contains four SLIs, namely:    Availability  % of requests per calendar year    Response time  milliseconds    Data chunk size limit  megabytes    Limit of the number of chunks  chunks per request   1 All percentile values  99.9%  and evaluation cycle periods  “per calendar year”  used in examples within this  chapter are conditional and presented for demonstration purposes only. Real services might have stricter requirements, such as 99.999%, and or shorter period, such as “per quarter.”  Clarifying Terminology      125   Service-Level Objective Service-Level Objective  SLO  is absolutely the same set of SLIs that an SLA has but is much less strict and usually raises the bar of the existing SLA. The SLO is not what we need to have, but what we want to have. Example: For the previously mentioned SLA, the availability indicator is set to 99%. For the SLO we might raise this value to 99.9%, and definition will look something like “99.9% of all requests per one calendar year should be served in 200 ms.” The difference between SLA and SLO lies only in the strength of restrictions. Here we will use only the SLA term for simplicity; however, all the facts that we discuss further are also correct to the SLO term. We can use them interchangeably, except cases for which the difference is explicitly mentioned. The principle that the user’s perspective is fundamental is very powerful and leads us to the understanding of vital service aspects. Knowing what is valuable for the cus‐ tomer provides a precise set of expectations that must be finally reflected in the SLA. And by being carefully crafted, the SLA can shed light on many dark corners of a project, predicting and preventing difficulties and obstacles. But first, the SLA is designated as a reference point to understand how well a service is performing. Let’s distinguish which indicators we should use to form an agreement that serves that purpose. There might be hundreds of metrics that reflect a service’s state, but not all of them are appropriate for an SLA. Although the number of SLIs tends to be as minimal as possible, the final list of SLIs should cover all major user necessities. Only two relatively simple questions should be answered positively to indicate that an investigated metric is a good candidate to be chosen or, otherwise, should definitely not be.    Is this metric visible to the user?   Is this metric important enough to the user  and from their perspective as a ser‐  vice customer  that it needs to be at a certain level or inside a particular range?  When  we  add  a  metric  to  an  SLA,  we  also  need  to  be  sure  that  the  metric,  as  we express it, has the same meaning for both sides—that is, for the service owner and for the customer. If it seems impossible to stabilize initial SLA values due to frequent project changes, we might consider revisiting the list of included SLIs to verify that all indicators are not exposed internal specifics and measurable by the user.  126      Chapter 9: From SysAdmin to SRE in 8,963 Words   For example, the SLA defined as the following:  99.9% of all requests per one calendar year should be served in 200 ms with no more than 1,000 requests per second.  At first glance, this is all correct: we have both guarantees and limitations. In reality, this limitation  or the Throughput SLI  has a fundamental difficulty: it has more than one  interpretation  that  can  lead  to  agreement  misunderstanding,  the  disclosure  of internal metrics, and frequent value changes. For the end user, the only case for which this throughput statement is meaningful is when the service is used by this customer exclusively. So they might understand this limitation as meaning that designated throughput is the one that is provisioned for them personally. But the SLA does not state this explicitly, and a user can only guess whether their understanding is correct. Another interpretation of 1,000 RPS  requests per second  is the “required amount of overall service capacity,” which might be meaningful for engineers who will design and support this service but is useless for the users, because a user will never know how much capacity is being used by others and how much of the 1,000 RPS is avail‐ able for them. Finally,  the  1,000  RPS  can  be  treated  as  a  value  that  shows  the  current  service capacity. And if the performance of the service or hardware is changed, the value in the SLA will be updated accordingly. If we want to clearly express that we want to limit the amount of incoming traffic, we can slightly adjust the SLA and say that every customer is provided with its dedicated throughput capacity:  99.9% of all requests per one calendar year should be served in 200 ms with no more than 1,000 requests per second from a single user account.  Now, this number is measurable on the user side and independent from the hardware and software capabilities. We can calculate how many users we can serve and are able to add more capacity and change software without touching the SLA. Establishing SLAs for Internal Components What if the service is not an end-customer-facing one. Should it have its own SLA, too? To clarify the “Yes” answer, let’s play with an imaginary message distribution service  see Figure 9-1  that consists of the following four main components: Data receiver  Accepting and registering messages  Data transformer  Adjusting message content with data from separate external sources  Establishing SLAs for Internal Components      127   Delivering messages to multiple endpoints  Distributor  Consumer  Receiving data from the endpoint over a “publisher–subscriber” model  Figure 9-1. Message distribution service components relationship  At the moment, this system is working fine: no errors, no alarms. One day, one of the top project managers comes to us and poses the following: “One of the projects we are working on now uses the ‘message distribution service.’ From time to time, we will need to send a huge amount of data in a short time period. Can we use this service as it is, or how should we adjust its capacity in order to handle the new traffic? Let’s work this out gradually. Having an actual number for the data amount is handy. Let’s  say  that  the  forecasted  traffic  will  be  three  times  higher  than  the  maximum known  peak-time  value.  However,  this  knowledge  will  not  provide  us  with  a  clear understanding of whether we will be able to handle such growth. The reason is sim‐ ple: even if we know how much data is managed by the service right now during peak times,  we  would  still  need  to  know  the  breakpoint  at  which  we  reach  the  service’s capacity limit to be able to compare it with the forecast. Our message distribution service has several components. The slowest component is the one that dictates overall service capacity: the “strength of the chain is determined by the weakest link.” So now we need to spend some time to establish a performance- testing  environment  and  identify  breakpoints2  for  every  component  separately  and determine which component is the bottleneck. So far, we have data that will tell us about traffic-handling possibilities: the number of messages that can be consumed without errors. And if it is fine, we are ready to go on  2 During performance testing, we gradually increase the traffic. The breakpoint is the amount after which the  quality of responses falls lower than prescribed by requirements. Service begins to respond slower or even start producing errors.  128      Chapter 9: From SysAdmin to SRE in 8,963 Words   to announce that no changes are required. That would be a great scenario if we had planned such growth in advance. Otherwise, if we can serve three times more traffic without rescaling, and service consists of more than one host  undividable resource unit ,  it  might  mean  that  up  until  now  two-thirds  of  all  allocated  resources  were never used, which might raise a question about cost efficiency. But that is a different story, and hopefully it is not the case. Comparison of the test results and the forecast reveals that we can deal only with half of the expected growth. And we now, as per the new request, need to at least double throughput. This  is  a  relatively  simple  task.  Traditionally,  to  solve  such  a  problem  SysAdmins determine the critical system resource—like CPU, memory, disk, network I O, and so on—that is used the most, and that is the one that actually needs to be doubled. Then, they  accordingly  request  new  hardware  to  cover  necessity.  And  there  is  nothing wrong with this approach. However, there is something else that is lying outside of its scope and hence not counted. From the SRE point of view, we were missing additional service-specific restrictions without which we cannot scale our service accurately. For our imaginary service, the goal is to pass a number of messages throughout from the entry point to the final consumer in a reasonable amount of time. Each compo‐ nent has only a limited time that it can stay inside each component before it will be sent to the next component. And this is our specific restriction: the time limit per component. Without such a limitation, during the traffic growth, some messages can be  delayed  or  stuck  for  a  very  long  time  somewhere  in  the  middle.  This  problem would not be visible; delayed messages are not marked as problematic because they do  not  raise  any  errors.  They  don’t  raise  errors  just  because  we  don’t  know  what duration is “good” and what isn’t. To overcome this inconvenience, we should express the overall delivery time in a par‐ ticular time value. Then, we should assign the portion of it to the specific component and, again, define it in the exact time value. Here, we are correcting the meaning of a breakpoint for the performance testing procedure. What we need is not the number of messages that we can receive without errors, but how many messages this compo‐ nent is able to pass through such that every message been processed for no longer than the defined time limit. Establishing these limits usually decreases the previously calculated capacity even further. Time constraint per component is nothing but another SLI  “Passing time”  and its value  is  determined  for  every  component  individually.  Also,  each  component  has other  valuable  indicators  such  as  “Availability”  and  “Response  time.”  When  com‐ bined they form a per-component SLA.  Establishing SLAs for Internal Components      129   Now we have an SLA of two natures. The first is the one that we mentioned earlier, which covers the entire service and is exposed to the services’ customers. We will use it to observe the overall service behavior. The  other  one  is  the  per-component  SLA.  It  is  not  presented  to  the  services’  end users; however, it allows us to determine the relationship between components, helps to identify quickly which component caused difficulties to the overall service, and is used to precisely scale the component. Its SLI’s values can  and should  be used dur‐ ing performance testing as limits to identify the correct breakpoint. For our toy service, the breakpoint will be identified as a maximum throughput at which a component will meet the following demands:    100% of requests get responses faster than the certain limit.   No errors occurred.   100% of messages are sent to the next component such that every message met its  time limitations.  Keep in mind that all three criteria must be verified simultaneously, because receiving messages and sending them further might be done asynchronously by different pro‐ cesses, and the good condition of one metric does not mean the same status of the other. Back to our scaling. We previously said that we need to double the throughput. Now, this  statement  needs  to  be  revised  because  we  changed  the  testing  procedure  by appending new requirements, and the results might not be the same as before. The forecasted input will be received by the first “Data receiver” component. Know‐ ing particular values of expected traffic and component performance metrics, we can finally estimate required capacity adjustments. We can calculate the potential maxi‐ mum capacity we have now, the required capacity that can handle the maximum traf‐ fic expected, and then find the delta between the two. But this will be true only to the “Data receiver” because the forecasted input defines the size for this component only. Do not forget that, for instance, doubling the throughput for a component does not necessarily mean that we need to double its fleet. Not all components scale linearly, so  for  some  of  them,  it  will  be  enough  to  add  only  a  few  machines.  For  others,  it might require adding much more than double the number of hosts. But what about the next component? In the same way, we can calculate its current capacity, but we don’t know how much data it will receive from the previous one. We can assume that its traffic will increase according to the growth of the previous com‐ ponent’s input, but this would be only an assumption, which might have nothing to do with reality.  130      Chapter 9: From SysAdmin to SRE in 8,963 Words   Knowing  what  a  component  does,  we  can  perform  a  set  of  experiments  that  will establish a ratio between the input and output of the component. Now we can approximate the amount of input data for every individual component from the originally forecasted amount. For example, if the input output ratio for the “Data  transformation”  component  is  1:2,  and  1:1  for  the  “Data  receiver,”  it  means that for every original megabyte of messages, the “Distributor” will get two. Before we move on, we should say a few more words about the bottleneck compo‐ nent. We keep track of it for a single reason: the bottleneck is the first component that will experience difficulties in case of traffic growth. In other words, if this com‐ ponent is overloaded, the entire service has a problem. Why do bottlenecks exist if we scale every component to the maximum performance and throughput needed? There are two factors. The first is that some components will be a bit overscaled. For example, if a single host can handle 1 million messages per minute, but the traffic is 1.1  million,  adding  the  second  host  will  leave  90%  of  its  capacity   or  45%  overall  unused. And this component will be fine until traffic reaches the 2 million messages per minute rate. But the other part of the service, in contrast, may already utilize its capacity up to 95% processing the same 1.1 million messages. The second factor is the input output ratio. If for the first component the growth of incoming traffic is small but the rate is high, the input to the next component might increase significantly and might require our attention. The bottleneck is the component that in comparison with others has minimal differ‐ ence between current load and maximum available capacity, is very sensitive to traffic changes, and is the first that will be overloaded. Returning  to  our  story,  to  illustrate  how  the  bottleneck  identification  works,  let’s imagine that the forecasted load peak is 1,000 RPS; the components’ capacity after rescaling is as follows:    Data receiver: 1,300 RPS   Data transformer: 1,250 RPS   Distributor: 1,100 RPS  With this information, we now have located a bottleneck; it is the Distributor because its  performance  is  closest  to  the  forecasted  peak  and,  in  spite  of  the  Data  receiver capabilities service, will work fine only until the load is less than 1,100 RPS. The next bottleneck is the Data transformer because it is the closest component to the current bottleneck, and so on.  Establishing SLAs for Internal Components      131   So far, we know the following:    Where the bottleneck is. And we can predict where the bottleneck will relocate from  where  it  is  now   literally  this  means  that  now  we  know  the  next  slowest component, the next after that one, and so on .    Expectations for every component  number of messages that can be processed by a single application instance and the expected amount of time that message can spend inside this component—literally an SLA .    Per-component incoming outgoing traffic ratio. We can predict the traffic vol‐  ume between components and fit capacities accordingly.    The overall and per-component capacity and how much of it is actually in use. We  are  also  able  to  predict  capacity  drops  in  case  of  a  variety  of  outages  and make resource reservations accordingly.3  Our scaling calculations are based on the forecasted values. Keep in mind that the real traffic can have different characteristics. Continuously tracking available capacity along  with  the  knowledge  of  whether  all  components  stay  within  their  SLAs  will clearly tell us how “good” our service is doing. The real-world scenario can become more complicated. There might be more than one  SLA  per  service  because  we  might  have  several  data  types,  and  each  of  them might  need  to  be  treated  differently   Figure  9-2 .  In  our  case,  different  messages might have different priorities and processing-time restraints. Without an individual SLA for each priority, it would be tough to say what we should do if there is a growth in the load from only one of the data types even though this might affect the others.  3 Every host in a fleet represents a portion of a service capacity. The outage of a single host will decrease overall  capacity by the amount represented by this host. Depending on the outage type that we need to sustain, we can add a number of hosts in advance so that during such an outage the overall service capacity will stay rea‐ sonable to deal with the traffic. For example, if 100% of required capacity is six hosts, but we need to with‐ stand a data center outage, as one of the possible solutions, we will need to have nine hosts in three data centers. The disruption of any of these data centers will decrease overall capacity by three hosts, but we will still have a sufficient amount of six in the other two data centers.  132      Chapter 9: From SysAdmin to SRE in 8,963 Words   Figure 9-2. Components relationship in terms of SLAs and traffic patterns for two mes‐ sage types  Here’s a short example: due to high load spikes in high-priority messages, other mes‐ sage deliveries will be slowed down from a few seconds to several minutes. The key question then is, is a delay of several minutes acceptable?” By applying the “divide and conquer” principle, we declare specific criteria for every type separately, and if we know exact barrier values and can quickly identify “good” and “bad” values, there will be no problem taking the right action. Otherwise, we will fall into a state of not knowing and can only guess what to do or whether we should do anything at all. Establishing SLAs for internal components helps to clarify the relationship between them and precisely coordinate their interaction. This is true not just across compo‐ nents but also across big services. SREs mostly focus on service efficiency and quality tracking  what  matters  to  the  user.  Service  architecture  and  particular  applications used  are  secondary,  at  least  until  a  service  delivers  results  according  to  a  user’s expectations. And this is still not the end of the story! Understanding External Dependencies As  you  might  recall,  the  “Data  transformer”  component  in  the  course  of  its  work adjusts  messages’  content  with  information  from  separate  external  sources.  These sources are called external dependencies. The difference between a dependency and the services’ components we have been discussing up to this point is that as opposed to a component, we cannot control a dependency and its behavior. Here, “Data trans‐ former” is playing the role of a customer, and these external services are just a set of “black boxes.”  Understanding External Dependencies      133   From this perspective, the question is, “What capacity can these external services pro‐ vide and how will their limitations affect our component’s performance and scalabil‐ ity?” We want to know what we can expect, and, technically, we are asking for an SLA. We need this to understand whether we can use the service as a dependency, or we need to look around to find another solution for our task. The provided SLA will give  us  a  clue  about  available  limitations   like  request  size ,  performance   like response time , and availability, but still, how is our component performance affected by this and does it suit our needs? If we imagine that we are SysAdmins who care mostly about service health and load and  have  nothing  to  do  with  SLAs—which,  by  the  way,  may  not  exist  at  all—the question we asked will be very tough to answer. However, knowing precise require‐ ments for our own service  while we change our point of view closer to an SRE per‐ spective , we can easily compare the values between SLAs that will direct us to the answer. Let’s see. If, for example, the requirement to pass a message through the Data transformer component is 50 ms and we spend half of this time on internal manipu‐ lations only, we have 25 more milliseconds that we can designate to requesting data from the external source. By having an SLA for that source that states the response time should be less than 20 ms, we can confidently say that it is safe to use this service as a dependency. As  another  example,  it  might  be  a  stated  capacity  limitation  that  the  dependency could serve only 900 RPS per customer account, and if the previously discussed 1,000 RPS of the forecasted peak value is still applied, we will either need to ask to raise the limit or to look for another solution. The final case worth mentioning is when the performance of the dependency dictates our own component throughput. Suppose that without dependency the Data trans‐ former  can  process  2,000  requests  per  second.  If  the  performance  of  the  external source is 1,100 RPS, the Data transformer will be limited by its performance to the same amount of 1,100 RPS. We’re definitely able to use this service as it is, but we should  keep  in  mind  that  the  performance  of  our  component  is  limited  not  by  its own performance or capacity, but by the external dependency capabilities. This is a very important point because if we, someday, will need to scale up throughput of the Data transformer higher than 1,100 RPS, changes on its capacity will not make sense. As is demonstrated here, we now can not only say whether we can use this service, but we’re also able to predict whether we will reach the dependency’s limits and if so, in what circumstances it would happen. Another aspect of the external dependency is the tightness of a relationship between our component and this service—in other words, how an external service outage will reflect on our component performance and availability. To demonstrate the meaning of tightness, let’s look at a couple of examples.  134      Chapter 9: From SysAdmin to SRE in 8,963 Words   The  most  widely  used  external  dependency  is  Domain  Name  System   DNS .  If  a component is frequently resolving domain names, an outage of the DNS might para‐ lyze it completely and the entire service will be affected. The less expected scenario is when only a part of the service experiences difficulties. The entire service can also be affected, but this time it might be a bit more difficult to trace the source of a problem. For the second example, we will discuss a less frequently used service called Light‐ weight Directory Access Protocol  LDAP . If we assume that the service calls LDAP only a few times during the start, the only moment during which our component can potentially be affected is while the service is starting or restarting. Overall, the tightness of the relationship between the component and LDAP service is very low compared to the one between the component and DNS. Both services can affect their dependents, but the severity of a failure event is significantly different. To visualize a dependency relationship, let’s make a list of all of the external services we are using along with their SLAs and potential outage effects. Having it readily at hand in an emergency such as a fire could be critical. It could save you many priceless minutes  that  otherwise  would  be  wasted  figuring  out  influences  on  the  fly.  You should update it every time you add a new major feature or a new dependency. From the SLA perspective, we need to make sure that our service’s promised level of reliability  is  not  higher  than  the  lowest  level  among  all  direct  dependencies.  If  we noticed that this might be the case, we need to either downgrade the SLA or find a technical  solution  to  mitigate  the  difference.  For  that  purpose,  in  some  cases,  we might  develop  a  thin  intermediate  layer  with,  for  example,  caching  or  replication. Keep in mind that we may compare only directly related dependencies and that the service is depending not only on software, but also on hardware, power supplies, a network provider, and so forth, and they all have their own availability limitations that can affect us. Even at this point, the understanding of whether the service is okay heavily depends on a personal point of view. The outage of a single dependency that blocks part of the service will be treated as a big problem by the classic SysAdmin role, but for SREs this service might appear to be only partially affected or even totally fine. For our messag‐ ing case, an outage of the “Data transformation” dependency might block this com‐ ponent entirely  service outage for SysAdmins . But even if we still can receive new messages and store them for a while, the high-priority messages would be affected immediately in terms of delivery timing. However, the low-priority traffic might not be affected at all  for SRE, service is only partially affected . Moreover, if there is no high-priority  traffic  at  the  moment,  the  service  is  totally  fine   unless  we  are  not breaching delivery time limits for the low-priority messages . Now we see how the new viewpoint can dramatically change the way we understand the current service condition. Interestingly, when we actually write down the ideas of  Understanding External Dependencies      135   what we expect from a service and begin to collect related data to measure what we have in reality, it usually turns out that the service is not doing as well as we believed it was doing. Measured  results  are  different  because,  without  an  SLA,  we  can  count  only  these problems that somebody complained about. And in our mind, the service is doing well because there were only a few complaints in the past that appear to have been successfully resolved. The problem is that we did not receive complaints about all of the issues we actually have and, in reality, there might be a thousand cases in which end users just leave silently with a bad taste in their mouth. Nontechnical Solutions Technical solutions are not the limit of an SLA’s potential. SLAs will give you a hand in other fields as well. The SLA determines, for instance, the right time to hand a new service over to the SRE team to support. This can be as simple as, “If a product meets expectations  i.e., does not violate an SLA , the product is ready; otherwise, it is not.” When we talk about the handover procedures, the first thing that comes to mind is a big checklist with dozens of bullet points. It covers all possible aspects of the service, from  architecture  decisions  and  comprehensive  administration  documentation  to monitoring alarms and troubleshooting runbooks. However, having all these bullets checked hardly gives you a strong sense of confidence that it is ready to be taken over. Unless  you  are  freezing  all  activities  except  transition  preparations,  that  is  a  very unusual thing to do; you will never be 100% sure that there will be no unpleasant sur‐ prises. Consider the following. All new software projects will have some prerequisites long before they pass architecture review and a couple of proof-of-concept models have been built. When it is believed that an application is ready to be officially launched and begin serving live production traffic, both SLA and SLO counters are reset4 and start to collect the real data. This is the starting point for the service condition meas‐ ure. Because the SLA defines statements over time  the frequently used period is one calendar year , the project should last in this state a significant portion of this time  several months or a quarter  to collect enough data points that confirm that the ser‐ vice is stable enough and that there are no agreement violation risks. If we add an “expectations reaching” goal as an additional checklist point along with the alarms limitations  amount of alarms for a service during the time period; e.g., “No more than one alarm per two weeks” , we will be much more confident that ser‐  4 You can set SLAs and SLOs for all environments  dev test prod , but they all will show the condition of the  particular environment each. The metric might be the same for every environment, but values will be unique. When we launch a service, we reset all counters so that we avoid influences from the prelaunch data.  136      Chapter 9: From SysAdmin to SRE in 8,963 Words   vice was not on fire for long enough before, that it is not on fire now, and that the developers team is able to maintain this state and it will not throw you a curveball. Similar to this example, you might put any task or idea through the SLA prism to see how impactful it is for your customer and treat it accordingly. This understanding will guide you not only about the points that need to be improved, but also about where  further  improvements  are  unnecessary  and  therefore  you  can  switch  your attention to something else. As an example, if your SLA stated the requirement of 99.9% availability, but for the last quarter the service is running as good as 99.999%, you do not need to do any more work on that service. Now, let’s see how theory meets with practice. Tracking Availability Level All right, so let’s assume that we’ve successfully defined an SLA. But from a practical perspective, how should we track all of these numbers and percentiles? Before jumping into the arithmetic, let’s clarify some meanings first. Here,  we  need  a  list  of  all  agreement  violation  conditions  to  take  them  all  into account.  Let’s  name  these  conditions  as  “failures”  to  distinguish  them  from  the “errors” we have been using previously. This separation is necessary to stress that we will not use these two terms interchangeably. Failures will contain not only errors, but  also  a  variety  of  other  events  that,  one  way  or  the  other,  affect  the  availability metric. Moreover, the spectrum of included errors also needs to be restricted, because the “404 Not Found” HTTP error, for example, is more of notification rather than an actual error caused by service internals and hence should be removed from the list. To return to our previous example, let’s say that for the high-priority traffic for the “Data receiver” component of the “Messaging bus” service the SLA is defined as fol‐ lows:  We will process all messages marked with the high-priority flag within 10 ms in 99.9% of cases over a calendar year.  The list of failures, in this case, will contain the following:    Messages that took longer to process than 10 ms.   Messages unable to be processed due to internal service problems but not caused  by incorrect incoming requests.  To  track  the  ongoing  availability  level,  we  need  to  subtract  the  number  of  failures from the total amount of messages received.  Tracking Availability Level      137   Here  we  are  dealing  with  a  single  component  and  not  with  the entire  “Message-passing  bus.”  As  a  result,  we  need  to  count  the total messages locally from the component perspective even if we have a set of load balancers in front of it. You should use the data from  load  balancers  to  track  the  overall  “Message-passing  bus” SLA. The same is true for the error counters. Collecting error counters from  the  load  balancers  will  definitely  depict  the  real  customer’s impact;  however,  at  the  same  time,  it  will  hide  the  actual  errors amount  happening  in  the  component  level  and  prevent  us  from reacting to them before they become visible from the outside.  So,  let’s  run  the  math.  Our  initial  availability  is  equal  to  100%.  After  the  first  24 hours, we will collect all metrics, calculate a number of failures, and finally get the new availability value to start the next day with it. If for the last day we receive 1 million messages and 200 of them were marked as fail‐ ures for whatever reason, the availability would be reduced to 99.98%, calculated as follows:  100% –  200   1,000,0000 × 100  = 99.98%  This  calculation  method  will  work  fine  to  a  certain  extent,  but  it  does  have  a  few problems.  The  first  one  is  that  availability  level  changes  in  only  one  direction  and once decreased will never recover. The second is that we do not take the “over time” distribution into account, which can lead to dramatic drops where it is not expected at all. For instance, suppose the traffic is served by two hosts fronted by a load balancer and there was a very quiet time period during which only 10 messages were sent for an entire day. What would happen to the statistics if one of the hosts begins producing errors for every received message? If all 10 messages were sent back to back, they were equally spread between the hosts and half of them failed. The load balancer resent them to the other host and finally all 10 were successfully delivered. From the calculation perspective, it would be a huge availability drop:  100% –  5   15 × 100  = 66.67%  A 33.33% drop without any customer impact! The last obvious constraint is that we are unable to plan maintenance that requires downtime because it is unclear how to integrate this intentional downtime from the messages amount perspective. This leads us to another difficulty: unsafe update rollouts. Any rollout with massive changes can potentially end up counting as an entire service outage regardless of the  138      Chapter 9: From SysAdmin to SRE in 8,963 Words   number of tests and canary5 stages the software passed. The problem is not the outage by itself; the problem is our inability to predict in advance whether such events will break the SLA. Even if we know how much time we need to detect an issue and per‐ form a rollback, it is still unclear how to compare this time with the available 0.1% of cases that are left apart from that 99.9 % that we agreed to serve correctly. The solution is hidden in the interpretation of the “99.9% cases per year” statement. The “year” is a value of time, and thus 99.9% can also be treated as a time value. According to this statement, we can count how much time per year we have left for failures and maintenance. A single year consists of 525,600 minutes. The 0.1% that is left beyond the SLA level is equal to almost 526 minutes. Google gives this value a unique name, the error budget. The percentage of time that service worked correctly over the year is usually referred to as a service’s “uptime,” and “99.9%” specified in the SLA is the acceptable level of this metric. Literally, we can interpret the error budget as follows: if the service does not produce even a single failure for the last 365 days, we can shut it down completely for about 8 hours and 45 minutes without violating the SLA. Now we are able to compare rollback timing requirements against such a budget to see whether the next rollout will put availability levels at risk. Also, availability level  uptime  now becomes recoverable. Previously, we were able only to decrease the uptime value, but the new method allows us to restore it over time  up  to  the  original  100%  mark.  Because  “a  year”  is  a  constant  duration  value, every new minute of operation appends new metric values into the head of a year timeline and discards the same amount of data from its tail. If we calculate availabil‐ ity on a daily basis, every time we will work with 1,440 minutes of data. By subtract‐ ing  all  of  the  failure  time  from  1,440  daily  minutes,  we  get  a  “daily  uptime.”  To calculate new overall service availability, we need to subtract daily uptime as it was a year  ago  from  the  previous  overall  availability  value  and  append  it  with  today’s uptime. For example, if we have the following:    Yesterday’s service availability level: 525,400  99.9619%    Error budget size: 326 minutes   Daily availability for the same day last year: 1,400 minutes   Today’s failures: 5 minutes  5 “Canary” rollout is a method wherein we gradually deploy new software over several stages that begin with a  tiny portion of a fleet at first and increase distribution wired up to 100%. At every stage, the new product must perform as expected for some time before it will be pushed forward to the next stage.  Tracking Availability Level      139   Today’s overall uptime is as follows:  Uptime = “Yesterday’s uptime” – “Last year’s daily uptime” + “Today’s daily uptime” 525,400 – 1,400 +  1440 – 5  = 525,435  99.9686%   Here is the current error budget calculated as a difference in minutes between the current overall uptime and the SLA level:  525,435 –  525,600 – 526  = 361  As we can see, last year we had only 1,400 minutes of correct operation, which means that  the  40  remaining  minutes  were  used  by  failures  and  cut  off  from  the  error budget.  After  a  year,  these  40  minutes  can  be  returned  to  the  budget  because  the period when they were encountered is moved beyond the one-calendar-year period  our current availability scope; see Figure 9-3 . So, by today, the budget increased by 40 minutes and decreased back by 5 minutes used by today’s failures.  Figure 9-3. Daily availability calculation scope  Finally, the last thing we need to do is establish a way to convert our service’s failure counts into error budget amounts. To avoid the previous problem in which a very few messages reflected on availability as a 33.33% drop, we will calculate day values, not as a single data set, but gradually by the smallest possible duration between available data points. For instance, if statis‐ tics are collected once a minute, we also can analyze the data on a per-minute basis. The conversion itself is very simple. For every analyzed minute interval, we know the number of messages and the number of failures. One interval represents 100% of the  140      Chapter 9: From SysAdmin to SRE in 8,963 Words   time, and the total message amount is also treated as the 100% value. From this we can derive the failures fraction and extract the same amount from the time interval. The sum of all converted failure times should be used to adjust current uptime and error budget values. Let’s see how all that will work on the “Data receiver” example. For this component we have two types of failures:    An error occurred, and we lost the message.   The message is stuck somewhere for more than 10 ms.  We collect metrics on a per-minute basis, and we also produce all zero values  if there were no errors, we would produce a metric that explicitly says that “Errors=0.” . We also recalculate availability once every 24 hours. For every minute interval, we have three data buckets:    Number of messages processed   Timers for every message   Number of errors  If the timers bucket is missing a data point, we don’t know what exactly happened with that message and we mark it as a failure. Similarly, we do this with the number of processed messages and number of errors buckets, and if one of them is empty, we mark the entire minute as a failure. Let’s recalculate our original example with 2 hosts and 10 messages and imagine that incident happened across a minute border and that was the only 10 messages for the entire 24-hour period. Here’s the calculation: Minute 1    Messages: 10   Delays: 0   Errors: 5  Minute 2    Messages: 5   Delays: 0   Errors: 0  Tracking Availability Level      141   The uptime for the first minute is 0.5 minutes  50% , and 1 minute  100%  for the second  minute.  Overall,  it  is  0.5  minutes  of  downtime.  Overall  daily  availability would be 1,439.5 minutes  1,440 – 0.5 = 1,439.5 . In a worst-case scenario, if this inci‐ dent  were  to  spread  over  a  longer  time  period  and  all  messages  happened  to  be delayed, we will lose only 20 minutes6 of a budget, which is quite different from the 33.33% we saw earlier. Do not be surprised if a freshly established error budget drains out very quickly even from tiny incidents and the real availability level is much lower than you thought ini‐ tially. We were establishing the SLA primarily to reveal this difference. At this point, we are able to derive failures from other events; we know that they are not  errors  only.  Also,  we  can  convert  these  failures  into  the  SLA-related  values  to track changes in the services’ availability level. And finally, we know how to calculate and use the error budget to plan time for experiments and to perform maintenance procedures without putting at risk agreements with our customers. Now, let’s see what else we should keep in mind while working with SLAs. Dealing with Corner Cases To avoid the impression that involving SRE and establishing SLAs is a simple and straightforward  way  to  address  various  difficulties  and  is  only  beneficial,  here  is  a brief overview of a couple of notable corner cases when SLA by itself introduces a bit of additional complexity. An SLA is not a constant, and to bring significant benefits, it should be well main‐ tained and sometimes needs to be changed. On the other hand, the SLA should not be  violated   by  design .  Moreover,  not  all  projects  need  to  have  a  strict  SLA  or involve SRE to maintain it. An SLA is an agreement not only with the user, but also among developers, SREs, and  management  that  are  all  committed  to  supporting  this  availability  level.  This means that “the level, as it is set, is the one that is needed.” An SLA violation is a severe incident, and to avoid it they all will need to interrupt their regular duties and put as much effort as needed to get the service back on track. Not all agreements-related difficulties are introduced by errors and outages. Some of them are accumulated over the time span little by little. For example, take a case in which for the preceding six months the response latency gradually doubled and drew  6 If 10 messages at first attempt will be landed to a bad host and retransmitted to a good one only by the next minute, every one of them will drive the entire minute to be counted as a 100% failure. Next, if all retries will be delayed, they will score another 10 minutes as 100% failures. Finally, because all of these events don’t share one-minute intervals, that day will be finalized with 20 minutes of downtime.  142      Chapter 9: From SysAdmin to SRE in 8,963 Words   very close to the SLA limit. There might not be a single commit to the code that was responsible for that regression, but thousands of small modifications, each of which introduced its own few nanoseconds’ delay, can eventually double the original value. There  would  be  no  single  developer  or  a  team  responsible  for  that.  However,  it  is clear that we cannot leave this situation in place. There are only two ways to deal with the situation at that point: adjust expectations  i.e., the list of indicators included into the SLA or the values that are related to one or several of these SLIs , or get under the hood and optimize the service. For the first way, if it somehow turns out that the problem is not severe enough to pay additional attention to it, it means that we set our SLA incorrectly and need to make corrections. This case is the most typical for startup companies. Many small organizations have a certain period of growth when releasing new features is much more  important  than  any  other  metric.  The  number  of  users  at  that  time  is  very small, and minor inconveniences will be forgiven in favor of new functionality. If this describes your circumstances, the recommendation will be to downgrade your SLA to the status of an SLO and return to strict requirements later when the company gains a critical mass of customers. Another possibility is that we overestimated customer needs or, even more surprisingly, there might be an aspect of a service’s behavior that we previously treated as a minor one that over time became an exciting feature and a crucial part of the service. For the second way, we need to deal with the issue from the technical perspective. The team should have a goal to return the problematic metric closer to its original state. To be clear, the goal is not to prevent latency growth caused by further product development; the goal is to reduce and keep the latency within a specified limit. We might introduce a bit of latency with a new feature, but at the same time it might be possible to decrease it again by refactoring an old component. Practically, we might dedicate one or two people to work on the optimization, while the rest of the team will continue working on ongoing projects. After a while  two weeks or a month , swap the people working on optimization with two other team members, similar to an on-call rotation.  Choosing the exact way to go is not a technical decision but a man‐ agerial one. Changes in an SLA are a change in business priorities. And  changes  in  business  priorities  can  lead  a  company  to  rapid growth or immediate death. The SRE’s job here is to provide man‐ agement with a clear overview of the current situation and resolu‐ tion possibilities to help with making the right decision.  Finally, it would be a mistake to say that every project needs to have an SRE onboard. Consider what would happen if we were involved with a project that does not require a high availability level. What if a maintained service was down for an entire weekend  Dealing with Corner Cases      143   until  developers  brought  it  back  online  at  the  next  business  day  and  there  was  no negative impact at all? At the least, it would become a conflict of priorities because SREs  and  developers  do  not  share  the  same  availability  goals  and  will  drive  the project in opposite directions. The number one priority for the developers might be to deliver new features as fast as they can, regardless of service availability, and the SREs will work hard to slow them down to, at least a little bit, stabilize the product. For such a project, having a SysAdmin who can help with system-level duties like ini‐ tial  configuration  and  supplementary  software  maintenance  would  be  more  than enough. So, before trying to apply SRE approaches to a particular service, we have to make sure that we do it in the right place. Otherwise, we will waste a lot of time and energy. Conclusion The SRE philosophy differs from that of the SysAdmin just by the point of view.7 The SRE philosophy was developed based on a simple, data-driven principle: look at the problem from the user and business perspectives, where the user perspective focuses on “take care of product quality,” the business perspective pays attention to “manag‐ ing product cases and efficiency,” and “data-driven” signifies “not allowing assump‐ tions.”8 Identify, measure, and compare all that is important. Everything else is the result of this. This is the core of all SRE practices. If this all sounds very difficult and complicated, start with the following short list of instructions  and  summarized  potential  outcomes,  applying  it  step  by  step  to  the smallest service component you have:  1. Begin a new project with the following quote: “Start with the customer and work  backward.”  2. Divide large services into a set of components and treat each component as an  individual service. That will help us to identify problematic spots easily.  3. List the vital service indicators  SLIs  that will provide us with a better under‐ standing of what we should care about the most. Of course, we may not rely only on our beliefs and need to keep an eye on actual customers’ experience.  7 This philosophy differs “just by the point of view” and not by practices because, having originated from this point of view, the set of SRE practices can be shared among SREs and SysAdmins. However, the successful adoption of a practice does not immediately make an SRE from a SysAdmin.  8 Having assumptions is fine. However, the assumption cannot be stated as true unless it is supported by data.  For example, if it is believed that a service is working fine, it cannot be stated that it is fine unless there is a known measurable meaning of the word “fine,” and a collection of data that supports this “fine” statement assumption.  144      Chapter 9: From SysAdmin to SRE in 8,963 Words   4. Indicators being aggregated to a complete SLO will explicitly express the mean‐  ing of a “good” and “bad” service state.  5. If the service you’re working with is already serving production traffic, try to pick values for the initial SLO such that they reflect the current service availability as close as possible. Work from that. This kind of SLO will not create an immediate necessity to change the service to match the desired level. We can raise the bar as the next step.  6. Start with a 100% uptime. Compute an error budget and make a list of all “fail‐ ure” conditions that will reduce its size. It will help you track how much time is left for issues, maintenance, and experiments.  7. Enumerate all dependencies and their influence. This will be an important indi‐ cator  of  issue  sources  or  performance  regressions.  You  will  need  to  consider whether an issue is caused by a problem with the service itself or due to a service with which it communicates.  8. Test service performance against SLO barrier values. That will indicate the rela‐  tionship between traffic and required capacity.  9. During the test, measure not only how much incoming traffic the application is able to handle, but also how much additional load to external services it will gen‐ erate. This will approximate the ratio between input and output  for every service separately , and precisely estimate requirements for dependent services. Repeat tests periodically, track changes in trends, and adjust capacity accordingly. Over time,  an  SLO  will  display  the  effective  service  reliability  level,  indicate  regres‐ sions, and draw attention to problematic conditions.  10. Evaluate  necessities  and  priorities  for  all  technical  and  nontechnical  decisions  against the SLO and from a customer perspective.  11. Promote the SLO to SLA and establish a new objectives level; then move on to  your next service.  Now, you can control your systems more accurately and can precisely know when, what, why, and how they should be adjusted. Following the “Focus on the user and all else will follow” principle, you can develop and enrich a set of your own best practi‐ ces to gain efficiency, enjoy lower costs, and raise the bar for an overall positive cus‐ tomer experience.  Vladimir Legeza is a site reliability engineer in the Search Operations team at Amazon Japan. For the last few decades, he has worked for various companies in a variety of sizes and business spheres such as business consulting, web portals development, online gaming, and TV broadcasting. Since 2010, Vladimir has primarily focused on large- scale, high-performance solutions. Before Amazon, he worked on search services and platform infrastructure at Yandex.  Conclusion      145    CHAPTER 10 Clearing the Way for SRE in the Enterprise  Damon Edwards, Rundeck, Inc.  “Sounds great, but how would that ever work here?” Do you work in a medium- to large-sized enterprise? Did this question cross your mind while reading this book? Know that you are not alone. Changing how an organization does its operations is difficult at any scale. However, it is in the enterprise where the challenges and roadblocks to change will often seem like insurmountable mountains. Change your tools? Complicated, but doable at any size. Teach  individuals  new  skills?  Difficult,  slow  work,  but  there  are  known  paths  for everyone to follow. Fundamentally change how your organization works? This is where you hit that meta‐ phorical mountain in all but the smallest of organizations. Fear not, moving from a classic enterprise operations model to an SRE model is doa‐ ble. Companies are doing it right now as you read this book. This chapter is for enterprise leaders who are seeking to transform their traditional operations organizations to SRE. You will learn how to identify and clear the obsta‐ cles that, if left unaddressed, would otherwise undermine your SRE transformation. This  chapter  comes  from  a  compilation  of  knowledge  gained  while  working  with large enterprises to transform their operations organizations. These folks have taken the uncharted path to SRE so that your journey might be easier. First, I will examine the systemic forces that will be standing in your way. Second, I will highlight techniques for clearing those obstacles so that you can start and sustain your SRE transformation.  147   How  should  you  measure  success?  Improved  reliability,  improved  MTTD MTTR,1 improved  organizational  agility,  and  fulfilled  effective  colleagues.  What  is  failure? Ending up with some new “SRE” job titles, but not much else changing. Toil, the Enemy of SRE Toil is the hidden villain in the journey to SRE. In organizations that have already internalized the SRE way of working, spotting and eliminating toil is a natural reflex. In organizations that are coming from a traditional operations culture, spotting and eliminating toil is a organization-wide skill that often needs to be learned. What is toil? Vivek Rau from Google articulates this answer well: “Toil is the kind of work tied to running a production service that tends to be manual, repetitive, auto‐ matable,  tactical,  devoid  of  enduring  value,  and  that  scales  linearly  as  a  service grows.” The more of these attributes a task has, the more certain you can be that the task should be classified as toil. Being classified as toil doesn’t mean that a task is frivolous or unnecessary. On the contrary,  most  organizations  would  grind  to  a  halt  if  the  toil  didn’t  get  done.  The value  of  toil  is  often  a  point  of  confusion  among  traditional  enterprise  operations teams. To some, manual intervention in the running of services is their job descrip‐ tion. Just because a task is necessary to deliver value to a customer, that doesn’t necessarily mean that it is value-adding work. For people who are familiar with Lean manufac‐ turing principles, this isn’t dissimilar to Type 1 Muda2  necessary, nonvalue adding tasks . Toil can be necessary at times, but it doesn’t add enduring value  i.e., a change in the perception of value by customers or the business . Instead of your SREs spending their time on non-value-adding toil, you want them spending as much of their time as possible on value-adding engineering work. Also pulling from Vivek Rau’s helpful definitions, engineering work can be defined as the  1 Mean Time to Detect  MTTD  is the average length of time between the onset of a problem and the problem  being detected. Mean Time to Repair  MTTR  is the average length of time between the onset of a problem and the resolution of the problem. Although these are commonly used operations metrics, their use is not without controversy. The first point of controversy is how precise of a judgment you can make using these metrics to evaluate operational performance. If no two incidents are alike, how valid is looking at an average? The other controversy is over which metric should take priority. Rapid detection can be an indicator of a sys‐ tem better instrumented and better understood, leading to more consistent resolution and better prevention. Rapid repair might indicate better automation or failover capabilities, and repairing restoring is the ultimate goal in any incident.  2 Womack, J. P., & Jones, D. T.  2010 . Lean Thinking: Banish Waste and Create Wealth in Your Corporation.  Riverside: Free Press.  148      Chapter 10: Clearing the Way for SRE in the Enterprise   creative and innovative work that requires human judgment, has enduring value, and can be leveraged by others  see Table 10-1 .  Engineering work  Table 10-1. Comparing characteristics of toil and engineering work Toil Lacks enduring value Builds enduring value Rote, repetitive Tactical Increases with scale Can be automated  Creative, iterative Strategic Enables scaling Requires human creativity  Working in an organization with a high ratio of engineering work–to-toil feels like, metaphorically speaking, everyone is swimming toward a goal. Working in an orga‐ nization with a low ratio of engineering work–to-toil feels more like you are treading water, at best, or sinking, at worst. A  goal  of  “no  toil”  sounds  nice  in  theory  but,  in  reality,  that  isn’t  attainable  in  an ongoing  business.  Technology  organizations  are  always  in  flux,  and  new  develop‐ ments  expected or unexpected  will almost always cause toil. The best we can hope for is to be effective at reducing toil and keeping toil at a man‐ ageable level across the organization. Toil will come from things you already know about but just don’t have the time or budget to automate initially  e.g., semimanual deployments, schema updates rollbacks, changing storage quotas, network changes, user adds, adding capacity, DNS changes, and service failover . Toil will also come from  any  number  of  the  unforeseen  conditions  that  can  cause  incidents  requiring manual  intervention   e.g.,  restarts,  diagnostics,  performance  checks,  and  changing config settings . Toil might seem innocuous in small amounts. Concern over individual incidents of toil is often dismissed with a response like “There’s nothing wrong with a little busy work.” However, when left unchecked, toil can quickly accumulate to levels that are toxic to both the individual and the organization. For the individual, high levels of toil lead to the following:    Discontent and a lack of feeling of accomplishment   Burnout   More errors, leading to time-consuming rework to fix   No time to learn new skills   Career stagnation  hurt by a lack of opportunity to deliver value-adding projects   For the organization, high levels of toil lead to the following:  Toil, the Enemy of SRE      149     Constant shortages of team capacity   Excessive operational support costs   Inability  to  make  progress  on  strategic  initiatives   the  “everybody  is  busy,  but  nothing is getting done” syndrome     Inability  to  retain  top  talent   and  acquire  top  talent  after  word  gets  out  about  how the organization functions   One of the most dangerous aspects of toil is that it requires engineering work to elim‐ inate  it.  Think  about  the  last  deluge  of  manual,  repetitive  tasks  you  experienced. Doing those tasks doesn’t prevent the next batch from appearing. Reducing  toil  requires  engineering  time  to  either  build  supporting  automation  to automate away the need for manual intervention or enhance the system to alleviate the need for the intervention in the first place. Engineering work needed to reduce toil will typically be a choice of creating external automation  i.e., scripts and automation tools outside of the service , creating inter‐ nal automation  i.e., automation delivered as part of the service , or enhancing the service to not require maintenance intervention. Toil eats up the time needed to do the engineering work that will prevent future toil. If  you  aren’t  careful,  the  level  of  toil  in  an  organization  can  increase  past  a  point where the organization won’t have the capacity needed to stop it. If aligned with the technical  debt  metaphor,  this  would  be  “engineering  bankruptcy,”  as  illustrated  in Figure 10-1.  Figure 10-1. Excessive toil consumes a team’s capacity to perform engineering work to both improve the business and reduce toil  150      Chapter 10: Clearing the Way for SRE in the Enterprise   The SRE model of working—and all of the benefits that come with it—depends on teams having ample capacity for engineering work. If toil eats up that capacity, the SRE model can’t be launched or sustained. An SRE perpetually buried under toil isn’t an SRE, just a traditional long-suffering SysAdmin with a new title. Toil in the Enterprise Enterprises are fertile ground for toil. First, when it comes to the concept of toil, leg‐ acy operations management philosophies were either blind  “Everybody looks busy. Great efficiency!”  or indifferent  “Why are you complaining about these headaches? I pay you to have headaches.” . Second, the high level of organizational complexity found in an enterprise creates toil and gets in the way of efforts to reduce it. For this discussion, let’s define an “enterprise” as any company that has had the his‐ torical success needed to accumulate a significant amount of legacy  culture, organi‐ zation, process, and technology . Enterprises have a distinct “look.” From a business perspective, you’ll find multiple business lines, each born or acquired during different eras that had unique context and  underlying  assumptions.  From  a  technology  perspective,  you’ll  find  multiple generations of platforms and tools—some brand new, some old and evolving, some orphaned—that all need to hang together to provide services to customers. It is good to keep in mind that nothing in an enterprise lives in isolation. What you are working on is dependent on others. What others are working on is dependent on you.  In  classic  architectures,  these  dependencies  are  fixed  and  obvious.  In  modern architectures,  these  dependencies  are  often  dynamic  and  abstracted  away,  but  still exist. At the human level, incentives, budgets, policies, beliefs, and cultural norms are all intertwined across the various ends of an enterprise. This interconnectedness makes eliminating toil significantly more challenging in the enterprise. The toil of your team’s own making can be eliminated with straightfor‐ ward engineering work. But what about all of the toil that is due to conditions or sys‐ tems that exist in other parts of the organization? Eliminating it is out of a team’s control unless that team can effect solutions across organizational boundaries, which anyone with enterprise experience knows is no trivial feat. Toil  that  is  partially,  or  wholly,  out  of  a  team’s  control  is  especially  dangerous.  It pushes the team closer to that bankruptcy threshold at which toil crowds out all engi‐ neering work. This is a common cause of the antipattern where SysAdmin teams are rebranded  “SRE  teams,”  but  the  lack  of  engineering  blocks  transformation  beyond the new name.  Toil in the Enterprise      151   Silos, Queues, and Tickets After you’ve established that excessive toil will prevent an enterprise from shifting to an SRE model, it logically follows that you are going to have to work across organiza‐ tional boundaries in order to effectively contain toil. However, working across organ‐ izational boundaries is one of the great challenges in enterprise IT. Working across organizational boundaries is difficult because of a confluence of silo effects, request queues, and the most venerable of all IT operations workhorses, tick‐ ets. Silos Get in the Way The silo metaphor is first attributed to Phil S. Ensor, who used it in 19883 to describe the organizational challenges at his employer, Goodyear Tire. Since then, the concept of “silos” has been discussed by the Lean manufacturing movement and the DevOps movement. Some people mistakenly think “silos = teams,” but in reality, silos don’t have  that  much  to  do  with  organizational  structure.  The  idea  of  a  silo  really  has everything to do with how a group s  works within an organization. In simple terms, a group is said to be “working in a silo” when its members are work‐ ing in a disconnected manner from other groups  whether they know it or not , as depicted in Figure 10-2. When spotting silos, look for situations in which a group is working in a different context from other groups, their work is coming from a differ‐ ent  source  than  other  groups   i.e.,  different  backlogs ,  and  the  group  is  working under  different  incentives  or  priorities   and  often  part  of  a  different  management chain . It is almost certain that this group is working in a silo and experiencing any number of symptoms: bottlenecks, slow handoffs, miscommunication, tooling mis‐ matches, delivery errors, excessive rework, and conflict  usually the finger-pointing type .  3 Ensor, Philip. S.  1988, Spring . The Functional Silo Syndrome. Target. Association for Manufacturing Excel‐  lence.  152      Chapter 10: Clearing the Way for SRE in the Enterprise   Figure 10-2. Silos describe a way of disconnected working rather than a specific organi‐ zational structure  If you’ve worked in operations in an enterprise, you probably recognize this condi‐ tion. Some might even ruefully describe it as “the way things have always worked.” It is endemic to the classic operations model in which you have lots of specialist teams divided by functional expertise and use ticket systems and a heavy reliance on project management to coordinate work and push it through the organization. Operations teams don’t set out to work in silos or suffer the consequences of silos. It is usually the natural byproduct of traditional management philosophies based on the human  urge  to  “optimize”  large-scale  efforts  by  sorting  people  according  to  func‐ tional  specialization,  grouping  like  with  like,  and  then  incentivizing  them  to  look inward and optimize. Problematic  handoffs   too  slow,  incorrect,  lots  of  rework,  etc.   are  the  most  com‐ monly cited problem that can be attributed to silo effects. This makes sense given that silos only become a problem when you need something from someone outside of the silo  or someone outside of the silo needs something from you . And remember, in the enterprise, nothing lives in isolation. Doing anything signifi‐ cant  usually  means  information  and  work  is  going  to  have  to  cross  one  or  more organizational boundaries. What goes wrong when work has to pass between siloed groups? It usually has to do with mismatches  see Figure 10-3 :  Silos, Queues, and Tickets      153   Information mismatches  The parties on either side of the handoff are working with different information or  are  processing  the  information  from  different  points  of  view,  leading  to  an increase in errors and rework  i.e., repeat work due to previous errors .  Process mismatches  The parties on either side of the handoff are following either different processes or processes that are nominally the same but take a different approach and pro‐ duce results not expected by the other party. Timing and cadence mismatches between  parts  of  a  process  that  take  place  in  different  silos  also  lead  to  an increase in errors and rework.  Tooling mismatches  An increase in errors and rework is seen when different parties on either side of silo boundaries are using different tooling or tooling that isn’t set up to connect seamlessly. When the work needs to be translated on the fly by a person moving information and artifacts by hand from one tool to another, delay, variance, and errors are bound to be introduced into the process.  Capacity mismatches  Bottlenecks and delays occur when the amount or rate of requests coming from one side of the silo boundary exceeds the capacity of those fielding the requests. Requests levels often exceed or are below expectations, which has the ripple effect of disrupting the planning and flow of work for other parts of the organization.  Figure 10-3. Handoffs between silos are problematic due to mismatches  154      Chapter 10: Clearing the Way for SRE in the Enterprise   Ticket-Driven Request Queues Are Expensive For decades, the go-to countermeasure for dealing with the handoff problems caused by silos is to insert a request queue to govern the handoff  usually a ticket system . On the surface, request queues might seem like an orderly and efficient way to man‐ age work that crosses the divides of an organization. However, if you look under the surface, you will find that request queues are a major source of economic waste for any  business.  Let’s  look  at  the  following  list,  created  by  noted  author  and  product development  expert,  Donald  G.  Reinertsen,  that  catalogs  the  negative  impact  of queues  see also Figure 10-4 :4 Longer cycle time  Queues increase cycle time because it takes longer to reach the front of a large queue than a small one. Even small delays can exponentially compound within a complex interdependent system like an enterprise IT organization.  Queues increase the time between request and fulfillment, which in turn increa‐ ses likelihood of the context of the original request changing  a race condition . If a problem does arise, the requestor is now in a different mental position  often working on something else  from where they were when they made the request.  Longer queues lead to high levels of utilization, and higher levels of utilization amplify  variability.  This  leads  to  longer  wait  times  and  a  higher  likelihood  of errors.  Queues add a management overhead for managing the queue, reporting on sta‐ tus,  and  handling  exceptions.  The  longer  the  queue,  the  more  these  overhead costs grow in a compounded manner.  Increased risk  More variability  More overhead  Lower quality  Queues lower quality by delaying feedback to those who are upstream in the pro‐ cess. Delays in feedback cause the cost of fixing problems to be much higher  e.g., bugs are easier to fix when caught sooner  and often mean that additional prob‐ lems  of  a  similar  origin  have  been  created  before  the  first  negative  feedback arrives.  4 Reinertsen, Donald G.  2009 . The Principles of Product Development Flow: Second Generation Lean Product  Development. Redondo Beach, CA: Celeritas Publishing.  Silos, Queues, and Tickets      155   Less motivation  Queues have a negative psychological effect by undermining motivation and ini‐ tiative.  This  is  due  to  queues   especially  longer  queues   removing  the  sense  of urgency and immediacy of outcomes from the requestor’s work. If you don’t feel the impact and don’t see the outcome, it’s human nature to grow negatively dis‐ connected from the work.  Figure 10-4. Queues have been proved to be economically expensive  Looking at the science behind the impact of queues, it is difficult to justify inserting queues  all  over  your  organization.  It  doesn’t  make  sense  to  willingly  inject  longer cycle times, slower feedback, more risk, more variability, more overhead, lower qual‐ ity, and decreased motivation into your organization. Despite this, what is the most common method of managing work within IT opera‐ tions organizations? Request queues in the form of ticket systems. When an organization manages the interaction between people working in other silos via tickets, you are undermining people’s capacity for value-adding engineering work both directly  more wait time, more overhead, more disconnects, more errors  and indirectly  more toil . Worse still, much of this lack of capacity and increased toil is beyond their capability to fix because the cause is in another silo. In  the  coming  sections,  I  discuss  how  to  eliminate  silos,  queues,  and  tickets—and how to avoid their harmful effects where they can’t be eliminated.  156      Chapter 10: Clearing the Way for SRE in the Enterprise   Take Action Now Hopefully, the first part of this chapter made the case that SRE demands a change to the fundamental conditions prevalent in legacy enterprise operations organizations. The following sections lay out steps that you can take to clear the obstacles to an SRE transformation. As with any transformation, you should favor a continuous improvement approach. Your organization is a complex system. Big bang transformations of complex systems are risky and have a poor record of success. No matter how much you plan up front for  an  SRE  transformation,  it  will  likely  go  differently  after  you  begin  moving. Encourage your team to embark on a series of steady, deliberate actions. The suggestions in the rest of this chapter should not be looked at as prerequisites to taking action or a definitive formula to follow  i.e., “the one true way” . These are patterns  and  lessons  to  apply  on  a  continuous  basis.  Gather  your  new  SRE  team. Empower it to begin transforming how it works. Empower the team to reach across organizational boundaries and collaborate on improvements that will benefit every‐ one. Favor action and continuous improvement. Start by Leaning on Lean If you are going to transform how your operations organization works, you might as well leverage a proven body of transformational knowledge. The Lean manufacturing movement  has  produced  a  wealth  of  design  patterns  and  techniques5  that  we  can apply to improve any work process. In particular, it is the principle of Kaizen  which roughly translates to “continuous improvement” , born from the Toyota Production System, that speeds transformations and drives an organization’s ability to learn con‐ tinuously.  To  bring  Kaizen  to  an  organization,  there  is  a  method  called  Kata,  also based on Toyota Production System. Kata is an excellent methodology to apply to the challenge of eliminating toil, silos, and request queues. Kata encourages organizations to look at the end-to-end flow of work and methodically experiment until the desired outcome is reached. Teams are encouraged to see beyond their work silo and think holistically about problems. Kata will help you identify what stands in the way of your goals and then stay aligned as you iterate toward those goals. It is important to point out that Kata is about teams improving themselves. Lasting performance improvement comes from when people know how to fix problems as part of their day-to-day work. One-off projects or external help might provide a spe‐  5 https:  www.goldratt.com and https:  www.lean.org  Take Action Now      157   cific benefit at a specific point in time. However, enterprises don’t stand still. New challenges, large and small, will arise all the time. SRE teams are meant to do engineering work to improve the reliability and operabil‐ ity  of  systems  and  reduce  toil   freeing  up  more  time  for  engineering  work .  SRE teams, by definition, must be learning teams who can spot and fix problems as part of their day-to-day work. Kata is an excellent reference to teach new SRE teams how to think and work like that. The  Kata  approach  is  already  becoming  a  common  practice  in  Agile  and  DevOps efforts looking to improve development and delivery processes. However, operations processes are rarely deemed worthy of the effort, especially in legacy operations cul‐ tures. This oversight is unfortunate because operational quality is equal in value to any  other  quality  measure  in  your  organization.  Even  ad  hoc  operations  processes  e.g., one-offs due to an incident or similar type of event  are processes that are wor‐ thy of study. If you are considering the move to SRE, you should assume that your organization already knows the value of investing in improving operations work. If not, a conver‐ sation on value needs to happen among both technical and business leaders in your company. There are many books, presentations, and other resources available to take you deep into the practice of Kata. I highly recommend the works of Mike Rother6 and John Shook.7 However, part of the beauty of Kata is that you don’t need much knowledge to get started and begin seeing benefits. Here is an overview of the Kata process in an operations context:  1. Pick a direction or challenge. This is your higher-level directional goal. Where do you want to go as an organization? What is the business value that you are there to maximize? What is the ideal state of operations work at your company? It is critical that there is consensus on what the value of operations is to the busi‐ ness and what performance and reliability levels need to be in order to maximize that  value.  The  entire  organization   from  frontline  engineers  to  leadership  should know why they are embarking on this SRE transformation and how it will improve the business.  6 “The Improvement Kata” 7 Rother, Mike, and John Shook  2009 . Learning to See Value-Stream Mapping to Create Value and Eliminate  Muda. Cambridge, MA: Lean Enterprise Institute; Shook, John  2010 . Managing to Learn: Using the A3 Management Process to Solve Problems, Gain Agreement, Mentor, and Lead. Cambridge, MA: Lean Enterprise Institute.  158      Chapter 10: Clearing the Way for SRE in the Enterprise   The answers don’t need to be overly detailed or address how you are going to get there. However, if the answers end up sounding like general platitudes or a vague mission statement, that is a failure  e.g., “Delight customers with highly available and speedy services” . Keep trying until people have a decent notion of where the organization  needs  to  go  and  why.  There  should  be  some  recognition  of  both measurable operational outcomes  e.g., reducing incidents, improving response times, and increased frequency of change  and desired business outcomes  e.g., increased sales and higher net promoter score .  2. Grasp  the  current  condition.  Build  a  clear  understanding  of  how  things  work today.  Take  an  end-to-end  view  of  the  process  that  you  want  to  improve  and strive to understand how it is actually done today. Why does it happen this way? What is needed to get it done? Who is needed to get it done? What gets in the way? When does it go wrong? Be  sure  to  look  at  each  of  the  types  of  work  in  your  organization.  Project- oriented  work  is  an  obvious  choice   systems  engineering,  environment  build outs,  etc. .  It  is  equally  important  to  look  at  incidents   i.e.,  failure  scenarios . Examining incidents as processes might sound odd because incidents rarely fol‐ low much of a standard process beyond some communication and information gathering  formalities.  However,  if  you  look  at  enough  incidents,  you  will  find common patterns. Incidents provide a very enlightening look into how an orga‐ nization is really “programmed” to work. Remember, silos are your enemy. Doing this kind of analysis in private or limit‐ ing it to a team of “experts” delivers minimal value to your organization. Enter‐ prises  are  notorious  for  their  compartmentalization  of  information.  Very  few people will know how end-to-end processes actually happen—and even they will disagree. You are bound to open a lot of eyes and hear comments like, “Hmm, that’s not how I thought that worked,” or, “I never knew that,” from even the longest-tenured employees. That alone is worth the effort. Do your analysis in the open and encourage participation from as many people as possible. Look upstream and downstream for participants with knowledge to contribute.  Invite  developers,  program  managers,  and  other  operations  teams. Your transformation to SRE has the best chance of success if as many people as possible have a similar understanding of what needs to change. Visual analysis is a very effective way to get a group of people aligned—assuming you  do  the  visual  analysis  as  a  group.  In  Lean  terminology,  this  is  known  as “Going to the Gemba,” which roughly translates to “going to the place where the work  happens,”  In  the  physical  manufacturing  world,  this  means  going  to  the factory and directly observing the work firsthand. In  IT  operations,  you  can’t  actually  see  most  of  the  work  happen.  Other  than some artifacts, the work is all abstractions and individual’s mental models. So,  Start by Leaning on Lean      159   Going to the Gemba in our context only works if you are getting people together and aligning those mental models the best you can in a process retrospective ses‐ sion.  Figure  10-5  shows  the  results  of  one  of  these  sessions.  This  alignment  is extremely  important  because  those  individual  mental  models  are  the  lens through  which  every  person  in  your  organization  evaluates  and  executes  their daily work.  Figure 10-5. Artifacts from visual analysis created during a process retrospective session  So how do you do this? This visual analysis works best when you look at one pro‐ cess at a time. Pick a process  a specific delivery of project work or a specific inci‐ dent .  Get  people  together  who  have  first-hand  knowledge  of  the  process  enough to look at it end-to-end . Draw out what actually happened  focus on the  flow  of  work,  with  people  and  tools  as  second-level  supporting  details . Finally, get the participants to identify and discuss what got in the way. Repeat this with enough instances of the process  or group of related processes  until there is reasonable consensus on how the process happens, why it happens  if it is an incident , and what gets in the way. There  are  other  resources  you  can  draw  on  to  develop  your  visualization  and analysis  technique.  Value  Stream  Mapping8  and  Lean  Waste  classification  are highly useful.  3. Establish your next target condition. This step is where you decide on the next improvement target that your organization will attempt to reach. Again, this is best done as a group. Based on the directional goal determined in step 1 and the current condition uncovered in step 2, what is the next major intermediate step we are going to try to accomplish? This is what the organization is going to focus on, so everyone should be able to articulate what it should be like when the target condition is achieved.  8 Rother, Mike, and John Shook.  2003 . Learning to See: Value-Stream Mapping to Create Value and Eliminate  Muda. Cambridge, MA: The Lean Enterprise Institute.  160      Chapter 10: Clearing the Way for SRE in the Enterprise   As  Mike  Rother  advises,  “Setting  the  target  condition  is  not  about  choosing between  existing  options  or  best  practices.  It’s  about  aspiring  to  new  perfor‐ mance. By setting a target condition and trying to achieve it, you learn why you cannot. That’s what you work on.”  4. Experiment  toward  the  target  condition.  Get  the  organization  into  an  iterative rhythm of forming hypotheses  e.g, “If we could stop start x, it would reduce  increase  y  by  z  amount” ,  testing  alternatives   e.g.,  process,  tooling,  or  org changes ,  and  evaluating  the  result.  If  the  result  moves  you  toward  the  target condition, implement whatever was tested and continue with other experiments. If you see the Scientific Method in this, you are correct. Repeat the experimenta‐ tion until you’ve reached the next target condition.  During step 4 of this process, be sure you revisit step 2 often to make sure everyone still grasps the current condition. Also, make sure that you are reviewing the specifi‐ cation of the next target condition often to ensure that everyone stays aligned. When you hit the target condition, repeat step 1  “is the goal still valid?”  and then steps 2 through 4. Get Rid of as Many Handoffs as Possible As was described earlier in this chapter, silos and their accompanying problematic handoffs and expensive request queues do considerable damage to an organization. So, perhaps it is obvious that the first strategy to relieve this problem is to get rid of as many silos and handoffs as you can. Forward-thinking organizations are transforming from a traditional “vertical” struc‐ ture aligned by functional skill to a “horizontal” structure aligned by value stream or product.  The  vertical  structure  is  the  classic  divide-by-function  strategy   Dev,  QA, Ops, Net, Sec, etc. . The horizontal structure comprises cross-functional teams that can own the entire end-to-end life cycle of a service. The idea behind cross-functional teams is that they will handle as much of the life cycle as possible without needing to hand off work to other teams. There are no sig‐ nificant handoffs or breaks in context, and everyone’s work flows from a single back‐ log aligned by common priorities. Bottlenecks are largely avoided, feedback loops are shorter, and cycle times are quicker. If a problem does arise, it is easier for a cross- functional team to respond and rectify the problem, as illustrated in Figure 10-6.  Get Rid of as Many Handoffs as Possible      161   Figure 10-6. Cross-functional teams alleviate the need for many handoffs  These  cross-functional  teams  are  also  often  referred  to  as  service-oriented  teams, product-oriented  teams,  or  market-oriented  teams.  These  labels  emphasize  that  the goal of these teams is alignment toward a business-recognizable delivery of value to a customer  i.e., a specific customer-identifiable service . This alignment allows for the team members to understand how their work impacts the business and then optimize the team for maximizing customer value  rather than functional efficiency . Although the idea of cross-functional teams appears simple, implementation is a sig‐ nificant structural and cultural change for enterprises. To understand why this is such a significant shift for enterprises, look no further than the rules of corporate accounting. Admittedly, this is an area in which most engineers thought  they  would  never  have  to  delve.  However,  some  basic  concepts  provide  a window into some of the most fundamental forces that shape an enterprise. Project-based funding is often the primary flow of money in enterprise IT. After a business need is identified, there is a process to define a project and fund it with a specific  budget.  After  funding,  the  project  will  proceed  through  the  various  func‐ tional silos of the IT organization until the project reaches production and is deemed complete. The next iteration is generally looked upon as a new project with a distinct budget. Project-based funding increases the pressure on operations in several ways:    First, the proliferation of services and bespoke infrastructure generates a signifi‐ cant  amount  of  toil  for  operations.  When  an  organization  is  project  oriented, teams move from project to project, often leaving a trail of new software and new infrastructure  behind.  The  movement  of  teams  to  new  projects  deprives  the teams  of  learning  from  operational  feedback.  Also,  as  discussed  earlier  in  this  162      Chapter 10: Clearing the Way for SRE in the Enterprise   chapter,  the  constant  creation  of  net  new  software  and  infrastructure   even  if built following documented patterns  creates new technical debt and errors from unknown  conditions.  These  behaviors  are  continuous  sources  of  toil  for  the teams on which operational responsibility is dumped.    Second,  in  the  drive  to  hit  a  project’s  finish  line,  operational  concerns  are  not always adequately addressed before being pushed to production. Delivering the project at or under the budget is what the delivery teams are judged on. It is only human nature that operational concerns like stability, scalability, deployability, configurability, observability, and manageability tend to get superficial treatment or are often the first thing cut in a time crunch. In the worst cases, these opera‐ tional concerns are not considered at all. Although the high-value development resources are quickly recycled and reattached to projects, an operations team has to catch each project and own reliability and scale. In a traditional model, this is a continuous stream of toil that usually requires adding more headcount. How‐ ever, even assuming that the operations teams are equipped with the skills to do engineering work in an SRE model, these teams are still in perpetual catch-up mode, and engineering bankruptcy is always a risk if toil levels get too high.    Third, with projects being the primary funding vehicle, operations budgets are viewed as mostly operations and maintenance expenses, or OpEx in accounting lingo. OpEx is the budget category that gets the most scrutiny and cost controls because it directly impacts the current year’s profitability. Teams whose funding largely comes from OpEx are naturally susceptible to management impulses for organization-by-function  and  efficiency  mandates,  both  of  which  encourage siloed working.  Toil mostly appears around operations and maintenance work, which is OpEx. Gen‐ erally, all of the project-based funding is CapEx  capital expenses . Engineering work to build or improve a system is usually CapEx because it is improving an asset  and can be amortized over several years, so the negative impact on current year profit is smaller . A team funded out of an OpEx budget and being managed for efficiency often won’t have the budget  or the general charter  to engage in significant engineer‐ ing work. To move to an SRE model, you do not need to be an accounting expert. However, it does pay dividends to be acutely aware of how the money flows in your company. If you want your move to SRE to be more than a change in job titles, you are going to need to make the case that the SRE team should be funded to do engineering work and attached to teams who own the service life cycle, from inception to decommis‐ sioning. The move from project-based funding to product-based funding  with cross- functional teams  is already gaining traction in Agile and DevOps discussions. If this idea is making headway in your organization, try to leverage it for your SRE transfor‐ mation.  Get Rid of as Many Handoffs as Possible      163   Two  common  patterns  are  shaping  up  in  enterprises  experimenting  with  cross- functional teams and SRE. The first is to have SREs  and other functional roles  join dedicated product teams. This creates cross-functional teams that can own a service from inception to decommissioning. Development and ongoing operations all hap‐ pen from within these cross-functional teams. From an enterprise perspective, this is often seen as a radical departure from traditional organization models. Sometimes, you might hear this referred to as the “Netflix” model. The  second  pattern  is  to  have  SRE  remain  a  distinct  organization.  Development teams have SRE skills and initially will own the full life cycle of a service. When the service  has  reached  a  certain  level  of  performance  and  stability,  there  is  a  formal handoff to an SRE team that has embedded development skills. This SRE team man‐ ages  the  availability  and  scalability  of  the  service  according  to  performance  agree‐ ments made with the development teams. If further changes made by the development team drop the performance of the ser‐ vice below the agreed-upon levels, there is a mechanism to return more operational responsibility to the development team. From an enterprise perspective, the general shape  of  this  model  seems  the  most  familiar.  However,  how  work  happens  in  this model is still a radical departure from traditional operations models. Sometimes this is referred to as the Google model. Of course, employees from both Netflix and Google will be quick to point out that there  is  a  lot  more  nuance  to  either  model.  However,  these  descriptions  provide  a possible high-level starting point for thinking about how to devise a model that works best for your company’s unique conditions. The options available to you will likely be constrained by how the rest of your com‐ pany  wants  to  operate.  For  example,  changing  organizational  structure  to  move  to product-aligned  teams  entirely  will  require  buy-in  from  at  least  development  and product management  which will likely need broader business discussions . Staying with a traditional development and operations organizational divide has the benefit of not upending the entrenched political structures within your company, but at the same time, those political structures can reinforce the old ways of working and unin‐ tentionally undermine improvement efforts. The visualization techniques described in step 2 of the Kata process  grasp the cur‐ rent condition  can help with discussions on organizational structure changes. Not only  does  visualization  help  people  understand  the  flow  of  work,  but  it  also  helps them see how organizational structure impacts that flow of work. In  any  model  you  choose,  there  is  still  bound  to  be  culture  shock  when  the  walls between traditionally development-only teams and operations-only teams are broken down. SREs play an essential role in bringing operations skill and experience to pre‐ viously development-only teams. In the physical-fitness world there is an expression,  164      Chapter 10: Clearing the Way for SRE in the Enterprise   “Great  abs  are  made  in  the  kitchen,  not  in  the  gym.”  Likewise,  “Great  operations begins  in  development,  not  in  production.”  SRE’s  play  the  critical  role  in  bringing operations skills and discipline to previously development-only teams. Replace Remaining Handoffs with Self-Service There are always cases in the enterprise in which you cannot put all of the required skills and knowledge into cross-functional teams. For practical, financial, or political reasons,  most  companies  operating  at  significant  scale  cannot  avoid  having  func‐ tional specialist teams. Networks, platforms, security, and data management are com‐ mon  areas  in  which  enterprises  rely  on  centralized  teams  because  of  either  not enough specialists to go around or the need  perceived or real  to centralize control. The presence of these specialist teams means that handoffs can’t be avoided during the normal flow of work through the organization. Either you need something from one of them to get your work done, or you are on one of these specialist teams where everyone needs something from you. Dependencies between services  a fact of life in the enterprise  require teams to con‐ sume services from other teams and make operational requests of those teams  e.g., configuration changes, health checks, performance tuning, deployment coordination, and adding accounts . This creates yet another set of unavoidable handoffs. As was discussed earlier, whenever there is a handoff point, and work has to move from one team’s context to another, there is an opportunity for silo effects to take hold and create problems. Because you cannot get rid of all handoffs, you need to apply techniques and tooling to mitigate the negative impact of those handoffs. Deploying the traditional solution of ticket-driven request queues is expensive, is toxic to the organization, and should only be a last resort. Instead, the go-to solution should be to deploy self-service capabilities at those hand‐ off  points.  These  self-service  capabilities  should  provide  pull-based  interfaces  to whatever was previously needed to be done by someone on the other end of a request queue  investigating performance issues, changing network firewall settings, adding capacity, updating database schemas, restarts, etc. . The point of self-service is to stay out of the way of people who need an operations task completed. Rather than having someone fill out a ticket and sitting in a request queue, you give them a GUI, API, or command-line tool to do it themselves, when they  need  to  do  it.  This  capability  eliminates  wait  time,  shortens  feedback  loops, avoids miscommunication, and improves the labor capacity of the teams that previ‐ ously had to field those requests, freeing them from repetitive requests so that they can focus on value-adding engineering work.  Replace Remaining Handoffs with Self-Service      165   Self-Service Is More Than a Button The idea of self-service isn’t new. However, the traditional approach to self-service is to  have  a  privileged  operations  team  create  somewhat  static  “buttons”  for  less- privileged teams to push  e.g., push-button deployment for new .war files . There are a limited number of scenarios in which this static approach works. Also, it still leaves the higher-privileged team as the bottleneck because they are the ones who need to build—and maintain—the button and the underlying automation. This maintenance burden alone limits the amount of static self-service capabilities an operations orga‐ nization can expose. To maximize the effectiveness of self-service, the ability to both define and execute automated  procedures  needs  to  be  provided,  as  demonstrated  in  Figure  10-7.  Of course, you need to put constraints in place to enforce security boundaries and help prevent mistakes, but providing the ability to define and execute delivers the most value.  Figure 10-7. Traditional ticket-driven request fulfillment versus full self-service  For example, consider the Elastic Compute Cloud  EC2  service from Amazon Web Services  AWS . The ability to press a button and get a running virtual machine was interesting. However, the ability to control your own destiny by making your own machine images  AMIs  and configuration was revolutionary. It empowered individ‐ uals and allowed teams to decouple and move at their own pace. However, it isn’t unfettered access. Users are constrained for security reasons by both AWS and their own self-selected security policies. Users are also constrained in their choices to pro‐ vide “guardrails” to keep users of the system from making some types of mistakes or impacting performance of other users. In this example, the ability to define and exe‐ cute automation is pushed to end users and governance is shared by the operators  AWS in this case  and end users.  166      Chapter 10: Clearing the Way for SRE in the Enterprise   To maximize the value of self-service in the enterprise, replicate this pattern of mov‐ ing the ability to define and execute automation to wherever in the organization it will improve the flow of work. Self-Service Helps SREs in Multiple Ways Within an enterprise, effective self-service capabilities are a boost to SRE efforts. The following is a list of some of the benefits: Reduces toil  With  few  exceptions,  fielding  repetitive  requests  is  toil.  Having  effective  self- service capabilities can help SREs reduce toil by quickly turning around automa‐ tion to reduce those repetitive requests. Often repetitive requests do not follow the same pattern each time, and this can undermine an SRE’s ability to set up reusable automation. If you can set up the right primitives and then give reques‐ tors the ability and permission to create their own automated procedures, you can create self-service for an even broader set of repetitive requests. Examining which  self-service  processes  end  up  being  built  can  also  indicate  to  both  SREs and  developers  where  their  current  engineering  efforts  should  be  focused  to reduce the need for future intervention. Alleviates security and compliance concerns  Security and regulatory compliance are unavoidable facts of life in the enterprise. Whether it is organizational scar tissue from past problems, a response to indus‐ try fears, or a directive from an auditor, your SRE transformation will need to work within existing security and compliance requirements. I do not advise try‐ ing to introduce a new operations model and question existing security or com‐ pliance policies. Pick your battles. Self-service capabilities can provide both a system of record for operations activ‐ ity and a point of policy enforcement. You can use the same self-service mecha‐ nisms  within  teams  and  across  team  boundaries.  This  will  give  you  both  the ability to track operational activity to meet compliance requirements and a way to safely expand the distribution of access privilege. By doing so, SREs can work across a wider scope of your infrastructure than was previously permitted. This also enables your traditionally nonoperations colleagues to participate in opera‐ tions activity without opening tickets for others to do it for them. Separation of duties is a standard requirement of most enterprises. No matter if it is to comply with specific regulations  e.g., PCI DSS Requirement 6.4  or to sat‐ isfy  more  general  regulatory  controls,  separation  of  duties  can  interfere  with  a team’s plans to take cross-functional ownership of the development, testing, and operations of a service.  Replace Remaining Handoffs with Self-Service      167   Self-service tooling can help by providing a mechanism through which a person in a development role can create a procedure that someone in an operations role can quickly vet and execute. Also, those in privileged operations roles can create preapproved, limited self-service capabilities that can be executed on demand by those in nonprivileged roles  e.g., development or QA roles . You can often per‐ suade  auditors  that  this  form  of  self-service  still  meets  a  separation  of  duties requirement  because  the  operations  role  sets  up  the  procedure,  grants  specific access, and receives the logs and notification of its use. Security concerns are also the culprit behind many of today’s repetitive requests that can be labeled as toil. Purely for security reasons, people are currently being forced into queues and are waiting for someone to do something for them. Self- service, done correctly, gives the requesters the ability to take action themselves. You can maintain security postures through fine-grained access control, full log‐ ging, and automated notifications.  Improves incident response  Self-service capabilities are helpful for capturing a team’s best practices as check‐ lists  and  automated  runbooks.  When  responding  to  incidents,  checklists  can improve both individual and team performance. Setting up checklists as automa‐ ted runbooks not only encourages consistency but also allows running the check‐ lists—and watching the output—to be a group activity. Maximizes the value of standard services and infrastructure  It is considered a best practice for SRE teams to use some of their engineering time to build and maintain standard infrastructure  e.g., platforms and environ‐ ments  and operations services  e.g., deployment systems and observability . The better the self-service capabilities are, the more an organization will be able to leverage these standard components and services.  Operations as a Service If you examine companies that both employ an SRE style of operations and are regar‐ ded as high performers by their peers, you will find that they have often built custom- purpose  tooling  to  enable  self-service  capabilities.  For  example,  look  at  Netflix’s combination  of  Spinnaker,9  Winston,10  and  Bolt.11  These  were  originally  purpose- built tools developed from the ground up for Netflix’s focused, purpose-built organi‐ zation.  Enterprises  will  probably  find  that  they  need  more  generic  self-service  9 https:  www.spinnaker.io 10 Netflix Technology blog, “Introducing Winston — Event driven Diagnostic and Remediation Platform”. 11 Netflix Technology blog, “Introducing Bolt: On Instance Diagnostic and Remediation Platform”  168      Chapter 10: Clearing the Way for SRE in the Enterprise   capabilities to embrace the heterogeneity that comes from decades of acquisition and accumulation. Operations as a Service  OaaS  is a generic and deceptively simple design pattern for creating generic self-service operations capabilities. The basic idea of OaaS is that it is a  platform  for  safely  distributing  the  ability  to  both  define  and  execute  automated procedures, as shown in Figure 10-8.  Figure 10-8. Overview of OaaS design pattern  Critical to the success of this design pattern is the requirement that the platform both is lightweight and works with any popular scripting language or tool. Forcing teams to standardize on one language or automation framework just isn’t realistic given the heterogeneous nature of modern enterprises. In fact, it is not just unrealistic, it can actually  slow  an  organization  down.  Teams  need  to  be  able  to  use  the  automation languages and tools that they want  or inherited  while allowing for other tools to orchestrate procedures across those underlying frameworks and languages. Access control and audibility are also critical to the success of this design pattern. For any solution to thrive in the enterprise, ultimate control needs to remain with people and teams that are deemed to have a higher level of access privilege. OaaS efforts have the best chance of exceeding expectations when paired with moni‐ toring and observability projects. Implementation projects tend to focus heavily on automation. However, when the project progresses, many organizations discover that visibility into operational health, state, and configuration is lacking. Metaphorically speaking, they are distributing the keys to the car without giving people the capability to see where there are going. To avoid this problem, put equal emphasis from the beginning on both “the view” and “the do.” The OaaS design pattern should be compatible with any operations operating model. Whether  you  are  moving  to  cross-functional  teams   Figure  10-9   or  staying  with  Replace Remaining Handoffs with Self-Service      169   to  a   closer  traditional  development  and  operations  organization  divide  Figure 10-10 , developing your organization’s self-service capabilities will pay divi‐ dends.  Figure 10-9. OaaS design pattern with a cross-functional teams organizational model  Figure 10-10. OaaS design pattern with a more traditional development and Ops SRE split organizational model  170      Chapter 10: Clearing the Way for SRE in the Enterprise   Error Budgets, Toil Limits, and Other Tools for Empowering Humans One of the more powerful developments to come out of the SRE movement is the popularization  of  a  set  of  management  concepts  that  formalize  the  expectations around an organization’s operational activity. For those who work in companies that had  an  SRE  model  from  inception,  the  following  ideas  might  be  self-evident.  For those  who  work  in  traditional  enterprise  IT  operations,  these  ideas  often  highlight how much of a departure the SRE model is from traditional operations beliefs and practices. Error Budgets There are often tensions in an organization over how much risk is tolerable for a par‐ ticular service and who is responsible when the Service-Level Objectives  SLOs  are not met. In traditional enterprise divides, the product end of a company is incentiv‐ ized to go faster, and the operations end is incentivized to avoid downtime and other performance  problems.  This  is  the  type  of  mismatch  in  incentives  that  encourages silos to form. How do you keep both interests aligned? How do you keep all roles investing in both speed and reliability? Error budgets are a framework for measuring, and utilizing, allowable risk. Specifi‐ cally,  it  is  the  gap  between  theoretically  perfect  reliability  and  an  acceptable  SLO agreed  upon  by  business  and  technology  stakeholders.  Error  budgets  provide  a framework to negotiate with the business on how much failure is acceptable and still be able to meet the needs of the business. It is not an accident that the metaphor of a budget is used. Budgets are a representa‐ tion of currency that is available to be spent. That is the same with error budgets. Developers and SREs can use that budget in attempts to move the business forward. If a service has a small error budget, developers and SREs must be more conservative to favor stability. If a service has a bigger error budget, developers and SREs can be more  aggressive  and  favor  speed  and  production  experimentation.  As  with  other types of budgets, the negotiation is about how to best spend it—and in some cases, don’t spend it at all  see Figure 10-11 .  Error Budgets, Toil Limits, and Other Tools for Empowering Humans      171   Figure 10-11. Error budget is the difference between perfection and the agreed-upon SLO  What happens if you exceed the error budget? Error budgets are attached to a service, not a particular role. All roles involved with the service must respect the budget. For example,  if  the  error  budget  is  violated  due  to  aggressive  or  problematic  change, those in development roles need to adapt their behavior  including often taking over more operational responsibility  and adapt the service to perform within the error budget allotted. This  is  a  sharp  contrast  to  the  traditional  business-mandated  Service-Level  Agree‐ ments  SLAs  found in enterprise IT operations. If those traditional SLAs were bro‐ ken, operations  in a service provider role  incurred the penalty and development— already on to their next project—usually did not. Also, the concept of a Service-Level Indicator  SLI; quantifiable performance measure , an SLO  performance target , and the error budget  current amount above the SLO  is more nuanced and pragmatic than traditional SLA approaches. You need to be sure that teams coming from tradi‐ tional operations cultures understand the differences. Toil Limits Toil limits are another concept that challenges traditional operations thinking. I have already  covered  why  toil  undermines  the  SRE  function.  Defining  a  limit  on  the amount of toil that is to be undertaken by an individual SRE or team both makes a statement about priorities and protects an individual or team’s capacity to do engi‐ neering work.  172      Chapter 10: Clearing the Way for SRE in the Enterprise   Toil limits are also indicators of a team’s health. If a team’s toil exceeds a predeter‐ mined threshold  e.g., Google’s default limit of 50% of an engineer’s capacity , the organization can swarm to the find out ways to help rectify the situation. Like error budgets, toil limits help SREs reach an agreement on expected behavior, provide clear signals when help is needed, and avoid being overrun by non-value-adding repetitive work. In a traditional IT operations culture, teams rarely have these types of protec‐ tions. The  concept  of  toil  is  largely  absent  from  traditional  enterprise  operations  culture  despite  high  levels  of  what  you  can  now  identify  as  toil .  For  people  working  in modern SRE-inspired organizations, toil feels bad. The urge is to find ways to elimi‐ nate it, and your colleagues support the effort. In traditional enterprise culture, toil is, at best, considered a “nice to fix” item and, at worst, just accepted. When introducing toil limits, you will need to educate your people to socialize the concept of toil and to get them to understand why toil is destructive to both the indi‐ vidual and the organization. Leverage Existing Enthusiasm for DevOps Although  DevOps  was  once  the  exclusive  domain  of  web-scale  startups,  it  has become an accepted ideal in most enterprises. Born in 2009, DevOps is a broad cul‐ tural and professional movement focused on “world-class quality, reliability, stability, and  security  at  ever  lower  cost  and  effort;  and  accelerated  flow  and  reliability throughout the technology value stream.”12 There is quite a bit of overlap between the goals of DevOps and SRE. There is also quite a bit of overlap between the theoretical underpinnings of DevOps and SRE.  Benjamin Treynor Sloss, the Google leader who first coined the term SRE and presi‐ ded  over  the  codification  of  Google’s  SRE  practices,  sees  a  clear  overlap  between DevOps and SRE:  One  could  view  DevOps  as  a  generalization  of  several  core  SRE  principles  to  a wider  range  of  organizations,  management  structures,  and  personnel.  One  could equivalently view SRE as a specific implementation of DevOps with some idiosyncratic extensions.13  Within the enterprise, DevOps has been applied most often to the limited scope that starts  with  software  development  and  moves  through  the  service  delivery  pipeline  from source code check-in to automated deployment . In these enterprises, the pen‐ etration of the DevOps transformation is minimal beyond deployment and the bulk  12 Kim, Gene, Patrick Debois, John Willis, and Jez Humble.  2017 . The DevOps Handbook. Portland, OR: IT  Revolution Press, LLC.  13 Site Reliability Engineering: How Google Runs Production Systems, Introduction.  Error Budgets, Toil Limits, and Other Tools for Empowering Humans      173   of operations practices have remained unchanged. SRE is an opportunity to leverage the momentum started by DevOps and continue the transformation efforts through‐ out to the rest of the post-deployment life cycle. I  recommend  looking  for  DevOps  momentum  in  your  organization  and  aligning your  SRE  transformation  efforts.  There  are  lessons  that  each  can  learn  from  the other.  Working  both  Dev-toward-Ops   DevOps   and  Ops-toward-Dev   SRE   will give your company’s transformation the best chance of success. Unify Backlogs and Protect Capacity The concept of working from a single, well-managed backlog and protecting a team’s capacity  did  not  start  with  the  SRE  movement.  These  are  both  fundamental  Lean concepts  for  improving  the  flow  of  work  and  are  popular  in  both  the  Agile  and DevOps  communities.  These  concepts  are  a  lot  less  prevalent  in  traditional  opera‐ tions cultures but can be extremely useful for managing the work of—and protecting —teams transitioning to SRE. SRE work, by definition, is a mix of planned and unplanned work. This mix of work types is among the most difficult to manage. Planned and unplanned work are differ‐ ent modes of working and do not mix well. Having different backlogs for each type of work makes things even worse. It is like serving multiple masters at the same time. It is easy to be pulled in too many directions or to be completely run over by the com‐ peting demands. Engineers  working  in  traditional  IT  operations  organizations  often  have  multiple backlogs with different sources of work managed in different ways. There is one sys‐ tem through which formal project work comes to them. Then there can be another way the team maintains a backlog and manages engineering work that doesn’t bubble up to a formal project. Then, of course, there is a different way that interrupt-driven requests, like incidents, are handled. Moving each team to a single, unified  planned + unplanned  backlog pays dividends. Rather  than  the  constant  tension  between  valuable  planned  work  and  necessary unplanned work, unified backlogs make prioritization and trade-offs clear. Unified backlogs also make it easier to reserve capacity for unplanned work  really another type of “budget” . Kanban is a management methodology that features ideas like unified backlogs and protected capacity. Kanban has been shown to significantly improve the flow of work in organizations with mixed types of work. The writing and presentations of longtime Kanban expert and author Dominica DeGrandis are a good place to start, as she is one of the pioneers in the application of Kanban to operations organizations.  174      Chapter 10: Clearing the Way for SRE in the Enterprise   Psychological Safety and Human Factors It might now seem like common sense that optimizing the performance of your most important assets, your people, is critical to the success of any technology business. However, this was not always the case. If you have been in the IT operations industry long enough, you have seen traditional operations cultures that treated its people like interchangeable cogs. If a cog wore out, it must not have been tough enough! If the machine broke, there must be a cog to blame! Cogs are just parts, so keep looking elsewhere for the cheapest supplier of cogs! If you want to build a highly effective, fast-moving organization, you need a culture that empowers your people to engage in reasonable risk-taking, bring up bad news to superiors, engage in creativity, and support their coworkers. Psychological safety and human factors14 are related fields of study that are much broader than IT but have a lot to offer to our industry. From examinations of airplane disasters to medical trag‐ edies,  there  is  a  reusable  body  of  knowledge  on  how  to  maximize  human  perfor‐ mance in stressful, complex situations. Join the Movement We  are  working  during  a  unique  moment  in  IT  operations  history.  Although  we often get new technologies and tools, rarely do we get an opportunity to reshape the structure, behaviors, and culture of operations. This is an opportunity to improve the work lives of operations professionals around the globe and improve outcomes for their employers. SRE  is  a  continuously  evolving  practitioner-led  movement.  The  best  part  of  a practitioner-led movement is that you can be a part of it. Whether it is online or in person at conferences or meetups, join in. As with most of IT, the early adopters and promoters tend to not be from enterprises. Don’t let this dissuade you. First, the lessons learned and the principles debated are more  generally  applicable  than  you  would  think.  Second,  enterprises  might  not  be quick to adopt new practices, but as community acceptance grows, enterprise adop‐ tion will certainly follow. Getting involved early with SRE is a way to both prepare your skills and make your company more competitive. It doesn’t matter if you are learning from the experiences of other practitioners or validating your learnings through sharing your experiences, your participation will return more value than the effort you put in. Now let’s roll up our sleeves and get to work.  14 https:  www.youtube.com watch?v=CFMJ3V4VakA and http:  stella.report  Join the Movement      175   Damon Edwards is a cofounder of Rundeck Inc., the makers of Rundeck, the popular open  source  operations  management  platform.  Damon  was  previously  a  managing partner  at  DTO  Solutions,  a  DevOps  and  IT  operations  improvement  consultancy focused on large enterprises. Damon is a frequent conference speaker, writer, and pod‐ cast host.  176      Chapter 10: Clearing the Way for SRE in the Enterprise   CHAPTER 11 SRE Patterns Loved by DevOps People Everywhere  Gene Kim, IT Revolution  This  is  an  edited  excerpt  from  The  DevOps  Handbook  by  Gene Kim, Jez Humble, John Willis, and Patrick Debois  O’Reilly, 2016 .  When David Blank-Edelman asked me to contribute a chapter on the SRE body of knowledge and the impact it has had on the DevOps community, I gave a very enthu‐ siastic “Yes!” Although some might argue that SRE and DevOps are mutually exclusive, I argue the opposite. In my opinion, it is difficult to overstate the impact that SRE has had on framing how the operations community can best contribute to organizational goals and improving the productivity of developers. As Ben Treynor Sloss, VP of SRE at Google, famously said in his 2014 SREcon presentation: “I define SRE as what hap‐ pens when software engineers create an operations group.” In that famous presentation, Treynor Sloss introduces the breathtaking concept of a truly  self-balancing  system,  where  an  organization  first  decides  on  an  acceptable error budget, which then guides the prioritization of nonfunctional requirements and gates the decision to deploy and release. During the research and writing of The DevOps Handbook  along with my coauthors Jez Humble, John Willis, and Patrick Debois , I couldn’t help but notice how many of the DevOps patterns that we love and now can take for granted were pioneered at Google.  177   Here are three of my favorite patterns, excerpted from The DevOps Handbook, that can be traced to the SRE body of knowledge. Almost any organization can integrate them into its daily work. Pattern 1: Birth of Automated Testing at Google Automated testing addresses a truly significant and unsettling problem. Gary Gruver observes that “without automated testing, the more code we write, the more time and money is required to test our code—in most cases, this is a totally unscalable business model for any technology organization.” Although Google now undoubtedly exemplifies a culture that values automated test‐ ing at scale, this wasn’t always the case. In 2005, when Mike Bland joined the organi‐ zation, deploying to Google.com was often extremely problematic, especially for the Google Web Server  GWS  team. As Bland explains:  The GWS team had gotten into a position in the mid-2000s where it was extremely difficult to make changes to the web server, a C++ application that handled all requests to Google’s home page and many other Google web pages. As important and promi‐ nent as Google.com was, being on the GWS team was not a glamorous assignment—it was often the dumping ground for all the different teams who were creating various search functionality, all of whom were developing code independently of each other. They had problems such as builds and tests taking too long, code being put into pro‐ duction  without  being  tested,  and  teams  checking  in  large,  infrequent  changes  that conflicted with those from other teams.  The  consequences  of  this  were  large—search  results  could  have  errors  or  become unacceptably slow, affecting thousands of search queries on google.com. The potential result was loss not only of revenue, but customer trust. Bland  describes  how  it  affected  developers  deploying  changes:  “Fear  became  the mind-killer.  Fear  stopped  new  team  members  from  changing  things  because  they didn’t understand the system. But fear also stopped experienced people from chang‐ ing things because they understood it all too well.”1 Bland was part of the group that was determined to solve this problem.  1 Bland described that at Google, one of the consequences of having so many talented developers was that it  created “imposter syndrome,” a term coined by psychologists to informally describe people who are unable to internalize their accomplishments. Wikipedia states that “despite external evidence of their competence, those exhibiting the syndrome remain convinced that they are frauds and do not deserve the success they have achieved. Proof of success is dismissed as luck, timing, or a result of deceiving others into thinking they are more intelligent and competent than they believe themselves to be.”  178      Chapter 11: SRE Patterns Loved by DevOps People Everywhere   GWS team lead Bharat Mediratta believed automated testing would help. As Bland describes:  They created a hard line: no changes would be accepted into GWS without accompa‐ nying automated tests. They set up a continuous build and religiously kept it passing. They set up test coverage monitoring and ensured that their level of test coverage went up over time. They wrote up policy and testing guides and insisted that contributors both inside and outside the team follow them.  The results were startling. As Bland notes:  GWS quickly became one of the most productive teams in the company, integrating large numbers of changes from different teams every week while maintaining a rapid release schedule. New team members were able to make productive contributions to this complex system quickly, thanks to good test coverage and code health. Ultimately, their radical policy enabled the Google.com home page to quickly expand its capabili‐ ties and thrive in an amazingly fast-moving and competitive technology landscape.  But GWS was still a relatively small team in a large and growing company. The team wanted  to  expand  these  practices  across  the  entire  organization.  Thus,  the  Testing Grouplet was born, an informal group of engineers who wanted to elevate automated testing practices across the entire organization. Over the next five years, they helped replicate this culture of automated testing across all of Google.2 Now when any Google developer commits code, it is automatically run against a suite of hundreds of thousands of automated tests. If the code passes, it is automatically merged into trunk, ready to be deployed into production. Many Google properties build hourly or daily, then pick which builds to release; others adopt a continuous “Push on Green” delivery philosophy. The stakes are higher than ever—a single code deployment error at Google can take down every property, all at the same time  such as a global infrastructure change or when a defect is introduced into a core library that every property depends upon . Eran  Messeri,  an  engineer  in  the  Google  Developer  Infrastructure  group,  notes, “Large failures happen occasionally. You’ll get a ton of instant messages and engi‐ neers knocking on your door. [When the deployment pipeline is broken,] we need to fix it right away, because developers can no longer commit code. Consequently, we want to make it very easy to roll back.” What  enables  this  system  to  work  at  Google  is  engineering  professionalism  and  a high-trust culture that assumes everyone wants to do a good job as well as the ability to detect and correct issues quickly. Messeri explains:  2 They created training programs, pushed the famous Testing on the Toilet newsletter  which they posted in the bathrooms , developed the Test Certified roadmap and certification program, and led multiple “fix-it” days  i.e., improvement blitzes , which helped teams improve their automated testing processes so that they could replicate the amazing outcomes that the GWS team was able to achieve.  Pattern 1: Birth of Automated Testing at Google      179   There are no hard policies at Google, such as, “If you break production for more than 10  projects,  you  have  an  SLA  to  fix  the  issue  within  10  minutes.”  Instead,  there  is mutual respect between teams and an implicit agreement that everyone does whatever it takes to keep the deployment pipeline running. We all know that one day, I’ll break your project by accident; the next day, you may break mine.  What Mike Bland and the Testing Grouplet team achieved has made Google one of the most productive technology organizations in the world. By 2013, automated test‐ ing  and  continuous  integration  at  Google  enabled  more  than  4,000  small  teams  to work together and stay productive, all simultaneously developing, integrating, test‐ ing, and deploying their code into production. All their code is in a single, shared repository, made up of billions of files, all being continuously built and integrated, with 50% of their code being changed each month. Some other impressive statistics on their performance include the following:    40,000 code commits day   50,000 builds day  on weekdays, this can exceed 90,000    120,000 automated test suites   75 million test cases run daily   100-plus engineers working on the test engineering, continuous integration, and release engineering tooling to increase developer productivity  making up 0.5% of the R&D workforce   Pattern 2: Launch and Handoff Readiness Review at Google Even when developers are writing and running their code in production-like envi‐ ronments in their daily work, operations can still experience disastrous production releases  because  it  is  the  first  time  we  actually  see  how  our  code  behaves  during  a release and under true production conditions. This result occurs because operational learnings often happen too late in the software life cycle. When this is left unaddressed, the result is often production software that is difficult to  operate.  As  an  anonymous  ops  engineer  once  said,  “In  our  group,  most  system administrators lasted only six months. Things were always breaking in production, the hours were insane, and application deployments were painful beyond belief—the worst part was pairing the application server clusters, which would take us six hours. During each moment, we all felt like the developers personally hated us.” This can be an outcome of not having enough ops engineers to support all the prod‐ uct teams and the services we already have in production, which can happen in both functionally and market-oriented teams.  180      Chapter 11: SRE Patterns Loved by DevOps People Everywhere   One potential countermeasure is to do what Google does, which is have development groups self-manage their services in production before they become eligible for a cen‐ tralized ops group to manage. By having developers be responsible for deployment and production support, we are far more likely to have a smooth transition to opera‐ tions. To prevent the possibility of problematic, self-managed services going into produc‐ tion and creating organizational risk, we can define launch requirements that must be met in order for services to interact with real customers and be exposed to real pro‐ duction traffic. Furthermore, to help the product teams, ops engineers should act as consultants to help them make their services production-ready. By creating launch guidance, we help ensure that every product team benefits from the cumulative and collective experience of the entire organization, especially opera‐ tions. Launch guidance and requirements will likely include the following: Defect counts and severity  Does the application actually perform as designed?  Type frequency of pager alerts  Is the application generating an unsupportable number of alerts in production?  Is the coverage of monitoring sufficient to restore service when things go wrong?  Is  the  service  loosely  coupled  enough  to  support  a  high  rate  of  changes  and deployments in production?  Is there a predictable, deterministic, and sufficiently automated process to deploy code into production?  Monitoring coverage  System architecture  Deployment process  Production hygiene  Is there evidence of enough good production habits that would allow production support to be managed by anyone else?  Superficially,  these  requirements  might  appear  similar  to  traditional  production checklists we have used in the past. However, the key differences are that we require effective monitoring to be in place, deployments to be reliable and deterministic, and an architecture that supports fast and frequent deployments. If any deficiencies are found during the review, the assigned ops engineer should help the feature team resolve the issues or even help re-engineer the service if necessary so that it can be easily deployed and managed in production. At this time, we might also want to learn whether this service is subject to any regula‐ tory compliance objectives or if it is likely to be in the future:  Pattern 2: Launch and Handoff Readiness Review at Google      181     Does the service generate a significant amount of revenue?  For example, if it is more than 5% of total revenue of a publicly held US corporation, it is a “signifi‐ cant  account”  and  in-scope  for  compliance  with  Section  404  of  the  Sarbanes- Oxley Act of 2002 [SOX].     Does the service have high user traffic or have high outage impairment costs  i.e.,  do operational issues risk creating availability or reputational risk ?    Does the service store payment cardholder information such as credit card num‐ bers, or personally identifiable information such as Social Security numbers or patient care records? Are there other security issues that could create regulatory, contractual obligation, privacy, or reputation risk?    Does  the  service  have  any  other  regulatory  or  contractual  compliance  require‐ ments associated with it, such as US export regulations, PCI-DSS, HIPAA, and so forth?  This information helps to ensure that we effectively manage not only the technical risks associated with this service, but also any potential security and compliance risks. It  also  provides  essential  input  into  the  design  of  the  production  control  environ‐ ment. See examples of the launch and handoff readiness reviews in Figures 11-1 and 11-2.  Figure 11-1. The launch readiness review at Google  source: “SRE@Google: Thousands of DevOps Since 2004”, YouTube video, 45:57, posted by USENIX, January 12, 2012   182      Chapter 11: SRE Patterns Loved by DevOps People Everywhere   Figure 11-2. The handoffs readiness review at Google  source: “SRE@Google: Thou‐ sands of DevOps Since 2004”, YouTube video, 45:57, posted by USENIX, January 12, 2012   As  Tom  Limoncelli,  coauthor  of  The  Practice  of  Cloud  System  Administration  Addison-Wesley, 2002  and a former site reliability engineer at Google, mentions in one of his talks, “In the best case, product teams have been using the LRR checklist as a  guideline,  working  on  fulfilling  it  in  parallel  with  developing  their  service,  and reaching out to SREs to get help when they need it.” Furthermore, Limoncelli once told me in 2016:  The teams that have the fastest HRR production approval are the ones that worked with SREs earliest, from the early design stages up until launch. And the great thing is, it’s always easy to get an SRE to volunteer to help with your project. Every SRE sees value in giving advice to project teams early and will likely volunteer a few hours or days to do just that.  The practice of SREs helping product teams early is an important cultural norm that is continually reinforced at Google. Limoncelli explained, “Helping product teams is a long-term investment that will pay off many months later when it comes time to launch. It is a form of ‘good citizenship’ and ‘community service’ that is valued; it is routinely considered when evaluating engineers for SRE promotions.” Pattern 3: Create a Shared Source Code Repository A firm-wide, shared source-code repository is one of the most powerful mechanisms used  to  integrate  local  discoveries  across  the  entire  organization.  When  engineers update anything in the source code repository  e.g., a shared library , it rapidly and automatically propagates to every other service that uses that library, and it is integra‐ ted through each team’s deployment pipeline.  Pattern 3: Create a Shared Source Code Repository      183   Google is one of the largest examples of using an organization-wide shared source- code  repository.  By  2015,  Google  had  a  single  shared  source-code  repository  with more than 1 billion files and more than 2 billion lines of code. This repository is used by every one of its 25,000 engineers and spans every Google property, including Goo‐ gle Search, Google Maps, Google Docs, Google+, Google Calendar, Gmail, and You‐ Tube.3 One of the valuable results of this is that engineers can leverage the diverse expertise of everyone in the organization. Rachel Potvin, a Google engineering manager over‐ seeing the Developer Infrastructure group, told Wired that every Google engineer can access “a wealth of libraries” because “almost everything has already been done.” Furthermore, as Eran Messeri explains, one of the advantages of using a single reposi‐ tory is that it allows users to easily access all of the code in its most up-to-date form, without the need for coordination. We put into our shared source-code repository not only source code, but also other artifacts that encode knowledge and learning, including the following:    Configuration  standards  for  our  libraries,  infrastructure,  and  environments   Chef recipes, Puppet manifests, etc.     Deployment tools   Testing standards and tools, including security   Deployment pipeline tools   Monitoring and analysis tools   Tutorials and standards  Encoding knowledge and sharing it through this repository is one of the most power‐ ful  mechanisms  we  have  for  propagating  knowledge.  As  Randy  Shoup  told  me  in 2014:  The  most  powerful  mechanism  for  preventing  failures  at  Google  is  the  single  code repository. Whenever someone checks in anything into the repo, it results in a new build,  which  always  uses  the  latest  version  of  everything.  Everything  is  built  from source rather than dynamically linked at runtime—there is always a single version of a library that is the current one in use, which is what gets statically linked during the build process.  In his book, Tom Limoncelli states that the value of having a single repository for an entire organization is so powerful, it is difficult to even explain.  3 The Chrome and Android projects reside in a separate source-code repository, and certain algorithms that  are kept secret, such as PageRank, are available only to certain teams.  184      Chapter 11: SRE Patterns Loved by DevOps People Everywhere   You can write a tool exactly once and have it be usable for all projects. You have 100% accurate knowledge of who depends on a library; therefore, you can refactor it and be 100% sure of who will be affected and who needs to test for breakage. I could probably list  a  hundred  more  examples.  I  can’t  express  in  words  how  much  of  a  competitive advantage this is for Google.  At Google, every library  e.g., libc, OpenSSL, as well as internally developed libraries such as Java threading libraries  has an owner who is responsible for ensuring that the library not only compiles, but also successfully passes the tests for all projects that depend upon it, much like a real-world librarian. That owner is also responsible for migrating each project from one version to the next. Consider the real-life example of an organization that runs 81 different versions of the Java Struts framework library in production—all but one of those versions have critical  security  vulnerabilities,  and  maintaining  all  of  those  versions,  each  with  its own quirks and idiosyncrasies, creates significant operational burden and stress. Fur‐ thermore, all of this variance makes upgrading versions risky and unsafe, which in turn discourages developers from upgrading. And the cycle continues. The single source repository solves much of this problem, as well as having automa‐ ted tests that allow teams to migrate to new versions safely and confidently. If we are not able to build everything off a single source tree, we must find another means to maintain known good versions of the libraries and their dependencies. For instance, we might have an organization-wide repository such as Nexus, Artifactory, or a Debian or RPM repository, which we must then update where there are known vulnerabilities, both in these repositories and in production systems. Conclusion I  hope  these  patterns  show  a  glimpse  of  the  obvious  bridges  between  SRE  and DevOps—they have far more in common than most would imagine and are certainly grounded in similar principles and goals. Further Reading and Source Material   DevOps Enterprise Summit 2015 talk by Mike Bland: “Pain Is Over, If You Want  It”, Slideshare.net, posted by Gene Kim, November 18, 2015.    GOTO Conference talk by Eran Messeri, “What Goes Wrong When Thousands of Engineers Share the Same Continuous Build?” Aarhus, Denmark, October 2, 2013.    Tom  Limoncelli,  “SRE@Google:  Thousands  Of  DevOps  Since  2004”,  YouTube video of USENIX Association Talk, NYC, posted by USENIX, 45:57, posted Jan‐ uary 12, 2012.  Conclusion      185     Ben Treynor, “Keys to SRE”  presentation, Usenix SREcon14, Santa Clara, CA,  May 30, 2014 .  Wired, September 16, 2015.    Cade  Metz,  “Google  Is  2  Billion  Lines  of  Code—and  It’s  All  in  One  Place”,    Eran  Messeri,  “What  Goes  Wrong  When  Thousands  of  Engineers  Share  the  Same Continuous Build?”  2013 .    Tom  Limoncelli,  “Yes,  you  can  really  work  from  HEAD”,  EverythingSysAd‐    Tom Limoncelli, “Python is better than Perl6”, EverythingSysAdmin.com, Janu‐  min.com, March 15, 2014.  ary 10, 2011.    “Which  programming  languages  does  Google  use  internally?,”  Quora.com forum, accessed May 29, 2016; “When will Google permit languages other than Python, C++, Java and Go to be used for internal projects?”, Quora.com forum, accessed May 29, 2016.    Tom Limoncelli, Strata Chalup, and Christina Hogan, The Practice of Cloud Sys‐  tem Administration  Addison-Wesley: 2002 .  Gene Kim is a multiple award-winning CTO, researcher, and coauthor of The Phoenix Project, The DevOps Handbook, and Accelerate  IT Revolution . He is the organizer of the DevOps Enterprise Summit conferences.  186      Chapter 11: SRE Patterns Loved by DevOps People Everywhere   CHAPTER 12 DevOps and SRE: Voices from the Community  As told to David N. Blank-Edelman  Background From almost the very beginning of my time interacting with the SRE community, I’ve been curious about the relationship between DevOps and SRE. I’ve had the pleasure of talking about this with many smart people and heard many smart things. As far as I can tell, it’s not a settled question. Each person I have talked to added something to my understanding. When it came time to find a contributor on this topic, it seemed like the best thing I could do would be to invite as many voices into the discussion as possible. Welcome to an experiment—an entirely crowdsourced chapter. Method At the end of February 2018, I put up a website with a page that asked the following:  In two paragraphs or less, what do you think is the relationship between DevOps and SRE? How are they similar? How are they different? Can both be implemented at every organization? Can the two exist in the same org at the same time? And so on…  I put a call out via Twitter and LinkedIn to my professional social network for contri‐ butions  and to ask them to pass a pointer to this web page to their network, as well, which many kindly did . In the end, Google Analytics reports 1,165 people from 34 separate countries came to the contribution page.  187   Results It was thrilling to see the sorts of thoughtful answers I received to these questions. I present to you now, only slightly copyedited, a sample of the responses1 I received in no particular order and with no particular organization  so it accurately reflects the “messy” nature of the discussion as you would find it on the internet . For each reply, I’ve listed the person’s name, title, and affiliation if they chose to share that informa‐ tion. It’s hardly a representative survey on the question, but I think it does an excellent job of  showing  both  the  heterogeneity  and  homogeneity  of  opinions  on  the  subject. Hopefully just hearing this discussion will provide an opportunity for you to begin to form your own thoughts on the matter. I should also say that I am intentionally pre‐ senting  these  responses  without  my  own  comments  or  opinions.  Rather  than attempting  to  wrap  this  non-Newtonian  fluid  of  a  subject  up  in  a  neat  bow   from which it would only escape  or impose my own impressions of the material on your thinking, I’m more interested in hearing your conclusions. Are you we any closer to an answer on these questions? There’s obviously much more that can be said on this topic. Feel free to get in touch if you’d like to share your ideas, too, I’d love to hear them. Replies Site reliability is operational reliability, scalability, and efficiency. This includes busi‐ ness continuity  disaster recovery, high availability . The operational site becomes a product in and of itself and may include their own CI CD for internal tooling. The automation tends toward custom tools; for example, Python with Boto library, Ruby with  AWS  [Amazon  Web  Services]  SDK,  and  Go  language  rather  than  the  use  of high-level  tools  like  Terraform  and  Ansible,  as  they  are  considered  inefficient. Though this is not absolute, just a trend. SRE is programming the operations to cre‐ ate reliable and efficient infrastructure. DevOps focuses on breaking down cultural silos and increasing efficiency or velocity of deployment  CI CD  pipeline, from development to delivery; this includes build‐ ing and pre post artifact testing  testing before and after the artifact is built , thus CT, or continual testing. It takes over where Agile left off and embraces aspects of Lean. DevOps  would  work  with  optimization  and  integration  upstream   build,  test   and downstream toward deployment and delivery. There is overlap, where deployment  delivery to an operational site is a shared domain with SRE. There is also opposing  1 Space constraints prohibit me from including more, but I’m grateful for each and every response I received.  Thank you!  188      Chapter 12: DevOps and SRE: Voices from the Community   ideals where DevOps is integrated across the pipeline, SRE is only on the operational infrastructure, and would be considered a silo under strict DevOps philosophy. —Joaquin Menchaca, Senior DevOps engineer, NinjaPants Consulting  While many consider DevOps to be a single framework, it is in fact an umbrella for a pipeline of practices that span the organization’s value stream from concept to value creation.  Most  DevOps  practices  focus  on  the  stages  from  development  through deployment  as  Continuous  Integration,  Continuous  Delivery  and  Continuous Deployment.  To  my  mind,  SRE  is  a  natural  extension  of  DevOps  as  Continuous Operations. Under that same umbrella, SRE plays a key role in aligning and evolving ITIL ITSM processes  to  DevOps.  Monolithic  processes  such  as  incident,  problem,  knowledge, change  and  service-level  management  become  less  of  a  constraint  and  more  of  an enabler when managed and executed at the SRE level in order to accommodate the faster  and  more  frequent  flow  of  changes.  Why?  Because  SRE  is  not  only  a  set  of practices:  there  is  a  defined  SRE  role  with  a  set  of  responsibilities.  That  is  another fundamental  difference  between  SRE  and  DevOps;  there  is  not  a  clearly  defined “DevOps Engineer” role. —Jayne Groll, CEO, DevOps Institute  DevOps is underpinned by three principles: systems thinking  looking at the whole system  not  just  your  slice ,  amplifying  feedback  loops,  and  a  culture  of  continual experimentation and learning. SRE adheres to these same principles, as evidenced by SLOs, error budgets, and its involvement in all aspects of a system. In some ways, SRE is one way to go about doing DevOps, much like scrum is one way to implement Agile. SRE  differentiates  itself  from  DevOps  by  its  focus  on  engineering  solutions.  This focus on code enables scaling at a level other solutions cannot approach, making SRE essentially  a  highly  scalable  DevOps.  Put  another  way,  SRE  is  an  engineering- solutions-focused implementation of DevOps. —Tanner Lund, Microsoft  ◆ ◆ ◆  ◆ ◆ ◆  ◆ ◆ ◆  Replies      189   I believe DevOps came up from the industry as an answer to the relentless increase in operational complexity driven by two huge trends in technology: the migration to the cloud,  and  immutable  infrastructure  plus  infrastructure-as-code.  With  every  com‐ pany  out  there  scrambling  desperately  to  move  to  cloud  providers  and  drastically change  their  cost  structure  and  therefore  valuation   cloud  providers  are  variable operational costs, not fixed , and all companies trying desperately to catch up with the  latest  infrastructure-as-code  breakthroughs   Docker  and  Kubernetes ,  opera‐ tional complexity is increasing exponentially. Most companies lack the skill set and engineering culture treats that enable for a healthy balance of forces to release inno‐ vation and operate it reliably. I  believe  SRE  is  just  Google’s,  Microsoft’s,  LinkedIn’s,  Facebook’s  and  other  huge companies’ efforts to tackle the same issue. These companies have been dealing with this  level  of  operational  complexity  of  this  scale  for  well  over  a  decade  now,  with many generations of engineering teams adopting it, executing, and moving on. Add to that a higher expectation of reliability and performance on their products, and you have a recipe for top-tier practices, well defined and polished for both learning and implementing at a team level. I believe SRE is basically DevOps, but it’s ahead by a whole decade of trial and error, and it now, thanks to a few books and whitepapers, has been made available for the rest of the industry to learn and adopt. —Santiago Suarez Ordoñez, CTO, Blameless  ◆ ◆ ◆  I think fundamentally these roles are very similar, but the difference is more in their focus in relation to the needs of the business. For a smaller business, or newer teams in larger businesses, things may be a bit more manual as there hasn’t been someone there to automate their common tasks. This may mean there is more of an immediate need for an operations engineer. I see a site reliability engineer as the natural progres‐ sion after teaching an operations engineer how to apply software engineering princi‐ ples to their work  DevOps , and they’ve automated away a sizable amount of their operational work. As part of this exercise, they’ve scaffolded and operated services that need to be available and reliable at scale. This gives them the necessary experi‐ ence  to  step  in  to  other  projects  and  help  implement  improvements  as  well  as  to intuit software engineering patterns that will result in better systems. I think these roles can be implemented at most organizations, but they will probably happen at slightly different times. The infrastructure being used to support the busi‐ ness should be built out, and then dedicated people can focus on building more relia‐ ble  systems  on  top  of  these  in  a  consistent  way.  This  would  really  mean  probably starting with operations engineers and focusing their work in a way that has them eventually acting like site reliability engineers. Starting to adopt an SRE program is  190      Chapter 12: DevOps and SRE: Voices from the Community   not a sign that those doing Ops aren’t needed, or are deprecated, but in fact they are probably needed more to help continue to iterate on the supporting infrastructure and systems to ensure it continues to meet the demands of the business. While the core focus of these two roles is slightly different, it’s important to remember that they both can complement each other when used effectively within a single organization. —Tim Heckman, senior site reliability engineer, Netflix, Inc.  DevOps and SRE are engineering practices with a heavy focus on collaboration and automation in a culture where learning from failure is championed. Whereas DevOps focuses on frequent delivery of customer functionality in an Agile way, SREs concen‐ trate on releasing products as reliably as possible by supporting DevOps teams with their expertise. Considering they share their fundamental principles, SRE and DevOps can coexist pretty easily as long as the organization is clear on the responsibility of build and run. Within ING, BizDevOps teams are end to end responsible for their service s  and its incident  response:  SREs  deliver  monitoring  and  alerting  solutions,  work  on  traffic modeling, ChatOps and introducing a blameless culture where we embrace failure to our  organization  via  postmortems  and  chaos  engineering.  The  ratio  of  SRE  versus DevOps is 7 to 1,700 engineers for the retail bank of ING in the Netherlands, and I think our SRE team is a bit on the smaller side compared to other organizations. Hir‐ ing SREs can be more difficult than hiring DevOps engineers due to the demand of automation capabilities, full-stack engineering skills, reliability mindset and soft skills required for consulting and educating others about SRE related topics. —Janna  Brummel  and  Robin  van  Zijll,  IT  chapter  lead  SRE  and  SRE product owner, ING, respectively  ◆ ◆ ◆  ◆ ◆ ◆  From  a  purist  view,  in  short  DevOps  is  practice  where  the  objective  is  to  increase feedback cycles by reducing handoffs and increasing collaboration, whereas SRE is more  aligned  to  actual  role  titles  and  is  more  engineering  focused,  driving  change specifically through engineering outcomes. Where they are similar is that they both require a mindset, behavior and culture change. I do not agree with the term DevOps engineer, but in my mind that is what I view an SRE to be. DevOps can and should be implemented in every organization adopting Agile or an iterative way of working so as to close the feedback loop cycle to product owners or those owning the backlog of features to ensure service management waste is known  Replies      191   and addressed. SRE’s are compatible with organizations that have or seek to foster an engineering culture and eliminate waste and find efficiencies with engineering out‐ comes. Both can coexist at the same time in the same organization when the organi‐ zation is big enough or old enough. Organizations with what many people term as “legacy systems” can easily allow for DevOps ways of working while they form up tar‐ geted SRE teams for more green fields or evergreen systems. As the wise old proverb says, how do you eat an elephant? One bite at a time. —Michael Ewald, head of operations  Though my own title violates my own advice  and advice noted elsewhere , the term “DevOps” in a job title is inherently impractical. DevOps is more a philosophy, meth‐ odology, or practice one follows, much the same as Agile. Though one having a job title  such  as  “Agile  Developer”  seems  absurd  in  today’s  world,  it’s  likely  “DevOps Engineer” may  and arguably should  one day follow suit. An SRE tends to generalize the functions of the job. Be it they follow DevOps princi‐ ples, Agile principles, or the next hot trend, their function remains the same: ensure the reliability of the “site” in all of its components, down to the underlying infrastruc‐ ture, using methodology adopted and recognized as a fit for accomplishing that task. —Keith McDuffee, senior manager, infrastructure & DevOps, Cardinal Health  ◆ ◆ ◆  ◆ ◆ ◆  DevOps and SRE are related, and we are all developers again. In my first job at IBM in the early 80’s, three kinds of people helped us ship products  “program products” back  then :  product  programmers,  systems  programmers,  and  electricians.  People who  worked  on  program  products  were  developers,  systems  programmers  were developers. We’ve just gone back to a good model; we’re all developers. Some devel‐ opers  are  responsible  for  keeping  their  software  up  and  running  as  a  platform  for other developers, those developers are responsible for keeping their software up and running for other types of customers. In the modern world, both DevOps and SRE are oriented toward building and operating software, which is the concern of anyone who builds and operates a software product. As to the differences between DevOps and SRE, and whether they can coexist, every organization will likely have a different approach, just as Agile can be practiced dif‐ ferently by different teams. But, whether the customers of your software product are other software developers, or people shopping for a new pair of shoes, you need to build  and  operate  products  holistically,  and  avoid  drawing  boundaries  around  192      Chapter 12: DevOps and SRE: Voices from the Community   DevOps SRE developers and other software developers. In some organizations, sepa‐ rating “DevOps” and “SRE” may be more driven by organizational or political struc‐ ture rather than by a meaningful difference between the two. We are all developers, again. —Paula Paul, technology principal, ThoughtWorks  Site  Reliability  Engineering:  we  don’t  know  what  DevOps  is,  but  we  know  we’re something slightly different. —Mike Doherty, successful reliability escapee  ◆ ◆ ◆  ◆ ◆ ◆  If you look at job advertisements the answer is quite clear: the industry has decided that DevOps engineers focus on the SDLC pipeline with occasional responsibilities for production operations. Job advertisements for SREs focus on production opera‐ tions with occasional responsibilities for the SDLC pipeline. The two are the same thing: The difference is in emphasis  Figure 12-1 .  Figure 12-1. The Limoncelli model of SRE, DevOps, and Agile strategies  Google SREs can focus on the later parts of the pipeline because within Google the earlier phases are either solved problems or delegated to other teams. Over time SRE has been able to purify their focus: excellence in operations. They dive into the earlier parts of the pipeline as a means to an end. If a production problem is rooted in insuf‐ ficient or sloppy CI CD discipline, SREs will parachute into the developer area and make  improvements  until  production  is  no  longer  affected.  DevOps,  historically,  Replies      193   have emphasized the earlier parts of the pipeline because DevOps are stuck solving the  entire  problem  and  experience  has  shown  that  transforming  a  project  to  use CI CD is a prerequisite to everything else. This is not to say that DevOps does not care about production operations, but if you look at DevOps job advertisements and stories of “DevOps Transformations” they all tend to focus on moving a company towards CI CD. Once CI CD enables rapid releases, the focus changes to production operations and likewise job titles often then change to SRE. —Thomas A. Limoncelli, SRE manager, Stack Overflow, Inc., Google SRE Alum  They are similar in that both have a heavy focus on automation around developer and operations workflows. DevOps focuses on upgrading old workflows and cultures with  efficient  tools  and  strategies  that  scale.  SRE  focuses  on  preventing  customer downtime by autohealing on events, knowing when there’s something wrong as soon as possible and load testing to plan for future capacity. DevOps is for newer teams to Agile that need to improve tooling and culture. SRE is for established Agile teams that are looking to improve uptime, monitoring, sanity and peace of mind. To use some actual examples, moving to the cloud should be a DevOps  initiative,  while  setting  up  a  failover  disaster  recovery  site  would  be  SRE. Both can be implemented together, as there will be people and parts of your company that are in different stages. —Jefferson Hale, site reliability engineer, Seismic Games  An  SRE  typically  doesn’t  manage  infrastructure.  They  might  manage  debugging dashboards or help to build them. DevOps seems more concerned with automating toil away from all humans. —Adam Shannon, software engineer, Jack Henry and Associates  ◆ ◆ ◆  ◆ ◆ ◆  ◆ ◆ ◆  DevOps  is,  at  its  core,  an  amalgam  of  various  IT  practices  from  Lean,  Agile,  etc. Instilling a DevOps culture in an organization is a lot like embracing open source in an organization. It’s a new way of doing the things you’ve done before. But, like open source, you can do them faster and cheaper. However, where open source is about the  software,  DevOps  is  about  the  people,  processes,  and  to  a  much  lesser  extent, tooling.  194      Chapter 12: DevOps and SRE: Voices from the Community   SRE, on the other hand, takes a very analytical approach to service and software in production. Utilizing metrics, thresholds, SLAs, and SLOs a team can point to con‐ crete facts about a system. There is no reason DevOps and SRE can’t work together. As a matter of fact, they make a lot of sense when paired together. Taking what an SRE team learns about production use of code and feeding it back into the delivery pipeline is literally “The Third Way” in DevOps. —Chris Short, senior DevOps advocate, SJ Technologies  To me, SRE and DevOps are similar but not the same. I view SRE as being the next evolution of a more traditional operations  system network  environment that brings what was missing most from such an environment, the view of the software engineer in the operations. It is still highly skilled and focused teams managing the infrastruc‐ ture, and the SRE world still seems to be a silo, but one that has taken system admin‐ istration  to  the  next  level  and  continues  to  evolve  and  grow  with  modern infrastructure, platforms, tools, and concepts. Compared to SRE, I think that DevOps is most valuable when it is used as a means to facilitate better relationships between all parts of the business. DevOps introduces an operations mindset to software devel‐ opers  and  a  software  mindset  and  understanding  to  the  operations  team.  DevOps also leverages and keeps up with modern infrastructure, platforms, tools, and con‐ cepts, but it’s the cultural shifts and building of a shared sense of ownership across all teams that is the most valuable. Can the two coexist? Yes. Should they coexist? It depends. What are the goals of the business,  the  organization s ,  and  team s ?  Not  everything  is  one  size  fits  all.  SRE works  great  for  very  large  organizations  like  Google,  where  it  was  born,  who  have very specific and very complex problems, but it might not make sense for you and your organization. The same is true for DevOps. As much as there is good that comes from DevOps, the benefits only come from everyone in the business being bought in. —Sean Lutner, infrastructure architect, Edgewise Networks, Inc.  ◆ ◆ ◆  ◆ ◆ ◆  To many large enterprises, the primary difference between DevOps and SRE are their ages as buzzwords. Outside of Google, the term DevOps is at least six years older than SRE. This means the books, blogs, conferences, and experts associated with SRE have had far less time to exert influence. So far, there have been significant overlaps in the subject matter and advice from DevOps and SRE. Both have produced great guidance on  tools,  processes,  platforms,  cultures,  and  places  of  work.  Not  everything  SRE  Replies      195   espouses will be entirely new, but it is all  well, mostly...  worth paying attention to, and anyone interested in the DevOps movement would be foolish to ignore it. It is, however, wrong to think of SRE as being just another name for DevOps. Often  and  slightly  ironically   people  talking  about  DevOps  tend  to  focus  more  on  the development and release part of the software delivery life cycle compared to the oper‐ ations  side.  SRE,  on  the  other  hand,  has  an  existential  leaning  toward  improving operations. To some extent, if DevOps can be thought of as stepping in where Agile moved a bottleneck in delivery from development to releasing code, SRE is stepping in to alleviate a new bottleneck in operating and scaling the live running of systems. SRE is also comfortable being something DevOps never was: a job title  and at that, one capable of giving an instant morale boost to folks working in operations . Finally, the SRE movement provides useful new terminology to increase discussion of topics such as repetitive manual work  toil  and release management  launch configuration engineer . —Mark Rendell, principal director, Accenture  ◆ ◆ ◆  Reliability engineering and DevOps aim to solve the same problem set that most of the world is now realizing they are faced with: keeping digital services always online and available while improving functionality and operability over time. While DevOps remains elusive in a manifesto-type definition, reliability engineering assigns a more concrete role and responsibility to the term in many ways. At its core, site reliability engineering embodies and encourages the same exact principles that have been asso‐ ciated with DevOps since the term began entering the web operations lexicon. Build‐ ing  and  operating  digital  services  with  24 7  availability  expectations  has  become  a necessity for more than just Software as a Service vendors. Businesses, governments, and organizations across nearly every industry are faced with balancing rapid devel‐ opment of new functionality with maintaining the health and availability of the ser‐ vice. Engineering  practices,  teams,  and  individuals  lumped  in  to  the  title  of  SRE  helps businesses assign a concrete effort to these roles. The core reason for the existence of an SRE  team, individual, etc.  is to leverage technical skills in systems architecture, automation, and problem-solving along with social skills of collaboration and com‐ munication to seek new ways to know more about the system as a whole. This then feeds engineering teams to continuously seek out new and better ways of developing software as well as improving the architecture and delivery pipelines. —Jason Hand, Senior Cloud Ops Advocate, Microsoft  formerly VictorOps   196      Chapter 12: DevOps and SRE: Voices from the Community   At PayPal, we believe that site reliability engineers are both the ultimate enablers as well as the ideal practitioners of DevOps. To that end, we engineer for reliability in two major ways. The first, as platform providers, building and constantly improving the key tools that enable other SRE, dev, and ops teams across the enterprise. The sec‐ ond, as the embedded experts within business-critical product teams where dedicated reliability  engineering  is  required  to  achieve  world-class  availability,  productivity, and more. As we’ve grown in our practice of DevOps, we’ve found that the mental model that a gifted  SRE  is  able  to  develop  around  a  complex  system  like  PayPal  is  increasingly needed  within  the  organization.  This  mental  model  is  key  to  the  development,  by SRE,  of  highly  effective  tools  and  platforms.  This  mental  model  also  enables  SRE capabilities  that  are  uniquely  valuable  to  the  wider  dev  and  ops  teams  such  as  the exporting of systems knowledge, sharing in ownership of critical services, drawing on expertise during critical incidents, and driving insightful postmortems. —Andrew Farmer, senior manager, SRE, PayPal, Inc.  I believe SRE is a practice that can help DevOps at scale. DevOps alone worked fine for us while we scaled the company from 8 to 20 teams. However, as the number of engineering teams practicing DevOps grew, it became apparent we needed something overarching  to  help  guide  and  support  those  teams.  Lessons  weren’t  being  shared across  all  teams,  and  reliable  practices  weren’t  always  getting  the  attention  they needed. SRE helps bring in a level of monitoring of how reliability focused our teams were beyond  their  recent  incident  history.  Through  testing  and  analysis,  both  manually and through tools, SRE is able to help the teams understand their reliability risks and help prioritize them. The SRE teams are also able to commit time to building cross- cutting tooling and services that support all the engineering teams in ensuring the reliability  of  their  products.  Just  as  DevOps  and  PaaS  [Platform  as  a  Service]  can coexist so I believe DevOps and Reliability as a Service  SRE  can coexist. —Bennie Johnston, head of SRE, Just Eat  ◆ ◆ ◆  ◆ ◆ ◆  ◆ ◆ ◆  Replies      197   The largest difference is that DevOps is an approach and SRE is a specific job role. Despite that it typically manifests as a guardian of the “Deployment System,” DevOps is rooted in the multidisciplinary examination of delivery flow and overall effective‐ ness of managing services. The SRE role is a manifestation of labor division for online services.  Unlike  traditional  specialization  which  focuses  on  function,  SRE’s  labor division aligns around the product regardless of function. A key cause of confusion between the two is that both strive for multidisciplinary or cross-functional methods applied on their focuses; but it is the focus which is what differentiates  them.  DevOps  is  focused  on  the  process,  and  SRE  is  focused  on  the product. —Chris McEniry, systems architect  SRE and DevOps have a wide scope of overlap, but they are distinct ideas. As used in practice, companies recruit for DevOps when they want their maintenance develop‐ ers to also look after the production and nonproduction infrastructure as they con‐ tinue  to  make  improvements  to  an  existing  software  system.  In  contrast  to  this, companies  tend  to  recruit  for  site  reliability  engineers  when  they  want  to  retain  a division of labor between infrastructure management and development. The SRE will have deeper and broader infrastructure skills encompassing more of load balancers, networking, databases, and container orchestration systems like Kubernetes; whereas the DevOps developer will usually have deeper expertise in the business domain and preferred programming language. DevOps  teams  are  more  likely  in  environments  with  either  common  technology stacks or cloud hosted infrastructure. In these cases, the cloud provider is taking care of the infrastructure so the developers can focus more on delivering business value. Increasingly,  SREs  are  found  in  holdout  companies  that  are  biased  toward  self- hosting  or  environments  that  have  complicated  infrastructure  requirements.  These SREs provide the operational support that used to be the domain of systems adminis‐ trators. The difference being that the SRE is expected to draw on programming skills to automate the environment buildouts and application deployments or write patches for infrastructure tools in ways that the traditional system administrator could not. —John Siegrist, release engineer, Deswik Mining  ◆ ◆ ◆  ◆ ◆ ◆  198      Chapter 12: DevOps and SRE: Voices from the Community   While DevOps and SRE roles overlap a lot technical execution–wise, the distinction likely comes from the size and scale of the organization. Since software and its eco‐ system do not follow economies of scale, it becomes imperative for teams to special‐ ize and focus, as an organization’s software and infrastructure grows with scale; that is where the probable transition from DevOps to SRE comes into play. For smaller businesses and organizations, DevOps culture reduces the impedance between devel‐ opment and operations resulting in rapid iterations and change. As an organization grows and scales it becomes imperative to standardize how software is developed and deployed to reduce the cognitive load and effort when it comes to understating and getting things done. Both DevOps and SRE roles can make contributions here, DevOps can focus more on developer productivity  e.g., tooling like build systems, trainings, better testing, etc. , while SREs focus on maintaining uptime, upkeep of production systems, and special‐ ized  tooling   e.g.,  distributed  tracing  infrastructure ,  both  the  roles  become  a  full- time job with scale. The common aim is again to deliver change from code commit to production deployment, which is as frictionless as possible, highly visible, and pre‐ dictable. Yes, DevOps and SRE roles can coexist, but the line is blurred and not easy to draw. It would largely depend on the organization, its business, and size. —Pranay Kanwar, staff site reliability engineer, LinkedIn  ◆ ◆ ◆  Having  a  lot  of  experience  with  both  DevOps  and  operations,  and  now  running  a number of SRE teams, this topic has always been something I found a bit fascinating. I believe that the two concepts have a lot of overlap, despite the fact that they remain rather distinct. On the one hand, we have DevOps, an international software develop‐ ment movement that emphasizes looking at the delivery of software from a systems- thinking  perspective.  On  the  other,  we  have  SRE,  which  takes  an  approach  of managing operations from a software engineering perspective. There are many parts of SRE that fit very easily into the DevOps CAMS model, especially as it relates to things like culture with error budgets and measurement with the four golden signals. I think for me, one of the most interesting things is that SRE brings back, from tradi‐ tional  operations  work,  a  bit  of  the  “wall  of  confusion”  that  we  always  talk  about eliminating as much as possible in DevOps. This in no way means in SRE software is “tossed over the wall” like we like to bemoan from traditional software development. However, unlike developers carrying pagers in the DevOps model, there is a clear line of responsibility in the SRE model where they are responsible for meeting the SLOs of the production systems, even if that means working on those problems with devel‐ opment. This is probably necessitated by scale. In some respects, SRE is a hybrid of the legacy model and the new DevOps model, bringing elements of the “Operations  Replies      199   is a clearly distinct group” from legacy, and many components of the CAMS model from DevOps. Instead of having a wall of confusion, there is a wall of conversation or collaboration  it’s a low wall! . It’s no wonder we struggle to differentiate the two. —Dave Mangot, former head of site reliability engineering, SolarWinds Cloud  ◆ ◆ ◆  ◆ ◆ ◆  DevOps  is  really  about  Dev  and  Ops  working  together,  with  complementary  and overlapping skills, but really focused on different areas. The main goal is supporting developers in their quest to rapidly develop high-quality code, features, and value to the end user. At the same time, to deploy it cleanly and quickly onto high-quality, highly maintainable, and reliable infrastructure, then support and operate it to bring reliability, scalability, performance, security, and cost savings. SRE on the other hand, is really about building and managing highly reliable systems and applications. Thus, it operates at a higher level in the conceptual stack, in that SRE  focused  on  the  architecture,  configuration,  tooling,  monitoring,  and  manage‐ ment plus processes so the DevOps teams can really deliver on their goals. Develop‐ ers and operations tend to be buried in the day-to-day, while SRE ensures the whole system and ecosystem can meet its goals. —Steve Mushero, CEO, OpsStack  The relationship between DevOps and SRE is where an actual SRE role is defined and the methodologies from DevOps are actually implemented and practiced within that SRE  team  along  with  operations.  Where  DevOps SRE  become  similar  is  asking  an engineer  system software  to apply software engineering principles to the operations of  a  software  system  that  is  providing  a  service  to  users  or  other  systems.  Where DevOps SRE go in different directions is there is no manifesto with rules and regula‐ tions on how to implement DevOps. For SRE, while there is no defined prescription, organizations are making great strides to provide a playbook on how to define and implement SRE, so it doesn’t get watered down. For example, public cloud compa‐ nies are providing customers free services to help implement SRE within their orga‐ nization.  200      Chapter 12: DevOps and SRE: Voices from the Community   When you implement SRE at your organization you’re automatically consuming the DevOps methodologies as part of that implementation, so, in a sense, you’re imple‐ menting within the organization. The idea to have both exist is to keep focus on the mission of leveraging software engineering principles when performing operational work of software systems and services the SRE team is supporting. —Chad Todd  SRE is DevOps when you’re driven by SLOs. Both aim for the same goals but take different  paths.  The  SLOs  help  steer  effort  and  investment  and  there’s  no  similar instrument in DevOps. DevOps is more common for startups, where the incentive to reach production as fast as possible clearly outweighs reliability or availability. SRE makes more sense on well-established businesses when the conflict between innovation and reliability start to emerge. —Luis Mineiro, principal site reliability engineer, Zalando  In my view, DevOps is a set of practices organizations can adopt to better enable the operation  and  delivery  of  products  to  their  customers.  The  simplest  summary  of these  practices  is  summed  up  in  CAMS,  which  stands  for  Culture,  Automation, Measurement and Sharing. SRE  site or service reliability engineering  is a role that is generally  filled  by  individuals  or  teams.  In  my  experience  these  folks  are  broadly focused on the stability and operation of products in an organization. By and large, most SRE professionals rely on the four pillars of CAMS to do their work and influ‐ ence the organization to address operational needs. It’s worth noting that not all organizations “do” SRE the same way, especially as it relates to organizational structure. The two main forms I’m familiar with are “Hori‐ zontal” SRE integration and “Vertical” SRE integration. An organization with hori‐ zontally  integrated  SRE  staff  will  generally  have  one  or  more  independent  teams made up of folks that do SRE work for the organization. Vertically integrated staff are generally  integrated  directly  with  engineering  teams  and  remain  dedicated  to  said team. —Aaron Blew, director of service reliability engineering  SRE , iovation  ◆ ◆ ◆  ◆ ◆ ◆  ◆ ◆ ◆  Replies      201   DevOps 10 years ago to me was a way to express the need for a better, nonfunctional and nonpolitic agreement between who made software and changes to said software and who had the task of keeping the service up. DevOps now is a way to describe someone that has initiative and knowledge on the part of the stack that is fronting the service and feels comfortable changing that. Sadly, DevOps is a term used to indicate a skill set and a willingness to be one of the developers taking care of Ansible or Jen‐ kins or tending to help fix package repositories and not someone to make what is called “silos” go away. SRE to me is a methodology and a set of roles with inverse energy as DevOps—SRE keep the lights on and have the ownership, budget, and final word on what goes to production and what needs care in all aspects I mentioned before. It is the evolution of the Ops teams we saw in the last decade, fit to companies that can handle that and better  adapted  to  survive  Conway’s  law.  In  essence  we  all  expect  that  developers know how their software works and their stack in [depth], but this is far from what someone tasked with running services does. —Gleicon Moraes, director of engineering  ◆ ◆ ◆  As the SRE nomenclature continues to grow, it is interesting that some people see it as in conflict with the term DevOps. In most cases, this would seem to require a nar‐ row view of at least one if not both of those terms, as they are really just different responses toward a common problem; the continually increasing pressure by compa‐ nies to build and deploy software within ever faster iterations. In both cases the reali‐ zation that the “old ways” are not going to work is paramount, and while the SRE model takes a more tactical, engineering-focused approach to solving the problem, DevOps approaches this more from the cultural side, but still with an idea of reduc‐ ing friction and getting people to work across cultural barriers. One interesting aspect [of] both of these ideas is that they suffer from their current adoption patterns. On the DevOps side, we have seen so much hyperbole around the term, and so many companies trying to twist DevOps to fit their own  often sales- based  definition, that it has diluted what “adopting DevOps” even means. Ironically, on the SRE side, that term is now so dominated by Google’s published information on  the  term  that  companies  who  don’t  operate  like  Google   i.e.,  almost  everyone  may not be able to be able to pick up the practice. Working in a company that started adopting  an  SRE-style  approach  to  operations  almost  10  years  ago,  it  has  become harder to hire for those positions as people’s expectations changed to looking for a specific set of practices rather than an agreed approach to operations to a more spe‐ cific set of practices. My hope is that the conversation around these topics will con‐  202      Chapter 12: DevOps and SRE: Voices from the Community   tinue to grow so that others can gain some of the benefits in ways that work for their organizations, whatever the form those organizations take. —Robert Treat, CEO, OmniTI  ◆ ◆ ◆  ◆ ◆ ◆  ◆ ◆ ◆  SRE  is  holistic  at  its  core—everything  on  both  Dev  and  Ops  is  connected  and  the approach is to identify, understand, and execute said connections by diving deep into the low- and high-level technical matters of Dev and Ops, and help the teams deploy the implementation with the provided help. Unfortunately, SRE is often interpreted as an overseer position, like if it was an oracle where people go to ask questions, when it shouldn’t. At the end of the day, SRE could be whatever your organization needs it to be to ensure that you’re providing the best service possible and bringing teams together. —Manuel Fernandez, SRE, VividCortex  DevOps is a cultural term that embodies a spirit or enablement and accountability across  operational  and  development  teams.  A  typical  team  that  adheres  to  the DevOps philosophy brings the operational mindset of service ownership to the devel‐ opment teams and provides the guidance and tools necessary for them to more effec‐ tively maintain a product through its full lifecycle. SRE is [a] class of operational user with a strong development background along with a deep understanding of opera‐ tional principles and designs. SRE teams should closely embody the DevOps philosophy, using its basic tenets to provide the operational integrity and stability to their environment through contact with service owners and other operational teams. —Matt Jones, senior infrastructure security operations engineer  I’m working in a medium-sized startup where I actually hold both SRE and DevOps position. For me, being the SRE is about managing the production systems, making them reliable, handling the scale, automating, etc. While for the DevOps position I rather  think  it  is  about  being  the  junction  between  our  Devs  and  the  production. Being that link includes helping Devs in many aspects: having available staging env,  Replies      203   CI CD pipelines, automated testing, etc. Overall I think the DevOps engineer should be everything possible to ease the Devs’ life so they can focus on coding. While the two positions are different on the scope they are similar on the implemen‐ tation. It would make sense that SREs build the staging and preproduction env, make it available to Devs, which would qualify as a DevOps mission  once again, ease the Devs’ life . The same goes for automation that can be implemented. —Julien Avenet, SRE, Kiwi.ki Gmbh  A DevOps team member is someone who is familiar with installing and maintaining software on the systems for which they’re responsible. They know about an upcom‐ ing  minor  database  upgrade,  and  what  bug  fixes  and  new  features  are  part  of  that upgrade.  They’re  aware  that  the  disk  space  situation  isn’t  that  dire  but  will  need addressing  in  the  next  three  to  six  months,  either  by  trimming  the  database,  or  at least  partitioning  some  tables.  The  switch  we’re  currently  using  is  adequate,  but might be up for an upgrade, as well. Normally, things are fine, but in high-traffic sit‐ uations, it has been struggling a little. An SRE team member is thinking about how a new piece of software will need to integrate with other systems. Some situations will require additional table space in the database, and that requirement needs to be discussed with the DevOps folks. Two classes of servers will need to have low latency and redundant network connections, which shouldn’t be an issue, but will be another point to raise. And the code review should be interesting—we want to make sure that the algorithm is as smart as possi‐ ble, but no smarter. —Alex Beamish, software developer, independent  ◆ ◆ ◆  ◆ ◆ ◆  I think of DevOps as an approach, and of SRE as a role. DevOps was a kick in the pants the infrastructure world needed. Its core message is simple: building and test‐ ing of infrastructure is something we can and should automate. It’s not prescriptive about how you structure your teams, though it definitely encourages structures that support collaboration between product and infrastructure engineers.  204      Chapter 12: DevOps and SRE: Voices from the Community   SRE  is  a  role  whose  tenets  align  well  with  the  goals  of  DevOps.  Having  dedicated SREs at an organization is a bit of a luxury. You have a group of people whose sole focus is making your service run better. Not every organization needs a whole team doing this! It’s perfectly fine to distribute the work around a more generalized engi‐ neering team, especially early in a company’s life. —Chris Sinjakli, site reliability engineer, GoCardless  ◆ ◆ ◆  It’s become repetitive for infrastructure-oriented Devs and SREs to repeat: “DevOps is  a  philosophy,  not  a  job  title.”  At  their  cores,  DevOps  and  SRE  are  both  about putting developers in charge of operations—the difference is in ownership. A small dev  team  can  practice  fantastic  DevOps—automated  and  frequent  rollouts  with CI CD, comprehensive monitoring and alerting, and a shared on-call rotation, while the Devs themselves still own the services. SRE is a way to have a separate group of developers  own  the  production  environment  when  its  reliability  is  critical  to  the organization. The difference is that SRE is structured, organized, and well defined, and DevOps is harder to pin down. In my opinion, organizations that want to practice DevOps are best suited to have a developer team read the SRE book and consider a few compo‐ nents to integrate across the entire team, not just for one or two “DevOps” people. Insofar as DevOps can be a superset of SRE, they can  and should  peacefully coexist in a healthy organization that recognizes a shared responsibility for reliability among all developers and SREs. —Jason Gwartz, software developer  Replies      205    CHAPTER 13 Production Engineering at Facebook  A discussion with Pedro Canahuati, Facebook, and David N. Blank-Edelman  David: What’s production engineering? Pedro:  Philosophically,  production  engineering  stems  from  the  belief  that  opera‐ tional problems should be solved through software solutions and that the engineers who are actually building the software are the best people to operate that software in production. In the early days of software, a developer who wrote the code also debugged and fixed it. Sometimes, they even had to dive into hardware issues. Over the years, with the advent of remote software systems, the internet, and large data centers, this practice changed dramatically. Today, it’s still common to see software engineers writing and developing applications, then handing off their code to a QA team for testing, and then handing that off to another team for deploying and debugging. In some envi‐ ronments, a release engineering team is responsible for deploying code and an opera‐ tions team ensures the system is stable and responds to alerts. This works fairly well when QA and operations have the knowledge required to fix problems, and when the feedback loops between the teams are healthy. When this isn’t the case, fixing and or debugging production issues needs to work its way back to the software engineers, and this workflow can significantly delay fixes. At Facebook, our production engi‐ neering [PE] team is simply bringing back the concept of integrating software engi‐ neering [SWE] and operations. We started the PE model a few years ago to focus on building a more collaborative culture between the software engineering and operations teams. Our goal is to ensure that  Facebook’s  infrastructure  is  healthy  and  our  robust  community  of  users  can access the platform at any hour. The PE team is a critical component in accomplish‐ ing this through automation, writing new tools to make operations easier for every‐  207   one, performance analysis, hardcore systems debugging, firefighting when necessary and by teaching others how to run their systems themselves. Facebook engineering has  built  common  infrastructure  that  everyone  uses  to  build  and  deploy  software. Facebook’s infrastructure has grown organically over the years, and while I’m confi‐ dent  that  we  will  solve  many  of  the  operational  problems  we  have  today,  we  still haven’t. Production engineers help bridge this gap to ensure teams can get back to solving the hard software problems we face and spend as little time as possible on operational problems. The  PE  team  not  only  writes  code  to  minimize  operational  complexity,  but  also debugs hard problems in live production that impact billions of people around the world—from backend services like Facebook’s Hadoop data warehouses, to frontend services like News Feed, to infrastructure components like caching, load balancing, and deployment systems. PE, working side by side with SWE, keeps Facebook run‐ ning. The team also helps software engineers understand how their software interacts with its environment. Think  of  production  engineering  as  the  intersection  of  large-scale  manufacturing  hardware,  automotive,  industrial,  etc. ,  expert  engineering,  and  good  operational management.  A  production  engineer  typically  has  wide  knowledge  of  engineering practices and is aware of the challenges related to production operations. The goal of PE is to ensure production is running in the smoothest way. A great analogy to our role is that of a production line for manufacturing automobiles. A team of designers creates the car, a team of engineers builds the hardware and another team is responsi‐ ble for the automation that puts it all together. When this process breaks, a produc‐ tion  engineer  steps  in  with  knowledge  of  the  whole  flow  of  the  production  line, including everything upstream and downstream. They understand how the automo‐ bile was designed, how it’s supposed to function, and what the software used to build it is supposed to do. Armed with this knowledge, production engineers can trouble‐ shoot, diagnose, and fix the issue if they need to, and they also work with the entire team to prevent it from happening in the future. At  Facebook,  production  engineers  are  not  line  operators,  but  they  do  know  how everything in the line actually operates. For example, when software is responding to user traffic, or even when it’s failing, production engineers are often the ones who best understand how the code interacts with its environment, how to fix and improve it, and how to make it performant over time. David: Can you say a little bit more about the origin story of PE? Pedro: During its early years, Facebook applied the then-industry-accepted approach to operating its production website and services via a dedicated operations team. The operations team consisted of separate Site Reliability Engineering [SRE] and Applica‐ tion Operations [AppOps] teams.  208      Chapter 13: Production Engineering at Facebook   The SRE team had more in common with a traditional communications provider’s network  operations  center  [NOC]  than  it  did  with  solving  operations  problems through software. At the time, a team of less than 20 monitored the systems infra‐ structure for issues, reacted to alerts, and triaged problems using a three-tier escala‐ tion process with the support of AppOps. SRE worked in three shifts to provide 24 7 coverage. AppOps, on the other hand, was a small set of individuals already embed‐ ded within Software Engineering [SWE] teams. The model back then was effectively one AppOps engineer per service  for example, newsfeed, ads, web chat, search, data infrastructure  with a ratio of production engineers to software engineers of approxi‐ mately 1:10 but could range as high as 1:40. Sometimes, there was one AppOps engi‐ neer  assigned  to  multiple  services.  For  example,  data  warehouse  and  centralized logging systems had one embedded AppOps engineer. Early on, AppOps was able to understand  the  full  application  stack  well  enough  to  react  to  and  resolve  outages quickly. Even though the SRE team was supposed to be engaged with SWE teams developing software, SRE was often absent in the early development process and regularly found out  about  new  software  changes  or  new  services  being  built  as  they  were  being deployed. The relationship between SRE, AppOps, and SWE wasn’t strong. The SRE team also had to juggle the additional responsibility of lighting up new data centers and  ensuring  capacity  needs  were  met  during  peak  traffic  hours.  SRE  ensured capacity needs were met by shifting users’ loads across multiple data centers using web interfaces on enterprise-level load balancers. There was some level of automation happening through ad hoc shell scripting and lightweight tooling that incorporated APIs from these devices or from in-house written code bases. While the SRE, AppOps, and SWE teams focused on scaling the website and infra‐ structure for web-based services, users started transitioning from desktop to mobile, and the whole company was reprioritized to follow a mobile-first strategy. This sig‐ nificantly increased service complexity and accelerated the need to provision an ever- increasing amount of infrastructure. The scaling of Facebook’s services in a mobile world,  combined  with  hyper-scale  customer  growth,  overwhelmed  the  SRE  and AppOps teams. The majority of the AppOps engineers had to be on call 24 7, and firefighting was the norm. As user growth and complexity increased, both SRE and AppOps  were  unable  to  focus  on  recruiting  and  couldn’t  hire  additional  staff  for months, pushing the teams further underwater. Adding to an already challenging situation, infrastructure capacity was falling behind due to a lack of automated data center provisioning from the SRE team and the con‐ stant need to fix individual servers in production. Without the necessary automation to keep up with server failures and adding new clusters, we ended up in a capacity crunch. People were overwhelmed and experiencing burnout. A better approach was clearly needed. Our management team recognized that the operations team had not  Production Engineering at Facebook      209   kept pace with changes in the business and that the current operating model was no longer adequate. The main challenges we faced were unclear expectations of the roles of SRE, AppOps, and  SWE;  an  inability  to  balance  firefighting,  server  failures,  and  turning  up  new capacity; weak credibility with the SWE teams when asking for changes in architec‐ ture. We first focused our efforts in clarifying the roles for SRE, AppOps, and SWE. We understood that we needed to be involved earlier in the software process and that the embedded  model  had  a  higher  likelihood  of  effecting  change.  We  also  needed  to establish  stronger  working  relationships  and  credibility  between  the  software  and operations  teams  and  ensure  that  the  SWE  teams  had  stronger  ownership  of  their services. We  executed  a  multistep  reorganization  and  hired  different  types  of  engineers.  To continue  managing  the  growing  infrastructure  needs,  we  decided  to  split  the  SRE staff into two. We would retire the SRE moniker and stand up a new Site Reliability Operations [SRO] team, and we would expand the existing AppOps team with some members  from  the  former  SRE  team.  By  moving  key  individuals  from  the  former SRE  team  we  were  able  to  expand  AppOps’  operational  knowledge  and  staffing quickly  and  double  the  team’s  size.  Retiring  the  SRE  moniker  also  helped  change expectations from the SWE teams over time. SRO’s remit was to focus in two areas. The first was to continue the needed and noble effort of firefighting outages. The second was to build software and automation to reduce the human involvement in these efforts. We also moved some of the reorgan‐ ized SREs into an SRO function focused on turning up new capacity. The discussions around expectations of these roles brought to light the fact that many SWEs didn’t have empathy for the role of these operational teams. Operations was a bit of a black box to many software engineers, and we needed to change this percep‐ tion. The embedded model would help to make sure that the SWEs understood what it takes to operate a service at scale, and, later, as we embraced shared on-call, we would make space for SWEs to gain more empathy. However, we didn’t always have the credibility needed to influence the SWE teams to change the architecture to be more stable. Part of the problem was that SWE, SRO, and AppOps didn’t speak the same language of algorithms, concurrency, scalability, and efficiency that goes into building a distributed system. There were a few engineers in SRO and AppOps that had this knowledge, but it became clear that we needed additional individuals who were more like software engineers and also understood operations to help make this transition.  As  a  first  step,  we  actively  recruited  and  hired  several  technical  leaders with deep experience in operations and infrastructure, focusing heavily on candidates who demonstrated a strong cultural fit in addition to technical acumen. These new hires  needed  both  algorithmic  and  practical  coding  skills  in  addition  to  an  under‐  210      Chapter 13: Production Engineering at Facebook   standing of low-level systems and building distributed services. We also desired sup‐ portive communication and influence skills. We were successful in finding this type of engineer and adding them to both SRO and AppOps. Changing  the  expectations  for  SRO  and  AppOps  from  SWEs  is  challenging.  Engi‐ neers  that  had  been  at  Facebook  when  SRE  and  AppOps  had  been  established expected  these  teams  to  merely  do  firefighting  and  operational  work  for  the  SWE teams.  A  typical  comment  overheard  in  meetings  when  discussing  remediation  of issues during outages and ownership of stability would have been: “Well, that’s the AppOps job. I shouldn’t be doing this as a SWE.” Even though there were some SWE teams  that  had  a  culture  of  strong  ownership  of  stability  and  did  firefighting  or worked  on  solving  operational  problems,  many  did  not.  We  needed  to  change  the expectation of what the operations teams did and we needed the SWEs to take own‐ ership of their stability and operational issues in production. It wasn’t enough that we had changed our hiring practices to focus more on software engineering skills, in addition to network fundamentals, systems internals, and large- scale systems designs. In order to reset the expectation bit, we needed to rebrand. We decided to remove the word “Ops” from the “AppOps” name and became Production Engineering. Removing “Ops” from the name had two main outcomes:    It communicated to the rest of the company that we were an engineering team building software to solve operational problems, not just traditional operators of systems.    It helped reset the notion that SWEs could just have their services run by some  other ops team.  We  still  had  one  major  hurdle  to  overcome  related  to  ownership  of  stability  and operational issues. As more SWEs and, now production engineers worked on scaling services,  SRO  continued  to  be  an  operational  backstop  during  service  outages  and responded to alerts 24 7. SRO embarked on building an automation framework we named FBAR [FaceBook Auto Remediation] that solved one of the main pain points at the time of running a service in production: removing failing servers and replacing them with healthy ones. SRO continued to grow while this system matured with a future eye of handing over this operational work to automation written by PEs and SWEs. After a few years of building up PE teams and hiring more SROs, the complexity of our infrastructure grew so much that our centralized SRO team no longer was able to comprehend how every service worked. Their ability to respond to outages became difficult, and SROs became worried that instead of fixing the issue, they may cause a new  outage.  We  realized  we  needed  to  dissolve  the  SRO  team  and  hand  complete  Production Engineering at Facebook      211   operational ownership 24 7 to the PE and SWE teams. SRO spent the next 18 months working with PEs and SWEs to write code that ran on FBAR to handle many of the server issues that required SRO response. Where we had PEs paired with SWEs, they created a shared on-call rotation. Where SWEs didn’t work with PEs, they took on- call ownership. At the end of these 18 months, the centralized SRO team was dis‐ solved, and its members moved over to production engineering. It took about four years to complete the transition from SRE and AppOps to all production engineer‐ ing. David: Let’s talk a little bit about structures. Some SRE models have this notion that software teams get an SRE after a certain amount of work is completed by the SWEs. SREs are sourced from a completely separate group that is not part of the product group or the service group. They live in their own org. How do you think of the orga‐ nization around PE? How do production engineers relate to the larger org tree? Pedro: Our model borrows from the separate organizational structure of some com‐ panies  and  the  embedded  nature  of  some  SRE  and  operations  teams,  where  they report into the product group or the business unit. We have a centralized reporting structure and a decentralized seating structure. The operations function needs to ensure that operations is running smoothly across all of the engineering functions. Sometimes, this may be hindered by competing pri‐ orities  of  the  people  who  report  to  the  highest  C-level  executive   for  example,  the CEO .  In  many  companies,  the  highest-level  operational  executive   Head  of  Ops  usually wants to report to the CEO, and I believe this causes more problems than it solves. I believe the operational executive should instead report to the most senior software  engineering  executive   Head  of  Eng .  This  effectively  makes  the  Head  of Ops a peer to other software engineering groups instead of a peer to the Head of Eng. This also makes the Head of Eng responsible for the success of operations. When per‐ formance assessments are taken seriously, and if the operations team isn’t succeed‐ ing, then the engineering executive is not succeeding. This, in my opinion, fixes a lot of the dynamics of “Dev versus Ops” because Ops is now part of Dev. I recognize that there are a lot of larger companies out there that have splintered their operations teams and they each report into the head of their business unit. I’m sure in some cases this works, but it’s very hard to make that work well. What I’ve seen hap‐ pen more often is that the head of a business unit forces the operational team to do their bidding and the escalation path is very muddy when it comes time to deal with a disagreement. So, to prevent some of these dynamics, we kept a centralized reporting structure for these main reasons:  212      Chapter 13: Production Engineering at Facebook   Flexibility  Motivation  Our production engineers can work on hardware design, UI, backend software, and  everything  in  between.  For  example,  a  production  engineering  team designed  and  built  out  a  Faraday  cage  for  wireless  mobile  phone  testing.  We developed a specialized rack with mobile devices where engineers can run their software on various types of mobile hardware and do on-device testing. The cen‐ tralized reporting structure gives us the flexibility to set our own goals and decide what kind of work we “should do” versus being told what work needs to be done by the lead of a specific business unit. The headcount for people doing PE work is managed separately from the broader engineering headcount and so the lead‐ ership  in  PE  can  ensure  this  work  won’t  get  reprioritized  by  the  business  lead who may have different priorities.  PE  managers  are  able  to  motivate  their  teams  to  get  work  done  based  on  the problem they’re trying to solve, as opposed to focusing on shipping a product or service. There’s a qualitative and a quantitative aspect to evaluating production engineers, and certain things that motivate individuals in production engineering are not necessarily what motivates software engineers. The PE management team is  able  to  give  people  the  guidance  to  work  on  the  context  switching  that operations-minded folks are really good at when it’s needed, and the ability to solve software engineering problems when it’s time to focus on them. I’ve found that there are those, like me, who run toward a problem instead of away. I’m pretty sure one of the reasons I chose to not go the pure software route was because I enjoyed the context switching. I have found that even though the folks who like PE-type work may complain about context switching, they actually enjoy it. There’s also a certain adrenaline-like rush they get when they finally fig‐ ure out that one little detail in the system that unblocked the problem, restored the  service,  and  allowed  everyone  to  breathe  easily  and  get  back  to  work.  I’ve always found that the best way to hire and manage a PE is to figure out what excites them about this kind of work and hire managers who can understand this aspect of their personality. Having performed the same  or similar  type of role in the past more often than not increases the likelihood that these managers will be able to successfully motivate them. I’ve found, conversely, that many of the managers who really just want to think about algorithms and write pure software are challenged by how to manage and motivate PEs.  Shared accountability with slight tension  We hire external software engineers that may come from a traditional Dev-to- QA-to-Ops  model  and  believe  operations  should  do  the  things  software  engi‐ neers don’t want to do. A separate organizational structure creates a buffer for the  production  engineering  leadership  team  between  operational  stability  and features. The SWE and PE teams need to come together to build a stable, reliable,  Production Engineering at Facebook      213   secure, efficient, and feature-rich service. If a software engineering manager and a production engineering manager have a disagreement, we need to make sure that they are working together to solve the problem. The software engineering manager can’t say to the PE team, “You should do all my operations work for me.” The production engineering manager also can’t say to the SWE team, “You should  only  work  on  stability  and  reliability.”  There  needs  to  be  a  balance between these two competing priorities. Our structure provides that healthy ten‐ sion  and  shared  accountability  for  doing  what’s  needed,  not  what  might  be defined by a single manager’s responsibility.  At  Facebook,  we  have  a  saying:  “code  wins  arguments.”  We’ve  applied  a  similar model  to  keep  teams  accountable  when  there’s  disagreement  between  operational load and new features. When this happens, we bring the managers and tech leads of both SWE and PE together to discuss their views with senior leaders in their respec‐ tive organizations. These managers and tech leads provide operational metrics that help us all understand what is not working in a system service and what the team is going to prioritize to improve these metrics. The discussion also needs to touch on what features may be delayed based on this prioritization. This way, all the leaders understand the potential impact to the business and can make an informed decision. Ultimately, I and my counterparts on the SWE teams are held accountable by our manager to build systems that are feature-rich, stable, and operable. Since the org tree for all of us meets at the same level, if we don’t do our jobs, our performance suffers. This, in my opinion, gives us the best of both worlds. The decentralized, embedded model gives us greater abilities to influence how serv‐ ices are built. PEs sit right next to their SWE peers. They go to their meetings and offsites;  they’re  available  for  hallway  conversations  and  ad  hoc  discussions  about architecture.  Both  the  SWE  and  PE  managers  work  on  what  the  road  map  should look like for a service, what will make the service more reliable, and also what fea‐ tures are needed to enable growth. They compromise between features and stability. With this embedded structure, the software team also gets the constant vigilance of the production engineers as well as the interaction of pointing out problems that are actively  happening  and  need  to  be  solved.  When  problems  surface  in  production, both  SWEs  and  PEs  huddle  together,  shoulder  to  shoulder,  solving  the  problem. When the software engineer is on call, the production engineer is sitting there with them, and the software engineer can also just lean over and ask, “Hey, I don’t know how to do this operation in production right now. Can you jump in with me, help me, and teach me how to do that so I can be more effective in the future?” I’m sure that this isn’t the only way that works and there are likely other organiza‐ tional  models  that  can  perform  in  the  same  way  and  don’t  need  to  have  the  same reporting structure with colocated people. This is the way that we’ve found works the best for us at Facebook.  214      Chapter 13: Production Engineering at Facebook   David:  You  mentioned  on-call  earlier  in  the  PE  organization’s  origin  story,  but  I thought you said that the software engineers were on call, and the PEs were not on call. Is that always the case? Pedro: No, not always. A phrase I often use is: “if you write code and release it to production, congratulations, you own it.” This meant we needed to get SWEs to take responsibility for keeping their services up in production and also taking primary on- call. PE is not primary on-call for services we don’t own or build outright. We have a shared on-call model when we’re embedded with an SWE team and how that on-call rotation manifests itself is situational. In most cases, it’s a weekly rotation. There’s a software engineer on call for one week, then a PE on call the next week, and so forth. In some cases, due to cognitive load or because the infrastructure is currently harder to manage, some teams do shorter on-call rotations of a few days. We have only two scenarios where solely software engineers or production engineers are on call:    A software engineering team that doesn’t have embedded production engineers. In  this  scenario,  they  have  no  choice  because  they  have  to  be  on  call  for  their service.    Places in our environment where production engineers build everything end to  end.  David: Like infrastructure, the DNS team, that sort of thing? Pedro: Yes, exactly—PE actually owns a couple of pieces of infrastructure, like the software  used  to  provision  servers,  or  manage  server  replacements  in  production   FBAR . FBAR is an automated system that handles repetitive issues so that engineers  SWE and PE  can focus on solving and preventing larger, more complex site disrup‐ tions.  We  also  built  systems  that  automate  service  migrations  during  maintenance and another focused on turning up new clusters from scratch. We own and built the L4 load balancer that sits in between our L7 load balancer and our web servers. For the Faraday cage-like rack I mentioned earlier, we built Augmented Traffic Control [ATC] that allows developers to test their mobile application across varying network conditions, easily emulating high-speed, mobile-carrier, and even severely impaired networks. In these last few examples  and there are others , we are on call 100% of the time, because we’re the ones who built these systems. It follows the same model I described earlier where the team that built and deploys the software into production owns it, and the team that built it has the accountability to fix it when it breaks. David: Given this structure, how do you manage the relationships between the PE org and other teams? Pedro:  Company-wide,  we  send  out  what  we  call  Pulse  Surveys  every  six  months. One of the questions asked relates to how well a team collaborates with their partner  Production Engineering at Facebook      215   teams. Facebook has built a lot of common software that is used by other teams and is critical to its operation. So teams need to collaborate well together, and this includes PEs embedded in SWE teams. The survey outputs a general favorability score that tells us if the PE team thinks it collaborates well with other teams. Often there have been signs along the way and this survey is now specific data we can use to narrow down  the  problem.  We  start  by  asking  a  bunch  of  questions.  For  example:  “Is  the relationship working well? Does the PE team feel like they have a voice? Are the PEs listening  to  the  SWE  team’s  needs  to  lessen  operational  load  and  building  tools  to solve  these  problems?  Do  the  PEs  feel  like  they’re  being  treated  as  equals?  Do  the SWEs understand what work PE should be doing, and vice versa?” If we find that the relationship isn’t healthy, we talk to the PE and SWE managers and tech leads to gather more feedback. If the feedback points to confusion around how to work with PE, or vice versa, we educate everyone on what a successful PE and SWE team engagement looks like and what a successful healthy partnership feels like. We also discuss what shared ownership means and how SWEs need to care about the stability of their system and that more PEs potentially doing firefighting isn’t sustain‐ able.  We  make  sure  the  PEs  aren’t  being  obstructionists  and  preventing  the  SWE team  from  innovating,  if  for  example,  they  are  constantly  saying  “No”  to  changes. People and relationships are challenging and sometimes value systems are just not aligned, but we can’t ignore these relationships and let them become toxic. Ultimately, if we can’t come to an agreement on how to work together, we need to make a change. If the relationship problems stem from the PEs, we work on remov‐ ing the individuals causing these problems, talk to them and their manager about our collaboration expectations, and begin the work to rebuild the team if necessary. If the relationship problems stem from SWEs not wanting to own their services in the way we’ve  described  or  constantly  dismissing  the  work  needed  to  make  a  system  more stable,  we  will  also  talk  to  their  manager  and  tech  leads.  We’ll  revisit  the  situation sometime  in  the  future  after  we’ve  given  everyone  time  to  work  things  out.  If  the SWE team’s expectations of PE continue to focus on handing off operational work, then we will gladly redeploy PEs into other SWE teams. There are plenty of places where the PE skill set and discipline will be valued, treated equally, and able to effect change. Removing an embedded PE team is a lever we pull as a last resort and after we’ve exhausted all our methods to build a strong relationship. I have only done it a small number of times because we can definitely lose credibility with the SWE team and it will make it much harder to build trust in the future. That being said, I would rather not  burn  out  PEs  trying  to  force  something.  In  the  few  cases  where  this  has  hap‐ pened,  the  SWE  teams  have  come  back  some  months  later  asking  to  try  to  work together again. The reality is that some software teams don’t have people who have an operational mindset and can quickly get overwhelmed with work due to a skills  216      Chapter 13: Production Engineering at Facebook   gap or the mounting operational debt. Sometimes, learning a lesson the hard way is the best way for everyone to start fresh. David:  Let’s  continue  down  this  path  of  organizational  structure  for  one  more moment. Does every project get a PE? What do you do when you show up? Pedro: You noted earlier that in some companies, SREs don’t arrive until they hit some level of maturity or operational stability. At Facebook, our M.O. has been to ruthlessly prioritize. Not every SWE team gets to work with an embedded production engineering team and it is situational. It has to do with the service itself and its stage of development, the maturity of the service and the team. We would ideally like to enter  in  the  nascent  phase  of  a  software  team,  because  they’re  building  something new and might not know exactly what it’s going to become. By embedding ourselves early on, we could get some of the operational work accomplished early and quickly. Sometimes that happens, and sometimes it doesn’t. There  are  software  engineering  teams  that  end  up  building  some  services  in  the beginning that don’t have production engineers. This is typically a new service that doesn’t quite have a well-defined use case. It could be someone’s innovative idea to solve a problem, but it will take a while to develop. Once that service is established, and they realize that they’re hitting the stage where scaling is critical, they come to us looking for help. We keep a running list of these services and teams and as we hire more  production  engineers,  we  use  that  list  to  prioritize  what  is  critical  and  what needs the most help. In  some  cases,  like  when  we  built  out  live  video  and  our  generalized  videos  infra‐ structure, we knew this was going to be a legitimate use case and so we were involved from  the  beginning.  Unfortunately,  we  had  to  pull  valuable  members  from  other teams to stand that team up, and there’s always a conversation about the trade-offs we’re making. When we engage in the scaling stage of a service, we need to figure out what work the team should tackle first, and that’s sometimes hard to pin down. The service may be suffering from any one of reliability problems, capacity, deployment issues, or moni‐ toring issues. Andrew Ryan, a production engineer on our team who often helps us with organizational design, came up with a “Service Pyramid” that is loosely based on  Maslow’s Hierarchy of Needs. I presented about this hierarchy of needs in an SRE‐ Con talk about production engineering. I later found out that Mickey Dickerson also presented about a similar service reliability hierarchy of needs at an O’Reilly confer‐ ence. It was nice to see that this concept for how to approach work was shared across a few other teams. We use this service hierarchy to prioritize the type of work production engineers do when they’re first engaged with a team. The bottom layer of the pyramid is focused on ensuring that the service is integrated well with standard Facebook tooling to deal  Production Engineering at Facebook      217   with  the  life  cycle  of  a  server   provisioning,  monitoring,  replacement,  migration, decommissioning   and  service  deployments   new  deployments,  integration  tests, canarying, etc . Once you have the basic needs of your server and service met, you can move up the layers of the pyramid to working on higher-level components like performance tuning and efficiency, disaster recovery, anomaly detection, and failure modeling. Only then can you efficiently work at the top of the pyramid on “weird stuff.” These are things that may not happen at smaller scale but happen in our environment due to the influx of data and the amount of work done on backend systems. Every PE and SWE has an obligation to investigate these weird issues, but if the basic needs of the service aren’t met, they might find themselves chasing issues that should have already been dealt with automatically instead of real problems of scalability. David: What do you mean by stages of development? Pedro: I see teams and services go through three phases in this order: Bootstrap phase  Do  anything  and  everything  that  you  need  to  get  the  service  up  and  running. That might mean a lot of firefighting and manual intervention to fix things. It might mean quick, iterative deployments that fail fast. It might mean just allow‐ ing a trickle of traffic at first. You build up these operational muscles and figure out what the failure modes are and how it affects other systems.  Scale phase  Once you’re out of the bootstrap phase, you start to move into the scale phase. This might include deploying the service into multiple regions, getting the ser‐ vice used by millions or billions of people depending on the type of service. The team gets much more mature at being able to operate the service and its feature set,  understanding  the  dependencies  on  other  systems,  and  the  architectural changes that may need to occur over time.  Awesomize phase  Now the service needs to become really, really awesome. Do the last 10 to 20% of work needed to optimize the service to be more efficient and more performant. I call  it  “awesomize”  because  when  I  try  to  ask  people  to  optimize  something, nobody really wants to do that. But everybody wants to make something awe‐ some, so I call it the awesomize phase.  The people required for each of these phases might be different. There is a certain set of people on both the software and production engineering teams who really love to do the bootstrapping work. There are also those who really love the middle of this continuum. The bootstrapping is done, and they want to scale the service: make it better, more resilient, and bigger; take on more users; and deal with the consistency and  concurrency  problems  and  the  big  disaster  recovery  problems  that  will  arrive  218      Chapter 13: Production Engineering at Facebook   with a higher level of maturity. There are even others that want to make something performant,  efficient,  and  rock-solid.  Some  people  will  evolve  over  time  and  grow with each of these phases, but my experience has been that most do not stay with the service  through  these  three  phases.  They’ll  find  their  sweet  spot,  and  they’ll  move around in the organization to find the work that plays to their strengths. Ultimately, we want everybody to do the awesomization work, but the reality is that not every‐ body does and that’s OK. David: Given these phases and people’s inclinations toward them, how do you create teams? Pedro: I’ve seen a tendency to try to stretch people to do everything and become a jack-of-all-trades that can work up and down the stack, from lower-level hardware issues, to middle-layer protocol problems, to UI programming. This model is useful and needed in many startup environments but does not work well at scale. There is no way that one human can do that type of work and make themselves sustainable over time and not burn out, so we focus more on matching individuals to technolo‐ gies.  For  example,  on  the  cache  team,  knowledge  about  network  protocols  and debugging is needed, but understanding how the overall system works is the ultimate goal. When we are starting up a new team, we look for four factors and ask these ques‐ tions:    Is there enough work for at least three people for the next 18 to 24 months? I came up with three because that number, to me, really defines a team. If there are only two people and one is sick or wants to take vacation, the other person has to take the entire workload. When a third person is added, then at least people can pair up on projects, define shared responsibilities, etc. It’s simple team dynamics.   Does the service fit our prioritization model? We need to understand how the service is solving a business need and that it will be something that’s used and not just a prototype that may never see production scale. Is it the right time to prioritize this team over another? This one is tricky because it’s much more sub‐ jective.    Do we have a manager who can work with the production engineers and build out a larger team? The manager is a critical component, making sure that engi‐ neers are focused and getting things done. It’s important that everyone is devel‐ oping and growing over time. We need to ensure the team is getting the right level of context for their work and that they’re learning from other people.    Is there a local SWE team to work with on this service? This is primarily in the case where we’re not the ones building the software. We need to make sure that  Production Engineering at Facebook      219   there is an SWE team that can engage in shoulder-to-shoulder debugging and in- person discussions around architecture and problems.  These  four  ingredients  have  to  come  together.  That  filters  out  a  bunch  of  nascent projects that may take up valuable people. Although it would be nice to have produc‐ tion engineers on every type of team, it doesn’t make sense based on our prioritiza‐ tion model. In  order  to  establish  a  new  location  with  production  engineering  teams,  the  four ingredients above need to exist and we add another constraint. We need to ensure the new location has the ability to sustain three different teams, of at least three people for 18 to 24 months working with SWEs locally. This means we establish PE teams later into the site’s maturity. David: It does, but what about the small stuff? The things that have to be done but aren’t going to take that long? Is there a catch-all team? Pedro: No, there’s no catch-all team that does that kind of work. In general, it is the software engineering team’s responsibility to manage their technical and operational debt. They do this for as long as they can, but eventually if their service needs to be prioritized and it makes sense to build a team, then we do. Oftentimes, PEs that have an affinity for certain work might see something on a team without PEs and spend a couple of weeks on it to make it better, and then come back to their original team. We think this is valuable overall and where we can, we encourage it because it can help the SWE team gain some quick operational efficiency and knowledge. In infrastructure, we generally focus on making operational things like bootstrapping go away. We have built a lot of services that give engineers “more for free.” They can use our containerization service for deployments. They will get general server health monitoring for free. We have a centralized monitoring system with built-in graphing, anomaly detection, and alerting. The service gets basic remediation for free through things like FBAR  originally built by SRO, then significantly augmented by PE . All the basics are handled for you so you can focus more on the higher-level software problems. This allows our software engineers to do rapid prototyping and work on the small things first and figure out whether there’s something worthwhile building versus having to focus on the small stuff. This kind of “more for free” stuff gets you through  some  of  the  bootstrapping  phase  I  discussed  earlier  without  needing  too much initial help because it’s all self-service. David: We’ve talked a little bit about how a PE gets involved with a team and the product or service. How does a PE leave the team? Pedro: Mobility is actually a core tenet of ours. We like to hire generalists. In addition to the core practical and algorithmic program‐ ming skills, we also look for other traits. We expect PEs we hire or train to under‐  220      Chapter 13: Production Engineering at Facebook   stand  network  protocols  and  how  to  debug  them.  They  need  to  have  lower-level systems knowledge and understand how software interacts with the kernel, the hard‐ ware, and the network layer. If they are further in their careers, they need to under‐ stand how to build distributed systems. These are the general skills we look for when hiring PEs. When they join a team, they may not be experts in every one of these dimensions,  but,  over  time,  they’ll  gain  this  knowledge  and  experience  and  they’ll become even stronger engineers. This knowledge allows them to move around more easily within PE. They  will  also  learn  how  to  use  Facebook-built  tools  and  services.  Many  of  these mimic services outside of Facebook, like a containerization service. If the service is using our in-house containerization system—be it Cache, Messaging, Ads, or News‐ feed, or anything else—it’s still the same containerization service. The inner workings of  the  system  they’re  working  with  and  the  problems  that  come  up—concurrency, consistency, disaster recovery, for example—will vary depending on the service. That is what a PE needs to learn when they land on a team, but the general skills of manag‐ ing systems in our environment and how to use the tools we’ve built appropriately are portable. PEs can take all of this knowledge and move to any team at Facebook as long as they are colocated with the SWE team building that service. It’s a lot easier to collaborate that way rather than having the operations team and software engineering team located in different areas or even different time zones. So,  to  answer  the  original  question,  we  influence  our  managers  to  ask  at  18  to  24 months into the PE’s time on the team whether they have thought about moving to another team. Generally, the answer to that question is, “No. I love the job that I’m doing. The service that I’m building still has to mature. I like my team and the work. Go away and talk to me later.” This is fine and it introduces the concept in their mind and lets them know that it’s OK to consider moving at some point and that we value mobility. We approach the question again at 24 to 36 months and we start looking for things that would complement their current knowledge. For example, if Jane, a PE on the storage team, has been there for a long time, we might ask her if she’s ever thought of joining an in-memory cache team. The conversation goes something like this: “Hey, Jane,  you’ve  been  on  the  storage  team  for  a  while,  and  I’d  like  to  make  sure  you become  a  more  well-rounded  engineer.  Have  you  considered  moving  to  the  cache team? They need a senior engineer like you and you have a lot of experience scaling systems rapidly. Sure, it’s cache and not storage, but you should go talk to Joan about this and see what’s going on.” Generally, her answer is, “That sounds interesting. Let me go talk to her.” Or Jane will come back and say, “You know, I’ve got three or four more months of work that I want to do. Let me finish this project, and then I’ll con‐ sider a transfer to Joan’s team or do a hack-a-month.”  Production Engineering at Facebook      221   Hack-a-months are something that spun out of hack-a-thons when we realized that in all  of  our  engineering  teams,  we  needed  a  better  way  to  give  people  the  chance  to learn  something  new.  A  hack-a-month  serves  two  main  purposes.  One:  encourage any engineer who has been on the same project for more than a year to leave their team for a month to work on something completely different. Many use these for a little break from their normal routine. Two: to find a new team and figure out if they want to move. In either case, the team has to be able to handle the person being out, so the manager needs to ensure their staffing is at a good level or needs to work on finding someone else to take their place. To best evaluate folks in these hack-a-months, the two managers need to be in sync on performance during that period of time. In the case of someone learning some‐ thing  new,  there’s  usually  a  well-defined  project  that  has  an  end  state  that  can  be objectively measured. In the case of moving to a new team, we give people the room to ramp up into the new space and we take that into consideration in their perfor‐ mance evaluation. After 36 months on the team, we more directly talk to the engineer about moving to another team. We do this because I believe that when people get stuck in a rut, they can slow down the progress of their team. When a new PE  or SWE  joins the team and proposes a new idea, they may be shot down by the engineer who is comfortable with the way the system works. The established engineer might reject this new idea because it changes their mental model and it changes their comfort with the system. This  might  stifle  innovation.  We’ve  actually  experienced  this,  and  so  we’re  much more prescriptive about moving engineers to new teams when they have been in the same team for three or more years. As we expand into new geographies, engineers that are hitting this three-year mark should have enough mobility to move into other teams. We have quite a few senior engineers who have done this over and over again. And they have their own thoughts behind this, such as how managers influence them, how they influence themselves, how they talk to each other, whether it is easy to move, and how they deal with impostor syndrome if it sets in again. We encourage these engineers to share their stories candidly because it gives others insight into this pro‐ cess from a nonmanager’s perspective. If an engineer experiences impostor syndrome but knows their manager has their back, it makes it easier. If the organization built around them provides this mechanism to try something new and not worry too much about performance, then mobility becomes a more fluid process. David: What are the things required to be a successful production engineer?  222      Chapter 13: Production Engineering at Facebook   Pedro: I’ll try to list a few of the key traits that come to mind: A focus on getting stuff done  Production  engineers  need  to  have  a  bias  for  action.  When  we’re  looking  at  a problem, building a system, or creating a team, there has to be a tangible prob‐ lem we can work on and fix for the long term. There’s definitely a reactive por‐ tion to the role and it’s needed because systems break all the time. PEs need to be building sustainable solutions to these problems so there needs to be a focus on proactively addressing things. If we’re stuck in one mode of operation, turning the  crank  on  the  same  thing  over  and  over  again,  we’re  not  succeeding  in  the role.  Supportive communication and influence skills  You can’t be a jerk. Jerks generally do not interact well with people. Those of us that have been in the industry for a long time remember that the image of some‐ one in operations associated with the BOFH [Bastard Operator from Hell]. Deal‐ ing with outages and fixing things on a regular basis can wear people down and potentially  turn  them  into  angry  curmudgeons,  so  we  need  to  ensure  we  hire folks who understand when to be direct and still be civil in the way they talk to people. It really irks me when someone wants to show off their knowledge and shoot somebody down who might not have as much experience as they do. Pro‐ duction engineers need to be good at communicating to be successful in the role. The “no asshole” rule applies here, and when you find these folks, you should coach  them  to  communicate  differently,  and  if  they  don’t,  you  should  aggres‐ sively manage them out because their toxicity can easily permeate through the team.  Technical knowledge and skills  Production engineers not only need to speak the same language as software engi‐ neers, they also need to be able to speak the language of others. For example: net‐ work engineers, capacity engineers, data center engineers, and project managers. This means that they need to have knowledge about these different disciplines and  be  able  to  jump  into  any  problem  in  the  stack,  whether  it  be  a  hardware problem or a UI issue. For example, in order to find a problem in the rendering layer, PEs need to be able to look at the code and understand how it renders data. Production  engineers  don’t  necessarily  need  to  be  experts  at  all  of  these  disci‐ plines, but they can’t shy away from them. This is why our interview looks for a variety  of  technical  skills  and  we  also  train  engineers  over  time  to  gain  more knowledge. Finding the unicorn that knows everything isn’t a goal, so we’re care‐ ful about expecting deep understanding of everything.  Flexibility  PEs play different roles at different times and need to understand when certain skills will be used. For example, on some teams, PEs need to be the communica‐  Production Engineering at Facebook      223   tor or the liaison. In other instances, they might need to be the problem solver, the  debugger,  or  the  fire  fighter.  Sometimes,  they  may  need  to  code  a  critical component of a service. The role of a production engineer isn’t neatly defined in a  box,  and  this  is  by  design.  We  specifically  chose  to  have  a  broad  definition because we engage at different stages in the service life cycle that I described ear‐ lier. The team’s composition is a key factor in ensuring success in the role and we need to draw on each other’s strengths in different areas to solve the problem. We really stress not falling into the “not my job” mentality, and PEs need to be open to doing things that aren’t well defined in a job description.  Collaboration and compromise  Our model is that we work with the software teams, not for the software teams, and vice versa. On the one hand, we want SWEs to care about reliability, stability, and operations. On the other, we as PEs need to ensure we’re not always using the stability hammer and need to care about features and delivering new services. We all need to work together to do what’s right for the business, the service, and the team. Sometimes, that means that we’ll need to compromise on operational issues,  and  sometimes  that  means  SWE  will  need  to  compromise  on  features. One dimension shouldn’t be the one that wins all the time. If there isn’t compro‐ mise when we work together, then this could lead to unhealthy working relation‐ ships. The PE leadership often talks about this concept with PE teams because it’s easy to get stuck in one mode of operating without taking the time to self-reflect and make sure that we’re not actually the ones causing the rift between Dev and Ops.  Willingness to teach and not be a SPOF [Single Point of Failure]  None  of  the  roles  we  play  are  sustainable  in  the  long  term.  PEs  need  to  be focused on building software, establishing processes, and evolving over time so they can no longer be needed. PEs need to be careful to always be the one the team calls on to solve a specific type of problem or own a specific domain on their  own.  PEs  need  to  ensure  they’re  building  tools  that  will  allow  them  to replace themselves.  Production engineering kind of resembles a trade within the engineering discipline, and understanding how to operate systems isn’t something that’s taught in school or university. Since it’s something that you learn on the job and through experience, it means that we have to teach others how to do what we know how to do. I have found that in the beginning of their careers, few software engineers are constantly thinking about failure or the “what if?” scenarios and building systems that will be resilient to these failures. There is definitely a mindset that comes with being a successful PE, but I strongly believe this can be learned behavior. It’s our responsibility to infuse those around us with this mindset. The more practiced software engineers know how to do this well, but the reality is that we’re adding more and more new software engineers into our ranks and I believe PE helps them gain the knowledge of how to build more  224      Chapter 13: Production Engineering at Facebook   resilient  software  faster.  This  also  means  you  need  to  make  room  for  this  kind  of training and development and not just expect quantitative output from production engineers on the team. David: Can we talk a bit about how you train new production engineers? Facebook is famous for its onboarding Bootcamp; do production engineers go through a Boot‐ camp? Is there a PE curriculum that is part of Bootcamp? Pedro: How we gained a stronger position in Bootcamp is the part of the PE origin story that I didn’t cover earlier. When I got to Facebook, operations wasn’t allowed to join  the  existing  software  engineering  Bootcamp.  I  had  read  about  Bootcamp  and seen the videos before joining and was excited. However, when I joined, I was told: “You’re in operations, why would you ever want to commit code?” I was pretty irate. I spent the first year or so of my career at Facebook working hard to change this per‐ ception and prove folks wrong. I came from a computer science background, and I had written code my whole career. It felt unfair to be penalized because I chose to be in operations because I like to solve broad operational problems through software. The first few weeks of an engineer’s time at Facebook is where they get taught the fundamentals of operating as an engineer: how to commit code, learn about Face‐ book’s  code  quality  standards,  work  with  our  tools,  deploy  stuff  into  production, learn how to monitor it and add instrumentation, and more. This seemed like very relevant material that we also needed to understand, but since we weren’t allowed to attend Bootcamp, we initially had to build our own version. In parallel, some of us spent more time with the Bootcamp leaders and through many conversations and by hiring more people with stronger software backgrounds, we were eventually able to gain entry. Bootcamp has now been part of the onboarding process of all production engineers for a long time. We have influenced a lot of the classes taught to both software and production  engineers,  so  that  everyone  gets  some  fundamental  operational  knowl‐ edge about systems before they land on a team. We follow the same model for team selection that software engineering does here at Facebook. PEs spend three or four weeks getting fundamental technical knowledge in Bootcamp, and then spend two-plus weeks on team selection. We hold a career fair, for lack of a better term, where all the teams that are looking for people get together with all the Bootcampers that are looking for teams. We try to find a match based on the needs of the team and the skill set and desire of the individual. Some people have an affinity for security, for example, while others have an affinity for product-facing services or backend systems. For the most part, this works fine but this team selection process  doesn’t  always  work  for  everyone.  Some  people  we  hire  want  to  know  the team they’ll be working on before they join Facebook. So we spend a bit more time up front learning about them and then narrowing down the set of teams to make it a little easier.  Production Engineering at Facebook      225   Over time, we found that there’s still some type of work that doesn’t necessarily apply to every software engineer joining Facebook. So, we created another mini Bootcamp that we call PE Fundamentals. That curriculum is geared specifically toward produc‐ tion  engineers,  network  engineers,  and  other  operational-like  teams.  In  Bootcamp, we’re trying to pack in a lot of information into a short period of time. I think it’s overall very successful, but the content isn’t as meaningful until the engineer has lan‐ ded on their team and has spent a little more time understanding the infrastructure. About four weeks after PEs exit Bootcamp and have been on their team, they come back to a more hands-on set of classes that explains the nuances of our tools. Now armed with context of the system they’re working on, they can make better connec‐ tions in their mind about how our tools apply to their work. We also do more cultural onboarding in PE Fundamentals. For example, we cover how to apply some healthy tension without going overboard. When we’re talking to engineers about their solutions, we need to be careful about always being the nay‐ sayer  because  something  might  not  be  perfect.  We  can’t  always  say  “No.”  In  these onboarding presentations, other PEs share their war stories so new folks can build examples in their mind for how to deal with potential disagreements on their team. PEs learn about being part of a culture that enables others to solve their problems as opposed to being a blocker to change. David:  Let’s  go  to  the  wider  picture  around  production  engineering  as  a  model. We’ve talked about the uniqueness of the PE organization at Facebook. Do you think that a PE organization could be implemented outside Facebook and still feel like a PE organization to you? Pedro: Yes, I think a similar PE organization can be created elsewhere. I believe that Facebook’s secret sauce is actually the people we hire and how they get things done. There are a lot of cultural factors that might need to be there, but I’d be wrong to assume that we’re the only company that has a culture of autonomy, independence, empowerment, and healthy debate, among others. I do worry that other companies may adopt monikers and don’t adopt the cultural implications that come with them. I’ve talked with a lot of teams outside some of the bigger companies that will build an SRE team, and in reality, it’s just a rebranded SysAdmin team with a completely sepa‐ rate Ops role and no expectation of automation, collaboration, and equality. I believe it’s  because  they  want  to  be  able  to  attract  people  externally,  but  the  work  doesn’t necessarily  change.  The  engagement  model  with  the  software  engineers  doesn’t change. The ability to influence change isn’t different. The relationship at the leader‐ ship levels and the shared accountability doesn’t exist. I strongly believe that software teams should ultimately be accountable for their oper‐ ations, but I also recognize that PEs can help ensure this isn’t too hectic. I want to emphasize again that PEs should not be doing operations for SWEs, they should be  226      Chapter 13: Production Engineering at Facebook   doing  it  with  them.  If  this  construct  of  working  with  others  instead  of  for  others exists, then I think this model can work in other places. David: So how do you know whether an org is a PE org in the same way you define it? Pedro: I want to be clear that the way we’ve built and run our org isn’t the only way. Many companies are trying to figure out how to best run operations in their environ‐ ment and it’s great if they seek other models to reference—we should all be learning more from each other. However, this model isn’t one-size-fits-all. When companies are trying to build their organizations, they should pick and choose the things that work for them and the things that can apply to their environment. If they choose to implement  some  of  the  concepts  I’ve  talked  about,  modify  them,  and  make  them their own, that’s great. If, however, they try to follow some set of strict rules, I can almost guarantee that it will fail because everyone is different, every company is dif‐ ferent, and every infrastructure challenge is different. With  that  being  said,  if  I  had  to  quickly  summarize  the  ways  to  evaluate  the  PE model, here are the main things to consider:    Shared on-call with an embedded and collaborative model   Strong relationships with technical credibility   Balance between operations and features   Leadership accountable to delivering a feature-rich and stable system  Let’s break those down into a bit more detail. When I want to gauge how close another team is to our PE model, the first question I ask is: “Who is on call? When things go down, who is responding to a service out‐ age?” If  they  respond  that  it’s  SRE,  production  engineering,  DevOps,  or  whatever,  as opposed to software engineering  or a shared on-call rotation between SWE and the operational team , then I know they’re not building the PE model in the way we’ve defined it. In my opinion, the ultimate accountability for a service’s stability needs to fall  on  those  who  are  building  the  service.  If  the  primary  builders  are  SWEs,  then that’s who needs to be on call. Another  factor  I  look  for  in  determining  how  closely  someone’s  implementation resembles our model is related to equality and perception. In my experience, there are software engineering teams that look down on operational work and, conversely, operational teams that don’t respect pure software work. Internally, I use a frame‐ work that highlights that Different != Bad.  Production Engineering at Facebook      227   Often, software engineers spend their mental time on algorithms and features instead of thinking about operational complexity. This isn’t inherently bad, but it can lead to perception issues from operations. PEs, on the other hand, spend mental time and energy on other things that aren’t purely software. Focusing on availability, scalabil‐ ity,  operability,  failure  modes,  security,  deployments,  reliability,  and  monitoring instead of software is also not inherently bad. The mental time spent is just on differ‐ ent things. It’s everyone’s responsibility to ensure there’s shared context with respect to decisions and actions being taken by everyone on the team. I look for this type of shared understanding in my evaluation of whether someone else is implementing a similar model. My typical follow-up question is about prioritization of features over operational sta‐ bility. “When faced with prioritizing between features and operational stability, how often  does  operational  stability  lose?”  When  a  team  wants  to  build  a  service  and repeatedly chooses to overrule the stability work even though they understand that the  operational  debt  is  going  to  be  high,  then  the  team  is  not  succeeding  at  this model. This  leads  me  to  another  set  of  questions  related  to  when  the  operational  team  is engaged in discussions regarding the architecture of a system. If the software team views the PEs, SREs, etc. as equal contributors, then discussions related to architec‐ ture will happen with both groups of people in the room. If they aren’t seen as equal, and  the  operational  team  is  consulted  after  the  fact,  then  the  implementation  isn’t going so well. I don’t think that any system or team will ever be perfect, but diversity of thought needs to exist in the team to build something that’s going to be feature-rich and resil‐ ient under pressure. This last factor has a lot more to do with the relationship than it does  with  the  level  of  technical  knowledge.  If  the  relationship  is  adversarial  or  if there’s a large technical gap between the groups, the result will be a weaker system. If there’s trust between the groups, there’s shared technical understanding and the con‐ versations are constructive, then a better, stronger system will be built. When it comes to features versus stability, I think everybody should win a little and lose a little. The software team should sometimes deprioritize features to ensure sta‐ bility. The PE team should sometimes deprioritize some gains in stability to ensure features are being built. Stability shouldn’t regress, but sometimes it might be OK to hold the line. The software teams need to continue to innovate and PE’s role is to help enable this innovation by reducing the operational load and solve operational problems through software. PE also needs to simultaneously work with the software team to reduce operational complexity. If PE is 100% focused on stability and relia‐ bility, then the software teams may dismiss them and the critical work won’t happen. The  work  needs  to  be  balanced  over  a  longer  time  horizon.  For  example,  some months may be heavily skewed toward features and then the following months may  228      Chapter 13: Production Engineering at Facebook   be heavily skewed toward operational stability. As long as this is balanced, then the implementation is working fine. The last component is related to accountability. If the system isn’t feature-rich and stable, are the leaders in software and operations held accountable together? When it comes to promotions and performance assessments, are these held to equal standards or are they different? At Facebook, for example, when we are assessing the perfor‐ mance of senior leaders in SWE and PE, we talk about them in the same set of discus‐ sions.  Their  impact,  their  ability  to  execute,  work  cross-functionally,  and  to  build healthy organizations is held to the same standard. As I said earlier, I do think that everyone’s implementation might be slightly different and they should pick and choose the things that work well in their environment. I’ve talked to a few companies, and while they aren’t doing the exact same things we are, overall, I think they’re being successful in their implementation.  With over 20 years of experience in software design, architecture, and operating robust services at scale, Pedro Canahuati is the Vice President of Production Engineering and Security at Facebook. In this capacity, Canahuati is responsible for ensuring Facebook’s infrastructure is stable and that the data of its over two billion users is secure. Through‐ out  Canahuati’s  career,  he  has  built  and  managed  global  engineering  teams  with  a focus on operationally scaling companies to provide users with the best experience.  Production Engineering at Facebook      229    PART II Near Edge SRE    Chaos engineering.   Privacy engineering.   Database reliability engineering.   Durability engineering.   Machine learning and SRE.  This is your near future…discuss.   You Know You’re an SRE When…  …your personal blog can withstand multiple AWS region failures. …you and your spouse resolve arguments by checking the error budget. …you justify buying a Roomba on the basis of automating away routine operational work. ...you  read  accident  reports  and  books  about  human  factors  for  enjoyment  on  the beach.   CHAPTER 14 In the Beginning, There Was Chaos  Casey Rosenthal, Backplane.io  formerly Netflix   Services  go  down  and  people  have  a  bad  time.  Customers  who  rely  on  the  service become frustrated, other systems that rely on the service stop working, and the peo‐ ple responsible for the system are paged. History suggests1 that even the most celebra‐ ted  online  services  are  vulnerable  to  outages,  even  with  hundreds  and  sometimes thousands of people dedicated to their operation and uptime. As software inexorably increases in complexity,2 old methods of preventing errors and outages prove insuffi‐ cient. In the not-so-distant past, best practices around testing, code style, and process gave us confidence that the code that we wrote and deployed would do what we expected it  to  do.  We  believe  that  practices  like  rigorous  testing,  Test-Driven  Development  TDD , Agile feedback loops, pair programming, and many others can help reduce bugs in the long run. Practices like these are still very important, but they are not suf‐ ficient for engineering modern complex systems. New  best  practices  are  needed  to  give  us  confidence  again  in  the  systems  that  we build. Best practices are emerging to meet this need, and chaos engineering is among them. Chaos engineering is a new discipline pioneered at Netflix specifically designed to optimize for availability in complex, distributed systems. We can have our confi‐ dence, and engineer it, too.  1 Amazon AWS outage on 2 28 17; Google Gdoc outage on 11 15 17; Facebook outage on 10 11 17; Apple  iCloud outage on 1 26 18  2 The second Law of Software Evolution  https:  ieeexplore.ieee.org document 1456074   states that net com‐ plexity will increase in a manner similar to entropy, and combined with the Law of Requisite Variety  http:   requisitevariety.co.uk what-is-requisite-variety  , we can safely assume that complexity in software will increase as long as more software is written.  233   I ran the Chaos Team at Netflix for three years, during the period when we formal‐ ized chaos engineering, built a community around it, and brought that definition to the  industry  at  large.  This  chapter  introduces  the  class  of  problems  that  threaten availability in complex distributed systems. Then, we review the evolution of chaos engineering to illustrate how it evolved to meet that class of problems. We list five more  advanced  concepts  in  chaos  engineering  and  close  the  chapter  with  a  FAQ curated from audiences of many presentations and workshops on the topic. The Problem with Systems You can think of the engineering organization at Netflix as about a hundred small engineering  teams,  of  around  five  to  seven  people  each.  The  service—that  product that the customers use—is architected as many hundreds of microservices working together. Every microservice is owned by just one team, and that team is responsible for everything: features, roadmap, operations, and uptime for that microservice. An example of a microservice might be a customer microservice, which for a given customer ID serves up the metadata stored with that person. Another might be a per‐ sonalization service that stores information about preferences related to a customer so that when a customer looks for something to watch on Netflix, we decorate their experience with context based on what they previously watched, and so on. And, of course, proxies and an API layer and specific datastores could all be microservices. Imagine that one day a customer is watching Stranger Things on a train late at night. Let’s call this customer CLR. At a particular point in the show, a frightening scene surprises CLR, causing him to drop his laptop. He retrieves his laptop, and the show is no longer streaming, so he does what any reasonable customer would do and furi‐ ously refreshes his browser. It doesn’t load instantaneously, so he refreshes again… about 100 times. Now CLR is on a train and happens to be between cell towers, so he is currently par‐ titioned from the internet. Those requests are actually being queued up in the web browser  and  the  operating  system.  When  internet  connectivity  returns,  all  100 requests go through at once. What happens on the Netflix side? The requests come in to a proxy, advance to an API  layer,  and  fan  out  to  multiple  services  like  a  personalization  service,  which extracts a user ID and requests that user from the customer service. The customer service is large, so it spans a cluster of many nodes. It doesn’t make sense to store every customer’s data on every node, so instead a consistent hash of the ID  directs  requests  for  any  particular  customer  to  one  particular  primary  node. Because  teams  are  responsible  for  the  operations  of  their  microservices,  they  also ensure that the clusters autoscale to responsibly use resources, and hand off data in the  case  that  a  node  goes  down.  They  also  have  fallbacks  in  place  to  protect  from  234      Chapter 14: In the Beginning, There Was Chaos   degraded  performance  or  errors;  for  example,  the  customer  service  can  serve  data from an in-memory cache if it can’t get data from disk, and the personalization ser‐ vice can serve a default user experience if it can’t get a response from the customer service. Getting back to CLR’s case, 100 requests suddenly arrive simultaneously to the per‐ sonalization service, which issues a corresponding 100 requests to the customer ser‐ vice. All 100 are consistently hashed and forwarded to one node in the cluster. This node can’t fetch the data from disk in time, so it does the sensible thing and returns results from the in-memory cache. These results might be somewhat stale, but that should be acceptable. The autoscaling rules look at I O and CPU load and scale up the cluster if the average work  climbs  too  high.  Conversely,  they  scale  down  the  cluster  if  the  average  work falls  too  low.  In  this  case,  serving  from  in-memory  cache  is  much  less  work  than fetching from disk, so the average I O and CPU load for the customer service drops. The autoscaling rules do the responsible thing and scale down the cluster, terminat‐ ing the node that was doing the least amount of work, and shifting its data to other members of the cluster. The personalization service sees that the last request that it sent to the customer ser‐ vice did not complete in time  because that node was being shut down , and so it does the sensible thing and returns a fallback default user experience. This last request is returned to CLR, the responses from the previous 99 having been thrown out. CLR looks at the default experience and can’t understand why it looks so different from the personalized experience that he’s used to. It doesn’t even have the bookmark to the point in the show before he dropped his laptop. CLR does what any reasonable customer would do: he refreshes the browser another 100 times. The  cycle  repeats:  the  customer  service  receives  100  requests,  flips  to  serving  data from in-memory cache, triggers the autoscaling policy, scales down, and the person‐ alization  service  again  returns  a  default  fallback  experience.  Now,  other  customers who are assigned to those two terminated nodes are also seeing the fallback experi‐ ence, so they too begin refreshing their browsers. We now have a user-induced retry storm. The additional traffic puts more pressure on the customer service, flipping the entire thing into serving from in-memory cache, dramatically reducing the average load, autoscaling the cluster down dramatically, until the remaining nodes can’t serve their function with stale data, the personalization service stalls, and the entire service collapses, bringing streaming video screeching to a halt. The above is entirely hypothetical and did not happen. But something like this could happen. So what went wrong? The important lesson to learn from an example such as this one is that no human made an incorrect decision. The engineers were smart, and they followed reasonable  The Problem with Systems      235   industry best practices. They didn’t make a mistake, and yet the holistic behavior of the system produced an undesirable result. By definition, a complex system is one in which no part can understand the whole. A complex system is unreasonable: you cannot reason about it. This is one of the rea‐ sons why Netflix doesn’t have a chief architect or similar role: no human  a compo‐ nent  in  the  system   can  hold  a  model  of  all  of  the  moving  parts  in  their  head simultaneously and thus have any predictive power over the behavior of the system. How,  then,  do  we  prevent  or  minimize  the  undesirable  behaviors  of  the  system? There are two logical options: reduce the complexity so as to make it not a complex system, or find another way to steer the system even without understanding how it works in intricate detail. We call this latter option navigating complexity. I don’t know of any practical, generalized theory for making a complex system not complex. Because many of the features we want in a system—feature velocity, perfor‐ mance, availability, and so on—necessarily introduce complexity, it isn’t clear to me that it’s even possible to make a system uncomplex without necessarily also making it worse. Instead of trying to reduce the complexity, we can then focus on the other avenue: learning to navigate the complexity. Economic Pillars of Complexity Kent Beck, the creator of Extreme Programming and proponent of TDD, introduced me to a model that I call the Economic Pillars of Complexity  EPC . The model iden‐ tifies  four  pillars  of  complexity  that  can  confound  progress  when  you’re  making  a product or delivering a service. We can think of the product or service as a system. Here are the four pillars: States  The number and nature of configurations that the system can be in.  The  number  and  nature  of  the  ways  in  which  parts  of  the  system,  including human operators, can interact.  Uncertainty introduced by the environment external to the system.  The degree to which a change to the system can be easily undone.  All or none of these forms of complexity can exist in your system, but if you find that business goals are being overwhelmed by rampant complexity, it might be possible to cap one of the pillars and focus on the others where it might be more manageable.  Relationships  Environment  Irreversibility  236      Chapter 14: In the Beginning, There Was Chaos   Ford is the classic example of tackling complexity in this way, limiting the states of the system by offering the Model T in only one color  black  for some years, and lim‐ iting relationships through assembly-line manufacturing. In software systems, we can seldom limit the states. In modern systems like microser‐ vice architectures, we also don’t have much control over the nature of relationships between microservices. Irreversibility is an interesting pillar to think about, though. If we  can  make  decisions,  try  things,  and  roll  them  back  quickly  enough,  we  can respond to the unreasonable nature of a complex system before unwanted systemic effects take hold or cause too much damage. Microservices help with this by decoupling deployments and allowing small parts to change and roll back asynchronously. Some development practices like XP and Agile also optimize for reversibility, putting regular checkpoints into the development pro‐ cess so that ideas can be tested and rolled back quickly and easily. Immutable archi‐ tectures,  Continuous  Delivery,  and,  arguably,  cloud  deployment  in  general  all  cap this pillar of complexity, allowing us to focus on navigating the other three pillars. For the majority of software teams, capping one of the pillars isn’t an option. The decisions that could possibly constrain states, relationships, environment, or irrever‐ sibility  have  already  been  made  or  are  thwarted  by  business  goals  and  objectives. Other methods are required to confront the complexity. Beginning Chaos When I arrived at Netflix in early 2015, Chaos Monkey had already been around for almost five years. When Netflix moved from data centers to the cloud in 2010 and 2011, the scaling profile changed from vertical to horizontal. On the cloud, services ran on many more but smaller instances. As a result of the higher number of instan‐ ces, it became much more likely that in any given hour an instance would catastroph‐ ically fail or blink out of existence. Disappearing  instances  impact  availability  if  they  are  Single  Points  of  Failure  SPOFs , or if the sudden change in traffic profile causes a cascading failure. There are many easily accessible best practices that we can implement to prevent this. At Netflix, there is no CTO or, as I mentioned earlier, chief architect to pick best practi‐ ces and issue an edict that all microservices must conform. The best practice solu‐ tions were at hand, but there was no efficient mechanism of delivery. Instead of dictating best practices, the engineers at Netflix decided to take the pain that  they  wanted  to  solve—instances  disappearing—and  bring  it  to  the  forefront. Chaos  Monkey  is  the  result.  Chaos  Monkey  pseudo-randomly  selects  about  one instance  of  each  service  per  day,  and  rudely  turns  it  off,  but  only  during  business hours.  Beginning Chaos      237   Suddenly many engineers couldn’t work on what they had planned in their roadmap. Chaos  Monkey  had  created  a  problem  for  them  and  put  it  right  in  front  of  their queue. Fortunately, engineers are great at solving problems right in front of them. All of those separate, small engineering teams were now all aligned to make their services resilient to instances disappearing. And it worked: in four years, Netflix has had many, many instances disappear, but only one outage related to a SPOF. The vast majority of microservices are now resil‐ ient to that event. In that one case where we did have an outage, it was Chaos Monkey that terminated the  instance.  Fortunately,  the  service  had  just  been  deployed  and  the  engineers responsible were still at the office, so context was readily at hand to resolve the outage quickly. Compare that to the situation if we didn’t have Chaos Monkey; that instance could become unstable months or years after deployment and none of the context would be at hand to identify and fix the issue in a timely fashion. Navigating Complexity for Safety The human factors expert Jens Rasmussen proposed a model that describes how sys‐ tems evolve over time away from boundaries that are visible to the people doing the work. The three boundaries are Economics, Workload, and Safety. These boundaries are crossed in failure. If something becomes too expensive, for example, it crosses the Economics  boundary  and  then  maybe  the  company  goes  out  of  business.  If  some‐ thing becomes too much work, the Workload boundary is crossed and nothing else gets done and everyone has a bad time, and so on. In most software engineering situations, it’s not difficult to model the team’s budget or get an idea of how expensive the resources are to run an application—the overall cost captured in Economics. Likewise, with Workload, in most cases engineers will have a good intuition about how many people work on developing and maintaining the project that they work on, how many hours they put in, and how difficult it is to make  progress—these  are  all  signals.  Cost  overruns,  coworkers  complaining  about how many hours they put in, expectations around urgency—these are also signals. These signals provide an awareness of the boundary. One of the roles of management is to reinforce this and provide a strong signal to a team or organization if it drifts too close to one of the boundaries. “Hey, we are run‐ ning out of money and have only two months left of runway to get this feature built,” is  a  great  example  of  a  strong  signal  for  the  Economics  boundary.  Most  software projects have strong signals for Economics and Workload boundaries. The  same  cannot  be  said  about  Safety.  Software  engineers  don’t  know  how  fragile their system is, right up until the moment that it unexpectedly breaks. In most cases, there is no signal for Safety. As a result, software projects are naturally motivated to  238      Chapter 14: In the Beginning, There Was Chaos   make their system less expensive and easier to operate. They drift away from the Eco‐ nomics and Workload boundary, toward the Safety boundary. Over time, a system in this situation drifts to being cheaper and doing more work and, unbeknownst to the operators, less safe. The beauty of Chaos Monkey is that it creates that signal of Safety. If the boundary for safety is crossed when an instance disappears, you will quickly discover that when it drops an instance during business hours. The repercussions of being vulnerable to an instance disappearing—whatever those repercussions may be—are exposed when that condition is forced. This is a strong signal to the service owners of just how safe they really are. As a result, the microservices at Netflix stay resilient to this type of safety issue, preventing the system from gradually drifting into a less safe arrange‐ ment. Chaos Goes Big Building on the success of Chaos Monkey, Netflix decided to go big. Instead of just terminating instances, we turn off an entire region. They call this Chaos Kong. In case you are unfamiliar with cloud terminology, a region can be thought of as a major data center. The control plane for Netflix is deployed in three geographically distributed regions. This handles traffic for all of the devices that interact with the streaming service. The Traffic team at Netflix has built orchestration that allows us to detect a severe outage in one region and move all of those customers over to the other two regions. Because control plane traffic alone accounts for more than 3% of the bits on the internet in North America, this is quite a large shift. Chaos Kong is exercised regularly to verify the orchestration that the Traffic team has built. More important, it verifies that all of the microservices can survive a regional outage. It generates a Safety signal for the microservice owners, to prevent the system from drifting into a less safe configuration. Formalization By the end of 2015, Chaos Monkey and Chaos Kong were well-understood fixtures at Netflix. These two programs generated pretty solid Safety signals for small-scale dis‐ ruptions   instances  disappearing   and  very-large-scale  disruptions   regions  disap‐ pearing . Not much existed to generate a Safety signal for the systemic effects: all of the interesting interactions between microservices that lead to unforeseeable effects, both good and bad. What would such a system look like? I was given a small budget  two headcount  to build a Chaos team. I went around the company and asked, “What is chaos engineer‐ ing?” The most common answer was some form of, “That’s when we break things in  Chaos Goes Big      239   production.” I asked around the industry and got the same reply. The problem with that answer is that it’s not a definition. Many things can break in production and pro‐ vide no value to business. In fact, breaking things in production from within a system is easy to do, but I suspect the Chaos team would get no appreciation within the com‐ pany if that was their only goal. I  sat  down  with  the  nascent  Chaos  team  to  formalize  the  practice.  The  result  is  a manifesto of sorts available at PrinciplesofChaos.org. We defined an empirical prac‐ tice  that  has  clear  goals,  boundaries,  and  best  practices.  This  allows  us  to  generate buy-in, plan, and evaluate. We now know whether we are doing chaos engineering or not, how well, and to what end. Western science is predicated on an empirical process of falsifiability. We build con‐ fidence in an explanation for some observed phenomena by proving that alternative explanations are incorrect. Chaos engineering borrows heavily from this. A definition from the manifesto captures the essence: “The facilitation of experiments to uncover systemic weaknesses.” The four steps of building an experiment are as follows:  1. Start by defining “steady state” as some measurable output of a system that indi‐  cates normal behavior.  experimental group.  2. Hypothesize that this steady state will continue in both the control group and the  3. Introduce  variables  that  reflect  real-world  events  like  servers  that  crash,  hard  drives that malfunction, network connections that are severed, and so on.  4. Try to disprove the hypothesis by looking for a difference in steady state between  the control group and the experimental group.3  This template for building experiments can be applied to any instantiation of chaos engineering  in  practice.  For  Chaos  Monkey  and  Chaos  Kong,  Netflix  looks  at  the number of videos streaming as the steady state given that this number is fairly well defined and well understood. Deviations in that number between a control group and an experimental group are easy to spot. Exploring the solution space of things that could disrupt the system’s steady state will never  be  exhaustive.  There  are  an  infinite  number  of  things  that  could  go  wrong. Because  certainty  isn’t  possible,  we  settle  for  confidence.  A  successful  program  of experimentation increases confidence. The more experiments that run without dis‐ proving our hypothesis, the more confidence we have in our system’s stability.  3 Quote from principlesofchaos.org.  240      Chapter 14: In the Beginning, There Was Chaos   There  are  many  technical  considerations  to  address  when  you  are  implementing  a chaos engineering program, but most of those are going to be specific to the system of interest. The template of experimentation should be universal. Advanced Principles We  can  use  the  template  of  experimentation  to  know  whether  we  are  performing chaos engineering. We also defined some advanced principles to know whether we are doing it well. Here are those advanced principles: Build a hypothesis around steady-state behavior  As engineers, we often have a proclivity to dig into a problem. We want to figure out  how  something  works.  But  chaos  engineering  isn’t  about  how  something works. That’s model validation. We want model verification. We already know that a complex system can’t fit inside any individual’s head. So instead of figuring out how something works, we want to keep the focus on whether it works. Focus‐ ing on steady-state behavior helps keep us closer to “whether,” which helps us get the most value out of chaos engineering.  Vary real-world events  The variables in the experiments should reflect the plausible range of conditions as accurately as can be conceived. There is an art to this, because it does rely on some  historical  knowledge  and  hence  expertise.  This  principle  provides  the opportunity to reflect on the risks to stability and prioritize accordingly.  Run experiments in production  Keep in mind that the point of chaos engineering is to generate new knowledge. If you know  or suspect  that running an experiment will disprove the hypothe‐ sis, don’t run it. Fix the problem first. Make your system resilient to the variable you want to verify. After you think you have a sound hypothesis, run the experi‐ ment to build confidence. Staged environments can never be identical to produc‐ tion environments. Run experiments in production for the best, most accurate results.  Automate experiments to run continuously  Complex systems are dynamic. The confidence that you should have in a system decays with the time since the last experiment was run. To create the safety signal that you desire, you need to run experiments continuously, not just on occasion or during game-day events.  Minimize blast radius  As the sophistication of chaos engineering tools increases, it opens the possibility of making experiments more precise. Precision becomes a cyclical positive rein‐ forcement for more experiments, covering more of the search space. Especially when  you’re  running  experiments  in  a  production  environment,  that  doesn’t  Advanced Principles      241   mean you necessarily have to expose all of production to the same variable. Scop‐ ing segments of production traffic to specific variables allows you to run more experiments concurrently and makes the discovery of a hazard safer.  Frequently Asked Questions Outside  of  technical  considerations,  there  are  often  many  techno-social  questions around chaos engineering. Here are the most common questions I have received in Q&A sessions and online forums, with general answers. Q: How do I get buy-in from management to implement a chaos program? A: The best way to get management to buy in to the value of chaos engineering is, as  Churchill said, “Never let a good crisis go to waste.” The safety signal is missing in most software organizations. In a complex system, you can’t know how close you are to a failure until just after you experience it. This can cause buy-in issues because the ROI of an investment in chaos engineering isn’t obvious until after the system drifts so far into an unsafe condition that it crosses the line into a failure. This was the case even at Netflix. Take advantage of the insight gained from an incident and use that to make a case for chaos engineering, which can prevent future failures by revealing the safety signal early. Q: How do I get Chaos Engineering started at my organization? A: This is highly dependent on the technical maturity of the system and the organiza‐ tional support that chaos engineering has. Nora Jones has spoken at length on some of the options available to people seeking to introduce chaos engineering into their organization. Q: Does chaos engineering apply in my industry? A: Early on, we would hear comments from the fintech industry along the lines of, “Sure, chaos engineering is fine at Netflix, but I have actually money on the line. We can’t experiment in production in fintech.” This statement hides a supposition that the likelihood and scope of a failure in the future is known. If the software in ques‐ tion  is  a  complex  system,  that  supposition  cannot  be  true.  To  put  it  another  way: Would you rather discover chaos in your system in a controlled manner or do you instead want to wait until a failure surprises you? Chaos engineering is now a com‐ mon practice at many fintech companies as well as large banks and financial asset institutions.  More  recently,  we  would  hear  comments  from  the  medical  industry along  the  lines  of,  “Sure,  chaos  engineering  is  fine  at  Netflix  or  banks,  but  I  have actual lives on the line. We can’t experiment in production in medicine.” The clinical trial is often held up as the pinnacle of Western science. We remind people working in  medicine  that  clinical  trials  are  basically  chaos  experiments  running  in production—with human lives on the line. The type of experimentation that inspires  242      Chapter 14: In the Beginning, There Was Chaos   chaos engineering was in fact pioneered in medicine. That doesn’t suggest that we can be careless when lives are on the line; rather, it should stand as a testament to the approach. Q: Do I really have to run experiments in production? A: If you aren’t, you are building confidence in a system that isn’t yours. The bound‐ ary that we draw around a system is arbitrary. If you experiment in a staged environ‐ ment  rather  than  a  production  environment,  you  might  still  learn  useful  things; however, by definition you can’t know all of the components that go into a complex system that could lead to undesirable behavior. This means that the boundary you draw around the staged environment cannot be an exact replica of production, no matter how much time you spend certifying the similarities. There is always the pos‐ sibility that the production environment will exhibit a behavior not seen in the staged environment. Running experiments in production is more accurate, and accuracy is crucial when you’re building confidence. Conclusion Reliability is created by people: the engineers who write the functionality, those who operate and maintain the system, and even the management that allocates resources toward it. SREs have a special role in creating that reliability, bringing to bear best practices and focused attention to this property of the system. Tools can help. Chaos engineering is another tool that SREs and dedicated chaos engineering practitioners can use to create reliability. Chaos  engineering  is  not  about  creating  chaos;  rather,  it  is  about  exposing  chaos inherent in the system. With a thoughtful discipline structured in basic principles of Western empiricism, chaos engineering can teach us about the complex systems that we build and manage. This discipline becomes increasingly relevant as more of soft‐ ware engineering moves into the realm of complex systems. As practitioners in this industry, our success relies not on removing the complexity from our systems, but on learning to live with it, navigate it, and optimize for other business-critical properties despite the underlying complexity.  Conclusion      243   Casey Rosenthal is the CTO of Backplane.io, providing availability and security in the form of a cloud-hosted traffic management service. Prior to Backplane.io, he managed both the Chaos Engineering team and the Traffic Engineering team simultaneously at Netflix. Casey Rosenthal co-wrote the book on chaos engineering.  244      Chapter 14: In the Beginning, There Was Chaos   CHAPTER 15 The Intersection of Reliability and Privacy  Betsy Beyer and Amber Yust, Google  Privacy  engineering  is  a  young  field  in  which  industry  players remain cautious with their public discussion. We hope that the rel‐ atively abstract concepts and approaches discussed in this chapter will spark ideas of more concrete privacy opportunities within your own  organization  and  build  an  environment  in  which  conversa‐ tions around privacy innovation can thrive.  With the recent publication of Google’s SRE book as well as a fair number of other publications and conferences about SRE, DevOps, and related movements, there’s a fairly  active  conversation  about  reliability  engineering  across  the  industry.  As  an inherently more sensitive topic, privacy engineering is less openly discussed, and, as a result, less well understood. Although many companies and organizations are begin‐ ning to think about many of the correct and important aspects of privacy and to con‐ sider privacy as an engineering discipline, this field is much less robust than SRE. Yet at the same time, privacy is a critically important concern for virtually every company or organization that handles private data, given that mishandling private data typi‐ cally can’t be undone. After scanning the contents of this volume, you might be asking yourself, “Why does a book about SRE dedicate an entire chapter to privacy?” Sure, any organization that actually cares about its users should invest energy in both reliability and privacy. But beyond that, why is privacy so relevant to SRE? As anyone working on reliability knows, SRE doesn’t exist in a vacuum. There are many concerns that arise when performing SRE work  e.g., reliability, cost, efficiency, scalability, and security ; privacy is one of these related concerns, and arguably one of the most important. Although privacy engineering, security engineering, and SRE are related disciplines, privacy engineering is a distinct field position because it requires  245   distinct  cultural  knowledge.  Privacy  engineers  bridge  social  and  technical  work  by maintaining all of the real-world context and perspective to understand what data is sensitive  to  whom  and  under  what  conditions.  This  chapter  will  help  you  build  a robust privacy engineering posture in your organization based on SRE-style princi‐ ples, regardless of your background. The Intersection of Reliability and Privacy Why,  as  someone  in  the  field  of  reliability  engineering,  do  you  also  need  to  think about privacy engineering, and why are you well positioned to do so? The starting point for both reliability and privacy is the same: to deeply understand your  systems  before  you  can  begin  to  reason  about  what  reliability  and  privacy should look like in a specific environment. From the opposite end, privacy and relia‐ bility resemble each other because they share an end goal: satisfying user expectations. Both disciplines examine their problem space through the lens of what users expect. Users expect a company or organization’s products to work, and to work most of the time;  they  also  expect  you  to  respect  their  privacy—including  data—appropriately. Both reliability engineering and privacy engineering boil down to the ultimate goal of ensuring user trust. Both ask, “Is the system working in a way that makes sense to the user, or will the user be surprised because the system doesn’t behave as expected?” One way to look at user expectations is to consider surprise a failure mode of a sys‐ tem: if a user can’t trust a system with their data, that system might as well be down. Reliability also intersects the privacy realm on a structural level: privacy is protected by actual technical and administrative operations. These systems need to work relia‐ bly to fulfill their mission of protecting user privacy. In terms of the software devel‐ opment  life  cycle,  both  privacy  and  reliability  concerns  are  also  analogous. Operations teams long ago realized that the earlier they are involved in the pipeline, the better the end result. The same principle holds true for privacy engineering. As experts in reliability, SREs are already concerned with meeting user expectations when it comes to providing a reliable system or project. But when it comes to the people using your product, some of their strongest expectations  whether they realize it or not  involve privacy. Privacy issues have come to the forefront in current events and have risen in public awareness in recent years. More than ever, users expect more from service providers when it comes to privacy. Because privacy is so fundamentally tied up with user expectations and trust, there’s a high demand for providers to do what they say with user data. The people tasked with safeguarding user expectations —whether  that  be  an  engineer,  technical  project  manager,  or  program  manager explicitly working on privacy engineering, or a counterpart in SRE who’s positioned to do so—will be the guardians of this realm. These teams are responsible for meeting users’ implicit and explicit expectations, fundamentally creating a reliable user expe‐ rience  UX .  246      Chapter 15: The Intersection of Reliability and Privacy   The  good  news  is  that  there  are  people  already  doing  work  in  the  area  of  privacy engineering,  and   as  discussed  in  the  section  “Privacy  and  SRE:  Common Approaches” later in this chapter  what they’ve learned can help you, as an SRE, to begin thinking about and approaching privacy in a productive way. Even better: if you’re just beginning to think about how to  better  engineer privacy into your sys‐ tem or service, you don’t need to start from scratch. SREs are already well equipped to create privacy value because SRE techniques are useful in the privacy space. The General Landscape of Privacy Engineering A privacy engineer’s goal is to go above and beyond compliance to try to make good products.  Privacy  engineering  is  not  solely  about  checking  boxes  to  achieve  legal compliance. Rather, it is about developing creative solutions to achieve products that people trust, often according to extremely challenging technical, administrative, and legal requirements. There is no single checklist that answers, “Is this privacy, or not?” As a complex disci‐ pline, privacy engineering is characterized by a certain amount of subjectivity: differ‐ ent people  users from all different walks of life, product visionaries, governments  might have quite different desires about privacy-related matters. This is one of the reasons we tend to think about “user respect”—although user respect isn’t an explicit mental model, it gets people asking the right questions: “Having seen and read the product’s notices and policies, would I, as a user, feel that this product works cor‐ rectly? What about another user who’s not like me?” In its ideal form, privacy engi‐ neering should pursue intentional diversity, incorporating a range of life experiences, demographics, and personal philosophies. To create really good products, you must incorporate as many perspectives as possible and make sure you don’t miss some‐ thing that would be obvious from a different frame of reference  see the comments by Amber Yust in the Buzzfeed article, “You’ve Never Heard of this Team at Google— But They’re Thinking of You” . Google engineer Lea Kissner argues in her G+ post “Privacy, Security, and Paranoia” that privacy engineers “stand between our users and the dark places of the internet.” Privacy work tends to fall into three main categories  we’ll leave security engineering out of scope for this chapter—this is a substantial topic that deserves its own thor‐ ough treatment : Guard  Find  and  solve  potential  privacy  problems  for  products.  Much  of  this  work involves  cultural  mindset-related  steps  in  addition  to  consulting  and  teaching good privacy practices to other teams, from product teams to business analysts to support teams.  The General Landscape of Privacy Engineering      247   Strengthen  Extinguish  Make it easy for all teams developing products to “do the right thing.” These are the  technological  infrastructure  steps  that  follow  the  cultural  mindset-related steps that we looked at earlier. To this end, privacy engineers design and build infrastructure,  work  with  teams  to  improve  existing  systems,  build  privacy- related product features, and develop and provide shared libraries for easy imple‐ mentation of privacy concepts.  When a fire does arise, privacy engineers put it out. They learn from these events by finding ways to generalize solutions or avoid problems in the future—not just for one team, but for many. Note that postmortem culture is just as useful for privacy as for SRE, although it’s often subject to wariness from legal departments due to the sometimes-sensitive nature of the fires.  Privacy engineers tend to think about the products and services they protect in a very specific way. When you ask a privacy engineer to evaluate a product or service, that engineer will be thinking about the following questions:    What data is involved?   Where is the data stored?   How is the data used?   What are the potential implications of having this data available?   What are the user’s expectations?   Who has access to the data, and how?  Note  that  this  list  is  somewhat  aspirational—these  are  nuanced  questions  that  you might not be able to answer straightaway. At the same time, this list is incomplete: a good privacy engineer considers the nuances of how a product works and its real- world implications, not just the raw flow of data. Privacy engineers drill into aspects of a system or user behavior that aren’t obvious to the untrained eye. Even though they necessarily must keep state around complexity, there’s also a skill set around practicing empathy and how to factor that practice into engineering work. For example, when debugging or remediating a bug or incident, a privacy engineer thinks not only about user impact, but also about the specific inten‐ tions of users—their human motivations, desires, and goals. Furthermore, they don’t just think about “typical” users, but consider the many different user audiences and their diverse assumptions and expectations around product behavior and privacy. For example: for cases in which a system bug has resulted in unintended behavior, a fix generally consists of two parts: ensuring the specific cause bug is fixed  “stopping the bleeding” , and then attempting to restore affected users to the intended happy  248      Chapter 15: The Intersection of Reliability and Privacy   state  “cleaning up the mess” . Privacy engineers are especially useful for that second step because what the “intended state” is for a particular user is often a complex ques‐ tion to answer  consider: does a user mashing a button really want to perform the action repeatedly? . Having a clear record of explicit user interactions makes it easier to work backward to what results the user expected versus what they got. This can make  a  huge  difference  in  how  long  it  takes  to  restore  expected  system  behavior. With  this  information,  an  engineer  can  analyze  and  replay  inputs  to  return  to  a known-good state, even if the outputs were corrupted. This ability can be particularly crucial when the state the bug affected is privacy-critical, such as privacy preference data or access control lists  ACLs . Privacy engineers try to envision these scenarios ahead of time and ensure that this safety net infrastructure is built in from the ground up. Privacy and SRE: Common Approaches Given that reliability engineering and privacy engineering share the goal of ensuring user trust—a goal that requires big-picture thinking about worst-case scenarios—it’s no surprise that both tend to attract people with similar mindsets and outlooks. In both disciplines, the ability to “see broken things” is a key aspect of a good engineer. Although  they  have  different  foci   availability  versus  respect ,  both  good  privacy engineers and reliability engineers look at a system and see how it breaks, not how it succeeds. Many of the lessons SREs have learned over time also apply to privacy engi‐ neering. Reducing Toil One key element that elevates SRE from straightforward operational work to a proper engineering discipline is its focus on reducing human time spent on toil. The same goal  can  be  applied  to  privacy  engineering:  frameworks  and  careful  selection  of defaults are two opportunities to reduce human toil.  Automation You  might  not  think  that  automation—a  tried-and-true  core  concept  of  SRE— applies  to  privacy  engineering  in  an  immediate  and  obvious  way.  Privacy-related matters are judgment calls and human decisions, which means that they can’t just be automated away, right? Actually, automation can be helpful in privacy engineering. Automation often entails writing a script, program, or service that programmatically eliminates  some  aspect  of  human  toil.  To  apply  this  model  to  privacy,  you  might write a script or simple program that checks to make sure auditing settings match up rather  than  requiring  a  human  to  perform  manual  verification.  A  simple  example: checking that only a specifically designated set of storage buckets are world-readable,  Privacy and SRE: Common Approaches      249   and all others are not. A more complex example: enforcing mutual exclusion between access to two datasets, if policy has determined they should never be cross-joined.  Default behavior for shared architectures Automation  makes  things  simpler  for  humans,  reducing  the  amount  of  effort required by humans and freeing up human time for other tasks. We can effectively “automate”  many  privacy  improvements  by  building  systems  that  “do  the  right thing”  by  default.  In  other  words,  “Make  correct  easy.”  Specifically,  we  can  imple‐ ment system defaults that handle a great deal of decision making, drastically reducing how  often  engineers  building  a  product  need  to  consciously  make  a  decision  for which an improper choice could lead to an undesirable privacy outcome. Instead  of  requiring  the  developers  building  your  products  to  repeatedly  face  the same  questions,  and  make  the  same  decisions,  sound  privacy  engineering  should have  them  enumerate  and  contemplate  these  decision  points  in  advance.  If,  for  a given situation, there’s a correct or safe choice that applies to 80% of situations, make that  choice  your  system  default.  Doing  so  relieves  many  people  of  that  decision- making burden 80% of the time. For example, a shared library, a schema, or a data access  layer  might  be  good  places  to  consider  implementing  defaults.  As  a  result, you’ll no longer need to spend human time dealing with decisions that could have been made in advance. Instead, you can then spend your human time on decisions that are actually difficult. By focusing your time, you can dig deeper into the hard problems to find better  and more repeatable  solutions. Note that one of the creative challenges privacy engineers face is when the common choice and the safest choice are not the same. It might not always be possible to adopt the  safest  choice  as  the  default,  which  means  downstream  developers  will  want  to carefully ensure that their usage matches their intent. Make sure these cases are well documented so that other developers know they exist.  Frameworks Reliability  and  privacy  concerns  span  multiple  products  and  services.  As  they encounter new systems, engineers tasked with reliability and or privacy must either find ways to understand a large variety of systems, or ways to standardize systems so that they don’t need to reunderstand them from scratch each time. Frameworks bake in  reliability  and  privacy  best  practices  in  an  efficient  and  scalable  way.  Factoring both aspects into system design also means that you don’t need to invest the energy and resources to retrofit a product to meet reliability and privacy standards. So  how  might  you  practically  apply  the  concept  of  frameworks  to  privacy  in  your organization? The following examples can get you started thinking about potential approaches:  250      Chapter 15: The Intersection of Reliability and Privacy     How you handle access control should be one of the most important properties of your system. Establishing a framework for handling ACLs will ensure that all  new   systems  can  easily  and  consistently  apply  your  recommended  best practices.    Deletion of user data is another canonical concern of privacy engineering. Hav‐ ing a consistent, organized system to propagate deletions throughout your sys‐ tem  including caches, syndication to third-party sites, etc.  helps ensure that you don’t leave data orphaned.  The industry still has some work to do when it comes to frameworks. To provide just one specific example, it seems that few startups currently use Role-Based Access Con‐ trol  RBAC , which is a basic and widely accepted tenet of good privacy engineering.1 Surveying the industry for any kind of standard frameworks  for example, baking in the principle of least privilege for free when turning up new products or services  also turns up few results. Efficient and Deliberate Problem Solving As it has evolved, SRE has worked out many of the kinks that lead to inefficient, dis‐ jointed troubleshooting and problem solving. Privacy engineering can embrace these aspects  of  SRE  culture  without  having  to  experience  the  same   sometimes  painful  journey. Here are just a couple of examples of how SRE best practices in this area can also directly apply to privacy engineering.  Solve challenges once When you solve a problem, prevent other people from needing to reinvent the wheel by publicizing your solution. Widely communicate what you did to investigate and solve the problem, the decisions you made, why you made these decisions, the results of your decisions, and when and why others should also adopt this solution. Be sure to document the scope of what you’ve solved in terms of constraints and context. For example,  when  you  address  a  privacy  concern  for  the  United  States,  that  solution might not apply when you expand to the EU. For example, in the privacy space, you might do the following:    Build a system to create the necessary audit trails for user consent screens and  then reuse it.  1 Also referred to as scoped access: narrowing the scope of access granted  both to humans and to production roles  not only helps protect privacy, it also reduces the potential impact of a security breach or production accident. This concept is sometimes alluded to in the general sysadmin advice of “don’t run everything as root,” but here is taken further and structured.  Privacy and SRE: Common Approaches      251     Build a differentially private experiment system and then reuse it.2   Perform UX studies to determine a clear and concise way of describing the pri‐ vacy implications of a feature. Then, push to have that language used across all products  with  that  type  of  feature.   Note  that  accessibility  features  intersect  in interesting ways here. 3  Find and address root causes Merely fixing symptoms means that the same issue is likely to recur in the future. Step back, take a look at the bigger picture, and invest in the extra levels of investiga‐ tion to determine the actual cause of the problem and fix it at its source. As discussed in relation to postmortems,4 an investigation that assigns blame to people is counter‐ productive. Instead, fix the technical or process factor underlying the issue. In the privacy space, you might apply this principle in the following ways:    If a bug results in a data leak, don’t just fix the bug. Sometimes you might end up revising your documentation, safeguards, or tests; sometimes you might deter‐ mine that something in a library or framework makes it difficult to do the right thing and therefore needs to be revised.    If you find the ACL on a storage directory to be overly broad, don’t just fix that particular ACL. Find the tool that sets up the directories and change its default ACLs to be narrower.    Create  memorable  ways  to  emphasize  privacy  concerns  to  other  job  functions early on in the design phase of a project. For example, you might have designs for sharing flows that consistently enumerate which user interface elements indicate each of “who-what-where”  who is sharing, what are they sharing, where are they sharing it . Eventually “who-what-where” will become a mantra.  Relationship Management Although concepts like automation and root causing might be obvious wins in the reliability and privacy environments,  as previously mentioned  neither privacy engi‐ neering nor SRE exist in a vacuum—both organizations work in a larger engineering and product ecosystem with multiple other players, each with its own priorities and  2 For an example of a system that’s similar in spirit, see the code for Rappor, a privacy reporting system. 3 Good examples of this standardization include app permission prompts on Android and iOS. 4 See the SRE Book, Chapter 15: Postmortem Culture: Learning from Failure and Postmortem Action Items:  Plan the Work and Work the Plan.  252      Chapter 15: The Intersection of Reliability and Privacy   goals. Note that privacy engineering is cross-functional in ways that differ from relia‐ bility engineering in that a lot of privacy work is driven not by engineering mandates, but  by  legal,  policy,  and  compliance  needs  and  business  risks.  Here,  we  focus  on product team relationships and the ways in which privacy can leverage SRE wisdom. A key aspect of relationship management when it comes to privacy is making sure that you focus on what has the biggest pragmatic impact for the user, not features that are flashy or high profile. Privacy is unique in its potential impact and the high stakes involved. Unlike deciding on the perfect color schema or menu bar for a prod‐ uct, or even making sure that a service doesn’t violate an agreed-upon Service-Level Agreements  SLA , most privacy-related pitfalls tend to be one-way ratchets: mishan‐ dling private data typically can’t be undone. Because of the high stakes, it’s important to foster strong collaborative relationships by providing your partners with actionable and constructive feedback. When giving guidance  to  product  teams,  avoid  merely  pointing  out  why  their  products  or  pro‐ cesses are flawed. Instead, focus on building a shared vision. Express feedback about how to meet your goals in the context of their goals and your larger shared goals. For example, align the privacy-centric goal of transparency and control with the product team’s goal of building trust in its product by describing how doing the right thing will delight users. Your feedback loop is a two-way street, as is your relationship: peo‐ ple  on  both  sides  of  the  equation  can  save  each  other  time  and  energy  by  making their value propositions clear, explicitly acknowledging the other people’s goals, and working together toward a shared goal. Early Intervention and Education Through Evangelism After your colleagues are aware that they need to factor reliability and privacy into product  decisions,  figure  out  where  your  talents  are  best  applied  and  how  to  scale your expertise effectively by educating others. Spread knowledge about your goals— not just what your goals are, but why you have these particular goals. Instead of sim‐ ply  telling  developers,  “Your  product  needs  to  do  x,”  tell  them  why  their  product needs to do x  “If your product doesn’t do x, the fallout is y and z” . Even better, also point them to other products that do x, with proven benefits a and b. In the reliability space, this conversation might look something like the following:  Not so great: “We need you to move your service onto this RPC framework.” Better: “We need you to move your service onto this RPC framework because it will allow us to better monitor requests. That way, we can understand where slowdowns are, and then work to improve product performance.”  Privacy and SRE: Common Approaches      253   In the privacy space, this conversation might look something like the following:  Not  so  great:  “We  need  your  product  to  integrate  with  the  new  privacy  settings account dashboard.” Better:  “We  need  your  product  to  integrate  with  the  new  privacy  settings  account dashboard. Products x, y, and z are already using this new dashboard, so integrating will help users find controls where they expect to find them. Our end goal here is to minimize user frustration by providing a consistent experience across products.”  When it comes to both reliability- and privacy-related matters, when people under‐ stand why you’re supplying them with a specific piece of guidance, both that immedi‐ ate project and future projects will benefit. If teams understand your areas of concern up front, next time they can proactively approach your team early in the project life cycle rather than shortly before they’re hoping to push to production. Again, the key to good communication here is to focus on your shared mission and assume good intent, rather than assigning blame. Early engagement is always best, and beyond providing frameworks to engage prod‐ ucts from the design phase  see “Frameworks” on page 250 , proactive education is your best  and sometimes only  hope of getting your partners to talk about privacy and reliability at an appropriate time. Otherwise, people don’t even realize that they should  engage  with  privacy  engineering  until  they’re  forced  to  talk  to  you,  which tends to happen last when developing a new product or feature  if it happens at all . Failing to engage with a product from its early stages means that the product will veer into directions you don’t want it to go. Having a broad network of people who under‐ stand what you care about and why also helps your partners detect outages and other potential issues earlier. Proactively  educating  others  about  privacy  also  allows  you  to  distribute  load.  The goal isn’t to avoid work your team should rightfully be handling, but to engage in knowledge sharing that enables you to spend your time on the hard problems that only you can solve. For issues that are clear-cut, after your partners understand what you care about and how to avoid obvious and predictable problems, they won’t need to come to you for these straightforward cases. The product team both saves time and avoids  having  to  potentially  redo  work  in  light  of  privacy  matters  that  they  could have considered from square one. For example, access control is a topic that every product team needs to approach stra‐ tegically. Instead of starting this discussion from the basics with each and every prod‐ uct  team,  educate  your  developers  on  the  benefits  of  having  well-structured  access control groups. From a reliability standpoint, this means engineers are less likely to cause an outage when making changes  for example, because some critical workflow is gated by an access path . From a privacy standpoint, it’s important to have good visibility into who has access to your systems so that you can prevent unauthorized access  to  user  data.  In  a  similar  vein,  you  should  also  make  sure  that  developers  254      Chapter 15: The Intersection of Reliability and Privacy   design their products to clearly track who’s talking to your service. If you can’t differ‐ entiate between the clients accessing your service, you won’t know who to work with to resolve a problem. You can make better decisions about the actually important and hard questions fac‐ ing your team when you don’t need to waste time answering basic questions or pro‐ viding  standard  design  advice  repeatedly.  Your  partners  also  benefit  because  they have a quicker turnaround time on their questions. Nuances, Differences, and Trade-Offs Despite their similarities, reliability engineering and privacy engineering have some fundamental differences. Although neither reliability nor privacy is strictly black and white, when it comes to user expectations around reliability, you have more latitude in defining an acceptable threshold that constitutes a “reliability outage.” Privacy “outages” are subject to many external factors, such as how users react to particular events and even legal and regu‐ latory requirements. Even though users might be perfectly happy if a service is avail‐ able for 99% of a year, they might not be so happy if you guarantee to handle only 99% of their data in the right way. Reliability issues are inherently also more “fixa‐ ble”: if your service is down, you can fix the problem by getting that service back up and running, but there’s no way to “fix” a compromised database: you can’t unring a bell. Some design decisions might end up trading one of these aspects for the sake of the other; however, creating technical reliability at the cost of user surprise isn’t necessar‐ ily  productive.  This  equation  is  far  more  likely  to  be  weighted  in  favor  of  privacy. Sometimes, it makes sense to ship a product that’s not “perfectly reliable” for the sake of shipping something usable—mandating that a launch or service is 100% risk-free means that you’d never ship anything. But because of the consequences of an “out‐ age,” privacy doesn’t have the same degree of flexibility. A service that goes down can be restored without lasting ill effects  customers understand an occasional outage , but  a  privacy  incident  can  have  permanent  effects.  These  long-term  effects  should factor into operational decisions. At the end of the day, you’re creating a product that actual people use, not a hypothetical technical service with abstract “users.” Conclusion Reliability  engineering  and  privacy  engineering  are  fundamentally  similar  in  many ways: both disciplines work from the same foundation and toward the same ultimate goal. Both can leverage many of the same best practices and approaches. Both are suf‐ ficiently important to users, and hard enough to get right, that they should be treated as proper engineering disciplines, not as afterthoughts. And both should be ingrained  Nuances, Differences, and Trade-Offs      255   in your company or organization’s culture. Although their states of maturity may dif‐ fer, SRE and privacy engineering are living, breathing, and quickly evolving fields—as their  core  tenants  gain  wider  adoption  across  the  industry,  they  both  must  evolve alongside user expectations. Google teams frequently make use of the techniques described in this chapter to build world-class products that respect user privacy. SREs are in an ideal position to advo‐ cate for user privacy, even if they don’t explicitly work in the privacy space  and par‐ ticularly if your organization can’t dedicate specific resources to privacy engineering . Working from the base of effective problem-solving skills, privacy engineers combine those  skills  with  empathy  and  societal  context  to  tackle  a  different  realm  of  user- centric challenges. As any well-seasoned SRE knows, metrics are just a means to an end; the user’s experience is what really matters. Further Reading   OECD Guidelines on the Protection of Privacy and Transborder Flows of Per‐  sonal Data    Google’s Privacy Policy and Privacy Technology and Principles  Betsy Beyer is a technical writer for Google in New York City specializing in site relia‐ bility engineering. In addition to editing Site Reliability Engineering  O’Reilly, 2016 , she  has  previously  written  documentation  for  Google’s  Data  Center  and  Hardware Operations teams in Mountain View and across its globally distributed data centers. Before moving to New York, Betsy was a lecturer on technical writing at Stanford Uni‐ versity. Amber Yust worked in Google SRE before joining Google’s privacy effort in 2014. As a staff privacy engineer, she now leads a team working to engineer reliable privacy into Google’s products at a fundamental level.  256      Chapter 15: The Intersection of Reliability and Privacy   CHAPTER 16 Database Reliability Engineering  Laine Campbell, Fastly  This is an edited excerpt from Database Reliability Engineering by Laine Campbell and Charity Majors  O’Reilly, 2017 .  In this chapter, I talk about the craft of database reliability engineering as a subset of SRE. The database tier is the tier with the least tolerance for risk and is thus one of the  greatest  opportunities  for  growth  through  a  culture  of  reliability  engineering. Traditionally, DBAs were in the business of crafting silos and snowflakes. Their tools were  different,  their  hardware  was  different,  their  languages  were  different.  DBAs were writing SQL, systems engineers were writing Perl, software engineers were writ‐ ing  C++,  web  developers  were  writing  PHP,  and  network  engineers  were  crafting their own perfect appliances. Only half of the teams were using version control in any kind of way, and they certainly didn’t talk or step on one another’s turf. How could they? It was like entering a foreign land. The  days  for  which  this  model  can  prove  itself  to  be  effective  and  sustainable  are numbered. This chapter is a view of reliability engineering as seen through a pair of database  engineering  glasses.  I  do  not  plan  on  covering  everything  possible  here. Instead,  I  am  describing  what  I  do  see  as  important,  through  the  lens  of  the  SRE experience. You can then apply this framework to multiple datastores, architectures, and organizations. Guiding Principles of the Database Reliability Engineer I have spent a considerable amount of time considering how the paradigm of reliabil‐ ity engineering fits in the world of database engineering. One of the first questions I  257   asked myself was what were the principles underlying this new iteration of the data‐ base profession? If the way people approached datastore design and management was changing, a definition for the foundations of this new world was required. Thus, here are the guiding principles of the database reliability engineer  DBRE . Protect the Data Traditionally, this always has been a foundational principle of the database professio‐ nal,  and  still  is.  The  generally  accepted  approach  has  been  attempted  via  the following:    A strict separation of duties between the software and the database engineer   Rigorous backup and recovery processes, regularly tested   Well-regulated security procedures, regularly audited   Expensive database software with strong durability guarantees   Underlying expensive storage with redundancy of all components   Extensive controls on changes and administrative tasks  For teams with collaborative cultures, the strict separation of duties can become not only burdensome, but restrictive of innovation and velocity. There are ways to create safety nets and reduce the need for separation of duties without impacting reliability. The new approach to data protection might look more like this:    Responsibility of the data shared by cross-functional teams   Standardized and automated backup and recovery processes blessed by DBRE   Standardized  security  policies  and  procedures  blessed  by  DBRE  and  security  teams    All policies enforced via automated provisioning and deployment   Data  requirements  dictate  the  datastore,  with  evaluation  of  durability  needs  becoming part of the decision-making process    Reliance  on  automated  processes,  redundancy,  and  well-practiced  procedures  rather than expensive, complicated hardware    Changes  incorporated  into  deployment  and  infrastructure  automation,  with  focus on testing, fallback, and impact mitigation  Self-Service for Scale A talented DBRE is a rare commodity. Most companies cannot justify or retain more than one or two of these specialists. So, we must create the most value possible, which  258      Chapter 16: Database Reliability Engineering   comes from creating self-service platforms for teams to use. By setting standards and providing tools, teams are able to deploy new services and make appropriate changes at the required pace without serializing on an overworked database engineer. Following are some examples of these kinds of self-service methods:    Ensuring the appropriate metrics are being collected from data stores by provid‐  ing the correct plug-ins    Building backup and recovery utilities that can be deployed for new data stores   Defining  reference  architectures  and  configurations  for  data  stores  that  are  approved for operations and can be deployed by teams    Working with Security to define standards for data store deployments   Building safe deployment methods and test scripts for database migrations to be  applied  In  other  words,  the  effective  DBRE  functions  by  empowering  others  and  guiding them, not functioning as a gatekeeper. Databases Are Not Special Databases  are  often  critical  components  of  an  organization’s  infrastructure.  They tend to generate the most risk in terms of needs for them to be available and com‐ plexity of recovering from failure and corruption. We must strive for standardization, automation, and resilience. Critical to this is the idea that the components of data‐ base clusters are not sacred. We should be able to lose any component, and efficiently replace  it  without  worry.  Fragile  data  stores  in  glass  rooms  should,  and  can,  be  a thing of the past. The metaphor of pets versus cattle is often used to show the difference between a spe‐ cial  snowflake  and  a  commodity  service  component.1  A  pet  server  is  one  that  you feed, care for, and nurture back to health when it is sick. It has a name—at Traveloc‐ ity in 2000, our servers were Simpsons characters. Our two SGI servers running Ora‐ cle were named Patty and Selma. I spent so many hours with those gals on late nights. They were high maintenance! Cattle servers have numbers, not names. You don’t spend time customizing servers, much less logging on to individual hosts. When they show signs of sickness, you cull them from the herd. You should, of course, keep those culled cattle around for foren‐ sics if you are seeing unusual amounts of sickness. But I’ll refrain from mangling this metaphor any further.  1 Original attribution for this goes to Bill Baker, Microsoft Distinguished Engineer.  Guiding Principles of the Database Reliability Engineer      259   Data stores are some of the last holdouts of “pethood.” After all, they hold “The Data” and simply cannot be treated as replaceable cattle with short lifespans and complete standardizations. What about the special replication rules for our reporting replica? What about the different configuration for the primary’s redundant standby?  Why Guiding Principles? Every day in our jobs, we find ourselves faced with an incredible number of decisions. Guiding principles reduce the cognitive over‐ head of these decisions, helping us to be consistent in our choices. Over  time,  each  of  these  incremental  decisions  adds  up  to  hope‐ fully contribute to a coherent culture.  A Culture of Database Reliability Engineering The reason I feel there a real need for an emphasis on reliability in not just the job title of the DBRE, but in everything they do, is because the database is one of those places where risk and chaos simply has no place. A lot of what is now commonplace in our day-to-day work came about from innovation in areas of computing where risk could be tolerated. Now that these paradigms are ubiquitous, it is up to the stew‐ ards of one of the organization’s most precious resources, the data, to find paths to bring databases into the fold. Much of the work to make persistent data stores a first-class citizen of the world of reliability engineering is still in its early phases. There is only so much risk that an organization can tolerate when data comes into play. Thus, how we introduce these concepts  to  the  rest  of  the  organization  or  how  we  respond  to  others  doing  so becomes  an  actual  discipline  and  job  function  for  us.  It  is  not  enough  to  have  the vision and the intent; we must simultaneously find ways to introduce this vision in such a way as to be successful. What  does  a  culture  of  database  reliability  look  like  and  how  can  you  promote  it? There are many items that people think of when they think of reliability culture that are not specific to the database world, including the following:    Blameless postmortems   Automating away repetitive work   Structured and rational decision making  This all makes sense, and everyone within an operations or SRE organization should constantly  be  working  toward  this.  In  this  chapter,  I  look  at  two  specific  database engineering functions that an SRE can focus on to ensure that their database infra‐ structure  and  the  supporting  organization  gains  the  benefits  of  reliability  culture. These functions are data integrity and durability, and Continuous Delivery  CD .  260      Chapter 16: Database Reliability Engineering   While these functions are only part of the work that must be accomplished by the DBRE  in  the  world  of  SRE,  they  encompass  an  excellent  cross-section  of  the approach to operations and engineering expected. Recoverability Let’s face it. Everyone considers backup and recovery to be dull and tedious. They think of it as the epitome of toil. It is often relegated to junior engineers, outside con‐ tractors, and third-party tooling with which the team is loath to interact. I’ve worked with some pretty horrible backup software before. Trust me, I empathize. Still, this is one of the most crucial processes in your operations toolkit. Moving your precious data between nodes, across data centers, and into long-term archives is the constant  movement  of  your  business’s  most  precious  commodity:  its  data.  Rather than relegating this to a second-class citizen of ops, we strongly suggest you treat it as a VIP. Everyone should not only understand the recovery targets, but be intimately familiar  with  operating  and  monitoring  the  processes.  Many  DevOps  philosophies propose that everyone should have an opportunity to write and push code to produc‐ tion. We propose that every engineer should participate at least once in the recovery processes of critical data. We create and store copies of data, otherwise known as backups and archives, as a means to accomplish the real need: recovery. Sometimes, this recovery is something nice and leisurely, such as building an environment for auditors or building an alter‐ nate  environment.  More  often,  though,  the  recovery  is  the  need  to  rapidly  replace failed nodes or to add capacity to existing clusters. Today, in distributed environments, we face new challenges in the backup and recov‐ ery realm. Now, as before, most local datasets are distributed to reasonable sizes, of up to a few terabytes at most. The difference is that those local datasets are only one fraction of a larger distributed dataset. Recovering a node is a relatively manageable task, but keeping state across the cluster becomes more challenging. Considerations for Recovery When  first  evaluating  an  effective  strategy,  you  should  look  back  to  your  Service- Level Objectives  SLOs . Specifically, you need to consider availability and durability indicators. Any strategy that you choose will require you to be able to recover data within the uptime constraints you have set. And you need to get data backed up fast enough to ensure that you meet the necessary parameters for durability. If you back up every day, and your transaction logs between backups remain on node-level stor‐ age, you might very well lose those transactions before the next backup. Additionally, you need to consider how the dataset functions within the holistic eco‐ system. For instance, your orders might be stored in a relational system where every‐  Recoverability      261   thing is committed in transactions and is thus easily recovered in relation to the rest of the data within that database. However, after an order is set, a workflow might be triggered via an event stored in a queuing system or a key-value store. Those systems might be eventually consistent, or even ephemeral, relying on the relational system for reference or recoverability. How do you account for those workflows when recov‐ ering? If you are in an environment with rapid development, you might also find that data stored in a backup was written and utilized by a different version of the application than the one running after the restore is done. How will the application interact with that older data? Hopefully the data is versioned to allow for that, but you must be aware of this and prepared for such eventualities. Otherwise, the application could logically corrupt that data and create even larger issues down the road. You must take into account each of these, and many other variables that you cannot plan for, when planning for data recovery. You simply can’t prepare for every even‐ tuality. But this is a critical service. Data recoverability is one of the most significant responsibilities of the DBRE. So, your plan for data recoverability must be able to be as broad as possible, taking into account as many potential issues as possible. Anatomy of a Recovery Strategy There  is  a  reason  why  I  say  recovery  strategy,  rather  than  a  backup  strategy.  Data recovery is the entire reason we do backups. Backups are simply a means to the end and  thus  are  dependent  on  the  true  requirement:  recovery  within  parameters.  The simple question “Is your database backed up?” is a question that should be followed with  the  response  “Yes,  in  multiple  ways,  depending  on  the  recovery  scenario.”  A simple yes is naive and promotes a false sense of security that is irresponsible and dangerous. An effective database recovery strategy not only approaches multiple scenarios with the most effective strategies, but also includes the detection of data loss corruption, recovery testing, and recovery validation. Building Block 1: Detection Early detection of potential data loss or corruption is crucial. This means that back‐ ups might even be aged out by the time the need for them is noticed. Thus, detection must be a high priority for all of engineering. In addition to building early detection around data loss or corruption, ensuring that there is as long of a window as possible in place to recover in case early detection fails is also critical. Let’s look at the differ‐ ent failure scenarios discussed and identify some real-world approaches to detection and lengthened recovery windows.  262      Chapter 16: Database Reliability Engineering   User error One of the biggest impacts in reducing time to identifying data loss is through not allowing manual or ad hoc changes to be executed in production creating wrappers for scripts, or even API-level abstractions; engineers can be guided through effective steps for ensuring all changes are as safe as possible, tested, logged, and pushed up to the appropriate teams. An effective wrapper or API will be able to do the following:    Execute in multiple environments via parameterization   A dry-run stage, where execution results can be estimated and validated   A test suite for the code execution   Validation post-execution to verify changes met expectations   Soft-deletion or easy rollback via the same API   Logging by ID of all data modified, for identification and recovery  By removing the ad hoc and manual components of these processes, you can increase the  likelihood  that  all  changes  will  be  trackable  by  troubleshooting  engineers.  All changes will be logged so that there is traceability and the change cannot simply dis‐ appear into the day-to-day noise. Similarly,  you  can  coach  software  developers  to  change  how  they  think  about  the removal,  or  deletion,  of  data  from  their  environments.  Instead  of  deleting  data,  or irrevocably changing data  also known as DELETE or UPDATE operations , devel‐ opers  can  record  a  continuous  timeline  of  actions   INSERT,  NEW  VERSION, DELETE . This allows an application to have a full history of an object, which allows for  software-level  recovery  and  auditing,  as  opposed  to  expensive  and  error-prone human  recoveries.  This  is  not  a  guarantee;  after  all,  manual  processes  can  be extremely  well  logged,  and  people  can  forget  to  set  up  logging  in  automated  pro‐ cesses, or they can bypass them.  Application errors When engineers introduce new objects and attributes, DBREs should work with them to identify data validation that can be done downstream, outside of the application itself. Initial work should focus on quick tests that provide fast feedback loops on crit‐ ical  data  components,  such  as  external  pointers  to  files,  relationship  mapping  to enforce  referential  integrity,  and  personal  identification  information   PII .  As  data and applications grow, this validation becomes more expensive and more valuable. Building  a  culture  that  holds  engineers  accountable  for  data  quality  and  integrity rather than the storage engines pays dividends in terms not only of flexibility to use different databases, but also of helping people feel more confident about experiment‐  Recoverability      263   ing and moving fast on application features. Validation functions as a guardrail, help‐ ing everyone feel braver and more confident.  Infrastructure services Any catastrophic infrastructure impacts that require recovery should be caught rap‐ idly by monitoring in the operational visibility stack. But there are changes that can cause silent data loss, data corruption, or availability. Using golden images and com‐ paring them regularly to your infrastructure components can help identify straying from  the  test  images  quickly.  Similarly,  versioned  infrastructure  can  help  identify straying infrastructure and alert the appropriate engineers or automation.  Operating system and hardware errors As  with  infrastructure  services,  the  majority  of  these  problems  should  be  rapidly caught  by  monitoring  of  logs  and  metrics.  Edge  cases  that  are  not  standard  will require  some  thought  and  experience  to  identify  and  add  to  monitoring  for  early detection. Checksums on disk blocks is an example of this. Not all filesystems will do this,  and  teams  working  with  critical  data  need  to  take  the  time  to  consider  the appropriate filesystems that can identify silent corruption via checksumming. Building Block 2: Diverse Storage An effective recovery strategy relies on data being placed on diverse areas of storage with  different  operating  characteristics.  Different  recovery  needs  can  be  served  by different storage areas, which not only ensures the right performance, but also the right cost and the right durability for any number of scenarios.  Online, high-performance storage This is the storage pool most of your production data stores will run on. It is charac‐ terized by a high amount of throughput, low latency, and, thus, a high price point. When recovery time is of the utmost importance, putting recent copies of the data‐ store and associated incremental backups on this tier is paramount. This allows for rapid  recovery  for  common  and  critical  recovery  scenarios,  including  full  dataset copies after host failures or adding nodes rapidly for additional capacity. These pools often also have snapshots available.  Online, low-performance storage This storage pool is often utilized for data that is not sensitive to latency. Larger disks have  low  throughput  and  latency  profiles,  and  a  lower  price  point.  These  storage pools are often much larger, holding more copies of data from further back in time. Relatively  infrequent,  low-impact,  or  long-running  recovery  scenarios  utilize  these  264      Chapter 16: Database Reliability Engineering   older  backups.  Typical  use  cases  include  finding  and  repairing  application  or  user errors that slipped by early detection.  Offline storage Snapshots, tape storage, or even something like Amazon Glacier are examples of this kind of storage. Snapshots store only Input Output  I O  blocks that have changed, allowing  for  even  more  copies  to  be  retained  at  some  level  of  resource  overhead. Snapshots,  if  available  via  your  volume  manager  or  filesystem,  can  allow  for  rapid point-in-time recovery. Tape or Glacier is typically off-site, requiring movement via vehicle or slow pipelines to bring it to an area where it can be made available for recovery. This type of storage can support business continuity and audit requirements but does not have a place in day-to-day recovery scenarios. Still, due to the size and cost, vast amounts of storage are available here, allowing for the potential of storing all data for the life of the busi‐ ness, or at least for a full legal compliance term.  Object storage Object storage is a storage architecture that manages data as objects rather than files or  blocks.  Object  storage  gives  features  not  available  through  traditional  storage architectures, such as APIs, object versioning, and high degrees of availability via rep‐ lication and distribution. This enables easy recovery of specific objects. Amazon Sim‐ ple Storage Service  Amazon S3  is a classic example of an inexpensive, scalable, and reliable object-level storage tier. Each of these tiers plays a part in a comprehensive strategy for recoverability across multiple potential scenarios. Without being able to predict every possible scenario, it is this level of breadth that is required. Next, we discuss the tools that utilize these storage tiers to provide recoverability. Building Block 3: A Varied Toolbox You will note that nowhere do I discuss replication as a way to effectively back up data for recovery. Replication is blind, and can cascade user errors, application errors, and corruption. You must look at replication as a necessary tool for data movement and synchronization, but not for creating useful recovery artifacts. Similarly, Redun‐ dant Array of Independent Disks  RAID  is not a backup; rather, it is a redundancy.  Full physical backups We know that we will need to do full restores at each level of scope: node level, clus‐ ter level and data center level. Rapid, portable full restores are incredibly powerful and  mandatory  in  dynamic  environments.  A  full  backup  can  be  done  via  full  data  Recoverability      265   copies over the network or via volumes that you can easily attach and detach from specific hosts or instances. To do this, you need full backups. A full backup using online, high-performance storage is for immediate replacement into  an  online  cluster.  These  backups  are  typically  uncompressed  because  decom‐ pressing them takes a lot of time. Full backups using online, low-performance storage are utilized for building different environments, such as test, or for analytics and data forensics. Compression is an effective tool to allow for longer timelines of full back‐ ups on limited storage pools.  Incremental physical backups Incremental backups bridge the gap between the last full backup and a place in time after it. Physical incremental backups are generally done via data blocks that have a changed piece of data within it. Because full backups can be expensive, both in terms of  performance  impact  during  the  backup  as  well  as  storage,  incremental  backups allow you to quickly bring up to date a full backup that might be older, for use in the cluster.  Full and incremental logical backups A  full  logical  backup  provides  for  portability  and  simpler  extraction  of  subsets  of data. They will not be used for rapid recovery of nodes, but instead are perfect tools for use in forensics, moving data between data stores and recovering specific subsets of data from large datasets.  Object stores Object stores, like logical backups, can provide for easy recovery of specific objects. In fact, object storage is optimized for this specific use case, and APIs can easily use it to programmatically recover objects as needed. Building Block 4: Testing For such an essential infrastructure process as recovery, it is astonishing how often testing tends to fall by the wayside. Testing is an essential process to ensure that your backups are usable for recovery. Testing is often set up as an occasional process, to be run  on  an  intermittent  basis  such  as  monthly  or  quarterly.  Although  this  is  better than nothing, it allows for large periods of time between tests during which backups can stop working. There are two effective approaches to adding testing into ongoing processes. The first one  is  incorporating  recovery  into  everyday  processes.  This  way,  recovery  is  con‐ stantly  tested,  allowing  for  rapid  identification  of  bugs  and  failures.  Additionally, constant recovery creates data about how long your recovery takes, which is essential in  calibrating  your  recovery  processes  to  meet  Service-Level  Agreements   SLAs .  266      Chapter 16: Database Reliability Engineering   Examples of constant integration of recovery into daily processes include the follow‐ ing:    Building integration environments   Building testing environments   Regularly replacing nodes in production clusters  If your environment does not allow for enough opportunities to rebuild data stores, you can also create a continuous testing process, whereby recovery of the most recent backup is a constant process, followed by verification of the success of that restore. Regardless of the presence of automation, even off-site backup tiers do require occa‐ sional testing. With these building blocks, you can create a defense in depth for different recovery scenarios. By mapping out the scenarios and tools used to recover them, you can then begin evaluating your needs in terms of development and resources. Championing Recovery Reliability Much of this section has been about creating infrastructure and focusing on enabling development teams to make better choices about how they store, change, and recover their data. This is the heart of reliability: leveraging the entire organization to do the right thing and giving them the best range of tools with which to do it. Next, let’s dis‐ cuss the function of continuous delivery. Continuous Delivery: From Development to Production One of the highest-value activities the SRE can do is working with software engineers to build, test, and deploy application features. The traditional DBA was a gatekeeper, reviewing each database migration, database object, and query accessing the database. When satisfied, the DBA would plan an appropriate hand-crafted change and shep‐ herd it through into production. A gatekeeper can rapidly become a bottleneck, how‐ ever,  leading  to  burnout  on  the  part  of  the  DBA  and  frustrations  in  software engineering. My goal in this section is to look at how to effectively utilize your time, skills, and experience to effectively support a software engineering process that utilizes Contin‐ uous Integration  CI  and even Continuous Deployment without becoming a bottle‐ neck. Education and Collaboration One of the first steps is educating the developer population. If a software engineer can make better choices about their data structures, their SQL, and the overall inter‐  Continuous Delivery: From Development to Production      267   action strategies, there will be fewer needs for your direct intervention. By taking on the role of educator, you can have greater impact on the organization while fostering better relationships, trust, and communication. I am suggesting that regular interactions and strategic efforts enable teams to have access to resources and autonomy for most decisions around the database. Remem‐ ber to keep everything you do specific, measurable, and actionable. Define key met‐ rics for your team’s success in this, and as you implement strategies and changes, see how they help the team. This requires cross-functional interactions between people of different backgrounds, skill levels, and professional contexts to collaborate closely. Education and collabora‐ tion is a huge part of this process; it’s a great opportunity to shift out of the legacy “DBA” mode and become an integrated part of your technical organization.  Architecture I am not a fan of static documentation separate from the processes that build and deploy architecture. With configuration management and orchestration systems, you get a lot of documentation for free. Putting tools on top of these to allow for easy dis‐ covery, borrowing, and, of course, annotation of notes and comments creates a living document for teams. Your job is to make knowledge, context, and history available to engineers who are making decisions daily while working on features without your oversight. Building a knowledge base of design documents creates the structure necessary to build context and history around the architecture. These documents might apply to full projects that require new architectural components, or they might relate to smaller incremen‐ tal changes or subprojects.  Data model Knowledge  of  the  organization’s  data  is  also  critical.  Knowing  what  is  stored  and where you can find it can eliminate a significant amount of redundancy and investi‐ gative time from the development process. This is also the opportunity to give best practices for which data stores are not appropriate for certain kinds of data.  Best practices and standards Giving engineers standards for the activities they engage in regularly is another effec‐ tive method for optimizing the amount of value you are able to generate. You can do this  incrementally  as  you  help  engineers  and  make  decisions.  If  your  organization regularly hires new engineers with no domain expertise, you can also converge these incremental  decisions  into  an  onboarding  and  training  program.  Here  are  some examples of this:  268      Chapter 16: Database Reliability Engineering     Datatype standards   Indexing   Metadata attributes   Datastores to use   Metrics to expose   Design patterns   Migration and database change patterns  Publishing these as you work with engineers allows for a self-service knowledge base, which teams can access at any time rather than being forced to bottleneck on you.  Tools Giving software engineers effective tools for their development process is the ultimate enabler. You might be helping them with benchmarking tools and scripts, data con‐ sistency evaluators, templates, or even configurators for new data stores. What you are doing is enabling greater velocity in the development process while simultane‐ ously freeing up your time for higher-value efforts. Collaboration If you’re regularly educating, creating tools, and empowering engineers, you will nat‐ urally create good relationships. Any engineer should be empowered to reach out to you to ask for information or for the chance to pair while they work. This gives great value bidirectionally, as the SWEs learn more about how the SRE team works and what they look for, and the SRE team learns more about the software development process. You can facilitate this further by proactively reaching out to engineers. There are fea‐ tures that have a large dependency and reliance on database development and refac‐ toring.  This  is  where  the  DBRE  should  be  focusing  their  efforts  to  help  guarantee success and efficiency. Ask to pair or be part of a team on these stories. Similarly, keeping  an  eye  on  migrations  being  committed  into  mainline  will  help  the  DBRE team cherry-pick where it needs to perform reviews. Next,  I  discuss  how  to  effectively  support  the  various  components  of  the  delivery pipeline. While Continuous Delivery is not a new concept by any means, organiza‐ tions have struggled to incorporate databases into the process. In each of the follow‐ ing sections, I will discuss how to effectively introduce the database layers into the full delivery cycle.  Collaboration      269   Deployment It makes sense to decompose data migrations in such a way that engineers can easily and incrementally modify environments with a minimum of risk. Your goal should be to empower engineers to recognize when their database changes require analysis and  management  by  experts  in  order  to  be  effectively  introduced  into  production. Additionally, you would be able to give those engineers the tools to safely and reliably introduce most changes into production themselves. Optimally, you would give them the  ability  to  push  changes  at  any  time  rather  than  during  restrictive  maintenance windows. Migrations and Versioning Each change set that is applied to the database should be given a numeric version. You generally do this by using incrementing integers that are stored in the database after a change set is applied. This way, your deployment system can easily look at the database and discover the current version. This allows you to easily apply changes when preparing to push code. If a code deployment is certified for database version 456, and the current database version is 455, the deployment team knows that it must apply the change set for 456 prior to pushing code. So, a software engineer has committed change set 456 into the code base, and integra‐ tion has been successfully run with no breaking changes. What comes next? Impact Analysis We  discussed  impact  analysis  in  the  previous  section  under  post-commit  testing. Some impacts, such as the invalidation of stored code in the database, or violation of security  controls,  are  gates  that  cannot  be  passed.  The  software  engineer  must  go back and modify their changes until these impacts have been mitigated. Some impacts can include the following:    Locking of shared objects   Saturation of resources   Data integrity issues   Broken or stalled replication  Migration Patterns After impact analysis, the software engineer should be able to make a decision on the appropriate way to deploy the migration. For many migrations, there is no reason to need to go through a lot of incremental changes and extensive review work to exe‐  270      Chapter 16: Database Reliability Engineering   cute. New objects, data inserts and other operations can be easily pushed through to production. After data is in the system, however, changes or removal of existing data, and modifi‐ cation  or  removal  of  objects  with  data  in  them,  can  create  opportunities  for  your migration to impact service levels, as discussed earlier. It is at this time that the engi‐ neer should bring the DBRE in. Luckily, there is a finite set of changes that can be planned for. As you work with engineers to plan and execute migrations, you build a repository  of  patterns  for  changes.  At  some  point,  if  these  migrations  happen  fre‐ quently and painlessly enough, you can automate them. The more flags and safeguards you put in place to enable safety in production for everyone, the more confidence you create in all teams. This results in development velocity. Now, assume that our intrepid software engineer who has checked in change set 456 has had their change flagged due to an alter that is deemed to be impactful. At this point, the engineer can use a migration pattern for that operation if it has been applied and documented. Otherwise, one should be created in collaboration with the DBRE team.  Migration testing Even though it might seem self-evident, it is imperative to recognize that if a change set’s implementation details are modified, the revised migration must be committed and fully integrated before deployment in post-integration environments, including production.  Rollback testing In  addition  to  testing  migrations  and  their  impact,  the  DBRE  and  their  supported teams must consider the failure of migrations or deploys and the rolling back of par‐ tial or full change sets. Database change scripts should be checked in at the same time as migrations. There can be autogenerated defaults for some migrations such as table creations, but there must be an accounting for data that comes in. Therefore, I don’t recommend reverting by simply dropping an object. Renaming tables allows them to still be accessible in case data was written and must be recovered. Migration patterns also enable ease in the process of defining rollbacks. The lack of an effective rollback script can be a gating factor in the integration and deployment process. To validate that the scripts work, you can use the following deployment and testing pattern:    Apply change set   Quick integration tests   Apply rollback change set  Deployment      271     Quick integration tests   Apply change set   Quick integration tests   Longer and periodic testing  Much like testing recoveries, testing fallbacks is critical and you must incorporate it into every build and deploy process. Championing CD Database  changes  are  inevitably  the  last  blocker  for  truly  continuous  delivery.  The more you enable rapid, safe, and predictable changes to the database tier, the greater the organization’s ability to be competitive in the marketplace. Additionally, this is the greatest opportunity for building trust and rapport between teams. It must be a priority! Making the Case for DBRE Throughout this chapter, I have attempted to show how the landscape of database engineering has shifted. I’d like to thank you for taking the time to read this. I am so very passionate about helping one of the most burdensome and byzantine of techni‐ cal careers to evolve. I believe that the DBRE movement is one that can drive so much value to data-driven services and organizations. My hope is that you are inspired to explore these shifts in your organization and that you  are  eager  to  learn  more.  But,  most  important,  I  hope  I’ve  helped  you  see  that there is opportunity to bring the time-honored role of DBA into the modern world and into the future. The role of DBA isn’t going away, and whether you are new to this career or a tried-and-scarred veteran, I want you to have a long career ahead of you as you drive value to every organization of which you are a part. Should you be inspired to dig more into this topic, I recommend the following readings. Further Reading   Campbell, Laine and Charity Majors. Database Reliability Engineering  O’Reilly,  2017 .    Kleppman, Martin. Designing Data Intensive Applications  O’Reilly, 2017 .   Morris, Kief. Infrastructure as Code  O’Reilly, 2015 .   Humble, Jez and David Farley. Continuous Delivery  Addison-Wesley .  272      Chapter 16: Database Reliability Engineering   Laine Campbell works as Senior Vice President of Engineering at Fastly. She was also founder  and  CEO  of  PalominoDB Blackbird,  a  consultancy  servicing  the  database needs  of  companies  including  Obama  for  America,  Activision  Call  of  Duty,  Adobe Echosign, Technorati, Livejournal, and Zendesk.  Further Reading      273    CHAPTER 17 Engineering for Data Durability  James Cowling, Dropbox, Inc.  SREs live and breathe reliability, but to many engineers the word reliability is synony‐ mous with availability: “How do we keep the site up?” Reliability is a multifaceted concern, however, and an extremely important part of this is durability: “How do we avoid losing or corrupting our data?” Engineering for durability is of paramount importance for any company that stores user data. Most companies can survive a period of downtime, but few can survive los‐ ing a significant fraction of user data. Building expertise in durable systems is partic‐ ularly challenging, however; most companies improve availability over time as they grow and as their systems mature, but a single durability mistake can be a company- ending event. It’s therefore important to invest effort ahead of time to understand real-world durability threats and how to engineer against them. Replication Is Table Stakes If you don’t want to lose your data, you should store multiple copies of it. You proba‐ bly  didn’t  need  a  book  to  tell  you  this.  We’ll  breeze  through  most  of  this  pretty quickly because this is really just the basic requirements when it comes to durability. Backups Back up your data. The great thing about backups is that they’re logically and physi‐ cally disjointed from your primary data store: an operational error that results in loss or corruption of database state probably won’t impact your backups. Ideally, these should be stored both local to your infrastructure and also off-site, to provide both fast local access and to safeguard against local physical disasters.  275   Backups have some major limitations, however, particularly with regard to recovery time and data freshness. Both of these can conspire to leave you vulnerable to more data loss or downtime than you might expect.  Restoration Restoring  state  from  backups  can  take  a  surprisingly  long  time,  especially  if  you haven’t practiced recovering from backup recently. In fact, if you haven’t tested your backups recently, it’s possible they don’t even work at all! In the early days of Dropbox, we were dismayed to discover that it was going to take eight  hours  just  to  restart  a  production  database  after  a  catastrophic  failure.  Even though  all  of  the  data  was  intact,  we  had  set  the  innodb_max_dirty_pages_pct MySQL parameter too high, resulting in MySQL taking eight hours just to scan the redo log during crash recovery.1 Fortunately, this database didn’t store critical data, and we were able bring dropbox.com back online in a couple of hours by bypassing the database altogether, but this was certainly a wake-up call. Our operational sophis‐ tication has improved dramatically since then, which I outline later in this chapter.  Freshness Backups represent a previous snapshot in time and will usually result in loss of recent data  when  you  restore  from  them.  You  typically  want  to  store  both  full-snapshot backups for fast recovery and for historical versioning, along with incremental back‐ ups of more recent state to minimize staleness. Any stronger guarantees than this, however, require an actual replication protocol. Replication Database  replication  techniques  range  from  asynchronous  replication  to  semi- synchronous replication to full quorum or consensus protocols. The choice of replica‐ tion strategy will be informed by how much inconsistency you can tolerate and, to some extent, the performance requirements of the database. Although replication is often essential for durability, it can also be a source of inconsistency. You must take care  with  asynchronous  replication  schemes  to  ensure  that  stale  data  isn’t  errone‐ ously read from a replica database. A database promotion to a replica that is lagging from the master can also introduce permanent data loss. Typically, a company will run one primary database and two replicas, and then just assume that the durability problem has been solved. More elaborate storage systems will often require more elaborate replication mechanisms, particularly when storage overhead is a serious concern. For a company like Dropbox that stores exabytes of  1 We were running MySQL 5.1 at the time; this is much less of a problem with versions 5.5 and above.  276      Chapter 17: Engineering for Data Durability   data,  there  are  more  effective  ways  of  providing  durability  than  basic  replication. Techniques  like  erasure  coding  are  adopted  instead,  which  store  coded  redundant chunks of data across many disks and achieve higher durability with lower storage overhead. Dropbox utilizes variants on erasure coding that are designed to distribute data across multiple geographical regions while minimizing the cross-region network bandwidth required to recover from routine disk failures.  Estimating durability Regardless  of  the  choice  of  replication  technique,  an  obvious  question  you  might have is, “How much durability do I actually have?” Do you need one database rep‐ lica? Two? More? One convenient way of estimating durability is to plug some numbers into a Markov model. Let’s jump into some  simplified  math here, so take a deep breath—or just skip to the end of this section. Let’s assume that each disk has a given mean time to failure  MTTF  measured in hours and that we have operational processes in place to replace and re-replicate a failed  disk  with  a  given  mean  time  to  recovery   MTTR   in  hours.  We’ll  represent these as failure and recovery rates of λ = 1 MTTR , respectively. This means that we assume that each disk fails at a rate of λ failures per hour on average, and that each individual disk failure is replaced at a rate of μ recoveries per hour. For a given replication scheme, let’s say that we have n disks in our replication group, and that we lose data if we lose more than m disks; for example, for three-way data‐ base replication we have n = 3 and m = 2; for RS 9,6  erasure coding, we have n = 9 and m = 3. We  can  take  these  variables  and  model  a  Markov  chain,  shown  in  Figure  17-1,  in which each state represents a given number of failures in a group and the transitions between states indicate the rate of disks failing or being recovered.  MTTF  and μ = 1  Figure 17-1. Durability Markov model  In this model, the flow from the first to second state is equal to nλ because there are n disks remaining that each fail at a rate of λ failures per hour. The flow from the sec‐ ond state back to the first is equal to μ because we have only one failed disk, which is recovered at a rate of μ recoveries per hour, and so on.  Replication Is Table Stakes      277   The  rate  of  data  loss  in  this  model  is  equivalent  to  the  rate  of  moving  from  the second-last state to the data-loss state. This flow can be computed as  R loss  = nλ ×  n − 1 λ  μ  × … ×  n − m λ  mμ  =  n !  m !  n − m − 1  ! × λ m+1 μ m  1  − 8, 766  ln  1 − AFR    this is approximately  8, 766  for a data loss rate of R loss  replication groups per hour. This simplified failure model allows us to plug in some numbers and estimate how likely we are to lose data. Let’s say our disks have an Annualized Failure Rate  AFR  of 3%. We can compute MTTF from AFR by using MTTF = AFR  ; in this case  MTTF  =  287,795  hours.  Let’s  say  we  have  some  reasonably  good  operational tooling  that  can  replace  and  re-replicate  a  disk  within  24  hours  after  failure,  so MTTR = 24. If we adopt three-way data replication we have n = 3, m = 2, λ = 287, 795 , and μ = 1 24 . Plugging these into the equation earlier, we get R loss  = 7.25 × 10–14 data loss inci‐ dents per hour or 6.35 × 10–10 incidents per year. This means that a given replication group is safe in a given year with probability 0.9999999994. Wait, that’s pretty safe, right? Well, nine 9s of durability is pretty decent. If you have a huge number of replication groups,  however,  failure  becomes  more  likely  than  this,  because  each  individual group has nine 9s of durability, but in aggregate it’s more likely that one will fail. If you’re a big consumer storage company, you’ll likely want to push durability further than this. If you reduce recovery delay or buy better disks or increase replication fac‐ tor, it’s pretty easy to push durability numbers much higher. Dropbox achieves theo‐ retical durability numbers well beyond twenty-four 9s, aided by some fancy erasure coding and automatic disk recovery systems. According to these models, it’s practi‐ cally impossible to lose data. Does that mean we get to pat ourselves on the back and go home? Unfortunately not… Your durability estimate is only an upper bound! It’s  fairly  easy  to  design  a  system  with  astronomically  high  durability  numbers. Twenty-four 9s is an MTTF of 1,000,000,000,000,000,000,000,000 years. When your MTTF dwarfs the age of the universe, it might be time to reevaluate your priorities. Should  we  trust  these  numbers,  though?  Of  course  not,  because  the  secret  truth  is that adherence to theoretical durability estimates is missing the point.  They  tell  you how likely you are to lose data due to routine disk failure, but routine disk failure is  278      Chapter 17: Engineering for Data Durability   easy  to  model  for  and  protect  against.  If  you  lose  data  due  to  routine  disk  failure, you’re probably doing something wrong. What’s the most likely way you’re going to lose data? Is it losing a specific set of half a dozen different disks across multiple geographical regions within a narrow window of time? Or, is it an operator accidentally running a script that deletes all your files, or a firmware bug that causes half your storage nodes to fail simultaneously, or a soft‐ ware bug that silently corrupts 1% of your objects? Protecting against these threats is where the real SRE work comes in. Real-World Durability In the real world, the incidents that affect you worst are the ones that you don’t see coming: the “unknown unknowns.” If you saw something coming and didn’t protect against it, you weren’t doing your job anyway! Because we never know when a truck will plow into a data center or when someone will let an intern loose on production infrastructure, we need to devote our efforts to not only reducing the scope of bad things that can happen, but also being able to recover from the things that do. Bad things  will  happen.  Reliable  companies  are  the  ones  that  are  able  to  respond  and recover before a user is impacted. So, where do we start when guarding against the unknown? We’re going to devote the rest of this chapter to durability strategies that span the four pillars of durability engineering:    Isolation   Protection   Verification   Automation  Isolation Strong isolation is the key to failure independence. Durability  on  the  basis  of  replication  hinges  entirely  on  these  replicas  failing independently. Even though this might seem pretty obvious, there is usually a large correlation between failure of individual components in a system. A data center fire might destroy on-site backups; a bad batch of hard drives might fail within a short timeframe;  a  software  bug  might  corrupt  all  your  replicas  simultaneously.  In  these situations, you’ve suffered from a lack of isolation. Physical  isolation  is  an  obvious  concern,  but  perhaps  even  more  important  are investments in logical and operational isolation.  Real-World Durability      279   Physical isolation To put it succinctly, store your stuff on different stuff. There’s  a  wide  spectrum  of  physical  failure  domains  that  spans  from  disks  to machines, to racks, to rows, to power feeds, to network clusters, to data centers, to regions  or  even  countries.  Isolation  improves  as  you  go  up  the  stack,  but  usually comes  with  a  significant  cost,  complexity,  or  performance  penalty.  Storing  state across  multiple  geographic  regions  often  comes  with  high  network  and  hardware costs, and often entails a big increase in latency. At Dropbox, we design data place‐ ment algorithms taking into consideration the power distribution schematics in the data center to minimize the impact of a power outage. For many companies this is clearly overkill. Every company needs to pick an isolation level that matches its desired guarantees and technical feasibility, but after a level is chosen, it’s important to own it. “If this data center burns down, we’re going to lose all of our data and the company is over” is not necessarily an irresponsible statement, but it is irresponsible to be caught by surprise when you’re unsure of what isolation guarantees you actually have. There are other dimensions to physical isolation, like dual-sourcing your hardware or operating multiple hardware revisions of disks and storage devices. Although this is outside the scope of many companies, large infrastructure providers will usually buy hardware components from multiple suppliers to minimize the impact of a produc‐ tion shortage or a hardware bug. You’d likely be surprised how often bugs show up in components like disk drivers, routing firmware, and RAID cards. Judicious adoption of  hardware  diversity  can  reduce  the  impact  of  catastrophic  failures  and  provide  a backup option if a new hardware class doesn’t pass production validation testing.  Logical isolation Failures tend to cascade and bugs tend to propagate. If one system goes down, it will often hose everything else in the process. If one bad node begins writing corrupt data to another node, that data can propagate through the system and corrupt an increas‐ ingly large share of your storage. These failures happen because distributed systems often have strong logical dependencies between components. The key to logical isolation is to build loosely coupled systems. Logical isolation is difficult to achieve. Database replicas will happily store corrupt data  issued  to  them  by  the  primary  database.  A  quorum  consensus  protocol  like Zookeeper will also suffer the same fate. These systems are not only tightly coupled from a durability perspective, they’re tightly coupled from an availability perspective: if a load spike leads to a failure of one component, there is usually an even greater load applied to the other components, which subsequently also fail.  280      Chapter 17: Engineering for Data Durability   Strong logical isolation usually needs to be designed into the underlying architecture of a system. One example at Dropbox is region isolation within the storage system. File objects at Dropbox  are  replicated  extensively  within  a  single  geographical  region,  but  also duplicated  in  a  completely  separate  geographical  region  on  the  other  side  of  the country.  The  replication  protocols  within  a  region  are  quite  complicated  and designed for high storage efficiency, but the API between the two regions is extremely simple: primarily just put and get, as illustrated in Figure 17-2.  Figure 17-2. Isolated multiregion storage architecture  Strong isolation between regions is a nice way to use modularity to hide complexity, but it comes at a significant cost in terms of replication overhead. So why would a company spend more than it needs to for storage? The abstraction boundary between storage regions makes it extremely hard for cas‐ cading issues to propagate across regions. The loose logical coupling and simple API makes it difficult for a bug or inconsistency in one region to impact the other. The loose coupling also makes it possible to take an entire region down in an emergency without impacting our end users; in fact, we take a region down every week as a test exercise. This architecture results in a system that is extremely reliable from an avail‐ ability and durability perspective, without imposing a significant operational burden on the engineers who run the system.  Operational isolation The most important dimension to isolation is operational isolation, yet it is one that is often overlooked. You can build one of the world’s most sophisticated distributed storage  systems,  with  extensive  replication  and  physical  and  logical  isolation,  but then let someone run a fleet-wide firmware upgrade that causes disk writes to fail, or  Real-World Durability      281   to accidentally reboot all your machines. These aren’t academic examples; we’ve seen both happen in the early days of Dropbox. Typically, the most dangerous component of a system is the people who operate it. Mature SRE organizations recognize this and build systems to protect against them‐ selves. I elaborate more on protections in the next section, but one critical set of pro‐ tections is isolation across release process, tooling, and access controls. This means implementing  restrictions  to  prevent  potentially  dangerous  batch  processes  from running across multiple isolation zones simultaneously. Operational isolation is enforced at many layers of the Dropbox stack, but one partic‐ ular  example  is  isolation  in  the  code  release  process  for  the  storage  system.  Some parts of the Dropbox code base are pushed to production every day, in use cases for which correctness or durability aren’t at stake, but for the underlying storage system, we adopt a multiweek release process that ensures that at least one fully replicated copy of data is stored on a thoroughly vetted version of the code at all times, as depic‐ ted in Figure 17-3.  Figure 17-3. Storage system code release process.  All new versions of storage code go through an extensive unit testing and integration testing process to catch any trivial errors, followed by overnight end-to-end durabil‐ ity testing on a randomized synthetic workload. We also run a set of long-running version-skew tests during which different code versions are run simultaneously on different nodes in the system; code pushes aren’t atomic, so we need to ensure that old and new versions of code are compatible with each other. After these tests have all passed, the release is automatically marked as “green” and is ready to be pushed to the staging cluster. The  staging  cluster  is  an  actual  production  deployment  of  the  storage  system  that stores a mirror of a subset of production data. This is a relatively large cluster storing many tens of petabytes across multiple geographical regions. The code runs on this cluster for a full week and is subjected to the same workloads, monitoring, and verifi‐ cation as the rest of the storage system. After a week in staging the individual “DRIs”  directly responsible individuals—the engineers who “own” each subsystem  sign off that each software component is operating correctly and without any significant per‐ formance degradation. At this point, the software is ready to be pushed to the first real production region.  282      Chapter 17: Engineering for Data Durability   Code is pushed to the first production region and runs for another week before it can be safely pushed to the remaining regions. Recall that the multiregion architecture of the storage system ensures that all data in one region is replicated in at least one other region, avoiding any risk of data loss from a single code push. Internal verification systems  are  tuned  to  detect  any  anomalies  within  this  one-week  timeframe,  but  in practice, any potential durability issues are caught well before making it this far. The multiregion deployment model is tremendously valuable, however, at catching any performance or availability issues that arise only at large scale. The  release  process  is  pipelined  so  that  there  are  always  multiple  versions  of  code rolling  out  to  production.  The  process  itself  is  also  automated  to  a  large  extent  to avoid any operator errors. For practical reasons, we also support a “break-glass” pro‐ cedure to allow sign-off on any emergency changes that need to be fast-tracked to production, along with extensive logging. The thoroughness in this release process allows us to provide an extremely high level of durability, but it’s clearly overkill for the vast majority of use cases. Operational isolation  comes  at  a  cost,  which  is  usually  measured  in  inconvenience  or  engineer frustration. At a small company, it can be incredibly convenient to run a batch job across all machines simultaneously, or for engineers to be able to SSH into any server in the fleet. As a company grows, though, investments in operational isolation not only guard against major disasters, but also allow a team to move faster by providing guardrails within which to iterate and develop quickly. Protection Your biggest durability threat is yourself. In an ideal world, we would always catch bad things before they happen. This is an unrealistic goal, of course, but still worth investing significant effort in attempting to achieve. In addition to safeguards to prevent disasters, you need a well-thought-out set of recovery mechanisms to mitigate any issues that slip through the cracks. Testing The first protection most engineers think of is simply testing. Everyone knows that you need good tests to produce reliable software. Companies often invest significant effort  in  unit  testing  to  catch  basic  logic  errors  but  underinvest  in  comprehensive end-to-end integration testing. Mocking out parts of the software stack allows fine- grained  testing  but  can  hide  complex  race  conditions  or  cross-system  interactions that occur in a real production deployment. In any distributed system, there is no substitute  for  running  the  full  software  stack  in  a  test  environment  and  validating correctness  on  long-running  workloads.  In  more  advanced  use  cases,  you  can  use fault-injection to trigger failures of system components during this integration test‐  Protection      283   ing. In our experience, codepaths that handle failures and corner cases are far more error-prone than more common flows. Safeguards As mentioned previously, the biggest durability risk in a system is often yourself. So how do you guard against your own fallibility? Let’s begin with an example of one of the worst production issues in the history of Dropbox. Many years ago, an operator accidentally left out a set of quotation marks when run‐ ning a distributed shell operation across a set of databases. The operator intended to run  dsh --group "hwclass=database lifecycle=reinstall" reimage.sh  but instead typed the following:  dsh --group hwclass=database lifecycle=reinstall reimage.sh  This is an easy mistake to make, but the end result was disastrous. This command caused the command lifecycle=reinstall reimage.sh to attempt to run on every database in the fleet instead of just running reimage.sh on databases that were slated for reinstall, taking down a large fraction of our production databases. The immedi‐ ate impact of this was a widely publicized two-day outage during which a number of Dropbox services were unavailable, but the long-term impact was a process of signifi‐ cant investment to prevent any such events from ever happening again. The immediate point of reflection following an outage such as this is that the operator is not to blame. If it’s possible for a simple mistake to cause a large-scale outage, that’s a process failure, not a personnel failure. Don’t hate the player, hate the game. There are many obvious safeguards to implement in a situation like this. We added access controls that prevent rebooting a live database host—a very basic protection, but one that’s often overlooked. We changed the syntax of the distributed shell com‐ mand  to  make  it  less  vulnerable  to  typos.  We  added  isolation-based  restrictions within our tooling that reject any distributed command that is run simultaneously across multiple isolation domains. Most important, however, we significantly inves‐ ted in automation to obviate the need for an operator to ever need to run a script like this, and instead relied on systems more reliable than a human at a keyboard. The need for protections must be maintained as an engineering principle so that safe‐ guards are developed alongside initial deployment of the systems they protect. Recovery Failures will happen. The mark of a strong operational team is the ability to recover from disasters quickly and without long-term impact on customers.  284      Chapter 17: Engineering for Data Durability   An important design consideration is to attempt to always have an undo button— that is, to design systems such that there is sufficient state to reverse an unintended operation or to recover from an unexpected corruption. Logs, backups, and historical versions  can  greatly  aid  in  recoverability  following  an  incident,  particularly  when accompanied by a well-rehearsed procedure for actually performing the recovery. One technique for mitigating the impact of dangerous transformations is to buffer the  underlying  mutation  for  sufficient  time  to  run  verification  mechanisms  on  the new state. The file deletion life cycle at Dropbox is designed in such a way so as to prevent live-deleting any object in the system, and instead wraps these transforma‐ tions with comprehensive safeguards, as demonstrated in Figure 17-4.  Figure 17-4. Object deletion flow  Application-level file deletion at Dropbox is a relatively complex process involving user-facing restoration features, version tracking, data retention policies, and refer‐ ence  counting.  This  process  has  been  extensively  tested  and  hardened  over  many years, but the inherent complexity poses a risk from a recovery perspective. As a final line of defense, we protect against deletions on the storage disks themselves, to guard against any potential issues in these higher layers. When  a  delete  is  issued  to  a  physical  storage  node,  the  object  isn’t  immediately unlinked. Instead, it is moved to a temporary “trash” location on the disk for safe- keeping while we perform further verification. A system called the Trash Inspector iterates over all volumes in trash and ensures that all objects have either been legiti‐ mately deleted or safely moved to other storage nodes  in the course of internal file‐ system operations . Only after trash inspection has passed and a given safety period has elapsed is the volume eligible for unlinking from disk via an asynchronous pro‐ cess that can be disabled during an emergency. Recovery mechanisms are often expensive to implement and maintain, especially in storage use cases in which they impose additional storage overhead and higher hard‐ ware  utilization.  It’s  thus  important  to  analyze  where  irrevocable  transformations occur in the stack and to make informed decisions about where recovery mechanisms need to be inserted to provide comprehensive risk mitigation.  Protection      285   Verification You will mess up; prioritize failure detection. In a large, complex system the best protection is often the ability to detect anomalies and recover from them as soon as they occur. This is clearly true when you are moni‐ toring availability of a production system or when tracking performance characteris‐ tics, but it is also particularly relevant when you’re engineering for durability. The Power of Zero Is your storage system correct? Is it really, really correct? Can you say with 100% con‐ fidence that the data in your system is consistent and none of it is missing? In many storage systems this is certainly not the case. Maybe there was a bug five years ago that left some dangling foreign key relations in your database. Maybe Bob ran a bad database split a while ago that resulted in some rows on the wrong shards. Maybe there’s a low background level of mysterious 404s from the storage system that you just keep an eye on and make sure the rate doesn’t go up. Issues like this exist in sys‐ tems everywhere, but they come at a huge operational cost. They obscure any new errors that occur, they complicate monitoring and development, and they can com‐ pound over time to further compromise the integrity of your data. Knowing with high certainty that a system has zero errors is a tremendously empow‐ ering  concept.  If  a  system  has  an  error  today  but  didn’t  yesterday,  something  bad happened  between  then  and  now.  It  allows  teams  to  set  tight  alert  thresholds  and respond rapidly to issues. It allows developers to build new features with the confi‐ dence that any bugs will be caught quickly. It minimizes the mental overhead of hav‐ ing  to  reason  about  corner  cases  regarding  data  consistency.  Most  important, however, it allows a team to sleep well at night knowing that they are being worthy of their users’ trust. The first step toward having zero errors is knowing whether you have any errors to begin  with.  This  insight  often  requires  significant  technical  investment  but  pays heavy dividends. Verification Coverage At Dropbox, we built a highly reliable geographically distributed storage system that stores  multiple  exabytes  of  data.  It’s  a  completely  custom  software  stack  from  the ground up, all the way from the disk scheduler to the frontend nodes servicing exter‐ nal traffic. Although developing this system in-house was a huge technical undertak‐ ing,  the  largest  part  of  the  project  wasn’t  actually  writing  the  storage  system  itself. Significantly more time and effort were devoted to developing verification systems  286      Chapter 17: Engineering for Data Durability   than the underlying system they are verifying! This might seem surprising outside the context of a system that is required to be absolutely correct at all times. We operate a deep stack of verification systems at Dropbox. In aggregate, these sys‐ tems generate almost as much system load as the production traffic generated by our users! Some of these verifiers cover individual critical system components and some provide  end-to-end  coverage,  with  the  full  stack  designed  to  provide  immediate detection of any serious data correctness issues.  Disk Scrubber The Disk Scrubber runs on every storage node and continually reads each block on disk,  validating  the  contents  against  our  application-level  checksums.  Internal  disk checksums and S.M.A.R.T. status reporting provide the illusion of a reliable storage medium,  but  corruptions  routinely  slip  through  these  checks,  especially  in  storage clusters  numbering  hundreds  of  thousands  or  millions  of  disks.  Blocks  on  disk become silently corrupted or go missing, disks fail without notice, and fsyncs don’t always fsync. The Disk Scrubber finds errors on disks every single day at Dropbox. These errors trigger an automated mechanism that re-replicates the data and remediates the disk failure.  Rapid  detection  and  recovery  from  disk  corruption  is  crucial  to  achieving durability  guarantees.  Recall  that  the  durability  formula  for  R loss   given  earlier  in this  chapter  depends  heavily  on  MTTR  from  failure.  If  MTTR  is  estimated  at  24 hours but it takes a month to notice a silent disk corruption, you will have missed your durability targets by several orders of magnitude.  Index Scanner The Index Scanner continually iterates over the top-level storage indices, looks up the set of storage nodes that are meant to be storing a given block, and then checks each of these nodes to make sure the block is there. The scanner provides an end-to-end check within the storage system to ensure that the storage metadata is consistent with the blocks stored by the disks themselves. The scanner itself is a reasonably large sys‐ tem totaling many hundreds of processes, generating over a million checks per sec‐ ond.  Storage Watcher One challenge we faced when building verifiers was that if the same engineer who wrote a component also implements the corresponding verifier, they can end up bak‐ ing any of their broken assumptions into the verifier itself. We wanted to ensure that we had true end-to-end black-box coverage, even if an engineer misunderstood an API contract or an invariant.  Verification      287   The  Storage  Watcher  was  written  by  an  SRE  who  wasn’t  involved  in  building  the storage system itself. It samples 1% of all blocks written to the system and attempts to fetch them back from storage after a minute, an hour, a day, a week, and a month. This  provides  full-stack  verification  coverage  if  all  else  fails  and  alerts  us  to  any potential issues that might arise late in the lifetime of a block. Watching the Watchers There’s nothing more comforting than an error graph that is literally a zero line, but how do you know that the verifier is even running? It might sound silly, but a verifi‐ cation system that never reports any errors looks a lot like a verifier system that isn’t working at all. At  steady  state,  the  Index  Scanner  mentioned  earlier  doesn’t  actually  report  any errors, because there aren’t any errors to report. At one point early in the develop‐ ment of the system the scanner actually stopped working, and it took us a few days to notice! Clearly this is a problem. Untested protections don’t protect anything. You should use a verification stack in conjunction with disaster recovery training to ensure that durability issues are quickly detected, appropriate alerts fire, and failover mechanisms provide continual service to customers. In our production clusters, these tests involve nondestructive changes like shutting down storage nodes, failing data‐ bases,  and  black-holing  traffic  from  entire  clusters.  In  our  staging  clusters,  this involves much more invasive tests like manually corrupting blocks on disk or forcing recovery of metadata from backup state on the storage nodes themselves. These tests not only demonstrate that the verification systems are working, but allow operators to train in the use of recovery mechanisms. Automation You don’t have time to babysit. One  of  the  highest-leverage  activities  an  SRE  can  engage  in  is  investing  in  strong automation. It’s important not to automate too fast: after all, you need to understand the problem before attempting to solve it. But days or weeks of automation work can save many months of time down the track. Automation isn’t just an efficiency initia‐ tive, however; it has important implications for correctness and durability. Window of Vulnerability As  discussed  previously,  MTTR  has  a  significant  impact  on  durability  because  it defines the window of vulnerability within which subsequent failures might cause a catastrophic  incident.  There  is  a  limit  to  how  tight  you  can  set  pager  thresholds, though, and to how many incidents an operator can respond to immediately. Auto‐ mation is needed to achieve sufficiently low response times.  288      Chapter 17: Engineering for Data Durability   One illustrative example is to compare a traditional Redundant Array of Independent Disks   RAID   array  to  a  distributed  storage  system  that  automatically  re-replicates data to other nodes if a disk fails. In the case of a RAID array, it might take a few days for an operator to show up on-site and replace a specific disk, whereas in an automa‐ ted system, this data might be re-replicated in a small number of hours. The same comparison applies to primary-backup database replication: smaller companies will often require an operator to manually trigger a database promotion if the primary fails, whereas in larger infrastructure footprints, automatic tooling is used to ensure that the availability and durability impact from a database failure is kept to a mini‐ mum. Operator Fatigue Operator fatigue is a real problem, and not just because your team will eventually all quit if they keep getting paged all day. Excessive alerting can cause alert blindness by which operators end up overlooking legitimate issues. We’ve observed situations in which  an  operator  has  been  tempted  just  to  pipe  the  Unix  yes  command  into  a command-line tool that asked hundreds of times whether various maintenance oper‐ ations were authorized, which defeated the purpose of the checks to begin with. Rules  and  training  can  go  only  so  far.  Good  automation  is  ultimately  a  necessary ingredient to allow operators to focus on high-leverage work and on critical inter‐ rupts rather than become bogged down in error-prone busy-work. As a system scales in size, the operational processes that accompany it also need to scale. It would be impossible for a team running many thousands of nodes to man‐ ually intervene any time there was a minor hardware issue or a configuration prob‐ lem.  Management  of  core  systems  at  Dropbox  are  almost  entirely  automated  by  a system  that  supports  plug-ins  both  for  detecting  problems  within  the  fleet  and  for safely resolving them, with extensive logging for analysis of persistent issues. Systems  that  have  the  ability  to  automatically  remediate  alerts  need  to  be  imple‐ mented extremely carefully, given that they have the power to accidentally sabotage the infrastructure that they’re designed to protect. Automatic remediation systems at Dropbox are subject to the same isolation protections discussed earlier in this chap‐ ter, along with stringent rate limits to ensure that no runaway changes are performed before an operator can intervene and recover. It is also important to implement sig‐ nificant monitoring for any trends in production issues—it is very easy for a system such as this to hide issues by automatically fixing them, obscuring any emergent neg‐ ative trends in hardware or software reliability.  Automation      289   Reliability The primary motivation for automation is simply reliability; you are not as reliable as a shell script. You can test, audit, and run automation systems continuously in ways in which you can’t rely on an operator. One  example  of  a  critical  automation  system  at  Dropbox  is  the  disk  remediation workflow,  shown  in  Figure  17-5,  which  is  designed  to  safely  manage  disk  failures without operator intervention.  Figure 17-5. Automatic disk-remediation process  When a minor disk issue such as a corrupt block or a bad sector is detected, the data is recovered from other copies in the system and re-replicated elsewhere, and then the storage node is just allowed to continue running. Issues like this are sufficiently routine such that they don’t justify further action unless disk failure rates begin shift‐ ing over time. After a certain number of errors, however, the system determines that there are serious problems with the disk or filesystem, recovers and re-replicates data that was on the disk, and then reprovisions the disk with a new clean filesystem. If this cycle repeats multiple times, the disk is marked as bad, the data is recovered and re-replicated, and a ticket is filed for data center operators to physically replace the disk. Because the data is immediately re-replicated, the replacement of the physical disk can happen any time down the track, and usually happens in infrequent batches. After a disk is removed from the machine it is kept for a safe-keeping period before eventually being physically shredded on-site. This entire process is completely automatic and involves no human intervention up until the disk is physically pulled from the machine. The process also maintains the invariant that no operator is ever allowed to touch a disk that holds critical produc‐ tion data. Disks stay in the system until all the data that they held has been recon‐ structed from other copies and re-replicated elsewhere. Investing in development of this  automation  system  allows  seamless  operation  of  an  enormous  storage  system with only a handful of engineers.  290      Chapter 17: Engineering for Data Durability   Conclusion There was a lot of stuff proposed in this chapter, and not every company needs to go as far as geo-distributed replication or complete alert autoremediation. The impor‐ tant point to remember is that the issues that will bite you aren’t the disk failure or database outage you expect, but the black swan event that no one saw coming. Repli‐ cation is only one part of the story and needs to be coupled with isolation, protection, recovery,  and  automation  mechanisms.  Designing  systems  with  failure  in  mind means that you’ll be ahead of the game when the next unknown-unknown threatens to become the next unforeseen catastrophe.  James Cowling is a principal engineer at Dropbox and served as technical lead on the project to build and deploy the multi-exabyte geo-distributed storage system that stores Dropbox file data. James has also served as team lead for the Filesystem and Metadata storage teams. Before joining the industry, James received his PhD at MIT specializing in large-scale distributed transaction processing and consensus protocols.  Conclusion      291    CHAPTER 18 Introduction to Machine Learning for SRE  Ricardo Amaro, Acquia  Why Use Machine Learning for SRE? In clear and simple words: because it makes sense and mostly because we  now  can. SRE, fundamentally, is what happens when you ask a software engineer to design an operations function.1 This chapter is based on the presentation I did at DrupalCon Vienna. Here we will be exploring some machine learning solutions for a few SRE open questions:    How do we automate those repetitive tasks that just generate toil and that no one  wants to do?  future?    How do we look at data and preview what’s going to happen to our system in the    How do we reinforce “applying software engineering to an operations function”?  The  automation  of  operation  processes  is  a  critical  target  we  pursue.  As  artificial intelligence   AI   and  machine  learning  get  better,  the  tasks  that  we  can  automate increase. If we keep the historical data to programmatically react to something new, we will be able to fix the issue beforehand because the system will alert us as to what is going to happen instead of having someone manually analyzing the past results and trying to preview the future.  I’ve just picked up a fault in the AE35 unit. It’s going to go 100% failure in 72 hours.  —HAL 9000, 2001: A Space Odyssey  1 Ben Treynor Sloss, Google Engineering.  293   That gives us the chance to use our time for more innovative tasks and feature devel‐ opment. Although this certainly is not an overnight achievement, lately we have seen the line between the work of machines and humans grow thin. Through the advances that machine learning and automation can offer, we can enable greater productivity among teams and businesses. I’ve tried to make this chapter as simple as possible because it is intended for people who  want  to  learn  some  basics  about  how  to  explore  and  improve  automated response on critical systems and lower the level of manual operational work. The goal here is not to dive too deeply into machine learning  see the references at the end of the chapter  but just to give you an idea of how easy it is to get started playing with some of these techniques. Therefore, I go over the basics of AI implementations and then give some examples for using machine learning techniques, behavioral analytics, statistics, and specific tools for this domain. Why and How Should My Company Be Engaging in This? A  few  major  and  medium-sized  companies  are  adopting  AI  and  machine  learning technology for its usefulness in augmenting human understanding of complex inter‐ action and datasets by uncovering the unknowns. This enables preventing toil that could have been avoided and also frees resources to be more innovative and creative, bringing value to the businesses. That is the job of the SRE. Some SRE Problems Machine Learning Can Help Solve Supposedly, we could solve all the problems that arise for SREs using techniques of automatic  treatment  of  routines  and  carefully  observing  all  past  events,  inferring strategies for regeneration of processes and services. But that day is yet to come, and we now can foresee such horizons only by investigating innovative forms of automa‐ tion that subtly arrive to our work in the form of AI. In my day job, we have several thousands of instances running, supporting a huge number of production sites. They are generating dozens of petabytes of data transfer‐ red  from  several  data  centers  and  the  corresponding  volume  of  logs  and  metrics. Although  this  can  seem  overwhelming  at  first,  we  are  continually  automating  and trying to find ways in which the machines can do the job for us. Nevertheless, alerts exist, in some amount, that might get out of hand if left unchecked. Here is a list of some operational challenges that we would like to solve in order to improve the SRE function:    Automate noise reduction to filter out a specific stream   Look for outliers with anomaly detection—for example, cluster malfunctions.   Automate workflows around “situations,” not individual alerts.  294      Chapter 18: Introduction to Machine Learning for SRE     Automate ticket categorization based on patterns of behavior.   Forecast short-term for service levels and long-term for capacity planning.  In addition to these are other existing solutions—for example, for text analysis like spam filtering, sentiment analysis, and information extraction. All of these will hopefully reduce toil and alerts for humans by letting the machine do its job. The Awakening of Applied AI As senior site reliability engineer at my organization, I tend to search for long-term solutions that make the machine do the work for us—the best path to reach durable automation. This is the story of an investigation that is still ongoing. It has left me sometimes frus‐ trated, there being so many complex software options  see Figure 18-1 , apart from a lot of work and hours spent collecting server data, arranging it, and processing it. But on one of those tiring nights, after putting the kids to bed, I sat down and randomly picked up a movie from 1968 to watch. This single masterpiece, produced 50 years ago, had so much to do with this research that it inspired me with a few clear ideas about what we could do. Kubrick’s 2001: A Space Odyssey has an almost encrypted message  that  intelligence  is  the  higher  paradigm  of  evolution;  it  combines  science, technology,  philosophy,  history,  memory—much  like  AI  combines  them.  The movie’s only problem was that it was too generous in foreseeing the future that would happen for the year 2001. It’s 2018 and we’re not quite there yet. The visionary for the movie was Arthur C. Clarke, who combined AI with a Fully Automated Service capable of forecasting system and hardware failures hours in advance. HAL 9000 is the dream  or the nightmare  of a self-sustained, self-regulated, and flawless machine that serves both the crew and the mission in order to achieve the objectives defined by humans. I strongly believe that we are now, in 2018, very close to 2001’s incredible imagined achievements. Data science and machine learning software have been available for some time now. The  languages  they  use  makes  them  more  reachable  by  engineers  to  explore  new ways to apply machine learning. In Figure 18-1, we see Python and the R language being the clear winners in this field. Notably, Anaconda, TensorFlow, and scikit-learn all  use  Python  for  interfacing  with  the  user.  For  the  hands-on  section  later  in  this chapter, we use TensorFlow, Python, and some Keras.  The Awakening of Applied AI      295   Figure 18-1. Different software for machine learning  source: https:  www.kdnug gets.com 2017 05 poll-analytics-data-science-machine-learning-software-leaders.html  What Is Machine Learning? Machine learning refers to the statistical methods used to create algorithms that learn to improve performance over time, with increased emphasis on using computers to statistically estimate complicated functions and proving confidence intervals around these functions. These methods use two central approaches to statistics: frequentist estimators and Bayesian inference. The AI market is projected to swell to $3 trillion by 2024, and machine learning, which is a part of AI, is the biggest chunk of this growth. Every time you use Google Search to find something, every time Facebook recognizes your friend’s face, every time your computer marks something as spam—those are all examples of machine learning. Machine learning works by using intelligent agents.2 An agent is an AI concept; it is anything that can be viewed as perceiving its environ‐ ment through data or sensors and acting upon that environment through data or sen‐  2 Russell, S. J. and Peter Norvig. Artificial Intelligence—A Modern Approach. Upper Saddle River, NJ: Pearson  Education, 2003, Chapter 2.  296      Chapter 18: Introduction to Machine Learning for SRE   sors  and  acting  upon  that  environment,  toward  achieving  goals,  by  actuators  and displaying, sending, or writing data. We can divide most machine learning algorithms into three broad categories: super‐ vised learning, unsupervised learning, and reinforcement learning. Figure 18-2 illus‐ trates these categories.  Figure 18-2. Machine learning categories What Do We Mean by Learning? Let’s look at a modern definition of learning in terms of the machine:  A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.  —Tom Mitchell3  Let’s give a clear example for learning to play checkers:    E = the experience of playing many games of checkers   T = the task of playing checkers.  3 Definition proposed by Tom Mitchell in 1998, Machine Learning Research.  What Is Machine Learning?      297     P = the probability that the program will win the next game  We can solve many kinds of tasks within the three main types of learning: In  supervised  learning,  the  agent  receives  some  examples  of  input  to  output  and learns a function that maps from input to output. Here are some of supervised learn‐ ing’s common tasks: Classification  In this type of task, the computer program is asked to specify to which of k cate‐ gories  some  input  belongs.  Example:  find  if  the  input  server  data  belongs  to either “Healthy,” “Unhealthy,” or some other class.  Classification with missing inputs  When some of the inputs might be missing, rather than providing a single classi‐ fication function, the learning algorithm must learn a set of functions.  Regression  A regression task is a kind of machine learning problem that tries to predict a continuous  typically, floating-point  value for an input based on previous infor‐ mation. Example: predict storage usage given a set of features of a server dataset. In unsupervised learning, the agent learns patterns in the input, despite the fact that no concrete feedback is given. Following is the most common task: Clustering  Clusters  of  useful  input  examples  are  detected.  For  example,  a  machine  might gradually develop a concept of “good site traffic” and “bad site traffic” without ever having a human teaching intervention.  In  reinforcement  learning,  the  agent  learns  by  using  rewards   reinforcements .  For example, it can be penalized for not spotting a security occurrence in a traffic stream or awarded points for every correct finding. The agent will then do better over time. Of  course,  other  tasks  and  types  of  tasks  are  possible,  like  anomaly  detection,  and there are still many being discovered as of this writing. The types of tasks we list here are intended only as examples of machine learning basics; keep in mind that AI has a larger scope than machine learning. From Chess to Go: How Deep Can We Dive? Historically, the reason why AI gained traction in the first place and made huge pro‐ gress in dedicated computation power has been due to search algorithms and their use in games. Although solving problems using search algorithms tends to be slower, code also needs to be implemented that, most of the time, demands a lot of memory with huge time complexity. This includes not only the MiniMax algorithm, shown in Figure 18-3 solving a simple tic-tac-toe puzzle, but also other search algorithms like  298      Chapter 18: Introduction to Machine Learning for SRE   Breadth-First  Traversal,  Depth-First  search,  A*  search,  and  many  more,  which  are used in modern games. You can find more information on these in the books refer‐ enced in “Further Reading” on page 320.  Figure 18-3. Solving problems by searching several levels of possibilities  To explain better the impact of games on AI development over the years, I would like to tell you a short story about two specific human–machine games. In 1997, in the second of two six-game chess matches, Garry Kasparov, the world chess champion, lost a chess game to Deep Blue, a machine from IBM. This outcome got everyone thinking that machines like Deep Blue would solve very important problems. In 2015, nearly 20 years later, the ancient Chinese game Go, which has many more possible moves than chess, was won by DeepMind,4 using a program called AlphaGo, via  the  application  of  deep  reinforcement  learning,  which  is  a  much  different approach than using search algorithms. Table 18-1 compares the two machines and their methodologies.  4 DeepMind Technologies is a British artificial intelligence company founded in September 2010. It was  acquired by Google in 2014.  What Is Machine Learning?      299   Table 18-1. The two machines and their methodologies Deep Blue; Chess; May 1997 DeepMind; AlphaGo; October 2015    Brute force   Search Algorithm   Developer: IBM   Adversary: Garry Kasparov    Deep learning   Machine learning   Developer: Google   Adversary: Fan Hui  But the first game that gripped the world’s attention in AI was checkers, with the pio‐ neer Arthur Samuel, who coined the term “machine learning” back in 1959. This first game used alpha–beta pruning. Since then, AI has been alternating between being the key to our civilization’s future and tossed on to technology’s trash heap. Today it’s absolutely unfair to say “human versus machine” in any of these and other games given that the machines are in a clear position of strength. Over the past few years,  we’ve  seen  AI  exploding,  mostly  due  to  Graphics  Processing  Units   GPUs  making parallel processing faster and cheaper as well as a flood of data of every stripe: images, text, transactions, logs—you name it. Why Now? What Changed for Us? In the SRE world, deployments are getting faster and faster, and we  humans  can react only so fast. We need to automate more and get decisions right. And we need smarter monitoring. The rise of AI is happening now because we have enough data5 from all of the big data initiatives out there and cheap GPUs6 for machine learning algorithms to churn on. In addition, what’s surprising about deep learning is how simple it is. Last decade, no one suspected that we would achieve these incredible results with respect to machine perception problems. Now, it turns out that all you need is sufficiently large paramet‐ ric models trained with gradient descent on sufficiently many examples.  5 According to a report from IBM, “10 Key Marketing Trends For 2017,” every day we create 2.5 quintillion  bytes of data and 90% of the data today has been created in the past two years alone. That corresponds to 10 million Blu-ray discs, which if stacked would be as tall as four Eiffel Towers.  6 Nvidia GeForce GPU cards were initially ramped up in terms of GigaFlop  GFLOP  s capabilities by games in need of good graphical performance to satisfy the greedy processing requirements of image rendering. GPUs surpassed CPUs in terms of GFLOP s around 2004, and today the best graphical cards are much faster than 5,000 GFLOP s, while CPUs are much slower at around 1,000 GFLOP s.  300      Chapter 18: Introduction to Machine Learning for SRE   It’s not complicated, it’s just a lot of it.  —Richard Feynman, in the 1972 interview, Take the World from a Different Point of View  So  that  gives  us  a  fairly  good  idea  of  why  we  now  can  do  deep  learning  in  such  a reachable way and who to blame for the whole thing—gamers and games. Next, we’ll explore three different machine learning techniques: decision trees, neural networks, and long short-term memory networks. Decision trees  Decision support tools that use a tree-like graph or model of decisions and their possible  consequences,  including  chance–event  outcomes,  resource  costs,  and utility.  Long short-term memory  LSTM  networks  Well suited to learn from experience to classify, process, and predict time series given time lags of unknown size and bound between important events.  Neural networks  A bit complicated to begin with, so let’s take a moment to dive into them sepa‐ rately.  What Are Neural Networks? In  this  section,  we  try  to  answer  this  question  by  briefly  walking  you  through  the basics of neural networks, deep learning techniques, and how to apply them to real data problems. Neurons and Neural Networks Unfortunately, it’s not possible for us to jump right into some deep learning magic Python for SRE without first talking a bit about neural networks to get a basis for our code. A  neural  network  is  composed  of  artificial  neurons.  An  artificial  neuron  is  just  a mathematical function conceived as a model of the biological neuron. It receives one or more inputs, and sums them to produce an output. The activation function usually has a sigmoid, or step function, shape, but can also take the form of other functions. Figure 18-4 demonstrates this relation.  What Are Neural Networks?      301   Figure 18-4. A biological neuron and an artificial neuron.  We can achieve the artificial neuron by using a simple Python function that in turn will be grouped in layers, thus creating the neural network. We directly interact with some of these layers, like the input and output layers, while some are hidden layers, as shown in Figure 18-5.  302      Chapter 18: Introduction to Machine Learning for SRE   Figure 18-5. An example of a neural network with two hidden layers How and When Should We Apply Neural Networks? The following checklist will help you get started with using a neural network to per‐ form machine learning tasks on your data:  1. See whether a traditional algorithm will work before applying a neural network. 2. Check whether a neural network can solve the problem. 3. Choose a library or language for that neural network architecture. 4. Format and batch the data so that is consumable. 5. Augment the data to increase the sample size for better training. 6. Feed batches to the neural network. 7. Validate during training and improve by using test data. 8. Test your model and save it for future use.  After we decide that we are indeed using a neural network or another machine learn‐ ing model, we need to get the raw data, which will be unstructured most of the time. Cleaning  and  structuring  that  data  will  be  the  biggest  chunk  of  the  process,  while building the model, testing, and predicting will be the last part of the process.  What Are Neural Networks?      303   Keep in mind that neural networks require clear and informative data  and mostly big  data   with  which  to  train.  The  more  training  you  can  provide,  the  better  your model will operate. And you need to have an appropriate type of neural network to solve the problem. Each problem has its own demands. So, the data defines the way you approach the problem. For instance, if the problem is of sequence generation,  recurrent neural networks are more suitable. But if it is an image-related problem, you would probably be better off using convolutional neural networks. Last but not least, running a deep neural network model can have substantial hardware requirements. Neural  networks  go  way  back,  but  they  are  more  accepted  now  because  computa‐ tional resources are better, cheaper, and more powerful. If you want to solve a real- life problem with neural networks, be ready to spend some money on computational power! In the end, you can even decide that the best thing for your problem is to apply sim‐ ple statistics or even common engineering data structure and algorithms. So, don’t rush applying machine learning; first carefully analyze the problem and the data. What Kinds of Data Can We Use? There are plenty of datasets out there that we can use. Some are more in line with what we normally have handy in our SRE dashboards; other sets are more generic or come from the most unexpected sources. Take your time to think about what you have, what you need to assemble, and how much you have of it. Don’t forget that the more data you have, the better results you can achieve. Here are the most common sources: Machine data  Includes log messages created by applications, trap events created by infrastruc‐ ture, and alerts generated by tools.  Network communications between different systems.  Traffic data  Agent data  From byte-code instrumentation and call-stack sampling. Code diagnostic tools often generate this data and are commonly used by development and QA teams.  Synthetic data  From synthetic transactions and service checks.  Human sentiment data  The electronic coding of human communications, often in the form of service- desk tickets and social media messages, capturing perceptions and sentiment.  304      Chapter 18: Introduction to Machine Learning for SRE   Practical Machine Learning In this section, we use some tools and dive into some practical examples that we can apply to SRE. Popular Libraries for Neural Networks Machine learning libraries’ popularity, shown in Figure 18-6, has been changing over time.  Figure 18-6. Recent GitHub popularity metrics  forks  of machine learning libraries  Popular libraries include the following:    TensorFlow   Keras   Caffe   SkLearn   Theano   Torch  Over the course of this investigation we worked with all of these, but in this chapter, I focus mostly on Keras and TensorFlow.  Practical Machine Learning      305   Practical Machine Learning Examples OK, so now let’s play with some basic machine learning SRE-oriented exercises so that you get a sense of how easy it is to get started.  Installing Python, IPython, and Jupyter Notebook For this practical guide, you need to install Python from python.org, and then run the pip command, as demonstrated here  Pip is already installed on Python version 2.7.9 and above or version 3.4 and above :  pip install --upgrade pip  I also advise that you use IPython and Jupyter Notebooks in order to have your work saved in files as you proceed with testing:  pip install jupyter    jupyter notebook  Now check your installation by creating a new notebook on the browser page that will open at http:  localhost:8888, as illustrated in Figure 18-7. Figure 18-8 shows a Jupyter notebook solving the Fibonacci sequence.  Figure 18-7. Creating a new notebook  306      Chapter 18: Introduction to Machine Learning for SRE   Figure 18-8. The Jupyter notebook solving the Fibonacci sequence  Decision trees To give concrete examples, let’s now look at some demonstrations. The first one uses a simple decision tree to determine whether a server is healthy or not, based on our sample data against the current CPU, RAM, and storage percent usage over the past six hours. These algorithms have always influenced a wide area of machine learning and are used both in classification and regression. In this kind of analysis, we can use a decision tree to visually and explicitly represent decisions and decision making. The model follows the branching structure of a tree as it makes decisions and is frequently used in data mining in order to reach a particular goal. First, you need to install NumPy, SciPy, and scikit-learn from the command line:  pip install numpy scipy scikit-learn graphviz  Running the following Python code in the notebook, we will train a decision tree:  from sklearn import tree import graphviz   training data:  Status,CPU,RAM,STORAGE data = [  ['healthy',45, 32, 65],  ['unhealthy', 87, 67, 100],  ['unhealthy', 100, 1, 1],  ['unhealthy', 76, 70, 90],  ['unhealthy', 1, 1, 100],  Practical Machine Learning      307    ['unhealthy', 31, 100, 50],  ['healthy', 12, 65, 39],  ['healthy', 20, 10, 46],  ['unhealthy', 100, 50, 50],  ['healthy', 34, 70, 37],  ['healthy', 1, 50, 50],  ['unhealthy', 50, 50, 100],  ['healthy', 50, 1, 50],  ['unhealthy', 1, 100, 1],  ['healthy', 50, 50, 1],  ['healthy', 53, 53, 80], ]  metrics = [row[1:] for row in data] states = [row[0] for row in data]  In this initial part of the code, notice that we begin by importing the tree model from sklearn library and also  graphviz for the graph that you’ll see in Figure 18-9. The data  variable  is  populated  with  samples  of  several  healthy  servers.  State  names  are given to the y-axis, and metrics to the x-axis:  mytree = tree.DecisionTreeClassifier   mytree = mytree.fit metrics, states   After populating the dataset, we instantiate the mytree object with the decision tree classifier from scikit-learn and train the model with our previous data.  mytree.fit takes  x-axis,y-axis  from earlier:   is 10% cpu, 80% RAM, 10% Storage healthy? print "10% CPU, 80% RAM, 10% Storage",  mytree.predict [[10, 80, 10]]     is 80% cpu, 10% RAM, 90% Storage healthy? print "80% CPU, 10% RAM, 90% Storage  high ", mytree.predict [[80, 10, 90]]    is 60% cpu, 90% RAM, 10% Storage healthy? print "60% CPU, 90% RAM  high , 10% Storage", mytree.predict [[60, 90, 10]]    This results in the following output:  10% CPU, 80% RAM, 10% Storage ['healthy'] 80% CPU, 10% RAM, 90% Storage  high  ['unhealthy'] 60% CPU, 90% RAM  high , 10% Storage ['unhealthy']  Next,  we  test  and  print  the  results  of  the  prediction,  as  illustrated  in  Figure  18-9. Notice  that  decision  tree  was  able  to  figure  out  that  the  unhealthy  ones  have  high CPU and RAM usage, as we intended with our training:   Visualize the decision tree dot_data = tree.export_graphviz mytree, feature_names=['CPU','RAM','Storage'], class_names=['healthy','unhealthy'], filled=True, rounded=True,out_file=None   graphviz.Source dot_data   308      Chapter 18: Introduction to Machine Learning for SRE   Figure 18-9. Resulting decision tree: right side branches are false, and left side branches get closer to true until the last branch  Finally, let’s briefly analyze how this works. Figure 18-9 shows the decision tree we created with our training. The data upon which it made its decision passes through a series of Boolean decisions based on the values it has for comparison. If any of the checks  return  false,  it  returns  'unhealthy',  but  if  all  checks  return  true,  we  have 'healthy'. This demonstration has shown that by using a simple set of data and a decision tree, we can already do a lot.  A neural network from scratch Next,  let’s  explore  some  more  examples,  like  some  simple  Python  code  to  train  a three-layer neural network from scratch. First, just using NumPy, we can construct a simple neuron using a “Sigmoid.” This is the activation of a neuron, a function that will map any value to a value between 0 and 1 so that it creates probabilities out of numbers:  import numpy as np  def nonlin x,deriv=False :  Practical Machine Learning      309     if deriv==True :     return  x* 1–x     return  1  1+np.exp -x     Figure 18-10 shows the preceding code in action.  Figure 18-10. A Sigmoid function  The artificial neuron that we saw in Figure 18-4 has different inputs  x1...xn  with dif‐ ferent weights  w1...wn . The weighted sum of these inputs is then passed through a Sigmoid or a Heaviside step function f, as shown here:   Initialize the dataset as a matrix with input Data: X = np.array [[0,0,1],              [0,1,1],              [1,0,1],              [1,1,1]]    Output Data with one output neuron each: Y = np.array [[1],               [0.7],               [1],               [0]]   Seed to make them deterministic np.random.seed 1   Create synapse matrices. synapse0 = 2 * np.random.random  3, 4   - 1 synapse1 = 2 * np.random.random  4, 1   - 1  310      Chapter 18: Introduction to Machine Learning for SRE   Next, we initialize the dataset as a matrix with input data  x . Each row is a different training example and each column represents a different neuron. The output data  y  has  one  output  neuron  each,  and  we  seed  them  to  make  them  deterministic;  this yields random numbers with the same starting point  useful for debugging , so we can get the same sequence of generated numbers every time we run the program. To complete this simple neural network, we create two synapse matrices and initialize the weights of the neural network. We have just created a neural network with two layers of weights:   Training code  loop  for j in xrange 100000 :      Layers layer0,layer1,layer2     layer0 = X      Prediction step     layer1 = nonlin np.dot layer0, synapse0       layer2 = nonlin np.dot layer1, synapse1        Get the error rate     layer2_error = Y - layer2      Print the average error     if j % 10000  == 0:         print "Error:" + str np.mean np.abs layer2_error         Multiply the error rate     layer2_delta = layer2_error * nonlin layer2, deriv=True       Backpropagation     layer1_error = layer2_delta.dot synapse1.T       Get layer1's delta     layer1_delta = layer1_error * nonlin layer1, deriv=True       Gradient Descent     synapse1 += layer1.T.dot layer2_delta      synapse0 += layer0.T.dot layer1_delta   The training code in the preceding example is a bit more involved, where we opti‐ mize the network for the given dataset. The first layer  layer0  is just our input data. The prediction step performs matrix multiplication between each layer and its syn‐ apse. We then run the Sigmoid function on the matrix to create the next layer. With the  layer1 layer2  prediction  of  the  output  in  layer2,  we  can  compare  it  to  the expected output data by using subtraction to get an error rate. We then keep printing the average error at a set interval to make sure it goes down every time. We multiply the error rate by the slope of the Sigmoid at the values in layer2 and do  backpropagation,7  which  is  short  for  “backward  propagation  of  errors”—i.e.,  what layer1 contributed to the error on layer2, and multiply layer2 delta by synapses 1’s transpose.  7 The primary algorithm for performing gradient descent on neural networks. First, the output values of each node are calculated  and cached  in a forward pass. Then, the partial derivative of the error with respect to each parameter is calculated in a backward pass through the graph.  Practical Machine Learning      311   Next, we get layer1’s delta by multiplying its error by the result of the Sigmoid func‐ tion and do gradient descent,8 a first-order iterative optimization algorithm for find‐ ing the minimum of a function, where we finally update weights. Now that we have deltas for each of our layers, we can use them to update our synapse rates to reduce the error rate even more every time we iterate. This produces the following:  This means the error is getting closer to zero, as our neural network learns. And if we print each layer2 and our objective:  Error:0.434545246367 Error:0.00426490134801 Error:0.00285547679431 Error:0.00226684843815 Error:0.00192718684831 Error:0.00170049171577 Error:0.00153593455208 Error:0.00140973826096 Error:0.00130913223749 Error:0.00122657710472  print "Output after training" print layer2  Output after training [[ 0.99998867]  [ 0.69999105]  [ 0.99832904]  [ 0.00293799]]     print "Initial Objective" print Y  Initial Objective [[ 1. ]  [ 0.7]  [ 1. ]  [ 0. ]]  we have successfully created a neural network using just NumPy and some math, and trained it to get closer to the initial objective by using backpropagation and gradient descent. This can be useful in bigger scenarios in which we teach a neural network to recognize  patterns  like  anomaly  detection,  sound,  images,  or  even  certain  occur‐ rences in our platform, as we will see.  8 A technique to minimize loss by computing the gradients of loss with respect to the model’s parameters, con‐  ditioned on training data. Informally, gradient descent iteratively adjusts parameters, gradually finding the best combination of weights and bias to minimize loss.  312      Chapter 18: Introduction to Machine Learning for SRE   Using TensorFlow and TensorBoard Google’s TensorFlow is nothing but the NumPy we just looked at with a huge twist, as we will see now. The major difference is that TensorFlow first builds a graph of all the operations to be done, and then when a “session” is called, it “runs” the graph. It’s built  to  be  scalable  and  takes  advantage  of  the  GPU  via  CUDA.  Keras,  another library, simplifies TensorFlow’s coding  see Figure 18-11 .  Figure 18-11. A tensor9  Keep in mind that TensorFlow doesn’t have “neurons,” per se, but it does love linear algebra. The neuron is a past fixation on the biological metaphor. Everything is just matrix math—or, for arbitrary dimensions, tensor math. This is the choice Tensor‐ Flow makes. It makes TensorFlow really flexible and lets it achieve efficient computa‐ tion that would be more difficult otherwise. You can install TensorFlow from the pip package manager:  pip install tensorflow  To test the installation, you can run this set of commands:  import tensorflow as tf a = tf.constant 1.0  b = tf.constant 2.0  c = a + b sess = tf.Session   print sess.run c    The resulting operation is diagrammed in Figure 18-12.  9 In mathematics, tensors are geometric objects that describe linear relations between geometric vectors,  scalars, and other tensors. Elementary examples of such relations include the dot product, the cross product, and linear maps.  Source: https:  en.wikipedia.org wiki Tensor.   Practical Machine Learning      313   Figure 18-12. Resulting operation  Ignore  any  warnings  about  the  CPU.  The  result  of  our  TensorFlow  run  should  be 3.0. The  preceding  test,  although  not  very  impressive,  shows  how  TensorFlow  declares things, before actually running them in a session. The following Python code,10 although not using a real dataset, does some data neu‐ ron training using TensorFlow: 1  import tensorflow as tf 2  3  x = tf.constant 1.0, name='input'  4  w = tf.Variable 0.8, name='weight'  5  y = tf.multiply w, x, name='output'  6  y_ = tf.constant 0.0, name='correct_value'  7  loss = tf.pow y – y_, 2, name='loss'  8  train_step = tf.train.GradientDescentOptimizer 0.025 .minimize loss  9  10 for value in[x, w, y, y_, loss]: 11     tf.summary.scalar value.op.name, value  12 13 session = tf.Session   14 15 summaries = tf.summary.merge_all   16 summary_writer = tf.summary.FileWriter 'log_simple_stats', session.graph  17 18 session.run tf.global_variables_initializer    19 for i in range 100 : 20      summary_writer.add_summary session.run summaries , i  21      session.run train_step  22      if  i % 10  == 0: print  session.run y   23 24 summary_writer.close    This code imports TensorFlow and creates a graph on it using the definitions from lines 3 through 8. Initially, the system takes the input 1.0 and returns 0.8, which is wrong because the 'correct_value' is 0.0. We need a way to measure how wrong the system is. Let’s call that measure the “loss” and give our system the goal of minimizing the loss in the  10 https:  www.oreilly.com learning hello-tensorflow; credit: Aaron Schumacher.  314      Chapter 18: Introduction to Machine Learning for SRE   training_step using the gradient descent optimizer to make the neuron learn what the value should be. On line 13, we start our TensorFlow session. Then, we prepare some summaries to visualize on the TensorBoard, and we finally run our session optimizing in a 100-step loop at line 19. The run shows the output getting closer and closer to 0 as expected by the optimiza‐ tion:  0.76 0.45504 0.272449 0.163125 0.0976692 0.0584782 0.035013 0.0209636 0.0125517 0.00751515  TensorBoard is a graphical dashboard that displays the summaries saved during the execution of one or more TensorFlow programs. You can obtain and visualize the graph and the values of this operation there:  25  !tensorboard --logdir=log_simple_stats     Starting TensorBoard b'54' at http:  localhost:6006    Press CTRL+C to quit   Notice that we ran the preceding optimization before starting TensorBoard, while the values are kept on log_simple_stats for later usage by TensorBoard. The results are now as expected, as demonstrated in Figures 18-13 and 18-14.  Figure 18-13. The loss function being minimized  Practical Machine Learning      315   Figure 18-14. The neuron’s output training 100 times  Looking at the Graphs tab on TensorBoard, we see our Main graph and an auxiliary GradientDescent graph that was used to find the minimum of our function, as depic‐ ted in Figure 18-15.  Figure 18-15. Main graph and an auxiliary GradientDescent graph in TensorBoard  Besides visualizing graphs and scalars data, TensorBoard also allows you to see image data, audio data, distributions, histograms, embeddings, and text. That’s why I think it’s a great tool that you should include in your machine learning for SRE learning path.  TensorBoard  produces  both  powerful  and  somewhat  complex  graphs.  How‐ ever, using TensorFlow with graph visualization can help you understand and debug them. There are a lot of ready-to-use TensorFlow models that you can find in the Tensor‐ Flow  GitHub  repository,  together  with  a  CPU  and  a  GPU  Dockerfile,  at  https:   github.com tensorflow models, including: Mnist  A basic model to classify image digits from the MNIST dataset.  316      Chapter 18: Introduction to Machine Learning for SRE   Resnet  Wide_deep  A deep residual network that you can use to classify both CIFAR-10 and Image‐ Net’s dataset of 1,000 classes.  A  model  that  combines  a  wide  model  and  deep  network  to  classify  census income data.  Time series: server requests waiting Up to this point, we haven’t yet seen the best out of machine learning for SRE. In my opinion, I believe that distinction rests with time series. A time series is a signal that is measured in regular time steps. In this example, we see a set of server requests that experiences a huge jump around May 2017, as shown in Figure 18-16.  Figure 18-16. A time series example  How nice would it be if we could do the following:    Use anomaly detection to find and trigger responsive automation   Forecast server requests a few hours in advance  Before we can use this data, there are a few important points to keep in mind:    The estimation of future values in a time series is commonly done using past val‐  ues of the same time series.  hours, days, years, and so on.    Notice that the time step of a series may be of any length, for example seconds,    We need to choose a correct time step for what we want to predict, and, for that, it needs to be tested. It needs to be tried several times and with different values before we get to a final choice.    Deep learning, despite being a data science, has a lot of trial and error before get‐  ting to a good solution, mostly because it deals with probabilities.  In the example that follows, shown in Figure 18-17, we grab several months of nginx requests data and try to predict the tendency over the next 20 hours. This model is  Practical Machine Learning      317   experimental right now, and like weather prediction, it can fail. This is a recurrent neural  network  with  sequential  model,  used  in  the  Keras  code  using  TensorFlow backend, which is available online:  Figure 18-17. Recurrent neural network with sequential model.  source: https:   github.com ricardoamaro MachineLearning4SRE blob master  demo_predicting_nginx_requests-Final.ipynb   The provided code in the repository loads the data, normalizes it, predicts sequences, and plots the results, which you can see in Figure 18-18. The actual training is done in 100 epochs.  Figure 18-18. Time series forecasting based on past data  318      Chapter 18: Introduction to Machine Learning for SRE   The  output  graph  shows  a  sample  of  160  hours  of  traffic  from  the  past  data  we obtained, which we call the test set. The model was trained using 120 days of data against a cross-validation set of also 160 hours. This was the ratio that returned the best  results.  The  black  dashed  lines  represent  around  20-hour  predictions  in  the future, which are not perfect but clearly give an idea that the model was able to pre‐ dict the tendency of the traffic. In conclusion, these results clearly show good progress in forecasting within a certain time frame in the future, including a presence of a spike, such that we could actually provision new hardware if needed, avoiding an outage problem. For SRE, this type of analysis can be useful—for instance, in incident response and capacity planning. Success Stories There are a few areas of enterprise IT for which AI has and will have a significant impact:    Log analysis   Capacity planning   Infrastructure scaling   Cost management   Performance tuning   Energy efficiency   Security  Recently, Google started managing data center cooling through DeepMind. In one instance, it managed to reduce the amount of energy used by 40 percent, as illustrated in Figure 18-19.  Success Stories      319   Figure 18-19. Reduction of 40% spent on data center energy using DeepMind  source: https:  deepmind.com blog deepmind-ai-reduces-google-data-centre-cooling-bill-40   It accomplished this by using the historical sensor data, such as temperatures, power, pump speeds, setpoints, and so on, that were already collected by thousands of units in the data center. This data was then used to train an ensemble of deep neural net‐ works on the average future Power Usage Effectiveness  PUE , and the model was used and tested directly on the data center cooling system. Neural networks are also being used extensively in image recognition. For example, data center security is using it to spot nonauthorized personnel by analyzing video frames from surveillance cameras in real time. Apart from neural networks and deep learning, there are, of course, other machine learning success examples out there, such as Naive Bayes classifiers, that filter email spam automatically and have been used in the industry for a long time with great suc‐ cess. Your email provider is probably using this today. Another  field  of  IT  where  we  have  seen  good  results  from  machine  learning  is  in security, where anomaly detection, for instance, is used for credit card fraud detec‐ tion and many other applications that improve the security automation immensely with outlier detection. Further Reading This chapter has taken a very light approach to the subjects of machine learning and AI. If you want to go deeper into your investigation, I suggest exploring some open source code, some of which you can find in my repository and two reference books. Deep Learning you can read online for free at deeplearningbook.org, or get the print version. It’s time to apply machine learning in your organization.  320      Chapter 18: Introduction to Machine Learning for SRE   My GitHub Repository   https:  github.com ricardoamaro MachineLearning4SRE  Recommended Books   Russell,  Stuart  J.,  Peter  Norvig,  and  John  F.  Canny.  Artificial  Intelligence:  A  Modern Approach. Upper Saddle River, NJ: Pearson International  2003 .    Goodfellow,  Ian,  Yoshua  Bengio,  and  Aaron  Courville.  Deep  Learning.  Cam‐  bridge, MA: MIT Press  2016 . www.mitpress.mit.edu books deep-learning.  Ricardo Amaro is currently performing senior site reliability engineering functions in Acquia, one of the largest companies in the world of Free Software with around 20,000 servers in production. Ricardo is president of ADP—Associação Drupal Portugal and had his first contact with open technologies and especially Linux in the ’90s. Ricardo started applying Agile techniques and encouraging the DevOps culture very early. He is also a passionate advocate of free software, digital rights, and is a frequent speaker at IT events.  Further Reading      321    PART III SRE Best Practices and Technologies    SRE needs better ways to do documentation.   Can you teach SRE through a game?   Your SLOs are measuring the wrong thing.   How do you know when SRE is a success?   What are the SRE antipatterns?   Immutable  infrastructure,  scriptable  load  balancers,  and  service  meshes  can  makes SRE easier.  Discuss.   You Know You’re an SRE When…  …you find yourself buying extras of everything because two is one and one is none. …you are involved in a new feature from design to deploy. …Production readiness reviews are something to look forward to. …you wonder what your power company’s SLO is.   CHAPTER 19 Do Docs Better: Integrating Documentation into the Engineering Workflow  Ríona MacNamara and Shylaja Nukala, with Stas Miasnikoŭ, Aaron Gillies, Jeremy Sharpe, Google, and Niall Richard Murphy, Microsoft  Across the software industry, confidence in engineering documentation is low. Stack Overflow’s  2016  developer  survey  ranked  documentation  as  the  number  two  chal‐ lenge facing developers. This is a problem. Missing, incomplete, or stale or inaccurate documentation hurts development velocity, software quality, and—critically for SREs—service reliability. And the frustration it creates can be a major cause of job unhappiness for developers. Documentation  takes  time  and  effort,  and  this  is  especially  challenging  for  SREs. SREs often spend 35% of their time on operational work, which leaves only 65% for development. Time spent on documentation needs to come out of the development budget, and this is challenging if there’s a perception that creating and maintaining documentation is grunge work that might not be recognized or rewarded during per‐ formance review and promotion processes. How can this be changed? How do you make your organization understand the value of  engineering  documentation,  encourage  engineers  to  create  and  maintain  it,  and convince management and leadership that doing so is an activity worthy of recogni‐ tion, funding, and reward? At Google, we’re lucky to have a strong technical writing organization, but our writ‐ ers tend to focus on high-impact, high-visibility projects. The truth is that most engi‐ neering and SRE teams need to create and maintain their own documentation, and they  have  not  always  found  this  easy  or  appealing.  However,  since  2015  we  have  325   made significant progress in improving the quality and availability of internal engi‐ neering information. In this chapter, we share what we’ve learned and provide rec‐ ommendations  in  the  hope  that  they  will  be  useful  to  the  SRE,  SWE,  or  technical writer looking to do docs better. In particular, we focus on the following: Defining documentation quality  We  propose  a  framework  and  vocabulary  for  defining  documentation  quality and show  with a focus on SRE operations  how you can use these to define doc‐ umentation requirements.  Integrating documentation into the engineering workflow  Documentation is a core part of engineering work. In theory! In practice, the cre‐ ation  and  maintenance  of  documentation  often  requires  expensive  context switching and, as a result, tends to be neglected. We share our experience of inte‐ grating documentation with our code base and workflow tools.  Communicating the value of documentation  To drive change in an organization and to convince leadership that documenta‐ tion is worth the investment, you need to be able to communicate the business impact of your documentation work.  It might be true that documentation is not fully integrated into engineering practices today. Luckily, there is a precedent we can look to for inspiration, vocabulary, and models: software testing. Ten years ago, testing was very much in the same situation that  documentation  is  now:  ad  hoc,  unstandardized,  and  unpredictable.  Today  at Google, unit testing, regression testing, and integration testing are all so deeply inte‐ grated into engineering processes and tooling that pushing untested code to produc‐ tion simply isn’t practical. Testing at Google is a rigorous, highly accepted practice that is fully integrated into the engineering workflow. We can do the same for docu‐ mentation. Defining Quality: What Do Good Docs Look Like? The definition of documentation quality is, essentially, simple:  A document is good when it fulfills its purpose.  But before we can measure or report on documentation quality, we need a vocabulary for describing it. In this section, we propose a vocabulary lifted directly from the field of software testing, where it is used to describe various aspects of software quality. Essentially,  there  are  two  aspects  of  documentation  quality—structural  quality  and functional quality—that each contribute to the overall quality of a document. Structural  quality  is  what  usually  comes  to  mind  when  one  is  describing  technical documentation, and it’s definitely the quality that is being invoked when technical writers  refer  to  themselves  as  “wordsmiths.”  Structural  quality  describes  what  the  326      Chapter 19: Do Docs Better: Integrating Documentation into the Engineering Workflow   documentation should be like. A document or document set has high structural qual‐ ity if it meets the following criteria:    Spelling and grammar are correct   It complies with style and usage guidelines   It uses proper voice and tone   It’s well organized and easy to navigate  Structural quality can be relatively easy to determine: spelling is either correct or it isn’t; style is followed or it isn’t; links work or they don’t. This ease of measurement makes it tempting to use structural quality as the default measure of quality for your doc set. If you’re measured and evaluated on the basis of the structural quality of your docs, it’s tempting to overemphasize it at the expense of overall quality. Docs need only be good enough for their purpose, and internal-facing docs describing a system that is constantly in flux require a lower level of polish and style than customer-facing content. Functional  quality,  on  the  other  hand,  describes  the  effectiveness  of  your  docs.  A document  with  high  functional  quality  is  one  that  satisfies  its  stated  requirements. These requirements generally reflect business goals. For example, outages cost money and erode customer experience, so they need to be resolved quickly. Therefore, play‐ books relied on by SREs need to be complete and up to date; service overviews need to  cover  the  basic  information  required  to  understand  a  service;  and  postmortems need to include information needed to understand what happened and provide a list of action items for reducing the possibility of recurrence and or make recovery more straightforward. For example, for a playbook, we can ask whether it provides all the information an SRE needs to handle an emergency or outage:    Does the playbook provide 100% coverage of alerts?   Can the team rely on the playbook to perform on-call duties?   Is the playbook reliable  highly available ?   Is it easy to create and update entries?   Is each alert description accurate and complete?   Does each entry give enough information to understand and resolve the alert?   Does the entry give guidance on escalation?  Now, it’s definitely true that functional quality often depends on structural quality. The most effective instructions in the world won’t be useful if SREs can’t navigate to  Defining Quality: What Do Good Docs Look Like?      327   them, or their links are broken, or the language is unintelligible. But the truth is that functional quality is always more important:    High structural quality + low functional quality = poor overall quality   OK structural quality + good functional quality = good overall quality  Of course, high functional quality + high structural quality is ideal. But it might not be  worth  the  effort.  SRE  doesn’t  strive  for  100%  reliability  in  production,  and  we shouldn’t strive for perfection in documentation. Perfection costs too much, and its pursuit comes at the cost of work that could improve other documentation or even your service itself. What’s important, especially for internal documentation, is that critical information exists and is clearly conveyed. Because  it’s  so  tightly  tied  to  business  and  product  goals,  functional  quality  has  a much stronger relationship to what we perceive as value. We believe that to meet the goals of our business, functional quality must be our primary goal. Functional Requirements for SRE Documentation In this section, we look at what SRE documentation must deliver to support the core SRE functions, including the following: Monitoring and metrics  Establishing  desired  service  behavior,  measuring  how  the  service  is  actually behaving, and correcting discrepancies  Noticing and responding effectively to service failures in order to preserve the service’s conformance to the Service-Level Agreement  SLA   Emergency response  Capacity planning  Projecting  future  demand  and  ensuring  that  a  service  has  enough  capacity  in appropriate locations to satisfy that demand  Service turnup and turndown  Instantiating  and  deleting  service  capacity  in  a  predictable  fashion,  often  as  a consequence of capacity planning  Change management  Performance  Altering the behavior of a service while preserving desired service behavior  Design,  development,  and  engineering  related  to  scalability,  isolation,  latency, throughput, and efficiency  328      Chapter 19: Do Docs Better: Integrating Documentation into the Engineering Workflow   To  do  their  job,  SREs  need  to  be  effective  across  two  domains   development  and operations , and they need documentation to understand and run services in produc‐ tion. However, in this section we limit ourselves to discussing examples of the core documents SREs rely on to run production services. These include service overviews, playbooks and procedures, postmortems, policies, and SLAs. A comprehensive discus‐ sion  of  all  types  of  SRE  documentation  is  unfortunately  beyond  the  scope  of  this chapter—as are the general requirements for project developer documentation—but you can apply the principles we describe to all technical documentation.  Service overviews These are critical for SRE understanding of the services they support. SREs need to understand the system architecture, components and dependencies, and service con‐ tacts and owners. Service overviews are a collaborative effort between the develop‐ ment  team  and  the  SRE  team.  They  are  designed  to  guide  and  prioritize  SRE engagement and uncover areas for further investigation. These overviews are often an output of the production readiness review process. A basic service overview provides SREs with enough information about the service to dig  deeper,  which  is  particularly  important  for  onboarding  new  SREs.  A  complete service overview provides a thorough description of the service and how it reacts with the world around it as well as links to dashboards, metrics, and related information that SREs need to solve unexpected issues.  Playbooks Playbooks are quintessential operational docs, and they enable on-call engineers to respond to alerts generated by service monitoring. Playbooks contain instructions for verification, troubleshooting, and escalation for each alert generated from monitor‐ ing processes. Playbooks typically match alert names generated from monitoring sys‐ tems.  They  contain  commands  and  steps  that  need  to  be  tested  and  reviewed  for accuracy.  They  often  require  updates  when  new  troubleshooting  processes  become available, and when new failure modes are uncovered or dependencies are added. Playbooks are not exclusive to alerts and can also include production procedures for pushing  releases,  monitoring,  and  troubleshooting.  Other  examples  of  production procedures  include  service  turnup  and  turndown,  service  maintenance,  and  emer‐ gency escalation.  Postmortems SREs  work  with  large-scale,  complex,  distributed  systems,  and  they  also  enhance services with new features and addition of new systems. Therefore, incidents and out‐ ages are inevitable given the velocity of change. Postmortems are an essential tool for SRE, and they represent SRE’s formalized process of learning from incidents.  Defining Quality: What Do Good Docs Look Like?      329   A  postmortem  describes  a  production  outage  or  paging  event,  including   at  mini‐ mum : a timeline, description of user impact, root cause, and action items and les‐ sons learned. The postmortem is written by a member of the group that experienced the outage, preferably someone who was involved and can take responsibility for the follow-up.  A  postmortem  needs  to  be  written  in  a  blameless  manner.  It  should include the information needed to understand what happened, and a list of action items that would significantly reduce the possibility of recurrence, reduce the impact, and or make recovery more straightforward.  Policies These are documents that mandate specific technical and nontechnical policies for production. Technical policies can apply to areas such as production change logging, logs reten‐ tion,  internal  service  naming   naming  conventions  engineers  should  adopt  as  they implement services , and use of protocols governing emergency access. Policies can also apply to process. Escalation policies help engineers to classify pro‐ duction issues as emergencies and nonemergencies, and provide recommendations on the appropriate action for each category. On-call expectations policies outline the structure and responsibilities of team members.  SLAs SRE teams document their service s  SLA for availability and latency, and monitor service performance relative to the SLA. Documenting and publishing an SLA and rigorously measuring the end-user experi‐ ence and comparing it to the SLA allows SRE teams to innovate more quickly. This also results in less friction between SRE and SWE teams because they can negotiate targets and results objectively and avoid subjective discussions of risk.  Defining success metrics As you define your documentation requirements, it’s also important to define how you will measure the functional quality of your docs. This is important both for ongo‐ ing maintenance and improvement for your docs, but it’s also essential if you want to be able to communicate the value of your work to the rest of your organization. For example, a playbook should enable an on-call SRE to respond to an alert or com‐ plete a procedure. A playbook has high functional quality if an SRE was able to han‐ dle an alert without the need for escalation, and if the postmortem did not identify documentation as a contributing factor to the incident. A service overview has high functional quality if it provides the SRE the context needed to handle an outage.  330      Chapter 19: Do Docs Better: Integrating Documentation into the Engineering Workflow   Integrating Docs into the Engineering Workflow Based on our experience, we strongly recommend that SWE and SRE keep documen‐ tation in source control, alongside its associated code. We discuss this in more detail in the rest of this chapter, but in summary: when documentation is written in a sim‐ ple format and stored in source control next to the code it describes, engineers can create and update that documentation as part of their regular engineering workflow, using  their  existing  tools.  In  addition,  this  approach  enables  integration  with  the engineering toolchain, such as code search and review tools, and IDEs; and because the relationship between code and docs is explicit, it supports automation and better discovery of content. The Google Experience: g3doc and EngPlay Back in the spring of 2014, a couple of Google technical writers with some time on their hands decided to do something unprecedented: talk to their customers  internal engineers  and understand the challenges that they faced with respect to documenta‐ tion. As expected, they found most engineers were deeply unhappy with the state of internal  engineering  docs.  But  almost  universally  they  felt  bad  about  feeling  bad. They understood docs were important, they knew it wasn’t possible for every team to have a technical writer, and they wished they could do a better job. Some of the issues came from mismatched expectations. For users, documentation is urgent: they need answers now. But for creators, documentation is often low-priority, especially given the obstacles to its creation. At Google, almost all of these obstacles stemmed from fragmentation: documentation was scattered across multiple reposito‐ ries,  including  our  internal  wiki,  Google  Docs,  our  intranet,  and  internal  Google Sites.  This  meant  that  in  order  to  create  or  edit  docs,  engineers  had  to  leave  their development environment, find the right doc location, and spend time on format‐ ting, layout, and other issues. Context switching like this is expensive. A 20-minute interruption while working on a project entails two context switches; realistically, this interruption results in a loss of a couple hours of truly productive work. Given limited time and tough requirements, how could you justify work on docu‐ mentation, especially when you weren’t confident that this work would be rewarded at performance review, and particularly when it stole focus from your primary task, coding? Something had to give. Generally, it was the docs. Granted, Google is an extreme case. Our code base, google3, is described as having more than 2 billion lines of code, and might be the biggest code base in the world. Many tens of thousands of engineers work with it and Piper  the version control sys‐ tem that sits on top of the code base , submitting tens of thousands of changes every day,  modifying  millions  of  lines  of  code  each  week.  The  workflow  tools  engineers used every day were deeply integrated with google3 and with Piper. Coding, building,  Integrating Docs into the Engineering Workflow      331   testing, and releasing software in a system like this provides extraordinary efficien‐ cies. But these efficiencies had no impact on documentation, which lived pretty much anywhere you wanted it to and had no explicit connection to the project code or stan‐ dard engineering workflow and tools. What would be possible if documentation was written in a simple portable format called  Markdown  and  stored  next  to  its  associated  code?  After  all,  GitHub’s  docu‐ mentation  model—in  which  docs  were  written  in  Markdown  and  stored  with  the code—worked  well.  Why  couldn’t  we  do  the  same  at  Google?  Our  initial  idea  was very  simple:  enable  engineers  to  find,  create,  and  maintain  docs  by  keeping  docu‐ mentation in a simple portable format  Markdown , next to its associated code, and rendering  it  at  a  URL  that  reflected  its  location  in  the  code  base.  This  strategy  had some obvious benefits:    Markdown is easy to learn and easily readable in source. Its simplicity also means that it’s portable: If what we designed didn’t work, it would be simple to take our Markdown content and move it somewhere else.    SWEs and SREs could edit their docs, send them for review, and submit them to  source control using the exact same tools they used to create and edit code.    Keeping the docs with their associated code would make it much easier for engi‐ neers  to  find  them.  Instead  of  leaving  their  IDE  and  hunting  around  in  Sites, wiki, or Docs, engineers could just edit a Markdown file in their regular IDE and send it for review along with the code that triggered its update.    It created the very powerful possibility of integrating rendering into our internal IDEs,  generating  a  default  expectation  that  at  least  a  minimal  document—a README—would be present.    Rendering the page at a URL that reflected the location of the doc in Piper would make the association between the doc and the project clear to users searching for information on our intranet.  Our original prototype did very little other than render plain Markdown. We tested it with a couple of teams. “Wow, that’s ugly,” they said. “But the idea? It’s…interest‐ ing.” Give us a better look and feel, they said, and some navigation, and better for‐ matting for things like code blocks, and we might use it. The g3doc team went away and began adding these basic features as well as theming that removed the need for authors to think about formatting and look and feel. Qui‐ etly, we began working with a small number of teams to help migrate their content to the new platform. Slowly, word began spreading. As interest grew, several things hap‐ pened:    The  g3doc  team  was  joined  by  engineers  and  writers  from  across  Google  who found  that  g3doc  was  missing  features  crucial  to  their  teams  and  who  volun‐  332      Chapter 19: Do Docs Better: Integrating Documentation into the Engineering Workflow   teered to build those features. This grass-roots, bottom-up model of development was, we believe, critical to ensuring that the platform reflected the requirements of a very broad set of users.    Other engineering tools, such as our internal code search and review tools, and our internal IDEs added g3doc-based features such as the ability to preview a file before it was submitted to the depot. These changes were small in and of them‐ selves, but they removed friction from the workflow and made it far more seam‐ less to create and edit files with regular engineering workflow tools.    The platform acquired champions: highly visible engineers and leaders who pro‐ moted its usage to their teams and organizations posted on internal social media and began requiring their teams to use g3doc.  For the first few months, adoption was slow, but about six months in, interest explo‐ ded. Adoption was largely viral: teams discovered g3doc and chose to migrate to it. Three  years  later   October  2017 ,  almost  all  engineering  documentation  at  Google  thousands of projects  use g3doc and its less engineering-centric cousin, Company‐ Doc. As a result, Google deprecated its internal wiki, resulting in less fragmentation of engineering knowledge across the company. But even though adoption figures like these are gratifying, anybody can create docu‐ mentation; what’s important is that the documentation is maintained up to date, and that requires a change in developer behavior. Our theory was that if you made it as simple as possible for engineers to create and maintain documentation, and if you enabled them to do that as part of their regular workflow, and if you created a cul‐ tural expectation that docs existed, the quality of engineering documentation would improve. And it appears to be working. About 75% of all Google engineers submit doc changes monthly; an average doc file is updated several times a month; and, cru‐ cially, around a third of all changelists containing code also contain documentation files. These metrics indicate that developer behavior is changing and that documenta‐ tion is in fact becoming a standard part of internal engineering practice at Google. We also launched EngPlay, a version of g3doc intended for SRE playbooks. EngPlay provides centralized hosting for playbooks within Google. EngPlay is more resilient than g3doc, to ensure that content is available in case of outages. EngPlay also has additional features to ensure that playbooks are complete and reflect the current state of the system:    Each alert has its own documentation page. A tool automatically compares alerts from the monitoring configs to the playbook entries and throws a warning if an alert is undocumented or documentation exists for a missing alert.    If  a  page  is  not  found  at  its  expected  URL,  a  minimalistic  mechanism  checks  likely variants of that URL, increasing discoverability.  Integrating Docs into the Engineering Workflow      333     Support for variables allows EngPlay to adjust alert documentation pages with information from our monitoring, such as the ID of the job that generated the alert, the ID of the affected cluster, and even live graphs that reflect the current state of the system.  Both g3doc and EngPlay use the same  EngDoc  server as a backend. EngPlay hosts playbooks with the sole focus of being extremely reliable, while g3doc is meant for general documentation. Therefore, the core differences between g3doc’s capabilities and EngPlay’s capabilities are conceptual. EngPlay’s reliability was a top priority from its beginning. It can survive almost all outages  and  is  available  if  and  when  there  is  an  emergency.  Due  to  this  need  for extreme  reliability,  which  requires  EngPlay  to  be  a  separate  job  and  have  its  own dedicated path in Piper to serve, its features are limited in comparison to g3doc. What We Learned This section outlines some of our core principles around documentation, based on our experience with g3doc and EngPlay.  Where possible, documentation should live in source control, alongside its associated code This enables SWEs and SREs to edit their docs, send for review, and submit to source control using the same tools they use to create and edit code. As a result, engineers are  far  more  likely  to  maintain  the  docs  they  do  create.  In  addition,  a  predictable location for docs makes those docs more discoverable by humans and systems. But  we  also  discovered  that  keeping  documentation  alongside  its  associated  code enabled new features and functionality that removed much of the toil from creating and maintaining documentation. For example:    A Markdown linter and a formatter, eventually integrated into our IDEs and into  the presubmit pipeline, removes the need to manually tinker with Markdown    The ability to expose project metadata  such as project code locations and key  contacts  from code configuration files on documentation sites    The ability to automatically detect broken links in the documentation corpus and  notify owners    The  ability  to  include  live  code  in  documentation  instead  of  relying  on  static  code blocks that are destined to decay as the underlying code changes  Our experience tells us that the best solution is for docs to live alongside code; how‐ ever, we realize it’s not always possible. If this g3doc-style system doesn’t work for your project, the most critical step is to choose a canonical repository for your docu‐ ments.  334      Chapter 19: Do Docs Better: Integrating Documentation into the Engineering Workflow   Pick the simplest markup language that supports your needs The two core principles behind g3doc’s decision to use Markdown are that content must be easy to edit and read in source, and that content should be separated from dis‐ play.  In  our  system,  the  g3doc  server  handles  rendering  and  formatting.  It  should always be possible to ditch the renderer and read the content in source, and, in fact, in the case of certain outages, it might be essential that SRE playbooks are readable in source. Raw HTML is allowed but not encouraged for reasons of readability. However, depending on your requirements, you might have different guiding princi‐ ples. DocBook is a nightmare to read in source, but it supports a lot of flexible output. Not everyone shares our love of Markdown. People often choose it because GitHub supports it, but GitHub also supports an awful lot of other formats such as reStruc‐ tured  Text   RST   and  ASCIIdoc.  There  are  a  lot  of  different  flavors  of  Markdown  g3doc  uses  GitHub-flavor  with  some  added  customizations,  and  our  renderer  is based on Hoedown . Just remember that your goal is to remove as much friction as possible  from  the  task  of  creating  and  editing  documentation.  Any  complexity  in your choice of markup language must be justified by the real benefit you get from it.  Integrations are key to adoption Our first integration was extremely simple: the ability to preview a rendered version of a document before it was submitted to source control. Yet adding this link to our code review tool was critical to adoption, and g3doc usage surged immediately after its  launch.  Similarly,  we  made  it  easy  to  view  a  rendered  version  of  a  doc  directly from our code search and browse tool. Many smaller integrations  such as support for short links for internal URLs and syntax highlighting  followed; none of them was critical in itself, but each removed a small piece of friction from the documentation process  and  made  the  platform  far  more  attractive  as  a  result.  Newer  integrations have focused on removing much of the toil from the process of writing documenta‐ tion. For example, a linter checks files for style and formatting issues, and a formatter automatically formats Markdown files so that they comply with the g3doc style guide. Both of these tools can be invoked from all IDEs used internally. Essentially,  the  most  important  lesson  we  learned  is  that  engineers  will  create  and maintain documentation if you make it as simple as possible for them to do so. Doing Docs Better: Best Practices In this section, we cover recommendations for documentation practices to improve the quality of the documentation for your organization.  Doing Docs Better: Best Practices      335   Create Templates for Each Documentation Type After you’ve determined the functional requirements and quality indicators for each document your service delivers, codify those requirements by building them into a set of templates. Templates make documentation easier to create and far easier to use.    They make it easy for authors to create documentation by providing a clear struc‐ ture that they can populate quickly with relevant information. With a good tem‐ plate, creating a simple document can be as easy as filling out a form.    They ensure that documentation is complete by including sections for all required pieces of documentation. If a certain piece of information is not yet available or doesn’t apply, it’s fine to mark these as “TBD” or “N A,” but each section should be addressed.    They make it easy for readers to quickly understand the topic of the doc, the type of  information  it’s  likely  to  contain,  and  how  it’s  organized.  In  addition,  your template should ensure that the reader knows when the doc was last updated and by whom.  The  Site  Reliability  Engineering  book  contains  several  examples  of  documentation templates. Here’s a sample Playbook template that provides structure and guidance for engineers filling in the content:  Title The title should be the name of the alert  e.g., Generic Alert_AlertTooGeneric . Author: Last updated:  Overview Address the following: What does this alert mean? Is it a paging or an email-only alert? What factors contributed to the alert? What parts of the service are affected? What other alerts accompany this alert? Who should be notified?  Alert Severity Indicate the reason for the severity  email or paging  of the alert  and the impact of the alerted condition on the system or service.  Verification Provide specific instructions on how to verify that the condition is ongoing.  Troubleshooting List and describe debugging techniques and related information sources here. Include links to relevant dashboards. Include warnings. Address the following:  336      Chapter 19: Do Docs Better: Integrating Documentation into the Engineering Workflow   What shows up in the logs when this alert fires? What debug handlers are available? What are some useful scripts or commands? What sort of output do they generate? What are some additional tasks that need to be done after the alert is resolved?  Solution List and describe possible solutions for addressing this alert.  Address the following: How do I fix the problem and stop this alert? What commands should be run to reset things? Who should be contacted if this alert happened due to user behavior? Who has expertise at debugging this issue?  Escalation List and describe paths of escalation. Identify whom to notify  person or team   and when. If there is no need to escalate, indicate that.  Related Links Provide links to relevant related alerts, procedures, and overview documentation.  Better > Best: Set Realistic Standards for Quality A core SRE principle is that Google strives to make our systems reliable enough to keep our users happy, but no more reliable than that. Increasing reliability comes at a cost:  maximizing  stability  limits  how  fast  new  features  can  be  developed  and  how quickly products can be delivered to users, and it dramatically increases their cost, which in turn reduces the numbers of features that a team can afford to offer. Requiring  high  standards  of  writing  can  be  counterproductive,  intimidating  engi‐ neers from creating docs. Similarly, polishing a document past the point where key information is up to date, discoverable, and clearly conveyed is a waste of time that could be spent improving other parts of the documentation  or your service itself . Just as code is an iterative process, so too is documentation. Learn to embrace what Anne Lamott describes as the “shitty first draft”: an imperfect document  is  infinitely  more  useful  than  a  perfect  one  that  does  not  yet  exist.  Ask yourself  this:  Does  this  doc  meet  its  functional  requirements  and  is  the  required information present and clearly conveyed? If the answer is yes, hold the doc changes to  the  minimum  reasonable  standard.  After  a  document  has  met  that  quality  bar, move on. Good writing is great to see, but don’t block changelists because the author used the passive voice in a doc. Don’t be that engineer. Require Docs as Part of Code Review Documentation is like testing: nobody really wants to do it. But in other cases, code reviewers have power: we can withhold approval until the docs are sufficient. Do this! Not all changes require doc updates, of course. Here’s a good rule of thumb:  Doing Docs Better: Best Practices      337   If a developer, SRE, or user of your project needs to change their behavior after this change, the changelist should include doc changes.  On the other hand, if a change doesn’t require tests, it probably doesn’t require docu‐ ments either. Examples include refactors and experiment tweaks. Use your judgment. As always, simplify and automate this process as much as possible. At Google, teams can enforce a presubmit check that either looks for a flag that indicates a doc update isn’t necessary  presubmit checks for style issues can prevent a lot of arguments, too . We also allow owners of a file to submit changes without a review. If  your  team  balks  at  the  requirement,  remind  members  that  simple  project  docu‐ mentation is about saving the information in your head so that others can access it later without bothering you. And doc updates aren’t usually onerous; generally, the size of your documentation change scales with the size of your CL. If your CL con‐ tains a thousand lines of code, you might need to write a few hundred lines of docu‐ mentation.  If  it  contains  a  one-line  change,  you  might  just  need  to  change  only  a word or two. Finally,  remember  that  docs  don’t  need  to  be  perfect;  they  need  merely  be  good enough. What’s important is that key information is conveyed clearly. Ruthlessly Prune Your Docs A small set of fresh and accurate docs is better than a large assembly of “documenta‐ tion” in various states of disrepair. Every  line  of  documentation,  just  like  every  line  of  code,  should  serve  a  purpose. When  documentation  isn’t  serving  a  useful  purpose,  it  should—like  code—be archived or deleted. Unnecessary or unhelpful documentation is a tax; it’s a kind of technical debt, adding complexity and uncertainty and often obscuring or even con‐ tradicting useful information. At Google, we encourage SWEs and SREs to fearlessly delete no-longer-useful docu‐ mentation. The magic of source control means that we can easily restore them if the need occurs, but it rarely does. If you don’t have a way to restore deleted documents, for the love of all that’s holy move them to an “Archived” directory, mark them as “Deprecated”  possibly the only legitimate use of   , and ideally provide a link to the new version. Recognize and Reward Documentation Documentation, as we have already noted, takes time and effort. One of the barriers to better documentation is that it has often been perceived as toil: drudge work that is not recognized or rewarded at performance review or in the promotion process. But documentation is core engineering work that reduces toil and improves the reliability  338      Chapter 19: Do Docs Better: Integrating Documentation into the Engineering Workflow   of a service. If SREs are to do docs right, that work needs to come out of the 50% time they have budgeted for development. This is a reality that needs to be faced. It requires managers and leads to rigorously identify  documentation  requirements,  establish  baseline  standards  for  quality,  and ensure that this work is budgeted for in the project schedule and recognized during performance review and promotion time. Because leadership support is so important to driving change, the next section focu‐ ses  on  how  to  best  communicate  the  value  and  impact  of  documentation  work  in your organization. Communicating the Value of Documentation If you want to convince fellow engineers and leadership to invest time and resources in documentation, it’s essential that you gather data that accurately demonstrates the quality, effectiveness, and value of your documentation. Avoid the temptation to rely on structural quality data. Such data can be easy to col‐ lect, but it’s uncompelling and provides a weak statement of impact that is unlikely to support your case; for example:    “I submitted 90 changelists that updated the documentation.”   “I wrote 37 pages of a service overview.”   “Everything was spelled correctly.”   “My style and usage conform to our style guide.”   “My documentation was mostly task based.”  Sure, those stats are a bad way to evaluate the work of a very junior writer who is just beginning to learn the craft of technical writing. But they’re a terrible way to demon‐ strate the value and impact of the noble work you have undertaken to improve docu‐ mentation in your organization. Remember, when you talk about the impact of your doc work, you’re talking about the business value of your output. While structural data is unpersuasive, functional data is convincing. Although structural quality is generally pretty easy to measure, functional data has a different shape and can be trickier to gather. It typically falls into three buckets: Measurable success  If  an  alert  was  successfully  handled  using  your  documentation,  and  no documentation-related issues are revealed in the postmortem, your documenta‐ tion has high functional quality.  Communicating the Value of Documentation      339   User behavior  Sentiment data  Your product team wants people to try out a new feature. If you can test how many users read the docs and try the feature, you can measure how well you sat‐ isfied this requirement.  This is trickier, is prone to misunderstanding, and can be too easily dismissed. “Not statistically significant!” “It’s just someone’s opinion!” But sentiment data is real data, and it can sometimes be the only data you can get. When a user tells you that they like your doc, they are explicitly telling you that the doc met their requirements. When a team adopts your documentation platform or uses your documentation templates, they are telling you that the platform or templates are useful to them. You can gather sentiment data via surveys, bug and issue track‐ ing, feedback mechanisms on the page itself, or by talking to the teams you sup‐ port.  Functional quality data enables you to build a compelling story around documenta‐ tion, like this:    Our SRE team had a goal to decrease the time it takes for a new engineer to go on  call.  alerts.    I pitched a proposal to create a playbook with complete documentation for all    The team accepted the proposal. I worked with the engineers to revamp the play‐ book so that each entry clearly conveyed what the alert meant and provided ways to immediately address and mitigate any negative effect.    As the result of the document effort, the revamped playbook received five times  more visits.    Engineers reported that they could rely on the playbook during on-call.   A follow-up study indicated that there was an x% decrease in the time it takes for  a new engineer to go for on-call.  Here’s another example, demonstrating how Stas communicated the value of moving SRE playbooks to EngPlay:  With EngPlay, I developed an improved version of our playbook service, but the adop‐ tion wasn’t great. People preferred to stay with the old clunky but battle-tested imple‐ mentation. After six months, only 10% moved onto the new version. Moreover, I was constantly dragged into explaining to people why I’ve done what I’ve done, why and how they should move onto a new version. I started feeling like I increased fragmenta‐ tion instead of cleaning the mess. At that point, I wrote a very basic set of documents outlining the most common questions:    Why the new platform for playbooks?  340      Chapter 19: Do Docs Better: Integrating Documentation into the Engineering Workflow     What are the benefits?   How to migrate to it?  As a result:    The stream of private conversations almost entirely drained. In the rare case that someone would ask, I could just send them a link to a document. I finally could concentrate on improving the platform itself.    When  the  documentation  was  available,  new  adopters  began  contributing  to  it. This created a chain reaction of more users → more improvements to the docu‐ mentation → more users.    In  the  next  three  months,  30%  more  projects  migrated  to  EngPlay—a  six-fold  increase in the adoption rate.    Six months later, almost all services had adopted EngPlay for their playbooks.  This is a story we can communicate in all sorts of ways:    With other SREs and with technical writers, who might be interested in learning  from your experiences and adopting your processes.    With our managers, who need to be able to understand why we’re spending our    At performance review and promotion time, when you need to be able to build a  time on documentation.  case for recognition.    With the other teams we work with and collaborate with on documentation.  Functional data is compelling. Gather it and use it to state a case for impact and con‐ vince your team, your organization, and your leadership that investing in documen‐ tation is worth doing. Because it is. Further Reading   ZWISCHENZUGS, “Things I Learned Managing Site Reliability for Some of the  World’s Busiest Gambling Sites”.  Ríona MacNamara is a senior staff technical writer at Google who has focused exten‐ sively on internal engineering documentation and process, and believes that documen‐ tation is as fundamental to the discipline of software engineering as testing. Shylaja  Nukala  is  a  technical  writing  lead  for  Google  Site  Reliability  Engineering, where she has worked for 12 years. She leads the documentation, information manage‐  Further Reading      341   ment, and select training efforts for SRE, Cloud, and Google engineers. Prior to Google, she was a staff technical writer at Epiphany and Sony Electronics. She holds a PhD in communication studies from Rutgers University. Stas Miasnikoŭ is a site reliability engineer at Google. Aaron Gillies manages a software documentation group in Google Cloud in New York City. Jeremy Sharpe is a senior software engineer at Google. Over six years with Google, he’s worked on ads, shopping, and cloud, but some of his favorite contributions have been to engineering documentation tools and education as part of his 20% time. Niall Richard Murphy has been working in internet infrastructure for over 20 years, and is currently Director of Software Engineering for Azure Production Infrastructure Engineering in Microsoft’s Dublin office. He is a company founder, an author, and a photographer,  and  holds  degrees  in  computer  science  and  mathematics  and  poetry studies. He is the instigator, coauthor, and editor of Site Reliability Engineering and The Site Reliability Workbook  both O’Reilly .  342      Chapter 19: Do Docs Better: Integrating Documentation into the Engineering Workflow   CHAPTER 20 Active Teaching and Learning  Laura Nolan, formerly Google  Spoon feeding in the long run teaches us nothing but the shape of the spoon.  —E.M. Forster  The distributed datastore is down. Writes are failing on all replicas and reads are tim‐ ing out. The SRE on call checks the monitoring; there are no clues as to the cause, yet it’s clear this key production service is in a bad state: the errors and latency graphs are the only ones going up and to the right. Revenue is being lost. The on-caller declares a production incident. The VP of engineering storms in, demanding to know what’s going on. The  other  SREs  in  the  room  just  laugh.  Why?  Because  this  is  Incident  Manager,  a game designed to teach incident response skills and teamwork, and the current player just drew a bad card. Incident management is a key SRE skill that can be learned—and it’s much better for your organization’s SLO budget  and the stress levels of your SRE team  to learn it via a fun and effective game rather than during an actual production incident. Because SREs are both generalists and experts  one of the major reasons why it is dif‐ ficult to hire them , they are constantly learning. The skillset of an SRE can span operating system internals, networking, monitoring and alerting, troubleshooting, debugging, incident management, software engineer‐ ing,  software  performance,  hardware,  distributed  systems,  systems  administration, capacity planning, security, and many other areas. Not all SREs are expert in all of these areas, of course; most SREs are “T-shaped”: broad in many areas, and deep in one, or a few.  343   This broad set of skills is channeled into a vast array of job functions, some of which might be new territory, such as the following:    Onboarding a new team member  or a few    Onboarding a new service  or a few    Making major changes to at least one existing service   Dealing with changes in the systems with which our systems interact   Dealing with explosive growth or some other major systems challenge  New team members are learning both their team’s systems and key SRE skills that they might not yet have. More established team members are developing their skills and knowledge as they deal with change and as they dive deeper into different areas of expertise. If we must be learning all the time, we should make our learning effective, and more importantly, fun. Active Learning  I am convinced that the best learning takes place when the learner takes charge.  —Seymour Papert  Seymour Papert, the inventor of the Logo programming language and the inspiration for  Lego  Mindstorms,  said  that  learning  was  “building  knowledge  structures,”  and that this works best when the learner is creating something. He also said that learning through play doesn’t mean that the material being learned is easy; it means it’s engag‐ ing.  In  Seymour  Papert’s  studies,  learners  called  their  learning  fun  because  it  was hard. Trying to solve problems, either successfully or unsuccessfully, and receiving timely feedback seems to be key to the learning process. Games are ideal for this.  Chess Is One of the Oldest Educational Games  This isn’t a new idea. Chess is said to have originated over a millennium ago as a way to teach strategy to noblemen. Kriegsspiel   which  means  “wargame”  in  German   was  a  system  created  in  1812  to train officers in the Prussian and German armies. It is a role-playing game, with a grid, gaming pieces and dice, and ways to simulate fog of war and communication difficulties.  Players  on  either  side  act  as  commanders,  and  a  gamemaster  acts  as umpire and decides the effects of player action.  344      Chapter 20: Active Teaching and Learning   Games are still used in education today. In March 2017, the CIA showed some of its internal training board games at SXSW.1 The games are designed to encourage prob‐ lem solving and collaboration. One, designed by Ruhnke, is designed to teach players about the political situation in Afghanistan, and he believes it gives players a much more nuanced mental model of the issues in the region than just reading briefs.  There are lots of examples of games in computing education in recent times, includ‐ ing  Regex  Golf,  in  which  players  write  regular  expressions  to  match  given  inputs accurately, and Deadlock Empire, in which the players attempt to reveal concurrency problems by acting as a scheduler. Games are effective learning tools because people enjoy them. They are a way to get effortlessly into a flow without fear of failure posing a distraction. They can be social. They  have  inbuilt  and  instant  feedback,  and  they  have  rewards  or  scoring  systems when players get things right. These are all great features of any learning experience. Other forms of active learning exist. Tutorials and discussion groups are a common example. These sorts of learning structures don’t have scores and levels like games do, but they do include active and creative engagement with the material, and feed‐ back should come from tutors and students in the group. Active Learning Example: Wheel of Misfortune Wheel of Misfortune is one of the best training tools we at Google have for getting new SREs, or SREs new to a system, up to speed. And it’s a game—basically, a role- playing game. There’s no board, or simulator, or any equipment needed  although a laptop and a whiteboard usually come in handy . Wheel of Misfortune starts out with a gamesmaster and a team member playing the on-call engineer. The gamesmaster describes some event happening to the on-caller, generally a page or maybe some escalation from another team. The engineer playing the on-caller then responds as they would in a real situation, mitigating, root-causing, fixing the issue, and escalating, as required. The gamesmaster acts as an oracle and will answer the on-caller’s questions about what the monitoring, logging, and so on would show. This is a deceptively simple but incredibly valuable exercise for everyone involved. As the gamesmaster, you generally spend some time beforehand getting your scenario ready, and you need to have thought it through in considerable detail before you put it to your colleagues. As the engineer playing the on-caller, or as another colleague  1 “The CIA Is Training Its Officers With D&D-Style Tabletop RPGs” by Beth Elderkin and “Why the CIA uses  board games to train its officers” by Selena Larson.  Active Learning      345   observing, you likewise learn about your system’s behavior, and also about its moni‐ toring, any recovery tools, points of escalation, and so on. There are also skills to be learned, like techniques for troubleshooting and root-cause analysis as well as incident management. Wheel of Misfortune is a great way to prac‐ tice these in a low-risk and  relatively  low-pressure environment. We take Wheel of Misfortune so seriously that in the Dublin  Ireland  Google SRE building, we have a room that was specially designed for it. It has amphitheater-style seating, enough for the largest team or a set of related teams; a whiteboard, and a large screen. The best part is that we SREs got to name it: it’s called Kobayashi Maru, after the fictional no-win training exercise from Star Trek  and yes, it is styled as a starship bridge . Luckily, our Wheel of Misfortune exercises generally have a much happier  ending  than  the  Kobayashi  Maru  scenario  does   usually  it  involves  noting some action items to improve dashboards or playbooks and going for coffee, rather than the Klingons destroying your starship . Active Learning Example: Incident Manager  a Card Game  Incident Manager  is  a  card  game  that  I’ve  used  as  the  basis  of  workshops  to  teach incident  management  skills   it  ran  at  SRECon  Europe  2016  and  I’ve  run  it  within Google .  Solid  incident  management  practices  can  mean  the  difference  between  a minor outage and a prolonged and a major outage of the kind that hurts user trust. They are key to the success of any SRE team. Incident management skills are about communication and coordination, and a sepa‐ ration of responsibilities. The difficult thing about incident management is that the techniques need to be applied during incidents, which are often stressful and confus‐ ing  moments.  The  incident  manager  game  was  devised  to  give  people  practice  in applying incident management in something approximating how dealing with pro‐ duction incidents can feel. One common example of the sort of thing that can go wrong while responding to an incident is simply lack of structure—responders jump in, don’t coordinate or com‐ municate with the rest of the organization, and collectively miss something vital, or all end up investigating the same potential root cause. Another  well-known  incident  response  antipattern  is  when  a  junior  member  of  a team  sees  a  potential  problem  but  isn’t  confident  enough  to  point  it  out.  Worse, sometimes when one person does point out an issue, others might still ignore it. I have seen postmortems describing more than one serious outage that was unnecessa‐ rily prolonged because the correct answer was suggested but no one listened. Experi‐ ence  as provided in this game  can help teach confidence, and it can also help people to learn to listen to other team members who might have valuable insights.  346      Chapter 20: Active Teaching and Learning   Like  Wheel  of  Misfortune,  Incident  Manager  presents  a  production  system  and  an outage scenario, with a gamesmaster presiding. With a team that works together on a real service, you could use that service. Running this with mixed groups, I use a short system description for a distributed storage system that looks much like the Google File System  GFS , including some notes about its monitoring and other deployment information. Participants split into two teams  representing two shifts, Europe Middle East Asia [EMEA] and US, of an on-call rotation for the service suffering the outage  and are dealt playing cards. The cards add the structure of a game, which a Wheel of Misfor‐ tune session lacks. They also provide useful prompts about the incident management process—their real purpose. The scenario adds the sorts of distraction from incident management processes that a real production incident does. There  are  four  card  types:  basic  rules,  roles,  keepers,  and  actions,  all  shown  in Figure 20-1.  Figure 20-1. Cards from the Incident Manager game  Active Learning      347   Here are the basic rules:  facilitator.  location  plays first.  player must play it.    Your team is collaborating to solve an incident, which will be described by the    The Incident Commander of the active shift  whichever is closest to the current    If a player starts a turn with no cards in hand, they draw two.   Each player may play one card per turn. On drawing a PLAY NOW card, the  Each player gets one role card, which they keep for the duration of the game. As in real incident management situations, roles come with responsibilities and with spe‐ cial powers. Each role card belongs either to an EMEA or US shift. The gamesmaster removes responder cards from the deck as needed to ensure that each shift has criti‐ cal lead roles staffed. Here are the roles: Incident commander  IC   The IC is the final authority on all decisions. You can veto any action taken by another player. The IC can take one action of their choice each round, subject to the facilitator’s approval  no action card required .  The  planning  lead  tracks  the  status  of  the  incident,  including  actions  taken, efforts under way, and anything that might need to be done later  for instance, undoing mitigation efforts . You can check and reorder the next five cards in the deck each turn.  Planning lead  Operations lead  The  operations  lead  coordinates  responders,  making  sure  that  efforts  are  not duplicated. You can check any one piece of monitoring or logging each turn  no action card required .  Communications lead  You are responsible for updating any affected teams or external users about the status of this incident. You can communicate with users or escalate internally to another team or a busi‐ ness owner each turn  no action card required .  348      Chapter 20: Active Teaching and Learning   Responder  The responder has no special abilities. Yours is but to contribute to the effort as your cards allow and as the IC thinks best.  As in real life, the incident commander should start an incident document describing the progress toward resolving the “incident”  this could be a shared document or a whiteboard flipchart . Each player on the active shift takes a turn, as described by the basic rules. They draw action cards—some of these can be kept, but some must be played immediately. The actions are the sort of things that might happen during an incident—some of these are helpful and some are not. Here are some examples of actions:    Check monitoring   Query a running process   Query logs for a given job   Roll back a process   Modify load-balancing configurations   Develop a new utility or script  the facilitator decides how many turns this will require, and the player who plays this will be out of the game for that number of turns     Modify a job  change flags, number of instances, etc.    Bug fix, cherry-pick and roll-out—again, requires a number of turns to be deci‐  ded by the facilitator    Escalate to developers  played by the facilitator —the player who plays this loses a number of subsequent turns depending on how well the facilitator feels the SRE team did on investigating the situation before escalating    Escalate to business owners  for an answer about any decision impacting users   Here are some examples of PLAY NOW cards:    Shift change: active shift loses all their cards; the other shift takes over   An unspecified terrible thing happens  the gamesmaster decides    Noisy alerting: paged for an irrelevant minor problem, lose a turn   Interfering executive: communications lead misses their next turn dealing with it   also a planning lead variant   Active Learning      349     Engineers not involved in the incident response overload your monitoring: lose  all your monitoring check cards  This game teaches incident management skills quickly, without being dull  and really, very little is duller than sitting through a bunch of slides about processes . Here, peo‐ ple get to actually use the process, with time and resource constraints like in a real incident. It builds confidence that they can use the incident management processes under stress. The cards reinforce real-world problems that arise in incident manage‐ ment   such  as  the  interfering  executives  who  can  descend  if  communication  isn’t good, or bystanders overloading monitoring if the operations lead isn’t coordinating who is working on different tasks . This game requires preparation to work well. It won’t work well if the group size is larger than around 12. The group also needs to have a shared understanding of the system  that  the  game  is  based  on,  either  because  it’s  their  actual  system  or  via  a description. Like Wheel of Misfortune, it needs careful preparation of the scenarios by the gamesmaster, so that the gamesmaster can answer the questions that players will ask  about  system  behavior.  You  should  also  organize  props,  like  a  shared  incident document, ahead of time. Active Learning Example: SRE Classroom SRE Classroom isn’t a game. It’s a workshop that a number of Google SREs  includ‐ ing  me   have  been  running  since  2012.  We’ve  run  it  at  many  public  conferences  including USENIX LISA, USENIX SREcon, O’Reilly Velocity, and FLOSS UK  and we’ve run it by invitation at Google offices. It has been run in two formats: half day, with only design exercises, and as a full day, which includes some talks. SRE Classroom is intended to teach people how to do practical distributed systems design and basic capacity planning. This is quite an ambitious thing; it’s a complex area of practice, and people attend with radically different levels of experience. I’m one of the very few SREs who have been on both sides of the fence for this partic‐ ular  event.  Before  I  joined  Google  I  attended  one  of  the  very  first  versions  of  this workshop, at the Google office in London in 2012. After I started at Google, I was quite active in updating the content of the workshop along with several others, and I taught it many times. Recently, I developed a more in-depth version of it  it’s around three  days  long  if  run  continuously   for  new  Google  SREs  without  systems  design experience, or for non-SRE Googlers who are interested in joining SRE. In the workshop, we throw the participants in at the deep end of the pool, to sink or swim.  At  the  beginning  of  the  workshop  we  split  the  participants  into  groups  of around five, each group having a facilitator. They get a problem statement and some nonfunctional requirements around scale and resilience to failure. These scaling and resilience requirements are really the heart of the problem. One of our problems uses  350      Chapter 20: Active Teaching and Learning   business logic that is as simple as a SQL join, but the system being designed must scale to roughly the number of page views that Google serves daily—which is a lot! Then,  each  team  is  asked  to  design  the  system  and  to  come  up  with  some  initial capacity estimates for the hardware needed to run it. At the end, we run through a design that meets the goals and then discuss. Teams work together on the design. The facilitator’s job isn’t to give answers. The facilitator’s job is to clarify anything in the problem that isn’t clear, to help the team by asking the right questions when needed to avoid spending too much time down rabbit holes, and to make sure that no one person is dominating the conversation. The team does the work together, because what we are trying to do is to develop their ability to design distributed systems  and that is not quite the same thing as coming up with a correct design for this particular system . In many ways, it doesn’t really matter if the team actually gets all the way to the “correct” answer. The process is the point. What really matters is that people have spent time wrestling with the problems and thinking about the trade-offs that different solutions imply. As I mentioned earlier, I’ve been on the nonteaching side of the fence as well, and it is in some ways hard work. But my experience of it, as both learner and teacher, is that it’s possible to lose yourself in the work so deeply that you and your team will entirely lose track of time working and discussing the problems. Like Seymour Papert’s stu‐ dents, they find that it is fun because it is hard. The large majority of those who have attended have enjoyed it and found it useful  as reported on post-session surveys . Participants are certainly significantly more skilled and confident at systems design and capacity estimation at the end of the workshop. Only a small minority of stu‐ dents  low single-digit percentages  strongly disliked the active learning experience. Those participants tended to be people who wanted to sit back and let someone read slides to them rather than learning through experience. This  sort  of  exercise  is  also  quite  an  effective  way  of  learning  the  reasons  for  the architecture  of  a  system.  We’ve  used  different  system  designs  as  the  basis  of  this workshop at different times, but one we’ve used quite often is Photon, a log-joining pipeline. The reason we like using this one is that for most workshop attendees, it’s different from the sorts of systems they work on day to day, so it’s more of a chal‐ lenge. Photon is a fairly complicated beast—I was on the SRE team that owned the service. I noticed that going through an exercise like the SRE Classroom workshop yields a much deeper and better understanding of Photon than just reading about it or watching someone talk about it. Putting in the effort to wrestle with the problems yourself really works. This really pays dividends when you are an SRE and trying to figure out the latest failure mode that your system is currently exhibiting. This is a technique I use myself now when I need to learn a new system: first figure out the functional and nonfunctional requirements, then sketch a design myself, and only then read about how the system actually works. Regardless of whether what I  Active Learning      351   came up with was close to reality, I find I understand its constraints and trade-offs much faster than I can just by learning about it passively. It takes longer, but in many cases the time investment is well worth it. If your team is taking on a new service, or using a new piece of infrastructure, you can do this sort of thing with your entire team, a whiteboard, and a little preparation, and it’s a lot more fun than sitting through a presentation. The downside of it is that it’s not a great technique for remote teams; being in a room together really helps. The Costs of Failing to Learn SRE  teams  are  sometimes  going  to  learn  things  the  hard  way.  We  might  cause  or extend an outage. Much worse, we might cause a security breach; we might lose user data. Production problems are going to bite us sometimes, but it’s a very, very expen‐ sive way to learn and we can’t afford to repeat the lesson. We need to do better. SRE organizations must therefore be training and learning organizations. There are other tangible costs to any SRE organization that isn’t a good training orga‐ nization. The first cost is a longer time for new hires or for those switching between teams to become productive. In larger organizations with multiple SRE teams, you might  also  find  less  cohesion  between  SRE  teams.  If  they  don’t  share  a  common training and learning system and share knowledge, teams will diverge in their mind‐ set, techniques, and approaches to the job. Teams are likely to create or adopt differ‐ ent  tools  and  processes  for  the  same  purpose,  increasing  development  cost  and adding barriers for those switching between teams. SRE teams that can’t share knowledge won’t work as effectively with other non-SRE engineering teams in their organization. SRE teams are, in effect, constantly training development teams and their own junior members about good production practices. They do this via design reviews, launch readiness reviews, postmortems, and maybe other avenues like formal training or rotations working with the SRE team. Another major cost for SRE teams who aren’t strong in training is that newer team members who aren’t confident in their skills and in their knowledge of their service can find themselves under increased stress, particularly when they are on call. This is a major risk factor for burnout, and it’s awful for team effectiveness and morale. Fur‐ thermore, a burned-out SRE team is a far less effective SRE team, and that can pose an existential risk to the entire enterprise. We owe it to ourselves and our teams to be good at both teaching and learning. SRE teams that don’t learn and don’t teach are ineffective. Learning and teaching are therefore  core  SRE  skills;  but  they’re  skills  in  which  many  of  us  aren’t  specifically trained. They’re also skills that most of us don’t really think about as being vital to SRE.  352      Chapter 20: Active Teaching and Learning   In  recent  years,  we  have  explicitly  added  teaching  and  education  to  the  SRE  job description at Google in order to recognize that it should be one of core competen‐ cies. Learning Habits of Effective SRE Teams Learning isn’t only about onboarding new team members or services. Effective SRE teams incorporate learning into their regular working practices. The two most wide‐ spread learning habits for SRE teams are regular production meetings and postmor‐ tem incident analysis. It is very difficult to run a large and complex service reliably without implementing both of these practices. Production Meetings You can view a weekly production meeting, in which developers and SREs meet to discuss the state of their service, partly as a process of learning. The team as a whole learns new things about how the service is performing and what problems are occur‐ ring. Individuals on the team usually learn some new things about how the service operates,  too.  Most  well-run  production  meetings  tend  to  spawn  sets  of  follow-up actions  that  require  people  to  go  and  find  answers  that  weren’t  known  during  the meeting. There are a couple of ways to maximize the value of your production meeting as an active learning opportunity. For  engineers  new  to  the  team,  consider  having  them  keep  a  list  of  anything  that comes up in the production meeting that isn’t clear to them. In most organizations, the new engineer should have a team member designated as a mentor to help them get up to speed. After each production meeting, that mentor can explain items noted from the production meeting to the new engineer. This is also a good opportunity to make sure your team documentation is up to date. When issues arising in the production meeting are not clear to experienced members of the team, another learning opportunity presents itself. One engineer  either some‐ one  who  does  understand  the  issue  or  who  is  tasked  to  research  it   can  bring  the knowledge back to the rest of the team, perhaps during the next production meeting. These sorts of poorly understood corners of your systems are also among the best sources of ideas for your Wheel of Misfortune sessions. Postmortems Postmortems are another great  if sometimes expensive  learning opportunity. Both the process of writing them and the final artifacts are really valuable for learning curi‐ ous corners of your systems and technologies  as well as for the headline purpose of actually avoiding repeating the incident .  Learning Habits of Effective SRE Teams      353   Postmortems,  however,  are  not  very  active  in  and  of  themselves.  You  can  activate them, though. I set up a postmortem reading club as a key avenue for a number of closely related SRE  teams  to  share  understanding  about  the  systems  we  run.  Every  fortnight  we choose the most interesting postmortem from the recent past, and we have a semi- structured discussion. There is a rotating chairperson, whose responsibility is to lead the  discussion  and  keep  notes.  We  generally  use  a  whiteboard  to  describe  the sequence of events to everyone’s satisfaction, drawing on the knowledge of the people in the room to fill in any blanks or required context. When that is complete, we look at action items, again drawing on the collective wisdom of the room to add meaning. Attending  one  of  these  sessions  is  a  much  richer  learning  experience  than  simply reading a document. Operations reviews are a lighter-hearted incarnation of the same sort of idea—pager- carriers and on-call people of all varieties come together for sociable and humorous discussions  of  recent  problems  and  outages.  Frequently  at  Google,  we  encourage attendance at such events with bribes such as cake. Expensing the weekly ops review donuts might be one of the best financial investments an SRE manager can make in their team’s development. A Call to Action: Ditch the Boring Slides In this chapter, I’ve talked about a variety of ways that I and other Googlers have used active methods to teach and learn. You can apply these sorts of techniques to your own team’s tools, systems, and processes, too. There is almost nothing more critical to the success of any SRE team than the ability of its members to learn. Active learning methods are among the most effective  and often the most engaging and fun  ways of achieving that. So, why not ditch the slides and use some of these techniques next time you need to run some kind of training?  Laura Nolan taught herself to program in her teens, did a CS degree, and worked as a developer and as a software performance engineer before joining Google SRE in early 2013. At Google, she worked mainly on large data processing pipelines and the network, so all tubes basically. She was co-chair of USENIX’s SREcon EMEA conference in 2017 and 2018.  354      Chapter 20: Active Teaching and Learning   CHAPTER 21 The Art and Science of the Service-Level Objective  Theo Schlossnagle, Circonus  You can’t meet or exceed expectations if no one agrees what the expectations are. In every  job,  you  must  understand  your  objectives  to  measure  your  success.  In  this chapter, we learn what it looks like when an SRE sets goals. Why Set Goals? The main objective of an SRE is to maintain the reliability of systems. Service-Level Objectives  SLOs  are the primary mechanism used by SREs to determine success in this objective. I’m sure you can see that it is difficult to “do your job well” without clearly defining “well.” SLOs provide the language we need to define “well.” You  might  be  more  familiar  with  Service-Level  Agreements   SLAs ,  so  let’s  begin there. SLAs are thought of by some as the heart of darkness, and by others as the light of redemption. Why the disparity? I believe it’s because of how they are defined. They can  be  defined  in  a  way  that  enables  producers  and  consumers  to  level-set  their expectations, or they can be tool for despair, false assurances, or exposing one to dan‐ gerous financial liability. Let’s not spend too much time in the darkness.  Oftentimes  today,  the  term  SLO  is  used,  and  for  the  purposes  of this text, an SLA is simply an SLO that two or more parties have “agreed” to. SLO and SLA are used fairly interchangeably herein, but  we  make  an  effort  to  consider  SLAs  as  “external”  multiparty agreements and SLOs as “internal” single-party goals.  355   Although the concept of an SLA is quite generic and it simply outlines in clear terms how a service will be delivered to a consumer, at least in the world of computing, they tend to focus on two specific criteria: availability and quality of service  QoS . Now, QoS can and should mean different things depending on the type of service. Note that producer and consumer ultimately means business and customer, respectively, but oftentimes, as we define the SLO between components in an architecture, it can mean producing component and consuming component. Examples of a producing compo‐ nent would be a network accessible block storage system and an authorization micro‐ service API; each would have many disparate customers and make promises to those other services about availability, performance, and sometimes even safety. SLAs  are  actually  quite  simple  in  that  you  aim  to  limit  your  risk  by  not  over- promising  and  appeal  to  customers  by  providing  assurances  that  make  them  feel comfortable consuming your services. The devil is in the details; while the concept is straightforward, there is art in choosing what to promise and science in quantifying how to promise it. In an SLA, you’re always promising something over time. This time is often monthly, sometimes daily. The time period over which you promise tends to align with your standard  billing  cycles  but  matches  your  refund  policy  perfectly.  This  last  part  is important.  If  you  are  promising  something  over  the  course  of  a  day  and  fail  to deliver, you are likely going to back up that promise with a refund for that day’s ser‐ vice. The implications are quite clear when that promise is for a month. Obviously,  many  SLAs  use  multiple  time  windows  to  provide  a  balance  between exposure and assurance. For example, if the SLA is violated for 1 minute in a given day, that day will be refunded, and if the SLA is violated for 1 hour in a given month, the entire bill for that month is forfeit. I will talk no more of the ramifications of SLA violation in terms of refunds, but instead of just violation. How you choose to recom‐ pense for a broken promise is out of the scope of this book; good luck. The  takeaway  here  is  that  SLAs  make  sense  only  when  considered  in  fixed  time‐ frames, which are called assurance windows. Given how services are trending today and  how  major  service  providers  disclose  their  outages   a  simplified  term  for  SLA violations ,  I’ll  assume  that  we  use  a  daily  context.  All  the  concepts  herein  can  be applied if that window is different, but do the math and understand your exposure when changing that assurance window. Now, let’s look at this in context instead of the abstract. I’ll tackle availability before QoS because that’s easier to reason about. Availability Availability simply means that your service is available to consume. It functions in only  the  most  basic  sense  of  the  word.  It  does  not  mean  that  consumers  get  the  356      Chapter 21: The Art and Science of the Service-Level Objective   answers they expect when they expect them. I’ll give a few examples of availability that might help illustrate the point. If you send a package via a major carrier, I would consider it available if the driver comes and picks up your package and it is delivered to the destination. The driver could show up late to pick it up, the contents could be exposed to extreme tempera‐ ture and thus be damaged, and it could show up three weeks later instead of two days, as requested. It’s available, but utterly unsatisfying. Imagine for a moment the oppo‐ site assurance: your package would be delivered on time in pristine condition every time it shipped, but the carrier never showed up to take the package. Now we have perfect QoS, but an utterly unsatisfying experience because it is unavailable. Availability in computing systems seems like it would be simple to quantify, but there are  many  different  ways  to  measure  availability,  and  if  you  articulate  them  incor‐ rectly, you are either exposed horribly or effectively promising nothing to the con‐ sumer.  The  latter  might  sound  great,  but  you’ll  eventually  face  the  music:  higher standards lead to success and consumers are smart. The most common ways to measure availability are the marking of time quanta or counting  failures.  The  classic  SLA  language  of  “99.9%  uptime”  can  put  into  these terms. Time Quanta The idea of time quanta is to take your assurance window  one day  and split it up into pieces. If we were to choose minutes, we’d end up with 1,440 time quanta in that day  unless it is a daylight saving time shift that leads you to question why you work in this field and how, given our bad ideas, humans are still alive at all . Within  each  of  those  time  quanta,  you  can  measure  for  failure.  If  any  failures  are detected, the specific time quantum is marked as bad. At the end of the day, your availability  is  simply  the  unmarked   good   time  quanta  over  the  total   1,440 : 1,439 1,440 = 99.930% and 1,438 1,440 = 99.861%. So, with a 99.9% uptime guaran‐ tee, you are allowed 1 minute of downtime in that day, but 2 and you’ve violated your SLA. This seems simple enough, but it has some flaws that can expose you unnecessarily or provide  no  acceptable  assurance  to  your  consumers—or  both.  I’m  going  to  use  an API service to articulate how this is. Let’s assume that we have a shipping-calculator API service that takes package weight, source, and destination postal code and pro‐ vides a price. If we do 100 million transactions per day, that would average to almost 70,000 per minute. If we failed to service one each minute, we’d violate every time quantum of our SLA and have 100% downtime. At the same time, we’ve only failed 1,440 out of 100 million transactions, which is a success rate of 99.998%. Unfair!  Availability      357   On the flipside, if the consumers really use the app only between 10 AM and 10:30 AM  when  they’re  scheduling  all  of  their  packages  for  delivery,  we  fail  every  single transaction from 10:20 to 10:21 and still meet our SLA. From the customer perspec‐ tive this is doubly bad. First, we’ve met our SLA and disappointed more consumers  3,333,333 failed transactions versus 1,440  and we’ve had an effective uptime of 1 minute out of the 30 that is important to them: a perceived uptime of only 96.666%. If your usage is spread evenly throughout the day, this approach can work and be simple to understand. Most services do not have an even distribution of transactions throughout a day, and, while simplicity is fantastic in SLAs, this method is too flawed to use. Transactions Another common method is to use the raw transactions themselves. Over the course of our assurance window, we simply need to count all of the attempted transactions and the number successfully performed. In many ways, this provides a much stronger assurance  to  a  client  because  it  clearly  articulates  the  probability  for  which  they should expect to receive service. Following the earlier example, if we have 100 million transactions and have a 99.9% uptime guarantee, we can fail to service 100,000 without breaking our SLA. Keep in mind that this might sound like a lot, but at an average 70,000 per minute, that is identical  to  a  continuous  2-minute  outage  resulting  in  an  SLA  violation.  The  big advantage should be obvious; it allows us to absorb aberrant behavior that affects a very small set of consumers over the course of our entire assurance window without violating the SLA  mutual best interest . One  significant  challenge  is  that  it  requires  you  to  be  able  to  measure  attempted transactions. In the network realm, this is impossible: if packets don’t show up, how would we know they were ever sent? Regarding online services, they are all connected by  a  network  that  could  potentially  fail.  This  means  that  it  is  impossible  for  us  to actually measure attempted transactions. Wow. So, even though this is clearly an ele‐ gant solution, it allows for too much deniability on the part of the producer to pro‐ vide strong assurances to the consumer. How can we fix this? Transactions over Time Quanta The  best  marriages  expose  the  strengths  of  the  partners  and  compensate  for  their weaknesses.  This  is  certainly  the  case  for  the  marriage  of  the  previous  two approaches. I like to call this “Quantiles over Quantums” just because it sounds cat‐ chy. By  reintroducing  the  time  quantum  into  transactional  analysis,  we  compromise  in the true sense of the word. We remove the fatal flaw of transactional quantification in  358      Chapter 21: The Art and Science of the Service-Level Objective   that if the “system is down” and attempted transactions cannot even be counted, we can  mark  a  quantum  as  failed.  This  tightly  bounds  the  transaction  method’s  fatal flaw. On the flip side, the elegance of the transactional method allowing insignificant failures throughout the entire assurance window is reduced in effectiveness  precisely by the number of time quanta in the assurance window . Now, in our previous example of 100 million transactions per day, instead of allow‐ ing for up to 100,000 failures throughout the day, we now have an additional con‐ straint that can’t have more than 69 per minute  assuming that we preserve the 99.9% to apply to the time quantum transaction assurance . The articulation of this SLA would be as follows:  The service will be available and accept requests 99.9% of the minutes  1,439 of 1,440  of each day. Availability is measured each minute and is achieved by servicing a mini‐ mum of 99.9% of the attempted transactions. If a system outage  within our control  causes  the  inability  to  measure  attempted  transaction,  the  minute  is  considered unavailable.  All of this is just a balancing act; this is the art of SLAs. Although it is more difficult to achieve this SLA, when you do miss, you can feasibly make statements like, “We were in violation for 8 minutes,” instead of, “We violated the SLA for Wednesday.” Calling an entire day a failure is bad for business and bad for morale. SLAs do a lot to inspire confidence in consumers, but don’t forget they also serve to inspire staff to deliver amazing service. For many years, we’ve established that “down” doesn’t mean “unavailable.” The man‐ tra “slow is the new down” is very apt. Most good SLAs now consider service times slower than a specific threshold to be “in violation.” All of the previously discussed methods still apply, but a second parameter dictating “too slow” is required. On Evaluating SLOs The first and most important rule of improving anything is that you cannot improve what you cannot measure; at least, you cannot show that you’ve improved it. We’ve talked about the premise of how SLOs should be designed. I’m in the “Quan‐ tiles of Quantums” camp of SLO design because I think it fairly balances risk. The challenge is how to quantitatively set SLOs. SLAs are data-driven measurements, and the  formation  of  an  SLO  should  be  an  objective,  data-driven  exercise  as  well.  Too often, I see percentiles and thresholds plucked from thin air  or less savory places  and aggressively managed too; this approach is fundamentally flawed. It turns out that quite a bit of math  and largely statistics  is required to really under‐ stand  and  guide  SLOs—too  much  math,  in  fact,  to  dive  into  in  a  single  overview chapter. That said, I’ll try to give the tersest summary of the required math.  On Evaluating SLOs      359   First, a Probability Density Function  PDF  is a function that takes a measurement as input and a probability that a given sample has that measurement as an output. In our systems these “functions” are really empirical sets of measurements. As an exam‐ ple, if I took the latency of 100,000 API requests against an endpoint, I could use that data to answer an estimation to the question: “What’s the probability that the next request will have a latency of 1.2 ms?” The PDF answers that question. A close dancing partner of the PDF is the Cumulative Density Function  CDF . It is simply the integral  hence the name “cumulative”  of the PDF. The CDF can answer the  question:  “What’s  the  probability  that  the  next  request  will  have  a  latency  less than  or greater than  1.2 ms?” Probabilities range from 0  nope  to 1  certainly , just as percentages range from 0% to 100%. Percentiles  like 99th percentile  use percentages as their terms; “quantiles” are the same thing, but they use probabilities as their terms. The 99th percentile is the 0.99 quantile or q 0.99 . The quantile function is simply a mapping to the CDF; in a typical graph representation of a CDF, we ask what is the x-axis value when the y-axis value is 0.99? In Figure 21-1, q 0.99   or the 99th percentile  is about 270.  Figure 21-1. CDF function, 0.99  y-axis  270  x-axis   Oftentimes  in  computing,  you’ll  hear  normal  or  Pareto  or  gamma  distributions, which are different mathematical models that indicate how you expect your measure‐ ment samples to be distributed  Figure 21-2 . The hard truth is that you will almost never see a normal distribution from a real-world computing system; it just doesn’t happen.  Things  sometimes  look  like  gamma  distributions,  but  computers  and  sys‐  360      Chapter 21: The Art and Science of the Service-Level Objective   tems of computers are complex systems and most sample distributions are actually a composition of several different distribution models. More important, your model is often less important than the actual data.  Figure 21-2. Common PDFs and CDFs  Pareto distributions model things like hard disk error rates and file size distributions. In my experience, because slow is also considered down, most SLAs and SLOs are articulated in terms of latency expectations. We track the latencies on services to both inform the definition of external SLAs as well as assess our performance to our inter‐ nal SLOs.  On Evaluating SLOs      361   Histograms Histograms allow us to compress a wealth of dense information  latency measure‐ ments  into a reasonably small amount of space informationally while maintaining the  ability  to  interrogate  important  aspects  of  the  distribution  of  data   such  as approximating quantile values . In Figure 21-3, we see what represents the seconds of latency under real-world usage. The x-axis is in seconds, so the measurements of 1.0 m and 1.5 m are in milli-units  in this case milliseconds . The y-axis represents the number of samples. The area of each  bar  represents  a  set  of  samples  that  fall  into  the  latency  range  depicted  by boundaries of the bar on x-axis. Just underneath the x-axis, we can see markers indi‐ cating “quantile boxing” showing the q 0   the minimum , q 0.25   the 25th percen‐ tile , q 0.5   the median , and q 0.75   the 75th percentile . The q 1   the maximum  isn’t visible on the graph, as much of the long tail of the distribution is outside the right side of the viewport of the graph. The vertical line  m  indicates the arithmetic average  mean  of the distribution. The graph in Figure 21-3 packs a punch! There’s a wealth of information in there.  Figure 21-3. An example latency histogram for data service requests  On the left side of the histogram, we see a spike  A , called a mode, that represents services  that  were  served  fast   likely  entirely  from  cache   and  the  distribution between  A  and 1.0 ms looks composed of several different behaviors  B ,  C , and  D . If you refer to the gamma distribution PDF graph, this looks a bit like several of those  graphs  stacked  atop  one  another.  The  largest  samples  in  this  histogram  are actually “off the graph” to the right in part of the distribution’s long tail  LT , but the blue underbox indicates that p 100   our “maximum”  is 120 ms. It’s important to recognize that most real-world distributions have complex cumulative patterns and long-tail  distributions,  making  simple  statistical  aggregates  like  min,  max,  median, average, and so on result in a poor understanding of the underlying data.  362      Chapter 21: The Art and Science of the Service-Level Objective   Given the data that is used to draw this histogram  bins and sample counts , we can estimate arbitrary quantiles  and inverse quantiles  and more advanced characteris‐ tics of the workload such as modal cardinality  the number of bumps in a histogram . In  addition  to  some  typical  99th  percentile  SLA,  we  might  also  want  to  establish internal SLOs that state 75% or more of the requests against this service should com‐ plete in 1 ms or less. The histogram in Figure 21-3 happens to comprise samples over a 1-second period and contains more than 60,000 samples. If you’ve been looking at “averages”  which happens to be vertical indicator line  of your data over time, this visualization of 1 second’s worth of data should be a wake-up call to the reality of your system behavior. The  more  challenging  question  in  setting  an  SLO  around  this  service  is  “how  fast should it be for most consumers?” The graph should inform us whether we’re suc‐ ceeding, but the business and technical requirements around the service should be the  driver  for  the  answer  to  that  question.  In  this  particular  case,  this  is  a  data- retrieval  API  and  the  applications  that  it  powers  have  negotiated  that  99%  of  the requests they make should be services within 5 ms. Doing analysis on the data, the previous example has a q 0.99  = 3.37 ms. Yay! Where Percentiles Fall Down  and Histograms Step Up  Knowing that our q 0.99  is 3.37 ms, we’re currently “exceeding” our SLO of 5 ms at 99%. But, how close are we to the edge? A percentile grants no insight into this. The first sample slower than the 99th percentile sample could be 10 ms or  more likely  could be less than 5 ms, but we don’t know. Also, we don’t know what percentage of our consumers are suffering from “slow” performance. Enter  the  power  of  histograms,  given  the  whole  histogram  to  work  with,  we  can actually  count  the  precise  number  of  samples  that  exceeded  our  5-ms  contract. Although you can’t tell from the visualization alone, with the underlying data we can calculate that 99.4597% of the population is faster than 5 ms, leaving 0.5403% with unsatisfying service, or 324 of our roughly 60,000 samples. Further, we can investi‐ gate the distribution and sprawl of those “outlier” samples. This is the power of going beyond averages, minimum, maximums, and even arbitrary percentiles. You cannot understand  the  true  behavior  of  your  systems  and  think  intelligently  about  them without measuring how they behave. Histograms are perhaps the single most power‐ ful tool in your arsenal. Parting Thought: Looking at SLOs Upside Down As an industry, we have arbitrarily selected both the percentile and the performance for which we set our SLOs. There should be more reason to it than that. If you are measuring at q 0.99 , is that really because it is satisfactory that 1% of your consum‐  Where Percentiles Fall Down  and Histograms Step Up       363   ers  can  receive  a  substandard  experience?  This  is  something  to  think  about  and debate within your team and organization as a whole. It  has  also  been  standard  to  measure  a  specific  quantile  and  construct  the  SLO around the latency at that specific quantile. Here is where I believe the industry might consider looking at this problem upside down. What is special about that 1% of con‐ sumers that they can receive worse than desirable performance? Would it be better if it were 0.5% or 0.1%? Of course it would. Now, what is special about your perfor‐ mance criteria? Is it the 5 ms in our data retrieval example  or perhaps 250 ms in a user experience example ? Would it be better if it were faster? The answer to this is not so obvious? Who would notice? Given  this,  instead  of  establishing  SLOs  around  the  q 0.99   latency,  we  should  be establishing SLOs around 5 ms and analyze the inverse quantile: q–1 5 ms ? The out‐ put of this function is much more insightful. The first just tells us how slow the 99th percentile is, while the latter indicates what percentage of the population is meeting or exceeding our performance target. Ultimately,  SLOs  should  align  with  how  you  and  your  potential  consumers  would expect the service to behave. SLOs are simply the precise way you and your team can articulate these expectations. Make SLOs match you, your services, and your custom‐ ers; another organization’s SLOs likely make no sense at all for you. Another advantage of SLOs is that by exceeding your SLOs they quantitatively play into your budgets for taking risk. When you’re close to violating an SLO, you turn the risk dial down; if you have a lot of headroom, turn the risk dial up and move faster or innovate! Further Reading   Navidi,  William.   2009 .  Statistics  for  Engineers  and  Scientists.  New  York:  McGraw-Hill Education.    Limoncelli, Thomas A., et al.  2014 . The Practice of Cloud System Administra‐ tion:  DevOps  and  SRE  Practices  for  Web  Services,  Vol.  2.  Boston:  Addison- Wesley Professional.   Theo Schlossnagle has been architecting, coding, building, and operating scalable sys‐ tems for 20 years. As a serial entrepreneur, he has founded four companies and helped grow  countless  engineering  organizations.  Theo  has  had  about  200  speaking  engage‐ ments on software, operations, and many meta issues related to the tech industry.  364      Chapter 21: The Art and Science of the Service-Level Objective   CHAPTER 22 SRE as a Success Culture  Kurt Andersen, LinkedIn  If you read much about the practices of Site Reliability Engineering  SRE  or the pro‐ fessional concerns discussed by site reliability engineers  also SRE , you could quickly gain the impression that the central concept is failure, or the results of failures, or having fewer failures, or avoiding failure in distributed computing systems. However, SRE  is  most  productive  and  valuable  when  focused  on  achieving  business  success than when focused on preventing or mitigating failure. Peter Senge captured some of the key mental shifts that characterize site reliability engineering:  [a] shift of mind from seeing parts to seeing wholes, ... from reacting to the present to creating the future.1  Unlocking  the  full  benefit  of  SRE  involves  cultural  changes  to  empower  teams  to optimize the full-service life cycle toward successfully delivering the business metrics  service level  to delight their users. SRE teams and practices are most effective when aligned with a supportive corporate culture. To get the full value from the SRE work in your company, it is also important to ensure that site reliability is considered pro‐ actively throughout the life cycle of services—from ideation through retirement. Where Did SRE Come From? The idea of SRE coalesced sometime around 2003 to 2007. This is roughly the same period when the idea of the “DevOps” movement came into being, and both origina‐ ted in the tech sector of Silicon Valley. Although DevOps focuses on the leading edge of  how  to  reliably  create  and  deploy  software  into  production,  the  companies  that  1 Senge, Peter M.  1990 . The Fifth Discipline: The Art and Practice of the Learning Organization. London: Ran‐  dom House.  365   were the innovators for SRE already had effective practices which would be labeled Continuous  Integration   CI   and  Continuous  Deployment   CD   today.  With  these fundamentals already in place, SRE practice was able to focus later in the value chain on customer benefit.  SREs work at the intersection of software and systems engineering to sustain highly reliable  and  scalable  distributed  systems  engineering  and  all  of  the  necessary  disci‐ plines to support those practices.2  The complete ecosystem of any given product or service includes not just the utilities  equipment, software, third-party functions  that are used to provide the service, but also the data, other service providers, the users, and the complex interplay among all of those components. All of these components are in the purview of a mature SRE team.  To  accomplish  their  work,  SREs  rely  on  measurement  and  data  rather  than guesses or instinct, following Lord Kelvin’s dictum:  When you can measure what you are speaking about, and express it in numbers, you know something about it; but when you cannot measure it, when you cannot express it in  numbers,  your  knowledge  is  of  a  meagre  and  unsatisfactory  kind;  it  may  be  the beginning of knowledge, but you have scarcely, in your thoughts, advanced to the stage of science, whatever the matter may be.3  SRE uses measurement to define and track the adherence to Service-Level Objectives  SLOs  and leading indicators that are traditionally known as Key Performance Indi‐ cators  KPIs . It is still somewhat challenging to directly measure “user satisfaction,” but SREs are ultimately focused on increasing user satisfaction and business success through providing the right levels of reliability and responsiveness that allow the rest of  the  development  teams  to  deliver  a  continuously  improving  feature  set  to  the users. Tim O’Reilly described SRE in WTF? What’s the Future and Why It’s Up to Us as “… its core [is] the practice of ‘debugging’ the disconnect between software development and operations…building new connective tissue.” This is particularly the case with regard to the complexities that arise as systems move from the domain of being com‐ plicated  into  being  complex chaotic  as  distributed  architectures.  Even  though  this characterization is true—SREs are highly capable of debugging complex distributed systems—that capability comes as a byproduct of focusing on entire system aggre‐ gates and, at the same time, being able to zoom in to critical minutiae that affect the behavior and performance of systems. In the most effective engagements, site reliabil‐ ity considerations are engineered into services from their initial conceptual stages just like bugs are most cheaply fixed early in the software life cycle.  2 Tim O’Reilly, WTF? What’s the Future and Why It’s Up to Us 3 Lecture on “Electrical Units of Measurement”  May 3, 1883 , published in Popular Lectures, Vol. I, p. 73.  366      Chapter 22: SRE as a Success Culture   SRE practices have arisen among groups and organizations supporting service deliv‐ ery models that have been “born in the cloud.” The lifeblood of these organizations is the continuous delivery of online value. With an online service, users are impacted in near real time by changes to the system rather than being removed in both time and place via some intermediate software distribution mechanism. If the site is not “up” and performing correctly with adequate response speed, it is not being reliable and it is not delivering the intended benefit to the end users. This immediate or near-real- time feedback loop facilitates reliability engineering at a whole-site level. Rapid feed‐ back  for  large-scale  system  behaviors  enables  a  qualitatively  different  approach, analogous to the way that unit tests and Test-Driven Design  TDD  changed software engineering and increased the reliability of individual pieces of software. Reliability engineering is a broader discipline that encompasses safety disciplines. SRE practices, as applied to computing functions, are extending beyond their cloud-origin cultures to  be  implemented  in  other  contexts,  including  critical  internal-facing  IT  services, banking, manufacturing and other segments. Key Values for SRE There are four key values for the practice of site reliability. Different organizations might phrase them slightly differently or have different relative rankings, but they are widely spread in the zeitgeist of the profession. Without establishing these as funda‐ mental  cultural  values  SRE  will  not  exist  in  the  way  it  is  currently  known.  Just  as Effective DevOps focused on the crucial importance of “building a culture of collabo‐ ration, affinity, and tooling at scale,” SRE is much more than a collection of techni‐ ques and practices. The techniques and practices organically grow out of core cultural values. Here are those fundamental values:    Keeping the site up   Empowering dev teams to “do the right thing” distributed decision making   Approaching operations as an engineering problem   Achieving business success through promises made, measured, and fulfilled  Keeping the Site Up For SRE teams, keeping the site  or website  “up” is the ultimate goal of all of their day-to-day work efforts. Although the use of the phrase “site up” and even the use of the term “site” within SRE hearkens back to the origins of the SRE practice, SRE work and skills are being employed on a growing basis to keep services, networks, internal and external-facing infrastructure, and products reliable. As “software is eating the  Key Values for SRE      367   world”4 and capabilities that were once the province of custom silicon move to hori‐ zontally  scaled  general-purpose  hardware  complemented  with  the  right  software, SRE practices become more important than chipset design or assembling the right constellation of unique hardware. For the sake of terminology, I will stick with the term “site”  or “website”  in this chapter, but please read that term in a widely inclu‐ sive sense. Nothing constrains the value of SRE practices to the HTTP protocol. As  pointed  out  in  “Gray  Failure:  The  Achilles’  Heel  of  Cloud-Scale  Systems”5,  for modern websites, there can be a lot of nuance in what “up” means. Distributed sys‐ tems can fail in catastrophic ways, such as the British Airways incident on May 27, 2017,6 and further notes a week later,7 but generally the influence of reliability engi‐ neering  will  serve  to  mitigate  the  impact  into  what  might  more  appropriately  be called “gray failures” or a “brown out” rather than a full “black out” outage. Some of the techniques that SREs use to keep as much as possible of the functionality opera‐ tional include the following:    Isolated failure domains   Redundant systems   Graduated degradation of service under load  Isolated failure domains If a site is built using a monolithic code base and with a single backing database, it will be vulnerable to a full “black-out” experience if any of those single points of fail‐ ure experiences an outage. To guard against such a scenario, site reliability practices will segment the code  using either a microservice architecture or through functional segmentation across different failure domains  and also the underlying data store so that a failure in any critical component will not take everything down. Note that in this aspect, as in a variety of others, SRE is extending good software engineering prac‐ tices to a “system” or more “macro” level. At each level, similar principles apply, but the tools that are available differ as the scale changes. Highly reliable system design includes a constant awareness to avoid single points of failure. As the use of horizontally scaled services across commodity hardware became more economical and commonplace than investing in “big iron,” distributed system  4 Marc Andreessen [paywall], “Why Software Is Eating The World”, republished at Andreessen Horowitz. 5 Peng Huang, Chuanxiong Guo, Lidong Zhou, Jacob R. Lorch, Yingnong Dang, Murali Chintalapati, Ran‐  dolph Yao, “Gray Failure: The Achilles’ Heel of Cloud-Scale Systems”  6 Jude Karabus, “BA’s ‘global IT system failure’ was due to ‘power surge’”. 7 Gareth Corfield, “BA IT systems failure: Uninterruptible Power Supply was interrupted”.  368      Chapter 22: SRE as a Success Culture   principles became much more important to the proper execution of systems. Perhaps foremost among those principles is the conclusion drawn from the fallacies of dis‐ tributed  computing  that  anything  can  fail  at  any  time,  even  things  that  used  to  be considered, in simpler environments, rock solid. Working at scale has another conse‐ quence, too: problems affect only “1 in a million” are painfully frequent when dealing with hundreds of millions or billions of events. In such an environment, minimizing the losses from expected failures becomes a critical design guideline. The goal has to shift away from failure avoidance toward reducing or containing failure impact.  Redundant systems Just like hardware teams have learned to employ multiple fallback redundancies, site reliability teams also employ redundancy to protect their sites. Redundancy can be through multiple locations, multiple copies of data, and multiple providers of services at every level of the stack, from Domain Name System  DNS  through Content Deliv‐ ery Network  CDN .  Graduated degradation Not all features are equally important and not all users need to use the same sets of features. As part of engineering reliable performance, economic and business trade- offs  can  be  made  to  ensure  the  most  important  features  continue  to  operate  while sacrificing less important features when there are problems within the environment. Ideally, well-designed boundaries exist between “sacrificial” systems and “essential” systems, and continuing to work with development teams to ensure that graceful deg‐ radation under failure conditions is a key function for site reliability teams. Empowering Teams to “Do the Right Thing” SREs focus on the operation of distributed systems, and the best teams recognize that they themselves are also subject to the same stresses and failure modes that they work to manage in their sites. To achieve the “Holy Grail” of scaling their own teams and effectiveness, SREs need to strive toward both educating their development counter‐ parts and empowering them to find the right information and make the right choices: fostering a shared ownership around the importance of “site up.” A lot of this hap‐ pens by cultivating shared values as well as demonstrating the ongoing value that SRE teams bring to the table. When a company grows to have distinct development teams, the individual develop‐ ment  teams  usually  end  up  being  highly  focused  on  the  particular  portion  of  the whole site “stack” for which they are responsible to deliver features. The rest of the stack  will  fade  into  a  peripheral  awareness,  but  for  the  SRE,  the  way  in  which  the individual pieces of the site fit together and interoperate is fundamental. This impor‐ tant perspective is stewarded by SREs as individual teams make design decisions.  Key Values for SRE      369   By  providing  tooling  frameworks  to  instrument  and  visualize  site  and  individual  micro service performance, SREs help the development teams to identify both good and bad aspects of their individual pieces of the website. With the right tools to facili‐ tate observability as well as monitoring and alerting around the measurements, SREs enable developers to respond to how their code is performing in near real time with live user interactions. This enables rapid iteration toward the achievement of business goals for each component of the website or service. Giving development teams the capability to understand the effect of their pieces in the context of the whole is both fundamental to the effectiveness of the SRE teams and to promoting a culture of self-responsibility in which problems are not “thrown over the wall” in an oppositional us-versus-them dynamic. Approaching Operations as an Engineering Problem Brian Koen describes engineering as follows:  pursuing a strategy for causing the best change in a poorly understood or uncertain situation within the available resources.8  Site reliability as a role has emerged to handle the complexity of modern distributed systems along with the near-real-time feedback loops that are enabled by online ser‐ vice delivery. Computer systems and networks used to be within the realm of per‐ sonal comprehensibility—as reflected in the naming patterns that would encompass the entire group of machines within the span of the “seven dwarves.” As systems in aggregate grew from being treated like pets to the scale where they need to be man‐ aged more like a herd of cattle, and now people are suggesting that they need to be considered even more transitory  or prolific  like poultry, the combinatorial compli‐ cations have undertaken a phase change into being not just complicated, but com‐ plex. Complex distributed systems do not necessarily respond in intuitive ways and require tooling to both expose inner state information as well as to exert control over the sys‐ tems. Complex systems are also characterized by nonlinear effects, so tooling is criti‐ cal for safe management of the larger systems. Only through this tooling can a team have any opportunity to handle the ever-expanding number of systems and software services that execute upon them, often coming into existence, performing activities for a short time, and then going away again while other services take their place. This ever-shifting and growing landscape is why “engineering” is a key discipline for strategically  working  toward  site  reliability.  As  Dave  Zwieback  wrote  in  Beyond Blame  O’Reilly :  8 Cited by John Allspaw: Koen, Billy V.  1985 . Definition of the Engineering Method. Washington, DC: Ameri‐  can Society for Engineering Education, p. 5.  370      Chapter 22: SRE as a Success Culture   The root cause for both the functioning and malfunctions in all complex systems is impermanence  i.e., the fact that all systems are changeable by nature . Knowing the root cause, we no longer seek it, and instead look for the many conditions that allowed a particular situation to manifest. We accept that not all conditions are knowable or fixable.  Mathias Lafeldt summarized this “single root cause” as: “All things are compounded objects in a continuous change of condition.”9 The daily task for an SRE team is to deal with the inconstant nature of the systems that they support: a point-in-time snapshot gives one an impression of a solid net‐ work of interrelated services, but the reality is that the relationships between the serv‐ ices and even the composition of the services themselves are an ever-shifting matrix of changeable components. These changeable components are further obscured from true understanding by the nature of the human–computer interface and even basic physics.10 As  described  in  the  Stella  Report  and  shown  in  Figure  28-1   also  from  the  Stella Report , people are critically dependent upon the tools that they build to represent information about their systems. Tim O’Reilly describes this in What the Future? as follows:  Every Silicon Valley firm builds two intertwined systems: the application that serves users, and a hidden set of applications that they use to understand what is happening so that they can continually improve their service.  Building  and  maintaining  the  “hidden  set  of  applications,”  or  tooling—which  can ensure that an up-to-date mental model is readily available to everyone who needs it —is a significant function of the SRE team. Achieving Business Success Through Promises  Service Levels  All of the work that goes into SRE must be oriented toward the fundamental core business objectives and the contributing enablers for the achievement of those objec‐ tives, or it will be wasted. As Peter Drucker wrote: There is surely nothing quite so useless as doing with great efficiency something that should not be done at all.11  9 Lafeldt, Mathias.  2017 . “Impermanence: The Single Root Cause”, Medium.com. 10 Watch Grace Hopper illustrate a nanosecond. Understanding the state of a computer system that executes  multiple instructions per nanosecond when your view of that system is an indeterminate length of time removed from the real execution means that the interface through which you come to understand the system is, at best, seeing “through a glass darkly.”  11 Drucker, Peter F.  1963 . “Managing for Business Effectiveness,” Harvard Business Review.  Key Values for SRE      371   Successful  SRE  teams  both  understand  the  business  metrics  and  have  developed frameworks of measurement that can help achieve those goals, supporting both site performance and usage patterns. The treatment of Service-Level Agreements  SLAs , objectives, and indicators covered in Site Reliability Engineering is probably the can‐ onical framing of these topics. The most important aspects of service levels  agree‐ ments,  objectives,  and  indicators   have  to  do  with  the  alignment  of  objectives between the development teams and their SREs. The goal is for both groups to work together  to  define  the  targets,  achieve  them,  and  maintain  performance  on  a  daily basis, in spite of the ongoing evolution in site components and user demand. Setting these benchmarks and then developing and executing plans to achieve them distinguishes SRE from other related disciplines such as software engineering. It is also the mechanism within the mantra: “Hope is not a plan.”  Progression in Service-Level Execution Initial  efforts  at  service-level  monitoring  often  start  because  of  contractual  obliga‐ tions.  At  this  level,  the  SLAs  are  extrinsically  driven  and  often  reflect  only  gross measures such as availability or some components of response times. The monitoring is often highly selective or ad hoc, generating contractually mandated reports on a periodic  basis  and  motivated  by  the  desire  to  avoid  financial  penalties.  Neither insight nor understanding of the service execution is an objective at this level of exe‐ cution. As an organization progresses, it will begin to understand the limitations of ad hoc SLA reporting and the mandatory measurements that support the reports, and will develop internally motivated SLOs that will begin to provide some insights about the execution characteristics of its services. Usually these SLOs will start by measuring parameters that are relatively easy to determine or measure. More advanced organizations will develop a set of “leading indicators” or KPIs that will enable them to proactively respond to threats that can endanger the achievement of their SLOs as well as provide more nuanced understandings of how their services operate and depend on related services  internal or external . Over time, the continu‐ ous  monitoring  of  KPIs  and  SLOs  will  provide  a  baseline  from  which  experienced SRE teams can make educated inferences about the services that they support and the relationships with adjacent services throughout the entirety of their stacks. Critical Enabling Functions of SRE The achievement of service levels is a difficult task for organizations that do not have them ingrained as a cultural tenet. To support a service-level-oriented practice, SRE teams work across five pillars of practice:    Monitoring, metrics, and KPIs  372      Chapter 22: SRE as a Success Culture     Incident management and emergency response   Capacity planning and demand forecasting   Performance analysis and optimization   Provisioning, change management, and velocity  Monitoring, Metrics, and KPIs As noted previously, if you can’t measure it, you really don’t understand whatever the “it” is. Having a robust set of metrics that relate to what really matters for the busi‐ ness   KPIs   is  critical  to  developing  service-level  definitions  and  reporting  against those definitions. Because metrics are so fundamental to the practices of SRE, SRE teams will often be involved in the core enabling technologies within their companies or organizations that  allow  and  encourage  the  larger  development  community  to  instrument  their code in consistent reliable ways. SREs are often also involved on the “export” end of the metrics pipelines in making the data visible and usable by whoever needs access to it, whether software engineers managing their own code or business executives needing high-level overviews. Incident Management and Emergency Response Because  SRE  teams  are  often  at  the  intersection  of  multiple  software  development teams and involved with the system-level dynamics between different services, they play a key role in incident response. Often SREs will be involved in incident com‐ mand and coordination roles, and their system-wide perspectives can be key to effec‐ tive learnings in postmortem analysis following incidents. SRE teams can also help with the meta-task of ensuring an effective on-call function within the engineering teams such as with the open sourced Iris and Oncall tools from LinkedIn SRE organi‐ zation  and  integrating  code  commit  responsibilities  into  a  distributed  response framework, as detailed in this blog post. Capacity Planning and Demand Forecasting Frequently,  software  engineers  will  not  understand  exactly  how  their  software  is being used in practice. The gap between the designed usage and the actual usage can have  huge  effects  and  impacts  on  capital  expenditures   CapEx   and  operational expenditures  OpEx  to implement and maintain a service. SRE teams are well posi‐ tioned to measure, report, and forecast both what the system can deliver  capacity  and, when looped into business discussions and ongoing design strategy, a sense of what future usage requirements will be  demand .  Critical Enabling Functions of SRE      373   Performance Analysis and Optimization A big part of understanding capacity involves knowing how a service  or constella‐ tion of services  performs, both with and without stress  aka failure . With the meas‐ urement  frameworks  available  to  them,  SREs  are  well  equipped  to  assist  their development partners to characterize the performance of services in aggregate and use  their  system-level  point  of  view  to  suggest  optimizations  that  can  increase capacity or performance. Provisioning, Change Management, and Velocity Although different organizations will have varying approaches to provisioning hard‐ ware  and  services  and  managing  change  within  their  environment,  the  SRE  teams play a key role by improving automation whenever possible to maximize the velocity of change while preserving performance within the boundaries of the established ser‐ vice  levels.  Sometimes  this  means  that  SRE  teams  help  to  establish  or  strengthen CI CD  frameworks  and  pipelines  or  that  they  apply  automation  tooling  ideas  to “racking and stacking” or reliably operating warehouses and other logistical entities. An equally important component of provisioning is tearing down  deprovisioning  obsolete services. Unfortunately, no one likes to plan for retirement  of their services  so  the  SRE  teams,  looking  across  the  landscape  of  functions  and  services,  are  fre‐ quently the ones who have to push for the removal of the “walking dead”—zombie services with no effective engineering support that consume resources without con‐ tributing to the benefit of the business. Phases of SRE Execution Many organizations will go through three or four major phases12 as they implement the cultural shifts necessary to achieve a functional SRE practice that spans the full life cycle of the service. This progression follows a move from the reactive end of the value  chain  back  toward  the  earlier  phases  of  the  service  life  cycle.  As  SRE  teams become more effective along this spectrum, the quality of their interactions with cor‐ responding  development  teams  improves  and  their  participation  becomes  more highly desired and sought after. Phase 1: Firefighting Reactive In the first phase, teams are starting from a reactive stance. Too many moving parts, overwhelming  complexity,  unexpected  interactions  that  cascade  in  unanticipated  12 See Benjamin Purgason’s presentation, “The Evolution of Site Reliability Engineering” for more on the sub‐  ject.  374      Chapter 22: SRE as a Success Culture   ways lead the teams to feel like it will never again see the light of day, much less have a hope of being able to apply any “engineering” to a chaotic situation. In this firefighting reactive stage, teams have little recourse but to keep struggling to respond to each crisis while at the same time trying to reserve enough attention and energy to gain leverage within the untenable situation. Calling a halt to feature devel‐ opment so that teams can collectively focus on wrestling the bugs in the system down to a manageable level is one strategy for getting some relief. Teams can also be sup‐ plemented  with  extra  personnel  to  help  structure  automation  that  can  reduce  the load of manual toil. This phase can be the hardest one for a team to overcome because it requires a dual perspective: on one hand “keeping the lights on,” while at the same time building a new approach through automation and enabling adjacent teams. Being reactive, this phase is also reflective of SRE teams that are engaged late in the process of releasing a new service. Perhaps they were called in to clean up the mess of a service that is already in production but performing badly, or perhaps the service is just about to go into production without having reliability engineered into it during the design phase. Phase 2: Gatekeepers As the crisis-driven approach wanes, and the team gets past the worst of the reactive toil, the gatekeeping phase of team evolution can happen. The temptation for SRE teams  in  this  phase  is  to  set  themselves  up  as  a  gatekeeper,  a  single  choke  point through which changes to production systems must pass and be approved. At small scales, this can be mostly functional, but as scale increases, teams will be under grow‐ ing pressure to get out of the way and must cede control by empowering the develop‐ ment teams to do the right thing. SREs in this stage need to become partners, not doors. The  gatekeeping  phase  is  also  a  common  next  step  for  teams  that  are  looking  to become engaged with their dev partners earlier in the process. It can be implemented as a “launch control,” “operational readiness review,” or “release to production” pro‐ cess. Even though those terms can reflect positive engagement with the development teams, they most often indicate a gatekeeper mentality that can lock teams in an us- versus-them struggle. If SREs position themselves as the gatekeepers, this sets them in opposition to the feature teams and incentivizes feature teams to circumvent the pro‐ cess  and the SREs  whenever they interfere with the feature priorities. Phase 3: Advocates Partners By  building  jointly  agreed-upon  frameworks,  such  as  the  error  budget  concept explained in The SRE Book, Chapter 3, SREs are able to remove their teams from the  Phases of SRE Execution      375   critical  path  for  service  releases  and  rely  on  an  objective  measure.  When  everyone agrees on adhering to this measure, the SRE team is freed up to partner with the dev team s  to help them achieve the agreed target and then to continue to meet it. The SRE team is also involved with continually evolving the components of the service measured to include all important aspects that impact the user experience. The partner phase sees SRE teams involved much earlier in the development process for upcoming services. They work to ensure that the designs consider reliability in their architecture and build defenses against the most likely or expensive failures. SRE teams become much more effective because the engagement is less antagonistic with the dev teams, and design changes that are introduced early are less expensive to implement and provide longer-term value. Teams often have much higher satisfac‐ tion among their members because they feel that their work is valued by their part‐ ners. The SRE team has become an important contributor rather than being viewed as the “cleanup crew” or a gatekeeping roadblock. SRE services become sought after by the dev teams who recognize this value. Phase 4: Catalytic The final stage for SREs to operate within their organizations is in a catalytic role. Just  as  a  chemical  catalyst  facilitates  reactions  and  can  guide  the  process  of  those reactions, SREs serve to consult with development teams, bringing a consistent mind‐ fulness of the business goals, reliability, and security into the full life cycle of every service  component.  SREs  should  be  involved  from  conception  through  decommis‐ sioning of every service and help to provide the right tools so that the development teams are intimately familiar with the effects that their code has on the overall site or service function. Executing SRE at this level provides the greatest leverage for the SRE teams, enabling their influence to scale beyond their headcount increase. SRE engagement with the dev  teams  has  moved  fully  into  a  “pull”  model  with  dev  teams  seeking  SRE  input rather than the Phase 1 and 2 “push” model that is usually imposed by management. Voluntary, value-based engagement is more enjoyable for all of the participants and sustains  a  virtuous  circle  of  ongoing  benefit  to  the  company  both  internally  and externally. Complications of Differing Phases Teams in different phases of execution have significantly different priorities and allo‐ cate  their  attention  in  very  different  ways.  This  can  cause  a  lot  of  friction  because teams will seem to be speaking different languages. It is important to understand a team’s context in order to avoid misunderstandings. If a Phase 4 team talks to a Phase 1 team about engaging during the design phase with their development counterparts, the Phase 1 team is likely to consider that completely unrealistic.  376      Chapter 22: SRE as a Success Culture   From discussions with a variety of people across the industry, it appears that there are few shortcuts that can enable a team to leap from the fires of Phase 1 to the bliss of Phase  4  without  progressively  growing  through  the  various  stages  of  engagement. With high-level management support, it is possible for SRE teams to engage only in Phase  3  and  above  engagements,  but  even  then,  it  requires  significant  fortitude  to withhold support from struggling development teams. This fortitude is usually only found  in  fairly  mature  SRE  organizations  who  have  experienced  the  benefits  of engaging with willing development partners. Focus on the Details of Success Having a holistic view of each service as well as the overall site, understanding aspects of networking and system performance that are taken for granted by most feature- oriented developers, and being able to develop and execute on plans for mitigating risk to a site and its services are all daily aspects of SRE. The most effective SREs epit‐ omize a growth mindset.13 SREs need to be able to analyze problems in the context of the bigger picture of the business metrics that are relevant. Like rock climbers or alpi‐ nists, they need to be able to take issues that appear to be insurmountable and work to overcome them step by step. SREs also need to be able to uncover and solve the problems that turn up around the edges of all of the leaky abstractions that most of the development teams don’t want to know or worry about—all in pursuit of site reli‐ ability.  If  the  site  is  not  “up,”  nothing  else  much  matters.  Business  success  for  an online service relies fundamentally on the effective work and the continual focus on site reliability through engineering attention and skill while features that delight the users are brought into being. Further Reading 1. Victor Chircu, “Understanding the 8 fallacies of Distributed Systems”. 2. Carol  Dweck  at  TEDxNorrkoping,  “The  power  of  believing  that  you  can  improve”.  Business Review.  3. Carol  Dweck,  “What  Having  a  ‘Growth  Mindset’  Actually  Means”,  Harvard  4. Senge, Peter M.  1990  The Fifth Discipline. The Art and Practice of the Learning  Organization. London: Random House.  5. McChrystal,  Stanley   2015 .  Team  of  Teams:  New  Rules  of  Engagement  for  a  Complex World. New York: Penguin Group.  13 Dweck, Carol.  2007 . Mindset: The New Psychology of Success, updated edition. New York: Ballantine Books;  https:  hbr.org 2016 01 what-having-a-growth-mindset-actually-means or https:  mindsetonline.com   Focus on the Details of Success      377   6. The paradigm shift to view SRE as focused on success rather than failure parallels the change in other industries, such as air traffic management; see Erik Hollnagel et al. “From Safety-I to Safety-II: A White Paper”. EUROCONTROL 2013.  Kurt Andersen started his career dealing with big data at NASA’s Jet Propulsion Labo‐ ratory.  He’s  been  involved  with  shared  standards  since  the  early  ’90s  instigating  the PERL DBD DBI specification and now works on various IETF standards. Kurt is cur‐ rently the senior individual contributor for the Product SRE team at LinkedIn. He also works as one of the program committee chairs for the Messaging, Malware, and Mobile Anti-Abuse  Working  Group.  He  has  spoken  at  M3AAWG,  Velocity,  SREcon,  and SANOG on reliability, authentication, and security.  378      Chapter 22: SRE as a Success Culture   CHAPTER 23 SRE Antipatterns  Blake Bisset, Dropbox, Inc.  Human  brains  are  built  and  trained  for  threat  avoidance.  We  might  be  terrible  at weighing relative risk,1 but we’re excellent at picking out the one thing in a pile of other things that looks like a failure mode that we’ve seen before.2 Let’s face it. Failure is fun!3 And failure makes a good story. So, it can often be both easier and more effective to catalog the things you shouldn’t do rather than just the things you should. But  “antipatterns”  are  not  your  average  “this  one  time,  at  Foo  camp”  Tale  of  Fail. They’re the things we’ve seen go horribly wrong not once, not twice, but over and over again. Antipatterns are attractive fallacies. Strategies that succeed for a little less time  than  you  will  find  you  needed  them  to.  Common  sense  that  turns  out  to  be more common than sensible. Throughout the rest of this book, you’ll find examples of things that you should do. That’s not what I’m about here in this chapter. Think of this section as your “Defense Against the ‘D’oh!’ Arts” glossary. Or just sit back and enjoy imagining all the stuff I and a host of colleagues past and present had to screw up in order to get to the point where I could share this short list with you. SREs are not perfect. Some of these mis‐ takes I’ve even made more than once myself. That’s why they’re antipatterns.  1 Perhaps not even as reliably as plants! See this article about Hagai Shemesh and Alex Kacelnik and the accom‐  panying link to a Society for Risk Analysis article.  2 For a great reading list on risk perception, check out Bruce Schneier’s bibliography for his Psychology of  Security essay.  3 As long as it’s someone else’s. Including sufficiently-long-enough-in-the-past-you, because, let’s admit it, in  the immortal words of Bugs Bunny: “What a maroon!”  379   Antipattern 1: Site Reliability Operations  A new mission cannot always be achieved with old tools and methods. Site Reliability Operations: The practice of rebranding your operations as Site Reliabil‐ ity without fundamentally changing their approach to problems and the nature of the work they are expected and empowered to accomplish.    Site Reliability Operations is not a thing. Site reliability is a software, network, and systems engineering discipline. You cannot take a bunch of technicians sitting in a Network Operations Center  NOC , give them a GitHub account and a public cloud budget, tell them to move some stuff to containers, and magically rebrand them as SREs.4 The NOC is an outgrowth of several outmoded ideas. The first is that there are spe‐ cific people whose job is to keep the systems that have already been built running at all costs. SREs don’t do this. SREs build systems to require less human intervention and to fail less often, and they modify existing systems to remove emergent failure modes. They do not babysit, or feed the machine with blood, sweat, tears, dinosaur grease, or any other biological product. SREs  should  spend  more  than  half  their  time  building  better  systems,  rather  than conducting or documenting operational tasks. In a word, they should be engineering. Good engineering requires flow. Flow dies in an interruptive environment. Give your teams the time and space they need to keep ahead of the technical problem set by doing  engineering,  and  you  will  get  increased  efficiency  in  all  things,  even  as  you increase scale, velocity, and scope. We’ve  seen  a  lot  of  people  coming  in  to  SRE  conferences  talking  about  the  NOC they’ve built for their SREs. NOCs are cool. They’re inspirational. The best of them can make you feel like a hero with the fate of the world—or at least the business— riding on your shoulders. But hero culture is an antipattern in and of itself, and SREs don’t work in a NOC, even though NOCs originally evolved for very understandable reasons. Sometimes, you can’t beat the communications bandwidth that comes from having everyone  working  on  a  problem  in  the  same  physical  space,  but  the  tools  they’re using  to  do  that  work  shouldn’t  be  tied  to  that  room  and  neither  should  the  jobs and or people. NOC’s aren’t conducive to good engineering work.  4 Well, you can, of course. Minus the magic part. But it isn’t going to end up any better than your existing  system. This does not seem to stop people from taking this approach time and again. And then giving talks about it. All of which seem to end right at the point of the rebranding, as though that were “Mission Accom‐ plished!” in and of itself without actually effecting measurable changes in reliability.  380      Chapter 23: SRE Antipatterns   The NOC is the most open of open plan offices, with extra blinky and noisy distrac‐ tions thrown in on top of the sea of coworkers to boot. And it is incomprehensible how our industry, which prides itself on being data driven, remains so willfully data- blind to the growing scientific evidence of how utterly unsuitable the open-plan office is for the work conducted by engineering teams.5 Don’t  spend  your  time  and  treasure  building  rooms  that  try  to  bring  ops  people closer to the machines and each other 24 7. The key here is distributed sharing and collaboration from anywhere, so that just the engineers  who  should  actually  be  on  call  at  the  moment  can  respond  immediately without leaving the productive comforts of their home, office, or really well-designed cube.6 If you want to share a link to a particular plot of a time series, you should be able to post it into a chat or an incident response tool where everyone interested in the incident can then look at the same plot with the same filters for traffic, start and end time, resolution, and so on. Ideally, this tooling should share live data, not just a static graph or a screenshot, so that folks can use it as a starting point to play with theories, and to dig in and find discrepancies  or  alternate  explanations  for  whatever  is  being  held  up  as  odd  or offered as a cause. Any ephemeral data from these live links should be preservable with a checkbox flag-type operation so that it will be available later for your postmor‐ tem. Having to have everybody who wants to discuss something have to put it up on a shared monitor where other folks can’t poke at it—like you do in an NOC or an old- school war room—isn’t as good. More brains and hands able to manipulate informa‐ tion  directly  while  still  preserving  collaboration  and  sharing  will  get  you  to  a remediation of your issues faster and more consistently. And freeing your engineers from the NOC will improve their ability to deliver on actual engineering work.  5 Decreased Productivity; Decreased Well Being; Increased Sick Days; 2014 New Yorker review of literature;  Memory performs better if we have our own consistent space; Effects of interruption on engineer productiv‐ ity.  6 Anything’s better than endless tables with no visual or auditory separation, and even cubes can be cool.  Antipattern 1: Site Reliability Operations      381   Antipattern 2: Humans Staring at Screens  If you have to wait for a human to detect an error, you’ve already lost. Humans Staring at Screens: Any practice for which the detection of a problem condi‐ tion relies on a human noticing that a particular series of data is abnormal, or a combi‐ nation of several datasets is problematic, or that a particular condition is relevant to a known error or outage rather than relying on thresholds, correlation engines, velocity metrics, structured logs parsers, and other tooling to detect those conditions and sur‐ face them for analysis by only the relevant humans.    Another old NOC paradigm is that having human beings looking at data—even par‐ tially aggregated or correlated data—is a good way to detect and respond to potential problems before they get too bad. It is not. It is an adequate way, but it is not good. Machines are much better at finding patterns in large datasets, and they should be used to do so whenever possible. Even  modeling  large  amounts  of  data  in  statistically  valid  and  yet  still  humanly understandable ways is difficult, let alone consuming it in any quantity or over any prolonged period. Don’t spend your innovation and attention on getting a feel for constantly evolving complex systems. Machines don’t need lots of tricky user experi‐ ence  UX  to consume structured data. Feed it to them and then focus on grooming only the bits that matter for human consumption. Instead of watching graphs and manually feeding an alert or ticket into a server to document  and  coordinate  response  when  you  detect  a  problem   people  feeding worthwhile work to machines , build systems that can watch data for you and detect when something is going wrong. Preferably systems that can attempt some form of automated response to it, before alerting a human if the canned playbook response doesn’t  resolve  the  condition   machines  feeding  people  worthwhile  work ,  both  to prevent interruptions and to speed recovery, because humans can’t process incident detection  and  response  fast  enough  without  it  as  Service-Level  Objectives   SLOs  begin to creep above four-9s. Build tools that make it easier for engineers who spend most of their time doing engi‐ neering work in good engineering conditions  not NOCs or open-plan offices  to be notified  reliably  and  immediately  when  something  needs  human  attention  and  to achieve rapid access to information, systems, and one another as necessary.  382      Chapter 23: SRE Antipatterns   Antipattern 3: Mob Incident Response  Keep your eyes on the ball, but your feet in your zone. Mob  Incident  Response:  All-hands-on-deck  incident  handling  with  little  thought  to coordination of effort, reserves, and OSHT7 troubleshooting, sleep cycles, human cog‐ nitive limits, or the deleterious effect of interrupts on engineering work.    One  of  the  other  problems  engendered  by  carrying  over  the  NOC  model  to  SRE teams, or even of using distributed systems that don’t carefully scope the alerts they generate, is that the natural human tendency is for everyone within the reach of an alert or a troubling graph to sort of pile on and begin poking at the problem. Or, at the  very  least,  they  are  pulled  out  of  their  flow  and  have  the  condition  taking  up attention at the back of their brain until it is resolved. Not only is this disruptive to engineering work, but without extremely good policies and discipline surrounding incident response, you can actually increase the time to analyze and resolve an issue. This is especially true if multiple people begin making changes to test multiple hypotheses simultaneously, creating unplanned interactions that prevent anyone realizing they’d found the solution to the original problem or hiding destroying evidence that would help in tracing the sources of the issue. Even if you avoid such complications through good coordination, teams can end up in situations where a problem drags on without an imminent resolution, and because everyone piled on immediately, there’s no fresh set of eyes or second shift to come in and manage the situation as the first responders’ effectiveness begins to wear down. The  Incident  Command  System   ICS 8  provides  a  good  procedural  framework  for handling such situations and learning and implementing something similar9 can help no matter what your tech stack or working environment looks like. That said, we all know that relying on humans to follow procedure consistently in abnormal situations is not the best choice for avoiding problems. Why place the bur‐ den on the people you work with to do the right thing every time? Build your detection and alerting and incident management systems to allow the nec‐ essary people to engage fully with managing the problem while protecting and pre‐  7 1. Observe the situation. 2. State the problem. 3. Hypothesize the cause solution. 4. Test the solution. 8 Wikipedia entry on Incident Command System. 9 pagerduty incident response “Being On-Call” and “Incident management at Google — adventures in SRE-  land” by Paul Newson.  Antipattern 3: Mob Incident Response      383   serving the attention and energies of the others until they are needed. Sooner or later, you will be glad for the foresight. Antipattern 4: Root Cause = Human Error If a well-intentioned human can “break” it, it was already broken. Root Cause = Human Error: Blaming failures upon the well-intentioned actions of a human who came to an erroneous conclusion as to the probable outcome of a given action based upon the best understanding of the system available at that time or, more generally, reducing the explanation for any unwanted outcome to a single cause.    When systems break, it is good and right to look for factors contributing to the fail‐ ure so that we can seek to reduce the likelihood that the system will fail in that same manner again. The desire to prevent such recurrent failures is a very powerful incen‐ tive to identify causes that can be understood and remedied. When, in the course of such  an  investigation,  we  arrive  at  a  human  making  a  choice  that  brings  about  an unintended and harmful consequence, it is very tempting to stop there, for several reasons. It allows us to feel we can shift from an investigative mode to “fixing the problem,” which gives us closure and the thought we might be protected from a future problem. It is also objectively harder to map all of the possible chains of contributing factors inherent in a person as opposed to a machine. Figuring out the context in which that person was operating—their knowledge, the exact actions they took, and the myriad possible inputs and factors they bring with them, not only from the work environ‐ ment, but even from their life outside the immediately relevant interaction—is toil‐ some and imprecise at best and can produce outright misleading and erroneous data and conclusions at worst.10 We  have  collectively  cut  our  teeth  on  decades  of  dualistic  explanations  seeking  to determine  whether  the  “cause”  of  an  incident  was  a  hardware  failure  or  a  human screw-up. But even “purely” mechanical failures could probably be traced back to a human error of commission or omission by an omniscient investigator. The bearing that seized wasn’t lubricated properly, or cooled too quickly during manufacturing, or was dropped, or inserted with too much force, or should have been replaced more frequently, or, or, or.  10 Assuming the human is even available to interview afterward, which in a severe accident is not always the case, the disturbing evidence of the past few decades is that many of the things we remember never actually took place: “The movie that doesn’t exist and the Redditors who think it does” by Amelia Tait and “Why Sci‐ ence Tells Us Not to Rely on Eyewitness Accounts” by Hal Arkowitz and Scott O. Lilienfeld  384      Chapter 23: SRE Antipatterns   And the same is true conversely for “human” causes. If a human didn’t perform any of those actions appropriately, there was probably a mechanical test that could have caught it, or a greater tolerance that could have been built into the part or the system, or  a  more  visual  checklist  indicator  that  could  have  made  the  failure  to  perform maintenance  an  immediately  apparent  and  recoverable  error  rather  than  a  cata‐ strophic production failure. There are two seductive but unhelpful tendencies at play here. The first is the misper‐ ception of human or hardware or software or any other particular class of error as a cause of failure rather than an effect of a flawed system that would inevitably generate failure again, given a chance. The  second  is  the  notion  that  post  hoc  incident  analysis  can  or  should  actually  be reduced to a “root cause” at all. Postmortem analysis should have as a goal a thor‐ ough understanding of all aspects of a failure, not the search for a discrete smoking gun. All too often, the “root cause” is just the place where we decided we knew enough to stop analyzing and trying to learn more, whether because it was difficult to go further or because the step in question matches well with a previous condition we think we understand or for which we have a likely solution. Instead of a root cause, or even causes, I like to think of contributing factors, whereby any one of them might not be enough to cause the observed behavior, but they all contributed to a pattern of failure that ultimately became perceptible to users of the system.  Analysis  needs  to  allow  for  systems  where  a  surprising  and or  undesirable emergent behavior is the result of a web of many separate conditions, all of which are necessary but none of which is sufficient to bring about or initiate the problem rather than  flowing  from  a  linear  causal  chain  of  consequences  back  to  the  first  toppled domino. The traditional postmortem template has spaces for both, but the most interesting things always turn up in the contributing factors. That’s where we find things that might not even have been the most critical flaw to directly influence the outcome cur‐ rently under investigation but which might have much more profound and far reach‐ ing influence on a broader group of systems and teams and outcomes in the overall organization. So, don’t hate on those horrible humans, and don’t race to root. Causal analysis is not Capture  the  Flag.  Even  if  you  think  you  already  know  what  happened  and  where  Antipattern 4: Root Cause = Human Error      385   things “went wrong,” take your time and explore the system as a whole and all the events and conditions leading up to a problem you’re trying to analyze.11 Antipattern 5: Passing the Pager  On-call can’t be off-loaded. Passing the Pager: Assigning ultimate responsibility for responding to system failures to teams or individuals who did not create the system generating the failures.    Another hangover from the old operations world is that a lot of product developers hear  about  the  way  Google  SREs  take  the  pager  for  services  and  think  that  means incident response isn’t still the product team’s job—that they should get one of those SRE teams so they don’t have to be on call. This  isn’t  the  first  or  last  time  I  will  say  this,  but  reliability  engineering  is  velocity engineering.  One  of  the  key  characteristics  of  highly  performant  organizations  is rapid feedback loops from the moment production code is created, through integra‐ tion, testing, and deployment, right up to the performance of that production code in the real world. Divorcing the creation of software from the production consequences of that soft‐ ware  by  off-loading  the  pager  entirely  breaks  that  feedback  cycle,  prevents  rapid learning  and  iteration  on  the  part  of  the  product  team,  and  sets  teams  up  for  an antagonistic relationship as production teams attempt to gain some measure of con‐ trol over the behavior of systems created by product developers who have no incen‐ tive so strong as the delivery of features. This is not the SRE paradigm. On-call is a shared responsibility. There are a lot of different patterns for how to share it  product devs rotating through SRE stints, prod‐ uct  taking  secondary  on-call,  both  product  and  SRE  in  the  same  on-call  rotation, etc. , but even where SREs completely own the primary pager response, the product development team needs to participate in the form of a product on-call that can be contacted by the SRE team to speed resolution of any problems that aren’t purely the result of deployment or infrastructure issues. In all cases, ultimate responsibility for handling production incidents remains at all times with the team developing a service. If the operations load becomes too heavy, the product team needs to take the overflow, fix the technical debt, or might even end up losing pager support entirely.  11 Most of my theory in this area comes from Allspaw’s writings and from the cross-disciplinary sources to  which they have introduced so many production engineers. For a more detailed discussion of this topic, see John Allspaw’s “The Infinite Hows” and Woods Dekker Cook Johannesen Sarter’s Behind Human Error.  386      Chapter 23: SRE Antipatterns   Of course, if the load is just the result of a healthy service whose operational load is growing sublinearly beyond the capacity of the SRE team, growing the SRE team is an option as well. Some people are tempted to do this even when they know the system is  overburdened  with  technical  debt:  “Let’s  just  add  some  folks  for  now,  and  then we’ll fix it down the line.” But good SREs are even harder to hire than good product developers, so it’s not really possible to hire ahead of the operational load of an undesirably burdensome service without dragging a team down into an operational quagmire from which the organi‐ zation you end up with will be unable to recover. It is almost impossible to hire your way out of technical debt. You need to commit your  organization  to  valuing  and  pursuing  reliability  and  scalability  with  whatever resources you have, and that means not accepting bad scaling models, whether tech‐ nical or human. Antipattern 6: Magic Smoke Jumping!  Elite warrior hero culture is a trap. Magic  Smoke  Jumping:  valuing  incident  response  heroics  over  prudent  design  and preventive planning. This includes situations where all three are being done, but it is the IR heroics that receive the only or the most effusive public praises and rewards.    Most of us have been guilty of this. SREs are not Smokejumpers. They are not Sys‐ tems SEALs. Yes, it is a calling that requires a rare combination of skills and knowl‐ edge. Yes, we continually prepare and drill and train to handle outages when they do come. And  yes,  it  feels  good  when  people  from  all  through  the  organization—especially senior people and business or product development folks not even in your depart‐ ment—recognize you as the responder who saved the day and thank you for every‐ thing you endured in order to prevent the End of the World. Plus, we get these neat patches and achievement badges and get to tell “war stories” about “fighting fires.” But the hero culture concept that lauds and rewards responders for personal sacrifice in the face of system failures is destructive. Not only does the response itself suffer, but rewarding operational endurance rather than good engineering and prevention provides the wrong incentives and leads directly to ops churn and engineer burnout. Unrested  engineers  are  unproductive  engineers  and,  on  the  whole,  more  unhappy people than they would be without prolonged or frequent interruptions in their work and personal life. Being on call is a good way to learn how complex services fail and keep in touch with the as-built characteristics of your systems. But it should be once or twice a quarter,  Antipattern 6: Magic Smoke Jumping!      387   not once or twice a week, and it shouldn’t involve sprinting a marathon because the team is too small and there’s no one else on it or outside of it to whom you can hand off. The incident load per shift should be low enough so as not to overwhelm the time available for work, sleep, family, or any other critical component in a sustainable life —a  couple  a  week  at  most  on  average,  and  when  someone  gets  blasted  with  more than that in a shift, there should be no shame in rotating in other engineers to pick up the overflow or to pick up other duties while the on-call catches up on sleep...or life. Instead of praising someone who takes on an entire outage themselves, we should be questioning  why  we  didn’t  rotate  in  additional  personnel  as  needed,  and  asking whether the system design is as simple, reliable, resilient, and autonomous as possi‐ ble. If you’re a leader in the site reliability function at your organization, you need to do everything you can to promote a culture throughout the company or institution, all the  way  to  the  highest  levels,  that  models  sustainable  work  and  incident  response, and  praises  engineering  work  that  improves  scale,  resilience,  robustness,  and  effi‐ ciency above responders who throw themselves into the breach left by the company as a whole not prioritizing such work. Antipattern 7: Alert Reliability Engineering  Monitoring is about ensuring the steady flow of traffic, not a steady flow of alerts. Alert Reliability Engineering: Creating a monitoring logging infrastructure that results in a constant flow of notifications to the system operators. Often a result of adding a new alert or threshold for every individual system or every historically observed fail‐ ure.    Alerts need to scale sublinearly with system size and activity just like everything else SRE does. You should not become uneasy because you haven’t heard any low-level, nonurgent, spammy system notices from your logs and monitoring in the last hour and it’s quiet; too quiet, and you need to begin looking for magic smoke signals and fire up those jump planes. Stop paging yourself for anything other than UX alerts. Or, at the very most, those user-facing outages plus any imminent failures on whatever Single Points of Failure that you might not yet have engineered out of your organization. For critical failures that are not immediately customer detectable, velocity-bounded thresholds are your friends here. If you are losing systems at an unsustainable rate, or if you get down to three replicas out of five, or if your data store detects corruption that you need to  388      Chapter 23: SRE Antipatterns   head off before it ends up replicated into your clean copies, by all means page away. But don’t do it naively at every failure or sign of trouble. There’s no shame in accommodating a system with a mixed maturity model where necessary. You start with the hand you’re given, and work to change out the jokers as you’re  able.  But  you’ll  never  get  to  a  big  payday  if  you  don’t  accept  that  it’s  more important to focus effort on improving your systems than it is to burden people try‐ ing to achieve a state of flow and do effective engineering work with a stream of con‐ stant alerts and attention interrupts. Outside of that, however, host alerts are worse than no alerts. Focus on alerting for system- or at least site-wide metrics. Otherwise, you will either end up with alert fati‐ gue and miss critical issues, or the vital project work that can get—and keep—you out of operational churn will be buried under an avalanche of interrupts, and you will never start the virtuous cycles of productivity efficiency that are at the heart of SRE. In a reasonable system, outages page. Lower-grade problems that can’t be resolved automatically go into your ticketing system. Anything else you feel you have to note goes into logs, if anywhere. No email spam. Email is for high-value, actionable data created by your colleagues,12 like meeting invites and 401K renewal notices.13 Don’t accept the noise floor and either alert too much or stop alerting on it. You can fix this with aggregation and velocity trending. Antipattern 8: Hiring a Dog-Walker to Tend Your Pets  Configuration management should not be used as a crutch. Hiring a Dog-Walker: Using advanced configuration-management tools like Puppet or Chef to scale mutable infrastructure and snowflake servers to large numbers of nodes, rather than to help migrate to an “immutable” infrastructure.    Configuration management is great. It is a prerequisite for doing real reliability engi‐ neering, but we’ve seen people presenting on how they’ve managed to scale it to sup‐ port hundreds of configurations for thousands of hosts. This is a trap. You will never be able to scale your staff sublinearly to your footprint if you don’t instead use configuration management as a tool to consolidate and migrate to immutable infrastructure. And if you’re not scaling your staff sublinearly, you’re  12 Artificial intelligence does not count as a colleague. Yet. 13 High-value examples courtesy of my ranting partner Jonah Horowitz, who also contributed to this book.  Antipattern 8: Hiring a Dog-Walker to Tend Your Pets      389   not doing SRE. Or not successfully, anyway. Sublinear scaling is the SRE watchword, for people, processes, systems—everything.14 Pets < Cattle15 < Poultry.16 Containers and microservices are the One True Path.17 At least until they are potentially replaced by “serverless” functions.18 In the meantime,19 get  your  fleet  standardized  on  as  few  platforms  as  possible,  running  idempotent pushes of hermetic build and config pairings. I could throw some more buzzwords at you here, but instead I’ll just tell you to read Jonah Horowitz’s chapter on immutable infrastructure, right here in this very book  Chapter 24 . Antipattern 9: Speed-Bump Engineering  Prevention of all errors is impossible, costly, and annoying to anyone trying to get things done. Speed-Bump Engineering: Any process that increases the length of the time between the creation of a change and its production release without either adding value to or providing definitive feedback on the production impacts of the change.    Don’t become a speed bump. Our job is to enable and enhance velocity, not impair it. Reliable systems enhance velocity, and systems with quick production pipelines and accurate  real-time  feedback  on  system  changes  and  problems  introduced  enhance reliability. Consider  using  error  budgets  to  control  release  priorities  and  approvals.20  If  you aren’t, define explicitly what criteria you’re using, instead, and how they provide an effective mechanism for controlling technical debt without requiring political conflict between production and product engineering. Whatever it is needs to be nonsubjective, and not require everyone in the conversa‐ tion to have an intimate technical understanding of every aspect of every component  14 Except, hopefully, compensation. :-  15 Thank you, Randy Bias. 16 Thank you, Bernard Golden. 17 Except, of course, where they aren’t. 18 Leading to the inevitable addition “Poultry < Insects.” Or whatever slightly less bug-related mascot we even‐  tually land on FaaS offerings like Lambda, GCF, Azure Functions, and OpenWhisk. Protozoa perhaps?  19 Which given how much shorter the reign of each successive paradigm in the Mainframe > Commoditization > Virtualization > Containerization timeline has been—and how much less mature when production services began shifting to it—should be approximately 8:45 AM next Tuesday.  20 Error budgets have been covered extensively in O’Reilly’s first SRE handbook as well as in an inordinate num‐  ber of conference talks  of which I am perhaps guilty of having given an inordinate percentage . If you don’t know about them already, find the book or one of the many videos, and prosper.  390      Chapter 23: SRE Antipatterns   in the service. Change Control Boards fail both of these tests, and generally fail at sat‐ isfying reliability, velocity, and engineering time efficiency as well. Studies  have  shown  that  lightweight,  peer-review-based  release  controls   whether pair coding, or pre- or post-commit code reviews  achieve higher software delivery performance, while additional controls external to the engineers creating the changes are negatively correlated with feature lead time, deployment frequency, and system restore time in the event of a failure. They also have no correlation with the rate of change-induced failures.21 There are legitimate reasons to gate releases, but they should only be around con‐ cerns not related to the contents of the releases themselves, such as capacity planning and so forth. They should also be ruthlessly pruned and analyzed regularly to make certain  they  apply  only  to  the  minimum  set  of  circumstances  possible   completely new product launches rather than feature releases on existing products, for example, or capacity planning requirements for launches on services over a certain percentage of total system capacity  and only for as long as the constraining circumstances that require them are applicable  so long as the company is building its own data centers  clusters  rather  than  contracting  with  third-party  cloud  providers  that  can  provide capacity on demand, perhaps, or only until a standardized framework can be created to automatically handle the implications of new privacy legislation appropriately . Be very careful about placing any obstacle between an engineer and the release of a change. Make certain that each one is critical and adds value, and revisit them often to make certain they still provide that value and have not been rendered irrelevant by other changes in the system. Antipattern 10: Design Chokepoints  Build better tools and frameworks to reduce the toil of service launches. Design Chokepoints: When the only way for every service, product, and so on in an organization to be adapted to current production best practices is to go through a non- lightweight process involving direct consultation with a limited number of production engineering staff.    Your reliability team should be consulting with every product design. But your relia‐ bility team cannot scale sublinearly if they do consult on every product design. How do you resolve this?  21 Forsgren, Nicole, Jez Humble, and Gene Kim.  2018 . Accelerate: The Science of Lean Software and DevOps.  Portland, OR: IT Revolution Press.  Antipattern 10: Design Chokepoints      391   Many teams use direct consultation models, either through embedding the engineers working on production tooling and site reliability within their product development organization or by holding office hours for voluntary SRE consults and conducting mandatory production reviews prior to launch. Direct or embedded engagement has many things to recommend it, and I still use it for building new relationships with product teams or for large, complex, and critical projects. But, eventually, we reach a point where even temporary direct engagements like  tech  talks,  developer  production  rotations boot  camps,  production  readiness reviews, and office hours can’t scale. That’s no reason not to do them, because they are incredibly beneficial in their own right to collaboration, education, and recruitment. But we need something more. If you’re not looking at creating and maintaining development frameworks for your organization that incorporate the production standards you want to maintain, you’re missing  a  great  opportunity  to  extend  SRE’s  impact,  increase  the  velocity  of  your development  and  launch  processes,  and  reduce  cognitive  load,  toil,  and  human errors. Frameworks can make certain that monitoring is compatible with existing produc‐ tion  systems;  that  data  layer  calls  are  safe;  that  distributed  deployment  is  balanced across  maintenance  zones;  that  global  and  local  load  balancing  follow  appropriate, familiar,  and  standardized  patterns;  and  that  nobody  forgets  to  check  the  network class of service on that tertiary synchronization service cluster or comes out of a pro‐ duction review surprised to learn that they have three weeks’ worth of monitoring configurations  to  write  and  a  month-long  wait  to  provision  redundant  storage  in Zone 7 before they’ll be able to launch because of a typo when they filled out the ser‐ vice template.22 Antipattern 11: Too Much Stick, Not Enough Carrot  SRE is a pull function, not a push function. Too Much Stick: The tendency to mandate adoption of systems, frameworks, or practi‐ ces, rather than providing them as an attractive option that accomplishes your goals while making it easier for your partners to achieve theirs at the same time.    A common theme uniting the previous two antipatterns but extending beyond them is that you will not get any place you want to go by trying to be the gatekeepers of production, or by building a tool and then trying to force people to use it.  22 Hypothetically. Nothing like this has ever actually happened to anyone I know who wasn’t using SRE sup‐  ported frameworks at any major internet company.  392      Chapter 23: SRE Antipatterns   Worse, it has been scientifically verified that you will do harm. Organizations where teams can choose their own tooling, and which have lightweight, intrateam review processes, deliver better results faster than those trying to impose such decisions on their teams externally.23 Security and production engineering teams that make it harder for people to do their jobs rather than easier will find they haven’t eliminated any risk; they’ve only created a cottage industry in the creative bypass of controls and policies. At Dropbox, the mascot for one of our major infrastructure efforts around rearchi‐ tecting the way teams deploy services is an astronaut holding a flag emblazoned with a carrot emblem  Figure 23-1 . It’s not just cute, it’s an important daily reminder to the teams involved.  Figure 23-1. To boldly grow24  You should focus on building better developer infrastructure and production utilities such  that  product  teams  will  see  productivity  wins  from  adopting  your  tools  and services,  either  because  they  are  better  than  their  own,  or  because  they  are  “good enough”  for  the  80%  use  case  while  still  offloading  significant  development  effort, and significant operational load as well, for turning up and supporting services. Your  goal  is  to  be  building  cool-enough  stuff  by  listening  to  your  colleagues’  pain points that they will see better advantage and an increase in their ability to execute  23 Forsgren, Nicole, Jez Humble, and Gene Kim.  2018 . Accelerate: The Science of Lean Software and DevOps.  Portland, OR: IT Revolution Press.  24 Thanks to Maggie Nelson and Serving Platform for being more awesome than a box of carrot cupcakes.  Antipattern 11: Too Much Stick, Not Enough Carrot      393   from  adding  an  engineer  to  the  SRE  team  than  they  would  to  their  own,  and  will begin  lamenting  that  you  can’t  find  more  people  to  hire  rather  than  fighting  over funding.25 The only way to do this is to build good tools and collect good metrics about the real productivity  benefits  they  provide.  The  good  news  is  that  reliability  engineering  is also velocity engineering. You are not a cost center. You are not a bureaucrat. You are not a build cop. You are a force multiplier for development and a direct contribu‐ tor to getting the customer what they want: reliable, performant access to the services that make their lives better and more efficient. Antipattern 12: Postponing Production  Overly cautious rollouts can produce bigger problems. Postponing Production: The imposition of excessive lead time and testing delays in a misguided  attempt  to  prevent  any  possibility  of  system  failure,  especially  where  it interferes with the capability for engineers to get feedback on the real impacts of their changes rapidly and easily.    Sometimes, in our desire to protect production from potentially bad changes, we set up  all  sorts  of  checks  and  tests  and  bake-in  periods  to  try  to  detect  any  problems before the new bits ever see a production request. Testing  before  production  is  important,  but  we  need  to  make  sure  that  it  doesn’t introduce a significant delay between when developers make changes and when they get real feedback on the impact of their release. The  best  way  to  do  this  is  through  automation  and  careful  curation  of  tests,  and, where  possible,  through  providing  early  opportunities  for  production  feedback through dark launches, production integration canaries, and “1%” pushes, potentially even in tandem with the execution of your slower and more time-consuming tests  load,  performance,  etc. ,  depending  on  your  service’s  tolerance  for,  or  ability  to retry, errors. We need to make it possible for product developers to know the actual impacts of their release as soon as possible. Did error rates go up? Down? How about latency? We  should  expose  these  kinds  of  impacts  automatically  as  part  of  their  workflow, rather than making them go look for information. Product  developers  are  part  of  your  team.  They  should  be  able  to  see  what  results their efforts are producing in production in as close to real time as possible.  25 Amusingly, any sufficiently large carrot is also functionally a stick.  394      Chapter 23: SRE Antipatterns   Focus on shortening the feedback loop for everything you can, from early develop‐ ment testing to performance testing to production metrics. Faster feedback delivers greater velocity while actually increasing safety, rather than imperiling it, especially if you are doing automated canary analysis and performance load testing. Computers are better at finding patterns in large datasets than humans. Don’t rely on humans. The  ability  to  quickly  roll  forward  or  backward,  in  conjunction  with  the  ability  to slice and dice your traffic along as many meaningful distinctions as possible and con‐ trol what portions get delivered to which systems, reduces the risk of these practices because when a problem is discovered you can react to it. When coupled with the ability to automate rollback of changes that are detectably bad or not performant, the risk drops almost to nothing. Antipattern 13: Optimizing Failure Avoidance Rather Than Recovery Time  MTTF > MTTR   Failure is inevitable. Get good at handling it, rather than betting everything on avoiding it. MTTF  >  MTTR:  Inappropriately  optimizing  for  failure  avoidance   increasing  Mean Time to Failure [MTTF] , especially to the neglect of the ability to rapidly detect and recover from failure  Mean Time to Recovery [MTTR] .    Delayed production rolls are essentially a variant of a broader antipattern, which is spending disproportionate design and operational effort to keep systems from failing, rather than ensuring that they can recover from the inevitable failures quickly and with minimal user impacts. But resilience trumps robustness except where entropy prevents it. The truth is, there are applications for which it is not possible to recover from some kinds  of  failures.  To  take  a  page  from  medicine,  when  someone  experiences  brain death, we can’t bring them back. So, it makes sense to try to make the body as robust as possible through exercise and healthy diet and do everything we can to prevent an organ  failure  or  other  breakdown  that  might  cascade  into  a  “negative  patient  out‐ come.” Exercise and diet can improve resilience and work to prevent the occurrence of, say, a heart attack that will cause the heart to stop. But even a single lifetime ago, we used to be unable to bring people back from acute hypothermia or cardiac arrest. This was followed by laborious manual resuscitation in the 50s, then centralized defibrillation by doctors dispatched from hospitals in the 60s, then rapid response decentralized portable  defibrillation  by  paramedics  in  the  70s,  then  even  more  rapid  automated highly distributed defibrillation by EMTs in the 80s, and ubiquitous automated defib‐  Antipattern 13: Optimizing Failure Avoidance Rather Than Recovery Time  MTTF > MTTR       395   rillation by the general public in the 90s and the implantable wearable units that fol‐ lowed.26 Now, we have recovery mechanisms that can help restore function for people suffer‐ ing  those  traumas  even  where  prevention  failed.  We  can  even  use  the  planned inducement of one failure  hypothermia  to help increase the survivability of other traumas  drowning, heart stoppage , or even cause a controlled heart attack to help us prevent a larger, more damaging unplanned one in the future. Every year, we get better at replacing all sorts of organs we couldn’t prevent from failing. We haven’t cracked  the  problem  yet,  but  we’re  already  seeing  rapid  detection,  response,  and recoverability averting catastrophes that prevention could not. We’re  much  further  along  that  path  when  it  comes  to  computing.  The  companies whose  site  reliability  efforts  I  have  knowledge  of  experience  failure  every  day.  But they expect it, anticipate it, and design everything in the sure and certain knowledge that things will break, traffic will need to be rerouted without pause, and failed sys‐ tems will need to be brought back into play in short order, with automated redeploy‐ ments rather than extensive administrative effort. Chaos Engineering is a critical tool of modern service planning and design. One hun‐ dred percent uptime is a myth, because all change introduces risk into your system. But freezing changes removes your ability to address existing risk before it results in failure, and there is always risk. Either way, your systems will fail, so it’s important to accept that failure will occur and seek to minimize the size of your failure domains. Introduce capabilities to toler‐ ate  failure  and  provide  degraded  service  rather  than  errors  where  possible.  Suffi‐ ciently  distribute  your  service  so  that  it  can  use  that  capacity  for  toleration  to continue service from the portions of your infrastructure outside the failure domain. And minimize the time and degree of human intervention needed to recover from failure and—where appropriate—reprocess any errors. One of the best ways to ensure that these design principles are adhered to in practice is to introduce survivable failures routinely into your system. An interviewer once asked me, “What are the characteristics of a good SRE?” One answer  is  that  SREs  need  to  be  good  at  writing  software,  debugging  systems,  and imagining how things can fail. That last one is part of what defines the difference for me between a software engi‐ neer working primarily on reliability and one working primarily on product develop‐  26 Seattle was a pioneer in cardiac response as well as cloud computing, so these dates might not match up  exactly with what you know from your own history, but the gist is probably the same: https:  en.wikipedia.org  wiki History_of_cardiopulmonary_resuscitation.  396      Chapter 23: SRE Antipatterns   ment.  We  play  a  game  sometimes  in  which  we’ll  take  a  running  instance  of production, isolate it, and then try to guess what will happen if we break something in a particular way. Then we go ahead and do it and see whether we get the predicted behavior or some other failure mode that’s maybe a little more exciting and unexpected. We try to find the  thresholds  and  the  tipping  points  and  the  corner-case  interactions  and  all  the wonderful ways in which a complex system you thought you knew intimately can still surprise you after several years. It’s a lot like hacking or software testing, just with a different focus, and so a different set of attack surfaces and leverage. After a while, we become connoisseurs of failure. And then we get to try to figure out how to keep the things we discover from affect‐ ing the users of the service, and then try to break those fixes all over again. It’s a good time, if you’re into that sort of thing. When you’ve played that game, though, the best thing is to take the most impactful of the lessons you’ve learned from it and incorporate them into your automated stress  canary production testing—your Chaos Monkey or whatever tooling you’re using— so  that  these  kinds  of  tests  are  applied  to  the  system  regularly  over  time  and  help make sure that future system changes don’t result in degraded robustness or resil‐ ience.  This  is  possible  only  if  you  have  good  resiliency,  and  specifically,  detection, rollback, and recoverability, so that production traffic can be shielded from the con‐ sequences of any newly introduced weaknesses regressions. When we work to make sure that systems will face these kinds of tests regularly, it forces site reliability developers to think more about product design and what infra‐ structure help they can provide to product teams. It also forces product developers to think  more  about  designing  for  scale  and  survivability  and  making  sure  they  take advantage  of  the  reliability  features  and  services  that  SRE  helps  provide  for  them. This  keeps  the  explicit  contract  at  the  genesis  of  SRE—to  prioritize  reliability  as  a technical feature wherever it is required—front and center in everything the organi‐ zation undertakes, without extraneous effort from either group. Antipattern 14: Dependency Hell  Dependency control is failure domain control. Dependency Hell: Any environment in which it is difficult or impossible to tell what systems depend upon one another, to tell whether any of those dependencies are circu‐ larities, or to control or receive notification of the addition of new dependency rela‐ tionships, as well as any impending changes to the interoperability or availability of entities within the dependency web.    Antipattern 14: Dependency Hell      397   In any mature organization, where the software development life cycle  SDLC  has reached the point that old projects and tools are being deprecated and retired as new platforms and components and launched, unless care is taken, it will inevitably reach the point at which the interdependence of those components grow beyond the easy knowledge of any one person. The ability to predict what might be affected by your changes or what other changes might affect the systems under your control and plan accordingly becomes humanly impossible. Make sure you have a facility to detect in an automated way what dependencies are being added to your services in something akin to real time  so that you can have timely conversations ahead of launching with the new dependency, if necessary  and to make sure that your disaster plans and road maps are updated accordingly. Chaos  engineering—or  even  simply  business  as  usual—will  point  this  out  to  you eventually, of course, but it’s far better to track and plan for this before things get to that point. As an added bonus, explicit tracking of this can be used bidirectionally and can save service owners a great deal of time and energy in the process of migra‐ tion, deprecation, and turndown. Antipattern 15: Ungainly Governance  You can’t steer a mosquito fleet like a supertanker. Ungainly Governance: It is difficult, if not impossible, to run an Agile service delivery and  dev prod  infrastructure  group  within  a  larger  organization  that  is  unwilling  to adopt Lean and Agile principles in its other operations as well, or at least at the point of demarcation.    If your larger organization is locked into an antique structure of traditional IT gover‐ nance, in which approvals, budget, and deliverables are tied to specific large projects or project bundles, you’re going to have a hard time realizing the kind of continuous development, improvement, and release processes that are at the core of SRE, even if you are Lean and Agile within your own purview. Rigid money buckets, inadequate or unused features, zombie projects, and all manner of pervasive capacity misalloca‐ tion will inevitably result. Budget should be something that flows to organizations, not projects. And organiza‐ tional  leadership  should  be  held  accountable  for  outcomes—the  results  delivered across  the  company  for  the  resources  invested  in  the  organization—rather  than which hardware was purchased for what prices and how many hours were spent on what tasks. Incentivize leaders to deliver quality results efficiently, and then trust them to budget and  drive  that  process  within  their  own  organization  rather  than  pushing  down requirements and prescriptions. Prefer tracking the metrics of how you’re improving  398      Chapter 23: SRE Antipatterns   current  or  eventual  business  outcomes  continually  over  the  course  of  work  rather than judging or enforcing outcomes by adherence to the original plan or budget and use those metrics to guide you in updating your planning and execution as you go. Look  at  using  strategic  alignment  techniques  like  Hoshin  Kanri,  Catchball,  and Objectives and Key Results  OKRs  to transmit broad goals to your reliability engi‐ neering organization, rather than fully enmeshing them in any more rigid systems that might pertain elsewhere in your enterprise. Honestly, this is one of those implicit assumptions that nobody talks about because most SRE organizations exist in a culture that has already abandoned the old gover‐ nance models, and SREs don’t realize we’re swimming in the water until we wash up on  the  shores  of  some  brown-field  opportunity  and  suddenly  find  ourselves  wide- eyed and gasping.27 Getting these kinds of foundational, cultural alignments in place is critical to getting an effective reliability engineering culture and teams under way in an existing tech‐ nology  organization,  right  up  there  with  not  Passing  the  Pager   Antipattern  2   or  re- inventing Site Reliability Operations  Antipattern 1 . Antipattern 16: Ill-Considered SLOh-Ohs  SLOs are neither primarily technical nor static measures. Ill-Considered SLOh-Ohs: SLOs set or existing in a vacuum of user and business input and either not tied bidirectionally to business outcomes, priorities, and commitments, or not updated to reflect changes in the same.    SLOs  are  business-level  objectives.  They  should  not  be  set  based  on  what  you  can deliver but based on what you need to deliver in order to be successful with your cus‐ tomers, whether internal or external. Time after time, we see teams slap a monitor on their system, measure all sorts of things for a month, and then pick some Service-Level Indicators  SLIs  and set their SLOs  based  on  what  they  measured  over  that  period.  And  then  never  think  about those levels again. The SLO process begins when you’re designing a system. It should be based on the business case and deliverables for the system, as discovered through product manage‐ ment, customer support, developer relations, and any number of other channels.  27 I’ve dealt with both systems before, but it’s been so long I would never have thought to call this out to people  if Mark Schwartz hadn’t made a point about it after coming back to Amazon following his stint in govern‐ ment.  Antipattern 16: Ill-Considered SLOh-Ohs      399   SLIs  should  be  chosen  based  on  intelligent  engineering  discussion  of  what  things matter in a system and how appropriate operation of those things can be proven. SLOs should be set through reasoned analysis of what performance and availability are  needed  to  be  useful  to   and  preferred  by!   customers  over  the  other  options already or soon to be available to them. If you’re not doing this, you’ll end up optimizing for the wrong things, and find that adoption is stunted in sectors that you were counting on, even though your service has good adoption in others. You’ll also find that you’ve made the wrong choices in instrumentation, so that when you conduct some kinds of maintenance, or experi‐ ence certain transmission problems, service errors, or capacity degradation, your sys‐ tem  becomes  useless  to  customers  without  you  even  being  aware  that  anything  is wrong until it shows up on Twitter or Reddit. SLOs should be a living document. If you don’t have a mechanism for revisiting them periodically or as needed, they are going to become irrelevant. If your targets don’t keep up with users’ changing needs, they will abandon your service. If you’re exceed‐ ing the promised level of service significantly and consistently, you’re going to sur‐ prise users in unpleasant ways when you make decisions about what strategies are acceptable for migrations, maintenance, production testing, or new rollouts. Most important of all, your entire business needs to be aligned behind your SLOs. Everyone needs to know explicitly how they tie to revenue or other business goals. The SLO is not simply a stick with which to beat SREs when goals are being missed. It is a lever that should be able to elicit support and resources from the larger organiza‐ tion  as  necessary  as  well  as  driving  design  decisions,  launch  schedules,  and  opera‐ tional work. These are business commitments, not just SRE commitments. Capital and operational budgets need to reflect the priorities expressed in these met‐ rics. Staffing, design decisions, and work prioritization will, at the root, flow from this data every bit as much as market research or product brainstorming sessions. Decisions need data, and if everything is working properly, the data about how cur‐ rent resource allocation is affecting service and how service is affecting revenue—or work outputs, or patient outcomes, or whatever top-level organizational goal it ulti‐ mately ties to—is one of the most fundamental decision inputs that will drive your organization. So make sure these key metrics and targets are well chosen, appropri‐ ately scoped, and broadly understood and accepted. At their heart, SLIs and SLOs are a tool for reasoning and communicating about the success of an organization. That organizations inevitably produce systems that reflect their organizational communication structures is received truth at this point.28  28 See Conway’s Law.  400      Chapter 23: SRE Antipatterns   We don’t have experimental evidence for causality in the other direction, but in the case of SLOs, it seems likely that organizations inevitably evolve to reflect their estab‐ lished communication structures every bit as much as do the systems they produce. Antipattern 17: Tossing Your API Over the Firewall  Server-side SLOs guarantee customer outages. Tossing  Your  API  Over  the  Firewall:  Failing  to  collaborate  and  integrate  with  key external  parties  using  the  well-established  methods  by  which  SREs  collaborate  and integrate  with  their  internal  customers  and  partners,  and  not  measuring,  sharing responsibility for, and attempting to remediate risks from outside your own systems to successful customer outcomes.    At the core of what the DevOps philosophy teaches us is the realization that opera‐ tional silos result in missed SLOs. The effects of laggy communication boundaries and “Somebody Else’s Problem” between distinct organizations are not as different as we would like to believe from the effects they create between the distinct teams within them. Tossing  your  API  over  the  internet  to  your  customers—that  is,  handing  them  a Service-Level Agreement  SLA  for response times at the edge of your network, cash‐ ing their check, and then waiting for their inevitable support tickets—is as much an antipattern as tossing your code or binaries over the wall to the ops team was a dec‐ ade ago, and for the same reasons. Before you become too indignant about this statement, remember: tossing require‐ ments and code over the wall was a conscientious, disciplined, and accepted business practice  a  decade  ago,  just  like  defining  SLIs  and  SLOs  based  only  on  the  systems controlled by your own company is today. I’m going to steal some math here for a moment,29 but over a 30-day window, hitting a 99.99% reliability target means you can miss your SLO for only 4.32 minutes. If you want to hit 99.999%, that drops to 26 seconds. What this math means is that without shared metrics and alerting, in which you are paying attention to the performance of the clients that are ultimately consuming your service, and any intermediate partners that own those clients and depend on you in order to service them, there is no way for your customers to consistently meet even a four 9s SLO for their own users, even if your system achieves that threshold itself.  29 Dave Rensin did a great presentation laying out all the technical principles behind this calculation at SRECon  Americas in 2017. If you haven’t seen it, go check out the recording and slides.  Antipattern 17: Tossing Your API Over the Firewall      401   Just the time needed for them to get paged, investigate, file a ticket that pages you, and have you investigate—even without repair time—will blow their error budget for the month. Likely for the entire quarter. After your own house is in order and you can deliver four 9s or higher, you need to get out ahead of this. Look at your traffic. Talk to your PMs. Figure out who your critical customers are. Integrate and establish cooperation in advance. Decide on shared metrics. You can’t have a conversation if you aren’t talking about the same things. Make sure everyone understands what the SLOs really mean, and that the on-call response guarantees make sense with the SLOs. Put shared communi‐ cation and incident response procedures in place before you have a problem, so you can have the same kinds of response patterns to your critical customers’ issues as you have to the issues of other teams within your organization. When I onboard significant external customers, we sit down and look at what they are measuring versus what we are measuring, and at what the system as a whole, both our parts and theirs, needs to deliver to their customers rather than just to them. We determine any changes needed to grow gracefully together, and we roll them out in conjunction. To do that, we need to commit to immediate alerting without first establishing that the problem is on one end or the other, and we have to provide remote pathways to shared  integration  testbeds  and  production  canaries.  Always,  of  course,  keeping  in mind  that  SRE’s  customers  must  be  good  partners  regardless  of  whether  they  are internal or external, and unresolved tech debt and unsustainable operational burdens will result in devolving this kind of unmediated engagement. It is not just the Googles and the Amazons that need to think about this sort of thing. As demand for ubiquitous services grows, and more companies build interdependent offerings, we’re going to have to collaborate more with our most valuable customers and  partners.  The  level  at  which  Google  CRE30  is  doing  this  might  be  beyond  the scope  needed  today  by  most  organizations,  but  the  ideas  are  readily  applicable  to many services. Antipattern 18: Fixing the Ops Team  Organizations produce the results that they value, not the results their components strive for. Fixing  the  Ops  Team:  The  mistaken  belief  that  service  delivery  can  be  improved  by bringing SREs to do SRE “to” a company, rather than for a company to start doing SRE.  30 Dave Rensin, “Introducing Google Customer Reliability Engineering”.  402      Chapter 23: SRE Antipatterns      This is another of those broad fallacies that people embracing the SRE brand for the first time don’t always initially understand. It’s basically the opposite end of the spec‐ trum  from  Site  Reliability  Operations   Antipattern  1   in  which  the  organization thinks that SRE is just a buzzword they can rebrand with and use to try and keep up in the recruiting competition with other companies. At this end, people think they have an ops problem that is centered in their ops team, and  if  they  just  replace  those  ops  engineers  with  SREs  who  have  the  secret  sauce, they’ll get the kind of results that Google and Facebook and the like get from their production engineering. But the problem isn’t the ops team, it’s the systemic struc‐ ture that makes their operations mission impossible to achieve at scale. SRE is not about fixing ops. It isn’t just an ops methodology. Successful SRE requires fundamentally reordering how the entire company or institution conducts business: how priorities are set; how planning is conducted; how decisions are made; how sys‐ tems are designed and built; and how teams interact with one another. SRE  is  a  particular  formula  for  doing  “DevOps”  in  a  way  that  is  organizationally more efficient and sustainable, and more focused on production service quality. It’s not just a job or a team. It’s a company-wide cultural shift to get away from gatekeep‐ ers, shadow operational costs, and institutionalized toil, and to create a healthy feed‐ back  system  for  balancing  the  allocation  of  engineering  resources  more  effectively and with reduced conflict. At the core of it is the commitment to appropriately scoped reliability as a core fea‐ ture of any systems the organization creates or relies upon; to relentlessly winnowing out operational toil, delay, and human error through process and software engineer‐ ing; and to shared responsibility for outcomes. As  with  Lean  and  DevOps,  both  of  which  SRE  ideally  has  many  characteristics  in common, it is an ongoing process that requires dedicated attention not just from the expert team responsible for coordinating the efforts, but also from all the other busi‐ ness units upon which the value stream depends. Thus,  absent  commitment  from  the  very  top  of  the  organization31  to  driving  this model, your ability to realize the benefits will be limited. You can still make substan‐ tial improvements in your team processes and systems. With enough grassroots sup‐ port  from  other  teams,  you  might  even  end  up  with  a  fairly  functional  DevOps model.  31 CEO is best, C-level is decent. At the least, the heads of your product development and production engineer‐  ing teams should agree on budget, staffing, on-call, toil, change control, and epic schwag.  Antipattern 18: Fixing the Ops Team      403   But if you want to reliably create virtuous cycles of productivity through prioritizing engineering over toil, even at the potential cost of an SLO shortfall now and then; to establish  well-aligned  priorities  across  the  organization  that  won’t  be  repeatedly abandoned or regularly mired in the conflict du jour; to create and safeguard the sus‐ tainable work balance and systems needed to attract and retain top talent then; the further up the organization that understanding and buy-in to SRE principles goes, the closer you will come to realizing your goals. Do whatever work you must in order to make certain the entire organization subscribes to the same strategy. So, That’s It, Then?  That’s all of them? We’ve got it all figured out? So, what does this all mean? What do we do with this information, with these hard- won lessons in unintended consequences? Just a handful of little dead ends to watch for, and as long as we avoid these we’ll never have a problem, right?    Sadly, not. Because each unhappy technology or business is unhappy in its own way, and because I haven’t been around to see even more than a fraction of them, we have to expect this list to keep growing and changing as the industry does. The important thing here is the process—the most fundamentally SRE process—of continuing to look at the places we and our peers have run into trouble, and then not only  creating  learning  opportunities  for  our  own  organizations,  but  turning  those catalogs of failure into stories that we can share across the industry as well. In fact, I think the time might even be ripe for an antipattern repository where peers can share, discuss, and categorize potential new patterns or variations. If you have one that we’ve left out, we definitely want to hear from you before we stub our own toes on it. So @BlakeBisset me on Twitter, SREantipatterns. Even if we do not find anything new, at least we might perhaps find the conversation very pleasant? Spes non consilium est!32  Blake Bisset got his first legal tech job at 16. He did three startups  one biochem, one pharma, and this one time a bunch of kids were sitting around wondering why they couldn’t  watch  movies  on  the  internet   before  becoming  an  SRM  at  YouTube  and Chrome, where his happiest accomplishment was holding the go bestpostmortem link at  Google  for  multiple  years.  He  currently  serves  on  the  USENIX  SRECon  program committee and as Head of Reliability Engineering at Dropbox.  32 Hope is not a strategy!  404      Chapter 23: SRE Antipatterns   CHAPTER 24 Immutable Infrastructure and SRE  Jonah Horowitz, formerly Netflix and Stripe  Immutable  infrastructure  can  significantly  reduce  the  amount  of  toil  required  to maintain a large fleet of production servers. It does this by reducing the number of variables in the system and making it easier to swap pieces out because all instances of a service are identical. Scalability, Reliability, and Performance SRE has perhaps become overloaded, but at its core, the definition boils down to a few  things:  driving  scalability,  reliability,  and  performance  for  web  operations  at scale.  A  challenging,  but  incredibly  powerful,  piece  of  that  is  through  immutable infrastructure. Immutable infrastructure is the practice of starting each build of a software compo‐ nent with a small base image and then installing your software onto that image. For every  release,  this  image  is  rebuilt  before  being  released  to  production.  After  it  is released, the image is never changed or updated. It is only replaced with a new image. The process is called “immutable” because the released software is frozen and never changed, only replaced. The running instances are never updated via a tool like Pup‐ pet, Chef, or even Secure Shell  SSH  after they are launched  they might still have SSH running for debugging purposes . Immutable infrastructure delivers scalability and performance by allowing horizontal scaling of your infrastructure. One of the challenges of adding and removing nodes from a running cluster is making sure that every node has the correct and identical configuration. It is possible to manage a 10-node cluster when you log in and hand- configure  each  node,  but  it  is  toilsome  and  prone  to  error.  It  is  silly  to  hand- administer 100 nodes, and with 1,000 nodes or more, it becomes literally impossible for one person if they have to spend more than 10 minutes per machine per week. In  405   the face of such toil, organizations turn to a configuration management tool to han‐ dle large clusters of servers. Chef and Puppet are two of the more popular choices, but there are many more—including writing one from scratch  which I do not rec‐ ommend . Configuration  management  systems  like  Chef  and  Puppet  work  well  initially,  but they  ultimately  fall  short  when  the  cluster  size  grows,  particularly  when  you  start needing to scale it dynamically. When each node must boot and run another tool— first, to install and afterward to configure your software—you end up wasting consid‐ erable time. If the nodes already have the software ready to go, they are constrained only by their boot time. With the movement to containers, the boot times become even faster. If you can rapidly add nodes when you need capacity, this allows you to save resour‐ ces, because you need only enough capacity online at any given time to support your current traffic load. Most websites serve significantly less traffic at 3 AM than at 7 PM. If you can expand capacity within minutes of seeing increased load, you can run with a smaller buffer, and you can be more confident in scaling down when your load is lower. Failure Recovery Another thing enabled by immutable infrastructure is the ability to recover quickly from  failure.  If  the  hardware  underlying  an  individual  node  fails,  you  can  rapidly start another node with the immutable image to replace it. This new node does not need additional configuration or software installation because it boots from the same image  that  every  node  in  your  cluster  already  uses.  Also,  this  allows  you  to  take advantage of tools like Netflix’s Chaos Monkey to induce failure into your system, ensuring your ability to handle that failure. Simpler Operations Simplicity is a fundamental component of velocity. An advantage of an immutable system is that it makes changes simpler. Components are replaced instead of modi‐ fied, so you no longer need to manage careful state transitions between an old state and a new state. That is regardless of whether you are rolling out a new release of the application that runs on your servers, a configuration change to your operating sys‐ tem, or an update to an underlying library. In any case, you run through the same process to release the change into production. This simplicity is an important point to stress. Not needing to worry about modifying state in place increases the velocity and reliability of your deployments. When your infrastructure  is  managed  traditionally,  so  much  code  is  dedicated  to  checking  the  406      Chapter 24: Immutable Infrastructure and SRE   current  state  before  a  change  and  then  making  sure  that  a  change  influenced  that state in the way you expected it would. Consider  a  simple  update  of  a  DNS  server  in   etc resolv.conf.  Without  immutable infrastructure, you must make sure that the line for the old DNS server is removed, and then make sure that the line for the new DNS server is inserted in the correct place. With immutable infrastructure, you just release a new build that points to the updated DNS server—a significantly simpler operation. Faster Startup Times When I started working in tech, it took us about a day to get a new server into the rotation in the load balancer. With configuration management, I was able to get that down  to  less  than  an  hour,  but  with  immutable  infrastructure,  it  is  down  to  5 minutes.  There’s  a  trade-off.  It  takes  longer  to  build  a  new  image,  but  each  image launches significantly faster. It mirrors a lot of other trade-offs we make; for example, we exchange compilation time with runtime using gcc -O2.1 Faster  startup  times  allow  us  to  scale  our  systems  more  efficiently.  If  your  startup time is an hour, you need to anticipate the load that your system will be under at least an hour in advance and start scaling to anticipate that. If your startup time is only 5 minutes, you need to predict only 5 minutes into the future. As with all predictions, the shorter your time horizon, the more accurate you are likely to be. This gives you the ability to scale down confidently, too. If your traffic is trending downward and you expect it has peaked for the evening, you can confidently scale down your cluster, knowing that you can get capacity back online quickly if your predictions were incor‐ rect. Faster  startup  time  allows  more  real-time  or  immediate  capacity  management,  but what  it  really  gives  us  is  added  resilience  to  failure.  The  ability  to  replace  nodes quickly means that when they die, either due to external events like a node going off‐ line,  or  internal  events  like  a  Chaos  exercise,  we  can  promptly  and  automatically replace  the  node  without  manual  intervention.  Without  immutable  infrastructure, adding deliberate failure to your infrastructure can end in true chaos. Known State One of the biggest concerns of both SRE teams and security teams is unknown dark corners  in  their  infrastructure.  If  a  machine  has  been  up  long  enough,  you  never really know its true state. Without immutable infrastructure, a machine can stay in  1 The -O2 flag in gcc increases both compilation time and the performance of the generated code  https:    gcc.gnu.org onlinedocs gcc Optimize-Options.html .  Faster Startup Times      407   production  for  years,  receiving  upgrades  and  patches  and  getting  new  software installed, and old software can be removed without a full reimage. You cannot know the full state of an instance like this. This  is  scary  from  a  security  perspective  because  you  do  not  know  whether  the machine was compromised long ago and has a persistent piece of malware on it. It is also scary from an SRE perspective because you do not know whether there’s a latent bug lurking on the system. Maybe one of your libraries was not upgraded, or one of your patches was not installed. Maybe another engineer at your company manually installed a package or an individual binary. With immutable infrastructure, you never have an instance running for more than a few weeks, optimally less than a few days. Like everything in complex systems, configuration management itself is not perfect. It sometimes fails, so even with the best-maintained infrastructure, you end up with a few systems that didn’t receive the latest updates. This could be because the agent on the box crashed in a bad state or because there was a network problem, but regardless of the reason, it does not deliver on the promise that all systems are configured in the way you expect. With  immutable  infrastructure,  you  can  be  sure  that  machines  booted  from  the image, and that the configuration is correct and up to date, because there’s only one configuration for the entire life cycle of the instance. If an instance is misbehaving for any reason, you can just shut it down and replace it with a new instance. Continuous Integration Continuous Deployment with Confidence In  a  traditional  infrastructure,  your  software  is  installed  on  Dev,  then  Test,  then maybe Staging, and then Prod. In each case, the software can be deployed slightly dif‐ ferently, particularly if the servers are reused between installations. The cruft and dif‐ ferences between servers compound over time. You cannot trust that a change that works in Test or Staging will perform the same way when it gets into Production. When you move to immutable infrastructure, this risk is significantly reduced. The image used is exactly the same for each environment, and you never hear a developer saying, “It worked fine in Test!” The best way of deploying immutable infrastructure is to use a blue green deploy‐ ment. The way this works is that if you have a cluster of 200 instances delivering a service, which we’ll label blue, you spin up 200 instances of the new release image, which we’ll label green. When the 200 new instances are up and serving traffic, you stop the traffic to the old instances. After a waiting period to determine that they’re healthy,  perhaps  an  hour,  you  terminate  the  old  instances.  This  process  allows  for very  smooth  rollouts  and  rollbacks.  If  there’s  an  increase  in  errors  with  the  new release, you just move the traffic back to the old release.  408      Chapter 24: Immutable Infrastructure and SRE   Security Immutable infrastructure has advantages when it comes to security as well. Because each instance is built from scratch every time, you have a far lower risk of cruft accu‐ mulating  on  a  node.  This  means  that  programs  installed  for  temporarily  trouble‐ shooting  an  issue  or  old  libraries  that  aren’t  in  use  anymore  won’t  increase  your attack surface. If an attacker gains control of a node, they have a limited window where they can uti‐ lize the node before it is replaced, so they cannot install long-running exploits. When rolling out a security patch, you can use the same release process as for appli‐ cation code changes. This minimizes risk because a well-exercised and understood process is less likely to fail. One of the more painful types of security updates is kernel patches. In a traditional infrastructure, they are risky because you must reboot each server and hope that the server comes back online. This can fail for any number of reasons. You can have a configuration problem that went unnoticed until you attempted the upgrade, you can also find latent hardware problems that you did not know were there. With immuta‐ ble infrastructure, you build new server images with the kernel patch already applied and then push that through your normal release pipeline. It is safer, it is faster, and it does  not  require  the  manual  tending   or  painful  scripting   of  your  servers  as  they reboot. Multiregion Operations Immutable  infrastructure  simplifies  multiregion  operations.  Instead  of  having  to propagate a different configuration management stack to all of your regions and keep them synchronized, you can deploy a single image to all of your regions. Deploying configuration management to multiple regions can be challenging to manage on its own. Each configuration management master node must maintain synchronization with the master nodes in other regions, which is an additional level of complexity. This is still true if you deploy a masterless configuration management tool because you still need to copy and keep some management artifact like a .tar file synchron‐ ized. When using immutable infrastructure, you need to add only a step to your deploy‐ ment pipeline that copies your new image out to each region before launching servers based on that image. With blue green deployment, you can have confidence that your deployment succeeded in one region before you roll it to additional regions. If your architecture requires that you run the same exact version in all regions at all times, you can accomplish that, too, by synchronizing the blue green flips.  Security      409   Release Engineering Configuration management tools are scary to developers and organizations because of  how  powerful  they  are.  One  misconfiguration  and  you  can  disable  your  entire infrastructure. Because of this, many companies do not trust their developers to roll out  changes  to  their  configuration  management  without  review  by  an  operations team.  Every  configuration  management  software  also  uses  its  own  domain-specific language, which developers are not as familiar with as their primary language. It is an additional burden on these developers to ask them to learn and use this language, and they’ll never be fully fluent in it because they don’t use it often. With immutable infrastructure, you remove these concerns. You never alter the state of any running instance. This significantly reduces the risk to your infrastructure of a bad change, even more so because all changes will affect only one type of instance, and with blue green deployments, changes are trivial to roll back in the event of a problem. With  these  changes,  you  can  enable  developers  to  deploy  code  and  configuration changes  to  production  with  confidence  that  you  can  trivially  repair  any  damage caused by an unforeseen bug. Building the Base Image The  first  step  to  implementing  immutable  infrastructure  is  to  create  a  good  base image. This image is then used for every running instance in your infrastructure. If you run a small shop, you might do this by using a default release image from an upstream  Linux  distribution  and  then  installing  the  latest  security  updates.  That would be the easiest option. If you have a team with a little more bandwidth, you can optimize the base image by doing some of the following:    Removing packages that you know you do not need in your production environ‐  ment  infrastructure    Hardening the image by changing the security settings   Tuning the network stack   Installing common libraries and internal packages that are used throughout your  Taken to the extreme, the base image can be entirely built from signed source code using a dedicated build and release pipeline that is immutable as well. Building an image in this way produces a bit-for-bit reproducible image that you can trust and verify. This exceeds the requirements of most organizations, though.  410      Chapter 24: Immutable Infrastructure and SRE   After you have an image you are happy with, you use that image to build all other images that you release to production. The cadence for rebuilding this image is up to you, but I recommend doing it weekly, and having an exception process so that you can build and deploy this image immediately after a security vulnerability affecting your systems is discovered. For releasing your new image, you should find a few applications in your organiza‐ tion  that  are  less  critical  and  have  more  capable  operations  teams  to  canary  each release before you roll it to your mission-critical applications. A canary is a small sub‐ set of your systems that you can deploy new code on to see whether there are issues that were not caught by your automated testing. In this way, you have both a canary and a release version available at any given time. You can configure the base image by using a configuration management tool or even just a shell script. This is completely different than running a configuration manage‐ ment tool in production because you are using it only as part of the deployment pipe‐ line, and not on your running infrastructure. Optimally, you should need to run the tool only once, bringing the upstream image into compliance with your desired con‐ figuration before the image is frozen. The team that is responsible for building and maintaining  the  base  image  should  take  input  and  collaborate  with  the  developers who will build applications that run on the base image. The developers in your com‐ pany will never need to care or even know about the configuration management tools you use to build your base image. Deploying Applications When each developer is ready to deploy, each application should go through an auto‐ mated multistep deployment process. First, you should compile and package the code using  either  the  packaging  system  for  your  operating  system   OS   or  a  language- specific packaging system. After that is complete, the package, along with its depen‐ dencies, can be installed on the base image. HashiCorp has an open source tool for this called Packer. Netflix also released an open source tool called Aminator. If you are running a container infrastructure, there are many tools to build Docker images. Docker images still need to run on a base OS. You should launch this base OS from an immutable system image. The Docker images should be pulled at runtime with the version managed by your container orchestration tools. When you deploy this way, you’ll have immutable Docker images running on immutable system images. When the image or Docker container is ready, you need to test it before deployment. Initially,  you  will  want  to  launch  it  in  a  development  or  integration  environment. After it passes your tests, run it in your Test or Staging environments before launch‐ ing a canary instance taking production traffic. Finally, you can either launch a new autoscaling group or Docker cluster with the new image or grow the canary cluster into the target of your next blue green deployment.  Deploying Applications      411   The idea is for the SRE team to enable any developer to deploy a new version of a service safely and quickly. Disadvantages So far, this chapter has covered the advantages and techniques for deploying immuta‐ ble infrastructure, but there are cases when it is not a good fit. It can be hard, if not impossible, to use immutable infrastructure on your persistent data layer. Some data‐ bases like Cassandra are designed to work in an immutable environment. It requires that you use a rolling release, in which you replace one node at a time instead of a blue green release. Other databases are hard to use in an immutable way without sig‐ nificant  modification.  Depending  on  the  application,  it  might  be  easier  to  run immutable  on  nodes  outside  the  data  layer,  and  a  more  traditional  configuration- managed infrastructure for the data layer. Immutable  infrastructure  can  also  increase  the  iteration  latency.  Building  a  new image, even if it’s just a Docker image, takes much longer than just copying a few files of code to a development instance. This iteration latency can decrease developer pro‐ ductivity, chiefly when your code base can be run on only a server and can’t be devel‐ oped locally on the developer’s workstation. One way to mitigate this is to facilitate synchronizing code from the developer workstation to a temporary dev server before the code is packaged and shipped to a Test or Staging environment. This approach balances the need for developer productivity with the advantages of immutable infra‐ structure. Conclusion When  adopted,  immutable  infrastructure  reduces  toil,  allowing  your  developers  to focus on product features; your SRE team can increase developer productivity and reduce  time  to  deploy  new  code,  leading  to  happier,  more  productive  developers while simultaneously leading to a safer and more reliable infrastructure.  Jonah Horowitz is a senior site reliability engineer with 18 years’ experience building and scaling production applications. He’s worked at several startups and large compa‐ nies including Quantcast, Netflix, and Stripe.  412      Chapter 24: Immutable Infrastructure and SRE   CHAPTER 25 Scriptable Load Balancers  Emil Stolarsky, DigitalOcean  formerly Shopify   When scalability problems occur, there’s no time to go back and rearchitect or refac‐ tor an entire web service. Inevitably the service you’re responsible for will go down as a result of being overloaded. Sometimes, you’re hoping for a miracle in the form of an  easily  fixable,  bad  database  query.  Or  maybe  you’re  able  to  simply  scale  up  the number of workers. But what about when you don’t have the time, money, or people needed for solving the problem at hand? Luckily, there’s a new suite of tools that are making  a  name  for  themselves  in  small  corners  of  our  industry  that  can  forever change the way we approach scalability and our roles as SREs: scriptable load balanc‐ ers. Scriptable load balancers are proxies that can have their request response processing flow  modified  through  a  scripting  language  such  as  Lua.  This  opens  new  ways  for infrastructure  teams  to  shard  applications,  mitigate  Distributed  Denial  of  Service  DDoS  attacks, and handle high load. Small teams can now solve difficult or seem‐ ingly  impossible  problems  in  novel  and  elegant  ways.  The  ability  to  add  high- performance, custom logic to a load balancing tier isn’t new; however, the ability to do  that  at  an  organization  of  any  size  is.  And  that’s  why  scriptable  load  balancers could be game changing. Scriptable Load Balancers: The New Kid on the Block Look at almost any web service, and often you’ll find a load balancer being the first thing a request hits, as Figure 25-1 depicts.1 These servers act as proxies, receiving  1 Limoncelli, Thomas A., Strata R. Chalup, and Christina J. Hogan.  2014 . “Application Architectures.” In  Practice of Cloud System Administration: DevOps and SRE Practices for Web Services. Boston: Addison- Wesley Professional.  413   requests  HTTP or otherwise  and forwarding them to a specified pool of upstream servers.  They  have  become  ubiquitous  for  their  value  in  improving  resiliency  and performance. Today, they’re often used for not much more than Secure Sockets Layer  SSL  offloading, simple caching, and distributing load across multiple upstreams.  Figure 25-1. Architecture of a typical web application with a user request hitting a load balancer before being routed to an application server.  Load balancers are highly capable, specialized components in our architecture. Their ability to process requests quickly and serve orders of magnitude more traffic than their  upstreams  is  a  powerful  trait.  Unfortunately,  any  attempt  at  adding  complex application  logic  is  quickly  shut  down  by  the  restrictive  configuration  languages shipped with load balancers. There are application-aware load balancers  e.g., Face‐ book’s Proxygen2  that are able to peer into the content of a request  e.g., read head‐ ers  and act on it, but you need to custom-build these. The goliaths of the industry  Facebook,  Google,  etc.   recognize  the  value  that  application-aware  load  balancers can bring and have taken to writing their own. For the rest of the industry that can’t spare a team of people, we’re left without them entirely. A few companies in the industry have begun to realize that there doesn’t need to be this dichotomy between traditional load balancers or application-aware, custom-built load balancers that take a team to support. This middle ground can be filled by script‐ able  load  balancers.  With  scriptable  load  balancers  you  can  modify  the  request  response processing flow  e.g., by modifying outgoing headers  through a scripting language  such  as  Lua.  You  can  write  extensions  in  a  high-level  language  to  add application-aware functionality on top of the load balancer’s existing features. The market for scriptable load balancers is still young. There are currently two main scriptable load balancer projects: OpenResty and nginScript. OpenResty is an Nginx C module with an embedded LuaJIT, created by Yichun “agentzh” Zhang. A steady community3  has  grown  around  it,  producing  multiple  libraries modules;  it’s  been  2 Shuff, Patrick.  2015 . “Building a Billion User Load Balancer.” Talk at SREcon15 EU. 3 Netcraft.  2016 . “September 2016 Web Server Survey.”  414      Chapter 25: Scriptable Load Balancers   adopted by companies such as Cloudflare, Tumblr, and Shopify. Of the two, OpenR‐ esty has the largest community and powers upward of 10% of internet traffic.4 Closely following is nginScript, Nginx Inc.’s implementation of a JavaScript scripting engine in Nginx. Why Scriptable Load Balancers? Scriptable load balancers have numerous advantages over custom-built and conven‐ tional load balancers. For starters, only a select number of organizations can spare the time and people nec‐ essary to build their own production-ready load balancer. More practically, few engi‐ neers have experience building a load balancer from scratch. Rather than spending time improving resiliency or performance, engineers will spend their time reinvent‐ ing the wheel, building routing and request processing engines. At the end of the day, the custom logic you add to the edge only makes up a fraction of what load balancers can do. Rebuilding that functionality is in no one’s best inter‐ est. Especially, when current capabilities are a moving target with constant improve‐ ments from the open source community. Traditional load balancers don’t offer the same capabilities as scriptable load balanc‐ ers. One drawback is that traditional load balancers ship with declarative configura‐ tion languages that make it difficult to express application logic. Sometimes, it’s even impossible to specify the desired behavior. Additionally, logic expressed through con‐ figuration is difficult to fully validate for correctness with tests. To compensate, many load balancers ship with a C-based plug-in system that allows developers to augment existing  functionality  through  modules.  These  plug-in  systems  address  many  con‐ cerns,  but  they  come  with  substantial,  unnecessary  downsides  when  compared  to using an embedded scripting language. Without built-in memory safety, C is prone to memory faults  e.g., buffer overflow and segmentation faults , making it inherently unsafe. In contrast, a scripting language, such as Lua, can be sandboxed with strict runtime and memory guarantees. When writing a C module, developers can’t ignore lower-level details that aren’t immediately relevant to the logic at hand, a problem scripting languages don’t have. Making the Difficult Easy Load  balancers  act  as  gatekeepers  to  services,  with  every  request  passing  through them.  Their  position  in  our  web  architecture  enables  powerful  abstractions.  This  4 Roberts, John.  2016 . “Control Your Traffic at the Edge with Cloudflare.” https:  blog.cloudflare.com   cloudflare-traffic .  Making the Difficult Easy      415   allows for solutions that aren’t typically available to SREs or infrastructure develop‐ ers.  For  example,  the  ability  to  pause  requests  during  deploys  to  avoid  returning errors to customers or proxying requests to the correct data center is powerful for an SRE team, but difficult to achieve with existing frameworks and applications. Typical web frameworks such as Rails or Django are designed for serving traditional CRUD  REST requests. Although they can perform nontraditional operations, it comes at the cost of resiliency. The rest of this section will show how you can use scriptable load balancers to solve common infrastructure problems to great effect. Shard-Aware Routing When applications grow sufficiently large, their data becomes too big to be stored on a single node. The solution is to break the data into manageable chunks, referred to as shards, and spread it out among multiple nodes and or databases. Sometimes, this is  done  internally  by  the  database,  and  all  that’s  required  to  scale  is  adding  more nodes to the database cluster. For the majority of applications, the data has to be logi‐ cally partitioned depending on data models and access patterns.5 After an application is sharded, there’s no guarantee that processes can access every shard. To render a response, therefore, you need to ensure that a request is routed to a  process  that  can  access  the  required  data.  How  a  particular  request  is  routed becomes an important problem to solve. Let’s now look at some common ways that you can route requests to the correct shard and how you can use scriptable load bal‐ ancers to solve this tricky problem.  Routing requests with DNS For data models that can be sharded by domain name  e.g., multitenant applications , you can use DNS to route to the correct shard. Every shard is given a unique domain that  resolves  to  a  process  which  can  serve  requests  for  the  shard,  as  shown  in Figure 25-2.  5 Matsudaira, Kate.  2012 . “Scalable Web Architecture and Distributed Systems.” In The Architecture of Open  Source Applications Volume II: Structure, Scale, and a Few More Fearless Hacks. Ed. Amy Brown and Greg Wilson.  CC BY 3.0.   416      Chapter 25: Scriptable Load Balancers   Figure 25-2. 1  Client requests DNS address of domain on a shard. 2  Client directs request to the IP returned by the DNS server to the location of the first shard. 3  Client requests DNS address of domain on a separate shard. 4  Client sends request to the cor‐ responding DC.  The  clear  benefit  of  this  method  is  its  simplicity.  Any  application  on  the  web  is already using DNS. But, as we’re all too familiar, DNS comes with a whole host of problems.  Convergence  in  changes  to  routing  information  can  take  unpredictably long times. Using DNS also limits what can be used to distinguish between shards  i.e., only a domain name .  Routing queries in the application You can avoid the problem of ensuring that a request is routed to the correct process if every process can access all shards. As Figure 25-3 shows us, the logic of connecting and communicating with the correct database then becomes a responsibility of the application.  Figure 25-3. For every request, the app server connects to the necessary shard on the fly.  Making the Difficult Easy      417   This adds nontrivial amounts of complexity and compromises the scalability of an application. When an application outgrows a single data center, the latency entailed in connecting across data centers makes connecting to every shard an expensive deci‐ sion in terms of performance. Even when the added latency isn’t a problem, the num‐ ber of connections each database and application process must manage leads to its own sets of problems  e.g., max connection limits .  Routing requests in the application At the cost of added complexity, you can place shard routing logic in the application, as illustrated in Figure 25-4. When a request cannot be handled, the application prox‐ ies it to the correct shard. This is known as backhauling a request. Applications are now required to be able to backhaul traffic—potentially high volumes of it—to the correct process. Traditional web frameworks aren’t designed for proxying requests. To top it off, this sort of functionality isn’t easy to add.  Figure 25-4. 1a  Client sends a request for shard 1 to the closest data center. 1b  The app server connects to the local shard. 2a  Client sends a request for shard 2 to the clos‐ est data center. 2b  The app server in DC1 proxies the request to DC2, where step 1b is repeated locally.  418      Chapter 25: Scriptable Load Balancers   Routing requests with a scriptable load balancer Scriptable load balancers allow you to move all shard routing logic into the load bal‐ ancing  tier,  enabling  new  routing  possibilities  and  giving  you  the  ability  to  com‐ pletely  abstract  away  the  idea  of  sharding  from  applications.  Processes  are  never made aware of requests they can’t serve, as depicted in Figure 25-5. With full control of the request routing logic, you’re able to extract the desired shard from any combination of attributes or properties  e.g., Host, URI, and client IP . It’s even possible to query a database that gives insight about the shard to which a request belongs. With the other approaches to routing, the process directly affects how data models can be partitioned. For example, if you use DNS routing, data models without their own domain can’t be partitioned. Infrastructure concerns shouldn’t dictate the design  of  an  application.  Embedding  routing  logic  in  load  balancers  removes  the leakiness of that abstraction.  Figure 25-5. 1a  Client sends a request for shard 1 to the closest data center. 1b  The load balancer in DC1 proxies the request to the local shard. 2a  Client sends a request for shard 2 to the closest data center. 2b  The load balancer in DC1 proxies the request to DC2 where step 2 is repeated locally.  Ideally, applications are minimally aware of their own sharding. With routing in the load balancer, an application is able to hold little to no awareness of its own sharding. By separating these concerns, you can reuse sharding logic among multiple applica‐  Making the Difficult Easy      419   tions. Google’s Slicer6 is a prime example of this approach. Slicer is an autosharding service for applications. A core component is the transparency to upstream applica‐ tions in how requests are routed to a particular shard through Google’s frontend load balancers  and  RPC  proxy.  The  service  is  in  production  today,  routing  7  million requests per second. Harnessing Potential The power in scriptable load balancers comes from the reuse of existing functionality. You  don’t  need  to  rewrite  Transport  Layer  Security   TLS   negotiation  or  health checking when your load balancers already excel at these tasks. Oftentimes, internet specifications will even make room for modifications. HTTP caching is a good exam‐ ple of this. The RFC allows for custom Cache-Control header extensions that modify how and when a request is cached.7 Instead of writing a new caching proxy for fine- grained cache key control  e.g., including specific cookies in the cache key , a simpler script running in the load balancer can reproduce the exact same functionality. Furthermore, load balancers specialize at routing and serving requests. An enormous amount of effort is invested in ensuring that they’re able to do this quickly, at high volume.  With  little  effort,  these  same  properties  can  be  inherited  by  the  modules added to scriptable load balancers. You can see this in Cloudflare’s Web Application Firewall  WAF 8 and Shopify’s Sorting Hat  L7 routing layer ,9 which both measure performance in microseconds. Case Study: Intermission For  most  services,  the  ability  to  ship  code  into  production  quickly  and  easily  is important.  One  component  of  being  able  to  do  that  is  zero-downtime  deploys.  As much as automation can lower the time it takes to perform disruptive maintenance, you can’t eliminate the downtime it takes entirely. You  can  use  scriptable  load  balancers  to  solve  zero-downtime  deploys  or  mainte‐ nance by adding the ability to enable disable request pausing. Take, for example, an application  that  errors  on  any  request  during  maintenance,  as  demonstrated  in Figure 25-6.  6 Adya, Atul, et al.  2016 . “Slicer: Auto-sharding for Datacenter Applications.” In Proceedings of the USENIX Conference on Operating Systems Design and Implementation. https:  www.usenix.org system files conference  osdi16 osdi16-adya.pdf.  7 Fielding, R., M. Nottingham, et al.  2014 . “Hypertext Transfer Protocol  HTTP 1.1 : Caching”. IETF Stand‐  ards Track RFC.  8 Graham, John.  2014 . “Building a Low-Latency WAF Inside NGINX Using Lua.” Talk at NginxConf. 9 Francis, Scott.  2015 . “Building an HTTP Request Router with NGINX and Lua.” Talk at NginxConf.  420      Chapter 25: Scriptable Load Balancers   Figure 25-6. The client request fails because the upstream service was unavailable dur‐ ing the maintenance window.  Request pausing is when a proxy waits before forwarding the request to the desired upstream. A request can be paused for a specified amount of time or toggled by a flag stored in a datastore. When the proxy resumes, requests are forwarded to their origi‐ nal target, as shown in Figure 25-7.  Making the Difficult Easy      421   Figure 25-7. When the client request hits the load balancer, the load balancer waits until request pausing is disabled to forward it to the application.  Rather  than  failing  a  request,  clients  receive  a  slower  response.  Depending  on  the Service-Level Objectives  SLO , this can be the difference between staying within this month’s error budget or bursting through it. Scriptable load balancers can be pro‐ grammatically instructed to start pausing requests for a service, track whether clients have disconnected, and slowly forward requests as the service comes online to avoid overloading upstream services. Service-Level Middleware Eliminating toil is one of the core principles in SRE.10 As organizations continue to adopt SRE best practices, more teams are moving to a product or service model to kick the ticket ops cycle. In  the  traditional  ops  model,  engineers  deploy  and  operate  dependencies  for  each application. A traditional example of this would be a database. The ops teams would  10 Beyer, Betsy, et al., eds.  2016 . “Part II. Principles.” In Site Reliability Engineering  O’Reilly .  422      Chapter 25: Scriptable Load Balancers   set up, and babysit, SQL servers for the applications they’re operating. With the prod‐ uct or service model, a dedicated team builds out an application-agnostic database as a  service.  This  service  is  then  exposed  by  an  API UI   e.g.,  what’s  offered  by  cloud providers . Although  a  product–service  model  is  substantially  better  than  toil  and  “operating snowflakes,” it still pushes the burden of interoperating with a provisioning service to applications. In some cases, this can quickly lead to high overhead. Calling out to a service requires that an application be aware of its existence. Picture an identity ser‐ vice that every application uses in order to authenticate requests. Each application is forced to hold awareness of the identity service, which leads to an increased mainte‐ nance burden and the overhead of calling to an outside service. With scriptable load balancers, product service models aren’t the only way SRE teams can eliminate toil. Middleware to the Rescue Scriptable  load  balancers  are  typically  the  first  component  that  touches  a  request. This makes them an ideal location for logic that affects multiple services. Rather than making  RPC  calls  to  outside  services,  applications  can  read  headers  added  to  the request before it was proxied. You can think of this model as middleware similar to those found in most web frameworks. A service-level middleware transparently oper‐ ates on requests before they hit the upstream application. Product developers can free themselves of the overhead and complexity of calling to an outside service. For certain problems, a middleware is a more natural solution.11 Take, for example, the identity service mentioned earlier. Rather than calling out directly to the identity service,  an  application  can  assume  any  request  that  has  a  certain  header  that  has already been authenticated. The header could contain the scope and access the client has. The identity middleware removes the latency and complexity overhead of calling to outside services from within upstream services. APIs of Service-Level Middleware All but the most trivial of middlewares require some application-specific configura‐ tion  e.g., specifying a different cache key on a particular page . The immediate go-to solution might be to hardcode this configuration in the middleware code base. But, over  time,  the  hardcoded  configuration  will  likely  diverge  from  reality  and  lead  to easily avoidable outages. To prevent divergence, it’s important to establish clear APIs through which applica‐ tions can constantly communicate to downstream middleware. You can use custom  11 Agababov, Victor, et al.  2015 . “Flywheel: Google’s Data Compression Proxy for the Mobile Web”. In Pro‐  ceedings of the USENIX Symposium on Networked Systems Design and Implementation.  Service-Level Middleware      423   HTTP request and response headers as the communication bus over which configu‐ ration is communicated. For data that doesn’t fit into the request response flow, you can use an out-of-band message bus to propagate state to all load balancers. Case Study: WAF Bot Mitigation It’s only a matter of time before a web service is targeted by a Distributed Denial-of- Service   DDoS   attack  or  exploited  through  automated  bots.12  DDoS  attacks  can cause nasty outages, with serious downtime. Automated bots beating out human cus‐ tomers often leads to unhappy customers. Rather than developing the same antibot or DDoS mitigation tooling in each application, you can use scriptable load balancers to build a layer of protection against these threats and use them on all web-exposed services.13 Cloudflare has built a business providing such a layer with its web application firewall functionality. Any service behind its middleware gains the same benefits of protec‐ tion against Open Web Application Security Project  OWASP  vulnerabilities, com‐ mon  DoS  vectors,  and  zero-day  exploits.  When  the  danger  or  authenticity  of  a request is ambiguous, the middleware is able to redirect to a challenge-response test to validate that the request comes from a legitimate source. Whereas  previously  protection  against  attacks  below  the  application  layer  would require making a decision based on the scope of a single packet, scriptable load bal‐ ancers allow you to make decisions after analyzing the entire transaction. Now you can  optimize  for  user  experience  while  keeping  your  services  secure.  The  most important takeaway is that a WAF middleware, such as that offered by Cloudflare, allows for the concentration of work into a single service, from which benefits are reaped across multiple applications. Avoiding Disaster When SLOs are defined, they’re often focused on the internal failures of a service.  SLA inversion is the idea that in order to get an accurate SLO of a service, you must take into account all direct  e.g., database  and indirect  e.g., internet routing  com‐ ponents on which the service depends.14 In many architectures, the load balancing tier  is  shared  by  multiple  applications  and  services.  Any  resiliency  compromises made in the load balancers affects the SLOs of all upstream applications. Given this  12 Kandula, Srikanth, et al.  2005 . “Botz-4-Sale: Surviving Organized DDoS Attacks That Mimic Flash Crowds”.  In Proceedings of the USENIX Symposium on Networked Systems Design and Implementation.  13 Majkowski, Marek.  2016 . “Building a DDoS Mitigation Pipeline.” Talk at Enigma. 14 Nygard, Michael T.  2007 . “SLA Inversion.” In Release It! Design and Deploy Production-Ready Software   O’Reilly .  424      Chapter 25: Scriptable Load Balancers   reality, it’s paramount to focus on maintaining a high level of resiliency in scriptable load balancers. With the numerous benefits offered by scriptable load balancers, it’s easy to overload load balancers with logic. The load balancers go from being a powerful solution for difficult  problems  to  a  hammer  that’s  looking  for  nails.  As  in  all  architectures,  it’s important that logic placed into scriptable load balancers doesn’t add single points of failure or negatively affect availability. A particularly dangerous pitfall when you are using  scriptable  load  balancers  is  mismanaging  state.  The  rest  of  this  chapter  dis‐ cusses how to best handle state on the edge. Getting Clever with State Through the previous examples, we’ve seen how powerful scriptable load balancers can be. Although past examples have focused on individual requests, in reality, when clients  interact  with  our  services,  it’s  typically  through  a  series  of  requests  that  we often refer to as a session  e.g., a customer browses a shop, adds items to their cart, and then goes to checkout . To be able to properly reason about a request, it’s important to know the context in which  it’s  being  made.  A  single  request  to  a  slow-loading  page  can  be  business  as usual, while a hundred in quick succession is a DoS attack. Storing state allows you to distinguish between these two events. If the way state is stored and accessed isn’t cau‐ tiously reasoned about, it can quickly lead to compromising the availability of our edge tier by adding a single point of failure or drastically increasing complexity. Depending on which load balancers share the same database, the state stored repre‐ sents a different snapshot of the world. If each load balancer were to talk to the same database on every request and the database offered consistent guarantees, we’d have a perfect understanding of the world. Depending on the number of processes, nodes, and  data  centers,  such  a  setup  would  have  impractical  overhead  and  latency  costs. Instead,  it’s  important  to  modify  how  you  approach  certain  problems  that  require consistent, global views of the world. Let’s take a throttling mechanism as an example. As Figure 25-8 illustrates, the throt‐ tle passes requests up to a certain limit for a given amount of time and blocks all sub‐ sequent requests until the next block of time. This sort of throttle typically requires having  a  single,  consistent  source  of  truth  shared  between  all  potential  throttling points.  Avoiding Disaster      425   Figure 25-8. Each request is registered in a database shared by both load balancers.  An  alternative  to  storing  a  counter  in  a  shared  data  store  would  be  to  store  the counter on each load balancer, as depicted in Figure 25-9, but divide the maximum throttle size by the number of load balancers receiving requests. If a single load bal‐ ancer reaches the limit, all other load balancers are likely to have reached the same limit as well. What we lose in precision, we gain in resiliency.  Figure 25-9. Requests are only registered on the load balancer accepting the request. Case Study: Checkout Queue Online shopping websites will have checkout queues, similar to a brick-and-mortar store, as a form of back pressure to ensure the service doesn’t fail during high-write traffic events  e.g., a sale . Shopify has had to implement such a checkout queue to handle  large  flash  sales  brought  on  by  high-profile  merchants.  The  instinctive approach is to store a queue in a database that can be accessed by all load balancers. The queue would store the order of customers attempting to check out with a cart. Unfortunately,  this  queue  database  adds  another  potential  source  of  failure  and increases complexity.  426      Chapter 25: Scriptable Load Balancers   Instead of storing the queue of customers in a database shared between multiple load balancing nodes, Shopify implemented a prioritized throttle that behaved similarly to a queue. This was done without any shared database, only a checkout queue entry timestamp,  paired  with  some  session  state,  stored  in  each  customer’s  browser through signed cookies. Each  load  balancer  has  a  throttle,  similar  to  what  we  described  earlier,  deciding whether there’s enough capacity for a customer to check out. Customers are assigned a checkout entry timestamp when they first attempt to proceed to checkout. If the service  is  overloaded,  customers  poll  in  the  background  on  a  queue  page. Figure  25-10  illustrates  the  flow  of  a  customer  going  through  a  throttled  checkout process.  Figure 25-10. Request flow of a customer attempting to checkout, being throttled, and then eventually making it through the queue.  With each poll, individual load balancers modify their view of how long the queue is. They do this by updating the internal node timestamp that decides whether a cus‐ tomer is allowed to attempt to pass the throttle to checkout. The node timestamps are modified with a Proportional-Integral-Derivative  PID  controller that aims to maxi‐ mize the number of requests passed to the throttle without exceeding its limit. With  Avoiding Disaster      427   slight shifts in thinking of how and which state to store, suitable solutions for scripta‐ ble load balancers with better resiliency present themselves. Looking to the Future and Further Reading As with all new technology, scriptable load balancers aren’t perfect. Adding logic to the load balancing tier is a double-edged sword. Without effort and focus on resil‐ iency, the consequences could be disastrous, lowering the availability of the applica‐ tions we support. A byproduct of being new is that there isn’t a clear consensus on best practices. Despite speed bumps, the ability for scriptable load balancers to improve resiliency and performance can’t be argued. There’s a real opportunity for scriptable load bal‐ ancers to be a watershed moment and become a common tool in every SRE’s toolbelt. I hope this chapter convinces you to give them a try. If  you  enjoyed  the  examples  in  this  chapter,  I  encourage  you  to  learn  more  about them.  Scott  Francis’s  talk  at  NginxConf,  “Building  an  HTTP  request  router  with NGINX and Lua,” goes into detail about building a shard routing layer in Nginx with OpenResty.  John  Graham-Cumming  as  well  as  Marek  Majkowski  have  spoken numerous times on how Cloudflare implements its WAF systems with scriptable load balancers.15,16 Intermission, the request pauser example, was inspired by a project by the same name built by Basecamp.17 Finally, I’ve written more in-depth about how the checkout queue throttling mechanism works in “Surviving Flashes of High-Write Traffic Using Scriptable Load Balancers.”18 For more general resources on load balancers, you can’t go wrong with Vivek Pan‐ yam’s  “Scaling  a  Web  Service:  Load  Balancing,”19  Matt  Klein’s  “Introduction  to Modern  Network  Load  Balancing  and  Proxying,”20  and  the  “Application  Architec‐ tures” chapter in Practice of Cloud System Administration: DevOps and SRE Practices for Web Services.21  15 Graham, John.  2014 . “Building a low-latency WAF inside NGINX using Lua”. Talk at NginxConf. 16 Majkowski, Marek.  2016 . “Building a DDoS Mitigation Pipeline”. Talk at Enigma. 17 Intermission. 18 Stolarsky, Emil.  2017 . “Surviving Flashes of High-Write Traffic Using Scriptable Load Balancers Part II”.  Blog post.  19 Panyam, Vivek.  2017 . “Scaling a Web Service: Load Balancing”. 20 Klein, Matt.  2017 . “Introduction to Modern Network Load Balancing and Proxying.” 21 Limoncelli, Thomas A., et al.  2014 . “Application Architectures.” In Practice of Cloud System Administration:  DevOps and SRE Practices for Web Services. Boston: Addison-Wesley Professional.  428      Chapter 25: Scriptable Load Balancers   Emil Stolarsky is an infrastructure engineer with a passion for load balancers, perfor‐ mance, and DNS tooling. When he’s not analyzing flame graphs, you can find him lis‐ tening to Flume and fighting his fear of heights in a nearby rock climbing gym.  Looking to the Future and Further Reading      429    CHAPTER 26 The Service Mesh: Wrangler of Your Microservices?  Matt Klein, Lyft  Over the past 5 to 10 years, microservices have become all the rage in distributed sys‐ tems design and operation. Once relegated to only the infrastructures of the largest internet companies, the technosphere now buzzes with phrases such as “immutable container provisioning and scheduling,” “continuous integration and deployment,” “decentralized  control,”  and  “polyglot  language  implementations.”  It  is  absolutely true  that  microservice  architectures  do  allow  large  development  teams  to  operate with more agility. However, often overlooked is the harsh reality that large internet companies  have  typically  invested  hundreds  of  person-years  in  development  and operational effort to make distributed architectures work well in practice. Reliability engineers are left wondering how to grapple with the myriad operational problems that come up when attempting to move away from our monolithic applications and deploy such an architecture in practice. How do services find and communicate with one another? How are distributed services observed and debugged? How are services packaged and deployed? What kind of complex failure scenarios can happen? As on-the-ground microservice practitioners soon realize, the majority of operational problems that arise when moving to a distributed architecture are ultimately groun‐ ded in two areas: networking and observability. It is simply an orders-of-magnitude- larger problem to network and debug a set of intertwined distributed services versus a  single  monolithic  application.  What  does  this  fact  ultimately  mean  for  reliability engineers? A giant confusing mess that is difficult to reliably operate.  431   Over the past few years, a new paradigm1 has emerged that is most commonly known as the service mesh. The service mesh offers a reprieve for those attempting to build and  operate  microservice  architectures  by  creating  a  generic  substrate  on  which applications communicate. Developers can write applications in any language, largely unaware  of  how  the  distributed  network  is  implemented,  instrumented,  and,  most important, reliably operated. This chapter explores the reasoning behind the intro‐ duction of the service mesh as well as the architectural benefits to microservice devel‐ opers  and  reliability  engineers  alike.  The  chapter  ends  with  a  case  study  of  the deployment of an Envoy-based service mesh at Lyft. Ready to Get Rid of the Monolith? Before anyone starts thinking about microservices, they typically already have a func‐ tioning  monolithic  application  composed  of  the  following  pieces,  as  shown  in Figure 26-1:    Internet load balancer  e.g., Amazon Web Services [AWS] Elastic Load Balancer  [ELB]     Stateless application stack  e.g., PHP or Node.js    Database  e.g., MongoDB or MySQL   Figure 26-1. Monolithic architecture  Very well-known internet applications  e.g., Twitter, Salesforce, Snapchat, and many others  became extremely large and handle high-traffic loads with only small varia‐ tions on the items in this list. Why? Because it is vastly simpler to understand and operate such an architecture versus a fully distributed one. This chapter does not go into detail on why companies eventually almost always make the switch from mono‐ lith to microservice architecture; much has already been written about that topic.2   1 “New” is used here in a very loose sense. Nothing is really ever new in computing. Service meshes have ori‐  gins going all the way back to mainframes and Enterprise Service Buses  ESBs .  2 The writings of Martin Fowler  https:  martinfowler.com microservices   are probably the most well known on  this topic.  432      Chapter 26: The Service Mesh: Wrangler of Your Microservices?   However, from a networking and observability perspective it’s instructive to discuss some of the operational problems that are already typically visible even with this very basic architecture. Let’s look at some here: Lack of networking visibility  In Figure 26-1, the client needs to communicate with the load balancer, the load balancer  with  the  application,  and  the  application  with  the  database.  Thus,  in actuality the simple monolith is already a distributed application with network‐ ing and observability complexities. If something goes wrong, how does an engi‐ neer  determine  the  source  of  the  problem?  They  will  rely  on  whatever  stats, logging, and tracing is available. Odds are, the debugging tools and data are mea‐ ger, hard to access, and different across all of the components, making diagnosis extremely difficult.  Application layer connection handling inefficiencies  Many monolithic web applications make use of an application language that is very productive but not necessarily well performing—for example, Ruby, Python, and Node.js. These languages typically do not deal very well with asynchronous computation and in particular network latency. As request volume increases, the chance of a network stall also increases. When a stall occurs, and without the aid of proper asynchronous back pressure, these platforms can quickly become over‐ whelmed and fall over.3  Even with this relatively “simple” architecture, practitioners are already seeing opera‐ tional and reliability pain primarily due to networking issues and observability  or rather  lack  thereof .  When  a  decision  is  made  to  move  away  from  the  monolith toward a distributed microservice architecture  often for good reasons , the nascent networking and observability issues already seen become substantially worse almost immediately. In fact, it is not uncommon for microservice architectural rollouts to be aborted  because  of  network  reliability  concerns.  Without  the  proper  substrate  in place, developers do not understand the network or how to debug it and thus they do not trust it. Developers then typically return to adding features to the monolith, leav‐ ing a few “noncritical” microservices running. This is a vicious cycle. In some sense, making the network reliable, transparent, and easy to operate is the requirement for a successful distributed architecture rollout.  3 The typical solution to this problem is to install a high-performance proxy such as HAProxy alongside the  application. This pattern has been used for many years, and in some sense, it can be considered a precursor to the full-service mesh.  Ready to Get Rid of the Monolith?      433   Current State of Microservice Networking At this point, it would be useful to step back and look at the current state of microser‐ vice  networking  in  the  industry.  At  a  high  level,  the  following  components  are involved: Languages and frameworks  Almost all modern applications are polyglot  multilingual . It is very rare these days to find an organization that is capable of restricting the set of languages in use  to  only  one  or  two.  Instead,  it  is  much  more  common  to  find  a  monolith written in Ruby, PHP, or Node.js, while services are written in Python, Go, Java, and C++. With each language comes wildly different performance characteristics as well as one or more frameworks on which applications are built  e.g., REST via Flask in Python and gRPC .  Protocols  Modern distributed applications are composed of many protocols related to real- time remote procedure calls  RPC   e.g., REST, gRPC, HTTP 1.1, and HTTP 2 , messaging  e.g., Kafka and Kinesis , caching  e.g., Redis and memcached , and databases  e.g., MySQL and MongoDB .  Infrastructures  Across the industry we now see applications deployed across on-prem assets, vir‐ tual machines within Infrastructure as a Service  IaaS; e.g., AWS Elastic Compute Cloud  [EC2]  and  Google  Compute  Engine  [GCE] ,  Containers  as  a  Service  CaaS; e.g., AWS Elastic Container Service [ECS] and Google Kubernetes Engine [GKE] , and “serverless” or Functions as a Service  FaaS; e.g., AWS Lambda and Google Cloud Functions .  Load balancers  Load balancers are a primary component of a distributed architecture. Deployed solutions range from traditional hardware devices by F5 and Juniper to virtual devices by the same companies as well as cloud solutions such as the AWS ELB and the Google Cloud Platform  GCP  Internal Load Balancing  ILB .  Service discovery  Distributed applications need to find each other. Mechanisms range in complex‐ ity from Domain Name System  DNS  to fully consistent solutions like Consul.  Distributed system best practices  At  a  theoretical  level,  microservice  practitioners  are  told  that  they  need  to employ best practices such as retries with exponential back-off, circuit breaking, rate limiting, and timeouts. The actual implementations of these best practices are usually varied or missing entirely.  434      Chapter 26: The Service Mesh: Wrangler of Your Microservices?   Authentication and authorization  Although most internet architectures utilize edge encryption via Transport Layer Security  TLS  and some type of edge authentication, the implementations vary widely,  from  proprietary  to  OAuth.  For  service-to-service  authentication  and authorization, many deployments do nothing. Others use HTTP basic authenti‐ cation, while a small percentage have extremely complex systems that make use of mutual TLS, a centralized certificate authority, and Role-Based Access Control  RBAC .  Networking libraries  Trying to stitch all of these things together is a huge variety of in-process net‐ working  libraries  in  every  language.  These  libraries  range  in  complexity  from simple  HTTP  request  and  response  libraries  such  as  the  Python  requests  and PHP cURL libraries, to the gRPC libraries in 10+ languages, to the Java Virtual Machine   JVM   only  but  hugely  feature-rich  Finagle.  Each  library  attempts  to make transparent one or more of the previously described networking features. One thing common to in-process library solutions is the pain of upgrade; large deployments  might  have  hundreds  of  services,  and  typically  the  only  way  to upgrade a library across every service is to get them all to deploy. This is painful but not debilitating for feature upgrades. However, for security issues, the num‐ ber of needed deploys leads to tremendous operational burden.  Observability  Ultimately, developers and reliability engineers need to operate the entire system. They do this via a combination of logging, metrics, and, if they are lucky, dis‐ tributed  tracing.  However,  all  of  these  components  typically  output  different combinations and formats of logs, metrics, and traces  and sometimes nothing! . The cognitive load of trying to piece together the outputs of all of these disparate systems into a coherent debugging and operations story is extremely difficult, if not impossible.  What do all of the previously described components yield for most practitioners? As stated  in  the  introduction,  a  giant  mess.  Another  important  takeaway  is  that  even though  all  of  these  listed  items  are  important,  ultimately  the  most  critical  thing  is observability. As I like to say: observability, observability, observability. Without easy and consistent introspection into the system, debugging the inevitable problems that arise  is  almost  impossible.  This  leads  directly  to  the  general  perception  that  dis‐ tributed networking is unreliable for application development. To have a successful distributed microservice rollout, it is absolutely critical that all deployed services can access a consistent set of features and observability. As an industry can we do better?  Current State of Microservice Networking      435   Service Mesh to the Rescue In the face of the confusing networking landscape presented in the previous section, how  can  developers  and  reliability  engineers  take  control  and  bring  sanity  back  to microservice application development? They have two real options: Option 1  Limit  the  number  of  languages  used  within  the  organization  and  introduce extremely complex libraries that encapsulate all of the required features in a con‐ sistent way. This is a hugely expensive option, but many of the largest internet companies,  including  Google,  Facebook,  and  Twitter,  have  used  it  successfully  when I mentioned hundreds of person-years in the introduction, this is where many of those years end up .  Option 2  Deploy a “sidecar” high-performance proxy alongside every application as shown in Figure 26-2. This proxy is written once in a single language, encapsulates all of the required features, and aims to make the network transparent to applications —no matter the language in which they are written. When every application is deployed alongside a sidecar proxy, applications talk only to the proxy, and the proxies are responsible for discovering and communicating among themselves, the architecture is now a mesh. A service mesh, to be exact.  Figure 26-2. Service mesh architecture The Benefits of a Sidecar Proxy Initially, the benefits of a sidecar proxy architecture are counterintuitive. How can introducing an entirely new component into the system increase developer efficiency and overall reliability? In fact, the benefits of this approach are numerous and include the following:  436      Chapter 26: The Service Mesh: Wrangler of Your Microservices?   Out-of-process architecture  The fact that the sidecar proxy is an independent server means that a large vari‐ ety  of  complicated  features  can  be  implemented  once.  You  can  then  use  these features alongside any application language, whether it be Java, Go, Python, or Haskell.  High-performance code base  Because of the self-contained server, it is reasonable for the proxy to be imple‐ mented in as high-performance a way as possible, typically still in C C++. Appli‐ cations  written  in  less-performant  languages  still  have  access  to  a  high- performance substrate  more on the implications of this in a moment .  Pluggability  You can make the proxy pluggable such that it supports different protocols and functionalities.  For  example,  in  addition  to  HTTP,  the  proxy  also  can  support Redis and MongoDB. You can add connection and or request-level global rate- limiting to both HTTP and MongoDB traffic. Economies of scale  only one code base!  make it possible to support a large variety of protocols and scenarios.  Advanced protocol support  The sidecar will support the most advanced protocols more quickly than many languages and frameworks; for example, HTTP 2, QUIC, and TLS 1.3.  Service discovery and active passive health checking  Distributed systems typically make use of multiple kinds of service discovery and health checking. The proxy can implement them all and hide them from applica‐ tions.  Advanced load balancing  Having consistent and reliable implementations of retry, timeouts, circuit break‐ ing, rate limiting, shadowing, outlier detection, and so on is critical for any mod‐ erately large system. When applications can entirely offload these functionalities to the proxy, the application code is far simpler to write, and operators can be assured that every application has access to the same functionality.  Observability  As has already been written several times  and I’ll expand upon shortly , consis‐ tent observability is by far the most important thing that the sidecar proxy pro‐ vides. Operators have access to consistent stats, logs, and traces for every hop in the distributed system. This 100% coverage enables the automatic generation of per-service  dashboards  and  alarms  and  a  high  level  of  operational  consistency across every service and the entire organization.  Service Mesh to the Rescue      437   Additional use as an edge proxy  As it turns out, 90% of what an edge proxy and a sidecar proxy do are the same. The operational efficiency gained from using the same software in both locations is large. Why learn how to deploy and monitor both an edge and a sidecar proxy if a single component can do both jobs?  Ease of deployment and upgrade  The sidecar proxy is easy to deploy and upgrade because it is not embedded into applications. Should operators deploy a new binary or configuration of the proxy across all hosts over a period of a few minutes? Probably not. Is it possible with an  out-of-process  solution?  Yes.   Further  consider  the  example  of  deploying  a TLS  upgrade  to  patch  a  security  vulnerability  when  TLS  is  embedded  in  each application.   The rest of the chapter covers some of the more interesting fine points of the sidecar or service mesh architecture. Eventually Consistent Service Discovery In distributed systems, service discovery is the mechanism by which distributed pro‐ cesses find one another. There are many varieties of service discovery, ranging from statically configured IPs, to DNS, to systems the rely on fully consistent leader elec‐ tion protocols such as Zookeeper. Over the past 5 to 10 years, it has become fairly common to use fully consistent leader election stores such as Zookeeper, etcd, and Consul for service discovery. The problem with fully consistent systems is that although required for certain use cases, they are extremely complex and require a large amount of thought and care to run, especially at scale with a high volume of data. It is not uncommon for large com‐ panies to have entire teams dedicated to running Zookeeper or etcd. Logic would say that  unless  a  problem  truly  requires  full  consistency   e.g.,  distributed  locks ,  you should not use full consistency! However, historically, even though service discovery is really an eventually consistent problem  does an operator really care whether every host has the same view of the network as every other host as long as the topology eventually converges? , fully consistent stores have been used, leading to many out‐ ages when the store goes down. The optimal service mesh design assumes that an eventually consistent service dis‐ covery  system  is  used  from  the  beginning.  The  data  from  the  discovery  system  is cross-checked with active as well as passive health checking. Active health checking is the act of sending out-of-band pings such as an HTTP request to a  healthcheck end‐ point. Passive health checking is the act of monitoring in-line request response data to determine remote endpoint health. For example, three HTTP 503 responses in a row might indicate that a remote endpoint is unavailable.  438      Chapter 26: The Service Mesh: Wrangler of Your Microservices?   Table 26-1 shows how the sidecar proxy uses the union of service discovery data and active  health  checking  to  yield  an  overall  health  status  for  a  backend  host.  Active health checking is considered to be more reliable than service discovery; this means that  if  service  discovery  data  is  lost  but  active  health  checking  is  still  passing,  the proxy  will  still  route  traffic.  Only  for  the  case  in  which  active  health  checking  has failed and the backend is absent from service discovery will the backend be purged. This yields high reliability because the service discovery data can be eventually con‐ sistent and come and go without affecting production.  Table 26-1. Service discovery and active health checking matrix   Service discovery present Route Route Service discovery absent  Don’t route Don’t route and purge  Active HC OK Active HC failed  The combination of active and passive health checking alongside an eventually con‐ sistent discovery store  e.g., every host checking in once per minute into a cache with a time to live [TTL] , yields extremely high reliability for possibly the most critical component of the entire distributed system. Observability and Alarming As  I  have  already  mentioned  several  times,  observability  is  ultimately  the  most important thing that the sidecar proxy and service mesh architecture provides. Prob‐ lems  at  the  network  layer  will  happen.  What’s  most  important  is  giving  reliability engineers the tools so that they can identify the problem, work around it, and fix it as quickly as possible. To this end, the proxy provides the following features: Consistent statistics for every hop  Because the sidecar proxy handles both ingress and egress traffic for every appli‐ cation in the system, it follows that every hop in the mesh has coverage by the same  robust  set  of  statistics.  These  range  from  relatively  simple  things  like requests per second, connections per second, and so on to fully dynamic statistics such as number of HTTP 502s per second and number of Mongo update com‐ mands per call site per second, etc.  A persistent request ID that can join logs and traces across the entire system  The proxy can form the trace request ID root such that a single ID is propagated throughout the system for all network hops. This allows traces to be generated, logs  to  be  joined,  and  consistent  sampling  to  be  done.   1%  sampled  logs  are much more useful if the sampling is done across the entire system and captures entire request flows versus completely random sampling .  Service Mesh to the Rescue      439   Consistent logging for every hop  As with statistics, if all applications are outputting the same request logs regard‐ less of language, it’s substantially easier for humans and tooling to process the data and understand it quickly.  Distributed tracing for the entire system  Tracing  is  an  incredibly  powerful  tool  to  visualize  distributed  system  request flows. Typically, however, it’s quite complex to introduce it into a microservice architecture because every application must be modified to create spans, forward context, and so on. The service mesh can provide 100% tracing coverage with no involvement from the application whatsoever.  Consistent system-wide alarming  Instead of relying on every application developer to set up basic alarming around service call success rate, latency, and so forth, the service mesh provides statistics, logging, and tracing, allowing basic alarms to be created automatically for every service. This is an incredibly powerful tool for reliability engineers who can now more easily enforce and audit system-wide behavior.  Using  the  previously  described  tools,  you  can  build  custom  autogenerated  dash‐ boards and tooling to expose the generated information to engineers in a way that makes it much easier to sort through everything and determine the root cause of a problem. These include things like “service to service” dashboards, which allow engi‐ neers to select a source and destination service from a drop-down list and see statis‐ tics, traces, and logs for that hop. The UI UX possibilities are broad and could be the subject of an entire other chapter! Sidecar Performance Implications By now, hopefully it is clear that the sidecar or service mesh design offers tremendous benefits  to  developers  and  reliability  engineers.  A  common  concern  often  follows, however:  What  about  performance?  Doesn’t  adding  all  of  these  extra  hops  slow things down a lot? It’s true that adding extra hops into a distributed system will add more latency and utilize more machine resources  CPU and RAM . Because of this, it’s important that the sidecar proxy be written in an extremely efficient manner. Many proxies are still written in native C C++. However, it’s important to unpack the performance ques‐ tion a bit and dig into the details. In discussions of performance, there are two primary metrics that are of interest: Throughput  How much data can be pushed through a component per unit of time  requests per second, connections per second, transactions per second, etc. ?  440      Chapter 26: The Service Mesh: Wrangler of Your Microservices?   Tail latency  As throughput ramps up, how does the component perform at the “tail” of the latency histogram? That is, how do per-transaction latency numbers at P50 com‐ pare to P99 or even P99.9?  Although throughput is important, in practice it really matters for only the largest companies. This is because for smaller companies, developer time is almost always worth  more  than  infrastructure  costs.  It’s  only  at  very  large  scale  that  overall throughput really begins to matter when considering total cost of ownership  TCO . Instead, tail latency ends up being the most important performance metric for both small and large companies. This is because the causes of high tail latency are hard to understand  and  lead  to  a  large  amount  of  developer  and  operator  cognitive  load. Engineer time is typically the most precious resource for an organization, and debug‐ ging tail latency issues is one of the most time-intensive things that engineers do. As a result, the sidecar proxy ends up being a double-edged sword. The proxy adds a tremendous amount of functionality to the system, so much that for all but the most performance-intensive applications, no one will notice an extra millisecond here or there as long as the tail latency properties are not highly affected by the proxy itself. This is primarily why the performance and, in particular, tail latency properties of the proxy are so important. If the proxy forms the basis by which the entire distributed system is observed, how can you trust the data if the proxy itself has a large amount of variability? This is why the best proxies are still written in native code with perfor‐ mance objectives similar to operating systems and databases. Thin Libraries and Context Propagation The benefits that the service mesh can provide to applications without the application doing anything at all are very large. However, this chapter has so far glossed over the unfortunate reality that no matter what, applications still have a role to play, primar‐ ily around context propagation. Context propagation is the act of taking context from an ingress network call, poten‐ tially modifying it, and then passing it to an egress network call. How this is done is language or platform specific and not something that the sidecar proxy can provide. Although propagated context has many uses, the primary use as it relates to the ser‐ vice mesh is propagation of request IDs and trace contexts. For HTTP-based archi‐ tectures  this  is  primarily  done  via  HTTP  headers  such  as  x-request-id  and  the Zipkin  x-b3-traceid header. Users of a service mesh need to provide application- and language-specific thin libraries to enable users to easily propagate the required HTTP  headers.  Although  it  is  out  of  scope  for  this  chapter,  I  would  expect  to  see some convergence of the libraries that developers use for this purpose over the next several years.  Service Mesh to the Rescue      441   The astute reader might now be asking: “But you said that the service mesh is magic! I still have to do something?” It is true that developers are still required to participate in the mesh at the application layer if full functionality is desired. However, this is still many orders of magnitude less code and functionality that would otherwise need to be duplicated in every lan‐ guage and framework. Configuration Management  Control Plane Versus Data Plane  So  far,  we  have  discussed  many  of  the  features  that  the  service  mesh  can  provide when  building  reliable  microservice  architectures.  We  have  not  discussed  how  the entire system is configured, which can become quite complicated. First, though, a few definitions: Data plane  This is the portion of the system that actually participates in forwarding network traffic. This includes load balancing, connection pooling, request and response modification, and so on. The data plane touches every byte of every request and response. Control plane  This is the portion of the system that sets up a topology and provides high-level configuration over what happens at any given time. This includes route tables, backend host registration, traffic modification rules, quotas, and so forth.  To use a subway analogy, the data plane is the actual subway cars going from point A to  point  B.  The  control  plane  is  the  high-level  switching  system  that  occasionally switches tracks, says how many trains should be moving at any one time, and so on. When deploying a service mesh architecture, it is important to separate the different components in the system. The sidecar proxy itself is the data plane. However, a con‐ trol  plane  is  required  that  can  take  user-defined  configuration,  which  is  generally fairly opinionated and depends on the deployment and configuration management system  in  use,  and  translate  that  configuration  into  a  format  that  the  sidecar  can understand. The configuration must then be distributed to all of the sidecar proxies. Typically, the further away from the proxy you get, the more opinionated the system becomes.  For  example,  a  user  who  runs  on-prem  in  a  data  center  on  bare-metal machines will have a vastly different service discovery process than someone running inside a CaaS system hosted by a cloud provider. The goal of the sidecar proxy data plane is to provide a consistent configuration API that can be targeted. In the sim‐ plest form, a centralized configuration generator might work like this:  1. Look for global system changes that require data plane modification. 2. Generate new configuration for every proxy in the system.  442      Chapter 26: The Service Mesh: Wrangler of Your Microservices?   3. Deploy the configuration changes to every proxy in the system via some mecha‐  nism.  4. Force the proxies to reload their configurations.  SmartStack  based  on  HAProxy  essentially  works  as  previously  described  and  has been deployed successfully by many companies. More  complex  sidecar  proxies  provide  a  dynamic  configuration  API,  as  shown  in Figure 26-3. The API is used to deliver route table changes, backend host changes, what  ports  to  listen  on,  and  what  to  do  when  a  new  connection  is  received.  This architecture  allows  a  centralized  management  service  to  control  all  of  the  proxies, much like a central control room oversees a subway network  to continue with the subway analogy . In general, the more logic that can be moved to a central location, the easier the system is to manage; a fleshed-out API allows the bootstrap configura‐ tion of each sidecar proxy to be trivial and essentially just know how to talk to the management server. All configuration from then on is centrally governed and thus avoids any extra infrastructure to deploy configuration files, drain traffic, force the proxy to restart, and so on.  Figure 26-3. Service mesh with dynamic configuration APIs The Service Mesh in Practice Although the service mesh concept has only recently gained favor among the micro‐ service community, there are already several competing solutions and large deploy‐ ments.  Proxy  solutions  include  HAProxy,  NGINX,  Linkerd,  Traefik,  and  Envoy. Fully managed solutions include SmartStack  built on HAProxy  and Istio  built on Envoy  and  now  also  supporting  Linkerd .  What  follows  is  a  short  description  of Lyft’s transition from a monolithic application to a complete service mesh architec‐ ture built on top of Envoy. This is by no means the only service mesh success story, but it’s one I am familiar with  I am the creator of Envoy  and will hopefully provide some  flavor  to  the  generic  service  mesh  architecture  description  that  most  of  this chapter has endeavored to describe.  The Service Mesh in Practice      443   The Origin and Development of Envoy at Lyft By early 2015, Lyft had a primarily monolithic PHP application backed by MongoDB along with tens of microservices written in Python. The application at that time was deployed  in  a  single  region  within  AWS.  Lyft  had  decided  to  make  the  jump  to  a microservice architecture for the same reasons that almost everyone else does: decou‐ pling and increased agility. However, early attempts at decomposition were not going well, primarily due to all of the reasons already laid out in this chapter. The network is inherently unreliable. Lyft developers were having a tremendous amount of trouble debugging network failures and tail latency issues. In some cases, planned services were aborted and more features were added to the monolith because the network was deemed too unreliable to support the workloads. It’s important to also note that Lyft does not currently have an SRE-like job title; all developers are expected to operate their  services  at  a  high  level  of  reliability.   The  industry  move  toward  DevOps  is extremely interesting and worthy of its own chapter; I touch on some of the implica‐ tions as it relates to Lyft’s service mesh migration in a few moments.  Envoy began development in the same early 2015 timeframe. The goal of the project was to build a very high-performance network substrate that Lyft developers trusted and would ultimately be fully transparent. This did not happen overnight. Envoy was first deployed at Lyft as an edge proxy to replace and enhance the existing AWS ELB fleet.  Immediately,  the  enhanced  observability  and  protocol  support  proved  to  be immensely valuable for triaging and debugging product issues. After the deployment of Envoy as Lyft’s edge proxy, the development effort moved on to helping with MongoDB stability. We added a Binary JSON  BSON  parser that can inspect MongoDB traffic as well as a raw TCP proxy that could help with limiting the  number  of  connections  between  our  application  and  the  database.  Later,  we added global rate-limiting support between our applications and the database. Envoy allowed these enhancements to be immediately provided across all of our application stacks without modification. Over time, Envoy was deployed as a sidecar alongside every application at Lyft. Dur‐ ing this time, we removed all internal centralized load balancers, built an eventually consistent  service  discovery  system  and  API,  and  deployed  a  dense  HTTP 2  mesh between all Envoys. The presence of the service mesh unlocks a tremendous number of features for Lyft developers, all of which have already been described in this chap‐ ter. How Lyft configures and operates Envoy has also evolved over time. Originally, all configurations were handwritten and deployed alongside the binary via an eventually consistent deploy process and using Salt. Over time this switched to templated and partially machine-generated configurations using Python and Jinja. As of this writ‐ ing, Lyft  and most other users of Envoy  is moving toward a fully centralized config‐ uration system supported by a complete set of discovery APIs.  444      Chapter 26: The Service Mesh: Wrangler of Your Microservices?   The net result of deploying Envoy at Lyft has been that developers no longer consider the network when building applications. When network problems do occur, develop‐ ers have the tooling in place to help discover and remediate the issue quickly. The networking  substrate  deployed  at  Lyft  has  increased  developer  productivity, increased overall success rate, and decreased Mean Time to Recovery  MTTR  during incidents. Operating Envoy at Lyft As I already described, for better or worse Lyft does not have an SRE-like job title. Instead, all developers are expected to be reliability engineers as well. I lead the net‐ work team at Lyft, and in addition to developing Envoy we also operate it. Although I have strong opinions about the emergence of DevOps culture in general  out of scope for this chapter , for a system component such as Envoy, I think having the develop‐ ers of the system also operate it has been a forcing function toward making sure the service mesh truly lives up to the hype of creating a transparent network. In the fol‐ lowing subsections, I briefly summarize a few of the more interesting learnings in this regard.  Operational learnings Automated creation of default dashboards, traces, logs, and alarms  At  Lyft,  we  have  machine  created  dashboards  for  every  service  that  includes Envoy-related stats, logging, and tracing. Off of this data, we automatically create alarms  for  each  service.  This  creates  an  operational  baseline  for  every  service, which makes it much easier to dive in and begin debugging issues. Additionally, we  have  a  service-to-service  dashboard  that  allows  the  user  to  select  both  the egress and ingress service from drop-down lists and immediately see pertinent statistics for that hop. Finally, we have a “global” Envoy dashboard, which aggre‐ gates service mesh-wide statistics into a single place and allows trivial inspection of system-wide health.  Documentation  I can’t state enough how important documentation is and how much we under‐ invest  in  it  within  the  industry.  Documentation  is  even  more  important  in  a DevOps  environment;  although  we  expect  all  developers  to  be  reliability  engi‐ neers, in reality the base of knowledge in the field varies greatly. My team spends a  tremendous  amount  of  time  helping  debug  system-wide  issues,  and  without quality  documentation  that  we  can  point  developers  to,  we  would  be  overrun. This  primarily  equates  to  an  entire  set  of  Lyft  internal  Envoy  documentation about  Lyft-specific  dashboards,  alarms,  runbooks,  FAQs,  configuration  guides, and so on, which are more specific than the public Envoy documentation and more accessible to developers without networking experience.  The Service Mesh in Practice      445   Templated configuration generation  Envoy configuration is extremely complex. We allow development teams at Lyft the flexibility to alter only a tiny portion of it via templates that the network team controls. This allows there to be consistency for most of the system, making it easier  to  understand  in  aggregate,  while  still  allowing  local  flexibility  where appropriate  e.g., custom circuit-breaking settings per service .  Hot restart for easy roll-forward and rollback  Lyft,  like  many  organizations,  does  not  currently  have  access  to  an  immutable container deployment system in production. Envoy has robust “hot restart” func‐ tionality, which means that it can be fully restarted, including both configuration and binary, without dropping any connections. This allows us to deploy and roll back quickly and easily.  Administration endpoint for on-node debugging  Envoy provides a robust set of local administration endpoints that are designed to  be  human  readable  and  easy  to  interact  with.  Although  it’s  nice  to  aim  to never have to log into a host and look at something, in reality reliability engi‐ neers do this all the time, and having easy access to runtime information is criti‐ cal for operational agility.  Development learnings Decoupled sidecar deploy process  To truly realize the benefits of the sidecar proxy model in terms of development agility, we allow Envoy to be deployed fully through staging, canary, and produc‐ tion  independent  of  every  application.  This  allows  us  to  roll  out  new  features, fixes, and debugging tools independent of the application deploy process.  Technical learnings Address resolution protocol  ARP  tables  Why am I bringing up ARP? This is the most interesting bug that we faced when deploying Envoy at Lyft and something any large service mesh deployment will need to be aware of. ARP is the process by which IP addresses are converted into next-hop MAC addresses  L3 to L2 . Kernels contain an ARP cache, which stores the most recently used mappings. In general, any thrashing in this cache will lead to horrendous performance because re-resolution will need to occur repeatedly as cache entries are flushed. The kernel cache size typically defaults to a relatively small value that is tuned for traditional IP-based networks in which the number of immediate L2 neighbors that a node talks to is small. This is not necessarily true for a service mesh! Some modern network designs utilize large and flat L3 IP subnets. In these designs, if a host can communicate with any other host in the subnet  especially if not using intermediate load balancers , the number of neigh‐  446      Chapter 26: The Service Mesh: Wrangler of Your Microservices?   bors might be large. At Lyft, we had to increase the default size of the per-node ARP  cache  to  account  for  this  reality.  On  Linux  this  involves  tuning  kernel parameters  such  as  net.ipv4.neigh.default.gc_thresh1   and  other  related values . Other operating systems have similar settings.  File descriptor limits  Somewhat related to ARP table size, it’s also important that the sidecar proxy be allowed to create a large number of file descriptors because the proxy will end up creating a large number of mesh connections. Additionally, and primarily based on one extremely bad production outage, Envoy treats a failure to create a file descriptor  the  same  as  an  out-of-memory   OOM   condition:  a  fatal  crashing error.  At  runtime,  exhausting  the  number  of  allowed  file  descriptors  can  be extremely difficult to diagnose. Making the condition  which should never hap‐ pen, given proper configuration  a fatal error leads to greater visibility into a seri‐ ous problem and ultimately greater reliability.  Overall, the deployment and operation of a service mesh via Envoy at Lyft has been relatively smooth given the focus on incremental delivery, ease of operations, and the points that we just discussed. The Future of the Service Mesh Service mesh development across both sidecar proxies as well as management sys‐ tems is going to see a large amount of investment over the next 5 to 10 years across both software vendors as well as the large cloud vendors. The benefits to application developers and reliability engineers are tremendous; after an engineer uses and oper‐ ates a microservice built on top of a well-functioning service mesh, it is very unlikely that  they  will  ever  want  to  deploy  an  application  without  one  again.  The  benefits obtained for free are too large. Further Reading   Envoy proxy   “Service mesh data plane vs. control plane”   Istio service mesh   “Lyft’s Envoy dashboards”   “The universal data plane API”   “Microservices Patterns With Envoy Sidecar Proxy: The series”   “Introduction to modern network load balancing and proxying”   “Embracing eventual consistency in SoA networking”  The Future of the Service Mesh      447   Matt Klein is a software engineer at Lyft and the creator of Envoy. Matt has been work‐ ing  on  operating  systems,  virtualization,  distributed  systems,  and  networking,  and making systems easy to operate, for more than 15 years across a variety of companies. Some highlights include leading the development of Twitter’s C++ L7 edge proxy and working on high-performance computing and networking in Amazon’s EC2.  448      Chapter 26: The Service Mesh: Wrangler of Your Microservices?   PART IV The Human Side of SRE    How do we keep SRE teams safe for the people in them?   We used to think humans were the cause of most reliability and resiliency prob‐ lems, but what if they are actually the solution? What is going on in their heads anyway?    What  are  the  mental  health  aspects  to  SRE  that  we  are  not  paying   sufficient   attention to?    On-call is a terrible idea; we should stop.   How do people relate to complex systems?   Can we do more with SRE than just make things better for the computers; can it  be used to make our world better, too?  Discuss.   You Know You’re an SRE When…  …you  get  a  text  in  the  middle  of  the  night  and  your  partner  isn’t  even  curious because they know your other bae is just PagerDuty. …you text “hello” and the reply is “uh oh!” …you get confused for a second every time HR tells you that you have “Time Off In Lieu”  TOIL  coming. …you write a postmortem every time dinner doesn’t turn out right. …talking with your family you use the phrase, “What are the metrics of success?” and then they look at you funny.   CHAPTER 27 Psychological Safety in SRE  John Looney, Facebook  formerly Google   This  work  was  previously  published  in  Intercom  and  in  ;login: magazine before being reworked specifically for an SRE audience.  The Primary Indicator of a Successful Team When I worked for Google as an SRE, I was lucky enough to travel around the world with  a  group  called  “Team  Development.”  Our  mission  was  to  design  and  deliver team-building courses to teams who wanted to work better together. Our work was based on research later published as Project Aristotle. It found that the primary indi‐ cator of a successful team wasn’t tenure, seniority, or salary levels, but psychological safety. Think of a team you work with closely. How strongly do you agree with these five statements?  1. If I take a chance and screw up, it will be held against me. 2. Our team has a strong sense of culture, and it’s difficult for new people to join. 3. My team is slow to offer help to people who are struggling. 4. Using my unique skills and talents comes second to the objectives of the team. 5. It’s uncomfortable to have open honest conversations about our team’s sensitive  issues.  Teams that score high on questions like these can be deemed to be “unsafe.” Unsafe to innovate, unsafe to resolve conflict, unsafe to admit they need help. Unsafe teams  451   can deliver for short periods of time, provided they can focus on goals and ignore interpersonal  problems.  Eventually,  unsafe  teams  will  underperform  or  shatter because they resist change. Let  me  highlight  the  impact  an  unsafe  team  can  have  on  an  individual,  as  seen through the eyes of an imaginary, capable, and enthusiastic new college graduate. This imaginary graduate—we’ll call her Karen—read about a low-level locking opti‐ mization for distributed databases and realized it applied to the service for which her team was on call. Test results showed a 15% CPU saving! She excitedly rolled it out to production. Changes to the database configuration file didn’t go through the usual code-review  process,  and,  unfortunately,  it  caused  a  hard-lock-up  of  the  database. There was a brief but total website outage. Thankfully, her more experienced collea‐ gues spotted the problem and rolled back the change within 10 minutes. Being pro‐ fessionals, the incident was discussed at the weekly postmortem meeting. 1. “If I take a chance, and screw up, it will be held against me” At the meeting, the engineering director asserted that causing downtime by chasing small  optimizations  was  unacceptable.  Karen  was  described  as  “irresponsible”  in front of the team. The team suggested ways to ensure that it wouldn’t happen again. Unlike Karen, the director soon forgot about this interaction. Karen would never try to innovate without explicit permission again. 2. “Our team has a strong sense of culture, and it’s hard for new people to join” The impact on Karen was magnified because no one stood up for her. No one poin‐ ted out the lack of code reviews on the database configuration. No one highlighted the difference between one irresponsible act and labeling someone as irresponsible. The team was proud of its system’s reliability, so defending its reputation was more important than defending a new hire. Karen learned that her team, and her manager, didn’t have her back. 3. “My team is slow to offer help to people who are struggling” Karen  was  new  to  being  on  call  for  a  “production”  system,  so  she  had  no  formal training in incident management, production hygiene, or troubleshooting distributed systems. Her team was mostly made up of people with decades of experience who never  needed  training  or  new-hire  documentation.  They  didn’t  need  playbooks. There were no signals that it was OK for a new graduate to spend time learning these skills. There were certainly no explicit offers of help, other than an initial whiteboard‐ ing session that seemed to spend more time on how it used to work than it did today. Karen  was  terrified  of  being  left  with  the  pager.  She  didn’t  understand  how  she passed the hiring process and frequently wondered why she hadn’t been fired yet. We call this impostor syndrome.  452      Chapter 27: Psychological Safety in SRE   4. “Using my unique skills and talents comes second to the goals of the team” Karen’s background was in algorithms, data structures, and distributed computing. She realized the existing system had design flaws and could never handle load spikes. The team had always blamed the customers for going over its contracted rates, which is like a parent blaming their infant for eating dirt. Karen rightly expected that her nonoperations  background  would  have  been  a  benefit  to  the  team.  It’s  not  always clear whether a problem will require understanding a database schema, Ruby debug‐ ging, C++ performance understanding, product knowledge, or people skills. Karen proposed a new design based on technology she’d used during her internship. Her coworkers were unfamiliar with the new technology and immediately considered it  too  risky.  Karen  dropped  her  proposal  without  discussion.  She  wanted  to  write code and build systems, not have pointless arguments. 5. “It’s uncomfortable to have open, honest conversations about our team’s sensi‐ tive issues” When a large customer traffic spike caused the product to be unavailable for a num‐ ber of hours, the CEO demanded a meeting with the operations team. Many details were  discussed,  and  Karen  explained  that  the  existing  design  meant  it  could  never deal with such spikes, and then she mentioned her design. Her director reminded her that her design had already been turned down at an Engineering Review, and then he promised the CEO they could improve the existing design. Karen discussed the meeting with one of her teammates afterward. She expressed dis‐ may that the director couldn’t see that his design was the root cause of their prob‐ lems. The teammate shrugged and pointed out that the team had delivered a really good service for the past five years and thus had no interest in arguing about alternate designs with the director. Karen left work early to look for a new job. The company didn’t miss her when she left.  After  all,  she  was  “reckless,  whiny,  and  had  a  problem  with  authority.”  They didn’t reflect on the design that would have saved the company from repeated out‐ ages that caused a customer exodus. How to Build Psychological Safety into Your Own Team What is special about operations that drives away so many promising engineers and suffers others to achieve less than their potential? We know that success requires a strong sense of culture, shared understandings, and common values. We need to balance that respect for our culture with an openness to change it as needed. A team—initially happy to work from home—needs to colocate if they take on interns. Teams—proud that every engineer is on call for their service  The Primary Indicator of a Successful Team      453   —might need to professionalize around a smaller team of operations-focused engi‐ neers as the potential production impact of an outage grows. We need to be thoughtful about how we balance work that people love with work the company needs to get done. Good managers are proactive about transferring out an engineer  who  cannot  make  progress  on  a  team’s  workload  due  to  a  mismatch  in interest or skills. Great managers expand their team’s remit to make better use of the engineers they have, so they feel their skills and talents are valued. Engineers whose skills  go  unused  grow  frustrated.  Engineers  who  are  ill-equipped  to  succeed  at assigned work will feel set up to fail.  Make respect part of your team’s culture It’s difficult to give 100% if you spend mental energy pretending to be someone else. We need to make sure people can be themselves by ensuring that we say something when  we  witness  disrespect.  David  Morrison   Australia’s  Chief  of  the  Army   cap‐ tured this sentiment perfectly, in his “The standard you walk past is the standard you accept” speech. Being thoughtless about people’s feelings and experiences can shut them down. Here are some examples in which I’ve personally intervened:    Someone welcomed a new female PM to the team on her first day but made the assumption  that  she  wasn’t  technical.  The  team  member  used  baby  words  to explain a service, as an attempt at humor. I immediately highlighted that the new PM  had  a  PhD  in  computer  science.  No  harm  was  intended,  and  the  speaker expressed embarrassment that their attempt at fun could be taken any other way. It’s sometimes hard to distinguish unconscious bias from innocence.    In a conversation about people’s previous positions, one person mentioned they had worked for a no-longer-successful company. A teammate mocked this per‐ son for being “brave enough” to admit it. I pointed out that mocking people is unprofessional  and  unwelcome,  and  everyone  present  became  aware  of  a  line that hadn’t been visible previously.    A quiet, bright engineer consistently was talked over by extroverts in meetings. I pointed out to the “loud” people that we were missing an important viewpoint by not ensuring everyone speaks up. Everyone becomes more self-aware, though I had to stress in 1:1s that I expect all senior people to speak up.  It’s essential to challenge lack of respect immediately, politely, and in front of every‐ one who heard the disrespect. It would have been wonderful had someone reminded Karen’s director, in front of the group, that Karen wasn’t irresponsible, the outage wasn’t a big deal, and the team should improve its test coverage. Imagine how grate‐ ful Karen would have been had a senior engineer at the Engineering Review offered  454      Chapter 27: Psychological Safety in SRE   to work on her design with her so that it was more acceptable to the team. Improve people’s ideas rather than discount them.  Make space for people to take chances Some companies talk of 20% time. Intercom, where I worked, has “buffer” weeks in between some of their six-week sprints. People often took that chance to scratch an itch that was bothering them, without it having an impact on the external commit‐ ments the team had made. Creating an expectation that everyone on the team has permission to innovate and at the same time encouraging the entire team to go off- piste sends a powerful message. Be careful that “innovation time” isn’t the only time people should take chances. I worked  with  one  company  in  the  automotive  industry  that  considers  “innovation time” to be 2:30 PM on Tuesdays! Ideas that people think are worthy should be given air-time at team design reviews, not just dismissed. Use them as an opportunity to share context on why an idea that seems good isn’t appropriate.  Make it obvious when your team is doing well One engineer describes his experience of on-call as “being like the maintenance crew at the fairground. No one notices our work, until there is a horrible accident.” Make sure people across the organization notice when your team is succeeding; let team members send out announcements of their success. Don’t let senior people hog the limelight during all-hands meetings. I love how my team writes goals on Post-it notes at our daily standups and weekly goal meetings. These visible marks of incremental success can be cheered as they are moved to the “done” pile. We can also celebrate glorious failure! Many years ago, when I was running one of Google’s storage SRE teams, we were halfway through a three-year project to replace the old Google File System. Through a  confluence  of  bad  batteries,  firmware  bugs,  poor  tooling,  untested  software,  an aggressive rollout schedule, and two power cuts, we lost an entire storage cell for a number  of  hours.  Though  all  services  would  have  had  storage  in  other  availability zones, the team spent three long days and three long nights rebuilding the cluster. After it was done, the team members—and I—were dejected. Demoralized. Defeated. An amazing manager  who happened to be visiting our office  realized I was down and  pointed  out  that  we’d  just  learned  more  about  our  new  storage  stack  in  those three days than we had in the previous three months. He reckoned a celebration was in order. I  bought  some  cheap  sparkling  wine  from  the  local  supermarket,  and  along  with another manager, took over a big conference room for a few hours. Each time some‐ one wrote something they learned on the whiteboard, we toasted them. The team that left that room was utterly different from the one that entered it.  The Primary Indicator of a Successful Team      455   I’m  sure  Karen  would  have  loved  being  appreciated  for  her  uncovering  the  team’s weak noncode test coverage and its undocumented love of uptime-above-all-else.  Make your communication clear and your expectations explicit Rather than yelling at an engineering team each time it has an outage, help the engi‐ neers build tools to measure what an outage is, a Service-Level Objective  SLO  that shows how they are doing, and a culture that means they use the space between their objective and reality to choose the most impactful work. When discussing failures, people need to feel safe to share all relevant information, with the understanding that they will be judged not on how they fail, but how their handling of failures improved their team, their product, and their organization as a whole.  Teams  with  operational  responsibilities  need  to  come  together  and  discuss outages and process failures. It’s essential to approach these as fun learning opportu‐ nities, not root-cause obsessed witch hunts. I’ve seen a team paralyzed, trying to decide whether to ship an efficiency win that would increase end-user latency by 20%. A short conversation with the product team resulted in updates to the SLO, detailing “estimated customer attrition due to differ‐ ent latency levels,” and the impact that would have on the company’s bottom line. Anyone on the team could see in seconds that low latency was far more important than hardware costs, and instead drastically over-provisioned. If  you  expect  someone  to  do  something  for  you,  ask  for  a  specific  commitment— “When  might  this  be  done?”—rather  than  assuming  that  everyone  agrees  on  its urgency.  Trust  can  be  destroyed  by  missed  commitments;  this  is  why  a  primary responsibility of management is to set context, to provide structure and clarity. Karen would have enjoyed a manager who told her in advance that the team consid‐ ered reliability sacred, and asked her to work on reliability improvements, rather than optimizations.  Make your team feel safe If you are inspired to make your team feel more psychologically safe, there are a few things you can do today:    Give your team a short survey  like the questions posed at the beginning of this chapter , and share the results with your team. Perhaps get a trusted person from outside the team to do 1:1s with them, promising to summarize and anonymize the feedback.    Discuss  what  “safety”  means  to  your  team;  see  if  they’ll  share  when  they  felt “unsafe,” because safety means different things to different people—it can mean having confidence to speak up, having personal connections with the team, or feeling trained and competent to succeed at their job.  456      Chapter 27: Psychological Safety in SRE     Build a culture of respect and clear communication, starting with your actions.  Treat psychological safety as a key business metric, as important as revenue, cost of sales, or uptime. This will feed into your team’s effectiveness, productivity, and staff retention and any other business metric you value. Don’t optimize for it exclusively— if  someone  who  feels  unsafe  quits  your  team,  your  metrics  go  up,  which  does  not indicate success!  Why are operations teams more likely to feel unsafe than other engineering teams? Let’s unpick the melange of personality quirks and organizational necessities that put operations teams, and specifically SRE, into danger.  We love interrupts and the torrents of information.    Humans suck at multitasking. Trying to do multiple things at once either doubles the time it takes to complete the task or doubles  the  mistakes.1  A  team  that’s  expected  to  make  progress  with  project  work while being expected to be available for interrupt work  tickets, on-call, walk-ups  is destined to fail. And yet, operations attracts people who like being distracted by novel events. Do one thing at a time. “Timebox” inbound communications as well as inter‐ rupt time. Operations teams are expected to manage risk and uncertainty for their organization. We build philosophies for reasoning about risk; strategies for coping with bad out‐ comes; defense in depth, playbooks, incident management, escalation policies, and so on.  When  humans  are  exposed  to  uncertainty,  the  resultant  “Information  Gap” results in a hunger for information, often exaggerated past the point of utility.2 This can  lead  to  information  overload  in  the  shape  of  ludicrously  ornate  and  hard-to- understand dashboards, torrents of email, alerts, and automatically filed bugs. We all know  engineers  who  have  hundreds  of  bugs  assigned  to  them,  which  they  cannot possibly ever fix, but refuse to mark them “Won’t Fix.” Another pathology is sub‐ scribing to developer mailing lists, to be aware of every change being made to the sys‐ tem. Our love of novelty blinds us to the lack of value in information we cannot act on. Admit that most information is not actionable; be brutal with your bugs, your mail filters,  and  your  open  chat  apps.  Tell  your  team  that  it’s  OK  to  assume  anything urgent will page; any other work can be picked up after they finish tasks.  On-call and operations.    The stress of on-call drives people away from operations roles. Curiously, 24 7 shifts are not the problem. The real problem is underpopulated on-  1 Paul Atchley, “You Can’t Multitask, So Stop Trying”, Harvard Business Review. 2 George Loewsenstein, “The Psychology of Curiosity”, Psychological Bulletin.  The Primary Indicator of a Successful Team      457   call  teams,  working  long,  frequent  shifts.  The  more  time  people  spend  on  call,  the more likely they are to suffer from depression and anxiety.3 The expectation of hav‐ ing to act is more stressful than acting itself.4 It’s one thing to accept that on-call is part of a job; it’s another thing altogether to tell your five-year-old daughter that you can’t bring her to the playground. We can mitigate this stress by ensuring on-call rotations of no less than six people, with  comp  time  for  those  with  significant  expectations  around  response  times,  or personal life curtailment. Compensate teams based on time expecting work, not time doing work. Having per-shift on-call payments  or giving comp time —as opposed to including on-call compensation in the base compensation package—implies employ‐ ers value the sacrifice of their employees’ personal time. Incident management train‐ ing  or  frequent  “Wheel  of  Misfortune”  drills  can  also  reduce  stress,  by  increasing people’s  confidence.  Ensure  on-call  engineers  prioritize  finding  someone  to  fix  a problem when multiple incidents happen concurrently.5  Cognitive overload.    Operations teams support software written by much larger teams. I know a team of 65 SREs that supports software written by 3,500 software engineers. Teams faced with supporting software written in multiple languages, with different underlying  technologies  and  frameworks,  spend  a  huge  amount  of  time  trying  to understand the system and consequently have less time to improve it. To  reduce  complexity,  software  engineers  deploy  more  and  more  abstractions.  Abstractions can be like quicksand. Object relational mappers  ORMs  are a wonder‐ ful example of a tool that can make a developer’s life easy by reducing the amount of time  thinking  about  database  schemas.  By  obviating  the  need  for  developers  to understand  the  underlying  schema,  developers  no  longer  consider  how  ORM changes  impact  production  performance.  Operations  now  need  to  understand  the ORM layer and why it impacts the database. Monolithic designs are often easier to develop and extend than microservices. There can be valid business reasons to avoid duplication of sensitive or complex code, and they have simpler orchestration configuration. However, because they attract hetero‐ geneous traffic classes and costs, monolithic architectures are a nightmare for opera‐ tions teams to troubleshoot or capacity-plan. Most of us understand that onboarding new, evolving software strains an operations team. We ignore the burden of mature “stable” services. There is rarely any glamor‐ ous work to be done on such services, but the team still needs to understand it. The  3 Anne-Marie Nicol and Jackie Botterill, “On-call work and health: a review”, Environ Health. 4 J. Dettmers et. al., “Extended work availability and its relation with start-of-day mood and cortisol”. 5 Dave O’Connor, “Bad Machinery”, SREcon15 EU.  458      Chapter 27: Psychological Safety in SRE   extra care required not to impair mature services while iterating on newer ones must be accounted during time-and-effort estimation. Ensure teams document the impact of cognitive load on development velocity. It has a direct and serious impact on the reliability of the software, the morale and well- being of the operations team, and the long-term success of the organization.  Imaginary expectations.    Good operations teams take pride in their work. When there is ambiguity around expectations of a service, we will err on the side of caution and do more work than needed. Do we consider all of our services to be as important as each other? Are there some we can drop to “best effort”? Do we really need to fix all bugs logged against our team, or can we say, “Sorry, that’s not our team’s focus”? Are our Service-Level Agreements  SLAs  worded well enough that the entire team knows where their effort is best directed on any given day? Do we start our team meeting with the team’s most important topics, or do we blindly follow process? Ensure  that  there  are  no  magic  numbers  in  your  alerts  and  SLAs.  If  your  team  is being held to account for something, verify that there is a good reason that everyone agrees with and understands.  Operations teams are bad at estimating their level of psychological safety.     Finally,  I’ll  leave you with a thought: people who are good at operations are bad at recognizing psy‐ chologically  unsafe  situations.  We  consider  occasionally  stressful  on-call  “normal,” and don’t feel it getting worse until we burn out. Over-emphasizing acts of sacrifice during work normalizes sacrifice, and turns sacrifice into expectations.6 The curiosity that allows us to be creative drives us to information overload. Despite being realistic about how terrible everything is, we stay strongly optimistic that the systems, soft‐ ware, and people we work with will get better. I’ve conducted surveys of deeply troubled teams for which every response seemed to indicate everything was wonderful. I’d love to hear from people who have experience uncovering such cognitive dissonance in engineers. After all these years, I’m still sur‐ prised when I uncover it.  6 Emily Gorcenski, “The Cult Ure  of Strength”, SRECON.  The Primary Indicator of a Successful Team      459   Further Reading   Kim Scott, Radical Candor.   Kerry Patterson, Joseph Grenny, Ron McMillan, and Al Switzler, Crucial Conver‐  sations.  John Looney is a production engineer at Facebook, managing a data center provision‐ ing team. Before that, he helped built a modern SaaS-based infrastructure platform for Intercom, one of the fastest-growing technology companies in the world. Before that, he was a full-stack SRE at Google who did everything from rack design and data center automation through ads-serving, stopping at GFS, Borg, and Colossus along the way. He wrote a chapter of the SRE book on automation and is on the steering committee for USENIX SRECon.  460      Chapter 27: Psychological Safety in SRE   CHAPTER 28 SRE Cognitive Work  John Allspaw and Richard Cook, Adaptive Capacity Labs  …irony that the more advanced a control system is, so the more crucial may be the contri‐ bution of the human operator.  —Bainbridge, 1983.  Introduction The  modern  “system”  is  a  constantly  changing  melange  of  hardware  and  software embedded in a variable world. Together, the hyperdistribution, fluctuant composi‐ tion, constantly varying workload, and continuous modification of modern technol‐ ogy  assemblies  comprises  a  unique  challenge  to  those  who  design,  maintain, diagnose, and repair them. We are involved in exploring this challenge and trying to understand how people are able to keep our systems working and, in particular, how they make sense out of what is happening around them. What we find is both inspir‐ ing and worrisome. Inspiring because the studies reveal highly refined expertise in people and groups along with novel mechanisms for bringing that expertise to bear. Worrisome because the technology and organization are so often poorly configured to make this expertise effective. Together with our colleagues, we have studied people doing SRE work, the problems they face, the approaches they take, and the issues that arise in the middle. From a distance this work is often imagined as narrowly technical, even mundane. Examin‐ ing the work as done, in contrast, reveals that SRE work is often stormy and some‐ times dangerous. This chapter gives a brief overview of what we think we now know about modern technology  and  the  work  of  people  who  design,  maintain,  diagnose,  and  repair  it. There are similarities between groups doing SRE work and those in working in other high-consequence domains. We conclude with some general suggestions about ways  461   to better support the work and more specific suggestions about how to better charac‐ terize work in this demanding, conflicted environment. What Do SRE People Do? The names given to physical and virtual places where people congregate to confront incidents1   e.g.,  “war  room”  and  “command  center”   suggest  that  incidents  some‐ times  have  existential  qualities  that  merit  comparison  to  being  at  the  center  of  a cyclone.  Yet,  even  in  the  midst  of  the  cyclone,  we  find  people  acting  deliberately, thoughtfully, and  at least outwardly  calmly to cope with what is happening. These people are working—doing purposeful things that employ a specific type of expertise and doing them because that is their job. Table 28-1 shows some of the duties of SREs around the time of an incident.  Table 28-1. Some activities of SREs during incidents Activity Example Some  but not all  locations reporting outage could mean a Decoding reports and observed behaviors into meaningful networking problem or a software version problem patterns that “explain” the event and its consequences Adding more compute instances to reduce pressure on Acting to keep the technologies working connected subsystems or components Bouncing the server  Weighing available courses of action, many of which cannot be rolled back Making decisions to protect the system, often decisions involving sacrifice of one or more goals Gathering and directing resources that are needed or might be needed Anticipating and avoiding future bottlenecks in the response Providing assessments of the immediate situation and its likely and possible future trajectories  Forcing a network partition to allow recovery; killing slow- running database queries until they can be fixed in code Coordinating with SaaS vendor technical staff; traffic shedding or rerouting during a DDOS attack Planning on-call shift handoff to rest the working team  Conference call with CIO; public-facing web page updates about the incident  1 A note about terms: Use of terms like anomaly, event, incident, and accident tend to evoke strident debates about their exact meanings. They are used inconsistently in tech and elsewhere. Frustration with their vari‐ able interpretation has led some to try to give them crisp definitions. Despite these efforts, none of these terms have fixed meanings. The situation is made even more difficult when word choice has significant conse‐ quences. For example, some tech firms have formal processes for handling an incident that do not apply to an event or an anomaly.  We have witnessed extensive discussions during event response about whether that event meets the organization’s threshold criteria for an incident! Declaring an incident would bring additional resources to bear, generate auditable documentary trails, and involve substantial future work.  The situation cannot be resolved by fiat. Instead, we need to pay attention to how these terms are used in context and espe‐ cially to the consequences of the choice of term. In this chapter, we use the term incident as a pointer to a set of activities, bounded in time, that are related to an undesirable system behavior.  462      Chapter 28: SRE Cognitive Work   Like cyclones, incidents in complex systems reverberate, sometimes for long periods. Repairing  the  damage,  restoring  the  capacity,  and  making  changes  to  avoid  future incidents can take weeks or even months to complete. Understanding the deep sour‐ ces of an incident is frequently impossible while the cyclone is raging. Many organi‐ zations  have  both  formal  and  informal  methods  for  incident  review  intended  to surface and identify these deep sources. SREs have duties here as well. Engagement with postmortems and the assignment of action items are examples. The activities shown in Table 28-1 are cognitive activities. The work is taking place inside the heads of SREs and expressed by their words and actions. To understand the work that they do requires understanding the cognition of the SREs. Understand‐ ing human cognition in complex work domains is challenging but possible. Useful  techniques  have  been  developed  in  other  fields  such  as  aviation,  nuclear power,  and  other  intrinsically  hazardous  domains.  It  is  interesting  to  note  that, although the examples on the right side of Table 28-1 are specific to the network- connected information technology, the left-side activities are more generic. We find similar  activities  going  on  in  a  host  of  other  high-consequence  domains,  from  air traffic control to intensive care units  Nemeth et al., 2004  There are groups of people doing SRE-like cognitive work across a variety of domains. Why Should We Care About Practitioner Cognition? It was once thought that systems could be designed and built so that their operation, maintenance,  and  repair  could  be  reduced  to  well-defined  tasks,  capable  of  being specified in all important aspects. The notion that work can be reduced to a narrowly specified  best  way  was  popularized  by  Fredrick  Winslow  Taylor   1856–1915 .  The idea of a well-structured, carefully regulated workplace was best exemplified by the automobile assembly lines of Henry Ford  1863–1947 . Experience during the second World War, especially with airplanes, began to under‐ mine deterministic views of human performance. Although some assembly processes might  be  completely  specified,  operational  situations  brought  humans  into  contact with diverse situations and settings that demanded expertise that went beyond writ‐ ten procedure following. In retrospect it is clear that there were very few situations where work could be reduced to a set of unambiguous rules. The expertise needed to make things work, to fix them when they are broken, and to manage the consequences of failure is often arcane, subtle, and tacit. Understanding where this expertise comes from, how it is deployed, and how it is sustained in com‐ munities of practice  Lave and Wenger, 1991  is inherently the study of cognition in the wild  Hutchins, 1995 . As Bainbridge  1986  observed, one of the ironies of auto‐ mation is that increasing its sophistication and power does not reduce the need for expertise but instead increases it. Building a thorough, grounded understanding of  Why Should We Care About Practitioner Cognition?      463   how practitioner cognition works is critical to the operation of modern systems and will only become more so as the complexity and reach of those systems expand. Critical Decisions Made Under Uncertainty and Time Pressure Cannot Be Scripted The modern view of human engagement with complex systems arose from a conflu‐ ence of ideas and research threads in the 1970s and broke into the open following the nuclear accident at Three Mile Island  TMI  on March 28, 1979. Prior to TMI, it was widely believed that nuclear power plants had invested so heavily in safety that major accidents were virtually impossible. Safety was encoded in design, the automated sys‐ tems, and the procedures. Plant operations were regarded as simply a matter of fol‐ lowing  those  procedures,  which  were  thought  to  be  objective  and  completely specified. TMI showed that this idea was fundamentally flawed. Complexity and cir‐ cumstance could and would combine to create novel situations for which procedures were  underspecified  and  inadequate.  These  challenging  situations  would  require operators to build understandings of the specific situation, to weigh the risks of dif‐ ferent approaches, to track the evolving condition of the plant and revise their under‐ standings, and redirect attention and effort. The  15-year  period  after  TMI  was  one  of  rich  and  rancorous  debate  about  safety complexity, automation, and human performance. It spawned multiple, productive research threads that continue to bear fruit today. And the idea of “error” as a dis‐ crete type of human performance that could be counted, channeled, and trained away simply collapsed. The approaches we use to study SREs at work were developed in large part because of TMI. What  TMI  made  clear  is  that  engineers,  operators,  and  others  involved  in  the moment-by-moment  workings  of  complex  systems  need  to  have  the  means  and opportunity  to  understand  how  those  systems  are  working  and  to  predict  how actions  taken  will  play  out—especially  when  the  systems  are  breaking  or  broken. Achieving this depends on being able to understand their cognition in the real world of work. Human Performance in Modern Complex Systems: The Main Themes There is an extensive literature about human performance in complex systems, the analysis of such systems, and the sources of successful and unsuccessful operations of these  systems.  There  are  several  main  themes  that  come  from  this  work   see Table 28-2 . Some of these have obvious application to SRE work, but even the most promising among them require careful preparation if they are to be made truly use‐ ful.  464      Chapter 28: SRE Cognitive Work   Table 28-2. Themes in human performance in complex systems Theme Critical decision  Examples    How does combining uncertainty, high-stakes, and time pressure influence decisions?   How do decision makers evaluate alternatives?   What makes work hard?  Sense making  Coordination across multiple agents  Resilience    How are anomalies recognized?   How does troubleshooting work?   How do experts keep track of evolving situations?    How does cooperation happen?   How do people and machines interact?   How are the costs of coordination managed?    What is needed for adaptation to novel situations?   What trade-offs are available?   What degrees of freedom are worth sustaining?  Although there are general results associated with each theme, these comprise mostly a guide for the further examination of specific work areas. Each work domain is dif‐ ferent in ways that matter. An approach that is useful in one domain might be useful in another but will always need careful tailoring to the specifics of the new domain if it  is  to  be  helpful.  Simply  mimicking  across  domains  seldom  produces  the  desired result. Instead, the research is useful in framing explorations of a new domain and interpret‐ ing the results of empirical studies in that domain. The situation is somewhat analo‐ gous to the way that “patterns” are useful in software. Patterns are not solutions but frames that allow solutions to be developed. Observations on SRE Cognitive Work Around Incidents Incidents are complex system failures  Cook, 1998; Cook, 2010 . They result from the combination of multiple, individually innocuous flaws, each individually incapable of producing an overt failure but, in combination, sufficient to do so. But the theory of complex system failure is too narrow to encompass SRE work, which includes man‐ aging the descent into failure, the repair and reassembly of a  sometimes partly  func‐ tional  system,  channeling  the  immediate  aftermath  response  to  limit  collateral damage, and constructing meaningful, useful descriptions of what has just happened.  Observations on SRE Cognitive Work Around Incidents      465   Every Incident Could Have Been Worse Even when an incident seems to have begun abruptly, a review of its evolution will show many forks. Even when the outcome is terrible, a cursory review will show it might have been worse. Even if they cannot forestall a breakdown, from their vantage point inside the center of the cyclone, SREs might have opportunities to shift the tra‐ jectory of the failure so as to preserve useful elements or cushion the blow. Especially  for  cascading  failures,  where  component  failures  multiply  to  expand  the area of damage, it is sometimes possible to, for example, disconnect portions of the distributed system to avoid extending the failure. Simply warning those  people and machines  using the system that it is failing can give those agents time to achieve safe shutdown in anticipation of a loss of service. Guiding a failing system toward a less painful outcome depends on the expertise of SREs  and  also  on  the  design  and  implementation  of  the  technology.  These  events often happen quickly and the window of opportunity might be small, putting addi‐ tional pressure on those trying to limit the damage. Only the most public, high-profile incidents are reviewed in sufficient depth to reveal how the intra-incident choices made things better or worse. Two historical examples are relevant, both involving the nuclear power industry.  During the accident at Three Mile Island, the operators became concerned that the extra cooling water flowing into the reactor would fill the pressurizer and cause the reactor to “go solid”—a catastrophic condition that would result in loss of control. This led them to reduce the water flow, which exposed the reactor core and stopped cooling it, resulting in a meltdown of the core.  TMI  Fukushima  Reactor  cooling  water  is  highly  purified  to  prevent  corrosion  of  the  reactor  piping and parts. During the accident at Fukushima, the power failure stopped cooling water flow over the reactors. An option available to the operators was to pump unpurified water into the reactor, making the reactor forever unusable but avoiding core dam‐ age. Operators for one reactor took this option early, bring the reactor to a safe shut‐ down.  Operators  of  another  reactor  continued  efforts  to  restart  the  pure  water cooling but were unsuccessful. That reactor core melted, resulting in a major leak of radiation to the environment.  466      Chapter 28: SRE Cognitive Work   Sacrifice Decisions Take Place Under Uncertainty Rarely do incidents arise with clear-cut boundaries, obvious solution pathways, and predictable  repair  times.  In  most  cases—and  in  all  interesting  cases!—the  primary feature of an incident is uncertainty. The normal operating system is complex; the breaking or broken one is doubly so. Systemic complexity and uncertainty are closely linked; as complexity increases, uncertainty increases. SREs frequently face situations in which there are only bad alternatives. Losses have already been sustained, more losses are accumulating, and cascade to enormous or even  catastrophic  loss  is  a  real  possibility.  Although  there  might  be  temporizing measures that can reduce or defer consequences  “let’s spin up another instance and see if that helps” , it’s common to confront rather stark, painful sacrifice choices. A sacrifice decision occurs when the decision is made to accept one loss in order to avoid  another,  more  significant  one.  Typically,  sacrifice  decisions  accept  a  well- defined  loss  in  order  to  reduce  the  possibility  of  a  more  severe  loss.  In  almost  all cases, the loss that is accepted is better characterized than the loss that is being avoi‐ ded  Woods, 2006 . Post-incident reviews are good for clarifying sacrifice decisions, but not as good at capturing  the  uncertainties,  pressures,  and  conflicting  data  that  commonly  swirl around such decisions. Ironically, when a sacrifice decision is successful—that is, the bigger, more threatening loss is avoided—there is a strong tendency to be critical of the sacrifice decision. After it is avoided, the bigger loss seems less likely and the deci‐ sion to accept the sacrifice seems ill-considered. Symmetrically, after a catastrophic failure,  the  inability  to  perceive  an  available  sacrifice  decision  or  unwillingness  to make  it  can  evoke  similar  criticism.  Let’s  take  a  look  at  two  more  examples  that demonstrate choosing the sacrifice.  New York Stock Exchange Outage  On  July  8,  2015,  the  New  York  Stock  Exchange   NYSE   stopped  trading  for  three hours  because  automated  stock  quotation  processes  failed.  Although  most  of  the automated exchange was working at the time of the shutdown, NYSE management decided to halt all trading. “It’s not a good day, and I don’t feel good for our custom‐ ers  who  are  having  to  deal  with  the  fallout,”  NYSE  President  Thomas  Farley  said, according to Reuters. NYSE later paid a penalty of $14 million to the Securities and Exchange Commission for this and “similar computer-related problems.”  Observations on SRE Cognitive Work Around Incidents      467   Knight Capital Collapse  On  August  1,  2012,  the  trading  firm  Knight  Capital  lost  $440  million  when  newly installed software interacted with testing code already in the system. Initial sudden losses were recognized and concern was raised about the system, but it was allowed to continue running until it had accumulated crippling losses. The company was sold a few weeks later, effectively going out of business.  Repairs to Functional Systems SRE work in failed and failing systems produces a narrow but deep understanding of how the sources of the failure are related. This understanding is especially important as plans to repair the system are developed. Broken but still-functional systems have properties  different  than  the  “nominally  working”  system.  New  vulnerabilities  and dependencies arise, often with implications for approaches to repair. The complicated problem of rollback illustrates this clearly. Large, highly regulated systems  with  tight  change  controls   e.g.,  financial  processors   often  implement  a group of changes together. When one of these changes breaks some part of the sys‐ tem, there are sometimes debates about the wisdom of reverting that change package. Because  the  entire  change  package  was  subjected  to  testing,  the  consequence  of reverting a single change puts the system in an untested state. Some organizations require  that  any  rollback  include  all  the  changes  in  the  package.  What  if  some changes in a package are urgently needed? More discussion and debate. Untangling the skein of changes and effects in large systems can be challenging, espe‐ cially  when  an  incident  has  just  demonstrated  how  flawed  and  incomplete  under‐ standings of the system are. We have observed quite vigorous discussions about how the system works [!] and about how to proceed with repairs. Because  these  issues  can  have  important  and  even  existential  consequences,  SREs rarely decide them alone. Unlike the situations described in the sections “Every Inci‐ dent  Could  Have  Been  Worse”  on  page  466  and  “Sacrifice  Decisions  Take  Place Under Uncertainty” on page 467 earlier in the chapter, repairs have a distinctly lower tempo and broader engagement of stakeholders. SREs are often called upon to exam‐ ine proposed repair schemes, to critique them, and to explain the workings of the sys‐ tem  to  stakeholders  with  authority  but  who  lack  the  deep  technical  knowledge  of SREs. We note that the concept of rollback is becoming difficult to sustain. As distributed systems include more external services and elaborate patterns of usage, the possibility of  reverting  to  a  “known  state”  becomes  less  feasible.  Modern  distributed  system design  emphasizes  the  need  to  build  systems  that  do  not  depend  on  maintaining  468      Chapter 28: SRE Cognitive Work   state. The result is that we now have systems that lack unique state. In such a world, reverting a software change can make the system take on a more familiar appearance, but it might not restore the world to the way it once was. Special Knowledge About Complex Systems The  situation  facing  SREs  is  seldom  simple.  The  fault-tolerance  mechanisms  built into the design of distributed systems and related automation handle most problems that arise. Because of this, incidents represent situations that fall outside of the “most problems” boundary. Reasoning about cause and effect here is often challenging. For example, simply observing that a process is failing does not necessarily mean that fixing that process will resolve the incident. Processes might simply be unable to han‐ dle the extreme conditions produced by the failure of other processes. In other cases, actions intended to mitigate or break further propagation of issues can return with results  that  do  more  to  shed  new  light  on  the  issues  than  containing  them  as intended. Terms like “back pressure”2 strongly suggest that SREs have special, condi‐ tional knowledge about system behaviors. Managing the Costs of Coordination SREs  seldom  work  alone.  Operating  and  supporting  large  distributed  systems involves many people and activities that have diverse and shifting connections. The course of an incident might engage a handful or hundreds of people acting as infor‐ mation contributors, troubleshooters, consequence limiters, and liaisons  internally, with customers, with regulators, etc. . Garnering and directing resources, coordinat‐ ing  them,  tracking  the  progression  of  the  incident,  and  coping  with  the consequences—often while the genesis and further progression of the event remain uncertain—are  themselves  demanding  tasks.  Many  organizations  have  sought  to “manage” the dense, noisy, intra-incident situation with varying degrees of success. Two popular methods of managing the costs of coordination are the use of classifica‐ tion schemes to parcel out resources and the assignment of formal roles for personnel during the incident.  Classification schemes Many  incidents  have  only  minor  impact  on  customers  or  business  process.  Fewer incidents have major impact. Addressing incidents can be expensive and disruptive.  2 Originally, the term backpressure referred to network traffic faults derived from switch failures. These mani‐ fested as growing message queues being accumulated in upstream components. We now use the term more broadly to refer to any disruption of interprocess communication that results in queue growth in the upstream process. Because displays of queue size are ubiquitous, increasing queue size is a conventional signal that a downstream process is failing.  Observations on SRE Cognitive Work Around Incidents      469   By classifying incidents, organizations seek to manage resource consumption: more important  incidents  get  more  resources,  while  less  important  ones  get  fewer.  It  is sometimes quite difficult, however, to understand the significance of an incident in progress. Severe incidents might initially manifest as mild disturbances. Cascades can propel a minor event into a major one. Ordinal scales  e.g., a 1 to 5 severity scale  are difficult  to  apply,  especially  early  in  the  event  evolution  when  resources  are  being gathered and assigned. A classification scheme based on immediate customer impact might not reflect the importance or difficulty posed by an event.  Formal role assignments Some organizations now use formal assignment of intra-incident roles. Examples are “incident commander,” “communications manager,” and so on. These expedients are intended  to  delineate  authority  and  responsibility  within  the  group  addressing  an incident and to allow for smooth coordination across the resources being brought to bear. We have seen this approach work well and also not so well. Experienced groups work well while inexperienced ones struggle, and we note that the success of leaders depends critically on the composition of the work group. One important function of the  leader  is  to  be  accountable  for  critical  decisions  that  have  operational  conse‐ quences. These methods of managing the costs of coordination have appeared because inci‐ dent management is challenging. Very little research has been done to validate their use. The actual effects of these methods is an area ripe for study. SREs Are Cognitive Agents Working in a Joint Cognitive System SREs’  work  is  never  accomplished  without  interaction  with  other  people  and machines. All these actors—human and machine—function as cognitive agents. It is impossible to adequately describe one actor’s work in isolation. Their interactions are based on each other’s capability, intention, and understanding of the situation. They coordinate, cooperate, and reason about others intentions. Together they form a joint cognitive system  JCS   Hollnagel and Woods, 2006 . Much of SRE work during incidents requires reasoning about automation. What is the automation doing? What will it do next? How does it understand the situation? What  will  happen  if  we  tell  it  to  do  this  or  that?  Typically,  automation  is  strong, silent, and hard to direct  Woods, 1997 . Efforts to make automation a more useful cognitive “team player” have focused mainly on safety critical systems, such as air‐ plane cockpits  e.g., Billings, 1996  or medical devices  e.g., Nemeth et al., 2005 . The growing importance of internet-facing business systems has raised the stakes around automation in this realm. SRE stories are filled with examples of automation-related troubles. Anyone who has deployed to production when they meant to deploy to test will have experienced this  470      Chapter 28: SRE Cognitive Work   firsthand! Ironically, people are so used both to balky, awkward automation and to making up for it that they regard it as normal.3 When automation is a poor team player, human cognitive agents must make up the deficiency by doing extra cognitive work. When this work is required at already busy times, the automation is “clumsy”  Cook et al., 1991 . A lot of automation presently deployed for use during incidents has this quality. Although we know how to make automation a team player  Klein et al., 2004 , the effort required and the rapid change of technology make it difficult to accomplish for all but the most critical systems. It is clear, however, that the improving overall per‐ formance of a JCS depends on making all the cognitive agents better team players  Klein et al., 2004 . The Calibration Problem SRE work depends on knowledge of how things work, how they don’t work, what interventions are available, and the consequences  likely and possible  of these inter‐ ventions. This will include general knowledge  e.g., about a particular operating sys‐ tem  feature ,  specific  knowledge   e.g.,  how  this  particular  system’s  Redis  is configured ,  and  quite  diverse  knowledge   e.g.,  the  code  “freeze”  scheduled  for tomorrow will encourage people to deploy in advance and so today might be more challenging than usual . How  do  people  know  whether  this  knowledge  is  current  or  stale?  What  triggers  a reassessment? In a fast-changing, complex world, no person can have a complete and detailed understanding. How can someone know that their current view is adequate for  the  immediate  work  they’re  performing?  What  investments  in  improving  that view are worthwhile? Mental Models Researchers  often  use  the  term  mental  model  to  describe  the  collected  knowledge about how a system works  Gentner and Stevens, 1983; for a discussion, see Cook, 2018 .  Although  general  knowledge  can  age  gracefully,  rapid  change  ensures  more specific knowledge will quickly become stale. Engineers know this and update their mental models. They compare their understanding of how components, subsystems,  3 The suit joke illustrates this well. A man goes to buy a suit. Trying it on, he notices that the arms are too long.  “No problem,” says the salesman, “just bend them at the elbow and hold them out in front of you.” He does but then notices that the jacket is long in back. “Just bend over a bit,” the salesman says. He does but now notices that the right pant leg is too short while the left one is too long. “Just rotate your hips a little,” the salesman says. “There, now you look perfect.” So, the man buys the suit and walks down the street past two old ladies sitting on a bench. “Oh,” says one, “look at that poor man, isn’t it sad that he is so twisted up like that.” “Yes, it is sad” replies the other, “but doesn’t his suit fit well!”  The Calibration Problem      471   networks,  and  applications  connect  and  interact,  and  so  on,  with  evidence  from  a multitude of sources. But  this  raises  an  important  problem:  how  do  engineers  know  when  their  mental models  are  stale  in  ways  that  matter?  Modern  systems  change  continuously.  New code  is  deployed,  new  users  stress  the  system  in  new  ways,  outside  services  stop working as expected, subtle factors combine to generate new interactions. It is impos‐ sible for any individual to keep up with all these changes—indeed, the agile paradigm reflects  this.  Given  that  so  much  change  is  happening  and  so  much  of  it  is  taking place out of view, when should an engineer decide to spend time and effort to update their mental model and how should an engineer decide where to focus that attention? This is the calibration problem. Woods’ Theorem states: “As the complexity of a system increases, the accuracy of any single agent’s own model of that system decreases rapidly.” No amount of effort will be sufficient to make a mental model accurate and precise for the whole system. Even a partial system model that is “good enough” at one moment becomes stale as the system changes. The best that an individual can do is to attempt to maintain a mental model  that  is  sufficiently  accurate  and  sufficiently  precise  for  the  needs  of  the moment. Each  person  has  a  unique  mental  model,  often  quite  different  from  the  models  of others. This is both an asset and a problem. It is an asset because the many different mental models present in an organization provide coverage of more aspects of the system than a single model could possibly give. It is a problem because the models are likely, where they overlap, to be incongruent. It is quite possible for individual mental models to disagree, which, in turn, creates a need to identify and resolve discrepan‐ cies across mental models. This situation is shown schematically in Figure 28-1  see Woods 2018, section 2.3 . Significantly, all the interaction with computing technology takes place through dis‐ play  artifacts.  Together  these  create  a  “line  of  representation”  that  provides  access and  shows  images  of  that  technology.  We  cannot  directly  perceive  the  technology itself—the computers, programs, APIs, networks, and so on. Instead, the technology is  represented  by  text  and  images  displayed  on  screens.  People  take  action  on  the underlying technology using keyboards, mice, and so forth and assess their impact via the representations. They interpret the responses via their mental models, infer‐ ring the composition and function of the underlying technology from these interac‐ tions. Broadly, people work “above the line” of representation, while the technology that they work on is “below the line.”  472      Chapter 28: SRE Cognitive Work      y b 6 1 0 2         t h g i r y p o c     .  n o i t a t n e s e r p e r   f o   e n i l   e h t       w o l e b d n a   e v o b a   s t n e m e l e   g n i d u l c n i    m e t s y s   a   f o   g n i w a r d   c i t a m e h c S   .  1 - 8 2   e r u g i F    n o i s s i  m r e p   y b d e s u       ,     k o o C d r a h c i R  The Calibration Problem      473   Incidents Trigger Individual Recalibration An incident begins when an individual encounters anomalous behavior and begins exploring its sources and consequences. Incidents are first and foremost anomalies. The difference between what is expected and what is observed is the driver for what happens next. This difference is a signal that the observer’s mental model is out of calibration with this part of the system. That mental model can be either coarse or highly precise, but the anomaly is evidence that it is not accurate. Incident response is, in a large sense, a process of recalibrating the observer’s mental model for this part of the system. Searching for a “cause” of the anomaly is driven by the mental model and serves to update it. Note particularly the emphasis on this part of the system. Complexity and change will foil any attempt at broad mental model updating. An observer’s recalibration of their mental model is bounded by the inci‐ dent and focused by the process of inquiry. We  hypothesize  that  experts  have  fine-grained,  accurate  mental  models  of  specific parts of the system that they understand are critical or likely to fail. They can exploit these  mental  models  to  solve  problems  quickly  and,  in  solving  them,  update  their mental models in ways that are likely to serve their future needs. Even though novices and  less  expert  practitioners  begin  with  coarser,  less  accurate  mental  models,  the experience of an incident can allow them to enhance their mental models of this part of the system. Incidents  also  serve  as  platforms  for  developing  expertise.  Significant  incident responses are rarely carried out by solo practitioners. Involvement of novices in inci‐ dent response appears to promote development of expertise. Observing exploration of the anomaly provides novices with opportunities to refine their mental models in tandem with the expert. Incident handling has sharp focus and takes on the complex‐ ity  of  the  real  world  in  ways  that  are  seldom  matched  by  more  formal  training activities. Incidents Are Opportunities for Collective Recalibration The problem facing the individual practitioner has a parallel in the larger organiza‐ tion: how can an organization know when its collective understanding of a complex system needs updating? Given the limited resources available, how should an organi‐ zation devote time and effort to maintaining people’s appreciation for the way that the system really works, what its vulnerabilities are, and what to expect in the future? We can use incidents to focus attention here as well. Incidents are messages from the underlying  system  about  where  trouble  lies.  Examining  these  events  more  broadly can provide useful insights into how the system really works, allowing the individuals involved to update their mental models. Incidents point to parts of the system where  474      Chapter 28: SRE Cognitive Work   trouble has occurred. They are the most relevant indicators of what part of the system is this part. The idea that incidents might be broadly useful to the organization is not new. Acci‐ dent research has long emphasized the value of incidents as pointers  cf. Woods et al., 2010  and quality theory encourages the use of incidents and windows into processes and problems  cf. Deming, 1982 . The use of postmortems  Allspaw, 2012  can pro‐ vide insights into systems and their operations that would not be otherwise available. The  key  idea  here  is  that  complexity  and  change  make  broad,  generic  efforts  at “understanding” existing systems relatively inefficient. In contrast, incidents point to specific places where mental models of the system are in need of recalibration. Inci‐ dents are timely, relevant information about where effort and attention should go. We acknowledge that this view is diametrically opposed to the approaches most often used to manage incidents. In many organizations, incidents are collected, reduced to a  database  entry,  and  quickly  forgotten.  Management  “by  the  numbers”  demands swift resolution of incidents and rewards their reduction to a set of categories. This has the effect of ignoring  or at least diminishing  the value of incidents as starting points for deeper and more thorough investigation. It is ironic that such expensively purchased data is so quickly discarded. What Are the Implications of All This? Incidents are remarkably frequent. For many systems the organizationally acknowl‐ edged incident rate is one or two per day. We know of some places where the rate is much higher. In many firms, coping with incidents is a constant struggle that con‐ sumes resources and attention. It is clear that this will continue and that, as the com‐ plexity and significance of systems increase, the consequences of incidents are likely to  increase.  Because  the  systems  and  environment  are  continuously  changing,  the loci, character, and impact of incidents will change. However, it’s equally true that incidents are valuable sources of information that can—under the right conditions and with effort—be harvested. Incidents Will Continue This  situation  is  not  going  to  get  better.  Instead  it  is  likely  that  the  problem  will become even more difficult. The stakes for system operations are rising as more busi‐ ness  functions  are  embedded  into  systems  and  daily  business  operations  become more  dependent  on  them.  The  complexity  of  these  systems  continues  to  grow. Attempts to tame this complexity have sometimes shifted the locus of human opera‐ tor attention but have not and will not eliminate the need for operators to make sense of what is happening.  What Are the Implications of All This?      475   As in other industries, technological progress appears to simultaneously reduce the number of major incidents while also increasing their consequences. The result is that major  events  occur  less  frequently  but,  when  they  do  happen,  have  greater  effects than in the past. The connectedness and responsiveness of modern systems makes disturbance propagation faster and more widespread than in the past. We  note  that  much  of  the  industry  dialog  around  development  of  large  systems implicitly assumes that future systems will be, practically speaking, “incident free.” This is a dangerous technofantasy, an idée fixe. The history of systems of all sorts, and especially that of information technology, does not support this idea. Incidents will continue to occur, to threaten, and to demand attention. Failure to build systems— technical  and  organizational—with  features  suitable  to  incident  management  is malpractice. Incidents Will Impose Costs The cost of incidents is difficult to measure but certainly may be large. Back-of-the- envelope calculations suggest that a day’s outage for a major airline can generate los‐ ses in excess of $200 million. It is practically impossible to calculate cost of existential events such as the Knight Capital collapse in 2012. The  direct  costs  of  incidents  include  their  immediate  effects  on  revenue,  costs  of response, and so on. Table 28-3 lists some of these.  Table 28-3. Some direct costs of incidents Incident Downtime  Costs    Revenue loss   Service penalty payments   Restoration effort    Staff salary for incident response   Opportunity cost of diverting staff attention to incidents   Cost of implementing post-incident action items  Response  Regulatory  Organizational overhead    Maintaining incident tracking systems    Compliance reporting   Financial penalties   Regulatory interference in dev ops  476      Chapter 28: SRE Cognitive Work   Although  these  costs  are  certainly  significant,  the  indirect  costs  of  incidents  are equally  disturbing.  Table  28-4  shows  some  of  these  costs.  Reputational  injury,  dis‐ traction, brittleness, and what we call “developmental drag” are not easily quantified. Even so, these indirect costs of incidents have the potential to be crippling. Again, there is an ironic quality here: the importance of the existing code base to successful operations is so great that modifications are shunned for fear of damaging that base. Any incident arising from or related to that code base reinforces that fear. Over time, that base becomes brittle and the number of people left in the organization willing to tackle it decreases.  Table 28-4. Some indirect costs of incidents Incident Reputation injury  Costs    Customers lose confidence in the product and firm   Other products and firms gain competitive advantage   Downward pressure on pricing  Organizational distraction  Increasing brittleness  Developmental drag    Management takes more active role in addressing problems and “handling” events   Incidents feed into  and can shape  internal political agendas    Fear of triggering hidden vulnerabilities or introducing new ones creates reluctance to repair   migrate refactor legacy code or infrastructure    More uncertainty about cost of adapting to new situations   Efforts to control change to prevent failures  CABs, code freezes, etc.  undermine agility   Technical staff lose confidence in management, develop covert work practices, or leave the firm  Of particular importance is the developmental drag that results from organizations seeking to manage vulnerability. Change “review” or “advisory” boards are an organi‐ zational  reflection  of  the  desire  to  avoid  future  failures.  Although  this  mechanism might  indeed  forestall  some  ill-conceived  changes,  the  burden  it  imposes  can  be great. Such boards consume valuable resources, generate delays in addressing prob‐ lems, lead to release clumping, and can even produce covert work systems  Hirsch‐ horn, 1998; p. 61 . Even though these costs are real, they are difficult to quantify and can escape from management view. From  a  management  perspective,  incidents  are  best  regarded  as  unplanned  invest‐ ments in system performance. The challenge to managers is to find ways to extract the greatest return on investment  ROI  from incidents.  What Are the Implications of All This?      477   Incident Patterns Will Change The complexity of systems and diversity of the surrounding environment continue to increase. New connections will be made and old ones will be severed. The pattern of incidents will necessarily change, as well; like the financial disclaimer says: “Past per‐ formance is no guarantee of future results.” Although much is invested in incident tracking systems, there is little evidence that these methods predict the locus or nature of future failures. For larger systems the associated administrative bureaucracy produces little of value. Instead, it appears to function largely as a kind of sea anchor—a drag from behind that the captain hopes will keep the organizational bow pointed in a safe direction. Incidents Point to Specific Calibration Problems and Locations The calibration problem  see “The Calibration Problem” on page 471  is a fundamen‐ tal  challenge.  Successfully  troubleshooting,  repairing,  and  modifying  a  system requires an accurate model of that system. Complexity and change ensure that any system model will become stale. No single agent  human or machine  can maintain an accurate model of such a system. Instead, the need to troubleshoot, repair, and modify a system demands continuous effort to improve the accuracy of the agent’s model. But the sheer scale of modern systems makes any generic effort at maintaining cali‐ bration an exercise in futility. No one can possibly read and digest all of the code, all of the manuals, all of the protocols that comprise a modern system. Instead, we need to focus the limited resources available for recalibration on what is most relevant. Incidents are salient guides to where recalibration is needed. Incidents are the only unambiguous  information  available  about  a  lack  of  calibration.  They  are  pointers  albeit untyped pointers  to areas of the system where recalibration is needed  Cook, 2017 . Incidents are messages sent from the underlying system about our miscalibra‐ tion. They are the most efficient means of knowing where to direct efforts at restoring calibration. What Should Happen Next? Harvesting  the  value  of  incidents  is  not  easy,  but  it  is  possible.  Some  promising approaches  to  gleaning  and  sharing  insights  from  incidents  have  been  identified. Much of the value of incidents comes from comparison and contrast across incidents. The many difficulties of incident management and post-incident reconstruction and analysis are ripe for innovative approaches. The value of incidents as efficient pointers to the need for calibration strongly suggests that learning how to learn from incidents will be crucial to developing the adaptive capacity to weather new forms of challenge and to grasp new opportunities.  478      Chapter 28: SRE Cognitive Work   Build a Corpus of Cases Through  efforts  such  as  the  SNAFU  Catcher  consortium,  there  is  a  clear  need  to identify and build a corpus of cases from modern software organizations. We have experience  in  doing  this  in  other  domains,  such  as  healthcare   e.g.,  Cook,  Woods, MacDonald, 1991 . Although it requires effort and expertise, it’s the basis for explor‐ ing the settings and challenges of software service worlds. There are, of course, many existing case collections. Indeed, every enterprise has large datasets  recording  many  incidents  that  have  occurred.  But  these  collections  are mostly mute about the human performance, problem representations, and cognitive processes that took place. The descriptions are, in terms of Figure 28-1, about “below the line.” The corpus of cases that is needed now explicitly and in detail characterize “above the line” processes. A corpus of cases will allow us to see how different types of “incidents” unfold and are managed from multiple perspectives, and allow for asking questions that probe for  the  similarities  and  differences  between  events,  organizations,  type  of  business, and so on. Examples of questions include the following:    How does the flow of attention move and or migrate as an incident evolves?   How do different teams of responders describe their vantage point and under‐  stand one another?    What do people actually do during the response to incidents and anomalies that they don’t make explicit? What tricks or shortcuts do they use that others aren’t aware of? What tools are useful? Which ones are distracting?    What influence does time and consequence pressure have on the management of a given incident? How do structured responses  e.g., incident handling training, assigning authority to specific individuals  work in practice?    How do teams perceive what parts of their application or systems are “risky” to  touch? How did this view arise? How accurate is it?  A corpus of cases does not provide answers. Instead, it helps identify lines of inquiry likely to prove useful. Focus on Making Automation a Team Player in SRE Work The SRE work world is a JCS, which is composed of cognitive agents, mostly people but, increasingly, automation as well. Making automation a team player in a JCS is challenging  Klein G. et al., 2004 . There is already a lot of automation in systems, and SREs  and others  make sense of system behaviors by reasoning about how it works and fails. Load balancers, failovers, rate limiters, alerters, and a host of other automatic processes are essential to making  What Should Happen Next?      479   systems work. Some incidents are simply unexpected interactions of these processes. Such  incidents  can  be  difficult  to  understand,  especially  when  the  individual  pro‐ cesses are “doing” their jobs correctly. More recently a new layer has appeared: automation intended to keep track of and direct the automation. When difficulty managing automation spurs the development of new automation, systems become even more difficult for SREs to understand and direct. The situation has an analog in medicine: the widespread use of antibiotics in hospitals promoted development of resistant microorganisms that produce nosoco‐ mial—that  is,  “hospital  acquired”—infections  that,  in  turn,  led  to  development  of even more potent antibiotics. The term nosocomial automation is sometimes used to describe automation intended to handle the problems caused by automation  Woods and Cook, 1991 . Ironically, although intended to relieve SREs of work, automation adds to systems’ complexity  and  can  easily  make  that  work  even  more  difficult   Cook  and  Woods, 1996 . This is likely to become an even more challenging issue for SREs as layers of automation  e.g., artificial intelligence  are added to the operational elements of our systems. The need for team play has never been more acute. Address the Calibration Problem Software engineering teams  SREs as well as others  continually build and calibrate their models of how systems are connected and behave. The need to keep these mod‐ els up to date extends across the entire organization and is especially acute for SREs. The calibration problem is critical to enterprise success. Support for SRE calibration would  promote  learning  about  narrow  technical  details  and  also  broader  relation‐ ships that nourish and constrain work above the line. Concentrating  on  SRE  calibration  alone  is  unlikely  to  produce  enterprise  success. SRE work takes place inside the framework of the enterprise and depends on resour‐ ces, guidance, and goals the SRE workforce does not control. The problems confront‐ ing  SREs  are  intimately  connected  to  development,  infrastructure,  marketing,  and customer support activities. We have argued that incidents point to where calibration is needed. This need is not, however, restricted to the SRE workforce. Incidents are the most relevant empirical data  about  how  the  system  works,  what  it  is  capable  of,  and  what  is  likely  in  the future. As technical systems become more complex, the need for calibration becomes more acute. Experience with incidents and their meanings needs to be shared across the organization. Building environments that support the development of coherent, relevant  descriptions  of  incidents  and  their  meanings  is  essential  if  the  “above  the line” part of the system is to make progress on the “below the line” part.  480      Chapter 28: SRE Cognitive Work   Especially  when  an  incident  has  significant  bottom-line  consequences,  extracting meaning  from  it  is  likely  to  take  place  in  a  charged  and  even  acrimonious  atmos‐ phere. Efforts to “spin” interpretations of the event become attractive, especially in highly  bureaucratic  organizations.  Internal  political  strife  can  lead  actors  to  shape incident stories to their purposes. In such settings, the need for solid, carefully con‐ ducted  and  presented  incident  investigations  takes  on  even  greater  organizational significance. There are opportunities for significant improvements in understanding how people work at the sharp end of practice and how to better support that work, both techni‐ cally and organizationally. Our studies indicate that the industry is more attuned to the need for calibration than in the past and that experiments that combine organiza‐ tional and technical components are underway in many firms. The presence of these experiments is encouraging. Capitalizing on these experiments to achieve velocity is hobbled by difficulties in sharing results, the “spin” that seems to follow every effort, and the paucity of tools for extracting the lessons learned from them. More efforts like the SNAFU Catchers consortium will be welcome. This  chapter  was  not  intended  to  slay  any  particular  dragons  that  the  SRE  world faces; it only sketches them and points to where they sleep. What Can You Do? “Incident” is a neutral-sounding word that suggests detachment and a dispassionate analytical stance. But a real “incident” is often accompanied by strong emotion and its  aftermath  can  be  devastating  for  individuals,  organizations,  or  whole  firms.  As companies become more dependent on technology for their success, disruptions of that  technology  pose  ever-greater  threats.  Meeting  this  challenge  requires  the capacity  to  not  just  survive  incidents  but  to  exploit  them  as  resources.  Doing  this requires some practice, some resources, some management buy-in, and some cour‐ age, but it is well within the reach of most organizations. To start, here are four suggested activities that, experience shows, are likely to pro‐ vide the early successes that will encourage further work: Build your own internal resources to do incident analysis  Expand on the current SRE practice around postmortems to develop this deeper practice  of  inquiry  about  cognitive  performance.  Recognize  those  who  do  this work as resources and encourage them to engage with others outside the organi‐ zation.  Select a few incidents for closer and deeper analysis  Even a few incidents examined closely can provide great value. Close examina‐ tion and deep analysis is often easier with medium-value incidents; high-value incidents evoke managerial and regulatory attention that can make exploratory  What Can You Do?      481   study difficult or even hazardous. Start small. Demonstrate what a more thor‐ ough analysis looks like and what insights you can generate by it. Cases that con‐ tain elements of surprise, uncertainty, or ambiguity are likely to be particularly rewarding. Try to avoid “filing off the rough edges” in order to build a single, coherent account. Instead, emphasize and capture individual perspectives of res‐ ponders  and  participants.  Look  for  a  forum  for  discussion  with  others  in  the organization and ask the responders to participate in any discussion.  Build or adjust tooling to capture data streams from incidents  Many  data  streams  are  already  available  to  aid  post-incident  reconstruction  of the  intra-incident  period.  Few  of  these  are  well  configured  for  post-incident work. Even a small amount of effort directed toward expanding, collating, and capturing these data streams is likely to be useful. Chat transcripts, video calls, database query logs, and other sources can all be useful if the effort needed to wrangle them into a coherent account is not too large. A bigger challenge is to record  actions  people  took  that  were  not  explicit  during  the  event,  especially those that turned out to be unhelpful. It can be especially valuable to identify and characterize the  ultimately  useless “trips down the rabbit hole.”  Make company-wide postmortem sessions regular events  Incidents are unplanned investments; gaining the full value of these investments should be a business priority. Presentation and discussion of incidents is one way of building a shared mental model of the system and the organization that runs it. Opening post-incident debriefings also emphasizes the organization’s primary commitment to learning.  Conclusion We’re  excited  about  the  possibility  of  understanding  SRE  cognitive  work.  Making cognitive work explicit for this group exposes a great deal of the system, both above and below the line. The continuing stream of incidents reminds us that calibration is important  and  difficult.  The  attention  and  effort  being  lavished  on  technical  and organizational tools to support SREs during and after incidents is testimony to the aspirations of many who understand this. We  have  confidence  that  the  methods  and  approaches  used  in  other  high- consequence  and  high-tempo  domains  are  applicable  in  the  SRE  world.  Applying those methods takes time and skill and is not for the faint-hearted! Even so, there appears to be a growing cadre of practitioners and researchers adopting the mantle of cognitive  systems  engineer,  many  of  them  with  direct  experience  in  our  industry. This bodes well for our collective future.  482      Chapter 28: SRE Cognitive Work   References 1. Allspaw, J.  2012 . Blameless PostMortems and a Just Culture.  Etsy Code as Craft blog.   https:  codeascraft.com 2012 05 22 blameless-postmortems ,  accessed  June 18, 2018.  2. Bainbridge, L.  1983 . Ironies of Automation. Automatica 19 6 : 775–779. 3. Billings, C. E.  1996 . Aviation Automation: The Search for a Human-Centered  Approach. Boca Raton, FL: CRC Press.  4. Cook, R. I.  2010 . How Complex Systems Fail. In Allspaw, J., ed. Web Opera‐  tions: Keeping the Data On Time. Sebatopol, CA: O’Reilly, 108–116.  5. Cook,  R.  I.   2017 .  Medication  Reconciliation  is  a  Window  into  “Ordinary” Work. In Smith, P.J. and Hoffman, R.R., eds., Cognitive Systems Engineering: The Future for a Changing World. Boca Raton, FL: CRC Press, 53–78.  6. Cook, R. I.  2017 . Where Complex Systems Fail.  SNAFUcatchers blog.  https:    www.snafucatchers.com single-post 2017 11 14 void-Incidents-as-Untyped- Pointers, accessed July 2, 2018.  7. Cook, R. I., Woods, D. D., and MacDonald, J. S.  1991 . Human Performance in Anesthesia: A Corpus of Cases  Tech. Report CSEL91.003 . Columbus, OH: Ohio State  University,  Cognitive  Systems  Engineering  Laboratory.  https:  www.drop box.com s arojonf2q6bcne9 Cook.1991.CorpusOfCases.pdf, accessed May 1, 2018. 8. Cook, R. I., Woods, D. D., McColligan, E., and Howie, M. B.  1991 . Cognitive Consequences  of  Clumsy  Automation  on  High  Workload,  High  Consequence Human  Performance.  NASA,  Lyndon  B.  Johnson  Space  Center,  Proceedings  of the  Fourth  Annual  Workshop  on  Space  Operations  Applications  and  Research  SOAR  90 ,  543–546,  accession  number  N91-20702.  https:  ntrs.nasa.gov  search.jsp?R=19910011398.  9. Cook, R. I. and Woods, D. D.  1996 . Implications of Automation Surprises in Aviation for the Future of Total Intravenous Anesthesia  TIVA . Journal of Clini‐ cal Anesthesia 8: S29–S37.  10. Deming, W. E.  1982 . Out of the Crisis. Cambridge, MA: MIT Press. 11. Gentner, D. and Stevens, A.  1983 . Mental Models. Mahwah, NJ: Lawrence Erl‐  baum Associates.  12. Hirschhorn, L.  1998 . Reworking Authority: Leading and Following in the Post-  modern Organization  Vol. 12 . Cambridge, MA: MIT Press.  13. Hollnagel, E. and Woods, D. D.  2006 . Joint Cognitive Systems: Foundations of [ISBN  Cognitive  Systems  Engineering.  Boca  Raton,  FL:  CRC  Press.  9780849339332]  14. Hutchins, E.  1995 . Cognition in the Wild. Cambridge, MA: MIT Press.  References      483   15. Klein, G., Woods, D. D., Bradshaw, J. M., Hoffman, R. R., and Feltovich, P. J.  2004 .  Ten  Challenges  for  Making  Automation  a  “Team  Player”  in  Joint Human-Agent  Activity.  IEEE  Intelligent  Systems  19 6 :  91–95.  [DOI:  10.1109  MIS.2004.74]  16. Lave, J. and Wenger, E.  1991  Situated Learning: Legitimate Peripheral Partici‐  pation. Cambridge, UK: University of Cambridge Press.  17. Nemeth, C. P., Cook, R. I., and Woods, D. D.  2004 . The Messy Details: Insights from the Study of Technical Work in Healthcare. IEEE Transactions on Systems Man and Cybernetics, Part A: Systems and Humans, 34 6 , 689–692.  18. Nemeth,  C.,  Nunnally,  M.,  O’Connor,  M.,  Klock,  P.  A.,  and  Cook,  R.   2005 . Making Information Technology a Team Player in Safety: The Case of Infusion Devices.  In:  Henriksen,  K.,  Battles,  J.  B.,  Marks,  E.  S.,  et  al.,  eds.  Advances  in Patient Safety: From Research to Implementation  Volume 1: Research Findings . Rockville,  MD:  Agency  for  Healthcare  Research  and  Quality   US .  https:   www.ncbi.nlm.nih.gov books NBK20467 .  19. Roth, E. M, DePass, E. P., Scott, R, Truxler, R., Smith, S. F., and Wampler, J. L.  2017 . Designing Collaborative Planning Systems: Putting Joint Cognitive Sys‐ tems Principles into Practice. In Smith, P.J. and Hoffman, R.R., eds., Cognitive Systems  Engineering:  The  Future  for  a  Changing  World.  Boca  Raton,  FL:  CRC Press: 247–268.  20. Woods,  D.  D.  and  Cook,  R.  I.   1991 .  Nosocomial  Automation:  Technology- induced  Complexity  and  Human  Performance.  Proceedings  of  the  1991  IEEE International Conference on Systems, Man and Cybernetics, Charlottesville, VA, 13–16  October  1991,  Vol  2.,  1279–82.  https:  ieeexplore.ieee.org document  169863 .  21. Woods, D. D.  1997 . Human-Centered Software Agents: Lessons from Clumsy Automation.  In  Flanagan,  J.,  Huang,  T.,  Jones,  P.,  and  Kasif,  S.,  eds.,  Human Centered Systems: Information, Interactivity, and Intelligence. Washington, DC: National Science Foundation, 288–293.  22. Woods, D. D.  2006 . Essential Characteristics of Resilience. In Resilience Engi‐  neering: Concepts and Precepts. Aldershot, UK: Ashgate Publishing, 21–34.  23. Woods, D. D., Dekker, S., Cook, R. I., and Johannsen, L.  2010 . Behind Human  Error. Boca Raton, FL: CRC Press.  24. Woods,  D.  D.   2017 .  STELLA:  Report  from  the  SNAFUcatchers  Workshop  on Coping  With  Complexity.  Columbus,  OH:  The  Ohio  State  University.  https:   snafucatchers.github.io .  484      Chapter 28: SRE Cognitive Work   John Allspaw heads Adaptive Capacity Labs and is the former CTO of Etsy. He is the coauthor,  with  Jessie  Robbins,  of  Web  Operations   O’Reilly,  2010   and  The  Art  of Capacity Planning  O’Reilly, 2017 . Richard Cook is a principal at Adaptive Capacity Labs and a research scientist at Ohio State University. His most often cited paper is “How Complex Systems Fail”.  References      485    CHAPTER 29 Beyond Burnout  James Meickle, Quantopian  Content warning: This chapter discusses mental disorders and their impact on life and work.  Topics  covered  include  medication  and  the  medical  system,  coercive  treatment, slurs, crisis, and laws and the legal system. Please  note  that  I  am  not  a  mental  healthcare  professional,  nor  am  I  a  lawyer  or  an expert in employment law. This chapter reflects my own analysis and opinions based on my  own  extensive  research  and  experience.  My  goal  is  to  inspire  thought,  dialog,  and appropriate action. You need to assess whether the strategies and suggestions in this arti‐ cle fit your situation. Depending on where you are doing business, and the nature of your work, some of the strategies and suggestions described in this article might not comply with the laws that apply to you. If you are interested in implementing any of these sug‐ gestions or strategies, in addition to making sure that they are right for your organiza‐ tion, you should consult a diversity and inclusion professional with sufficient expertise to ensure that your inclusivity initiatives comply with applicable law.    Decades  of  research  have  highlighted  many  ways  in  which  our  jobs,  as  well  as  the broader system of capitalism, make us all sicker. There are well-documented physical impacts  from  sedentary  workplaces,  open  office  plans,  and  even  artificial  lighting. Psychologists agree that these unnatural working conditions also directly harm our mental health, as do the psychological demands of the modern workplace, such as frequent interruptions or performing emotional labor1 for coworkers and customers. SRE  wouldn’t  exist  in  its  present  form  without  human-factors  research.  The  disci‐ pline has been defined by findings from psychology, industrial accident analysis and  1 Emotional labor refers to work managing the emotions of others, such as being expected to keep up a smile or  politely respond to aggression. This work is often expected of retail and service employees, particularly women, and is usually uncompensated.  487   prevention, cybernetics, and many other fields. This research has helped SREs con‐ verge  on  an  understanding  that  all  human  operators  are  fallible,  that  blaming  or shaming them doesn’t lead to improvements, and that truly resilient systems must be built in ways that respect human limits. Discussing and managing these limitations is considered an SRE best practice, and conferences frequently feature presentations about topics like burnout or hero cul‐ ture  that  provide  a  blueprint  for  building  more  sustainable  team  cultures.2  Not  all companies have adopted these best practices yet, but it has been heartening to see so many SREs working to build a culture that values mental health. I’m proud to work in an industry in which some of my most technically respected colleagues are also some of the most confident voices in support of a cultural shift toward sustainability and caring.  What Is Mental Health?  Within this chapter, I use the term “mental health” to describe the overall state of your  mind  and  emotions.  It  refers  to  your  general  condition  of  well-being,  rather than any specific diagnosis. You can be in poor mental health without any disorder or diagnosis.  And  having  a  mental  disorder  does  not  by  itself  indicate  poor  mental health. Your mental health can be impacted by any diagnoses, but also by your stress, personal life, and coping strategies.  Unfortunately, many SRE best practices implicitly treat job duties  such as pages or toil  as the primary stressor that needs managing. These recommendations tend to assume that, if only enough unpleasant work could be automated or improved, per‐ haps everyone could have great mental health in the workplace. That might sound very far away indeed for SREs with mental disorders, who have typically lived with them before their current jobs and likely will after. Discussions about mental health as  a  workplace  goal  are  powerful  and  necessary,  but  they’re  also  incomplete  when they’re held without consideration for the specific needs of people with mental disor‐ ders. This chapter guides you through refactoring your SRE organization to be more inclu‐ sive  of  people  with  mental  disorders.  I  begin  by  providing  a  working  definition  of mental  disorders  to  use  throughout  the  chapter.  With  that  in  hand,  I  explain  why mental disorders are an important part of any inclusivity initiative, and why high- performing SRE teams need to allow every member to be their whole self. I close out  2 Jennifer Davis on hero culture  DevOpsDays Boston 2014  , John Sawers on emotions  DevOpsDays Boston  2017 , Emily Gorcenski on strength and emotional labor  SREcon17 EMEA , Jaime Woo on psychological safety  SREcon18 Americas , and many others.  488      Chapter 29: Beyond Burnout   the chapter with inclusivity recommendations  and antipatterns  for each stage of the employee life cycle, as well as resources for further reading. Defining Mental Disorders In  this  chapter,  I  use  the  term  “mental  disorder”  as  a  catch-all  that  includes  both mental illnesses and neurodevelopmental disorders.3 These terms aren’t unanimous even among practitioners, so I define them, too. Mental illnesses are changes in thinking, emotion, or behavior that result in distress or difficulty in life. They often occur in adolescence through early adulthood but can do so at any age. Mental illnesses have highly varied severity and prognosis and can range from intermittent episodes to lifelong conditions. Major depressive disorder, generalized anxiety disorder, bipolar disorder, and eating disorders are examples of common mental illnesses. What’s not a mental illness? Short-lived reactions to work or life stressors, such as job burnout or grieving, are not mental illnesses. Neither are political views, or emotional reactions that are stressful but provoked or justified. Finally, “eccentric” behavior that doesn’t cause a person distress or harm the people around them is not considered a mental illness. Neurodevelopmental disorders are deficits in cognition or memory that relate to spe‐ cific  brain  processes  or  regions,  which  are  often  present  from  infancy.  A  disorder might  manifest  as  one  highly  specific  deficit  or  a  range  of  affected  abilities.  Many neurodevelopmental disorders are compatible with a high quality of life, and people who live with them often suffer only when society is reluctant to make appropriate accommodations. Learning disorders, autism, and attention-deficit hyperactivity dis‐ order  ADHD  fit into this category. Mental illnesses and neurodevelopmental disorders have very dissimilar etiology and prognosis, but within the context of the workplace it makes sense to talk about them collectively  as  mental  disorders.  Both  categories  of  conditions  can  result  in  similar symptoms, which means that people living with them might require similar accom‐ modations. Unfortunately, people with mental illnesses and people with neurodeve‐ lopmental disorders are also united by having to struggle for equal treatment in the face of discrimination.  3 Note that there are also many brain disorders that can cause mental symptoms—for example, stroke, Alz‐  heimer’s disease, and traumatic brain injury. Although not classified as mental disorders due to the different cause and prognosis, they are likely to benefit from similar accommodations in the workplace.  Defining Mental Disorders      489   At  any  point  in  time,  approximately  25%  of  people  have  one  or  more  diagnosable mental  disorders.4  Approximately  50%  of  people  will  meet  the  criteria  for  one  or more mental disorders at some point during their lives. Poor accessibility of mental health  services  means  that  many  will  never  receive  diagnosis  or  treatment  of  their symptoms. Mental Disorders Are Missing from the Diversity Conversation More people in the United States will have a mental disorder in their lives than will have cancer,5 yet people with mental disorders are one of the most stigmatized groups in America. Employment discrimination against Americans with mental disorders is largely illegal, but still encouraged by a culture that jokes about “not dating crazy”6 and horror films about asylum escapees. It’s therefore unsurprising that mental dis‐ orders are concentrated among poor people, and that poverty is concentrated among people with mental disorders. People with severe7 mental disorders  such as schizo‐ phrenia or borderline personality disorder  are almost absent from public life due in part to the intense stigmatization of these diagnoses. For people with mental disor‐ ders, there is an inclusivity crisis—not just in tech, but everywhere. Because  people  with  mental  disorders  often  face  significant  interpersonal  and  sys‐ temic discrimination, it’s unsurprising that most will choose to hide their status when they have the option to do so. Far too few feel able to discuss these conditions with their  friends  and  family,  and  even  fewer  will  opt  to  disclose  them  in  a  workplace where they might face retaliation. With so few visible data points, it can be hard to even notice larger discriminatory trends, leaving mental disorders as a hidden inclu‐ sivity problem in engineering.  4 Different sources cite a range of numbers; I have seen reported lifetime rates from 15% through 70% depend‐ ing on what specifically is being measured and how. For instance, it is common to study mental illnesses apart from neurodevelopmental disorders; also, studies that monitor rates of self-reported symptoms will differ from studies that monitor rates of diagnosis. For further reading on these numbers, see https:  www.cdc.gov  mentalhealth learn index.htm.  5 In a typical life span in the United States, you have about a 1 in 2 chance of being diagnosed with any mental  disorder, and about a 1 in 3 chance of being diagnosed with any cancer.  Source: https:  www.cancer.org  cancer cancer-basics lifetime-probability-of-developing-or-dying-from-cancer.html.   6 Crazy is considered a slur by mental disorder advocates, who discourage its use under any circumstances  other than by those with mental disorders describing themselves  i.e., “reclaiming” it . Also regarded as slurs are any other terms that use a mental disorder as a negative comparison: “insane,” “nuts,” “retarded,” and so on.  7 Any mental disorder can vary in severity from person to person. Referring to a disorder as severe is a state‐  ment about trends across the population, rather than any statement about particular individuals.  490      Chapter 29: Beyond Burnout   Mental disorders are also important to the diversity conversation because they can so readily intersect with other identities that have struggled to reach parity in engineer‐ ing. Women tend to be diagnosed with mental illnesses more often, and with differ‐ ent  proportions  of  diagnoses.8  Race  has  a  dramatic  impact  on  availability  of  care, especially as it pertains to difficulties in obtaining care that is culturally sensitive and appropriate. LGBTQ+ people face rates of mental illness up to three times as high as the  general  population  due  to  intense  stigmatization  that  often  starts  in  childhood and is frequently perpetuated by their own families. But just how pervasive is a lack of inclusion for SREs with mental disorders? In my experience, most engineering managers are not aware of any employees with mental disorders. Given the numbers presented earlier, this is quite improbable. What is the most likely explanation for that gap between perception and reality? The best case is that engineering culture hasn’t made engineers feel comfortable talking about their disorders. I describe this as the best case because the alternative is even worse: engi‐ neers with mental disorders are finding it challenging to get or stay employed. Sanity Isn’t a Business Requirement Discrimination against people with mental disorders is unfortunately quite real. Peo‐ ple do not hide their mental disorders simply out of a personal sense of shame; they also choose to hide them because they know that there are economic consequences to letting their employer find out. Intense stigmatization both discourages hiring people with mental disorders and then encourages them to stay “closeted”9 if they do man‐ age to secure a position. But sanity isn’t a business requirement. Mental disorders can affect workplace perfor‐ mance,  but  for  most  people  with  most  mental  disorders,  this  variation  is  much smaller in scale than what’s inherent to all humans. Most mental disorders are man‐ ageable chronic health conditions wholly compatible with SRE positions if the right support structures and team culture are in place. In fact, the ability to absorb people with different cognitive and emotional traits is a strong indicator of a healthy SRE team. Instead of expecting SREs with mental disorders to stay closeted and perform, SRE organizations should build a culture that unlearns stigma, actively includes, and makes accommodations.  8 For example, women are up to twice as likely to be diagnosed with depression or anxiety as men.  Source:  http:  www.who.int mental_health prevention genderwomen en .   9 A term originating in the LGBTQ+ community, referring to hiding parts of your identity that society discrim‐  inates against. Living a closeted life is incredibly psychologically harmful, but so is discrimination, forcing people who can hide into choosing between two bad options.  Sanity Isn’t a Business Requirement      491   What  does  stigma  look  like?  People  with  mental  disorders  are  often  considered “unreliable.” If phrased as not being capable of working a “regular nine to five”: yes, they might not be. But that likely also applies to your employees who are raising chil‐ dren, managing chronic physical illnesses, or even just traveling for work. Inability to cope with flexible schedules indicates a rigid organization or an unhealthily low bus factor.10  A  healthy  SRE  team  does  not  need  100%  uptime  from  its  SREs  any  more than a service needs 100% uptime from its servers.11 Another  common  form  of  stigma  is  to  assume  that  people  with  mental  disorders can’t  handle  intellectually  or  emotionally  challenging  work.  It’s  true  than  an employee with ADHD might miss a step on a checklist, but the lack of automation is what  made  that  business  process  fragile.  Anything  that  requires  a  human  brain  to store  information  will  fail  eventually,  whether  that’s  because  of  a  mental  disorder affecting concentration or because that brain’s owner is awake at 3 AM or hungry or has three other pager alerts firing.  Or all three.  Ultimately, everyone has cognitive and emotional limitations, and expecting flawless operators is a poor strategy for suc‐ cess. The most extreme form of stigmatization is to see people with mental disorders as prone to irrational or frightening behavior, such as emotional dysregulation or epi‐ sodes of mania. This simply isn’t the everyday experience of most people with mental disorders. When it does occur, it is likely to be successfully treated as an acute health crisis. The mandate of SREs is to build reliable systems from unreliable components, and  it’s  not  acceptable  to  build  an  SRE  organization  that  fails  when  an  employee experiences a medical emergency. Thoughts and Prayers Aren’t Scalable Ask around, and many or even most engineers will express their desire to support their colleagues with mental disorders. How can that be reconciled with the limited progress on inclusion? My take on this is that the most common strategies are some of the least effective. Companies and foundations promote awareness campaigns for mental disorders, but increased attention always comes at a cost to something else. A few brave individuals publicly share their own experiences with mental disorders, but that doesn’t necessar‐ ily help people with different diagnoses. Many people speak out about ending stigma and  stereotypes,  but  that  won’t  guarantee  that  all  barriers  to  inclusion  have  been removed. Almost everyone agrees that discrimination against people with mental dis‐  10 The risk of over-concentrating responsibility or knowledge in a team such that it would be vulnerable to a  particular member having a bus-related accident.  Source: https:  en.wikipedia.org wiki Bus_factor.   11 Thanks to Marianne Bellotti for the phrasing.  492      Chapter 29: Beyond Burnout   orders is wrong and that they’d never do it, and yet it’s possible for a system to be discriminatory even when everyone taking part in it believes they’re against that out‐ come. Ultimately,  individualistic  solutions  cannot  fully  include  people  who  have  been excluded by systems. Caring a lot about people is one of the most beautiful ways to express humanity, but it isn’t inclusion—not by itself. Inclusion requires organiza‐ tional change to remove exclusionary policies, barriers to access, and unintentional discrimination. As SREs, we spend our entire career thinking about systems, and we’re well aware that  manual  effort  won’t  scale.  Let’s  use  our  analysis  skills  on  the  right  question: instead of “How can I support engineers with mental disorders,” let’s ask, “How can I make engineering workplaces more inclusive for people with mental disorders?” Full-Stack Inclusivity The rest of this chapter is a deep dive into pro-inclusion patterns as well as a few anti‐ patterns that you should make sure your SRE organization is avoiding. Every organi‐ zation is structured differently, but there’s a convenient common thread to follow: the  employee  life  cycle.  For  each  stage  of  employment,  I’ll  suggest  ways  that  your team can be more inclusive of people with mental disorders. Some of these recommendations focus on not excluding people with mental disor‐ ders or exacerbating their symptoms, or, in other words, not being a hostile work‐ place. But a lack of workplace hostility is simply tolerance of someone despite their differences. Other recommendations will focus on making people with mental disor‐ ders feel included. Inclusion is the positive sense that someone is welcome because of who they are, which includes but is not limited to their differences. What every recommendation has in common is that it focuses on systemic change that you can push your team, organization, or company to make. Many of these rec‐ ommendations take the form of policies that you can implement without reference to any  specific  person,  before  anyone  asks  for  accommodations.  That  being  said,  you should also consider which interpersonal steps you can take on your own time, like learning more about specific mental disorders or changing how you communicate. It’s important to note that most of these recommendations are not mine. I have com‐ piled  them  from  a  variety  of  sources:  governmental  entities,  professional  diversity and  inclusion  specialists,  neurodiversity  self-advocates,  activists  for  marginalized people, conference organizers and speakers, and so many more. The work that I have done to adapt these recommendations to an SRE workplace is inconsequential com‐ pared to the hard work that these organizations engage in every day to change minds and build a better world for everyone.  Full-Stack Inclusivity      493   Application Applicant résumés are expected to leave a good first impression, but hiring managers rarely return the favor with well-crafted job postings. Solicit your entire team’s feed‐ back  when  writing  a  new  job  posting,  and  make  sure  to  free  enough  time  in  their schedule  for  them  to  meaningfully  contribute.  These  few  paragraphs  might  be  the first time an applicant has heard of your company, after all! Clearly identify essential job functions prominently in your job listings. Rather than declaring  your  stance  on  tabs  versus  spaces,  talk  about  what  a  typical  work  week looks  like.  How  much  time  will  be  spent  coding  versus  attending  meetings  versus writing documentation? Will employees be expected to contribute to project manage‐ ment or user interviews? How many hours will they be expected to work, and how often can you take vacation? Your job listings should also include specific information on compensation and ben‐ efits  particularly insurance and sick leave .12 Finding out that a job won’t support your mental health needs after you’re already at the interview means that everyone involved has already wasted hours of time. Be as up front as possible so that jobseek‐ ers can avoid unsuitable companies or teams—it’s really in everyone’s best interest. Joblint13 is an automated tool to scan your job postings for problematic language such as gendered assumptions or over-competitiveness. You should run all your job post‐ ings  through  joblint;  but  remember,  the  goal  isn’t  to  “fix”  your  postings!  Try  to understand why joblint warned you about your phrasing, and how your team’s cul‐ ture influenced your choice of words. Do you need to fix a word or do you need to fix your team? Ultimately,  be  as  honest  as  you  can  be  about  what  your  job  will  require  out  of employees and what it will provide them in exchange. People with mental disorders know their needs and limits, and it’s up to you to provide them the information they need to make the right choice.  12 As a former employee of Harvard University, I want to call that institution out as doing this very well. Each job listing contains enough information to look up salary ranges, available insurance plans, union member‐ ship, tuition coverage, and any other benefits before you even apply.  13 Joblint is available online, or runnable locally from this GitHub page. The GitHub project also accepts pull  requests if you want to add further rules.  494      Chapter 29: Beyond Burnout   Is On-Call an Essential Job Function?  There’s  no  hard  definition  for  what  makes  a  job  function  “essential,”  but  the  US Equal  Employment  Opportunity  Commission   EEOC   considers  three  factors:  the degree of skill required to perform the task, whether the job was created to perform the  task,  and  whether  other  employees  could  perform  it,  instead.  Do  any  of  these apply to on-call? Being on call is an extremely skill-dependent task in a high-skill industry, so I’ll give that a firm “yes.” If I’m being honest, many SRE teams exist because someone needs to  be  on  call;  at  the  same  time,  the  team’s  goal  is  to  end  it!  That  sounds  like  a “maybe.” Finally, because SRE teams should have pager rotations and high bus fac‐ tors anyways, that’s a clear “no.” I interpret that mixed rating as on-call being essential for many SRE teams but that it’s not essential that every SRE carries a pager. Consider whether your team could structure on-call as an opt-in duty with extra compensation. For more thoughts along these lines, check out Liz Frost’s presentation about on-call equity.  Interviewing Interviewing is where prospective employees get their first close look at your team and company culture. Unfortunately, this means that it’s often where they first notice signs that your workplace is not welcoming or inclusive. Determine your interview questions and success criteria in advance and communi‐ cate  that  information  to  both  the  interviewers  and  interviewee.  You  don’t  need  to share every question with an interviewee in advance, but you should make sure that they know what to expect from the process and what you are looking for them to demonstrate. When drafting your own engineering interview policies, consider read‐ ing through 18F’s as an outstanding example from which to draw inspiration. You should hire a diversity and inclusivity professional to help you write your inter‐ view policies and to conduct antibias trainings for your hiring managers. Psychology research has repeatedly demonstrated that even people with the best intentions sub‐ consciously stereotype others; worse yet, being aware of these biases is not sufficient to negate  them.  A  skilled  trainer  will  teach  you  structural  strategies  to  mitigate  the impact of your biases, such as anonymizing résumés or avoiding subjective criteria  e.g., “friendliness” and “culture fit” . Don’t structure your interview like an exam. Level of coding skill is a relevant job qualification, but pseudocode on a whiteboard is approximately no one’s favorite lan‐ guage  or  editor,  and  coding  is  just  one  part  of  engineering.  Instead  of  asking “whiteboard-friendly” algorithm questions like inverting binary trees or implement‐  Full-Stack Inclusivity      495   ing  sorts,  ask  higher-level  questions  that  test  a  broad  range  of  skills  like  systems design, writing tests, or interpersonal communication. People with mental disorders often work around impairments by leaning into other areas, so make sure that your interview provides opportunities to demonstrate a range of abilities. Never interview using “puzzles” or other artificially stressful situations, which can be very unwelcoming to people with anxiety disorders. A particularly egregious example saw interviewees playing a simulated bomb defusal video game, with the interviewers giving disarmament instructions. Skill at a video game you’ve never played before is neither a fair representation of job duties nor a sensible factor in hiring decisions, anyways. Compensation It’s engineering tradition to use multiple job offers as leverage for negotiating higher pay at the place you actually want to work. But high-stakes negotiation is so stressful that  it  can  force  a  choice  between  mental  health  or  salary.  Removing  negotiation from the picture can help people with anxiety, depression, and many other disorders. It’s also helpful to the many other groups of marginalized people who are negatively impacted by having to argue for their right to fair compensation. Start by creating objective criteria for determining a candidate’s offer. If you give a set of reviewers these criteria and a résumé, they should agree to within a few thousand dollars of one another. This is far from an impossible change; many workplaces  par‐ ticularly in government and academia  already grade salaries by job description and limit pay differences within the same grade. Make  sure  that  you  never  base  salaries  on  prior  compensation,  or  better  yet,  just don’t  inquire  about  prior  compensation  at  all.14  This  disturbingly  common  trend amplifies any prior discrimination! Taken over the duration of a career, even a small initial disparity can easily compound to a shortfall of millions of dollars of lifetime earnings. If you work at a large company, encourage it to share anonymized demographic and compensation data.15 Publishing regular reports on how mental disorders are associ‐ ated with compensation is a great way to hold your company accountable as well as to drive change throughout the entire industry.  14 This might even be illegal, depending on your state, as many are moving to ban the question outright. 15 Privacy engineering principles apply here, too, of course.  496      Chapter 29: Beyond Burnout   Benefits It’s  an  unfortunate  reality  of  America’s  for-profit  healthcare  system  that  many  job seekers have to make employment decisions on the basis of whether their conditions will be covered appropriately. But what does “appropriately” mean? As anyone with a chronic condition can tell you, not all insurance plans are created equal. When evaluating which health insurance plan to offer to your employees, choose one that  includes  comprehensive  mental  healthcare  coverage.  Many  insurance  plans “cover mental health,” but are low quality or high cost. Closely examine what treat‐ ment options are covered, whether there are any lifetime limits, and even whether the copays for therapist visits and psychiatric medications are excessive. As previously discussed, society’s treatment of LGBTQ+ people results in higher rates of  mental  disorders,  which  means  that  availability  of  mental  healthcare  is  also  an LGBTQ+  rights  issue.  And  although  being  transgender  is  not  a  mental  disorder, availability of care is still a pressing mental health need. You should consider trans‐ gender inclusivity when evaluating insurance plans, which might exclude necessary procedures or require excessive and invasive documentation. Consult with a special‐ ist to figure out what’s best for your transgender employees, and do so before they ask you to. Remember, your workplace policies are a huge factor in when and how your employees decide to undergo gender transition! Even if your benefits are inclusive, there is another problem: America’s lack of single- payer healthcare has resulted in a system so complex that it can be difficult to under‐ stand  how  to  receive  care,  how  much  it  will  cost,  and  how  much  of  that  will  be covered by your benefits. People with chronic illnesses, including mental disorders, often invest so much time into navigating the healthcare system that they may even joke about being “professional patients.” Unfortunately, the symptoms of many men‐ tal disorders can make it even harder to understand medical billing or make lengthy phone calls to dispute errors. Dedicate additional resources to helping your employ‐ ees with mental disorders fully utilize their benefits and avoid disruptions in care. Employees with mental disorders are often unaware of their options for taking time away from work. You should clearly document that sick days can be applied to men‐ tal disorders. Also make sure to document how longer leaves such as short-term disa‐ bility or unpaid leave apply to employees with mental disorders. It’s unfortunately the case that employees with chronic conditions will resort to using their  “vacation”  days  to  cover  their  health  needs.  This  is  especially  common  for employees with mental disorders who might prefer to hide why they’re taking time off due to stigmatization. Needing to reserve vacation time for health purposes can  Full-Stack Inclusivity      497   interact  quite  poorly  with  the  current  trend  toward  unlimited  vacation  policies.16 How “unlimited” is your policy in practice, and are certain types of time off seen as more legitimate than others? Consider the impact on people with mental disorders prior to making any changes in your vacation policy. If you do adopt an unlimited vacation  policy,  consider  pairing  it  with  a minimum  number  of  vacation  days  that you expect employees to take per year. If you’re not convinced, you can also think of minimum vacation days as an SRE best practice: you need your teammates to take time off in order to detect whether they’re single points of failure for any critical serv‐ ices.17 Onboarding Make sure to consider mental disorders when putting together onboarding packets. At a minimum that means including mental health information and resource docu‐ ments, as described in the previous section. Companies can go a step further by pro‐ viding  social  aids  that  describe  the  culture  and  people  that  new  employees  will interact with. Having a seating chart or dictionary of in-jokes is low cost, but they could  make  someone’s  first  day  among  strangers  that  much  more  comfortable  to navigate. Conduct  post-hiring  interviews  for  all  your  new  employees  and  actively  ask  about needs and accommodations, making clear that you provide accommodations for both physical and mental disorders. If you make accommodations only for employees who ask, you’ll be leaving out the many employees who aren’t aware of the existence of the Americans  with  Disabilities  Act   ADA   or  that  it  also  applies  to  mental  disorders. During this interview, give examples of accommodations to employees so that they have  an  idea  of  what  they  can  ask  for.  Remember,  some  accommodations  might require ordering equipment or reshuffling offices, so it’s important that you have this conversation  before  an  employee’s  first  day.  You  should  also  ask  whether  the employee  wants  to  designate  any  emergency  contacts  to  call  in  case  of  a  medical emergency. If  your  employees  do  ask  for  accommodations,  first  focus  on  meeting  their  needs, and then examine whether any “exceptions” you’re making could become new poli‐ cies that cover anyone else in a similar situation. This will help future employees who want  an  accommodation  but  are  unaware  that  they  can  receive  it  or  uncertain whether  it  is  safe  to  ask.  This  helps  to  create  a  company  culture  in  which  making accommodations is just another aspect of continuous improvement.  16 Unlimited vacation policies have seen rapid adoption because they avoid the accounting liability from  accrued vacation days. They are not inherently pro-employee, and although evidence is limited, they might even result in employees taking fewer vacation days.  17 Thanks to Amy Tobey for this suggestion.  498      Chapter 29: Beyond Burnout   Finally, don’t string your employees along. If the accommodations that they’re asking for are more than what you can provide, you need to be honest with them so that they can evaluate their next steps. Working Conditions The modern workplace can be stressful; thus, minimizing unnecessary noise and dis‐ tractions can be incredibly important to people with post-traumatic stress disorder  PTSD   and or  ADHD.  Open  office  plans  can  make  this  challenging,  but  you  can make a difference even if you’re stuck with one. You’re best off asking your employ‐ ees what they need from their office space, but likely requests include requiring meet‐ ings to be held in noise-isolated rooms, avoiding bells or gongs, and banning Nerf- gun fights. Flexible scheduling and remote work can make anyone’s life better, but they can be particularly important for people with mental disorders. A person with agoraphobia  the fear of being in situations where escape might be difficult or help wouldn’t be available  might find it very draining to leave the house, or in a severe case, might be unable to; someone in a depressive episode might spend hours just mustering enough energy  to  face  their  commute.  Create  a  policy  that  formally  details  what  flexibility your employees have in where and when they work.18 Make sure that your policy sets clear  expectations  for  check-ins,  meeting  attendance,  and  other  work  duties  that implicitly assume a shared office. It’s also critical that this policy documents how in- office employees must adjust their own workflows to support their remote coworkers. Don’t expect employees to be their own project managers  unless you stated that as a job requirement when hiring . Executive dysfunction is a symptom of many mental disorders, and it can manifest as “simple” tasks  like making a checklist  being far more  difficult  than  “complex”  tasks   like  writing  code .  By  breaking  work  into actionable tasks and prioritizing them appropriately in a task tracking system, you’re helping your employees get back to using the skills you actually hired them for. Let employees socialize on their own terms. Social team-building exercises can build trust, but that can come at the cost of excluding anyone who doesn’t or can’t partici‐ pate.  By  all  means,  hold  the  occasional  outing,  but  make  sure  that  “beers  with  the team”19  isn’t  the  only  pathway  to  feeling  included  and  trusted  by  your  colleagues. Also, make sure to never describe an event as “optional” if it comes with professional  18 A full discussion of remote workplaces deserves a chapter in its own right, but consider https:  www.remo  teonly.org and https:  stackoverflow.blog 2017 02 08 means-remote-first-company  as a starting point.  19 This is a very intentional example: events centered around alcohol consumption are often unsafe for people  with mental disorders. Alcohol can be a mental disorder symptom trigger, many people with PTSD feel unsafe around alcohol consumption, and many mental disorders are a risk factor for developing substance- abuse disorders.  Full-Stack Inclusivity      499   consequences when missed. Implicit social expectations can be especially harmful for many autistic people. One final note: if someone insults or jokes about mental disorders, make it known that this is unacceptable workplace behavior. If the person persists, fire them and let the rest of the company know why you did. The exception to this is people making jokes about their own mental disorders. This can be an important coping strategy for many, and yet it is often met with concern or even punishment. It’s completely rea‐ sonable to decide that this style of “gallows humor” isn’t appropriate for your work‐ place,  but  just  make  sure  to  apply  that  prohibition  evenly:  would  you  equally discourage employees from joking about their experiences with blindness or cancer? Job Duties Evaluate  employees  on  the  value  they  contribute  rather  than  on  whether  they  can perform each duty that is expected of the team. It’s already common for SRE teams to hire their way into new technology skills, but the same can be done for job duties, like giving presentations or writing documentation. Ideally, an SRE team should be a safe place to have a discussion about whether and how to distribute work according to the interests and abilities of the team members. Covering for each other is why you’re a team, after all! As a corollary, you should value and promote many forms of contribution. Teams often overvalue debate, public speaking, and releasing new features. In contrast, they tend to undervalue collaboration, mentoring, and maintenance and documentation work.  Call  out  major  milestones  that  aren’t  as  “impressive”  as  winning  an  impas‐ sioned  debate  or  giving  an  all-hands  presentation.  Make  sure  that  everyone  doing good work on your team has a chance to get a round of applause. Contributing to team decisions is worth a special note because it’s an important part of both daily duties and career progression. Many teams rely on adversarial decision- making styles in which each side advocates strongly for their opinion until someone “wins.” Open conflict is challenging and unpleasant for most people, and it can also be a trigger for some mental disorders  e.g., complex post-traumatic stress disorder [C-PTSD]20  or  bipolar  disorder .  Make  sure  that  your  team  is  also  open  to  other decision-making styles where appropriate, such as consensus or voting. While you’re at it, make most meetings and presentations optional; better yet, stream or  record  them.  This  is  an  important  part  of  including  remote  employees  and  20 C-PTSD is a form of PTSD resulting from prolonged abuse or neglect, often during childhood. People with C-PTSD usually have different symptoms and triggers than those whose PTSD stems from experiences like battlefield trauma or survivor’s guilt.  500      Chapter 29: Beyond Burnout   employees on different schedules, and, as we’ve seen earlier, those are good options for many employees with mental disorders. Training Whereas anyone might express a preference for interactive example code over prose documentation  or vice versa , that preference might instead be a necessity for a pro‐ grammer with dyslexia or ADHD. Provide an independent training budget to let peo‐ ple  learn  at  their  own  pace  from  their  choice  of  materials;  alternatively,  purchase seats for a subscription service that provides a wide range of content.21 Consider starting a mentorship program at your company. By taking a more holistic view of skills and career goals, a mentor can help someone grow beyond the immedi‐ ate needs of their team, with an eye toward the future. A meeting with a mentor can also be a safer place to discuss personal topics than with your immediate manager, who might treat you differently based on what you disclose. Note that marginalized people tend to see the most benefit from mentoring when it comes from someone like them. It can feel pointless to discuss career advancement with someone who does not understand your mental disorder s ; “speak at conferen‐ ces” may be inaccessible to someone with a panic disorder, and “improve your focus” might be completely out of reach to a person with ADHD. An inclusive mentorship program requires diverse mentors and, to keep it sustainable, appropriate compensa‐ tion. The surest way to kill a diversity initiative is to expect a day’s work and then also unpaid diversity and inclusion work  which is deeply emotionally taxing . Promotion Treating your employees well means providing opportunities for their achievements to be recognized. However, it’s crucial to examine whether your criteria for advance‐ ment have implicit ability requirements. Unfortunately, this is often the case: key cri‐ teria often focus on coordinating or managing people, presenting at conferences, or working long or irregular hours. A more egalitarian approach is to offer multiple pro‐ motion tracks for different skill sets  i.e., manager versus IC , and to value multiple types of contributions during performance reviews. Of course, your employees won’t be recognized for their achievements if they don’t get to achieve. Many attempts to include people with mental disorders end up “pro‐ tecting” them from career-enhancing work, such as not giving them assignments that you think they’d find stressful. Don’t let your good intentions stifle someone’s career! Avoid paternalism, which is the idea that you know what is best for someone else or can make decisions for them. Instead, practice informed consent. Check in privately  21 Such as O’Reilly Safari.  Full-Stack Inclusivity      501   with them and have an honest discussion about what the assignment will require and how important it is likely to be to the team and to the rest of the company. That being said, don’t treat advancement as a requirement on your team or think less of  employees  who  don’t  take  on  more  responsibility  over  time.  Many  managers exhibit  an  unfortunate  tendency  toward  devaluing  employees  who  don’t  have  an upward trajectory within the organization, such as assuming that they lack a “growth mindset.” But career growth is just one type of growth, and in the face of constraints on mental health or ability, an employee might quite reasonably choose to focus on other areas. Make sure not to evaluate performance at anyone’s current job by their level of interest in pursuing a different job. What gets interpreted as “lacking ambi‐ tion” is often “knowing what’s healthy.” Leaving Whether by choice or not, employees will eventually leave your team or your com‐ pany.  Their  stated  reasons  for  departure  might  omit  more  sensitive  topics  such  as their stress or health. You should conduct exit interviews with any departing employ‐ ees, and that interview shouldn’t be conducted by their manager or mentor. You need to do this work to discover whether your employees are leaving for other companies because  you  failed  to  provide  appropriate  accommodations  for  their  mental  disor‐ ders. Of course, mental disorders can be serious conditions, and depression in particular is one of the leading causes of disability in the United States. When accommodations on the job are no longer enough, you should offer a variety of options for medical leave  or  disability.  Even  so,  an  employee  might  find  unemployment  preferable depending on the particulars of their situation. Work with them to find the best solu‐ tion. If an employee does need to take leave due to their mental disorder s , you should go out of your way to offer them a chance to return to work as soon as they’ve recovered. But remember never to make an offer that you can’t follow through on: this means that,  first,  you  need  to  build  a  culture  in  which  someone  can  come  back  and  feel included when their mental health improves.  502      Chapter 29: Beyond Burnout   A Note on Crisis  Crisis does not define mental illness, but, unfortunately, it can be a component of it. A mental health crisis can be frightening not just for that person, but for everyone around them. It’s completely reasonable to be distressed or scared by someone acting erratically,  exhibiting  sudden  personality  changes,  or  making  threats  about  them‐ selves or others. The most common response to a mental health crisis is to call the police. Unfortu‐ nately,  officers  typically  do  not  have  mental  healthcare  training.  Also,  undergoing coercive psychological treatment can prevent a crisis from escalating but often at the cost of permanent harm such as PTSD or a legal record of involuntary commitment. Do not call the police unless you are comfortable with the consequences, up to and including  death.  There  have  been  many  interactions  with  police   particularly  in America  where a person with a mental disorder ended up dead, and this is especially true for people of color. It  takes  advance  planning  to  create  strategies  that  avoid  having  to  call  the  police. Respect the mental health of your employees and do your best to avoid triggering a crisis.  Make  information  on  phone  and  digital  helplines  available  to  all  employees. Ask  about  emergency  mental  health  contacts  in  advance,  preferably  during  the onboarding interview. Train your managers to recognize and respond appropriately to mental health crises. If a crisis does occur, weigh all of your options. Could you call their therapist, one of their friends, or another emergency contact they provided you during your onboard‐ ing process? Are any other employees willing to volunteer to take them to a hospital? If they are causing a disruption, could you close the office for the day rather than call‐ ing the police? Always keep consent and autonomy in mind, even when it’s not conve‐ nient!  Inclusivity for Anyone Helps Everyone Most  workplaces  will  already  have  implemented  some  of  these  recommendations while still having room to make progress on others. I hope that you’re fired up to contribute to that progress! But I encourage you to think of this as a marathon, not a sprint. These changes are deeply necessary, but you’re making them to encourage a healthy and sustainable workplace, so I implore you to promote change in a healthy and  sustainable  way.  Burnout  in  SRE  is  nothing  compared  to  burnout  among activists. What does healthy and sustainable progress look like? It means not trying to do all of this work yourself. It’s just as important to document and track the work yet to be done, to talk about it openly, and to convince others to help. It’s crucial to build rela‐  Inclusivity for Anyone Helps Everyone      503   tionships with teams outside of your SRE organization who have influence over rele‐ vant policies, because a recommendation that would be an uphill fight for you to put into  place  could  be  resolved  with  a  single  email  from  the  right  stakeholder.  This chapter might be oriented toward SREs, but your biggest contribution might be talk‐ ing about it with HR. Healthy  and  sustainable  also  means  treating  these  recommendations  as  starting points for conversation rather than rigid policy requirements. There is no “average employee” or “average workplace,” and there will always be exceptions and history and  context.  You  need  to  be  aware  of  what  people  with  mental  disorders  actually want from your workplace and not impose your expectations or assumptions onto them. Mental disorders don’t strip away someone’s dignity; that’s something that a person  does  by  treating  another  person  as  something  less  than  human,  and  that includes insisting on providing unwanted help. Coming to an individual understand‐ ing of needs can require more effort, but it is effort well spent. At the end of the day, inclusivity for anyone helps everyone. Changing your policies to help people with depression can also help those who are grieving. Providing multi‐ ple types of training material to support your employees with learning disorders is also  great  for  the  next  junior  engineer  that  joins  your  team.  Improving  your  leave policies  will  help  people  who  need  inpatient  psychiatric  care  but  also  people  who want to have children or take time off to work on a political campaign. In fact, the person who benefits most from your work on inclusivity might one day be you! Mental Disorder Resources If you are experiencing a mental health crisis, the National Suicide Prevention Life‐ line has trained counselors available 24 7, either via phone at 1-800-273-8255 or via web chat. Trans Lifeline is staffed by transgender people, for transgender people. You can reach them at  877  565-8860 or online. For mental health and mental disorder resources that focus on the needs of software engineers:    Open Sourcing Mental Illness   mhprompt   burnout.io  For general reading on mental health and mental disorders:    National Alliance on Mental Illness  504      Chapter 29: Beyond Burnout     American Psychiatric Association   National Institutes of Mental Health   Autism Women’s Network   Autistic Self Advocacy Network  James  Meickle  is  a  site  reliability  engineer  at  Quantopian,  a  Boston  startup  making algorithmic  trading  accessible  to  everyone.  Between  NYSE  trading  days,  he  advises DevOpsDays  Boston  and  conducts  Ansible  trainings  on  O’Reilly’s  Safari  platform. What free time remains is dedicated to cooking, sci-fi, permadeath video games, and Satanism.  Mental Disorder Resources      505    CHAPTER 30 Against On-Call: A Polemic  Niall Richard Murphy, Microsoft  On-call, as we know it, must end. It is damaging to the people who do it,1 and ineffi‐ cient as a means of keeping systems running. It is especially galling to see it continu‐ ing today given the real potential we have at this historical moment to eliminate it. The time for a reevaluation of what we do when we are on call and, more important, why we do it, is long overdue. How long overdue? I can find evidence of on-call-style activities more than 75 years ago,2 and in truth there have been people tending computers in emergencies for as long  as  there  have  been  both  computers  and  emergencies.  Yet,  though  there  have been huge improvements in computing systems generally since then,3 the practice of out-of-hours,  often  interrupt-driven  support—more  generally  called  on-call—has continued  essentially  unaltered  from  the  beginning  of  computing  right  through  to today. Ultimately, however, whether the continuity is literally from the dawn of com‐ puting or whether it is merely from the last few decades, we still have fundamental questions to ask about on-call, the most important of which is why? Why are we still doing this? Furthermore, is it good that we are? Finally, is there a genuine alternative to doing this work in this way? Our profession derives a great deal of its sense of mis‐ sion, urgency, and, frankly, individual self-worth, from incident response and resolv‐  1 See, for example, “How on-call and irregular scheduling harm the American workforce” from The Conversa‐ tion, or “Why You Should End On-Call Scheduling and What to Do Instead” from When I Work, outlining the impact on income and family; the costs to the systems themselves are hard to estimate, but “friendly fire” in on-call situations is estimated to occur in over 1% of on-call shifts.  2 Bletchley Park and its complement of WRNS  Women’s Royal Naval Service  on-call operators. 3 According to, for example, Tom’s Hardware, around the Bletchley Park era, “in a large system, [a vacuum  tube] failed every hour or so.”  507   ing  production  problems.  It  is  rare  to  hear  us  ask  if  we  should,  and  I  think  the evidence  clearly  shows  us  that  we  should  not  and  that  a  genuine  alternative  is possible. But first, let us look at the rationale for doing on-call in the first place. The Rationale for On-Call Many SREs have an intuitive idea of why on-call is necessary: to wit, getting a system working again, and we shall come back to that shortly. However, for a fuller under‐ standing, it is useful to look at the role of on-call in other professions, to focus our idea of what is unique to our case. Let us look at examples from medicine. First, Do No Harm In a medical context, the person on-call is the person on duty, ready to respond.4 In emergency  medicine,  the  function  of  the  on-call  doctor  is  in  some  sense  to  be  a portable decision-maker, with appropriate expertise where possible, and the ability to summon it otherwise. The first function is to perform triage, in which the doctor fig‐ ures out from the signals available whether or not the patient should be in A&E5 in the first place and otherwise attempts to make the patient better. Not necessarily cure them; that is not the domain of emergency medicine. The goal of emergency medi‐ cine  is  to  stabilize  the  situation  so  the  patient  can  be  moved  to  ward  medicine,  to manage a cure, the treatment of slow decline, or otherwise non-life-threatening, non- immediate situations. Parallels with SRE Emergency  medicine  is  strongly  interrupt-driven  work,  and  the  broad  context  of bringing people with expertise to fix a problem is exactly the same as with the SRE on-call context. Those are the strongest point of correspondence. Another parallel is the act of triage, which is also performed in the SRE context, although usually by soft‐ ware deciding that some metric has reached an unacceptable threshold and paging rather  than  manual  action.6  A  similar  act  is  also  performed  when  an  SRE  on-call operator  decides  some  alert  is  not  actually  important  enough  to  bother  with.  Broadly, you could look at the overall emergency medicine challenge as how we can  4 See, for example, this MedicineNet article or this free medical dictionary, making specific reference to being  reachable in 30 minutes of being paged.  5 “Accident & Emergency” in the UK Ireland; Emergency Room  ER  in the US. 6 Note that doctors get a lot of automatic alerting as well, it’s just that it seems that a lot of it is very low quality;  see, for example, this Washington Post article.  508      Chapter 30: Against On-Call: A Polemic   deliver relatively well-understood treatment plans in an efficient way while wrestling with many other simultaneous demands.  Preparations for an on-call shift are similar, too. A demanding—potentially longer than 24-hour—work shift7 requires physiological preparation, and in on-call prepara‐ tion documents I have also seen the equivalent of an SRE playbook, which is to say, a list of specific, somewhat tactical suggestions, for how to respond to different kinds of known failure in human biological systems.  In the case of On Call Principles and Protocols, sixth edition  Elsev‐ ier , for example, the bulk of the chapters are about specific “sub‐ system” failures  e.g., abdominal pain, chest pain, seizures , with a seemingly  strong  Pareto  Principle–style  assumption  that  80%  of cases arise from 20% of the total causes.8  Finally, when a system is restored to an everyday working state, but serious cleanup work is necessary, that can be left to a normal daytime activity—or ward work, in other words. Differences with SRE Strong though the parallels are, working with software is fundamentally different. In some ways, the frightening thing about SRE on-call is that because so many of the systems we look at are changing so quickly, the act of being on call for a particular system in January is not hugely relevant when it comes to being on call for it in June. This is not the case in A&E, where people’s bodies and the life-threatening traumas that  befall  them  tend  to  have  quite  well-understood  manifestations  and  diagnoses, wide genetic variety and environmental factors notwithstanding. Although that rate of change can fluctuate given particular team or industry circumstances, it is almost never zero, and if it is, the software environment itself is almost always changing, too.  An SRE approach enables fast change, so this is to be expected.  A useful analogy would be that SRE on-call is like dealing with entirely new kinds of human  beings  every  A&E  shift,  in  which  the  patient  presents  with  an  additional unexplained internal organ for which someone is still writing the documentation. If medicine had this to cope with, the treatment plan would have to be derived from  7 See, for example, this article from Medical Protection Ireland, emphasizing not eating junk food, paying bills  in advance of a week of night-shift work, and double-checking calculations made during night shifts.  8 For example, this article claims that 5% of their ER admissions gave rise to 22% of their costs; this piece  argues more broadly that Pareto Principle–style effects are distributed throughout medicine; and this article showed that adverse drug effects obeyed a Pareto Principle–like distribution across a sample of 700-plus cases.  The Rationale for On-Call      509   first principles every time. That unbounded quality—that the causes and extent of the emergency might involve arbitrarily novel situations every time9—seems to be unique to software. Of course, despite this, most adjacent shifts, and the problems encountered therein, are  quite  similar.  But  the  worst-case  scenario—that  almost  everything  could  have changed since your last shift—can and does happen. Underlying Assumptions Driving On-Call for Engineers So, the rationale for on-call in other professions is to bring expertise and resources to a problem as quickly as possible, to resolve that problem, and  often  to prevent a similar  or  larger  problem  from  developing.10  Today,  we  place  humans  in  those situations because the complexity of the world remains such that a robot program‐ med as well as we know how to do it today could not effectively act as, for example, a medic.11 However, the highly restricted environment inside a machine or machines, although complicated under certain conditions, is not as complicated as the real world. If the argument is that the real world is complicated enough to need a human doing the on- call, the same argument applied to the datacenter is not as clearly true. Yet we continue to put engineers12 on call for services. Why? Let me be brutally frank, fellow operations engineers: we are sometimes put on call for bad reasons. One well-known bad reason is because it is cheaper than solving the real problem; that is, it is cheaper to pay a human to just react and fix things manually when problems happen, rather than extend the software to do so. Another bad reason would be because on-call work is perceived to be awful. Therefore, product engineers who are not trained for it are very reluctant to do it, and the desire grows to pass this work off to a lower caste: operations engineers. Yet another one is the assumption that  mission  criticality   however  critical  that  turns  out  to  be ,  a  “keep  the  site  up” mentality,  and  cultivating  a  sense  of  urgency  around  production  state  all  require being  on  call.  Ultimately,  however,  these  are  dogma:  firmly  and  long-held  beliefs, which might or might not be useful.  9 As best I can tell, this situation is unique to software: industries that deal with very complex hardware, such as  airplanes, do have problems related to complexity, and uncover latent problems with particular revisions of sensors, and so on, but the nature of software being changed all the time is found, as far as I know, nowhere else.  10 On-call in the medical professional also serves as a triage function, which is partially outsourced to monitor‐  ing software in the SRE case.  11 Leaving aside the considerable problems with persuading the public, this would be a good idea. 12 This applies to operations engineers generally, and sometimes to product software engineers.  510      Chapter 30: Against On-Call: A Polemic   I take a different approach and use the language of risk management for the purposes of outlining the valid reasons why we are on-call today. This allows us to be focused on matters relating to the impact on the supported systems rather than beliefs that might or might not be true or might change over time. Let us therefore group the reasons into the following categories: Known-knowns  Consider a system that has known bugs, and the circumstances under which they are  triggered  are  known,  their  effect  is  known,  and  the  remediation  is  known, too. Many on-call professionals today will be familiar with the sensation of being paged for something fitting this description. Of course, the obvious question is then, why are humans fixing this at all? As discussed, sometimes it is cheaper, sometimes not, but it is typically a decision made by business owners that the particular  code  paths  that  recover  a  system  should  be  run  partially  inside  the brains of their staff rather than inside the CPUs of their systems.  I suppose this is externalizing your call stack with a vengeance.  In this bucket, therefore, engi‐ neers are put on call because of cost; in reality, the problem is perfectly resolvable with software. Known-unknowns  Many software failures result from external action or interaction of some kind, whether  change  management,  excessive  resource  usage  beyond  a  quota  limit, access control violations, or similar. In general, failures of this kind are definitely foreseeable in principle, particularly after you have some experience under your belt, even if the specific way in which  as an example  quota exhaustion comes into play is not clear in advance. For example, these problems can sometimes be related to rapid spikes in traffic or other exceptional events. Although you can’t necessarily predict why in advance, it’s usually essentially statistically guaranteed that you’ll have a few really big spikes a year. Engineers are therefore put on-call because the correct automation or scaling is not currently available; the problem is again perfectly resolvable.  Unknown-unknowns  Despite theoretical positions to the contrary,13 systems and software do indeed fail. Today, certain kinds of failure can be automatically recovered from or other‐  13 For the purposes of this footnote, I want to attack the notion that failure is unavoidable and that everything in  computing is wobbly stacks built on soggy marshes of unpredictability. This is not the case. There are large classes of software systems that run without issue for years. In fact, we might propose  say  a Murphy’s First Law of Production: a system operating stably in production continues to operate stably in production unless acted on by an external force. This is true for many embedded systems, disconnected from the internet. It is not largely true for non-embedded, internet-connected systems. Complexity is surely part of the answer why, but there is something we are missing.  The Rationale for On-Call      511   wise responded to without human intervention—if not self-healing, then at least non-self-destroying. But there are large classes of failures that aren’t, and, what’s worse, these failures typically change as a system itself changes, its dependency list grows, and so on. In the language of engineering risk management, these are unknown unknowns:14 things that you don’t know that you don’t know, but you do  know  that  you  don’t  know  everything,  so  you  can  foresee  their  theoretical existence. Engineers are therefore put on-call because of the system potentially failing in ways that could not have been seen beforehand and therefore seemingly requires the kind of context-jumping response that only a human can give; the thing that makes this hard to respond to automatically is the complexity of what could have gone wrong. The Wisdom of Production  This is conceptually very similar to our first two reasons; the difference is mostly in intention. We choose to put engineers on-call for a system to learn real things about how it behaves in real situations. We might learn unpleasant things about it or we might learn pleasant things, but we are doing it explicitly to gather infor‐ mation and to decide on where to put our effort for how to improve it.  It seems clear to me that most of the significant opposition to removing on-call as a job responsibility for SREs is because of the third category: unknown-unknowns. On-Call Is Emergency Medicine Instead of Ward Medicine But, actually, the only valid reason to put engineers on-call is the last one: The Wis‐ dom of Production. The other categories are ultimately distractions. The  first  category,  known-knowns,  involves  humans  executing  procedures  to  fix things that could  by definition  be done perfectly well by machine; it merely happens to be cheaper or simpler for some set of particular humans to do it at that moment. This category is about expediency, cost control, and prioritization, not engineering. There  is  nothing  here  preventing  a  completely  automatable  approach  other  than money and time, which are obviously crucial things, but not a barrier in principle. Yet this category persists as a source of outages today, perhaps because of a widely held practice in the industry of treating operations as a cost center, meaning that no business owner will invest it in, because it is not seen as something that can generate revenue for the business, as opposed to simply accumulate costs.15 For known-unknown problems, the path away from manual action is generally more resources or pausing normal processing in a controlled way, with some buffer of nor‐  14 See this definition of the phrase. 15 See, for example, this wonderful article.  512      Chapter 30: Against On-Call: A Polemic   mal operation before more-detailed remediation work is required. Indeed, the condi‐ tions in which the full concentration of an on-call engineer are legitimately needed to resolve a known-unknown problem usually involve a flaw in the higher-level system behavior. An application layer problem, such as a query of death16 or resource usage that  begins  to  grow  superlinearly  with  input,  is  again  amenable  to  programmatic ways of keeping the system running  automatically blocking queries found to be trig‐ gering restarts, graceful degradation to a different datacenter where the query is not hitting, etc. . The important questions, then, are how flaws in higher-level systems are introduced and whether there is a meaningful way of working around this. The situation is even more dispiriting for, as an example, system-level change-control problems: perhaps a problematic Access Control List  ACL  prevents access to a criti‐ cal  dependency;  or  maybe  a  runtime  flag  changes  startup  servers  to  a  set  that  no longer  exists  or  are  vastly  slower;  or  possibly  a  GRANT  command  accidentally removes access to the systems for the entity performing the GRANT. Although these might seem outside of the domain of automated response, it is canarying,17 which is the actual solution: it allows us to pilot a variety of difficult-to-reason about changes and observe the effects in a systematic way, and it does not require a fundamental rewrite in core systems, merely the ability to partition activity. Yet, instead, we typi‐ cally pay for humans to make a change and watch a process; perhaps reasoning that if we must have someone around to handle unknown-unknowns, we might as well have them do the other ones, too. So, it really does come down to the problem of unknown-unknowns: what are the lurking unanticipatable problems present in the system that prevent us from turning an emergency situation into plain-old ward medicine? Surely, runs the argument, we can’t know these in advance, and therefore we need a human around in order to be able to observe the system in its entirety and take the correct response? The situation is more nuanced than that, however. Not all problems that could result in outages happen to a system: only some of them will. Not all of the lurking difficul‐ ties can be hypothesized in advance, but that’s actually OK because we don’t need to  16 See, for example, this definition. 17 Note that I am being deliberately careful with my wording here. Canarying has limits: for example, you  shouldn’t canary in a nontransactional environment in which each operation is “important”  as opposed to a simple retryable web query ; an operation will irrevocably alter state  perhaps with some monetary value attached  and yet can’t be rolled back. You also can’t canary in an environment in which fractional traffic isn’t routable safely to a subset of processors. Running a canary fleet is more expensive, too, unless you’re canni‐ balizing your production serving capacity, which is problematic on its own, and, finally, your cloud colo etc. provider might not provide easy hooks for canarying. But I  carefully  don’t say, “canarying can solve every‐ thing.” I say, “It’s hard to think of a situation where you can’t canary to good effect.” Those are different state‐ ments, and I expect that canarying could solve a very wide array of problems that it doesn’t solve today, mostly because we put people on call rather than go to the time and expense of setting up a canarying infra‐ structure.  The Rationale for On-Call      513   have a solution for every possible problem in advance. Instead, we need to translate problematic states of the system  requiring emergency medicine  into ones requiring business hours intervention  ward medicine . There is a very big difference between solving the general case of all unanticipatable problems and the specific case of con‐ structing  software  to  be  much  more  resilient  to  unexpected  problems  within  their own domain. Perhaps a little like the halting problem, solving the general case is cer‐ tainly intractable because modeling program state in order to predict halting is too big; but predicting when a simple FOR loop will halt is trivial. It is absolutely true that there have been many sizeable incidents in which extremely delicate effects have played  a  part  in  major  outages.  However,  most  people  do  not  ask  themselves  why those delicate effects emerged in the first place, and this is partially because the state of the industry does not seek to build reliable systems out of well-known building blocks that fail in particularly well-understood ways; instead, unfortunately, the state of the art today is that most successful application layers, whether for startups or for huge multinationals, are reinvented time and time again from the ground up.18 This leads to a situation in which a number of building blocks are indeed composed together, typically in some kind of microservice architecture, but because each orga‐ nization does it from first principles every time, there is no meaningful industry-wide cooperation on a single stack for serving, data processing, and so on that could pro‐ duce  resilient  and  well-tested  software  units.  It’s  as  if  the  construction  industry derived bricks from first principles every time it built a house, and a row of houses would  only  share  bricks  because  the  team  members  happened  to  sit  next  to  each other at lunch. To put it another way, part of the reason we see unknown-unknowns to having bad effects on systems in the first place is because the fine detail in how systems and code interact  is  poorly  understood.  And  the  reason  they  are  poorly  understood  is  not because people are stupid or software is hard  neither of which is necessarily true  but because each team needs to understand things from the beginning. If we had a con‐ solidated set of components that behaved in well-understood ways, we could offset these risks significantly, perhaps completely depending on the domain. Or, putting it in the language of solutions rather than problems, we need a safe cloud stack—or at least cloud components that behave in safe ways. Another way to think about this problem is to think about the set of postmortems you’ve assembled for your service over the years. When you look at the set of root causes and contributing factors over a long enough period, you can ask yourself the questions:  what  proportion  of  those  outages  were  genuinely  unforeseeable  in advance, and what proportion of them would have been remediated if fairly simple protections had been put more consistently in place? My experience suggests that, as  18 In this context, “ground” often equals POSIX libc, although it shouldn’t.  514      Chapter 30: Against On-Call: A Polemic   per the earlier analysis of the ER, 80% of outages are caused by 20% of root causes; the rest is in the unknown-unknowns bucket. We can attack those in turn by building more resilient systems that fail safely, can allow for canarying, and separate applica‐ tion logic from a solid systems layer. Counterarguments An important counterargument is that the previous observations are all very well, but here  in  the  current  universe,  with  bespoke  software  aplenty  and  limited  budgets, there is no prospect of avoiding unknown-unknowns, and so we are still locked into on-call for the indefinite future. This might well be true, in the sense of there being lots of software and not enough money,  but  almost  every  piece  of  software  everywhere  does  go  through  a  rewrite cycle at some point. The prospect of adopting key reliability frameworks for cloud consumers, particularly if they are easy to use and cover very common cases  HTTP servers, storage, etc.  is not as far removed as you might think. It will take time, cer‐ tainly, but it is not impossible. Another  counterargument  is  that  this  will  prevent  on-call  engineers  from  under‐ standing and effectively troubleshooting problems when they do arise; therefore, we should continue to do on-call across the industry. Well, as outlined earlier, it is true that one valid reason for doing on-call is the wisdom of production, and electing to do so is perfectly fine. However, if the objection is to the idea of an industry popula‐ tion of SREs that is increasingly feeble at on-call, the idea of this article is to move away  from  on-call  as  emergency  medicine,  and  toward  on-call  as  ward  medicine, where observation over time and support from colleagues is available. I do not think we will remove every failure from every piece of software ever, just that it is certainly more possible to remove enough of them that we don’t need to suffer the costs of being on-call that we do today. Some commentary states that19 machine learning might be an effective substitute for human on-call. Permit me, for a moment, a more skeptical view. Although I would gladly welcome a piece of software able to cope with anything, in my opinion an arbi‐ trarily complicated unknown-unknowns failure situation would require the machine learning software to fully understand the stacks it is working with, which is not possi‐ ble today and might never be possible. Finally, it could be pointed out that building blocks exist today and are being used by engineers  everywhere  for  specific  tasks   Kafka,  Spark,  Redis,  etc. ,  and  yet  we  still have this industry-wide problem. Of course, the adoption of one framework or tool‐ set might help with one class of problem, but there is nothing today that matches the  19 See, for example, this devopsdays talk by Hannah Foxwell.  The Rationale for On-Call      515   description of a hardened cloud stack with known good choices for each functional element. As many of the use cases must be covered as possible, or too much is left to chance. The Cost to Humans of Doing On-Call A more pointed counterargument to SREs performing on-call is perhaps to be found in human factors analysis, cognitive psychology, and the general effect of stress on the human being. In  general,  humans  perform  quite  poorly20  in  stressful  situations,  which  is  over‐ whelmingly  what  on-call  is.  Not  only  that,  it  is  extremely  costly  to  the  individuals involved. I will cover this in more detail shortly. But  the  larger  picture  is  that,  yes,  this  matters  because  of  the  human  toll,  but  also because it undermines the argument that there is no effective substitute to human on-call. This in turn is because the industry, outside of operations engineers them‐ selves, has very much an incomplete understanding of the costs of doing on-call. Fur‐ thermore, because of the rationale underpinning the “necessity” for human on-call— to wit, unknown-unknowns—business owners believe that whatever the cost is, they are committed to paying it because they see no alternative. Precisely because people assume that there is no alternative to human on-call, it is rare to see these costs fully outlined, except at operations conferences and in one-to-one conversations. Which at the very least is a pity, because if we do not know precisely what we are paying, we cannot know if it is worth it. Let us leave the exact definition of “stressful” to one side for a moment, and for the purposes of this paragraph presume the typical properties of SRE on-call fulfill that definition—to wit, out-of-hours requirements to work; potentially large or unboun‐ ded financial reputational impact to the organization as a whole, depending on your individual  performance;  sleep  disrupted,  truncated,  or  in  extreme  circumstances, impossible for the duration of the incident; and, if in an out-of-hours context, poten‐ tial difficulty obtaining help from one’s colleagues. Humans are not in fact natural smooth performers in stressful situations. Studies of human error, specifically of stress in the context of on-call, are rare; there are many precedents  in  similar  domains,  however,  including  programming  itself,  chess,  and industrial situations, such as nuclear power plant meltdown situations. Ultimately, of course, most of these are in some way inaccurate proxies for real-world performance, but  remain,  for  the  moment,  the  best  that  we  have.  The  more  generic  studies  on  20 Poorly in comparison to what, is a valid question: here, I mean compared with a notional resolution that  involves no mistakes or blind alleys, but does involve the usual delays in detection, starting analysis, and so on.  516      Chapter 30: Against On-Call: A Polemic   human error seem to arrive at a background rate of between 0.5% and 10% for vari‐ ous “trivial” activities, including typing, reading a graph, and writing in exams.21 Sim‐ ilarly, A Guide to Practical Human Reliability Assessment  Kirwan  shows a table that lists “Stressful complicated non-routine task” as having an error rate of about 30%. Dr. David J. Smith’s Reliability, Maintainability, and Risk states a similar rate, 25%, for  complicated  tasks,  and  somewhat  depressingly,  a  50%  error  rate  for  “trivial” things such as noticing that valves are in the wrong position.22 In a paper on the sub‐ ject,23Microsoft shows that middle-rank chess players double their chance of a serious blunder  as  they  move  from  10  seconds  to  0  seconds  left  on  their  clock,  and  back‐ ground  error  rates  for  programming—in  the  absence  of  any  particular  stressor— range between  13% in this comparison table. Any way you look at it, it is clear that to err is very definitely human. Another  potentially  large  effect  on  on-call  performance  is  cognitive  bias,  which   if you accept the overall psychological framework , strongly implies that human beings make errors in stressful situations in very systemic ways. A write-up goes into this in more detail here, but suffice to say that if you read Thinking, Fast and Slow and won‐ der if there’s any evidence SREs are affected by cognitive kinks of some kinds, there is indeed quite a lot of evidence to support it; for example, anchoring effects in the con‐ text of time-limited graph interpretation, closely matching the awkward constraints of on-call. But most of what we’ve been discussing merely  perhaps loosely  supports what you more or less knew already about the human condition, being human yourself. There is also a subtler effect, which is that the fear of on-call is often enough by itself to rad‐ ically change people’s behavior. Entire development teams reject outright the notion of going on call, because of the impact on their personal lives, family, and in-hours effectiveness. Many such teams are perfectly happy for operations teams without the authority to actually fix problems to keep things ticking over “during the night shift” as best they can—as long as those developer teams don’t have to do on-call them‐ selves. Additionally, diversity and inclusion in SRE therefore suffers as caregivers—of what‐ ever kind, parental or otherwise—deliberately opt out of a situation that promises to place them in direct and certain conflict with their other responsibilities.  21 See, for example, Ray Panko’s site for a comparison table. 22 More shocking and yet also completely believable is a line item stating that failing to act correctly within 1  minute of an emergency situation developing has a probability of 90%; see this article for more.  23 Ashton Anderson, Jon Kleinberg, Sendhil Mullainathan, “Assessing Human Error Against a Benchmark of  Perfection”.  The Cost to Humans of Doing On-Call      517   When we turn our attention to the actual effects of serious on-call work on human beings, it makes for similarly, if not more, sobering reading. There is evidence to sug‐ gest that even the possibility of being called increases the need for recovery in on-call workers24; sleep deprivation has a long list of negative effects and there is serious evi‐ dence suggesting that it shortens life expectancy25; and finally, this survey of papers examining  the  effects  of  on-call  generally  suggests  excellent  evidence  that  on-call work can and does have a variety of effects on physical health  e.g., gastrointestinal and reproductive  and mental health  e.g., anxiety and depression . There’s simply no meaningful upside for the practitioners. All of this is not even to mention the experience of on-call, which is often dispiriting. Not  every  team  looks  after  their  monitoring  and  alerting  as  assiduously  as  they should, so an on-call engineer can be assailed on their shift with noisy alerts, alerts that are mainly or wholly unactionable, monitoring that doesn’t catch actual outages and actual users that do, poor or nonexistent documentation, blame-laden reactions when things inevitably go wrong, a complete lack of training for doing it, and, worst of all, a systemic lack of follow-up to any of the issues discovered, meaning that one braces oneself each shift for the structural unsolved problems that paged one the last shift. And in some companies, you do all of this without either extra financial com‐ pensation or time-off-in-lieu. Clearly,  as  a  species,  we  don’t  like  doing  on-call,  we’re  not  terribly  good  at  it,  it’s actively harmful for us, and it can often be one of the most unpleasant experiences we have. Given all that, I ask again: why aren’t we talking about meaningful alternatives? We don’t need another hero Perhaps part of the reason is us. The  mission  SRE  has—the  protection  of  products  in  production—mixes  well  with that cohort of people motivated to “step up” and work hard at resolving production incidents. But throwing oneself relentlessly against a production incident, although in some ways admirable, and in another sense what we are paid for, has at least as many drawbacks as it does merits; in particular, the negative consequences of heroism. The bad consequences of heroism are simultaneously subtle and coarse. Many people like the approval of their peers, and also like the satisfaction of knowing that their work has had a direct  even positive  effect on their team, the system they support, their company, and so on. So, there is a direct psychological link between stepping in  24 See this paper on on-call fatigue. 25 Mark O’Connell, “Why We Sleep by Matthew Walker review – how more sleep can save your life”.  518      Chapter 30: Against On-Call: A Polemic   to be The Hero fixing the problem and the approval one can get from fellow team members,  the  product  development  team,  one’s  manager,  and  so  on.  Both  explicit and  implicit  incentives  to  repeat  hero-like  behavior  can  evolve.  Perhaps  company management comes to expect it; you kept the system going last time in this way, why aren’t you doing it this time? Worse than management in some ways, perhaps your peers come to expect it, particularly in positively oriented cultures like Google where peers can award each other small bonuses at the click of a button. But worse than that in turn, when a heroine steps up and fills a particular role, often out  of  hours  or  in  demanding  circumstances,  this  means  they’re  not  doing  work they’d otherwise been scheduled to do. Therefore, a replacement heroine is required to do the things that wouldn’t otherwise get done, and another in turn, and so on. Granted, a team often must make do with what it can, but a long-running production incident has definite physiological effects and someone has to pick up the slack. If this can’t be the directly affected team member, it’s someone else from the team, and so on. Not to mention the bad effect of modeling hero culture26 to the rest of the team and seeing it being rewarded. Finally, it is worth noting that there can be a dichotomy in how we see ourselves ver‐ sus how our value is perceived by others. As we just discussed, the profession takes on-call very seriously and tries to be good at it; yet, it is very, very rare to be promo‐ ted as a function of on-call performance. In 11 years at Google, I never saw it. It was difficult to be promoted if you were bad at on-call, but it was impossible if you were good at on-call and bad at other things. Alice Goldfuss talks about this in her 2017 Monitorama talk. The peculiarity of being rewarded in small ways for heroic behav‐ ior and yet being denied larger rewards as a consequence of that same behavior is unsettling. Yet, despite all of this, it is still seen in many quarters as a sign of weakness when someone  dislikes  on-call.  Perhaps  this  is  connected  to  our  unwillingness  to  even think  about  alternatives  to  on-call,  other  than  leaving  the  profession  for  a  role without it. Actual Solutions Summarizing everything up this point, the chain of argument is as follows. If we accept the preceding facts—that on-call is used for many things, but primarily to  cover  the  availability  gap  during  software  failures  provoked  by  unknown- unknowns, and that humans are not in fact very good at it and it is bad for them to do it, it seems natural to ask if there’s anything else we can do.  26 See, for example, Emily Gorcenski’s talk at SRECon Europe 2017.  Actual Solutions      519   Broadly speaking, we can try to make the existing situation better, or we can try to do something fundamentally new. For making the existing situation better, let us subdivide this into training, prioritiza‐ tion, accommodations, and improving on-the-job performance. Training Training  is  one  of  the  worst  problems,  which  is  surprising  given  that  it  is  also,  in theory, one of the easiest to solve. Part of the reason for this is because of the vexed question  about  when  in  a  career  we  might  expect  such  training  to  take  place.  For example, there is, as far as I’m aware, no academic-level treatments of on-call in the course  of  any  computer  science  degree  anywhere.27  Do  please  correct  me  if  I’m wrong. So, people new to the sector often come to the on-call portion of a job com‐ pletely unequipped to either support, critique, or modify the on-call situation they find when they get there. It is easy and very understandable to chafe at that responsi‐ bility and decide that the right thing to do is to  effectively  have others suffer. This does of course set up a lot of the bad relationships and contributes to incentive struc‐ tures that end up making it worse for everyone. The good news is that there are mul‐ tiple venues where best practices are discussed, so even if there is a dearth of publicly reshareable material—perhaps Site Reliability Engineering’s chapters on on-call and troubleshooting are a reasonable place to start, although PagerDuty’s training materi‐ als perhaps assume less about what the reader knows—it’s still possible to improve. In any event, there are useful, widely available supports on how to do so. Prioritization But the larger part of improving is actually wanting to improve. That is firmly in the domain of culture, as Cindy Sridharan controversially but not unfairly pointed out.28 In  this  context,  a  better  culture  would  involve  teaching  product  developers  that actually,  the  way  to  have  a  better  system  while  on  call  is  not  to  sternly  resist  any attempts to be put on call, but instead to prioritize the fixes and engineering effort required  to  make  it  better  when  one  does  go  on  call.  A  piece  of  software  that  is improved in its operational characteristics is a universally better piece of software  For some nonexhaustive arguments why, see the SRE book’s chap‐ ter on automation.  27 In contrast with medicine, as discussed earlier. 28 Cindy Sridharan, “On-call doesn’t have to suck”, Medium.com.  520      Chapter 30: Against On-Call: A Polemic   However, if a dichotomy emerges where product developers are the group most capa‐ ble of improving a system’s behavior, and yet are the most insulated from when the behavior is bad, nothing good will result from this broken feedback loop. The busi‐ ness might happen to be successful, but it will always be paying a cost for decoupling that loop—whether it is in resource costs, staffing attrition costs, or agility costs—and fixing that eternal cost is, in a way, part of what SRE promises. A similar argument applies to postmortem follow-up actions.  Accommodations Making accommodations for folks who are on call is also a key remediation. If more accommodations were made, on-call would inspire less fear, and more people would be able to do it. If more people were able to do it, the work would be spread across more people, which means more progress could be made. If more progress could be made, some time could be spent on operational clean-up work, and on-call would inspire less fear. A virtuous circle. Accommodations include but are not limited to: compensation for on-call work, par‐ ticularly  out  of  hours;  reasonably  flexible  schedules  so  caregivers  and  others  can move on-call work around  depending on how onerous the shift is, it can make even running the simplest of errands very hard ; support for recovery and follow-up after‐ ward;  and  mechanisms  for  those  who  are  literally  unable  to  do  it  to  be  excluded without backlash.  Compensation Many people still work in roles for which there is no on-call compensation, or cer‐ tainly  no  formal  scheme.  It  is  certainly  immediately  cheaper  to  run  a  company without one, but it is very much short-sighted. It is neither a good use of people’s lives nor morally correct. It’s also not even good for the business, despite short-term savings. Instead, we should have a well-articulated industry-wide model, or series of models so that each organization can pick the one that’s best for it, and the engineers working for it can pick and choose accordingly. A huge advantage of acknowledging the domain of human obligation in on-call  by providing supports for it  would be increasing  the  pool  of  people  who  elect  to  do  it,  with  consequent  benefit  for  team diversity, team members’ lives, and so on.  Flexible schedules To enable flexible schedules, you should address the sources of inflexibility. Manage‐ ment might need to be convinced that performance will improve and hiring will be easier with more flexible schedules. Hours of coverage can vary, depending on when the  statistical  likelihood  of  outages  happens.  A  formal  Service-Level  Agreement  SLA  helps immensely with not only Service-Level Objectives  SLO –based alerting, which often lowers raw paging numbers, but even the act of negotiating one from  Actual Solutions      521   where there was none before can help business owners to figure out that they don’t actually need to try  fruitlessly  for 100% availability. This in turn enables more flexi‐ ble schedules.  Recovery Most on-call compensation schemes that I’ve seen focus on enabling compensation or  time-off-in-lieu,  but  this  does  not  necessarily  happen  immediately  after  a  tiring shift. Companies could move to offering a morning off from the night before, after the activity during the shift had passed a certain threshold, outside of the terms of any time-off-in-lieu arrangement that might operate.  Exclusion backlash Making sure people feel safe on your team—no matter what their attitude to on-call is  or  capability  to  do  it—is  mostly  part  of  successful  line  management  rather  than company policy. But companies could signal in advance that they adopt a “opt-out of on-call without retaliation” policy, which would again help to attract more diversity in their workforce. Improving On-the-Job Performance On-the-job performance is in many ways the least important component of attacking the problem of on-call. For a start, it doesn’t actually advance the agenda of getting rid  of  it.  Additionally,  bad  but  passable  on-call  performance  is  unlikely  to  get  you fired. If it did, an organization might well end up removing more people from an on- call rotation than are needed, resulting in an ever-increasing spiral until the subset of people left behind burn out and quit. Instead, typically speaking, what happens is that only the people who make egregious mistakes are let go, and what most management cares about is a good faith effort to solve a problem, not consistently low Mean Time to Recovery  MTTR  metrics in a Taylorist-style29 scheme. Therefore, after the on-call person shows up and makes a good effort, there’s usually little enough incentive within the system itself to get better at it beyond not wanting to write yet another postmortem this week. Having said that, there are many techniques one can use to improve.  Cognitive hacks For a start, there are many cognitive “hacks” that can improve performance in on-call situations;  for  example,  frowning  can  help  us  to  feel  less  powerful,  which  in  turn  29 Frederick Taylor was a scientific management theorist who introduced the all-too-successful idea of dehu‐  manizing people in work situations by closely managing metrics.  522      Chapter 30: Against On-Call: A Polemic   makes us less likely to “anchor” on specific scenarios to the exclusion of all else; read‐ ing John Allspaw’s materials on blameless postmortems can also help to shake out assumptions about what is actually at fault when things go wrong in distributed sys‐ tems, and helps to increase self-reflection, which is a crucial component of avoiding pure reactivity. As discussed earlier, reacting is a serious impediment to finding out complex truths, and there are no other types of truth in distributed systems. Another potential hack is doing pair on-call; it has the very useful effect that when you  need  to  explain  yourself  to  someone  else,  having  to  articulate  your  idea  often helps you figure out if you’re wrong, just by saying it. This is different from active  inactive  primary secondary  shifts  because  it  implies  that  the  people  are  working closely together interactively during the shift.  This is most tractable during business hours and is hard to organize with small teams.  Aggressive hypothesizing, which is the technique of tossing out idea after idea about what’s going wrong, all as different as  possible,  can  also  be  useful.  This  is  particularly  so  when  a  number  of  things  go wrong at the same time, due to the fact that increased stress narrows the mind. It also helps to correct for cognitive bias problems like anchoring. Finally, good discipline, such as always maintaining hand-off documents and following a well-drilled incident management procedure, is necessary to coping effectively with rapidly moving inci‐ dents; the “guide rails” provided by extensive drilling on a procedure helps you to react correctly in uncertain situations. We Need a Fundamental Change in Approach As  useful  as  all  of  these  practical  recommendations  are,  there  is  something  funda‐ mentally unsatisfying with always chasing something you’ll never catch up with. A more interesting question, then, is what different things can we do to fundamentally change this situation? Of course, this depends on how you characterize the problem. Recall the central con‐ flict at the heart of software systems: we know that they are deterministic,30 yet they continually  surprise  us,  in  the  worst  of  ways,  in  how  their  complexity  and  their behaviors interact to produce outages. But how does that unreliability arise? A survey of Google outages in The Site Reliabil‐ ity  Workbook  suggests  that  binary  and  configuration  pushes  together  constitute almost 70% of factors leading to an outage, with software itself and a failure in the development process turning up in almost 62% of root causes, and “complex system behaviors” being in only around 17% of postmortems. Remember these numbers, or at least the fact that analyzed outages have a structure to their causes.  30 So they keep saying, anyway.  We Need a Fundamental Change in Approach      523   At this point, let me introduce two positions that I refer to in the rest of this chapter; Strong-Anti-On-Call  SAOC  and Weak-Anti-On-Call  WAOC . Strong-Anti-On-Call Strong-Anti-On-Call  SAOC  runs as follows. Software systems are deterministic. We have two possible approaches to attacking the problem of outages. We can either remove the sources of the outages, or we can pre‐ vent outages from having a catastrophic effect. Because software systems are deter‐ ministic, if we remove all sources of outage, the system will not fail.  SAOC does not believe that doing both of these things is useful.  Let us take a moment to recapitulate what the sources of unreliability are when con‐ structing a software system. We can make a simple programming error, a typo, or equivalent. We can make a design error, constructing something that is guaranteed to go  wrong.  We  can  insulate  ourselves  from  the  environment  incorrectly   libraries, dependencies, or simply parsing data incorrectly . We can treat remote dependencies incorrectly—for example, behaving as if they will always be reachable or will always return correct data. As discussed earlier, the maddening thing about outages is that so many of them are entirely  avoidable.  The  SAOC  position  involves  preventing  each  of  the  identified sources of error in the list of categories in “Underlying Assumptions Driving On-Call for Engineers” on page 510 from manifesting in your system. Although there is too much  to  address  in  detail  here,  the  good  news  is  that  many  of  the  difficulties  of change  management  are  already  well  understood,  and  what  we  are  trying  to  do  is more successfully implement something that is already relatively well understood. The bad news is that, if we keep getting it wrong, there might be a reason for that. However, to SAOC, this does not matter. We’ll get it right eventually. But we do have one hurdle to overcome first, which is the causes of outages that result from larger interactions, not necessarily system failures within the system itself or from simple “one-step” interactions, such as ingesting bad data. So, we need to take the complexity out of our systems. Practically speaking, there are really only two known ways of doing that: building simple subcomponents with veri‐ fied,  known  behavior,  composed  in  deterministic  ways;  and  running  systems  for  a very long time to see where we were mistaken about their stability and then fixing those problems. Today, as an industry, we make it too easy to write unreliable soft‐ ware, by not consistently taking advantage of those two techniques and facilitating the fact that it’s still software engineers finding it easier to write code than read it that creates unreliable code.  524      Chapter 30: Against On-Call: A Polemic   Instead, I argue we need to change the basic layer at which we build software. Today that is POSIX libc, win32, or the logical equivalent; in the future, particularly the dis‐ tributed future, it must be at a higher level, and with more cross-cloud  or at least, cross-platform  features. To write a server, a product developer should pluck a well- known class off the shelf, with good monitoring, logging, crisis load handling, grace‐ ful degradation, and failure characteristics for free and enabled by default. It should be actively difficult to write something bad. Innovation long ago moved out of the plat‐ form layer, yet we pay so much for the right to rewrite from the ground up, and gain so little in return, it is difficult to understand why we tolerate it. Simultaneously, we need better ways to insulate bad application layer logic from the rest of the platform; today, across the industry, such insulation is essentially infinitely gradated across the spectrum—from none at all, which is unfortunately common, to completely isolated, which is unfortunately uncommon. Again, this speaks to a pre‐ existing toolkit that is easier to use than not. Thus, SAOC finds itself arguing for a methodical approach to eliminating sources of error  and  a  toolkit  approach,  not  because  of  resilience,  but  because  of  eliminating complexities of interaction. There are a number of weaknesses to this position, but one  glaringly  obvious  one  is  that  the  methodical  elimination  is  only  ever  going  to happen after a software system is deployed; ideally, we would be reusing only pieces of  software  to  which  this  elimination  had  already  been  applied.  That  argues  for  a toolkit approach again, except one in which the software components have already been “ground down” to deterministic subcomponents. Weak-Anti-On-Call If you have come this far but do not agree that the strong case is either practical or worth striving for, allow me to convince you of a slightly weaker but still useful posi‐ tion. In this view of the world, you might indeed believe that software is deterministic, but you are convinced that we will never be able to react successfully and programmati‐ cally  to  unknown-unknowns  and  that  interaction  between  arbitrarily  complicated systems  will  always  produce  failure  of  some  kind.  We  can  still  make  progress  on eliminating on-call, except we need to look at it in a different way. Unlike with SAOC, we are not trying to eliminate emergency medicine by replacing it with ward medicine. Instead, we are trying to implement driverless trains: systems for which we know we cannot control the environment completely, but we more or less have one useful reaction  stop the individual train, if not the entire system , and the question is to what extent we can stop the train and not imperil the system as a whole until the human driver arrives.  We Need a Fundamental Change in Approach      525   This  approach  therefore  relies  not  on  attempting  to  prevent  human  intervention from occurring, but on delaying it until some notion of “business hours” arrives, or otherwise removing it from the domain of emergency medicine. In this view of the world, the most important thing to do is not to prevent outages, but to insulate the system against failure better. The strange thing is that this position actually  looks  quite  similar  to  the  previous  one:  we  need  the  same  thing,  reusable standardized toolkit software, except instead of optimizing for complexity reduction, we optimize for the equivalent of driverless train failure: we automatically stop in the safest way possible, depending on what we can learn about the circumstances. It is interesting to consider how much thought a typical engineer puts into having a soft‐ ware system fail safely as opposed to making forward progress successfully. Failure cases  are  often  disregarded  in  the  hazy  optimism  of  a  favored  editor,  and  there  is every reason to suppose that we would be more successful at engineering resilience if this was more of a focus or, more important, if more guide rails were provided to do so. The details of safe failure would vary so much from domain to domain it is not effi‐ cient to talk about them here, but the key principle is again that the work performed should  be  amenable  to  partitioning  such  that  components  of  the  system  that encounter fatal errors can be safely removed, with no significant impact on system capacity, or at least a limited one; enough to give the human operator a little leeway to deal with it in the office. Then it is a matter of scaling the system such that the failure domains meet this goal, and then you can avoid on-call, safe in the knowledge that you might be drastically over-dimensioned in hardware, but at least you won’t be paying attrition-related staff costs. A Union of the Two Yes, there is nothing to stop us combining the viewpoints of both approaches. In fact, even though the theoretical positions are quite different, you have already seen that both of them call for same remedies: in particular, standardized toolkits for the con‐ struction of software, albeit for different reasons. It seems clear to me as, a result, that the industry needs put effort into making very reliable subcomponents, out of which most services could be composed; they might even be proved formally correct,31 harking back to the cloud stack conversation out‐ lined earlier. Yet I agree that it feels a little unrealistic to suggest this in a world where a  startup  can  become  wildly  successful  with  some  hacked-together  Ruby  that  just happens to be a good product-market fit for a particular use case: today, what drives usage is the product-market fit, not the operability.  31 This is not as ridiculous as you might think; for example, AWS uses formal methods.  526      Chapter 30: Against On-Call: A Polemic   Furthermore,  as  long  as  a  huge  multinational  corporation  finds  it  cheaper  to  hire operations engineers to take pagers and reboot systems than to fix real problems with their software operability, there cannot be any real change. For real change, we must fix the problem at its point of origin: we must change how easy it is to write unrelia‐ ble software. We must make it, if not impossible, then actively hard to write poorly operable software. Anything else will not lead to a fundamental change. The benefits of standardization on a cloud stack also play to the assumption of the SAOC position: it is way easier to methodically eliminate sources of error if you’re not  rediscovering  them  separately  inside  your  company  each  time.  A  single  attack surface, inspected by many eyeballs, will surely converge more quickly than multiple attack surfaces inspected by fewer. Conclusion To get rid of the scourge of on-call will require an industry-wide effort, given that what we are proposing is collaboration on toolkits explicitly designed to be rewritten as little as possible, yet used by as many people as possible. But if we did do this, the benefits would be incalculable: an industry-wide set of archi‐ tectures so stable they could be taught in schools; a consistent approach to layering business logic; a more welcoming environment for caregivers and minorities; a set of methodically applied best practices and consistent data processing across companies, never mind within teams in the same company, never mind within individuals in the same team! It might sound impossible, but in essence is a task of convergence. As an industry and indeed  in  society,  we  have  in  the  past  converged  on  odder  things:  VHS,  the  x86 instruction set, and English come to mind, but there are surely more. Now the task is to push for it, because it would be of benefit to all, even if it is only us right now who see what the future could be.  Niall Richard Murphy has been working in internet infrastructure for over 20 years, and is currently Director of Software Engineering for Azure Production Infrastructure Engineering in Microsoft’s Dublin office. He is a company founder, an author, a pho‐ tographer, and holds degrees in computer science and mathematics and poetry studies. He is the instigator, coauthor, and editor of Site Reliability Engineering and The Site Reliability Workbook.  Conclusion      527    CHAPTER 31 Elegy for Complex Systems  Mikey Dickerson, Layer Aleph  formerly United States Digital Service   March 19, 2018: Farmhouse Motel, Paso Robles, California. Diverted here and lost a day on the way south, because of a rainstorm that was too much for a motorcycle. 18 hours left to make it 220 miles to the next appointment. This isn’t going to plan. Nothing ever goes to plan. Six years ago, I was a middle manager in SRE at Google. I was about three years into an enormous, and mostly unnecessary, project to reinvent MySQL on Google storage, and my last two promotion applications had not been read by anyone. I was, in the phrasing that our HR person liked to use, “loose in the saddle.” The high-energy col‐ lision that sent me completely out of orbit was that I was asked by President Obama to  attempt  a  last-ditch  rescue  of  healthcare.gov,  the  so-called  “federally  facilitated marketplace” that was threatening to sink the Affordable Care Act, and with it, the concept of universal healthcare for another generation. The healthcare.gov rescue grew into a round-the-clock effort by a couple dozen SREs, mostly from the Bay Area. Most of them today agree they would not trade the experi‐ ence for anything, and also that they would never wish a similar experience on any‐ one  else.  The  rescue  in  turn  grew  into  a  new  government  entity,  the  US  Digital Service. And we were set loose on the most stubborn IT issues in the US government, most of which started out as an attempt to replace one large and complex system with another. There’s a linguistic clue to the first major challenge: Who talks about “IT” anymore? Well, an enormous, sprawling, “enterprise”-industrial complex does. The federal gov‐ ernment alone spends more than $80 billion a year on what it calls “IT.” The Nation of IT was founded approximately in the post-WWII era, the idea being to replace the paper  and  analog  processes  of  every  big  bureaucracy  with  computerized  versions.  529   Forms became screens, filing cabinets became databases, memos became emails. Tre‐ mendous improvements in efficiency were realized and then gradually forgotten as the infrastructure became geriatric and developed its own, all-new problems. The Nation of IT’s jargon, job titles  “CIO,” “enterprise architect” , and technology  mainframes and PCs  calcified and did not adapt to the discovery of the internet, an event which has made a lot of people angry and is widely regarded as a bad move. Those of us at Google or Facebook in the 2010s belonged to what might as well be another nation. The Nation of Tech has new buzzwords and new technology fads. Culturally, it claims not to care about job titles, hierarchy, or fashion; none of these claims stands up to scrutiny, but they are important to an understanding of how Tech sees itself. The Nation of Tech refers to anything from the Nation of IT as “legacy.” IT refers to anything from Tech as a “fad.” Lost somewhere in the middle is the fact that the core problem—the problem that hobbles the best efforts of both nations—is unchanged: we are much better at building complexity than at managing it. Somewhere between Google, the IRS, Medicare, the Department of Defense, the FBI, electronic medical records, and a dozen others, I have become convinced that all suf‐ ficiently complex systems have a number of behaviors in common. The Computer and Human Systems Cannot Be Separated It was 1967 when Mel Conway asserted that “all organizations which design systems are constrained to produce designs which are copies of the communication structures of  the  organizations.”  It  seems  as  if  every  organization  since  1967  has  considered “Conway’s Law” an affront and has attempted to disprove it. They rarely succeed. And by “rarely,” I mean “never, only I don’t feel like arguing about  it.”  There  are  good  reasons  why  software  components  end  up  arranged  like human components. In either case, the large system cannot function unless the inter‐ faces between components are carefully defined and change slowly. Whatever they may be called, they are contracts that let each side rely on an abstraction that tells them what to expect from the other side. When  a  system  falls  into  chronic  dysfunction,  the  technical  problems  and  people problems  are  inevitably  congruent.  Show  me  two  software  components  that  fail  to interoperate, and I will show you two teams that do not interact. A week ago, I inter‐ viewed people at a large team whose project has a basic requirement of being able to execute transactions on a DB2 database on a mainframe. In two years, no one has been  able  to  do  this.  With  some  difficulty,  we  located  one  of  the  contractors  that maintains the database. He arrived about 10 minutes late and explained that it took him a while to find the conference room because he’d never been to this end of the building before.  530      Chapter 31: Elegy for Complex Systems   If you are called on to diagnose a system that does not have a tidy issue with a “root cause,” but rather seems to be misfiring all over and not quite right anywhere, a great place to start is by drawing two diagrams, an org chart and a system flow diagram, to see how badly they have diverged. Remember that they are both mutable, and a solu‐ tion can arise from changes in either one. Decoherence and Cascading Failure Any system of any size can get a simple question wrong, such as rendering an incor‐ rect decision on an insurance claim, or the wrong billing address for a customer. But only a large, distributed system can disagree with itself. This special failure mode in which  the  system  is  not  able  to  render  any  answer  at  all  is  sometimes  called decoherence. As always, there are abundant examples in humans and software. Anyone who works on distributed storage is accustomed to the problem where multiple replicas, chunk servers, or tablet servers have become desynchronized. In such a situation, the answer to  a  query  depends  on  which  server  you  ask,  which  violates  the  abstraction  the designers have tried to create. Typically, the solution is some sort of voting scheme in which one version of the data wins, and the minority opinion is quietly deep-sixed, never to be spoken of again. Theory papers usually assume that the majority opinion will be “correct.” Human examples are even easier to find. At the time of this writing, the US legal sys‐ tem is unable to answer the question “is marijuana legal.” The answer is both yes and no at the same time, and it depends on which court you ask. Thousands of veterans are both alive and dead according to the VA and its computers. Errors in the voter registration records are everywhere, and also almost nonexistent. Decoherence  has  a  well-known  relative  called  cascading  failure.  When  a  system  is tightly coupled, a failure in one component tends to cause failures in others that are connected. These days, half the fun of the job derives from the fact that “connected” can have many surprising definitions. You probably know how your software com‐ ponents exchange data according to the architecture diagram. Do you know which ones run on the same machines? Or share a network switch? Or a power distribution unit? Or BIOS version? If you are responsible for a large system for long enough, you will. When debugging incorrect outcomes rendered by a complex system, always remem‐ ber that it is not enough to look for a “root cause,” popping out of hiding now and then to flip bits—probably while cackling and twirling its mustache. You might be looking at nonintuitive results of the error-correction algorithms that protect the sys‐ tem against disagreements with itself. These algorithms are only sometimes designed on purpose.  Decoherence and Cascading Failure      531   Always in a State of Partial Failure Working backward through the wreckage of cascading failures in systems that you maintain, you will realize that various parts of the system had already failed before the catastrophe. Large systems exist in a state of partial failure at all times. This is an inescapable fact of the interaction between Mean Time Between Failures  MTBF  and Mean Time to Repair  MTTR . Operations groups often take on big projects to increase MTBF and decrease MTTR, usually at the level of hardware components, because this is the only level at which assumptions of rationality hold well enough for “mean time to anything” to be well defined. It’s certainly worthwhile for a team to optimize within one “accountability domain” like this. Even so, when you’re looking at the entire system, an increase in fault toler‐ ance beyond “barely acceptable” tends to be immediately eaten up by another layer. Suppose  that  you  have  a  distributed  storage  system  that  was  deployed  to  tolerate three simultaneous disk failures in an array. Then the hardware team, full of gump‐ tion and wishing to be promoted, takes clever measures to “guarantee” that the array will never have more than one disk down. Check back in a year, and you will find that the  application  has  been  reoptimized  and  can  now  tolerate  only  one  failure.  Why not? A statistical retrospective showed that this would “always” be good enough. The right hand giveth nines, and the left hand taketh them away. A large number of teams  and system components, which are the same thing  inter‐ acting will exhibit many examples of this give-and-take, and since human accounta‐ bility is diffused, the laws of evolution take over. Just as in biology, evolution does not produce  the  optimum  solution;  it  produces  a  solution  just  good  enough  to  get  by. The natural state of such a system is “one failure away from catastrophe.” A great illustration of the cycle of how safety margins are created and consumed and how failures accumulate unnoticed in the progression toward “one failure away from catastrophe” is found in the after-accident investigations of both the Columbia and Challenger  space  shuttle  disasters.  These  were  tightly  managed  works  of  complex engineering, in which every component was over-built to ensure incredible reliabil‐ ity. And the final safety record of the shuttle system was two catastrophic failures in 135 flights. If  you  should  be  so  lucky  as  to  review  the  design  of  a  new  application  before  it  is launched, watch out for plans that assume infrastructure components will perform to published Service-Level Agreements  SLAs . To do so is to consume all of the safety margin of the underlying layers. At the very least, try to design for the performance measured and documented in the historical record.  532      Chapter 31: Elegy for Complex Systems   Novelty Priority Inversion I have mentioned engineer promotions twice now, and not  merely  to be facetious. Because  the  human  system  and  the  computer  system  are  isomorphic,  the  human  incentive structure is the invisible hand that drives the change in the computer sys‐ tem over time. The desire of the hardware maintainers to optimize for MTTR is pre‐ dictable  because  it  will  get  them  promoted.  The  desire  of  the  storage  operators  to optimize the new safety margin away, and cut costs, is predictable because it will get them promoted. As  someone  who  was  once  responsible  for  staffing  dozens  of  engineering  projects that  meant  life  or  death  to  thousands  of  people,  I  have  recruited  on  “mission”  as much as anyone. Mission will carry you a good distance, and when you have little money to work with, a good one sure helps. Someday burnout will catch up with even the most pure-hearted, and quality-of-life concerns become dispositive. The values of the Nation of Tech being what they are, it is rare for a successful engi‐ neer to be rewarded with less work to do. That leaves us with “quality of life” defined as money. This is not a culture that is leading to great outcomes, but it is what the Nation of Tech has. Money,  in  turn,  is  governed  by  bureaucratic,  heavyweight,  bonus  and  promotion processes.  So,  these  processes  determine  the  long-term  behavior  of  your  company and every system you manage. What do they reward? Ignore what the company says it rewards; instead, look at the list of who was promoted. Behaviors associated with these people will be emulated. Behaviors associated with those left behind will not. This  evolutionary  pressure  will  overwhelm  any  stated  intentions  of  the  company leaders. This is the oracle to consult any time you wonder whether to start a new project, or help an adjacent team, or pay the cost of adopting the latest new thing. You can still choose to row against the tide. For a while. But it will always help to know which way all the other boats are going to drift. Nobody Anticipates the Overhead of Coordination Heavyweight promotion processes bring to mind the last point, which is that nobody, ever, anticipates how expensive it is to communicate across a large system. Legend has it that the Pentagon was designed so that it is possible for a messenger to travel from any location in the building to any other location in about 8 minutes. No one is quite sure if this is true. But it was built under the existential crisis of World War II and meant for coordinating decisions as fast as possible across a large organi‐ zation. The first thing that might strike you about the building is the corridors are as  Novelty Priority Inversion      533   much as 40 feet wide along main routes. This gives a clue to how many people and objects the architects expected to be in transit at a given moment. The Pentagon reminded me of Google designs for internal cluster networking, which were  meant  to  offer  wire-speed  transfers  between  any  two  points  in  the  cluster   a similar constraint . These ended up being phenomenally expensive to implement. It is likely that there were more watts and more CPUs routing packets than doing any other work in the cluster. Tech  companies  introduce  new  tools  or  programming  languages  all  the  time.  An engineer might need to spend a couple dozen hours in a quarter, or fewer, to learn shiny new things, yet plans will be made for training, classes, workshops, announce‐ ments, and support staff. The same engineers will spend a couple dozen hours a week on meetings, video con‐ ferences, and phone calls. I have never seen a curriculum designed to teach the effec‐ tive use of any of these tools, much less a training or workshop to help cope with the introduction of a new remote office, possibly in a different company, with a different culture. If you are responsible for the smooth operation of any large human or computer sys‐ tem, pay attention to these costs. They become the largest sink of your watts, or calo‐ ries, much sooner than you think. Your healthcare.gov Is Out There These few ideas will not be everything you need to know to maintain and repair large complex  systems.  They  are  ones  that  I  think  tend  to  be  overlooked  and  under- appreciated. And they are by no means specific to the narrow concept of “large com‐ plex system” that SREs usually look at. A few of us, former SREs from the Nation of Technology, found that to be true and left those settled posts in the hierarchy a few years ago. Now we spend our time at the uneasy frontier between IT and Technology, studying and occasionally solving prob‐ lems for large and important human–machine systems. This is how I came to leave Mountain View for a godforsaken beige office park in Virginia, then Maryland, then the White House Situation Room, and last week, Sac‐ ramento to look at the California child welfare system. Which is how I come to be in this motel, on the road between the Bay Area and Los Angeles. Where it is time to go to  sleep,  because  I  have  to  get  up  early  enough  to  make  it  220  miles  tomorrow. Tomorrow there will be another important system that is a deeply entangled human– machine hybrid, one step away from catastrophe. Maybe I’ll see you there.  534      Chapter 31: Elegy for Complex Systems   To Get Involved If these sorts of problems interest you, and you are looking to work on harder prob‐ lems in much less comfort, consider getting in touch with any of the following: Layer Aleph LLC, Ad Hoc LLC, Nava PBC, or Truss. Each group is a little different, but they all contain fine people that often run into one another at the scene of a big-old tire fire. Further Reading Let me be the first to say that I might not have written a single original idea in this chapter. If you would like to read more, and in the original English, please consider the following:    Inside Bureaucracy by Anthony Downs, published by the RAND Corporation in 1967. It’s out of print, but you can usually find it on Amazon  where I have per‐ sonally driven up the price by 300%; sorry . Do not confuse this with the 36-page paper published with the same title in 1964. The paper is also good, and you can download it from RAND for free, but it is a tiny fraction of the book. If you read only one book, read this one.    The  Tyranny  of  Structurelessness  by  Jo  Freeman   aka  Joreen ,  1970.  Printed, reprinted, edited, and copied in dozens of places in subsequent years, but cur‐ rently canonically available at jofreeman.com. If you read only one article, read this one.    The  Mythical  Man-Month  by  Fred  Brooks,  1975.  Of  course.  Covers  Conway’s  Law and a lot more.    The Rogers Commission report on the Challenger disaster, published in 1986. But mostly Appendix F, being the “personal observations” of Richard Feynman, who could not reach any agreement with the rest of the committee and threatened to remove  his  endorsement  from  the  report  if  his  chapter  was  not  included.   In homage, and out of the same necessities, there are a handful of Obama-era inter‐ nal policy papers that include an appendix singly authored by one Mikey Dicker‐ son. I don’t think anyone ever realized what I was doing.     How Complex Systems Fail published by Richard Cook at the University of Chi‐ cago in 1998, available online. Also presented at O’Reilly Velocity 2012, which is on YouTube.    Engineering a Safer World by Nancy Leveson, MIT Press, 2012. Professor Leve‐ son  has  published  dozens  of  papers  on  “systems  safety,”  across  three  decades, which you can find on her web page. She has taught workshops and consulted for dozens of companies and government agencies.  Further Reading      535     And  anything  you  can  find  on  the  history  of  nuclear  accidents  will  be fascinating—two  good  surveys  are  Command  and  Control  by  Eric  Schlosser, 2014, and Atomic Accidents by James Muhaffey, 2015.  Mikey Dickerson graduated with a math degree from Pomona College. After various odd jobs, he was hired by the obviously desperate Google Site Reliability Engineering department  in  2006.  He  was  eventually  promoted  into  middle  management.  Having thus achieved every child’s dream, he was ready to move on to help the Obama admin‐ istration repair healthcare.gov in 2013, then accepted a White House appointment to run the US Digital Service from August 2014 until the end of the administration.  536      Chapter 31: Elegy for Complex Systems   CHAPTER 32 Intersections Between Operations and Social Activism  Emily Gorcenski and Liz Fong-Jones  Measurement,  risk  mitigation,  crisis  response,  and  managing  long-term  follow-up are well-accepted parts of SRE and broadly of the discipline of software operations. We  do  design  reviews  to  head  issues  off  early,  measure  key  service  indicators,  use incident management structures to manage complexity during outages, write post‐ mortems, and guide our future work, based on what we learn. Our focus on measure‐ ment and data enables us to better advocate for users. The  collaborative,  multidisciplinary  approach  of  SRE  involves  coordinating  many different stakeholders, and requires individuals to work together to avoid becoming overwhelmed  by  complexity  or  burned  out.  Managing  human  factors  is  often  the most important skill that SREs learn. But  our  job  as  engineers  does  not  stop  purely  with  adherence  to  Service-Level Objectives  SLOs . A service that does a reliable job of harming people, exacerbating injustices,  or  excluding  marginalized  groups  is  not  a  service  worth  building  and maintaining. Technology is poised to change the world, for good or for ill, and engi‐ neers of all kinds share a responsibility to ensure that their work is “for the public good” and “does not diminish quality of life, diminish privacy, or harm the environ‐ ment.”1 Therefore, we must turn our attention to how we ensure that our work is just and serves the public interest. Fortunately, successful activism for social change requires the  same  skills  of  measurement,  risk  mitigation,  crisis  response,  and  managing  1 ACM Code of Ethics  537   follow-up as SREs have honed through our professional practice. We can apply these skills  to  advocating  for  justice  in  the  products  we  build,  the  broader  industries  we work in, and the communities we live in. In this chapter, we provide an overview of how social activism movements such as those that Emily participated in as an antiracist organizer in Charlottesville, Virginia, collectively organize; describe their similarities to SRE; and then suggest ways to par‐ ticipate in social change that is meaningful to readers according to their own princi‐ ples, both in the spheres of technology and the broader world. Emily approaches this case study from the perspective of an activist in tech and data ethics and who lives in a city where a deep legacy of anti-Black racism has left enduring scars. Liz’s career as an SRE has included forays into product inclusion and working-conditions advocacy, for which SRE skills better prepared her. Through this case study, we also see how activists have learned similar lessons as SREs about scaling incident response, devel‐ oping empathy, and managing burnout. Before, During, After But Mousie, thou art no thy-lane, In proving foresight may be vain: The best laid schemes o’ Mice an’ Men Gang aft agley, An’ lea’e us nought but grief an’ pain, For promis’d joy!  —Robert Burns, “To a Mouse”  To an outsider, the roles and responsibilities of organizers and engineers might not appear  to  have  much  in  common.  When  we  see  media  coverage  of  protest  move‐ ments, strikes, and other popular demonstrations, we witness the result of weeks or months of behind-the-scenes labor. Political organization often relies on the tireless efforts of experienced people with diverse skills. But, just as developers cannot build a product  requiring  no  operations,  even  talented  planners  with  broad  skills  cannot foresee every possible circumstance. Good organizers, like good engineers, will not optimize for the elimination of errors in the moment, but rather position themselves to build resilient structures that will respond most effectively when unforeseen break‐ downs occur. The workflows of political organization can be examined through the same lens as those of software engineering. These efforts are often broken down into pre-action, action,  and  post-action  phases,  much  like  software  engineering  cycles  can  contain sprint  planning  and  launch  preparation,  development  and  deployment,  and  retro‐ spective phases. Larger demonstrations, like major software releases, are often built up through series of smaller, more granular efforts leading up to the main event.  538      Chapter 32: Intersections Between Operations and Social Activism   Creating the Perfect Plan All  efforts  must  begin  somewhere,  and  planning  is  a  critical  aspect  of  designing, building, and launching anything, whether it be a piece of software or an advocacy campaign. Political actions are the result of tireless dedicated hours spent planning, organizing, staging, and coordinating people, groups, institutions, and resources. The problems organizers of these events face will be familiar to any engineering manager: asymmet‐ ric  distribution  of  talents,  scarce  resources   usually  in  terms  of  time,  money,  and labor , contingency planning, and so forth. In  the  summer  of  2017,  local  political  activists  in  Charlottesville  became  acutely aware of these planning issues. In June of that year, the community learned of the two major events that would go on to shock the community: the Ku Klux Klan rally on July 8, and the “Unite the Right” rally on the weekend of August 12. Charlottes‐ ville is a small city with an even smaller activist community; the task of planning non‐ violent counter-demonstrations for these events fell to a few dozen people. The local community  knew  that  the  August  12  Unite  the  Right  rally  would  be  bigger,  more dangerous, and potentially more violent than the KKK rally. As a result, local activist organizations spent very little time preparing for July 8 until just a few days before. Nevertheless, organizers were able to quickly marshal community resources and scale operations  to  oppose  the  KKK.  By  engaging  with  groups  and  organizations  rather than individuals, planners could delegate critical work to people who were not neces‐ sarily central to the planning process. This work included preparing and transporting food,  water,  and  other  wellness  needs   to  prevent  hypoglycemia  and  dehydration ; planning legal support in the event of arrests; preparing medical and mental health support; creating and distributing educational materials; in-action communication  coordination training; offsite support  from those not in physical attendance at the rally ;  social  media  monitoring;  press  and  conventional  media  engagement;  and more. Many of these efforts have parallels in the engineering world: we coordinate meals for our engineers on release day; we prepare legal teams to review copy and terms of ser‐ vice;  we  create  documentation  and  prepare  marketing  materials;  we  have  external and internal comms plans; we staff people to watch Twitter to track social trends and identify bugs; and we work with the media to ensure product launches receive appro‐ priate coverage. Indeed, the modern project manager has a very similar role to the modern political organizer. However,  activists  often  speak  fondly  of  nonhierarchical  organizing  models.  This may  seem  odd  to  engineers  accustomed  to  being  held  accountable  by  managers, shareholders, clients, and teammates. It is important to note that a lack of hierarchy does not necessarily imply a lack of accountability. In fact, by holding organizations  Before, During, After      539   rather than individuals accountable for specific, measurable tasks, activists are often able to get work done through the process of coalition-building. A central element to this concept is an implicit understanding that not every organization will share every outlook  or  value,  but  that  in  the  context  of  the  present  struggle,  the  organizations occupy enough common ground to get work done. A common model used by activists is a spokescouncil. In this model, organizations are held  accountable  to  their  commitments  as  groups,  rather  than  individuals.  The organizations involved are each represented by a spokesperson, who shares their con‐ cerns,  needs,  and  capabilities,  and  receives  those  of  other  organizations,  as  well. When this model operates well, each organization is able to voice its concerns and offer its best gifts, even when some of those are in conflict with other groups. Struc‐ turing responsibility and action around groups, rather than individuals, helps avoid placing the blame on specific people in what is sure to be a high-stress, often danger‐ ous or deadly, environment. Importantly, it empowers autonomy and allows people to work most closely with those in whom they trust the most. Principles of Organizing In 2008, activists organizing protests against the Republican National Convention in St. Paul, Minnesota, developed a set of operating guidelines to encourage disparate groups to work toward a common goal. Known now as the St. Paul Principles,2 these guidelines seek to aid coalition building. Although one may not agree with the poli‐ tics of their originators, the St. Paul Principles generally provide a set of guardrails for preparing for high-stress, high-conflict scenarios. The four principles are as follows:    Our solidarity will be based on respect for a diversity of tactics and the plans of  other groups.  space.    The actions and tactics used will be organized to maintain a separation of time or    Any debates or criticisms will stay internal to the movement, avoiding any public  or media denunciations of fellow activists and events.    We  oppose  any  state  repression  of  dissent,  including  surveillance,  infiltration, disruption, and violence. We agree not to assist law enforcement actions against activists and others.  Although the vast majority of the services we operate as SREs are not as life-or-death critical as situations encountered by protesters, they still have immense impact upon the world. Therefore, we similarly need to ensure that we have psychological safety  2 D’Arcy, Steven.  2014 . Languages of the Unheard: Why Militant Protest Is Good for Democracy. London: Zed  Books.  540      Chapter 32: Intersections Between Operations and Social Activism    see  Chapter  27 ,  have  trust  among  our  teams,  and  have  planned  out  how  we  will bring chaotic situations under control.  Principles 1 and 2  interfaces and incident command  SREs can interpret the first two principles as an equivalent to our understanding that teams need to respect one another’s scope of responsibilities and plans. In incident management, each individual is empowered with a specific scope, and random free‐ lancing outside of the parameters of engagement is discouraged. In organizing, this often means keeping your tactics and actions physically separated from others’ so as to contain risks. Model-View-Controller paradigms, virtual machines, containers, and microservices are all highly specific variations on this theme. SREs define clear interfaces between, and SLOs for, systems and ensure that each component has a specific role, to avoid a tangled web of interdependencies or dependence upon implementation details. The operational principle is of isolation of the consequence of failures.  Principles 3 and 4  blameless retrospectives and psychological safety  The parallels of Principles 3 and 4 to SRE lie in how we approach postmortems and blameless interactions with each other during and after an incident. We must agree to  avoid  blameful  behavior  and  keep  our  discussions  about  how  to  prevent  future failures constructive. If we do not foster this atmosphere of collaboration, our collea‐ gues will be unable to fully trust one another and we will fail. Principle 3 involves acknowledging the realities that people are susceptible to the out‐ side world. Our teams, coalitions, companies, and communities must be aware of this in  the  lead-up  to  major  events.  Solidarity  is  the  goal,  even  when  things  go  badly. Honoring the good-faith commitments of our teammates or comrades is necessary. In  practice,  upholding  Principle  3  means  that  hard  discussions  are  often  required early and often, before the magnitude of failure makes it more difficult to keep the underlying issues private. This can be a very unpleasant process, and it has sundered many a relationship in organizing spaces. Managing interpersonal conflict is univer‐ sally challenging. To  activists,  it  is  critically  important  to  manage  interpersonal  conflicts  before  the action. Life-altering outcomes are expected in political activism; in the worst of cases, lives  have  been  lost.  Activists  must  be  able  to  trust  one  another  and  must  be  held accountable to one another. Proper preparation will instill a clarity of roles so that individual actors can focus on their own responsibilities to avoid making chaotic sit‐ uations worse.  Before, During, After      541   Managing Crisis: Responding When Things Break Down Despite the best planning and best intentions, reality is messy. Indeed, the purpose of planning is less about having a prescribed set of behaviors with foreseeable outcomes and more about developing the habits, intentions, and trust to drive a dynamic sce‐ nario toward the most favorable result.  Handling chaos: contrast in responses during the July 8 KKK rally On  July  8  in  Charlottesville,  almost  2,000  community  members  arrived  at  Justice Park to oppose the messages of bigotry and hate being brought to the Charlottesville community by 50 or so members of the Loyal White Knights of the Ku Klux Klan. The vast majority of these counter-demonstrators did not attend planning sessions and  did  not  train  for  the  event.  As  a  result,  organizers  could  not  coordinate  their actions and responses. In technology terms, the community counter-demonstrators were  our  customers  of  our  process,  behaving  in  often  inscrutable  and  frustrating ways. But this behavior was not entirely unpredictable. Local activists prepared for the community response through role differentiation and relied on experience with managing  potentially angry and violent  crowds. Activists came prepared with extra banners and signs for people to carry. Outreach volunteers distributed educational materials explaining elements of social justice and activism. And chant-leaders corralled and led the crowd in ways that ensured that large gather‐ ings of people didn’t linger long, avoiding violent encounters on a hot summer day. Additionally,  activists  had  a  plan  for  civil  disobedience  and  prepared  to  physically blockade the KKK from entering the park by standing in front of the entrance and interlocking arms. Blockading, an act of unlawful but nonviolent resistance, is a long- standing tradition in the movement for social justice. Local organizers alone were not enough  to  accomplish  the  task,  but  by  inspiring  impassioned  local  counter- demonstrators to join the picket, the community realized an effective action of civil disobedience.3 The  counter-demonstration  on  July  8  was  initially  peaceful  and  effective.  At  one point during the afternoon, almost everything was operating perfectly according to plan.4  Media  team  members  were  giving  interviews,  education  and  outreach  team members were distributing materials and holding conversations about social justice  3 On this day, 23 arrests were made, the majority of them related to the blockade action. After a local Black  activist was found not guilty for obstructing a public passage, the remaining obstruction charges were dropped. The legacy of being willing to risk arrest and break minor laws in the pursuit of justice is a proud one and in doing so one acknowledges that civility and order are meaningless virtues in the face of racial injustice and white supremacist terror.  4 Experienced SREs reading this are likely thinking, “Uh oh...”  542      Chapter 32: Intersections Between Operations and Social Activism   and the racial history of Charlottesville, and the snack squad was distributing protein bars and bottled water to keep people fed and hydrated. For the moment, it appeared as though the afternoon would pass without major incident. But  after  the  KKK  departed,  the  heavily  armed  and  armored  police  turned  on  the crowd. Within minutes of the Klan’s departure, police declared an unlawful assem‐ bly. Shortly after, the police deployed tear gas against the peaceful crowd, injuring bystanders, media, and several of their own officers in the process. Organizers had to pivot rapidly to address a situation with increasing tension to keep it from spiraling out of control. An injured and immobilized counter-protester was in the path of advancing riot police. People who had been arrested earlier in the day had been processed and released and were seeking their friends and loved ones amidst the confusion. Uncertain and confusing messages were being exchanged as people were trying to figure out what was happening. This  pattern  may  be  familiar  to  any  SRE  who  has  experienced  a  system  in  crisis. Seemingly  smooth  operations  suddenly  fail.  Alerting  fires  a  storm  of  pages  at  on- callers. Incident response is spun up. There is no way to stop time to fix the issue in place, and the longer the outage lasts, the more disappointed our customers become. Managing crisis response is a skill unto itself. The ability to triage information, pri‐ oritize work tasks, and handle communication is critical to successful crisis recovery. An after-action report commissioned by the City of Charlottesville5 identified several issues with the police response, including a lack of interagency communication and unclear chains of command. Lacking proper oversight, an officer without the author‐ ization  to  do  so  gave  the  order  to  deploy  three  tear  gas  grenades  into  the  crowd. When confronted by his superiors about this decision, the officer who gave the order exclaimed, “You are damn right I gassed them; it needed to be done!” The impact of this decision had unintended long-lasting impacts: in trying to escape the noxious fumes, three counter-demonstrators covered their faces with t-shirts and bandanas and were consequently arrested on felony masking charges.6 The  July  8  KKK  rally  showed  in  great  relief  the  differences  between  proper  and improper  response.  While  activists  were  able  to  handle  the  rapidly  escalating  sce‐ nario, providing medical aid to people, clearing vulnerable adults and children from the  tear  gas,  and  providing  support  to  arrestees,  the  police  response  demonstrated poor  communication  and  a  lack  of  ability  to  process  dynamic  information  in  real time.  5 Final Report: Independent Review of the 2017 Protest Events in Charlottesville, Virginia 6 All three felony charges for unlawful masking were later dropped by the Commonwealth, citing the required element of intent to disguise one’s identity. Given the use of chemical weapons, these arrests could have been avoided had police adequately trained for the expected response to chemical deployments.  Before, During, After      543   Preparing for the worst: handling terror at Unite the Right During Unite the Right, a terrible and tragic event occurred when James Alex Fields allegedly attacked a crowd of counter-demonstrators with his car,7 killing Charlottes‐ ville  resident  Heather  Heyer  and  injuring  35  others  in  an  apparent  act  of  white- supremacist terror. The attack occurred when an antifascist crowd marched up Fourth Street SE, several blocks away from the rally, in a “victory march” celebrating the fact that the authori‐ ties  had  shut  down  the  rally  after  hours  of  brutal  street  violence.  Because  of  the expected crowd size, Fourth Street SE was supposed to be closed all day; the street should have been blocked off to any vehicle traffic and open to pedestrians. These road  closures  were  well-publicized  before  the  event;  in  the  moments  prior  to  the attack, local activists were trying to move the march off of Water Street, which was open  to  traffic,  and  onto  Fourth  Street  SE  for  safety  and  to  avoid  potential  police escalation. Although  this  event  made  national  news  due  to  Heyer’s  death  and  the  overt  Nazi imagery  present  throughout  the  weekend,  there  are  elements  of  the  story  that  the public has overlooked. The crowd that Fields attacked had many experienced politi‐ cal activists who have been through dramatic, chaotic, and violent events before. Immediately following the incident, dozens of injured people were left lying on the street. Well-trained medics immediately began to render emergency care while other activists helped clear the street to make way for ambulances and other first respond‐ ers. But other skills in that moment proved just as critical. One of the cars damaged in the attack contained young children; preschool teachers in the crowd were able to calm and protect them while their parents received medical care. Clergy members nearby immediately ran toward the attack to provide comfort and support in a moment of deep trauma and uncertainty. And activists used their large banners and signs to create a “medic wall” that helped protect the privacy and safety  of  the  injured.  Each  person  had  a  job  and  each  person  had  to  do  that  job. Without that preparedness and that sense of trust, it is certain that the aftermath of the attack would have been far worse. Although the average SRE  hopefully  does not have to prepare for armed engage‐ ment with private militias and neo-Nazi terrorists, the parallels are nevertheless easy to draw. The success or failure of a company can depend on the ability to deliver on its promises. Failures in critical services can lead to millions or billions of dollars in productivity losses. As “Internet of Things” development continues and we integrate  7 At the time of this writing, Fields has been charged with first degree murder and eight charges of aggravated malicious wounding, among others. He awaits trial in late 2018 and is presumed innocent until proven guilty in a court of law.  544      Chapter 32: Intersections Between Operations and Social Activism   connected technology into spaces like healthcare, transportation, and more, it does not require a stretch of the imagination to envision an environment when reliability that  deviates  from  the  specification  literally  leads  to  life-or-death  outcomes.  As  in activism, proper planning and preparation in the form of incident management pro‐ tocols and predeveloped safeguards are necessary to be ready to address crisis situa‐ tions. In  the  independent  review  of  the  Unite  the  Right  rally,  investigators  found  that Fourth  Street  was  guarded  only  by  a  single  police  school  resource  officer.  When police declared an unlawful assembly and cleared the park, rally-goers and counter- protesters were forced to disperse in the direction of Fourth Street. The officer, fear‐ ing for her safety, requested and was given permission to leave the area. This left the entrance  to  Fourth  Street  unprotected  save  for  a  single  small  plastic  barrier,  easily bypassed. The Public Works Department for the City of Charlottesville had offered dump trucks and school buses to provide a hard physical barrier to the closed streets; city officials planning the event did not respond. The political events of the summer of 2017 in Charlottesville should be seen as a les‐ son in preparedness. Not every outcome could have been predicted, but it is evident that individuals were able to plan for many possible outcomes, and that planning was able to prevent bad situations from being much worse. The crises were managed by individuals who developed frameworks of trust and who had diverse skills. The fail‐ ures that enabled the crises to happen in the first place can largely be characterized by lack of trust and communication. It may not be possible to plan for every potential crisis. But it is possible to prepare mitigation and reaction strategies for when crisis inevitably strikes, particularly when a major event, whether it be a political rally or a software release, has a firm fixed date at some point in the future.  The corollary to trust is forgiveness Chaotic situations are naturally stressful. To manage this stress, it is often necessary that one relinquish some control. The act of triage is, ultimately, the acknowledgment that  some  problems  must  be  solved  now,  some  problems  can  be  solved  later,  and some problems cannot be solved at all. For engineers, this can be an emotionally dif‐ ficult position to be in. Many engineers find it hard to let go of a problem they know they can solve given enough time. They lack the cognitive space to coordinate and plan while they are deep in the weeds problem solving. Fundamentally, crises are simply a scarcity of time and attention. In a crisis environ‐ ment, one must optimize for mitigating damage and minimizing lost time. The time- cost of poor decisions made under stress is measured both by the time invested in the wrong decision but also the additional time needed to mitigate the damage done by that wrong decision and reorient along the right path.  Before, During, After      545   Activists  have  acknowledged  this  cost  in  the  Second  St.  Paul  Principle:  the  actions and tactics used will be organized to maintain a separation of time or space. In politi‐ cal action, it is possible to perform a justifiable act at an unjustifiable time or place. This is doubly true when one is faced with life-or-death crises in political movements. When a planned action starts going badly, it is necessary to choose actions that mini‐ mize harms while maximizing impact. It does the movement no good to endanger people who cannot share the consequences of those actions, who cannot be endan‐ gered. Ultimately, successful triage depends on having a team of people with diverse skill sets who can be depended on to recognize the boundaries of their own expertise. Not every activist is a medic, not every medic is a media representative, not every media representative is arrestable. Trust is critical. Think  of  managing  a  large-scale  outage.  Each  principal  in  the  incident   incident commander,  communications  lead,  operations  lead   plays  a  distinct  role,  and  may further delegate and coordinate even more specialized responders. And in a situation spanning  multiple  teams  or  companies,  hundreds  of  people  might  be  organized according to the incident command system playbook, each fulfilling a critical role in the process of identifying the fault in the system, repairing the damage, and fortifying it against similar failures. Each party needs to trust the other to do their job, so they can focus on doing their own jobs. As Picard teaches,8 it is possible to make no mistakes and still lose. Nowhere is this truer than in the quest for social justice. Oppression is not the result of people failing to make the right decisions to unlock the secrets of success, but rather the result of deliberate and patterned efforts to deny privilege and power. Decisions made in cha‐ otic moments with limited information ultimately may not look like the right deci‐ sions when the complete truth is laid bare. When in a triage environment, people and things get left behind who do not deserve it. We will lose allies, customers, comrades, clients, safety, data, freedom, money. A crisis is a condition in which not everyone and everything can emerge unscathed. We  must  recognize  and  hold  sacred  the  ability  to  forgive  each  other  in  the  chaos. Without this, we can never truly achieve trust. It is possible, and even likely, that we will make mistakes when things go badly. The ability to forgive mistakes is the key to sustainability. Forgiveness allows us to act with conviction in the moment; without it, we may be pushed to brashness or frozen by inaction. It is not enough to be blameless in the retrospective; we must be free to act with our best gifts.  8 Star Trek: The Next Generation, “Peak Performance,” Season 2, Episode 21  546      Chapter 32: Intersections Between Operations and Social Activism   Writing Our Own History: Making Sense of What Went Down A commonly held perspective is that the real work begins when the crisis ends. Crises expose flaws that were previously unknown, or to put it another way, every user is also a QA analyst. Likewise, every activist becomes a historian, as the true and com‐ plete  stories  of  political  movements  are  rarely  told  in  the  newspapers  and  history books. Reflecting on the events and writing our own histories is important in acti‐ vism as well as engineering. In our careers as SREs, we would be exceptionally lucky to experience only a single crisis. Anyone wishing for a long career as an SRE, or as an organizer, should expect to handle multiple major incidents. How we respond and reflect in the days, weeks, and months following a major event is as important as how we planned for it and acted during it. Retrospective analysis is the necessary step of “closing the loop” and establishing  productive  feedback  cycles  that  help  educate  and  inform  behaviors  to prepare for the next crisis.  Charlottesville in review: assigning and avoiding blame How did this play out for activists and police authorities in Charlottesville? Following the white supremacist rallies in the summer of 2017, the City of Charlottesville com‐ missioned an independent review of the city’s handling of the events. This review was conducted by a former US attorney—a prosecutor not commonly known for antipo‐ lice or antistate sentiments. Nevertheless, the review criticized how the city and the police handled the events. The report correctly identified many of the matters addressed earlier in this chapter, particularly with regard to police training and preparation. With respect to the July 8 rally, the report says:9  Unfortunately, the [three other law enforcement] agencies that did send personnel to assist CPD [Charlottesville Police Department] on July 8 were not well integrated. The disconnect between the agencies’ respective approaches to the Klan event began well in advance of July 8. There were no joint trainings for all personnel detailing the rules of engagement for this event. VSP [Virginia State Police] and CPD operated under sepa‐ rate operational plans on July 8. While CPD shared their plan with VSP leadership, VSP did not disclose its plan to CPD. The CPD officers and VSP officers who were assigned  to  work  together  in  particular  zones  did  not  meet  each  other  or  have  any communication prior to July 8. This represents a failure in preparation that undercut operational cohesion and effectiveness during the event.  Although these failures would be damning if July 8 was the only event of the summer, the  fact  that  the  police  forces  failed  to  properly  learn  from  these  events  led  to  the  9 Final Report: Independent Review of the 2017 Protest Events in Charlottesville, Virginia, page 66.  Before, During, After      547   chaos that ultimately ended in the death of Heather Heyer and the injuries of 35 oth‐ ers during the August 12 rally. Quoting again from the investigation report:10.  [T]he men and women tasked with preparing for the Unite The Right event did not sufficiently appreciate the challenge presented. They did not ask for advice or assis‐ tance  from  fairly  obvious  potential  sources.  They  “didn’t  know  what  they  didn’t know,” which in part led to the inadequate preparation for this event. Despite  the  obvious  similarity  between  these  prior  events  and  the  Unite  The  Right rally, CPD planners did not sufficiently consult with officials in other jurisdictions as they  planned  for  August  12.  While  they  had  brief  discussions  with  officers  in  other jurisdictions,  they  did  not  make  any  attempt  to  incorporate  lessons  learned  in  their own operational plans. They did not travel to other jurisdictions, obtain other depart‐ ments’ operational plans for similar events, or share their thoughts about Unite The Right with counterparts with more experience. This failure represents a tremendous missed opportunity.  Any engineer working in a dysfunctional environment would see similarities in these failures  to  learn  from  errors.  This  reflects  a  culture  of  nonaccountability;  when  a group is unwilling or unable to accept or address its own failures, it is doomed to repeat  those  failures.  The  police  refused  to  acknowledge  any  mismanagement  or wrongdoing on July 8. As a result, they failed to engage in practices that could have corrected those flaws between then and August 12.  Beyond culpability: building capacity instead of assigning blame Engineering lately has adopted the concept of the blameless retrospective. The goal is to avoid turning individual engineers into scapegoats or pariahs. But blamelessness does  not  imply  an  absence  of  future  accountability,  and  indeed  it  is  necessary  to address failures and flaws in the act of writing our own histories so that we will not repeat  the  same  failures  more  than  once.  Learning  requires  periodic  failure,  and equally importantly, the ability to learn from the failures of others. Engineering teams that implement blameless retrospectives can learn from failure while building trust and sustainability. This is an area where political organizing and social activism communities can learn from  the  engineering  world.  Many  activist  communities  have  difficulty  sustaining activist energy. Deep divides have grown between communities, organizations, and individuals, in part as a result of the high stakes in the quest for social justice. Francis Lee writes:11  Black Lives Matter cofounder Alicia Garza gave an explosive speech to a roomful of brilliant and passionate organizers. She urged us to set aside our distrust and critique  10 Ibid, page 153 11 “Lee, Francis  2017 . Excommunicate Me from the Church of Social Justice,” Autostraddle, July 13.  548      Chapter 32: Intersections Between Operations and Social Activism   of newer activists and accept that they will hurt and disappoint us. Don’t shut them out because they don’t have the latest analysis on oppression, or they aren’t using the same language as us. If we are interested in building the mass movements needed to destroy mass oppres‐ sion, our movements must include people not like us, people with whom we will never fully agree, and people with whom we have conflict.  Sustainability and healing does not mean we have to get everyone on the same page as ourselves. It means honoring the objectives, struggles, and expectations of others who work alongside us. It is easy to celebrate their victories when it suits our objec‐ tives; it is harder to avoid blaming their failures when the outcomes are not as favora‐ ble to ourselves. But to sustain and grow requires us to be uncomfortable. This  may  mean  accepting  that  we  own  suboptimal  environments,  accepting  that things would be better if they were done differently in the past, and that while we seek to mitigate the damage of factors outside of our control, those factors are ulti‐ mately outside of our control. Trust and forgiveness are as important now as in the moment, because the purpose of recovery is not to avoid the next crisis, but to ensure that we are ready to address the next one at full strength. In the cloud world of critical dependencies upon others’ services, the principles of trust and forgiveness are especially important to the functioning of digital businesses. Demanding that engineers actively working an incident update an uninvolved stake‐ holder, rather than trust that engineer to do their best possible work, slows resolution of  the  underlying  issues.  Transparency  and  forgiveness  have  become  key  corner‐ stones of reassuring technological partners that causes of the failure in services are well-understood and will not recur once mitigations are put into place. There are no perfect  systems,  and  empathy  is  the  best  way  to  build  interconnected  systems. hugops is a common refrain between customers and suppliers, and between com‐ petitors, precisely because it emphasizes our own humanity and that any day it could be one of us in the hot seat solving an outage. The Long Tail: Turning Action into Change  Activism can be the journey rather than the arrival.  —Grace Lee Boggs  In the technology industry, we work toward building better products and services as our sustained, long-term goal; toil, on-call shifts, and release days are necessary steps along the way, but it would be disingenuous to treat release days as the raison d’être of the average engineer’s career. For activists working on social justice, their goal is to bend  the  moral  arc  of  the  universe  toward  justice  through  intentional  action,  sus‐ tained growth, and faithful optimism—which takes time. In reality, both planned and spontaneous action are but a fraction of the meaningful work contributed toward the  The Long Tail: Turning Action into Change      549   movement.  The  majority  of  an  activist’s  time  is  spent  fostering  and  growing  com‐ munities based on mutual support, love, and protection. Managing  the  moments  between  the  capital-M  Moments  is  perhaps  the  most  vital part of sustainable activism. In these times, we learn hard lessons about who we are and whom we are among. In the period of time after a major incident, we are often beset by doubts, feelings of guilt, and an urge to assign blame. Relationships, particu‐ larly those that were tenuous before the incident, can be strained or broken. In the long tail after an incident, it is worthwhile to reflect on which of those relation‐ ships we wish to maintain, which we wish to repair, which we wish to strengthen, and which we wish to sever. But broken trust is not an indication of a lack of worth or ability. The reality is that sometimes trust is broken beyond the point of repair. A broken  relationship  is  unlikely  to  mend  during  the  next  crisis.  Instead,  the  break‐ down of trust will lead to a dysfunctional dynamic that may cause internal tension and lead to forces working against each other that should be working for each other. It may be better to address these difficult truths during the downtimes between crises than to risk exacerbating failure at an inopportune moment. On the other hand, downtime  in the human sense  provides a fantastic opportunity to strengthen relationships that work well. If a team works well in its response to a crisis, it’s a worthy investment to reflect on what elements of those interpersonal rela‐ tionships  are  strong  and  strive  to  improve  them.  Chances  are,  building  on  those strengths will have an impact in more areas than crisis response. And sharing best practices with other groups will strengthen their resiliency. In the wake of the summer of 2017, it is certain that some relationships between acti‐ vists will never be repaired, and that some have strengthened immensely. By all indi‐ cations, Charlottesville will remain a target for white supremacist activity. The work continues,  and  the  community  will  have  to  build  on  its  effective  trust  models  and relationships to keep moving forward. One approach is to use relatively stable periods to integrate new people into the work. By  bringing  in  new  people,  which  can  include  junior  team  members  learning  the ropes for the first time, or seasoned veterans coming onto a new team or new role, skills are refined through continuous training. The old saying, “the best way to learn is to teach,” holds true. Teaching allows us a chance to revisit our retrospectives with fresh perspectives. Teaching also allows us to hold close another truth: that we must accept imperfec‐ tion.  All  people  are  imperfect,  whether  they  be  activists,  engineers,  organizers,  or managers.  We  should  always  be  interrogating  our  weaknesses,  and  the  downtime between major events is a good time to allow ourselves to probe those weaknesses. Ultimately, the best use of downtime is preparation. There will always be a next crisis, a next release, a next action. The status quo should never be considered the default  550      Chapter 32: Intersections Between Operations and Social Activism   state of affairs. In the social justice world, maintaining the status quo means living with injustice and inequality. In the world of technology, maintaining the status quo means features aren’t shipping fast enough, our response skills are becoming rusty, or we aren’t making progress on reducing operational load. There always are postmor‐ tem action items to finish to make the system more resilient for the next time. We must  reorient  our  thinking  that  the  default  is  chaos  separated  by  long  periods  of learning, not stability punctuated by brief bursts of chaos. This can be a difficult proposition to accept, and it is as exhausting as it sounds. Ulti‐ mately, we must see this as shift work. We must allow ourselves to take breaks, to shelve our desire to get involved, to control the action, or to know everything. If we have done our jobs well, we have ensured that others are capable of doing our work. There is no movement or organization that finds long-term success by burning out its best. Activism and Change Within a Company Not all activism looks like protesting in the streets. As technology workers, we pos‐ sess influence that can be spent to the benefit of our coworkers, our customers, and the world at large. The simple steps include changes such as revising technical docu‐ mentation to remove microaggressions or changing service-level indicators to meas‐ ure  availability  for  a  wider  range  of  users.  More  complex  efforts  include  doing product advocacy in our role as trusted advisors in the design and launch process of software. But what about if you aren’t heard at first? If you encounter resistance from colleagues or from management in trying to address issues of equity in your company’s products, policies, or working conditions, there are several important considerations that will serve you well in advocating for equity. First, keep ears to the ground about things happening within your company so that you don’t find out after it’s too late to change easily. It’s a lot easier to change a deci‐ sion that’s being contemplated or not yet implemented compared to a shipped prod‐ uct.  Listen  carefully  to  emotional  and  rational  content  of  what  others  are  saying. Listen to your colleagues when they have concerns and figure out whether they want empathy  or  problem-solving.  Band  together  and  form  coalitions  so  you  have  a broader collective voice. Second, keep your venting separate from engaging in constructive problem solving. There is a place for each of them, but constructive problem-solving with leadership requires minimizing emotional temperature. Vent to friends privately or find some‐ one else who can advocate neutrally while you vent. Avoid saying things you’ll regret having attributed to you in a lawsuit or newspaper. You aren’t personally accountable for pushing the rock all the time.  The Long Tail: Turning Action into Change      551   Third, make sure that you’ve identified the right person to escalate your concerns to; it may not be the messenger or team directly working on the issue you disagree with. Addressing feedback to someone who can’t do anything frustrates you and them; fig‐ ure  out  who  the  decision-maker  is.  Once  you’ve  found  the  right  decision-maker, assume at least initially that they’re also trying to act in good faith for the best interest of your company and users, but potentially with different axioms or priorities. Iden‐ tify who they listen to that you can persuade to ally with you. Keep their trust and cultivate a positive working relationship; chances are that you’ll wind up needing to escalate to them in the future. And make sure that, rather than having a hundred dif‐ ferent conversations, that you have a smaller number of liaisons working to proxy concerns to avoid defensiveness against a mob. Don’t allow rogue bystanders to get in the way. Fourth,  try  to  understand  what  factors  influenced  the  decision-maker’s  choice  and explain your concerns in language that directly attempts to change the variables of the original decision. Your instincts about others’ rationales may be naive and quite off the mark; don’t argue against a nonexistent strawperson. Understanding the con‐ text around decisions and goals is critical. Framing of concerns requires explaining who is impacted in what situations, what specific interactions with your company’s products will result in a negative outcome for those people, and what the net harm is. Demonstrate that the company’s goals can still be met with a less harmful option. If all else fails, there are options to try before giving up. Employee petitions open let‐ ters can be effective at getting the attention of executives if carefully worded and suc‐ cinct. Less than 5% of a company openly dissenting can be effective, but only works if your  employer  tolerates  internal  dissent.  Be  cautious  about  undermining  trust  progress by talking to the press; leaks can result in negative press and subsequently entrench the status quo due to defensiveness. The threshold of effort and badness to achieve enough negative press to force a change through is too high to be practical in many cases. If that last resort tactic doesn’t succeed, you can always vote with your feet if you can afford to. Change  can  take  a  long  time,  and  ensuring  that  your  activism  work  is  sustainable over time is critical. Burnout must be managed, and initial bursts of energy devoted to creating pressure can be used to empower a small, focused group working on long- term  objectives  with  the  mandate  they  need  to  succeed.  Once  chartered,  working groups need trust and some breathing room; negotiations tend to be more successful when kept confidential until a mutually agreeable solution is finalized. Whether  by  quiet  resolution  via  1:1  chat,  formation  of  a  working  group  without semi-public pressure, or a large crisis as a forcing function, employee voice is a way of ensuring  that  your  company’s  products  conform  to  engineering  ethics.  The  same communication and escalation skills that keep SREs adept at juggling product devel‐ opment,  product  management,  and  other  SREs  as  stakeholders  are  great  for  doing  552      Chapter 32: Intersections Between Operations and Social Activism   internal activism. It’s a small step from the culturally ingrained and supported prac‐ tice  of  pushing  back  on  launches  due  to  reliability  concerns  to  pushing  back  on  a launch for ethical reasons. It’s the least we can do for our industry and for society. Conclusion As SREs, we have an interest in ensuring that our customers can use our products, but we often limit ourselves to the technical aspects of that rather than considering the social implications of our products and who can use them to full effect in prac‐ tice. Our SLOs need to consider the experiences of all users and provide a positive experience across the board. Applying  our  skills  to  social  activism  can  make  the  world  a  more  just  place  and enable users to have more equal access to the products that we build. You can start small by ensuring that service level indicators are inclusively measuring the experien‐ ces of all users, not just able-bodied users with fast, low-latency internet connections. Expanding your reach further, you can advocate for equity in products you contrib‐ ute to. And your SRE skills come in useful if you choose to participate in social move‐ ments.  Emily  Gorcenski  is  a  data  scientist  and  anti-racist  activist  from  Charlottesville,  Vir‐ ginia, who now lives in Berlin. Her passion is the intersection of technology, regulation, and society, and she is a tireless advocate of transgender rights. Liz Fong-Jones is a developer advocate, activist, and site reliability engineer  SRE  with 14+ years of experience based out of Brooklyn, New York, and San Francisco, Califor‐ nia. She lives with her wife, metamour, and a Samoyed Golden Retriever mix. In her spare time, she plays classical piano, leads an EVE Online alliance, and advocates for transgender rights as a board member of the National Center for Transgender Equality.  Conclusion      553    We have to get on, we have to get on. We have so much time and so little to do. Strike that. Reverse it.  —Gene Wilder as Willy Wonka  I don’t want the book to end here, either. Let’s pretend instead that it is a pause. A “you don’t have to go home, but you can’t stay here” moment. It is up to you to keep these discussions going. Please talk to your fellow and future practitioners about anything you found interest‐ ing in this book or omitted from it. Submit talks to conferences      SREcon   . Reach out to me and other members of the SRE community over Twitter   DMs  at  @otterbook  always  open!   and  other  social  media  and  introduce yourself. Write a book, compose an opera, or create an interpretive dance about SRE —whatever lets you contribute from your whole self. We need you. Thank you for reading this book. I look forward to being part of your conversations.  CHAPTER 33 Conclusion  555    Index  A abandonment expense  AbEx , 48 access control, 254 accommodations, for on-call personnel, 521 active learning, 343-354  basics, 344 costs of failing to learn, 352 Incident Manager card game, 346-350 learning habits of effective SRE teams,  353-354  postmortems and, 353 production meetings and, 353 SRE Classroom, 350 Wheel of Misfortune game, 345  activism  see social activism  address resolution protocol  ARP  tables, 446 adopt-to-buy abandonment scenario, 48 advocate phase of SRE execution, 375 Affordable Care Act, 529 Agilent Technologies, 111-122 alarming, 439 alerts, 90 Allspaw, John, SRE Cognitive Work, 461-482 Almeida, Daniel Prata, SRE Without SRE,  81-109  AlphaGo, 299 Amaro, Ricardo, Introduction to Machine  Learning for SRE, 293-320  Amazon Glacier, 265 Amazon Web Services  AWS , 166 Andersen, Kurt, SRE as a Success Culture,  365-377  Anderson, Brian, ix antipatterns, 379-404  APIs  53  of service-level middleware, 423 reporting as first step towards building SLIs,  third-party integrations and, 61  application errors, 263 Application Operations  AppOps  teams,  208-212  assurance windows, 356  see also time quanta   automation  and operator fatigue, 289 and reliability, 290 as team player in SRE work, 479 data durability engineering, 288-290 privacy engineering and, 249 SRE teams and, 374 testing at Google, 178-180 third-party integrations, 60 window of vulnerability and, 288  as SLO, 356-359 time quanta as metric, 357 tracking availability level, 137-142 transactions as metric, 358 transactions over time quanta, 358  availability  Avenet, Julien, 204  B backhauling a request, 418 backlogs, 174 backpressure, 469 backpropagation, 311 backups  Index      557   assigning avoiding, in reviews of social acti‐  Campbell, Laine, Database Reliability Engineer‐  data durability engineering and, 275 logical, 266 physical, 265   see also recovery recoverability of data   Bainbridge, Lisanne  on automation, 463 on human operators and advanced systems,  461  base image, 410 Bayesian inference, 296 Beamish, Alex, 204 Beck, Kent, 236 benefits, for job applicants with mental disor‐  Beyer, Betsy, The Intersection of Reliability and  ders, 497-498  Privacy, 245-256  biases, in job interviewing, 16 Bisset, Blake, SRE Antipatterns, 379-404 black boxes  running like a service, 52 SLIs on, 53  blame  vism, 547  in cross-team postmortems, 74  blameless retrospectives in social activism, 548 SRE social activism parallels, 541   see also postmortems   Bland, Mike, 178, 180 Blank-Edelman, David  Context Versus Control in SRE, 3-13 DevOps and SRE, 187-205 Production Engineering at Facebook,  207-229  Blew, Aaron, 201 blind resume review, 16 blue green deployment  about, 408 rolling release vs., 412  Boggs, Grace Lee, 549 Bootcamp, at Facebook, 225-226 bot attacks, 424 bottlenecks, 131 breakpoints, 128 Brummel, Janna, 191 build buy adopt decision, 43-49  assessing project considerations, 46 core competencies and, 47  558      Index  deciding which option to take, 45 determining importance of an integration,  44  integration timeline, 47 LinkedIn case study, 49 project operating expense and abandon‐  ment expense, 48  stakeholder identification, 44  Burns, Robert, 538 business case, for SRE  preparing, 113 presenting, 116  buy-to-adopt abandonment scenario, 48 buy-to-buy abandonment scenario, 48  C calibration problem, 471-475  addressing, 480 incidents and recalibration, 474 incidents and specific calibration problems,  474  mental models, 471  ing, 257-272  and Sharing , 201  Facebook, 207-229  CAMS  Culture, Automation, Measurement  Canahuati, Pedro, Production Engineering at  canary rollout method, 139 canarying, 513 capacity planning, 373 capacity, team, 174 capital expenditure  CapEx , 45, 163, 373 cascading failure, 531 catalytic role SREs, 376 certificate authorities  CAs , 44 Chakrabarti, Saunak Jai, SRE Without SRE,  81-109  Challenger space shuttle disaster, 532 champions, for SRE, 114 change management  parallels between social activism and tech‐  nology industry, 549-553  SRE teams and, 374  chaos engineering, 233-243 advanced principles, 241 and Chaos Kong, 239 and Economic Pillars of Complexity, 236 FAQs, 242   inherent problems with complex systems,  complex systems, 529-535  234-236  navigating complexity for safety, 238 origins of, 237 Chaos Kong, 239 Charlottesville, Virginia, demonstrations coun‐  terdemonstrations  2017   see social acti‐ vism   Check, Martin, Using Incident Metrics to  Improve SRE at Scale, 33-40  choke points, 59, 391 Churchill, Winston, 242 CIA, training games used by, 345 Clarke, Arthur C., 295 classification schemes, 469 cloud services, 239 code review, documentation as part of, 337 cognitive hacks, 522 cognitive overload, 458 cognitive work, 461-482  activities of SREs during incidents, 462 calibration problem, 471-475 critical decisions made under uncertainty  and time pressure, 464  human performance in modern complex  importance of understanding practitioner  systems, 464  cognition, 463-465  activism, 537-553  managing coordination costs, 469 observations on SRE cognitive work around  incidents, 465-471  opportunities to mitigate worst aspects of  incident, 466  repairs to functional systems, 468 sacrifice decisions and uncertainty, 467 special knowledge about complex systems,  469  SREs as cognitive agents working in a joint  cognitive system, 470  Columbia space shuttle disaster, 532 communication for teams, 456 third-party integrations and, 62  compensation  as always in state of partial failure, 532 community for, 535 decoherence and cascading failure, 531 defining characteristics, 236 Economic Pillars of Complexity, 236 human performance in, 464 incidents as inevitable in, 475 inherent problems with, 234-236 inseparability of computer and human sys‐  tems, 530  novelty priority inversion, 533 overhead of coordination, 533 special knowledge about, 469 tooling for management of, 370  configuration management  immutable infrastructure and, 410  containers, 68 Content Delivery Network  CDN , 56, 59 context propagation, 441 context, control vs., 3-13 continuous delivery deployment  championing, 272 database reliability engineering and,  267-269  deployment, 270-272 immutable infrastructure and, 408 third-party integrations and, 55  immutable infrastructure and, 408 third-party integrations and, 55  contract termination, 62 control plane, 442 convolutional neural networks, 304 Conway's law, 76, 530 Conway, Mel, 530 Cook, Richard, SRE Cognitive Work, 461-482 coordination overhead  in complex systems, 533 managing, 469  costs of incidents, 476-477 Cowling, James, Engineering for Data Durabil‐  ity, 275-291  crises  and persons with mental disorders, 503 as scarcity of time attention, 545  intersections between operations and social  continuous integration  for on-call, 521 job applicants with mental disorders, 496 novelty priority inversion and, 533  crisis management, 542-546 cross-domain failures, 69-71 cross-functional teams, 161-165  Index      559   cross-team reliability, 73 cultural fit interviews, 20 culture  see success culture, SRE as   D dashboards, 40 data durability, engineering for, 275-291  automation, 288-290 backups, 275 estimating durability, 277-279 freshness, 276 isolation, 279-283 protection, 283-285 real-world strategies, 279-290 recovery, 284 replication, 275-279 replication techniques, 276-279 restoration, 276 safeguards, 284 testing, 283 verification, 286-288 window of vulnerability, 288 zero-errors system, 286  data loss  application errors and, 263 detection of, 262-264 infrastructure services and, 264 operating system hardware errors, 264 user error and, 263  data plane, 442 database reliability engineering, 257-272  anatomy of a recovery strategy, 262-267 best practices and standards, 268 collaboration, 269 considerations for recovery, 261 continuous delivery and, 267-269 culture of, 260 data protection, 258 deployment of CD, 270-272 documentation, 268 educating developers, 267-269 guiding principles, 257-260 impact analysis, 270 making the case for, 272 migration patterns, 270-272 migration testing, 271 migrations and versioning, 270 organization's data model, 268 pet vs. cattle servers, 259  560      Index  recoverability, 261-267 rollback testing, 271 self-service for scale, 258 tools for, 269  de-provisioning, 374 Debois, Patrick, SRE Patterns Loved by  DevOps, 177-185  decision trees, 301, 307-309 decision-making, uncertainty and, 464 decoherence, 531 decommissioning, 62 Deep Blue, 299 deep reinforcement learning, 299 DeepMind, 299, 319 degradation, graduated, 369 demand forecasting, 373 dependencies, external, 133-136 deployment process, immutable infrastructure  and, 411  dev owners, 88 development teams  embedding SRE teams into, 66 interaction with SRE teams, 27  DevOps  and enterprise operations model–SRE tran‐  sition, 173  and SRE origins, 365 automated testing at Google, 178-180 creating shared source code repository,  183-185 defined, 173 DevOps and SRE, 187-205 favorite SRE patterns, 177-185  Dickerson, Mikey, 217  Elegy for Complex Systems, 529-535  disaster planning, 61 Disk Scrubber, 287 Distributed Denial-of-Service  DDoS  attack,  424  DNS  Domain Name System  as external dependency, 135 at Spotify, 100 routing requests, 416  Docker images, 411 documentation  archiving deleting unnecessary docs, 338 best practices for, 335-339 communicating value of, 339-341 database reliability engineering and, 268   Envoy, 445 functional requirements for, 328-330 Google and, 331-335 integrating into engineering workflow,  325-341  integrations as key to adoption, 335 markup language for, 335 of policies, 330 playbooks, 329 postmortems, 329 quality characteristics, 326-330 recognizing rewarding, 338 requiring as part of code review, 337 service overviews, 329 setting realistic quality standards, 337 SLAs, 330 source control and, 334 templates for, 336  Doherty, Mike, 193 downtime  direct impact, 50 indirect impact, 51 third parties, 50-52  Dropbox, data durability engineering at,  275-291  Drucker, Peter, 371 durability  see data durability, engineering for  dynamic configuration API, 443  E ecommerce, scriptable load balancers and,  426-428  Economic Pillars of Complexity  EPC , 236 Edge Side Includes  ESI , 49 Edwards, Damon, Clearing the Way for SRE in  the Enterprise, 147-175  Ek, Dainel, 102 Eklund, Jeff, SRE Without SRE, 81-109 Elastic Compute Cloud  EC2 , 166 embedded SREs, 66, 77 emergency response, 373 emotion, engineering problems and, 28 EngPlay, 333-335 Ensor, Phil S., 152 enterprise operations model–SRE transition,  147-175 DevOps and, 173 error budgets, 171-172  Lean manufacturing concepts applied to,  minimizing handoffs, 161-165 psychological safety human factors, 175 replacing handoffs with self-service,  157-161  165-170  silos, queues, and tickets, 152-156 steps in clearing obstacles to, 157-175 toil as enemy of SRE, 148-151 toil in the enterprise, 151 toil limits in, 172 unifying backlogs and protecting capacity,  174  Envoy  development learnings, 446 operation of, 445-447 operational learnings, 445 origin and development of, 444 technical learnings, 446  Equal Employment Opportunity Commission,  U.S.  EEOC , 495  error budgets  defined, 139 in enterprise operations model–SRE transi‐  tion, 171-172  eventually consistent service discovery, 438 Ewald, Michael, 192 exclusion backlash, 522 exit interviews, 502 expectations  ambiguous imaginary, 459 for teams, 456  F Facebook, production engineering at, 207-229 failure  cascading, 531 complex systems as always in state of partial  failure, 532  mean time to  see mean time to failure   failure domains, isolating, 368 failure recovery, 406 Farley, Thomas, 467 Farmer, Andrew, 197 Fast Properties, 6 fatigue, of operator, 289 FBAR  Facebook Auto Remediation , 211, 215 Fernandez, Manuel, 203 Fields, James Alex, 544  Index      561   firefighting phase of SRE execution, 374 first-class citizens, third parties as, 49-63 flexible scheduling, 499, 521 Fong, Andrew, Interviewing Site Reliability  Engineers, 15-22  Fong-Jones, Liz, Intersections Between Opera‐  Gorcenski, Emily, Intersections Between Oper‐  ations and Social Activism, 537-553  gradient descent, 312 graduated degradation, 369 Gustavsson, Niklas, SRE Without SRE, 81-109 Gwartz, Jason, 205  tions and Social Activism, 537-553  Ford, Henry, 111, 463 forgiveness, trust and, 545 Forster, E. M., 343 frameworks, privacy engineering and, 250 frequentist estimators, 296 Fukushima Daiichi nuclear disaster, 466 functional quality, of documentation, 327 funnel  hiring process , 16  onsite interview, 19 phone screens, 18 take-home questions, 21  G g3doc, 331-335 games, for active learning, 344-350 gamma distribution, 360 Garza, Alicia, 548 gatekeeper phase of SRE execution, 375 Gillies, Aaron, Do Docs Better, 325-341 Go  game , 299 goalies, 92 goals, for SRE teams, 117 Going to the Gemba, 159 Golden Path, 105 Goldfuss, Alice, 519 Gollapalli, Sriram, Introducing SRE in Large  Enterprises, 111-122  Google  automated testing at, 178-180 data center cooling management with AI,  319  DevOps–SRE relationship, 193 documentation at, 325, 331-335 launch and handoff readiness review,  180-183  shared source code repository at, 183-185 SRE teams embedded with software engi‐  neering teams, 44  Wheel of Misfortune training tool, 345  Google model of cross-functional teams, 164 Google Web Server  GWS , 178-180  562      Index  H hack-a-months, 222 HAL 9000, 293, 295 Hale, Jefferson, 194 Hand, Jason, 196 handoff readiness review  HRR , 180-183 handoffs  minimizing, 161-165 replacing with self-service, 165-170  HAProxy, 414 hardware errors, 264 healthcare.gov, 529 Heckman, Tim, 191 heroics  negative consequences of, 518  Heyer, Heather, 544, 548 hiring  see job application hiring process  histograms  percentiles vs., 363 SLOs and, 362-363  Hopper, Grace, 371 Horowitz, Jonah, Immutable Infrastructure and  SRE, 405-412  human error, 384-386 human resources, 451-459 human-factors research, 487 Humble, Jez, SRE Patterns Loved by DevOps,  177-185  I immutable infrastructure, 405-412  base image construction, 410 continuous integration continuous deploy‐  ment with confidence, 408  defined, 405 deploying applications, 411 disadvantages of, 412 failure recovery, 406 faster startup times, 407 known state, 407 multiregion operations, 409 release engineering, 410   scalability, reliability, and performance, 405 security, 409 simplicity, 406  impact analysis, 270 impact monitoring, 56 incentive structure  for SRE buy-in, 76 novelty priority inversion and, 533  incident analysis, 481 incident command, 541 Incident Manager  card game , 346-350 incident metrics, 33-40  improving SRE at scale with, 33-40 real-time dashboards, 40 repair debt, 37 reviewing, 36 surrogate metrics, 36 time to detect, 34 time to engage, 34 time to fix, 35 virtual repair debt, 38 virtuous cycle, 33-35  incident response management  activities of SREs during incidents, 462 calibration problem, 471-475 classification schemes, 469 cognitive work and, 461-482 formal role assignments, 470 managing coordination costs, 469 observations on SRE cognitive work around  incidents, 465-471  opportunities to mitigate worst aspects of  incident, 466  repairs to functional systems, 468 sacrifice decisions and uncertainty, 467 special knowledge about complex systems,  469  SRE teams and, 373 SREs as cognitive agents working in a joint  cognitive system, 470  addressing the calibration problem, 480 and collective recalibration, 474 and individual recalibration, 474 and specific calibration problems, 474 automation as team player in SRE work, 479 building a corpus of cases, 479 changing patterns of, 478 costs imposed by, 476-477  incidents  harvesting value of, 478-481 inevitability in complex systems, 475  Index Scanner, 287 infrastructure services, 264 infrastructure, immutable  see immutable  infrastructure   insourcing, growing teams via, 117 integration monitoring, 56 integrations, documentation and, 335 intelligent agents, 296 interrupt work, project work vs., 457 interviewing  job interviews , 15-22  advice for hiring managers, 21 and persons with mental disorders, 495 basics, 15-17 biases in, 16 funnel basics, 16 funnels, 18-22 industry vs. university candidate profiles, 15 onsite interview, 19 phone screens, 18 selling candidates on your organization, 21 take-home questions, 21 walking away from a candidate, 22  IPython, 306 isolation  and data durability, 279-283 logical, 280 of failure domains, 368 operational, 281-283 physical, 280  J Jansson, Mattias, SRE Without SRE, 81-109 job application hiring process  as funnel process, 16 benefits for applicants with mental disor‐  compensation and applicants with mental  ders, 497-498  disorders, 496  for persons with mental disorders, 495 interviewing SREs  see interviewing  onboarding packets, 498  job duties, for persons with mental disorders,  500  job function, on-call as, 495 job postings, 494 Joblint, 494 Johnston, Bennie, 197  Index      563   joint cognitive system  JCS , 470, 479 Jones, Matt, 203 Jupyter Notebook, 306  K Kaizen  continuous improvement , 157-161 Kanban, 174 Kanwar, Pranay, 199 Kasparov, Garry, 299 Kata method, 157-161 Kelvin, Lord  William Thomson , 123, 366 Key Performance Indicators  KPIs , 366, 373 Kim, Gene, SRE Patterns Loved by DevOps,  177-185  Kissner, Lea, 247 Klein, Matt, 119  on service meshes, 431-447  Knight Capital, 468 known-knowns, software failure and, 511 known-unknowns, software failure and, 511 Kobayashi Maru, 346 Koen, Brian, 370 Kriegsspiel, 344 Kubrick, Stanley, 295  L Lafeldt, Mathias, 371 Lamott, Anne, 337 large enterprises, 111-122  defining current state, 112-114 defining SRE for, 115-116 DevOps–SRE relationship, 195 identifying educating stakeholders, 114 implementing the SRE team, 117-119 introducing SRE into, 112-122 lessons learned from process of introducing  SRE, 120  preparing business case for SRE, 113 presenting business case for SRE, 116 sample implementation roadmap, 121  launch readiness review  LRR , 180-183 leaders, as advocates for SRE, 114 Lean manufacturing movement, 157-161 learning, active  see active learning  Lee, Francis, 548 Legeza, Vladimir, From SysAdmin to SRE in  8,963 Words, 123-145  LGBTQ+ inclusivity, 491, 497  564      Index  Lightweight Directory Access Protocol  LDAP ,  135  Limoncelli, Thomas A.  on DevOps–SRE relationship, 194 on LRR, 183 on shared source repository, 184  LinkedIn, 56, 59 load balancers  see scriptable load balancers  logging, 61 logical backups, 266 logical isolation, 280 long short-term memory  LSTM  networks,  Looney, John, Psychological Safety in SRE,  301  451-459  Lund, Tanner, 189 Lutner, Sean, 195 Lyft  operation of Envoy at, 445-447 origin and development of Envoy, 444  M machine learning, 293-320  AI background, 295 basics, 296-301 current SRE environment and, 300 decision trees, 307-309 defined, 296 enterprise IT areas affected by, 319 human–machine games, 298-300 modern definition of learning in terms of  machine, 297  neural networks, 301-305 on-call substitute, 515 practical examples, 306-319 Python IPython Jupyter Notebook installa‐  tion, 306  reasons for company to use, 293-320 reasons to use, 293 Spotify and, 108 SRE problems addressed by, 294 TensorFlow and TensorBoard, 313-317 time series: server requests waiting, 317-319 training a neural network from scratch,  309-312  MacNamara, Ríona, Do Docs Better, 325-341 Mangot, Dave, 200 Markdown, 332, 335 market-oriented teams, 162   Markov model, 277 markup language, 335 Maslow's Hierarchy of Needs, 217 Master Service Agreements  MSAs , 54 McDuffee, Keith, 192 McEniry, Chris, 198 Mean Time Between Failures  MTBF , 532 Mean Time to Detect  MTTD , 148 Mean Time to Failure  MTTF   in Markov chain, 277 inappropriate optimization of, 395-397 Mean Time to Recovery  MTTR , 395-397 Mean Time to Repair  MTTR , 532  defined, 148 window of vulnerability and, 288  Mediratta, Bharat, 179 Meickle, James, Beyond Burnout, 487-505 Menchaca, Joaquin, 189 mental disorders, persons with  and diversity conversation, 490 benefits for, 497-498 business environment, 491 compensation in job application process,  496  crisis and, 503 defined, 489 importance of detailed job postings, 494 inclusivity as beneficial to all, 503 ineffectiveness of common workplace  strategies towards, 492 interviewing for job, 495 job duties, 500 leaving a job, 502 on-call work and, 495 onboarding packets, 498 pro-inclusion patterns antipatterns,  493-505  promotion, 501 resources, 504 training, 501 working conditions, 499-500 workplace inclusivity and, 487-505  mental health, 487-505  crises, 503 defined, 488  mental models, 471 mentorship programs, 501 Mercereau, Jonathan, Working with Third Par‐  ties Shouldn’t Suck, 43-63  Messeri, Eran, 179, 184 metrics, 28, 117, 373   see also incident metrics   Miasnikoŭ, Stas, Do Docs Better, 325-341 Michel, Drew, SRE Without SRE, 81-109 microservices  context vs. control at Netflix, 3-13 current state of microservice networking,  434  service mesh and  see service mesh   mid-sized organizations  see Soundcloud, SRE  at   migration cost, 48   see also abandonment expense   migrations, database reliability engineering  and, 270-272  Mineiro, Luis, 201 Mitchell, Tom, 297 money  see compensation  monitoring  SRE teams and, 373 third-party integrations, 56-59  monolithic architecture disadvantages of, 432 service mesh vs., 432  Moraes, Gleicon, 202 Morrison, David, 454 Most Favored Customer  MFC  clauses, 62 multiregion operations, immutable infrastruc‐  ture and, 409  Murphy, Niall Richard  Against On-Call, 507-527 Do Docs Better, 325-341  Mushero, Steve, 200  N Netflix  chaos engineering at, 233-243 context vs. control in SRE, 3-13  Netflix model  cross-functional teams , 164 Network Operations Center  NOC , 380 neural networks, 301-305  artificial neurons and, 301 datasets for, 304 popular libraries for, 305 training from scratch, 309-312 when to apply, 303  New York Stock Exchange outage  2015 , 467 nginScript, 414  Index      565   Nolan, Laura, Active Teaching and Learning,  Weak-Anti-On-Call position, 525  343-354  normal distribution, 360 nosocomial automation, 480 Nukala, Shylaja, Do Docs Better, 325-341  O O'Reilly, Tim  on complex systems, 371 on SRE, 366  Obama, Barack, 529 object stores, 266 object-relational mapping  ORM , 458 observability, service mesh and, 435, 439 on-call  accommodations for on-call personnel, 521 alternatives to current approach, 519-523 arguments for keeping current system, 515 at Spotify, 90 cognitive hacks, 522 cost to humans, 516-519 emergency medicine SRE differences, 509 emergency medicine SRE parallels, 508 emergency medicine ward medicine dis‐  tinction, 512-515  exclusion backlash and, 522 flexible schedules and, 521 improving on-the-job performance, 522 industry-wide compensation model, 521 need for fundamental change in approach  to, 523-527  negative consequences of heroism, 518 opt-out policy, 522 pager fatigue, 71-73 pair on-call, 523 persons with mental disorders and, 495 prioritization, 520-522 production engineering team at Facebook,  215  psychological safety and, 457 rationale for, 508-516 reasons to end, 507-527 recovery time immediately after on-call  shift, 522  Strong-Anti-On-Call position, 524 Strong Weak-Anti-On-Call position, 526 training, 520 triage function, 508 underlying assumptions, 510-512  566      Index  onboarding packets, 498 onsite interview, 19  coding and system questions, 20 cultural fit interviews, 20 deep dives and architecture questions, 20  OpenResty, 414 operating system errors, data loss and, 264 operational debt, 37 operational expenditures  OpEx , 45, 163, 373 operational isolation, 281-283 operations  approaching as engineering problem,  intersections between social activism and,  370-371  537-553  Spotify's early focus on, 82, 85 Spotify's growth and, 87  Operations as a Service  OaaS , 168 operations organization  see enterprise opera‐  tions model–SRE transition   balancing squad autonomy with tech stack  operator fatigue, 289 ops owners, 89 Ops Teams  at Spotify, 91 Ops-in-Squads  consistency, 104-107  benefits, 105 expansion of concept, 102 rollout, 101 Spotify's use of, 98-104 trade-offs, 106 outsourcing, 117   see also third parties   overhead of coordination, 533  P pager fatigue, 71-73 pair on-call, 523 Papert, Seymour, 344 Pareto distribution, 361 partner phase of SRE execution, 375 patterns  automated testing at Google, 178-180 creating shared source code repository,  183-185  DevOps and, 177-185   see also antipatterns    launch and handoff readiness review at  implementing a PE organization outside of  Google, 180-183  of incidents, 478 pro-inclusion of employees with mental dis‐  orders, 493-505  Paul, Paula, 193 PayPal, 197 Pentagon building, 533 percentiles, histograms vs., 363 performance analysis, 374 performance optimization, 374 phone screening, 18 Photon, 351 physical backups, 265 physical isolation, 280 playbooks, documentation of, 329 Poblador i Garcia, David, SRE Without SRE,  81-109  policies, documentation of, 330 political organizing  see social activism  postmortems  as active learning opportunity, 353 documentation of, 329 resolving cross-team reliability issues with,  73  250  248  social activism, 547-549  Potvin, Rachel, 184 privacy engineering, 245-256  automation and, 249 commonalities with SRE, 249-255 default behavior for shared architectures,  differences from SRE, 255 early intervention and education through  evangelism, 253-255  efficient and deliberate problem solving, 251 frameworks and, 250 goals of, 247-249 intersection of reliability and privacy, 246 questions for evaluating products services,  root causing, 252 team relationship management, 252 toil reduction with, 249-251  Probability Density Function  PDF , 360 procurement teams, 48 product-oriented teams, 162 production engineering  PE , 207-229  centralized reporting structure for, 212-214  Facebook, 226-229  key traits of successful engineers, 222-225 limited on-call status of, 215 organizational model for, 212-214 origins at Facebook, 208-212 relationships with other teams at Facebook,  215  stages of team service development, 218 team creation, 219  production meetings, as active learning oppor‐  tunity, 353  production readiness, 47 project operating expense  PrOpEx , 48 project-based funding, 162 Prometheus, 72 promotion of persons with mental disorders,  501  Proof-of-Concept  PoC , 45 provisioning, 374 psychological safety  avoiding ambiguous expectations, 459 avoiding information overload, 457 building into your team, 453-459 clear communication explicit expectations,  456  intersections between operations and social  activism, 537-553  on-call rotations and pager fatigue, 71-73 on-call work, 457, 516-519 operations teams vs. other engineering  operations teams' difficulty in estimating  teams, 457  level of, 459  publicizing celebrating teams successes, 455 respect as part of team culture, 454 space for people to take chances, 455 SRE cognitive work, 461-482 SRE teams and, 28 SRE social activism parallels, 541 successful teams and, 451-459 ways to make teams feel safe, 456  Python, 306  Q quality of service  QoS , 356 queues, ticket-driven, 152-154  Index      567   R Rabenstein, Björn, How to Apply SRE Princi‐ ples Without Dedicated SRE Teams, 65-79 Rampke, Matthias, How to Apply SRE Princi‐ ples Without Dedicated SRE Teams, 65-79  Rasmussen, Jens, 238 Rau, Vivek, 148 reactive phase of SRE execution, 374 real-time dashboards, 40 Real-User Monitoring  RUM   SLIs informed by, 54 third-party integrations and, 51, 58  recovery time  see Mean Time to Recovery   MTTR    recovery recoverability of data  championing recovery reliability, 267 considerations for, 261 database reliability engineering and,  261-267  detection of data loss corruption, 262-264 diverse storage, 264 full physical backups, 265 incremental physical backups, 266 logical backups, 266 object stores, 266 testing, 266 varied toolbox for, 265  recurrent neural networks, 304 Redundant Array of Independent Disks   RAID , 289  redundant systems, 369 Reinertsen, Donald G., 155 release engineering, 410 remote work, 499 Rendell, Mark, 196 repair debt, 37 replication techniques  data durability, engineering for, 276-279 estimating durability, 277-279  reporting APIs, 53, 59, 61 Republican National Convention protests   2008 , 540  request pausing, 420-422 request queues, 152-154 respect, team culture and, 454 resumes, blind review of, 16 roles, formal assignment of, 470 rollbacks, 271 rolling release, blue green release vs., 412  568      Index  root access, 85 root causing  privacy engineering, 252  Root, Lynn, SRE Without SRE, 81-109 Rosenthal, Casey, In the Beginning, There Was  Chaos, 233-243 Rother, Mike, 161 routing, shard-aware  see shard-aware routing  Russek, Johannes, SRE Without SRE, 81-109 Ryan, Andrew, 217  S sacrifice decisions, 467 St. Paul Principles, 540, 546 Samuel, Arthur, 300 scaling, 33-40 scheduling, flexible, 499, 521 Schlossnagle, Theo, The Art and Science of the  Service-Level Objective, 355-364  Schwartz, Mark, 399 scriptable load balancers, 413-428  about, 413-415 advantages of, 415 checkout queue case study, 426-428 defined, 413 future issues, 428 harnessing the potential of, 420 maintaining resiliency, 424-428 problems solved by, 415-422 request pausing, 420-422 routing requests with, 419 service-level middleware, 422-424 shard-aware routing, 416-420 state and, 425  immutable infrastructure and, 409 self-service and, 167 third-party vendors and, 44  security  self-service  and OaaS, 168 benefits to SREs, 167 database reliability engineering and, 258 maximizing effectiveness of, 166 replacing handoffs with, 165-170  separation of duties, 167 servers as cattle vs. servers as pets, 259 service discovery, 438 service meshes, 431-447  configuration management, 442   context propagation, 441 control plane vs. data plane, 442 current state of microservice networking,  eventually consistent service discovery sys‐  434  tem, 438  in practice, 443-447 monolithic architecture vs., 432 observability and alarming, 439 operation of Envoy at Lyft, 445-447 origin and development of Envoy at Lyft,  444  sidecar performance implications, 440 sidecar proxy, 436-438 thin libraries, 441  service overviews, documentation of, 329 Service Pyramid, at Facebook, 217 Service-Level Agreements  SLAs   business priorities and changes in, 143 corner cases and, 142-144 documentation of, 330 error budgets and, 172 negotiating with vendors, 54 nontechnical solutions in SysAdmin–SRE  transition, 136  progression in service-level execution, 372 SLOs and, 126, 355 success culture and, 372 SysAdmin–SRE transition and, 125,  127-133  tracking availability level for, 137-142  Service-Level Indicators  SLIs   black boxes, 53 real-time data and, 54 reporting APIs, 53 RUM and, 54 synthetic monitoring, 54 SysAdmin–SRE transition and, 125 third-party services and, 53-55  service-level middleware  APIs of, 423 scriptable load balancers, 422-424 WAF Bot mitigation case study, 424 Service-Level Objectives  SLOs , 355-364  aligning performance standards with cus‐  tomers' needs, 363  availability, 356-359 data recovery strategies and, 261 error budgets and, 171  evaluating, 359 goal setting and, 355 histograms and, 362-363 percentiles vs. histograms, 363 SysAdmin–SRE transition and, 126 third-party services and, 54  service-oriented teams, 162 Shannon, Adam, 194 shard-aware routing  routing queries in the application, 417 routing requests in the application, 418 routing requests with a scriptable load bal‐  ancer, 419  routing requests with DNS, 416  Sharpe, Jeremy, Do Docs Better, 325-341 Shopify, 426-428 shopping websites, 426-428 Short, Chris, 195 Shoup, Randy, 184 sidecar proxy, 436-438, 440 Siegrist, John, 198 Sigmoid function, 309-312 silos  enterprise operations model–SRE transition  and, 152-154  Kata and, 159 mismatches and, 153 missed SLOs and, 401 Spotify and, 92  Single Points of Failure  SPOFs , 237, 368 Single-Page Application  SPA  frameworks, 51 Sinjakli, Chris, 205 site up, as SRE goal, 367-369  graduated degradation, 369 isolating failure domains, 368 redundant systems, 369  SLA inversion, 424 Slicer, 420 snapshots, 265 social activism  assigning avoiding blame in reviews of, 547 building capacity instead of assigning  blame, 548  crisis management, 542-546 forgiveness as corollary to trust, 545 intersections between operations and,  537-553  planning stage, 539 postmortems, 547-549  Index      569   principles of organizing, 540 software engineering as analogous to, 538 turning action into change, 549-553  Soundcloud, SRE at, 65-79 deployment platform, 67 embedded SREs, 66 failure of team approach, 65-67 getting buy-in, 76-78 implementation details, 71-78 need to adjust approach to circumstances,  66  on-call rotations and pager fatigue, 71-73 postmortems for resolving cross-team relia‐  bility issues, 73  Production Engineering team, 69-71 uniform infrastructure tooling vs. autonomy innovation, 74-76  you build it, you run it approach, 67-71  source control, documentation in, 334 source-code repository, Google, 183-185 space shuttle disasters, 532 speed at scale, safely  s3 , 107 Spotify, 81-109  balancing squad autonomy with tech stack  consistency, 104-107  beta and release period, 84-86 bringing scalability and reliability to the  forefront, 85  challenges in ops team scaling, 96-98 deployment time slots, 90 dev owner role, 88 engineering culture, 81 excessive server growth, 94 forensics, 92 formalizing core services, 89 future of SRE at, 107-109 goalie role, 92 growth and early success  2010 , 87-93 growth-related challenges  2011 , 94-95 interruptions, 92 lead time issues, 91 lightening backend engineers' manual load,  limits to manual deployments, 97 moving away from Ops-owner approach,  99  101-102  new ownership model, 88 on-call and alerting, 90 operations focus in early history, 82  570      Index  ops owner role, 89 Ops-in-Squads  2013-2015 , 98-104 reorganization of dev teams op teams, 95 splitting ops team into Production Ops and  Internal IT Ops, 91  unintentional specialization misalignment,  92  SRE  generally   antipatterns  see antipatterns  defining for larger enterprises, 115-116   see also enterprise operations model–  SRE transition  DevOps and, 177-185 origins of, 365-367 phases of execution, 374-377 success culture and  see success culture,  without teams  see Soundcloud   see Spo‐  SRE as   tify   SRE Classroom  workshop , 350 SRE teams  see teams  staging of third-party integrations, 55 stakeholder identification  in build buy adopt decision, 44 in large enterprises, 114  state, immutable infrastructure and, 407 Stolarsky, Emil, Scriptable Load Balancers,  Stone, Luke, So, You Want to Build an SRE  413-428  Team?, 25-30  storage  and database recovery strategy, 264 object storage, 265 offline, 265 online, high-performance, 264 online, low-performance, 264  Storage Watcher, 287 structural quality, of documentation, 326-328 Suarez Ordoñez, Santiago, 190 success culture, SRE as, 365-377  advocate partner phase of SRE execution,  375  approaching operations as engineering  problem, 370-371  business success through promises  service  levels , 371  capacity planning demand forecasting, 373 catalytic stage of SRE execution, 376   complications of differing phases of SRE  teams  execution, 376  critical enabling functions of SRE, 372-374 empowering teams to do the right thing,  369  374  373  firefighting reactive phase of SRE execution,  focusing on details of success, 377 gatekeeper phase of SRE execution, 375 incident management emergency response,  key values for SRE, 367-372 monitoring, metrics, and KPIs, 373 origins of SRE, 365-367 performance analysis optimization, 374 phases of SRE execution, 374-377 progression in service-level execution, 372 provisioning change management velocity,  374  site up, 367-369  surrogate metrics, 36 synthetic monitoring  SLIs and, 54 third-party integrations and, 51, 57  SysAdmin–SRE transition, 123-145  availability level tracking for SLAs, 137-142 concerns of Site Reliability Engineers vs. those of System Administrators, 123  corner cases and SLAs, 142-144 establishing SLAs for internal components,  127-133  external dependencies, 133-136 key steps in, 144 nontechnical solutions for SLAs, 136 SLAs in, 125 SLIs in, 125 SLOs in, 126 terminology for, 124-127  SysOps, 66  code deploys and, 68 Production Engineering team and, 69-71  systems  see complex systems   T tail latency, 441 taking chances, space for people to, 455 tape backup, 265 Taylor, Frederick Winslow, 463 teaching, active  see active learning   456  119  373  117  374  avoiding ambiguous expectations, 459 avoiding cognitive overload, 458 avoiding information overload, 457 building, 25-30 building psychological safety into, 453-459 capacity planning demand forecasting, 373 choosing SRE for right reasons, 26-28 clear communication explicit expectations,  commitment to SRE, 29 control-based models, 3 cross-functional, 161-165 defining the role of supporting divisions,  empowering to do the right thing, 369 Facebook PE team creation, 219 file pillars of practice, 372-374 goal-setting and metrics, 117 incident management emergency response,  insource outsource approaches to growth,  large enterprises and, 117-119 learning habits of effective SRE teams,  353-354  making a decision about SRE, 30 monitoring, metrics, and KPIs, 373 orienting to a data-driven approach, 28 performance analysis optimization, 374 provisioning change management velocity,  psychological safety for, 451-459 publicizing celebrating successes of, 455 respect as part of culture, 454 rotation of engineering team members into  SRE team, 118  scaling to company size, 66 space for people to take chances, 455 SRE in the development cycle, 118 ways to make teams feel safe, 456  TensorBoard, 315-317 TensorFlow, 313-317 termination, contract, 62 third parties  build buy adopt decision, 43-49 direct impact of downtime, 50 downtime, 50-52 first-class citizens, 49-63  Index      571   growing teams via, 117 indirect impact of downtime, 51 LinkedIn case study, 49 negotiating SLAs with vendors, 54 running the black box like a service, 52 SLIs, SLOs, SLAs, 53-55 SLOs, 54 working with, 43-63 third-party integrations  automation, 60 communication, 62 contract termination, 62 decommissioning, 62 disaster planning, 61 LinkedIn case study, 56 logging, 61 monitoring, 56-59 playbook for, 55-63 reporting APIs, 61 synthetic monitoring, 57 testing and staging, 55 tooling, 59  Three Mile Island nuclear disaster, 464, 466 throttling, 425 ticket-driven request queues, 152-154 time quanta, 357 time to detect  TTD , 34, 37 time to engage  TTE , 34, 37 time to fix  TTF , 35 Todd, Chad, 201 toil  as enemy of SRE, 148-151 defined, 148 engineering work vs., 148-151 enterprise operations model–SRE transi‐  tion, 172  privacy engineering and, 249-251 self-service capabilities and, 167  tooling, third-party integrations and, 59 Toyota Production System, 157 training  for persons with mental disorders, 501 on-call, 520  transactions, as availability metric, 358 transgender inclusivity, 497 Treat, Robert, 203 Treynor Sloss, Benjamin, 173, 177 triage, 508 trust, forgiveness as corollary to, 545  572      Index  2001: A Space Odyssey  movie , 295  U unknown-unknowns, software failure and, 511,  513  US Digital Service, 529 user error, data loss and, 263  V vacation time, 497 van Zijll, Robin, 191 velocity of change, 374 vendor lock-in, 48 verification  coverage, 286-288 data durability engineering and, 286-288 testing the verification system, 288 zero-errors system, 286  virtual repair debt, 38 virtuous cycle, 33-35 visual analysis, 159  W wages  see compensation  Watson, Coburn, Context Versus Control in  Wheel of Misfortune  game , 345 Willis, John, SRE Patterns Loved by DevOps,  SRE, 3-13  177-185  window of vulnerability, 288 Woods' Theorem, 472 work-life balance, 71-73   see also psychological safety   Y you build it, you run it  at Soundcloud, 67-71 deployment platform, 67 Production Engineering team and, 69-71 SysOps and code deploys, 68  Yust, Amber, The Intersection of Reliability  and Privacy, 245-256  Z zero-errors systems, 286 Zhang, Yichun agentzh, 414 Zwieback, Dave, 370   About the Editor David N. Blank-Edelman has over 30 years of experience in the SRE DevOps sysad‐ min field in large multiplatform environments. He currently works for Microsoft as a senior  cloud  operations  advocate  focusing  on  site  reliability  engineering.  He  is  a cofounder of the wildly popular SREcon conferences hosted globally by USENIX, and is  the  author  of  the  O’Reilly  Otter  book   Automating  Systems  Administration  with Perl . Colophon The animal on the cover of Seeking SRE is an Eastern lesser bamboo lemur  Hapale‐ mur griseus , also known as a gray bamboo lemur or gray gentle lemur. It is native to the  island  of  Madagascar.  Lemurs  somewhat  resemble  primates,  but  evolved  inde‐ pendently of monkeys and apes after Madagascar broke away from the African conti‐ nent; the divergence point is estimated to be about 58 to 63 million years ago. The Eastern lesser bamboo lemur has gray fur and is about 11 inches long on average  in addition to a tail length of 12–15 inches . Its diet is largely made up of bamboo shoots  from 75–90% , supplemented with fruit, flowers, and other plant matter. The lemurs have very keen hand-eye coordination and dexterity, and leap vertically from one stalk to another within dense bamboo groves. Bamboo lemurs use a wide range of vocalizations in the wild, including distinct calls for aerial predators, ground predators, mating readiness, and identification. They live in small groups of 6–9, typically made up of one male, breeding females, and depen‐ dent offspring. Females usually have one infant per year, which will be weaned after four  months.  Though  they  do  transport  babies  in  their  mouth  or  on  their  back, mother bamboo lemurs more often leave their young hidden within bamboo stands for short periods while they are away foraging. Some lemur populations may actually benefit from deforestation, as bamboo thrives as secondary growth in cleared land. However, this species is threatened by overhunt‐ ing, especially for the pet trade. Many of the animals on O’Reilly covers are endangered; all of them are important to the world. To learn more about how you can help, go to animals.oreilly.com. The cover image is from the Natural History of Animals. The cover fonts are URW Typewriter and Guardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.

@highlight

Organizations — big and small — have started to realize just how crucial system and application reliability is to their business. At the same time, they’ve also learned just how difficult it is to maintain that reliability while iterating at the speed demanded by the marketplace. Site Reliability Engineering (SRE) is a proven approach to this challenge. SRE is a large and rich topic to discuss. Google led the way with Site Reliability Engineering, the wildly successful O’Reilly book that described Google’s creation of the discipline and the implementation that has allowed them to operate at a planetary scale. Inspired by that earlier work, this book explores a very different part of the SRE space. The more than two dozen chapters in Seeking SRE bring you into some of the important conversations going on in the SRE world right now. Listen as engineers and other leaders in the field discuss different ways of implementing SRE and SRE principles in a wide variety of settings; how SRE relates to other approaches like DevOps; the specialities on the cutting edge that will soon be common place in SRE; best practices and technologies that make practicing SRE easier; and finally hear what people have to say about the important, but rarely discussed human side of SRE. David N. Blank-Edelman is the book’s curator and editor.