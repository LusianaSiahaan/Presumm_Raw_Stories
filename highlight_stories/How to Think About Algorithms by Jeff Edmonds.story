This page intentionally left blank   P1: KAE  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   CUUS154-FM CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  17:52  HOW TO THINK ABOUT ALGORITHMS  There are many algorithm texts that provide lots of well-polished code and proofs of correctness. Instead, this one presents insights, notations, and analogies to help the novice describe and think about algorithms like an expert. It is a bit like a carpenter studying hammers instead of houses. Jeff Edmonds provides both the big picture and easy step-by-step methods for developing algorithms, while avoiding the comon pitfalls. Paradigms such as loop invariants and recursion help to unify a huge range of algorithms into a few meta-algorithms. Part of the goal is to teach students to think abstractly. Without getting bogged down in formal proofs, the book fosters deeper understanding so that how and why each algorithm works is trans- parent. These insights are presented in a slow and clear manner accessible to second- or third-year students of computer science, preparing them to ﬁnd on their own innovative ways to solve problems.  Abstraction is when you translate the equations, the rules, and the under- lying essences of the problem not only into a language that can be commu- nicated to your friend standing with you on a streetcar, but also into a form that can percolate down and dwell in your subconscious. Because, remem- ber, it is your subconscious that makes the miraculous leaps of inspiration, not your plodding perspiration and not your cocky logic. And remember, unlike you, your subconscious does not understand Java code.  i   P1: KAE  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   CUUS154-FM CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  17:52  ii   P1: KAE  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   CUUS154-FM CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  17:52  HOW TO THINK ABOUT ALGORITHMS  JEFF EDMONDS York University  iii   CAMBRIDGE UNIVERSITY PRESS Cambridge, New York, Melbourne, Madrid, Cape Town, Singapore, São Paulo  Cambridge University Press The Edinburgh Building, Cambridge CB2 8RU, UK Published in the United States of America by Cambridge University Press, New York www.cambridge.org Information on this title: www.cambridge.org 9780521849319    Jeff Edmonds 2008  This publication is in copyright. Subject to statutory exception and to the provision of  relevant collective licensing agreements, no reproduction of any part may take place  without the written permission of Cambridge University Press.  First published in print format  2008  ISBN-13 978-0-511-41370-4  eBook  EBL   ISBN-13    978-0-521-84931-9  hardback  ISBN-13    978-0-521-61410-8  paperback  Cambridge University Press has no responsibility for the persistence or accuracy of urls  for external or third-party internet websites referred to in this publication, and does not  guarantee that any content on such websites is, or will remain, accurate or appropriate.   P1: KAE  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   CUUS154-FM CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  17:52  Dedicated to my father, Jack, and to my sons, Joshua and Micah.  May the love and the mathematics continue to ﬂow between the generations.  v   P1: KAE  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   CUUS154-FM CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  17:52  Problem Solving  Out of the Box Leaping  Deep Thinking  Creative Abstracting  Logical Deducing  with Friends Working  Fun Having  Fumbling and Bumbling Bravely Persevering  Joyfully Succeeding  vi   P1: KAE  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   CUUS154-FM CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  17:52  CONTENTS  vii  Preface   page  xi  Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..  1  PART ONE. ITERATIVE ALGORITHMS AND LOOP INVARIANTS  1 Iterative Algorithms: Measures of Progress and Loop Invariants . . . . . 5  1.1 A Paradigm Shift: A Sequence of Actions vs. a Sequence of  Assertions  2.1 Coloring the Plane 2.2 Deterministic Finite Automaton 2.3 More of the Input vs. More of the Output  1.2 The Steps to Develop an Iterative Algorithm 1.3 More about the Steps 1.4 Different Types of Iterative Algorithms 1.5 Typical Errors 1.6 Exercises  5 8 12 21 26 27 2 Examples Using More-of-the-Input Loop Invariants . . . . . . . . . . . . 29 29 31 39 3 Abstract Data Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 44 51 56 57 4 Narrowing the Search Space: Binary Search . . . . . . . . . . . . . . . . 60 60 62 65 69 5 Iterative Sorting Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . 71 71  3.1 Speciﬁcations and Hints at Implementations 3.2 Link List Implementation 3.3 Merging with a Queue 3.4 Parsing with a Stack  4.1 Binary Search Trees 4.2 Magic Sevens 4.3 VLSI Chip Testing 4.4 Exercises  5.1 Bucket Sort by Hand   P1: KAE  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   CUUS154-FM CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  17:52  Contents  5.2 Counting Sort  a Stable Sort  5.3 Radix Sort 5.4 Radix Counting Sort  72 75 76 6 Euclid’s GCD Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 79 7 The Loop Invariant for Lower Bounds . . . . . . . . . . . . . . . . . . . 85  viii  PART TWO. RECURSION  8.1 Thinking about Recursion 8.2 Looking Forward vs. Backward 8.3 With a Little Help from Your Friends 8.4 The Towers of Hanoi 8.5 Checklist for Recursive Algorithms 8.6 The Stack Frame 8.7 Proving Correctness with Strong Induction  8 Abstractions, Techniques, and Theory . . . . . . . . . . . . . . . . . . . 97 97 99 100 102 104 110 112 9 Some Simple Examples of Recursive Algorithms . . . . . . . . . . . . . 114 114 122 127 128 10 Recursion on Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130 133 135 138 141 149 11 Recursive Images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153  10.1 Tree Traversals 10.2 Simple Examples 10.3 Generalizing the Problem Solved 10.4 Heap Sort and Priority Queues 10.5 Representing Expressions with Trees  9.1 Sorting and Selecting Algorithms 9.2 Operations on Integers 9.3 Ackermann’s Function 9.4 Exercises  11.1 Drawing a Recursive Image from a Fixed Recursive and a Base  Case Image  153 156 12 Parsing with Context-Free Grammars . . . . . . . . . . . . . . . . . . . 159  11.2 Randomly Generating a Maze  PART THREE. OPTIMIZATION PROBLEMS 13 Deﬁnition of Optimization Problems . . . . . . . . . . . . . . . . . . . 171 14 Graph Search Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . 173 174 179 183 188 192 194 196  14.1 A Generic Search Algorithm 14.2 Breadth-First Search for Shortest Paths 14.3 Dijkstra’s Shortest-Weighted-Path Algorithm 14.4 Depth-First Search 14.5 Recursive Depth-First Search 14.6 Linear Ordering of a Partial Order 14.7 Exercise   P1: KAE  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   CUUS154-FM CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  17:52  ix  Contents  15 Network Flows and Linear Programming . . . . . . . . . . . . . . . . 198 200 206 214 219 223  15.1 A Hill-Climbing Algorithm with a Small Local Maximum 15.2 The Primal–Dual Hill-Climbing Method 15.3 The Steepest-Ascent Hill-Climbing Algorithm 15.4 Linear Programming 15.5 Exercises  16.1 Abstractions, Techniques, and Theory 16.2 Examples of Greedy Algorithms  16 Greedy Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225 225 236 236 240 244 250  16.2.1 Example: The Job Event Scheduling Problem 16.2.2 Example: The Interval Cover Problem 16.2.3 Example: The Minimum-Spanning-Tree Problem  16.3 Exercises  17 Recursive Backtracking . . . . . . . . . . . . . . . . . . . . . . . . . . . 251 251 256 260 261 265  17.1 Recursive Backtracking Algorithms 17.2 The Steps in Developing a Recursive Backtracking 17.3 Pruning Branches 17.4 Satisﬁability 17.5 Exercises  18.1 Start by Developing a Recursive Backtracking 18.2 The Steps in Developing a Dynamic Programming Algorithm 18.3 Subtle Points  18 Dynamic Programming Algorithms . . . . . . . . . . . . . . . . . . . . 267 267 271 277 278 281 284 288 291 292  18.3.1 The Question for the Little Bird 18.3.2 Subinstances and Subsolutions 18.3.3 The Set of Subinstances 18.3.4 Decreasing Time and Space 18.3.5 Counting the Number of Solutions 18.3.6 The New Code  19 Examples of Dynamic Programs . . . . . . . . . . . . . . . . . . . . . . 295 295  19.1 The Longest-Common-Subsequence Problem 19.2 Dynamic Programs as More-of-the-Input Iterative Loop  19.3 A Greedy Dynamic Program: The Weighted Job Event  Invariant Algorithms  Scheduling Problem  19.4 The Solution Viewed as a Tree: Chains of Matrix Multiplications 19.5 Generalizing the Problem Solved: Best AVL Tree 19.6 All Pairs Using Matrix Multiplication 19.7 Parsing with Context-Free Grammars 19.8 Designing Dynamic Programming Algorithms via Reductions  300  303 306 311 314 315 318   P1: KAE  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   CUUS154-FM CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  17:52  Contents  x  20 Reductions and NP-Completeness . . . . . . . . . . . . . . . . . . . . . 324 326 330 338  20.1 Satisﬁability Is at Least as Hard as Any Optimization Problem 20.2 Steps to Prove NP-Completeness 20.3 Example: 3-Coloring Is NP-Complete 20.4 An Algorithm for Bipartite Matching Using the Network  Flow Algorithm  342 21 Randomized Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . 346 347 350  21.1 Using Randomness to Hide the Worst Cases 21.2 Solutions of Optimization Problems with a Random Structure  23.1 The Time  and Space  Complexity of an Algorithm 23.2 The Time Complexity of a Computational Problem  PART FOUR. APPENDIX 22 Existential and Universal Quantiﬁers . . . . . . . . . . . . . . . . . . . 357 23 Time Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 366 366 371 24 Logarithms and Exponentials . . . . . . . . . . . . . . . . . . . . . . . 374 25 Asymptotic Growth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377 379 384 26 Adding-Made-Easy Approximations . . . . . . . . . . . . . . . . . . . . 388 389 393 27 Recurrence Relations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 398 398 401 28 A Formal Proof of Correctness . . . . . . . . . . . . . . . . . . . . . . . 408  26.1 The Technique 26.2 Some Proofs for the Adding-Made-Easy Technique  25.1 Steps to Classify a Function 25.2 More about Asymptotic Notation  27.1 The Technique 27.2 Some Proofs  PART FIVE. EXERCISE SOLUTIONS . . . . . . . . . . . . . . . . . . . . . . . . . . . . 411  Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 437  Index  439   P1: KAE  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   CUUS154-FM CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  17:52  PREFACE  xi  To the Educator and the Student  This book is designed to be used in a twelve-week, third-year algorithms course. The goal is to teach students to think abstractly about algorithms and about the key algo- rithmic techniques used to develop them.  Meta-Algorithms: Students must learn so many algorithms that they are sometimes overwhelmed. In order to facilitate their understanding, most textbooks cover the standard themes of iterative algorithms, recursion, greedy algorithms, and dynamic programming. Generally, however, when it comes to presenting the algorithms them- selves and their proofs of correctness, the concepts are hidden within optimized code and slick proofs. One goal of this book is to present a uniform and clean way of thinking about algorithms. We do this by focusing on the structure and proof of correctness of iterative and recursive meta-algorithms, and within these the greedy and dynamic programming meta-algorithms. By learning these and their proofs of correctness, most actual algorithms can be easily understood. The challenge is that thinking about meta-algorithms requires a great deal of abstract thinking.  Abstract Thinking: Students are very good at learning how to apply a concrete code to a concrete input in- stance. They tend, however, to ﬁnd it difﬁcult to think abstractly about the algorithms. I maintain that the more abstractions a person has from which to view the problem, the deeper his understanding of it will be, the more tools he will have at his disposal, and the bet- ter prepared he will be to design his own innovative ways to solve new problems. Hence, I present a number of different notations, analogies, and paradigms within which to develop and to think about algorithms.   P1: KAE  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   CUUS154-FM CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  17:52  Preface  Way of Thinking: People who develop algorithms have various ways of thinking and intuition that tend not to get taught. The assumption, I suppose, is that these cannot be taught but must be ﬁgured out on one’s own. This text attempts to teach students to think like a designer of algorithms.  xii  Not a Reference Book: My intention is not to teach a speciﬁc selection of algorithms for speciﬁc purposes. Hence, the book is not organized according to the application of the algorithms, but according to the techniques and abstractions used to develop them.  Developing Algorithms: The goal is not to present completed algorithms in a nice clean package, but to go slowly through every step of the development. Many false starts have been added. The hope is that this will help students learn to develop al- gorithms on their own. The difference is a bit like the difference between studying carpentry by looking at houses and by looking at hammers.  Proof of Correctness: Our philosophy is not to follow an algorithm with a formal proof that it is correct. Instead, this text is about learning how to think about, de- velop, and describe algorithms in such way that their correctness is transparent.  Big Picture vs. Small Steps: For each topic, I attempt both to give the big picture and to break it down into easily understood steps.  Level of Presentation: This material is difﬁcult. There is no getting around that. I have tried to ﬁgure out where confusion may arise and to cover these points in more detail. I try to balance the succinct clarity that comes with mathematical formalism against the personiﬁed analogies and metaphors that help to provide both intuition and humor.  Point Form: The text is organized into blocks, each containing a title and a single thought. Hopefully, this will make the text easier to lecture and study from.  Prerequisites: The text assumes that the students have completed a ﬁrst-year programming course and have a general mathematical maturity. The Appendix  Part Four  covers much of the mathematics that will be needed.  Homework Questions: A few homework questions are included. I am hoping to de- velop many more, along with their solutions. Contributions are welcome.  Read Ahead: The student is expected to read the material before the lecture. This will facilitate productive discussion during class.  Explaining: To be able to prove yourself on a test or on the job, you need to be able to explain the material well. In addition, explaining it to someone else is the best way to learn it yourself. Hence, I highly recommend spending a lot of time explaining   P1: KAE  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   CUUS154-FM CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  17:52  Preface  the material over and over again out loud to yourself, to each other, and to your stuffed bear.  Dreaming: I would like to emphasis the importance of thinking, even daydreaming, about the material. This can be done while going through your day – while swim- ming, showering, cooking, or lying in bed. Ask ques- tions. Why is it done this way and not that way? In- vent other algorithms for solving a problem. Then look for input instances for which your algorithm gives the wrong answer. Mathematics is not all linear thinking. If the essence of the material, what the questions are really asking, is allowed to seep down into your subconscious then with time little thoughts will begin to percolate up. Pursue these ideas. Sometimes even ﬂashes of inspiration appear.  xiii  Acknowledgments  I would like to thank Andy Mirzaian, Franck van Breugel, James Elder, Suprakash Datta, Eric Ruppert, Russell Impagliazzo, Toniann Pitassi, and Kirk Pruhs, with whom I co-taught and co-researched algorithms for many years. I would like to thank Jen- nifer Wolfe and Lauren Cowles for their fantastic editing jobs. All of these people were a tremendous support for this work.   P1: KAE  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   CUUS154-FM CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  17:52  xiv   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 25, 2008  19:8  Introduction  1  From determining the cheapest way to make a hot dog to monitoring the workings of a factory, there are many complex computational problems to be solved. Before executable code can be produced, computer scientists need to be able to design the algorithms that lie behind the code, be able to understand and describe such algo- rithms abstractly, and be conﬁdent that they work correctly and efﬁciently. These are the goals of computer scientists.  A Computational Problem: A speciﬁcation of a computational problem uses pre- conditions and postconditions to describe for each legal input instance that the com- putation might receive, what the required output or actions are. This may be a func- tion mapping each input instance to the required output. It may be an optimization problem which requires a solution to be outputted that is “optimal” from among a huge set of possible solutions for the given input instance. It may also be an ongoing system or data structure that responds appropriately to a constant stream of input.  Example: The sorting problem is deﬁned as follows:  Preconditions: The input is a list of n values, including possible repetitions.  Postconditions: The output is a list consisting of the same n values in non- decreasing order.  An Algorithm: An algorithm is a step-by-step procedure which, starting with an in- put instance, produces a suitable output. It is described at the level of detail and ab- straction best suited to the human audience that must understand it. In contrast, code is an implementation of an algorithm that can be executed by a computer. Pseu- docode lies between these two.  An Abstract Data Type: Computers use zeros and ones, ANDs and ORs, IFs and GOTOs. This does not mean that we have to. The description of an algorithm may talk of abstract objects such as integers, reals, strings, sets, stacks, graphs, and trees;   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  Introduction  March 25, 2008  19:8  abstract operations such as “sort the list,” “pop the stack,” or “trace a path”; and ab- stract relationships such as greater than, preﬁx, subset, connected, and child. To be useful, the nature of these objects and the effect of these operations need to be un- derstood. However, in order to hide details that are tedious or irrelevant, the precise implementations of these data structure and algorithms do not need to be speciﬁed. For more on this see Chapter 3.  2  Correctness: An algorithm for the problem is correct if for every legal input instance, the required output is produced. Though a certain amount of logical thinking is re- quireds, the goal of this text is to teach how to think about, develop, and describe algorithms in such way that their correctness is transparent. See Chapter 28 for the formal steps required to prove correctness, and Chapter 22 for a discussion of forall and exist statements that are essential for making formal statements.  Running Time: It is not enough for a computation to eventually get the correct answer. It must also do so using a reasonable amount of time and memory space. The running time of an algorithm is a function from the size n of the input in- stance given to a bound on the number of operations the computation must do.  See Chapter 23.  The algorithm is said to be feasible if this function is a polynomial like Time n  =  cid:1  n2 , and is said to be infeasible if this function is an exponential like Time n  =  cid:1  2n .  See Chapters 24 and 25 for more on the asymptotics of functions.   cid:1  To be able to compute the running time, one needs to be able to add up the times taken in each iteration of a loop and to solve the recurrence relation deﬁning the i=1 i =  cid:1  n2 , time of a recursive program.  See Chapter 26 for an understanding of and Chapter 27 for an understanding of T n  = 2T  n 2   + n =  cid:1  n log n .   n  Meta-algorithms: Most algorithms are best described as being either iterative or recursive. An iterative algorithm  Part One  takes one step at a time, ensuring that each step makes progress while maintaining the loop invariant. A recursive algorithm  Part Two  breaks its instance into smaller instances, which it gets a friend to solve, and then combines their solutions into one of its own.  Optimization problems  Part Three  form an important class of computational problems. The key algorithms for them are the following. Greedy algorithms  Chap- ter 16  keep grabbing the next object that looks best. Recursive backtracking algo- rithms  Chapter 17  try things and, if they don’t work, backtrack and try something else. Dynamic programming  Chapter 18  solves a sequence of larger and larger in- stances, reusing the previously saved solutions for the smaller instances, until a solu- tion is obtained for the given instance. Reductions  Chapter 20  use an algorithm for one problem to solve another. Randomized algorithms  Chapter 21  ﬂip coins to help them decide what actions to take. Finally, lower bounds  Chapter 7  prove that there are no faster algorithms.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  PART ONE  Iterative Algorithms and Loop Invariants  3   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  4   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  1 Iterative Algorithms: Measures of  Progress and Loop Invariants  5  Using an iterative algorithm to solve a computa- tional problem is a bit like following a road, possibly long and difﬁcult, from your start location to your destination. With each iteration, you have a method that takes you a single step closer. To ensure that you move forward, you need to have a measure of progress telling you how far you are either from your starting location or from your destination. You cannot expect to know exactly where the algorithm will go, so you need to expect some weaving and winding. On the other hand, you do not want to have to know how to handle every ditch and dead end in the world. A compromise between these two is to have a loop invariant, which deﬁnes a road  or region  that you may not leave. As you travel, worry about one step at a time. You must know how to get onto the road from any start location. From every place along the road, you must know what actions you will take in order to step forward while not leaving the road. Finally, when sufﬁcient progress has been made along the road, you must know how to exit and reach your destination in a reasonable amount of time.  A Paradigm Shift: A Sequence of Actions vs. a Sequence  1.1 of Assertions  Understanding iterative algorithms requires understanding the difference between a loop invariant, which is an assertion or picture of the computation at a particular point in time, and the actions that are required to maintain such a loop invariant. Hence, we will start with trying to understand this difference.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  6  Iterative Algorithms and Loop Invariants  One of the ﬁrst important paradigm shifts that programmers struggle to make is from viewing an algorithm as a sequence of actions to viewing it as a sequence of snapshots of the state of the computer. Programmers tend to ﬁxate on the ﬁrst view, because code is a sequence of instructions for action and a computation is a sequence of actions. Though this is an impor- tant view, there is another. Imagine stopping time at key points during the computation and taking still pictures of the state of the computer. Then a computation can equally be viewed as a sequence of such snapshots. Having two ways of viewing the same thing gives one both more tools to handle it and a deeper understanding of it. An example of viewing a computation as an alteration between assertions about the current state of the computation and blocks of actions that bring the state of the computation to the next state is shown here.  Max a,     b,     c   m = a  PreCond: Input has 3 numbers.  assert: m is max in {a}.  assert: m is max in {a,b}.  if b > m   m = b  end if  if c > m   m = c  end if  return m   assert: m is max in {a,b,c}.  PostCond: return max in {a,b,c}.  end algorithm  The Challenge of the Sequence-of-Actions View: Suppose one is designing a new algorithm or explaining an algorithm to a friend. If one is thinking of it as se- quence of actions, then one will likely start at the beginning: Do this. Do that. Do this. Shortly one can get lost and not know where one is. To handle this, one simulta- neously needs to keep track of how the state of the computer changes with each new action. In order to know what action to take next, one needs to have a global plan of where the computation is to go. To make it worse, the computation has many IFs and LOOPS so one has to consider all the various paths that the computation may take.  The Advantages of the Sequence of Snapshots View: This new paradigm is useful one from which one can think about, explain, or develop an algorithm.  Pre- and Postconditions: Before one can consider an algorithm, one needs to care- fully deﬁne the computational problem being solved by it. This is done with pre- and postconditions by providing the initial picture, or assertion, about the input instance and a corresponding picture or assertion about required output.  Start in the Middle: Instead of starting with the ﬁrst line of code, an alternative way to design an algorithm is to jump into the middle of the computation and to draw a static picture, or assertion, about the state we would like the computation to be in at this time. This picture does not need to state the exact value of each variable.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  Measures of Progress and Loop Invariants  Instead, it gives general properties and relationships between the various data struc- tures that are key to understanding the algorithm. If this assertion is sufﬁciently gen- eral, it will capture not just this one point during the computation, but many similar points. Then it might become a part of a loop.  Sequence of Snapshots: Once one builds up a sequence of assertions in this way, one can see the entire path of the computation laid out before one.  7  Fill in the Actions: These assertions are just static snapshots of the computation with time stopped. No actions have been considered yet. The ﬁnal step is to ﬁll in actions  code  between consecutive assertions.  One Step at a Time: Each such block of actions can be executed completely inde- pendently of the others. It is much easier to consider them one at a time than to worry about the entire computation at once. In fact, one can complete these blocks in any order one wants and modify one block without worrying about the effect on the others.  Fly In from Mars: This is how you should ﬁll in the code between the ith and the i + 1st assertions. Suppose you have just ﬂown in from Mars, and absolutely the only thing you know about the current state of your computation is that the ith assertion holds. The computation might actually be in a state that is completely impossible to arrive at, given the algorithm that has been designed so far. It is allowing this that provides independence between these blocks of actions.  Take One Step: Being in a state in which the ith assertion holds, your task is simply to write some simple code to do a few simple actions, that change the state of the computation so that the i + 1st assertion holds.  Proof of Correctness of Each Step: The proof that your algorithm works can also be done one block at a time. You need to prove that if time is stopped and the state of the computation is such that the ith assertion holds and you start time again just long enough to execute the next block of code, then when you stop time again the state of the computation will be such that the i + 1st assertion holds. This proof might be a formal mathematical proof, or it might be informal handwaving. Either way, the formal statement of what needs to be proved is as follows:   cid:2 ith−assertion cid:3 & codei ⇒  cid:2 i + 1st−assertion cid:3   Proof of Correctness of the Algorithm: All of these individual steps can be put together into a whole working algorithm. We assume that the input instance given meets the precondition. At some point, we proved that if the precondition holds and the ﬁrst block of code is executed, then the state of the computation will be such   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  Iterative Algorithms and Loop Invariants  that ﬁrst assertion holds. At some other point, we proved that if the ﬁrst assertion holds and the second block of code is executed then the state of the computation will be such that second assertion holds. This was done for each block. All of these independently proved statements can be put together to prove that if initially the input instance meets the precondition and the entire code is executed, then in the end the state of the computation will be such that the postcondition has been met. This is what is required to prove that algorithm works.  8  1.2  The Steps to Develop an Iterative Algorithm  Iterative Algorithms: A good way to structure many computer programs is to store the key information you currently know in some data structure and then have each iteration of the main loop take a step towards your destination by making a simple change to this data.  Loop Invariant: A loop invariant expresses important relationships among the variables that must be true at the start of every iteration and when the loop termi- nates. If it is true, then the computation is still on the road. If it is false, then the algorithm has failed.  The Code Structure: The basic structure of the code is as follows.  begin routine  cid:2 pre-cond cid:3  codepre-loop % Establish loop invariant loop cid:2 loop-invariant  cid:3   end loop codepost-loop % Clean up loose ends  cid:2 post-cond  cid:3   end routine  exit when  cid:2 exit-cond  cid:3  codeloop % Make progress while maintaining the loop invariant  Proof of Correctness: Naturally, you want to be sure your algorithm will work on all speciﬁed inputs and give the correct answer.  Running Time: You also want to be sure that your algorithm completes in a reason- able amount of time.  The Most Important Steps: If you need to design an algorithm, do not start by typ- ing in code without really knowing how or why the algorithm works. Instead, I recom- mend ﬁrst accomplishing the following tasks. See Figure 1.1. These tasks need to ﬁt   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  Measures of Progress and Loop Invariants  Define Problem  Define Loop  Invariants  Define Measure of  Progress  79 km  to school  Define Step  Define Exit Condition  Maintain Loop Inv  9  Exit  Make Progress  Initial Conditions  Ending  Figure 1.1: The requirements of an iterative algorithm.  together in very subtle ways. You may have to cycle through them a number of times, adjusting what you have done, until they all ﬁt together as required.  1  Speciﬁcations: What problem are you solving? What are its pre- and postcon- ditions—i.e., where are you starting and where is your destination?  2  Basic Steps: What basic steps will head you more or less in the correct direction?  3  Measure of Progress: You must deﬁne a measure of progress: where are the mile markers along the road?  4  The Loop Invariant: You must deﬁne a loop invariant that will give a picture of the state of your computation when it is at the top of the main loop, in other words, deﬁne the road that you will stay on.  5  Main Steps: For every location on the road, you must write the pseudocode codeloop to take a single step. You do not need to start with the ﬁrst location. I rec- ommend ﬁrst considering a typical step to be taken during the middle of the compu- tation.  6  Make Progress: Each iteration of your main step must make progress according to your measure of progress.  7  Maintain Loop Invariant: Each iteration of your main step must ensure that the loop invariant is true again when the computation gets back to the top of the loop.  Induction will then prove that it remains true always.   8  Establishing the Loop Invariant: Now that you have an idea of where you are go- ing, you have a better idea about how to begin. You must write the pseudocode   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  Iterative Algorithms and Loop Invariants  codepre-loop to initially establish the loop invariant. How do you get from your house onto the correct road? 9  Exit Condition: You must write the condition  cid:2 exit-cond  cid:3  that causes the compu- tation to break out of the loop.  10  10  Ending: How does the exit condition together with the invariant ensure that the problem is solved? When at the end of the road but still on it, how do you produce the required output? You must write the pseudocode codepost-loop to clean up loose ends and to return the required output.  11  Termination and Running Time: How much progress do you need to make be- fore you know you will reach this exit? This is an estimate of the running time of your algorithm.  12  Special Cases: When ﬁrst attempting to design an algorithm, you should only consider one general type of input instances. Later, you must cycle through the steps again considering other types of instances and special cases. Similarly, test your al- gorithm by hand on a number of different examples.  13  Coding and Implementation Details: Now you are ready to put all the pieces to- gether and produce pseudocode for the algorithm. It may be necessary at this point to provide extra implementation details.  14  Formal Proof: If the above pieces ﬁt together as required, then your algorithm works.  EXAMPLE 1.2.1  The Find-Max Two-Finger Algorithm to Illustrate These Ideas  1  Speciﬁcations: An input instance consists of a list L 1..n  of elements. The output consists of an index i such that L i  has maximum value. If there are multiple entries with this same value, then any one of them is returned.  2  Basic Steps: You decide on the two-ﬁnger method. Your right ﬁnger runs down the list.  3  Measure of Progress: The measure of progress is how far along the list your right ﬁnger is.  4  The Loop Invariant: The loop invariant states that your left ﬁnger points to one of the largest entries encountered so far by your right ﬁnger.  5  Main Steps: Each iteration, you move your right ﬁnger down one entry in the list. If your right ﬁnger is now pointing at an entry that is larger then the left ﬁnger’s entry, then move your left ﬁnger to be with your right ﬁnger.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  Measures of Progress and Loop Invariants  6  Make Progress: You make progress because your right ﬁnger moves one entry.  7  Maintain Loop Invariant: You know that the loop invariant has been maintained as follows. For each step, the new left ﬁnger element is Max old left ﬁnger element, new element . By the loop invariant, this is Max Max shorter list , new element . Mathe- matically, this is Max longer list .  8  Establishing the Loop Invariant: You initially establish the loop invariant by point- ing both ﬁngers to the ﬁrst element.  9  Exit Condition: You are done when your right ﬁnger has ﬁnished traversing the list.  10  Ending: In the end, we know the problem is solved as follows. By the exit condi- tion, your right ﬁnger has encountered all of the entries. By the loop invariant, your left ﬁnger points at the maximum of these. Return this entry.  11  Termination and Running Time: The time required is some constant times the length of the list.  12  Special Cases: Check what happens when there are multiple entries with the same value or when n = 0 or n = 1. 13  Coding and Implementation Details:  11  algorithm Find Max L   cid:2  pre-cond cid:3 : L is an array of n values.  cid:2  post-cond cid:3 : Returns an index with maximum value. begin  i = 1; j = 1 loop cid:2 loop-invariant cid:3 : L[i] is max in L[1..j ].  exit when  j ≥ n  % Make progress while maintaining the loop invariant j = j + 1 if  L[i] < L[j ]   then i = j  end loop return i  end algorithm  14  Formal Proof: The correctness of the algorithm follows from the above steps.  A New Way of Thinking: You may be tempted to believe that measures of progress and loop invariants are theoretical irrelevancies. But industry, after many expensive mistakes, has a deeper appreciation for the need for correctness. Our philosophy is to learn how to think about, develop, and describe algorithms in such a way that their correctness is transparent. For this, measures of progress and loop invariants are   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  Iterative Algorithms and Loop Invariants  essential. The description of the preceding algorithms and their proofs of correctness are wrapped up into one.  12  Keeping Grounded: Loop invariants constitute a life philosophy. They lead to feel- ing grounded. Most of the code I mark as a teacher makes me feel ungrounded. It cycles, but I don’t know what the variables mean, how they ﬁt together, where the algorithm is going, or how to start thinking about it. Loop invariants mean starting my day at home, where I know what is true and what things mean. From there, I have enough conﬁdence to venture out into the unknown. However, loop invariants also mean returning full circle to my safe home at the end of my day.  EXERCISE 1.2.1 What are the formal mathematical things involving loop invariants that must be proved, to prove that if your program exits then it obtains the postcondi- tion?  1.3 More about the Steps  In this section I give more details about the steps for developing an iterative algo- rithm.  1  Speciﬁcations: Before we can design an iterative algorithm, we need to know precisely what it is supposed to do.  Preconditions: What are the legal input instances? Any assertions that are promised to be true about the input instance are referred to as preconditions.  Postconditions: What is the required output for each legal instance? Any asser- tions that must be true about the output are referred to as postconditions.  Correctness: An algorithm for the problem is correct if for every legal input in- stance, the required output is produced. If the input instance does not meet the preconditions, then all bets are off. Formally, we express this as   cid:2 pre-cond  cid:3  & codealg ⇒  cid:2 post-cond  cid:3  This correctness is only with respect to the speciﬁcations.  Example: The sorting problem is deﬁned as follows:  Preconditions: The input is a list of n values, including possible repeatations.  Postconditions: The output is a list consisting of the same n values in non- decreasing order.  The Contract: Pre- and postconditions are, in a sense, the contract between the implementer and the user  or invoker  of the coded algorithm.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  Measures of Progress and Loop Invariants  Implementer: When you are writing a subroutine, you can assume the input comes to your program in the correct form, satisfying all the preconditions. You must write the subroutine so that it ensures that the postconditions hold after execution.  User: When you are using the subroutine, you must ensure that the input you provide meets the preconditions of the subroutine. Then you can trust that the output meets its postconditions.  13  2  Basic Steps: As a preliminary to designing the algorithm it can be helpful to con- sider what basic steps or operations might be performed in order to make progress towards solving this problem. Take a few of these steps on a simple input instance in order to get some intuition as to where the computation might go. How might the information gained narrow down the computation problem?  3  Measure of Progress: You need to deﬁne a function that, when given the cur- rent state of the computation, returns an integer value measuring either how much progress the computation has already made or how much progress still needs to be made. This is referred to either as a measure of progress or as a potential function. It must be such that the total progress required to solve the problem is not inﬁnite and that at each iteration, the computation makes progress. Beyond this, you have com- plete freedom to deﬁne this measure as you like. For example, your measure might state the amount of the output produced, the amount of the input considered, the extent to which the search space has been narrowed, some more creative function of the work done so far, or how many cases have been tried. Section 1.4 outlines how these different measures lead to different types of iterative algorithms.  4  The Loop Invariant: Often, coming up with the loop invariant is the hardest part of designing an algorithm. It requires practice, perseverance, creativity, and in- sight. However, from it the rest of the algorithm often follows easily. Here are a few helpful pointers.  Deﬁnition: A loop invariant is an assertion that is placed at the top of a loop and that must hold true every time the computation returns to the top of the loop.  Assertions: More generally, an assertion is a statement made at some particular point during the execution of an algorithm about the current state of the com- putation’s data structures that is either true or false. If it is false, then something has gone wrong in the logic of the algorithm. Pre- and postconditions are special cases of assertions that provide clean boundaries between systems, subsystems, routines, and subroutines. Within such a part, assertions can also provide check- points along the path of the computation to allow everyone to know what should have been accomplished so far. Invariants are the same, except they apply either   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  14  Iterative Algorithms and Loop Invariants  to a loop that is executed many times or to an object-oriented data structure that has an ongoing life.  Designing, Understanding, and Proving Correct: Generally, assertions are not tasks for the algorithm to perform, but are only comments that are added to assist the designer, the implementer, and the reader in understanding the algorithm and its correctness.  Debugging: Some languages allow you to insert assertions as lines of code. If during the execution such an assertion is false, then the program automati- cally stops with a useful error message. This is helpful both when debugging and after the code is complete. It is what is occurring when an error box pops up during the execution of a program telling you to contact the vendor if the error persists. Not all interesting assertions, however, can be tested feasibly within the computation itself.  Picture from the Middle: A loop invariant should describe what you would like the data structure to look like when the computation is at the beginning of an iteration. Your description should leave your reader with a visual image. Draw a picture if you like.  Don’t Be Frightened: A loop invariant need not consist of formal mathematical mumbo jumbo if an informal description gets the idea across better. On the other hand, English is sometimes misleading, and hence a more mathematical lan- guage sometimes helps. Say things twice if necessary. I recommend pretending that you are describing the algorithm to a ﬁrst-year student.  On the Road: A loop invariant must ensure that the computation is still on the road towards the destination and has not fallen into a ditch or landed in a tree.  A Wide Road: Given a ﬁxed algorithm on a ﬁxed input, the computation will fol- low one ﬁxed line. When the algorithm designer knows exactly where this line will go, he can use a very tight loop invariant to deﬁne a very narrow road. On the other hand, because your algorithm must work for an inﬁnite number of input instances and because you may pass many obstacles along the way, it can be dif- ﬁcult to predict where the computation might be in the middle of its execution. In such cases, using a very loose loop invariant to deﬁne a very wide road is com- pletely acceptable. The line actually followed by the computation might weave and wind, but as long as it stays within the boundaries of the road and continues to make progress, all is well. An advantage of a wide road is that it gives more ﬂexibility in how the main loop is implemented. A disadvantage is that there are then more places where the computation might be, and for each the algorithm must deﬁne how to take a step.  Example: As an example of a loose loop invariant, in the ﬁnd-max two-ﬁnger algorithm, the loop invariant does not completely dictate which entry your   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  Measures of Progress and Loop Invariants  left ﬁnger should point at when there are a number of entries with the same maximum value.  Meaningful and Achievable: You want a loop invariant that is meaningful, mean- ing it is strong enough that, with an appropriate exit condition, it will guarantee the postcondition. You also want the loop invariant to be achievable, meaning you can establish and maintain it.  15  Know What a Loop Invariant Is: Be clear about what a loop invariant is. It is not code, a precondition, a postcondition, or some other inappropriate piece of in- formation. For example, stating something that is always true, such as “1 + 1 = 2” or “The root is the max of any heap,” may be useful information for the answer to the problem, but should not be a part of the loop invariant.  Flow Smoothly: The loop invariant should ﬂow smoothly from the begin- ning to the end of the algorithm.  cid:1  At the beginning, it should follow  easily from the preconditions.   cid:1  It should progress in small natural  steps.   cid:1  Once the exit condition has been the postconditions should  met, easily follow.  Ask for 100%: A good philosophy in life is to ask for 100% of what you want, but not to assume that you will get it.  Dream: Do not be shy. What would you like to be true in the middle of your computation? This may be a reasonable loop invariant, and it may not be.  Pretend: Pretend that a genie has granted your wish. You are now in the mid- dle of your computation, and your dream loop invariant is true.  Maintain the Loop Invariant: From here, are you able to take some compu- tational steps that will make progress while maintaining the loop invariant? If so, great. If not, there are two common reasons.  Too Weak: If your loop invariant is too weak, then the genie has not pro- vided you with everything you need to move on.  Too Strong: If your loop invariant is too strong, then you will not be able to establish it initially or maintain it.  No Unstated Assumptions: You don’t want loop invariants that lack detail or are too weak to proceed to the next step. Don’t make assumptions that you don’t   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  Iterative Algorithms and Loop Invariants  16  state. As a check, pretend that you are a Martian who has jumped into the top of the loop knowing nothing that is not stated in the loop invariant.  Example: In the ﬁnd-max two-ﬁnger algorithm, the loop in- variant does make some unstated assumptions. It assumes that the numbers above your right ﬁnger have been en- countered by your right ﬁnger and those below it have not. Perhaps more importantly for, ±1 errors, is whether or not the number currently being pointed has been encountered already. The loop invariant also assumes that the numbers in the list have not changed from their original values.  A Starry Night: How did van Gogh come up with his famous painting, A Starry Night? There’s no easy answer. In the same way, coming up with loop invariants and algorithms is an art form.  Use This Process: Don’t come up with the loop invariant after the fact. Use it to design your algorithm.  5  Main Steps: The pseudocode codeloop must be deﬁned so that it can be taken not just from where you think the computation might be, but from any state of the data structure for which the loop invariant is true and the exit condition has not yet been met.  Worry about one step at a time. Don’t get pulled into the strong desire to under- stand the entire computation at once. Generally, this only brings fear and unhap- piness. I repeat the wisdom taught by both the Buddhists and the twelve-step pro- grams: Today you may feel like like you were dropped off in a strange city without knowing how you got there. Do not worry about the past or the future. Be reassured that you are somewhere along the correct road. Your goal is only to take one step so that you make progress and stay on the road. Another analogy is to imagine you are part of a relay race. A teammate hands you the baton. Your job is only to carry it once around the track and hand it to the next teammate.  6  Make Progress: You must prove that progress of at least one unit of your mea- sure is made every time the algorithm goes around the loop. Sometimes there are odd situations in which the algorithm can iterate without making any measurable progress. This is not acceptable. The danger is that the algorithm will loop forever. You must either deﬁne another measure that better shows how you are making progress during such iterations or change the step taken in the main loop so that progress is made. The formal proof of this is similar to that for maintaining the loop invariant.  7  Maintain the Loop Invariant: You must prove that the loop invariant is main- tained in each iteration.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  17  Measures of Progress and Loop Invariants  The Formal Statement: Whether or not you want to prove it formally, the formal statement that must be true is   cid:2 loop-invariant   cid:1  cid:3  & not cid:2 exit-cond  cid:3  & codeloop ⇒  cid:2 loop-invariant   cid:1  cid:1  cid:3   Proof Technique:  cid:1  Assume that the computation is at the top of the loop.  cid:1  Assume that the loop invariant is satisﬁed; otherwise the program would have already failed. Refer back to the picture that you drew to see what this tells you about the current state of the data structure.   cid:1  You can also assume that the exit condition is not satisﬁed, because otherwise   cid:1  Execute the pseudocode codeloop, in one iteration of the loop. How does this  the loop would exit.  change the data structure?   cid:1  Prove that when you get back to the top of the loop again, the requirements set  by the loop invariant are met once more.  Different Situations: Many subtleties can arise from the huge number of differ- ent input instances and the huge number of different places the computation might ﬁnd itself in.  cid:1  I recommend ﬁrst designing the pseudocode codeloop to work for a general middle iteration when given a large and general input instance. Is the loop in- variant maintained in this case?   cid:1  Then try the ﬁrst and last couple of iterations.  cid:1  Also try special case input instances. Before writing separate code for these, check whether the code you already have happens to handle these cases. If you are forced to change the code, be sure to check that the previously handled cases still are handled.   cid:1  To prove that the loop invariant is true in all situations, pretend that you are at the top of the loop, but you do not know how you got there. You may have dropped in from Mars. Besides knowing that the loop invariant is true and the exit condition is not, you know nothing about the state of the data structure. Make no other assumptions. Then go around the loop and prove that the loop invariant is maintained. Differentiating between Iterations: The assignment x = x + 2 is meaningful as a line of code, but not as a mathematical statement. Deﬁne x to be the value of x at the beginning of the iteration and x that after going around the loop one more time. The effect of the code x = x + 2 is that x  cid:1  + 2.   cid:1  cid:1  = x   cid:1  cid:1    cid:1   8  Establishing the Loop Invariant: You must prove that the initial code estab- lishes the loop invariant.  The Formal Statement: The formal statement that must be true is   cid:2 pre-cond  cid:3  & codepre-loop ⇒  cid:2 loop-invariant  cid:3    P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  Iterative Algorithms and Loop Invariants  Proof Technique:  cid:1  Assume that you are just beginning the computation.  cid:1  You can assume that the input instance satisﬁes the precondition; otherwise  you are not expected to solve the problem.  18   cid:1  Execute the code codepre-loop before the loop.  cid:1  Prove that when you ﬁrst get to the top of the loop, the requirements set by the  loop invariant are met.  Easiest Way: Establish the loop invariant in the easiest way possible. For exam- ple, if you need to construct a set such that all the dragons within it are purple, the easiest way to do it is to construct the empty set. Note that all the dragons in this set are purple, because it contains no dragons that are not purple.  Careful: Sometimes it is difﬁcult to know how to set the variables to make the loop invariant initially true. In such cases, try setting them to ensure that it is true after the ﬁrst iteration. For example, what is the maximum value within an empty list of values? One might think 0 or ∞. However, a better answer is −∞. When adding a new value, one uses the code newMax = max oldMax, newValue . Start- ing with oldMax = −∞, gives the correct answer when the ﬁrst value is added.  9  Exit Condition: Generally you exit the loop when you have completed the task.  Stuck: Sometimes, however, though your intuition is that your algorithm de- signed so far is making progress each iteration, you have no clue whether, head- ing in this direction, the algorithm will ever solve the problem or how you would know it if it happens. Because the algorithm cannot make progress forever, there must be situations in which your algorithm gets stuck. For such situations, you must either think of other ways for your algorithm to make progress or have it exit. A good ﬁrst step is to exit. In step 10, you will have to prove that when your algorithm exits, you actually are able to solve the problem. If you are unable to do this, then you will have to go back and redesign your algorithm.  Loop While vs Exit When: The following are equivalent: while  A and B    loop cid:2 loop-invariant  cid:3   . . .  end while  exit when  not A or not B  . . .  end loop  The second is more useful here because it focuses on the conditions needed to exit the loop, while the ﬁrst focuses on the conditions needed to continue. An- other advantage of the second is that it also allows you to slip in the loop invariant between the top of the loop and the exit condition.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  Measures of Progress and Loop Invariants  10  Ending: In this step, you must ensure that once the loop has exited you will be able to solve the problem.  The Formal Statement: The formal statement that must be true is   cid:2 loop-invariant  cid:3  &  cid:2 exit-cond  cid:3  & codepost-loop ⇒  cid:2 post-cond  cid:3   19  Proof Technique:  cid:1  Assume that you have just broken out of the loop.  cid:1  You can assume that the loop invariant is true, because you have maintained  that it is always true.  exited.   cid:1  You can also assume that the exit condition is true by the fact that the loop has   cid:1  Execute the code codepost-loop after the loop to give a few last touches towards  solving the problem and to return the result.   cid:1  From these facts alone, you must be able to deduce that the problem has been  solved correctly, namely, that the postcondition has been established.  11  Termination and Running Time: You must prove that the algorithm does not loop forever. This is done by proving that if the measure of progress meets some stated amount, then the exit condition has deﬁnitely been met.  If it exits earlier than this, all the better.  The number of iterations needed is then bounded by this stated amount of progress divided by the amount of progress made each iteration. The run- ning time is estimated by adding up the time required for each of these iterations. For some applications, space bounds  i.e., the amount of memory used  may also be im- portant. We discuss important concepts related to running time in Chapters 23–26: time and space complexity, the useful ideas of logarithms and exponentials, BigOh  O  and Theta   cid:1   notation and several handy approximations.  12  Special Cases: When designing an algorithm, you do not want to worry about every possible type of input instance at the same time. Instead, ﬁrst get the algorithm to work for one general type, then another and another. Though the next type of in- put instances may require separate code, start by tracing out what the algorithm that you have already designed would do given such an input. Often this algorithm will just happen to handle a lot of these cases automatically without requiring separate code. When adding code to handle a special case, be sure to check that the previously handled cases still are handled.  13  Coding and Implementation Details: Even after the basic algorithm is out- lined, there can be many little details to consider. Many of these implementation details can be hidden in abstract data types  see Chapter 3 . If a detail does not really make a difference to an algorithm, it is best to keep all possibilities open, giving extra ﬂexibility to the implementer. For many details, it does not matter which choice you make, but bugs can be introduced if you are not consistent and clear as to what you   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  20  Iterative Algorithms and Loop Invariants  have chosen. This text does not focus on coding details. This does not mean that they are not important.  14  Formal Proof: Steps 1–11 are enough to ensure that your iterative algorithm works, that is, that it gives the correct answer on all speciﬁed inputs. Consider some instance which meets the preconditions. By step 8 we establish the loop invariant the ﬁrst time the computation is at the top of the loop, and by step 7 we maintain it each iteration. Hence by way of induction, we know that the loop invariant is true every time the computation is at the top of the loop.  See the following discussion.  Hence, by step 5, the step taken in the main loop is always deﬁned and executes without crashing until the loop exits. Moreover, by step 6 each such iteration makes progress of at least one. Hence, by step 11, the exit condition is eventually met. Step 10 then gives that the postcondition is achieved, so that the algorithm works in this instance. Mathematical Induction: Induction is an extremely important mathematical technique for proving universal statements and is the cornerstone of iterative algorithms. Hence, we will consider it in more detail.  Induction Hypothesis: For each n ≥ 0, let S n  be the statement “If the loop has not yet exited, then the loop invariant is true when you are at the top of the loop after going around n times.” Goal: The goal is to prove that ∀n ≥ 0, S n , namely, “As long as the loop has not yet exited, the loop invariant is always true when you are at the top of the loop.”  Proof Outline: Proof by induction on n.  Base Case: Proving S 0  involves proving that the loop invariant is true when the algorithm ﬁrst gets to the top of the loop. This is achieved by proving the statement  cid:2 pre-cond  cid:3  & codepre-loop ⇒  cid:2 loop-invariant  cid:3 . Induction Step: Proving S n−1  ⇒ S n  involves proving that the loop invariant is maintained. This is achieved by proving the statement  cid:2 loop-invariant Conclusion: By way of induction, we can conclude that ∀n ≥ 0, S n , i.e., that the loop invariant is always true when at the top of the loop.   cid:1  cid:3  & not  cid:2 exit-cond  cid:3  & codeloop ⇒  cid:2 loop-invariant   cid:1  cid:1  cid:3 .  The Process of Induction:  S 0  is true S 0  ⇒ S 1    by base case   by induction step, n = 1   hence, S 1  is true  S 1  ⇒ S 2    by induction step, n = 2   hence, S 2  is true  S 2  ⇒ S 3    by induction step, n = 3   hence, S 3  is true . . .   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  Measures of Progress and Loop Invariants  Other Proof Techniques: Other formal steps for proving correctness are de- scribed in Chapter 28.  Faith in the Method: Convince yourself that these steps are sufﬁcient to deﬁne an algorithm so that you do not have reconvince yourself every time you need to design an algorithm.  21  1.4  Different Types of Iterative Algorithms  To help you design a measure of progress and a loop invariant for your algorithm, here are a few classic types, followed by examples of each type.  More of the Output: If the solution is a structure composed of many pieces  e.g., an array of integers, a set, or a path , a natural thing to try is to construct the solution one piece at a time.  Measure of Progress: The amount of the output constructed.  Loop Invariant: The output constructed so far is correct.  More of the Input: Suppose the input consists of n objects  e.g., an array of n inte- gers or a graph with n nodes . It would be reasonable for the algorithm to read them in one at a time.  Measure of Progress: The amount of the input considered.  Loop Invariant: Pretending that this preﬁx of the input is the entire input, I have a complete solution.  Examples: After i iterations of the preceding ﬁnd-max two-ﬁnger algorithm, the left ﬁnger points at the highest score within the preﬁx of the list seen so far. After i iterations of one version of insertion sort, the ﬁrst i elements of the input are sorted. See Figure 1.2.  Input  7  14  2  5  32  23  8  12 3  16  Selection Sort  Insertion Sort  2  3  5  7  8  14  32  23  12  16  2  5  7  14  32  23  8  12  3  16  <  2  3  5  7  8  12  14  16  23  3 2  Output  Figure 1.2: The loop invariants for insertion sort and selection sort are demonstrated.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  Iterative Algorithms and Loop Invariants  Bad Loop Invariant: A common mistake is to give the loop invariant “I have han- dled and have a solution for each of the ﬁrst i objects in the input.” This is wrong because each object in the input does not need a separate solution; the input as a whole does. For example, in the ﬁnd-max two-ﬁnger algorithm, one cannot know whether one element is the maximum by considering it in isolation from the other elements. An element is only the maximum in comparison with the other elements in the sublist.  22  Narrowing the Search Space: If you are searching for something, try narrowing the search space, maybe decreasing it by one or, even better, cutting it in half.  Measure of Progress: The size of the space in which you have narrowed the search.  Loop Invariant: If the thing being searched for is anywhere, then then it is in this narrowed sublist.  Example: Binary search.  Work Done: The measure of progress might also be some other more creative func- tion of the work done so far.  Example: Bubble sort measures its progress by how many pairs of elements are out of order.  Case Analysis: Try the obvious thing. For which input instances does it work, and for which does it not work? Now you only need to ﬁnd an algorithm that works for those later cases. An measure of progress might include which cases you have tried.  We will now give a simple examples of each of these. Though you likely know these al- gorithms already, use them to understand these different types of iterative algorithms and to review the required steps.  EXAMPLE 1.4.1 More of the Output—Selection Sort  1  Speciﬁcations: The goal is to rearrange a list of n values in nondecreasing order.  2  Basic Steps: We will repeatedly select the smallest unselected element.  3  Measure of Progress: The measure of progress is the number k of elements se- lected.  4  The Loop Invariant: The loop invariant states that the selected elements are the k smallest of the elements and that these have been sorted. The larger elements are in a set on the side.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  Measures of Progress and Loop Invariants  5  Main Steps: The main step is to ﬁnd the smallest element from among those in the remaining set of larger elements and to add this newly selected element to the end of the sorted list of elements.   cid:1  cid:3  & not  6  Make Progress: Progress is made because k increases.  cid:2 exit− 7  Maintain Loop Invariant: We must prove that  cid:2 loop-invariant cond cid:3  & codeloop ⇒  cid:2 loop-invariant  cid:1  cid:1  cid:3 . By the previous loop invariant, the newly selected element is at least the size of the previously selected elements. By the step, it is no bigger than the elements on the side. It follows that it must be the k + 1st element in the list. Hence, moving this element from the set on the side to the end of the sorted list ensures that the selected elements in the new list are the k + 1 smallest and are sorted. 8  Establishing the Loop Invariant: We must prove that  cid:2 pre-cond  cid:3  & codepre-loop ⇒  cid:2 loop-invariant  cid:3 . Initially, k = 0 are sorted and all the elements are set aside. 9  Exit Condition: Stop when k = n. 10  Ending: We must prove  cid:2 loop-invariant  cid:3  &  cid:2 exit-cond  cid:3  & codepost-loop ⇒  cid:2 post-cond  cid:3 . By the exit condition, all the elements have been selected, and by the loop invariant these selected elements have been sorted.  11  Termination and Running Time: We have not considered how long it takes to ﬁnd the next smallest element or to handle the data structures.  23  EXAMPLE 1.4.2 More of the Input—Insertion Sort  1  Speciﬁcations: Again the goal is to rearrange a list of n values in nondecreasing order.  2  Basic Steps: This time we will repeatedly insert some element where it belongs.  3  Measure of Progress: The measure of progress is the number k of elements in- serted.  4  The Loop Invariant: The loop invariant states that the k inserted elements are sorted within a list and that, as before, the remaining elements are off to the side some- where.  5  Main Steps: The main step is to take any of the elements that are off to the side and insert it into the sorted list where it belongs.   cid:2 exit-cond  cid:3  & codeloop ⇒  cid:1  cid:1  cid:3 . You know that the loop invariant has been maintained because the  6  Make Progress: Progress is made because k increases. 7  Maintain Loop Invariant:  cid:2 loop-invariant  cid:1  cid:3  & not  cid:2 loop-invariant new element is inserted in the correct place in the previously sorted list. 8  Establishing the Loop Invariant: Initially, with k = 1, think of the ﬁrst element in the array as a sorted list of length one.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  Iterative Algorithms and Loop Invariants  9  Exit Condition: Stop when k = n. 10  Ending:  cid:2 loop-invariant  cid:3  &  cid:2 exit-cond  cid:3  & codepost-loop ⇒  cid:2 post-cond  cid:3 . By the exit condition, all the elements have been inserted, and by the loop invariant, these in- serted elements have been sorted.  24  11  Termination and Running Time: We have not considered how long it takes to in- sert the element or to handle the data structures.  Example 1.4.3  Narrowing the Search Space—Binary Search  1  Speciﬁcations: An input instance consists of a sorted list A[1..n] of elements and a key to be searched for. Elements may be repeated. If the key is in the list, then the output consists of an index i such that A[i] = key. If the key is not in the list, then the output reports this.  2  Basic Steps: Continue to cut the search space in which the key might be in half.  4  The Loop Invariant: The algorithm maintains a sublist A[i..j ] such that if the key is contained in the original list A[1..n], then it is contained in this narrowed sublist.  If the element is repeated, then it might also be outside this sublist.   3  Measure of Progress: The measure of progress is the number of elements in our sublist, namely j − i + 1. 5  Main Steps: Each iteration compares the key with the element at the center of the sublist. This determines which half of the sublist the key is not in and hence which half to keep. More formally, let mid index the element in the middle of our current sublist A[i..j ]. If key ≤ A[mid], then the sublist is narrowed to A[i..mid]. Otherwise, it is narrowed to A[mid + 1..j ]. 6  Make Progress: The size of the sublist decreases by a factor of two.  cid:2 exit-cond  cid:3  & codeloop ⇒ 7  Maintain Loop Invariant:  cid:2 loop-invariant  cid:1  cid:1  cid:3 . The previous loop invariant gives that the search has been nar-  cid:2 loop-invariant rowed down to the sublist A[i..j ]. If key > A[mid], then because the list is sorted, we know that key is not in A[1..mid] and hence these elements can be thrown away, narrowing the search to A[mid + 1..j ]. Similarly if key < A[mid]. If key = A[mid], then we could report that the key has been found. However, the loop invariant is also maintained by narrowing the search down to A[i..mid]. 8  Establishing the Loop Invariant:  cid:2 pre-cond  cid:3  & codepre-loop ⇒  cid:2 loop−invariant cid:3 . Initially, you obtain the loop invariant by considering the entire list as the sublist. It trivially follows that if the key is in the entire list, then it is also in this sublist.   cid:1  cid:3  & not  9  Exit Condition: We exit when the sublist contains one  or zero  elements. 10  Ending:  cid:2 loop-invariant  cid:3  &  cid:2 exit-cond  cid:3  & codepost-loop ⇒  cid:2 post-cond  cid:3 . By the exit condition, our sublist contains at most one element, and by the loop invariant, if the   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  Measures of Progress and Loop Invariants  key is contained in the original list, then the key is contained in this sublist, i.e., must be this one element. Hence, the ﬁnal code tests to see if this one element is the key. If it is, then its index is returned. If it is not, then the algorithm reports that the key is not in the list.  11  Termination and Running Time: The sizes of the sublists are approximately n, n 16 , . . . , 8, 4, 2, 1. Hence, only  cid:1  log n  splits are needed. Each split takes O 1  time. Hence, the total time is  cid:1  log n .  8 , n  4 , n  2 , n  25  12  Special Cases: A special case to consider is when the key is not contained in the original list A[1..n]. Note that the loop invariant carefully takes this case into account. The algorithm will narrow the sublist down to one  or zero  elements. The counter pos- itive of the loop invariant then gives that if the key is not contained in this narrowed sublist, then the key is not contained in the original list A[1..n]. 13  Coding and Implementation Details: In addition to testing whether key ≤ A[mid], each iteration could test to see if A[mid] is the key. Though ﬁnding the key in this way would allow you to stop early, extensive testing shows that this extra comparison slows down the computation.  EXAMPLE 1.4.4 Work Done—Bubble Sort  1  Speciﬁcations: The goal is to rearrange a list of n values in nondecreasing order.  2  Basic Steps: Swap elements that are out of order.  3  Measure of Progress: An involution is a pair of elements that are out of order, i.e., a pair i, j where 1 ≤ i   A[j ]. Our measure of progress will be the number of involutions in our current ordering of the elements. For example, in [1, 2, 5, 4, 3, 6], there are three involutions.  4  The Loop Invariant: The loop invariant is relatively weak, stating only that we have a permutation of the original input elements.  5  Main Steps: The main step is to ﬁnd two adjacent elements that are out of order and to swap them.   cid:2 exit-cond  cid:3  & codeloop ⇒  cid:1  cid:1  cid:3 . By the previous loop invariant we had a permutation of the  6  Make Progress: Such a step decreases the number of involutions by one. 7  Maintain Loop Invariant:  cid:2 loop-invariant  cid:2 loop-invariant elements. Swapping a pair of elements does not change this. 8  Establishing the Loop Invariant:  cid:2 pre-cond  cid:3  & codepre-loop ⇒  cid:2 loop−invariant  cid:3 . Initially, we have a permutation of the elements.   cid:1  cid:3  & not  9  Exit Condition: Stop when we have a sorted list of elements.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  Iterative Algorithms and Loop Invariants  10  Ending:  cid:2 loop-invariant  cid:3  &  cid:2 exit-cond  cid:3  & codepost-loop ⇒  cid:2 post-cond  cid:3 . By the loop invariant, we have a permutation of the original elements, and by the exit condition these are sorted.  11  Termination and Running Time: Initially, the measure of progress cannot be higher than n n − 1  2 because this is the number of pairs of elements there are. In each iteration, this measure decreases by one. Hence, after at most n n − 1  2 itera- tions, the measure of progress has decreased to zero. At this point the list has been sorted and the exit condition has been met. We have not considered how long it takes to ﬁnd two adjacent elements that are out of order.  26  EXERCISE 1.4.1  See solution in Part Five.  Give the implementation details and the running times for selection sort.  EXERCISE 1.4.2  See solution in Part Five.  Give the implementation details and the running times for insertion sort. Does using binary search to ﬁnd the smallest element or to ﬁnd where to insert help? Does it make a difference whether the elements are stored in an array or in a linked list?  EXERCISE 1.4.3  See solution in Part Five.  Give the implementation details and the running times for bubble sort: Use another loop invariant to prove that the total num- ber of comparisons needed is O n2 .  1.5  Typical Errors  In a study, a group of experienced programmers was asked to code binary search. Easy, yes? 80% got it wrong! My guess is that if they had used loop invariants, they all would have got it correct.  Be Clear: The code speciﬁes the current subinterval A[i..j ] with two integers i and j . Clearly document whether the sublist includes the end points i and j or not. It does not matter which, but you must be consistent. Confusion in details like this is the cause of many bugs.  Math Details: Small math operations like computing the index of the middle ele- ment of the subinterval A i..j   are prone to bugs. Check for yourself that the answer is mid =  cid:9  i+j   cid:10 .  2  6  Make Progress: Be sure that each iteration progress is made in every special case. For example, in binary search, when the current sublist has even length, it is rea- sonable  as done above  to let mid be the element just to the left of center. It is also reasonable to include the middle element in the right half of the sublist. However,   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  Measures of Progress and Loop Invariants together these cause a bug. Given the sublist A[i..j ] = A[3, 4], the middle will be the element indexed with 3, and the right sublist will be still be A[mid..j ] = A[3, 4]. If this sublist is kept, no progress will be made, and the algorithm will loop forever.  7  Maintain Loop Invariant: Be sure that the loop invariant is maintained in ev- ery special case. For example, in binary search, it is reasonable to test whether key < A[mid] or key ≥ A[mid]. It is also reasonable for it to cut the sublist A[i..j ] into A[i..mid] and A[mid + 1..j ]. However, together these cause a bug. When key and A[mid] are equal, the test key < A[mid] will fail, causing the algorithm to think the key is bigger and to keep the right half A[mid + 1..j ]. However, this skips over the key.  27  Simple Loop: Code like “i = 1; while i ≤ n  A[i] = 0; i = i + 1; end while” is surpris- ingly prone to the error of being off by one. The loop invariant “When at the top of the loop, i indexes the next element to handle” helps a lot.  EXERCISE 1.5.1  See solution in Part Five.  You are now the professor. Which of the steps to develop an iterative algorithm did the student fail to do correctly in the follow- ing code? How? How would you ﬁx it?  algorithm Eg I    cid:1   cid:2  pre-cond cid:3 : I is an integer.  cid:2  post-cond cid:3 : Outputs begin  I j=1 j .  s = 0 i = 1 while  i ≤ I    s = s + i i = i + 1  end loop  return s  end algorithm  1.6  Exercises   cid:2 loop-invariant cid:3 : Each iteration adds the next  term giving that s = cid:1   i j=1 j .  EXERCISE 1.6.1 You are in the middle of a lake of radius 1. You can swim at a speed of 1 and can run inﬁnitely fast. There is a smart monster on the shore who can’t go in the water but can run at a speed of 4. Your goal is to swim to shore, arriving at a spot where the monster is not, and then run away. If you swim directly to shore, it will take you 1   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  Iterative Algorithms and Loop Invariants  time unit. In this time, the monster will run the distance  cid:2  < 4 around to where you land and eat you. Your better strategy is to maintain the most obvious loop invariant while increasing the most obvious measure of progress for as long as possible and then swim for it. Describe how this works. EXERCISE 1.6.2 Given an undirected graph G such that each node has at most d + 1 neighbors, color each node with one of d + 1 colors so that for each edge the two nodes have different colors. Hint: Don’t think too hard. Just color the nodes. What loop in- variant do you need?  28   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  2 Examples Using More-of-the-Input  29  Loop Invariants  We are now ready to look at more examples of iterative algorithms. For each example, look for the key steps of the loop invariant paradigm. What is the loop invariant? How is it obtained and maintained? What is the measure of progress? How is the correct ﬁnal answer ensured?  In this chapter, we will encounter some of those algorithms that use the more- of-the-input type of loop invariant. The algorithm reads the n objects making up the input one at a time. After reading the ﬁrst i of them, the algorithm temporarily pre- tends that this preﬁx of the input is in fact the entire input. The loop invariant is “I currently have a solution for the input consisting solely of these ﬁrst i objects  and maybe some additional information .” In Section 2.3, we also encounter some algo- rithms that use the more-of-the-output type of loop invariant.  2.1  Coloring the Plane  See Figure 2.1.  1  Speciﬁcations: An input instance consists of a set of n  inﬁnitely long  lines. These lines form a subdivision of the plane, that is, they partition the plane into a ﬁnite number of regions  some of them unbounded . The output consists of a color- ing of each region with either black or white so that any two regions with a common boundary have different colors. An algorithm for this problem proves the theorem that such a coloring exists for any such subdivision of the plane.  2  Basic Steps: When an instance consists of a set of objects, a common technique is to consider them one at a time, incrementally solving the problem for those objects considered so far.  3  Measure of Progress: The measure of progress is the number of lines, i, that have been considered.  4  The Loop Invariant: We have considered the ﬁrst i lines. C is a proper coloring of the plane subdivided by these lines.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  Iterative Algorithms and Loop Invariants  30  Figure 2.1: An example of coloring the plane.  5  Main Steps: We have a proper coloring C for the ﬁrst i lines. Line i + 1 cuts the plane in half, cutting many regions in half. Each of these halves needs a different color. Then when we change the color of one region, its neighbors must change color too. We will accomplish this by keeping all the colors on one side of line i + 1 the same and ﬂipping those on the other side from white to black and from black to white.  6  Make Progress: Each iteration increases i by one. 7  Maintain Loop Invariant:  cid:2 loop-invariant  cid:1  cid:3 & not cid:2 exit-cond  cid:3 &codeloop ⇒ cid:2 loop-  cid:1  cid:1  cid:3 . We need to check each boundary to make sure that the regions on either invariant side have opposite colors. Boundaries formed by one of the ﬁrst i lines had opposite colors before the change. The colors of the regions on either side were either neither ﬂipped or both ﬂipped. Hence, they still have opposite colors. Boundaries formed by line i + 1 had the same colors before the change. One of these colors was ﬂipped, the other not. Hence, they now have opposite colors. 8  Establishing the Loop Invariant:  cid:2 pre-cond  cid:3  & codepr e−loop⇒ cid:2 loop-invariant  cid:3 . With i = 0 lines, the plane is all one region. The coloring that makes the entire plane white works. 9  Exit Condition: Exit when all n lines have been considered, i.e., i = n. 10  Ending:  cid:2 loop-invariant  cid:3 & cid:2 exit-cond  cid:3 &codepost-loop ⇒  cid:2 post-cond  cid:3  . If C is a proper coloring given the ﬁrst i lines and i = n, then clearly C is a proper coloring given all of the lines.  11  Termination and Running Time: Clearly, only n iterations are needed.  12  Special Cases: There are no special cases we need to consider.  13  Coding and Implementation Details:  algorithm ColoringPlane lines   cid:2  pre-cond cid:3 : lines speciﬁes n  inﬁnitely long  lines.  cid:2  post-cond cid:3 : C is a proper coloring of the plane subdivided by the lines.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  Examples Using More-of-the-Input Loop Invariants  begin  C = “the coloring that colors the entire plane white.” i = 0 loop   cid:2 loop-invariant cid:3 : C is a proper coloring of the plane subdivided by the ﬁrst i lines. exit when  i = n  % Make progress while maintaining the loop invariant Line i + 1 cuts the plane in half. On one half, the new coloring C On the other half, the new coloring C i = i + 1 & C = C  except white is switched to black and black to white.  is the same as the old one C.   cid:1    cid:1    cid:1   is the same as the old one C,  31  end loop return C  end algorithm  2.2  Deterministic Finite Automaton  One large class of problems that can be solved using an iterative algorithm with the help of a loop invariant is the class of regular languages. You may have learned that this is the class of languages that can be decided by a deterministic ﬁnite automata  DFA  or described using a regular expression.  Applications: This class is useful for modeling  cid:1  simple iterative algorithms  cid:1  simple mechanical or electronic devices like elevators and calculators  cid:1  simple processes like the job queue of an operating system  cid:1  simple patterns within strings of characters.  Features: All of these have the following similar features.  Input Stream: They receive a stream of information to which they must react. For example, the stream of input for a simple algorithm consists of the characters read from input; for a calculator, it is the sequence of buttons pushed; for the job queue, it is the stream of jobs arriving; and for the pattern within a string, one scans the string once from left to right.  Read-Once Input: Once a token of the information has arrived, it cannot be re- quested for again.  Bounded Memory: The algorithm, device, process, or pattern matcher has lim- ited memory with which to remember the information that it has seen so far.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  Iterative Algorithms and Loop Invariants  Though the amount of memory can be any ﬁxed amount, this amount cannot grow even if the input instance becomes really big.  EXAMPLE 2.2.1  A Simple DFA  32  1  Speciﬁcations: Given a string α as the input instance, determine whether it is con- tained in the set  language   L = {α ∈ {0, 1}∗  α has length at most three and the number of 1’s is odd}  In most, but not all, DFAs, the computation’s task it to either accept or reject the input.  2  Basic Steps: The characters of the input instance are read one at a time. Because the computation will never be able to read a character again, it must remember what it needs about what it has read so far.  3  Measure of Progress: The measure of progress is the number of characters read so far.  4  The Loop Invariant: Let ω denote the preﬁx of the input instance read so far. The loop invariant states what information is remembered about it. Its length and the num- ber of 1’s read so far cannot be remembered with a bounded amount of memory, be- cause these counts would grow arbitrarily large were the input instance to grow arbi- trarily long. Luckily, the language is only concerned with this length up to three and whether the number of 1’s is even or odd. This can be accomodated with two vari- ables: length, l ∈ {0, 1, 2, 3, more}, and parity, r ∈ {even, odd}. This requires only a ﬁxed amount of memory.  5  Main Steps: Read a character, and update what we know about the preﬁx.  6  Make Progress: Progress increases because the number of characters read so far increases by one.  7  Maintaining the Loop Invariant: After reading another character c, the preﬁx read is now ωc. We know that the length of ωc is one more than that of ω and that the number of 1’s is either one more mod 2 or the same, depending on whether or not the new character c is a 1.  8  Establishing the Loop Invariant: At the beginning of the computation, the preﬁx that has been read so far is the empty string ω =  cid:5 , whose length is l = 0, and the num- ber of 1’s is r = even. 9  Exit Condition: We exit when the entire input instance has been read.  10  Ending: When the input instance has been completely read in, the knowledge that the loop invariant states what we know is sufﬁcient for us to compute the ﬁnal answer. We accept if the instance has length at most three and the number of 1’s is odd.  11  Termination and Running Time: The number of iterations is clearly the length n of the input instance.  12  Special Cases: There are no special cases we need to consider.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  Examples Using More-of-the-Input Loop Invariants  13  Coding and Implementation Details:  algorithm DFA    cid:2  pre-cond cid:3 : The input instance α will be read in one character at a time.  cid:2  post-cond cid:3 : The instance will be accepted if it has length at most three and the  number of 1’s is odd.  begin  l = 0 and r = even loop cid:2 loop-invariant cid:3 : When the iterative program has read in some pre- ﬁx ω of the input instance α, the bounded memory of the machine remembers the length l ∈ {0, 1, 2, 3, more} of this preﬁx and whether the number of 1’s in it is r ∈ {even, odd}.  33  exit when end of input get c  % Reads next character of input if l < 4  then l = l + 1 if c = 1  then r =  r + 1  mod 2  end loop if l < 4 AND r = odd  then  accept  else  end if  reject  end algorithm  Mechanically Compiling an Iterative Program into a DFA: Any iterative pro- gram with bounded memory and an input stream can be mechanically compiled into a DFA that solves the same problem. This provides another model or notation for understanding the algorithm. A DFA is speciﬁed by M =  cid:2  cid:6 , Q, δ, s, F cid:3 .  Alphabet cid:1 —Precondition: The precondition of the problem provides an alpha- bet  cid:6  of characters and speciﬁes that any string of these characters is a valid input instance. This may be {a, b}, {a, b, . . . , z}, ASCII, or any other ﬁnite set of tokens that the program may input. In Example 2.2.1, which we are continuing,  cid:6  = {0, 1}.  Set of States, Q—The Loop Invariant: The loop invariant states what information is remembered about the preﬁx ω read so far. A discrete way of stating this loop invariant is by constructing the set Q of different states that this remember- ing iterative program might be in when at the top of the loop. In this example, these states are Q = {q cid:2 l=0,r=even cid:3 , q cid:2 l=0,r=odd cid:3 , . . . , q cid:2 l=mor e,r=odd cid:3 }, because at each point in time the computation remembers both the length l ∈ {0, 1, 2, 3, more} and parity r ∈ {even, odd} of the preﬁx read.  Recall that a restriction that we are imposing on DFAs is that the amount of memory that they use is ﬁxed and cannot grow even if the input instance   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  34  Iterative Algorithms and Loop Invariants  becomes really big. The consequence of this is that the number Q of states that the DFA might be in is ﬁxed to some ﬁnite number. Each state q ∈ Q of the DFA speciﬁes a value for each of the program’s vari- ables. If the variables are allocated in total r bits of memory, then there are Q = 2r different states that these variable might be in. Conversely, with Q states, a DFA can remember r = log2 Q bits of information. If the algorithm has two variables, one with Q1 different states and one with Q2, then the algorithm can be in Q = Q1 × Q2 different states.  In our example, Q = 5 × 2 = 10. Be sure to assign meaningful names to the states, i.e., not q0, q1, . . . , qQ, as I  have often seen.  Sometimes, when tightening up the algorithm, some of these states can be collapsed into one, if there is no need for the algorithm to differentiate between them. In our example, there are three states that can be collapsed into a dead state from which the ﬁnal answer is known to be reject. Also, the state q cid:2 l=0,r=odd cid:3  should be deleted because it is impossible to be in it.  Graphical Representation: Because the number of states that the computation might be in is ﬁxed to some ﬁnite number, the DFA can be represented graphi- cally by having one node for each state:  leng 0  leng  1  leng  2  leng  3  leng >3  0  0  1 1  0  1 1 0  0  0  1 1  0,1  dead  Even  0,1  Odd   cid:1   Transition Function δ—Maintain Loop Invariant: Suppose that the computation has read the preﬁx ω and is at top of the loop. By the loop invariant, the DFA will be remembering something about this preﬁx ω and a result will be in some state q ∈ Q. After reading another character c, the preﬁx read is now ωc. We maintain the loop invariant by putting the DFA into the state q corresponding to what it is to remember about this new preﬁx ωc. Because the DFA does not know anything about the present preﬁx ω other than the fact that it is in state q, the next state that the DFA will be in can depend only on its present state q and on the next character c read. The DFA’s transition function δ deﬁnes how the machine transitions from state to state. Formally, it is a function δ : Q ×  cid:6  → Q. If the DFA’s current state is q ∈ Q and the next input character is c ∈  cid:6 , then the next state of the DFA is given  cid:1  = δ q, c . Consider some state q ∈ Q and some character c ∈  cid:6 . Set the pro- by q gram’s variables to the values corresponding to state q, assume the character read  cid:1  = δ q, c  of the is c, and execute the code once around the loop. The new state q   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  Examples Using More-of-the-Input Loop Invariants  DFA is deﬁned to be the state corresponding to the values of the program’s vari- ables when the computation has reached the top of the loop again.  In a graph representation of a DFA, for each state q and character c, there is  an edge labeled c from node q to node q   cid:1  = δ q, c .  The Start State s—Establishing the Loop Invariant: The start state s of the DFA M is the state in Q corresponding to the initial values that the program assigns to its variables before reading any input characters. In the graph representation, the corresponding node has an arrow to it.  35  Accept States F —Ending: When the input instance has been completely read in, the DFA might be in any one of the states q ∈ Q. Because the DFA does not know anything about the input instance other than the fact that it is in state q, the result of the computation can only depend on this state. If the task of the DFA is only to either accept or reject the input instance, then the set of states Q, must be partitioned into accept and reject states. If the DFA is in an accept state when the instance ends, then the instance is accepted. Otherwise, it is rejected. When the DFA is speciﬁed by M =  cid:2  cid:6 , Q, δ, s, F cid:3 , F denotes the set of these accept states. In the graph representation these nodes are denoted by double circles.  EXAMPLE 2.2.2  Addition  In the standard elementary school algorithm for addition, the input consists of two in- tegers x and y represented as strings of digits. The output is the sum z, also represented as a string of digits. The input can be viewed as a stream if the algorithm is ﬁrst given the lowest digits of x and of y, then the second lowest, and so on. The algorithm out- puts the characters of z as it proceeds. The only memory required is a single bit to store the carry bit. Because of these features, the algorithm can be modeled as a DFA.  algorithm Adding     cid:2  pre-cond cid:3 : The digits of two integers x and y are read in backwards in parallel.  cid:2  post-cond cid:3 : The digits of their sum will be outputted backwards.  begin  allocate carry ∈ {0, 1} carry = 0 loop  cid:2 loop-invariant cid:3 : If the low-order i digits of x and of y have been read, then the low-order i digits of the sum z = x + y have been out- putted. The bounded memory of the machine remembers the carry.  exit when end of input get  cid:2 xi, yi cid:3   s = xi + yi + carry zi = low order digit of s carry = high order digit of s put zi    P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  Iterative Algorithms and Loop Invariants  end loop if carry = 1  then  put carry   end if  end algorithm  36  The DFA is as follows. Set of States: Q = {q cid:2 carry=0 cid:3 , q cid:2 carry=1 cid:3 }. Alphabet:  cid:6  = { cid:2 xi, yi cid:3   xi, yi ∈ [0..9]}. Start State: s = q cid:2 carry=0 cid:3 . Transition Function: δ q cid:2 carry=c cid:3 ,  cid:2 xi, yi cid:3   =  cid:2 qcarry=c cid:1 , zi cid:3   where c   cid:1   is the high-order digit and zi is the low order digit of xi + yi + c.  EXAMPLE 2.2.3  Division   cid:2   cid:2    cid:1  =  z  =  3959  7  Dividing an integer by seven requires a fairly complex algorithm. Surprisingly, it can be done by a DFA. The input consists of an integer x read in one digit at a time, starting  cid:10  are outputted. In with the high-order digit. Simultaneously, the digits of the output  cid:9  x  cid:10  = 5655. After the preﬁx ω = the end, the remainder is provided. Try computing  cid:9  39591  cid:10  = 56 has been 395 has been read, the loop invariant states that the answer z =  cid:9  395 outputted and its remainder r = 395 mod 7 = 3 ∈ {0, 1, . . . , 6} has been remembered. When the next character xi = 9 is read, we must do the same for ω9 = 3959. The new answer is  7  7  7   cid:3   cid:3    cid:2    cid:4  cid:5  cid:6   395 7   cid:3  =  cid:3    cid:9    cid:7  × 7 + r  cid:2    cid:8  × 10 + 9  cid:3   7 3 × 10 + 9  395 × 10 + 9  cid:2   7 r × 10 + 9  7  7  395 7  × 10 +  = z × 10 +  = 56 × 10 + 5 = 565.  = In general, for r ∈ {0, 1, . . . , 6} and xi ∈ {0, 1, . . . , 9}, the value zi =  cid:9  r×10+xi   cid:10  is a sin- as a string is z concatenated with this gle digit that is easy to compute. This gives that z new digit zi. Given that z has already been outputted, what remains is to output zi. Sim-  cid:1  = 3959 mod 7 =  395 × 10 + 9  mod 7 =   395 mod 7  × ilarly, the new remainder r  cid:1  = 10 +  9 mod 7   mod 7 =   3  × 10 +  2   mod 7 = 32 mod 7 = 4. More generally, r r × 10 + c mod 7. Initially, the preﬁx read so far is the empty string ω representing 0,  cid:10  has been giving z =  cid:9  0 outputted, and what remains is to output its remainder r . The DFA to compute this will have seven states q0, . . . , q6. The transition function is δ qr , c  = q cid:2 r·10+c mod 7 cid:3 .   cid:10  = 0 = empty string and r = 0 mod 7 = 0. In the end, z =  cid:9  x  7  7  7   cid:1   EXAMPLE 2.2.4  Calculator  Invariants can be used to understand a computer system that, instead of simply com- puting one function, continues dynamically to take in inputs and produce outputs. In   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  Examples Using More-of-the-Input Loop Invariants  our simple calculator, the keys are limited to  cid:6  = {0, 1, 2, . . . , 9, +, clr}. You can enter a number. As you do so it appears on the screen. The + key adds the number on the screen to the accumulated sum and displays the sum on the screen. The clr key re- sets both the screen and the accumulator to zero. The machine only can store positive integers from zero to 99999999. Additions are done mod 108. Set of States: Q = {q cid:2 acc,cur,scr cid:3   acc, cur ∈ {0..108 − 1} and scr ∈{showA, showC}}.  There are 108 × 108 × 2 states in this set, so you would not want to draw the  37  diagram. Alphabet:  cid:6  = {0, 1, 2, . . . , 9, +, clr}. Start State: s = q cid:2 0,0,showC cid:3 . Transition Function:  cid:1  For c ∈ {0..9}, δ q cid:2 acc,cur,scr cid:3 , c  = q cid:2 acc,10×cur+c,showC cid:3 .  cid:1  δ q cid:2 acc,cur,scr cid:3 , +  = q cid:2 acc+cur,cur,showA cid:3 .  cid:1  δ q cid:2 acc,cur,scr cid:3 , clr   = q cid:2 0,0,showC cid:3 .  EXAMPLE 2.2.5  Longest Block of Ones  Suppose that the input consists of a sequence A[1..n] of zeros and ones, and we want to ﬁnd a longest contiguous block A[p, q] of ones. For example, on input A[1..n] = [1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0], the block A[5..7] of length 3 is a suitable solution, and so is the block A[10..12]. Here are some things we must consider when designing the loop invariant.  Nonﬁnite Memory: Both the size of the longest block and the indices of its beginning and end are integers in [1..n]. These require O log n  bits to remember. Hence, this algorithm will not be a deterministic ﬁnite automaton.  Remember the Solution for the Preﬁx: After reading the preﬁx A[1..i], it is clear that you need to remember the longest block. Is this enough for a loop invariant? How would you maintain this loop invariant when reading in only the next character A[i + 1]? For example, if A[1..i] = [0, 1, 1, 0, 0, 1, 1], then the loop invariant may give us only the block A[2..3] of length 2. Then if we read A[i + 1] = 1, then the longest block of A[1..i + 1] = [0, 1, 1, 0, 0, 1, 1, 1] becomes A[6..8] of length 3. How would your program know about this block?  Remember the Longest Current Block: You also must keep a pointer to the beginning of the current block being worked on, i.e., the longest one ending in the value A[i], and its size. With this the algorithm can know whether the current increasing contiguous subsequence gets to be longer than the previous one. This needs to be included in the loop invariant.  Maintaining the Loop Invariant: If you have this information about A[1..i], then you can learn it about A[1..i + 1] as follows. If A[i + 1] = 1, then the longest block of ones ending in the current value increases in length by one. Otherwise, it shrinks to being   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  Iterative Algorithms and Loop Invariants  the empty string. If this block increases to be longer than our previous longest, then it replaces the previous longest. In the end, we know the longest block of ones.  Empty Blocks: A[3..3] is a block of length 1, and A[4..3] is a block of length zero end- ing in A[3]. This is why initially, with i = 0, the blocks are set to A[1..0], and when the current block ending in A[i + 1] becomes empty, it is set to A[i + 2..i + 1].  38  Dynamic Programming: Dynamic programming, covered in Chapter 18, is a very powerful technique for solving optimization problems. Many of these amount to reading the elements of the input instance A[1..n] one at a time and, when at A[i], saving the optimal solution for the preﬁx A[1..i] and its cost. This amounts to a de- terministic nonﬁnite automaton. The maximum-block-of-ones problem is a trivial example of this. The solutions to the following two problems and more problems can be found in Chapter 19.2.  Longest Increasing Contiguous Subsequence: The input consists of a sequence A[1..n] of integers, and we want to ﬁnd the longest contiguous subsequence A[k1..k2] such that the elements are monotonically increasing. For example, the optimal solu- tion for [5, 3, 1, 3, 7, 9, 8] is [1, 3, 7, 9].  Longest Increasing Subsequence: This is a harder problem. Again the input con- sists of a sequence A of integers of size n. However, now we want to ﬁnd the longest  not necessarily contiguous  subsequence S ⊆ [1..n] such that the elements, in the order that they appear in A, are monotonically increasing. For example, an optimal solution for [5, 1, 5, 7, 2, 4, 9, 8] is [1, 5, 7, 9], and so is [1, 2, 4, 8].  EXERCISE 2.2.1  See solution in Part Five.  Give the code for these examples: 1. Divide 2. Calculator 3. Longest block of ones  EXERCISE 2.2.2 For the longest block of ones, what are  cid:6 , Q, δ, s, and F ?  EXERCISE 2.2.3 For each of the following examples, give the code, and either give the DFA or, if necessary, give a deterministic nonﬁnite automaton as done in Exam- ple 2.2.5. L = {0n1n  n ≥ 0} = {α ∈ {0, 1}∗  α has zero or more zeros followed by the same 1. number of ones }. L = {α ∈ {0, 1}∗  every third character of α is a 1}. e.g., 1010011110110 ∈ L,  cid:5  ∈ L, 0 ∈ L, and 100  cid:15 ∈ L. LOR = {α ∈ {0, 1}∗  α has length at most three OR the number of 1’s is odd }. L = {α ∈ {0, 1}∗  α contains the substring 0101}. For example, α = 1110101101 ∈ L, because it contains the substring, namely, α = 111 0101 101.  3. 4.  2.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  Examples Using More-of-the-Input Loop Invariants  2.3 More of the Input vs. More of the Output  Sometimes it is not clear at ﬁrst whether to use more-of-the-input or more-of-the- output loop invariants. This section gives two similar problems, of which the ﬁrst works better for one and in which the second works better for the other.  39  EXAMPLE 2.3.1  Tournament  A tournament is a directed graph  see Section 3.1  formed by taking the complete undi- rected graph and assigning arbitrary directions to the edges, i.e., a graph G =  V, E  such that for each u, v ∈ V , exactly one of  cid:2 u, v cid:3  or  cid:2 v, u cid:3  is in E. A Hamiltonian path is a path through a graph that can start and ﬁnish anywhere but must visit every node exactly once each. Design an algorithm that, given any tournament, ﬁnds a Hamil- tonian path through it. Because it ﬁnds a Hamiltonian path for any tournament, this algorithm, in itself, acts as proof that every tournament has a Hamiltonian path.  More of the Output: It is natural to want to push forward and ﬁnd the required path through a graph. The measure of progress would be the amount of the path outputted and the loop invariant would say “I have the ﬁrst i nodes  or edges  in the ﬁnal path.” Maintaining this loop invariant would require extending the path constructed so far by one more node. The problem, however, is that the algorithm might get stuck when the path constructed so far has no edges from the last node to a node that has not yet been visited. This makes the loop invariant as stated false.  Recursive Backtracking: One is then tempted to have the algorithm backtrack when it gets stuck, trying a different direction for the path to go. This results in a ﬁne algo- rithm. See the recursive backtracking algorithms in Chapter 17. However, unless one is really careful, such algorithms tend to require exponential time.  More of the Input: Instead, try solving this problem using a more-of-the-input loop invariant. Assume the nodes are numbered 1 to n in an arbitrary way. The algorithm temporarily pretends that the subgraph on the ﬁrst i of the nodes is the entire input instance. The loop invariant is “I currently have a solution for this subinstance.” Such a solution is a Hamiltonian path u1, . . . , ui that visits each of the ﬁrst i nodes exactly once and that itself is simply a permutation the ﬁrst i nodes. Maintaining this loop invariant requires constructing a path for the ﬁrst i + 1 nodes. There is no requirement that this new path resemble the previous path. For this problem, however, it can be accomplished by ﬁnding a place to insert the i + 1st node within the permutation of the ﬁrst i nodes. In this way, the algorithm looks a lot like insertion sort.  Case Analysis: When developing an algorithm, a good technique is to see for which input instances the obvious thing works and then try to design another algorithm for the remaining cases:   a   u1  u  2 u  3  ui   b   1u  u  u2  3  iu   c   u1  2u  u  j  u  j+1  u  i  vi+1  v  i+1  vi+1   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  Iterative Algorithms and Loop Invariants   a  If  cid:2 vi+1, u1 cid:3  is an edge, then the extended path is easily vi+1, u1, . . . , ui.  b  Similarly, if  cid:2 ui, vi+1 cid:3  is an edge, then the extended path is easily u1, . . . , ui, vi+1.  c  Otherwise, because the graph is a tournament, both  cid:2 u1, vi+1 cid:3  and  cid:2 vi+1, ui cid:3  are edges. Color each node uj red if  cid:2 uj , vi+1 cid:3  is an edge, and blue if  cid:2 vi+1, uj cid:3  is. Because u1 is red and ui is blue, there must be some place uj to uj+i in the path where it changes color from red to blue. Because both  cid:2 uj , vi+1 cid:3  and  cid:2 vi+1, uj+i cid:3  are edges, we can form the extended path u1, . . . , uj , vi+1, uj+i, . . . , ui.  40  EXAMPLE 2.3.2  Euler Cycle  An Eulerian cycle in an undirected graph is a cycle that passes through each edge ex- actly once. A graph contains an Eulerian cycle iff it is connected and the degree of each vertex is even. Given such a graph, ﬁnd such a cycle.  More of the Output: We will again start by attempting to solve the problem using the more-of-the-output technique, namely, start at any node and build the output path one edge at a time. Not having any real insight into which edge should be taken next, we will choose them in a blind or greedy way  see Chapter 16 . The loop invariant is that after i steps you have some path through i different edges from some node s to some node v.  Getting Stuck: The next step in designing this algorithm is to determine when, if ever, this simple blind algorithm gets stuck, and either to ﬁgure out how to avoid this situation or to ﬁx it. Making Progress: If s  cid:15 = v, then the end node v must be adjacent to an odd number of edges that are in the path. See Figure 2.2.a. This is because there is the last edge in the path, and for every edge in the path coming into the node there is one leaving. Hence, because v has even degree, it follows that v is adjacent to at least one edge that is not in the path. Follow this edge, extending the path by one edge. This maintains the loop invariant while making progress. This process can get stuck only when the path happens to cycle back to the starting node, giving s = v. In such a case, join the path here to form a cycle.  Ending: If the cycle created covers all of the edges, then we are done.  Getting Unstuck: If the cycle we have created from our chosen node s back to s does not cover all the edges, then we look for a node u within this cycle that is adjacent to an edge not in the cycle. See Figure 2.2.b. Change s to be this new node u. We break the cycle at u, giving us a path from u back to u. The difference with this path is that we can extend it past u along the unvisited edge. Again the loop invariant has been maintained while making progress.   a  s  v   b  s  v  u = new s   Figure 2.2: Path constructed thus far by the Euler algorithm within the undirected graph.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  Examples Using More-of-the-Input Loop Invariants  u Exists: The only thing remaining to prove is that when v comes around to meet s again and we are not done, then there is in fact a node u in the path that is adjacent to an edge not in the path. Because we are not done, there is an edge e in the graph that is not in our path. Because the graph is connected, there must be a path in the graph from e to our constructed path. The node u at which this connecting path meets our constructed path must be as required, because the last edge {u, w} in the connecting path is not in our constructed path.  Extended Loop Invariant: To avoid having to ﬁnd such a node u when it is needed, we extend the loop invariant to state that in addition to the path, the algorithm remem- bers some node u other than s and v that is in the path and is adjacent to an edge not in the path.  41  EXERCISE 2.3.1  See solution in Part Five.  Iterative cake cutting: The famous algo- rithm for fairly cutting a cake in two is for one person to cut the cake in the place that he believes is half and for the other person to choose which “half” he likes. One player may value the icing and while the other the cake more, but it does not matter. The sec- ond player is guaranteed to get a piece that he considers to be worth at least a half, because he chooses between two pieces whose sum worth for him is at least one. Be- cause the ﬁrst person cut it in half according to his own criteria, he is happy which ever piece is left for him. Our goal is write an iterative algorithm that solves this same problem for n players.  To make our life easier, we view a cake not as three-dimensional thing, but as the line from zero to one. Different players value different subintervals of the cake differ- ently. To express this, each player assigns some numerical value to each subinterval. For example, if player pi’s name is written on the subinterval [ i−1 i 2n ] of cake, then he 2n , might allocate a higher value to it, say 1 2 . The only requirement is that the total value of the cake is one. Your algorithm is only allowed the following two operations. In an evaluation query, v = Eval p, [a, b] , the algorithm asks a player p how much  v  he values a particular subinterval [a, b] of the whole cake [0, 1]. In a cut query, b = Cut p, a, v , the protocol asks the player p to identify the shortest subinterval [a, b], starting at a given left endpoint a, with a given value v. In the above example, Eval pi, [ i−1 i 2n ]  2n , returns 1 2n . Using these, the two-player algorithm is as follows:  2 and Cut pi, i−1  2   returns i  2n , 1  [a, b] ⊆ [0, 1] is a subinterval of the whole cake.  algorithm Partition2 {p1, p2}, [a, b]   cid:2  pre-cond cid:3 : p1 and p2 are players.  cid:2  post-cond cid:3 : Returns a partitioning of [a, b] into two disjoint pieces [a1, b1] and [a2, b2] so that player pi values [ai, bi] at least half as much as he values [a, b]. v1 = Eval p1, [a, b]  c = Cut p1, a, v1 2    begin   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:0  Iterative Algorithms and Loop Invariants  if  Eval p2, [a, c]  ≤ Eval p2, [c, b]   then [a1, b1] = [a, c] and [a2, b2] = [c, b] [a1, b1] = [c, b] and [a2, b2] = [a, c]  else  42  end if return [a1, b1] and [a2, b2]   end algorithm  The problem that you must solve is the following: algorithm Partition n, P   cid:2  pre-cond cid:3 : P is a set of n players.  cid:2  post-cond cid:3 : Returns a partitioning of [0, 1] into n disjoint pieces [ai, bi] so that  Each player in P values the whole cake [0, 1] by at least 1. for each i ∈ P, the player pi values [ai, bi] by at least 1 n .  begin . . . end algorithm  1. Can you cut off n pieces of cake, each of size strictly bigger than 1  n , and have cake left over? Is it sometimes possible to have allocated a disjoint piece to each player, each worth much more than 1 n , to the receiving player, and for there to still be cake left? Explain.  2. As a big hint to designing an iterative algorithm, I will tell you what the ﬁrst itera- tion accomplishes.  Later iterations may do slightly modiﬁed things.  Each player speciﬁes where he would cut if he were to cut off the ﬁrst fraction 1 n of the [a, b] cake. The player who wants the smaller amount of this ﬁrst part of the cake is given this piece of the cake. The code for this is as follows:  loop i ∈ P  ci = Cut pi, 0, 1 n    end loop imin = the i ∈ P that minimizes ci  [aimin , bimin ] = [0, cimin ]  As your ﬁrst step in designing the algorithm, what is your loop invariant? It should include:  a  how the cake has been cut so far  b  who has been given cake, and how he feels about it  c  how the remaining players feel about the remaining cake.  3. Give the iterative pseudocode. 4. Formally prove that the loop invariant is established. 5. Formally prove that the loop invariant is maintained. 6. Formally prove that the postcondition is established. 7. What is the running time of this algorithm? 8.  Is this a more-of-the-input or a more-of-the-output loop invariant?   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  17:16  3 Abstract Data Types  43  Abstract data types  ADTs  provide both a language for talking about and tools for operating on complex data structures. Each is deﬁned by the types of objects that it can store and the operations that can be performed. Unlike a function that takes an input and produces an output, an ADT is more dynamic, periodically receiving infor- mation and commands to which it must react in a way that reﬂects its history. In an object-oriented language, these are implemented with objects, each of which has its own internal variables and operations. A user of an ADT has no access to its internal structure except through the operations provided. This is referred to as information hiding and provides a clean boundary between the user and the ADT. One person can use the ADT to develop other algorithms without being concerned with how it is implemented or worrying about accidentally messing up the data structure. An- other can implement and modify the ADT without knowing how it is used or worry- ing about unexpected effects on the rest of the code. A general purpose ADT—not just the code, but also the understanding and the mathematical theory—can be reused in many applications. Having a limited set of operations guides the implementer to use techniques that are efﬁcient for these operations yet may be slow for the operations excluded. Conversely, using an ADT such as a stack in your algorithm automatically tells someone attempting to understand your algorithm a great deal about the pur- pose of this data structure. Generally, the running time of an operation is not a part of the description of an ADT, but is tied to a particular implementation. However, it is useful for the user to know the relative expense of using operations so that he can make his own choices about which ADTs and which operations to use.  This chapter will treat the following ADTs: lists, stacks, queues, priority queues, graphs, trees, and sets. From the user’s perspective, these consist of a data structure and a set of operations with which to access the data. From the perspective of the data structure itself, it is a ongoing system that continues to receive a stream of com- mands to which it must react dynamically. ADTs have a set of invariants or integrity constraints  both public and hidden  that must be true every time the system is en- tered or left. Imagining a big loop around the system allows us to regard them as a kind of loop invariant.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  17:16  Iterative Algorithms and Loop Invariants  3.1  Speciﬁcations and Hints at Implementations  The following are examples frequently used ADTs.  44  Simple Types: Integers, ﬂoating point numbers, strings, arrays, and records are ab- stract data types provided by all programming languages.  The List ADT:  Speciﬁcation: A list consists of an ordered sequence of elements. Unlike arrays, they contain no empty positions. Elements can be inserted, deleted, read, modi- ﬁed, and searched for.  Array Implementations: There are different implementations that have tradeoffs in the running time, memory requirements, and difﬁculty of implementing. The obvious implementation of a list is to put the elements in an array. If the elements are packed one after the other, then the ith element can be accessed in  cid:1  1  time, but inserting or deleting an element requires  cid:1  n  time because all the el- ements need to be shifted. Alternatively, blank spaces could be left between the elements. This leaves room to insert or delete elements in  cid:1  1  time, but ﬁnding the ith element might now take  cid:1  n  time.  Linked List Implementations: A problem with the array implementation is that the array needs to be allocated some ﬁxed size of memory when initialized. An alternative implementation, which can be expanded or shrunk in size as needed, uses a linked list. This implementation has the disadvantage of requiring  cid:1  n  time to access a particular element. See Section 3.2.  Tree Implementations: A nice balance between the advantages of array and the linked list implementations is data structure called a heap. Heaps can do every operation in  cid:1  log n  time. See Section 10.4. Adelson-Velsky–Landis  AVL  trees and red–black trees have similar properties.  The Stack ADT:  Speciﬁcation: A stack ADT is the same as a list ADT, except its operations are limited. It is analogous to a stack of plates. A push is the operation of adding a new element to the top of the stack. A pop is the operation of removing the top element from the stack. The rest of the stack is hidden from view. This order is referred to as last in, ﬁrst out  LIFO .  Use: Stacks are the key data structure for recursion and parsing. Having the op- erations limited means that all operations can implemented easily and be per- formed in constant time.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  Abstract Data Types  April 5, 2008  17:16  Array Implementation: The hidden invariants in an array implementation of a stack are that the elements in the stack are stored in an array starting with the bottom of the stack and that a variable top indexes the entry of the array contain- ing the top element. It is not difﬁcult to implement push and pop so that they maintain these invariants. The stack grows to the right as elements are pushed and shrinks to the left as elements are popped. For the code, see Exercise 3.1.1.  45  top  1  2  3  4  5  6  7  8              Linked List Implementation: As with lists, stacks are often implemented using linked lists. See Section 3.2.  The Queue ADT:  Speciﬁcation: The queue ADT is also the same as a list ADT, except with a differ- ent limited set of operations. A queue is analogous to a line-up for movie tickets. One is able to insert an element at the rear and remove the element that is at the front. This order is ﬁrst in ﬁrst out  FIFO .  Queue Use: An operating system will have a queue of jobs to run and a network hub will have a queue of packets to transmit. Again all operations can be imple- ment easily to run in constant time.  Array Implementation:  Trying Small Steps: If the front element is always stored at index 1 of the array, then when the current front is removed, all the remaining elements would need to shift by one to take its place. To save time, once an element is placed in the array, we do not want to move it until it is removed. The effect is that the rear moves to the right as elements arrive, and the front moves to the right as elements are removed. We use two different variables, front and rear, to index their locations. As the queue migrates to the right, eventually it will reach the end of the array. To avoid getting stuck, we will treat the array as a circle, indexing modulo the size of the array. This allows the queue to migrate around and around as elements arrive and leave.  Hidden Invariants: The elements are stored in order from the entry indexed by front to that indexed by rear possibly wrapping around the end of the array.  rear  front  7  8              1  2  3  4  5  6   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  17:16  Iterative Algorithms and Loop Invariants  Extremes: It turns out that the cases of a completely empty and a com- pletely full queue are indistinguishable, because with both front will be one to the left of rear. The easiest solution is not to let the queue get completely full.  46  Code: See Exercises 3.1.2 and 3.1.3.  Linked List Implementation: Again see Section 3.2.  The Priority Queue ADT:  Speciﬁcation: A priority queue is still analogous to a line-up for movie tickets. However, in these queues the more important elements are allowed to move to the front of the line. When inserting an element, its priority must be speciﬁed. This priority can later be changed. When removing, the element with the highest priority in the queue is removed and returned. Ties are broken arbitrarily.  Tree Implementations: Heaps, AVL trees, and red–black trees can do each oper- ation in  cid:1  log n  time. See Sections 4.1, 10.2, and 10.4.  The Set ADT:  Speciﬁcation: A set is basically a bag within which you can put any elements that you like. It is the same as a list, except that the elements cannot be repeated or ordered.  Indicator Vector Implementation: If the universe of possible elements is sufﬁ- ciently small, then a good data structure is to have a Boolean array indexed with each of these possible elements. An entry being true will indicate that the corre- sponding element is in the set. All set operations can be done in constant time, i.e., in a time independent of the number of items in the set.  Hash Table Implementation: Surprisingly, even if the universe of possible ele- ments is inﬁnite, a similar trick can be done, using a data structure called a hash table. A pseudorandom function H is chosen that maps possible elements of the set to the entries [1, N] in the table. It is a deterministic function in that it is easy to compute and always maps an element to the same entry. It is pseudorandom in that it appears to map each element into a random place. Hopefully, all the elements that are in your set happen to be placed into different entries in the ta- ble. In this case, one can determine whether or not an element is contained in the set, ask for an arbitrary element from the set, determine the number of ele- ments in the set, iterate through all the elements, and add and delete elements— all in constant time, i.e., independently of the number of items in the set. If col- lisions occur, meaning that two of your set elements get mapped to the same entry, then there are a number of possible methods to rehash them somewhere else.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  17:16  Abstract Data Types  The Set System ADT:  Speciﬁcation: A set system allows you to have a set  or list  of sets. Operations allow the creation, union, intersection, complementation, and subtraction of sets. The ﬁnd operator determines which set a given element is contained in.  List-of-Indicator-Vectors or Hash-Table Implementations: One way to imple- ment these is to have a list of elements implemented using an array or a linked list where each of these elements is an implementation of a set. What remains is to implement operations that operate on multiple sets. Generally, these opera- tions take  cid:1  n  time.  Union–Find Set System Implementation: Another quite surprising result is that on disjoint sets, the union and ﬁnd operations can be done on average in a con- stant amount of time for all practical purposes. See the end of this section.  47  The Dictionary ADT: A dictionary associates a meaning with each word. Similarly, a dictionary ADT associates data with each key.  Graphs:  Speciﬁcation: A graph is set of nodes with edges between them. They can rep- resent networks of roads between cities or friendships between people. The key information stored is which pairs of nodes are connected by an edge. Sometimes data, such as weight, cost, or length, can be associated with each edge or with each node. Though a drawing implicitly places each node at some location on the page, a key abstraction of a graph is that the location of a node is not speci- ﬁed. The basic operations are to determine whether an edge is in a graph, to add or delete an edge, and to iterate through the neighbors of a node. There is a huge literature of more complex operations that one might want to do. For example, one might want to determine which nodes have paths between them or to ﬁnd the shortest path between two nodes. See Chapter 14.  u  v  w  u  v w  1  1  1  u v  w  v  u  w  u  v  w  A graph  An adjacency matrix  An adjacency list  Adjacency Matrix Implementation: This consists of an n × n matrix with M u, v  = 1 if  cid:2 u, v cid:3  is an edge. It requires  cid:1  n2  space  corresponding to the num- ber of potential edges  and  cid:1  1  time to access a given edge, but  cid:1  n  time to ﬁnd the edges adjacent to a given node, and  cid:1  n2  to iterate through all the nodes. This is only a problem when the graph is large and sparse.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  17:16  Iterative Algorithms and Loop Invariants  animal  vertebrate  invertebrate  mammal  bird  reptile  48  homosapien  canine  lizard  snake  human  cat  bear  f  *  x  +   b   y  7  dad  gamekeeper gamekeeper  black  panda  polar  cheetah  a   Figure 3.1: Classiﬁcation tree of animals and a tree representing the expression f = x ×  y + 7 .  Adjacency List Implementation: It lists for each node the nodes adjacent to it. It requires  cid:1  E  space  corresponding to the number of actual edges  and can iterate quickly through the edges adjacent to a give node, but requires time pro- portional to the degree of a node to access a speciﬁc edge.  Trees:  Speciﬁcation: Data is often organized into a hierarchy. A person has children, who have children of their own. The boss has people under her, who have people under them. The abstract data type for organizing this data is a tree.  Uses: There is a surprisingly large list of applications for trees. For two examples see Figure 3.1 and Section 10.5.  Pointer Implementation: Trees are generally implemented by having each node point to each of its children:  f  *  x  +  y  7  Orders: Imposing rules on how the nodes can be ordered speeds up certain op- erations.  Binary Search Tree: A binary search tree is a data structure used to store keys along with associated data. The nodes are ordered so that for each node, all the keys in its left subtree are smaller than its key, and all those in the right subtree are larger. Elements can be found in such a tree, using binary search, in O height  instead of O n  time. See Sections 4.1 and 10.2.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  Abstract Data Types  April 5, 2008  17:16  Heaps: A heap requires that the key of each node be bigger than those of both its children. This allows one to ﬁnd the maximum key in O 1  time. All updates can be done in O log n  time. Heaps are useful for a sorting algo- rithm known as heap sort and for the implementation of priority queues. See Section 10.4.  Balanced Trees: If a binary tree is balanced, it takes less time to traverse down it, because it has height at most log2 n. It is too much work to maintain a perfectly balanced tree as nodes are added and deleted. There are, however, a number of data structures that are able to add and delete in O log2 n  time while ensuring that the tree remains almost balanced. Here are two.  AVL Trees: Every node has a balance factor of −1, 0, or 1, deﬁned as the dif- ference between the heights of its left and right subtrees. As nodes are added or deleted, this invariant is maintained using rotations like the following  see Exercise 3.1.5 :  49  Rotate  10  5  5  10  [10,..  [5,10]  ..,5]  [5,10]  [10,..  ..,5]  Red–Black Trees: Every node is either red or black. If a node is red, then both its children are black. Every path from the root to a leaf contains the same number of black nodes. See Exercise 3.1.6.  Balanced Binary Search Tree: By storing the elements in a balanced binary search tree, insertions, deletions, and searches can be done in  cid:1  log n  time.  Union–Find Set System: This data structure maintains a number of disjoint sets of elements.  Operations:  1  Makeset v , which creates a new set containing the speciﬁed el- ement v;  2  Find v , which determines the name of the set containing a speci- ﬁed element  each set is given a distinct but arbitrary name ; and  3  Union u, v , which merges the sets containing the speciﬁed elements u and v.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  17:16  Iterative Algorithms and Loop Invariants  Use: One application of this is in the minimum-spanning-tree algorithm in Section 16.2.3.  50  Running Time: On average, for all practical purposes, each of these operations can be completed in a constant amount of time. More formally, the total time to do m of these operations on n elements is  cid:1  mα n  , where α is the inverse Ackermann’s function. This function is so slow growing that even if n equals the number of atoms in the universe, then α n  ≤ 4. See Section 9.3.  Implementation: The data structure used is a rooted tree for each set, containing a node for each element in the set. The difference is that each node points to its parent instead of to its children. The name of the set is the contents of the root node. Find w  is accomplished by tracing up the tree from w to the root u. Union u, v  is accomplished by having node u point to node v. From then on, Find w  for a node w in u’s tree will trace up and ﬁnd v instead. What makes this fast on average is that whenever a Find operation is done, all nodes that are encountered during the ﬁnd are changed to point directly to the root of the tree, collapsing the tree into a shorter tree.  c  a  e  u  d  w g  b  f  Union u,v   find w   returns v   v  v  v  b  w  g  d  u  a  c  e  f  c  a  e  u  d  w  g  b  f  EXERCISE 3.1.1 Implement the push and pop operations on a stack using an array as described in Section 3.1.  EXERCISE 3.1.2 Implement the insert and remove operations on a queue using an array as described in Section 3.1.  EXERCISE 3.1.3 When working with arrays, as in Section 3.1, what is the difference between “rear =  rear + 1  mod MAX” and “rear =  rear mod MAX  + 1,” and when should each be used?  Figure: The top row shows three famous graphs: the complete graph on four nodes, the cube, and the Peterson graph. The bottom row shows the same three graphs with their nodes laid out differently.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  Abstract Data Types  April 5, 2008  17:16  EXERCISE 3.1.4 For each of the three pairs of graphs, number the nodes in such the way that  cid:2 i, j cid:3  is an edge in one if and only if it is an edge in the other.  EXERCISE 3.1.5  See solution in Part Five.  Prove that the height of an AVL tree with n nodes is  cid:1  log n .  51  EXERCISE 3.1.6 Prove that the height of a red–black tree with n nodes is  cid:1  log n .  3.2  Link List Implementation  As said, a problem with the array implementation of the list ADT is that the array needs to be allocated some ﬁxed size of memory when it is initialized. A solution to this is to implement these operations using a linked list, which can be expanded in size as needed. This implementation is particularly efﬁcient when the operations are restricted to those of a stack or a queue.  List ADT Speciﬁcation: A list consists of an ordered sequence of elements. Unlike arrays, it has no empty positions. Elements can be inserted, deleted, read, modiﬁed, and searched for. There are tradeoffs in the running time. Arrays can access the ith element in  cid:1  1  time, but require  cid:1  n  time to insert an element. A linked list is an alternative implementation in which the memory allocated can grow and shrink dy- namically with the needs of the program. Linked lists allow insertions in  cid:1  1  time, but require  cid:1  n  time to access the ith element. Heaps can do both in  cid:1  log n  time.  first  info  link  info link  info link  info link  Hidden Invariants: In a linked list, each node contains the information for one el- ement and a pointer to the next. The variable ﬁrst points to the ﬁrst node, and last to the last. The last node has its pointer variable contain the value nil. When the list contains no nodes, ﬁrst and last also point to nil.  Notation: A pointer, such as ﬁrst, is a variable that is used to store the address of a block of memory. The information stored in the info ﬁeld of such a block is denoted by ﬁrst.info in Java and ﬁrst− > info in C. We will adopt the ﬁrst nota- tion. Similarly, ﬁrst.link denotes the pointer ﬁeld of the node. Being a pointer itself, ﬁrst.link.info denotes the information stored in the second node of the linked list, and ﬁrst.link.link.info in the third.  Adding a Node to the Front: Given a list ADT and new Info to store in an element, this operation is to insert an element with this information into the front the list.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  17:16  Iterative Algorithms and Loop Invariants  52  first  first  item  temp  first  item  temp  first  item  temp  last  last  last  last  last  first  last  first item  last  first item  last  first item  last  first item  temp  temp  temp  temp  first  item  first  item  killNode  first  item  killNode  first  killNode  last  last  last  last  last  item  last  item  last  item  last  first  first  killNode  first  killNode  first  killNode  last  first  Figure 3.2: Adding and removing a node from the front of a linked list.  General Case: We need the following steps  with pseudocode given to the right  for a large and general linked list. See Figure 3.2.   cid:1  Allocate space for the new node.  cid:1  Store the information for the new element.  cid:1  Point the new node at the rest of the list.  cid:1  Point ﬁrst at the new node.  New temp temp.info = Info temp.link = ﬁrst ﬁrst = temp  Special Case: The main special case is an empty list. Sometimes we are lucky and the code written for the general case also works for such special cases. Inserting a node starting with both ﬁrst and last pointing to nil, everything works except for last. Add the following to the bottom of the code.   cid:1  Point last to the new and only node.  if  last = nil   then last = temp end if  Whenever adding code to handle a special case, be sure to check that the previ- ously handled cases still are handled correctly.  Removing Node from Front: Given a list ADT, this operation is to remove the ele- ment in the front the list and to return the information Info stored within it.  General Case:   cid:1  Point a temporary variable kill Node to  point to the node to be removed.   cid:1  Move ﬁrst to point to the second node.  cid:1  Save the value to be returned.  cid:1  Deallocate the memory for the ﬁrst node.  cid:1  Return the value.  kill Node = ﬁrst ﬁrst = ﬁrst.link Info = kill Node.info free kill Node return item    P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  Abstract Data Types  April 5, 2008  17:16  Special Cases: If the list is already empty, a node cannot be removed. The only other special case occurs when there is one node pointed to by both ﬁrst and last. At the end of the code, ﬁrst points to nil, which is correct for an empty list. However, last still points to the node that has been deleted. This can be solved by adding the following to the bottom of the code:   cid:1  The list becomes empty.  53  if  ﬁrst = nil   then last = nil end if  Note that the value of ﬁrst and last change. If the routine Pop passes these pa- rameters in by value, the routine needs to be written to allow this to happen.  Testing Whether Empty: A routine that returns whether the list is empty returns true if ﬁrst = nil and false otherwise. It does not look like this routine does much, but it serves two purposes. It hides these implementation details from the user, and by calling this routine instead of doing the test directly, the user’s code becomes more readable. See Exercise 3.2.1.  Adding Node to End: See Exercise 3.2.2.  Removing Node from End: It is easy to access the last node and delete it, because last is pointing at it. However, in order to maintain this invariant, last must be pointed at the node that had been the second-to-last node. It takes  cid:1  n  time to walk down the list from the ﬁrst node to ﬁnd this second-to-last node. Luckily, neither stacks nor queues need this operation. For a faster implementation see Exercise 3.2.3.  Walking Down the Linked List: Now suppose that the elements in the linked lists are sorted by the ﬁeld info. When given an info value newElement, our task is to point the pointer next at the ﬁrst element in the list with that value. The pointer prev is to point to the previous element in the list. This needs to be saved, because if it is needed, there is no back pointer to back up to it. If such an element does not exist, then prev and next are to sandwich the location where this element would go. For example, if newElement had either the value 6 or the value 8, the result of the search would be  first  3  4  8  prev  newElement  = 6 or  8  next  last  9   cid:1  Walk down the list  cid:1  maintaining the two pointers  loop  cid:2 loop-inv  cid:3 : prev and next point to consecutive nodes before or at our desired location.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  17:16  Iterative Algorithms and Loop Invariants   cid:1  until the desired location is found   cid:1  pointing prev where next is pointing  cid:1  and pointing next to the next node.  or next .info ≥ newElement  exit when next = nil prev = next next = next.link end loop  54  Running Time: This can require O n  time, where n is the length of the list.  Initialize the Walk: To initially establish the loop invariant, prev and next must sandwich the location before the ﬁrst node. We do this as follows:  first  3  4  8  prev  next  last  9  prev = nil next = ﬁrst   cid:1  Sandwich the location before the ﬁrst  node.  Adding a Node:  Into the Middle: The general case to consider ﬁrst is adding the node into the middle of the list.  first  first  3  3  4  4  8  8  prev  next  last  9  last  9  prev  6  next   cid:1  Allocate space for the new node.  cid:1  Store the information for the new element.  cid:1  Point the previous node to the new node.  cid:1  Point the new node to the next node.  new temp temp.info = item prev.link = temp temp.link = next  At the Beginning: If the new node belongs at the beginning of the list  say value 2 , then prev.link = temp would not work, because prev is not pointing at a node. We will replace this line with the following:   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  Abstract Data Types  April 5, 2008  17:16  first  3  4  8  prev  next  last  9   cid:1  If the new node is to be the ﬁrst node,  cid:1  point ﬁrst at the new node  cid:1  else  cid:1  point the previous node to the new node.  if prev = nil then ﬁrst = temp prev.link = temp  else  end if  55  At the End: Now what if the new node is to be added on the end  e.g. value 12 ? The variable last will no longer point at the last node. Adding the following code to the bottom will solve the problem:  first  3  4  8  last  9   cid:1  If the new node will be the last node,  cid:1  point last at the new node.  prev  next  if prev = last then last = temp  end if  To an Empty List: Another case to consider is when the initial list is empty. In this case, all the variables, ﬁrst, last, prev, and next, will be nil. The new code works in this case as is.  Compete Code for Adding a Node: One needs to put all of these pieces together into one insert routine. See Exercise 3.2.5.  Deleting a Node:  From the Middle: Again the general case to consider ﬁrst is deleting the node from the middle of the list. We must maintain the linked list before destroying the node. Otherwise, we will drop the list.  first  3  8  prev  next  first  3  prev  next  4  4  last  9  last  9   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  17:16  Iterative Algorithms and Loop Invariants   cid:1  Bypass the node being deleted.  cid:1  Deallocate the memory pointed to by  next.  prev.link = next .link free next  From the Beginning or the End: As before, you need to consider all the potential special cases. See Exercise 3.2.6.  56  prev  next   a   first   c   first  3  3  4  4  8  8  last  9  last  9   b   first   d   first  3  3  4  4  8  8  last  9  last  9  prev  next  prev  next  prev  next  EXERCISE 3.2.1 Implement testing whether a linked list is empty.  EXERCISE 3.2.2 Implement adding a node to the end of a linked list.  EXERCISE 3.2.3 Double pointers: Describe how this operation can be done in  cid:1  1  time if there are pointers in each node to both the previous and the next node.  EXERCISE 3.2.4  See solution in Part Five.  In the code for walking down the linked list, what effect, if any, would it have if the order of the exit conditions were switched to “exit when next .info ≥ newElement or next = nil”?  EXERCISE 3.2.5 Implement the complete code insert that, when given an info value newElement, inserts a new element where it belongs into a sorted linked list. This in- volves only putting together the pieces just provided.  EXERCISE 3.2.6 Implement the complete code Delete that, when given an info value newElement, ﬁnds and deletes the ﬁrst element with this value, if it exists. This in- volves also considering the four special cases listed for deleting a node from the begin- ning or the end of a linked list.  3.3 Merging with a Queue  Merging consists of combining two sorted lists, A and B, into one completely sorted list, C. Here A, B, and C are each implemented as queues. The loop invariant main- tained is that the k smallest of the elements are sorted in C.  This is a classic more- of-the-output loop invariant. It is identical to that for selection sort.  The larger ele- ments are still in their original lists A and B. The next smallest element will be either   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  Abstract Data Types  April 5, 2008  17:16  ﬁrst element in A or the ﬁrst element in B. Progress is made by removing the smaller of these two ﬁrst elements and adding it to the back of C. In this way, the algorithm proceeds like two lanes of trafﬁc merging into one. At each iteration, the ﬁrst car from one of the incoming lanes is chosen to move into the merged lane. This increases k by one. Initially, with k = 0, we simply have the given two lists. We stop when k = n. At this point, all the elements will be sorted in C. Merging is a key step in the merge sort algorithm presented in Section 9.1.  57  algorithm Merge list : A , B   cid:2  pre-cond cid:3 : A and B are two sorted lists.  cid:2  post-cond cid:3 : C is the sorted list containing the elements of the other two. begin  loop   cid:2 loop-invariant cid:3 : The k smallest of the elements are sorted in C.  The larger elements are still in their original lists A and B.  exit when A and B are both empty if  the ﬁrst in A is smaller than the ﬁrst in B or B is empty   then  next Element = Remove ﬁrst from A next Element = Remove ﬁrst from B  else  end if Add next Element to C  end loop return  C   end algorithm  3.4  Parsing with a Stack  One important use of stack is for parsing.  Speciﬁcations:  Preconditions: An input instance consists of a string of brackets.  Postconditions: The output indicates whether the brackets match. Moreover, each left bracket is allocated an integer 1, 2, 3, . . . , and each right bracket is al- located the integer from its matching left bracket.  Example:  Input: Output:    1  [ 2  { 3  } 3    4    4  ] 2    5    5  { 6    7    7  } 6    1   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  17:16  Iterative Algorithms and Loop Invariants  The Loop Invariant: Some preﬁx of the input instance has been read, and the cor- rect integer allocated to each of these brackets.  Thus, it is a more-of-the-input loop invariant.  The left brackets that have been read and not matched are stored along with their integers in left-to-right order in a stack, with the rightmost on top. The variable c indicates the next integer to be allocated to a left bracket.  58  Maintaining the Loop Invariant: If the next bracket read is a left bracket, then it is allocated the integer c. Not being matched, it is pushed onto the stack. c is incre- mented. If the next bracket read is a right bracket, then it must match the rightmost left bracket that has been read in. This will be on the top of the stack. The top bracket on the stack is popped. If it matches the right bracket, i.e., we have    , {}, or [], then the right bracket is allocated the integer for this left bracket. If not, then an error message is printed.  Initial Conditions: Initially, nothing has been read and the stack is empty.  Ending: If the stack is empty after the last bracket has been read, then the string has been parsed.  algorithm Par sing  s   cid:2  pre-cond cid:3 : s is a string of brackets.  cid:2  post-cond cid:3 : Prints out a string of integers that indicate how the brackets  Code:  match.  begin  i = 0, c = 1 loop  its left brackets are on the stack.   cid:2 loop-invariant cid:3 : Preﬁx s[1, i] has been allocated integers, and exit when i = n if s[i + 1] is a left bracket  then  print c  push  cid:2 s[i + 1], c cid:3   c = c + 1  elseif s[i + 1] = right bracket  then  if  stackempty      return “Cannot parse”   cid:2 left, d cid:3  = pop    if left matches s[i + 1]  then print d  else return “Cannot parse”    P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  17:16  Abstract Data Types  else  end if i = i + 1  end algorithm  return “Invalid input character”   end loop if  stackempty      return “Parsed”  else return “Cannot parse”   59  Parsing only “   ”: If you only need to parse one type of brackets and you only want to know whether or not the brackets match, then you do not need the stack in the above algorithm, only an integer storing the number of left brackets in the stack.  Parsing with Context-Free Grammars: To parse more complex sentences see Chapter 12 and Section 19.8.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  17:16  60  4 Narrowing the Search Space:  Binary Search  In this chapter, we will consider more binary search algorithms, which use the narrowing-the-search-space type of loop invariant. In this case, if the thing being searched for is anywhere, then it is in the narrowed sublist. We ﬁrst look at gen- eral binary search trees, which are often used in recursive algorithms  see Sec- tion 10.2  and then look at another example of an algorithm that incorporates binary search.  4.1  Binary Search Trees  Section 3.1 deﬁnes a binary search tree to be a binary tree data structure in which each node stores an element  and some asso- ciated data . The nodes are ordered so that for each node all the elements in its left subtree are smaller than that node’s element and all those in its right subtree are larger. I will show here how to search quickly for a element with a given tree.  14  5  2  8  21  25  Binary search tree  1  Speciﬁcations: Given a binary search tree and a key, ﬁnd a node whose element is this key or report that there is no such node.  2  Basic Steps: The restricted search space will be a subtree. Just as in binary search, our goal is to cut the size of this search space in half.  3  Measure of Progress: For binary search Example 1.4.3, the measure of progress was the number of elements in the current sublist. With binary search trees this number is not as predictable when the tree is not balanced. Hence, the measure of progress will be the number of edges in the path from the root of the entire subtree to the root of the current subtree.  4  The Loop Invariant: The loop invariant states that if the key is contained some- where in the entire binary search tree, then it is contained in our current subtree.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  17:16  Narrowing the Search Space: Binary Search  5  Main Steps: In binary search, we compared the key with the element in the mid- dle of the current search space. With an unbalanced binary search tree, we do not know which node is at the exact middle. Instead, we compare the key with the ele- ment at the root of the current subtree. If it contains the key, then we are done. If the key is smaller than it, then we know that if the key is anywhere, then it is in the left subtree of our current tree, else we know that it is in the right subtree.  61  6  Make Progress: As the root of our current subtree moves down to either its left or its right subtree, the measure of progress increases by one.  cid:1  cid:3  & not  cid:2 exit-cond  cid:3  & codeloop ⇒ 7  Maintain Loop Invariant:  cid:2 loop-invariant  cid:2 loop-invariant  cid:1  cid:1  cid:3 . The previous loop invariant gives that the search has been nar- rowed down to the current subtree. The property of binary search trees is that all the elements within this subtree that are smaller than the root of this subtree are in this subtree’s left subtree, and all those larger in its right. Hence, the main steps narrow the search space to the subtree, that would contain the key. 8  Establishing the Loop Invariant:  cid:2 pre-cond cid:3  & codepre-loop⇒ cid:2 loop-invariant cid:3 . Initially, you obtain the loop invariant by considering the entire tree of elements as the current subtree.  9  Exit Condition: We exit either when the key is found or when the current subtree becomes empty. 10  Ending:  cid:2 loop-invariant  cid:3  &  cid:2 exit-cond  cid:3  & codepost-loop ⇒  cid:2 post-cond cid:3 . By the exit condition, either we have found the key, in which case we are done, or we have narrowed the search space to an empty subtree. The loop invariant says that if the key is contained in the original list, then the key is contained in this empty subtree, which it is not, and hence the algorithm can safely report that the key is not in the original list.  11  Termination and Running Time: The number of iterations of this algorithm is at most the height of the binary search tree, which  if the tree more or less balanced  is  cid:1  log n .  12  Special Cases: There are no special cases here: any input is either equal to, larger than, or smaller than the root of the current subtree.  13  Coding and Implementation Details:  algorithm SearchBST tree, keyToFind   cid:2  pre-cond cid:3 : tree is a binary tree whose nodes contain key and data ﬁelds. keyToFind is a key.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  17:16  Iterative Algorithms and Loop Invariants   cid:2  post-cond cid:3 : If there is a node with this key in the tree, then the associated data is returned.  begin  subtree = tree loop cid:2 loop-invariant cid:3 : If the key is contained in tree, then the key is  62  contained in subtree.  if  subtree = emptyTree   then result  “key not in tree”   subtree = leftSub subtree   else if  keyToFind < rootKey subtree    then else if  keyToFind = rootKey subtree    then  else if  keyToFind > rootKey subtree    then  result  rootData subtree    subtree = rightSub subtree   end if end loop end algorithm  4.2 Magic Sevens  My mom gave my son Joshua a book of magic tricks. The book says, “This trick really is magic. It comes right every time you do it, but there is no explanation why.” As it turns out, there is a bug in the way that they ex- plain the trick. Our task is to ﬁx the bug and to counter “there is no explanation why.” The only magic is that of loop invariants. The algorithm is a variant on binary search.  1  Speciﬁcations:   cid:1  Let c, an odd integer, be the number of columns. The book uses c = 3.  cid:1  Let r , an odd integer, be the number of rows. The book uses r = 7.  cid:1  Let n = c · r be the number of cards. The book uses n = 21.  cid:1  Let t be the number of iterations. The book uses t = 2.  cid:1  Let f be the ﬁnal index of the selected card. The book uses f = 11.  cid:1  Ask someone to select one of the n cards and then shufﬂe the deck.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  17:16  Narrowing the Search Space: Binary Search   cid:1  Repeat t times:   cid:1  Spread the cards out as follows. Put c cards in a row left to right face up. Put a second row on top, but shifted down slightly so that you can see what both the ﬁrst and second row of cards are. Repeat for r rows. This forms c columns of r cards each.   cid:1  Ask which column the selected card is in.  cid:1  Stack the cards in each column. Put the selected column in the middle.  This  63  is why c is odd.    cid:1  Output the f th card.  Our task is to determine for which values c, r , n, t, and f this trick ﬁnds the selected card.  Easier Version: Analyzing this trick turns out to be harder than I initially thought. Hence, we’ll consider the following easier trick ﬁrst. Instead of putting the selected column in the middle of the other columns, we put it in front.  2  Basic Steps: At each iteration we gain some information about which card had been selected. The trick seems to be similar to binary search. A difference is that bi- nary search splits the current sublist into two parts, whereas this trick splits the entire pile into c parts. In both algorithms, at each iteration we learn which of these piles the sought-after element is in.  4  Loop Invariant: A good ﬁrst guess for a loop invariant would be that used by binary search. The loop invariant will state that some subset Si of the cards contains the selected card. In this easier version, the column containing the card is continually moved to the front of the stack. Hence, let us guess that Si = {1, 2, . . . , si} indexes the ﬁrst si cards in the deck. We will later solve a recurrence relation to determine that si =  cid:7 n ci cid:8 .   cid:1  cid:3  & not  cid:2 exit-cond  cid:3  & codeloop ⇒ 7  Maintain Loop Invariant:  cid:2 loop-invariant  cid:2 loop-invariant  cid:1  cid:1  cid:3 . By the previous loop invariant, the selected card is one of the ﬁrst si−1 in the deck. When the cards are laid out, the ﬁrst si−1 cards will be spread on the tops of the c columns. Some columns will get  cid:7 si−1 c cid:8  of these cards, and some will get  cid:9 si−1 c cid:10  of them. When we are told which column the selected card is in, we will know that the selected card is one of the ﬁrst  cid:7 si−1 c cid:8  cards in this column. In conclusion,   cid:1    cid:2    cid:3  cid:4    cid:6    cid:5   si =  =  si−1 c  n ci−1 c   cid:2   .   cid:1   =  n ci   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  17:16  Iterative Algorithms and Loop Invariants  8  Establishing the Loop Invariant: Again, as done in binary search, we initially obtain the loop invariant by considering the entire stack of cards, giving s0 =  cid:7 n c0 cid:8  = n.  64  9  Exit Condition: When sufﬁcient rounds have occurred so that st = 1, the search space has narrowed down to containing only the ﬁrst card. Hence, the algorithm is able to select card f = 1.  11  Running Time: After t =  cid:7 logc n cid:8  rounds, st =  cid:7 n ct cid:8  = 1.  For a matching lower bound on the number of iterations needed see Chapter 7. The book has n = 21, c = 3, and t = 2. Because 21 = n  cid:11 ≤ ct = 32 = 9, the trick in  the book does not work. Two rounds is not enough. There must be three.  Original Trick: Consider again the original trick where the selected column is put into the middle.  4  The Loop Invariant: Because the selected column is put into the middle, let us guess that Si consists of the middle si cards. More formally, let di =  n − si  2. Neither the ﬁrst nor the last di cards will be the selected card. Instead it will be one of Si = {di + 1, . . . , di + si}. Note that both n and si need to be odd.  8  Establishing the Loop Invariant: For i = 0, we have s0 = n, d0 = 0, and the se- lected card can be any card in the deck.  7  Maintain Loop Invariant: Suppose that before the ith iteration, the selected card is not one of the ﬁrst di−1 cards, but is one of the middle si−1 in the deck. Then when the cards are laid out, the ﬁrst di−1 cards will be spread on the tops of the c columns. Some columns will get  cid:7 di−1 c cid:8  of these cards, and some will get  cid:9 di−1 c cid:10  of them. In general, however, we can say that the ﬁrst  cid:9 di−1 c cid:10  cards of each col- umn are not the selected card. We use the ﬂoor instead of the ceiling here, because this is the worst case. By symmetry, we also know that the selected card is not one of the last  cid:9 di−1 c cid:10  cards in each column. When the person points at a column, we learn that the selected card is somewhere in that column. However, from be- fore we knew that the selected card is not one of the ﬁrst or last  cid:9 di−1 c cid:10  cards in this column. There are only r cards in the column. Hence, the selected card must be one of the middle r − 2 cid:9 di−1 c cid:10  cards in the column. Deﬁne si to be this value. The new deck is formed by stacking the columns together with these cards in the middle.  9  Exit Condition: When sufﬁcient rounds have occurred so that st = 1, then the selected card will be in the middle indexed by f =  cid:7  n   cid:8 .  2   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  17:16  65  Narrowing the Search Space: Binary Search Trick in Book: The book has n = 21, c = 3, and r = 7. Thus  c  2   cid:10   si = r − 2 cid:9  di−1 s0 = n = 21 s1 = 7 − 2 cid:9  0 s2 = 7 − 2 cid:9  7 s3 = 7 − 2 cid:9  9  = 0 = 7 = 9 = 10 Again three and not two rounds are needed.  di = n−si d0 = 21−21 d1 = 21−7 d2 = 21−3 d3 = 21−1   cid:10  = 7  cid:10  = 3  cid:10  = 1  3  2  2  3  2  2  3  Si = {di + 1, . . . , di + si} S0 = {1, 2, . . . , 21} S1 = {8, 9, . . . , 14} S2 = {10, 11, 12} S3 = {11}   cid:8    cid:7   11  Running Time: Temporarily ignoring the ﬂoor in the equation for si makes the analysis easier. We have si = r − 2  = n c Again, this recurrence relation gives that si = n ci. If we include the ﬂoor, challeng- ing manipulations give that si = 2 cid:9 si−1 2c − 1  cid:10  + 1. More calculations give that si is always n ci rounded up to the next odd integer.   n − si−1  2  = si−1 c  ≈ r − 2  di−1 c  di−1 c  − 2  c  2  .  EXERCISE 4.2.1 Give code for the original Magic Sevens trick.  EXERCISE 4.2.2 Suppose S and T are sorted arrays, each containing n elements. Find the nth smallest of the 2n numbers.  4.3  VLSI Chip Testing  The following is a strange problem with strange rules. However, it is no stranger than the problems that you will need to solve in the world. We will use this as an example of how to develop a strange loop invariant with which the algorithm and its correctness become transparent.  Speciﬁcation: Our boss has n supposedly identical VLSI chips that are potentially capable of testing each other. His test jig accommodates two chips at a time. The result is either that they are the same  that is, both are good or both are bad , or that they are different  that is at least one is bad . The professor hires us to design an algorithm to distinguish good chips from bad ones.  Impossible? Some computational problems have exponential-time algorithms, but no polynomial time algorithms. Because we are limited in what we are able to do, this problem may not have an algorithm at all. It is often hard to know. A good thing to do with a new problem is to alternate between author and critic. The author does his best to design an algorithm for the problem. The critic does his best to prove that the author’s algorithm does not work or, even better, prove that no algorithm works.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  17:16  Iterative Algorithms and Loop Invariants  Suppose that the professor happened to have one good chip and one bad chip. The single test that he has tells him that these chips are different, but does not tell him which is which. There is no algorithm using only this single test that accomplishes the task. The professor may not be happy with our ﬁndings, but he will not be able to blame us.  66  Though we have shown that there is no algorithm that distinguishes these two  chips, perhaps we can ﬁnd an algorithm that can be of some use for the professor.  A Data Structure: It is useful to have a good data structure with which to store the information that has been collected. Here we can have a graph with a node for each chip. After testing a pair of chips, we put a solid edge between the corresponding nodes if they are reportedly the same and a dotted edge if they are different.  The Brute Force Algorithm: One way of understanding a problem better is to ini- tially pretend that you have unbounded time and energy. With this, what tasks can you accomplish? With  cid:1  n2  tests we can test every pair of chips. We assume that the test is tensitive, meaning that if chip a tests to be the same as b, which tests to be the same as c, then a will test to be the same as c. Given this, we can conclude that the tests will partition the chips into sets of chips that are the same.  In graph theory we call these sets cliques, as in a clique of friends in which everyone in the group is friends with everyone else in the group.  There is, however, no test available to deter- mine which of these sets contain the good chips.  Change the Problem: When you get stuck, a useful thing to do is to go back to your boss or to the application at hand and see if you can change the problem to make it easier. There are three ways of doing this.  More Tools: One option is to allow the algorithm more powerful tools. A test that told you whether a chip was good would solve the problem. On the other hand, if the professor had such a test, you would be out of a job.  Change the Preconditions: You can change the preconditions to require addi- tional information about the input instance or to disallow particularly difﬁcult instances. You need some way of distinguishing between the good chips and the various forms of bad chips. Perhaps you can get the professor to assure you that more than half of the chips are good. With this, you can solve the problem. Test all pairs of chips and partition the chips into the sets of equivalent chips. The largest of these sets will be the good chips.  Change the Postconditions: Another option is to change the postconditions by not requiring so much in the output. Instead of needing to distinguish com- pletely between good and bad chips, an easier task would be to ﬁnd a single good chip.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  17:16  Narrowing the Search Space: Binary Search  A Faster Algorithm: Once we have our brute force algorithm, we will want to at- tempt to ﬁnd a faster algorithm. Let us attempt to ﬁnd a single good chip from among n chips, assuming that more than n 2 of the chips are good, using an iterative algo- rithm. Hopefully, it will be faster than  cid:1  n2  time.  Designing the Loop Invariant: In designing an iterative algorithm for this prob- lem, the most creative step is designing the loop invariant.  67  Start with Small Steps: What basic steps might we follow to make some kind of progress? Certainly the ﬁrst step is to test two chips. There are two cases.  Different: Suppose that we determine that the two chips are different. One way to make progress is to narrow down the input instance while maintain- ing what we know about it. What we know is that more than half of the chips are good. Because we know that at least one of the two tested chips is bad, we can throw both of them away. We know that we do not go wrong by do- ing this, because we maintain the loop invariant that more than half of the chips are good. From this we know that there is still at least one good chip remaining, which we can return as the answer.  Same: If the two chips test the same, we cannot throw them away, be- cause they might both be good. However, this too seems like we are making progress, because, as in the brute force algorithm, we are building up a set of chips that are the same.  Picture from the Middle: From our single step, we saw two forms of progress. First, we saw that some chips will have been set aside. Let S denote the subset containing all the chips that we have not set aside. Second, we saw that we were building up sets of chips that we know to be the same. It may turn out that we will need to maintain a number of these sets. To begin, however, let’s start with the simplest picture and build only one such set.  The Loop Invariant: We maintain two sets. The set S contains the chips that we have not set aside. We maintain that more than half of the chips in S are good. The set C is a subset of S. We maintain that all of the chips in C are the same, though we do not know whether they are all good or all bad.  Type of Loop Invariant: This is a strange loop invariant, but it has a number of familiar aspects.  More of the Input: We do consider the chips one at a time in order.  More of the Output: The set C is our ﬁrst guess at what the outputted good chips will be  but this may change .  Narrow the Search Space: The narrowed set S contains at least one good chip.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  17:16  Iterative Algorithms and Loop Invariants  Case Analysis: We check different cases as to whether the chips are the same or different.  Work Done: The sets S and C keep track of the work done so that it need not be redone.  68   cid:1  cid:3 & not cid:2 exit-cond  cid:3 &codeloop ⇒ Maintaining the Loop Invariant:  cid:2 loop-invariant  cid:2 loop-invariant  cid:1  cid:1  cid:3 . Assume that all we know is that the loop invariant is true. It being the only thing that we know how to do, we must test two chips. Testing two from C is not useful, because we already know that they are the same. Testing two that are not in C is dangerous, because if we learn that they are same, then we will have to start a second set of alike chips, yet we previously decided to maintain only one. The remaining possibility is to choose any chip from C and any from S−C and test them. Let us denote these chips by c and s.  Same: If the conclusion is that the chips are the same, then add chip s to C. We have not changed S, so its loop invariant still holds. From our test, we know that s has the same characteristic as c. From the loop invariant, we know that c is that same as all the other chips in C. Hence, we know that s is the same as all the other chips in C, and the loop invariant follows.  Different: If the conclusion is that at least one is bad, then delete both c and s from C and S. Now S has lost two chips, at least one of which is bad. Hence, we have maintained the fact that more than half of the chips in S are good. Also, C has only become smaller, and hence we have maintained the fact that its chips are all the same.  Either way, we maintain the loop invariant while making some  yet undeﬁned  progress. Handle All Cases: We can only test one chip from C and one from S−C if both are nonempty. We need to consider the cases in which they are not.  S Is Empty: If S is empty, then we are in trouble, because we have no more chips to return as the answer. We must stop before this. S−C Is Empty: If S−C is empty, then we know that all the chips in S = C are the same. Because more than half of them must be good, we know that all of them are good. Hence, we are done.  C Is Empty: If C is empty, take any chip from S and add it to C. We have not changed S, so its loop invariant still holds. The single chip in C is the same as itself.  The Measure of Progress: The measure cannot be S, because this does not de- crease when the chips are the same. Instead, let the measure be S−C. In two of our   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  17:16  69  Narrowing the Search Space: Binary Search cases, we remove a chip from S−C and add it to C. In another case, we remove a chip from S−C and one from C. Therefore, in all cases this measure decreases by 1. Initial Code:  cid:2 pre-cond  cid:3  & codepre-loop ⇒  cid:2 loop-invariant  cid:3 . The initial code sets S to be all the chips and C to be empty. More than half of the chips in S are good according to the precondition. Because there are no chips in C, all the chips that are in it are the same. Exiting Loop:  cid:2 loop-invariant  cid:3 & cid:2 exit-cond  cid:3 &codepost-loop ⇒ cid:2 post-cond cid:3  . S − C= 0 is a good halting condition, but not the ﬁrst. Halt when C > S 2, and return any chip from C. According to the loop invariant, the chips in C are either all good or all bad. The chips in C constitute more than half the chips, so if they were all bad, more than half of the chips in S would also be bad. This contradicts the loop invariant. Hence, the chips in C are all good. Running Time: Initially, the measure of progress S−C is n. We showed that it de- creases by at least 1 each iteration. Hence, there are at most n steps before S−C is empty. We are guaranteed to exit the loop by this point, because S−C = 0 assures us that the exit condition C = S > S 2 is met. S must contain at least one chip, because by the loop invariant more than half of them are good.  Additional Observations: C can ﬂip back and forth between being all bad and be- ing all good many times. Suppose it is all bad. If s from S−C happens to be bad, then C gets bigger. If s from S−C happens to be good, then C gets smaller. If C ever be- comes empty during this process, then a new chip is added to C. This chip may be good or bad. The process repeats.  Extending the Algorithm: The algorithm ﬁnds one good chip. This good chip will tell you which of the other chips are good in O n  time.  Randomized Algorithm: Chapter 21 provides a much easier randomized algo- rithm for this problem.  4.4  Exercises  EXERCISE 4.4.1  See solution in Part Five.  Search a sorted matrix: The input consists of a real number x and a matrix A[1..n, 1..m] of nm real numbers such that each row A[i, 1..m] is sorted and each column A[1..n, j ] is sorted. The goal is to ﬁnd the maxi- mum array entry A[i, j ] that is less than or equal to x, or report that all elements of A are larger than x. Design and analyze an iterative algorithm for this problem that ex- amines as few matrix entries as possible. Becareful if you believe that a simple binary search solves the problem. Exercise 7.0.7 asks for a lower bound, and Exercise 9.1.3 for a recursive algorithm.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  17:16  70  Iterative Algorithms and Loop Invariants  EXERCISE 4.4.2 Suppose a train was supposed to start at station A, pause at stations B, C, D, . . . , Y, and ﬁnish at station Z. However, it did not arrive at Z. Suppose in your factory the piece of equipment or process labeled A makes part B work, which in turn makes C, D, . . . , and Z work. However, Z is not working. You want to ﬁnd out why. What algorithm do you use? Think of other applications of this technique.  EXERCISE 4.4.3 The following question is not about narrowing the search space. If anything, it is about doubling the size of the search space. But it still has a binary search feel. This question ﬁnds the length Dmin u, v  of the shortest path between any pair of nodes in a directed  or undirected  graph. The input provides the length d u, v  ≥ 0 of each edge  cid:2 u, v cid:3  in the complete graph. It is not necessary that d u, v  = d v, u , and these lengths may be ∞. The distance along a path is the sum of the d u, v  values along its edges. Let di u, v  denote the length of the shortest path from u to v with at most 2i edges. Do steps 7, 8, 10, and 11, proving that the loop invariant is established and maintained, proving that on exiting the postcondition is established, and bounding the running time. As a hint, trace the algorithm on a graph consisting of a single path, namely, for j ∈ [1, n − 1], d uj , uj+1  = 1 and all other edges have d u, v  = ∞.  algorithm Alg d   cid:2  pre-cond cid:3 : d u, v  ∈ [0,∞] is the length of edge  cid:2 u, v cid:3  in the complete graph.  cid:2  post-cond cid:3 : Returned is the length Dmin u, v  of the shortest path from u to v for each pair of nodes.  begin  for each edge  cid:2 u, v cid:3 , D u, v  = d u, v  loop log2 n  times   cid:2 loop-invariant cid:3 : After i iterations, Dmin u, v  ≤ D u, v  ≤ di u, v  for each edge  cid:2 u, v cid:3   iteratively or in parallel  % Di u, v  = Min Di−1 u, v , Minw[Di−1 u, w  + Di−1 w, v ]  loop over nodes w  if D u, v  ≥ D u, w  + D w, v   then  D u, v  = D u, w  + D w, v   end if  end loop  end loop  end loop return   D   end algorithm  One can ﬁnd more about this in Exercise 19.6.2.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes5 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  18:19  5 Iterative Sorting Algorithms  71  Sorting is a classic computational problem. During the ﬁrst few decades of comput- ers, almost all computer time was devoted to sorting. Many sorting algorithms have been developed. It is useful to know a number of them, because sorting needs to be done in many different situations. Some depend on low time complexity, other on small memory, others on simplicity. Throughout the book, we consider a number of sorting algorithms because they are simple yet provide a rich selection of examples for demonstrating different algorithmic techniques. We have already looked at selec- tion, insertion, and bubble sort in Section 1.4. In this chapter we start with a simple version of bucket sort and then look at counting sort. Radix sort, which is another surprising sort, is considered. Finally, counting and radix sort are combined to give radix counting sort. Most sorting algorithms are said to be comparison-based, because the only way of accessing the input values is by comparing pairs of them, i.e., ai ≤ a j . Radix count- ing sort manipulates the elements in other ways. Another strange thing about this algorithm is that its loop invariants are rather unexpected.  In Section 9.1, we consider merge sort and quick sort, which is a recursive and  randomized version of bucket sort. We look at heap sort in Section 10.4.  5.1  Bucket Sort by Hand  Speciﬁcations: As a professor, I often have to sort a large stack of students’ papers by last name. The algorithm that I use is an iterative version of quick sort and bucket sort. See Section 9.1.  Basic Steps:  Partitioning into Five Buckets: Computers are good at using a single comparison to determine whether an element is greater than the pivot value or not. Humans, on the other hand, tend to be good at quickly determining which of ﬁve buckets an element belongs in. I ﬁrst partition the papers based on which of the following   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes5 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  18:19  72  Iterative Algorithms and Loop Invariants  ranges the ﬁrst letter of the name is within: [A–E], [F–K], [L–O], [P–T], or [U–Z]. Then I partition the [A–E] bucket into the subbuckets [A], [B], [C], [D], and [E]. Then I partition the [A] bucket based on the second letter of the name. This works for this application because the list to be sorted consists of names whose ﬁrst letters are fairly predictably distributed through the alphabet.  A Stack of Buckets: One difﬁculty with this algorithm is keeping track of all the buckets. For example, after the second partition, we will have nine buckets: [A], [B], [C], [D], [E], [F–K], [L–O], [P–T], and [U–Z]. After the third, we will have 13. On a computer, the recursion of the algorithm is implemented with a stack of stack frames. Correspondingly, when I sort the student’s papers, I have a stack of buckets.  The Loop Invariant: I use the following loop invariant to keep track of what I am doing. The papers are split between a pile of already sorted papers and a stack of piles of partially sorted papers. The papers in the sorted pile  initially empty  come before all the partially sorted papers. Within the partially sorted stack of piles, the papers within each pile are out of order. However, each paper in a pile belongs before each paper in a later pile. For example, at some point in the algorithm, the papers starting with [A–C] will be sorted, and the piles in my stack will consist of [D], [E], [F–K], [L–O], [P–T], and [U–Z].  Maintain Loop Invariant: I make progress while maintaining this loop invariant as follows. I take the top pile off the stack, here the [D]. If it only contains a half dozen or so papers, I sort them using insertion sort. These are then added to the top of the sorted pile, [A–C], giving [A–D]. On the other hand, if the pile [D] taken off the stack is larger then this, I partition it into ﬁve piles, [DA–DE], [DF–DK], [DL–DO], [DP– DT], and [DU-DZ], which I push back onto the stack. Either way, my loop invariant is maintained.  Exit Condition: When the last bucket has been removed from the stack, the papers are sorted.  EXERCISE 5.1.1 Try sorting a deck of cards using this algorithm.  EXERCISE 5.1.2 Give code for this algorithm.  5.2  Counting Sort  a Stable Sort   The counting sort algorithm is only useful in the special case where the elements to be sorted have very few possible values.  Speciﬁcations:  Preconditions: The input is a list of N values a0, . . . , a N−1, each within the range 0, . . . , k − 1.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes5 CUUS154-Edmonds 978 0 521 84931 9  Iterative Sorting Algorithms  April 2, 2008  18:19  Postconditions: The output is a list consisting of the same N values in nonde- creasing order. The sort is stable, meaning that if two elements have the same value, then they must appear in the same order in the output as in the input.  This is important when extra data is carried with each element.   Basic Steps:  73  Where an Element Goes: Consider any element of the input. By counting, we will determine where this element belongs in the output, and then we simply put it there. Where it belongs is determined by the number of elements that must ap- pear before it. To simplify the argument, let’s index the locations with [0, N − 1]. the element in location cid:1 c has cid:1 c elements before it. This way, the element in the location indexed by 0 has no elements before it, and smaller value must go before it. Let’s denote this count with  cid:1 cv, that is,  cid:1 cv = Suppose that the element ai has the value v. Every element that has a strictly {j  a j < v}. The only other elements that go before ai are elements with exactly the same value. Because the sort must be stable, the number of these that go be- happens to be qai , then element ai belongs in location cid:1 cv + qai . In particular, the fore it is the same as the number that appear before it in the input. If this number ﬁrst element in the input with value v goes in location cid:1 cv + 0.  Example:  Input: 1 0 1 0 2 0 0 1 2 0 Output: 0 0 0 0 0 1 1 1 2 2 Index: 0 1 2 3 4 5 6 7 8 9  The ﬁrst element to appear in the input with value 0 goes into location 0, be-  cause there are cid:1 c0 = 0 elements with smaller values. The next such element 5, because there are cid:1 c1 = 5 elements with smaller values. The next such ele-  The ﬁrst element to appear in the input with value 1 goes into location  goes into location 1, the next into 2, and so on.  ment goes into location 6, and the next into 7.  Similarly, the ﬁrst element with value 2 goes into location cid:1 c2 = 8.  Computing cid:1 cv: We could compute cid:1 cv by making a pass through the input, count- ing the number of elements that have values smaller than v. Doing this sepa- rately for each value v ∈ [0..k − 1], however, would take O k N   time, which is too much. Instead, let’s ﬁrst count how many times each value occurs in the input. For each v ∈ [0..k − 1], let cv = {i  ai = v}. This count can be computed with one Given the cv values, we could compute cid:1 cv = cid:2  pass through the input. For each element, if the element has value v, increment the counter cv. This requires only O N  addition and indexing operations.  cid:1 cv would require O k  additions, and computing all of them would take O k2   v−1 v cid:1 =0 cv. Computing one such  additions, which is too much.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes5 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  18:19  Iterative Algorithms and Loop Invariants  Alternatively, note that cid:1 c0 = 0 and cid:1 cv = cid:1 cv−1 + cv−1. Of course, we must have such cid:1 cv takes O 1  additions, and computing all of them takes only O k  addi-  computed the previous values before computing the next. Now computing one  tions.  74  Put in Place: The main loop in the algorithm considers the input elements one at a time in the order a0, . . . , a N−1 that they appear in the input and places them in the output array where they belong.  The Loop Invariant:  input element with value v goes.  1. The input elements that have already been considered have been put in their correct places in the output.  establishes the loop invariant before any input elements are considered, because this  2. For each v ∈ [0..k − 1], cid:1 cv gives the index in the output array where the next Establishing the Loop Invariant: Compute the counts cid:1 cv as described above. This  cid:1 cv value gives the location where the ﬁrst element with value v goes. tion indexed by cid:1 cv. Then increment cid:1 cv. Maintain Loop Invariant:  cid:4 loop-invariant  cid:4 exit-cond  cid:5  & codeloop ⇒  cid:4 loop-invariant  cid:1  cid:1  cid:5 . By the loop invariant, we know that if the next input element has value v, then it belongs in the output location indexed by cid:1 cv. Hence, it is being put after this current one in the output, i.e., into location cid:1 cv + 1. Hence, incrementing cid:1 cv  Main Step: Take the next input element. If it has value v, place it in the output loca-  in the correct place. The next input element with value v will then go immediately   cid:1  cid:5  & not  maintains the second part of the loop invariant.  Exit Condition: Once all the input elements have been considered, the ﬁrst loop invariant establishes that the list has been sorted.  Code:  ∀v ∈ [0..k − 1], cv = 0 loop i = 0 to N − 1 + + ca[i]  cid:1 c0 = 0 loop v = 1 to k − 1  cid:1 cv = cid:1 cv−1 + cv−1 loop i = 0 to N − 1 b[ cid:1 ca[i]] = a[i] + + cid:1 ca[i]   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes5 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  18:19  Iterative Sorting Algorithms Running Time: The total time is O N + k  addition and indexing operations. If the input can only contain k = O N  possible values, then this algorithm works in linear time. It does not work well if the number of possible values is much higher.  The radix sort is a useful algorithm that dates back to the days of card-sorting ma- chines, now found only in computer museums.  75  5.3  Radix Sort  Speciﬁcations:  Preconditions: The input is a list of N values. Each value is an integer with d dig- its. Each digit is a value from 0 to k − 1, i.e., the value is viewed as an integer base k.  Postconditions: The output is a list consisting of the same N values in nonde- creasing order.  Basic Steps: For some digit i ∈ [1..d], sort the input according to the ith digit, ignor- ing the other digits. Use a stable sort, such as counting sort.  Examples: Old computer punch cards were organized into d = 80 columns, and in each column a hole could be punched in one of k = 12 places. A card-sorting machine could mechanically examine each card in a deck and distribute the card into one of 12 bins, depending on which hole had been punched in a speciﬁed column.  A “value” might consist of a year, a month, and a day. You could then sort the  elements by the year, by the month, or by the day.  Order in Which to Consider the Digits: It is most natural to sort with respect to the most signiﬁcant digit ﬁrst. The ﬁnal sort, after all, has all the elements with a 0 as the ﬁrst digit at the beginning, followed by those with a 1.  If the operator of the card-sorting machine sorted ﬁrst by the most signiﬁ- cant digit, he would get 12 piles. Each of these piles would then have to be sorted separately, according to the remaining digits. Sorting the ﬁrst pile according to the second digit would produce 12 more piles. Sorting the ﬁrst of those piles ac- cording to the third digit would produce 12 more piles. The whole process would be a nightmare. Sorting with respect to the least signiﬁcant digit seems silly at ﬁrst. Sorting  cid:4 79, 94, 25 cid:5  gives  cid:4 94, 25, 79 cid:5 , which is completely wrong. Even so, this is what the algorithm does.  The Algorithm: Loop through the digits from low to high order. For each, use a stable sort to sort the elements according to the current digit, ignoring the other digits.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes5 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  18:19  Iterative Algorithms and Loop Invariants  76  Example:  184 192 195 243 271 311  Sorted by ﬁrst 3 digits  Considering 4th digit  Stably sorted by 4th digit  3184 5192 1195 1243 3271 1311  1195 1243 1311 3184 3271 5192  The result is sorted by the ﬁrst four digits.  Loop Invariant: After sorting with respect to  wrt  the ﬁrst i low-order digits, the elements are sorted wrt the value formed from these i digits.   cid:1  cid:5  & not  Establishing the Loop Invariant: The loop invariant is initially trivially true, be- cause initially no digits have been considered. Maintain Loop Invariant:  cid:4 loop-invariant  cid:4 exit-cond  cid:5  & codeloop ⇒  cid:4 loop-invariant  cid:1  cid:1  cid:5 . Suppose that the elements are sorted wrt the value formed from the lowest i − 1 digits. For the elements to be sorted wrt the value formed from the lowest i digits, all the elements with a 0 in the ith digit must come ﬁrst, followed by those with a 1, and so on. This can be accomplished by sorting the elements wrt the ith digit while ignoring the other digits. Moreover, the block of elements with a 0 in the ith digit must be sorted wrt the lowest i − 1 digits. By the loop invariant, they were in this order, and because the sorting wrt the ith digit was stable, these elements will remain in the same relative order. The same is true for the block of elements with a 1 or 2 or . . . in the ith digit. Ending:  cid:4 loop-invariant  cid:5  &  cid:4 exit-cond  cid:5  & codepost-loop ⇒  cid:4 post−cond cid:5 . When i = d, they are sorted wrt the value formed from all d digits, and hence are sorted.  5.4  Radix Counting Sort  I will now combine the radix and counting sorts. The resulting algorithm is said to run in linear  cid:1  n  time, whereas merge, quick, and heap sort are said to run in  cid:1  n log n  time. This makes radix counting appear to be faster, but this is confusing and mis- leading. Radix counting requires  cid:1  n  bit operations, where n is the total number of bits in the input instance. Merge, quick, and heap sort require  cid:1  N log N   com- parisons, where N is the number of numbers in the list. Assuming that the N num- bers to be sorted are distinct, each needs  cid:1  log N   bits to be represented, for a total of n =  cid:1  N log N   bits. Hence, merge, quick, and heap sort are also linear time in that they require  cid:1  n  bit operations, where n is the total number of bits in the input instance.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes5 CUUS154-Edmonds 978 0 521 84931 9  Iterative Sorting Algorithms  April 2, 2008  18:19  In practice, the radix counting algorithm may be a little faster than the other al- gorithms. However, quick and heap sort have the advantage of being done “in place” in memory, while the radix counting sort requires an auxiliary array of memory to transfer the data to.  Speciﬁcations:  77  Preconditions: The input is a list of N values. Each value is an l-bit integer.  Postconditions: The output is a list consisting of the same N values in nonde- creasing order.  The Algorithm: The algorithm is to use radix sort with counting sort to sort each digit. To do this, we need to view each l-bit value as an integer with d digits, where each digit is a value from 0 to k − 1. This is done by splitting the l bits into d blocks d bits each and treating each such block as a digit between 0 and k − 1, where of l k = 2l d. Here d is a parameter to be set later. Example: Consider sorting the numbers 30, 41, 28, 40, 31, 26, 47, 45. Here N = 8 and l = 6. Let’s set d = 2 and split the l = 6 bits into d = 2 blocks of l = 3 bits each. Treat each of these blocks as a digit between 0 and k − 1, where k = 23 = 8. For ex- ample, 30 = 0111102 gives the blocks 0112 = 3 and 1102 = 6.  d  For all the numbers: 30 = 368 = 011 1102 41 = 518 = 101 0012 28 = 348 = 011 1002 40 = 508 = 101 0002 31 = 378 = 011 1112 26 = 328 = 011 0102 47 = 578 = 101 1112 45 = 558 = 101 1012  This is sorted.  Stable sorting wrt the ﬁrst digit: 40 = 508 = 101 0002 41 = 518 = 101 0012 26 = 328 = 011 0102 28 = 348 = 011 1002 45 = 558 = 101 1012 30 = 368 = 011 1102 31 = 378 = 011 1112 47 = 578 = 101 1112  Stable sorting wrt the second digit: 26 = 328 = 011 0102 28 = 348 = 011 1002 30 = 368 = 011 1102 31 = 378 = 011 1112 40 = 508 = 101 0002 41 = 518 = 101 0012 45 = 558 = 101 1012 47 = 578 = 101 1112  Running Time: Using the counting sort to sort with respect to one of the d digits takes  cid:1  N + k  operations. Hence, the entire algorithm takes  cid:1  d ·  N + k   opera- tions. We have d = l The parameter k  like l  is not dictated by the speciﬁcations of the problem, but can be chosen freely by the algorithm. Exercise 23.1.4 sets k = O N  in order to min- imize the running time to T =  cid:1    ·  N + k   operations.  log k , giving T =  cid:1    l log N N   operations.  Formally, time complexity measures the number of bit operations performed as a function of the number of bits to represent the input. When we say that  log k  l   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes5 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  18:19  Iterative Algorithms and Loop Invariants counting sort takes  cid:1  N + k  operations, a single operation must be able to add two values with magnitude  cid:1  N   or to index into arrays of size N  or k . Each of these takes  cid:1  log N   bit operations. Hence, the total time to sort is T =  cid:1   l log N N  oper- ations × log N  bit operations  operation =  cid:1  l · N   bit operations. The input, con- sisting of N l-bit values, requires n = l · N bits to represent it. Hence, the running time  cid:1  l · N   =  cid:1  n  is linear in the size of the input. One example is when you are sorting N values in the range 0 to N r . Each value re- quires l = log N r = r log N bits to represent it, for a total of n = N log N r   = r N bits. Our settings would then be k = N, d = l = r , and T =  cid:1  d · N   =  cid:1  r N   =  cid:1  n .  log N  78   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  March 25, 2008  19:49  6 Euclid’s GCD Algorithm  79  More-of-the-input iterative algorithms extend a solution for a smaller input instance into a larger one. We will see in Chapter 9 that recursive algorithms do this too. The following is an amazing algorithm that does this. It ﬁnds the greatest common divisor  GCD  of two integers. For example, GCD  18, 12  = 6. It was ﬁrst done by Euclid, an ancient Greek. Without the use of loop invariants, you would never be able to under- stand what the algorithm does; with their help, it is easy.  Speciﬁcations: An input instance consists of two positive integers, a and b. The output is GCD  a, b .  The Loop Invariant: Like many loop invariants, designing this one required cre- ativity. The algorithm maintains two variables x and y whose values change with each iteration of the loop under the invariant that their GCD, GCD  x, y , does not change, but remains equal to the required output GCD  a, b .  Type of Loop Invariant: This is a strange loop invariant. The algorithm is more like recursion. A solution to a smaller instance of the problem gives the solution to the original.  Establishing the Loop Invariant: The easiest way of establishing the loop invari- ant that GCD  x, y  = GCD  a, b  is by setting x to a and y to b.  Measure of Progress: Progress is made by making x or y smaller.  Ending: We will exit when x or y is small enough that we can compute their GCD easily. By the loop invariant, this will be the required answer.  A Middle Iteration on a General Instance: Let us ﬁrst consider a general situa- tion in which x is bigger than y and both are positive.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  March 25, 2008  19:49  80  Iterative Algorithms and Loop Invariants  Main Steps: Our goal is to make x or y smaller without changing their GCD. A useful fact is that GCD  x, y = GCD  x − y, y , e.g., GCD  52, 10 = GCD  42, 10  = 2, because any value that divides x and y also divides x − y, and similarly any value that divides x − y and y also divides x. Hence, replacing x with x − y would make progress while maintaining the loop invariant.  Exponential Running Time? A good idea when you are considering a loop in- variant and iterations is to jump ahead in designing the algorithm and esti- mate its running time. A loop executing only x = x − y will iterate a b times. However, even if b = 1, this is only a iterations. This looks like it is linear time. However, you should express the running time of an algorithm as a function of input size. See Section 23.1. The number of bits needed to represent the in- stance  cid:2 a, b cid:3  is n = log a + log b. Expressed in these terms, the running time is Time n  =  cid:1  a  =  cid:1  2n . This is exponential time. If a = 1,000,000,000,000,000 and b = 1, I would not want to wait for it. Faster Main Steps: One thing to try when faced with exponential running time is to look for a way to speed up the main steps. Instead of subtracting one y from x each iteration, why not speed up the process by subtracting a multiple of y all at once? We could set xnew = x − d · y for some integer value of d. Our goal is to make xnew as small as possible without making it negative. Clearly, d should be  cid:9  x  cid:10  · y = x mod y, which is within the range [0..y − 1] and is the remainder when dividing y into x. For example, 52 mod 10 = 2. Maintaining the Loop Invariant: The step xnew = x mod y maintains the loop invariant because GCD  x, y  = GCD  x mod y, y , e.g., GCD  52, 10  = GCD  2, 10  = 2.   cid:10 . This gives xnew = x −  cid:9  x  y  y  Making Progress: The step xnew = x mod y makes progress by making x smaller only if x mod y is smaller than x. This is only true if x is greater than or equal to y. Suppose that initially this is true because a is greater than b. After one iteration, xnew = x mod y becomes smaller than y. Then the next iteration will do nothing. A solution is to then swap x and y.  New Main Steps: Combining xnew = x mod y with a swap gives the main steps of xnew = y and ynew = x mod y. Maintaining the Loop Invariant: This maintains our original loop invariant be- cause GCD  x, y  = GCD  y, x mod y , e.g., GCD  52, 10  = GCD  10, 2  = 2. It also maintains the new loop invariant that 0 ≤ y ≤ x.  Making Progress: Because ynew = x mod y ∈ [0..y − 1] is smaller than y, we make progress by making y smaller. Special Cases: Setting x = a and y = b does not establish the loop invariant, which says that x is at least y if a is smaller than b. An obvious solution is to initially test   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  Euclid’s GCD Algorithm  March 25, 2008  19:49  for this and to swap x and y if necessary. However, as advised in Section 1.2, it is sometimes fruitful to try tracing out what the algorithm that you have already de- signed would do given such an input. Suppose a = 10 and b = 52. The ﬁrst iteration would set xnew = 52 and ynew = 10 mod 52. This last value is a number within the range [0..51] that is the remainder when dividing 10 by 52. Clearly this is 10. Hence, the code automatically swaps the values by setting xnew = 52 and ynew = 10. Hence, no new code is needed. Similarly, if a and b happen to be negative, the initial iteration will make y positive, and the next will make both x and y positive.  81  Exit Condition: We are making progress by making y smaller. We should stop when y is small enough that we can compute the GCD easily. Let’s try small values of y. Using GCD  x, 1  = 1, the GCD is easy to compute when y = 1; however, we will never get this unless GCD  a, b  = 1. How about GCD  x, 0 ? This turns out to be x, because x divides evenly into both x, and 0. Let’s try an exit condition of y = 0. Termination: We know that the program will eventually stop as follows: ynew = x mod y ∈ [0..y − 1] ensures that each step y gets strictly smaller and does not go negative. Hence, eventually y must be zero. Ending: Formally we prove that  cid:2 loop-invariant  cid:3  &  cid:2 exit-cond  cid:3  & codepost-loop ⇒  cid:2 post-cond cid:3 . We see that  cid:2 loop-invariant cid:3  gives GCD  x, y  = GCD  a, b  and  cid:2 exit- cond  cid:3  gives y = 0. Hence, GCD  a, b  = GCD  x, 0  = x. The ﬁnal code will return the value of x. This establishes the  cid:2 post-cond cid:3  that GCD  a, b  is returned.  Code:  algorithm GCD  a, b   cid:2  pre-cond cid:3 : a and b are integers.  cid:2  post-cond cid:3 : Returns GCD  a, b . begin  int x,y x = a y = b loop cid:2 loop-invariant cid:3 : GCD x,y  = GCD a,b .  if y = 0  exit xnew = y , ynew = x mod y x = xnew y = ynew  end loop return  x   end algorithm   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  March 25, 2008  19:49  Iterative Algorithms and Loop Invariants Example: The following traces the algorithm given two input instances, cid:2 a, b cid:3  =  cid:2 22, 33 cid:3  and  cid:2 a, b cid:3  =  cid:2 1,000,000,005, 999,999,999 cid:3 .  Iteration  Value of x  Value of y  Iteration  Value of x  Value of y  82  22 32 22 10 2  1st 2nd 3rd 4th 5th GCD  22, 32  = 2.  32 22 10 2 0  1,000,000,005 999,999,999 6 3  1st 2nd 3rd 4th GCD  1,000,000,005, 999,999,999 = 3  999,999,999 6 3 0  Running Time: For the running time to be linear in the size of the input, the num- ber of bits  log y  to represent y must decrease by at least one in each iteration. This means that the value of y must decrease by at least a factor of two. Consider the exam- ple of x = 19 and y = 10. Then ynew becomes 19 mod 10 = 9, which is only a decrease of one. However, the next value of y will be 10 mod 9 = 1, which is a huge drop. We will be able to prove that every two iterations, y drops by a factor of 2, namely, that yk+2 < yk  2. There are two cases. In the ﬁrst case, yk+1 ≤ yk  2. Then we are done, because, as stated above, yk+2 < yk+1. In the second case, yk+1 ∈ [yk  2 + 1, yk − 1]. Unwinding the algorithm gives that yk+2 = xk+1 mod yk+1 = yk mod yk+1. One algorithm for computing yk mod yk+1 is to continually subtract yk+1 from yk until the amount is less than yk+1. Because yk is more than yk+1, this yk+1 is subtracted at least once. It follows that yk mod yk+1 ≤ yk − yk+1. By the case, yk+1 > yk  2. In conclusion, yk+2 = yk mod yk+1 ≤ yk − yk+1 < yk  2. We prove that the number of times that the loop iterates is O log min a, b    = O n , as follows. After the ﬁrst or second iteration, y is min a, b . Every iteration y goes down by at least a factor of 2. Hence, after k iterations, yk is at most min a, b  2k, and after O log min a, b    iterations it is at most one.  The algorithm iterates a linear number O n  of times. Each iteration must do a mod operation. Poor Euclid had to compute these by hand, which must have gotten very tedious. A computer may be able to do mods in one operation; however, the number of bit operations needed for two n-bit inputs is O n log n . Hence, the time complexity of this GCD algorithm is O n2 log n .  Lower Bound: We will prove a lower bound, not of the minimum time for any al- gorithm to ﬁnd the GCD, but of this particular algorithm, by ﬁnding a family of in- put values  cid:2 a, b cid:3  for which the program loops  cid:1  log min a, b    times. Unwinding the code gives yk+2 = xk+1 mod yk+1 = yk mod yk+1. As stated, yk mod yk+1 is com- puted by subtracting yk+1 from yk a number of times. We want the y’s to shrink as slowly as possible. Hence, let us say that it is subtracted only once. This gives yk+2 = yk − yk+1 or yk = yk+1 + yk+2. This is the deﬁnition of Fibonacci numbers,   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  March 25, 2008  19:49  Euclid’s GCD Algorithm only backwards, i.e., Fib 0  = 0, Fib 1  = 1, and Fib n  = Fib n−1  + Fib n−2 .  See Exercise 27.2.1.  On input a = Fib n + 1  and b = Fib n , the program iterates n times. This is  cid:1  log min a, b   , because Fib n  = 2 cid:1  n .  EXERCISE 6.0.1  algorithm Converge xoriginal   cid:2  pre-cond cid:3 : xoriginal ∈ [0, . . . , 1].  cid:2  post-cond cid:3 : This algorithm returns the converged value ??? or runs forever  83  begin  x = xoriginal loop cid:2 loop-invariant cid:3 : x ∈ [0, . . . , 1]  exit when  x = f  x   x = f  x   end loop return x  end algorithm  1.0  0.8  0.6  0.4  0.2  f x   –0.2  0.0  0.4  0.2 This is the function f being used.  0.8  0.6  1.0  x  1.2  1. Prove that the algorithm correctly establishes the loop invariant. 2. Prove that the loop invariant is maintained. 3. Fill in the rest of the postcondition by giving as speciﬁcally as possible which value is returned by this algorithm when it does converge. Prove that if the algorithm halts, then this postcondition is met.  4. Change the algorithm so that also has an integer input N and it halts after N iter- ations. What is the running time  time complexity  of this algorithm as a function of the size of the input?  5. Change the algorithm so that it halts after a billion iterations. What is the running  time  time complexity  of this algorithm as a function of the size of the input?  EXERCISE 6.0.2  See solution in Part Five.  The ancient Egyptians and Ethiopians had advanced mathematics. Merely by halving and doubling, they could multiply any two numbers correctly. Say they wanted to buy 15 sheep at 13 Ethiopian dollars each. Here is how they ﬁgured out the product. Put 13 in a left column, 15 on the right. Halve the left value; you get 6 1 2 . Double the right value. Repeat this  keeping all intermediate values  until the left value is 1. What you have is  2 . Ignore the 1  13 6 3 1  15 30 60 120   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  March 25, 2008  19:49  84  Iterative Algorithms and Loop Invariants  Even numbers in the left column are evil and, according to the story, must be destroyed, along with their guilty partners. So scratch out the 6 and its partner 30. Now add the right column, giving 15 + 60 + 120 = 195, which is the correct answer. 1. Write pseudocode that, given two positive integers x and y, follows this procedure and outputs the resulting value. Part of the loop invariant is that the variable  cid:3  holds the current left value, r the current right value, and s the sum of all previous right values that will be included in the ﬁnal answer. Break the algorithm within the loop into two steps. In the ﬁrst step, if  cid:3  is odd, it decreases by one. In the second step  cid:3   now even  is divided by two. These steps must update r and s as needed.  2. Give a meaningful loop invariant relating the current values of  cid:3 , r , s, x, and y.  Hint: Look at the GCD loop invariant.  In addition to this invariant being true every time the computation is at the top of the loop, it will also be true every time the computation is between the ﬁrst and the second step of each iteration. Prove that your algorithm establishes and maintains the loop invariant as stated.  5.  3. Draw pictures to give a geometric explanation for the steps. 4. What is the Ethiopian exit condition? How might you improve on this? How do the exit condition, the loop invariant, and perhaps some extra code establish the postcondition? Suppose that the input instances x and y are each n-bit numbers. How many bit operations are used by your algorithm, as a function of n?  Adding two n -bit numbers requires O n   time.  Suppose the Ethiopians counted with pebbles. How many pebble operations did their algorithm require? How do these times compare? How do these times compare with the high school algorithm for multiplying? How do they compare with laying out a rectangle of x by y pebbles and then counting them?   cid:1    cid:1   6. This algorithm seems very strange. Compare it with using the high school algo-  rithm for multiplying in binary.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  11:37  7 The Loop Invariant for Lower Bounds  85  Time Complexity: The time complexity of a computational problem P is the mini- mum time needed by an algorithm to solve it:   cid:10   ∃A , ∀I, ∀A , ∃I,   cid:10  A I   = P I   and Time A , I   ≤ Tupper  I  A I    cid:11 = P I   or Time A , I   ≥ Tlower I    cid:9   cid:9   Asymptotic Notation: When we want to bound the running time of an algorithm while ignoring multiplicative constants, we use the following notation.  Name  Theta BigOh Omega  Standard Notation f  n  =  cid:1  g n   f  n  = O g n   f  n  =  cid:4  g n    My Notation f  n  ∈  cid:1  g n   f  n  ≤ O g n   f  n  ≥  cid:4  g n    Meaning f  n  ≈ c · g n  f  n  ≤ c · g n  f  n  ≥ c · g n   See Chapter 25.  An Upper Bound Is an Algorithm: An upper bound for P is obtained by con- structing an algorithm A that outputs the correct answer, namely A I   = P I  , within the bounded time, i.e., Time A , I   ≤ Tupper  I , on every input instance I .  A Lower Bound Is an Algorithm: Amusingly enough, a lower bound, proving that there is no faster algorithm for the problem P, is also obtained by constructing an algorithm, but it is an algorithm for a different problem. The input to this problem is an algorithm A claiming to solve P in the required time. The output, as proof that this is false, is an input instance I on which the given algorithm A either does not give the correct answer, namely, A I    cid:11 = P I  , or uses too much time, namely, Time A , I   ≥ Tlower I .  Read the Appendix: To understand this better you may have to read two discus- sions in the appendix  Part Four : Chapter 22 on how to think of statements with   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  11:37  Iterative Algorithms and Loop Invariants  existential and universal quantiﬁers as a game between two players, and Section 23.2 on time complexity.  86  Circular Argument: Proving lower bounds can lead to the following circular argu- ment. Given an arbitrary algorithm A, we must ﬁnd an input instance I for which A gives the wrong answer. The problem is that you do not know for which input in- stance the algorithm will give the wrong answer until you know what the algorithm does. But you do not know what the algorithm does until you give it an input instance and run it. This paradox is avoided by stepping through the computation on A one time step at a time, at each step narrowing the search space for I . This makes your al- gorithm for solving the lower bound problem an iterative algorithm. As such, it needs a loop invariant.  The Loop Invariant Argument:  Knowledge: At each time step, the actions taken by algorithm A depend on the knowledge that it has collected already. For example, if the input is  cid:2 x1, . . . , xn cid:3  and during the ﬁrst time step A tests if x5 < x6, then A can base what it does during the second time step on whether or not x5 < x6, but it does not yet know anything else about the input instance. We deﬁne A’s knowledge, or state, to be determined by the values of its variables  except for the variables storing the in- put instance  and which line of code it is on.  The Loop Invariant: The loop invariant will be a classic narrowing-the-search- space type. It states that we have a set S of input instances on which algorithm A’s knowledge and actions for its ﬁrst t time steps are identical.  Establishing the Loop Invariant: Initially the set S is some large set of instances that we want to focus on. The loop invariant is trivially established for t = 0, be- cause initially the algorithm knows nothing and has done nothing.  Maintaining the Loop Invariant: The loop invariant is maintained as follows. As- sume that it is true at time t − 1. Though we do not know which input instance I from S will ultimately be given to algorithm A, we do know that what A learns during its ﬁrst t − 1 time steps is independent of this choice. A, knowing what it has learned during these ﬁrst t − 1 steps, but unaware that it has not been given a speciﬁc input instance, will then state what action it will do during time step t. What A learns at time t from this action will depend on which instance I ∈ S is given to A. We then partition S based on what A learns and narrow S down to one such part. This maintains the loop invariant, i.e., that we have a set S of input instances on which the algorithm A’s knowledge and actions for its ﬁrst t time steps are identical.  Measure of Progress: The measure of progress for our lower bound algorithm is that S does not get too much smaller.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  11:37  The Loop Invariant for Lower Bounds  Exit Condition: The exit condition is then t = Tlower I .   cid:1    cid:1   Ending: From the loop invariant and the exit condition, we obtain the post- condition by ﬁnding two input instances I and I in S for which the computa- tional problem P requires different outputs: P I    cid:11 = P I  . If, on instance I , al- gorithm A either does not give the correct answer, so that A I    cid:11 = P I  , or uses too much time, so that Time A , I   ≥ Tlower I , then the postcondition is met. Otherwise, we turn our attention to instance I . By the loop invariant and the exit condition, the computation of A is identical on the two instances I and I for the ﬁrst Tlower I  time steps, because both I and I are in S. Hence, their outputs must be identical: A I   = A I  cid:1   . Because A I   = P I  , it follows that A I  . Again we have found an instance on which A does not give the correct answer, and the postcondition is met.   . By our choice of instances, P I    cid:11 = P I  cid:1      cid:11 = P I   cid:1    cid:1    cid:1    cid:1    cid:1   87  EXAMPLE 7.1  Sorting  We have seen a number of algorithms that can sort N numbers using O N log N  com- parisons between the elements, such as merge, quick, and heap sort. We will prove that no algorithm can sort faster.  Information Theory: The lower bounds technique just described does not con- sider the amount of work that must get done to solve the problem, but the amount of information that must be transferred from the input to the output. The problem with these lower bounds is that they are not bigger than linear with respect to the bit size of the input and the output. n =  cid:1  N log N  : At ﬁrst it may appear that this is a superlinear lower bound. However, N is the number of elements in the list. Assuming that the N numbers to be sorted are distinct, each needs  cid:1  log N  bits to be represented, for a total of n =  cid:1  N log N  bits. Hence, the lower bound does not in fact say that more than  cid:1  n  bit operations are required when n is the total number of bits in the input instance.  Deﬁnition of Binary Operation: Before we can prove that no algorithm exists that quickly sorts, we need to ﬁrst be very clear about what an algorithm is and what its running time is. This is referred to as a model of computation. For this sorting lower bound, we will be very generous. We will allow the algorithm to perform any binary operation. This operation can use any information about the input or about what has already been computed by the algorithm, but the result of the operation is restricted to a yes–no answer. For example, as is done in merge sort, it could ask whether the ith element is less than the j th element. For a stranger example, it could ask with one operation whether the number of odd elements is odd.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  11:37  88  Iterative Algorithms and Loop Invariants  Deﬁnition of the Sorting Problem P : The standard sorting problem, given N el- ements, is to output the same N elements in sorted order. To make our life easier, we will deﬁne the problem P to sort pointers to the elements instead of sorting the elements themselves. Note this is often done when elements are too large to move easily. For example, if the input is I =  cid:2 19, 5, 81 cid:3  pointers to these ele- ments are  cid:2 1, 2, 3 cid:3 , the output will be  cid:2 2, 1, 3 cid:3 , because the ﬁrst element in the sorted order  cid:2 5, 19, 81 cid:3  was second in I , the second was the ﬁrst, and the third el-  cid:1  =  cid:2 19, 81, 5 cid:3  will be  cid:2 3, 1, 2 cid:3 . What ement was the third. Similarly, the output for I makes our lives easier in this version of the sorting problem is that the instances I =  cid:2 19, 5, 81 cid:3  and I  cid:1  =  cid:2 19, 81, 5 cid:3  have different outputs, while in the standard problem deﬁnition, they would both have the output  cid:2 5, 19, 81 cid:3 . This change is reasonable because any sorting algorithm needs to learn the order that the ele- ments should be in.  The Initial Set of Instances: Because of the way we modiﬁed the sorting prob- lem, the nature of the elements being sorted does not matter, only their initial order. Hence, we may as well assume that we are sorting the numbers 1 to N. Let the initial set S of input instances being considered consist of every permutation of these numbers. P I    cid:1 = P I the sorting problem P, i.e., P I    cid:11 = P I posed to end by ﬁnding two input instances I and I case.   cid:1  ∈ S have different outputs for  . This is good because our search is sup- in S for which this is the   : Note that each pair of instances I, I   cid:2    cid:1    cid:1   The Measure of Progress: Our measure of progress, as we search for an instance I on which algorithm A does not work, will be the number S of instances still being considered. Initially, because S consists of all permutations of N elements, S = N!. We will prove that each iteration, S does not decrease by more than a factor of 2. Hence, after t iterations, S ≥ N! 2t . By setting Tlower I  to be log2 N!  − 1, we know that in the end we have at least two input instances re- maining to be our I and I Math: In N! = 1 × 2 × 3 × ··· × N, N 2 , and all N of the factors are at most N. Hence, N ! is in the range [N 2N 2, NN]. Hence, log N! is in the range [ N  2 of the factors are at least N  .   cid:1   2 log N  2 , N log N].  Maintaining the Loop Invariant: Assume that the loop invariant is true at time t − 1 and that S is the set of input instances on which algorithm A’s knowledge and actions for its ﬁrst t − 1 time steps are identical. Given this, the action A will perform during time step t is ﬁxed. The model of computation dictates that the result of A’s actions is restricted to a yes–no answer. We then partition S into two sets based on whether this answer on this instance I is yes or no. We simply narrow S down to the part of S that is larger of the two. Restricting the algorithm to learning only this one answer maintains the loop invariant. Clearly, the larger of the two parts has size at least a half.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  The Loop Invariant for Lower Bounds  March 29, 2008  11:37  The Lower Bound: This completes the lower bound that any algorithm requires at least  cid:4  N log N  binary operations to correctly solve the sorting problem.  EXAMPLE 7.2  Binary Search Returning Index  Consider the problem of searching a sorted list of N elements where the output states the index of the key in the list. Binary search solves the problem with log2 N  compar- isons. We will now prove a matching lower bound.  89  The Initial Set of Instances: To follow the same technique that we did for sorting, we need a set of legal input instances each of which has a unique output. Now, however, there are now only N possible outputs. Let the initial set of instances be S = {Ij  j ∈ [1, N]}, where Ij is the input instance searching for the key 5 within the list that has the ﬁrst j − 1 elements zero, the j th element 5, and the last n − j elements 10. The Measure of Progress: Initially, S = N. As before, S does not decrease by more than a factor of 2 at each iteration. Hence, after t iterations, S ≥ N 2t . By setting Tlower I  to be log2 N  − 1, we know that in the end we have at least two input in- stances remaining to be our I and I  .   cid:1   The Lower Bound: The rest of the lower bound is the same, proving that any algorithm requires at least  cid:4  log N  binary operations to correctly solve the problem of searching a sorted list.  You Have to Look at the Data Lower Bounds: The following lower bounds do not really belong in the iterative algorithms part of this book, because in these cases we do not ﬁnd the instances I and I iteratively. However, the basic idea is the same.  cid:1  ≤ N operations are required on an input of These lower bounds say that at least N size N, because you must look at at least N  of the input values.   cid:1    cid:1   EXAMPLE 7.3  Parity  The easiest example is for the problem of computing parity. The input consists of n bits, and the output simply states whether the number of ones is even or odd.  The Information-Theoretic Approach Does Not Work: The information-theoretic approach given above allows the model of computation to charge only one time step for any yes–no operation about the input instance, because it counts only the bits of information learned. However, this does not work for the parity problem. If any yes–no operation about the input instance is allowed, then the algorithm can simply ask for the parity. This solves the problem in one time step.  Reading the Input: Suppose, on the other hand, the model of computation charges one time step for reading a single bit of the input.  We could even give any additional operations for free.  Clearly, an algorithm cannot know the parity of the input un- til it has read all of the bits. This proves the lower bound that any algorithm solving the problem requires at least n time. We will see, however, that there is a bug in this argument.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  11:37  Iterative Algorithms and Loop Invariants  EXAMPLE 7.4  Multiplexer  90  The multiplexer computational problem has two inputs: an n-bit string x, and a log2 n -bit index i, which has the range 1 to n. The output is simply the ith bit of x. As we did for parity, we might give a lower bound of n for this problem as follows: If the algorithm does not read a particular bit of x, then it will give the wrong answer when this bit is the required output. This argument is clearly wrong, because the following is a correct algorithm that has running time log2 n  + 1: It reads i and then learns the answer by reading the ith bit of x.  Dynamic Algorithms: Proving a lower bound based on how many bits need to be read is a little harder, because an algorithm is allowed to change which bits it reads based on what it has read before. Given any single instance, the algorithm might read only n − 1 of the bits, but which bit is not read depends on the input instance.  Fixing One Instance and Flipping a Bit: Before we can know what the algorithm A does, we must give it a speciﬁc input instance I . We must choose one. Then we de- termine the set J ⊆ [1, n] of bits of this instance that are critical, meaning for each j ∈ J , if you ﬂip just the j th bit of I but leave the rest of the instance alone, then the answer to the computation problem on this instance changes. We then obtain a  cid:1  = J  on the time required to solve the problem as follows. We run lower bound of n  cid:1  = J  bits, the algorithm on I and see which bits of this instance it reads. If it reads n then we are done. Otherwise, there is some bit j ∈ J that the algorithm does not read on this instance. Because the algorithm does not read it, we can ﬂip this bit of the in- stance without affecting the answer the algorithm gives. We are not allowed to change any of the bits that the algorithm does read, because not only may this change the answer that it gives, it may also change which bits it reads. We have made sure, how- ever, that the ﬂipping of this single bit changes the answer to the computation ques- tion. Hence, on one of the two input instances, the algorithm must give the wrong answer.   cid:1    cid:1   EXAMPLE 7.3  Parity  We now obtain a formal lower bound of n for the parity problem, as follows. Let I be the all-zero instance. Let J = [1, n] be the set of all bits of the input. For each j ∈ J , changing the j th bit of I changes the answer from even parity to odd parity. Hence, if the algorithm does not read the j th bit when given instance I , it gives the wrong answer either on instance I or on the instance with this bit ﬂipped.  EXAMPLE 7.4  Multiplexer  We have obtained a log2 n  + 1-time algorithm for solving the multiplexer problem. Now we obtain a matching lower bound. Let I be the instance with x = 100,000, i.e.,   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  The Loop Invariant for Lower Bounds  March 29, 2008  11:37  one in its ﬁrst bit and zero in the rest, and with j = 1. Let J consist of the log2 n  bits of j and the ﬁrst bit of x. The output of the multiplexer on I is one, because this is the j th bit of x. But if you ﬂip any bit of j , then a different bit of x is indexed and the answer changes to zero. If you ﬂip the ﬁrst bit of x from being a one, then the answer also changes. Hence, if the algorithm does not read one of these bits when given instance I , it either gives the wrong answer on instance I or does so on the instance with this bit ﬂipped.  91  EXAMPLE 7.2   cid:1   Binary Search Returning Yes or No:  In Example 7.2 we proved a log2 N  lower bound for searching a sorted list of N ele- ments. We are to do the same again. The difference now is that if the key is in the list, the problem returns only the output yes, and if not then no.  The Approach:  The Information-Theoretic Approach Does Not Work: Again informat- ion-theoretic approach does not work, because if any yes–no operation on the input instance is allowed, then the algorithm can simply ask whether the key is in the list, solving the problem in one time step.  the  The Set- J -of-Bits-to-Flip-Approach Does Not Work: The I needs to consist of the key being searched for and some sorted list. Given this, there are not many elements J that can be changed in order to change the output of the searching problem.  instance  initial  Some Combination: Instead, we will use a combination of the two lower bound approaches.  The Initial Set of Instances: As we did when we proved the lower bound for this prob- lem in Example 7.2, we consider the input instance Ij , which is to search for the key 5 within the list with the ﬁrst j − 1 elements zero, the j th element 5, and the last n − j elements 10. Unlike before, however, these instances all have the same outputs: yes.  cid:1  j be the same instance, except the j th el- As in the set-J -of-bits-to-ﬂip approach, let I  cid:1  j have opposite answers. Considering ement is changed to from 5 to 6 so that Ij and I these, let the initial set of instances be S = {Ij  j ∈ [1, n]} ∪ {I   j ∈ [1, n]}.   cid:1  j  The Standard Loop Invariant: As before, the loop invariant states that we have a set S of input instances on which algorithm A’s knowledge and actions for its ﬁrst t time steps are identical.  Another Loop Invariant: We have additional loop invariants stating that the current structure of S is S = {Ij  j ∈ [j 1, j 2]} ∪ {I  j ∈ [j 1, j 2]}, where [j 1, j 2] is a subinterval of the sorted list of size. Moreover, [j 1, j 2] ≥ N 2t .   cid:1  j   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  11:37  92  Iterative Algorithms and Loop Invariants  Maintaining the Loop Invariant: We maintain the loop invariant as follows. Assume that the loop invariant is true for time t − 1. Let m be the index of the element read at time t by the algorithm on all inputs instances in S.  m cid:1 ∈ [ j1, j2]: If the algorithm reads an element m before our subrange [j 1, j 2], then for all instances in S, this mth element is zero. Similarly, if m is after, then this element is deﬁnitely 10. In either case, the algorithm learns nothing that has not already been ﬁxed. The loop invariant is maintained trivially without changing anything.  m∈ [ j1, jmid]: Let [j 1, jmid] and [jmid + 1, j 2] split our subrange in half. If m is in the ﬁrst half [j 1, jmid], then we set our new subinterval to be the second half [jmid + 1, j 2]. This narrows our set of instances down to S = {Ij  j ∈ [jmid + 1, j 2]} ∪ {I  j ∈ [jmid + 1, j 2]}. For all instances in this new S, the mth element is zero. The algorithm reads and learns the value zero and proceeds. The loop invari- ant is maintained.  m∈ [ jmid + 1, j2]: If m is in the second half [jmid + 1, j 2], then we set our new   cid:1  j  subinterval to be the ﬁrst half, and for all instances in the new S, the mth element is 10.  Ending: The exit condition is then t = Tlower I  = log2 N . From the loop invariant, when we exit our subinterval [j 1, j 2], its size is at least N 2t = 1. Let j = j 1 = j 2. Our set  cid:1  S still contains the two instances Ij and I j . By the deﬁnition of these instances, the ﬁrst requires the answer P Ij   = yes and the second P I j   = no. From the loop invariant,  cid:1  the computation of A is identical on these instances for the ﬁrst Tlower I  time steps, and hence their outputs must be identical: A Ij   = A I  cid:1  j  . Hence, the computation must give a wrong answer on at least one of them.  The Lower Bound: The rest of the lower bound is the same, proving that any algorithm requires at least  cid:4  log N  binary operations to correctly solve the problem of searching a sorted list.  Current State of the Art in Proving Lower Bounds: Lower bounds are hard to prove, because you must consider every algorithm, no matter how strange or com- plex. After all, there are examples of algorithms that start out doing very strange things and then in the end magically produce the required output.  Information Theory: The technique used here to prove lower bounds does not consider the amount of work that must get done to solve the problem, but the amount of information that must be transferred from the input to the output. The problem with these lower bounds is that they are not bigger than linear with respect to the bit size of the input and the output.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  The Loop Invariant for Lower Bounds  March 29, 2008  11:37  Restricted Model: A common method of proving lower bounds is to consider only algorithms that have a restricted structure. My PhD thesis proved lower bounds on the tradeoffs between the time and space needed to check  s − t - connectivity of a graph in a model that only allows pebbles to slide along edges and jump between each other.  General Model: The theory community is just now managing to prove the ﬁrst nonlinear lower bounds on a general model of computation. This is quite excit- ing for those of us in the ﬁeld.  93  EXERCISE 7.0.1 How would the lower bound change if a single operation, instead of being only a yes–no question, could be a question with at most r different answers? Here r is some ﬁxed parameter.  EXERCISE 7.0.2  See solution in Part Five.  Recall the Magic Sevens card trick intro- duced in Section 4.2. Someone selects one of n cards, and the magician must determine what it is by asking questions. Each round, the magician rearranges the cards into rows and asks which of the r rows the card is in. Give an information-theoretic argument to prove a lower bound on the number of rounds, t, that are needed.  EXERCISE 7.0.3  See solution in Part Five.  Suppose that you have n objects that are completely identical except that one is slightly heavier. The problem P is to ﬁnd the heavier object. You have a scale. A single operation consists of placing any two disjoint sets of the objects the two sides of the scale. If one side is heavier, then the scale tips over. Give matching upper and lower bounds for this problem.  EXERCISE 7.0.4 Communication complexity: Consider the following problem: Alice has some object from the M objects {I1, . . . , IM}, and she must communicate which object she has to Bob by sending a string of bits. The string sent will be an identiﬁer for the object. The goal is to assign each object a unique identiﬁer so that the longest one has as few bits as possible.  EXERCISE 7.0.5 State and prove a lower bound when instead of bits Alice can send Bob letters from some ﬁxed alphabet  cid:5 .  EXERCISE 7.0.6  See solution in Part Five.  The AND computational problem given n bits determines whether at least one of the bits is a one. This is the same as the game show problem mentioned in Chapter 21, which requires ﬁnding which of the n doors conceals a prize. The way this differs from the parity problem is that the algorithm can stop as soon as it ﬁnds a prize. Give a tight lower bound for this problem. In the lower bound for the parity problem, which initial instances I work? Which ones work for the AND problem?  EXERCISE 7.0.7 Search a sorted matrix: The input consists of a real number x and a matrix A[1..n, 1..m] of nm real numbers such that each row A[i, 1..n] is sorted and each   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes3 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  11:37  Iterative Algorithms and Loop Invariants  column A[1..n, j ] is sorted. The goal is to ﬁnd the maximum array entry A[i, j ] that is less than or equal to x, or report that all elements of A are larger than x. 1. Exercise 4.4.1 gives an iterative algorithm that accesses T n, n  = m + n − 1 = 2n − 1 entries when n = m. Prove a matching lower bound of T n, n  = 2n − 1 for this case.  Start with a lower bound of n if you like.  ments when m  cid:19  n. Prove a matching lower bound.  2. Exercise 9.1.3 gives a recursive algorithm that accesses T n, m  = n log2  m  n   ele-  94  EXERCISE 7.0.8 Consider the problem of determining the smallest element in a max heap. The smallest elements of a max heap must be one of the  cid:7 n 2 cid:8  leaves.  Otherwise, there must be a nonleaf that is smaller than one of its descendants, which means the tree is not a max heap.  Thus, it is sufﬁcient to search all leaves. Prove a lower bound that searching all the leaves is necessary.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes8 CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:59  PART TWO  Recursion  95   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes8 CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:59  96   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes8 CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:59  8 Abstractions, Techniques, and Theory  97  Iterative algorithms start at the beginning and take one step at a time towards the ﬁnal destination. Another technique used in many algorithms is to slice the given task into a number of disjoint pieces, solve each of these separately, and then combine these answers into an answer for the original task. This is the divide-and-conquer method. When the subtasks are different, it leads to different subroutines. When they are instances of the original problem, it leads to recursive algorithms.  People often ﬁnd recursive algorithms very difﬁcult. To understand them, it is important to have a good solid understanding of the theory and techniques pre- sented in this chapter.  8.1  Thinking about Recursion  There are a number of ways to view a recursive algorithm. Though the resulting algo- rithm is the same, having the different paradigms at your disposal can be helpful.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes8 CUUS154-Edmonds 978 0 521 84931 9  Recursion  March 28, 2008  21:59  Code: Code is useful for implementing an algorithm on a computer. It is precise and succinct. However, code is prone to bugs, is language-dependent, and often lacks higher levels of intuition.  98  Stack of Stack Frames: Recursive algorithms are executed using a stack of stack frames. See Section 8.6. Though this should be understood, tracing out such an exe- cution is painful.  Tree of Stack Frames: This is a useful way of viewing the entire computation at once. It is particularly useful when computing the running time of the algorithm. However, the structure of the computation tree may be very complex and difﬁcult to understand all at once.  Friends, on Strong Induction: The easiest method is to focus on one step at a time. Suppose that someone gives you an instance of the computational problem. You solve it as follows. If it is sufﬁciently small, solve it your- self. Otherwise, you have a number of friends to help you. You construct for each friend an instance of the same computational problem that is smaller then your own. We refer to these as subinstances. Your friends magically provide you with the solutions to these. You then combine these subsolutions into a solution for your original instance.  I refer to this as the friends level of abstraction. If you prefer, you can call it the strong induction level of abstraction and use the word “recursion” instead of “friend.” Either way, the key is that you concern yourself only about your task. Do not worry about how your friends solve the subinstances that you assigned them. Similarly, do not worry about whoever gave you your instance and what he does with your answer. Leave these things up to him. Trust your friends.  Use It: I strongly recommend using this method when designing, understand- ing, and describing a recursive algorithm.  Faith in the Method: As with the loop invariant method, you do not want to be rethinking the issue of whether or not you should steal every time you walk into a store. It is better to have some general principles with which to work. You do not want to be rethinking the issue of whether or not you believe in recursion every time you consider a hard algorithm. Understanding the algorithm itself will be hard enough. While reading this chapter you should once and for all come to understand and believe how the following steps are sufﬁcient to describing a recursive algorithm. Doing this can be difﬁcult. It requires a whole new way of looking at algorithms. However, at least for now, adopt this as something that you believe in.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes8 CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:59  Abstractions, Techniques, and Theory  8.2  Looking Forward vs. Backward  Circular Argument? Recursion involves designing an algorithm by using it as if it already exists. At ﬁrst this looks paradoxical. Suppose, for example, the key to the house that you want to get into is in that same house. If you could get in, you could get the key. Then you could open the door, so that you could get in. This is a circular argument. It is not a legal recursive program because the subinstance is not smaller.  99  One Problem and a Row of Instances: Consider a row of houses. Each house is bigger than the next. Your task is to get into the biggest one. You are locked out of all the houses. The key to each house is locked in the house of the next smaller size. The recursive problem consists in getting into any speciﬁed house. Each house in the row is a separate instance of this problem.  To get into my house  I must get the key from a smaller house  The Algorithm: The smallest house is small enough that one can use brute force to get in. For example, one could simply lift off the roof. Once in this house, we can get the key to the next house, which is then easily opened. Within this house, we can get the key to the house after that, and so on. Eventually, we are in the largest house as required.  Focus on One Step: Though this algorithm is quite simple to understand, more complex algorithms are harder to understand all at once. Instead we focus on one step at a time. Here, one step consists in opening house i. We ask a friend to open house i − 1, out of which we take the key with which we open house i. We do not worry about how to open house i − 1.  Working Forward vs. Backward: An iterative algorithm works forward. It knows about house i − 1. It uses a loop invariant to show that this house has been opened. It searches this house and learns that the key within it is that for house i. Because of this, it decides that house i would be a good one to go to next. A recursion algorithm works backward. It knows about house i. It wants to get it open. It determines that the key for house i is contained in house i − 1. Hence, opening house i − 1 is a subtask that needs to be accomplished.  There are two advantages of recursive algorithms over iterative ones. The ﬁrst is that sometimes it is easier to work backward than forward. The second is that a recursive algorithm is allowed to have more than one subtask to be solved. This forms a tree of houses to open instead of a row of houses.  Do Not Trace: When designing a recursive algorithm it is tempting to trace out the entire computation. “I must open house n, so I must open house n − 1, . . . . The smallest house I rip the roof off. I get the key for house 1 and open it. I get the key   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes8 CUUS154-Edmonds 978 0 521 84931 9  Recursion  March 28, 2008  21:59  for house 2 and open it. . . . I get the key for house n and open it.” Such an explanation is complicated and unnecessary.  Solving Only Your Instance: An important quality of any leader is knowing how to delegate. Your job is to open house i. Delegate to a friend the task of opening house i − 1. Trust him, and leave the responsibility to him.  100  8.3 With a Little Help from Your Friends  The following are the steps to follow when developing a recursive algorithm within the friends level of abstraction.  Speciﬁcations: Carefully write the speciﬁcations for the problem.  Preconditions: The preconditions state any assumptions that must be true about the input instance for the algorithm to operate correctly.  Postconditions: The postconditions are statements about the output that must be true when the algorithm returns.  This step is even more important for recursive algorithms than for other algorithms, because there must be tight agreement between what is expected from you in terms of pre- and postconditions and what is expected from your friends.  Size: Devise a measure of the size of each instance. This measure can be anything you like and corresponds to the measure of progress within the loop invariant level of abstraction.  General Input: Consider a large and general instance of the problem.  Magic: Assume that by magic a friend is able to provide the solution to any in- stance of your problem as long as the instance is strictly smaller than the current instance  according to your measure of size . More speciﬁcally, if the instance that you give the friend meets the stated preconditions, then her solution will meet the stated postconditions. Do not, however, expect your friend to accom- plish more than this.  In reality, the friend is simply a mirror image of yourself.   Subinstances: From the original instance, construct one or more subinstances, which are smaller instances of the same problem. Be sure that the preconditions are met for these smaller instances. Do not refer to these as “subproblems.” The problem does not change, just the input instance to the problem.  Subsolutions: Ask your friend to  recursively  provide solutions for each of these subinstances. We refer to these as subsolutions even though it is not the solution, but the instance, that is smaller.  Solution: Combine these subsolutions into a solution for the original instance.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes8 CUUS154-Edmonds 978 0 521 84931 9  Abstractions, Techniques, and Theory  March 28, 2008  21:59  Generalizing the Problem: Sometimes a subinstance you would like your friend to solve is not a legal instance according to the preconditions. In such a case, start over, redeﬁning the preconditions in order to allow such instances. Note, however, that now you too must be able to handle these extra instances. Similarly, the solution provided by your friend may not provide enough information about the subinstance for you to be able to solve the original problem. In such a case, start over, redeﬁning the postcondition by increasing the amount of information that your friend provides. Again, you must now also provide this extra information. See Section 10.3.  101  Natural Pre- and Postconditions: On the other hand, have the more generalized problem still be a natural problem. Do not attempt to pass it lots of extra infor- mation about your instance. For the very ﬁrst call  or stack frame  of the com- putation to pass a value through the chain of recursive calls is a type of global- variable “cheat.” It also makes it look like you are micromanaging your friends. Similarly, a stack frame  friend  should not know what level of recursion it is on.  Both the Pre- and the Postconditions Act as Loop Invariants: The loop invariant in an iterative algorithm states what is maintained as the control gets passed from iteration to iteration. It provides a picture of what you want to be true in the middle of this computation. With recursion, however, there are two direc- tions. The precondition states what you want to be true halfway down the recur- sion tree. The postcondition states what you want to be true halfway back up the recursion tree.  Minimizing the Number of Cases: You must ensure that the algorithm that you develop works for every valid input instance. To achieve this, the algorithm will often require many separate pieces of code to handle inputs of different types. Ideally, the algorithm developed has as few such cases as possible. One way to help you min- imize the number of cases needed is as follows. Initially, consider an instance that is as large and as general as possible. If there are a number of different types of in- stances, choose one whose type is as general as possible. Design an algorithm that works for this instance. Afterwards, if there is another type of instance that you have not yet considered, consider a general instance of this type. Before designing a sepa- rate algorithm for this new instance, try executing your existing algorithm on it. You may be surprised to ﬁnd that it works. If, on the other hand, it fails to work for this instance, then repeat the above steps to develop a separate algorithm for this case. You may need to repeat this process a number of times.  For example, suppose that the input consists of a binary tree. You may well ﬁnd that the algorithm designed for a tree with a full left child and a full right child also works for a tree with a missing child and even for a child consisting of only a single node. The only remaining case may be the empty tree.  Base Cases: When all the remaining unsolved instances are sufﬁciently small, solve them in a brute force way.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes8 CUUS154-Edmonds 978 0 521 84931 9  Recursion  March 28, 2008  21:59  Running Time: Use a recurrence relation or a tree of stack frames to estimate the running time.  A Link to the Techniques for Iterative Algorithms: The techniques that often arise in iterative algorithms also arise in recursive algorithms, though sometimes in a slightly different form.  102  More of the Input: When the input includes n objects, this technique for itera- tive algorithms extends  for i = 1, . . . , n − 1  a solution for the ﬁrst i − 1 objects into a solution for the ﬁrst i. This same technique also can be used for recursive algorithms. Your friend provides you a solution for the ﬁrst n − 1 objects in your instance, and then you extend this to a solution to your entire instance. This it- erative algorithm and this recursive algorithm would be two implementations of the same algorithm. The recursion is more interesting when one friend can pro- vide you a solution for the ﬁrst  cid:2  n  cid:3  objects in your instance, another friend can provide a solution for the next  cid:4  n  cid:5  objects, and you combine them into a solution for the whole.  2  2  More of the Output: This technique for iterative algorithms builds the output one piece at a time. Again a recursive algorithm could have a friend build all but the last piece and have you add the last piece. However, it is better to have one friend build the ﬁrst half of the output, another the second half, and you combine them somehow.  Narrowing the Search Space: Some iterative algorithms repeatedly narrow the search space in which to look for something. Instead, a recursive algorithm may split the search space in half and have a friend search each half.  Case Analysis: Instead of trying each of the cases oneself, one could give one case to each friend.  Work Done: Work does not accumulate in recursive algorithms as it does in it- erative algorithms. We get each friend to do some work, and then we do some work, ourselves to combine these solution.  8.4  The Towers of Hanoi  The towers of Hanoi is a classic puzzle for which the only possible way of solving it is to think recursively.  Speciﬁcation: The puzzle consists of three poles and a stack of N disks of different sizes.  Precondition: All the disks are on the ﬁrst of the three poles.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes8 CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:59  Abstractions, Techniques, and Theory  Figure 8.1: The towers of Hanoi problem.  Postcondition: The goal is to move the stack over to the last pole. See the ﬁrst and the last parts of Figure 8.1.  103  You are only allowed to take one disk from the top of the stack on one pole and place it on the top of the stack on another pole. Another rule is that no disk can be placed on top of a smaller disk.  Lost with First Step: The ﬁrst step must be to move the smallest disk. But it is by no means clear whether to move it to the middle or to the last pole.  Divide: Jump into the middle of the computation. One thing that is clear is that at some point, you must move the biggest disk from the ﬁrst pole to the last. In order to do this, there can be no other disks on either the ﬁrst or the last pole. Hence, all the other disks need to be stacked on the middle pole. See the second and the third parts of Figure 8.1. This point in the computation splits the problem into two subproblems that must be solved. The ﬁrst is how to move all the disks except the largest from the ﬁrst pole to the middle. See the ﬁrst and second parts of Figure 8.1. The second is how to move these same disks from the middle pole to the last. See the third and fourth parts of Figure 8.1.  Conquer: Together these steps solve the entire problem. Starting with all disks on the ﬁrst pole, somehow move all but the largest to the second pole. Then, in one step, move the largest from the ﬁrst to the third pole. Finally, somehow move all but the largest from the second to the third pole.  Magic: In order to make a clear separation between task of solving the entire prob- lem and that of solving each of the subproblems, I like to say that we delegate to one friend the task of solving one of the subproblems and delegate to another friend the other.  More General Speciﬁcation: The subproblem of moving all but the largest disk from the ﬁrst to the middle pole is very similar to original towers of Hanoi problem. However, it is an instance of a slightly more general problem, because not all of the disks are moved. To include this as an instance of our problem, we generalize the problem as follows.  Precondition: The input speciﬁes the number n of disks to be moved and the roles of the three poles. These three roles for poles are polesource, poledestination, and polespare. The precondition requires that the smallest n disks be currently on polesource. It does not care where the larger disks are.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes8 CUUS154-Edmonds 978 0 521 84931 9  Recursion  March 28, 2008  21:59  Postcondition: The goal is to move these smallest n disks to poledestination. Pole polespare is available to be used temporarily. The larger disks are not moved.  104  Subinstance: Our task is to move all the disks from the ﬁrst to the last pole. This is speciﬁed by giving n = N, polesource = ﬁrst, poledestination = last, and polespare = middle. We will get one friend to move all but the largest disk from the ﬁrst to the middle pole. This is speciﬁed by giving n = N − 1, polesource = fir st, poledestination = middle, and polespare = last. On our own, we move the largest disk from the ﬁrst to the last disk. Finally, we will get another friend to move all but the largest disk from the middle to the last pole. This is speciﬁed by giving n = N − 1, polesource = middle, poledestination = last, and polespare = ﬁrst.  Code:  algorithm TowersOfHanoi n, source, destination, spare   cid:6  pre-cond cid:7 : The n smallest disks are on polesource.  cid:6  post-cond cid:7 : They are moved to poledestination. begin  if n ≤ 0   else  end if  end algorithm  Nothing to do TowersOfHanoi n − 1, source, spare, destination  Move the nth disk from polesource to poledestination. TowersOfHanoi n − 1, spare, destination, source   Running Time: Let T n  be the time to move n disks. Clearly, T 1  = 1 and T n  = 2 · T n − 1  + 1. Solving this gives T n  = 2n − 1.  8.5  Checklist for Recursive Algorithms  Writing a recursive algorithm is surprisingly hard when you are ﬁrst starting out and surprisingly easy when you get it. This section contains a list of things to think about to make sure that you do not make any of the common mistakes.  0  The Code Structure: The code does not need to be much more complex than the following.  algorithm Alg a, b, c   cid:6  pre-cond cid:7 : Here a is a tuple, b an integer, and c a binary tree.  cid:6  post-cond cid:7 : Outputs x, y, and z, which are useful objects.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes8 CUUS154-Edmonds 978 0 521 84931 9  Abstractions, Techniques, and Theory  March 28, 2008  21:59  105  begin  if   cid:6 a, b, c cid:7  is a sufﬁciently small instance  return   cid:6 0, 0, 0 cid:7     cid:6 asub1, bsub1, csub1 cid:7  = a part of  cid:6 a, b, c cid:7   cid:6 xsub1, ysub1, zsub1 cid:7  = Alg  cid:6 asub1, bsub1, csub1 cid:7    cid:6 asub2, bsub2, csub2 cid:7  = a different part of  cid:6 a, b, c cid:7   cid:6 xsub2, ysub2, zsub2 cid:7  = Alg  cid:6 asub2, bsub2, csub2 cid:7    cid:6 x, y, z cid:7  = combine  cid:6 xsub1, ysub1, zsub1 cid:7  and  cid:6 xsub2, ysub2, zsub2 cid:7  return   cid:6 x, y, z cid:7     end algorithm  1  Speciﬁcations: You must clearly deﬁne what the algorithm is supposed to do.  2  Variables: A great deal is understood about an algorithm by understanding its variables. As in any algorithm, you want variables to be well documented and to have meaningful names. It is also important to carefully check that you give variables val- ues of the correct type, e.g., k is an integer, G is a graph, and so on. Moreover, with recursive programs there are variables that play speciﬁc roles and should be used in speciﬁc ways. This can be a source of many confusions and mistakes. Hence, I outline these carefully here.  2.1.  Your Input: Your mission, if you are to accept it, is received through your inputs. The ﬁrst line of your code, algorithm Alg a, b, c , speciﬁes both the name Alg of the routine and the names of its inputs. Here  cid:6 a, b, c cid:7  is the input in- stance that you need to ﬁnd a solution for. I sometimes use Alg  cid:6 a, b, c cid:7   be- cause it emphasizes the viewpoint that we are receiving one instance, even if that instance might be composed of a tuple of things. Your preconditions must clearly specify what each of these components a, b, and c are and any restric- tions on their values. You must be able to handle any instance that meets these conditions. 2.2.  Your Output: You must return a solution  cid:6 x, y, z cid:7  to your instance  cid:6 a, b, c cid:7  through a return statement return  cid:6 x, y, z cid:7  . Your postconditions must clearly specify what each of the components x, y, and z of your solution are and their required relation to the input instance  cid:6 a, b, c cid:7 .  2.2.1.  Every Path: Given any instance meeting the precondition, you must return a correct solution. Hence, if your code has if or loop statements, then every path through the code must end with a return statement. 2.2.2.  Type of Output: Each return statement must return a solution  cid:6 x, y, z cid:7  of the right type. The one partial exception to this is: if the postcondi- tion leaves open the possibility that a solution does not exist, then some   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes8 CUUS154-Edmonds 978 0 521 84931 9  Recursion  March 28, 2008  21:59  paths through the code may end with the statement return “no solution exists” .  106  2.3.  Your Friend’s Input: To get help from friends, you must create a subin- stance  cid:6 asub, bsub, csub cid:7  for each friend. You pass this to a friend by recursing with Alg  cid:6 asub, bsub, csub cid:7  . To be able to give a subinstance to a friend, it needs to meet the preconditions of your problem. Do not recurse with Alg asub, bsub .  2.4.  Your Friend’s Output: You can trust that each friend will give you a correct solution  cid:6 xsub, ysub, zsub cid:7  to the subinstance  cid:6 asub, bsub, csub cid:7  that you give her. Be sure to save her result in variables of the correct type, using the code  cid:6 xsub, ysub, zsub cid:7  = Alg  cid:6 asub, bsub, csub cid:7  . In contrast, the code Alg  cid:6 asub, bsub, csub cid:7   as a line by itself is insulting to your friend, because you got her to do all of this work and then you dropped her result in the garbage.  2.5.  Rarely Need New Inputs or Outputs: I did speak of the need to generalize the problem by adding new inputs and or outputs. This, however, is needed far less often than people think. Try hard to solve the problem using the friend anal- ogy without extra variables. Only add them if absolutely necessary. If you do add extra inputs or outputs, clearly specify in the pre- and postconditions what they are for. Do not have inputs or outputs that are not explained.  2.6.  No Global Variables or Global Effects: When you recurse with the line  cid:6 xsub, ysub, zsub cid:7  = Alg  cid:6 asub, bsub, csub cid:7  , the only thing that should happen is that your friend passes back a correct solution  cid:6 xsub, ysub, zsub cid:7  to the subinstance  cid:6 asub, bsub, csub cid:7  that you gave her. If the code has a local variable n, then your variable n is completely different than your friend’s.  They are stored in different stack frames. See Section 8.6.  If you set your variable n to 5 and then recurse, your friend’s variable n will not have this value. If you want him to have a 5, you must pass it in as part of his subinstance  cid:6 asub, 5, csub cid:7 . Similarly, if your friend sets his variable n to 6 and then returns, your variable n will not have this value, but will still have the value 5. If you want him to give you a 6, he must return it as part of his solution  cid:6 xsub, 6, zsub cid:7 .  I often suspect that people intend for a parameter in their algorithm’s argu- ments to both pass a value in and pass a value out. Though I know there are pro- gramming languages that allow this, I strongly recommend not doing this. The code  cid:6 xsub, ysub, zsub cid:7  = Alg  cid:6 asub, bsub, csub cid:7   does not change the values of asub, bsub, or csub. I have seen lots of code that loops n times recursing Alg  cid:6 asub, bsub, csub cid:7   on the exact same subinstance  cid:6 asub, bsub, csub cid:7 . One deﬁnition of insanity is repeat- ing the same thing over and over and expecting to get a different result. Your friend on the same subinstance will give you the same solution. Do not waste her time.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes8 CUUS154-Edmonds 978 0 521 84931 9  Abstractions, Techniques, and Theory  March 28, 2008  21:59  It is tempting to use a global variable that everyone has access too. However, this is very bad form, mainly because it is very prone to errors and side effects that you did not expect.  Similarly, you can have no global returns. For example, suppose your friend’s friend’s friend’s friend ﬁnds something that you are looking for. It needs to be passed back friend to friend, because things returned by your friends do not get returned to your boss unless you do the returning.  107  2.7.  Few Local Variables: An iterative algorithm consists of a big loop with a set of local variables holding the current state. Each iteration these variables get up- dated. Because thinking iteratively comes more naturally to people, they want to do this with recursive algorithms. Don’t. Generally there is no need for a loop in a recursive algorithm unless you require the immediate help of many friends and you loop through them, creating subinstances for each and considering their subsolutions. In fact, despite the name “variable,” rarely is there a need to change the value of a variable once initially set. For example, the variables  cid:6 a, b, c cid:7  storing your instance are sacred. This is the instance you must solve. Why ever change it? You must construct a solution  cid:6 x, y, z cid:7 . Create it and return it. Why ever change it? Similarly for what you give  cid:6 asub, bsub, csub cid:7  and receive  cid:6 xsub, ysub, zsub cid:7  from each friend. Other local variables are rarely needed. If you do need them, be sure to document what they are for.  3  Tasks to Complete: Your mission, given an arbitrary instance  cid:6 a, b, c cid:7  meeting the preconditions, is to construct and return a solution  cid:6 x, y, z cid:7  that meets the post- condition. The following are the only steps that you should be following towards this goal.  3.1.  Accept Your Mission: Imagine that you have an  cid:6 a, b, c cid:7  meeting the pre- conditions. Know the range of things that your instance might be. For example, if the input instance is a binary tree, make sure that your program works for a general tree with big left and right subtrees, a tree with big left and empty right, a tree with empty left and big right, and the empty tree. Also know what is require of your output.  3.2.  Construct Subinstances: For each friend, construct from your instance  cid:6 a, b, c cid:7  a subinstance  cid:6 asub, bsub, csub cid:7  to give this friend. Sometimes this re- quires a block of code. Sometimes it happens right in place. For example, if your instance is  cid:6  cid:6 a1, a2, . . . , an cid:7 , b, c cid:7 , you might construct the subinstance  cid:6  cid:6 a1, a2, . . . , an−1 cid:7 , b − 5, leftSub c  cid:7  for your friend by stripping the last object off the tuple a, subtracting 5 from the integer b, and taking the left subtree of the tree c. This subinstance can be constructed and passed to your friend in the one line   cid:6 xsub, ysub, zsub cid:7  = Alg  cid:6  cid:6 a1, a2, . . . , an−1 cid:7 , b − 5, leftSub c  cid:7     March 28, 2008  21:59  P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes8 CUUS154-Edmonds 978 0 521 84931 9  Recursion  108  3.2.1.  Valid Subinstance: Be sure that the subinstance  cid:6 asub, bsub, csub cid:7  that you give your friend meets the preconditions. 3.2.2.  Smaller Subinstance: Be sure that the subinstance  cid:6 asub, bsub, csub cid:7  that you give your friend is smaller in some way than your own subinstance  cid:6 a, b, c cid:7 .  3.3.  Trust Your Friend: Focus on only your mission. Trust your friend to give you a correct solution  cid:6 xsub, ysub, zsub cid:7  to the instance  cid:6 asub, bsub, csub cid:7  that you give her. Do not worry about how she gets her answer. Do not trace through the entire computation. Do not talk of your friends’ friends’ friends. I cannot emphasize this enough. Time and time again, I see students not trusting. It causes them no end of trouble until they ﬁnally see the light and let go. 3.4.  Construct Your Solution: Using the solutions  cid:6 xsub, ysub, zsub cid:7  provided by your friends for your subinstances  cid:6 asub, bsub, csub cid:7 , your next task is to con- struct a solution  cid:6 x, y, z cid:7  for your subinstance  cid:6 a, b, c cid:7 . This generally requires a block of code, but sometimes it can be contained in a single line. For ex- ample, if the only output is a single integer x, then the one line of code return Alg asub1, bsub1, csub1  + Alg asub2, bsub2, csub2   combines the friends’ solu- tions xsub1 and xsub2 to give your solution x = xsub1 + xsub2 and returns it. 3.5.  Base Cases: Consider which instance get solved by your program. For those that don’t, either add more cases to solve them recursively or add base cases to solve them in a brute force way. If your input instance is sufﬁciently small according to your deﬁnition of size then you must solve it yourself as a base case.  This is all that you need to do. Do not do more.  EXERCISE 8.5.1 You are now the professor. Which of the above steps to develop a recursive algorithm did the students fail to do correctly in the following code? How? How would you ﬁx it? See Exercise 10.3.1 for corrrect code for this problem.  algorithm Smallest tree, k, num, v   cid:6  pre-cond cid:7 : tree is a binary search tree and k > 0 is an integer.  cid:6  post-cond cid:7 : Outputs the kth smallest element s. begin  if  k = 0   return  0   if  v = k   return  element   n = 0 Smallest leftSub tree   + + n   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes8 CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:59  109  Abstractions, Techniques, and Theory  if  n = k    return  root tree     end if Smallest rightSub tree   end if  end algorithm  algorithm Smallest tree, k, num, v   cid:6  pre-cond cid:7 : tree is a binary search tree and k > 0 is an integer.  cid:6  post-cond cid:7 : Outputs the kth smallest element s. begin  n = 0 while  n < k    Smallest leftSub tree   + + n Smallest rightSub tree    end while return  element    end algorithm  EXERCISE 8.5.2  See solution in Part Five.  In the friends level of abstracting recursion, you can give your friend any legal instance that is smaller than yours according to some measure as long as you solve in your own any instance that is sufﬁciently small. For which of these algorithms has this been done? If so, what is your measure of the size of the instance? On input instance  cid:6 n, m cid:7 , either bound the depth to which the algorithm recurses as a function of n and m, or prove that there is at least one path down the recursion tree that is inﬁnite.  algorithm Ra  n, m   cid:6  pre-cond cid:7 : n & m ints.  cid:6  post-cond cid:7 : Say Hi begin  if n ≤ 0   Print "Hi"  Ra  n − 1, 2m   else  end if  end algorithm  algorithm Rb n, m   cid:6  pre-cond cid:7 : n & m ints.  cid:6  post-cond cid:7 : Say Hi begin  if n ≤ 0   else  Print "Hi"  Rb n − 1, m  Rb n, m − 1   end if  end algorithm   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes8 CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:59  110  Recursion  algorithm Rc n, m   cid:6  pre-cond cid:7 : n & m ints.  cid:6  post-cond cid:7 : Say Hi begin if n ≤ 0 or m ≤ 0   else  Print "Hi"  Rc n − 1, m  Rc n, m − 1   end if  end algorithm  8.6  The Stack Frame   d  Replace recursive lines with  Rd n − 1, m + 2  Re n + 6, m − 3    e  Replace recursive lines with  Re n − 4, m + 2  Re n + 6, m − 3   Tree of Stack Frames: Tracing out the entire computation of a recursive algorithm, one line of code at a time, can get incredibly complex. This is why the friends level of abstraction, which considers one stack frame at a time, is the best way to understand, explain, and design a recursive algorithm. However, it is also useful to have some pic- ture of the entire computation. For this, the tree-of-stack-frames level of abstraction is best.  The key thing to understand is the difference between a particular routine and a particular execution of a routine on a particular input instance. A single routine can at one moment in time have many executions going on. Each such execution is re- ferred to as a stack frame. You can think of each as the task given to a separate friend. Even though each friend may be executing exactly the same routine, each execution may currently be on a different line of code and have different values for the local variables.  If each routine makes a number of subroutine calls  recursive or not , then the stack frames that get executed form a tree. In the example in Figure 8.2, instance A is called ﬁrst. It executes for a while and at some point recursively calls B. When B returns, A then executes for a while longer before calling H. When H returns, A exe- cutes for a while before completing. We have skipped over the details of the execution of B. Let’s go back to when instance A calls B. Then B calls C, which calls D. D com- pletes; then C calls E. After E, C completes. Then B calls F , which calls G. Then G completes, F completes, B completes, and A goes on to call H. It does get compli- cated.  Stack of Stack Frames: The algorithm is actually implemented on a computer by a stack of stack frames. What is stored in the computer memory at any given point in time is only a single path down the tree. The tree represents what occurs throughout time. In Figure 8.2, when instance G is active, A, B, F , and G are in the stack. C, D, and E have been removed from memory as these have completed. H, I , J , and K have   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes8 CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:59  111  Abstractions, Techniques, and Theory  A  H  J  C  F  I  K  B  E  D  G  Figure 8.2: Tree of stack frames.  not been started yet. Although we speak of many separate stack frames executing on the computer, the computer is not a parallel machine. Only the top stack frame G is actively being executed. The other instances are on hold, waiting for the return of a subroutine call that it made.  Memory: Here is how memory is managed for the simultaneous execution of many instances of the same routine. The routine itself is described only once, by a block of code that appears in static memory. This code declares a set of variables. On the other hand, each instance of this routine that is currently being executed may be storing different values in these variables and hence needs to have its own separate copy of these variables. The memory requirements of each of these instances are stored in a separate stack frame. These frames are stacked on top of each other within stack memory.  Using a Stack Frame: Recall that a stack is a data structure in which either a new element is pushed onto the top or the last element to have been added is popped off  Section 3.1 . Let us denote the top stack frame by A. When the execution of A makes a subroutine call to a routine with some input values, a stack frame is created for this new instance. This frame denoted B is pushed onto the stack af- ter that for A. In addition to a separate copy of the local variables for the routine, it contains a pointer to the next line of code that A must execute when B returns. When B returns, its stack frame is popped, and A continues to execute at the line of code that had been indicated within B. When A completes, it too is popped off the stack.  Silly Example: This example demonstrates how difﬁcult it is to trace out the full stack-frame tree, yet how easy it is to determine the output using the friends  strong- induction  method:  algorithm Fun n   cid:6  pre-cond cid:7 : n is an integer.  cid:6  post-cond cid:7 : Outputs a silly string. begin  if  n > 0   then   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes8 CUUS154-Edmonds 978 0 521 84931 9  March 28, 2008  21:59  112  Recursion  if  n = 1   then else if  n = 2   then  put “X”  put “Y”  else  put “A” Fun n − 1  Put “B” Fun n − 2  Put “C”  end if  end if  end algorithm  EXERCISE 8.6.1 Attempt to trace out the tree of stack frames for the silly example Fun 5 .  EXERCISE 8.6.2  See solution in Part Five.  Now try the following simpler approach. What is the output of Fun 1 ? What is the output of Fun 2 ? Trust the answers to all previous questions; do not recalculate them.  Assume a trusted friend gave you the an- swer.  Now, what is the output of Fun 3 ? Repeat this approach for n = 4, 5, and 6.  8.7  Proving Correctness with Strong Induction  Whether you give your subinstances to friends or you recurse on them, this level of abstraction considers only the algorithm for the top stack frame. We must now prove that this sufﬁces to produce an algorithm that successfully solves the problem for every input instance. When proving this, it is tempting to talk about stack frames. This stack frame calls this one, which calls that one, until you hit the base case. Then the solutions bubble back up to the surface. These proofs tend to make little sense. Instead, we use strong induction to prove formally that the friends level of abstraction works.  Strong Induction: Strong induction is similar to induction, except that instead of assuming only S n − 1  to prove S n , you must assume all of S 0 , S 1 , S 2 , . . . , S n − 1 .  A Statement for Each n: For each value of n ≥ 0, let S n  represent a Boolean statement. For some values of n this statement may be true, and for others it may be false. Goal: Our goal is to prove that it is true for every value of n, namely that ∀n ≥ 0, S n .   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes8 CUUS154-Edmonds 978 0 521 84931 9  Abstractions, Techniques, and Theory  March 28, 2008  21:59  Proof Outline: Proof by strong induction on n.  Induction Hypothesis: For each n ≥ 0, let S n  be the statement that . . . .  It is important to state this clearly.   Base Case: Prove that the statement S 0  is true. Induction Step: For each n ≥ 0, prove S 0 , S 1 , S 2 , . . . , S n − 1  ⇒ S n . Conclusion: By way of induction, we can conclude that ∀n ≥ 0, S n .  113  See Exercises 8.7.1 and 8.7.2.  Proving the Recursive Algorithm Works:  Induction Hypothesis: For each n ≥ 0, let S n  be the statement “The recursive algorithm works for every instance of size n.” Goal: Our goal is to prove that ∀n ≥ 0, S n , i.e. that the recursive algorithm works for every instance.  Proof Outline: The proof is by strong induction on n.  Base Case: Proving S 0  involves showing that the algorithm works for the base cases of size n = 0. Induction Step: The statement S 0 , S 1 , S 2 , . . . , S n − 1  ⇒ S n  is pro- ved as follows. First assume that the algorithm works for every instance of size strictly smaller than n, and then prove that it works for every instance of size n. This mirrors exactly what we do on the friends level of abstrac- tion. To prove that the algorithm works for every instance of size n, consider an arbitrary instance of size n. The algorithm constructs subinstances that are strictly smaller. By our induction hypothesis we know that our algorithm works for these. Hence, the recursive calls return the correct solutions. On the friends level of abstraction, we proved that the algorithm constructs the correct solutions to our instance from the correct solutions to the subin- stances. Hence, the algorithm works for this arbitrary instance of size n. The S n  follows. Conclusion: By way of strong induction, we can conclude that ∀n ≥ 0, S n , i.e., the recursive algorithm works for every instance.  EXERCISE 8.7.1 Give the process of strong induction as we did for regular induction.  EXERCISE 8.7.2  See solution in Part Five.  As a formal statement, the base case can be eliminated in strong induction because it is included in the formal induction step. How is this?  In practice, the base cases are still proved separately.    P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes9 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  22:5  114  9 Some Simple Examples of  Recursive Algorithms  I will now give some simple examples of recursive algorithms. Even if you have seen them before, study them again, keeping the techniques and theory from Chapter 8 in mind. For each example, look for the key steps of the friend paradigm. What are the subinstances given to the friend? What is the size of an instance? Does it get smaller? How are the friend’s solutions combined to give your solution? What does the tree of stack frames look like? What is the time complexity of the algorithm?  9.1  Sorting and Selecting Algorithms  The classic divide-and-conquer algorithms are merge sort and quick sort. They both have the following basic structure.  General Recursive Sorting Algorithm:   cid:1  Take the given list of objects to be sorted  numbers, strings, student records, etc. .  cid:1  Split the list into two sublists.  cid:1  Recursively have friends sort each of the two sublists.  cid:1  Combine the two sorted sublists into one entirely sorted list.  This process leads to four different algorithms, depending on the following factors  see Exercise 9.1.1 :  Sizes: Do you split the list into two sublists each of size n and one of size one?  2 , or one of size n − 1  Work: Do you put minimal effort into splitting the list but put lots of effort into recombining the sublists, or put lots of effort into splitting the list but put mini- mal effort into recombining the sublists?   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes9 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  22:5  Some Simple Examples of Recursive Algorithms  EXAMPLE 9.1.1 Merge Sort  Minimal Work to Split in Half   This is the classic recursive algorithm.  Friend’s Level of Abstraction: Recursively give one friend the ﬁrst half of the input to sort and another friend the second half to sort. Then combine these two sorted sublists into one completely sorted list. This combining process is referred to as merging. A simple linear-time algorithm for it can be found in Section 3.3.  115  Size: The size of an instance is the number of elements in the list. If this is at least two, then the sublists are smaller than the whole list. Hence, it is valid to recurse on them with the reassurance that your friends will do their parts correctly. On the other hand, if the list contains only one element, then by default it is already sorted and nothing needs to be done.  Generalizing the Problem: If the input is assumed to be received in an array indexed from 1 to n, then the second half of the list is not a valid instance, because it is not indexed from 1. Hence, we redeﬁne the preconditions of the sorting problem to require as input both an array A and a subrange [i, j ]. The postcondition is that the speciﬁed sublist is to be sorted in place.  Running Time: Let T n  be the total time required to sort a list of n elements. This total time consists of the time for two subinstances of half the size to be sorted, plus  cid:1  n  time for merging the two sublists together. This gives the recurrence re- lation T n  = 2T n 2  +  cid:1  n . See Chapter 27 to learn how to solve recurrence rela- = 1 and f  n  =  cid:1  n1 , so c = 1. Because tions like these. In this example, log a = c, the technique concludes that the time is dominated by all levels and T n  = log b loga  cid:1  f  n  log n  =  cid:1  n log n . log b Tree of Stack Frames: The following is a tree of stack frames for a concrete example:  = log 2  log 2                    In:  100 21 40 97 53 9 25 105 99 8 45 10                     Out: 8 9 10 21 25 40 45 53 97 99 100 105                                                                                                                                                                                In:  100 21 40 97 53 9       In:  25 105 99 8 45 10              Out: 9 21 40 53 97 100       Out: 8 10 25 45 99 105                                                                                                                                                                                                              In:  100 21 40      In:  97 53 9      In:  25 105 99       In:  8 45 10      Out: 21 40 100      Out: 9 53 97      Out: 25 99 105       Out: 8 10 45                                                                                                                                                                                                                           In:    100 21     40     97 53     9     25 105      99      8 45     10   Out:   21 100     40     53 97     9     25 105      99      8 45     10                                                                                                                                                                                                                  In:   100      21       97     53       25      105       8     45    Out:  100      21       97     53       25      105       8     45      P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes9 CUUS154-Edmonds 978 0 521 84931 9  Recursion  116  April 2, 2008  22:5  EXAMPLE 9.1.2 Quick Sort  Minimal Work to Recombine the Halves   The following is one of the fastest sorting algorithms. Hence the name.  Friend’s Level of Abstraction: The algorithm partitions the list into two sublists where all the elements that are less than or equal to a chosen pivot element are to the left of the pivot element and all the elements that are greater than it are to the right of it.  There are no requirements on the order of the elements in the sublists.  Next, re- cursively have a friend sort those elements before the pivot and those after it. Finally,  without effort  put the sublists together, forming one completely sorted list.  The ﬁrst step in the algorithm is to choose one of the elements to be the pivot  element. How this is to be done is discussed below.  Tree of Stack Frames: The following is a tree of stack frames for a speciﬁc input:                     In:  100 21 40 97 53 9 25 105 99 8 45 10                      Out: 8 9 10 21 25 40 45 53 97 99 100 105                                                                                                                                                                                           In:  21 9 8 10       25      In:  100 40 97 53 105 99 45               Out: 8 9 10 21               Out: 40 45 53 97 99 100 105                                                                                                                                                                                                                          In:  9 8     10      In:  21      In:  40 53 45     97    In:  100 105 99        Out: 8 9             Out: 21      Out: 40 45 53           Out: 99 100 105                                                                                                                                                                                                                                           In:    8    9                        40 45   53                 99   100    105   Out:   8                             40 45                      99          105                                                                                                                                                                                                      In:       40   45                                 Out:           45     Running Time: The computation time depends on the choice of the pivot element.  Median: If we are lucky and the pivot element is close to having the median value, then the list will be split into two sublists of size approximately n 2. We will see that partitioning the array according to the pivot element can be done in time  cid:1  n . In this case, the timing is T n  = 2T n 2  +  cid:1  n  =  cid:1  n log n . Reasonable Split: The above timing is quite robust with respect to the choice of the pivot. For example, suppose that the pivot always partitions the list into one sublist of one-ﬁfth the original size and one of four-ﬁfths the original size. The total time is then the time to partition plus the time to sort the sublists of these = 1, this evaluates 5n  + T  4 sizes. This gives T n  = T  1 to T n  =  cid:1  n log n .  See Chapter 27.  Worst Case: On the other hand, suppose that the pivot always splits the list into one of size n − 1 and one of size 1. In this case, T n  = T n − 1  + T 1  +  cid:1  n , which evaluates to T n  =  cid:1  n2 . This is the worst case scenario.  5n  +  cid:1  n . Because 1  + 4  5  5  We will return to quick sort after considering the following related problem.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes9 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  22:5  Some Simple Examples of Recursive Algorithms  EXAMPLE 9.1.3  Finding the kth Smallest Element  Given an unsorted list and an integer k, this example ﬁnds the kth smallest element from the list. It is not clear at ﬁrst that there is an algorithm for doing this that is any faster than sorting the entire list. However, it can be done in linear time using the sub- routine Pivot.  Friend’s Level of Abstraction: The algorithm is like that for binary search. Ignoring in- put k, it proceeds just like quick sort. A pivot element is chosen randomly, and the list is split into two sublists, the ﬁrst containing all elements that are all less than or equal to the pivot element and the second those that are greater than it. Let  cid:2  be the number of elements in the ﬁrst sublist. If  cid:2  ≥ k, then we know that the kth smallest element from the entire list is also the kth smallest element from the ﬁrst sublist. Hence, we can give this ﬁrst sublist and this k to a friend and ask him to ﬁnd it. On the other hand, if  cid:2  < k, then we know that the kth smallest element from the entire list is the  k −  cid:2  th smallest element from the second sublist. Hence, on giving the second sublist and k −  cid:2  to a friend, he can ﬁnd it.  Tree of Stack Frames: The following is a tree of stack frames for our input.  117  In:     100 21 40 97 53 9 25 105 99 8 45 10  Sorted: 8 9 10 21 25 40 45 53 97 99 100 105  k = 7                                        Pivot = 25                                   Left:   21 9 25 8 10                         Right:  100 40 97 53 105 99 45               Left Size = 5 < k                            Out = 45                                       In:     100 40 97 53 105 99 45  Sorted: 40 45 53 97 99 100 105  k = 7–5 = 2                     Pivot = 97                      Left:   40 97 53 45             Right:  100 105 99              Left Size = 4 >= k              Out = 45                         In:     40 97 53 45  Sorted: 40 45 53 97      k = 2                Pivot = 45           Left:   40 45        Right:  97 53        Left Size = 2 >= k   Out = 45              In:     40 45      Sorted: 40 45      k = 2              Pivot = 40         Left:   40         Right:  45         Left Size = 1 < k  Out = 45            In:     45      Sorted: 45      k=2–1=1 Out:45    P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes9 CUUS154-Edmonds 978 0 521 84931 9  Recursion  April 2, 2008  22:5  118  EXAMPLE 9.1.3  Finding the kth Smallest Element  cont.   Running Time: Again, the computation time depends on the choice of the pivot ele- ment.  Median: If we are lucky and the pivot element is close to the median value, then the list will be split into two sublists of size approximately n 2. Because the routine recurses on only one of the halves, the timing is T n  = T n 2  +  cid:1  n  =  cid:1  n . Reasonable Split: If the pivot always partitions the list so that the larger half is at 5n  +  cid:1  n , which is still linear most 4 time, T n  =  cid:1  n . Worst Case: In the worst case, the pivot splits the list into one of size n − 1 and one of size 1. In this case, T n  = T n − 1  +  cid:1  n , which is T n  =  cid:1  n2 .  5n, then the total time is at most T n  = T  4  Choosing the Pivot: In Examples 9.1.2 and 9.1.3, the timing depends on choosing a good pivot element quickly.  Fixed Value: If you know that you are sorting elements that are numbers within the range [1..100], then it is reasonable to partition these elements based on whether they are smaller or larger than 50. This is often referred to as bucket sort. See Section 5.1. However, there are two problems with this technique. The ﬁrst is that in general we do not know what range the input elements will lie in. The sec- ond is that at every level of recursion another pivot value is needed with which to partition the elements. The solution is to use the input itself to choose the pivot value.  Use A[1] as the Pivot: The ﬁrst thing one might try is to let the pivot be the ele- ment that happens to be ﬁrst in the input array. The problem with this is that if the input happens to be sorted  or almost sorted  already, then this ﬁrst element will split the list into one of size zero and one of size n − 1. This gives a worst case time of  cid:1  n2 . Given random data, the algorithm will execute quickly. On the other hand, if you forget that you sorted the data and you run it a second time, then the second run will take a long time to complete.  Use A[ n 2 ] as the Pivot: Motivated by the last attempt, one might use the element that happens to be located in the middle of the input array. For all practical pur- poses, this would likely work well. It would work exceptionally well when the list is already sorted. However, there are some strange inputs cooked up for the sole purpose of being nasty to this particular implementation of the algorithm, on which the algorithm runs in  cid:1  n2  time. The adversary will provide such an in- put, giving a worst case time complexity of  cid:1  n2 .  A Randomly Chosen Element: In practice, what is often done is to choose the pivot element randomly from the input elements. See Section 21.1. The advan- tage of this is that the adversary who is choosing the worst case input instance   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes9 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  22:5  119  Some Simple Examples of Recursive Algorithms  knows the algorithm, but does not know the random coin tosses. Hence, all input instances are equally good and equally bad.  We will prove that the expected computation time is  cid:1  n log n . What this means is that if you ran the algorithm 1,000,000 times on the same input, then the average running time would be  cid:1  n log n .  Intuition: One often gains good intuition by assuming that what we expect to happen happens reasonably often. If the pivot always partitions the list into one sublist of one-ﬁfth the original size and one of four-ﬁfths the orig- inal size, then the total time is T n  = T  1 5n  +  cid:1  n  =  cid:1  n log n . When a pivot is chosen randomly, the probability that it partitions the list at least this well is 3 5 . When a partition is worse than this, it is not a big problem. We just say that no signiﬁcant progress is made, and we try again. After all, we expect to make progress in approximately three of every ﬁve partitions.  5n  + T  4  More Formal: Formally, we set up and solve a difﬁcult recurrence relation. Suppose that the randomly chosen pivot element happens to be the ith smallest element. This splits the list into one of size i and one of size n − i, in  cid:1  which case the running time is T i  + T n − i  +  cid:1  n . Averaging this over all  cid:2  possible values ofi gives the recursive relation T n  = Avgi∈[0..n] T i  + T n − i  +  cid:1  n  . With a fair bit of work, this evaluates to  cid:1  n log n .  Randomly Choose Three Elements: Another option is to randomly select three elements from the input list and use the middle one as the pivot. Doing this greatly increases the probability that the pivot is close to the middle and hence decreases the probability of the worst case occurring. However, it so also takes time. All in all, the expected running time is worse.  A Deterministic Algorithm: Though in practice such a probabilistic algorithm is easy to code and works well, theoretical computer scientists like to ﬁnd a deter- ministic algorithm that is guaranteed to run quickly.  The following is a deterministic method of choosing the pivot that leads to a worst case running time of  cid:1  n  for ﬁnding the kth smallest element. First group the n elements into n 5 groups of ﬁve elements each. Within each group of ﬁve elements, do  cid:1  1  work to ﬁnd the median of the group. Let Smedian be the set of n 5 elements containing the median from each group. Recursively ask a friend to ﬁnd the median element from the set Smedian. This element will be used as our pivot.  I claim that this pivot element has at least 3  10n elements that are less than or equal to it and another 3 10n elements that are greater or equal to it. The proof of the claim is as follows. Because the pivot is the median within Smedian, there are Smedian elements within Smedian that are less than or equal to the pivot. 10n = 1 1 Consider any such element xi ∈ Smedian. Because xi is the median within its group of ﬁve elements, there are three elements within this group  including xi itself  that are less than or equal to xi and hence in turn less than or equal to the pivot.  2   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes9 CUUS154-Edmonds 978 0 521 84931 9  Recursion  120  April 2, 2008  22:5  Counting all these gives 3 × 1 that are greater than or equal to the pivot.  10n elements. A similar argument counts this many  The algorithm to ﬁnd the kth largest element proceeds as stated originally. A friend is asked to ﬁnd either the kth smallest element within all elements that are less than or equal to the pivot or the  k −  cid:2  th smallest element from all those that are greater than it. The claim ensures that the size of the sublist given to the friend is at most 7  10n.  Unlike the ﬁrst algorithm for the ﬁnding kth smallest element, this algo- rithm recurses twice. Hence, one would naively assume that the running time is  cid:1  n log n . However, careful analysis shows that it is only  cid:1  n . Let T n  de- note the running time. Finding the median of each of the 1 5n groups takes  cid:1  n  time. Recursively ﬁnding the median of Smedian takes T  1 5n  time. Recursing on the remaining at most 7 10n  time. This gives a to- tal of T n  = T  1 5n  + T  7 + 7 < 1, this evaluates to T n  =  cid:1  n .  See Chapter 27.   10n elements takes at most T  7 10n  +  cid:1  n  time. Because 1  10  5  A deterministic quick sort algorithm can use this deterministic  cid:1  n -time algorithm for the ﬁnding the kth smallest element, to ﬁnd the median of the list to be the pivot. Because partitioning the elements according to the pivot already takes  cid:1  n  time, the timing is still T n  = 2T  n  2   +  cid:1  n  =  cid:1  n log n .  Partitioning According to the Pivot Element: This is an iterative step. The input consists of a list of elements A[I ], . . . , A[J ] and a pivot element. The output con- sists of the rearranged elements and an index i, such that the elements A[I ], . . . , A[i − 1] are all less than or equal to the pivot element, A[i] is the pivot element, and the elements A[i + 1], . . . , A[J ] are all greater than it.  The loop invariant is that there are indices I ≤ i ≤ j ≤ J for which: 1. The values in A[I ], . . . , A[i − 1] are less than or equal to the pivot element. 2. The values in A[j + 1], . . . , A[J ] are greater than the pivot element.  3. The pivot element has been removed and is on the side, leaving an empty en- try either at A[i] or at A[j ].  4. The other elements in A[i], . . . , A[j ] have not been considered.  The loop invariant is established by setting i = I and j = J , making A[i] empty by putting the element in A[i] where the pivot element is and putting the pivot element aside.  If the loop invariant is true and i < j , then there are four possible cases  see  Figure 9.1 :  Case A. A[i] is empty and A[ j] ≤ pivot: A[j ] belongs on the left, so move it to the empty A[i]. Now A[j ] is empty. Increase the left side by increasing i by one.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes9 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  22:5  Some Simple Examples of Recursive Algorithms  A:  Pre:  Post:  B:  Pre:  Post:  I  I  I  I  Less  Less  Less  Less  ?  ?  ?  ?  j  j  j  j  i  i  i  i  More  More  More  More J  J  J  J  C:  Pre:  Post:  D:  Pre:  Post:  I  I  I  I  Less  Less  Less  Less  ?  i  ?  ?  ?  j  j  j  j  i  i  i  More  More  More  More  J  J  J  J  Figure 9.1: The four cases of how to iterate are shown.  121  Case B. A[i] is empty and A[ j] > pivot: A[j ] belongs on the right and is already there. Increase the right side by decreasing j by one. Case C. A[ j] is empty and A[i] ≤ pivot: A[i] belongs on the left and is already there. Increase the left side by increasing i by one.  Case D. A[ j] is empty and A[i] > pivot: A[i] belongs on the right, so move it to the empty A[j ]. Now A[i] is empty. Increase the right side by decreasing j by one. In each case, the loop invariant is maintained. Progress is made because j − i de- creases. When i = j , the list is split as needed, leaving A[i] empty. Put the pivot there. The  postcondition follows.  EXERCISE 9.1.1  See solution in Part Five.  Consider the algorithm that puts minimal effort into splitting the list into one of size n − 1 and one of size one, but puts lots of effort into recombining the sublists. Also consider the algorithm that puts lots of effort into splitting the list into one of size n − 1 and one of size one, but puts minimal effort into recombining the sublists. What are these two algorithms?  EXERCISE 9.1.2  See solution in Part Five.  One-friend recursion vs iteration. 1. Your task is to accept a tuple  cid:5 a1, a2, . . . , an cid:6  and return the reversed tuple  cid:5 an, an−1, . . . , a1 cid:6 . Being lazy, you will only strip off an element from one end or add an element back onto one end. But you have recursive friends to help you. Provide both a paragraph containing the friend’s explanation of the algorithm, and the recursive code.  2. Now suppose that you have a stack, but no friends.  See Chapter 3 . Quickly sketch an iterative program that solves this same problem. Be sure to include loop invari- ants and other the key steps required for describing an iterative algorithm.  3. Trace each of these two programs. Step by step, compare and contrast their com-  putations on a computer.  EXERCISE 9.1.3 Exercise 4.4.1 asks for an iterative algorithm for searching within a matrix A[1..n, 1..m] in which each row is sorted and each column is sorted. This requires that T n, m  = n + m − 1 of the matrix entries be examined. Exercise 7.0.7   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes9 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  22:5  Recursion proves that this is tight when n = m. But it is clearly too big when m >> n, given one can do binary search in each row in time n log m << n + m − 1. The goal now is to de- sign a recursive algorithm that accesses T n, m  ≈ n log2  m n   entries. As a huge hint, the recurrence relation will be T n, m  = maxn cid:1 ∈[1,n] T n 2   + log2 n. 2   + T n − n , m , m You must look at the recursive tree in order to get some intuition to why the time is T n, m  ≈ n log2  m n  . You can also plug T n, m  = n log2  m n   + 2n − log n  − 2 into this recurrence relation and see that it satisﬁes it.   cid:1    cid:1   122  9.2 Operations on Integers Raising an integer to a power b N, multiplying x × y, and matrix multiplication each have surprising divide-and-conquer algorithms.  EXAMPLE 9.2.1  bN  Suppose that you are given two integers b and N and want to compute b N.  The Iterative Algorithm: The obvious iterative algorithm simply multiplies b to- gether N times. The obvious recursive algorithm recurses with Power  b, N  = b × Power  b, N − 1 . This requires the same N multiplications. The Straightforward Divide-and-Conquer Algorithm: The obvious divide-and-con-  cid:11  =  cid:9  × b  cid:10  N quer technique cuts the problem into two halves using the property that b 2  cid:9   ×  cid:11  = b N. This leads to the recursive algorithm Power  b, N  = Power  b,  cid:8  N  cid:8  N b 2 2   + 1 multiplications. The Power  b,  cid:10  N = 1 and f  N  =  cid:1  N 0 , so c = 0. Be- technique in Chapter 27 notes that log a log b cause log a > c, the technique concludes that time is dominated by the base cases and T N  =  cid:1  N  log a   log b   =  cid:1  N . This is no faster than the standard iterative algorithm. log b   cid:11  . Its recurrence relation gives T N  = 2T  N  = log 2   cid:9 + cid:10  N 2   cid:8  N 2  log 2  2  2  Reducing the Number of Recursions: This algorithm can be improved by noting that the two recursive calls are almost the same and hence need only to be made once. The 2   + 1 multiplications. Here log a = 0 new recurrence relation gives T N  = 1T  N = c, we conclude that the time is dominated and f  N  =  cid:1  N 0 , so c = 0. Because log a by all levels and T N  =  cid:1  f  N  log N  =  cid:1  log N  multiplications. Code:  = log 1  log 2  log b  log b  algorithm Power  b, N   cid:5  pre-cond cid:6 : N ≥ 0  N and b not both 0   cid:5  post-cond cid:6 : Outputs bn. begin  if  N = 0   then return 1  half =  cid:10  N p = Power  b, half    else   cid:11   2   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes9 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  22:5  Some Simple Examples of Recursive Algorithms  if  2 · half = N   then  return  p · p   % if N is even, b N = b return  p · p · b   % if N is odd, b N = b · b   cid:10 N 2 cid:11  · b   cid:10 N 2 cid:11    cid:10 N 2 cid:11  · b   cid:10 N 2 cid:11   123  else  end if  end if  end algorithm  Tree of Stack Frames:  In: b=2, N=5 Out: 32 = 4x4x2      In: b=2, N=2     Out: 4 = 2x2                            In: b=2, N=1        Out: 2 = 1x1x2              In: b=2, N=0             Out: 1   Running Time:  Input Size: One is tempted to say that the ﬁrst two  cid:1  N  algorithms require a lin- ear number of multiplications and that the last  cid:1  log N  one requires a logarithmic number. However, in fact the ﬁrst two require exponential  cid:1  2n  number and the last a linear  cid:1  n  number in the size of the input, which is typically taken as the number of bits, n = log N, needed to represent the number. Operation: Is it fair to count the multiplications and not the bit operations in this case? I say not. The output b N contains  cid:1  N log b  = 2 cid:1  n  bits, and hence it will take this many bit operations to simply output the answer. Given this, it is not really fair to say that the time complexity is only  cid:1  n .  EXAMPLE 9.2.2  x × y  The time complexity of Example 9.2.1 was measured in terms of the number of multi- plications. This ignores the question of how quickly one can multiply.  The input for the next problem consists of two strings of n digits each. These are viewed as two integers x and y, either in binary or in decimal notation. The problem is to multiply them.  The Iterative Algorithm: The standard elementary school algorithm considers each pair of digits, one from x and the other from y, and multiplies them together. These n2 products are shifted appropriately and summed. The total time is  cid:1  n2 . It is hard to believe that one could do it faster.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes9 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  22:5  124  Recursion  EXAMPLE 9.2.2  x × y  cont.   8 5  1 8 6 8  5  8  7 6 2  2 9 4 2  3  9  2  4  1 2 3 0  2  7  1 0 9  4 4  The Straightforward Divide-and-Conquer Algorithm: Let us see how well the divide- and-conquer technique can work. Split each sequence of digits in half, and consider each half as an integer. This gives x = x1 × 10n 2 + x0 and y = y1 × 10n 2 + y0. Multiply- ing these symbolically gives   cid:3  x × y = =  cid:5    cid:4  x1 × 10 n 2 + x0  cid:6  × 10n + cid:5   x1y1   cid:3    cid:4  2 + y0  cid:6  × 10 n 2 + cid:5    cid:6   x0y0  .  × y1 × 10 n x1y0 + x0y1  The obvious divide-and-conquer algorithm would recursively compute the four sub- problems x1y1, x1y0, x0y1, and x0y0, each of n 2 digits. This would take 4T  n 2   time. Then these four products are shifted appropriately and summed. Note that additions can be 2   +  cid:1  n . Here done in  cid:1  n  time. See Section 2.2. Hence, the total time is T n  = 4T  n > c, the technique concludes that the time is dominated by the base cases and T n  =  cid:1  n loga   log b   =  cid:1  n2 . This is no improvement in time.  = 2 and f  n  =  cid:1  n1 , so c = 1. Because log a  = log 4  log a log b  log 2  log b  Reducing the Number of Recursions: Suppose that we could ﬁnd a trick so that we only needed to recurse three times instead of four. One’s intuition might be that this would only provide a linear time saving, but in fact the saving is much more. T n  = = 1.58 . . . , which is still bigger than c. Hence, time is 3T  n log b   =  cid:1  n1.58... . This is still dominated by the base cases, but now this is T n  =  cid:1  n a signiﬁcant improvement over  cid:1  n2 .  2   +  cid:1  n . Now log a  = log 3  log 2  log b  log a  The Trick: The ﬁrst step is to multiply x1y1 and x0y0 recursively as required. This leaves us only one more recursive multiplication. If you review the symbolic expansion for x × y, you will see that we do not actually need to know the values of x1y0 and x0y1. We only need to know their sum. Symbolically, we can observe the following:  x1y0 + x0y1 =  cid:1  =  cid:1    cid:2  − x1y1 − x0y0  x1y1 + x1y0 + x0y1 + x0y0  x1 + x0    cid:6  cid:2  − x1y1 − x0y0  y1 + y0   cid:5   Hence, the sum x1y0 + x0y1 that we need can be computed by adding x1 to x0 and y1 to y0; multiplying these sums; and subtracting off the values x1y1 and x0y0 that we know   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes9 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  22:5  Some Simple Examples of Recursive Algorithms  from before. This requires only one additional recursive multiplication. Again we use the fact that additions are fast, requiring only  cid:1  n  time.  Code:  algorithm Multiply x, y   cid:5  pre-cond cid:6 : x and y are two integers represented as an array of n digits  cid:5  post-cond cid:6 : The output consists of their product represented as an array of n + 1 digits  125  begin  if n = 1  then  result  x × y   % product of single digits  else  cid:5 x1, x0 cid:6  = high- and low-order n  cid:5 y1, y0 cid:6  = high- and low-order n A = Multiply x1, y1  C = Multiply x0, y0  B = Multiply x1 + x0, y1 + y0  − A − C result  A × 10n + B × 10 n  2 + C    2 digits of x 2 digits of y  end if  end algorithm  It is surprising that this trick reduces the time from  cid:1  n2  to  cid:1  n1.58 .  Dividing into More Parts: The next question is whether the same trick can be ex- tended to improve the time even further. Instead of splitting each of x and y into two pieces, let’s split them each into d pieces. The straightforward method recursively mul- tiplies each of the d 2 pairs of pieces together, one from x and one from y. The total = 2 > c. This gives time is T n  = d 2T  n T n  =  cid:1  n2 . Again, we are back where we began.  d   +  cid:1  n . Here a = d 2, b = d, c = 1, and log d2  log d  Reducing the Number of Recursions: The trick now is to do the same with fewer re- cursive multiplications. It turns out it can be done with only 2d − 1 of them. This d   +  cid:1  n . Here a = 2d − 1, b = d, c = 1, and gives time of only T n  =  2d − 1 T  n ≈ c. By increasing d, the times for the top stack frame and log 2d−1  log d  for the base cases become closer and closer to being equal. Recall that when this hap- pens, we must add an extra  cid:1  log n  factor to allow for the  cid:1  log n  levels of recursion. This gives T n  =  cid:1  n log n , which is a surprising running time for multiplication.  = 1 + 1  ≈ log d +1  log d   log d   Fast Fourier Transformation: I will not describe the trick for reducing the number of recursive multiplications from d 2 to only 2d − 1. Let it sufﬁce to say that it involves thinking of the problem as the evaluation and interpolation of polynomials. When d becomes large, other complications arise. These are solved by using the 2d th roots of unity over a ﬁnite ﬁeld. Performing operations over this ﬁnite ﬁeld requires  cid:1  log log n  time. This increases the total time from  cid:1  n log n  to  cid:1  n log n log log n . This algorithm is used often for multiplication and many other applications such as signal processing. It is referred to as fast Fourier transformation.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes9 CUUS154-Edmonds 978 0 521 84931 9  Recursion  126  April 2, 2008  22:5  EXAMPLE 9.2.3  Strassen’s Matrix Multiplication  The next problem is to multiply two n × n matrices. The Iterative Algorithm: The obvious iterative algorithm computes the  cid:5 i, j cid:6  entry of the product matrix by multiplying the ith row of the ﬁrst matrix with the j th column of the second. This requires  cid:1  n  scalar multiplications. Because there are n2 such en- tries, the total time is  cid:1  n3 .   cid:7    cid:8  cid:7   The Straightforward Divide-and-Conquer Algorithm: When designing a divide-and- conquer algorithm, the ﬁrst step is to divide these two matrices into four submatrices each. Multiplying these symbolically gives the following:  e f  g h  a b d c × n Computing the four n 2 submatrices in this product in this way requires recur- 2 sively multiplying eight pairs of n 2 matrices. The total computation time is given by the recurrence relation T n  = 8T n 2  +  cid:1  n2  =  cid:1  n log 8  log 2   =  cid:1  n3 . This is 2 no faster than the standard iterative algorithm.  × n  ae + bf ce + df  ag + bh cg + dh   cid:8    cid:7   =   cid:8   Reducing the Number of Recursions: Strassen found a way of computing the four × n n 2 submatrices in this product using only seven such recursive calls. This gives T n  = 7T n 2  +  cid:1  n2  =  cid:1  n log 7  log 2   =  cid:1  n2.8073 . I will not include the details 2 of the algorithm.  EXERCISE 9.2.1  See solution in Part Five  Recursive GCD.  1. Write a recursive program to ﬁnd the GCD of two numbers. The program should  mirror the iterative algorithm found in Chapter 6.  2. Rewrite this recursive algorithm to solve the following more general problem. The input still consists of two integers a and b. The output consists of three inte- gers g, u, and v, such that ua + vb = g = GCD a, b . For example, on a = 25 and b = 15 the algorithm outputs  cid:5 5, 2, −3 cid:6 , because 2 × 25 − 3 × 15 = 50 − 45 = 5 = GCD 25, 15 . Provide both a paragraph containing the friend’s explanation of the algorithm, and the recursive code.  3. Write an algorithm for the following problem. The input consist of three integers a, b, and w. Assume that you live in a country that has two types of coins, one worth a dollars and the other b dollars. Both you and the storekeeper have a pocket full of each. You must pay him w dollars. You can give him any number of coins, and he may give you change with any number of coins. Your algorithm must determine whether or not this is possible and, if so, describe some way of doing it  not neces- sarily the optimal way . [Hint: Compute GCD a, b , and use the three values g, u, and v. Consider the two cases when g divides w and when it does not.  If you want to ﬁnd the optimal number of coins, basically you change a solution by using the fact that   b 4. Designing an algorithm that, when given a prime p and an integer x ∈ [1, p − 1], outputs an inverse y such that x · y ≡mod p 1. [Hint: First show that GCD p, x  = 1.  g   · b = 0. ]  g   · a −   a   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes9 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  22:5  Some Simple Examples of Recursive Algorithms  Then compute GCD p, x  and use the values u, and v. Proving that every x has such an inverse proves that the integers modulo a prime form a ﬁeld.]  9.3  Ackermann’s Function  If you are wondering just how slowly a program can run, consider the algorithm be- low. Assume the input parameters n and k are natural numbers.  127  Algorithm:  algorithm A k, n  if  k = 0  then  else  return  n +1 + 1   if  n = 0  then  if  k = 1  then return  0    else  return  1    else  end if  end if  end algorithm  return  A k − 1, A k, n − 1     Recurrence Relation: Let Tk n  denote the value returned by A k, n . This gives T0 n  = 2 + n, T1 0  = 0, Tk 0  = 1 for k ≥ 2, and Tk n  = Tk−1 Tk n − 1   for k > 0 and n > 0.  Solving: T0 n  = 2 + n T1 n  = T0 T1 n − 1   = 2 + T1 n − 1  = 4 + T1 n − 2   = 2i + T1 n − i  = 2n + T1 0  = 2n  T2 n  = T1 T2 n − 1   = 2 · T2 n − 1  = 22 · T2 n − 2  = 2i · T2 n − i  = 2n · T2 0  = 2n  ⎡ ⎣222...2  cid:11  cid:12  cid:13  cid:14   ⎤ ⎦T3 n−i   ⎡ ⎣222...2  cid:11  cid:12  cid:13  cid:14   ⎤ ⎦T3 0   =  n   cid:11  cid:12  cid:13  cid:14  = 222...2  n  T3 n  = T2 T3 n − 1   = 2T3 n−1  = 22T3 n−2 =  cid:11  cid:12  cid:13  cid:14  T4 0  = 1. T4 1  = T3 T4 0   = T3 1  = 222...2  i  = 2.   cid:11  cid:12  cid:13  cid:14  T4 2  = T3 T4 1   = T3 2  = 222...2  1  = 22 = 4.  2   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes9 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  22:5  Recursion  Note that  128   cid:11  cid:12  cid:13  cid:14  T4 3  = T3 T4 2   = T3 4  = 222...2  = 2222 = 224 = 216 = 65, 536.  4   cid:11  cid:12  cid:13  cid:14  222...2  5  = 265,536 ≈ 1021,706  while the number of atoms in the universe is less than 10100. We have   cid:11  cid:12  cid:13  cid:14  T4 4  = T3 T4 3   = T3 65, 536  = 222...2  Ackermann’s function is deﬁned to be A n  = Tn n . We see that A 4  is bigger than any number in the natural world. A 5  is unimaginable.  65,536  Running Time: The only way that the program builds up a big number is by contin- ually incrementing it by one. Hence, the number of times one is added is at least as huge as the value Tk n  returned.  Crashing: Programs can stop at run time because of  1  overﬂow in an integer value;  2  running out of memory;  3  running out of time. Which is likely to happen ﬁrst? If the machine’s integers are 32 bits, then they hold a value that is about 1010. Incre- menting up to this value will take a long time. However, much worse than this, each two increments need another recursive call creating a stack of about this many recur- sive stack frames. The machine is bound to run out of memory ﬁrst. EXERCISE 9.3.1 Design the algorithm and compute the running time when d = 3.  9.4  Exercises  EXERCISE 9.4.1 Review the problem of iterative cake cutting  Section 2.3 . You are now to write a recursive algorithm for the same problem. You will, of course, need to make the pre- and postconditions more general so that when you recurse, your subin- stances meet the preconditions. As in moving from insertion sort to merge sort, you need to make the algorithm faster by cutting the problem in half.  1. You will need to generalize the problem so that the subinstance you would like your friend to solve is a legal instance according to the preconditions and so that the postconditions state the task you would like him to solve. Make the new prob- lem, however, natural. Do not, for example, pass the number n of players in the original problem or the level of recursion. The input should simply be a set of play- ers and a subinterval of cake. The postcondition should state the requirements on how this subinterval is to be divided among these players. To make the problem easier, assume that the number of players is n = 2i for some integer i.  2. Give recursive pseudocode for this algorithm. As a big hint, towards designing a recursive algorithm, I will tell you the ﬁrst things that the algorithm does. Each   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes9 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  22:5  Some Simple Examples of Recursive Algorithms  player speciﬁes where he would cut if he were to cut the cake in half. Then one of these spots is chosen. You need to decide which one and how to create two subin- stances from this.  3. Prove that if your instance meets the preconditions, then your two subinstances  4. Prove that if your friend’s solutions meet the postconditions, then your solution  129  also meet the preconditions.  meets the postcondition.  5. Prove that your solution for the base case meets the postconditions. 6. Give and solve the recurrence relation for the running time of this algorithm. 7. Now suppose that n is not 2i for any integer i. How would we change the algorithm so that it handles the case when n is odd? I have two solutions: one that modiﬁes the recursive algorithm directly, and one that combines the iterative algorithm and the recursive algorithm. You only need to do one of the two  as long as it works and does not increase the BigOh of the running time.    P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes10 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  10:3  130 10 Recursion on Trees  One key application of recursive algorithms is to perform actions on trees, because trees themselves have a recursive deﬁnition. Terminology for trees is summarized in the following table:  Term  Deﬁnition  Root RootInfo tree  Child of node u Parent of node u Siblings Ancestors of node u Descendants of node u Leaf Height of tree  Node at the top The information stored at the root node One of the nodes just under node u The unique node immediately above node u Nodes with same parent The nodes on the unique path from the root to the node u All the nodes below node u A node with no children The maximum level. Some deﬁnitions say that a tree with a  Depth of node u  The number of nodes  or edges  on the path from the root  single node has height 0, others say height 1. It depends on whether you count nodes or edges.  Binary tree  leftSub tree  rightSub tree   to u.  Each node has at most two children. Each of these is designated as either the right child or the left child.  Left subtree of root Right subtree of root  Recursive Deﬁnition of Tree: A tree is either:   cid:1  an empty tree  zero nodes  or  cid:1  a root node with some subtrees as children.  A binary tree is a special kind of tree where each node has a right and a left  subtree.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes10 CUUS154-Edmonds 978 0 521 84931 9  Recursion on Trees  March 29, 2008  10:3  Binary Tree  Tree representing  x + y  * z   *  +  z  x  y  131  EXAMPLE 10.1  Number of Nodes in a Binary Tree  We will now develop a recursive algorithm that will compute the number of nodes in a binary tree.  Speciﬁcations:  Preconditions: The input is any binary tree. Trees with an empty subtree are valid trees. So are trees consisting of a single node and the empty tree.  Postconditions: The output is the number of nodes in the tree.  Size: The size of an instance is the number of nodes in it.  General Input: Consider a large binary tree with two complete subtrees.  Magic: We assume that by magic a friend is able to count the nodes in any tree that is strictly smaller than ours.  Subinstances: The subinstances of our instance tree will be the tree’s left and its right subtree. These are valid instances that are strictly smaller than ours because the root  and the other subtree  have been removed.  Subsolutions: We ask one friend to recursively count the number of nodes in the left subtree and another friend to do so in the right subtree.  Solution: The number of nodes in our tree is the number in its left subtree plus the number in its right subtree plus one for the root.  Other Instances: Suppose the instance is a tree with the right subtree missing. Surpris- ingly, the algorithm still works. The number of nodes in our tree’s right subtree is zero. This is the answer that our friend will return. Hence, the algorithm returns the number in the left subtree plus zero plus one for the root. This is the correct answer. Similarly, the algorithm works when the left subtree is empty or when the instance consists of a single leaf node.  The remaining instance is the empty tree. The algorithm does not work in this case, because it does not have any subtrees. Hence, the algorithm can handle all trees except the empty tree with one piece of code.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes10 CUUS154-Edmonds 978 0 521 84931 9  Recursion  March 29, 2008  10:3  EXAMPLE 10.1  Number of Nodes in a Binary Tree  cont.   Base Cases: The empty tree is sufﬁciently small that we can solve it in a brute force way. The number of nodes in it is zero.  132  The Tree of Stack Frames: There is one recursive stack frame for each node in the tree, and the tree of stack frames directly mirrors the structure of the tree.  Running Time: Because there is one recursive stack frame for each node in the tree and each stack frame does a constant amount of work, the total time is linear in the number of nodes in the input tree, i.e., T n  =  cid:1  n . Proved another way, the recur- rence relation is T n  = T nleft  + T nright  +  cid:1  1 . Plugging the guess T n  = cn gives cn = cnleft + cnright +  cid:1  1 , which is correct because n = nleft + nright + 1. Code:  algorithm NumberNodes tree   cid:2  pre-cond cid:3 : tree is a binary tree.  cid:2  post-cond cid:3 : Returns the number of nodes in the tree. begin  if  tree = emptyTree   then  result  0    else  end if  end algorithm  result  NumberNodes leftSub tree    +NumberNodes rightSub tree   + 1    We have ensured that the algorithm developed works for every valid input instance.  Problem with the Single-Node Base Case: Many people are tempted to use trees with a single node as the base case. A minor problem with this is that it means that the routine no longer works for the empty tree, i.e., the tree with zero nodes. A bigger problem is that the routine no longer works for trees that contain a node with a left child but no right child, or vice versa. This tree is not a base case, because it has more than one node. However, when the routine recurses on the right subtree, the new subinstance consists of the empty tree. The routine, however, no longer works for this tree. See Exercise 10.2.1 for more on this.  Answer for the Empty Tree: A common mistake is to provide the wrong answer for the empty tree. When in doubt as to what answer should be given for the empty tree, consider an instance with the left or right subtree empty. What answer do you need to receive from the empty tree to make this tree’s answer correct?  Height: For example, a tree with one node can either be deﬁned to have height 0 or height 1. It is your choice. However, if you say that it has height 0, then be careful when deﬁning the height of the empty tree.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes10 CUUS154-Edmonds 978 0 521 84931 9  Recursion on Trees  March 29, 2008  10:3  Deﬁnition of Binary Search Tree: Another example is that people often say that the empty tree is not a binary search tree  Section 3.1 . However, it is. A binary tree fails to be a binary search tree when certain relationships between the nodes exist. Because the empty tree has no nodes, none of these violating conditions exist. Hence, by default it is a binary search tree.  Max: What is the maximum value within an empty list of values? One might think 0 or ∞. However, a better answer is −∞. When adding a new value, one uses the code newMax = max oldMax, newValue . Starting with oldMax = −∞ gives the correct answer when the ﬁrst value is added.  133  10.1 Tree Traversals  A task one needs to be able to perform on a binary tree is to traverse it, visiting each node once, in one of three deﬁned orders. Before one becomes familiar with recur- sive programs, one tends to think about computation iteratively, “I visit this node ﬁrst, then this one, then this one, and so on.” Each iteration, the program says “I just visited this node, so now let me ﬁnd the next node to visit.” Surprisingly, such a com- putation is hard to code. The reason is that binary trees by their very nature have a recursive structure. At the end of this section, I include code that traverses a binary tree in an iterative way, but only to convince you that this is much harder than doing it recursively and should be avoided.  Recursion, on the other hand, provides a very easy and slick algorithm for traversing a binary tree. Such a tree is composed of three parts. There is the root node, its left subtree, and its right subtree. You, being lazy, get one friend to traverse the left and another to traverse the right. You, feeling that you need to do some work yourself, visit the root. The order in which the three of you preform your tasks dictates the order in which the nodes get visited. The three classic orders to visit the nodes of a binary tree are preﬁx, inﬁx, and postﬁx, in which the root is visited before, between, or after its left and right subtrees are visited.  algorithm PreFix  tree    cid:2  pre-cond cid:3 : tree is a binary tree.  cid:2  post-cond cid:3 : Visits the nodes in preﬁx order.  begin  if tree  cid:5 = emptyTree   then  put rootInfo tree  PreFix leftSub tree   PreFix rightSub tree    end if  end algorithm  algorithm InFix  tree    cid:2  pre-cond cid:3 : tree is a binary tree.  cid:2  post-cond cid:3 : Visits the nodes in inﬁx order.  begin  if tree  cid:5 = emptyTree   then InFix leftSub tree   put rootInfo tree  InFix rightSub tree    end if  end algorithm   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes10 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  10:3  134  begin  Recursion  algorithm PostFix  tree    cid:2  pre-cond cid:3 : tree is a binary tree.  cid:2  post-cond cid:3 : Visits the nodes in postﬁx order.  if  tree  cid:5 = emptyTree   then PostFix leftSub tree   PostFix rightSub tree   put rootInfo tree   end if  end algorithm  The following order is produced if you tracing out these computations on the two trees displayed below:  PreFix 5 3 1 2 4 6 * + 3 4 7  InFix 1 2 3 4 5 6 3 + 4 * 7  PostFix 2 1 4 3 6 5 3 4 + 7 *  5  6  *  +  7  1  4  3  4  3  2  These three orders have different applications. In math the typical order to put oper- ators is inﬁx notation as in 3 + 4 ∗ 7. However, printed like this produces the wrong order of operations. What is required is  3 + 4  ∗ 7. Using pre- and postﬁx order, the precedence of the operators is correctly determined even without brackets. For this reason, Hewlett Packard’s ﬁrst calculator used postﬁx notations 3 4 + 7∗. It was called reverse Polish notation after Jan L cid:1  ukasiewicz, who developed it in the 1920s. These were a bit of a pain, but luckily for me, the technology improved enough by the time I was starting high school in 1977 so that I did not need to use one. However, such calculators still have their partisans, and Hewlett Packard still makes them.  PreFix visits the nodes in the same order that a depth-ﬁrst search ﬁnds the nodes. See Section 14.4 for the iterative algorithm for doing depth-ﬁrst search of a more gen- eral graph, and Section 14.5 for the recursive version of the algorithm.  Below is the iterative program for visiting the nodes in inﬁx order. As said, it is  needlessly complex and is included only to show you what to avoid.  algorithm IterativeTraversal tree   cid:2  pre-cond cid:3 : tree is a binary tree. As usual, each node has a value and pointers to the roots of its left and right subtrees. In addition, each node has a pointer to its parent.  cid:2  post-cond cid:3 : Does an inﬁx traversal of tree.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes10 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  10:3  Recursion on Trees  begin  element = root tree  count = zero loop cid:2 loop-invariant cid:3 : element is some node in the tree, and some nodes  % Current node in traversal % Current count of nodes  have been visited. if  element has a left child and it has not been visited   then  element = leftChild element   elseif  element has no left child or its left child have been visited  and element has not been visited   then  visit element  elseif  element’s left subtree and element itself has been visited  and element has a right child and it has not been visited   then  element = rightChild element   elseif  element’s left subtree, element itself, and right subtree have been  visited and element has a parent   then  element = parent element   elseif  Everything has been visited and element is the root of the global  135  tree   then exit  end if end loop end algorithm  10.2 Simple Examples  Here is a list of problems involving binary trees.  1. Return the maximum of data ﬁelds of nodes.  2. Return the height of the tree.  3. Return the number of leaves in the tree.  A harder one.   4. Copy the tree.  Try them ﬁrst on your own. See Figure 10.1.  5 3  1  −    4  4  1  Φ  2  5  5  −   Φ  2  2  −    Φ  2  2  2  1  2  1  1  1  1  1  1  3  0 Φ  3  0 Φ  0  Φ  0  Φ  0  Φ  0  Φ  Maximum  Height  The number of leaves  Figure 10.1: The result returned on each subtree is provided.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes10 CUUS154-Edmonds 978 0 521 84931 9  Recursion  March 29, 2008  10:3  Maximum: Given a binary tree, your task is to determine its maximum value. The ﬁrst step is to decide how to create subinstances for your friends. As said, when the input instance is a binary trees, the most natural subinstances are its left and right subtrees. Your friends must solve the same problem that you do. Hence, assume that they provide you with the maximum value within each of these trees. Luckily, the maximum value within a tree is either the maximum on the left, the maximum on the right, or the value at the root. Our only job then is to determine which of these three is the maximum. As described in the beginning of Chapter 10, the maximum of the empty list is −∞.  136  algorithm Max tree   cid:2  pre-cond cid:3 : tree is a binary tree.  cid:2  post-cond cid:3 : Returns the maximum of data ﬁelds of nodes. begin  if  tree = emptyTree   then  result  −∞    else  end if  end algorithm  result  max Max leftSub tree  , Max rightSub tree  , rootData tree     Height: In this problem, your task it to ﬁnd the height of your binary tree. Again, your friends can easily give you the height of your left and right subtrees. The height of your tree is determined by the deeper of its subtrees. Given their heights, you de- termine which is deeper and add one to take the root into account. The height of the empty tree is discussed in the beginning of this chapter.  algorithm Height tree   cid:2  pre-cond cid:3 : tree is a binary tree.  cid:2  post-cond cid:3 : Returns the height of the tree measured in nodes, e.g., a tree with one node has height 1.  begin  if tree = emptyTree   then  else  end if  end algorithm  result  0   result  max Height leftSub tree  , Height rightSub tree    + 1    Exercise 10.2.3 considers another version of this algorithm.  Number of Leaves: This problem is harder than the previous ones. We start by con- sidering a tree with both a left an a right subtree. For this, the number of leaves in the entire tree is the sum of the numbers in the left and right subtrees. If the tree   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes10 CUUS154-Edmonds 978 0 521 84931 9  Recursion on Trees  March 29, 2008  10:3  has one subtree, but the other is empty, then this same algorithm still works. If the tree is empty, then it has zero leaves. However, if this were all of the code, then the answer returned would always be zero. The case that still needs to be consid- ered is the tree consisting of a root with no children. This root is a leaf. We need to count it.  137  algorithm NumberLeaves tree   cid:2  pre-cond cid:3 : tree is a binary tree.  cid:2  post-cond cid:3 : Returns the number of leaves in the tree. begin  if  tree = emptyTree   then else if  leftSub tree  = emptyTree and rightSub tree  = emptyTree   then  result  0    result  1   result  NumberLeaves leftSub tree   + NumberLeaves rightSub tree      else  end if  end algorithm  Copy Tree: If you want to make a copy of a tree, you might be tempted to use the code treeCopy = tree. However, the effect of this will only be that both the variables treeCopy and tree refer to the same tree data structure that tree originally did. This would be sufﬁcient if you only want to have read access to the data structure from both variables. However, if you want to modify one of the copies, then you need a completely separate copy. To obtain this, the copy routine must allocate memory for each of the nodes in the tree, copy over the information in each node, and link the nodes together in the appropriate way. The following simple recursive algorithm, treeCopy = Copy tree , accomplishes this.  algorithm Copy tree   cid:2  pre-cond cid:3 : tree is a binary tree.  cid:2  post-cond cid:3 : Returns a copy of the tree. begin  if  tree = emptyTree   then  result  emptyTree    else  treeCopy = allocate memory for one node rootInfo treeCopy  = rootInfo tree  leftSub treeCopy  = Copy leftSub tree   rightSub treeCopy  = Copy rightSub tree   result  treeCopy    % copy overall data in root node  % copy left subtree % copy right subtree  end if  end algorithm   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes10 CUUS154-Edmonds 978 0 521 84931 9  Recursion  March 29, 2008  10:3  EXERCISE 10.2.1 Many texts that present recursive algorithms for trees do not con- sider the empty tree to be a valid input instance, but by not considering empty trees the algorithm requires many more cases. Redesign the algorithm of Example 10.1 to return the number of nodes in the input tree without considering the empty tree.  138  EXERCISE 10.2.2 Develop an algorithm that returns the sum of the values within the nodes of a binary tree.  EXERCISE 10.2.3 We have given a recursive algorithm for ﬁnding the height of a binary tree measured in nodes, e.g., a tree with one node has height 1. Rewrite this algorithm so that the height is measured in edges, e.g., a tree with one node has height 0.  EXERCISE 10.2.4 If the computer system does not have garbage collection, then it is the responsibility of the programmer to deallocate the memory used by all the nodes of a tree when the tree is discarded. Develop a recursive algorithm, Deallocate tree , that accomplishes this. How much freedom is there in the order of the lines of the code?  EXERCISE 10.2.5 Develop an algorithm that searches for a key within a binary search tree.  10.3 Generalizing the Problem Solved  Sometimes when writing a recursive algorithm for a problem it is easier to solve a more general version of the problem, providing more information about the original instance or asking for more information about subinstances. Remember, however, that anything that you ask your friend to do, you must be able to do yourself.  EXAMPLE 10.3.1  Is the Tree a Binary Search Tree?  The required algorithm returns whether or not the given tree is a binary search tree  BST .  An Inefﬁcient Algorithm:  algorithm IsBSTtree  tree   cid:2  pre-cond cid:3 : tree is a binary tree.  cid:2  post-cond cid:3 : The output indicates whether it is a binary search tree. begin  if tree = emptyTree  then  return Yes  return Yes  else if  IsBSTtree leftSub tree   and IsBSTtree rightSub tree    and Max leftSub tree   ≤ rootKey tree  ≤ Min rightSub tree    then   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes10 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  10:3  Recursion on Trees  else  end if  return No  end algorithm  Running Time: For each node in the input tree, the above algorithm computes the minimum or the maximum value in the node’s left and right subtrees. Though these operations are relatively fast for binary search trees, performing them for each node increases the time complexity of the algorithm, because each node may be traversed by either the Min or the Max routine many times. Suppose, for example, that the input tree is completely unbalanced, i.e., a single path. For node i, computing the max of its subtree involves traversing to the bottom of the path and takes time n − i. Hence, the  total running time is T n  = cid:1   i=1..n n − i  =  cid:1  n2 . This is far too slow.  139  Ask for More Information about the Subinstance: It is better to combine the IsBSTtree and the Min and Max routines into one routine so that the tree only needs to be traversed once.  In addition to whether or not the tree is a BST, the routine will return the mini- mum and the maximum value in the tree. If our instance tree is the empty tree, then we return that it is a BST with minimum value ∞ and with maximum value −∞.  See Section 8.5.  Otherwise, we ask one friend about the left subtree and another about the right. They tell us the minimum and the maximum values of these and whether they are BST. If both subtrees are BSTs and leftMax ≤ rootKey tree  ≤ rightMin, then our tree is a BST. Our minimum value is min leftMin, rightMin, rootKey tree  , and our maximum value is max leftMax, rightMax, rootKey tree  .  algorithm IsBSTtree  tree   cid:2  pre-cond cid:3 : tree is a binary tree.  cid:2  post-cond cid:3 : The output indicates whether it is a BST. It also gives the minimum and the maximum values in the tree.  begin  if tree = emptyTree  then return  cid:2 Yes, ∞, −∞ cid:3   else  cid:2 leftIs,leftMin,leftMax cid:3  = IsBSTtree leftSub tree      cid:2 rightIs,rightMin,rightMax cid:3  = IsBSTtree  rightSub tree    min = min leftMin, rightMin, rootKey tree   max = max leftMax, rightMax, rootKey tree   if  leftIs and rightIs and leftMax ≤ rootKey tree  ≤ rightMin   then  isBST = Yes isBST = No  else   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes10 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  10:3  Recursion  end if return  cid:2 isBST, min, max cid:3   end if  end algorithm  140  You might ask why the left friend provides the minimum of the left subtree even though it is not used. There are two related reasons. First, the postconditions re- quire her to do so. You can change the postconditions if you like, but whatever con- tract is made, everyone needs to keep it. Second, the left friend does not know that she is the left friend. All she knows is that she is given a tree as input. The algo- rithm designer must not assume that the friend knows anything about the context in which she is solving her problem other than what she is passed within the input instance.  Provide More Information about the Original Instance: Another elegant algo- rithm for the IsBST problem generalizes the problem in order to provide your friend more information about your subinstance. Here the more general problem, in ad- dition to the tree, will provide a range of values [min,max] and ask whether the tree is a BST with values within this range. The original problem is solved using IsBSTtree tree, [−∞, ∞] .  cid:2  algorithm IsBSTtree  cid:2  pre-cond cid:3 : tree is a binary tree. In addition, [min, max] is a range of values.  cid:2  post-cond cid:3 : The output indicates whether it is a BST with values within this range.  tree, [min,max]   cid:3   begin  if tree = emptyTree  then  return Yes  else if   rootKey tree  ∈ [min, max] and IsBSTtree  leftSub tree , [min,rootKey tree ]  and IsBSTtree  rightSub tree ,[rootKey tree ,max]  then  return Yes  return No  else  end if  end algorithm  EXERCISE 10.3.1  See solution in Part Five.  Write a recursive program that takes a BST and an integer k as input and returns the kth smallest element in the tree. Recall that all the nodes in the left subtree are smaller than the root and all those in the right are larger.   P1: ...  Gutter margin: 7 8  Top margin: 3 8  TheNotes10 CUUS154-Edmonds 978 0 521 84931 9   cid:1  cid:1   Recursion on Trees   cid:1  cid:1   2  1  Contents of this node are stored in array element A[3].  3  4  5  6  7  n=9  8  9  1  2  3  4  5  6  7  8  9  March 29, 2008  10:3  141  Figure 10.2: The mapping between the nodes in a balanced binary tree and the elements of an array.  10.4 Heap Sort and Priority Queues  Heap sort is a fast sorting algorithm that is easy to implement. Like quick sort, it has the advantage of being done in place in memory, whereas merge and radix–counting sorts require an auxiliary array of memory to transfer the data to. I include heap sort in this chapter because it is implemented using recursion within a tree data structure.  Completely Balanced Binary Tree: We will visualize the values being sorted as stored in a binary tree that is completely balanced, i.e., every level of the tree is com- pletely full except for the bottom level, which is ﬁlled in from the left.  Array Implementation of a Balanced Binary Tree: Because the tree always has this balanced shape, we do not have to bother with the overhead of having nodes with pointers. In actuality, the values are stored in a simple array A[1, n]. See Figure 10.2. The mapping between the visualized tree structure and the actual array structure is done by indexing the nodes of the tree 1, 2, 3, . . . , n, starting with the root of the tree and ﬁlling each level in from left to right.   cid:1  The root is stored in A[1].  cid:1  The parent of A[i] is A[ cid:9  i  cid:10 ].  cid:1  The left child of A[i] is A[2 · i].  cid:1  The right child of A[i] is A[2 · i + 1].  cid:1  The node in the far right of the bottom level is stored in A[n].  cid:1  If 2i + 1 > n, then the node does not have a right child.  2  Deﬁnition of a Heap: A heap imposes a partial order  see Section 14.6  on the set of values, requiring that the value of each node be greater than or equal to that of each of the node’s children. There are no rules about whether the left or the right child is larger. See Figure 10.3.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes10 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  10:3  Recursion  9  6  8  5  4  7  1  3  2  142  Figure 10.3: An example of nodes ordered into a heap.  Maximum at Root: An implication of the heap rules is that the root contains the maximum value. The maximum may appear repeatedly in other places as well.  Exercise 10.4.1 gives you more practice understanding this deﬁnition.  The Heapify Problem:  Speciﬁcations:  Precondition: The input is a balanced binary tree such that its left and right subtrees are heaps.  That is, it is a heap except that its root might not be larger than that of its children.   Postcondition: Its values are rearranged in place to make it complete heap.  Recursive Algorithm: The ﬁrst task in making this tree into a heap is to put its maximum value at the root. See Figure 10.4. Because the left and right subtrees are heaps, the maxima of these trees are at their roots. Hence, the maximum of the entire tree is either at the root, at its left child node, or at its right child node. You ﬁnd the maximum among these three. If the maximum is at the root, then you are ﬁnished. Otherwise, for the purpose of discussion, assume that the maximum is in the root’s left child. Swap this maximum value with that of the root. The root and the right subtree now form a heap, but the left subtree might not. You will give the subtask of making your left subtree into a heap to a recur- sive friend. Before you can do this, you need to make sure that this subinstance meets the preconditions of the problem. By our precondition, our left subtree was a heap when we received it. We changed its root. After this change, it still has the property that its left and right subtrees are heaps. Hence, the preconditions of our problem are met and you can give your left subtree to your friend. By the postcondition, the friend makes this subtree into a heap. Your entire tree is now a heap.  5  9  8  9  5  8  9  6  8  6  4  7  1  6  4  7  1  5  4  7  1  3  2  3  2  3  2  Figure 10.4: An example computation of Heapify.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes10 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  10:3  Recursion on Trees  Code:  algorithm Heapify r    cid:2  pre-cond cid:3 : The balanced binary tree rooted at A[r ] is such that its left and right subtrees are heaps.  cid:2  post-cond cid:3 : Its values are rearranged in place to make it complete heap.  143  begin  if A[rightchild r  ] is max of {A[r ], A[rightchild r  ], A[leftchild r  ]}   then  swap A[r ], A[rightchild r  ]  Heapify rightchild r     elseif A[leftchild r  ] is max of {A[r ], A[rightchild r  ], A[leftchild r  ]}   then  swap A[r ], A[leftchild r  ]  Heapify leftchild r     else % A[r ] is max of {A[r ], A[rightchild r  ], A[leftchild r  ]}  exit  end if  end algorithm  Running Time: T n  = 1 · T n 2  +  cid:1  1 . From Chapter 27 we know that log a dominated by all levels and T n  =  cid:1  f  n  log n  =  cid:1  log n .  = 0 and f  n  =  cid:1  n0 , so c = 0. Because log a  = = c, we conclude that time is  log 1 log 2  log b  log b  Because this algorithm recurses only once per call, it can easily be made into an iter- ative algorithm.  Iterative Algorithm: A good loop invariant would be “The entire tree is a heap except that nodei might not be greater or equal to both of its children. As well, the value of i’s parent is at least the value of i and of i’s children.” When i is the root, this is the precondition. The algorithm proceeds as in the recursive algorithm. Node i follows one path down the tree to a leaf. When i is a leaf, the whole tree is a heap.  algorithm Heapify r    cid:2  pre-cond cid:3 : The balanced binary tree rooted at A[r ] is such that its left and right subtrees are heaps.  cid:2  post-cond cid:3 : Its values are rearranged in place to make it complete heap.  Code:  begin  i = r loop   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes10 CUUS154-Edmonds 978 0 521 84931 9  Recursion  144  March 29, 2008  10:3   cid:2 loop-invariant cid:3 : The entire tree rooted at A[r ] is a heap except that node i might not be greater or equal to both of its children. As well, the value of i’s parent is at least the value of i and of i’s children. exit when i is a leaf if A[rightchild i ] is max of {A[i], A[rightchild i ],  A[leftchild i ]} then swap A[i], A[rightchild i ]  i = rightchild i  elseif A[leftchild i ] is max of {A[i], A[rightchild i ], A[leftchild i ]}   then  swap A[i], A[leftchild i ]  i = leftchild i   else % A[i] is max of {A[i], A[rightchild i ], A[leftchild i ]}  exit  end if end loop end algorithm  The MakeHeap Problem:  Speciﬁcations:  Running Time: T n  =  cid:1  height of tree  =  cid:1  log n .  Precondition: The input is an array of numbers, which can be viewed as a balanced binary tree of numbers.  Postcondition: Its values are rearranged in place to make it heap.  2   cid:12  of the numbers into a heap, make another  cid:9  n−1  Recursive Algorithm: The obvious recursive algorithm is to recursively make  cid:11  n−1  cid:10  into a heap, and put the remaining number at the root of a tree with these two heaps as children. This now meets the precondition for Heapify, which turns the whole thing into a heap.  2  Running Time: T n  = 2T  n  2   +  cid:1  log n . Again from Chapter 27, time is dominated the base cases and T n  =  cid:1  nlog a  log b  =  cid:1  n .  = 1 and f  n  =  cid:1  n0 log n , so c = 0. Because loga  = > c, we conclude that  loga log b  log 2 log 2  log b  The structure of the recursive tree for this algorithm is very predictable, so it can eas- ily be made into an iterative algorithm, which calls Heapify on exactly the same nodes though in a slightly different order.  Iterative Algorithm: See Figure 10.5. The loop invariant is that all subtrees of height i are heaps. Initially, the leaves of height i = 1 are already heaps. Sup- pose that all subtrees of height i are heaps. The subtrees of height i + 1 have the   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes10 CUUS154-Edmonds 978 0 521 84931 9  Recursion on Trees  March 29, 2008  10:3  Heapify  3  Heapify  3  Heapify  3  Heapify  9  3  6  1  6  1  9  4  7  8  9  4  7  8  5  2  5  2  8  4  7  1  6  9  5  2  9  6  5  2  8  8  4  7  1  4  7  1  6  5  3  2  Figure 10.5: An example of the iterative version of MakeHeap.  property that their left and right subtrees are heaps. Hence, we can use Heapify to make them into heaps. This maintains the loop invariant while increasing i by one. The postcondition clearly follows from the loop invariant and the exit condition that i = log n.  145  Code:  algorithm MakeHeap     cid:2  pre-cond cid:3 : The input is an array of numbers, which can be viewed as a balanced binary tree of numbers.  cid:2  post-cond cid:3 : Its values are rearranged in place to make it a heap. begin  loop k =  cid:9  n   cid:10 ,  cid:9  n Heapify k   2  2   cid:10  − 1,  cid:9  n   cid:10  − 2, . . . , 2, 1  2  end loop end algorithm  a total time of T n  = cid:1 log n  Running Time: The number of subtrees of height i is 2 log n −i, because each such tree has its root at level  log n  − i in the tree. Each take  cid:1  i  to heapify. This gives i=1  2 log n −i i. This sum is geometric. Hence, its total is theta of its maximum term. The ﬁrst term with i = 1 is  2 log n −i i =  cid:1  2log n  =  cid:1  n . The last term with i = log n is  2 log n −i i =  20  log n = log n. The ﬁrst term is the biggest, giving a total time of  cid:1  n . See Chapter 26 for more on approxima- tion summations.  The HeapSort Problem:  Speciﬁcations:  Precondition: The input is an array of numbers.  Postcondition: Its values are rearranged in place to be in sorted order.  Algorithm: The loop invariant is that for some i ∈ [0, n], the n − i largest ele- ments have been removed and are sorted on the side, and the remaining i ele- ments form a heap. See Figures 10.6 and 10.7. The loop invariant is established for i = n by forming a heap from the numbers using the MakeHeap algorithm. When i = 0, the values are sorted.  Suppose that the loop invariant is true for i. The maximum of the remaining values is at the root of the heap. Remove it and put it in its sorted place on the left   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes10 CUUS154-Edmonds 978 0 521 84931 9  Recursion  March 29, 2008  10:3  12  1  8  2  5  4  2  5  3  9  6  7  12  1  8  2  9  3  2  5  23  6  31  7  5  4 57 9  35  12 1  8 8 2  146  8  9  1  2  3  4  5  23 6  31 7  35 8  57 9  9 3  5  4  2  23 6  31 7  35 8  57 9  5  Figure 10.6: The left diagram shows the loop invariant with n − i = 9 − 5 = 4 of the largest elements in the array and the remaining i = 5 elements forming a heap. The right diagram emphasizes the fact that though a heap is viewed as being stored in a tree, it is actually implemented in an array. When some of the elements are in still in the tree and some are in the array, these views overlap.  end of the sorted list. Take the bottom right-hand element of the heap, and ﬁll the newly created hole at the root. This maintains the correct shape of the tree. The tree now has the property that its left and right subtrees are heaps. Hence, you can use Heapify to make it into a heap. This maintains the loop invariant while decreasing i by one.  Array Implementation: The heap sort can occur in place within the array. As the heap gets smaller, the array entries on the right become empty. These can be used to store the sorted list that is on the side. Putting the root element where it  3  BuildHeap  9  6  8  Heapify  8  6  7  5  4  7  1  5  4  7  1  5  4  2  1  3  9  3  9  2  1  2  8  3  3  3  6  6  9  4  9  2  9  8  8  8  6  4  2  3  3  3  Heapify  8  Heapify  5  9  2  9  1  9  8  8  1  Heapify  6  1  9  4  7  5  2  8  3  3  2  Heapify  6  7  8  9  2  Heapify  3  3  3  5  9  2  9  2  9  8  8  8  1  1  4  5  6  7  7  5  3 3  6  9  4  9  2 2  8  8  3  3  1 1  Heapify  8 8  9 9  Figure 10.7: An example computation of HeapSort.  4  5  6  7  4 4  5 5  6 6  7 7  4  5  6  7  4  5  6  7  1  4  6  7  1  2  6  7  1  5  6  7  1  5  6  7  5  4  2  1  5  4  2  1  5  4  2  7  1  4  2  7   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes10 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  10:3  Recursion on Trees  Code:  belongs, putting the bottom left element at the root, and decreasing the size of the heap can be accomplished by swapping the elements at A[1] and at A[i] and decrementing i.  algorithm HeapSort     cid:2  pre-cond cid:3 : The input is an array of numbers.  cid:2  post-cond cid:3 : Its values are rearranged in place to be in sorted order. begin  147  MakeHeap    i = n loop cid:2 loop-invariant cid:3 : The n − i  largest elements have been re- moved and are sorted in A[i + 1, n], and the remaining i ele- ments form a heap in A[1, i]. exit when i = 1 swap A[root], A[i]  i = i − 1 Heapify root   % On a heap of size i.  end loop end algorithm  log i , for a total of T n  =  cid:1  n  + cid:1   Running Time: MakeHeap takes  cid:1  n  time, heapifying a tree of size i takes time 1 i=n logi. This sum behaves like an arithmetic  sum. Hence, its total is n times its maximum value, i.e.,  cid:1  n log n .  Common Mistakes When Describing These Algorithms: Statements that are al- ways true, such as “The root is the max of any heap,” give no information about the state of the program within the loop. For Heapify, “The left subtree and the right sub- tree of the current node are heaps” is useful. However, in the end the subtree becomes a leaf, at which point this loop invariant does not tell you that the whole tree is a heap. For HeapSort, “The tree is a heap” is good, but how do you get a sorted list from this in the end? Do not run routines without making sure that their preconditions are met, such as having HeapSort call Heapify without being sure that the left and right subtrees of the given node are heaps.  Priority Queues: Like stacks and queues, priority queues are an important ADT.  Deﬁnition: A priority queue consists of:  Data: A set of elements, each of which is associated with an integer that is referred to as the priority of the element.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes10 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  10:3  Recursion  Operations:  Insert an Element: An element, along with its priority, is added to the queue. Coding this is left for Exercise 10.4.2.  148  Change Priority: The priority of an element already in the queue is changed. The routine is passed a pointer to the element within the pri- ority queue and its new priority. Coding this is left for Exercise 10.4.3.  Remove an Element: Removes and returns an element of the highest pri- ority from the queue.  Implementations:  separate queue for each priority level  delete, ﬁnd ﬁrst nonempty queue   Implementation  Insert Time Change Time Remove Time  Sorted in an array or linked list by  O n   priority  Unsorted in an array or linked list  O 1   O n   O 1   O 1   O n    To add, go to correct queue; to  O 1   O 1   O No. of priorities   Heaps  O log n   O log n   O log n   Heap Implementation: The elements of a priority queue are stored in a heap or- dered according to the priority of the elements.  Operations:  Remove an Element: The element of the highest priority is at the top of the heap. It can be removed, and the heap then should be reheapiﬁed as done in HeapSort.  Insert an Element: Place the new element in the lower right corner of the heap, and then bubble it up the heap until it ﬁnds the correct place ac- cording to its priority.  Change Priority: The routine is passed a pointer to the element whose priority is changing. After making the change, this element is bubbled either up or down the heap, depending on whether the priority has in- creased or decreased.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes10 CUUS154-Edmonds 978 0 521 84931 9  Recursion on Trees  March 29, 2008  10:3  EXERCISE 10.4.1  See solution in Part Five.  Consider a heap storing the values 1, 2, 3, . . . , 15.  1. Where in the heap can the value 1 go? 2. Which values can be stored in entry A[2]? 3. Where in the heap can the value 15 go? 4. Where in the heap can the value 6 go?  149  EXERCISE 10.4.2 Design an algorithm to insert a new element into the heap imple- mentation of the priority queue.  EXERCISE 10.4.3 Design an algorithm to change the priority of an element in the heap implementation of the priority queue.  10.5 Representing Expressions with Trees  We will now consider how to represent multivariate expressions using binary trees. We will develop the algorithms to evaluate, copy, differentiate, simplify, and print such an expression. Though these are seemingly complex problems, they have sim- ple recursive solutions.  Recursive Deﬁnition of an Expression:   cid:1  Single variables x, y, and z and single real values are themselves expressions.  cid:1  If f and g are expressions, then f + g, f − g, f ∗ g, and f g are also expressions.  Tree Data Structure: The recursive deﬁnition of an expression directly mirrors that of a binary tree. Because of this, a binary tree is a natural data structure for storing an expression.  Conversely, you can use an expression to represent a binary tree.   EXAMPLE 10.5.1  Evaluate Expression  This routine evaluates an expression that is represented by a tree. For example, it can evaluate f = x ∗  y + 7 , with xvalue = 2, yvalue = 3, and zvalue = 5, and return 2 ∗  3 + 7  = 20.  f  *  x  +  y  7   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes10 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  10:3  Recursion  Code:  EXAMPLE 10.5.1  Evaluate Expression  cont.   150  algorithm Eval f, xvalue, yvalue, zvalue   cid:2  pre-cond cid:3 : f is an expression whose only variables are x, y, and z, and xvalue, yvalue, and zvalue are the three real values to assign to these variables.  cid:2  post-cond cid:3 : The returned value is the evaluation of the expression at these values for x, y, and z. The expression is unchanged.  begin  result  f    if  f = a real value   then else if  f = “x”   then result  xvalue   else if  f = “y”   then result  yvalue   else if  f = “z”   then result  zvalue   else if  rootOp f   = “+”   then  result  Eval left Sub tr ee , xvalue, yvalue, zvalue  +Eval rightSub tree , xvalue,yvalue, zvalue     else if  rootOp f   = “−”   then  result  Eval leftSub tree , xvalue, yvalue, zvalue  −Eval rightSub tree , xvalue, yvalue, zvalue     else if  rootOp f   = “*”   then  result  Eval leftSub tree , xvalue, yvalue, zvalue  ×Eval rightSub tree , xvalue, yvalue, zvalue     else if  rootOp f   = “ ”   then  result  Eval leftSub tree , xvalue, yvalue, zvalue    Eval rightSub tree , xvalue, yvalue, zvalue     end if  end algorithm  EXAMPLE 10.5.2 Differentiate Expression  This routine computes the derivative of a given expression with respect to an indicated variable.  Speciﬁcation:  Preconditions: The input consists of  cid:2 f, x cid:3 , where f is an expression represented by a tree and x is a string giving the name of a variable.   cid:1  = df dx. This derivative should be Postconditions: The output is the derivative f an expression represented by a tree whose nodes are separate from those of f . The data structure f should remain unchanged. See Figure 10.8.  Coding this is left for Exercise 10.5.1, and tracing it for Exercise 10.5.2.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes10 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  10:3  151  Recursion on Trees  f 1  ′ f1  +  +  x  1  y  0  f  2  ′ f2  *  +  x  *  y  *  x  y  f  3  ′ f3        −    *  1  y  x  0  *  y  y  1  y  x  0  *  ′ f4     ′  simplify f4     v  x  *  D  x  D  s  −1  v  x  * D  D  x  f4     A  x     C  B  x  D  x  A′     m  −    n  1  *  C  B′  p  * C′  C  1  B  x  x  s  −    u  *  D′  1  A  x     C  B  x  t  *  D  x  q  x  *  C  x  Figure 10.8: Four functions and their derivatives. The fourth derivative has been simpliﬁed.  EXAMPLE 10.5.3  Simplify Expression  This routine simpliﬁes a given expression. For example, the derivative of x ∗ y with respect to x will be computed to be 1 ∗ y + x ∗ 0. This should be simpliﬁed to y. Speciﬁcation:  Preconditions: The input consists of an expression f represented by a tree.  Postconditions: The output is another expression that is a simpliﬁcation of f . Its nodes should be separate from those of f , and f should remain unchanged.  Code:  algorithm Simplify f    cid:2  pre-cond cid:3 : f is an expression.  cid:2  post-cond cid:3 : The output is a simpliﬁcation of this expression. begin  if  f = a real value or a single variable   then  else % f is of the form  g  result  Copy f      cid:1  op h g = Simplify leftSub f    h = Simplify rightSub f      cid:1       P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes10 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  10:3  Recursion  EXAMPLE 10.5.3  Simplify Expression  cont.   152  if  one of the following forms applies:  1 ∗ h = h 0 + h = h 0 h = 0 6 ∗ 2 = 12  g ∗ 1 = g g + 0 = g g 1 = g 6 2 = 3 result  the simpliﬁed form    0 ∗ h = 0 g − 0 = g g 0 = ∞ 6 + 2 = 8  g ∗ 0 = 0 x − x = 0 x x = 1 6 − 2 = 4    then  else  end if  result  g op h    end if  end algorithm  This is traced out in Exercise 10.5.3.  EXERCISE 10.5.1  See solution in Part Five.  Describe the algorithm for the derivative. Do not give the complete code. Only give the key ideas.  EXERCISE 10.5.2 Trace out the execution of the derivative algorithm on the instance f =  x x  x given above. In other words, draw a tree with a box for each time a routine is called. For each box, include only the function f passed and derivative returned.   cid:1   EXERCISE 10.5.3  See solution in Part Five.  Trace out the execution of Simplify on the obtained in Exercise 10.5.1, where f =  x x  x. In other words, draw a derivative f tree with a box for each time a routine is called. For each box, include only the function f passed and the simpliﬁed expression returned.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes11 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  20:31  11 Recursive Images  153  Recursion can be used to construct very complex and beautiful pictures. We begin by combining the same two ﬁxed images recursively over and over again. This pro- duces fractal-like images whose substructures are identical to the whole. Next we will generate random mazes by using randomness to slightly modify these two images so that the substructures are not identical.  11.1 Drawing a Recursive Image from a Fixed Recursive  and a Base Case Image  Drawing an Image: An image is speciﬁed by a set of lines, circles, and arcs and by two points A and B that are referred to as the handles. Before such an image can be drawn on the screen, its location, size, and orientation on the screen need to be speciﬁed. We will do this by specifying two points A and B on the screen. Then a simple program can translate, rotate, scale, and draw the image on the screen in such a way that the two handle points of the image land on these two speciﬁed points on the screen.  Specifying a Recursive Image: A recursive image is speciﬁed by the following:  1. a base case image  2. a recurse image  3. a set of places within the recurse image to recurse  4. the two points A and B on the screen at which the recursive image should be drawn.  5. an integer n. The Base Case: If n = 1, then the base case image is drawn.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes11 CUUS154-Edmonds 978 0 521 84931 9  Recursion  April 5, 2008  20:31  154  Figure 11.1:  a  Man recursively framed;  b  rotating square.  Recursing: If n > 1, then the recursive image is drawn on the screen at the location speciﬁed. Included in the recursive image are a number of places to recurse. These are each depicted by an arrow, —> >—. When the recursive image is translated, rotated, scaled, and drawn on the screen, these arrows are located somewhere on the screen. The arrows themselves are not drawn. Instead, the same picture is drawn recursively at these locations, but with the value n − 1.  Examples:  Man Recursively Framed: See Figure 11.1.a. The base case for this construction consists of a happy face. When n = 1, this face is drawn. The recursive image consists of a man holding a frame. There is one place to recurse within the frame. Hence, when n = 2, this man is drawn with the n = 1 happy face inside it. For n = 3, the man is holding a frame containing the n = 2 image of a man holding a framed n = 1 happy face. The recursive image provided is with n = 5. It consists of a man holding a picture of a man holding a picture of a man holding a picture of . . . a face. In general, the recursive image for n contains R n  = R n − 1  + 1 = n − 1 men and B n  = B n − 1  = 1 happy faces.  Rotating Square: See Figure 11.1.b. This image is constructed similarly to the previous one. Here, however, the n = 1 base case consists of a circle. The recur- sive image consists of a single square with the n − 1 image shrunk and rotated within it. The squares continue to spiral inward until the base case is reached.  Birthday Cake: See Figure 11.2. The birthday cake recursive image is different in that it recurses in two places. The n = 1 base case consists of a single circle. The recursive image consists of a single line with two smaller copies of the n − 1 image drawn above it. In general, the recursive image for n contains R n  =  Base case figure  Recursive figure  A  B  A  B  Figure 11.2: Birthday cake.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes11 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  20:31  Recursive Images  Leaf Figure  Base case figure  Non-base-case figure  A  B  A  B  Figure 11.3: Leaf.  155  2R n − 1  + 1 = 2n−1 − 1 lines from the recursive image and B n  = 2B n − 1  = 2n−1 circles from the base case image.  Leaf: See Figure 11.3. A leaf consists of a single stem plus eight subleaves along it. Each subleaf is an n − 1 leaf. The base case image is empty, and the recursive im- age consists of the stem plus the eight places to recurse. Hence, the n = 1 image is blank. The n = 2 image consists of a lone stem. The n = 3 image is a stem with eight stems for leaves, and so on. In general, the recursive image for n contains R n  = 8R n − 1  + 1 = 1  7  8n−1 − 1  stems from the recursive image.  Fractal: See Figure 11.4. This recursive image is a classic. The base case is a single line. The recursive image is empty except for four places to recurse. Hence, n = 1 consists of the line. n = 2 consists of four lines, forming a line with an equilateral triangle jutting out of it. As n becomes large, the image becomes a snowﬂake. It is a fractal in that every piece of it looks like a copy of the whole.  The classic way to construct it is slightly different than done here. In the clas- sical method, we are allowed the following operation. Given a line, divide it into three equal parts. Replace the middle part with the two equal-length lines form- ing an equilateral triangle. Starting with a single line, construct the fractal by re- peatedly applying this operation to all the lines that appear. In general, the recursive image for n contains B n  = 4B n − 1  = 4n−1 base case lines. The length of each of these lines is L n  = 1  total length of all these lines is B n  · L n  = cid:1    cid:2 n−1. The  cid:2 n−1. As n approaches inﬁnity, the  3 L n − 1  = cid:1   1 3  4 3  fractal becomes a curve of inﬁnite length.  Three–four figure  Base case figure  Non-base-case figure  A  B  A  B  Figure 11.4: Fractal.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes11 CUUS154-Edmonds 978 0 521 84931 9  Recursion  April 5, 2008  20:31  Base case figure  Recursive figure  Base case figure  Recursive figure  156  A  B  A  B  A  B  A  B   A    B    C   Figure 11.5: Three more examples.  EXERCISE 11.1.1  See solution in Part Five.  See Figure 11.5.a. Construct the recursive image that arises from the base case and recursive image for some large n. Describe what is happening.  EXERCISE 11.1.2  See solution in Part Five.  See Figure 11.5.b. Construct the recursive image that arises from the base case and recursive image for some large n. Note that one of the places to recurse is pointing opposite the other. To line the image up with ◦ these arrows, the image must be rotated 180  . The image cannot be ﬂipped.  EXERCISE 11.1.3 See Figure 11.5.c. This construction looks simple enough. The dif- ﬁculty is keeping track of at which corners the circle is. Construct the base case and the recursive image from which the given recursive image arises. Describe what is happening.  11.2 Randomly Generating a Maze  We will use similar methods to generate a random maze. The maze M will be rep- resented by an n × m two-dimensional array with entries from {brick, ﬂoor, cheese}. Walls consist of lines of bricks. A mouse will be able to move along ﬂoor squares in any of the eight directions. The maze generated will not contain corridors as such, but only many small rectangular rooms. Each room will either have one door in one corner of the room or two doors in opposite corners. The cheese will be placed in a room that is chosen randomly from among the rooms that are far enough from the start location.  Precondition: The routine AddWalls is passed a matrix representing the maze as constructed so far and the coordinates of a room within it. The room will have a sur- rounding wall except for one door in one of its corners. The room will be empty of walls. The routine is also passed a ﬂag indicating whether or not cheese should be added somewhere in the room.  Postcondition: The output is the same maze with a randomly chosen submaze added within the indicated room and cheese added as appropriate.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes11 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  20:31  Recursive Images  j  i  157  Cheese  Figure 11.6: A maze containing cheese.  Initial Conditions: To meet the preconditions of AddWalls, the main routine ﬁrst constructs the four outer walls with the top right corner square left as a ﬂoor tile to act as a door into the maze and as the start square for the mouse. Calling AddWalls on this single room completes the maze.  Subinstances: If the indicated room has height and width of at least 3, then the rou- tine AddWalls will choose a single location  i, j   uniformly at random from all those in the room that are not right next to one of its outer walls.  The  i, j   chosen by the top stack frame in Figure 11.6 is indicated.  A wall is added within the room all the way across row i and all the way down column j , subdividing the room into four smaller rooms. To act as a door connecting these four rooms, the square at location  i, j   remains a ﬂoor tile. See Figure 11.7. Then four friends are asked to ﬁll a maze into each of these four smaller rooms. If our room is to have cheese, then one of the three rooms not containing the door to our room is selected to contain the cheese.  Friend 1’s  room  Friend 2’s  room   i, j   Friend 3’s  room  Friend 4’s  room  Figure 11.7: Partitioning a room of the maze.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes11 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  20:31  Recursion Running Time: The time required to construct an n × n maze is  cid:1  n2 . This can be seen two ways. For the easy way, note that a brick is added at most once to any entry of the matrix and that there are  cid:1  n2  entries. The hard way solves the recurrence relation T n  = 4T n 2  +  cid:1  n  =  cid:1  n2 . Searching the Maze: One way of representing a maze is by a graph. Chapter 14 presents a number of iterative algorithms for searching a graph. Section 14.5 presents the recursive version of the depth-ﬁrst search algorithm. All of these could be used by a mouse to ﬁnd the cheese.  158  EXERCISE 11.2.1 Write the code for generating a maze of rooms.  EXERCISE 11.2.2 Tiling: The precondition to the problem is that you are given three integers  cid:3 n, i, j cid:4 , where i and j are in the range 1 to 2n. You have a 2n by 2n square board of squares. You have a sufﬁcient number of tiles each with the shape . Your goal is to place nonoverlapping tiles on the board to cover each of the 2n × 2n tiles except for the single square at location  cid:3 i, j cid:4 . Give a recursive algorithm for this problem in which you place one tile yourself and then have four friends help you. What is your base case?   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes12 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:13  12 Parsing with Context-Free Grammars  159  An important computer science problem is parsing a string according a given context-free grammar. A context-free grammar is a means of describing which strings of characters are contained within a particular language. It consists of a set of rules and a start nonterminal symbol. Each rule speciﬁes one way of replacing a nontermi- nal symbol in the current string with a string of terminal and nonterminal symbols. When the resulting string consists only of terminal symbols, we stop. We say that any such resulting string has been generated by the grammar.  Context-free grammars are used to understand both the syntax and the seman- tics of many very useful languages, such as mathematical expressions, Java, and En- glish. The syntax of a language indicates which strings of tokens are valid sentences in that language. The semantics of a language involves the meaning associated with strings. In order for a compiler or natural-language recognizers to determine what a string means, it must parse the string. This involves deriving the string from the grammar and, in doing so, determining which parts of the string are noun phrases, verb phrases, expressions, and terms.  Some context-free grammars have a property called look ahead one. Strings from such grammars can be parsed in linear time by what I consider to be one of the most amazing and magical recursive algorithms. This algorithm is presented in this chap- ter. It demonstrates very clearly the importance of working within the friends level of abstraction instead of tracing out the stack frames: Carefully write the speciﬁca- tions for each program, believe by magic that the programs work, write the programs calling themselves as if they already work, and make sure that as you recurse, the instance being input gets smaller.  In Section 19.8 we will analyze an elegant dynamic programming algorithm that parses a string from any context-free grammar, not just look ahead one, in  cid:1  n3  time.  The Grammar: We will look at a very simple grammar that considers expressions over × and +. In this grammar, a factor is either a simple integer or a more complex   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes12 CUUS154-Edmonds 978 0 521 84931 9  Recursion  April 2, 2008  20:13  expression within brackets; a term is one or more factors multiplied together; and an expression is one or more terms added together. More precisely:  160  ⇒ term + term + . . . + term  ⇒ fact ∗ fact ∗ ··· ∗ fact  exp ⇒ term  term ⇒ fact  fact ⇒ int  ⇒  exp   Nonterminals, Terminals, and Rules: More generally, a grammar is deﬁned by a set of nonterminals, a set of terminals, a start nonterminal, and a set of rules. Here the nonterminals are ‘exp,’ ‘term,’ and ‘fact.’ The terminals are integers, the character ‘+,’ and the character ‘*’. The start nonterminal is ‘exp.’ The preceding display gives the list of rules for this grammar.  A Derivation of a String: A grammar deﬁnes a language of strings that can be de- rived in the following way. A derivation of a string starts with the start symbol  a non- terminal . Then each rule, like those just given, says that you can replace the nonter- minal on the left with the string of terminals and nonterminals on the right.  A Parsing of an Expression: Let s be a string consisting of terminals. A parsing of this string is a tree. Each internal node of the tree is labeled with a nonterminal sym- bol, and the root with the start nonterminal. Each internal node must correspond to a rule of the grammar. For example, for rule A ⇒ BC, the node is labeled A and its two children are labeled B and C. The leaves of the tree, read left to right, give the input string s of terminals. Figure 12.1 is an example.  exp  term  fact  fact  term fact exp  fact  term  fact  term  fact  fact  fact  fact exp  term fact 2  term fact 42  term  fact exp  term fact 5  term fact  + 12  6  *  8  +        +     *        +  978  *  7  *  123  +  15  *  54     Figure 12.1: A parse tree for the string s = 6 ∗ 8 +   2 + 42  ∗  5 + 12  + 987 ∗ 7 ∗ 123 + 15 ∗ 54 .   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes12 CUUS154-Edmonds 978 0 521 84931 9  Parsing with Context-Free Grammars  April 2, 2008  20:13  161  The Parsing Abstract Data Type: The following is an example where it is use- ful not to give the full implementation details of an abstract data type. If fact, we will even leave the speciﬁcation of parsing structure open for the implementer to decide. For our purposes, we will only say the following: When p is a variable of type parsing, we will use “p = 5” to indicate that it is assigned a parsing of the expression ‘5.’ We will go on to overload the operations ∗ and + as operations that join two pars- ings into one. For example, if p1 is a parsing of the expression ‘2 ∗ 3’ and p2 of ‘5 ∗ 7’, then we will use p = p1 + p2 to denote a parsing of the expression ‘2 ∗ 3’ + 5 ∗ 7.  The implementer deﬁnes the structure of a parsing by specifying in more detail what these operations do. For example, if the implementer wants a parsing to be a binary tree representing the expression, then p1 + p2 would be the operation of con- structing a binary tree with the root being a new ‘+’ node, the left subtree being the binary tree p1, and the right subtree being the binary tree p2. On the other hand, if the implementer wants a parsing to be simply an integer evaluation of the expres- sion, then p1 + p2 would be the integer sum of the integers p1 and p2.  Speciﬁcations for the Parsing Algorithm:  Precondition: The input consists of a string of tokens s. The possible tokens are the characters ‘*’ and ‘+’ and arbitrary integers. The tokens are indexed as s[1], s[2], s[3], . . . , s[n].  Postcondition: If the input is a valid expression generated by the grammar, then the output is a parsing of the expression. Otherwise, an error message is output.  The algorithm consists of one routine for each nonterminal of the grammar: GetExp, GetTerm, and GetFact.  Speciﬁcations for GetExp:  Precondition: The input of GetExp consists of a string s of tokens and an index i that indicates a starting point within s. Postcondition: The output consists of a parsing of the longest substring s[i], s[i + 1], . . . , s[j − 1] of s that starts at index i and is a valid expression. The output also includes the index j of the token that comes immediately after the parsed expression.  If there is no valid expression starting at s[i], then an error message is output.  The speciﬁcations for GetTerm and GetFact are the same as for GetExp, except that they return the parsing of the longest term or factor starting at s[i] and ending at s[j − 1].   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes12 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:13  Recursion  Examples of GetExp, GetTerm, and GetFact: See Figure 12.2  162  Figure 12.2: Example input instances are given for Get Exp, Get Term, and Get Fact. The string s is the same for all examples. The beginning of the rectangle indicates the input index i at which the parsing should begin. The contents of the rectangle indicates resulting parsing. The end of the rectangle indicates the output index j at which the parsing ends.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes12 CUUS154-Edmonds 978 0 521 84931 9  Parsing with Context-Free Grammars  April 2, 2008  20:13  163  Reasoning for GetExp: Consider some input string s and some index i. The longest substring s[i], . . . , s[j − 1] that is a valid expression consists of some number of terms added together. In all of these cases, it begins with a term. By magic, assume that the GetTerm routine already works. Calling GetTerm s, i  will return pterm and jterm, where pterm is the parsing of this ﬁrst term and jterm indexes the token immediately after this term. Speciﬁcally, if the expression has another term then jterm indexes the ‘+’ that is between these terms. Hence, we can determine whether there is another term by checking s[jterm]. If s[jterm] = ‘+,’ then GetExp will call GetTerm again to get the next term. If s[jterm] is not a ‘+’ but some other character, then GetExp is ﬁnished reading in all the terms. GetExp then constructs the parsing consisting of all of these terms added together.  The reasoning for GetTerm is the same.  GetExp Code:  algorithm GetExp  s, i   cid:4  pre-cond cid:5 : s is a string of tokens, and i is an index that indicates a starting point within s.  cid:4  post-cond cid:5 : The output consists of a parsing p of the longest substring s[i], s[i + 1], . . . , s[j − 1] of s that starts at index i and is a valid expression. The output also includes the index j of the token that comes immediately after the parsed expression. begin  if  i > s  return “Error: Expected characters past end of string.” end if  cid:4 p cid:4 term,1 cid:5 , j cid:4 term,1 cid:5  cid:5  = GetTerm s, i  k = 1 loop cid:4 loop-invariant cid:5 : The ﬁrst k terms of the expression have been  read. exit when s[j cid:4 term,k cid:5 ]  cid:6 = ‘+’  cid:4 p cid:4 term,k+1 cid:5 , j cid:4 term,k+1 cid:5  cid:5  = GetTerm s, j cid:4 term,k cid:5  + 1  k = k + 1  end loop pexp = p cid:4 term,1 cid:5  + p cid:4 term,2 cid:5  + . . . + p cid:4 term,k cid:5  jexp = j cid:4 term,k cid:5  return  cid:4 pexp, jexp cid:5   end algorithm  GetTerm Code:  algorithm GetTerm  s, i   cid:4  pre-cond cid:5 : s is a string of tokens, and i is an index that indicates a starting point within s.  cid:4  post-cond cid:5 : The output consists of a parsing p of the longest substring s[i], s[i + 1], . . . , s[j − 1] of s that starts at index i and is a valid term. The output also in- cludes the index j of the token that comes immediately after the parsed term.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes12 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:13  Recursion  begin  164  if  i > s  return “Error: Expected characters past end of string.” end if  cid:4 p cid:4 fact,1 cid:5 , j cid:4 fact,1 cid:5  cid:5  = GetFact s, i  k = 1 loop   cid:4 loop-invariant cid:5 : The ﬁrst k facts of the term have been read. exit when s[j cid:4 fact,k cid:5 ]  cid:6 = ‘*’  cid:4 p cid:4 fact,k+1 cid:5 , j cid:4 fact,k+1 cid:5  cid:5  = GetFact s, j cid:4 fact,k cid:5  + 1  k = k + 1  end loop pterm = p cid:4 fact,1 cid:5  ∗ p cid:4 fact,2 cid:5  ∗ ··· ∗ p cid:4 fact,k cid:5  jterm = j cid:4 fact,k cid:5  return  cid:4 pterm, jterm cid:5   end algorithm  Reasoning for GetFact: The longest substring s[i], . . . , s[j − 1] that is a valid factor has one of the following two forms:  fact ⇒ int fact ⇒  exp   Hence, we can determine which form the factor has by testing s[i]. If s[i] is an integer, then we are ﬁnished. pfact is a parsing of this single integer s[i], and jfact = i + 1. The +1 moves the index past the integer. If s[i] = ‘ ’, then for s to be a valid factor there must be a valid expression start- ing at jterm + 1, followed by a closing bracket ‘ ’. We can parse this expression with GetExp s, jterm + 1 , which returns pexp and jexp. The closing bracket after the expres- sion must be in s[jexp]. Our parsed factor will be pfact =  pexp  and jfact = jexp + 1. The +1 moves the index past the ‘ ’.  If s[i] is neither an integer nor a ‘ ’, then it cannot be a valid factor. Give a mean-  ingful error message.  GetFact Code:  algorithm GetFact  s, i   cid:4  pre-cond cid:5 : s is a string of tokens and i is an index that indicates a starting point within s.  cid:4  post-cond cid:5 : The output consists of a parsing p of the longest substring s[i], s[i + 1], . . . , s[j − 1] of s that starts at index i and is a valid factor. The output also includes the index j of the token that comes immediately after the parsed factor. begin  if  i > s  return “Error: Expected characters past end of string.” end if if  s[i] is an int  pfact = s[i] jfact = i + 1 return  cid:4 pfact, jfact cid:5    P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes12 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:13  165  Parsing with Context-Free Grammars  else if  s[i] = ‘ ’    cid:4 pexp, jexp cid:5  = GetExp s, i + 1   cid:2  pfact = cid:1  if  s[jexp] = ‘ ’  pexp jfact = jexp + 1 return  cid:4 pfact, jfact cid:5   Output “Error: Expected ‘ ’ at index jexp”  Output “Error: Expected integer or ‘ ’ at index i”  else  end if  else  end if  end algorithm  Tree of Stack Frames: GetExp calls GetTerm, which calls GetFact, which may call GetExp, and so on. If one were to draw out the entire tree of stack frames showing who calls whom, this would exactly mirror the parse tree that it created. See Exer- cise 12.0.1.  Running Time: We prove that the running time of this entire computation is linear in the size of the parse tree produced, which in turn is linear in the size  cid:1  n  of the input string.  To prove the ﬁrst, it is sufﬁcient to prove that the running time of each stack frame either is constant or is linear in the number of children of the node in the parse tree that this stack frame produces. For example, if the stack frame for GetFact ﬁnds an integer, then its node in the parse tree has no children, but GetFact uses only a constant amount of time. In contrast, if a stack frame for GetExp reads in t terms, then its running time will be some constant times t, and its node in the parse tree will have t children.  We now prove that the size to the parse tree produced is linear in the size  cid:1  n  of the input string. If the grammar is such that every nonterminal goes to at least one terminal or at least two nonterminals, then each node in the parse tree either is a leaf or has at least two children. It follows that the number of nodes in the parse tree will be at most some constant times the number of leaves, which is the size of the input string. In our grammar, however, an expression might go to a single term, which can go to a single factor. This creates a little path of outdegree one. It cannot, however, be longer than this, because a factor either is a leaf or has three children: one is ‘ ’, the second an expression, and the third ‘ ’. Such little paths can only increase the size of the parse tree by a factor of 3.  In conclusion, the running time is  cid:1  n .  Proof of Correctness: To prove that a recursive program works, we must consider the size of an instance. The routine need only consider the postﬁx s[i], s[i + 1], . . ., which contains s − i + 1 characters. Hence, we will deﬁne the size of the instance   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes12 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:13  166  s[i], and all three routines return an error message. It follows that H 0  is true.  Recursion  cid:4 s, i cid:5  to be  cid:4 s, i cid:5  = s − i + 1. Let H n  be the statement “Each of GetFac, GetTerm, and GetExp works on the instance  cid:4 s, i cid:5  when  cid:4 s, i cid:5  = s − i + 1 ≤ n.” We prove by induction that ∀n ≥ 0, H n . If  cid:4 s, i cid:5  = 0, then i > s: There is no valid expression, term, or factor starting at If  cid:4 s, i cid:5  = 1, then there is one remaining token: For this to be a factor, term, or expression, it must be a single integer. GetFac is written to give the correct answer in this situation. GetTerm gives the correct answer, because it calls GetFac. GetExp gives the correct answer, because it calls GetTerm, which in turn calls GetFac. It follows that H 1  is true. Assume H n − 1  is true, that is, that each of GetFac, GetTerm, and GetExp works on instances of size at most n − 1. Consider GetFac s, i  on an instance of size s − i + 1 = n. It makes at most one subroutine call, GetExp s, i + 1 . The size of this instance is s −  i + 1  + 1 = n − 1. Hence, by assumption, this subroutine call returns the correct answer. Because all of GetFac s, i ’s subroutine calls return the correct answer, it follows that GetFac s, i  works on all instances of size n. Now consider GetTerm s, i  on an instance of size s − i + 1 = n. It calls GetFac some number of times. The input instance for the ﬁrst call GetFac s, i  still has size n. Hence, the induction hypothesis H n − 1  does not claim that it works. However, the previous paragraph proves that this routine does in fact work on instances of size n. The remaining calls are on smaller instances. Finally, consider GetExp s, i  on an instance  cid:4 s, i cid:5  of size s − i + 1 = n. We use  the previous paragraph to prove that is ﬁrst subroutine call GetTerm s, i  works.  In conclusion, all three work on all instances of size n and hence on H n . This  completes the induction step.  Look Ahead One: A grammar is said to be look ahead one if, given any two rules for the same nonterminal, the ﬁrst place that the rules differ is a difference in a terminal.  Equivalently, the rules can be viewed as paths down a tree.  This feature allows our parsing algorithm to look only at the next token in order to decide what to do next. Thus the algorithm runs in linear time. An example of a good set of rules would be  A ⇒ B ‘u’ C ‘w’ E A ⇒ B ‘u’ C ‘x’ F A ⇒ B ‘u’ C A ⇒ B ‘v’ G H  A  B  v  G  H  u  C  w x  E F   Actually, even this grammar could also be problematic if when s = ‘bbbucccweee,’ B could either be parsed as ‘bbb’ or as ‘bbbu.’ Having B eat the ‘u’ would be a problem.    P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes12 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:13  167  Parsing with Context-Free Grammars  An example of a bad set of rules would be A ⇒ BC A ⇒ DE  With such a grammar, you would not know whether to start parsing the string as a B or a D. If you made the wrong choice, you would have to back up and repeat the process. EXERCISE 12.0.1  See solution in Part Five.  Consider s =    1  ∗ 2 + 3  ∗ 5 ∗ 6 + 7 . 1. Give a derivation of the expression s. 2. Draw the tree structure of the expression s. 3. Trace out the execution of your program on GetExp s, 1 . In other words, draw a tree with a box for each time a routine is called. For each box, include only whether it is an expression, term, or factor and the string s[i], . . . , s[j − 1] that is parsed.  EXERCISE 12.0.2 Consider a grammar G that includes the four lookahead rules for A. Give the code for GetA  s, i  that is similar to that for GetExp  s, i . We can assume that it can be parsed, so do not bother with error detection.  The loop invariant is that you have parsed a preﬁx s[i], s[i + 1], . . . , s[j  EXERCISE 12.0.3 If you are feeling bold, try to write a recursive program for a generic parsing algorithm. The input is  cid:4 G, T, s, i cid:5 , where G is a look-ahead-one grammar, T is a nonterminal of G, s is a string of terminals, and i is an index. The output consists of a parsing of the longest substring s[i], s[i + 1], . . . , s[j − 1] of s that starts at index i and is a valid T according to the grammar G. In other words, the parsing starts with nonterminal T and ends with the string s[i], s[i + 1], . . . , s[j − 1]. The output also in- cludes the index j of the token that comes immediately after the parsed expression. For example, GetExp s, i  is the same as calling this algorithm on  cid:4 G, exp, s, i cid:5  where G is the grammar given above.  cid:1  − 1] of s,  cid:1  + 1], . . . , s[j − 1], producing a partial parsing, p and the rest of the string, s[j will be parsed using one of the partial rules in the set R. For example, suppose the grammar G includes the four lookahead rules for A given above, we are starting with the non-terminal T = A, and we are parsing the string s = ‘bbbucccweee’. Ini- tially, we have parsed nothing, and R contains all of each of the four rules, namely R = {BuCwE, BuCxF, BuC, BvGH}. After two iterations, we have parsed ‘bbbu’ using a parsing pB for ‘bbb’ followed by the character ‘u’. We must parse the rest of the string cccweee using one of the rules in R = {CwE, CxF, C}. Note that the used-up preﬁx Bu from the consistent rules and the inconsistent rules were deleted. Because the gram- mar is look ahead one, we know that either the ﬁrst token in each rule of R is the same nonterminal B, or each rule of R begins with a terminal or is the empty rule. These are the two cases your iteration needs to deal with.  ], s[j   cid:1    P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes12 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:13  168   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes13 CUUS154-Edmonds 978 0 521 84931 9  March 24, 2008  15:50  PART THREE  Optimization Problems  169   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes13 CUUS154-Edmonds 978 0 521 84931 9  March 24, 2008  15:50  170   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes13 CUUS154-Edmonds 978 0 521 84931 9  March 24, 2008  15:50  13 Deﬁnition of Optimization Problems  171  Many important and practical problems can be expressed as optimization problems. Such problems involve ﬁnding the best of an exponentially large set of solutions. It can be like ﬁnding a needle in a haystack. The obvious algorithm, considering each of the solutions, takes too much time because there are so many solutions. Some of these problems can be solved in polynomial time using network ﬂow, linear pro- gramming, greedy algorithms, or dynamic programming. When not, recursive back- tracking can sometimes ﬁnd an optimal solution for some instances in some prac- tical applications. Approximately optimal solutions can sometimes be found more easily. Random algorithms, which ﬂip coins, sometimes have better luck. However, for the most optimization problems, the best known algorithm require 2 cid:1  n  time on the worst case input instances. The commonly held belief is that there are no polynomial-time algorithms for them  though we may be wrong . NP-completeness helps to justify this belief by showing that some of these problems are universally hard amongst this class of problems. I now formally deﬁne this class of problems.  Ingredients: An optimization problem is speciﬁed by deﬁning instances, solutions, and costs.  Instances: The instances are the possible inputs to the problem.  Solutions for Instance: Each instance has an exponentially large set of solutions. A solution is valid if it meets a set of criteria determined by the instance at hand.  Measure of Success: Each solution has an easy-to-compute cost, value, or mea- sure of success that is to be minimized or maximized.  Speciﬁcation of an Optimization Problem:  Preconditions: The input is one instance.  Postconditions: The output is one of the valid solutions for this instance with optimal  minimum or maximum as the case may be  measure of success.  The solution to be outputted need not be unique.    P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes13 CUUS154-Edmonds 978 0 521 84931 9  March 24, 2008  15:50  Optimization Problems  Examples:  172  Longest Common Subsequence: This is an example for which we have a poly- nomial-time algorithm.  Instances: An instance consists of two sequences, e.g., X =  cid:2 A , B, C, B, D, A , B cid:3  and Y =  cid:2 B, D, C, A , B, A cid:3 . Solutions: A subsequence of a sequence is a subset of the elements taken in the same order. For example, Z =  cid:2 B, C, A cid:3  is a subsequence of X =  cid:2 A , B, C, B, D, A, B cid:3 . A solution is a sequence Z that is a subsequence of both X and Y. For example, Z =  cid:2 B, C, A cid:3  is solution, because it is a subsequence common to both X and Y  Y =  cid:2 B, D, C, A, B, A cid:3  . Measure of Success: The value of a solution is the length of the common subsequence, e.g., Z = 3. Goal: Given two sequences X and Y, the goal is to ﬁnd the longest common subsequence  LCS for short . For the example given above, Z =  cid:2 B, C, B, A cid:3  is a longest common subsequence.  Course Scheduling: This is an example for which we do not have a polynomial- time algorithm.  Instances: An instance consists of the set of courses speciﬁed by a university, the set of courses that each student requests, and the set of time slots in that courses can be offered.  Solutions: A solution for an instance is a schedule that assigns each course a time slot.  Measure of Success: A conﬂict occurs when two courses are scheduled at the same time even though a student requests them both. The cost of a schedule is the number of conﬂicts that it has.  Goal: Given the course and student information, the goal is to ﬁnd the schedule with the fewest conﬂicts.  Airplane: The following is an example of a practical problem.  Instances: An instance speciﬁes the requirements of a plane: size, speed, fuel efﬁciency, etc.  Solutions: A solution for an instance is a speciﬁcation of a plane, right down to every curve and nut and bolt.  Measure of Success: The company has a way of measuring how well the speciﬁcation meets the requirements.  Goal: Given plane requirements, the goal is to ﬁnd a speciﬁcation that meets them in an optimal way.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes14 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:15  14 Graph Search Algorithms  173  An optimization problem requires ﬁnding the best of a large number of solutions. This can be compared to a mouse ﬁnding cheese in a maze. Graph search algorithms provide a way of systematically searching through this maze of possible solutions.  Another example of an optimization problem is ﬁnding the shortest path be- tween two nodes in a graph. There may be an exponential number of paths between these two nodes. It would take too much time to consider each such path. The algo- rithms used to ﬁnd a shortest one demonstrate many of the principles that will arise when solving harder optimization problems.  A surprisingly large number of problems in computer science can be expressed as graph theory problems. In this chapter, we will ﬁrst learn a generic search algo- rithm that ﬁnds more and more of the graph by following arbitrary edges from nodes that have already been found. We also consider the more speciﬁc orders of depth-ﬁrst and breadth-ﬁrst search to traverse the graph.  Using these ideas, we are able to solve the optimization problem of discovering shortest paths between pairs of nodes and to learn about the structure of the graph.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes14 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:15  Optimization Problems  14.1 A Generic Search Algorithm  The Reachability Problem:  174  Preconditions: The input is a graph G  either directed or undirected  and a source node s.  Postconditions: The output consists of all the nodes u that are reachable by a path in G from s.  Basic Steps: Suppose you know that node u is reachable from s  denoted as s −→ u  and that there is an edge from u to v. Then you can conclude that v is reachable from s  i.e., s −→ u → v . You can use such steps to build up a set of reachable nodes.   cid:1  s has an edge to v4 and v9. Hence, v4 and v9 are reachable.  cid:1  v4 has an edge to v7 and v3. Hence, v7 and v3 are reachable.  cid:1  v7 has an edge to v2 and v8. . . .  Difﬁculties:  Data Structure: How do you keep track of all this?  Exit Condition: How do you know that you have found all the nodes? Halting: How do you avoid cycling, as in s → v4 → v7 → v2 → v4 → v7 → v2 → v4 → v7 → v2 → v4 → ···, forever?  Ingredients of the Loop Invariant:  Found: If you trace a path from s to a node, then we will say that the node has been found.  Handled: At some point in time after node u has been found, you will want to follow all the edges from u and ﬁnd all the nodes v that have edges from u. When you have done that for node u, we say that it has been handled.  Data Structure: You must maintain  1  the set of nodes foundHandled that have been found and handled and  2  the set of nodes foundNotHandled that have been found but not handled. See Figure 14.1.  The Loop Invariant:  LI1: For each found node v, we know that v is reachable from s, because we have traced out a path s −→ v from s to it. LI2: If a node has been handled, then all of its neighbors have been found.  These loop invariants are simple enough that establishing and maintaining them should be easy. But do they sufﬁce to prove the postcondition? We will see.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes14 CUUS154-Edmonds 978 0 521 84931 9  Graph Search Algorithms  s  s  s  s  s  s  April 2, 2008  20:15  175  foundHandled  foundNotHandled  Node just handled  Figure 14.1: The generic search algorithm handles one found node at a time by ﬁnding its neighbors.  Body of the Loop: A reasonable step would be:   cid:1  Choose some node u from foundNotHandled, and handle it. This involves follow-   cid:1  Newly found nodes are now added to the set foundNotHandled  if they have not  ing all the edges from u.  been found already .   cid:1  u is moved from foundNotHandled to foundHandled.  Code:  algorithm GenericSearch  G, s   cid:3  pre-cond cid:4 : G is a  directed or undirected  graph, and s is one of its nodes.  cid:3  post-cond cid:4 : The output consists of all the nodes u that are reachable by a path in G from s.  begin  foundHandled = ∅ foundNotHandled = {s} loop cid:3 loop-invariant cid:4 : See LI1, LI2. exit when foundNotHandled = ∅ let u be some node from foundNotHandled for each v connected to u  if v has not previously been found then  add v to foundNotHandled  end if  end for   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes14 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:15  Optimization Problems  end loop return foundHandled  end algorithm  move u from foundNotHandled to foundHandled  176  Maintaining the Loop Invariant   cid:1 LI  cid:1  Suppose that LI  the statement of the loop invariant before the iteration  is true, the exit condition  cid:3  exit cid:4  is not, and we have executed another iteration of the algorithm.   cid:2  cid:3  & not  cid:1 exit cid:3  & codeloop →  cid:1 LI   cid:2  cid:2  cid:3  :  Maintaining LI1: After the iteration, the node v is considered found. Hence, in order to maintain the loop invariant, we must be sure that v is reachable from s. Because u was in foundNotHandled, the loop invariant assures us that we have traced out a path s −→ u to it. Now that we have traced the edge u → v, we have traced a path s −→ u → v to v. Maintaining LI2: Node u is designated handled only after ensuring that all its neighbors have been found.  The Measure of Progress: The measure of progress requires the following three properties:  Progress: We must guarantee that our measure of progress increases by at least one every time around the loop. Otherwise, we may loop forever, making no progress.  Bounded: There must be an upper bound on the progress required before the loop exits. Otherwise, we may loop forever, increasing the measure of progress to inﬁnity.  Conclusion: When sufﬁcient progress has been made to exit, we must be able to conclude that the problem is solved.  An obvious measure would be the number of found nodes. The problem is that when handling a node, you may only ﬁnd nodes that have already been found. In such a case, no progress is actually made.  A better measure of progress is the number of nodes that have been handled. We can make progress simply by handling a node that has not yet been handled. We also know that if the graph G has only n nodes, then this measure cannot increase past n.  Exit Condition: Given our measure of progress, when are we ﬁnished? We can only handle nodes that have been found and not handled. Hence, when all the nodes that have been found have also been handled, we can make no more progress. At this point, we must stop. Initial Code   cid:1 pre-cond  cid:3  & codepre-loop ⇒ cid:1 loop-invariant  cid:3  : Initially, we know only that s is reachable from s. Hence, let’s start by saying that s is found but not handled and that all other nodes have not yet been found.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes14 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:15  Graph Search Algorithms Exiting Loop   cid:1 LI  cid:3  &  cid:1 exit  cid:3  →  cid:1  post  cid:3  : Our output will be the set of found nodes. The postcondition requires the following two claims to be true.  Claim: Found nodes are reachable from s.  This is clearly stated in the loop invariant.  Claim: Every reachable node has been found. A logically equivalent statement is that every node that has not been found is not reachable.  177  One Proof: Draw a circle around the nodes of the graph G that have been found. If there are no edges going from the inside of the circle to the outside of the circle, then there are no paths from s to the nodes outside the circle. Hence, we can claim we have found all the nodes reachable from s. How do we know that this circle has no edges leaving it? Consider a node u in the circle. Because u has been found and foundNotHandled = ∅, we know that u has also been handled. By the loop invariant LI2, if  cid:3 u, v cid:4  is an edge, then v has been found and thus is in the circle as well. Hence, if u is in the circle and  cid:3 u, v cid:4  is an edge, then v is in the circle as well  i.e., no edges leave the circle .  Closure Property: This is known as a closure property. See Section 18.3.3 for more information on this property.  Another Proof: Proof by contradiction.  Suppose that w is reachable from s and that w has not been found. Consider a path from s to w. Because s has been found and w has not, the path starts in the set of found nodes and at some point leaves it. Let  cid:3 u, v cid:4  be the ﬁrst edge in the path for which u but not v has been found. Because u has been found and foundNotHandled = ∅, it follows that u has been handled. Because u has been handled, v must be found. This contradicts the deﬁnition of v.  Running Time:  A Simple but False Argument: For every iteration of the loop, one node is han- dled, and no node is handled more than once. Hence, the measure of progress  the number of nodes handled  increases by one with every loop. G only has V = n nodes. Hence, the algorithm loops at most n times. Thus, the running time is O n .  This argument is false, because while handling u we must consider v for ev-  ery edge coming out of u.  Overestimation: Each node has at most n edges coming out of it. Hence, the run- ning time is O n2 .  Correct Complexity: Each edge of G is looked at exactly twice, once from each direction. The algorithm’s time is dominated by this fact. Hence, the running time is O E , where E is the set of edges in G.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes14 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  April 2, 2008  20:15  The Order of Handling Nodes: This algorithm speciﬁcally did not indicate which node u to select from foundNotHandled. It did not need to, because the algorithm works no matter how this choice is made. We will now consider speciﬁc orders in which to handle the nodes and speciﬁc applications of these orders.  178  Queue  Breadth-First Search : One option is to handle nodes in the order they are found in. This treats foundNotHandled as a queue: “ﬁrst in, ﬁrst out.” The effect is that the search is breadth ﬁrst, meaning that all nodes at distance 1 from s are handled ﬁrst, then all those at distance 2, and so on. A byproduct of this is that we ﬁnd for each node v a shortest path from s to v. See Section 14.2.  Priority Queue  Shortest  Weighted  Paths : Another option calculates for each node v in foundNotHandled the minimum weighted distance from s to v along any path seen so far. It then handles the node that is closest to s according to this approximation. Because these approximations change throughout time, foundNotHandled is implemented using a priority queue: “highest current prior- ity out ﬁrst.” Like breadth-ﬁrst search, the search handles nodes that are closest to s ﬁrst, but now the length of a path is the sum of its edge weights. A byproduct of this method is that we ﬁnd for each node v the shortest weighted path from s to v. See Section 14.3.  Stack  Depth-First Search : Another option is to handle the node that was found most recently. This method treats foundNotHandled as a stack: “last in, ﬁrst out.” The effect is that the search is depth ﬁrst, meaning that a particular path is fol- lowed as deeply as possible into the graph until a dead end is reached, forcing the algorithm to backtrack. See Section 14.4.  EXERCISE 14.1.1 Try searching the following graph using queue  breadth-ﬁrst search , priority queue  shortest  weighted  paths , and stack  depth-ﬁrst search .  6  4  1  f  a  2  e  b  3  2  s  5  1  5  g  1  1  7  d  h  5  c  1  1   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes14 CUUS154-Edmonds 978 0 521 84931 9  Graph Search Algorithms  April 2, 2008  20:15  14.2 Breadth-First Search for Shortest Paths  We will now develop an algorithm for an optimization problem called the shortest- path problem. The algorithm uses a breadth-ﬁrst search. This algorithm is a less generic version of the algorithm in Section 14.1, because the order in which the nodes are handled is now speciﬁed more precisely. The loop invariants are strengthened in order to solve the shortest-path problem.  179  The Shortest-Path Problem  Multiple Sink : Generally, the shortest-path prob- lem ﬁnds a shortest path between a source node s and a sink node t in a graph G. Here, however, we will consider the case where we simultaneously consider all nodes v to be the sink.  Precondition:  cid:3 G, s cid:4  consists of a graph G and a source node s. The graph G can be directed or undirected.  Postconditions: The output consists of a d v  and a π v  for each node of G. It has the following properties:  1. For each node v, d v  gives the length δ s, v  of the shortest path from s to v.  2. The shortest-path or breadth-ﬁrst search tree is deﬁned using a function π as follows: s is the root of the tree. π v  is the parent of v in the tree. For each node v, one of the shortest paths from s to v is given back- ward, with v, π v , π π v  , π π π v   , . . . , s. A recur- sive deﬁnition is that this shortest path from s to v is the given shortest path from s to π v , followed by the edge  cid:3 π v , v cid:4 .  π π π π   s  =  v      v     π π π  π π   v    π   v   v  An Optimization Problem: The single-source, single-sink version of the shortest- path problem can be viewed as an optimization problem. See Chapter 13.  Instances: An instance  cid:3 G, s, t cid:4  consists of a graph and two nodes s and t. Solutions for Instance: A solution for the instance  cid:3 G, s, t cid:4  is a path π from s to t.  Measure of Success: The length  or cost  of a path π is the number of edges in the path. Goal: Given an instance  cid:3 G, s, t cid:4 , the goal is to ﬁnd an optimal solution, i.e., a shortest path from s to t in G.  Brute Force Algorithm: As is often the case with optimization problems, the number of solutions for an instance may well be exponential. We do not want to check them all.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes14 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:15  Optimization Problems  Handled  s  V k V k+1  180  u  Queue of foundNotHandled  Not found  Figure 14.2: Breadth-ﬁrst search tree: We cannot assume that the graph is a tree. Here I have presented only the tree edges given by π. The ﬁgure helps to explain the loop invariant, showing which nodes have been found, which found but not handled, and which handled.  Prove Path Is Shortest: In order to claim that the shortest path from s to v is of some length d v , you must do two things:  Not Further: You must produce a suitable path of this length. We call this path a witness of the fact that the distance from s to v is at most d v . In ﬁnding a node, we trace out a path from s to it. If we have already traced out a shortest path from s to u with d u  edges in it and we trace an edge from u to v, then we have traced a path from s to v with d v  = d u  + 1 edges in it. In this path from s to v, the node preceding v is π v  = u. Not Closer: You must prove that there are no shorter paths. This is harder. Other than checking an exponential number of paths, how can you prove that there are no shorter paths? We will do it using the following trick: Suppose we can ensure that the order in which we ﬁnd the nodes is according to the length of the shortest path from s to them. Then, when we ﬁnd v, we know that there isn’t a shorter path to it, or else we would have found it already.  Deﬁnition of V j: Let Vj denote the set of nodes at distance j from s.  The Loop Invariant: See Figure 14.2.  LI1: For each found node v, the values of d v  and π v  are as required, that is, they give the shortest length and a shortest path from s to the node.  LI2: If a node has been handled, then all of its neighbors have been found.  LI3: So far, the order in which the nodes have been found is according to the length of the shortest path from s to it, that is, the nodes in Vj before those in Vj+1.  Order in Which to Handle Nodes: The only way in which we are changing the generic search algorithm of Section 14.1 is being more careful in our choice of which   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes14 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:15  181  Graph Search Algorithms  d=0  S1  d=1  2  7  d=2 6  3  8  5  4  9  Handled    {Queue}  1   1,2   1,2,3   1,2,3,4   1,2,3,4,5   1,2,3,4,5,6,7,8,9  {1} {2,3,4,5} {3,4,5,6,7} {4,5,6,7} {5,6,7,8} {6,7,8,9}  Figure 14.3: Breadth-ﬁrst search of a graph. The numbers show the order in which the nodes were found. The contents of the queue are given at each step. The tree edges are darkened.  node from foundNotHandled to handle next. According to LI3, the nodes that were found earlier are closer to s than those that are found later. The closer a node is to s, the closer are its neighbors. Hence, in an attempt to ﬁnd close nodes, the algo- rithm will next handle the earliest found node. This is accomplished by treating the set foundNotHandled as a queue, “ﬁrst in, ﬁrst out.”  Example: See Figure 14.3.  Body of the Loop: Remove the ﬁrst node u from the foundNotHandled queue and handle it as follows. For every neighbor v of u that has not been found,   cid:1  add the node to the queue,  cid:1  let d v  = d u  + 1,  cid:1  let π v  = u, and  cid:1  consider u to be handled and v to be in foundNotHandled.  Code:  algorithm ShortestPath  G, s   cid:3  pre-cond cid:4 : G is a  directed or undirected  graph, and s is one of its nodes.  cid:3  post-cond cid:4 : π speciﬁes a shortest path from s to each node of G, and d speciﬁes their lengths.  begin  foundHandled = ∅ foundNotHandled = {s} d s  = 0, π s  =  cid:3  loop cid:3 loop-invariant cid:4 : See above.  exit when foundNotHandled = ∅ let u be the node in the front of the queue foundNotHandled for each v connected to u  if v has not previously been found then  add v to foundNotHandled   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes14 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:15  Optimization Problems  d v  = d u  + 1 π v  = u  end if  end for move u from foundNotHandled to foundHandled  182  end loop  for unfound v, d v  = ∞  return  cid:3 d, π cid:4   end algorithm  Maintaining the Loop Invariant   cid:1 LI  cid:2  cid:2  cid:3  : Supp-  cid:1  ose that LI  the statement of the loop invariant before the iteration  is true, the exit condition  cid:3 exit cid:4  is not, and we have executed another iteration of the algorithm.   cid:2  cid:3  & not  cid:1 exit cid:3  & codeloop →  cid:1 LI  Closer Nodes Have Already Been Found: We will need the following claim twice.  Claim: If the ﬁrst node in the queue foundNotHandled, that is, u, is in Vk, then  1. all the nodes in V0, V1, V2, . . . , Vk−1 have already been found and han- dled, and  2. all the nodes in Vk have already been found.   cid:1   denote any node in V0, V1, V2, . . . , Vk−1. Be- ensures that nodes have been found in the order of their distance must have been found earlier than u. cannot be in the queue foundNotHandled, or else it would be ear-  Proof of Part 1 of Claim: Let u  cid:1  cause LI3 and because u Hence, u lier in the queue than u, yet u is ﬁrst. This proves that u  is closer to s than u, u  has been handled.   cid:1    cid:1    cid:1    cid:1    cid:1   be the previous node in this path. Because the subpath to u  Proof of Part 2 of Claim: Consider any node v in Vk and any path of length k to it. Let u is of length k − 1, u is in Vk−1, and hence by claim 1 has already been handled.  cid:1  Therefore, by LI2 , of which v is one, must have been found.  , the neighbors of u   cid:1    cid:1    cid:1   Maintaining LI1: During this iteration, all the neighbors v of node u that had not been found are now considered found. Hence, their d v  and π v  must now give the shortest length and a shortest path from s. The code sets d v  to d u  + 1 and π v  to u. Hence, we must prove that the neighbors v are in Vk+1.  Not Further: There is a path from s to v of length k + 1: follow the path of length k to u, and then take the edge to v. Hence, the shortest path to v can be no longer then this.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes14 CUUS154-Edmonds 978 0 521 84931 9  Graph Search Algorithms  April 2, 2008  20:15  Not Closer: We know that there isn’t a shorter path to v, or it would have been found already. More formally, the claim states that all the nodes in V0, V1, V2, . . . , Vk have already been found. Because v has not already been found, it cannot be one of these.  Maintaining LI2: Node u is designated handled only after ensuring that all its neighbors have been found.  183  Maintaining LI3: By the claim, all the nodes in V0, V1, V2, . . . , Vk have already been found and hence have already been added to the queue. We have also al- ready proved that the node v being found is in Vk+1. It follows that the order in which the nodes are found continues to be according to their distance from s.  Initial Code   cid:1 pre cid:3 → cid:1 LI cid:3  : The initial code puts the source s into foundNotHandled and sets d s  = 0 and π s  =  cid:3 . This is correct, given that initially s has been found but not handled. The other nodes have not been found, and hence their d v  and π v  are irrelevant. The loop invariants follow easily. Exiting Loop   cid:1 LI cid:3  &  cid:1 exit cid:3  →  cid:1  post cid:3  : The general-search postconditions from Section 14.1 prove that all reachable nodes have been found. LI1 states that for these nodes the values of d v  and π v  are as required. For the nodes that are unreachable from s, you can set d v  = ∞ or you can leave them undeﬁned. In some applications  such as the World Wide Web , you have no access to unreachable nodes. An advantage of this algorithm is that it never needs to know about a node unless it has been found. EXERCISE 14.2.1  See solution in Part Five.  Suppose u is being handled, u ∈ Vk, and v is a neighbor of u. For each of the following cases, explain which Vk cid:1  v might be in:   cid:1   cid:3 u, v cid:4  is an undirected edge, and v has been found before.  cid:1   cid:3 u, v cid:4  is an undirected edge, and v has not been found before.  cid:1   cid:3 u, v cid:4  is a directed edge, and v has been found before.  cid:1   cid:3 u, v cid:4  is a directed edge, and v has not been found before.  EXERCISE 14.2.2  See solution in Part Five.  Estimate the time required to ﬁnd the shortest path between two given nodes s and t.  14.3 Dijkstra’s Shortest-Weighted-Path Algorithm  We will now make the shortest-path problem more general by allowing each edge to have a different weight  length . The length of a path from s to v will be the sum of the weights on the edges of the path. This makes the problem harder, because the shortest path to a node may wind deep into the graph along many short edges instead of along a few long edges. Despite this, only small changes need to be made to the algorithm. The new algorithm is called Dijkstra’s algorithm.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes14 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:15  Optimization Problems  100  1  s: 1,1  u: 4,3  1  1  Node: order found, order handled  v: 2,4  t: 3,2  Figure 14.4: The shortest paths algorithm handles the nodes in the order of length of the shortest path to them.  184  Speciﬁcations of the Shortest-Weighted-Path Problem:  Preconditions: The input is a graph G  either directed or undirected  and a source node s. Each edge  cid:3 u, v cid:4  is allocated a nonnegative weight w cid:3 u,v cid:4 . Postconditions: The output consists of d and π, where for each node v of G, d v  gives the length δ s, v  of the shortest weighted path from s to v, and π deﬁnes a shortest-weighted-path tree.  See Section 14.2.   Prove Path Is Shortest: As before, proving that the shortest path from s to v is of some length d v  involves producing a suitable path of this length and proving that there are no shorter paths.  Not Further: As before, a witness that there is such a path is produced by tracing it out. The only change is that when we ﬁnd a path s −→ u → v, we compute its length to be d v  = d u  + w cid:3 u,v cid:4  instead of only d v  = d u  + 1. Not Closer: Unlike the breadth-ﬁrst search shortest-path algorithm from Sec- tion 14.2, the algorithm does not ﬁnd the nodes in the order of length of the shortest path to them from s. It does, however, handle the nodes in this order. See Figure 14.4. Because of this, when we handle a node, we know that there is no shorter path to it, because otherwise we would have handled it already.  The Next Node To Handle: The algorithm must choose which of the unhandled nodes to handle next. The difﬁculty is that initially we do not know the length of the shortest path. Instead, we choose the node closest to s according to our current ap- proximation. In Figure 14.4, after handling s our best approximation of the distance to v is 100 and to t is only 1. Hence, we handle t next.  An Adaptive Greedy Criterion: This choice amounts to an adaptive greedy crite- rion. See Chapter 16 for more on greedy algorithms.  Growing a Tree One Node at a Time: It turns out that the next node to be han- dled will always be only one edge from a previously handled node. Hence, the tree of handled nodes expands out, one node at a time.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes14 CUUS154-Edmonds 978 0 521 84931 9  Graph Search Algorithms  April 2, 2008  20:15  Handled nodes  u  P  A handled path to u A potentially shorter path to u  Handled nodes  u  Case 1: P=d′ v  Case 2: P=d′  u +w u,v    d u  d u′   <  < P′ u  <  P  u′  v  s  P′ u  u′   a   s   b   185  Figure 14.5:  a  shows a handled path to node u and is used to maintain LI1.  b  is used to maintain LI2.  Approximation of Shortest Distances: For every node v, before getting its short- est overall distance, we will maintain d v  and π v  as the shortest length and path from s to v from among those paths that we have handled so far.  Updating: This information is continuously updated as we ﬁnd shorter paths to v. For example, if we ﬁnd v when handling u, then we update these values as follows:  foundPathLength = d u  + w cid:3 u,v cid:4  if d v  > foundPathLength then  d v  = foundPathLength π v  = u  end if  Previous path of length d v   v π  s  v  w   u,v   u  New path of length d u  + w   u,v   When handling s in our example, d v  is set to d s  + w cid:3 s,v cid:4  = 0 + 100 = 100. Later, when handling u, it is updated to d u  + w cid:3 u,v cid:4  = 2 + 1 = 3.  Deﬁnition of a Handled Path: We say that a path has been handled if it contains only handled edges. Such paths start at s, visit as any number of handled nodes, and then follow one last edge to a node that may or may not be handled.  See the solid path to u in Figure 14.5.a.   Priority Queue: The next node to be handled is the one with the smallest d u  value. Searching the set of unhandled nodes for this node during each iteration would be too time-consuming. Re-sorting the nodes each iteration as the d u  values change would also be too time-consuming. A more efﬁcient implemen- tation uses a priority queue to hold the unhandled nodes prioritized according to their current d u  value. This can be implemented using a heap.  See Sec- tion 10.4.  We will denote this priority queue by notHandled.  Consider All Nodes “Found”: No path has yet been handled to any node that has not yet been found, and hence d v  = ∞. If we add these nodes to the queue, they will be selected last. Therefore, there is no harm in adding them. Hence, we will distinguish only between those nodes that have been handled and those that have not.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes14 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:15  Optimization Problems  The Loop Invariant:  LI1: For each handled node v, the values of d v  and π v  give the shortest length and a shortest path from s  and this path contains only handled nodes .  186  LI2: For each of the unhandled nodes v, the values of d v  and π v  give the shortest length and path from among those paths that have been handled.  Body of the Loop: Take the next node u from the priority queue notHandled, and handle it. This involves handling all edges  cid:3 u, v cid:4  out of u. Handling the edge  cid:3 u, v cid:4  involves updating the d v  and π v  values. The priorities of these nodes are changed in the priority queue as necessary.  Example: See Figure 14.6.  Code:  algorithm DijkstraShortestWeightedPath G, s   cid:3  pre-cond cid:4 : G is a weighted  directed or undirected  graph, and s is one of its nodes.  cid:3  post-cond cid:4 : π speciﬁes a shortest weighted path from s to each node of G, and d speciﬁes their lengths. begin  d s  = 0, π s  =  cid:3  for other v, d v  = ∞ and π v  = nil handled = ∅ notHandled = priority queue containing all nodes. Priorities given by d v . loop cid:3 loop-invariant cid:4 : See above. exit when notHandled = ∅ let u be a node from notHandled with smallest d u  for each v connected to u  foundPathLength = d u  + w cid:3 u,v cid:4  if d v  > foundPathLength then  d v  = foundPathLength π v  = u  update the notHandled priority queue   end if  end for move u from notHandled to handled  end loop return  cid:3 d, π cid:4   end algorithm   cid:2  cid:2  cid:3  : The loop han- Maintaining LI1   cid:1 LI1 dles a node u with smallest d u  from notHandled. Hence to maintain LI1, we must ensure that its d u  and π u  values give an overall shortest path to u. Consider some   cid:2  cid:3  & not  cid:1 exit cid:3  & codeloop →  cid:1 LI1   cid:2 , LI2   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes14 CUUS154-Edmonds 978 0 521 84931 9  Graph Search Algorithms  April 2, 2008  20:15  187   cid:1    cid:1    cid:1   other path P to u. We will see that it is no shorter. See Figure 14.5.a. Because the path P starts at the handled node s and ends at the previously unhandled node u, there has to be some node u that is the ﬁrst previously unhandled node along P.  cid:1  = u.  By the choice of u, u has the smallest d u  from notHandled.  It is possible that u Hence, d u  ≤ d u  cid:1   . Let Pu  cid:1  be the part the path that goes from s to u . This is a pre-   ≤ Pu  cid:1 . Since Pu  cid:1  is a subpath and there viously handled path. Hence, by LI2 are no negative weights, Pu  cid:1  ≤ P. Combining these gives d u  ≤ P. In conclusion, d u  is the length of the shortest path to u, and hence LI1 has been maintained. Maintaining LI2   cid:1 LI1  cid:2  cid:2  cid:3  : Setting d  cid:1  cid:1   v  to min{d  u  + w cid:3 u,v cid:4 } ensures that there is a handled path with this length to v. To maintain LI2, we must prove that there does not exist a shorter one from among those paths that now are considered handled. Such paths can now include the newly han- dled node u. Let P be a shortest one. See Figure 14.5.b. Let u be the second last node in P. Because P is a handled path, u   cid:2  cid:3  & not  cid:1 exit cid:3  & codeloop →  cid:1 LI2  must be a handled node. There are two cases:   cid:2 , LI2   v , d  , d u   cid:1    cid:1    cid:1    cid:1    cid:1    cid:1    cid:2   : If u  is a previously handled node, then by the second part of LI1  u  cid:6 = u , the shortest path to it does not need to contain the newly handled node u. It follows that this path P to v is a previously handed path. Hence, its length is at least the length of the shortest previously handed path to v, which by LI2  v . This in turn is at least min{d u = u is the length of the shortest path to u, which we now know is d weight of the edge  cid:3 u, v cid:4 . It follows that P ≥ min{d  : If the second last node in P is the newly handled node u, then its length  u , plus the   u  + w cid:3 u,v cid:4 } = d   u  + w cid:3 u,v cid:4 } = d   v , d   v , d  , is d   v .   v .   cid:1  cid:1    cid:1  cid:1    cid:2    cid:1    cid:1    cid:1    cid:1    cid:1    cid:1    cid:1    cid:1    cid:1  cid:1    v . Hence, LI2 is maintained.  Either way, the shortest path P to v that is now considered to be handled has length at least d Initial Code   cid:1 pre cid:3  →  cid:1 LI cid:3  : The initial code is the same as that for the breadth-ﬁrst search shortest-path algorithm from Section 14.2, that is, s is found but not handled with d s  = 0, π s  =  cid:3 . Initially no paths to v have been handled, and hence the  Handled s  d values a c  b  d  e  f  g  h  i  j  Handled  d values c  d  e  f  g  h  6  4 1 f  b  3 2  s  5  1  5  g  5  5  c  j  a  2  e  i  1  d  7  h  1  1  1  0  6  5  1  5 2  5  6  8  4  3  7  8 8  s d c g h b a f e i j  24  18  a  b  c  13 3  4  8  15  d 8  e  2  4  12  6  7  f  9  g  h  8  a  0  b  24  16 16  13  15  21  15 25  20  24  24  27 27  34 28  28  a  d  f  b  e  g  h c  Figure 14.6: Dijkstra’s algorithm. The d value at each step is given for each node. The tree edges are darkened.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes14 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:15  Optimization Problems length of the shortest handled path to v is d v  = ∞. This satisﬁes all three loop invariants. Exiting Loop   cid:1 LI cid:3  &  cid:1 exit cid:3  →  cid:1  post cid:3  : See the shortest-path algorithm.  188  EXERCISE 14.3.1 Estimate the running time of Dijkstra’s shortest-weighted-path al- gorithm.  EXERCISE 14.3.2  See solution in Part Five.  Given a graph where each edge weight is one, compare and contrast the computation of the breadth-ﬁrst search shortest-path algorithm from Section 14.2 and that of Dijkstra’s shortest-weighted-path algorithm. How do their choices of the next node to handle and their loop invariants compare?  EXERCISE 14.3.3 Dijkstra’s algorithm:  1. Give the full loop invariant for Dijkstra’s algorithm. Include the deﬁnition of any  terms you use.  2. What is the exit condition for Dijkstra’s algorithm? 3. Prove that the postcondition is obtained. 4. Consider a computation of Dijkstra’s algorithm on the following graph when the circled nodes have been handled. The start node is a. On the left, give the current values of d.  d=  a  24  18  b  c  d=  d=  13  3  4  8  d  8  d=  e  d=  2  Handled  d=  a  Handled  24  18  d=  b  13  3  4  8  d  8  d=  e  d=  2  7  h  d=  d=  c  7  h  d=  5. On the right, change the ﬁgure to take one step in Dijkstra’s algorithm. Include as  well any π’s that change.  EXERCISE 14.3.4 Give a simple graph with an edge with a negative weight, and show that Dijkstra’s algorithm gives the wrong answer.  14.4 Depth-First Search  We have considered breadth-ﬁrst search, which ﬁrst visits nodes at distance 1 from s, then those at distance 2, and so on. We will now consider a depth-ﬁrst search, which   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes14 CUUS154-Edmonds 978 0 521 84931 9  Graph Search Algorithms  April 2, 2008  20:15  189  s  s  Path is stack of tuples s  Found—partially Handled   a    b   Handled  Not found   c   Figure 14.7: If the next node in the stack was completely handled, then the initial order in which nodes are found is given in  a . If the next node is only partially handled, then this initial order is given in  b .  c  presents more of the order in which the nodes are found. Though the input graph may not be a tree, the ﬁgure only shows the tree edges given by π.  continues to follow some path as deeply as possible into the graph before it is forced to backtrack. I give an iterative algorithm in this section and a recursive one in Sec- tion 14.5.  Changes to the Generic Search Algorithm in Section 14.1: The next node u we handle is the one most recently found. foundNotHandled will be implemented as a stack of tuples  cid:3 v, iv cid:4 . At each iteration, we pop the most recently pushed tuple  cid:3 u, iu cid:4  and handle the  iu + 1 st edge from u. Try this out on a graph  or on a tree . The pattern in which nodes are found consists of a single path with single edges hanging off it. See Figure 14.7.  In order to prevent the single edges hanging off the path from being searched, we make a second change to the original searching algorithm: We no longer completely handle one node before we start handling edges from other nodes. From s, an edge is followed to one of its neighbors v1. Before visiting the other neighbors of s, the current path to v1 is extended to v2, v3, . . . .  See Figure 14.7.b.  We keep track of what has been handled by storing an integer iu for each node u. We maintain that for each u, the ﬁrst iu edges of u have already been handled.  Loop Invariants:  LI1: The nodes in the stack foundNotHandled are ordered so that they deﬁne a path starting at s. LI2: foundNotHandled is a stack of tuples  cid:3 v, iv cid:4  such that for each v, the ﬁrst iv edges of v have been handled. Each node v appears no more than once.  Code:  algorithm DepthFirstSearch  G, s   cid:3  pre-cond cid:4 : G is a  directed or undirected , graph, and s is one of its nodes.  cid:3  post-cond cid:4 : The output is a depth-ﬁrst search tree of G rooted at s.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes14 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:15  Optimization Problems  begin  foundHandled = ∅ foundNotHandled = { cid:3 s, 0 cid:4 } time = 0 % Used for time stamping. See following discussion. loop cid:3 loop-invariant cid:4 : See preceding list.  190  exit when foundNotHandled = ∅ pop  cid:3 u, i cid:4  off the stack foundNotHandled if u has an  i+1 st edge  cid:3 u, v cid:4  push  cid:3 u, i + 1 cid:4  onto foundNotHandled if v has not previously been found then  π v  = u  cid:3 u, v cid:4  is a tree edge push  cid:3 v, 0 cid:4  onto foundNotHandled s v  = time; time = time + 1  cid:3 u, v cid:4  is a back edge else  v has been completely handled   cid:3 u, v cid:4  is a forward or cross edge  end if  else  move u to foundHandled f  v  = time; time = time + 1  else if v has been found but not completely handled then  end if end loop return foundHandled  end algorithm  Example: See Figure 14.8.  Establishing and Maintaining the Loop Invariant: It is easy to see that with foundNotHandled = { cid:3 s, 0 cid:4 }, the loop invariant is established. If the stack does con- tain a path from s to u and u has an unhandled edge to v, then u is kept on the stack and v is pushed on top. This extends the path from u onward to v. If u does not have an unhandled edge, then u is popped off the stack. This decreases the path from s by one.  Classiﬁcation of Edges: The depth-ﬁrst search algorithm can be used to classify edges:  Tree Edges: Tree edges are the edges  cid:3 u, v cid:4  in the depth-ﬁrst search tree. When such edges are handled, v has not yet been found. Back Edges: Back edges are the edges  cid:3 u, v cid:4  such that v is an ancestor of u in the depth-ﬁrst search tree. When such edges are handled, v is in the stack, that is, found but not completely handled.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes14 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:15  191  Graph Search Algorithms  Graph S 1  Recursive stack frames  s=1  9  2 7  8  3  4 5  6  Types of edges  Tree edges Back edges Forward edges Cross edges  2  3  8  6  9  7  4  5  Iterative algorithm  Handled  Stack {s=1} {1,2,3,4,5,6} {1,2} {1,2,7,8} {1,2} {1,2,9}  6,5,4,3 6,5,4,3 6,5,4,3,8,7 6,5,4,3,8,7 6,5, 4,3,8, 7,9, 2,1  Figure 14.8: Depth-ﬁrst search of a graph. The numbers give the order in which the nodes are found. The contents of the stack are given at each step.  Cyclic: A graph is cyclic if and only if it has a back edge.  Proof  ⇐ : The loop invariant of the depth-ﬁrst search algorithm en- sures that the contents of the stack form a path from s through v and onward to u. Adding on the edge  cid:3 u, v cid:4  creates a cycle back to v. Proof  ⇒ : Later we prove that if the graph has no back edges, then there is a total ordering of the nodes respecting the edges and hence the graph has no cycles.  Bipartite: A graph is bipartite if and only if there is no back edge between any two nodes with the same level parity, that is, iff it has no odd-length cycles.  Forward Edges and Cross Edges: Forward edges are edges  cid:3 u, v cid:4  such that v is a descendant of u in the depth-ﬁrst search tree. Cross edges  cid:3 u, v cid:4  are such that u and v are in different branches of the depth- ﬁrst search tree  that is, are neither ancestors nor descendants of each other  and v’s branch is traversed before  to the left of  u’s branch.  When forward edges and cross edges are handled, v has been completely handled. The depth-ﬁrst search algorithm does not distinguish between forward edges and cross edges.  Time Stamping: Some implementations of depth-ﬁrst search time-stamp each node u with a start time s u  and a ﬁnish time f  u . Here time is measured by starting a counter at zero and incrementing it every time a node is found for the ﬁrst time   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes14 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  April 2, 2008  20:15  or a node is completely handled. s u  is the time at which node u is ﬁrst found, and f  u  is the time at which it is completely handled. The time stamps are useful in the following way:   cid:1  v is a descendant of u if and only if the time interval [s v , f  v ] is completely  contained in [s u , f  u ].  192   cid:1  If u and v are neither ancestor or descendant of each other, then the time inter-  vals [s u , f  u ] and [s v , f  v ] are completely disjoint.  Using the time stamps, this can be determined in constant time.  EXERCISE 14.4.1 Prove that when doing depth-ﬁrst search on undirected graphs there are never any forward or cross edges.  14.5 Recursive Depth-First Search  I now present a recursive implementation of a depth-ﬁrst search algorithm, which directly mirrors the iterative version. The only difference is that the iterative version uses a stack to keep track of the route back to the start node, while the recursive version uses the stack of recursive stack frames. The advantage of the recursive algorithm is that it is easier to code and easier to understand. The iterative algorithm might run slightly faster, but a good compiler will convert the recursive algorithm into an iterative one.  Code:  algorithm DepthFirstSearch  s   cid:3  pre-cond cid:4 : An input instance consists of a  directed or undirected  graph G with some of its nodes marked found and a source node s.  cid:3  post-cond cid:4 : The output is the same graph G, except all nodes v reachable from s without passing through a previously found node are now also marked as being found. The graph G is a global variable ADT, which is assumed to be both input and output to the routine.  begin  if s is marked as found then  do nothing  else  mark s as found for each v connected to s DepthFirstSearch  v   end for  end if  end algorithm   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes14 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:15  Graph Search Algorithms  S=s  c  u v  Instance  Graph when routine returns  Our stack frame  First friend  Second friend  Third friend  Our stack frame  a  b  a=s  b  b=s  a  a  b  c=s  S  u v  S  c  S  c  u v  u v  S  c  S  c  u v  u v  a  b  a  b  a  b  Unchanged  Figure 14.9: An example instance graph.  193  S  c  u v  Pruning Paths: Consider the instance graph in Figure 14.9. There are two obvi- ous paths from node S to node v. However, there are actually an inﬁnite num- ber of such paths. One path of interest is the one that starts at S, and traverses around past u up to c and then down to v. All of these equally valued paths will be pruned from consideration, except the one that goes from S through b and u directly to v.  Three Friends: Given this instance, we ﬁrst mark our source node S with an x, and then we recurse three times, once from each of a, b, and c.  F riend a: Our ﬁrst friend marks all nodes that are reachable from its source node a = s without passing through a previously marked node. This includes only the nodes in the leftmost branch, because when we marked our source S, we blocked his route to the rest of the graph.  F riend b: Our second friend does the same. He ﬁnds, for example, the path that goes from b through u directly to v. He also ﬁnds and marks the nodes back around to c.  F riend c: Our third friend is of particular interest. He ﬁnds that his source node, c, has already been marked. Hence, he returns without doing anything. This prunes off this entire branch of the recursion tree. The reason that he can do this is that for any path to a node that he would consider, another path to the same node has already been considered.  Achieving the Postcondition: Consider the component of the graph reachable from our source s without passing through a previously marked nodes.  Because our instance has no marked nodes, this includes all the nodes.  To mark the nodes within this component, we do the following. First, we mark our source s. This partitions our component of reachable nodes into subcomponents that are still reachable from   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes14 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  April 2, 2008  20:15  each other. Each such subcomponent has at least one edge from s into it. When we traverse the ﬁrst such edge, the friend marks all the nodes within this subcomponent.  194  Running Time: Marking a node before it is recursed from ensures that each node is recursed from at most once. Recursing from a node involves traversing each edge from it. Hence, each edge is traversed at most twice: once from each direction. Hence, the running time is linear in the number of edges.  EXERCISE 14.5.1 Trace out the iterative and the recursive algorithm on the same graph, and see how they compare. Do they have the same running time?  14.6 Linear Ordering of a Partial Order  Finding a linear order consistent with a given partial order is one of many applica- tions of a depth-ﬁrst search.  Hint: If a question ever mentions that a graph is directed acyclic, always start by running this algorithm.   Deﬁnition of Total Order: A total order of a set of objects V speciﬁes for each pair of objects u, v ∈ V either  1  that u is before v or  2  that v is before u. It must be transitive, in that if u is before v and v is before w, then u is before w.  Deﬁnition of Partial Order: A partial order of a set of objects V supplies only some of the information of a total order. For each pair of objects u, v ∈ V , it speciﬁes either that u is before v, that v is before u, or that the order of u and v is undeﬁned. It must also be transitive.  For example, you must put on your underwear before your pants, and you must put on your shoes after both your pants and your socks. According to transitivity, this means you must put your underwear on before your shoes. However, you do have the freedom to put your underwear and your socks on in either order. My son, Josh, when six, mistook this partial order for a total order and refused to put on his socks before his underwear. When he was eight, he explained to me that the reason that he could get dressed faster than I was that he had a “shortcut,” consisting in putting his socks on before his pants. I was thrilled that he had at least partially understood the idea of a partial order:  underwear  \  socks pants \   shoes  A partial order can be represented by a directed acyclic graph  DAG  G. The ver- tices consist of the objects V , and the directed edge  cid:3 u, v cid:4  indicates that u is before v. It follows from transitivity that if there is a directed path in G from u to v, then we   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes14 CUUS154-Edmonds 978 0 521 84931 9  Graph Search Algorithms  April 2, 2008  20:15  know that u is before v. A cycle in G from u to v and back to u presents a contradiction, because u cannot be both before and after v.  Speciﬁcations of the Topological Sort Problem:  Preconditions: The input is a directed acyclic graph G representing a partial order.  195  Postconditions: The output is a total order consistent with the partial order given by G, that is, for all edges  cid:3 u, v cid:4  ∈ G, u appears before v in the total order.  An Easy but Slow Algorithm:  The Algorithm: Start at any node v of G. If v has an outgoing edge, walk along it to one of its neighbors. Continue walking until you ﬁnd a node t that has no out- going edges. Such a node is called a sink. This process cannot continue forever, because the graph has no cycles. total order, delete t from G, and recursively repeat the process on G − v. Running Time: It takes up to n time to ﬁnd the ﬁrst sink, n − 1 to ﬁnd the second, and so on. The total time is  cid:4  n2 .  The sink t can go after every node in G. Hence, you should put t last in the  Algorithm Using a Depth-First Search: Start at any node s of G. Do a depth-ﬁrst search starting at node s. After this search completes, nodes that are considered found will continue to be considered found, and so should not be considered again. Let s . Repeat the process until all nodes have been found.  be any unfound node of G. Do a depth-ﬁrst search starting at node s   cid:1    cid:1   Use the time stamp f  u  to keep track of the order in which nodes are completely  handled, that is, removed from the stack. Output the nodes in reverse order.  If you ever ﬁnd a back edge, then stop and report that the graph has a cycle.  Proof of Correctness:  Lemma: For every edge  cid:3 u, v cid:4  of G, node v is completely handled before u. Proof of Lemma: Consider some edge  cid:3 u, v cid:4  of G. Before u is completely han- dled, it must be put onto the stack foundNotHandled. At this point in time, there are three cases:  Tree Edge: v has not yet been found. Because u has an edge to v, v is put onto the top of the stack above u before u has been completely han- dled. No more progress will be made towards handling u until v has been completely handled and removed from the stack.  Back Edge: v has been found, but not completely handled, and hence is on the stack somewhere below u. Such an edge is a back edge. This contradicts the fact that G is acyclic.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes14 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:15  Optimization Problems  196  a  f  b  c  d  e  g  h  i  j  k  l  Stack {d} {d,e,f,g} {d,e,f} {d,e,f,l} {} {i} {i,j,k} {} {a} {a,b,c} {a} {a,h} {}  Handled  g g g,l,f,e,d g,l,f,e,d g,l,f,e,d g,l,f,e,d,k,j,i g,l,f,e,d,k,j,i g,l,f,e,d,k,j,i g,l,f,e,d,k,j,i,c,b g,l,f,e,d,k,j,i,c,b g,l,f,e,d,k,j,i,c,b,h,a      Toplogical sort  = a,h,b,c,i,j,k,d,e,f,l,g  Figure 14.10: A topological sort is found using a depth-ﬁrst search.  Forward or Cross Edge: v has already been completely handled and re- moved from the stack. In this case, we are done: v was completely han- dled before u.  Topologically Sorted: Exercise 14.6.1 asks to show that this lemma is sufﬁ- cient to prove that the reverse order in which the nodes were completely handled is a correct topological sort.  Example: See the example instance in Figure 14.10.  Running Time: As with the depth-ﬁrst search, no edge is followed more than once. Hence, the total time is  cid:4  E .  Shortest-Weighted Path on a DAG: Suppose you want to ﬁnd the shortest- weighted path for a directed graph G that you know is acyclic. You could use Dijk- stra’s algorithm from Section 14.3. However, as hinted above, whenever a question mentions that a graph is acyclic, it is always fastest to start by ﬁnding a linear order consistent with the edges of the graph. Once this has been completed, you can han- dle the nodes  as done in Dijkstra’s algorithm  in this linear order. Exercise 14.6.2 asks you to prove the correctness of this algorithm.  EXERCISE 14.6.1 Show that in order to prove that the reverse order in which the nodes were completely handled is a correct topological sort, it is sufﬁcient to prove that for every edge  cid:3 u, v cid:4  of G, node v is completely handled before u.  EXERCISE 14.6.2  See solution in Part Five.  Prove the correctness and estimate the running time of this algorithm for shortest weighted paths for DAGs.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes14 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:15  Graph Search Algorithms  14.7 Exercise  EXERCISE 14.7.1 Trace breadth-ﬁrst and depth-ﬁrst searches on the following two graphs. For each do the following:  1.  Start at node s, and when there is a choice, follow edges from left to right. Number the nodes 1, 2, 3, . . . in the order that they are found, starting with node s = 1.  2. Darken the edges of the tree speciﬁed by the predecessor array π. 3. What is the data structure used by each search to store nodes that are found but  197  not yet handled?  4. Circle the nodes that are in this data structure when node 8 is ﬁrst found.  s  1  s  1   a  Breadth-first search   b  Depth-first search   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes15 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:21  198 15 Network Flows and Linear Programming  Network ﬂow is a classic computational problem with a surprisingly large number of applications, such as routing trucks and matching happy couples. Think of a given directed graph as a network of pipes starting at a source node s and ending at a sink node t. Through each pipe water can ﬂow in one direction at some rate up to some maximum capacity. The goal is to ﬁnd the maximum total rate at which water can ﬂow from the source node s to the sink node t. If this were a physical system of pipes, you could determine the answer simply by pushing as much water through as you could. However, achieving this algorithmically is more difﬁcult than you might at ﬁrst think, because the exponentially many paths from s tot overlap, winding forward and backward in complicated ways.  An Optimization Problem: Network ﬂow is another example of an optimization problem, which involves searching for a best solution from some large set of solu- tions. The formal speciﬁcations are described in Chapter 13. Network Flow Speciﬁcation: Given an instance  cid:2 G, s, t cid:3 , the goal is to ﬁnd a max- imum rate of ﬂow through graph G from node s to node t.  Precondition: We are given one of the following instances.  Instances: An instance  cid:2 G, s, t cid:3  consists of a directed graph G and speciﬁc nodes s and t. Each edge  cid:2 u, v cid:3  is associated with a positive capacity c cid:2 u,v cid:3 . For example, see Figure 15.1.a.  Postcondition: The output is a solution with maximum value and the value of that solution.  Solutions for Instance: A solution for the instance is a ﬂow F , which speciﬁes the ﬂow F cid:2 u,v cid:3  through each edge of the graph. The requirements of a ﬂow are as follows. For example, see Figure 15.1.b.  Unidirectional Flow: For any pair of nodes, it is easiest to assume that ﬂow does not go in both directions between them. Hence, we will require that at least one of F cid:2 u,v cid:3  and F cid:2 v,u cid:3  be zero and that neither be negative.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes15 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:21  Network Flows and Linear Programming   a  Network   b  Max flow   c  Min cut  a  e  i  3  2  4  b  f  j  1  5  3  c  g  k  5  7  9  d  h  l  3  4  6  1 3  1 8  2 6  2 2  s  t  a  e  i  1 1  2 5  b  f  j  1 5  2 7  c  g  k  d  h  l  1 3  2 4  0 6  U  t  s  a  e  i  8 6  5  3  2  4  b  f  j  1  5  3  c  g  k  5  7  9  d  h  l  3  4  6  0 5  0 4  0 3  0 9  s  8 6  5  V  t  Figure 15.1:  a  A network with its edge capacities labeled.  b  A maximum ﬂow in this network. The ﬁrst value associated with each edge is its ﬂow, and the second is its capacity. The total rate of the ﬂow is 3 = 1 + 2 − 0. Note that no more ﬂow can be pushed along the top path, because the edge  cid:2 b, c cid:3  is at capacity. Similarly for the edge  cid:2 e, f cid:3 . Note also that no ﬂow is pushed along the bottom path, because this would decrease the total from s to t.  c  A minimum cut in this network. The capacity of this min cut is 3 = 1 + 2. Note that the capacity of the edge  cid:2 j, i cid:3  is not included in the capacity of the cut, because it is going in the wrong direction.  b  vs  c : The rate of the maximum ﬂow is the same as the capacity of the min cut. The edges crossing forward across the cut are at capacity in the ﬂow, while those crossing backward have zero ﬂow. These things are not coincidences.  199  Edge Capacity: The ﬂow through any edge cannot exceed the capacity of the edge, namely F cid:2 u,v cid:3  ≤ c cid:2 u,v cid:3 . No Leaks: No water can be added at any node other than the source s, and no water can be drained at any node other than the sink t. At each other node the total ﬂow into the node equals the total ﬂow out, i.e., for all nodes u  cid:5 ∈ {s, t},   cid:1  v F cid:2 v,u cid:3  = cid:1   v F cid:2 u,v cid:3 .  leaves s without coming back, rate F   = cid:1  Measure of Success: The value of a ﬂow F , denoted rate F  , is the total rate of ﬂow from the source s to the sink t. We will deﬁne this to be the total that v[F cid:2 s,v cid:3  − F cid:2 v,s cid:3 ]. Agreeing with our  cid:1  intuition, we will later prove that because no ﬂow leaks or is created in be- tween s and t, this ﬂow equals that ﬂowing into t without leaving it, namely v[F cid:2 v,t cid:3  − F cid:2 t,v cid:3 ].  Min Cut Speciﬁcation: Another interesting and perhaps surprisingly related opti- mization problem is min cut. Given an instance  cid:2 G, s, t cid:3 , the goal is to ﬁnd a cut be- tween s and t that has the possible minimum capacity crossing it from the s side to the t side.  Precondition: We are given one of the following instances.  Instances: An instance  cid:2 G, s, t cid:3  consists of a directed graph G and speciﬁc nodes s and t. Each edge  cid:2 u, v cid:3  is associated with a positive capacity c cid:2 u,v cid:3 . Note that the network ﬂow and min cut Problems have the same instances. For example, see Figure 15.1.a.  Postcondition: The output is a solution with minimum value and the value of that solution.  Solutions for the Instance: A solution for the instance is a cut C =  cid:2 U, V cid:3 , which is a partitioning of the nodes of the graph into two sets U and V such that the source s is in U and the sink t is in V . For example, see Figure 15.1.c.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes15 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  April 2, 2008  20:21  For example, if G gives the roads, s is Toronto, and t is Berkeley, a cut could be the Canadian–US border. Because the nodes in a graph do not have a location as cities do, there is no reason for the partition of the nodes to be geographically contiguous. Anyone of the exponential number of partitions will do.  edges from U to V , namely, cap C  = cid:1    cid:1  Measure of Success: The capacity of a cut is the sum of the capacities of all v∈V c cid:2 u,v cid:3 . Note that this does  u∈U  not include the capacities c cid:2 v,u cid:3  of the edges going back from V to U.  200  In Section 15.1, we will design an algorithm for the network ﬂow problem. We will see that this algorithm is an example of a hill-climbing algorithm and that it does not necessarily work, because it may get stuck in a small local maximum. In Section 15.2, we will modify the algorithm and use the primal–dual method, which uses a min cut to guarantee that it has found a global maximum. This algorithm, however, may have exponential running time. In Section 15.3, we prove that the steepest-ascent version of this hill climbing algorithm runs in polynomial time. Finally, Section 15.4 relates these ideas to another, more general problem called linear programming.  EXERCISE 15.0.1 Suppose we ensured that ﬂow goes in only one direction between any two nodes, not by requiring that the ﬂow in one direction F cid:2 v,u cid:3  be zero, but by requiring that F cid:2 v,u cid:3  = −F cid:2 u,v cid:3 . This is less consistent with intuition and obscures some  cid:1  subtleties. The change does, however have the advantage of simplifying many of the v F cid:2 u,v cid:3  = 0. How does equations. For example, the no-leak requirement simpliﬁes to this change all of the other equations in this section?  15.1 A Hill-Climbing Algorithm with a Small Local Maximum  Hiking at the age of seven, my son Josh stated that the way to ﬁnd the top of the hill is simply to keep walking in a direction that takes you up, and you know you are there when you cannot go up any more. Little did he know that this is also a common technique for ﬁnding the best solution for many optimization problems. The algorithm maintains one solution for the problem and re- peatedly makes one of a small set of prescribed changes to this solution in a way that makes it a better solution. It stops when none of these changes seems able to make a better solution.  There are two problems with this technique. First, it is not necessarily clear how long it will take until the algorithm stops. Second, some- times it ﬁnds a small local maximum, i.e., the   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes15 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:21  Network Flows and Linear Programming  Network  a  20  21  s  2  31  30  b  c  75  10  t  Path  30  b  c  c  s  s  a  20  t  t  Flow  a  20 20  20 21  s  0 2  50 75  c  t  30 31  0 10  30 30  rate=50  b  201  Figure 15.2: A network, with its edge capacities labeled, is given on the left. In the middle are two paths through which ﬂow can be pushed, with the resulting ﬂow on the right. The ﬁrst value associated with each edge is its ﬂow, and the second is its capacity. The total rate of the ﬂow is 50.  top of a small hill, instead of the overall global maximum. Many hill-climbing al- gorithms, however, are used extensively even though they are not guaranteed to work, because in practice they seem to work well. In this section, we describe a hill- climbing algorithm that is guaranteed to quickly solve its problem.  Basic Ideas: I will start by giving some ideas that do not work.  Push from Source: The ﬁrst obvious thing to try is to simply start pushing water out of s. If the capacities of the edges near s are large, then they can take lots of ﬂow. Further down the network, however, the capacities may be smaller, in which case the ﬂow that we started will get stuck. To avoid causing capacity violation or leaks, we will have to back off the ﬂow that we started. Even further down the network, an edge may fork into edges with larger capacities, in which case we will need to decide in which direction to route the ﬂow. Keeping track of this could be a headache.  Plan Path for a Drop of Water: A solution to both the problem of ﬂow getting stuck and the problem of routing ﬂow along the way is to ﬁrst ﬁnd an entire path from s to t through which ﬂow can take place. In the example in Figure 15.2, wa- ter can ﬂow along the path  cid:2 s, b, c, t cid:3 : see the top middle path. We then can push as much as possible through this path. It is easy to see that the bottleneck is the edge  cid:2 b, c cid:3  with capacity 30. Hence, we add a ﬂow of 30 to each edge along this path. That working well, we can try adding more water through another path. Let us try the path  cid:2 s, a, c, t cid:3 . The ﬁrst interesting thing to note is that the edge  cid:2 c, t cid:3  in this path already has ﬂow 30 through it. Because this edge has a capacity of 75, the maximum ﬂow that can be added to it is 75 − 30 = 45. This, however, turns out not to be the bottleneck, because the edge  cid:2 s, a cid:3  has capacity 20. Adding a ﬂow of 20 to each edge along this path gives the ﬂow shown on the right in Fig- ure 15.2. For each edge, the left value gives its ﬂow and the right gives its capacity. There being no more paths forward from s to t, we are now stuck. Is this the max- imum ﬂow?  A Winding Path: Water has a funny way of seeping from one place to another. It does not need to only go forward. Though the path  cid:2 s, b, a, c, t cid:3  winds backward,   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes15 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:21  Optimization Problems  Flow  a  20 20  s  20 21  0 2  30 31  30 30  50 75  c  0 10  rate=50  New Flow a  21 21  1 2  20 20  s  31 31  c  51 75  0 10  30 30  rate=51  b  b  t  t  202   Faulty  augmentation graph  Path  20−20=0 s  21−20=1  2−0=2  c  75−50=25 t  ?  31−30=1  30−30=0  a  1  s  c  t  a  a  b  b  20−20=0 s  31−31=0  21−21=0  2−1=1  c  75−51=24 t  ?  30−30=0  1  b  No path  Figure 15.3: The top left is the same ﬂow given in Figure 15.2, the ﬁrst value associated with each edge being its ﬂow, and the second being its capacity. The top middle is a ﬁrst attempt at an augmentation graph for this ﬂow. Each edge is labeled with the amount of more ﬂow that it can handle, namely c cid:2 u,v cid:3  − F cid:2 u,v cid:3 . The top right is the path in this augmentation graph through which ﬂow is augmented. The bottom left is the resulting ﬂow. The bottom middle is its  faulty  augmentation graph. No more ﬂow can be added through it.  more ﬂow can be pushed through it. Another way to see that the concept of “for- ward” is not relevant to this problem. The bottleneck in adding ﬂow through this path is the edge  cid:2 a, c cid:3 . Already having a ﬂow 20, its ﬂow can only increase by 1. Adding a ﬂow of 1 along this path gives the ﬂow shown on the bottom left in Fig- ure 15.3. Though this example reminds us that we need to consider all possible paths from s to t, we know that ﬁnding paths through a graph is easy using either breadth-ﬁrst or depth-ﬁrst search  Sections 14.2 and 14.4 . However, in addition, we want to make sure that the path we ﬁnd is such that we can add a nonzero amount of ﬂow through it. For this, we introduce the idea of an augmentation graph.  The  Faulty  Augmentation Graph: Before we can ﬁnd a path through which more ﬂow can be added, we need to compute for each edge the amount of ﬂow that can be added through it. To keep track of this information, we construct from the current ﬂow F a graph denoted by G F and called an augmentation graph.  Augment means to add on. The augmentation is the amount you add on.  This graph initially will have the same directed edges as our network, G. Each of these edges is labeled with the amount by which its ﬂow can be increase. We will call this the edge’s augmentation capacity. Assuming that the current ﬂow through the edge is F cid:2 u,v cid:3  and its capacity is c cid:2 u,v cid:3 , this augmentation capacity is given by c cid:2 u,v cid:3  − F cid:2 u,v cid:3 . Any edge for which this capacity is zero is deleted from the augmen- tation graph. Because of this, nonzero ﬂow can be added along any path found from s to t within this augmentation graph. The path chosen will be called the augmentation path. The minimum augmentation capacity of any of its edges is the amount by which the ﬂow in each of its edges is augmented. For an example,   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes15 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:21  Network Flows and Linear Programming  see Figure 15.3. In this case, the only path happens to be  cid:2 s, b, a, c, t cid:3 , which is the path that we used. Its path is augmented by a ﬂow of 1.  The  Faulty  Algorithm: We have now deﬁned the basic steps and can easily ﬁll in the remaining detail of the algorithm.  203  The Loop Invariant: The most obvious loop invariant is that at the top of the main loop we have a legal ﬂow. It is possible that some more complex invariant will be needed, but for the time being this seems to be enough.  The Measure of Progress: The obvious measure of progress is how much ﬂow the algorithm has managed to get between s and t, that is, the rate rate F   of the current ﬂow.  The Main Steps: Given some current legal ﬂow F through the network G, the al- gorithm improves the ﬂow as follows: It constructs the augmentation graph G F for the ﬂow; ﬁnds an augmentation path from s to t through this graph using breadth-ﬁrst or depth-ﬁrst search; ﬁnds the edge in the path whose augmenta- tion capacity is the smallest; and increases the ﬂow by this amount through each edge in the path.  Maintaining the Loop Invariant: We must prove that the newly created ﬂow is a legal ﬂow in order to prove that  cid:2 loop-invariant  cid:1  cid:3 & not cid:2 exit-cond cid:3 &codeloop ⇒  cid:2 loop-invariant   cid:1  cid:1  cid:3 .  Edge Capacity: We are careful never to increase the ﬂow of any edge by more than the amount c cid:2 u,v cid:3  − F cid:2 u,v cid:3 . Hence, its ﬂow never increases beyond its ca- pacity c cid:2 u,v cid:3 .  No Leaks: We are careful to add the same amount to every edge along a path from s to t. Hence, for any node u along the path, there is one edge  cid:2 v, u cid:3  into the node whose ﬂow changes and one edge  cid:2 u, v  cid:1  cid:3  out of the node whose ﬂow  cid:1   cid:1  changes. Because these change by the same amount, the ﬂow into the node remains equal to that out, so that for all nodes u  cid:5 ∈ {s, t}, we have v F cid:2 v,u cid:3  = v F cid:2 u,v cid:3 . In this way, we maintain the fact that the current ﬂow has no leaks.  Making Progress: Because the edges whose ﬂows could not change were deleted from the augmenting graph, we know that the ﬂow through the path that was found can be increase by a positive amount. This increases the total ﬂow. Because the capacities of the edges are integers, we can prove inductively that the ﬂows are always integers and hence the ﬂow increases by at least one.  Having fractions as capacities is ﬁne, but having irrationals as capacities can cause the algorithm to run forever.   Initial Code: We can start with a ﬂow of zero through each edge. This establishes the loop invariant, because it is a legal ﬂow.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes15 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  204  April 2, 2008  20:21  Exit Condition: At the moment, it is hard to imagine how we will know whether or not we have found the maximum ﬂow. However, it is easy to see what will cause our algorithm to get stuck. If the augmenting graph for our current ﬂow is such that there is no path in it from s to t, then unless we can think of something better to do, we must exit.  Termination: As usual, we prove that this iterative algorithm eventually termi- nates because at every iteration the rate of ﬂow increases by at least one and because the total ﬂow certainly cannot exceed the sum of the capacities of all the edges.  This completely deﬁnes an algorithm.  Getting Stuck at a Local Maximum: Hill-climbing algorithms move up until they cannot go up any more. The reason that they are not allowed to go down is the same reason that iterative algorithms need to make progress every iteration, namely, to ensure that the algorithm eventually stops.  A major problem with this is that sometimes they ﬁnd a small local maximum, that is, the top of a small hill, instead of the overall global maximum. Because a hill-climbing algorithm is not allowed to move down, it gets stuck at such a local maximum.  This is similar to the class of algorithms known as greedy algorithms, described in Chapter 16. In these, no decision that is made is revoked. Our network ﬂow algo- rithm could be considered to be greedy in that once the algorithm decides to put ﬂow through an edge, it may later add more, but it never removes ﬂow. Given that our goal is to get as much ﬂow from s to t as possible and that it does not matter how that ﬂow gets there, it makes sense that such a greedy approach would work. However, we will see that it does not work.  A Counterexample: Proving that a given algorithm works for every input instance can be a major challenge. However, in order to prove that it does not work, we only need to give one input instance in which it fails. Figure 15.4 gives such an example. It traces out the algorithm on the same instance from Figure 15.2 that we did before. However, this time the algorithm happens to choose different paths. First it puts a ﬂow of 2 through the path  cid:2 s, b, a, c, t cid:3 , followed by a ﬂow of 19 through  cid:2 s, a, c, t cid:3 , followed by a ﬂow of 29 through  cid:2 s, b, c, t cid:3 . At this point, we are stuck because the augmenting graph does not contain a path from s to t. This is a problem because the current ﬂow is only 50, whereas we have already seen that the ﬂow for this network can be 51. In hill-climbing terminology, this ﬂow is a small local maximum, because we cannot improve it using the steps that we have allowed; but it is not a global max- imum, because there is a better solution.  Where We Went Wrong: From a hill-climbing perspective, we took a step in an arbitrary direction that takes us up, but with our ﬁrst attempt we happened to head up the big hill and in the second we happened to head up the small hill.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes15 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:21  Network Flows and Linear Programming  Network  20  s  31  b  2  a  30  21  75  10  c  t  s  b  2  a  c  Path  Path  19  a  a  c  c  t  t  t   Faulty  augmentation graph  205  Flow  0 20  2 21  2 75  2 31  s  b  2 2  a  c  0 10  t  31–2=29  s  b  2–2=0  c  ?  t  s  b  75–2=73  21–2=19 a  0 30  rate=2  30–0=30  20–0=20  20–19=1  30–0=30  20–19=1  s  2 31  b  2 2  a  21 21  21 75  c  0 10  t  0 30  rate=21  19 20  19 20  s  31–2=29  b  2–2=0  21–21=0 a  75–21=54  c  ?  t  29  s  b  s  31 31  b  2 2  a  c  0 10  t  31–31=0  s  b  2–2=0  21 21  50 75  21–21=0 a  75–50=25  c  ?  t  No path  29 30  rate=50  30–29=1  Figure 15.4: The faulty algorithm is traced on the instance from Figure 15.2. The nodes in this graph are laid out differently to emphasize the ﬁrst path chosen. The current ﬂow is given on the left, the corresponding augmentation graph in the middle, the augmenting path on the right, and the resulting ﬂow on the next line. The algorithm gets stuck at a suboptimal local maximum.  The ﬂow of 51 that we obtained ﬁrst turns out to be the unique maximum so- lution  often there are more than one possible maximum solutions . Hence, we can compare it with our present solution to see where we went wrong. In the ﬁrst step, we put a ﬂow of 2 through the edge  cid:2 b, a cid:3 ; however, in the end it turns out that putting more than 1 through it is a mistake.  Fixing the Algorithm: The following are possible ways of ﬁxing the kind of bugs we found.  Make Better Decisions: If we start by putting ﬂow through the path  cid:2 s, b, c, t cid:3 , then the algorithm works, but if we start with the path  cid:2 s, b, a, c, t cid:3 , it does not. One way of ﬁxing the bug is to ﬁnd some way to choose which path to add ﬂow to next so that we do not get stuck in this way. From the greedy algorithm’s per- spective, if we are going to commit to a choice, then we had better make a good one. I know of no way to ﬁx the network ﬂow algorithm in this way.  Backtrack: Chapter 17 describes another class of algorithms, known as recursive backtracking algorithms, that continually notice when they have made a mistake and that backtrack, trying other options, until a correct sequence of choices is made. In this example, we need to ﬁnd a way of decreasing the ﬂow through the edge  cid:2 b, a cid:3  from 2 to 1. A general danger of backtracking algorithms in compari- son with greedy algorithms is that the algorithm will have a much longer running time if it keeps changing its mind.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes15 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  April 2, 2008  20:21  Take Bigger Steps: One way of avoiding getting stuck at the top of a small hill is to take a step that is big enough so that you step over the valley onto the slope of the bigger hill and a little higher up. Doing this requires redeﬁne your deﬁnition of a step. This is the approach that we will take. We need to ﬁnd a way of de- creasing the ﬂow through the edge  cid:2 b, a cid:3  from 2 to 1 while maintaining the loop invariant that we have a legal ﬂow and increasing the overall ﬂow from s to t. The place in the algorithm at which we consider how the ﬂow through an edge is al- lowed to change is where we deﬁne the augmenting graph. In the next section we reconsider this deﬁnition.  206  15.2 The Primal–Dual Hill-Climbing Method  We will now deﬁne a larger step that the hill-climbing algorithm may take in hopes of avoiding a local maximum.  The  Correct  Algorithm:  The Augmentation Graph: As before, the augmentation graph expresses how the ﬂow in each edge is able to change.  Forward Edges: As before, when an edge  cid:2 u, v cid:3  has ﬂow F cid:2 u,v cid:3  and capacity c cid:2 u,v cid:3 , we put the corresponding edge  cid:2 u, v cid:3  in the augmentation graph with augmentation capacity c cid:2 u,v cid:3  − F cid:2 u,v cid:3  to indicate that we are allowed to add this much ﬂow from u to v.  Reverse Edges: Now we see that there is a possibility that we might want to decrease the ﬂow from u to v. Given that its current ﬂow is F cid:2 u,v cid:3 , this is the amount that it can be decreased by. Effectively, it is the same as increasing the ﬂow from v to u by this same amount. Moreover, if the reverse edge  cid:2 v, u cid:3  is also in the graph and has capacity c cid:2 v,u cid:3 , then we are able to increase the ﬂow from v to u by this second amount c cid:2 v,u cid:3  as well. Therefore, when the edge  cid:2 u, v cid:3  has ﬂow F cid:2 u,v cid:3  and the reverse edge  cid:2 v, u cid:3  has capacity c cid:2 v,u cid:3 , we  I can walk 75 to the right or 10 to the left.  I can walk 75−21=54 to the right or 21+10=31 to the left.  –10  0  75  –10  0  21  75  Figure 15.5: Suppose that from my home, I can walk 75 km to the right or 10 to the left. If I am already 21 km to the right, then I can walk 75 − 21 = 54 km to the right or 21 + 10 = 31 to the left. More over, walking 31 to the left and −31 to the right are the same. Similarly, suppose that my bank account can only hold $75 or go into overdraft of up to $10. If I already have $21 in the account, then I am able to add 75 − 21 = $54 or remove 21 + 10 = $31. Removing $31 and adding −$31 are also the same.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes15 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:21  Augmentation Graph  207  Network Flows and Linear Programming  Network  20  21  75  10  c  t  s  31  b  2  a  30  2 21  2 75  s  2 31  b  2 2  a  c  0 10  t  0 30  rate=2  2–2=0  2–0=2  a  30  19  2  75–2=73  c  t  2+10=12  s  2 31  b  2 2  a  21 21  21 75  c  0 10  t  rate=21  0 30  0  2  a  30  75–21=54  c  t 21+10=31  29  Flow  0 20  19 20  19 20  20 20  s  31 31  b  2 2  a  21 21  29 30  50 75  0 10 rate=50  s  31 31  b  1 2  a  21 21  51 75  30 30  rate=51  c  c  t  t  29  2  29  2  0  31  0  31  s  s  s  s  20  b  19 1  b  19 1  b  20 0  b  20  0  2  1 1  0  21  0 21  0  21  V  21  a 1 29  a 0 30  a  30  cap = 51 = rate  25  60  24  61  c  c  75  10  c  t  t  t  Cut  U  31  s  2  b  s  b  2  a  c  Path  Path  19  a  a  a 1  c  c  c  No path  s  s  s  b  b  1  b  t  t  t  t  Figure 15.6: A trace of the correct algorithm on the instance from Figure 15.4. The current ﬂow is given on the left, the corresponding augmentation graph in the middle, the augmentation path on the right, and the resulting ﬂow on the next line. The optimal ﬂow is obtained. The bottom diagram shows a minimum cut C =  cid:2 U, V cid:3 .  also put the reverse edge  cid:2 v, u cid:3  in the augmentation graph with augmen- tation capacity F cid:2 u,v cid:3  + c cid:2 v,u cid:3 . For more intuition see Figure 15.5, and for an example see edge  cid:2 c, t cid:3  in the second augmentation graph in Figure 15.6.  The Main Steps: Little else changes in the algorithm. Given some current legal ﬂow F through the network G, the algorithm improves the ﬂow as follows: It constructs the augmentation graph G F for the ﬂow; ﬁnds an augmentation path from s to t through this graph using breadth-ﬁrst or depth-ﬁrst search; ﬁnds the edge in the path whose augmentation capacity is the smallest; and increases the ﬂow by this amount through each edge in the path. If the edge in the augment- ing graph is in the opposite direction to that in the ﬂow graph, then this involves decreasing its ﬂow by this amount. This is because increasing ﬂow from v to u is effectively the same as decreasing it from u to v.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes15 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  208  April 2, 2008  20:21  Does It Work on the Counterexample?: Figure 15.6 traces this new algorithm on the same example as in Figure 15.4. The new augmenting graphs include edges in the reverse direction. Each step is the same as that in Figure 15.4, until the last step, in which these reverse edges provide the path  cid:2 s, a, b, c, t cid:3  from s to t. The bottleneck in this path is 1. Hence, we increase the ﬂow by 1 in each edge in the path. The effect is that the ﬂow through the edge  cid:2 b, a cid:3  decreases from 2 to 1, giving the optimal ﬂow that we had obtained before.  Bigger Step: The reverse edges that have been added to the augmentation graph may well not be needed. They do, after all, undo ﬂow that has already been added through an edge. On the other hand, having more edges in the augmentation graph can only increase the possibility of there being a path from s to t thro- ugh it.  Maintaining the Loop Invariant and Making Progress: See Exercise 15.2.1.  Exit Condition: As before the algorithm exits when it gets stuck because the aug- menting graph for our current ﬂow is such that there is no path in it from s to t. However, with more edges in our augmenting graph this may not occur as soon.  Code:  algorithm Networ k Flow  G, s, t   cid:2  pre-cond cid:3 : G is a network given by a directed graph with capacities on the edges. s is the source node. t is the sink.  cid:2  post-cond cid:3 : F speciﬁes a maximum ﬂow through G, and C speciﬁes a min- imum cut.  begin  F = the zero ﬂow loop   cid:2 loop-invariant cid:3 : F is a legal ﬂow. G F = the augmentation graph for F , where  edge  cid:2 u, v cid:3  has augmentation capacity c cid:2 u,v cid:3  − F cid:2 u,v cid:3  and edge  cid:2 v, u cid:3  has augmentation capacity c cid:2 v,u cid:3  + F cid:2 u,v cid:3 .  exit when s is not connected to t in G F P = a path from s to t in G F w = the minimum augmentation capacity in P Add w to the ﬂow F in every edge in P  end loop U = nodes reachable from s in G F V = nodes not reachable from s in G F C =  cid:2 U, V cid:3  return  F ,C   end algorithm   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes15 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:21  Network Flows and Linear Programming  Ending: The next step is to prove that this improved algorithm always ﬁnds a global maximum without getting stuck at a small local maximum. Using the notation of iterative algorithms, we must prove that  cid:2 loop-invariant cid:3  &  cid:2 exit-cond cid:3  & codepost-loop ⇒  cid:2 post-cond cid:3 . From the loop invariant we know that the algorithm has a legal ﬂow. Because we have exited, we know that the augmenting graph does not contain a path from s to t and hence we are at a local maxima. We must prove that there are no small local maxima and hence we must be at a global maximum and hence have an optimal ﬂow. The method used is called the primal–dual method.  209  Primal–Dual Hill Climbing: Suppose that over the hills on which we are climbing there are an exponential number of roofs, one on top of the other. As before, our problem is to ﬁnd a place to stand on the hills that has maximum height. We call this the primal optimization problem. An equally challenging problem is to ﬁnd the lowest roof. We call this the dual optimization problem. The situation is such that each roof is above each place to stand. It follows trivially that the lowest and hence optimal roof is above the highest and hence optimal place to stand, but offhand we do not know how far above it is.  We say that a hill-climbing algorithm gets stuck when it is unable to step in a way that moves it to a higher place to stand. A primal–dual hill-climbing algorithm is able to prove that the only reason for getting stuck is that the place it is standing is pressed up against a roof. This is proved by proving that from any location, it can either step to a higher location or specify a roof to which this location is adjacent. We will now see how these conditions are sufﬁcient for proving what we want.  Lemma: Finds Optimal. A primal–dual hill-climbing algorithm is guaran- teed to ﬁnd an optimal solution to both the primal and the dual optimization problems.  Proof: By the design of the algorithm, it only stops when it has a location L and a roof R with matching heights height L  = height R . This location must be optimal, because every other location L must be below this roof and hence cannot be higher than this location, that is, ∀L   ≤ height R  = height L . We say that this dual solution R witnesses the fact that the primal solution L is optimal. Similarly, L witnesses the fact that R is op- timal, that is, ∀R   ≥ height L  = height R . This is called the du- ality principle.  , height L  , height R   cid:1    cid:1    cid:1    cid:1    cid:1   Cuts as Upper Bounds: In order to apply these ideas to the network ﬂow prob- lem, we must ﬁnd some upper bounds on the ﬂow between s and t. Through a single path, the capacity of each edge acts as an upper bound, because the ﬂow through the path cannot exceed the capacity of any of its edges. The edge with the smallest capacity, being the lowest upper bound, is the bottleneck. In a gen- eral network  see Figure 15.1 , a single edge cannot act as a bottleneck, because the ﬂow might be able to go around this edge via other edges. A similar approach,   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes15 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  210  April 2, 2008  20:21  however, works. Suppose that we wanted to bound the trafﬁc between Toronto and Berkeley. We know that any such ﬂow must cross the Canadian–US border. Hence, there is no need to worry about what the ﬂow might do within Canada or within the US. We can safely say that the ﬂow from Toronto to Berkeley is bounded above by the sum of the capacities of all the border crossings. Of course, this does not mean that that ﬂow can be achieved. Other upper bounds can be obtained by summing the border crossing for other regions. For example, you could bound the trafﬁc leaving Toronto, leaving Ontario, entering California, or entering Berkeley. This brings us to the following deﬁnition.  Cut of a Graph: A cut C =  cid:2 U, V cid:3  of a graph is a partitioning of the nodes of the graph into two sets U and V such that the source s is in U and the sink t is in V . The capacity of a cut is the sum of the capacities of all edges from U  to V , namely, cap C  = cid:1    cid:1  v∈V c cid:2 u,v cid:3 .  u∈U  Because the nodes in a graph do not have a location as cities do, there is no reason for the partition of the nodes to be geographically contiguous. Any one of the exponential number of partitions will do.  Flow across a Cut: To be able to compare the rate of ﬂow from s to t with the capacity of a cut, we will ﬁrst need to deﬁne the ﬂow across a cut.  u∈U   cid:1  rate F, C : Deﬁne rate F, C  to be the current ﬂow F across the cut C, which is the total of all the ﬂow in edges that cross from U to V minus the v∈V [F cid:2 u,v cid:3  −  total of all the ﬂow that comes back, i.e., rate F, C  = cid:1  F cid:2 v,u cid:3 ]. rate F   = rate F, cid:1 {s}, G − {s} cid:2  : We deﬁned the ﬂow from s to t to be  cid:1  the total ﬂow that leaves s without coming back, namely, rate F   = v[F cid:2 s,v cid:3  − F cid:2 v,s cid:3 ]. This is precisely the equation for the ﬂow across the cut that puts s all by itself, namely, rate F   = rate F,  cid:2 {s}, G − {s} cid:3  . Lemma: rate F, C  = rate F  . Intuitively this makes sense. Because no water leaks or is created between the source s and the sink t, the ﬂow out of s equals the ﬂow across any cut between s and t, which in turn equals the ﬂow into t. It is because these are the same that we simply call this the ﬂow from s to t. Since the ﬂow into a node is the same as that out of the node, if you move the node from one side of the cut to the other this does not change the total ﬂow across the cut. Hence we can change the cut one node at a time from being the one containing only s to being the cut that we are interested in. More formally this is done by induction on the size of U. For the base case, rate F   = rate F,  cid:2 {s}, G − {s} cid:3   gives us that our hypothesis rate F, C  = rate F   is true for every cut that has only one node in U. Now suppose that, by way of induction, we assume that it is true for every cut that has i nodes in U. We will now prove it for those cuts that have i + 1 nodes in it. Let C =  cid:2 U, V cid:3  be any such cut. Choose one node   P1: ...  Gutter margin: 7 8  Top margin: 3 8  TheNotes15 CUUS154-Edmonds 978 0 521 84931 9   cid:1  cid:1   April 2, 2008  20:21  Network Flows and Linear Programming   cid:1  cid:1   x  C  v  C′  u  x  211  Figure 15.7: The edges across the cut that do not cancel in rate F, C  − rate F, C   cid:1    .  x  other then s  from U and move it across the border. This gives us a  cid:1  =  cid:2 U − {x}, V ∪ {x} cid:3 , where the side U − {x} contains only new cut C i nodes. Our assumption then gives us that the ﬂow across this cut is   = rate F  . Hence, in order to prove that equal to the ﬂow of F : rate F, C rate F, C  = rate F  , we only need to prove that rate F, C  = rate F, C  cid:1   . We will do this by proving that the difference between these is zero. By deﬁnition,   cid:1    cid:4   ⎡ ⎣  cid:3   −   cid:3   u∈U−{x},  v∈V∪{x}  ⎤ ⎦  F cid:2 u,v cid:3  − F cid:2 v,u cid:3    Figure 15.7 shows the terms that do not cancel   rate F, C  − rate F, C   cid:1       cid:2  cid:3    cid:3   u∈U  v∈V  =  =  =  =   cid:2  cid:3   cid:2  cid:3   cid:2  cid:3   v∈V  v∈V  v  F cid:2 u,v cid:3  − F cid:2 v,u cid:3   cid:4   cid:4   cid:4   +  −  F cid:2 x,v cid:3  − F cid:2 v,x cid:3   F cid:2 x,v cid:3  − F cid:2 v,x cid:3    cid:2  cid:3   cid:2  cid:3   u∈U  v∈U  F cid:2 x,v cid:3  − F cid:2 v,x cid:3   = 0   cid:4   cid:4   F cid:2 u,x cid:3  − F cid:2 x,u cid:3   F cid:2 x,v cid:3  − F cid:2 v,x cid:3   This is the total ﬂow out of the node, x minus the total ﬂow into the node, which is zero by the requirement that no node leaks. This proves that rate F, C  = rate F   for every cut that has i + 1 nodes in U. By induction, it then is true for all cuts for every size of U. This formally proves that the rate of any given ﬂow F is the same across any cut C.  Lemma: rate F   ≤ cap C : It is now easy to prove that rate F   of any ﬂow F is at most the capacity of any cut C. In the primal–dual analogy, this proves that each roof is above each place to stand. Given rate F   = rate F, C , it is cut. This follows easily from the deﬁnition rate F, C  = cid:1   cid:1   cid:1  F cid:2 v,u cid:3 ] ≤ cid:1  sufﬁcient to prove that the ﬂow across a cut is at most the capacity of the v∈V [F cid:2 u,v cid:3  − v∈V [F cid:2 u,v cid:3 ], because having positive ﬂow backwards across the cut from V to U only decreases the ﬂow. Then this sum is at most  u∈U  u∈U   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes15 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:21  212  Optimization Problems   cid:1    cid:1  v∈V [c cid:2 u,v cid:3 ], because no edge can have ﬂow exceeding its capacity. This is the deﬁnition of the capacity cap C  of the cut. This proves the required rate F   ≤ cap C .  u∈U  Take a Step or Find a Cut: The primal–dual method requires that from any loca- tion, you can either step to a higher location or specify a roof to which this loca- tion is adjacent. In the network ﬂow problem, this translates to: given any legal ﬂow F , either ﬁnd a better ﬂow or ﬁnd a cut whose capacity is equal to the rate of the current ﬂow. The augmentation graph G F includes those edges through which the ﬂow rate can be increased. Hence, the nodes reachable from s in this graph are the nodes to which more ﬂow could be pushed. Let U denote this set of nodes. In contrast, the remaining set of nodes, which we will denote by V , are those to which more ﬂow cannot be pushed. See the cut at the bottom of Fig- ure 15.6. No ﬂow can be pushed across the border between U and V , because all the edges crossing over are at capacity. If t is in U, then there is a path from s to t through which the ﬂow can be increased. On the other hand, if t is in V , then C =  cid:2 U, V cid:3  is a cut separating s and t. What remains is to formalize the proof that the capacity of this cut is equal to rate of the current ﬂow.  For another example, see the cut in Figure 15.1.c. Although cap C  = rate F  , this cut was not formed as described here because the node i is not reachable from s in the augmentation graph.  Since we know rate F, C  = rate F  , it remains only to prove rate F, C  = cap C , that is, that the current ﬂow across the cut C is equal to the capacity of the cut.  u∈U  u∈U   cid:1  v∈V [F cid:2 u,v cid:3  − F cid:2 v,u cid:3 ] = cid:1    cid:1  v∈V [c cid:2 u,v cid:3  − 0] = cap C .  Lemma: rate F, C  = cap C : To prove this, it is sufﬁcient to prove that every edge  cid:2 u, v cid:3  crossing from U to V has ﬂow in F at capacity  F cid:2 u,v cid:3  = c cid:2 u,v cid:3   and rate F, C  = cid:1  every edge  cid:2 v, u cid:3  crossing back from V to U has zero ﬂow in F . These give that F cid:1 u,v cid:2  = c cid:1 u,v cid:2 : Consider any edge  cid:2 u, v cid:3  crossing from U to V . If F cid:2 u,v cid:3  < c cid:2 u,v cid:3 , then the edge  cid:2 u, v cid:3  with augmentation capacity c cid:2 u,v cid:3  − F cid:2 u,v cid:3  would be added to the augmentation graph. However, having such an edge in the augmentation graph contradicts the fact that u is reachable from s in the augmentation graph and v is not. F cid:1 v,u cid:2  = 0: If F cid:2 v,u cid:3  > 0, then the edge  cid:2 u, v cid:3  with augmentation capacity c cid:2 u,v cid:3  + F cid:2 v,u cid:3  would be added to the augmentation graph. Again, having such an edge is a contradiction. This proves that rate F, C  = cap C .  Lemma: cap C  = rate F  : cap C  = rate F, C  = rate F  , that is, the ﬂow we have found equals the capacity of the cut, as required.  Ending: This improved network ﬂow algorithm always ﬁnds a global maximum without getting stuck at a small local maximum. In each iteration it either ﬁnds a   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes15 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:21  Network Flows and Linear Programming  path in the augmenting graph through which it can improve the current ﬂow or ﬁnds a cut that witnesses the fact that there are no better ﬂows.  Max-Flow–Min-Cut Duality Principle: The max ﬂow and min cut problems were deﬁned at the beginning of this chapter, both being interesting problems in their own right. Now we see that cuts can be used as the ceilings on the ﬂows.  213  Max Flow = Min Cut: We have proved that, given any network as input, the net- work ﬂow algorithm ﬁnds a maximum ﬂow and—almost as an accident—ﬁnds a minimum cut as well. This proves that the maximum ﬂow through any network equals its minimum cut.  Dual Problems: We say that the dual of the max ﬂow problem is the min cut problem, and conversely that the dual of the min cut problem is the max ﬂow problem.  Credits: This algorithm was developed by Ford and Fulkerson in 1962.  Running Time Exponential? Suppose that the network graph has m edges, each with a capacity that is represented by an O  cid:1   bit number. Each capacity could be as large as O 2 cid:1  , and the total maximum ﬂow could be as large as O m · 2 cid:1  . Starting out as zero and increasing by about one each iteration, the algorithm would need O m · 2 cid:1   iterations until the maximum ﬂow is found. This running time is polynomial in the number of edges, m. However, the size of the input instance, which in this case is the number of bits  or digits  needed to represent all of the values, is O m ·  cid:1  . If  cid:1  is large, then the number of iterations, O m · 2 cid:1  , is exponential in this size. This is a common problem with hill-climbing algorithms.  EXERCISE 15.2.1 Prove that the network ﬂow algorithm presented in this section maintains the loop invariant that it always holds a legal ﬂow. Do this by proving that the changes to the ﬂow do not violate any edge capacities or create leaks at nodes. Also prove that progress is made because the total ﬂow increases. You need to be careful with your plus and minus signs.  EXERCISE 15.2.2 Starting with the ﬂow given below, complete the network ﬂow algorithm.  s  7 7  t  s  t  2 9  0 4  4 10  0 3  4 4  2 2  2 3  0 2  0 2 4 4   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes15 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  April 2, 2008  20:21  214  EXERCISE 15.2.3 In hill-climbing algorithms there are steps that make lots of progress and steps that make very little progress. For example, the ﬁrst iteration on the input given in Figure 15.2 might ﬁnd a path through the augmentation graph through which a ﬂow of 30 can be added. It might, however, ﬁnd the path through which only a ﬂow of 2 can be added. How bad might the running time be when the computation is unlucky enough to always take the worst legal step allowed by the algorithm? Start by taking the step that increases the ﬂow by 2 for the input given in Figure 15.2. Then continue to take the worst possible step. You could draw out each and every step, but it is better to use this opportunity to use loop invariants. What does the ﬂow look like after i iterations? Repeat this process on the same graph except that the four edges forming the square now have capacities 1,000,000,000,000,000 and the crossover edge has capacity 1.  Also move t to c or give that last edge a large capacity.   1. What is the worst case number of iterations of this network ﬂow algorithm as a function of the number of edges m in the input network? 2. What is the ofﬁcial “size” of a network? 3. What is the worst case number of iterations of this network ﬂow algorithm as a function of the size of the input network?  EXERCISE 15.2.4 If all the capacities in the given network are integers, prove that the algorithm always returns a solution in which the ﬂow through each edge is an integer. For some applications, this fact is crucial.  EXERCISE 15.2.5  See solution in Part Five.  Give an algorithm for solving the min cut problem. Given a network  cid:2 G, s, t cid:3  with capacities on the edges, ﬁnd a minimum  cid:1  cut C =  cid:2 U, V cid:3  where s ∈ U and t ∈ V . The cost of the cut is its capacity cap C  =   cid:1  v∈V c cid:2 u,v cid:3 .  Hint: you have already been told how to do this.   u∈U  15.3 The Steepest-Ascent Hill-Climbing Algorithm  We have all experienced that climbing a hill can take a long time if you wind back and forth, barely increasing your height at all. In contrast, you get there much faster if you head energetically straight up the hill. This method, which is call the method of steepest ascent, is to always take the step that increases your height the most. If you already know that the hill-climbing algorithm in which you take any step up the hill works, then this new, more speciﬁc algorithm also works. However, if you are lucky, it ﬁnds the optimal solution faster.  In our network ﬂow algorithm, the choice of what step to take next involves choosing which path in the augmentation graph to take. The amount the ﬂow in- creases is the smallest augmentation capacity of any edge in this path. It follows that the choice that would give us the biggest improvement is the path whose small- est edge is the largest for any path from s to t. Our steepest-ascent network ﬂow algorithm will augment such a best path each iteration. What remains to be done   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes15 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:21  Network Flows and Linear Programming  is to give an algorithm that ﬁnds such a path and to prove that this ﬁnds a maximum ﬂow within a polynomial number of iterations.  Finding the Augmentation Path with the Biggest Smallest Edge: The input consists of a directed graph with positive edge weights and with special nodes s and t. The output consists of a path from s to t through this graph whose smallest weighted edge is as big as possible.  215  Easier Problem: Before attempting to develop an algorithm for this, let us con- sider an easier but related problem. In addition to the directed graph, the input to the easier problem provides a weight, denoted wmin. It either outputs a path from s to t whose smallest weighted edge is at least as big as wmin, or states that no such path exists.  Using the Easier Problem: Assuming that we can solve this easier problem, we solve the original problem by running the ﬁrst algorithm with wmin being every edge weight in the graph, until we ﬁnd the weight for which there is a path with such a smallest weight, but there is no path with a bigger smallest weight. This is our answer.  See Exercise 15.3.1.   Solving the Easier Problem: A path whose smallest weighted edge is at least as big as wmin will obviously not contain any edge whose weight is smaller than wmin. Hence, the answer to this easier problem will not change if we delete from the graph all edges whose weight is smaller. Any path from s to t in the remaining graph will meet our needs. If there is no such path, then we also know there is no such path in our original graph. This solves the problem.  Implementation Details: In order to ﬁnd a path from s to t in a graph, the al- gorithm branches out from s using breadth-ﬁrst or depth-ﬁrst search, marking every node reachable from s with the predecessor of the node in the path to it from s. If in the process t is marked, then we have our path.  See Section 14.1.  It seems a waste of time to have to redo this work for each wmin, so let’s use an iterative algorithm. The loop invariant will be that the work for the previous wmin has been done and is stored in a useful way. The main loop will then complete the work for the current wmin, reusing as much of the previous work as possi- ble. This can be implemented as follows. Sort the edges from biggest to smallest  breaking ties arbitrarily . Consider them one at a time. When considering wi, we must construct the graph formed by deleting all the edges with weights smaller than wi. Denote this by Gwi . We must mark every node reachable from s in this graph. Suppose that we have already done these things in the graph Gwi−1. We form Gwi from Gwi−1 by adding the single edge with weight wi. Let  cid:2 u, v cid:3  denote this edge. Nodes are reachable from s in Gwi that were not reachable in Gwi−1 only if u was reachable and v was not. This new edge then allows v to be reach- able. Unmarked nodes now reachable from s via v can all be marked reachable by starting a depth-ﬁrst search from v. The algorithm will stop at the ﬁrst edge that allows t to be reached. The edge with the smallest weight in this path to t   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes15 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  216  April 2, 2008  20:21  will be the edge with weight wi added during this iteration. There is no path from s to t in the input graph with a larger smallest weighted edge, because t was not reachable when only the larger edges were added. Hence, this path is a path to t in the graph whose smallest weighted edge is the largest. This is the required output of this subroutine.  Running Time: Even though the algorithm for ﬁnding the path with the largest smallest edge runs depth-ﬁrst search for each weight wi, because the work done before is reused, no node in the process is marked reached more than once, and hence no edge is traversed more than once. It follows that this process requires only O m  time, where m is the number of edges. This time, however, is domi- nated by the time O m log m  to sort the edges.  Code:  algorithm LargestShortestWeight  G, s, t   cid:2  pre-cond cid:3 : G is a weighted directed  augmenting  graph. s is the source node. t is the sink.  cid:2  post-cond cid:3 : P speciﬁes a path from s to t whose smallest edge weight is as large as possible.  cid:2 u, v cid:3  is its smallest weighted edge. begin  Sort the edges by weight from largest to smallest  cid:1  = graph with no edges G mark s reachable loop   cid:2 loop-invariant cid:3 : Every node reachable from s in G reachable. exit when t is reachable  cid:2 u, v cid:3  = the next largest weighted edge in G Add  cid:2 u, v cid:3  to G if  u is marked reachable and v is not   then   cid:1    cid:1   is marked  Do a depth-ﬁrst search from v, marking all reachable nodes not marked before.  end if end loop P = path from s to t in G return  P,  cid:2 u, v cid:3      cid:1   end algorithm  Running Time of Steepest Ascent: How many times must the network ﬂow algo- rithm augment the ﬂow in a path when the path chosen is that whose augmentation capacity is the largest possible?  Decreasing the Remaining Distance by a Constant Factor: The ﬂow starts out as zero and may need to increase to be as large as O m · 2 cid:1   when there are m edges   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes15 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:21  217  Network Flows and Linear Programming  with  cid:1  bit capacities. We would like the number of steps to be not exponential but linear in  cid:1 . One way to achieve this is to ensure that the current ﬂow dou- bles each iteration. This, however, is likely not to happen. Another possibility is to turn the measure of progress around. After the ith iteration, let Ri denote the remaining amount that the ﬂow must increase. More formally, suppose that the maximum ﬂow is ratemax and that the rate of the current ﬂow is rate F  . The re- maining distance is then Ri = ratemax − rate F  . We will show that the amount wmin by which the ﬂow increases is at least some constant fraction of Ri.  Bounding the Remaining Distance: The funny thing about this measure of progress is that the algorithm does not know what the maximum ﬂow ratemax is. It is only needed as part of the analysis. We must bound how big the remain- ing distance, Ri = ratemax − rate F  , is. Recall that the augmentation graph for the current ﬂow is constructed so that the augmentation capacity of each edge gives the amount that the ﬂow through this edge can be increased by. Hence, just as the sum of the capacities of the edges across any cut C =  cid:2 U, V cid:3  in the network acts as an upper bound to the total ﬂow possible, the sum of the augmentation capacities of the edges across any cut C =  cid:2 U, V cid:3  in the augmentation graph acts as an upper bound to the total amount that the current ﬂow can be increased.  Choosing a Cut: We need to choose which cut we will use.  This is not part of the algorithm.  As before, the natural cut to use comes out of the algorithm that ﬁnds the path from s to t. Let wmin = wi denote the smallest augmentation ca- pacity in the path whose smallest augmentation capacity is largest. Let Gwi−1 be the graph created from the augmenting graph by deleting all edges whose aug- mentation capacities are smaller than or equal to wmin. This is the last graph that the algorithm that ﬁnds the augmenting path considers before adding the edge with weight wmin that connects s and t. We know that there is not a path from s to t in Gwi−1, or else there would be an path in the augmenting graph whose smallest augmenting capacity was larger then wmin. Form the cut C =  cid:2 U, V cid:3  by letting U be the set of all the nodes reachable from s in Gwi−1 and letting V be those that are not. Now consider any edge in the augmenting graph that crosses this cut. This edge cannot be in the graph Gwi−1, or else it would be crossing from a node in U that is reachable from s to a node that is not reachable from s, which is a contradiction. Because this edge has been deleted in Gwi−1, we know that its augmentation capacity is at most wmin. The number of edges across this cut is at most the number of edges in the network, which has been denoted by m. It follows that the sum of the augmentation capacities of the edges across this cut C =  cid:2 U, V cid:3  is at most m · wmin. Bounding the Increase, wmin ≥ 1 m Ri: We have determined that the remaining amount that the ﬂow needs to be increased, Ri = ratemax − rate F  , is at most the sum of the augmentation capacities across the cut C, which is at most m · wmin, that is, Ri ≤ m · wmin. Rearranging this gives that wmin ≥ 1  m Ri.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes15 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  218  April 2, 2008  20:21  e  m m = 1  m or to note that limm→∞ 1 − 1  2 RI . As long as this is the case, Ri decreases by at least 1  The Number of Iterations: If the ﬂow increases each iteration by at least 1 m times the remaining amount Ri, it decreases the remaining amount, giving that Ri+1 ≤ Ri − 1 m Ri. You might think that it follows that the maximum ﬂow is obtained in only m iterations. This would be true if Ri+1 ≤ Ri − 1 m R0. However, it is not, be- cause the smaller Ri gets, the smaller the amount it decreases by. One way to bound the number of iterations needed is to note that Ri ≤  1 − 1 m i R0 and then either to bound logarithms to base 1 − 1 ≈ 1 2.17 . However, I prefer the following method. As long as Ri is big, we know that it decreases by a lot. After some I th iteration, say that Ri is still big when it is still at least 1 2m RI . Af- ter m such iterations, Ri would decrease from RI to 1 2 RI . The only reason that it would not continue to decrease this fast would be that it already had de- creased that much. Either way, we know that every m iterations, Ri decreases by a factor of two. This process may make you think of Zeno’s paradoxes. If you cut the remaining distance in half and then in half again and so on, then though you get very close very fast, you never actually get there. However, if all the capacities are integers, then all values will be integers, and hence when Ri decreases to less than one, it must in fact be zero, giving us the maximum ﬂow. Initially, the remaining amount Ri = ratemax − rate F   is at most O m · 2 cid:1  . Hence, if it decreases by at least a factor of 2 each m iterations, then after mj iterations, this amount is at most O m · 2 cid:1  2j  . This reaches one when j = O log2 m · 2 cid:1    = O  cid:1  + log m , or in O m cid:1  + m log m  iterations. If your capaci- ties are real numbers, then you will be able to approximate the maximum ﬂow to within  cid:1  cid:1   bits of accuracy in another m cid:1  cid:1   m Ri ≥ 1  iterations.  Bounding the Running Time: We have determined that each iteration takes m log m time and that only O m cid:1  + m log m  iterations are required. It follows that this steepest-ascent network ﬂow algorithm runs in time O  cid:1 m2 log m + m2 log2 m .  √  Fully Polynomial Time: A lot of work has been done ﬁnding an algorithm that is what is known as fully polynomial. This requires that the number of iterations be polynomial in the number of values and not depend at all on the values them- selves. Hence, if you charge only one time step for addition and subtraction, even if the capacities are strange things like 2, then the algorithm gives the exact an- swer  at least symbolically  in polynomial time. My father, Jack Edmonds, and a colleague, Richard Karp, developed such an algorithm in 1972. It is a version of the original Ford–Fulkerson algorithm. In it, however, in each iteration, the path from s to t in the augmentation graph with the smallest number of edges is aug- mented. This algorithm iterates at most O nm  times, where n is the number of nodes and m the number of edges. In practice, this is slower than the O m cid:1  -time steepest-ascent algorithm.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes15 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:21  Network Flows and Linear Programming  EXERCISE 15.3.1 Could we use binary search on the weights wmin to ﬁnd the critical weight  see Section 1.4 , and if so, would that be faster? Why?  15.4 Linear Programming  When I was an undergraduate, I had a summer job with a food company. Our goal was to make cheap hot dogs. Every morning we got the prices of thousands of ingredients: pig hearts, sawdust, etc. Each ingredient had an associated variable indicating how much of it to add to the hot dogs. There are thousands of linear constraints on these variables: so much meat, so much moisture, and so on. Together these constraints specify which combinations of ingredients constitute a hot dog. The cost of the hot dog is a linear function of the quantities you put into it and their prices. The goal is to determine what to put into the hot dogs that day to minimize the cost. This is an example of a general class of problems referred to as linear programs.  219  Formal Speciﬁcation: A linear program is an optimization problem whose con- straints and objective functions are linear functions. The goal is to ﬁnd a setting of variables that optimizes the objective function, while respecting all of the constraints.  Precondition: We are given one of the following instances.  Instances: An input instance consists of  1  a set of linear constraints on a set of variables and  2  a linear objective function.  Postcondition: The output is a solution with minimum cost and the cost of that solution.  Solutions for Instance: A solution for the instance is a setting of all the vari- ables that satisﬁes the constraints.  Measure of Success: The cost or value of a solutions is given by the objective function.  Example 15.4.1: Instance Example  maximize 7x1 − 6x2 + 5x3 + 7x4 subject to 3x1 + 7x2 + 2x3 + 9x4 ≤ 258 6x1 + 3x2 + 9x3 − 6x4 ≤ 721 2x1 + 1x2 + 5x3 + 5x4 ≤ 524 3x1 + 6x2 + 2x3 + 3x4 ≤ 411 4x1 − 8x2 − 4x3 + 4x4 ≤ 685   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes15 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:21  Optimization Problems  x 2  220  Optimal solution  Hill-climbing algorithm  Initial solution  Objective function  1x  Figure 15.8: The Euclidean space representation of a linear program with n = 2.  Matrix Representation: A linear program can be expressed very compactly using matrix algebra. Let n denote the number of variables and m the number of con- straints. Let a denote the row of n coefﬁcients in the objective function, let M denote the matrix with m rows and n columns of coefﬁcients on the left-hand side of the con- straints, let b denote the column of m coefﬁcients on the right-hand side of the con- straints, and ﬁnally let x denote the column of n variables. Then the goal of the linear program is to maximize a · x subject to M · x ≤ b.  Network Flows: The network ﬂows problem can be expressed as instances of linear programming. See Exercise 15.4.1.  The Euclidean Space Interpretation: Each possible solution, giving values to the variables x1, . . . , xn, can be viewed as a point in n-dimensional space. This space is easiest to view when there are only two or three dimensions, but the same ideas hold for any number of variables.  Constraints: Each constraint speciﬁes a boundary in space, on one side of which a valid solution must lie. When n = 2, this constraint is a one-dimensional line. See Figure 15.8. When n = 3, it is a two-dimensional plane, like the side of a box. In general, it is an  n − 1 -dimensional space. The space bounded by all of the constraints is called a polyhedral.  Vertices: The boundary of the polyhedral is deﬁned by vertices where a number of constraints intersect. When n = 2, pairs of line constraints intersect at a vertex. See Figure 15.8. For n = 3, three sides of a box deﬁne a vertex  corner . In general,   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes15 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:21  Network Flows and Linear Programming  it requires n constraints to intersect to deﬁne a single vertex. This is because say- ing that the solution is on the constraint is saying that the linear equation holds with equality and not with “less than or equal to.” Then recall that n linear equa- tions with n unknowns are sufﬁcient to specify a unique solution.  The Objective Function: The objective function gives a direction in Euclidean space. The goal is to ﬁnd a point in the bounded polyhedral that is the furthest in this direction. The best way to visualize this is to rotate the Euclidean space so that the objective function points straight up. The goal is to ﬁnd a point in the bounded polyhedral that is as high as possible.  221  A Vertex Is an Optimal Solution: As you can imagine from looking at Figure 15.8, if there is a unique solution, it will be at a vertex where n constraints meet. If there is a whole region of equivalently optimal solutions, then at least one of them will be a vertex. Our search for an optimal solution will focus on these vertices.  The Hill-Climbing Algorithm: The obvious algorithm simply climbs the hill formed by the outside of the bounded polyhedral until the top is reached. In deﬁning a hill-climbing algorithm for linear programming we just need to devise a way to ﬁnd an initial valid solution and to deﬁne what constitutes a step to a better solution.  A Step: Suppose, by the loop invariant, we have a solution that, in addition to being valid, is also a vertex of the bounding polyhedral. More formally, the solu- tion satisﬁes all of the constraints and meets n of the constraints with equality. A step will involve climbing along the edge  one-dimensional line  between two adjacent vertices. This involves relaxing one of the constraints that is met with equality, so that it no longer is met with equality, and tightening one of the con- straints that was not met with equality so that it now is met with equality. This is called is called pivoting out one equation and in another. The new solution will be the unique solution that satisﬁes with equality the n presently selected equa- tions. Of course, at each iteration such a step can be taken only if it continues to satisfy all of the constraints and improves the objective function. There are fast ways of ﬁnding a good step to take. However, even if you do not know these, there are only n · m choices of steps to try, when there are n variables and m equations. Finding an Initial Valid Solution: If we are lucky, the origin will be a valid solu- tion. However, in general ﬁnding some valid solution is itself a challenging prob- lem. Our algorithm to do so will be an iterative algorithm that includes the con- straints one at a time. Suppose we have a vertex solution that satisﬁes all of the constraints in Example 15.4.1 except the last one. We will then treat the negative of this next constraint as the objective function, namely −4x1 + 8x2 + 4x3 − 4x4. We will run our hill-climbing algorithm, starting with the vertex we have, until we have a vertex solution that maximizes this new objective function subject to the ﬁrst i equations. This is equivalent to minimizing the objective 4x1 − 8x2 − 4x3 + 4x4. If its minimum is less that 685, then we have found a vertex solution   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes15 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:21  Optimization Problems  that satisﬁes the ﬁrst i + 1 equations. If not, then we have determined that no such solution exists.  No Small Local Maximum: To prove that the algorithm eventually ﬁnds a global maximum, we must prove that it will not get stuck in a small local maximum.  222  Convex: Because the bounded polyhedral is the intersection of straight cuts, it is what we call convex. More formally, this means that the line between any two points in the polyhedral is also in the polyhedral. This means that there cannot be two local maximum points, because between these two hills there would need to be a valley and a line between two points across this valley would be outside the polyhedral.  The Primal–Dual Method: The primal–dual method formally proves that a global maximum will be found. Given any linear program, deﬁned by an opti- mization function and a set of constraints, there is a way of forming its dual min- imization linear program. Each solution to this dual acts as a roof or upper bound on how high the primal solution can be. Then each iteration either ﬁnds a better solution for the primal or provides a solution for the dual linear program with a matching value. This dual solution witnesses the fact that no primal solution is bigger.  Forming the Dual: If the primal linear program is to maximize a · x sub- ject to Mx ≤ b, then the dual is to minimize bT · y subject to MT · y ≥ a T , where bT , MT , and a T are the transposes formed by interchanging rows and columns. The dual of Example 15.4.1 is  minimize 258 + 721y2 + 524y3 + 411y4 + 685y5 subject to 3y1 + 6y2 + 2y3 + 3y4 + 4y5 ≥ 7 7y1 + 3y2 + 1y3 + 6y4 − 8y5 ≥ −6 2y1 + 9y2 + 5y3 + 2y4 − 4y5 ≥ 5 9y1 − 6y2 + 5y3 + 3y4 + 4y5 ≥ 7  The dual will have a variable for each constraint in the primal and a con- straint for each of its variables. The coefﬁcients of the objective function be- come the numbers on the right-hand sides of the inequalities, and the num- bers on the right-hand sides of the inequalities become the coefﬁcients of the objective function. Finally, “maximize” becomes “minimize.” The dual is the same as the original primal.  Upper Bound: We prove that the value of any solution to the primal linear program is at most the value of any solution to the dual linear program. The value of the primal solution x is a · x. The constraints MT · y ≥ a T can be   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes15 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:21  Network Flows and Linear Programming  turned around to give a ≤ y T · M. This gives that a · x ≤ y T · M · x. Using the constraints Mx ≤ b, this is at most y T · b. This can be turned around to give bT · y, which is the value of the dual solution y.  Running Time: The primal–dual hill-climbing algorithm is guaranteed to ﬁnd the optimal solution. In practice, it works quickly  though for my summer job, the com- puters would crank for hours . However, there is no known hill-climbing algorithm that is guaranteed to run in polynomial time.  There is another algorithm that solves this problem, called the ellipsoid method. Practically, it is not as fast, but theoretically it provably runs in polynomial time.  223  EXERCISE 15.4.1 Express the network ﬂow instance in Figure 15.2 as a linear program.  15.5 Exercises EXERCISE 15.5.1  See solution in Part Five.  Let G =  L ∪ R, E  be a bipartite graph with nodes L on the left and R on the right. A matching is a subset of the edges such that each node appears at most once. For any A ⊆ L, let N A  be the neighborhood set of A, namely N A  = {v ∈ R  ∃u ∈ A such that  u, v  ∈ E}. Prove Hall’s theorem, which states that there exists a matching in which every node in L is matched if and only if ∀A ⊆ L, A ≤ N A . 1. For each of the following two bipartite graphs, give a short witness either to the fact that it has a perfect matching or to the fact that it does not. Use Hall’s theorem in your explanation why a graph does not have a matching. No need to mention ﬂows or cuts.  1  2  3  4  5  1  2  3  4  5  A  B  C  D  E  A  B  C  D  E  3.  2. ⇒: Suppose there exists a matching in which every node in L is matched. For u ∈ L, let M u  ∈ R specify one such matching. Prove that ∀A ⊆ L, A ≤ N A . Section 20.4 describes a network with nodes {s} ∪ L ∪ R ∪ {t} with a directed edge from s to each node in L, the edges E from L to R in the bipartite graph directed from L to R, and a directed edge from each node in R to t. The notes give each edge capacity 1. However, the edges  cid:2 u, v cid:3  across the bipartite graph could just as well be given capacity ∞. Consider some cut  U, V   in this network. Note that U contains s, some nodes of L, and some nodes of R, whereas V contains the remaining nodes   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes15 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:21  Optimization Problems  that the capacity of this cut, i.e., cap U, V   = cid:1   of L, the remaining nodes of R, and t. Assume that ∀A ⊆ L, A ≤ N A . Prove 4. ⇐: Assume that ∀A ⊆ L, A ≤ N A  is true. Prove that there exists a matching in which every node in L is matched.  Hint: Use everything you know about network ﬂows.  Suppose that there is some integer k ≥ 1 such that every node in L has degree at least k and every node in R has degree at most k. Prove that there exists a matching in which every node in L is matched.   cid:1  v∈V c cid:2 u,v cid:3 , is at least L.  u∈U  5.  224   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes16 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:31  16 Greedy Algorithms  225  Every two-year-old knows the greedy algo- rithm. In order to get what you want, just start grabbing what looks best.  16.1 Abstractions, Techniques, and Theory  Speciﬁcations: A very select number of optimization problems can be solved using a greedy algorithm. Most of these have the following form.  Precondition: We are given one of the following instances.  Instances: An instance consists of a set of objects and a relationship between them. Think of the objects as being prizes that you must choose among.  Postcondition: Given an instance, the goal is to ﬁnd one of the valid solutions for this instance with optimal  minimum or maximum as the case may be  measure of success.  The solution to be outputted need not be unique.   Solutions for Instance: A solution requires the algorithm to make a choice about each of the objects in the instance. Sometimes, this choice is complex, but usually it is simply whether or not to keep it. In this case, a solution is the subset of the objects that you have kept. The catch is that some subsets are not allowed because these objects conﬂict somehow with each other.  Measure of Success: Each solution is assigned a cost or measure of success. Often, when a solution consists of a nonconﬂicting subset of the objects, this   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes16 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  April 2, 2008  20:31  measure is the number of objects in the subset or the sum of the costs of its individual objects. Sometimes the measure is a more complex function.  226  The Brute Force Algorithm  Exponential Time : The brute force algorithm for an optimi- zation problem considers each possible solution for the given instance, computes its cost, and outputs the cheapest. Because each instance has an exponential number of solutions, this algorithm takes exponential time.  The Greedy Choice: The greedy step is the ﬁrst that would come to mind when de- signing an algorithm for a problem. Given the set of objects speciﬁed in the input instance, the greedy step chooses and commits to one of these objects because, ac- cording to some simple criterion, it seems to be the best. When proving that the algo- rithm works, we must be able to prove that this locally greedy choice does not have negative global consequences.  EXAMPLE 16.1.1  The Game Show  Suppose the instance speciﬁes a set of prizes and an integer m and allows you to choose m of the prizes. The criterion, according to which some of these prizes appear to be better than others, may be its dollar price, the amount of joy it would bring you, how practical it is, or how much it would impress the neighbors. At ﬁrst it seems ob- vious that you should choose your ﬁrst prize us- ing the greedy approach. However, some of these prizes conﬂict with each other, and  as is often the case in life  compromises need to be made. For example, if you take the pool, then your yard is too full to be able to take many of the other prizes. A lion might impress your neighbors, but it might eat your dog. As is also true in life, it is sometimes hard to look into the future and predict the ramiﬁcations of the choices made today.  EXAMPLE 16.1.2 Making Change  The goal of this optimization problem is to ﬁnd the minimum number of quarters, dimes, nickels, and pennies that total to a given amount. An instance consists of a set of objects and a relationship between them. Here, the set is a huge pile of coins, and the relationship is that the chosen coins must total to the given amount. The cost of a solution, which is to be minimized, is the number of coins in the solution.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes16 CUUS154-Edmonds 978 0 521 84931 9  Greedy Algorithms  April 2, 2008  20:31  227  EXAMPLE 16.1.2 Making Change  cont.   The Greedy Choice: The coin that appears to be best to take is a quarter, because it makes the most progress towards making our required amount while only incurring a cost of one.  A Valid Choice: Before committing to a quarter, we must make sure that it does not conﬂict with the possibility of arriving at a valid solution. If the sum to be obtained happens to be less than $0.25, then this quarter should be rejected, even though it appears at ﬁrst to be best. On the other hand, if the amount is at least $0.25, then we can commit to the quarter without invalidating the solution we are building.  Leading to an Optimal Solution: A much more difﬁcult and subtle question is whether or not committing to a quarter leads us towards obtaining an optimal solu- tion. In this case, it happens that it does, though this not at all obvious.  Going Wrong: Suppose that the problem is generalized to include as part of the input the set of coin denominations available. Then greedy algorithm does not work. For example, suppose we have 4-, 3-, and 1-cent coins. If the given amount is 6, than the optimal solution contains two 3-cent coins. We go wrong by greedily committing to a 4-cent coin.  Proof of Correctness Using Loop Invariants: Committing to the pool, to the lion, or to the 4-cent coin, though they locally appear to be the best objects, does not lead to an optimal solution. However, for some problems and for some deﬁnitions of “best,” the greedy algorithm does work. When it does not work, we will prove it using one simple counterexample as we did above. When it does work, we will prove it using the same method with which we prove all iterative algorithms correct: loop invariants.  Main Steps: Each iteration the algorithm chooses the best object from among those not considered so far and either commits to it or rejects it.  Make Progress: One more object has been considered.  Exit Condition: All objects have been considered.  Loop Invariant: Recall that a loop invariant makes a statement each time the al- gorithm is at the top of the loop about what it has accomplished. So far the al- gorithm has made one iteration, during which it has committed to one of the objects. What statement do we want to make about this action?  Types of Loop Invariants: In order to ﬁnd a good loop invariants, let us start by considering the types of loop invariants considered in Section 1.4. It turns out that our chosen loop invariant will be of the narrowing-the-search-space type.  More of the Output: Recall that the more-of-the-output loop invariant states that the ﬁrst i items of the output have been produced and the output   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes16 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  228  April 2, 2008  20:31  constructed so far is correct. In the case of greedy algorithms, the output is a decision about each object, and the algorithm makes another decision each it- eration. Hence, the ﬁrst part of the loop invariant is easily maintained. However, what does it even mean for the output constructed so far to be correct?  No Conﬂict Yet: If our goal is to obtain a solution that has no conﬂicts, a good loop invariant might be that within the commitments made so far, there are no conﬂicts. The problem with this is that the commitments made so far may have backed the algorithm into a corner so that future conﬂicts are inevitable. See Exercise 16.1.3.  The Postcondition as a Loop Invariant: The postcondition is that the algo- rithm has committed to an optimal solution. However, at this point we can- not talk about the algorithm’s “solution,” because before the algorithm com- pletes it may have made some commitments already, but this partial work does not yet constitute a valid solution.  Optimal So Far: The loop invariant might be “What the algorithm has done so far is optimal.” However, being optimal is a property of full solutions, and we don’t have one yet.  More of the Input: Recall that the more-of-the-input loop invariant states that if we pretend that the preﬁx of the input read so far by the algorithm is the entire in- put, then the algorithm has an optimal solution. Though this is tempting, it does not work. That what you have is optimal within this part of the instance is only a local property. It won’t necessarily guarantee global optimality. For example, if the input of the game show problem consisted of only a lion, then the optimal solution would be to take the lion. However, when there are also more objects, the lion is not part of the solution. Similarly, in the make change problem with 4-, 3-, and 1-cent coins, if the input is to make the amount 4, then one should take a 4; but not if the input is to make the amount 6.  Narrowing the Search Space: Recall the narrowing-the-search-space type of loop invariant. While searching for something, the algorithm narrows the search space. The loop invariant is “If the thing being searched for is anywhere, then it is in this narrowed search space.” Greedy algorithms are in fact searching for an optimal solution from the search space of all possible solutions. Every time the algorithm commits to something, the set of possible solutions that it might still output narrows to those that are consistent with the decisions made so far. A nat- ural loop invariant would then be “If the optimal solution being searched for is anywhere, then it is consistent with the decisions made so far.”  “The” Optimal Solution Contains the “Best” Object: Before committing to the seemingly best object or making the ﬁrst decision, we need to prove that we do not go wrong by doing so. As a ﬁrst attempt, we might try to prove that for every set of objects that might be given as an instance, the “best” of   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes16 CUUS154-Edmonds 978 0 521 84931 9  Greedy Algorithms  April 2, 2008  20:31  these objects is deﬁnitely in its optimal solution. The problem with this is that there may be more than one optimal solution. It might not be the case that all of them contain the chosen object.  At Least One Optimal Solution Remaining: Instead of requiring all optimal solutions to contain the “best” object, what we need to prove is that at least one does. The effect of this is that though committing to the “best” object may eliminate the possibility of some of the optimal solutions, it does not eliminate all of them. There is the saying, “Do not burn your bridges behind you.” The message here is slightly different. It is o.k. to burn a few of your bridges as long as you do not burn all of them.  If There Is an Optimal Solution: For most problems, it is clear that an in- stance has at least one solution  maybe a trivial solution  and hence there is a best solution. Hence, this ﬁrst conditional part of the loop invariant is usually dropped. Be aware, however, that it is sometimes needed. See Exer- cise 16.1.3.  229  The Chosen Loop Invariant: To recap, the loop invariant is that we have not gone wrong. If there is a solution, then there is at least one optimal solution consistent with the choices made so far.  The Second Step: After the “best” object has been chosen and committed to, the algorithm must continue and choose the remaining objects for the solution. You can think about this process within either the iterative or the recursive paradigm. Though the resulting algorithm is  usually  the same, having the different paradigms at your disposal can be helpful.  Iterative: In the iterative version, there is a main loop. At each iteration, you choose the best from amongst the objects that have not yet been considered. The algorithm then commits to some choice about this object. Usually, this in- volves deciding whether to commit to putting this chosen object in the solution or to commit to rejecting it.  A Valid Choice: The most common reason is that it conﬂicts with the objects committed to previously. Another reason is that the object ﬁlls no require- ments that are not already ﬁlled by the objects already committed to.  Cannot Predict the Future: At each step, the choice that is made can depend on the choices that were made in the past, but it cannot depend on the choices that will be made in the future. Because of this, no backtracking is required.  Making Change Example: The greedy algorithm for ﬁnding the minimum number of coins summing to a given amount is as follows. Commit to   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes16 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  April 2, 2008  20:31  quarters until the next quarter increases your current sum above the re- quired amount. Then reject the remaining quarters. Then do the same with the dimes, the nickels, and the pennies.  Recursive: A recursive greedy algorithm makes a greedy ﬁrst choice and then re- curses once or twice in order to solve the remaining subinstance.  230  Making Change Example: After committing to a quarter, we could subtract $0.25 from the required amount and ask a friend to ﬁnd the minimum num- ber of coins to make this new amount. Our solution will be his solution plus our original quarter.  Binary Search Tree Example: The recursive version of a greedy algorithm is more useful when you need to recurse more than once. For example, sup- pose you want to construct a binary search tree for a set of keys that mini- mizes the total height of the tree, i.e., a balanced tree. The greedy algorithm will commit to the middle key being at the root. Then it will recurse once for the left subtree and once for the right.  To learn more about how to recurse after the greedy choice has been made, see the recursive backtracking algorithms in Chapter 17.  Proof of Correctness: Greedy algorithms themselves are very easy to understand and to code. If your intuition is that they should not work, then your intuition is correct. For most optimization search problems, no greedy algorithms that are tried work. By some miracle, however, for some problems there is a greedy algorithm that works. The proof that they work, however, is very subtle and difﬁcult. As with all iter- ative algorithms, we prove that they work using loop invariants.   cid:1  cid:3  & not  cid:2 exit cid:3  & codeloop →  cid:2 LI  A Formal Proof: Chapter 1 proves that an iterative algorithm works if the loop in- variant can be established and maintained and from it the postcondition can be proved. The loop invariant here is that the algorithm has not gone wrong: there is at least one optimal solution consistent with the choices made so far. This is es- tablished   cid:2 pre cid:3  →  cid:2 LI  cid:3   by noting that initially no choices have been made and hence all optimal solutions are consistent with these choices. The loop invariant is maintained   cid:2 LI  cid:1  cid:1  cid:3   as follows. If it is true when at the top of the loop, then let optSLI denote one such solution. codeloop during the next iteration either commits to or rejects the next best object. The proof describes a method for modifying optSLI into optSours and proves that this is a valid solution, is consistent both with the choices made previously by the algo- rithm and with this new choice, and is optimal. The existence of such a optSours proves that the loop invariant has been maintained. The last step is to prove that in the end, the algorithm has a concrete optimal solution   cid:2 LI cid:3  &  cid:2 exit cid:3  →  cid:2 post cid:3  . Progress is made at each step by committing to or rejecting another object. When each object has been considered, the algorithm exits. These choices specify a   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes16 CUUS154-Edmonds 978 0 521 84931 9  Greedy Algorithms  April 2, 2008  20:31  solution. The loop invariant states there is an optimal solution consistent with these choices. Hence, the solution obtained must be optimal.  We will now redo the proof in a more intuitive, fun, and detailed way.  The Loop Invariant: The loop invariant maintained is that we have not gone wrong. There is at least one optimal solution consistent with the choices made so far, that is, containing the objects committed to so far and not containing the objects rejected so far.  231  Three Players: To help understand this proof, we will tell a story involving three characters: the algorithm, the prover, and a fairy godmother.  The Algorithm: At each iteration, the algorithm chooses the best object from amongst those not considered so far and either commits to it or rejects it.  The Prover: The prover’s task is to prove that the loop invariant is main- tained. Having a separate prover emphasizes that fact that his actions are not a part of the algorithm and hence do not need to be coded or executed.  The Fairy Godmother: Instead of the prover pretending that he has and is manipulating a hypothetical optimal solution optSLI , he can pretend that he has a fairy godmother who holds and manipulates one for him. We say that this solution witnesses the fact such a solution exists. Having a separate fairy godmother emphasizes that neither the algorithm nor the prover actually knows the solution.  Initially   cid:1  pre cid:2  →  cid:1 LI cid:2  : Initially, the algorithm has made no choices, neither committing to nor rejecting any objects. The prover then establishes the loop invariant as follows. Assuming that there is at least one legal solution, he knows that there must be an optimal solution. He goes on to note that this optimal so- lution by default is consistent with the choices made so far, because no choices have been made so far. Knowing that such a solution exists, the prover kindly asks his fairy godmother to ﬁnd one. She, being all-powerful, has no problem doing   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes16 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  April 2, 2008  20:31  this. If there are more than one equally good optimal solutions, then she chooses one arbitrarily. Maintaining the Loop Invariant   cid:1 LI nsider an arbitrary iteration.   cid:4  cid:2  & not  cid:1 exit cid:2  & codeloop →  cid:1 LI   cid:4  cid:4  cid:2  : Now co-  232  What We Know: At the beginning of this iteration, the algorithm has a set Commit of objects committed to so far, and a set Reject of objects rejected so far. The prover knows that the loop invariant is true, that is, that there is at least one optimal solution consistent with these choices made so far; how- ever, he does not know one. Witnessing that there is one, the fairy godmother is holding one such optimal solution. We will use optSLI to denote the solu- tion that she holds. In addition to containing those objects in Commit and not those in Reject, this solution may contain objects that the algorithm has not considered yet.  Taking a Step: During the iteration, the algorithm proceeds to choose the best object from amongst those not considered so far and either commits to it or rejects it. In order to prove that the loop invariant has been maintained, the prover must prove that there is at least one optimal solution consistent with both the choices made previously and this new choice. He is going to accomplish this by getting his fairy godmother to witness this fact by switch- ing to such an optimal solution.  Weakness in Communication: It would be great if the prover could simply ask the fairy godmother whether such a solution exists. However, he cannot ask her to ﬁnd such a solution if he is not already conﬁdent that it exists, because he does not want to ask her to do anything that is impossible.  Modify Instructions: The prover accomplishes his task by giving his fairy godmother detailed instructions. He starts by saying, “If it happens to be the case that the optimal solution that you hold is consistent with this new choice that was made, then we are done, because this will witness the fact that there is at least one optimal solution consistent with both the choices made previously and this new choice.” “Otherwise,” he says, “you must mod- ify the optimal solution that you have in the following ways.” The fairy god- mother follows the detailed instructions that he gives her, but gives him no feedback as to how they go. We will use optSours to denote what she con- structs.  Making Change Example: If the remaining amount required is at least $0.25, then the algorithm commits to another quarter. Excluding the committed-to coins, the fairy godmother’s optimal solution optSLI must be making up the same remaining amount. This amount must contain either an additional quarter, three dimes, two dimes and a nickel, one dime and three nickels, ﬁve nickels, or combinations with at least ﬁve   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes16 CUUS154-Edmonds 978 0 521 84931 9  Greedy Algorithms  April 2, 2008  20:31  pennies. The prover tells her to replace the three dimes with the newly committed-to quarter and nickel, and the other options with just the quarter. If the algorithm, on the other hand, rejects the next  and later all remaining  quarters because the remaining amount required is less than $0.25, then the prover is conﬁdent that optimal solution held by his fairy godmother cannot contain additional quarters either.  233  Proving That She Has a Witness: It is the job of the prover to prove that the thing optSours that his fairy godmother now holds is a valid, consistent, and optimal solution.  Proving a Valid Solution: Because he knows that what she had been holding, optSLI , at the beginning of the iteration was a valid solution, he knows that the objects in it did not conﬂict in any way. Hence, all he needs to do is to prove that he did not introduce any conﬂicts that he did not ﬁx.  Making Change Example: The prover was careful that the changes he made did not change the total amount that she was holding.  Proving Consistency: He must also prove that the solution she is now holding is consistent both with the choices made previously by the al- gorithm and with this new choice. Because he knows that what she had been holding was consistent with the previous choices, he only needs to prove that he modiﬁed it to be consistent with the new choices without messing up earlier ones.  Making Change Example: Though the prover may have removed some of the coins that the algorithm has not considered yet, he was sure not to have her remove any of the previously committed-to coins. He also managed to add the newly committed-to quarter.  Proving Optimal: You might think that proving that the solution optSours is optimal would be hard, given that we do not even know the cost of an optimal solution. However, the prover can be assured that it is opti- mal as long as its cost is the same as the optimal solution, optSLI , from which it was derived. If there were a case in which the prover managed to improve the solution, then this would contradict the fact that optSLI is optimal. This contradiction only proves that such a case will not occur. However, the prover does not need to concern himself with this prob- lem.  Making Change Example: Each change that the prover instructs his fairy godmother to make either keeps the number of coins the same or de- creases the number. Hence, because optSLI is optimal, optSours is as well.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes16 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  234  April 2, 2008  20:31  This completes the prover’s proof that his fairy godmother now has an op- timal solution consistent both with the previous choices and with the latest choice. This witnesses the fact that such a solution exists.  This proves that the loop invariant has been maintained.  Continuing: This completes everybody’s requirements for this iteration. The process is repeated over and over again. Each iteration, the algorithm commits to more about the solution, and the fairy godmother’s solution is changed to be consistent with these commitments. Exiting Loop   cid:1 LI cid:2  &  cid:1 exit cid:2  →  cid:1  post cid:2  : After the algorithm has considered every object in the instance and each has either been committed to or rejected, the algorithm exits. We still know that the loop invariant is true. Hence, the prover knows that there is an optimal schedule optSLI consistent with all of these choices. Previously, this optimal solution was only imagined. However, now we concretely know that this imagined solution consists of those objects committed to. Hence, the algorithm can return this set as the solution.  Running Time: Greedy algorithms are very fast, because they take only a small amount of time per object in the instance.  Fixed vs. Adaptive Priority: Iterative greedy algorithms come in two ﬂavors, ﬁxed priority and adaptive priority.  Fixed Priority: A ﬁxed-priority greedy algorithm begins by sorting the objects in the input instance from best to worst according to a ﬁxed greedy criterion. For example, it might sort the objects based on the cost of the object or the arrival time of the object. The algorithm then considers the objects one at a time in this order.  Adaptive Priority: In an adaptive priority greedy algorithm, the greedy criterion is not ﬁxed, but depends on which objects have been committed to so far. At each step, the next-best object is chosen according to the current greedy crite- rion. Blindly searching the remaining list of objects each iteration for the next- best object would be too time-consuming. So would re-sorting the objects each iteration according to the new greedy criterion. A more efﬁcient implemen- tation uses a priority queue to hold the remaining objects prioritized accord- ing to the current greedy criteria. This can be implemented using a heap.  See Section 10.4.   Code:  algorithm AdaptiveGreedy  set of objects    cid:2  pre-cond cid:3 : The input consists of a set of objects.  cid:2  post-cond cid:3 : The output consists of an optimal subset of them.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes16 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:31  Greedy Algorithms  begin  Put objects in a priority queue according to the initial greedy criterion Commit = ∅ % set of objects previously committed to loop   cid:2 loop-invariant cid:3 : There is at least one optimal solution consistent with the choices made so far exit when the priority queue is empty Remove “best” object from priority queue If this object does not conﬂict with those in Commit and is needed, then  Add object to Commit  end if Update the priority queue to reﬂect the new greedy criterion This is done by changing the priorities of the objects effected.  235  end loop return  Commit   end algorithm  Example: Dijkstra’s shortest-weighted-path algorithm  Section 14.3  can be considered to be a greedy algorithm with an adaptive priority criteria. It chooses the next edge to include in the optimal shortest-weighted-path tree based on which node currently seems to be the closest to s. Those yet to be chosen are organized in a priority queue. Even breadth-ﬁrst and depth-ﬁrst search can be considered to be adaptive greedy algorithms. In fact, they very closely resemble Prim’s minimal-spanning-tree algorithm  Section 16.2.3 , in how a tree is grown from a source node. They are adaptive in that as the algorithm proceeds, the set from which the next edge is chosen changes.  EXERCISE 16.1.1 We proved that the greedy algorithm does not work for the making change problem when the denominations of the coins are 4, 3, and 1 cent, but it does work when the denominations are 25, 10, 5, and 1. Does it work when the denomina- tions are 25, 10, and 1, with no nickels? EXERCISE 16.1.2 Suppose the coin denominations are c1 > c2 > ··· > cr in the or- der taken by the greedy algorithm. An interesting problem is determining whether the greedy algorithm works. A complete answer to this question is too hard. However, what restrictions on the coin denominations are sufﬁcient to ensure that the greedy algo- rithm works?   cid:1  Suppose, for each i, each coin ci is an integer multiple of the next smaller ci+1, e.g., 120, 60, 12, 3, 1. If this is true, do we know that the greedy algorithm works? If it is not true, do we know that the greedy algorithm does not work?   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes16 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:31  Optimization Problems   cid:1  Suppose, for each i, each coin is more than twice the previous, that is, ci ≥ 2ci+1. Do we know that the greedy algorithm works? If this is not true, do we know that the greedy algorithm does not work?   cid:1  Other interesting characteristics?  236  EXERCISE 16.1.3 The 2-coloring problem is as follows: Given any undirected n- node graph, color the nodes with two colors so that no edge connects two nodes of the same color  i.e., adjacent nodes always have different colors , or report that it is impossible.  1. A tempting loop invariant is “There are no adjacent nodes of the same color in my partial solution.” In order for this to be a good loop invariant, one needs to be able to arrive from Mars, knowing nothing about what the algorithm has done so far except that the loop invariant is true. From here that algorithm must be able to continue until the postcondition is obtained. Prove that this is a bad loop invariant as follows:  a  Provide a graph that has a valid 2-coloring.  b  Provide a valid partial coloring that a  faulty  algorithm may provide for   c  Prove that the algorithm has gone wrong because there are no valid colorings  which the loop invariant holds.  consistent with this partial solution.  2. This is an example in which an instance might not have any valid solution. One possible algorithmic technique is to check the validity of the solution after all the nodes have been colored. Another is to check as it proceeds, to make sure that the coloring being created has no mistakes.  Which technique is used in the binary search algorithm?  For each of these two techniques, give a loop invariant and then prove  cid:2 pre-cond cid:3  & codealg ⇒  cid:2 post-cond cid:3 .  3. Brieﬂy describe a greedy algorithm. The algorithm should be able to handle graphs  that are not connected. Is the greedy criterion used adaptive or nonadaptive? Does it need to be?  4. 5. Prove that the loop invariant is maintained.   cid:2 loop-invariant   cid:1  cid:3  & not cid:2 exit-cond cid:3   cid:1  cid:1  cid:3  . Be sure to handle boundary conditions, e.g., the  ⇒  cid:2 loop-invariant  & codeloop ﬁrst iteration, and graphs that are not connected.  6. What is the running time of your algorithm?  16.2 Examples of Greedy Algorithms  16.2.1 Example: The Job Event Scheduling Problem  Suppose that many people want to use your conference room for events and you must schedule as many of these as possible.  The version in which some events are given a higher priority is considered in Section 19.3.  We examine a ﬁxed-priority greedy algorithm.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes16 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:31  Greedy Algorithms  Speciﬁcations:  Precondition: We are given one of the following instances.  Instances: An instance is  cid:2  cid:2 s1, f1 cid:3 ,  cid:2 s2, f2 cid:3 , . . . ,  cid:2 sn, fn cid:3  cid:3 , where 0 ≤ si ≤ fi are the starting and ﬁnishing times for the ith event.  Postcondition: The output is a solution with maximum number of events sched- uled.  237  Solutions: A solution for an instance is a schedule S. This consists of a subset S ⊆ [1..n] of the events that don’t conﬂict by overlapping in time.  Measure of Success: The success of a solution S is the number of events scheduled, that is, S.  Possible Criteria for Deﬁning “Best”:  The Shortest Event fi−si: It seems that it would be best to schedule short events ﬁrst, because they increase the number of events scheduled without booking the room for a long period of time. This greedy approach does not work.  Counterexample: Suppose that the following lines indicate the starting and completing times of three events to schedule.  We would be going wrong to schedule the short event in the middle, because the only optimal schedule does not include it.  The Earliest Starting Time si or the Latest Finishing Time fi: First come ﬁrst ser- ved, which is a common scheduling algorithm, does not work either.  Counterexample:  The long event is both the earliest and the latest. Committing to scheduling it would be a mistake.  Event Conﬂicting with the Fewest Other Events: Scheduling an event that con- ﬂicts with other events prevents you from scheduling these events. Hence a rea- sonable criterion would be to ﬁrst schedule the event with the fewest conﬂicts.  Counterexample: In the following example, the middle event would be com- mitted to ﬁrst. This eliminates the possibility of scheduling four events.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes16 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:31  Optimization Problems  Commit  Figure 16.1: A set of events and those committed to by the earliest-ﬁnishing-time-ﬁrst Greedy algorithm.  238  Earliest Finishing Time fi: This criterion may seem a little odd at ﬁrst, but it makes sense. It says to schedule the event that will free up your room for some- one else as soon as possible. We will see that this criterion works for every set of events.  Example: You can trace out this algorithm on the example shown in Figure 16.1.  Code: A greedy algorithm for the event scheduling problem.  algorithm Scheduling  cid:2  cid:2 s1, f1 cid:3 ,  cid:2 s2, f2 cid:3 , . . . ,  cid:2 sn, fn cid:3  cid:3    cid:2  pre-cond cid:3 : The input consists of a set of events.  cid:2  post-cond cid:3 : The output consists of a schedule that maximizes the number of events scheduled.  begin  Sort the events based on their ﬁnishing times fi Commit = ∅ loop i = 1 . . . n % Consider the events in sorted order.  % The set of events committed to be in the schedule  if  event i does not conﬂict with an event in Commit   then  Commit = Commit ∪ {i}  end loop return Commit   end algorithm  The Loop Invariant: The loop invariant is that we have not gone wrong: There is at least one optimal solution consistent with the choices made so far, that is, containing the objects committed to so far and not containing the objects rejected so far. Initial Code   cid:1  pre cid:2  →  cid:1 LI cid:2  : Initially, no choices have been made, and hence triv- ially all optimal solutions are consistent with these choices.  Commit  j<i  optS  LI  i  j≥i  Figure 16.2: A set of events, those committed to at the current point in time, those rejected, those in the optimal solution assumed to exist, and the next event to be considered.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes16 CUUS154-Edmonds 978 0 521 84931 9  April 2, 2008  20:31  239  Greedy Algorithms  cid:4  cid:4  cid:2  : We Maintaining the Loop Invariant   cid:1 LI are at the top of the loop of the algorithm. Consider the example in Figure 16.2.   cid:4  cid:2  & not  cid:1 exit cid:2  & codeloop →  cid:1 LI  Hypothetical Optimal Solution: Let optSLI denote one of the hypothetical opti- mal schedules assumed to exist by the loop invariant.  Algorithm’s Actions: If event i conﬂicts with an event in Commit, then the al- gorithm rejects it. optSLI contains all the events in Commit and hence cannot contain event i either. Hence, the loop invariant is maintained. From here on, let us assume that i does not conﬂict with any event in Commit and hence will be added to it.  Modifying Optimal Solutions: If we are lucky, the schedule optSLI already con- tains event i. In this case, we are done. Otherwise, we will modify the schedule optSLI into another schedule optSours by adding i and removing any events that conﬂict with it. This modifying is not part of the algorithm, as we do not actually have an optimal schedule yet.  A Valid Solution: Our modiﬁed set optSours contains no conﬂicts, because optSLI contained none and we were careful to introduce none.  Consistent with Choices Made: optSLI was consistent with the previous choices. We added event i to make optSours consistent with these choices. We did not re- move any events from Commit, because these do not conﬂict with event i.  Optimal: To prove that optSours has the optimal number of events in it, we need only to prove that it has at least as many as optSLI . We added one event to the schedule. Hence, we must prove that we have not removed more than one. Let j denote a deleted event. Being in optSLI and not in Commit, it must be an event not yet considered. Because the events are sorted based on their ﬁnishing time, j > i implies that event j ﬁnishes after event i ﬁnishes, that is, fj ≥ fi. If event j conﬂicts with event i, it follows that it also starts before it ﬁnishes, i.e., s j ≤ fi.  In Figure 16.2, there are three future events conﬂicting with i.  Combining fj ≥ fi and s j ≤ fi gives that such an event j is running at the ﬁnishing time fi of event i. Hence, any two such events j conﬂict with each other. Therefore, they cannot both be in the schedule optSLI , because it contains conﬂicts.  Loop Invariant Has Been Maintained: In conclusion, we have constructed a valid optimal schedule optSours that contains the events in Commit ∪ {i} and no rejected events. This proves that the loop invariant is maintained.  Exiting Loop   cid:1 LI cid:2  &  cid:1 exit cid:2  →  cid:1  post cid:2  : By LI, there is an optimal schedule optSLI containing the events in Commit and not containing the previous events not in Commit. Because all events are previous events, it follows that Commit = optSLI is in an optimal schedule for our instance.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes16 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  April 2, 2008  20:31  Figure 16.3: An example instance of the interval cover problem. The intervals in one optimal solution are highlighted.  240  Running Time: The loop is iterated once for each of the n events. The only work is determining whether event i conﬂicts with a event within Commit. Because of the ordering of the events, event i ﬁnishes after all the events in Commit. Hence, it con- ﬂicts with an event in Commit if and only if it starts before the last ﬁnishing time of an event in it. It is easy to remember this last ﬁnishing time, because it is simply the ﬁnishing time of the last event to be added to Commit. Hence, the main loop runs in  cid:1  n  time. The total time of the algorithm then is dominated by the time to sort the events.  16.2.2 Example: The Interval Cover Problem  For this problem, we will develop an adaptive priority greedy algorithm.  Speciﬁcations: Given a set of points and intervals, the goal is to ﬁnd an optimal cover, that is, a subset of the intervals that covers all the points and that contains the minimum number of intervals.  Precondition: We are given one of the following instances.  Instances: An instance consists a set P of points and a set I of intervals on the real line. An interval consists of a starting and a ﬁnishing time  cid:2 si, fi cid:3 . See the example in Figure 16.3.  Postcondition: The output is a solution with minimum cost and the cost of that solution.  Solutions: A solution for an instance is a subset S of the intervals that covers all the points. It is ﬁne if the intervals overlap. Measure of Success: The cost of a solution S is S, the number of intervals required. Having longer or shorter intervals does not matter. Covering the points more than once does not matter. Covering parts of the line without points does not matter. Only the number of intervals matters.  The Adaptive Greedy Criterion: The algorithm sorts the points and covers them in order from left to right. If the intervals committed to so far cover all of the points in P, then the algorithm stops. Otherwise, let Pi denote the leftmost point in P that is not covered by Commit. The next interval committed to must cover this next uncov- ered point, Pi. Of the intervals that start to the left of the point, the algorithm greedily takes the one that extends as far to the right as possible. The hope in doing so is that the chosen interval, in addition to covering Pi, will cover as many other points as pos- sible. Let Ij denote this interval. If there is no such interval Ij or it does not extend to   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes16 CUUS154-Edmonds 978 0 521 84931 9  Greedy Algorithms  April 2, 2008  20:31  241  the right far enough to cover the point, Pi, then no interval covers this point, and the algorithm reports that no subset of the intervals covers all of the points. Otherwise, the algorithm commits to this interval by adding it to Commit. This greedy criterion with which to select the next interval changes as the point Pi to be covered changes.   cid:4  cid:2  & not  cid:1 exit cid:2  & codeloop →  cid:1 LI  The Loop Invariant: The loop invariant is that we have not gone wrong: There is at least one optimal solution consistent with the choices made so far, that is, containing the objects committed to so far and not containing the objects rejected so far. Maintaining the Loop Invariant   cid:1 LI  cid:4  cid:4  cid:2  : Assume that we are at the top of the loop and that the loop invariant is true, so that there exists an optimal cover that contains all of the intervals in Commit. Let optSLI denote such a cover that is assumed to exist. If we are lucky and optSLI already contains the interval Ij being committed to in this iteration, then we automatically know that there exists an optimal cover that contains all of the intervals in Commit ∪ {Ij} and hence the loop invariant has been maintained. If, on the other hand, the interval Ij being committed to is not in opt SLI , then we must modify this optimal solution into another optimal solution that does contain it.  Modifying optSLI into optSours: The optimal solution optSLI must cover the point Pi. Let Ij  cid:1  denote one of the intervals in optSLI that covers Pi. Our solution optSours is the same as optSLI except that Ij  cid:1  is removed and Ij is added. We know that Ij  cid:1  is not in Commit, because the point Pi is not covered by Commit. Hence, as constructed, optSours contains all the intervals in Commit ∪ {Ij}. optSours Is an Optimal Solution: Because optSLI is an optimal cover, we can prove that optSours in an optimal cover, by proving that it covers all of the points covered by optSLI and that it contains the same number of intervals.  The algorithm considered the point Pi because it was the leftmost uncovered point. It follows that the intervals in Commit cover all the points to the left of Pi. The interval Ij  cid:1 , because it covers Pi, must start to the left of Pi. Hence, the algorithm must have considered it when it chose Ij . Now Ij , being the interval that extends as far to the right as possible of those that start to the left of Pi, must extend at least as far to the right as Ij  cid:1 , and hence Ij covers as many points to the right as Ij  cid:1  covers. It follows that optSours covers all of the points covered by optSLI . Because optSours is an optimal solution containing Commit ∪ {Ij}, we have  proved such a solution exists. Hence, the loop invariant has been maintained.  Maintaining the Greedy Criterion: As the point Pi to be covered changes, the greedy criterion according to which the next interval is chosen changes. Blindly searching for the interval that is best according to the current greedy criterion would be too time-consuming. The following data structures help to make the algorithm more efﬁcient.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes16 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  242  April 2, 2008  20:31  An Event Queue: The progress of the algorithm can be viewed as an event marker moving along the real line. An event in the algorithm occurs when this marker reaches either the start of an interval or a point to be covered. This is imple- mented not with an actual marker, but with an event queue. The queue is con- structed initially by sorting the intervals according to their start time and the points according to their position, and merging these two lists together. The al- gorithm removes and processes these events one at a time.  Additional Loop Invariants: The following additional loop invariants relate the current position event marker with to current greedy criterion.  LI1, Points Covered: All the points to the left of the event marker have been covered by the intervals in Commit.  LI2, the Priority Queue: A priority queue contains all intervals  except pos- sibly those in Commit  that start to the left of the event marker. The priority according to which they are organized is how far fj to the right the inter- val extends. This priority queue can be implemented using a heap.  See Sec- tion 10.4.   LI3, Last Place Covered: A variable last indicates the rightmost place cov- ered by an interval in Commit.  Maintaining the Additional Loop Invariants:  A Start Interval Event: When the event marker passes the starting time s j of an interval Ij , this interval from then on will start to the left of the event marker and hence is added to the priority queue, its priority being its ﬁnish- ing time fj .  An End Interval Event: When the event marker passes the ﬁnishing time fj of an interval Ij , we learn that this interval will not be able to cover future points Pi. Though the algorithm no longer wants to consider this interval, the algorithm will be lazy and leave it in the priority queue. Its priority will be lower than those actually covering Pi. The algorithm does not consider these events, there being nothing useful to do with them.  A Point Event: When the event marker reaches a point Pi in P, the algorithm uses last ≥ Pi to check whether the point is already covered by an interval in Commit. If it is covered, then nothing needs to be done. If not, then LI1 assures us that this point is the leftmost uncovered point. LI2 ensures that the priority queue is organizing the intervals according to the current greedy criterion, namely, it contains all intervals that start to the left of the point Pi, sorted according to how far to the right the interval extends. Let Ij denote the highest-priority interval in the priority queue. Assuming that it covers Pi, the algorithm commits to it. As well, if it covers Pi, then it must extend further to the right than other intervals in Commit, and hence last is updated to fj .  The   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes16 CUUS154-Edmonds 978 0 521 84931 9  Greedy Algorithms  April 2, 2008  20:31  algorithm can either remove the interval Ij committed to from the priority queue or not. That interval will not extend far enough to the right to cover the next uncovered point, and hence its priority will be low in the queue.   Code: Interval point cover.  243  algorithm IntervalPointCover  P, I    cid:2  pre-cond cid:3 : P is a set of points, and I is a set of intervals on a line.  cid:2  post-cond cid:3 : The output consists of the smallest set of intervals that covers all of the points.  begin  Sort P = {P1, . . . , Pn} in ascending order of pi’s. Sort I = { cid:2 s1, f1 cid:3 , . . . ,  cid:2 sm, fm cid:3 } in ascending order of s j ’s. Events = Merge P, I   % sorted in ascending order consideredI = ∅ % the priority queue of intervals being considered Commit = ∅ % solution set: covering subset of intervals last = −∞ % rightmost point covered by intervals in Commit for each event e ∈ Events, in ascending order do  if e =  cid:2 s j , fj cid:3   then  Insert interval  cid:2 s j , fj cid:3  into the priority queue consideredI with priority fj else  e = Pi   if Pi > last  then % Pi is not covered by Commit   cid:2 s j , fj cid:3  = ExtractMax consideredI   % fj is max in consideredI if consideredI was empty or Pi > fj   then  else  return  Pi cannot be covered  Commit = Commit ∪ {j} last = fj  end if  end if  end if  end for return  Commit   end algorithm  Running Time: The initial sorting takes O  n + m  log n + m   time. The main loop iterates n + m times, once per event. Since H contains a subset of I , the priority queue operations Insert and ExtractMax each take O log m  time. The remaining operations of the loop take O 1  time per iteration. Hence, the loop takes a total of O  n + m  log m  time. Therefore, the running time of the algorithm is O  n + m  log n + m  .   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes16 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  April 2, 2008  20:31  16.2.3 Example: The Minimum-Spanning-Tree Problem  Suppose that you are building a network of computers. You need to decide which pairs of computers to run a communication line between. You want all of the com- puters to be connected via the network. You want to do it in a way that minimizes your costs. This is an example of the minimum-spanning-tree problem.  244  Deﬁnitions: Consider a subset S of the edges of an undirected graph G.  A Tree: S is said to be tree if it contains no cycles and is connected.  Spanning Set: S is said to span the graph iff every pair of nodes is connected by a path through edges in S.  Spanning Tree: S is said to be a spanning tree of G iff it is a tree that spans the graph. Cycles will cause redundant paths between pairs of nodes.  Minimal Spanning Tree: S is said to be a minimal spanning tree of G iff it is a spanning tree with minimal total edge weight.  Spanning Forest: If the graph G is not connected, then it cannot have a spanning tree. S is said to be a spanning forest of G iff it is a collection of trees that spans each of the connected components of the graph. In other words, pairs of nodes that are connected by a path in G are still connected by a path if we consider only the edges in S.  Speciﬁcation: The goal of the minimum-spanning-tree  -forest  problem is to ﬁnd a minimum spanning tree  forest  for a given graph.  Precondition: We are given one of the following instances.  Instances: An instance consists of an undirected graph G. Each edge {u, v} is labeled with a real-valued  possibly negative  weight w{u,v}.  Postcondition: The output is a solution with minimum cost and the cost of that solution.  Solutions: A solution for an instance is a spanning tree  forest  S of the graph G.  Measure of Success: The cost of a solution S is the sum of its edge weights.  Possible Criteria for Deﬁning “Best”:  Cheapest Edge  Kruskal’s Algorithm : The obvious greedy algorithm simply commits to the cheapest edge that does not create a cycle with the edges com- mitted to already. See the example in Figure 16.4.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes16 CUUS154-Edmonds 978 0 521 84931 9  Greedy Algorithms  April 2, 2008  20:31  245  Checking for a Cycle: One task that this algorithm must be able to do quickly is to determine whether the new edge i creates a cycle with the edges in Commit. As a task in itself, this would take a while. However, if we maintain an extra data structure, this task can be done very quickly.  Connected Components of Commit: We partition the nodes in the graph into sets so that between any two nodes in the same set, Commit pro- vides a path between them, and between any two nodes in different sets, Commit does not connect them. These sets are referred to as compo- nents of the subgraph induced by the edges in Commit. The algorithm can determine whether the new edge i creates a cycle with its edges in Commit by checking whether the end points of the edge i = {u, v} are contained in the same component. The required operations on compo- nents are handled by the following union–ﬁnd data structure.  Union–Find Set System: This data structure maintains a number of dis- joint sets of elements and allows three operations:  1  Makeset v , which creates a new set containing the speciﬁed element v;  2  Find v , which determines the name of the set containing a speciﬁed element; and  3  Union u, v , which merges the sets containing the speciﬁed elements u and v. On average, for all practical purposes, each of these operations can be competed in a constant amount of time. See Section 3.1.  Code: Kruskal’s algorithm with the union–ﬁnd data structure incorpo- rated is as follows.  algorithm KruskalMST  G   cid:2  pre-cond cid:3 : G is an undirected graph.  cid:2  post-cond cid:3 : The output consists of a minimal spanning tree. begin  Sort the edges based on their weights w{u,v}. Commit = ∅ % The set of edges committed to loop for each v  % With no edges in Commit, each node is in a component by itself. MakeSet v   end for loop i = 1 . . . m % Consider the edges in sorted order.  u and v are end points of edge i. if  Find u   cid:13 = Find v    then  % The end points of edge i are in different components and hence do not create a cycle with edges in Commit. Commit = Commit ∪ {i}   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes16 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  April 2, 2008  20:31  Union u, v  % Edge i connects the two components:  hence they are merged into one component.  246  end if end loop return Commit   end algorithm  Running Time: The initial sorting takes O m log m  time when G has m edges. The main loop iterates m times, once per edge. Checking for a cycle takes time α n  ≤ 4 on average. Therefore, the running time of the algorithm is O m log m .  Cheapest Connected Edge  Prim’s Algorithm : The following greedy algorithm expands out a tree of edges from a source node as done in the generic search algorithm of Section 14.1. At each iteration it commits to the cheapest edge of those that expand this tree, that is, the cheapest from amongst those edges that are connected to the tree Commit and yet do not create a cycle with it. See the example in Figure 16.4.  Advantage: If you are, for example, trying to ﬁnd a minimum spanning tree of the World Wide Web, then you may not know about an edge until you have expanded out to it.  Input graph:  s  81  53  61  42  97  16  12  67  84  64  56  27  95  75  89  85  99  Prim’s algorithm: priority queue  53  61  42  81  53  61  27  95  81  53  61  12  64  85  99  95  81  53  61  67  84  64  85  99  95  81  61  67  84  64  85  99  95  81  56  67  84  64  85  99  95  81  97  75  67  84  64  85  99  95  81  97  75  84  85  99  95  81  97  16  89  84  85  99  95  81  97  89  84  85  99  95  81  97  89  99  95  97  99  95  Kruskal’s algorithm:  12  16  27  42  53  56  61  64  67  75  81  84  85  89  95  97  99   Figure 16.4: Both Kruskal’s and Prim’s algorithms are run on the given graph. For Kruskal’s algorithm the sorted order of the edges is shown. For Prim’s algorithm the running content of the priority queue is shown  edges are in no particular order . Each iteration, the best edge is considered. If it does not create a cycle, it is added to the minimum spanning tree. This is shown by circling the edge weight and darkening the graph edge. For Prim’s algorithm, the lines out of the circles indicate how the priority queue is updated. If the best edge does create a cycle, then the edge weight is crossed out.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes16 CUUS154-Edmonds 978 0 521 84931 9  Greedy Algorithms  April 2, 2008  20:31  Adaptive: This is an adaptive greedy algorithm, because which edges are considered changes as the algorithm proceeds. A priority queue is main- tained with the allowed edges, with priorities given by their weights w{u,v}. Because the allowed edges are those that are connected to the tree Commit, when a new edge i is added to Commit, the edges connected to i are added to the queue.  247  Code: Prim’s MST algorithm. algorithm PrimMST  G   cid:2  pre-cond cid:3 : G is an undirected graph.  cid:2  post-cond cid:3 : The output consists of a minimum spanning tree. begin  Let s be the start node in G. Commit = ∅ Queue = edges adjacent to s % Priority queue Loop until Queue = ∅  % The set of edges committed to  i = cheapest edge in Queue if  edge i does not create a cycle with the edges in Commit   then Commit = Commit ∪ {i} Add to Queue edges adjacent to edge i that have not been added before  end if end loop return Commit   end algorithm  Checking for a Cycle: As in the generic search algorithm of Section 14.1, be- cause one tree is grown, one end of the edge i will have been found already. It will create a cycle iff the other end has also been found already.  Running Time: The main loop iterates m times, once per edge. The priority queue operations Insert and ExtractMax each takes O log m  time. Therefore, the running time of the algorithm is O m log m .  A More General Algorithm: When designing an algorithm it is best to leave as many implementation details unspeciﬁed as possible. This gives more freedom to anyone who may want to implement or to modify your algorithm. Also, it pro- vides better intuition as to why the algorithm works. The following is a greedy criterion that is quite general.  Cheapest Connected Edge of Some Component: Partition the nodes of the graph G into connected components of nodes that are reachable from each other only through the edges in Commit. Nodes with no adjacent edges will be in a component by themselves. Each iteration, the algorithm is free to   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes16 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  248  April 2, 2008  20:31  choose one of these components for any reason it likes. Denote this com- ponent by C.  C can be the union of a number of different components.  Then the algorithm greedily commits to the cheapest edge of those that ex- pand this component, that is, the cheapest from amongst those edges that are connected to the component and yet do not create a cycle with it. When- ever it likes, the algorithm also has the freedom to throw away uncommitted edges that create a cycle with the edges in Commit.  Generalizing: This greedy criterion is general enough to include both Kruskal’s and Prim’s algorithms. Therefore, if we prove that this greedy algo- rithm works no matter how it is implemented, then we automatically prove that both Kruskal’s and Prim’s algorithms work.  Cheapest Edge  Kruskal’s Algorithm : This general algorithm may cho- ose the component that is connected to the cheapest uncommitted edge that does not create a cycle. Then when it chooses the cheapest edge out of this component, it gets the overall cheapest edge.  Cheapest Connected Edge  Prim’s Algorithm : This general algorithm may always choose the component that contains the source node s. This amounts to Prim’s algorithm.  The Loop Invariant: The loop invariant is that we have not gone wrong. There is at least one optimal solution consistent with the choices made so far.   cid:4  cid:2  & not  cid:1 exit cid:2  & codeloop →  cid:1 LI   cid:4  cid:4  cid:2  : See the Maintaining the Loop Invariant   cid:1 LI example in Figure 16.5. If we are unlucky and the optimal minimum spanning tree optSLI that is assumed to exist by the loop invariant does not contain the edge i being committed to this iteration, then we must modify this optimal solution optSLI into another one optSours that contains all of the edges in Commit ∪ {i}. This proves that the loop invariant has been maintained.  Modifying optSLI into optSours: To the optimal solution optSLI we add the edge i = {u, v} being committed to this iteration. Because optSLI spans the graph G, there is some path P within it from node u to node v. This path along with the edge i = {u, v} creates a cycle, which must be broken by removing one edge from P. Let C denote the component of Commit that the general greedy algorithm chooses the edge i from. Because i expands the component without creating a cycle with it, one and only one of the edge i’s nodes u and v is within the com- ponent. Hence, this path P starts within C and ends outside of C. Hence, there must be some edge j in the path that leaves C. We will delete this edge from our optimal minimum spanning tree: optSours  ∪ {i} − {j}.  = optSLI   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes16 CUUS154-Edmonds 978 0 521 84931 9  Greedy Algorithms  C  j  not shown  Boundries of components Edges in Commit Edges from which to choose i Edges in Opt LI Other edges  i  April 2, 2008  20:31  249  Figure 16.5: C is one of the components of the graph induced by the edges in Commit. Edge i is the cheapest out of C. Edge j is an edge in optSLI that is out of C. Finally, optSours is formed by removing j and adding i.  optSours is an Optimal Solution:  optSours Has No Cycles: Because optSLI has no cycles, we create only one cy- cle by adding edge i, and we destroy this cycle by deleting edge j .  optSours Spans G: Because optSLI spans G, optSours spans it as well. Any path between two nodes that goes through edge j in optSLI will now follow the remaining edges of path P together with edge i in optSours.  optSours Has Minimum Weight: Because optSLI has minimum weight, it is sufﬁcient to prove that edge i is at least as cheap as edge j . Note that by construction edge j leaves component C and does not create a cycle with it. Because edge i was chosen because it was the cheapest such edges,  or at least one of the cheapest  it is true that edge i is at least as cheap as edge j .  EXERCISE 16.2.1 Show how the earliest-ﬁnishing-time criterion works on the three counterexamples in Section 16.2.1 and on the example in Figure 16.1.  EXERCISE 16.2.2  See solution in Part Five.  For the example in Section 16.2.2, con- sider the greedy criterion that selects the interval that covers the largest number of un- covered points. Does this work? If not, give a counterexample.  EXERCISE 16.2.3 In Figure 16.4, suppose we decide to change the weight 56 to some other real number from −∞ to +∞. What is the interval of values that it could be changed to for which the minimum spanning tree remains the same? Explain your answer. Similarly for the weight 85.   April 2, 2008  20:31  P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes16 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  16.3 Exercises  250  EXERCISE 16.2.4 Give a simple graph with edge weights for which the tree of shortest weighted paths from node s is not a minimum spanning tree.  EXERCISE 16.3.1 One aspect of the game called magic  or jugio  is as follows. You have n defense cards, the ith of which, denoted Di, has worth wi and defense ability di. Your opponent has n attack cards, the j th of which, denoted A j , has attack ability a j . You can see all of the cards. Your task is to deﬁne a one-to-one matching between the attacking cards and the defending cards, that is, each attack card is allocated to a unique defense card. If card Di is defending against A j and di < a j , then di dies. Your goal is to maximize the sum of the worths wi of your cards that live. Give your algorithm. Then prove that it works.  EXERCISE 16.3.2  See solution in Part Five.  Review the job event scheduling prob- lem from Section 16.2.1. This problem is the same except you have r rooms processors within which to schedule that the set of jobs events in two rooms or on two processors. An instance is  cid:2 r,  cid:2 s1, f1 cid:3 ,  cid:2 s2, f2 cid:3 , . . . ,  cid:2 sn, fn cid:3  cid:3 , where, as before, 0 ≤ si ≤ fi are the start- ing and ﬁnishing times for the ith event. But now the input also speciﬁes the number of rooms, r . A solution for an instance is a schedule S =  cid:2 S1, . . . , Sr cid:3  for each of the rooms. Each of these consists of a subset Sj ⊆ [1..n] of the events that don’t conﬂict by over- lapping in time. The success of a solution S is the number of events scheduled, that is,  ∪j∈[r ] S. Consider the following four algorithms: 1. Find the greedy solution for the ﬁrst room. Then ﬁnd the greedy solution for the  second room from the remaining events. Then for the third room, and so on.  2. The algorithm starts by sorting the events by their ﬁnishing times, just as in the one-room case. Then, it looks at each event in turn, scheduling it if possible. If it can be scheduled in more than one room, we assign it in the ﬁrst room in which it ﬁts, i.e., ﬁrst try room 1, then room 2, and so on, until it ﬁts. If it cannot be scheduled in any room, then it is not scheduled.  3. The same, except that the next event is scheduled in the room with the latest last- scheduled ﬁnishing time. For example, if the last event scheduled in room 5 ﬁn- ishes at time 10 and the last event scheduled in room 12 ﬁnishes at time 15 and the next event starts at time 17, then the next event could be scheduled into either of these rooms. This algorithm would schedule it, however, in room 12.  4. The same except that the next event is scheduled in the room with the earliest last-  scheduled ﬁnishing time.  Prove that three of these algorithms do not lead to an optimal schedule, and the re- maining one does.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes17 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  20:37  17 Recursive Backtracking  251  The brute force algorithm for an optimization problem is to simply compute the cost or value of each of the exponential number of possible solutions and return the best. A key problem with this algorithm is that it takes exponential time. Another  not ob- viously trivial  problem is how to write code that enumerates over all possible solu- tions. Often the easiest way to do this is recursive backtracking. The idea is to design a recurrence relation that says how to ﬁnd an optimal solution for one instance of the problem from optimal solutions for some number of smaller instances of the same problem. The optimal solutions for these smaller instances are found by recursing. After unwinding the recursion tree, one sees that recursive backtracking effectively enumerates all options. Though the technique may seem confusing at ﬁrst, once you get the hang of recursion, it really is the simplest way of writing code to accomplish this task. Moreover, with a little insight one can signiﬁcantly improve the running time by pruning off entire branches of the recursion tree. In practice, if the instance that one needs to solve is sufﬁciently small and has enough structure that a lot of pruning is possible, then an optimal solution can be found for the instance reason- ably quickly. For some problems, the set of subinstances that get solved in the recur- sion tree is sufﬁciently small and predictable that the recursive backtracking algo- rithm can be mechanically converted into a quick dynamic programming algorithm. See Chapter 18. In general, however, for most optimization problems, for large worst case instances, the running time is still exponential.  17.1 Recursive Backtracking Algorithms  An Algorithm as a Sequence of Decisions: An algorithm for ﬁnding an optimal solution for your instance must make a sequence of small decisions about the solu- tion: “Do we include the ﬁrst object in the solution or not?” “Do we include the sec- ond?” “The third?” . . . , or “At the ﬁrst fork in the road, do we go left or right?” “At the second fork which direction do we go?” “At the third?” . . . . As one stack frame in the recursive algorithm, our task is to deal only with the ﬁrst of these decisions. A recur- sive friend will deal with the rest. We saw in Chapter 16 that greedy algorithms make   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes17 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  April 5, 2008  20:37  decisions simply by committing to the option that looks best at the moment. How- ever, this usually does not work. Often, in fact, we have no inspirational technique to know how to make each decision in a way that leads to an optimal  or even a sufﬁ- ciently good  solution. The difﬁculty is that it is hard to see the global consequences of the local choices that we make. Sometimes a local initial sacriﬁce can globally lead to a better overall solution. Instead, we use perspiration. We try all options.  252  EXAMPLE 17.1.1  Searching a Maze  When we come to a fork in the road, all possible directions need to be tried. For each, we get a friend to search exhaustively, backtrack to the fork, and report the highlights. Our task is to determine which of these answers is best overall. Our friends will have their own forks to deal with. However, it is best not to worry about this, since their path is their responsibility, not ours.  High-Level Code: The following is the basic structure that the code will take.  algorithm Alg  I    cid:2  pre-cond cid:3 : I is an instance of the problem.  cid:2  post-cond cid:3 : optSol is one of the optimal solutions for the instance I , and optCost is its cost. begin  if  I is small   then  return  brute force answer    else  % Deal with the ﬁrst decision by trying each of the K possibilities.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes17 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  20:37  Recursive Backtracking  for k = 1 to K  % Temporarily, commit to making the ﬁrst decision in the kth way.  cid:2 optSolk, optCostk  cid:3  = Recursively deal with all the remaining deci- sions, and in doing so ﬁnd the best solution optSolk for our instance I from among those consistent with this kth way of making the ﬁrst decision. optCostk is its cost.  end for % Having the best, optSolk, for each possibility k, we keep the best of these best. kmin = “a k that minimizes optCostk” optSol = optSolkmin optCost = optCostkmin return  cid:2 optSol, optCost cid:3   253  end if  end algorithm  EXAMPLE 17.1.2  Searching for the Best Animal  Suppose, instead of searching through a structured maze, we are searching through a large set of objects, say for the best animal at the zoo. See Figure 17.1. Again we break the search into smaller searches, each of which we delegate to a friend. We might ask one friend for the best vertebrate and another for the best invertebrate. We will take the better of these best as our answer. This algorithm is recursive. The friend with the vertebrate task asks a friend to ﬁnd the best mammal, another for the best bird, and another for the best reptile.  A Classiﬁcation Tree of Solutions: This algorithm unwinds into the tree of stack frames that directly mirrors the taxonomy tree that classiﬁes animals. Each solution is identiﬁed with a leaf.  Iterating through the Solutions to Find the Optimal One: This algorithm amounts to using depth-ﬁrst search  Section 14.4  to traverse this classiﬁcation tree, iterating through all the solutions associated with the leaves. Though this algorithm may seem complex, it is often the easiest way to iterate through all solutions.  Speeding Up the Algorithm: This algorithm is not any faster than the brute force al- gorithm that simply compares each animal with every other. However, the structure that the recursive backtracking adds can possibly be exploited to speed up the algo- rithm. A branch of the tree can be pruned off when we know that this does not elim- inate all optimal solutions. Greedy algorithms  Chapter 16  prune off all branches ex- cept one path down the tree. In Section 18.2, we will see how dynamic programming reuses the optimal solution from one subtree within another subtree.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes17 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  April 5, 2008  20:37  animal  vertebrate  invertebrate  mammal  bird  reptile  254  Homo  canine  lizard  snake  H. sapiens  cat  bear  dad  gamekeeper gamekeeper  cheetah  black  panda  polar  Figure 17.1: Classiﬁcation tree of animals.  The Little Bird Abstraction: I like to use a little bird abstraction to help focus on two of the most difﬁcult and creative parts of designing a recursive backtracking al- gorithm.  What Question to Ask: The key difference between searching a maze and searching for the best ani- mal is that in the ﬁrst the forks are ﬁxed by the problem, but in the second the algorithm designer is able to choose them. Instead of forking on vertebrates vs invertebrates, we could fork on brown animals vs green animals. This choice is a difﬁcult and cre- ative part of the algorithm design process. It dictates the entire structure of the algorithm, which in turns dictates how well the algorithm can be sped up. I like to view this process of forking as asking a little bird a question, “Is the best animal a vertebrate or an invertebrate?” or “Is the best vertebrate, a mammal, a bird, a reptile, or a ﬁsh?” The classiﬁcation tree becomes a strategy for the game of twenty questions. Each sequence of possible answers  for example, vertebrate– mammal–cat–cheetah  uniquely speciﬁes an animal. Worrying only about the top level of recursion, the algorithm designer must formulate one small question about the optimal solution that is being searched for. The question should be such that having a correct answer greatly reduces your search.  Constructing a Subinstance for a Friend: The second creative part of designing a recursive backtracking algorithm is how to express the problem “Find the best mammal” as a smaller instance of the same search problem. The little bird helps again. We pretend that she answered “mammal.” Trusting  at least temporar- ily  in her answer helps us focus on the fact that we are now only considering mammals, and this helps us to design a subinstance that asks for the best one. A solution to this subinstance needs to be translated before it is in the correct form to be a solution to our instance.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes17 CUUS154-Edmonds 978 0 521 84931 9  Recursive Backtracking  April 5, 2008  20:37  Given one instance  Set of solutions for instance   Tree of questions to learn a solution  First question  ....  Possible answers  ....  ....  ....  ....  ....  ....  ....  ....  ....  ....  255  All solutions  ...  Classification of solutions bases on first question  Classification of solutions based on first question  Find best solution in each class  Choose the best of the best  Figure 17.2: Classifying solutions and taking the best of the best.  A Flock of Stupid Birds vs. a Wise Little Bird: The following two ways of thinking about the algorithm are equivalent.  A Flock of Stupid Birds: Suppose that our question about whether the opti- mal solution is a mammal, a bird, or a reptile has K different answers. For each, we pretend that a bird gave us this answer. Giving her the beneﬁt of doubt, we ask a friend to give us the optimal solution from among those that are consistent with this answer. At least one of these birds must have been telling us the truth. We ﬁnd it by taking the best of the optimal solutions ob- tained in this way. See Figure 17.2 for an illustration of these ideas.  A Wise Little Bird: If we had a little bird who would answer our questions correctly, designing an algorithm would be a lot easier: We ask the little bird “Is the best animal a bird, a mammal, a reptile, or a ﬁsh?” She tells us a mammal. We ask our friend for the best mammal. Trusting the little bird and the friend, we give this as the best animal. Just as nondeterministic ﬁ- nite automata  NFAs  and nondeterministic Turing machines can be viewed as higher powers that provide help, our little bird can be viewed as a limited higher power. She is limited in that we can only ask her questions that do not have too many possible answers, because in reality we must try all these possible answers.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes17 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  April 5, 2008  20:37  17.2 The Steps in Developing a Recursive Backtracking  This section presents the steps that I recommend using when developing a recursive backtracking algorithm. To demonstrate them, we will develop an algorithm for the queens problem.  256  EXAMPLE 17.2.1  The Queens Problem  Physically get yourself  or make on paper  a chessboard and eight tokens to act as queens. More generally, you could consider an n × n board and n queens. A queen can move as far as she pleases, horizontally, vertically, or diagonally. The goal is to place all the queens on the board in a way such that no queen is able to capture any other.  Try It: Before reading on, try yourself to place the queens on an 8-by-8 board. How would you do it?  1  Speciﬁcation: The ﬁrst step is to be very clear about what problem needs to be solved. For an optimization problem, we need to be clear about what the set of in- stances is; for each instance, what its set of solutions is; and for each solution, what its cost or value is.  Queens: The set of possible solutions is the set of ways placing the queens on the board. We do not value one solution over another as long as it is valid, i.e., no queen is able to capture any other queen. Hence, the value of a solution can simply be one if it is a valid solution and zero if not. What is not clear is what an input instance for this problem is, beyond the dimension n. We will need to generalize the problem to include more instances in order to be able to recurse. However, we will get back to that later.  2  Design a Question and Its Answers for the Little Bird: Suppose the little bird knows one of the optimal solutions for our instance. You, the algorithm designer, must formulate a small question about this solution and the list of its possible answers.  Question about the Solution: The question should be such that having a cor- rect answer greatly reduces the search. Generally, we ask for the ﬁrst part of the solution.  Queens: We might ask the bird, “Where should I place the ﬁrst queen?”  The Possible Bird Answers: Together with your question, you provide the little bird with a list A 1, A 2, . . . , A K of possible answers, and she simply returns the index k ∈ [1..K ] of her answer. To be consistent, we will always use the letter k to index the bird answers. In order for the ﬁnal algorithm to be efﬁcient, it is impor- tant that the number K of different answers be small.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes17 CUUS154-Edmonds 978 0 521 84931 9  Recursive Backtracking  April 5, 2008  20:37  Queens: Given that there are n queens and n rows and that two queens can- not be placed in the same row, the ﬁrst observation is that the ﬁrst queen must be in the ﬁrst row. Hence, there are K = n different answers the bird might give.  3  Constructing Subinstances: Suppose that the bird gives us the kth of her answers. This giving us some of the solution; we want to ask a recursive friend for the rest of the solution. The friend must be given a smaller instance of the same search prob- lem. You must formulate subinstance subI for the friend so that he returns to us the information that we desire.  257  Queens: I start by  at least temporarily  trusting the bird, and I place a queen where she says in the ﬁrst row. In order to give an instance to the friend, we need to go back to step 1 and generalize the problem. An input instance will specify the locations of queens in the ﬁrst r rows. A solution is a valid way of putting the queens in the remaining rows. Given such an instance, the question for the bird is “Where does the queen in the next row go?” Trusting the bird, we place a queen where she says. The instance, subI, we give the friend is the board with these r + 1 queens placed. Trust the Friend: We proved in Section 8.7 that we can trust the friend to pro- vide an optimal solution to the subinstance subI, because he is really a smaller recursive version of ourselves.  4  Constructing a Solution for My Instance: Suppose that the friend gives you an optimal solution opt SubSol for his instance subI . How do you produce an optimal solution opt Sol for your instance I from the bird’s answer k and the friend’s solution opt SubSol?  Queens: The bird tells you where on the r + 1st row the queen should go and your friend tells you where on the rows r + 2 to n the queens should go. Your solu- tion combines these to tell where the on the rows r + 1 to n the queens should go. We can trust the friend to give provide an optimal solution to the subin- stance subI , because he is really a smaller recursive version of ourselves. Recall, in Section 8.7, we used strong induction to prove that we can trust our recursive friends.  5  Costs of Solutions and Subsolutions: We must also return the cost optCost of our solution optSol.  Queens: The solutions in this case, don’t have costs.  6  Best of the Best: Try all the bird’s answers, and take best of the best.  Queens: If we trust both the bird and the friend, we conclude that this process ﬁnds us a best placement of the queens. If, however, our little bird gave us the wrong placement of the queen in the r + 1st row, then this might not be the best placement. However, our work was not wasted, because we did succeed in ﬁnd- ing the best placement from amongst those consistent with this bird’s answer.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes17 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  258  April 5, 2008  20:37  Not trusting the little bird, we repeat this process, ﬁnding a best placement start- ing with each of the possible bird answers. Because at least one of the bird an- swers must be correct, one of these placements must be an overall best. We re- turn the best of these best as the overall best placement.  To ﬁnd the best of these best placements, let optSolk and optCostk denote the optimal solution for our instance I and its cost that we formed when tem- porarily trusting the kth bird’s answer. Search through this list of costs optCost1, optCost2, . . . , optCostK , ﬁnding the best one. Denote the index of the chosen one by kmax. The optimal solution that we will return is then optSol = optSolkmax and its cost is optCost = optCostkmax. 7  Base Cases: The base case instances are instances of your problem that are small enough that they cannot be solved using steps 2–6, but they can be solved easily in a brute force way. What are these base cases, and what are their solutions?  Queens: If all n queens have been placed, then there is nothing to be done.  8  Code: The following code might be made slightly simpler, but in order to be con- sistent we will always use this same basic structure.  algorithm Queens  C, n, r    cid:2  pre-cond cid:3 : C =  cid:2  cid:2 1, c1 cid:3 ,  cid:2 2, c2 cid:3 , . . . ,  cid:2 r, cr cid:3  cid:3  places the j th queen in the j th row and the c j th column. The remaining rows have no queen.  cid:2  post-cond cid:3 : Returned if possible is a placement optSol of the n queens consis- tent with this initial placement of the ﬁrst r queens. A placement is legal if no two queens can capture each other. Whether this is possible is ﬂagged with optCost equal to one or zero.  begin  % Base case: If all the queens have been already been placed, then the problem is easy to solve. if  r = n   then  if  C is legal   then optSol = C & optCost = 1 else optCost = 0 return  cid:2 optSol, optCost cid:3   else % General case:  % Try each possible bird answer. loop k = 1 . . . n  % The bird-and-friend algorithm: The bird tells us the column k in which to put the r + 1st queen. We ask the friend to place the remaining queens. This will be our best solution optSolk amongst those consistent with this bird’s answer.  cid:1  = C ∪  cid:2 r + 1, k cid:3  % Place a queen at this location. C  cid:2 optSolk, optCostk , n, r + 1   cid:3  = Queens   cid:2    cid:1   C   cid:1   end for   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes17 CUUS154-Edmonds 978 0 521 84931 9  Recursive Backtracking  April 5, 2008  20:37  % Having the best, optSolk, for each bird’s answer k, we keep the best of these best. kmax = a k that maximizes optCostk, % i.e., if possible k for which optCostk optSol = optSolkmax optCost = optCostkmax return  cid:2 optSol, optCost cid:3   = 1  259  end if  end algorithm  9  Recurrence Relations: At the core of every recursive backtracking and dynamic programming algorithm is a recurrence relation. These deﬁne one element of a se- quence as a function of previous elements in the same sequence. The following are examples.  The Fibonacci Sequence: If Fib i  is the ith element in the famous Fibonacci se- quence, then Fib n  = Fib n − 1  + Fib n − 2 . The base cases Fib 0  = 0 and Fib 1  = 1 are also needed.  Running Time: If T n  is the running time of merge sort on an input of n num- bers, then we have T n  = 2T  n  2   + n. The base case T 1  = 1 is also needed.  Optimal Solution: Suppose Solution[I ] is deﬁned to be an optimal solution for instance I of a problem, and Cost[I ] is its cost. This is not a sequence of ele- ments like the previous examples, because I is not in integer. However, in Chap- ter 18 the instances will be indexed by integers i so that Solution[i] and Cost[i] are sequences. But even when I is an arbitrary input instance, a recurrence relation can be developed as follows from the bird – friend algorithm.  Solution[my instance] = Mink∈[K ][An optimal solution to my instance from those that  are consistent with the kth bird’s answer]  = Mink∈[K ][combine bird’s answer and friend’s answer] = Mink∈[K ][combine bird’s answer and Solution[friend’s instance] ]  Cost[my instance]  = Mink∈[K ][cost of optimal solution to my instance from those that  are consistent with the kth bird’s answer]  = Mink∈[K ][combine bird’s cost and friend’s cost] = Mink∈[K ][combine bird’s cost and Cost[friend’s instance] ]  Base cases “Solution[small instance] = solution” are also needed.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes17 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  April 5, 2008  20:37  Queens: For our queens example this becomes  Queens  C, n, r   = Mink∈[n]Queens  C ∪  cid:2 r + 1, k cid:3 , n, r + 1   This is bit of a silly example, because here all the work is done in the base cases.  260  Running Time: A recursive backtracking algorithm faithfully enumerates all solu- tions for your instance and hence requires exponential time. We will see that this time can be reduced by pruning off branches of the recursion tree.  17.3 Pruning Branches  The following are typical reasons why an entire branch of the solution classiﬁcation tree can be pruned off.  Invalid Solutions: Recall that in a recursive backtracking algorithm, the little bird tells the algorithm something about the solution and then the algorithm recurses by asking a friend a question. Then this friend gets more information about the solu- tion from his little bird, and so on. Hence, following a path down the recursive tree speciﬁes more and more about a solution until a leaf of the tree fully speciﬁes one particular solution. Sometimes it happens that partway down the tree, the algorithm has already received enough information about the solution to determine that it con- tains a conﬂict or defect making any such solution invalid. The algorithm can stop recursing at this point and backtrack. This effectively prunes off the entire subtree of solutions rooted at this node in the tree.  Queens: Before we try placing a queen on the square  cid:2 r + 1, k cid:3 , we should check to make sure that this does not conﬂict with the locations of any of the queens on  cid:2  cid:2 1, c1 cid:3 ,  cid:2 2, c2 cid:3 , . . . ,  cid:2 r, cr cid:3  cid:3 . If it does, we do not need to ask for help from this friend. Exercise 17.5.4 bounds the resulting running time.  Time Saved: The time savings can be huge. Recall that for Example 9.2.1 in Sec- tion 9.2, reducing the number of recursive calls from two to one decreased the running time from  cid:1  N  to  cid:1  log N , and how in Example 9.2.2 reducing the number of recursive calls from four to three decreased the running time from  cid:1  n2  to  cid:1  n1.58... .  No Highly Valued Solutions: Similarly, when the algorithm arrives at the root of a subtree, it might realize that no solutions within this subtree are rated sufﬁciently high to be optimal—perhaps because the algorithm has already found a solution provably better than all of these. Again, the algorithm can prune this entire subtree from its search.  Greedy Algorithms: Greedy algorithms are effectively recursive backtracking algo- rithms with extreme pruning. Whenever the algorithm has a choice as to which little   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes17 CUUS154-Edmonds 978 0 521 84931 9  Recursive Backtracking  April 5, 2008  20:37  bird’s answer to take, i.e., which path down the recursive tree to take, instead of it- erating through all of the options, it goes only for the one that looks best according to some greedy criterion. In this way the algorithm follows only one path down the recursive tree. Greedy algorithms are covered in Chapter 16.  Modifying Solutions: Let us recall why greedy algorithms are able to prune, so that we can use the same reasoning with recursive backtracking algorithms. In each step in a greedy algorithm, the algorithm commits to some decision about the solution. This effectively burns some of its bridges, because it eliminates some solutions from consideration. However, this is ﬁne as long as it does not burn all its bridges. The prover proves that there is an optimal solution consistent with the choices made by modifying any possible solution that is not consistent with the latest choice into one that has at least as good value and is consistent with this choice. Similarly, a recursive backtracking algorithm can prune of branches in its tree when it knows that this does not eliminate all remaining optimal solutions.  261  Queens: By symmetry, any solution that has the queen in the second half of the ﬁrst row can be modiﬁed into one that has the this queen in the ﬁrst half, simply by ﬂipping the solution left to right. Hence, when placing a queen in the ﬁrst row, there is no need to try placing it in the second half of the row.  Depth-First Search: Recursive depth-ﬁrst search  Section 14.5  is a recursive backtracking algorithm. A solution to the optimization problem of searching a maze for cheese is a path in the graph starting from s. The value of a solution is the weight of the node at the end of the path. The algorithm marks nodes that it has visited. Then, when the algorithm revisits a node, it knows that it can prune this subtree in this recursive search, because it knows that any node reach- able from the current node has already been reached. In Figure 14.9, the path  cid:2 s, c, u, v cid:3  is pruned because it can be modiﬁed into the path  cid:2 s, b, u, v cid:3 , which is just as good.  17.4 Satisﬁability  A famous optimization problem is called satisﬁability, or SAT for short. It is one of the basic problems arising in many ﬁelds. The recursive backtracking algorithm given here is referred to as the Davis–Putnam algorithm. It is an example of an algorithm whose running time is exponential for worst case inputs, yet in many practical situ- ations can work well. This algorithm is one of the basic algorithms underlying auto- mated theorem proving and robot path planning, among other things.  The Satisﬁability Problem:  Instances: An instance  input  consists of a set of constraints on the assignment to the binary variables x1, x2, . . . , xn. A typical constraint might be  cid:2 x1 or x3 or x8 cid:3 ,   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes17 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  April 5, 2008  20:37  meaning  x1 = 1 or x3 = 0 or x8 = 1  or equivalently that either x1 is true, x3 is false, or x8 is true. More generally an instance could be a more general circuit built with AND, OR, and NOT gates, but we leave this until Section 20.1.  Solutions: Each of the 2n assignments is a possible solution. An assignment is valid for the given instance if it satisﬁes all of the constraints.  Measure of Success: An assignment is assigned the value one if it satisﬁes all of the constraints, and the value zero otherwise.  Goal: Given the constraints, the goal is to ﬁnd a satisfying assignment.  262  Iterating through the Solutions: The brute force algorithm simply tries each of the 2n assignments of the variables. Before reading on, think about how you would nonrecursively iterate through all of these solutions. Even this simplest of examples is surprisingly hard.  Nested Loops: The obvious algorithm is to have n nested loops each going from 0 to 1. However, this requires knowing the value of n before compile time, which is not likely.  Incrementing Binary Numbers: Another option is to treat the assignment as an n-bit binary number and then loop through the 2n assignments by incrementing this binary number each iteration.  Recursive Algorithm: The recursive backtracking technique is able to iterate through the solutions with much less effort in coding. First the algorithm com- mits to assigning x1 = 0 and recursively iterates through the 2n−1 assignments of the remaining variables. Then the algorithm backtracks, repeating these steps with the choice x1 = 1. Viewed another way, the ﬁrst little bird question about the solutions is whether the ﬁrst variable x1 is set to zero or one, the second question asks about the second variable x2, and so on. The 2n assignments of the variables x1, x2, . . . , xn are associated with the 2n leaves of the complete binary tree with depth n. A given path from the root to a leaf commits each variable xi to being either zero or one by having the path turn to either the left or to the right when reaching the ith level.  Instances and Subinstances: Given an instance, the recursive algorithm must construct two subinstances for its friends’ to recurse with. There are two techniques for doing this.  Narrowing the Class of Solutions: Associated with each node of the classiﬁca- tion tree is a subinstance deﬁned as follows: The set of constraints remains un- changed except that the solutions considered must be consistent in the variables x1, x2, . . . , xr with the assignment given by the path to the node. Traversing a step further down the classiﬁcation tree further narrows the set of solutions.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes17 CUUS154-Edmonds 978 0 521 84931 9  Recursive Backtracking  April 5, 2008  20:37  Reducing the Instance: Given an instance consisting of a number of constraints on n variables, we ﬁrst try assigning x1 = 0. The subinstance to be given to the ﬁrst friend will be the constraints on remaining variables given that x1 = 0. For example, if one of our original constraints is  cid:2 x1 OR x3 OR x8 cid:3 , then after assign- ing x1 = 0, the reduced constraint will be  cid:2 x3 OR x8 cid:3 . This is because it is no longer possible for x1 to be true, given that one of x3 or x8 must be true. On the other hand, after assigning x1 = 1, the original constraint is satisﬁed independently of the values of the other variables, and hence this constraint can be removed.  263  Pruning: This recursive backtracking algorithm for SAT can be sped up. This can ei- ther be viewed globally as a pruning off of entire branches of the classiﬁcation tree or be viewed locally as seeing that some subinstances, after they have been sufﬁciently reduced, are trivial to solve.  Pruning Branches Off the Tree: Consider the node of the classiﬁcation tree ar- rived at down the subpath x1 = 0, x2 = 1, x3 = 1, x4 = 0, . . . , x8 = 0. All of the assignment solutions consistent with this partial assignment fail to satisfy the constraint  cid:2 x1 OR x3 OR x8 cid:3 . Hence, this entire subtree can be pruned off. Trivial Subinstances: When the algorithm tries to assign x1 = 0, the constraint  cid:2 x1 OR x3 OR x8 cid:3  is reduced to  cid:2 x3 or x8 cid:3 . Assigning x2 = 1 does not change this particular constraint. Assigning x3 = 1 reduces this constraint further to simply  cid:2 x8 cid:3 , stating that x8 must be true. Finally, when the algorithm is considering the value for x8, it sees from this constraint that x8 is forced to be one. Hence, the x8 = 1 friend is called, but the x8 = 0 friend is not.  Stop When an Assignment is Found: The problem speciﬁcation only asks for one satisfying assignment. Hence, the algorithm can stop when one is found.  Davis–Putnam: The above algorithm branches on the values of each variable, x1, x2, . . . , xn, in order. However, there is no particular reason that this order needs to be ﬁxed. Each branch of the recursive algorithm can dynamically use some heuristic to decide which variable to branch on next. For example, if there is a variable, like x8 in the preceding example, whose assignment is forced by some constraint, then clearly this assignment should be done immediately. Doing so removes this variable from all the other constraints, simplifying the instance. Moreover, if the algorithm branched on x4, . . . , x7 before the forcing of x8, then this same forcing would need to be repeated within all 24 of these branches.  If there are no variables to force, a common strategy is to branch on the variable that appears in the largest number of constraints. The thinking is that the removal of this variable may lead to the most simpliﬁcation of the instance. An example of how different branches may set the variables in a different order is the following. Suppose that  cid:2 x1 OR x2 cid:3  and  cid:2 x1 OR x3 cid:3  are two of the constraints.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes17 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  20:37  Optimization Problems Assigning x1 = 0 will simplify the ﬁrst constraint to  cid:2 x2 cid:3  and remove the second con- straint. The next step would be to force x2 = 1. On the other hand, assigning x1 = 1 will simplify the second constraint to forcing x3 = 1.  Code:  264  algorithm DavisPutnam  c   cid:2  pre-cond cid:3 : c is a set of constraints on the assignment to  cid:6 x.  cid:2  post-cond cid:3 : If possible, optSol is a satisfying assignment and optCost is also one. Otherwise optCost is zero.  begin  if  c has no constraints or no variables   then  % c is trivially satisﬁable. return  cid:2 ∅, 1 cid:3   else if  c has both a constraint forcing a variable xi to 0  and one forcing the same variable to 1  then % c is trivially not satisﬁable. return  cid:2 ∅, 0 cid:3   else  for any variable forced by a constraint to some value  substitute this value into c.  let xi be the variable that appears the most often in c % Loop over the possible bird answers. for k = 0 to 1  unless a satisfying solution has been found    cid:1   be the constraints c with k substituted in for xi  % Get help from friend. let c  cid:2 optSubSol, optSubCost cid:3  = DavisPutnam optSolk optCostk   cid:1  cid:2   cid:1  c =  cid:2 forced values, xi = k, optSubSol cid:3  = optSubCost  end for % Take the best bird answer. kmax = a k that maximizes optCostk optSol = optSolkmax optCost = optCostkmax return  cid:2 optSol, optCost cid:3   end if  end algorithm  Running Time: If no pruning is done, then clearly the running time is  cid:2  2n , as all 2n assignments are tried. Considerable pruning needs to occur to make the al- gorithm polynomial-time. Certainly in the worst case, the running time is 2 cid:1  n . In practice, however, the algorithm can be quite fast. For example, suppose that the in- stance is chosen randomly by choosing m constraints, each of which is the OR of   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes17 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  20:37  Recursive Backtracking three variables or their negations, e.g.,  cid:2 x1 OR x3 OR x8 cid:3 . If few constraints are cho- sen  say m is less than about 3n , then with very high probability there are many satisfying assignments and the algorithm quickly ﬁnds one of these assignments. If a lot of constraints are chosen,  say m is at least n2 , then with very high probability there are many conﬂicting constraints, preventing there from being any satisfying as- signments, and the algorithm quickly ﬁnds one of these contradictions. On the other hand, if the number of constraints chosen is between these thresholds, then it has been proven that the Davis–Putnam algorithm takes exponential time.  265  17.5 Exercises  EXERCISE 17.5.1  See solution in Part Five.  In one version of the game Scrabble, an input instance consists of a set of letters and a board, and the goal is to ﬁnd a word that returns the most points. A student described the following recursive backtracking algorithm for it. The bird provides the best word out of the list of letters. The friend provides the best place on the board to put the word. Why are these bad questions?  EXERCISE 17.5.2  See solution in Part Five.  Consider the following Scrabble problem. An instance consists of a set of letters and a dictionary. A solution consists of a permuta- tion of a subset of the given letters. A solution is valid if it is in the dictionary. The value of a solution depends on its placement on the board. The goal is to ﬁnd a highest-value word that is in the dictionary.  EXERCISE 17.5.3  See solution in Part Five.  Trace the queens algorithm  Sec- tion 17.2.1  on the standard 8-by-8 board. What are the ﬁrst dozen legal outputs for the algorithm? To save time note that the ﬁrst two or three queens do not move so fast. Hence, it might be worth it to draw a board with all squares conﬂicting with these crossed out.  EXERCISE 17.5.4  See solution in Part Five.  What is the running time of the queens algorithm  Section 17.2.1  for the n-by-n board when there is no pruning? Give rea- sonable upper and lower bounds on the running time of this algorithm after all the pruning occurs.  EXERCISE 17.5.5  See solution in Part Five.  An instance may have many optimal so- lutions with exactly the same cost. The postcondition of the problem allows any one of these to become output. In any recursive backtracking algorithm, which line of code chooses which of these optimal solutions will be selected?  EXERCISE 17.5.6 Suppose you are solving SAT from Section 17.4. Suppose your in- stance is x AND y, and the little bird tells you to set x to one. What is the instance that you give to your friend? Do the same for instances ¬x AND y, x OR y, and ¬x OR y.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes17 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  April 5, 2008  20:37  EXERCISE 17.5.7 Independent set: Given a graph, ﬁnd a largest subset of the nodes for which there are no edges between any pair in the set. Give the bird-and-friend abstraction of a recursive backtracking algorithm for this problem. What do you ask the bird, and what do give your friend?  266  EXERCISE 17.5.8 Graph 3-coloring  3-COL : Given a graph, determine whether its nodes can be colored with three colors so that two nodes do not have the same color if they have an edge between them. What difﬁculty arises when attempting to design a recursive backtracking algorithm for it? Redeﬁne the problem so that the input consists of a graph and a partial coloring of the nodes. The new goal is to determine whether there is a coloring of the graph consistent with the partial coloring given. Give the bird- and-friend abstraction of a recursive backtracking algorithm for this problem. What do you ask the bird, and what do give your friend?   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes18 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  13:40  18 Dynamic Programming Algorithms  267  Dynamic programming is another powerful tool for solving optimization problems. Just like recursive backtracking, it has as a key component a recurrence relation that says how to ﬁnd an optimal solution for one instance of the problem from optimal solutions for some number of smaller instances of the same problem. Instead of re- cursing on these subinstances, dynamic programming iteratively ﬁlls in a table with an optimal solution for each, so that each only needs to be solved once. Dynamic programming provides polynomial-time algorithms for many important and practi- cal problem.  Personally, I do not like the name “dynamic programming.” It is true that dy- namic programming algorithms have a program of subinstances to solve. But these subinstances are chosen in a ﬁxed prescheduled order, not dynamically. In contrast, in recursive backtracking algorithms, the subinstances are constructed dynamically. One way to design a dynamic programming algorithm is to start by guessing the set of subinstances that need to be solved. However, I feel that it is easier to start by designing the recurrence relation, and the easiest way to do this is to ﬁrst design a recursive backtracking algorithm for the problem. Once you have done this, you can use a technique referred to as memoization to mechanically convert this recursive backtracking algorithm into a dynamic programming algorithm.  18.1 Start by Developing a Recursive Backtracking  This section reviews the recommended steps for developing a recursive backtracking algorithm.  EXAMPLE 18.1.1  Shortest Weighted Path within a Directed Leveled Graph  To demonstrate the steps, we will develop an algorithm for a version of the shortest- weighted-path problem from Chapter 14. We generalize the problem by allowing neg- ative weights on the edges and simplify it by requiring the input graph to be leveled.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes18 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  13:40  268  Optimization Problems  v1  3  v6  s  2  3  4  v2  6  5 v4  3  7  2  v7  7  4   a   9  t  4  v3  2 v5  7  5  5  7  1  v8  ?  1v  Opt. path weight = 10  s  2  3  2v  Next node 1v 2v 5v 3v  Weight 10+3=13 11+2=13  6+7=13  8+4=12  3v  Opt. path    weight =  8  opt.  4 Opt. path weight = 11 7  v5  Opt. path weight =  6  t   b   Figure 18.1:  a  The directed layered weighted graph G for Example 18.1.1.  b  The recursive backtracking algorithm.  1  Speciﬁcation: The ﬁrst step is to be very clear about what problem needs to be solved. For an optimization problem, we need to be clear about what the set of in- stances is, for each instance what its set of solutions is, and for each solution what its cost is.  Precondition: We are given one of the following instances.  Instances: An instance consists of  cid:2 G, s, t  cid:3 , where G is a weighted directed layered graph, s is a speciﬁed source node and t is a speciﬁed destination node. See Figure 18.1.a. The graph G has n nodes. Each node has maximum in- and outdegree d. Each edge  cid:2 vi, v j cid:3  is labeled with a real-valued  possibly negative  weight w cid:2 vi ,v j cid:3 . The nodes are partitioned into levels so that each edge is directed from some node to a node in a lower level to prevent cycles. It is easiest to assume that the nodes are ordered so that an edge can go from node vi to node v j only if i < j .  Postcondition: The output is a solution with minimum cost and the cost of that solution.  Solutions: A solution for an instance is a path from source node s to desti- nation node t.  Cost of Solution: The cost of a solution is the sum of the weights of the edges within the path.  Brute Force Algorithm: The problem with simply trying all paths is that there may be an exponential number of them.  2  Design a Question and Its Answers for the Little Bird: Suppose the little bird knows one of the optimal solutions for our instance. You, the algorithm designer,   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes18 CUUS154-Edmonds 978 0 521 84931 9  Dynamic Programming Algorithms  March 29, 2008  13:40  must formulate a small question about this solution and the list of its possible an- swers. The question should be such that having a correct answer greatly reduces the search.  Question about the End of the Solution: When designing recursive backtracking algorithms, one generally asks about the ﬁrst part of the solution. We will later see that if our ultimate goal is a dynamic programming algorithm, then it is best to turn this around and ask for the last part of the solution  see Section 18.3.1 .  269  Leveled Graph: Not knowing yet why we ask about the last part of the solution, we will design this algorithm by asking instead about the ﬁrst part. Given a graph and nodes s and t, I ask, “Which edge should we take ﬁrst to form an optimal path to t?” She assures me that taking edge  cid:2 s, v1 cid:3  is good. The speciﬁcation of the problem gives that there are at most d edges out of any node. Hence, this is a bound on the number K of different answers.  To be consistent, we will always use the letter k to index the bird answers. In order for the ﬁnal algorithm to be efﬁcient, it is important that the number K of different answers be small.  3  Constructing Subinstances: Suppose that the bird gives us the kth of her answers. This gives us some of the solution, and we want to ask a recursive friend for the rest of the solution. The friend must be given a smaller instance of the same search prob- lem. You must formulate subinstance subI for the friend so that he returns to us the information that we desire.  Leveled Graph: I start by  at least temporarily  trusting the bird and take a step along the edge  cid:2 s, v1 cid:3 . Standing at v1, the natural question to ask my friend is “Which is the best path from v1 to t?” Expressed as  cid:2 G, v1, t  cid:3 , this is a subinstance of the same computational problem.  4  Constructing a Solution for My Instance: Suppose that the friend gives us an op- timal solution optSubSol for his instance subI. How do we produce an optimal so- lution optSol for your instance I from the bird’s answer k and the friend’s solution optSubSol?  Leveled Graph: My friend will faithfully give me the path optSubSol =  cid:2 v1, v6, t  cid:3 , this being a best path from v1 to t. The difﬁculty is that this is not a solution for my instance  cid:2 G, s, t  cid:3 , because it is not, in itself, a path from s to t. The path from s is formed by ﬁrst taking the step from s to vi and then follow- ing the best path from there to t, namely optSol =  cid:2 bird answer cid:3  + optSubSol =  cid:2 s, v1 cid:3  +  cid:2 v1, v6, t  cid:3  =  cid:2 s, v1, v6, t  cid:3 .  We proved in Section 8.7 that we can trust the friend to give provide an optimal so- lution to the subinstance subI, because he is really a smaller recursive version of our- selves.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes18 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  March 29, 2008  13:40  5  Costs of Solutions and Subsolutions: We must also return the cost optCost of our solution optSol. How do we determine it from the bird’s k and the cost optSubCost of the friend’s optSubSol?  270  Leveled Graph: The cost of the entire path from s to t is the cost of the edge  cid:2 s, v1 cid:3  plus the cost of the path from v1 to t. Luckily, our friend gives the latter: optCostk  = w cid:2 s,v1 cid:3  + optSubCost = 3 + 10 = 13.  6  Best of the Best: Try all the bird’s answers, and take best of the best.  Leveled Graph: If we trust both the bird and the friend, we conclude that this path from s to t is a best path. It turns out that because our little bird gave us the wrong ﬁrst edge, this might not be the best path from s to t. However, our work was not wasted, because we did succeed in ﬁnding the best path from among those that start with the edge  cid:2 s, v1 cid:3 . Not trusting the little bird, we repeat this process, ﬁnding a best path starting with each of  cid:2 s, v2 cid:3 ,  cid:2 s, v5 cid:3 , and  cid:2 s, v3 cid:3 . At least one of these four paths must be an overall best path. We give the best of these best as the overall best path.  7  Base Cases: The base case instances are instances of your problem that are small enough that they cannot be solved using steps 2–6, but they can be solved easily in a brute force way. What are these base cases, and what are their solutions?  Leveled Graph: The only base case is ﬁnding a best path from s to t when s and t are the very same node. In this case, the bird would be unable to give the ﬁrst edge in the best path, because it contains no edges. The optimal solution is the empty path, and its cost is zero.  8  Code: From steps 1–7, the code can always be put together using the same basic structure.  algorithm LeveledGraph  G, s, t   cid:2  pre-cond cid:3 : G is a weighted directed layered graph, and s and t are nodes.  cid:2  post-cond cid:3 : optSol is a path with minimum total weight from s to t, and optCost is its weight.  begin  % Base case: The only base case is for the best path from t to t. Its solution is the empty path with cost zero. if s = t  then  return  cid:2 ∅, 0 cid:3   else  % General case: % Try each possible bird answer. for each of the d edges  cid:2 s, vk cid:3    P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes18 CUUS154-Edmonds 978 0 521 84931 9  Dynamic Programming Algorithms  March 29, 2008  13:40  % The bird-and-friend algorithm: The bird tells us that the ﬁrst edge in an optimal path from s to t is  cid:2 s, vk cid:3 . We ask the friend for an optimal path from vk to t. He solves this recursively giving us optSubSol. To this, we add the bird’s edge, giving us optSolk. This optSolk is a best path from s to t from amongst those paths consistent with the bird’s answer.  cid:2 optSubSol, optSubCost cid:3  = LeveledGraph   cid:2 G, vk, t  cid:3   optSolk optCostk  =  cid:2 s, vk cid:3  + optSubSol = w cid:2 s,vk cid:3  + optSubCost  end for % Having the best, optSolk, for each bird’s answer k, we keep the best of these best. kmin = “a k that minimizes optCostk” optSol = optSolkmin optCost = optCostkmin return  cid:2 optSol, optCost cid:3   271  end if  end algorithm  9  Recurrence Relations: The recurrence relation at the core of this recursive back- tracking algorithm is the following. LeveledGraphSolution  G, s, t  = Minvk∈N s  cid:2 s, vk cid:3   LeveledGraphCost  G, s, t  = Minvk∈N s  w cid:2 s,vk cid:3  + LeveledGraphCost   cid:2 G, vk, t  cid:3    + LeveledGraphSolution   cid:2 G, vk, t  cid:3    where N s  is the set of nodes vk with edge  cid:2 s, vk cid:3 .  Running Time: The recursive backtracking algorithm faithfully enumerates all so- lutions for your instance and hence requires exponential time.  EXERCISE 18.1.1 Give a directed leveled graph on n nodes that has a small number of edges and as many paths from s to t as possible.  18.2 The Steps in Developing a Dynamic Programming Algorithm  Though the recursive backtracking algorithm for Example 18.1.1 may seem complex, how else would you iterate through all paths? We will now use memoization tech- niques to mechanically convert this algorithm into a dynamic programming algo- rithm that runs in polynomial time. The word “memoization” comes from “memo.” This technique speeds up a recursive algorithm by saving the result for each subin- stance it encounters so that it does not need to be recomputed. Dynamic program- ming takes the idea of memoization one step further. Instead of traversing the tree of   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes18 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  March 29, 2008  13:40  recursive stack frames, keeping track of which friends are waiting for answers from which friends, it ﬁrst determines the complete set of subinstances for which solu- tions are needed and then computes them in an order such that no friend must wait. As it goes, it ﬁlls out a table containing an optimal solution for each subinstance. The technique for ﬁnding an optimal solution for a given subinstance is identical to the technique used in the recursive backtracking algorithm. The only difference is that instead of recursing to solve a sub-subinstance, the algorithm looks up in the table an optimal solution found earlier. When the entire table has been completed, the last entry will contain an optimal solution for the original instance.  272  1  The Set of Subinstances: We obtain the set of subinstances that need to be solved by the dynamic programming algorithm by tracing the recursive backtracking algo- rithm through the tree of stack frames starting with the given instance I . The set con- sists of the initial instance I , its subinstances, their subinstances, and so on. We en- sure that this set contains all the required subinstances by making sure that is closed under this sub operation, or equivalently that no subinstance is lonely, because if it’s included then so are all its friends. See Section 18.3.3. Also ensure that all  or at least most  of these subinstances are needed.  Leveled Graph:  Include  cid:1 G, v7, t cid:2 : On  cid:2 G, s, t  cid:3 , the little bird, among other things, suggests taking the edge  cid:2 s, v1 cid:3  leading to the subinstance  cid:2 G, v1, t  cid:3 . On this subin- stance, the little bird, among other things, suggests taking the edge  cid:2 v1, v4 cid:3  leading to the subinstance  cid:2 G, v4, t  cid:3 . On this subinstance, the little bird, among other things, suggests taking the edge  cid:2 v4, v 7 cid:3  leading, as said, to the subinstance  cid:2 G, v 7, t  cid:3 . Hence, this subinstance must be considered by the dynamic programming algorithm. See Figure 18.2.a,  cid:2 G, v 7, t  cid:3 . Exclude  cid:1 G, v81, t cid:2 : Given the same instance I ,  cid:2 G, v81, t  cid:3  is not a subin- stance, because node v81 is not a node in the graph, so it never arises. Exclude  cid:1 G, v1, v8 cid:2 : Neither is  cid:2 G, v1, v 8 cid:3  a required subinstance, because each subinstance that arises is looking for a path that ends in the node t. Guess the Set: Starting with the instance  cid:2 G, s, t  cid:3 , the complete set of subin- stances called will be { cid:2 G, vi, t  cid:3   vi above t}. See Figure 18.2.b.  Redundancy: We can speed up the recursive backtracking algorithm only when it solves the same subinstance many times.  Leveled Graph: The recursive backtracking algorithm of Section 18.1 traverses each of the exponentially many paths from s tot. Within this exponential amount of work, there is a great deal of redundancy. Different friends are assigned the   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes18 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  13:40  Construct path  from what each  friend stored  273  Dynamic Programming Algorithms   a   Repeated work, exponential time  s  1v  2v  3v  5v  6v  7v  7v  8v  4v  t   b   Solve each subinstance  t  8v 7v ...  s   c   6  4v 2  1v 10  3  3  4  6  5  3  7  7  6v  9  s 12  2  11  7  2v  4  6  7  5  7v  5  4  4  0  t  8  3v  7  2  5v  1  5  8v   d   optCost table  12 s  10 1v  6811 3vv2 v4  6 5v  0547 t 7vv6  v8  Figure 18.2:  a  The recursive algorithm.  b  The dynamic programming algorithm: The little indicates the ﬁrst edge in an optimal path from vi to t, and the value arrow out of node vi within the circle on the node gives the cost of this path.  c  The optimal path from s to t.  d  The contents of the optCost table. It is ﬁlled in backwards.  exact same task. In fact, for each path from s to vi, some friend is asked to solve the subinstance  cid:2 G, vi, t  cid:3 . See Figure 18.2.a.  One Friend per Subinstance: To save time, the dynamic programming algorithm solves each of these subinstances only once. We allocate one friend to each of these subinstances, whose job it is to ﬁnd an optimal solution for it and to provide this solution to any other friend who needs it.  2  Count the Subinstances: The running time of the dynamic programming algo- rithm is proportional to the number of subinstances. At this point in the design of the algorithm, you should count how many subinstances an instance I has as a func- tion of the size n = I of the instance. If there are too many of them, then start at the very beginning, designing a new recursive backtracking algorithm with a different question for the little bird.  Leveled Graph: The number of subinstances in the set { cid:2 G, vi, t  cid:3   vi above t} is n, the number of nodes in the graph G.  3  Construct a Table Indexed by Subinstances: The algorithm designer constructs a table. It must have one entry for each subinstance. Generally, the table will have one dimension for each parameter used to specify a particular subinstance. To be consistent, we will always use the letter i and if necessary j to index the subinstances. Each entry in the table is used to store an optimal solution for the subinstance along with its cost. Often we split this table into two tables: one for the solution and one for the cost.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes18 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  March 29, 2008  13:40  Leveled Graph: The single parameter used to specify a particular subinstance is i. Hence, suitable tables would be optSol[0..n] and optCost[0..n], where optSol[i] will store the best path from node vi to node t and optCost[i] will store its cost. See Figure 18.2.d.  274  4  Solution from Subsolutions: The dynamic programming algorithm for ﬁnding an optimal solution to a given instance from an optimal solution to a subinstance is identical to that within the recursive backtracking algorithm, except that instead of recursing to solve a subinstance, the algorithm ﬁnds its optimal solution in the table.  Leveled Graph: The task of Friendi in the dynamic programming algorithm is not to solve  cid:2 G, s, t  cid:3  but  cid:2 G, vi, t  cid:3 , looking for a best path from vi to t. He does this as follows. He asks the little bird for the ﬁrst edge in his path and tries each of her possible answers. When the bird suggests the edge  cid:2 vi, vk cid:3 , he asks a friend for a best path from vk to t. This task is the subinstance  cid:2 G, vk, t  cid:3 , which has been allo- cated to Friendk and whose solution has been stored in optSol[k]. The recursive backtracking code   cid:2 optSubSol, optSubCost cid:3  = LeveledGraph   cid:2 G, vk, t  cid:3    optSolk optCostk  =  cid:2 s, vk cid:3  + optSubSol = w cid:2 s,vk cid:3  + optSubCost  is changed to simply  optSolk optCostk  =  cid:2 vi, vk cid:3  + optSol[k] = w cid:2 vi ,vk cid:3  + optCost[k]  Friendi tacks the bird’s edge cid:2 vi, vk cid:3  onto this path from vk tot, giving a path that is the best path from vi to t amongst those consistent with the bird’s answer  cid:2 vi, vk cid:3 . After trying each bird’s answer, Friendi saves the best of these best paths in the table.  5  Base Cases: The base case instances are exactly the same as with the recursive backtracking algorithm. The dynamic programming algorithm starts its computation by storing in the table an optimal solution for each of these and their costs.  Leveled Graph: The only base case is ﬁnding a best path from s to t when s and t are the very same node. This only occurs with the subinstance  cid:2 G, vn, t  cid:3 , where vn is another name for t. The recursive backtracking code  if s = t  then  return  cid:2 ∅, 0 cid:3    P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes18 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  13:40  275  Dynamic Programming Algorithms  is changed to  % Base Case: optSol[n] = ∅ optCost[n] = 0  6  The Order in Which to Fill the Table: When a friend in the recursive backtrack- ing algorithm needs help from a friend, the algorithm recurses, and the stack frame for the ﬁrst friend waits until the stack frame for the second friend returns. This forms a tree of recursive stack frames, keeping track of which friends are waiting for answers from which friends. In contrast, in a dynamic programming algorithm, the friends solve their subinstances in an order such that nobody has to wait. Every recursive algorithm must guarantee that it recurses only on smaller instances. Hence, if the dynamic programming algorithm ﬁlls in the table from smaller to larger instances, then when an instance is being solved, the solution for each of its subinstances is al- ready available. Alternatively, the algorithm designer can simply choose any order to ﬁll the table that respects the dependences between the instances and their subin- stances. The table must be indexed by subinstances. When allocating the table, be clear what subinstance each entry of the table represents.  Leveled Graph: Friendi, with instance  cid:2 G, vi, t  cid:3 , depends on Friendk when there is an edge  cid:2 vi, vk cid:3 . From the precondition of the problem, we know that each edge must go from a higher level to a lower one, and hence we know that k > i. Fill- ing the table in the order t = vn, vn−1, vn−2, . . . , v2, v1, v0 = s ensures that when Friendi does his work, Friendk has already stored his answer in the table. The subinstance  cid:2 G, vn, t  cid:3  has been solved already. The following loop is put around the general case code of the recursive backtracking algorithm:  % General Cases: Loop over subinstances in the table. for i = n − 1 to 0 % Solve instance  cid:2 G, vi, t  cid:3  and ﬁll in table entry  cid:2 i cid:3 .  Viewing the dynamic programming algorithm as an iterative algorithm, the loop in- variant when working on a particular subinstance is that all smaller subinstances that will be needed have been solved. Each iteration maintains the loop invariant while making progress by solving this next subinstance.  7  The Final Solution: The original instance will be the last subinstance to be solved. When complete the dynamic program simply returns this answer.  Leveled Graph: The original instance  cid:2 G, s, t  cid:3  is the same as the subinstance  cid:2 G, v0, t  cid:3 , where v0 is another name for s. The dynamic program ends with the code  return  cid:2 optSol[0], optCost[0] cid:3    P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes18 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  March 29, 2008  13:40  8  Code: From steps 1–7, the code can always be put together using the same basic structure:  algorithm LeveledGraph  G, s, t   cid:2  pre-cond cid:3 : G is a weighted directed layered graph, and s and t are nodes.  cid:2  post-cond cid:3 : optSol is a path with minimum total weight from s to t, and optCost is its weight.  276  begin  % Table: optSol[i] stores an optimal path from vi to t, and optCost[i] its cost. table[0..n] optSol, optCost  % Base case: The only base case is for the best path from t to t. Its solution is the empty path with cost zero. optSol[n] = ∅ optCost[n] = 0  % General cases: Loop over subinstances in the table. for i = n − 1 to 0 % Solve instance  cid:2 G, vi, t  cid:3  and ﬁll in table entry  cid:2 i cid:3 . % Try each possible bird answer. for each of the d edges  cid:2 vi, vk cid:3   % The bird-and-friend Algorithm: The bird tells us that the ﬁrst edge in an optimal path from vi to t is  cid:2 vi, vk cid:3 . We ask the friend for an optimal path from vk to t. He gives us optSol[k], which he had stored in the table. To this we add the bird’s edge. This gives us optSolk which is a best path from vi to t from among those paths consistent with the bird’s answer. optSolk optCostk  =  cid:2 vi, vk cid:3  + optSol[k] = w cid:2 vi ,vk cid:3  + optCost[k]  end for % Having the best, optSolk, for each bird’s answer k, we keep the best of these best. kmin = a k that minimizes optCostk optSol[i] = optSolkmin optCost[i] = optCostkmin end for return  cid:2 optSol[0], optCost[0] cid:3   end algorithm  Consistent Structure: To be consistent, we will always use this same structure for all dynamic programming code. Even when there are small ways that the code could be optimized, we stick to this same structure. Even when different variable names   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes18 CUUS154-Edmonds 978 0 521 84931 9  Dynamic Programming Algorithms  March 29, 2008  13:40  would be more meaningful, we stick to i and if necessary j to index the subinstances, and k to index the bird answers. I believe that this consistency will make it easier for you  and for the marker  to understand the many dynamic programming algorithms.  9  Running Time: We can see that the code loops over each subinstance and, for each, loops over each bird answer. From this, the running time seems to be the num- ber of subinstances in the table times the number K of answers to the bird’s question. We will see in Section 18.3.4 that actually the running time of this version of the al- gorithm is a factor of n bigger than this. The same section will tell how to remove this extra factor of n.  277  Leveled Graph: The running time of this algorithm is now polynomial. There are only n friends, one for each node in the graph. For the instance  cid:2 G, vi, t  cid:3 , there is a bird answer for each edge out of its source node vi. There are at most d of these. The running time is then only O n · d  times this extra factor of n.  18.3 Subtle Points  Before listing a few of the more subtle points in developing recursive backtracking and a dynamic programming algorithms, I give another example problem.  EXAMPLE 18.3.1  Printing Neatly  Consider the problem of printing a paragraph neatly on a printer. The input text is a sequence of n words with lengths l1, l2, . . . , ln, measured in characters. Each printer line can hold a maximum of M characters. Our criterion for neatness is for there to be as few spaces on the ends of the lines as possible. Preconditions: An instance  cid:2 M; l1, . . . , ln cid:3  consists of the line length and the word lengths. Generally, M will be thought of as a constant, so we will leave it out when it is clear from the context.  Postconditions: The goal is to split the text into lines in a way that minimizes the cost.  Solutions: A solution for an instances is a list giving the number of words for each line,  cid:2 k1, . . . , kr cid:3 . Cost of Solution: Given the number of words in each line, the cost of this solution is the sum of the cubes of the numbers of blanks on the end of each line  including for now the last line .  Example: Suppose that one way of breaking the text into lines gives 10 blanks on the end of one of the lines, while another way gives 5 blanks on the end of one line and 5 on the end of another. Our sense of esthetics dictates that the second way is “neater.” Our cost heavily penalizes having a large number of blanks on a single line by cubing the number. The cost of the ﬁrst solution is 103 = 1,000 whereas the cost of the second is only 53 + 53 = 250.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes18 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  March 29, 2008  13:40  278  EXAMPLE 18.3.1  Printing Neatly  cont.   Example: Consider printing neatly the silly text “This week has seven dates in it ok” in column with width M = 11. This is represented as the printing neatly instance  cid:2 M; l1, . . . , ln cid:3  =  cid:2 11; 4, 4, 3, 5, 5, 2, 2, 2 cid:3 . Three of the possible ways to print this text are as follows:  cid:1 k1, k2, . . . , kr cid:2 = cid:1 2,2,2,2 cid:2   cid:1 k1, k2, . . . , kr cid:2 = cid:1 1,2,2,3 cid:2   cid:1 k1, k2, . . . , kr cid:2 = cid:1 2,2,1,3 cid:2  This.week.. has.seven.. dates.in... it.ok......  This....... week.has... seven.dates in.it.ok...  This.week.. has.seven.. dates...... in.it.ok...  Cost  Cost  Cost  23 23 63 33 = 259  23 23 33 63 = 259  73 33 03 33 = 397  Of these three, the ﬁrst and the last are the cheapest and are likely the cheapest of all the possible solutions.  18.3.1 The Question for the Little Bird  The designer of a recursive backtracking algorithm, a dynamic programming algo- rithm, or a greedy algorithm must decide which question to ask the little bird. That is, the algorithm designer must decide which sequence of decisions will specify the so- lution constructed by the algorithm: which things will the algorithm try before back- tracking to try something else? This is one of the main creative steps in designing the algorithm.  Local vs. Global Considerations: One of the reasons that optimization problems are difﬁcult is that we are able to make what we call local observations and decisions but it is hard to see the global consequences of these decisions.  Leveled Graph: Which edge out of s is cheapest is a local question. Which path is the overall cheapest is a global question. We were tempted to follow the cheapest edge out of the source s. However, sometimes one can arrive at a better overall path by starting with a ﬁrst edge that is not the cheapest.  Printing Neatly: If we follow a greedy algorithm, we will put as many words on the ﬁrst line as possible. However, a local sacriﬁce of putting fewer words on this line may lead globally to a better overall solution.  Ask about a Local Property: The question that we ask the bird is about some local property of the solution:  First Object: If the solution is a sequence of objects, a good question would be “What is the ﬁrst object in the sequence?”   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes18 CUUS154-Edmonds 978 0 521 84931 9  Dynamic Programming Algorithms  March 29, 2008  13:40  Leveled Graph: If the solution is a path though a graph, we might ask, “What is the ﬁrst edge in the path?”  Printing Neatly: If the solution is a sequence of how many words to put on each line, we ask, “How many words k do we put on the ﬁrst line?”  Yes or No: If the instance is a sequence of objects and a solution is a subset of these object, a good question would be “Is the ﬁrst object of the instance in- cluded in the optimal solution?”  279  Event Scheduling: If the solution states which events to schedule, we will ask, “Do we schedule the ﬁrst event?”  Which Root: If a solution is a binary tree of objects, a good question would be “What object is at the root of the tree?”  The Best Binary Search Tree: Note that the ﬁrst question is not whether to take the left or right branch of the given binary search tree, but what the root should be when constructing the binary search tree.  In contrast, asking the bird for the number of edges in the best path in the leveled graph is a global, not a local, question.  The Number K of Different Bird Answers: You can only ask the bird a little question.  It is only a little bird.  In a little question, the number K of different an- swers A 1, A 2, . . . , A K that the bird might give must be small. The smaller K is, the more efﬁcient the ﬁnal algorithm will be.  Leveled Graph: When asking for an edge out of s, the number K of answers is the degree of the node. This gives a bound on K .  Printing Neatly: K is the maximum number of the ﬁrst words of the text that would ﬁt on the ﬁrst line. Event Scheduling: “Do we schedule the ﬁrst event?” has K = 2 answers, yes and no.  Brute Force: The obvious question to ask the little bird is for her to tell you an entire optimal solution. However, the number of solutions for your instance I is likely exponential; each solution is a possible answer. Hence, K would be expo- nential. After getting rid of the bird, the resulting algorithm would be the usual brute force algorithm.  Repeated Questions: Although you want to avoid thinking about it, each of your recursive friends will have to ask his little bird a similar question. Hence, you should   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes18 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  March 29, 2008  13:40  280   a   s  v1  v2  4v Opt. path weight = 8  Next node Weight 6+7=13 8+9=17  6+7=13 7+5=12  v1 v2 v5 v3  v3  opt.  5v  8v  ?  7  4  5  t   b   Solve  each subinstance  s  v1  v2  ...  t  s  0   c   v1 3  3  v3  4  7  1  2  6  5v  6  4v  2  3  4  8  5  3  7  9  4  2  2  7  v2  7  9  4  5 7v  5  12 t  0 s  3 v1  2 4 8 6 6 9 7 12 t 2v v3  6v v7  v5  4v  8v   d   optSol table  v 6  Opt. path weight = 6  7v  9  Opt. path weight  = 9  Opt. path weight  = 7  v 6  6  7  8v  Construct path  from what each  friend stored  Figure 18.3:  a  The recursive algorithm.  b  The dynamic programming algorithm: The little arrow out of node vi indicates the last edge in an optimal path from s to vi, and the value within the circle at a node gives the cost of this path.  c  The optimal path from s to t.  d  The contents of the optCost table. In contrast with that in Figure 18.2, it is ﬁlled in forwards.  choose a question that provides a reasonable follow-up question of a similar form. For example:   cid:1  “What is the second object in the sequence?”  cid:1  “Is the second object of the instance included in the optimal solution?”  cid:1  “What is the root in the left  the right  subtree?”  In contrast, asking the bird for the number of edges in the best path in the leveled graph does not have a good follow-up question.  Reversing the Order: This change is purely for aesthetic reasons. The dynamic program loops backwards through the nodes of the graph, t = vn, vn−1, vn−2, . . ., v2, v1, v0 = s. The standard way to do it is to work forward. The recursive backtracking al- gorithm worked forward from s. The dynamic programming technique reverses the recursive backtracking algorithm by completing the subinstances from smallest to largest. In order to have the ﬁnal algorithm move forward, the recursive backtrack- ing algorithm needs to go backwards. To do this, the little bird should ask something about the end of the solution and not about the beginning. Compare Figure 18.2 and Figure 18.3, and see the ﬁnal code in Section 18.3.6.  Last Object: “What is the last object in the sequence?” What is the last edge in the optimal path, and how many words do we put on the last line?  Yes or No: “Is the last object of the instance included in the optimal solution?” Do we schedule the last event?  Which Root: We still ask about the root. It is not useful to ask about the leaves.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes18 CUUS154-Edmonds 978 0 521 84931 9  Dynamic Programming Algorithms  March 29, 2008  13:40  EXERCISE 18.3.1 Start over and redevelop the leveled graph dynamic programming algorithm with this new question for the little bird, so that the work is completed start- ing at the top of the graph.  18.3.2 Subinstances and Subsolutions  281  Getting a trustworthy answer from the little bird narrows our search problem down to the task of ﬁnding the best solution from among those solutions consistent with this answer. It would be great if we could simply ask a friend to ﬁnd us such a so- lution; however, we are only allowed to ask our friend to solve subinstances of the original computational problem. Our task within this subsection is to formulate a subinstance to our computational problem such that the search for its optimal solu- tions some how parallels our narrowed search task.  The Recursive Structure of the Problem: In order for us to be able to design a re- cursive backtracking algorithm for an optimization problem, the problem needs to have a recursive structure. For a solution of the instance to be optimal, some part of the solution must itself be optimal. The computational problem has a recursive structure if the task of ﬁnding an optimal way to construct this part of the solution is a subinstance of the same computational problem.  Leveled Graph: For a path from s to t to be optimal, the subpath from some vi to some v j along the path must itself be an optimal path between these nodes. The computational problem has a recursive structure because the task of ﬁnding an optimal way to construct this part of the path is a subinstance of the same computational problem.  Printing Neatly: In order for all the words to be printed neatly, the words that are on the last ten lines need to be printed neatly on these ten lines.  Question from Answer: Sometimes it is a challenge to know what subinstance to ask our friend. It turns out that it is easier to know what answer  subsolution  we want from him. Knowing the answer we want will be a huge hint as to what the question should be.  Each Solution as a Sequence of Answers: One task that you as the algorithm de- signer must do is to organize the information needed to specify a solution into a sequence of ﬁelds, sol =  cid:2 ﬁeld1, ﬁeld2, . . . , ﬁeldm   cid:3 .  Best Animal: In the zoo problem, each solution consists of an animal, which we will identify with the sequence of answers to the little bird’s questions, sol =  cid:2 vertebrate,mammal,cat,cheetah cid:3 .   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes18 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  March 29, 2008  13:40  Leveled Graph: In the leveled graph problem, a solution consists of a path,  cid:2 s, v1, v6, t cid:3 , which we will identify with the sequence of edges sol =  cid:2  cid:2 s, v1 cid:3 ,  cid:2 v1, v6 cid:3 ,  cid:2 v6, t  cid:3  cid:3 .  282  Printing Neatly: A solution for the printing neatly problem gives the number of words for each line,  cid:2 k1, . . . , kr cid:3 .  Bird’s Question and Remaining Task: The algorithm asks the little bird for the last ﬁeld ﬁeldm of one of the instance’s optimal solutions and asks the friend for the remaining ﬁelds  cid:2 ﬁeld1, . . . , ﬁeldm−1  cid:3 . We will let k denote the answer pro- vided by the bird, and optSubSol the one provided by the friend. Given both, the algorithm constructs the ﬁnal solution by simply concatenating these two parts together, namely, optSol =  cid:2 optSubSol, k cid:3  =  cid:2  cid:2 ﬁeld1, . . . , ﬁeldm−1   cid:3 , ﬁeldm   cid:3 .  Leveled Graph: Asking for the last ﬁeld of an optimal solution optSol =  cid:2  cid:2 s, v1 cid:3 ,  cid:2 v1, v6 cid:3 ,  cid:2 v6, t  cid:3  cid:3  amounts to asking for the last edge that the path should take. The bird answers  cid:2 v6, t  cid:3 . The friend provides optSubSol =  cid:2  cid:2 s, v1 cid:3 ,  cid:2 v1, v6 cid:3  cid:3 . Concatenating these forms our solution. Printing Neatly: Asking for the last ﬁeld of an optimal solution optSol =  cid:2 k1, . . . , kr cid:3  amounts to asking how many words should go on the last line.  Formulating the Subinstance: We need to ﬁnd an instance of the computational problem whose optimal solution is optSubSol =  cid:2 ﬁeld1, . . . , ﬁeldm−1  cid:3 . The in- stance is that whose set of valid solutions is setSubSol = {subSol   cid:2 subSol, k cid:3  ∈ setSol}.  Leveled Graph: The instance whose solution is optSubSol =  cid:2  cid:2 s, v1 cid:3 ,  cid:2 v1, v6 cid:3  cid:3  is  cid:2 G, s, v6 cid:3 , asking for the optimal path from s to v6.  Printing Neatly: If the bird told you that an optimal number of words to put on the last line is k, then an optimal printing of the words is an optimal print- ing of the ﬁrst n − k words followed by the remaining k words on a line by themselves. Your friend can ﬁnd an optimal way of printing these ﬁrst words by solving the subinstance  cid:2 M; l1, . . . , ln−k cid:3 . All you need to do then is to add the last k words, i.e., optSol =  cid:2 optSubSol, k cid:3 .  Costs of Solutions: In addition to ﬁnding an optimal solution for the instance I , the algorithm must also produce the cost of this solution. To be helpful, the friend provides the cost of his solution, optSubSol. Due to the recursive struc- ture of the problem, the costs of these solutions optSol =  cid:2  cid:2 ﬁeld1, . . . , ﬁeldm−1  cid:3 ,  cid:3  usually differ in some uniform way. For  cid:1  ﬁeldm example, often the cost is the sum of the costs of the ﬁelds, that is, cost optSol  = In this case we have that cost optSol  = cost optSubSol  +   cid:3  and optSubSol =  cid:2 ﬁeld1, . . . , ﬁeldm−1  m i=1 cost ﬁeldi .  cost ﬁeldm .   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes18 CUUS154-Edmonds 978 0 521 84931 9  Dynamic Programming Algorithms  March 29, 2008  13:40  Leveled Graph: The cost of a path from s to t is the cost of the last edge plus the cost of the rest of the path. Printing Neatly: The total cost of an optimal solution for the given instance  cid:2 M; l1, . . . , ln cid:3  is the cost of the optimal solution for the subinstance  cid:2 M; l1, . . . , ln−k cid:3  k words, i.e., optCost = optSubCost +  M − k + 1 − cid:1  plus the cube of the number of blanks on the end of the line that contains the last  n j=n−k+1 l j  3.  283  Formal Proof of Correctness:  Recursive Structure of Costs: In order for this recursive backtracking method to solve an optimization problem, the costs that the problem allocates to the solu- tions must have the following recursive structure. Consider two solutions sol =  cid:2 subSol, k cid:3  and sol , k cid:3  both consistent with the same bird’s answer k. If the given cost function dictates that the solution sol is better than the solu- tion sol , then the subsolution subSol of sol will also be better than the subsolu- tion subSol . This ensures that any optimal subsolution of the subinstance leads to an optimal solution of the original instance.   cid:1  =  cid:2 subSol  of sol   cid:1    cid:1    cid:1    cid:1   Theorem 18.3.1: The solution optSol returned is a best solution for I from amongst those that are consistent with the information k provided by the bird.  Proof: By way of contradiction, assume not. Then there must be another solu- tion betterSol consistent with k whose cost is strictly better then that for optSol. From the way we constructed our friend’s subinstance subI, this better solution must have the form betterSol =  cid:2 betterSubSol, k cid:3  where betterSubSol is a solution for subI. We proved in Section 8.7, using strong induction, that we can trust the friend to provide an optimal solution to the subinstance subI. Because the cost of betterSol is better than that of optSol, it follows that the cost of betterSubSol is better than that of optSubSol. This contradicts the statement that optSubSol is an optimal solution for the subinstance subI.  Size of an Instance: In order to avoid recursing indeﬁnitely, the subinstance that you give your friend must be smaller than your own instance according to some measure of size. By the way that we formulated the subinstance, we know that its valid solutions subSol =  cid:2 ﬁeld2, . . . , ﬁeldm  cid:3  are shorter than the valid solutions sol =  cid:2 ﬁeld1, ﬁeld2, . . . , ﬁeldm  cid:3  of the instance. Hence, a reasonable measure of the size of an instance is the length of its longest valid solution. This measure only fails to work when an instance has valid solutions that are inﬁnitely long.  Leveled Graph: The size of the instance  cid:2 G, s, t  cid:3  is the length of the longest path, or simply the number of levels, between s and t. Given this, the size of the subin- stance  cid:2 G, s, vk cid:3 , which is the number of levels between s and vk, is smaller.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes18 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  March 29, 2008  13:40  Each Solution as a Tree of Answers: A few recursive backtracking, dynamic pro- gramming, and greedy algorithms have the following more complex structure than in the previous sections. In these, the ﬁelds specifying a solution are organized into a tree instead of a sequence. For example, if the problem is to ﬁnd the best binary search tree, then it is quite reasonable that the ﬁelds are the nodes of the tree and these ﬁelds should be organized as the tree itself. The algorithm asks the little bird to tell it the ﬁeld at the root of one of the instance’s optimal solutions. One friend is asked to ﬁll in the left subtree, and another the right. See Sections 19.5 and 19.6.  284  18.3.3 The Set of Subinstances  Can Be Difﬁcult: When using the memoization technique to mechanically convert a recursive algorithm into an iterative algorithm, the most difﬁcult step is determin- ing for each input instance the complete set of subinstances that will get called by the recursive algorithm, starting with this instance.  Leveled Graph: We speculated that a subinstance for the leveled graph problem consists of { cid:2 G, vi, v j cid:3   pair vi, v j}, namely, the task of ﬁnding the best path be- tween each pair of nodes. Later, by tracing the recursive algorithm, we saw that these subinstances are not all needed, because the subinstances called always search for a path ending in the same ﬁxed node t. After changing the bird ques- tion to ask about the last edge, all subinstances called always search for a path beginning with the ﬁxed node s.  Guess and Check: The technique to ﬁnd the set of subinstances is to ﬁrst try to trace out the recursive algorithm on a small example and guess what the set of subin- stances will be. Then Lemma 18.3.2 can be used to check if this set is big enough and not too big.  A Set Being Closed under an Operation: We say that the set of even integers is closed under addition and multiplication because the sum and the product of any two even numbers is even. In general, we say a set is closed under an operation if applying the operation to any elements in the set results in an element that is also in the set.  The Construction Game: Consider the following game: I give you the integer 2. You are allowed to construct new objects by taking objects you already have and either adding them or multiplying them. What is the complete set of numbers that you are able to construct?  Guess a Set: You might guess that you are able to construct the set of positive even integers. How do you know that this set is big enough and not too big?   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes18 CUUS154-Edmonds 978 0 521 84931 9  Dynamic Programming Algorithms  March 29, 2008  13:40  285  Big Enough: Because the set of positive even integers is closed under addition and multiplication, I claim you will never construct an object that is not a positive even number.  Proof: We prove by induction on t ≥ 0 that after t steps you only have posi- tive even numbers. This is true for t = 0, because initially you only have the positive even integer 2. If it is true for t, the object constructed in step t + 1 is either the sum or the product of previously constructed objects, which are all positive even integers. Because the set of positive even integers is closed under these operations, the resulting object must also be positive even. This completes the inductive step.  Not Too Big: Every positive even integer can be generated by this game.  Proof: Consider some positive even number i = 2j . Initially, we have only 2. We construct i by adding 2 + 2 + 2 + ··· + 2 a total of j times.  Conclusion: The set of positive even integers accurately characterizes which numbers can be generated by this game, no less and no more.  Lemma 18.3.2: The set S will be the complete set of subinstances called starting from our initial instance Istart iff 1. Istart ∈ S. 2. S is closed under the sub operator.  S is big enough.  The sub operator is deﬁned as follows: Given a particular instance of the problem, applying the sub operator pro- duces all the subinstances constructed from it by a single stack frame of the recursive algorithm. 3. Every subinstance I ∈ S can be generated from Istart using the sub operator.  S is not too big.  This ensures that there are not any instances in S that are not needed. The dynamic programming algorithm will work ﬁne if your set of subinstances con- tains subinstances that are not called. However, you do not want the set too much larger than necessary, because the running time depends on its size.  Examples:  The Wedding Invitation List: One of the nightmares when getting married is de- ciding upon the invitation list. The goal is to make everyone there happy while keeping the number of people small. The three rules in the lemma also apply here.  Istart ∈ S: Clearly the bride and groom need to be invited. Closure Ensures That Everyone Is Happy: If you invite aunt Hilda, then in order to keep her happy, you need to invite her obnoxious son. Similarly,   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes18 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  286  March 29, 2008  13:40  you need to keep all of your subinstances happy, by being sure that for every subinstance included, its immediate friends are also included. In the wed- ding list problem, you will quickly invite the entire world. The six-degrees- of-separation principle states that the set consisting of your friends’ friends’ friends’ friends’ friends’ friends includes everyone. Similarly, for most opti- mization problems the number of subinstances needed tends to be expo- nential in the size of the instance. A problem has a good dynamic program- ming algorithm when the number of subinstances is small.  Everyone Needed: We see that everyone on the list must be invited. Even the obnoxious son must come. Because the bride must come, her mother must come. Because her mother must come, aunt Hilda must come, and hence the son.  Leveled Graph:  Guess a Set: The guessed set is { cid:2 G, s, vi cid:3   vi below s}. Closed: Consider an arbitrary subinstance  cid:2 G, s, vi cid:3  from this set. The sub operator considers some edge  cid:2 vk, vi cid:3  and forms the subinstance  cid:2 G, s, vk cid:3 . This is contained in the stated set of subinstances. Generating: Consider an arbitrary subinstance  cid:2 G, s, vi cid:3 . It will be called by the recursive algorithm if and only if there is a path  cid:2 vi, vk1, vk2, . . . , vkr , t  cid:3  from vi to the original destination t. The initial stack frame on the in- stance  cid:2 G, s, t  cid:3 , among other things, recurses on  cid:2 G, s, vkr  cid:3 , which recurses on  cid:2 G, s, vkr−1  cid:3 , which recurses on  cid:2 G, s, vi cid:3 . If the node vi cannot be reached from node s, then the subinstance  cid:2 G, s, vi cid:3  will never be called by the recursive algorithm. Despite this, we will include it in our dynamic program, because this is not known about vi until after the algorithm has run.   cid:3 , . . ., which recurses on  cid:2 G, s, vk1  Printing Neatly: By tracing the recursive algorithm, we see that the set of subin- stance used consists only of preﬁxes of the words, namely, { cid:2 M; l1, . . . , li cid:3   i ∈ [0, n]}.  Closed: We know that this set contains all subinstances generated by the re- cursive algorithm, because it contains the initial instance and is closed under the sub operator. Consider an arbitrary subinstance  cid:2 M; l1, . . . , li cid:3  from this set. Applying the sub operator constructs the subinstances  cid:2 M; l1, . . . , li−w cid:3  for 1 ≤ w ≤ i, which are contained in the stated set of subinstances. Generating: Consider the arbitrary subinstance  cid:2 M; l1, . . . , li cid:3 . We demon- strate that it is called by the recursive algorithm as follows: The initial stack frame on the instance  cid:2 M; l1, . . . , ln cid:3 , among other things, sets w to 1 and re- curses on  cid:2 M; l1, . . . , ln−1 cid:3 . This stack frame also sets w to 1 and recurses on   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes18 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  13:40  Dynamic Programming Algorithms   cid:2 M; l1, . . . , ln−2 cid:3 . This continues n − i times, until the desired  cid:2 M; l1, . . . , li cid:3  is called.  The Number of Subinstances: A dynamic programming algorithm is fast only if the given instance does not have many subinstances.  287  Subinstance is a Subsequence: A common reason for the number of subin- stances of a given instance being polynomial is that the instance consists of a sequence of things rather than a set of things. In such a case, each subinstance can be a contiguous  continuous  subsequence of the things rather than an arbitrary subset of them. There are only O n2  contiguous subsequences of a sequence of length n, because one can be speciﬁed by specifying the two end points. Even better, there are even fewer subinstances if they are deﬁned to be a preﬁx of the sequence. There are only n preﬁxes, because one can be speciﬁed by specifying the one end point. On the other hand, there are 2n subsets of a set, because for each object you must decide whether or not to include it.  Leveled Graph: As stated in the deﬁnition of the problem, it is easiest to as- sume that the nodes are ordered so that an edge can go from node vi to node v j only if i < j . We initially guessed that three were O n2  subinstances con- sisting of subsequences between two nodes vi and v j . We then decreased this to only the O n  postﬁxes from the ﬁxed source s to some node vi. Note that subinstances cannot be subsequences of the instance if the input graph is not required to be leveled.  Printing Neatly: Because the subinstance used consists only of preﬁxes of the words, namely { cid:2 M; l1, . . . , li cid:3  i ∈ [0, n]}, the number of them is O n . The single parameter used to specify a particular subinstance is i. Hence, suit- able tables would be birdAdvice[0..n] and cost[0..n]. The size of subinstance is simply the number of words, i. Hence, the table is ﬁlled in by looping with i from 0 to n.  Reusing the Table: Sometimes you can solve many related instances of the same problem using the same table.  Leveled Graph: The algorithm gives you for free the shortest path from s to each of the nodes.  Printing Neatly: When actually printing text neatly, it does not matter how many spaces are on the end of the very last line. Hence, the cube of this number should not be included in the cost. We could use the original algorithm to ﬁnd for k = 1, 2, 3, . . . how to print all but the last k words and then put these last k on the last line. However, this would take a total of O n · n2  time. Instead, time can be saved by ﬁlling in the table only once. One can get the costs for these different instances off this single table. After determining which is best,   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes18 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  March 29, 2008  13:40  18.3.4 Decreasing Time and Space  288  call PrintingNeatlyWithAdvice once to construct the solution for this instance. The total time is reduced to only O n2 .  Recap of a Dynamic Programming Algorithm: A dynamic programming algo- rithm has two nested loops  or sets of loops . The ﬁrst iterates through all the subin- stances represented in the table, ﬁnding an optimal solution for each. When ﬁnding an optimal solution for the current subinstance, the second loop iterates through the K possible answers to the little bird’s question, trying each of them. Within this in- ner loop, the algorithm must ﬁnd a best solution for the current subinstance from amongst those consistent with the current bird’s answer. This step seems to require only a constant amount of work. It involves looking up in the table an optimal solu- tion for a sub-subinstance of the current subinstance and using this to construct a solution for the current subinstance.  Running Time? The running time is clearly the number of subinstances in the table times the number K of answers to the bird’s question times what appears  falsely  to be constant time.  Friend-to-Friend Information Transfer: In both a recursive backtracking and a dynamic programming algorithm, information is transferred from subfriend to friend. In recursive backtracking, this information is transferred by returning it from a subroutine call. In dynamic programming, this information is transferred by having the subfriend store the information in the table entry associated with his subinstance and having the friend look this information up from the table. The information trans- ferred is an optimal solution and its cost. The cost, being only an integer, is not a big deal. However, an optimal solution generally requires  cid:1  n  characters to write down. Hence, transferring this information requires this much time.  = optSol[k] +  cid:2 vk, vi cid:3 ,” Friendi asks Leveled Graph: In the line of code “optSolk Friendk for his best path. This path may contain n nodes. Hence, it could take Friendi O n  time steps simply to transfer the answer from Friendk.  Time and Space Bottleneck: Being within these two nested loops, this informa- tion transfer is the bottleneck on the running time of the algorithm. In a dynamic programming algorithm, this information for each subinstance is stored in the table for the duration of the algorithm. Hence, this is a bottleneck on the memory space requirements of the algorithm.  Leveled Graph: The total time is O n · d · n . The total space is  cid:1  n · n , being O n  for each of the n table entries.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes18 CUUS154-Edmonds 978 0 521 84931 9  Dynamic Programming Algorithms  March 29, 2008  13:40  A Faster Dynamic Programming Algorithm: We will now modify the dynamic programming algorithm to decrease its time and space requirements. The key idea is to reduce the amount of information transferred.  Cost from Subcost: The subfriends do not need to provide an optimal subsolution in order to ﬁnd the cost of an optimal solution to the current subinstance. The sub- friends need only provide the cost of this optimal subsolution. Transferring only the costs speeds up the algorithm.  289  Leveled Graph: In order for Friendi to ﬁnd the cost of a best path from s to vi, he need receive only the best subcost from his friends.  See the numbers within the circles in Figure 18.3.b.  For each of the edges  cid:2 vk, vi cid:3  from his destination node vi, he learns from Friendk the cost of a best path from s to vk. He adds the cost of the edge  cid:2 vk, vi cid:3  to this to determine the cost of a best path from s to vi from amongst those that take this edge. Then he determines the cost of an overall best path from s to vi by taking the best of these best costs. Note that this algorithm requires O n · d —not O n · d · n —time, because this best cost can be transferred from Friendk to Friendi in constant time. How- ever, this algorithm ﬁnds only the cost of the best path; it does not ﬁnd a best path.  The Little Bird’s Advice:  Deﬁnition of Advice: A friend trying to ﬁnd an optimal solution to his subin- stance asks the little bird a question about this optimal solution. The answer, usually denoted k, to this question classiﬁes the solutions. If this friend had an all-powerful little bird, then she could advise him which class of solutions to search in to ﬁnd an optimal solution. Given that he does not have such a bird, he must simply try all K of the possible answers and determine himself which answer is best. Either way, we will refer to this best answer as the little bird’s ad- vice.  Leveled Graph: The bird’s advice to Friendi, who is trying to ﬁnd a best path from s to vi, is which edge to take last. This edge for each friend is indicated by the little arrows in Figure 18.3.b.  Advice from Cost: Within the algorithm that transfers only the cost of an optimal solution, each friend is able to determine the little bird’s advice to him.  Leveled Graph: Friendi determines, for each of the edges  cid:2 vk, vi cid:3  into his node, the cost of a best path from s to vi from amongst those that take this edge, and then determines which of these is best. Hence, though this Friendi never learns a best path from s to vi in its entirety, he does learn which edge is taken last. In other words, he does determine, even without help from the little bird, what the little bird’s advice would be.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes18 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  March 29, 2008  13:40  Transferring the Bird’s Advice: The bird’s advice does not need to be transferred from Friendk to Friendi, because Friendi does not need it. However, Friendk will store this advice in the table so that it can be used at the end of the algorithm. This advice can usually be stored in constant space. Hence, it can be stored along with the best cost in constant time without slowing down the algorithm.  Leveled Graph: The advice indicates a single edge. Theoretically, taking O log n  bits, this takes more than constant space; practically, however, it can be stored using two integers.  290  Information Stored in Table: In order to make the dynamic programming algo- rithm faster, the information stored in the table will no longer be an optimal solution and its cost. Instead, only the cost of an optimal solution and the little bird’s advice k are stored.  Leveled Graph: The dynamic programming code in Section 18.2 for Leveled- = optSol[k] +  cid:2 vk, vi cid:3 ” Graph has only two small changes. The line “optSolk within the inner loop and the line “optSol[i] = optSolkmin,” which stores the opti- mal solution, are commented out  I recommend leaving them in as comments to add clarity for the reader . The second of these is replaced with the line “birdAdvice[i] = kmin,” which stores the bird’s advice. See Section 18.3.6 for the new code.  Time and Space Requirements: The running time of the algorithm computing the costs and the bird’s advice is  Time =  the number of subinstances indexing your table   ×  the number of different answers K to the bird’s question   The space requirement is  Space = the number of subinstances indexing your table  Leveled Graph: As said, there are n subinstances and a bird answer for each edge out of his source node vi. Hence, the running time is O n · d . Printing Neatly: There are  cid:1  n  subinstances in the table, and the number of possible answers for the bird is  cid:1  n , because she has the option of telling you pretty well any number of words to put on the last line. Hence, the total running time is  cid:1  n  ·  cid:1  n  =  cid:1  n2 , and the space requirement is  cid:1  n .  Constructing an Optimal Solution: With these modiﬁcations, the algorithm no longer constructs an optimal solution. An optimal solution for the original instance is required, but not for the subinstances. We construct an optimal solution for the in- stance using a separate algorithm that is run after the faster dynamic programming algorithm ﬁlls the table in with costs and bird’s advice. This new algorithm starts over from the beginning, solving the optimization problem. However, now we know what   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes18 CUUS154-Edmonds 978 0 521 84931 9  Dynamic Programming Algorithms  March 29, 2008  13:40  answer the little bird would give for every subinstance considered. Hence, we can simply run the bird–friend algorithm.  A Recursive Bird–Friend Algorithm: The second run of the algorithm will be identical to the recursive algorithm, except now we only need to follow one path down the recursion tree. Each stack frame, instead of branching for each of the K answers that the bird might give, recurses only on the single answer given by the bird. This algorithm runs very quickly. Its running time is proportional to the number of ﬁelds needed to represent the optimal solution.  Leveled Graph: A best path from s = v0 to t = vn is found as follows. Each friend knows which edge is the last edge taken to get to his node. What remains is to put these pieces together by walking backwards through the graph, following the indicated directions. See Figure 18.3.c. Friendt knows that the last edge is  cid:2 v 8, t  cid:3 . Friend8 knows that the previous one is  cid:2 v5, v 8 cid:3 . Friend5 knows the edge  cid:2 v3, v5 cid:3 . Finally, Friend3 knows the edge  cid:2 s, v3 cid:3 . This completes the path.  291  algorithm LeveledGraphWithAdvice  cid:2 G, s, vi cid:3 , birdAdvice   cid:2  pre- & post-cond  cid:3 : Same as LeveledGraph except with advice. begin  if s = vi  then return ∅  kmin = birdAdvice[i] optSubSol = LeveledGraphWithAdvice  cid:2 G, s, vkmin optSol = optSubSol +  cid:2 vk, vi cid:3  return optSol   cid:3 , birdAdvice   end algorithm  EXERCISE 18.3.2 Because the algorithm for leveled graphs with advice only recurses once per stack frame, it is easy to turn it into an iterative algorithm. The algorithm is a lot like a greedy algorithm. In that it always knows which greedy choice to make. Design this iterative algorithm.  18.3.5 Counting the Number of Solutions  The dynamic programming algorithms we have considered so far return one of the possibly many optimal solutions for the given instance. There may be an exponential number of solutions. In this section, we see how to change the algorithms so that they also output the number of possible optimal solutions.  Counting Fruit: If I want to count the number of pieces of fruit in a bowl, I can ask one friend to count for me the red fruit and another the green fruit and another the orange. My answer is the sum of these three. If all the orange fruit is rotten, then I might not include the number of orange fruits in my sum.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes18 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  March 29, 2008  13:40  Double Counting: To be sure that we do not double-count some optimal solutions, we need the set of solutions consistent with one bird answer to be disjoint from those consistent with another bird answer.  292  Fruit: If some fruits are half red and half green, these might get double counted. We want to make sure that color disjointly partitions the fruit according to the answer given.  Leveled Graph: If the bird’s answer tells us the ﬁrst edge of the optimal path, then those paths beginning in the ﬁrst edge are clearly different than the paths ending in the second.  Computing the Count: In the new dynamic programming algorithm, each friend stores the number of optimal solutions for the subinstance, in addition to the cost of the optimal solution for his subinstance and the bird’s advice. This is computed as follows. A friend with one of these subinstances tries each of the K possible bird’s answers. When trying k, he ﬁnds the best solution to his instance from amongst its solutions that are consistent with this bird’s answer k by asking a friend of his to solve some subinstance. This friend tells him the number of optimal solutions to this subinstance. Let numk be this number. Generally, there is a one-to-one mapping be- tween our friend’s optimal solutions and solutions to his instance that are consistent with this bird’s answer k. Hence, he knows that there are numk of these. As before, let optCostk be the cost of the best solution to my instance from among its solutions that are consistent with this bird’s answer k. The friend computes the cost of his opti- mal solution simply by optCost = maxk∈[K ] optCostk. There may be a number of bird’s answers that lead to this same optimal cost optCost. Each of these lead to optimal of optimal solutions to my instance is num = cid:1  solutions. We were careful that the set of solutions consistent with one bird answer is disjoint from that consistent with another bird answer. Hence, the total number =optCost} numk. See Sec-  k∈{k  optCostk  tion 18.3.6 for the new leveled graph code.  Bounding the Number of Solutions: Let Num n  denote the maximum number of possible optimal solutions that any dynamic program as described above might most n − 1. There are at most K bird answers. Hence, num = cid:1  output, given an instance of size n. The number numk of optimal solutions reported by our friend’s friend is at most Num n − 1 , because his subinstance has size at numk can be at most Num n  = cid:1  =optCost} k∈[1..K ] Num n − 1  = K × Num n − 1  = K n. Note that it would take exponential time to output these optimal solutions. However, the number of bits to represent this number is only log2  K n  = log2 K   × n =  cid:1  n . Hence, this number can be outputted.  k∈{k  optCostk  18.3.6 The New Code  Three Changes: The dynamic programming algorithm for the leveled graph prob- lem developed in Section 18.2 has been changed in three ways.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes18 CUUS154-Edmonds 978 0 521 84931 9  Dynamic Programming Algorithms  March 29, 2008  13:40  Reversing the Order: As described in Section 18.3.1, the algorithm was redevel- oped with the little bird being asked for the last edge in the path instead of for the ﬁrst edge. This has the aesthetic advantage of having the algorithm now branch out forward from s instead of backward from t.  Storing Bird’s Advice Instead of Solution: As described in Section 18.3.4, the al- gorithm now stores the little bird’s advice instead of the optimal solution. This is done to decrease the time and space used by the algorithm by a factor of n.  293  Counting the Number of Solutions: As described in Section 18.3.5, the algo- rithm, in addition to one of the many optimal solutions for the given instance, now also outputs the number of possible optimal solutions.  Code:  begin  algorithm LeveledGraph  G, s, t   cid:2  pre-cond cid:3 : G is a weighted directed layered graph, and s and t are nodes.  cid:2  post-cond cid:3 : optSol is a path with minimum total weight from s to t, and optCost is its weight, and optNum is the number of possible optimal solutions.  % Table: optSol[i] would store an optimal path from s to vi, but actually we store only the bird’s advice for the subinstance and the cost of its solution. table[0..n] birdAdvice, optCost, optNum % Base case: The only base case is for the best path from s to s. It only has one optimal solution, which is the empty path with cost zero. % optSol[0] = ∅ optCost[0] = 0 birdAdvice[0] = ∅ optNum[0] = 1 % General cases: Loop over subinstances in the table. for i = 1 to n % Solve instance  cid:2 G, s, vi cid:3  and ﬁll in table entry  cid:2 i cid:3 . % Try each possible bird answer. for each of the d edges  cid:2 vk, vi cid:3   % The bird-and-friend algorithm: The bird tells us that the last edge in an optimal path from s to vi is  cid:2 vk, vi cid:3 . We ask the friend for an optimal path from s to vk. He gives us optSol[k], which he has stored in the table. To this we add the bird’s edge. This gives us optSolk, which is a best path from s to vi from amongst those paths consistent with the bird’s answer. % optSolk optCostk  = optSol[k] +  cid:2 vk, vi cid:3  = optCost[k] + w cid:2 vk ,vi cid:3   end for   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes18 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  March 29, 2008  13:40  294  % Having the best, optSolk, for each bird’s answer k, we keep the best of these best. kmin = a k that minimizes optCostk % optSol[i] = optSolkmin optCost[i] = optCostkmin optNum[i] = cid:1  birdAdvice[i] = kmin  k∈{k  optCostk  end for optSol = LeveledGraphWithAdvice return  cid:2 optSol, optCost[n], optNum[n] cid:3   =optCostkmin  } optNum[k].   cid:2  cid:2 G, s, vn cid:3 , birdAdvice  cid:3   end algorithm  EXERCISE 18.3.3  See solution in Part Five.  Give the code for printing neatly.  EXERCISE 18.3.4 Consider printing neatly the silly text “This week has seven dates in it ok” in a column with width M = 11. This is represented as the printing neatly instance  cid:2 M; l1, . . . , ln cid:3  =  cid:2 11; 4, 4, 3, 5, 5, 2, 2, 2 cid:3 .  a  Fill in the birdAdvice[0..n] and cost[0..n] tables for this example. The original instance, its solution, and its cost are in the bottom row.  b  When ﬁlling in this last row, give solutions and costs associated with each of the possible bird answers.  EXERCISE 18.3.5 I saw this puzzle on a Toronto subway. The question is how many times the word “TRAINS” appears in the accompanying diagram. Each occurence of the word must fol- low a connected path so that each of its letters are adjacent to its previous letter. In order to learn the number of such occurences, we could count them, but this might be exponential in the num- ber of squares. Instead, for each box do a con- stant amount of work and write one integer. In the end, the answer should appear in the box with a “T.” You should give a few sentences ex- plaining the order in which you ﬁll the boxes, how you do it, and how much work it is.  S  S N  S  S N I  N  S  S  N I A  I  N S  1S N  I A R  A  I N S  1S N3 I  A R T  R  A  I N  S  1S N  I A R  A  I N S  S  N I A  I  N S  S N I  N  S  S N  S  S  This completes the presentation of the general techniques and the theory behind dynamic programming algorithms. We will now develop algorithms for other opti- mization problems.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes19 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  18:9  19 Examples of Dynamic Programs  295  19.1 The Longest-Common-Subsequence Problem  There is a big demand for algorithms that ﬁnd patterns in strings, for example, DNA. The following optimization problem is called the longest common subsequence  LCS .  Longest Common Subsequence:  Instances: An instance consists of two sequences X =  cid:2 x1, . . . , xn cid:3  and Y =  cid:2 y1, . . . , ym cid:3  For example, X =  cid:2 B, D, C, A , B, A cid:3  and Y =  cid:2 A , B, C, B, D, A , B cid:3 . Solutions: A subsequence of a sequence is a subset of the elements taken in the same order. A solution is a subsequence Z =  cid:2 z1, . . . , z l cid:3  that is common to both X and Y.  Measure of Success: The cost  or success  of a solution is the length of the com- mon subsequence.  Goal: Given two sequences X and Y, the goal is to ﬁnd the LCS. Example: Z =  cid:2 B, C, A cid:3  is a solution because it is a subsequence of X =  cid:2 B, D, C, A, B, A cid:3  and Y  Y =  cid:2 A , B, C, B, D, A, B cid:3  . The cost  success  of this subsequence is Z = 3. Here Z =  cid:2 B, C, B, A cid:3  would be a longer common sub- sequence with cost 4.  Greedy Algorithm: Suppose X =  cid:2 A , B, C, D cid:3  and Y =  cid:2 B, C, D, A cid:3 . A greedy algo- rithm might commit to matching the two A’s. However, this would be a mistake, be- cause the optimal answer is Z =  cid:2 B, C, D cid:3 .  Possible Little Bird Answers: Typically, the question asked of the little bird is for some detail about the end of an optimal solution.  Case xn  cid:1 = zl: Suppose that the bird assures us that the last character of X is not the last character of at least one LCS Z of X and Y. We could then simply   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes19 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  296  March 29, 2008  18:9   cid:1  =   cid:1  =  cid:2 x1, . . . , xn−1 cid:3  and Y  ignore this last character of X. We could ask a friend to give us a LCS of X  cid:2 x1, . . . , xn−1 cid:3  and Y, and this would be a LCS of X and Y. Case ym  cid:1 = zl: Similarly, if we are told that the last character of Y is not used, then we can ignore it. Case xn = ym = zl: Suppose that the little bird tells us that the last character of both X and Y is the last character of an optimal Z. This, of course, implies that the last characters of X and Y are the same. In this case, we could simply ignore this last character of both X and Y. We could ask a friend to give us a longest  cid:1  =  cid:2 y1, . . . , ym−1 cid:3 . A LCS of X common subsequence of X and Y would be the same, except with the character xn = ym tacked on to the end, that is, Z = Z xn. On the other hand, if the little bird answers this case with xn = ym = z l and we on our own observe that xn  cid:4 = yn, then we know that the little bird is wrong. Case zl =?: Even more extreme, suppose that the little bird goes as far as to tell us the last character of a LCS Z. We could then delete the last characters of X and Y up to and including the last occurrence of this character. A friend could give us a LCS of the remaining X and Y, and then we could add on the known character to give us Z. Case xn = zl  cid:1 = ym: Suppose that the bird assures us that the last character of X is the last character of an optimal Z. This will tell us the last character of Z, and hence the last case will apply.   cid:1   The Question for the Little Bird: We have a number of different answers that the little bird might give, each of which would help us ﬁnd an optimal Z. We could add even more possible answers to the list. However, the larger the number K of possible answers is, the more work our algorithm will have to do. Hence, we want to narrow this list of possibilities down as far as possible. We will consider only the ﬁrst three bird answers. This is sufﬁcient because for every possible solution at least one of these is true, namely, either  1  xn  cid:4 = z l,  2  ym  cid:4 = z l, or  3  xn = ym = z l. Of course, it may be the case that xn  cid:4 = z l and ym  cid:4 = z l, but if this is true, than the bird has a choice whether to answer  1  or  2 .  The Set of Subinstances: We guess that the set of subinstances of the instance  cid:2  cid:2 x1, . . . , xn cid:3 ,  cid:2 y1, . . . , ym cid:3  cid:3  is { cid:2  cid:2 x1, . . . , xi cid:3 ,  cid:2 y1, . . . , y j cid:3  cid:3   i ≤ n,  j ≤ m}.  Closed: We know that this set contains all subinstances generated by the re- cursive algorithm, because it contains the initial instance and is closed under the sub operation. Consider an arbitrary subinstance,  cid:2  cid:2 x1, . . . , xi cid:3 ,  cid:2 y1, . . . , y j cid:3  cid:3 . Applying the sub operator constructs the subinstances  cid:2  cid:2 x1, . . . , xi−1 cid:3 ,  cid:2 y1, . . . , y j−1 cid:3  cid:3 ,  cid:2  cid:2 x1, . . . , xi−1 cid:3 ,  cid:2 y1, . . . , y j cid:3  cid:3 , and  cid:2  cid:2 x1, . . . , xi cid:3 ,  cid:2 y1, . . . , y j−1 cid:3  cid:3 , which are all contained in the stated set of subinstances.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes19 CUUS154-Edmonds 978 0 521 84931 9  Examples of Dynamic Programs  March 29, 2008  18:9  Generating: We know that the speciﬁed set of subinstances does not contain subinstances not called by the recursive program, because we can construct any arbitrary subinstance from the set with the sub operator. Consider an arbitrary subinstance  cid:2  cid:2 x1, . . . , xi cid:3 ,  cid:2 y1, . . . , y j cid:3  cid:3 . The recursive program on the instance  cid:2  cid:2 x1, . . . , xn cid:3 ,  cid:2 y1, . . . , ym cid:3  cid:3  can recurse on the ﬁrst option n − i times and then on the second option m − j times. This results in the subinstance  cid:2  cid:2 x1, . . . , xi cid:3 ,  cid:2 y1, . . . , y j cid:3  cid:3 .  297  Constructing a Table Indexed by Subinstances: We now construct a table hav- ing one entry for each subinstance. It will have a dimension for each of the parame- ters i and j used to specify a particular subinstance. The tables will be cost[0..n, 0..m] and birdAdvice[0..n, 0..m]. Base Cases: The subinstance represented by cost[0, j ] is  cid:2 ∅,  cid:2 y1, . . . , ym cid:3  cid:3  and has no characters in X. Hence, it is not reasonable to ask the bird about the last character of X. Besides, this is an easy case to handle as a base case. The only subsequence of the empty string is the empty string. Hence, the LCS is the empty string. The cost of this solution is cost[0, j ] = 0.  Order in Which to Fill the Table: The ofﬁcial order in which to ﬁll the table with subinstances is from smaller to larger. Here the size of the subinstance  cid:2  cid:2 x1, . . . , xi cid:3 ,  cid:2 y1, . . . , y j cid:3  cid:3  is i + j . Thus, you would ﬁll in the table along the diago- nals. However, the obvious order of looping, for i = 0 to n and from j = 0 to m, also respects the dependences between the instances and thus could be used instead.  Code:  algorithm LCS  cid:2  cid:2 x1, . . . , xn cid:3 ,  cid:2 y1, . . . , ym cid:3  cid:3    cid:2  pre-cond cid:3 : An instance consists of two sequences.  cid:2  post-cond cid:3 : optSol is a LCS, and optCost is its length. begin  % Table: optSol[i, j ] would store an optimal LCS for  cid:2  cid:2 x1, . . . , xi cid:3 ,  cid:2 y1, . . . , y j cid:3  cid:3 , but actually we store only the bird’s advice for the subinstance and the cost of its solution. table[0..n, 0..m] birdAdvice, cost  % Base cases: The base cases consist of when one string or the other is empty, i.e., when i = 0 or when j = 0. For each, the solution is the empty string with cost zero. for j = 0 to m  % optSol[0, j ] = ∅ cost[0, j ] = 0 birdAdvice[0, j ] = ∅   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes19 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  18:9  Optimization Problems  end for for i = 0 to n % optSol[i, 0] = ∅ cost[i, 0] = 0 birdAdvice[i, 0] = ∅  298  end for  % General cases: Loop over subinstances in the table. for i = 1 to n  for j = 1 to m  % Solve instance  cid:2  cid:2 x1, . . . , xi cid:3 ,  cid:2 y1, . . . , y j cid:3  cid:3 , and ﬁll in table entry  cid:2 i, j cid:3 . % The bird-and-friend algorithm: The bird tells us either  1  xi  cid:4 = zl cid:1 ,  2  y j  cid:4 = zl cid:1 , or  3  xi = y j = zl cid:1 . We remove this last letter  1  xi,  2  y j , or  3  both and ask the friend for an optimal LCS for the re- maining words. He gives us  1  optSol[i − 1, j ],  2  optSol[i, j − 1], or  3  optSol[i − 1, j − 1], which he has stored in the table. For cases 1 and 2, we leave this the same. For case 3, we add on the bird’s letter, assuming that xi = y j . This gives us optSolk which is a LCS for  cid:2 i, j cid:3  from amongst those printings consistent with the bird’s answer. % Try each possible bird answers. % Case k = 1 : xi  cid:4 = zl cid:1  = optSol[i − 1, j ]  = optSol[i, j − 1]  % optSol1 cost1 = cost[i − 1, j ] % Case k = 2 : y j  cid:4 = zl cid:1  % optSol2 cost2 = cost[i, j − 1] if xi = y j then % optSol3 cost3 = cost[i − 1, j − 1] + 1  % Case k = 3 : xi = y j = zl cid:1   = optSol[i − 1, j − 1] + xi  else  end if  % Bird was wrong. =? % optSol3 cost3 = −∞  % end cases % Having the best, optSolk, for each bird’s answer k, we keep the best of these best. kmax = a k ∈ [1, 2, 3] that maximizes costk % optSol[i, j ] = optSolkmax   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes19 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  18:9  Examples of Dynamic Programs  cost[i, j ] = costkmax birdAdvice[i, j ] = kmax  end for  end for optSol = LCSWithAdvice  cid:2  cid:2 x1, . . . , xn cid:3 ,  cid:2 y1, . . . , ym cid:3  cid:3 , birdAdvice  return  cid:2 optSol, cost[n, m] cid:3   299  end algorithm  Using Information about the Subinstance: This algorithm only has three differ- ent bird answers. Surely, this is good enough. However, the number of the possi- ble answers can be narrowed even further. We know the instance is  cid:2  cid:2 x1, . . . , xi cid:3 ,  cid:2 y1, . . . , y j cid:3  cid:3 ; hence, without asking the bird, we know whether or not the last char- acters are the same, that is, whether or not xi = y j . If xi  cid:4 = y j , the third case with xi = y j = zl cid:1  is clearly not possible. Conversely, when xi = y j , one might also wonder whether the case xi  cid:4 = zl cid:1  could ever lead to an optimal solution. Exercise 19.1.1 shows that in this case we only need to consider xi = y j = zl cid:1 . As in a greedy algorithm, we know the answer even before asking the question.  Constructing an Optimal Solution:  algorithm LCSWithAdvice  cid:2  pre- & post-cond cid:3 : Same as LCS except with advice. begin   cid:1  cid:2  cid:2 x1, . . . , xi cid:3 ,  cid:2 y1, . . . , y j cid:3  cid:3 , birdAdvice   cid:2   if  i = 0 or j = 0  then  optSol = ∅ return optSol  end if kmax = birdAdvice[i, j ] if kmax = 1 then  else if kmax = 2 then  optSubSol = LCSWithAdvice  cid:2  cid:2 x1, . . . , xi−1 cid:3 ,  cid:2 y1, . . . , y j cid:3  cid:3 , birdAdvice  optSol = optSubSol optSubSol = LCSWithAdvice  cid:2  cid:2 x1, . . . , xi cid:3 ,  cid:2 y1, . . . , y j−1 cid:3  cid:3 , birdAdvice  optSol = optSubSol optSubSol = LCSWithAdvice  cid:2  cid:2 x1, . . . , xi−1 cid:3 ,  cid:2 y1, . . . , y j−1 cid:3  cid:3 , birdAdvice  optSol =  cid:2 optSubSol, xi cid:3   else if kmax = 3 then  end if return optSol  end algorithm  Time and Space Requirements: See Exercise 19.1.2.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes19 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  March 29, 2008  18:9  j  0 jy  1  0  2  1  3  0  4  1  5  1  6  0  7  1  8  0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  0  1  1  1  2  2  2  0  1  2  2  2  3  3  0  1  2  2  3  3  4  0  0  0  1  2  2  3  3  4  1  2  3  3  4  4  1  2  3  4  4  5  0  1  2  3  4  5  5  300  i  0  1  2  3  4  5  6  x  i  1  0  0  1  0  1  0  3  2  1  0  7 Figure 19.1: The tables generated for the instance X = 1001010 and Y = 01011010. Each number is cost[i, j ], which is the length of the longest common subsequence of the ﬁrst i characters of X and the ﬁrst j characters of Y. The arrow indicates whether the bird’s advice is to include xi = y j , to exclude xi, or to exclude y j . The circled digits of X and Y give an optimal solution.  4  4  5  5  6  Example: See Figure 19.1. EXERCISE 19.1.1  See solution in Part Five.   a  Prove that if xi = y j , then we only need to consider the third case, and if xi  cid:4 = y j , then we only need to consider the ﬁrst two cases.  b  Show how this changes the code.  EXERCISE 19.1.2  See solution in Part Five.  Calculate the running time of this algorithm.  19.2 Dynamic Programs as More-of-the-Input Iterative  Loop Invariant Algorithms  A dynamic program can be thought of from two very different perspectives: as an op- timized recursive backtracking algorithm and as an iterative loop invariant algorithm that ﬁlls in a table. From the perspective of an iterative algorithm, the loop invariant maintained is that the previous table entries have been ﬁlled in correctly. Progress is made while maintaining this loop invariant by ﬁlling in the next entry. This is accom- plished using the solutions stored in the previous entries.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes19 CUUS154-Edmonds 978 0 521 84931 9  Examples of Dynamic Programs  March 29, 2008  18:9  In more-of-the-input iterative algorithms  see Section 1.2 , the subinstances are preﬁxes of the instance, and hence the algorithm iterates through the subinstances by iterating in some way through the elements of the instance.  SIMPLE EXAMPLE 19.2.1  Longest Increasing Contiguous Subsequence  301  Suppose that the input consists of a sequence A[1..n] of integers and we want to ﬁnd the longest contiguous subsequence A[k1, k2] such that the elements are monotoni- cally increasing. For example, the optimal solution for [5, 3, 1, 3, 7, 9, 8] is [1, 3, 7, 9].  Longest Block of Ones: The problem is very similar to the longest-block-of-ones problem given in Section 2.2.  Deterministic Nonﬁnite Automation: The algorithm will read the input characters one at a time. Let A[1..i] denote the subsequence read so far. The loop invariant will be that some information about this preﬁx A[1..i] is stored. From this information about A[1..i] and the element A[i + 1], the algorithm must be able to determine the required information about the preﬁx A[1..i + 1]. In the end, the algorithm must be able to de- termine the solution from this information about the entire sequence A[1..n]. Such an algorithm is a deterministic ﬁnite automaton  DFA  if only a constant amount of infor- mation is stored at each point in time.  See Section 2.2.  However, in this chapter more memory will be required.  The Algorithm: After reading A[1..i], remember the longest increasing contiguous subsequence A[k1..k2] read so far and its size. In addition, so that you know whether the current increasing contiguous subsequence gets to be longer than the previous one, save the longest one ending in the value A[i], and its size. If you have this information about A[1..i − 1], then you can learn it about A[1..i] as follows. If A[i − 1] ≤ A[i], then the longest increasing contiguous subsequence ending in the current value increases in length by one. Otherwise, it shrinks to being only the one element A[i]. If this subsequence increases to be longer than our previous longest, then it replaces the previous longest. In the end, we know the longest increasing con- tiguous subsequence.  The running time is  cid:1  n . This is not a DFA, because the amount of space to re-  member an index and a count  amounting to  cid:1  n  values  is  cid:1  log n  bits.  HARDER EXAMPLE 19.2.2  Longest Increasing Subsequence  Again the input consists of a sequence A of integers of size n. However, now we want to ﬁnd the longest  not necessarily contiguous  subsequence S ⊆ [1..n] such that the ele- ments, in the order that they appear in A, are monotonically increasing. For example, an optimal solution for [5, 1, 5, 7, 2, 4, 9, 8] is [1, 5, 7, 9], and so is [1, 2, 4, 8].  Dynamic Programming Deterministic Nonﬁnite Automation: Again the algorithm will read the input characters one at a time. But now the algorithm will store suitable   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes19 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  March 29, 2008  18:9  HARDER EXAMPLE 19.2.2  Longest Increasing Subsequence  cont.   information, not only about the current subsequence A[1..i], but also about each pre- vious subsequence ∀j ≤ i, A[1..j ]. Each of these subsequences A[1..j ] will be referred to as a subinstance of the original instance A[1..n].  302  The Algorithm: As before, we will store both the longest increasing sequence seen so far and the longest one s  that we are currently growing.  The Loop Invariant: Suppose that the subsequence read so far is 10, 20, 1, 30, 40, 2, 50. Then 10, 20, 30, 40, 50 is the longest increasing subsequence so far. A shorter one is 1, 2, 50. The problem is that these end in a relatively large number, so we may not be able to extend them further. If the rest of the string is 3, 4, 5, 6, 7, 8, we will have to have remembered that 1, 2 is the longest increasing subsequence that ends in the value 2. In fact, for many values v, we need to remember the longest increasing subsequence ending in this value v  or a smaller one , because in the end, it may be that many of the remaining elements increase starting from this value. We only need to do this for values v that have been seen so far in the array. Hence, one possibility is to store, for each j ≤ i, the longest increasing sub- sequence in A[1..j ] that ends with the value A[j ]. Maintaining the Loop Invariant: If we have this information for each j ≤ i − 1, then we learn it about A[i] as follows. For each j ≤ i − 1, if A[j ] ≤ A[i], then A[i] can extend the subsequence ending with A[j ]. Given this construction, the maxi- mum length for i will then be one more than that for j . We get the longest one for i by taking the best of these overall such j . If there is no such j , then the count for i will be 1, namely, simply A[i] itself.  Final Solution: In the end, the solution is the increasing subsequence ending in A[j ], where j ∈ [1..n] is that for which this count is the largest.  Running Time: The time for ﬁnding this best subsequence ending in A[j ] from which to extend to A[i] would be  cid:1  i  if each j ∈ [1..i − 1] needed to be checked. However, by storing this information in a heap, the best j can be found in  cid:1  logi  time. This gives a total time of  cid:1     cid:3  i=1 logi  =  cid:1  n log n  for this algorithm.  n  Recursive Backtracking: We can also understand this same algorithm from the recur- sive backtracking perspective. Given the goal of ﬁnding the longest increasing sub- sequence of A[1..i], we might ask the little bird whether or not the last element A[i] should be included. Both options need to be tried. If A[i] is not to be included, then the remaining subtask is to ﬁnd the longest increasing subsequence of A[1..i − 1]. This is clearly a subinstance of the same problem. However, if A[i] is to be included, then the remaining subtask is to ﬁnd the longest increasing subsequence of A[1..i − 1] that ends in a value that is smaller than or equal to this last value A[i]. This is another way of seeing that we need to learn both the longest increasing sequence of the preﬁx A[1..i] and the longest one ending in A[i].   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes19 CUUS154-Edmonds 978 0 521 84931 9  Examples of Dynamic Programs  March 29, 2008  18:9  303  19.3 A Greedy Dynamic Program: The Weighted Job Event  Scheduling Problem  We revisit the event scheduling problem from Section 16.2.1, now prioritizing the events. Our original greedy algorithm won’t work, but dynamic programming will.  The Weighted Event Scheduling Problem: Suppose that many events want to use your conference room. Some of these events are given a higher priority than oth- ers. Your goal is to schedule the room in the optimal way.  Instances: An instance is  cid:2  cid:2 s1, f1, w1 cid:3 ,  cid:2 s2, f2, w2 cid:3 , . . . ,  cid:2 sn, fn, wn cid:3  cid:3 , where 0 ≤ si ≤ fi are the starting and ﬁnishing times and wi the priority weight for n events.  Solutions: A solution for an instance is a schedule S. This consists of a subset S ⊆ [1..n] of the events that don’t conﬂict by overlapping in time.  Measure of Success: The cost  or success  C S  of a solution S is the sum of the weights of the events scheduled, i.e.,  i∈S wi.  Goal: The goal of the algorithm is to ﬁnd the optimal solution, that is, that maxi- mizes the total scheduled weight.   cid:3   Failed Algorithms:  Greedy Earliest Finishing Time: The greedy algorithm used in Section 16.2.1 for the unweighted version greedily selects the event with the earliest ﬁnishing time fi. This algorithm fails when the events have weights. The following is a coun- terexample:  1  1000  The speciﬁed algorithm schedules the top event for a total weight of 1. The opti- mal schedule schedules the bottom event for a total weight of 1000.  Greedy Largest Weight: Another greedy algorithm selects the ﬁrst event using the criterion of the largest weight wi. The following is a counterexample for this:  2 111  1 1 1  111  The top event has weight 2 and the bottom ones each have weight 1. The speci- ﬁed algorithm schedules the top event for a total weight of 2. The optimal sched- ule schedules the bottom events for a total weight of 9.  Dynamic Programming: The obvious dynamic programming algorithm is for the little bird to tell you whether or not to schedule event Jn.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes19 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  304  March 29, 2008  18:9  Bird-and-Friend Algorithm: Consider an instance J =  cid:2  cid:2 s1, f1, w1 cid:3 ,  cid:2 s2, f2, w2 cid:3 , . . . ,  cid:2 sn, fn, wn cid:3  cid:3 . The little bird considers an optimal schedule. We ask the little bird whether or not to schedule event Jn. If she says yes, then the remaining possible events to schedule are those in J , excluding event Jn and excluding all events that conﬂict with event Jn. We ask a friend to schedule these. Our schedule is his with event Jn added. If instead the bird tells us not to schedule event Jn, then the remaining possible events to schedule are those in J excluding event Jn.  The Set of Subinstances: When tracing the recursive algorithm on small ex- amples, we see that the set of subinstance used can be exponentially large. See the left side of Figure 19.2 for an example. The events in the instance are paired so that for i ∈ [1.. n +i, but jobs between pairs do not conﬂict. After the little bird tells you whether or not to schedule jobs J n 2 ], job Ji will remain in the subinstance if and only if job 2 +i was not scheduled. This results in at least 2n 2 different paths down the J n 2 tree of stack frames in the recursive backtracking algorithm, each leading to a different subinstance.  2 ], job Ji conﬂicts with job J n  +i for i ∈ [1.. n  2  In contrast, look at the instance on the right in Figure 19.2. It only has a linear number of subinstances, because each subinstance is a preﬁx of the events. The only difference between these examples is the order of the events.  Greedy Dynamic Programming: First sort the events in increasing order of their ﬁnishing times fi  a greedy thing to do . Then run the same dynamic programming algorithm in which the little bird tells you whether or not to schedule event Jn.  The Set of Subinstances: When tracing the recursive algorithm on small ex- it looks hopeful that the set of subinstance used is { cid:2  cid:2 s1, f1, w1 cid:3 , amples,  cid:2 s2, f2, w2 cid:3 , . . . ,  cid:2 si, fi, wi cid:3  cid:3   i ∈ [0..n]}. If so, the algorithm is polynomial time. The danger is that when excluding those events that conﬂict with event Jn, the subin- stance created will no longer have a contiguous preﬁx of the events. For example, if event J2 conﬂicts with Jn but event J3 does not, then the new subinstance will need to be cid:2  cid:2 s1, f1, w1 cid:3 ,  cid:2 s3, f3, w3 cid:3 , . . . , ?? cid:3 , which is not included. Needing to solve this subinstance as well may make the running time exponential.  Closed: For this algorithm, more than those previous, we must be care- ful to show that this set contains all subinstances generated by the recur- sive algorithm by showing that it is closed under the sub operation. Con- sider an arbitrary subinstance  cid:2  cid:2 s1, f1, w1 cid:3 ,  cid:2 s2, f2, w2 cid:3 , . . . ,  cid:2 si, fi, wi cid:3  cid:3  in the set. If we delete from this event Ji and all events that conﬂict with it, we  cid:1  ∈ [0..i − 1] be must show that this new subinstance is again in our set. Let i the largest index such that fi cid:1  ≤ si. Because the events have been sorted by   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes19 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  18:9  Examples of Dynamic Programs  Job numbers  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  10  12  14  16  18  Subinstance after committing to keeping 18  1  2  3  4  5  6  7  8  Subinstance after committing to keeping 18 1  13  11  9  7  5  3  15  1  2  3  4  5  6  7  8  9  1 1  13  15  17  10  11  12  13  14  15  16  17  2  4  6  8  1 0  12  14  16  Subinstance after committing to rejecting 17  1  2  3  4  5  6  7  8  Subinstance after committing to rejecting 16 1  1 1  13  3  5  7  9  15  305  10  11  12  13  14  15  16  2  4  6  8  10  12  14  Subinstance after committing to keeping 16  1  2  3  4  5  6  Subinstance after committing to keeping 15 1  13  11  9  7  5  3  10  11  12  13  14  15  2  4  6  8  1 0  12  14  Subinstance after committing to rejecting 15  1  2  3  4  5  6  Subinstance after committing to rejecting 14 1  1 1  13  3  5  7  9  10  11  12  13  14  2  4  6  8  10  12  Subinstance after committing to keeping 14  1  2  3  4  Subinstance after committing to rejecting 13 1  1 1  3  5  7  9  10  11  12  13  2  4  6  8  10  12  8  8  8  8  6  6  Subinstance after committing to keeping 13  1  2  3  10  11  12  and so on  Subinstance after committing to keeping 12 1  9  7  5  3  2  4  6  8  1 0  Figure 19.2: Two examples of the subinstances formed from the recursive backtracking algorithm.  their ﬁnishing time, we know that all events Jk in  cid:2  cid:2 s1, f1, w1 cid:3 ,  cid:2 s2, f2, w2 cid:3 , . . . ,  cid:2 si cid:1 , fi cid:1 , wi cid:1  cid:3  cid:3  also have fk ≤ si and hence do not conﬂict with Ji. All events Jk in  cid:2  cid:2 si cid:1 +1, fi cid:1 +1, wi cid:1 +1 cid:3 , . . . ,  cid:2 si, fi, wi cid:3  cid:3  have si < fk ≤ fi and hence conﬂict with Ji. It follows that the resulting subinstance is  cid:2  cid:2 s1, f1, w1 cid:3 ,  cid:2 s2, f2, w2 cid:3 , . . . ,  cid:2 si cid:1 , fi cid:1 , wi cid:1  cid:3  cid:3 , which is our set of subinstances. If, on the other hand, only event Ji is deleted, then the resulting subinstance is  cid:2  cid:2 s1, f1, w1 cid:3 , . . . ,  cid:2 si−1, fi−1, wi−1 cid:3  cid:3 , which is obviously in our set of subinstances. It is because this works out that the algorithm is polynomial-time. Generating: Consider the arbitrary subinstance  cid:2  cid:2 s1, f1, w1 cid:3 ,  cid:2 s2, f2, w2 cid:3 , . . . ,  cid:2 si, fi, wi cid:3  cid:3 . It is generated by the recursive algorithm when the little bird states that none of the later events are included in the solution.  The Table: The dynamic programming table is a one-dimensional array indexed by i ∈ [0..n]. The order to ﬁll it in is with increasing i. As in the greedy algorithm, the events are considered to be ordered by earliest ﬁnishing time ﬁrst. The i entry is ﬁlled in by trying each of the two answers the bird might give.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes19 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  March 29, 2008  18:9  Time and Space Requirements: Generally, the running time is the number of subinstances times the number of possible bird answers, and the space is the number of subinstances. This would give T =  cid:1  n × 2  and S =  cid:1  n . In this case, however, the running time is larger than that. The reason is that when the event Ji is to be included, it takes O log n  time to do a binary search to ﬁnd which earlier events conﬂict with it and hence need to be deleted. This gives a running time of O n log n , apart from the initial O n log n  time for sorting of the activities by ﬁnishing time.  306  EXERCISE 19.3.1 Write out the pseudocode for this algorithm.  19.4 The Solution Viewed as a Tree: Chains of Matrix Multiplications  We now look at an example in which the ﬁelds of information specifying a solution are organized into a tree instead of into a sequence  see Section 18.3.2 . The algo- rithm asks the little bird to tell it the ﬁeld at the root of one of the instance’s optimal solutions, and then a separate friend will be asked for each of the solution’s subtrees. The optimization problem determines how to optimally multiply together a chain of matrices. Multiplying an a1 × a2 matrix by a a2 × a3 matrix requires a1 · a2 · a3 scalar multiplications. Matrix multiplication is associative, meaning that  M1 · M2  · M3 = M1 ·  M2 · M3 . Sometimes different bracketing of a sequence of matrix multiplications can lead to the total number of scalar multiplications being very dif- ferent. For example,    cid:2 5 × 1,000 cid:3  ·  cid:2 1,000 × 2 cid:3   ·  cid:2 2 × 2,000 cid:3  =  cid:2 5 × 2 cid:3  ·  cid:2 2 × 2,000 cid:3  =  cid:2 5 × 2,000 cid:3   requires 5 × 1,000 × 2 + 5 × 2 × 2,000 = 10,000 + 20,000 = 30,000 scalar multiplica- tions. However,   cid:2 5 × 1,000 cid:3  ·   cid:2 1,000 × 2 cid:3  ·  cid:2 2 × 2,000 cid:3   =  cid:2 5 × 1,000 cid:3  ·  cid:2 1,000 × 2,000 cid:3   =  cid:2 5 × 2,000 cid:3   requires 1,000× 2× 2,000 + 5× 1,000× 2,000 = 4,000,000 + 10,000,000 = 14,000,000. The problem considered here is to ﬁnd how to bracket a sequence of matrix mul- tiplications in order to minimize the number of scalar multiplications.  Chains of Matrix Multiplications:  Instances: An instance is a sequence of n matrices  cid:2 A 1, A 2, . . . , A n cid:3 .  A precondi- tion is that for each k ∈ [1..n − 1], width A k  = height A k+1 .  Solutions: A solution is a way of bracketing the matrices, e.g.,   A 1A 2   A 3 A 4A 5   . A solution can equivalently be viewed as a binary tree with the ma- trices A 1, . . . , A n at the leaves. The binary tree would give the order in which to multiply the matrices:   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes19 CUUS154-Edmonds 978 0 521 84931 9  Examples of Dynamic Programs  March 29, 2008  18:9  A1  A2  A3  A4  A5  Measure of Success: The cost of a solution is the number of scalar multiplica- tions needed to multiply the matrices according to the bracketing.  307  Goal: Given a sequence of matrices, the goal is to ﬁnd a bracketing that requires the fewest multiplications.  A Failed Greedy Algorithm: An obvious greedy algorithm selects where the last multiplication will occur according to which is cheapest. We can prove that any such simple greedy algorithm will fail, even when the instance contains only three ma- trices. Let the matrices A 1, A 2, and A 3 have height and width  cid:2 a0, a1 cid:3 ,  cid:2 a1, a2 cid:3 , and  cid:2 a2, a3 cid:3 . There are two orders in which these can be multiplied. Their costs are as follows:  cost  A 1 · A 2  · A 3  = a0a1a2 + a0a2a3 cost A 1 ·  A 2 · A 3   = a1a2a3 + a0a1a3  Consider the algorithm that chooses so that the last multiplication is the cheapest. Let us assume that the algorithm uses the ﬁrst order. This gives that a0a2a3 < a0a1a3, that is, a2   a1a2a3 + a0a1a3, that is, if a0 >> a3. Let us now assign simple values meeting a2 < a1 and a0 >> a3. Say a0 = 1000, a1 = 2, a2 = 1, and a3 = 1. Plugging these in gives cost  A 1 · A 2  · A 3  = 1000 × 2 × 1 + 1000 × 1 × 1 = 2000 + 1000 = 3000 cost A 1 ·  A 2 · A 3   = 2 × 1 × 1 + 1000 × 2 × 1 = 2 + 2000 = 2002  This is an instance in which the algorithm gives the wrong answer. Because 1000 < 2000, it uses the ﬁrst order. However, the second order is cheaper.  A Failed Dynamic Programming Algorithm: An obvious question to ask the little bird would be which pair of consecutive matrices to multiply together last. Though this algorithm works, it has exponential running time. The problem is that there are an exponential number of different subinstances. Consider paths down the tree of stack frames in which for each pair A 2i and A 2i+1, the bird either gets us to mul- tiply them together or does not. This results in 2n 2 different paths down the tree of stack frames in the recursive backtracking algorithm, each leading to a different subinstance.  The Question to Ask the Little Bird: A better question is to ask the little bird to give us the splitting k so that the last multiplication multiplies the product   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes19 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  18:9  Optimization Problems of  cid:2 A 1, A 2, . . . , A k cid:3  and of  cid:2 A k+1, . . . , A n cid:3 . This is equivalent to asking for the root of the binary tree. For each of the possible answers, the best solution is found that is consistent with this answer, and then the best of these best solutions is returned.  308  Reduced to Subinstance: With this advice, our search for an optimal bracketing is simpliﬁed. We need only solve two subinstances: ﬁnding an optimal bracketing of  cid:2 A 1, A 2, . . . , A k cid:3  and of  cid:2 A k+1, . . . , A n cid:3 . In the example with  cid:2 A 1, A 2, . . . , A 5 cid:3  above, the bird splits the problem into subinstances  cid:2 A 1, A 2 cid:3  and  cid:2 A 3, A 4, A 5 cid:3 . Recursive Structure: An optimal bracketing of the matrices  cid:2 A 1, A 2, . . . , A n cid:3  multi- plies the sequence  cid:2 A 1, . . . , A k cid:3  with its optimal bracketing, and  cid:2 A k+1, . . . , A n cid:3  with its optimal bracketing, and then multiplies these two resulting matrices together, that is, optSol =  optLeft  optRight .  The Cost of the Optimal Solution Derived from the Cost for Subinstances: The total number of scalar multiplications used in this optimal bracketing is the number used to multiply  cid:2 A 1, . . . , A k cid:3 , plus the number for  cid:2 A k+1, . . . , A n cid:3 , plus the number to multiply the ﬁnal two matrices.  cid:2 A 1, . . . , A k cid:3  evaluates to a matrix whose height is the same as that of A 1 and whose width is that of A k. Similarly,  cid:2 A k+1, . . . , A n cid:3  becomes a height A k+1  × width A n  matrix. Multiplying these re- quires a number height A 1  × width A k  × width A n  scalar multiplications. Hence, in total, cost = costLeft + costRight + height A 1  × width A k  × width A n .  subinstances of  The Set of Subinstances Called: The set of the instance  cid:2 A 1, A 2, . . . , A n cid:3  is  cid:2 Ai, Ai+1, . . . , A j cid:3  for every choice of end points 1 ≤ i ≤ j ≤ n. This set of subinstances contains all the subinstances called, because it is closed under the sub operation. Applying the sub operator to an arbitrary subin- stance  cid:2 Ai, Ai+1, . . . , A j cid:3  from this set constructs subinstances  cid:2 Ai, . . . , A k cid:3  and  cid:2 A k+1, . . . , A j cid:3  for i ≤ k < j , which are contained in the stated set of subinstances. Similarly, the set does not contain subinstances not called by the recursive program, because we easily can construct any arbitrary subinstance in the set with the sub operator. For example,  cid:2 A 1, . . . , A n cid:3  sets k = j and calls  cid:2 A 1, . . . , A j cid:3 , which sets k = i + 1 and calls  cid:2 Ai, . . . , A j cid:3 .  Constructing a Table Indexed by Subinstances: The table indexed by the set of subinstances will have a dimension for each of the parameters i and j used to specify a particular subinstance. The tables will be cost[1..n, 1..n] and birdAdvice[1..n, 1..n]. See Figure 19.3.  Order in Which to Fill the Table: The size of a subinstance is the number of ma- trices in it. We will ﬁll the table in this order.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes19 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  18:9  309  Examples of Dynamic Programs   i, j = 2,7    k+1,j = 5,7   A7  A6  A5  A4  A3   i,k = 2,4   A2  j  A1  i  begin  Figure 19.3: The table produced by the dynamic programming solution for Matrix Multiplica- tion. When searching for the optimal bracketing of A 2, . . . , A 7, one of the methods to consider is [A 2, . . . , A 4][A 5, . . . , A 7].  algorithm MatrixMultiplication   cid:2 A 1, A 2, . . . , A n cid:3    cid:2  pre-cond cid:3 : An instance is a sequence of n matrices.  cid:2  post-cond cid:3 : optSol is a bracketing that requires the fewest multiplications, and optCost is the resulting number of multiplications.  % Table: optSol[i, j ] would store an optimal way of bracketing the matrices  cid:2 Ai, Ai+1, . . . , A j cid:3 , but actually we store only the bird’s advice for the subinstance and the cost of its solution. table[1..n, 1..n] birdAdvice, cost % Base cases: The base cases are when there is only one matrix, i.e.,  cid:2 Ai cid:3 . For each, the solution is the empty bracketing with cost zero. for i = 1 to n  % optSol[i, i] = Ai cost[i, i] = 0 birdAdvice[i, i] = ∅  end for  % General cases: Loop over subinstances in the table. for size = 2 to n  for i = 1 to n − size + 1  j = i + size − 1 % Solve instance  cid:2 i, j cid:3  and ﬁll in the table.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes19 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  310  March 29, 2008  18:9  % Try each possible bird answers. for k = i to j − 1  % The bird-and-friend algorithm: The bird gives us the split- ting k, so that the last multiplication multiplies the prod- uct of  cid:2 Ai, Ai+1, . . . , A k cid:3  and that of  cid:2 A k+1, . . . , A j cid:3 . One friend gives us optSol[i, k], an optimal bracketing  cid:2 Ai, . . . , A k cid:3 , and another gives us optSol[k + 1, j ], an optimal bracketing of  cid:2 A k+1, . . . , A j cid:3 . We combine these friends’ and the bird’s infor- mation, obtaining optSolk, which is a best bracketing for  cid:2 i, j cid:3  from amongst those consistent with the bird’s answer. % Get help from friend % optSolk costk = cost[i, k] + cost[k + 1, j ] + height Ai   =  opt Left  optRight  × width A k  × width A j    end for  % Having the best, optSolk, for each bird’s answer k, we keep the best of these best. kmin = a k that minimizes costk % optSol[i, j ] = optSolkmin cost[i, j ] = costkmin birdAdvice[i, j ] = kmin  end for  end for optSol = MatrixMultiplicationWithAdvice  cid:2 A 1, A 2, . . . , A n cid:3 , birdAdvice  return  cid:2 optSol, cost[1, n] cid:3   end algorithm  Constructing an Optimal Solution:  algorithm MatrixMultiplicationWithAdvice  cid:2 Ai, A 2, . . . , A j cid:3 , birdAdvice   cid:2  pre- & post-cond cid:3 : Same as MatrixMultiplication except with advice.  begin  if  i = j   then optSol = Ai return optSol  end if kmin = birdAdvice[i, j ] optLeft = MatrixMultiplicationWithAdvice  cid:2 A 1, . . . , A kmin optRight = MatrixMultiplicationWithAdvice  cid:2 A kmin+1, . . . , A j cid:3 , birdAdvice  optSol =  optLeft  optRight  return optSol   cid:3 , birdAdvice   end algorithm   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes19 CUUS154-Edmonds 978 0 521 84931 9  Examples of Dynamic Programs  March 29, 2008  18:9  Time and Space Requirements: The running time is the number of subinstances times the number of possible bird answers, and the space is the number of subin- stances. The number of subinstances is  cid:1  n2 , and the bird chooses one of  cid:1  n  places to split the sequence of matrices. Hence, the running time is  cid:1  n3 , and the space requirements are  cid:1  n2 .  EXERCISE 19.4.1 Give the steps to ﬁnd a counterexample for the greedy algorithm that multiplies the cheapest pair together ﬁrst. EXERCISE 19.4.2  See solution in Part Five.  Use a picture to make sure that when  cid:2 i, j cid:3  is ﬁlled,  cid:2 i, k cid:3  and  cid:2 k + 1, j cid:3  are already ﬁlled for all i ≤ k < j . Give two other orders that work.  311  19.5 Generalizing the Problem Solved: Best AVL Tree  As discussed in Section 8.3, it is sometimes useful to generalize the problem solved so that you can either give or receive more information from your friend in a recur- sive algorithm. This was demonstrated in Chapter 10 with a recursive algorithm for determining whether or not a tree is an AVL tree. This same idea is useful for dy- namic programming. I will now demonstrate this by giving an algorithm for ﬁnding the best AVL tree. To begin, we will develop an algorithm for the best binary search tree.  The Best Binary Search Tree:  Instances: An instance consists of n probabilities p1, . . . , pn to be associated with the n keys a1 < a2 < ··· < an. The values of the keys themselves do not matter. Hence, we can assume that ai = i. Solutions: A solution for an instance is a binary search tree containing the keys. A binary search tree is a binary tree such that the nodes are labeled with the keys and for each node all the keys in its left subtree are smaller and all those in the right are larger.  Measure of Success: The cost of a solution is the expected depth of a key i∈[1..n] when choosing a key according to the given probabilities, namely [pi · depth of ai in tree]. Goal: Given the keys and the probabilities, the goal is to ﬁnd a binary search tree with minimum expected depth.   cid:3   Expected Depth: The time required to search for a key is proportional to the depth of the key in the binary search tree. Finding the root is fast. Finding a deep leaf takes much longer. The goal is to design the search tree so that the keys that are searched for often are closer to the root. The probabilities p1, . . . , pn, given as part of the input,   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes19 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  18:9  312  Optimization Problems  specify the frequency with which each key is searched for; e.g., p3 = 1 a3 is search for on average one out of every eight times.  8 means that key  One minimizes the depth of a binary search tree by making it completely bal- anced. Having it balanced, however, dictates the location of each key. Although hav- ing the tree partially unbalanced increases its overall height, it may allow for the keys that are searched for often to be placed closer to the top.  We will manage to put some of the nodes close to the root, and others we will not. The standard mathematical way of measuring the overall success of putting  cid:3  more likely keys closer to the top is the expected depth of a key when the key is chosen randomly according to the given probability distribution. It is calculated by i∈[1..n] pi · di, where di is the depth of ai in the search tree. One way to understand this is to suppose that we needed to search for a billion keys. If p3 = 1 8 , then a3 is searched for on average one out of every eight times. Because we are searching for so many keys, it is almost certain that the number of times we search for this key is very close to 1 8 billion. In general, the number of times we search for ai is pi billion. To compute the average depth of these bil- lion searches, we sum their depths and divide by a billion, namely 1 k∈[1..109] [depth of kth search] = 1 109   cid:3  i∈[1..n] pi × 109  · di = cid:3   i∈[1..n] pi · di.   cid:3   109  Bird-and-Friend Algorithm: I am given an instance consisting of n probabilities p1, . . . , pn. I ask the bird which key to put at the root. She answers ak. I ask one friend for the best binary search tree for the keys a1, . . . , ak−1 and its expected depth. I ask another friend for the best tree of the speciﬁed height for ak+1, . . . , an and its expected depth. I build the tree with ak at the root and these as the left and right subtrees.   cid:3  Generalizing the Problem Solved: A set of probabilities p1, . . . , pn deﬁning a i∈[1..n] pi = 1. However, we probability distribution should have the property that will generalize the problem by removing this restriction. This will allow us to ask our friend to solve subinstances that are not ofﬁcially legal. Note that the probabilities given to the friends in the above algorithm do not sum to 1.  The Cost of an Optimal Solution Derived from the Costs for the Subinstances: The expected depth of my tree is computed from that given by my friend as follows.  Cost =  cid:3  =  cid:3  i∈[1..n][pi · depth of ai in tree] + cid:3  i∈[1..k−1][pi ·  depth of ai in left subtree  +1 ] + [pk · 1]  cid:5   cid:5  + pk + Costright + cid:4  cid:3  = Costleft + cid:4  cid:3  i∈[k+1..n][pi ·  depth of ai in right subtree  + 1]  cid:5  + Costright = Costleft + cid:4  cid:3  i∈[1..k−1]pi i∈[1..n]pi = Costleft + Costright + 1  i∈[k+1..n]pi   19.1    19.2    19.3    19.4    19.5    19.6    P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes19 CUUS154-Edmonds 978 0 521 84931 9  Examples of Dynamic Programs  March 29, 2008  18:9  The Complete Set of Subinstances That Will Get Called: The complete set of subinstances is S = { cid:2 ai, . . . , a j ; pi, . . . , pj cid:3   1 ≤ i ≤ j ≤ n}. The table is two- dimensional with size  cid:1  n × n . Running Time: The table has size  cid:1  n × n . The bird can give n different answers. Hence, the time is  cid:1  n3 .  313  We now change the problem so that it is looking for the best AVL search tree.  The Best-AVL-Tree Problem:  Instances: An instance consists of n probabilities p1, . . . , pn to be associated with the n keys a1 < a2 < ··· < an.  Solutions: A solution for an instance is an AVL tree containing the keys. An AVL tree is a binary search tree with the property that every node has a balance factor of −1, 0, or 1. Its balance factor is the difference between the heights of its left and its right subtrees.  cid:3  Measure of Success: The cost of a solution is the expected depth of a key, i∈[1..n][pi · depth of ai in T]. Goal: Given the keys and the probabilities, the goal is to ﬁnd an AVL tree with minimum expected depth.  Cannot Coordinate Friends: We could simply ask friends to build the left and right sub-AVL-trees, but then what would we do if the difference in their heights were greater than one? We cannot expect friends to coordinate their answers.  The New Generalized Problem: An instance consists of the keys, the probabili- ties, and a required height. The goal is to ﬁnd the best AVL tree with the given height.  EXERCISE 19.5.1  See solution in Part Five.   1. What are the possible heights for the left and the right subtrees of an AVL tree of  2. What question would you ask the bird? It is O.K. to ask two questions. What subin-  stance would you give your friend?  3. How would you ensure the balance between the heights of the left and right sub-  height h?  trees?  4. What is the complete set of subinstances that will get called? 5. What is the running time of your algorithm? 6.  In the original problem, the height was not ﬁxed. How would you use the table to solve this problem?   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes19 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  314  March 29, 2008  18:9  19.6 All Pairs Using Matrix Multiplication  There is another dynamic programming algorithm that also ﬁnds the shortest path between every pair of nodes. It is similar in some ways to the Floyd-Warshall–Johnson algorithm, but it is fun because it can be viewed as matrix multiplication.  EXERCISE 19.6.1 Let G =  V, E  be a  directed or undirected  graph, and k ≤ n some integer. Let Mk be a matrix with a both a row and a column for each node in the graph, such that for each pair of nodes u, v ∈ V the element Mk[u, v] gives the number of dis- tinct paths from u to v that contain exactly k edges. Here a path may visit a node more than once. M1[u, v] is one if there is an edge  cid:2 u, v cid:3  and zero otherwise, and M1[u, u] = 1 because there is a path of length zero from u to u. Prove that Mi+j = Mi M j , where × is  standard matrix multiplication, i.e., Mi+j [u, v] = cid:3  ative  on each edge. Redeﬁne  cid:6 Mk[u, v] to give the weight of the shortest path from u EXERCISE 19.6.2 Now let the graph G =  V, E  have weights wu,v  positive or neg- edges. Note that  cid:6 M1[u, v] is the weight of the edge wu,v  or inﬁnity , and  cid:6 M1[u, u] = 0 because there is a path of length zero from u to u. Prove that  cid:6 Mi+j =  cid:6 Mi ×  cid:6 Mj , where × and that + is changed to Min, i.e.,  cid:6 Mi+j [u, v] = Minw[ cid:6 Mi[u, w] +  cid:6 Mj [w, v]]. Compare is standard matrix multiplication except that scalar multiplication is changed to +  to v with the smallest total weight from amongst these paths that contains exactly k  w[Mi[u, w] · Mj [w, v]].  this exercise with Exercise 4.4.3.  EXERCISE 19.6.3 If all the edge weights are positive, then the shortest weighted path  contains at most n − 1 edges. Hence,  cid:6 MN[u, v] for N ≥ n − 1 gives the overall shortest weighted path from u to v. Given  cid:6 M1, what is the fastest way of computing  cid:6 MN for some N ≥ n?  EXERCISE 19.6.4 If there is a path from u to v containing a negative weighted cycle, then this cycle can be repeated inﬁnitely often, giving a path with negative inﬁnite  weight and inﬁnitely many edges. To detect this, compute  cid:6 MN[u, v] and  cid:6 M 2N[u, v] for some large N and see if they are different. The questions is how large N needs to be. You each with positive or negative  cid:2 -bit integer weights, for which  cid:6 Mk[u, v] =  cid:6 M1[u, v] for would think that N = n − 1 would be sufﬁcient, but it is not. Give a graph with n edges, k ∈ [1, N] for some very large N, but  cid:6 M N+1[u, v] is smaller.  EXERCISE 19.6.5 The standard algorithm for standard matrix multiplication takes  cid:1  n3  time. Strassen’s algorithm  Section 9.2  is able to do it in  cid:1  n2.8073  time. Does this same algorithm work for this strange multiplication? The equations x ·  y + z  = x · y + x · z and  x − y  + y = x are true for all real numbers. Are they true on replacing · with + and replacing + with Min?   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes19 CUUS154-Edmonds 978 0 521 84931 9  Examples of Dynamic Programs  March 29, 2008  18:9  19.7 Parsing with Context-Free Grammars  In Chapter 12 we developed an elegant recursive algorithm for parsing a string according to a given context-free grammar that works only for look-ahead-one grammars. We now develop a dynamic programming algorithm that works for any context-free grammar.  Given a grammar G and a string s, the ﬁrst step in parsing is to convert the gram- mar into one in Chomsky normal form, which is deﬁned below. Although a dynamic program could be written to work directly for any context-free grammar, it runs much faster if the grammar is converted ﬁrst.  315  The Parsing Problem:  Instance: An instance consists of  cid:2 G, Tstart, s cid:3 , where G is a grammar in Chom- sky normal form, Tstart is the nonterminal of G designated as the start symbol, and s is the string  cid:2 a1, . . . , an cid:3  of terminal symbols to be generated. The grammar G consists of a set of nonterminal symbols V =  cid:2 T1, . . . , TV cid:3  and a set of rules  cid:2 r1, . . . , rm cid:3 . The deﬁnition of Chomsky normal form is that each rule rq has one of the following three forms:  cid:1  Aq ⇒ BqCq, where Aq, Bq, and Cq are nonterminal symbols.  cid:1  Aq ⇒ bq, where bq is a terminal symbol.  cid:1  Tstart ⇒  cid:3 , where Tstart is the start symbol and  cid:3  is the empty string. This rule may only be used to parse the string s =  cid:3 . It may not be used within the pars- ing of a larger string.  Solution: A solution is a partial parsing P, consisting of a tree. Each internal node of the tree is labeled with a nonterminal symbol; the root is labeled with the spec- iﬁed symbol Tstart. Each internal node must correspond to a rule of the grammar G. For example, for the rule A ⇒ BC, the node is labeled A and its two children are labeled B and C. In a complete parsing, each leaf of the tree is labeled with a terminal symbol. In a partial parsing, some leaves may still be labeled with non- terminals.  Measure of Success: A parsing P is said to generate the string s if the leaves of the parsing in order form s. The cost of P will be one if it generates the string s, and will be zero otherwise. Goal: The goal of the problem is, given an instance  cid:2 G, Tstart, s cid:3 , to ﬁnd a parsing P that generates s.  Not Look Ahead One: The grammar G might not be look-ahead-one. For exam- ple, in  A ⇒ B C A ⇒ D E   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes19 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  March 29, 2008  18:9  you do not know whether to start parsing the string as a B or a D. If you make the wrong choice, you have to back up and repeat the process. However, this problem is a perfect candidate for a dynamic programming algorithm.  316  The Parsing Abstract Data Type: We will use the following abstract data type to represent parsings. Suppose that there is a rule rq = “Aq ⇒ BqCq” that generates Bq and Cq from Aq. Suppose as well that the string s1 =  cid:2 a1, . . . , ak cid:3  is generated starting with the symbol Bq using the parsing P1  Bq is the root of P1  and that s2 =  cid:2 ak+1, . . . , an cid:3  is generated from Cq using P2. Then we say that the string s = s1 ◦ s2 =  cid:2 a1, . . . , an cid:3  is generated from Aq using the parsing P =  cid:2 Aq, P1, P2 cid:3 .  The Number of Parsings: Usually, the ﬁrst algorithmic attempts at parsing are some form of brute force algorithm. The problem is that there are an exponential number of parsings to try. This number can be estimated roughly as follows. When parsing, the string of symbols needs to increase from being of size 1  consisting only of the start symbols  to being of size n  consisting of s . Applying a rule adds only one more symbol to this string. Hence, rules need to be applied n − 1 times. Each time you apply a rule, you have to choose which of the m rules to apply. Hence, the total number of choices may be  cid:1  mn . The Question to Ask the Little Bird: Given an instance  cid:2 G, Tstart, s cid:3 , we will ask the little bird a question that contains two subquestions about a parsing P that gen- erates s from Tstart. The ﬁrst subquestion is the index q of the rule rq = “Tstart ⇒ BqCq” that is applied ﬁrst to our start symbol Tstart. Although this is useful information, I don’t see how it alone could lead to a subinstance. We don’t know P, but we do know that P generates s =  cid:2 a1, . . . , an cid:3 . It follows that, for some k ∈ [1..n], after P applies its ﬁrst rule rq = “Tstart ⇒ BqCq”, it then gen- erates the string s1 =  cid:2 a1, . . . , ak cid:3  from Bq and the string s2 =  cid:2 ak+1, . . . , an cid:3  from Cq, so that overall it generates s = s1 ◦ s2 =  cid:2 a1, . . . , an cid:3 . Our second subquestion asked of the bird is to tell us this k that splits the string s.  Help from Friend: What we do not know about the parsing tree P is how Bq gen- erates s1 =  cid:2 a1, . . . , ak cid:3  and how Cq generates s2 =  cid:2 ak+1, . . . , an cid:3 . Hence, we ask our friends for optimal parsings for the subinstances  cid:2 G, Bq, s1 cid:3  and  cid:2 G, Cq, s2 cid:3 . They respond with the parsings P1 and P2. We conclude that P =  cid:2 Tstart, P1, P2 cid:3  gener- ates s = s1 ◦ s2 =  cid:2 a1, . . . , an cid:3  from Tstart. If either friend gives us a parsing with zero cost, then we know that no parsing consistent with the information provided by the bird is possible. The cost of our parsings in this case is zero as well. This can be achieved by setting the cost of the new parsing to be the minimum of those for P1 and for P2. The line of code will be cost cid:2 q,k cid:3  = min cost[Bq, 1, k], cost[Cq, k + 1, n] .   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes19 CUUS154-Edmonds 978 0 521 84931 9  Examples of Dynamic Programs  March 29, 2008  18:9  The Set of Subinstances: The set of subinstances that get called by the recursive program consisting of you, your friends, and their friends is { cid:2 G, Th, ai, . . . , a j cid:3   h ∈ V, 1 ≤ i ≤ j ≤ n}.  Closed: We know that this set contains all subinstances generated by the recur- sive algorithm because it contains the initial instance and is closed under the sub operation. Consider an arbitrary subinstance  cid:2 G, Th, ai, . . . , a j cid:3  in the set. Its subinstances are  cid:2 G, Bq, ai, . . . , ak cid:3  and  cid:2 G, Cq, ak+1, . . . , a j cid:3 , which are both in the set.  317  Generating: Some of these subinstances will not be generated. However, most of our instances will.  Constructing a Table Indexed by Subinstances: The table will be three-dimen- sional. The solution for the subinstance  cid:2 G, Th, ai, . . . , a j cid:3  will be stored in the entry Table[h, i, j ] for h ∈ V and 1 ≤ i ≤ j ≤ n. See Figure 19.4. The Order in Which to Fill the Table: The size of the subinstance  cid:2 G, Th, ai, . . . , a j cid:3  is the length of the string to be generated, i.e., j − i + 1. We will start with smaller strings and then move to longer and longer strings. Base Cases: One base case is the subinstance  cid:2 G, Tstart,  cid:3  cid:3 . This empty string  cid:3  is parsed with the rule Tstart ⇒  cid:3 , assuming that this is a legal rule. The other base cases are the subinstances  cid:2 G, Aq, bq cid:3 . This string, consisting of the single character bq, is parsed with the rule Aq ⇒ bq, assuming that this is a legal rule.  Constructing an Optimal Solution:  algorithm ParsingWithAdvice  cid:2 G, Th, ai, . . . , a j cid:3 , birdAdvice   cid:2  pre- & post-cond cid:3 : Same as Parsing except with advice. begin cid:2 q, k cid:3  = birdAdvice[h, i, j ]  if i = j   then  else  Rule rq must have the form “Aq ⇒ bq”, where Aq is Th and bq is ai Parsing P =  cid:2 Th, ai cid:3  Rule rq must have the form “Aq ⇒ Cq Bq”, where Aq is Th. P1 = ParsingWithAdvice  cid:2 G, Bq, ai, . . . , ak cid:3 , birdAdvice  P2 = ParsingWithAdvice  cid:2 G, Cq, ak+1, . . . , a j cid:3 , birdAdvice  Parsing P =  cid:2 Th, P1, P2 cid:3   end if return P  end algorithm   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes19 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  March 29, 2008  18:9  318  h   1,i,j    1,i,k   h=1  j  Order to fill   1,1,1   i  : our instance  : subinstances   V,n,n    1,n,n    1,k,j   k   7,n,n    7,k,j   k  h=7  j  k   1,1,1   i   5,n,n   h=5  j   5,i,k    1,1,1   i  Figure 19.4: The dynamic programming table for parsing. The table entry corresponding to the instance  cid:2 G, T1, ai, . . . , a j cid:3  is represented by the little circle. Using the rule T1 ⇒ T1T1, the subinstances  cid:2 G, T1, ai, . . . , ak cid:3  and  cid:2 G, T1, ak+1, . . . , a j cid:3  are formed. Using the rule T1 ⇒ T5T7, the subinstances  cid:2 G, T5, ai, . . . , ak cid:3  and  cid:2 G, T7, ak+1, . . . , a j cid:3  are formed. The table entries corre- sponding to these subinstances are represented by the dots within the ovals.  EXERCISE 19.7.1  See solution in Part Five.   a  Give the code for the parsing algo- rithm.  b  Give the running time for this algorithm.  19.8 Designing Dynamic Programming Algorithms via Reductions  Sometimes, when trying to develop an algorithm for a new problem, it is easier to look for the similarities between your new problem and a problem that you already have an algorithm for. With these insights you can make the new algorithm similar to the old. When done formally, this is called doing a reduction from the one problem to the another. Chapter 20 covers these ideas in depth. Here we will be looking for similarities between problems more informally. We will start by using this technique to developing a dynamic programming algorithm for a harder version of the event scheduling problem.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes19 CUUS154-Edmonds 978 0 521 84931 9  Examples of Dynamic Programs  March 29, 2008  18:9  319  Event Scheduling Problem: The problem is to schedule a tour trying to attend the greatest worth of events possible. Unlike the version from Section 19.3, the events occur at different locations. An instance is  cid:2 E, d cid:3 . Here E = {E1, E2, . . . , En} is a set of n events. For j ∈ [n], event E j is speciﬁed by  cid:2 s j , fj , w j cid:3 , where s j its start time, fj its ﬁnishing time, and w j its worth in dollars. The events occur at different lo- cations. For each pair of events E j and E j  cid:1 , dj, j  cid:1  is the time required to travel be- tween the locations of these events. Note that the second parameter d is an n-by-n matrix. A solution is a schedule of events to attend. This includes which events you at- tend and the order that you attend them,  e.g., S =  cid:2 E5, E32, E16, . . . , E21 cid:3 . The restric- tion is that to attend an event, you must get to the location of the event before it starts and you must stay until it completes. More formally, suppose after you attend event E j , you next attend event E j  cid:1 . Now E j ﬁnishes at time fj , and E j  cid:1  starts at time s j  cid:1 . Hence, you have s j  cid:1  − fj time to travel the distance dj, j  cid:1  between them. This re- quires dj, j  cid:1  ≤ s j  cid:1  − fj .  Note that we assume that the distances dj, j  cid:1  meet the triangle inequality, so that if from event E j you can reach E j  cid:1  and from event E j  cid:1  you can reach E j  cid:1  cid:1 , then by transitivity from event E j you can reach E j  cid:1  cid:1 .   The worth of a solution is the total of the worths w j of the events attended. The  goal is to maximize the worth of the schedule.  Dynamic Programming: Start by attempting to design a dynamic programming al- gorithm for this problem. Do not be surprised if you ﬁnd it hard. This is why we are going to compare this problem with a previously known problem.  Similarity to Best Path: The solution to this problem is a schedule of events, which effectively is a path through a subset of the events. We have looked at many algo- rithms for ﬁnding a best path within graphs, so let us try to model this problem with a graph.  Reduction: Given an instance of  cid:2 E, d cid:3  of the scheduling problem, we solve it as follows. We ﬁrst map it to an instance of the graph problem. Our graph algorithm ﬁnds the best path with in this graph.Then we map this best path to a solution of the scheduling problem.  Forming a Graph Instance: We now consider how to form a graph representing a given set of events.  Nodes: A solution to the scheduling problem is a path of events, and to the graph problem is a path of nodes. This indicates a link between events and nodes. Hence, in the graph, we construct a node for each event ej .   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes19 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  320  March 29, 2008  18:9  Edges: In the graph problem, whether or not there is an edge between two nodes indicates whether or not the path can travel from the one node to the other. Hence, for each pair of events ej and ej  cid:1 , we add the directed edge  cid:2 ej , ej  cid:1  cid:3  if ej can proceed ej  cid:1  in the schedule. More formally,  cid:2 ej , ej  cid:1  cid:3  is an edge if and only if dj, j  cid:1  ≤ s j  cid:1  − fj . So far the correspondence between these problems is good, be- cause every path through this graph corresponds to a legal schedule of events and vice versa.  Nodes s and t: The standard problems for ﬁnding paths in a graph assume that the input speciﬁes a start node s and a ﬁnishing node t, whereas the scheduling algorithm does not specify a ﬁrst or last event to attend. The standard way to get around this problem is to simply add an extra start event es and ﬁnal event et . Giving event es ﬁnishing time fs = −∞ means that imposing the constraint that the schedule starts with es does not affect which schedules are legal, because one can get from event es to any other event. Giving event es worth ws = 0 means that including event s does not change the worth of the ﬁnal solution. Similarly, let st = ∞ and wt = 0. From the graph perspective, we add two new nodes s and t and a directed edge from s to every other node and from every node to t.  Costs and Weights: The worth or cost of a path is also different in the two prob- lems. The worth of a schedule is the sum of the worths of the events, which be- comes the sum of the weights of the nodes, while the worth or cost of a path through a graph tends to be deﬁned as the sum of the weights of the edges. This difference, however, should not be too signiﬁcant. One option is to go back to the graph path algorithms and try to get them to work where the nodes and not the edges have weights. Another option is to simply shift the weight of each event onto the outgoing edges. The edges  cid:2 ei, ej cid:3  and  cid:2 ei, t cid:3  will have the value vi of the ﬁrst event ei. The edges  cid:2 s, ej cid:3  will have the value zero. Minimize or Maximize: Another difference between these problems is that the scheduling problem is looking for the schedule of maximum value, whereas the graph problem is looking for the path of minimum value. It turns out that ﬁnding the maximum weighted path in a general directed graph is a hard problem. This motivates looking at other structures related to the graph that we know.  The Longest Weighted Path within a Directed Level Graph: Recall the dynamic programming algorithm for ﬁnding the shortest weighted path within a directed level graph that is given in Sections 18.1 and 18.2. Here the nodes can be leveled so that all the edges go forward. In turns out that minimal changes are needed to this algorithm so that it instead ﬁnds the longest weighted path within a directed level graph, i.e., solves the same problem but ﬁnds a path from s to t with the largest weight. When taking the best of the best, we simply take the max and not the min path. Because there are no cycles, we don’t have to worry about paths that cycle in order to have longer weight. Hence, the proof of correctness goes through just as it did before.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes19 CUUS154-Edmonds 978 0 521 84931 9  Examples of Dynamic Programs  March 29, 2008  18:9  Leveled: In order to use this leveled graph algorithm, we need to make sure that the graph that arises from the scheduling problem is in fact leveled. The level of a node ej can be the start time s j of the corresponding event. The rules for when edges are added ensure that each edge is directed from some node to a node in a lower level.  321  Mapping Back the Algorithm: Given this similarity between these two problems, let us recall the dynamic programming algorithm given in Section 18.2 for ﬁnding the shortest weighted path within a directed level graph, and let us use it to ﬁnd a dynamic programming algorithm for the event scheduling problem.  Set of Instances: A key part of a dynamic programming algorithm is the set of subinstances solved. Starting with the instance  cid:2 G, s, t cid:3  and looking for the short- est path from s to t, the algorithm ﬁnds the shortest path from s to each node vi, so that the complete set of subinstances solved by the dynamic programming algorithm will be { cid:2 G, s, vi cid:3 vi}. Similarly, for the scheduling problem, given the instance  cid:2 E, d cid:3 , the set of subinstances will be { cid:2 E, d, i cid:3   ei}, where the instance  cid:2 E, d, i cid:3  asks for the best schedule of events S =  cid:2 e5, e32, e16, . . . , e21, ei cid:3  that ends in event ei. Note that there may be events that occur after event ei, but the solu- tion schedule is not allowed to attend them. The Bird–Friend Algorithm: Given an instance  cid:2 E, d, i cid:3 , we know that the second to last event before ei in the optimal solution must be an event ek for which dk,i ≤ si − fk. We ask the bird to tell us this k. We get the friend to solve the instance  cid:2 E, d, k cid:3 . Our solution is the same with ei added on the end. The value of our solution is the same with vk added in.  The Code: The ﬁnal code to solve the event scheduling problem is almost iden- tical to that for ﬁnding the shortest weighted path within a directed level graph.  algorithm Schedule  E, d   cid:2  pre-cond cid:3 : An instance consists of a set of events E = {ej}, with start time s j , ﬁnishing time fj , worth w j , and distances dj, j  cid:1  between them.  cid:2  post-cond cid:3 : optSol is an optimal valid schedule of events that ends with event en.  begin  Add an imaginary start event s = e0 with s0 = f0 = −∞. Add an imaginary ﬁnishing event s = en+1 with sn+1 = fn+1 = ∞. Sort the events by starting time. % Table: optSol[i] stores an optimal schedule of events ending with ei and costSol[i] its cost. table[0..n + 1] optCost, birdAdvice   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes19 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  322  March 29, 2008  18:9  % Base case: The only base case is for the optimal set ending in e0. Its solution consists of the empty set with cost zero. % optSol[0] = ∅ optCost[0] = 0 birdAdvice[0] = ∅ % General cases: Loop over subinstances in the table. for i = 1 to n + 1 % Solve instance  cid:2 E, d, i cid:3 , and ﬁll in table entry  cid:2 i cid:3 . % Try each possible bird answer. for each k for which dk,i ≤ si − fk  % The bird–friend algorithm: The last event must be ei. The bird tells us that the second to last event is ek. We ask the friend for an optimal set ending in ek. He gives us optSol[k], which he has stored in the table. To this we add ei. This gives us optSolk which is a best solution ending in ei from amongst those paths consis- tent with the bird’s answer. = optSol[k] + ei % optSolk = optCost[k] + vi optCostk  end for % Having the best, optSolk, for each bird’s answer k, we keep the best of these best. kmin = a k that maximizes optCostk % optSol[i] = optSolkmax optCost[i] = optCostkmax birdAdvice[i] = kmax  end for optSol = SchedulingWithAdvice  cid:2 E, d cid:3 , birdAdvice  return  cid:2 optSol, optCost[n + 1] cid:3   end algorithm  Running Time: The number of subinstances is n + 1, and the number of bird an- swers is at most n + 1. Hence, the running time is O n2 .  Bigger-Is-Smarter Elephant Problem: We will now consider another problem.  Instances: A set of elephants E = {e1, e2, ..., en}, where ei =  cid:2 wi, si, vi cid:3  represents the ith elephant, wi its weight, si its intelligence, and vi its value.  To make life easier, assume that the weights and intelligences are unique values, i.e., wi  cid:4 = w j and si  cid:4 = s j .  Solutions: A subset of elephants S ⊆ E for which bigger is smarter.  Formally, ∀i, j ∈ S, [[wi < w j ] iff [si < s j ]]. An equivalent way of looking at it is that if you were to sort the elephants in S in increasing order of their weight,   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes19 CUUS154-Edmonds 978 0 521 84931 9  Examples of Dynamic Programs  March 29, 2008  18:9  then this same order would sort them with respect to intelligence.  Hint: It is useful to assume that the elephants in the solution are sorted in this way.   Measure of Success: The cost of the solution is the sum of the values of the ele- phants,  i∈S vi.   cid:3   Goal: We should ﬁnd a maximum-valued solution.  323  EXERCISE 19.8.1 Design a dynamic programming algorithm for the bigger-is-smarter elephant problem by comparing it, as done previously, with the problem of ﬁnding the longest weighted path within a directed level graph problem.  EXERCISE 19.8.2  See solution in Part Five.  Design a dynamic programming algo- rithm for the bigger-is-smarter elephant problem by comparing it with the longest- common-subsequence problem given in Section 19.1. To do this the LCS problem needs to be generalized to have weights on the letters.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes20 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  14:8  324 20 Reductions and NP-Completeness  A giraffe with its long neck is a very different beast than a mouse, which is differ- ent than a snake. However, Darwin and gang observed that the ﬁrst two have some key similarities, both being social, nursing their young, and having hair. The third is completely different in these ways. Studying similarities and differences between things can reveal subtle and deep understandings of their underlining nature that would not have been noticed by studying them one at a time. Sometimes things that at ﬁrst appear to be completely different, when viewed in another way, turn out to be the same except for superﬁcial, cosmetic differences. This section will teach how to use reductions to discover these similarities between different optimization problems. Reduction P1 ≤poly P2: We say that we can reduce problem P1 to problem P2 if we can write a polynomial-time  n cid:1  1   algorithm for P1 using a supposed algorithm for P2 as a subroutine.  Note we may or may not actually have an algorithm for P2.  The standard notation for this is P1 ≤poly P2.  Why Reduce? A reduction lets us compare the time complexities and underlying structures of the two problems. Reduction is useful in providing algorithms for new problems  upper bounds , for giving evidence that there are no fast algorithms for certain problems  lower bounds , and for classifying problems according to their dif- ﬁculty. Upper Bounds: From the reduction P1 ≤poly P2 alone, we cannot conclude that there is a polynomial-time algorithm for P1. But it does tell us that if there is a polynomial-time algorithm for P2, then there is one for P1. This is useful in two ways. First, it allows us to construct algorithms for new problems from known algorithms for other problems. Moreover, it tells us that P1 is at least as easy as P2.  Hot Dogs ≤poly Linear Programming: Section 15.4 describes how to solve the problem of making a cheap hot dog using an algorithm for linear programming.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes20 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  14:8  Reductions and NP-Completeness  Bipartite Matching ≤poly Network Flows: We will develop an algorithm for bipar- tite matching in Section 20.4 that uses the network ﬂow algorithm.  Lower Bounds: The contrapositive of the last statement is that if there is not a polynomial-time algorithm for P1, then there cannot be one for P2  otherwise there would be one for P1.  This tells us that P2 is at least as hard as P1.  325   Any Optimization Problem  ≤poly CIR-SAT: This small-looking statement, pro- ved by Steve Cook in 1971, has become one of the foundations of theoreti- cal computer science. There are many interesting optimization problems. Some people have worked hard on discovering fast algorithms for this one and others have done the same for that one. Cook’s theorem shows that it is sufﬁcient to focus on the optimization problem CIR-SAT, because if you can solve it quickly, then you can solve them all quickly. However, after many years of working hard, people have given up and strongly suspect that at least one optimization prob- lem is hard. This gives strong evidence that CIR-SAT is hard. Cook’s theorem is proved  and the problems deﬁned  in Section 20.1. CIR-SAT ≤poly 3-COL: See Section 20.3. This states that the optimization prob- lem 3-COL is as hard as CIR-SAT, already known to be hard problem. This gives evidence that 3-COL is also hard. Moreover, reductions are transitive, meaning that P1 ≤poly P2 and P2 ≤poly P3 automatically gives that P1 ≤poly P3. Hence, to- gether these last two statements give that  any optimization problem  ≤poly 3- COL. 3-COL ≤poly Course Scheduling, 3-COL ≤poly Independent Set, 3-COL ≤poly 3-SAT: These give evidence that course scheduling, independent set, and 3-SAT are hard. See Sections 20.2 and 20.3.  Halting Problem  ≤poly  What Does This Turing Machine Do : It can be proved that the halting problem  given a Turing machine M and an input I , does the M halt on I ?  is undecidable  no algorithm can always answer it correctly in ﬁnite time . Given this, reductions can be used to prove that almost any problem asking what the computation of a given Turing machine does is also undecidable.  Reverse Reductions: Knowing P1 ≤poly P2 and knowing that there is not a polynomial-time algorithm for P2 does not tell us anything about the whether there is a polynomial-time algorithm for P1. Though it does tell us that the algorithm for P1 given in the reduction does not work, there well may be another, completely dif- ferent algorithm for P1. Similarly, knowing that there is a polynomial-time algorithm for P1 does not tell us anything about whether there is one for P2. To reach these two conclusions, you must prove the reverse reduction P2 ≤poly P1.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes20 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  14:8  326  Optimization Problems  Classifying Problems: Reductions are used to classify problems.  The Same Problem Except for Superﬁcial Differences: More than just being able to compare their time complexities, knowing P1 ≤poly P2 and P2 ≤poly P1 reveals that the two problems are somehow fundamentally the same problem, asking the same types of questions. Sometimes this similarity is quite superﬁcial. They sim- ply use different vocabulary. However, at other times this connection between the problems is quite surprising, providing a deeper understanding of each of the problems. One way in which we can make a reduction even more striking is by restricting the algorithm for the one to call the algorithm for the other only once. Then the mapping between them is even more direct.  NP-Completeness: We have shown that the optimization problems CIR-SAT, 3- COL, course scheduling, independent set, and 3-SAT are all reducible to each other and in that sense are all fundamentally the same problem. In fact, there are thousands of very different problems that are equivalent to these. These prob- lems are said to be NP-complete. We discuss this more in Section 20.2.  Halting-Problem Completeness: Another important class deﬁned in this way consists of all problems that are equivalent to the halting problem.  20.1 Satisﬁability Is at Least as Hard as Any Optimization Problem  In Chapter 13 we saw that optimization problems involve searching through the ex- ponential set of solutions for an instance to ﬁnd one with optimal cost. Though there are quick  i.e., polynomial  algorithms for some of these problems, for most of them the best known algorithms require 2 cid:1  n  time on the worst case input instances, and it is strongly believed that there are no polynomial-time algorithms for them. The main reason for this belief is that many smart people have devoted many years of research to looking for fast algorithms and have not found them. This section uses reductions to prove that some of these optimization problems are universally hard, or complete, among the class of optimization problems, because if you could design an algorithm to solve such a problem quickly, then you could translate this algorithm into one that solves any optimization problem quickly. Conversely  and more likely , if there is even one optimization problem that cannot be solved quickly, then none of these complete problems can be either. Proving in this way that a problem that your boss wants you to solve is hard is useful, because you will know not to spend much time trying to design an all-purpose algorithm for it.   Any Optimization Problem  ≤poly CIR-SAT: This reduction will prove the satisﬁ- ability problem is complete for the class of optimization problems, meaning that it is universally hard for this class.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes20 CUUS154-Edmonds 978 0 521 84931 9  Reductions and NP-Completeness  March 29, 2008  14:8  The Circuit Satisﬁability Problem: This famous computational problem, CIR- SAT, requires one to ﬁnd a satisfying assignment for a given circuit. Section 17.4 gives a recursive backtracking algorithm for the satisﬁability problem, but in the worst case its running time is 2 cid:1  n .  Circuit: A circuit can be either a useful notation for describing an algorithm in detail or a practical thing built in silicon in your computer.  327  y  x  x  y  NOT  NOT  AND  Construction: It is built with AND, OR, and NOT gates. At the top are n wires labeled with the binary variables x1, x2, . . . , xn. To specify the circuit’s input, each of these will take on either 1 or 0, true or false, 5 volts or 0 volts. Each AND gate has two wires coming into it, either from an input xi or from the output of another gate. An AND gate outputs true if both of its inputs are true. Similarly, each OR gate outputs true if at least one of its inputs is true, and each NOT gate outputs true if its single input is false. We will only consider circuits that have no cycles, so these true and false values percolate down to the output wires. There will be a single output wire if the circuit computes a true–false function of its input x1, x2, . . . , xn and will have m output wires if it outputs an m-bit string, which can be used to encode some required information.  A circuit for x     y  AND  OR  ⊕  Binary Endoding: The function f that you want to compute may take as in- put some abstract object like a graph G and return another abstract object, like a path through this graph; however, whether the computation of f is done by a Java program, a Turing machine, or a circuit, these abstract objects ﬁrst need to be encoded into strings of zeros and ones. Compute Any Function: Given any function f : {0, 1}n → {0, 1}m, a circuit can compute it with a most O nm · 2n  gates as follows. For any ﬁxed input instance  cid:4 x1, x2, . . . , xn cid:5  =  cid:4 1, 0, . . . , 1 cid:5 , a circuit can say “the input is this ﬁxed instance” simply by computing [ x1 = 1  AND NOT x2 = 1  AND . . . AND  xn = 1 ]. Then the circuit computes the ith bit of the function’s output by outputting 1 if [“the input is this ﬁxed instance” OR “this instance” OR . . . OR “this instance”], where each instance is listed for which the ith bit of the function’s output is 1.  Polynomial Size for Polynomial Time: More importantly, given any algo- rithm whose Turing machine’s running time is T n  and given any ﬁxed inte- ger n, there is an easily constructed circuit with at most  cid:1  T n 2  gates that computes the output of the algorithm given any n-bit input instance. Change the deﬁnition of a Turing machine slightly so that each cell is big enough so that the cell currently being pointed to by the head can store not only its contents but also the current state of the machine. This cell’s contents can   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes20 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  328  March 29, 2008  14:8  be encoded with  cid:1  1  bits. Because the Turing machine uses only T n  time, it can use at most the ﬁrst T n  cells of memory. For each of the T n  steps of the algorithm, the circuit will have a row of  cid:1  1  · T n  wires whose values encode the contents of memory of these T n  cells during this time step. The gates of the circuit between these rows of wires compute the next contents of memory from the current contents. Because the contents of cell i at time t depend only on the contents of cells i − 1, i, and i + 1 at time t − 1 and each of these is only a  cid:1  1  number of bits, this dependence can computed us- ing a circuit with  cid:1  1  gates. This is repeated in a matrix of T n  time steps and T n  cells, for a total of  cid:1  T n 2  gates. At the bottom, the circuit com- putes the output of the function from the contents of memory of the Turing machine at time T n .  Circuit Satisﬁability Speciﬁcation: The CIR-SAT problem takes as input a circuit with a single true–false output and returns an assignment to the vari- ables x1, x2, . . . , xn for which the circuit gives true, if such an assignment exists.  Optimization Problems: This reduction will select a generic optimization problem and show that CIR-SAT is at least as hard as it is. To do this, we need to have a clear deﬁnition of what a generic optimization problem looks like.  Deﬁnition: Each such problem has a set of instances that might be given as in- put; each instance has a set of potential solutions, some of which are valid; and each solution has a cost. The goal, given an instance, is to ﬁnd one of its valid solutions with optimal cost. An important feature of an optimization problem is that there are polynomial-time algorithms for the following:  Valid I, S : Given an instance I and a potential solution S, there is an algo- rithm Valid I, S  running in timeIO 1  that determines if I is a valid instance for the optimization problem and that S is a valid solution for I .  Cost S : Given a valid solution S, there is an algorithm Cost S  running in time IO 1  that computes the cost of the solution S.  Example: Course Scheduling: Given the set of courses requested by each student and the set of time slots available, ﬁnd a schedule that minimizes the number of conﬂicts.  I: The set of courses requested by each student and the set of time slots available  S: A schedule speciﬁes at which time start each course will be taught.  Valid I, S : An algorithm that returns whether the schedule S allocates each course requested in I to exactly one time slot provided in I.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes20 CUUS154-Edmonds 978 0 521 84931 9  Reductions and NP-Completeness  March 29, 2008  14:8  Cost S : A conﬂict occurs in the schedule when two courses requested by the same student are scheduled at the same time. Cost  S  is an algorithm that returns the number of conﬂicts in the schedule S.  Alg for the Optimization Problem: Given a fast algorithm AlgCIR-SAT for CIR-SAT and the descriptions Valid I, S  and Cost S  of an optimization problem P we will now design a fast algorithm AlgP for the optimization problem and use it to prove that the problem CIR-SAT is at least as hard as the optimization problem, i.e. that P ≤poly CIR-SAT.  329  Binary Search for Cost: Given some instance IP of the optimization problem, AlgP’s ﬁrst task is to determine the cost copt of the optimal solution for I . AlgP starts by determining whether or not there is a valid solution for I that has cost at least c = 1. If it does, AlgP repeats this with c = 2, 4, 8, 16, . . . . If it does not, AlgP tries c = 0, −1, −2, −4, . . . , until it ﬁnds c1 and c2 between which it knows the cost of an optimal solution lies. Then it does binary search to ﬁnd copt. The last step is to ﬁnd a solution for I that has this optimal cost.  Finding a Solution with Given Cost: AlgP determines whether I has a solution S with cost at least c or ﬁnds a solution with cost copt, as follows. AlgP will con- struct a circuit C and calls the algorithm AlgCIR-SAT, which provides a satisfying assignment to C. AlgP wants the satisfying assignment that AlgCIR-SAT provides to be the solution S that it needs. Hence, AlgP designs C to be satisﬁed by the assignment S only if S is a solution S for I with a cost as required, i.e., C S  ≡ [Valid I, S  and Cost S  ≥ c]. Because there are polynomial-time algorithms for Valid I, S , for Cost S , and for ≥, the algorithm AlgP can easily construct such a circuit C S . If such a solution S satisfying C exists, then AlgCIR-SAT kindly provides one.  This completes the reduction P ≤poly CIR-SAT of any Optimization problem to CIR- SAT.  EXERCISE 20.1.1 For each of the following problems, deﬁne I , S, Valid I, S , and Cost S .  1. Graph coloring: Given a graph, color its nodes so that two nodes do not have the  2.  same color if they have an edge between them. Use as few colors as possible. Independent set: Given a graph, ﬁnd a largest subset of the nodes for which there are no edges between any pair in the set.  3. Airplane: Given the requirements of an airplane, design it, optimizing its perfor-  mance.  proﬁts.  4. Business: Given a description of a business, make a business plan to maximize its 5. Factoring: Given an integer, factor it, e.g., 6 = 2 × 3. 6. Cryptography: Given an encrypted message, decode it.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes20 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  March 29, 2008  14:8  20.2 Steps to Prove NP-Completeness  In this section we deﬁne the class NP and give steps for proving that a computational problem is NP-complete.  330  Completeness for Nondeterministic Polynomial-Time Decision Problems: The set of computational problems that are complete  universally hard  for op- timization problems is extremely rich and varied. Studying them has become a fascinating ﬁeld of research.  NP Decision Problems: Theoretical computer scientists generally only consider a subclass of the optimization problems, referred to as the class of nondetermin- istic polynomial time problems  NP .  One Level of Cost: Instead of worrying about whether one solution has a bet- ter cost than another, we will completely drop the notion of the cost of a so- lution. S will only be considered to be a valid solution for the instance I if it is a solution with a sufﬁciently good cost. This is not a big restriction, be- cause if you want to consider solutions with different costs, you can always do binary search, as already done for the cost of the optimal solution.  Decision Problem: Given an instance to the problem, the goal is to deter- mine either yes I does have a valid solution or no it does not.  Witness: A solution for an instance is often referred to as a witness, because, though it may take exponential time to ﬁnd it, if it were provided by a  non- deterministic  fairy godmother, then it could be used in polynomial time to witness the fact that the answer for this instance is yes. In this respect, NP problems are asymmetrical in that there does not seem to be a witness that quickly proves that an instance does not have a solution.  Formal Deﬁnition: We say that such a computational problem P is in the class of nondeterministic polynomial-time problems  NP  if there is a polynomial-time algorithm Valid I, S  that speciﬁes Yes when S is a  sufﬁ- ciently good  solution for the instance I , and No if not. More formally, P can be deﬁned as follows:  P I   ≡ [∃S, Valid I, S ]  Examples:  Circuit Satisﬁability  CIR-SAT : Circuit satisﬁability could be deﬁned as a decision problem: Given a circuit, determine whether there is an as- signment that satisﬁes it.  Graph 3-Coloring  3-COL : Given a graph, determine whether its nodes can be colored with three colors so that two nodes do not have the same color if they have an edge between them.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes20 CUUS154-Edmonds 978 0 521 84931 9  Reductions and NP-Completeness  March 29, 2008  14:8  Course Scheduling: Given the set of courses requested by each student, the set of time slots available, and an integer K , determine whether there is schedule with at most K conﬂicts.  Cook vs. Karp Reductions: Stephen Cook ﬁrst proved that CIR-SAT is complete for the class of NP problems. His deﬁnition of a reduction P1 ≤poly P2 is that one can write an algorithm Alg1 for the problem P1 using an algorithm Alg2 for the problem P2 as a subroutine. In general, this algorithm Alg1 may call Alg2 as many times as it likes and do anything it likes with the answers that it receives. Richard Karp later observed that when the problems P1 and P2 are sufﬁciently similar, the algorithm Alg1 used in the reduction need only call Alg1 once and answers Yes if and only if Alg1 answers Yes. These two deﬁnitions of reductions are referred to as Cook and Karp reductions. Though we have deﬁned Cook reductions because they are more natural, we will consider only Karp reductions from here on.  331  NP-Completeness: For the problem P to be NP-complete, it has to be hard, but not too hard. We say that a computational problem P is NP-complete if  cid:2  ≤poly P : We say a problem P is NP-Hard if it is as Sufﬁciently Hard, P hard as every other problem P in the class NP. Intuitively, this means that if one did ﬁnd a quick algorithm for P then this algorithm could be translated into quick algorithms for each NP problem P . More formally, it means that every language in NP can be polynomially reduced to it using a Karp reduction.   cid:1    cid:1   ∀ optimization problems P   cid:1   , P   cid:1  ≤poly P.  To prove this, it is sufﬁcient to prove that our computational problem is at least as hard as some problem already known to be NP-complete. For example, because we now know that CIR-SAT is NP-complete, it is sufﬁcient to prove that CIR-SAT ≤poly P. Not Too Hard, P ∈ NP: On the other hand, for the problem P to be com- plete for the class NP, it has to be sufﬁciently easy that it is itself in the class. For example, the Halting problem is sufﬁciently hard to be NP- hard, but is far to hard to be in NP.  The Steps to Prove NP-Completeness: Proving that a problem P is NP-complete can be a bit of an art, but once you get the hang of it, it can be fun. I will now care- fully lay out the steps needed.  After step 3, we will use Poracle to denote the prob- lem instead of P, because at that point we will be assuming that we have an oracle for it.   Running Example: Course Scheduling is NP-Complete: The problem P that we will prove is NP-complete will be the course scheduling problem.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes20 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  14:8  332  Optimization Problems  0  P ∈ NP: As said, in order for the problem P to be NP-complete, it needs to be sufﬁciently easy to be in NP. To prove this we effectively need to provide a non- deterministic polynomial time for it. This is accomplished by by providing polynomial-time algorithm Valid I, H  that speciﬁes whether S is a valid solution for instance I .  Course Scheduling: It is not hard to determine in polynomial time whether the instance I and the solution S are properly deﬁned and to check that within this schedule S, the number of times that a student wants to take two courses that are offered at the same time is at most K .  1  What to Reduce to It: An important and challenging step in proving that a prob- lem is NP-complete is deciding which NP-complete problem to reduce to it. We will denote this problem with Palg because later we will be designing an algorithm for it.  3-COL ≤poly Course Scheduling: We will reduce 3-COL to course scheduling, that is, we will prove the reduction 3-COL ≤poly course scheduling. I will save the proof that 3-COL is NP-complete for our next example, because it is much harder.  Hint: You want to choose a problem that is “similar” in nature to yours. In order to have more to choose from, it helps to know a large collection of problems that are NP-complete. There are entire books devoted to this. When in doubt, 3-SAT and 3-COL are good problems to use.  2  What is What: It is important to remember what everything is.  3-COL ≤poly Course Scheduling:   cid:1  Palg = 3COL is the graph 3-coloring problem.  cid:1  Ialg = Igraph, an instance of it, is an undirected graph.  cid:1  Salg = Scoloring, a potential solution, is a coloring of each of its nodes with either red, blue, or green. It is a valid solution if no edge has two nodes with the same color.   cid:1  Algalg is an algorithm that takes the graph Igraph as input and determines  whether it has a valid coloring.  cid:1  Poracle = course scheduling.  cid:1  Ioracle = Icourses, an instance of it, is the set of courses requested by each stu- dent, the set of time slots available, and the integer K.  cid:1  Soracle = Sschedule, a potential solution, is a schedule assigning courses to time slots. It is a valid solution if it has at most K conﬂicts.   cid:1  Algoracle is an algorithm that takes Icourses as input and determines whether it  has a valid schedule.  Such instances may or may not be satisﬁable and such potential solutions may or may not be valid.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes20 CUUS154-Edmonds 978 0 521 84931 9  Reductions and NP-Completeness  March 29, 2008  14:8  333  Warning: Be especially careful about what is an instance and what is a solution for each of the two problems.  3  Direction of Reduction and Code: Another common source of mistakes is doing the reduction in the wrong direction. I recommend not memorizing this direction, but working it out each time. Our goal is to prove that the problem P sufﬁciently hard to be NP-complete. Hence, you must put P on the hard side of the inequal- ity 3-COL ≤poly P with the problem, say 3-COL, chosen in step 1 on the easy side. At this point, turn your thinking around. Instead of proving P is relatively hard, we will prove that the problem 3-COL is relatively easy. To do this, we must designing a fast algorithm Algalg for it. Because this is are goal from here on, we will denote the problem 3-COL with Palg. Our belief is that there is not a fast algorithm for this problem. Hence, to help us we will use a supposed fast algorithm Algoracle for P as a subroutine. Typically for reductions people assume that Algoracle is an oracle mean- ing that it solves its problem in one time step. Hence, from here on we will use Poracle to denote the problem instead of P. The code for our algorithm for Palg will be as follows.  algorithm Algalg Ialg   cid:4  pre-cond cid:5 : Ialg is an instance of Palg.  cid:4  post-cond cid:5 : Determine whether Ialg has a solution Salg, and if so, return it. begin  Ioracle = InstanceMap Ialg  % Ioracle is an instance of Poracle  cid:4 ansoracle, Soracle cid:5  = Algoracle Ioracle  % If there is one, Soracle is solution for Ioracle if  ansoracle = Yes  then  ansalg = Yes Salg = SolutionMap Soracle  ansalg = No Salg = nil  else  end if return  cid:4 ansalg, Salg cid:5    end algorithm  4  Look for Similarities: Though the problems Palg and Poracle may appear to be very different, the goal in this step is to look for underlying similarities. Compare how their solutions, Salg and Soracle, are formed out of their instances, Ialg and Ioracle. Generally, the instances can be thought of as sets of elements with a set of constraints between them. Can you view their solutions as subsets of these elements or as labelings of them? What allows a solution to form, and what constrains how it is formed? Can   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes20 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  334  March 29, 2008  14:8  you talk about the two problems using the same language? For example, a subset of elements can be viewed as a labeling of the elements with zero and one. Similarly, a labeling of each element e with  cid:2 e ∈ [1..L] can be viewed as a subset of the pairs  cid:4 e,  cid:2 e cid:5 .  I  easier  English  I harder  3-COL ≤poly Course Scheduling: A solution Scoloring is a coloring that assigns a color to each node. A solution Sschedule is a sched- ule that assigns a time slot to each course. This similarity makes it clear that there is a similarity between the roles of the nodes of Igraph and of the courses of Icourses and between the colors of Scoloring and the time slots of Sschedule. Each coloring conﬂict arises from an edge between nodes, and each scheduling conﬂict arises from a student wanting two courses. This similar- ity makes it clear that there is a similarity between the roles of the edges of Igraph and of the course requests of Icourses.  S harder  Mon. 2 pm   Mon. 2 pm  Tue. 3 pm  Science  easier  green  Math  blue  blue  S  5  InstanceMap: You must deﬁne a polynomial-time algorithm InstanceMap Ialg  that, given an instance Ialg of Palg, constructs an instance Ioracle of Poracle that has similar sorts of solutions. The main issue is that the constructed instance Ioracle has a solution if and only if the given instance Ialg has a solution, that is, Yes instances, get mapped to Yes, instances and No to No.  3-COL ≤poly Course Scheduling: Given a graph Igraph to be colored, we design an instance Icourses = InstanceMap Igraph  to be scheduled. Using the similarities observed in step 4, our mapping we will have one course for each node of the graph, and one time slot for each of the three colors green, red, and blue. For each edge between nodes u and v in the graph, we will have a student who requests both course u and course v. The coloring problem does not allow any conﬂicts. Hence, we set K = 0.  Not Onto or 1-1: It is important that each instance Ialg be mapped to some in- stance Ioracle, but it is not important whether an instance Poracle is mapped to more than one or none at all. In our example, we never mention instances to be scheduled that have more than three time slots or that allow K > 0 conﬂicts.  Warning: Be sure to do this mapping in the correct direction. The ﬁrst step in designing an algorithm Algalg is to suppose that you have been given an input Ialg for it. Before your algorithm can call the algorithm Algoracle as a subroutine, your must construct an instance Ioracle to give to it.  Warning: Do not deﬁne the mapping only for the Yes instances or use a solution Salg for Ialg for determining the instance Ioracle mapped to. The algorithm Algalg that you are designing is given an instance Ialg, but it does not know whether   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes20 CUUS154-Edmonds 978 0 521 84931 9  Reductions and NP-Completeness  March 29, 2008  14:8  335  or not the instance has a solution. The whole point is to give an argument that ﬁnding a solution may take exponential time. It is safer, when deﬁning the map- ping InstanceMap Ialg , not to even mention whether the instance Ialg has a so- lution or what that solution might be.  6  SolutionMap: You must also deﬁne a polynomial-time algorithm SolutionMap  Soracle  mapping each valid solution Soracle for the instance Ioracle = InstanceMap Ialg  you just constructed to a valid solution Salg for the instance Ialg that was given as in- put. Valid solutions can be subtle, and the instance Ioracle may have some solutions that you had not intended when you constructed it. One way to help avoid missing some is to throw a much wider net by considering all potential solutions. In this step, for each potential solution Soracle for Ioracle, you must either give a reason why it is not a valid solution or map it to a solution Salg = SolutionMap Soracle  for Ialg. It is ﬁne if some of the solutions that you map happen not to be valid.  3-COL ≤poly Course Scheduling: Given a schedule Sschedule assigning course u to time slots c, we deﬁne Scoloring = SolutionMap Sschedule  to be the coloring that colors node u with color c.  Warning: When the instance Ioracle you constructed has solutions that you did not expect, there are two problems. First, the unknown algorithm Algoracle may give you one of these unexpected solutions. Second, there is a danger that Ioracle has solutions but your given instance Ialg does not. For example, if, in step 5, our Ioracle allowed more than three time slots or more than K = 0 conﬂicts, then the instance might have many unexpected solutions. In such cases, you may have to redo step 5, adding extra constraints to the instance Ioracle so that it no longer has these solutions.  7  Valid to Valid: In order to prove that the algorithm Algalg Ialg  works, you must prove that if Soracle is a valid solution for Ioracle = InstanceMap Ialg , then Salg = SolutionMap Soracle  is a valid solution for Ialg.  3-COL ≤poly Course Scheduling: Supposing that the schedule is valid, we prove that the coloring is valid as follows. The instance to be scheduled is constructed so that, for each edge of the given graph, there is a student who requests the courses u and v associated with the nodes of this edge. Because the schedule is valid, there are K = 0 course conﬂicts, and hence these courses are all scheduled at different time slots. The constructed coloring therefore allocates different col- ors to these nodes.  8  ReverseSolutionMap: Though we do not need it for the code, for the proof you  cid:1  must deﬁne an algorithm ReverseSolutionMap S alg  mapping in the reverse direction  cid:1  from each potential solution S oracle for the instance Ioracle.   cid:1  alg for the instance Ialg to a potential solution S   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes20 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  14:8  336  Optimization Problems  3-COL ≤poly Course Scheduling: Given a coloring S  cid:1  coloring coloring node u with = ReverseSolutionMap S  cid:1  color c, we deﬁne S coloring  to be the schedule as- signing course u to time slots c.   cid:1  schedule   cid:1  Warning: ReverseSolutionMap S alg  does not need to be the inverse map of SolutionMap Soracle . You must deﬁne the mapping ReverseSolutionMap Salg  for  cid:1  every possible solution S alg, not just those mapped to by SolutionMap Soracle . Otherwise, there is the danger is that Ialg has solutions but your constructed in- stance Ioracle does not.  9  Reverse Valid to Valid: You must also prove the reverse direction: that if S valid solution for Ialg, then S Ioracle = InstanceMap Ialg .  = ReverseSolutionMap S   cid:1  oracle   cid:1  alg is a  cid:1  alg  is a valid solution for  3-COL ≤poly Course Scheduling: Supposing that the coloring is valid, we prove that the schedule is valid as follows. The instance to be scheduled is constructed so that each student requests the courses u and v associated with nodes of some edge. Because the coloring is valid, these nodes have been allocated different col- ors and hence the courses are all scheduled in different time slots. Hence, there will be K = 0 course conﬂicts.  10  Working Algorithm: Given the above steps, it is now possible to prove that if the supposed algorithm Algoracle correctly solves Poracle, then our algorithm Algalg cor- rectly solves Palg.   cid:1  oracle  = ReverseSolutionMap S  Yes to Yes: We start by proving that Algalg answers Yes when given an instance for which the answer is Yes. If Ialg is a Yes instance, then by the deﬁnition of  cid:1  the problem Palg, it must have a valid solution. Let us denote by S alg one such  cid:1  valid solution. Then by step 9, it follows that S alg  is a valid solution for Ioracle = InstanceMap Ialg . This witnesses the fact that Ioracle has a valid solution and hence Ioracle is an instance for which the answer is Yes. If Algoracle works correctly as supposed, then it returns Yes and a valid solution Soracle. Our code for Algalg will then return the correct answer Yes and Salg = SolutionMap Soracle , which by step 7 is a valid solution for Ialg. No to No: We must now prove the reverse, that if the instance Ialg given to Algalg is a No instance, then Algalg answers No. The problem with No instances is that they have no witness to prove that they are No instances. Luckily, to prove something, it is sufﬁcient to prove the contrapositive. Instead of proving A ⇒ B, where A = “Ialg is a No instance” and B = “Algalg answers No”, we will prove that ¬B ⇒ ¬A, where ¬B = “Algalg answers Yes” and ¬A = “Ialg is a Yes instance”. Con- vince yourself that this is equivalent.  If Algalg is given the instance Ialg and answers Yes, our code is such that Algoracle must have returned Yes. If Algoracle works correctly as supposed, the in- stance Ioracle = InstanceMap Ialg  that it was given must be a Yes instance. Hence,   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes20 CUUS154-Edmonds 978 0 521 84931 9  Reductions and NP-Completeness  March 29, 2008  14:8  Ioracle must have a valid solution. Let us denote by Soracle one such valid solution. Then by step 7, Salg = SolutionMap Soracle  is a valid solution for Ialg, witnessing Ialg being a Yes instance. This is the required conclusion ¬A.  This completes the proof that if the supposed algorithm Algoracle correctly solves Poracle, then our algorithm Algalg correctly solves Palg.  337  11  Running Time: The remaining step is to prove that the constructed algo- rithm Algalg runs in polynomial-time  Ialg cid:1  1  . Steps 5 and 6 require that both InstanceMap Ialg  and SolutionMap Soracle  work in polynomial-time. Hence, if Poracle can be solved quickly, then Algalg runs in polynomial-time. Typically, for reductions people assume that Algoracle is an oracle, meaning that it solves its problem in one time step. Exercise 20.2.5 explores the issue of running time further.  This concludes the proof that Poracle = course scheduling is NP-complete  as- suming, of course, that Palg = 3-COL has already been proven to be NP-complete . EXERCISE 20.2.1 We began this section by proving  any optimization problem  ≤poly CIR-SAT. To make this proof more concrete, redo it, completing each of the above steps speciﬁcally for 3-COL ≤poly CIR-SAT.  Hint: The circuit Ioracle = InstanceMap Ialg  should have a variable x cid:4 u,c cid:5  for each pair  cid:4 u, c cid:5 .   EXERCISE 20.2.2 3-SAT is a subset of the CIR-SAT problem in which the input circuit must be a big AND of clauses, each clause must be the OR of at most three literals, and each literal is either a variable or its negation. Prove that 3-SAT is NP-compete by proving that 3-COL ≤poly 3-SAT.  Hint: The answer is almost identical to that for Exercise 20.2.1.   EXERCISE 20.2.3 Let CIR-SAT be the complement of the CIR-SAT problem, namely, the answer is Yes if and only if the input circuit is not satisﬁable. Can you prove CIR-SAT ≤poly CIR-SAT using Cook reductions? Can you prove it using Karp reduc- tions?  EXERCISE 20.2.4  See solution in Part Five.  Suppose problem P1 is a restricted version of P2, in that they are the same except P1 is deﬁned on a subset I1 ⊆ I2 of the instances that P2 is deﬁned on. For example, 3-SAT is a restricted version of CIR-SAT, because both determine whether a given circuit has a satisfying assignment; however, 3-SAT only considers special types of circuits with clauses of three literals. How hard is it to prove P1 ≤poly P2? How hard is it to prove P2 ≤poly P1? EXERCISE 20.2.5  See solution in Part Five.  Suppose that when proving Palg ≤poly Poracle, the routines InstanceMap Ialg  and SolutionMap Soracle  each run in O Ialg3  time, and that the mapping InstanceMap Ialg  constructs from the instance Ialg an   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes20 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  14:8  Optimization Problems instance Ioracle that is much bigger, namely, Ioracle = Ialg2. Given the following two running times of the algorithm Algoracle, determine the running time of the algorithm Algalg.  Careful!  1. Time Algoracle  =  cid:1  2n 2. Time Algoracle  =  cid:1  nc  for some constant c.  1 3    338  20.3 Example: 3-Coloring Is NP-Complete  We will now use the steps again to prove that 3-coloring is NP-complete.  0  In NP: The problem 3-COL is in NP because, given an instance graph Igraph and a solution coloring Scoloring, it is easy to have an algorithm Valid Igraph, Scoloring  check that each node is colored with one of three colors and that the nodes of each edge have different colors.  1  What to Reduce to It: We will reduce CIR-SAT to 3-COL by proving CIR-SAT ≤poly 3-COL. In Section 20.1 we proved that  any optimization problem  ≤poly CIR- SAT and that 3-COL ≤poly course scheduling. By transitivity, this gives us that CIR- SAT, 3-COL, and course scheduling are each NP-complete problems.  2  What is What:   cid:1  Palg is the circuit satisﬁability problem  CIR-SAT .  cid:1  Icircuit, an instance of it, is a circuit.  cid:1  Sassignment, a potential solution, is an assignment to the circuit variables x1,   cid:1  Poracle is the graph 3-coloring problem  3-COL .  cid:1  Igraph, an instance to it, is a graph.  cid:1  Scoloring, a potential solution, is an coloring of the nodes of the graph with three  x2, . . . , xn.  colors.  3  Direction of Reduction and Code: To prove 3-COL is at least as hard, we must prove that CIR-SAT is at least as easy, i.e., CIR-SAT ≤poly 3-COL. To do this, we must design an algorithm for CIR-SAT given an algorithm for 3-COL. The code will be iden- tical to that in Section 20.2.  4  Look for Similarities: An assignment allocates true or false values to each variable, which in turn induces true or false values to the output of each gate. A coloring allo- cates one of three colors to each node. This similarity hints at mapping the variables and outputs of each gate to nodes in the graph and mapping true to one color and false to another. With these ideas in mind, Steven Rudich made a computer search for the smallest graph that behaves like an OR gate when colored with three colors. The graph found is shown in Figure 20.1. He calls it an OR gadget.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes20 CUUS154-Edmonds 978 0 521 84931 9  Reductions and NP-Completeness  March 29, 2008  14:8  vT vR  Fv  vx 1v 3vv2  4v outv OR gadget  vy v 5 v6  r g b  r  r  g  b  r  g  r  g  b  b  r  g  vF  Tv v R  xv  vout  NOT gadget   F OR T  = T   F OR F  = F  339  g b g  vz  r g r  r g r  Rv Tv  vF  yv  v  not z   Graph  vout  r g b  b r g b  g  g  r  r  b  r g b  g  r  g b g  r  F  F  F  OR  NOT  F  T  OR  T  Coloring  Circuit  x  y  z  OR  NOT  xv  v  x or y   OR  out  Circuit  Figure 20.1: On the top, the ﬁrst diagram is the OR gadget. The next two are colorings of this gadget demonstrating  false or true  = true and   false or false  = false. The top right diagram is the NOT gadget. On the bottom, the ﬁrst diagram is the circuit given as an instance to SAT. The next is the graph that it is translated into. The next is a 3-coloring of this graph. The last is the assignment for the circuit obtained from the coloring.  Translating between Colors and True False: The three nodes vT , vF , and vR in the OR gadget are referred to as the pallet. Because of the edges between them, when the gadget is properly colored, these nodes need to be assigned different colors. We will call whatever color is assigned to the node vT the color indicating true; that assigned to vF , the color indicating false; and that assigned to vR, the remaining color. For example, in all the colorings in Figure 20.1, green  g  indi- cates true, red  r  indicates false, and blue  b  is the remaining color.  Input and Output Values: The nodes vx and vy in the OR gadget act as the gad- get’s inputs, and the node vout as its output. Because each of these nodes has an edge to node vR, they cannot be colored with the remaining color. The node will be said to have the value true, if it is assigned the same color as vT , and false if the same as vF . The coloring in the second ﬁgure in Figure 20.1 sets x = false, y = true, and the output = true. The coloring in the third ﬁgure sets x = false, y = false, and the output = false.  Theorem 20.3.1: Rudich’s OR gadget acts like an OR gate, in that it always can be and always must be colored so that the value of its output node vout is the OR of the values of its two input nodes vx and vy . Similarly for the NOT gate.  Proof: There are four input instances to the gate to consider.   false OR true  = true: If node vx is colored false and vy is colored true, then because v5 has an edge to each, it must be colored the remaining color. v6, with edges to vF and v5, must be colored true. v4, with edges to vR and v6,   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes20 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  340  March 29, 2008  14:8  must be colored false. vout, with edges to vR and v4, must be colored true. The coloring in the second diagram in Figure 20.1 proves that such a coloring is possible.  false OR false  = false: If nodes vx and vy are both colored false, then nei- ther nodes v1 nor v3 can be colored false. Because of the edge between them, one of them must be true and the other the remaining color. Because v2 has an edge to each of them, it must be colored false. v4, with edges to vR and v2, must be colored true. vout, with edges to vR and v4, must be colored false. The coloring in the third diagram in Figure 20.1 proves that such a coloring is possible.  true OR true  = true, and  true OR false  = true: See Exercise 20.3.1 for these cases and for the NOT gate.  5  InstanceMap, Translating the Circuit into a Graph: Our algorithm for CIR-SAT takes as input a circuit Icircuit to be satisﬁed and, in order to receive help from the 3-COL algorithm, constructs from it a graph Igraph = I nstanceMap Icircuit  to be col- ored. See the ﬁrst two diagrams on the bottom of Figure 20.1. The graph will have one pallet of nodes vT , vF , and vR with which to deﬁne the true and the false color. For each variable xi of the circuit, it will have one node labeled xi. It will also have one node labeled xout. For each OR gate and NOT gate in the circuit, the graph will have one copy of the OR gadget or the NOT gadget. The AND gates could be translated into a similar AND gadget or translated to [x AND y] = [NOT NOT x  O R NOT y  ]. All of these gadgets share the same three pallet nodes. If in the circuit the output of one gate is the input of another, then the corresponding nodes in the graph are the same. Finally, one extra edge is added to the graph from the vF node to the vout node.  6  SolutionMap, Translating a Coloring into an Assignment: When the supposed algorithm ﬁnds a coloring Scoloring for the graph Igraph = InstanceMap Icircuit , our al- gorithm must translate this coloring into an assignment Sassignment = SolutionMap  Scoloring  of the variables x1, x2, . . . , xn for the circuit. See the last two diagrams on the bottom of Figure 20.1. The translation is accomplished by setting xi to true if node vxi is colored the same color as node vT , and false if the same as vF . If node vxi has the same color as node vR, then this is not a valid coloring  because there is an edge in the graph from node vxi to node vR  and hence need not be considered.  Warning: Suppose that the graph constructed had a separate node for each time that the circuit used the variable xi. The statement “set xi to true when the node vxi has some color” would then be ambiguous, because the different nodes rep- resenting xi might be given different colors.  7  Valid to Valid: Here we must prove that if the supposed algorithm gives us a valid coloring Scoloring for the graph Igraph = InstanceMap Icircuit , then Sassignment = SolutionMap Scoloring  is an assignment that satisﬁes the circuit. By the gadget   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes20 CUUS154-Edmonds 978 0 521 84931 9  Reductions and NP-Completeness  March 29, 2008  14:8  theorem, each gadget in the graph must be colored in a way that acts like the cor- responding gate. Hence, when we apply the assignment to the circuit, the output of each gate will have the value corresponding to the color of corresponding node. It is as if the coloring of the graph were performing the computation of the circuit. It follows that the output of the circuit will have the value corresponding to the color of node vout. Because node vout has an edge to vR and an extra edge to vF , vout must be colored true. Hence, the assignment is one for which the output of the circuit is true.  341  8  ReverseSolutionMap: For the proof we must also deﬁne the reverse map- ping from each assignment Sassignment to a coloring Scoloring = ReverseSolutionMap  Sassignment . Start by coloring the pallet nodes true, false, and the remaining color. Color each node vxi true or false according to the assignment. Then Theorem 20.3.1 states that no matter how the input nodes to a gadget are colored, the entire gadget can be colored with the output node having the color indicated by the output of the corresponding gate.  9  Reverse Valid to Valid: Now we prove that if the assignment Sassignment satisﬁes the circuit, then the coloring Scoloring = Rever seSolutionMap Sassignment  is valid. The- orem 20.3.1 ensured that each edge in each gadget has two different colors. The only edge remaining to consider is the extra edge. As the colors percolate down the graph, node vout must have color corresponding to the output of the circuit, which must be the true color, because the assignment satisﬁes the circuit. This ensures that even the extra edge from vF to vout is colored with two different colors.  10  and 11 : These steps are always the same. InstanceMap Icircuit  maps Yes circuit instances to Yes 3-COL instances and No to No. Hence, if the supposed algorithm 3-COL works correctly in polynomial-time, then our designed algorithm correctly solves CIR-SAT in polynomial-time. It follows that CIR-SAT ≤poly 3-COL. In conclu- sion, 3-coloring is NP-complete.  EXERCISE 20.3.1  a  Complete the proof of Theorem 20.3.1 by proving the cases  true OR true  = true and  true OR false  = true.  b  Prove a similar theorem for the NOT gadget. See the top right diagram in Figure 20.1. EXERCISE 20.3.2 Verify that each edge in the graph Igraph = InstanceMap Icircuit  is needed, by showing that if it were not there, then it would be possible for the graph to have a valid coloring even when the circuit is not satisﬁed.  EXERCISE 20.3.3  See solution in Part Five.  Prove that independent set is NP-compete by proving that 3-COL ≤ poly Independent set.  Hint: A 3-coloring for the graph G3-COL can be thought of as a subset of the pairs  cid:4 u, c cid:5  where u is a node of G3-COL and c is a color. An independent set of the graph GInd selects a subset of its nodes. Hence, a   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes20 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  14:8  Optimization Problems way to construct the graph Gind in the instance  cid:4 GInd, NInd cid:5  = InstanceMap G3-COL  would be to have a node for each pair  cid:4 u, c cid:5 . Be careful when deﬁning the edges for the graph GInd = InstanceMap G3-COL  so that each valid independent set of size n in the constructed graph corresponds to a valid 3-coloring of the original graph. If the constructed graph has unexpected independent sets, you may need to add more edges to it.   342  20.4 An Algorithm for Bipartite Matching Using the Network  Flow Algorithm  Up to now we have been justifying our belief that certain computational problems are difﬁcult by reducing them to other problems believed to be difﬁcult. Here, we will give an example of the reverse, by proving that the problem of bipartite matching can be solved easily by reducing it to the network ﬂow problem, which we already know is easy because we gave an polynomial-time algorithm for it in Chapter 15.  Bipartite Matching: Bipartite matching is a classic optimization problem. As al- ways, we deﬁne the problem by giving a set of instances, a set of solutions for each instance, and a cost for each solution.  Instances: An input instance to the problem is a bipartite graph. A bipartite graph is a graph whose nodes are partitioned into two sets U and V and all edges in the graph go between U and V . See the ﬁrst diagram in Figure 20.2.  Solutions for an Instance: Given an instance, a solution is a matching. A match- ing is a subset M of the edges such that no node appears more than once in M. See the last diagram in Figure 20.2.  Cost of a Solution: The cost  or success  of a matching is the number of pairs matched. It is said to be a perfect matching if every node is matched.  Goal: Given a bipartite graph, the goal of the problem is to ﬁnd a matching that matches as many pairs as possible.  Network Flow: Network ﬂow is another example of an optimization problem that involves searching for a best solution from some large set of solutions.  Instances: An instance  cid:4 G, s, t cid:5  consists of a directed graph G and speciﬁc nodes s and t. Each edge  cid:4 u, v cid:5  is associated with a positive capacity c cid:4 u,v cid:5 . Solutions for the Instance: A solution for the instance is a ﬂow F , which speciﬁes a ﬂow F cid:4 u,v cid:5  ≤ c cid:4 u,v cid:5  through each edges of the network with no leaking or addi- tional ﬂow at any node.  Measure of Success: The cost  or success  of a ﬂow is the amount of ﬂow out of node s.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes20 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  14:8  Reductions and NP-Completeness  cap =  1  flow = 1 flow = 0  s  t  s  t  343  Network  Bipartite graph Figure 20.2: The ﬁrst diagram is the bipartite graph given as an instance to bipartite matching. The next is the network that it is translated into. The next is a ﬂow through this network. The last is the matching obtained from the ﬂow.  Matching  Flow  Goal: Given an instance  cid:4 G, s, t cid:5 , the goal is to ﬁnd an optimal solution, that is, a maximum ﬂow.  Bipartite Matching ≤poly Network Flows: We go through the same steps as before.  3  Direction of Reduction and Code: We will now design an algorithm for bipar- tite matching given an algorithm for network ﬂows.  4  Look for Similarities: A matching decides which edges to keep, and a ﬂow de- cides which edges to put ﬂow though. This similarity suggests keeping the edges that have ﬂow through them.  5  InstanceMap, Translating the Bipartite Graphs into a Network: Our algorithm for bipartite matching takes as input a bipartite graph Gbipartite. The ﬁrst step is to translate this into a network Gnetwork = InstanceMap Gbipartite . See the ﬁrst two diagrams in Figure 20.2. The network will have the nodes U and V from the bipartite graph, and for each edge  cid:4 u, v cid:5  in the bipartite graph, the network has a directed edge  cid:4 u, v cid:5 . In addition, the network will have a source node s with a directed edge from s to each node u ∈ U. It will also have a sink node t with a directed edge from each node v ∈ V to t. Every edge out of s and every edge into t will have capacity one. The edges  cid:4 u, v cid:5  across the bipartite graph could be given capacity one as well, but they could just as well be given capacity ∞.  6  SolutionMap, Translating a Flow into a Matching: When the network ﬂow al- gorithm ﬁnds a ﬂow Sﬂow through the network, our algorithm must translate this ﬂow into a matching Smatching = SolutionMap Sﬂow . See the last two diagrams in Figure 20.2.  SolutionMap: The translation puts the edge  cid:4 u, v cid:5  in the matching if there is a ﬂow of one through the corresponding edge in the network, and not if there is no ﬂow in the edge.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes20 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  344  March 29, 2008  14:8  Warning: Be careful to map every possible ﬂow to a matching. The above mapping is ill deﬁned when there is a ﬂow of 1 2 through an edge. This needs to be ﬁxed and could be quite problematic.  Integer Flow: Luckily, Exercise 15.2.4 proves that if all the capacities in the given network are integers, then the algorithm always returns a solution in which the ﬂow through each edge is an integer. Given that our capacities are all one, each edge will have a ﬂow either of zero or of one. Hence, in our trans- lation, it is well deﬁned whether to include the edge  cid:4 u, v cid:5  in the matching or not.  7  Valid to Valid: Here we must prove that if the ﬂow Sﬂow is valid, than the matching Smatching is also valid.  Each u Matched at Most Once: Consider a node u ∈ U. The ﬂow into u can be at most one, because there is only one edge into it and it has capacity one. For the ﬂow to be valid, the ﬂow out of this node must equal that into it. Hence, it too can be at most one. Because each edge out of u either has ﬂow zero or one, it follows that at most one edge out of u has ﬂow. We can conclude that u is matched to at most one node v ∈ V .  Each v Matched at Most Once: See Exercise 20.4.1.  Cost to Cost: To be sure that the matching we obtain contains the maximum number of edges, it is important that the cost of the matching Smatching = SolutionMap Sﬂow  equal the cost of the ﬂow. The cost of the ﬂow is the amount of ﬂow out of node s, which equals the ﬂow across the cut  cid:4 U, V cid:5 , which equals the number of edges  cid:4 u, v cid:5  with ﬂow of one, which equals the number of edges in the matching, which equals the cost of the matching.  8  ReverseSolutionMap: The reverse mapping from each matching Smatching to a valid ﬂow Sﬂow = ReverseSolutionMap Smatching  is straightforward. If the edge  cid:4 u, v cid:5  is in the matching, then put a ﬂow of one from the source s, along the edge  cid:4 s, u cid:5  to node u, across the corresponding edge  cid:4 u, v cid:5 , and then on through the edge  cid:4 v, t cid:5  to t.  9  Reverse Valid to Valid: We must also prove that if the matching Smatching is valid, then the ﬂow Sﬂow = ReverseSolutionMap Smatching  is also valid.  Flow in Equals Flow Out: Because the ﬂow is a sum of paths, we can be as- sured that the ﬂow in equals the ﬂow out of every node except for the source and the sink. Because the matching is valid, each u and each v is matched at most once. Hence the ﬂows through the edges  cid:4 s, u cid:5 ,  cid:4 u, v cid:5 , and  cid:4 v, t cid:5  will be at most their capacity one.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes20 CUUS154-Edmonds 978 0 521 84931 9  Reductions and NP-Completeness  March 29, 2008  14:8  Cost to Cost: Again, we need to prove that the cost of the ﬂow Sﬂow = ReverseSolutionMap Smatching  is the same as the cost of the matching. See Exercise 20.4.2.  10  and 11 : These steps are always the same. InstanceMap Gbipartite  maps bi- partite graph instances to network ﬂow instances Gﬂow with the same cost. Hence, because algorithm Algﬂow correctly solves network ﬂows quickly, our de- signed algorithm correctly solves bipartite matching quickly.  345  In conclusion, bipartite matching can be solved in the same time that network ﬂow is solved.  EXERCISE 20.4.1 Give a proof for the case where each v is matched at most once. EXERCISE 20.4.2 Give a proof that the cost of the ﬂow Sﬂow = ReverseSolutionMap  Smatching  is the same as the cost of the matching  EXERCISE 20.4.3 Section 19.9 constructs three dynamic programming algorithms using reductions. For each of these, carry out the formal steps required for a reduction.  EXERCISE 20.4.4 There is a collection of software packages S1, . . . , Sn that you are considering buying. These are partitioned into two groups. For those i ∈ N ⊆ [n], the costs of buying it out ways the beneﬁts and hence it effectively costs you a given amount bi ≥ 0 to buy it. For those j ∈ P ⊆ [n], the beneﬁts out way the costs of buy- ing it and hence it effectively costs you a given amount b j ≥ 0 to not buy it. Some of these packages rely on each other; if Si relies on Sj , then you will incur an additional cost of a cid:4 i, j cid:5  ≥ 0 if you buy Si but not Sj . Provide a polynomial-time algorithm to de- cost S  = cid:1  cide the subset S ⊆ [n] of S2, . . . , Sn that you should buy. The cost of your solution is i∈S, j∈S a cid:4 i, j cid:5 .  Hint: Do not design a new algorithm  i∈S∩N bi + cid:1   j∈S∩P bi + cid:1   but do a reduction to min cut similar to that done for matching the boys and girls.    P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes21 CUUS154-Edmonds 978 0 521 84931 9  March 25, 2008  13:32  346 21 Randomized Algorithms  For some computational problems, allowing the algorithm to ﬂip coins  i.e., use a random number generator  makes for a simpler, faster, easier-to-analyze algorithm. The following are the three main reasons.  Hiding the Worst Cases from the Adversary: The running time of a randomized algorithms is analyzed in a different way than that of a deterministic algorithm. At times, this way is fairer and more in line with how the algorithm actually performs in practice. Suppose, for example, that a deterministic algorithm quickly gives the correct answer on most input instances, yet is very slow or gives the wrong answer on a few instances. Its running time and its correctness are generally measured to be those on these worst case instances. A randomized algorithm might also sometimes be very slow or give the wrong answer.  See the discussion of quick sort, Section 9.1 . However, we accept this, as long as on every input instance, the probability of doing so  over the choice of random coins  is small.  Probabilistic Tools: The ﬁeld of probabilistic analysis offers many useful tech- niques and lemmas that can make the analysis of the algorithm simple and elegant.  Solution Has a Random Structure: When the solution that we are attempting to construct has a random structure, a good way to construct it is to simply ﬂip coins to decide how to build each part. Sometimes we are then able to prove that with high probability the solution obtained this way has better properties than any solution we know how to construct deterministically. Moreover, if we can prove that the solu- tion constructed randomly has extremely good properties with some very small but nonzero probability  for example, prob = 10 −100 , then this proves the existence of such a solution even though we have no reasonably quick way of ﬁnding one. An- other interesting situation is when the randomly constructed solution very likely has the desired properties, for example with probability 0.999999, but, there is no quick way of testing whether what we have produced has the desired properties.  This chapter considers these ideas further.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes21 CUUS154-Edmonds 978 0 521 84931 9  Randomized Algorithms  March 25, 2008  13:32  347  21.1 Using Randomness to Hide the Worst Cases  The standard way of measuring the running time and correctness of a deterministic algorithm is based on the worst case input instance chosen by some nasty adversary who has studied the algorithm in detail. This is not fair if the algorithm does very well on all but a small number of very strange and unlikely input instances. On the other hand, knowing that the algorithm works well on most instances is not always satis- factory, because for some applications it is just those hard instances that you want to solve. In such cases, it might be more comforting to use a randomized algorithm that guarantees that on every input instance, the correct answer will be obtained quickly with high probability.  A randomized algorithm is able to ﬂip coins as it proceeds to decide what ac- tions to take next. Equivalently, a randomized algorithm A can be thought of as a set of deterministic algorithms A 1, A 2, A 3, . . . where Ar is what algorithm A does when the outcome of the coin ﬂips is r =  cid:2 heads, tails, heads, heads, . . . , tails cid:3 . Each such deterministic algorithm Ar will have a small set of worst case input instances on which it either gives the wrong answer or runs too slow. The idea is that these al- gorithms A 1, A 2, A 3, . . . have different sets of worst case instances. This randomized algorithm is good if for each input instance, the fraction of the deterministic algo- rithms A 1, A 2, A 3, . . . for which it is not a worst case instance is at least p. Then when one of these Ar is chosen randomly, it solves this instance quickly with probability at least p.  I sometimes ﬁnd it useful to consider the analysis of randomized algorithms as a game between an algorithm designer and an adversary who tries to construct input instances that will be bad for the algorithm. In the game, it is not always fair for the adversarial input chooser to know the algorithm ﬁrst, because then she can choose the instance that is the worst case for this algorithm. Similarly, it is not always fair for the algorithm designer to know the input instance ﬁrst or even which instances are likely, because then he can design the algorithm to work well on these. The way we analyze the running time of randomized algorithms compromises between these two. In this game, the algorithm designer, without knowing the input instance, must ﬁrst ﬁx what his algorithm will do given the outcome of the coins. Knowing this, but not knowing the outcomes of the coins, the instance chooser chooses the worst case instance. We then ﬂip coins, run the algorithm, and see how well it does.  Three Models: The following are formal deﬁnitions of three models.  Deterministic Worst Case: In a worst case analysis, a deterministic algorithm A for a computational problem P must always give the correct answer quickly:  ∀I, [A I   = P I   and Time A , I   ≤ Tupper I ]  Las Vegas: The algorithm is said to be Las Vegas if it is always guaranteed to give the correct answer, but its running time depends on the outcomes of the random   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes21 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  348  March 25, 2008  13:32  coin ﬂips. The goal is to prove that on every input instance, the expected running time is small:  ∀I, [∀r, Ar  I   = P I   and Expr [Time Ar , I  ] ≤ Tupper I ]  Monte Carlo: The algorithm is said to be Monte Carlo if the algorithm is guar- anteed to stop quickly, but it can sometimes, depending on the outcomes of the random coin ﬂips, give the wrong answer. The goal is to prove that on every in- put, the probability of it giving the wrong answer is small:  ∀I, [Prr [Ar  I    cid:6 = P I  ] ≤ pfails and ∀r, Time Ar , I   ≤ Tupper I ]  The following examples demonstrate these ideas.  Quick Sort: Recall the quick sort algorithm from Section 9.1. The algorithm chooses a pivot element and partitions the list of numbers to be sorted into those that are smaller than the pivot and those that are larger than it. Then it recurses on each of these two parts. The running time varies from  cid:1  n log n  to  cid:1  n2 , depending on the choices of pivots.  Deterministic Worst Case: A reasonable choice for the pivot is to always use the element that happens to be located in the middle of the array to be sorted. For all practical purposes, this would likely work well. It would work exceptionally well when the list is already sorted. However, there are some strange inputs, cooked up for the sole purpose of being nasty to this particular implementation of the algorithm, on which the algorithm runs in  cid:1  n2  time. The adversary will provide such an input, giving a worst case time complexity of  cid:1  n2 .  Las Vegas: In practice, what is often done is to choose the pivot element ran- domly from the input elements. This makes it irrelevant in which order the ad- versary puts the elements in the input instance. The expected computation time is  cid:1  n log n .  The Game Show Problem: The input I to the game show problem speciﬁes which of N doors have prizes behind them. At least half the doors are promised to have prizes. An algorithm A is able to look behind the doors in any order that it likes, but nothing else. It solves the problem correctly when it ﬁnds a prize. The running time is the number of doors opened.  Deterministic Worst Case: Any deterministic algorithm ﬁxes the order in which it looks behind the doors. Knowing this order, the adversary places no prizes be- hind the ﬁrst N  2 doors looked behind.  Las Vegas: In contrast, a random algorithm will look behind doors in random order. It does not matter where the adversary puts the prizes; the probability that one is not found after t doors is 1 2t , and the expected time until a prize is found  is Exp[T] = cid:1   t Pr[T = t] · t = 2.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes21 CUUS154-Edmonds 978 0 521 84931 9  Randomized Algorithms  March 25, 2008  13:32  Monte Carlo: If the promise is that either at least half the doors have prizes or none of them do and if the algorithm stops after 10 empty doors and claims that there are no prizes, then this algorithm is always fast, but gives the wrong answer with probability 1 210.  Randomized Primality Testing: An integer x is said to be composite if it has factors other than one and itself. Otherwise, it is said to be prime. For example, 6 = 2 × 3 is composite and 2, 3, 5, 7, 11, 13, 17, . . . are prime. See Chapter 23.1, Example 23.2, for explanations of why it takes 2 cid:1  n  time to factor an n-bit number.1 Here I give an easy randomized algorithm, due to Rabin and Miller, for this problem.  349  Fermat’s Little Theorem: Don’t worry about the math, but Fermat’s little theorem says that if x is prime, then for every a ∈ [1, x − 1], it is the case that a x−1 ≡ mod x  1.  If we want to test if x is prime, then we can pick random a’s in the interval and see if the equality holds. If the equality does not hold for a value of a, then x is composite. If the equality does hold for many values of a, then we can say that x is probably prime, or a what we call a pseudoprime. The Game Show Problem: Finding an a for which a x−1  cid:6 ≡ mod x  1 is like ﬁnding a prize behind door a. See Exercise 21.1.1.  Randomized Counting: In many applications, one wants to count the number of occurrences of something. This problem can often be expressed as follows: Given the input instance x, count the number of y for which f  x, y  = 1. It is likely very difﬁcult to determine the exact number. However, a good way to approximate this number is to randomly choose some large number of values y. For each, test whether f  x, y  = 1. Then the fraction of y for which f  x, y  = 1 can be approxi- mated by [the number you found] [the number you tried]. The number of y for which f  x, y  = 1 can be approximated by [the fraction you found]×[the total number of y].  For example, suppose you had some strange shape and you wanted to ﬁnd its area. Then x would specify the shape, y would specify some point within a surrounding box, and f  x, y  = 1 if the point is within the shape. Then the number of y for which f  x, y  = 1 gives you the area of your shape.  y for which f x,y =0  x  y for which f x,y =1  EXERCISE 21.1.1 Given an integer x, suppose that you have one door for each a ∈ [1, x − 1]. We will say that there is a prize behind this door if a x−1  cid:6 ≡ mod x  1. Fermat’s little theorem says that if x is pseudoprime, then none of the doors have prizes behind  1 A major breakthrough by Agrawal et al. in 2002 was to ﬁnd a polynomial-time deterministic algo-  rithm for determining whether an n-bit number is prime.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes21 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  March 25, 2008  13:32  350  them, and if it is composite, then at least half the doors have prizes. The algorithm attempts to determine which is the case by opening t randomly chosen doors for some integer t.  1.  2.  3.  If the algorithm ﬁnds a prize, what do you know about the integer? If it does not ﬁnd a prize, what do you know? If the algorithm must always give the correct answer, how many doors need to be opened, as a function of the number n of digits in the instance x? If t doors are open and the input instance x is a pseudoprime, what is the prob- ability that the algorithm gives the correct answer? If the instance is composite, what is this probability?  EXERCISE 21.1.2 Section 4.3 designed an iterative algorithm for separating n VLSI chips into those that are good and those that are bad by testing two chips at a time and learning either that they are the same or that they are different. To help, at least half of the chips are promised to be good. Now design  much easier  a randomized algorithm for this problem. Here are some hints.   cid:1  Randomly select one of the chips. What is the probability that the chip is good?  cid:1  How can you learn whether or not the selected chip is good?  cid:1  If it is good, how can you easily partition the chips into good and bad chips?  cid:1  If the chip is not good, what should your algorithm do?  cid:1  When should the algorithm stop?  cid:1  What is the expected running time of this algorithm?  21.2 Solutions of Optimization Problems with a Random Structure  Optimization problems involve looking for the best solution for an instance. Some- times good solutions have a random structure. In such cases, a good way to construct one is to simply ﬂip coins to decide how to build each part. I give two examples. The ﬁrst one, max cut, being NP-complete, likely requires exponential time to ﬁnd the best solution. However, in O n  time, we can ﬁnd a solution which is likely to be at least half as good as optimal. The second example, expander graphs, is even more extreme. Though there are deterministic algorithms for constructing graphs with fairly good expansion properties, a random graph almost for sure has much bet- ter expansion properties  with probability p ≥ 0.999999 . A complication, however, is that there is no polynomial-time algorithm that tests whether this randomly con- structed graph has the desired properties. Pushing the limits further, it can be proved that the same random graph has extremely good properties with some very small but nonzero probability  e.g., p ≥ 10 −100 . Though we have no quick way to construct such a graph, this does proves that such a graph exists.  The Max Cut Problem: The input to the max cut problem is an undirected graph. The output is a partition of the nodes into two sets U and V such that the number of   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes21 CUUS154-Edmonds 978 0 521 84931 9  Randomized Algorithms  March 25, 2008  13:32  edges that cross over from one side to the other is as large as possible. This problem is NP-complete, and hence the best known algorithm for ﬁnding an optimal solution requires 2 cid:1  n  time. The following randomized algorithm runs in time  cid:1  n  and is expected to obtain a solution for which half the edges cross over. This algorithm is incredibly simple. It simply ﬂips a coin for each node to decide whether to put it into U or into V . Each edge will cross over with probability 1 2 . Hence, the expected number E of edges to cross over is 2 . The optimal solution cannot have more than all the edges cross over, so the randomized algorithm is expected to perform at least half as well as the optimal solution can do.  351  Expander Graphs: An n-node degree-d graph is said to be an expander graph if moving from a set of its nodes across its edges expands us out to an even larger set of nodes. More formally, for 0 < α < 1 and 1 < β < d, a graph G =  cid:2 V, E cid:3  is an  cid:2 α, β cid:3 - expander if for every subset S ⊆ V of its nodes, if S ≤ αn then N S   ≥ βS. Here N S   is the neighborhood of S, that is, the set of all nodes with an edge from some node in S.  neighbors overlap a lot, then the total number of neighbors N S   = cid:2   Nonoverlapping Sets of d Neighbors: Because each node v ∈ V has d neighbors N v , a set S has dS edges leaving these nodes. However, if these sets N v  of v∈S N v  of S might be very small. We can’t expect N S   to be bigger than dS, but we do want it to have size at least βS where 1 < β < d. If S is too big, we can’t expect it to expand further. Hence, we only require this expansion property for sets S of size at most αn. Because we do expect sets of size αn to expand to a neighborhood of size βαn, we require that αβ < 1.  Connected with Short Paths: If αβ > 1 nected with a path of length at most 2 log n 2   2 , then every pair of nodes in G is con- log β  .  Proof: Consider two nodes u and v. The node u has d neighbors, N u . These neighbors N u  must have at least βN u  = βd neighbors N N u  . These neighbors N N u   must have at least β2d neighbors. It follows that there are at least βi−1d nodes with distance i from u. The last time we are allowed to do this expands the neighbor set of size S = αn to N S   ≥ βS = βαn. By the requirement that αβ > 1 2 , this new neighbor set has size greater than 2 nodes. The distance of these nodes from u is at most i = logβ n n 2 . This set might not contain v. However, starting from v there is another set of more than half the nodes that are distant i = logβ n 2 from v. These two sets must over lap at some node w. Hence, there is a path from u to w to v of length at most 2 log n 2   .  log β  Uses: Expander graphs are very useful both in practice and for proving theorems.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes21 CUUS154-Edmonds 978 0 521 84931 9  Optimization Problems  March 25, 2008  13:32  Fault-Tolerant Networks: As we have seen, every pair of nodes in an ex- pander graph are connected. This is still true if a large number of nodes or edges fail. Hence, this is a good pattern for wiring a communications network.  352  Pseudorandom Generators: Taking a short random walk in an expander graph quickly gets you to a random node. This is useful for generating long random looking strings from a short seed string.  Concentrating and Recycling Random Bits: If we have a source that has some randomness in it  say n coin tosses with an unknown probability and with unknown dependences between the coins , we can use expander graphs to produce a string of m bits appearing to be the result of m fair and independent coins.  Error-Correcting Codes: Expander graphs are also useful in designing ways of encoding a message into a longer code so that if any reasonable fraction of the longer code is corrupted, the original message can still be recovered. The faulty bits are connected by short paths to correct bits.  If αβ < 1, Then Expander Graphs Exist: We will now prove that for any constants α and β for which αβ < 1 there exists an  cid:2 α, β cid:3 -expander graph with n nodes and degree d for some sufﬁciently big constant d. For example, if α = 1 2 , then d = 5 is sufﬁcient. To make the analysis easier, we will consider directed graphs where each node u is connected to d nodes chosen independently at random.  If we ignore the directions of the edges, then each node has average degree 2d and neighborhood sets are only bigger.  We prove that the probability we do not get such an expander graph is strictly less than one. Hence, one must exist.  2 , β = 3  Event E S,T : The graph G will not be a  cid:2 α, β cid:3 -expander if there is some set S for which S ≤ αn and N S   < βS. Hence, for each pair of sets S and T, with S ≤ αn and T < βS, let E S,T denote the bad event that N S   ⊆ T. Let us bound the probability of E S,T when we choose G randomly. Each node in S needs d neighbors, for a total of dS randomly chosen neighbors. The probability of a particular one of these landing in T is T n. Because these edges are chosen in- dependently, the probability of them all landing in T is   cid:3 T n   cid:4 dS  .  Probability of Some Bad Event: The probability that G is not an expander is the probability that at least one of these bad events E S,T happens, which is at most the sum of the probabilities of these individual events:  Pr[G not an expander] = Pr[At least one of the events E S,T occurs]   cid:5    cid:5    S  S =s    T  T=βs   Pr[E S,T ]   cid:5  ≤  cid:6   cid:5   S,T   cid:5  Pr[E S,T ] =  cid:7 dS   cid:7  cid:6 T  cid:7  cid:6    s≤αn   =  s≤αn  n s  n βs  n   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes21 CUUS154-Edmonds 978 0 521 84931 9  March 25, 2008  13:32  Randomized Algorithms  We now use the result that  Pr[G not an expander] ≤   cid:4 a :  cid:7 βs cid:6   cid:7 β cid:6   en βs  βs n  en βαn  n a  en a  en s   cid:4  ≤ cid:3   cid:3   cid:6   cid:9 s  cid:8   cid:5   cid:10  cid:8   cid:9  cid:6   cid:5   cid:4   cid:4  + β  en αn  s≤αn  s≤αn  ≤   cid:14    cid:7 ds =  cid:7 d  βαn   cid:10  cid:8   cid:5   cid:11 s =  cid:5   s≤αn  en s   cid:11 s  cid:7 d  cid:13 s  cid:4 d−β  βs n   cid:7 β cid:6  · cid:3   αβ   cid:9  cid:6   cid:12   en βs eβ+1 α  s≤αn  n if d is sufﬁciently big  d ≥  cid:12   cid:5    cid:13 s  Pr  G not an expander  < 1   cid:15  ≤  1 2  s≤αn   cid:3    cid:4    cid:3   The requirement log   log  2eβ+1 α  is that αβ < 1. Hence, 1 αβ  , then the bracketed summand is at most 1 2 :  It follows that Pr[G is an expander] > 0, meaning that there exists at least one such G that is an expander.  353   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes21 CUUS154-Edmonds 978 0 521 84931 9  March 25, 2008  13:32  354   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes22 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  17:39  PART FOUR  Appendix  355   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes22 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  17:39  356   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes22 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  17:39  22 Existential and Universal Quantiﬁers  357  Existential and universal quantiﬁers provide an extremely useful language for mak- ing formal statements. You must understand them. A game between a prover and a veriﬁer is a level of abstraction within which it is easy to understand and prove such statements.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes22 CUUS154-Edmonds 978 0 521 84931 9  Appendix  March 29, 2008  17:39  The Loves Example: Suppose the relation  predicate  Loves p1, p2  means that per- son p1 loves person p2. Then we have  358  Expression ∃p2 Loves Sam, p2  ∀p2 Loves Sam, p2  ∃p1∀p2 Loves p1, p2  ∀p1∃p2 Loves p1, p2  ∃p2∀p1 Loves p1, p2  ∃p1∃p2  Loves p1, p2  and ¬Loves p2, p1    Meaning  “Sam loves somebody.” “Sam loves everybody.” “Somebody loves everybody.” “Everybody loves somebody.” “Theres one person who is loved  by everybody.”  “Somebody loves in vain.”  relation  Deﬁnition of Relation: A like Loves p1, p2  states for every pair of objects  say p1 = Sam and p2 = Mary  that the re- lation either holds between them or does not. Though we will use the word relation, Loves p1, p2  is also considered to be a pred- icate. The difference is that a predicate takes only one argument and hence focuses on whether the property is true or false about the given tuple  cid:4 p1, p2 cid:5  =  cid:4 Sam, Mary cid:5 .  Representations: Relations  predicates  can be represented in a number of ways.  Functions: A relation can be viewed as a function mapping tuples of ob- jects either to true or to false, for example, Loves : {p1p1 is a person } × {p2p2 is a person } ⇒ {true, false}. Set of Tuples: Alternatively, it can be viewed as a set containing the tuples for which it is true, for example Loves = { cid:4 Sam, Mary cid:5 ,  cid:4 Sam, Ann cid:5 ,  cid:4 Bob, Ann cid:5 , . . .}.  cid:4 Sam, Mary cid:5  ∈ Loves iff Loves Sam, Mary  is true. Directed Graph Representation: If the relation only has two arguments, it can be represented by a directed graph. The nodes consist of the objects in the domain. We place a directed edge  cid:4 p1, p2 cid:5  between pairs for which the relation is true. If the domains for the ﬁrst and second objects are disjoint, then the graph is bi- partite. Of course, the Loves relation could be deﬁned to include Loves Bob, Bob . See Figure 22.1.  Quantiﬁers: You will be using the following quantiﬁers and properties.  The Existential Quantiﬁer: The quantiﬁer ∃ means that there is at least one ob- ject in the domain with the property. This quantiﬁer relates to the Boolean   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes22 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  17:39  Existential and Universal Quantiﬁers  Figure 22.1: A directed graph representa- tion of the Loves relation.  Sam  Bob  Ron  Mary  Ann  Jone  Sam  Mary  Bob  Ann  Ron  Jone  359  operator OR. For example, ∃p1 Loves Sam, p1  ≡ [Loves Sam, Mary  OR Loves Sam, Ann  OR Loves Sam, Bob  OR . . .]. The Universal Quantiﬁer: The quantiﬁer ∀ means that all of the objects in the domain have the property. It relates to the Boolean operator AND. For example, ∀p1 Loves Sam, p1  ≡ [Loves Sam, Mary  AND Loves Sam, Ann  AND Loves Sam, Bob  AND . . .].  Combining Quantiﬁers: Quantiﬁers can be combined. The order of opera- tions is such that ∀p1∃p2 Loves p1, p2  is understood to be bracketed as ∀p1[∃p1 Loves p1, p2 ], i.e., “Every person has the property ‘he loves some other person’.” It relates to the following Boolean formula:  Mary  OR  Ann  Jone  Mary  OR  Ann  Jone  Loves Sam,Mary  Loves Sam,Ann  Loves Sam,Jone   Loves Bob,Mary   Loves Bob,Ann   Loves Bob,Jone   Loves Ron,Mary  Loves Ron,Ann  Loves Ron,Jone   Sam  AND  Bob  Ron  Mary  OR  Ann  Jone  Order of Quantiﬁers: The order of the quantiﬁers matters. For example, if b is the class of boys and g is the class of girls, ∀b∃g Loves b, g  and ∃g∀b Loves b, g  mean different things. The second one states that the same girl is loved by every boy. For it to be true, there needs to be a Marilyn Monroe sort of girl that all the boys love. The ﬁrst statement says that every boy loves some girl. A Marilyn Monroe sort of girl will make this statement true. However, it is also true in a monogamous situation in which every boy loves a different girl. Hence, the ﬁrst statement can be true in more different ways than the second one. In fact, the second statement implies the ﬁrst one, but not vice versa. Deﬁnition of Free and Bound Variables: The statement ∃p2 Loves Sam, p2  means “Sam loves someone.” This is a statement about Sam. Similarly, the state- ment ∃p2 Loves p1, p2  means “p1 loves someone.” This is a statement about person p1. Whether the statement is true depends on who p1 is referring to. The statement is not about p2. The variable p2 is used as a local variable  similar to for i = 1;i <= 10;i + +   to express “someone.” It could be a brother or a friend or a dog. In this expression, we say that the variable p2 is bound, while p1 is free, because p2 has a quantiﬁer and p1 does not.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes22 CUUS154-Edmonds 978 0 521 84931 9  Appendix  March 29, 2008  17:39  Figure 22.2: ∀g∃b∃p  Loves b, g  and Loves b, p  and g  cid:9 = p . On the left is an example of a situation in which the statement is true, and on the right is one in which it is false.  Sam  Bob  360  Mary  Ann  Jone  Sam  Mary  Bob  Ann  True  False  Deﬁning Other Relations: You can deﬁne other relations by giving an expression with free variables. For example, you can deﬁne the unary relation LovesSomeone  p1  ≡ ∃p2 Loves p1, p2 .  Building Expressions: Suppose you wanted to state that every girl has been cheated on, using the Loves relation. It may be helpful to break the problem into three steps.  Step 1. Assuming Other Relations: Suppose you have the relation Cheats Sam, Mary , indicating that Sam cheats on Mary. How would you express the state- ment that every girl has been cheated on? The advantage of using this function is that we can focus on this one part of the statement. We are not claiming that every boy cheats. One boy may have broken every girl’s heart.  Given this, the answer is ∀g∃b Cheats b, g .  Step 2. Constructing the Other Predicate: Here we do not have a Cheats function. Hence, we must construct a sentence from the loves function stating that Sam cheats on Mary. Clearly, there must be someone else involved besides Mary, so let’s start with ∃p. Now, in order for cheating to occur, who needs to love whom?  For simplicity’s sake, let’s assume that cheating means loving more than one per- son at the same time.  Certainly, Sam must love p. He must also love Mary. If he did not love her, then he would not be cheating on her. Must Mary love Sam? No. If Sam tells Mary he loves her dearly and then a moment later he tells Sue he loves her dearly, then he has cheated on Mary regardless of how Mary feels about him. Therefore, Mary does not have to love Sam. In conclusion, we might deﬁne Cheats Sam, Mary  ≡ ∃p  Loves Sam, Mary  and Loves Sam, p  . However, we have made a mistake here. In our example, the other person and Mary cannot be the same person. Hence, we must deﬁne the relation as Cheats Sam, Mary  ≡ ∃p  Loves Sam, Mary  and Loves Sam, p  and p  cid:9 = Mary . Step 3. Combining the Parts: Combining the two relations together gives you ∀g∃b∃p  Loves b, g  and Loves b, p  and p  cid:9 = g . This statement expresses that ev- ery girl has been cheated on. See Figure 22.2.  The Domain of a Variable: Whenever you state ∃g or ∀g, there must be an under- stood set of values that the variable g might take on. This set is called the domain of the variable. It may be explicitly given or implied, but it must be understood. Here   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes22 CUUS154-Edmonds 978 0 521 84931 9  Existential and Universal Quantiﬁers  March 29, 2008  17:39  the domain is “the” set of girls. You must make clear whether this means all girls in the room, all the girls currently in the world, or all girls that have ever existed. For example,  ∀x∃y x × y = 1  states that every value has a reciprocal. It is certainly not true of the domain of in- tegers, because two does not have an integer reciprocal. It seems to be true of the domain of reals. Be careful, however: zero does not have a reciprocal. It would be better to write  361  or equivalently  ∀x  cid:9 = 0, ∃y x × y = 1  ∀x∃y  x × y = 1 OR x = 0 .  The Negation of a Statement: The negation of a statement is formed by putting a negation sign on the left-hand side.  Brackets sometimes help.  A negated statement, however, is best understood by moving the negation as deep  as far right  into the statement as possible. This is done as follows.  Negating AND and OR: A negation on the outside of an AND or an OR statement can be moved deeper into the statement using De Morgan’s law. Recall that the AND is replaced by an OR and the OR is replaced with an AND.  ¬ Loves S, M  AND Loves S, A   iff ¬Loves S, M  OR ¬Loves S, A : The negat- ion of “Sam loves Mary and Ann” is “Either Sam does not love Mary or he does not love Ann.” He can love one of the girls, but not both. A common mistake is to make the negation ¬Loves Sam, Mary  AND ¬Loves Sam, Ann . However, this says that Sam loves neither Mary nor Ann. ¬ Loves S, M  OR Loves S,A   iff ¬Loves S, M  AND ¬Loves S, A : The negation of “Sam either loves Mary or he loves Ann” is “Sam does not love Mary and he does not love Ann.”  Negating Quantiﬁers: Similarly, a negation can be moved past one or more quantiﬁers either to the right or to the left. However, you must then change these quantiﬁers from existential to universal and vice versa. Suppose d is the set of dogs. Then we have:  ¬ ∃d Loves Sam, d    iff ∀d¬Loves Sam, d  : The negation of “There is a dog that Sam loves” is “There is no dog that Sam loves” or “All dogs are unloved by Sam.” A common mistake is to state the negation as ∃d ¬Loves Sam, d  . However, this says that “There is a dog that is not loved by Sam.” ¬ ∀d Loves Sam, d    iff ∃d¬Loves Sam, d  : The negation of “Sam loves ev- ery dog” is “There is a dog that Sam does not love.”   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes22 CUUS154-Edmonds 978 0 521 84931 9  Appendix  362  March 29, 2008  17:39  ¬ ∃b∀d Loves b, d    iff ∀b¬ ∀d Loves b, d    iff ∀b∃d¬Loves b, d  : The nega- tion of “There is a boy who loves every dog” is “There are no boys who love every dog” or “For every boy, it is not the case that he loves every dog” or “For every boy, there is some dog that he does not love.” ¬ ∃d1∃d2 Loves Sam, d1  AND Loves Sam, d2  AND d1  cid:3 = d2  iff ∀d1∀d2 ¬ Loves Sam, d1  AND Loves Sam, d2  AND d1  cid:3 = d2  iff ∀d1∀d2 ¬Loves Sam, d1  OR ¬Loves Sam, d2  OR d1 = d2: The negation of “There are two  distinct  dogs that Sam loves” is “Given any pair of  distinct  dogs, Sam does not love both” or “Given any pair of dogs, either Sam does not love the ﬁrst or he does not love the second, or you gave me the same dog twice.”  The Domain Does Not Change: The negation of ∃x ≥ 5, x + 2 = 4 is ∀x ≥ 5, x + 2  cid:9 = 4. The negation does not begin ∃x < 5 . . .. Both the statement and its nega- tion are about numbers greater than 5. Is there or is there not a number with the property such that x + 2 = 4?  Proving a Statement True: There are a number of seemingly different techniques for proving that an existential or universal statement is true. The core of all these techniques, however, is the same. Personally, I like to view the proof as a strategy for winning a game against an adversary.  Techniques for Proving ∃d Loves Sam,d  :  Proof by Example or by Construction: The classic technique to prove that something with a given property exists is by example. You either directly pro- vide an example, or you describe how to construct such an object. Then you prove that your example has the property. For the above statement, the proof would state “Let d be Fido” and then would prove that Sam loves Fido.  Proof by Adversarial Game: Suppose you claim to an adversary that there is a dog that Sam loves. What will the adversary say? Clearly, he challenges, “Oh, yeah? What dog?” You then meet the challenge by producing a speciﬁc dog d and proving that Loves Sam, d  , that is, that Sam loves d. The statement is true if you have a strategy guaranteed to beat any adversary in this game.   cid:1  If the statement is true, then you can produce some dog d.  cid:1  If the statement is false, then you will not be able to.  Techniques for Proving ∀d Loves Sam,d  :  Proof by Example Does Not Work: Proving that Sam loves Fido is interesting, but it does not prove that he loves all dogs.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes22 CUUS154-Edmonds 978 0 521 84931 9  Existential and Universal Quantiﬁers  March 29, 2008  17:39  363  Proof by Case Analysis: The laborious way of proving that Sam loves all dogs is to consider each dog, one at a time, and prove that Sam loves it.  This method is impossible if the domain of dogs is inﬁnite.  Proof by Arbitrary Example: The classic technique to prove that every ob- ject from some domain has a given property is to let some symbol repre- sent an arbitrary object from the domain and then to prove that that object has the property. Here the proof would begin “Let d be any arbitrary dog.” Because we don’t actually know which dog d is, we must either  1  prove Loves Sam, d   simply from the properties that d has because d is a dog or  2  go back to doing a case analysis, considering each dog d separately.  Proof by Adversarial Game: Suppose you claim to an adversary that Sam loves every dog. What will the adversary say? Clearly he challenges, “Oh, yeah? What about Fido?” You meet the challenge by proving that Sam loves Fido. In other words, the adversary provides a dog d . You win if you can prove that Loves Sam, d   .   cid:1    cid:1   The only difference between this game and the one for existential quan- tiﬁers is who provides the example. Interestingly, the game only has one round. The adversary is only given one opportunity to challenge you. A proof of the statement ∀d Loves Sam, d   consists of a strategy for win- , provided by the .” Again, because we don’t actually   simply has because he is a dog or  2  go back to doing a  cid:1  If the statement ∀d Loves Sam, d   is true, then you have a strategy. No matter how the adversary plays, no matter which dog d he gives you, Sam loves it. Hence, you can win the game by proving that Loves Sam, d  ning the game. Such a strategy takes an arbitrary dog d adversary, and proves that “Sam loves d know which dog d from the properties that d case analysis, considering each dog d  is, we must either  1  prove that Loves Sam, d  separately.   .   cid:1    cid:1    cid:1    cid:1    cid:1    cid:1    cid:1    cid:1    cid:1  If the statement is false, then there is a dog d  that Sam does not love. Any true adversary  not just a friend   will produce this dog, and you will lose the game. Hence, you cannot have a winning strategy.   cid:1   Proof by Contradiction: A classic technique for proving the statement ∀d Loves Sam, d   is proof by contradiction. Except in the way that it is ex- pressed, it is exactly the same as the proof by an adversary game. ∃d ¬Loves Sam, d   is true. Let d Then you must prove that in fact Sam does love d  cid:1  ment that Sam does not love d ∀d Loves Sam, d   is true.  By way of contradiction assume that the statement is false, that is, be some such dog that Sam does not love. . This contradicts the state- . Hence, the initial assumption is false, and   cid:1    cid:1   Proof by Adversarial Game for More Complex Statements: The advantage to this technique is that it generalizes into a nice game for arbitrarily long statements.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes22 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  17:39  Appendix  The Steps of the Game:  364  Left to Right: The game moves from left to right, providing an object for each quantiﬁer. Prover Provides ∃b: You, as the prover, must provide any existential ob- jects. Adversary Provides ∀d : The adversary provides any universal objects.  To Win, Prove the Relation Loves b  : Once all the objects have been provided, you  the prover  must prove that the innermost relation is in fact true. If you can, then you win. Otherwise, you lose.   cid:4    cid:4 , d  A Proof Is a Strategy: A proof of the statement consists of a strategy such that you win the game no matter how the adversary plays. For each possible move that the adversary takes, such a strategy must specify what move you will counter with.  Negations in Front: To prove a statement with a negation in the front of it, ﬁrst put the statement into standard form with the negation moved to the right. Then prove the statement in the same way.  Examples:   cid:1    cid:1   ∃b∀d Loves b,d  : To prove “There is a boy that loves every dog,” you  cid:1  must produce a speciﬁc boy b . Then the adversary, knowing your boy , tries to prove that ∀d Loves b  cid:1   cid:1  b , d   is false. He does this by providing  cid:1  an arbitrary dog d does not love. You must prove “b loves d ¬ ∃b∀d Loves b,d    iff ∀b∃d¬Loves b, d  : With the negation moved to the right, the ﬁrst quantiﬁer is universal. Hence, the adversary ﬁrst pro-  cid:1  duces a boy b . Finally, you prove that ¬Loves b  . Then, knowing the adversary’s boy, you produce a dog d  that he hopes b  , d  .”   .   cid:1    cid:1    cid:1    cid:1   Your proof of the statement could be viewed as a function G that given by the adversary and outputs the dog   is an example of a dog that    countered by you. Here, d  does not love. The proof must prove that ∀b¬Loves b, D b  .  takes as input the boy b  cid:1  = D b d boy b   cid:1  = D b   cid:1    cid:1    cid:1    cid:1   EXERCISE 22.0.1 Let Loves b, g  denote that boy b loves girl g. If Sam loves Mary and Mary does not love Sam back, then we say that Sam loves in vain.  1. Express the following statements using universal and existential quantiﬁers. Move  any negations to the right.  a  “Sam has loved in vain.”  b  “There is a boy who has loved in vain.”   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes22 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  17:39  Existential and Universal Quantiﬁers   c  “Every boy has loved in vain.”  d  “No boy has loved in vain.”  2. For each of the above statements and each of the two relations below, prove either  that the statement is true for the relation or that it is false:  Sam  Mary  Mary  365  Sam  Bob  EXERCISE 22.0.2  See solution in Part Five.  For each, prove whether true or not when each variable is a real value. Be sure to play the correct game as to who is providing what value: 1. ∀x ∃y x + y = 5. ∃y ∀x x + y = 5. 2. 3. ∀x ∃y x · y = 5. ∃y ∀x x · y = 5. 4. 5. ∀x ∃y x · y = 0. ∃y ∀x x · y = 0. 6. [∀x ∃y P x, y ] ⇒ [∃y ∀x P x, y ]. 7. [∀x ∃y P x, y ] ⇐ [∃y ∀x P x, y ]. 8. 9. ∀a ∃y ∀x x ·  y + a  = 0. ∃a ∀x ∃y [x = 0 or x · y = 5]. 10.  EXERCISE 22.0.3 The game ping has two rounds. Player A goes ﬁrst. Let mA his ﬁrst move. Player B goes next. Let mB player B goes mB these moves.  1 denote 2 , and 2   is true iff player A wins with  1 denote his move. Then player A goes mA  2 . The relation A Wins mA  2 , mB  1 , mB  1 , mA  1. Use universal and existential quantiﬁers to express the fact that player A has a 2 as  strategy with which he wins no matter what player B does. Use mA variables.  2 , mB  1 , mB  1 , mA  2. What steps are required in the prover–adversary technique to prove this statement? 3. What is the negation of the above statement in standard form? 4. What steps are required in the prover–adversary technique to prove this negated  statement?  EXERCISE 22.0.4 Why does [∀n0, ∃n > n0, P n ] imply that there are an inﬁnite number of values n for which the property P n  is true?   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes23 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  20:47  366 23 Time Complexity  It is important to classify algorithms based whether they solve a given computational problem and, if so, how quickly. Similarly, it is important to classify computational problems based whether they can be solved and, if so, how quickly.  23.1 The Time  and Space  Complexity of an Algorithm  Purpose:  Estimate Duration: To estimate how long an algorithm or program will run.  Estimate Input Size: To estimate the largest input that can reasonably be given to the program.  Compare Algorithms: To compare the efﬁciency of different algorithms for solv- ing the same problem.  Parts of Code: To help you focus your attention on the parts of the code that are executed the largest number of times. This is the code you need to improve to reduce the running time.  Choose Algorithm: To choose an algorithm for an application:   cid:1  If the input size won’t be larger than six, don’t waste your time writing an   cid:1  If the input size is a thousand, then be sure the program runs in polynomial,  extremely efﬁcient algorithm.  not exponential, time.   cid:1  If you are working on the Gnome project and the input size is a billion, then  be sure the program runs in linear time.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes23 CUUS154-Edmonds 978 0 521 84931 9  Time Complexity  April 5, 2008  20:47  Time and Space Complexities Are Functions, T n  and S n : The time com- plexity of an algorithm is not a single number, but is a function indicating how the running time depends on the size of the input. We often denote this by T n , giving the number of operations executed on the worst case input instance of size n. An example would be T n  = 3n2 + 7n + 23. Similarly, S n  gives the size of the rewritable memory the algorithm requires.  367  Ignoring Details,  cid:1  T n   and O T n  : Generally, we ignore the low-order terms in the function T n  and the multiplicative constant in front. We also ignore the func- tion for small values of n and focus on the asymptotic behavior as n becomes very large. Some of the reasons are the following.  Model-Dependent: The multiplicative constant in front of the time depends on how fast the computer is and on the precise deﬁnition of “size” and “operation.”  Too Much Work: Counting every operation that the algorithm executes in pre- cise detail is more work than it is worth.  Not Signiﬁcant: It is much more signiﬁcant whether the time complexity is T n  = n2 or T n  = n3 than whether it is T n  = n2 or T n  = 3n2.  Only Large n Matter: One might say that we only consider large input instances in our analysis, because the running time of an algorithm only becomes an issue when the input is large. However, the running time of some algorithms on small input instances is quite critical. In fact, the size n of a realistic input instance depends both on the problem and on the application. The choice was made to consider only large n in order to provide a clean and consistent mathematical deﬁnition.  See Chapter 25 on the Theta and BigOh notations.  Deﬁnition of Size: The formal deﬁnition of the size of an instance is the number of binary digits  bits  required to encode it. More practically, the size could be con- sidered to be the number of digits or characters required to encode it. Intuitively, the size of an instance could be deﬁned to be the area of paper needed to write down the instance, or the number of seconds it takes to communicate the instance along a narrow channel. These deﬁnitions are all within a multiplicative constant of each other.  An Integer: Suppose that the input is the value N = 8,398,346,386,236,876. The number of bits required to encode it is Size N  = log2 N  = log2 8,398,346, 386,236,876  = 53, and the number of decimal digits is Size N  = log10 8,398, 346,386,236,876  = 16. Chapter 24 explains why these are within a multiplica- tive constant of each other. The one deﬁnition that you must not use is the value   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes23 CUUS154-Edmonds 978 0 521 84931 9  Appendix  368  April 5, 2008  20:47  of the integer, Size N  = N = 8,398,346,386,236,876, because it is exponentially different than Size N  = log2 N  = 53. A Tuple: Suppose that the input is the tuple of b integers I =  cid:2 x1, x2, . . . , xb cid:3 . The number of bits required to encode it is Size I   = log2 x1  + log2 x2  + ··· + log2 xb  ≈ log2 xi  · b. A natural deﬁnition of the size of this tuple is the number of integers in it, Size I   = b. With this deﬁnition, it is a much stronger statement to say that an algorithm requires only Time b  integer operations independent of how big the integers are. A Graph: Suppose that the input is the graph G =  cid:2 V, E cid:3  with V nodes and E edges. The number of bits required to encode it is Size G  = 2 Size node  · E = 2 log2 V  · E. Another reasonable deﬁnition of the size of G is the number of edges, G n  = E. Often the time is given as a function of both V and E. This is within a log factor of the other deﬁnitions, which for most applications is ﬁne.  Deﬁnition of an Operation: The deﬁnition of an operation can be any reasonable operation on two bits, characters, nodes, or integers, depending on whether time is measured in bits, characters, nodes, or integers. An operation could also be deﬁned to be any reasonable line of code or the number of seconds that the computation takes on your favorite computer.  Which Input: T n  is the number of operations required to execute the given algo- rithm on an input of size n. However, there are 2n input instances with n bits. Here are three possibilities:  A Typical Input: The problem with considering a typical input instance this is that different applications will have very different typical inputs.  Average or Expected Case: The problem with taking the average over all input instances of size n is that it assumes that all instances are equally likely to occur.  Worst Case: The usual measure is to consider the instance of size n on which the given algorithm is the slowest, namely, T n  = maxI∈{I  I=n} Time I  . This mea- sure provides a nice clean mathematical deﬁnition and is the easiest to analyze. The only problem is that sometimes the algorithm does much better than the worst case, because the worst case is not a reasonable input. One such algorithm is quick sort  see Section 9.1 .  Time Complexity of a Problem: The time complexity of a problem is the running time of the fastest algorithm that solves the problem.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes23 CUUS154-Edmonds 978 0 521 84931 9  Time Complexity  April 5, 2008  20:47  EXAMPLE 23.1  Polynomial Time vs Exponential Time  Suppose program P1 requires T1 n  = n4 operations and P2 requires T2 n  = 2n. Sup- pose that your machine executes 106 operations per second. If n = 1,000, what is the running time of these programs?  Answer:  1. T1 n  =  1,000 4 = 1012 operations, requiring 106 seconds, or 11.6 days. 2. T2 n  = 2 103  operations. The number of years is 106×60×60×24×365 . This is too big for my calculator. The log of this number is 103 × log10 2  − log10 106 − log10 60 × 60 × 24 × 365  = 301.03 − 6 − 7.50 = 287.53. Therefore, the number of years is 10287.53 = 3.40 × 10287. Don’t wait for it.  2 103   369  EXAMPLE 23.2  Instance Size N vs Instance Value N  Two simple algorithms, summation and factoring.  The Problems and Algorithms:  Summation: The task is to sum the N entries of an array, that is, A 1  + A 2  + A 3  + ··· + A N . Factoring: The task is to ﬁnd divisors of an integer N. For example, on input N = 5917 we output that N = 97 × 61.  This problem is central to cryptography.  The algorithm checks whether N is divisible by 2, by 3, by 4, . . . by N.  Time: Both algorithms require T = N operations  additions or divisions . How Hard? The summation algorithm is considered to be very fast, while the factoring algorithm is considered to be very time-consuming. However, both algorithms take T = N time to complete. The time complexity of these algorithms will explain why. Typical Values of N: In practice, the N for factoring is much larger than that for sum- mation. Even if you sum all the entries on the entire 8-G byte hard drive, then N is still only N ≈ 1010. On the other hand, the military wants to factor integers N ≈ 10100. However, the measure of complexity of an algorithm should not be based on how it happens to be used in practice. Size of the Input: The input for summation contains is n ≈ 32N bits. The input for factoring contains is n = log2 N bits. Therefore, with a few hundred bits you can write down a difﬁcult factoring instance that is seemingly impossible to solve. Time Complexity: The running time of summation is T n  = N = 1 32n, which is linear in its input size. The running time of factoring is T N  = N = 2n, which is exponential in its input size. This is why the summation algorithm is considered to be feasible, while the factoring algorithm is considered to be infeasible.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes23 CUUS154-Edmonds 978 0 521 84931 9  Appendix  April 5, 2008  20:47  EXERCISE 23.1.1  See solution in Part Five.  For each of the two programs considered in Example 23.1, if you want it to complete in 24 hours, how big can your input be?  EXERCISE 23.1.2 In Example 23.1, for which input size, approximately, do the pro- grams have the same running times?  370  EXERCISE 23.1.3 This problem compares the running times of the following two al- gorithms for multiplying:  algorithm KindergartenAdd a, b   cid:2  pre-cond cid:3 : a and b are integers.  cid:2  post-cond cid:3 : Outputs a × b. begin  c = 0 loop i = 1 . . . a c = c + b  end loop return c  end algorithm  c = 0 loop i = 1 . . . s  loop j = 1 . . .t  c = c + aib j × 10i+j .  end loop  end loop return c  end algorithm  algorithm GradeSchoolAdd a, b   cid:2  pre-cond cid:3 : a and b are integers.  cid:2  post-cond cid:3 : Outputs a × b. begin  Let as−1 . . .a3a2a1a0 be the decimal digits of a, so that a = cid:1  Let bt−1 . . . b3b2b1b0 be the decimal digits of b, so that b = cid:1   i=0 ai × 10i. s−1 j=0 b j × 10j . t−1  For each of these algorithms, answer the following questions.  1.  Suppose that each addition to c requires time 10 seconds and every other operation  for example, multiplying two single digits such as 9 × 8 and shifting by zero  is free. What is the running time of each of these algorithms, either as a function of a and b or as a function of s and t? Give everything for this entire question exactly, i.e., not BigOh.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes23 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  20:47  Time Complexity 2. Let a = 9,168,391 and b = 502.  Without handing it in, trace the algorithm.  With 10 seconds per addition, how much time  seconds, minutes, etc.  does the compu- tation require? 3. The formal size of an input instance is the number n of bits needed to write it down. What is n as a function of our instance  cid:2 a, b cid:3 ? Suppose your job is to choose the worst case instance  cid:2 a, b cid:3   i.e., the one that max- imizes the running time , but you are limited in that you can only use n bits to represent your instance. Do you set a big and b small, a small and b big, or a and b the same size? Give the worst case a and b, or s and t, as a function of n.  4.  5. The running time of an algorithm is formally deﬁned to be a function T n  from n to the time required for the computation on the worst case instance of size n. Give T n  for each of these algorithms. Is this polynomial time?  371  EXERCISE 23.1.4  See solution in Part Five.  Suppose that someone has developed an algorithm to solve a certain problem, which runs in time T n, k  ∈  cid:1  f  n, k  , where n is the size of the input, and k is a parameter we are free to choose  we can choose it to depend on n . In each case determine the value of the parameter k n  to achieve the  asymptotically  best running time. Justify your answer. I recommend not trying much fancy math. Think of n as being some big ﬁxed number. Try some value of k, say k = 1, k = na , or k = 2an for some constant a. Then note whether increasing or decreasing k increases or decreases f . Recall that “asymptotically” means that we only need the minimum to within a multiplicative constant. 1. You might want to ﬁrst prove that g + h =  cid:1  max g, h  . 2.  log k . This is needed for the radix–counting sort in Section 5.4.  f  n, k  = n+k f  n, k  = n3 + k · n. f  n, k  = log3 k + 2n k . f  n, k  = 8nn2  k  k  + k · 2n + k2.  3. 4. 5.  23.2 The Time Complexity of a Computational Problem  The Formal Deﬁnition of the Time Complexity of a Problem: As said, the time complexity of a problem is the running time of the fastest algorithm that solves the problem. We will now deﬁne this more carefully, using the existential and universal quantiﬁers that were deﬁned in Chapter 22.  The Time Complexity of a Problem: The time complexity of a computational problem P is the minimum time needed by an algorithm to solve it.  Upper Bound: Problem P is said to be computable in time Tupper  n  if there is an algorithm A that outputs the correct answer, namely A I   = P I  , within   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes23 CUUS154-Edmonds 978 0 521 84931 9  Appendix  372  April 5, 2008  20:47  the bounded time, namely Time A , I   ≤ Tupper  I , on every input instance I . The formal statement is  ∃A , ∀I, [A I   = P I   and Time A , I   ≤ Tupper I ]  Tupper n  is said to be only an upper bound on the complexity of the problem P, because there may be another algorithm that runs faster. For example, P = Sorting is computable in Tupper n  = O n2  time. It is also computable in Tupper n  = O n log n .  Lower Bound of a Problem: A lower bound on the time needed to solve a problem states that no matter how smart you are, you cannot solve the prob- lem faster than the stated time Tlower n , because such algorithm simply does not exist. There may be algorithms that give the correct answer or run sufﬁ- ciently quickly on some input instances. But for every algorithm, there is at least one instance I for which either the algorithm gives the wrong answer, i.e., A I    cid:9 = P I  , or it takes too much time, i.e., Time A , I   ≥ Tlower I . The formal statement is the negation  except for ≥ vs >  of that for the upper bound:  ∀A , ∃I, [A I    cid:9 = P I   or Time A , I   ≥ Tlower I ]  Tlower = √  For example, it should be clear that no algorithm can sort n values in only n time, because in that much time the algorithm could not even  look at all the values.  Proofs Using the Prover–Adversary Game: Recall the technique described in Chapter 22 for proving statements with existential and universal quantiﬁers.  Upper Bound: We can use the prover–adversary game to prove the upper bound statement ∃A , ∀I, [A I   = P I   and Time A , I   ≤ Tupper I ] as fol- lows: You, the prover, provide the algorithm A. Then the adversary provides an input I . Then you must prove that your A on input I gives the correct out- put in the allotted time. Note this is what we have been doing throughout the book: providing algorithms and proving that they work. Lower Bound: A proof of the lower bound ∀A , ∃I, [A I    cid:9 = P I   or Time  A , I   ≥ Tlower I ] consists of a strategy that, when given an algorithm A by an adversary, you, the prover, study his algorithm and provide an input I . Then you prove either that his A on input I gives the wrong output or that it runs in more than the allotted time.  EXERCISE 23.2.1  See solution in Part Five.  Let Wor ks P, A , I   to true if algorithm A halts and correctly solves problem P on input instance I . Let P = Halting be the halting problem that takes a Java program I as input and tells you whether or not it halts on the empty string. Let P = Sorting be the sorting problem that takes a list of   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes23 CUUS154-Edmonds 978 0 521 84931 9  Time Complexity  April 5, 2008  20:47  numbers I as input and sorts them. For each part, explain the meaning of what you are doing and why you don’t do it another way.  1. Recall that a problem is computable if and only if there is an algorithm that halts and returns the correct solution on every valid input. Express in ﬁrst-order logic that Sorting is computable.  either prove or disprove it: ∀I, ∃A , Works Halting, A , I  .  2. Express in ﬁrst-order logic that Halting is not computable. 3. Express in ﬁrst-order logic that there are uncomputable problems. 4. Explain what the following means  not simply by saying the same in words , and 5. Explain what the following means, and either prove or disprove it: ∀A , ∃P, ∀I, Works P, A , I  .  Hint: An algorithm A on an input I can either halt and give the correct answer, halt and give the wrong answer, or run forever.   373   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes24 CUUS154-Edmonds 978 0 521 84931 9  March 25, 2008  14:30  374 24 Logarithms and Exponentials  Logarithms log2 n   and exponentials 2n arise often when analyzing algorithms.  Uses: These are some of the places that you will see them.  Divide a Logarithmic Number of Times: Many algorithms repeatedly cut the in- put instance in half. A classic example is binary search  Section 1.4 : You take something of size n and you cut it in half, then you cut one of these halves in half, and one of these in half, and so on. Even for a very large initial object, it does not take very long until you get a piece of size below 1. The number of divi- sions required is about log2 n  . Here the base 2 is because you are cutting them in half. If you were to cut them into thirds, then the number of times to cut would be about log3 n  .  A Logarithmic Number of Digits: Logarithms are also useful because writing down a given integer value n requires  cid:2 log10 n + 1  cid:3  decimal digits. For example, suppose that n = 1,000,000 = 106. You would have to divide this number by 10 six times to get to 1. Hence, by deﬁnition, log10 n   = 6. This, however, is the num- ber of zeros, not the number of digits. We forgot the leading digit 1. The formula  cid:2 log10 n + 1  cid:3  = 7 does the trick. For the value n = 6,372,845, the number of dig- its is given by log10 6,372,846  = 6.804333, rounded up to 7. Being in computer science, we store our values using bits. Similar arguments give that  cid:2 log2 n + 1  cid:3  is the number of bits needed.  Height and Size of Binary Tree: A complete balanced binary tree of height h has 2h leaves and n = 2h+1 − 1 nodes. Conversely, if it has n nodes, then its height is h ≈ log2 n.  Exponential Search: Suppose a solution to your problem is represented by n dig- its. There are 10n such strings of n digits. Doing a blind search through them all would take too much time.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes24 CUUS154-Edmonds 978 0 521 84931 9  Logarithms and Exponentials  March 25, 2008  14:30  n   cid:4    cid:2  cid:3   Rules: There are lots of rules about logs and exponentials that one might learn. Per- sonally, I like to conﬁne them to the following:   cid:1  b× b× b× ··· × b: This is the deﬁnition of exponentiation. bn is n b’s mul-  bn = tiplied together. bn × bm = bn+m : This is proved simply by counting the number of b’s being multiplied:   cid:1  b × b × b × ··· × b  ×       cid:2  cid:3    cid:4   n   cid:4   cid:1  b × b × b × ··· × b  =   cid:2  cid:3   m   cid:4   cid:1  b × b × b × ··· × b .   cid:2  cid:3   n+m  375  n: By deﬁnition,  √ n is the positive number that when multiplied by itself  b0 = 1: One might guess that zero b’s multiplied together is zero, but it needs to be one. One argument for this is as follows. bn = b0+n = b0 × bn. For this to be true, b0 must be one. 2 = √ b1 gives n. b 1 −n = 1 bn : The fact that this needs to be true can be argued in a similar way. b 1 = bn+ −n   = bn × b  bn m = bn×m : Again we count the number of b’s:  cid:4   −n. For this to be true, b  cid:2  cid:3   2 meets this deﬁnition because b 1  −n must be 1 bn.  2 = b1 = b. + 1   cid:1  b × b × b × ··· × b  × ··· ×     cid:4   cid:4   cid:1  b × b × b × ··· × b   2 = b 1  2 × b 1   cid:2  cid:3    cid:2  cid:3    cid:2  cid:3    cid:4   m  n  n  2  n   cid:1   cid:1  b × b × b × ··· × b  ×      cid:4   cid:1  b × b × b × ··· × b .   cid:2  cid:3   n×m  =  If x = logb n  then n = b x: This is the deﬁnition of logarithms. logb 1  = 0: This follows from b0 = 1. logb b x  = x and blogb n  = n: Substituting n = bx into x = logb n   gives the ﬁrst, and substituting x = logb n   into n = bx gives the second. logb n× m  = logb n  + logb m : The number of digits to write down the prod- uct of two integers is the number to write down each of them separately  up to rounding errors . We prove it by applying the deﬁnition of logarithms and the above rules: blogb n×m   = n × m = blogb n   × blogb m   = blogb n  +logb m  . It follows that logb n × m  = logb n   + logb m . logb nd  = d× logb n : This is an extension of the above rule. logb n  − logb m  = logb n  + logb  1 above rule. d c log2 n  = nc log2 d : This rule states that you can move things between the base and the exponent as long as you insert or remove a log. The proof is as follows.  m : This is another extension of the  m  = logb  n   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes24 CUUS154-Edmonds 978 0 521 84931 9  March 25, 2008  14:30  Appendix  dc log2 n   =  2log2 d  c log2 n   = 2log2 d ×c log2 n   = 2log2 n  ×c log2 d  =  2log2 n   c log2 d  = nc log2 d . log2 n  = 3.32 . . . × log10 n : The number of bits needed to express the integer n is 3.32. . . times the number of decimal digits needed. This can be seen as follows. Suppose x = log2 n. Then n = 2x, giving log10 n = log10 2x  = x · log10 2. Finally,  376  x =  1  log10 2  log10 n   = 3.32 . . . log10 n  Which Base: We will write  cid:1  log n    without giving an explicit base. A high school student might use base 10 as the default, a scientist base e = 2.718 . . . , and a com- puter scientist base 2. My policy is to exclude the base when it does not matter. As seen above, log10 n  , log2 n  , and loge n   differ only by multiplicative constants. In general, we ignore multiplicative constants, and hence the base used is irrelevant. I only include the base when the base matters. For example, 2n and 10n differ by much more than a multiplicative constant.  The Ratio log a log b: When computing the ratio between two logarithms, the base used does not matter, because changing the base will introduce the same constant on both the top and the bottom, which will cancel. Hence, when computing such a ratio, you can choose whichever base makes the calculation the easiest. For example, to comp- = 4 ute log 16 3 . On the other hand, to com- = 2 pute log 9 3 . EXERCISE 24.0.1  See solution in Part Five.  Simplify the following exponentials: a 3 × a 5, 3a × 5a , 3a + 5a , 26 log4 n+7, n3  log2 n.  log 8 , the obvious base to use is 2, because log2 16 log2 8 log 27 , the obvious base to use is 3, because log3 9 log3 27   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes25 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  14:34  25 Asymptotic Growth  377  Classes of Growth Rates: It is important to be able to classify functions f  n   based on how quickly they grow: The following table outlines the few easy rules with which to classify functions with the basic form f  n  =  cid:1  ban · nd · logen .  c > 0  ba > 1 = 1  Class Exponentials Polynomials:  cid:1  Quadratic  cid:1  Sorting time  cid:1  Linear  Polylogarithms:  cid:1  Logarithms  Constants  d Any > 0 = 2 = 1 = 1 = 0  < 0  Any  e Any Any  Any = 1 = 0 > 0 = 1 = 0 Any  Any  < 1  Decreasing polynomials  Decreasing exponentials   cid:1  2 cid:1  n   n cid:1  1   cid:1  n2   cid:1  n log n     cid:1  n   log cid:1  1  n    cid:1  log n     cid:1  1   1  1  n cid:1  1   2 cid:1  1   Examples 2n, 30.001n n100 n4, 5n0.0001 log100 n   5n2, 2n2 + 7n + 8 5n log n + 3n 5n + 3 5 log3 n   5 log n   5, 5 + sin n n4 , 5 log100 n   2n , n100 30.001n  n0.0001  1  1  Asymptotic Notation: When we want to bound the growth of a function while ignoring multiplicative constants, we use the following notation:  Name  Theta BigOh Omega  Standard Notation My Notation f  n   ∈  cid:1  g n    f  n   =  cid:1  g n    f  n   ≤ O g n    f  n   = O g n    f  n   ≥  cid:2  g n    f  n   =  cid:2  g n     Meaning f  n   ≈ c · g n   f  n   ≤ c · g n   f  n   ≥ c · g n     P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes25 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  14:34  Appendix  Purpose:  Time Complexity: Generally, the functions that we will be classifying will be the time or space complexities of programs. On the other hand, these ideas can also be used to classify any function.  378  Function Growth: The purpose of classifying a function is to give an idea of how fast it grows without going into too much detail.  Asymptotic Growth Rate: When classifying animals, Darwin decided not to con- sider whether the animal sleeps during the day, but to consider whether it has hair. When classifying functions, complexity theorists decided to not consider its behav- ior for small values of n or even whether it is monotone increasing, but how quickly it grows when its input n grows really big. This is referred to as the asymptotics of the function. Here are some examples of different growth rates:  Function T n   5 √ log2 n n  n n log n n2 n3 2n  Approximate value of T n  for n =  10  100  1,000  10,000  Animal  5 3 3 10 30 100 1,000 1,024  5 6 10 100 600 10,000 106 1030  5 9 31 1,000 9,000 106 109 10300  5 13 100 10,000 130,000 108 1012 103000  Virus Amoeba Bird Human Giant Elephant Dinosaur The universe  Note: The universe contains approximately 1080 particles.  Exponential vs Polynomial: The table shows that an exponential function like f  n   = 2n grows extremely quickly. In fact, for sufﬁciently big n, this exponential 2n grows much faster than any polynomial, even n1,000,000. To take this to an ex- treme, the function f  n   = 20.001n is also an exponential. It too grows much faster than n1,000,000 for sufﬁciently large n.  Polynomial vs Logarithmic: The table also shows that a logarithmic function like f  n   = log2 n grows, but very slowly. Hence, for sufﬁciently large n, it is bigger than any constant, but smaller than any polynomial.  EXERCISE 25.0.1 Give a value of n for which n1,000,000 < 10n.  Give a value of n for which n1,000 < 100.001n.  EXERCISE 25.0.2 Give a value of n for which  log10 n  1,000,000 < n.  Give a value of n for which  log10 n  1,000 < n0.001.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes25 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  14:34  Asymptotic Growth  25.1 Steps to Classify a Function  Given a function f  n  , we will classify it according to its growth using the following steps.  1  Put f  n  into Basic Form: Though there are strange functions out there, most functions f  n   can be put into a basic form consisting of the sum of a number of terms, where each term has the basic form c · ban · nd ·  log n  e, where a, b, c, d, and e are real constants.  379  Examples:   cid:1  If f  n   = 3 · 24n · n7 ·  log n  5, then a = 4, b = 2, c = 3, d = 7, and e = 5.  cid:1  Suppose f  n   = n2. This has no exponential part ban, but can be viewed as having a = 0, or b = 1, or ab = 1.  Recall x0 = 1 and 1x = 1.  The exponent on the polynomial n is d = 2. There is no logarithmic factor, so we have e = 0. Finally, the constant in front is c = 1.  cid:1  In f  n   = 1 n6 = n  cid:1  If f  n   = n2  log n + 5, then the function has two terms. In the ﬁrst, ab = 1, c = 1, d = 2, and e = −1. In the second, ab = 1, c = 5, d = 0, and e = 0.  −6, it is also useful to see that d = −6.  2  Get the Big-Picture Growth: We classify the set of all vertebrate animals into mammals, birds, reptiles, and so on. Similarly, we will classify functions into the ma- jor groups exponentials 2 cid:1  n  , polynomials n cid:1  1 , polylogarithms log cid:1  1  n  , and con- stants  cid:1  1 .  Exponentials 2 cid:1  n : If the function f  n   is the sum of a bunch of things, one of which is c · ban · nd ·  log n  e, where ba > 1, then f  n   is considered to be an ex- ponential.  Examples Included:   cid:1  f  n   = 2n and f  n   = 35n  cid:1  f  n   = 2n · n2 log2 n − 7n8 and f  n   = 2n −1·n = cid:1   Examples Not Included:  cid:1  f  n   = 1n = 20·n = 1, small   cid:1  f  n   = n! ≈ nn = 2n log2 n and f  n   = 2n2  too big   f  n   = 2  n2  1 2   cid:2 n, and f  n   = n1,000,000   too  Deﬁnition of an Infeasible Algorithm: An algorithm is considered to be in- feasible if it runs in exponential time. This is because such functions grow extremely quickly as n gets larger.  ba n: We require b a > 1 because ban =  ba  n, which grows as long as the base ba is at least one.  The Notation 2 cid:1  n : We will see later that  cid:1  1  denotes any constant greater than zero. The notation 2 cid:1  n   = 2 cid:1  1 ·n is used to represent the class of   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes25 CUUS154-Edmonds 978 0 521 84931 9  Appendix  March 29, 2008  14:34  380  exponentials, because ban = 2 a log2 b ·n and the constant a log2 b = log2 ba   is greater than zero as long as ba is greater than 1. Recall log2 1 = 0. Bounded Between: By these rules, f  n   = c · ban · nd ·  log n  e is exponential if ba > 1, no matter what the constants c, d, e, and f are. Consider f  n   = 2n n100. The rule states that it is an exponential because ba = 21 > 1 and d = −100. We might question this, thinking that dividing by n100 would not let it grow faster enough to be considered to be an exponential. We see that it does grow fast enough by proving that it is bounded between the two exponential functions 20.5n and 2n.  Polynomial n  cid:1  1 : If f  n   = c · ban · nd ·  log n  e is such that ba = 1, then we can ignore ban, giving f  n   = c · nd ·  log n  e. If d > 0, then the function f  n   is con- sidered to be a polynomial.  Examples Included:   cid:1  f  n   = 3n2 and f  n   = 7n2 − 8n log n + 2n − 17  cid:1  f  n   = √ n = n1 2 and f  n   = n3.1  cid:1  f  n   = n2 log2 n and f  n   = n2  cid:1  f  n   = 7n3 log7 n − 8n2 log n + 2n − 17  log2 n  Examples Not Included:   cid:1  f  n   = n0 = 1, f  n   = n  cid:1  f  n   = nlog n and f  n   = 2n  too big   −1 = 1  n , and f  n   = log n  too small   Deﬁnition of a Feasible Algorithm: An algorithm is considered to be feasible if it runs in polynomial time.  This is not actually true if f  n   = n1,000,000.   The Notation n cid:1  1 :  cid:1  1  denotes any constant greater than zero, and hence n cid:1  1  represents any function f  n   = nd where d > 0.  Bounded Between: Though it would not be considered one in a mathemat- ical study of polynomials, we also consider f  n   = 3n2 log n to be a poly- nomial, because it is bounded between n2 and n3, which clearly are poly- nomials.  Polylogarithms log cid:1  1  n : Powers of logs like  log n 3 are referred to as poly- logarithms. These are often written as log3 n =  log n 3. This is different than log n3  = 3 log n.  Example Included:   cid:3   cid:1  f  n   = 7 log2 n  5, f  n   = 7 log2 n, and f  n   = 7  cid:1  f  n   = 7 log2 n  5 + 6 log2 n  3 − 19 + 7 log2 n  2 n  log2 n   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes25 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  14:34  Asymptotic Growth  Example Not Included:  cid:1  f  n   = n  too big   Constants  cid:1  1 : A constant function is one whose output does not depend on its input, for example, f  n   = 7. One algorithm for which this function arises is popping an element off a stack that is stored as a linked list. This takes maybe seven operations, independent of the number n of elements in the stack.  381  The Notation n  cid:1  1 : We use the notation  cid:1  1  to replace any constant when we do not care what the actual constant is because determining whether it is 7, 9, or 8.829 may be more work and more detail than we need. On the other hand, in most applications being negative [f  n   = −1] or zero [f  n   = 0] would be quite a different matter. Hence, these are excluded. Bounded Between: A function like f  n   = 8 + sin n changes continuously between 7 and 9, and f  n   = 8 + 1 n changes continuously on approaching 8. However, if we don’t care whether it is 7, 9, or 8.829, why should we care if it is changing between them? Hence, both of these functions are included in  cid:1  1 . On the other hand, the function f  n   = 1 n is not included, because the only constant that it is bounded below by is zero and the zero function is not included.  Examples Included:   cid:1  f  n   = 7 and f  n   = 8.829  cid:1  f  n   = 8 + sin n, f  n   = 8 + 1  n  Examples Not Included:   cid:1  f  n   = −1 and f  n   = 0  fails c > 0   cid:1  f  n   = sin n  fails c > 0   cid:1  f  n   = 1 n  too small   cid:1  f  n   = log2 n  too big   3  Determine  cid:1   f  n  : We further classify mammals into humans, cats, dogs, and so on. Similarly, we further classify the polynomials n cid:1  1  into linear functions  cid:1  n  , the time for sorting  cid:1  n log n  , quadratics  cid:1  n2 , and so on. These are classes that ignore the multiplicative constant.  Steps: One “takes the Theta” of a function f  n   by dropping the low-order terms and then dropping the multiplicative constant c in front of the largest term.  Dropping Low-Order Terms: If f  n   is a set of things added or subtracted to- gether, then each of these things is called a term. We determine which of the terms grows fastest as n gets large. The slower-growing terms are referred to as low-order terms. We drop them because they are not signiﬁcant.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes25 CUUS154-Edmonds 978 0 521 84931 9  Appendix  March 29, 2008  14:34  Ordering Terms: The fastest-growing term is determined by ﬁrst taking the term c · ban · nd ·  log n  e with the largest ba value. If the ba ’s of terms are equal, then we take the term with the largest d value. If the d’s are also equal, then we take the term with the largest e value.  382  Dropping the Multiplicative Constant: The running time of an algorithm might be f  n   = 3n2 or f  n   = 100n2. We say it is  cid:1  n2  when we do not care what the multiplicative constant c is. The function f  n   = c · ban · nd ·  log n  e is in the class of functions denoted  cid:1  ban · nd ·  log n  e .  Examples of Functions:   cid:1  f  n   = 3n3 log n − 1000n2 + n − 29 is in the class  cid:1  n3 log n .  cid:1  f  n   = 7 · 4n · n2  log3 n + 8 · 2n + 17 · n2 + 1000 · n is in the class  cid:1  4n · n2  log3 n  . + 18 is in the class  cid:1  1 . Since 1 n2 + 1  n is a lower-order term than 18, it is dropped.  n is in the class  cid:1   1  n2 is a smaller term.  n  , because 1   cid:1  1 n  cid:1  1  Examples of Classes:  Linear Functions  cid:1  n : The classic linear function is f  n   = c · n + b. The notation  cid:1  n   excludes any with c ≤ 0 but includes any function that is bounded between two such functions.  What Can Be Done in  cid:1  n  Time: Given an input of n items, it takes  cid:1  n   time simply to look at the input. Looping over the items and doing a constant amount of work for each takes another  cid:1  n   time. Say we take t1 n   = 2n and t2 n   = 4n for a total of 6n time. Now if you do not want to do more than linear time, are you allowed to do any more work? Sure. You can do something that takest3 n   = 3n time and something else that takes t4 n   = 5n time. You are even allowed to do a few things that take a constant amount of time, totaling say t5 n   = 13. The entire algorithm then takes the sum of these, t n   = 14n + 13 time. This is still considered to be linear time.  Examples Included:   cid:1  f  n   = 7n and f  n   = 8.829n  cid:1  f  n   =  8 + sin n  n and f  n   = 8n + log10 n + 1  − 1,000,000  n  Examples Not Included:   cid:1  f  n   = −n and f  n   = 0n  fails c > 0   cid:1  f  n   = n log2 n  too small   cid:1  f  n   = n log2 n  too big   Quadratic Functions  cid:1  n2 : Two nested loops from 1 to n take  cid:1  n2  time if each inner iteration takes a constant amount of time. An n × n matrix re- quires  cid:1  n2  space if each element takes constant space.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes25 CUUS154-Edmonds 978 0 521 84931 9  Asymptotic Growth  March 29, 2008  14:34  Time for Sorting,  cid:1  n log n : Another running time that arises often in algo- rithms is  cid:1  n log n  . For example, this is the number of comparisons needed to sort n elements.  Not Linear: The function f  n   = n log n grows slightly too quickly to be in the linear class of functions  cid:1  n  . This is because n log n is log n times n, and log n is not constant.  383  A Polynomial: The classes  cid:1  n  ,  cid:1  n log n  , and  cid:1  n2  are subclasses of the class of polynomial functions n cid:1  1 . For example, though the func- tion f  n   = n log n is too big for  cid:1  n   and too small  cid:1  n2 , it is in n cid:1  1  because it is bounded between n1 and n2, both of which are in n cid:1  1 .  Logarithms  cid:1  log n  : See Chapter 24 for how logarithmic functions like log2 n   arise and for some of their rules.  Which Base: We write  cid:1  log n    without giving an explicit base. As shown in the list of rules about logarithms, log10 n  , log2 n  , and loge n   differ only by a multiplicative constant. Because we are ignoring mul- tiplicative constants anyway, which base is used is irrelevant. The rules also indicate that 8 log2 n5  also differs only by a multiplicative constant. All of these functions are include in  cid:1  log n   .  EXERCISE 25.1.1 Which grows faster, 34n or 43n?  EXERCISE 25.1.2 Does the notation   cid:1  1  n mean the same thing as 2θ n  ? EXERCISE 25.1.3 Prove that 20.5n ≤ 2n n100 ≤ 2n for sufﬁciently big n. EXERCISE 25.1.4  See solution in Part Five.  Prove that n2 ≤ 3n2 log n ≤ n3 for sufﬁ- ciently big n. EXERCISE 25.1.5  See solution in Part Five.  Sort the terms in f  n   = 100n100 + 34n + log1,000 n + 43n + 20.001n n100.  EXERCISE 25.1.6 For each of the following functions, sort its terms by growth rate. Get the big picture growth by classifying it into 2 cid:1  n  , n cid:1  1 , log cid:1  1  n  ,  cid:1  1  or into a similar and appropriate class. Also give its Theta approximation.  1. 2. 3. 4. 5. 6.  f  n   = 5n3 − 17n2 + 4 f  n   = 5n3 log n + 8n3 f  n   = 225n f  n   = 73 log2 n f  n   = { 1 if n is odd, 2 if n is even } f  n   = 2 · 2n · n2 log2 n − 7n8 + 7 3n  n2   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes25 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  14:34  Appendix  −5n + 17  f  n   = 100n100 + 34n + log1,000 n   + 43n f  n   = 6 n4 + 8n1002 log3 n n2 + 5 log n f  n   = 1 √ √ n + 6 3 f  n   = 7 5 n f  n   = 6n5.2+7n7.5 2n3.1+7n2.4 f  n   = −2n f  n   = 5nlog3 n  n  7. 8.  9. 10. 11. 12. 13.  384  EXERCISE 25.1.7 For each pair of classes of functions, how are they similar? How are they different? If possible, give a function that is included in the ﬁrst of these but not included in the second. If possible, do the reverse, giving a function that is included in the second but not in the ﬁrst.  1.  cid:1  22n  and  cid:1  23n  2.  cid:1  2n   and 3 cid:1  n    25.2 More about Asymptotic Notation  Other Useful Notations:  Name  Theta BigOh Omega Little Oh Little Omega Tilde  Standard Notation My Notation f  n   ∈  cid:1  g n    f  n   =  cid:1  g n    f  n   ≤ O g n    f  n   = O g n    f  n   ≥  cid:2  g n    f  n   =  cid:2  g n    f  n   = o g n    f  n   << o g n    f  n   = ω g n    f  n   >> ω g n    f  n   = ~ cid:1  g n    f  n   ∈ ~ cid:1  g n     Meaning f  n   ≈ c · g n   f  n   ≤ c · g n   f  n   ≥ c · g n   f  n   << g n   f  n   >> g n   f  n   ≈ log cid:1  1  ·g n    Same: 7 · n3 is within a constant of n3. Hence, it is in  cid:1  n3 , O n3 , and  cid:2  n3 . However, because it is not much smaller than n3, it is not in o n3 , and because it is not much bigger, it not in ω n3 . Smaller: 7 · n3 is asymptotically much smaller than n4. Hence, it is in O n4  and in o n4 , but it is not in  cid:1  n4 ,  cid:2  n4 , or ω n4 . Bigger: 7 · n3 is asymptotically much bigger than n2. Hence, it is in  cid:2  n2  and in ω n2 , but it is not in  cid:1  n2 , O n2 , or o n2 . Log Factors: 7n3 log2 n = ~ cid:1  n3  ignores the logarithmic factors.  Notation Considerations:  “∈” vs “=”: I consider  cid:1  n   to be a class of functions, so I ought to use the set notation, f  n   ∈  cid:1  g n   , to denote membership.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes25 CUUS154-Edmonds 978 0 521 84931 9  Asymptotic Growth  March 29, 2008  14:34  On the other hand, ignoring constant multiplicative factors, f  n   has the same asymptotic growth as g n  . Because of this, the notation 7n =  cid:1  n   makes sense. This notation is standard. Even the statements 3n2 + 5n − 7 = n cid:1  1  and 23n = 2 cid:1  n   make better sense when you think of the symbol  cid:1  to mean “some constant.” However, be sure to remember that 4n · n2 = 2 cid:1  n   is also true. “=” vs “≤”: 7n = O n2  is also standard notation. This makes less sense to me. Because it means that 7n is at most some constant times n2, a better notation would be 7n ≤ O n2 . The standard notation is even more awkward, because O n   = O n2  should be true, but O n2  = O n   should be false. What sense does that make?  385  More Details: You can decide how much information about a function you want to reveal. If f  n   = 5n2 + 3n, you could say   cid:1  f  n   ∈ n cid:1  1 , i.e., a polynomial  cid:1  f  n   ∈  cid:1  n2 , i.e., a quadratic  cid:1  f  n   ∈  5 + o 1  n2 = 5n2 + o n2 , i.e., 5n2 plus some low-order terms.  cid:1  f  n   ∈ 5n2 + O n  , i.e., 52 plus at most some linear terms.  iff iff iff iff iff iff  The Formal Deﬁnitions of Theta and BigOh: f  n   ∈  cid:1  g n    f  n   ∈ O g n    f  n   ∈  cid:2  g n    f  n   ∈ n cid:1  1  f  n   ∈ 2 cid:1  n   f  n    cid:9 ∈  cid:1  g n     ∃c1, c2 > 0 ∃n0 ∀n ≥ n0, c1 · g n   ≤ f  n   ≤ c2 · g n   ∃n0 ∀n ≥ n0, 0 ≤ f  n   ≤ c · g n   ∃c > 0 ∃n0 ∀n ≥ n0, c · g n   ≤ f  n   ∃c > 0 ∃c1, c2 > 0 ∃n0 ∀n ≥ n0, nc1 ≤ f  n   ≤ nc2 ∃c1, c2 > 0 ∃n0 ∀n ≥ n0, 2c1n ≤ f  n   ≤ 2c2n ∀c1, c2 > 0 ∀n0 ∃n ≥ n0, [c1 · g n   > f  n   or f  n   > c2 · g n  ] Bounded Between: The statement f  n  ∈  cid:1  g n   means that the function f  n   is bounded between c1 · g n   and c2 · g n  . See Figure 25.1. Requirements on c1 and c2: The only requirements on the constants are that c1 be sufﬁciently small  e.g., 0.001  but positive and c2 be sufﬁciently large  e.g., 1,000  to work, and that they be ﬁxed  that is, do not depend on n . We allow un- reasonably extreme values like c2 = 10100, to make the deﬁnition mathematically clean and not geared to a speciﬁc application. Sufﬁciently Large n: Given ﬁxed c1 and c2, the statement c1g n   ≤ f  n   ≤ c2g n   should be true for all sufﬁciently large values of n,  i.e., ∀n ≥ n0 . Deﬁnition of Sufﬁciently Large n0: Again to make the mathematics clean and not geared to a speciﬁc application, we will simply require that there exist some deﬁnition of sufﬁciently large n0 that works. Exercise 25.0.2 gives an example in which n0 needs to be unreasonably large.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes25 CUUS154-Edmonds 978 0 521 84931 9  Appendix  386  f n   c2  1c  March 29, 2008  14:34  c  g n   2  f n   c1g n    g n   Figure 25.1: f  n   ∈  cid:1  1  and f  n   ∈  cid:1  g n   . Proving f  n  ∈  cid:1  g n  : Use the prover–adversary game.   cid:1  You as the prover provide c1, c2, and n0.  cid:1  Some adversary gives you an n that is at least your n0.  cid:1  You then prove that c1g n   ≤ f  n   ≤ c2g n  . Example: For example, 2n2 + 100n =  cid:1  n2 . Let c1 = 2, c2 = 3, and n0 = 100. Then, for all n ≥ 100, we have c1g n   = 2n2 ≤ 2n2 + 100n = f  n   and f  n   = 2n2 + 100n ≤ 2n2 + n · n = 3n2 = c2g n  . The values of c1, c2, and n0 are not unique. For example, n0 = 1, c2 = 102, and n0 = 1 also work, because for all n ≥ 1 we have f  n   = 2n2 + 100n ≤ 2n2 + 100n2 = 102n2 = c2g n  .  The Formal Deﬁnitions of Little Oh and Little Omega:  Class f  n   =  cid:1  g n    f  n   = o g n    f  n   = ω g n    ∞  limn→∞ f  n  g n   =  Some constant Zero  A practically equivalent deﬁnition f  n   = O g n    and f  n   =  cid:2  g n    f  n   = O g n   , but f  n    cid:9 =  cid:2  g n    f  n    cid:9 = O g n   , but f  n   =  cid:2  g n     Examples:   cid:1  2n2 + 100n =  cid:1  n2  and limn→∞ 2n2+100n  cid:1  2n + 100 = o n2  and limn→∞ 2n+100 n2 = 0  cid:1  2n3 + 100n = ω n2  and limn→∞ 2n3+100n  n2  = 2 = ∞  n2  EXERCISE 25.2.1 As in Exercise 25.1.7, compare the classes  5 + o 1  n2 and 5n2 + O n  .  EXERCISE 25.2.2  See solution in Part Five.  Formally prove or disprove the following:  14n9 + 5,000n7 + 23n2 log n ∈ O n9  2n2 − 100n ∈  cid:1  n2  14n8 − 100n6 ∈ O n7   1. 2. 3.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes25 CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  14:34  Asymptotic Growth  14n8 + 100n6 ∈  cid:1  n9  2n+1 ∈ O 2n   22n ∈ O 2n    4. 5. 6. EXERCISE 25.2.3 Prove that if f1 n   ∈  cid:1  g1 n    and f2 n   ∈  cid:1  g2 n   , then f1 n   + f2 n   ∈ max  cid:1  g1 n   ,  cid:1  g2 n    . EXERCISE 25.2.4 Prove that if f1 n  , f2 n   ∈ n cid:1  1 , then f1 n   · f2 n   ∈ n cid:1  1 .  387  EXERCISE 25.2.5 Let f  n   be a function. As you know,  cid:1  f  n    drops low-order terms and the leading coefﬁcient. Explain what each of the following does: 2 cid:1  log2 f  n    and log2  cid:1  2f  n    . For each, explain to what extent the function is approximated. EXERCISE 25.2.6 Let x be a real value. As you know,  cid:12 x cid:13  rounds it down to the next integer. Explain what each of the following does: 2 ·  cid:12  x EXERCISE 25.2.7 Suppose that y =  cid:1  log x . Which of the following are true: x =  cid:1  2y   and x = 2 cid:1  y ? Why?  ·  cid:12 2 · x cid:13 , and 2   cid:12 log2 x cid:13    cid:13 , 1  .  2  2  EXERCISE 25.2.8  See solution in Part Five.  It is impossible to algebraically solve the equation x = 7y 3 log2 y 18 for y. 1. Approximate 7y 3 log2 y 18 and then solve for y. This approximates the value of y. 2. Get a better approximation as follows. Plug in your above approximation for y to express  log2 y 18 in terms of x. Plug this into x = 7y 3 log2 y 18. Now solve for y again.  You could repeat this step for better and better approximations.  proximate a solution for  log10 n  1,000,000 = n.  3. Observe how a similar technique was used in Exercises 25.0.1 and 25.0.2 to ap-   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes26 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  18:22  388 26 Adding-Made-Easy Approximations  algorithm Eg n  loop i = 1..n  loop j = 1..i  loop k = 1..j put “Hi”  end loop  end loop  end loop end algorithm   cid:1   cid:1    cid:1   cid:1  k=1 1 = cid:1  k=1 1 = j .  cid:1  k=1 1 = cid:1   j  j  j  i  j=1 j =  cid:1  i2 .  cid:1  i2  =  cid:1  n3 . n i=1  The inner loop requires time The next requires n i=1 The total is  i j=1 i j=1   cid:1    cid:1   Sums arise often in the study of computer algorithms. For example, if the ith iteration of a loop takes time f  i  and it loops n times, then the total time is f  1  +  cid:2  f  2  + f  3  + ··· + f  n . This we denote as n i=1 f  i . It n  cid:1  can be approximated by the integral x=1 f  x  δx, be- cause the ﬁrst is the area under the stairs of height f  i  and the second under the curve f  x .  In fact, both  from the Greek letter sigma  and  from the old long S   are S for sum.  Note that, even though the individual terms are indexed by i  or x , the total is a function of n i=1 f  i  for various n. The goal now is to approximate functions f  i .   cid:2   cid:1   f n   f x   f i  1 i   cid:1   n  m   cid:1    cid:1   n i=1  n i=1 i, and Beyond learning the classic techniques for computing 1 i , we do not study how to evaluate sums exactly, but only how to approximate them to within a constant factor. We develop easy rules that most computer scien- tists use but for some reason are not usually taught, partly because they are not al- ways true. We have formally proven when they are true and when not. We call them collectively the adding-made-easy technique.  n i=1 2i,   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes26 CUUS154-Edmonds 978 0 521 84931 9  Adding-Made-Easy Approximations  April 5, 2008  18:22  26.1 The Technique   cid:1  The following table outlines the few easy rules with which you will be able to compute i=1 f  i   for functions with the basic form f  n  =  cid:1  ban · nd · loge n .  We consider  cid:1   more general functions at the end of this section.   n   cid:1   389  d  b a > 1 Any  Type of Sum  e Any Geometric Increase  n i=1 f  i    cid:1  f  n     dominated by last term   = 1 > −1 Any Arithmetic-like  half of terms approximately equal    cid:1  n · f  n    = −1 =0 Harmonic < −1 Any Bounded tail   cid:1  ln n    cid:1  1    dominated by ﬁrst term   < 1 Any  Any  Examples  n   cid:1   cid:1  i=0 22i  cid:1  n i=0 bi  cid:1  n i=0 2i  cid:1  n i=1 id  cid:1  n i=1 i2  cid:1  n i=1 i  cid:1  n i=1 1  cid:1  n i=1  cid:1  n i=1  cid:1  n i=1  cid:1  n 1 i=1 i2  cid:1  n i=1  1 2  i −i n i=0 b  1 i  1 i0.99  1  i1.001  ≈ 1 · 22n =  cid:1  bn  =  cid:1  2n  =  cid:1  n · nd  =  cid:1  nd+1  =  cid:1  n · n2  =  cid:1  n3  =  cid:1  n · n  =  cid:1  n2  =  cid:1  n · 1  =  cid:1  n  =  cid:1  n · = loge n  +  cid:1  1  =  cid:1  1  =  cid:1  1  =  cid:1  1  =  cid:1  1   n0.99   =  cid:1  n0.01   1  Four Different Classes of Solutions: All of the sums that we will consider have one of four different classes of solutions. The intuition for each is quite straightfor- ward.  Geometrically Increasing: If the terms grow very quickly, the total is dominated by the last and biggest term f  n . Hence, one can approximate the sum by only considering the last term:   cid:1  i=1 f  i  =  cid:1  f  n  .  n  n  Examples: Consider the classic sum in which each of the n terms is twice the previous, 1 + 2 + 4 + 8 + 16 + ··· + 2n. Either by examining areas within  cid:1  Figure 26.1.a or 26.1.b or using simple induction, one can prove that the total i=0 2i = 2 × 2n − 1 =  cid:1  2n . is always one less than twice the biggest term: · bn, which can be approximated by  cid:1  f  n   = More generally,  cid:1  bn .  Similarly, ln b bn.  The same is true for even fastergrowing n functions like   cid:1  i=1 f  i  =  cid:1  f  n  , works Basic-Form Exponentials: The same technique, for all basic-form exponentials, i.e., for f  n  =  cid:1  ban · nd · loge n  with b a > 1, we have that   cid:1   cid:2  i=0 bi ≈ b  cid:1  b−1 x=0 bx δx = 1 i=0 22i ≈ 1 × 22n .  cid:1  i=1 f  i  =  cid:1  ban · nd · loge n .  n  n  n  n   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes26 CUUS154-Edmonds 978 0 521 84931 9  Appendix  April 5, 2008  18:22  1  390  64  32  16  8  4 1 2  1+2+4+8+16+32+64 = 2×64 –1       a   64  32  16 8  4 2  1   b   1  1 2  1 2  n+1  1 4  1 8  1 16  1 64  1 32  n=10  c   1+1 2+1 4+1 8+1 16+1 32+1 64  = 2 –1 64    d   1 4  1 8 1 16   e   1 32  1 64  Figure 26.1: Examples of geometrically increasing, arithmetic-like, and bounded-tail function.   cid:1  Arithmetic-like: If half of the terms are roughly the same size, then the total is i=1 f  i  =  cid:1  n · f  n  . roughly the number of terms times the last term, namely  cid:1  i=1 1 = n. This is  cid:1  n ·  Constant: Clearly the sum of n ones is n, i.e., f  n  .  Examples:  n  n  n  n  n   cid:1  Linear: The classic example is the sum in which each of the n terms i=1 i = 1 + 2 + 3 + 4 + 5 + ··· + is only one bigger than the previous, n = n n+1  . This can be approximated using  cid:1  n · f  n   =  cid:1  n2 . See Fig- 2  cid:1  ure 26.1.  cid:1  2n2 + 1 i=1 i2 = 1 3n3 + 1  cid:2  6n Polynomials: Both d+1nd+1 +  cid:1  nd  i=1 id = 1 erally be can  cid:1  n · f  n   =  cid:1  n · nd  =  cid:1  nd+1 .  Similarly, x=0 xd δx = 1  cid:1  n n i=1 Above Harmonic:  cid:1  n · f  n   =  cid:1  n · n −0.999  =  cid:1  n0.001 .  cid:1  i=1 f  i  =  cid:1  n · f  n  ,  cid:1  Basic-Form Polynomials: The works for all basic-form polynomials, constants, and slowly decreasing functions, i.e., for f  n  =  cid:1  nd · loge n  with d > 1 we have that i=1 f  i  =  cid:1  nd+1 · loge n .  n0.999 ≈ 1,000 n0.001 can be approximated with  gen- and more approximated with  d+1nd+1.   technique,  same  n  n  1  Bounded Tail: If the terms shrink quickly, the total is dominated by the ﬁrst and biggest term f  1 , which is assumed here to be  cid:1  1 , i.e.,  n   cid:1  i=1 f  i  =  cid:1  1 .  4  8  2  1  16  n i=1  + 1  + 1  + 1  + ··· + 1  cid:1   n1.001 ≈ 1, 000 =  cid:1  1  and  Examples: The classic sum here is when each term is half of the previ- ous, 1 + 1  cid:1  2n . See Figure 26.1.d and 26.1.e. The total ap- 2  n =  cid:1  1 . Similarly, proaches but never reaches 2, so that 1  2  i = 2 −   1 ≈ 1.5497 =  cid:1  1 .  cid:1  i=1 f  i  =  cid:1  1 , Basic-Form with Bounded Tail: The works for all basic-form polynomially or exponentially decreasing func- tions, i.e., for f  n  =  cid:1  ban · nd · loge n  with b a = 1 and d < 1 or with b a < 1.   cid:1  n2 ≈  cid:3  same  n i=0  1 6  technique,  n i=1  n   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes26 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  18:22  Adding-Made-Easy Approximations   cid:1   1 The Harmonic Sum: The sum i is referred to as the harmonic sum because of its connection to music. It arises surprisingly often and it has an unexpected total:  n i=1  1  1  1  1  391  f i  = 1  Σ 1 = n    2 f i  = 1 2 Σ Θ 21 2    =  f i  = 1 i1 3 Σ 1 i    =   1 3  f i  = 1 i Σ  Θ  Θ n         2 3  Θ .   1    1    n f n    =  1 i = ln n  +   cid:1   cid:1  i=1 f  i  = On the Boundary: The boundary between those sums for which  cid:1  n · f  n   and those for which i=1 f  i  =  cid:1  1  occurs when these approx-  cid:1  imations meet, i.e., when  cid:1  n · f  n   =  cid:1  1 . This occurs at the harmonic function f  n  = 1 =  cid:1  1 , it is reasonable to think that this is the answer, but it is not.  cid:1   The Total: It turns out that the total is within 1 of the natural logarithm, δx = loge n +  cid:1  1 .  See Figure 26.2.  n . Given that both approximations say the total is  = loge n +  cid:1  1 .  Similarly,  n i=1  n i=1   cid:2   n+1 x=1  1 i  n  n  1 x  1 i  More Examples:  Geometric Increasing:  Arithmetic  Increasing :   cid:1    cid:1    cid:1    cid:1    cid:1    cid:1    cid:1   n  n  n   cid:1   cid:1  i100 + i3 =  cid:1   2n n i=1 8 2i n100    cid:1  i=1 3i logi + 5i + i100 =  cid:1  3n log n  n  cid:1  i=1 2i2 + i2 logi =  cid:1  2n2  i=1 22i−i2 =  cid:1  22n−n2   cid:1   cid:1  i=1 i4 + 7i3 + i2 =  cid:1  n5  i=1 i4.3 log3 i + i3 log9 i =  cid:1  n5.3 log3 n   cid:1   cid:1   cid:1   cid:1   cid:1   i0.6 =  cid:1  n0.4 log3 n   n i=2 n i=1  logi log3 i  n  1  Arithmetic  Decreasing : log n    =  cid:1   n   cid:1    cid:1    cid:1    cid:1   n i=1 n i=1 n i=1  Bounded Tail: =  cid:1  1  log3 i i1.6+3i 2i =  cid:1  1  i100 22i =  cid:1  1   cid:1  i=m f  i  = cid:1  Stranger Examples:  cid:1  A useful fact is  cid:1  log m  =  cid:1  log n m .  n  1  i=1 f  i  − cid:1   n   cid:1   m−1 i=1 f  i . Hence,  =  cid:1  log n  −  n i=m  1 i   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes26 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  18:22  Appendix  f n    n  1.0001   n10,000  Geometric: f n  is an exponential, Σf n  =  f n  Θ    392  Arithmetic: f n  is a polynomial or slowly decreasing, Σ f n  = Θ     nf n    –0.9999  n  –1.0001  –1n  Harmonic: f n  = 1 n, Θ Σ  f n  =    log n   Bounded tail: f n  is quickly decreasing, Σ f n  =    1 Θ  n  Figure 26.2: Boundaries between geometric, arithmetic, harmonic, and bounded tail.   cid:1    cid:1  If the sum is arithmetic, then the sum is the number of terms times the largest   cid:1   cid:1  i=m i2 =  cid:1  n ·  m + n 2 . m+n  cid:1  i3 logi, let N = 5n2 + n denote the number of terms. Then  term. This gives 5n2+n i=1  cid:1  To solve i3 logi =  cid:1  N · f  N    =  cid:1  N 4 log N  . Substituting back in for N gives i3 logi =  cid:1   5n2 + n 4 log 5n2 + n   =  cid:1  n8 log n .  cid:1   cid:1  Between terms, i changes, but n does not. Hence, n can be treated like a constant.  For example,  N i=1 5n2+n i=1  1 i2 , the terms are decreasing fast enough to be bounded by the ﬁrst term.   cid:1  i=1 i · n · m = nm · cid:1   cid:3   i=1 i = nm ·  cid:1  n2  =  cid:1  n3m .  cid:4   Here, however, the ﬁrst term is not  cid:1  1 , but is   cid:4    cid:3   n i= n   cid:1  In  n  n  2   cid:1   1   n 2  2  =  cid:1   1 n2   cid:1 log2 n  cid:1  When in doubt, start by determining the ﬁrst term, the last term, and the num- i=1 2log2 n−i · i2, the ﬁrst term is f  1  = 2log n−1 · 12 =  cid:1  n , and ber of terms. In the last term is f  log n  = 2log n−log n ·  log n 2 =  cid:1  log2 n . The terms decrease ge-  cid:1  j=0 i2 j 3 = cid:1  ometrically in i. The total is then  cid:1  f  1   =  cid:1  n .  cid:1  n4  cid:1  n3  =  cid:1  n7 .  j=0 j 3] = cid:1   i=1 i2 cid:1  n4  =  cid:1  n4 [   cid:1  i=1 i2] =  n i=1 i2[   cid:1    cid:1   n i=1  n  n  n  n   cid:1   EXERCISE 26.1.1 Give the  cid:1  approximation of the following sums. Indicate which rule you use, and show your work.  n   cid:1   cid:1  i=0 7i3 − 300i2 + 16  cid:1  i=0 i8 + 23i n i=0  1 i1.1  i2  n  1. 2. 3.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes26 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  18:22  Adding-Made-Easy Approximations  − 300i2 log9 i  1 i0.9   cid:1   cid:1  n i=0  cid:1  n i=0 7 i3.72 log2 i  cid:1 log n loge i n i=1  cid:1   cid:1  i=1 n · i2  cid:1   cid:1  n m j j=0 i=0 i  cid:1   cid:1  i n j=1 j i i=1 i2 n j=1 ij log i  i=1  i  4. 5.  6.  7. 8.  9.  10.  393  26.2 Some Proofs for the Adding-Made-Easy Technique  This section presents a few of the classic techniques for summing and sketches the proof of the adding-made-easy technique.  Simple Geometric Sums:  Theorem: When b > 1,   cid:1  i=1 bi =  cid:1  f  n   and when b < 1,  n   cid:1  i=1 f  i  =  cid:1  1 .  n  Proof:  Subtracting those two equations gives  S = 1 + b + b2 + ··· bn  b · S = b + b2 + b3 + ··· bn+1.   1 − b  · S = 1 − bn+1 S = 1 − bn+1 bn+1 − 1 b − 1 1 − b =  cid:1  max f  0 , f  n     or  Ratio between Terms: To prove that a geometric sum is not more than a constant times the biggest term, we must compare each term f  i  with this biggest term. One way to do this is to ﬁrst compare each consecutive pairs of terms f  i  and f  i + 1 .   cid:1   cid:1  Theorem: If for all sufﬁciently large i, the ratio between terms is bounded away from one, i.e., ∃b > 1, ∃n0, ∀i ≥ n0, f  i + 1  f  i  ≥ b, then i=1 f  i  =  cid:1  f  n  . i=1 f  i  =  cid:1  1 .  Conversely, if ∃b < 1, ∃n0, ∀i ≥ n0,  ≤ b, then  f  i+1  f  i   n  n  Examples:  Typical: With f  i  = 2i  i, the ratio between consecutive terms is  f  i + 1  f  i   = 2i+1 i + 1  · i 2i  = 2 ·  i i + 1  = 2 ·  1 1 + 1  i  which is at least 1.99 for sufﬁciently large i. Similarly for any f  n  =  cid:1  ban · nd · loge n  with b a > 1.   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes26 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  18:22  Appendix  f n   394  n − i  1 b     f n   f i   f n  0 n0  Σn n0  f i   =   f n     Θ  n  n0 f i  = Θ n0f n0     Σ 1  = Θ 1   f n0    i − n0 b f i   f n0    n0  Σn f i   = Θ n0   1    f n  n  n0 f i  = Θ n 0f n0     Σ 1  = Θ 1   Figure 26.3: In both pictures, the total before n gets sufﬁciently large is some constant. On the left, the total for large n is bounded by an growing exponential, and on the right by a decreasing exponential.  Not Bounded Away: On the other hand, the arithmetic function f  i  = i has a ratio between the terms of i+1 i . Though this is always bigger than one, it is not bounded away from one by any constant b > 1.  = 1 + 1  i  Proof: If ∀i ≥ n0, f  i + 1  f  i  ≥ b > 1, then it follows either by unwinding or in- duction that  f  i  ≤ n cid:5    cid:3    cid:4 1  1 b  f  i + 1  ≤   cid:3    cid:4 2  1 b   cid:3    cid:4 3  1 b   cid:4 n−i  f  i + 2  ≤  cid:3   See Figure 26.3. This gives that  f  i  = n0 cid:5   f  i  + n cid:5   f  i  ≤  cid:1  1  + n cid:5   i=1  i=1  i=n0  1 b  i=n0  which we have already proved is  cid:1  f  n  .   cid:3    cid:4 n−i  1 b  f  i + 3  ≤ ··· ≤  f  n  ≤  cid:1  1  + f  n  · n cid:5   f  n    cid:3    cid:4 j  1 b  j=0   cid:1  i=1 i =  cid:1  n · f  n   =  cid:1  n2 :  n  A Simple Arithmetic Sum: We prove as follows that  S = 1 + 2 + 3 + ··· + n − 2 + n − 1 + n S = n + n − 1 + n − 2 + ··· + 3 + 2 + 1 2S = n + 1 + n + 1 + n + 1 + ··· + n + 1 + n + 1 + n + 1 S = 1  = n ·  n + 1  2n ·  n + 1   Arithmetic Sums: We will now justify the intuition that if half of the terms are roughly the same size, then the total is roughly the number of terms times the last term, namely   cid:1  i=1 f  i  =  cid:1  n · f  n  .  n  Theorem: If for sufﬁciently large n, the function f  n  is nondecreasing, and 2   =  cid:1  f  n  , then f   n  n   cid:1  i=1 f  i  =  cid:1  n · f  n  .   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes26 CUUS154-Edmonds 978 0 521 84931 9  Adding-Made-Easy Approximations  April 5, 2008  18:22  Examples:   cid:7 d = 1  2   = cid:6   n 2  Typical: The function f  n  = nd for d ≥ 0 is non- decreasing and f   n 2d f  n . Similarly for f  n  =  cid:1  nd · loge n . We consider −1 < d < 0 later. Without the Property: The function f  n  = 2n 2   = does not have this property, because f   n 2n 2 = 1  2n 2 f  n .  f n   395  f i   n  n  Proof: Because f  i  is nondecreasing, half of the terms are at least the middle term f   n 2   ≤ cid:1  2  , and all  cid:1  of the terms are at most the biggest term f  n . Hence, · f   n i=1 f  i  ≤ n · f  n . Because f   n 2   =  cid:1  f  n  , these bounds match, giv- n i=1 f  i  =  cid:1  n · f  n  . 2 ing  cid:1   cid:1   The Harmonic Sum: The harmonic sum is a famous sum that arises surprisingly 1 often. The total i is within 1 of loge n. However, we will not bound it quite so closely.  n i=1  f n 2   n 2  n  Theorem:  =  cid:1  log n .  n i=1  1 i  Proof: One way of approximating the harmonic sum is to break it into log2 n blocks with 2k terms in the kth block, and then to prove that the total for each block is between 1  n cid:5   i=1  =  1 i  = 1  2 and 1: ≥1· 1   cid:8  cid:9  cid:10  cid:11  1 cid:10   cid:11  cid:8   cid:9   +  2 1  2  ≤1·1=1  4  2  8  2  +  ≥4· 1  ≥2· 1   cid:8   cid:9  cid:10   cid:11   cid:8  = 1  cid:10   cid:11  cid:8   cid:9   cid:10  + 1 3 · log2 n ≤ cid:1  =1   cid:11   cid:9  + 1 7   cid:8   cid:9  cid:10  = 1  cid:10   cid:11  cid:8  + 1 + 1 6 5 =1 ≤4· 1 ≤ 1 · log2 n. n 1 i=1 i  1 2 ≤2· 1  +  1 4  1 8  4  2  2  16  = 1   cid:11   cid:9  cid:10  ≥8· 1  cid:11  cid:8   cid:9  + ··· + 1 15  ≤8· 1  8  =1  +···  From this, it follows that 1 2   cid:1   Close to Harmonic: We will now use a similar technique to prove the remaining two cases of the adding-made-easy technique.  Theorem: f  n  =  cid:1  nd · loge n  with d   −1.   is  cid:1  1  if d  n i=1 1 id   cid:1    cid:1  > 1 and is  cid:1  n · f  n   if d  cid:1  < 1.  Similarly for  cid:1   n i=1 f  n  into blocks Proof: As we did with the harmonic sum, we break the sum where the kth block has the 2k terms f  i . Because the terms are decreas- ing, the total for the block is at most F  k  = 2k · f  2k . The total overall is then at most  2k+1−1 i=2k   cid:1   log2 n cid:5   k=0  F  k  = log2 n cid:5   2k · f  2k  = log2 n cid:5    2k d cid:1  = N cid:5   2k  k=0  k=0  k=0  1  2k· d cid:1 −1   .   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes26 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  18:22  Appendix   cid:1  > 1, then this sum is exponentially decreasing and converges to  cid:1  1 . If d   cid:1  < If d 1, then this sum is exponentially increasing and diverges to  cid:1  F  N    =  cid:1  2log2 n · f  n   =  cid:1  n · f  n   .  396  Functions without the Basic Form:  Warning: This topic may be a little hard.  Until now we have only considered functions with the basic form f  n  =  cid:1  ban · nd · loge n . We would like to generalize the adding-made-easy technique as follows:  Geometric Increasing  cid:1  f  n  ≥ 2 cid:4  n  i=1 f  i  =  cid:1  f  n    n  If then  Arithmetic  cid:1  f  n  = n cid:1  1 −1 i=1 f  i  =  cid:1  n · f  n   n or f  n  = n  n  Harmonic  cid:1  f  n  =  cid:1   1 n   i=1 f  i  =  cid:1  log n   n  Bounded Tail  cid:1  f  n  ≤ n −1− cid:4  1  i=1 f  i  =  cid:1  1   n  Example: Consider f  n  = n8+ 1 n . They are bounded between nd1 and nd2 for constants d2 ≥ d1 > 0 − 1, and hence for both we have f  n  ∈ n cid:1  1 −1. i = Adding made easy then gives that  cid:1  n9+ 1   cid:1  i=1 f  i  =  cid:1  n · f  n  , so that   cid:1  i=1 i8+ 1  i =  cid:1  n1− 1 − 1 n  .  n   and   cid:1   − 1  n  n  n i=1 i  n i=1 f  i  from the value Counterexample: The goal here is to predict the sum of the last term f  n . We are unable to do this if the terms oscillate like those created with sines, cosines, ﬂoors, and ceilings. Exercise 26.2.4 proves that f  n  = 2 cos π log2 n +1.5]·n are counterexamples for the geometric case  cid:8 log2 n cid:9  22 and that f  n  = 22  and f  n  = 2[ 1  cid:10 log log n cid:11   is one for the arithmetic case.   cid:1   2n  2  n  2  f n   n  f n   n  n 2  n   Not to scale   n n  Not to scale   Simple Analytical Functions: We can prove that the adding-made-easy technique works for all functions f  n  that can be expressed with n, real constants, plus, minus, times, divide, exponentiation, and logarithms. Such functions are said to be simple analytical.  Proof Sketch: I will only give a sketch of the proof here. For the geometric case, we must prove that if f  n  is simple analytical and f  n  ≥ 2 cid:4  n , then ∃b > 1,   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes26 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  18:22  397  Adding-Made-Easy Approximations  ∃n0, ∀n ≥ n0, above gives that   cid:1  f  n + 1  f  n  ≥ b. From this, the ratio-between-terms theorem i=1 f  i  =  cid:1  f  n  .  n  Because the function is growing exponentially, we know that generally it grows at least as fast as fast as bn for some constant b > 1 and hence f  n + 1  f  n  ≥ b, or equivalently h n  = log f  n + 1  − log f  n  − log b > 0 for an inﬁnite number of values for n.  A deep theorem about simple analytical functions is that they cannot os- cillate forever and hence can change sign at most a ﬁnite number of places. It follows that there must be a last place n0 at which the sign changes. We can con- clude that ∀n ≥ n0, h n  > 0 and hence f  n + 1  f  n  ≥ b. The geometrically decreasing case is the same except f  n + 1  f  n  ≤ b. The arithmetic case is similar except that it proves that if f  n  is simple analytical and f  n  = n cid:1  1 −1, then f   n  2   =  cid:1  f  n  .  EXERCISE 26.2.1  See solution in Part Five.  Zeno’s classic paradox is that Achilles is traveling 1 km hr and has 1 km to travel. First he must cover half his distance, then half of his remaining distance, then half of this remaining distance, . . . . He never arrives. By Bryan Magee states, “People have found it terribly disconcerting. There must be a fault in the logic, they have said. But no one has yet been fully successful in demonstrating what it is.” Resolve this ancient paradox by adding up the time required for all steps.   cid:1  EXERCISE 26.2.2 Prove that i=n0 f  i  =  cid:1  f  n0   =  cid:1  1 .  n  if ∃b < 1, ∃n0, ∀i ≥ n0,  f  i + 1  f  i  ≤ b,  then  EXERCISE 26.2.3 A seeming paradox is how one could have a vessel that has ﬁnite volume and inﬁnite surface area. This  theoretical  vessel could be ﬁlled with a small amount of paint but require an inﬁnite amount of paint to paint. For h ∈ [1, ∞ , its cross section at h units from its top is a circle with radius r = 1 hc for some constant c. Integrate  or add up  its cross-sectional circumference to compute its surface area, and integrate  or add up  its cross-sectional area to compute its volume. Give a value for c such that its surface area is inﬁnite and its volume is ﬁnite.  n  EXERCISE 26.2.4  See solution in Part Five.   cid:1  , prove that f  n  ≥ 2 cid:4  n  1. For f  n  = 22  cid:8 log2 n cid:9  i=1 f  i   cid:13 =  cid:1  f  n  .  cid:1  2. and that 3. For f  n  = 22 , prove that f  n  = n cid:1  1 −1 i=1 f  i   cid:13 =  cid:1  n · f  n  . 4. and that 5. Plot f  n  = 2[ 1 geometric case.   cid:10 log log n cid:11   n  2 cos π log2 n +1.5]·n, and prove that it is also a counterexample for the   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes27 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  19:5  398 27 Recurrence Relations  A wise man told the king to give him one grain of rice one for the ﬁrst square of a chessboard and for the each remaining square to give him twice the number for the previous square. Thirty-two days later, the king realized that there is not enough rice in all of world to reward him. The number of grains on the nth square is given by the recurrence relation T 1  = 1 and T n   = 2T n − 1 . The algebraic equation x2 = x + 2 speciﬁes the value of an unknown real that = f  x  speciﬁes functions from reals must be found. The differential equation δf  x  to reals that must be found. Similarly, a recurrence relations like T n   = 2 × T n − 1  δx speciﬁes functions from integers to reals. One way to solve each of these is to guess a solution and check to see if it works. Here T n   = 2n works, i.e., 2n = 2 × 2n−1. How- ever, T n   = c · 2n also works for each value of c. Making the further requirement that T 1  = 1 narrows the solution set to only T n   = 1  · 2n = 2n−1.  2  27.1 The Technique   cid:1   Timing of Recursive Programs: Recursive relations are used to determine the running time of recursive programs.  See Chapter 8.  For example, if a routine, when given an instance of size n, does f  n   work itself and then recurses a times on subin- stances of size n  b , then the running time is T n   = a · T   cid:2  + f  n  .  See Section 8.6 to learn more about the tree of stack frames. Each stack frame consists of one execution of the routine on a single instance, ignoring subroutine calls. The top-level stack frame is called by the user on the required input instance. It recurses on a number of subinstances, creating the next level of stack frames. These in turn recurse again until the instance is sufﬁciently small that the stack frame returns without recursing. These ﬁnal stack frames are referred to as base cases.  n b  Let T n   denote the number of “Hi”s that the entire tree of stack frames, given the following code, prints on an instance of size n. The top level stack frame prints “Hi” f  n   times. It then recurses a times on subinstances of size n b . If T n   is the number of   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes27 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  19:5  Recurrence Relations  “Hi”s for instances of size n, then it follows that T b . Repeating this a times will take time a · T size n satisﬁes the recursive relation T n   = a · T determine which function T n   satisﬁes this relation. recurrence relation will be T n   = a · T  n − b  + f  n  .  n b  n b   cid:1   If instead the routine recurses a times on instances of size n − b, then the related   cid:2    cid:1   cid:2   cid:1   cid:2  + f  n  . The goal of this section is to  n is the number for instances of b . It follows that the total number  399  algorithm Eg In    cid:2  pre-cond cid:3 : In is an instance of size n.  cid:2  post-cond cid:3 : Prints T n   “Hi”s. begin  n = In if  n ≤ 1  then put “Hi” loop i = 1..f  n    else  put “Hi” end loop loop i = 1..a  b = an input of size n I n b Eg I n b   end loop  end if  end algorithm  When the input has size zero or one, only one “Hi” is printed. In general, we will assume that recursive programs spend  cid:2  1  time for instances of size  cid:2  1 . We express this as T 1  = 1, or more generally as T  cid:2  1   =  cid:2  1 .  cid:1   Solving Recurrence Relations: Consider T n   = a · T  cid:2  nc · logd n   or f  n   = 0.  n b   cid:2  + f  n  , where f  n   =  cid:3   cid:1  Example  cid:1  T n   = 9 · T T n   = 9 · T  cid:1   cid:1  T n   = 9 · T t n   = 9 · T   cid:4   cid:2  + n4 = 2  cid:2  + n 2  cid:2  + n 2  cid:2   Solution  cid:2  n4   log3 9 log3 3 n 3   cid:2  n 2   log2n  n 3  n 3  n 3   cid:2  n 2 log n    log a log b vs c < =  >  d Any > −1 < −1 Any  Dominated by Top level  T n   cid:2  f  n     All levels   cid:2  f  n   log n     cid:5    cid:6    cid:2   log a log b  n  Base cases   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes27 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  19:5  Appendix  Consider T n   = a · T  n − b  + f  n  , where f  n   =  cid:2  nc · logd n   or f  n   = 0. a > 1 = 1  Dominated by Base cases  All levels   cid:2  n5   Solution  cid:2  9 n 3    Example T n   = 9 · T  n − 3  + n4 T n   = T  n − 3  + n4 T n   = T  n − 3   T n   cid:2  a n b    cid:2  n · f  n     cid:2  1   f  n  Any ≥ 1 = 0  Base cases   cid:2  1   400  A Growing Number of Subinstances of Shrinking Size: Each instance having a subinstances means that the number of subinstances grows exponentially by a factor of a. On the other hand, the sizes of the subinstances shrink exponentially by a factor of b. The amount of work that the instance must do is the function f of this instance size. Whether the growing or the shrinking dominates this process depends upon the relationship between a, b, and f  n  .  Dominated By: When total work T n   done in the tree of stack frames is dominated by the work f  n   done by the top stack frame, we say that the work is dominated by the top level of the recursion. The solution in this case will be T n   =  cid:2  f  n   . Conversely, we say that it is dominated by the base cases when the total is dominated by the sum of the work done by the base cases. Because each base case does only a constant amount of work, the solution will be T n   =  cid:2   of base cases , which is  cid:2  nlog a   log b ,  cid:2  a n b  , or  cid:2  1  in the above examples. Finally, if the amounts of work at the different levels of recursion are sufﬁciently close to each other, then we say that the total work is dominated by all the levels and the total is the number of levels times this amount of work, namely T n   =  cid:2  log n · f  n    or  cid:2  n · f  n   .  The Ratio log a log b: See Chapter 24 for a discussion about logarithms. One trick that it gives us is that when computing the ratio between two logarithms, the base used does not matter, because changing the base will introduce the same constant both on the top and the bottom, which will cancel. Hence, when computing such a ratio, you can choose whichever base makes the calculation the easiest. For example, to compute log 16 3 . This is useful in giving that T n   = 16 · T  n 8   + f  n   =  cid:2  nlog 16  log 8  =  cid:2  n4 3 . On the other hand, to com- 3 , and hence we have T n   = log 27 , the obvious base to use is 3, because log3 9 pute log 9 log3 27 27   + f  n   =  cid:2  n2 3 . Another interesting fact given is that log 1 = 0, which gives 9 · T  n that T n   = 1 · T  n  log 8 , the obvious base to use is 2, because log2 16 log2 8 = 2  2   + f  n  , T n   =  cid:2  nlog 1  log 2  =  cid:2  n0  =  cid:2  1 .  = 4  EXERCISE 27.1.1  See solution in Part Five.  Give solutions for the following examples: 1. T n   = 2T  n 2. T n   = 2T  n 3. T n   = 4T  n  2   + n 2   + 1 2   +  cid:2   n3     log3 n   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes27 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  19:5  Recurrence Relations 4. T n   = 32T  n 5. T n   = 27T  n 6. T n   = 8T  n 7. T n   = 4T  n  4   +  cid:2  log n   3   +  cid:2  n3 log4 n   4   +  cid:2    n log n  1.5  2   +  cid:2   n 2 log n    EXERCISE 27.1.2 Give solutions for the following stranger examples: 1. T n   = 4T  n 2. T n   = 4T  n 3. T n   = 4T  n 4. T n   = 4T  n  2   +  cid:2  n3 log log n   2   +  cid:2  2n   2   +  cid:2  log log n   − √  n + log n − 5  +  cid:2  n3   2  401  27.2 Some Proofs  I now present a few of the classic techniques for computing recurrence relations. As our example we will solve T n   = GT n 0  + f  n  , for f  n   = nc.  cid:1  Guess and Verify: To begin consider the example T n   = 4T   cid:2  + n and T 1  = 1.  n 2  Plugging In: If we can guess T n   = 2n 2 − n, the ﬁrst way to verify that this is the solution is to simply plug it into the two equations and make sure that they are satisﬁed:  Left Side T n   = 2n 2 − n T 1  = 2n 2 − n = 1  Right Side  4T  n  2   + n = 4   cid:7    cid:1   2  n 2   cid:2 2 − cid:1   n 2   cid:2  cid:8   − n = 2n 2 − n  1  Proof by Induction: Similarly, we can use induction to prove that this is the solu- tion for all n  at least for n = 2i .  Base Case: Because T 1  = 2 1 2 − 1 = 1, it is correct for n = 20. Induction Step: Let n = 2i. Assume that it is correct for 2i−1 = n T n   = 4T  n  + n = 2n 2 − n, it is also true for n.   cid:2 2 − cid:1    cid:2  cid:8   2   + n = 4   cid:7    cid:1   n 2  n 2  2  2 . Because  Calculate Coefﬁcients: Suppose that instead we are only able to guess that the formula has the form T n   = an 2 + bn + c for some constants a, b, and c:  Left Side T n   = an 2 + bn + c 4T  n T 1  = a + b + c  Right Side   cid:7    cid:1    cid:1    cid:2 2 + b  n 2   cid:8    cid:2  + c  2   + n = 4  a  n 2  1  − n = an 2 +  2b + 1 n + 4c   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes27 CUUS154-Edmonds 978 0 521 84931 9  Appendix  402  April 5, 2008  19:5  These left and right sides must be equal for all n. Both have a as the coefﬁcient of n 2, which is good. To make the coefﬁcient in front n be the same, we need that b = 2b + 1, which gives b = −1. To make the constant coefﬁcient be the same, we need that c = 4c, which gives c = 0. To make T 1  = a 1 2 + b 1  + c = a 1 2 −  1  + 0 = 1, we need that a = 2. This gives us the solution T n   = 2n 2 − n that we had before. Calculate Exponent: If we were to guess that a · T  cid:1  then T n   = a · T some constant α. Plugging this into T n   = a · T a. Taking the log gives α · log b = loga, and solving gives α = log a T n   =  cid:2  nlog a   log b  =  cid:2  nlog 4  log 2  =  cid:2  n 2 .   cid:2  gives nα = a · cid:1   is much bigger than f  n  , . Further we guess that T n   = nα for   cid:2  + f  n   ≈ a · T   cid:2 α, or bα =  log b . In conclusion,   cid:1    cid:1    cid:2    cid:2    cid:1   n b  n b  n b  n b  n b   cid:7    cid:3    cid:4    cid:4    cid:4  cid:8   Unwinding: A useful technique is to unwind a recursive relation for a few steps and to look for a pattern: T n   = f  n   + a · T  cid:3  = f  n   + af n  cid:3  b = f  n   + af n  cid:3  = h−1 cid:9  b  + a · T  cid:3  = f  n   + af n  cid:4  b = ··· + a 3 · T  cid:10   cid:3  h cid:9   = f  n   + a · n  cid:4  b + a 2 · T  cid:3  + a 2 f  n b 2 + a 2 ·  cid:4  cid:11    cid:3   cid:4   cid:4   cid:4   + a · T  + a h · T 1  =  cid:2   n b3 ai · f  ai · f   cid:3   cid:4    cid:4  cid:8   n b 2  n b 2  n b 2  n b3   cid:3    cid:4    cid:3    cid:4    cid:3    cid:3   n b   cid:7   f  f  .  n bi  i=0  n bi  i=0  Filling the Table: My recommended way to evaluate recursive relations is to ﬁll out a table like that in Figure 27.1.   a  Number of Stack Frames at the ith Level: Level 0 contains the one initial stack frame at the top of the tree of stack frame. It recursively calls a times. Hence, level 1 has a stack frames. Each of these recursively calls a times, giving a 2 stack frames at level 2. Each successive level, the number of stack frames goes up by a factor of a, giving ai at level i.   b  Size of Instance at the ith Level: The top stack frame at level 0 is given an in- stance of size n. It recurses on a subinstances of size n b . Stack frames at level 1, given instances of size n b , recurse on subinstance of size n b 2. Each successive level decreases the instance size by a factor of b, giving size n bi at level i.   c  Time within One Stack Frame: On an instance of size n, a single stack frame requires f  n   time. Hence, a stack frame at the ith level, with an instance of size n bi, requires f  n bi  time.   d  Number of Levels: The recursive program stops recursing when the instance becomes sufﬁciently small, say of size 0 or 1. Let h denote the level at which this   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes27 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  19:5  2 n +    4   n   T 2 =    n   T  2 n +    3   n   T 9 =    n   T  n +    2   n   T 4 =    n   T  e l p m a x E    n g o l     cid:2  =  n g o  l  4 g o  l  = h    h c a e e m   cid:2   2   cid:2   i  i  n4   cid:1   18  12   cid:1   n =  2 g o  l  4 g o  l  n =  i  ·  ·  2  i t      l e v e l    n g o 0 l   =    cid:2  n  cid:12  g o 0 l   =  cid:2   cid:12   t a     ·  i  i  0 = hi  2 n =   cid:12   =  : e s a e r c e d c i r t e m o e G  n g o  l  4 g o  l  2 = h 2   cid:2     n   f   cid:1   l e v e l  p o t   cid:2   cid:2  =  cid:1   2 n    n   T   cid:2  =   cid:2   2  i  n4   cid:1    cid:2   =  1 =  i  n4  i  2   cid:1   i  n4  f  h n4  403    n g o l     cid:2  =  n g o  l  3 g o  l  = h   cid:2   2    h c a e e m  i  n3  1  2 n =  9 g o  l  3 g o  l  n =   cid:1   i  ·  ·  9  i t      l e v e l    n g o 0 l   =    cid:2  n  cid:12  g o 0 l   =  cid:2   cid:12   t a     ·  i  i  0 = hi  2 n =   cid:12   =  n g o  l  3 g o  l  9 = h 9   cid:2   n g o  l   n   f   cid:2   n g o  l  2 n   cid:1   s l e v e l  l l a   cid:2  =  cid:1     n   T   cid:2  =   cid:2   2  i  n3   cid:1    cid:2   =  1 =  i  n3  i  9   cid:1   i  n3  f  h n3  :  m u s c i t e m h t i r A  : e s a e r c n  i c i r t e m o e G  ? y b d e t a n m o D  i    g     cid:2    cid:1   nb  l  . e b a t  e h t n  i  g n  i l l  ﬁ  y b   c  n    cid:2  +  T  ·  a =    n   T  i  g n v o S  l  :  .  1 7 2  e r u g F  i      n   T     cid:2     h      n g o l     cid:2  =  n g o  l  2 g o  l  = h    h c a e e m  i t    2 n =   cid:2   i  n2   cid:1   i  2  ·  i  ·  4    l e v e l    n g o   0 l n   =  cid:2  g o  cid:12  0 l   =  cid:2   cid:12   t a     ·  i  i  0 = hi  n =   cid:12   =  n g o  l  2 g o  l  4 g o  l  2 g o  l  n =  4 = h 4   cid:2    cid:2    cid:1   2 n   cid:2  =  b g o  l   a g o n  l   cid:1    cid:4   s e s a c e s a b   cid:2  =  cid:3   4 g o  l  2 g o  l  n    n   T   cid:2  =   cid:2   i  n2   cid:1    cid:2   =  1 =  i  n2  i  4   cid:1   i  n2  f  h n2  l e v e l  h t i e h t  t a s e m a r f  f o  .  o N    a    l e v e l  h t i t a e z i s e c n a t s n I    b    e m a r f k c a t s e n o n h t i  i  w e m T   c    i  s l e v e l  f o  .  o N    d    s e m a r f k c a t s e s a c e s a b f o  .  o N    e    m u s a s a   n   T   f     April 5, 2008  19:5  P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes27 CUUS154-Edmonds 978 0 521 84931 9  Appendix  404  occurs. We have seen that the instances at level h have size n bh. Setting n bh = 1 and solving for h gives h = log n log b .   e  Number of Base Case Stack Frames: The number of stack frames at level i is ai. Hence, the number of base case stack frames is a h = a log n  log b. Though this looks ugly, Chapter 24 gives a log n  log b =  2log a  log n  log b = 2log a·log n  log b =  2log n log a   log b = nlog a   log b. Given that loga log b is simply some constant, nloga   log b is a simple polynomial in n.   f  T n  as a Sum: There are ai stack frames at level i, and each requires f  n bi  time, for a total of ai · f  n bi  at the level. We obtain the total time T n   for the recursion by summing the times at all of these levels. This gives   cid:13  h−1 cid:9    cid:4  cid:14    cid:3    cid:10   h cid:9   n bi  T n   =  ai · f  cid:10  Plugging in f  n   = nc gives h cid:9  T n   =  cid:2   i=0  i=0  + a h · T 1  =  cid:2   cid:10   cid:4 c  cid:3  nc · h cid:9   =  cid:2    cid:11   ai · f  cid:4 i  cid:3   a bc  i=0  ai ·  n bi  i=0   cid:4  cid:11   .   cid:3   n bi   cid:11   ◦  ◦   g  Dominated By: The key things to remember about this sum are that it has   = f  n   and the base case term being  cid:2  log n   terms, the top term being a an f  n bn   = a log n  log b f  n   = nlog a   log b  cid:2  nlog a   log b . According to the adding- made-easy approximations given in Chapter 26, if either the top term or the base case term is sufﬁciently bigger then the other, then the total is dominated by this term. On the other hand, if they are roughly the same, then the total is approxi- mately the number of terms times a typical term.  f  n b  log a log b   h  Evaluating the Sum: If T n   =   cid:2  nc · cid:12   cid:2  nc · cid:12   h i=0  a Similarly, if  cid:2  base case term  =  cid:2  nlog a   log b .  < c, then a  bc < 1, giving that the terms in bc  i  decrease exponentially, giving T n   =  cid:2  top term  =  cid:2  f  n   . > c, then the terms increase exponentially, giving T n   = bc  i  =  cid:2  nc · cid:12  bc = 1, giving T n   =  i=0 1  =  cid:2  nc · h  =  cid:2  f  n   log n .  then a  = c,  h i=0  a  log a log b  log a log b  If  h  EXERCISE 27.2.1  See solution in Part Five.  Solve the famous Fibonacci recurrence relation Fib 0  = 0, Fib 1  = 1, and Fib n   = Fib n − 1  + Fib n − 2  by plugging in Fib n   = αn and solving for α.  EXERCISE 27.2.2  See solution in Part Five.  Solve the following by unwinding them: 1. T n   = T n − 1  + n 2. T n   = 2 · T n − 1  + 1   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes27 CUUS154-Edmonds 978 0 521 84931 9  April 5, 2008  19:5  405  . t s e n a e l c e h t h t a m e h t  s e k a m o r e z e z i s  f o e s a c e s a b a g n i v a H  , e s a c  e s a b  e h t  r o f  t p e c x e    1    cid:2  k r o w s a h h c i h w  0 =    b  ·  i  − n    f    1    cid:2   · 1 +  0 · 1 1 − 0 b =   n    h c a e e m  i t      l e v e l  t a      cid:4   ·   cid:12   cid:3   0 = hi   cid:12   =  i    1    cid:2  =  : e s a e r c e d c i r t e m o e G    1     cid:2  =    n   T  s e s a c e s a b  1  1  b  ·  i  − n  c   b  ·  i  − n    =    b  ·  i  − n    f  i  a  l e v e l  h t i e h t  t a s e m a r f  f o  .  o N    a    e m a r f k c a t s e n o n h t i  i  w e m T   c    i  l e v e l  h t i t a e z i s e c n a t s n I    b    nb  = h  ,  0 = b  ·  h − n    h c a e e m  i t      l e v e l  t a     c   b  ·  i  − n    · 1 0 b =   n  i  ·   cid:12   0 = hi   cid:12   =   cid:2    cid:1   :  m u s c i t e m h t i r A  c  n  ·  nb   cid:2  =    1 + c  n    cid:2  =  s l e v e l  l l a    n   T  c   b  ·  i  − n    ·  i  a 0 b =   n  i    h c a e e m  i t      l e v e l  t a     ·  n 1b a = h a   cid:12   0 = hi   cid:12   =  : e s a e r c n  i c i r t e m o e G   cid:2    cid:1   b   n a  s e s a c e s a b   cid:2  =    n   T  s e m a r f k c a t s e s a c e s a b f o o N    e    m u s a s a   n   T   f    s l e v e l  f o  .  o N    d    y b d e t a n m o D  i    g        n   T    cid:2     h    l  . e b a t  e h t n  i  g n  i l l  ﬁ  y b   c  n    cid:2  +    b − n   T  ·  a =    n   T  i  g n v o S  l  :  .  2 7 2 e r u g F  i  0 +    b − n   T =    n   T  c n +    b − n   T =    n   T  c n +    b − n   T a =    n   T  e l p m a x E   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes27 CUUS154-Edmonds 978 0 521 84931 9  Appendix  April 5, 2008  19:5  EXERCISE 27.2.3 Does setting the size of the base case to 5 have any practical effect? How about setting the size to zero, i.e., n bh = 0? Why does this happen? If instead instances at the ith level had size n − itshape AB,  would an instance size of 0, 1, or 2 be better? How many levels h are there?  406  EXERCISE 27.2.4  See solution in Part Five.  Section 27.2 solves T n   = a T n b   + f  n   for f  n   = nc. If f  n   = nc logd n and log a = c, then the math is harder. Compute the sum for d > −1, d = −1, d < −1.  Hint: Reverse the order of the terms.   log b  EXERCISE 27.2.5 Use the method in Figure 27.2 to compute each of the following recursive relations. 1. T n   = nT n − 1  + 1 √ 2. T n   = 2T  n  + n 3. T n   = T u · n   + T v · n   +  cid:2  n   where u + v = 1.  EXERCISE 27.2.6 Running time:  algorithm Careful n   cid:2  pre-cond cid:3 : n is an integer.  cid:2  post-cond cid:3 : Q n   “Hi”s are printed for some odd function Q begin  if  n ≤ 1    else  PrintHi 1  loop i = 1 . . . n PrintHi i  end loop loop i = 1 . . . 8 Careful  n 2    end loop  end if  end algorithm  algorithm PrintHi n   cid:2  pre-cond cid:3 : n is an integer.  cid:2  post-cond cid:3 : n 2 “Hi”s are printed begin  loop i = 1 . . . n 2 Print “Hi”   end loop  end algorithm   P1: ...   cid:1  cid:1  Gutter margin: 7 8   cid:1  cid:1  Top margin: 3 8  TheNotes27 CUUS154-Edmonds 978 0 521 84931 9  Recurrence Relations  1. Give and solve the recurrence relation for the number of “Hi”s, Q n  . Show your  work. Give a sentence or two giving the intuition.  2. What is the running time  time complexity  of this algorithm as a function of the  size of the input?  April 5, 2008  19:5  407   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes28 CUUS154-Edmonds 978 0 521 84931 9  January 31, 2008  15:45  408 28 A Formal Proof of Correctness  Though I mean is not to be too formal, it is useful to at least understand the required steps in a formal proof of correctness.  Speciﬁcations: Before we prove that an algorithm is correct, we need to know pre- cisely what it is supposed to do.  Preconditions: Assertions that are promised be true about the input instance.  Postconditions: Assertions that must be true about the output.  Correctness: Consider some instance. If this instance meets the preconditions, then after the code has been run, the output must meet the postconditions:   cid:2 pre-cond cid:3  & codealg ⇒  cid:2 post-cond cid:3   The correctness of an algorithm is only with respect to the stated speciﬁcations. It does not guarantee that it will work in situations that are not taken into account by this speciﬁcation.  Breaking the Computation Path into Fragments: The method to prove that an algorithm is correct is as follows. Assertions are inserted into the code to act as check- points. Each assertion is a statement about the current state of the computation’s data structures that is either true or false. If it is false, then something has gone wrong in the logic of the algorithm. These assertions break the path of the computation into fragments. For each such fragment, we prove that if the assertion at the beginning of the fragment is true and the fragment gets executed, then the assertion at the end of the fragment will be true. Combining all these fragments back together gives that if the ﬁrst assertion is true and the entire computation is executed, then the last as- sertion will be true.  A Huge Number of Paths: There are likely an exponential number or even an in- ﬁnite number of different paths that the computation might take, depending on the input instance and the tests that occur along the way. In contrast, there are not many   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes28 CUUS154-Edmonds 978 0 521 84931 9  A Formal Proof of Correctness  January 31, 2008  15:45  different computation path fragments. Hence, it is much easier to prove the correct- ness of each fragment than of each path.  The following table outlines the computational path fragments that need to be  tested for different code structures.  Single Line of Code:  cid:2 pre-assignment-cond cid:3 : The variables x and y have meaningful values. z = x + y  cid:2 post-assignment-cond cid:3 : The variable z takes on the sum of the value of x and  the value of y. The previous value of z is lost.  409  Blocks of Code:  cid:2 assertion0 cid:3  code1  cid:2 assertion1 cid:3  code2  cid:2 assertion2 cid:3   If Statements:  cid:2 pre-if -cond cid:3  if   cid:2 test cid:3    then codetrue else codefalse end if  cid:2 post-if -cond cid:3   Loops:  cid:2 pre-loop-cond cid:3  loop cid:2 loop-invar cid:3  exit when  cid:2 exit-cond cid:3  codeloop end loop  cid:2 post-loop-cond cid:3   Function Call:  cid:2 pre-call-cond cid:3  output = Func input   cid:2 post-call-cond cid:3    cid:1    cid:1   [ cid:2 assertion0 cid:3  & code1 ⇒  cid:2 assertion1 cid:3 ] [ cid:2 assertion1 cid:3  & code2 ⇒  cid:2 assertion2 cid:3 ] ⇒ [ cid:2 assertion0 cid:3  & code1&2 ⇒  cid:2 assertion2 cid:3 ]   cid:1   ⎫⎪⎪⎪⎬ ⎪⎪⎪⎭  [ cid:2 pre-if -cond cid:3  &  cid:2 test cid:3  & codetrue ⇒  cid:2 post-if -cond cid:3 ] [ cid:2 pre-if -cond cid:3  & ¬ cid:2 test cid:3  & codefalse ⇒  cid:2 post-if -cond cid:3 ] ⇒ [ cid:2 pre-if -cond cid:3  & code ⇒  cid:2 post-if -cond cid:3 ]   cid:1  cid:3  & ¬ cid:2 exit-cond cid:3  & codeloop ⇒  cid:2 loop-invar  [ cid:2 pr e-loop-cond cid:3  ⇒  cid:2 loop-invar cid:3 ] [ cid:2 loop-invar [ cid:2 loop-invar cid:3  &  cid:2 exit-cond cid:3  ⇒  cid:2 post-loop-cond cid:3 ] Termination ⇒ [ cid:2 pr e-loop-cond cid:3  & code ⇒  cid:2 post-loop-cond cid:3 ]   cid:1  cid:1  cid:3 ]  [ cid:2 pre-call-cond cid:3  ⇒  cid:2 pre-cond cid:3 Func] [ cid:2 post-cond cid:3 Func ⇒  cid:2 post-call-cond cid:3 ] ⇒ [ cid:2 pre-call-cond cid:3  & code ⇒  cid:2 post-call-cond cid:3 ]   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   TheNotes28 CUUS154-Edmonds 978 0 521 84931 9  January 31, 2008  15:45  410   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   ExSol CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  16:4  PART FIVE  411  Exercise Solutions  Chapter 1. Iterative Algorithms: Measures of Progress and Loop Invariants  1.4.1 Selection Sort: If the input for selection sort is presented as an array of values, then sorting can happen in place. The ﬁrst k entries of the array store the sorted sublist, while the remaining entries store the set of values that are on the side. Finding the smallest value from A[k + 1] . . . A[n] simply involves scanning the list for it. Once it is found, moving it to the end of the sorted list involves only swapping it with the value at A[k + 1]. The fact that the value A[k + 1] is moved to an arbitrary place in the right-hand side of the array is not a problem, because these values are considered to be an unsorted set anyway. The running time is computed as follows. We must se- lect n times. Selecting from a sublist of size i takes  cid:1  i  time. Hence, the total time is  cid:1  n +  n−1  + ··· + 2 + 1  =  cid:1  n2   see Chapter 26 .  1.4.2 Insertion Sort: There are two steps involved in inserting an element into a sorted list. The most obvious step is to locate where it belongs. The second step to shift all the el- ements that are bigger than the new element one to the right to make room for it. You can ﬁnd the location for the new element quickly using a binary search. However, it is easier to search and shift the larger elements simultaneously.  Linked List: Having the sorted elements stored in a linked list allows one to insert the new element in constant time. However, it then takes  cid:1  k  time to ﬁnd where the new element goes.  Running Time: We must insert n times. Inserting into a sublist of size i takes  cid:1  i  time. Hence, the total time is  cid:1  1 + 2 + 3 + ··· + n  =  cid:1  n2 .  Heap Sort: We will see in Section 10.4 that each of these steps can be done in  cid:1  log n  time when the elements are stored in a data structure called a heap.  1.4.3 The algorithm repeatedly passes through the array, swapping adjacent pairs if needed. After k such passes, the largest k elements have bubbled up to where they belong. Hence, it requires at most n passes until all elements are in place. Each pass requires n comparisons.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   ExSol CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  16:4  412  Exercise Solutions  1.5.1 There are a number of problems.  1. A loop invariant shall to be a picture of the current state and not say what the itera- 2. The loop invariant is not established correctly. With i = 1, the loop invariant requires  tion does. The loop invariant should simply be s = cid:1  s = cid:1   j=1 j = 1, not s = 0. The choice s = 0 and i = 0 would be better.  i j=1 j .  1  3. The loop invariant is not maintained correctly. Let s  be the values of s and i  cid:1  = cid:1   cid:1  cid:1  when at the top of the loop. Let s and i be the values after going around again. The  cid:1   cid:1  cid:1  = s  cid:1  + 1.  cid:1  i j=1 j . The code gives that s loop invariant gives that s  cid:1  cid:1  =   j=1 j   + i  cid:1 +1  cid:1  i i j=1 j as required, because Together these give that s  cid:1  + 1 should be added in order to maintain the loop  cid:1  i is being added in twice. i invariant.  . This is not   cid:1  cid:1  = i   cid:1  + i   cid:1   and i  and i   cid:1  cid:1    cid:1    cid:1    cid:1    cid:1   4. The exit condition is not very well stated. An equivalent and easier-to-see exit condi-  tion would be “exit when i > I .”  5. The exit condition, i > I , and the loop invariant, s = cid:1  postcondition. Instead, they give that s = cid:1  to s = cid:1   i−1 j=1 j .  I+1 j=1 j is returned.  6. The algorithm as a whole happens to work. A quick ﬁx is to change the loop invariant  i j=1 j , together do not give the  Chapter 2. Examples Using More-of-the-Input Loop Invariant  2.2.1  Divide: Sorry, not provided.  Calculator: algorithm Calculator     cid:2  pre-cond cid:3 : A stream of commands are entered.  cid:2  post-cond cid:3 : The results are displayed on a screen. begin  allocate accum,current ∈ {0..108 − 1} allocate screen ∈ {showA, showC} accum = current = 0 screen = showC loop cid:2 loop-invariant cid:3 : The bounded memory of the machine remembers the  current value of the accumulator and the current value being entered. It also has a Boolean variable that indicates whether the screen should display the current or the accumulator value. get c  if  c ∈ {0..9}   then  else if  c = cid:1  + cid:1   current = 10 × current + c mod 108 screen = showC   then accum = accum + current mod 108 current = 0 screen = showA  cid:1    then clr accum = 0  else if  c = cid:1    P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   ExSol CUUS154-Edmonds 978 0 521 84931 9  Exercise Solutions  March 29, 2008  16:4  413  current = 0 screen = showC  end if if  screen = showC   then  display current   else  end if  display accum   end loop end algorithm  Longest Block of Ones:  length leng.  begin  algorithm LongestBlockOfOnes A , n   cid:2  pre-cond cid:3 : The input is A, a 0, 1 array of length n.  cid:2  post-cond cid:3 : The output is the location A[k1..k2] of the longest block of ones and its  i = 0; pmax = 1; qmax = 0; lengmax = 0 ∈ {0..n} loop cid:2 loop-invariant cid:3 : A[pmax, qmax] is a longest block of ones in A[1..i] and  = 0; pcurrent = 1; lengcurrent  = qmax − pmax+1 is its length. = i − pcurrent+1 is its length.  lengmax A[pcurrent, i] is the longest block of ones in A[1..i] ending in A[i] and lengcurrent exit when i = n if  A[i+1] = 1   then  = lengcurrent  + 1  else  lengcurrent pcurrent = i+2 = 0 lengthcurrent  end if if  lengmax < lengcurrent   then  pmax = pcurrent qmax = i + 1 lengthmax  = lengthcurrent  end if i = i + 1  end loop return A[pmax, qmax]  end algorithm  2.3.1   2 : The loop invariant is: a. The beginning [0, a] of the cake has been partitioned into Q disjoint pieces. b. Each player pi ∈ Q has been allocated a piece [ai, bi] worth at least 1 n to him. c. The remaining [a, 1] interval of the cake is worth at least  n − Q  n to each of the  remaining players, i.e., to those in P − Q.   March 29, 2008  16:4  P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   ExSol CUUS154-Edmonds 978 0 521 84931 9  Exercise Solutions  414   3 : algorithm Partition P   cid:2  pre-cond cid:3 : As above  cid:2  post-cond cid:3 : As above begin  a = 0 and Q = ∅ loop cid:2 loop-invariant cid:3 : As above.  exit when Q = n loop i ∈ P − Q  ci = Cut pi, a, 1 n    end loop imin = the i ∈ P − Q that minimizes ci [aimin , bimin ] = [a, cimin ] a = cimin Q = Q + imin  end loop return all parts [ai, bi] for each i ∈ P  end algorithm   1 ,  4 – 7 : Sorry, the remaining solutions are not provided.  Chapter 3. Abstract Data Types  3.1.5 Instead of bounding the height given the number of nodes, it is easier to compute the reverse relation. Let N h  be the minimal number of nodes in an AVL tree of height h. In order for a tree to be of height h, it must have at least one subtree of height h − 1. In order for it to be an AVL tree, the other subtree can differ by at most one, so it must have height at least h − 2. It follows that the number of nodes in this tree is at least N h  = N h−1  + N h−2  + 1. Except for the +1 of the root, this is that same as the famous Fibonacci numbers deﬁned by Fib n  = Fib n−1  + Fib n−2 . Exercise 27.2.1 goes on to prove that Fib n  =  cid:1  αn , where α = 1+√  . If N h  =  cid:1  αn , then H n  =  cid:1  log n .  5  2  3.2.4 The tests will be executed in the order that they are listed. If next = nil is tested ﬁrst and passes, then because there is an OR between the conditions, there is no need to test the second. However, if next.info ≥ key is the ﬁrst test and next is nil, then using next.info to retrieve the information in the node pointed to by next will cause a run-time error.  Chapter 4. Narrowing the Search Space: Binary Search  4.4.1 Doing binary search in O log n × m   time is impossible. See the lower bound in ques- tion  Exercise 7.0.7 . If you take O n log m  time doing binary search in each row, then you are taking too much time. It can be done by examining n + m − 1 entries. Ob- serve that the values in the matrix increase from A[1, 1] to A[n, m]. Hence, the bound- ary between values that are less than or equal to x and those that are greater follows some monotonic path from A[1, m] to A[n, 1]. The algorithm traces this path starting at A[1, m]. When it is at the point A[i, j ], the loop invariant is that we have stored the   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   ExSol CUUS154-Edmonds 978 0 521 84931 9  Exercise Solutions  March 29, 2008  16:4  best answer from those outside the subrectangle A[i..n, 1..j ]. Initially, this is true for [i, j ] = [1, m], because none of the matrix is excluded. Now suppose it is true for an arbi- trary [i, j ]. The algorithm then compares A[i, j ] with x. If it is better than our current best answer, then our current best is replaced. If A[i, j ] ≤ x, then because the values in the row A[i, 1..j ] are all smaller than or equal to A[i, j ], these are worse answers, and hence we can conclude that we now have the best answer from those outside the subrectan- gle A[i + 1..n, 1..j ]. We maintain the loop invariant by increasing i by one. On the other hand, if A[i, j ] > x, then it is too big and so are all the elements in the column A[i..n, j ] which are even bigger. We can conclude that we have the best answer from those outside of the subrectangle A[i..n, 1..j − 1]. We maintain the loop invariant by decreasing j by one. The exit condition is i..n = 0 or 1..j = 0  i.e., i > n or j < 1. When this occurs, the subrectangle A[i..n, 1..j ] is empty. Hence, our best answer, which by the loop invariant is the best from those outside this subrectangle, must be the best overall. The measure of progress, i..n + 1..j − 1 =  n − i + 1  +  j   − 1, is initially n + m − 1 and decrease by one each iteration. After n + m − 1 iterations, either the algorithm has already halted or the measure has reached zero, at which point the exit condition is deﬁnitely met.  415  Chapter 6. Euclid’s GCD Algorithm  6.0.2   cid:1    cid:1    cid:1   s  , r  , s   2 : The loop invariant  cid:3  × r + s = x × y is established trivially by setting  cid:3  = x, r = y, and s = 0. Let  cid:3  cid:1  be the values when at the top of the loop, and assume that  cid:3  cid:1  ×  cid:1  + s  cid:1  = x × y. r is odd, then  cid:3  cid:1  cid:1  =  cid:3  cid:1  − 1 and s  cid:1  cid:1  + In the ﬁrst step, if  cid:3  cid:1   cid:1  +  s  cid:1  + s  cid:1  cid:1  =   cid:3  cid:1  − 1  × r  cid:1  + r In the second step,  cid:3  cid:1  cid:1  cid:1  =  cid:3  cid:1  cid:1  2 and r  cid:1  cid:1  + s   + s  cid:1  cid:1     =  cid:3  cid:1  × r , which by the loop invariant is x × y.  , which by the loop invariant is x × y.  cid:1   cid:1  cid:1  cid:1  = 2r   2r  4 : The Ethiopians exit when  cid:3  = 1. But this being odd, they must add r to s. We will iterate one more time and exit when  cid:3  = 0. This exit condition gives s =  cid:3  × r + s, and the loop invariant gives  cid:3  × r + s = x × y. Hence, in the end s = x × y.  1 ,  3 ,  5 , and  6 : Sorry, not provided.  . This gives that  cid:3  cid:1  cid:1  cid:1  × r  . This gives that  cid:3  cid:1  cid:1  × r   cid:1  cid:1  cid:1  =   cid:3  cid:1  cid:1  2  ×   cid:1  cid:1  =  cid:3  cid:1  cid:1  × r   cid:1  cid:1  cid:1  + s   cid:1  cid:1  = s   cid:1  + r   cid:1  cid:1    cid:1  cid:1    cid:1   Chapter 7. The Loop Invariant for Lower Bounds  7.0.2 The bound is n ≤ r t .  Each round, he selects one row; hence, there are r possible answers. After t rounds,  there are r t possible combinations of answers.  The only information that you know is which of these combinations he gave you. Which card you produce depends deterministically  no magic  on the combination of answers given to you. Hence, depending on his answers, there are at most r t cards that you might output. n ≤ r t .  However, there are n cards, any of which may be the selected card. In conclusion, The book has n = 21, r = 3, and t = 2. Because 21 = n  cid:9 ≤ r t = 32 = 9, the trick in the  book does not work.  Two rounds is not enough. There need to be three rounds.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   ExSol CUUS154-Edmonds 978 0 521 84931 9  Exercise Solutions  March 29, 2008  16:4  7.0.3 It is a trick question, because with a balance there are three, not two, outcomes and hence only log3 n operations are needed. Divide the objects into three piles, two of equal size and the third as close as possible. Put the ﬁrst two piles on the scale. If one is heavier, then it contains the heavier object; otherwise the third pile does. Recurse on this one pile.  7.0.6 In the lower bound for parity, any starting input I would have worked equally well. Here, however, there is only one input that will work, and that is I being the all-zero string. This ensures that, as before, changing the j th bit of I , for any j ∈ J = [1, n], changes the answer from the AND being zero to the AND being one. This proves that any algorithm solving the problem requires time of at least n.  416  Chapter 8. Abstractions, Techniques, and Theory 8.5.2Ra : One might complain that if my instance is  cid:2 n, m cid:3 , then my friend’s instance cannot be  cid:2 n − 1, 2m cid:3 , because 2m is not smaller then m. However, we can deﬁne the size of instance  cid:2 n, m cid:3  to be simply n. According to this measure, my friend’s instance is indeed smaller. Moreover, when the instance becomes of size zero or smaller, then n ≤ 0 and the recursion stops. We prove that the depth of recursion is at most n as fol- lows. On instance  cid:2 n, m cid:3 , the size starts at n and decreases by at least one every level of recursion, so after n levels the size is at most zero and the algorithm stops recurs- ing further. For example, starting with  cid:2 5, 2 cid:3 , it recurses on  cid:2 4, 4 cid:3 ,  cid:2 3, 8 cid:3 , . . . ,  cid:2 0, 64 cid:3 , and then halts. Rb: One might claim that all is well because both friends get instances   cid:2 n − 1, m cid:3  and  cid:2 n, m − 1 cid:3   that are smaller. However, for this to be true for both friends, the size must be something like n + m. However, according to this deﬁnition, the instance  cid:2 5, −5 cid:3  is small, but the algorithm does not halt. There is a path down this recursive tree that is inﬁnite, namely  cid:2 n, m cid:3 ,  cid:2 n, m − 1 cid:3 ,  cid:2 n, m − 2 cid:3 , . . . cid:2 n, 1 cid:3 ,  cid:2 n, 0 cid:3 ,  cid:2 n, −1 cid:3 ,  cid:2 n, −2 cid:3 . . . . Rc: Here the size of the instance  cid:2 n, m cid:3  can be deﬁned to be n + m. According to this measure, each friend is given a smaller instance. Moreover, if the size on the instance is zero, then either n ≤ 0 or m ≤ 0. Either way the program halts. The depth of recur- sion can be at most n + m because this is the initial size and the size decreases by one each iteration. Rd : Let the size of the instance  cid:2 n, m cid:3  be 5n + 2m. Then the ﬁrst friend’s instance  cid:2 n − 1, m + 2 cid:3  has size 5 n − 1  + 2 m + 2  = 5n + 2m − 1, which is one smaller. The second friend’s instance  cid:2 n + 1, m − 3 cid:3  has size 5 n + 1  + 2 m − 3  = 5n + 2m − 1, which is also one smaller. Moreover, if the size on the instance is zero, then either n ≤ 0 or m ≤ 0. Either way the program halts. The depth of recursion can be at most 5n + 2m because this is the initial size and the size decreases by one each iteration. Re: I claim that there is a path down this recursion tree that is inﬁnite. If my instance is  cid:2 n, m cid:3 , then my ﬁrst friend has  cid:2 n − 4, m + 2 cid:3 , his ﬁrst friend has  cid:2 n − 8, m + 4 cid:3 , his ﬁrst friend has  cid:2 n − 12, m + 6 cid:3 , his second friend has  cid:2 n − 6, m + 3 cid:3 , and his second friend has  cid:2 n, m cid:3  which is the same as my instance. This can be repeated inﬁnitely often. It is interesting that the last two examples can be generalized to the friend’s instances of size  cid:2 n − a, m + b cid:3  and  cid:2 n + c, m − d cid:3 . If ad > bc then the program halts, else it does not.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   ExSol CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  16:4  417  Exercise Solutions  8.6.2  Fun 1  = X Fun 2  = Y Fun 3  = AYBXC Fun 4  = A AYBXC B Y C Fun 5  = A AAYBXCBYC B AYBX C Fun 6  = A AAAYBXCBYCBAYBXC B AAYBXCBYC C  8.7.2 To prove S 0 , let n = 0 in the inductive step. There are no values k where 0 ≤ k < n.  Hence, no assumptions are being made. Hence, your proof proves S 0  on its own.  Chapter 9. Some Simple Examples of Recursive Algorithms  9.1.1 Insertion sort and selection sort. 9.1.2 1. Given  cid:2 a1, a2, . . . , an cid:3 , I remove the last character an. I give  cid:2 a1, a2, . . . , an−1 cid:3  to my friend, and he returns the reversed tuple  cid:2 an−1, . . . , a1 cid:3  to me. I add an to the front of the tuple, producing  cid:2 an, an−1, . . . , a1 cid:3  as required. If my initial tuple has only zero  or one  element, then there is nothing to do. algorithm Rever se  cid:2 a1, a2, . . . , an cid:3    cid:2  pre-cond cid:3 : An instance is a tuple.  cid:2  post-cond cid:3 : The output is the reverse tuple  cid:2 an, an−1, . . . , a1 cid:3 .  begin  if n = 0 or n = 1  then  return  instance unchanged   return   cid:2 an, Reverse  cid:2 a1, a2, . . . , an−1 cid:3   cid:3     else  end if  end algorithm  2. The iterative program has two  nonnested  loops. The ﬁrst pushes each element on the stack, one at a time, starting with an. The loop invariant is that after i iterations what remains in the tuple is  cid:2 a1, a2, . . . , an−i−1, an−i cid:3  and the stack contains  cid:2 an−i+1, an−i+2, . . . , an cid:3  with an−i+1 at the top. At the i = 0 iteration, the loop invariant is trivially true. The next iteration removes the last element an−i from the tuple and pushes it on the stack. This maintains the loop invariant while making progress. In the end, with i = n,  cid:2 a1, a2, . . . , an cid:3  is on the stack with a1 at the top. The second loop pops each element off the stack and puts it at the beginning of the tuple. The loop in- variant is that after i iterations, the stack again contains  cid:2 ai+1, ai+2, . . . , an cid:3  with ai+1 at the top, but now the tuple is  cid:2 ai, ai−1, . . . , a2, a1 cid:3 . With i = n, the stack is empty and the tuple is  cid:2 an, an−1, . . . , a2, a1 cid:3 . 3. Recursion is implemented on a computer, using a stack of stack frames. The ﬁrst stack frame is given  cid:2 a1, a2, . . . , an cid:3 , and it removes and remembers the last char- acter an. Its friend is the second stack frame, which is given  cid:2 a1, a2, . . . , an−1 cid:3 . It re- moves and remembers its last character an−1. As we recurse deeper, the stack frames that have not yet completed are pushed on a stack. After i such stack frames, the loop invariant is that the the friend’s friend’s friend’s . . . friend is given the tuple   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   ExSol CUUS154-Edmonds 978 0 521 84931 9  Exercise Solutions  March 29, 2008  16:4  418  9.2.1   cid:2 a1, a2, . . . , an−i−1, an−i cid:3  and the stack contains stack frames, each remembering one of the elements an−i+1, an−i+2, . . . , an, with an−i+1 at the top. Note this is the same loop invariant as the iterative program. The recursive base case is reached when i = n and the stack frame is given the empty tuple. Then, one at a time, in reverse order the stack frames complete their computations by each adding its element to the beginning of the tuple. The loop invariant is that after i such returns, the stack of stack frames is remembering ai+1, ai+2, . . . , an with ai+1 at the top, and the cur- rent stack frame is returning the tuple  cid:2 ai, ai−1, . . . , a2, a1 cid:3 . Again, note that this is the same loop invariant as the iterative algorithm. With i = n, the stack is empty and the ﬁrst stack frame returns the tuple  cid:2 an, an−1, . . . , a2, a1 cid:3 .  1 . Given the integers a and b, the iterative algorithm creates two numbers x = b and y = a mod b. It notes that GCD a, b  = GCD x, y , and hence it can return GCD x, y  instead of GCD a, b . This algorithm is even easier when you have a friend. We sim- ply give the subinstance  cid:2 x, y cid:3  to the friend, and he computes GCD x, y  for us. For the iterative algorithm, we need to make sure we are making progress, and for the re- cursive algorithm, we need to make sure that we give the friend a smaller instance. Either way, we make sure that in some way  cid:2 x, y cid:3  is smaller than  cid:2 a, b cid:3 . For the it- erative algorithm, we need an exit condition that we are sure to eventually meet, and for the recursive algorithm, we need base cases such that every possible in- stance is handled. Either way, we consider the case when y or b is zero. The resulting code is  algorithm GCD a, b   cid:2  pre-cond cid:3 : a and b are integers.  cid:2  post-cond cid:3 : Returns GCD a, b . begin  if b = 0  then return  a    else  end if  return  GCD b, a mod b      cid:11 , then a = r · b + y or y = a − r · b.  end algorithm 2 . We will need to understand this relationship y = a mod b better. Here y is the re- mainder when you divide a by b. If we let r =  cid:10  a When we generalize the problem, the friend, in addition to g, also gives us usub and vsub such that usub · x + vsub · y = g = GCD x, y  = GCD a, b . Plugging in x = b and y = a − r · b gives usub · b + vsub ·  a − r · b  = g, or vsub · a +  usub − vsub · r   · b = g. Hence, if we set u = vsub and v = usub − vsub · r , then we get u · a + v · b = g = GCD  a, b  as required. I simply provide these answers. For the base case with b = 0, we have g = GCD a, b  = a. Hence, u = 1 and v = 0 gives that u · a + v · b = g = GCD a, b . The resulting code is  b  algorithm GCD a, b   cid:2  pre-cond cid:3 : a and b are integers.  cid:2  post-cond cid:3 : Returns integers g, u, and v such that u · a + v · b = g = GCD a, b .   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   ExSol CUUS154-Edmonds 978 0 521 84931 9  Exercise Solutions  begin  if b = 0  then  else  b   cid:11   return   cid:2 a, 1, 0 cid:3    x = b r =  cid:10  a y = a − r · b  cid:2 g, usub, vsub cid:3  = GCD x, y  u = vsub v = usub − vsub · r return   cid:2 g, u, v cid:3     end if  March 29, 2008  16:4  419  end algorithm 3 . Our goal is to ﬁnd two integers U and V such that U · a + V · t = w. Then you give the storekeeper U of the a coins and V of the b coins for a total worth of w dollars. If U or V is negative, this amounts to the storekeeper giving you coins as change. integers g, u, and v such that u · a + v · b = g = GCD a, b .  To ﬁnd U and V , let’s start by calling the GC D algorithm on a and b. This returns g   · b = If g divides evenly into w, then multiplying through by w g   = w, and we are done. g  w By the deﬁnition of g = GC D a, b , we know g divides a and b, and hence it divides U · a + V · b evenly. It follows that if g does not divide w evenly, then there is no integer solution to U · a + V · b = w. 4 . Solution not provided.  g   · a +   vw  g gives   uw  Chapter 10. Recursion on Trees  10.3.1  algorithm Smallest tr ee, k   cid:2  pre-cond cid:3 : tr ee is a binary search tree, and k > 0 is an integer.  cid:2  post-cond cid:3 : Outputs the kth smallest element s and the number n of elements. If it this index is out of range, we output s = NotPossible. begin  if  tree = emptyTree   then  else  result   cid:2 NotPossible, 0 cid:3     cid:2 sl, nl cid:3  = Smallest leftSub tree , k  % There are nl + 1 nodes before the right subtree  cid:2 sr , nr cid:3  = Smallest rightSub tree , k −  nl + 1   n = nl + 1 + nr if  k ∈ [1..nl]  then elseif  k = nl + 1  then elseif  k ∈ [nl + 2..n]  then  s = sl s = root tree  s = sr   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   ExSol CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  16:4  Exercise Solutions  else then  s = OutOfRange  endif result   cid:2 s, n cid:3     420  end if  end algorithm  10.4.1 1. Where in the heap can the value 1 go? It must be in one of the leaves. If 1 were not at a leaf, then the nodes below it would need a smaller number, of which there are none. 2. Which values can be stored in entry A[2]? It can contain any value in the range 7–14. It can’t contain 15, because 15 must go in A[1]. We already know that A[2] must be greater than each of the seven nodes in its subtree. Hence, it can’t contain a value less than 7. For each of the other values, a heap can be constructed such that A[2] has that value.  3. Where in the heap can the value 15 go? 15 must go in A[1]  as we have mentioned . 4. Where in the heap can the value 6 go? 6 can go anywhere except A[1], A[2], or A[3].  A[1] must contain 15, and A[2] and A[3] must be at least 7.  10.5.1  algorithm Derivative f, x   cid:2  pre-cond cid:3 : f is an equation and x is a variable  cid:2  post-cond cid:3 : The derivative of f with respect to x is returned. begin  if  f = “x”   then else if  f = a real value or a single variable other than “x”   then  result  1    % Copy needed for “*” and “ ”. % Three copies needed for “ ”.  result  0    end if  % if f is of the form  goph  g = Copy leftSub f    h = Copy rightSub f     cid:1  = Derivative leftSub f  , x  g  cid:1  = Derivative rightSub f  , x  h if  f = g + h   then  cid:1  + h  cid:1   cid:1  − h  cid:1   else if  f = g − h   then else if  f = g ∗ h   then  % See Figure 10.5.3.a result  g  result  g        % See Figure 10.5.3.b  cid:1  ∗ h + g ∗ h  cid:1  result  g    else if  f = g h   then  % See Figure 10.5.3.c  cid:1  ∗ h − g ∗ h  cid:1  result  g     h ∗ h     end if  end algorithm   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   ExSol CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  16:4  Exercise Solutions  10.5.3  simplify f’  = s v = 1  x*x   s = N–I = 0–K = 1  v = D*D = x*x  t = A’*D = 0*D = 0  u = A*D’ = 1*1 = 1  D=x  D=x  A’ = m q = 0 q = 0  D = x  A = x x = 1  D’ = 1  421  m = n–p = x–x = 0  q = x*x  n = 1*x = x  p = x*1 = x  Chapter 11. Recursive Images 11.1.1 Falling Line: This construction consists of a single line with image n − 1 raised, tilted,  and shrunk:  11.1.2 Binary Tilt: This image is the same as the birthday cake. The only differences are that  the two places to recurse are tilted and one of them has be ﬂipped upside down:  Chapter 12. Parsing with Context-Free Grammars  12.0.1  s =       1   * 2 + 3   * 5 * 6 + 7   -exp--------------------------- -term-------------------------- -fact--------------------------   -exp-----------------------    -term------------------ + t f -fact---------- * f * f   -exp-------   6 7  5  -t----- + t f -f- * f   e   2 3  t f 1  s =       1   * 2 + 3   * 5 * 6 + 7    Sorry, the solution for 2 and 3 are not included.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   ExSol CUUS154-Edmonds 978 0 521 84931 9  Exercise Solutions  422  March 29, 2008  16:4  Chapter 14. Graph Search Algorithms  14.2.1 The node v can’t be in Vk cid:1  for k   cid:1  > k + 1, because there is a path of length k + 1 to it, namely the path to u followed by the edge  cid:2 u, v,  cid:3 . If v has not been found before, then we are just ﬁnding it. Its d v  is being set to d u  + 1 = k + 1. By LI1, this must be its distance from s. Hence, v must be in Vk+1. If v has been found before, it is because a shortest path has already been found to it. If the edge  cid:2 u, v,  cid:3  is directed, then this previous path could  cid:1  ≤ k + 1. However, if this edge is undirected, then there is a catch. have any length k  cid:1  + 1 from s to v followed Suppose v is in Vk cid:1  . Then a possible path to u is that of length k by the edge {u, v, } backwards to u. because the shortest path to u is of length k, we have  cid:1  + 1 ≥ k or k k   cid:1  ∈ {k − 1, k, k + 1}.  14.2.2 The shortest-path algorithm given in this section is identical to the generic search algo- rithm in Section 14.1 except that a queue is used. Hence, the running time is  cid:1  E . The time is not less if you are searching for a path to a speciﬁc node t.  14.3.2 Despite differences in the algorithms, on a graph with edge weights one, breadth-ﬁrst search and Dijkstra’s algorithm are identical. Breadth-ﬁrst search handles the ﬁrst node in its queue, whereas Dijkstra’s algorithm handles the node with the next smallest d v . However, breadth-ﬁrst search’s third loop invariant ensures that the nodes are found and added to the queue in the order of distance d v . Hence, handling the next in the queue amounts to handling the next smallest d v . Breadth-ﬁrst search’s ﬁrst loop in- variant states that the correct minimal distance d v  to v is obtained when the node v is ﬁrst found, whereas with Dijkstra’s algorithm we are not sure to have it until the node is handled. However, with edge weights one, when v is ﬁrst found in Dijkstra’s algorithm, d v  is set to the length of the overall shortest path and never changed again.  14.6.2 The shortest path to node v will not contain any nodes u that appear after it in the total order, because by the requirements of the total order there is no path from u to v. Hence, it is ﬁne to handle v, committing to a shortest path to v, before considering u. Hence, it is ﬁne to handle the nodes in the order given by the total order. The advantage of this algorithm is that you do not need to maintain a priority queue, as done in Dijkstra’s algorithm. This decreases the time from  cid:1  E logV  to  cid:1  E .  Chapter 15. Network Flows and Linear Programming 15.2.5 Given a network  cid:2 G, s, t cid:3 , run the max ﬂow algorithm on it. In addition to returning a maximum ﬂow, it also returns a minimum cut, which is used to witness the fact that there is no better ﬂow.  15.5.1 1. The ﬁrst does not have a matching. A witness is the fact that nodes 1, 3, and 5 are only connected to B and D. Hence, the three can’t be matched to the two. In the language of Hall’s theorem, let A = {1, 3, 5}; then N A  = {B, D}. Because A > N A , Hall’s theorem gives that there is no matching. The second does have a matching. A witness is the following matching:  1  2  3  4  5  1  2  3  4  5  A  B  C  D  E  A  B  C  D  E   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   ExSol CUUS154-Edmonds 978 0 521 84931 9  Exercise Solutions  March 29, 2008  16:4  423  2. Consider an arbitrary A ⊆ L. Note that the set B = {M u   u ∈ A} contains A distinct nodes and that B ⊆ N A . Hence, A ≤ N A . 3. Let A = U ∩ L be the set of nodes that are both on the left side of the bipartite graph and on the left side of the cut. Consider any node v ∈ N A . Because v ∈ N A , there is a node u ∈ A ⊂ U such that  cid:2 u, v cid:3  is an edge. If v ∈ V , then this edge  u, v  crosses the cut. But this edge has capacity ∞. In this case, the capacity of the cut is well over L. On the other hand, if v ∈ U, then the edge from v to t is across the cut. Now consider any node u ∈ L − A ⊂ V . The edge from s to u crosses the cut. This proves that the number of edges across the cut is at least N A  +  L − A , which by our assumption is at least A +  L − A  = L. 4. We have seen that there is a matching with L edges iff the max ﬂow in this graph has value L iff the min cut in this graph has capacity L. The cut with s on one side by itself has L edges going across the cut, namely those edges from s to L. By the last question, if ∀A ⊆ L, A ≤ N A , then every cut has at least L edges across it. Hence, the min cut must be L. Hence, the max ﬂow has value L. Hence, there is a matching with L edges. All the nodes in L must be matched. 5. By the last question, it is sufﬁcient to prove that ∀A ⊆ L, A ≤ N A  is true. Con- sider some set A ⊆ L. Because each every node in A ⊆ L has degree at least k, we know that at least k · A edges leave A. All of the edges that leave A must enter its neighborhood set N A . Hence, the number that leave A is at most the num- ber that enter N A . Because every node in N A  ⊆ R has degree at most k, we know that at most k · N A  enter N A . It follows that k · A ≤  leave A ≤  enter N A  ≤ k · N A . Hence, A ≤ N A , as is needed.  Chapter 16: Greedy Algorithms  16.2.2 In the following instance of the interval cover problem, the greedy criterion that selects the interval that covers the largest number of uncovered points would commit to the top interval. However, the optimal solution does not contain this interval, but contains the bottom two intervals.  16.3.2 Algorithms 1, 2, and 4 are suboptimal for the following counterexample instance:   a   Room1  Room2    b   Diagram  a  gives the events in the instance and the optimal schedule in two rooms. Diagram  b  gives the suboptimal schedule produced by these three algorithms. Note that the third algorithm, which schedules the next event in the room with the latest last- scheduled ﬁnishing time, gives the optimal schedule. We will now prove that it always gives an optimal solution.  As with all greedy algorithms, the loop invariant is that there is at least one opti- mal solution optSL I consistent with the choices made so far, that is, scheduling in the same rooms the same events whose schedule have been committed to so far and not scheduling the events rejected so far. Initially, no choices have been made, and hence trivially all optimal solutions are consistent with these choices. We prove that the loop   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   ExSol CUUS154-Edmonds 978 0 521 84931 9  Exercise Solutions  424  March 29, 2008  16:4  invariant is maintained by modifying the schedule optSL I into another schedule optSours and prove that this new schedule is valid, consistent with all previous and cur- rent choices, and optimal. There are three cases.  If our greedy algorithm did not schedule the next event i, then this event must conﬂict in each room with a previously scheduled event. Hence, optSL I cannot have this next event i scheduled either, because it too has scheduled these previous events. Hence, optSL I itself is already consistent with the most recent choice.  If our greedy algorithm did schedule the next event i in room j and optSL I does not schedule this event at all, then we modify the schedule optSL I into optSours by adding i to room j and removing any events from j that conﬂict with it. Just as done with the one-room scheduling algorithm in Section 16.2.1, we can prove that only one event is removed and hence optSours is valid, consistent, and optimal.   cid:1   The remaining case occurs when our greedy algorithm scheduled the next event i in room j and optSL I schedules it in room j .  See diagram  a .  We modify the schedule optSL I into optSours as follows.  See diagram  b .  We cannot move the events whose schedule has already been committed to by the algorithm, because optSours needs to remain consistent with these choices.  See the events in the Commit circle.  We need to move event i from room j to room j so that it too is consistent with what the algorithm has done. But making this change may create conﬂicts. To ﬁx these, we swap every event scheduled by optSL I in room j with the ﬁnishing time of event i or later with every such job scheduled in room j .  See the events in the rectangle.    cid:1    cid:1   We now prove that the resulting solution optSours is valid, consistent, and optimal.  A Valid Solution: Our modiﬁed solution optSours contains no conﬂicts, because optSL I contained none and we will prove now that no new conﬂicts were introduced. There are no new conﬂicts between the previously committed events  circle , because they did not change. There are no new conﬂicts between the later-committed events  rectangle , because they ﬂipped rooms all together. Event i does not conﬂict with the previously committed events in room j , because the algorithm scheduled it there. The even later events that were in room j don’t either, because they are even later. The later events that were in room j won’t conﬂict with the previously scheduled events in room j , because they did not conﬂict with those in room j and we know by the algorithm’s choice of room j that the last-scheduled ﬁnishing time for j is later than that for room j  .   cid:1    cid:1    cid:1   Consistent with Choices Made: optSL I was consistent with the previous choices. We moved event i from room j to room j to make optSours consistent with this most recent choice. We did not move any events in Commit.   cid:1   Optimal: Schedule optSours has the optimal number of events in it, because it has the same number of events as optSL I .  Loop Invariant Has Been Maintained: In conclusion, we have constructed a valid op- timal schedule optSours that is consistent with the choices made by the algorithm. This proves that the loop invariant has been maintained.  The rest of the proof of correctness of this greedy algorithm is the same as that of all the others.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   ExSol CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  16:4  Exercise Solutions  Chapter 17. Recursive Backtracking  17.5.1 Asking to provide the best word is not a “little question” for the bird. She would be doing most of the work for you. Asking the friend to provide the best place on the board to put the word is not a subinstance of the same problem as that of the given instance.  17.5.2 The simple brute force algorithm searches the dictionary for each permutation of each subset of the letters. The backtracking algorithm tries all of the possibilities for the ﬁrst letter and then recurses. Each of these stack frames tries all of the remaining possibil- ities for the second letter, and so on. This can be pruned by observing that if the word constructed so far, e.g., ‘xq’, does not match the ﬁrst letters of any word in the dictionary, then there is no need for this stack frame to recurse any further.  Another improvement on the running time ensures that the words are searched for in the dictionary in alpha- betical order.   425  17.5.3 1.  cid:2 1, 5, 8, 6, 3, 7, 2, 4 cid:3  2.  cid:2 1, 6, 8, 3, 7, 4, 2, 5 cid:3  3.  cid:2 1, 7, 4, 6, 8, 2, 5, 3 cid:3  4.  cid:2 1, 7, 5, 8, 2, 4, 6, 3 cid:3  5.  cid:2 2, 4, 6, 8, 3, 1, 7, 5 cid:3  6.  cid:2 2, 5, 7, 1, 3, 8, 6, 4 cid:3  7.  cid:2 2, 5, 7, 4, 1, 8, 6, 3 cid:3  8.  cid:2 2, 6, 1, 7, 4, 8, 3, 5 cid:3  9.  cid:2 2, 6, 8, 3, 1, 4, 7, 5 cid:3  10.  cid:2 2, 7, 3, 6, 8, 5, 1, 4 cid:3  11.  cid:2 2, 7, 5, 8, 1, 4, 6, 3 cid:3  12.  cid:2 2, 8, 6, 1, 3, 5, 7, 4 cid:3    cid:2    cid:3  n  17.5.4 We will prove that the running time is bounded between  6 and nn and hence is n cid:1  n  = 2 cid:1  n log n . Without any pruning, there are n choices on each of n rows as to where to place the row’s queen. This gives nn different placements of the queens. Each of these solutions would correspond to a leaf of the tree of stack frames. This is clearly an upper bound on the number when there is pruning.  n 2  I will now give a lower bound on how many stack frames will be executed by this algorithm. Let j be one of the ﬁrst n 6 rows. I claim that each time that a stack frame is placing a queen on this row, it has at least n 2 choices as to where to place it. The stack frame can place the queen on any of the n squares in the row as long as this square cannot be captured by one of the queens placed above it. If row i is above our row j , then the queen placed on row i can capture at most three squares of row j : one by moving on a diagonal to the left, one by moving straight down, and one by mov- ing on a diagonal to the right. Because j is one of the ﬁrst n 6 rows, there are at most this number of rows i above it, and hence at most 3 × n 6 of row j ’s squares can be captured. This leaves, as claimed, n 2 squares on which the stack frame can place the queen.   cid:3    cid:2   From the above claim, it follows that within the tree of stack frames, each stack 2 children. Hence, at the 6 different stack frames. Many of these  frame within the tree’s ﬁrst n n 6  6 levels branches out to at least n  th level of the tree there are at least   cid:3  n   cid:2   n 2   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   ExSol CUUS154-Edmonds 978 0 521 84931 9  Exercise Solutions  March 29, 2008  16:4  will terminate without ﬁnding a complete valid placement. However, this is a lower bound on the running time of the algorithm, because the algorithm recurses to each of them.  17.5.5 The line “kmin = a k that maximizes costk.”  Chapter 18. Dynamic Programming Algorithms  426  18.3.3  algorithm PrintingNeatly   cid:2 M; l1, . . . , ln cid:3    cid:2  pre-cond cid:3 :  cid:2 l1, . . . , ln cid:3  are the lengths of the words, and M is the length of each line.  cid:2  post-cond cid:3 : opt Sol splits the text into lines in an optimal way, and cost is its cost. begin  % Table: optSol[i] would store an optimal way to print the ﬁrst i words of the input, but actually we store only the bird’s advice for the subinstance and the cost of its solution. table[0..n] birdAdvice, cost  % Base case: The only base case is for the best printing of the ﬁrst zero words. Its solution is the empty printing with cost zero. % optSol[0] = ∅ cost[0] = 0 birdAdvice[0] = ∅  % General cases: Loop over subinstances in the table. for i = 1 to n  % Solve instance  cid:2 M; l1, . . . , li cid:3  and ﬁll in table entry  cid:2 i cid:3 . K = maximum number k such that the words of length li−k+1, . . . , li ﬁt on a single line. % Try each possible bird answers. for k = 1 to K  % The bird-and-friend algorithm: The bird tells us to put k words on the last line. We ask the friend for an optimal printing of the ﬁrst i − k words. He gives us optSol[i − k], which he had stored in the table. To this we add the bird’s k words on a new last line. This gives us optSolk, which is a best printing of the ﬁrst i words from amongst those printings consistent with the bird’s answer. % optSolk  costk = cost[i − k] +  M − k + 1 − cid:1   =  cid:2 optSub[i − k], k cid:3   i j=i−k+1 l j  3  end for % Having the best, optSolk, for each bird’s answer k, we keep the best of these best. kmin = a k that minimizes costk  birdAdvice[i] = kmin   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   ExSol CUUS154-Edmonds 978 0 521 84931 9  Exercise Solutions  March 29, 2008  16:4  % optSol[i] = optSolkmin % cost[i] = costkmin  end for optSol = PrintingNeatlyWithAdvice return  cid:2 optSol, cost[n] cid:3   end algorithm   cid:2  cid:2 M; l1, . . . , ln cid:3 , birdAdvice   cid:3   427  Chapter 19. Examples of Dynamic Programs 19.1.1  a : If xn = ym, then we must prove that there is at least one optimal solution that con- tains both of these last characters. Consider an optimal solution. It must end in this last character; otherwise it could be extended to contain it. It might not contain both of them, as in the case of X =  cid:2 A, B, B cid:3 , Y =  cid:2 A, B cid:3 , with optimal solution Z =  cid:2 A , B cid:3 . However, as in this case, we can just as well assume that the optimal solution takes both. If xn  cid:9 = ym, then the optimal solution cannot take both. Hence, it either does not take the last of X or does not take the last of Y. It might not take the last of either, but this is included in both the other two cases. See Section 17.3 for a further answer.   b  The loop over subinstances is then changed as follows.  % Solve instance  cid:2  cid:2 x1, . . . , xi cid:3 ,  cid:2 y1, . . . , y j cid:3  cid:3  and ﬁll in table entry  cid:2 i, j cid:3 . if xi = y j then  birdAdvice[i, j ] = 3 % optSol[i, j ] = optSol[i − 1, j − 1] + xi cost[i, j ] = cost[i − 1, j − 1] + 1  else  % Try possible bird answers. % cases k = 1, 2 % optSol1 cost1 = cost[i − 1, j ] % optSol2 cost2 = cost[i, j − 1]  = optSol[i − 1, j ] = optSol[i, j − 1]  we keep the best of these best.  % end cases % Having the best, optSolk, for each bird’s answer k, kmax = a k ∈ [1, 2] that maximizes costk % optSol[i, j ] = optSolkmax cost[i, j ] = costkmax birdAdvice[i, j ] = kmax  end if  19.1.2 The running time is the number of subinstances times the number of possible bird an- swers, and the space is the number of subinstances. The number of subinstances is  cid:1  n2 , and the bird has K = 3 possible answers for you. Hence, the time and space re- quirements are both  cid:1  n2 .   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   ExSol CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  16:4  Exercise Solutions  19.4.2  A7  A6  A5  428  A4  A3  A2  j A1  i  According to  size  A7  A6  A5  A4  A7  A6  A5  A4  A3  A2  j  A1  i  A3  A2  j  A1 i  for j = 1 up to n          for i = j down to 1                 Slove instance 〈Ai..., Aj  〈  for i = n down to 1          for j = i up to n                % Slove instance 〈Ai..., Aj  〈  19.5.1 1. An AVL tree of height h has left and right subtrees of heights either  cid:2 h − 2, h − 1 cid:3 ,  cid:2 h − 1, h − 1 cid:3 , or  cid:2 h − 1, h − 2 cid:3 . 2. The bird tells me whether the subtree heights are  cid:2 h − 2, h − 1 cid:3 ,  cid:2 h − 1, h − 1 cid:3 , or  cid:2 h − 1, h − 2 cid:3 . She also tells me which value ak will be at the root. I can then ask the friends for the best left and right subtrees of the speciﬁed height.  3. In each of these three cases, the heights are within 1. 4. The complete set of subinstances is as following. Recall that in Chapter 10 we proved that the minimum height of an AVL tree with n nodes is h = log2 n and that its maximum height is h = 1.455 log2 n. Hence, the complete set of subinstances is S = { cid:2 h;ai, . . . , a j ; pi, . . . , pj cid:3   1 ≤ i ≤ j ≤ n, h ∈ [log2 j − i + 1 ..1.455 log2 j − i + 1 ]}. The table is a three-dimensional  cid:1  n × n × log n  box. 5. The table has size  cid:1  n × n × log n . The bird can give 3 · n different answers. Hence,  the running time is  cid:1  n3 log n .  6. In the original problem, the height was not ﬁxed. To solve this problem, we could simply run the previous algorithm for each h and take the best of the resulting AVL trees. However, after running the previous algorithm once, the table already contains the cost of the best AVL for each of the possible heights h. To ﬁnd the best overall AVL tree, we need only compare those listed in the table.  algorithm Parsing   cid:2 G, Tstart, a1, . . . , an cid:3    cid:2  pre-cond cid:3 : G is a Chomsky normal form grammar, Tstart is a nonterminal, and s is the string  cid:2 a1, . . . , an cid:3  of terminal symbols.  cid:2  post-cond cid:3 : P, if possible, is a parsing that generates s starting from Tstart using G. begin  % Table: optSol[h, i, j ] would store an optimal solution for  cid:2 G, Th, ai, . . . , a j cid:3 , namely a parsing for ai, . . . , a j starting with nonterminal Th. Instead, we store only the bird’s advice for the subinstance and the cost of its solution. table[V, n, n] birdAdvice, cost  19.7.1   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   ExSol CUUS154-Edmonds 978 0 521 84931 9  Exercise Solutions  March 29, 2008  16:4  429  % The case s =  cid:4  is handled separately if n = 0 then  if Tstart ⇒  cid:4  is a rule then  P = the parsing applies this one rule. P = ∅  else  end if return P   end if  % Base cases: The base cases are when the string to parse consists of only one character ai. For i = 1 to n  For each nonterminal Th  If there is a rule rq = “Aq ⇒ bq ”, where Aq is Th and bq is ai, then  birdAdvice[h, i, i] =  cid:2 q, ? cid:3  cost[h, i, i] = 1 birdAdvice[h, i, i] =  cid:2 ?, ? cid:3  cost[h, i, i] = 0  else  end if  end loop  end loop  % General cases: Loop over subinstances in the table. for size = 2 to n % length of substring  cid:2 ai, . . . , a j cid:3   for i = 1 to n − size + 1  j = i + size − 1 For each nonterminal Th, i.e., h ∈ [1..V]  % Solve instance  cid:2 G, Th, ai, . . . , a j cid:3 , and ﬁll in table entry  cid:2 h, i, j cid:3 . % Loop over possible bird answers. for each rule rq = “Aq ⇒ Bq Cq ” for which Aq is Th  for each split in the string k = i to j − 1  % Ask friend if you can generate  cid:2 ai, . . . , ak cid:3  from Bq . % Ask another friend if you can generate  cid:2 ak+1, . . . , a j cid:3 . from Cq cost cid:2 q,k cid:3  = min cost[Bq , i, k], cost[Cq , k + 1, j ]   end for  end for % Take the best bird answer, i.e., one of cost one if s can be generated.  cid:2 qmin, kmax cid:3  = a  cid:2 q, k cid:3  that maximizes cost cid:2 q,k cid:3  birdAdvice[h, i, j ] =  cid:2 qmin, kmax cid:3  cost[h, i, j ] = cost cid:2 qmax,kmax cid:3   end for  end for  end for   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   ExSol CUUS154-Edmonds 978 0 521 84931 9  Exercise Solutions  March 29, 2008  16:4  % Constructing the solution P if cost[1, 1, n] = 1  then P = ParsingWithAdvice P = ∅  else  % i.e., if s can be generated from Tstart   cid:2  cid:2 G, Tstart, a1, . . . , an cid:3 , birdAdvice   cid:3   430  end if return P  end algorithm  Time and space requirements: The running time is the number of subinstance times the number of possible bird answers, and the space is the number of subinstances. The number of subinstances indexing your table is  cid:1  Vn2 , namely, Table[h, i, j ] for h ∈ V and 1 ≤ i ≤ j ≤ n. The number of answers that the bird might give you is at most O mn , namely,  cid:2 q, k cid:3  for each of the m rules rq and the split k ∈ [1..n − 1]. This gives time = O Vn2 · mn . If the grammar G is ﬁxed, then the time is  cid:1  n3 . A tighter analysis would note that the bird would only answer q for rules rq = “Aq ⇒ Bq Cq ”, for which the left-hand side Aq is the nonterminal Th speciﬁed in the instance. Let mTh be the number of such rules. Then the loop over nonterminals Th and the loop over rules rq would not require Vm time, but = m. This gives a total time of  cid:1  n3m .  Th∈V mTh   cid:1   19.8.2 Given an instance E = {e1, e2, ..., en} of the elephant problem, we map this to an instance  cid:2 X, Y cid:3  of the LCS problem as follows. Each elephant will be distinct. Let X =  cid:2 x1, . . . , xn cid:3  be the elephants sorted by weight wi, and let Y =  cid:2 y1, . . . , ym cid:3  be the same sorted by smartness si. A solution to LCS is a subsequence Z =  cid:2 z1, . . . , zl cid:3  that is common to both X and Y. Note that Z is a subset of elephants S ⊆ E for which bigger is smarter. This is because Z is sorted both with respect to weight and with respect to intelligence. The only difference between these problems is that the cost  or success  of a LCS solution is simply the length of Z, while for the elephant problem it is the sum of the values of the elephants. Hence, for this to work, the LCS problem needs to be generalized to have weights on the letters. But this would not change the dynamic programming algorithm at all.  Chapter 20. Reductions and NP-Completeness 20.2.4 The ﬁrst is easy. InstanceMap I1  simply maps each instance I1 ∈ S1 ⊆ S2 of P1 to itself, I1, which is a valid instance of P2. The second is much harder, because InstanceMap I2  must map each instance I2 ∈ S2 of P2 to some instance I1 within the restricted set S1.  Ioracle  20.2.5 The running time Time Algoracle  is measured as a function of its own input size, namely  cid:1  2  , in terms of Algalg’s input size. The extra O Ialg3  time for the mappings is not substantial. Hence, Algalg’s total running time is  cid:1  2n2 3  .  3  . But because Ioracle = Ialg2, this same time is  cid:1  2  1  Ialg  2 3   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   ExSol CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  16:4  431  Exercise Solutions  20.3.3  Similarly, if Algoracle runs in polynomial time, namely Time Algoracle  =  cid:1  Ioraclec , then so does Algalg, namely Time Algalg  =  cid:1  Ialg2c + Ialg3 , but note that the poly- nomal is a different one.  Step 5: Given a graph GCOL that we want to color, we construct an instance  cid:2 GInd, NInd cid:3  = InstanceMap GCOL  to give to the independent set oracle as follows. As said, GInd will have a node for each pair  cid:2 u, c cid:3  where u is a node of GCOL and c is one of the three colors. For each node u ∈ GCOL, we put a triangle of edges around  cid:2 u, red cid:3 ,  cid:2 u, blue cid:3 , and  cid:2 u, green cid:3 . For each edge  cid:2 u, v cid:3  ∈ GCOL, we put three parallel edges be- tween  cid:2 u, c cid:3  and  cid:2 v, c cid:3 , for each color c. The size of the required independent set will be the number of nodes, NInd = VCOL, in the graph GCOL. Step 6: Given an independent-set solution SInd to GInd of size VCOL, we construct a coloring SCOL for GCOL by coloring u with color c if node  cid:2 u, c cid:3  is in the independent set. Step 7: We now show that if SInd is a valid independent set of size VCOL, then SCOL is a valid 3-coloring. First we show that it is impossible for a node to be given more than one color, because the edge between  cid:2 u, c cid:3  and  cid:2 u, c  cid:1  cid:3  prevents both of these nodes being in the independent set. Because the independent set is of size VCOL and no node u appears more than once, it follows that every node u appears exactly once. Hence, every node u is given a color. Finally, we show that the nodes in the edge  cid:2 u, v cid:3  ∈ GCOL cannot both have the color c, because the edge between  cid:2 u, c cid:3  and  cid:2 v, c cid:3  prevents both of these nodes from being in the independent set.  Step 8: Given a coloring SCOL for GCOL, we construct an independent set SInd for GInd of size VCOL by putting node  cid:2 u, c cid:3  in the independent set if u is colored c. Step 9: We now show that if SCOL is a valid 3-coloring, then SInd is a valid independent set. We need to show that for each edge in GInd, both nodes are not in SInd. There is an edge between  cid:2 u, c cid:3  and  cid:2 u, c . There is an edge between  cid:2 u, c cid:3  and  cid:2 v, c cid:3 , but u and v cannot both have color c.   cid:1  cid:3 , but u cannot have both colors c and c   cid:1   x + y = 5. x + y  cid:9 = 5.  Chapter 22. Existential and Universal Quantiﬁers 22.0.2 1. ∀x ∃y x + y = 5 is true. Let x have an arbitrary real value, and let y = 5 − x. Then 2. ∃y ∀x x + y = 5 is false. Let y have an arbitrary real value, and let x = 6 − y. Then 3. ∀x ∃y x · y = 5 is false. Let x = 0. Then y must be 5 0 , which is impossible. 4. ∃y ∀x x · y = 5 is false. Let y have an arbitrary real value, and let x = 6 y if y  cid:9 = 0 and 5. ∀x ∃y x · y = 0 is true. Let x have an arbitrary real value, and let y = 0. Then 6. ∃y ∀x x · y = 0 is true. Let y = 0, and let x have an arbitrary real value. Then 7. [∀x ∃y P x, y ] ⇒ [∃y ∀x P x, y ] is false. Let P x, y  = [x + y = 5]. Then, as already  x = 0 if y = 0. Then x · y  cid:9 = 5. x · y = 0. x · y = 0.  seen, the ﬁrst is true and the second is false.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   ExSol CUUS154-Edmonds 978 0 521 84931 9  Exercise Solutions  March 29, 2008  16:4  8. [∀x ∃y P x, y ] ⇐ [∃y ∀x P x, y ] is true. Assume the right side is true. Let y0 be the y for which [∀x P x, y ] is true. We prove the left side as follows. Let x have an arbitrary real value, and let y = y0. Then P x, y0  is true.  9. Sorry, not provided.  432  Chapter 23. Time Complexity 23.1.1 The number of operations is T = 24 × 60 × 60 × 106 = 8.64 × 1010. We have n1 = T 1 4 =  542, n2 = log2 T = log T  log 2  = 36.  23.1.4 1. We ﬁrst prove that g + h =  cid:1  max g, h   as follows. max g, h  ≤ g + h, assuming that  both g and h are positive and g + h ≤ 2 max g, h . 2. One can set k to absolutely minimize f  n, k  by setting f ’s derivative wrt k equal to zero and solving for k. Sometimes this is hard. Because f  n, k  =  cid:1  max  n k log k    log k , and we do not care about the multiplicative constant, let us instead set k in order to minimize max  n k log k  . Observe that if k is very small, then the ﬁrst term, be- ing very big, dominates. Hence, we can make the whole expression smaller by in- creasing k. Similarly, if k is big, the second term dominates the expression, and we can decrease it by decreasing k. So for the optimal solution the two terms should be roughly the same. In this case n log n . This log k is  asymptotically  the best result, because decreasing k increases the ﬁrst term and increasing k increases the second.  log k gives n = k and f  n, k  = n+k  = n  = k  log k ,  log k  3. Sorry, not included.  merge sort, that works for every input instance I .  rithm fails to work for at least one input instance I .  23.2.1 1. ∃A , ∀I, Works Sorting, A , I  . We know that there at least one algorithm, e.g., A = 2. ∀A , ∃I, ¬Works Halting, A , I   We know that contrary statement is true. Every algo- 3. ∃P, ∀A , ∃I, ¬Works P, A , I   4. It says that every input has some algorithm that happens to output the right answer. It is true. Consider an arbitrary instance I . If on instance I , Halting happens to say yes, then let A be the algorithm that simply halts and says yes. Otherwise, let A be the algorithm that simply halts and says no. Either way, A works for this instance I .  5. It says that every algorithm correctly solves some problem. This is not true, because some algorithms do not halt on some input instances. We prove the complement ∃A , ∀P, ∃I, ¬Works P, A , I   as follows. Let A be an algorithm that runs forever on some instance I on which A does not halt. Note that Works P, A , I   is not true.  . Let P be an arbitrary problem. Let I be an instance I   cid:1    cid:1   Chapter 24. Logarithms and Exponentials 24.0.1 a 3 × a 5 = a 8, 3a × 5a = 15a , 3a + 5a =?, 25log4n+7 = [40.5]6log4n × 27 = [4log4n]3 · 128 =  128n3, n3 log2n = [2log2 n]3 log2n = 23 = 8.  Chapter 25. Asymptotic Growth 25.1.4 Exercise 25.0.2 proves that 3 log n << n for sufﬁciently big n. Hence, n2 ≤ 3n2 log n ≤  n · n2 = n3.  25.1.5 34n >> 43n >> 20.001n  n100 >> 100n100 >> log1,000 n.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   ExSol CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  16:4  433  Exercise Solutions  25.2.2  2 n7 + n6  f  n  = 14n9 + 5,000n7 + 23n2 ≤ 14n9 + n2  1. 14n9 + 5, 000n7 + 23n2 log n ∈ O n9 : Let c = 15 and n0 = 100. For all n ≥ 100, 2 n2 log n ≤ 14n9 + n9 = We have c · g n . 2. 2n2 − 100n ∈  cid:1  n2 : Let c1 = 1, c2 = 2, and n0 = 100. For all n ≥ 100, c1g n  = 1n2 = 2n2 − n · n ≤ 2n2 − 100n = f  n  ≤ 2n2 = c2g n . 3. 14n8 − 100n6  cid:9 ∈ O n7 : Let c and n0 be arbitrary values given to us by some adversary. We then let n = max 10, c, n0 . Then we demonstrate that f  n  is too big. Because n ≥ 10, we have 100n6 ≤ n8. This gives f  n  = 14n8 − 100n6 ≥ 14n8 − 1n8 > n · n7 ≥ c · n7. 4. 14n8 + 100n6  cid:9 ∈  cid:1  n9 : Let c1, c2, and n0 be arbitrary values given to us by some adversary. Let us make n = max 15 c1, 11, n0 . Then we demonstrate that f  n  is · c1n8 = 14n8 + n8 = 14n8 + n2 · n6 ≥ 14n8 + too small: c1g n  = c1n9 = n · c1n8 ≥ 15  11 2 · n6 > 14n8 + 100n6 = f  n .  5. 2n+1 ∈ O 2n : Let c = 2 and n0 = 0. For all n ≥ 0, f  n  = 2n+1 ≤ 2 × 2n = c × g n . 6. 22n  cid:9 ∈ O 2n : Let c and n0 be arbitrary values. Let n = max 1 + log2 c, n0 . Then we  c1  25.2.8  have that f  n  = 22n = 2n · 2n > c · g n .  cid:5   1. x = 7y 3 log2 y 18 ∈ y 3+o 1  = cid:4  gives  cid:1  y 3 log2 x 18 . Solving this gives y =  cid:1   in y = x1 3+o 1   2. Substituting  y 3, y 3+ cid:4   . Solving this gives y = x1  3+o 1  .  cid:6   x = 7y 3 log2 y 18 = 7y 3 log2 x1  3+o 1   18 = x 1 3   log x 6   cid:7   .  Chapter 26. Adding-Made-Easy Approximations   cid:1 ∞  26.2.1 His ﬁrst such step takes him half an hour, his second a quarter, his third an eighth, . . . for + ··· = 1 hour. Given that he travels one kilome-  + 1  + 1  + 1  1  2i = 1  2  a total of only ter at one kilometer an hour, this is reasonable.  i=1  16  8  4  26.2.4  1. The function f  n  = 22   cid:21 log2 n cid:22   n  2. We show    = 22k+1 = 22n.   cid:1  = n + 1 we have  cid:21 log n  is 2 cid:5  n , because it is bounded between 2n and 22n. Let  cid:1  cid:22  =  n = 2k. Then  cid:21 log2 n cid:22  = k and f  n  = 22k = 2n, but for n  cid:1  k + 1 and f  n  cid:1  i=1 f  i   cid:9 =  cid:1  f  n   as follows. With n = 2k, both functions are more or less  cid:1  2   to f  n . Because they behave like arithmetic functions within this i=1 f  i  =  cid:1  n · f  n   and not √ n and n. Let n = 22k .  cid:1  cid:11  = k −  cid:1  i=1 f  i   cid:9 ∈  cid:1  n · f  n   as follows. Again let n = 22k , so that f  n  = n, yet n + n. every previous term is most Because the last term f  n  is so much bigger than the previous ones, the total is not  cid:1  n · f  n  , which is  cid:1  n2 .  constant from f   n range, the adding-made-easy techniques give that  cid:1  f  n  . Then  cid:10 log log n cid:11  = k and f  n  = 22k = n, but for n 1 and f  n  cid:1  i=1 f  i  then is at most  n−1  · √   cid:10 log log n cid:11  ∈ n cid:1  1 −1 by bounding it between  3. We show that f  n  = 22   = 22k−1 = √   cid:1  = n − 1 we have  cid:10 log log n  √ n. The total of  4. We show that  n.  n  n  n   cid:1    P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   ExSol CUUS154-Edmonds 978 0 521 84931 9  Exercise Solutions  March 29, 2008  16:4  5. The function f  n  = 2[ 1  2 cos π log2 n +1.5]·n is a geometric counterexample squeezed be-  tween 2n and 22n, just as f  n  = 22   cid:21 log2 n cid:22   is:  434  6  5  4  3  2  1  0  0  2  4  6  8  10  Chapter 27. Recurrence Relations  27.1.1 Examples:  T n   2T  n  2T  n  4T  n  2   + n 2   + 1 2   +  cid:1  n3  log3 n  4   +  cid:1  log n  3   +  cid:1  n3 log4 n  4   +  cid:1   n  log n 1.5  2   +  cid:1  n2  log n   32T  n  27T  n  8T  n  4T  n  loga log b  log2 2 log2 2 log2 2 log2 2 log2 4 log2 2 log2 32 log2 4 log3 27 log3 3 log2 8 log 4  log2 4 log2 2  = 1 = 1 = 2 = 5 = 3 = 3 = 2  2  2  1  c  1  0  3  0  3  3 2  2  d vs −1  Dom.  Rule  Solution  0 >  All   cid:1  f  n  log n    cid:1  n log n   Base  Top  Base  All   cid:1  nloga   log b    cid:1  n    cid:1  f  n     cid:1  n3  log3 n    cid:1  nloga   log b    cid:1  n2.5    cid:1  f  n  log n     cid:1  n3 log5 n   Base   cid:1  nloga   log b    cid:1  n1.5   Exercise 27.2.4 gives   cid:1  nc · log log n  =  cid:1  n2 log log n   4 > −1.5 < −1 =  27.2.1 Plugging Fib n  = αn into Fib n  = Fib n − 1  + Fib n − 2  gives that αn = αn−1 + αn−2. Dividing through by αn−2 gives α2 = α + 1. Solving this gives that either α = 1+√ 2 or α = 1−√ . Any linear combination of these two solutions will also be a valid solution, namely Fib n  = c1 ·  α1 n + c2 ·  α2 n. Using the fact that Fib 0  = 0 and Fib 1  = 1 and solving for c1 and c2 gives that  5  5  2   cid:8  cid:8    cid:8    cid:9 n −   cid:9 n cid:9   .  1 − √  5  2  1 + √  5  2  Fib n  = 1√ 5   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   ExSol CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  16:4  Exercise Solutions  27.2.2 Unwinding:  27.2.4 Suppose f  n  = nc · logd n and c = loga  1. T n  = T n−1  + n: Because a = 1 and c > 1,  the table give T n  =  cid:1  n · T n  = n + T n−1  = n +  n−1  + T n−2  =  the table gives T n  =  cid:1  a n  2. T n  = 2 · T n − 1  + 1: Because a = 2,  f  n   =  cid:1  n2 . Unwinding gives n +  n−1  +  n−2  + T n−3  = n +  n−1  +  n−2  + ··· +  n−i + 1  + T n−i  = n +  n−1  +  n−2  + ··· + 1 =  cid:1  n2 . b   =  cid:1  2n . Unwinding gives T n  = 1 + 2T n−1  = 1 + 2 + 4T n−2  = 1 + 2 + 4 + ··· + 2i−1 + 2i T n−i  = 1 + 2 + 4 + ··· + 2n−1 + 2n =  cid:1  2n .  h i=0 ai h i=0 1 i[log n bi ]d  . The expression n bi takes on the val-   cid:1  log b ; then T n  =  cid:1    cid:5 d   =  cid:5 d is a constant that we can hide in the Theta. This   n bi c logd  n bi   =  cid:1  nc · cid:1  ues n, n b, n b2, . . . , 1. Reversing this order gives T n  =  cid:1  nc · cid:1   cid:4   cid:5 d  . Here  cid:1  nc · cid:1  gives T n  =  cid:1  nc · cid:1  h j=0 j d  . The adding-made-easy approximations state that this sum is arithmetic as long as d > −1. In this case, the total is T n  =  cid:1  nc · h · hd   =  cid:1   cid:1  nc · logd+1 n  =  cid:1  f  n  log n . If d = −1, then we get the harmonic sum T n  =  cid:1  nc · j   =  cid:1  nc · log h  =  cid:1  nc log log n . If d < −1, then the sum has a bounded tail, giving T n  =  cid:1  nc ·  cid:1  1   =  cid:1  nloga   log b .   cid:1  i=0 ai f  n bi   =  cid:1    log b j    j log b  log b  h j=0  h j=0  h j=0   cid:4    cid:4   h  1  435   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   ExSol CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  16:4  436   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   Conclusion CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  11:16  CONCLUSION  437  The overall goal of this entire text has been to teach skills in abstract thinking. I hope that it has been fruitful for you. Good luck at applying these skills to new problems that arise in other courses and in the workplace.  We say goodbye to our friends.   P1: ...  Gutter margin: 7 8  Top margin: 3 8   cid:1  cid:1    cid:1  cid:1   Conclusion CUUS154-Edmonds 978 0 521 84931 9  March 29, 2008  11:16  438   P1: JZP  Gutter margin: 7 8′′  Top margin: 3 8′′  CUUS154-IND CUUS154-Edmonds 978 0 521 84931 9  April 24, 2008  17:11  INDEX  439  abstract data type  ADT , 1, 43  exercise solutions, 414 functions vs., 43 merging with queue, 56 speciﬁcations implementations, 44  dictionary, 47 graphs, 47 link list implementation, 51 list, 44 orders, 48 priority queue, 46 queue, 45 set, 46 set system, 47 simple types, 44 stack, 44; parsing with stack, 57 trees, 48 AVL tree, 49 binary search tree, 48 recursive deﬁnition of tree, 130 union-ﬁnd set system, 49  Ackermann’s function, 127  algorithm, 127 crashing, 128 recurrence relation, 127 running time, 128 solving, 127  388  examples, 391 exercise solutions, 433 proofs, 393  analytical functions, simple, 396 arithmetic sums, 394 close to harmonic, 395 functions, without basic form,  396  geometric sums, simple, 393  harmonic sum, 395 ratio between terms, 393  solution classes, 389 technique, 389  ADT. See abstract data type algorithms. See also recursive algorithms  Ackermann’s function, 127 best AVL tree bird-and-friend algorithm, 312 bipartite matching using network ﬂow, 342 brute force algorithm, 66, 226 deﬁned, 1 Dijkstra’s shortest-weighted path algorithm,  dynamic programming algorithms and  183  examples, 267  all pairs, matrix multiplication, 314 best AVL tree, 311 chains of matrix multiplications, 306 context-free grammar parsing, 315 dynamic programming algorithms. via  reductions, 318  longest-common-sequence problem,  longest increasing contiguous  subsequence example, 301  longest increasing subsequence example,  graph example, 267  weighted job event scheduling problem,  Euclid’s greatest common divisor  GCD   algorithm, 79 graph algorithms  expander graphs, 351 max cut problem, 350 minimum spanning tree, 244 network ﬂows, 198  295  301  303  adding-made-easy approximations,  shortest weight path, directed leveled   P1: JZP  Gutter margin: 7 8′′  Top margin: 3 8′′  CUUS154-IND CUUS154-Edmonds 978 0 521 84931 9  April 24, 2008  17:11  440  Index  algorithms  cont.   shortest weight path, directed leveled  graph example, 267 3-Colouring, 330, 338  graph search algorithms, 173  breadth-ﬁrst search, shortest path, 179, 181 depth-ﬁrst search, 188 generic search algorithm, 174 partial order linear ordering, 194; depth-  ﬁrst search algorithm, 195; easy but slow algorithm, 195  recursive depth-ﬁrst search, 192  greedy algorithms, 225, 260, 295, 307 hill-climbing algorithm, 221 iterative algorithms, 8, 12, 21 iterative sorting algorithms, 71, 72, 75, 76 looking forward vs backward, recursive  algorithms, 99  meta-algorithms, xiii, 2 parsing algorithm speciﬁcations, 161 randomized algorithms, 346 sorting and selection algorithms, 114 steepest-ascent hill-climbing algorithm, 214 Strassen’s matrix multiplication, 126 time space complexity, 82, 85, 324, 347, 366,  378, 406, 432  analytical functions, simple, 396 arithmetic sums, 394. See also  asymptotic growth, 377  asymptotic growth rate, 378 asymptotic notation, 85, 377, 384  BigOh deﬁnition, 85, 385 Little Oh deﬁnition, 85, 386 Little Omega deﬁnition, 85, 386 loop invariant for lower bounds, 85 Theta deﬁnition, 85, 385  exercise solutions, 432 function classiﬁcation, 379 growth rates classes, 377 purpose, 378  AVL tree, 49  best AVL tree, 311  best AVL tree problem, 313  binary search 24  for cost in optimization problems, 329 narrowing the search space example, 24 returning index example, 89 returning yes no, 91 trees, 48, 60  ADT, 48 balanced, 49 best binary search tree problem, 311 basic steps, 60  coding implementation details, 61 ending, 61 establishing loop invariant, 61 exit condition, 61 is tree a binary tree example, 138 loop invariant, 60 main steps, 61 maintain loop invariant, 61 make progress, 61 measure of progress, 60 nodes in binary search tree example, 131 running time, 61 speciﬁcations, 60  typical errors, 26  BigOh deﬁnition, 385 bipartite edges, 191 bipartite matching using network ﬂow, 342 breadth-ﬁrst search, 178 shortest path, 179, 181  code, 181 exiting loop, 183 initial code, 183 loop body, 180 loop invariant, 180 maintaining loop invariant, 181 optimization problem, 179 shortest path problem, 179 shortest path proof, 180  basic steps, 71 exit condition, 72 loop invariant, 72 maintain loop invariant, 72 speciﬁcations, 71  chains of matrix multiplications, 306  failed dynamic programming algorithm,  failed greedy algorithm, 307 little bird question, 307 optimal solution construction, 310 optimal solution cost, cost for subinstances,  recursive structure, 308 reduced to subinstance, 308 set of subinstances called, 308 table ﬁll order, 308 table indexed by subinstances construction,  307  308  308  time space requirements, 311  coloring the plane, 29  basic steps, 29 coding implementation details, 30 ending, 30 establishing loop invariant, 30  adding-made-easy approximations  bucket sort by hand, 71   P1: JZP  Gutter margin: 7 8′′  Top margin: 3 8′′  CUUS154-IND CUUS154-Edmonds 978 0 521 84931 9  April 24, 2008  17:11  441  Index  exit condition, 30 loop invariant, 29 main steps, 30 maintain loop invariant, 30 measure of progress, 29 running time, 30 special cases, 30 speciﬁcations, 29.  computational complexity, 1  asymptotic notations, 85, 377, 384 formal proof of correctness, 408 nondeterministic polynomial-time decision  time space complexity, 82, 85, 324, 347, 366,  problems  NP , 330  378, 406, 432  context-free grammar parsing, 159  abstract data type parsing, 161 correctness proof, 165 dynamic programming example, 315  abstract data type parsing, 316 base cases, 317 help from friend, 316 little bird question, 316 not look ahead one, 315 number of parsings, 316 optimal solution construction, 317 parsing problem, 315 set of subinstances, 317 table ﬁll order, 317 table indexed by subinstances  construction, 317 exercise solutions, 421 expression parsing, 160 GetExp code, 163 GetExp expressions, 161 GetExp, Get Term, GetFact examples, 162 GetFact code, 164 GetTerm code, 163 grammar, 159 look ahead one, 159, 165 nonterminals, 160 parsing algorithm speciﬁcations, 161 rules, 160 running time, 165 semantics and, 159 string derivation, 160 syntax and, 159 terminals, 160 tree of stack frames, 165  correctness, 2  formal proof of correctness, 408  for context- free grammar parsing, 165 for depth-ﬁrst search, 196 for dynamic programming, 283 for greedy algorithms, 230  for recursive algorithms, by strong  induction, 113  counting sort, 72 basic steps, 73 code, 74 establishing loop invariant, 74 exit condition, 74 loop invariant, 74 main step, 74 maintain loop invariant, 74 running time, 75 speciﬁcations, 72  cyclic edges, 191  Davis-Putnam, 263 depth-ﬁrst search, 178, 188  code, 189 edges classiﬁcation, 190 establish maintain loop invariant, 190 generic search algorithm changes, 189 loop invariants, 189 recursive, 192 time stamping, 191  deterministic ﬁnite automation  DFA , 31  addition example, 35 applications, 31 calculator example, 36 compiling iterative program into DFA, 33 division example, 36 dynamic programming, 38 longest block of ones example, 37 longest increasing contiguous subsequence,  38  longest increasing subsequence, 38  Dijkstra’s shortest-weighted-path algorithm, 183  code, 186 exiting loop, 188 initial code, 187 loop body, 186 loop invariant, 185 maintaining LI1, 186 maintaining LI2, 187 problem speciﬁcations, 183 shortest distance approximation, 184 shortest path proof, 184  dynamic programming, 267  exercise solutions, 426, 427 steps in developing, 267  base cases, 274 code, 275 count subinstances, 273 ﬁnal solution, 275 redundancy, 272 running time, 277 set of subinstances, 272   P1: JZP  Gutter margin: 7 8′′  Top margin: 3 8′′  CUUS154-IND CUUS154-Edmonds 978 0 521 84931 9  April 24, 2008  17:11  442  dynamic programming  cont.   Euclid’s GCD algorithm. See greatest common  Index  solution from subsolutions, 274 table ﬁll order, 275 table indexed by subinstances  construction, 273  recursive backtracking, 267  running time, 271  subtle points, 277  dynamic programming algorithms and  examples  all pairs, matrix multiplication, 314 best AVL tree, 311  best AVL tree problem, 313 best binary search tree, 311  chains of matrix multiplications, 306 context-free grammar parsing, 315 dynamic programming algorithms via  reductions, 318  best path similarity, 319 bigger-is-smarter elephant problem,  322  event scheduling problem, 319 graph instance formation, 319 mapping back algorithm, 321  longest-common-sequence problem, 295  base cases, 297 code, 297 greedy algorithm, 295 information about subinstance, 299 little bird possible answers, 295 little bird question, 296 longest common sequence, 295 optimal solution construction, 299 set of subinstances, 296 table ﬁll order, 297 table indexed by subindexes construction,  297  time space requirements, 299  example, 301  longest increasing subsequence example, 301  time space requirements, 300  shortest weight path, directed leveled graph  example, 267  weighted job event scheduling problem, 303  failed algorithms, 303 greedy dynamic programming, 304  edges classiﬁcation, 190  back edges, 190 bipartite edges, 191 cross edges, 191 cyclic edges, 191 forward edges, 191 tree edges, 190  divisor algorithm  Euler cycle, 40 event scheduling 236, 319  weighted job event scheduling problem,  303  existential universal quantiﬁers, 357, 358, 372  bound variables deﬁnition, 359 combining quantiﬁers, 359 exercise solutions, 431 expressions building, 360 free variables deﬁnition, 359 Loves example, 358 negation, 361 quantiﬁers order, 359 relation deﬁnition, 358 representations, 358 variable domain, 360  exponentials, 374  base, 376 exercise solutions, 432 ratio, 376 rules, 375 uses, 374  fast Fourier transformation, 125 ﬁnd-max two-ﬁnger algorithm example, 10 forward cross edges, 191 friends level of abstraction, recursive  algorithms, 100  base cases, 101 general input, 100 generalizing problem, 101 link to techniques, iterative algorithms, 102 minimizing number of cases, 101 running time, 102 size, 100 speciﬁcations, 100  abstract data types vs., 43 analytical functions, simple, 396 linear function, 382 quadratic function, 382 time space complexity as, 367  GCD algorithm. See greatest common divisor   GCD  algorithm  geometric sums, simple, 393 GetExp code, 163 GetExp expressions, 161 GetExp, Get Term, GetFact examples, 162 GetExp reasoning, 163 GetFact code, 164 GetFact reasoning, 164 GetTerm code, 163  longest increasing contiguous subsequence  functions. See also Ackermann’s function   P1: JZP  Gutter margin: 7 8′′  Top margin: 3 8′′  CUUS154-IND CUUS154-Edmonds 978 0 521 84931 9  April 24, 2008  17:11  Index  global vs. local considerations, 200, 204, 222,  226, 228, 252, 278  graph algorithms  expander graphs, 351 max cut problem, 350 minimum spanning tree, 244 network ﬂows, 198 shortest weight path, directed leveled graph  example, 267  3-Colouring, 330, 338  graph search algorithms, 173  breadth-ﬁrst search, shortest path, 179, 181 depth-ﬁrst search, 188 Dijkstra’s shortest-weighted-path algorithm,  183  exercise solutions, 422 generic search algorithm, 174  basic steps, 174 code, 175 exit condition, 176 exiting loop, 177 handling nodes order, 178 initial code, 176 loop body, 175 loop invariant, 174 maintaining loop invariant, 176 measure of progress, 176 reachability problem, 174 running time, 177  partial order linear ordering, 194 recursive depth-ﬁrst search, 192  graph theory problems, 173 greatest common divisor  GCD  algorithm, 79  code, 81 ending, 79, 81 establishing loop invariant, 79 example, 82 exercise solutions, 415 exit condition, 81 iteration on general instance, 79 loop invariant, 79 lower bound, 82 making progress, 80, 80 recursive, 126 running time, 82 special cases, 80 speciﬁcations, 79 termination, 81  greedy algorithms, 225  brute force algorithm, 226 correctness proof, 230  using loop invariants, 227  examples  game show, 226 interval cover problem, 240  job event scheduling problem, 236 minimum spanning tree problem, 244  exercise solutions, 423 ﬁxity vs. adaptive priority, 234 greedy choice, 226 loop invariants, types, 227  speciﬁcations, 225  harmonic sum, 395  close to harmonic, 395  heap sort priority queues, 141  array implementation, balanced binary tree,  141  common mistakes, 147 completely balanced binary tree, 141 heap deﬁnition, 141 heapify problem, 142  443  code, 143 iterative algorithm, 143 recursive algorithm, 142 running time, 143, 144 speciﬁcations, 142  heapsort problem, 145  algorithm, 145 array implementation, 146 code, 147 speciﬁcation, 145  makeheap problem, 144  iterative algorithm, 144 recursive algorithms, 144 running time, 145 speciﬁcations, 144 priority queues, 147  hill-climbing algorithm, 221. See also  primal-dual hill climbing method; steepest-ascent hill-climbing algorithm  small local maximum, 200  algorithm, faulty, 203 algorithm ﬁxing, 205 augmentation graph, faulty, 202 basic ideas, 201 counterexample, 204 local maximum, 204  image drawing see recursive image drawing information hiding, 43 information theoretic lower bounds, 87, 92 iterative algorithms, 8, 12  basic steps, 13 code structure, 8 coding implementation details, 19 correctness proof, 8 ending, 19 establishing loop invariant, 17 exit condition, 18   P1: JZP  Gutter margin: 7 8′′  Top margin: 3 8′′  CUUS154-IND CUUS154-Edmonds 978 0 521 84931 9  April 24, 2008  17:11  444  Index  iterative algorithms  cont.   logarithms, 374  search space narrowing, 22  binary search narrowing example, 24  297  ﬁnd-max two ﬁngeralgorithm example, 10 formal proof, 20 loop invariants for, 8, 13, 15 main steps, 16 maintain loop invariant, 16 make progress, 16 measure of progress, 13 running time, 8, 19 special cases, 19 speciﬁcation, 12 types of iterative algorithms, 21  case analysis, 22 more of input, 21; insertion sort example,  more of output, 21; selection sort example,  23  22  work done, 22  bubble sort example, 25  iterative sorting algorithms  bucket sort by hand, 71 counting sort, 72 radix counting sort, 76 radix sort, 75  job event scheduling see event scheduling  kth smallest element example, 117  Las Vegas model, 347 linear function, 382 linear programming, 219 see also network ﬂow  Euclidean space interpretation, 220 example, 219 formal speciﬁcation, 219 hill-climbing algorithm, 221 matrix representation, 220 network ﬂows, 220 running time, 223 small local maximum, 222  link list implementation, 51  adding node to end, 53 adding node to front, 51 deleting node, 55 hidden invariants, 51 initialize walk, 54 notation, 51 removing node from end, 53 removing node from front, 52 testing whether empty, 53 walking down linked list, 53  Little Oh deﬁnition, 85, 386 Little Omega deﬁnition, 85, 386  base, 376 exercise solutions, 432 ratio, 376 rules, 375 uses, 374  longest block of ones example, 37 longest-common-sequence problem, 169, 295  base cases, 297 code, 297 greedy algorithm, 295 little bird possible answers, 295 little bird question, 296 longest common sequence, 295 optimal solution construction, 299 set of subinstances, 296 table ﬁll order, 297 table indexed by subindexes construction,  time space requirements, 299  longest increasing contiguous subsequence  example, 301  longest increasing subsequence example, 301 looking forward vs. backward, recursive  algorithms, 99  algorithm, 99  loop invariant for lower bounds, 85  asymptotic notation, 85 binary search returning index example, 89 binary search returning yes no, 91 dynamic algorithms, 90 exercise solutions, 415 ﬂipping a bit, 90 loop invariant argument, 86 lower bounds proof, state of art, 92 multiplexer example, 90 parity example, 89, 90 sorting example, 87 time complexity, 85 upper bound, algorithm, 85  loop invariants for iterative algorithms, 8, 13,  15  lower bounds, 82, 85, 92, 264, 325, 415, 425 See  also loop invariant for lower bounds  for GCD, 82 loop invariant for lower bounds, 85 lower bounds proof, state of art, 92 reductions, 324  magic sevens, 62 basic steps, 63 establishing loop invariant, 64 exit condition, 64 loop invariant, 63 maintain loop invariant, 63   P1: JZP  Gutter margin: 7 8′′  Top margin: 3 8′′  CUUS154-IND CUUS154-Edmonds 978 0 521 84931 9  April 24, 2008  17:11  445  Index  running time, 64 speciﬁcations, 62  matrix multiplication  all pairs, 314 chains of, solution as tree, 306 Strassen’s matrix multiplication example, 126  measures of progress, loop invariants merge sort example, 114 merging with queue, 56 meta-algorithms, xiii, 2 min cut speciﬁcation, 199 Monte Carlo model, 348 more of input more-of-the-input iterative loop invariant  algorithms, 21, 29, 67, 102, 227, 300, 412  coloring the plane example, 29 deterministic ﬁnite automation, 31 exercise solutions, 412 in dynamic programming  longest increasing contiguous  subsequence example, 301  longest increasing subsequence example,  in greedy algorithms, 228 more-of-the-input vs. more-of-the-output, 39  tournament example, 39  recursive algorithms, link to, 102 in VLSI chip testing example, 67  more-of the-output loop invariant algorithms,  301  21  selection sort example, 22 Euler cycle example, 40  multiplexer example, 90, 90  narrowing the search space, 22, 24, 60, 86, 102,  228, 414  binary search example, 24 binary search trees, 60 exercise solutions, 414 magic sevens, 62 VLSI chip testing, 65  network ﬂows linear programming, 198  bipartite matching using network ﬂow, 342 exercise solutions, 422 hill-climbing algorithm, small local  maximum, 200  linear programming, 219 min cut speciﬁcation, 199 primal-dual hill climbing method, 206 speciﬁcation, 198 steepest-ascent hill-climbing algorithm, 214  nondeterministic polynomial-time decision  problems  NP  completeness, 324  bipartite matching, network ﬂow algorithm,  342, 343  classifying problems, 326 exercise solutions, 430 lower bounds, 325 NP completeness proof steps, 330, 331  nondeterministic polynomial-time  decision problems  NP , 330  reduction P1poly P2, 324 reverse reductions, 325 satisﬁability vs. optimization  Alg for optimization problem, 329 CIR-SAT, 326 optimization problems, 328  3 coloring example, 324 upper bounds, 324 why reduce, 324  operations on integers, 122  b N example, 122 Strassen’s matrix multiplication example, 126 xyz example, 123  optimization problems, 171  examples, 172 airplane, 172 course scheduling, 172 longest common sequence, 172  network ﬂow, 198 problem speciﬁcation, 171  parity example, 89, 90 parsing with stack, 57. See also context-free  parsing  code, 58 ending, 58 example, 57 initial conditions, 58 loop invariant, 58 maintaining loop invariant, 58 parsing only, 59 parsing with context-free grammar, 59 speciﬁcations, 57  partial order linear ordering, 194  depth-ﬁrst search algorithm, 195 easy but slow algorithm, 195 partial order deﬁnition, 194 shortest weight path, DAG, 196 topological sort problem speciﬁcations,  195  total order deﬁnition, 194  postconditions, 1 preconditions, 1 primal-dual hill climbing method, 206  algorithm, 206 ending, 209 max-ﬂow-min-cut duality principle, 213 running time, 213   P1: JZP  Gutter margin: 7 8′′  Top margin: 3 8′′  CUUS154-IND CUUS154-Edmonds 978 0 521 84931 9  April 24, 2008  17:11  Index  primality testing, randomized, 349 printing neatly example, 277  quantiﬁers. See existential universal quantiﬁers quadratic function, 382 quick sort  446  example, 116 randomized, 348  recursive deﬁnition of tree, 130 representing expressions with trees, 149 simple examples, 135  recursive algorithms see also recursive  backtracking checklist for, 104  code structure, 104 speciﬁcations, 105 tasks to complete, 107 variables, 105  correctness proof, with strong induction, 113 examples  Ackermann’s function, 127 exercise solutions, 417 operations on integers, 122 b N example, 122 Strassen’s matrix multiplication example,  126  xyz example, 123  exercise solutions, 416  base cases, 101 general input, 100 generalizing the problem, 101 link to techniques, iterative algorithms, 102 minimizing number of cases, 101 running time, 102 size, 100 speciﬁcations, 100  looking forward vs. backward, 99 solving, 127 sorting selecting algorithms, 114  choosing the pivot, 118 ﬁnding the kth smallest example, 117 general recursive sorting algorithm, 114 merge sort example, 114 partitioning according to pivot element,  quick sort example, 116  stack frame, 110 strong induction, 112, 112 tower of Hanoi, 102  recursive backtracking algorithms, 251, 425  as sequence of decisions, 251  best animal searching example, 253 maze searching example, 252  developing steps, 256 exercise solutions, 425 pruning branches, 260  greedy algorithms, 260  queens problem example, 256 satisﬁability, 261  code, 264 Davis-Putnam, 263 instances subinstances, 262 pruning, 263  radix counting sort, 76  algorithm, 77 example, 77 running time, 78 speciﬁcations, 77  radix sort, 75  basic steps, 75 ending, 76 establishing loop invariant, 76 loop invariant, 76 maintain loop invariant, 76 speciﬁcation, 75  randomized algorithms, 346  hiding worst cases from adversary, 346, 347 deterministic worst case model, 347 game show problem, 348 Las Vegas model, 34 Monte Carlo model, 348 quick sort, 348 randomized counting, 349 randomized primality testing, 349 optimization problems with random  structure, 350  expander graphs, 351 max cut problem, 350  VLSI chip testing, 69  initial conditions, 157 postcondition, 156 precondition, 156 running time, 158 searching maze, 158 subinstances, 157  recurrence relations, 398 exercise solutions, 433 proofs, 401 in recursive backtracking algorithms, 259 recursive programs timing, 398 solving recursive relations, 399  recursion 97  friends level of abstration, 100 on trees  exercise solutions, 419 generalizing problem solved, 138 heap sort priority queues, 141  randomly generating maze, 156  120   P1: JZP  Gutter margin: 7 8′′  Top margin: 3 8′′  CUUS154-IND CUUS154-Edmonds 978 0 521 84931 9  April 24, 2008  17:11  447  Index  running time, 264 satisﬁability problem, 261 solutions iterating, 262  recursive depth-ﬁrst search, 192 achieving postcondition, 193 code, 192 example, 193 running time, 194 recursive images, 153  exercise solutions, 421 ﬁxed recursive base case image, 153  base case, 153 birthday cake, 154 examples, 154 fractal, 155 image drawing, 153 man recursively framed, 154 recursing, 154 rotating square, 154  randomly generating maze, 156 recursive image speciﬁcation, 153 reduction, 324  dynamic programming algorithms via,  318  322  best path similarity, 319 bigger-is-smarter elephant problem,  event scheduling problem, 319 graph instance formation, 319 mapping back algorithm, 321  lower bound, 325 optimization problems reduction, 326 reverse reduction, 325 upper bound, 324 use in classifying problems, 326  Ackermann’s function,128 best binary search tree, 313 binary search, 24 binary search trees, 61 coloring the plane, 30 context-free grammar parsing, 165 counting sort, 75 dynamic programming, 277 GCD algorithm, 82 generic search algorithm, 177 heapsort, 147 heapify,143 interval cover, 243 iterative algorithms, 8, 19 job event scheduling, 240 linear programming, 223 makeheap, 145 magic sevens, 62 merge sort, 115  running time, 2. See also time space complexity  minimum spanning tree, 247 nodes in binary tree, 132 queens problem, 260 quick sort, 116 primal-dual hill climbing method, 213 radix counting sort, 78 randomly generating maze, 158 recursive algorithms, 102 recursive backtracking, 271 satisﬁability, 264 steepest-ascent hill-climbing algorithm,  216  towers of Hanoi, 104 union ﬁnd set system, 50 VLSI chip testing, 69  shortest weight path, directed leveled graph  example, 267  sorting and selection algorithms, 114  bubble sort example, 25 ﬁnding the kth smallest example, 117  choosing the pivot, 118  iterative sorting algorithms  bucket sort by hand, 71 counting sort, 72 radix counting sort, 76 radix sort, 75 merge sort, 115 quick sort, 116 randomized quick sort, 348 recursive sorting algorithm, general, 114  stack frame, 110 memory, 111 stack of stack frames, 110 tree of stack frames, 98, 110, 132, 165 using, 111  steepest-ascent hill-climbing algorithm, 214  augmentation path, 215 running time, 216  Strassen’s matrix multiplication example, 126  time space complexity, 82, 82, 85, 324, 347, 366,  378, 406, 432  examples, 369, 369 exercise solutions, 432 formal deﬁnition, 371 as functions, 367 operation deﬁnition, 368 purpose, 366 size deﬁnition, 367 time complexity of problem, 368  tournament example, 39 towers of Hanoi, 102  code, 104   P1: JZP  Gutter margin: 7 8′′  Top margin: 3 8′′  CUUS154-IND CUUS154-Edmonds 978 0 521 84931 9  April 24, 2008  17:11  448  Index  towers of Hanoi  cont.   divide and conquer, 103 running time, 104 speciﬁcation, 102, 103 subinstance, 104  tree edges, 190 trees, 48. See also binary search trees; recursion  on trees AVL tree, 49 best AVL tree problem, 313 best binary search tree, 311 binary search tree, 48 chains of matrix multiplications, solution as  tree example, 306  is a tree a binary search tree example, 138 nodes in binary tree example, 131 representing expressions with  differentiate expression example, 150 evaluate expression example, 149 recursive deﬁnition of expression, 149 simplify expression example, 151 tree data structure, 149  recursive deﬁnition of, 130 of stack frames, 98, 110, 132, 165 traversals, 133  universal quantiﬁers. See existential universal  quantiﬁers upper bound, 82  cuts as upper bound, 209 for hill-climbing, 222  VLSI chip testing, 65  brute force algorithm, 66 data structure, 66 exiting loop, 69 extending the algorithm, 69 faster algorithm, 67 initial code, 69 loop invariant design, 67 maintaining loop invariant, 68 measure of progress, 68 randomized algorithm, 69 running time, 69 speciﬁcation, 65  weighted job event scheduling problem, 303  failed algorithms, 303 greedy dynamic programming, 304 weighted event scheduling problem, 303  work done-bubble sort example, 25

@highlight

There are many algorithm texts that provide lots of well-polished code and proofs of correctness. This book is not one of them. Instead, this book presents insights, notations, and analogies to help the novice describe and think about algorithms like an expert. By looking at both the big picture and easy step-by-step methods for developing algorithms, the author helps students avoid the common pitfalls. He stresses paradigms such as loop invariants and recursion to unify a huge range of algorithms into a few meta-algorithms. Part of the goal is to teach the students to think abstractly. Without getting bogged with formal proofs, the book fosters a deeper understanding of how and why each algorithm works. These insights are presented in a slow and clear manner accessible to second- or third-year students of computer science, preparing them to find their own innovative ways to solve problems.