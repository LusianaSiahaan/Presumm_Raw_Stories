Algorithms for Reinforcement Learning   Copyright   2010 by Morgan & Claypool  All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form or by any means—electronic, mechanical, photocopy, recording, or any other except for brief quotations in printed reviews, without the prior permission of the publisher.  Algorithms for Reinforcement Learning  Csaba Szepesvári  www.morganclaypool.com  ISBN: 9781608454921 ISBN: 9781608454938  paperback ebook  DOI 10.2200 S00268ED1V01Y201005AIM009  A Publication in the Morgan & Claypool Publishers series SYNTHESIS LECTURES ON ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING  Lecture 9 Series Editors: Ronald J. Brachman, Yahoo! Research  Thomas Dietterich, Oregon State University  Series ISSN Synthesis Lectures on Artiﬁcial Intelligence and Machine Learning Print 1939-4608 Electronic 1939-4616   Synthesis Lectures on Artiﬁcial  Intelligence and Machine  Learning  Editors Ronald J. Brachman, Yahoo! Research Thomas Dietterich, Oregon State University  Algorithms for Reinforcement Learning Csaba Szepesvári 2010  Data Integration: The Relational Logic Approach Michael Genesereth 2010  Markov Logic: An Interface Layer for Artiﬁcial Intelligence Pedro Domingos and Daniel Lowd 2009  Introduction to Semi-Supervised Learning XiaojinZhu and Andrew B.Goldberg 2009  Action Programming Languages Michael Thielscher 2008  Representation Discovery using Harmonic Analysis Sridhar Mahadevan 2008  Essentials of Game Theory: A Concise Multidisciplinary Introduction Kevin Leyton-Brown and Yoav Shoham 2008   iv  A Concise Introduction to Multiagent Systems and Distributed Artiﬁcial Intelligence Nikos Vlassis 2007  Intelligent Autonomous Robotics: A Robot Soccer Case Study Peter Stone 2007   Algorithms for Reinforcement Learning  Csaba Szepesvári University of Alberta  SYNTHESIS LECTURES ON ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING 9  CM&  Morgan  cLaypool  publishers  &   ABSTRACT Reinforcement learning is a learning paradigm concerned with learning to control a system so as to maximize a numerical performance measure that expresses a long-term objective. What distin- guishes reinforcement learning from supervised learning is that only partial feedback is given to the learner about the learner’s predictions. Further, the predictions may have long term effects through inﬂuencing the future state of the controlled system. Thus, time plays a special role. The goal in reinforcement learning is to develop efﬁcient learning algorithms, as well as to understand the al- gorithms’ merits and limitations. Reinforcement learning is of great interest because of the large number of practical applications that it can be used to address, ranging from problems in artiﬁcial intelligence to operations research or control engineering. In this book, we focus on those algorithms of reinforcement learning that build on the powerful theory of dynamic programming. We give a fairly comprehensive catalog of learning problems, describe the core ideas, note a large number of state of the art algorithms, followed by the discussion of their theoretical properties and limitations.  KEYWORDS reinforcement learning, Markov Decision Processes, temporal difference learn- ing, stochastic approximation, two-timescale stochastic approximation, Monte-Carlo methods, simulation optimization, function approximation, stochastic gradient meth- ods, least-squares methods, overﬁtting, bias-variance tradeoff, online learning, active learning, planning, simulation, PAC-learning, Q-learning, actor-critic methods, policy gradient, natural gradient   Contents  vii  1  2  3  Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix  Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiii  Markov Decision Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1  1.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.2 Markov Decision Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.3 Value functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 1.4 Dynamic programming algorithms for solving MDPs . . . . . . . . . . . . . . . . . . . . . . . . . 10  Value Prediction Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11  2.1 Temporal difference learning in ﬁnite state spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.1.1 Tabular TD 0  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.1.2 Every-visit Monte-Carlo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .14 2.1.3 TD λ : Unifying Monte-Carlo and TD 0  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 2.2 Algorithms for large state spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 2.2.1 TD λ  with function approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 2.2.2 Gradient temporal difference learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 2.2.3 Least-squares methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .27 2.2.4 The choice of the function space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33  Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .37  3.1 A catalog of learning problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 3.2 Closed-loop interactive learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 3.2.1 Online learning in bandits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .38 3.2.2 Active learning in bandits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 3.2.3 Active learning in Markov Decision Processes . . . . . . . . . . . . . . . . . . . . . . . . . . 41   viii CONTENTS  3.2.4 Online learning in Markov Decision Processes . . . . . . . . . . . . . . . . . . . . . . . . . 42 3.3 Direct methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .47 3.3.1 Q-learning in ﬁnite MDPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 3.3.2 Q-learning with function approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .49 3.4 Actor-critic methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .52 3.4.1 Implementing a critic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 3.4.2 Implementing an actor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .56  4  For Further Exploration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63  4.1 Further reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .63 4.2 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 4.3 Software . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .64  A The Theory of Discounted Markovian Decision Processes . . . . . . . . . . . . . . . . . . . . . 65 A.1 Contractions and Banach’s ﬁxed-point theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .65 A.2 Application to MDPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69  Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73  Author’s Biography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .89   Preface  Reinforcement learning  RL  refers to both a learning problem and a subﬁeld of machine learning. As a learning problem, it refers to learning to control a system so as to maximize some numerical value which represents a long-term objective. A typical setting where reinforcement learning operates is shown in Figure 1: A controller receives the controlled system’s state and a reward associated with the last state transition. It then calculates an action which is sent back to the system. In response, the system makes a transition to a new state and the cycle is repeated. The problem is to learn a way of controlling the system so as to maximize the total reward. The learning problems differ in the details of how the data is collected and how performance is measured.  In this book, we assume that the system that we wish to control is stochastic. Further, we assume that the measurements available on the system’s state are detailed enough so that the con- troller can avoid reasoning about how to collect information about the state. Problems with these characteristics are best described in the framework of Markovian Decision Processes  MDPs . The standard approach to ‘solve’ MDPs is to use dynamic programming, which transforms the problem of ﬁnding a good controller into the problem of ﬁnding a good value function. However, apart from the simplest cases when the MDP has very few states and actions, dynamic programming is infea- sible. The RL algorithms that we discuss here can be thought of as a way of turning the infeasible dynamic programming methods into practical algorithms so that they can be applied to large-scale problems.  There are two key ideas that allow RL algorithms to achieve this goal. The ﬁrst idea is to use samples to compactly represent the dynamics of the control problem. This is important for   cid:1  cid:10  cid:11  cid:2  cid:4  cid:12   cid:1  cid:10  cid:11  cid:2  cid:4  cid:12    cid:2  cid:3  cid:4  cid:5  cid:6  cid:7    cid:1  cid:2  cid:3  cid:2  cid:4    cid:5  cid:6  cid:2  cid:7  cid:8  cid:9    cid:8  cid:9  cid:10  cid:11  cid:6  cid:9  cid:12  cid:12  cid:3  cid:6   cid:8  cid:9  cid:10  cid:11  cid:6  cid:9  cid:12  cid:12  cid:3  cid:6    cid:1    cid:1   Figure 1: The basic reinforcement learning scenario   x PREFACE  two reasons: First, it allows one to deal with learning scenarios when the dynamics is unknown. Second, even if the dynamics is available, exact reasoning that uses it might be intractable on its own.The second key idea behind RL algorithms is to use powerful function approximation methods to compactly represent value functions. The signiﬁcance of this is that it allows dealing with large, high-dimensional state- and action-spaces. What is more, the two ideas ﬁt nicely together: Samples may be focused on a small subset of the spaces they belong to, which clever function approximation techniques might exploit. It is the understanding of the interplay between dynamic programming, samples and function approximation that is at the heart of designing, analyzing and applying RL algorithms.  The purpose of this book is to allow the reader to have a chance to peek into this beau- tiful ﬁeld. However, certainly we are not the ﬁrst to set out to accomplish this goal. In 1996, Kaelbling et al. have written a nice, compact survey about the approaches and algorithms avail- able at the time  Kaelbling et al., 1996 . This was followed by the publication of the book by Bertsekas and Tsitsiklis  1996 , which detailed the theoretical foundations. A few years later Sutton and Barto, the ‘fathers’ of RL, published their book, where they presented their ideas on RL in a very clear and accessible manner  Sutton and Barto, 1998 . A more recent and comprehensive overview of the tools and techniques of dynamic programming optimal control criteria, as well as various classes of controlled systems is given in the two-volume book by Bertsekas  2007a,b  which devotes one chapter to RL methods.1 At times, when a ﬁeld is rapidly developing, books can get out of date pretty quickly. In fact, to keep up with the growing body of new results, Bertsekas maintains an online version of his Chapter 6 of Volume II of his book, which, at the time of writing this survey counted as much as 160 pages  Bertsekas, 2010 . Other recent books on the subject include the book of Gosavi  2003  who devotes 60 pages to reinforcement learning algorithms in Chapter 9, concentrating on average cost problems, or that of Cao  2007  who focuses on policy gradient methods. Powell  2007  presents the algorithms and ideas from an operations research perspective and emphasizes methods that are capable of handling large control spaces, Chang et al.  2008  fo- cuses on adaptive sampling  i.e., simulation-based performance optimization , while the center of the recent book by Busoniu et al.  2010  is function approximation.  Thus, by no means do RL researchers lack a good body of literature. However, what seems to be missing is a self-contained and yet relatively short summary that can help newcomers to the ﬁeld to develop a good sense of the state of the art, as well as existing researchers to broaden their overview of the ﬁeld, an article, similar to that of Kaelbling et al.  1996 , but with an updated contents. To ﬁll this gap is the very purpose of this short book.  Having the goal of keeping the text short, we had to make a few, hopefully, not too troubling compromises. The ﬁrst compromise we made was to present results only for the total expected discounted reward criterion. This choice is motivated by that this is the criterion that is both widely used and the easiest to deal with mathematically. The next compromise is that the background  1In this book, RL is called neuro-dynamic programming or approximate dynamic programming. The term neuro-dynamic pro- gramming stems from the fact that, in many cases, RL algorithms are used with artiﬁcial neural networks.   PREFACE xi  on MDPs and dynamic programming is kept ultra-compact  although an appendix is added that explains these basic results. Apart from these, the book aims to cover a bit of all aspects of RL, up to the level that the reader should be able to understand the whats and hows, as well as to implement the algorithms presented. Naturally, we still had to be selective in what we present. Here, the decision was to focus on the basic algorithms, ideas, as well as the available theory. Special attention was paid to describing the choices of the user, as well as the trade offs that come with these. We tried to be impartial as much as possible, but some personal bias, as usual, surely remained. The pseudocode of almost twenty algorithms was included, hoping that this will make it easier for the practically inclined reader to implement the algorithms described.  The target audience is advanced undergraduate and graduate students, as well as researchers and practitioners who want to get a good overview of the state of the art in RL quickly. Researchers who are already working on RL might also enjoy reading about parts of the RL literature that they are not so familiar with, thus broadening their perspective on RL. The reader is assumed to be familiar with the basics of linear algebra, calculus, and probability theory. In particular, we assume that the reader is familiar with the concepts of random variables, conditional expectations, and Markov chains. It is helpful, but not necessary, for the reader to be familiar with statistical learning theory, as the essential concepts will be explained as needed. In some parts of the book, knowledge of regression techniques of machine learning will be useful.  This book has three parts. In the ﬁrst part, in Section 1, we provide the necessary background. It is here where the notation is introduced, followed by a short overview of the theory of Markov De- cision Processes and the description of the basic dynamic programming algorithms. Readers familiar with MDPs and dynamic programming should skim through this part to familiarize themselves with the notation used. Readers, who are less familiar with MDPs, must spend enough time here before moving on because the rest of the book builds heavily on the results and ideas presented here.  The remaining two parts are devoted to the two basic RL problems  cf. Figure 2 , one part devoted to each. In Section 2  the problem of learning to predict values associated with states is studied. We start by explaining the basic ideas for the so-called tabular case when the MDP is small enough so that one can store one value per state in an array allocated in a computer’s main memory. The ﬁrst algorithm explained is TD λ , which can be viewed as the learning analogue to value iteration from dynamic programming. After this, we consider the more challenging situation when there are more states than what ﬁts into a computer’s memory. Clearly, in this case, one must compress the table representing the values. Abstractly, this can be done by relying on an appropriate function approximation method. First, we describe how TD λ  can be used in this situation. This is followed by the description of some new gradient based methods  GTD2 and TDC , which can be viewed as improved versions of TD λ  in that they avoid some of the convergence difﬁculties that TD λ  faces. We then discuss least-squares methods  in particular, LSTD λ  and λ-LSPE  and compare them to the incremental methods described earlier. Finally, we describe choices available for implementing function approximation and the trade offs that these choices come with.   xii PREFACE   cid:1  cid:2  cid:3  cid:4  cid:5  cid:6  cid:7  cid:5  cid:8  cid:9    cid:12    cid:8   cid:2   cid:7    cid:9   cid:8   cid:18    cid:10  cid:11  cid:12  cid:13  cid:3  cid:14  cid:5  cid:7  cid:3  cid:2  cid:11  cid:7  cid:5  cid:8  cid:9    cid:1  cid:8  cid:12  cid:5  cid:6  cid:15  cid:14  cid:5  cid:7  cid:3  cid:2  cid:11  cid:7  cid:5  cid:8  cid:9    cid:1  cid:8  cid:12  cid:5  cid:6  cid:15  cid:14  cid:16  cid:3  cid:11  cid:2  cid:6  cid:17   Figure 2: Types of reinforcement problems and approaches.  The second part  Section 3  is devoted to algorithms that are developed for control learning. First, we describe methods whose goal is optimizing online performance. In particular, we describe the “optimism in the face of uncertainty” principle and methods that explore their environment based on this principle. State of the art algorithms are given both for bandit problems and MDPs. The message here is that clever exploration methods make a large difference, but more work is needed to scale up the available methods to large problems. The rest of this section is devoted to methods that aim at developing methods that can be used in large-scale applications. As learning in large-scale MDPs is signiﬁcantly more difﬁcult than learning when the MDP is small, the goal of learning is relaxed to learning a good enough policy in the limit. First, direct methods are discussed which aim at estimating the optimal action-values directly. These can be viewed as the learning analogue of value iteration of dynamic programming.This is followed by the description of actor-critic methods, which can be thought of as the counterpart of the policy iteration algorithm of dynamic programming. Both methods based on direct policy improvement and policy gradient  i.e., which use parametric policy classes  are presented.  The book is concluded in Section 4, which lists some topics for further exploration.  Csaba Szepesvári June 2010   Acknowledgments  I am truly indebted to my family for their love, support and patience. Thank you Mom, Beáta, Dávid, Réka, Eszter, Csongor! Special thanks to Réka who has helped me drawing Figure 1.1. A number of individuals have read various versions of the manuscript, full or in parts and helped me to reduce the number of mistakes by sending corrections. They include Dimitri Bertsekas, Gábor Balázs,Bernardo Avila Pires,Warren Powell,Rich Sutton,Nikos Vlassis,Hengshuai Yao and Shimon Whiteson. Thank You! Of course, all the remaining mistakes are mine. If I have left out someone from the above list, this was by no means intentional. If this is the case, please remind me in an e-mail  better yet, send me some comments or suggestions . Independently of whether they have contacted me before or not, readers are encouraged to e-mail me if they ﬁnd errors, typos or they just think that some topic should have been included  or left out . I plan to periodically update the text and I will try to accommodate all the requests. Finally, I wish to thank Remi Munos and Rich Sutton, my closest collaborators over the last few years, from whom I have learned and continue to learn a lot. I also wish to thank all my students, the members of RLAI group and all researchers of RL who continue to strive to push the boundaries of what we can do with reinforcement learning. This book is made possible by you.  Csaba Szepesvári June 2010    C H A P T E R 1  1  Markov Decision Processes  The purpose of this section is to introduce the notation that will be used in the subsequent parts and the most essential facts that we will need from the theory of Markov Decision Processes  MDPs  in the rest of the book. Readers familiar with MDPs should skim through this section to familiarize themselves with the notation. Readers unfamiliar with MDPs are suggested to spend enough time with this section to understand the details. Proofs of most of the results  with some simpliﬁcations  are included in Appendix A. The reader who is interested in learning more about MDPs is suggested to consult one of the many excellent books on the subject, such as the books of Bertsekas and Shreve  1978 , Puterman  1994 , or the two-volume book by Bertsekas  2007a,b .  d   cid:2   PRELIMINARIES  1.1 We use N to denote the set of natural numbers: N = {0, 1, 2, . . .}, while R denotes the set of reals. dimensional vectors, u, v ∈ Rd is  cid:4 u, v cid:5  = cid:2  By a vector v  unless it is transposed, v  , we mean a column vector.The inner product of two ﬁnite- i=1 ui vi. The resulting 2-norm is  cid:6 u cid:6 2 =  cid:4 u, u cid:5 . The maximum norm for vectors is deﬁned by  cid:6 u cid:6 ∞ = maxi=1,...,d ui, while for a function f : X → R,  cid:6 ˙ cid:6 ∞ is deﬁned by  cid:6 f cid:6 ∞ = supx∈X f  x . A mapping T between the metric spaces  M1, d1 ,  M2, d2  is called Lipschitz with modulus L ∈ R if for any a, b ∈ M1, d2 T  a , T  b   ≤ L d1 a, b . If T is Lipschitz with a modulus L ≤ 1, it is called a non-expansion. If L < 1, the mapping is called a contraction. The indicator function of event S will be denoted by I{S}  i.e., I{S} = 1 if S holds and I{S} = 0, otherwise . If v = v θ, x , ∂ ∂θ v shall denote the partial derivative of v with respect to θ, which, if θ, which is a d-dimensional row vector if θ ∈ Rd. The total derivative of some expression v dθ v  and will be treated as a row vector . Further,∇θ v =   d  cid:2  with respect to θ will be denoted by d . dθ v  If P is a distribution or a probability measure, then X ∼ P means that X is a random variable  drawn from P .  1.2 MARKOV DECISION PROCESSES For ease of exposition, we restrict our attention to countable MDPs and the discounted total expected reward criterion. However, under some technical conditions, the results extend to continuous state- action MDPs, too. This also holds true for the results presented in later parts of this book. A countable MDP is deﬁned as a triplet M =  X ,A,P0 , where X is the countable non- empty set of states, A is the countable non-empty set of actions. The transition probability kernel P0 assigns to each state-action pair  x, a  ∈ X × A a probability measure over X × R, which we shall denote by P0 ·x, a .The semantics of P0 is the following: For U ⊂ X × R,P0 Ux, a  gives   2  1. MARKOV DECISION PROCESSES the probability that the next state and the associated reward belongs to the set U provided that the current state is x and the action taken is a.1 We also ﬁx a discount factor 0 ≤ γ ≤ 1 whose role will become clear soon. The transition probability kernel gives rise to the state transition probability kernel, P, which, for any  x, a, y  ∈ X × A × X triplet gives the probability of moving from state x to some other state y provided that action a was chosen in state x:  P x, a, y  = P0 {y} × R x, a .  In addition to P, P0 also gives rise to the immediate reward function r : X × A → R, which gives the expected immediate reward received when action a is chosen in state x: If  Y x,a , R x,a   ∼ P0 · x, a , then   cid:3    cid:4   r x, a  = E  R x,a   .  In what follows, we shall assume that the rewards are bounded by some quantity R > 0: for any  x, a  ∈ X × A,R x,a  ≤ R almost surely. It is immediate that if the random rewards are bounded by R then  cid:6 r cid:6 ∞ = sup x,a ∈X×A r x, a  ≤ R also holds. An MDP is called ﬁnite if both X and A are ﬁnite. Markov Decision Processes are a tool for modeling sequential decision-making problems where a decision maker interacts with a system in a sequential fashion. Given an MDP M, this interaction happens as follows: Let t ∈ N denote the current time  or stage , let Xt ∈ X and At ∈ A denote the random state of the system and the action chosen by the decision maker at time t, respectively. Once the action is selected, it is sent to the system, which makes a transition:   cid:3    Xt+1, Rt+1  ∼ P0 · Xt , At  .  1.1   cid:4  = r Xt , At  . The decision maker then observes the next state In particular, Xt+1 is random and P  Xt+1 = yXt = x, At = a  = P x, a, y  holds for any x, y ∈ X , a ∈ A. Further, E Xt+1 and reward Rt+1, chooses a new action At+1 ∈ A and the process is repeated. The goal of the decision maker is to come up with a way of choosing the actions so as to maximize the expected total discounted reward.  Rt+1Xt , At  The decision maker can select its actions at any stage based on the observed history. A rule describing the way the actions are selected is called a behavior. A behavior of the decision maker and some initial random state X0 together deﬁne a random state-action-reward sequence   Xt , At , Rt+1 ; t ≥ 0 , where  Xt+1, Rt+1  is connected to  Xt , At   by  1.1  and At is the action prescribed by the behavior based on the history X0, A0, R1, . . . , Xt−1, At−1, Rt , Xt.2 1The probability P0 Ux, a  is deﬁned only when U is a Borel-measurable set. Borel-measurability is a technical notion whose purpose is to prevent some pathologies. The collection of Borel-measurable subsets of X × R include practically all “interesting” subsets X × R. In particular, they include subsets of the form {x} × [a, b] and subsets which can be obtained from such subsets by taking their complement, or the union  intersection  of at most countable collections of such sets in a recursive fashion. 2Mathematically, a behavior is an inﬁnite sequence of probability kernels π0, π1, . . . , πt , . . ., where πt maps histories of length t to a probability distribution over the action space A: πt = πt  ·x0, a0, r0, . . . , xt−1, at−1, rt−1, xt  .   The return underlying a behavior is deﬁned as the total discounted sum of the rewards incurred:  1.2. MARKOV DECISION PROCESSES 3  ∞ cid:5  t=0  R =  γ t Rt+1.  Thus, if γ < 1 then rewards far in the future worth exponentially less than the reward received at the ﬁrst stage. An MDP when the return is deﬁned by this formula is called a discounted reward MDP. When γ = 1, the MDP is called undiscounted.  The goal of the decision-maker is to choose a behavior that maximizes the expected return,  irrespectively of how the process is started. Such a maximizing behavior is said to be optimal.  Example 1.1 Inventory control with lost sales Consider the problem of day-to-day control of an inventory of a ﬁxed maximum size in the face of uncertain demand: Every evening, the decision maker must decide about the quantity to be ordered for the next day. In the morning, the ordered quantity arrives with which the inventory is ﬁlled up. During the day, some stochastic demand is realized, where the demands are independent with a common ﬁxed distribution, see Figure 1.1. The goal of the inventory manager is to manage the inventory so as to maximize the present monetary value of the expected total future income. The payoff at time step t is determined as follows: The cost associated with purchasing At items is K I{At >0} + cAt. Thus, there is a ﬁxed entry cost K of ordering nonzero items and each item must be purchased at a ﬁxed price c. Here K, c > 0. In addition, there is a cost of holding an inventory of size x > 0. In the simplest case, this cost is proportional to the size of the inventory with proportionality factor h > 0. Finally, upon selling z units the manager is paid the monetary amount of p z, where p > 0. In order to make the problem interesting, we must have p > h, otherwise there is no incentive to order new items. This problem can be represented as an MDP as follows: Let the state Xt on day t ≥ 0 be the size of the inventory in the evening of that day. Thus, X = {0, 1, . . . , M}, where M ∈ N is the maximum inventory size. The action At gives the number of items ordered in the evening of day t. Thus, we can choose A = {0, 1, . . . , M} since there is no need to consider orders larger than the inventory size. Given Xt and At, the size of the next inventory is given by  Xt+1 =   Xt + At   ∧ M − Dt+1   +   1.2  + = a ∨ 0 = where a ∧ b is a shorthand notation for the minimum of the numbers a, b,  a  max a, 0  is the positive part of a, and Dt+1 ∈ N is the demand on the  t + 1 th day. be higher than that of ∧ and ∨ . By assumption,  Dt; t > 0  is a sequence of independent and identically distributed  i.i.d.  integer-valued random variables. The revenue made on day t + 1 is  ,  Rt+1 = −K I{At >0} − c   Xt + At   ∧ M − Xt    + + p   Xt + At   ∧ M − Xt+1   − h Xt  +  .   1.3    4  1. MARKOV DECISION PROCESSES   cid:1  cid:2  cid:3  cid:4  cid:4    cid:5  cid:3  cid:4  cid:4    cid:1  cid:6  cid:3  cid:4  cid:4   Figure 1.1: Illustration of the inventory management problem   1.4   Equations  1.2 – 1.3  can be written in the compact form  Xt+1, Rt+1  = f  Xt , At , Dt+1 ,  with an appropriately chosen function f . Then, P0 is given by ∞ cid:5  d=0  P0 U  x, a  = P  f  x, a, D  ∈ U   =  I{f  x,a,d ∈U} pD d .  Here pD ·  is the probability mass function of the random demands and D ∼ pD · . This ﬁnishes the deﬁnition of the MDP underlying the inventory optimization problem.  Inventory control is just one of the many operations research problems that give rise to an MDP. Other problems include optimizing transportation systems, optimizing schedules or production. MDPs arise naturally in many engineering optimal control problems, too, such as the optimal control of chemical, electronic or mechanical systems  the latter class includes the problem of controlling robots . Quite a few information theory problems can also be represented as MDPs  e.g., optimal coding, optimizing channel allocation, or sensor networks . Another important class of problems comes from ﬁnance. These include, amongst others, optimal portfolio management and option pricing.  In the case of the inventory control problem, the MDP was conveniently speciﬁed by a transition function f  cf.,  1.4  . In fact, transition functions are as powerful as transition kernels: any MDP gives rise to some transition function f and any transition function f gives rise to some MDP.  In some problems, not all actions are meaningful in all states. For example, ordering more items than what one has room for in the inventory does not make much sense. However, such meaningless actions  or forbidden actions  can always be remapped to other actions, just like it was done above. In some cases, this is unnatural and leads to a convoluted dynamics. Then, it might be better to introduce an additional mapping which assigns the set of admissible actions to each state.   1.2. MARKOV DECISION PROCESSES 5  In some MDPs, some states are impossible to leave: If x is such a state, Xt+s = x holds almost surely3 for any s ≥ 1 provided that Xt = x, no matter what actions are selected after time t. By convention, we will assume that no reward is incurred in such terminal or absorbing states. An MDP with such states is called episodic. An episode then is the  generally random  time period from the beginning of time until a terminal state is reached. In an episodic MDP, we often consider undiscounted rewards, i.e., when γ = 1. Example 1.2 Gambling A gambler enters a game whereby she may stake any fraction At ∈ [0, 1] of his current wealth Xt ≥ 0. She wins his stake back and as much more with probability p ∈ [0, 1], while she loses his stake with probability 1 − p. Thus, the fortune of the gambler evolves according to Here  St; t ≥ 1  is a sequence of independent random variables taking values in {−1,+1} with P  St+1 = 1  = p. The goal of the gambler is to maximize the probability that his wealth reaches an a priori given value w ∗] This problem can be represented as an episodic MDP, where the state space is X = [0, w and the action space is A = [0, 1].4 We deﬁne  > 0. It is assumed that the initial wealth is in [0, w  Xt+1 =  1 + St+1At  Xt .  ∗].  ∗  when 0 ≤ Xt < w is zero as long as Xt+1 < w  ∗  ∗  and make w  ,  ∗  Xt+1 =  1 + St+1At  Xt ∧ w a terminal state: Xt+1 = Xt if Xt = w ∗  cid:6   ∗ and is one when the state reaches w and Xt+1 = w Rt+1 =  1, Xt < w 0,  otherwise.  ∗  ∗  for the ﬁrst time: ∗;   1.5   . The immediate reward  If we set the discount factor to one, the total reward along any trajectory will be one or zero depending on whether the wealth reaches w . Thus, the expected total reward is just the probability that the gambler’s fortune reaches w  ∗  ∗  .  Based on the two examples presented so far, the reader unfamiliar with MDPs might believe that all MDPs come with handy ﬁnite,one-dimensional state- and action-spaces.If only this was true! In fact, in practical applications the state- and action-spaces are often very large, multidimensional spaces. For example, in a robot control application, the dimensionality of the state space can be 3—6 times the number of joints the robot has. An industrial robot’s state space might easily be 12—20 dimensional, while the state space of a humanoid robot might easily have 100 dimensions. In a real-world inventory control application, items would have multiple types, the prices and costs  3“Almost surely” means the same as “with probability one” and is used to refer to the fact that the statement concerned holds everywhere on the probability space with the exception of a set of events with measure zero. 4Hence, in this case the state and action spaces are continuous. Notice that our deﬁnition of MDPs is general enough to encompass this case, too.   6  1. MARKOV DECISION PROCESSES would also change based on the state of the “market”, whose state would thus also become part of the MDP’s state. Hence, the state space in any such practical application would be very large and very high dimensional.The same holds for the action spaces.Thus, working with large, multidimensional state- and action-spaces should be considered the normal situation, while the examples presented in this section with their one-dimensional, small state spaces should be viewed as the exceptions.  1.3  VALUE FUNCTIONS  The obvious way of ﬁnding an optimal behavior in some MDP is to list all behaviors and then identify the ones that give the highest possible value for each initial state. Since, in general, there are too many behaviors, this plan is not viable. A better approach is based on computing value functions. In this approach, one ﬁrst computes the so-called optimal value function, which then allows one to determine an optimal behavior with relative easiness.  x , of state x ∈ X gives the highest achievable expected return when ∗ : X → R is called the optimal value function. A  the process is started from state x. The function V behavior that achieves the optimal values in all states is optimal.  The optimal value, V  ∗  Deterministic stationary policies represent a special class of behaviors, which, as we shall see soon, play an important role in the theory of MDPs. They are speciﬁed by some mapping π, which maps states to actions  i.e., π : X → A . Following π means that at any time t ≥ 0 the action At is selected using  At = π Xt  .   1.6   More generally, a stochastic stationary policy  or just stationary policy  π maps states to distri- butions over the action space. When referring to such a policy π, we shall use π ax  to denote the probability of action a being selected by π in state x. Note that if a stationary policy is followed in an MDP, i.e., if  At ∼ π · Xt  ,  t ∈ N,  the state process  Xt; t ≥ 0  will be a  time-homogeneous  Markov chain. We will use  cid:7 stat to denote the set of all stationary policies. For brevity, in what follows, we will often say just “policy” instead of “stationary policy”, hoping that this will not cause confusion. A stationary policy and an MDP induce what is called a Markov reward processes  MRP : An MRP is determined by the pair M =  X ,P0 , where now P0 assigns a probability measure over X × R to each state. An MRP M gives rise to the stochastic process   Xt , Rt+1 ; t ≥ 0 , where  Xt+1, Rt+1  ∼ P0 · Xt  .  Note that  Zt; t ≥ 0 , Zt =  Xt , Rt   is a time-homogeneous Markov process,where R0 is an arbitrary random variable,while   Xt , Rt+1 ; t ≥ 0  is a second-order Markov process.  Given a stationary policy π and the MDP M =  X ,A,P0 , the transition kernel of the a∈A π ax P0 · x, a . An MRP  X ,P π MRP is called ﬁnite if its state space is ﬁnite.  0   induced by π and M is deﬁned using P π  0  · x  = cid:2    1.3. VALUE FUNCTIONS 7   cid:7  ∞ cid:5    cid:9    cid:8  cid:8  cid:8  X0 = x  Let us now deﬁne value functions underlying stationary policies.5 For this, let us ﬁx some  policy π ∈  cid:7 stat. The value function, V π : X → R, underlying π is deﬁned by  V π  x  = E  γ t Rt+1  x ∈ X ,  t=0   1.7  with the understanding  i  that the process  Rt; t ≥ 1  is the “reward-part” of the process   Xt , At , Rt+1 ; t ≥ 0  obtained when following policy π and  ii  X0 is selected at random such that P  X0 = x  > 0 holds for all states x. This second condition makes the conditional expectation in  1.7  well-deﬁned for every state. If the initial state distribution satisﬁes this condition, it has no inﬂuence on the deﬁnition of values.  ,  The value function underlying an MRP is deﬁned the same way and is denoted by V :   cid:7  ∞ cid:5   t=0   cid:9    cid:8  cid:8  cid:8  X0 = x  V  x  = E  γ t Rt+1  x ∈ X .  ,  It will also be useful to deﬁne the action-value function, Qπ : X × A → R, underlying a policy π ∈  cid:7 stat in an MDP: Assume that the ﬁrst action A0 is selected randomly such that P  A0 = a  > 0 holds for all a ∈ A, while for the subsequent stages of the decision process the actions are chosen by following policy π. Let   Xt , At , Rt+1 ; t ≥ 0  be the resulting stochastic process, where X0 is as in the deﬁnition of V π . Then Qπ  x, a  = E   cid:8  cid:8  cid:8  X0 = x, A0 = a  x ∈ X , a ∈ A.   cid:7  ∞ cid:5   γ t Rt+1   cid:9   ,  t=0  ∗  ∗  x , the optimal action-value Q  Similarly to V   x, a  at the state-action pair  x, a  is deﬁned as the maximum of the expected return under the constraints that the process starts at state x, and the ∗ : X × A → R is called the optimal action-value ﬁrst action chosen is a.The underlying function Q function.  The optimal value- and action-value functions are connected by the following equations:  ∗   x  = sup  ∗  x, a  = r x, a  + γ  a∈A Q  V ∗   x, a ,  Q   cid:5  y∈X  P x, a, y V  ∗   y ,  x ∈ X , x ∈ X , a ∈ A.  In the class of MDPs considered here, an optimal stationary policy always exists:  V  ∗   x  = sup π∈ cid:7 stat  V π  x ,  x ∈ X .  5Value functions can also be deﬁned underlying any behavior analogously to the deﬁnition given below.   8  1. MARKOV DECISION PROCESSES  In fact, any policy π ∈  cid:7 stat which satisﬁes cid:5   π ax  Q ∗   x, a  = V  ∗  a∈A   1.8  simultaneously for all states x ∈ X is optimal. Notice that in order  1.8  to hold, π ·x  must be  x,· . In general, given some action-value ∗ concentrated on the set of actions that maximize Q function, Q : X × A → R, an action that maximizes Q x,·  for some state x is called greedy with respect to Q in state x. A policy that chooses greedy actions only with respect to Q in all states is called greedy w.r.t. Q.   x   ∗ Thus, a greedy policy with respect to Q  for ﬁnding an optimal policy. Similarly, knowing V ∗  The next question is how to ﬁnd V  or Q  ∗  ∗ is optimal, i.e., the knowledge of Q  , r and P also sufﬁces to act optimally.  ∗  alone is sufﬁcient  . Let us start with the simpler question of how to  ﬁnd the value function of a policy: Fact 1.3 Bellman Equations for Deterministic Policies Fix an MDP M =  X ,A,P0 , a dis- count factor γ and deterministic policy π ∈  cid:7 stat. Let r be the immediate reward function of M. Then V π satisﬁes  V π  x  = r x, π x   + γ  P x, π x , y V π  y ,  x ∈ X .   1.9    cid:5  y∈X  This system of equations is called the Bellman equation for V π .Deﬁne the Bellman operator underlying π, T π : R  X → R  X , by  T π V   x  = r x, π x   + γ   cid:5  y∈X  P x, π x , y V  y ,  x ∈ X .  With the help of T π , Equation  1.9  can be written in the compact form  T π V π = V π .   1.10  Note that this is a linear system of equations in V π and T π is an afﬁne linear operator. If 0 < γ < 1 then T π is a maximum-norm contraction and the ﬁxed-point equation T π V = V has a unique solution. When the state space X is ﬁnite, say, it has D states, R can be identiﬁed with the D- dimensional Euclidean space and V ∈ R can be thought of as a D-dimensional vector: V ∈ RD. With this identiﬁcation, T π V can also be written as r π + γ P π V with an appropriately deﬁned vector r π ∈ RD and matrix P π ∈ RD×D. In this case,  1.10  can be written in the form  X  X  The above facts also hold true in MRPs, where the Bellman operator T : R  X → R  X  is deﬁned  r π + γ P π V π = V π .   1.11   by   T V   x  = r x  + γ  P x, y V  y ,  x ∈ X .   cid:5  y∈X   The optimal value function is known to satisfy a certain ﬁxed-point equation:  1.3. VALUE FUNCTIONS 9  Fact 1.4 Bellman Optimality Equations The optimal value function satisﬁes the ﬁxed-point equation   cid:5  y∈X  ⎧⎨ ⎩r x, a  + γ ⎧⎨ ⎩r x, a  + γ  P x, a, y V  ∗   y   ⎫⎬ ⎭ , ⎫⎬ ⎭ , P x, a, y V  y   X → R  , by  X  ∗ : R  cid:5  y∈X  ∗   x  = sup a∈A  V   T  ∗  V   x  = sup a∈A  Deﬁne the Bellman optimality operator operator, T  x ∈ X .   1.12   x ∈ X .   1.13   ∗  , Equation  1.12   Note that this is a nonlinear operator due to the presence of sup.With the help of T can be written compactly as  ∗  T  V  ∗ = V  ∗  .  V = V If 0 < γ < 1, then T has a unique solution. In order to minimize clutter, in what follows we will write expressions like  is a maximum-norm contraction, and the ﬁxed-point equation T  ∗  ∗   T π V   x  as T π V  x , with the understanding that the application of operator T π takes precedence to the application of the point evaluation operator, “·  x ”.  The action-value functions underlying a policy  or an MRP  and the optimal action-value  function also satisfy some ﬁxed-point equations similar to the previous ones:  Fact 1.5 Bellman Operators and Fixed-point Equations for Action-value Functions With a slight abuse of notation, deﬁne T π : R T π Q x, a  = r x, a  + γ Q x, a  = r x, a  + γ ∗  ∗ : R P x, a, y Q y, π x  ,  cid:17  P x, a, y  sup   x, a  ∈ X × A,  x, a  ∈ X × A.  X×A → R  as follows:   1.14   and T  X×A  X×A   ,  T   1.15   X×A → R  cid:5   cid:5  y∈X y∈X   cid:17 ∈A Q y, a  a  Note that T π is again afﬁne linear, while T are maximum- norm contractions. Further, the action-value function of π, Qπ , satisﬁes T π Qπ = Qπ and Qπ is ∗ the unique solution to this ﬁxed-point equation. Similarly, the optimal action-value function, Q , satisﬁes T  is the unique solution to this ﬁxed-point equation.  is nonlinear. The operators T π and T  ∗ = Q ∗  and Q  Q  ∗  ∗  ∗  ∗   10  1. MARKOV DECISION PROCESSES  1.4 DYNAMIC PROGRAMMING ALGORITHMS FOR  SOLVING MDPS  The above facts provide the basis for the value- and policy-iteration algorithms.  Value iteration generates a sequence of value functions k ≥ 0,  Vk+1 = T  Vk,  ∗  where V0 is arbitrary. Thanks to Banach’s ﬁxed-point theorem,  Vk; k ≥ 0  converges to V geometric rate.  ∗  at a  Value iteration can also be used in conjunction with action-value functions; in which case, it  takes the form  Qk+1 = T  ∗  Qk,  k ≥ 0,  which again converges to Q  resp., Q  ∗  ∗  at a geometric rate. The idea is that once Vk  or Qk  is close to V   , a policy that is greedy with respect to Vk  resps., Qk  will be close-to-optimal.  In particular, the following bound is known to hold: Fix an action-value function Q and let π be a greedy policy w.r.t. Q. Then the value of policy π can be lower bounded as follows  e.g., Singh and Yee, 1994, Corollary 2 : V π  x  ≥ V   cid:6 Q − Q  x ∈ X .  ∗ cid:6 ∞,  ∗   1.16    x  − 2 1 − γ  ∗  Policy iteration works as follows. Fix an arbitrary initial policy π0. At iteration k > 0, compute the action-value function underlying πk  this is called the policy evaluation step . Next, given Qπk, deﬁne πk+1 as a policy that is greedy with respect to Qπk  this is called the policy improvement step . After k iterations, policy iteration gives a policy not worse than the policy that is greedy w.r.t. to the value function computed using k iterations of value iteration if the two procedures are started with the same initial value function. However, the computational cost of a single step in policy iteration is much higher  because of the policy evaluation step  than that of one update in value iteration.   C H A P T E R 2  11  Value Prediction Problems  In this section, we consider the problem of estimating the value function V underlying some Markov reward process  MRP . Value prediction problems arise in a number of ways: Estimating the proba- bility of some future event, the expected time until some event occurs, or the  action- value function underlying some policy in an MDP are all value prediction problems. Speciﬁc applications are esti- mating the failure probability of a large power grid  Frank et al., 2008  or estimating taxi-out times of ﬂights on busy airports  Balakrishna et al., 2008 , just to mention two of the many possibilities. Since the value of a state is deﬁned as the expectation of the random return when the process is started from the given state, an obvious way of estimating this value is to compute an average over multiple independent realizations started from the given state. This is an instance of the so-called Monte-Carlo method. Unfortunately, the variance of the returns can be high, which means that the quality of the estimates will be poor. Also, when interacting with a system in a closed-loop fashion  i.e., when estimation happens while interacting with the system , it might be impossible to reset the state of the system to some particular state. In this case, the Monte-Carlo technique cannot be applied without introducing some additional bias. Temporal difference  TD  learning  Sutton, 1984, 1988 , which is without doubt one of the most signiﬁcant ideas in reinforcement learning, is a method that can be used to address these issues.  2.1 TEMPORAL DIFFERENCE LEARNING IN FINITE STATE  SPACES  The unique feature of TD learning is that it uses bootstrapping: predictions are used as targets during the course of learning. In this section, we ﬁrst introduce the most basic TD algorithm and explain how bootstrapping works. Next, we compare TD learning to  vanilla  Monte-Carlo methods, we and argue that both of them have their own merits. Finally, we present the TD λ  algorithm that uniﬁes the two approaches. Here we consider only the case of small, ﬁnite MRPs, when the value-estimates of all the states can be stored in the main memory of a computer in an array or table, which is known as the tabular case in the reinforcement learning literature. Extensions of the ideas presented here to large state spaces, when a tabular representations is not feasible, will be described in the subsequent sections.  2.1.1 TABULAR TD 0  Fix some ﬁnite Markov Reward Process M. We wish to estimate the value function V underlying M given a realization   Xt , Rt+1 ; t ≥ 0  of M. Let ˆVt  x  denote the estimate of state x at time t   12  2. VALUE PREDICTION PROBLEMS  V is the array storing the current value estimates  Algorithm 1 The function implementing the tabular TD 0  algorithm.This function must be called after each transition. function TD0 X, R, Y, V   Input: X is the last state, Y is the next state, R is the immediate reward associated with this transition, 1: δ ← R + γ · V[Y] − V[X] 2: V[X] ← V[X] + α · δ 3: return V  say, ˆV0 ≡ 0 . In the tth step TD 0  performs the following calculations: δt+1 = Rt+1 + γ ˆVt  Xt+1  − ˆVt  Xt  ,  ˆVt+1 x  = ˆVt  x  + αt δt+1 I{Xt=x},  x ∈ X .   2.1   Here the step-size sequence  αt; t ≥ 0  consists of  small  nonnegative numbers chosen by the user. Algorithm 1 shows the pseudocode of this algorithm. A closer inspection of the update equation reveals that the only value changed is the one associated with Xt, i.e., the state just visited  cf. line 2 of the pseudocode . Further, when αt ≤ 1, the value of Xt is moved towards the “target” Rt+1 + γ ˆVt  Xt+1 . Since the target depends on the estimated value function, the algorithm uses bootstrapping. The term “temporal difference” in the name of the algorithm comes from that δt+1 is deﬁned as the difference between values of states corresponding to successive time steps. In particular, δt+1 is called a temporal difference error.  Just like many other algorithms in reinforcement learning, tabular TD 0  is a stochastic ap- proximation  SA  algorithm. It is easy to see that if it converges, then it must converge to a function ˆV such that the expected temporal difference given ˆV ,   cid:16   F ˆV  x   def= E  Rt+1 + γ ˆV  Xt+1  − ˆV  Xt     cid:17    cid:8  cid:8  cid:8  Xt = x  ,  is zero for all states x, at least for all states that are sampled inﬁnitely often. A simple calculation shows that F ˆV = T ˆV − ˆV , where T is the Bellman-operator underlying the MRP considered. By Fact 1.3, F ˆV = 0 has a unique solution, the value function V . Thus, if TD 0  converges  and all states are sampled inﬁnitely often  then it must converge to V . To study the algorithm’s convergence properties, for simplicity, assume that  Xt; t ∈ N  ˆVt is a stationary, ergodic Markov chain.1 Further, identify the approximate value functions ˆVt,i = ˆVt  xi  , i = 1, . . . , D, where D = X and with D-dimensional vectors as before  e.g., X = {x1, . . . , xD} . Then, assuming that the step-size sequence satisﬁes the Robbins-Monro  RM  1Remember that a Markov chain  Xt; t ∈ N  is ergodic it is irreducible, aperiodic and positive recurrent. Practically, this means that the law of large number holds for sufﬁciently regular functions of the chain.   2.1. TEMPORAL DIFFERENCE LEARNING IN FINITE STATE SPACES 13  conditions,  ∞ cid:5  t=0  αt = ∞,  t < +∞, α2  ∞ cid:5  t=0  t ≥ 0,  ˙v t   = c F  v t   ,  ˙v = r +  γ P − I  v.  the sequence   ˆVt ∈ RD; t ∈ N  will track the trajectories of the ordinary differential equation  ODE   2.2  where c = 1 D and v t   ∈ RD  e.g., Borkar, 1998 . Borrowing the notation used in  1.11 , the above ODE can be written as Note that this is a linear ODE. Since the eigenvalues of γ P − I all lie in the open left half complex plane, this ODE is globally asymptotically stable. From this, using standard results of SA it follows that ˆVt converges almost surely to V . On step-sizes Since many of the algorithms that we will discuss use step-sizes, it is worthwhile spending some time on discussing their choice. A simple step-size sequence that satisﬁes the above conditions is αt = c t, with c > 0. More generally, any step-size sequence of the form αt = ct −η will work as long as 1 2 < η ≤ 1. Of these step-size sequences, η = 1 gives the smallest step-sizes. Asymptotically, this choice will be the best, but from the point of view of the transient behavior of the algorithm, choosing η closer to 1 2 will work better  since with this choice the step-sizes are bigger and thus the algorithm will make larger moves . It is possible to do even better than this. In fact, a simple method, called iterate-averaging due to Polyak and Juditsky  1992 , is known to achieve the best possible asymptotic rate of convergence. However, despite its appealing theoretical properties, iterate-averaging is rarely used in practice. In fact, in practice people often use constant step-sizes, which clearly violates the RM conditions. This choice is justiﬁed based on two grounds: First, the algorithms are often used in a non-stationary environment  i.e., the policy to be evaluated might change . Second, the algorithms are often used only in the small sample regime.  When a constant step-size is used, the parameters converge in distribution. The variance of the limiting distribution will be proportional to the step-size chosen.  There is also a great deal of work going into developing methods that tune step-sizes automatically, see  Sutton, 1992; Schraudolph, 1999; George and Powell, 2006  and the references therein. However, the jury is still out on which of these methods is the best. With a small change, the algorithm can also be used on an observation sequence of the form   Xt , Rt+1, Yt+1 ; t ≥ 0 , where  Xt; t ≥ 0  is an arbitrary ergodic Markov chain over X ,  Yt+1, Rt+1  ∼ P0 · Xt  . The change concerns the deﬁnition of temporal differences:  δt+1 = Rt+1 + γ ˆV  Yt+1  − ˆV  Xt  .  Then, with no extra conditions, ˆVt still converges almost surely to the value function underlying the MRP  X ,P0 . In particular, the distribution of the states  Xt; t ≥ 0  does not play a role here.   14  2. VALUE PREDICTION PROBLEMS  This is interesting for multiple reasons. For example, if the samples are generated using a simulator, we may be able to control the distribution of the states  Xt; t ≥ 0  independently of the MRP. This might be useful to counterbalance any unevenness in the stationary distribution underlying the Markov kernel P. Another use is to learn about some target policy in an MDP while following some other policy, often called the behavior policy. Assume for simplicity that the target policy is deterministic.Then   Xt , Rt+1, Yt+1 , t ≥ 0  could be obtained by skipping all those state- action-reward-next state quadruples in the trajectory generated by using the behavior policy, where the action taken does not match the action that would have been taken in the given state by the target policy, while keeping the rest. This technique might allow one to learn about multiple policies at the same time  more generally, about multiple long-term prediction problems . When learning about one policy, while following another is called off-policy learning. Because of this, we shall also call learning based on triplets   Xt , Rt+1, Yt+1 ; t ≥ 0  when Yt+1  cid:20 = Xt+1 off-policy learning. A third, technical use is when the goal is to apply the algorithm to an episodic problem. In this case, the triplets  Xt , Rt+1, Yt+1  are chosen as follows: First, Yt+1 is sampled from the transition kernel P X,· . If Yt+1 is not a terminal state, we let Xt+1 = Yt+1; otherwise, Xt+1 ∼ P0 · , where P0 is a user-chosen distribution over X . In other words, when a terminal state is reached, the process is restarted from the initial state distribution P0. The period between the time of a restart from P0 and reaching a terminal state is called an episode  hence the name of episodic problems . This way of generating a sample shall be called continual sampling with restarts from P0.  √ Being a standard linear SA method, the rate of convergence of tabular TD 0  will be of the usual order O 1  t    consult the paper by Tadi´c  2004  and the references therein for precise results . However, the constant factor in the rate will be largely inﬂuenced by the choice of the step-size sequence, the properties of the kernel P0 and the value of γ .  2.1.2 EVERY-VISIT MONTE-CARLO As mentioned before, one can also estimate the value of a state by computing sample means, giving rise to the so-called every visit Monte-Carlo method. Here we deﬁne more precisely what we mean by this and compare the resulting method to TD 0 .  To ﬁrm up the ideas, consider some episodic problem  otherwise, it is impossible to ﬁnitely compute the return of a given state since the trajectories are inﬁnitely long . Let the underlying MRP be M =  X ,P0  and let   Xt , Rt+1, Yt+1 ; t ≥ 0  be generated by continual sampling in M with restarts from some distribution P0 deﬁned over X . Let  Tk; k ≥ 0  be the sequence of times when an episode starts  thus, for each k, XTk is sampled from P0 . For a given time t, let k t   be the unique episode index such that t ∈ [Tk, Tk+1 . Let Rt = Tk t  +1−1 cid:5   γ s−t Rs+1   2.3   s=t   2.1. TEMPORAL DIFFERENCE LEARNING IN FINITE STATE SPACES 15  Algorithm 2 The function that implements the every-visit Monte-Carlo algorithm to estimate value functions in episodic MDPs. This routine must be called at the end of each episode with the state-reward sequence collected during the episode. Note that the algorithm as shown here has linear time- and space-complexity in the length of the episodes. function EveryVisitMC X0, R1, X1, R2, . . . , XT −1, RT , V   Input: Xt is the state at time t, Rt+1 is the reward associated with the tth transition, T is the length of the episode, V is the array storing the current value function estimate 1: sum ← 0 2: for t ← T − 1 downto 0 do sum ← Rt+1 + γ · sum t arget[Xt] ← sum V[Xt] ← V[Xt] + α ·  t arget[Xt] − V[Xt]   3: 4: 5: 6: end for 7: return V  cid:4   cid:3 RtXt = x denote the return from time t on until the end of the episode. Clearly, V  x  = E any state x such that P  Xt = x  > 0. Hence, a sensible way of updating the estimates is to use  , for  ˆVt+1 x  = ˆVt  x  + αt  Rt − ˆVt  x   I{Xt=x},  x ∈ X .  Monte-Carlo methods such as the above one, since they use multi-step predictions of the return  cf. Equation  2.3  , are called multi-step methods. The pseudo-code of this update-rule is shown as Algorithm 2. This algorithm is again an instance of stochastic approximation. As such, its behavior is governed by the ODE ˙v t   = V − v t  . Since the unique globally asymptotically stable equilibrium of this ODE is V , ˆVt again converges to V almost surely. Since both algorithms achieve the same goal, one may wonder which algorithm is better.   cid:17    cid:16  ˆVt  3   TD 0  or Monte-Carlo? First, let us consider an example when TD 0  converges faster. Consider the undiscounted episodic MRP shown on Figure 2.1. The initial states are either 1 or 2. With high probability the process starts at state 1, while the process starts at state 2 less frequently. Consider now how TD 0  will behave at state 2. By the time state 2 is visited the kth time, on the average state 3 has already been visited 10 k times. Assume that αt = 1  t + 1 . At state 3 the TD 0  update reduces to averaging the Bernoulli rewards incurred upon leaving state 3. At the kth visit of state 2, = V  3  = 0.5 . Thus, the target of the update of state 2 Var  cid:4  = 0.25, i.e., the variance of the will be an estimate of the true value of state 2 with accuracy increasing with k. Now, consider the Monte-Carlo method. The Monte-Carlo method ignores the estimate of the value of state 3 and uses the Bernoulli rewards directly. In particular, Var target does not change with time. On this example, this makes the Monte-Carlo method slower to converge, showing that sometimes bootstrapping might indeed help.   cid:3 RtXt = 2  ≈ 1  10 k   clearly, E   cid:16  ˆVt  3    cid:17    16  2. VALUE PREDICTION PROBLEMS   cid:5  cid:5    cid:1    cid:1  cid:1  cid:2  cid:3   cid:4   cid:5  cid:1  cid:6    cid:1    cid:8  cid:9  cid:10  cid:1  cid:5  cid:1  cid:11  cid:3    cid:2    cid:2    cid:1    cid:2  cid:2    cid:3  cid:3    cid:4  cid:4    cid:1    cid:1  cid:1  cid:7  cid:3   cid:4   cid:5  cid:1  cid:2    cid:1    cid:1    cid:1   Figure 2.1: An episodic Markov reward process. In this example, all transitions are deterministic. The reward is zero, except when transitioning from state 3 to state 4, when it is given by a Bernoulli random variable with parameter 0.5. State 4 is a terminal state. When the process reaches the terminal state, it is reset to start at state 1 or 2. The probability of starting at state 1 is 0.9, while the probability of starting at state 2 is 0.1.  To see an example when bootstrapping is not helpful, imagine that the problem is modiﬁed so that the reward associated with the transition from state 3 to state 4 is made deterministically equal to one. In this case, the Monte-Carlo method becomes faster since Rt = 1 is the true target value, while for the value of state 2 to get close to its true value, TD 0  has to wait until the estimate of the value at state 3 becomes close to its true value. This slows down the convergence of TD 0 . In fact, one can imagine a longer chain of states, where state i + 1 follows state i, for i ∈ {1, . . . , N} and the only time a nonzero reward is incurred is when transitioning from state N − 1 to state N. In this example, the rate of convergence of the Monte-Carlo method is not impacted by the value of N, while TD 0  would get slower with N increasing  for an informal argument, see Sutton, 1988; for a formal one with exact rates, see Beleznay et al., 1999 .  2.1.3 TD λ : UNIFYING MONTE-CARLO AND TD 0  The previous examples show that both Monte-Carlo and TD 0  have their own merits. Interestingly, there is a way to unify these approaches. This is achieved by the so-called TD λ  family of methods  Sutton, 1984, 1988 . Here, λ ∈ [0, 1] is a parameter that allows one to interpolate between the Monte-Carlo and TD 0  updates: λ = 0 gives TD 0   hence the name of TD 0  , while λ = 1, i.e., TD 1  is equivalent to a Monte-Carlo method. In essence, given some λ > 0, the targets in the   2.1. TEMPORAL DIFFERENCE LEARNING IN FINITE STATE SPACES 17  TD λ  update are given as some mixture of the multi-step return predictions  Rt:k = t+k cid:5   s=t  γ s−t Rs+1 + γ k+1 ˆVt  Xt+k+1 ,  where the mixing coefﬁcients are the exponential weights  1 − λ λk, k ≥ 0. Thus, for λ > 0 TD λ  will be a multi-step method. The algorithm is made incremental by the introduction of the so-called eligibility traces.  In fact, the eligibility traces can be deﬁned in multiple ways and hence TD λ  exists in correspondingly many multiple forms. The update rule of TD λ  with the so-called accumulating traces is as follows:  δt+1 = Rt+1 + γ ˆVt  Xt+1  − ˆVt  Xt  ,  zt+1 x  = I{x=Xt} + γ λ zt  x , ˆVt+1 x  = ˆVt  x  + αt δt+1 zt+1 x , z0 x  = 0,  x ∈ X .  Here zt  x  is the eligibility trace of state x. The rationale of the name is that the value of zt  x  modulates the inﬂuence of the TD error on the update of the value stored at state x. In another variant of the algorithm, the eligibility traces are updated according to  zt+1 x  = max I{x=Xt}, γ λ zt  x  ,  x ∈ X .   cid:2   This is called the replacing traces update. In these updates, the trace-decay parameter λ controls the amount of bootstrapping: When λ = 0 the above algorithms become identical to TD 0   since k≥0 λkRt:k = Rt:0 = Rt+1 + γ ˆVt  Xt+1  . When λ = 1, we get the TD 1  al- limλ→0+ 1 − λ  gorithm, which with accumulating traces will simulate the previously described every-visit Monte- Carlo algorithm in episodic problems.  For an exact equivalence, one needs to assume that the value updates happen only at the end of trajectories, up to which point the updates are just accumulated. The statement then follows because the discounted sum of temporal differences along a trajectory from a start state to a terminal state telescopes and gives the sum of rewards along the trajectory.  Replacing traces and λ = 1 correspond to a version of the Monte-Carlo algorithm where a state is updated only when it is encountered for the ﬁrst time in a trajectory. The corresponding algo- rithm is called ﬁrst-visit Monte-Carlo method. The formal correspondence between the ﬁrst-visit Monte-Carlo method and TD 1  with replacing traces is known to hold for the undiscounted case only  Singh and Sutton, 1996 . Algorithm 3 gives the pseudocode corresponding to the variant with replacing traces.  In practice, the best value of λ is determined by trial and error. In fact, the value of λ can be changed even during the algorithm, without impacting convergence. This holds for a wide range of other possible eligibility trace updates  for precise conditions, see Bertsekas and Tsitsiklis, 1996,   18  2. VALUE PREDICTION PROBLEMS  Algorithm 3 The function that implements the tabular TD λ  algorithm with replacing traces. This function must be called after each transition. function TDLambda X, R, Y, V , z  Input: X is the last state, Y is the next state, R is the immediate reward associated with this transition, V is the array storing the current value function estimate, z is the array storing the eligibility traces 1: δ ← R + γ · V[Y] − V[X] 2: for all x ∈ X do z[x] ← γ · λ · z[x] if X = x then z[x] ← 1  3: 4: 5: 6: 7: 8: end for 9: return  V , z   end if V[x] ← V[x] + α · δ · z[x]  Section 5.3.3 and 5.3.6 . The replacing traces version of the algorithm is believed to perform better in practice  for some examples when this happens, consult Sutton and Barto, 1998, Section 7.8 . It has been noted that λ > 0 is helpful when the learner has only partial knowledge of the state, or  in the related situation  when function approximation is used to approximate the value functions in a large state space – the topic of the next section.  In summary, TD λ  allows one to estimate value functions in MRPs. It generalizes Monte- Carlo methods, it can be used in non-episodic problems, and it allows for bootstrapping. Further, by appropriately tuning λ it can converge signiﬁcantly faster than Monte-Carlo methods or TD 0 .  2.2 ALGORITHMS FOR LARGE STATE SPACES When the state space is large  or inﬁnite , it is not feasible to keep a separate value for each state in the memory. In such cases, we often seek an estimate of the values in the form  Vθ  x  = θ   cid:2   ϕ x ,  x ∈ X ,  where θ ∈ Rd is a vector of parameters and ϕ : X → Rd is a mapping of states to d-dimensional vectors. For state x, the components ϕi  x  of the vector ϕ x  are called the features of state x and ϕ is called a feature extraction method. The individual functions ϕi : X → R deﬁning the components of ϕ are called basis functions.  Examples of function approximation methods Given access to the state, the features  or basis func- tions  can be constructed in a great many different ways. If x ∈ R  i.e., X ⊂ R  one may use a polynomial, Fourier, or wavelet basis up to some order. For example, in the case of a polynomial basis, ϕ x  =  1, x, x2, . . . , xd−1  , or, an orthogonal system of polynomials if a suitable measure   cid:2    2.2. ALGORITHMS FOR LARGE STATE SPACES 19   such as the stationary distribution  over the states is available.This latter choice may help to increase the convergence speed of the incremental algorithms that we will discuss soon.  In the case of multidimensional state spaces, the tensor product construction is a commonly used way to construct features given features of the states’ individual components. The ten- sor product construction works as follows: Imagine that X ⊂ X1 × X2 × . . . × Xk. Let ϕ i  : Xi → Rdi be a feature extractor deﬁned for the ith state component. The tensor product ϕ = ϕ 1  ⊗ . . . ⊗ ϕ k  feature extractor will have d = d1d2 . . . dk components, which can be conve- niently indexed using multi-indices of the form  i1, . . . , ik , 1 ≤ ij ≤ dj , j = 1, . . . , k. Then ϕ i1,...,ik   x  = ϕ 1   xk . When X ⊂ Rk, one particularly popular choice is to use radial basis function  RBF  networks, when ϕ i  xi   =  G xi − x 1      cid:2  . ∈ R  j = 1, . . . , di   is ﬁxed by the user and G is a suitable function. A typical choice Here x for G is G z  = exp −η z2   where η > 0 is a scale parameter. The tensor product construct in this cases places Gaussians at points of a regular grid and the ith basis function becomes   , . . . , G xi − x di     x2  . . . ϕ k  ik   x1 ϕ 2  i2   j   i  i1  i  i  ϕi  x  = exp −η cid:6 x − x i  cid:6 2 ,  d  d  θi  Vθ  x  =  where x i  ∈ X now denotes a point on a regular d1 × . . . × dk grid. A related method is to use kernel smoothing:  = d cid:5  i=1 θi si  x , where si ≥ 0 and   cid:2   cid:2  i=1 θi G  cid:6 x − x i  cid:6   j=1 G  cid:6 x − x j   cid:6   More generally, one may use Vθ  x  = cid:2    cid:2  G  cid:6 x − x i  cid:6   j=1 G  cid:6 x − x j   cid:6    2.4   cid:2  i=1 si  x  ≡ 1 holds for any x ∈ X . In this case, we say that Vθ is an averager. Averagers are important in reinforcement learning because the mapping θ  cid:24 → Vθ is a non-expansion in the max-norm, which makes them “well-behaved” when used together with approximate dynamic programming. An alternative to the above is to use binary features, i.e., when ϕ x  ∈ {0, 1}d. Binary Vθ  x  = cid:2  features may be advantageous from a computational point of view: when ϕ x  ∈ {0, 1}d then i:ϕi  x =1 θi. Thus, the value of state x can be computed at the cost of s additions if ϕ x  is s-sparse  i.e., if only s elements of ϕ x  are non-zero , provided that there is a direct way of computing the index of the non-zero components of the feature vector.  i=1  d  d  d  .   cid:2   This is the case when the state aggregation is used to deﬁne the features. In this case, the coordinate functions of ϕ  the individual features  correspond to indicators of non-overlapping regions of the state space X whose union covers X  i.e., the regions form a partition of the state space .Clearly,in this case, θ ϕ x  will be constant over the individual regions,thus state aggregation essentially “discretizes” the state space. A state aggregator function approximator is also an averager. Another choice that leads to binary features is tile coding  originally called CMAC, Albus, 1971, 1981 . In the simplest version of tile coding, the basis functions of ϕ correspond to indicator functions of multiple shifted partitions  tilings  of the state space: if s tilings are used, ϕ will be s-sparse. To make tile coding an effective function approximation method, the offsets of the tilings corresponding to different dimensions should be different.   20  2. VALUE PREDICTION PROBLEMS  ε  The curse of dimensionality The issue with tensor product constructions, state aggregation and straightforward tile coding is that when the state space is high dimensional they quickly become intractable: For example,a tiling of[0, 1]D with cubical regions with side-lengths of ε gives rise to d = −D-dimensional feature- and parameter-vectors. If ε = 1 2 and D = 100, we get the enormous number d ≈ 1030. This is problematic since state-representations with hundreds of dimensions are common in applications. At this stage, one may wonder if it is possible at all to successfully deal with applications when the state lives in a high dimensional space. What often comes at rescue is that the actual problem complexity might be much lower than what is predicted by merely counting the number of dimensions of the state variable  although, there is no guarantee that this happens . To see why sometimes this holds, note that the same problem can have multiple representations, some of which may come with low-dimensional state variables, some with high. Since, in many cases, the state-representation is chosen by the user in a conservative fashion, it may happen that in the chosen representation many of the state variables are irrelevant. It may also happen that the states that are actually encountered lie on  or lie close to  a low-dimensional submanifold of the chosen high dimensional “state-space”.  To illustrate this, imagine an industrial robot arm with say 3 joints and 6 degrees of freedom. The intrinsic dimensionality of the state is then 12, twice the number of degrees of freedom of the arm since the dynamics is second-order. One  approximate  state representation is to take high resolution camera images of the arm in close succession  to account for the dynamics  from multiple angles  to account for occlusions . The dimensionality of the chosen state representation will easily be in the range of millions, yet the intrinsic dimensionality will still be 12. In fact, the more cameras we have, the higher the dimensionality will be. A simple-minded approach, which aims for minimizing the dimensionality would suggest to use as few cameras as possible. But more information should not hurt! Therefore, the quest should be for clever algorithms and function approximation methods that can deal with high-dimensional but low complexity problems.  Possibilities include using strip-like tilings combined with hash functions, interpolators that use low-discrepancy grids  Lemieux, 2009, Chapter 5 and 6 , or random projections  Dasgupta and Freund, 2008 . Nonlinear function approximation methods  examples of which in- clude neural networks with sigmoidal transfer functions in the hidden layers or RBF networks where the centers are also considered as parameters  and nonparametric techniques also hold great promise.  Nonparametric methods In a nonparametric method, the user does not start with a ﬁxed ﬁnite- dimensional representation, such as in the previous examples, but allows for the representation to grow and change as needed. For example, in a k-nearest neighbor method for regression, given the data Dn = [ x1, v1 , . . . ,  xn, vn ], where xi ∈ Rk, vi ∈ R, the value at location x is predicted using  V  k D  x  = n cid:5   K  k D  x, xi    ,  k  vi  i=1    cid:17    cid:17     is one when x  2.2. ALGORITHMS FOR LARGE STATE SPACES 21 otherwise. Note that k = cid:2  is closer to x then the kth closest neighbor of x in D and is zero where K  k D  x, x j=1 K  k D  x, xj  . Replacing k in the above expression with this sum and replacing K  k D  x,·  with some other data based kernel KD  e.g., a Gaussian centered around x with standard deviation proportional to the distance to the kth nearest neighbor , we arrive at nonparametric kernel smoothing:  n  V  k D  x  = n cid:5   vi  i=1   cid:2  KD x, xi   j=1 KD x, xj    n  ,  which should be compared to its parametric counterpart  2.4 . Other examples include methods that work by ﬁnding an appropriate function in some large  inﬁnite dimensional  function space that ﬁts an empirical error. The function space is usually a Reproducing Kernel Hilbert space, which is a convenient choice from the point of view of optimization. In special cases, we get spline smoothers  Wahba, 2003  and Gaussian process regression  Rasmussen and Williams, 2005 . Another idea is to split the input space recursively into ﬁner regions using some heuristic criterion and then predict with some simple method the values in the leafs, leading to tree-based methods.The border between parametric and nonparametric methods is blurry. For example, a linear predictor when the number of basis functions is allowed to change  i.e.,when new basis functions are introduced as needed  becomes a nonparametric method. Thus, when one experiments with different feature extraction methods, from the point of view of the overall tuning process, we can say that one really uses a nonparametric technique. In fact, if we take this viewpoint, it follows that in practice “true” parametric methods are rarely used if they are used at all.  The advantage of nonparametric methods is their inherent ﬂexibility. However, this comes usually at the price of increased computational complexity. Therefore, when using nonparametric methods, efﬁcient implementations are important  e.g., one should use k-D trees when implement- ing nearest neighbor methods, or the Fast Gaussian Transform when implementing a Gaussian smoother . Also, nonparametric methods must be carefully tuned as they can easily overﬁt or under- ﬁt. For example, in a k-nearest neighbor method if k is too large, the method is going to introduce too much smoothing  i.e., it will underﬁt , while if k is too small, it will ﬁt to the noise  i.e., overﬁt . Overﬁtting will be further discussed in Section 2.2.4. For more information about nonparametric regression, the reader is advised to consult the books by Härdle  1990 ; Györﬁ et al.  2002 ;Tsybakov  2009 .  Although our discussion below will assume a parametric function approximation method  and in many cases linear function approximation , many of the algorithms can be extended to nonparametric techniques. We will mention when such extensions exist as appropriate.  Up to now, the discussion implicitly assumed that the state is accessible for measurement. This is, however, rarely the case in practical applications. Luckily, the methods that we will discuss below do not actually need to access the states directly, but they can perform equally well when some “sufﬁciently descriptive feature-based representation” of the states is available  such as the camera images in the robot-arm example . A common way of arriving at such a representation is to construct   22  2. VALUE PREDICTION PROBLEMS  Algorithm 4 The function implementing the TD λ  algorithm with linear function approximation. This function must be called after each transition. function TDLambdaLinFApp X, R, Y, θ, z  Input: X is the last state, Y is the next state, R is the immediate reward associated with this transition, θ ∈ Rd is the parameter vector of the linear function approximation, z ∈ Rd is the vector of eligibility traces 1: δ ← R + γ · θ ϕ[Y] − θ 2: z ← ϕ[X] + γ · λ · z 3: θ ← θ + α · δ · z 4: return  θ, z   ϕ[X]   cid:2    cid:2   state estimators  or observers, in control terminology  based on the history of the observations, which has a large literature both in machine learning and control. The discussion of these techniques, however, lies outside of the scope of the present paper.  2.2.1 TD λ  WITH FUNCTION APPROXIMATION Let us return to the problem of estimating a value function V of a Markov reward process M =  X ,P0 , but now assume that the state space is large  or even inﬁnite . Let D =   Xt , Rt+1 ; t ≥ 0  be a realization of M. The goal, as before, is to estimate the value function of M given D in an incremental manner. Choose a smooth parametric function-approximation method  Vθ; θ ∈ Rd   goal is to approx- imate the value function V underlying M.  i.e., for any θ ∈ Rd, Vθ : X → R is such that ∇θ Vθ  x  exists for any x ∈ X  .The generalization of tabular TD λ  with accumulating eligibility traces to the case when the value functions are approximated using members of  Vθ; θ ∈ Rd   uses the following updates  Sutton, 1984, 1988 :  δt+1 = Rt+1 + γ Vθt  Xt+1  − Vθt  Xt  , zt+1 = ∇θ Vθt  Xt   + γ λ zt , θt+1 = θt + αt δt+1 zt+1, z0 = 0.   2.5   To see that this algorithm is indeed a generalization of tabular TD λ  assume that X = ϕ x  with ϕi  x  = I{x=xi}. Note that since Vθ is linear in the ϕ , it holds that ∇θ Vθ = ϕ. Hence, identifying zt,i  θt,i  with zt  xi    resp.,  Here zt ∈ Rd. Algorithm 4 shows the pseudocode of this algorithm. {x1, . . . , xD} and let Vθ  x  = θ parameters  i.e., Vθ = θ ˆVt  xi    we see that the update  2.5 , indeed, reduces to the previous one. In the off-policy version of TD λ , the deﬁnition of δt+1 becomes   cid:2    cid:2   δt+1 = Rt+1 + γ Vθt  Yt+1  − Vθt  Xt  .   2.2. ALGORITHMS FOR LARGE STATE SPACES 23  Unlike the tabular case, under off-policy sampling, convergence is no longer be guaranteed, but, in fact, the parameters may diverge  see, e.g., Bertsekas and Tsitsiklis, 1996, Example 6.7, p. 307 . This is true for linear function approximation when the distributions of  Xt; t ≥ 0  do not match the stationary distribution of the MRP M. Another case when the algorithm may diverge is when it is used with a nonlinear function-approximation method  see, e.g., Bertsekas and Tsitsiklis, 1996, Example 6.6, p. 292 . For further examples of instability, see Baird  1995 ; Boyan and Moore  1995 . On the positive side, almost sure convergence can be guaranteed when  i  a linear function- approximation method is used with ϕ : X → Rd;  ii  the stochastic process  Xt; t ≥ 0  is an ergodic Markov process whose stationary distribution μ is the same as the stationary distribution of the MRP M; and  iii  the step-size sequence satisﬁes the RM conditions  Tsitsiklis and Van Roy, 1997; Bertsekas and Tsitsiklis, 1996, p. 222, Section 5.3.7 . In the results cited, it is also assumed that the components of ϕ  i.e., ϕ1, . . . , ϕd  are linearly independent. When this holds, the limit of the parameter vector will be unique. In the other case, i.e., when the features are redundant, the parameters will still converge, but the limit will depend on the parameter vector’s initial value. However, the limiting value function will be unique  Bertsekas, 2010 .  Assuming that TD λ  converges, let θ  λ . denote the limiting value of θt. Let  F = {Vθ  θ ∈ Rd}  be the space of functions that can be represented using Vθ . Note that F is a linear subspace of the vector space of all real-valued functions with domain X . The limit θ  λ  is known to satisfy the so-called projected ﬁxed-point equation  Vθ  λ  =  cid:7 F ,μ T  λ Vθ  λ  ,  where the operators T  λ  and  cid:7 F ,μ are deﬁned as follows: For m ∈ N let T Bellman operator:  [m]  [m] ˆV  x  = E  T   cid:7   m cid:5  t=0   cid:8  cid:8  cid:8  X0 = x   cid:9   .  γ t Rt+1 + γ m+1 ˆV  Xm+1  [m]  Clearly, V , the value function to be estimated is a ﬁxed point of T λ < 1. Then, operator T  λ  is deﬁned as the exponentially weighted average of T  [0]  [1]  , T  , . . .:  for any m ≥ 0. Assume that   2.6   be the m-step lookahead  T  λ  ˆV  x  =  1 − λ   [m] ˆV  x .  λm T  ∞ cid:5  m=0  For λ = 1, we let T  1  ˆV = limλ→1− T  λ  ˆV = V . Notice that for λ = 0, T  0  = T . Operator  cid:7 F ,μ is a projection: It projects functions of states to the linear space F with respect to the weighted 2-norm  cid:6 f cid:6 2  = cid:2   x∈X f 2 x μ x :  μ   cid:7 F ,μ  ˆV = argmin f∈F   cid:6  ˆV − f cid:6 μ.   24  2. VALUE PREDICTION PROBLEMS  The essence of the proof of convergence of TD λ  is that the composite operator  cid:7 F ,μT  λ  is a contraction with respect to the norm  cid:6 · cid:6  μ. This result heavily exploits that μ is the stationary distribution underlying M  which deﬁnes T  λ  . For other distributions, the composite operator might not be a contraction; in which case, TD λ  might diverge.  As to the quality of the solution found, the following error bound holds for the ﬁxed point  of  2.6 :   cid:18  cid:18    cid:18  cid:18   Vθ  λ  − V  ≤  μ   cid:18  cid:18   1 cid:19  1 − γλ   cid:18  cid:18   μ .   cid:7 F ,μV − V  = γ  1 − λ   1 − λγ   is the contraction modulus of  cid:7 F ,μT  λ   Tsitsiklis and Van Roy, Here γλ 1999a; Bertsekas, 2007b .  For sharper bounds, see Yu and Bertsekas 2008; Scherrer 2010.  From the error bound we see that Vθ  1  is the best approximation to V within F with respect to the norm  cid:6 · cid:6  μ  this should come at no surprise as TD 1  minimizes this mean-squared error by design . We also see that as we let λ → 0 the bound allows for larger errors. It is known that this is not an artifact of the analysis. In fact, in Example 6.5 of the book by Bertsekas and Tsitsiklis  1996   p. 288 , a simple MRP with n states and a one-dimensional feature extractor ϕ is given such that Vθ  0  is a very poor approximation to V , while Vθ  1  is a reasonable approximation. Thus, in order to get good accuracy when working with λ < 1, it is not enough to choose the function space F so that the best approximation to V has small error. At this stage, however, one might wonder if using λ < 1 makes sense at all. A recent paper by Van Roy  2006  suggests that when considering performance loss bounds instead of approximation errors and the full control learning task  cf. Section 3 , λ = 0 will in general be at no disadvantage compared to using λ = 1, at least, when state-aggregation is considered. Thus, while the mean-squared error of the solution might be large, when the solution is used in control, the performance of the resulting policy will still be as good as that of one that is obtained by calculating the TD 1  solution. However, the major reason to prefer TD λ  with λ < 1 over TD 1  is because empirical evidence suggests that it converges much faster than TD 1 , the latter of which, at least for practical sample sizes, often produces very poor estimates  e.g., Sutton and Barto, 1998, Section 8.6 .  TD λ  solves a model Sutton et al.  2008  and Parr et al.  2008  observed independently of each other that the solution obtained by TD 0  can be thought of as the solution of a deterministic MRP with a linear dynamics. In fact, as we will argue now this also holds in the case of TD λ . This suggests that if the deterministic MRP captures the essential features of the original MRP, Vθ  λ  will be a good approximation to V . To ﬁrm up this statement, following Parr et al.  2008 , let us study the Bellman error   cid:13  λ   ˆV   = T  λ  ˆV − ˆV   cid:18  cid:18  cid:18 V − ˆV of ˆV : X → R under T  λ . Note that  cid:13  λ   ˆV   : X → R. A simple contraction argument shows that   cid:18  cid:18  cid:18 ∞. Hence, if  cid:13  λ   ˆV   is small, ˆV is close to V .   cid:18  cid:18  cid:18 ∞ ≤ 1   cid:18  cid:18  cid:18  cid:13  λ   ˆV    1−γ   2.2. ALGORITHMS FOR LARGE STATE SPACES 25  The following error decomposition can be shown to hold:2   cid:5  m≥0  [r]   cid:13  λ  Vθ  λ    =  1 − λ  [ϕ] [r] m = r m −  cid:7 F ,μr m and  cid:13  m = P m+1ϕ  cid:4   λm cid:13    cid:3   m  ⎫⎬ ⎭ θ  λ .  [ϕ]  m  λm cid:13   ⎧⎨  cid:5  ⎩ 1 − λ  m≥0  cid:2  −  cid:7 F ,μP m+1ϕ  cid:2   + γ  Rm+1  X0 = x  Here  cid:13  are the errors of modeling the m-step rewards and transitions with respect to the features ϕ, respectively; r m : X → R is deﬁned by r m x  = E and P m+1ϕ  cid:2  denotes a function that maps states to d-dimensional  x  =  P m+1ϕ1 x , . . . , P m+1ϕd  x  . Here P mϕi : row-vectors and which is deﬁned by P m+1ϕ  cid:2  X → R is the function deﬁned by P mϕi  x  = E [ϕi  Xm  X0 = x]. Thus, we see that the Bellman error will be small if the m-step immediate rewards and the m-step feature-expectations are well captured by the features. We can also see that as λ gets closer to 1, it becomes more important for the features to capture the structure of the value function, and as λ gets closer to 0, it becomes more important to capture the structure of the immediate rewards and the immediate feature-expectations. This suggests that the “best” value of λ  i.e., the one that minimizes  cid:6  cid:13  λ  Vθ  λ    cid:6   may depend on whether the features are more successful at capturing the short-term or the long-term dynamics  and rewards .  2.2.2 GRADIENT TEMPORAL DIFFERENCE LEARNING In Section 2.2.3, we will see some methods using which the issue of divergence of TD λ  can be avoided. However, the computational  time and storage  complexity of these methods is signiﬁcantly larger than that ofTD λ .In this section,we present two recent algorithms introduced by Sutton et al.  2009b,a , which also overcome the instability issue, converge to the TD λ  solutions in the on-policy case, and yet they are almost as efﬁcient as TD λ . For simplicity, we consider the case when λ = 0,   Xt , Rt+1, Yt+1 ; t ≥ 0  is a stationary process, Xt ∼ ν   ν can be different from the stationary distribution of P  and when linear function approximation is used with linearly independent features. Assume that θ  0 , the solution to  2.6 , exists. Consider the objective function  J  θ   = cid:18  cid:18   Vθ −  cid:7 F ,ν T Vθ   cid:18  cid:18 2  ν .   2.7   Notice that all solutions to  2.6  are minimizers of J , and there are no other minimizers of J when  2.6  has solutions. Thus, minimizing J will give a solution to  2.6 . Let θ∗ denote a minimizer of J . Since, by assumption, the features are linearly independent, the minimizer of J is unique, i.e., θ∗ is well-deﬁned. Introduce the shorthand notations = Rt+1 + γ θ ϕt = ϕ Xt  , = ϕ Yt+1 .  cid:17  t+1  δt+1 θ   = Rt+1 + γ Vθ  Yt+1  − Vθ  Xt     cid:17  t+1 ϕ  − θ   2.8   ϕt ,   cid:2    cid:2   ϕ  2Parr et al.  2008  observed this for λ = 0. The extension to λ > 0 is new.   26  2. VALUE PREDICTION PROBLEMS  Algorithm 5 The function implementing the GTD2 algorithm. This function must be called after each transition. function GTD2 X, R, Y, θ, w  Input: X is the last state, Y is the next state, R is the immediate reward associated with this transition, θ ∈ Rd is the parameter vector of the linear function approximation, w ∈ Rd is the auxiliary weight 1: f ← ϕ[X]  cid:17  ← ϕ[Y] 3: δ ← R + γ · θ  cid:17  − θ f 4: a ← f 5: θ ← θ + α ·  f − γ · f  cid:17  6: w ← w + β ·  δ − a  · f 7: return  θ, w     · a  2: f   cid:2    cid:2    cid:2   w  f  A simple calculation allows us to rewrite J in the following form:   cid:3   J  θ   = E  δt+1 θ  ϕt   cid:16    cid:17 −1   cid:2    cid:3   E  ϕt ϕ t  E   cid:4   .  Taking the gradient of the objective function we get   cid:4  cid:2   cid:16   δt+1 θ  ϕt  cid:17   w θ  ,  ∇θ J  θ   = −2E  cid:16   w θ   = E   cid:2   t   ϕt − γ ϕ  cid:17  t+1 ϕ  cid:17 −1  cid:3    cid:2    cid:4   ϕt ϕ t  E  δt+1 θ  ϕt  .  where   2.9    2.10   Let us now introduce two sets of weights: θt to approximate θ∗ and wt to approximate w θ∗ . In GTD2  “gradient temporal difference learning, version 2” , the update of θt is chosen to follow the negative stochastic gradient of J based on  2.10  assuming that wt ≈ w θt  , while the update of wt is chosen so that for any ﬁxed θ, wt would converge almost surely to w θ  :  θt+1 = θt + αt  ϕt − γ ϕ  cid:2   cid:17  t+1  ϕ t wt , wt+1 = wt + βt  δt+1 θt   − ϕ  cid:2  t wt   ϕt .  Here  αt; t ≥ 0 ,  βt; t ≥ 0  are two step-size sequences. Note that the update equation for  wt; t ≥ 0  is just the basic Least-Mean Square  LMS  rule, which is a widely used update rule in signal processing  Widrow and Stearns, 1985 . Sutton et al.  2009a  have shown that under the standard RM conditions on the step-sizes and some other mild technical conditions  θt   converges to the minimizer of J  θ  , almost surely. However, unlike for TD 0 , convergence is guaranteed independently of the distribution of  Xt; t ≥ 0 . At the same time, the update of GTD2 costs only twice as much as the cost of TD 0 . Algorithm 5 shows the pseudocode of GTD2.   To arrive at the second algorithm called TDC  “temporal difference learning with correc-  2.2. ALGORITHMS FOR LARGE STATE SPACES 27  tions” , write the gradient as  Leaving the update wt unchanged, we then arrive at,   cid:20    cid:3    cid:16    cid:4  − γ E   cid:17    cid:21    cid:2    cid:17  t+1ϕ  t  w θ    .  ∇θ J  θ   = −2  E  ϕ  δt+1 θ  ϕt  cid:20   cid:20  θt+1 = θt + αt δt+1 θt  ϕt − γ ϕ δt+1 θt   − ϕ wt+1 = wt + βt  cid:2  t wt   cid:21   ,   cid:2  t wt   cid:21   cid:17  t+1 ϕ ϕt .  The pseudocode of this update is identical to that of GTD2 except that line 5 should be replaced by  θ ← θ + α ·  δ · f − γ · a · f   cid:17    .  In TDC, the update of wt must use larger step-sizes than the update of θt: αt = o βt  . This makes TDC a member of the family of the so-called two-timescale stochastic approximation algorithms  Borkar, 1997, 2008 . If, in addition to this condition, the standard RM conditions are also satisﬁed by both step-size sequences, θt → θ∗ holds again almost surely  Sutton et al., 2009a . More recently, these algorithms have been extended to nonlinear function approximation  Maei et al., 2010a . Also, one can show that it sufﬁces if αt  cid:25  βt  Maei, 2010, personal communication . The algorithms can also be extended to use eligibility traces  Maei and Sutton, 2010 .  Note that although these algorithms are derived from the gradient of an objective function, they are not true stochastic gradient methods in the sense that the expected weight update direction can be different from the direction of the negative gradient of the objective function. In fact, these methods belong to the larger class of pseudo-gradient methods.The two methods differ in how they approximate the gradients, and it remains to be seen whether one of them is better than the other.  2.2.3 LEAST-SQUARES METHODS The methods discussed so far are similar to the LMS algorithm of adaptive ﬁltering in that they are taking small steps in the parameter space following some noisy, gradient-like signal. As such, similarly to the LMS algorithm, they are sensitive to the choice of the step-sizes, the distance between the initial parameter and the limit point θ  λ , or the eigenvalue structure of the matrix A that determines the dynamics of updates  e.g., for TD 0 , A = E  . Over the years, many ideas appeared in the literature to address these issues. These are essentially parallel to those available in the adaptive ﬁltering literature. A non-exhaustive list includes the use of adaptive step-sizes  Sutton, 1992; George and Powell, 2006 , normalizing the updates  Bradtke, 1994  or reusing previous samples  Lin, 1992 . Although these techniques can indeed help, each have their own weaknesses. In adaptive ﬁltering, the algorithm that is known to address all the deﬁciencies of LMS is known as the LS  “least-squares”  algorithm. In this section, we review the analogous methods of reinforcement learning.  ϕt  ϕt − γ ϕ   cid:17  t+1    cid:2  cid:4    cid:3    28  2. VALUE PREDICTION PROBLEMS  LSTD: Least-squares temporal difference learning TD 0  ﬁnds a parameter vector θ that satisﬁes   cid:3   E  ϕt δt+1 θ     cid:4  = 0,  In the limit of an inﬁnite number of examples,   2.11   where we used the notation of the previous section. Given a ﬁnite sample  Dn =   X0, R1, Y1 ,  X1, R2, Y2 , . . . ,  Xn−1, Rn, Yn  ,  1 n  one can approximate  2.11  by  n−1 cid:5  ϕt δt+1 θ   = 0. t=0  cid:2  Plugging in δt+1 θ   = Rt+1 −  ϕt − γ ϕ  cid:17   cid:2  t+1  if the matrix ˆAn = 1 n−1 t=0 ϕt  ϕt − γ ϕ  cid:17   cid:2  t+1  θn = ˆA −1  cid:2  t=0 Rt+1ϕt. If inverting ˆAn can be afforded  i.e., the dimensionality of the features n−1  where ˆbn = 1 is not too large and the method is not called too many times  then this method can give a better approximation to the equilibrium solution than TD 0  or some other incremental ﬁrst-order method since the latter are negatively impacted by the eigenvalue spread of the matrix E  θ, we see that this equation is linear in θ. In particular, is non-singular, the solution is simply   cid:16  ˆAn  ˆbn,   2.13    2.12    cid:17   .  n  n  n  The idea of directly computing the solution of  2.12  is due to Bradtke and Barto  1996 , who call the resulting algorithm least-squares temporal difference learning or LSTD. Using the terminology of stochastic programming, LSTD can be seen to use sample average approximation  Shapiro, 2003 . In the terminology of statistics, it belongs to the so-called Z-estimation family of procedures  e.g., Kosorok, 2008, Section 2.2.5 . It is a simple observation that when the LSTD solution exists, LSTD minimizes the empirical approximation to the projected squared Bellman error, μ, over the linear space F  Antos et al., 2008 .   cid:7 F ,μ T V − V     cid:18  cid:18 2   cid:18  cid:18   Using the Sherman-Morrison formula, one can derive an incremental version of LSTD, analogously to how the recursive least-squares  RLS  method is derived in adaptive ﬁltering Widrow and Stearns  1985 . The resulting algorithm is called “recursive LSTD”  RLSTD  and works as follows  Bradtke and Barto, 1996 : Choose θ0 ∈ Rd and let C0 ∈ Rd×d such that C0 is a “small” positive deﬁnite matrix  e.g., C0 = βI, for β > 0 “small” . Then, for t ≥ 0,  Ct+1 = Ct − Ct ϕt  ϕt − γ ϕ  cid:17  t+1  1 +  ϕt − γ ϕ  cid:17   cid:2  t+1  θt+1 = θt + 1 +  ϕt − γ ϕ  cid:17   cid:2  t+1   Ct   cid:2   Ct  Ct ϕt  ,  Ct ϕt  δt+1 θt  ϕt .  The computational complexity of one update is O d2 . Algorithm 6 shows the pseudocode of this algorithm.   2.2. ALGORITHMS FOR LARGE STATE SPACES 29  Algorithm 6 The function implementing the RLSTD algorithm. This function must be called after each transition. Initially, C should be set to a diagonal matrix with small positive diagonal elements: C = β I, with β > 0. function RLSTD X, R, Y, C, θ  Input: X is the last state, Y is the next state, R is the immediate reward associated with this transition, C ∈ Rd×d, and θ ∈ Rd is the parameter vector of the linear function approximation 1: f ← ϕ[X]  cid:17  ← ϕ[Y] 3: g ←  f − γf 4: a ← 1 + gf 5: v ← Cf 6: δ ← R + γ · θ  cid:2  7: θ ← θ + δ   a · v 8: C ← C − v g   a 9: return  C, θ     cid:28  g is a 1 × d row vector   cid:17  − θ  2: f   cid:2    cid:2   C  f  f      cid:17   Boyan  2002  extended LSTD to incorporate the λ parameter of TD λ  and called the re- sulting algorithm LSTD λ .  Note that for λ > 0 to make sense one needs Xt+1 = Yt+1; otherwise, the TD errors do not telescope . The LSTD λ  solution is derived from  2.5 . It is deﬁned as the parameter value that makes the cumulated updates zero:  n−1 cid:5  t=0  1 n  δt+1 θ  zt+1 = 0,   2.14   where zt+1 = cid:2   t  s=0 γ λ t−s ϕs are the eligibility traces. This is again linear in θ and the previous comments apply. The recursive form of LSTD λ , RLSTD λ , has been studied by Xu et al.  2002  and  independently  by Nediˇc and Bertsekas  2003 .  See Algorithm 16 for the pseudocode of a closely related algorithm.   One issue with LSTD λ  as stated here that Equation  2.14  might fail to have a solution. In the on-policy case, for large enough sample sizes at least, a solution will always exist. When a solution does not exist, a commonly suggested trick is to add a small positive diagonal matrix to the matrix to be inverted  this corresponds to starting with a diagonal matrix in RLSTD . However, this trick is not guaranteed to work. A better approach is based on the observation that when the matrix is invertible then the LSTD parameter vector is a minimizer of the projected Bellman error. Since the minimizer of the projected Bellman error is always well deﬁned, instead of the solving for the zero of  2.14  , one can aim for minimizing the projected Bellman error.  Under standard assumptions on the sample, it follows from the law of large numbers and a simple continuity argument that LSTD λ   and its recursive variants  converge almost surely to the solution of the projected ﬁxed-point equation  2.6  if this solution exists. This was formally shown   30  2. VALUE PREDICTION PROBLEMS  for λ = 0 by Bradtke and Barto  1996 , and for λ > 0 by Xu et al.  2002  and Nediˇc and Bertsekas  2003 . Although these results were shown only for the on-policy case, it is easy to see that they also hold in the off-policy case provided that the limiting solution exists.  As promised,  R LSTD λ  avoids the difﬁculties associated with tuning the incremental algorithms: It neither relies on step-sizes, nor is it sensitive to the eigenvalue structure of A, or the choice of the initial value of θ. Experimental results by Bradtke and Barto  1996 ; Boyan  2002 ; Xu et al.  2002  and others have indeed conﬁrmed that the parameters obtained using  R LSTD λ  converge faster than those obtained by TD λ . However, their computational properties are quite different from those of TD λ . We will discuss the implications of this after we have reviewed the LSPE algorithm.  LSPE: Least-squares policy evaluation An alternative to LSTD  and LSTD λ   is λ-least squares policy evaluation  λ-LSPE for short  due to Bertsekas and Ioffe  1996 . The basic idea of this al- gorithm is to mimic multi-step value iteration. Again, the method assumes that linear function- approximation is used.  It works as follows. Deﬁne the  n − s -step prediction of the value of Xs as  and deﬁne the loss  ˆV  λ  s,n  θ   = θ   cid:2   Jn ˆθ , θ   = 1  ϕs + n−1 cid:5  q=s  cid:20 ˆθ n−1 cid:5  s=0  n   γ λ q−s δq+1 θ    cid:21 2  ϕs − ˆV  λ   s,n  θ     cid:2   .  Then, λ-LSPE updates the parameters by  Jnt  ˆθ , θt   − θt  ,  θt+1 = θt + αt  argminˆθ   2.15  where  αt; t ≥ 0  is a step-size sequence and  nt; t ≥ 0  is a non-decreasing sequence of integers.  Bertsekas and Ioffe  1996  only considered the case when nt = t, which is a logical choice when the algorithm is used in an online learning scenario. When the algorithm is used with a ﬁnite  say, n  observations, we can set nt = n or nt = min n, t  .  Note that Jn is quadratic in ˆθ, hence the solution to the minimization problem can be obtained in closed form. The resulting algorithm is shown as Algorithm 7. A recursive, incremental version of λ-LSPE is also available. Similarly to LSTD λ , it requires O d2  operations per time step when nt = t. To get a sense of the behavior of λ-LSPE, consider the update in the special case when λ = 0 and αt = 1:  θt+1 = argminˆθ  1 nt   cid:22 ˆθ  nt−1 cid:5  s=0   cid:2   ϕ Xs   −  Rs+1 + γ Vθt  Ys+1     cid:23 2  .   2.2. ALGORITHMS FOR LARGE STATE SPACES 31  Algorithm 7 The function implementing the batch-mode λ-LSPE update. This function must be called repeatedly until convergence. function LambdaLSPE D, θ  Input: D =   Xt , At , Rt+1, Yt+1 ; t = 0, . . . , n − 1  is a list of transitions, θ ∈ Rd is the parameter vector 1: A, b, δ ← 0  cid:28  A ∈ Rd×d, b ∈ Rd, δ ∈ R 2: for t = n − 1 downto 0 do f ← ϕ[Xt] δ ← γ · λ · δ +  cid:24  3: v ← θ 4: 5: b ← b +  v + δ  · f 6: A ← A + f · f 7: 8: end for  cid:17  ← A 9: θ 10: θ ← θ + α ·  θ 11: return θ  Rt+1 + γ · θ  ϕ[Yt+1] − v   cid:17  − θ    −1b   cid:25    cid:2    cid:2    cid:2   f   cid:3    cid:4   Rs+1 + γ Vθt  Ys+1 Xs = x  Thus, in this case, λ-LSPE solves a linear regression problem, implementing the so-called ﬁtted value iteration algorithm for policy evaluation with linear function approximation. For a ﬁxed, non- random value of θt, the true regression function underlying the above least-squares problem is , which is just T Vθt  x . Thus, if the function space F is rich enough E  cid:2  t+1ϕ to be close to T Vθt  x , and we see that the and the sample size nt is large, one may expect θ algorithm implements value iteration in an approximate manner. The case when λ > 0 can be given a similar interpretation. When αt < 1, the parameters are moved towards the minimizer of Jnt  ·, θt   in proportion to the size of αt. The role of smoothing the updates this way is  i  to stabilize the parameters for small sample sizes  i.e., when nt and d are in the same range  and  ii  to ensure that policies are changed gradually when the algorithm is used as a subroutine of a control algorithm  cf. Section 3 . The idea of smoothing the parameter updates could also be used together with LSTD. Just like LSTD λ ,the multi-step version of λ-LSPE  i.e.,when λ > 0  requires Xt+1 = Yt+1. The parameter λ plays a role similar to its role in other TD methods: Increasing λ is expected to reduce bias and increase variance, though unlike TD λ , λ-LSPE bootstraps even when λ = 1. However, the effect of bootstrapping is diminishing with nt → ∞. Under standard assumptions on the sample and when nt = t, λ-LSPE is known to con- verge almost surely to the solution of the projected ﬁxed-point equation  2.6 , both for decreasing  Nediˇc and Bertsekas, 2003  and constant step-sizes  Bertsekas et al., 2004 . In the latter case, con- vergence is guaranteed if 0 < αt ≡ α <  2 − 2γ λ   1 + γ − 2γ λ . Note that 1 is always included in this range.   32  2. VALUE PREDICTION PROBLEMS  According to Bertsekas et al.  2004  λ-LSPE is competitive with LSTD in the sense that the distance between the parameters updated by LSTD λ  and λ-LSPE becomes, very soon, smaller than the statistical inaccuracy, resulting from the use of a ﬁnite sample. Experimental results obtained by Bertsekas et al.  2004  and earlier by Bertsekas and Ioffe  1996  to train a Tetris playing program indicate that λ-LSPE is, indeed, a competitive algorithm. Moreover, λ-LSPE is always well-deﬁned  all inverses involved exist in the limit or with appropriate initialization , whereas LSTD λ  might be ill-deﬁned in off-policy settings.  Comparing least-squares and TD-like methods. The price of the increased stability and accuracy of least-squares techniques is their increased computational complexity. In particular, for a sample of size n, the complexity of a straightforward implementation of LSTD is O nd2 + d3 , while the complexity of RLSTD is O nd2   the same applies to LSPE . For comparison, the computational complexity of the lightweight, incremental methods discussed previously is only O nd   or less when the features are sparse . Thus, the lightweight algorithms can do d passes on the sample while a least-squares method makes a single pass.The trick of saving and reusing the observations to increase the accuracy of TD-based algorithms was ﬁrst suggested by Lin  1992 , who dubbed his method “experience replay”. When the value of d is large, this may be enough for the lightweight methods to perform as well as the least-squares methods given the same computation time. When d is very large, the least-squares methods might not be feasible at all. For example, Silver et al.  2007  use over a million features when building a value function for the game of Go. When d is in this range, least-squares methods are not feasible.  It becomes very complicate to compare these approaches if we take into account the frequency at which the observations arrive, the storage space available, the access time of storage, etc. Hence, here we look at one interesting case when new observations are available at negligible cost. In this case, there is no need to  store and  reuse data and the quality of solutions will depend on the methods’ computation speed. To compare the two approaches,ﬁx some time T available for computation.In time T ,the least- squares methods are limited to process a sample of size n ≈ T  d2, while the lightweight methods  cid:17  ≈ nd. Let us now look at the precision of the resulting parameters. can process a sample of size n Assume that the limit of the parameters is θ∗. Denote by θt the parameter obtained by  say  LSTD  cid:17  after processing t observations and denote by θ t the parameter obtained by a TD-method. Then, one expects that  cid:6 θt − θ∗ cid:6  ≈ C1t − θ∗ cid:6  ≈ C2t 2 and  cid:6 θ − 1  cid:17  − θ∗ cid:6   cid:6 θ  cid:6 θn − θ∗ cid:6  ≈ C2  2 . Thus,  − 1 2 .   2.16   − 1  C1  d   cid:17    cid:17   n  t  Hence, if C2 C1 < d1 2 then the lightweight TD-like method will achieve a better accuracy, while in the opposite case the least-squares procedures will perform better. As usual, it is difﬁcult to decide this a priori. As a rule of thumb, based on  2.16 , we expect that when d is relatively small, least- squares methods might be converging faster; while if d is large, then the lightweight, incremental methods will give better results given a ﬁxed computation budget. Notice that this analysis is not   2.2. ALGORITHMS FOR LARGE STATE SPACES 33  speciﬁc to reinforcement learning methods, but it applies in all cases when an incremental lightweight procedure is compared to a least-squares-like procedure  for a similar analysis in a supervised learning problem see, e.g., Bottou and Bousquet, 2008 .  Realizing the need for efﬁcient and robust methods, Geramifard et al.  2007  have recently introduced an incremental version of LSTD, called iLSTD, which, just like LSTD, computes the matrix ˆAn and vector ˆbn, but in each time step, only one dimension of the parameter vector is updated. For sparse features, i.e., when only s components of the feature vector are nonzero, the per-iteration complexity of this method is O sd , while experimentally it has been demonstrated that it is almost as accurate as LSTD given the same number of samples  assuming again sparsity .The storage space needed by iLSTD after processing n samples is O min ns2 + d, d2  . Thus, when the features are sparse and ns2  cid:25  d2, iLSTD might be competitive with LSTD and incremental TD-methods.  2.2.4 THE CHOICE OF THE FUNCTION SPACE In order to be able to discuss the choice of the function space in a meaningful manner, we need to deﬁne how the quality of an approximate value function is measured. When the ultimate goal is value-prediction, a reasonable choice is to use the mean-squared error  MSE  with respect to an appropriate distribution over the states  say, μ .The choice of a metric is less clear when the goal is to learn a good controller and value estimation is only used as a subroutine of a more complex algorithm  such as the ones that will be reviewed in Section 3 . Therefore, in lack of a good understanding of this case, for the purpose of this section, we will stick to the MSE as the quality-measure. However, we believe that most of the conclusions of this section would hold for other measures, too. Learning can be viewed as the process of selecting some function from some space of functions  F  that can be represented  ﬁnitely  in the computer’s memory.3 For simplicity, assume that the functions available are described by d parameters:F = {Vθ  θ ∈ Rd}.One measure that characterizes the choice of F is how well functions from F can approximate the target function V , leading to the deﬁnition of the approximation error underlying F:   cid:6 Vθ − V cid:6 μ.  inf Vθ∈F  To decrease the approximation error, one is encouraged to choose a larger function space  i.e., when Vθ is linear we may add independent features to make F larger . However, as we will argue now, since learning by deﬁnition uses incomplete information, increasing the size of the function space is a double-edged sword.  For simplicity, let us consider linear function approximation, and assume that LSTD, as speciﬁed by  2.12 , is used to obtain the parameters.To make the situation even simpler, assume that the discount factor, γ , is zero and   Xt , Rt+1 ; t ≥ 0  is an i.i.d. sample with Xt ∼ μ. In this case,   cid:3    cid:4   V  x  = r x  = E  Rt+1Xt = x  .  3We do not deal with issues of precision, i.e., that computers cannot really represent real numbers.   n−1 cid:5  t=0   cid:16   34  2. VALUE PREDICTION PROBLEMS  Thanks to γ = 0, LSTD can actually be seen to compute the minimizer of the empirical loss function,  Ln θ   = 1  n   cid:2   ϕ Xt   − Rt+1 2.   θ   cid:2    cid:2   , . . . , ϕ Xn−1   Assume that the dimensionality of the feature space, d, is so large that the matrix whose rows has full column rank  in particular, assume that d ≥ n . This implies are ϕ X0  n ϕ Xt   = Rt+1  cid:2  that the minimum of Ln is zero and if θn denotes the solution of  2.12  then θ holds for t = 0, . . . , n − 1. If the observed rewards are noisy, the resulting function will be a poor approximation to the value function, V , i.e., the estimation error,  cid:6 θ n ϕ − V cid:6 μ, will be large. The  cid:2  phenomenon of ﬁtting to the “noise” is called overﬁtting. If a smaller d is chosen  in general, if a smaller function space F is chosen , then overﬁtting will be less likely to happen. However, in this case, the approximation error will get larger. Hence, there is a tradeoff between the approximation and the estimation errors.  To quantify this tradeoff, let θ∗ be the parameter vector that minimizes the loss  That is,  L θ   = E   cid:2   ϕ Xt   − Rt+1 2   θ  .   cid:17   θ∗ = argmin  L θ  .   A simple argument shows that Vθ∗ is actually the projection of V to F.  The following bound is known to hold if the random rewards are bounded by some value R and, after ﬁnding the optimal weights, at prediction time, the predicted values are back-projected to [−R,R]  Györﬁ et al., 2002, Theorem 11.3, p. 192 :  θ   cid:16    cid:17    cid:6 θ  n ϕ − V cid:6 2  cid:2   ≤ C1  E  d  1 + log n   n  + C2  cid:6 θ   cid:2 ∗ ϕ − r cid:6 2.   2.17   Here C2 is a universal constant, while C1 is a constant that scales linearly with the variance and range of the random rewards.4 The ﬁrst term on the right-hand side bounds the estimation error, while the second term is due to the approximation error. Increasing d increases the ﬁrst term, while it is generally expected to decrease the second.  The argument that leads to bounds of the above form is as follows: By the law of large numbers, Ln θ   converges to L θ   for any ﬁxed value of θ. Hence, by minimizing Ln θ  , one hopes to obtain  cid:2 ∗ ϕ . However, that Ln θ   converges to L θ   at a good approximation to θ∗  more precisely, to θ every value of θ does not mean that the function Ln ·  is uniformly close to L · . Thus, the minimizer of Ln might not give a small loss, as measured by L  cf. Figure 2.2 . Guaranteeing that the two functions are uniformly close to each other  say, over the set {θ  Ln θ   ≤ Ln 0 }  is harder when the dimensionality of θ is larger, hence the tradeoff between estimation and approximation errors. 4Note that without truncation, C1 would not be independent of the distribution of Xt . Although the theorem is stated for the expected error, similar results can be shown to hold with high probability.   2.2. ALGORITHMS FOR LARGE STATE SPACES 35  Loss  L  cid:2    Ln  cid:2    L  Ln      cid:2 > cid:1    cid:1    cid:2 > ¤      cid:2 >n  cid:1   Functions  Figure 2.2: Convergence of Ln ·  to L · . Shown are the curves of the empirical loss, Ln and the true loss L, as a function of the parameter θ. If the true curves are uniformly close to each other  i.e., for every θ, Ln θ   − L θ   is small , then one can expect that the loss of θ   cid:2  n ϕ will be close to the loss of θ   cid:2 ∗ ϕ.  Bounds similar to  2.17  hold even when γ > 0, e.g. for value functions estimated using LSTD and even when the sequence  Xt; t ≥ 0  is dependent provided that it “mixes well”  for some initial steps in this direction consult the work of Antos et al.  2008  . In fact, when γ  cid:20 = 0, the noise comes both from the immediate rewards Rt+1 and the “next states”, Yt+1. The trade- off between the approximation and estimation errors also shows up when control algorithms are used: Munos and Szepesvári  2008  and Antos et al.  2007, 2008  derive ﬁnite-sample performance bounds for some variants of ﬁtted value iteration, a ﬁtted actor-critic method and an approximate policy iteration method, respectively.  Recognizing the importance of the choice of the function space, as well as the difﬁculty of choosing it right, there has been a growing interest in automating this choice lately. One class of methods aims at constructing a parsimonious set of features  basis functions . These include tuning the parameter of Gaussian RBF either using a gradient- or the cross-entropy-method in the con- text of LSTD  Menache et al., 2005 , deriving new basis functions with nonparametric techniques   36  2. VALUE PREDICTION PROBLEMS   Keller et al., 2006; Parr et al., 2007  or using a combination of numerical analysis and nonpara- metric techniques  Mahadevan, 2009 . These methods, however, do not attempt to control the tradeoff between the approximation and estimation errors. To account for this deﬁciency, other re- searchers explore nonparametric techniques originating at supervised learning. Examples of this line of research include the use of regression trees  Ernst et al., 2005 , or “kernelizing” the value esti- mation algorithms  e.g., Rasmussen and Kuss, 2004; Engel et al., 2005; Ghavamzadeh and Engel, 2007; Xu et al., 2007; Taylor and Parr, 2009 . These approaches implicitly or explicitly regularize the estimates to control the loss. Kolter and Ng  2009  designed an algorithm inspired by LASSO that uses  cid:16 1-regularization to implement feature selection in the context of LSTD. Although the approaches above are inspired by principled methods of supervised learning, not much is known about their statistical properties. Recently, Farahmand et al.  2009, 2008  have developed another regularization-based approach that comes with statistical guarantees.  The difﬁculty of using  some  nonparametric techniques is that they are computationally expensive. As a result, when the algorithms are used for planning and a fast simulator is used to generate data  so that the cost of generating new data is negligible , it might be better to use an appropriate fast incremental method also act as a way of regularization  and a simple linear function- approximation method with many features than to use a sophisticated but computationally expensive nonparametric method. Computational efﬁciency is less important if a limited amount of data is available only and the quality of the solutions is the primary concern, or when the problem is complex enough so that tuning the function approximation is necessary, but hand-tuning is infeasible.   C H A P T E R 3 Control  37  We now turn to the problem of learning a  near- optimal policy. We start by discussing the various forms of control learning problems  Section 3.1 , followed by a discussion of interactive learning  Section 3.2 . In the last two sections  Sections 3.3 and 3.4 , the learning counterparts of the classical methods of dynamic programming are discussed.  3.1 A CATALOG OF LEARNING PROBLEMS Figure 3.1 shows the basic types of control learning problems. The ﬁrst criterion that the space of problems is split upon is whether the learner can actively inﬂuence the observations. In case she can, then we talk about interactive learning, otherwise one is facing a non-interactive learning problem.1 Interactive learning is potentially easier since the learner has the additional option to inﬂuence the distribution of the sample. However, the goal of learning is usually different in the two cases, making these problems incomparable in general.  In the case of non-interactive learning, the natural goal is to ﬁnd a good policy given the observations. A common situation is when the sample is ﬁxed. For example, the sample can be the result of some experimentation with some physical system that happened before learning started. In machine learning terms, this corresponds to batch learning.  Batch learning problems are not to be confused with batch learning methods, which are the opposite of incremental a.k.a. recursive, or iterative methods.  Since the observations are uncontrolled, the learner working with a ﬁxed sample 1The terms “active learning” and “passive learning” might appeal and their meaning indeed covers the situations discussed here. However, unfortunately, the term “active learning” is already reserved in machine learning for a special case of interactive learning. As a result, we also decided against calling non-interactive learning “passive learning” so that no one is tempted to call interactive learning “active learning”.   cid:2   cid:15   cid:4   cid:12   cid:11   cid:7   cid:8   cid:5   cid:12   cid:2   cid:18    cid:15   cid:4   cid:8   cid:7   cid:2   cid:5   cid:11   cid:17   cid:6   cid:9   cid:2   cid:2   cid:8   cid:7   cid:5   cid:16    cid:4    cid:1  cid:2  cid:3  cid:4  cid:2  cid:5  cid:6  cid:3  cid:5  cid:7  cid:8  cid:2  cid:4  cid:2  cid:9    cid:10  cid:11  cid:12  cid:4  cid:13  cid:5  cid:6  cid:3  cid:5  cid:7  cid:8  cid:2  cid:4  cid:2  cid:9    cid:14  cid:15  cid:6  cid:4  cid:2  cid:12  cid:5  cid:8  cid:7  cid:11  cid:12  cid:4  cid:15  cid:2   Figure 3.1: Types of reinforcement problems   38  3. CONTROL  has to deal with an off-policy learning situation. In other cases, the learner can ask for more data  i.e., when a simulator is used to generate new data . Here the goal might be to learn a good policy as quickly as possible.  Consider now interactive learning. One possibility is that learning happens while interacting with a real system in a closed-loop fashion. A reasonable goal then is to optimize online performance, making the learning problem an instance of online learning. Online performance can be measured in different ways. A natural measure is to use the sum of rewards incurred during learning. An alternative cost measure is the number of times the learner’s future expected return falls short of the optimal return, i.e., the number of times the learner commits a “mistake”. Another possible goal is to produce a well-performing policy as soon as possible  or ﬁnd a good policy given a ﬁnite number of samples , just like in non-interactive learning. As opposed to the non-interactive situation, however, here the learner has the option to control the samples so as to maximize the chance of ﬁnding such a good policy. This learning problem is an instance of active learning.  When a simulator is available, the learning algorithms can be used to solve planning problems. In planning the previous performance metrics become irrelevant and the algorithms’ running time and memory requirements become the primary concern.  3.2 CLOSED-LOOP INTERACTIVE LEARNING The special feature of interactive learning is the need to explore. In this section, we ﬁrst use bandits  i.e., MDPs with a single state  to illustrate the need for exploration, both in online and active learning. Next, we discuss active learning in MDPs. This is followed by a discussion of algorithms available for online learning in MDPs.  3.2.1 ONLINE LEARNING IN BANDITS Consider an MDP that has a single state. Let the problem be that of maximizing the return while learning. Since there is only one state, this is an instance of the classical bandit problems  Robbins, 1952 . A basic observation then is that a bandit learner who always chooses the action with the best estimated payoff  i.e., who always makes the greedy choice  can fail to ﬁnd the best action with positive probability, which in turn leads to a large loss. Thus, a good learner must take actions that look suboptimal, i.e., must explore. The question is then how to balance the frequency of exploring and exploiting  i.e., greedy  actions.  A simple strategy is to ﬁx ε > 0 and choose a randomly selected action with probability ε, and go with the greedy choice otherwise. This is the so-called ε-greedy strategy. Another simple strategy is the so-called “Boltzmann exploration” strategy, according to which, given the sample means,  Qt  a ; a ∈ A , of the actions at time t, the next action is drawn from the multinomial distribution  π a ; a ∈ A , where   cid:2   π a  =  exp  β Qt  a     cid:17 ∈A exp  β Qt  a   cid:17   .       a   3.2. CLOSED-LOOP INTERACTIVE LEARNING 39  Here β > 0 controls the greediness of action selection  β → ∞ results in a greedy choice . The difference between Boltzmann exploration and ε-greedy is that ε-greedy does not take into account the relative values of the actions, while Boltzmann exploration does. These algorithms extend easily to the case of unrestricted MDPs provided that some estimates of the action-values is available.  If the parameter of ε-greedy is made a function of time and the resulting sequence is appropri- ately tuned, ε-greedy can be made competitive with other, more sophisticated algorithms. However, the best choice is problem dependent and there is no known automated way of obtaining good results with ε-greedy  Auer et al., 2002 . The same holds for the Boltzmann exploration strategy.  A better approach might be to implement the so-called optimism in the face of uncertainty  OFU  principle due to Lai and Robbins  1985 , according to which the learner should choose the action with the best upper conﬁdence bound  UCB . A very successful recent algorithm, UCB1, implements this principle by assigning the following UCB to action a at time t  Auer et al., 2002 :   cid:26   Ut  a  = rt  a  + R  2 log t nt  a   .  Here nt  a  is the number of times action a was selected up to time t and rt  a  is the sample mean of the nt  a  rewards observed for action a, whose range is [−R,+R]. It can be shown that the failure −4. Notice that an action’s UCB is larger if less information is available for probability of Ut  a  is t it. Further, an action’s UCB value increases even if it is not tried. Algorithms 8 and 9 show the pseudocode of UCB1, in the form of two routines, one to be used for action selection and the other for updating the internal statistics. When the variance of the rewards associated with some of the actions are small, it makes sense to estimate these variances and use them in place of the range R in the above algorithm. A principled way of doing this was proposed and analyzed by Audibert et al.  2009 . The resulting algorithm often outperforms UCB1 and can be shown to be essentially unimprovable.The algorithm that we will describe in Section 3.2.4 implements the OFU principle in MDPs in a way similar to UCB1.  The setting considered here is called the frequentist agnostic setting, where the only assump- tion made about the distribution of rewards is that they are independent across the actions and time steps and that they belong to the [0, 1] interval. However, there is no other a priori knowledge about their distributions. An alternative, historically signiﬁcant, variant of the problem is when the reward distributions have some known parametric form and the parameters are assumed to be drawn form a known prior distribution. The problem then is to ﬁnd a policy which maximizes the total expected cumulated discounted reward, where the expectation is both over the random rewards and the parameters of their distributions. This problem can be represented as an MDP whose state at time t is the posterior over the parameters of the reward distributions. For example, if the rewards assume Bernoulli distributions and their parameters are sampled from a Beta distribution then the state at time t will be a 2A dimensional vector  since the Beta distribution has two parameters . Thus, the state space of this MDP can be rather complicated even for the simplest examples. In   40  3. CONTROL  Algorithm 8 The function implementing action selection in UCB1. By assumption, initially n[a] = 0, r[a] = 0 and the reward received lie in the [0, 1] interval. Further, for c > 0, c 0 = ∞. function UCB1Select r, n, t  Input: r, n are arrays of size A, t is the number of time steps so far 1: U max ← −∞ 2: for all a ∈ A do  U ← r[a] + R · sqrt  2 · log t   n[a]  if U > U max then   cid:17  ← a, U max ← U  3: 4: 5: 6: 7: end for 8: return a  a end if  cid:17   his groundbreaking paper, Gittins  1989  has shown that rather surprisingly, the optimal policy in this MDP assumes a simple index-form, which, in some special cases can be calculated exactly and efﬁciently  e.g., in the case of Bernoulli reward distributions mentioned above . The conceptual difﬁculty of this so-called Bayesian approach is that although the policy is optimal on the average for a collection of randomly chosen environments, there is no guarantee that the policy will perform well on the individual environments. The appeal of the Bayesian approach, however, is that it is conceptually very simple and the exploration problem is reduced to a computational problem.  Algorithm 9 The function implementing the update routine of UCB1. The update, which updates the action counters and the estimates of the average reward, must be called after each interaction. function UCB1Update A, R, r, n  Input: A is the last action selected, R is the associated reward, r, n are arrays of size A, t is the 1: n[A] ← n[A] + 1 2: r[A] = r[A] + 1.0   n[A] ·  R − r[A]  3: return r, n  number of time steps so far  3.2.2 ACTIVE LEARNING IN BANDITS Consider now active learning, still in the case when the MDP has a single state. Let the goal be to ﬁnd an action with the highest immediate reward given  say  T interactions. Since the rewards received during the course of interaction do not matter, the only reason not to try an action is if it can be seen to be worse than some other action with sufﬁcient certainty. The remaining actions should be tried in the hope of proving that some are suboptimal. A simple way to achieve this is to   compute upper and lower conﬁdence bounds for each action:  3.2. CLOSED-LOOP INTERACTIVE LEARNING 41   cid:27   cid:27  Ut  a  = Qt  a  + R Lt  a  = Qt  a  − R  cid:17   cid:17 ∈A Lt  a  log 2AT  δ  log 2AT  δ   2t  ,  ,  2t  and eliminate an action a if Ut  a  < maxa  . Here 0 < δ < 1 is a user chosen parameter which speciﬁes the target conﬁdence with which the algorithm is allowed to fail to return an action with the highest expected reward. Apart from constant factors and using estimated variances in the conﬁdence bounds, this algorithm is unimprovable  Even-Dar et al., 2002; Tsitsiklis and Mannor, 2004; Mnih et al., 2008 .  3.2.3 ACTIVE LEARNING IN MARKOV DECISION PROCESSES There exist only a few theoretical works that consider active learning in MDPs. Deterministic environments have been considered by Thrun  1992   see also Berman, 1998 . It turns out that the bounds given in Thrun  1992  can be signiﬁcantly improved as follows:2 Assume that the MDP M is deterministic. Then the MDP’s transition structure can be recovered in at most n2m steps, where n = X, m = A  Ortner, 2008 . A procedure that achieves this is as follows: The task is to explore all actions in all states exactly once. Hence, at any time t, given the “known part” of the dynamics, we ﬁnd the closest state with an unexplored action. In at most n − 1 steps, this state is reached and the action chosen is explored. Since there are altogether nm state-action pairs to explore, the total number of time steps needed is n2m.3 Given the transition structure, the reward structure can be explored up to accuracy ε with probability 1 − δ after at most k = log nm δ  ε2 visits to all state-action pairs. If it takes e ≤ n2m  time-steps to visit all state-action pairs by some exploration policy, then in ke steps the learner will have an ε-accurate model of the environment. Knowing such a model allows the learner to ﬁnd a policy whose value is 4γ ε  1 − γ  2-close to the optimal value in each state  assuming, for simplicity, γ ≥ 0.5 . Thus, altogether, to ﬁnd an overall ε-optimal policy, at most n2m + 4e log nm δ    1 − γ  2ε 2 steps are needed.  According to the author’s knowledge, there exist no works that consider the analogous problem of ﬁnding a uniformly almost-optimal strategy in a stochastic MDP. Even-Dar et al.  2002  consider active-learning in ﬁnite stochastic MDPs but only under the  strong  assumption that the learner can reset the state of the MDP to an arbitrary state. This way they avoid the challenge of navigating in the unknown MDP.  That this is indeed a major challenge can be seen because there exists MDPs where random exploration takes exponential time in the MDPs’ size to visit all parts of the state space. Consider for example a chain-like MDP with n states, say X = {1, 2, . . . , n}. Let A = {L1, L2, R}. Actions L1 and L2 decrement the state by one, while action R increments it by one.The 2Curiously, this argument is new. 3This bound curiously improves the bound of Thrun  1992 . The bound can be shown to be tight in an asymptotic sense.   42  3. CONTROL state is not changed at the boundaries when the action would lead to a state outside of X . The policy that selects actions uniformly at random will need 3 2n − n − 1  steps on average to reach state n from state 1  Howard, 1960 . However, a policy that systematically explores the state space will only need O n  actions to reach state n from state 1. Assume now that all immediate rewards are zero except at state n where the reward is one. Consider an explore-then-exploit learner that explores the MDP randomly until its estimates are sufﬁcient accurate  e.g., until it visited all state-action pairs sufﬁciently many times . Clearly, the learner will take exponentially many steps before switching to exploitation and hence the learner will suffer a huge regret. The situation is not much better if the agent uses a simple exploration strategy based on some estimates of the values of the actions.  A problem closely related to active learning  without a reset  was studied by Kearns and Singh  2002 . They proposed the E3-algorithm that explores an unknown  stochastic  MDP and stops when it knows a good policy for the state just visited. They proved that in discounted MDPs E3 needs a polynomial number of interactions and uses poly-resources in the relevant parameters of the problem before it stops. In a follow-up work, Brafman and Tennenholtz  2002  introduced the R-max algorithm which reﬁnes the E3 algorithm and proved similar results. Another reﬁnement of E3 is due to Domingo  1999  who proposed to use adaptive sampling to increase efﬁciency when the MDP has many near-deterministic transitions. If the problem is undiscounted, both E3 and R-max need the knowledge of the so-called ε-mixing time of the MDP to work properly. When this knowledge is not available, the algorithms do not know when to stop  Brafman and Tennenholtz, 2002 .  Little is known about the performance of active learning algorithms on practical prob- lems. Some experimental results  for some heuristic algorithms  can be found in the paper by ¸Sim¸sek and Barto  2006 .  3.2.4 ONLINE LEARNING IN MARKOV DECISION PROCESSES Let us now return to online learning in MDPs. One possible goal then is to minimize regret, i.e., the difference of the total reward that would have been achieved by the optimal policy and that of received by the learner. This problem is considered in the ﬁrst part of this section. Another possible goal is to minimize the number of time steps when the algorithm’s future expected return falls short of the optimal expected return by some prespeciﬁed amount. This problem is considered in the second part of this section.  Consider a ﬁnite  small  MDP M = Regret minimization and the UCRL2 algorithm  X ,A,P0 . Assume that the random immediate rewards are bound to lie in[0, 1] and, for simplicity, assume that all deterministic  stationary  policies visit all states eventually with probability one, i.e., the MDP is unichain.   3.2. CLOSED-LOOP INTERACTIVE LEARNING 43 Under this condition, every policy π gives rise to a recurrent Markov chain on X and a unique  stationary distribution μπ . Deﬁne the long-run average reward of π by   cid:5  x∈X  ρπ =  μπ  x r x, π x  .  ∗ = max π∈ cid:7 stat  ρ  ρπ .   If we want to emphasize the dependence of the long-run average reward on the MDP M, we will write ρπ  M .  Let ρ  denote the optimal long-run average reward:  ∗  = RA  Consider some learning algorithm A  i.e., A is a history dependent behavior . Deﬁne the regret of A by = cid:2   T −1 whereRA t=0 Rt+1 is the sum of rewards received up to time T while followingA.Minimizing the regret is clearly equivalent to maximizing the total reward. Hence, from now on, we consider the = o T  , i.e., if the rate of growth of the regret is problem of minimizing regret. Notice that if R sublinear then the long-term average reward of A is ρ initial state, T ≥ 1, ε > 0, with probability 1 − 3δ the regret of UCRL2 δ  satisﬁes  The UCRL2 algorithm described below achieves logarithmic regret. In particular, for any  , i.e., A is consistent.  − T ρ  A T  A T  R  ∗  ∗  ,  T  T  RUCRL2 δ   T  = O D2X2A log T  δ  ε + εT  .  Here D, the so-called diameter of the MDP, is deﬁned as the largest number of steps  on average  it takes to reach some state from some other state in the MDP. Moreover, if the conﬁdence parameter of UCRL2 is set to δ = 1  3T   then   cid:16    cid:17   E  RUCRL2 1  3T     T  = O D2X2A log T  g ,  where g is the “gap” between the performance of the optimal policy and the second best policy  Auer et al., 2010 . One issue with this bound is that the gap g could be very small, in which case, the bound might be vacuous for small values of T . An alternative bound, which is independent of g, takes the form   cid:16    cid:17   = O DX cid:19 AT log T    E  RUCRL2 1  3T     T   Auer et al., 2010 .  Note that these bounds are vacuous when the MDP has an inﬁnite diameter. This happens if the MDP has some states which are not accessible from some other states, i.e., if the MDP has transient states.The only algorithm that is known to enjoy some regret bounds even when the MDP has transient states is due to Bartlett and Tewari  2009 . However, this algorithm requires the a priori knowledge of some parameter of the MDP. At present, it is not known if such a knowledge   44  3. CONTROL  is necessary to achieve a non-trivial bound. The issue with transient states is that it is very costly to distinguish between transient states and states that are just hard to reach.  The UCRL2 algorithm  Algorithm 10  implements the optimism in the face of uncertainty principle: It constructs conﬁdence intervals around the estimates of the transition probabilities and the immediate reward function. These deﬁne a set of plausible MDPs, Ct. When it comes to ∈ Ct and a policy π compute a policy, UCRL2 ﬁnds a model M∗ ∗ t that gives  approximately  the highest average reward within this class: √ t  M∗ ∗  ρπ  M  − 1   ρπ  t .  t  t   ≥ max π,M∈Ct  A crucial step of the algorithm is the computation of π  Note that an essential element of UCRL2 is that it does not update the policy in each time step but waits until the quality of the statistics available for at least one state-action pair is sufﬁciently improved. This is implemented in line 6 by the check of the visit-count of the current state-action pair.  ∗ t . This is done by the procedure OptSolve  cf. Algorithm 11 , using undiscounted value iteration over a special MDP. In this MDP, the actions are of the form of a pair  a, p , where a ∈ A and p is a plausible next-state distribution given the statistics collected so far at  x, a .The next-state distribution associated to  a, p  is exactly p. Further, the immediate reward at x associated to  a, p  is the highest plausible reward given the local statistics at  x, a .  PAC-MDP algorithms As mentioned before,an alternative to minimizing the regret is to minimize the number of times the learner’s future expected return falls short of the optimal return by a prespec- iﬁed margin  Kakade, 2003 . An online learning algorithm is called PAC-MDP if this measure can be bounded with high probability as a polynomial function of the natural parameters of the MDP and if in each time step polynomially many computational steps are performed. Algorithms that are known to be PAC-MDP include R-max  Brafman and Tennenholtz, 2002; Kakade, 2003 ,4 MBIE  Strehl and Littman, 2005 , Delayed Q-learning  Strehl et al., 2006 , the optimistic-initialization- based algorithm of Szita and L˝orincz  2008 , and MorMax by Szita and Szepesvári  2010 . Of these, MorMax enjoys the best bound for the number of ε-suboptimal steps, Tε. According to this bound, with probability 1 − δ,   cid:28    cid:29   Tε = ˜O  XA  Vmax  ε 1 − γ  2   cid:30 2   cid:29    cid:30  cid:31   1 δ  log  ,  where ˜O ·  hides terms which are logarithmic in the MDP parameters and Vmax is an upper bound on the optimal value function  i.e., Vmax ≤  cid:6 r cid:6 ∞  1 − γ   . One notable feature of this bounds that it scales  log  linearly with the size of the state space. A similar bound is available for Delayed Q- learning  though the dependence of this bound is worse on the other parameters , but no bounds with  4The published proofs for E3  Kearns and Singh, 1998  and R-max concern a slightly different criterion; see the discussion of the previous section. Kakade  2003  proved that  an improved version of   R-max is PAC-MDP. He also proved lower bounds.   3.2. CLOSED-LOOP INTERACTIVE LEARNING 45   cid:28  Initialize policy  cid:28  Initialize arrays   cid:28  Enough new information?  cid:28  Update model  cid:28  Update policy   cid:28  Execute action in the “world”   cid:17  ← 0   cid:17   cid:17  2: n2, n3, r, n 2, n 3, r  A ← π[X] [X, A] ≥ max 1, n2[X, A]  then  cid:17  if n n2 ← n2 + n 2, n3 ← n3 + n  cid:17  2  cid:17  ← 0  cid:17   cid:17  2, n 3, r π ← OptSolve n2, n3, r, δ, t   A ← π[X]  Algorithm 10 The UCRL2 algorithm. function UCRL2 δ  Input: δ ∈ [0, 1] is a conﬁdence parameter 1: for all x ∈ X do π[x] ← a1 3: t ← 1 4: repeat 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: until True  end if  R, Y   ← ExecuteInWorld  A   [X, A] + 1 [X, A] ← n  cid:17   cid:17  n [X, A, Y] ← n [X, A, Y] + 1  cid:17   cid:17  2 2 n  cid:17 [X, A] ← r  cid:17 [X, A] + R 3 3 r X ← Y t ← t + 1  n  3, r ← r + r  cid:17    cid:17   this feature are yet available for the other algorithms. The algorithms mentioned here all implement the OFU principle in some way.  The main issue with all these algorithms  including UCRL and its variants  is that they are in- herently limited to  small  ﬁnite spaces. Larger state-spaces are explicitly considered by Kakade et al.  2003  and Strehl and Littman  2008 , who considered restricted classes of MDPs and provided “meta-algorithms” to address the exploration problem. There are two difﬁculties with these ap- proaches. First, in practice, it may be difﬁcult to verify if the particular problem one is interested in belongs to the said classes of MDPs. Second, the proposed algorithms require black-box MDP solvers. Since solving large MDPs is a difﬁcult problem on its own, the algorithms may be hard to implement.  An alternative to the above techniques is to use a Bayesian approach to address the exploration issue  e.g., Dearden et al., 1998, 1999; Strens, 2000; Poupart et al., 2006; Ross and Pineau, 2008 . The pros and contras of this approach are the same as in the case of bandits, the only difference being that the computational challenges multiply.  To the best of our knowledge, the only experimental works that concerns online learn- ing in continuous state MDPs are due to Jong and Stone  2007  and Nouri and Littman  2009 . Jong and Stone  2007  proposed a method that can be interpreted as a practical implementation   46  3. CONTROL   cid:28  Initialize policy   cid:28  u[idx[1]] ≥ u[idx[2]] ≥ . . .  M ← −∞, m ← ∞ idx ← sort u  for all x ∈ X do unew[·] ← −∞ for all a ∈ A do  Algorithm 11 Procedure for ﬁnding an optimistic policy used by UCRL2. function OptSolve n2, n3, r, δ, t  Input: n2, n3 store counters, r stores total rewards, δ ∈ [0, 1] is a conﬁdence parameter 1: u[·] ← 0, π[·] ← a1 2: repeat 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26:  r ← r[x, a]   n2[x, a] + sqrt  7 · ln 2 · X · A · t   δ     2 · max 1, n2[x, a]     c ← sqrt  14 · ln 2 · A · t   δ    max 1, n2[x, a]    p[·] ← n3[x, a,·]   n2[x, a] p[idx[1]] ← min 1, p[idx[1]] + c 2  j ← X + 1 repeat j ← j − 1 P ← sum p[·]  − p[idx[j]] p[idx[j]] ← min 0, 1 − P   until P + p[idx[j]] > 1 v ← r + inner_product p[·], u[·]  if v > unew then  end for M ← max M, unew − u[x] , m ← min m, unew − u[x]   cid:17 [x] ← unew end for u ← u  cid:17   π[x] ← a, unew ← v  end if  u  27: until M − m ≥ 1   sqrt t   28: return π  of the ideas in Kakade et al.  2003 , while Nouri and Littman  2009  experimented with multi- resolution regression trees and ﬁtted Q-iteration. The main message of these works is that explicit exploration control can indeed be beneﬁcial.  Despite the potential huge performance gains that can result from using systematic explo- ration, current practitioners of reinforcement learning largely neglect the issue of systematic explo- ration or, at best, use simple heuristics to guide exploration. Certainly, there are some cases when systematic exploration is not needed  e.g., Szepesvári, 1997; Nascimento and Powell, 2009 . Further,   3.3. DIRECT METHODS 47  Algorithm 12 The function implementing the tabular Q-learning algorithm. This function must be called after each transition. function QLearning X, A, R, Y, Q  Input: X is the last state, A is the last action, R is the immediate reward received, Y is the next state, Q is the array storing the current action-value function estimate 1: δ ← R + γ · maxa  cid:17 ∈A Q[Y, a 2: Q[X, A] ← Q[X, A] + α · δ 3: return Q   cid:17 ] − Q[X, A]  some simple methods, such as optimistic initialization, might give reasonable performance in prac- tice. Since systematic exploration is hardly possible without a good collection of learning algorithms aimed at learning good policies in an efﬁcient manner, in what follows, we will focus on reviewing the algorithms that might belong to such a collection.  3.3 DIRECT METHODS  In this section, we review algorithms whose aim is to approximate the optimal action-value function ∗ directly. The reviewed algorithms can be thought of as sample-based, approximate versions of Q value iteration that generate some sequence of action-value functions  Qk; k ≥ 0 . The idea is that if Qk is close to Q , the policy that is greedy with respect to Qk will be close to optimalas shown by the bound  1.16 .  ∗  The ﬁrst algorithm that we review is Q-learning by Watkins  1989 . We start by describing this algorithm for  small  ﬁnite MDPs, which is followed by a description of its various extensions that work even in large MDPs.  3.3.1 Q-LEARNING IN FINITE MDPS Fix a ﬁnite MDP M =  X ,A,P0  and a discount factor γ . The Q-learning algorithm of Watkins  x, a  for each state-action pair  x, a  ∈ X × A. Upon ∗  1989  keeps an estimate Qt  x, a  of Q observing  Xt , At , Rt+1, Yt+1 , the estimates are updated as follows:   − Q Xt , At  ,  δt+1 Q  = Rt+1 + γ max   cid:17    cid:17 ∈A Q Yt+1, a  Qt+1 x, a  = Qt  x, a  + αt δt+1 Qt   I{x=Xt ,a=At},  a   x, a  ∈ X × A.   3.1   Here At ∈ A and  Yt+1, Rt+1  ∼ P0 · Xt , At  . When learning from a trajectory, Xt+1 = Yt+1, but this is not necessary for the convergence of the algorithm. Q-learning is an instance of TD learning: the updates are based on the TD-error δt+1 Qt  . Algorithm 12 shows the pseudocode of Q-learning.   48  3. CONTROL  In stochastic equilibrium, one must have E  X × A that is visited inﬁnitely often. A trivial calculation shows that Q  x, a  − Q x, a ,   cid:8  cid:8  cid:8  Xt = x, At = a  δt+1 Q   = T   cid:16    cid:17   ∗  E   cid:3   δt+1 Q  Xt = x, At = a   cid:4  = 0 for any  x, a  ∈ x ∈ X , a ∈ A,  ∗  is the Bellman optimality operator deﬁned by  1.15 .Hence,under the minimal assumption where T Q = that every state-action pair is visited inﬁnitely often, in stochastic equilibrium, one must have T Q. Using Fact 1.5, we see that if the algorithm converges, it must converge to Q under the stated condition. The sequence  Qt; t ≥ 0  is indeed known to converge to Q ∗ when appropriate local learning rates are used  Tsitsiklis, 1994; Jaakkola et al., 1994 .5 The rate of convergence of Q-learning was studied by Szepesvári  1997  in an asymptotic setting and later by Even-Dar and Mansour  2003  in a ﬁnite-sample setting.  ∗  ∗  The key observation that lead to the discovery of Q-learning is that unlike the optimal state values, the optimal action-values can be expressed as expectations  compare Equations  1.13  and  1.15  . This, in turn, allows one to estimate the action-values in an incremental manner.  There exist multi-step versions of Q-learning  e.g., Sutton and Barto, 1998, Section 7.6 . However, these are not as appealing  and straightforward  as the multi-step extensions of TD 0  since Q-learning is an inherently off-policy algorithm: the temporal differences underlying Q- learning do not telescope even when Xt+1 = Yt+1. What policy to follow during learning? A major attraction of Q-learning is its simplicity and that it allows one to use an arbitrary sampling strategy to generate the training data provided that in the limit, all state-action pairs are updated inﬁnitely often. In a closed-loop situation, the commonly used strategies are to sample the actions following the ε-greedy action selection scheme or the Boltzmann scheme  in the latter case, the probability of selecting action a at time t is chosen to be proportional to e βQt  Xt ,a   .With appropriate tuning,one can then achieve asymptotic consistency of the behavior policy  cf., Szepesvári, 1998, Section 5.2 and Singh et al., 2000 . However, as discussed in Section 3.2, in closed-loop learning, more systematic exploration might be necessary to achieve reasonable online performance  cf. Section 3.2 .  Post-decision states than X × A can be identiﬁed such that the transition probabilities decompose according to  In many practical problems, a set Z  the set of “post-decision states”  smaller  P x, a, y  = PA f  x, a , y ,  x, y ∈ X , a ∈ A.  Here f : X × A → Z is some known transition function and PA : Z × X → [0, 1] is an appro- priate probability kernel. Function f determines the deterministic “effect” of the actions, while PA captures their stochastic effect. Many operations research problems enjoy this structure. For exam- ple, in the inventory control problem  Example 1.1 , f  x, a  =  x + a  ∧ M. Further examples are given by Powell  2007 . Note that Sutton and Barto  1998  calls post-decision states “afterstates”. 5 Watkins  1989  did not provide a rigorous convergence analysis. Watkins and Dayan  1992  gave a proof for the case when all policies eventually lead to an absorbing state.   If a problem admits post-decision states, learning the immediate reward function  if it is not  known  and the so-called post-decision state optimal value function, V A z ∈ Z,  PA z, y  V  A z  = ∗   y ,  ∗  V   cid:5  y∈X  3.3. DIRECT METHODS 49  ∗  : Z → R, deﬁned by  might be both more economical and efﬁcient than learning an action-value function.Update rules and ∗ ∗ action selection strategies can be derived based on the identity Q A f  x, a  , which follows immediately from the deﬁnitions.   x, a  = r x, a  + γ V  To see another potential advantage of using post-decision state value functions, assume that we have access to the transition probabilities. In such a case, we might be tempted to approx-  cid:2  imate the state-value function instead of approximating the action-value function. Then, in or- der to compute the greedy action  which is necessary in many algorithms , we need to compute argmaxa∈A r x, a  + γ y∈X P x, a, y V  y . This is a so-called stochastic optimization problem  the modiﬁer “stochastic” refers to that the optimization objective is deﬁned with an expectation . This problem might be computationally challenging when the number of next states is large, and or the number of actions is large  e.g., if A is a large or inﬁnite subset of a Euclidean space , and or P does not enjoy a nice structure. On the other hand, if one uses a post-decision state value function VA, then computing a greedy action reduces to ﬁnding argmaxa∈A r x, a  + γ VA f  x, a  . Thus, the expectation is avoided, i.e., no stochastic optimization problem needs to be solved. Further, with a judiciously chosen approximation architecture  such as piecewise linear, concave, separable , the optimization problem might be tractable even for large  or inﬁnite  action spaces. Thus, post- decision state value functions might be advantageous as they allow one to avoid another layer of complexity. Of course, the same applies to using action-value functions, but, as discussed previously, post-decision state value functions may require less storage and potentially require fewer samples to learn than action-value functions. For further details, ideas and examples consult  Powell, 2007 .  3.3.2 Q-LEARNING WITH FUNCTION APPROXIMATION The obvious extension of Q-learning to function approximation with parametric forms  Qθ; θ ∈ Rd  is  compare this with  2.5  when λ = 0 . Algorithm 13 shows the pseudocode corresponding to the case when a linear function-approximation method is used, i.e., when Qθ = θ ϕ where ϕ : X × A → Rd.  θt+1 = θt + αt δt+1 Qθt  ∇θ Qθt  Xt , At  .   cid:2   Although the above update rule is widely used in practice, little can be said about its con- vergence properties. In fact, since TD 0  is a special case of this algorithm  when there is only one action for every state , just like TD 0 , this update rule will also fail to converge when off-policy sampling or nonlinear function approximation is used  cf. Section 2.2.1 . The only known conver- gence result is due to Melo et al.  2008  who prove convergence under rather restrictive conditions on the sample distribution. More recently, along the line of the recent gradient-like TD algorithms,   50  3. CONTROL  Algorithm 13 The function implementing the Q-learning algorithm with linear function approxi- mation. This function must be called after each transition. function QLearningLinFApp X, A, R, Y, θ  Input: X is the last state, Y is the next state, R is the immediate reward associated with this transition, θ ∈ Rd parameter vector 1: δ ← R + γ · maxa  cid:2   cid:17 ∈A θ 2: θ ← θ + α · δ · ϕ[X, A] 3: return θ  ϕ[X, A]  ϕ[Y, a   cid:17 ] − θ   cid:2   Maei et al.  2010b  proposed the greedy gradient Q-learning  greedy GQ  algorithm which lifts the previous restrictive conditions: This new algorithm is guaranteed to converge independently of the sampling distribution. However, since the objective function used in the derivation of this algorithm is non-convex, the algorithm may get stuck in local minima even when used with linear function approximation.  State aggregation Since the above update rule may fail to converge, it is natural to restrict the value function-approximation method employed and or modify the update procedure as necessary. In this spirit, let us ﬁrst consider the case when Qθ is a state  and action  aggregator  cf. Section 2.2 . Then, if   Xt , At  ; t ≥ 0  is stationary then the algorithm will behave exactly like tabular Q-learning in an appropriately deﬁned “induced MDP”. Hence it will converge to some approximation of the optimal action-value function Q   Bertsekas and Tsitsiklis, 1996, Section 6.7.7 .  ∗  d  d  Soft state aggregation One undesirable property of aggregation is that the value function will not the form of a linear averager: Qθ  x, a  = cid:2  be smooth at the boundaries of the underlying regions. Singh et al.  1995  proposed to address this  cid:2  by a “softened” version of Q-learning. In their algorithm, the approximate action-value function has i=1 si  x, a θi, where si  x, a  ≥ 0  i = 1, . . . , d  and i=1 si  x, a  = 1. The update rule is modiﬁed so that at any time, only one component of the pa- rameter vector θt is updated.The component to be updated is selected by randomly drawing an index It ∈ {1, . . . , d} from the multinomial distribution with parameters  s1 Xt , At  , . . . , sd  Xt , At   . Interpolation-based Q-learning Szepesvári and Smart  2004  proposed a modiﬁcation of this al- gorithm, which they call interpolation based Q-learning  IBQ-learning . IBQ simultaneously updates all the components of the parameter vector thereby reducing the updates’ variance. IBQ-learning can also be viewed as a generalization of Q-learning used with state and action aggregation to interpolators  Tsitsiklis and Van Roy, 1996, Section 8 discusses interpolators in the context of ﬁtted value iteration with known models . The idea is to treat every component θi of the parameter vec- tor as a value estimate of some “representative” state-action pair,  xi , ai   ∈ X × A  i = 1, . . . , d . That is,  Qθ; θ ∈ Rd   is chosen such that Qθ  xi , ai   = θi holds for all i = 1, . . . , d. This makes Qθ an interpolator  explaining the name of the algorithm . Next, choose the similarity functions si : X × A → [0,∞ . For example, one can use si  x, a  = exp −c1d1 x, xi  2 − c2d2 a, ai  2 ,   where c1, c2 > 0, and d1, d2 are appropriate “distance” functions. The update rule of IBQ-learning is as follows:  3.3. DIRECT METHODS 51  δt+1,i = Rt+1 + γ max  cid:17   cid:17 ∈A Qθt  Yt+1, a θt+1,i = θt,i + αt,i δt+1,i si  Xt , At  , i = 1, . . . , d.  a    − Qθt  xi , ai  ,  Each component is updated based on how well it predicts the total future reward and how similar its associated state-action pair is to the state-action pair just visited. If the similarity is small, the impact of the error δt+1,i on the change of the component will also be small. The algorithm uses local step-size sequences,  αt,i; t ≥ 0 , i.e., one step-size for each of the components. Szepesvári and Smart  2004  prove that this algorithm converges almost surely as long as  i  the function-class Qθ satisﬁes the above interpolation property and the mapping θ  cid:24 → Qθ is a non-expansion  i.e.,  cid:6 Qθ − Qθ  cid:17  ∈ Rd ;  ii  the local step-size sequences  αt,i; t ≥ 0  are appropriately chosen and  iii  all regions of the state-action space X × A are “sufﬁciently visited” by   Xt , At  ; t ≥ 0 . They also provide error bounds on the quality of the action-value function learned. The heart of the analysis is that since θ  cid:24 → Qθ is a non-expansion, the algorithm implements an incremental approximate version of value iteration, with the underlying operator being a contraction. This is because a non-expansion applied after a contraction or a contraction applied after a non-expansion is a contraction. The idea of using non-expansions has ﬁrst appeared in the works of Gordon  1995  and Tsitsiklis and Van Roy  1996  in the study of ﬁtted value iteration.   cid:17  cid:6 ∞ holds for any θ, θ   cid:17  cid:6 ∞ ≤  cid:6 θ − θ  Fitted Q-iteration Fitted Q-iteration implements ﬁtted value iteration with action-value func- tions. Given the previous iterate, Qt, the idea is to form a Monte-Carlo approximation to Qt   x, a  at selected state-action pairs and then regress on the resulting points using one’s  T favorite regression method. Algorithm 14 shows the pseudocode of this method.  ∗  It is known that ﬁtted Q-iteration might diverge unless a special regressor is used  Baird, 1995; Boyan and Moore, 1995; Tsitsiklis and Van Roy, 1996 . Ormoneit and Sen  2002  suggest to use kernel averaging, while Ernst et al.  2005  suggest using tree based regressors. These are guaranteed to converge  say, if the same data is fed to the algorithm in each iteration  as they implement local averaging and as such results of Gordon  1995 ; Tsitsiklis and Van Roy  1996  are applicable to them. Riedmiller  2005  reports good empirical results with neural networks, at least when new observations obtained by following a policy greedy with respect to the latest iterate are incrementally added to the set of samples used in the updates.That the sample is changed is essential if no good initial policy is available, i.e., when in the initial sample states which are frequently visited by “good” policies are underrepresented  a theoretical argument for why this is important is given by Van Roy  2006  in the context of state aggregation .  Antos et al.  2007  and Munos and Szepesvári  2008  prove ﬁnite-sample performance bounds that apply to a large class of regression methods that use empirical risk minimization over a ﬁxed space F of candidate action-value functions. Their bounds depend on the worst-case Bellman   52  3. CONTROL  Algorithm 14 The function implementing one iteration of the ﬁtted Q-iteration algorithm. The function must be called until some criterion of convergence is met. appends the new item n to the list S and returns the modiﬁed list. The methods predict and regress are speciﬁc to the regression method chosen. The method predict z, θ   should return the predicted value at the input z given the regression parameters θ, while regress S , given a list of input-output pairs S, should implement a regression algorithm that solves the regression problem given by S and returns new parameters that can be used in predict. function FittedQ D, θ  Input: D =   Xi , Ai , Ri+1, Yi+1 ; i = 1, . . . , n  is a list of transitions, θ are the regressor param- eters 1: S ← []  cid:28  Create empty list 2: for i = 1 → n do  cid:28  Target at  Xi , Ai     cid:17 ∈A predict  Yi+1, a   , θ     cid:17   T ← Ri+1 + maxa S ← append S, cid:4  Xi , Ai  , T  cid:5    3: 4: 5: end for 6: θ ← regress S  7: return θ  error of F:   cid:18  cid:18    cid:18  cid:18   1 F   = sup ∗  e  Q∈F inf  cid:17 ∈F  Q   cid:17  − T  ∗  Q  Q  μ ,  ∗  ∗F def= {T  1 F   measures how ∗ where μ is the distribution of state-action pairs in the training sample. That is, e Q Q ∈ F}.The bounds derived have the form of the ﬁnite-sample bounds closeF is to T that hold in supervised learning  cf. Equation 2.17 , except that the approximation error is measured 1 F  . Note that in the earlier-mentioned counterexamples to the convergence of ﬁtted-value ∗ by e 1 F   = ∞, suggesting that it is the lack of ﬂexibility of the function approximation ∗ iteration, e method that causes divergence.  3.4 ACTOR-CRITIC METHODS Actor-critic methods implement generalized policy iteration. Remember that policy iteration works by alternating between a complete policy evaluation and a complete policy improvement step. When using sample-based methods or function approximation, exact evaluation of the policies may require inﬁnitely many samples or might be impossible due to the restrictions of the function-approximation technique. Hence, reinforcement learning algorithms simulating policy iteration must change the policy based on incomplete knowledge of the value function.  Algorithms that update the policy before it is completely evaluated are said to implement generalized policy iteration  GPI . In GPI, there are two closely interacting processes of an actor and a critic: the actor aims at improving the current policy, while the critic evaluates the current policy,   3.4. ACTOR-CRITIC METHODS 53   cid:1  cid:10  cid:11  cid:2  cid:4  cid:12   cid:1  cid:10  cid:11  cid:2  cid:4  cid:12    cid:2  cid:3  cid:4  cid:5  cid:6  cid:7    cid:1  cid:2  cid:3  cid:2  cid:4    cid:5  cid:6  cid:2  cid:7  cid:8  cid:9    cid:1    cid:8  cid:9  cid:10  cid:11  cid:6   cid:8  cid:9  cid:10  cid:11  cid:6    cid:14  cid:5  cid:15  cid:16  cid:3  cid:17    cid:12  cid:6  cid:13  cid:10  cid:13  cid:9  cid:12  cid:6  cid:13  cid:10  cid:13  cid:9    cid:1   Figure 3.2: The Actor-Critic Architecture  thus helping the actor. The interaction of the actor and the critic is illustrated on Figure 3.2 in a closed-loop learning situation.  Note that, in general, the policy that is used to generate the samples  i.e., the behavior policy  could be different from the one that is evaluated and improved in the actor-critic system  i.e., the target policy . This can be useful because the critic must learn about actions not preferred by the current target policy so that the critic can improve the target policy. This is impossible to achieve if the behavior policy is the same as the target policy and if the target policy is deterministic. This is one reason the target policy is usually a stochastic policy. However, even if the target policy is stochastic, the quality of the estimates of the values of low-probability actions can be very poor since less information is received for such actions. It might appear then that choosing actions completely at random might give the most information. However, this is clearly not the case since such a random policy might not visit the important parts of the state-space, as discussed before. Therefore, in practice, the behavior policy often mixes a certain  small  amount of exploration into the target policy.  There are many ways to implement an actor-critic architecture. If the action-space is small, the critic may, e.g., use an approximate action-value function and the actor could follow an ε-greedy or Boltzmann exploration strategy. If the action-space is large or continuous, the actor itself may use function-approximation.  Note that unlike perfect policy iteration, a GPI method may generate a policy that is sub- stantially worse than the previous one. Thus, the quality of the sequence of generated policies may oscillate or even diverge when the policy evaluation step is incomplete, irrespective of whether pol- icy improvement is exact or approximate  Bertsekas and Tsitsiklis, 1996, Example 6.4, p. 283 . In practice, GPI tends to generate policies that improve at the beginning. However, at later stages, the   54  3. CONTROL  policies often oscillate. A common practice, therefore, is to store the sequence of policies obtained and when learning is over, measure the performance of the stored policies by running some tests and then select the empirically best performing one.  Just like in the case of ﬁtted value iteration, the performance of actor-critic methods can be controlled by increasing the “ﬂexibility” of the function approximation methods. Finite-sample performance bounds are given by Antos et al.  2007  when both the actor and the critic use function approximation.  In the next section  Section 3.4.1 , we ﬁrst describe value estimation methods  used by the critic , while in Section 3.4.2, we describe some methods that implement policy improvement  used by the actor . In particular, we ﬁrst describe greedy methods of policy improvement, followed by a somewhat different idea when the actor uses gradient ascent on the performance function deﬁned by a parametric family of policies.  IMPLEMENTING A CRITIC  3.4.1 The job of the critic is to estimate the value of the current target policy of the actor. This is a value prediction problem. Therefore, the critic can use the methods described in Section 2. Since the actor needs action values, the algorithms are typically modiﬁed so that they estimate action values directly. When TD λ  is appropriately extended, the algorithm known as SARSA λ  is obtained. This is the ﬁrst algorithm that we describe below. When LSTD λ  is extended, we get LSTD-Q λ , which is described next. λ-LSPE could also be extended, but, for the sake of brevity, this extension is not discussed here.  SARSA In the case of ﬁnite  and small  state and action spaces, similarly to Q-learning, SARSA keeps track of the action-value underlying the possible state-action pairs  Rummery and Niranjan, 1994 :  t+1  − Q Xt , At  , δt+1 Q  = Rt+1 + γ Q Yt+1, A  cid:17  Qt+1 x, a  = Qt  x, a  + αt δt+1 Qt   I{x=Xt ,a=At},   3.2  Here  Yt+1, Rt+1  ∼ P0 · Xt , At   and A ∼ π ·Yt+1 . Compared to Q-learning, the difference is in the deﬁnition of the TD error. The algorithm got its name from its use of the current State, current Action, next Reward, next State, and next Action. When π is ﬁxed, SARSA is just TD 0  applied to state-action pairs. Hence, its convergence follows from the convergence results underlying TD 0 .   x, a  ∈ X × A.   cid:17  t+1  The multi-step extension of SARSA follows along the lines of the similar extension of TD 0 , giving rise to the SARSA λ  algorithm due to Rummery and Niranjan  1994 ; Rummery  1995 . The extension of the tabular algorithms to the case of function approximation follows along the same lines as the extension of TD λ . Algorithm 15 shows the pseudocode of SARSA λ  when it is used with linear function approximation. Being a TD-algorithm, the resulting algorithm is subject to the same limitations as TD λ   cf. Section 2.2.1 , i.e., it might diverge in off-policy situations. It is, however, possible to extend GTD2 and TDC to work with action values  and use λ > 0  so that the   3.4. ACTOR-CRITIC METHODS 55  Algorithm 15 The function implementing the SARSA λ  algorithm with linear function approxi- mation. This function must be called after each transition.  cid:17  function SARSALambdaLinFApp X, A, R, Y, A , θ, z  Input: X is the last state, A is the last action chosen, R is the immediate reward received when is chosen. θ ∈ Rd is the parameter vector of the linear   cid:17    cid:2   transitioning to Y , where action A function approximation, z ∈ Rd is the vector of eligibility traces ϕ[Y, A 1: δ ← R + γ · θ 2: z ← ϕ[X, A] + γ · λ · z 3: θ ← θ + α · δ · z 4: return  θ, z   ϕ[X, A]   cid:17 ] − θ   cid:2   resulting algorithms would become free of these limitations. For details consult  Maei and Sutton, 2010 .  LSTD-Q λ  When LSTD λ  is generalized to action-value functions, we get the LSTD-Q λ  algorithm, which solves  2.14 , where now ϕt = ϕ Xt , At  , ϕ : X × A → Rd, and  δt+1 θ   = Rt+1 + γ Vt+1 − Qθ  Xt , At  ,  where, assuming that the policy π to be evaluated is a stochastic policy, Vt+1 is given by π aYt+1 ϕ Yt+1, a  cid:5 .  π aYt+1 Qθ  Yt+1, a  =  cid:4  θ,  Vt+1 =   cid:5  a∈A   cid:5  a∈A   For deterministic policies this simpliﬁes to Vt+1 = Qθ  Yt+1, π Yt+1  .   cid:2  If the action space is large and stochastic policies are considered, evaluating the sums a∈A π ax ϕ x, a   or integrals in the case of continuous action spaces  might be infeasi- ∼ π ·Yt+1  and use  cid:17  t+1 ble. One possibility then is to sample actions from the policy π: A Vt+1 = Qθ  Yt+1, A = At+1,  cid:17   cid:17  t+1 . When the sample consists of trajectories of π, one may set A t+1  cid:2  which gives rise to “SARSA-like” version of LSTD-Q λ . An alternative, which is expected to produce better estimates, is to introduce some state features, ψ : X → Rd, restrict ϕ so that a∈A π ax ϕ x, a  = 0 holds for any state x ∈ X and deﬁne Qθ  x, a  = θ ψ  x . Hence, setting Vt+1 = Vθ  Yt+1  does not introduce any bias, while it is expected to reduce variance since  cid:17  Vt+1 does not depend on the randomness of A t+1  Peters et al., 2003; Peters and Schaal, 2008 .6 We will further discuss this choice in the next section.   ψ  x  + ϕ x, a  .Then Vθ  x  = cid:2   a∈A π ax Qθ  x, a  = θ   cid:2    cid:2   The pseudocode of LSTD-Q λ  is shown as Algorithm 16. Note that just like in the case of LSTD λ , the inverse in line 9 might not exist. Following standard steps, it is possible to derive a recursive version of LSTD-Q λ . 6 Peters et al.  2003 ; Peters and Schaal  2008  consider the special case when the parameters between the approximate state-value function Vθ and the action-value function are not shared.   56  3. CONTROL  Algorithm 16 The function implementing the LSTD-Q λ  algorithm with linear function approx- imation to evaluate a policy π. Note that if π is a deterministic policy than the sum in line 5 can be replaced by g ← ϕ[Yt+1, π Yt+1 ]. function LSTDQLambda D, π  Input: D =   Xt , At , Rt+1, Yt+1 ; t = 0, . . . , n − 1  is a list of transitions, π is the stochastic policy to be evaluated 1: A, b, z ← 0  cid:28  A ∈ Rd×d, b, z ∈ Rd 2: for t = 0 to n − 1 do f ← ϕ[Xt , At] z ← γ · λ · z + f g ← sum  π ·Yt+1  · ϕ[Yt+1,·]   A ← A + z ·  f − γ · g  b ← b + Rt+1 · f   cid:2   3: 4: 5: 6: 7: 8: end for 9: θ ← A 10: return θ  −1b  Finally, we note that the various TD-errors deﬁned in this section can also be used in the  SARSA algorithm.  IMPLEMENTING AN ACTOR  3.4.2 Policy improvement can be implemented in two ways: One idea is moving the current policy towards the greedy policy underlying the approximate action-value function obtained from the critic.Another idea is to perform gradient ascent directly on the performance surface underlying a chosen parametric policy class. In the next sections, we describe speciﬁc methods that implement these ideas.  Greedy improvements The closest to policy iteration is to let the critic evaluate the current policy based on a lot of data and then switch to the policy that is greedy with respect to the obtained action- value function. Notice that if the action space is ﬁnite, the action choices of the greedy policy can be computed “on the ﬂy”  on an as needed basis , i.e., the greedy policy does not need to be explicitly computed or stored, making it possible to use this algorithm in very large, or inﬁnite state spaces. If the policy is evaluated by LSTD-Q 0 , this strategy gives rise to the LSPI  least-squares policy iteration  algorithm of Lagoudakis and Parr  2003 . The variant that uses LSTD-Q λ  to evaluate policies with a batch of data is shown as Algorithm 17.  Finite-sample performance bounds for LSPI and generalizations of  it are obtained by Antos et al.  2008 . Antos et al.  2007  extend these results to continuous action spaces, where   3.4. ACTOR-CRITIC METHODS 57  Algorithm 17 The function implementing the LSPI λ  algorithm with linear function approxima- tion. In practice, the convergence criterion is often replaced by some other criterion. GreedyPolicy θ  should return a function that takes as arguments a pair of the form  x, a  and return 1 for the action that maximizes θ function LSPI D, ε  Input: D =   Xt , At , Rt+1, Yt+1 ; t = 0, . . . , n − 1  is a list of transitions, ε is an accuracy param-  ψ  x,· , while it returns 0, otherwise.   cid:2   eter  cid:17  ← 0 1: θ 2: repeat θ ← θ 3:  cid:17  ← LSTDQLambda D, GreedyPolicy θ   4: 5: until  cid:6 θ − θ 6: return θ   cid:17  cid:6  > ε  θ   cid:17   given the current action value function Q, the next policy is chosen to maximize  ρQ,π =  μ x    cid:5  x∈X    A Q x, a  π dax   over a restricted policy class. They argue for the necessity of restricting the policies to prevent overﬁtting in this case.  The methods mentioned above switch policies without enforcing continuity. This may be dangerous when the action-value function estimate of the last policy is inaccurate since if the new policy is radically different than the previous one, it might be hard for the algorithm to recover from this “failure”. In such cases, incremental changes might work better. One way to ensure incremental changes is to update the parameters ω of a parametric policy- class  πω; ω ∈ Rdω   by performing stochastic gradient ascent on ρQ,πω  e.g., Bertsekas and Tsitsiklis, 1996, p. 317; Kakade and Langford, 2002 considers such incremental updates when the policies are given in a tabular form . An indirect way of performing  approximately  greedy updates is to choose the target policy to be an ε-greedy policy  or a Boltzmann-policy  corresponding to the current action-value function. Perkins and Precup  2003  analyze this choice with linear function approximation and when the behavior and target policies are the same. They prove the following result: Let  cid:20  be the mapping of action-value functions to policies that deﬁnes the policy updates. Assume that  i  the exact TD 0  solution is obtained in each iteration and  ii   cid:20  is globally Lipschitz with a Lipschitz constant smaller than c M  and the image space of  cid:20  contains only ε-soft policies  with some ﬁxed ε > 0 . Then the sequence of policies generated by the algorithm converges almost surely. The Lipschitzness property means that  cid:6  cid:20 Q1 −  cid:20 Q2 cid:6  ≤ L cid:6 Q1 − Q2 cid:6  holds for all action- value functions, where both norms are the  unweighted  2-norms. The constant c M  depends on the MDP M. A policy π is called ε-soft, if π ax  ≥ ε holds for all x ∈ X , a ∈ A. More recently, Van Roy  2006  obtained non-trivial performance bounds for state aggregation for a similar setting.   58  3. CONTROL  The methods discussed so far update the policy quite infrequently. An alternative is to inter- leave the updates of the policy and the value function. Singh et al.  2000  prove asymptotic consis- tency when GLIE  greedy in the limit with inﬁnite exploration  policies are followed and tabular SARSA 0  is used as the critic.  Policy gradient In this section,we review policy gradient methods  for a sensitivity-based approach, see Cao, 2007 . These methods perform stochastic gradient ascent on the performance surface in- duced by a smoothly parameterized policy class  cid:7  =  πω; ω ∈ Rdω   of stochastic stationary policies. When the action space is ﬁnite, a popular choice is to use the so-called Gibbs policies:  πω ax  =   cid:2    cid:2   exp ω  cid:17 ∈A exp ω   cid:2   a  ξ x, a     cid:17   ,  ξ x, a      x ∈ X , a ∈ A.  Here ξ : X × A → Rdω is an appropriate feature-extraction function. If the action space is a subset of a dA-dimension Euclidean space, a popular choice is to use Gaussian policies when given some parametric mean gω x, a  and covariance  cid:22 ω x, a  functions; the density specifying the action- selection distribution under ω is deﬁned by  πω ax  =   cid:19   1   2π  dAdet  cid:22 ω x, a     cid:20   − a − gω x, a    cid:2   ω  x, a   a − gω x, a  −1   cid:22   .  exp   cid:21   Care must be taken to ensure that  cid:22 ω is positive deﬁnite. For simplicity,  cid:22 ω is often taken to be  cid:22 ω = βI with some β > 0.  Given  cid:7 , formally, the problem is to ﬁnd the value of ω corresponding to the best performing  policy:  ρω =?  argmax  ω  Here, the performance, ρω, can be measured by the expected return of policy πω, with respect to some initial distribution over the states.7 The initial distribution can be the stationary distribution underlying the policy chosen, in which case maximizing ρω will be equivalent to maximizing the long-run average reward  Sutton et al., 1999a .  The policy gradient theorem Assume that the Markov chain resulting from following any policy πω is ergodic, regardless of the choice of ω. The question is how to estimate the gradient of ρω.  Let ψω : X × A → Rdω be the score function underlying πω:  ψω x, a  = ∂  log πω ax ,   x, a  ∈ X × A.   cid:2  For example, in the case of Gibbs policies, the score function takes the form ψω x, a  = ξ x, a  −  ∂ω   cid:17 ∈A πω a  a   cid:17 x ξ x, a   cid:17    .  7An overall best policy, as measured by the value function, might not exist within the restricted class  cid:7 .    cid:4   Deﬁne  G ω  = cid:24    cid:25   Qπω  X, A  − h X   ψω X, A .   3.3   3.4. ACTOR-CRITIC METHODS 59  Here  X, A  is a sample from the stationary state-action distribution underlying policy πω, Qπω is the action-value function of πω and h is an arbitrary bounded function. According to the policy gradient theorem  see, e.g., Bhatnagar et al., 2009 and the references therein , G ω  is an unbiased estimate of the gradient:  Let  Xt , At   be a sample from the stationary distribution underlying πωt . Then, for  βt; t ≥ 0 , the update rule  ∇ωρω = E [G ω ] .  cid:25  ˆGt = cid:24  ˆQt  Xt , At   − h Xt   ωt+1 = ωt + βt  ψω Xt , At  ,   3.4   ˆGt ,  cid:16  ˆQt  Xt , At  ψωt  Xt , At    cid:17  implements stochastic gradient ascent as long as   cid:3   = E  .  E  Qπωt  X, A ψωt  Xt , At     3.5  The role of h in  3.4  is to reduce the variance of the gradient estimate ˆGt so as to speed up the rate of convergence of the algorithm. Although a good choice can only gain a constant factor in terms of speeding up convergence, in practice, the gain can be substantial. One choice is h = V πωt , i.e., the value function underlying policy πωt . Although this will not explicitly minimize the variance of ˆGt  nor that of G ωt   , it is still expected to reduce the variance compared to using h = 0 and is thus generally recommended. Of course, the value function of the current policy will normally not be available, but it must be estimated. This can be done together with constructing an estimator ˆQt, as we shall see soon. As the update rule  3.4  is an instance of stochastic gradient ascent, the sequence  ωt   will converge almost surely to some local optimum of ρω, provided that the step-size sequence  βt; t ≥ 0  satisﬁes the RM conditions and the problem is sufﬁciently regular  in general, though, only convergence to a stationary point of ρω can be proven . The difﬁculty in implementing  3.4  is twofold:  i  One needs to construct an appropriate estimator ˆQt  and possibly h ;  ii  The random variables  Xt , At   must come from the stationary distribution of π ωt . In episodic problems, these difﬁculties can be addressed by updating the param- eters at the end of the episodes, giving rise to Williams’ REINFORCE algorithm  Williams, 1987 . Note that REINFORCE is a direct policy search algorithm as it does not use value functions. It is also a member of the family of likelihood ratio methods Glynn, 1990. In non-episodic problems, a two-timescale algorithm can be used that constructs on estimator ˆQt on the faster timescale using an appropriate value-function-estimation method and updates the policy parameters on the slower timescale. We now describe an interesting proposal to implement this, due to Sutton et al.  1999a  and Konda and Tsitsiklis  1999 .   60  3. CONTROL  Compatible function approximation Assume that a linear-in-the-parameters function approxima- tion is used to estimate ˆQt, but choose the feature-extraction function to be the score function underlying the policy class:  Qθ  x, a  = θ   cid:2   ψω x, a ,   x, a  ∈ X × A.   3.6   This choice of the function approximation method is called compatible with the policy parameteri- zation. Note that the basis functions depend on ω  as ω = ωt changes, ψω will also change . What is a suitable value of θ for a ﬁxed value ωt? Substituting Qθ for ˆQt in  3.5 , we get  cid:4    cid:2  cid:17  , gω = E [Qπω  X, A ψω X, A ] and let θ∗ ω  be the solu-  Qπωt  Xt , At  ψωt  Xt , At    ψωt  Xt , At  ψωt  Xt , At    θ = E   cid:2  cid:4    cid:16    cid:3    cid:3   Deﬁne Fω = E ψω X, A ψω X, A  tion to the linear system of equations  E  .  Fω θ = gω.  When this equation holds, Qθ∗ ωt   satisﬁes  3.5 . Notice that θ∗ ω  is the parameter that minimizes the mean-squared error   cid:16    cid:17    Qθ  X, A  − Qπω  X, A  2  .  E  The above derivations suggest the following closed-loop learning algorithm:  i  at any time t policy πωt is followed;  ii  θt is updated on the faster timescale  say  by an appropriate version of SARSA 1   iii  the policy parameters are updated on the slower timescale by  ωt+1 = ωt + βt  Qθt  Xt , At   − h Xt    ψωt  Xt , At  .   3.7    cid:25    cid:24   Algorithm 18 shows the corresponding pseudocode. Konda and Tsitsiklis  2003  proved that  under some regularity conditions  lim inf t→∞ ∇ωρωt = 0 holds almost surely if the average-cost version of SARSA 1  is used to update θt. They have also shown that if SARSA λ  is used and mλ = lim inf t→∞ ∇ωρωt then limλ→1 mλ = 0. Natural actor-critic Another possible update rule is  ωt+1 = ωt + βt θt ,   3.8   which deﬁnes the natural actor-critic  NAC  algorithm  the pseudocode of the resulting algorithm differs from that of Algorithm 18 only in that in line 9 the update of ω should be replaced by ω ← ω + β · θ . Assuming that Fω is positive deﬁnite, since gω = ∇ωρω and θ∗ ω  = F ∇ωρω, ∇ωρω > 0 unless ∇ωρω = 0. This shows that the above we see that θ∗ ω  algorithm implements a stochastic pseudo-gradient algorithm, and thus it converges under the same conditions as  3.7 .   cid:2 ∇ωρω = ∇ωρ   cid:2  ω F  −1  −1  ω  ω  Interestingly, the NAC update result in a faster convergence rate than the previous rule. The reason is that θ∗ ω  can be shown to be a so-called natural gradient  Amari, 1998  of ρω. This   3.4. ACTOR-CRITIC METHODS 61  Algorithm 18 An actor-critic algorithm that uses compatible function approximation and SARSA 1 . function SARSAActorCritic 1: ω, θ, z ← 0 2: A ← a1 3: repeat  R, Y   ← ExecuteInWorld  A   4:  cid:17  ← Draw πω Y,·   5:  θ, z  ← SARSALambdaLinFApp X, A, R, Y, A  cid:17  6: ψ ← ∂ ω ← ω + β · cid:24  ∂ω log πω X, A  7: v ← sum  πω Y,·   · θ  cid:2  8: 9: X ← Y 10: A ← A  cid:17  11: 12: until True   cid:28  Use λ = 1 and α  cid:29  β   cid:25  · ψ ϕ[X,·]    ϕ[X, A] − v  , θ, z    cid:2   A  θ  was ﬁrst noted by Kakade  2001 . Following a natural gradient means that the algorithm performs gradient ascent directly in a metric space underlying the objects of interest, in this case in the space of stochastic policies  with an appropriate metric , as opposed to performing gradient ascent in the  Euclidean  metric space of the parameters  note that the deﬁnition of a gradient is dependent on the metric used . In particular, that θ∗ ω  is a natural gradient implies that the actual parameterization becomes irrelevant in the sense that the trajectories underlying the ODE ˙ω = θ∗ ω  are invariant to arbitrary smooth equivalent reparameterizations of the policy class  πω; ω ∈ Rdω  . In the case of the Gibbs policy class, a non-singular linear transformation of the features is a simple example for such a reparameterization. Because of this invariance property, a natural gradient is said to be covariant. It is believed that following a natural gradient generally improves the behavior of gradient ascent methods. This is nicely demonstrated by Kakade  2001  on a simple two-state MDP, where the “normal” gradient is very small in a large part of the parameter space, while the natural gradient behaves in a reasonable manner. Other positive examples were given by Bagnell and Schneider  2003 ; Peters et al.  2003  and Peters and Schaal  2008 .  As to the estimation of θ   ω , Peters and Schaal  2008   and earlier Peters et al.  2003   sug- gest to use LSTD-Q λ . In particular, they suggested using both state features and the compatible state-action features as described in Section 3.4.1  note, however, that only λ = 1 gives an unbiased estimate of the gradient . Their algorithm keeps the value of ωt ﬁxed until the parameter θt as calcu- lated by LSTD-Q λ  stabilizes. When this happens ωt is updated by  3.8  and the internal statistics collected by LSTD-Q λ  is “discounted” by a discount factor 0 < β < 1. They also observe that the original actor-critic  Barto et al., 1983; Sutton, 1984  when used in a ﬁnite MDP with no function approximation implements a NAC update. More recently, Bhatnagar et al.  2009  proposed several  ∗   62  3. CONTROL  two-timescale algorithms and proved the convergence of the policy parameters to a neighborhood of the local maxima of the objective function when the critic uses TD 0 -like updates.   C H A P T E R 4  For Further Exploration  63  Inevitably, due to space constraints, this review must miss a large portion of the reinforcement learning literature.  FURTHER READING  4.1 One topic of particular interest not discussed is efﬁcient sampling-based planning  Kearns et al., 1999; Szepesvári, 2001; Kocsis and Szepesvári, 2006; Chang et al., 2008 . The main lesson here is that off-line planning in the worst-case can scale exponentially with the dimensionality of the state space  Chow and Tsitsiklis, 1989 , while online planning  i.e., planning for the “current state”  can break the curse of dimensionality by amortizing the planning effort over multiple time steps  Rust, 1996; Szepesvári, 2001 .  of  topics  Other  include  interest  approaches  de Farias and Van Roy, 2003, 2004, 2006 , dual dynamic programming  Wang et al., 2008 , techniques based on sample average approximation  Shapiro, 2003  such as PEGASUS  Ng and Jordan, 2000 , online learning in MDPs with arbitrary reward processes  Even-Dar et al., 2005; Yu et al., 2009; Neu et al., 2010 , or learning with  almost  no restrictions in a competitive framework  Hutter, 2004 .  programming-based  linear  the  Other important topics include learning and acting in partially observed MDPs  for re- cent developments, see, e.g., Littman et al., 2001; Toussaint et al., 2008; Ross et al., 2008 , learn- ing and acting in games or under some other optimization criteria  Littman, 1994; Heger, 1994; Szepesvári and Littman, 1999; Borkar and Meyn, 2002 , or the development of hierarchical and multi-time-scale methods  Dietterich, 1998; Sutton et al., 1999b .  reinforcement  learning in games  successful applications of  4.2 APPLICATIONS learning include  in no particu- The numerous  e.g., Backgammon  Tesauro, 1994  and Go  Silver et al., lar order  2007  , applications in networking  e.g., packet routing  Boyan and Littman, 1994 , chan- research problems nel allocation  Singh and Bertsekas, 1997  , applications  e.g., targeted marketing  Abe et al., 2004 , maintenance problems  Gosavi, 2004 , job-shop scheduling  Zhang and Dietterich, 1995 , elevator control  Crites and Barto, 1996 , pricing  Rusmevichientong et al., 2006 , vehicle routing  Proper and Tadepalli, 2006 , inventory control  Chang et al., 2007 , ﬂeet management  Simão et al., 2009  , learning in robotics  e.g., control- ling quadrupedales  Kohl and Stone, 2004 , humanoid robots  Peters et al., 2003 , or helicopters  to operations   64  4. FOR FURTHER EXPLORATION   Abbeel et al., 2007  , and applications to ﬁnance  e.g., option pricing  Tsitsiklis and Van Roy, 1999b, 2001; Yu and Bertsekas, 2007; Li et al., 2009  . For further applications, see the lists at the URLs   cid:129  http:  www.cs.ualberta.ca ˜szepesva RESEARCH RLApplications.html and   cid:129  http:  umichrl.pbworks.com Successes-of-Reinforcement-Learning.  SOFTWARE  4.3 There are numerous software packages that support the development and testing of RL algorithms. Perhaps, the most notable of these are the RL-Glue and RL-Library packages. The RL-Glue package available from http:  glue.rl-community.org is intended for helping to standardize RL experiments. It is a free, language-neutral software package that implements a standardized RL interface  Tanner and White, 2009 . The RL-Library  http:  library.rl-community.org  builds on the top of RL-Glue. Its purpose is to provide trusted implementations of various RL testbeds and algorithms. The most notable other RL software packages are CLSquare,1 PIQLE,2 RL Toolbox,3 JRLF4 and LibPG.5 These offer the implementation of a large number of algorithms, testbeds, intuitive visualizations, programming tools, etc. Many of these packages support RL-Glue.  1http:  www.ni.uos.de index.php?id=70 2http:  piqle.sourceforge.net  3http:  www.igi.tugraz.at ril-toolbox  4http:  mykel.kochenderfer.com ?page_id=19 5http:  code.google.com p libpgrl    A P P E N D I X A  65  The Theory of Discounted  Markovian Decision Processes  The purpose of this section is to give a short proof of the basic results of the theory of Markovian decision processes. All the results will be worked out for the discounted expected total cost criterion. First, we give a short overview of contraction mappings and Banach’s ﬁxed-point theorem. Next, we show how this powerful result can be applied to proof a number of basic results about value functions and optimal policies.  A.1 CONTRACTIONS AND BANACH’S FIXED-POINT  THEOREM  We start with some basic deﬁnitions that we will need in the rest of this section. Deﬁnition A.1  Norm . Let V be a vector space over the reals. Then f : V → R V provided that the following hold:  + 0 is a norm on  1. If f  v  = 0 for some v ∈ V then v = 0; 2. For any v, u ∈ V , f  v + u  ≤ f  v  + f  u .  A vector space together with a norm is called a normed vector space. According to the deﬁnition a  norm is a function that assigns a nonnegative number to each vector. This number is often called the “length” or just the “norm” of the vector. The norm of a vector v is often denoted by  cid:6 v cid:6 . Example A.2 Here are a few examples of norms over the vector space V =  Rd ,+, λ· .  1.  cid:16 p norms: For p ≥ 1,  ∞  2.  cid:16   norms:   cid:28   d cid:5  i=1   cid:6 v cid:6 p =  vip   cid:31 1 p  .   cid:6 v cid:6 ∞ = max 1≤i≤d  vi.   66 A. THE THEORY OF DISCOUNTED MARKOVIAN DECISION PROCESSES  3. The weighted variants of these norms are deﬁned as follows:   cid:20  cid:2   ⎧⎨ ⎩  d  i=1  max1≤i≤d  vip vi  wi  ,  wi   cid:21 1 p  ,   cid:6 v cid:6 p =  if 1 ≤ p < ∞; if p = ∞,  where wi > 0.  4. The matrix-weighted 2-norm is deﬁned as follows:   cid:6 v cid:6 2  P  = vT P v.  Here P is a ﬁxed, positive deﬁnite matrix.  Similarly, one can deﬁne norms over spaces of functions. For example, if V is the vector space of functions over the domain X which are uniformly bounded then   cid:6 f cid:6 ∞ = sup x∈X  f  x .   A function is called uniformly bounded exactly when  cid:6 f cid:6 ∞ < +∞.  We will be interested in the  convergence of sequences in normed vector spaces. Deﬁnition A.3  Convergence in norm . Let V =  V , cid:6  ·  cid:6   be a normed vector space.Let vn ∈ V be a sequence of vectors  n ∈ N . The sequence  vn; n ≥ 0  is said to converge to the vector v in the norm  cid:6  ·  cid:6  if limn→∞  cid:6 vn − v cid:6  = 0. This will be denoted by vn → cid:6 · cid:6  v. Note that in a d- dimensional vector space vn → cid:6 · cid:6  v is the same as requiring that for each 1 ≤ i ≤ d, vn,i → vi  here vn,i denotes the ith component of vn . However, this does not hold for inﬁnite dimensional vector spaces. Take for example X = [0, 1] and the space of uniformly bounded functions over X . Let   cid:6   fn x  =  1, 0,  if x < 1 n; otherwise .  Then fn x  → 0 for each x  i.e., fn converges to f  x  ≡ 0 pointwise . However,  cid:6 fn − f cid:6 ∞ =  cid:6 fn cid:6 ∞ = 1  cid:20 → 0. If we have a sequence of real-numbers  an; n ≥ 0 , we can test if the sequence converges without the knowledge of the limiting value by verifying if it is a Cauchy sequence, i.e., whether an − am = 0.  ‘Sequences with vanishing oscillations’ is possibly a more descrip- limn→∞ supm≥n tive name for Cauchy sequences.  It is a quite notable property of the real numbers that every Cauchy sequence of reals assumes a limit.  The extension of the concept of Cauchy sequences to normed vector spaces is straightforward:   A.1. CONTRACTIONS AND BANACH’S FIXED-POINT THEOREM 67  Deﬁnition A.4 Cauchy sequence. Let  vn; n ≥ 0  be a sequence of vectors of a normed vector- space V =  V , cid:6  ·  cid:6  . Then vn is called a Cauchy-sequence if limn→∞ supm≥n   cid:6 vn − vm cid:6  = 0.  Normed vector spaces where all Cauchy sequences are convergent are special: one can ﬁnd examples of normed vector spaces such that some of the Cauchy sequences in the vector space do not have a limit.  Deﬁnition A.5 Completeness. A normed vector space V is called complete if every Cauchy sequence in V is convergent in the norm of the vector space.  To pay tribute to Banach, the great Polish mathematicians of the ﬁrst half of the 20th century,  we have the following deﬁnition:  Deﬁnition A.6 Banach space. A complete, normed vector space is called a Banach space.  One powerful result in the theory of Banach spaces concerns contraction mappings, or con-  traction operators. These are special Lipschitzian mappings:  Let V =  V , cid:6  ·  cid:6   be a normed vector space. A mapping T : V → V is called  Deﬁnition A.7 L-Lipschitz if for any u, v ∈ V ,   cid:6 T u − T v cid:6  ≤ L cid:6 u − v cid:6 .  A mapping T is called a non-expansion if it is Lipschitzian with L ≤ 1. It is called a contraction if it is Lipschitzian with L < 1. In this case L is called the contraction factor of T and T is called an L-contraction.  Note that if T is Lipschitz, it is also continuous in the sense that if vn → cid:6 · cid:6  v then also  T vn → cid:6 · cid:6  T v. This is because  cid:6 T vn − T v cid:6  ≤ L cid:6 vn − v cid:6  → 0 as n → ∞. Deﬁnition A.8 Fixed point. Let T : V → V be some mapping.The vector v ∈ V is called a ﬁxed point of T if T v = v.  Theorem A.9 Banach’s ﬁxed-point theorem. Let V be a Banach space and T : V → V be a contraction mapping. Then T has a unique ﬁxed point. Further, for any v0 ∈ V , if vn+1 = T vn then vn → cid:6 · cid:6  v, where v is the unique ﬁxed point of T and the convergence is geometric:   cid:6 vn − v cid:6  ≤ γ n cid:6 v0 − v cid:6 .   68 A. THE THEORY OF DISCOUNTED MARKOVIAN DECISION PROCESSES  Proof. Pick any v0 ∈ V and deﬁne vn as in the statement of the theorem. We ﬁrst demonstrate that  vn  converges to some vector. Then we will show that this vector is a ﬁxed point of T . Finally, we show that T has a single ﬁxed point. Assume that T is a γ -contraction.  To show that  vn  converges it sufﬁces to show that  vn  is a Cauchy sequence  since V is a  Banach, i.e., complete normed vector-space . We have  cid:6 vn+k − vn cid:6  =  cid:6 T vn−1+k − T vn−1 cid:6  ≤ γ cid:6 vn−1+k − vn−1 cid:6  = γ cid:6 T vn−2+k − T vn−2 cid:6  ≤ γ 2 cid:6 vn−2+k − vn−2 cid:6  ...≤ γ n cid:6 vk − v0 cid:6  ≤ γ n   cid:6 vk cid:6  +  cid:6 v0 cid:6   .  Now,   cid:6 vk cid:6  ≤  cid:6 vk − vk−1 cid:6  +  cid:6 vk−1 − vk−2 cid:6  + . . . +  cid:6 v1 − v0 cid:6  and by the same logic as used before  cid:6 vi − vi−1 cid:6  ≤ γ i−1 cid:6 v1 − v0 cid:6 . Hence,   cid:20    cid:6 vk cid:6  ≤   cid:21   γ k−1 + γ k−2 + . . . + 1  cid:29    cid:6 vn+k − vn cid:6  ≤ γ n   cid:6 v1 − v0 cid:6  ≤ 1 1 − γ   cid:6 v1 − v0 cid:6 .  cid:30    cid:6 v1 − v0 cid:6  +  cid:6 v0 cid:6   1 1 − γ  Thus,  and so   cid:6 vn+k − vn cid:6  = 0,  n→∞ sup lim k≥0  showing that  vn; n ≥ 0  is indeed a Cauchy sequence. Let v be its limit. Now, let us go back to the deﬁnition of the sequence  vn; n ≥ 0 :  vn+1 = T vn.  Taking the limes of both sides, on the one hand, we get that vn+1 → cid:6 · cid:6  v. On the other hand, T vn → cid:6 · cid:6  T v, since T is a contraction, hence it is continuous. Thus, the left-hand side converges to v, while the right-hand side converges to T v, while the left and right-hand sides are equal.Therefore, we must have v = T v, showing that v is a ﬁxed point of T .  cid:17  Let us consider the problem of uniqueness of the ﬁxed point of T . Let us assume that v, v are both ﬁxed points of T . Then,  cid:6 v − v  cid:17  cid:6  ≤ 0.  cid:17  cid:6  ≤ γ cid:6 v − v  cid:17  cid:6  =  cid:6 T v − T v Since a norm takes only nonnegative values and γ < 1, we get that  cid:6 v − v  cid:17  = 0, or v = v   cid:17  cid:6 , or  1 − γ   cid:6 v − v  cid:17  cid:6  = 0. Thus, v − v  .   cid:17    Finally,  A.2. APPLICATION TO MDPS 69   cid:6 vn − v cid:6  =  cid:6 T vn−1 − T v cid:6  ≤ γ cid:6 vn−1 − v cid:6  = γ cid:6 T vn−2 − T v cid:6  ≤ γ 2 cid:6 vn−2 − v cid:6  ...≤ γ n cid:6 v0 − v cid:6 .   cid:2   A.2 APPLICATION TO MDPS ∗ by For the purpose of this section, we deﬁne V  x  = sup π∈ cid:7 stat  ∗  V  ∗  V π  x ,  x ∈ X .   x  is an upper bound on the value that we can achieve by choosing some stationary policy Thus, V π. Note that if the supremum was taken over the larger class of all policies, we could possibly get a larger function. However, in the case of MDPs considered in this section, these two optimal value functions are actually the same. Although, this is not hard to prove, we omit the proof.  Let B X   be the space of uniformly bounded functions with domain X :  B X   = { V : X → R :  cid:6 V cid:6 ∞ < +∞} .  In what follows, we will view B X   as a normed-vector space with the norm  cid:6  ·  cid:6 ∞. It is easy to show that  B X  , cid:6  ·  cid:6 ∞  is complete: If  Vn; n ≥ 0  is a Cauchy sequence in it then for any x ∈ X ,  Vn x ; n ≥ 0  is also a Cauchy sequence over the reals. Denoting by V  x  the limit of  Vn x  , one can show that  cid:6 Vn − V cid:6 ∞ → 0. Vaguely speaking, this holds because  Vn; n ≥ 0  is a Cauchy sequence in the norm  cid:6  ·  cid:6 ∞ so the rate of convergence of Vn x  to V  x  is independent of x. Pick any stationary policy π. Remember that the Bellman operator underlying π, T π : B X   → B X  , is deﬁned by   T π V   x  = r x, π x   + γ  P x, π x , y V  y ,  x ∈ X .  Note that T π is well-deﬁned: If U ∈ B X  , then T π U ∈ B X   holds true.  It is easy to see that V π as deﬁned by  1.7  is a ﬁxed point to T π :  V π  x  = E [R1X0 = x] + γ  P x, π x , y E  γ t Rt+2X1 = y   cid:9    cid:7  ∞ cid:5   t=0  = T π V  x .   cid:5  y∈X   cid:5  y∈X   70 A. THE THEORY OF DISCOUNTED MARKOVIAN DECISION PROCESSES  It is also easy to see that T π is a contraction in  cid:6  ·  cid:6 ∞:  cid:8  cid:8  cid:8  cid:5   cid:5  y∈X  cid:5  y∈X y∈X   cid:6 T π U − T π V cid:6 ∞ = γ sup x∈X ≤ γ sup x∈X ≤ γ sup x∈X = γ  cid:6 U − V cid:6 ∞, y∈X P x, π x , y  = 1.  where the last line follows from   cid:2    cid:8  cid:8  cid:8   P x, π x , y  U  y  − V  y   P x, π x , y U  y  − V  y  P x, π x , y  cid:6 U − V cid:6 ∞  It follows that in order to ﬁnd V π , one can construct the sequence V0, T π V0,  T π  2V0, . . .,  which, by Banach’s ﬁxed-point theorem will converge to V π at a geometric rate.  Now, recall the deﬁnition of the Bellman optimality operator: T  ∗ : B X   → B X  , ⎫⎬ ⎭ ,  x ∈ X .   A.1   P x, a, y V  y    T  ∗  V   x  = sup a∈A  ⎧⎨ ⎩r x, a  + γ   cid:5  y∈X  ∗  is well-deﬁned.We now show that T  is also a γ -contraction with respect to the supremum  ∗  Again, T norm  cid:6  ·  cid:6 ∞.  To see this ﬁrst note that a∈A f  a  − sup  sup  a∈A g a  ≤ sup a∈A  f  a  − g a ,  which can be seen using an elementary case analysis. Using this inequality and then proceeding as with the analysis of T π we get,   cid:6 T  ∗  U − T  ∗  sup   cid:5  V cid:6 ∞ ≤ γ  cid:5  y∈X ≤ γ  x,a ∈X×A y∈X = γ  cid:6 U − V cid:6 ∞,   x,a ∈X×A  sup  P x, a, y U  y  − V  y  P x, a, y  cid:6 U − V cid:6 ∞   cid:2  y∈X P x, a, y  = 1.  thus proving the statement. Here, the last equality follows by  The main result of this section is the following theorem:  Theorem A.10 V : T π V = T ∗  V . Then V = V  ∗  and π is an optimal policy.  Let V be the ﬁxed point of T  and assume that there is policy π which is greedy w.r.t  ∗   A.2. APPLICATION TO MDPS 71  T  ∗  ∗  ∗  ∗  V π . Since T  V π , i.e., V π ≤ T  in the sense that for any function V ∈ B X  , V holds  U ≤ V means that U  x  ≤ V  x  holds for any x ∈ X  . Thus, V π = T π V π ≤ V follows from U ≤ V , we also have T  2V π .  2V π . Continuing this way, we get for all n ≥ 0 that ∗ is a contraction, the right-hand side converges to V , the unique ﬁxed or not . Thus, V π ≤ V . Since π was arbitrary,  Proof. Pick any stationary policy π. Then T π ≤ T T π V ≤ T U ≤ T ∗ ∗ Chaining the inequalities, we get V π ≤  T V π ≤  T point of T we get that V V . Since V is the ﬁxed-point of T T π V = V . Since T π has a unique ﬁxed point, V π , we have V π = V , showing that V that π is an optimal policy.  ∗  nV π . Since T  at this stage we cannot know if V = V ∗ ∗ ≤ V . Pick now a policy π such that T π V = T ∗  ∗ , we have ∗ = V and  V π ≤  T   cid:2   ∗  ∗  ∗  ∗  In the statement of the theorem, we were careful in assuming that a greedy policy w.r.t. V exists. Note that this always holds for ﬁnite action spaces, and it will hold for inﬁnite action spaces under some extra  continuity  assumptions.  The following theorem serves as the basis of the policy iteration algorithm:  Theorem A.11 Policy improvement theorem. Choose some stationary policy π0 and let π be greedy w.r.t. V π0: T π V π0 = T V π0. Then V π ≥ V π0, i.e., π is an improvement upon π0. In particular, if ∗ V π0  x  > V π0  x  for some state x then π strictly improves upon π0 at x: V π  x  > V π0  x . On the  ∗  T other hand, when T  ∗  V π0 = V π0 then π0 is an optimal policy.  ∗  For the second part, notice that we have  T π  nV π0  x  ≥ T  Proof. We have T π V π0 = T V π0 ≥ T π0 V π0 = V π0. Applying T π to both sides, we get  T π  2V π0 ≥ T π V π0 ≥ V π0. Continuing this way, we get that for any n ≥ 0,  T π  nV π0 ≥ V π0. Taking the limit of both sides, we get that V π ≥ V π0. the limit, we have V π  x  ≥ T is a contraction, it has a single ﬁxed point, V .Thus V = V π0. But we also know that V π0 ≤ V Hence, π0 must be an optimal policy.  V π0  x  > V π0  x . Hence, taking ∗ . Since T ∗ ≤ V .  V π0  x  > V π0  x . ∗ The third part is proven as follows: Since T  V π0 = V π0, V π0 is a ﬁxed point of T   cid:2   ∗  ∗  ∗  The policy iteration procedure generates a sequence of policy π1, π2, . . . such that πi is greedy w.r.t. V πi−1, i = 1, 2, . . .. Let us assume further that when choosing a greedy policy, if no improvement is possible, we keep the previous policy and stop the iteration.  We have the following immediate corollary:  If the MDP is ﬁnite, the policy iteration procedure terminates in a ﬁnite number of steps Corollary A.12 and returns an optimal policy. Further, a stationary policy of an MDP is optimal if and only if its value function is a ﬁxed point of T  ∗  .   72 A. THE THEORY OF DISCOUNTED MARKOVIAN DECISION PROCESSES  Proof. From the previous theorem, we know that the sequence of policies is strictly improving. Since in a ﬁnite MDP there are a ﬁnite number of policies, the procedure must thus terminate. When the procedure terminates, for the ﬁnal policy π, we have T V π = T π V π = V π . Thus, by the last part of the previous theorem, π is an optimal policy.  The second part follows immediately from Theorem A.11.   cid:2   Let V be the unique ﬁxed point of T  Corollary A.13 optimal policy. Further, if there exists an optimal stationary policy π ∗ greedy w.r.t. V  .  . Then any policy that is greedy w.r.t. V is an is  then V = V  and the policy π  ∗  ∗  ∗  ∗  Proof. The ﬁrst part follows immediately from Theorem A.10.  For the second part, assume that π  ∗  ∗ = T π V π ∗ . Thus V π  ∗ ≤ T ∗ ≤ V  ∗  ∗ V π ∗ ≤ V = V π  ∗  V π V π  ∗ = V . By the second part of Corollary A.12, we must in fact have T  is an optimal stationary policy. Hence, V π  ∗  ∗ ∗  , i.e., all of them are equal and T π  V  ∗  ∗ = T  ∗  V  ∗  .  . Thus, ∗ = V π   cid:2   The second part of this corollary in essence shows that the only policies that are optimal are  the ones which are greedy w.r.t. to V  ∗  .   Bibliography  73   2010 . Proceedings of the 27th Annual International Conference on Machine Learning  ICML 2010 ,  ACM International Conference Proceeding Series, New York, NY, USA. ACM. 81, 84, 86  A. Prieditis, S. J. R., editor  1995 . Proceedings of the 12th International Conference on Machine  Learning  ICML 1995 , San Francisco, CA, USA. Morgan Kaufmann. 74, 78  Abbeel, P., Coates, A., Quigley, M., and Ng, A. Y.  2007 . An application of reinforcement learning  to aerobatic helicopter ﬂight. In Schölkopf et al.  2007 , pages 1–8. 64  Abe, N., Verma, N. K., Apté, C., and Schroko, R.  2004 . Cross channel optimized marketing by reinforcement learning. In Kim, W., Kohavi, R., Gehrke, J., and DuMouchel, W., editors, Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 767–772, New York, NY, USA. ACM. 63  Albus, J. S.  1971 . A theory of cerebellar function. Mathematical Biosciences, 10:25–61.  DOI: 10.1016 0025-5564 71 90051-4 19  Albus, J. S.  1981 . Brains, Behavior, and Robotics. BYTE Books, Subsidiary of McGraw-Hill,  Peterborough, New Hampshire. 19  Amari, S.  1998 . Natural gradient works efﬁciently in learning. Neural Computation, 10 2 :251–276.  DOI: 10.1162 089976698300017746 60  Antos, A., Munos, R., and Szepesvári, C.  2007 . Fitted Q-iteration in continuous action-space  MDPs. In Platt et al.  2008 , pages 9–16. 35, 51, 54, 56  Antos, A., Szepesvári, C., and Munos, R.  2008 . Learning near-optimal policies with Bellman- residual minimization based ﬁtted policy iteration and a single sample path. Machine Learning, 71 1 :89–129. Published Online First: 14 Nov, 2007. DOI: 10.1007 s10994-007-5038-2 28, 35, 56  Audibert, J.-Y., Munos, R., and Szepesvári, C.  2009 . Exploration-exploitation tradeoff using variance estimates in multi-armed bandits. Theoretical Computer Science, 410 19 :1876–1902. DOI: 10.1016 j.tcs.2009.01.016 39  Auer, P., Cesa-Bianchi, N., and Fischer, P.  2002 . Finite time analysis of the multiarmed bandit  problem. Machine Learning, 47 2-3 :235–256. DOI: 10.1023 A:1013689704352 39   74 BIBLIOGRAPHY  Auer, P., Jaksch, T., and Ortner, R.  2010 . Near-optimal regret bounds for reinforcement learning.  Journal of Machine Learning Research, 11:1563—1600. 43  Bagnell, J. A. and Schneider, J. G.  2003 . Covariant policy search. In Gottlob, G. and Walsh, T., editors, Proceedings of the Eighteenth International Joint Conference on Artiﬁcial Intelligence  IJCAI- 03 , pages 1019–1024, San Francisco, CA, USA. Morgan Kaufmann. 61  Baird, L. C.  1995 . Residual algorithms: Reinforcement learning with function approximation. In  A. Prieditis  1995 , pages 30–37. 23, 51  Balakrishna, P., Ganesan, R., Sherry, L., and Levy, B.  2008 . Estimating taxi-out times with a reinforcement learning algorithm. In 27th IEEE AIAA Digital Avionics Systems Conference, pages 3.D.3–1 – 3.D.3–12. DOI: 10.1109 DASC.2008.4702812 11  Bartlett, P. L. and Tewari, A.  2009 . REGAL: A regularization based algorithm for reinforce- ment learning in weakly communicating MDPs. In Proceedings of the 25th Annual Conference on Uncertainty in Artiﬁcial Intelligence. 43  Barto, A. G., Sutton, R. S., and Anderson, C. W.  1983 . Neuronlike adaptive elements that can solve difﬁcult learning control problems. IEEE Transactions on Systems, Man, and Cybernetics, 13:834–846. 61  Beleznay, F., Gr˝obler, T., and Szepesvári, C.  1999 . Comparing value-function estimation algo- rithms in undiscounted problems. Technical Report TR-99-02, Mindmaker Ltd., Budapest 1121, Konkoly Th. M. u. 29-33, Hungary. 16  Berman, P.  1998 . On-line searching and navigation. In Fiat, A. and Woeginger, G., editors, Online  Algorithms: The State of the Art, chapter 10. Springer, Berlin, Heidelberg. 41  Bertsekas, D. P.  2007a . Dynamic Programming and Optimal Control, volume 1. Athena Scientiﬁc,  Belmont, MA, 3 edition. x, 1  Belmont, MA, 3 edition. x, 1, 24  Bertsekas, D. P.  2007b . Dynamic Programming and Optimal Control, volume 2. Athena Scientiﬁc,  Bertsekas, D. P.  2010 . Approximate dynamic programming  online chapter . In Dynamic Pro- gramming and Optimal Control, volume 2, chapter 6. Athena Scientiﬁc, Belmont, MA, 3 edition. x, 23  Bertsekas, D. P., Borkar, V. S., and Nediˇc, A.  2004 . Improved temporal difference methods with linear function approximation. In Si, J., Barto, A. G., Powell, W. B., and Wunsch II, D., editors, Learning and Approximate Dynamic Programming, chapter 9, pages 235–257. IEEE Press. 31, 32  Bertsekas, D. P. and Ioffe, S.  1996 . Temporal differences-based policy iteration and applications  in neuro-dynamic programming. LIDS-P-2349, MIT. 30, 32   BIBLIOGRAPHY 75  Bertsekas, D. P. and Shreve, S.  1978 . Stochastic Optimal Control  The Discrete Time Case . Academic  Press, New York. 1  Bertsekas, D. P. and Tsitsiklis, J. N.  1996 . Neuro-Dynamic Programming. Athena Scientiﬁc,  Belmont, MA. x, 17, 23, 24, 50, 53, 57  Bhatnagar, S., Sutton, R. S., Ghavamzadeh, M., and Lee, M.  2009 . Natural actor-critic algorithms.  Automatica. in press. DOI: 10.1016 j.automatica.2009.07.008 59, 61  Borkar, V. S.  1997 . Stochastic approximation with two time scales. Systems & Control Letters,  29 5 :291–294. DOI: 10.1016 S0167-6911 97 90015-3 27  Borkar, V. S.  1998 . Asynchronous stochastic approximations. SIAM J. Control and Optimization,  36 3 :840–851. DOI: 10.1137 S0363012995282784 13  Borkar,V. S.  2008 . Stochastic Approximation: A Dynamical Systems Viewpoint. Cambridge University  Press. 27  Borkar, V. S. and Meyn, S. P.  2002 .  Risk-sensitive optimal control for Markov deci- sion processes with monotone cost. Mathematics of Operations Research, 27 1 :192—209. DOI: 10.1287 moor.27.1.192.334 63  Bottou, L. and Bousquet, O.  2008 . The trade offs of large scale learning. In Platt et al.  2008 ,  pages 161–168. 33  Boyan, J. A.  2002 . Technical update: Least-squares temporal difference learning. Machine Learning,  49:233–246. DOI: 10.1023 A:1017936530646 28, 30  Boyan, J. A. and Littman, M. L.  1994 . Packet routing in dynamically changing networks: A reinforcement learning approach. In Cowan, J. D., Tesauro, G., and Alspector, J., editors, NIPS- 6: Advances in Neural Information Processing Systems: Proceedings of the 1993 Conference, pages 671–678. Morgan Kauffman, San Francisco, CA, USA. 63  Boyan, J. A. and Moore, A. W.  1995 . Generalization in reinforcement learning: Safely approxi-  mating the value function. In Tesauro et al.  1995 , pages 369–376. 23, 51  Bradtke, S. J.  1994 . Incremental Dynamic Programming for On-line Adaptive Optimal Control. PhD thesis, Department of Computer and Information Science, University of Massachusetts, Amherst, Massachusetts. 27  Bradtke, S. J. and Barto, A. G.  1996 . Linear least-squares algorithms for temporal difference  learning. Machine Learning, 22:33–57. DOI: 10.1007 BF00114723 28, 30  Brafman, R. I. and Tennenholtz, M.  2002 . R-MAX - a general polynomial time algorithm Journal of Machine Learning Research, 3:213–231.  for near-optimal reinforcement learning. DOI: 10.1162 153244303765208377 42, 44   76 BIBLIOGRAPHY  Busoniu, L., Babuska, R., Schutter, B., and Ernst, D.  2010 . Reinforcement Learning and Dynamic Programming Using Function Approximators. Automation and Control Engineering Series. CRC Press. x  Cao, X. R.  2007 . Stochastic Learning and Optimization: A Sensitivity-Based Approach. Springer,  New York. x, 58  Chang,H.S.,Fu,M.C.,Hu,J.,and Marcus,S.I. 2007 . An asymptotically efﬁcient simulation-based algorithm for ﬁnite horizon stochastic dynamic programming. IEEE Transactions on Automatic Control, 52 1 :89–94. 63  Chang, H. S., Fu, M. C., Hu, J., and Marcus, S. I.  2008 . Simulation-based Algorithms for Markov  Decision Processes. Springer Verlag. x, 63  Chow, C. S. and Tsitsiklis, J. N.  1989 . The complexity of dynamic programming.  Journal of  Complexity, 5:466–488. DOI: 10.1016 0885-064X 89 90021-6 63  Cohen, W. W. and Hirsh, H., editors  1994 . Proceedings of the 11th International Conference on  Machine Learning  ICML 1994 , San Francisco, CA, USA. Morgan Kaufmann. 79, 81  Cohen, W. W., McCallum, A., and Roweis, S. T., editors  2008 . Proceedings of the 25th Interna- tional Conference Machine Learning  ICML 2008 , volume 307 of ACM International Conference Proceeding Series, New York, NY, USA. ACM. DOI: 10.1145 1390156 78, 81, 82, 86  Cohen, W. W. and Moore, A., editors  2006 . Proceedings of the 23rd International Conference on Machine Learning  ICML 2006 , volume 148 of ACM International Conference Proceeding Series, New York, NY, USA. ACM. DOI: 10.1145 1143844 76, 80, 83, 85  Crites, R. H. and Barto, A. G.  1996 . Improving elevator performance using reinforcement learning. In Touretzky, D., Mozer, M. C., and Hasselmo, M. E., editors, NIPS-8: Advances in Neural Information Processing Systems: Proceedings of the 1995 Conference, pages 1017–1023, Cambridge, MA, USA. MIT Press. 63  ¸Sim¸sek, O. and Barto, A.  2006 . An intrinsic reward mechanism for efﬁcient exploration. In  Cohen and Moore  2006 , pages 833—840. 42  Danyluk, A. P., Bottou, L., and Littman, M. L., editors  2009 . Proceedings of the 26th Annual International Conference on Machine Learning  ICML 2009 , volume 382 of ACM International Conference Proceeding Series, New York, NY, USA. ACM. DOI: 10.1145 1553374 80, 85, 87  Dasgupta, S. and Freund, Y.  2008 . Random projection trees and low dimensional manifolds. In Ladner, R. E. and Dwork, C., editors, 40th Annual ACM Symposium on Theory of Computing, pages 537–546. ACM. 20   BIBLIOGRAPHY 77  de Farias, D. P. and Van Roy, B.  2003 . The linear programming approach to approximate dynamic  programming. Operations Research, 51 6 :850–865. DOI: 10.1287 opre.51.6.850.24925 63  de Farias, D. P. and Van Roy, B.  2004 . On constraint sampling in the linear programming ap- proach to approximate dynamic programming. Mathematics of Operations Research, 29 3 :462–478. DOI: 10.1287 moor.1040.0094 63  de Farias, D. P. and Van Roy, B.  2006 . A cost-shaping linear program for average-cost approx- imate dynamic programming with performance guarantees. Mathematics of Operations Research, 31 3 :597–620. DOI: 10.1287 moor.1060.0208 63  De Raedt, L. and Wrobel, S., editors  2005 . Proceedings of the 22nd International Conference on Machine Learning  ICML 2005 , volume 119 of ACM International Conference Proceeding Series, New York, NY, USA. ACM. 77, 85  Dearden, R., Friedman, N., and Andre, D.  1999 . Model based Bayesian exploration. In Laskey, K. and Prade, H., editors, Proceedings of the Fifteenth Conference on Uncertainty in Artiﬁcial Intelligence  UAI’99 , pages 150–159. Morgan Kaufmann. 45  Dearden, R., Friedman, N., and Russell, S.  1998 . Bayesian Q-learning. In Proceedings of the 15th  National Conference on Artiﬁcial Intelligence  AAAI , pages 761–768. AAAI Press. 45  Dietterich, T.  1998 . The MAXQ method for hierarchical reinforcement learning. In Shavlik   1998 , pages 118–126. 63  Dietterich, T. G., Becker, S., and Ghahramani, Z., editors  2001 . Advances in Neural Information  Processing Systems 14, Cambridge, MA, USA. MIT Press. 79, 81  Domingo, C.  1999 . Faster near-optimal reinforcement learning: Adding adaptiveness to the E3 algorithm. In Watanabe, O. and Yokomori, T., editors, Proc. of the 10th International Conference on Algorithmic Learning Theory, volume 1720 of Lecture Notes in Computer Science, pages 241–251. Springer. 42  Engel, Y., Mannor, S., and Meir, R.  2005 . Reinforcement learning with Gaussian processes. In  De Raedt and Wrobel  2005 , pages 201–208. DOI: 10.1145 1102351.1102377 36  Ernst, D., Geurts, P., and Wehenkel, L.  2005 . Tree-based batch mode reinforcement learning.  Journal of Machine Learning Research, 6:503–556. 36, 51  Even-Dar, E., Kakade, S. M., and Mansour, Y.  2005 . Experts in a Markov decision process. In Saul, L. K., Weiss, Y., and Bottou, L., editors, Advances in Neural Information Processing Systems 17, pages 401–408, Cambridge, MA, USA. MIT Press. 63   78 BIBLIOGRAPHY  Even-Dar, E., Mannor, S., and Mansour,Y.  2002 . PAC bounds for multi-armed bandit and Markov decision processes. In Kivinen,J.and Sloan,R.H.,editors,Proceedings of the 15th Annual Conference on Computational Learning Theory Computational Learning Theory  COLT 2002 , volume 2375 of Lecture Notes in Computer Science, pages 255–270. Springer. 41  Even-Dar, E. and Mansour, Y.  2003 . Learning rates for Q-learning. Journal of Machine Learning  Research, 5:1–25. 48  Farahmand, A., Ghavamzadeh, M., Szepesvári, C., and Mannor, S.  2008 . Regularized ﬁtted Q- iteration: Application to planning. In Girgin, S., Loth, M., Munos, R., Preux, P., and Ryabko, D., editors,Revised and Selected Papers of the 8th European Workshop on Recent Advances in Reinforcement Learning  EWRL 2008 , volume 5323 of Lecture Notes in Computer Science, pages 55–68. Springer. 36  Farahmand, A., Ghavamzadeh, M., Szepesvári, C., and Mannor, S.  2009 . Regularized policy  iteration. In Koller et al.  2009 , pages 441–448. 36  Frank, J., Mannor, S., and Precup, D.  2008 . Reinforcement learning in the presence of rare events.  In Cohen et al.  2008 , pages 336–343. DOI: 10.1145 1390156.1390199 11  Fürnkranz, J., Scheffer, T., and Spiliopoulou, M., editors  2006 . Proceedings of the 17th European  Conference on Machine Learning  ECML-2006 . Springer. DOI: 10.1007 11871842 80, 83  George, A. P. and Powell, W. B.  2006 .  recursive estimation with applications in approximate dynamic programming. Machine Learning, 65:167–198. DOI: 10.1007 s10994-006-8365-9 13, 27  Adaptive stepsizes for  Geramifard, A., Bowling, M. H., Zinkevich, M., and Sutton, R. S.  2007 . iLSTD: Eligibility traces  and convergence analysis. In Schölkopf et al.  2007 , pages 441–448. 33  Ghahramani, Z., editor  2007 . Proceedings of the 24th International Conference on Machine Learning  ICML 2007 , volume 227 of ACM International Conference Proceeding Series, New York, NY, USA. ACM. 78, 82  Ghavamzadeh, M. and Engel, Y.  2007 . Bayesian actor-critic algorithms. In Ghahramani  2007 ,  pages 297–304. 36  Gittins, J. C.  1989 . Multi-armed Bandit Allocation Indices. Wiley-Interscience series in systems  and optimization. Wiley, Chichester, NY. 40  Glynn, P. W.  1990 . Likelihood ratio gradient estimation for stochastic systems. Communications  of the ACM, 33 10 :75–84. DOI: 10.1145 84537.84552 59  Gordon, G. J.  1995 . Stable function approximation in dynamic programming. In A. Prieditis   1995 , pages 261–268. 51   BIBLIOGRAPHY 79  Gosavi,A. 2003 . Simulation-based optimization: parametric optimization techniques and reinforcement  learning. Springer Netherlands. x  Gosavi, A.  2004 . Reinforcement learning for long-run average cost. European Journal of Operational  Research, 155 3 :654–674. DOI: 10.1016 S0377-2217 02 00874-3 63  Györﬁ, L., Kohler, M., Krzy ˙zak, A., and Walk, H.  2002 . A distribution-free theory of nonparametric  regression. Springer-Verlag, New York. DOI: 10.1007 b97848 21  Györﬁ,L.,Kohler,M.,Krzy ˙zak,A.,and Walk,H. 2002 . A Distribution-FreeTheory of Nonparametric  Regression. Springer-Verlag. DOI: 10.1007 b97848 34  Härdle, W.  1990 . Applied nonparametric regression. Cambridge University Press Cambridge. 21  Heger, M.  1994 . Consideration of risk in reinforcement learning. In Cohen and Hirsh  1994 ,  pages 105–111. 63  MA. 42  Howard, R. A.  1960 . Dynamic Programming and Markov Processes. The MIT Press, Cambridge,  Hutter, M.  2004 . Universal Artiﬁcial Intelligence: Sequential Decisions based on Algorithmic Proba-  bility. Springer, Berlin. 300 pages, http:  www.idsia.ch ˜marcus ai uaibook.htm. 63  Jaakkola, T.,  Jordan, M., and Singh, S.   1994 .  tic iterative dynamic programming algorithms. DOI: 10.1162 neco.1994.6.6.1185 48  On the convergence of  stochas- Neural Computation, 6 6 :1185–1201.  Jong, N. K. and Stone, P.  2007 . Model-based exploration in continuous state spaces. In Miguel, I. and Ruml, W., editors, 7th International Symposium on Abstraction, Reformulation, and Approxi- mation  SARA 2007 , volume 4612 of Lecture Notes in Computer Science, pages 258–272, Whistler, Canada. Springer. 45  Kaelbling, L., Littman, M., and Moore, A.  1996 . Reinforcement learning: A survey. Journal of  Artiﬁcial Intelligence Research, 4:237–285. x  Kakade, S.  2001 . A natural policy gradient. In Dietterich et al.  2001 , pages 1531–1538. 61  Kakade, S.  2003 . On the sample complexity of reinforcement learning. PhD thesis, Gatsby Compu-  tational Neuroscience Unit, University College London. 44  Kakade, S., Kearns, M. J., and Langford, J.  2003 . Exploration in metric state spaces. In Fawcett, T. and Mishra, N., editors, Proceedings of the 20th International Conference on Machine Learning  ICML 2003 , pages 306–312. AAAI Press. 45, 46   80 BIBLIOGRAPHY  Kakade, S. and Langford, J.  2002 . Approximately optimal approximate reinforcement learning. In Sammut, C. and Hoffmann, A. G., editors, Proceedings of the 19th International Conference on Machine Learning  ICML 2002 , pages 267–274, San Francisco, CA, USA. Morgan Kaufmann. 57  Kearns, M. and Singh, S.  2002 . Near-optimal reinforcement learning in polynomial time. Machine  Learning, 49 2–3 :209–232. DOI: 10.1023 A:1017984413808 42  Kearns, M. and Singh, S. P.  1998 . Near-optimal performance for reinforcement learning in poly-  nomial time. In Shavlik  1998 , pages 260–268. DOI: 10.1023 A:1017984413808 44  Kearns, M. J., Mansour, Y., and Ng, A. Y.  1999 . Approximate planning in large POMDPs via  reusable trajectories. In Solla et al.  1999 , pages 1001–1007. 63  Keller, P. W., Mannor, S., and Precup, D.  2006 . Automatic basis function construction for approx- imate dynamic programming and reinforcement learning. In Cohen and Moore  2006 , pages 449–456. DOI: 10.1145 1143844.1143901 36  Kocsis, L. and Szepesvári, C.  2006 . Bandit based Monte-Carlo planning. In Fürnkranz et al.   2006 , pages 282–293. 63  Kohl, N. and Stone, P.  2004 . Policy gradient reinforcement learning for fast quadrupedal locomo- tion. In Proceedings of the 2004 IEEE International Conference on Robotics and Automation, pages 2619–2624. IEEE. 63  Koller,D.,Schuurmans,D.,Bengio,Y.,and Bottou,L.,editors  2009 . Advances in Neural Information  Processing Systems 21, Cambridge, MA, USA. MIT Press. 78, 82, 86  Kolter, J. Z. and Ng, A. Y.  2009 . Regularization and feature selection in least-squares temporal difference learning. In Danyluk et al.  2009 , pages 521–528. DOI: 10.1145 1553374.1553442 36  Konda, V. R. and Tsitsiklis, J. N.  1999 . Actor-critic algorithms. In Solla et al.  1999 , pages  1008–1014. 59  Konda, V. R. and Tsitsiklis, J. N.  2003 . On actor-critic algorithms. SIAM J. Control and Optimiza-  tion, 42 4 :1143–1166. DOI: 10.1137 S0363012901385691 60  Kosorok, M. R.  2008 . Introduction to Empirical Processes and Semiparametric Inference. Springer.  DOI: 10.1007 978-0-387-74978-5 28  Lagoudakis, M. and Parr, R.  2003 . Least-squares policy iteration. Journal of Machine Learning  Research, 4:1107–1149. DOI: 10.1162 jmlr.2003.4.6.1107 56  Lai, T. L. and Robbins, H.  1985 . Asymptotically efﬁcient adaptive allocation rules. Advances in  Applied Mathematics, 6:4–22. DOI: 10.1016 0196-8858 85 90002-8 39   BIBLIOGRAPHY 81  Lemieux, C.  2009 . Monte Carlo and Quasi-Monte Carlo Sampling. Springer. 20  Li, Y., Szepesvári, C., and Schuurmans, D.  2009 . Learning exercise policies for american options. In Proc. of theTwelfth International Conference on Artiﬁcial Intelligence and Statistics, JMLR: W&CP, volume 5, pages 352–359. 64  Lin, L.-J.  1992 . Self-improving reactive agents based on reinforcement learning, planning and  teaching. Machine Learning, 9:293–321. DOI: 10.1023 A:1022628806385 27, 32  Littman, M. L.  1994 . Markov games as a framework for multi-agent reinforcement learning. In  Cohen and Hirsh  1994 , pages 157–163. 63  Littman, M. L., Sutton, R. S., and Singh, S. P.  2001 . Predictive representations of state. In  Dietterich et al.  2001 , pages 1555–1561. 63  Maei, H., Szepesvári, C., Bhatnagar, S., Silver, D., Precup, D., and Sutton, R.  2010a . Convergent temporal-difference learning with arbitrary smooth function approximation. In NIPS-22, pages 1204–1212. 27  Maei, H., Szepesvári, C., Bhatnagar, S., and Sutton, R.  2010b . Toward off-policy learning control  with function approximation. In ICM  2010 . 50  Maei, H. R. and Sutton, R. S.  2010 . GQ λ : A general gradient algorithm for temporal-difference prediction learning with eligibility traces. In Baum, E., Hutter, M., and Kitzelmann, E., editors, Proceedings of the Third Conference on Artiﬁcial General Intelligence, pages 91–96. Atlantis Press. 27, 55  Mahadevan, S.  2009 . Learning representation and control in Markov decision processes: New frontiers. Foundations and Trends in Machine Learning, 1 4 :403–565. DOI: 10.1561 2200000003 36  McAllester, D. A. and Myllymäki, P., editors  2008 . Proceedings of the 24th Conference in Uncertainty  in Artiﬁcial Intelligence  UAI’08 . AUAI Press. 83, 86, 87  Melo,F.S.,Meyn,S.P.,and Ribeiro,M.I. 2008 . An analysis of reinforcement learning with function  approximation. In Cohen et al.  2008 , pages 664–671. DOI: 10.1145 1390156.1390240 49  Menache, I., Mannor, S., and Shimkin, N.  2005 .  poral difference reinforcement learning. DOI: 10.1007 s10479-005-5732-z 35  Basis function adaptation in tem- Annals of Operations Research, 134 1 :215–238.  Mnih, V., Szepesvári, C., and Audibert, J.-Y.  2008 . Empirical Bernstein stopping. In Cohen et al.   2008 , pages 672–679. DOI: 10.1145 1390156.1390241 41  Munos, R. and Szepesvári, C.  2008 . Finite-time bounds for ﬁtted value iteration.  Journal of  Machine Learning Research, 9:815–857. 35, 51   82 BIBLIOGRAPHY  Nascimento, J. and Powell, W.  2009 . An optimal approximate dynamic programming algo- rithm for the lagged asset acquisition problem. Mathematics of Operations Research, 34:210–237. DOI: 10.1287 moor.1080.0360 46  Nediˇc, A. and Bertsekas, D. P.  2003 . Least squares policy evaluation algorithms with linear function approximation. Discrete Event Dynamic Systems, 13 1 :79–110. DOI: 10.1023 A:1022192903948 29, 30, 31  Neu, G., György, A., and Szepesvári, C.  2010 . The online loop-free stochastic shortest-path  problem. In COLT-10. 63  Ng,A.Y.and Jordan,M. 2000 . PEGASUS:A policy search method for large MDPs and POMDPs. In Boutilier, C. and Goldszmidt, M., editors, Proceedings of the 16th Conference in Uncertainty in Artiﬁcial Intelligence  UAI’00 , pages 406–415, San Francisco CA. Morgan Kaufmann. 63  Nouri, A. and Littman, M.  2009 . Multi-resolution exploration in continuous spaces. In Koller et al.   2009 , pages 1209–1216. 45, 46  Ormoneit, D. and Sen, S.  2002 . Kernel-based reinforcement learning. Machine Learning, 49:161–  178. DOI: 10.1023 A:1017928328829 51  Ortner,R. 2008 . Online regret bounds for Markov decision processes with deterministic transitions. In Freund, Y., Györﬁ, L., Turán, G., and Zeugmann, T., editors, Proc. of the 19th International Conference on Algorithmic Learning Theory  ALT 2008 , volume 5254 of Lecture Notes in Computer Science, pages 123–137. Springer. 41  Parr, R., Li, L., Taylor, G., Painter-Wakeﬁeld, C., and Littman, M. L.  2008 . An analysis of linear models, linear value-function approximation, and feature selection for reinforcement learning. In Cohen et al.  2008 , pages 752–759. DOI: 10.1145 1390156.1390251 24, 25  Parr, R., Painter-Wakeﬁeld, C., Li, L., and Littman, M. L.  2007 .  In Ghahramani  Analyzing feature  2007 , pages 737–744.  generation for value-function approximation. DOI: 10.1145 1273496.1273589 36  Perkins, T. and Precup, D.  2003 . A convergent form of approximate policy iteration. In S. Becker, S. T. and Obermayer, K., editors, Advances in Neural Information Processing Systems 15, pages 1595–1602, Cambridge, MA, USA. MIT Press. 57  Peters, J. and Schaal, S.  2008 . Natural actor-critic. Neurocomputing, 71 7–9 :1180–1190.  DOI: 10.1016 j.neucom.2007.11.026 55, 61  Peters, J., Vijayakumar, S., and Schaal, S.  2003 . Reinforcement learning for humanoid robotics. In Humanoids2003,Third IEEE-RAS International Conference on Humanoid Robots, pages 225—230. 55, 61, 63   BIBLIOGRAPHY 83  Platt, J. C., Koller, D., Singer, Y., and Roweis, S. T., editors  2008 . Advances in Neural Information  Processing Systems 20, Cambridge, MA, USA. MIT Press. 73, 75, 85, 88  Polyak, B. and Juditsky, A.  1992 . Acceleration of stochastic approximation by averaging. SIAM  Journal on Control and Optimization, 30:838–855. DOI: 10.1137 0330046 13  Poupart, P., Vlassis, N., Hoey, J., and Regan, K.  2006 .  An analytic solution to dis- In Cohen and Moore  2006 , pages 697–704.  crete Bayesian reinforcement DOI: 10.1145 1143844.1143932 45  learning.  Powell, W. B.  2007 . Approximate Dynamic Programming: Solving the curses of dimensionality. John  Wiley and Sons, New York. DOI: 10.1002 9780470182963 x, 48, 49  Proper, S. and Tadepalli, P.  2006 . Scaling model-based average-reward reinforcement learning for  product delivery. In Fürnkranz et al.  2006 , pages 735–742. 63  Puterman, M.  1994 . Markov Decision Processes — Discrete Stochastic Dynamic Programming. John  Wiley & Sons, Inc., New York, NY. 1  Rasmussen, C. and Williams, C.  2005 . Gaussian Processes for Machine Learning  Adaptive Compu-  tation and Machine Learning . The MIT Press. 21  Rasmussen, C. E. and Kuss, M.  2004 . Gaussian processes in reinforcement learning. In Thrun, S., Saul, L. K., and Schölkopf, B., editors, Advances in Neural Information Processing Systems 16, pages 751–759, Cambridge, MA, USA. MIT Press. 36  Riedmiller, M.  2005 . Neural ﬁtted Q iteration – ﬁrst experiences with a data efﬁcient neural reinforcement learning method. In Gama, J., Camacho, R., Brazdil, P., Jorge, A., and Torgo, L., editors, Proceedings of the 16th European Conference on Machine Learning  ECML-05 , volume 3720 of Lecture Notes in Computer Science, pages 317–328. Springer. 51  Robbins, H.  1952 . Some aspects of the sequential design of experiments. Bulletin of the American  Mathematics Society, 58:527–535. DOI: 10.1090 S0002-9904-1952-09620-8 38  Ross, S. and Pineau, J.  2008 . Model-based Bayesian reinforcement learning in large structured  domains. In McAllester and Myllymäki  2008 , pages 476–483. 45  Ross, S., Pineau, J., Paquet, S., and Chaib-draa, B.  2008 . Online planning algorithms for POMDPs.  Journal of Artiﬁcial Intelligence Research, 32:663–704. 63  Rummery, G. A.  1995 . Problem solving with reinforcement learning. PhD thesis, Cambridge  University. 54  Rummery, G. A. and Niranjan, M.  1994 . On-line Q-learning using connectionist systems. Tech- nical Report CUED F-INFENG TR 166, Cambridge University Engineering Department. 54   84 BIBLIOGRAPHY  Rusmevichientong, P., Salisbury, J. A., Truss, L. T., Van Roy, B., and Glynn, P. W.  2006 . Opportunities and challenges in using online preference data for vehicle pricing: A case study at General Motors. Journal of Revenue and Pricing Management, 5 1 :45–61. DOI: 10.1057 palgrave.rpm.5160011 63  Rust, J.  1996 . Using randomization to break the curse of dimensionality. Econometrica, 65:487–516.  DOI: 10.2307 2171751 63  Scherrer, B.  2010 . Should one compute the temporal difference ﬁx point or minimize the Bellman  residual? The uniﬁed oblique projection view. In ICM  2010 . 24  Schölkopf, B., Platt, J. C., and Hoffman,T., editors  2007 . Advances in Neural Information Processing  Systems 19, Cambridge, MA, USA. MIT Press. 73, 78  Schraudolph, N.  1999 . Local gain adaptation in stochastic gradient descent.  In Ninth In- ternational Conference on Artiﬁcial Neural Networks  ICANN 99 , volume 2, pages 569–574. DOI: 10.1049 cp:19991170 13  Shapiro, A.  2003 . Monte Carlo sampling methods.  In Stochastic Programming, Hand- in OR & MS, volume 10. North-Holland Publishing Company, Amsterdam.  books DOI: 10.1016 S0927-0507 03 10006-0 28, 63  Shavlik, J. W., editor  1998 . Proceedings of the 15th International Conference on Machine Learning   ICML 1998 , San Francisco, CA, USA. Morgan Kauffmann. 77, 80  Silver, D., Sutton, R. S., and Müller, M.  2007 . Reinforcement learning of local shape in the game of Go. In Veloso, M. M., editor, Proceedings of the 20th International Joint Conference on Artiﬁcial Intelligence  IJCAI 2007 , pages 1053—1058. 32, 63  Simão, H. P., Day, J., George, A. P., Gifford, T., Nienow, J., and Powell, W. B.  2009 . An ap- proximate dynamic programming algorithm for large-scale ﬂeet management: A case application. Transportation Science, 43 2 :178–197. DOI: 10.1287 trsc.1080.0238 63  Singh, S. P. and Bertsekas, D. P.  1997 . Reinforcement learning for dynamic channel allocation in cellular telephone systems. In Mozer, M. C., Jordan, M. I., and Petsche, T., editors, NIPS- 9: Advances in Neural Information Processing Systems: Proceedings of the 1996 Conference, pages 974–980, Cambridge, MA, USA. MIT Press. 63  Singh,S.P.,Jaakkola,T.,and Jordan,M.I. 1995 . Reinforcement learning with soft state aggregation.  In Tesauro et al.  1995 , pages 361–368. 50  Singh, S. P., Jaakkola, T., Littman, M. L., and Szepesvári, C.  2000 . Convergence results for single-step on-policy reinforcement-learning algorithms. Machine Learning, 38 3 :287–308. DOI: 10.1023 A:1007678930559 48, 58   BIBLIOGRAPHY 85  Singh, S. P. and Sutton, R. S.  1996 . Reinforcement learning with replacing eligibility traces.  Machine Learning, 32:123–158. DOI: 10.1023 A:1018012322525 17  Singh, S. P. and Yee, R. C.  1994 . An upper bound on the loss from approximate optimal-value  functions. Machine Learning, 16 3 :227–233. DOI: 10.1023 A:1022693225949 10  Solla, S. A., Leen, T. K., and Müller, K. R., editors  1999 . Advances in Neural Information Processing  Systems 12, Cambridge, MA, USA. MIT Press. 80, 85  Strehl, A. L., Li, L., Wiewiora, E., Langford, J., and Littman, M. L.  2006 .  In Cohen and Moore  PAC  2006 , pages 881–888.  model-free DOI: 10.1145 1143844.1143955 44  reinforcement  learning.  Strehl, A. L. and Littman, M. L.  2005 . A theoretical analysis of model-based interval estimation.  In De Raedt and Wrobel  2005 , pages 857–864. DOI: 10.1145 1102351.1102459 44  Strehl, A. L. and Littman, M. L.  2008 . Online linear regression and its application to model-based  reinforcement learning. In Platt et al.  2008 , pages 1417–1424. 45  Strens, M.  2000 . A Bayesian framework for reinforcement learning. In Langley, P., editor, Pro- ceedings of the 17th International Conference on Machine Learning  ICML 2000 , pages 943–950. Morgan Kaufmann. 45  Sutton, R. S.  1984 . Temporal Credit Assignment in Reinforcement Learning. PhD thesis, University  of Massachusetts, Amherst, MA. 11, 16, 22, 61  Sutton, R. S.  1988 . Learning to predict by the method of temporal differences. Machine Learning,  3 1 :9–44. DOI: 10.1007 BF00115009 11, 16, 22  Sutton, R. S.  1992 . Gain adaptation beats least squares. In Proceedings of the 7th Yale Workshop on  Adaptive and Learning Systems, pages 161—166. 13, 27  Sutton, R. S. and Barto, A. G.  1998 . Reinforcement Learning: An Introduction. Bradford Book.  MIT Press. x, 18, 24, 48  Sutton, R. S., Maei, H. R., Precup, D., Bhatnagar, S., Silver, D., Szepesvári, C., and Wiewiora, E.  2009a . Fast gradient-descent methods for temporal-difference learning with linear function approximation. In Danyluk et al.  2009 , pages 993—1000. DOI: 10.1145 1553374.1553501 25, 26, 27  Sutton, R. S., McAllester, D. A., Singh, S. P., and Mansour, Y.  1999a . Policy gradient methods for reinforcement learning with function approximation. In Solla et al.  1999 , pages 1057–1063. 58, 59   86 BIBLIOGRAPHY  Sutton, R. S., Precup, D., and Singh, S. P.  1999b . Between MDPs and semi-MDPs: A frame- work for temporal abstraction in reinforcement learning. Artiﬁcial Intelligence, 112:181–211. DOI: 10.1016 S0004-3702 99 00052-1 63  Sutton, R. S., Szepesvári, C., Geramifard, A., and Bowling, M. H.  2008 . Dyna-style planning with linear function approximation and prioritized sweeping. In McAllester and Myllymäki  2008 , pages 528–536. 24  Sutton, R. S., Szepesvári, C., and Maei, H. R.  2009b . A convergent O n  temporal-difference algorithm for off-policy learning with linear function approximation. In Koller et al.  2009 , pages 1609–1616. 25  Szepesvári, C.  1997 . The asymptotic convergence-rate of Q-learning. In Jordan, M. I., Kearns, M. J., and Solla, S. A., editors, Advances in Neural Information Processing Systems 10, pages 1064– 1070, Cambridge, MA, USA. MIT Press. 48  Szepesvári, C.  1997 . Learning and exploitation do not conﬂict under minimax optimality. In Someren, M. and Widmer, G., editors, Machine Learning: ECML’97  9th European Conf. on Machine Learning, Proceedings , volume 1224 of Lecture Notes in Artiﬁcial Intelligence, pages 242– 249. Springer, Berlin. 46  Szepesvári, C.  1998 . Static and Dynamic Aspects of Optimal Sequential Decision Making. PhD thesis, Bolyai Institute of Mathematics, University of Szeged, Szeged, Aradi vrt. tere 1, HUNGARY, 6720. 48  Szepesvári, C.  2001 . Efﬁcient approximate planning in continuous space Markovian decision  problems. AI Communications, 13:163–176. 63  Szepesvári, C. and Littman, M. L.  1999 . A uniﬁed analysis of value-function-based reinforcement- learning algorithms. Neural Computation, 11:2017–2059. DOI: 10.1162 089976699300016070 63  Szepesvári, C. and Smart, W. D.  2004 . Interpolation-based Q-learning. In Brodley, C. E., editor, Proceedings of the 21st International Conference on Machine Learning  ICML 2004 , pages 791–798. ACM. 50, 51  Szita, I. and L˝orincz, A.  2008 . The many faces of optimism: a unifying approach. In Cohen et al.   2008 , pages 1048–1055. DOI: 10.1145 1390156.1390288 44  Szita, I. and Szepesvári, C.  2010 . Model-based reinforcement learning with nearly tight exploration  complexity bounds. In ICM  2010 . 44  Tadi´c, V. B.  2004 . On the almost sure rate of convergence of linear stochastic approximation algo- rithms. IEEETransactions on InformationTheory, 5 2 :401–409. DOI: 10.1109 TIT.2003.821971 14   BIBLIOGRAPHY 87  Tanner, B. and White, A.  2009 . RL-Glue: Language-independent software for reinforcement-  learning experiments. Journal of Machine Learning Research, 10:2133–2136. 64  Taylor, G. and Parr, R.  2009 . Kernelized value function approximation for reinforcement learning.  In Danyluk et al.  2009 , pages 1017–1024. 36  Tesauro, G.  1994 . TD-Gammon, a self-teaching backgammon program, achieves master-level  play. Neural Computation, 6 2 :215–219. DOI: 10.1162 neco.1994.6.2.215 63  Tesauro, G., Touretzky, D., and Leen, T., editors  1995 . NIPS-7: Advances in Neural Information Processing Systems: Proceedings of the 1994 Conference, Cambridge, MA, USA. MIT Press. 75, 84  Thrun, S. B.  1992 . Efﬁcient exploration in reinforcement learning. Technical Report CMU-CS-  92-102, Carnegie Mellon University, Pittsburgh, PA. 41  Toussaint, M., Charlin, L., and Poupart, P.  2008 . Hierarchical POMDP controller optimization  by likelihood maximization. In McAllester and Myllymäki  2008 , pages 562–570. 63  Tsitsiklis, J. N.  1994 . Asynchronous stochastic approximation and Q-learning. Machine Learning,  16 3 :185–202. DOI: 10.1007 BF00993306 48  Tsitsiklis, J. N. and Mannor, S.  2004 . The sample complexity of exploration in the multi-armed  bandit problem. Journal of Machine Learning Research, 5:623–648. 41  Tsitsiklis, J. N. and Van Roy, B.  1996 . Feature-based methods for large scale dynamic programming.  Machine Learning, 22:59–94. DOI: 10.1023 A:1018008221616 50, 51  Tsitsiklis, J. N. and Van Roy, B.  1997 . An analysis of temporal difference learning with function approximation. IEEE Transactions on Automatic Control, 42:674–690. DOI: 10.1109 9.580874 23  Tsitsiklis, J. N. and Van Roy, B.  1999a . Average cost temporal-difference learning. Automatica,  35 11 :1799–1808. DOI: 10.1016 S0005-1098 99 00099-0 24  Tsitsiklis, J. N. and Van Roy, B.  1999b . Optimal stopping of Markov processes: Hilbert space theory, approximation algorithms, and an application to pricing ﬁnancial derivatives. IEEE Transactions on Automatic Control, 44:1840–1851. DOI: 10.1109 9.793723 64  Tsitsiklis, J. N. and Van Roy, B.  2001 . Regression methods for pricing complex American-style  options. IEEE Transactions on Neural Networks, 12:694–703. DOI: 10.1109 72.935083 64  Tsybakov, A.  2009 . Introduction to nonparametric estimation. Springer Verlag. 21  Van Roy, B.  2006 . Performance loss bounds for approximate value iteration with state aggregation.  Mathematics of Operations Research, 31 2 :234–244. DOI: 10.1287 moor.1060.0188 24, 51, 57   88 BIBLIOGRAPHY  Wahba, G.  2003 . Reproducing kernel Hilbert spaces – two brief reviews. In Proceedings of the 13th  IFAC Symposium on System Identiﬁcation, pages 549–559. 21  Wang, T., Lizotte, D. J., Bowling, M. H., and Schuurmans, D.  2008 . Stable dual dynamic pro-  gramming. In Platt et al.  2008 . 63  Watkins, C. J. C. H.  1989 . Learning from Delayed Rewards. PhD thesis, King’s College, Cambridge,  UK. 47, 48  26, 28  Watkins, C. J. C. H. and Dayan, P.  1992 . Q-learning. Machine Learning, 3 8 :279–292.  DOI: 10.1023 A:1022676722315 48  Widrow, B. and Stearns, S.  1985 . Adaptive Signal Processing. Prentice Hall, Englewood Cliffs, NJ.  Williams, R. J.  1987 . A class of gradient-estimating algorithms for reinforcement learning in neural networks. In Proceedings of the IEEE First International Conference on Neural Networks, San Diego, CA. 59  Xu, X., He, H., and Hu, D.  2002 . Efﬁcient reinforcement learning using recursive least-squares  methods. Journal of Artiﬁcial Intelligence Research, 16:259–292. 29, 30  Xu, X., Hu, D., and Lu, X.  2007 . Kernel-based least squares policy iteration for reinforcement learning. IEEE Transactions on Neural Networks, 18:973–992. DOI: 10.1109 TNN.2007.899161 36  Yu, H. and Bertsekas, D.  2007 . Q-learning algorithms for optimal stopping based on least squares.  In Proceedings of the European Control Conference. 64  Yu, J. and Bertsekas, D. P.  2008 . New error bounds for approximations from projected linear equa- tions. Technical Report C-2008-43, Department of Computer Science, University of Helsinki. revised July, 2009. 24  Yu, J. Y., Mannor, S., and Shimkin, N.  2009 . Markov decision processes with arbitrary reward  processes. Mathematics of Operations Research. to appear. DOI: 10.1287 moor.1090.0397 63  Zhang, W. and Dietterich,T. G.  1995 . A reinforcement learning approach to job-shop scheduling. In Perrault, C. R. and Mellish, C. S., editors, Proceedings of the Fourteenth International Joint Conference on Artiﬁcial Intelligence  IJCAI 95 ,pages 1114–1120,San Francisco,CA,USA.Morgan Kaufmann. 63   Author’s Biography  89  CSABA SZEPESVÁRI Csaba Szepesvári received his PhD in 1999 from “Jozsef Attila” University, Szeged, Hungary. He is currently an Associate Professor at the Department of Computing Science of the University of Alberta and a principal investigator of the Alberta Ingenuity Center for Machine Learning. Previ- ously, he held a senior researcher position at the Computer and Automation Research Institute of the Hungarian Academy of Sciences, where he headed the Machine Learning Group. Before that, he spent 5 years in the software industry. In 1998, he became the Research Director of Mindmaker, Ltd., working on natural language processing and speech products, while from 2000, he became the Vice President of Research at the Silicon Valley company Mindmaker Inc. He is the coauthor of a book on nonlinear approximate adaptive controllers, published over 80 journal and conference papers and serves as the Associate Editor of IEEE Transactions on Adaptive Control and AI Communi- cations, is on the board of editors of the Journal of Machine Learning Research and the Machine Learning Journal, and is a regular member of the program committee at various machine learning and AI conferences. His areas of expertise include statistical learning theory, reinforcement learning and nonlinear adaptive control.

@highlight

Reinforcement learning is a learning paradigm concerned with learning to control a system so as to maximize a numerical performance measure that expresses a long-term objective.What distinguishes reinforcement learning from supervised learning is that only partial feedback is given to the learner about the learner's predictions. Further, the predictions may have long term effects through influencing the future state of the controlled system. Thus, time plays a special role. The goal in reinforcement learning is to develop efficient learning algorithms, as well as to understand the algorithms' merits and limitations. Reinforcement learning is of great interest because of the large number of practical applications that it can be used to address, ranging from problems in artificial intelligence to operations research or control engineering. In this book, we focus on those algorithms of reinforcement learning that build on the powerful theory of dynamic programming.We give a fairly comprehensive catalog of learning problems, describe the core ideas, note a large number of state of the art algorithms, followed by the discussion of their theoretical properties and limitations.