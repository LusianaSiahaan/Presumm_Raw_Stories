Foundations of Machine Learning  second edition   Adaptive Computation and Machine Learning  Francis Bach, Editor  A complete list of books published in The Adaptive Computations and Machine Learning series appears at the back of this book.   Foundations of Machine Learning  second edition  Mehryar Mohri  Afshin Rostamizadeh  Ameet Talwalkar  The MIT Press Cambridge, Massachusetts London, England   c cid:13  2018 Massachusetts Institute of Technology  All rights reserved. No part of this book may be reproduced in any form by any electronic or mechanical means  including photocopying, recording, or information storage and retrieval  without permission in writing from the publisher.  This book was set in LATEX by the authors. Printed and bound in the United States of America.  Library of Congress Cataloging-in-Publication Data  Names: Mohri, Mehryar, author.  Rostamizadeh, Afshin, author.  Talwalkar,  Ameet, author.  Title: Foundations of machine learning   Mehryar Mohri, Afshin Rostamizadeh,  and Ameet Talwalkar.  Description: Second edition.  Cambridge, MA : The MIT Press, [2018]  Series: Adaptive computation and machine learning series  Includes bibliographical references and index.  Identiﬁers: LCCN 2018022812  ISBN 9780262039406  hardcover : alk. paper  Subjects: LCSH: Machine learning.  Computer algorithms. Classiﬁcation: LCC Q325.5 .M64 2018  DDC 006.3 1--dc23 LC record available  at https:  lccn.loc.gov 2018022812  10 9 8 7 6 5 4 3 2 1   Contents  1  2  3  4  Preface  Introduction 1.1 What is machine learning? 1.2 What kind of problems can be tackled using machine learning? 1.3 1.4 1.5 1.6  Some standard learning tasks Learning stages Learning scenarios Generalization  The PAC Learning Framework 2.1 2.2 2.3 2.4  The PAC learning model Guarantees for ﬁnite hypothesis sets — consistent case Guarantees for ﬁnite hypothesis sets — inconsistent case Generalities 2.4.1 2.4.2 Chapter notes Exercises  Deterministic versus stochastic scenarios Bayes error and noise  2.5 2.6  Rademacher Complexity and VC-Dimension 3.1 3.2 3.3 3.4 3.5 3.6  Rademacher complexity Growth function VC-dimension Lower bounds Chapter notes Exercises  Model Selection 4.1 4.2 4.3  Estimation and approximation errors Empirical risk minimization  ERM  Structural risk minimization  SRM   xiii  1 1 2 3 4 6 7  9 9 15 19 21 21 22 23 23  29 30 34 36 43 48 50  61 61 62 64   vi  5  6  4.4 4.5 4.6 4.7 4.8 4.9  Cross-validation n-Fold cross-validation Regularization-based algorithms Convex surrogate losses Chapter notes Exercises  Support Vector Machines 5.1 5.2  Linear classiﬁcation Separable case 5.2.1 5.2.2 5.2.3 5.2.4 Non-separable case 5.3.1 5.3.2 5.3.3  Primal optimization problem Support vectors Dual optimization problem Leave-one-out analysis  Primal optimization problem Support vectors Dual optimization problem  5.3  5.4 Margin theory Chapter notes 5.5 5.6 Exercises  Deﬁnitions Reproducing kernel Hilbert space Properties  6.3  Kernel Methods Introduction 6.1 Positive deﬁnite symmetric kernels 6.2 6.2.1 6.2.2 6.2.3 Kernel-based algorithms 6.3.1 6.3.2 6.3.3 Negative deﬁnite symmetric kernels Sequence kernels 6.5.1 Weighted transducers 6.5.2 Approximate kernel feature maps Chapter notes Exercises  SVMs with PDS kernels Representer theorem Learning guarantees  Rational kernels  6.6 6.7 6.8  6.4 6.5  7  Boosting 7.1 7.2  Introduction AdaBoost 7.2.1 7.2.2 7.2.3  Bound on the empirical error Relationship with coordinate descent Practical use  Contents  68 71 72 73 77 78  79 79 80 81 83 83 85 87 88 89 90 91 100 100  105 105 108 108 110 112 116 116 117 117 119 121 122 126 130 135 137  145 145 146 149 150 154   Contents  8  9  7.3  7.4 7.5 7.6 7.7  VC-dimension-based analysis  Theoretical results 7.3.1 7.3.2 L1-geometric margin 7.3.3 Margin-based analysis 7.3.4 Margin maximization 7.3.5 Game-theoretic interpretation L1-regularization Discussion Chapter notes Exercises  Randomized weighted majority algorithm Exponential weighted average algorithm  On-Line Learning 8.1 Introduction Prediction with expert advice 8.2 8.2.1 Mistake bounds and Halving algorithm 8.2.2 Weighted majority algorithm 8.2.3 8.2.4 Linear classiﬁcation 8.3.1 8.3.2 Winnow algorithm On-line to batch conversion Game-theoretic connection Chapter notes Exercises  Perceptron algorithm  8.4 8.5 8.6 8.7  8.3  Multi-Class Classiﬁcation 9.1 Multi-class classiﬁcation problem 9.2 9.3  Decision trees  Generalization bounds Uncombined multi-class algorithms 9.3.1 Multi-class SVMs 9.3.2 Multi-class boosting algorithms 9.3.3 Aggregated multi-class algorithms 9.4.1 One-versus-all 9.4.2 One-versus-one 9.4.3 Structured prediction algorithms Chapter notes Exercises  Error-correcting output codes  9.4  9.5 9.6 9.7  10  Ranking 10.1 The problem of ranking 10.2 Generalization bound 10.3 Ranking with SVMs  vii  154 154 155 157 161 162 165 167 168 170  177 178 178 179 181 183 186 190 190 198 201 204 205 206  213 213 215 221 221 222 224 228 229 229 231 233 235 237  239 240 241 243   viii  Contents  10.4 RankBoost  10.4.1 Bound on the empirical error 10.4.2 Relationship with coordinate descent 10.4.3 Margin bound for ensemble methods in ranking  10.5 Bipartite ranking  10.5.1 Boosting in bipartite ranking 10.5.2 Area under the ROC curve  10.6 Preference-based setting  10.6.1 Second-stage ranking problem 10.6.2 Deterministic algorithm 10.6.3 Randomized algorithm 10.6.4 Extension to other loss functions  10.7 Other ranking criteria 10.8 Chapter notes 10.9 Exercises  11  Regression 11.1 The problem of regression 11.2 Generalization bounds  11.2.1 Finite hypothesis sets 11.2.2 Rademacher complexity bounds 11.2.3 Pseudo-dimension bounds  11.3 Regression algorithms  11.3.1 Linear regression 11.3.2 Kernel ridge regression 11.3.3 Support vector regression 11.3.4 Lasso 11.3.5 Group norm regression algorithms 11.3.6 On-line regression algorithms  11.4 Chapter notes 11.5 Exercises  12  Maximum Entropy Models 12.1 Density estimation problem  12.1.1 Maximum Likelihood  ML  solution 12.1.2 Maximum a Posteriori  MAP  solution  12.2 Density estimation problem augmented with features 12.3 Maxent principle 12.4 Maxent models 12.5 Dual problem 12.6 Generalization bound 12.7 Coordinate descent algorithm 12.8 Extensions 12.9 L2-regularization  244 246 248 250 251 252 255 257 257 259 260 262 262 263 264  267 267 268 268 269 271 275 275 276 281 285 289 289 290 292  295 295 296 297 297 298 299 299 303 304 306 308   Contents  12.10 Chapter notes 12.11 Exercises  13  Conditional Maximum Entropy Models 13.1 Learning problem 13.2 Conditional Maxent principle 13.3 Conditional Maxent models 13.4 Dual problem 13.5 Properties  13.5.1 Optimization problem 13.5.2 Feature vectors 13.5.3 Prediction  13.6 Generalization bounds 13.7 Logistic regression  13.7.1 Optimization problem 13.7.2 Logistic model  13.8 L2-regularization 13.9 Proof of the duality theorem 13.10 Chapter notes 13.11 Exercises  Algorithmic Stability 14.1 Deﬁnitions 14.2 Stability-based generalization guarantee 14.3 Stability of kernel-based regularization algorithms  14.3.1 Application to regression algorithms: SVR and KRR 14.3.2 Application to classiﬁcation algorithms: SVMs 14.3.3 Discussion  14.4 Chapter notes 14.5 Exercises  14  15  Dimensionality Reduction 15.1 Principal component analysis 15.2 Kernel principal component analysis  KPCA  15.3 KPCA and manifold learning  15.3.1 Isomap 15.3.2 Laplacian eigenmaps 15.3.3 Locally linear embedding  LLE   15.4 Johnson-Lindenstrauss lemma 15.5 Chapter notes 15.6 Exercises  16  Learning Automata and Languages 16.1 Introduction  ix  312 313  315 315 316 316 317 319 320 320 321 321 325 325 325 326 328 330 331  333 333 334 336 339 341 342 342 343  347 348 349 351 351 352 353 354 356 356  359 359   x  Contents  16.2 Finite automata 16.3 Efﬁcient exact learning  16.3.1 Passive learning 16.3.2 Learning with queries 16.3.3 Learning automata with queries  16.4 Identiﬁcation in the limit  16.4.1 Learning reversible automata  16.5 Chapter notes 16.6 Exercises  17  Reinforcement Learning 17.1 Learning scenario 17.2 Markov decision process model 17.3 Policy  17.3.1 Deﬁnition 17.3.2 Policy value 17.3.3 Optimal policies 17.3.4 Policy evaluation  17.4 Planning algorithms  17.4.1 Value iteration 17.4.2 Policy iteration 17.4.3 Linear programming  17.5 Learning algorithms  17.5.1 Stochastic approximation 17.5.2 TD 0  algorithm 17.5.3 Q-learning algorithm 17.5.4 SARSA 17.5.5 TD λ  algorithm 17.5.6 Large state space  17.6 Chapter notes  Conclusion  A  Linear Algebra Review A.1 Vectors and norms A.1.1 Norms A.1.2 Dual norms A.1.3 Relationship between norms  A.2 Matrices  A.2.1 Matrix norms A.2.2 A.2.3  Singular value decomposition Symmetric positive semideﬁnite  SPSD  matrices  360 361 362 363 364 369 370 375 376  379 379 380 381 381 382 382 385 387 387 390 392 393 394 397 398 402 402 403 405  407  409 409 409 410 411 411 411 412 412   Contents  B  C  D  E  Convex Optimization B.1 B.2 B.3 B.4  Differentiation and unconstrained optimization Convexity Constrained optimization Fenchel duality B.4.1 B.4.2 Core B.4.3 Conjugate functions Chapter notes Exercises  Subgradients  B.5 B.6  Probability Review C.1 C.2 C.3 C.4 C.5 C.6 Moment-generating functions C.7  Probability Random variables Conditional probability and independence Expectation and Markov’s inequality Variance and Chebyshev’s inequality  Exercises  Binomial distribution tails: Upper bounds Binomial distribution tails: Lower bound Azuma’s inequality  Hoeffding’s inequality Sanov’s theorem  Concentration Inequalities D.1 D.2 D.3 Multiplicative Chernoff bounds D.4 D.5 D.6 D.7 McDiarmid’s inequality D.8 D.9 D.10 Maximal inequality D.11 Chapter notes D.12 Exercises  Normal distribution tails: Lower bound Khintchine-Kahane inequality  Entropy Relative entropy  Notions of Information Theory E.1 E.2 E.3 Mutual information E.4 E.5 E.6  Bregman divergences Chapter notes Exercises  xi  415 415 415 419 422 422 423 423 426 427  429 429 429 431 431 432 434 435  437 437 438 439 440 440 441 442 443 443 444 445 445  449 449 450 453 453 456 457   xii  F  Notation  Bibliography  Index  Contents  459  461  475   Preface  This book is a general introduction to machine learning that can serve as a reference book for researchers and a textbook for students. It covers fundamental modern topics in machine learning while providing the theoretical basis and conceptual tools needed for the discussion and justiﬁcation of algorithms. It also describes several key aspects of the application of these algorithms.  We have aimed to present the most novel theoretical tools and concepts while giving concise proofs, even for relatively advanced results. In general, whenever possible, we have chosen to favor succinctness. Nevertheless, we discuss some crucial complex topics arising in machine learning and highlight several open research questions. Certain topics often merged with others or treated with insuﬃcient attention are discussed separately here and with more emphasis: for example, a diﬀerent chapter is reserved for multi-class classiﬁcation, ranking, and regression. Although we cover a very wide variety of important topics in machine learning, we have chosen to omit a few important ones, including graphical models and neural networks, both for the sake of brevity and because of the current lack of solid theoretical guarantees for some methods.  The book is intended for students and researchers in machine learning, statistics and other related areas. It can be used as a textbook for both graduate and advanced undergraduate classes in machine learning or as a reference text for a research seminar. The ﬁrst three or four chapters of the book lay the theoretical foundation for the subsequent material. Other chapters are mostly self-contained, with the exception of chapter 6 which introduces some concepts that are extensively used in later ones and chapter 13, which is closely related to chapter 12. Each chapter concludes with a series of exercises, with full solutions presented separately. The reader is assumed to be familiar with basic concepts in linear algebra, prob- ability, and analysis of algorithms. However, to further help, we have included an extensive appendix presenting a concise review of linear algebra, an introduction to convex optimization, a brief probability review, a collection of concentration   xiv  Preface  inequalities useful to the analyses and discussions in this book, and a short intro- duction to information theory.  Our goal has been to give a uniﬁed presentation of multiple topics and areas, as opposed to a more specialized presentation adopted by some books which favor a particular viewpoint, such as for example a Bayesian view, or a particular topic, such as for example kernel methods. The theoretical foundation of this book and its deliberate emphasis on proofs and analysis make it also very distinct from many other presentations.  In this second edition, we have updated the entire book. The changes include a diﬀerent writing style in most chapters, new ﬁgures and illustrations, many simpliﬁ- cations, some additions to existing chapters, in particular chapter 6 and chapter 17, and several new chapters. We have added a full chapter on model selection  chap- ter 4 , which is an important topic that was only brieﬂy discussed in the previous edition. We have also added a new chapter on Maximum Entropy models  chap- ter 12  and a new chapter on Conditional Maximum Entropy models  chapter 13  which are both essential topics in machine learning. We have also signiﬁcantly changed the appendix. In particular, we have added a full section on Fenchel dual- ity to appendix B on convex optimization, made a number of changes and additions to appendix D dealing with concentration inequalities, added appendix E on infor- mation theory, and updated most of the material. Additionally, we have included a number of new exercises and their solutions for existing and new chapters.  Most of the material presented here takes its origins in a machine learning grad- uate course  Foundations of Machine Learning  taught by the ﬁrst author at the Courant Institute of Mathematical Sciences in New York University over the last fourteen years. This book has considerably beneﬁted from the comments and sug- gestions from students in these classes, along with those of many friends, colleagues and researchers to whom we are deeply indebted.  We are particularly grateful to Corinna Cortes and Yishay Mansour who made a number of key suggestions for the design and organization of the material presented in the ﬁrst edition, with detailed comments that we have fully taken into account and that have greatly improved the presentation. We are also grateful to Yishay Mansour for using a preliminary version of the ﬁrst edition of the book for teaching, and for reporting his feedback to us.  We also thank for discussions, suggested improvement, and contributions of many kinds the following colleagues and friends from academic and corporate research laboratories: Jacob Abernethy, Cyril Allauzen, Kareem Amin, Stephen Boyd, Aldo Corbisiero, Giulia DeSalvo, Claudio Gentile, Spencer Greenberg, Lisa Hellerstein, Sanjiv Kumar, Vitaly Kuznetsov, Ryan McDonald, Andr`es Mu˜noz Medina, Tyler Neylon, Peter Norvig, Fernando Pereira, Maria Pershina, Borja de Balle Pigem,   Preface  xv  Ashish Rastogi, Michael Riley, Dmitry Storcheus, Ananda Theertha Suresh, Umar Syed, Csaba Szepesv´ari, Toshiyuki Tanaka, Eugene Weinstein, Jason Weston, Scott Yang, and Ningshan Zhang.  Finally, we thank the MIT Press publication team for their help and support in  the development of this text.    1 Introduction  This chapter presents a preliminary introduction to machine learning, including an overview of some key learning tasks and applications, basic deﬁnitions and termi- nology, and the discussion of some general scenarios.  1.1 What is machine learning?  Machine learning can be broadly deﬁned as computational methods using experi- ence to improve performance or to make accurate predictions. Here, experience refers to the past information available to the learner, which typically takes the form of electronic data collected and made available for analysis. This data could be in the form of digitized human-labeled training sets, or other types of informa- tion obtained via interaction with the environment. In all cases, its quality and size are crucial to the success of the predictions made by the learner.  An example of a learning problem is how to use a ﬁnite sample of randomly selected documents, each labeled with a topic, to accurately predict the topic of unseen documents. Clearly, the larger is the sample, the easier is the task. But the diﬃculty of the task also depends on the quality of the labels assigned to the documents in the sample, since the labels may not be all correct, and on the number of possible topics.  Machine learning consists of designing eﬃcient and accurate prediction algo- rithms. As in other areas of computer science, some critical measures of the quality of these algorithms are their time and space complexity. But, in machine learning, we will need additionally a notion of sample complexity to evaluate the sample size required for the algorithm to learn a family of concepts. More generally, theoreti- cal learning guarantees for an algorithm depend on the complexity of the concept classes considered and the size of the training sample.  Since the success of a learning algorithm depends on the data used, machine learn- ing is inherently related to data analysis and statistics. More generally, learning   2  Chapter 1  Introduction  techniques are data-driven methods combining fundamental concepts in computer science with ideas from statistics, probability and optimization.  1.2 What kind of problems can be tackled using machine learning?  Predicting the label of a document, also known as document classiﬁcation, is by no means the only learning task. Machine learning admits a very broad set of practical applications, which include the following:    Text or document classiﬁcation. This includes problems such as assigning a topic to a text or a document, or determining automatically if the content of a web page is inappropriate or too explicit; it also includes spam detection.    Natural language processing  NLP . Most tasks in this ﬁeld, including part-of- speech tagging, named-entity recognition, context-free parsing, or dependency parsing, are cast as learning problems. In these problems, predictions admit some structure. For example, in part-of-speech tagging, the prediction for a sentence is a sequence of part-of-speech tags labeling each word. In context-free parsing the prediction is a tree. These are instances of richer learning problems known as structured prediction problems.    Speech processing applications. This includes speech recognition, speech synthe- sis, speaker veriﬁcation, speaker identiﬁcation, as well as sub-problems such as language modeling and acoustic modeling.    Computer vision applications. This includes object recognition, object identiﬁ- cation, face detection, Optical character recognition  OCR , content-based image retrieval, or pose estimation.    Computational biology applications. This includes protein function prediction,  identiﬁcation of key sites, or the analysis of gene and protein networks.    Many other problems such as fraud detection for credit card, telephone or in- surance companies, network intrusion, learning to play games such as chess, backgammon, or Go, unassisted control of vehicles such as robots or cars, medical diagnosis, the design of recommendation systems, search engines, or information extraction systems, are tackled using machine learning techniques.  This list is by no means comprehensive. Most prediction problems found in practice can be cast as learning problems and the practical application area of machine learning keeps expanding. The algorithms and techniques discussed in this book can be used to derive solutions for all of these problems, though we will not discuss in detail these applications.   1.3 Some standard learning tasks  3  1.3 Some standard learning tasks  The following are some standard machine learning tasks that have been extensively studied:    Classiﬁcation: this is the problem of assigning a category to each item. For example, document classiﬁcation consists of assigning a category such as politics, business, sports, or weather to each document, while image classiﬁcation consists of assigning to each image a category such as car, train, or plane. The number of categories in such tasks is often less than a few hundreds, but it can be much larger in some diﬃcult tasks and even unbounded as in OCR, text classiﬁcation, or speech recognition.    Regression: this is the problem of predicting a real value for each item. Examples of regression include prediction of stock values or that of variations of economic variables. In regression, the penalty for an incorrect prediction depends on the magnitude of the diﬀerence between the true and predicted values, in contrast with the classiﬁcation problem, where there is typically no notion of closeness between various categories.    Ranking: this is the problem of learning to order items according to some criterion. Web search, e.g., returning web pages relevant to a search query, is the canonical ranking example. Many other similar ranking problems arise in the context of the design of information extraction or natural language processing systems.    Clustering: this is the problem of partitioning a set of items into homogeneous subsets. Clustering is often used to analyze very large data sets. For example, in the context of social network analysis, clustering algorithms attempt to identify natural communities within large groups of people.    Dimensionality reduction or manifold learning: this problem consists of trans- forming an initial representation of items into a lower-dimensional representation while preserving some properties of the initial representation. A common example involves preprocessing digital images in computer vision tasks.  The main practical objectives of machine learning consist of generating accurate predictions for unseen items and of designing eﬃcient and robust algorithms to produce these predictions, even for large-scale problems. To do so, a number of algorithmic and theoretical questions arise. Some fundamental questions include: Which concept families can actually be learned, and under what conditions? How well can these concepts be learned computationally?   4  Chapter 1  Introduction  1.4 Learning stages  Here, we will use the canonical problem of spam detection as a running example to illustrate some basic deﬁnitions and describe the use and evaluation of machine learning algorithms in practice, including their diﬀerent stages.  Spam detection is the problem of learning to automatically classify email messages as either spam or non-spam. The following is a list of deﬁnitions and terminology commonly used in machine learning:    Examples: Items or instances of data used for learning or evaluation. In our spam problem, these examples correspond to the collection of email messages we will use for learning and testing.    Features: The set of attributes, often represented as a vector, associated to an example. In the case of email messages, some relevant features may include the length of the message, the name of the sender, various characteristics of the header, the presence of certain keywords in the body of the message, and so on. In classiﬁcation problems, examples are assigned speciﬁc categories, for instance, the spam and non-spam categories in our binary classiﬁcation problem. In regression, items are assigned real-valued labels.    Labels: Values or categories assigned to examples.    Hyperparameters: Free parameters that are not determined by the learning algo-  rithm, but rather speciﬁed as inputs to the learning algorithm.    Training sample: Examples used to train a learning algorithm.  In our spam problem, the training sample consists of a set of email examples along with their associated labels. The training sample varies for diﬀerent learning scenarios, as described in section 1.5.    Validation sample: Examples used to tune the parameters of a learning algorithm when working with labeled data. The validation sample is used to select appro- priate values for the learning algorithm’s free parameters  hyperparameters .    Test sample: Examples used to evaluate the performance of a learning algorithm. The test sample is separate from the training and validation data and is not made available in the learning stage. In the spam problem, the test sample consists of a collection of email examples for which the learning algorithm must predict labels based on features. These predictions are then compared with the labels of the test sample to measure the performance of the algorithm.    Loss function: A function that measures the diﬀerence, or loss, between a pre- dicted label and a true label. Denoting the set of all labels as Y and the set of possible predictions as Y cid:48 , a loss function L is a mapping L : Y × Y cid:48  → R+. In most cases, Y cid:48  = Y and the loss function is bounded, but these conditions do not always hold. Common examples of loss functions include the zero-one  or   1.4 Learning stages  5  Figure 1.1 Illustration of the typical stages of a learning process.  misclassiﬁcation  loss deﬁned over {−1, +1} × {−1, +1} by L y, y cid:48   = 1y cid:48  cid:54 =y and the squared loss deﬁned over I× I by L y, y cid:48   =  y cid:48 − y 2, where I ⊆ R is typically a bounded interval.   Hypothesis set: A set of functions mapping features  feature vectors  to the set of labels Y. In our example, these may be a set of functions mapping email features to Y = {spam, non-spam}. More generally, hypotheses may be functions mapping features to a diﬀerent set Y cid:48 . They could be linear functions mapping email feature vectors to real numbers interpreted as scores  Y cid:48  = R , with higher score values more indicative of spam than lower ones.  We now deﬁne the learning stages of our spam problem  see ﬁgure 1.1 . We start with a given collection of labeled examples. We ﬁrst randomly partition the data into a training sample, a validation sample, and a test sample. The size of each of these samples depends on a number of diﬀerent considerations. For example, the amount of data reserved for validation depends on the number of hyperparameters of the algorithm, which are represented here by the vector Θ. Also, when the labeled sample is relatively small, the amount of training data is often chosen to be larger than that of the test data since the learning performance directly depends on the training sample.  Next, we associate relevant features to the examples. This is a critical step in the design of machine learning solutions. Useful features can eﬀectively guide the learning algorithm, while poor or uninformative ones can be misleading. Although it is critical, to a large extent, the choice of the features is left to the user. This choice reﬂects the user’s prior knowledge about the learning task which in practice can have a dramatic eﬀect on the performance results.  Now, we use the features selected to train our learning algorithm A by tuning the values of its free parameters Θ  also called hyperparameters . For each value of these parameters, the algorithm selects a diﬀerent hypothesis out of the hypothesis set. We choose the one resulting in the best performance on the validation sample  Θ0 . Finally, using that hypothesis, we predict the labels of the examples in the test sample. The performance of the algorithm is evaluated by using the loss  labeled data  algorithm  prior knowledge  training sample  validation data  test sample  A Θ    AAACGnicdVDLSgMxFM34rPVV7dJNsAh1U6alULuruHFZobWFdiiZ9E4bmnmQ3BGGoZ8ibvU7XIlbN36Gf2D6EKyPAyGHc+5NDseNpNBo2+ W2vrG5tZ2Zie7u7d cJg7Or7VYaw4tHkoQ9V1mQYpAmijQAndSAHzXQkdd3I18zt3oLQIgxYmETg+GwXCE5yhkQa5fN9nONZcpZfTYr81BmTng1zBLtVrdqVepb9JuWTPUSBLNAe5j 4w5LEPAXLJtO6V7QidlCkUXMI02481RIxP2Ah6hgbMB+2k8 BTemaUIfVCZU6AdK5+30iZr3Xiu2ZyHvWnNxP 8noxehdOKoIoRgj44iMvlhRDOmuCDoUCjjIxhHElTFbKx0wxjqavlZeWFTkpxOYSEU6zpqOvIuj pF0p1Uvlm2qh0ViWlSEn5JQUSZnUSINckyZpE04S8kAeyZN1bz1bL9brYnTNWu7kyQqst09hwaIM   AAACGnicdVDLSgMxFM34rPVV7dJNsAh1U6alULuruHFZobWFdiiZ9E4bmnmQ3BGGoZ8ibvU7XIlbN36Gf2D6EKyPAyGHc+5NDseNpNBo2+ W2vrG5tZ2Zie7u7d cJg7Or7VYaw4tHkoQ9V1mQYpAmijQAndSAHzXQkdd3I18zt3oLQIgxYmETg+GwXCE5yhkQa5fN9nONZcpZfTYr81BmTng1zBLtVrdqVepb9JuWTPUSBLNAe5j 4w5LEPAXLJtO6V7QidlCkUXMI02481RIxP2Ah6hgbMB+2k8 BTemaUIfVCZU6AdK5+30iZr3Xiu2ZyHvWnNxP 8noxehdOKoIoRgj44iMvlhRDOmuCDoUCjjIxhHElTFbKx0wxjqavlZeWFTkpxOYSEU6zpqOvIuj pF0p1Uvlm2qh0ViWlSEn5JQUSZnUSINckyZpE04S8kAeyZN1bz1bL9brYnTNWu7kyQqst09hwaIM   AAACGnicdVDLSgMxFM34rPVV7dJNsAh1U6alULuruHFZobWFdiiZ9E4bmnmQ3BGGoZ8ibvU7XIlbN36Gf2D6EKyPAyGHc+5NDseNpNBo2+ W2vrG5tZ2Zie7u7d cJg7Or7VYaw4tHkoQ9V1mQYpAmijQAndSAHzXQkdd3I18zt3oLQIgxYmETg+GwXCE5yhkQa5fN9nONZcpZfTYr81BmTng1zBLtVrdqVepb9JuWTPUSBLNAe5j 4w5LEPAXLJtO6V7QidlCkUXMI02481RIxP2Ah6hgbMB+2k8 BTemaUIfVCZU6AdK5+30iZr3Xiu2ZyHvWnNxP 8noxehdOKoIoRgj44iMvlhRDOmuCDoUCjjIxhHElTFbKx0wxjqavlZeWFTkpxOYSEU6zpqOvIuj pF0p1Uvlm2qh0ViWlSEn5JQUSZnUSINckyZpE04S8kAeyZN1bz1bL9brYnTNWu7kyQqst09hwaIM   AAACGnicdVDLSgMxFM34rPVV7dJNsAh1U6alULuruHFZobWFdiiZ9E4bmnmQ3BGGoZ8ibvU7XIlbN36Gf2D6EKyPAyGHc+5NDseNpNBo2+ W2vrG5tZ2Zie7u7d cJg7Or7VYaw4tHkoQ9V1mQYpAmijQAndSAHzXQkdd3I18zt3oLQIgxYmETg+GwXCE5yhkQa5fN9nONZcpZfTYr81BmTng1zBLtVrdqVepb9JuWTPUSBLNAe5j 4w5LEPAXLJtO6V7QidlCkUXMI02481RIxP2Ah6hgbMB+2k8 BTemaUIfVCZU6AdK5+30iZr3Xiu2ZyHvWnNxP 8noxehdOKoIoRgj44iMvlhRDOmuCDoUCjjIxhHElTFbKx0wxjqavlZeWFTkpxOYSEU6zpqOvIuj pF0p1Uvlm2qh0ViWlSEn5JQUSZnUSINckyZpE04S8kAeyZN1bz1bL9brYnTNWu7kyQqst09hwaIM   A Θ0    AAACHHicdVDLSgMxFM34rPVVFVdugkWomzIthdpdxY3LCh1baIeSSW b0MyD5I5QhvkWcavf4UrcCn6Gf2D6EKyPAyGHc+5NDseLpNBo2+ Wyura+sZmZiu7vbO7t587OLzVYaw4ODyUoWp7TIMUATgoUEI7UsB8T0LLG19N dYdKC3CoImTCFyfDQMxEJyhkXq5467PcKS5Si7TQrc5AmQ9+7yXy9vFWtUu1yr0NykV7RnyZIFGL fR7Yc89iFALpnWnZIdoZswhYJLSLPdWEPE+JgNoWNowHzQbjKLn9Izo TpIFTmBEhn6veNhPlaT3zPTM7C vSm4l9eJ8bBhZuIIIoRAj7 aBBLiiGddkH7QgFHOTGEcSVMVspHTDGOprGllxYluQnE5hIRplnT0VcR9H ilIu1Yummkq XF2VlyAk5JQVSIlVSJ9ekQRzCSUIeyCN5su6tZ+vFep2PrliLnSOyBOvtE6TKoq8=   AAACHHicdVDLSgMxFM34rPVVFVdugkWomzIthdpdxY3LCh1baIeSSW b0MyD5I5QhvkWcavf4UrcCn6Gf2D6EKyPAyGHc+5NDseLpNBo2+ Wyura+sZmZiu7vbO7t587OLzVYaw4ODyUoWp7TIMUATgoUEI7UsB8T0LLG19N dYdKC3CoImTCFyfDQMxEJyhkXq5467PcKS5Si7TQrc5AmQ9+7yXy9vFWtUu1yr0NykV7RnyZIFGL fR7Yc89iFALpnWnZIdoZswhYJLSLPdWEPE+JgNoWNowHzQbjKLn9Izo TpIFTmBEhn6veNhPlaT3zPTM7C vSm4l9eJ8bBhZuIIIoRAj7 aBBLiiGddkH7QgFHOTGEcSVMVspHTDGOprGllxYluQnE5hIRplnT0VcR9H ilIu1Yummkq XF2VlyAk5JQVSIlVSJ9ekQRzCSUIeyCN5su6tZ+vFep2PrliLnSOyBOvtE6TKoq8=   AAACHHicdVDLSgMxFM34rPVVFVdugkWomzIthdpdxY3LCh1baIeSSW b0MyD5I5QhvkWcavf4UrcCn6Gf2D6EKyPAyGHc+5NDseLpNBo2+ Wyura+sZmZiu7vbO7t587OLzVYaw4ODyUoWp7TIMUATgoUEI7UsB8T0LLG19N dYdKC3CoImTCFyfDQMxEJyhkXq5467PcKS5Si7TQrc5AmQ9+7yXy9vFWtUu1yr0NykV7RnyZIFGL fR7Yc89iFALpnWnZIdoZswhYJLSLPdWEPE+JgNoWNowHzQbjKLn9Izo TpIFTmBEhn6veNhPlaT3zPTM7C vSm4l9eJ8bBhZuIIIoRAj7 aBBLiiGddkH7QgFHOTGEcSVMVspHTDGOprGllxYluQnE5hIRplnT0VcR9H ilIu1Yummkq XF2VlyAk5JQVSIlVSJ9ekQRzCSUIeyCN5su6tZ+vFep2PrliLnSOyBOvtE6TKoq8=   AAACHHicdVDLSgMxFM34rPVVFVdugkWomzIthdpdxY3LCh1baIeSSW b0MyD5I5QhvkWcavf4UrcCn6Gf2D6EKyPAyGHc+5NDseLpNBo2+ Wyura+sZmZiu7vbO7t587OLzVYaw4ODyUoWp7TIMUATgoUEI7UsB8T0LLG19N dYdKC3CoImTCFyfDQMxEJyhkXq5467PcKS5Si7TQrc5AmQ9+7yXy9vFWtUu1yr0NykV7RnyZIFGL fR7Yc89iFALpnWnZIdoZswhYJLSLPdWEPE+JgNoWNowHzQbjKLn9Izo TpIFTmBEhn6veNhPlaT3zPTM7C vSm4l9eJ8bBhZuIIIoRAj7 aBBLiiGddkH7QgFHOTGEcSVMVspHTDGOprGllxYluQnE5hIRplnT0VcR9H ilIu1Yummkq XF2VlyAk5JQVSIlVSJ9ekQRzCSUIeyCN5su6tZ+vFep2PrliLnSOyBOvtE6TKoq8=   features  parameter selection  evaluation   6  Chapter 1  Introduction  function associated to the task, e.g., the zero-one loss in our spam detection task, to compare the predicted and true labels. Thus, the performance of an algorithm is of course evaluated based on its test error and not its error on the training sample.  1.5 Learning scenarios  We next brieﬂy describe some common machine learning scenarios. These scenarios diﬀer in the types of training data available to the learner, the order and method by which training data is received and the test data used to evaluate the learning algorithm.    Supervised learning: The learner receives a set of labeled examples as training data and makes predictions for all unseen points. This is the most common sce- nario associated with classiﬁcation, regression, and ranking problems. The spam detection problem discussed in the previous section is an instance of supervised learning.    Unsupervised learning: The learner exclusively receives unlabeled training data, and makes predictions for all unseen points. Since in general no labeled example is available in that setting, it can be diﬃcult to quantitatively evaluate the per- formance of a learner. Clustering and dimensionality reduction are example of unsupervised learning problems.    Semi-supervised learning: The learner receives a training sample consisting of both labeled and unlabeled data, and makes predictions for all unseen points. Semi-supervised learning is common in settings where unlabeled data is easily accessible but labels are expensive to obtain. Various types of problems arising in applications, including classiﬁcation, regression, or ranking tasks, can be framed as instances of semi-supervised learning. The hope is that the distribution of unlabeled data accessible to the learner can help him achieve a better performance than in the supervised setting. The analysis of the conditions under which this can indeed be realized is the topic of much modern theoretical and applied machine learning research.    Transductive inference: As in the semi-supervised scenario, the learner receives a labeled training sample along with a set of unlabeled test points. However, the objective of transductive inference is to predict labels only for these particular test points. Transductive inference appears to be an easier task and matches the scenario encountered in a variety of modern applications. However, as in the semi-supervised setting, the assumptions under which a better performance can be achieved in this setting are research questions that have not been fully resolved.   1.6 Generalization  7    On-line learning: In contrast with the previous scenarios, the online scenario involves multiple rounds where training and testing phases are intermixed. At each round, the learner receives an unlabeled training point, makes a prediction, receives the true label, and incurs a loss. The objective in the on-line setting is to minimize the cumulative loss over all rounds or to minimize the regret, that is the diﬀerence of the cumulative loss incurred and that of the best expert in hindsight. Unlike the previous settings just discussed, no distributional assumption is made in on-line learning. In fact, instances and their labels may be chosen adversarially within this scenario.    Reinforcement learning: The training and testing phases are also intermixed in reinforcement learning. To collect information, the learner actively interacts with the environment and in some cases aﬀects the environment, and receives an im- mediate reward for each action. The object of the learner is to maximize his reward over a course of actions and iterations with the environment. However, no long-term reward feedback is provided by the environment, and the learner is faced with the exploration versus exploitation dilemma, since he must choose between exploring unknown actions to gain more information versus exploiting the information already collected.    Active learning: The learner adaptively or interactively collects training examples, typically by querying an oracle to request labels for new points. The goal in active learning is to achieve a performance comparable to the standard supervised learning scenario  or passive learning scenario , but with fewer labeled examples. Active learning is often used in applications where labels are expensive to obtain, for example computational biology applications.  In practice, many other intermediate and somewhat more complex learning scenar- ios may be encountered.  1.6 Generalization  Machine learning is fundamentally about generalization. As an example, the stan- dard supervised learning scenario consists of using a ﬁnite sample of labeled exam- ples to make accurate predictions about unseen examples. The problem is typically formulated as that of selecting a function out of a hypothesis set, that is a subset of the family of all functions. The function selected is subsequently used to label all instances, including unseen examples.  How should a hypothesis set be chosen? With a rich or complex hypothesis set, the learner may choose a function or predictor that is consistent with the training sample, that is one that commits no error on the training sample. With a less com- plex family, incurring some errors on the training sample may be unavoidable. But,   8  Chapter 1  Introduction  Figure 1.2 The zig-zag line on the left panel is consistent over the blue and red training sample, but it is a complex separation surface that is not likely to generalize well to unseen data. In contrast, the decision surface on the right panel is simpler and might generalize better in spite of its misclassiﬁcation of a few points of the training sample.  which will lead to a better generalization? How should we deﬁne the complexity of a hypothesis set?  Figure 1.2 illustrates these two types of solution: one is a zig-zag line that perfectly separates the two populations of blue and red points and that is chosen from a complex family; the other one is a smoother line chosen from a simpler family that only imperfectly discriminates between the two sets. We will see that, in general, the best predictor on the training sample may not be the best overall. A predictor chosen from a very complex family can essentially memorize the data, but generalization is distinct from the memorization of the training labels.  We will see that the trade-oﬀ between the sample size and complexity plays a critical role in generalization. When the sample size is relatively small, choosing from a too complex a family may lead to poor generalization, which is also known as overﬁtting. On the other hand, with a too simple a family it may not be possible to achieve a suﬃcient accuracy, which is known as underﬁtting.  In the next chapters, we will analyze more in detail the problem of generalization and will seek to derive theoretical guarantees for learning. This will depend on diﬀerent notions of complexity that we will thoroughly discuss.   2 The PAC Learning Framework  Several fundamental questions arise when designing and analyzing algorithms that learn from examples: What can be learned eﬃciently? What is inherently hard to learn? How many examples are needed to learn successfully? Is there a gen- eral model of learning? In this chapter, we begin to formalize and address these questions by introducing the Probably Approximately Correct  PAC  learning frame- work. The PAC framework helps deﬁne the class of learnable concepts in terms of the number of sample points needed to achieve an approximate solution, sample complexity, and the time and space complexity of the learning algorithm, which depends on the cost of the computational representation of the concepts.  We ﬁrst describe the PAC framework and illustrate it, then present some general learning guarantees within this framework when the hypothesis set used is ﬁnite, both for the consistent case where the hypothesis set used contains the concept to learn and for the opposite inconsistent case.  2.1 The PAC learning model  We ﬁrst introduce several deﬁnitions and the notation needed to present the PAC model, which will also be used throughout much of this book.  We denote by X the set of all possible examples or instances. X is also sometimes referred to as the input space. The set of all possible labels or target values is denoted by Y. For the purpose of this introductory chapter, we will limit ourselves to the case where Y is reduced to two labels, Y = {0, 1}, which corresponds to the so-called binary classiﬁcation. Later chapters will extend these results to more general settings. A concept c : X → Y is a mapping from X to Y. Since Y = {0, 1}, we can identify c with the subset of X over which it takes the value 1. Thus, in the following, we equivalently refer to a concept to learn as a mapping from X to {0, 1}, or as a subset of X. As an example, a concept may be the set of points inside a triangle   10  Chapter 2 The PAC Learning Framework  or the indicator function of these points. In such cases, we will say in short that the concept to learn is a triangle. A concept class is a set of concepts we may wish to learn and is denoted by C. This could, for example, be the set of all triangles in the plane.  We assume that examples are independently and identically distributed  i.i.d.  according to some ﬁxed but unknown distribution D. The learning problem is then formulated as follows. The learner considers a ﬁxed set of possible concepts H, called a hypothesis set, which might not necessarily coincide with C. It re- ceives a sample S =  x1, . . . , xm  drawn i.i.d. according to D as well as the labels  c x1 , . . . , c xm  , which are based on a speciﬁc target concept c ∈ C to learn. The task is then to use the labeled sample S to select a hypothesis hS ∈ H that has a small generalization error with respect to the concept c. The generalization error of a hypothesis h ∈ H, also referred to as the risk or true error  or simply error   of h is denoted by R h  and deﬁned as follows.1 Deﬁnition 2.1  Generalization error  Given a hypothesis h ∈ H, a target concept c ∈ C, and an underlying distribution D, the generalization error or risk of h is deﬁned by  [h x   cid:54 = c x ] = E where 1ω is the indicator function of the event ω.2  R h  = P x∼D  x∼D cid:2 1h x  cid:54 =c x  cid:3  ,  The generalization error of a hypothesis is not directly accessible to the learner since both the distribution D and the target concept c are unknown. However, the learner can measure the empirical error of a hypothesis on the labeled sample S.  Deﬁnition 2.2  Empirical error  Given a hypothesis h ∈ H, a target concept c ∈ C, and a sample S =  x1, . . . , xm , the empirical error or empirical risk of h is deﬁned by   2.1    2.2   1 m  m cid:88 i=1   cid:98 RS h  =  1h xi  cid:54 =c xi .  Thus, the empirical error of h ∈ H is its average error over the sample S, while the generalization error is its expected error based on the distribution D. We will see in this chapter and the following chapters a number of guarantees relating these two quantities with high probability, under some general assumptions. We can already note that for a ﬁxed h ∈ H, the expectation of the empirical error based on an i.i.d.  1 The choice of R instead of E to denote an error avoids possible confusions with the notation for expectations and is further justiﬁed by the fact that the term risk is also used in machine learning and statistics to refer to an error. 2 For this and other related deﬁnitions, the family of functions H and the target concept c must be measurable. The function classes we consider in this book all have this property.   11   2.3   2.1 The PAC learning model  sample S is equal to the generalization error:  E  S∼Dm  [ cid:98 RS h ] = R h .  Indeed, by the linearity of the expectation and the fact that the sample is drawn i.i.d., we can write  for any x in sample S. Thus,  E  S∼Dm  1 m  m cid:88 i=1 [ cid:98 RS h ] = [ cid:98 RS h ] = E  E  S∼Dm  E  S∼Dm  [1h xi  cid:54 =c xi ] =  1 m  m cid:88 i=1  E  S∼Dm  [1h x  cid:54 =c x ],  [1h x  cid:54 =c x ] = E x∼D  S∼Dm  [1h x  cid:54 =c x ] = R h .  The following introduces the Probably Approximately Correct  PAC  learning framework. Let n be a number such that the computational cost of representing any element x ∈ X is at most O n  and denote by size c  the maximal cost of the computational representation of c ∈ C. For example, x may be a vector in Rn, for which the cost of an array-based representation would be in O n . In addition, let hS denote the hypothesis returned by algorithm A after receiving a labeled sample S. To keep notation simple, the dependency of hS on A is not explicitly indicated. Deﬁnition 2.3  PAC-learning  A concept class C is said to be PAC-learnable if there exists an algorithm A and a polynomial function poly ·,·,·,·  such that for any  cid:15  > 0 and δ > 0, for all distributions D on X and for any target concept c ∈ C, the following holds for any sample size m ≥ poly 1  cid:15 , 1 δ, n, size c  :  P  S∼Dm  [R hS  ≤  cid:15 ] ≥ 1 − δ.   2.4   If A further runs in poly 1  cid:15 , 1 δ, n, size c  , then C is said to be eﬃciently PAC- learnable. When such an algorithm A exists, it is called a PAC-learning algorithm for C.  A concept class C is thus PAC-learnable if the hypothesis returned by the algorithm after observing a number of points polynomial in 1  cid:15  and 1 δ is approximately correct  error at most  cid:15   with high probability  at least 1 − δ , which justiﬁes the PAC terminology. The parameter δ > 0 is used to deﬁne the conﬁdence 1 − δ and  cid:15  > 0 the accuracy 1 −  cid:15 . Note that if the running time of the algorithm is polynomial in 1  cid:15  and 1 δ, then the sample size m must also be polynomial if the full sample is received by the algorithm.  Several key points of the PAC deﬁnition are worth emphasizing. First, the PAC framework is a distribution-free model : no particular assumption is made about the distribution D from which examples are drawn. Second, the training sample and the test examples used to deﬁne the error are drawn according to the same distribution D. This is a natural and necessary assumption for generalization to   12  Chapter 2 The PAC Learning Framework  Figure 2.1 Target concept R and possible hypothesis R cid:48 . Circles represent training instances. A blue circle is a point labeled with 1, since it falls within the rectangle R. Others are red and labeled with 0.  be possible in general. It can be relaxed to include favorable domain adaptation problems. Finally, the PAC framework deals with the question of learnability for a concept class C and not a particular concept. Note that the concept class C is known to the algorithm, but of course the target concept c ∈ C is unknown. In many cases, in particular when the computational representation of the con- cepts is not explicitly discussed or is straightforward, we may omit the polynomial dependency on n and size c  in the PAC deﬁnition and focus only on the sample complexity.  We now illustrate PAC-learning with a speciﬁc learning problem.  Example 2.4  Learning axis-aligned rectangles  Consider the case where the set of in- stances are points in the plane, X = R2, and the concept class C is the set of all axis-aligned rectangles lying in R2. Thus, each concept c is the set of points inside a particular axis-aligned rectangle. The learning problem consists of determining with small error a target axis-aligned rectangle using the labeled training sample. We will show that the concept class of axis-aligned rectangles is PAC-learnable.  Figure 2.1 illustrates the problem. R represents a target axis-aligned rectangle and R cid:48  a hypothesis. As can be seen from the ﬁgure, the error regions of R cid:48  are formed by the area within the rectangle R but outside the rectangle R cid:48  and the area within R cid:48  but outside the rectangle R. The ﬁrst area corresponds to false negatives, that is, points that are labeled as 0 or negatively by R cid:48 , which are in fact positive or labeled with 1. The second area corresponds to false positives, that is, points labeled positively by R cid:48  which are in fact negatively labeled.  To show that the concept class is PAC-learnable, we describe a simple PAC- learning algorithm A. Given a labeled sample S, the algorithm consists of returning the tightest axis-aligned rectangle R cid:48  = RS containing the points labeled with 1. Figure 2.2 illustrates the hypothesis returned by the algorithm. By deﬁnition, RS does not produce any false positives, since its points must be included in the target concept R. Thus, the error region of RS is included in R.  R0  R   2.1 The PAC learning model  13  Figure 2.2 Illustration of the hypothesis R cid:48  = RS returned by the algorithm. Let R ∈ C be a target concept. Fix  cid:15  > 0. Let P[R] denote the probability mass  of the region deﬁned by R, that is the probability that a point randomly drawn according to D falls within R. Since errors made by our algorithm can be due only to points falling inside R, we can assume that P[R] >  cid:15 ; otherwise, the error of RS is less than or equal to  cid:15  regardless of the training sample S received. Now, since P[R] >  cid:15 , we can deﬁne four rectangular regions r1, r2, r3, and r4 along the sides of R, each with probability at least  cid:15  4. These regions can be constructed by starting with the full rectangle R and then decreasing the size by moving one side as much as possible while keeping a distribution mass of at least  cid:15  4. Figure 2.3 illustrates the deﬁnition of these regions. Let l, r, b, and t be the four real values deﬁning R: R = [l, r] × [b, t]. Then, for example, the left rectangle r4 is deﬁned by r4 = [l, s4] × [b, t], with s4 = inf{s : P[[l, s] × [b, t]] ≥  cid:15  4}. It is not hard to see that the probability of the region r4 = [l, s4[×[b, t] obtained from r4 by excluding the rightmost side is at most  cid:15  4. r1, r2, r3 and r1, r2, r3 are deﬁned in a similar way. Observe that if RS meets all of these four regions ri, i ∈ [4], then, because it is a rectangle, it will have one side in each of these regions  geometric argument . Its error area, which is the part of R that it does not cover, is thus included in the union of the regions ri, i ∈ [4], and cannot have probability mass more than  cid:15 . By contraposition, if R RS  >  cid:15 , then RS must miss at least one of the regions ri, i ∈ [4]. As a result, we can write [∪4 i=1{RS ∩ ri = ∅}] P  S∼Dm   2.5   P   by the union bound   [{RS ∩ ri = ∅}]  [R RS  >  cid:15 ] ≤ P S∼Dm 4 cid:88 i=1 ≤ ≤ 4 1 −  cid:15  4 m ≤ 4 exp −m cid:15  4 ,  S∼Dm   since P[ri] ≥  cid:15  4   R0  R   14  Chapter 2 The PAC Learning Framework  Figure 2.3 Illustration of the regions r1, . . . , r4.  where for the last step we used the general inequality 1 − x ≤ e−x valid for all x ∈ R. For any δ > 0, to ensure that PS∼Dm [R RS  >  cid:15 ] ≤ δ, we can impose  4 exp − cid:15 m 4  ≤ δ ⇔ m ≥  4  cid:15   log  4 δ  .   2.6   Thus, for any  cid:15  > 0 and δ > 0, if the sample size m is greater than 4 δ , then PS∼Dm [R RS  >  cid:15 ] ≤ δ. Furthermore, the computational cost of the represen- tation of points in R2 and axis-aligned rectangles, which can be deﬁned by their four corners, is constant. This proves that the concept class of axis-aligned rectan- gles is PAC-learnable and that the sample complexity of PAC-learning axis-aligned rectangles is in O  1   cid:15  log 4   cid:15  log 1  δ  .  An equivalent way to present sample complexity results like  2.6 , which we will often see throughout this book, is to give a generalization bound . A generalization bound states that with probability at least 1 − δ, R RS  is upper bounded by some quantity that depends on the sample size m and δ. To obtain this, it suﬃces to set δ to be equal to the upper bound derived in  2.5 , that is δ = 4 exp −m cid:15  4  and solve for  cid:15 . This yields that with probability at least 1 − δ, the error of the algorithm is bounded as follows:  R RS  ≤  4 m  log  4 δ  .   2.7   Other PAC-learning algorithms could be considered for this example. One alterna- tive is to return the largest axis-aligned rectangle not containing the negative points, for example. The proof of PAC-learning just presented for the tightest axis-aligned rectangle can be easily adapted to the analysis of other such algorithms.  Note that the hypothesis set H we considered in this example coincided with the concept class C and that its cardinality was inﬁnite. Nevertheless, the problem admitted a simple proof of PAC-learning. We may then ask if a similar proof can readily apply to other similar concept classes. This is not as straightforward because the speciﬁc geometric argument used in the proof is key. It is non-trivial to extend the proof to other concept classes such as that of non-concentric circles  r4  r1  r3  R0 r2  R   2.2 Guarantees for ﬁnite hypothesis sets — consistent case  15   see exercise 2.4 . Thus, we need a more general proof technique and more general results. The next two sections provide us with such tools in the case of a ﬁnite hypothesis set.  2.2 Guarantees for ﬁnite hypothesis sets — consistent case  In the example of axis-aligned rectangles that we examined, the hypothesis hS returned by the algorithm was always consistent, that is, it admitted no error on the training sample S. In this section, we present a general sample complexity bound, or equivalently, a generalization bound, for consistent hypotheses, in the case where the cardinality H of the hypothesis set is ﬁnite. Since we consider consistent hypotheses, we will assume that the target concept c is in H.  1  1  m ≥  Theorem 2.5  Learning bound — ﬁnite H, consistent case  Let H be a ﬁnite set of func- tions mapping from X to Y. Let A be an algorithm that for any target concept c ∈ H and i.i.d. sample S returns a consistent hypothesis hS:  cid:98 RS hS  = 0. Then, for any  cid:15 , δ > 0, the inequality PS∼Dm [R hS  ≤  cid:15 ] ≥ 1 − δ holds if  cid:15  cid:16  log H + log δ cid:17 . δ cid:17 . m cid:16  log H + log  This sample complexity result admits the following equivalent statement as a gen- eralization bound: for any  cid:15 , δ > 0, with probability at least 1 − δ,  Proof: Fix  cid:15  > 0. We do not know which consistent hypothesis hS ∈ H is selected by the algorithm A. This hypothesis further depends on the training sample S. Therefore, we need to give a uniform convergence bound , that is, a bound that holds for the set of all consistent hypotheses, which a fortiori includes hS. Thus, we will bound the probability that some h ∈ H would be consistent and have error more than  cid:15 . For any  cid:15  > 0, deﬁne H cid:15  by H cid:15  = {h ∈ H : R h  >  cid:15 }. The probability that a hypothesis h in H cid:15  is consistent on a training sample S drawn i.i.d., that is, that it would have no error on any point in S, can be bounded as follows:  R hS  ≤   2.8    2.9   1  1  Thus, by the union bound, the following holds:  P cid:104 ∃h ∈ H cid:15  :  cid:98 RS h  = 0 cid:105  = P cid:104  cid:98 RS h1  = 0 ∨ ··· ∨  cid:98 RS hH cid:15   = 0 cid:105    union bound   P[ cid:98 RS h  = 0] ≤  1 −  cid:15  m. P cid:104  cid:98 RS h  = 0 cid:105  ≤  cid:88 h∈H cid:15  ≤  cid:88 h∈H cid:15    1 −  cid:15  m ≤ H 1 −  cid:15  m ≤ He−m cid:15 .   16  Chapter 2 The PAC Learning Framework  Setting the right-hand side to be equal to δ and solving for  cid:15  concludes the proof. cid:3  The theorem shows that when the hypothesis set H is ﬁnite, a consistent algorithm A is a PAC-learning algorithm, since the sample complexity given by  2.8  is dom- inated by a polynomial in 1  cid:15  and 1 δ. As shown by  2.9 , the generalization error of consistent hypotheses is upper bounded by a term that decreases as a function of the sample size m. This is a general fact: as expected, learning algorithms beneﬁt from larger labeled training samples. The decrease rate of O 1 m  guaranteed by this theorem, however, is particularly favorable.  The price to pay for coming up with a consistent algorithm is the use of a larger hypothesis set H containing target concepts. Of course, the upper bound  2.9  increases with H. However, that dependency is only logarithmic. Note that the term log H, or the related term log2 H from which it diﬀers by a constant factor, can be interpreted as the number of bits needed to represent H. Thus, the generalization guarantee of the theorem is controlled by the ratio of this number of bits, log2 H, and the sample size m. We now use theorem 2.5 to analyze PAC-learning with various concept classes.  Example 2.6  Conjunction of Boolean literals  Consider learning the concept class Cn of conjunctions of at most n Boolean literals x1, . . . , xn. A Boolean literal is either a variable xi, i ∈ [n], or its negation xi. For n = 4, an example is the conjunction: x1 ∧ x2 ∧ x4, where x2 denotes the negation of the Boolean literal x2.  1, 0, 0, 1  is a positive example for this concept while  1, 0, 0, 0  is a negative example.  Observe that for n = 4, a positive example  1, 0, 1, 0  implies that the target con- cept cannot contain the literals x1 and x3 and that it cannot contain the literals x2 and x4. In contrast, a negative example is not as informative since it is not known which of its n bits are incorrect. A simple algorithm for ﬁnding a consistent hypoth- esis is thus based on positive examples and consists of the following: for each positive example  b1, . . . , bn  and i ∈ [n], if bi = 1 then xi is ruled out as a possible literal in the concept class and if bi = 0 then xi is ruled out. The conjunction of all the liter- als not ruled out is thus a hypothesis consistent with the target. Figure 2.4 shows an example training sample as well as a consistent hypothesis for the case n = 6. We have H = Cn = 3n, since each literal can be included positively, with nega- tion, or not included. Plugging this into the sample complexity bound for consistent hypotheses yields the following sample complexity bound for any  cid:15  > 0 and δ > 0:  m ≥  1   cid:15  cid:16  log 3 n + log  1  δ cid:17 .   2.10   Thus, the class of conjunctions of at most n Boolean literals is PAC-learnable. Note that the computational complexity is also polynomial, since the training cost per example is in O n . For δ = 0.02,  cid:15  = 0.1, and n = 10, the bound becomes   2.2 Guarantees for ﬁnite hypothesis sets — consistent case  17  Figure 2.4 Each of the ﬁrst six rows of the table represents a training example with its label, + or −, indicated in the last column. The last row contains 0  respectively 1  in column i ∈ [6] if the ith entry is 0  respectively 1  for all the positive examples. It contains “?” if both 0 and 1 appear as an ith entry for some positive example. Thus, for this training sample, the hypothesis returned by the consistent algorithm described in the text is x1 ∧ x2 ∧ x5 ∧ x6.  m ≥ 149. Thus, for a labeled sample of at least 149 examples, the bound guarantees 90% accuracy with a conﬁdence of at least 98%.  Example 2.7  Universal concept class  Consider the set X = {0, 1}n of all Boolean vectors with n components, and let Un be the concept class formed by all sub- sets of X. Is this concept class PAC-learnable? To guarantee a consistent hypo-  thesis the hypothesis class must include the concept class, thus H ≥ Un = 2 2n .  Theorem 2.5 gives the following sample complexity bound:  m ≥  1   cid:15  cid:16  log 2 2n + log  1  δ cid:17 .   2.11   Here, the number of training samples required is exponential in n, which is the cost of the representation of a point in X. Thus, PAC-learning is not guaranteed by the theorem. In fact, it is not hard to show that this universal concept class is not PAC-learnable.  Example 2.8  k-term DNF formulae  A disjunctive normal form  DNF  formula is a formula written as the disjunction of several terms, each term being a conjunction of Boolean literals. A k-term DNF is a DNF formula deﬁned by the disjunction of k terms, each term being a conjunction of at most n Boolean literals. Thus, for k = 2 and n = 3, an example of a k-term DNF is  x1 ∧ x2 ∧ x3  ∨  x1 ∧ x3 . Is the class C of k-term DNF formulae PAC-learnable? The cardinality of the class is 3nk, since each term is a conjunction of at most n variables and there are 3n such conjunctions, as seen previously. The hypothesis set H must contain C for  0  0  0  0  1  0  0  1  1  0  1  0  1  1  1  1  1  1  0  0  ?  0  1  1  1  1  0  ?  1  1  0  1  1  1  1  1  1  1  1  0  1  1  +  +  -  +  -  +   18  Chapter 2 The PAC Learning Framework  consistency to be possible, thus H ≥ 3nk. Theorem 2.5 gives the following sample complexity bound:  m ≥  1   cid:15  cid:16  log 3 nk + log  1  δ cid:17 ,   2.12   which is polynomial. However, it can be shown by a reduction from the graph 3-coloring problem that the problem of learning k-term DNF, even for k = 3, is not eﬃciently PAC-learnable, unless RP, the complexity class of problems that admit a randomized polynomial-time decision solution, coincides with NP RP = NP , which is commonly conjectured not to be the case. Thus, while the sample size needed for learning k-term DNF formulae is only polynomial, eﬃcient PAC-learning of this class is not possible if RP  cid:54 = NP. Example 2.9  k-CNF formulae  A conjunctive normal form  CNF  formula is a con- junction of disjunctions. A k-CNF formula is an expression of the form T1 ∧ . . .∧ Tj with arbitrary length j ∈ N and with each term Ti being a disjunction of at most  k Boolean attributes.  The problem of learning k-CNF formulae can be reduced to that of learning con- junctions of Boolean literals, which, as seen previously, is a PAC-learnable concept class. This can be done at the cost of introducing  2n k new variables Yu1,...,uk using the following bijection:   u1, . . . , uk  → Yu1,...,uk ,   2.13   where u1, . . . , uk are Boolean literals over the original variables x1, . . . , xn. The value of Yu1,...,uk is determined by Yu1,...,uk = u1 ∨ ··· ∨ uk. Using this mapping, the original training sample can be transformed into one deﬁned in terms of the new variables and any k-CNF formula over the original variables can be written as a conjunction over the variables Yu1,...,uk . This reduction to PAC-learning of conjunctions of Boolean literals can aﬀect the original distribution of examples, but this is not an issue since in the PAC framework no assumption is made about the distribution. Thus, using this transformation, the PAC-learnability of conjunctions of Boolean literals implies that of k-CNF formulae.  This is a surprising result, however, since any k-term DNF formula can be written as a k-CNF formula. Indeed, using associativity, a k-term DNF T1 ∨ ··· ∨ Tk with Ti = ui,1 ∧ ··· ∧ ui,ni for i ∈ [k] can be rewritten as a k-CNF formula via  k cid:95 i=1  ui,1 ∧ ··· ∧ ui,ni =  u1,j1 ∨ ··· ∨ uk,jk ,   cid:94   j1∈[n1],...,jk∈[nk]  To illustrate this rewriting in a speciﬁc case, observe, for example, that   u1 ∧ u2 ∧ u3  ∨  v1 ∧ v2 ∧ v3  =   ui ∨ vj .  3 cid:94 i,j=1   2.3 Guarantees for ﬁnite hypothesis sets — inconsistent case  19  But, as we previously saw, k-term DNF formulae are not eﬃciently PAC-learnable if RP  cid:54 = NP! What can explain this apparent inconsistency? The issue is that converting into a k-term DNF a k-CNF formula we have learned  which is equivalent to a k-term DNF  is in general intractable if RP  cid:54 = NP. This example reveals some key aspects of PAC-learning, which include the cost of the representation of a concept and the choice of the hypothesis set. For a ﬁxed concept class, learning can be intractable or not depending on the choice of the representation.  2.3 Guarantees for ﬁnite hypothesis sets — inconsistent case  In the most general case, there may be no hypothesis in H consistent with the la- beled training sample. This, in fact, is the typical case in practice, where the learning problems may be somewhat diﬃcult or the concept classes more complex than the hypothesis set used by the learning algorithm. However, inconsistent hy- potheses with a small number of errors on the training sample can be useful and, as we shall see, can beneﬁt from favorable guarantees under some assumptions. This section presents learning guarantees precisely for this inconsistent case and ﬁnite hypothesis sets.  To derive learning guarantees in this more general setting, we will use Hoeﬀding’s inequality  theorem D.2  or the following corollary, which relates the generalization error and empirical error of a single hypothesis. Corollary 2.10 Fix  cid:15  > 0. Then, for any hypothesis h : X → {0, 1}, the following inequalities hold:  By the union bound, this implies the following two-sided inequality:  P  P  S∼Dm cid:104  cid:98 RS h  − R h  ≥  cid:15  cid:105  ≤ exp −2m cid:15 2  S∼Dm cid:104  cid:98 RS h  − R h  ≤ − cid:15  cid:105  ≤ exp −2m cid:15 2 . S∼Dm cid:104  cid:12  cid:12  cid:98 RS h  − R h  cid:12  cid:12  ≥  cid:15  cid:105  ≤ 2 exp −2m cid:15 2 .  P  Proof: The result follows immediately from theorem D.2.  Setting the right-hand side of  2.16  to be equal to δ and solving for  cid:15  yields  immediately the following bound for a single hypothesis. Corollary 2.11  Generalization bound — single hypothesis  Fix a hypothesis h : X → {0, 1}. Then, for any δ > 0, the following inequality holds with probability at least 1 − δ:  R h  ≤  cid:98 RS h  + cid:115  log 2  δ 2m  .  The following example illustrates this corollary in a simple case.   2.14    2.15    2.16   cid:3    2.17    20  Chapter 2 The PAC Learning Framework  Example 2.12  Tossing a coin  Imagine tossing a biased coin that lands heads with probability p, and let our hypothesis be the one that always guesses tails. Then  the true error rate is R h  = p and the empirical error rate  cid:98 RS h  = cid:98 p, where  cid:98 p is  the empirical probability of heads based on the training sample drawn i.i.d. Thus, corollary 2.11 guarantees with probability at least 1 − δ that  .  δ 2m  p − cid:98 p ≤ cid:115  log 2 p − cid:98 p ≤ cid:114  log 10   Therefore, if we choose δ = 0.02 and use a sample of size 500, with probability at  least 98%, the following approximation quality is guaranteed for  cid:98 p:  1000 ≈ 0.048.  Can we readily apply corollary 2.11 to bound the generalization error of the hypothesis hS returned by a learning algorithm when training on a sample S? No, since hS is not a ﬁxed hypothesis, but a random variable depending on the training sample S drawn. Note also that unlike the case of a ﬁxed hypothesis for which the expectation of the empirical error is the generalization error  equation  2.3  , the generalization error R hS  is a random variable and in general distinct from the  expectation E[ cid:98 RS hS ], which is a constant.  Thus, as in the proof for the consistent case, we need to derive a uniform conver- gence bound, that is a bound that holds with high probability for all hypotheses h ∈ H. Theorem 2.13  Learning bound — ﬁnite H, inconsistent case  Let H be a ﬁnite hypoth- esis set. Then, for any δ > 0, with probability at least 1− δ, the following inequality holds:   2.18    2.19   ∀h ∈ H, R h  ≤  cid:98 RS h  + cid:115  log H + log 2  2m  δ  .   2.20   Proof: Let h1, . . . , hH be the elements of H. Using the union bound and applying corollary 2.11 to each hypothesis yield:  P cid:104 ∃h ∈ H cid:12  cid:12  cid:98 RS h  − R h  cid:12  cid:12  >  cid:15  cid:105   = P cid:104  cid:0  cid:12  cid:12  cid:98 RS h1  − R h1  cid:12  cid:12  >  cid:15  cid:1  ∨ . . . ∨ cid:0  cid:12  cid:12  cid:98 RS hH  − R hH  cid:12  cid:12  >  cid:15  cid:1  cid:105  ≤  cid:88 h∈H ≤ 2H exp −2m cid:15 2 .  P cid:104  cid:12  cid:12  cid:98 RS h  − R h  cid:12  cid:12  >  cid:15  cid:105   Setting the right-hand side to be equal to δ completes the proof.   cid:3    2.4 Generalities  21  Thus, for a ﬁnite hypothesis set H,  R h  ≤  cid:98 RS h  + O cid:32  cid:114  log2 H m  cid:33  .  As already pointed out, log2 H can be interpreted as the number of bits needed to represent H. Several other remarks similar to those made on the generalization bound in the consistent case can be made here: a larger sample size m guarantees better generalization, and the bound increases with H, but only logarithmically. But, here, the bound is a less favorable function of log2 H m ; it varies as the square root of this term. This is not a minor price to pay: for a ﬁxed H, to attain the same guarantee as in the consistent case, a quadratically larger labeled sample is needed. Note that the bound suggests seeking a trade-oﬀ between reducing the empirical error versus controlling the size of the hypothesis set: a larger hypothesis set is penalized by the second term but could help reduce the empirical error, that is the ﬁrst term. But, for a similar empirical error, it suggests using a smaller hypothesis set. This can be viewed as an instance of the so-called Occam’s Razor principle named after the theologian William of Occam: Plurality should not be posited with- out necessity, also rephrased as, the simplest explanation is best. In this context, it could be expressed as follows: All other things being equal, a simpler  smaller  hypothesis set is better.  2.4 Generalities  In this section we will discuss some general aspects of the learning scenario, which, for simplicity, we left out of the discussion of the earlier sections.  2.4.1 Deterministic versus stochastic scenarios In the most general scenario of supervised learning, the distribution D is deﬁned over X× Y, and the training data is a labeled sample S drawn i.i.d. according to D:  S =   x1, y1 , . . . ,  xm, ym  .  The learning problem is to ﬁnd a hypothesis h ∈ H with small generalization error  R h  = P  [h x   cid:54 = y] = E   x,y ∼D  [1h x  cid:54 =y].   x,y ∼D  This more general scenario is referred to as the stochastic scenario. Within this setting, the output label is a probabilistic function of the input. The stochastic scenario captures many real-world problems where the label of an input point is not unique. For example, if we seek to predict gender based on input pairs formed by the height and weight of a person, then the label will typically not be unique.   22  Chapter 2 The PAC Learning Framework  For most pairs, both male and female are possible genders. For each ﬁxed pair, there would be a probability distribution of the label being male.  The natural extension of the PAC-learning framework to this setting is known as  the agnostic PAC-learning. Deﬁnition 2.14  Agnostic PAC-learning  Let H be a hypothesis set. A is an agnostic PAC-learning algorithm if there exists a polynomial function poly ·,·,·,·  such that for any  cid:15  > 0 and δ > 0, for all distributions D over X × Y, the following holds for any sample size m ≥ poly 1  cid:15 , 1 δ, n, size c  :  P  S∼Dm  [R hS  − min h∈H  R h  ≤  cid:15 ] ≥ 1 − δ.   2.21   If A further runs in poly 1  cid:15 , 1 δ, n , then it is said to be an eﬃcient agnostic PAC-learning algorithm.  When the label of a point can be uniquely determined by some measurable func- tion f : X → Y  with probability one , then the scenario is said to be deterministic. In that case, it suﬃces to consider a distribution D over the input space. The training sample is obtained by drawing  x1, . . . , xm  according to D and the labels are obtained via f : yi = f  xi  for all i ∈ [m]. Many learning problems can be formulated within this deterministic scenario.  In the previous sections, as well as in most of the material presented in this book, we have restricted our presentation to the deterministic scenario in the interest of simplicity. However, for all of this material, the extension to the stochastic scenario should be straightforward for the reader.  2.4.2 Bayes error and noise In the deterministic case, by deﬁnition, there exists a target function f with no generalization error: R h  = 0. In the stochastic case, there is a minimal non-zero error for any hypothesis. Deﬁnition 2.15  Bayes error  Given a distribution D over X × Y, the Bayes error R∗ is deﬁned as the inﬁmum of the errors achieved by measurable functions h : X → Y:  2.22   R cid:63  =  R h .  inf h  h measurable  A hypothesis h with R h  = R∗ is called a Bayes hypothesis or Bayes classiﬁer.  By deﬁnition, in the deterministic case, we have R∗ = 0, but, in the stochastic case, R∗  cid:54 = 0. Clearly, the Bayes classiﬁer hBayes can be deﬁned in terms of the conditional probabilities as:  ∀x ∈ X,  hBayes x  = argmax y∈{0,1}  P[yx].   2.23    2.5 Chapter notes  23  The average error made by hBayes on x ∈ X is thus min{P[0x], P[1x]}, and this is  the minimum possible error. This leads to the following deﬁnition of noise. Deﬁnition 2.16  Noise  Given a distribution D over X × Y, the noise at point x ∈ X is deﬁned by  2.24   noise x  = min{P[1x], P[0x]}.  The average noise or the noise associated to D is E[noise x ]. Thus, the average noise is precisely the Bayes error: noise = E[noise x ] = R∗. The noise is a characteristic of the learning task indicative of its level of diﬃculty. A point x ∈ X, for which noise x  is close to 1 2, is sometimes referred to as noisy and is of course a challenge for accurate prediction.  2.5 Chapter notes  The PAC learning framework was introduced by Valiant [1984]. The book of Kearns and Vazirani [1994] is an excellent reference dealing with most aspects of PAC- learning and several other foundational questions in machine learning. Our example of learning axis-aligned rectangles, also discussed in that reference, is originally due to Blumer et al. [1989].  The PAC learning framework is a computational framework since it takes into ac- count the cost of the computational representations and the time complexity of the learning algorithm. If we omit the computational aspects, it is similar to the learn- ing framework considered earlier by Vapnik and Chervonenkis [see Vapnik, 2000]. The deﬁnition of noise presented in this chapter can be generalized to arbitrary loss functions  see exercise 2.14 .  Occam’s razor principle is invoked in a variety of contexts, such as in linguistics to justify the superiority of a set of rules or syntax. The Kolmogorov complexity can be viewed as the corresponding framework in information theory. In the context of the learning guarantees presented in this chapter, the principle suggests selecting the most parsimonious explanation  the hypothesis set with the smallest cardinality . We will see in the next sections other applications of this principle with diﬀerent notions of simplicity or complexity.  2.6 Exercises  2.1 Two-oracle variant of the PAC model. Assume that positive and negative ex- amples are now drawn from two separate distributions D+ and D−. For an accuracy  1 −  cid:15  , the learning algorithm must ﬁnd a hypothesis h such that:  P  x∼D+  [h x  = 0] ≤  cid:15  and  [h x  = 1] ≤  cid:15  .  P  x∼D−   2.25    24  Chapter 2 The PAC Learning Framework   a    b   Figure 2.5  a  Gertrude’s regions r1, r2, r3.  b  Hint for solution.  Thus, the hypothesis must have a small error on both distributions. Let C be any concept class and H be any hypothesis space. Let h0 and h1 represent the identically 0 and identically 1 functions, respectively. Prove that C is eﬃciently PAC-learnable using H in the standard  one-oracle  PAC model if and only if it is eﬃciently PAC-learnable using H ∪ {h0, h1} in this two-oracle PAC model. 2.2 PAC learning of hyper-rectangles. An axis-aligned hyper-rectangle in Rn is a set of the form [a1, b1] × . . . × [an, bn]. Show that axis-aligned hyper-rectangles are PAC-learnable by extending the proof given in Example 2.4 for the case n = 2. 2.3 Concentric circles. Let X = R2 and consider the set of concepts of the form c = { x, y  : x2 + y2 ≤ r2} for some real number r. Show that this class can be   cid:15 , δ -PAC-learned from training data of size m ≥  1  cid:15   log 1 δ . 2.4 Non-concentric circles. Let X = R2 and consider the set of concepts of the form c = {x ∈ R2 : x − x0 ≤ r} for some point x0 ∈ R2 and real number r. Gertrude, an aspiring machine learning researcher, attempts to show that this class of concepts may be   cid:15 , δ -PAC-learned with sample complexity m ≥  3  cid:15   log 3 δ , but she is having trouble with her proof. Her idea is that the learning algorithm would select the smallest circle consistent with the training data. She has drawn three regions r1, r2, r3 around the edge of concept c, with each region having probability  cid:15  3  see ﬁgure 2.5 a  . She wants to argue that if the generalization error is greater than or equal to  cid:15 , then one of these regions must have been missed by the training data, and hence this event will occur with probability at most δ. Can you tell Gertrude if her approach works?  Hint: You may wish to use ﬁgure 2.5 b  in your solution .  r1  r2  r3  r1  r2  r3   2.6 Exercises  25  Figure 2.6 Axis-aligned right triangles.  2.5 Triangles. Let X = R2 with orthonormal basis  e1, e2 , and consider the set of concepts deﬁned by the area inside a right triangle ABC with two sides parallel to the axes, with −−→AB  cid:107 −−→AB cid:107  = e1 and −→AC  cid:107 −→AC cid:107  = e2, and  cid:107 −−→AB cid:107   cid:107 −→AC cid:107  = α for some positive real α ∈ R+. Show, using similar methods to those used in the chapter for the axis-aligned rectangles, that this class can be   cid:15 , δ -PAC-learned from training data of size m ≥  3  cid:15   log 3 δ .  Hint: You may consider using ﬁgure 2.6 in your solution .  2.6 Learning in the presence of noise — rectangles.  In example 2.4, we showed that the concept class of axis-aligned rectangles is PAC-learnable. Consider now the case where the training points received by the learner are subject to the following noise: points negatively labeled are unaﬀected by noise but the label of a positive training point is randomly ﬂipped to negative with probability η ∈  0, 1 2  . The exact value of the noise rate η is not known to the learner but an upper bound η cid:48  is supplied to him with η ≤ η cid:48  < 1 2. Show that the algorithm returning the tightest rectangle containing positive points can still PAC-learn axis-aligned rectangles in the presence of this noise. To do so, you can proceed using the following steps:  a  Using the same notation as in example 2.4, assume that P[R] >  cid:15 . Suppose that R R cid:48   >  cid:15 . Give an upper bound on the probability that R cid:48  misses a region rj, j ∈ [4] in terms of  cid:15  and η cid:48 ?   b  Use that to give an upper bound on P[R R cid:48   >  cid:15 ] in terms of  cid:15  and η cid:48  and  conclude by giving a sample complexity bound.  2.7 Learning in the presence of noise — general case.  In this question, we will seek a result that is more general than in the previous question. We consider a ﬁnite hypothesis set H, assume that the target concept is in H, and adopt the following noise model: the label of a training point received by the learner is  C  A  C”  A”  A’  B”  B’  B   26  Chapter 2 The PAC Learning Framework  randomly changed with probability η ∈  0, 1 2  . The exact value of the noise rate η is not known to the learner but an upper bound η cid:48  is supplied to him with η ≤ η cid:48  < 1 2.  a  For any h ∈ H, let d h  denote the probability that the label of a training point received by the learner disagrees with the one given by h. Let h∗ be the target hypothesis, show that d h∗  = η.   b  More generally, show that for any h ∈ H, d h  = η +  1 − 2η  R h , where  R h  denotes the generalization error of h.   c  Fix  cid:15  > 0 for this and all the following questions. Use the previous questions  fraction of the points in S whose labels disagree with those given by h. We will consider the algorithm L which, after receiving S, returns the hypothesis  to show that if R h  >  cid:15 , then d h  − d h∗  ≥  cid:15  cid:48 , where  cid:15  cid:48  =  cid:15  1 − 2η cid:48  .  d  For any hypothesis h ∈ H and sample S of size m, let  cid:98 d h  denote the hS with the smallest number of disagreements  thus  cid:98 d hS  is minimal . To with high probability  cid:98 d h  ≥  cid:98 d h∗ . First, show that for any δ > 0, with probability at least 1 − δ 2, for m ≥ 2  show PAC-learning for L, we will show that for any h, if R h  >  cid:15 , then  δ , the following holds:   cid:15  cid:48 2 log 2   e  Second, show that for any δ > 0, with probability at least 1 − δ 2, for  m ≥ 2   cid:15  cid:48 2  log H + log 2  δ  , the following holds for all h ∈ H:   f  Finally, show that for any δ > 0, with probability at least 1 − δ, for m ≥ δ  , the following holds for all h ∈ H with R h  >  cid:15 :   cid:15 2 1−2η cid:48  2  log H + log 2  2  2.8 Learning intervals. Give a PAC-learning algorithm for the concept class C  use previous questions to lower bound each of these three terms .   Hint: use  cid:98 d h − cid:98 d h∗  = [ cid:98 d h − d h ] + [d h − d h∗ ] + [d h∗ − cid:98 d h∗ ] and formed by closed intervals [a, b] with a, b ∈ R. C2 formed by unions of two closed intervals, that is [a, b]∪[c, d], with a, b, c, d ∈ R. Extend your result to derive a PAC-learning algorithm for the concept class Cp formed by unions of p ≥ 1 closed intervals, thus [a1, b1] ∪ ··· ∪ [ap, bp], with ak, bk ∈ R for k ∈ [p]. What are the time and sample complexities of your  2.9 Learning union of intervals. Give a PAC-learning algorithm for the concept class  algorithm as a function of p?   cid:98 d h∗  − d h∗  ≤  cid:15  cid:48  2 d h  − cid:98 d h  ≤  cid:15  cid:48  2   cid:98 d h  − cid:98 d h∗  ≥ 0.   2.6 Exercises  27  2.10 Consistent hypotheses. In this chapter, we showed that for a ﬁnite hypothesis set H, a consistent learning algorithm A is a PAC-learning algorithm. Here, we consider a converse question. Let Z be a ﬁnite set of m labeled points. Suppose that you are given a PAC-learning algorithm A. Show that you can use A and a ﬁnite training sample S to ﬁnd in polynomial time a hypothesis h ∈ H that is consistent with Z, with high probability.  Hint: you can select an appropriate distribution D over Z and give a condition on R h  for h to be consistent.   2.11 Senate laws. For important questions, President Mouth relies on expert advice.  He selects an appropriate advisor from a collection of H = 2,800 experts.   a  Assume that laws are proposed in a random fashion independently and iden- tically according to some distribution D determined by an unknown group of senators. Assume that President Mouth can ﬁnd and select an expert senator out of H who has consistently voted with the majority for the last m = 200 laws. Give a bound on the probability that such a senator incor- rectly predicts the global vote for a future law. What is the value of the bound with 95% conﬁdence?   b  Assume now that President Mouth can ﬁnd and select an expert senator out of H who has consistently voted with the majority for all but m cid:48  = 20 of the last m = 200 laws. What is the value of the new bound?  2.12 Bayesian bound. Let H be a countable hypothesis set of functions mapping X to {0, 1} and let p be a probability measure over H. This probability measure represents the prior probability over the hypothesis class, i.e. the probability that a particular hypothesis is selected by the learning algorithm. Use Hoeﬀding’s inequality to show that for any δ > 0, with probability at least 1 − δ, the following inequality holds:  ∀h ∈ H, R h  ≤  cid:98 RS h  + cid:115  log 1  p h  + log 1  δ  .  2m   2.26   Compare this result with the bound given in the inconsistent case for ﬁnite hypothesis sets  Hint: you could use δ cid:48  = p h δ as conﬁdence parameter in Hoeﬀding’s inequality .  2.13 Learning with an unknown parameter.  In example 2.9, we showed that the concept class of k-CNF is PAC-learnable. Note, however, that the learning algorithm is given k as input. Is PAC-learning possible even when k is not provided? More generally, consider a family of concept classes {Cs}s where Cs is the set of concepts in C with size at most s. Suppose we have a PAC-learning algorithm A that can be used for learning any concept class Cs when s is given.   28  Chapter 2 The PAC Learning Framework  Can we convert A into a PAC-learning algorithm B that does not require the knowledge of s? This is the main objective of this problem.  To do this, we ﬁrst introduce a method for testing a hypothesis h, with high probability. Fix  cid:15  > 0, δ > 0, and i ≥ 1 and deﬁne the sample size n by n = 32 δ ]. Suppose we draw an i.i.d. sample S of size n according to some unknown distribution D. We will say that a hypothesis h is accepted if it makes at most 3 4 cid:15  errors on S and that it is rejected otherwise. Thus, h is   cid:15  [i log 2 + log 2   a  Assume that R h  ≥  cid:15 . Use the  multiplicative  Chernoﬀ bound to show   b  Assume that R h  ≤  cid:15  2. Use the  multiplicative  Chernoﬀ bounds to show   c  Algorithm B is deﬁned as follows: we start with i = 1 and, at each round δ  cid:99 . We draw a sample S of size n  which depends on i  to test the hypothesis hi returned  accepted iﬀ  cid:98 R h  ≤ 3 4 cid:15 . that in that case PS∼Dn [h is accepted] ≤ δ 2i+1 . that in that case PS∼Dn [h is rejected] ≤ δ 2i+1 . i ≥ 1, we guess the parameter size s to be  cid:101 s =  cid:98 2 i−1   log 2 by A when it is trained with a sample of size SA  cid:15  2, 1 2, cid:101 s , that is the  cid:101 s  we ignore the size of the representation of each example here . If hi is next iteration. Show that if at iteration i, the estimate  cid:101 s is larger than or equal to s, then P[hi is accepted] ≥ 3 8.  d  Show that the probability that B does not halt after j =  cid:100 log 2 ations with cid:101 s ≥ s is at most δ 2.  e  Show that for i ≥  cid:100 1 +  log2 s  log 2 δ cid:101 , the inequality cid:101 s ≥ s holds.  f  Show that with probability at least 1 − δ, algorithm B halts after at most j cid:48  =  cid:100 1 +  log2 s  log 2 δ cid:101  + j iterations and returns a hypothesis with error at most  cid:15 .  sample complexity of A for a required precision  cid:15  2, conﬁdence 1 2, and size  accepted, the algorithm stops and returns hi, otherwise it proceeds to the  5 cid:101  iter-  δ   log 8  2.14 In this exercise, we generalize the notion of noise to the case of an arbitrary loss  function L : Y × Y → R+.  a  Justify the following deﬁnition of the noise at point x ∈ X:  noise x  = min y cid:48 ∈Y  E [L y, y cid:48  x]. y  What is the value of noise x  in a deterministic scenario? Does the deﬁnition match the one given in this chapter for binary classiﬁcation?   b  Show that the average noise coincides with the Bayes error  minimum loss  achieved by a measurable function .   3 Rademacher Complexity and VC-Dimension  The hypothesis sets typically used in machine learning are inﬁnite. But the sample complexity bounds of the previous chapter are uninformative when dealing with inﬁnite hypothesis sets. One could ask whether eﬃcient learning from a ﬁnite sample is even possible when the hypothesis set H is inﬁnite. Our analysis of the family of axis-aligned rectangles  Example 2.4  indicates that this is indeed possible at least in some cases, since we proved that that inﬁnite concept class was PAC- learnable. Our goal in this chapter will be to generalize that result and derive general learning guarantees for inﬁnite hypothesis sets.  A general idea for doing so consists of reducing the inﬁnite case to the analysis of ﬁnite sets of hypotheses and then proceed as in the previous chapter. There are diﬀerent techniques for that reduction, each relying on a diﬀerent notion of complexity for the family of hypotheses. The ﬁrst complexity notion we will use is that of Rademacher complexity. This will help us derive learning guarantees using relatively simple proofs based on McDiarmid’s inequality, while obtaining high- quality bounds, including data-dependent ones, which we will frequently make use of in future chapters. However, the computation of the empirical Rademacher complexity is NP-hard for some hypothesis sets. Thus, we subsequently introduce two other purely combinatorial notions, the growth function and the VC-dimension. We ﬁrst relate the Rademacher complexity to the growth function and then bound the growth function in terms of the VC-dimension. The VC-dimension is often easier to bound or estimate. We will review a series of examples showing how to compute or bound it, then relate the growth function and the VC-dimensions. This leads to generalization bounds based on the VC-dimension. Finally, we present lower bounds based on the VC-dimension for two diﬀerent settings: The realizable setting, where there is at least one hypothesis in the hypothesis set under consideration that achieves zero expected error, as well as the non-realizable setting, where no hypothesis in the set achieves zero expected error.   30  Chapter 3 Rademacher Complexity and VC-Dimension  3.1 Rademacher complexity  We will continue to use H to denote a hypothesis set as in the previous chapters. Many of the results of this section are general and hold for an arbitrary loss function  L : Y× Y → R. In what follows, G will generally be interpreted as the family of loss functions associated to H mapping from Z = X × Y to R: G = {g :  x, y   cid:55 → L h x , y  : h ∈ H}.  However, the deﬁnitions are given in the general case of a family of functions G mapping from an arbitrary input space Z to R.  The Rademacher complexity captures the richness of a family of functions by measuring the degree to which a hypothesis set can ﬁt random noise. The following states the formal deﬁnitions of the empirical and average Rademacher complexity.  Deﬁnition 3.1  Empirical Rademacher complexity  Let G be a family of functions map- ping from Z to [a, b] and S =  z1, . . . , zm  a ﬁxed sample of size m with elements in Z. Then, the empirical Rademacher complexity of G with respect to the sample S is deﬁned as:  σ cid:34 sup  g∈G  1 m  m cid:88 i=1  σig zi  cid:35  ,   cid:98 RS G  = E   3.1   where σ =  σ1, . . . , σm  cid:62 , with σis independent uniform random variables taking values in {−1, +1}.3 The random variables σi are called Rademacher variables. Let gS denote the vector of values taken by function g over the sample S: gS =  g z1 , . . . , g zm   cid:62 . Then, the empirical Rademacher complexity can be rewritten as  σ cid:34 sup  g∈G  σ · gS  m  cid:35  .   cid:98 RS G  = E  The inner product σ · gS measures the correlation of gS with the vector of random σ·gS m is a measure of how well the function class G noise σ. The supremum supg∈G correlates with σ over the sample S. Thus, the empirical Rademacher complexity measures on average how well the function class G correlates with random noise on S. This describes the richness of the family G: richer or more complex families G can generate more vectors gS and thus better correlate with random noise, on average.  3 We assume implicitly that the supremum over the family G in this deﬁnition is measurable and in general will adopt the same assumption throughout this book for other suprema over a class of functions. This assumption does not hold for arbitrary function classes but it is valid for the hypotheses sets typically considered in practice in machine learning, and the instances discussed in this book.   3.1 Rademacher complexity  31   3.2   Deﬁnition 3.2  Rademacher complexity  Let D denote the distribution according to which samples are drawn. For any integer m ≥ 1, the Rademacher complexity of G is the expectation of the empirical Rademacher complexity over all samples of size m drawn according to D:  Rm G  = E  S∼Dm  [ cid:98 RS G ].  We are now ready to present our ﬁrst generalization bounds based on Rademacher complexity.  1 m  δ 2m  E[g z ] ≤  and E[g z ] ≤  Theorem 3.3 Let G be a family of functions mapping from Z to [0, 1]. Then, for any δ > 0, with probability at least 1 − δ over the draw of an i.i.d. sample S of size m, each of the following holds for all g ∈ G:  g zi  + 2Rm G  + cid:115  log 1 m cid:88 i=1 g zi  + 2 cid:98 RS G  + 3 cid:115  log 2 m cid:88 i=1 Proof: For any sample S =  z1, . . . , zm  and any g ∈ G, we denote by cid:98 ES[g] the em- m cid:80 m pirical average of g over S:  cid:98 ES[g] = 1 g∈G cid:16  E[g] − cid:98 ES[g] cid:17 .  Let S and S cid:48  be two samples diﬀering by exactly one point, say zm in S and z cid:48 m in S cid:48 . Then, since the diﬀerence of suprema does not exceed the supremum of the diﬀerence, we have  McDiarmid’s inequality to function Φ deﬁned for any sample S by  i=1 g zi . The proof consists of applying  Φ S  = sup  δ 2m  1 m   3.5    3.3    3.4   .  Φ S cid:48   − Φ S  ≤ sup  g zm  − g z cid:48 m   m  1 m  .  ≤   3.6   Similarly, we can obtain Φ S  − Φ S cid:48   ≤ 1 m, thus Φ S  − Φ S cid:48   ≤ 1 m. Then, by McDiarmid’s inequality, for any δ > 0, with probability at least 1 − δ 2, the following holds:  g∈G  g∈G cid:16  cid:98 ES[g] − cid:98 ES cid:48 [g] cid:17  = sup [Φ S ] + cid:115  log 2  Φ S  ≤ E  δ 2m  S  .   3.7    32  Chapter 3 Rademacher Complexity and VC-Dimension  We next bound the expectation of the right-hand side as follows:  E [Φ S ] = E S  E  g∈G  1 m  S cid:104  sup g∈G cid:0  E[g] − cid:98 ES g  cid:1  cid:105  S cid:48  cid:2  cid:98 ES cid:48  g  − cid:98 ES g  cid:3  cid:105  S cid:104  sup = E S,S cid:48  cid:104  sup g∈G cid:0  cid:98 ES cid:48  g  − cid:98 ES g  cid:1  cid:105  ≤ E S,S cid:48  cid:104  sup  g z cid:48 i  − g zi   cid:105  m cid:88 i=1 = E σ,S,S cid:48  cid:104  sup σi g z cid:48 i  − g zi   cid:105  m cid:88 i=1 σig z cid:48 i  cid:105  + E σ,S cid:48  cid:104  sup σ,S cid:104  sup m cid:88 i=1 ≤ E σig zi  cid:105  = 2Rm G . σ,S cid:104  sup m cid:88 i=1 = 2 E  g∈G 1 m  = E  1 m  1 m  g∈G  g∈G  g∈G  g∈G  1 m  m cid:88 i=1  −σig zi  cid:105    3.8    3.9    3.10    3.11    3.12    3.13   of the supremum function.  Equation  3.8  uses the fact that points in S cid:48  are sampled in an i.i.d. fashion and  thus E[g] = ES cid:48 [ cid:98 ES cid:48  g ], as in  2.3 . Inequality 3.9 holds due to the sub-additivity  In equation  3.11 , we introduce Rademacher variables σi, which are uniformly distributed independent random variables taking values in {−1, +1} as in deﬁni- tion 3.2. This does not change the expectation appearing in  3.10 : when σi = 1, the associated summand remains unchanged; when σi = −1, the associated sum- mand ﬂips signs, which is equivalent to swapping zi and z cid:48 i between S and S cid:48 . Since we are taking the expectation over all possible S and S cid:48 , this swap does not aﬀect the overall expectation; we are simply changing the order of the summands within the expectation.  Equation  3.12  holds by the sub-additivity of the supremum function, that is the inequality sup U + V   ≤ sup U   + sup V  . Finally,  3.13  stems from the deﬁnition of Rademacher complexity and the fact that the variables σi and −σi are distributed in the same way. The reduction to Rm G  in equation  3.13  yields the bound in equation  3.3 ,  using δ instead of δ 2. To derive a bound in terms of  cid:98 RS G , we observe that, by deﬁnition 3.1, changing one point in S changes  cid:98 RS G  by at most 1 m. Then, using  again McDiarmid’s inequality, with probability 1 − δ 2 the following holds:   3.14   Rm G  ≤  cid:98 RS G  + cid:115  log 2  δ 2m  .   3.1 Rademacher complexity  Finally, we use the union bound to combine inequalities 3.7 and 3.14, which yields with probability at least 1 − δ:  Φ S  ≤ 2 cid:98 RS G  + 3 cid:115  log 2  δ 2m  ,  which matches  3.4 .  The following result relates the empirical Rademacher complexities of a hypothesis set H and to the family of loss functions G associated to H in the case of binary loss  zero-one loss . Lemma 3.4 Let H be a family of functions taking values in {−1, +1} and let G be the family of loss functions associated to H for the zero-one loss: G = { x, y   cid:55 → X × {−1, +1}, let SX denote its projection over X: SX =  x1, . . . , xm . Then, the following relation holds between the empirical Rademacher complexities of G and H:  1h x  cid:54 =y : h ∈ H cid:9 . For any sample S =   x1, y1 , . . . ,  xm, ym   of elements in  Proof: For any sample S =   x1, y1 , . . . ,  xm, ym   of elements in X × {−1, +1}, by deﬁnition, the empirical Rademacher complexity of G can be written as:  33   3.15    cid:3    3.16    cid:98 RS G  = E  = E  1  1 m  h∈H   cid:98 RS G  = m cid:88 i=1 m cid:88 i=1 m cid:88 i=1 m cid:88 i=1  σ cid:104  sup σ cid:104  sup σ cid:104  sup σ cid:104  sup  2 cid:98 RSX  H . σi1h xi  cid:54 =yi cid:105   cid:105  −σiyih xi  cid:105  σih xi  cid:105  =  h∈H E  1−yih xi   h∈H  h∈H  1 m  1 m  1 m  1 2  1 2  σi  E  2  =  =  1  2 cid:98 RSX  H ,  where we used the fact that 1h xi  cid:54 =yi =  1− yih xi   2 and the fact that for a ﬁxed  cid:3  yi ∈ {−1, +1}, σi and −yiσi are distributed in the same way. Note that the lemma implies, by taking expectations, that for any m ≥ 1, Rm G  = 1 2 Rm H . These connections between the empirical and average Rademacher com- plexities can be used to derive generalization bounds for binary classiﬁcation in terms of the Rademacher complexity of the hypothesis set H.  Theorem 3.5  Rademacher complexity bounds – binary classiﬁcation   Let H be a family of functions taking values in {−1, +1} and let D be the distribution over the input space X. Then, for any δ > 0, with probability at least 1 − δ over a sample S of   34  Chapter 3 Rademacher Complexity and VC-Dimension  size m drawn according to D, each of the following holds for any h ∈ H:  R h  ≤  cid:98 RS h  + Rm H  + cid:115  log 1 and R h  ≤  cid:98 RS h  + cid:98 RS H  + 3 cid:115  log 2  δ 2m  δ 2m  .   3.17    3.18    cid:3   Proof: The result follows immediately by theorem 3.3 and lemma 3.4.  The theorem provides two generalization bounds for binary classiﬁcation based on the Rademacher complexity. Note that the second bound,  3.18 , is data-dependent:  S drawn. Thus, this bound could be particularly informative if we could compute  the empirical Rademacher complexity  cid:98 RS H  is a function of the speciﬁc sample  cid:98 RS H . But, how can we compute the empirical Rademacher complexity? Using  again the fact that σi and −σi are distributed in the same way, we can write   cid:98 RS H  = E  σ cid:104  sup  h∈H  1 m  m cid:88 i=1  −σih xi  cid:105  = − E σ cid:104  inf m cid:80 m  1  h∈H  1 m  m cid:88 i=1  σih xi  cid:105 .  Now, for a ﬁxed value of σ, computing inf h∈H i=1 σih xi  is equivalent to an empirical risk minimization problem, which is known to be computationally hard  for some hypothesis sets. Thus, in some cases, computing  cid:98 RS H  could be compu-  tationally hard. In the next sections, we will relate the Rademacher complexity to combinatorial measures that are easier to compute and also of independent interest for their usefulness in the analysis of learning in many contexts.  3.2 Growth function  Here we will show how the Rademacher complexity can be bounded in terms of the growth function.  Deﬁnition 3.6  Growth function  The growth function ΠH : N → N for a hypothesis  set H is deﬁned by:  ∀m ∈ N, ΠH m  =  max  {x1,...,xm}⊆X cid:12  cid:12  cid:12  cid:8  cid:0 h x1 , . . . , h xm  cid:1  : h ∈ H cid:9  cid:12  cid:12  cid:12 .  In other words, ΠH m  is the maximum number of distinct ways in which m points can be classiﬁed using hypotheses in H. Each one of these distinct classiﬁcations is called a dichotomy and, thus, the growth function counts the number of dichotomies that are realized by the hypothesis. This provides another measure of the richness of the hypothesis set H. However, unlike the Rademacher complexity, this measure does not depend on the distribution, it is purely combinatorial.   3.19    3.2 Growth function  sart’s lemma.  To relate the Rademacher complexity to the growth function, we will use Mas-  Theorem 3.7  Massart’s lemma  Let A ⊆ Rm be a ﬁnite set, with r = maxx∈A  cid:107 x cid:107 2,  then the following holds:  E  σ cid:20  1  m  sup x∈A  m cid:88 i=1  σixi cid:21  ≤  r cid:112 2 log A  m  ,  where σis are independent uniform random variables taking values in {−1, +1} and x1, . . . , xm are the components of vector x. Proof: The result follows immediately from the bound on the expectation of a maximum given by Corollary D.11 since the random variables σixi are independent  cid:3   and each σixi takes values in [−xi,xi] with cid:112  cid:80 m  Using this result, we can now bound the Rademacher complexity in terms of the growth function. Corollary 3.8 Let G be a family of functions taking values in {−1, +1}. Then the following holds:  i ≤ r2.  i=1 x2  Rm G  ≤ cid:114  2 log ΠG m   m  .  Proof: For a ﬁxed sample S =  x1, . . . , xm , we denote by GS the set of vectors of function values  g x1 , . . . , g xm   cid:62  where g is in G. Since g ∈ G takes values in {−1, +1}, the norm of these vectors is bounded by √m. We can then apply Massart’s lemma as follows:  By deﬁnition, GS is bounded by the growth function, thus,  Rm G  = E  1 m  u∈GS  m cid:88 i=1  σiui cid:35  cid:35  ≤ E S cid:34 E σ cid:34  sup S cid:34 √m cid:112 2 log ΠG m   S cid:34 √m cid:112 2 log GS  cid:35  = cid:114  2 log ΠG m   m  m  m  ,   cid:35  .  Rm G  ≤ E  which concludes the proof.  Combining the generalization bound  3.17  of theorem 3.5 with corollary 3.8 yields immediately the following generalization bound in terms of the growth function.  Corollary 3.9  Growth function generalization bound  Let H be a family of functions taking values in {−1, +1}. Then, for any δ > 0, with probability at least 1 − δ, for any h ∈ H,  R h  ≤  cid:98 RS h  + cid:114  2 log ΠH m   m  + cid:115  log 1  δ 2m  .  35   3.20    3.21    cid:3    3.22    36  Chapter 3 Rademacher Complexity and VC-Dimension   a    b   Figure 3.1 VC-dimension of intervals on the real line.  a  Any two points can be shattered.  b  No sample of three points can be shattered as the  +, −, +  labeling cannot be realized.  Growth function bounds can be also derived directly  without using Rademacher complexity bounds ﬁrst . The resulting bound is then the following:  P cid:104  cid:12  cid:12  cid:12 R h  −  cid:98 RS h  cid:12  cid:12  cid:12  >  cid:15  cid:105  ≤ 4ΠH 2m  exp cid:18 −  m cid:15 2  8  cid:19  ,  which only diﬀers from  3.22  by constants.   3.23   The computation of the growth function may not be always convenient since, by deﬁnition, it requires computing ΠH m  for all m ≥ 1. The next section introduces an alternative measure of the complexity of a hypothesis set H that is based instead on a single scalar, which will turn out to be in fact deeply related to the behavior of the growth function.  3.3 VC-dimension  Here, we introduce the notion of VC-dimension  Vapnik-Chervonenkis dimension . The VC-dimension is also a purely combinatorial notion but it is often easier to compute than the growth function  or the Rademacher Complexity . As we shall see, the VC-dimension is a key quantity in learning and is directly related to the growth function.  To deﬁne the VC-dimension of a hypothesis set H, we ﬁrst introduce the concept of shattering. Recall from the previous section, that given a hypothesis set H, a dichotomy of a set S is one of the possible ways of labeling the points of S using a hypothesis in H. A set S of m ≥ 1 points is said to be shattered by a hypothesis set H when H realizes all possible dichotomies of S, that is when ΠH m  = 2m. Deﬁnition 3.10  VC-dimension  The VC-dimension of a hypothesis set H is the size of the largest set that can be shattered by H:  VCdim H  = max{m : ΠH m  = 2m}.   3.24   Note that, by deﬁnition, if VCdim H  = d, there exists a set of size d that can be shattered. However, this does not imply that all sets of size d or less are shattered and, in fact, this is typically not the case.  - -  + -  - +  + +  + - +   3.3 VC-dimension  37   a    b   Figure 3.2 Unrealizable dichotomies for four points using hyperplanes in R2.  a  All four points lie on the convex hull.  b  Three points lie on the convex hull while the remaining point is interior.  To further illustrate this notion, we will examine a series of examples of hy- pothesis sets and will determine the VC-dimension in each case. To compute the VC-dimension we will typically show a lower bound for its value and then a match- ing upper bound. To give a lower bound d for VCdim H , it suﬃces to show that a set S of cardinality d can be shattered by H. To give an upper bound, we need to prove that no set S of cardinality d + 1 can be shattered by H, which is typically more diﬃcult. Example 3.11  Intervals on the real line  Our ﬁrst example involves the hypothesis class of intervals on the real line. It is clear that the VC-dimension is at least two, since all four dichotomies  +, + ,  −,− ,  +,− ,  −, +  can be realized, as illus- trated in ﬁgure 3.1 a . In contrast, by the deﬁnition of intervals, no set of three points can be shattered since the  +,−, +  labeling cannot be realized. Hence, VCdim intervals in R  = 2. Example 3.12  Hyperplanes  Consider the set of hyperplanes in R2. We ﬁrst observe that any three non-collinear points in R2 can be shattered. To obtain the ﬁrst three dichotomies, we choose a hyperplane that has two points on one side and the third point on the opposite side. To obtain the fourth dichotomy we have all three points on the same side of the hyperplane. The remaining four dichotomies are realized by simply switching signs. Next, we show that four points cannot be shattered by considering two cases:  i  the four points lie on the convex hull deﬁned by the four points, and  ii  three of the four points lie on the convex hull and the remaining point is internal. In the ﬁrst case, a positive labeling for one diagonal pair and a negative labeling for the other diagonal pair cannot be realized, as illustrated in ﬁgure 3.2 a . In the second case, a labeling which is positive for the points on the convex hull and negative for the interior point cannot be realized, as illustrated in ﬁgure 3.2 b . Hence, VCdim hyperplanes in R2  = 3. More generally in Rd, we derive a lower bound by starting with a set of d+1 points in Rd, setting x0 to be the origin and deﬁning xi, for i ∈ {1, . . . , d}, as the point whose ith coordinate is 1 and all others are 0. Let y0, y1, . . . , yd ∈ {−1, +1} be an  -  -  +  +  +  -  +  +   38  Chapter 3 Rademacher Complexity and VC-Dimension  arbitrary set of labels for x0, x1, . . . , xd. Let w be the vector whose ith coordinate is yi. Then the classiﬁer deﬁned by the hyperplane of equation w · x + y0 2 = 0 shatters x0, x1, . . . , xd since for any i ∈ {0, . . . , d},  sgn cid:16 w · xi +  y0  2 cid:17  = sgn cid:16 yi +  y0  2 cid:17  = yi.   3.25   To obtain an upper bound, it suﬃces to show that no set of d + 2 points can be shattered by halfspaces. To prove this, we will use the following general theorem. Theorem 3.13  Radon’s theorem  Any set X of d + 2 points in Rd can be partitioned into two subsets X1 and X2 such that the convex hulls of X1 and X2 intersect.  Proof: Let X = {x1, . . . , xd+2} ⊂ Rd. The following is a system of d + 1 linear  equations in α1, . . . , αd+2:  αixi = 0  and  αi = 0,   3.26   d+2 cid:88 i=1  d+2 cid:88 i=1  since the ﬁrst equality leads to d equations, one for each component. The number of unknowns, d + 2, is larger than the number of equations, d + 1, therefore the i=1 βi = 0, both I1 = {i ∈ [d + 2] : βi > 0} and I2 = {i ∈ [d + 2] : βi ≤ 0} are non-empty sets and X1 = {xi : i ∈ I1} and X2 = {xi : i ∈ I2} form a partition of X. By the last  system admits a non-zero solution β1, . . . , βd+2. Since  cid:80 d+2 equation of  3.26 , cid:80 i∈I1 βi = − cid:80 i∈I2 βi. Let β = cid:80 i∈I1 βi. Then, the ﬁrst part  of  3.26  implies  βi β   cid:88 i∈I1  xi = cid:88 i∈I2 with cid:80 i∈I1 β ≥ 0 for i ∈ I1 and −βi deﬁnition of the convex hulls  B.6 , this implies that  cid:80 i∈I1  β = cid:80 i∈I2  −βi β = 1, and βi  −βi β  xi,  βi  the convex hull of X1 and to that of X2. Now, let X be a set of d + 2 points. By Radon’s theorem, it can be partitioned into two sets X1 and X2 such that their convex hulls intersect. Observe that when two sets of points X1 and X2 are separated by a hyperplane, their convex hulls are also separated by that hyperplane. Thus, X1 and X2 cannot be separated by a hyperplane and X is not shattered. Combining our lower and upper bounds, we have proven that VCdim hyperplanes in Rd  = d + 1.  β ≥ 0 for i ∈ I2. By βi β xi belongs both to  cid:3   Example 3.14  Axis-aligned Rectangles  We ﬁrst show that the VC-dimension is at least four, by considering four points in a diamond pattern. Then, it is clear that all 16 dichotomies can be realized, some of which are illustrated in ﬁgure 3.3 a . In contrast, for any set of ﬁve distinct points, if we construct the minimal axis- aligned rectangle containing these points, one of the ﬁve points is in the interior of   3.3 VC-dimension  39   a    b   Figure 3.3 VC-dimension of axis-aligned rectangles.  a  Examples of realizable dichotomies for four points in a diamond pattern.  b  No sample of ﬁve points can be realized if the interior point and the remaining points have opposite labels.  this rectangle. Imagine that we assign a negative label to this interior point and a positive label to each of the remaining four points, as illustrated in ﬁgure 3.3 b . There is no axis-aligned rectangle that can realize this labeling. Hence, no set of ﬁve distinct points can be shattered and VCdim axis-aligned rectangles  = 4.  Example 3.15  Convex Polygons  We focus on the class of convex d-gons in the plane. To get a lower bound, we show that any set of 2d+1 points can be shattered. To do this, we select 2d+1 points that lie on a circle, and for a particular labeling, if there are more negative than positive labels, then the points with the positive labels are used as the polygon’s vertices, as in ﬁgure 3.4 a . Otherwise, the tangents of the negative points serve as the edges of the polygon, as shown in  3.4  b . To derive an upper bound, it can be shown that choosing points on the circle maximizes the number of possible dichotomies, and thus VCdim convex d-gons  = 2d + 1. Note also that VCdim convex polygons  = +∞. Example 3.16  Sine Functions  The previous examples could suggest that the VC- dimension of H coincides with the number of free parameters deﬁning H. For ex- ample, the number of parameters deﬁning hyperplanes matches their VC-dimension. However, this does not hold in general. Several of the exercises in this chapter il- lustrate this fact. The following provides a striking example from this point of  view. Consider the following family of sine functions: {t  cid:55 → sin ωt  : ω ∈ R}. One  instance of this function class is shown in ﬁgure 3.5. These sine functions can be  -  -  +  -  -  -  -  +  +  +  +  -  +  -  +  +  +  +  +  -  +   40  Chapter 3 Rademacher Complexity and VC-Dimension   a    b   Figure 3.4 Convex d-gons in the plane can shatter 2d + 1 points.  a  d-gon construction when there are more negative labels.  b  d-gon construction when there are more positive labels.  Figure 3.5 An example of a sine function  with ω = 50  used for classiﬁcation.  used to classify the points on the real line: a point is labeled positively if it is above the curve, negatively otherwise. Although this family of sine functions is de- ﬁned via a single parameter, ω, it can be shown that VCdim sine functions  = +∞  exercise 3.20 .  The VC-dimension of many other hypothesis sets can be determined or upper- bounded in a similar way  see this chapter’s exercises . In particular, the VC- dimension of any vector space of dimension r < ∞ can be shown to be at most r  exercise 3.19 . The next result, known as Sauer’s lemma, clariﬁes the connection between the notions of growth function and VC-dimension.  -  +  - -  -  +  + - -  positive points < negative points  +  -  + +  -  +  -  -  +  positive points > negative points   3.3 VC-dimension  41  Figure 3.6 Illustration of how G1 and G2 are constructed in the proof of Sauer’s lemma.  Theorem 3.17  Sauer’s lemma  Let H be a hypothesis set with VCdim H  = d. Then,  for all m ∈ N, the following inequality holds: d cid:88 i=0 cid:18 m i cid:19 .  ΠH m  ≤   3.27   Proof: The proof is by induction on m + d. The statement clearly holds for m = 1 and d = 0 or d = 1. Now, assume that it holds for  m − 1, d − 1  and  m − 1, d . Fix a set S = {x1, . . . , xm} with ΠH m  dichotomies and let G = HS be the set of concepts H induced by restriction to S. Now consider the following families over S cid:48  = {x1, . . . , xm−1}. We deﬁne G1 = GS cid:48  as the set of concepts H induced by restriction to S cid:48 . Next, by identifying each concept as the set of points  in S cid:48  or S  for which it is non-zero, we can deﬁne 2 as  G2 = {g cid:48  ⊆ S cid:48  :  g cid:48  ∈ G  ∧  g cid:48  ∪ {xm} ∈ G }.  Since g cid:48  ⊆ S cid:48 , g cid:48  ∈ G means that without adding xm it is a concept of G. Further, the constraint g cid:48  ∪ {xm} ∈ G means that adding xm to g cid:48  also makes it a concept of G. The construction of G1 and G2 is illustrated pictorially in ﬁgure 3.6. Given our deﬁnitions of G1 and G2, observe that G1 + G2 = G. Since VCdim G1  ≤ VCdim G  ≤ d, then by deﬁnition of the growth function and using the induction hypothesis,  Further, by deﬁnition of G2, if a set Z ⊆ S cid:48  is shattered by G2, then the set Z∪{xm} is shattered by G. Hence,  G1 ≤ ΠG1  m − 1  ≤  d cid:88 i=0 cid:18 m − 1 i  cid:19 .  VCdim G2  ≤ VCdim G  − 1 = d − 1,  G1 = GS0 G2 = g0 ⊆ S0 :  g0 ∈ G  ∧  g0 ∪ {xm} ∈ G    x1 1  1  0  1  1  x2 1  1  1  0  0  ··· xm−1 xm  0  0  1  0  0  1  1  1  1  0  0  1  1  0  1  ···  ···  ···  ···  ···   42  Chapter 3 Rademacher Complexity and VC-Dimension  and by deﬁnition of the growth function and using the induction hypothesis,  G2 ≤ ΠG2  m − 1  ≤  d−1 cid:88 i=0 cid:18 m − 1 i  cid:19 . d−1 cid:88 i=0 cid:0 m−1 d cid:88 i=0 cid:0 m−1 i  cid:1  =  Thus,  G = G1 + G2 ≤  d cid:88 i=0 cid:0 m−1 i  cid:1  +  which completes the inductive proof.  i  cid:1  + cid:0 m−1 i−1 cid:1  =  d cid:88 i=0 cid:0 m i cid:1 ,   cid:3   The signiﬁcance of Sauer’s lemma can be seen by corollary 3.18, which remarkably shows that growth function only exhibits two types of behavior: either VCdim H  = d < +∞, in which case ΠH m  = O md , or VCdim H  = +∞, in which case ΠH m  = 2m. Corollary 3.18 Let H be a hypothesis set with VCdim H  = d. Then for all m ≥ d,  3.28   = O md .  ΠH m  ≤ cid:16  em d  cid:17 d  Proof: The proof begins by using Sauer’s lemma. The ﬁrst inequality multiplies each summand by a factor that is greater than or equal to one since m ≥ d, while the second inequality adds non-negative summands to the summation.  ≤  ≤  ΠH m  ≤  d cid:88 i=0 cid:18 m i cid:19  i cid:19  cid:16  m d cid:88 i=0 cid:18 m d cid:17 d−i m cid:88 i=0 cid:18 m i cid:19  cid:16  m d cid:17 d−i i cid:19  cid:18  d d cid:17 d m cid:88 i=0 cid:18 m m cid:19 i = cid:16  m m cid:19 m d cid:17 d cid:18 1 + ≤ cid:16  m = cid:16  m d cid:17 d After simplifying the expression using the binomial theorem, the ﬁnal inequality  cid:3  follows using the general inequality  1 − x  ≤ e−x. The explicit relationship just formulated between VC-dimension and the growth function combined with corollary 3.9 leads immediately to the following generaliza- tion bounds based on the VC-dimension.  ed.  d  Corollary 3.19  VC-dimension generalization bounds  Let H be a family of functions taking values in {−1, +1} with VC-dimension d. Then, for any δ > 0, with proba-   3.4 Lower bounds  bility at least 1 − δ, the following holds for all h ∈ H:  Thus, the form of this generalization bound is  d  m  + cid:115  log 1 R h  ≤  cid:98 RS h  + cid:114  2d log em R h  ≤  cid:98 RS h  + O cid:32  cid:115  log m d   m d   cid:33  ,  δ 2m  .  43   3.29    3.30   which emphasizes the importance of the ratio m d for generalization. The theorem provides another instance of Occam’s razor principle where simplicity is measured in terms of smaller VC-dimension.  VC-dimension bounds can be derived directly without using an intermediate Rademacher complexity bound, as for  3.23 : combining Sauer’s lemma with  3.23  leads to the following high-probability bound  R h  ≤  cid:98 RS h  + cid:115  8d log 2em  d + 8 log 4 m  δ  ,  which has the general form of  3.30 . The log factor plays only a minor role in these bounds. A ﬁner analysis can be used in fact to eliminate that factor.  3.4 Lower bounds  In the previous section, we presented several upper bounds on the generalization error. In contrast, this section provides lower bounds on the generalization error of any learning algorithm in terms of the VC-dimension of the hypothesis set used.  These lower bounds are shown by ﬁnding for any algorithm a ‘bad’ distribution. Since the learning algorithm is arbitrary, it will be diﬃcult to specify that particular distribution. Instead, it suﬃces to prove its existence non-constructively. At a high level, the proof technique used to achieve this is the probabilistic method of Paul Erd¨os. In the context of the following proofs, ﬁrst a lower bound is given on the expected error over the parameters deﬁning the distributions. From that, the lower bound is shown to hold for at least one set of parameters, that is one distribution.  Theorem 3.20  Lower bound, realizable case  Let H be a hypothesis set with VC- dimension d > 1. Then, for any m ≥ 1 and any learning algorithm A, there exist a distribution D over X and a target function f ∈ H such that  P  S∼Dm cid:20 RD hS, f   >  d − 1  32m cid:21  ≥ 1 100.   3.31   Proof: Let X = {x0, x1, . . . , xd−1} ⊆ X be a set that is shattered by H. For any  cid:15  > 0, we choose D such that its support is reduced to X and so that one point  x0    44  Chapter 3 Rademacher Complexity and VC-Dimension  has very high probability  1− 8 cid:15  , with the rest of the probability mass distributed uniformly among the other points:  P D  [x0] = 1 − 8 cid:15  and ∀i ∈ [d − 1], P  D  [xi] =  8 cid:15  d − 1  .   3.32   With this deﬁnition, most samples would contain x0 and, since X is shattered, A can essentially do no better than tossing a coin when determining the label of a point xi not falling in the training set. We assume without loss of generality that A makes no error on x0. For a sample S, we let S denote the set of its elements falling in {x1, . . . , xd−1}, and let S be the set of samples S of size m such that S ≤  d − 1  2. Now, ﬁx a sample S ∈ S, and consider the uniform distribution U over all labelings f : X → {0, 1}, which are all in H since the set is shattered. Then, the following lower bound holds:  E f∼U  [RD hS, f  ] = cid:88 f  cid:88 x∈X ≥ cid:88 f  cid:88 x cid:54 ∈S = cid:88 x cid:54 ∈S cid:16  cid:88 f 2 cid:88 x cid:54 ∈S P[x] ≥  =  1  1hS  x  cid:54 =f  x   1hS  x  cid:54 =f  x   P[x] P[f ]  P[x] P[f ]  1hS  x  cid:54 =f  x   P[f ] cid:17  P[x]  1 2  d − 1  2  8 cid:15  d − 1  = 2 cid:15 .   3.33   muted, thus,  The ﬁrst lower bound holds because we remove non-negative terms from the sum- mation when we only consider x  cid:54 ∈ S instead of all x in X. After rearranging terms, the subsequent equality holds since we are taking an expectation over f ∈ H with uniform weight on each f and H shatters X. The ﬁnal lower bound holds due to the deﬁnitions of D and S, the latter which implies that X − S ≥  d − 1  2. Since  3.33  holds for all S ∈ S, it also holds in expectation over all S ∈ S:  ES∈S cid:2  Ef∼U[RD hS, f  ] cid:3  ≥ 2 cid:15 . By Fubini’s theorem, the expectations can be per- This implies that ES∈S[RD hS, f0 ] ≥ 2 cid:15  for at least one labeling f0 ∈ H. Decom- posing this expectation into two parts and using RD hS, f0  ≤ PD[X − {x0}], we [RD hS, f0 ] =  cid:88 S :RD hS ,f0 ≥ cid:15  RD hS, f0  P[RD hS, f0 ] E S∈S  [RD hS, f  ] cid:105  ≥ 2 cid:15 .  f∼U cid:104  E  obtain:   3.34   S∈S  E  RD hS, f0  P[RD hS, f0 ] +  cid:88 S :RD hS ,f0 < cid:15  [RD hS, f0  ≥  cid:15 ] +  cid:15  P [X − {x0}] P S∈S S∈S [RD hS, f0  ≥  cid:15 ] +  cid:15  cid:0 1 − P  ≤ P ≤ 8 cid:15  P S∈S  S∈S  D  [RD hS, f0  <  cid:15 ]  [RD hS, f0  ≥  cid:15 ] cid:1 .   3.4 Lower bounds  45   3.35   Collecting terms in PS∈S[RD hS, f0  ≥  cid:15 ] yields  P S∈S  [RD hS, f0  ≥  cid:15 ] ≥   2 cid:15  −  cid:15   =  1 7 cid:15   1 7  .  Thus, the probability over all samples S  not necessarily in S  can be lower bounded as  P [RD hS, f0  ≥  cid:15 ] ≥ P S∈S S   3.36  This leads us to ﬁnd a lower bound for P[S]. By the multiplicative Chernoﬀ bound  Theorem D.4 , for any γ > 0, the probability that more than  d − 1  2 points are drawn in a sample of size m veriﬁes:  [RD hS, f0  ≥  cid:15 ] P[S] ≥  P[S].  1 7  1 − P[S] = P[Sm ≥ 8 cid:15 m 1 + γ ] ≤ e−8 cid:15 m γ2  3 .   3.37   Therefore, for  cid:15  =  d − 1   32m  and γ = 1,  P[Sm ≥ d−1  2 ] ≤ e− d−1  12 ≤ e−1 12 ≤ 1 − 7δ,   3.38   cid:3  for δ ≤ .01. Thus P[S] ≥ 7δ and PS[RD hS, f0  ≥  cid:15 ] ≥ δ. The theorem shows that for any algorithm A, there exists a ‘bad’ distribution over X and a target function f for which the error of the hypothesis returned by A is a constant times d m with some constant probability. This further demonstrates the key role played by the VC-dimension in learning. The result implies in particular that PAC-learning in the realizable case is not possible when the VC-dimension is inﬁnite.  Note that the proof shows a stronger result than the statement of the theorem: the distribution D is selected independently of the algorithm A. We now present a theorem giving a lower bound in the non-realizable case. The following two lemmas will be needed for the proof.  Lemma 3.21 Let α be a uniformly distributed random variable taking values in {α−, α+}, where α− = 1 2 , and let S be a sample of m ≥ 1 random variables X1, . . . , Xm taking values in {0, 1} and drawn i.i.d. according to the distribution Dα deﬁned by PDα[X = 1] = α. Let h be a function from Xm to {α−, α+}, then the following holds:  2 and α+ = 1  2 −  cid:15   2 +  cid:15   where Φ m,  cid:15   = 1  E  α cid:104  P  S∼Dm  α  [h S   cid:54 = α] cid:105  ≥ Φ 2 cid:100 m 2 cid:101 ,  cid:15  , 1− cid:15 2 cid:1  cid:17  for all m and  cid:15 .  4 cid:16 1 − cid:113 1 − exp cid:0  − m cid:15 2  Proof: The lemma can be interpreted in terms of an experiment with two coins with biases α− and α+. It implies that for a discriminant rule h S  based on a sample S drawn from Dα− or Dα+, to determine which coin was tossed, the sample size m must be at least Ω 1  cid:15 2 . The proof is left as an exercise  exercise D.3 .  cid:3    3.39    46  Chapter 3 Rademacher Complexity and VC-Dimension  We will make use of the fact that for any ﬁxed  cid:15  the function m  cid:55 → Φ m, x  is convex, which is not hard to establish.  Lemma 3.22 Let Z be a random variable taking values in [0, 1]. Then, for any γ ∈ [0, 1 ,   3.40   Proof: Since the values taken by Z are in [0, 1],  P[Z = z]z  E[Z] − γ 1 − γ  > E[Z] − γ.  P[z > γ] ≥ E[Z] = cid:88 z≤γ P[Z = z]z + cid:88 z>γ ≤ cid:88 z≤γ P[Z = z]γ + cid:88 z>γ = γ P[Z ≤ γ] + P[Z > γ] = γ 1 − P[Z > γ]  + P[Z > γ] =  1 − γ  P[Z > γ] + γ,  P[Z = z]  which concludes the proof.   cid:3    3.41    3.42   Theorem 3.23  Lower bound, non-realizable case  Let H be a hypothesis set with VC- dimension d > 1. Then, for any m ≥ 1 and any learning algorithm A, there exists a distribution D over X × {0, 1} such that:  P  S∼Dm cid:20 RD hS  − inf  h∈H  RD h  > cid:114  d  320m cid:21  ≥ 1 64.  Equivalently, for any learning algorithm, the sample complexity veriﬁes  d  m ≥  320 cid:15 2 .  Proof: Let X = {x1, . . . , xd} ⊆ X be a set shattered by H. For any α ∈ [0, 1] and any vector σ =  σ1, . . . , σd  cid:62  ∈ {−1, +1}d, we deﬁne a distribution Dσ with support X × {0, 1} as follows: ∀i ∈ [d],  [ xi, 1 ] =   3.43   P Dσ  σiα  +  Thus, the label of each point xi, i ∈ [d], follows the distribution PDσ [·xi], that of  a biased coin where the bias is determined by the sign of σi and the magnitude of α. To determine the most likely label of each point xi, the learning algorithm will  therefore need to estimate PDσ [1xi] with an accuracy better than α. To make this further diﬃcult, α and σ will be selected based on the algorithm, requiring, as in lemma 3.21, Ω 1 α2  instances of each point xi in the training sample.  2  cid:17 .  1  d cid:16  1  2   3.4 Lower bounds  47  P[yxi] =   x .  3.44   Clearly, the Bayes classiﬁer h∗Dσ is deﬁned by h∗Dσ  xi  = argmaxy∈{0,1} 1σi>0 for all i ∈ [d]. h∗Dσ is in H since X is shattered. For all h ∈ H, RDσ  h − RDσ  h∗Dσ   =  1h x  cid:54 =h∗Dσ   x  =  +  α  α  2  1  d cid:88 x∈X cid:16  α  2 cid:17 1h x  cid:54 =h∗Dσ  d  cid:88 x∈X  Let hS denote the hypothesis returned by the learning algorithm A after receiving a labeled sample S drawn according to Dσ. We will denote by Sx the number of occurrences of a point x in S. Let U denote the uniform distribution over {−1, +1}d. Then, in view of  3.44 , the following holds:  E σ∼U S∼Dm 1  =  1  S∼Dm  E σ∼U S∼Dm E  α cid:2 RDσ  hS  − RDσ  h∗Dσ   cid:3  cid:21  σ  cid:20  1  x  cid:105  d cid:88 x∈X d cid:88 x∈X d cid:88 x∈X d cid:88 x∈X d cid:88 x∈X  σ  cid:104 1hS  x  cid:54 =h∗Dσ σ∼U cid:104  P σ∼U cid:104  P m cid:88 n=0 m cid:88 n=0 Φ n + 1, α  P[Sx = n]  Φ m d + 1, α   S∼Dm  E  1  1  1  =  =  ≥  ≥  = Φ m d + 1, α .  σ  cid:2 hS x   cid:54 = h∗Dσ  x  cid:3  cid:105  σ  cid:2 hS x   cid:54 = h∗Dσ  x  cid:12  cid:12 Sx = n cid:3  P[Sx = n] cid:105    lemma 3.21    convexity of Φ ·, α  and Jensen’s ineq.   Since the expectation over σ is lower-bounded by Φ m d + 1, α , there must exist some σ ∈ {−1, +1}d for which  Then, by lemma 3.22, for that σ, for any γ ∈ [0, 1],  S∼Dm  E  P  S∼Dm  σ  cid:20  1 α cid:2 RDσ  hS  − RDσ  h∗Dσ   cid:3  cid:21  > Φ m d + 1, α . σ  cid:20  1 α cid:2 RDσ  hS  − RDσ  h∗Dσ   cid:3  > γu cid:21  >  1 − γ u, σ  cid:2 RDσ  hS  − RDσ  h∗Dσ   >  cid:15  cid:3  > δ.  S∼Dm  P  where u = Φ m d + 1, α . Selecting δ and  cid:15  such that δ ≤  1 − γ u and  cid:15  ≤ γαu gives   3.45    3.46    3.47    48  Chapter 3 Rademacher Complexity and VC-Dimension  To satisfy the inequalities deﬁning  cid:15  and δ, let γ = 1 − 8δ. Then,  δ ≤  1 − γ u ⇐⇒ u ≥  1 8   3.48    3.49    3.50    3.51    3.52    m d + 1 α2  1 − α2   cid:19  cid:33  ≥  1 8  ⇐⇒  ⇐⇒ ⇐⇒  1  ≤ log  1 − α2   m d + 1 α2  4 cid:32 1 − cid:115 1 − exp cid:18 − α2 − 1 cid:17  log d ≤ cid:16  1 64 cid:15 2 − 1 cid:19  log d ≤ cid:18   1 − 8δ 2  4 3 4 3 − 1.  m  Selecting α = 8 cid:15   1 − 8δ  gives  cid:15  = γα 8 and the condition 4 3 − 1.  m  Let f  1  cid:15 2  denote the right-hand side. We are seeking a suﬃcient condition of the form m d ≤ ω  cid:15 2. Since  cid:15  ≤ 1 64, to ensure that ω  cid:15 2 ≤ f  1  cid:15 2 , it suﬃces to impose  ω   1 64 2 = f cid:0   1   1 64 2 cid:1 . This condition gives  1  ω =  7 64 2 log 4 3  −  1 64 2 log 4 3  + 1  ≈ .003127 ≥ 1 320 = .003125.  320 m d  is suﬃcient to ensure the inequalities.   cid:3  Thus,  cid:15 2 ≤ The theorem shows that for any algorithm A, in the non-realizable case, there exists a ‘bad’ distribution over X × {0, 1} such that the error of the hypothesis m with some constant probability. The VC- dimension appears as a critical quantity in learning in this general setting as well. In particular, with an inﬁnite VC-dimension, agnostic PAC-learning is not possible.  returned by A is a constant times cid:113  d  3.5 Chapter notes  The use of Rademacher complexity for deriving generalization bounds in learn- ing was ﬁrst advocated by Koltchinskii [2001], Koltchinskii and Panchenko [2000], and Bartlett, Boucheron, and Lugosi [2002a], see also [Koltchinskii and Panchenko, 2002, Bartlett and Mendelson, 2002]. Bartlett, Bousquet, and Mendelson [2002b] introduced the notion of local Rademacher complexity, that is the Rademacher complexity restricted to a subset of the hypothesis set limited by a bound on the variance. This can be used to derive better guarantees under some regularity as- sumptions about the noise.  Theorem 3.7 is due to Massart [2000]. The notion of VC-dimension was introduced by Vapnik and Chervonenkis [1971] and has been since extensively studied [Vapnik, 2006, Vapnik and Chervonenkis, 1974, Blumer et al., 1989, Assouad, 1983, Dudley,   3.5 Chapter notes  49  1999]. In addition to the key role it plays in machine learning, the VC-dimension is also widely used in a variety of other areas of computer science and mathematics  e.g., see Shelah [1972], Chazelle [2000] . Theorem 3.17 is known as Sauer’s lemma in the learning community, however the result was ﬁrst given by Vapnik and Cher- vonenkis [1971]  in a somewhat diﬀerent version  and later independently by Sauer [1972] and Shelah [1972].  In the realizable case, lower bounds for the expected error in terms of the VC- dimension were given by Vapnik and Chervonenkis [1974] and Haussler et al. [1988]. Later, a lower bound for the probability of error such as that of theorem 3.20 was given by Blumer et al. [1989]. Theorem 3.20 and its proof, which improves upon this previous result, are due to Ehrenfeucht, Haussler, Kearns, and Valiant [1988]. Devroye and Lugosi [1995] gave slightly tighter bounds for the same problem with a more complex expression. Theorem 3.23 giving a lower bound in the non-realizable case and the proof presented are due to Anthony and Bartlett [1999]. For other examples of application of the probabilistic method demonstrating its full power, consult the reference book of Alon and Spencer [1992].  There are several other measures of the complexity of a family of functions used in machine learning, including covering numbers, packing numbers, and some other complexity measures discussed in chapter 11. A covering number Np G,  cid:15   is the minimal number of Lp balls of radius  cid:15  > 0 needed to cover a family of loss func- tions G. A packing number Mp G,  cid:15   is the maximum number of non-overlapping Lp balls of radius  cid:15  centered in G. The two notions are closely related, in partic- ular it can be shown straightforwardly that Mp G, 2 cid:15   ≤ Np G,  cid:15   ≤ Mp G,  cid:15   for G and  cid:15  > 0. Each complexity measure naturally induces a diﬀerent reduction of inﬁnite hypothesis sets to ﬁnite ones, thereby resulting in generalization bounds for inﬁnite hypothesis sets. Exercise 3.31 illustrates the use of covering numbers for deriving generalization bounds using a very simple proof. There are also close re- lationships between these complexity measures: for example, by Dudley’s theorem, the empirical Rademacher complexity can be bounded in terms of N2 G,  cid:15   [Dudley, 1967, 1987] and the covering and packing numbers can be bounded in terms of the VC-dimension [Haussler, 1995]. See also [Ledoux and Talagrand, 1991, Alon et al., 1997, Anthony and Bartlett, 1999, Cucker and Smale, 2001, Vidyasagar, 1997] for a number of upper bounds on the covering number in terms of other complexity measures.   50  Chapter 3 Rademacher Complexity and VC-Dimension  3.6 Exercises  3.1 Growth function of intervals in R. Let H be the set of intervals in R. The VC-dimension of H is 2. Compute its shattering coeﬃcient ΠH m , m ≥ 0. Compare your result with the general bound for growth functions.  3.2 Growth function and Rademacher complexity of thresholds in R. Let H be the family of threshold functions over the real line: H = {x  cid:55 → 1x≤θ : θ ∈ R}∪{x  cid:55 → 1x≥θ : θ ∈ R}. Give an upper bound on the growth function Πm H . Use that  to derive an upper bound on Rm H .  3.3 Growth function of linear combinations. A linearly separable labeling of a set X of vectors in Rd is a classiﬁcation of X into two sets X+ and X− with X+ = {x ∈ X : w · x > 0} and X− = {x ∈ X : w · x < 0} for some w ∈ Rd. Let X = {x1, . . . , xm} be a subset of Rd.  a  Let {X+, X−} be a dichotomy of X and let xm+1 ∈ Rd. Show that {X+ ∪ {xm+1}, X−} and {X+, X−∪{xm+1}} are linearly separable by a hyperplane going through the origin if and only if {X+, X−} is linearly separable by a hyperplane going through the origin and xm+1.  b  Let X = {x1, . . . , xm} be a subset of Rd such that any k-element subset of X with k ≤ d is linearly independent. Then, show that the number of k  cid:1 .  Hint: prove linearly separable labelings of X is C m, d  = 2 cid:80 d−1 by induction that C m + 1, d  = C m, d  + C m, d − 1 .   c  Let f1, . . . , fp be p functions mapping Rd to R. Deﬁne F as the family of  k=0 cid:0 m−1  classiﬁers based on linear combinations of these functions:  F = cid:26 x  cid:55 → sgn cid:18  p cid:88 k=1  akfk x  cid:19  : a1, . . . , ap ∈ R cid:27 .  Deﬁne Ψ by Ψ x  =  f1 x , . . . , fp x  . Assume that there exists x1, . . . , xm ∈ Rd such that every p-subset of {Ψ x1 , . . . , Ψ xm } is linearly independent.  Then, show that  ΠF m  = 2  p−1 cid:88 i=0 cid:18 m − 1 i  cid:19 .  3.4 Lower bound on growth function. Prove that Sauer’s lemma  theorem 3.17  is tight, i.e., for any set X of m > d elements, show that there exists a hypothesis  class H of VC-dimension d such that ΠH m  = cid:80 d  i=0 cid:0 m i cid:1 .   3.6 Exercises  51  3.5 Finer Rademacher upper bound.  Show that a ﬁner upper bound on the Rademacher complexity of the family G can be given in terms of ES[Π G, S ], where Π G, S  is the number of ways to label the points in sample S.  3.6 Singleton hypothesis class. Consider the trivial hypothesis set H = {h0}.   a  Show that Rm H  = 0 for any m > 0.   b  Use a similar construction to show that Massart’s lemma  theorem 3.7  is  tight.  3.7 Two function hypothesis class. Let H be a hypothesis set reduced to two func-  tions: H = {h−1, h+1} and let S =  x1, . . . , xm  ⊆ X be a sample of size m.   a  Assume that h−1 is the constant function taking value −1 and h+1 the constant function taking the value +1. What is the VC-dimension d of H?  Upper bound the empirical Rademacher complexity  cid:98 RS H   Hint: express  cid:98 RS H  in terms of the absolute value of a sum of Rademacher variables and apply Jensen’s inequality  and compare your bound with cid:112 d m.   b  Assume that h−1 is the constant function taking value −1 and h+1 the function taking value −1 everywhere except at x1 where it takes the value +1. What is the VC-dimension d of H? Compute the empirical Rademacher  complexity  cid:98 RS H .  3.8 Rademacher identities. Fix m ≥ 1. Prove the following identities for any α ∈ R  and any two hypothesis sets H and H cid:48  of functions mapping from X to R:   a  Rm αH  = αRm H .  b  Rm H + H cid:48   = Rm H  + Rm H cid:48  .  c  Rm {max h, h cid:48   : h ∈ H, h cid:48  ∈ H cid:48 }  ≤ Rm H  + Rm H cid:48  ,  where max h, h cid:48   denotes the function x  cid:55 → maxx∈X h x , h cid:48  x    Hint: you 2 [a + b + a − b] valid for all a, b ∈ R and could use the identity max a, b  = 1 Talagrand’s contraction lemma  see lemma 5.7  .  3.9 Rademacher complexity of intersection of concepts. Let H1 and H2 be two families of functions mapping X to {0, 1} and let H = {h1h2 : h1 ∈ H1, h2 ∈ H2}. Show that the empirical Rademacher complexity of H for any sample S of size m can be bounded as follows:   cid:98 RS H  ≤  cid:98 RS H1  + cid:98 RS H2 .   52  Chapter 3 Rademacher Complexity and VC-Dimension  Hint: use the Lipschitz function x  cid:55 → max 0, x− 1  and Talagrand’s contraction lemma.  Use that to bound the Rademacher complexity Rm U  of the family U of in- tersections of two concepts c1 and c2 with c1 ∈ C1 and c2 ∈ C2 in terms of the Rademacher complexities of C1 and C2.  3.10 Rademacher complexity of prediction vector. Let S =  x1, . . . , xm  be a sample  of size m and ﬁx h : X → R.  a  Denote by u the vector of predictions of h for S: u =  cid:20  h x1 ... h xm  cid:21 . Give an upper bound on the empirical Rademacher complexity  cid:98 RS H  of H = {h,−h} in terms of  cid:107 u cid:107 2  Hint: express  cid:98 RS H  in terms of the expectation  of an absolute value and apply Jensen’s inequality . Suppose that h xi  ∈ {0,−1, +1} for all i ∈ [m]. Express the bound on the Rademacher complexity in terms of the sparsity measure n = {i  h xi   cid:54 = 0}. What is that upper bound for the extreme values of the sparsity measure?   b  Let F be a family of functions mapping X to R. Give an upper bound on the empirical Rademacher complexity of F + h = {f + h : f ∈ F} and that  of F ± h =  F + h  ∪  F − h  in terms of  cid:98 RS F  and  cid:107 u cid:107 2.  3.11 Rademacher complexity of regularized neural networks. Let the input space be X = Rn1. In this problem, we consider the family of regularized neural networks deﬁned by the following set of functions mapping X to R:  where σ is an L-Lipschitz function. As an example, σ could be the sigmoid function which is 1-Lipschitz.  H =  x  cid:55 →  n2 cid:88 j=1  ,  wjσ uj · x  :  cid:107 w cid:107 1 ≤ Λ cid:48 , cid:107 uj cid:107 2 ≤ Λ,∀j ∈ [n2] Eσ cid:104 sup cid:107 u cid:107 2≤Λ  cid:80 m σi Φ ◦ h  xi  cid:12  cid:12  cid:12  cid:12  cid:12   cid:35  ≤  i=1 σiσ u · xi  cid:105 . σih xi  cid:12  cid:12  cid:12  cid:12  cid:12  h∈H cid:12  cid:12  cid:12  cid:12  cid:12  σ cid:34  sup  m cid:88 i=1   cid:35  ,  L m  E  and L-Lipschitz function Φ:  m   a  Show that  cid:98 RS H  = Λ cid:48  h∈H cid:12  cid:12  cid:12  cid:12  cid:12  σ cid:34  sup m cid:88 i=1  1 m  E   b  Use the following form of Talagrand’s lemma valid for all hypothesis sets H   3.6 Exercises  53  to upper bound  cid:98 RS H  in terms of the empirical Rademacher complexity of  H cid:48 , where H cid:48  is deﬁned by  H cid:48  = {x  cid:55 → s u · x  :  cid:107 u cid:107 2 ≤ Λ, s ∈ {−1, +1}} .   c  Use the Cauchy-Schwarz inequality to show that  E  Λ m  σ cid:34  cid:13  cid:13  cid:13  cid:13  cid:13  m cid:88 i=1  d  Use the inequality Ev[ cid:107 v cid:107 2] ≤ cid:112 Ev[ cid:107 v cid:107 2 ity to upper bound  cid:98 RS H cid:48  .   cid:98 RS H cid:48   =  σixi cid:13  cid:13  cid:13  cid:13  cid:13 2 cid:35  .   e  Assume that for all x ∈ S,  cid:107 x cid:107 2 ≤ r for some r > 0. Use the previous questions to derive an upper bound on the Rademacher complexity of H in terms of r.  2], which holds by Jensen’s inequal-  3.12 Rademacher complexity. Professor Jesetoo claims to have found a better bound on the Rademacher complexity of any hypothesis set H of functions taking values in {−1, +1}, in terms of its VC-dimension VCdim H . His bound is of cannot be correct?  Hint: consider a hypothesis set H reduced to just two simple functions.    cid:1 . Can you show that Professor Jesetoo’s claim  the form Rm H  ≤ O cid:0  VCdim H   m  3.13 VC-dimension of union of k intervals. What is the VC-dimension of subsets of  the real line formed by the union of k intervals?  3.14 VC-dimension of ﬁnite hypothesis sets. Show that the VC-dimension of a ﬁnite  hypothesis set H is at most log2 H.  3.15 VC-dimension of subsets. What is the VC-dimension of the set of subsets Iα of the real line parameterized by a single parameter α: Iα = [α, α+1]∪[α+2, +∞ ?  3.16 VC-dimension of axis-aligned squares and triangles.   a  What is the VC-dimension of axis-aligned squares in the plane?   b  Consider right triangles in the plane with the sides adjacent to the right angle both parallel to the axes and with the right angle in the lower left corner. What is the VC-dimension of this family?   54  Chapter 3 Rademacher Complexity and VC-Dimension  3.17 VC-dimension of closed balls in Rn. Show that the VC-dimension of the set of all closed balls in Rn, i.e., sets of the form {x ∈ Rn :  cid:107 x − x0 cid:107 2 ≤ r} for some x0 ∈ Rn and r ≥ 0, is less than or equal to n + 2.  3.18 VC-dimension of ellipsoids. What is the VC-dimension of the set of all ellipsoids  in Rn?  3.19 VC-dimension of a vector space of real functions. Let F be a ﬁnite-dimensional  vector space of real functions on Rn, dim F   = r < ∞. Let H be the set of  hypotheses:  H = {{x : f  x  ≥ 0} : f ∈ F}.  Show that d, the VC-dimension of H, is ﬁnite and that d ≤ r.  Hint: select an arbitrary set of m = r + 1 points and consider linear mapping u : F → Rm  deﬁned by: u f   =  f  x1 , . . . , f  xm  .   3.20 VC-dimension of sine functions. Consider the hypothesis family of sine functions   Example 3.16 : {x → sin ωx  : ω ∈ R} .  a  Show that for any x ∈ R the points x, 2x, 3x and 4x cannot be shattered by  this family of sine functions.   b  Show that the VC-dimension of the family of sine functions is inﬁnite.  Hint:  show that {2−i : i ≤ m} can be shattered for any m > 0.   3.21 VC-dimension of union of halfspaces. Provide an upper bound on the VC-  dimension of the class of hypotheses described by the unions of k halfspaces.  3.22 VC-dimension of intersection of halfspaces. Consider the class Ck of convex in- tersections of k halfspaces. Give lower and upper bound estimates for VCdim Ck .  3.23 VC-dimension of intersection concepts.   a  Let C1 and C2 be two concept classes. Show that for any concept class  C = {c1 ∩ c2 : c1 ∈ C1, c2 ∈ C2},  ΠC m  ≤ ΠC1 m  ΠC2 m .   3.53    b  Let C be a concept class with VC-dimension d and let Cs be the concept class formed by all intersections of s concepts from C, s ≥ 1. Show that the VC-dimension of Cs is bounded by 2ds log2 3s .  Hint: show that log2 3x  < 9x  2e  for any x ≥ 2.    3.6 Exercises  55  3.24 VC-dimension of union of concepts. Let A and B be two sets of functions mapping from X into {0, 1}, and assume that both A and B have ﬁnite VC- dimension, with VCdim A  = dA and VCdim B  = dB. Let C = A ∪ B be the union of A and B.   a  Prove that for all m, ΠC m  ≤ ΠA m  + ΠB m .  b  Use Sauer’s lemma to show that for m ≥ dA + dB + 2, ΠC m  < 2m, and  give a bound on the VC-dimension of C.  3.25 VC-dimension of symmetric diﬀerence of concepts. For two sets A and B, let A∆B denote the symmetric diﬀerence of A and B, i.e., A∆B =  A∪B − A∩B . Let H be a non-empty family of subsets of X with ﬁnite VC-dimension. Let A be an element of H and deﬁne H∆A = {X∆A : X ∈ H}. Show that  VCdim H∆A  = VCdim H .  3.26 Symmetric functions. A function h : {0, 1}n → {0, 1} is symmetric if its value is uniquely determined by the number of 1’s in the input. Let C denote the set of all symmetric functions.   a  Determine the VC-dimension of C.   b  Give lower and upper bounds on the sample complexity of any consistent  PAC learning algorithm for C.   c  Note that any hypothesis h ∈ C can be represented by a vector  y0, y1, . . . , yn  ∈ {0, 1}n+1, where yi is the value of h on examples having precisely i 1’s. Devise a consistent learning algorithm for C based on this representation.  3.27 VC-dimension of neural networks.  Let C be a concept class over Rr with VC-dimension d. A C-neural network with one intermediate layer is a concept deﬁned over Rn that can be represented by a directed acyclic graph such as that of Figure 3.7, in which the input nodes are those at the bottom and in which each other node is labeled with a concept c ∈ C. The output of the neural network for a given input vector  x1, . . . , xn  is ob- tained as follows. First, each of the n input nodes is labeled with the correspond-  ing value xi ∈ R. Next, the value at a node u in the higher layer and labeled  with c is obtained by applying c to the values of the input nodes admitting an   56  Chapter 3 Rademacher Complexity and VC-Dimension  Figure 3.7 A neural network with one intermediate layer.  edge ending in u. Note that since c takes values in {0, 1}, the value at u is in {0, 1}. The value at the top or output node is obtained similarly by applying the corresponding concept to the values of the nodes admitting an edge to the output node.   a  Let H denote the set of all neural networks deﬁned as above with k ≥ 2 internal nodes. Show that the growth function ΠH m  can be upper bounded in terms of the product of the growth functions of the hypothesis sets deﬁned at each intermediate layer.   b  Use that to upper bound the VC-dimension of the C-neural networks  Hint: you can use the implication m = 2x log2 xy  ⇒ m > x log2 ym  valid for m ≥ 1, and x, y > 0 with xy > 4 .   c  Let C be the family of concept classes deﬁned by threshold functions C =  j=1 wjxj  : w ∈ Rr}. Give an upper bound on the VC-dimension of  H in terms of k and r.  {sgn  cid:80 r  3.28 VC-dimension of convex combinations. Let H be a family of functions mapping from an input space X to {−1, +1} and let T be a positive integer. Give an upper bound on the VC-dimension of the family of functions FT deﬁned by  F = cid:40 sgn cid:32  T cid:88 t=1  αtht cid:33  : ht ∈ H, αt ≥ 0,  αt ≤ 1 cid:41  .  T cid:88 t=1   Hint: you can use exercise 3.27 and its solution .  3.29 Inﬁnite VC-dimension.   3.6 Exercises  57   a  Show that if a concept class C has inﬁnite VC-dimension, then it is not  PAC-learnable.   b  In the standard PAC-learning scenario, the learning algorithm receives all examples ﬁrst and then computes its hypothesis. Within that setting, PAC- learning of concept classes with inﬁnite VC-dimension is not possible as seen in the previous question.  Imagine now a diﬀerent scenario where the learning algorithm can alternate between drawing more examples and computation. The objective of this problem is to prove that PAC-learning can then be possible for some concept classes with inﬁnite VC-dimension.  Consider for example the special case of the concept class C of all subsets of natural numbers. Professor Vitres has an idea for the ﬁrst stage of a learning algorithm L PAC-learning C. In the ﬁrst stage, L draws a suﬃcient number of points m such that the probability of drawing a point beyond the maximum value M observed be small with high conﬁdence. Can you complete Professor Vitres’ idea by describing the second stage of the algorithm so that it PAC- learns C? The description should be augmented with the proof that L can PAC-learn C.  3.30 VC-dimension generalization bound – realizable case. In this exercise we show that the bound given in corollary 3.19 can be improved to O  d log m d    in the m realizable setting. Assume we are in the realizable scenario, i.e. the target concept is included in our hypothesis class H. We will show that if a hypothesis h is consistent with a sample S ∼ Dm then for any  cid:15  > 0 such that m cid:15  ≥ 8  P[R h  >  cid:15 ] ≤ 2 cid:104  2em d  cid:105 d  2−m cid:15  2 .   3.54    a  Let HS ⊆ H be the subset of hypotheses consistent with the sample S, let S cid:48  as another independent sample drawn from Dm. Show that the following inequality holds for any h0 ∈ HS:   cid:98 RS h  denote the empirical error with respect to the sample S and deﬁne  where B m,  cid:15   is a binomial random variable with parameters  m,  cid:15  .  Hint:   cid:15   P cid:104  sup h∈HS  cid:98 RS h  −  cid:98 RS cid:48  h  > prove and use the fact that P[ cid:98 RS h  ≥  cid:15   2 cid:105  ≥ P cid:104 B m,  cid:15   >  m cid:15   2  cid:105  P[R h0  >  cid:15 ] ,  2 ] ≥ P[ cid:98 RS h  >  cid:15   2 ∧ R h  >  cid:15 ].    58  Chapter 3 Rademacher Complexity and VC-Dimension  2 . Use this inequality along with the result   b  Prove that P cid:104 B m,  cid:15   > m cid:15   2  cid:105  ≥ 1  from  a  to show that for any h0 ∈ HS  P cid:104 R h0  >  cid:15  cid:105  ≤ 2 P cid:104  sup  h∈HS  cid:98 RS h  −  cid:98 RS cid:48  h  >   cid:15   2 cid:105  .   c  Instead of drawing two samples, we can draw one sample T of size 2m then uniformly at random split it into S and S cid:48 . The right hand side of part  b  can then be rewritten as:   cid:15   2 cid:105  = P T→[S,S cid:48 ] cid:104 ∃h∈ H :  cid:98 RS h  = 0 ∧  cid:98 RS cid:48  h  >   cid:15   2 cid:105  .  2 be the total number of errors h0 makes on T . Show that the probability of all l errors falling into S cid:48  is upper bounded by 2−l.  2 and let l > m cid:15   T∼D2m:  P cid:104  sup h∈HS  cid:98 RS h − cid:98 RS cid:48  h  > Let h0 be a hypothesis such that  cid:98 RT  h0  >  cid:15  T→ S,S cid:48   cid:104  cid:98 RS h  = 0 ∧  cid:98 RS cid:48  h  >   d  Part  b  implies that for any h ∈ H  T∼D2m:  P  Use this bound to show that for any h ∈ H   cid:15   2 cid:105  ≤ 2−l .   cid:15   P  2  cid:12  cid:12  cid:12   cid:98 RT  h0  > 2 cid:105  ≤ 2−  cid:15 m T→ S,S cid:48   cid:104  cid:98 RS h  = 0 ∧  cid:98 RS cid:48  h  > T→ S,S cid:48   cid:104 ∃h ∈ H :  cid:98 RS h  = 0 ∧  cid:98 RS cid:48  h  >  cid:15   T∼D2m:  T∼D2m:  2 .   cid:15   2 cid:105 . Show that  we can achieve a high probability generalization bound that is of the order O  d log m d    .  m   e  Complete the proof of inequality  3.54  by using the union bound to up-  per bound P  3.31 Generalization bound based on covering numbers. Let H be a family of functions  mapping X to a subset of real numbers Y ⊆ R. For any  cid:15  > 0, the covering number N  H,  cid:15   of H for the L∞ norm is the minimal k ∈ N such that H can be covered with k balls of radius  cid:15 , that is, there exists {h1, . . . , hk} ⊆ H such that, for all h ∈ H, there exists i ≤ k with  cid:107 h − hi cid:107 ∞ = maxx∈X h x  − hi x  ≤  cid:15 . In particular, when H is a compact set, a ﬁnite covering can be extracted from a covering of H with balls of radius  cid:15  and thus N  H,  cid:15   is ﬁnite. Covering numbers provide a measure of the complexity of a class of functions: the larger the covering number, the richer is the family of functions. The objec- tive of this problem is to illustrate this by proving a learning bound in the case of the squared loss. Let D denote a distribution over X × Y according to which   3.6 Exercises  59  labeled examples are drawn. Then, the generalization error of h ∈ H for the squared loss is deﬁned by R h  = E  x,y ∼D[ h x −y 2] and its empirical error for i=1 h xi  − yi 2. We will assume that H is bounded, that is there exists M > 0 such that h x  − y ≤ M for all  x, y  ∈ X × Y. The following is the generalization bound proven in this problem:  a labeled sample S =   x1, y1 , . . . ,  xm, ym   by  cid:98 RS h  = 1  m cid:80 m  P  S∼Dm cid:104  sup  h∈H R h  −  cid:98 RS h  ≥  cid:15  cid:105  ≤ N cid:16 H,  The proof is based on the following steps.   cid:15   8M cid:17 2 exp cid:16 −m cid:15 2 2M 4  cid:17  .   3.55    a  Let LS = R h  −  cid:98 RS h , then show that for all h1, h2 ∈ H and any labeled  sample S, the following inequality holds:  LS h1  − LS h2  ≤ 4M cid:107 h1 − h2 cid:107 ∞ .   b  Assume that H can be covered by k subsets B1, . . . , Bk, that is H = B1 ∪ . . . ∪ Bk. Then, show that, for any  cid:15  > 0, the following upper bound holds:  P  S∼Dm cid:104  sup  h∈H LS h  ≥  cid:15  cid:105  ≤  k cid:88 i=1  P  S∼Dm cid:104  sup  h∈Bi LS h  ≥  cid:15  cid:105  .   c  Finally, let k = N  H,   cid:15  8M   and let B1, . . . , Bk be balls of radius  cid:15   8M   centered at h1, . . . , hk covering H. Use part  a  to show that for all i ∈ [k],  P  S∼Dm cid:104  sup  h∈Bi LS h  ≥  cid:15  cid:105  ≤ P  S∼Dm cid:104 LS hi  ≥   cid:15   2 cid:105  ,  and apply Hoeﬀding’s inequality  theorem D.2  to prove  3.55 .    4 Model Selection  A key problem in the design of learning algorithms is the choice of the hypothesis set H. This is known as the model selection problem. How should the hypothesis set H be chosen? A rich or complex enough hypothesis set could contain the ideal Bayes classiﬁer. On the other hand, learning with such a complex family becomes a very diﬃcult task. More generally, the choice of H is subject to a trade-oﬀ that can be analyzed in terms of the estimation and approximation errors.  Our discussion will focus on the particular case of binary classiﬁcation but much of what is discussed can be straightforwardly extended to diﬀerent tasks and loss functions.  4.1 Estimation and approximation errors  Let H be a family of functions mapping X to {−1, +1}. The excess error of a hypothesis h chosen from H, that is the diﬀerence between its error R h  and the Bayes error R∗, can be decomposed as follows:  R h  − R∗ = cid:16 R h  − inf  cid:123  cid:122   h∈H estimation   cid:124   R h  cid:17   cid:125   h∈H  + cid:16  inf  cid:124   R h  − R∗ cid:17   cid:123  cid:122   cid:125   approximation  .  The ﬁrst term is called the estimation error , the second term the approximation error . The estimation error depends on the hypothesis h selected. It measures the error of h with respect to the inﬁmum of the errors achieved by hypotheses in H, or that of the best-in-class hypothesis h∗ when that inﬁmum is reached. Note that the deﬁnition of agnostic PAC-learning is precisely based on the estimation error. The approximation error measures how well the Bayes error can be approximated using H. It is a property of the hypothesis set H, a measure of its richness. For a more complex or richer hypothesis H, the approximation error tends to be smaller at the price of a larger estimation error. This is illustrated by Figure 4.1.   4.1    62  Chapter 4 Model Selection  Figure 4.1 Illustration of the estimation error  in green  and approximation error  in orange . Here, it is assumed that there exists a best-in-class hypothesis, that is h∗ such that R h∗  = inf h∈H R h .  Model selection consists of choosing H with a favorable trade-oﬀ between the ap- proximation and estimation errors. Note, however, that the approximation error is not accessible, since in general the underlying distribution D needed to determine R∗ is not known. Even with various noise assumptions, estimating the approxima- tion error is diﬃcult. In contrast, the estimation error of an algorithm A, that is, the estimation error of the hypothesis hS returned after training on a sample S, can sometimes be bounded using generalization bounds as shown in the next section.  4.2 Empirical risk minimization  ERM   A standard algorithm for which the estimation error can be bounded is Empiri- cal Risk Minimization  ERM . ERM seeks to minimize the error on the training sample:4  hERM S  = argmin  h∈H  cid:98 RS h .  Proposition 4.1 For any sample S, the following inequality holds for the hypothesis returned by ERM:  S    − inf h∈H  P cid:104 R hERM  R h  >  cid:15  cid:105  ≤ P cid:20  sup R h cid:15   ≤ inf h∈H R h  +  cid:15 . Thus, using  cid:98 RS hERM  S  h∈H R h  −  cid:98 RS h  >   cid:15   2 cid:21  .  Proof: By deﬁnition of inf h∈H R h , for any  cid:15  > 0, there exists h cid:15  such that    ≤  cid:98 RS h cid:15  , which holds by the  4 Note that, if there exists multiple hypotheses with minimal error on the training sample, then ERM returns an arbitrary one.   4.2    4.3   h∗  h  H  hBayes   4.2 Empirical risk minimization  ERM   63  Illustration of the decomposition of a rich family H = cid:83   Figure 4.2  γ∈Γ Hγ .  deﬁnition of the algorithm, we can write  R hERM  S    − inf h∈H  R h   S  S    − R h cid:15   + R h cid:15   − inf h∈H   − R h cid:15   +  cid:15   R h  = R hERM ≤ R hERM   −  cid:98 RS hERM = R hERM   −  cid:98 RS hERM ≤ R hERM h∈H R h  −  cid:98 RS h  +  cid:15 . ≤ 2 sup  S  S  S  S  S    − R h cid:15   +  cid:15     +  cid:98 RS hERM   +  cid:98 RS h cid:15   − R h cid:15   +  cid:15   Since the inequality holds for all  cid:15  > 0, it implies the following:  R hERM  S    − inf h∈H  R h  ≤ 2 sup  h∈H R h  −  cid:98 RS h ,  which concludes the proof.   cid:3   The right-hand side of  4.3  can be upper-bounded using the generalization bounds presented in the previous chapter in terms of the Rademacher complexity, the growth function, or the VC-dimension of H. In particular, it can be bounded by 2e−2m[ cid:15 −Rm H ]2 . Thus, when H admits a favorable Rademacher complexity, for example a ﬁnite VC-dimension, for a suﬃciently large sample, with high probability, the estimation error is guaranteed to be small. Nevertheless, the performance of ERM is typically very poor. This is because the algorithm disregards the complexity of the hypothesis set H: in practice, either H is not complex enough, in which case the approximation error can be very large, or H is very rich, in which case the bound on the estimation error becomes very loose. Additionally, in many cases, determining the ERM solution is computationally intractable. For example, ﬁnding  increasing γ  h∗  h  Hγ  hBayes   64  Chapter 4 Model Selection  Figure 4.3 Choice of γ∗ with the most favorable trade-oﬀ between estimation and approximation errors.  a linear hypothesis with the smallest error on the training sample is NP-hard, as a function of the dimension of the space.  4.3 Structural risk minimization  SRM   In the previous section, we showed that the estimation error can be sometimes bounded or estimated. But, since the approximation error cannot be estimated, how should we choose H? One way to proceed is to choose a very complex family H with no approximation error or a very small one. H may be too rich for generalization bounds to hold for H, but suppose we can decompose H as a union of increasingly  complex hypothesis sets Hγ, that is H =  cid:83 γ∈Γ Hγ, with the complexity of Hγ increasing with γ, for some set Γ. Figure 4.2 illustrates this decomposition. The problem then consists of selecting the parameter γ∗ ∈ Γ and thus the hypothesis set Hγ∗ with the most favorable trade-oﬀ between estimation and approximation errors. Since these quantities are not known, instead, as illustrated by Figure 4.3, a uniform upper bound on their sum, the excess error  also called excess risk , can be used.  This is precisely the idea behind the Structural Risk Minimization  SRM  method. For SRM, H is assumed to be decomposable into a countable set, thus, we will  write its decomposition as H = cid:83 k≥1 Hk. Additionally, the hypothesis sets Hk are  assumed to be nested: Hk ⊂ Hk+1 for all k ≥ 1. However, many of the results presented in this section also hold for non-nested hypothesis sets. Thus, we will not make use of that assumption, unless explicitly speciﬁed. SRM consists of choosing the index k∗ ≥ 1 and the ERM hypothesis h in Hk∗ that minimize an upper bound on the excess error.  r o r r e  estimation approximation upper bound  γ∗  γ   4.3 Structural risk minimization  SRM   65  Figure 4.4 Illustration of structural risk minimization. The plots of three errors are shown as a function of the index k. Clearly, as k, or equivalently the complexity the hypothesis set Hk, increases, the training error decreases, while the penalty term increases. SRM selects the hypothesis minimizing a bound on the generalization error, which is a sum of the empirical error and the penalty term.  As we shall see, the following learning bound holds for all h ∈ H: for any δ > 0, with probability at least 1 − δ over the draw of a sample S of size m from Dm, for all h ∈ Hk and k ≥ 1,  Thus, to minimize the resulting bound on the excess error  R h  − R∗ , the index k and the hypothesis h ∈ Hk should be chosen to minimize the following objective function:  m  R h  ≤  cid:98 RS h  + Rm Hk h   + cid:114  log k Fk h  =  cid:98 RS h  + Rm Hk  + cid:114  log k  S  :  .  m This is precisely the deﬁnition of the SRM solution hSRM  + cid:115  log 2  δ 2m  .  hSRM S  = argmin k≥1,h∈Hk  Fk h  = argmin  k≥1,h∈Hk  cid:98 RS h  + Rm Hk  + cid:114  log k  m  .  Thus, SRM identiﬁes an optimal index k∗ and therefore hypothesis set Hk∗ , and re- turns the ERM solution based on that hypothesis set. Figure 4.4 further illustrates the selection of the index k∗ and hypothesis set Hk∗ by SRM by minimizing an upper  bound on the sum of the training error and the penalty term Rm Hk  + cid:112 log k m.  The following theorem shows that the SRM solution beneﬁts from a strong learning guarantee. For any h ∈ H, we will denote by Hk h  the least complex hypothesis set among the Hks that contain h.   4.4   r o r r e  generalization bound penalty term empirical error  k   66  Chapter 4 Model Selection  Theorem 4.2  SRM Learning guarantee  For any δ > 0, with probability at least 1 − δ over the draw of an i.i.d. sample S of size m from Dm, the generalization error of the hypothesis hSRM  returned by the SRM method is bounded as follows:  S  R hSRM  S  h∈H cid:32 R h  + 2Rm Hk h   + cid:114  log k h   m  cid:33  + cid:115  2 log 3  m  δ    ≤ inf  .  Proof: Observe ﬁrst that, by the union bound, the following general inequality holds:  P cid:20  sup  h∈H  ≤  k≥1  R h  − Fk h  h  >  cid:15  cid:21  = P cid:20  sup ∞ cid:88 k=1 ∞ cid:88 k=1 ∞ cid:88 k=1 ∞ cid:88 k=1 = e−2m cid:15 2 ∞ cid:88 k=1  e−2m cid:15 2  ≤  ≤  =  h∈Hk  sup h∈Hk  R h  − Fk h  >  cid:15  cid:21  R h  − Fk h  >  cid:15  cid:21  R h  −  cid:98 RS h  − Rm Hk  >  cid:15  + cid:114  log k m  cid:21   P cid:20  sup P cid:20  sup exp cid:16  − 2m cid:104  cid:15  + cid:113  log k m  cid:105 2 cid:17   h∈Hk   4.5   e−2 log k  1 k2 =  π2 6  e−2m cid:15 2  ≤ 2e−2m cid:15 2  .  Next, for any two random variables X1 and X2, if X1 + X2 >  cid:15 , then either X1 or  X2 must be larger than  cid:15  2. In view of that, by the union bound, P[X1 + X2 >  cid:15 ] ≤ P[X1 >  cid:15  2 ]. Using this inequality, inequality  4.5 , and the inequality Fk hSRM , we    ≤ Fk h  h , which holds for all h ∈ H, by deﬁnition of hSRM  2 ] + P[X2 >  cid:15    hSRM  S  S  S   4.3 Structural risk minimization  SRM   67   cid:3   can write, for any h ∈ H,  P cid:104 R hSRM  S  S  S  S   cid:15     >    hSRM    − Fk hSRM   hSRM    − R h  − 2Rm Hk h   − cid:113  log k h  m >  cid:15  cid:105  ≤ P cid:104 R hSRM 2 cid:105  + P cid:104 Fk hSRM ≤ 2e− m cid:15 2 = 2e− m cid:15 2 = 2e− m cid:15 2    − R h  − 2Rm Hk h   − cid:113  log k h  2 + P cid:104 Fk h  h  − R h  − 2Rm Hk h   − cid:113  log k h  2 + P cid:104  cid:98 RS h  − R h  − Rm Hk h   >  2 = 3e− m cid:15 2 2 .  2 + e− m cid:15 2  2 cid:105    cid:15   S  S  m >  m >   cid:15   2 cid:105  2 cid:105    cid:15   Setting the right-hand side to be equal to δ completes the proof.  The learning guarantee just proven for SRM is remarkable. To simplify its discus- sion, let us assume that there exists h∗ such that R h∗  = inf h∈H R h , that is, that there exists a best-in-class classiﬁer h∗ ∈ H. Then, the theorem implies in particular that, with probability at least 1− δ, the following inequality holds for all h ∈ H:  R hSRM  S    ≤ R h∗  + 2Rm Hk h∗   + cid:114  log k h∗   m  + cid:115  2 log 3  m  δ  .   4.6   Observe that, remarkably, this bound is similar to the estimation error bound for  Hk h∗ : it diﬀers from it only by the term cid:112 log k h∗  m. Thus, modulo that term,  the guarantee for SRM is as favorable as the one we would have obtained, had an oracle informed us of the index k h∗  of the best-in-class classiﬁer’s hypothesis set. Furthermore, observe that when H is rich enough that R h∗  is close to the Bayes error, the learning bound  4.6  is approximately a bound on the excess error of the SRM solution. Note that, if for some k0, the empirical error of the ERM solution for Hk0 is zero, which holds in particular if Hk0 contains the Bayes error, then, we have minh∈Hk Fk0 h  ≤ minh∈Hk Fk h  for all k > k0 and only ﬁnitely many indices need to be considered in SRM. Assume more generally that if minh∈Hk Fk h  ≤ minh∈Hk+1 Fk h  for some k, then indices beyond k + 1 need not be inspected. This property may hold for example if the empirical error cannot be further improved after some index k. In that case, the minimizing index k∗ can be determined via a binary search in the interval [1, kmax], given some maximum value kmax. kmax itself can be found by inspecting minh∈H2n Fk h  for exponentially growing indices 2n, n ≥ 1, and setting kmax = 2n for n such that minh∈H2n Fk h  ≤ minh∈H2n+1 Fk h . The number of ERM computations needed to ﬁnd kmax is in O n  = O log kmax  and similarly the   68  Chapter 4 Model Selection  number of ERM computations due to the binary search is in O log kmax . Thus, if n is the smallest integer such that k∗ < 2n, the overall number of ERM computations is in O log k∗ .  While it beneﬁts from a very favorable guarantee, SRM admits several drawbacks. First, the decomposability of H into countably many hypothesis sets, each with a converging Rademacher complexity, remains a strong assumption. As an example, the family of all measurable functions cannot be written as a union of countably many hypothesis sets with ﬁnite VC-dimension. Thus, the choice of H or that of the hypothesis sets Hk is a key component of SRM. Second, and this is the main disadvantage of SRM, the method is typically computationally intractable: for most hypothesis sets, ﬁnding the solution of ERM is NP-hard and in general SRM requires determining that solution for a large number of indices k.  4.4 Cross-validation  An alternative method for model selection, cross-validation, consists of using some fraction of the training sample as a validation set to select a hypothesis set Hk. This is in contrast with the SRM model which relies on a theoretical learning bound assigning a penalty to each hypothesis set. In this section, we analyze the cross- validation method and compare its performance to that of SRM.  As in the previous section, let  Hk k≥1 be a countable sequence of hypothesis sets with increasing complexities. The cross-validation  CV  solution is obtained as follows. Let S be an i.i.d. labeled sample of size m. S is divided into a sample S1 of size  1 − α m and a sample S2 of size αm, with α ∈  0, 1  typically chosen to be relatively small. S1 is reserved for training, S2 for validation. For any k ∈ N, let hERM S1,k denote the solution of ERM run on S1 using the hypothesis set Hk. The hypothesis hCV S1,k with the best performance on S2:  returned by cross-validation is the ERM solution hERM  S  hCV S =  argmin  S1,k : k≥1 cid:9  cid:98 RS2  h . h∈ cid:8 hERM   4.7   The following general result will help us derive learning guarantees for cross-validation. Proposition 4.3 For any α > 0 and any sample size m ≥ 1, the following general inequality holds:  P cid:34 sup k≥1 cid:12  cid:12  cid:12 R hERM  S1,k   −  cid:98 RS2 hERM  S1,k   cid:12  cid:12  cid:12  >  cid:15  + cid:114  log k  αm cid:35  ≤ 4e−2αm cid:15 2  .   69   4.8   4.4 Cross-validation  P cid:34 sup k≥1 cid:12  cid:12  cid:12 R hERM  Proof: By the union bound, we can write  =  ≤  αm cid:35  S1,k   cid:12  cid:12  cid:12  >  cid:15  + cid:114  log k S1,k   −  cid:98 RS2 hERM P cid:34  cid:12  cid:12  cid:12 R hERM αm cid:35  S1,k   cid:12  cid:12  cid:12  >  cid:15  + cid:114  log k ∞ cid:88 k=1 S1,k   −  cid:98 RS2 hERM E cid:34 P cid:34  cid:12  cid:12  cid:12 R hERM αm  cid:12  cid:12  cid:12  cid:12  S1 cid:35  cid:35  . S1,k   cid:12  cid:12  cid:12  >  cid:15  + cid:114  log k ∞ cid:88 k=1 S1,k   −  cid:98 RS2  hERM αm  cid:12  cid:12  cid:12  cid:12  S1 cid:35  ≤ 2e−2αm cid:0  cid:15 +√ log k S1,k   cid:12  cid:12  cid:12  >  cid:15  + cid:114  log k αm cid:1 2 ≤ 2e−2αm cid:15 2−2 log k  S1,k   −  cid:98 RS2  hERM  .  =  2  k2 e−2αm cid:15 2  .  P cid:34  cid:12  cid:12  cid:12 R hERM  The hypothesis hERM S1,k is ﬁxed conditioned on S1. Furthermore, the sample S2 is independent from S1. Therefore, by Hoeﬀding’s inequality, we can bound the conditional probability as follows:  Plugging in the right-hand side of this bound in  4.8  and summing over k yields  e−2αm cid:15 2  < 4e−2αm cid:15 2  ,  P cid:34 sup k≥1 cid:12  cid:12  cid:12 R hERM  S1,k   −  cid:98 RS2 hERM  αm cid:35  ≤ S1,k   cid:12  cid:12  cid:12  >  cid:15  + cid:114  log k  π2 3   cid:3   S1  which completes the proof. Let R hSRM   be the generalization error of the SRM solution using a sample S1 of size  1 − αm  and R hCV S , S  the generalization error of the cross-validation solution using a sample S of size m. Then, using Proposition 4.3, the following learning guarantee can be derived which compares the error of the CV method to that of SRM.  Theorem 4.4  Cross-validation versus SRM  For any δ > 0, with probability at least 1 − δ, the following holds:  R hCV  S   − R hSRM  S1    ≤ 2 cid:115  log max k hCV  S  , k hSRM αm  S1      + 2 cid:115  log 4  δ 2αm  ,  where, for any h, k h  denotes the smallest index of a hypothesis set containing h. Proof: By Proposition 4.3 and Theorem 4.2, using the property of hCV as a min- imizer, for any δ > 0, with probability at least 1 − δ, the following inequalities  S   Chapter 4 Model Selection  70  hold:  R hCV  S   ≤  cid:98 RS2  hCV ≤  cid:98 RS2  hSRM ≤ R hSRM  S1  δ 2αm  + cid:115  log 4 + cid:115  log 4 + cid:115  log k hSRM  δ 2αm  S1 αm  S1  αm  S     S     S   + cid:114  log k hCV   + cid:114  log k hCV   + cid:114  log k hCV   + 2 cid:115  log max k hCV  S     αm  αm  S  , k hSRM αm  S1      ≤ R hSRM which completes the proof.  S1      δ 2αm  + 2 cid:115  log 4 + 2 cid:115  log 4  δ 2αm  ,   cid:3   The learning guarantee just proven shows that, with high probability, the gener- alization error of the CV solution for a sample of size m is close to that of the SRM solution for a sample of size  1 − α m. For α relatively small, this suggests a guarantee similar to that of SRM, which, as previously discussed, is very favor- able. However, in some unfavorable regimes, an algorithm  here SRM  trained on  1 − α m points may have a signiﬁcantly worse performance than when trained on m points  avoiding this phase transition issue is one of the main motivations behind the use of the n-fold cross-validation method in practice, see section 4.5 . Thus, the bound suggests in fact a trade-oﬀ: α should be chosen suﬃciently small to avoid the unfavorable regimes just mentioned and yet suﬃciently large for the right-hand side of the bound to be small and thus informative.  The learning bound for CV can be made more explicit in some cases in practice. Assume for example that the hypothesis sets Hk are nested and that the empirical errors of the ERM solutions hERM S1,k are decreasing before reaching zero: for any k, S1,k+1  ≤ S1,k   > 0 implies at least one error for hERM S1,n   = 0 for all n ≥ m + 1. Thus, we have hERM S1,m+1 for all n ≥ m + 1 and we can assume that k fCV   ≤ m + 1. Since the complexity of Hk increases with k we also have k fSRM   ≤ m + 1. In view of that, we obtain the following more explicit learning bound for cross-validation:  S1,k   > 0 and  cid:98 RS1  hERM m . In view of that, we must then have  cid:98 RS1  hERM  S1,k+1  <  cid:98 RS1  hERM  cid:98 RS1 hERM S1,k   otherwise. Observe that  cid:98 RS1  hERM  cid:98 RS1 hERM S1,k , therefore  cid:98 RS1 hERM  S1,k   for all k such that  cid:98 RS1 hERM  S1,n = hERM  S1,k   > 1  R fCV , S  − R fSRM , S1  ≤ 2 cid:115  log  4  δ   2αm  + 2 cid:114  log m + 1   αm  .   4.5 n-Fold cross-validation  71  4.5 n-Fold cross-validation  In practice, the amount of labeled data available is often too small to set aside a validation sample since that would leave an insuﬃcient amount of training data. Instead, a widely adopted method known as n-fold cross-validation is used to exploit the labeled data both for model selection and for training.  Let θ denote the vector of free parameters of the algorithm. For a ﬁxed value of θ, the method consists of ﬁrst randomly partitioning a given sample S of m labeled examples into n subsamples, or folds. The ith fold is thus a labeled sample   xi1, yi1 , . . . ,  ximi, yimi   of size mi. Then, for any i ∈ [n], the learning algorithm is trained on all but the ith fold to generate a hypothesis hi, and the performance of hi is tested on the ith fold, as illustrated in ﬁgure 4.5a. The parameter value θ is evaluated based on the average error of the hypotheses hi, which is called the  cross-validation error . This quantity is denoted by  cid:98 RCV θ  and deﬁned by  L hi xij , yij   .   cid:98 RCV θ  =  1 n  n cid:88 i=1  1 mi  mi cid:88 j=1   cid:124   error of hi on the ith fold   cid:123  cid:122    cid:125   The folds are generally chosen to have equal size, that is mi = m n for all i ∈ [n]. How should n be chosen? The appropriate choice is subject to a trade-oﬀ. For a large n, each training sample used in n-fold cross-validation has size m − m n = m 1 − 1 n   illustrated by the right vertical red line in ﬁgure 4.5b , which is close to m, the size of the full sample, and also implies all training samples are quite similar. At the same time, the ith fold used to measure the error is relatively small and thus the cross-validation error tends to have a small bias but a large variance. In contrast, smaller values of n lead to more diverse training samples but their size  shown by the left vertical red line in ﬁgure 4.5b  is signiﬁcantly less than m. In this regime, the ith fold is relatively large and thus the cross-validation error tends to have a smaller variance but a larger bias.  In applications, n is typically chosen to be 5 or 10. n-fold cross-validation is used as follows in model selection. The full labeled data is ﬁrst split into a training and a test sample. The training sample of size m is then used to compute the n-  fold cross-validation error  cid:98 RCV θ  for a small number of possible values of θ. The free parameter θ is next set to the value θ0 for which  cid:98 RCV θ  is smallest and the  algorithm is trained with the parameter setting θ0 over the full training sample of size m. Its performance is evaluated on the test sample as already described in the previous section.  The special case of n-fold cross-validation where n = m is called leave-one-out cross-validation, since at each iteration exactly one instance is left out of the train-   72  Chapter 4 Model Selection   a    b   Figure 4.5 n-fold cross-validation.  a  Illustration of the partitioning of the training data into 5 folds.  b  Typical plot of a classiﬁer’s prediction error as a function of the size of the training sample m: the error decreases as a function of the number of training points. The red line on the left side marks the region for small values of n, while the red line on the right side marks the region for large values of n.  ing sample. As shown in chapter 5, the average leave-one-out error is an approxi- mately unbiased estimate of the average error of an algorithm and can be used to derive simple guarantees for some algorithms. In general, the leave-one-out error is very costly to compute, since it requires training m times on samples of size m− 1, but for some algorithms it admits a very eﬃcient computation  see exercise 11.9 . In addition to model selection, n-fold cross-validation is also commonly used for performance evaluation. In that case, for a ﬁxed parameter setting θ, the full labeled sample is divided into n random folds with no distinction between training and test samples. The performance reported is the n-fold cross-validation error on the full sample as well as the standard deviation of the errors measured on each fold.  4.6 Regularization-based algorithms  A broad family of algorithms inspired by the SRM method is that of regularization- based algorithm. This consists of selecting a very complex family H that is an  uncountable union of nested hypothesis sets Hγ: H = cid:83 γ>0 Hγ. H is often chosen  to be dense in the space of continuous functions over X. For example, H may be chosen to be the set of all linear functions in some high-dimensional space and Hγ the subset of those functions whose norm is bounded by γ: Hγ = {x  cid:55 → w · Φ x  :  cid:107 w cid:107  ≤ γ}. For some choices of Φ and the high-dimensional space, it can be shown that H is indeed dense in the space of continuous functions over X.  train  train  train  train  test  train  train  train  test  train  ...  ...  ...  test  train  train  train  train   4.7 Convex surrogate losses  73  Given a labeled sample S, the extension of the SRM method to an uncountable union would then suggest selecting h based on the following optimization problem:  argmin  γ>0,h∈Hγ  cid:98 RS h  + Rm Hγ  + cid:114  log γ  m  ,  where other penalty terms pen γ, m  can be chosen in lieu of the speciﬁc choice  pen γ, m  = Rm Hγ  + cid:113  log γ m . Often, there exists a function R : H → R such that, for any γ > 0, the constrained optimization problem argminγ>0,h∈Hγ  cid:98 RS h  +  pen γ, m  can be equivalently written as the unconstrained optimization problem  argmin  h∈H  cid:98 RS h  + λR h ,  for some λ > 0. R h  is called a regularization term and λ > 0 is treated as a hyperparameter since its optimal value is often not known. For most algorithms, the regularization term R h  is chosen to be an increasing function of  cid:107 h cid:107  for some choice of the norm  cid:107  ·  cid:107 , when H is the subset of a Hilbert space. The variable λ is often called a regularization parameter . Larger values of λ further penalize more complex hypotheses, while, for λ close or equal to zero, the regularization term has no eﬀect and the algorithm coincides with ERM. In practice, λ is typically selected via cross-validation or using n-fold cross-validation. When the regularization term is chosen to be  cid:107 h cid:107 p for some choice of the norm and p ≥ 1, then it is a convex function of h, since any norm is convex. How- ever, for the zero-one loss, the ﬁrst term of the objective function is non-convex, thereby making the optimization problem computationally hard. In practice, most regularization-based algorithms instead use a convex upper bound on the zero-one loss and replace the empirical zero-one term with the empirical value of that convex surrogate. The resulting optimization problem is then convex and therefore admits more eﬃcient solutions than SRM. The next section studies the properties of such convex surrogate losses.  4.7 Convex surrogate losses  The guarantees for the estimation error that we presented in previous sections hold either for ERM or for SRM, which itself is deﬁned in terms of ERM. However, as already mentioned, for many choices of the hypothesis set H, including that of linear functions, solving the ERM optimization problem is NP-hard mainly because the zero-one loss function is not convex. One common method for addressing this problem consists of using a convex surrogate loss function that upper bounds the zero-one loss. This section analyzes learning guarantees for such surrogate losses in terms of the original loss.   74  Chapter 4 Model Selection  The hypotheses we consider are real-valued functions h : X → R. The sign of h deﬁnes a binary classiﬁer fh : X → {−1, +1} deﬁned for all x ∈ X by  fh x  = cid:40 +1  −1  if h x  ≥ 0 if h x  < 0.  The loss or error of h at point  x, y  ∈ X × {−1, +1} is deﬁned as the binary classiﬁcation error of fh:  1fh x  cid:54 =y = 1yh x <0 + 1h x =0∧y=−1 ≤ 1yh x ≤0.   x,y ∼D cid:2 1fh x  cid:54 =y cid:3 . For We will denote by R h  the expected error of h: R h  = E any x ∈ X, let η x  denote η x  = P[y = +1x] and let DX denote the marginal  distribution over X. Then, for any h, we can write  R h  = E = E = E   x,y ∼D cid:2 1fh x  cid:54 =y cid:3  x∼DX cid:2 η x 1h x  0 +  1 − η x  1h x =0 cid:3  x∼DX cid:2 η x 1h x <0 +  1 − η x  1h x ≥0 cid:3 .  In view of that, the Bayes classiﬁer can be deﬁned as assigning label +1 to x when η x  ≥ 1 2 , −1 otherwise. It can therefore be induced by the function h∗ deﬁned by  4.9   h∗ x  = η x  − 1 2 .  We will refer to h∗ : X → R as the Bayes scoring function and will denote by R∗ the error of the Bayes classiﬁer or Bayes scoring function: R∗ = R h∗ .  Lemma 4.5 The excess error of any hypothesis h : X → R can be expressed as follows in terms of η and the Bayes scoring function h∗:  Proof: For any h, we can write  x∼DX cid:104 h∗ x  1h x h∗ x ≤0 cid:105 . R h  − R∗ = 2 E x∼DX cid:104 η x 1h x <0 +  1 − η x  1h x ≥0 cid:105  x∼DX cid:104 η x 1h x <0 +  1 − η x   1 − 1h x <0  cid:105  x∼DX cid:104 [2η x  − 1]1h x <0 +  1 − η x   cid:105  x∼DX cid:104 2h∗ x 1h x <0 +  1 − η x   cid:105 ,  R h  = E = E = E = E   4.7 Convex surrogate losses  75  where we used for the last step equation  4.9 . following holds:  In view of that, for any h, the  R h  − R h∗  = E = E = 2 E  x∼DX cid:104 2[h∗ x ] 1h x ≤0 − 1h∗ x ≤0  cid:105  x∼DX cid:104 2[h∗ x ] sgn h∗ x  1 h x h∗ x ≤0 ∧  h x ,h∗ x   cid:54 = 0,0   cid:105  x∼DX cid:104 h∗ x  1h x h∗ x ≤0 cid:105 ,   cid:3  which completes the proof, since R h∗  = R∗. Let Φ : R → R be a convex and non-decreasing function so that for any u ∈ R, 1u≤0 ≤ Φ −u . The Φ-loss of a function h : X → R at point  x, y  ∈ X × {−1, +1} is deﬁned as Φ −yh x   and its expected loss given by  LΦ h  = E = E   x,y ∼D cid:2 Φ −yh x   cid:3  x∼DX cid:104 η x Φ −h x   +  1 − η x  Φ h x   cid:105 .   4.10   Notice that since 1yh x ≤0 ≤ Φ −yh x  , we have R h  ≤ LΦ h . For any x ∈ X, let u  cid:55 → LΦ x, u  be the function deﬁned for all u ∈ R by  LΦ x, u  = η x Φ −u  +  1 − η x  Φ u .  Then, LΦ h  = Ex∼DX [LΦ x, h x  ]. Since Φ is convex, u  cid:55 → LΦ x, u  is convex as a sum of two convex functions. Deﬁne h∗Φ : X → [−∞, +∞] as the Bayes solution for the loss function LΦ. That is, for any x, h∗Φ x  is a solution of the following convex optimization problem:  h∗Φ x  = argmin  LΦ x, u   u∈[−∞,+∞]  = argmin  u∈[−∞,+∞]  η x Φ −u  +  1 − η x  Φ u .  The solution of this optimization is in general not unique. When η x  = 0, h∗Φ x  is a minimizer of u  cid:55 → Φ u  and since Φ is non-decreasing, we can choose h∗Φ x  = −∞ in that case. Similarly, when η x  = 1, we can choose h∗Φ x  = +∞. When η x  = 1 2 , LΦ x, u  = 1 2   = Φ 0 . Thus, we can choose h∗Φ x  = 0 in that case. For all other values of η x , in case of non-uniqueness, an arbitrary minimizer is chosen in this deﬁnition. We will denote by L∗Φ the Φ-loss of h∗Φ: L∗Φ = E  2 [Φ −u  + Φ u ], thus, by convexity, LΦ x, u  ≥ Φ − u  2 + u   x,y ∼D cid:2 Φ −yh∗Φ x   cid:3 .  Proposition 4.6 Let Φ be a convex and non-decreasing function that is diﬀerentiable at 0 with Φ cid:48  0  > 0. Then, the minimizer of Φ deﬁnes the Bayes classiﬁer: for any x ∈ X, h∗Φ x  > 0 iﬀ h∗ x  > 0 and h∗ x  = 0 iﬀ h∗Φ x  = 0, which implies L∗Φ = R∗.   76  Chapter 4 Model Selection  2 and h∗Φ x  = −∞, thus h∗ x  2 and  Proof: Fix x ∈ X. If η x  = 0, then h∗ x  = − 1 and h∗Φ x  admit the same sign. Similarly, if η x  = 1, then h∗ x  = + 1 h∗Φ x  = +∞, and h∗ x  and h∗Φ x  admit the same sign. Let u∗ denote the minimizer deﬁning h∗Φ x . u∗ is a minimizer of u  cid:55 → LΦ x, u  iﬀ the subdiﬀerential of that function at u∗ contains 0, that is, since ∂LΦ x, u∗  = −η x ∂Φ −u∗  +  1 − η x  ∂Φ u∗ , iﬀ there exist v1 ∈ ∂Φ −u∗  and v2 ∈ ∂Φ u∗  such that  4.11   η x v1 =  1 − η x  v2.  If u∗ = 0, by the diﬀerentiability of Φ at 0 we have v1 = v2 = Φ cid:48  0  > 0 and thus η x  = 1 2 , that is h∗ x  = 0. Conversely, If h∗ x  = 0, that is η x  = 1 2 , then, by deﬁnition, we have h∗Φ x  = 0. Thus, h∗ x  = 0 iﬀ h∗Φ x  = 0 iﬀ η x  = 1 2 . We can assume now that η x  is not in {0, 1, 1 2}. We ﬁrst show that for any u1, u2 ∈ R with u1 < u2, and any two choices of the subgradients at u1 and u2, v1 ∈ ∂Φ u1  and v2 ∈ ∂Φ u2 , we have v1 ≤ v2. By deﬁnition of the subgradients at u1 and u2, the following inequalities hold:  Φ u2  − Φ u1  ≥ v1 u2 − u1   Φ u1  − Φ u2  ≥ v2 u1 − u2 .  Summing up these inequalities yields v2 u2 − u1  ≥ v1 u2 − u1  and thus v2 ≥ v1, since u1 < u2. Now, if u∗ > 0, then we have −u∗ < u∗. By the property shown above, this implies v1 ≤ v2. We cannot have v1 = v2  cid:54 = 0 since  4.11  would then imply η x  = 1 2 . We also cannot have v1 = v2 = 0 since by the property shown above, we must have Φ cid:48  0  ≤ v2 and thus v2 > 0. Thus, we must have v1   0, which, by  4.11 , implies η x  > 1 − η x , that is h∗ x  > 0. Conversely, if h∗ x  > 0 then η x  > 1 − η x . We cannot have v1 = v2 = 0 or v1 = v2  cid:54 = 0 as already shown. Thus, since η x   cid:54 = 1, by  4.11 , this implies v1 < v2. We cannot have u∗ < −u∗ since, by the property shown above, this would imply v2 ≤ v1. Thus, we must have −u∗ ≤ u∗, that is u∗ ≥ 0, and more speciﬁcally  cid:3  u∗ > 0 since, as already shown above, u∗ = 0 implies h∗ x  = 0.  Theorem 4.7 Let Φ be a convex and non-decreasing function. Assume that there exists s ≥ 1 and c > 0 such that the following holds for all x ∈ X:  h∗ x s = cid:12  cid:12 η x  − 1 2 cid:12  cid:12 s  ≤ cs cid:2 LΦ x, 0  − LΦ x, h∗Φ x   cid:3 .  Then, for any hypothesis h, the excess error of h is bounded as follows:  R h  − R∗ ≤ 2c cid:2 LΦ h  − L∗Φ cid:3  1  s   4.8 Chapter notes  Proof: We will use the following inequality which holds by the convexity of Φ:  ≤ η x Φ  −h x    +  1 − η x  Φ h x   = LΦ x, h x  .   4.12   2 , we can write  Φ cid:0 −2h∗ x h x  cid:1  = Φ cid:0  1 − 2η x  h x  cid:1   = Φ cid:0 η x  −h x   +  1 − η x  h x  cid:1  By Lemma 4.5, Jensen’s inequality, and h∗ x  = η x  − 1 R h  − R h∗  x∼DX cid:104 2η x  − 1 1h x h∗ x ≤0 cid:105  = E x∼DX cid:104 2η x  − 1s 1h x h∗ x ≤0 cid:105  1 ≤ E x∼DX cid:104  cid:2 Φ 0  − LΦ x, h∗Φ x   cid:3  1h x h∗ x ≤0 cid:105  1 ≤ 2c E x∼DX cid:104  cid:2 Φ cid:0 −2h∗ x h x  cid:1  − LΦ x, h∗Φ x   cid:3  1h x h∗ x ≤0 cid:105  1 ≤ 2c E x∼DX cid:104 [LΦ x, h x   − LΦ x, h∗Φ x  ] 1h x h∗ x ≤0 cid:105  1 ≤ 2c E x∼DX cid:104 LΦ x, h x   − LΦ x, h∗Φ x   cid:105  1 ≤ 2c E which completes the proof, since Ex∼DX [LΦ x, h∗Φ x  ] = L∗Φ.  ,  s  s  s  s  s   Jensen’s ineq.    assumption    Φ non-decreasing    convexity ineq.  4.12    77   cid:3   The theorem shows that, when the assumption holds, the excess error of h can be upper bounded in terms of the excess Φ-loss. The assumption of the theorem holds in particular for the following convex loss functions:   Hinge loss, where Φ u  = max 0, 1 + u , with s = 1 and c = 1 2 .   Exponential loss, where Φ u  = exp u , with s = 2 and c = 1√2 .   Logistic loss, where Φ u  = log2 1 + eu , with s = 2 and c = 1√2 They also hold for the square loss and the squared Hinge loss  see Exercises 4.2 and 4.3 .  .  4.8 Chapter notes  The structural risk minimization  SRM  technique is due to Vapnik [1998]. The original penalty term used by Vapnik [1998] is based on the VC-dimension of the hypothesis set. The version of SRM with Rademacher complexity-based penalties that we present here leads to ﬁner data-dependent learning guarantees. Penalties based on alternative complexity measures can be used similarly leading to learning bounds in terms of the corresponding complexity measure [Bartlett et al., 2002a].   78  Chapter 4 Model Selection  An alternative model selection theory of Voted Risk Minimization  VRM  has been recently developed by Cortes, Mohri, and Syed [2014] and other related pub- lications [Kuznetsov et al., 2014, DeSalvo et al., 2015, Cortes et al., 2015].  Theorem 4.7 is due to Zhang [2003a]. The proof given here is somewhat diﬀerent  and simpler.  4.9 Exercises  4.1 For any hypothesis set H, show that the following inequalities hold:  E  S∼Dm cid:104  cid:98 RS cid:0 hERM  S   cid:1  cid:105  ≤ inf  h∈H  R h  ≤ E  S∼Dm cid:104 R cid:0 hERM  S   cid:1  cid:105 .   4.13   4.2 Show that for the squared loss, Φ u  =  1 + u 2, the statement of Theorem 4.7 2 and therefore that the excess error can be upper  holds with s = 2 and c = 1 bounded as follows:  4.3 Show that for the squared Hinge loss, Φ u  = max 0, 1 + u 2, the statement of 2 and therefore that the excess error can  Theorem 4.7 holds with s = 2 and c = 1 be upper bounded as follows:  R h  − R∗ ≤  cid:2 LΦ h  − L∗Φ cid:3  1  2 .  R h  − R∗ ≤  cid:2 LΦ h  − L∗Φ cid:3  1  2 .  4.4 In this problem, the loss of h : X → R at point  x, y  ∈ X × {−1, +1} is deﬁned  to be 1yh x ≤0.   a  Deﬁne the Bayes classiﬁer and a Bayes scoring function h∗ for this loss.  b  Express the excess error of h in terms of h∗  counterpart of Lemma 4.5, for  loss considered here .   c  Give a counterpart of the result of Theorem 4.7 for this loss.  4.5 Same questions as in Exercise 4.5 with the loss of h : X → R at point  x, y  ∈  X × {−1, +1} deﬁned instead to be 1yh x <0.   5 Support Vector Machines  This chapter presents one of the most theoretically well motivated and practically most eﬀective classiﬁcation algorithms in modern machine learning: Support Vector Machines  SVMs . We ﬁrst introduce the algorithm for separable datasets, then present its general version designed for non-separable datasets, and ﬁnally provide a theoretical foundation for SVMs based on the notion of margin. We start with the description of the problem of linear classiﬁcation.  5.1 Linear classiﬁcation  Consider an input space X that is a subset of RN with N ≥ 1, and the output or target space Y = {−1, +1}, and let f : X → Y be the target function. Given a hypothesis set H of functions mapping X to Y, the binary classiﬁcation task is formulated as follows. The learner receives a training sample S of size m drawn i.i.d. from X according to some unknown distribution D, S =   x1, y1 , . . . ,  xm, ym   ∈  X × Y m, with yi = f  xi  for all i ∈ [m]. The problem consists of determining a hypothesis h ∈ H, a binary classiﬁer , with small generalization error:  RD h  = P x∼D  [h x   cid:54 = f  x ].   5.1   Diﬀerent hypothesis sets H can be selected for this task. In view of the results pre- sented in chapter 3, which formalized Occam’s razor principle, hypothesis sets with smaller complexity — e.g., smaller VC-dimension or Rademacher complexity — provide better learning guarantees, everything else being equal. A natural hypoth- esis set with relatively small complexity is that of linear classiﬁers, or hyperplanes, which can be deﬁned as follows:  H = {x  cid:55 → sign w · x + b  : w ∈ RN , b ∈ R}.   5.2   The learning problem is then referred to as a linear classiﬁcation problem. The  general equation of a hyperplane in RN is w · x + b = 0, where w ∈ RN is a   80  Chapter 5 Support Vector Machines  Figure 5.1 Two possible separating hyperplanes. The right-hand side ﬁgure shows a hyperplane that maxi- mizes the margin.  non-zero vector normal to the hyperplane and b ∈ R a scalar. A hypothesis of the form x  cid:55 → sign w · x + b  thus labels positively all points falling on one side of the hyperplane w · x + b = 0 and negatively all others. 5.2 Separable case  In this section, we assume that the training sample S can be linearly separated, that is, we assume the existence of a hyperplane that perfectly separates the train- ing sample into two populations of positively and negatively labeled points, as illustrated by the left panel of ﬁgure 5.1. This is equivalent to the existence of   w, b  ∈  RN − {0}  × R such that ∀i ∈ [m],  yi w · xi + b  ≥ 0.   5.3   But, as can be seen from ﬁgure 5.1, there are then inﬁnitely many such separating hyperplanes. Which hyperplane should a learning algorithm select? The deﬁnition of the SVM solution is based on the notion of geometric margin.  Deﬁnition 5.1  Geometric margin  The geometric margin ρh x  of a linear classiﬁer h : x  cid:55 → w·x+b at a point x is its Euclidean distance to the hyperplane w·x+b = 0:  5.4   ρh x  = w · x + b  .   cid:107 w cid:107 2  The geometric margin ρh of a linear classiﬁer h for a sample S =  x1, . . . , xm  is the minimum geometric margin over the points in the sample, ρh = mini∈[m] ρh xi , that is the distance of the hyperplane deﬁning h to the closest sample points.  The SVM solution is the separating hyperplane with the maximum geometric margin and is thus known as the maximum-margin hyperplane. The right panel of ﬁgure 5.1 illustrates the maximum-margin hyperplane returned by the SVM  w·x+b = 0  w·x+b = 0   5.2 Separable case  81  Figure 5.2 An illustration of the geometric margin of a point x in the case w · x > 0 and b > 0.  algorithm in the separable case. We will present later in this chapter a theory that provides a strong justiﬁcation for this solution. We can observe already, however, that the SVM solution can also be viewed as the “safest” choice in the following sense: a test point is classiﬁed correctly by a separating hyperplane with geometric margin ρ even when it falls within a distance ρ of the training samples sharing the same label; for the SVM solution, ρ is the maximum geometric margin and thus the “safest” value.  5.2.1 Primal optimization problem We now derive the equations and optimization problem that deﬁne the SVM so- lution. By deﬁnition of the geometric margin  see also ﬁgure 5.2 , the maximum margin ρ of a separating hyperplane is given by  ρ =  max  w,b : yi w·xi+b ≥0  min i∈[m]  w · xi + b   cid:107 w cid:107   = max w,b  min i∈[m]  yi cid:0 w · xi + b cid:1    cid:107 w cid:107   .   5.5   The second equality follows from the fact that, since the sample is linearly separable,  for the maximizing pair  w, b , yi cid:0 w · xi + b cid:1  must be non-negative for all i ∈ [m].  Now, observe that the last expression is invariant to multiplication of  w, b  by a positive scalar. Thus, we can restrict ourselves to pairs  w, b  scaled such that mini∈[m] yi w · xi + b  = 1:  ρ =  max w,b :  mini∈[m] yi w·xi+b =1  ∀i∈[m],yi w·xi+b ≥1  1  cid:107 w cid:107   =  max w,b :  .  1  cid:107 w cid:107    5.6   The second equality results from the fact that for the maximizing pair  w, b , the minimum of yi w · xi + b  is 1. Figure 5.3 illustrates the solution  w, b  of the maximization  5.6 . In addition to the maximum-margin hyperplane, it also shows the marginal hyperplanes, which are  w·x = 0  w·x+b = 0   0, 0   w  x  w·x kwk  b kwk   82  Chapter 5 Support Vector Machines  Figure 5.3 Maximum-margin hyperplane solution of  5.6 . The marginal hyperplanes are represented by dashed lines on the ﬁgure.  the hyperplanes parallel to the separating hyperplane and passing through the clos- est points on the negative or positive sides. Since they are parallel to the separating hyperplane, they admit the same normal vector w. Furthermore, since w·x+b = 1 for the closest points, the equations of the marginal hyperplanes are w· x + b = ±1. Since maximizing 1  cid:107 w cid:107  is equivalent to minimizing 1 2 cid:107 w cid:107 2, in view of  5.6 , the pair  w, b  returned by SVM in the separable case is the solution of the following convex optimization problem:  min w,b  1 2 cid:107 w cid:107 2  subject to: yi w · xi + b  ≥ 1, ∀i ∈ [m] .   5.7   The objective function F : w  cid:55 → 1 2 cid:107 w cid:107 2 is inﬁnitely diﬀerentiable. Its gradient is ∇F  w  = w and its Hessian is the identity matrix ∇2F  w  = I, whose eigenval- ues are strictly positive. Therefore, ∇2F  w   cid:31  0 and F is strictly convex. The constraints are all deﬁned by aﬃne functions gi :  w, b   cid:55 → 1 − yi w · xi + b  and are therefore qualiﬁed. Thus, in view of the results known for convex optimization  see appendix B for details , the optimization problem of  5.7  admits a unique solution, an important and favorable property that does not hold for all learning algorithms.  Moreover, since the objective function is quadratic and the constraints are aﬃne, the optimization problem of  5.7  is in fact a speciﬁc instance of quadratic program- ming  QP , a family of problems extensively studied in optimization. A variety of commercial and open-source solvers are available for solving convex QP problems. Additionally, motivated by the empirical success of SVMs along with its rich theo- retical underpinnings, specialized methods have been developed to more eﬃciently solve this particular convex QP problem, notably the block coordinate descent al- gorithms with blocks of just two coordinates.  w·x+b = 0  margin  w·x+b = +1  w·x+b =−1   5.2 Separable case  83  m cid:88 i=1 m cid:88 i=1  5.2.2 Support vectors Returning to the optimization problem  5.7 , we note that the constraints are aﬃne and thus qualiﬁed. The objective function as well as the aﬃne constraints are convex and diﬀerentiable. Thus, the requirements of theorem B.30 hold and the KKT conditions apply at the optimum. We shall use these conditions to both analyze the algorithm and demonstrate several of its crucial properties, and subsequently derive the dual optimization problem associated to SVMs in section 5.2.3. We introduce Lagrange variables αi ≥ 0, i ∈ [m], associated to the m constraints and denote by α the vector  α1, . . . , αm  cid:62 . The Lagrangian can then be deﬁned for all w ∈ RN , b ∈ R, and α ∈ Rm  + , by  L w, b, α  =  1 2 cid:107 w cid:107 2 −  αi[yi w · xi + b  − 1] .   5.8   m cid:88 i=1  The KKT conditions are obtained by setting the gradient of the Lagrangian with respect to the primal variables w and b to zero and by writing the complementarity conditions:  ∇wL = w −  αiyixi = 0  =⇒  w =  αiyixi   5.9    5.10   αiyi = 0  αiyi = 0  =⇒ =⇒  ∇bL = − αi = 0 ∨ yi w · xi + b  = 1.  5.11  ∀i, αi[yi w · xi + b  − 1] = 0 By equation  5.9 , the weight vector w at the solution of the SVM problem is a linear combination of the training set vectors x1, . . . , xm. A vector xi appears in that expansion iﬀ αi  cid:54 = 0. Such vectors are called support vectors. By the complementarity conditions  5.11 , if αi  cid:54 = 0, then yi w· xi + b  = 1. Thus, support vectors lie on the marginal hyperplanes w · xi + b = ±1. Support vectors fully deﬁne the maximum-margin hyperplane or SVM solution, which justiﬁes the name of the algorithm. By deﬁnition, vectors not lying on the marginal hyperplanes do not aﬀect the deﬁnition of these hyperplanes — in their absence, the solution to the SVM problem remains unchanged. Note that while the solution w of the SVM problem is unique, the support vectors are not. In dimension N , N + 1 points are suﬃcient to deﬁne a hyperplane. Thus, when more than N + 1 points lie on a marginal hyperplane, diﬀerent choices are possible for the N + 1 support vectors.  5.2.3 Dual optimization problem To derive the dual form of the constrained optimization problem  5.7 , we plug into the Lagrangian the deﬁnition of w in terms of the dual variables as expressed in  m cid:88 i=1  m cid:88 i=1   84  Chapter 5 Support Vector Machines   5.9  and apply the constraint  5.10 . This yields  L =  1 2 cid:107   m cid:88 i=1   cid:124   αiyixi cid:107 2 −  cid:80 m  − 1  2  which simpliﬁes to  i,j=1 αiαj yiyj  xi·xj    m cid:88 i=1  m cid:88 i=1  cid:124   0   cid:123  cid:122    cid:125    cid:125   αiαjyiyj xi · xj   −  αiyib  +  αi ,   5.12   L =  αi −  αiαjyiyj xi · xj  .   5.13   This leads to the following dual optimization problem for SVMs in the separable case:  max  α  αi −  αiαjyiyj xi · xj    5.14   m cid:88 i,j=1  cid:123  cid:122  m cid:88 i=1  1 2  m cid:88 i,j=1  m cid:88 i=1  1 2  m cid:88 i,j=1 m cid:88 i=1  subject to: αi ≥ 0 ∧  αiyi = 0, ∀i ∈ [m] .  i=1 αi − 1  2 cid:80 m  i,j=1 αiαjyiyj xi · xj  is inﬁnitely  The objective function G : α  cid:55 →  cid:80 m diﬀerentiable. Its Hessian is given by ∇2G = −A, with A = cid:0 yixi · yjxj cid:1 ij. A is  the Gram matrix associated to the vectors y1x1, . . . , ymxm and is therefore positive semideﬁnite  see section A.2.3 , which shows that ∇2G  cid:22  0 and that G is a concave function. Since the constraints are aﬃne and convex, the maximization problem  5.14  is a convex optimization problem. Since G is a quadratic function of α, this dual optimization problem is also a QP problem, as in the case of the primal optimization and once again both general-purpose and specialized QP solvers can be used to obtain the solution  see exercise 5.4 for details on the SMO algorithm, which is often used to solve the dual form of the SVM problem in the more general non-separable setting .  Moreover, since the constraints are aﬃne, they are qualiﬁed and strong duality holds  see appendix B . Thus, the primal and dual problems are equivalent, i.e., the solution α of the dual problem  5.14  can be used directly to determine the hypothesis returned by SVMs, using equation  5.9 :  h x  = sgn w · x + b  = sgn cid:16  m cid:88 i=1  αiyi xi · x  + b cid:17 .   5.15    5.2 Separable case  85   5.16    5.17    5.18    5.19   Since support vectors lie on the marginal hyperplanes, for any support vector xi, w · xi + b = yi, and thus b can be obtained via  b = yi −  αjyj xj · xi  .  m cid:88 j=1  The dual optimization problem  5.14  and the expressions  5.15  and  5.16  reveal an important property of SVMs: the hypothesis solution depends only on inner products between vectors and not directly on the vectors themselves. This obser- vation is key and its importance will become clear in Chapter 6 where we introduce kernel methods.  Equation  5.16  can now be used to derive a simple expression of the geometric margin ρ in terms of α. Since  5.16  holds for all i with αi  cid:54 = 0, multiplying both sides by αiyi and taking the sum leads to  m cid:88 i=1  αiyib =  αiy2  i −  αiαjyiyj xi · xj  .  m cid:88 i=1  m cid:88 i,j=1  Using the fact that y2  i = 1 along with equation  5.9  then yields  0 =  αi −  cid:107 w cid:107 2.  m cid:88 i=1  ρ2 =  1  cid:107 w cid:107 2  2  =  1 i=1 αi   cid:80 m  =  1  cid:107 α cid:107 1  .  Noting that αi ≥ 0, we obtain the following expression of the margin ρ in terms of the L1 norm of α:  5.2.4 Leave-one-out analysis We now use the notion of leave-one-out error to derive a ﬁrst learning guarantee for SVMs based on the fraction of support vectors in the training set.  Deﬁnition 5.2  Leave-one-out error  Let hS denote the hypothesis returned by a learn- ing algorithm A, when trained on a ﬁxed sample S. Then, the leave-one-out error of A on a sample S of size m is deﬁned by   cid:98 RLOO A  =  1 m  m cid:88 i=1  1hS−{xi} xi  cid:54 =yi.  Thus, for each i ∈ [m], A is trained on all the points in S except for xi, i.e., S−{xi}, and its error is then computed using xi. The leave-one-out error is the average of these errors. We will use an important property of the leave-one-out error stated in the following lemma.   86  Chapter 5 Support Vector Machines  Lemma 5.3 The average leave-one-out error for samples of size m ≥ 2 is an unbiased estimate of the average generalization error for samples of size m − 1:  where D denotes the distribution according to which points are drawn.  E  S∼Dm  [ cid:98 RLOO A ] =  [R hS cid:48  ],  S cid:48 ∼Dm−1   5.20   Proof: By the linearity of expectation, we can write  E  E  E  S∼Dm  [ cid:98 RLOO A ] =  1 m = E  m cid:88 i=1  [1hS−{xi} xi  cid:54 =yi]  S∼Dm [1hS−{x1} x1  cid:54 =y1] E  S∼Dm  [1hS cid:48   x1  cid:54 =y1] [1hS cid:48   x1  cid:54 =y1]]  =  =  =  E  S cid:48 ∼Dm−1,x1∼D [ E x1∼D [R hS cid:48  ].  S cid:48 ∼Dm−1  E  S cid:48 ∼Dm−1  For the second equality, we used the fact that, since the points of S are drawn in  an i.i.d. fashion, the expectation ES∼Dm [1hS−{xi} xi  cid:54 =yi ] does not depend on the  cid:3  choice of i ∈ [m] and is thus equal to ES∼Dm[1hS−{x1} x1  cid:54 =y1].  In general, computing the leave-one-out error may be costly since it requires training m times on samples of size m−1. In some situations however, it is possible to derive  the expression of  cid:98 RLOO A  much more eﬃciently  see exercise 11.9 .  Theorem 5.4 Let hS be the hypothesis returned by SVMs for a sample S, and let NSV  S  be the number of support vectors that deﬁne hS. Then,  E  S∼Dm  [R hS ] ≤  E  S∼Dm+1 cid:20  NSV S  m + 1  cid:21  .  Proof: Let S be a linearly separable sample of m + 1. If x is not a support vector for hS, removing it does not change the SVM solution. Thus, hS−{x} = hS and hS−{x} correctly classiﬁes x. By contraposition, if hS−{x} misclassiﬁes x, x must be a support vector, which implies   cid:98 RLOO SVM  ≤  NSV S  m + 1  .   5.21   cid:3   Taking the expectation of both sides and using lemma 5.3 yields the result.  Theorem 5.4 gives a sparsity argument in favor of SVMs: the average error of the algorithm is upper bounded by the average fraction of support vectors. One may hope that for many distributions seen in practice, a relatively small number of the training points will lie on the marginal hyperplanes. The solution will then be sparse in the sense that a small fraction of the dual variables αi will be non-   5.3 Non-separable case  87  Figure 5.4 A separating hyperplane with point xi classiﬁed incorrectly and point xj correctly classiﬁed, but with margin less than 1.  zero. Note, however, that this bound is relatively weak since it applies only to the average generalization error of the algorithm over all samples of size m. It provides no information about the variance of the generalization error. In section 5.4, we present stronger high-probability bounds using a diﬀerent argument based on the notion of margin.  5.3 Non-separable case  In most practical settings, the training data is not linearly separable, which implies that for any hyperplane w · x + b = 0, there exists xi ∈ S such that  yi [w · xi + b]  cid:54 ≥ 1 .  Thus, the constraints imposed in the linearly separable case discussed in section 5.2 cannot all hold simultaneously. However, a relaxed version of these constraints can indeed hold, that is, for each i ∈ [m], there exist ξi ≥ 0 such that  yi [w · xi + b] ≥ 1 − ξi .  The variables ξi are known as slack variables and are commonly used in optimization to deﬁne relaxed versions of constraints. Here, a slack variable ξi measures the distance by which vector xi violates the desired inequality, yi w · xi + b  ≥ 1. Figure 5.4 illustrates the situation. For a hyperplane w · x + b = 0, a vector xi with ξi > 0 can be viewed as an outlier . Each xi must be positioned on the correct side of the appropriate marginal hyperplane to not be considered an outlier. As a consequence, a vector xi with 0 < yi w · xi + b  < 1 is correctly classiﬁed by the hyperplane w · x + b = 0 but is nonetheless considered to be an outlier, that   5.22    5.23   w·x+b = 0  ξj  ξi  w·x+b =−1  w·x+b = +1   88  Chapter 5 Support Vector Machines  is, ξi > 0. If we omit the outliers, the training data is correctly separated by w · x + b = 0 with a margin ρ = 1  cid:107 w cid:107  that we refer to as the soft margin, as opposed to the hard margin in the separable case.  How should we select the hyperplane in the non-separable case? One idea consists of selecting the hyperplane that minimizes the empirical error. But, that solution will not beneﬁt from the large-margin guarantees we will present in section 5.4. Furthermore, the problem of determining a hyperplane with the smallest zero-one loss, that is the smallest number of misclassiﬁcations, is NP-hard as a function of the dimension N of the space.  total amount of slack due to outliers, which can be measured by cid:80 m generally by cid:80 m  Here, there are two conﬂicting objectives: on one hand, we wish to limit the i=1 ξi, or, more i for some p ≥ 1; on the other hand, we seek a hyperplane with a large margin, though a larger margin can lead to more outliers and thus larger amounts of slack.  i=1 ξp  5.3.1 Primal optimization problem This leads to the following general optimization problem deﬁning SVMs in the non-separable case where the parameter C ≥ 0 determines the trade-oﬀ between margin-maximization  or minimization of  cid:107 w cid:107 2  and the minimization of the slack  penalty cid:80 m  i=1 ξp i :  min w,b,ξ  1 2 cid:107 w cid:107 2 + C  ξp i  m cid:88 i=1  subject to yi w · xi + b  ≥ 1 − ξi ∧ ξi ≥ 0, i ∈ [m] ,  where ξ =  ξ1, . . . , ξm  cid:62 . The parameter C is typically determined via n-fold cross- validation  see section 4.5 .  i =  cid:107 ξ cid:107 p  any p ≥ 1. In particular, ξ  cid:55 → cid:80 m  As in the separable case,  5.24  is a convex optimization problem since the con- straints are aﬃne and thus convex and since the objective function is convex for p is convex in view of the convexity of  i=1 ξp  the norm  cid:107  ·  cid:107 p. There are many possible choices for p leading to more or less aggressive penaliza- tions of the slack terms  see exercise 5.1 . The choices p = 1 and p = 2 lead to the most straightforward solutions and analyses. The loss functions associated with p = 1 and p = 2 are called the hinge loss and the quadratic hinge loss, respectively. Figure 5.5 shows the plots of these loss functions as well as that of the standard zero-one loss function. Both hinge losses are convex upper bounds on the zero-one loss, thus making them well suited for optimization. In what follows, the analysis is presented in the case of the hinge loss  p = 1 , which is the most widely used loss function for SVMs.   5.24    5.3 Non-separable case  89  Figure 5.5 Both the hinge loss and the quadratic hinge loss provide convex upper bounds on the binary zero-one loss.  5.3.2 Support vectors As in the separable case, the constraints are aﬃne and thus qualiﬁed. The objective function as well as the aﬃne constraints are convex and diﬀerentiable. Thus, the hypotheses of theorem B.30 hold and the KKT conditions apply at the optimum. We use these conditions to both analyze the algorithm and demonstrate several of its crucial properties, and subsequently derive the dual optimization problem associated to SVMs in section 5.3.3. We introduce Lagrange variables αi ≥ 0, i ∈ [m], associated to the ﬁrst m constraints and βi ≥ 0, i ∈ [m] associated to the non-negativity constraints of the slack variables. We denote by α the vector  α1, . . . , αm  cid:62  and by β the vector  β1, . . . , βm  cid:62 . The Lagrangian can then be deﬁned for all w ∈ RN , b ∈ R, and ξ, α, β ∈ Rm L w, b, ξ, α, β  =  αi[yi w·xi +b −1+ξi]−  1 2 cid:107 w cid:107 2 +C  βiξi .  5.25   ξi−  + , by  m cid:88 i=1  m cid:88 i=1  m cid:88 i=1  The KKT conditions are obtained by setting the gradient of the Lagrangian with respect to the primal variables w, b, and ξis to zero and by writing the complemen-   90  Chapter 5 Support Vector Machines  tarity conditions:  m cid:88 i=1 m cid:88 i=1  m cid:88 i=1  m cid:88 i=1  ∇wL = w −  αiyixi = 0  =⇒ w =  αiyixi   5.26    5.27   αiyi = 0  αiyi = 0  =⇒ =⇒ αi + βi = C  5.28  =⇒ αi = 0 ∨ yi w · xi + b  = 1 − ξi  5.29  =⇒ βi = 0 ∨ ξi = 0 .  5.30   ∇bL = − ∇ξiL = C − αi − βi = 0 ∀i, αi[yi w · xi + b  − 1 + ξi] = 0 ∀i, βiξi = 0 By equation  5.26 , as in the separable case, the weight vector w at the solution of the SVM problem is a linear combination of the training set vectors x1, . . . , xm. A vector xi appears in that expansion iﬀ αi  cid:54 = 0. Such vectors are called support vectors. Here, there are two types of support vectors. By the complementarity condition  5.29 , if αi  cid:54 = 0, then yi w·xi+b  = 1−ξi. If ξi = 0, then yi w·xi+b  = 1 and xi lies on a marginal hyperplane, as in the separable case. Otherwise, ξi  cid:54 = 0 and xi is an outlier. In this case,  5.30  implies βi = 0 and  5.28  then requires αi = C. Thus, support vectors xi are either outliers, in which case αi = C, or vectors lying on the marginal hyperplanes. As in the separable case, note that while the weight vector w solution is unique, the support vectors are not.  5.3.3 Dual optimization problem To derive the dual form of the constrained optimization problem  5.24 , we plug into the Lagrangian the deﬁnition of w in terms of the dual variables  5.26  and apply the constraint  5.27 . This yields  L =  1 2 cid:107   m cid:88 i=1   cid:124   αiyixi cid:107 2 −  cid:80 m  − 1  2  i,j=1 αiαj yiyj  xi·xj    m cid:88 i=1  m cid:88 i=1  cid:124   0   cid:123  cid:122    cid:125    cid:125   αiαjyiyj xi · xj   −  αiyib  +  αi .   5.31   Remarkably, we ﬁnd that the objective function is no diﬀerent than in the separable case:  L =  αi −  αiαjyiyj xi · xj  .   5.32   m cid:88 i,j=1  cid:123  cid:122  m cid:88 i=1  1 2  m cid:88 i,j=1  However, here, in addition to αi ≥ 0, we must impose the constraint on the Lagrange variables βi ≥ 0. In view of  5.28 , this is equivalent to αi ≤ C. This leads to the following dual optimization problem for SVMs in the non-separable case, which   5.4 Margin theory  91  only diﬀers from that of the separable case  5.14  by the constraints αi ≤ C:  max  α  αi −  1 2  m cid:88 i=1  m cid:88 i,j=1 m cid:88 i=1  subject to: 0 ≤ αi ≤ C ∧  αiyi = 0, i ∈ [m].  αiαjyiyj xi · xj    5.33   Thus, our previous comments about the optimization problem  5.14  apply to  5.33  as well. In particular, the objective function is concave and inﬁnitely diﬀerentiable and  5.33  is equivalent to a convex QP. The problem is equivalent to the primal problem  5.24 .  The solution α of the dual problem  5.33  can be used directly to determine the  hypothesis returned by SVMs, using equation  5.26 :  h x  = sgn w · x + b  = sgn cid:16  m cid:88 i=1  αiyi xi · x  + b cid:17 .  Moreover, b can be obtained from any support vector xi lying on a marginal hyper- plane, that is any vector xi with 0 < αi < C. For such support vectors, w·xi+b = yi and thus   5.34    5.35   b = yi −  αjyj xj · xi  .  m cid:88 j=1  As in the separable case, the dual optimization problem  5.33  and the expressions  5.34  and  5.35  show an important property of SVMs: the hypothesis solution depends only on inner products between vectors and not directly on the vectors themselves. This fact can be used to extend SVMs to deﬁne non-linear decision boundaries, as we shall see in chapter 6.  5.4 Margin theory  This section presents generalization bounds which provide a strong theoretical jus- tiﬁcation for the SVM algorithm.  Recall that the VC-dimension of the family of hyperplanes or linear hypotheses in RN is N + 1. Thus, the application of the VC-dimension bound  3.29  of corol- lary 3.19 to this hypothesis set yields the following: for any δ > 0, with probability at least 1 − δ, for any h ∈ H,  R h  ≤  cid:98 RS h  + cid:115  2 N + 1  log em  m  N +1  + cid:115  log 1  δ 2m  .   5.36    92  Chapter 5 Support Vector Machines  When the dimension of the feature space N is large compared to the sample size m, this bound is uninformative. Remarkably, the learning guarantees presented in this section are independent of the dimension N and thus hold regardless of its value. The guarantees we will present hold for real-valued functions such as the function x  cid:55 → w · x + b returned by SVMs, as opposed to classiﬁcation functions returning +1 or −1, such as x  cid:55 → sgn w · x + b . They are based on the notion of conﬁdence margin. The conﬁdence margin of a real-valued function h at a point x labeled with y is the quantity yh x . Thus, when yh x  > 0, h classiﬁes x correctly but we interpret the magnitude of h x  as the conﬁdence of the prediction made by h. The notion of conﬁdence margin is distinct from that of geometric margin and does not require a linear separability assumption. But, the two notions are related as follows in the separable case: for h : x  cid:55 → w · x + b with geometric margin ρgeom, the conﬁdence margin at any point x of the training sample with label y is at least ρgeom cid:107 w cid:107 , i.e. yh x  ≥ ρgeom cid:107 w cid:107 . In view of the deﬁnition of the conﬁdence margin, for any parameter ρ > 0, we will deﬁne a ρ-margin loss function that, as with the zero-one loss, penalizes h with the cost of 1 when it misclassiﬁes point x  yh x  ≤ 0 , but also penalizes h  linearly  when it correctly classiﬁes x with conﬁdence less than or equal to ρ  yh x  ≤ ρ . The main margin-based generalization bounds of this section are presented in terms of this loss function, which is formally deﬁned as follows.  Deﬁnition 5.5  Margin loss function  For any ρ > 0, the ρ-margin loss is the function  Lρ : R × R → R+ deﬁned for all y, y cid:48  ∈ R by Lρ y, y cid:48   = Φρ yy cid:48   with,  Φρ x  = min cid:18 1, max cid:16 0, 1 −  x  ρ cid:17  cid:19  =  1 1 − x 0  ρ  if x ≤ 0 if 0 ≤ x ≤ ρ if ρ ≤ x.  This loss function is illustrated by ﬁgure 5.6. The parameter ρ > 0 can be in- terpreted as the conﬁdence margin demanded from a hypothesis h. The empirical margin loss is similarly deﬁned as the margin loss over the training sample.  Deﬁnition 5.6  Empirical margin loss  Given a sample S =  x1, . . . , xm  and a hypoth- esis h, the empirical margin loss is deﬁned by   cid:98 RS,ρ h  =  1 m  m cid:88 i=1  Φρ yih xi   .   5.37   Note that, for any i ∈ [m], Φρ yih xi   ≤ 1yih xi ≤ρ. Thus, the empirical margin loss can be upper-bounded as follows:   cid:98 RS,ρ h  ≤  1 m  m cid:88 i=1  1yih xi ≤ρ .   5.38    5.4 Margin theory  93  Figure 5.6 The margin loss illustrated in red, deﬁned with respect to margin parameter ρ = 0.7.  In all the results that follow, the empirical margin loss can be replaced by this upper bound, which admits a simple interpretation: it is the fraction of the points in the training sample S that have been misclassiﬁed or classiﬁed with conﬁdence less than ρ. In other words, the upper bound is then the fraction of the points in the training data with margin less than ρ. This corresponds to the loss function indicated by the blue dotted line in ﬁgure 5.6.  A key beneﬁt of using a loss function based on Φρ as opposed to the zero-one loss or the loss deﬁned by the blue dotted line of ﬁgure 5.6 is that Φρ is 1 ρ-Lipschitz, since the absolute value of the slope of the function is at most 1 ρ. The following lemma bounds the empirical Rademacher complexity of a hypothesis set H after composition with such a Lipschitz function in terms of the empirical Rademacher complexity of H. It will be needed for the proof of the margin-based generalization bound. Lemma 5.7  Talagrand’s lemma  Let Φ1, . . . , Φm be l-Lipschitz functions from R to R and σ1, . . . , σm be Rademacher random variables. Then, for any hypothesis set H of real-valued functions, the following inequality holds:  1 m  E  σ cid:104  sup  h∈H  m cid:88 i=1  σi Φi ◦ h  xi   cid:105  ≤  σih xi  cid:105  = l cid:98 RS H  .  In particular, if Φi = Φ for all i ∈ [m], then the following holds:  E  l m  h∈H  σ cid:104  sup m cid:88 i=1  cid:98 RS Φ ◦ H  ≤ l cid:98 RS H  . σm cid:104  sup  σ1,...,σm−1 cid:104  E  1 m  E  h∈H  Proof: First we ﬁx a sample S =  x1, . . . , xm , then, by deﬁnition,  1 m  E  σ cid:104  sup  h∈H  m cid:88 i=1  σi Φm ◦ h  xi  cid:105  =  um−1 h  + σm Φm ◦ h  xm  cid:105  cid:105 ,   94  Chapter 5 Support Vector Machines  i=1 σi Φi ◦ h  xi . By deﬁnition of the supremum, for any  um−1 h  +  Φm ◦ h  xm  cid:105  um−1 h  −  Φm ◦ h  xm  cid:105 .   cid:15  > 0, there exist h1, h2 ∈ H such that  where um−1 h  =  cid:80 m−1 um−1 h1  +  Φm ◦ h1  xm  ≥  1 −  cid:15   cid:104  sup and um−1 h2  −  Φm ◦ h2  xm  ≥  1 −  cid:15   cid:104  sup Thus, for any  cid:15  > 0, by deﬁnition of Eσm,  1 −  cid:15   E =  1 −  cid:15   cid:20  1  σm cid:104  sup um−1 h  + σm Φm ◦ h  xm  cid:105  h∈H cid:104 um−1 h  +  Φm ◦ h  xm  cid:105  +  h∈H sup  2  1  h∈H  h∈H  2 cid:104  sup  h∈H  um−1 h  −  Φm ◦ h  xm  cid:105  cid:21   1 2  h∈H  σm cid:104  sup  [um−1 h2  −  Φm ◦ h2  xm ].  [um−1 h1  +  Φm ◦ h1  xm ] +  ≤ Let s = sgn h1 xm  − h2 xm  . Then, the previous inequality implies  1 −  cid:15   E ≤ =  um−1 h  + σm Φm ◦ h  xm  cid:105   [um−1 h1  + um−1 h2  + sl h1 xm  − h2 xm  ] [um−1 h1  + slh1 xm ] + sup h∈H  1 2 [um−1 h  + slh xm ] +  [um−1 h2  − slh2 xm ] 1 2  [um−1 h  − slh xm ]  sup h∈H  1 2  1 2 1 2 1 ≤ 2 = E  h∈H  Since the inequality holds for all  cid:15  > 0, we have  σm cid:104  sup σm cid:104  sup  um−1 h  + σmlh xm  cid:105 . um−1 h  + σm Φm ◦ h  xm  cid:105  ≤ E  h∈H  E   Lipschitz property    rearranging    deﬁnition of sup    deﬁnition of E     σm  σm cid:104  sup  h∈H  um−1 h  + σmlh xm  cid:105 .   cid:3   Proceeding in the same way for all other σi  i  cid:54 = m  proves the lemma. The following is a general margin-based generalization bound that will be used  in the analysis of several algorithms.  Theorem 5.8  Margin bound for binary classiﬁcation  Let H be a set of real-valued func- tions. Fix ρ > 0, then, for any δ > 0, with probability at least 1 − δ, each of the following holds for all h ∈ H:  R h  ≤  cid:98 RS,ρ h  + R h  ≤  cid:98 RS,ρ h  +  2 ρ  δ 2m  Rm H  + cid:115  log 1 ρ cid:98 RS H  + 3 cid:115  log 2  δ 2m  2  .   5.39    5.40    5.4 Margin theory  95  ,  1 m  δ 2m  E[g z ] ≤  taking values in [0, 1]:  and thus, for all h ∈ H,  Proof: Let  cid:101 H = {z =  x, y   cid:55 → yh x  : h ∈ H}. Consider the family of functions  cid:101 H = {Φρ ◦ f : f ∈  cid:101 H} . By theorem 3.3, with probability at least 1 − δ, for all g ∈  cid:101 H, g zi  + 2Rm  cid:101 H  + cid:115  log 1 m cid:88 i=1 E[Φρ yh x  ] ≤  cid:98 RS,ρ h  + 2Rm cid:0 Φρ ◦  cid:101 H cid:1  + cid:115  log 1 R h  ≤  cid:98 RS,ρ h  + 2Rm cid:0 Φρ ◦  cid:101 H cid:1  + cid:115  log 1 S,σ cid:104  sup m cid:88 i=1  Since Φρ is 1 ρ-Lipschitz, by lemma 5.7, we have Rm cid:0 Φρ ◦  cid:101 H cid:1  ≤ 1 Rm  cid:101 H  can be rewritten as follows: Rm  cid:101 H  =  Since 1u≤0 ≤ Φρ u  for all u ∈ R, we have R h  = E[1yh x ≤0] ≤ E[Φρ yh x  ], thus  ρ Rm  cid:101 H  and σih xi  cid:105  = Rm cid:0 H cid:1  .  This proves  5.39 . The second inequality,  5.40 , can be derived in the same way  cid:3  by using the second inequality of theorem 3.3,  3.4 , instead of  3.3 .  σiyih xi  cid:105  =  S,σ cid:104  sup  m cid:88 i=1  δ 2m  δ 2m  h∈H  h∈H  1 m  1 m  E  E  .  .  The generalization bounds of theorem 5.8 suggest a trade-oﬀ: a larger value of ρ decreases the complexity term  second term , but tends to increase the empirical  margin-loss  cid:98 RS,ρ h   ﬁrst term  by requiring from a hypothesis h a higher conﬁdence  margin. Thus, if for a relatively large value of ρ the empirical margin loss of h remains relatively small, then h beneﬁts from a very favorable guarantee on its generalization error. For theorem 5.8, the margin parameter ρ must be selected beforehand. But, the bounds of the theorem can be generalized to hold uniformly  for all ρ ∈  0, 1] at the cost of a modest additional term cid:113  log log2  , as shown in the following theorem  a version of this theorem with better constants can be derived, see exercise 5.2 .  m  2 ρ   96  Chapter 5 Support Vector Machines  Theorem 5.9 Let H be a set of real-valued functions. Fix r > 0. Then, for any δ > 0, with probability at least 1 − δ, each of the following holds for all h ∈ H and ρ ∈  0, r]:  4 ρ  Rm H  + cid:115  log log2 ρ cid:98 RS H  + cid:115  log log2  m  m  4  2r ρ  2r ρ  δ 2m  + cid:115  log 2 + 3 cid:115  log 4  δ 2m   5.41   .   5.42   R h  ≤  cid:98 RS,ρ h  + R h  ≤  cid:98 RS,ρ h  + P cid:20  sup  h∈H  R h  −  cid:98 RS,ρk  h  >  Proof: Consider two sequences  ρk k≥1 and   cid:15 k k≥1, with  cid:15 k ∈  0, 1]. By theo- rem 5.8, for any ﬁxed k ≥ 1,  2 ρk  Rm H  +  cid:15 k cid:21  ≤ exp −2m cid:15 2  k .   5.43   m , then, by the union bound, the following holds:  2 ρk  h∈H k≥1  Choose  cid:15 k =  cid:15  + cid:113  log k P sup R h  −  cid:98 RS,ρk  h  − ≤ cid:88 k≥1 = cid:88 k≥1 ≤ cid:88 k≥1 = cid:0  cid:88 k≥1  =  π2 6  exp −2m cid:15 2 k   Rm H  −  cid:15 k > 0 exp cid:2  − 2m  cid:15  + cid:112  log k  m 2 cid:3  exp −2m cid:15 2  exp −2 log k  1 k2 cid:1  exp −2m cid:15 2  exp −2m cid:15 2  ≤ 2 exp −2m cid:15 2 .  We can choose ρk = r 2k. For any ρ ∈  0, r], there exists k ≥ 1 such that ρ ∈  ρk, ρk−1], with ρ0 = r. For that k, ρ ≤ ρk−1 = 2ρk, thus 1 ρk ≤ 2 ρ and √log k =  cid:112 log log2 r ρk  ≤  cid:112 log log2 2r ρ . Furthermore, for any h ∈ H,  cid:98 RS,ρk  h  ≤  cid:98 RS,ρ h . Thus, the following inequality holds: −  cid:15  > 0 ≤ 2 exp −2m cid:15 2 , P sup  Rm H  − cid:114  log log2 2r ρ   R h  −  cid:98 RS,ρ h  −  which proves the ﬁrst statement. The second statement can be proven in a similar  cid:3  way.  h∈H ρ∈ 0,r]  4 ρ  m   5.4 Margin theory  97   cid:98 RS H  =  ≤  The Rademacher complexity of linear hypotheses with bounded weight vector can  be bounded as follows. Theorem 5.10 Let S ⊆ {x :  cid:107 x cid:107  ≤ r} be a sample of size m and let H = {x  cid:55 → w· x :  cid:107 w cid:107  ≤ Λ}. Then, the empirical Rademacher complexity of H can be bounded as follows:  Proof: The proof follows through a series of inequalities:  .  m  E   cid:98 RS H  ≤ cid:114  r2Λ2 σiw · xi cid:21  = σ cid:20  sup m cid:88 i=1 2 cid:21  cid:21  1 m cid:20  E σixi cid:13  cid:13  cid:13  cid:21  ≤ σ cid:20  cid:13  cid:13  cid:13  σixi cid:13  cid:13  cid:13  m cid:88 i=1  cid:107 xi cid:107 2 cid:21  1 m cid:20  m cid:88 i=1 σiσj xi · xj  cid:105  cid:21  1   cid:107 w cid:107 ≤Λ  w ·  1 m  ≤  Λ  Λ  2  2  2   cid:107 w cid:107 ≤Λ  E  E  Λ m  1 m  σ cid:20  sup σ cid:20  cid:13  cid:13  cid:13  m cid:88 i=1 m cid:20  E σ cid:104  m cid:88 i,j=1  σixi cid:21   m cid:88 i=1  Λ  =  Λ√mr2  The ﬁrst inequality makes use of the Cauchy-Schwarz inequality and the bound on  = cid:114  r2Λ2  cid:107 w cid:107 , the second follows by Jensen’s inequality, the third by E[σiσj] = E[σi] E[σj] =  cid:3  0 for i  cid:54 = j, and the last one by  cid:107 xi cid:107  ≤ r. Combining theorem 5.10 and theorem 5.8 gives directly the following general margin bound for linear hypotheses with bounded weight vectors, presented in corollary 5.11.  ≤  m  m  ,  Corollary 5.11 Let H = {x  cid:55 → w · x :  cid:107 w cid:107  ≤ Λ} and assume that X ⊆ {x :  cid:107 x cid:107  ≤ r}. Fix ρ > 0, then, for any δ > 0, with probability at least 1 − δ over the choice of a sample S of size m, the following holds for any h ∈ H:  R h  ≤  cid:98 RS,ρ h  + 2 cid:114  r2Λ2 ρ2  m  + cid:115  log 1  δ 2m  .   5.44   As with theorem 5.8, the bound of this corollary can be generalized to hold uni-  formly for all ρ ∈  0, 1] at the cost of an additional term cid:113  log log2  theorems 5.10 and 5.9.  m  by combining  2 ρ  This generalization bound for linear hypotheses is remarkable, since it does not depend directly on the dimension of the feature space, but only on the margin. It suggests that a small generalization error can be achieved when ρ  rΛ  is large  small second term  while the empirical margin loss is relatively small  ﬁrst term . The latter occurs when few points are either classiﬁed incorrectly or correctly, but with margin less than ρ. When the training sample is linearly separable, for a linear hypothesis with geometric margin ρgeom and the choice of the conﬁdence margin   98  Chapter 5 Support Vector Machines  parameter ρ = ρgeom, the empirical margin loss term is zero. Thus, if ρgeom is relatively large, this provides a strong guarantee for the generalization error of the corresponding linear hypothesis.  The fact that the guarantee does not explicitly depend on the dimension of the feature space may seem surprising and appears to contradict the VC-dimension lower bounds of theorems 3.20 and 3.23. Those lower bounds show that for any learning algorithm A there exists a bad distribution for which the error of the The bound of the corollary does not rule out such bad cases, however: for such bad distributions, the empirical margin loss would be large even for a relatively small margin ρ, and thus the bound of the corollary would be loose in that case.  hypothesis returned by the algorithm is Ω  cid:112 d m  with a non-zero probability.  Thus, in some sense, the learning guarantee of the corollary hinges upon the hope of a good margin value ρ: if there exists a relatively large margin value ρ > 0 for which the empirical margin loss is small, then a small generalization error is guaranteed by the corollary. This favorable margin situation depends on the distribution: while the learning bound is distribution-independent, the existence of a good margin is in fact distribution-dependent. A favorable margin seems to appear relatively often in applications.  The bound of the corollary gives a strong justiﬁcation for margin-maximization algorithms such as SVMs. Choosing Λ = 1, by the generalization of corollary 5.11 to a uniform bound over ρ ∈  0, r], for any δ > 0, with probability at least 1 − δ,  the following holds for all h ∈ cid:8 x  cid:55 → w · x :  cid:107 w cid:107  ≤ 1 cid:9  and ρ ∈  0, r]: + cid:115  log 2  + cid:115  log log2  δ 2m  2r ρ  m  m  .  The inequality also trivially holds for ρ larger than r since in that case, by the Cauchy-Schwarz inequality, for any w with  cid:107 w cid:107  ≤ 1, we have yi w · xi  ≤ r ≤ ρ  R h  ≤  cid:98 RS,ρ h  + 4 cid:114  r2 ρ2 and  cid:98 RS,ρ h  is equal to one for all h.  loss:  Now, for any ρ > 0, the ρ-margin loss function is upper bounded by the ρ-hinge  ∀u ∈ R, Φρ u  = min cid:18 1, max cid:18 0, 1 −  u  ρ cid:19  cid:19  ≤ max cid:18 0, 1 −  u  ρ cid:19  .  Thus, with probability at least 1 − δ, the following holds for all h ∈  cid:8 x  cid:55 → w · x :  cid:107 w cid:107  ≤ 1 cid:9  and all ρ > 0: + cid:115  log 2 max cid:18 0, 1 −   cid:19  + 4 cid:114  r2 ρ2  + cid:115  log log2  yi w · xi   R h  ≤  δ 2m  1 m  2r ρ  m  m  ρ  .  m cid:88 i=1   5.45    5.4 Margin theory  99  Since for any ρ > 0, h ρ admits the same generalization error as h, with probability  at least 1− δ, the following holds for all h ∈ cid:8 x  cid:55 → w· x :  cid:107 w cid:107  ≤ 1 ρ cid:9  and all ρ > 0:  + cid:115  log log2  m  2r ρ  + cid:115  log 2  δ 2m  .  5.46   R h  ≤  1 m  m cid:88 i=1  max cid:0 0, 1− yi w· xi  cid:1  + 4 cid:114  r2 ρ2  m  This inequality can be used to derive an algorithm that selects w and ρ > 0 to minimize the right-hand side. The minimization with respect to ρ does not lead to a convex optimization and depends on theoretical constant factors aﬀecting the second and third terms, which may not be optimal. Thus, instead, ρ is left as a free parameter of the algorithm, typically determined via cross-validation.  Now, since only the ﬁrst term of the right-hand side depends on w, for any ρ > 0, the bound suggests selecting w as the solution of the following optimization problem:  Introducing a Lagrange variable λ ≥ 0, the optimization problem can be written equivalently as  min  cid:107 w cid:107 2≤ 1  ρ2  1 m  m cid:88 i=1  max cid:0 0, 1 − yi w · xi  cid:1  .  min  w  λ cid:107 w cid:107 2 +  1 m  m cid:88 i=1  max cid:0 0, 1 − yi w · xi  cid:1  .   5.47    5.48   Since for any choice of ρ in the constraint of  5.47  there exists an equivalent dual variable λ in the formulation of  5.48  that achieves the same optimal w, λ can be freely selected via cross-validation.5 The resulting algorithm precisely coincides with SVMs. Note that an alternative objective function and thus algorithm would be based on the empirical margin loss instead of the hinge loss. However, the advantage of the hinge loss is that it is convex, while the margin loss is not.  As already pointed out, the bounds just discussed do not directly depend on the dimension of the feature space but guarantee good generalization when given a favorable margin. Thus, they suggest seeking large-margin separating hyperplanes in a very high-dimensional space. In view of the form of the dual optimization problems for SVMs, determining the solution of the optimization and using it for prediction both require computing many inner products in that space. For very high-dimensional spaces, the computation of these inner products could become very costly. The next chapter provides a solution to this problem which further provides a generalization of SVMs to non-vectorial input spaces.  5 An equivalent analysis consists of choosing ρ = 1  cid:107 w cid:107  in  5.46 .   100  Chapter 5 Support Vector Machines  5.5 Chapter notes  The maximum-margin or optimal hyperplane solution described in section 5.2 was introduced by Vapnik and Chervonenkis [1964]. The algorithm had limited applica- tions since in most tasks in practice the data is not linearly separable. In contrast, the SVM algorithm of section 5.3 for the general non-separable case, introduced by Cortes and Vapnik [1995] under the name support-vector networks, has been widely adopted and been shown to be eﬀective in practice. The algorithm and its theory have had a profound impact on theoretical and applied machine learning and inspired research on a variety of topics. Several specialized algorithms have been suggested for solving the speciﬁc QP that arises when solving the SVM problem, for example the SMO algorithm of Platt [1999]  see exercise 5.4  and a variety of other decomposition methods such as those used in the LibLinear software library [Hsieh et al., 2008], and [Allauzen et al., 2010] for solving the problem when using rational kernels  see chapter 6 .  Much of the theory supporting the SVM algorithm  [Cortes and Vapnik, 1995, Vapnik, 1998] , in particular the margin theory presented in section 5.4, has been adopted in the learning theory and statistics communities and applied to a variety of other problems. The margin bound on the VC-dimension of canonical hyper- planes  exercise 5.7  is by Vapnik [1998], the proof is very similar to Novikoﬀ’s margin bound on the number of updates made by the Perceptron algorithm in the separable case. Our presentation of margin guarantees based on the Rademacher complexity follows the elegant analysis of Koltchinskii and Panchenko [2002]  see also Bartlett and Mendelson [2002], Shawe-Taylor et al. [1998] . Our proof of Ta- lagrand’s lemma 5.7 is a simpler and more concise version of a more general result given by Ledoux and Talagrand [1991, pp. 112–114]. See H¨oﬀgen et al. [1995] for hardness results related to the problem of ﬁnding a hyperplane with the minimal number of errors on a training sample.  5.6 Exercises  5.1 Soft margin hyperplanes. The function of the slack variables used in the op- i=1 ξi.  timization problem for soft margin hyperplanes has the form: ξ  cid:55 →  cid:80 m Instead, we could use ξ  cid:55 → cid:80 m  i , with p > 1.  i=1 ξp   a  Give the dual formulation of the problem in this general case.   b  How does this more general formulation  p > 1  compare to the standard  setting  p = 1 ? In the case p = 2 is the optimization still convex?   5.6 Exercises  101  5.2 Tighter Rademacher Bound. Derive the following tighter version of the bound of theorem 5.9: for any δ > 0, with probability at least 1 − δ, for all h ∈ H and ρ ∈  0, 1] the following holds:  Rm H  + cid:115  log logγ  m  2γ ρ  γ ρ  + cid:115  log 2  δ 2m   5.49   R h  ≤  cid:98 RS,ρ h  +  for any γ > 1.  5.3 Importance weighted SVM. Suppose you wish to use SVMs to solve a learning problem where some training data points are more important than others. More formally, assume that each training point consists of a triplet  xi, yi, pi , where 0 ≤ pi ≤ 1 is the importance of the ith point. Rewrite the primal SVM con- strained optimization problem so that the penalty for mis-labeling a point xi is scaled by the priority pi. Then carry this modiﬁcation through the derivation of the dual solution.  5.4 Sequential minimal optimization  SMO . The SMO algorithm is an optimiza- tion algorithm introduced to speed up the training of SVMs. SMO reduces a  potentially  large quadratic programming  QP  optimization problem into a series of small optimizations involving only two Lagrange multipliers. SMO re- duces memory requirements, bypasses the need for numerical QP optimization and is easy to implement. In this question, we will derive the update rule for the SMO algorithm in the context of the dual formulation of the SVM problem.   a  Assume that we want to optimize equation  5.33  only over α1 and α2. Show  that the optimization problem reduces to  max α1,α2  α1 + α2 −  1 2  K11α2  1 −  1 2  K22α2  2 − sK12α1α2 − y1α1v1 − y2α2v2   cid:124   subject to: 0 ≤ α1, α2 ≤ C ∧ α1 + sα2 = γ ,  Ψ1 α1,α2    cid:123  cid:122    cid:125   i=3 yiαi, s = y1y2 ∈ {−1, +1}, Kij =  xi · xj  and vi =  where γ = y1 cid:80 m  cid:80 m  j=3 αjyjKij for i = 1, 2.   b  Substitute the linear constraint α1 = γ − sα2 into Ψ1 to obtain a new objec- tive function Ψ2 that depends only on α2. Show that the α2 that minimizes Ψ2  without the constraints 0 ≤ α1, α2 ≤ C  can be expressed as  α2 =  s K11 − K12 γ + y2 v1 − v2  − s + 1  ,  η  where η = K11 + K22 − 2K12.   102  Chapter 5 Support Vector Machines   c  Show that  v1 − v2 = f  x1  − f  x2  + α∗2y2η − sy2γ K11 − K12   where f  x  =  cid:80 m  i=1 α∗i yi xi · x  + b∗ and α∗i are values for the Lagrange multipliers prior to optimization over α1 and α2  similarly, b∗ is the previous value for the oﬀset .   d  Show that  α2 = α∗2 + y2   y2 − f  x2   −  y1 − f  x1    .  η   e  For s = +1, deﬁne L = max{0, γ − C} and H = min{C, γ} as the lower and upper bounds on α2. Similarly, for s = −1, deﬁne L = max{0,−γ} and H = min{C, C − γ}. The update rule for SMO involves “clipping” the value of α2, i.e.,  αclip  2 =  if L < α2 < H α2 if α2 ≤ L L H if α2 ≥ H  .  We subsequently solve for α1 such that we satisfy the equality constraint, resulting in α1 = α∗1 + s α∗2 − αclip  . Why is “clipping” is required? How are L and H derived for the case s = +1?  2  5.5 SVMs hands-on.   a  Download and install the libsvm software library from:  http:  www.csie.ntu.edu.tw ~cjlin libsvm .   b  Download the satimage data set found at:  http:  www.csie.ntu.edu.tw ~cjlin libsvmtools datasets .  Merge the training and validation sets into one. We will refer to the resulting set as the training set from now on. Normalize both the training and test vectors.   c  Consider the binary classiﬁcation that consists of distinguishing class 6 from the rest of the data points. Use SVMs combined with polynomial kernels  see chapter 6  to solve this classiﬁcation problem. To do so, randomly split the training data into ten equal-sized disjoint sets. For each value of the polynomial degree, d = 1, 2, 3, 4, plot the average cross-validation error plus or minus one standard deviation as a function of C  let the other parameters of polynomial kernels in libsvm, γ and c, be equal to their default values 1 .   5.6 Exercises  103  Report the best value of the trade-oﬀ constant C measured on the validation set.   d  Let  C∗, d∗  be the best pair found previously. Fix C to be C∗. Plot the ten-fold cross-validation training and test errors for the hypotheses obtained as a function of d. Plot the average number of support vectors obtained as a function of d.   e  How many of the support vectors lie on the margin hyperplanes?   f  In the standard two-group classiﬁcation, errors on positive or negative points are treated in the same manner. Suppose, however, that we wish to penalize an error on a negative point  false positive error  k > 0 times more than an error on a positive point. Give the dual optimization problem corresponding to SVMs modiﬁed in this way.   g  Assume that k is an integer. Show how you can use libsvm without writing any additional code to ﬁnd the solution of the modiﬁed SVMs just described.   h  Apply the modiﬁed SVMs to the classiﬁcation task previously examined and  compare with your previous SVMs results for k = 2, 4, 8, 16.  5.6 Sparse SVM. One can give two types of arguments in favor of the SVM algo- rithm: one based on the sparsity of the support vectors, another based on the notion of margin. Suppose that instead of maximizing the margin, we choose instead to maximize sparsity by minimizing the Lp norm of the vector α that deﬁnes the weight vector w, for some p ≥ 1. First, consider the case p = 2. This gives the following optimization problem:  α2  i + C  ξi  m cid:88 i=1  1 2  min α,b  m cid:88 i=1 subject to yi cid:16  m cid:88 j=1  ξi, αi ≥ 0, i ∈ [m].  αjyjxi · xj + b cid:17  ≥ 1 − ξi, i ∈ [m]   5.50    a  Show that modulo the non-negativity constraint on α, the problem coincides  with an instance of the primal optimization problem of SVM.   b  Derive the dual optimization of problem of  5.50 .   c  Setting p = 1 will induce a more sparse α. Derive the dual optimization in  this case.  5.7 VC-dimension of canonical hyperplanes. The objective of this problem is derive a bound on the VC-dimension of canonical hyperplanes that does not depend on   104  Chapter 5 Support Vector Machines  the dimension of feature space. Let S ⊆ {x :  cid:107 x cid:107  ≤ r}. We will show that the VC-dimension d of the set of canonical hyperplanes {x  cid:55 → sgn w·x  : minx∈S w· x = 1 ∧  cid:107 w cid:107  ≤ Λ} veriﬁes   5.51   d ≤ r2Λ2 .   a  Let {x1, . . . , xd} be a set that can be shattered. Show that for all y =   b  Use randomization over the labels y and Jensen’s inequality to show that   y1, . . . , yd  ∈ {−1, +1}d, d ≤ Λ cid:107  cid:80 d d ≤ Λ cid:113  cid:80 d  i=1  cid:107 xi cid:107 2.   c  Conclude that d ≤ r2Λ2.  i=1 yixi cid:107 .   6 Kernel Methods  Kernel methods are widely used in machine learning. They are ﬂexible techniques that can be used to extend algorithms such as SVMs to deﬁne non-linear decision boundaries. Other algorithms that only depend on inner products between sample points can be extended similarly, many of which will be studied in future chapters. The main idea behind these methods is based on so-called kernels or kernel func- tions, which, under some technical conditions of symmetry and positive-deﬁniteness, implicitly deﬁne an inner product in a high-dimensional space. Replacing the orig- inal inner product in the input space with positive deﬁnite kernels immediately extends algorithms such as SVMs to a linear separation in that high-dimensional space, or, equivalently, to a non-linear separation in the input space.  In this chapter, we present the main deﬁnitions and key properties of positive deﬁnite symmetric kernels, including the proof of the fact that they deﬁne an inner product in a Hilbert space, as well as their closure properties. We then extend the SVM algorithm using these kernels and present several theoretical results including general margin-based learning guarantees for hypothesis sets based on kernels. We also introduce negative deﬁnite symmetric kernels and point out their relevance to the construction of positive deﬁnite kernels, in particular from distances or metrics. Finally, we illustrate the design of kernels for non-vectorial discrete structures by introducing a general family of kernels for sequences, rational kernels. We describe an eﬃcient algorithm for the computation of these kernels and illustrate them with several examples.  6.1  Introduction  In the previous chapter, we presented an algorithm for linear classiﬁcation, SVMs, which is both eﬀective in applications and beneﬁts from a strong theoretical jus- tiﬁcation. In practice, linear separation is often not possible. Figure 6.1a shows an example where any hyperplane crosses both populations. However, one can use   106  Chapter 6 Kernel Methods   a    b   Figure 6.1 Non-linearly separable case. The classiﬁcation task consists of discriminating between blue and red points.  a  No hyperplane can separate the two populations.  b  A non-linear mapping can be used instead.  more complex functions to separate the two sets as in ﬁgure 6.1b. One way to de- ﬁne such a non-linear decision boundary is to use a non-linear mapping Φ from the input space X to a higher-dimensional space H, where linear separation is possible  see ﬁgure 6.2 .  The dimension of H can truly be very large in practice. For example, in the case of document classiﬁcation, one may wish to use as features sequences of three consecutive words, i.e., trigrams. Thus, with a vocabulary of just 100,000 words, the dimension of the feature space H reaches 1015. On the positive side, the margin bounds presented in section 5.4 show that, remarkably, the generalization ability of large-margin classiﬁcation algorithms such as SVMs do not depend on the dimension of the feature space, but only on the margin ρ and the number of training examples m. Thus, with a favorable margin ρ, such algorithms could succeed even in very high-dimensional space. However, determining the hyperplane solution requires multiple inner product computations in high-dimensional spaces, which can become be very costly.  A solution to this problem is to use kernel methods, which are based on kernels  or kernel functions.  Deﬁnition 6.1  Kernels  A function K : X × X → R is called a kernel over X. The idea is to deﬁne a kernel K such that for any two points x, x cid:48  ∈ X, K x, x cid:48   be equal to an inner product of vectors Φ x  and Φ y :6  ∀x, x cid:48  ∈ X, K x, x cid:48   =  cid:104 Φ x , Φ x cid:48   cid:105  ,   6.1   6 To diﬀerentiate that inner product from the one of the input space, we will typically denote it by  cid:104 ·, · cid:105 .   6.1  Introduction  107  Figure 6.2 An example of a non-linear mapping from 2-dimensions to 3-dimensions, where the task becomes linearly seperable.  for some mapping Φ : X → H to a Hilbert space H called a feature space. Since an  inner product is a measure of the similarity of two vectors, K is often interpreted as a similarity measure between elements of the input space X.  An important advantage of such a kernel K is eﬃciency: K is often signiﬁcantly more eﬃcient to compute than Φ and an inner product in H. We will see several common examples where the computation of K x, x cid:48   can be achieved in O N   while that of  cid:104 Φ x , Φ x cid:48   cid:105  typically requires O dim H   work, with dim H   cid:29  N . Furthermore, in some cases, the dimension of H is inﬁnite.  Perhaps an even more crucial beneﬁt of such a kernel function K is ﬂexibility: there is no need to explicitly deﬁne or compute a mapping Φ. The kernel K can be arbitrarily chosen so long as the existence of Φ is guaranteed, i.e. K satisﬁes Mercer’s condition  see theorem 6.2 .  Theorem 6.2  Mercer’s condition  Let X ⊂ RN be a compact set and let K : X×X → R  be a continuous and symmetric function. Then, K admits a uniformly convergent expansion of the form  with an > 0 iﬀ for any square integrable function c  c ∈ L2 X  , the following condition holds:  K x, x cid:48   =  anφn x φn x cid:48  ,  ∞ cid:88 n=0   cid:90   cid:90 X×X  c x c x cid:48  K x, x cid:48  dxdx cid:48  ≥ 0.  This condition is important to guarantee the convexity of the optimization problem for algorithms such as SVMs, thereby ensuring convergence to a global minimum. A condition that is equivalent to Mercer’s condition under the assumptions of the theorem is that the kernel K be positive deﬁnite symmetric  PDS . This property  Φ   108  Chapter 6 Kernel Methods  is in fact more general since in particular it does not require any assumption about X. In the next section, we give the deﬁnition of this property and present several commonly used examples of PDS kernels, then show that PDS kernels induce an inner product in a Hilbert space, and prove several general closure properties for PDS kernels.  6.2 Positive deﬁnite symmetric kernels  6.2.1 Deﬁnitions  Deﬁnition 6.3  Positive deﬁnite symmetric kernels  A kernel K : X × X → R is said to be positive deﬁnite symmetric  PDS  if for any {x1, . . . , xm} ⊆ X, the matrix K = [K xi, xj ]ij ∈ Rm×m is symmetric positive semideﬁnite  SPSD .  K is SPSD if it is symmetric and one of the following two equivalent conditions holds:    the eigenvalues of K are non-negative;    for any column vector c =  c1, . . . , cm  cid:62  ∈ Rm×1,  c cid:62 Kc =  cicjK xi, xj  ≥ 0.  n cid:88 i,j=1   6.2   For a sample S =  x1, . . . , xm , K = [K xi, xj ]ij ∈ Rm×m is called the kernel  matrix or the Gram matrix associated to K and the sample S.  Let us insist on the terminology: the kernel matrix associated to a positive deﬁ- nite kernel is positive semideﬁnite . This is the correct mathematical terminology. Nevertheless, the reader should be aware that in the context of machine learning, some authors have chosen to use instead the term positive deﬁnite kernel to imply a positive deﬁnite kernel matrix or used new terms such as positive semideﬁnite kernel .  The following are some standard examples of PDS kernels commonly used in  applications.  Example 6.4  Polynomial kernels  For any constant c > 0, a polynomial kernel of de-  gree d ∈ N is the kernel K deﬁned over RN by:  ∀x, x cid:48  ∈ RN , K x, x cid:48   =  x · x cid:48  + c d.   6.3   Polynomial kernels map the input space to a higher-dimensional space of dimension   cid:0 N +d d  cid:1   see exercise 6.12 . As an example, for an input space of dimension N = 2,  a second-degree polynomial  d = 2  corresponds to the following inner product in   6.2 Positive deﬁnite symmetric kernels  109   a    b   Figure 6.3 Illustration of the XOR classiﬁcation problem and the use of polynomial kernels.  a  XOR problem linearly non-separable in the input space.  b  Linearly separable using second-degree polynomial kernel.  dimension 6:  ∀x, x cid:48  ∈ R2, K x, x cid:48   =  x1x cid:48 1 + x2x cid:48 2 + c 2 =  .   6.4   x2 1 x2  2√2 x1x2 √2c x1 √2c x2  c        ·  x cid:48 2 1 x cid:48 2 2√2 x cid:48 1x cid:48 2 √2c x cid:48 1 √2c x cid:48 2  c    Thus, the features corresponding to a second-degree polynomial are the original features  x1 and x2 , as well as products of these features, and the constant feature. More generally, the features associated to a polynomial kernel of degree d are all the monomials of degree at most d based on the original features. The explicit expression of polynomial kernels as inner products, as in  6.4 , proves directly that they are PDS kernels.  To illustrate the application of polynomial kernels, consider the example of ﬁg- ure 6.3a which shows a simple data set in dimension two that is not linearly sep- arable. This is known as the XOR problem due to its interpretation in terms of the exclusive OR  XOR  function: the label of a point is blue iﬀ exactly one of its coordinates is 1. However, if we map these points to the six-dimensional space deﬁned by a second-degree polynomial as described in  6.4 , then the problem be- comes separable by the hyperplane of equation x1x2 = 0. Figure 6.3b illustrates that by showing the projection of these points on the two-dimensional space deﬁned by their third and fourth coordinates.  x2   −1, 1    1, 1   x1   −1,−1    1,−1   √2 x1x2   1, 1, +√2, −  √2, −  √2, 1    1, 1, +√2, +√2, +√2, 1   √2 x1   1, 1, −  √2, −  √2, +√2, 1    1, 1, −  √2, +√2, −  √2, 1    110  Chapter 6 Kernel Methods  Example 6.5  Gaussian kernels  For any constant σ > 0, a Gaussian kernel or radial basis function  RBF  is the kernel K deﬁned over RN by:  ∀ x, x cid:48  ∈ RN , K x, x cid:48   = exp cid:18 − cid:107 x cid:48  − x cid:107 2  2σ2   cid:19  .   6.5   Gaussian kernels are among the most frequently used kernels in applications. We will prove in section 6.2.3 that they are PDS kernels and that they can be derived by  pansion of the exponential function, we can rewrite the expression of K cid:48  as follows:  normalization from the kernels K cid:48  :  x, x cid:48    cid:55 → exp cid:0  x·x cid:48  σ2  cid:1 . Using the power series ex- +∞ cid:88 n=0  x · x cid:48  n σ2n n!  ∀ x, x cid:48  ∈ RN , K cid:48  x, x cid:48   =  which shows that the kernels K cid:48 , and thus Gaussian kernels, are positive linear combinations of polynomial kernels of all degrees n ≥ 0. Example 6.6  Sigmoid kernels  For any real constants a, b ≥ 0, a sigmoid kernel is the kernel K deﬁned over RN by:  ,  ∀x, x cid:48  ∈ RN , K x, x cid:48   = tanh cid:0 a x · x cid:48   + b cid:1 .   6.6   Using sigmoid kernels with SVMs leads to an algorithm that is closely related to learning algorithms based on simple neural networks, which are also often deﬁned via a sigmoid function. When a < 0 or b < 0, the kernel is not PDS and the corresponding neural network does not beneﬁt from the convergence guarantees of convex optimization  see exercise 6.18 .  6.2.2 Reproducing kernel Hilbert space Here, we prove the crucial property of PDS kernels, which is to induce an inner product in a Hilbert space. The proof will make use of the following lemma. Lemma 6.7  Cauchy-Schwarz inequality for PDS kernels  Let K be a PDS kernel. Then, for any x, x cid:48  ∈ X, K x cid:48 ,x  K x cid:48 ,x cid:48   cid:17 . By deﬁnition, if K is PDS, Proof: Consider the matrix K =  cid:16  K x,x  K x,x cid:48   then K is SPSD for all x, x cid:48  ∈ X. In particular, the product of the eigenvalues of K, det K , must be non-negative, thus, using K x cid:48 , x  = K x, x cid:48  , we have  K x, x cid:48  2 ≤ K x, x K x cid:48 , x cid:48  .   6.7   det K  = K x, x K x cid:48 , x cid:48   − K x, x cid:48  2 ≥ 0,  which concludes the proof.  The following is the main result of this section.   cid:3   Theorem 6.8  Reproducing kernel Hilbert space  RKHS    Let K : X × X → R be a PDS kernel. Then, there exists a Hilbert space H  see deﬁnition A.2  and a mapping Φ   111   6.8    6.9   6.2 Positive deﬁnite symmetric kernels  from X to H such that:  Furthermore, H has the following property known as the reproducing property:  ∀x, x cid:48  ∈ X, K x, x cid:48   =  cid:104 Φ x , Φ x cid:48   cid:105  .  ∀h ∈ H,∀x ∈ X,  h x  =  cid:104 h, K x,·  cid:105  .  H is called a reproducing kernel Hilbert space  RKHS  associated to K.  Proof: For any x ∈ X, deﬁne Φ x  : X → RX as follows: ∀x cid:48  ∈ X, Φ x  x cid:48   = K x, x cid:48  . aiΦ xi  : ai ∈ R, xi ∈ X,I < ∞ cid:27 .  H0 = cid:26  cid:88 i∈I  We deﬁne H0 as the set of ﬁnite linear combinations of such functions Φ x :  Now, we introduce an operation  cid:104 ·,· cid:105  on H0 × H0 deﬁned for all f, g ∈ H0 with f = cid:80 i∈I aiΦ xi  and g = cid:80 j∈J bjΦ x cid:48 j  by aibjK xi, x cid:48 j  = cid:88 j∈J   cid:104 f, g cid:105  =  cid:88 i∈I,j∈J  bjf  x cid:48 j  = cid:88 i∈I  aig xi .  By deﬁnition,  cid:104 ·,· cid:105  is symmetric. The last two equations show that  cid:104 f, g cid:105  does not depend on the particular representations of f and g, and also show that  cid:104 ·,· cid:105  is  bilinear. Further, for any f = cid:80 i∈I aiΦ xi  ∈ H0, since K is PDS, we have  aiajK xi, xj  ≥ 0.   cid:104 f, f cid:105  =  cid:88 i,j∈I  Thus,  cid:104 ·,· cid:105  is positive semideﬁnite bilinear form. This inequality implies more generally using the bilinearity of  cid:104 ·,· cid:105  that for any f1, . . . , fm and c1, . . . , cm ∈ R,  m cid:88 i,j=1  cicj cid:104 fi, fj cid:105  = cid:68  m cid:88 i=1  cifi,  m cid:88 j=1  cjfj cid:69  ≥ 0.  Hence,  cid:104 ·,· cid:105  is a PDS kernel on H0. Thus, for any f ∈ H0 and any x ∈ X, by  lemma 6.7, we can write   cid:104 f, Φ x  cid:105 2 ≤  cid:104 f, f cid:105  cid:104 Φ x , Φ x  cid:105 .  Further, we observe the reproducing property of  cid:104 ·,· cid:105 : for any f = cid:80 i∈I aiΦ xi  ∈ H0, by deﬁnition of  cid:104 ·,· cid:105 , ∀x ∈ X,  aiK xi, x  =  cid:104 f, Φ x  cid:105  .   6.10   f  x  = cid:88 i∈I   112  Chapter 6 Kernel Methods  Thus, [f  x ]2 ≤  cid:104 f, f cid:105 K x, x  for all x ∈ X, which shows the deﬁniteness of  cid:104 ·,· cid:105 . This implies that  cid:104 ·,· cid:105  deﬁnes an inner product on H0, which thereby becomes a pre-Hilbert space. H0 can be completed to form a Hilbert space H in which it is dense, following a standard construction. By the Cauchy-Schwarz inequality, for any x ∈ X, f  cid:55 →  cid:104 f, Φ x  cid:105  is Lipschitz, therefore continuous. Thus, since H0 is  cid:3  dense in H, the reproducing property  6.10  also holds over H. The Hilbert space H deﬁned in the proof of the theorem for a PDS kernel K is called the reproducing kernel Hilbert space  RKHS  associated to K. Any Hilbert space H such that there exists Φ : X → H with K x, x cid:48   =  cid:104 Φ x , Φ x cid:48   cid:105  for all x, x cid:48  ∈ X will denote by  cid:107  ·  cid:107 H the norm induced by the inner product in feature space H:  cid:107 w cid:107 H =  cid:112  cid:104 w, w cid:105  for all w ∈ H. Note that the feature spaces associated to K  are in general not unique and may have diﬀerent dimensions. In practice, when referring to the dimension of the feature space associated to K, we either refer to the dimension of the feature space based on a feature mapping described explicitly, or to that of the RKHS associated to K.  is called a feature space associated to K and Φ is called a feature mapping. We  Theorem 6.8 implies that PDS kernels can be used to implicitly deﬁne a feature space or feature vectors. As already underlined in previous chapters, the role played by the features in the success of learning algorithms is crucial: with poor features, uncorrelated with the target labels, learning could become very challenging or even impossible; in contrast, good features could provide invaluable clues to the algo- rithm. Therefore, in the context of learning with PDS kernels and for a ﬁxed input space, the problem of seeking useful features is replaced by that of ﬁnding useful PDS kernels. While features represented the user’s prior knowledge about the task in the standard learning problems, here PDS kernels will play this role. Thus, in practice, an appropriate choice of PDS kernel for a task will be crucial.  6.2.3 Properties This section highlights several important properties of PDS kernels. We ﬁrst show that PDS kernels can be normalized and that the resulting normalized kernels are also PDS. We also introduce the deﬁnition of empirical kernel maps and describe their properties and extension. We then prove several important closure properties of PDS kernels, which can be used to construct complex PDS kernels from simpler ones.  To any kernel K, we can associate a normalized kernel K cid:48  deﬁned by  ∀x, x cid:48  ∈ X, K cid:48  x, x cid:48   =  0 √K x,x K x cid:48 ,x cid:48    K x,x cid:48    if  K x, x  = 0  ∨  K x cid:48 , x cid:48   = 0  otherwise.   6.11    6.2 Positive deﬁnite symmetric kernels  113  By deﬁnition, for a normalized kernel K cid:48 , K cid:48  x, x  = 1 for all x ∈ X such that K x, x   cid:54 = 0. An example of normalized kernel is the Gaussian kernel with param- eter σ > 0, which is the normalized kernel associated to K cid:48  :  x, x cid:48    cid:55 → exp cid:0  x·x cid:48  σ2  cid:1 : ∀x, x cid:48  ∈ RN ,  = exp cid:18 − cid:107 x cid:48  − x cid:107 2  e  cid:107 x cid:107 2 2σ2 e  K cid:48  x, x cid:48     cid:19  .   6.12    cid:107 x cid:48  cid:107 2 2σ2  x·x cid:48  σ2  2σ2  =  e  Lemma 6.9  Normalized PDS kernels  Let K be a PDS kernel. Then, the normalized kernel K cid:48  associated to K is PDS.   cid:112 K cid:48  x, x K cid:48  x cid:48 , x cid:48    Proof: Let {x1, . . . , xm} ⊆ X and let c be an arbitrary vector in Rm. We will show that the sum cid:80 m i,j=1 cicjK cid:48  xi, xj  is non-negative. By lemma 6.7, if K xi, xi  = 0 then K xi, xj  = 0 and thus K cid:48  xi, xj  = 0 for all j ∈ [m]. Thus, we can assume that K xi, xi  > 0 for all i ∈ [m]. Then, the sum can be rewritten as follows: m cid:88 i,j=1  cicj  cid:104 Φ xi , Φ xj  cid:105   cid:107 Φ xi  cid:107 H  cid:107 Φ xj  cid:107 H  m cid:88 i,j=1  cicjK xi, xj   H ≥ 0,  ciΦ xi   =   cid:112 K xi, xi K xj, xj   where Φ is a feature mapping associated to K, which exists by theorem 6.8.   cid:107 Φ xi  cid:107 H cid:13  cid:13  cid:13  cid:13  cid:13   = cid:13  cid:13  cid:13  cid:13  cid:13  m cid:88 i=1   cid:3   2  As indicated earlier, PDS kernels can be interpreted as a similarity measure since they induce an inner product in some Hilbert space H. This is more evident for a normalized kernel K since K x, x cid:48   is then exactly the cosine of the angle between the feature vectors Φ x  and Φ x cid:48  , provided that none of them is zero: Φ x  and  Φ x cid:48   are then unit vectors since  cid:107 Φ x  cid:107 H =  cid:107 Φ x cid:48   cid:107 H = cid:112 K x, x  = 1.  While one of the advantages of PDS kernels is an implicit deﬁnition of a fea- ture mapping, in some instances, it may be desirable to deﬁne an explicit feature mapping based on a PDS kernel. This may be to work in the primal for various optimization and computational reasons, to derive an approximation based on an explicit mapping, or as part of a theoretical analysis where an explicit mapping is more convenient. The empirical kernel map Φ associated to a PDS kernel K is a feature mapping that can be used precisely in such contexts. Given a training  sample containing points x1, . . . , xm ∈ X, Φ : X → Rm is deﬁned for all x ∈ X by  Φ x  =  K x, x1   ...  K x, xm    .  Thus, Φ x  is the vector of the K-similarity measures of x with each of the training points. Let K be the kernel matrix associated to K and ei the ith unit vector. Note that for any i ∈ [m], Φ xi  is the ith column of K, that is Φ xi  = Kei. In   114  Chapter 6 Kernel Methods  particular, for all i, j ∈ [m],   cid:104 Φ xi , Φ xj  cid:105  =  Kei  cid:62  Kej  = e cid:62 i K2ej.  Thus, the kernel matrix K cid:48  associated to Φ is K2. It may desirable in some cases to deﬁne a feature mapping whose kernel matrix coincides with K. Let K† 2 denote the SPSD matrix whose square is K†, the pseudo-inverse of K. K† 2 can be derived from K† via singular value decomposition and if the matrix K is invertible, K† coincides with K−1 2  see appendix A for properties of the pseudo-inverse . Then, Ψ can be deﬁned as follows using the empirical kernel map Φ:  1 2  1  1  ∀x ∈ X, Ψ x  = K†  1  2 Φ x .  Using the identity KK†K = K valid for any symmetric matrix K, for all i, j ∈ [m], the following holds:   cid:104 Ψ xi , Ψ xj  cid:105  =  K†  1  1  2 Kei  cid:62  K†  2 Kej  = e cid:62 i KK†Kej = e cid:62 i Kej.  Thus, the kernel matrix associated to Ψ is K. Finally, note that for the feature  mapping Ω : X → Rm deﬁned by  ∀x ∈ X, Ω x  = K†Φ x ,  for all i, j ∈ [m], we have  cid:104 Ω xi , Ω xj  cid:105  = e cid:62 i KK†K†Kej = e cid:62 i KK†ej, using the identity K†K†K = K† valid for any symmetric matrix K. Thus, the kernel matrix associated to Ω is KK†, which reduces to the identity matrix I ∈ Rm×m when K is invertible, since K† = K−1 in that case.  As pointed out in the previous section, kernels represent the user’s prior knowl- edge about a task. In some cases, a user may come up with appropriate similarity measures or PDS kernels for some subtasks — for example, for diﬀerent subcate- gories of proteins or text documents to classify. But how can the user combine these PDS kernels to form a PDS kernel for the entire class? Is the resulting combined kernel guaranteed to be PDS? In the following, we will show that PDS kernels are closed under several useful operations which can be used to design complex PDS kernels. These operations are the sum and the product of kernels, as well as the tensor product of two kernels K and K cid:48 , denoted by K ⊗ K cid:48  and deﬁned by  K ⊗ K cid:48   x1, x cid:48 1, x2, x cid:48 2  = K x1, x2 K cid:48  x cid:48 1, x cid:48 2 .  ∀x1, x2, x cid:48 1, x cid:48 2 ∈ X,  They also include the pointwise limit: given a sequence of kernels  Kn n∈N such that for all x, x cid:48  ∈ X  Kn x, x cid:48   n∈N admits a limit, the pointwise limit of  Kn n∈N is the kernel K deﬁned for all x, x cid:48  ∈ X by K x, x cid:48   = limn→+∞ Kn  x, x cid:48  . Similarly, if  cid:80 ∞n=0 anxn is a power series with radius of convergence ρ > 0 and K a kernel taking values in  −ρ, +ρ , then cid:80 ∞n=0 anK n is the kernel obtained by composition   6.2 Positive deﬁnite symmetric kernels  115  of K with that power series. The following theorem provides closure guarantees for all of these operations.  Theorem 6.10  PDS kernels — closure properties  PDS kernels are closed under sum, product, tensor product, pointwise limit, and composition with a power series   cid:80 ∞n=0 anxn with an ≥ 0 for all n ∈ N. Proof: We start with two kernel matrices, K and K cid:48 , generated from PDS kernels K and K cid:48  for an arbitrary set of m points. By assumption, these kernel matrices are SPSD. Observe that for any c ∈ Rm×1,   c cid:62 Kc ≥ 0  ∧  c cid:62 K cid:48 c ≥ 0  ⇒ c cid:62  K + K cid:48  c ≥ 0.  By  6.2 , this shows that K + K cid:48  is SPSD and thus that K + K cid:48  is PDS. To show closure under product, we will use the fact that for any SPSD matrix K there exists M such that K = MM cid:62 . The existence of M is guaranteed as it can be generated via, for instance, singular value decomposition of K, or by Cholesky decomposition. The kernel matrix associated to KK cid:48  is  KijK cid:48 ij ij. For any c ∈ Rm×1, expressing  Kij in terms of the entries of M, we can write  cicj KijK cid:48 ij  =  m cid:88 i,j=1  MikMjk cid:105 K cid:48 ij cid:19  cicjMikMjkK cid:48 ij cid:21   cicj cid:18  cid:104  m cid:88 k=1 m cid:88 i,j=1 m cid:88 k=1 cid:20  m cid:88 i,j=1 m cid:88 k=1  z cid:62 k K cid:48 zk ≥ 0,  =  =  with zk =  cid:20  c1M1k... cmMmk cid:21 . This shows that PDS kernels are closed under product. The tensor product of K and K cid:48  is PDS as the product of the two PDS kernels  x1, x cid:48 1, x2, x cid:48 2   cid:55 → K x1, x2  and  x1, x cid:48 1, x2, x cid:48 2   cid:55 → K cid:48  x cid:48 1, x cid:48 2 . Next, let  Kn n∈N be a sequence of PDS kernels with pointwise limit K. Let K be the kernel matrix associated to K and Kn the one associated to Kn for any n ∈ N. Observe that   ∀n, c cid:62 Knc ≥ 0  ⇒ lim n→∞  c cid:62 Knc = c cid:62 Kc ≥ 0.  This shows the closure under pointwise limit. Finally, assume that K is a PDS  kernel with K x, x cid:48   < ρ for all x, x cid:48  ∈ X and let f : x  cid:55 → cid:80 ∞n=0 anxn, an ≥ 0 be a power series with radius of convergence ρ. Then, for any n ∈ N, K n and thus anK n are PDS by closure under product. For any N ∈ N, cid:80 N n=0 anK n is PDS by closure under sum of anK ns and f ◦ K is PDS by closure under the limit of cid:80 N n=0 anK n  cid:3   as N tends to inﬁnity.   116  Chapter 6 Kernel Methods  The theorem implies in particular that for any PDS kernel matrix K, exp K  is PDS, since the radius of convergence of exp is inﬁnite. In particular, the kernel is PDS. Thus, by lemma 6.9, this shows that a Gaussian kernel, which is the normalized kernel associated to K cid:48 , is PDS.  σ2  cid:1  is PDS since  x, x cid:48    cid:55 → x·x cid:48   K cid:48  :  x, x cid:48    cid:55 → exp cid:0  x·x cid:48   σ2  6.3 Kernel-based algorithms  In this section we discuss how SVMs can be used with kernels and analyze the impact that kernels have on generalization.  6.3.1 SVMs with PDS kernels In chapter 5, we noted that the dual optimization problem for SVMs as well as the form of the solution did not directly depend on the input vectors but only on inner products. Since a PDS kernel implicitly deﬁnes an inner product  theorem 6.8 , we can extend SVMs and combine it with an arbitrary PDS kernel K by replacing each instance of an inner product x· x cid:48  with K x, x cid:48  . This leads to the following general form of the SVM optimization problem and solution with PDS kernels extending  5.33 :  αiαjyiyjK xi, xj    6.13   subject to: 0 ≤ αi ≤ C ∧  αiyi = 0, i ∈ [m].  max  α  αi −  1 2  m cid:88 i=1  h x  = sgn cid:16  m cid:88 i=1  m cid:88 i,j=1 m cid:88 i=1 αiyiK xi, x  + b cid:17 ,  In view of  5.34 , the hypothesis h solution can be written as:  with b = yi − cid:80 m  j=1 αjyjK xj, xi  for any xi with 0 < αi < C. We can rewrite the optimization problem  6.13  in a vector form, by using the kernel matrix K associated to K for the training sample  x1, . . . , xm  as follows:   6.14    6.15   max  α  2 1 cid:62 α −  α ◦ y  cid:62 K α ◦ y   subject to: 0 ≤ α ≤ C ∧ α cid:62 y = 0.  In this formulation, α ◦ y is the Hadamard product or entry-wise product of the vectors α and y. Thus, it is the column vector in Rm×1 whose ith component equals αiyi. The solution in vector form is the same as in  6.14 , but with b = yi −  α ◦ y  cid:62 Kei for any xi with 0 < αi < C.   6.3 Kernel-based algorithms  117  This version of SVMs used with PDS kernels is the general form of SVMs we will consider in all that follows. The extension is important, since it enables an implicit non-linear mapping of the input points to a high-dimensional space where large-margin separation is sought.  Many other algorithms in areas including regression, ranking, dimensionality re- duction or clustering can be extended using PDS kernels following the same scheme  see in particular chapters 9, 10, 11, 15 .  6.3.2 Representer theorem Observe that modulo the oﬀset b, the hypothesis solution of SVMs can be written as a linear combination of the functions K xi,· , where xi is a sample point. The following theorem known as the representer theorem shows that this is in fact a general property that holds for a broad class of optimization problems, including that of SVMs with no oﬀset.  Theorem 6.11  Representer theorem  Let K : X × X → R be a PDS kernel and H its corresponding RKHS. Then, for any non-decreasing function G : R → R and any loss function L : Rm → R ∪ {+∞}, the optimization problem  argmin h∈H  F  h  = argmin h∈H  G  cid:107 h cid:107 H  + L cid:0 h x1 , . . . , h xm  cid:1   increasing, then any solution has this form.  admits a solution of the form h∗ = cid:80 m i=1 αiK xi,· . If G is further assumed to be Proof: Let H1 = span {K xi,·  : i ∈ [m]} . Any h ∈ H admits the decomposition h = h1 + h⊥ according to H = H1 ⊕ H⊥1 , where ⊕ is the direct sum. Since G is non-decreasing, G  cid:107 h1 cid:107 H  ≤ G  cid:112  cid:107 h1 cid:107 2H +  cid:107 h⊥ cid:107 2H  = G  cid:107 h cid:107 H . By the reproducing property, for all i ∈ [m], h xi  =  cid:104 h, K xi,·  cid:105  =  cid:104 h1, K xi,·  cid:105  = h1 xi . Thus, L cid:0 h x1 , . . . , h xm  cid:1  = L cid:0 h1 x1 , . . . , h1 xm  cid:1  and F  h1  ≤ F  h . This proves the  cid:107 h⊥ cid:107 H > 0 and any solution of the optimization problem must be in H1.  If G is further increasing, then F  h1  < F  h  when  cid:3   ﬁrst part of the theorem.  6.3.3 Learning guarantees Here, we present general learning guarantees for hypothesis sets based on PDS kernels, which hold in particular for SVMs combined with PDS kernels.  The following theorem gives a general bound on the empirical Rademacher com- plexity of kernel-based hypotheses with bounded norm, that is a hypothesis set  of the form H = {h ∈ H :  cid:107 h cid:107 H ≤ Λ}, for some Λ ≥ 0, where H is the RKHS associated to a kernel K. By the reproducing property, any h ∈ H is of the form x  cid:55 →  cid:104 h, K x,·  cid:105  =  cid:104 h, Φ x  cid:105  with  cid:107 h cid:107 H ≤ Λ, where Φ is a feature mapping associ- ated to K, that is of the form x  cid:55 →  cid:104 w, Φ x  cid:105  with  cid:107 w cid:107 H ≤ Λ.   118  Chapter 6 Kernel Methods  Theorem 6.12  Rademacher complexity of kernel-based hypotheses  Let K : X× X → R be a PDS kernel and let Φ : X → H be a feature mapping associated to K. Let S ⊆ {x : K x, x  ≤ r2} be a sample of size m, and let H = {x  cid:55 →  cid:104 w, Φ x  cid:105  :  cid:107 w cid:107 H ≤ Λ} for some Λ ≥ 0. Then  ≤ cid:114  r2Λ2  m  .   6.16   Proof: The proof steps are as follows:   cid:98 RS H  =  m  Λ cid:112 Tr[K] σiΦ xi  cid:69  cid:21   Λ  E  E  Λ m  1 m   cid:98 RS H  ≤ σ cid:20  sup  cid:107 w cid:107 ≤Λ cid:68 w, m cid:88 i=1 σiΦ xi  cid:13  cid:13  cid:13 H cid:21  σ cid:20  cid:13  cid:13  cid:13  m cid:88 i=1 σ cid:20  cid:13  cid:13  cid:13  m cid:20  E H cid:21  cid:21 1 2 σiΦ xi  cid:13  cid:13  cid:13  m cid:88 i=1  cid:107 Φ xi  cid:107 2H cid:105  cid:21 1 2 m cid:20  E σ cid:104  m cid:88 i=1 K xi, xi  cid:105  cid:21 1 2 m cid:20  E σ cid:104  m cid:88 i=1 ≤ cid:114  r2Λ2 Λ cid:112 Tr[K]  m  m  Λ  Λ  2  .  =  ≤  =  =  =   Cauchy-Schwarz, eq. case    Jensen’s ineq.    i  cid:54 = j ⇒ E  σ  [σiσj] = 0   The initial equality holds by deﬁnition of the empirical Rademacher complexity  deﬁnition 3.1 . The ﬁrst inequality is due to the Cauchy-Schwarz inequality and  cid:107 w cid:107 H ≤ Λ. The following inequality results from Jensen’s inequality  theorem B.20  applied to the concave function √·. The subsequent equality is a consequence of Eσ[σiσj] = Eσ[σi] Eσ[σj] = 0 for i  cid:54 = j, since the Rademacher variables σi and σj are independent. The statement of the theorem then follows by noting that  cid:3  Tr[K] ≤ mr2. The theorem indicates that the trace of the kernel matrix is an important quantity for controlling the complexity of hypothesis sets based on kernels. Observe that by the Khintchine-Kahane inequality  D.24 , the empirical Rademacher complexity   cid:98 RS H  = Λ  m  Eσ[ cid:107  cid:80 m  i=1 σiΦ xi  cid:107 H] can also be lower bounded by 1√2  only diﬀers from the upper bound found by the constant K x, x  ≤ r2 for all x ∈ X, then the inequalities 6.16 hold for all samples S. The bound of theorem 6.12 or the inequalities 6.16 can be plugged into any of the Rademacher complexity generalization bounds presented in the previous chapters. In particular, in combination with theorem 5.8, they lead directly to the following margin bound similar to that of corollary 5.11.  1√2  Λ√Tr[K]  , which . Also, note that if  m   6.4 Negative deﬁnite symmetric kernels  119  Corollary 6.13  Margin bounds for kernel-based hypotheses  Let K : X × X → R be a PDS kernel with r2 = supx∈X K x, x . Let Φ : X → H be a feature mapping asso- ciated to K and let H = {x  cid:55 → w · Φ x  :  cid:107 w cid:107 H ≤ Λ} for some Λ ≥ 0. Fix ρ > 0. Then, for any δ > 0, each of the following statements holds with probability at least 1 − δ for any h ∈ H:  R h  ≤  cid:98 RS,ρ h  + 2 cid:114  r2Λ2 ρ2 R h  ≤  cid:98 RS,ρ h  + 2 cid:112 Tr[K]Λ2 ρ2  m  m  δ 2m  + cid:115  log 1 + 3 cid:115  log 2  δ 2m  .   6.17    6.18   6.4 Negative deﬁnite symmetric kernels  Often in practice, a natural distance or metric is available for the learning task considered. This metric could be used to deﬁne a similarity measure. As an ex- ample, Gaussian kernels have the form exp −d2 , where d is a metric for the input vector space. Several natural questions arise such as: what other PDS kernels can we construct from a metric in a Hilbert space? What technical condition should d satisfy to guarantee that exp −d2  is PDS? A natural mathematical deﬁnition that helps address these questions is that of negative deﬁnite symmetric  NDS  kernels.  is said to be negative-deﬁnite symmetric  NDS  if it is symmetric and if for all  Deﬁnition 6.14  Negative deﬁnite symmetric  NDS  kernels   A kernel K : X × X → R {x1, . . . , xm} ⊆ X and c ∈ Rm×1 with 1 cid:62 c = 0, the following holds:  c cid:62 Kc ≤ 0.  Clearly, if K is PDS, then −K is NDS, but the converse does not hold in general. The following gives a standard example of an NDS kernel.  Example 6.15  Squared distance — NDS kernel  The squared distance  x, x cid:48    cid:55 →  cid:107 x cid:48  − x cid:107 2 in RN deﬁnes an NDS kernel. Indeed, let c ∈ Rm×1 with cid:80 m i=1 ci = 0. Then,   120  Chapter 6 Kernel Methods  for any {x1, . . . , xm} ⊆ X, we can write  m cid:88 i,j=1  cicjxi − xj2 =  cjxj  =  =  m cid:88 i,j=1 m cid:88 i,j=1 m cid:88 i,j=1 m cid:88 i,j=1 = cid:16  m cid:88 j=1  ≤  cixi ·  cicj  cid:107 xi cid:107 2 +  cid:107 xj cid:107 2  − 2  cicj  cid:107 xi cid:107 2 +  cid:107 xj cid:107 2 − 2xi · xj  m cid:88 i=1 m cid:88 i=1 cicj  cid:107 xi cid:107 2 +  cid:107 xj cid:107 2  − 2 cid:13  cid:13  ci  cid:107 xi cid:107 2 cid:17  + cid:16  m cid:88 i=1 cj cid:17  cid:16  m cid:88 i=1  cicj  cid:107 xi cid:107 2 +  cid:107 xj cid:107 2   m cid:88 j=1 cixi cid:13  cid:13 2 ci cid:17  cid:16  m cid:88 j=1  cj cid:107 xj cid:107 2 cid:17  = 0.  The next theorems show connections between NDS and PDS kernels. These  results provide another series of tools for designing PDS kernels. Theorem 6.16 Let K cid:48  be deﬁned for any x0 by  K cid:48  x, x cid:48   = K x, x0  + K x cid:48 , x0  − K x, x cid:48   − K x0, x0   for all x, x cid:48  ∈ X. Then K is NDS iﬀ K cid:48  is PDS. Proof: Assume that K cid:48  is PDS and deﬁne K such that for any x0 we have K x, x cid:48   = K x, x0 +K x0, x cid:48  −K x0, x0 −K cid:48  x, x cid:48  . Then for any c ∈ Rm such that c cid:62 1 = 0 and any set of points  x1, . . . , xm  ∈ Xm we have cjK x0, xj  cid:17  m cid:88 i,j=1 cicjK cid:48  xi, xj  ≤ 0 .  cicjK xi, xj  = cid:16  m cid:88 i=1 − cid:16  m cid:88 i=1  cj cid:17  + cid:16  m cid:88 i=1 ci cid:17  cid:16  m cid:88 j=1 m cid:88 i,j=1  ciK xi, x0  cid:17  cid:16  m cid:88 j=1 m cid:88 i,j=1  cicjK cid:48  xi, xj  = −  K x0, x0  −  ci cid:17 2  which proves K is NDS.  Now, assume K is NDS and deﬁne K cid:48  for any x0 as above. Then, for any c ∈ Rm, we can deﬁne c0 = −c cid:62 1 and the following holds by the NDS property for any points  x1, . . . , xm  ∈ Xm as well as x0 deﬁned previously:  cid:80 m i,j=0 cicjK xi, xj  ≤ 0. This implies that  cid:16  m cid:88 i=0 cjK x0, xj  cid:17  m cid:88 i,j=0 cicjK cid:48  xi, xj  = −  cj cid:17  + cid:16  m cid:88 i=0 ci cid:17  cid:16  m cid:88 j=0 m cid:88 i,j=0  ciK xi, x0  cid:17  cid:16  m cid:88 j=0 − cid:16  m cid:88 i=0  cicjK cid:48  xi, xj  ≤ 0 ,  K x0, x0  −  ci cid:17 2   6.5 Sequence kernels  121  i,j=1 cicjK cid:48  xi, xj  ≥ −2c0 cid:80 m which implies 2 cid:80 m The equality holds since ∀x ∈ X, K cid:48  x, x0  = 0. This theorem is useful in showing other connections, such the following theorems,  0K cid:48  x0, x0  = 0.  cid:3   i=0 ciK cid:48  xi, x0  + c2  which are left as exercises  see exercises 6.17 and 6.18 .  Theorem 6.17 Let K : X × X → R be a symmetric kernel. Then, K is NDS iﬀ exp −tK  is a PDS kernel for all t > 0. The theorem provides another proof that Gaussian kernels are PDS: as seen earlier  Example 6.15 , the squared distance  x, x cid:48    cid:55 →  cid:107 x − x cid:48  cid:107 2 in RN is NDS, thus  x, x cid:48    cid:55 → exp −tx − x cid:48 2  is PDS for all t > 0. Theorem 6.18 Let K : X × X → R be an NDS kernel such that for all x, x cid:48  ∈ X, K x, x cid:48   = 0 iﬀ x = x cid:48 . Then, there exists a Hilbert space H and a mapping Φ : X → H such that for all x, x cid:48  ∈ X,  K x, x cid:48   =  cid:107 Φ x  − Φ x cid:48   cid:107 2.  Thus, under the hypothesis of the theorem, √K deﬁnes a metric. This theorem can be used to show that the kernel  x, x cid:48    cid:55 → exp −x − x cid:48 p  in R is not PDS for p > 2. Otherwise, for any t > 0, {x1, . . . , xm} ⊆ X and c ∈ Rm×1,  we would have:  m cid:88 i,j=1  cicje−txi−xjp  =  cicje−t1 pxi−t1 pxjp  ≥ 0.  m cid:88 i,j=1  This would imply that  x, x cid:48    cid:55 → x − x cid:48 p is NDS for p > 2, which can be proven  via theorem 6.18  not to be valid.  6.5 Sequence kernels  The examples given in the previous sections, including the commonly used poly- nomial or Gaussian kernels, were all for PDS kernels over vector spaces. In many learning tasks found in practice, the input space X is not a vector space. The examples to classify in practice could be protein sequences, images, graphs, parse trees, ﬁnite automata, or other discrete structures which may not be directly given as vectors. PDS kernels provide a method for extending algorithms such as SVMs originally designed for a vectorial space to the classiﬁcation of such objects. But, how can we deﬁne PDS kernels for these structures?  This section will focus on the speciﬁc case of sequence kernels, that is, kernels for sequences or strings. PDS kernels can be deﬁned for other discrete structures in somewhat similar ways. Sequence kernels are particularly relevant to learning algorithms applied to computational biology or natural language processing, which are both important applications.   122  Chapter 6 Kernel Methods  How can we deﬁne PDS kernels for sequences, which are similarity measures for sequences? One idea consists of declaring two sequences, e.g., two documents or two biosequences, as similar when they share common substrings or subsequences. One example could be the kernel between two sequences deﬁned by the sum of the product of the counts of their common substrings. But which substrings should be used in that deﬁnition? Most likely, we would need some ﬂexibility in the deﬁnition of the matching substrings. For computational biology applications, for example, the match could be imperfect. Thus, we may need to consider some number of mismatches, possibly gaps, or wildcards. More generally, we might need to allow various substitutions and might wish to assign diﬀerent weights to common substrings to emphasize some matching substrings and deemphasize others.  As can be seen from this discussion, there are many diﬀerent possibilities and we need a general framework for deﬁning such kernels. In the following, we will introduce a general framework for sequence kernels, rational kernels, which will include all the kernels considered in this discussion. We will also describe a general and eﬃcient algorithm for their computation and will illustrate them with some examples.  The deﬁnition of these kernels relies on that of weighted transducers. Thus, we  start with the deﬁnition of these devices as well as some relevant algorithms.  6.5.1 Weighted transducers Sequence kernels can be eﬀectively represented and computed using weighted trans- ducers. In the following deﬁnition, let Σ denote a ﬁnite input alphabet, ∆ a ﬁnite output alphabet, and  cid:15  the empty string or null label, whose concatenation with any string leaves it unchanged.  Deﬁnition 6.19 A weighted transducer T is a 7-tuple T =  Σ, ∆, Q, I, F, E, ρ  where Σ is a ﬁnite input alphabet, ∆ a ﬁnite output alphabet, Q is a ﬁnite set of states, I ⊆ Q the set of initial states, F ⊆ Q the set of ﬁnal states, E a ﬁnite multiset of transitions elements of Q ×  Σ ∪ { cid:15 }  ×  ∆ ∪ { cid:15 }  × R × Q, and ρ : F → R a ﬁnal weight function mapping F to R. The size of transducer T is the sum of its number of states and transitions and is denoted by T.7 Thus, weighted transducers are ﬁnite automata in which each transition is labeled with both an input and an output label and carries some real-valued weight. Fig- ure 6.4 shows an example of a weighted ﬁnite-state transducer. In this ﬁgure, the input and output labels of a transition are separated by a colon delimiter, and the weight is indicated after the slash separator. The initial states are represented by  7 A multiset in the deﬁnition of the transitions is used to allow for the presence of several transitions from a state p to a state q with the same input and output label, and even the same weight, which may occur as a result of various operations.   6.5 Sequence kernels  123  Figure 6.4 Example of weighted transducer.  a bold circle and ﬁnal states by double circles. The ﬁnal weight ρ[q] at a ﬁnal state q is displayed after the slash.  The input label of a path π is a string element of Σ∗ obtained by concatenating input labels along π. Similarly, the output label of a path π is obtained by con- catenating output labels along π. A path from an initial state to a ﬁnal state is an accepting path. The weight of an accepting path is obtained by multiplying the weights of its constituent transitions and the weight of the ﬁnal state of the path.  A weighted transducer deﬁnes a mapping from Σ∗ × ∆∗ to R. The weight associ- ated by a weighted transducer T to a pair of strings  x, y  ∈ Σ∗ × ∆∗ is denoted by T  x, y  and is obtained by summing the weights of all accepting paths with input label x and output label y. For example, the transducer of ﬁgure 6.4 associates to the pair  aab, baa  the weight 3 × 1 × 4 × 2 + 3 × 2 × 3 × 2, since there is a path with input label aab and output label baa and weight 3 × 1 × 4 × 2, and another one with weight 3 × 2 × 3 × 2. The sum of the weights of all accepting paths of an acyclic transducer, that is a transducer T with no cycle, can be computed in linear time, that is O T , using a general shortest-distance or forward-backward algorithm. These are simple algorithms, but a detailed description would require too much of a digression from the main topic of this chapter.  Composition An important operation for weighted transducers is composition, which can be used to combine two or more weighted transducers to form more complex weighted transducers. As we shall see, this operation is useful for the creation and computation of sequence kernels. Its deﬁnition follows that of compo- sition of relations. Given two weighted transducers T1 =  Σ, ∆, Q1, I1, F1, E1, ρ1  and T2 =  ∆, Ω, Q2, I2, F2, E2, ρ2 , the result of the composition of T1 and T2 is a  1  a:a 1  a:b 3  b:a 4  0  a:a 2  3 2  b:b 2  b:a 3  2 8  b:b 2   124  Chapter 6 Kernel Methods  weighted transducer denoted by T1 ◦ T2 and deﬁned for all x ∈ Σ∗ and y ∈ Ω∗ by  6.19   T1 x, z  · T2 z, y ,   T1 ◦ T2  x, y  =  cid:88 z∈∆∗  where the sum runs over all strings z over the alphabet ∆. Thus, composition is similar to matrix multiplication with inﬁnite matrices.  There exists a general and eﬃcient algorithm to compute the composition of two weighted transducers. In the absence of  cid:15 s on the input side of T1 or the output side of T2, the states of T1 ◦ T2 =  Σ, ∆, Q, I, F, E, ρ  can be identiﬁed with pairs made of a state of T1 and a state of T2, Q ⊆ Q1 × Q2. Initial states are those obtained by pairing initial states of the original transducers, I = I1 × I2, and similarly ﬁnal states are deﬁned by F = Q ∩  F1 × F2 . The ﬁnal weight at a state  q1, q2  ∈ F1 × F2 is ρ q  = ρ1 q1 ρ2 q2 , that is the product of the ﬁnal weights at q1 and q2. Transitions are obtained by matching a transition of T1 with one of T2 from appropriate transitions of T1 and T2:  E =   cid:93    q1,a,b,w1,q2 ∈E1  q cid:48 1,b,c,w2,q cid:48 2 ∈E2   cid:26  cid:18  q1, q cid:48 1 , a, c, w1 ⊗ w2,  q2, q cid:48 2  cid:19  cid:27 .  Here,  cid:93  denotes the standard join operation of multisets as in {1, 2}  cid:93  {1, 3} = {1, 1, 2, 3}, to preserve the multiplicity of the transitions. In the worst case, all transitions of T1 leaving a state q1 match all those of T2 leaving state q cid:48 1, thus the space and time complexity of composition is quadratic: O T1T2 . In practice, such cases are rare and composition is very eﬃcient. Fig- ure 6.5 illustrates the algorithm in a particular case.  As illustrated by ﬁgure 6.6, when T1 admits output  cid:15  labels or T2 input  cid:15  labels, the algorithm just described may create redundant  cid:15 -paths, which would lead to an incorrect result. The weight of the matching paths of the original transducers would be counted p times, where p is the number of redundant paths in the result of composition. To avoid with this problem, all but one  cid:15 -path must be ﬁltered out of the composite transducer. Figure 6.6 indicates in boldface one possible choice for that path, which in this case is the shortest. Remarkably, that ﬁltering mechanism itself can be encoded as a ﬁnite-state transducer F  ﬁgure 6.6b .  To apply that ﬁlter, we need to ﬁrst augment T1 and T2 with auxiliary symbols let ˜T1   ˜T2  be the weighted transducer that make the semantics of  cid:15  explicit: obtained from T1  respectively T2  by replacing the output  respectively input   cid:15  labels with  cid:15 2  respectively  cid:15 1  as illustrated by ﬁgure 6.6. Thus, matching with the symbol  cid:15 1 corresponds to remaining at the same state of T1 and taking a transition of T2 with input  cid:15 .  cid:15 2 can be described in a symmetric way. The ﬁlter transducer F disallows a matching   cid:15 2,  cid:15 2  immediately after   cid:15 1,  cid:15 1  since this can be done   6.5 Sequence kernels  125   a    b    c   Figure 6.5  a  Weighted transducer T1.  b  Weighted transducer T2.  c  Result of composition of T1 and T2, T1 ◦ T2. Some states might be constructed during the execution of the algorithm that are not co-accessible, that is, they do not admit a path to a ﬁnal state, e.g.,  3, 2 . Such states and the related transitions  in red  can be removed by a trimming  or connection  algorithm in linear time.  instead via   cid:15 2,  cid:15 1 . By symmetry, it also disallows a matching   cid:15 1,  cid:15 1  immediately after   cid:15 2,  cid:15 2 . In the same way, a matching   cid:15 1,  cid:15 1  immediately followed by   cid:15 2,  cid:15 1  is not permitted by the ﬁlter F since a path via the matchings   cid:15 2,  cid:15 1   cid:15 1,  cid:15 1  is possible. Similarly,   cid:15 2,  cid:15 2   cid:15 2,  cid:15 1  is ruled out. It is not hard to verify that the ﬁlter transducer F is precisely a ﬁnite automaton over pairs accepting the complement of the language  L = σ∗   cid:15 1,  cid:15 1   cid:15 2,  cid:15 2  +   cid:15 2,  cid:15 2   cid:15 1,  cid:15 1  +   cid:15 1,  cid:15 1   cid:15 2,  cid:15 1  +   cid:15 2,  cid:15 2   cid:15 2,  cid:15 1  σ∗,  where σ = {  cid:15 1,  cid:15 1 ,   cid:15 2,  cid:15 2 ,   cid:15 2,  cid:15 1 , x}. Thus, the ﬁlter F guarantees that exactly one  cid:15 -path is allowed in the composition of each  cid:15  sequences. To obtain the correct result of composition, it suﬃces then to use the  cid:15 -free composition algorithm already described and compute  ˜T1 ◦ F ◦ ˜T2.   6.20   2  b:a 0.5  0  3 0.6  a:b 0.3  b:b 0.1  a:b 0.4  1  b:a 0.2   2, 1   a:a 0.1  b:a .06   3, 1   a:b .24   3, 3   a:b .18   0, 0   a:b .01  b:a .08   1, 1   a:a .02  a:a .04   3, 2    0, 1    126  Chapter 6 Kernel Methods  T1  ˜T1  T2  ˜T2   a    b   Figure 6.6 Redundant  cid:15 -paths in composition. All transition and ﬁnal weights are equal to one.  a  A straightforward generalization of the  cid:15 -free case would generate all the paths from  1, 1  to  3, 2  when composing T1 and T2 and produce an incorrect results in non-idempotent semirings.  b  Filter transducer F . The shorthand x is used to represent an element of Σ.  Indeed, the two compositions in ˜T1 ◦ F ◦ ˜T2 no longer involve  cid:15 s. Since the size of the ﬁlter transducer F is constant, the complexity of general composition is the same as that of  cid:15 -free composition, that is O T1T2 . In practice, the augmented transducers ˜T1 and ˜T2 are not explicitly constructed, instead the presence of the auxiliary symbols is simulated. Further ﬁlter optimizations help limit the number of non-coaccessible states created, for example, by examining more carefully the case of states with only outgoing non- cid:15 -transitions or only outgoing  cid:15 -transitions.  6.5.2 Rational kernels The following establishes a general framework for the deﬁnition of sequence kernels.  Deﬁnition 6.20  Rational kernels  A kernel K : Σ∗ × Σ∗ → R is said to be rational if it coincides with the mapping deﬁned by some weighted transducer U : ∀x, y ∈ Σ∗, K x, y  = U  x, y .                    0  a:d  1  !:e  2  d:a  3                                                     !"!  !"%  "!  !"&  $"!  %"!  %"%  !"!  !"!  !"!  !"!  !"!  ,!-  A  B  ,-  ,$-  A'  ,%-  B'  !:!!:!  !"!  Redundant ε-Paths Problem  %"%  "!  $"!  ! "!  ! "!  ! "!  a:a  !"%  b:!  ! "&  ! "!   MM, Pereira, and Riley, 1996; Pereira and Riley, 1997  T2  !:e  d:d  a:d  d:a  %"!  c:!  !:!!:!  !:!!:!  !:!!:!  !:!!:!  !2:!  !2:!  !2:!  !2:!  a:a  b:!  c:!  d:d  a:d  !1:        e  d:a   .   !"%  , " -  '.'  !"& ,!!"!!-  '.   "! ,!"!-  "& ,!"!!$  "! ,!"!-   .'  !"&  ,!!"!!-   .   $"! ,!"!-  $"! ,!"!-  *.'  !"&  ,!!"!!-  *.   %"! , " -  +.*  !T2  !2:! x:x  !1:!  !2:!  !1:!  x:x  !2:!  x:x  F  T = !T1 F !T2.                        6.5 Sequence kernels  127  Note that we could have instead adopted a more general deﬁnition: instead of us- ing weighted transducers, we could have used more powerful sequence mappings such as algebraic transductions, which are the functional counterparts of context- free languages, or even more powerful ones. However, an essential need for kernels is an eﬃcient computation, and more complex deﬁnitions would lead to substan- tially more costly computational complexities for kernel computation. For rational kernels, there exists a general and eﬃcient computation algorithm.  Computation We will assume that the transducer U deﬁning a rational kernel K does not admit any  cid:15 -cycle with non-zero weight, otherwise the kernel value is inﬁnite for all pairs. For any sequence x, let Tx denote a weighted transducer with just one accepting path whose input and output labels are both x and its weight equal to one. Tx can be straightforwardly constructed from x in linear time O x . Then, for any x, y ∈ Σ∗, U  x, y  can be computed by the following two steps: 1. Compute V = Tx◦U◦Ty using the composition algorithm in time O UTxTy . 2. Compute the sum of the weights of all accepting paths of V using a general  shortest-distance algorithm in time O V  .  By deﬁnition of composition, V is a weighted transducer whose accepting paths are precisely those accepting paths of U that have input label x and output label y. The second step computes the sum of the weights of these paths, that is, exactly U  x, y . Since U admits no  cid:15 -cycle, V is acyclic, and this step can be performed in linear time. The overall complexity of the algorithm for computing U  x, y  is then in O UTxTy . Since U is ﬁxed for a rational kernel K and Tx = O x  for any x, this shows that the kernel values can be obtained in quadratic time O xy . For some speciﬁc weighted transducers U , the computation can be more eﬃcient, for example in O x + y   see exercise 6.20 . PDS rational kernels For any transducer T , let T −1 denote the inverse of T , that is the transducer obtained from T by swapping the input and output labels of every transition. For all x, y, we have T −1 x, y  = T  y, x . The following theorem gives a general method for constructing a PDS rational kernel from an arbitrary weighted transducer.  Theorem 6.21 For any weighted transducer T =  Σ, ∆, Q, I, F, E, ρ , the function K = T ◦ T −1 is a PDS rational kernel.  Proof: By deﬁnition of composition and the inverse operation, for all x, y ∈ Σ∗,  K x, y  =  cid:88 z∈∆∗  T  x, z  T  y, z .   128  Chapter 6 Kernel Methods   a    b   Figure 6.7  a  Transducer Tbigram deﬁning the bigram kernel Tbigram◦T −1 Tgappy bigram deﬁning the gappy bigram kernel Tgappy bigram ◦ T −1 λ ∈  0, 1 .  bigram for Σ = {a, b}.  b  Transducer gappy bigram with gap penalty  K is the pointwise limit of the kernel sequence  Kn n≥0 deﬁned by: T  x, z  T  y, z ,  ∀n ∈ N,∀x, y ∈ Σ∗, Kn x, y  =  cid:88 z≤n  where the sum runs over all sequences in ∆∗ of length at most n. Kn is PDS since its corresponding kernel matrix Kn for any sample  x1, . . . , xm  is SPSD. This can be see form the fact that Kn can be written as Kn = AA cid:62  with A =  Kn xi, zj  i∈[m],j∈[N ], where z1, . . . , zN is some arbitrary enumeration of the set of strings in Σ∗ with length at most n. Thus, K is PDS as the pointwise limit of the  cid:3  sequence of PDS kernels  Kn n∈N. The sequence kernels commonly used in computational biology, natural language processing, computer vision, and other applications are all special instances of ra- tional kernels of the form T ◦ T −1. All of these kernels can be computed eﬃciently using the same general algorithm for the computational of rational kernels presented in the previous paragraph. Since the transducer U = T ◦ T −1 deﬁning such PDS rational kernels has a speciﬁc form, there are diﬀerent options for the computation of the composition Tx ◦ U ◦ Ty:   compute U = T ◦ T −1 ﬁrst, then V = Tx ◦ U ◦ Ty;   compute V1 = Tx ◦ T and V2 = Ty ◦ T ﬁrst, then V = V1 ◦ V −1   compute ﬁrst V1 = Tx ◦ T , then V2 = V1 ◦ T −1, then V = V2 ◦ Ty, or the similar series of operations with x and y permuted.  2  ;  All of these methods lead to the same result after computation of the sum of the weights of all accepting paths, and they all have the same worst-case complexity. However, in practice, due to the sparsity of intermediate compositions, there may be substantial diﬀerences between their time and space computational costs. An alternative method based on an n-way composition can further lead to signiﬁcantly more eﬃcient computations.  b:ε 1 a:ε 1  b:ε 1 a:ε 1  0  a:a 1  b:b 1  1  a:a 1  b:b 1  2 1  b:ε 1 a:ε 1  b:ε λ a:ε λ  b:ε 1 a:ε 1  0  a:a 1  b:b 1  1  a:a 1  b:b 1  2 1   6.5 Sequence kernels  129  Example 6.22  Bigram and gappy bigram sequence kernels  Figure 6.7a shows a weighted transducer Tbigram deﬁning a common sequence kernel, the bigram sequence kernel , for the speciﬁc case of an alphabet reduced to Σ = {a, b}. The bigram kernel as- sociates to any two sequences x and y the sum of the product of the counts of all bigrams in x and y. For any sequence x ∈ Σ∗ and any bigram z ∈ {aa, ab, ba, bb}, Tbigram x, z  is exactly the number of occurrences of the bigram z in x. Thus, by deﬁnition of composition and the inverse operation, Tbigram ◦ T −1 bigram computes exactly the bigram kernel. Figure 6.7b shows a weighted transducer Tgappy bigram deﬁning the so-called gappy bigram kernel . The gappy bigram kernel associates to any two sequences x and y the sum of the product of the counts of all gappy bigrams in x and y penalized by the length of their gaps. Gappy bigrams are sequences of the form aua, aub, bua, or bub, where u ∈ Σ∗ is called the gap. The count of a gappy bigram is multiplied by λu for some ﬁxed λ ∈  0, 1  so that gappy bigrams with longer gaps contribute less to the deﬁnition of the similarity measure. While this deﬁnition could appear to be somewhat complex, ﬁgure 6.7 shows that Tgappy bigram can be straightforwardly derived from Tbigram. The graphical representation of rational kernels helps understanding or modifying their deﬁnition.  Counting transducers The deﬁnition of most sequence kernels is based on the counts of some common patterns appearing in the sequences. In the examples just exam- ined, these were bigrams or gappy bigrams. There exists a simple and general method for constructing a weighted transducer counting the number of occurrences of patterns and using them to deﬁne PDS rational kernels. Let X be a ﬁnite au- tomaton representing the set of patterns to count. In the case of bigram kernels with Σ = {a, b}, X would be an automaton accepting exactly the set of strings {aa, ab, ba, bb}. Then, the weighted transducer of ﬁgure 6.8 can be used to compute exactly the number of occurrences of each pattern accepted by X.  Theorem 6.23 For any x ∈ Σ∗ and any sequence z accepted by X, Tcount x, z  is the number of occurrences of z in x.  Proof: Let x ∈ Σ∗ be an arbitrary sequence and let z be a sequence accepted by X. Since all accepting paths of Tcount have weight one, Tcount x, z  is equal to the number of accepting paths in Tcount with input label x and output z.  Now, an accepting path π in Tcount with input x and output z can be decomposed as π = π0 π01 π1, where π0 is a path through the loops of state 0 with input label some preﬁx x0 of x and output label  cid:15 , π01 an accepting path from 0 to 1 with input and output labels equal to z, and π1 a path through the self-loops of state 1 with input label a suﬃx x1 of x and output  cid:15 . Thus, the number of such paths is exactly   130  Chapter 6 Kernel Methods  Figure 6.8 Counting transducer Tcount for Σ = {a, b}. The “transition” X : X 1 stands for the weighted transducer created from the automaton X by adding to each transition an output label identical to the existing label, and by making all transition and ﬁnal weights equal to one.  the number of distinct ways in which we can write sequence x as x = x0zx1, which  cid:3  is exactly the number of occurrences of z in x.  The theorem provides a very general method for constructing PDS rational kernels Tcount ◦ T −1 count that are based on counts of some patterns that can be deﬁned via a ﬁnite automaton, or equivalently a regular expression. Figure 6.8 shows the transducer for the case of an input alphabet reduced to Σ = {a, b}. The general case can be obtained straightforwardly by augmenting states 0 and 1 with other self-loops using other symbols than a and b. In practice, a lazy evaluation can be used to avoid the explicit creation of these transitions for all alphabet symbols and instead creating them on-demand based on the symbols found in the input sequence x. Finally, one can assign diﬀerent weights to the patterns counted to emphasize or deemphasize some, as in the case of gappy bigrams. This can be done simply by changing the transitions weight or ﬁnal weights of the automaton X used in the deﬁnition of Tcount.  6.6 Approximate kernel feature maps  In the previous sections, we have seen the beneﬁts that kernel methods can provide by implicitly and eﬃciently mapping a learning problem from the input space X to a richer feature space H. One potential drawback when using kernel methods, is that the kernel function needs to be evaluated on all pairs of points in the training If this set contains a very large number of instances, then the O m2  cost set. in memory and O m2CK  cost in computation, where CK is the cost of a single kernel function evaluation, may be prohibitive. Another consideration is the cost of making predictions with a trained model. Evaluating the kernelized function i=1 αiK xi, x  + b requires O m  storage and O mCK  computation cost  the exact amount of storage and number of operations depends on the number of support vectors .  h x  = cid:80 m Note that if we use explicit feature vectors x ∈ RN , then the primal formulation of  the SVM problem can be used for training. The primal formulation incurs only an O N m  storage cost and evaluation requires only O N   storage and computation  b:ε 1 a:ε 1  b:ε 1 a:ε 1  X:X 1  0  1 1   6.6 Approximate kernel feature maps  131  Table 6.1 Examples of normalized shift-invariant kernels  deﬁned over x, x cid:48  ∈ RN   and their corresponding densities  deﬁned over ω ∈ RN  .  G x − x cid:48    exp cid:0  −  cid:107 x−x cid:48  cid:107 2  cid:1   cid:1  exp cid:0  −  cid:107 x − x cid:48  cid:107 1  cid:81 N  2  2  1+ xi−x cid:48 i 2  i=1   2π  −D   cid:1   p ω   2 exp cid:0  −  cid:107 ω cid:107 2  cid:81 N exp cid:0  −  cid:107 ω cid:107 1  cid:1   π 1+ω2 i    i=1  2  1  Gaussian  Laplacian  Cauchy  time: h x  = w· x + b. However, these observations are only useful if N < m, which is likely not the case when considering the explicit feature maps Φ x  induced by a kernel function. For example, given an input feature space of dimension N , the dimension of the kernel feature map for a polynomial kernel of degree d is O N d . In the case of Gaussian kernels the explicit feature map dimension is inﬁnite. So clearly using explicit kernel feature maps in general is not possible and again emphasizes that using kernel functions to compute inner products implicitly is crucial.  In this section we show that a compromise is possible by constructing approximate kernel feature maps. These are feature maps with a user-speciﬁed dimension D,  Ψ x  ∈ RD, which guarantee Ψ x  · Ψ x cid:48   ≈ K x, x cid:48   when using a suﬃciently  large dimension D. To begin, we state a classical result from the ﬁeld of harmonic analysis. Theorem 6.24  Bochner’s theorem  A continuous kernel of the form K x, x cid:48   = G x− x cid:48   deﬁned over a locally compact set X is positive deﬁnite if and only if G is the Fourier transform of a non-negative measure. That is,  G x  = cid:90 X  p ω eiω·xdω,  where p is a non-negative measure. Kernels of the form K x, x cid:48   = G x − x cid:48   are called shift-invariant kernels. Note that if the kernel is scaled such that G 0  = 1, then p is in fact a probability distribution. Several examples of such kernels and their corresponding distributions are displayed in table 6.1. The next proposition provides a simpliﬁed expression in the case of real-valued kernels.  Proposition 6.25 Let K be a continuous real-valued shift-invariant kernel and let p denote its corresponding non-negative measure as in theorem 6.24. Furthermore, assume that for all x ∈ X we have K x, x  = 1 so that p is a probability distribution. Then, the following identity holds:  E  ω∼p cid:104  cid:2  cos ω · x , sin ω · x  cid:3  cid:62  cid:2  cos ω · x cid:48  , sin ω · x cid:48   cid:3  cid:105  = K x, x cid:48   .   132  Chapter 6 Kernel Methods  Proof: First, since both K and p are real-valued, it suﬃces to consider only the real portion of eix when invoking theorem 6.24. Thus, using Re[eix] = Re[cos x  + i sin x ] = cos x , we have  K x, x cid:48   = Re[K x, x cid:48  ] = cid:90 X  p ω  cos ω ·  x − x cid:48    dω .  Next, by the standard trigonometric identity cos a−b  = cos a  cos b +sin a  sin b , we have   cid:90 X p ω  cos ω ·  x − x cid:48    dω = cid:90 X p ω  cid:0  cos ω · x  cos ω · x cid:48   + sin ω · x  sin ω · x cid:48   cid:1  dω ω∼p cid:104  cid:2  cos ω · x , sin ω · x  cid:3  cid:62  cid:2  cos ω · x cid:48  , sin ω · x cid:48   cid:3  cid:105  ,  = E   cid:3   which completes the proof of the proposition.  This proposition provides the motivation for a very simple method for generating  for any D ≥ 1, an approximate kernel map Ψ ∈ R2D, deﬁned for all x ∈ X by  Ψ x  = cid:114  1  D cid:104  cos ω1 · x , sin ω1 · x , . . . , cos ωD · x , sin ωD · x  cid:105  cid:62  ,  where ωis, i = 1, . . . , D, are sampled i.i.d. according to the measure p over X corresponding to kernel K considered. Thus,   6.21   Ψ x  · Ψ x cid:48   =  1 D  D cid:88 i=1 cid:104  cos ωi · x , sin ωi · x  cid:105  cid:62  cid:104  cos ωi · x cid:48  , sin ωi · x cid:48   cid:105   is the empirical analog of the expectation computed in proposition 6.25. The follow- ing theorem shows that this empirical estimate converges uniformly over all points in a compact domain X as D grows.  Lemma 6.26 Let K be a continuously diﬀerentiable kernel function that satisﬁes the conditions of proposition 6.25 and has associated measure p. Furthermore, assume X is compact and let N denote its dimension, R denote the radius of the Euclidean ball containing X, and σ2  6.21 , the following holds for any 0   0:  p = Eω∼p[ cid:107 ω cid:107 2] < ∞. Then, for Ψ ∈ RD as deﬁned in  P cid:20  sup x,x cid:48 ∈X cid:12  cid:12 Ψ x  · Ψ x cid:48   − K x, x cid:48   cid:12  cid:12  ≥  cid:15  cid:21  ≤ 2N  2R, r  exp cid:16  −  D cid:15 2  8  cid:17  +  4rσp  .   cid:15   Where the probability is with respect to the draws of ω ∼ p and N  R, r  denotes the minimal number of balls of radius r needed to cover a ball of radius R. Proof: Deﬁne Z = {z : z = x− x cid:48 , x, x cid:48  ∈ X} and note that Z is contained in a ball of radius at most 2R. Z is a closed set since X is closed and thus Z is a compact set. For convenience, deﬁne B = N  2R, r  the number of balls of radius r needed   6.6 Approximate kernel feature maps  133  to cover Z and let zj, for j ∈ [B], denote the center of the covering balls. Thus, for any z ∈ Z there exists a j such that z = zj + δ where δ < r. Next, deﬁne S z  = Ψ x  · Ψ x cid:48   − K x, x cid:48  , where z = x − x cid:48 . Since S is continuously diﬀerentiable over the compact set Z, it is L-Lipschitz with L = supz∈Z  cid:107 ∇S z  cid:107 . Note that if L <  cid:15  2r and for all j ∈ [B] we have S zj  <  cid:15  2 , then the following inequality holds for all z = zj + δ ∈ Z:  S z  = S zj + δ  ≤ Lzj −  zj + δ  + S zj  ≤ rL +  <  cid:15  .   6.22    cid:15  2  2r and 2 . Note, all following probabilities and expectations are with respect to  The remainder of this proof bounds the probability of the events L ≥  cid:15  S zj  ≥  cid:15  the random variables ω1, . . . , ωD. To bound the probability of the ﬁrst event, we use proposition 6.25 and the  linearity of expectation, which implies the key fact E[∇ Ψ x · Ψ x cid:48   ] = ∇K x, x cid:48  .  We proceed with the following series of inequalities:  E[L2] = E cid:20  sup = E cid:20  sup ≤ 2 E cid:20  sup = 2 E cid:20  sup ≤ 4 E cid:20  sup  z∈Z cid:107 ∇S z  cid:107 2 cid:21  x,x cid:48 ∈X cid:107 ∇ Ψ x  · Ψ x cid:48    − ∇K x, x cid:48   cid:107 2 cid:21  x,x cid:48 ∈X cid:107 ∇ Ψ x  · Ψ x cid:48    cid:107 2 cid:21  + 2 sup x,x cid:48 ∈X cid:107 ∇ Ψ x  · Ψ x cid:48    cid:107 2 cid:21  + 2 sup x,x cid:48 ∈X cid:107 ∇ Ψ x  · Ψ x cid:48    cid:107 2 cid:21  ,  x,x cid:48 ∈X cid:107 ∇K x, x cid:48   cid:107 2 x,x cid:48 ∈X cid:107  E[∇ Ψ x  · Ψ x cid:48   ] cid:107 2  where the ﬁrst inequality holds due to the the inequality  cid:107 a + b cid:107 2 ≤ 2 cid:107 a cid:107 2 + 2 cid:107 b cid:107 2  which follows from Jensen’s inequality  and the subadditivity of the supremum function. The second inequality also holds by Jensen’s inequality  applied twice  and again the subadditivity of supremum function. Furthermore, using a sum- diﬀerence trigonometric identity and computing the gradient with respect to z = x − x cid:48 , yield the following for any x, x cid:48  ∈ X: ∇ Ψ x  · Ψ x cid:48    = ∇ cid:18  1 = ∇ cid:18  1  cos ωi · x  cos ωi · x cid:48   + sin ωi · x  sin ωi · x cid:48   cid:19  cos ωi ·  x − x cid:48    cid:19  = ωi sin ωi ·  x − x cid:48    .  1 D  D  D  D cid:88 i=1 D cid:88 i=1  D cid:88 i=1   134  Chapter 6 Kernel Methods  Combining the two previous results gives  1 D  ωi sin ωi ·  x − x cid:48    cid:13  cid:13  cid:13  cid:13  x,x cid:48 ∈X cid:13  cid:13  cid:13  cid:13  2 cid:21  E[L2] ≤ 4 E cid:20  sup D cid:88 i=1  cid:107 ωi cid:107  cid:17 2 cid:21  ω1,...,ωN cid:20  cid:16  1 D cid:88 i=1  cid:107 ωi cid:107 2 cid:21  = 4 E ω1,...,ωN cid:20  1 D cid:88 i=1 ω cid:2  cid:107 ω cid:107 2 cid:3  = 4σ2  ≤ 4  ≤ 4  E  E  D  D  p ,  which follows from the triangle inequality,  sin ·  ≤ 1, Jensen’s inequality and the fact that the ωis are drawn i.i.d. derive the ﬁnal expression. Thus, we can bound the probability of the ﬁrst event via Markov’s inequality:  P cid:104 L ≥   cid:15   2r cid:105  ≤ cid:16  4rσp  cid:15   cid:17 2  .   6.23    cid:15    cid:15   B cid:88 i=1  2 cid:105  ≤  Hoeﬀding’s inequality and the union bound, we can write  To bound the probability of the second event, note that, by deﬁnition, S z  is a sum of D i.i.d. variables, each bounded in absolute value by 2 D  since, for all x and x cid:48 , we have K x, x cid:48   ≤ 1 and Ψ x  · Ψ x cid:48   ≤ 1 , and E[S z ] = 0. Thus, by P cid:104 ∃j ∈ [B] : S zj  ≥  P cid:104 S zj  ≥ P cid:104  sup z∈Z S z  ≥  cid:15  cid:105  ≤ 2N  2R, r  exp cid:16  −  8  cid:17  . 2 cid:105  ≤ 2B exp cid:16  − 8  cid:17  + cid:16  4rσp  cid:15   cid:17 2  Finally, combining  6.22 ,  6.23 ,  6.24 , and the deﬁnition of B we have   cid:3  which completes the lemma. A key factor in the bound of the lemma is the covering number N  2R, r , which strongly depends on the dimension of the space N . In the following lemma, we make this dependency explicit for one especially simple case, although similar arguments hold for more general scenarios as well.   6.24   D cid:15 2  D cid:15 2  ,  Lemma 6.27 Let X ⊂ RN be a compact and let R denote the radius of the smallest  enclosing ball. Then, the following inequality holds:  N  R, r  ≤ cid:16  3R r  cid:17 N  .  Proof: First, by using the volume of balls in RN we already see that RN   r 3 N =  3R r N is a trivial upper bound on the number of balls of radius r 3 that can be packed into a ball of radius R without intersecting. Now, consider a maximal packing of at most  3R r N balls of radius r 3 into the ball of radius R. Every   6.7 Chapter notes  135  point in the ball of radius R is at distance at most r from the center of at least one of the packing balls. If this were not true, we would be able to ﬁt another ball into the packing, thereby contradicting the assumption that it is a maximal packing. Thus, if we grow the radius of the at most  3R r N balls to r, they will  cid:3  then provide a  not necessarily minimal  cover of the ball of radius R.  Finally, by combining the two previous lemmas, we can present an explicit ﬁnite sample approximation bound.  following holds  Theorem 6.28 Let K be a continuously diﬀerentiable kernel function that satisﬁes the conditions of proposition 6.25 and has associated measure p. Furthermore, assume σ2  Proof: We use lemma 6.27 in conjunction with lemma 6.26 with the following choice of r:  p = Eω∼p[ cid:107 ω cid:107 2] < ∞ and X ⊂ RN . Let R denote the radius of the Euclidean ball containing X. Then, for Ψ ∈ RD as deﬁned in  6.21  and any 0 <  cid:15  ≤ 32Rσp, the 4 N + 2  cid:19  . x,x cid:48 ∈X cid:12  cid:12 Ψ x  · Ψ x cid:48   − K x, x cid:48   cid:12  cid:12  ≥  cid:15  cid:21  ≤ cid:18  48Rσp P cid:20  sup  cid:15   cid:19 2  cid:35  2 exp cid:18  −  r = cid:34  2 6R N exp − D cid:15 2  cid:0  4σp  cid:15   cid:1 2 P cid:20  sup z∈Z S z  ≥  cid:15  cid:21  ≤ 4 cid:18  24Rσp  cid:15   cid:19  2N  4 N + 2  cid:19  .  which results in the following expression  exp cid:18  −  D cid:15 2  D cid:15 2  8    N +2  N +2  ,  Since 32Rσp  cid:15  ≥ 1, the exponent 2N proof.  N +2 can be replaced by 2, which completes the  cid:3   The previous theorem provides the guarantee that a good estimate of the kernel function can be found, with high probability, by sampling a ﬁnite number of co- ordinates D. In particular, for an absolute error of at most  cid:15  it suﬃces to sample  D = O cid:16  N   cid:15 2 log cid:16  Rσp   cid:15   cid:17  cid:17  coordinates.  6.7 Chapter notes  The mathematical theory of PDS kernels in a general setting originated with the fundamental work of Mercer [1909] who also proved the equivalence of a condition similar to that of theorem 6.2 for continuous kernels with the PDS property. The connection between PDS and NDS kernels, in particular theorems 6.18 and 6.17, are due to Schoenberg [1938]. A systematic treatment of the theory of reproducing kernel Hilbert spaces was presented in a long and elegant paper by Aronszajn [1950]. For an excellent mathematical presentation of PDS kernels and positive deﬁnite   136  Chapter 6 Kernel Methods  functions we refer the reader to Berg, Christensen, and Ressel [1984], which is also the source of several of the exercises given in this chapter.  The fact that SVMs could be extended by using PDS kernels was pointed out by Boser, Guyon, and Vapnik [1992]. The idea of kernel methods has been since then widely adopted in machine learning and applied in a variety of diﬀerent tasks and settings. The following two books are in fact speciﬁcally devoted to the study of kernel methods: Sch¨olkopf and Smola [2002] and Shawe-Taylor and Cristianini [2004]. The classical representer theorem is due to Kimeldorf and Wahba [1971]. A generalization to non-quadratic cost functions was stated by Wahba [1990]. The general form presented in this chapter was given by Sch¨olkopf, Herbrich, Smola, and Williamson [2000].  Rational kernels were introduced by Cortes, Haﬀner, and Mohri [2004]. A general class of kernels, convolution kernels, was earlier introduced by Haussler [1999]. The convolution kernels for sequences described by Haussler [1999], as well as the pair- HMM string kernels described by Watkins [1999], are special instances of rational kernels. Rational kernels can be straightforwardly extended to deﬁne kernels for ﬁnite automata and even weighted automata [Cortes et al., 2004]. Cortes, Mohri, and Rostamizadeh [2008b] study the problem of learning rational kernels such as those based on counting transducers.  The composition of weighted transducers and the ﬁlter transducers in the pres- ence of  cid:15 -paths are described in Pereira and Riley [1997], Mohri, Pereira, and Riley [2005], and Mohri [2009]. Composition can be further generalized to the N -way composition of weighted transducers [Allauzen and Mohri, 2009]. N -way compo- sition of three or more transducers can substantially speed up computation, in particular for PDS rational kernels of the form T ◦ T −1. A generic shortest-distance algorithm which can be used with a large class of semirings and arbitrary queue disciplines is described by Mohri [2002]. A speciﬁc instance of that algorithm can be used to compute the sum of the weights of all paths as needed for the computation of rational kernels after composition. For a study of the class of languages linearly separable with rational kernels, see Cortes, Kontorovich, and Mohri [2007a].  The use of cosine-based approximate kernel feature maps was introduced by Rahimi and Recht [2007], as were the corresponding uniform convergence bounds, though their proofs were not complete. Sriperumbudur and Szab´o [2015] gave an improved approximation bound that reduces the dependence on the radius of the data from O R2  to only O log R  . Bochner’s theorem, which plays a central role in deriving an approximate map, is a classical result of harmonic analysis  for ex- ample, see Rudin [1990] . The general form of the theorem is due to Weil [1965], while Solomon Bochner recognized its importance to harmonic analysis.   6.8 Exercises  6.8 Exercises  137  6.1 Let K : X × X → R be a PDS kernel, and let α : X → R be a positive function. α x α y  is a PDS  Show that the kernel K cid:48  deﬁned for all x, y ∈ X by K cid:48  x, y  = K x,y  kernel.  6.2 Show that the following kernels K are PDS:  i − y2   a  K x, y  = cos x − y  over R × R.  b  K x, y  = cos x2 − y2  over R × R.  c  For all integers n > 0, K x, y  = cid:80 N i=1 cosn x2  d  K x, y  =  x + y −1 over  0, +∞  ×  0, +∞ .  e  K x, x cid:48   = cos ∠ x, x cid:48   over Rn × Rn, where ∠ x, x cid:48   is the angle between x  f  ∀λ > 0, K x, x cid:48   = exp cid:0  − λ[sin x cid:48  − x ]2 cid:1  over R × R.  Hint: rewrite [sin x cid:48  − x ]2 as the square of the norm of the diﬀerence of two vectors.   g  ∀σ > 0, K x, y  = e−  cid:107 x−y cid:107 σ  i   over RN × RN .  over RN × RN .   Hint: you could show that K is the normalized kernel of a kernel K cid:48  and show that K cid:48   cid:107 x − y cid:107  =  is PDS using the following equality:  and x cid:48 .  dt valid for all x, y.   1 2Γ  1  2   cid:82  +∞  0  1−e−t cid:107 x−y cid:107 2  3 2  t   h  K x, y  = min x, y  − xy over [0, 1] × [0, 1].  you could consider the two integrals  cid:82  1  Hint: 0 1t∈[x,1]1t∈[y,1]dt.  1√1− x·x cid:48    over x, x cid:48  ∈ X = {x ∈ RN :  cid:107 x cid:107 2 < 1}.   i  K x, x cid:48   =   cid:82  1  0 1t∈[0,x]1t∈[0,y]dt and   Hint: one approach is to ﬁnd an explicit expression of a feature mapping Φ by considering the Taylor expansion of the kernel function.    j  ∀σ > 0, K x, y  =  1  1+  cid:107 x−y cid:107 2  0  σ2  for the proof.    Hint: the function x  cid:55 → cid:82  +∞  k  ∀σ > 0, K x, y  = exp cid:18  cid:80 N  Hint: the function  x0, y0   cid:55 → cid:82  +∞  could be useful for the proof.   σ2  0  i=1 min xi,yi   over RN × RN . e−sxe−sds deﬁned for all x ≥ 0 could be useful   cid:19  over RN × RN . 1t∈[0,x0]1t∈[0,y0]dt deﬁned over R × R   138  Chapter 6 Kernel Methods  6.3 Graph kernel. Let G =  V, E  be an undirected graph with vertex set V and edge set E. V could represent a set of documents or biosequences and E the  set of connections between them. Let w[e] ∈ R denote the weight assigned to edge e ∈ E. The weight of a path is the product of the weights of its constituent edges. Show that the kernel K over V×V where K p, q  is the sum of the weights of all paths of length two between p and q is PDS  Hint: you could introduce the matrix W =  Wpq , where Wpq = 0 when there is no edge between p and q, Wpq equal to the weight of the edge between p and q otherwise .  6.4 Symmetric diﬀerence kernel. Let X be a ﬁnite set. Show that the kernel K  deﬁned over 2X, the set of subsets of X, by  ∀A, B ∈ 2X, K A, B  = exp cid:16  −  1  2A∆B cid:17 ,  where A∆B is the symmetric diﬀerence of A and B is PDS  Hint: you could use the fact that K is the result of the normalization of a kernel function K cid:48  .  6.5 Set kernel. Let X be a ﬁnite set. Let K0 be a PDS kernel over X, show that K cid:48   deﬁned by  is a PDS kernel.  ∀A, B ∈ 2X, K cid:48  A, B  =  cid:88 x∈A,x cid:48 ∈B  K0 x, x cid:48    6.6 Show that the following kernels K are NDS:   a  K x, y  = [sin x − y ]2 over R × R.  b  K x, y  = log x + y  over  0, +∞  ×  0, +∞ .  6.7 Deﬁne a diﬀerence kernel as K x, x cid:48   = x − x cid:48  for x, x cid:48  ∈ R. Show that this  kernel is not positive deﬁnite symmetric  PDS .  6.8 Is the kernel K deﬁned over Rn × Rn by K x, y  =  cid:107 x− y cid:107 3 2 PDS? Is it NDS? 6.9 Let H be a Hilbert space with the corresponding dot product  cid:104 ·,· cid:105 . Show that the kernel K deﬁned over H × H by K x, y  = 1 −  cid:104 x, y cid:105  is negative deﬁnite.  6.10 For any p > 0, let Kp be the kernel deﬁned over R+ × R+ by  Kp x, y  = e− x+y p  .   6.25   Show that Kp is positive deﬁnite symmetric  PDS  iﬀ p ≤ 1.  Hint: you can use the fact that if K is NDS, then for any 0 < α ≤ 1, K α is also NDS.    6.8 Exercises  139  6.11 Explicit mappings.   a  Denote a data set x1, . . . , xm and a kernel K xi, xj  with a Gram matrix K. Assuming K is positive semideﬁnite, then give a map Φ ·  such that K xi, xj  =  cid:104 Φ xi , Φ xj  cid:105 .   b  Show the converse of the previous statement, i.e., if there exists a mapping Φ x  from input space to some Hilbert space, then the corresponding matrix K is positive semideﬁnite.  6.12 Explicit polynomial kernel mapping. Let K be a polynomial kernel of degree  d, i.e., K : RN × RN → R, K x, x cid:48   =  x · x cid:48  + c d, with c > 0, Show that the  dimension of the feature space associated to K is   cid:18 N + d d  cid:19 .   6.26    6.27   Write K in terms of kernels ki :  x, x cid:48    cid:55 →  x · x cid:48  i, i ∈ {0, . . . , d}. What is the weight assigned to each ki in that expression? How does it vary as a function of c?  6.13 High-dimensional mapping. Let Φ : X → H be a feature mapping such that the dimension N of H is very large and let K : X × X → R be a PDS kernel deﬁned  by  K x, x cid:48   = E  i∼D cid:2 [Φ x ]i[Φ x cid:48  ]i cid:3 ,  where [Φ x ]i is the ith component of Φ x   and similarly for Φ cid:48  x   and where D is a distribution over the indices i. We shall assume that [Φ x ]i ≤ R for all x ∈ X and i ∈ [N ]. Suppose that the only method available to compute K x, x cid:48   involved direct computation of the inner product  6.27 , which would require O N   time. Alternatively, an approximation can be computed based on random selection of a subset I of the N components of Φ x  and Φ x cid:48   according to D, that is:  K cid:48  x, x cid:48   =  D i [Φ x ]i[Φ x cid:48  ]i,   6.28   1  n cid:88 i∈I  where I = n.   a  Fix x and x cid:48  in X. Prove that  P  I∼Dn  [K x, x cid:48   − K cid:48  x, x cid:48   >  cid:15 ] ≤ 2e −n cid:15 2  2r2 .   6.29    Hint: use McDiarmid’s inequality .   140  Chapter 6 Kernel Methods   b  Let K and K cid:48  be the kernel matrices associated to K and K cid:48 . Show that , with probability at least 1 − δ,   cid:15 2 log m m+1   for any  cid:15 , δ > 0, for n > r2 K cid:48 ij − Kij ≤  cid:15  for all i, j ∈ [m].  δ  6.14 Classiﬁer based kernel. Let S be a training sample of size m. Assume that S has been generated according to some probability distribution D x, y , where  x, y  ∈ X × {−1, +1}.   a  Deﬁne the Bayes classiﬁer h∗ : X → {−1, +1}. Show that the kernel K∗ deﬁned by K∗ x, x cid:48   = h∗ x h∗ x cid:48   for any x, x cid:48  ∈ X is positive deﬁnite symmetric. What is the dimension of the natural feature space associated to K∗?   b  Give the expression of the solution obtained using SVMs with this kernel. What is the number of support vectors? What is the value of the mar- gin? What is the generalization error of the solution obtained? Under what condition are the data linearly separable?   c  Let h : X → R be an arbitrary real-valued function. Under what condition on h is the kernel K deﬁned by K x, x cid:48   = h x h x cid:48  , x, x cid:48  ∈ X, positive deﬁnite symmetric?  6.15 Image classiﬁcation kernel. For α ≥ 0, the kernel  Kα :  x, x cid:48    cid:55 →  min xkα,x cid:48 kα    6.30   N cid:88 k=1  over RN × RN is used in image classiﬁcation. Show that Kα is PDS for all α ≥ 0. To do so, proceed as follows.  a  Use the fact that  f, g   cid:55 → cid:82  +∞ t=0 f  t g t dt is an inner product over the set of measurable functions over [0, +∞  to show that  x, x cid:48    cid:55 → min x, x cid:48   is a PDS kernel.  Hint: associate an indicator function to x and another one to x cid:48 .    b  Use the result from  a  to ﬁrst show that K1 is PDS and similarly that Kα  with other values of α is also PDS.  6.16 Fraud detection. To prevent fraud, a credit-card company decides to contact Professor Villebanque and provides him with a random list of several thou- sand fraudulent and non-fraudulent events. There are many diﬀerent types of events, e.g., transactions of various amounts, changes of address or card-holder   6.8 Exercises  141  information, or requests for a new card. Professor Villebanque decides to use SVMs with an appropriate kernel to help predict fraudulent events accurately. It is diﬃcult for Professor Villebanque to deﬁne relevant features for such a diverse set of events. However, the risk department of his company has created a complicated method to estimate a probability P[U ] for any event U . Thus, Professor Villebanque decides to make use of that information and comes up with the following kernel deﬁned over all pairs of events  U, V  :  K U, V   = P[U ∧ V ] − P[U ] P[V ].   6.31   Help Professor Villebanque show that his kernel is positive deﬁnite symmetric.  6.17 Relationship between NDS and PDS kernels. Prove the statement of theo- rem 6.17.  Hint: Use the fact that if K is PDS then exp K  is also PDS, along with theorem 6.16.   6.18 Metrics and Kernels. Let X be a non-empty set and K : X×X → R be a negative  deﬁnite symmetric kernel such that K x, x  = 0 for all x ∈ X.  a  Show that there exists a Hilbert space H and a mapping Φ x  from X to H  such that:  K x, y  = Φ x  − Φ x cid:48  2 .  deﬁnes a metric on X.  Assume that K x, x cid:48   = 0 ⇒ x = x cid:48 . Use theorem 6.16 to show that √K  b  Use this result to prove that the kernel K x, y  = exp −x− x cid:48 p , x, x cid:48  ∈ R,  c  The kernel K x, x cid:48   = tanh a x · x cid:48   + b  was shown to be equivalent to a two-layer neural network when combined with SVMs. Show that K is not positive deﬁnite if a < 0 or b < 0. What can you conclude about the corresponding neural network when a < 0 or b < 0?  is not positive deﬁnite for p > 2.  6.19 Sequence kernels. Let X = {a, c, g, t}. To classify DNA sequences using SVMs, we wish to deﬁne a kernel between sequences deﬁned over X. We are given a ﬁnite set I ⊂ X∗ of non-coding regions  introns . For x ∈ X∗, denote by x the length of x and by F  x  the set of factors of x, i.e., the set of subsequences of x with contiguous symbols. For any two strings x, y ∈ X∗ deﬁne K x, y  by  K x, y  =  cid:88 z ∈ F  x ∩F  y  −I  ρz,   6.32   where ρ ≥ 1 is a real number.   142  Chapter 6 Kernel Methods   a  Show that K is a rational kernel and that it is positive deﬁnite symmetric.   b  Give the time and space complexity of the computation of K x, y  with  respect to the size s of a minimal automaton representing X∗ − I.   c  Long common factors between x and y of length greater than or equal to n are likely to be important coding regions  exons . Modify the kernel K to assign weight ρz2 Show that the resulting kernel is still positive deﬁnite symmetric.  to z when z ≥ n, ρz1 otherwise, where 1 ≤ ρ1  cid:28  ρ2.  6.20 n-gram kernel. Show that for all n ≥ 1, and any n-gram kernel Kn, Kn x, y  can be computed in linear time O x + y , for all x, y ∈ Σ∗ assuming n and the alphabet size are constants.  6.21 Mercer’s condition. Let X ⊂ RN be a compact set and K : X × X → R a  continuous kernel function. Prove that if K veriﬁes Mercer’s condition  theo- rem 6.2 , then it is PDS.  Hint: assume that K is not PDS and consider a set i,j=1 cicjK xi, xj   {x1, . . . , xm} ⊆ X and a column-vector c ∈ Rm×1 such that cid:80 m feature map Φ : X → H and kernel K x, x cid:48   = Φ x  · Φ x cid:48  .  6.22 Anomaly detection. For this problem, consider a Hilbert space H with associated  < 0.    a  First, let us consider ﬁnding the smallest enclosing sphere for a given sample  S =  x1, . . . , xm . Let c ∈ H denote the center of the sphere and let r > 0 be  its radius, then clearly the following optimization problem searches for the smallest enclosing sphere:  min  r>0,c∈H r2  subject to: ∀i ∈ [m], cid:107 Φ xi  − c cid:107 2 ≤ r2.  Show how to derive the equivalent dual optimization  max  α  αiK xi, xi  −  αiαjK xi, xj   m cid:88 i=1  m cid:88 i,j=1  m cid:88 i=1  subject to: α ≥ 0 ∧  αi = 1 ,   6.8 Exercises  143  and prove that the optimal solution satisﬁes c = cid:80 i αiΦ xi . In other words  the location of the sphere only depends on points xi with non-zero coeﬃcients αi. These points are analogous to the support vectors of SVM.   b  Consider the hypothesis class  H = {x  cid:55 → r2 −  cid:107 Φ x  − c cid:107 2 :  cid:107 c cid:107  ≤ Λ, 0 < r ≤ R} .  A hypothesis h ∈ H can be used to detect anomalies in data, where h x  ≥ 0 indicates a non-anomalous point and h x  < 0 indicates an anomaly.  Show that if supx  cid:107 Φ x  cid:107  ≤ M , then the solution to the optimization problem in part  a  is found in the hypothesis set H with Λ ≤ M and R ≤ 2M .   c  Let D denote the distribution of non-outlier points deﬁne the associated  expected loss R h  = Ex∼D[1h x <0] and empirical margin loss  cid:98 RS,ρ h  =  cid:80 m  1 m 1h xi <ρ. These losses measure errors caused by false-positive predictions, i.e. errors caused by incorrectly labeling a point anomalous.  m Φρ h xi   ≤  cid:80 m  i=1  i=1  1  i. Show that the empirical Rademacher complexity for the hypothesis class  H from part  b  can be upper bound as follows:   cid:98 RS H  ≤  R2 + Λ2  √m  + Λ cid:112 Tr[K] ,  where K is the kernel matrix constructed with the sample.  ii. Prove that with probability at least 1−δ, the following holds for all h ∈ H  and ρ ∈  0, 1]:  R h  ≤  cid:98 RS,ρ h  +  4  ρ cid:16  R2 + Λ2  √m  + Λ cid:112 Tr[K] cid:17  + cid:115  log log2  m  2 ρ  + 3 cid:115  log 4  δ 2m  .   d  Just as in the case of soft-margin SVM, we can also deﬁne a soft-margin objective for the smallest enclosing sphere that allows us tune the sensitivity to outliers in the training set by adjusting a regularization parameter C:  min  r2 + C  r>0,c∈H,ξ subject to: ∀i ∈ [m], cid:107 Φ xi  − c cid:107 2 ≤ r2 + ξi ∧ ξi ≥ 0.  ξi  m cid:88 i=1   144  Chapter 6 Kernel Methods  Show that the equivalent dual formulation of this problem is  αiαjK xi, xj   α  max  m cid:88 i=1  αiK xi, xi  −  m cid:88 i,j=1 m cid:88 i=1 and that at the optimum we have c = cid:80 m  subject to: 0 ≤ α ≤ C1 ∧  αi = 1 ,  i=1 αiΦ xi .   7 Boosting  Ensemble methods are general techniques in machine learning for combining sev- eral predictors to create a more accurate one. This chapter studies an important family of ensemble methods known as boosting, and more speciﬁcally the AdaBoost algorithm. This algorithm has been shown to be very eﬀective in practice in some scenarios and is based on a rich theoretical analysis. We ﬁrst introduce AdaBoost, show how it can rapidly reduce the empirical error as a function of the number of rounds of boosting, and point out its relationship with some known algorithms. Next, we present a theoretical analysis of the generalization properties of AdaBoost based on the VC-dimension of its hypothesis set and then based on the notion of margin. The margin theory developed in this context can be applied to other similar ensemble algorithms. A game-theoretic interpretation of AdaBoost further helps analyzing its properties and revealing the equivalence between the weak learning assumption and a separability condition. We end with a discussion of AdaBoost’s beneﬁts and drawbacks.  7.1  Introduction  It is often diﬃcult, for a non-trivial learning task, to directly devise an accurate algorithm satisfying the strong PAC-learning requirements of chapter 2. But, there can be more hope for ﬁnding simple predictors guaranteed only to perform slightly better than random. The following gives a formal deﬁnition of such weak learners. As in the PAC-learning chapter, we let n be a number such that the computational cost of representing any element x ∈ X is at most O n  and denote by size c  the maximal cost of the computational representation of c ∈ C. Deﬁnition 7.1  Weak learning  A concept class C is said to be weakly PAC-learnable if there exists an algorithm A, γ > 0, and a polynomial function poly ·,·,·  such that for any δ > 0, for all distributions D on X and for any target concept c ∈ C,   146  Chapter 7 Boosting  AdaBoost S =   x1, y1 , . . . ,  xm, ym     for i ← 1 to m do  for t ← 1 to T do  m  D1 i  ← 1 ht ← base classiﬁer in H with small error  cid:15 t = Pi∼Dt cid:2 ht xi   cid:54 = yi cid:3  αt ← 1 Zt ← 2 cid:2  cid:15 t 1 −  cid:15 t  cid:3  1   cid:46  normalization factor  2 log 1− cid:15 t  for i ← 1 to m do   cid:15 t  2  Dt+1 i  ← Dt i  exp −αtyiht xi    Zt  1  2  3  4  5  6  7  8  9  f ← cid:80 T  10 return f  t=1 αtht  Figure 7.1 AdaBoost algorithm for a base classiﬁer set H ⊆ {−1, +1}X.   7.1   the following holds for any sample size m ≥ poly 1 δ, n, size c  :  P  S∼Dm cid:104 R hS  ≤  1  2 − γ cid:105  ≥ 1 − δ ,  where hS is the hypothesis returned by algorithm A when trained on sample S. When such an algorithm A exists, it is called a weak learning algorithm for C or a weak learner. The hypotheses returned by a weak learning algorithm are called base classiﬁers.  The key idea behind boosting techniques is to use a weak learning algorithm to build a strong learner , that is, an accurate PAC-learning algorithm. To do so, boosting techniques use an ensemble method: they combine diﬀerent base classiﬁers returned by a weak learner to create a more accurate predictor. But which base classiﬁers should be used and how should they be combined? The next section addresses these questions by describing in detail one of the most prevalent and successful boosting algorithms, AdaBoost.  7.2 AdaBoost  We denote by H the hypothesis set out of which the base classiﬁers are selected, which we will sometimes refer to as the base classiﬁer set. Figure 7.1 gives the   7.2 AdaBoost  147   a    b   Figure 7.2 Example of AdaBoost with axis-aligned hyperplanes as base classiﬁers.  a  The top row shows decision boundaries at each boosting round. The bottom row shows how weights are updated at each round, with incorrectly  resp., correctly  points given increased  resp., decreased  weights.  b  Visualization of ﬁnal classiﬁer, constructed as a non-negative linear combination of base classiﬁers.  pseudocode of AdaBoost in the case where the base classiﬁers are functions mapping from X to {−1, +1}, thus H ⊆ {−1, +1}X. The algorithm takes as input a labeled sample S =   x1, y1 , . . . ,  xm, ym  , with  xi, yi  ∈ X×{−1, +1} for all i ∈ [m], and maintains a distribution over the indices {1, . . . , m}. Initially  lines 1-2 , the distribution is uniform  D1 . At each round of boosting, that is each iteration t ∈ [T ] of the loop 3–8, a new base classiﬁer ht ∈ H is selected that minimizes the error on the training sample weighted by the distribution Dt:  ht ∈ argmin h∈H  P  i∼Dt cid:2 h xi   cid:54 = yi cid:3  = argmin  h∈H  m cid:88 i=1  Dt i 1h xi  cid:54 =yi.  decision  boundary  updated weights  t = 1  t = 2  t = 3  α1  + α2  + α3  =   148  Chapter 7 Boosting  Zt is simply a normalization factor to ensure that the weights Dt+1 i  sum to one. The precise reason for the deﬁnition of the coeﬃcient αt will become clear later. For now, observe that if  cid:15 t, the error of the base classiﬁer, is less than 1 2 , then 1− cid:15 t > 1 and αt is positive  αt > 0 . Thus, the new distribution Dt+1 is deﬁned  cid:15 t from Dt by substantially increasing the weight on i if point xi is incorrectly classiﬁed  yiht xi  < 0 , and, on the contrary, decreasing it if xi is correctly classiﬁed. This has the eﬀect of focusing more on the points incorrectly classiﬁed at the next round of boosting, less on those correctly classiﬁed by ht.  After T rounds of boosting, the classiﬁer returned by AdaBoost is based on the sign of function f , which is a non-negative linear combination of the base classiﬁers ht. The weight αt assigned to ht in that sum is a logarithmic function of the ratio of the accuracy 1 −  cid:15 t and error  cid:15 t of ht. Thus, more accurate base classiﬁers are assigned a larger weight in that sum. Figure 7.2 illustrates the AdaBoost algorithm. The size of the points represents the distribution weight assigned to them at each boosting round. For any t ∈ [T ], we will denote by ft the linear combination of the base classiﬁers s=1 αshs. In particular, we have fT = f . The distribution Dt+1 can be expressed in terms of ft and the normalization factors Zs, s ∈ [t], as follows:  after t rounds of boosting: ft = cid:80 t  ∀i ∈ [m], Dt+1 i  =   7.2   We will make use of this identity several times in the proofs of the following sections. It can be shown straightforwardly by repeatedly expanding the deﬁnition of the distribution over the point xi:  .  e−yift xi  s=1 Zs  m cid:81 t  Dt+1 i  =  Dt i e−αtyiht xi   Dt−1 i e−αt−1yiht−1 xi e−αtyiht xi   Zt  =  =  Zt−1Zt  .  e−yi  s=1 αshs xi   s=1 Zs   cid:80 t m cid:81 t  The AdaBoost algorithm can be generalized in several ways:    Instead of a hypothesis with minimal weighted error, ht can be more generally  the base classiﬁer returned by a weak learning algorithm trained on Dt;   The range of the base classiﬁers could be [−1, +1], or more generally a bounded subset of R. The coeﬃcients αt can then be diﬀerent and may not even admit a closed form. In general, they are chosen to minimize an upper bound on the empirical error, as discussed in the next section. Of course, in that general case, the hypotheses ht are not binary classiﬁers, but their sign could deﬁne the label, and their magnitude could be interpreted as a measure of conﬁdence.   7.2 AdaBoost  149  In rest of this chapter, the range of the base classiﬁers in H will be assumed to be included in [−1, +1]. We now further analyze the properties of AdaBoost and discuss its typical use in practice.  7.2.1 Bound on the empirical error We ﬁrst show that the empirical error of AdaBoost decreases exponentially fast as a function of the number of rounds of boosting.  Theorem 7.2 The empirical error of the classiﬁer returned by AdaBoost veriﬁes:  2 −  cid:15 t cid:17 2 cid:21 .   cid:98 RS f   ≤ exp cid:20  − 2 Furthermore, if for all t ∈ [T ], γ ≤ cid:0  1 Proof: Using the general inequality 1u≤0 ≤ exp −u  valid for all u ∈ R and iden- T cid:89 t=1  T cid:88 t=1 cid:16  1 2 −  cid:15 t cid:1 , then  cid:98 RS f   ≤ exp −2γ2T   . m cid:88 i=1 cid:20 m m cid:88 i=1  Zt cid:21 DT +1 i  =  tity 7.2, we can write:  1yif  xi ≤0 ≤  e−yif  xi  =  m cid:88 i=1  T cid:89 t=1  1 m  1 m  1 m   7.3    7.4   Zt.  Since for all t ∈ [T ], Zt is a normalization factor, it can be expressed in terms of  cid:15 t by:   cid:98 RS f   =  Zt =  m cid:88 i=1  Dt i e−αtyiht xi  =  cid:88 i:yiht xi =+1 =  1 −  cid:15 t e−αt +  cid:15 teαt =  1 −  cid:15 t  cid:114   cid:15 t  1 −  cid:15 t  Dt i e−αt +  cid:88 i:yiht xi =−1  Dt i eαt  +  cid:15 t cid:114  1 −  cid:15 t   cid:15 t  = 2 cid:112  cid:15 t 1 −  cid:15 t  .  Thus, the product of the normalization factors can be expressed and upper bounded as follows:  Zt =  T cid:89 t=1  T cid:89 t=1  T cid:89 t=1 cid:113 1 − 4 cid:0  1  2 cid:112  cid:15 t 1 −  cid:15 t  =  2 −  cid:15 t cid:1 2 cid:105  exp cid:104  − 2 cid:0  1 2 −  cid:15 t cid:1 2 cid:105  , T cid:88 t=1 cid:0  1 where the inequality follows from the inequality 1 − x ≤ e−x valid for all x ∈ R.  cid:3   T cid:89 t=1 = exp cid:104  − 2  2 −  cid:15 t cid:1 2  Note that the value of γ, which is known as the edge, and the accuracy of the base classiﬁers do not need to be known to the algorithm. The algorithm adapts to their  ≤   150  Chapter 7 Boosting  Figure 7.3 Visualization of the zero-one loss  blue  and the convex and diﬀerentiable upper bound on the zero-one loss  red  that is optimized by AdaBoost.  accuracy and deﬁnes a solution based on these values. This is the source of the extended name of AdaBoost: adaptive boosting.  log  .  7.5   1 −  cid:15 t  cid:15 t  The proof of theorem 7.2 reveals several other important properties. First, observe that αt is the minimizer of the function ϕ : α  cid:55 →  1 −  cid:15 t e−α +  cid:15 teα. Indeed, ϕ is convex and diﬀerentiable, and setting its derivative to zero yields: 1 ϕ cid:48  α  = − 1 −  cid:15 t e−α +  cid:15 teα = 0 ⇔  1 −  cid:15 t e−α =  cid:15 teα ⇔ α = 2 Thus, αt is chosen to minimize Zt = ϕ αt  and, in light of the bound  cid:98 RS f   ≤  cid:81 T  t=1 Zt shown in the proof, these coeﬃcients are selected to minimize an upper bound on the empirical error. In fact, for base classiﬁers whose range is [−1, +1] or R, αt can be chosen in a similar fashion to minimize Zt, and this is the way AdaBoost is extended to these more general cases. Observe also that the equality  1 −  cid:15 t e−αt =  cid:15 teαt just shown in  7.5  implies that at each iteration, AdaBoost assigns equal distribution mass to correctly and incorrectly classiﬁed instances, since  1 −  cid:15 t e−αt is the total distribution assigned to correctly classiﬁed points and  cid:15 teαt that of incorrectly classiﬁed ones. This may seem to contradict the fact that AdaBoost increases the weights of incorrectly classiﬁed points and decreases that of others, but there is in fact no inconsistency: the reason is that there are always fewer incorrectly classiﬁed points, since the base classiﬁer’s accuracy is better than random.  7.2.2 Relationship with coordinate descent AdaBoost was originally designed to address the theoretical question of whether a weak learning algorithm could be used to derive a strong learning one. Here,   7.2 AdaBoost  151  we will show that it coincides in fact with a very simple algorithm, which consists of applying a general coordinate descent technique to a convex and diﬀerentiable objective function.  For simplicity, in this section, we assume that the base classiﬁer set H is ﬁnite, with cardinality N : H = {h1, . . . , hN}. An ensemble function f such as the one returned by AdaBoost can then be written as f = cid:80 N j=1 ¯αjhj, with ¯αj ≥ 0. Given a labeled sample S =   x1, y1 , . . . ,  xm, ym  , let F be the objective function deﬁned for all ¯α =  ¯α1, . . . , ¯αN   ∈ RN by  cid:80 N m cid:88 i=1  j=1 ¯αj hj  xi  .  e−yif  xi  =  m cid:88 i=1  F   ¯α  =  e−yi  1 m  1 m   7.6   Since the exponential loss u  cid:55 → e−u is an upper bound on the zero-one loss u  cid:55 → 1u≤0  see ﬁgure 7.3 , F is an upper bound on the empirical error:  1 m  m cid:88 i=1   cid:98 RS f   =  1yif  xi ≤0 ≤  1 m  m cid:88 i=1  e−yif  xi .   7.7   F is a convex function of ¯α since it is a sum of convex functions, each obtained by composition of the  convex  exponential function with an aﬃne function of ¯α. F is also diﬀerentiable since the exponential function is diﬀerentiable. We will show that F is the objective function minimized by AdaBoost.  Diﬀerent convex optimization techniques can be used to minimize F . Here, we will use a variant of the coordinate descent technique. Coordinate descent is applied over T rounds. Let ¯α0 = 0 and let ¯αt denote the parameter vector at the end of iteration t. At each round t ∈ [T ], a direction ek corresponding to the kth coordinate of ¯α in RN is selected, as well as a step size η along that direction. ¯αt is obtained from ¯αt−1 according to the update ¯αt = ¯αt−1 +ηek, where η is the step size chosen along the direction ek. Observe that if we denote by ¯gt the ensemble function deﬁned by j=1 ¯αt,jhj, then the coordinate descent update coincides with the update ¯gt = ¯gt−1 + ηhk, which is also the AdaBoost update. Thus, since both algorithms start with ¯g0 = 0, to show that AdaBoost coincides with coordinate descent applied to F , it suﬃces to show at every iteration t, coordinate descent selects the same base hypothesis hk and step η as AdaBoost. We will assume by induction that this holds up to iteration t−1, which implies the equality ¯gt−1 = ft−1, and will show then that it also holds at iteration t.  ¯αt, that is ¯gt =  cid:80 N  The variant of coordinate descent we consider here consists of selecting, at each iteration, the maximum descent direction, that is the direction ek along which the derivative of F is the largest in absolute value, and of selecting the best step along that direction, that is of choosing η to minimize F   ¯αt−1 + ηek . To give the expressions of the direction and the step at each iteration, we ﬁrst introduce similar   152  Chapter 7 Boosting  quantities to those appearing in the analysis of the boosting algorithm. For any t ∈ [T ], we deﬁne a distribution ¯Dt over the indices {1, . . . , m} as follows:   cid:80 N  ¯Zt  e−yi  j=1 ¯αt−1,j hj  xi   e−yi ¯gt−1 xi   ¯Dt i  =  =  ¯Zt   cid:80 N  ,  where ¯Zt is the normalization factor ¯Zt = cid:80 m j=1 ¯αt−1,j hj  xi . Observe that, since ¯gt−1 = ft−1, ¯Dt coincides with Dt. We also deﬁne for any base hypothesis hj, j ∈ [N ], its expected error ¯ cid:15 t,j with respect to the distribution ¯Dt:  i=1 e−yi  ¯ cid:15 t,j = E  i∼ ¯Dt cid:2 1yihj  xi ≤0 cid:3 .  The directional derivative of F at ¯αt−1 along ek is denoted by F  cid:48   ¯αt−1, ek  and deﬁned by  F   ¯αt−1 + ηek  − F   ¯αt−1   .  η  j=1 ¯αt−1,j hj  xi −ηyihk xi , the directional deriva-  F  cid:48   ¯αt−1, ek  = lim  cid:80 N η→0  1 m   cid:80 N  i=1 e−yi  yihk xi e−yi  tive along ek can be expressed as follows:  Since F   ¯αt−1 + ηek  = cid:80 m m cid:88 i=1 F  cid:48   ¯αt−1, ek  = − m cid:88 i=1 = − cid:34  m cid:88 i=1 ¯Dt i 1yihk xi =+1 − = − cid:104  1 − ¯ cid:15 t,k  − ¯ cid:15 t,k cid:105  ¯Zt  yihk xi  ¯Dt i  ¯Zt  = −  1 m  m  j=1 ¯αt−1,j hj  xi   ¯Dt i 1yihk xi =−1 cid:35  ¯Zt  m  m cid:88 i=1  = cid:104 2¯ cid:15 t,k − 1 cid:105  ¯Zt  m  .  ¯Zt m does not depend on k, the maximum descent direction k is the one mini- Since mizing ¯ cid:15 t,k. Thus, the hypothesis hk selected by coordinate descent at iteration t is the one with the smallest expected error on the sample S, where the expectation is taken with respect to ¯Dt = Dt. This matches exactly the choice made by AdaBoost at the tth round.  The step size η is selected to minimize the function along the direction ek chosen: argminη F   ¯αt−1 + η ek . Since F   ¯αt−1 + η ek  is a convex function of η, to ﬁnd the   7.2 AdaBoost  153  Figure 7.4 Examples of several convex upper bounds on the zero-one loss.  minimum, it suﬃces to set its derivative to zero:   cid:80 N  dF   ¯αt−1 + ηek   dη  = 0 ⇔ −  yihk xi e−yi  j=1 ¯αt−1,j hj  xi e−ηyihk xi  = 0  m cid:88 i=1 m cid:88 i=1 m cid:88 i=1  ⇔ −  ⇔ −  yihk xi  ¯Dt i  ¯Zte−ηyihk xi  = 0  yihk xi  ¯Dt i e−ηyihk xi  = 0  ⇔ − cid:2  1 − ¯ cid:15 t,k e−η − ¯ cid:15 t,keη cid:3  = 0  log  .  ⇔ η =  1 2  1 − ¯ cid:15 t,k ¯ cid:15 t,k  This proves that the step size chosen by coordinate descent coincides with the weight αt assigned by AdaBoost to the classiﬁer chosen in the tth round. Thus, coordinate descent applied to exponential objective F precisely coincides with AdaBoost and F can be viewed as the objective function that AdaBoost seeks to minimize.  In light of this relationship, one may wish to consider similar applications of coordinate descent to other convex and diﬀerentiable functions of ¯α upper-bounding the zero-one loss. In particular, the logistic loss x  cid:55 → log2 1 + e−x  is convex and diﬀerentiable and upper bounds the zero-one loss. Figure 7.4 shows other examples of alternative convex loss functions upper-bounding the zero-one loss. Using the logistic loss, instead of the exponential loss used by AdaBoost, leads to an objective that coincides with logistic regression.   154  Chapter 7 Boosting  7.2.3 Practical use Here, we brieﬂy describe the standard practical use of AdaBoost. An important requirement for the algorithm is the choice of the base classiﬁers or that of the weak learner. The family of base classiﬁers typically used with AdaBoost in practice is that of decision trees, which are equivalent to hierarchical partitions of the space  see chapter 9, section 9.3.3 . Among decision trees, those of depth one, also known as stumps, are by far the most frequently used base classiﬁers.  Boosting stumps are threshold functions associated to a single feature. Thus, a stump corresponds to a single axis-aligned partition of the space, as illustrated If the data is in RN , we can associate a stump to each of the N in ﬁgure 7.2. components. Thus, to determine the stump with the minimal weighted error at each round of boosting, the best component and the best threshold for each component must be computed.  To do so, we can ﬁrst presort each component in O m log m  time with a total computational cost of O mN log m . For a given component, there are only m + 1 possible distinct thresholds, since two thresholds between the same consecutive component values are equivalent. To ﬁnd the best threshold at each round of boosting, all of these possible m + 1 values can be compared, which can be done in O m  time. Thus, the total computational complexity of the algorithm for T rounds of boosting is O mN log m + mN T  .  Observe, however, that while boosting stumps are widely used in combination with AdaBoost and can perform well in practice, the algorithm that returns the stump with the minimal  weighted  empirical error is not a weak learner  see deﬁ- nition 7.1 ! Consider, for example, the simple XOR example with four data points lying in R2  see ﬁgure 6.3a , where points in the second and fourth quadrants are labeled positively and those in the ﬁrst and third quadrants negatively. Then, no decision stump can achieve an accuracy better than 1 2 .  7.3 Theoretical results  In this section we present a theoretical analysis of the generalization properties of AdaBoost.  7.3.1 VC-dimension-based analysis We start with an analysis of AdaBoost based on the VC-dimension of its hypothesis set. The family of functions FT out of which AdaBoost selects its output after T rounds of boosting is  FT = cid:40  sgn cid:32  T cid:88 t=1  αtht cid:33  : αt ≥ 0, ht ∈ H, t ∈ [T ] cid:41 .   7.8    7.3 Theoretical results  155  Figure 7.5 An empirical result using AdaBoost with C4.5 decision trees as base learners. In this example, the training error goes to zero after about 5 rounds of boosting  T ≈ 5 , yet the test error continues to decrease for larger values of T .  The VC-dimension of FT can be bounded as follows in terms of the VC-dimension d of the family of base hypothesis H  exercise 7.1 :  VCdim FT   ≤ 2 d + 1  T + 1  log2  T + 1 e  .   7.9   The upper bound grows as O dT log T  , thus, the bound suggests that AdaBoost could overﬁt for large values of T , and indeed this can occur. However, in many cases, it has been observed empirically that the generalization error of AdaBoost decreases as a function of the number of rounds of boosting T , as illustrated in ﬁgure 7.5! How can these empirical results be explained? The following sections present a margin-based analysis in support of AdaBoost that can serve as a theo- retical explanation for these empirical observations.  7.3.2 L1-geometric margin In chapter 5, we introduced the deﬁnition of conﬁdence margin and presented a series of general learning bounds based on that notion which, in particular, provided strong learning guarantees for SVMs. Here, we will similarly derive general learning bounds based on that same notion of conﬁdence margin for ensemble methods, which we will use, in particular, to derive learning guarantees for AdaBoost.  Recall that the conﬁdence margin of a real-valued function f at a point x labeled with y is the quantity yf  x . For SVMs, we also deﬁned the notion of geometric margin which, in the separable case, is a lower bound on the conﬁdence margin of a linear hypothesis with a normalized weighted vector w,  cid:107 w cid:107 2 = 1. Here, we will also deﬁne a notion of geometric margin for linear hypotheses with a norm-1 constraint, such as the ensemble hypotheses returned by AdaBoost, and similarly relate that notion to that of conﬁdence margin. This will also serve as an opportunity for us to point out the connection between several concepts and terminology used in the context of SVMs and those used in the context of boosting.  training error  r o r r e  0  test error  10  100  1000  number of rounds -        log T     156  Chapter 7 Boosting  Figure 7.6 Maximum margin hyperplanes for norm-2 and norm-∞.  First note that a function f =  cid:80 T  t=1 αtht that is a linear combination of base hypotheses h1, . . . , hT can be equivalently expressed as an inner product f = α· h, where α =  α1, . . . , αT   cid:62  and h = [h1, . . . , hT ] cid:62 . This makes the similarity between the linear hypotheses considered in this chapter and those of chapter 5 and chapter 6 evident: the vector of base hypothesis values h x  can be viewed as a feature vector associated to x, which was denoted by Φ x  in previous chapters, and α is the weight vector that was denoted by w. For ensemble linear combinations such as those returned by AdaBoost, additionally, the weight vector is non-negative: α ≥ 0. Next, we introduce a notion of geometric margin for such ensemble functions which diﬀers from the one introduced for SVMs only by the norm-1 used instead of norm-2, using the notation just introduced. Deﬁnition 7.3  L1-geometric margin  The L1-geometric margin ρf  x  of a linear func-  tion f = cid:80 T  t=1 αtht with α  cid:54 = 0 at a point x ∈ X is deﬁned by  ρf  x  = f  x   cid:107 α cid:107 1  =  cid:80 T  t=1 αtht x   cid:107 α cid:107 1  =  cid:12  cid:12 α · h x  cid:12  cid:12    cid:107 α cid:107 1  .  The L1-margin of f over a sample S =  x1, . . . , xm  is its minimum margin at the points in that sample:   7.10    7.11   ρf = min i∈[m]  ρf  xi  = min  i∈[m] cid:12  cid:12 α · h xi  cid:12  cid:12    cid:107 α cid:107 1  .  This deﬁnition of geometric margin diﬀers from deﬁnition 5.1 given in the context of the SVM algorithm only by the norm used for the weight vector: L1-norm here, L2-norm in deﬁnition 5.1. To distinguish them in the discussion that follows, let ρ1 x  denote the L1-margin and ρ2 x  the L2-margin at point x  deﬁnition 5.1 :  ρ1 x  = α · h x   and ρ2 x  = α · h x   .   cid:107 α cid:107 1   cid:107 α cid:107 2  Norm  · 2 .  Norm  · ∞ .   7.3 Theoretical results  157  ρ2 x  is then the norm-2 distance of the vector h x  to the hyperplane of equation  α·x = 0 in RT . Similarly, ρ1 x  is the norm-∞ distance of h x  to that hyperplane.  This geometric diﬀerence is illustrated by ﬁgure 7.6.8  We will denote by  f t=1 αt  =  f  cid:107 α cid:107 1  ¯f =   cid:80 T  the normalized version of the function f returned by AdaBoost. Note that if a point x with label y is correctly classiﬁed by f  or ¯f  , then the conﬁdence margin of ¯f at x coincides with the L1-geometric margin of f : y ¯f  x  = yf  x  = ρf  x . Observe  cid:107 α cid:107 1 that, since the coeﬃcients αt are non-negative, ρf  x  is then a convex combination of the base hypothesis values ht x . In particular, if the base hypotheses ht take values in [−1, +1], then ρf  x  is in [−1, +1]. 7.3.3 Margin-based analysis To analyze the generalization properties of AdaBoost, we start by examining the Rademacher complexity of convex linear ensembles. For any hypothesis set H of real-valued functions, we denote by conv H  its convex hull deﬁned by  conv H  = cid:26  p cid:88 k=1  µkhk : p ≥ 1,∀k ∈ [p], µk ≥ 0, hk ∈ H,   7.12   µk ≤ 1 cid:27 .  p cid:88 k=1  The following lemma shows that, remarkably, the empirical Rademacher complexity of conv H , which in general is a strictly larger set including H, coincides with that of H. Lemma 7.4 Let H be a set of functions mapping from X to R. Then, for any sample S, we have   cid:98 RS cid:0  conv H  cid:1  =  cid:98 RS H  .  8 More generally, for p, q ≥ 1, p and q conjugate, that is 1 of h x  to the hyperplane of equation α · h x  = 0.  p + 1  q = 1,  α·h x   cid:107 α cid:107 p  is the norm-q distance   158  Chapter 7 Boosting  Proof: The proof follows from a straightforward series of equalities:   cid:98 RS cid:0  conv H  cid:1  =  µkhk xi  cid:21  σihk xi  cid:21   h1,...,hp∈H,µ≥0, cid:107 µ cid:107 1≤1  1 m  1 m  1 m  1 m  E  E  σ cid:20  σ cid:20  σ cid:20  σ cid:20  sup  h∈H  E  E  sup  h1,...,hp∈H  sup  h1,...,hp∈H  m cid:88 i=1  σi  µk  sup  sup  µ≥0, cid:107 µ cid:107 1≤1  p cid:88 k=1 m cid:88 i=1 p cid:88 k=1 m cid:88 i=1 σihk xi  cid:21  σih xi  cid:21  =  cid:98 RS H ,  m cid:88 i=1  max k∈[p]  =  =  =  where the third equality follows the deﬁnition of the dual norm  see section A.1.2  or the observation that the maximizing vector µ for a convex combination of p  cid:3  terms is the one placing all the weight on the largest term.  This theorem can be used directly in combination with theorem 5.8 to derive the following Rademacher complexity generalization bound for convex combination en-  margin ρ.  sembles of hypotheses. Recall that  cid:98 RS,ρ h  denotes the empirical margin loss with  Corollary 7.5  Ensemble Rademacher margin bound  Let H denote a set of real-valued functions. Fix ρ > 0. Then, for any δ > 0, with probability at least 1 − δ, each of the following holds for all h ∈ conv H :  R h  ≤  cid:98 RS,ρ h  + R h  ≤  cid:98 RS,ρ h  +  2 ρ  δ 2m  Rm cid:0 H cid:1  + cid:115  log 1 ρ cid:98 RS cid:0 H cid:1  + 3 cid:115  log 2  δ 2m  2  .  Using corollary 3.8 and corollary 3.18 to bound the Rademacher complexity in terms of the VC-dimension yields immediately the following VC-dimension-based generalization bounds for convex combination ensembles of hypotheses.  Corollary 7.6  Ensemble VC-Dimension margin bound  Let H be a family of functions taking values in {+1,−1} with VC-dimension d. Fix ρ > 0. Then, for any δ > 0, with probability at least 1 − δ, the following holds for all h ∈ conv H :  2  ρ cid:114  2d log em  m  d  + cid:115  log 1  δ 2m  .  R h  ≤  cid:98 RS,ρ h  +  These bounds can be generalized to hold uniformly for all ρ ∈  0, 1], at the price as in theorem 5.9. They cannot be  of an additional term of the form  cid:113  log log2  m  2 δ   7.13    7.14    7.15    7.3 Theoretical results  159   cid:80 T  t=1 αtht  cid:107 α cid:107 1  directly applied to the function f returned by AdaBoost, since it is not a convex combination of base hypotheses, but they can be applied to its normalized version, ¯f = ∈ conv H . Notice that from the point of view of binary classiﬁca-  tion, f and ¯f are equivalent since sgn f   = sgn cid:0  f Let f =  cid:80 T  t=1 αtht denote the function deﬁning the classiﬁer returned by Ad- aBoost after T rounds of boosting when trained on sample S. Then, in view of  7.13 , for any δ > 0, the following holds with probability at least 1 − δ:   cid:107 α cid:107 1 cid:1 , thus R f   = R cid:0  f  their empirical margin losses are distinct.   cid:107 α cid:107 1 cid:1 , but  R f   ≤  cid:98 RS,ρ cid:0  ¯f cid:1  +  2 ρ  Rm cid:0 H cid:1  + cid:115  log 1  δ 2m  .   7.16   Similar bounds can be derived from  7.14  and  7.15 . Remarkably, the number of rounds of boosting T does not appear in the generalization bound  7.16 . The bound depends only on the conﬁdence margin ρ, the sample size m, and the Rademacher complexity of the family of base classiﬁers H. Thus, the bound guarantees an  eﬀective generalization if the margin loss Rρ cid:0  ¯f cid:1  is small for a relatively large ρ.  Recall that the margin loss can be upper bounded by the fraction of the points x labeled with y in the training sample with conﬁdence margin at most ρ, that is yf  x   cid:107 α cid:107 1 ≤ ρ  see  5.38  . With our deﬁnition of L1-margin, this can also be written as follows:   cid:98 RS,ρ cid:0  ¯f cid:1  ≤ {i ∈ [m] : yi ρf  xi  ≤ ρ}  m  .  Additionally, the following theorem provides a bound on the empirical margin loss, which decreases with T under conditions discussed later.  Theorem 7.7 Let f = cid:80 T rounds of boosting and assume for all t ∈ [T ] that  cid:15 t < 1 Then, for any ρ > 0, the following holds:  t=1 αtht denote the function returned by AdaBoost after T 2 , which implies αt > 0.   7.17   T cid:89 t=1 cid:113  cid:15 1−ρ  t   1 −  cid:15 t 1+ρ .   cid:98 RS,ρ  ¯f   ≤ 2T m cid:81 T  Proof: Using the general inequality 1u≤0 ≤ exp −u  valid for all u ∈ R, identity 7.2, , the equality Zt = 2 cid:112  cid:15 t 1 −  cid:15 t  from the proof of that is Dt+1 i  = e−yif  xi  t=1 Zt   160  Chapter 7 Boosting  theorem 7.2, and the deﬁnition of αt = 1  2 log  1− cid:15 t   cid:15 t    in AdaBoost, we can write:  1 m  m cid:88 i=1  1yif  xi −ρ cid:107 α cid:107 1≤0 ≤  exp −yif  xi  + ρ cid:107 α cid:107 1   1 m  1 m  m cid:88 i=1 m cid:88 i=1  =  = eρ cid:107 α cid:107 1  = 2T  Zt cid:21 DT +1 i  eρ cid:107 α cid:107 1 cid:20 m T cid:89 t=1 Zt = eρ cid:80  T cid:89 t=1 T cid:89 t=1 T cid:89 t=1 cid:104  cid:113  1− cid:15 t  cid:15 t  cid:105 ρ cid:112  cid:15 t 1 −  cid:15 t  ,  t cid:48  αt cid:48   Zt  which concludes the proof. Moreover, if for all t ∈ [T ] we have γ ≤   1 4 cid:15 1−ρ  1 −  cid:15 t 1+ρ is maximized at  cid:15 t = 1 empirical margin loss can then be bounded by  t   cid:3  2 −  cid:15 t  and ρ ≤ 2γ, then the expression 2 − γ.9 Thus, the upper bound on the   cid:98 RS,ρ  ¯f   ≤ cid:104  1 − 2γ 1−ρ 1 + 2γ 1+ρ cid:105  T Observe that  1 − 2γ 1−ρ 1 + 2γ 1+ρ =  1 − 4γ2  cid:0  1+2γ 1−2γ cid:1 ρ function of ρ since we have cid:0  1+2γ  it can be strictly upper bounded as follows  1−2γ cid:1  > 1 as a consequence of γ > 0. Thus, if ρ < γ,  . This is an increasing   7.18   .  2   1 − 2γ 1−ρ 1 + 2γ 1+ρ <  1 − 2γ 1−γ 1 + 2γ 1+γ.  The function γ  cid:55 →  1 − 2γ 1−γ 1 + 2γ 1+γ is strictly upper bounded by 1 over the interval  0, 1 2 , thus, if ρ < γ, then  1− 2γ 1−ρ 1 + 2γ 1+ρ < 1 and the right-hand side of  7.18  decreases exponentially with T . Since the condition ρ  cid:29  O 1 √m  is necessary in order for the given margin bounds to converge, this places a condition of γ  cid:29  O 1 √m  on the edge value. In practice, the error  cid:15 t of the base classiﬁer at  round t may increase as a function of t. Informally, this is because boosting presses the weak learner to concentrate on instances that are harder and harder to classify, for which even the best base classiﬁer could not achieve an error signiﬁcantly better than random. If  cid:15 t becomes close to 1 2 relatively fast as a function of t, then the bound of theorem 7.7 becomes uninformative.  9 The diﬀerential of f :  cid:15   cid:55 → log[ cid:15 1−ρ 1 −  cid:15  1+ρ] =  1 − ρ  log  cid:15  +  1 + ρ  log 1 −  cid:15   over the interval 2  − cid:15  2 − ρ  0, 1  is given by f cid:48   cid:15   = 1−ρ 2 − ρ   1 2  ,  cid:15  1−e  2 − γ  when γ ≥ ρ which implies that it is increasing over  0, 1 2 .  . Thus, f is an increasing function over  0, 1   cid:15  − 1+ρ  1− cid:15  = 2   7.3 Theoretical results  161  The analysis and discussion that precede show that if AdaBoost admits a positive  edge  γ > 0 , then, for ρ < γ, the empirical margin loss  cid:98 RS,ρ  ¯f   becomes zero for  T suﬃciently large  it decreases exponentially fast . Thus, AdaBoost achieves an L1-geometric margin of γ over the training sample. In section 7.3.5, we will see that the edge γ is positive if and only if the training sample is separable. In that case, the edge can be chosen to be as large as half the maximum L1-geometric margin ρmax that can be achieved on the sample: γ = ρmax 2 . Thus, for a separable data set, AdaBoost can asymptotically achieve a geometric margin that is at least half the maximum geometric margin, ρmax 2 .  This analysis can serve as a theoretical explanation of the empirical observation that, in some tasks, the generalization error decreases as a function of T even after the error on the training sample is zero: the geometric margin continues to increase when the training sample is separable. In  7.16 , for the ensemble function f determined by AdaBoost after T rounds, as T increases, ρ can be chosen as a larger  quantity for which the ﬁrst term on the right-hand side vanishes   cid:98 RS,ρ  ¯f   = 0   while the second term becomes more favorable since it decreases as 1 ρ .  But, does AdaBoost achieve the maximum L1-geometric margin ρmax? No. It has been shown that AdaBoost may converge, for a linearly separable sample, to a geometric margin that is signiﬁcantly smaller than the maximum margin  e.g., 1 3 instead of 3  8  .  7.3.4 Margin maximization In view of these results, several algorithms have been devised with the explicit goal of maximizing the L1-geometric margin. These algorithms correspond to diﬀerent methods for solving a linear program  LP .  By deﬁnition of the L1-margin, the maximum margin for a linearly separable  sample S =   x1, y1 , . . . ,  xm, ym   is given by  ρ = max  α  min i∈[m]  yi cid:0 α · h xi  cid:1    cid:107 α cid:107 1  .   7.19   By deﬁnition of the maximization, the optimization problem can be written as:  max  ρ  α  subject to:  yi cid:0 α · h xi  cid:1    cid:107 α cid:107 1  ≥ ρ, ∀i ∈ [m].  Since α·h xi  is invariant to the scaling of α, we can restrict ourselves to  cid:107 α cid:107 1 = 1.  cid:107 α cid:107 1 Further seeking a non-negative α as in the case of AdaBoost leads to the following   162  optimization:  Chapter 7 Boosting  max  ρ  α  subject to: yi cid:0 α · h xi  cid:1  ≥ ρ, ∀i ∈ [m];  αt = 1 cid:33  ∧ cid:16 αt ≥ 0,∀t ∈ [T ] cid:17 .   cid:32  T cid:88 t=1  This is a linear program  LP , that is, a convex optimization problem with a linear objective function and linear constraints. There are several diﬀerent methods for solving relative large LPs in practice, using the simplex method, interior-point methods, or a variety of special-purpose solutions.  Note that the solution of this algorithm diﬀers from the margin-maximization deﬁning SVMs in the separable case only by the deﬁnition of the geometric mar- gin used  L1 versus L2  and the non-negativity constraint on the weight vector. Figure 7.6 illustrates the margin-maximizing hyperplanes found using these two distinct margin deﬁnitions in a simple case. The left ﬁgure shows the SVM solu- tion, where the distance to the closest points to the hyperplane is measured with respect to the norm  cid:107  ·  cid:107 2. The right ﬁgure shows the solution for the L1-margin, where the distance to the closest points to the hyperplane is measured with respect to the norm  cid:107  ·  cid:107 ∞. By deﬁnition, the solution of the LP just described admits an L1-margin that is larger or equal to that of the AdaBoost solution. However, empirical results do not show a systematic beneﬁt for the solution of the LP. In fact, it appears that in many cases, AdaBoost outperforms that algorithm. The margin theory described does not seem suﬃcient to explain that performance.  7.3.5 Game-theoretic interpretation In this section, we show that AdaBoost admits a natural game-theoretic inter- pretation. The application of von Neumann’s theorem then helps us relate the maximum margin and the optimal edge and clarify the connection of AdaBoost’s weak-learning assumption with the notion of L1-margin. We ﬁrst introduce the deﬁnition of the edge of a base classiﬁer for a particular distribution. Deﬁnition 7.8 The edge of a base classiﬁer ht for a distribution D over the training sample S =   x1, y1 , . . . ,  xm, ym   is deﬁned by  γt D  =  1 2 −  cid:15 t =  1 2  m cid:88 i=1  yiht xi D i .   7.20   AdaBoost’s weak learning condition can now be formulated as follows: there exists γ > 0 such that for any distribution D over the training sample and any base   7.3 Theoretical results  163  Table 7.1 The loss matrix for the standard rock-paper-scissors game.  rock  paper  scissors  rock paper scissors  0 -1 +1  +1 0 -1  -1 +1 0  classiﬁer ht, the following holds:  γt D  ≥ γ.   7.21   This condition is required for the analysis of theorem 7.2 and the non-negativity of the coeﬃcients αt. We will frame boosting as a two-person zero-sum game. Deﬁnition 7.9  Zero-sum game  A ﬁnite two-person zero-sum game consists of a loss  matrix M ∈ Rm×n, where m is the number of possible actions  or pure strategies   for the row player and n the number of possible actions for the column player. The entry Mij is the loss for the row player  or equivalently the payoﬀ for the column payer  when the row player takes action i and the column player takes action j.10  An example of a loss matrix for the familiar “rock-paper-scissors” game is shown in table 7.1.  Deﬁnition 7.10  Mixed strategy  A mixed strategy for the row player is a distribution p over the m possible row actions; a mixed strategy for the column player is a distribution q over the n possible column actions. The expected loss for the row player  expected payoﬀ for the column player  with respect to the mixed strategies p and q is  [Mij] =  piMijqj = p cid:62 Mq.  E i∼p j∼q  m cid:88 i=1  n cid:88 j=1  The following is a fundamental result in game theory proven in chapter 8.  Theorem 7.11  Von Neumann’s minimax theorem  For any ﬁnite two-person zero-sum game deﬁned by the matrix M, the following equality holds:  min  max  p cid:62 Mq = max  p cid:62 Mq .  min  p  q  p  q   7.22   The common value in  7.22  is called the value of the game. The theorem states that for any two-person zero-sum game, there exists a mixed strategy for each player  10 To be consistent with the results discussed in other chapters, we consider the loss matrix as opposed to the payoﬀ matrix  its opposite .   164  Chapter 7 Boosting  such that the expected loss for one is the same as the expected payoﬀ for the other, both of which are equal to the value of the game.  Note that, given the row player’s strategy, the column player can choose a pure strategy optimizes their payoﬀ. That is, the column player can choose the sin- gle strategy corresponding the largest coordinate of the vector p cid:62 M. A similar comment applies to the reverse. Thus, an alternative and equivalent form of the minimax theorem is  min  p  max j∈[n]  p cid:62 Mej = max  q  e cid:62 i Mq,  min i∈[m]   7.23   where ei denotes the ith unit vector.  We can now view AdaBoost as a zero-sum game, where an action of the row player is the selection of a training instance xi, i ∈ [m], and an action of the column player the selection of a base learner ht, t ∈ [T ]. A mixed strategy for the row player is thus a distribution D over the training points’ indices [m]. A mixed strategy for the column player is a distribution over the based classiﬁers’ indices [T ]. This can be deﬁned from a non-negative vector α ≥ 0: the weight assigned to t ∈ [T ] is αt  cid:107 α cid:107 1. The loss matrix M ∈ {−1, +1}m×T for AdaBoost is deﬁned by Mit = yiht xi  for all  i, t  ∈ [m] × [T ]. By von Neumann’s theorem  7.23 , the following holds:  min D∈D  max t∈[T ]  D i yiht xi  = max α≥0  min i∈[m]  yiht xi ,   7.24   T cid:88 t=1  αt  cid:107 α cid:107 1  m cid:88 i=1  where D denotes the set of all distributions over the training sample. Let ρα x  t=1 αtht. The result  denote the margin of point x for the classiﬁer deﬁned by f = cid:80 T  can be rewritten as follows in terms of the margins and edges:  2γ∗ = 2 min D  max t∈[T ]  γt D  = max  α  min i∈[m]  ρα xi  = ρ∗,   7.25   where ρ∗ is the maximum margin of a classiﬁer and γ∗ the best possible edge. This result has several implications. First, it shows that the weak learning condition  γ∗ > 0  implies ρ∗ > 0 and thus the existence of a classiﬁer with positive margin, which motivates the search for a non-zero margin. AdaBoost can be viewed as an algorithm seeking to achieve such a non-zero margin, though, as discussed earlier, AdaBoost does not always achieve an optimal margin and is thus suboptimal in that respect. Furthermore, we see that the “weak learning” assumption, which originally appeared to be the weakest condition one could require for an algorithm  that of performing better than random , is in fact a strong condition: it implies that the training sample is linearly separable with margin 2γ∗ > 0. Linear separability often does not hold for the data sets found in practice.   7.4 L1-regularization  7.4 L1-regularization  165  In practice, the training sample may not be linearly separable and AdaBoost may not admit a positive edge, in which case the weak learning condition does not hold. It may also be that AdaBoost does admit a positive edge but with γ very small. In such cases, running AdaBoost may result in large total mixture weights for some base classiﬁers hj. This can be because the algorithm increasingly concentrates on a few examples that are hard to classify and whose weights keep growing. Only a few base classiﬁers might achieve the best performance for those examples and the algorithm keeps selecting them, thereby increasing their total mixture weights. These base classiﬁers with relatively large total mixture weight end up dominating in an ensemble f and therefore solely dictating the classiﬁcation decision. The performance of the resulting ensemble is typically poor since it almost entirely hinges on that of a few base classiﬁers.  There are several methods for avoiding such situations. One consists of limiting the number of rounds of boosting T , which is also known as early-stopping. Another one consists of controlling the magnitude of the mixture weights. This can be done by augmenting the objective function of AdaBoost with a regularization term based on a norm of the vector of mixture weights. Using a norm-1 regularization leads to an algorithm that we will refer to as L1-regularized AdaBoost. Given a labeled sample S =   x1, y1 , . . . ,  xm, ym  , the objective function G minimized by L1-  e−yif  xi  + λ cid:107  ¯α cid:107 1 =  regularized AdaBoost is deﬁned for all ¯α =  ¯α1, . . . , ¯αN   ∈ RN by   cid:80 N j=1 ¯αj hj  xi  + λ cid:107  ¯α cid:107 1, where, as for AdaBoost, f is an ensemble function deﬁned by f = cid:80 N  j=1 ¯αjhj, with ¯αj ≥ 0. The objective function G is a convex function of ¯α as the sum of the convex objective of AdaBoost and the norm-1 of ¯α. L1-regularized AdaBoost consists of applying coordinate-descent to the objective function G.  m cid:88 i=1  m cid:88 i=1  G  ¯α  =   7.26   e−yi  1 m  1 m  We now show that the algorithm can be directed derived from the margin-based guarantee for ensemble methods of Corollary 7.5 or Corollary 7.6. Thus, in that way, L1-regularized AdaBoost beneﬁts from a more favorable and natural theoretical guarantee than AdaBoost.  By the generalization of Corollary 7.5 to a uniform convergence bound over ρ, for any δ > 0, with probability at least 1 − δ, the following holds for all ensemble  functions f = cid:80 N  R f   ≤  j=1 ¯αjhj with  cid:107  ¯α cid:107 1 ≤ 1 and all ρ ∈  0, 1]: 1 m  Rm cid:0 H cid:1  + cid:115  log log2  1f  xi ≤ρ +  m cid:88 i=1  2 ρ  m  2 ρ  + cid:115  log 2  δ 2m  .   7.27    166  Chapter 7 Boosting  2 ρ  1 m  ρ +  e1− f  xi   and all ρ > 0:  The inequality also trivially holds for ρ > 1 since, in that case, the ﬁrst term on the right-hand side of the bound is equal to one. Indeed, in that case, by H¨older’s inequality, for any x ∈ X, we have f  x  = cid:80 N j=1 ¯αihj x  ≤  cid:107  ¯α cid:107 1 maxj∈[N ] hj x  ≤  cid:107  ¯α cid:107 1 ≤ 1 < ρ. Now, in view of the general upper bound 1u≤0 ≤ e−u valid for all u ∈ R, with probability at least 1 − δ, the following holds for all f = cid:80 N j=1 ¯αjhj with  cid:107  ¯α cid:107 1 ≤ 1 Rm cid:0 H cid:1  + cid:115  log log2 at least 1− ρ, the following inequality holds for all f = cid:80 N Rm cid:0 H cid:1  + cid:115  log log2  Since for any ρ > 0, f  ρ admits the same generalization error as f , with probability j=1 ¯αjhj with  cid:107  ¯α cid:107 1 ≤ 1 ρ  + cid:115  log 2  + cid:115  log 2  and all ρ > 0:  R f   ≤  R f   ≤  m cid:88 i=1  m cid:88 i=1  e1−f  xi  +  This inequality can be used to derive an algorithm that selects ¯α and ρ > 0 to minimize the right-hand side. The minimization with respect to ρ does not lead to a convex optimization and depends on theoretical constant factors aﬀecting the second and third terms, which may not be optimal. Thus, instead, ρ is left as a free parameter of the algorithm, typically determined via cross-validation.  δ 2m  δ 2m   7.29    7.28   1 m  2 ρ  m  m  2 ρ  2 ρ  .  .  Now, since only the ﬁrst term of the right-hand side depends on ¯α, the bound  suggests selecting ¯α as the solution of the following optimization problem:  min  cid:107  ¯α cid:107 1≤ 1  ρ  1 m  m cid:88 i=1   cid:80 N  1 m  m cid:88 i=1  e−f  xi  =  e−  j=1 ¯αj hj  xi .   7.30   Introducing a Lagrange variable λ ≥ 0, the optimization problem can be written equivalently as  min  cid:107  ¯α cid:107 1≤ 1  ρ  e−  1 m  m cid:88 i=1   cid:80 N j=1 ¯αj hj  xi  + λ cid:107  ¯α cid:107 1.  Since for any choice of ρ in the constraint of  7.30  there exists an equivalent dual variable λ in the formulation  7.31  that achieves the same optimal ¯α, λ ≥ 0 can be freely selected via cross-validation. The resulting objective function therefore precisely coincides with that of L1-regularized AdaBoost.   7.31    7.5 Discussion  7.5 Discussion  167  AdaBoost oﬀers several advantages: it is simple, its implementation is straightfor- ward, and the time complexity of each round of boosting as a function of the sample size is rather favorable. As already discussed, when using decision stumps, the time complexity of each round of boosting is in O mN  . Of course, if the dimension of the feature space N is very large, then the algorithm could become in fact quite slow.  AdaBoost additionally beneﬁts from a rich theoretical analysis. Nevertheless, there are still many theoretical questions related to the algorithm. For example, as we saw, the algorithm in fact does not maximize the margin, and yet algorithms that do maximize the margin do not always outperform it. This suggests that perhaps a ﬁner analysis based on a notion diﬀerent from that of minimal margin could shed more light on the properties of the algorithm.  A minor drawback of the algorithm is the need to select the parameter T and the base classiﬁer set. The choice of the number of rounds of boosting T  stopping criterion  is crucial to the performance of the algorithm. As suggested by the VC- dimension analysis, larger values of T can lead to overﬁtting. In practice, T is typically determined via cross-validation. The choice of the base classiﬁers is also crucial. The complexity of the family of base classiﬁers H appeared in all the bounds presented and it is important to control it in order to guarantee generalization. On the other hand, insuﬃciently complex hypothesis sets could lead to low margins.  Probably the most serious disadvantage of AdaBoost is its performance in the presence of noise, at least in some tasks. The distribution weight assigned to exam- ples that are harder to classify substantially increases with the number of rounds of boosting, by the nature of the algorithm. These examples may end up dominating the selection of the base classiﬁers, which, with a large enough number of rounds, will play a detrimental role in the deﬁnition of the linear combination deﬁned by AdaBoost. Several solutions have been proposed to address these issues. One con- sists of using a “less aggressive” objective function than the exponential function of AdaBoost, such as the logistic loss, to penalize less incorrectly classiﬁed points. Another solution is based on a regularization, e.g., the L1-regularized AdaBoost described in the previous section.  An empirical study of AdaBoost has shown that uniform noise severely damages its accuracy. This has also been corroborated by recent theoretical results showing that boosting algorithms based on convex potentials do not tolerate even low levels of random noise. Moreover, these issues have been shown to persist even when using L1-regularization or early stopping. However, the uniform noise model used in those experiments or analysis is rather unrealistic and seems unlikely to appear   168  Chapter 7 Boosting  in practice. The model assumes that a label corruption with some ﬁxed probability aﬀects all instances uniformly. Clearly, the performance of any algorithm should degrade in the presence of such noise. Empirical results suggest, however, that the performance of AdaBoost tends to degrade more than that of other algorithms for this uniform noise model.  Finally, notice that the behavior of AdaBoost in the presence of noise can be used, in fact, as a useful feature for detecting outliers, that is, examples that are incorrectly labeled or that are hard to classify. Examples with large weights after a certain number of rounds of boosting can be identiﬁed as outliers.  7.6 Chapter notes  The question of whether a weak learning algorithm could be boosted to derive a strong learning algorithm was ﬁrst posed by Kearns and Valiant [1988, 1994], who also gave a negative proof of this result for a distribution-dependent setting. The ﬁrst positive proof of this result in a distribution-independent setting was given by Schapire [1990], and later by Freund [1990].  These early boosting algorithms, boosting by ﬁltering [Schapire, 1990] or boosting by majority [Freund, 1990, 1995] were not practical. The AdaBoost algorithm introduced by Freund and Schapire [1997] solved several of these practical issues. Freund and Schapire [1997] further gave a detailed presentation and analysis of the algorithm including the bound on its empirical error, a VC-dimension analysis, and its applications to multi-class classiﬁcation and regression.  Early experiments with AdaBoost were carried out by Drucker, Schapire, and Simard [1993], who gave the ﬁrst implementation in OCR with weak learners based on neural networks and Drucker and Cortes [1995], who reported the empirical per- formance of AdaBoost combined with decision trees, in particular decision stumps. The fact that AdaBoost coincides with coordinate descent applied to an exponen- tial objective function was later shown by Duﬀy and Helmbold [1999], Mason et al. [1999], and Friedman [2000]. Friedman, Hastie, and Tibshirani [2000] also gave an interpretation of boosting in terms of additive models. They also pointed out the close connections between AdaBoost and logistic regression, in particular the fact that their objective functions have a similar behavior near zero or the fact that their expectation admit the same minimizer, and derived an alternative boosting algorithm, LogitBoost, based on the logistic loss. Laﬀerty [1999] showed how an incremental family of algorithms, including LogitBoost, can be derived from Breg- man divergences and designed to closely approximate AdaBoost when varying a parameter. Kivinen and Warmuth [1999] gave an equivalent view of AdaBoost as an entropy projection. They showed that the distribution over the sample found   7.6 Chapter notes  169  by Adaboost at each round is approximately the solution to the problem of ﬁnding the closest distribution to the one at the previous round, subject to the constraint that it be orthogonal to the vector of errors of the current base hypotheses. Here, closeness is measured by a Bregman divergence, which, for AdaBoost is the un- normalized relative entropy. Collins, Schapire, and Singer [2002] later showed that boosting and logistic regression were special instances of a common framework based on Bregman divergences and used that to give the ﬁrst convergence proof of AdaBoost. Another direct relationship between AdaBoost and logistic regression is given by Lebanon and Laﬀerty [2001] who showed that the two algorithms min- imize the same extended relative entropy objective function subject to the same feature constraints, except from an additional normalization constraint for logistic regression.  A margin-based analysis of AdaBoost was ﬁrst presented by Schapire, Freund, Bartlett, and Lee [1997], including theorem 7.7 which gives a bound on the empirical margin loss. Our presentation is based on the elegant derivation of margin bounds by Koltchinskii and Panchenko [2002] using the notion of Rademacher complexity. Rudin et al. [2004] gave an example showing that, in general, AdaBoost does not maximize the L1-margin. R¨atsch and Warmuth [2002] provided asymptotic lower bounds for the margin achieved by AdaBoost under some conditions. The L1- margin maximization based on an LP is due to Grove and Schuurmans [1998]. R¨atsch, Onoda, and M¨uller [2001] suggested a modiﬁcation of that algorithm using a soft-margin instead and pointed out its connections with SVMs. The game- theoretic interpretation of boosting and the application of von Neumann’s minimax theorem [von Neumann, 1928] in that context were pointed out by Freund and Schapire [1996, 1999b]; see also Grove and Schuurmans [1998] and Breiman [1999]. The L1-regularized AdaBoost algorithm described in Section 7.4 is presented and analyzed by R¨atsch, Mika, and Warmuth [2001]. Cortes, Mohri, and Syed [2014] introduced a new boosting algorithm, DeepBoost, which they proved to beneﬁt from ﬁner learning guarantees, including favorable ones even when using as base classiﬁer set relatively rich families, for example a family of very deep decision trees, or other similarly complex families. In DeepBoost, the decisions in each iteration of which classiﬁer to add to the ensemble and which weight to assign to that classiﬁer, depend on the  data-dependent  complexity of the sub-family to which the classiﬁer belongs. Cortes, Mohri, and Syed [2014] further showed that empirically DeepBoost achieves a better performance than AdaBoost, Logistic Regression, and their L1- regularized variants. Both AdaBoost and L1-regularized AdaBoost can be viewed as special instances of DeepBoost.  Dietterich [2000] provided extensive empirical evidence for the fact that uniform noise can severely damage the accuracy of AdaBoost. This has been reported by   170  Chapter 7 Boosting  a number of other authors since then. Long and Servedio [2010] further recently showed the failure of boosting algorithms based on convex potentials to tolerate random noise, even with L1-regularization or early stopping.  There are several excellent surveys and tutorials related to boosting [Schapire, 2003, Meir and R¨atsch, 2002, Meir and R¨atsch, 2003], including the recent book of Schapire and Freund [2012] fully dedicated to this topic, with an extensive list of references and a detailed presentation.  7.7 Exercises  7.1 VC-dimension of the hypothesis set of AdaBoost. Prove the upper bound on the VC-dimension of the hypothesis set FT of AdaBoost after T rounds of boosting, as stated in equation  7.9 .  7.2 Alternative objective functions. This problem studies boosting-type algorithms deﬁned with objective functions diﬀerent from that of AdaBoost. We assume that the training data are given as m labeled examples  x1, y1 , . . . ,  xm, ym  ∈ X × {−1, +1}. We further assume that Φ is a strictly increasing convex and diﬀerentiable function over R such that: ∀x ≥ 0, Φ x  ≥ 1 and ∀x   0.  a  Consider the loss function L α  =  cid:80 m combination of base classiﬁers, i.e., f = cid:80 T  i=1 Φ −yif  xi   where f is a linear t=1 αtht  as in AdaBoost . Derive In particular, a new boosting algorithm using the objective function L. characterize the best base classiﬁer hu to select at each round of boosting if we use coordinate descent.   b  Consider the following functions:  1  zero-one loss Φ1 −u  = 1u≤0;  2  least squared loss Φ2 −u  =  1 − u 2;  3  SVM loss Φ3 −u  = max{0, 1 − u}; and  4  logistic loss Φ4 −u  = log 1 + e−u . Which functions satisfy the assumptions on Φ stated earlier in this problem?   c  For each loss function satisfying these assumptions, derive the corresponding  boosting algorithm. How do the algorithm s  diﬀer from AdaBoost?  7.3 Update guarantee. Assume that the main weak learner assumption of AdaBoost holds. Let ht be the base learner selected at round t. Show that the base learner ht+1 selected at round t + 1 must be diﬀerent from ht.  7.4 Weighted instances. Let the training sample be S =   x1, y1 , . . . ,  xm, ym  . Suppose we wish to penalize diﬀerently errors made on xi versus xj. To do that, we associate some non-negative importance weight wi to each point xi and deﬁne   7.7 Exercises  171  the objective function F  α  = cid:80 m  t=1 αtht. Show that this function is convex and diﬀerentiable and use it to derive a boosting- type algorithm.  i=1 wie−yif  xi , where f = cid:80 T  7.5 Deﬁne the unnormalized correlation of two vectors x and x cid:48  as the inner product between these vectors. Prove that the distribution vector  Dt+1 1 , . . . , Dt+1 m   deﬁned by AdaBoost and the vector of components yiht xi  are uncorrelated.  4 negative points all at coordinate  1, 1 , another m  7.6 Fix  cid:15  ∈  0, 1 2 . Let the training sample be deﬁned by m points in the plane with m 4 negative points all at coordinate  −1,−1 , m 1− cid:15   positive points all at coordinate  1,−1 , and positive points all at coordinate  −1, +1 . Describe the behavior of AdaBoost when run on this sample using boosting stumps. What solution does the algorithm return after T rounds?  m 1+ cid:15    4  4  7.7 Noise-tolerant AdaBoost. AdaBoost may be signiﬁcantly overﬁtting in the pres- ence of noise, in part due to the high penalization of misclassiﬁed examples. To reduce this eﬀect, one could use instead the following objective function:  where G is the function deﬁned on R by  F =  m cid:88 i=1 G x  = cid:40  ex  G −yif  xi  ,  if x ≤ 0 otherwise.  x + 1   7.32    7.33    a  Show that the function G is convex and diﬀerentiable.   b  Use F and greedy coordinate descent to derive an algorithm similar to Ad-   c  Compare the reduction of the empirical error rate of this algorithm with that  aBoost.  of AdaBoost.  7.8 Simpliﬁed AdaBoost. Suppose we simplify AdaBoost by setting the parameter  αt to a ﬁxed value αt = α > 0, independent of the boosting round t.   a  Let γ be such that   1  2 −  cid:15 t  ≥ γ > 0. Find the best value of α as a function  of γ by analyzing the empirical error.   b  For this value of α, does the algorithm assign the same probability mass to correctly classiﬁed and misclassiﬁed examples at each round? If not, which set is assigned a higher probability mass?   172  Chapter 7 Boosting  AdaBoost M, tmax  1 λ1,j ← 0 for i = 1, . . . , m 2  for t ← 1 to tmax do   cid:80 m k=1 exp − Mλt k  for i = 1, . . . , m  dt,i ← exp − Mλt i  jt ← argmaxj d cid:62 t M j rt ←  d cid:62 t M j t 2 log cid:0  1+rt 1−rt cid:1  αt ← 1 λt+1 ← λt + αtej t, where ej t is 1 in position jt and 0 elsewhere.  return λtmax  cid:107 λtmax cid:107 1  3  4  5  6  7  8  Figure 7.7 AdaBoost deﬁned with respect to a matrix M, which encodes the accuracy of each weak classiﬁer on each training point.   c  Using the previous value of α, give a bound on the empirical error of the algorithm that depends only on γ and the number of rounds of boosting T .   d  Using the previous bound, show that for T > log m  2γ2 , the resulting hypothesis  is consistent with the sample of size m.   e  Let s be the VC-dimension of the base learners used. Give a bound on the  1 rounds of boosting.  Hint: Use the fact that the VC-dimension of the fam-  generalization error of the consistent hypothesis obtained after T = cid:106  log m 2γ2  cid:107 + ily of functions {sgn  cid:80 T t=1 αtht  : αt ∈ R} is bounded by 2 s+1 T log2 eT   . be said if γ m  = O  cid:113  log m m  ?   Suppose now that γ varies with m. Based on the bound derived, what can  7.9 AdaBoost example.  In this exercise we consider a concrete example that consists of eight training points and eight weak classiﬁers.   a  Deﬁne an m × n matrix M where Mij = yihj xi , i.e., Mij = +1 if training example i is classiﬁed correctly by weak classiﬁer hj, and −1 otherwise. Let dt, λt ∈ Rn,  cid:107 dt cid:107 1 = 1 and dt,i  respectively λt,i  equal the ith component  of dt  respectively λt . Now, consider AdaBoost as described in ﬁgure 7.7   7.7 Exercises  173  and deﬁne M as below with eight training points and eight weak classiﬁers.  M =    1  −1 −1 1 1 −1 1 −1 1 −1 1  1  1 1 −1 −1 1  1 −1 −1 1  1 1 −1 1  1  1 1 −1 1  1  1 1 −1 1  1  1  1  1  1 1 1 −1 1 −1 1 1 −1  1 1 −1  1  1 1 −1 −1  1 −1 1 −1 1 1  1  1    Assume that we start with the following initial distribution over the data- points:  d1 = cid:18  3 − √5  8  3 − √5  ,  ,  8  1 6  ,  1 6  ,  1 6  ,  √5 − 1  ,  √5 − 1  8  8  , 0 cid:19  cid:62   Compute the ﬁrst few steps of the AdaBoost algorithm using M, d1, and tmax = 7. What weak classiﬁer is picked at each round of boosting? Do you notice any pattern?   b  What is the L1 norm margin produced by AdaBoost for this example?   c  Instead of using AdaBoost, imagine we combined our classiﬁers using the 16 . What is the margin in this  [2, 3, 4, 1, 2, 2, 1, 1] × 1 following coeﬃcients: case? Does AdaBoost maximize the margin?  7.10 Boosting in the presence of unknown labels. Consider the following variant of the classiﬁcation problem where, in addition to the positive and negative labels +1 and −1, points may be labeled with 0. This can correspond to cases where the true label of a point is unknown, a situation that often arises in practice, or more generally to the fact that the learning algorithm incurs no loss for predicting −1 or +1 for such a point. Let X be the input space and let Y = {−1, 0, +1}. As in standard binary classiﬁcation, the loss of f : X → R on a pair  x, y  ∈ X × Y is deﬁned by 1yf  x <0. Consider a sample S =   x1, y1 , . . . ,  xm, ym   ∈  X× Y m and a hypothesis set H of base functions taking values in {−1, 0, +1}. For a base hypothesis ht ∈ H and a distribution Dt over indices i ∈ [m], deﬁne  cid:15 s t for s ∈ {−1, 0, +1} by t = Ei∼Dt[1yiht xi =s].  cid:15 s   174  Chapter 7 Boosting   a  Derive a boosting-style algorithm for this setting in terms of  cid:15 s  t s, using the same objective function as that of AdaBoost. You should carefully justify the deﬁnition of the algorithm.   b  What is the weak-learning assumption in this setting?   c  Write the full pseudocode of the algorithm.   d  Give an upper bound on the training error of the algorithm as a function of  the number of rounds of boosting and  cid:15 s  t s.  7.11 HingeBoost. As discussed in the chapter, AdaBoost can be viewed as coordinate descent applied to an exponential objective function. Here, we consider an alternative ensemble method algorithm, HingeBoost, that consists of applying coordinate descent to an objective function based on the hinge loss. Consider  the function F deﬁned for all α ∈ RN by  F  α  =  m cid:88 i=1  max0, 1 − yi  N cid:88 j=1  αjhj xi  ,   7.34   where the hjs are base classiﬁers belonging to a hypothesis set H of functions taking values −1 or +1.   a  Show that F is convex and admits a right- and left-derivative along any  direction.   b  For any j ∈ [N ], let ej denote the direction corresponding to the base hy- pothesis hj. Let αt denote the vector of coeﬃcients αt,j, j ∈ [N ] obtained j=1 αt,jhj the predic-  after t ≥ 0 iterations of coordinate descent and ft = cid:80 N  tor obtained after t iterations. Give the expression of the right-derivative F  cid:48 + αt−1, ej  and the left-derivative F  cid:48  − αt−1, ej  after t − 1 iterations in terms of ft−1.   c  For any j ∈ [N ], deﬁne the maximum directional derivative δF  αt−1, ej  at  αt−1 as follows:  δF  αt−1, ej  =  0 F  cid:48 + αt−1, ej  F  cid:48  − αt−1, ej   if F  cid:48  if F  cid:48  if 0 ≤ F  cid:48   − αt−1, ej  ≤ 0 ≤ F  cid:48 + αt−1, ej  − αt−1, ej  ≤ F  cid:48 + αt−1, ej  ≤ 0 − αt−1, ej  ≤ F  cid:48 + αt−1, ej .    The direction ej considered by the coordinate descent considered here is the one maximizing δF  αt−1, ej . Once the best direction j is selected, the   7.7 Exercises  175  step η can be determined by minimizing F  αt−1 + ηej  using a grid search. Give the pseudocode of HingeBoost.  7.12 Empirical margin loss boosting. As discussed in the chapter, AdaBoost can be viewed as coordinate descent applied to a convex upper bound on the empirical error. Here, we consider an algorithm seeking to minimize the empirical margin i=1 1yif  xi ≤ρ denote the empirical for a labeled sample  loss. For any 0 ≤ ρ < 1 let  cid:98 RS,ρ f   = 1  margin loss of a function f of the form f = S =   x1, y1 , . . . ,  xm, ym  .  m cid:80 m   cid:80 T  cid:80 T  t=1 αtht t=1 αt   a  Show that  cid:98 RS,ρ f   can be upper bounded as follows:  αtht xi  + ρ  1 m   cid:98 RS,ρ f   ≤  Gρ α  =  1 m  exp cid:32 −yi m cid:88 i=1 exp−yi  m cid:88 i=1  T cid:88 t=1  N cid:88 j=1  αjhj xi  + ρ  αt cid:33  . T cid:88 t=1 αj ,  N cid:88 j=1   b  For any ρ > 0, let Gρ be the objective function deﬁned for all α ≥ 0 by  with hj ∈ H for all j ∈ [N ], with the notation used in class in the boosting lecture. Show that Gρ is convex and diﬀerentiable.   c  Derive a boosting-style algorithm Aρ by applying  maximum  coordinate descent to Gρ. You should justify in detail the derivation of the algorithm, in particular the choice of the base classiﬁer selected at each round and that of the step. Compare both to their counterparts in AdaBoost.   d  What is the equivalent of the weak learning assumption for Aρ  Hint: use  non-negativity of the step value ?   e  Give the full pseudocode of the algorithm Aρ. What can you say about the  A0 algorithm?   f  Provide a bound on  cid:98 RS,ρ f  . i. Prove the upper bound  cid:98 RS,ρ f   ≤ exp cid:16  cid:80 T  t=1 Zt, where the normalization factors Zt are deﬁned as in the case of AdaBoost  with αt the step chosen by Aρ at round t .  t=1 αtρ cid:17  cid:81 T  ii. Give the expression of Zt as a function of ρ and  cid:15 t, where  cid:15 t is the weighted error of the hypothesis found by Aρ at round t  deﬁned in the same way   176  Chapter 7 Boosting  as for AdaBoost in class . Use that to prove the following upper bound   cid:98 RS,ρ f   ≤ cid:16 u  1+ρ  2 + u− 1−ρ  2  cid:17 T T cid:89 t=1 cid:113  cid:15 1−ρ  t   1 −  cid:15 t 1+ρ,  where u = 1−ρ 1+ρ .  iii. Assume that for all t ∈ [T ], 1−ρ  previous question to show that  2 −  cid:15 t > γ > 0. Use the result of the  2γ2T  1 − ρ2 cid:19  .   cid:98 RS,ρ f   ≤ exp cid:18 − 2  cid:17  cid:113  cid:15 1−ρ   Hint: you can use without proof the following identity:  1+ρ  2 + u− 1−ρ   1 −  cid:15 t 1+ρ ≤ 1 − 2 cid:0  1−ρ  cid:16 u 2 −  cid:15 t > 0.  Show that for T ≥  log m  1−ρ2   valid for 1−ρ training data have margin at least ρ.  2γ2  t  2 −  cid:15 t cid:1 2  ,  1 − ρ2 , all points of the   8 On-Line Learning  This chapter presents an introduction to on-line learning, an important area with a rich literature and multiple connections with game theory and optimization that is increasingly inﬂuencing the theoretical and algorithmic advances in machine learn- ing. In addition to the intriguing novel learning theory questions that they raise, on-line learning algorithms are particularly attractive in modern applications since they provide an eﬃcient solution for large-scale problems.  These algorithms process one sample at a time with an update per iteration that is often computationally cheap and easy to implement. As a result, they are typically signiﬁcantly more eﬃcient both in time and space and more practical than batch algorithms, when processing modern data sets of several million or billion points. They are also typically easy to implement. Moreover, on-line algorithms do not require any distributional assumption; their analysis assumes an adversarial scenario. This makes them applicable in a variety of scenarios where the sample points are not drawn i.i.d. or according to a ﬁxed distribution.  We ﬁrst introduce the general scenario of on-line learning, then present and an- alyze several key algorithms for on-line learning with expert advice, including the deterministic and randomized weighted majority algorithms for the zero-one loss and an extension of these algorithms for convex losses. We also describe and analyze two standard on-line algorithms for linear classiﬁcation, the Perceptron and Win- now algorithms, as well as some extensions. While on-line learning algorithms are designed for an adversarial scenario, they can be used, under some assumptions, to derive accurate predictors for a distributional scenario. We derive learning guaran- tees for this on-line to batch conversion. Finally, we brieﬂy point out the connection of on-line learning with game theory by describing its use to derive a simple proof of von Neumann’s minimax theorem.   178  Chapter 8 On-Line Learning  8.1  Introduction  The learning framework for on-line algorithms is in stark contrast to the PAC learning or stochastic models discussed up to this point. First, instead of learning from a training set and then testing on a test set, the on-line learning scenario mixes the training and test phases. Second, PAC learning follows the key assumption that the distribution over data points is ﬁxed over time, both for training and test points, and that points are sampled in an i.i.d. fashion. Under this assumption, the natural goal is to learn a hypothesis with a small expected loss or generalization In contrast, with on-line learning, no distributional assumption is made, error. and thus there is no notion of generalization. Instead, the performance of on-line learning algorithms is measured using a mistake model and the notion of regret. To derive guarantees in this model, theoretical analyses are based on a worst-case or adversarial assumption.  The general on-line setting involves T rounds. At the tth round, the algorithm It then receives  receives an instance xt ∈ X and makes a prediction  cid:98 yt ∈ Y. the true label yt ∈ Y and incurs a loss L  cid:98 yt, yt , where L : Y × Y → R+ is a loss function. More generally, the prediction domain for the algorithm may be Y cid:48   cid:54 = Y and the loss function deﬁned over Y cid:48  × Y. For classiﬁcation problems, we often have Y = {0, 1} and L y, y cid:48   = y cid:48  − y, while for regression Y ⊆ R and typically L y, y cid:48   =  y cid:48  − y 2. The objective in the on-line setting is to minimize the cumulative loss:  cid:80 T  t=1 L  cid:98 yt, yt  over T rounds.  8.2 Prediction with expert advice  We ﬁrst discuss the setting of online learning with expert advice, and the associated notion of regret. In this setting, at the tth round, in addition to receiving xt ∈ X, the algorithm also receives advice yt,i ∈ Y, i ∈ [N ], from N experts. Following the general framework of on-line algorithms, it then makes a prediction, receives the true label, and incurs a loss. After T rounds, the algorithm has incurred a cumu- lative loss. The objective in this setting is to minimize the regret RT , also called external regret, which compares the cumulative loss of the algorithm to that of the best expert in hindsight after T rounds:  RT =  T cid:88 t=1  L  cid:98 yt, yt  −  N min i=1  T cid:88 t=1  L  cid:98 yt,i, yt .   8.1    8.2 Prediction with expert advice  179  Figure 8.1 Weather forecast: an example of a prediction problem based on expert advice.  This problem arises in a variety of diﬀerent domains and applications. Figure 8.1 illustrates the problem of predicting the weather using several forecasting sources as experts.  8.2.1 Mistake bounds and Halving algorithm Here, we assume that the loss function is the standard zero-one loss used in classi- ﬁcation. To analyze the expert advice setting, we ﬁrst consider the realizable case, that is the setting where at least one of the experts makes no errors. As such, we discuss the mistake bound model , which asks the simple question “How many mistakes before we learn a particular concept?” Since we are in the realizable case, after some number of rounds T , we will learn the concept and no longer make errors in subsequent rounds. For any ﬁxed concept c, we deﬁne the maximum number of mistakes a learning algorithm A makes as  MA c  = max  x1,...,xT mistakes A, c .  Further, for any concept in a concept class C, the maximum number of mistakes a learning algorithm makes is   8.2    8.3   MA C  = max c∈C  MA c .  Our goal in this setting is to derive mistake bounds, that is, a bound M on MA C . We will ﬁrst do this for the Halving algorithm, an elegant and simple algorithm for which we can guarantee surprisingly favorable mistake bounds. At each round, the Halving algorithm makes its prediction by taking the majority vote over all active experts. After any incorrect prediction, it deactivates all experts that gave faulty advice. Initially, all experts are active, and by the time the algorithm has converged to the correct concept, the active set contains only those experts that are consistent with the target concept. The pseudocode for this algorithm is shown in ﬁgure 8.2. We also present straightforward mistake bounds in theorems 8.1 and 8.2, where the former deals with ﬁnite hypothesis sets and the latter relates mistake bounds to VC-dimension. Note that the hypothesis complexity term in theorem 8.1 is identical to the corresponding complexity term in the PAC model bound of theorem 2.5.  wunderground.com bbc.com  weather.com  cnn.com  algorithm  ?   180  Chapter 8 On-Line Learning  Halving H  1 H1 ← H 2  for t ← 1 to T do Receive xt   3  4  5  6  7  8  9  Receive yt    cid:98 yt ← MajorityVote Ht, xt  if   cid:98 yt  cid:54 = yt  then  Ht+1 ← {c ∈ Ht : c xt  = yt}  else Ht+1 ← Ht  return HT +1  Figure 8.2 Halving algorithm.  Theorem 8.1 Let H be a ﬁnite hypothesis set. Then  MHalving H  ≤ log2 H.   8.4   Proof: Since at each round the algorithm makes predictions using majority vote from the active set, at each mistake, the active set is reduced by at least half. Hence, after log2 H mistakes, there can only remain one active hypothesis, and since we are in the realizable case, this hypothesis must coincide with the target  cid:3  concept.  Theorem 8.2 Let opt H  be the optimal mistake bound for H. Then,  VCdim H  ≤ opt H  ≤ MHalving H  ≤ log2 H.   8.5   Proof: The second inequality is true by deﬁnition and the third inequality holds based on theorem 8.1. To prove the ﬁrst inequality, we let d = VCdim H . Then there exists a shattered set of d points, for which we can form a complete binary tree of the mistakes with height d, and we can choose labels at each round of learning to ensure that d mistakes are made. Note that this adversarial argument is valid  cid:3  since the on-line setting makes no statistical assumptions about the data.   8.2 Prediction with expert advice  181  1  2  3  4  5  6  7  8  9  10  11  12  13  Weighted-Majority N    for i ← 1 to N do  w1,i ← 1  for t ← 1 to T do Receive xt   if  cid:80 i : yt,i=1 wt,i ≥ cid:80 i : yt,i=0 wt,i then  cid:98 yt ← 1 else  cid:98 yt ← 0 if   cid:98 yt  cid:54 = yt  then  for i ← 1 to N do  Receive yt   if  yt,i  cid:54 = yt  then wt+1,i ← βwt,i else wt+1,i ← wt,i  14 return wT +1  Figure 8.3 Weighted majority algorithm, yt, yt,i ∈ {0, 1}.  8.2.2 Weighted majority algorithm In the previous section, we focused on the realizable setting in which the Halving algorithm simply discarded experts after a single mistake. We now move to the non- realizable setting and use a more general and less extreme algorithm, the Weighted Majority  WM  algorithm, that weights the importance of experts as a function of their mistake rate. The WM algorithm begins with uniform weights over all N experts. At each round, it generates predictions using a weighted majority vote. After receiving the true label, the algorithm then reduces the weight of each incorrect expert by a factor of β ∈ [0, 1 . Note that this algorithm reduces to the Halving algorithm when β = 0. The pseudocode for the WM algorithm is shown in ﬁgure 8.3.  Since we are not in the realizable setting, the mistake bounds of theorem 8.1 cannot apply. However, the following theorem presents a bound on the number of mistakes mT made by the WM algorithm after T ≥ 1 rounds of on-line learning as a function of the number of mistakes made by the best expert, that is the expert   182  Chapter 8 On-Line Learning  who achieves the smallest number of mistakes for the sequence y1, . . . , yT . Let us emphasize that this is the best expert in hindsight. Theorem 8.3 Fix β ∈  0, 1 . Let mT be the number of mistakes made by algorithm WM after T ≥ 1 rounds, and m∗T be the number of mistakes made by the best of the N experts. Then, the following inequality holds: log N + m∗T log 1 β   8.6   mT ≤  log 2 1+β  .  Proof: To prove this theorem, we ﬁrst introduce a potential function. We then derive upper and lower bounds for this function, and combine them to obtain our result. This potential function method is a general proof technique that we will use throughout this chapter.  i=1 wt,i. Since predic- tions are generated using weighted majority vote, if the algorithm makes an error at round t, this implies that  For any t ≥ 1, we deﬁne our potential function as Wt = cid:80 N 2  cid:21  Wt.  Wt+1 ≤ cid:2 1 2 +  1 2 β cid:3 Wt = cid:20  1 + β  Since W1 = N and mT mistakes are made after T rounds, we thus have the following upper bound:   8.7   WT ≤ cid:20  1 + β 2  cid:21 mT  N.  Next, since the weights are all non-negative, it is clear that for any expert i, WT ≥ wT,i = βmT ,i , where mT,i is the number of mistakes made by the ith expert after T rounds. Applying this lower bound to the best expert and combining it with the upper bound in  8.8  gives us:  N  βm∗T ≤ WT ≤ cid:20  1 + β 2  cid:21 mT ⇒ m∗T log β ≤ log N + mT log cid:20  1 + β 2  cid:21  1 + β cid:21  ≤ log N + m∗T log ⇒ mT log cid:20  2  1 β  ,   8.8    cid:3   which concludes the proof.  Thus, the theorem guarantees a bound of the following form for algorithm WM:  mT ≤ O log N   + constant × mistakes of best expert.  Since the ﬁrst term varies only logarithmically as a function of N , the theorem guarantees that the number of mistakes is roughly a constant times that of the best expert in hindsight. This is a remarkable result, especially because it requires no   8.2 Prediction with expert advice  183  assumption about the sequence of points and labels generated. In particular, the sequence could be chosen adversarially. In the realizable case where m∗T = 0, the bound reduces to mT ≤ O log N   as for the Halving algorithm. 8.2.3 Randomized weighted majority algorithm In spite of the guarantees just discussed, the WM algorithm admits a drawback that aﬀects all deterministic algorithms in the case of the zero-one loss: no deterministic algorithm can achieve a regret RT = o T   over all sequences. Clearly, for any deterministic algorithm A and any t ∈ [T ], we can adversarially select yt to be 1 if the algorithm predicts 0, and choose it to be 0 otherwise. Thus, A errs at every point of such a sequence and its cumulative mistake is mT = T . Assume for example that N = 2 and that one expert always predicts 0, the other one always 1. The error of the best expert over that sequence  and in fact any sequence of that length  is then at most m∗T ≤ T  2. Thus, for that sequence, we have  RT = mT − m∗T ≥ T  2,  which shows that RT = o T   cannot be achieved in general. Note that this does not contradict the bound proven in the previous section, since for any β ∈  0, 1 , log 1 β 1+β ≥ 2. As we shall see in the next section, this negative result does not hold log 2 for any loss that is convex with respect to one of its arguments. But for the zero-one loss, this leads us to consider randomized algorithms instead. In the randomized scenario of on-line learning, we assume that a set A = {1, . . . , N} of N actions is available. At each round t ∈ [T ], an on-line algorithm A selects a distribution pt over the set of actions, receives a loss vector lt, whose ith com- ponent lt,i ∈ [0, 1] is the loss associated with action i, and incurs the expected i=1 pt,i lt,i. The total loss incurred by the algorithm over T rounds t=1 lt,i. The minimal loss of a single action is denoted by Lmin T = mini∈A LT,i. The regret RT of the algorithm after T rounds is then typically deﬁned by the diﬀerence of the loss of the algorithm and that of the best single action:11  t=1 Lt. The total loss associated to action i is LT,i =  cid:80 T  loss Lt =  cid:80 N is LT =  cid:80 T  RT = LT − Lmin T .  Here, we consider speciﬁcally the case of zero-one losses and assume that lt,i ∈ {0, 1} for all t ∈ [T ] and i ∈ A.  11 Alternative deﬁnitions of the regret with comparison classes diﬀerent from the set of single actions can be considered.   184  Chapter 8 On-Line Learning  Randomized-Weighted-Majority  N    1  2  3  4  5  6  7  8  9  10  11  12  for i ← 1 to N do  w1,i ← 1 p1,i ← 1 N  for t ← 1 to T do Receive lt  for i ← 1 to N do  if  lt,i = 1  then  wt+1,i ← βwt,i else wt+1,i ← wt,i  Wt+1 ← cid:80 N  i=1 wt+1,i for i ← 1 to N do  pt+1,i ← wt+1,i Wt+1  13 return wT +1  Figure 8.4 Randomized weighted majority algorithm.  The WM algorithm admits a straightforward randomized version, the random- ized weighted majority  RWM  algorithm. The pseudocode of this algorithm is given in ﬁgure 8.4. The algorithm updates the weight wt,i of expert i as in the case of the WM algorithm by multiplying it by β. The following theorem gives a strong guarantee on the regret RT of the RWM algorithm, showing that it is in O √T log N  . Theorem 8.4 Fix β ∈ [1 2, 1 . Then, for any T ≥ 1, the loss of algorithm RWM on any sequence can be bounded as follows:  LT ≤  log N 1 − β  +  2 − β Lmin T .   8.9   In particular, for β = max{1 2, 1 − cid:112  log N   T}, the loss can be bounded as: potential function Wt =  cid:80 N  Proof: As in the proof of theorem 8.3, we derive upper and lower bounds for the i=1 wt,i, t ∈ [T ], and combine these bounds to obtain  T + 2 cid:112 T log N .  LT ≤ Lmin   8.10    8.2 Prediction with expert advice  185  the result. By deﬁnition of the algorithm, for any t ∈ [T ], Wt+1 can be expressed as follows in terms of Wt:  Wt+1 =  cid:88 i : lt,i=0  wt,i + β  cid:88 i : lt.i=1  wt,i = Wt +  β − 1   cid:88 i : lt,i=1 = Wt +  β − 1 Wt  cid:88 i : lt,i=1  wt,i  pt,i  = Wt +  β − 1 WtLt = Wt 1 −  1 − β Lt .  Thus, since W1 = N , it follows that WT +1 = N cid:81 T t=1 1 −  1 − β Lt . On the other hand, the following lower bound clearly holds: WT +1 ≥ maxi∈[N ] wT +1,i = βLmin T . This leads to the following inequality and series of derivations after taking the log and using the inequalities log 1 − x  ≤ −x valid for all x < 1, and − log 1 − x  ≤ x + x2 valid for all x ∈ [0, 1 2]:  βLmin  T ≤ N  T cid:89 t=1  1 −  1 − β Lt  =⇒ Lmin  T  T cid:88 t=1  log β ≤ log N +  log 1 −  1 − β Lt   Lt  T cid:88 t=1  Lmin  T  =⇒ Lmin =⇒ Lmin =⇒ LT ≤  T  T  T  log β  log N  log β ≤ log N −  1 − β  log β ≤ log N −  1 − β LT log N 1 − β − 1 − β − log N 1 − β  1 − βLmin log 1 −  1 − β   1 − β +  2 − β Lmin T . T ≤ T , this also implies +  1 − β T + Lmin T .   8.11   =⇒ LT ≤ =⇒ LT ≤ This shows the ﬁrst statement. Since Lmin  LT ≤  log N 1 − β  Diﬀerentiating the upper bound with respect to β and setting it to zero gives log N   1−β 2 − T = 0, that is β = 1 − cid:112  log N   T < 1. Thus, if 1 − cid:112  log N   T ≥ 1 2, β0 = 1 − cid:112  log N   T is the minimizing value of β, otherwise the boundary value  β0 = 1 2 is the optimal value. The second statement follows by replacing β with  cid:3  β0 in  8.11 . The bound  8.10  assumes that the algorithm additionally receives as a parameter the number of rounds T . As we shall see in the next section, however, there exists a general doubling trick that can be used to relax this requirement at the price of a   186  Chapter 8 On-Line Learning  small constant factor increase. Inequality 8.10 can be written directly in terms of the regret RT of the RWM algorithm:   8.12  Thus, for N constant, the regret veriﬁes RT = O √T   and the average regret or regret per round RT  T decreases as O 1 √T  . These results are optimal, as shown by the following theorem.  RT ≤ 2 cid:112 T log N .  Theorem 8.5 Let N = 2. There exists a stochastic sequence of losses for which the  regret of any on-line learning algorithm veriﬁes E[RT ] ≥ cid:112 T  8. Proof: For any t ∈ [T ], let the vector of losses lt take the values l01 =  0, 1  cid:62  and l10 =  1, 0  cid:62  with equal probability. Then, the expected loss of any randomized algorithm A is  E[LT ] = E cid:104  T cid:88 t=1  pt · lt cid:105  =  T cid:88 t=1  pt · E[lt] =  pt,1 +   1 − pt,1  = T  2,  1 2  1 2  T cid:88 t=1  T  can be written as follows:  where we denoted by pt the distribution selected by A at round t. By deﬁnition, Lmin Lmin T = min{LT,1,LT,2} = using the fact that LT,1 + LT,2 = T . Thus, the expected regret of A is   LT,1 + LT,2 − LT,1 − LT,2  = T  2 − LT,1 − T  2,  1 2  E[RT ] = E[LT ] − E[Lmin  T ] = E[LT,1 − T  2].  Let σt, t ∈ [T ], denote Rademacher variables taking values in {−1, +1}, then LT,1 t=1 σt. Thus, introducing scalars xt = 1 2, t ∈ [T ], by the Khintchine-Kahane inequality,  D.24  we have:  1+σt 2 = T  2 + 1  t=1  can be rewritten as LT,1 =  cid:80 T E[RT ] = E cid:104  T cid:88 t=1  2 cid:80 T t = cid:112 T  8,  x2  σtxt cid:105  ≥ cid:118  cid:117  cid:117  cid:116  1  2  T cid:88 t=1  which concludes the proof.   cid:3  More generally, for T ≥ N , a lower bound of RT = Ω √T log N   can be proven for  the regret of any algorithm.  8.2.4 Exponential weighted average algorithm The WM algorithm can be extended to other loss functions L taking values in [0, 1]. The Exponential Weighted Average algorithm presented here can be viewed as that extension for the case where L is convex in its ﬁrst argument. Note that this algorithm is deterministic and yet, as we shall see, admits a very favorable regret   8.2 Prediction with expert advice  187  Exponential-Weighted-Average  N    1  2  3  4  5  6  7  8  9  for i ← 1 to N do  w1,i ← 1 for t ← 1 to T do  cid:80 N Receive xt   cid:80 N Receive yt  for i ← 1 to N do   cid:98 yt ←  i=1 wt,iyt,i  i=1 wt,i  return wT +1  wt+1,i ← wt,i e−ηL  cid:98 yt,i,yt   Figure 8.5  Exponential weighted average, L  cid:98 yt,i, yt  ∈ [0, 1]. guarantee. Figure 8.5 gives its pseudocode. At round t ∈ [T ], the algorithm’s prediction is   cid:98 yt =  cid:80 N  cid:80 N  i=1 wt,iyt,i  ,  i=1 wt,i   8.13    8.14   where yt,i is the prediction by expert i and wt,i the weight assigned by the algorithm to that expert. Initially, all weights are set to one. The algorithm then updates the weights at the end of round t according to the following rule:  wt+1,i ← wt,i e−ηL  cid:98 yt,i,yt  = e−ηLt,i,  where Lt,i is the total loss incurred by expert i after t rounds. Note that this algorithm, as well as the others presented in this chapter, are simple, since they do not require keeping track of the losses incurred by each expert at all previous rounds but only of their cumulative performance. Furthermore, this property is also computationally advantageous. The following theorem presents a regret bound for this algorithm.  Theorem 8.6 Assume that the loss function L is convex in its ﬁrst argument and takes values in [0, 1]. Then, for any η > 0 and any sequence y1, . . . , yT ∈ Y, the regret of the Exponential Weighted Average algorithm after T rounds satisﬁes  RT ≤  log N  η  +  ηT 8  .   8.15    188  Chapter 8 On-Line Learning  i=1 wt,i   8.16   i=1 wt,i  the diﬀerence of two consecutive potential values:  RT ≤ cid:112  T  2  log N .  In particular, for η = cid:112 8 log N T , the regret is bounded as using as potential Φt = log cid:80 N {1, . . . , N} with pt,i = wt,i cid:80 N Φt+1 − Φt = log cid:80 N  Proof: We apply the same potential function analysis as in previous proofs but i=1 wt,i, t ∈ [T ]. Let pt denote the distribution over . To derive an upper bound on Φt, we ﬁrst examine i=1 wt,i e−ηL  cid:98 yt,i,yt   cid:80 N  with X = −L  cid:98 yt,i, yt  ∈ [−1, 0]. To upper bound the expression appearing in the variable X − Ept[X], then Jensen’s inequality  theorem B.20  using the convexity pt cid:2 eη X−E[X] +η E[X] cid:3  cid:1  Φt+1 − Φt = log cid:0  E 8 − η E ≤ −ηL cid:0  E [ cid:98 yt,i], yt cid:1  + = −ηL  cid:98 yt, yt  +  right-hand side, we apply Hoeﬀding’s lemma  lemma D.1  to the centered random  Summing up these inequalities yields the following upper bound:  [L  cid:98 yt,i, yt ]  of L with respect to its ﬁrst argument:  = log cid:0  E   convexity of ﬁrst arg. of L   [eηX ] cid:1 ,   Hoeﬀding’s lemma   + η E  [X] =  η2 8  η2 8  η2 8  ≤  η2  pt  pt  pt  pt  .  ΦT +1 − Φ1 ≤ −η   8.17   T cid:88 t=1  η2T 8  .  L  cid:98 yt, yt  + e−ηLT ,i−log N = −η  N min i=1  LT,i−log N.  We obtain a lower bound for the same quantity as follows:  ΦT +1−Φ1 = log  e−ηLT ,i−log N ≥ log  N  max i=1  Combining the upper and lower bounds yields:  N min i=1  LT,i − log N ≤ −η  η2T 8  T cid:88 t=1  L  cid:98 yt, yt  +  +  ηT 8  ,  N min i=1  LT,i ≤  log N  η  L  cid:98 yt, yt  −  and concludes the proof.   cid:3   N cid:88 i=1  − η  T cid:88 t=1  =⇒   8.2 Prediction with expert advice  189  The optimal choice of η in theorem 8.6 requires knowledge of the horizon T , which is an apparent disadvantage of this analysis. However, we can use a standard doubling trick to eliminate this requirement, at the price of a small constant factor. This consists of dividing time into periods [2k, 2k+1−1] of length 2k with k = 0, . . . , n and in each period. The following theorem presents a regret bound when using the doubling trick to select η. A more general  T ≥ 2n− 1, and then choosing ηk = cid:113  8 log N method consists of interpreting η as a function of time, i.e., ηt =  cid:112  8 log N   t,  which can lead to a further constant factor improvement over the regret bound of the following theorem.  2k  Theorem 8.7 Assume that the loss function L is convex in its ﬁrst argument and takes values in [0, 1]. Then, for any T ≥ 1 and any sequence y1, . . . , yT ∈ Y, the regret of the Exponential Weighted Average algorithm after T rounds is bounded as follows:  √2  RT ≤  √2 − 1 cid:112  T  2  log N + cid:112 log N 2.  Proof: Let T ≥ 1 and let Ik = [2k, 2k+1 − 1], for k ∈ [0, n], with n =  cid:98 log T + 1  cid:99 . Let LIk denote the loss incurred in the interval Ik. By theorem 8.6  8.16 , for any k ∈ {0, . . . , n}, we have  Thus, we can bound the total loss incurred by the algorithm after T rounds as:  LIk −  LT =  LIk ≤  n cid:88 k=0  N min i=1  LIk,i ≤ cid:113 2k 2 log N . n cid:88 k=0  N min i=1  LIk,i +  n cid:88 k=0 cid:113 2k  log N   2  LT,i + cid:112  log N   2 ·  n cid:88 k=0  k 2 ,  2  N min i=1  ≤   8.18    8.19    8.20   where the second inequality follows from the super-additivity of min, that is mini Xi + mini Yi ≤ mini Xi + Yi  for any sequences  Xi i and  Yi i, which implies k=0 LIk,i. The geometric sum appearing in the right-  k=0 minN   cid:80 n  hand side of  8.20  can be expressed as follows:  i=1 LIk,i ≤ minN  i=1 cid:80 n √2√T + 1 − 1 2 n+1  2 − 1 √2 − 1 ≤  √2 − 1  k 2 =  2  n cid:88 k=0  √2 √T + 1  − 1  =  √2 − 1  √2√T √2 − 1  ≤  + 1.   cid:3  Plugging back into  8.20  and rearranging terms yields  8.18 . The O √T   dependency on T presented in this bound cannot be improved for general loss functions.   190  Chapter 8 On-Line Learning   cid:46  typically w0 = 0  Perceptron w0  1 w1 ← w0 2  for t ← 1 to T do Receive xt   3  4  5  6  7  8  9  Receive yt    cid:98 yt ← sgn wt · xt  if   cid:98 yt  cid:54 = yt  then  else wt+1 ← wt  wt+1 ← wt + ytxt  return wT +1  Figure 8.6 Perceptron algorithm.  8.3 Linear classiﬁcation   cid:46  more generally ηytxt, η > 0.  This section presents two well-known on-line learning algorithms for linear classiﬁ- cation: the Perceptron and Winnow algorithms.  8.3.1 Perceptron algorithm The Perceptron algorithm is one of the earliest machine learning algorithms. It is an on-line linear classiﬁcation algorithm. Thus, it learns a decision function based on a hyperplane by processing training points one at a time. Figure 8.6 gives its pseudocode.  The algorithm maintains a weight vector wt ∈ RN deﬁning the hyperplane learned, starting with an arbitrary vector w0. At each round t ∈ [T ], it predicts the label of the point xt ∈ RN received, using the current vector wt  line 4 . When  the prediction made does not match the correct label  lines 6-7 , it updates wt by adding ytxt. More generally, when a learning rate η > 0 is used, the vector added is ηytxt. This update can be partially motivated by examining the inner product of the current weight vector with ytxt, whose sign determines the classiﬁcation of xt. Just before an update, xt is misclassiﬁed and thus ytwt · xt is negative; afterward, ytwt+1 · xt = ytwt · xt + η cid:107 xt cid:107 2, thus, the update corrects the weight vector in the direction of making the inner product ytwt · xt positive by augmenting it with the quantity η cid:107 xt cid:107 2 > 0.   8.3 Linear classiﬁcation  191  Figure 8.7 An example path followed by the iterative stochastic gradient descent technique. Each inner contour indicates a region of lower elevation.   8.21   The Perceptron algorithm can be shown in fact to seek a weight vector w minimiz- ing an objective function F precisely based on the quantities  −ytw · xt , t ∈ [T ]. Since  −ytw · xt  is positive when xt is misclassiﬁed by w, F is deﬁned for all w ∈ RN by  F  w  =  1 T  T cid:88 t=1  max cid:16 0,−yt w · xt  cid:17  = E x∼ cid:98 D  [ cid:101 F  w, x ],  where  cid:101 F  w, x  = max cid:0 0,−f  x  w· x  cid:1  with f  x  denoting the label of x, and  cid:98 D is  the empirical distribution associated with the sample  x1, . . . , xT  . For any t ∈ [T ], w  cid:55 → −yt w · xt  is linear and thus convex. Since the max operator preserves convexity, this shows that F is convex. However, F is not diﬀerentiable. Never- theless, the Perceptron algorithm coincides with the application of the stochastic subgradient descent technique to F .  The stochastic  or on-line  subgradient descent technique examines one point  wt · xt = 0. hull of 0 and −ytxt, may be used for the update step  see B.4.1 . Choosing the subgradient −ytxt, we arrive at the following general update for each point xt:  xt at a time. Note, the function  cid:101 F  ·, xt  is non-diﬀerentiable for any wt where In such a case any subgradient of  cid:101 F , i.e. any vector in the convex wt+1 ← cid:40 wt − η∇w cid:101 F  wt, xt   if wt · xt  cid:54 = 0 otherwise,  wt + ηytxt   8.22   where η > 0 is a learning rate parameter. Figure 8.7 illustrates an example path  the gradient descent follows. In the speciﬁc case we are considering, w  cid:55 →  cid:101 F  w, xt  is diﬀerentiable at any w such that yt w · xt   cid:54 = 0 with ∇w cid:101 F  w, xt  = −yxt if yt w · xt    0. Thus, the stochastic gradient  w5  w4  w3  w1  w2   192  Chapter 8 On-Line Learning  descent update becomes  wt+1 ← cid:40 wt + ηytxt  wt  if yt wt · xt  ≤ 0; if yt wt · xt  > 0,   8.23   which coincides exactly with the update of the Perceptron algorithm.  The following theorem gives a margin-based upper bound on the number of mis- takes or updates made by the Perceptron algorithm when processing a sequence of T points that can be linearly separated by a hyperplane with margin ρ > 0.  Theorem 8.8 Let x1, . . . , xT ∈ RN be a sequence of T points with  cid:107 xt cid:107  ≤ r for all t ∈ [T ], for some r > 0. Assume that there exist ρ > 0 and v ∈ RN such that for all t ∈ [T ], ρ ≤ yt v·xt   cid:107 v cid:107  algorithm when processing x1, . . . , xT is bounded by r2 ρ2.  . Then, the number of updates made by the Perceptron  Proof: Let I be the subset of the T rounds at which there is an update, and let M be the total number of updates, i.e., I = M . Summing up the assumption inequalities yields:  M ρ ≤  v · cid:80 t∈I ytxt   cid:107 v cid:107    Cauchy-Schwarz inequality     deﬁnition of updates    telescoping sum, w0 = 0    telescoping sum, w0 = 0    cid:107 wt + ytxt cid:107 2 −  cid:107 wt cid:107 2   deﬁnition of updates    cid:107 wt+1 cid:107 2 −  cid:107 wt cid:107 2  =  cid:107 wT +1 cid:107   ≤ cid:13  cid:13  cid:13  cid:88 t∈I ytxt cid:13  cid:13  cid:13   wt+1 − wt  cid:13  cid:13  cid:13  = cid:13  cid:13  cid:13  cid:88 t∈I = cid:115  cid:88 t∈I = cid:115  cid:88 t∈I = cid:118  cid:117  cid:117  cid:116  cid:88 t∈I ≤ cid:115  cid:88 t∈I  ≤0  cid:107 xt cid:107 2 ≤  2 ytwt · xt   cid:123  cid:122    cid:124   + cid:107 xt cid:107 2   cid:125  √M r2.  Comparing the left- and right-hand sides gives √M ≤ r ρ, that is, M ≤ r2 ρ2.  cid:3    8.3 Linear classiﬁcation  193  By deﬁnition of the algorithm, the weight vector wT after processing T points is a  linear combination of the vectors xt at which an update was made: wT = cid:80 t∈I ytxt.  Thus, as in the case of SVMs, these vectors can be referred to as support vectors for the Perceptron algorithm.  The bound of theorem 8.8 is remarkable, since it depends only on the normalized margin ρ r and not on the dimension N of the space. This bound can be shown to be tight, that is the number of updates can be equal to r2 ρ2 in some instances  see exercise 8.3 to show the upper bound is tight .  The theorem required no assumption about the sequence of points x1, . . . , xT . A standard setting for the application of the Perceptron algorithm is one where a ﬁnite sample S of size m < T is available and where the algorithm makes multiple passes over these m points. The result of the theorem implies that when S is linearly separable, the Perceptron algorithm converges after a ﬁnite number of updates and thus passes. For a small margin ρ, the convergence of the algorithm can be quite slow, however. In fact, for some samples, regardless of the order in which the points in S are processed, the number of updates made by the algorithm is in Ω 2N    see exercise 8.1 . Of course, if S is not linearly separable, the Perceptron algorithm does not converge. In practice, it is stopped after some number of passes over S.  There are many variants of the standard Perceptron algorithm which are used in practice and have been theoretically analyzed. One notable example is the voted  Perceptron algorithm, which predicts according to the rule sgn cid:0   cid:80 t∈I ctwt  · x cid:1 ,  where ct is a weight proportional to the number of iterations that wt survives, i.e., the number of iterations between wt and wt+1.  For the following theorem, we consider the case where the Perceptron algorithm is trained via multiple passes till convergence over a ﬁnite sample that is linearly separable. In view of theorem 8.8, convergence occurs after a ﬁnite number of updates.  For a linearly separable sample S, we denote by rS the radius of the smallest origin-centered sphere containing all points in S and by ρS the largest margin of a separating hyperplane for S. We also denote by M  S  the number of updates made by the algorithm after training over S. Theorem 8.9 Assume that the data is linearly separable. Let hS be the hypothesis returned by the Perceptron algorithm after training over a sample S of size m drawn according to some distribution D. Then, the expected error of hS is bounded as follows:  E  S∼Dm  [R hS ] ≤  E  S∼Dm+1 cid:20  min cid:0 M  S , r2  m + 1  S ρ2  S cid:1    cid:21 .  Proof: Let S be a linearly separable sample of size m + 1 drawn i.i.d. according to D and let x be a point in S. If hS−{x} misclassiﬁes x, then x must be a support vector for hS. Thus, the leave-one-out error of the Perceptron algorithm on sample   194  Chapter 8 On-Line Learning  S is at most M  S  m+1 . The result then follows lemma 5.3, which relates the expected leave-one-out error to the expected error, along with the upper bound on M  S   cid:3  given by theorem 8.8.  This result can be compared with a similar one given for the SVM algorithm  with no oﬀset  in the following theorem, which is an extension of theorem 5.4. We denote by NSV S  the number of support vectors that deﬁne the hypothesis hS returned by SVMs when trained on a sample S. Theorem 8.10 Assume that the data is linearly separable. Let hS be the hypothesis returned by SVMs used with no oﬀset  b = 0  after training over a sample S of size m drawn according to some distribution D. Then, the expected error of hS is bounded as follows:  E  S∼Dm  [R hS ] ≤  E  S∼Dm+1 cid:20  min cid:0 NSV S , r2  m + 1  S ρ2  S cid:1    cid:21 .  Proof: The fact that the expected error can be upper bounded by the average fraction of support vectors  NSV S   m + 1   was already shown by theorem 5.4. Thus, it suﬃces to show that it is also upper bounded by the expected value of  r2 S   m + 1 . To do so, we will bound the leave-one-out error of the SVM algorithm for a sample S of size m + 1 by  r2 S   m + 1 . The result will then follow by lemma 5.3, which relates the expected leave-one-out error to the expected error.  S ρ2  S ρ2  Let S =  x1, . . . , xm+1  be a linearly separable sample drawn i.i.d. according to D and let x be a point in S that is misclassiﬁed by hS−{x}. We will analyze the case where x = xm+1, the analysis of other cases is similar. We denote by S cid:48  the sample  x1, . . . , xm . For any q ∈ [m + 1], let Gq denote the function deﬁned over Rq by Gq : α  cid:55 → 2 cid:80 q  cid:80 q i=1 αi − 1 i,j=1 αiαjyiyj xi · xj . Then, Gm+1 is the objective function of the dual optimization problem for SVMs associated to the sample S and Gm the one for the sample S cid:48 . Let α ∈ Rm+1 denote a solution of the dual SVM problem maxα≥0 Gm+1 α  and α cid:48  ∈ Rm+1 the vector such that  α cid:48 1, . . . , α cid:48 m  cid:62  ∈ Rm is a solution of maxα≥0 Gm α  and α cid:48 m+1 = 0. Let em+1 denote the  m + 1 th unit vector in Rm+1. By deﬁnition of α and α cid:48  as maximizers, maxβ≥0 Gm+1 α cid:48  + βem+1  ≤ Gm+1 α  and Gm+1 α − αm+1em+1  ≤ Gm α cid:48  . Thus, the quantity A = Gm+1 α  − Gm α cid:48   admits the following lower and upper bounds: max β≥0  Gm+1 α cid:48  + βem+1  − Gm α cid:48   ≤ A ≤ Gm+1 α  − Gm+1 α − αm+1em+1 . i=1 yiαixi denote the weight vector returned by SVMs for the sample S. Since hS cid:48  misclassiﬁes xm+1, xm+1 must be a support vector for hS, thus  Let w = cid:80 m+1   8.3 Linear classiﬁcation  195  ym+1w · xm+1 = 1. In view of that, the upper bound can be rewritten as follows:  Gm+1 α  − Gm+1 α − αm+1em+1   m+1 cid:88 i=1  = αm+1 −   yiαixi  ·  ym+1αm+1xm+1  +  1 2  m+1 cid:107 xm+1 cid:107 2 α2  1 2  α2 m+1 cid:107 xm+1 cid:107 2  = αm+1 1 − ym+1w · xm+1  + =  m+1 cid:107 xm+1 cid:107 2. α2  1 2 i=1 yiα cid:48 ixi. Then, for any β ≥ 0, the quantity maximized in  the lower bound can be written as  Similarly, let w cid:48  = cid:80 m Gm+1 α cid:48  + βem+1  − Gm α cid:48    = β cid:0 1 − ym+1 w cid:48  + βym+1xm+1  · xm+1 cid:1  + = β 1 − ym+1w cid:48  · xm+1  − β2 cid:107 xm+1 cid:107 2. The right-hand side is maximized for the following value of β: Plugging in this value in the right-hand side gives 1 2   1−ym+1w cid:48 ·xm+1 2  1 2  1 2  β2 cid:107 xm+1 cid:107 2  1−ym+1w cid:48 ·xm+1  .   cid:107 xm+1 cid:107 2  . Thus,  A ≥  1 2   1 − ym+1w cid:48  · xm+1 2   cid:107 xm+1 cid:107 2   cid:107 xm+1 cid:107 2 1 2 cid:107 xm+1 cid:107 2 ,  ≥  using the fact that ym+1w cid:48 ·xm+1 < 0, since xm+1 is misclassiﬁed by w cid:48 . Comparing 2 cid:107 xm+1 cid:107 2 ≤ this lower bound on A with the upper bound previously derived leads to 1 2 α2  m+1 cid:107 xm+1 cid:107 2, that is  1  The analysis carried out in the case x = xm+1 holds similarly for any xi in S that is misclassiﬁed by hS−{xi}. Let I denote the set of such indices i. Then, we can write:  αm+1 ≥  1   cid:107 xm+1 cid:107 2 ≥  1 r2 S  .  αi ≥ I  r2 S  .   cid:88 i∈I  By  5.19 , the following simple expression holds for the margin:  cid:80 m+1  Using this identity leads to  i=1 αi = 1 ρ2 S.  S cid:88 i∈I I ≤ r2  αi ≤ r2  S  αi =  r2 S ρ2 S  .  m+1 cid:88 i=1   196  Chapter 8 On-Line Learning  Since by deﬁnition I is the total number of leave-one-out errors, this concludes  cid:3  the proof.  Thus, the guarantees given by theorem 8.9 and theorem 8.10 in the separable case have a similar form. These bounds do not seem suﬃcient to distinguish the ef- fectiveness of the SVM and Perceptron algorithms. Note, however, that while the same margin quantity ρS appears in both bounds, the radius rS can be replaced by a ﬁner quantity that is diﬀerent for the two algorithms: in both cases, instead of the radius of the sphere containing all sample points, rS can be replaced by the radius of the sphere containing the support vectors, as can be seen straightforwardly from the proof of the theorems. Thus, the position of the support vectors in the case of SVMs can provide a more favorable guarantee than that of the support vectors  update vectors  for the Perceptron algorithm. Finally, the guarantees given by these theorems are somewhat weak. These are not high probability bounds, they hold only for the expected error of the hypotheses returned by the algorithms and in particular provide no information about the variance of their error.  The following two theorems give bounds on the number of updates or mistakes made by the Perceptron algorithm in the more general scenario of a non-linearly separable sample in terms of the ρ-Hinge losses of an arbitrary weight vector v. Theorem 8.11 Let I denote the set of indices t ∈ [T ] at which the Perceptron algo- rithm makes an update when processing a sequence x1, . . . , xT with  cid:107 xt cid:107  ≤ r for some r > 0. Then, the number of updates M = I made by the algorithm can be bounded as follows:  r  2  inf  M ≤  ρ + cid:113  r2  ρ2 + 4 cid:107 lρ cid:107 1 2  ρ>0, cid:107 v cid:107 2≤1   where lρ =  lt t∈I with lt = max cid:8 0, 1 − yt v·xt  Proof: Fix ρ > 0 and v with  cid:107 v cid:107 2 = 1. By deﬁnition of lt, for any t, we have 1 − yt v·xt   ρ>0, cid:107 v cid:107 2≤1 cid:18  r  cid:9 .  ≤ lt. Summing up these inequalities over all t ∈ I yields  + cid:113  cid:107 lρ cid:107 1 cid:19 2  ≤  inf  ρ  ρ  ρ  ,  lt + cid:88 t∈I M ≤ cid:88 t∈I =  cid:107 lρ cid:107 1 + cid:88 t∈I  cid:80   yt v · xt   yt v · xt   ρ  ρ  √M r2  ,  ρ  ≤  cid:107 lρ cid:107 1 +   8.24   v·  where the last inequality holds by the bound shown in the proof of the separable √M r2. Now, solving the resulting second-degree ≤ case  theorem 8.8 : ρ + cid:113  r2 gives √M ≤ 1 ρ2 + 4 cid:107 lρ cid:107 1 cid:1  , which proves 2 cid:0  r inequality M ≤  cid:107 lρ cid:107 1 + the ﬁrst inequality. The second inequality follows from the sub-additivity of the  cid:3  square-root function.  t∈I ytxt  cid:107 v cid:107  √M r2  ρ   8.3 Linear classiﬁcation  197  Theorem 8.12 Let I denote the set of indices t ∈ [T ] at which the Perceptron algo- rithm makes an update when processing a sequence x1, . . . , xT with  cid:107 xt cid:107  ≤ r for some r > 0. Then, the number of updates M = I made by the algorithm can be bounded as follows:  ρ>0, cid:107 v cid:107 2≤1 cid:18  r where lρ =  lt t∈I with lt = max cid:8 0, 1 − yt v·xt   + cid:113  cid:107 lρ cid:107 2 cid:19 2  cid:9 .  M ≤  inf  ρ  ρ  ,   cid:3   Proof: Fix ρ > 0 and v with  cid:107 v cid:107 2 = 1. Starting with line  8.24  of theorem 8.11 and using  cid:107 lρ cid:107 1 ≤ √M cid:107 lρ cid:107 2, which holds by the Cauchy-Schwarz inequality, give  √M r2  M ≤  cid:107 lρ cid:107 1 + √r2 ρ and proves the statement.  ≤  ρ  ρ  √M cid:107 lρ cid:107 2 +  √M r2  .  This implies √M ≤  cid:107 lρ cid:107 2 +  These bounds strictly generalize the bounds given in the separable case  theo- rem 8.8  since in that case the vector v can be chosen to be that of a maximum- margin hyperplane with no Hinge loss at any point. The main diﬀerence between the two bounds is the L1-norm of the vector of Hinge losses in Theorem 8.11 ver- sus the L2-norm in Theorem 8.12. Note that, since the L2-norm bound follows from upper bounding inequality  8.24 , which is equivalent to the ﬁrst inequality of Theorem 8.11, the ﬁrst L1-norm bound of Theorem 8.11 is always tighter than the L2-norm bound of Theorem 8.12.  The Perceptron algorithm can be generalized, as in the case of SVMs, to deﬁne a linear separation in a high-dimensional space. It admits an equivalent dual form, the dual Perceptron algorithm, which is presented in ﬁgure 8.8. The dual Perceptron  algorithm maintains a vector α ∈ RT of coeﬃcients assigned to each point xt, t ∈ [T ]. The label of a point xt is predicted according to the rule sgn w· xt , where w = cid:80 T s=1 αsysxs. The coeﬃcient αt is incremented by one when this prediction does not match the correct label. Thus, an update for xt is equivalent to augmenting the weight vector w with ytxt, which shows that the dual algorithm matches exactly the standard Perceptron algorithm. The dual Perceptron algorithm can be written solely in terms of inner products between training instances. Thus, as in the case of SVMs, instead of the inner product between points in the input space, an arbitrary PDS kernel can be used, which leads to the kernel Perceptron algorithm detailed in ﬁgure 8.9. The kernel Perceptron algorithm and its average variant, i.e., voted Perceptron with uniform weights ct, are commonly used algorithms in a variety of applications.   198  Chapter 8 On-Line Learning   cid:46  typically α0 = 0  s=1 αsys xs · xt    DualPerceptron α0  1 α ← α0 2  for t ← 1 to T do Receive xt   3  Receive yt    cid:98 yt ← sgn  cid:80 T if   cid:98 yt  cid:54 = yt  then  αt ← αt + 1  else αt ← αt  return α  4  5  6  7  8  9  Figure 8.8 Dual Perceptron algorithm.  8.3.2 Winnow algorithm This section presents an alternative on-line linear classiﬁcation algorithm, the Win- now algorithm. Thus, it learns a weight vector deﬁning a separating hyperplane by sequentially processing the training points. As suggested by the name, the algorithm is particularly well suited to cases where a relatively small number of dimensions or experts can be used to deﬁne an accurate weight vector. Many of the other dimensions may then be irrelevant.  The Winnow algorithm is similar to the Perceptron algorithm, but, instead of the additive update of the weight vector in the Perceptron case, Winnow’s update is multiplicative. The pseudocode of the algorithm is given in ﬁgure 8.10. The algorithm takes as input a learning parameter η > 0. It maintains a non-negative weight vector wt with components summing to one   cid:107 wt cid:107 1 = 1  starting with the uniform weight vector  line 1 . At each round t ∈ [T ], if the prediction does not match the correct label  line 6 , each component wt,i, i ∈ [N ], is updated by multiplying it by exp ηytxt,i  and dividing by the normalization factor Zt to ensure that the weights sum to one  lines 7–9 . Thus, if the label yt and xt,i share the same sign, then wt,i is increased, while, in the opposite case, it is signiﬁcantly decreased. The Winnow algorithm is closely related to the WM algorithm: when xt,i ∈ {−1, +1}, sgn wt·xt  coincides with the majority vote, since multiplying the weight of correct or incorrect experts by eη or e−η is equivalent to multiplying the weight   8.3 Linear classiﬁcation  199  KernelPerceptron α0  1 α ← α0 2  for t ← 1 to T do Receive xt   3   cid:46  typically α0 = 0  s=1 αsysK xs, xt    Receive yt    cid:98 yt ← sgn  cid:80 T if   cid:98 yt  cid:54 = yt  then  αt ← αt + 1  else αt ← αt  return α  4  5  6  7  8  9  Figure 8.9 Kernel Perceptron algorithm for PDS kernel K.  of incorrect ones by β = e−2η. The multiplicative update rule of Winnow is of course also similar to that of AdaBoost.  The following theorem gives a mistake bound for the Winnow algorithm in the separable case, which is similar in form to the bound of theorem 8.8 for the Per- ceptron algorithm.  ∞ ρ2  ∞  log N .  . Then, for η = ρ∞ r2 ∞  Theorem 8.13 Let x1, . . . , xT ∈ RN be a sequence of T points with  cid:107 xt cid:107 ∞ ≤ r∞ for all t ∈ [T ], for some r∞ > 0. Assume that there exist v ∈ RN , v ≥ 0, and ρ∞ > 0 such that for all t ∈ [T ], ρ∞ ≤ yt v·xt  , the number of updates  cid:107 v cid:107 1 made by the Winnow algorithm when processing x1, . . . , xT is upper bounded by 2  r2 Proof: Let I ⊆ [T ] be the set of iterations at which there is an update, and let M be the total number of updates, i.e., I = M . The potential function Φt, t ∈ [T ], used for this proof is the relative entropy of the distribution deﬁned by the normalized weights vi  cid:107 v cid:107 1 ≥ 0, i ∈ [N ], and the one deﬁned by the components of the weight vector wt,i, i ∈ [N ]:  Φt =  N cid:88 i=1  vi  cid:107 v cid:107 1  log  vi  cid:107 v cid:107 1  wt,i  .  To derive an upper bound on Φt, we analyze the diﬀerence of the potential functions at two consecutive rounds. For all t ∈ I, this diﬀerence can be expressed and   200  Chapter 8 On-Line Learning  Winnow η  1 w1 ← 1 N 2  for t ← 1 to T do Receive xt   3  4  5  6  7  8  9  10  Receive yt    cid:98 yt ← sgn wt · xt  if   cid:98 yt  cid:54 = yt  then Zt ← cid:80 N  i=1 wt,i exp ηytxt,i   for i ← 1 to N do  wt+1,i ← wt,i exp ηytxt,i   Zt  else wt+1 ← wt  11 return wT +1  Figure 8.10 Winnow algorithm, with yt ∈ {−1, +1} for all t ∈ [T ].  bounded as follows:  Φt+1 − Φt =  log  wt,i wt+1,i  N cid:88 i=1 N cid:88 i=1  vi  cid:107 v cid:107 1 vi  cid:107 v cid:107 1  =  log  Zt  exp ηytxt,i   ytxt,i  vi  cid:107 v cid:107 1  = log Zt − η  ≤ log cid:104  N cid:88 i=1  N cid:88 i=1 wt,i exp ηytxt,i  cid:105  − ηρ∞ i∼wt cid:2  exp ηytxt,i  cid:3  − ηρ∞ i∼wt cid:2  exp ηytxt,i − ηytwt · xt + ηytwt · xt  cid:3  − ηρ∞  cid:124   ≤ log cid:2  exp η2 2r∞ 2 8  cid:3  + ηyt wt · xt   cid:125  ≤ η2r2  = log E = log E  ∞ 2 − ηρ∞.  −ηρ∞   cid:123  cid:122   ≤0   8.4 On-line to batch conversion  201  The ﬁrst inequality follows the deﬁnition of ρ∞. The subsequent equality rewrites the summation as an expectation over the distribution deﬁned by wt. The next inequality uses Hoeﬀding’s lemma  lemma D.1  and the last one the fact that there has been an update at t, which implies yt wt · xt  ≤ 0. Summing up these inequal- ities over all t ∈ I yields:  ΦT +1 − Φ1 ≤ M  η2r2 Next, we derive a lower bound by noting that  ∞ 2 − ηρ∞ .  Φ1 =  N cid:88 i=1  vi  cid:107 v cid:107 1  log  vi  cid:107 v cid:107 1 1 N  = log N +  N cid:88 i=1  vi  cid:107 v cid:107 1  vi  log   cid:107 v cid:107 1 ≤ log N .  Additionally, since the relative entropy is always non-negative, we have ΦT +1 ≥ 0. This yields the following lower bound:  ΦT +1 − Φ1 ≥ 0 − log N = − log N .  ∞ 2 − ηρ∞ .  cid:3   yields the statement of the theorem.  Combining the upper and lower bounds we see that − log N ≤ M  η2r2 Setting η = ρ∞ r2 ∞ The margin-based mistake bounds of theorem 8.8 and theorem 8.13 for the Percep- tron and Winnow algorithms have a similar form, but they are based on diﬀerent norms. For both algorithms, the norm  cid:107  ·  cid:107 p used for the input vectors xt, t ∈ [T ], is the dual of the norm  cid:107  ·  cid:107 q used for the margin vector v, that is p and q are conjugate: 1 p + 1 q = 1: in the case of the Perceptron algorithm p = q = 2, while for Winnow p = ∞ and q = 1. These bounds imply diﬀerent types of guarantees. The bound for Winnow is fa- vorable when a sparse set of the experts i ∈ [N ] can predict well. For example, if v = e1 where e1 is the unit vector along the ﬁrst axis in RN and if xt ∈ {−1, +1}N  for all t, then the upper bound on the number of mistakes given for Winnow by theorem 8.13 is only 2 log N , while the upper bound of theorem 8.8 for the Percep- tron algorithm is N . The guarantee for the Perceptron algorithm is more favorable in the opposite situation, where sparse solutions are not eﬀective.  8.4 On-line to batch conversion  The previous sections presented several algorithms for the scenario of on-line learn- ing, including the Perceptron and Winnow algorithms, and analyzed their behavior within the mistake model, where no assumption is made about the way the train- ing sequence is generated. Can these algorithms be used to derive hypotheses with small generalization error in the standard stochastic setting? How can the interme-   202  Chapter 8 On-Line Learning  diate hypotheses they generate be combined to form an accurate predictor? These are the questions addressed in this section.  Let H be a hypothesis of functions mapping X to Y cid:48 , and let L : Y cid:48  × Y → R+ be a bounded loss function, that is L ≤ M for some M ≥ 0. We assume a standard supervised learning setting where a labeled sample S =   x1, y1 , . . . ,  xT , yT    ∈  X × Y T is drawn i.i.d. according to some ﬁxed but unknown distribution D. The sample is sequentially processed by an on-line learning algorithm A. The algorithm starts with an initial hypothesis h1 ∈ H and generates a new hypothesis ht+1 ∈ H, after processing pair  xt, yt , t ∈ [m]. The regret of the algorithm is deﬁned as before by  RT =  L ht xt , yt  − min h∈H  T cid:88 t=1  T cid:88 t=1  L h xt , yt .   8.25   The generalization error of a hypothesis h ∈ H is its expected loss R h  = E  x,y ∼D[L h x , y ]. The following lemma gives a bound on the average of the generalization errors of the hypotheses generated by A in terms of its average loss 1 t=1 L ht xt , yt . Lemma 8.14 Let S =   x1, y1 , . . . ,  xT , yT    ∈  X × Y T be a labeled sample drawn i.i.d. according to D, L a loss bounded by M and h1, . . . , hT the sequence of hy- potheses generated by an on-line algorithm A sequentially processing S. Then, for any δ > 0, with probability at least 1 − δ, the following holds:  T  cid:80 T  1 T  T cid:88 t=1  R ht  ≤  1 T  T cid:88 t=1  L ht xt , yt  + M cid:115  2 log 1  T  δ  .   8.26   Proof: For any t ∈ [T ], let Vt be the random variable deﬁned by Vt = R ht  − L ht xt , yt . Observe that for any t ∈ [T ],  E[Vtx1, . . . , xt−1] = R ht  − E[L ht xt , yt ht] = R ht  − R ht  = 0.  Since the loss is bounded by M , Vt takes values in the interval [−M, +M ] for all t ∈ [T ]. Thus, by Azuma’s inequality  theorem D.7 , P[ 1 t=1 Vt ≥  cid:15 ] ≤ exp −2T  cid:15 2  2M  2  . Setting the right-hand side to be equal to δ > 0 yields the  cid:3  statement of the lemma.  T  cid:80 T  When the loss function is convex with respect to its ﬁrst argument, the lemma can be used to derive a bound on the generalization error of the average of the hypotheses generated by A, 1 t=1 ht, in terms of the average loss of A on S, or in terms of the regret RT and the inﬁmum error of hypotheses in H. Theorem 8.15 Let S =   x1, y1 , . . . ,  xT , yT    ∈  X × Y T be a labeled sample drawn i.i.d. according to D, L a loss bounded by M and convex with respect to its ﬁrst argument, and h1, . . . , hT the sequence of hypotheses generated by an on-line algo- rithm A sequentially processing S. Then, for any δ > 0, with probability at least  T  cid:80 T   203   8.27    8.28   L ht xt , yt  + M cid:115  2 log 1 + 2M cid:115  2 log 2  RT T  T  T  .  δ  δ  R h  +  8.4 On-line to batch conversion  1 − δ, each of the following holds: 1 T  T  T  h∈H  T cid:88 t=1 T cid:88 t=1  ht cid:19  ≤ R cid:18  1 T cid:88 t=1 ht cid:19  ≤ inf R cid:18  1 T  cid:80 T T  cid:80 T t=1 ht x , y  ≤ 1 T  cid:80 T t=1 ht  ≤ 1 ht cid:19  ≤ T cid:88 t=1  R cid:18  1  T cid:88 t=1  1 T  T  Proof: By the convexity of L with respect to its ﬁrst argument, for any  x, y  ∈ X× Y, we have L  1 t=1 L ht x , y . Taking the expectation gives R  1 t=1 R ht . The ﬁrst inequality then follows by lemma 8.14. Thus, by deﬁnition of the regret RT , for any δ > 0, the following holds with probability at least 1 − δ 2:  T  cid:80 T  δ  L ht xt , yt  + M cid:115  2 log 2 T cid:88 t=1  L h xt , yt  +  RT T  T  + M cid:115  2 log 2  T  δ  .  1 T  ≤ min h∈H  By deﬁnition of inf h∈H R h , for any  cid:15  > 0, there exists h∗ ∈ H with R h∗  ≤ inf h∈H R h  +  cid:15 . By Hoeﬀding’s inequality, for any δ > 0, with probability at least 1 − δ 2, 1 . Thus, for any  cid:15  > 0, by the union bound, the following holds with probability at least 1 − δ:  t=1 L h∗ xt , yt  ≤ R h∗  + M cid:113  2 log 2 T  cid:80 T R cid:18  1  ht cid:19  ≤  L h∗ xt , yt  +  RT T  1 T  T  T  δ  T cid:88 t=1  T cid:88 t=1  δ  + M cid:115  2 log 2 + M cid:115  2 log 2  T  T  δ  ≤ R h∗  + M cid:115  2 log 2  T  δ  +  RT T  = R h∗  +  + 2M cid:115  2 log 2  T  δ  RT T  R h  +  cid:15  +  ≤ inf h∈H  + 2M cid:115  2 log 2  T  δ  .  RT T  Since this inequality holds for all  cid:15  > 0, it implies the second statement of the  cid:3  theorem.  The theorem can be applied to a variety of on-line regret minimization algorithms, for example when RT  T = O 1 √T  . In particular, we can apply the theorem to the exponential weighted average algorithm. Assuming that the loss L is bounded   204  Chapter 8 On-Line Learning  by M = 1 and that the number of rounds T is known to the algorithm, we can use the regret bound of theorem 8.6. The doubling trick  used in theorem 8.7  can be used to derive a similar bound if T is not known in advance. Thus, for any δ > 0, with probability at least 1 − δ, the following holds for the generalization error of the average of the hypotheses generated by exponential weighted average:  R cid:18  1  T  T cid:88 t=1  ht cid:19  ≤ inf  h∈H  R h  + cid:114  log N  2T  + 2 cid:115  2 log 2  T  δ  ,  where N is the number of experts, or the dimension of the weight vectors.  8.5 Game-theoretic connection  The existence of regret minimization algorithms can be used to give a simple proof of von Neumann’s theorem. For any m ≥ 1, we will denote by ∆m the set of all distributions over {1, . . . , m}, that is ∆m = {p ∈ Rm : p ≥ 0 ∧  cid:107 p cid:107 1 = 1}. Theorem 8.16  Von Neumann’s minimax theorem  Let m, n ≥ 1. Then, for any two- person zero-sum game deﬁned by matrix M ∈ Rm×n, min p∈∆m  p cid:62 Mq = max q∈∆n  min p∈∆m  max q∈∆n  p cid:62 Mq .   8.29   Proof: The inequality maxq minp p cid:62 Mq ≤ minp maxq p cid:62 Mq is straightforward, since by deﬁnition of min, for all p ∈ ∆m, q ∈ ∆n, we have minp p cid:62 Mq ≤ p cid:62 Mq. Taking the maximum over q of both sides gives: maxq minp p cid:62 Mq ≤ maxq p cid:62 Mq for all p, subsequently taking the minimum over p proves the inequality.12 To show the reverse inequality, consider an on-line learning setting where at each round t ∈ [T ], algorithm A returns pt and incurs loss Mqt. We can assume that qt is selected in the optimal adversarial way, that is qt ∈ argmaxq∈∆m p cid:62 t Mq, and that A is a regret minimization algorithm, that is RT  T → 0, where RT =  cid:80 T t=1 p cid:62 t Mqt − minp∈∆m cid:80 T q  cid:16  1 p cid:62 Mq ≤ max  t=1 p cid:62 Mqt. Then, the following holds:  pt cid:17  cid:62 Mq ≤  p cid:62 t Mq =  p cid:62 t Mqt.  min p∈∆m  max q∈∆n  max  1 T  1 T  T  q  T cid:88 t=1  T cid:88 t=1  T cid:88 t=1  12 More generally, the maxmin is always upper bounded by the minmax for any function or two arguments and any constraint sets, following the same proof.   8.6 Chapter notes  205  By deﬁnition of regret, the right-hand side can be expressed and bounded as follows:  1 T  T cid:88 t=1  p cid:62 t Mqt = min p∈∆m  1 T  T cid:88 t=1  p cid:62 Mqt +  RT T  p cid:62 M cid:16  1  T  T cid:88 t=1  qt cid:17  +  RT T  RT T This implies that the following bound holds for the minmax for all T ≥ 1:  p cid:62 Mq +  min p∈∆m  .  = min p∈∆m ≤ max q∈∆n  max q∈∆n  p cid:62 Mq ≤ max q∈∆n  min p∈∆m T = 0, this shows that minp maxq p cid:62 Mq ≤ maxq minp p cid:62 Mq. cid:3   p cid:62 Mq +  min p∈∆m  RT  RT T  Since limT→+∞  8.6 Chapter notes  Algorithms for regret minimization were initiated with the pioneering work of Han- nan [1957] who gave an algorithm whose regret decreases as O √T   as a function of T but whose dependency on N is linear. The weighted majority algorithm and the randomized weighted majority algorithm, whose regret is only logarithmic in N , are due to Littlestone and Warmuth [1989]. The exponential weighted aver- age algorithm and its analysis, which can be viewed as an extension of the WM algorithm to convex non-zero-one losses is due to the same authors [Littlestone and Warmuth, 1989, 1994]. The analysis we presented follows Cesa-Bianchi [1999] and Cesa-Bianchi and Lugosi [2006]. The doubling trick technique appears in Vovk [1990] and Cesa-Bianchi et al. [1997]. The algorithm of exercise 8.7 and the analysis leading to a second-order bound on the regret are due to Cesa-Bianchi et al. [2005]. The lower bound presented in theorem 8.5 is from Blum and Mansour [2007].  While the regret bounds presented are logarithmic in the number of the experts N , when N is exponential in the size of the input problem, the computational complexity of an expert algorithm could be exponential. For example, in the on- line shortest paths problem, N is the number of paths between two vertices of a directed graph. However, several computationally eﬃcient algorithms have been presented for broad classes of such problems by exploiting their structure [Takimoto and Warmuth, 2002, Kalai and Vempala, 2003, Zinkevich, 2003].  The notion of regret  or external regret  presented in this chapter can be gen- eralized to that of internal regret or even swap regret, by comparing the loss of the algorithm not just to that of the best expert in retrospect, but to that of any modiﬁcation of the actions taken by the algorithm by replacing each occurrence of some speciﬁc action with another one  internal regret , or even replacing actions via an arbitrary mapping  swap regret  [Foster and Vohra, 1997, Hart and Mas-Colell, 2000, Lehrer, 2003]. Several algorithms for low internal regret have been given   206  Chapter 8 On-Line Learning  [Foster and Vohra, 1997, 1998, 1999, Hart and Mas-Colell, 2000, Cesa-Bianchi and Lugosi, 2001, Stoltz and Lugosi, 2003], including a conversion of low external regret to low swap regret by Blum and Mansour [2005].  The Perceptron algorithm was introduced by Rosenblatt [1958]. The algorithm raised a number of reactions, in particular by Minsky and Papert [1969], who ob- jected that the algorithm could not be used to recognize the XOR function. Of course, the kernel Perceptron algorithm already given by Aizerman et al. [1964] could straightforwardly succeed to do so using second-degree polynomial kernels. The margin bound for the Perceptron algorithm was proven by Novikoﬀ [1962] and is one of the ﬁrst results in learning theory. We presented two extensions of Novikoﬀ’s result which hold in the more general non-separable case: Theorem 8.12 due to Freund and Schapire [1999a] and Theorem 8.11 due to Mohri and Ros- tamizadeh [2013]. Our proof of Theorem 8.12 is signiﬁcantly more concise that the original proof given by Freund and Schapire [1999a] and shows that the bound of Theorem 8.11 is always tighter than that of Theorem 8.12. See [Mohri and Rostamizadeh, 2013] for other more general data-dependent upper bounds on the number of updates made by the Perceptron algorithm in the non-separable case. The leave-one-out analysis for SVMs is described by Vapnik [1998]. The Winnow algorithm was introduced by Littlestone [1987].  The analysis of the on-line to batch conversion and exercises 8.10 and 8.11 are from Cesa-Bianchi et al. [2001, 2004]  see also Littlestone [1989] . Von Neumann’s minimax theorem admits a number of diﬀerent generalizations. See Sion [1958] for a generalization to quasi-concave-convex functions semi-continuous in each argument and the references therein. The simple proof of von Neumann’s theorem presented here is entirely based on learning-related techniques. A proof of a more general version using multiplicative updates was presented by Freund and Schapire [1999b]. On-line learning is a very broad and fast-growing research area in machine learn- ing. The material presented in this chapter should be viewed only as an introduction to the topic, but the proofs and techniques presented should indicate the ﬂavor of most results in this area. For a more comprehensive presentation of on-line learning and related game theory algorithms and techniques, the reader could consult the book of Cesa-Bianchi and Lugosi [2006].  8.7 Exercises  8.1 Perceptron lower bound. Let S be a labeled sample of m points in RN with  xi =   −1 i, . . . ,  −1 i,  −1 i+1  cid:125   i ﬁrst components   cid:123  cid:122    cid:124   , 0, . . . , 0   and yi =  −1 i+1.   8.30    8.7 Exercises  207  On-line-SVM w0  1 w1 ← w0 2  for t ← 1 to T do   cid:46  typically w0 = 0  3  4  5  6  7  8  9  Receive xt, yt  if yt wt · xt  < 1 then  wt+1 ← wt − η wt − Cytxt   elseif yt wt · xt  > 1 then  wt+1 ← wt − ηwt  else wt+1 ← wt  return wT +1  Figure 8.11 On-line SVM algorithm.  Show that the Perceptron algorithm makes Ω 2N   updates before ﬁnding a separating hyperplane, regardless of the order in which it receives the points.  8.2 Generalized mistake bound. Theorem 8.8 presents a margin bound on the maxi- mum number of updates for the Perceptron algorithm for the special case η = 1. Consider now the general Perceptron update wt+1 ← wt + ηytxt, where η > 0. Prove a bound on the maximum number of mistakes. How does η aﬀect the bound?  8.3 Sparse instances. Suppose each input vector xt, t ∈ [T ], coincides with the tth unit vector of RT . How many updates are required for the Perceptron algorithm to converge? Show that the number of updates matches the margin bound of theorem 8.8.  8.4 Tightness of lower bound. Is the lower bound of theorem 8.5 tight? Explain  why or show a counter-example.  8.5 On-line SVM algorithm. Consider the algorithm described in ﬁgure 8.11. Show that this algorithm corresponds to the stochastic gradient descent technique applied to the SVM problem  5.24  with hinge loss and no oﬀset  i.e., ﬁx p = 1 and b = 0 .   208  Chapter 8 On-Line Learning  MarginPerceptron   1 w1 ← 0 2  for t ← 1 to T do Receive xt  Receive yt   4  3  if  cid:0  wt = 0  or   ytwt·xt   cid:107 wt cid:107  wt+1 ← wt + ytxt  else wt+1 ← wt  < ρ  2   cid:1  then  return wT +1  5  6  7  8  Figure 8.12 Margin Perceptron algorithm.  8.6 Margin Perceptron. Given a training sample S that is linearly separable with a maximum margin ρ > 0, theorem 8.8 states that the Perceptron algorithm run cyclically over S is guaranteed to converge after at most R2 ρ2 updates, where R is the radius of the sphere containing the sample points. However, this theorem does not guarantee that the hyperplane solution of the Perceptron algorithm achieves a margin close to ρ. Suppose we modify the Perceptron algorithm to ensure that the margin of the hyperplane solution is at least ρ 2. In particular, consider the algorithm described in ﬁgure 8.12. In this problem we show that this algorithm converges after at most 16R2 ρ2 updates. Let I denote the set of times t ∈ [T ] at which the algorithm makes an update and let M = I be the total number of updates.   a  Using an analysis similar to the one given for the Perceptron algorithm, show ρ , then M < 4R2 ρ2.  that M ρ ≤  cid:107 wT +1 cid:107 . Conclude that if  cid:107 wT +1 cid:107  < 4R2  For the remainder of this problem, we will assume that  cid:107 wT +1 cid:107  ≥ 4R2 ρ .    b  Show that for any t ∈ I  including t = 0 , the following holds:   cid:107 wt+1 cid:107 2 ≤   cid:107 wt cid:107  + ρ 2 2 + R2.   c  From  b , infer that for any t ∈ I we have   cid:107 wt+1 cid:107  ≤  cid:107 wt cid:107  + ρ 2 +  R2   cid:107 wt cid:107  +  cid:107 wt+1 cid:107  + ρ 2  .   8.7 Exercises  209   d  Using the inequality from  c , show that for any t ∈ I such that either   cid:107 wt cid:107  ≥ 4R2  ρ or  cid:107 wt+1 cid:107  ≥ 4R2  ρ , we have   cid:107 wt+1 cid:107  ≤  cid:107 wt cid:107  +  3 4  ρ.   e  Show that  cid:107 w1 cid:107  ≤ R ≤ 4R2 ρ. Since by assumption we have  cid:107 wT +1 cid:107  ≥ 4R2 ρ , conclude that there must exist a largest time t0 ∈ I such that  cid:107 wt0 cid:107  ≤ 4R2 and  cid:107 wt0+1 cid:107  ≥ 4R2 ρ .  ρ   f  Show that  cid:107 wT +1 cid:107  ≤  cid:107 wt0 cid:107  + 3  4 M ρ. Conclude that M ≤ 16R2 ρ2.  8.7 Second-order regret bound. Consider the randomized algorithm that diﬀers from the RWM algorithm only by the weight update, i.e., wt+1,i ←  1− 1−β lt,i wt,i, t ∈ [T ], which is applied to all i ∈ [N ] with 1 2 ≤ β < 1. This algorithm can be used in a more general setting than RWM since the losses lt,i are only assumed to be in [0, 1]. The objective of this problem is to show that a similar upper bound can be shown for the regret.   a  Use the same potential Wt as for the RWM algorithm and derive a simple  upper bound for log WT +1:  log WT +1 ≤ log N −  1 − β LT .   Hint: Use the inequality log 1 − x  ≤ −x for x ∈ [0, 1 2].    b  Prove the following lower bound for the potential for all i ∈ [N ]:  log WT +1 ≥ − 1 − β LT,i −  1 − β 2  l2 t,i .  T cid:88 t=1   Hint: Use the inequality log 1 − x  ≥ −x − x2, which is valid for all x ∈ [0, 1 2].    c  Use upper and lower bounds to derive the following regret bound for the  algorithm: RT ≤ 2√T log N .  8.8 Polynomial weighted algorithm. The objective of this problem is to show how another regret minimization algorithm can be deﬁned and studied. Let L be a loss function convex in its ﬁrst argument and taking values in [0, M ].  We will assume N > e2 and then for any expert i ∈ [N ], we denote by rt,i the  instantaneous regret of that expert at time t ∈ [T ], rt,i = L  cid:98 yt, yt  − L yt,i, yt ,   210  Chapter 8 On-Line Learning  and by Rt,i its cumulative regret up to time t: Rt,i =  cid:80 t s=1 rt,i. For conve- nience, we also deﬁne R0,i = 0 for all i ∈ [N ]. For any x ∈ R,  x + denotes max x, 0 , that is the positive part of x, and for x =  x1, . . . , xN   cid:62  ∈ RN ,  x + =   x1 +, . . . ,  xN  +  cid:62 .   cid:80 n  cid:80 n  i=1 wt,i  i=1 wt,iyt,i  Let α > 2 and consider the algorithm that predicts at round t ∈ [T ] according to  cid:98 yt = , with the weight wt,i deﬁned based on the αth power of the regret up to time  t − 1 : wt,i =  Rt−1,i α−1 + . The potential function we use to analyze the algorithm is based on the function Φ deﬁned over RN by Φ : x  cid:55 →  cid:107  x + cid:107 2  a  Show that Φ is twice diﬀerentiable over RN−B, where B is deﬁned as follows:  + cid:3  2 B = {u ∈ RN :  u + = 0}.  α = cid:2  cid:80 N  i=1 xi α  α .   b  For any t ∈ [T ], let rt denote the vector of instantaneous regrets, rt =  rt,1, . . . , rt,N   cid:62 , and similarly Rt =  Rt,1, . . . , Rt,N   cid:62 . We deﬁne the po- tential function as Φ Rt  =  cid:107  Rt + cid:107 2 α. Compute ∇Φ Rt−1  for Rt−1  cid:54 ∈ B and show that ∇Φ Rt−1  · rt ≤ 0  Hint: use the convexity of the loss with respect to the ﬁrst argument . α valid for all r ∈ RN and u ∈ RN − B  Hint: write the Hessian ∇2Φ u  as a sum of a diagonal matrix and a positive semideﬁnite matrix multiplied by  2 − α . Also, use H¨older’s for any p > 1 and q > 1 with inequality generalizing Cauchy-Schwarz: 1 p + 1   c  Prove the inequality r cid:62 [∇2Φ u ]r ≤ 2 α − 1  cid:107 r cid:107 2  q = 1 and u, v ∈ RN , u · v ≤  cid:107 u cid:107 p cid:107 v cid:107 q .   d  Using the answers to the two previous questions and Taylor’s formula, show α, if γRt−1 +  1− γ Rt  cid:54 ∈ B  that for all t ≥ 1, Φ Rt − Φ Rt−1  ≤  α− 1  cid:107 rt cid:107 2 for all γ ∈ [0, 1].   e  Suppose there exists γ ∈ [0, 1] such that  1 − γ Rt−1 + γRt ∈ B. Show that   f  Using the two previous questions, derive an upper bound on Φ RT   expressed  Φ Rt  ≤  α − 1  cid:107 rt cid:107 2 α.  in terms of T , N , and M .  the algorithm.   g  Show that Φ RT   admits as a lower bound the square of the regret RT of   h  Using the two previous questions give an upper bound on the regret RT . For what value of α is the bound the most favorable? Give a simple expression of the upper bound on the regret for a suitable approximation of that optimal value.   8.7 Exercises  211  8.9 General inequality. In this exercise we generalize the result of exercise 8.7 by  using a more general inequality: log 1 − x  ≥ −x − x2  α for some 0 < α < 2.   a  First prove that the inequality is true for x ∈ [0, 1 − α  imply about the valid range of β?  2 ]. What does this   b  Give a generalized version of the regret bound derived in exercise 8.7 in terms  of α, which shows:  RT ≤  log N 1 − β  +  1 − β α  T .  What is the optimal choice of β and the resulting bound in this case?   c  Explain how α may act as a regularization parameter. What is the optimal  choice of α?  8.10 On-line to batch — non-convex loss.  The on-line to batch result of theorem 8.15 heavily relies on the fact that the loss is convex in order to provide a generalization guarantee for the uniformly averaged hypothesis 1 i=1 hi. For general losses, instead of using the averaged hypothesis we will use a diﬀerent strategy and try to estimate the best single base hypothesis and show the expected loss of this hypothesis is bounded.  T  cid:80 T  Let mi denote the cumulative loss of hypothesis hi on the points  xi, . . . , xT  , t=i L hi xt , yt . Then we deﬁne the penalized risk estimate of  that is mi = cid:80 T  hypothesis hi as,  mi  T − i + 1  + cδ T − i + 1  where cδ x  = cid:114  1  2x  T  T + 1   .  log  δ  The term cδ penalizes the empirical error when the test sample is small. Deﬁne   cid:98 h = hi∗ where i∗ = argmini mi  T − i + 1  + cδ T − i + 1 . We will then show  under the same conditions of theorem 8.15  with M = 1 for simplicity , but without requiring the convexity of L, that the following holds with probability at least 1 − δ:  L hi xi , yi  + 6 cid:114  1  T  2 T + 1   .  log  δ   8.31   1 T  T cid:88 i=1  R  cid:98 h  ≤   a  Prove the following inequality:  min i∈[T ]   R hi  + 2cδ T − i + 1   ≤  R hi  + 4 cid:114  1  T  1 T  T cid:88 i=1  T + 1  .  log  δ   212  Chapter 8 On-Line Learning   b  Use part  a  to show that with probability at least 1 − δ,  min i∈[T ]   R hi  + 2cδ T − i + 1    L hi xi , yi  + cid:114  2  T  log  + 4 cid:114  1  T  1 δ  T + 1  .  log  δ   c  By design, the deﬁnition of cδ ensures that with probability at least 1 − δ   R hi  + 2cδ T − i + 1   .  Use this property to complete the proof of  8.31 .  <  T cid:88 i=1 R  cid:98 h  ≤ min  i∈[T ]  8.11 On-line to batch — kernel Perceptron margin bound. In this problem, we give a margin-based generalization guarantee for the kernel Perceptron algorithm. Let h1, . . . , hT be the sequence of hypotheses generated by the kernel Perceptron  zero-one loss. We now wish to more precisely bound the generalization error of  algorithm and let  cid:98 h be deﬁned as in exercise 8.10. Finally, let L denote the  cid:98 h in this setting. T cid:88 i=1  max cid:18 0, 1 −  ρ  cid:19  +  ρ cid:115  cid:88 i∈I  L hi xi , yi  ≤  h∈H: cid:107 h cid:107 ≤1   a  First, show that  T cid:88 i=1  K xi, xi ,  yih xi   inf  1  where I is the set of indices where the kernel Perceptron makes an update and where δ and ρ are deﬁned as in theorem 8.12.   b  Now, use the result of exercise 8.10 to derive a generalization guarantee for  following holds with probability at least 1 − δ:   cid:98 h in the case of kernel Perceptron, which states that for any 0 < δ ≤ 1, the R  cid:98 h  ≤ where  cid:98 RS,ρ h  = 1  K xi, xi  + 6 cid:114  1  cid:1 . Compare this result with the  ρT cid:115  cid:88 i∈I i=1 max cid:0 0, 1 − yih xi   h∈H: cid:107 h cid:107 ≤1 cid:98 RS,ρ h  +  margin bounds for kernel-based hypotheses given by corollary 6.13.  T  cid:80 T  2 T + 1   log  inf  T  1  δ  ρ  ,   9 Multi-Class Classiﬁcation  The classiﬁcation problems we examined in the previous chapters were all binary. However, in most real-world classiﬁcation problems the number of classes is greater than two. The problem may consist of assigning a topic to a text document, a category to a speech utterance or a function to a biological sequence. In all of these tasks, the number of classes may be on the order of several hundred or more.  In this chapter, we analyze the problem of multi-class classiﬁcation. We ﬁrst in- troduce the multi-class classiﬁcation learning problem and discuss its multiple set- tings, and then derive generalization bounds for it using the notion of Rademacher complexity. Next, we describe and analyze a series of algorithms for tackling the multi-class classiﬁcation problem. We will distinguish between two broad classes of algorithms: uncombined algorithms that are speciﬁcally designed for the multi- class setting such as multi-class SVMs, decision trees, or multi-class boosting, and aggregated algorithms that are based on a reduction to binary classiﬁcation and re- quire training multiple binary classiﬁers. We will also brieﬂy discuss the problem of structured prediction, which is a related problem arising in a variety of applications.  9.1 Multi-class classiﬁcation problem  Let X denote the input space and Y denote the output space, and let D be an unknown distribution over X according to which input points are drawn. We will distinguish between two cases: the mono-label case, where Y is a ﬁnite set of classes that we mark with numbers for convenience, Y = {1, . . . , k}, and the multi-label case where Y = {−1, +1}k. In the mono-label case, each example is labeled with a single class, while in the multi-label case it can be labeled with several. The latter can be illustrated by the case of text documents, which can be labeled with several diﬀerent relevant topics, e.g., sports, business, and society. The positive components of a vector in {−1, +1}k indicate the classes associated with an example.   214  Chapter 9 Multi-Class Classiﬁcation  In either case, the learner receives a labeled sample S = cid:0  x1, y1 , . . . ,  xm, ym  cid:1  ∈   X× Y m with x1, . . . , xm drawn i.i.d. according to D, and yi = f  xi  for all i ∈ [m], where f : X → Y is the target labeling function. Thus, we consider a deterministic scenario, which, as discussed in section 2.4.1, can be straightforwardly extended to a stochastic one that admits a distribution over X × Y. Given a hypothesis set H of functions mapping X to Y, the multi-class classiﬁ- cation problem consists of using the labeled sample S to ﬁnd a hypothesis h ∈ H with small generalization error R h  with respect to the target f :  R h  = E x∼D R h  = E  [1h x  cid:54 =f  x ]  x∼D cid:104  k cid:88 l=1  1[h x ]l cid:54 =[f  x ]l cid:105   mono-label case   9.1   multi-label case.   9.2    9.3    9.4   The notion of Hamming distance dH , that is, the number of corresponding compo- nents in two vectors that diﬀer, can be used to give a common formulation for both errors:  The empirical error of h ∈ H is denoted by  cid:98 RS h  and deﬁned by  x∼D cid:104 dH  h x , f  x   cid:105 . m cid:88 i=1  dH  h xi , yi  .  1 m  R h  = E   cid:98 RS h  =  Several issues, both computational and learning-related, often arise in the multi- class setting. Computationally, dealing with a large number of classes can be prob- lematic. The number of classes k directly enters the time complexity of the al- gorithms we will present. Even for a relatively small number of classes such as k = 100 or k = 1,000, some techniques may become prohibitive to use in practice. This dependency is even more critical in the case where k is very large or even inﬁnite as in the case of some structured prediction problems.  A learning-related issue that commonly appears in the multi-class setting is the existence of unbalanced classes. Some classes may be represented by less than 5 percent of the labeled sample, while others may dominate a very large fraction of the data. When separate binary classiﬁers are used to deﬁne the multi-class solution, we may need to train a classiﬁer distinguishing between two classes with only a small representation in the training sample. This implies training on a small sample, with poor performance guarantees. Alternatively, when a large fraction of the training instances belong to one class, it may be tempting to propose a hypothesis always returning that class, since its generalization error as deﬁned earlier is likely to be relatively low. However, this trivial solution is typically not the   9.2 Generalization bounds  215  one intended. Instead, the loss function may need to be reformulated by assigning diﬀerent misclassiﬁcation weights to each pair of classes.  Another learning-related issue is the relationship between classes, which can be hierarchical. For example, in the case of document classiﬁcation, the error of mis- classifying a document dealing with world politics as one dealing with real estate should naturally be penalized more than the error of labeling a document with sports instead of the more speciﬁc label baseball. Thus, a more complex and more useful multi-class classiﬁcation formulation would take into consideration the hi- erarchical relationships between classes and deﬁne the loss function in accordance with this hierarchy. More generally, there may be a graph relationship between classes as in the case of gene ontology in computational biology. The use of hierar- chical relationships between classes leads to a richer and more complex multi-class classiﬁcation problem.  9.2 Generalization bounds  In this section, we present margin-based generalization bounds for multi-class clas- siﬁcation in the mono-label case. In the binary setting, classiﬁers are often deﬁned based on the sign of a scoring function. In the multi-class setting, a hypothesis is  deﬁned based on a scoring function h : X × Y → R. The label associated to point x  is the one resulting in the largest score h x, y , which deﬁnes the following mapping from X to Y:  This naturally leads to the following deﬁnition of the margin ρh x, y  of the function h at a labeled example  x, y :  x  cid:55 → argmax  y∈Y  h x, y .  ρh x, y  = h x, y  − max y cid:48  cid:54 =y  h x, y cid:48  .  Thus, h misclassiﬁes  x, y  iﬀ ρh x, y  ≤ 0. For any ρ > 0, we can deﬁne the empirical margin loss of a hypothesis h for multi-class classiﬁcation as   cid:98 RS,ρ h  =  1 m  m cid:88 i=1  Φρ ρh xi, yi  ,   9.5   where Φρ is the margin loss function  deﬁnition 5.5 . Thus, the empirical margin loss for multi-class classiﬁcation is upper bounded by the fraction of the training points misclassiﬁed by h or correctly classiﬁed but with conﬁdence less than or   216  equal to ρ:  Chapter 9 Multi-Class Classiﬁcation   9.6    9.7    9.8    cid:98 RS,ρ h  ≤  1 m  m cid:88 i=1  1ρh xi,yi ≤ρ.  The following lemma will be used in the proof of the main result of this section.  Lemma 9.1 Let F1, . . . ,Fl be l hypothesis sets in RX, l ≥ 1, and let G = {max{h1, . . . , hl} : hi ∈ Fi, i ∈ [l]}. Then, for any sample S of size m, the empirical Rademacher complexity of G can be upper bounded as follows:   cid:98 RS G  ≤  l cid:88 j=1 cid:98 RS Fj .  Proof: Let S =  x1, . . . , xm  be a sample of size m. We ﬁrst prove the result in the case l = 2. By deﬁnition of the max operator, for any h1 ∈ F1 and h2 ∈ F2,  max{h1, h2} =  [h1 + h2 + h1 − h2].  1 2  Thus, we can write:   cid:98 RS G  =  E  E  1 m  1 2m  h1∈F1 h2∈F2  σ cid:104  sup σ cid:104  sup 2 cid:98 RS F1  +  1  h1∈F1 h2∈F2 1  σi max{h1 xi , h2 xi } cid:105  m cid:88 i=1 σi cid:0 h1 xi  + h2 xi  +  h1 − h2  xi  cid:1  cid:105  m cid:88 i=1 2 cid:98 RS F2  +  σ cid:104  sup  m cid:88 i=1  h1∈F1 h2∈F2  σi h1 − h2  xi  cid:105 ,  1 2m  E  =  ≤  using the sub-additivity of sup. Since x  cid:55 → x is 1-Lipschitz, by Talagrand’s lemma  lemma 5.7 , the last term can be bounded as follows  1 2m  E  σ cid:104  sup  h1∈F1 h2∈F2  m cid:88 i=1  σi h1 − h2  xi  cid:105  ≤  h1∈F1 h2∈F2  1  E  1 2m  σ cid:104  sup 2 cid:98 RS F1  + 2 cid:98 RS F1  +  1  E  1 2m  σi h1 − h2  xi  cid:105  m cid:88 i=1 σ cid:104  sup 2 cid:98 RS F2 ,  m cid:88 i=1  h2∈F2  1  ≤  =  −σih2 xi  cid:105    9.9   where we again use the sub-additivity of sup for the second inequality and the fact that σi and −σi have the same distribution for any i ∈ [m] for the last equality. be derived from the case l = 2 using max{h1, . . . , hl} = max{h1, max{h2, . . . , hl}}  cid:3  and an immediate recurrence.  Combining  9.8  and  9.9  yields  cid:98 RS G  ≤  cid:98 RS F1  + cid:98 RS F2 . The general case can   9.2 Generalization bounds  217  For any family of hypotheses mapping X × Y to R, we deﬁne Π1 H  by  Π1 H  = {x  cid:55 → h x, y  : y ∈ Y, h ∈ H}.  The following theorem gives a general margin bound for multi-class classiﬁcation.  Theorem 9.2  Margin bound for multi-class classiﬁcation  Let H ⊆ RX×Y be a hypo- thesis set with Y = {1, . . . , k}. Fix ρ > 0. Then, for any δ > 0, with probability at least 1− δ, the following multi-class classiﬁcation generalization bound holds for all h ∈ H:  Rm Π1 H   + cid:115  log 1  δ 2m  4k ρ  .   9.10   Proof: We will need the following deﬁnition for this proof:  R h  ≤  cid:98 RS,ρ h  +  ρθ,h x, y  = min y cid:48    h x, y  − h x, y cid:48   + θ1y cid:48 =y ,  where θ > 0 is an arbitrary constant. Observe that E[1ρh x,y ≤0] ≤ E[1ρθ,h x,y ≤0] since the inequality ρθ,h x, y  ≤ ρh x, y  holds for all  x, y  ∈ X × Y:  ρθ,h x, y  = min  y cid:48   cid:0 h x, y  − h x, y cid:48   + θ1y cid:48 =y cid:1  y cid:48  cid:54 =y cid:0 h x, y  − h x, y cid:48   + θ1y cid:48 =y cid:1  y cid:48  cid:54 =y cid:0 h x, y  − h x, y cid:48   cid:1  = ρh x, y ,  ≤ min = min  where the inequality follows from taking the minimum over a smaller set.  Now, similar to the proof of theorem 5.8, let  cid:101 H = { x, y   cid:55 → ρθ,h x, y  : h ∈ H} and  cid:101 H = {Φρ ◦ cid:101 h : cid:101 h ∈  cid:101 H}. By theorem 3.3, with probability at least 1 − δ, for all  h ∈ H,  E cid:2 Φρ ρθ,h x, y   cid:3  ≤  1 m  m cid:88 i=1  Φρ ρθ,h xi, yi   + 2Rm  cid:101 H  + cid:115  log 1  δ 2m  .  Since 1u≤0 ≤ Φρ u  for all u ∈ R, the generalization error R h  is a lower bound on the left-hand side, R h  = E[1ρh x,y ≤0] ≤ E[1ρθ,h x,y ≤0] ≤ E cid:2 Φρ ρθ,h x, y   cid:3 , and  we can write:  R h  ≤  1 m  m cid:88 i=1  Φρ ρθ,h xi, yi   + 2Rm  cid:101 H  + cid:115  log 1  δ 2m  .  Fixing θ = 2ρ, we observe that Φρ ρθ,h xi, yi   = Φρ ρh xi, yi  . Indeed, either ρθ,h xi, yi  = ρh xi, yi  or ρθ,h xi, yi  = 2ρ ≤ ρh xi, yi , which implies the desired since Φρ is a 1  result. Furthermore, Talagrand’s lemma  lemma 5.7  yields Rm  cid:101 H  ≤ 1  ρ -Lipschitz function. Therefore, for any δ > 0, with probability at  ρ Rm  cid:101 H    218  Chapter 9 Multi-Class Classiﬁcation  least 1 − δ, for all h ∈ H:  .  y  E  E  2 ρ  1 m  1 m  h∈H  δ 2m  σi h xi, yi  − max  R h  ≤  cid:98 RS,ρ h  +  Rm  cid:101 H  + cid:115  log 1 and to complete the proof it suﬃces to show that Rm  cid:101 H  ≤ 2kRm Π1 H  . Here Rm  cid:101 H  can be upper-bounded as follows:  h xi, y  − 2ρ1y=yi   cid:21  Rm  cid:101 H  = S,σ cid:20  sup m cid:88 i=1 σih xi, y 1yi=y cid:21  σ cid:20  sup m cid:88 i=1 cid:88 y∈Y σih xi, y 1yi=y cid:21  σ cid:20  sup m cid:88 i=1 m cid:88 y∈Y σih xi, y  cid:18   cid:15 i σ cid:20  sup m cid:88 i=1 = cid:88 y∈Y  m cid:88 i=1 m cid:88 i=1 σih xi, yi  cid:21  =  S,σ cid:20  sup S,σ cid:20  sup σ cid:20  sup m cid:88 i=1  σih xi, yi  cid:21  +  Now we bound the ﬁrst term above. Observe that  2 cid:19  cid:21 ,  h∈H E  σi max  h∈H  h∈H  h∈H  h∈H  h∈H  1 m  1 m  1 m  1 m  ≤  ≤  E  E  E  E  +  1  2  1  y   h xi, y  − 2ρ1y=yi  cid:21 .  where  cid:15 i = 2 · 1yi=y − 1. Since  cid:15 i ∈ {−1, +1}, we have that σi and σi cid:15 i admit the same distribution and, for any y ∈ Y, each of the terms of the right-hand side can be bounded as follows:  1 m  E  2  1  +  ≤  h∈H  h∈H 1 E 2m  σ cid:20  sup  2 cid:17  cid:21  σih xi, y  cid:16   cid:15 i m cid:88 i=1 σi cid:15 ih xi, y  cid:21  + σ cid:20  sup m cid:88 i=1 ≤  cid:98 Rm Π1 H  . ES,σ cid:2  suph∈H cid:80 m  h xi, y  − 2ρ1y=yi   cid:21  m cid:88 i=1  σi max  y  1 m  E  S,σ cid:20  sup  h∈H  1 2m  E  σ cid:20  sup  h∈H  m cid:88 i=1  σih xi, y  cid:21   Thus, we can write 1 m the second term, we ﬁrst apply lemma 9.1 which immediately yields that  i=1 σih xi, yi  cid:3  ≤ k Rm Π1 H  . To bound  ≤ cid:88 y∈Y  1 m  E  S,σ cid:20  sup  h∈H  m cid:88 i=1  σi h xi, y  − 2ρ1y=yi  cid:21    9.2 Generalization bounds  219   cid:3   and since Rademacher variables are mean zero, we observe that  E  S,σ cid:20  sup  h∈H  m cid:88 i=1  σi h xi, y  − 2ρ1y=yi  cid:21  = E  S,σ cid:20  sup h∈H cid:18  m cid:88 i=1 S,σ cid:20  sup m cid:88 i=1  h∈H  σih xi, y  cid:19  − 2ρ m cid:88 i=1 σih xi, y  cid:21  ≤ Rm Π1 H    σi1y=yi cid:21   = E  which completes the proof.  These bounds can be generalized to hold uniformly for all ρ > 0 at the cost of  other margin bounds presented in previous sections, they show the conﬂict between two terms: the larger the desired pairwise ranking margin ρ, the smaller the middle  an additional term cid:112  log log2 2 ρ   m, as in theorem 5.9 and exercise 5.2. As for term, at the price of a larger empirical multi-class classiﬁcation margin loss  cid:98 RS,ρ.  Note, however, that here there is additionally a dependency on the number of classes k. This suggests either weaker guarantees when learning with a large number of classes or the need for even larger margins ρ for which the empirical margin loss would be small.  For some hypothesis sets, a simple upper bound can be derived for the Rademacher complexity of Π1 H , thereby making theorem 9.2 more explicit. We will show this  be a feature mapping associated to K. In multi-class classiﬁcation, a kernel-based  for kernel-based hypotheses. Let K : X× X → R be a PDS kernel and let Φ : X → H hypothesis is based on k weight vectors w1, . . . , wk ∈ H. Each weight vector wl, l ∈ [k], deﬁnes a scoring function x  cid:55 → wl · Φ x  and the class associated to point x ∈ X is given by  We denote by W the matrix formed by these weight vectors: W =  w1, . . . , wk  cid:62  and for any p ≥ 1 denote by  cid:107 W cid:107 H,p the LH,p group norm of W deﬁned by  argmax  y∈Y  wy · Φ x .   cid:107 W cid:107 H,p = cid:0  k cid:88 l=1   cid:107 wl cid:107 pH cid:1 1 p  .  For any p ≥ 1, the family of kernel-based hypotheses we will consider is13 HK,p = { x, y  ∈ X × {1, . . . , k}  cid:55 → wy · Φ x  : W =  w1, . . . , wk  cid:62 , cid:107 W cid:107 H,p ≤ Λ}. Proposition 9.3  Rademacher complexity of multi-class kernel-based hypotheses  Let K : X × X → R be a PDS kernel and let Φ : X → H be a feature mapping associated to where  cid:107 h cid:107 K,p = cid:0  cid:80 k 13 The hypothesis set H can also be deﬁned via H = {h ∈ RX×Y : h ·, y  ∈ H ∧  cid:107 h cid:107 K,p ≤ Λ},  y=1  cid:107 h ·, y  cid:107 pH cid:1 1 p, without referring to a feature mapping for K.   220  Chapter 9 Multi-Class Classiﬁcation  m  E  y∈Y  1 m  Rm Π1 HK,p   =  K. Assume that there exists r > 0 such that K x, x  ≤ r2 for all x ∈ X. Then, for any m ≥ 1, Rm Π1 HK,p   can be bounded as follows: .  Proof: Let S =  x1, . . . , xm  denote a sample of size m. Observe that for all =  cid:107 W cid:107 H,p holds. Thus, the condition  cid:107 W cid:107 H,p ≤ Λ implies that  cid:107 wl cid:107 H ≤ Λ for all l ∈ [k]. In view of that, the Rademacher complexity of the hypothesis set Π1 HK,p  can be expressed and bounded as follows:  Rm Π1 HK,p   ≤ cid:114  r2Λ2 l ∈ [k], the inequality  cid:107 wl cid:107 H ≤  cid:0  cid:80 k l=1  cid:107 wl cid:107 pH cid:1 1 p S,σ cid:20  sup σiΦ xi  cid:69  cid:21   cid:107 W cid:107 ≤Λ cid:68 wy, m cid:88 i=1 S,σ cid:20  sup σiΦ xi  cid:13  cid:13  cid:13 H cid:21   Cauchy-Schwarz ineq.    cid:107 wy cid:107 H cid:13  cid:13  cid:13  m cid:88 i=1 S,σ cid:20  cid:13  cid:13  cid:13  σiΦ xi  cid:13  cid:13  cid:13 H cid:21  m cid:88 i=1 H cid:105  cid:21 1 2 m cid:20  E σiΦ xi  cid:13  cid:13  cid:13  S,σ cid:104  cid:13  cid:13  cid:13  m cid:88 i=1 m cid:20  E  cid:107 Φ xi  cid:107 2H cid:105  cid:21 1 2 S,σ cid:104  m cid:88 i=1 K xi, xi  cid:105  cid:21 1 2 m cid:20  E S,σ cid:104  m cid:88 i=1 = cid:114  r2Λ2 Λ√mr2   i  cid:54 = j ⇒ E   Jensen’s inequality   [σiσj] = 0    cid:107 W cid:107 ≤Λ  Λ m  1 m  y∈Y  ≤  ≤  ≤  ≤  E  E  m  m  =  =  Λ  Λ  Λ  σ  2  ,  which concludes the proof.  Combining theorem 9.2 and proposition 9.3 yields directly the following result.  Corollary 9.4  Margin bound for multi-class classiﬁcation with kernel-based hypotheses   Let K : X× X → R be a PDS kernel and let Φ : X → H be a feature mapping associ- ated to K. Assume that there exists r > 0 such that K x, x  ≤ r2 for all x ∈ X. Fix ρ > 0. Then, for any δ > 0, with probability at least 1− δ, the following multi-class classiﬁcation generalization bound holds for all h ∈ HK,p:   cid:3    9.11   R h  ≤  cid:98 RS,ρ h  + 4k cid:114  r2Λ2 ρ2  m  + cid:115  log 1  δ 2m  .   9.3 Uncombined multi-class algorithms  221  In the next two sections, we describe multi-class classiﬁcation algorithms that belong to two distinct families: uncombined algorithms, which are deﬁned by a single optimization problem, and aggregated algorithms, which are obtained by training multiple binary classiﬁcations and by combining their outputs.  9.3 Uncombined multi-class algorithms  In this section, we describe three algorithms designed speciﬁcally for multi-class classiﬁcation. We start with a multi-class version of SVMs, then describe a boosting- type multi-class algorithm, and conclude with decision trees, which are often used as base learners in boosting.  9.3.1 Multi-class SVMs We describe an algorithm that can be derived directly from the theoretical guar- antees presented in the previous section. Proceeding as in section 5.4 for classiﬁ- cation, the guarantee of corollary 9.4 can be expressed as follows: for any δ > 0, with probability at least 1 − δ, for all h ∈ HK,2 = { x, y  → wy · Φ x  : W =   w1, . . . , wk  cid:62 , cid:80 k  l=1  cid:107 wl cid:107 2 ≤ Λ2},  R h  ≤  1 m  m cid:88 i=1  ξi + 4k cid:114  r2Λ2  m  + cid:115  log 1  δ 2m  ,  where ξi = max cid:0 1 − [wyi · Φ xi  − maxy cid:48  cid:54 =yi wy cid:48  · Φ xi ], 0 cid:1  for all i ∈ [m]. or equivalently  cid:80 k  An algorithm based on this theoretical guarantee consists of minimizing the right- hand side of  9.12 , that is, minimizing an objective function with a term corre- sponding to the sum of the slack variables ξi, and another one minimizing  cid:107 W cid:107 H,2 l=1  cid:107 wl cid:107 2. This is precisely the optimization problem deﬁning the multi-class SVM algorithm:   9.12   min W,ξ  1 2  k cid:88 l=1   cid:107 wl cid:107 2 + C  ξi  m cid:88 i=1  subject to: ∀i ∈ [m],∀l ∈ Y − {yi},  wyi · Φ xi  ≥ wl · Φ xi  + 1 − ξi, ξi ≥ 0.  The decision function learned is of the form x  cid:55 → argmaxl∈Y wl · Φ x . As with the primal problem of SVMs, this is a convex optimization problem: the objective func- tion is convex, since it is a sum of convex functions, and the constraints are aﬃne and thus qualiﬁed. The objective and constraint functions are diﬀerentiable, and the KKT conditions hold at the optimum. Deﬁning the Lagrangian and applying   222  Chapter 9 Multi-Class Classiﬁcation  these conditions leads to the equivalent dual optimization problem, which can be expressed in terms of the kernel function K alone:  max α∈Rm×k  m cid:88 i=1  1 2  m cid:88 i=1  αi · eyi −   αi · αj K xi, xj   subject to: ∀i ∈ [m],  0 ≤ αiyi ≤ C  ∧  ∀j  cid:54 = yi, αij ≤ 0  ∧  αi · 1 = 0 .  Here, α ∈ Rm×k is a matrix, αi denotes the ith row of α, and el the lth unit vector in Rk, l ∈ [k]. Both the primal and dual problems are simple QPs generalizing  those of the standard SVM algorithm. However, the size of the solution and the number of constraints for both problems is in Ω mk , which, for a large number of classes k, can make it diﬃcult to solve. However, there exist speciﬁc optimization solutions designed for this problem based on a decomposition of the problem into m disjoint sets of constraints.  9.3.2 Multi-class boosting algorithms We describe a boosting algorithm for multi-class classiﬁcation called AdaBoost.MH , which in fact coincides with a special instance of AdaBoost. An alternative multi- class classiﬁcation algorithm based on similar boosting ideas, AdaBoost.MR, is described and analyzed in exercise 9.4. AdaBoost.MH applies to the multi-label setting where Y = {−1, +1}k. As in the binary case, it returns a convex combination of base classiﬁers selected from a hypothesis set H = {h1, . . . , hN}. Let F be the following objective function deﬁned for all samples S =   x1, y1 , . . . ,  xm, ym   ∈  X × Y m and ¯α =  ¯α1, . . . , ¯αN   ∈ RN , N ≥ 1, by k cid:88 l=1 m cid:88 i=1  e−yi[l] cid:80 N  e−yi[l]fN  xi,l  =  j=1 ¯αj hj  xi,l ,  m cid:88 i=1  k cid:88 l=1  F   ¯α  =   9.13   where fN =  cid:80 N  j=1 ¯αjhj and where yi[l] denotes the lth coordinate of yi for any i ∈ [m] and l ∈ [k]. F is a convex and diﬀerentiable upper bound on the multi-class multi-label loss:  m cid:88 i=1  k cid:88 l=1  1yi[l] cid:54 =fN  xi,l  ≤  m cid:88 i=1  k cid:88 l=1  e−yi[l]fN  xi,l ,   9.14   since for any x ∈ X with label y = f  x  and any l ∈ [k], the inequality 1y[l] cid:54 =fN  x,l  ≤ e−y[l]fN  x,l  holds. Using the same arguments as in section 7.2.2, we see that Ad- aBoost.MH coincides exactly with the application of coordinate descent to the objective function F . Figure 9.1 gives the pseudocode of the algorithm in the case where the base classiﬁers are functions mapping from X × Y to {−1, +1}. The algorithm takes as input a labeled sample S =   x1, y1 , . . . ,  xm, ym   ∈  X × Y m and maintains a distribution Dt over {1, . . . , m} × Y. The remaining details of the   9.3 Uncombined multi-class algorithms  223  1  2  3  4  5  6  7  8  9  AdaBoost.MH S =   x1, y1 , . . . ,  xm, ym     for i ← 1 to m do  for l ← 1 to k do D1 i, l  ← 1  mk  for j ← 1 to N do  2 log 1− cid:15 j  hj ← base classiﬁer in H with small error  cid:15 j = P ¯αj ← 1 Zt ← 2[ cid:15 j 1 −  cid:15 j ] for i ← 1 to m do   cid:46  normalization factor   cid:15 j  1 2  for l ← 1 to k do  Dj+1 i, l  ← Dj  i,l  exp − ¯αj yi[l]hj  xi,l    Zj  10  11 fN ← cid:80 N  12 return h = sgn fN    j=1 ¯αjhj   i,l ∼Dj [hj xi, l   cid:54 = yi[l]]  Figure 9.1 AdaBoost.MH algorithm, for H ⊆  {−1, +1}k X×Y.  algorithm are similar to AdaBoost. In fact, AdaBoost.MH exactly coincides with AdaBoost applied to the training sample derived from S by splitting each labeled point  xi, yi  into k labeled examples   xi, l , yi[l] , with each example  xi, l  in X × Y and its label in {−1, +1}:   xi, yi  →   xi, 1 , yi[1] , . . . ,   xi, k , yi[k] , i ∈ [m].  Let S cid:48  denote the resulting sample, then S cid:48  =   x1, 1 , y1[1] , . . . ,  xm, k , ym[k]  . S cid:48  contains mk examples and the expression of the objective function F in  9.13  coincides exactly with that of the objective function of AdaBoost for the sample S cid:48 . In view of this connection, the theoretical analysis along with the other observations we presented for AdaBoost in chapter 7 also apply here. Hence, we will focus on aspects related to the computational eﬃciency and to the weak learning condition that are speciﬁc to the multi-class scenario.  The complexity of the algorithm is that of AdaBoost applied to a sample of size  mk. For X ⊆ Rd, using boosting stumps as base classiﬁers, the complexity of the  algorithm is therefore in O  mk  log mk  + mkdN  . Thus, for a large number of classes k, the algorithm may become impractical using a single processor. The   224  Chapter 9 Multi-Class Classiﬁcation  Figure 9.2 Left: example of a decision tree with numerical questions based on two variables X1 and X2. Here, each leaf is marked with the region it deﬁnes. The class labeling for a leaf is obtained via majority vote based on the training points falling in the region it deﬁnes. Right: Partition of the two-dimensional space induced by that decision tree.  weak learning condition for the application of AdaBoost in this scenario requires that at each round there exists a base classiﬁer hj : X × Y → {−1, +1} such that P  i,l ∼Dj [hj xi, l   cid:54 = yi[l]] < 1 2. This may be hard to achieve if some classes diﬃcult to distinguish between. It is also more diﬃcult in this context to come up with “rules of thumb” hj deﬁned over X × Y. 9.3.3 Decision trees We present and discuss the general learning method of decision trees that can be used in multi-class classiﬁcation, but also in other learning problems such as regres- sion  chapter 11  and clustering. Although the empirical performance of decision trees often is not state-of-the-art, decision trees can be used as weak learners with boosting to deﬁne eﬀective learning algorithms. Decision trees are also typically fast to train and evaluate and relatively easy to interpret.  Deﬁnition 9.5  Binary decision tree  A binary decision tree is a tree representation of a partition of the feature space. Figure 9.2 shows a simple example in the case of a two-dimensional space based on two features X1 and X2, as well as the partition it represents. Each interior node of a decision tree corresponds to a question related to the features. It can be a numerical question of the form Xi ≤ a for a feature variable Xi, i ∈ [N ], and some threshold a ∈ R, as in the example of ﬁgure 9.2, or a categorical question such as Xi ∈ {blue, white, red}, when feature Xi takes a categorical value such as a color. Each leaf is labeled with a label l ∈ Y. Decision trees can be deﬁned using more complex node questions, resulting in partitions based on more complex decision surfaces. For example, binary space  X1 < a1  X1 < a2  X2 < a3  X2 < a4  R3  R4  R5  R1  R2  X2  a4  R2  R1  R3  a3  R5  R4  a2 a1  X1   9.3 Uncombined multi-class algorithms  225  GreedyDecisionTrees S =   x1, y1 , . . . ,  xm, ym     1  2  3  4  5  tree ← {n0}  cid:46  root node. for t ← 1 to T do   nt, qt  ← argmax n,q   cid:101 F  n, q   Split tree, nt, qt   return tree  Figure 9.3 Greedy algorithm for building a decision tree from a labeled sample S. The procedure Split tree, nt, qt  splits node nt by making it an internal node with question qt and leaf chil- dren n− n, q  and n+ n, q , each labeled with the dominating class of the region it deﬁnes, with ties broken arbitrarily.  on questions of the form  cid:80 n  partition  BSP  trees partition the space with convex polyhedral regions, based i=1 αiXi ≤ a, and sphere trees partition with pieces of spheres based on questions of the form  cid:107 X − a0 cid:107  ≤ a, where X is a feature vector, a0 a ﬁxed vector, and a is a ﬁxed positive real number. More complex tree questions lead to richer partitions and thus hypothesis sets, which can cause overﬁtting in the absence of a suﬃciently large training sample. They also increase the computational complexity of prediction and training. Decision trees can also be generalized to branching factors greater than two, but binary trees are most commonly used due their more limited computational cost. Prediction partitioning: To predict the label of any point x ∈ X we start at the root node of the decision tree and go down the tree until a leaf is found, by moving to the right child of a node when the response to the node question is positive, and to the left child otherwise. When we reach a leaf, we associate x with the label of this leaf.  Thus, each leaf deﬁnes a region of X formed by the set of points corresponding exactly to the same node responses and thus the same traversal of the tree. By deﬁnition, no two regions intersect and all points belong to exactly one region. Thus, leaf regions deﬁne a partition of X, as shown in the example of ﬁgure 9.2. In multi-class classiﬁcation, the label of a leaf is determined using the training sample: the class with the majority representation among the training points falling in a leaf region deﬁnes the label of that leaf, with ties broken arbitrarily.  Learning: We will discuss two diﬀerent methods for learning a decision tree using a labeled sample. The ﬁrst method is a greedy technique. This is motivated by the fact that the general problem of ﬁnding a decision tree with the smallest   226  Chapter 9 Multi-Class Classiﬁcation  error is NP-hard. The method consists of starting with a tree reduced to a single  root  node, which is a leaf whose label is the class that has majority over the entire sample. Next, at each round, a node nt is split based on some question qt. The pair  nt, qt  is chosen so that the node impurity is maximally decreased according to some measure of impurity F . We denote by F  n  the impurity of n. The decrease in node impurity after a split of node n based on question q is deﬁned as follows. Let n+ n, q  denote the right child of n after the split, n− n, q  the left child, and η n, q  the fraction of the points in the region deﬁned by n that are moved to n− n, q . The total impurity of the leaves n− n, q  and n+ n, q  is therefore  η n, q F  n− n, q  + 1−η n, q  F  n+ n, q  . Thus, the decrease in impurity  cid:101 F  n, q  Figure 9.3 shows the pseudocode of this greedy construction based on  cid:101 F . In practice,   cid:101 F  n, q  = F  n  − [η n, q F  n− n, q   +  1 − η n, q  F  n+ n, q  ].  the algorithm is stopped once all nodes have reached a suﬃcient level of purity, when the number of points per leaf has become too small for further splitting or based on some other similar heuristic. For any node n and class l ∈ [k], let pl n  denote the fraction of points at n that belong to class l. Then, the three most commonly used measures of node impurity F are deﬁned as follows:  by that split is given by  F  n  =  − cid:80 k  cid:80 k  1 − maxl∈[k] pl n   l=1 pl n  log2 pl n   misclassiﬁcation; entropy;  l=1 pl n  1 − pl n   Gini index .  Figure 9.4 illustrates these deﬁnitions in the special cases of two classes  k = 2 . The entropy and Gini index impurity functions are upper bounds on the misclassiﬁcation impurity function. All three functions are concave, which ensures that  F  n  − [η n, q F  n− n, q   +  1 − η n, q  F  n+ n, q  ] ≥ 0.  However, the misclassiﬁcation function is piecewise linear, so  cid:101 F  n, q  is zero if the  fraction of positive points remains less than  or more than  half after a split. In some cases, the impurity cannot be decreased by any split using that criterion. In contrast, the entropy and Gini functions are strictly concave, which guarantees a strict decrease in impurity. Furthermore, they are diﬀerentiable which is a useful feature for numerical optimization. Thus, the Gini index and the entropy criteria are typically preferred in practice.  The greedy method just described faces some issues. One issue relates to the greedy nature of the algorithm: a seemingly bad split may dominate subsequent useful splits, which could lead to trees with less impurity overall. This can be   9.3 Uncombined multi-class algorithms  227  Figure 9.4 Three node impurity deﬁnitions plotted as a function of the fraction of positive examples in the binary case: misclassiﬁcation, entropy  scaled by 0.5 to set the maximum to the same value for all three functions , and the Gini index.  addressed to a certain extent by using a look-ahead of some depth d to determine the splitting decisions, but such look-aheads can be computationally very costly. Another issue relates to the size of the resulting tree. To achieve some desired level of impurity, trees of relatively large sizes may be needed. However, larger trees deﬁne overly complex hypotheses with high VC-dimensions  see exercise 9.5  and thus could overﬁt.  An alternative method for learning decision trees using a labeled training sample is based on the so-called grow-then-prune strategy. First a very large tree is grown until it fully ﬁts the training sample or until no more than a very small number of points are left at each leaf. Then, the resulting tree, denoted as tree, is pruned back to minimize an objective function deﬁned  based on generalization bounds  as the sum of an empirical error and a complexity term. The complexity can be expressed  in terms of the size of  cid:103 tree, the set of leaves of tree. The resulting objective is   9.15   Gλ tree  =  cid:88 n∈ cid:103 tree  nF  n  + λ cid:103 tree ,  where λ ≥ 0 is a regularization parameter determining the trade-oﬀ between mis- classiﬁcation, or more generally impurity, versus tree complexity. For any tree tree cid:48 ,  we denote by  cid:98 R tree cid:48   the total empirical error cid:80 n∈ cid:103 tree cid:48  nF  n . We seek a sub-tree  treeλ of tree that minimizes Gλ and that has the smallest size. treeλ can be shown to be unique. To determine treeλ, the following pruning method is used, which deﬁnes a ﬁnite sequence of nested sub-trees tree 0 , . . . , tree n . We start with the full tree tree 0  = tree and for any i ∈ {0, . . . , n − 1}, deﬁne tree i+1  from tree i  by   228  Chapter 9 Multi-Class Classiﬁcation  collapsing an internal node n cid:48  of tree i , that is by replacing the sub-tree rooted at n cid:48  with a leaf, or equivalently by combining the regions of all the leaves dominated by n cid:48 . n cid:48  is chosen so that collapsing it causes the smallest per node increase in   cid:98 R tree i  , that is the smallest r tree i , n cid:48   deﬁned by  cid:103 tree cid:48  − 1  r tree i , n cid:48   = n cid:48 F  n cid:48   −  cid:98 R tree cid:48    ,  where n cid:48  is an internal node of tree i . If several nodes n cid:48  in tree i  cause the same smallest increase per node r tree i , n cid:48  , then all of them are pruned to deﬁne tree i+1  from tree i . This procedure continues until the tree tree n  obtained has a single node. The sub-tree treeλ can be shown to be among the elements of the sequence tree 0 , . . . , tree n . The parameter λ is determined via n-fold cross- validation.  Decision trees seem relatively easy to interpret, and this is often underlined as one of their most useful features. However, such interpretations should be carried out with care since decision trees are unstable: small changes in the training data may lead to very diﬀerent splits and thus entirely diﬀerent trees, as a result of their hierarchical nature. Decision trees can also be used in a natural manner to deal with the problem of missing features, which often appears in learning applications; in practice, some features values may be missing because the proper measurements were not taken or because of some noise source causing their systematic absence. In such cases, only those variables available at a node can be used in prediction. Finally, decision trees can be used and learned from data in a similar way in the regression setting  see chapter 11 .14  9.4 Aggregated multi-class algorithms  In this section, we discuss a diﬀerent approach to multi-class classiﬁcation that reduces the problem to that of multiple binary classiﬁcation tasks. A binary clas- siﬁcation algorithm is then trained for each of these tasks independently, and the multi-class predictor is deﬁned as a combination of the hypotheses returned by each of these algorithms. We ﬁrst discuss two standard techniques for the reduction of multi-class classiﬁcation to binary classiﬁcation, and then show that they are both special instances of a more general framework.  14 The only changes to the description for classiﬁcation are the following. For prediction, the label of a leaf is deﬁned as the mean squared average of the labels of the points falling in that region. For learning, the impurity function is the mean squared error.   9.4 Aggregated multi-class algorithms  229  9.4.1 One-versus-all Let S =   x1, y1 , . . . , xm, ym   ∈  X×Y m be a labeled training sample. A straight- forward reduction of the multi-class classiﬁcation to binary classiﬁcation is based on the so-called one-versus-all  OVA  or one-versus-the-rest technique. This tech- nique consists of learning k binary classiﬁers hl : X → {−1, +1}, l ∈ Y, each seeking to discriminate one class l ∈ Y from all the others. For any l ∈ Y, hl is obtained by training a binary classiﬁcation algorithm on the full sample S after relabeling points in class l with 1 and all others with −1. For l ∈ Y, assume that hl is derived from the sign of a scoring function fl : X → R, that is hl = sgn fl , as in the case of many of the binary classiﬁcation algorithms discussed in the previous chapters. Then, the multi-class hypothesis h : X → Y deﬁned by the OVA technique is given by:  ∀x ∈ X,  h x  = argmax  fl x .  l∈Y   9.16   This formula may seem similar to those deﬁning a multi-class classiﬁcation hypoth- esis in the case of uncombined algorithms. Note, however, that for uncombined algorithms the functions fl are learned together, while here they are learned inde- pendently. Formula  9.16  is well-founded when the scores given by functions fl can be interpreted as conﬁdence scores, that is when fl x  is learned as an esti- mate of the probability of x conditioned on class l. However, in general, the scores given by functions fl, l ∈ Y, are not comparable and the OVA technique based on  9.16  admits no principled justiﬁcation. This is sometimes referred to as a cali- bration problem. Clearly, this problem cannot be corrected by simply normalizing the scores of each function to make their magnitudes uniform, or by applying other similar heuristics. When it is justiﬁable, the OVA technique is simple and its com- putational cost is k times that of training a binary classiﬁcation algorithm, which is similar to the computation costs for many uncombined algorithms.  9.4.2 One-versus-one An alternative technique, known as the one-versus-one  OVO  technique, consists of using the training data to learn  independently , for each pair of distinct classes  l, l cid:48   ∈ Y2, l  cid:54 = l cid:48 , a binary classiﬁer hll cid:48  : X → {−1, 1} discriminating between classes l and l cid:48 . For any  l, l cid:48   ∈ Y2, hll cid:48  is obtained by training a binary classiﬁcation algorithm on the sub-sample containing exactly the points labeled with l or l cid:48 , with the value +1 returned for class l cid:48  and −1 for class l. This requires training  cid:0 k 2 cid:1  = k k− 1  2 classiﬁers, which are combined to deﬁne a multi-class classiﬁcation hypothesis h via majority vote:  ∀x ∈ X,  h x  = argmax  l cid:48 ∈Y  cid:12  cid:12 {l : hll cid:48  x  = 1} cid:12  cid:12 .   9.17    230  Chapter 9 Multi-Class Classiﬁcation  Table 9.1 Comparison of the time complexity the OVA and OVO techniques for both training and testing. The table assumes a full training sample of size m with each class represented by m k points. The time for training a binary classiﬁcation algorithm on a sample of size n is assumed to be in O nα . Thus, the training time for the OVO technique is in O k2 m k α  = O k2−αmα . ct denotes the cost of testing a single classiﬁer.  Testing OVA O kct  OVO O k2−αmα  O k2ct   Training O kmα   l cid:48  since  cid:12  cid:12 {l : hll cid:48  x  = 1} cid:12  cid:12  = k − 1 and no other class can reach  k − 1  wins. By  Thus, for a ﬁxed point x ∈ X, if we describe the prediction values hll cid:48  x  as the results of the matches in a tournament between two players l and l cid:48 , with hll cid:48  x  = 1 indicating l cid:48  winning over l, then the class predicted by h can be interpreted as the one with the largest number of wins in that tournament. Let x ∈ X be a point belonging to class l cid:48 . By deﬁnition of the OVO technique, if hll cid:48  x  = 1 for all l  cid:54 = l cid:48 , then the class associated to x by OVO is the correct class contraposition, if the OVO hypothesis misclassiﬁes x, then at least one of the  k−1  binary classiﬁers hll cid:48 , l  cid:54 = l cid:48 , incorrectly classiﬁes x. Assume that the generalization error of all binary classiﬁers hll cid:48  used by OVO is at most r, then, in view of this discussion, the generalization error of the hypothesis returned by OVO is at most  k − 1 r. The OVO technique is not subject to the calibration problem pointed out in the case of the OVA technique. However, when the size of the sub-sample containing members of the classes l and l cid:48  is relatively small, hll cid:48  may be learned without suﬃcient data or with increased risk of overﬁtting. Another concern often raised for the use of this technique is the computational cost of training k k − 1  2 binary classiﬁers versus that of the OVA technique.  Taking a closer look at the computational requirements of these two methods reveals, however, that the disparity may not be so great and that in fact under some assumptions the time complexity of training for OVO could be less than that of OVA. Table 9.1 compares the computational complexity of these methods both for training and testing assuming that the complexity of training a binary classiﬁer on a sample of size m is in O mα  and that each class is equally represented in the training set, that is by m k points. Under these assumptions, if α ∈ [2, 3  as in the case of some algorithms solving a QP problem, such as SVMs, then the time complexity of training for the OVO technique is in fact more favorable than that of OVA. For α = 1, the two are comparable and it is only for sub-linear algorithms that the OVA technique would beneﬁt from a better complexity. In all cases, at test time, OVO requires k k − 1  2 classiﬁer evaluations, which is  k − 1    9.4 Aggregated multi-class algorithms  231  times more than OVA. However, for some algorithms the evaluation time for each classiﬁer could be much smaller for OVO. For example, in the case of SVMs, the average number of support vectors may be signiﬁcantly smaller for OVO, since each classiﬁer is trained on a signiﬁcantly smaller sample. If the number of support vectors is k times smaller and if sparse feature representations are used, then the time complexities of both techniques for testing are comparable.  9.4.3 Error-correcting output codes A more general method for the reduction of multi-class to binary classiﬁcation is based on the idea of error-correcting output codes  ECOC . This technique consists of assigning to each class l ∈ Y a code word of length c ≥ 1, which in the simplest case is a binary vector Ml ∈ {−1, +1}c. Ml serves as a signature for class l, and together these vectors deﬁne a matrix M ∈ {−1, +1}k×c whose lth row is Ml, as illustrated by ﬁgure 9.5. Next, for each column j ∈ [c], a binary classiﬁer hj : X → {−1, +1} is learned using the full training sample S, after all points that belong to a class represented by +1 in column j are labeled with +1, while all other points are labeled with −1. For any x ∈ X, let h x  denote the vector h x  =  h1 x , . . . , hc x   cid:62 . Then, the multi-class hypothesis h : X → Y is deﬁned by  9.18   h x  = argmin  ∀x ∈ X,  dH cid:0 Ml, h x  cid:1 .  l∈Y  Thus, the class predicted is the one whose signatures is the closest to h x  in Hamming distance. Figure 9.5 illustrates this deﬁnition: no row of matrix M matches the vector of predictions h x  in that case, but the third row shares the largest number of components with h x .  The success of the ECOC technique depends on the minimal Hamming distance  between the class code words. Let d denote that distance, then up to r0 = cid:4  d−1 2  cid:5   binary classiﬁcation errors can be corrected by this technique: by deﬁnition of d, even if r < r0 binary classiﬁers hl misclassify x ∈ X, h x  is closest to the code word of the correct class of x. For a ﬁxed c, the design of error-correction matrix M is subject to a trade-oﬀ, since larger d values may imply substantially more diﬃcult binary classiﬁcation tasks. In practice, each column may correspond to a class feature determined based on domain knowledge.  The ECOC technique just described can be extended in two ways. First, instead of using only the label predicted by each classiﬁer hj the magnitude of the scores deﬁning hj is used. Thus, if hj = sgn fj  for some function fj whose values can be interpreted as conﬁdence scores, then the multi-class hypothesis h : X → Y is   232  Chapter 9 Multi-Class Classiﬁcation  Figure 9.5 Illustration of error-correcting output codes for multi-class classiﬁcation. Left: binary code matrix M, with each row representing the code word of length c = 6 of a class l ∈ [8]. Right: vector of predictions h x  for a test point x. The ECOC classiﬁer assigns label 3 to x, since the binary code for the third class yields the minimal Hamming distance with h x   distance of 1 .  deﬁned by  ∀x ∈ X,  h x  = argmin  L mljfj x  ,   9.19   where  mlj  are the entries of M and where L : R → R+ is a loss function. When L is deﬁned by L x  = 1−sgn x   for all x ∈ X and hl = fl, we can write:  2  c cid:88 j=1  l∈Y  L mljfj x   =  1 − sgn mljhj x    2  = dh Ml, h x  ,  c cid:88 j=1  c cid:88 j=1  and  9.19  coincides with  9.18 . Furthermore, ternary codes can be used with matrix entries in {−1, 0, +1} so that examples in classes labeled with 0 are disre- garded when training a binary classiﬁer for each column. With these extensions, both OVA and OVO become special instances of the ECOC technique. The matrix M for OVA is a square matrix, that is c = k, with all terms equal to −1 except from the diagonal ones which are all equal to +1. The matrix M for OVO has c = k k − 1  2 columns. Each column corresponds to a pair of distinct classes  l, l cid:48  , l  cid:54 = l cid:48 , with all entries equal to 0 except from the one with row l, which is −1, and the one with row l cid:48 , which is +1. Since the values of the scoring functions are assumed to be conﬁdence scores, mljfj x  can be interpreted as the margin of classiﬁer j on point x and  9.19  is thus based on some loss L deﬁned with respect to the binary classiﬁer’s margin.  A further extension of ECOC consists of extending discrete codes to continuous ones by letting the matrix entries take arbitrary real values and by using the training sample to learn matrix M. Starting with a discrete version of M, c binary classiﬁers  codes  1  2  3  4  5  6  1 -1 -1 -1 +1 -1 -1  2 +1 -1 -1 -1 -1 -1  f1 x  f2 x  f3 x  f4 x  f5 x  f6 x   +1 +1 new example    x  -1 +1 +1  3 -1 +1 +1 -1 +1 -1  -1  s e s s a l  c  4 +1 +1 -1 -1 -1 -1  5 +1 +1 -1 -1 +1 -1  6 -1 -1 +1 +1 -1 +1  7 -1 -1 +1 -1 -1 -1  8 -1 +1 -1 +1 -1 -1   9.5 Structured prediction algorithms  233  with scoring functions fl, l ∈ [c], are ﬁrst learned as described previously. We will denote by F x  the vector  f1 x , . . . , fc x   cid:62  for any x ∈ X. Next, the entries of M are relaxed to take real values and learned from the training sample with the objective of making the row of M corresponding to the class of any point x ∈ X more similar to F x  than other rows. The similarity can be measured using any PDS kernel K. An example of an algorithm for learning M using a PDS kernel K and the idea just discussed is in fact multi-class SVMs, which, in this context, can be formulated as follows:  M,ξ  cid:107 M cid:107 2 min  F + C  ξi  m cid:88 i=1  subject to: ∀ i, l  ∈ [m] × Y,  K f  xi , Myi  ≥ K f  xi , Ml  + 1 − ξi.  Similar algorithms can be deﬁned using other matrix norms. The resulting multi- class classiﬁcation decision function has the following form:  h : x  cid:55 → argmax l∈{1,...,k}  K f  x , Ml .  9.5 Structured prediction algorithms  In this section, we brieﬂy discuss an important class of problems related to multi- class classiﬁcation that frequently arises in computer vision, computational biology, and natural language processing. These include all sequence labeling problems and complex problems such as parsing, machine translation, and speech recognition.  In these applications, the output labels have a rich internal structure. For exam- ple, in part-of-speech tagging the problem consists of assigning a part-of-speech tag such as N  noun , V  verb , or A  adjective , to every word of a sentence. Thus, the label of the sentence ω1 . . . ωn made of the words ωi is a sequence of part-of-speech tags t1 . . . tn. This can be viewed as a multi-class classiﬁcation problem where each sequence of tags is a possible label. However, several critical aspects common to such structured output problems make them distinct from the standard multi-class classiﬁcation.  First, the label set is exponentially large as a function of the size of the output. For example, if Σ denotes the alphabet of part-of-speech tags, for a sentence of length n there are Σn possible tag sequences. Second, there are dependencies between the substructures of a label that are important to take into account for an accurate prediction. For example, in part-of-speech tagging, some tag sequences may be ungrammatical or unlikely. Finally, the loss function used is typically not a  zero-one loss, but one that depends on the substructures. Let L : Y× Y → R denote   234  Chapter 9 Multi-Class Classiﬁcation  a loss function such that L y cid:48 , y  measures the penalty of predicting the label y cid:48  ∈ Y instead of the correct label y ∈ Y.15 In part-of-speech tagging, L y cid:48 , y  could be for example the Hamming distance between y cid:48  and y. The relevant features in structured output problems often depend on both the input and the output. Thus, we will denote by Φ x, y  ∈ RN the feature vector associated to a pair  x, y  ∈ X × Y. To model the label structures and their dependency, the label set Y is typically assumed to be endowed with a graphical model structure, that is, a graph giving a probabilistic model of the conditional dependence between the substructures. It is also assumed that both the feature vector Φ x, y  associated to an input x ∈ X and output y ∈ Y and the loss L y cid:48 , y  factorize according to the cliques of that graphical model.16 A detailed treatment of this topic would require a further background in graphical models, and is thus beyond the scope of this section.  The hypothesis set used by most structured prediction algorithms is then deﬁned as the set of functions h : X → Y such that  ∀x ∈ X,  h x  = argmax  w · Φ x, y ,  y∈Y   9.20   for some vector w ∈ RN . Let S =   x1, y1 , . . . , xm, ym   ∈  X × Y m be an i.i.d.  labeled sample. Since the hypothesis set is linear, we can seek to deﬁne an algorithm similar to multi-class SVMs. The optimization problem for multi-class SVMs can be rewritten equivalently as follows:  min  w  1 2 cid:107 w cid:107 2 +C  m cid:88 i=1  max y cid:54 =yi  max cid:16 0, 1 − w · [Φ xi, yi −Φ xi, y ] cid:17 ,   9.21   However, here we need to take into account the loss function L, that is L y, yi  for each i ∈ [m] and y ∈ Y, and there are multiple ways to proceed. One possible way is to let the margin violation be penalized additively with L y, yi . Thus, in that case L y, yi  is added to the margin violation. Another natural method consists of penalizing the margin violation by multiplying it with L y, yi . A margin violation with a larger loss is then penalized more than one with a smaller loss.  15 More generally, in some applications, the loss function could also depend on the input. Thus, L is then a function mapping L : X× Y× Y → R, with L x, y cid:48 , y  measuring the penalty of predicting the label y cid:48  instead of y given the input x. 16 In an undirected graph, a clique is a set of fully connected vertices.   9.6 Chapter notes  235  The additive penalization leads to the following algorithm known as Maximum  Margin Markov Networks  M3N :  min  w  1 2 cid:107 w cid:107 2 +C  m cid:88 i=1  max y cid:54 =yi  max cid:16 0, L yi, y  − w · [Φ xi, yi −Φ xi, y ] cid:17 .   9.22   An advantage of this algorithm is that, as in the case of SVMs, it admits a natural use of PDS kernels. As already indicated, the label set Y is assumed to be endowed with a graph structure with a Markov property, typically a chain or a tree, and the loss function is assumed to be decomposable in the same way. Under these assump- tions, by exploiting the graphical model structure of the labels, a polynomial-time algorithm can be given to determine its solution.  A multiplicative combination of the loss with the margin leads to the following  algorithm known as SVMStruct:  min  w  1 2 cid:107 w cid:107 2 +C  m cid:88 i=1  max y cid:54 =yi  L yi, y  max cid:16 0, 1 − w · [Φ xi, yi −Φ xi, y ] cid:17 .   9.23   This problem can be equivalently written as a QP with an inﬁnite number of con- straints. In practice, it is solved iteratively by augmenting at each round the ﬁnite set of constraints of the previous round with the most violating constraint. This method can be applied in fact under very general assumptions and for arbitrary loss deﬁnitions. As in the case of M3N, SVMStruct naturally admits the use of PDS kernels and thus an extension to non-linear models for the solution.  Another standard algorithm for structured prediction problems is Conditional Random Fields  CRFs . We will not describe this algorithm in detail, but point out its similarity with the algorithms just described, in particular M3N. The opti- mization problem for CRFs can be written as  exp cid:16 L yi, y  − w · [Φ xi, yi −Φ xi, y ] cid:17 .   9.24   w  min  m cid:88 i=1  1 2 cid:107 w cid:107 2 +C  log cid:88 y∈Y function  x1, . . . , xk   cid:55 → log  cid:80 k  Assume for simplicity that Y is ﬁnite and has cardinality k and let f denote the j=1 exj  . f is a convex function known as the soft- max , since it provides a smooth approximation of  x1, . . . , xk   cid:55 → max x1, . . . , xk . Then, problem  9.24  is similar to  9.22  modulo the replacement of the max oper- ator with the soft-max function just described.  9.6 Chapter notes  The margin-based generalization bound for multi-class classiﬁcation presented in theorem 9.2 is due to Kuznetsov, Mohri, and Syed [2014]. It admits only a lin-   236  Chapter 9 Multi-Class Classiﬁcation  ear dependency on the number of classes. This improves over a similar result by Koltchinskii and Panchenko [2002], which admits a quadratic dependency on the number of classes. Proposition 9.3 bounding the Rademacher complexity of multi- class kernel-based hypotheses and corollary 9.4 are new.  An algorithm generalizing SVMs to the multi-class classiﬁcation setting was ﬁrst introduced by Weston and Watkins [1999]. The optimization problem for that algorithm was based on k k − 1  2 slack variables for a problem with k classes and thus could be ineﬃcient for a relatively large number of classes. A simpliﬁcation of  that algorithm by replacing the sum of the slack variables cid:80 j cid:54 =i ξij related to point  xi by its maximum ξi = maxj cid:54 =i ξij considerably reduces the number of variables and leads to the multi-class SVM algorithm presented in this chapter [Crammer and Singer, 2001, 2002].  The AdaBoost.MH algorithm is presented and discussed by Schapire and Singer [1999, 2000]. As we showed in this chapter, the algorithm is a special instance of AdaBoost. Another boosting-type algorithm for multi-class classiﬁcation, Ad- aBoost.MR, is presented by Schapire and Singer [1999, 2000]. That algorithm is also a special instance of the RankBoost algorithm presented in chapter 10. See ex- ercise 10.5 for a detailed analysis of this algorithm, including generalization bounds. The most commonly used tools for learning decision trees are CART  classiﬁcation and regression tree  [Breiman et al., 1984] and C4.5 [Quinlan, 1986, 1993]. The greedy technique we described for learning decision trees beneﬁts in fact from an interesting analysis: remarkably, it has been shown by Kearns and Mansour [1999], Mansour and McAllester [1999] that, under a weak learner hypothesis assumption, such decision tree algorithms produce a strong hypothesis. The grow-then-prune method is from CART. It has been analyzed by a variety of diﬀerent studies, in particular by Kearns and Mansour [1998] and Mansour and McAllester [2000], who give generalization bounds for the resulting decision trees with respect to the error and size of the best sub-tree of the original tree pruned. Hardness of ERM for decision trees of a ﬁxed size was shown by Grigni et al. [2000].  The idea of the ECOC framework for multi-class classiﬁcation is due to Diet- terich and Bakiri [1995]. Allwein et al. [2000] further extended and analyzed this method to margin-based losses, for which they presented a bound on the empirical error and a generalization bound in the more speciﬁc case of boosting. While the OVA technique is in general subject to a calibration issue and does not have any justiﬁcation, it is very commonly used in practice. Rifkin [2002] reports the results of extensive experiments with several multi-class classiﬁcation algorithms that are rather favorable to the OVA technique, with performances often very close or better than for those of several uncombined algorithms, unlike what has been claimed by some authors  see also Rifkin and Klautau [2004] .   9.7 Exercises  237  The CRFs algorithm was introduced by Laﬀerty, McCallum, and Pereira [2001]. M3N is due to Taskar, Guestrin, and Koller [2003] and StructSVM was presented by Tsochantaridis, Joachims, Hofmann, and Altun [2005]. An alternative technique for tackling structured prediction as a regression problem was presented and analyzed by Cortes, Mohri, and Weston [2007c].  9.7 Exercises  9.1 Generalization bounds for multi-label case. Use similar techniques to those used in the proof of theorem 9.2 to derive a margin-based learning bound in the multi-label case.  9.2 Multi-class classiﬁcation with kernel-based hypotheses constrained by an Lp norm. Use corollary 9.4 to deﬁne alternative multi-class classiﬁcation algorithms with kernel-based hypotheses constrained by an Lp norm with p  cid:54 = 2. For which value of p ≥ 1 is the bound of proposition 9.3 tightest? Derive the dual optimization of the multi-class classiﬁcation algorithm deﬁned with p = ∞.  9.3 Alternative multi-class boosting algorithm. Consider the objective function G deﬁned for any sample S =   x1, y1 , . . . ,  xm, ym   ∈  X × Y m and α =  α1, . . . , αn  ∈ Rn, n ≥ 1, by  G α  =  e− 1  k  l=1 yi[l]fn xi,l  =  e− 1  k  t=1 αtht xi,l .   9.25    cid:80 k l=1 yi[l] cid:80 n   cid:80 k  m cid:88 i=1  m cid:88 i=1  Use the convexity of the exponential function to compare G with the objective function F deﬁning AdaBoost.MH. Show that G is a convex function upper bounding the multi-label multi-class error. Discuss the properties of G and derive an algorithm deﬁned by the application of coordinate descent to G. Give theoretical guarantees for the performance of the algorithm and analyze its running-time complexity when using boosting stumps.  9.4 Multi-class algorithm based on RankBoost. This problem requires familiarity with the material presented both in this chapter and in chapter 10. An alterna- tive boosting-type multi-class classiﬁcation algorithm is one based on a ranking criterion. We will deﬁne and examine that algorithm in the mono-label setting. Let H be a family of base hypotheses mapping X× Y to {−1, +1}. Let F be the   238  Chapter 9 Multi-Class Classiﬁcation  following objective function deﬁned for all samples S =   x1, y1 , . . . ,  xm, ym   ∈  X × Y m and ¯α =  ¯α1, . . . , ¯αN   ∈ RN , N ≥ 1, by m cid:88 i=1 cid:88 l cid:54 =yi e−  j=1 ¯αj  hj  xi,yi −hj  xi,l  .  e− fN  xi,yi −fN  xi,l   =   cid:80 N   9.26   F   ¯α  =  m cid:88 i=1 cid:88 l cid:54 =yi where fN = cid:80 N  j=1 ¯αjhj.   a  Show that F is convex and diﬀerentiable.  b  Show that 1  i=1 1ρfN  xi,yi  ≤ 1  m cid:80 m  k−1 F   ¯α , where fN = cid:80 N  j=1 ¯αjhj.   c  Give the pseudocode of the algorithm obtained by applying coordinate de- scent to F . The resulting algorithm is known as AdaBoost.MR. Show that AdaBoost.MR exactly coincides with the RankBoost algorithm applied to the problem of ranking pairs  x, y  ∈ X × Y. Describe exactly the ranking target for these pairs.   d  Use question  9.4b  and the learning bounds of this chapter to derive margin-  based generalization bounds for this algorithm.   e  Use the connection of the algorithm with RankBoost and the learning bounds of chapter 10 to derive alternative generalization bounds for this algorithm. Compare these bounds with those of the previous question.  9.5 Decision trees. Show that VC-dimension of a binary decision tree with n nodes  in dimension N is in O n log N  .  9.6 Give an example where the generalization error of each of the k k − 1  2 binary classiﬁers hll cid:48 , l  cid:54 = l cid:48 , used in the deﬁnition of the OVO technique is r and that of the OVO hypothesis  k − 1 r.   10 Ranking  The learning problem of ranking arises in many modern applications, including the design of search engines, information extraction platforms, and movie recommen- dation systems. In these applications, the ordering of the documents or movies returned is a critical aspect of the system. The main motivation for ranking over classiﬁcation in the binary case is the limitation of resources: for very large data sets, it may be impractical or even impossible to display or process all items labeled as relevant by a classiﬁer. A standard user of a search engine is not willing to con- sult all the documents returned in response to a query, but only the top ten or so. Similarly, a member of the fraud detection department of a credit card company cannot investigate thousands of transactions classiﬁed as potentially fraudulent, but only a few dozens of the most suspicious ones.  In this chapter, we study in depth the learning problem of ranking. We distinguish two general settings for this problem: the score-based and the preference-based set- tings. For the score-based setting, which is the most widely explored one, we present margin-based generalization bounds using the notion of Rademacher complexity. We then describe an SVM-based ranking algorithm that can be derived from these bounds and describe and analyze RankBoost, a boosting algorithm for ranking. We further study speciﬁcally the bipartite setting of the ranking problem where, as in binary classiﬁcation, each point belongs to one of two classes. We discuss an eﬃcient implementation of RankBoost in that setting and point out its connec- tions with AdaBoost. We also introduce the notions of ROC curves and area under the ROC curve  AUC  which are directly relevant to bipartite ranking. For the preference-based setting, we present a series of results, in particular regret-based guarantees for both a deterministic and a randomized algorithm, as well as a lower bound in the deterministic case.   240  Chapter 10 Ranking  10.1 The problem of ranking  We ﬁrst introduce the most commonly studied scenario of the ranking problem in machine learning. We will refer to this scenario as the score-based setting of the ranking problem. In section 10.6, we present and analyze an alternative setting, the preference-based setting.  The general supervised learning problem of ranking consists of using labeled in- formation to deﬁne an accurate ranking prediction function for all points. In the scenario examined here, the labeled information is supplied only for pairs of points and the quality of a predictor is similarly measured in terms of its average pairwise misranking. The predictor is a real-valued function, a scoring function: the scores assigned to input points by this function determine their ranking.  Let X denote the input space. We denote by D an unknown distribution over X× X according to which pairs of points are drawn and by f : X× X → {−1, 0, +1} a target labeling function or preference function. The three values assigned by f are interpreted as follows: f  x, x cid:48   = +1 if x cid:48  is preferred to x or ranked higher than x, f  x, x cid:48   = −1 if x is preferred to x cid:48 , and f  x, x cid:48   = 0 if both x and x cid:48  have the same preference or ranking, or if there is no information about their respective ranking. This formulation corresponds to a deterministic scenario which we adopt for simpliﬁcation. As discussed in section 2.4.1, it can be straightforwardly extended to a stochastic scenario where we have a distribution over X × X × {−1, 0, +1}. Note that in general no particular assumption is made about the transitivity of the order induced by f : we may have f  x, x cid:48   = 1 and f  x cid:48 , x cid:48  cid:48   = 1 but f  x, x cid:48  cid:48   = −1 for three points x, x cid:48 , and x cid:48  cid:48 . While this may contradict an intuitive notion of preference, such preference orders are in fact commonly encountered in practice, in particular when they are based on human judgments. This is sometimes because the preference between two items are decided based on diﬀerent features: for example, an individual may prefer movie x cid:48  to x because x cid:48  is an action movie and x a musical, and prefer x cid:48  cid:48  to x cid:48  because x cid:48  cid:48  is an action movie with more special eﬀects than x cid:48 . Nevertheless, they may prefer x to x cid:48  cid:48  because the cost of the movie ticket for x cid:48  cid:48  is signicantly higher. Thus, in this example, two features, the genre and the price, are invoked, each aﬀecting the decision for diﬀerent pairs. In fact, in general, no assumption is made about the preference function, not even the antisymmetry of the order induced; thus, we may have f  x, x cid:48   = 1 and f  x cid:48 , x  = 1 and yet x  cid:54 = x cid:48 . The learner receives a labeled sample S = cid:0  x1, x cid:48 1, y1 , . . . ,  xm, x cid:48 m, ym  cid:1  ∈ X × X × {−1, 0, +1} with  x1, x cid:48 1 , . . . ,  xm, x cid:48 m  drawn i.i.d. according to D and yi = f  xi, x cid:48 i  for all i ∈ [m]. Given a hypothesis set H of functions mapping X to R, the ranking problem consists of selecting a hypothesis h ∈ H with small expected   10.2 Generalization bound  241   10.1   pairwise misranking or generalization error R h  with respect to the target f :  R h  =  P   x,x cid:48  ∼D cid:104  cid:0 f  x, x cid:48    cid:54 = 0 cid:1  ∧ cid:0 f  x, x cid:48   h x cid:48   − h x   ≤ 0 cid:1  cid:105  .  The empirical pairwise misranking or empirical error of h is denoted by  cid:98 RS h  and  deﬁned by  1 yi cid:54 =0 ∧ yi h x cid:48 i −h xi  ≤0  .   10.2   Note that while the target preference function f is in general not transitive, the linear ordering induced by a scoring function h ∈ H is by deﬁnition transitive. This is a drawback of the score-based setting for the ranking problem since, regardless of the complexity of the hypothesis set H, if the preference function is not transitive, no hypothesis h ∈ H can faultlessly predict the target pairwise ranking. 10.2 Generalization bound  1 m  m cid:88 i=1   cid:98 RS h  =  In this section, we present margin-based generalization bounds for ranking. To simplify the presentation, we will assume for the results of this section that the pairwise labels are in {−1, +1}. Thus, if a pair  x, x cid:48   is drawn according to D, then either x is preferred to x cid:48  or the opposite. The learning bounds for the general case have a very similar form but require more details. As in the case of classiﬁcation, for any ρ > 0, we can deﬁne the empirical margin loss of a hypothesis h for pairwise ranking as  Φρ yi h x cid:48 i  − h xi   ,   10.3   where Φρ is the margin loss function  deﬁnition 5.5 . Thus, the empirical margin loss for ranking is upper bounded by the fraction of the pairs  xi, x cid:48 i  that h is misranking or correctly ranking but with conﬁdence less than ρ:  1yi h x cid:48 i −h xi  ≤ρ.   10.4   We denote by D1 the marginal distribution of the ﬁrst element of the pairs in X× X derived from D, and by D2 the marginal distribution with respect to the second element of the pairs. Similarly, S1 is the sample derived from S by keeping only the  ﬁrst element of each pair: S1 = cid:0  x1, y1 , . . . ,  xm, ym  cid:1  and S2 the one obtained by keeping only the second element: S2 = cid:0  x cid:48 1, y1 , . . . ,  x cid:48 m, ym  cid:1 . We also denote by m  H  = E[ cid:98 RS2 H ]. Clearly,  m  H  = E[ cid:98 RS1 H ], and similarly RD2  m  H  the Rademacher complexity of H with respect to the marginal distribution  RD1 D1, that is RD1   cid:98 RS,ρ h  =  1 m  m cid:88 i=1   cid:98 RS,ρ h  ≤  1 m  m cid:88 i=1   242  Chapter 10 Ranking  m  H .  m  H  = RD2  if the distribution D is symmetric, the marginal distributions D1 and D2 coincide and RD1 Theorem 10.1  Margin bound for ranking  Let H be a set of real-valued functions. Fix ρ > 0; then, for any δ > 0, with probability at least 1− δ over the choice of a sample S of size m, each of the following holds for all h ∈ H:  R h  ≤  cid:98 RS,ρ h  + R h  ≤  cid:98 RS,ρ h  +  2  m  H  + RD2  m  H  cid:1  + cid:115  log 1 ρ cid:0 RD1 ρ cid:0  cid:98 RS1 H  + cid:98 RS2  H  cid:1  + 3 cid:115  log 2  δ 2m  δ 2m  2   10.5   .   10.6   .  Proof: The proof is similar to that of theorem 5.8. Let  cid:101 H be the family of hy- potheses mapping  X × X  × {−1, +1} to R deﬁned by  cid:101 H = {z =   x, x cid:48  , y   cid:55 → y[h x cid:48   − h x ] : h ∈ H}. Consider the family of functions  cid:101 H = {Φρ ◦ f : f ∈  cid:101 H} derived from  cid:101 H which are taking values in [0, 1]. By theorem 3.3, for any δ > 0  with probability at least 1 − δ, for all h ∈ H,  E cid:2 Φρ y[h x cid:48   − h x ]  cid:3  ≤  cid:98 RS,ρ h  + 2Rm cid:0 Φρ ◦  cid:101 H cid:1  + cid:115  log 1  δ 2m  write:  δ 2m  Since Φρ is a  1 ρ -Lipschitz function, by Talagrand’s lemma  lemma 5.7  we have  Since 1u≤0 ≤ Φρ u  for all u ∈ R, the generalization error R h  is a lower bound on left-hand side, R h  = E[1y[h x cid:48  −h x ]≤0] ≤ E cid:2 Φρ y[h x cid:48   − h x ]  cid:3 , and we can  R h  ≤  cid:98 RS,ρ h  + 2Rm cid:0 Φρ ◦  cid:101 H cid:1  + cid:115  log 1 that Rm cid:0 Φρ ◦  cid:101 H cid:1  ≤ 1 ρ Rm  cid:101 H . Here, Rm  cid:101 H  can be upper bounded as follows: S,σ cid:104  sup m cid:88 i=1 Rm  cid:101 H  = S,σ cid:104  sup m cid:88 i=1 S,σ cid:104  sup m cid:88 i=1 S cid:104 RS2 H  + RS1 H  cid:105   σiyi h x cid:48 i  − h xi   cid:105  σi h x cid:48 i  − h xi   cid:105  m cid:88 i=1  σih x cid:48 i  + sup h∈H  σih xi  cid:105    yiσi and σi: same distrib.    deﬁnition of S1 and S2    by sub-additivity of sup   1 ≤ m = E = RD2  h∈H  h∈H  h∈H  1 m  1 m  m  H  + RD1  m  H  ,  E  E  E  =  .  which proves  10.5 . The second inequality,  10.6 , can be derived in the same way  cid:3  by using the second inequality of theorem 3.3,  3.4 , instead of  3.3 .   10.3 Ranking with SVMs  243  increases as a function of ρ.  These bounds can be generalized to hold uniformly for all ρ > 0 at the cost of  other margin bounds presented in previous sections, they show the conﬂict between two terms: the larger the desired pairwise ranking margin ρ, the smaller the middle  an additional term cid:112  log log2 2 ρ   m, as in theorem 5.9 and exercise 5.2. As for term. However, the ﬁrst term, the empirical pairwise ranking margin loss  cid:98 RS,ρ,  Known upper bounds for the Rademacher complexity of a hypothesis set H, including bounds in terms of VC-dimension, can be used directly to make theo- rem 10.1 more explicit. In particular, using theorem 10.1, we obtain immediately the following margin bound for pairwise ranking using kernel-based hypotheses. Corollary 10.2  Margin bounds for ranking with kernel-based hypotheses  Let K : X × X → R be a PDS kernel with r = supx∈X K x, x . Let Φ : X → H be a feature mapping associated to K and let H = {x  cid:55 → w·Φ x  :  cid:107 w cid:107 H ≤ Λ} for some Λ ≥ 0. Fix ρ > 0. Then, for any δ > 0, the following pairwise margin bound holds with probability at least 1 − δ for any h ∈ H:  R h  ≤  cid:98 RS,ρ h  + 4 cid:114  r2Λ2 ρ2  m  + cid:115  log 1  δ 2m  .   10.7   As with theorem 5.8, the bound of this corollary can be generalized to hold uni-  formly for all ρ > 0 at the cost of an additional term  cid:112  log log2 2 ρ   m. This  generalization bound for kernel-based hypotheses is remarkable, since it does not depend directly on the dimension of the feature space, but only on the pairwise ranking margin. It suggests that a small generalization error can be achieved when ρ r is large  small second term  while the empirical margin loss is relatively small  ﬁrst term . The latter occurs when few points are either classiﬁed incorrectly or correctly but with margin less than ρ.  10.3 Ranking with SVMs  In this section, we discuss an algorithm that is derived directly from the theoretical guarantees just presented. The algorithm turns out to be a special instance of the SVM algorithm.  Proceeding as in section 5.4 for classiﬁcation, the guarantee of corollary 10.2 can for any δ > 0, with probability at least 1 − δ, for all  be expressed as follows: h ∈ H = {x  cid:55 → w · Φ x  :  cid:107 w cid:107  ≤ Λ},  R h  ≤  1 m  m cid:88 i=1  ξi + 4 cid:114  r2Λ2  m  + cid:115  log 1  δ 2m  ,   10.8   where ξi = max cid:0 1− yi cid:2 w· cid:0 Φ x cid:48 i −Φ xi  cid:1  cid:3 , 0 cid:1  for all i ∈ [m], and where Φ : X → H  is a feature mapping associated to a PDS kernel K. An algorithm based on this   244  Chapter 10 Ranking  theoretical guarantee consists of minimizing the right-hand side of  10.8 , that is minimizing an objective function with a term corresponding to the sum of the slack variables ξi, and another one minimizing  cid:107 w cid:107  or equivalently  cid:107 w cid:107 2. Its optimization problem can thus be formulated as  min w,ξ  1 2 cid:107 w cid:107 2 + C  ξi  m cid:88 i=1  subject to: yi cid:104 w · cid:0 Φ x cid:48 i  − Φ xi  cid:1  cid:105  ≥ 1 − ξi  ξi ≥ 0,  ∀i ∈ [m] .   10.9   This coincides exactly with the primal optimization problem of SVMs, with a fea-  ture mapping Ψ : X × X → H deﬁned by Ψ x, x cid:48   = Φ x cid:48   − Φ x  for all  x, x cid:48   ∈ X × X, and with a hypothesis set of functions of the form  x, x cid:48    cid:55 → w · Ψ x, x cid:48  . Thus, clearly, all the properties already presented for SVMs apply in this instance. In particular, the algorithm can beneﬁt from the use of PDS kernels. Problem  10.9  admits an equivalent dual that can be expressed in terms of the kernel matrix K cid:48  deﬁned by K cid:48 ij = Ψ xi, x cid:48 i ·Ψ xj, x cid:48 j  = K xi, xj +K x cid:48 i, x cid:48 j −K x cid:48 i, xj −K xi, x cid:48 j ,  10.10  for all i, j ∈ [m]. This algorithm can provide an eﬀective solution for pairwise ranking in practice. The algorithm can also be used and extended to the case where the labels are in {−1, 0, +1}. The next section presents an alternative algorithm for ranking in the score-based setting.  10.4 RankBoost  This section presents a boosting algorithm for pairwise ranking, RankBoost, similar to the AdaBoost algorithm for binary classiﬁcation. RankBoost is based on ideas analogous to those discussed for classiﬁcation: it consists of combining diﬀerent base rankers to create a more accurate predictor. The base rankers are hypotheses returned by a weak learning algorithm for ranking. As for classiﬁcation, these base hypotheses must satisfy a minimal accuracy condition that will be described precisely later.  Let H denote the hypothesis set from which the base rankers are selected. Al- gorithm 10.1 gives the pseudocode of the RankBoost algorithm when H is a set of functions mapping from X to {0, 1}. For any s ∈ {−1, 0, +1}, we deﬁne  cid:15 s  t by   cid:15 s t =  m cid:88 i=1  Dt i 1yi ht x cid:48 i −ht xi  =s = E i∼Dt  [1yi ht x cid:48 i −ht xi  =s],   10.11    10.4 RankBoost  245  RankBoost S =   x1, x cid:48 1, y1  . . . ,  xm, x cid:48 m, ym     1  2  3  4 5  6  7  8  9  for i ← 1 to m do  D1 i  ← 1  m  for t ← 1 to T do 2 log  cid:15 +  cid:15 −t t + 2[ cid:15 +  ht ← base ranker in H with smallest  cid:15 −t −  cid:15 + αt ← 1 t  cid:15 −t ] Zt ←  cid:15 0 for i ← 1 to m do   cid:46  normalization factor  1 2  t  Dt i  exp cid:2 −αtyi cid:0 ht x cid:48 i −ht xi  cid:1  cid:3   Zt  Dt+1 i  ←  t=1 αtht  f ← cid:80 T  10 return f  t = − E  i∼Dt cid:104 yi cid:0 ht x cid:48 i  − ht xi  cid:1  cid:105   Figure 10.1 RankBoost algorithm for H ⊆ {0, 1}X.  t  instead of  cid:15 −1  and simplify the notation  cid:15 +1 these deﬁnitions, clearly the following equality holds:  cid:15 0  t and similarly write  cid:15 −t t +  cid:15 +  into  cid:15 +  The algorithm takes as input a labeled sample S = cid:0  x1, x cid:48 1, y1 , . . . ,  xm, x cid:48 m, ym  cid:1   with elements in X × X × {−1, 0, +1}, and maintains a distribution over the subset of the indices i ∈ {1, . . . , m} for which yi  cid:54 = 0. To simplify the presentation, we will assume that yi  cid:54 = 0 for all i ∈ {1, . . . , m} and consider distributions deﬁned over {1, . . . , m}. This can be guaranteed by simply ﬁrst removing from the sample the pairs labeled with zero.  t +  cid:15 −t = 1.  . With  t  Initially  lines 1–2 , the distribution is uniform  D1 . At each round of boosting, that is at each iteration t ∈ [T ] of the loop 3–8, a new base ranker ht ∈ H is selected with the smallest diﬀerence  cid:15 −t −  cid:15 + t , that is one with the smallest pairwise misranking error and largest correct pairwise ranking accuracy for the distribution Dt:  h∈H  cid:110  − E Note that  cid:15 −t −  cid:15 + t =  cid:15 −t −  1 −  cid:15 −t −  cid:15 0 t − 1. Thus, ﬁnding the smallest is equivalent to seeking the smallest 2 cid:15 −t + cid:15 0 diﬀerence  cid:15 −t − cid:15 + t , which itself coincides with seeking the smallest  cid:15 −t when  cid:15 0 t = 0. Zt is simply a normalization factor to ensure that the weights Dt+1 i  sum to one. RankBoost relies on the assumption  i∼Dt cid:104 yi cid:0 h x cid:48 i  − h xi  cid:1  cid:105  cid:111 .  ht ∈ argmin  t   = 2 cid:15 −t +  cid:15 0  t   246  Chapter 10 Ranking  t −  cid:15 −t > 0 that at each round t ∈ [T ], for the hypothesis ht found, the inequality  cid:15 + holds; thus, the probability mass of the pairs correctly ranked by ht  ignoring pairs with label zero  is larger than that of misranked pairs. We denote by γt the edge of the base ranker ht: γt =  cid:15 +  .  t − cid:15 −t 2  t −  cid:15 −t > 0, then  cid:15 +  The precise reason for the deﬁnition of the coeﬃcient αt  line 5  will become clear t   cid:15 −t > 1 and αt > 0. Thus, later. For now, observe that if  cid:15 + the new distribution Dt+1 is deﬁned from Dt by increasing the weight on i if the pair  xi, x cid:48 i  is misranked  yi ht x cid:48 i  − ht xi  < 0 , and, on the contrary, decreasing it if  xi, x cid:48 i  is ranked correctly  yi ht x cid:48 i  − ht xi  > 0 . The relative weight is unchanged for a pair with ht x cid:48 i  − ht xi  = 0. This distribution update has the eﬀect of focusing more on misranked points at the next round of boosting. After T rounds of boosting, the hypothesis returned by RankBoost is f , which is a linear combination of the base classiﬁers ht. The weight αt assigned to ht in that t and  cid:15 −t . Thus, more accurate base sum is a logarithmic function of the ratio of  cid:15 + rankers are assigned a larger weight in that sum. For any t ∈ [T ], we will denote by ft the linear combination of the base rankers s=1 αtht. In particular, we have fT = f . The distribution Dt+1 can be expressed in terms of ft and the normalization factors Zs, s ∈ [t], as follows:  after t rounds of boosting: ft = cid:80 t  ∀i ∈ [m], Dt+1 i  =   10.12   We will make use of this identity several times in the proofs of the following sections. It can be shown straightforwardly by repeatedly expanding the deﬁnition of the distribution over the point xi:  e−yi ft x cid:48 i  −ft xi   .  s=1 Zs  m cid:81 t  Dt i e−αtyi ht x cid:48 i −ht xi    Dt+1 i  =  Zt  Dt−1 i e−αt−1yi ht−1 x cid:48 i −ht−1 xi  e−αtyi ht x cid:48 i −ht xi    =  =   cid:80 t s=1 αs hs x cid:48 i −hs xi    Zt−1Zt  e−yi  .  s=1 Zs  m cid:81 t  10.4.1 Bound on the empirical error We ﬁrst show that the empirical error of RankBoost decreases exponentially fast as a function of the number of rounds of boosting when the edge γt of each base ranker ht is lower bounded by some positive value γ > 0.   10.4 RankBoost  247   10.13    10.14   Theorem 10.3 The empirical error of the hypothesis h : X → {0, 1} returned by Rank- Boost veriﬁes:   cid:17 2 cid:21 .  t −  cid:15 −t  2   cid:98 RS h  ≤ exp cid:20  − 2  T cid:88 t=1 cid:16   cid:15 +  cid:98 RS h  ≤ exp −2γ2T   .  Furthermore, if there exists γ such that for all t ∈ [T ], 0 < γ ≤  cid:15 +  t − cid:15 −t 2  , then  Proof: Using the general inequality 1u≤0 ≤ exp −u  valid for all u ∈ R and iden-  tity 10.12, we can write:  1 m  m cid:88 i=1 m cid:88 i=1 cid:20 m  1 m  m cid:88 i=1   cid:98 RS h  =  1yi f  x cid:48 i −f  xi  ≤0 ≤  e−yi f  x cid:48 i −f  xi    T cid:89 t=1 By the deﬁnition of normalization factor, for all t ∈ [T ], we have Zt = cid:80 m i=1 Dt i  e−αtyi ht x cid:48 i −ht xi  . By grouping together the indices i for which yi ht x cid:48 i −ht xi   takes the values in +1, −1, or 0, Zt can be rewritten as  Zt cid:21 DT +1 i  =  T cid:89 t=1  1 m  Zt.  ≤  Zt =  cid:15 +  t e−αt +  cid:15 −t eαt +  cid:15 0  t =  cid:15 +  t cid:115   cid:15 −t   cid:15 + t  +  cid:15 −t cid:115   cid:15 +  t  cid:15 −t  +  cid:15 0  t = 2 cid:113  cid:15 +  t  cid:15 −t +  cid:15 0 t .  t −  cid:15 −t  2.  Thus, assuming that  cid:15 0  Since  cid:15 +  t = 1 −  cid:15 −t −  cid:15 0 t  cid:15 −t =   cid:15 +  4 cid:15 +  t  t  2 −   cid:15 +  t −  cid:15 −t  2 =  1 −  cid:15 0  t , we have t +  cid:15 −t  2 −   cid:15 + t < 1, Zt can be upper bounded as follows: t −  cid:15 −t  2 +  cid:15 0 t  2 −   cid:15 + t   cid:115 1 − t −  cid:15 −t  2   cid:15 + t  2 +  cid:15 0  1 −  cid:15 0 t −  cid:15 −t  2   cid:15 +  1 −  cid:15 0 t   t −  cid:15 −t  2   cid:15 + 2 1 −  cid:15 0  t −  cid:15 −t  2   cid:15 +  2  t  Zt = cid:113  1 −  cid:15 0 =  1 −  cid:15 0 ≤ cid:115 1 − ≤ exp cid:18 −  t    cid:19  ≤ exp cid:18 − t −  cid:15 −t   2]2 cid:1  , t ≤ 1, and the inequality 1 − x ≤ e−x valid for all x ∈ R t = 1  cid:3   where we used for the ﬁrst inequality the concavity of the square-root function and the fact that 0 < 1 −  cid:15 0 for the second inequality. This upper bound on Zt also trivially holds when  cid:15 0 since in that case  cid:15 +   cid:19  = exp cid:0 −2[  cid:15 +  t =  cid:15 −t = 0. This concludes the proof.   248  Chapter 10 Ranking  As can be seen from the proof of the theorem, the weak ranking assumption γ ≤  cid:15 + t − cid:15 −t with γ > 0 can be replaced with the somewhat weaker requirement γ ≤ 2  cid:15 + t − cid:15 −t , with  cid:15 0 2√1− cid:15 0  cid:54 = 0, where the quantity  cid:15 + can be interpreted as a  normalized  relative diﬀerence between  cid:15 +  t  cid:54 = 1, which can be rewritten as γ ≤ 1  t − cid:15 −t√ cid:15 +  t − cid:15 −t√ cid:15 +  , with  cid:15 +  t +  cid:15 −t  t + cid:15 −t  t + cid:15 −t   cid:15 +  2  t  t and  cid:15 −t .  The proof of the theorem also shows that the coeﬃcient αt is selected to minimize Zt. Thus, overall, these coeﬃcients are chosen to minimize the upper bound on t=1 Zt, as for AdaBoost. The RankBoost algorithm can be  the empirical error  cid:81 T generalized in several ways:   Instead of a hypothesis with minimal diﬀerence  cid:15 −t −  cid:15 + t , ht can be more generally t >  cid:15 −t ; a base ranker returned by a weak ranking algorithm trained on Dt with  cid:15 +   The range of the base rankers could be [0, +1], or more generally R. The coeﬃ- cients αt can then be diﬀerent and may not even admit a closed form. However, t=1 Zt on the empirical  in general, they are chosen to minimize the upper bound cid:81 T  error.  10.4.2 Relationship with coordinate descent RankBoost coincides with the application of the coordinate descent technique to a convex and diﬀerentiable objective function F deﬁned for all samples S =   cid:0  x1, x cid:48 1, y1 , . . . ,  xm, x cid:48 m, ym  cid:1  ∈ X × X × {−1, 0, +1} and ¯α =  ¯α1, . . . , ¯αn  ∈ RN ,  N ≥ 1 by   cid:80 N j=1 ¯αj [hj  x cid:48 i −hj  xi ] ,   10.15   j=1 ¯αjhj. This loss function is a convex upper bound on the zero- i=1 1yi[fN  x cid:48 i −fN  xi ]≤0, which is not convex. Let ek denote the unit vector corresponding to the kth coordinate in RN and let ¯αt−1 = ¯αt + ηek denote the parameter vector after iteration t  with ¯α0 = 0 . Also, j=1 ¯αt,jhj and distribution ¯Dt over  e−yi  F   ¯α  =  m cid:88 i=1  e−yi[fN  x cid:48 i −fN  xi ] =  m cid:88 i=1 where fN = cid:80 N one pairwise loss function ¯α  cid:55 →  cid:80 m for any t ∈ [T ], we deﬁne the function ¯ft = cid:80 N m cid:81 t  the indices {1, . . . , m} as follows:  ¯Dt+1 i  =  s=1  where ¯Zt is the analogue of Zt computed as a function of ¯Dt instead of Dt. Similarly, t ,  cid:15 −t , and  cid:15 t deﬁned with respect to ¯Dt let ¯ cid:15 + instead of Dt.  t , ¯ cid:15 −t , and ¯ cid:15 t denote the analogues of  cid:15 +  Following very similar steps as in section 7.2.2, we will use an inductive argument argument to show that coordinate descent on F and the RankBoost algorithm are  e−yi  ¯ft x cid:48 i  − ¯ft xi    ,  ¯Zs   10.16    10.4 RankBoost  249  in fact equivalent. Clearly, ¯Dt+1 = Dt+1 if we have ¯ft = ft for all t. We trivially have ¯f0 = f0, so we will make the inductive assumption that ¯ft−1 = ft−1 and show that this implies ¯ft = ft. At each iteration t ≥ 1, the direction ek selected by coordinate descent is the one minimizing the directional derivative, which is deﬁned as:  F  cid:48   ¯αt−1, ek  = lim  cid:80 t−1 η→0  i=1 e−yi  Since F   ¯αt−1 + ηek  = cid:80 m  F  cid:48   ¯αt−1, ek   rectional derivative along ek can be expressed as follows:  F   ¯αt−1 + ηek  − F   ¯αt−1   .  η  j=1 ¯αt−1,j  hj  x cid:48 i −hj  xi  −ηyi hk x cid:48 i −hk xi  , the di-  = −  = −  m cid:88 i=1 m cid:88 i=1 = − cid:20  m cid:88 i=1  ¯αt−1,j hj x cid:48 i  − hj xi   cid:105  yi hk x cid:48 i  − hk xi   exp cid:104  − yi N cid:88 j=1 ¯Zs cid:105  yi hk x cid:48 i  − hk xi   ¯Dt i  cid:104 m t−1 cid:89 s=1 ¯Dt i 1yi ht x cid:48 i −ht xi  =−1 cid:21  cid:104 m m cid:88 i=1 ¯Dt i 1yi ht x cid:48 i −ht xi  =+1 − t − ¯ cid:15 −t ] cid:104 m  = −[¯ cid:15 + one follows from  10.12 . In view of the ﬁnal equality, since m cid:81 t−1  The ﬁrst equality holds by diﬀerentiation and evaluation at η = 0 and the second ¯Zs is ﬁxed, the direction ek selected by coordinate descent is the one minimizing ¯ cid:15 t. By the inductive hypothesis ¯Dt = Dt and ¯ cid:15 t =  cid:15 t, thus, the chosen base ranker corresponds exactly to the base ranker ht selected by RankBoost.  ¯Zs cid:105 .  ¯Zs cid:105   t−1 cid:89 s=1  t−1 cid:89 s=1  s=1  The step size η is identiﬁed by setting the derivative to zero in order to minimize the function in the chosen direction ek. Thus, using identity 10.12 and the deﬁnition   250  Chapter 10 Ranking  of ¯ cid:15 t, we can write: dF   ¯αt−1 + ηek   = 0   cid:80 N j=1 ¯αt−1,j  hj  x cid:48 i −hj  xi  e−ηyi hk x cid:48 i −hk xi   = 0  dη yi ht x cid:48 i  − ht xi  e−yi yi ht x cid:48 i  − ht xi   ¯Dt i  cid:104 m yi ht x cid:48 i  − ht xi   ¯Dt i  e−ηyi ht x cid:48 i −ht xi   = 0  t−1 cid:89 s=1  ⇔ −  ⇔ −  m cid:88 i=1 m cid:88 i=1 m cid:88 i=1 ⇔ − t e−η − ¯ cid:15 −t eη] = 0 ⇔ −[¯ cid:15 + 1 ⇔ η = 2  log  ¯ cid:15 + t ¯ cid:15 −t  .  ¯Zs cid:105 e−ηyi hk x cid:48 i −hk xi   = 0  t and ¯ cid:15 −t =  cid:15 −t and this proves that By the inductive hypothesis, we have ¯ cid:15 + the step size chosen by coordinate descent matches the base ranker weight αt of RankBoost. Thus, by combining the previous results we have ¯ft = ft and the proof by induction is complete. This shows that coordinate descent applied to F precisely coincides with the RankBoost algorithm.  t =  cid:15 +  As in the classiﬁcation case, other convex loss functions upper bounding the zero- one pairwise misranking loss can be used. In particular, the following objective func- i=1 log 1 + e−yi[fN  x cid:48 i −fN  xi ]   tion based on the logistic loss can be used: ¯α  cid:55 → cid:80 m  to derive an alternative boosting-type algorithm.  10.4.3 Margin bound for ensemble methods in ranking To simplify the presentation, we will assume for the results of this section, as in section 10.2, that the pairwise labels are in {−1, +1}. By lemma 7.4, the empirical Rademacher complexity of the convex hull conv H  equals that of H. Thus, theo- rem 10.1 immediately implies the following guarantee for ensembles of hypotheses in ranking.  Corollary 10.4 Let H be a set of real-valued functions. Fix ρ > 0; then, for any δ > 0, with probability at least 1 − δ over the choice of a sample S of size m, each of the following ranking guarantees holds for all h ∈ conv H :  R h  ≤  cid:98 RS,ρ h  + R h  ≤  cid:98 RS,ρ h  +  2  m  H  + RD2  m  H  cid:1  + cid:115  log 1 ρ cid:0 RD1 ρ cid:0  cid:98 RS1 H  + cid:98 RS2  H  cid:1  + 3 cid:115  log 2  δ 2m  δ 2m  2   10.17   .   10.18    10.5 Bipartite ranking  251  For RankBoost, these bounds apply to f   cid:107 α cid:107 1, where f is the hypothesis returned by the algorithm. Since f and f   cid:107 α cid:107 1 induce the same ordering of the points, for any δ > 0, the following holds with probability at least 1 − δ:  R f   ≤  cid:98 RS,ρ f   cid:107 α cid:107 1  +  2  ρ cid:0 RD1  m  H  + RD2  m  H  cid:1  + cid:115  log 1  δ 2m  Remarkably, the number of rounds of boosting T does not appear in this bound. The bound depends only on the margin ρ, the sample size m, and the Rademacher complexity of the family of base classiﬁers H. Thus, the bound guarantees an eﬀec-  tive generalization if the pairwise margin loss  cid:98 RS,ρ f   cid:107 α cid:107 1  is small for a relatively  large ρ. A bound similar to that of theorem 7.7 for AdaBoost can be derived for the empirical pairwise ranking margin loss of RankBoost  see exercise 10.3  and similar comments on that result apply here.  These results provide a margin-based analysis in support of ensemble methods in ranking and RankBoost in particular. As in the case of AdaBoost, however, RankBoost in general does not achieve a maximum margin. But, in practice, it has been observed to obtain excellent pairwise ranking performances.   10.19   10.5 Bipartite ranking  This section examines an important ranking scenario within the score-based setting, the bipartite ranking problem. In this scenario, the set of points X is partitioned into two classes: X+ the class of positive points, and X− that of negative ones. The problem consists of ranking positive points higher than negative ones. For example, for a ﬁxed search engine query, the task consists of ranking relevant  positive  documents higher than irrelevant  negative  ones.  The bipartite problem could be treated in the way already discussed in the pre- vious sections with exactly the same theory and algorithms. However, the setup typically adopted for this problem is diﬀerent: instead of assuming that the learner receives a sample of random pairs, here pairs of positive and negative elements, it is assumed that it receives a sample of positive points from some distribution and a sample of negative points from another. This leads to the set of all pairs made of a positive point of the ﬁrst sample and a negative point of the second.  More formally, the learner receives a sample S+ =  x cid:48 1, . . . , x cid:48 m  drawn i.i.d. ac- cording to some distribution D+ over X+, and a sample S− =  x1, . . . , xn  drawn i.i.d. according to some distribution D− over X−.17 Given a hypothesis set H of  17 This two-distribution formulation also avoids a potential dependency issue that can arise for some modeling of the problem: if pairs are drawn according to some distribution D over X− × X+   252  Chapter 10 Ranking  functions mapping X to R, the learning problem consists of selecting a hypothesis h ∈ H with small expected bipartite misranking or generalization error R h :  R h  = P  [h x cid:48   < h x ] .   10.20   x∼D− x cid:48 ∼D+  The empirical pairwise misranking or empirical error of h is denoted by  cid:98 RS+,S−  and deﬁned by   h   1h x cid:48 i <h xj   .   10.21    h  =   cid:98 RS+,S−  1 mn  m cid:88 i=1  n cid:88 j=1  Note that while the bipartite ranking problem bears some similarity with binary classiﬁcation, in particular, the presence of two classes, they are distinct problems, since their objectives and measures of success clearly diﬀer.  By the deﬁnition of the formulation of the bipartite ranking just presented, the learning algorithm must typically deal with mn pairs. For example, the application of SVMs to ranking in this scenario leads to an optimization with mn slack variables or constraints. With just a thousand positive and a thousand negative points, one million pairs would need to be considered. This can lead to a prohibitive computational cost for some learning algorithms. The next section shows that RankBoost admits an eﬃcient implementation in the bipartite scenario.  10.5.1 Boosting in bipartite ranking This section shows the eﬃciency of RankBoost in the bipartite scenario and dis- cusses the connection between AdaBoost and RankBoost in this context.  The key property of RankBoost leading to an eﬃcient algorithm in the bipartite setting is the fact that its objective function is based on the exponential function. As a result, it can be decomposed into the product of two functions, one depending on only the positive and the other on only the negative points. Similarly, the distribution Dt maintained by the algorithm can be factored as the product of two t and D−t . This is clear for the uniform distribution D1 at the distributions D+ 1  i D−1  j  with ﬁrst round as for any i ∈ [m] and j ∈ [n], D1 i, j  = 1  mn  = D+ 1  i  = 1 m and D−1  j  = 1 n. This property is recursively preserved since, in D+ view of the following, the decomposition of Dt implies that of Dt+1 for any t ∈ [T ].  and the learner makes use of this information to augment its training sample, then the resulting sample is in general not i.i.d. This is because if  x1, x cid:48  2  are in the sample, then so are the pairs  x1, x cid:48  1  and thus the pairs are not independent. However, without sample augmentation, the points are i.i.d., and this issue does not arise.  2  and  x2, x cid:48   1  and  x2, x cid:48    10.5 Bipartite ranking  253  For any i ∈ [m] and j ∈ [n], by deﬁnition of the update, we can write:  Dt i, j e−αt[ht x cid:48 i −ht xj  ]  D+  t  i e−αtht x cid:48 i   D−t  j eαtht xj    Dt+1 i, j  =  Zt  =  Zt,+  ,  Zt,−  i=1 D+  t =  cid:80 m  since the normalization factor Zt can also be decomposed as Zt = Z−t Z + t , with j=1 D−t  j eαtht xj  . Furthermore, the Z + pairwise misranking of a hypothesis h ∈ H based on the distribution Dt used to determine ht can also be computed as the diﬀerence of two quantities, one depending only on positive points, the other only on negative ones:  t  i e−αtht x cid:48 i  and Z−t =  cid:80 n  E  [h x cid:48 i  − h xj ] = E i∼D+  t  [ E j∼D−t  [h x cid:48 i  − h xj ]] = E i∼D+  [h x cid:48 i ] − E j∼D−t   i,j ∼Dt Thus, the time and space complexity of RankBoost depends only on the total number of points m+n and not the number of pairs mn. More speciﬁcally, ignoring the call to the weak ranker or the cost of determining ht, the time and space complexity of each round is linear, that is, in O m + n . Furthermore, the cost of determining ht is a function of O m + n  and not O mn . Figure 10.2 gives the pseudocode of the algorithm adapted to the bipartite scenario.  t  [h xj ].  In the bipartite scenario, a connection can be made between the classiﬁcation algo- rithm AdaBoost and the ranking algorithm RankBoost. In particular, the objective  function of RankBoost can be expressed as follows for any α =  α1, . . . , αT   ∈ RT , T ≥ 1:  FRankBoost α  =  m cid:88 j=1 n cid:88 i=1 = cid:16  m cid:88 i=1  e−  exp −[f  x cid:48 i  − f  xj ]  e+ cid:80 T  cid:80 T t=1 αtht x cid:48 i  cid:17  cid:16  n cid:88 j=1  = F+ α F− α ,  t=1 αtht xj   cid:17   where F+ denotes the function deﬁned by the sum over the positive points and F− the function deﬁned over the negative points. The objective function of AdaBoost can be deﬁned in terms of these same two functions as follows:  FAdaBoost α  =  m cid:88 i=1 m cid:88 i=1  =  exp −y cid:48 if  x cid:48 i   +  cid:80 T t=1 αtht x cid:48 i  +  e−  exp −yjf  xj   e+ cid:80 T  t=1 αtht xj    n cid:88 j=1 n cid:88 j=1  = F+ α  + F− α .   Chapter 10 Ranking  t = E j∼D−t  [h xj ] − E i∼D+  t  [h x cid:48 i ]  254  1  2  3  4  5  6  7  8  9  10  11  12  13  BipartiteRankBoost S =  x cid:48 1, . . . , x cid:48 m, x1, . . . , xn    for j ← 1 to m do  D+ 1  j  ← 1 for i ← 1 to n do  m  D−1  i  ← 1  n  for t ← 1 to T do  t  ht ← base ranker in H with smallest  cid:15 −t −  cid:15 + 2 log  cid:15 + αt ← 1  cid:15 −t Z + t ← 1 −  cid:15 + for i ← 1 to m do D+  t  cid:15 −t  t + cid:112  cid:15 + Z−t ← 1 −  cid:15 −t + cid:112  cid:15 +  for j ← 1 to n do  D+ t+1 i  ←  Z+ t  t  cid:15 −t  t  i  exp cid:2 −αtht x cid:48 i  cid:3  D−t  j  exp cid:2 +αtht xj   cid:3   Zt  D−t+1 j  ←  t=1 αtht  14 f ← cid:80 T  15 return f  Figure 10.2 Pseudocode of RankBoost in a bipartite setting, with H ⊆ {0, 1}X,  cid:15 +  cid:15 − t = E  [h xj  ].  j∼D−t  t = E  i∼D+  t  [h x cid:48   i ] and  Note that the gradient of the objective function of RankBoost can be expressed in terms of AdaBoost as follows:  ∇αFRankBoost α  = F− α ∇αF+ α  + F+ α ∇αF− α    10.22  = F− α  ∇αF+ α  + ∇αF− α   +  F+ α  − F− α  ∇αF− α  = F− α ∇αFAdaBoost α  +  F+ α  − F− α  ∇αF− α .  If α is a minimizer of FAdaBoost, then ∇αFAdaBoost α  = 0 and it can be shown that the equality F+ α  − F− α  = 0 also holds for α, provided that the fam- ily of base hypotheses H used for AdaBoost includes the constant hypothesis h0 : x  cid:55 → 1, which often is the case in practice. Then, by  10.22 , this implies that ∇αFRankBoost α  = 0 and therefore that α is also a minimizer of the convex   10.5 Bipartite ranking  255  Figure 10.3 The AUC  area under the ROC curve  is a measure of the performance of a bipartite ranking.  function FRankBoost. In general, FAdaBoost does not admit a minimizer. Never- theless, it can be shown that if limk→∞ FAdaBoost αk  = inf α FAdaBoost α  for some sequence  αk k∈N, then, under the same assumption on the use of a con- stant base hypothesis and for a non-linearly separable dataset, the following holds: limk→∞ FRankBoost αk  = inf α FRankBoost α .  The connections between AdaBoost and RankBoost just mentioned suggest that AdaBoost could achieve a good ranking performance as well. This is often observed empirically, a fact that brings strong support to the use of AdaBoost both as a classiﬁer and a ranking algorithm. Nevertheless, RankBoost may converge faster and achieve a good ranking faster than AdaBoost.  10.5.2 Area under the ROC curve The performance of a bipartite ranking algorithm is typically reported in terms of the area under the receiver operating characteristic  ROC  curve, or the area under the curve  AUC   for short.  Let U be a test sample used to evaluate the performance of h  or a training sample  with m positive points z cid:48 1, . . . , z cid:48 m and n negative points z1, . . . , zn. For  any h ∈ H, let  cid:98 R h, U   denote the average pairwise misranking of h over U . Then, the AUC of h for the sample U is precisely 1− cid:98 R h, U  , that is, its average pairwise  ranking accuracy on U :  AUC h, U   =  1 mn  m cid:88 i=1  n cid:88 j=1  z∼ cid:98 D−U 1h z cid:48 i ≥h zj   = P z cid:48 ∼ cid:98 D+  U  [h z cid:48   ≥ h z ].  U denotes the empirical distribution corresponding to the positive points in U the empirical distribution corresponding to the negative ones. AUC h, U    Here,  cid:98 D+ U and cid:98 D+  1  .8  .6  .4  .2  0  e t a r   e v  i t i s o p   e u r T  AUC  0  .2  .4  .6  .8 False positive rate  1   256  Chapter 10 Ranking  Figure 10.4 An example ROC curve and illustrated threshold. Varying the value of θ from one extreme to the other generates points on the curve.  is thus an empirical estimate of the pairwise ranking accuracy based on the sample U , and by deﬁnition it is in [0, 1]. Higher AUC values correspond to a better ranking performance. In particular, an AUC of one indicates that the points of U are ranked perfectly using h. AUC h, U   can be computed in linear time from a sorted array containing the m + n elements h z cid:48 i  and h zj , for i ∈ [m] and j ∈ [n]. Assuming that the array is sorted in increasing order  with a positive point placed higher than a negative one if they both have the same scores  the total number of correctly ranked pairs r can be computed as follows. Starting with r = 0, the array is inspected in increasing order of the indices while maintaining at any time the number of negative points seen n and incrementing the current value of r with n whenever a positive point is found. After full inspection of the array, the AUC is given by r  mn . Thus, assuming that a comparison-based sorting algorithm is used, the complexity of the computation of the AUC is in O  m + n  log m + n  . As indicated by its name, the AUC coincides with the area under the ROC curve  ﬁgure 10.3 . An ROC curve plots the true positive rate, that is, the percentage of positive points correctly predicted as positive as a function of the false positive rate, that is, the percentage of negative points incorrectly predicted as positive. Figure 10.4 illustrates the deﬁnition and construction of an ROC curve.  Points are generated along the curve by varying a threshold value θ as in the right panel of ﬁgure 10.4, from higher values to lower ones. The threshold is used to determine the label of any point x  positive or negative  based on sgn h x − θ . At one extreme, all points are predicted as negative; thus, the false positive rate is zero, but the true positive rate is zero as well. This gives the ﬁrst point  0, 0  of the plot. At the other extreme, all points are predicted as positive; thus, both the true and the false positive rates are equal to one, which gives the point  1, 1 . In the ideal case, as already discussed, the AUC value is one, and, with the exception of  0, 0 , the curve coincides with a horizontal line reaching  1, 1 .  e t a r   e v  i t i s o p   e u r T  1  .8  .6  .4  .2  0  -  θ +  h x3   h x14   h x5   h x1   h x23   0  .2  .4  .6  .8  1  False positive rate  sorted scores  -  θ +  h x3   h x14   h x5   h x1   h x23   sorted scores   10.6 Preference-based setting  257  10.6 Preference-based setting  This section examines a diﬀerent setting for the problem of learning to rank: the preference-based setting. In this setting, the objective is to rank as accurately as possible any test subset X ⊆ X, typically a ﬁnite set that we refer to as a ﬁnite query subset. This is close to the query-based scenario of search engines or information extraction systems and the terminology stems from the fact that X could be a set of items needed to rank in response to a particular query. The advantage of this setting over the score-based setting is that here the learning algorithm is not required to return a linear ordering of all points of X, which may be impossible to achieve faultlessly in accordance with a general possibly non-transitive pairwise preference labeling. Supplying a correct linear ordering for a query subset is more likely to be achievable exactly or at least with a better approximation.  The preference-based setting consists of two stages. In the ﬁrst stage, a sample of labeled pairs S, exactly as in the score-based setting, is used to learn a preference function h : X × X  cid:55 → [0, 1], that is, a function that assigns a higher value to a pair  u, v  when u is preferred to v or is to be ranked higher than v, and smaller values in the opposite case. This preference function can be obtained as the output of a standard classiﬁcation algorithm trained on S. A crucial diﬀerence with the score-based setting is that, in general, the preference function h is not required to induce a linear ordering. The relation it induces may be non-transitive; thus, we may have, for example, h u, v  = h v, w  = h w, u  = 1 for three distinct points u, v, and w. In the second stage, given a query subset X ⊆ X, the preference function h is used to determine a ranking of X. How can h be used to generate an accurate ranking? This will be the main focus of this section. The computational complexity of the algorithm determining the ranking is also crucial. Here, we will measure its running time complexity in terms of the number of calls to h.  When the preference function is obtained as the output of a binary classiﬁcation algorithm, the preference-based setting can be viewed as a reduction of ranking to classiﬁcation: the second stage speciﬁes how a ranking is obtained from a classiﬁer’s output.  10.6.1 Second-stage ranking problem The ranking problem of the second stage is modeled as follows. We assume that a preference function h is given. From the point of view of this stage, the way the function h has been determined is immaterial, it can be viewed as a black box. As already discussed, h is not assumed to be transitive. But, we will assume that it is pairwise consistent, that is h u, v  + h v, u  = 1, for all u, v ∈ X.   258  Chapter 10 Ranking  Let D be an unknown distribution according to which pairs  X, σ∗  are drawn where X ⊆ X is a query subset and σ∗ a target ranking or permutation of X, that is, a bijective function from X to {1, . . . ,X}. Thus, we consider a stochastic scenario, and σ∗ is a random variable. The objective of a second-stage algorithm A consists of using the preference function h to return an accurate ranking A X  for any query subset X. The algorithm may be deterministic, in which case A X  is uniquely determined from X or it may be randomized, in which case we denote by s the randomization seed it may depend on.  The following loss function L can be used to measure the disagreement between a ranking σ and a desired one σ∗ over a set X of n ≥ 1 elements: 1σ u <σ v 1σ∗ v <σ∗ u ,  L σ, σ∗  =   10.23   2  n n − 1  cid:88 u cid:54 =v  where the sum runs over all pairs  u, v  with u and v distinct elements of X. All the results presented in the following hold for a broader set of loss functions described later. Abusing the notation, we also deﬁne the loss of the preference function h with respect to a ranking σ∗ of a set X of n ≥ 1 elements by h u, v 1σ∗ v <σ∗ u .  L h, σ∗  =   10.24   2  n n − 1  cid:88 u cid:54 =v  The expected loss for a deterministic algorithm A is thus E  X,σ∗ ∼D[L A X , σ∗ ]. The regret of algorithm A is then deﬁned as the diﬀerence between its loss and that of the best ﬁxed global ranking. This can be written as follows:  E  Reg A  = X denotes the ranking induced on X by a global ranking σ cid:48  of X. Similarly,  where σ cid:48  we deﬁne the regret of the preference function as follows  [L A X , σ∗ ] − min  X , σ∗ ] ,   X,σ∗ ∼D   X,σ∗ ∼D   10.25   [L σ cid:48   σ cid:48   E  Reg h  =  E   X,σ∗ ∼D  [L hX , σ∗ ] − min  h cid:48   E   X,σ∗ ∼D  [L h cid:48   X , σ∗ ] ,   10.26   where hX denotes the restriction of h to X × X, and similarly with h cid:48 . The regret results presented in this section hold assuming the following pairwise independence on irrelevant alternatives property:  E  σ∗X1  [1σ∗ v <σ∗ u ] = E σ∗X2  [1σ∗ v <σ∗ u ],   10.27    10.6 Preference-based setting  259  for any u, v ∈ X and any two sets X1 and X2 containing u and v, and where σ∗X denotes the random variable σ∗ conditioned on X.18 Similar regret deﬁnitions can be given for a randomized algorithm additionally taking the expectation over s.  Clearly, the quality of the ranking output by the second-stage algorithm inti- mately depends on that of the preference function h. In the next sections, we discuss both a deterministic and a randomized second-stage algorithm for which the regret can be upper bounded in terms of the regret of the preference function.  10.6.2 Deterministic algorithm A natural deterministic algorithm for the second-stage is based on the sort-by-degree algorithm. This consists of ranking each element of X based on the number of other elements it is preferred to according to the preference function h. Let Asort-by-degree denote this algorithm. In the bipartite setting, the following bounds can be proven for the expected loss of this algorithm and its regret:  E X,σ∗  [L Asort-by-degree X , σ∗ ] ≤ 2 E Reg Asort-by-degree X   ≤ 2 Reg h  .  X,σ∗  [L h, σ∗ ]   10.30    10.31   These results show that the sort-by-degree algorithm can achieve an accurate rank- ing when the loss or the regret of the preference function h is small. They also bound the ranking loss or regret of the algorithm in terms of the classiﬁcation loss or regret of h, which can be viewed as a guarantee for the reduction of ranking to classiﬁcation using the sort-by-degree algorithm.  Nevertheless, in some cases, the guarantee given by these results is weak or un- informative owing to the presence of the factor of two. Consider the case of a binary classiﬁer h with an error rate of just 25 percent, which is quite reasonable in many applications. Assume that the Bayes error is close to zero for the classi- ﬁcation problem and, similarly, that for the ranking problem the regret and loss approximately coincide. Then, using the bound in  10.30  guarantees a worst-case pairwise misranking error of at most 50 percent for the ranking algorithm, which is the pairwise misranking error of random ranking.  18 More generally, they hold without that assumption using the following weaker notions of regret:  Reg cid:48  A  =  Reg cid:48  h  =  E  E   X,σ∗ ∼D   X,σ∗ ∼D  [L A X , σ∗ ] − E [L hX , σ∗ ] − E   cid:104   X  X  min σ cid:48   E σ∗X E σ∗X  min h cid:48   [L σ cid:48 , σ∗ ]  [L h cid:48 , σ∗ ]  ,  where σ cid:48  denotes a ranking of X and h cid:48  a preference function deﬁned over X × X.   10.28    10.29    cid:105    cid:105    cid:104    260  Chapter 10 Ranking   a    b    c   Figure 10.5 Illustration of the proof of theorem 10.5.  Furthermore, the running time complexity of the algorithm quadratic, that is in Ω X2  of a query set X, since it requires calling the preference function for every pair  u, v  with u and v in X. As shown by the following theorem, no deterministic algorithm can improve upon the factor of two appearing in the regret guarantee of the sort-by-degree algorithm.  Theorem 10.5  Lower bound for deterministic algorithms  For any deterministic algori- thm A, there is a bipartite distribution for which  Reg A  ≥ 2 Reg h .   10.32   Proof: Consider the simple case where X = X = {u, v, w} and where the preference function induces a cycle as illustrated by ﬁgure 10.5a. An arrow from u to v indicates that v is preferred to u according to h. The proof is based on an adversarial choice of the target σ∗. Without loss of generality, either A returns the ranking u, v, w  ﬁgure 10.5b  or w, v, u  ﬁgure 10.5c . In the ﬁrst case, let σ∗ be deﬁned by the labeling indicated in the ﬁgure. In that case, we have L h, σ∗  = 1 3, since u is preferred to w according to h while w is labeled positively and u negatively. The loss of the algorithm is L A, σ∗  = 2 3, since both u and v are ranked higher than the positively labeled w by the algorithm. Similarly, σ∗ can be deﬁned as in ﬁgure 10.5c in the second case, and we ﬁnd again that L h, σ∗  = 1 3 and L A, σ∗  = 2 3. This concludes  cid:3  the proof.  The theorem suggests that randomization is necessary in order to achieve a better guarantee. In the next section, we present a randomized algorithm that beneﬁts both from better guarantees and a better time complexity.  10.6.3 Randomized algorithm The general idea of the algorithm described in this section is to use a straightforward extension of the randomized QuickSort algorithm in the second stage. Unlike in      best in deterministic case.  }  u  h  v  w  Lower Bound  If    returns          , then  choose     as:  u, v w  +-  If    returns          , then  choose     as:  +-  w, v u  +-   10.6 Preference-based setting  261  Figure 10.6 Illustration of randomized QuickSort based on a preference function h  not necessarily transitive .  the standard version of QuickSort, here the comparison function is based on the preference function, which in general is not transitive. Nevertheless, it can be shown here, too, that the expected time complexity of the algorithm is in O n log n  when applied to an array of size n.  The algorithm works as follows, as illustrated by ﬁgure 10.6. At each recursive step, a pivot element u is selected uniformly at random from X. For each v  cid:54 = u, v is placed on the left of u with probability h v, u  and to its right with the remaining probability h u, v . The algorithm proceeds recursively with the array to the left of u and the one to its right and returns the concatenation of the permutation returned by the left recursion, u, and the permutation returned by the right recursion. Let AQuickSort denote this algorithm. In the bipartite setting, the following guar- antees can be proven: E  [L AQuickSort X, s , σ∗ ] = E  X,σ∗  X,σ∗,s  [L h, σ∗ ]   10.33    10.34   Reg AQuickSort  ≤ Reg h  .  Thus, here, the factor of two of the bounds in the deterministic case has vanished, which is substantially more favorable. Furthermore, the guarantee for the loss is an equality. Moreover, the expected time complexity of the algorithm is only in O n log n , and, if only the top k items are needed to be ranked, as in many applications, the time complexity is reduced to O n + k log k .  For the QuickSort algorithm, the following guarantee can also be proven in the  case of general ranking setting  not necessarily bipartite setting :  E  X,σ∗,s  [L AQuickSort X, s , σ∗ ] ≤ 2 E  X,σ∗  [L h, σ∗ ].   10.35   Randomized QS  h v, u   h u, v   v  u  random   pivot  left recursion  right recursion   262  Chapter 10 Ranking  10.6.4 Extension to other loss functions All of the results just presented hold for a broader class of loss functions Lω deﬁned in terms of a weight function or emphasis function ω. Lω is similar to  10.23 , but measures the weighted disagreement between a ranking σ and a desired one σ∗ over a set X of n ≥ 1 elements as follows:  Lω σ, σ∗  =  ω σ∗ v , σ∗ u   1σ u <σ v  1σ∗ v <σ∗ u ,   10.36   2  n n − 1  cid:88 u cid:54 =v  where the sum runs over all pairs  u, v  with u and v distinct elements of X, and where ω is a symmetric function whose properties are described below. Thus, the loss counts the number of pairwise misrankings of σ with respect to σ∗, each weighted by ω. The function ω is assumed to satisfy the following three natural axioms:    symmetry: ω i, j  = ω j, i  for all i, j;   monotonicity: ω i, j  ≤ ω i, k  if either i   j > k;   triangle inequality: ω i, j  ≤ ω i, k  + ω k, j . The motivation for this last property stems from the following: if correctly ordering items in positions  i, k  and  k, j  is not of great importance, then the same should hold for items in positions  i, j .  Using diﬀerent functions ω, the family of functions Lω can cover several familiar and important losses. Here are some examples. Setting ω i, j  = 1 for all i  cid:54 = j yields the unweighted pairwise misranking measure. For a ﬁxed integer k ≥ 1, the function ω deﬁned by ω i, j  = 1  i≤k ∨ j≤k  ∧ i cid:54 =j  for all  i, j  can be used to emphasize ranking at the top k elements. Misranking of pairs with at least one element ranked among the top k is penalized by this function. This can be of interest in applications such as information extraction or search engines where the ranking of the top documents matters more. For this emphasis function, all elements ranked below k are in a tie. Any tie relation can be encoded using ω. Finally, in a bipartite ranking scenario with m+ positive and m− negative points and m+ + m− = n, choosing ω i, j  = n n−1  2m−m+ yields the standard loss function coinciding with 1 − AUC. 10.7 Other ranking criteria  The objective function for the ranking problems discussed in this chapter were all based on pairwise misranking. Other ranking criteria have been introduced in information retrieval and used to derive alternative ranking algorithms. Here, we brieﬂy present several of these criteria.   10.8 Chapter notes  263    Precision, precision@n, average precision, recall. All of these criteria assume that points are partitioned into two classes  positives and negatives , as in the bi- partite ranking setting. Precision is the fraction of positively predicted points that are in fact positive. Whereas precision takes into account all positive predic- tions, precision@n only considers the top n predictions. For example, precision@5 considers only the top 5 positively predicted points. Average precision involves computing precision@n for each value of n, and averaging across these values. Each precision@n computation can be interpreted as computing precision for a ﬁxed value of recall , or the fraction of positive points that are predicted to be positive  recall coincides with the notion of true positive rate .    DCG, NDCG. These criteria assume the existence of relevance scores associated with the points to be ranked, e.g., given a web search query, each website returned by a search engine has an associated relevance score. Moreover, these criteria mea- sure the extent to which points with large relevance scores appear at or near the beginning of a ranking. Deﬁne  ci i∈N as a predeﬁned sequence of non-increasing and non-negative discount factors, e.g., ci = log i −1. Then, given a ranking of m points and deﬁning ri as the relevance score of the ith point in this ranking, i=1 ciri. Note that DCG is an increasing function of m. In contrast, the normalized discounted cumulative gain  NDCG  normalizes the DCG across values of m by dividing the DCG by the IDCG, or the ideal DCG that would result from an optimal ordering of the points.  the discounted cumulative gain  DCG  is deﬁned as DCG =  cid:80 m  10.8 Chapter notes  The problem of learning to rank is distinct from the purely algorithmic one of rank aggregation, which, as shown by Dwork, Kumar, Naor, and Sivakumar [2001], is NP-hard even for k = 4 rankings. The Rademacher complexity and margin-based generalization bounds for pairwise ranking given in theorem 10.1 and corollary 6.13 are novel. Margin bounds based on covering numbers were also given by Rudin, Cortes, Mohri, and Schapire [2005]. Other learning bounds in the score-based setting of ranking, including VC-dimension and stability-based learning bounds, have been given by Agarwal and Niyogi [2005], Agarwal et al. [2005] and Cortes et al. [2007b].  The ranking algorithm based on SVMs presented in section 10.3 has been used and discussed by several researchers. One early and speciﬁc discussion of its use can be found in Joachims [2002]. The fact that the algorithm is simply a special instance of SVMs seems not to be clearly stated in the literature. The theoretical justiﬁcation presented here for its use in ranking is novel.   264  Chapter 10 Ranking  RankBoost was introduced by Freund et al. [2003]. The version of the algo- rithm presented here is the coordinate descent RankBoost from Rudin et al. [2005]. RankBoost in general does not achieve a maximum margin and may not increase the margin at each iteration. A Smooth Margin ranking algorithm [Rudin et al., 2005] based on a modiﬁed version of the objective function of RankBoost can be shown to increase the smooth margin at every iteration, but the comparison of its empirical performance with that of RankBoost has not been reported. For the empirical ranking quality of AdaBoost and the connections between AdaBoost and RankBoost in the bipartite setting, see Cortes and Mohri [2003] and Rudin et al. [2005].  The Receiver Operating Characteristics  ROC  curves were originally developed in signal detection theory [Egan, 1975] in connection with radio signals during World War II. They also had applications to psychophysics [Green and Swets, 1966] and have been used since then in a variety of other applications, in particular for medical decision making. The area under an ROC curve  AUC  is equivalent to the Wilcoxon-Mann-Whitney statistic [Hanley and McNeil, 1982] and is closely related to the Gini index [Breiman et al., 1984]  see also chapter 9 . For a statistical analysis of the AUC and conﬁdence intervals depending on the error rate, see Cortes and Mohri [2003, 2005]. The deterministic algorithm in the preference-based setting discussed in this chapter was presented and analyzed by Balcan et al. [2008]. The randomized algorithm as well as much of the results presented in section 10.6 are due to Ailon and Mohri [2008].  A somewhat related problem of ordinal regression has been studied by some authors [McCullagh, 1980, McCullagh and Nelder, 1983, Herbrich et al., 2000] which consists of predicting the correct label of each item out of a ﬁnite set, as in multi- class classiﬁcation, with the additional assumption of an ordering among the labels. This problem is distinct, however, from the pairwise ranking problem discussed in this chapter.  The DCG ranking criterion was introduced by J¨arvelin and Kek¨al¨ainen [2000], and has been used and discussed in a number of subsequent studies, in particular Cossock and Zhang [2008] who consider a subset ranking problem formulated in terms of DCG, for which they consider a regression-based solution.  10.9 Exercises  10.1 Uniform margin-bound for ranking. Use theorem 10.1 to derive a margin-based learning bound for ranking that holds uniformly for all ρ > 0  see similar binary classiﬁcation bounds of theorem 5.9 and exercise 5.2 .   10.9 Exercises  265  10.2 On-line ranking. Give an on-line version of the SVM-based ranking algorithm  presented in section 10.3.  10.3 Empirical margin loss of RankBoost. Derive an upper bound on the empirical pairwise ranking margin loss of RankBoost similar to that of theorem 7.7 for AdaBoost.  10.4 Margin maximization and RankBoost. Give an example showing that Rank-  Boost does not achieve the maximum margin, as in the case of AdaBoost.  10.5 RankPerceptron. Adapt the Perceptron algorithm to derive a pairwise ranking algorithm based on a linear scoring function. Assume that the training sample is linear separable for pairwise ranking. Give an upper bound on the number of updates made by the algorithm in terms of the ranking margin.  10.6 Margin-maximization ranking. Give a linear programming  LP  algorithm re- turning a linear hypothesis for pairwise ranking based on margin maximization.  10.7 Bipartite ranking. Suppose that we use a binary classiﬁer for ranking in the bipartite setting. Prove that if the error of the binary classiﬁer is  cid:15 , then that of the ranking it induces is also at most  cid:15 . Show that the converse does not hold.  10.8 Multipartite ranking. Consider the ranking scenario in a k-partite setting where X is partitioned into k subsets X1, . . . , Xk with k ≥ 1. The bipartite case  k = 2  is already speciﬁcally examined in the chapter. Give a precise formulation of the problem in terms of k distributions. Does RankBoost admit an eﬃcient implementation in this case? Give the pseudocode of the algorithm.  10.9 Deviation bound for the AUC. Let h be a ﬁxed scoring function used to rank the points of X. Use Hoeﬀding’s bound to show that with high probability the AUC of h for a ﬁnite sample is close to its average.  10.10 k-partite weight function. Show how the weight function ω can be deﬁned so that Lω encodes the natural loss function associated to a k-partite ranking scenario.    11 Regression  This chapter discusses in depth the learning problem of regression, which consists of using data to predict, as closely as possible, the correct real-valued labels of the points or items considered. Regression is a common task in machine learning with a variety of applications, which justiﬁes the speciﬁc chapter we reserve to its analysis. The learning guarantees presented in the previous sections focused largely on classiﬁcation problems. Here we present generalization bounds for regression, both for ﬁnite and inﬁnite hypothesis sets. Several of these learning bounds are based on the familiar notion of Rademacher complexity, which is useful for characterizing the complexity of hypothesis sets in regression as well. Others are based on a combinatorial notion of complexity tailored to regression that we will introduce, pseudo-dimension, which can be viewed as an extension of the VC-dimension to regression. We describe a general technique for reducing regression problems to classiﬁcation and deriving generalization bounds based on the notion of pseudo- dimension. We present and analyze several regression algorithms, including linear regression, kernel ridge regression, support-vector regression, Lasso, and several on-line versions of these algorithms. We discuss in detail the properties of these algorithms, including the corresponding learning guarantees.  11.1 The problem of regression  We ﬁrst introduce the learning problem of regression. Let X denote the input space and Y a measurable subset of R. Here, we will adopt the stochastic scenario and will denote by D a distribution over X × Y. As discussed in section 2.4.1, the deterministic scenario is a straightforward special case where input points admit a unique label determined by a target function f : X → Y. As in all supervised learning problems, the learner receives a labeled sample  S =  cid:0  x1, y1 , . . . ,  xm, ym  cid:1  ∈  X × Y m drawn i.i.d. according to D. Since the  labels are real numbers, it is not reasonable to hope that the learner could predict   268  Chapter 11 Regression  precisely the correct label when it is unique, or precisely its average label. Instead, we can require that its predictions be close to the correct ones. This is the key diﬀerence between regression and classiﬁcation: in regression, the measure of error is based on the magnitude of the diﬀerence between the real-valued label predicted and the true or correct one, and not based on the equality or inequality of these  two values. We denote by L : Y × Y → R+ the loss function used to measure the magnitude of error. The most common loss function used in regression is the squared loss L2 deﬁned by L y, y cid:48   = y cid:48  − y2 for all y, y cid:48  ∈ Y, or, more generally, an Lp loss deﬁned by L y, y cid:48   = y cid:48  − yp, for some p ≥ 1 and all y, y cid:48  ∈ Y. Given a hypothesis set H of functions mapping X to Y, the regression problem consists of using the labeled sample S to ﬁnd a hypothesis h ∈ H with small expected loss or generalization error R h  with respect to the target f :  R h  = E   x,y ∼D cid:2 L cid:0 h x , y cid:1  cid:3  . m cid:88 i=1  L cid:0 h xi , yi cid:1  .  1 m   cid:98 RS h  =  As in the previous chapters, the empirical loss or error of h ∈ H is denoted by   cid:98 RS h  and deﬁned by  In the common case where L is the squared loss, this represents the mean squared error of h on the sample S. When the loss function L is bounded by some M > 0, that is L y cid:48 , y  ≤ M for all y, y cid:48  ∈ Y or, more strictly, L h x , y  ≤ M for all h ∈ H and  x, y  ∈ X × Y, the problem is referred to as a bounded regression problem. Much of the theoretical results presented in the following sections are based on that assumption. The analysis of unbounded regression problems is technically more elaborate and typically requires some other types of assumptions.   11.1    11.2   11.2 Generalization bounds  This section presents learning guarantees for bounded regression problems. We start with the simple case of a ﬁnite hypothesis set.  11.2.1 Finite hypothesis sets In the case of a ﬁnite hypothesis, we can derive a generalization bound for regression by a straightforward application of Hoeﬀding’s inequality and the union bound.  Theorem 11.1 Let L be a bounded loss function. Assume that the hypothesis set H is ﬁnite. Then, for any δ > 0, with probability at least 1 − δ, the following inequality   11.2 Generalization bounds  269  holds for all h ∈ H:  .  δ  2m  R h  ≤  cid:98 RS h  + M cid:115  log H + log 1 P cid:104 R h  −  cid:98 RS h  >  cid:15  cid:105  ≤ e− 2m cid:15 2  M 2 .  Proof: By Hoeﬀding’s inequality, since L takes values in [0, M ], for any h ∈ H, the following holds:  Thus, by the union bound, we can write  P cid:104 ∃h ∈ H : R h  −  cid:98 RS h  >  cid:15  cid:105  ≤  cid:88 h∈H  P cid:104 R h  −  cid:98 RS h  >  cid:15  cid:105  ≤ He− 2m cid:15 2  M 2 .  Setting the right-hand side to be equal to δ yields the statement of the theorem.  cid:3  With the same assumptions and using the same proof, a two-sided bound can be derived: with probability at least 1 − δ, for all h ∈ H,  R h  −  cid:98 RS h  ≤ M cid:115  log H + log 2  2m  δ  .  These learning bounds are similar to those derived for classiﬁcation. In fact, they coincide with the classiﬁcation bounds given in the inconsistent case when M = 1. Thus, all the remarks made in that context apply identically here. In particular, a larger sample size m guarantees better generalization; the bound increases as a function of log H and suggests selecting, for the same empirical error, a smaller hypothesis set. This is an instance of Occam’s razor principle for regression. In the next sections, we present other instances of this principle for the general case of inﬁnite hypothesis sets using the notions of Rademacher complexity and pseudo- dimension.  11.2.2 Rademacher complexity bounds Here, we show how the Rademacher complexity bounds of theorem 3.3 can be used to derive generalization bounds for regression in the case of the family of Lp loss functions. We ﬁrst show an upper bound for the Rademacher complexity of a relevant family of functions. Proposition 11.2  Rademacher complexity of µ-Lipschitz loss functions  Let L : Y× Y → R be a non-negative loss upper bounded by M > 0  L y, y cid:48   ≤ M for all y, y cid:48  ∈ Y  and such that for any ﬁxed y cid:48  ∈ Y, y  cid:55 → L y, y cid:48   is µ-Lipschitz for some µ > 0. Then, for any sample S =   x1, y1 , . . . ,  xm, ym  , the Rademacher complexity of the family G = { x, y   cid:55 → L h x , y  : h ∈ H} is upper bounded as follows:   cid:98 RS G  ≤ µ cid:98 RS H  .   270  Chapter 11 Regression  Proof: Since for any ﬁxed yi, y  cid:55 → L y, yi  is µ-Lipschitz, by Talagrand’s contraction lemma  lemma 5.7 , we can write   cid:98 RS G  =  1 m  E  σ cid:20  m cid:88 i=1  σiL h xi , yi  cid:21  ≤  1 m  E  σ cid:20  m cid:88 i=1  σiµ h xi  cid:21  = µ cid:98 RS H ,  which completes the proof.   cid:3   E  E  1 m  1 m  δ 2m  m cid:88 i=1 m cid:88 i=1   x,y ∼D cid:2 L x, y  cid:3  ≤  x,y ∼D cid:2 L x, y  cid:3  ≤  Theorem 11.3  Rademacher complexity regression bounds  Let L : Y×Y → R be a non- negative loss upper bounded by M > 0  L y, y cid:48   ≤ M for all y, y cid:48  ∈ Y  and such that for any ﬁxed y cid:48  ∈ Y, y  cid:55 → L y, y cid:48   is µ-Lipschitz for some µ > 0. L xi, yi  + 2µ Rm H  + M cid:115  log 1 L xi, yi  + 2µ cid:98 RS H  + 3M cid:115  log 2 σiµ h xi  cid:21  = µ cid:98 RS H .  Proof: Since for any ﬁxed yi, y  cid:55 → L y, yi  is µ-Lipschitz, by Talagrand’s contraction lemma  lemma 5.7 , we can write  σiL h xi , yi  cid:21  ≤  Combining this inequality with the general Rademacher complexity learning bound  cid:3  of theorem 3.3 completes the proof. Let p ≥ 1 and assume that h x  − y ≤ M for all  x, y  ∈ X × Y and h ∈ H. Then, since for any y cid:48  the function y  cid:55 → y−y cid:48 p is pM p−1-Lipschitz for  y−y cid:48   ∈ [−M, M ], the theorem applies to any Lp-loss. As an example, for any δ > 0, with probability at least 1 − δ over a sample S of size m, each of the following inequalities holds for all h ∈ H:   cid:98 RS G  =  σ cid:20  m cid:88 i=1  σ cid:20  m cid:88 i=1  δ 2m  1 m  1 m  E  E  .  E   x,y ∼D cid:104  cid:12  cid:12 h x  − y cid:12  cid:12 p cid:105  ≤  1 m  m cid:88 i=1 cid:12  cid:12 h xi  − yi cid:12  cid:12 p  + 2pM p−1Rm H  + M p cid:115  log 1  δ 2m  .  As in the case of classiﬁcation, these generalization bounds suggest a trade-oﬀ between reducing the empirical error, which may require more complex hypothesis sets, and controlling the Rademacher complexity of H, which may increase the empirical error. An important beneﬁt of the last learning bound of the theorem is that it is data-dependent. This can lead to more accurate learning guarantees. The upper bounds on Rm H  or RS H  for kernel-based hypotheses  theorem 6.12  can be used directly here to derive generalization bounds in terms of the trace of the kernel matrix or the maximum diagonal entry.   11.2 Generalization bounds  271  Figure 11.1 Illustration of the shattering of a set of two points {z1, z2} with witnesses t1 and t2.  11.2.3 Pseudo-dimension bounds As previously discussed in the case of classiﬁcation, it is sometimes computation- ally hard to estimate the empirical Rademacher complexity of a hypothesis set. In chapter 3, we introduce other measures of the complexity of a hypothesis set such as the VC-dimension, which are purely combinatorial and typically easier to compute or upper bound. However, the notion of shattering or that of VC-dimension intro- duced for binary classiﬁcation are not readily applicable to real-valued hypothesis classes.  We ﬁrst introduce a new notion of shattering for families of real-valued functions. As in previous chapters, we will use the notation G for a family of functions, when- ever we intend to later interpret it  at least in some cases  as the family of loss func- tions associated to some hypothesis set H: G = {z =  x, y   cid:55 → L h x , y  : h ∈ H}. Deﬁnition 11.4  Shattering  Let G be a family of functions from a set Z to R. A set {z1, . . . , zm} ⊆ X is said to be shattered by G if there exist t1, . . . , tm ∈ R such that,     cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12   ...   sgn cid:0 g z1  − t1 cid:1  sgn cid:0 g zm  − tm cid:1    : g ∈ G  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12  cid:12   = 2m .  When they exist, the threshold values t1, . . . , tm are said to witness the shattering. Thus, {z1, . . . , zm} is shattered if for some witnesses t1, . . . , tm, the family of func- tions G is rich enough to contain a function going above a subset A of the set of points I = { zi, ti  : i ∈ [m]} and below the others  I − A , for any choice of the subset A. Figure 11.1 illustrates this shattering in a simple case. The notion of shattering naturally leads to the following deﬁnition.  t1  t2  z1  z2   272  Chapter 11 Regression  Figure 11.2 A function g : z =  x, y   cid:55 → L h x , y   in blue  deﬁned as the loss of some ﬁxed hypothesis h ∈ H, and its thresholded version  x, y   cid:55 → 1L h x ,y >t  in red  with respect to the threshold t  in yellow .  Deﬁnition 11.5  Pseudo-dimension  Let G be a family of functions mapping from X to R. Then, the pseudo-dimension of G, denoted by Pdim G , is the size of the largest set shattered by G.  By deﬁnition of the shattering just introduced, the notion of pseudo-dimension of a family of real-valued functions G coincides with that of the VC-dimension of the corresponding thresholded functions mapping X to {0, 1}:  Pdim G  = VCdim cid:16  cid:8  x, t   cid:55 → 1 g x −t >0 : g ∈ G cid:9  cid:17  .  Figure 11.2 illustrates this interpretation. In view of this interpretation, the follow- ing two results follow directly the properties of the VC-dimension. Theorem 11.6 The pseudo-dimension of hyperplanes in RN is given by   11.3   Pdim {x  cid:55 → w · x + b : w ∈ RN , b ∈ R}  = N + 1 .  Theorem 11.7 The pseudo-dimension of a vector space of real-valued functions H is equal to the dimension of the vector space:  Pdim H  = dim H  .  The following theorem gives a generalization bound for bounded regression in terms of the pseudo-dimension of a family of loss function G = {z =  x, y   cid:55 → L h x , y  : h ∈ H} associated to a hypothesis set H. The key technique to derive these bounds consists of reducing the problem to that of classiﬁcation by making use of the following general identity for the expectation of a random variable X:  E[X] = − cid:90  0  −∞  P[X < t]dt + cid:90  +∞  0  P[X > t]dt ,   11.4   L h x , y  t 1L h x ,y >t  s s o L  1.5  1.0  0.5  0.0  -2  -1  1  2  0 z   11.2 Generalization bounds  273  which holds by deﬁnition of the Lebesgue integral. In particular, for any distribution D and any non-negative measurable function f , we can write  E z∼D  [f  z ] = cid:90  ∞  0  P z∼D  [f  z  > t]dt .   11.5   Theorem 11.8 Let H be a family of real-valued functions and G = { x, y   cid:55 → L h x , y  : h ∈ H} the family of loss functions associated to H. Assume that Pdim G  = d and that the loss function L is non-negative and bounded by M . Then, for any δ > 0, with probability at least 1− δ over the choice of am i.i.d. sample S of size m drawn from Dm, the following inequality holds for all h ∈ H:  R h  ≤  cid:98 RS h  + M cid:114  2d log em  m  d  + M cid:115  log 1  δ 2m  .   11.6   Proof: Let S be a sample of size m drawn i.i.d. according to D and let  cid:98 D denote  the empirical distribution deﬁned by S. For any h ∈ H and t ≥ 0, we denote by c h, t  the classiﬁer deﬁned by c h, t  :  x, y   cid:55 → 1L h x ,y >t. The error of c h, t  can be deﬁned by  R c h, t   = P  [c h, t  x, y  = 1] = P  [L h x , y  > t],   x,y ∼D   x,y ∼D  E   x,y ∼D  Now, in view of the identity  11.5  and the fact that the loss function L is bounded  by M , we can write:  and, similarly, its empirical error is  cid:98 RS c h, t   = P R h  −  cid:98 RS h  = cid:12  cid:12  cid:12   x,y ∼ cid:98 D [L h x , y ] − E = cid:12  cid:12  cid:12  cid:12  cid:12    x,y ∼ cid:98 D[L h x , y  > t]. [L h x , y ] cid:12  cid:12  cid:12   x,y ∼ cid:98 D [L h x , y  > t] − P  x,y ∼D  x,y ∼ cid:98 D [L h x , y  > t] − P P  0  cid:32  P  cid:90  M t∈[0,M ] cid:12  cid:12  cid:12  cid:12  cid:12  t∈[0,M ] cid:12  cid:12  cid:12 R c h, t   −  cid:98 RS c h, t   cid:12  cid:12  cid:12  .  [L h x , y  > t] cid:33  dt cid:12  cid:12  cid:12  cid:12  cid:12  [L h x , y  > t] cid:12  cid:12  cid:12  cid:12  cid:12  M . t∈[0,M ] cid:12  cid:12  cid:12 R c h, t   −  cid:98 RS c h, t   cid:12  cid:12  cid:12  >  h∈H R h  −  cid:98 RS h  >  cid:15  cid:21  ≤ P sup P cid:20  sup  This implies the following inequality:  The right-hand side can be bounded using a standard generalization bound for classiﬁcation  corollary 3.19  in terms of the VC-dimension of the family of hy-  ≤ M sup  = M sup   x,y ∼D  h∈H   cid:15    274  Chapter 11 Regression  Figure 11.3 For N = 1, linear regression consists of ﬁnding the line of best ﬁt, measured in terms of the squared loss.  potheses {c h, t  : h ∈ H, t ∈ [0, M ]}, which, by deﬁnition of the pseudo-dimension,  cid:3  is precisely Pdim G  = d. The resulting bound coincides with  11.6 .  The notion of pseudo-dimension is suited to the analysis of regression as demon- strated by the previous theorem; however, it is not a scale-sensitive notion. There exists an alternative complexity measure, the fat-shattering dimension, that is scale- sensitive and that can be viewed as a natural extension of the pseudo-dimension. Its deﬁnition is based on the notion of γ-shattering. Deﬁnition 11.9  γ-shattering  Let G be a family of functions from Z to R and let γ > 0. A set {z1, . . . , zm} ⊆ X is said to be γ-shattered by G if there exist t1, . . . , tm ∈ R such that for all y ∈ {−1, +1}m, there exists g ∈ G such that:  ∀i ∈ [m],  yi g zi  − ti  ≥ γ .  Thus, {z1, . . . , zm} is γ-shattered if for some witnesses t1, . . . , tm, the family of functions G is rich enough to contain a function going at least γ above a subset A of the set of points I = { zi, ti  : i ∈ [m]} and at least γ below the others  I − A , for any choice of the subset A. Deﬁnition 11.10  γ-fat-dimension  The γ-fat-dimension of G, fatγ G , is the size of the largest set that is γ-shattered by G.  Finer generalization bounds than those based on the pseudo-dimension can be de- rived in terms of the γ-fat-dimension. However, the resulting learning bounds, are not more informative than those based on the Rademacher complexity, which is also a scale-sensitive complexity measure. Thus, we will not detail an analysis based on the γ-fat-dimension.   11.3 Regression algorithms  275  11.3 Regression algorithms  The results of the previous sections show that, for the same empirical error, hypoth- esis sets with smaller complexity measured in terms of the Rademacher complexity or in terms of pseudo-dimension beneﬁt from better generalization guarantees. One family of functions with relatively small complexity is that of linear hypotheses. In this section, we describe and analyze several algorithms based on that hypothe- sis set: linear regression, kernel ridge regression  KRR , support vector regression  SVR , and Lasso. These algorithms, in particular the last three, are extensively used in practice and often lead to state-of-the-art performance results.  11.3.1 Linear regression We start with the simplest algorithm for regression known as linear regression. Let  Φ : X → RN be a feature mapping from the input space X to RN and consider the  family of linear hypotheses  H = {x  cid:55 → w · Φ x  + b : w ∈ RN , b ∈ R} .  Linear regression consists of seeking a hypothesis in H with the smallest empirical  mean squared error. Thus, for a sample S = cid:0  x1, y1 , . . . ,  xm, ym  cid:1  ∈  X × Y m,  the following is the corresponding optimization problem:  min w,b  1 m  m cid:88 i=1   w · Φ xi  + b − yi 2 .  min W  F  W  =  1 m cid:107 X cid:62 W − Y cid:107 2,  Figure 11.3 illustrates the algorithm in the simple case where N = 1. The opti- mization problem admits the simpler formulation:  ym cid:21 . The objec- using the notation X = cid:2  Φ x1  ... Φ xm  tive function F is convex, by composition of the convex function u  cid:55 →  cid:107 u cid:107 2 with the aﬃne function W  cid:55 → X cid:62 W − Y, and it is diﬀerentiable. Thus, F admits a global minimum at W if and only if ∇F  W  = 0, that is if and only if  b  cid:35  and Y = cid:20  y1...   cid:3 , W = cid:34  w1...  wN  ...  1  1  2 m  X X cid:62 W − Y  = 0 ⇔ XX cid:62 W = XY .   11.10   When XX cid:62  is invertible, this equation admits a unique solution. Otherwise, the equation admits a family of solutions that can be given in terms of the pseudo-inverse of matrix XX cid:62   see appendix A  by W =  XX cid:62  †XY +  I −  XX cid:62  † XX cid:62   W0, where W0 is an arbitrary matrix in RN×N . Among these,   11.7    11.8    11.9    276  Chapter 11 Regression  the solution W =  XX cid:62  †XY is the one with the minimal norm and is often preferred for that reason. Thus, we will write the solutions as  W = cid:40  XX cid:62  −1XY if XX cid:62  is invertible,   XX cid:62  †XY otherwise.   11.11   The matrix XX cid:62  can be computed in O mN 2 . The cost of its inversion or that of computing its pseudo-inverse is in O N 3 .19 Finally, the multiplication with X and Y takes O mN 2 . Therefore, the overall complexity of computing the solution W is in O mN 2 + N 3 . Thus, when the dimension of the feature space N is not too large, the solution can be computed eﬃciently.  While linear regression is simple and admits a straightforward implementation, it does not beneﬁt from a strong generalization guarantee, since it is limited to minimizing the empirical error without controlling the norm of the weight vector and without any other regularization. Its performance is also typically poor in most applications. The next sections describe algorithms with both better theoretical guarantees and improved performance in practice.  11.3.2 Kernel ridge regression We ﬁrst present a learning guarantee for regression with bounded linear hypotheses in a feature space deﬁned by a PDS kernel. This will provide a strong theoretical support for the kernel ridge regression algorithm presented in this section. The learning bounds of this section are given for the squared loss. Thus, in particular, the generalization error of a hypothesis h is deﬁned by R h  = E   x,y ∼D cid:2  h x  − y 2 cid:3 . Theorem 11.11 Let K : X × X → R be a PDS kernel, Φ : X → H a feature mapping associated to K, and H = {x  cid:55 → w · Φ x  :  cid:107 w cid:107 H ≤ Λ}. Assume that there exists r > 0 such that K x, x  ≤ r2 and M > 0 such that h x  − y < M for all  x, y  ∈ X × Y. Then, for any δ > 0, with probability at least 1 − δ, each of the following inequalities holds for all h ∈ H:  R h  ≤  cid:98 RS h  + 4M cid:114  r2Λ2 4M Λ cid:112 Tr[K] R h  ≤  cid:98 RS h  +  m  m  δ 2m  + M 2 cid:115  log 1 + 3M 2 cid:115  log 2  δ 2m  .  19 In the analysis of the computational complexity of the algorithms discussed in this chapter, the cubic-time complexity of matrix inversion can be replaced by a more favorable complexity O N 2+ω , with ω = .376 using asymptotically faster matrix inversion methods such as that of Coppersmith and Winograd.   11.3 Regression algorithms  277  Proof: By the bound on the empirical Rademacher complexity of kernel-based hypotheses  theorem 6.12 , the following holds for any sample S of size m:  Λ cid:112 Tr[K]  m  ≤ cid:114  r2Λ2  m  ,   cid:98 RS H  ≤ which implies that Rm H  ≤ cid:113  r2Λ2  ing bounds of Theorem 11.3 yield immediately the inequalities claimed.  m . Combining these inequalities with the learn-  cid:3   The learning bounds of the theorem suggests minimizing a trade-oﬀ between the empirical squared loss  ﬁrst term on the right-hand side , and the norm of the weight vector  upper bound Λ on the norm appearing in the second term , or equivalently the norm squared. Kernel ridge regression is deﬁned by the minimization of an objective function that has precisely this form and thus is directly motivated by the theoretical analysis just presented:  min  w  F  w  = λ cid:107 w cid:107 2 +   w · Φ xi  − yi 2 .   11.12   m cid:88 i=1  Here, λ is a positive parameter determining the trade-oﬀ between the regularization term  cid:107 w cid:107 2 and the empirical mean squared error. The objective function diﬀers from that of linear regression only by the ﬁrst term, which controls the norm of w. As in the case of linear regression, the problem can be rewritten in a more compact form as  min W  F  W  = λ cid:107 W cid:107 2 +  cid:107 X cid:62 W − Y cid:107 2,   11.13   where X ∈ RN×m is the matrix formed by the feature vectors, X = [ Φ x1  ... Φ xm  ], W = w, and Y =  y1, . . . , ym  cid:62 . Here too, F is convex, by the convexity of w  cid:55 →  cid:107 w cid:107 2 and that of the sum of two convex functions, and is diﬀerentiable. Thus F admits a global minimum at W if and only if  ∇F  W  = 0 ⇔  XX cid:62  + λI W = XY ⇔ W =  XX cid:62  + λI −1XY.   11.14   Note that the matrix XX cid:62  + λI is always invertible, since its eigenvalues are the sum of the non-negative eigenvalues of the symmetric positive semideﬁnite matrix XX cid:62  and λ > 0. Thus, kernel ridge regression admits a closed-form solution.  An alternative formulation of the optimization problem for kernel ridge regression  equivalent to  11.12  is  min  w   w · Φ xi  − yi 2  subject to:  cid:107 w cid:107 2 ≤ Λ2.  m cid:88 i=1  This makes the connection with the bounded linear hypothesis set of theorem 11.11 even more evident. Using slack variables ξi, for all i ∈ [m], the problem can be   278  Chapter 11 Regression  equivalently written as  min  w  ξ2 i  m cid:88 i=1  subject to:   cid:107 w cid:107 2 ≤ Λ2  ∧ cid:0 ∀i ∈ [m], ξi = yi − w · Φ xi  cid:1 .  This is a convex optimization problem with diﬀerentiable objective function and constraints. To derive the equivalent dual problem, we introduce the Lagrangian L, which is deﬁned for all ξ, w, α cid:48 , and λ ≥ 0 by  L ξ, w, α cid:48 , λ  =  ξ2 i +  α cid:48 i yi − ξi − w · Φ xi   + λ  cid:107 w cid:107 2 − Λ2  .  m cid:88 i=1  m cid:88 i=1  The KKT conditions lead to the following equalities:  m cid:88 i=1  1 2λ  m cid:88 i=1  α cid:48 iΦ xi   w =  ξi = α cid:48 i 2  α cid:48 iΦ xi  + 2λw = 0  =⇒ =⇒  ∇wL = − ∇ξiL = 2ξi − α cid:48 i = 0 ∀i ∈ [m], α cid:48 i yi − ξi − w · Φ xi   = 0 λ  cid:107 w cid:107 2 − Λ2  = 0. Plugging in the expressions of w and ξis in that of L gives m cid:88 i=1 + λ cid:16  1 m cid:88 i=1 m cid:88 i=1  m cid:88 i=1 α cid:48 iyi − α cid:48 iΦ xi  cid:107 2 − Λ2 cid:17  m cid:88 i=1 m cid:88 i=1 m cid:88 i,j=1 α cid:48 iyi − m cid:88 i,j=1 m cid:88 i=1 αiαjΦ xi  cid:62 Φ xj  − λΛ2,  m cid:88 i,j=1  α cid:48 i 2 −  m cid:88 i=1  αiyi −  = −λ  4λ2 cid:107   α cid:48 2 i 4  = −  L =  1 4λ  1 2λ  i + 2  α cid:48 2  i +  α2  1 4  +  2  α cid:48 iα cid:48 jΦ xi  cid:62 Φ xj  − λΛ2  α cid:48 iα cid:48 jΦ xi  cid:62 Φ xj   with α cid:48 i = 2λαi. Thus, the equivalent dual optimization problem for KRR can be written as follows:  or, more compactly, as  max  α∈Rm −λα cid:62 α + 2α cid:62 Y − α cid:62  X cid:62 X α ,  max α∈Rm  G α  = −α cid:62  K + λI α + 2α cid:62 Y ,   11.15    11.16   where K = X cid:62 X is the kernel matrix associated to the training sample. The objective function G is concave and diﬀerentiable. The optimal solution is obtained   11.3 Regression algorithms  279  by diﬀerentiating the function and setting it to zero:  ∇G α  = 0 ⇐⇒ 2 K + λI α = 2Y ⇐⇒ α =  K + λI −1Y .   11.17   Note that  K+λI  is invertible, since its eigenvalues are the sum of the eigenvalues of the SPSD matrix K and λ > 0. Thus, as in the primal case, the dual optimization problem admits a closed-form solution. By the ﬁrst KKT equation, w can be determined from α by  w =  αiΦ xi  = Xα = X K + λI −1Y.   11.18   m cid:88 i=1  The hypothesis h solution can be given as follows in terms of α:  m cid:88 i=1  ∀x ∈ X,  h x  = w · Φ x  =  αiK xi, x  .   11.19   Note that the form of the solution, h =  cid:80 m  i=1 αiK xi,· , could be immediately predicted using the Representer theorem, since the objective function minimized by KRR falls within the general framework of theorem 6.11. This also could show that w could be written as w = Xα. This fact, combined with the following simple lemma, can be used to determine α in a straightforward manner, without the intermediate derivation of the dual problem.  Lemma 11.12 The following identity holds for any matrix X:   XX cid:62  + λI −1X = X X cid:62 X + λI −1 .  Proof: Observe that  XX cid:62  + λI X = X X cid:62 X + λI . Left-multiplying by  XX cid:62  + λI −1 this equality and right-multiplying it by  X cid:62 X + λI −1 yields the statement  cid:3  of the lemma.  Now, using this lemma, the primal solution of w can be rewritten as follows:  w =  XX cid:62  + λI −1XY = X X cid:62 X + λI −1Y = X K + λI −1Y.  Comparing with w = Xα gives immediately α =  K + λI −1Y.  Our presentation of the KRR algorithm was given for linear hypotheses with no oﬀset, that is we implicitly assumed b = 0. It is common to use this formulation and to extend it to the general case by augmenting the feature vector Φ x  with an extra component equal to one for all x ∈ X and the weight vector w with an extra component b ∈ R. For the augmented feature vector Φ cid:48  x  ∈ RN +1 and weight vector w cid:48  ∈ RN +1, we have w cid:48  · Φ cid:48  x  = w · Φ x  + b. Nevertheless, this formulation does not coincide with the general KRR algorithm where a solution of the form x  cid:55 → w · Φ x  + b is sought. This is because for the general KRR, the regularization term is λ cid:107 w cid:107 , while for the extension just described it is λ cid:107 w cid:48  cid:107 .   280  Chapter 11 Regression  Table 11.1 Comparison of the running-time complexity of KRR for computing the solution or the prediction value of a point in both the primal and the dual case. κ denotes the time complexity of computing a kernel value; for polynomial and Gaussian kernels, κ = O N  .  Solution  Prediction  Primal O mN 2 + N 3  O κm2 + m3  Dual  O N   O κm   In both the primal and dual cases, KRR admits a closed-form solution. Table 11.1 gives the time complexity of the algorithm for computing the solution and the one for determining the prediction value of a point in both cases. In the primal case, determining the solution w requires computing matrix XX cid:62 , which takes O mN 2 , the inversion of  XX cid:62  + λI , which is in O N 3 , and multiplication with X, which is in O mN 2 . Prediction requires computing the inner product of w with a feature vector of the same dimension that can be achieved in O N  . The dual solution ﬁrst requires computing the kernel matrix K. Let κ be the maximum cost of computing K x, x cid:48   for all pairs  x, x cid:48   ∈ X × X. Then, K can be computed in O κm2 . The inversion of matrix K + λI can be achieved in O m3  and multiplication with Y takes O m2 . Prediction requires computing the vector  K x1, x , . . . , K xm, x   cid:62  for some x ∈ X, which requires O κm , and the inner product with α, which is in O m .  Thus, in both cases, the main step for computing the solution is a matrix inversion, which takes O N 3  in the primal case, O m3  in the dual case. When the dimension of the feature space is relatively small, solving the primal problem is advantageous, while for high-dimensional spaces and medium-sized training sets, solving the dual is preferable. Note that for relatively large matrices, the space complexity could also be an issue: the size of relatively large matrices could be prohibitive for memory storage and the use of external memory could signiﬁcantly aﬀect the running time of the algorithm.  For sparse matrices, there exist several techniques for faster computations of the matrix inversion. This can be useful in the primal case where the features can be relatively sparse. On the other hand, the kernel matrix K is typically dense; thus, there is less hope for beneﬁting from such techniques in the dual case. In such cases, or, more generally, to deal with the time and space complexity issues arising when m and N are large, approximation methods using low-rank approximations via the Nystr¨om method or the partial Cholesky decomposition can be used very eﬀectively.  The KRR algorithm admits several advantages: it beneﬁts from favorable theo- retical guarantees since it can be derived directly from the generalization bound we   11.3 Regression algorithms  281  Figure 11.4 SVR attempts to ﬁt a “tube” with width  cid:15  to the data. Training data within the “epsilon tube”  blue points  incur no loss.  presented; it admits a closed-form solution, which can make the analysis of many of its properties convenient; and it can be used with PDS kernels, which extends its use to non-linear regression solutions and more general features spaces. KRR also admits favorable stability properties that we discuss in chapter 14.  The algorithm can be generalized to learning a mapping from X to Rp, p > 1. This can be done by formulating the problem as p independent regression problems, each consisting of predicting one of the p target components. Remarkably, the computation of the solution for this generalized algorithm requires only a single matrix inversion, e.g.,  K + λI −1 in the dual case, regardless of the value of p.  One drawback of the KRR algorithm, in addition to the computational issues for determining the solution for relatively large matrices, is the fact that the solution it returns is typically not sparse. The next two sections present two sparse algorithms for linear regression.  11.3.3 Support vector regression In this section, we present the support vector regression  SVR  algorithm, which is inspired by the SVM algorithm presented for classiﬁcation in chapter 5. The main idea of the algorithm consists of ﬁtting a tube of width  cid:15  > 0 to the data, as illustrated by ﬁgure 11.4. As in binary classiﬁcation, this deﬁnes two sets of points: those falling inside the tube, which are  cid:15 -close to the function predicted and thus not penalized, and those falling outside, which are penalized based on their distance to the predicted function, in a way that is similar to the penalization used by SVMs in classiﬁcation. Using a hypothesis set H of linear functions: H = {x  cid:55 → w · Φ x  + b : w ∈ RN , b ∈ R}, where Φ is the feature mapping corresponding some PDS kernel K,  the optimization problem for SVR can be written as follows:  min w,b  1 2 cid:107 w cid:107 2 + C  m cid:88 i=1 cid:12  cid:12 yi −  w · Φ xi  + b  cid:12  cid:12  cid:15  ,   11.20   y  ✏  w·Φ x +b  Φ x    282  Chapter 11 Regression  where  ·  cid:15  denotes the  cid:15 -insensitive loss:  ∀y, y cid:48  ∈ Y,  y cid:48  − y cid:15  = max 0,y cid:48  − y −  cid:15  .   11.21   The use of this loss function leads to sparse solutions with a relatively small number of support vectors. Using slack variables ξi ≥ 0 and ξ cid:48 i ≥ 0, i ∈ [m], the optimization problem can be equivalently written as  min  w,b,ξ,ξ cid:48   1 2 cid:107 w cid:107 2 + C   ξi + ξ cid:48 i   m cid:88 i=1  subject to  w · Φ xi  + b  − yi ≤  cid:15  + ξi yi −  w · Φ xi  + b  ≤  cid:15  + ξ cid:48 i ξi ≥ 0, ξ cid:48 i ≥ 0, ∀i ∈ [m].   11.22    11.24    11.25    11.26   This is a convex quadratic program  QP  with aﬃne constraints. Introducing the Lagrangian and applying the KKT conditions leads to the following equivalent dual problem in terms of the kernel matrix K:  α,α cid:48  −  cid:15  α cid:48  + α  cid:62 1 +  α cid:48  − α  cid:62 y − max   α cid:48  − α  cid:62 K α cid:48  − α    11.23   1 2  subject to:  0 ≤ α ≤ C  ∧  0 ≤ α cid:48  ≤ C  ∧   α cid:48  − α  cid:62 1 = 0  .  Any PDS kernel K can be used with SVR, which extends the algorithm to non- linear regression solutions. Problem  11.23  is a convex QP similar to the dual problem of SVMs and can be solved using similar optimization techniques. The solutions α and α cid:48  deﬁne the hypothesis h returned by SVR as follows:  ∀x ∈ X,  h x  =  m cid:88 i=1  α cid:48 i − αi K xi, x  + b ,  where the oﬀset b can be obtained from a point xj with 0 < αj < C by  or from a point xj with 0 < α cid:48 j < C via  b = −  b = −  m cid:88 i=1  α cid:48 i − αi K xi, xj  + yj +  cid:15 , m cid:88 i=1  α cid:48 i − αi K xi, xj  + yj −  cid:15 .  αi cid:0  w · Φ xi  + b  − yi −  cid:15  − ξi cid:1  = 0 α cid:48 i cid:0  w · Φ xi  + b  − yi +  cid:15  + ξ cid:48 i cid:1  = 0.  By the complementarity conditions, for all i ∈ [m], the following equalities hold:   11.3 Regression algorithms  283  Thus, if αi  cid:54 = 0 or α cid:48 i  cid:54 = 0, that is if xi is a support vector, then, either  w · Φ xi  + b − yi−  cid:15  = ξi holds or yi−  w· Φ xi  + b −  cid:15  = ξ cid:48 i. This shows that support vectors points lying outside the  cid:15 -tube. Of course, at most one of αi or α cid:48 i is non-zero for any point xi: the hypothesis either overestimates or underestimates the true label by more than  cid:15 . For the points within the  cid:15 -tube, we have αj = α cid:48 j = 0; thus, these points do not contribute to the deﬁnition of the hypothesis returned by SVR. Thus, when the number of points inside the tube is relatively large, the hypothesis returned by SVR is relatively sparse. The choice of the parameter  cid:15  determines a trade-oﬀ between sparsity and accuracy: larger  cid:15  values provide sparser solutions, since more points can fall within the  cid:15 -tube, but may ignore too many key points for determining an accurate solution.  The following generalization bounds hold for the  cid:15 -insensitive loss and kernel- based hypotheses and thus for the SVR algorithm. We denote by D the distribution  deﬁned by a training sample of size m.  according to which sample points are drawn and by  cid:98 D the empirical distribution Theorem 11.13 Let K : X × X → R be a PDS kernel, let Φ : X → H be a feature mapping associated to K and let H = {x  cid:55 → w · Φ x  :  cid:107 w cid:107 H ≤ Λ}. Assume that there exists r > 0 such that K x, x  ≤ r2 and M > 0 such that h x  − y ≤ M for all  x, y  ∈ X × Y. Fix  cid:15  > 0. Then, for any δ > 0, with probability at least 1 − δ, each of the following inequalities holds for all h ∈ H,  E   x,y ∼D cid:2 h x  − y cid:15  cid:3  ≤ E  x,y ∼D cid:2 h x  − y cid:15  cid:3  ≤ E   x,y ∼ cid:98 D cid:2 h x  − y cid:15  cid:3  + 2 cid:114  r2Λ2 2Λ cid:112 Tr[K]  x,y ∼ cid:98 D cid:2 h x  − y cid:15  cid:3  +  E  m  m  δ 2m  + M cid:115  log 1 + 3M cid:115  log 2  δ 2m  .  Proof: Since for any y cid:48  ∈ Y, the function y  cid:55 → y − y cid:48  cid:15  is 1-Lipschitz, the result follows Theorem 11.3 and the bound on the empirical Rademacher complexity of  cid:3  H.  These results provide theoretical guarantees for the SVR algorithm. Notice, how- ever, that the theorem does not provide guarantees for the expected loss of the hypotheses in terms of the squared loss. For 0 <  cid:15  < 1 4, the inequality x2 ≤ x cid:15  holds for all x in [−η cid:48  cid:15 ,−η cid:15 ] ∪ [η cid:15 , η cid:48  cid:15 ] with η cid:15  = 1−√1−4 cid:15  . For small values of  cid:15 , η cid:15  ≈ 0 and η cid:48  cid:15  ≈ 1, thus, if M = 2rλ ≤ 1, then, the squared loss can be upper bounded by the  cid:15 -insensitive loss for almost all values of  h x  − y  in [−1, 1] and the theorem can be used to derive a useful generalization bound for the squared loss.  and η cid:48  cid:15  = 1+√1−4 cid:15   2  2  More generally, if the objective is to achieve a small squared loss, then, SVR can be modiﬁed by using the quadratic  cid:15 -insensitive loss, that is the square of the  cid:15 - insensitive loss, which also leads to a convex QP. We will refer by quadratic SVR to   284  Chapter 11 Regression  this version of the algorithm. Introducing the Lagrangian and applying the KKT conditions leads to the following equivalent dual optimization problem for quadratic SVR in terms of the kernel matrix K:  α,α cid:48  −  cid:15  α cid:48  + α  cid:62 1 +  α cid:48  − α  cid:62 y − max  1 2   α cid:48  − α  cid:62  cid:16 K +  1 C  I cid:17  α cid:48  − α    11.27   subject to:  α ≥ 0  ∧  α cid:48  ≥ 0  ∧  α cid:48  − α  cid:62 1 = 0  .  m cid:88 i=1  Any PDS kernel K can be used with quadratic SVR, which extends the algorithm to non-linear regression solutions. Problem  11.27  is a convex QP similar to the dual problem of SVMs in the separable case and can be solved using similar optimization techniques. The solutions α and α cid:48  deﬁne the hypothesis h returned by SVR as follows:  h x  =   α cid:48 i − αi K xi, x  + b ,   11.28   where the oﬀset b can be obtained from a point xj with 0 < αj < C or 0 < α cid:48 j < C exactly as in the case of SVR with  non-quadratic   cid:15 -insensitive loss. Note that for  cid:15  = 0, the quadratic SVR algorithm coincides with KRR as can be seen from the dual optimization problem  the additional constraint  α cid:48  − α  cid:62 1 = 0 appears here due to use of an oﬀset b . The following generalization bound holds for quadratic SVR. It can be shown in a way that is similar to the proof of theorem 11.13 using the fact that the quadratic  cid:15 -insensitive function x  cid:55 → x2  cid:15  is 2M -Lipschitz over the interval [−M, +M ]. Theorem 11.14 Let K : X × X → R be a PDS kernel, let Φ : X → H be a feature mapping associated to K and let H = {x  cid:55 → w · Φ x  :  cid:107 w cid:107 H ≤ Λ}. Assume that there exists r > 0 such that K x, x  ≤ r2 and M > 0 such that h x  − y ≤ M for all  x, y  ∈ X × Y. Fix  cid:15  > 0. Then, for any δ > 0, with probability at least 1 − δ, each of the following inequalities holds for all h ∈ H,  E   x,y ∼D cid:2 h x  − y2  x,y ∼D cid:2 h x  − y2  E   cid:15  cid:3  + 4M cid:114  r2Λ2  x,y ∼ cid:98 D cid:2 h x  − y2 4M Λ cid:112 Tr[K]  cid:15  cid:3  +  x,y ∼ cid:98 D cid:2 h x  − y2   cid:15  cid:3  ≤ E  cid:15  cid:3  ≤ E  m  m  δ 2m  + M 2 cid:115  log 1 + 3M 2 cid:115  log 2  δ 2m  .  This theorem provides a strong justiﬁcation for the quadratic SVR algorithm. Al- ternative convex loss functions can be used to deﬁne regression algorithms, in par- ticular the Huber loss  see ﬁgure 11.5 , which penalizes smaller errors quadratically and larger ones only linearly.  SVR admits several advantages: the algorithm is based on solid theoretical guar- antees, the solution returned is sparse, and it allows a natural use of PDS kernels,   11.3 Regression algorithms  285  Figure 11.5 Alternative loss functions that can be used in conjunction with SVR.  which extend the algorithm to non-linear regression solutions. SVR also admits fa- vorable stability properties that we discuss in chapter 14. However, one drawback of the algorithm is that it requires the selection of two parameters, C and  cid:15 . These can be selected via cross-validation, as in the case of SVMs, but this requires a relatively larger validation set. Some heuristics are often used to guide the search for their values: C is searched near the maximum value of the labels in the absence of an oﬀset  b = 0  and for a normalized kernel, and  cid:15  is chosen close to the average diﬀerence of the labels. As already discussed, the value of  cid:15  determines the number of support vectors and the sparsity of the solution. Another drawback of SVR is that, as in the case of SVMs or KRR, it may be computationally expensive when dealing with large training sets. One eﬀective solution in such cases, as for KRR, consists of approximating the kernel matrix using low-rank approximations via the Nystr¨om method or the partial Cholesky decomposition. In the next section, we discuss an alternative sparse algorithm for regression.  11.3.4 Lasso Unlike the KRR and SVR algorithms, the Lasso  least absolute shrinkage and selection operator  algorithm does not admit a natural use of PDS kernels. Thus, here, we assume that the input space X is a subset of RN and consider a family of linear hypotheses H = {x  cid:55 → w · x + b : w ∈ RN , b ∈ R}. Let S = cid:0  x1, y1 , . . . ,  xm, ym  cid:1  ∈  X×Y m be a labeled training sample. Lasso is  based on the minimization of the empirical squared error on S with a regularization term depending on the norm of the weight vector, as in the case of the ridge regression, but using the L1 norm instead of the L2 norm and without squaring the   286  Chapter 11 Regression  Figure 11.6 Comparison of the Lasso and ridge regression solutions.  norm:  min w,b  F  w, b  = λ cid:107 w cid:107 1 +   w · xi + b − yi 2 .   11.29   m cid:88 i=1  Here λ denotes a positive parameter as for ridge regression. This is a convex optimization problem, since  cid:107 · cid:107 1 is convex as with all norms and since the empirical error term is convex, as already discussed for linear regression. The optimization for Lasso can be written equivalently as  min w,b   w · xi + b − yi 2  subject to:  cid:107 w cid:107 1 ≤ Λ1,   11.30   m cid:88 i=1  where Λ1 is a positive parameter.  The key property of Lasso as in the case of other algorithms using the L1 norm constraint is that it leads to a sparse solution w, that is one with few non-zero components. Figure 11.6 illustrates the diﬀerence between the L1 and L2 regular- izations in dimension two. The objective function of  11.30  is a quadratic function, thus its contours are ellipsoids, as illustrated by the ﬁgure  in blue . The areas cor- responding to L1 and L2 balls of a ﬁxed radius Λ1 are also shown in the left and right panel  in red . The Lasso solution is the point of intersection of the contours with the L1 ball. As can be seen form the ﬁgure, this can typically occur at a corner of the L1 ball where some coordinates are zero. In contrast, the ridge regression solution is at the point of intersection of the contours and the L2 ball, where none of the coordinates is typically zero.  L1 regularization  L2 regularization   11.3 Regression algorithms  287  The following results show that Lasso also beneﬁts from strong theoretical guaran- tees. We ﬁrst give a general upper bound on the empirical Rademacher complexity of L1 norm-constrained linear hypotheses . Theorem 11.15  Rademacher complexity of linear hypotheses with bounded L1 norm  Let  X ⊆ RN and let S =  cid:0  x1, y1 , . . . ,  xm, ym  cid:1  ∈  X × Y m be a sample of size m.  Assume that for all i ∈ [m],  cid:107 xi cid:107 ∞ ≤ r∞ for some r∞ > 0, and let H = {x ∈ X  cid:55 → w · x :  cid:107 w cid:107 1 ≤ Λ1}. Then, the empirical Rademacher complexity of H can be bounded as follows:  ∞Λ2 1 log 2N   m  .   11.31   Proof: For any i ∈ [m] we denote by xij the jth component of xi.   cid:98 RS H  =  E  E   cid:107 w cid:107 1≤Λ1   cid:98 RS H  ≤ cid:114  2r2 σiw · xi cid:35  σ cid:34  sup m cid:88 i=1 σixi cid:13  cid:13  cid:13 ∞ cid:35  σ cid:34  cid:13  cid:13  cid:13  m cid:88 i=1 σixij cid:12  cid:12  cid:12  cid:12  cid:12  j∈[N ] cid:12  cid:12  cid:12  cid:12  cid:12  σ cid:34  max m cid:88 i=1 σ cid:34  max σizi cid:35  , σ cid:34 sup m cid:88 i=1   cid:35  m cid:88 i=1  s∈{−1,+1}  j∈[N ]  z∈A  max  E  E  E  s  1 m  Λ1 m  Λ1 m  Λ1 m  Λ1 m  σixij cid:35   =  =  =  =   by deﬁnition of the dual norm    by deﬁnition of  cid:107  ·  cid:107 ∞    by deﬁnition of  cid:107  ·  cid:107 ∞   where A denotes the set of N vectors {s x1j, . . . , xmj  cid:62  : j ∈ [N ], s ∈ {−1, +1}}. For any z ∈ A, we have  cid:107 z cid:107 2 ≤  cid:112 mr2 ∞ = r∞√m. Thus, by Massart’s lemma  theorem 3.7 , since A contains at most 2N elements, the following inequality holds:   cid:98 RS H  ≤ Λ1r∞  which concludes the proof.  √m cid:112 2 log 2N    m  = r∞Λ1 cid:114  2 log 2N    m  ,   cid:3   Note that dependence of the bound on the dimension N is only logarithmic, which suggests that using very high-dimensional feature spaces does not signiﬁcantly aﬀect generalization.  Combining the Rademacher complexity bound just proven and the general result of Theorem 11.3 yields the following generalization bound for the hypothesis set used by Lasso, using the squared loss.  Theorem 11.16 Let X ⊆ RN and H = {x ∈ X  cid:55 → w · x :  cid:107 w cid:107 1 ≤ Λ1}. Assume that there exists r∞ > 0 such for all x ∈ X,  cid:107 x cid:107 ∞ ≤ r∞ and M > 0 such that   288  Chapter 11 Regression  h x  − y ≤ M for all  x, y  ∈ X × Y. Then, for any δ > 0, with probability at least 1 − δ, each of the following inequalities holds for all h ∈ H:  R h  ≤  cid:98 RS h  + 2r∞Λ1M cid:114  2 log 2N    m  + M 2 cid:115  log 1  δ 2m  .   11.32   As in the case of ridge regression, we observe that the objective function minimized by Lasso has the same form as the right-hand side of this generalization bound.  There exist a variety of diﬀerent methods for solving the optimization problem of Lasso, including an eﬃcient algorithm  LARS  for computing the entire regulariza- tion path of solutions, that is, the Lasso solutions for all values of the regularization parameter λ, and other on-line solutions that apply more generally to optimization problems with an L1 norm constraint.  λ  j=1 w+  Here, we show that the Lasso problems  11.29  or  11.30  are equivalent to a quadratic program  QP , and therefore that any QP solver can be used to compute the solution. Observe that any weight vector w can be written as w = w+ − w−, j = 0 or w−j = 0 for any j ∈ [N ], which implies with w+ ≥ 0, w− ≥ 0, and w+  cid:107 w cid:107 1 = cid:80 N j + w−j . This can be done by deﬁning the jth component of w+ as wj if wj ≥ 0, 0 otherwise, and similarly the jth component of w− as −wj if wj ≤ 0, 0 otherwise, for any j ∈ [N ]. With the replacement w = w+ − w−, with w+ ≥ 0, w− ≥ 0, and  cid:107 w cid:107 1 = cid:80 N N cid:88 j=1  m cid:88 i=1 cid:0  w+ − w−  · xi + b − yi cid:1 2  j + w−j , the Lasso problem  11.29  becomes  w+≥0,w−≥0,b  j + w−j   +  j=1 w+   11.33    w+  min  j − δj  and w−j with  w−j − δj  would not aﬀect w+  Conversely, a solution w = w+ − w− of  11.33  veriﬁes the condition w+ j = 0 or j when wj ≥ 0 and wj = −w−j when wj ≤ 0. w−j = 0 for any j ∈ [N ], thus wj = w+ j , w−j   > 0 for some j ∈ [N ], replacing w+ This is because if δj = min w+ j with j − δ −  w−j − δ ,  w+ j + w−j   in the objective function by 2δj > 0 and but would reduce the term  w+ provide a better solution. In view of this analysis, problems  11.29  and  11.33  admit the same optimal solution and are equivalent. Problem  11.33  is a QP since the objective function is quadratic in w+, w−, and b, and since the constraints are aﬃne. With this formulation, the problem can be straightforwardly shown to admit a natural online algorithmic solution  exercise 11.10 .20  j − w−j =  w+  .  Thus, Lasso has several advantages: it beneﬁts from strong theoretical guarantees and returns a sparse solution, which is advantageous when there are accurate so- lutions based on few features. The sparsity of the solution is also computationally  20 The technique we described to avoid absolute values in the objective function can be used similarly in other optimization problems.   11.3 Regression algorithms  289  attractive; sparse feature representations of the weight vector can be used to make the inner product with a new vector more eﬃcient. The algorithm’s sparsity can also be used for feature selection. The main drawback of the algorithm is that it does not admit a natural use of PDS kernels and thus an extension to non-linear regression, unlike KRR and SVR. One solution is then to use empirical kernel maps, as discussed in chapter 6. Also, Lasso’s solution does not admit a closed-form so- lution. This is not a critical property from the optimization point of view but one that can make some mathematical analyses very convenient.  11.3.5 Group norm regression algorithms Other types of regularization aside from the L1 or L2 norm can be used to deﬁne regression algorithms. For instance, in some situations, the feature space may be naturally partitioned into subsets, and it may be desirable to ﬁnd a sparse solution that selects or omits entire subsets of features. A natural norm in this setting is the group or mixed norm L2,1, which is a combination of the L1 and L2 norms.  Imagine that we partition w ∈ RN as w1, . . . , wk, where wj ∈ RNj for 1 ≤ j ≤ k and  cid:80 j Nj = N , and deﬁne W =  w cid:62 1 , . . . , w cid:62 k   cid:62 . Then the L2,1 norm of W is  deﬁned as   cid:107 W cid:107 2,1 =   cid:107 wj cid:107  .  k cid:88 j=1  Combining the L2,1 norm with the empirical mean squared error leads to the Group Lasso formulation. More generally, an Lq,p group norm regularization can be used for q, p ≥ 1  see appendix A for the deﬁnition of group norms . 11.3.6 On-line regression algorithms The regression algorithms presented in the previous sections admit natural on- line versions. Here, we brieﬂy present two examples of these algorithms. These algorithms are particularly useful for applications to very large data sets for which a batch solution can be computationally too costly to derive and more generally in all of the on-line learning settings discussed in chapter 8.  Our ﬁrst example is known as the Widrow-Hoﬀ algorithm and coincides with the application of stochastic gradient descent techniques to the linear regression objective function. Figure 11.7 gives the pseudocode of the algorithm. A similar algorithm can be derived by applying the stochastic gradient technique to ridge regression. At each round, the weight vector is augmented with a quantity that depends on the prediction error  wt · xt − yt . Our second example is an online version of the SVR algorithm, which is obtained by application of stochastic gradient descent to the dual objective function of SVR. Figure 11.8 gives the pseudocode of the algorithm for an arbitrary PDS kernel K   290  Chapter 11 Regression  WidrowHoff w0  1 w1 ← w0 2  for t ← 1 to T do Receive xt    cid:46  typically w0 = 0  3  4  5  6  7   cid:98 yt ← wt · xt  Receive yt  wt+1 ← wt + 2η wt · xt − yt xt  return wT +1   cid:46  learning rate η > 0.  Figure 11.7 The Widrow-Hoﬀ algorithm.  11.4 Chapter notes  in the absence of any oﬀset  b = 0 . Another on-line regression algorithm is given by exercise 11.10 for Lasso.  The generalization bounds presented in this chapter are for bounded regression problems. When {x  cid:55 → L h x , y  : h ∈ H}, the family of losses of the hypotheses, is not bounded, a single function can take arbitrarily large values with arbitrarily small probabilities. This is the main issue for deriving uniform convergence bounds for unbounded losses. This problem can be avoided either by assuming the existence of an envelope, that is a single non-negative function with a ﬁnite expectation lying above the absolute value of the loss of every function in the hypothesis set [Dudley, 1984, Pollard, 1984, Dudley, 1987, Pollard, 1989, Haussler, 1992], or by assuming that some moment of the loss functions is bounded [Vapnik, 1998, 2006]. Cortes, Greenberg, and Mohri [2013]  see also [Cortes et al., 2010a]  give two-sided generalization bounds for unbounded losses with ﬁnite second moments. The one- sided version of their bounds coincides with that of Vapnik [1998, 2006] modulo a constant factor, but the proofs given by Vapnik in both books seem to be incomplete and incorrect.  The notion of pseudo-dimension is due to Pollard [1984].  Its equivalent deﬁ- nition in terms of VC-dimension is discussed by Vapnik [2000]. The notion of fat-shattering was introduced by Kearns and Schapire [1990]. The linear regression algorithm is a classical algorithm in statistics that dates back at least to the nine-   11.4 Chapter notes  291  OnLineDualSVR   1 α ← 0 2 α cid:48  ← 0 3  for t ← 1 to T do Receive xt   4  5  6  7  8  9  Receive yt   s=1 α cid:48 s − αs K xs, xt    cid:98 yt ← cid:80 t α cid:48 t+1 ← α cid:48 t + min max η yt − cid:98 yt −  cid:15  ,−α cid:48 t , C − α cid:48 t  αt+1 ← αt + min max η  cid:98 yt − yt −  cid:15  ,−αt , C − αt  return cid:80 T  t=1 α cid:48 t − αt K xt,·   Figure 11.8 An on-line version of dual SVR.  teenth century. The ridge regression algorithm is due to Hoerl and Kennard [1970]. Its kernelized version  KRR  was introduced and discussed by Saunders, Gammer- man, and Vovk [1998]. An extension of KRR to outputs in Rp with p > 1 with possible constraints on the regression is presented and analyzed by Cortes, Mohri, and Weston [2007c]. The support vector regression  SVR  algorithm is discussed in Vapnik [2000]. Lasso was introduced by Tibshirani [1996]. The LARS algorithm for solving its optimization problem was later presented by Efron et al. [2004]. The Widrow-Hoﬀ on-line algorithm is due to Widrow and Hoﬀ [1988]. The dual on-line SVR algorithm was ﬁrst introduced and analyzed by Vijayakumar and Wu [1999]. The kernel stability analysis of exercise 10.3 is from Cortes et al. [2010b].  For large-scale problems where a straightforward batch optimization of a primal or dual objective function is intractable, general iterative stochastic gradient descent methods similar to those presented in section 11.3.6, or quasi-Newton methods such as the limited-memory BFGS  Broyden-Fletcher-Goldfard-Shanno  algorithm [Nocedal, 1980] can be practical alternatives in practice.  In addition to the linear regression algorithms presented in this chapter and their kernel-based non-linear extensions, there exist many other algorithms for regression, including decision trees for regression  see chapter 9 , boosting trees for regression, and artiﬁcial neural networks.   292  11.5 Exercises  Chapter 11 Regression  11.1 Pseudo-dimension and monotonic functions.  Assume that φ is a strictly monotonic function and let φ ◦ H be the family of functions deﬁned by φ ◦ H = {φ h ·   : h ∈ H}, where H is some set of real-valued functions. Show that Pdim φ ◦ H  = Pdim H .  11.2 Pseudo-dimension of linear functions. Let H be the set of all linear functions in  dimension d, i.e. h x  = w cid:62 x for some w ∈ Rd. Show that Pdim H  = d.  11.3 Linear regression.  is invertible?   a  What condition is required on the data X in order to guarantee that XX cid:62    b  Assume the problem is under-determined. Then, we can choose a solution w such that the equality X cid:62 w = X cid:62  XX cid:62  †Xy  which can be shown to equal X†Xy  holds. One particular choice that satisﬁes this equality is w∗ =  XX cid:62  †Xy. However, this is not the unique solution. As a function of w∗, characterize all choices of w that satisfy X cid:62 w = X†Xy  Hint: use the fact that XX†X = X .  11.4 Perturbed kernels. Suppose two diﬀerent kernel matrices, K and K cid:48 , are used to train two kernel ridge regression hypothesis with the same regularization parameter λ. In this problem, we will show that the diﬀerence in the optimal dual variables, α and α cid:48  respectively, is bounded by a quantity that depends on  cid:107 K cid:48  − K cid:107 2.  a  Show α cid:48  − α = cid:0  K cid:48  + λI −1 K cid:48  − K  K + λI −1 cid:1 y.  Hint: Show that for any invertible matrix M, M cid:48 1 − M1 = −M cid:48 −1 M cid:48  − M M−1.    b  Assuming ∀y ∈ Y,y ≤ M , show that   cid:107 α cid:48  − α cid:107  ≤  √mM cid:107 K cid:48  − K cid:107 2  .  λ2  11.5 Huber loss. Derive the primal and dual optimization problem used to solve the  SVR problem with the Huber loss:  Lc ξi  = cid:40  1  2 ξ2 i , cξi − 1  2 c2,  if ξi ≤ c otherwise  ,  where ξi = w · Φ xi  + b − yi.   11.5 Exercises  293  OnLineLasso w+  0  1 ← w+ 1 w+ 2 w−1 ← w−0 3  0 , w−0    cid:46  w+ 0 ≥ 0  cid:46  w−0 ≥ 0  for t ← 1 to T do  Receive xt, yt  for j ← 1 to N do  4  5  6  7  8  Figure 11.9 On-line algorithm for Lasso.  w+  t+1j ← max cid:16 0, w+ tj − η cid:104 λ − cid:2 yt −  w+ w−t+1j ← max cid:16 0, w−tj − η cid:104 λ + cid:2 yt −  w+ T +1 − w−T +1  t − w−t   · xt cid:3 xtj cid:105  cid:17  t − w−t   · xt cid:3 xtj cid:105  cid:17   return w+  11.6 SVR and squared loss. Assuming that 2rΛ ≤ 1, use theorem 11.13 to derive a  generalization bound for the squared loss.  11.7 SVR dual formulations. Give a detailed and carefully justiﬁed derivation of the dual formulations of the SVR algorithm both for the  cid:15 -insensitive loss and the quadratic  cid:15 -insensitive loss.  11.8 Optimal kernel matrix. Suppose in addition to optimizing the dual variables  α ∈ Rm, as in  11.16 , we also wish to optimize over the entries of the PDS kernel matrix K ∈ Rm×m.  min K cid:23 0  α −λα cid:62 α − α cid:62 Kα + 2α cid:62 y , max  s.t.  cid:107 K cid:107 2 ≤ 1   a  What is the closed-form solution for the optimal K for the joint optimization?   b  Optimizing over the choice of kernel matrix will provide a better value of the objective function. Explain, however, why the resulting kernel matrix is not useful in practice.  11.9 Leave-one-out error. In general, the computation of the leave-one-out error can be very costly since, for a sample of size m, it requires training the algorithm m times. The objective of this problem is to show that, remarkably, in the case of   294  Chapter 11 Regression  kernel ridge regression, the leave-one-out error can be computed eﬃciently by training the algorithm only once.  Let S =   x1, y1 , . . . ,  xm, ym   denote a training sample of size m and for any i ∈ [m], let Si denote the sample of size m − 1 obtained from S by removing  xi, yi : Si = S − { xi, yi }. For any sample T , let hT denote a hypothesis obtained by training T . By deﬁnition  see deﬁnition 5.2 , for the squared loss, the leave-one-out error with respect to S is deﬁned by   cid:98 RLOO KRR  =  1 m  m cid:88 i=1   hSi xi  − yi 2 .   a  Let S cid:48 i =   x1, y1 , . . . ,  xi, hSi  yi  , . . . ,  xm, ym  . Show that hSi = hS cid:48 i.  b  Deﬁne yi = y − yiei + hSi xi ei, that is the vector of labels with the ith component replaced with hSi xi . Prove that for KRR hSi xi  = y cid:62 i  K + λI −1Kei.   c  Prove that the leave-one-out error admits the following simple expression in  terms of hS:   cid:98 RLOO KRR  =  1 m  m cid:88 i=1 cid:20   hS xi  − yi  e cid:62 i  K + λI −1Kei cid:21 2  .   11.34    d  Suppose that the diagonal entries of matrix M =  K + λI −1K are all equal  to γ. How do the empirical error  cid:98 RS of the algorithm and the leave-one-out error  cid:98 RLOO relate? Is there any value of γ for which the two errors coincide?  11.10 On-line Lasso. Use the formulation  11.33  of the optimization problem of Lasso and stochastic gradient descent  see section 8.3.1  to show that the problem can be solved using the on-line algorithm of ﬁgure 11.9.  11.11 On-line quadratic SVR. Derive an on-line algorithm for the quadratic SVR  algorithm  provide the full pseudocode .   12 Maximum Entropy Models  In this chapter, we introduce and discuss maximum entropy models, also known as Maxent models, a widely used family of algorithms for density estimation that can exploit rich feature sets. We ﬁrst introduce the standard density estimation problem and brieﬂy describe the Maximum Likelihood and Maximum a Posteriori solutions. Next, we describe a richer density estimation problem where the learner additionally has access to features. This is the problem addressed by Maxent models.  We introduce the key principle behind Maxent models and formulate their pri- mal optimization problem. Next, we prove a duality theorem showing that Maxent models coincide with Gibbs distribution solutions of a regularized Maximum Like- lihood problem. We present generalization guarantees for these models and also give an algorithm for solving their dual optimization problem using a coordinate descent technique. We further extend these models to the case where an arbitrary Bregman divergence is used with other norms, and prove a general duality theorem leading to an equivalent optimization problem with alternative regularizations. We also give a speciﬁc theoretical analysis of Maxent models with L2-regularization, which are commonly used in applications.  12.1 Density estimation problem  Let S =  x1, . . . , xm  be a sample of size m drawn i.i.d. from an unknown distri- bution D. Then, the density estimation problem consists of using that sample to select out of a family of possible distributions P a distribution p that is close to D. The choice of the family P is critical. A relatively small family may not contain D or even any distribution close to D. On the other hand, a very rich family deﬁned by a large set of parameters may make the task of selecting p very diﬃcult if only a sample of a relatively modest size m is available.   296  Chapter 12 Maximum Entropy Models  12.1.1 Maximum Likelihood  ML  solution One common solution adopted for selecting a distribution p is based on the maxi- mum likelihood principle. This consists of choosing a distribution out of the family P that assigns the largest probability to the sample S observed. Thus, using the fact that the sample is drawn i.i.d., the solution pML selected by maximum likelihood is deﬁned by  pML = argmax  p xi  = argmax  log p xi .   12.1   m cid:89 i=1  p∈P  m cid:88 i=1  p∈P  The maximum likelihood principle can be equivalently formulated in terms of the  sample S. Then, pML coincides with the distribution p with respect to which the  relative entropy. Let  cid:98 D denote the empirical distribution corresponding to the empirical distribution  cid:98 D admits the smallest relative entropy: D  cid:98 D cid:107  p  = cid:88 x  cid:98 D x  log cid:98 D x  − cid:88 x  cid:98 D x  log p x   This can be seen straightforwardly from the following:  D  cid:98 D cid:107  p .  pML = argmin   12.2   p∈P  i=1 1x=xi  log p x   log p x   = −H  cid:98 D  − cid:88 x  cid:80 m m cid:88 i=1 cid:88 x = −H  cid:98 D  − m cid:88 i=1 = −H  cid:98 D  −  m  1x=xi  m  log p xi   ,  m  since the ﬁrst term of the last expression, the negative entropy of the empirical distribution, does not vary with p.  As an example of application of the maximum likelihood principle, suppose we wish to estimate the bias p0 of a coin from an i.i.d. sample S =  x1, . . . , xm  where xi ∈ {h, t} with h denoting heads and t tails. p0 ∈ [0, 1] is the probability of h according to the unknown distribution D. Let P be the family of all distributions p =  p, 1 − p  where p ∈ [0, 1] is an arbitrary possible bias value. Let nh denote  the number of occurrences of h in S. Then, choosing p =   cid:98 pS, 1 − cid:98 pS  =  cid:98 D where m leads to D  cid:98 D cid:107  p  = 0, which, by  12.2 , shows that pML =  cid:98 D. Thus, the  cid:98 pS = nh  maximum likelihood estimate pML of the bias is the empirical value   12.3   pML =  nh m  .   12.2 Density estimation problem augmented with features  297  12.1.2 Maximum a Posteriori  MAP  solution An alternative solution based on the so-called Maximum a Posteriori solution con- sists of selecting a distribution p ∈ P that is the most likely, given the observed sample S and a prior P[p] over the distributions p ∈ P. By the Bayes rule, the  problem can be formulated as follows:  pMAP = argmax  P[pS] = argmax p∈P  p∈P  P[Sp] P[p]  P[S]  = argmax  p∈P  P[Sp] P[p].   12.4   Notice that, for a uniform prior, P[p] is a constant and the Maximum a Posteriori solution then coincides with the Maximum Likelihood solution. The following is a standard example illustrating the MAP solution and its diﬀerence with the ML solution.  Example 12.1  Application of the MAP solution  Suppose we need to determine if a pa- tient has a rare disease, given a laboratory test of that patient. We consider a set of two simple distributions: d  disease with probability one  and ¯d  no disease with probability one , thus P = {d, ¯d}. The laboratory test is either pos  positive  or neg  negative , thus S ∈ {pos, neg}. Suppose that the disease is rare, say P[d] = .005 and that the laboratory is relatively accurate: P[posd] = .98, and P[neg¯d] = .95. Then, if the test is positive,  what should be the diagnosis? We can compute the right-hand side of  12.4  for both outcomes, given the positive test result, to determine the MAP estimate:  P[posd] P[d] = .98 × .005 = .0049 P[pos¯d] P[¯d] =  1 − .95  ×  1 − .005  = .04975 > .0049.  Thus, in this case, the MAP prediction is no disease: according to the MAP solution, with the values indicated, a patient with a positive test result is nonetheless more likely not to have the disease!  We will not analyze the properties of the Maximum Likelihood and Maximum a Posteriori solutions here, which depend on the size of the sample and the choice of the family P. Instead, we will consider a richer density estimation problem where the learner has access to features, which is the learning problem addressed by Maximum Entropy  Maxent  models.  12.2 Density estimation problem augmented with features  As with the standard density estimation problem, we consider a scenario where the learner receives a sample S =  x1, . . . , xm  ⊆ X of size m drawn i.i.d. according to some distribution D. But, here, additionally, we assume that the learner has access to a feature mapping Φ from X to RN with  cid:107 Φ cid:107 ∞ ≤ r. In the most general case,   298  Chapter 12 Maximum Entropy Models  we may have N = +∞. We will denote by H a family of real-valued functions containing the component feature functions Φj with j ∈ [N ]. Diﬀerent feature functions can be considered in practice. H may be the family of threshold functions x  cid:55 → 1xi≤θ, x ∈ Rn, θ ∈ R, deﬁned over n variables as for boosting stumps, or it  may be a family of functions deﬁned by more complex decision trees or regression trees. Other features often used in practice are monomials of degree k based on the input variables. To simplify the presentation, in what follows, we will assume that the input set X is ﬁnite.  12.3 Maxent principle  Maxent models are derived from a principle based on the key property that, with high probability, the empirical average of any feature is close to its true average. By the Rademacher complexity bound, for any δ > 0, the following inequality holds with probability at least 1 − δ over the choice of a sample S of size m:   cid:13  cid:13  cid:13  E  x∼D  [Φ x ] cid:13  cid:13  cid:13 ∞ ≤ 2Rm H  + r cid:114  log 2 x∼ cid:98 D [Φ x ] − E  δ 2m  ,  where we denote by  cid:98 D the empirical distribution deﬁned by the sample S. This is  the theoretical guarantee that guides the deﬁnition of the Maxent principle. Let p0 be a distribution over X with p0 x  > 0 for all x ∈ X, which is often chosen to be the uniform distribution. Then, the Maxent principle consists of seeking a distribution p that is as agnostic as possible, that is as close as possible to the uniform distribution or, more generally, to a prior p0, while verifying an inequality similar to  12.5 :   12.5    12.6    cid:13  cid:13  cid:13  E  x∼p  [Φ x ] cid:13  cid:13  cid:13 ∞ ≤ λ, x∼ cid:98 D [Φ x ] − E  where λ ≥ 0 is a parameter. Here, closeness is measured using the relative entropy. Choosing λ = 0 corresponds to standard Maxent or unregularized Maxent and to requiring the expectation of the features with respect to p to precisely match the empirical averages. As we will see later, its relaxation, that is the inequality case  λ  cid:54 = 0 , translates into a regularization. Notice that, unlike Maximum likelihood, the Maxent principle does not require specifying a family of probability distributions P to choose from.   299   12.7   12.4 Maxent models  12.4 Maxent models  Let ∆ denote the simplex of all distributions over X, then, the Maxent principle can be formulated as the following optimization problem:  min p∈∆  D p cid:107  p0   subject to: cid:13  cid:13  cid:13  E  x∼p  [Φ x ] cid:13  cid:13  cid:13 ∞ ≤ λ. x∼ cid:98 D [Φ x ] − E  This deﬁnes a convex optimization problem since the relative entropy D is con- vex with respect to its arguments  appendix E , since the constraints are aﬃne, and since ∆ is a convex set. The solution is in fact unique since the relative en- tropy is strictly convex. The empirical distribution is clearly a feasible point, thus problem  12.7  is feasible.  For a uniform prior p0, problem  12.7  can be equivalently formulated as an entropy maximization, which explains the name given to these models. Let H p  =  − cid:80 x∈X p x  log p x  denote the entropy of p. Then, the objective function of  12.7   can be rewritten as follows:  D p cid:107  p0  = cid:88 x∈X = − cid:88 x∈X  p x  log  p x  p0 x   p x  log p0 x  + cid:88 x∈X  = log X − H p .  p x  log p x   Thus, since log X is a constant, minimizing the relative entropy D p cid:107  p0  is then equivalent to maximizing H p .  Maxent models are the solutions of the optimization problem just described. As already discussed, they admit two important beneﬁts: they are based on a fun- damental theoretical guarantee of closeness of empirical and true feature averages, and they do not require specifying a particular family of distributions P. In the next sections, we will further analyze the properties of Maxent models.  12.5 Dual problem  Here, we derive an equivalent dual problem for  12.7  which, as we will show, can be formulated as a regularized maximum likelihood problem over the family of Gibbs distributions. For any convex set K, let IK denote the function deﬁned by IK x  = 0 if x ∈ K, IK x  = +∞ otherwise. Then, the Maxent optimization problem  12.7  can be equivalently expressed as the unconstrained optimization problem minp F  p  with,   300  Chapter 12 Maximum Entropy Models  for all p ∈ RX, F  p  = cid:101 D p cid:107  p0  + IC E with  cid:101 D p cid:107  p0  = D p cid:107  p0  if p is in the simplex ∆,  cid:101 D p cid:107  p0  = +∞ otherwise, and  x,y ∼ cid:98 D[Φ x, y ] cid:107 ∞ ≤ λ}. with C ⊆ RN the convex set deﬁned by C = {u :  cid:107 u − E  The general form of a Gibbs distribution pw with prior p0, parameter w, and   12.8   [Φ] ,  p  feature vector Φ is  pw[x] =  p0[x]ew·Φ x   Z w    12.9    12.10   where Z w  = cid:80 x∈X p0[x]ew·Φ x  is a normalization factor also known as the par- tition function. Let G be the function deﬁned for all w ∈ RN by  G w  =  1 m  m cid:88 i=1  log cid:20  pw[xi]  p0[xi] cid:21  − λ cid:107 w cid:107 1.  Then, the following theorem shows the equivalence of the primal problem  12.7  or  12.8  and a dual problem based on G.  Theorem 12.2  Maxent duality  Problems  12.7  or  12.8  are equivalent to the opti- mization problem supw∈RN G w : sup w∈RN  G w  = min   12.11   F  p .  p  Furthermore, let p∗ = argminp F  p  and d∗ = supw∈RN G w , then, for any  cid:15  > 0 and any w such that G w −d∗ <  cid:15 , the following inequality holds: D p∗  cid:107  pw  ≤  cid:15 . Proof: The ﬁrst part of the proof follows by application of the Fenchel duality theorem  theorem B.39  to the optimization problem  12.8  with the functions f ,  g, and A deﬁned for all p ∈ RX and u ∈ RN by f  p  = cid:101 D p cid:107  p0 , g u  = IC u  and Ap = cid:80 x∈X p x Φ x . A is a bounded linear map since for any p, we have  cid:107 Ap cid:107  ≤  cid:107 p cid:107 1 supx  cid:107 Φ x  cid:107 ∞ ≤ r cid:107 p cid:107 1. Also, notice that for all w ∈ RN , A∗w = w · Φ. x∼ cid:98 D[Φ x ] = A cid:98 D. Since cid:98 D is in ∆ = dom f  , Consider u0 ∈ RN deﬁned by u0 = E u0 is in A dom f   . Furthermore, since λ > 0, u0 is in int C . g = IC equals zero over int C  and is therefore continuous over int C , thus g is continuous at u0 and we have u0 ∈ A dom f    ∩ cont g . Thus, the assumptions of Theorem B.39 hold. By Lemma B.37, the conjugate of f is the function f∗ : RX → R deﬁned by f∗ q  = log cid:0  cid:80 x∈X p0[x]eq[x] cid:1  for all q ∈ RX. The conjugate function of g = IC is   12.5 Dual problem  301  the function g∗ deﬁned for all w ∈ RN by  g∗ w  = sup   w · u   =  sup  u∈C  u  cid:0 w · u − IC u  cid:1  = sup  cid:107 u−E cid:99 D[Φ] cid:107 ∞≤λ = w · E cid:98 D = E cid:98 D   cid:107 u cid:107 ∞≤λ [w · Φ] + λ cid:107 w cid:107 1,   w · u    w · u   [Φ] + sup  where the last equality holds by deﬁnition of the dual norm. identities, we can write  In view of these  −f∗ A∗w  − g∗ −w  = − log cid:0  cid:88 x∈X  p0[x]ew·Φ x  cid:1  + E cid:98 D  [w · Φ] − λ cid:107 w cid:107 1  = − log Z w  +  w · Φ xi  − λ cid:107 w cid:107 1  1 m  m cid:88 i=1  =  =  log  1 m  1 m  pw[x]  log cid:20  pw[xi]  ew·Φ xi  Z w  − λ cid:107 w cid:107 1  p0[xi] cid:21  − λ cid:107 w cid:107 1 = G w ,  m cid:88 i=1 m cid:88 i=1 which proves that supw∈RN G w  = minp F  p . Now, for any w ∈ RN , we can write G w  − D p∗  cid:107  p0  + D p∗  cid:107  pw  p0[x] cid:21  + E x∼ cid:98 D cid:20 log p0[x] cid:21  − λ cid:107 w cid:107 1 − E x∼p∗ cid:20 log = E p0 x  cid:21  x∼p∗ cid:20 log p0[x] cid:21  − E x∼ cid:98 D cid:20 log = −λ cid:107 w cid:107 1 + E x∼ cid:98 D [w · Φ x  − log Z w ] − E = −λ cid:107 w cid:107 1 + E x∼p∗ = −λ cid:107 w cid:107 1 + w · cid:104  E [Φ x ] cid:105 . x∼ cid:98 D [Φ x ] − E x∼p∗ The solution of the primal optimization, p∗, veriﬁes the constraint IC Ep∗ [Φ]  = 0, x∼ cid:98 D[Φ x ] − Ex∼p∗ [Φ x ] cid:107 ∞ ≤ λ. By H¨older’s inequality, this implies that is  cid:107  E −λ cid:107 w cid:107 1 + w · cid:104  E x∼ cid:98 D  [Φ x ] cid:105  ≤ −λ cid:107 w cid:107 1 + λ cid:107 w cid:107 1 = 0.  [w · Φ x  − log Z w ]  [Φ x ] − E x∼p∗  x∼p∗ cid:20 log  pw[x] cid:21   the following inequality:  pw x   p∗[x]  p∗[x]  pw[x]   302  Chapter 12 Maximum Entropy Models  Thus, we can write, for any w ∈ RN ,  D p∗  cid:107  pw  ≤ D p∗  cid:107  p0  − G w .  Now, assume that w veriﬁes G w  − supw∈RN G w  ≤  cid:15  for some  cid:15  > 0. Then, D p∗  cid:107  p0  − G w  =  supw G w   − G w  ≤  cid:15  implies D p∗  cid:107  pw  ≤  cid:15 . This con-  cid:3  cludes the proof of the theorem.  In view of the theorem, if w is an  cid:15 -solution of the dual optimization problem, then D p∗  cid:107  pw  ≤  cid:15 , which, by Pinsker’s inequality  Proposition E.7  implies that pw is √2 cid:15 -close in L1-norm to the optimal solution of the primal:  cid:107 p∗ − pw cid:107 1 ≤ √2 cid:15 . Thus, the solution of our Maxent problem can be determined by solving the dual problem, which can be written equivalently as follows:  inf w∈RN  λ cid:107 w cid:107 1 −  1 m  m cid:88 i=1  log pw[xi].   12.12   Notice that the solution may not be achieved for any ﬁnite w for λ = 0, which is why the inﬁmum is needed. This result may seem surprising since it shows that Maxent coincides with Maximum Likelihood  λ = 0  or regularized Maximum Likelihood  λ > 0  over a speciﬁc family P of distributions, that of Gibbs distributions, while, as pointed out earlier, the Maxent principle does not explicitly specify any family P. What can then explain that the solution of Maxent belongs to the speciﬁc family of Gibbs distributions? The reason is the speciﬁc choice of the relative entropy as the measure of closeness of p to the prior distribution p0. Other measures of closeness between distributions lead to diﬀerent forms for the solution. Thus, in some sense, the choice of the measure of closeness is the  dual  counterpart of that of the family of distributions P in maximum likelihood. Gibbs distributions form a very rich family. In particular, when X is a subset of a vector space and the features Φj x  associated to x =  x1, . . . , xn  ∈ X are monomials of degree at most 2 based on the input variables xj, that is xjxk, xj, or the constant a ∈ R, then w · Φ x  is a quadratic form as a function of the  xjs. Thus, Gibbs distributions include the family of distributions deﬁned by the normalized exponential of a quadratic form, which includes as a special case Gaus- sian distributions but also bi-modal distributions and normalized exponentials of non-positive deﬁnite quadratic forms. More complex multi-modal distributions can be further deﬁned using higher-order monomials or more complex functions of the input variables. Figure 12.1 shows two examples of Gibbs distributions illustrating the richness of this family.   12.6 Generalization bound  303   a    b   Figure 12.1 Examples of Gibbs distributions in R2. 1+x2 e− x2 2  Z malization factor.  ;  b  Bimodal distribution p[ x1, x2 ] = e− x4   a  Unimodal Gaussian distribution p[ x1, x2 ] =  2 +x2 1+x4 Z  1−x2  2  . In each case, Z is a nor-  12.6 Generalization bound  δ  the empirical distribution deﬁned by a sample S.  Let LD w  denote the log-loss of the distribution pw with respect to a distribution D, LD w  = Ex∼D[− log pw[x]], and similarly LS w  its log-loss with respect to Theorem 12.3 Fix δ > 0. Let  cid:98 w be a solution of the optimization  12.12  for λ = 2Rm H  + r cid:113  log 2  2m . Then, with probability at least 1 − δ over the draw of an i.i.d.  sample S of size m from D, the following inequality holds:  w LD w  + 2 cid:107 w cid:107 1 cid:34 2Rm H  + r cid:114  log 2 2m  cid:35 .  Proof: Using the deﬁnition of LD w  and LS w , H¨older’s inequality, and inequal- ity  12.5 , with probability at least 1 − δ, the following holds: [Φ] − E  LD  cid:98 w  ≤ inf LD  cid:98 w  − LS  cid:98 w  =  cid:98 w · [E cid:98 D [Φ]] ≤  cid:107  cid:98 w cid:107 1  cid:107  E cid:98 D Thus, since  cid:98 w is a minimizer, we can write, for any w, ≤ λ cid:107  cid:98 w cid:107 1 + LS  cid:98 w  − LD w   LD  cid:98 w  − LD w  = LD  cid:98 w  − LS  cid:98 w  + LS  cid:98 w  − LD w   ≤ λ cid:107 w cid:107 1 + LS w  − LD w  ≤ 2λ cid:107 w cid:107 1,  [Φ] cid:107 ∞ ≤ λ cid:107  cid:98 w cid:107 1.  [Φ] − E  D  D  δ   304  Chapter 12 Maximum Entropy Models  where we used for the last inequality the left inequality counterpart of inequality  cid:3   12.5 . This concludes the proof. Assume that w∗ achieves the inﬁmum of the loss, that is LD w∗  = inf w LD w  and that Rm H  =  1 √m . Then, the theorem shows that, with high probability, the following inequality holds:  w LD w  + O cid:18  cid:107 w∗ cid:107 1√m  cid:19 .  LD  cid:98 w  ≤ inf  12.7 Coordinate descent algorithm  The dual objective function in the optimization  12.12  is convex since the Lagrange dual is always concave  appendix B . Ignoring the constant term − 1 i=1 log p0[xi], the optimization problem  12.12  can be rewritten as inf w J w  with p0[x]ew·Φ x  cid:35  .  J w  = λ cid:107 w cid:107 1 − w · E cid:98 D  m cid:80 m  [Φ] + log cid:34  cid:88 x∈X  Note in particular that the function w  cid:55 → log cid:2  cid:80 x∈X p0[x]ew·Φ x  cid:3  is convex as the  conjugate function f∗ of the function f deﬁned in the proof of Theorem 12.2.  Diﬀerent optimization techniques can be used to solve this convex optimization problem, including standard stochastic gradient descent and several special-purpose techniques. In this section, we will describe a solution based on coordinate descent which is particularly advantageous in presence of a very large number of features. Function J is not diﬀerentiable but since it is convex, it admits a subdiﬀerential at any point. The Maxent algorithm we describe consists of applying coordinate descent to the objective function  12.12 . Direction Let wt−1 denote the weight vector deﬁned after  t − 1  iterations. At each iteration t ∈ [T ], the direction ej, j ∈ [N ] considered by coordinate descent is δJ wt−1, ej . If wt−1,j  cid:54 = 0, then J admits a directional derivative along ej given by  where  cid:15 t−1,j = Epwt−1  derivatives along ej:  J cid:48  wt−1, ej  = λ sgn wt−1,j  +  cid:15 t−1,j. [Φj]−E cid:98 D[Φj]. If wt−1,j = 0, J admits right and left directional  J cid:48 + wt−1, ej  = λ +  cid:15 t−1,j  J cid:48  − wt−1, ej  = −λ +  cid:15 t−1,j.   12.7 Coordinate descent algorithm  305  CDMaxent S =  x1, . . . , xm    for t ← 1 to T do  for j ← 1 to N do  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  if  wt−1,j  cid:54 = 0  then  dj ← λ sgn wt−1,j  +  cid:15 t−1,j  elseif  cid:15 t−1,j ≤ λ then  dj ← 0  else dj ← −λ sgn  cid:15 t−1,j  +  cid:15 t−1,j  dj  j ← argmax j∈[N ] if  wt−1,jr2 −  cid:15 t−1,j ≤ λ  then  η ← −wt−1,j  elseif  wt−1,jr2 −  cid:15 t−1,j > λ  then  η ← 1 r2 [−λ −  cid:15 t−1,j] else η ← 1 r2 [λ −  cid:15 t−1,j] wt ← wt−1 + ηej  cid:80  pwt ← p0[x]ewt·Φ x   x∈X p0[x]ewt·Φ x   16 return pwt  Figure 12.2  Pseudocode of the Coordinate Descent Maxent algorithm. For all j ∈ [N ],  cid:15 t−1,j = Epwt−1 E cid:98 D  [Φj ].  [Φj ] −  Thus, in summary, we can deﬁne, for all j ∈ [N ], λ sgn wt−1,j  +  cid:15 t−1,j 0 −λ sgn  cid:15 t−1,j  +  cid:15 t−1,j  δJ wt−1, ej  =  if  wt−1,j  cid:54 = 0   else if cid:12  cid:12  cid:15 t−1,j cid:12  cid:12  ≤ λ  otherwise.  The coordinate descent algorithm selects the direction ej with the largest absolute value of δJ wt−1, ej . Step size Given the direction ej, the optimal step value η is given by argminη J wt−1 + η ej . η can be found via a line search or other numerical methods. A closed-form expression for the step can also be derived by minimizing an upper   306  Chapter 12 Maximum Entropy Models  bound on J wt−1 + η ej . Notice that we can write J wt−1 +η ej −J wt−1  = λ wj +η−wj −η E In view of Φj ∈ [−r, +r], by Hoeﬀding’s lemma, the following inequality holds:  [Φj]+log cid:20  E  [eηΦj ] cid:21  .  12.13   pwt−1  S  log E pwt−1  [eηΦj ] ≤ η E pwt−1  [Φj] +  η2r2  .  2  Combining this inequality with  12.13  and disregarding constant terms, minimizing the resulting upper bound on J wt−1 + η ej  − J wt−1  becomes equivalent to minimizing ϕ η  deﬁned for all η ∈ R by  ϕ η  = λwj + η + η cid:15 t−1,j +  η2r2  .  2  Let η∗ denote the minimizer of ϕ η . If wt−1,j + η∗ = 0, then the subdiﬀerential of wt−1,j +η at η∗ is the set {ν : ν ∈ [−1, +1]}. Thus, in that case, the subdiﬀerential ∂ϕ η∗  contains 0 iﬀ there exists ν ∈ [−1, +1] such that  λν +  cid:15 t−1,j + η∗r2 = 0 ⇔ wt−1,jr2 −  cid:15 t−1,j = λν.  The condition is therefore equivalent to wt−1,jr2 −  cid:15 t−1,j ≤ λ. If wt−1,j + η∗ > 0, then ϕ is diﬀerentiable at η∗ and ϕ cid:48  η∗  = 0, that is  λ +  cid:15 t−1,j + η∗r2 = 0 ⇔ η∗ =  1 r2 [−λ −  cid:15 t−1,j].  In view of that expression, the condition wt−1,j + η∗ > 0 is equivalent to wt−1,jr2 −  cid:15 t−1,j > λ. Similarly, if wt−1,j + η∗ < 0, ϕ is diﬀerentiable at η∗ and ϕ cid:48  η∗  = 0, which gives  η∗ =  1 r2 [λ −  cid:15 t−1,j].  Figure 12.2 shows the pseudocode of the Coordinate Descent Maxent algorithm using the closed-form solution for the step size just presented. Note that we do not need to update distribution pwt at every iteration of the algorithm  line 15  and we only need to be able to compute Epwt [Φj] which deﬁnes  cid:15 t,j. Various approximation strategies can be used to do this eﬃciently, including for instance rejection sampling techniques.  12.8 Extensions  As already pointed out, the Gibbs distribution form of the Maxent models is tightly related to the choice of the divergence  relative entropy  used to measure closeness in the Maxent principle. For distributions, the relative entropy coincides with the   12.8 Extensions  307  unnormalized relative entropy, which is a Bregman divergence. Maxent models can be generalized by using an arbitrary Bregman divergence BΨ instead  appendix E , where Ψ is a convex function. Moreover, other norms  cid:107  ·  cid:107  can be used to bound the diﬀerence of the empirical and true average feature vectors. This leads to the following general primal optimization problem for Maxent models:  min p∈∆  BΨ p cid:107  p0   subject to: cid:13  cid:13  cid:13  E  x∼p  [Φ x ] cid:13  cid:13  cid:13  ≤ λ, x∼ cid:98 D [Φ x ] − E   12.14   which, as with  12.7 , is a convex optimization problem since BΨ is convex with respect to its ﬁrst argument and in fact strictly convex if Ψ is strictly convex. The following general duality theorem gives the form of the dual problem equivalent to  12.14  in terms of the conjugate function Ψ∗ of Ψ. Here,  cid:107 · cid:107  is an arbitrary norm over RN and  cid:107  ·  cid:107 ∗ its conjugate. We will assume here that supx  cid:107 Φ x  cid:107  ≤ r. Theorem 12.4 Let Ψ be a convex function deﬁned over RX. Then, problem  12.14  admits the following equivalent dual:  min p∈∆  BΨ p cid:107  p0   subject to: cid:13  cid:13  cid:13  E [Φ x ] cid:13  cid:13  cid:13  ≤ λ x∼ cid:98 D [Φ x ] − E w∈RN −Ψ∗ cid:0 w · Φ + ∇Ψ p0  cid:1  + w · E x∼ cid:98 D  x∼p  = sup  [Φ x ] − λ cid:107 w cid:107 ∗ − C p0 ,  where C p0  = Ψ p0  −  cid:104 ∇Ψ p0 , p0 cid:105 . Proof: The proof is similar to that of Theorem 12.2 and follows by application of the Fenchel duality theorem  Theorem B.39  to the following optimization problem:  min  f  p  + g Ap ,  p   12.15   with the functions f , g, and A deﬁned for all p ∈ RX and u ∈ RN by f  p  = BΨ p cid:107  p0  + I∆ p , g u  = IC u , and Ap = cid:80 x∈X p x Φ x . Given these deﬁni- tions, problem  12.15  is equivalent to  12.14 . A is a bounded linear map since for any p, we have  cid:107 Ap cid:107  ≤  cid:107 p cid:107 1 supx  cid:107 Φ x  cid:107  ≤ r cid:107 p cid:107 1. Also, notice that for all w ∈ RN , A∗w = w · Φ. x∼ cid:98 D[Φ x ] = A cid:98 D. Since cid:98 D is in ∆ = dom f  , Consider u0 ∈ RN deﬁned by u0 = E  u0 is in A dom f   . Furthermore, since λ > 0, u0 is in int C . g = IC equals zero over int C  and is therefore continuous over int C , thus g is continuous at u0 and we have u0 ∈ A dom f    ∩ cont g . Thus, the assumptions of Theorem B.39 hold.   308  Chapter 12 Maximum Entropy Models  The conjugate function of f is deﬁned for all q ∈ RX by  f∗ q  = sup  = sup  = sup  p  cid:104 p, q cid:105  − BΨ p cid:107  p0  − I∆ p  p∈∆ cid:104 p, q cid:105  − BΨ p cid:107  p0  p∈∆ cid:104 p, q cid:105  − Ψ p  + Ψ p0  +  cid:104 ∇Ψ p0 , p − p0 cid:105  p∈∆ cid:104 p, q + ∇Ψ p0  cid:105  − Ψ p  + Ψ p0  −  cid:104 ∇Ψ p0 , p0 cid:105  = Ψ∗ q + ∇Ψ p0   + Ψ p0  −  cid:104 ∇Ψ p0 , p0 cid:105 . The conjugate function of g = IC is deﬁned for all w ∈ RN by  = sup  g∗ w  = sup  = sup  u  cid:104 w, u cid:105  − IC u  u∈C cid:104 w, u cid:105   cid:107 u−E cid:99 D[Φ] cid:107 ≤λ cid:104 w, u cid:105  =  cid:104 w, E cid:98 D [Φ] cid:105  + sup  sup  =   cid:107 u cid:107 ≤λ cid:104 w, u cid:105  =  cid:104 w, E cid:98 D  [Φ] cid:105  + λ cid:107 w cid:107 ∗,  where the last equality holds by deﬁnition of the dual norm. identities, by Theorem B.39, we have  In view of these  min  f  p  + g Ap  = sup  p  = sup  w∈RN −f∗ A∗w  − g∗ w  w∈RN −Ψ∗ w · Φ + ∇Ψ p0   + w · E cid:98 D − Ψ p0  +  cid:104 ∇Ψ p0 , p0 cid:105 ,  [Φ] − λ cid:107 w cid:107 ∗  which completes the proof.  Note, the previous proof and its use of Fenchel duality holds even when considering norms that are not inner product norms and, more generally, Banach spaces are considered  as mentioned in section B.4 .  Much of the analysis and theoretical guarantees presented in previous sections in the special case of the unnormalized relative entropy straightforwardly extend to a broad family of Bregman divergences.   cid:3   12.9 L2-regularization  In this section, we study a common variant of the Maxent algorithm where a reg- ularization based on the norm-2 squared of the weight vector w is used. Observe that this is not covered by the general framework discussed in the previous section   12.9 L2-regularization  309  where the regularization was based on some norm of w. The corresponding  dual  optimization problem is the following:  min w∈RN  λ cid:107 w cid:107 2  2 −  1 m  m cid:88 i=1  log pw[xi].   12.16   Let LD w  denote the log-loss of the distribution pw with respect to a distribution D, LD w  = Ex∼D[− log pw[x]], and similarly LS w  its log-loss with respect to  the empirical distribution deﬁned by a sample S. Then, the algorithm admits the following guarantee.  Theorem 12.5 Let  cid:98 w be a solution of the optimization problem  12.16 . Then, for  any δ > 0, with probability at least 1 − δ over the draw of an i.i.d. sample S of size m from D, the following inequality holds:  r2  λm cid:18 1 + cid:114 log  1  δ cid:19 2  .  2 +  w LD w  + λ cid:107 w cid:107 2  optimization problem  12.16  can be formulated as follows:  LD  cid:98 w  ≤ inf x∼ cid:98 D cid:2  log pw[x] cid:3  = λ cid:107 w cid:107 2 2 − E  Proof: Let  cid:98 D denote the empirical distribution deﬁned by the sample S. Then, the where Z w  = cid:0  cid:80 x exp w · Φ x   cid:1 . Similarly, let wD denote the solution of the  minimization problem with the distribution D:  x∼ cid:98 D 2 − w · E  [Φ x ] + log Z w ,  λ cid:107 w cid:107 2  min w∈RN  [Φ x ] + log Z w .  min w∈RN  λ cid:107 w cid:107 2  x∼D cid:2  log pw[x] cid:3  = λ cid:107 w cid:107 2 2 − E  2 − w · E x∼D  of LS wD  − LD wD  in terms of average feature values, and ﬁnally the Cauchy- Schwarz inequality and the optimality of wD:  We ﬁrst give an upper-bound on LD  cid:98 w  valid for all w ∈ RN , starting with a decom- position of LD  cid:98 w  as a sum of terms, next using the expression of LD  cid:98 w −LS  cid:98 w  in terms of average feature values, then the optimality of  cid:98 w, next the expression LD  cid:98 w  = LD  cid:98 w  − LS  cid:98 w  + LS  cid:98 w  − LD wD  + LD wD  + λ cid:107  cid:98 w cid:107 2 =  cid:98 w · cid:104  E x∼ cid:98 D [Φ x ] − E x∼D ≤  cid:98 w · cid:104  E x∼ cid:98 D [Φ x ] − E x∼D [Φ x ] cid:105  + LD wD  + λ cid:107 wD cid:107 2 ≤ [ cid:98 w − wD] · cid:104  E x∼ cid:98 D [Φ x ] − E x∼D [Φ x ] cid:13  cid:13  cid:13 2 ≤  cid:107  cid:98 w − wD cid:107 2 cid:13  cid:13  cid:13  E x∼ cid:98 D [Φ x ] − E + LD w  + λ cid:107 w cid:107 2 2. x∼D  2 − λ cid:107  cid:98 w cid:107 2 [Φ x ] cid:105  + LS  cid:98 w  − LD wD  + LD wD  + λ cid:107  cid:98 w cid:107 2 [Φ x ] cid:105  + LS wD  − LD wD  + LD wD  + λ cid:107 wD cid:107 2 2 − λ cid:107  cid:98 w cid:107 2  2  2  2  2 − λ cid:107  cid:98 w cid:107 2 2 − λ cid:107  cid:98 w cid:107 2  2   310  Chapter 12 Maximum Entropy Models  Next, we bound  cid:107  cid:98 w− wD cid:107 2 using the fact that  cid:98 w and wD are solutions of the min-  imization of convex and diﬀerentiable objectives functions, whose gradients must be zero at the minimizing values:  2  which implies  [Φ x ] + ∇ log Z wD  = 0,  [Φ x ] + ∇ log Z wD  − ∇ log Z  cid:98 w .  in view of the convexity of w  cid:55 → log Z w . Using the Cauchy-Schwarz inequality and simplifying, we obtain  x∼ cid:98 D 2λ cid:98 w − E [Φ x ] + ∇ log Z  cid:98 w  = 0 2λwD − E x∼D x∼ cid:98 D [Φ x ] − E 2λ  cid:98 w − wD  = E x∼D Multiplying both sides by   cid:98 w − wD  gives 2λ cid:107  cid:98 w − wD cid:107 2 = cid:104  E [Φ x ] cid:105  · [ cid:98 w − wD] − [∇ log Z  cid:98 w  − ∇ log Z wD ] · [ cid:98 w − wD] x∼ cid:98 D [Φ x ] − E x∼D [Φ x ] cid:105  · [ cid:98 w − wD], ≤ cid:104  E x∼ cid:98 D [Φ x ] − E x∼D  cid:107  cid:98 w − wD cid:107 2 ≤  cid:13  cid:13  cid:13  E Plugging this back in the upper bound previously derived for LD  cid:98 w  yields x∼ cid:98 D[Φ x ] − Ex∼D[Φ x ] cid:13  cid:13  cid:13  LD  cid:98 w  ≤  cid:13  cid:13  cid:13  E x∼ cid:98 D[Φ x ] − Ex∼D[Φ x ] cid:13  cid:13  cid:13 2 We now use McDiarmid’s inequality to bound cid:13  cid:13  cid:13  E [Φ x ] cid:13  cid:13  cid:13 2 cid:12  cid:12  cid:12  cid:12  Ψ S cid:48   − Ψ S  = cid:12  cid:12  cid:12  cid:12  cid:13  cid:13  cid:13  E [Φ x ] cid:13  cid:13  cid:13 2 − cid:13  cid:13  cid:13  E x∼ cid:98 D cid:48  x∼ cid:98 D [Φ x ] − E x∼D [Φ x ] cid:13  cid:13  cid:13 2 ≤ cid:13  cid:13  cid:13  E x∼ cid:98 D x∼ cid:98 D cid:48  [Φ x ] − E  cid:13  cid:13  cid:13 2 ≤ ≤ cid:13  cid:13  cid:13  Φ x cid:48 m  − Φ xm  [Ψ S ] + 2r cid:115  log 1  . Let Ψ S  denote this quantity for a sample S. Let S cid:48  be a sample diﬀering from S by one point, say xm for S, x cid:48 m for S cid:48 . Then, by the triangle inequality, [Φ x ] − E x∼D  Thus, for any δ > 0, with probability at least 1 − δ, the following inequality holds  x∼ cid:98 D[Φ x ] − Ex∼D[Φ x ] cid:13  cid:13  cid:13 2  + LD w  + λ cid:107 w cid:107 2 2.  δ 2m  Ψ S  ≤ E S∼Dm  2r m  2λ  2λ  m  2  2  .  .  .   12.9 L2-regularization  For any i ∈ [m], let Zi denote the random variable E Then, by Jensen’s inequality, ES∼Dm[Ψ S ] can be upper-bounded as follows:  Since the random variables Zis are i.i.d. and centered  E[Zi] = 0 , we have  311  2  1 m  x∼ cid:98 D[Φ xi ] − Ex∼D[Φ x ]. 2 cid:21 . Zi cid:13  cid:13  cid:13  m cid:88 i=1 E[Zi] · E[Zj] cid:21   E  S∼Dm  1 m  E cid:20  cid:13  cid:13  cid:13   [Ψ S ] = E cid:20  cid:13  cid:13  cid:13  2 cid:21  = Zi cid:13  cid:13  cid:13  m cid:88 i=1  1  1 m  Zi cid:13  cid:13  cid:13 2 cid:21  ≤ cid:118  cid:117  cid:117  cid:116 E cid:20  cid:13  cid:13  cid:13  E cid:2  cid:107 Zi cid:107 2 cid:3  + cid:88 i cid:54 =j  m cid:88 i=1 m2 cid:20  m cid:88 i=1 E cid:2  cid:107 Z1 cid:107 2 cid:3 m E cid:2  cid:107 Z1 cid:107 2 +  cid:107 Z2 cid:107 2 cid:3  E cid:2  cid:107 Z1 − Z2 cid:107 2 cid:3   2m  2m  .  =  =  =  where, for the last equality, we used the fact that E[Z1·Z2] = E[Z1]·E[Z2] = 0. This shows that E[Ψ S ] ≤ 2r√2m and that, with probability at least 1 − δ, the following  holds  and therefore also  1  2r  Ψ S  ≤  δ cid:19 ,  √2m cid:18 1 + cid:114 log m  cid:18 1 + cid:114 log δ cid:19 2 λm cid:18 1 + cid:114 log δ cid:19 2 + LD w  + λ cid:107 w cid:107 2 2,  + LD w  + λ cid:107 w cid:107 2  1 2λ r2  2r2  1  1  2  LD  cid:98 w  ≤  ≤   cid:3  which ends the proof. Assume that w∗ achieves the inﬁmum of the loss, that is LD w∗  = inf w LD w  and that we are given an upper bound Λ2 on its norm:  cid:107 w∗ cid:107 2 ≤ Λ2. Then, we can use that upper bound and choose λ to minimize the two terms containing λ: λΛ2 Λ2√m and the theorem would then guarantee the following  λm , that is λ = r  2 = r2  inequality with probability 1 − δ for  cid:98 w:  w LD w  +  LD  cid:98 w  ≤ inf  rΛ2√m cid:34 1 + cid:18 1 + cid:114 log  1  δ cid:19 2 cid:35 .   312  Chapter 12 Maximum Entropy Models  12.10 Chapter notes  The Maxent principle was ﬁrst explicitly advocated by Jaynes [1957]  see also Jaynes [1983]  who referred to Shannon’s notion of entropy  appendix E  to support this principle. As seen in Section 12.5, standard Maxent models coincide with Gibbs distributions, as in the original Boltzmann models in statistical mechanics. In fact, Jaynes [1957] argued that statistical mechanics could be viewed as a form of statistical inference, as opposed to a physical theory, and that the thermodynamic notion of entropy could be replaced by the information-theoretical notion. The justiﬁcation of the Maxent principle presented in this chapter is instead based upon learning theory arguments.  Maximum entropy models, commonly referred to as Maxent models, are used in a variety of tasks in natural language processing [Berger et al., 1996, Rosenfeld, 1996, Pietra et al., 1997, Malouf, 2002, Manning and Klein, 2003, Ratnaparkhi, 2010] and in many other applications, including species habitat modeling [Phillips et al., 2004, 2006, Dud´ık et al., 2007, Elith et al., 2011]. One key beneﬁt of Maxent models is that they allow the use of diverse features that can be selected and augmented by the user. The richness of the features used in many tasks as well as small sample sizes have motivated the use of regularized Maxent models where the L1-norm [Kazama and Tsujii, 2003] or the L2-norm [Chen and Rosenfeld, 2000, Lebanon and Laﬀerty, 2001] of the parameter vector deﬁning the Gibbs distribution is controlled. This can be shown to be equivalent to the introduction of a Laplacian or Gaussian prior over the parameter vectors in a Bayesian interpretation [Williams, 1994, Goodman, 2004], thereby making Maxent models coincide with Maximum a Posteriori solutions with speciﬁc choices of the prior.  An extensive theoretical study of these regularizations and the introduction of other more general ones were given by Dud´ık, Phillips, and Schapire [2007] and by Altun and Smola [2006] who studied the extensions to arbitrary Bregman diver- gences and norms  Section 12.8  using Fenchel duality  see also [Laﬀerty, Pietra, and Pietra, 1997] . Cortes, Kuznetsov, Mohri, and Syed [2015] give a more general family of density estimation models, Structural Maxent models, with feature func- tions selected from a union of possibly very complex sub-families for which they also give a duality theorem, strong learning guarantees, and algorithms. These models can also be viewed as Maxent with a more general type of regularization.  The Maxent duality theorem is due to Pietra, Pietra, and Laﬀerty [1997]  see also [Dud´ık et al., 2007] and [Altun and Smola, 2006] . Theorem 12.2 is a slight extension giving a guarantee for an  cid:15 -solution of the dual and is a special instance of a more general theorem given for Structural Maxent models [Cortes et al., 2015]. The generalization bounds of Sections 12.6 and 12.9 and their proofs are variants   12.11 Exercises  313  of results due to Dud´ık et al. [2007]. The stability analysis used in the proof of Theorem 12.5 is equivalent to the one described in Chapter 14 using Bregman divergences.  A variety of diﬀerent techniques have been suggested to solve the Maxent op- timization problem including standard gradient descent and stochastic gradient descent. Some speciﬁc algorithms were introduced for this problem, including gen- eralized iterative scaling  GIS  [Darroch and Ratcliﬀ, 1972] and improved iterative scaling  IIS  [Pietra et al., 1997]. It was shown by Malouf [2002] that these algo- rithms perform poorly in several natural language processing tasks in comparison with conjugate gradient techniques and limited-memory BFGS methods  see also [Andrew and Gao, 2007] . The coordinate descent solution presented in this chap- ter is due to Cortes et al. [2015]. It is a simpler version of an algorithm of Dud´ık et al. [2007] which uses a tighter upper bound on J wt−1 + η ej  but which is subject to various technical conditions. Both algorithms beneﬁt from a similar asymptotic convergence rate [Cortes et al., 2015] and are particularly adapted to cases where the number of features is very large and where updating all feature weights is impractical. A sequential greedy approximation due to Zhang [2003b] is also advocated by Altun and Smola [2006] as a general algorithm for general forms of the Maxent problem.  12.11 Exercises  12.1 Convexity. Prove directly that the function w  cid:55 → log Z w  = log  cid:80 x∈X ew·Φ x    is convex  Hint: compute its Hessian .  12.2 Lagrange duality. Derive the dual problem of the Maxent problem and justify it carefully in the case of the stricter constraint of positivity for the distribution p: p x  > 0 for all x ∈ X.  12.3 Dual of norm-2 squared regularized Maxent. Derive the dual formulation of the  norm-2 squared regularized Maxent optimization shown in equation  12.16 .  12.4 Extension to Bregman divergences. Derive theoretical guarantees for the ex- tensions discussed in Section 12.8. What additional property is needed for the Bregman divergence so that your learning guarantees hold?  12.5 L2-regularization. Let w be the solution of Maxent with a norm-2 squared  regularization.   314  Chapter 12 Maximum Entropy Models   a  Prove the following inequality:  cid:107 w cid:107 2 ≤ 2r  λ  Hint: you could compare the values of the objective function at w and 0. . Generalize this result to other  cid:107  ·  cid:107 p  p-regularizations with p > 1.   b  Use the previous question to derive an explicit learning guarantee for Maxent with norm-2 squared regularization  Hint: you could use the last inequality given in Section 12.9 and derive an explicit expression for Λ2 .   13 Conditional Maximum Entropy Models  This chapter presents algorithms for estimating the conditional probability of a class given an example, rather than only predicting the class label for that example. This is motivated by several applications where conﬁdence values are sought, in addition to the class prediction. The algorithms discussed, conditional Maxent models, also known as multinomial logistic regression algorithms, are among the most well-known and most widely used multi-class classiﬁcation algorithms. In the special case of two classes, the algorithm is known as logistic regression.  As suggested by their name, these algorithms can be viewed as Maxent models for conditional probabilities. To introduce them, we will extend the ideas discussed in the previous chapter  Chapter 12 , starting from an extension of the Maxent principle to the conditional case. Next, we will prove a duality theorem leading to an equivalent dual optimization problem for conditional Maxent. We will speciﬁcally discuss diﬀerent aspects of multi-class classiﬁcation using conditional Maxent and reserve a special section to the analysis of logistic regression.  13.1 Learning problem  We consider a multi-class classiﬁcation problem with c classes, c ≥ 1. Let Y = {1, . . . , c} denote the output space and D a distribution over X × Y. The learner receives a labeled training sample S =   x1, y1 , . . . ,  xm, ym   ∈  X × Y m drawn i.i.d. according to D. As in Chapter 12, we assume that, additionally, the learner has access to a feature mapping Φ : X×Y → RN with RN a normed vector space and with  cid:107 Φ cid:107 ∞ ≤ r. We will denote by H a family of real-valued functions containing the component feature functions Φj with j ∈ [N ]. Note that in the most general case, we may have N = +∞. The problem consists of using the training sample S to learn an accurate conditional probability p[·x], for any x ∈ X.   316  Chapter 13 Conditional Maximum Entropy Models  13.2 Conditional Maxent principle  As for Maxent models, conditional Maxent or logistic regression models can be derived from a key concentration inequality. By the general Rademacher complexity bound  Theorem 3.3 , for any δ > 0, the following inequality holds with probability at least 1 − δ over the choice of a sample of size m:  ≤ 2Rm H  + cid:115  log 2  δ 2m  ,   13.1    cid:13  cid:13  cid:13  cid:13   E   x,y ∼D   x,y ∼ cid:98 D [Φ x, y ] − E  [Φ x, y ] cid:13  cid:13  cid:13  cid:13 ∞  where we denote by  cid:98 D the empirical distribution deﬁned by the sample S. We will also denote by  cid:98 D1 x  the empirical distribution of x in the sample S. For any  x ∈ X, let p0[·x] denote a conditional probability, often chosen to be the uniform distribution. Then, the conditional Maxent principle consists of seeking conditional probabilities p[·x] that are as agnostic as possible, that is as close as possible to the uniform distribution, or, more generally, to priors p0[·x], while verifying an inequality similar to  13.1 :   cid:13  cid:13  cid:13  cid:13  cid:13   E  x∼ cid:98 D1  y∼p[·x]   x,y ∼ cid:98 D [Φ x, y ] − E  [Φ x, y ] cid:13  cid:13  cid:13  cid:13  cid:13 ∞  ≤ λ,   13.2   entropy  appendix E  based on the empirical marginal distribution  cid:98 D1 of input  where λ ≥ 0 is a parameter. Here, closeness is deﬁned via the conditional relative points. Choosing λ = 0 corresponds to standard conditional Maxent or unregu- larized conditional Maxent and to requiring the expectation of the features based on D1 and the conditional probabilities p[·x] to precisely match the empirical av- erages. As we will see later, its relaxation, that is the inequality case  λ  cid:54 = 0 , translates into a regularization. Notice that the conditional Maxent principle does not require specifying a family of conditional probability distributions P to choose from.  13.3 Conditional Maxent models  Let ∆ denote the simplex of the probability distributions over Y, X1 = supp  cid:98 D1  the support of  cid:98 D1, and ¯p ∈ ∆X1 the family of conditional probabilities, ¯p =  p[·x] x∈X1 .  Then, the conditional Maxent principle can be formulated as the following optimiza-   317   13.3   13.4 Dual problem  tion problem:  min  ¯p∈∆X1  cid:88 x∈X1 cid:98 D1 x  D cid:0 p[·x] cid:107  p0[·x] cid:1  s.t. cid:13  cid:13  cid:13  cid:13  cid:13   x,y ∼ cid:98 D [Φ x, y ] − E  x∼ cid:98 D1  y∼p[·x]  E  [Φ x, y ] cid:13  cid:13  cid:13  cid:13  cid:13 ∞  ≤ λ.  This deﬁnes a convex optimization problem since the objective is a positive sum of relative entropies and since the relative entropy D is convex with respect to its arguments  appendix E , since the constraints are aﬃne functions of ¯p, and since ∆X1 is a convex set. The solution is in fact unique, since the objective is strictly convex as a positive sum of relative entropies, each strictly convex. The empirical  conditional probabilities  cid:98 D1 ·x , x ∈ X1, clearly form a feasible solution, thus problem  12.7  is feasible. For uniform priors p0[·x], problem  13.3  can be equivalently formulated as a conditional entropy maximization, which explains the name given to these models. x∼ cid:98 D1 cid:2  cid:80 y∈Y p[yx] log p[yx] cid:3  denote the conditional entropy of p Let ¯H ¯p  = − E with respect to the marginal  cid:98 D1. Then, the objective function of  12.7  can be  rewritten as follows:  D cid:0 p[·x] cid:107  p0[·x] cid:1  = E  p[yx] log  p[yx]  p0[yx] cid:35   p[yx] log 1 c  + cid:88 y∈Y  p[yx] log p[yx] cid:35   x∼ cid:98 D1 cid:34  cid:88 y∈Y x∼ cid:98 D1 cid:34  − cid:88 y∈Y  = E  = log c  − ¯H ¯p .  Thus, since log c  is a constant, minimizing the objective is then equivalent to maximizing ¯H ¯p .  Conditional Maxent models are the solutions of the optimization problem just described. As in the non-conditional case, they admit two important beneﬁts: they are based on a fundamental theoretical guarantee of closeness of empirical and true feature averages, and they do not require specifying a particular family of distributions P. In the next sections, we will further analyze the properties of conditional Maxent models.  13.4 Dual problem  Here, we derive an equivalent dual problem for  13.3  which, as we will show, can be formulated as a regularized conditional maximum likelihood problem over the family of Gibbs distributions.   318  Chapter 13 Conditional Maximum Entropy Models  The Maxent optimization problem  13.3  can be equivalently expressed as the  unconstrained optimization problem min¯p F  ¯p  with, for all ¯p =  p[·x] ∈  RY X1,  F  ¯p  = E  x∼ cid:98 D1 cid:20  cid:101 D cid:0 p[·x] cid:107  p0[·x] cid:1  cid:21  + IC cid:32  E x∼ cid:98 D1  y∼p[·x]  [Φ x, y ] cid:33 ,   13.4   convex set.  with  cid:101 D cid:0 p[·x] cid:107  p0[·x] cid:1  = D cid:0 p[·x] cid:107  p0[·x] cid:1  if p[·x] is in ∆,  cid:101 D cid:0 p[·x] cid:107  p0[·x] cid:1  = +∞  x,y ∼ cid:98 D[Φ x, y ] cid:107 ∞ ≤ λ}, which is a otherwise, and with C = {u ∈ RN :  cid:107 u − E Let G be the function deﬁned for all w ∈ RN by p0[yixi] cid:21  − λ cid:107 w cid:107 1 , log cid:20  pw[yixi]  G w  =   13.5   1 m  with, for all x ∈ X1 and y ∈ Y, p0[yx]ew·Φ x,y   pw[yx] =  Z w, x   p0[yx]ew·Φ x,y .   13.6   m cid:88 i=1 and Z w, x  = cid:88 y∈Y  Then, the following theorem gives a result similar to the duality theorem presented in the non-conditional case  Theorem 12.2, Section 12.5 .  Theorem 13.1 Problem  13.3  is equivalent supw∈RN G w :  to the dual optimization problem  sup w∈RN  G w  = min ¯p∈ RY X1  F  ¯p .   13.7   Furthermore, let ¯p∗ = argmin¯p F  ¯p . Then, for any  cid:15  > 0 and any w such that G w  − supw∈RN G w  <  cid:15 , we have E  x∼ cid:98 D1 cid:2 D cid:0 ¯p∗[·x] cid:107  p0[·x] cid:1  cid:3  ≤  cid:15 .  The proof is similar to that of Theorem 12.2 and is given at the end of this chapter since it is somewhat longer  Section 13.9 .  E  In view of the theorem, if w is an  cid:15 -solution of the dual optimization problem,  then E inequality  Proposition E.7  implies that  x∼ cid:98 D1 cid:2 D cid:0 p∗[·x] cid:107  p0[·x] cid:1  cid:3  ≤  cid:15 , which, by Jensen’s inequality and Pinsker’s x∼ cid:98 D1 cid:104  cid:13  cid:13 p∗[·x] − pw[·x] cid:13  cid:13 1 cid:105  ≤ cid:114  E  x∼ cid:98 D1 cid:104  cid:13  cid:13 p∗[·x] − pw[·x] cid:13  cid:13 2 1 cid:105  ≤  Thus, pw[·x] is then √2 cid:15 -close in  cid:98 D1-averaged L1-norm to the optimal solution of  the primal and the theorem suggests that the solution of the conditional Maxent problem can be determined by solving the dual problem, which can be written equivalently as follows for a uniform prior:  √2 cid:15 .  inf w  λ cid:107 w cid:107 1 −  1 m  m cid:88 i=1  log cid:2 pw[yixi] cid:3 .   13.8    13.5 Properties  319  Similar remarks to those made for non-conditional Maxent models apply here. In particular, the solution may not be achieved for any ﬁnite w for λ = 0, which is why the inﬁmum is needed. Also, this result may seem surprising since it shows that conditional Maxent coincides with conditional Maximum Likelihood  λ = 0  or regularized conditional Maximum Likelihood  λ > 0  using for the family P of conditional probabilities to choose from that of Gibbs distributions, while the conditional Maxent principle does not explicitly specify any family of conditional probabilities P. The reason is the speciﬁc choice of the conditional relative entropy as the measure of closeness of p[·x] to the prior conditional distributions p0[·x]. Other measures of closeness between distributions lead to diﬀerent forms for the solution. Thus, in some sense, the choice of the measure of closeness is the  dual  counterpart of that of the family of conditional distributions in maximum likelihood. Also, as already mentioned in the standard Maxent case, Gibbs distributions form a very rich family.  Notice that both the primal and the dual optimization problems for conditional Maxent involve only conditional probabilities p[·x] for x in X1, that is for x in the training sample. Thus, they do not provide us with any information about other conditional probabilities. However, the dual shows that, for x in X1, the solution admits the same general form pw[·x], which only depends on the weight vector w. In view of that, we extend the deﬁnition of Maxent conditional probabilities to all x ∈ X by using the same general form pw[·x] and the same vector w for all x. Observe also that in the deﬁnition of the primal or dual problems we could have  used some other distribution Q over X in lieu of  cid:98 D1. It is straightforward to verify  that the duality theorem would continue to hold in that case using the same proof. In fact, ideally, we would have chosen Q to be D1. However, that optimization problem would require knowledge of the feature vectors for all x ∈ supp D1 , which of course is not accessible to us given a ﬁnite sample. The weighted vector w found  when using  cid:98 D1 can be viewed as an approximation of the one obtained if using D1.  13.5 Properties  In this section, we discuss several aspects of conditional Maxent models, including the form of the dual optimization problems, the feature vectors used, and prediction with these models.   320  Chapter 13 Conditional Maximum Entropy Models  1 m  and Z x  = cid:88 y∈Y m cid:88 i=1 log cid:34  cid:88 y∈Y  13.5.1 Optimization problem L1-regularized conditional Maxent models are therefore conditional probability models solutions of the primal problem  13.3  or, equivalently, models deﬁned by  pw[yx] =  ew·Φ x,y   Z x   ew·Φ x,y ,   13.9   where w is solution of the dual problem  min w∈RN  λ cid:107 w cid:107 1 −  log pw[yixi],  with λ ≥ 0 is a parameter. Using the expression of the conditional probabilities, this optimization problem can be written more explicitly as  min w∈RN or, equivalently, as  λ cid:107 w cid:107 1 +  1 m  m cid:88 i=1  min w∈RN  λ cid:107 w cid:107 1 − w ·  1 m  m cid:88 i=1  Φ xi, yi  +  1 m  ew·Φ xi,y −w·Φ xi,yi  cid:35 . log cid:20  cid:88 y∈Y  ew·Φ xi,y  cid:105 .  m cid:88 i=1   13.10    13.11   By deﬁnition of the dual problem, this is an unconstrained convex optimization problem in w. This can be also seen from the fact that the log-sum function  w  cid:55 → log cid:2  cid:80 y∈Y ew·Φ x,y  cid:3  is convex for any x ∈ X.  There are many optimization solutions available for this problem, including sev- eral special-purpose algorithms, general ﬁrst-order and second-order solutions, and special-purpose distributed solutions. One common method is simply to use stochas- tic gradient descent  SGD , which has been reported to be more eﬃcient than most special-purpose methods in applications. When the dimension of the feature vec- tors Φ  or the cardinality of the family of feature functions H  is very large, these methods are typically ineﬃcient. An alternative method then consists of applying coordinate descent to solve this problem. In that case, the resulting algorithm coin- cides with the version of L1-regularized boosting where, instead of the exponential function, the logistic function is used.  13.5.2 Feature vectors Using feature vectors Φ x, y  depending on both the input x and the output y is often important in applications. For example, in machine translation, it is conve- nient to use features whose values may depend on the presence of some words in the input sentence and some others in the output sequence. A common choice of the feature vector is however one where the column vectors Φ x, y  and w admit c   13.6 Generalization bounds  321  blocks of equal size and where only the block in Φ x, y  corresponding to the class y is non-zero and equal to a feature vector Γ x  independent of the class labels:  Φ x, y  =  Γ x   0...  0  0...  0    w =  w1... wy−1 wy wy+1...  wc  .    In view of that, the inner product of w and Φ x, y  can be expressed in terms of the feature vector Γ x , which only depends on x, but with a distinct parameter vector wy:  w · Φ x, y  = wy · Γ x .  The optimization problem for L1-regularized conditional Maxent can then be writ- ten in terms of the vectors wy as follows:  min w∈RN  λ cid:88 y∈Y   cid:107 wy cid:107 1 +  1 m  m cid:88 i=1  log cid:20  cid:88 y∈Y  ewy·Γ xi −wyi·Γ xi  cid:21 .   13.12   Notice that, if the vectors wy were not correlated via the second term of the objec- tive function  for example if, instead of the log of the sum, this term were replaced by the sum of the logs , then the problem would be reduced to c separate optimiza- tion functions learning a distinct weight vector for each class, as in the one-vs-all setup of multi-class classiﬁcation.  13.5.3 Prediction  parameter w is given by  Finally, note that the class  cid:98 y x  predicted by a conditional Maxent model with   13.13   pw[yx] = argmax  w · Φ x, y .  y∈Y  Thus, conditional Maxent models deﬁne linear classiﬁers. Conditional Maxent mod- els are also sometimes referred to as log-linear models.   cid:98 y x  = argmax  y∈Y  13.6 Generalization bounds  In this section, we will present learning guarantees for conditional Maxent models in two diﬀerent settings: one where the dimension of the feature vectors Φ  or the cardinality of the family of feature functions H  is inﬁnite or extremely large and where a coordinate-descent or boosting-type algorithm is more suitable, and another one where the dimension of the feature vectors Φ is ﬁnite and not too large.   322  Chapter 13 Conditional Maximum Entropy Models  We start with the case where the dimension of the feature vectors Φ is very large.  The following margin-based guarantee holds in that case. Theorem 13.2 For any δ > 0, with probability at least 1 − δ over the draw of an i.i.d. sample S of size m, the following holds for all ρ > 0 and f ∈ F = { x, y   cid:55 → w · Φ x, y  :  cid:107 w cid:107 1 ≤ 1}:  R f   ≤  1 m  m cid:88 i=1  logu0 cid:18  cid:88 y∈Y  e  f  xi,y −f  xi,yi   ρ  Rm Π1 H  + cid:115  log log2  m  8c ρ  4r ρ  + cid:115  log 2  δ 2m  ,   cid:19 +  where u0 = log 1 + 1 e  and Π1 H  = {x  cid:55 → φ x, y  : φ ∈ H, y ∈ Y}. Proof: For any f :  x, y   cid:55 → w· Φ x, y  and i ∈ [m], let ρf  xi, yi  denote the margin of f at  xi, yi :  ρf  xi, yi  = min y cid:54 =yi  f  xi, yi  − f  xi, y  = min y cid:54 =yi  w ·  Φ xi, yi  − Φ xi, y  .  Fix ρ > 0. Then, by Theorem 9.2, for any δ > 0, with probability at least 1 − δ, the following inequality holds for all f ∈ H and ρ ∈  0, 2r]:  R f   ≤  1ρf  xi,yi ≤ρ +  1 m  m cid:88 i=1  Rm Π1 F   + cid:115  log log2  m  4c ρ  4r ρ  + cid:115  log 2  δ 2m  ,  where Π1 F  = {x  cid:55 → f  x, y  : y ∈ Y, f ∈ H}. The inequality trivially holds for all ρ > 0 since for ρ ≥ 2r, by H¨older’s inequality, we have w · Φ x, y  ≤  cid:107 w cid:107 1  cid:107 Φ x, y  cid:107 ∞ ≤ r for  cid:107 w cid:107 1 ≤ 1, and thus miny cid:54 =yi f  xi, yi  − f  xi, y  ≤ 2r ≤ ρ for all i ∈ [m] and y ∈ Y. Now, for any ρ > 0, the ρ-margin loss can be upper bounded by the ρ-logistic loss:  ∀u ∈ R, 1u≤ρ = 1 u  ρ −1≤0 ≤ logu0 1 + e− u  ρ  .  Thus, the ρ-margin loss of f at  xi, yi  can be upper bounded as follows:  1ρf  xi,yi ≤ρ ≤ logu0 1 + e− ρ f,xi,yi   ρ     e  = logu0 1 + max y cid:54 =yi  ≤ logu0 cid:18 1 +  cid:88 y cid:54 =yi  e  f  xi,y −f  xi,yi   ρ     f  xi,y −f  xi,yi   ρ   cid:19  = logu0 cid:18  cid:88 y∈Y  e  f  xi,y −f  xi,yi   ρ   cid:19 .  Thus, with probability at least 1 − δ, the following inequality holds for all f ∈ H and ρ > 0:  R f   ≤  1 m  m cid:88 i=1  logu0 cid:18  cid:88 y∈Y  e  f  xi,y −f  xi,yi   ρ  Rm Π1 F  + cid:115  log log2  m  4c ρ  4r ρ  + cid:115  log 2  δ 2m  .   cid:19 +   13.6 Generalization bounds  323  For any sample S =  x1, . . . , xm  of size m, the empirical Rademacher complexity of Π1 F  can be bounded as follows:   cid:98 RS Π1 F   =  1 m  1 m  1 m  1 m  E  E  σi  wj  y∈Y  y∈Y   cid:107 w cid:107 1≤1   cid:107 w cid:107 1≤1  σ cid:34  sup wjΦj xi, y  cid:35  m cid:88 i=1 N cid:88 j=1 σ cid:34  sup σiΦj xi, y  cid:35  N cid:88 j=1 m cid:88 i=1 σ cid:34  sup  cid:35  σiΦj xi, y  cid:12  cid:12  cid:12  cid:12  y∈Y  cid:12  cid:12  cid:12  cid:12  m cid:88 i=1  cid:35  ≤ 2 cid:98 RS Π1 H  , σ cid:34  sup y∈Y cid:12  cid:12  cid:12  cid:12  σiΦ xi, y  cid:12  cid:12  cid:12  cid:12  m cid:88 i=1  j∈[N ]  Φ∈H  E  E  =  =  ≤   cid:3   which completes the proof.  The learning guarantee of the theorem is remarkable since it does not depend on the dimension N and since it only depends on the complexity of the family H of feature functions  or base hypotheses . Since for any ρ > 0, f  ρ admits the same generalization error as f , the theorem implies that with probability at least 1 − δ, the following inequality holds for all f ∈ { x, y   cid:55 → w · Φ x, y  :  cid:107 w cid:107 1 ≤ 1 ρ} and ρ > 0:  R f   ≤  1 m  m cid:88 i=1  logu0 cid:18  cid:88 y∈Y  ef  xi,y −f  xi,yi  cid:19 +  8c ρ  Rm Π1 H  + cid:115  log log2  m  4r ρ  + cid:115  log 2  δ 2m  .  This inequality can be used to derive an algorithm that selects w and ρ > 0 to minimize the right-hand side. The minimization with respect to ρ does not lead to a convex optimization and depends on theoretical constant factors aﬀecting the second and third term. Thus, instead, ρ is left as a free parameter of the algorithm, typically determined via cross-validation.  Now, since only the ﬁrst term of the right-hand side depends on w, for any ρ > 0, the bound suggests selecting w as the solution of the following optimization problem:  min  cid:107 w cid:107 1≤ 1  ρ  1 m  m cid:88 i=1  log cid:18  cid:88 y∈Y  ew·Φ xi,y −w·Φ xi,yi  cid:19 .   13.14    324  Chapter 13 Conditional Maximum Entropy Models  Introducing a Lagrange variable λ ≥ 0, the optimization problem can be written equivalently as  min  w  λ cid:107 w cid:107 1 +  1 m  m cid:88 i=1  log cid:18  cid:88 y∈Y  ew·Φ xi,y −w·Φ xi,yi  cid:19 .   13.15   Since for any choice of ρ in the constraint of  13.14 , there exists an equivalent dual variable λ in the formulation of  13.15  that achieves the same optimal w, λ can be freely selected via cross-validation. The resulting algorithm precisely coincides with conditional Maxent.  When the dimension N of the feature vectors Φ is ﬁnite, the following margin-  based guarantee holds. Theorem 13.3 For any δ > 0, with probability at least 1 − δ over the draw of an i.i.d. sample S of size m, the following holds for all ρ > 0 and f ∈ F = { x, y   cid:55 → w · Φ x, y  :  cid:107 w cid:107 1 ≤ 1}:  R f   ≤  1 m  m cid:88 i=1  logu0 cid:18  cid:88 y∈Y  e  f  xi,y −f  xi,yi   ρ   cid:19 +  4cr cid:112 2 log 2cN    ρ  + cid:115  log log2  m  4r ρ  + cid:115  log 2  δ 2m  ,  where u0 = log 1 + 1 e . Proof: The proof coincides with that of Theorem 13.2, modulo the upper bound on Rm Π1 F  . For any sample S =  x1, . . . , xm  of size m, the empirical Rademacher complexity of Π1 F  can be bounded as follows:   cid:98 RS Π1 F   =  σiw · Φ xi, y  cid:35  σiΦ xi, y  cid:35  m cid:88 i=1 σiΦ xi, y  cid:13  cid:13  cid:13  cid:13 ∞ cid:35  m cid:88 i=1  s  m cid:88 i=1  w ·  =  E  E  y∈Y  y∈Y  1 m  1 m   cid:107 w cid:107 1≤1   cid:107 w cid:107 1≤1  σ cid:34  sup σ cid:34  sup σ cid:34  sup y∈Y cid:13  cid:13  cid:13  cid:13  σ cid:34  ≤ r cid:112 2 log 2cN  ,  1 m  1 m  E  E  =  =  m cid:88 i=1  sup j∈[N ]  y∈Y,s∈{−1,+1}  σiΦj xi, y  cid:35   where the third equality holds by deﬁnition of the dual norm, and the last inequality by the maximal inequality  Corollary D.11 , since the supremum is taken over 2cN  cid:3  choices.   13.7 Logistic regression  325  This learning guarantee of the theorem is very favorable even for relatively high- dimensional problems since its dependency on the dimension N is only logarithmic.  13.7 Logistic regression  The binary case of conditional Maxent models  c = 2  is known as logistic regression and is one of the most well-known algorithms for binary classiﬁcation.  13.7.1 Optimization problem In the binary case, the sum appearing in the optimization problem of conditional Maxent models can be simpliﬁed as follows:  ew·Φ xi,y −w·Φ xi,yi  = ew·Φ xi,+1 −w·Φ xi,yi  + ew·Φ xi,−1 −w·Φ xi,yi    cid:88 y∈Y  = 1 + e−yiw·[Φ xi,+1 −Φ xi,−1 ] = 1 + e−yiw·Ψ xi ,  where for all x ∈ X, Ψ x  = Φ x, +1  − Φ x,−1 . This leads to the following optimization problem, which deﬁnes L1-regularized logistic regression:  min w∈RN  λ cid:107 w cid:107 1 +  1 m  m cid:88 i=1  log cid:104 1 + e−yiw·Ψ xi  cid:105 .   13.16   As discussed in the general case, this is a convex optimization problem which ad- mits a variety of diﬀerent solutions. A common solution is SGD, another one is coordinate descent. When coordinate descent is used, then the algorithm coin- cides with the alternative to AdaBoost where the logistic loss is used instead of the exponential loss  φ −u  = log2 1 + e−u  ≥ 1u≤0 . 13.7.2 Logistic model In the binary case, the conditional probability deﬁned by the weight vector w can be expressed as follows:  pw[y = +1  x] =  ew·Φ x,+1   ,  Z x    13.17   with Z x  = ew·Φ x,+1  + ew·Φ x,−1 . Thus, prediction is based on a linear decision rule deﬁned by the sign of log-odds ratio:  log  pw[y = +1  x] pw[y = −1  x]  = w · cid:0 Φ x, +1  − Φ x,−1  cid:1  = w · Ψ x .   326  Chapter 13 Conditional Maximum Entropy Models  Figure 13.1 Plot of the logistic function flogistic.  This is why logistic regression is also known as a log-linear model . Observe also that the conditional probability admits the following logistic form:  1  1 + e−w·[Φ x,+1 −Φ x,−1 ] =  pw[y = +1  x] = where flogistic is the function deﬁned over R by flogistic : x  cid:55 → 1 1+e−x . Figure 13.1 shows the plot of this function. The logistic function maps the images of the linear function x  cid:55 → Ψ x  to the interval [0, 1], which makes them interpretable as probabilities.  1 + e−w·Ψ x  = flogistic cid:0 w · Ψ x  cid:1 ,  1  L1-regularized logistic regression beneﬁts from the strong learning guarantees already presented for conditional maxent models, in the special case of two classes  c = 2 . The learning guarantees for L2-regularized logistic regression will be similarly special cases of those presented in the next section.  13.8 L2-regularization  A common variant of conditional Maxent models is one where the dimension N is ﬁnite and where the regularization is based on the norm-2 squared of the weight vector w. The optimization problem is thus given by  λ cid:107 w cid:107 2  2 −  log pw[yixi],  1 m  m cid:88 i=1  min w∈RN where for all  x, y  ∈ X × Y,  pw[yx] =  exp w · Φ x, y    Z x   and Z x  = cid:88 y∈Y  exp w · Φ x, y  .   13.18   As for the norm-1 regularization, there are many optimization solutions available for this problem, including special-purpose algorithms, general ﬁrst-order and second-   13.8 L2-regularization  327  order solutions, and special-purpose distributed solutions. Here, the objective is additionally diﬀerentiable. A common optimization method is simply stochastic gradient descent  SGD .  In contrast to norm-1-regularized conditional Maxent models, which lead to sparser weight vectors, norm-2 conditional Maxent models lead to non-sparse solutions, which may be preferable and lead to more accurate solutions in some applications such as natural language processing. The following margin-based guarantee holds for norm-2 regularized conditional Maxent, assuming that the norm-2 of the feature vector is bounded. Theorem 13.4 For any δ > 0, with probability at least 1 − δ over the draw of an i.i.d. sample S of size m, the following holds for all ρ > 0 and f ∈ F = { x, y   cid:55 → w · Φ x, y  :  cid:107 w cid:107 2 ≤ 1}:  R f   ≤  1 m  m cid:88 i=1  logu0 cid:18  cid:88 y∈Y  e  f  xi,y −f  xi,yi   ρ   cid:19  +  4r2c2 ρ√m  + cid:115  log log2  m  4r2 ρ  + cid:115  log 2  δ 2m  ,  where u0 = log 1 + 1 e  and r2 = sup x,y   cid:107 Φ x, y  cid:107 2. Proof: The proof is similar to that of Theorem 13.3, modulo the observation that here w · Φ x, y  ≤  cid:107 w cid:107 2 cid:107 Φ x, y  cid:107 2 ≤ r2 and modulo the upper bound on Rm Π1 F  . For any sample S =  x1, . . . , xm  of size m, the empirical Rademacher complexity of Π1 F  can be bounded as follows:   cid:98 RS Π1 F   =  E  E  E  y∈Y  y∈Y  1 m  1 m  1 m  w ·   cid:107 w cid:107 2≤1   cid:107 w cid:107 2≤1  m cid:88 i=1  σ cid:34  sup σiw · Φ xi, y  cid:35  σ cid:34  sup σiΦ xi, y  cid:35  m cid:88 i=1 σ cid:34  sup σiΦ xi, y  cid:13  cid:13  cid:13  cid:13 2 cid:35  y∈Y cid:13  cid:13  cid:13  cid:13  m cid:88 i=1 σ cid:34  cid:13  cid:13  cid:13  cid:13  σiΦ xi, y  cid:13  cid:13  cid:13  cid:13 2 cid:35  m cid:88 i=1 m cid:88 y∈Y  cid:118  cid:117  cid:117  cid:116 E σ cid:34  cid:13  cid:13  cid:13  cid:13  2 cid:35  σiΦ xi, y  cid:13  cid:13  cid:13  cid:13  m cid:88 i=1 m cid:88 y∈Y  cid:118  cid:117  cid:117  cid:116  2 cid:35  ≤ m cid:88 i=1 m cid:88 y∈Y   cid:107 Φ xi, y  cid:107 2  E  1  1  1  2  r2c √m  ,  =  =  ≤  ≤  =  where the third equality holds by deﬁnition of the dual norm, and the second  cid:3  inequality by Jensen’s inequality.   328  Chapter 13 Conditional Maximum Entropy Models  The learning guarantee of the theorem for L2-regularized conditional maxent models admits the advantage that the bound does not depend on the dimension. It can be very favorable for r2 relatively small. The algorithm can then be very eﬀective, provided that a small error can be achieved by a non-sparse weight vector.  13.9 Proof of the duality theorem  In this section, we give the full proof of Theorem 13.1. Proof: The proof is similar to that of Theorem 12.2 and follows by application of the Fenchel duality theorem  theorem B.39  to the optimization problem  13.4   with the functions f and g deﬁned for all ¯p ∈  RY X1 and u ∈ RN by f  ¯p  = x∼ cid:98 D1 cid:104  cid:101 D cid:0 p[·x] cid:107  p0[·x] cid:1  cid:105 , g u  = IC u  and Ap = cid:80 x∈X cid:80 y∈Y cid:98 D1 x p[yx]Φ x, y . E A is a bounded linear map since we have  cid:107 A¯p cid:107  ≤  cid:107 ¯p cid:107 1 supx∈X,y∈Y  cid:107 Φ x, y  cid:107 ∞ ≤ r cid:107 ¯p cid:107 1 for any ¯p ∈  RY X1. Also, notice that the conjugate of A is given for all w ∈ RN and  x, y  ∈ X1 × Y by  A∗w  x, y  = w · cid:0  cid:98 D1 x Φ x, y  cid:1 .  x,y ∼ cid:98 D[Φ x, y ] = A¯p0 with ¯p0 =  D ·x  x∈X1 . Consider u0 ∈ RN deﬁned by u0 = E Since ¯p0 is in dom f   = ∆X1, u0 is in A dom f   . Furthermore, since λ is positive, u0 is contained in int C . g = IC equals zero over int C  and is therefore continuous over int C , thus g is continuous at u0 and we have u0 ∈ A dom f    ∩ cont g . Thus, the assumptions of Theorem B.39 hold. The conjugate function of f is deﬁned for all ¯q =  q[·x] x∈X1 ∈  RY X1 by f∗ ¯q  = sup  ¯p∈ RY X1 cid:26  cid:104 p, q cid:105  − cid:88 x∈X cid:98 D1 x  cid:101 D p[·x] cid:107  p0[·x]  cid:27  ¯p∈ RY X1 cid:40   cid:88 x∈X1 cid:98 D1[x] cid:88 y∈Y ¯p∈ RY X1 cid:40  cid:88 y∈Y =  cid:88 x∈X1 cid:98 D1 x   cid:98 D1 x  cid:19 , =  cid:88 x∈X1 cid:98 D1 x f∗x cid:18  q[yx]  −  cid:88 x∈X1 cid:98 D1[x] cid:101 D p[·x] cid:107  p0[·x]  cid:41   cid:98 D1[x]  cid:98 D1 x  cid:21  − cid:101 D p[·x] cid:107  p0[·x]  cid:41  p[yx] cid:20  q[yx] where, for all x ∈ X1 and p ∈ RX1, fx is deﬁned by fx ¯p  =  cid:101 D p[·x] cid:107  p0[·x] . By Lemma B.37, the conjugate function f∗x is given for all ¯q ∈  RY X1 by f∗x cid:16  q[yx]  cid:98 D1 x  cid:17  = log cid:18  cid:80 y∈Y p0[yx]e  q[yx] cid:99 D1 x  cid:19 . Thus, f∗ is given for all ¯q ∈  RY X1 by  p[yx]q[yx]  = sup  sup  f∗ q  = E  x∼ cid:98 D1 cid:20  log cid:16  cid:88 y∈Y  p0[yx]e  q[yx] cid:99 D1 x  cid:17  cid:21 .   13.9 Proof of the duality theorem  329  As in the proof of Theorem 12.2, the conjugate function of g = IC is given for all   x,y ∼ cid:98 D[w · Φ x, y ] + λ cid:107 w cid:107 1. In view of these identities, we  w ∈ RN by g∗ w  = E can write, for all w ∈ RN , − f∗ A∗w  − g∗ −w  x∼ cid:98 D1 cid:20  log cid:16  cid:88 y∈Y = − E x∼ cid:98 D1 = − E m cid:88 i=1 m cid:88 i=1  log cid:20  pw[yixi]  1 m  1 m  log  =  =  [log Z w, x ] +  ew·Φ xi,yi  Z w, xi  − λ cid:107 w cid:107 1  p0[yixi] cid:21  − λ cid:107 w cid:107 1 = G w ,  p0[yx]ew·Φ x,y  cid:17  cid:21  + E  x,y ∼ cid:98 D  1 m  m cid:88 i=1  w · Φ xi, yi  − λ cid:107 w cid:107 1  [w · Φ x, y ] − λ cid:107 w cid:107 1  p∗[yx]  p∗[yx]  pw[yx]  we can write  pw[yx] cid:21   which proves that supw∈RN G w  = min¯p∈ RY X1 F  ¯p . The second part of the proof is similar to that of Theorem 12.2. For any w ∈ RN , x∼ cid:98 D1 x∼ cid:98 D1 G w  − E [D p∗[·x] cid:107  p0[·x] ] + E [D p∗[·x] cid:107  pw[·x] ]  x,y ∼ cid:98 D cid:20 log p0[yx] cid:21  − λ cid:107 w cid:107 1 − E y∼p∗[·x] cid:20 log x∼ cid:98 D1 pw[yx] = E  x,y ∼ cid:98 D cid:20 log p0[yx] cid:21  − E y∼p∗[·x] cid:20 log x∼ cid:98 D1 = −λ cid:107 w cid:107 1 + E  x,y ∼ cid:98 D = −λ cid:107 w cid:107 1 + E = −λ cid:107 w cid:107 1 + w · cid:104  As the solution of the primal optimization, ¯p∗ veriﬁes IC cid:18  E that is cid:13  cid:13  cid:13  cid:13  x∼ cid:98 D1 − cid:107 w cid:107 1 + w · cid:20   p0[yx] cid:21  + E y∼p∗[·x] cid:20 log x∼ cid:98 D1 p0[yx] cid:21  pw[yx] x∼ cid:98 D1 [w · Φ x, y  − log Z w, x ] − E [Φ x, y ] cid:105 .  x,y ∼ cid:98 D E  x,y ∼ cid:98 D[Φ x, y ] cid:13  cid:13  cid:13  cid:13 ∞ x∼ cid:98 D1 [w · Φ x, y ] − E  [w · Φ x, y ] cid:21  ≤ − cid:107 w cid:107 1 +  cid:107 w cid:107 1 = 0.  ≤ λ. By H¨older’s inequality, this  [Φ x, y ] cid:19  = 0,  x∼ cid:98 D1 [Φ x, y ] − E  implies the following inequality:  [Φ x, y ]− E  E y∼p∗[·x]   x,y ∼ cid:98 D  x∼ cid:98 D1  y∼p∗[·x]  y∼p∗[·x]  y∼p∗[·x]  E  [w · Φ x, y  − log Z w, x ]  y∼p∗[·x]   330  Chapter 13 Conditional Maximum Entropy Models  Thus, we can write, for any w ∈ RN ,  E  x∼ cid:98 D1 cid:104 D p∗[·x] cid:107  pw[·x]  cid:105  ≤ E  x∼ cid:98 D1 cid:104 D p∗[·x] cid:107  p0[·x]  cid:105  − G w . Now, assume that w veriﬁes G w  − supw∈RN G w  ≤  cid:15  for some  cid:15  > 0. Then, x∼ cid:98 D1 [D p∗[·x] cid:107  p0[·x] ]−G w  =  supw G w  −G w  ≤  cid:15  implies the inequality E x∼ cid:98 D1 [D p∗[·x] cid:107  pw[·x] ] ≤  cid:15 . This concludes the proof of the theorem.  cid:3  E  13.10 Chapter notes  The logistic regression model is a classical model in statistics. The term logistic was introduced by the Belgian mathematician Verhulst [1838, 1845]. An early reference for logistic regression is the publication of Berkson [1944] who advocated the use of the logistic function, instead of the cumulative distribution function of the standard normal distribution  probit model  .  Conditional maximum entropy models in natural language processing were in- troduced by Berger et al. [1996] and were widely adopted for a variety of diﬀerent tasks, including part-of-speech tagging, parsing, machine translation, and text cate- gorization  see tutorial by Manning and Klein [2003] . Our presentation of the con- ditional Maxent principle, including their regularized variants, the duality theorem for conditional Maxent models  Theorem 13.1  and their theoretical justiﬁcations are based on [Cortes, Kuznetsov, Mohri, and Syed, 2015]. This chapter provided two types of justiﬁcation for these models: one based on the conditional Maxent principle, another based on standard generalization bounds.  As in the case of Maxent models for density estimation, conditional Maxent mod- els can be extended by using other Bregman divergences [Laﬀerty, Pietra, and Pietra, 1997] and other regularizations. Laﬀerty [1999] presented a general frame- work for incremental algorithms based on Bregman divergences that admits logistic regression as as special case, see also [Collins et al., 2002] who showed that boost- ing and logistic regression were special instances of a common framework based on Bregman divergences. The regularized conditional Maxent models presented in this chapter can be extended similarly using other Bregman divergences. In the binary classiﬁcation case, when coordinate descent is used to solve the optimiza- tion problem of regularized conditional Maxent models, the algorithm coincides with L1-regularized AdaBoost modulo the use of the logistic loss instead of the exponential loss.  Cortes, Kuznetsov, Mohri, and Syed [2015] presented a more general family of conditional probability models, conditional structural Maxent models, for which they also presented a duality theorem and gave strong learning guarantees. These Maxent models are based on feature functions selected from a union of possibly   13.11 Exercises  331  very complex sub-families. The resulting algorithms coincide with the DeepBoost algorithms of Cortes, Mohri, and Syed [2014] in the binary classiﬁcation case or the multi-class DeepBoost algorithm of Kuznetsov, Mohri, and Syed [2014] in the multi- class classiﬁcation case, when the logistic function is used as a convex surrogate loss function.  13.11 Exercises  13.1 Extension to Bregman divergences.   a  Show how conditional Maxent models can be extended by using arbitrary  Bregman divergences instead of the  unnormalized  relative entropy.   b  Prove a duality theorem similar to Theorem 13.1 for theses extensions.   c  Derive theoretical guarantees for these extensions. What additional property is needed for the Bregman divergence so that your learning guarantees hold?  13.2 Stability analysis for L2-regularized conditional Maxent.   a  Give an upper bound on the stability of the L2-regularized conditional Max- ent in terms of the sample size and λ  Hint: use the techniques and results of Chapter 14 .   b  Use the previous question to derive a stability-based generalization guarantee  for the algorithm.  13.3 Maximum conditional Maxent. An alternative measure of closeness, instead of the conditional relative entropy, is the maximum relative entropy over all x ∈ X1.   a  Write the primal optimization problem for this maximum conditional Maxent formulation. Show that it is a convex optimization problem, and discuss its feasibility and the uniqueness of its solution.   b  Prove a duality theorem for maximum conditional Maxent and write the  equivalent dual problem.   c  Analyze the properties of maximum conditional Maxent and give a general-  ization bound for the algorithm.   332  Chapter 13 Conditional Maximum Entropy Models  13.4 Conditional Maxent with other marginal distributions: discuss and analyze con-  ditional Maxent models when using a distribution Q over X instead of  cid:98 D1. Prove  that a duality theorem similar to Theorem 13.1 holds.   14 Algorithmic Stability  In chapters 2–5 and several subsequent chapters, we presented a variety of general- ization bounds based on diﬀerent measures of the complexity of the hypothesis set H used for learning, including the Rademacher complexity, the growth function, and the VC-dimension. These bounds ignore the speciﬁc algorithm used, that is, they hold for any algorithm using H as a hypothesis set.  One may ask if an analysis of the properties of a speciﬁc algorithm could lead to ﬁner guarantees. Such an algorithm-dependent analysis could have the beneﬁt of a more informative guarantee. On the other hand, it could be inapplicable to other algorithms using the same hypothesis set. Alternatively, as we shall see in this chapter, a more general property of the learning algorithm could be used to incorporate algorithm-speciﬁc properties while extending the applicability of the analysis to other learning algorithms with similar properties.  This chapter uses the property of algorithmic stability to derive algorithm-dependent  learning guarantees. We ﬁrst present a generalization bound for any algorithm that is suﬃciently stable. Then, we show that the wide class of kernel-based regular- ization algorithms enjoys this property and derive a general upper bound on their stability coeﬃcient. Finally, we illustrate the application of these results to the analysis of several algorithms both in the regression and classiﬁcation settings, in- cluding kernel ridge regression  KRR , SVR, and SVMs.  14.1 Deﬁnitions  We start by introducing the notation and deﬁnitions relevant to our analysis of algorithmic stability. We denote by z a labeled example  x, y  ∈ X × Y. The hy- potheses h we consider map X to a set Y cid:48  sometimes diﬀerent from Y. In particular, for classiﬁcation, we may have Y = {−1, +1} while the hypothesis h learned takes values in R. The loss functions L we consider are therefore deﬁned over Y cid:48 × Y, with Y cid:48  = Y in most cases. For a loss function L : Y cid:48  × Y → R+, we denote the loss of   334  Chapter 14 Algorithmic Stability  a hypothesis h at point z by Lz h  = L h x , y . We denote by D the distribution according to which samples are drawn and by H the hypothesis set. The empirical error or loss of h ∈ H on a sample S =  z1, . . . , zm  and its generalization error are deﬁned, respectively, by  1 m  m cid:88 i=1   cid:98 RS h  =  Lzi h   and R h  = E z∼D  [Lz h ].  Given an algorithm A, we denote by hS the hypothesis hS ∈ H returned by A when trained on sample S. We will say that the loss function L is bounded by M ≥ 0 if for all h ∈ H and z ∈ X× Y, Lz h  ≤ M . For the results presented in this chapter, a weaker condition suﬃces, namely that Lz hS  ≤ M for all hypotheses hS returned by the algorithm A. We are now able to deﬁne the notion of uniform stability, the algorithmic property  used in the analyses of this chapter. Deﬁnition 14.1  Uniform stability  Let S and S cid:48  be any two training samples that dif- fer by a single point. Then, a learning algorithm A is uniformly β-stable if the hypotheses it returns when trained on any such samples S and S cid:48  satisfy  ∀z ∈ Z,  Lz hS  − Lz hS cid:48   ≤ β.  The smallest such β satisfying this inequality is called the stability coeﬃcient of A. In other words, when A is trained on two similar training sets, the losses incurred by the corresponding hypotheses returned by A should not diﬀer by more than β. Note that a uniformly β-stable algorithm is often referred to as being β-stable or even just stable  for some unspeciﬁed β . In general, the coeﬃcient β depends on the sample size m. We will see in section 14.2 that β = o 1 √m  is necessary for the convergence of the stability-based learning bounds presented in this chapter. In section 14.3, we will show that a more favorable condition holds, that is, β = O 1 m , for a wide family of algorithms.  14.2 Stability-based generalization guarantee  In this section, we show that exponential bounds can be derived for the general- ization error of stable learning algorithms. The main result is presented in theo- rem 14.2. Theorem 14.2 Assume that the loss function L is bounded by M ≥ 0. Let A be a β-stable learning algorithm and let S be a sample of m points drawn i.i.d. according   14.2 Stability-based generalization guarantee  335  to distribution D. Then, with probability at least 1 − δ over the sample S drawn, the following holds:  R hS  ≤  cid:98 RS hS  + β +  2mβ + M   cid:115  log 1  δ 2m  .  Proof: The proof is based on the application of McDiarmid’s inequality  theo-  rem D.8  to the function Φ deﬁned for all samples S by Φ S  = R hS  −  cid:98 RS hS .  Let S cid:48  be another sample of size m with points drawn i.i.d. according to D that diﬀers from S by exactly one point. We denote that point by zm in S, z cid:48 m in S cid:48 , i.e.,  S =  z1, . . . , zm−1, zm   and S cid:48  =  z1, . . . , zm−1, z cid:48 m .  By deﬁnition of Φ, the following inequality holds:  Φ S cid:48   − Φ S  ≤ R hS cid:48   − R hS  +  cid:98 RS cid:48  hS cid:48   −  cid:98 RS hS .  We bound each of these two terms separately. By the β-stability of A, we have [Lz hS  − Lz hS cid:48  ] ≤ β.  R hS  − R hS cid:48   =  E  [Lz hS cid:48  ] ≤ E  z   14.1   Using the boundedness of L along with β-stability of A, we also have  z  z  1  [Lz hS ] − E m cid:12  cid:12  cid:12  cid:12  cid:12   cid:18  m−1 cid:88 i=1 m cid:34  cid:18  m−1 cid:88 i=1  1  ≤  ≤  m − 1 m  β +  M m ≤ β +  M m  .  Lzi hS  − Lzi hS cid:48   cid:19  + Lzm hS  − Lz cid:48 m  hS cid:48   cid:12  cid:12  cid:12  cid:12  cid:12  Lzi hS  − Lzi hS cid:48   cid:19  + Lzm hS  − Lz cid:48 m  hS cid:48   cid:35    cid:98 RS hS  −  cid:98 RS cid:48  hS cid:48   =  Thus, in view of  14.1 , Φ satisﬁes the condition Φ S  − Φ S cid:48   ≤ 2β + M m . By applying McDiarmid’s inequality to Φ S , we can bound the deviation of Φ from its mean as  P cid:104 Φ S  ≥  cid:15  + E  S  [Φ S ] cid:105  ≤ exp cid:18  −2m cid:15 2   2mβ + M  2 cid:19  ,  or, equivalently, with probability 1 − δ,  Φ S  <  cid:15  + E  [Φ S ],  S   14.2   where δ = exp cid:16  −2m cid:15 2   14.2  and rearrange terms, then, with probability 1 − δ, we have   2mβ+M  2 cid:17 . If we solve for  cid:15  in this expression for δ, plug into Φ S  ≤ E S∼Dm  [Φ S ] +  2mβ + M   cid:115  log 1  δ 2m   14.3   .   336  Chapter 14 Algorithmic Stability  We now bound the expectation term, ﬁrst noting that by linearity of expectation  ES[Φ S ] = ES[R hS ] − ES[ cid:98 RS hS ]. By deﬁnition of the generalization error,  [R hS ] = E  [Lz hS ].   14.4   E  E  [Lz hS ] cid:3  =  S,z∼Dm+1  S∼Dm  By the linearity of expectation,  z∼D  S∼Dm cid:2  E m cid:88 i=1  1 m  S∼Dm  E  S∼Dm  [ cid:98 RS hS ] =  E  [Lzi hS ] = E  [Lz1  hS ],   14.5   S∼Dm  where the second equality follows from the fact that the zi are drawn i.i.d. and  thus the expectations ES∼Dm[Lzi  hS ], i ∈ [m], are all equal. The last expression in  14.5  is the expected loss of a hypothesis on one of its training points. We can rewrite it as ES∼Dm [Lz1 hS ] = E S,z∼Dm+1 [Lz hS cid:48  ], where S cid:48  is a sample of m points containing z extracted from the m + 1 points formed by S and z. Thus, in view of  14.4  and by the β-stability of A, it follows that E  E   E S∼Dm  [Φ S ] = cid:12  cid:12   [Lz hS ] −  S,z∼Dm+1  S,z∼Dm+1  E  S,z∼Dm+1 cid:2 Lz hS  − Lz hS cid:48   cid:3   E  [β] = β.  S,z∼Dm+1  ≤ ≤  [Lz hS cid:48  ] cid:12  cid:12    cid:3  We can thus replace ES[Φ S ] by β in  14.3 , which completes the proof. The bound of the theorem converges for  mβ  √m = o 1 , that is β = o 1 √m . In particular, when the stability coeﬃcient β is in O 1 m , the theorem guarantees In the next section, we show that kernel-based regularization algorithms precisely admit this property under some general assumptions.  that R hS  −  cid:98 RS hS  = O 1 √m  with high probability.  14.3 Stability of kernel-based regularization algorithms  Let K be a positive deﬁnite symmetric kernel, H the reproducing kernel Hilbert space associated to K, and  cid:107  ·  cid:107 K the norm induced by K in H. A kernel-based regularization algorithm is deﬁned by the minimization over H of an objective function FS based on a training sample S =  z1, . . . , zm  and deﬁned for all h ∈ H  by:  In this equation,  cid:98 RS h  = 1  i=1 Lzi h  is the empirical error of hypothesis h with respect to a loss function L and λ ≥ 0 a trade-oﬀ parameter balancing the emphasis on the empirical error versus the regularization term  cid:107 h cid:107 2 K. The hypothesis set H  FS h  =  cid:98 RS h  + λ cid:107 h cid:107 2 m cid:80 m  K.   14.6    14.3 Stability of kernel-based regularization algorithms  337  is the subset of H formed by the hypotheses possibly returned by the algorithm. Algorithms such as KRR, SVR and SVMs all fall under this general model.  We ﬁrst introduce some deﬁnitions and tools needed for a general proof of an upper bound on the stability coeﬃcient of kernel-based regularization algorithms. Our analysis will assume that the loss function L is convex and that it further veriﬁes the following Lipschitz-like smoothness condition.  Deﬁnition 14.3  σ-admissibility  A loss function L is σ-admissible with respect to the  hypothesis class H if there exists σ ∈ R+ such that for any two hypotheses h, h cid:48  ∈ H and for all  x, y  ∈ X × Y,  L h cid:48  x , y  − L h x , y  ≤ σh cid:48  x  − h x .   14.7   This assumption holds for the quadratic loss and most other loss functions where  the hypothesis set and the set of output labels are bounded by some M ∈ R+: ∀h ∈ H,∀x ∈ X,h x  ≤ M and ∀y ∈ Y,y ≤ M . We will use the notion of Bregman divergence, BF which can be deﬁned for any convex and diﬀerentiable function F : H → R as follows: for all f, g ∈ H,  BF  f cid:107 g  = F  f   − F  g  −  cid:104 f − g,∇F  g  cid:105  .  Section E.4 presents the properties of the Bregman divergence in more detail and also contains ﬁgure E.2 which illustrates the geometric interpretation of the Breg- man divergence. We generalize the deﬁnition of Bregman divergence to cover the case of convex but non-diﬀerentiable loss functions F by using the notion of sub-  gradient. For a convex function F : H → R, we denote by ∂F  h  the subdiﬀerential  of F at h, which is deﬁned as follows:  ∂F  h  = {g ∈ H : ∀h cid:48  ∈ H, F  h cid:48   − F  h  ≥  cid:104 h cid:48  − h, g cid:105 }.  Thus, ∂F  h  is the set of vectors g deﬁning a hyperplane supporting function F at point h  see ﬁgure 14.1 . Elements of the subdiﬀerential are called subgradients  see section B.4.1 for more discussion . Note, the subgradient found in ∂F  h  coincides with ∇F  h  when F is diﬀerentiable at h, i.e. ∂F  h  = {∇F  h }. Furthermore, at a point h where F is minimal, 0 is an element of ∂F  h . The subgradient is additive, that is, for two convex function F1 and F2, ∂ F1 + F2  h  = {g1 + g2 : g1 ∈ ∂F1 h , g2 ∈ ∂F2 h }. For any h ∈ H, we ﬁx δF  h  to be an  arbitrary  element of ∂F  h . For any such choice of δF , we can deﬁne the generalized Bregman divergence associated to F by:  ∀h cid:48 , h ∈ H, BF  h cid:48   cid:107  h  = F  h cid:48   − F  h  −  cid:104 h cid:48  − h, δF  h  cid:105  .   14.8   Note that by deﬁnition of the subgradient, BF  h cid:48   cid:107  h  ≥ 0 for all h cid:48 , h ∈ H. Starting from  14.6 , we can now deﬁne the generalized Bregman divergence of FS. Let N denote the convex function h →  cid:107 h cid:107 2 K. Since N is diﬀerentiable,   338  Chapter 14 Algorithmic Stability  Figure 14.1 Illustration of the notion of subgradient: supporting hyperplanes  shown in red, orange and green  for the function F  shown in blue  at point h are deﬁned by elements of the subdiﬀerential ∂F  h .  δN  h  = ∇N  h  for all h ∈ H, and thus δN  as well as BN   is uniquely deﬁned. To make the deﬁnition of the Bregman divergences for FS and  cid:98 RS compatible so that BFS = B cid:98 RS + λBN , we deﬁne δ cid:98 RS in terms of δFS by: δ cid:98 RS h  = δFS h − λ∇N  h  for all h ∈ H. Furthermore, we choose δFS h  to be 0 for any point h where FS is minimal and let δFS h  be an arbitrary element of ∂FS h  for all other h ∈ H. We proceed in a similar way to deﬁne the Bregman divergences for FS cid:48  and  cid:98 RS cid:48  so that BFS cid:48  = B cid:98 RS cid:48   We will use the notion of generalized Bregman divergence for the proof of the following general upper bound on the stability coeﬃcient of kernel-based regular- ization algorithms.  + λBN .  Proposition 14.4 Let K be a positive deﬁnite symmetric kernel such that for all  x ∈ X, K x, x  ≤ r2 for some r ∈ R+ and let L be a convex and σ-admissible  loss function. Then, the kernel-based regularization algorithm deﬁned by the mini- mization  14.6  is β-stable with the following upper bound on β:  σ2r2 mλ  .  β ≤  Proof: Let h be a minimizer of FS and h cid:48  a minimizer of FS cid:48 , where samples S and S cid:48  diﬀer exactly by one point, zm in S and z cid:48 m in S cid:48 . Since the generalized Bregman divergence is non-negative and since BFS = B cid:98 RS + λBN ,  + λBN and BFS cid:48  = B cid:98 RS cid:48   we can write  BFS  h cid:48  cid:107 h  + BFS cid:48   h cid:107 h cid:48   ≥ λ cid:0 BN  h cid:48  cid:107 h  + BN  h cid:107 h cid:48   cid:1 .   14.3 Stability of kernel-based regularization algorithms  339  Observe that BN  h cid:48  cid:107 h  + BN  h cid:107 h cid:48   = − cid:104 h cid:48  − h, 2h cid:105  −  cid:104 h − h cid:48 , 2h cid:48  cid:105  = 2 cid:107 h cid:48  − h cid:107 2 K. Let ∆h denote h cid:48  − h, then we can write  2λ∆h cid:107 2  K  ≤ BFS  h cid:48   cid:107  h  + BFS cid:48   h cid:107  h cid:48   = FS h cid:48   − FS h  −  cid:104 h cid:48  − h, δFS h  cid:105  + FS cid:48  h  − FS cid:48  h cid:48   −  cid:104 h − h cid:48 , δFS cid:48  h cid:48   cid:105  = FS h cid:48   − FS h  + FS cid:48  h  − FS cid:48  h cid:48   =  cid:98 RS h cid:48   −  cid:98 RS h  +  cid:98 RS cid:48  h  −  cid:98 RS cid:48  h cid:48  .  The second equality follows from the deﬁnition of h cid:48  and h as minimizers and our choice of the subgradients for minimal points which together imply δFS cid:48  h cid:48   = 0 and δFS h  = 0. The last equality follows from the deﬁnitions of FS and FS cid:48 . Next, we express the resulting inequality in terms of the loss function L and use the fact that S and S cid:48  diﬀer by only one point along with the σ-admissibility of L to get  2λ cid:107 ∆h cid:107 2  K ≤ ≤  1 m σ m  [Lzm h cid:48   − Lzm  h  + Lz cid:48 m h  − Lz cid:48 m h cid:48  ] [∆h xm  + ∆h x cid:48 m ].   14.9   By the reproducing kernel property and the Cauchy-Schwarz inequality, for all x ∈ X,  ∆h x  =  cid:104 ∆h, K x,·  cid:105  ≤  cid:107 ∆h cid:107 K cid:107 K x,·  cid:107 K = cid:112 K x, x  cid:107 ∆h cid:107 K ≤ r cid:107 ∆h cid:107 K. In view of  14.9 , this implies  cid:107 ∆h cid:107 K ≤ σr reproducing property, the following holds:  λm . By the σ-admissibility of L and the  ∀z ∈ X × Y,Lz h cid:48   − Lz h  ≤ σ∆h x  ≤ rσ cid:107 ∆h cid:107 K,  which gives  and concludes the proof.  ∀z ∈ X × Y,Lz h cid:48   − Lz h  ≤  σ2r2 mλ  ,   cid:3   Thus, under the assumptions of the proposition, for a ﬁxed λ, the stability coeﬃcient of kernel-based regularization algorithms is in O 1 m .  14.3.1 Application to regression algorithms: SVR and KRR Here, we analyze more speciﬁcally two widely used regression algorithms, Support Vector Regression  SVR  and Kernel Ridge Regression  KRR , which are both special instances of the family of kernel-based regularization algorithms.   340  Chapter 14 Algorithmic Stability  SVR is based on the  cid:15 -insensitive loss L cid:15  deﬁned for all  y, y cid:48   ∈ Y × Y by:  L cid:15  y cid:48 , y  = cid:40 0  if y cid:48  − y ≤  cid:15 ;  y cid:48  − y −  cid:15  otherwise.   14.10   We now present a stability-based bound for SVR assuming that L cid:15  is bounded for the hypotheses returned by SVR  which, as we shall later see in lemma 14.7, is indeed the case when the label set Y is bounded . Corollary 14.5  Stability-based learning bound for SVR  Assume that K x, x  ≤ r2 for all x ∈ X for some r ≥ 0 and that L cid:15  is bounded by M ≥ 0. Let hS denote the hypothesis returned by SVR when trained on an i.i.d. sample S of size m. Then, for any δ > 0, the following inequality holds with probability at least 1 − δ:  R hS  ≤  cid:98 RS hS  +  r2 mλ  + cid:16  2r2  λ  + M cid:17  cid:115  log 1  δ 2m  .  Proof: We ﬁrst show that L cid:15  ·  = L cid:15  ·, y  is 1-Lipschitz for any y ∈ Y. For any y cid:48 , y cid:48  cid:48  ∈ Y, we must consider four cases. First, if y cid:48  − y ≤  cid:15  and y cid:48  cid:48  − y ≤  cid:15 , then L cid:15  y cid:48  cid:48  −L cid:15  y cid:48   = 0. Second, if y cid:48 −y >  cid:15  and y cid:48  cid:48 −y >  cid:15 , then L cid:15  y cid:48  cid:48  −L cid:15  y cid:48   = y cid:48  cid:48  − y − y cid:48  − y ≤ y cid:48  cid:48  − y cid:48 , by the triangle inequality. Third, if y cid:48  − y ≤  cid:15  and y cid:48  cid:48 −y >  cid:15 , then L cid:15  y cid:48  cid:48  −L cid:15  y cid:48   = y cid:48  cid:48 −y− cid:15  = y cid:48  cid:48 −y− cid:15  ≤ y cid:48  cid:48 −y−y cid:48 −y ≤ y cid:48  cid:48  − y cid:48 . Fourth, if y cid:48  cid:48  − y ≤  cid:15  and y cid:48  − y >  cid:15 , by symmetry the same inequality is obtained as in the previous case. Thus, in all cases, L cid:15  y cid:48  cid:48 , y −L cid:15  y cid:48 , y  ≤ y cid:48  cid:48 −y cid:48 . This implies in particular that L cid:15  is σ-admissible with σ = 1 for any hypothesis set H. By proposition 14.4, under the assumptions made, SVR is β-stable with β ≤ r2 mλ . Plugging this expression into  cid:3   the bound of theorem 14.2 yields the result.  We next present a stability-based bound for KRR, which is based on the square loss L2 deﬁned for all y cid:48 , y ∈ Y by:  L2 y cid:48 , y  =  y cid:48  − y 2.   14.11   As in the SVR setting, we assume in our analysis that L2 is bounded for the hypotheses returned by KRR  which, as we shall later see again in lemma 14.7, is indeed the case when the label set Y is bounded . Corollary 14.6  Stability-based learning bound for KRR  Assume that K x, x  ≤ r2 for all x ∈ X for some r ≥ 0 and that L2 is bounded by M ≥ 0. Let hS denote the hypothesis returned by KRR when trained on an i.i.d. sample S of size m. Then, for any δ > 0, the following inequality holds with probability at least 1 − δ:  R hS  ≤  cid:98 RS hS  +  4M r2 λm  + cid:16  8M r2  λ  + M cid:17  cid:115  log 1  δ 2m  .   14.3 Stability of kernel-based regularization algorithms  341  Proof: For any  x, y  ∈ X × Y and h, h cid:48  ∈ H,  L2 h cid:48  x , y  − L2 h x , y  = cid:12  cid:12  h cid:48  x  − y 2 −  h x  − y 2 cid:12  cid:12   = cid:12  cid:12  cid:12  cid:2 h cid:48  x  − h x ][ h cid:48  x  − y  +  h x  − y  cid:3  cid:12  cid:12  cid:12  ≤  h cid:48  x  − y + h x  − y h x  − h cid:48  x  ≤ 2√Mh x  − h cid:48  x , where we used the M -boundedness of the loss. Thus, L2 is σ-admissible with σ = 2√M . Therefore, by proposition 14.4, KRR is β-stable with β ≤ 4r2M mλ .  cid:3   Plugging this expression into the bound of theorem 14.2 yields the result.  The previous two corollaries assumed bounded loss functions. We now present a lemma that implies in particular that the loss functions used by SVR and KRR are bounded when the label set is bounded.  Lemma 14.7 Assume that K x, x  ≤ r2 for all x ∈ X for some r ≥ 0 and that for all y ∈ Y, L 0, y  ≤ B for some B ≥ 0. Then, the hypothesis hS returned by a kernel-based regularization algorithm trained on a sample S is bounded as follows:  ∀x ∈ X,hS x  ≤ r cid:112 B λ.  Proof: By the reproducing kernel property and the Cauchy-Schwarz inequality, we can write  ∀x ∈ X,hS x  =  cid:104 hS, K x,·  cid:105  ≤  cid:107 hS cid:107 K cid:112 K x, x  ≤ r cid:107 hS cid:107 K.   14.12  The minimization  14.6  is over H, which includes 0. Thus, by deﬁnition of FS and hS, the following inequality holds:  FS hS  ≤ FS 0  =  L 0, yi  ≤ B.  1 m  m cid:88 i=1  Since the loss L is non-negative, we have λ cid:107 hS cid:107 2 Combining this inequality with  14.12  yields the result.  K ≤ FS hS  and thus λ cid:107 hS cid:107 2  K ≤ B.  cid:3   14.3.2 Application to classiﬁcation algorithms: SVMs This section presents a generalization bound for SVMs, when using the standard  hinge loss deﬁned for all y ∈ Y = {−1, +1} and y cid:48  ∈ R by  Lhinge y cid:48 , y  = cid:40 0  1 − yy cid:48   if 1 − yy cid:48  ≤ 0; otherwise.   14.13   Corollary 14.8  Stability-based learning bound for SVMs  Assume that K x, x  ≤ r2 for all x ∈ X for some r ≥ 0. Let hS denote the hypothesis returned by SVMs when   342  Chapter 14 Algorithmic Stability  trained on an i.i.d. sample S of size m. Then, for any δ > 0, the following inequality holds with probability at least 1 − δ:  R hS  ≤  cid:98 RS hS  +  r2 mλ  + cid:16  2r2  λ  +  r √λ  + 1 cid:17  cid:115  log 1  δ 2m  .  Proof: It is straightforward to verify that Lhinge ·, y  is 1-Lipschitz for any y ∈ Y and therefore that it is σ-admissible with σ = 1. Therefore, by proposition 14.4, SVMs is β-stable with β ≤ r2 mλ . Since Lhinge 0, y  ≤ 1 for any y ∈ Y, by lemma 14.7, ∀x ∈ X,hS x  ≤ r √λ. Thus, for any sample S and any x ∈ X and y ∈ Y, the loss is bounded as follows: Lhinge hS x , y  ≤ r √λ + 1. Plugging this value of M  cid:3   and the one found for β into the bound of theorem 14.2 yields the result.  Since the hinge loss upper bounds the binary loss, the bound of the corollary 14.8 also applies to the generalization error of hS measured in terms of the standard binary loss used in classiﬁcation.  14.3.3 Discussion Note that the learning bounds presented for kernel-based regularization algorithms  λ√m cid:1 . Thus, these bounds are informative are of the form R hS  −  cid:98 RS hS  ≤ O cid:0  1 only when λ  cid:29  1 √m. The regularization parameter λ is a function of the sample  size m: for larger values of m, it is expected to be smaller, decreasing the emphasis on regularization. The magnitude of λ aﬀects the norm of the linear hypotheses used for prediction, with a larger value of λ implying a smaller hypothesis norm. In this sense, λ is a measure of the complexity of the hypothesis set and the condition required for λ can be interpreted as stating that a less complex hypothesis set guarantees better generalization.  Note also that our analysis of stability in this chapter assumed a ﬁxed λ: the regularization parameter is assumed to be invariant to the change of one point of the training sample. While this is a mild assumption, it may not hold in general.  14.4 Chapter notes  The notion of algorithmic stability was ﬁrst used by Devroye, Rogers and Wag- ner [Rogers and Wagner, 1978, Devroye and Wagner, 1979a,b] for the k-nearest neighbor algorithm and other k-local rules. Kearns and Ron [1999] later gave a formal deﬁnition of stability and used it to provide an analysis of the leave-one- out error. Much of the material presented in this chapter is based on Bousquet and Elisseeﬀ [2002]. Our proof of proposition 14.4 is novel and generalizes the re- sults of Bousquet and Elisseeﬀ [2002] to the case of non-diﬀerentiable convex losses. Moreover, stability-based generalization bounds have been extended to ranking al- gorithms [Agarwal and Niyogi, 2005, Cortes et al., 2007b], as well as to the non-i.i.d.   14.5 Exercises  343  scenario of stationary Φ- and β-mixing processes [Mohri and Rostamizadeh, 2010], and to the transductive setting [Cortes et al., 2008a]. Additionally, exercise 14.5 is based on Cortes et al. [2010b], which introduces and analyzes stability with respect to the choice of the kernel function or kernel matrix.  Note that while, as shown in this chapter, uniform stability is suﬃcient for de- riving generalization bounds, it is not a necessary condition. Some algorithms may generalize well in the supervised learning scenario but may not be uniformly stable, for example, the Lasso algorithm [Xu et al., 2008]. Shalev-Shwartz et al. [2009] have used the notion of stability to provide necessary and suﬃcient conditions for a technical condition of learnability related to PAC-learning, even in general scenarios where learning is possible only by using non-ERM rules.  14.5 Exercises  14.1 Tighter stability bounds   a  Assuming the conditions of theorem 14.2 hold, can one hope to guarantee a generalization with slack better than O 1 √m  even if the algorithm is very stable, i.e. β → 0?   b  Can you show an O 1 m  generalization guarantee if L is bounded by C √m  a very strong condition ? If so, how stable does the learning algorithm need to be?  14.2 Quadratic hinge loss stability. Let L denote the quadratic hinge loss function  deﬁned for all y ∈ {+1,−1} and y cid:48  ∈ R by  L y cid:48 , y  = cid:40 0   1 − y cid:48 y 2  if 1 − y cid:48 y ≤ 0; otherwise.  Assume that L h x , y  is bounded by M , 1 ≤ M < ∞, for all h ∈ H, x ∈ X, and y ∈ {+1,−1}, which also implies a bound on h x  for all h ∈ H and x ∈ X. Derive a stability-based generalization bound for SVMs with the quadratic hinge loss.  14.3 Stability of linear regression.   a  How does the stability bound in corollary 14.6 for ridge regression  i.e. kernel  ridge regression with a linear kernel  behave as λ → 0?   b  Can you show a stability bound for linear regression  i.e. ridge regression  with λ = 0 ? If not, show a counter-example.   344  Chapter 14 Algorithmic Stability  14.4 Kernel stability. Suppose an approximation of the kernel matrix K, denoted K cid:48 , is used to train the hypothesis h cid:48   and let h denote the non-approximate hypoth-  esis . At test time, no approximation is made, so if we let kx = cid:2 K x, x1 , . . . , K x, xm  cid:3  cid:62  we can write h x  = α cid:62 kx and h cid:48  x  = α cid:48  cid:62 kx. Show that if ∀x, x cid:48  ∈ X, K x, x cid:48   ≤ r then  h cid:48  x  − h x  ≤  rmM λ2  cid:107 K cid:48  − K cid:107 2 .   Hint: Use exercise 10.3   14.5 Stability of relative-entropy regularization.   a  Consider an algorithm that selects a distribution g over a hypothesis class which is parameterized by θ ∈ Θ. Given a point z =  x, y  the expected loss is deﬁned as  H g, z  = cid:90 Θ  L hθ x , y g θ  dθ ,  with respect to a base loss function L. Assuming the loss function L is bounded by M , show that the expected loss H is M -admissible, i.e. show  H g, z  − H g cid:48 , z  ≤ M cid:82 Θ g θ  − g cid:48  θ  dθ.   b  Consider an algorithm that minimizes the entropy regularized objective over  the choice of distribution g:  FS g  =  H g, zi   +λK g, f0  .   cid:124   1 m  m cid:88 i=1  cid:98 RS  g   cid:123  cid:122  K g, f0  = cid:90 Θ   cid:125   Here, K is the Kullback-Leibler divergence  or relative entropy  between two distributions,  g θ  log  g θ  f0 θ   dθ ,   14.14   and f0 is some ﬁxed distribution. Show that such an algorithm is stable by performing the following steps:  i. First use the fact 1  to show  2   cid:82 Θ g θ −g cid:48  θ  dθ 2 ≤ K g, g cid:48    Pinsker’s inequality ,   cid:16  cid:90 Θ gS θ  − gS cid:48  θ  dθ cid:17 2  ≤ BK .,f0  g cid:107 g cid:48   + BK .,f0  g cid:48  cid:107 g  .   14.5 Exercises  345  ii. Next, let g be the minimizer of FS and g cid:48  the minimizer of FS cid:48 , where S  and S cid:48  diﬀer only at the index m. Show that  BK .,f0  g cid:107 g cid:48   + BK .,f0  g cid:48  cid:107 g   ≤  1  mλ cid:12  cid:12 H g cid:48 , zm  − H g, zm  + H g, z cid:48 m  − H g cid:48 , z cid:48 m  cid:12  cid:12   2M  mλ cid:90 Θ g θ  − g cid:48  θ  dθ .  ≤  iii. Finally, combine the results above to show that the entropy regularized  algorithm is 2M 2  mλ -stable.    15 Dimensionality Reduction  In settings where the data has a large number of features, it is often desirable to reduce its dimension, or to ﬁnd a lower-dimensional representation preserving some of its properties. The key arguments for dimensionality reduction  or manifold learning  techniques are:    Computational : to compress the initial data as a preprocessing step to speed up  subsequent operations on the data.    Visualization: to visualize the data for exploratory analysis by mapping the input  data into two- or three-dimensional spaces.    Feature extraction: to hopefully generate a smaller and more eﬀective or useful  set of features.  The beneﬁts of dimensionality reduction are often illustrated via simulated data, such as the Swiss roll dataset. In this example, the input data, depicted in ﬁg- ure 15.1a, is three-dimensional, but it lies on a two-dimensional manifold that is “unfolded” in two-dimensional space as shown in ﬁgure 15.1b. It is important to note, however, that exact low-dimensional manifolds are rarely encountered in practice. Hence, this idealized example is more useful to illustrate the concept of dimensionality reduction than to verify the eﬀectiveness of dimensionality reduction algorithms.  Dimensionality reduction can be formalized as follows. Consider a sample S =   x1, . . . , xm , a feature mapping Φ : X → RN and the data matrix X ∈ RN×m  deﬁned as  Φ x1 , . . . , Φ xm  . The ith data point is represented by xi = Φ xi , or the ith column of X, which is an N -dimensional vector. Dimensionality reduction techniques broadly aim to ﬁnd, for k  cid:28  N , a k-dimensional representation of the data, Y ∈ Rk×m, that is in some way faithful to the original representation X.  In this chapter we will discuss various techniques that address this problem. We ﬁrst present the most commonly used dimensionality reduction technique called principal component analysis  PCA . We then introduce a kernelized version of PCA  KPCA  and show the connection between KPCA and manifold learning   348  Chapter 15 Dimensionality Reduction   a    b   Figure 15.1 The “Swiss roll” dataset.  a  high-dimensional representation.  b  lower-dimensional representa- tion.  algorithms. We conclude with a presentation of the Johnson-Lindenstrauss lemma, a classical theoretical result that has inspired a variety of dimensionality reduction methods based on the concept of random projections. The discussion in this chapter relies on basic matrix properties that are reviewed in appendix A.  15.1 Principal component analysis  Fix k ∈ [N ] and let X be a mean-centered data matrix, that is, cid:80 m  i=1 xi = 0. Deﬁne Pk as the set of N -dimensional rank-k orthogonal projection matrices. PCA consists of projecting the N -dimensional input data onto the k-dimensional linear subspace that minimizes reconstruction error , that is the sum of the squared L2-distances between the original data and the projected data. Thus, the PCA algorithm is completely deﬁned by the orthogonal projection matrix solution P∗ of the following minimization problem:  min  P∈Pk  cid:107 PX − X cid:107 2 F .   15.1   The following theorem shows that PCA coincides with the projection of each data point onto the k top singular vectors of the sample covariance matrix, i.e., C = 1 m XX cid:62  for the mean-centered data matrix X. Figure 15.2 illustrates the basic intuition behind PCA, showing how two-dimensional data points with highly correlated features can be more succinctly represented with a one-dimensional rep- resentation that captures most of the variance in the data. Theorem 15.1 Let P∗ ∈ Pk be the PCA solution, i.e., the orthogonal projection matrix solution of  15.1 . Then, P∗ = UkU cid:62 k , where Uk ∈ RN×k is the matrix   15.2 Kernel principal component analysis  KPCA   349  formed by the top k singular vectors of C = 1 m XX cid:62 , the sample covariance matrix corresponding to X. Moreover, the associated k-dimensional representation of X is given by Y = U cid:62 k X.  Proof: Let P = P cid:62  be an orthogonal projection matrix. By the deﬁnition of the Frobenius norm, the linearity of the trace operator and the fact that P is idempotent, i.e., P2 = P, we observe that   cid:107 PX − X cid:107 2  F = Tr[ PX − X  cid:62  PX − X ] = Tr[X cid:62 P2X − 2X cid:62 PX + X cid:62 X] = − Tr[X cid:62 PX] + Tr[X cid:62 X] .  Since Tr[X cid:62 X] is a constant with respect to P, we have  argmin P∈Pk   cid:107 PX − X cid:107 2  F = argmax P∈Pk  Tr[X cid:62 PX] .   15.2   By deﬁnition of orthogonal projections in Pk, P = UU cid:62  for some U ∈ RN×k  containing orthogonal columns. Using the invariance of the trace operator under cyclic permutations and the orthogonality of the columns of U, we have  Tr[X cid:62 PX] = Tr[U cid:62 XX cid:62 U] =  u cid:62 i XX cid:62 ui ,  k cid:88 i=1  where ui is the ith column of U. By the Rayleigh quotient  section A.2.3 , it is clear that the largest k singular vectors of XX cid:62  maximize the rightmost sum above. Since XX cid:62  and C diﬀer only by a scaling factor, they have the same singular vectors, and thus Uk maximizes this sum, which proves the ﬁrst statement of the theorem. Finally, since PX = UkU cid:62 k X, Y = U cid:62 k X is a k-dimensional  cid:3  representation of X with Uk as the basis vectors. By deﬁnition of the covariance matrix, the top singular vectors of C are the di- rections of maximal variance in the data, and the associated singular values are equal to these variances. Hence, PCA can also be viewed as projecting onto the subspace of maximal variance. Under this interpretation, the ﬁrst principal com- ponent is derived from projection onto the direction of maximal variance, given by the top singular vector of C. Similarly, the ith principal component, for 1 ≤ i ≤ k, is derived from projection onto the ith direction of maximal variance, subject to orthogonality constraints to the previous i − 1 directions of maximal variance  see exercise 15.1 for more details .  15.2 Kernel principal component analysis  KPCA   In the previous section, we presented the PCA algorithm, which involved projecting onto the singular vectors of the sample covariance matrix C. In this section, we   350  Chapter 15 Dimensionality Reduction   a    b   Figure 15.2 Example of PCA.  a  Two-dimensional data points with features capturing shoe size measured with diﬀerent units.  b  One-dimensional representation that captures the most variance in the data, generated by projecting onto largest principal component  red line  of the mean-centered data points.  present a kernelized version of PCA, called KPCA. In the KPCA setting, Φ is a feature mapping to an arbitrary RKHS  not necessarily to RN   and we work exclusively with a kernel function K corresponding to the inner product in this RKHS. The KPCA algorithm can thus be deﬁned as a generalization of PCA in which the input data is projected onto the top principle components in this RKHS. We will show the relationship between PCA and KPCA by drawing upon the deep connections among the SVDs of X, C and K. We then illustrate how various manifold learning algorithms can be interpreted as special instances of KPCA. Let K be a PDS kernel deﬁned over X × X and deﬁne the kernel matrix as K = X cid:62 X. Since X admits the following singular value decomposition: X = UΣV cid:62 , C and K can be rewritten as follows:  C =  UΛU cid:62   K = VΛV cid:62  ,   15.3   1 m  where Λ = Σ2 is the diagonal matrix of the singular values  equivalently eigenval- ues  of mC and U is the matrix of the singular vectors  equivalently eigenvectors  of C  and mC .  Starting with the SVD of X, note that right multiplying by VΣ−1 and using the relationship between Λ and Σ yields U = XVΛ−1 2. Thus, the singular vector u of C associated to the singular value λ m coincides with Xv√λ , where v is the singular vector of K associated to λ. Now ﬁx an arbitrary feature vector x = Φ x  for x ∈ X. Then, following the expression for Y in theorem 15.1, the one-dimensional   15.3 KPCA and manifold learning  351   15.4    15.5   representation of x derived by projection onto Pu = uu cid:62  is deﬁned by  x cid:62 u = x cid:62  Xv √λ  =  k cid:62 x v √λ  ,  where kx =  K x1, x , . . . , K xm, x   cid:62 . If x is one of the data points, i.e., x = xi for 1 ≤ i ≤ m, then kx is the ith column of K and  15.4  can be simpliﬁed as follows:  x cid:62 u =  k cid:62 x v √λ  =  λvi√λ  = √λvi ,  where vi is the ith component of v. More generally, the PCA solution of theo- rem 15.1 can be fully deﬁned by the top k singular vectors  or eigenvectors  of K, v1, . . . , vk, and the corresponding singular values  or eigenvalues . This alternative derivation of the PCA solution in terms of K precisely deﬁnes the KPCA solution, providing a generalization of PCA via the use of PDS kernels  see chapter 6 for more details on kernel methods .  15.3 KPCA and manifold learning  Several manifold learning techniques have been proposed as non-linear methods for dimensionality reduction. These algorithms implicitly assume that high-dimensional data lie on or near a low-dimensional non-linear manifold embedded in the input space. They aim to learn this manifold structure by ﬁnding a low-dimensional space that in some way preserves the local structure of high-dimensional input data. For instance, the Isomap algorithm aims to preserve approximate geodesic distances, or distances along the manifold, between all pairs of data points. Other algorithms, such as Laplacian eigenmaps and locally linear embedding, focus only on preserv- ing local neighborhood relationships in the high-dimensional space. We will next describe these classical manifold learning algorithms and then interpret them as speciﬁc instances of KPCA.  Isomap  15.3.1 Isomap aims to extract a low-dimensional data representation that best preserves all pairwise distances between input points, as measured by their geodesic distances along the underlying manifold. It approximates geodesic distance assuming that L2 distance provides good approximations for nearby points, and for faraway points it estimates distance as a series of hops between neighboring points. The Isomap algorithm works as follows:   352  Chapter 15 Dimensionality Reduction  1. Find the t nearest neighbors for each data point based on L2 distance and construct an undirected neighborhood graph, denoted by G, with points as nodes and links between neighbors as edges.  2. Compute the approximate geodesic distances, ∆ij, between all pairs of nodes  i, j  by computing all-pairs shortest distances in G using, for instance, the Floyd-Warshall algorithm.  3. Convert the squared distance matrix into a m×m similarity matrix by perform- ing double centering, i.e., compute KIso = − 1 2 H∆H, where ∆ is the squared distance matrix, H = Im − 1 m 11 cid:62  is the centering matrix, Im is the m × m identity matrix and 1 is a column vector of all ones  for more details on double centering see exercise 15.2 .  4. Find the optimal k-dimensional representation, Y = {yi}n  argminY cid:48  cid:80 i,j cid:0  cid:107 y cid:48 i − y cid:48 j cid:107 2  2 − ∆2  ij cid:1 . The solution is given by,  Y =  ΣIso,k 1 2U cid:62 Iso,k  i=1, such that Y =   15.6   where ΣIso,k is the diagonal matrix of the top k singular values of KIso and UIso,k are the associated singular vectors.  KIso can naturally be viewed as a kernel matrix, thus providing a simple connection between Isomap and KPCA. Note, however, that this interpretation is valid only when KIso is in fact positive semideﬁnite, which is indeed the case in the continuum limit for a smooth manifold.  15.3.2 Laplacian eigenmaps The Laplacian eigenmaps algorithm aims to ﬁnd a low-dimensional representation that best preserves neighborhood relations as measured by a weight matrix W. The algorithm works as follows:  1. Find t nearest neighbors for each point.  xj cid:107 2  2. Construct W, a sparse, symmetric m × m matrix, where Wij = exp cid:0  −  cid:107 xi − 2 σ2 cid:1  if  xi, xj  are neighbors, 0 otherwise, and σ is a scaling parameter. 3. Construct the diagonal matrix D, such that Dii = cid:80 j Wij.  4. Find the k-dimensional representation by minimizing the weighted distance be-  tween neighbors as,  Y = argmin  Y cid:48   cid:88 i,j  Wij cid:107 y cid:48 i − y cid:48 j cid:107 2 2.   15.7   This objective function penalizes nearby inputs for being mapped to faraway outputs, with “nearness” measured by the weight matrix W. The solution to the minimization in  15.7  is Y = U cid:62 L,k, where L = D − W is the graph Laplacian   15.3 KPCA and manifold learning  353  and U cid:62 L,k are the bottom k singular vectors of L, excluding the last singular vector corresponding to the singular value 0  assuming that the underlying neighborhood graph is connected .  The solution to  15.7  can also be interpreted as ﬁnding the largest singular vec- tors of L†, the pseudo-inverse of L. Deﬁning KL = L† we can thus view Laplacian Eigenmaps as an instance of KPCA in which the output dimensions are normalized to have unit variance, which corresponds to setting λ = 1 in  15.5 . Moreover, it can be shown that KL is the kernel matrix associated with the commute times of diﬀusion on the underlying neighborhood graph, where the commute time between nodes i and j in a graph is the expected time taken for a random walk to start at node i, reach node j and then return to i.  15.3.3 Locally linear embedding  LLE  The locally linear embedding  LLE  algorithm also aims to ﬁnd a low-dimensional representation that preserves neighborhood relations as measured by a weight ma- trix W. The algorithm works as follows:  1. Find t nearest neighbors for each point. 2. Construct W, a sparse, symmetric m×m matrix, whose ith row sums to one and contains the linear coeﬃcients that optimally reconstruct xi from its t neighbors. More speciﬁcally, if we assume that the ith row of W sums to one, then the reconstruction error is   cid:16 xi −  cid:88 j∈Ni  Wijxj cid:17 2  = cid:16   cid:88 j∈Ni  Wij xi − xj  cid:17 2  =  cid:88 j,k∈Ni  WijWikC cid:48 jk   15.8   where Ni is the set of indices of the neighbors of point xi and C cid:48 jk =  xi − xj  cid:62  xi − xk  the local covariance matrix. Minimizing this expression with the constraint cid:80 j Wij = 1 gives the solution   15.9   Wij =  cid:80 k C cid:48 −1 jk  cid:80 st C cid:48 −1 st  .  Note that the solution can be equivalently obtained by ﬁrst solving the system  of linear equations cid:80 j C cid:48 kjWij = 1, for k ∈ Ni, and then normalizing so that  the weights sum to one.  3. Find the k-dimensional representation that best obeys neighborhood relations  as speciﬁed by W, i.e.,  Y = argmin  Y cid:48   cid:88 i  cid:16 y cid:48 i − cid:88 j  Wijy cid:48 j cid:17 2  .   15.10    354  Chapter 15 Dimensionality Reduction  The solution to the minimization in  15.10  is Y = U cid:62 M,k, where M =  I − W cid:62   I − W cid:62   and U cid:62 M,k are the bottom k singular vectors of M, excluding the last singular vector corresponding to the singular value 0.  As discussed in exercise 15.5, LLE coincides with KPCA used with a particular kernel matrix KLLE whereby the output dimensions are normalized to have unit variance  as in the case of Laplacian Eigenmaps .  15.4  Johnson-Lindenstrauss lemma  The Johnson-Lindenstrauss lemma is a fundamental result in dimensionality reduc- tion that states that any m points in high-dimensional space can be mapped to a much lower dimension, k ≥ O  log m  cid:15 2  , without distorting pairwise distance between any two points by more than a factor of  1 ±  cid:15  . In fact, such a mapping can be found in randomized polynomial time by projecting the high-dimensional points onto randomly chosen k-dimensional linear subspaces. The Johnson-Lindenstrauss lemma is formally presented in lemma 15.4. The proof of this lemma hinges on lemma 15.2 and lemma 15.3, and it is an example of the “probabilistic method”, in which probabilistic arguments lead to a deterministic statement. Moreover, as we will see, the Johnson-Lindenstrauss lemma follows by showing that the squared norm of a random vector is sharply concentrated around its mean when the vector is projected onto a k-dimensional random subspace.  First, we prove the following property of the χ2-squared distribution  see deﬁni-  tion C.7 in appendix , which will be used in lemma 15.3. Lemma 15.2 Let Q be a random variable following a χ2-squared distribution with k degrees of freedom. Then, for any 0 <  cid:15  < 1 2, the following inequality holds:  P[ 1 −  cid:15  k ≤ Q ≤  1 +  cid:15  k] ≥ 1 − 2e−  cid:15 2− cid:15 3 k 4 .   15.11   Proof: By Markov’s inequality, we can write  P[Q ≥  1 +  cid:15  k] = P[exp λQ  ≥ exp λ 1 +  cid:15  k ] ≤  E[exp λQ ] exp λ 1 +  cid:15  k   1 − 2λ −k 2 exp λ 1 +  cid:15  k   ,  =  where we used for the ﬁnal equality the expression of the moment-generating func- tion of a χ2-squared distribution, E[exp λQ ], for λ < 1 2  equation  C.25  . Choosing λ =  cid:15  2 1+ cid:15   < 1 2, which minimizes the right-hand side of the ﬁnal equal- ity, and using the inequality 1 +  cid:15  ≤ exp  cid:15  −   cid:15 2 −  cid:15 3  2  yield  P[Q ≥  1 +  cid:15  k] ≤ cid:18  1 +  cid:15   exp  cid:15   cid:19 k 2  ≤ cid:18  exp cid:0  cid:15  −  cid:15 2− cid:15 3  exp  cid:15    2   cid:1    cid:19 k 2  = exp cid:16  −  k 4    cid:15 2 −  cid:15 3  cid:17  .   15.4 Johnson-Lindenstrauss lemma  355  The statement of the lemma follows by using similar techniques to bound P[Q ≤  cid:3   1 −  cid:15  k] and by applying the union bound. Lemma 15.3 Let x ∈ RN , deﬁne k < N and assume that entries in A ∈ Rk×N are  sampled independently from the standard normal distribution, N  0, 1 . Then, for any 0 <  cid:15  < 1 2,  1 √k  P cid:20  1 −  cid:15   cid:107 x cid:107 2 ≤  cid:107  j ] = E cid:20  cid:16  N cid:88 i=1 E[ cid:98 x2  Proof: Let cid:98 x = Ax and observe that  Ax cid:107 2 ≤  1 +  cid:15   cid:107 x cid:107 2 cid:21  ≥ 1 − 2e−  cid:15 2− cid:15 3 k 4 . Ajixi cid:17 2 cid:21  = E cid:20  N cid:88 i=1  i cid:21  =  N cid:88 i=1  x2 i =  cid:107 x cid:107 2 .  jix2  A2  The second and third equalities follow from the independence and unit variance,  j=1 T 2  independent standard normal random variables since the Aij are i.i.d. standard j ] =  cid:107 x cid:107 2. Thus, the variable Q deﬁned by Q = j follows a χ2-squared distribution with k degrees of freedom and we have  respectively, of the Aij. Now, deﬁne Tj =  cid:98 xj  cid:107 x cid:107  and note that the Tjs are normal random variables and E[ cid:98 x2  cid:80 k P cid:20  1 −  cid:15   cid:107 x cid:107 2 ≤  cid:107  cid:98 x cid:107 2  k ≤  1 +  cid:15   cid:107 x cid:107 2 cid:21  = P cid:20  1 −  cid:15  k ≤  j ≤  1 +  cid:15  k cid:21   k cid:88 j=1  T 2   15.12   = P cid:20  1 −  cid:15  k ≤ Q ≤  1 +  cid:15  k cid:21  ≥ 1 − 2e−  cid:15 2− cid:15 3 k 4 ,  where the ﬁnal inequality holds by lemma 15.2, thus proving the statement of the  cid:3  lemma.  Lemma 15.4  Johnson-Lindenstrauss  For any 0   4, let k = 20 log m such that for all u, v ∈ V ,  . Then for any set V of m points in RN , there exists a map f : RN → Rk   cid:15 2   1 −  cid:15   cid:107 u − v cid:107 2 ≤  cid:107 f  u  − f  v  cid:107 2 ≤  1 +  cid:15   cid:107 u − v cid:107 2.   15.13   A where k < N and entries in A ∈ Rk×N are sampled inde- Proof: Let f = 1√k pendently from the standard normal distribution, N  0, 1 . For ﬁxed u, v ∈ V , we can apply lemma 15.3, with x = u − v, to lower bound the success probability by 1 − 2e−  cid:15 2− cid:15 3 k 4. Applying the union bound over the O m2  pairs in V , setting k = 20   cid:15 2 log m and upper bounding  cid:15  by 1 2, we have  P[success] ≥ 1 − 2m2e−  cid:15 2− cid:15 3 k 4 = 1 − 2m5 cid:15 −3 > 1 − 2m−1 2 > 0 .  Since the success probability is strictly greater than zero, a map that satisﬁes the  cid:3  desired conditions must exist, thus proving the statement of the lemma.   356  Chapter 15 Dimensionality Reduction  15.5 Chapter notes  PCA was introduced in the early 1900s by Pearson [1901]. KPCA was introduced roughly a century later, and our presentation of KPCA is a more concise derivation of results given by Mika et al. [1999]. Isomap and LLE were pioneering works on non-linear dimensionality reduction introduced by Tenenbaum et al. [2000], Roweis and Saul [2000]. Isomap itself is a generalization of a standard linear dimensionality reduction technique called Multidimensional Scaling [Cox and Cox, 2000]. Isomap and LLE led to the development of several related algorithms for manifold learning, e.g., Laplacian Eigenmaps and Maximum Variance Unfolding [Belkin and Niyogi, 2001, Weinberger and Saul, 2006]. As shown in this chapter, classical manifold learning algorithms are special instances of KPCA [Ham et al., 2004]. The Johnson- Lindenstrauss lemma was introduced by Johnson and Lindenstrauss [1984], though our proof of the lemma follows Vempala [2004]. Other simpliﬁed proofs of this lemma have also been presented, including Dasgupta and Gupta [2003].  15.6 Exercises  15.1 PCA and maximal variance. Let X be an uncentered data matrix and let  ¯x = 1  m cid:80 i xi be the sample mean of the columns of X.   a  Show that the variance of one-dimensional projections of the data onto an  arbitrary vector u equals u cid:62 Cu, where C = 1 sample covariance matrix.  m cid:80 i xi − ¯x  xi − ¯x  cid:62  is the   b  Show that PCA with k = 1 projects the data onto the direction  i.e., u cid:62 u =  1  of maximal variance.  15.2 Double centering. In this problem we will prove the correctness of the double centering step in Isomap when working with Euclidean distances. Deﬁne X and ¯x as in exercise 15.1, and deﬁne X∗ as the centered version of X, that is, let x∗i = xi − ¯x be the ith column of X∗. Let K = X cid:62 X, and let D denote the Euclidean distance matrix, i.e., Dij =  cid:107 xi − xj cid:107 .  a  Show that Kij = 1 2  Kii + Kjj + D2 ij .  b  Show that K∗ = X∗ cid:62 X∗ = K − 1 m K11 cid:62  − 1  c  Using the results from  a  and  b  show that  m2 11 cid:62 K11 cid:62 .  m 11 cid:62 K + 1  K∗ij = −  1  2 cid:20 D2  ij −  1 m  m cid:88 k=1  D2  ik −  1 m  D2  kj + ¯D cid:21  ,  m cid:88 k=1   15.6 Exercises  357  where ¯D = 1  u,v is the mean of the m2 entries in D.   d  Show that K∗ = − 1  2 HDH, where H = Im − 1  m 11 cid:62 .  m2 cid:80 u cid:80 v D2  15.3 Laplacian eigenmaps. Assume k = 1 and we seek a one-dimensional representa- tion y. Show that  15.7  is equivalent to y = argminy cid:48  y cid:48  cid:62 Ly cid:48 , where L is the graph Laplacian.  15.4 Nystr¨om method. Deﬁne the following block representation of a kernel matrix:  K = cid:34  W K cid:62 21 K21 K22 cid:35   and C = cid:34  W K21 cid:35  .  The Nystr¨om method uses W ∈ Rl×l and C ∈ Rm×l to generate the approxi- mation  cid:101 K = CW†C cid:62  ≈ K.  a  Show that W is SPSD and that  cid:107 K −  cid:101 K cid:107 F =  cid:107 K22 − K21W†K cid:62 21 cid:107 F .  b  Let K = X cid:62 X for some X ∈ RN×m, and let X cid:48  ∈ RN×l be the ﬁrst l columns of X. Show that  cid:101 K = X cid:62 PUX cid:48  X, where PUX cid:48  is the orthogonal projection  c  Is  cid:101 K SPSD?  d  If rank K  = rank W  = r  cid:28  m, show that  cid:101 K = K. Note: this statement  holds whenever rank K  = rank W , but is of interest mainly in the low- rank setting.  onto the span of the left singular vectors of X cid:48 .   e  If m = 20M and K is a dense matrix, how much space is required to store K if each entry is stored as a double? How much space is required by the Nystr¨om method if l = 10K?  15.5 Expression for KLLE. Show the connection between LLE and KPCA by deriving  the expression for KLLE.  15.6 Random projection, PCA, and nearest neighbors.   a  Download the MNIST test set of handwritten digits at:  http:  yann.lecun.com exdb mnist t10k-images-idx3-ubyte.gz.  Create a data matrix X ∈ RN×m from the ﬁrst m = 2,000 instances of this  dataset  the dimension of each instance should be N = 784 .   b  Find the ten nearest neighbors for each point in X, that is, compute Ni,10 for 1 ≤ i ≤ m, where Ni,t denotes the set of the t nearest neighbors for the   358  Chapter 15 Dimensionality Reduction  ith datapoint and nearest neighbors are deﬁned with respect to the L2 norm. Also compute Ni,50 for all i.   c  Generate ˜X = AX, where A ∈ Rk×N , k = 100 and entries of A are sampled  independently from the standard normal distribution. Find the ten nearest neighbors for each point in ˜X, that is, compute ˜Ni,10 for 1 ≤ i ≤ m. m cid:80 m   d  Report the quality of approximation by computing score10 = 1 i=1 Ni,50 ∩ ˜Ni,10.  ˜Ni,10. Similarly, compute score50 = 1  i=1 Ni,10∩   e  Generate two plots that show score10 and score50 as functions of k  i.e., perform steps  c  and  d  for k = {1, 10, 50, 100, 250, 500} . Provide a one- or two-sentence explanation of these plots.  m cid:80 m   f  Generate similar plots as in  e  using PCA  with various values of k  to generate ˜X and subsequently compute nearest neighbors. Are the nearest neighbor approximations generated via PCA better or worse than those gen- erated via random projections? Explain why.   16 Learning Automata and Languages  This chapter presents an introduction to the problem of learning languages. This is a classical problem explored since the early days of formal language theory and computer science, and there is a very large body of literature dealing with related mathematical questions. In this chapter, we present a brief introduction to this problem and concentrate speciﬁcally on the question of learning ﬁnite automata, which, by itself, has been a topic investigated in multiple forms by thousands of technical papers. We will examine two broad frameworks for learning automata, and for each, we will present an algorithm. In particular, we describe an algorithm for learning automata in which the learner has access to several types of query, and we discuss an algorithm for identifying a sub-class of the family of automata in the limit.  16.1  Introduction  Learning languages is one of the earliest problems discussed in linguistics and com- puter science. It has been prompted by the remarkable faculty of humans to learn natural languages. Humans are capable of uttering well-formed new sentences at an early age, after having been exposed only to ﬁnitely many sentences. Moreover, even at an early age, they can make accurate judgments of grammaticality for new sentences.  In computer science, the problem of learning languages is directly related to that of learning the representation of the computational device generating a language. Thus, for example, learning regular languages is equivalent to learning ﬁnite au- tomata, or learning context-free languages or context-free grammars is equivalent to learning pushdown automata.  There are several reasons for examining speciﬁcally the problem of learning ﬁnite automata. Automata provide natural modeling representations in a variety of dif- ferent domains including systems, networking, image processing, text and speech   360  Chapter 16 Learning Automata and Languages   a    b   Figure 16.1  a  A graphical representation of a ﬁnite automaton. automaton.   b  Equivalent  minimal  deterministic  processing, logic and many others. Automata can also serve as simple or eﬃ- cient approximations for more complex devices. For example, in natural language processing, they can be used to approximate context-free languages. When it is possible, learning automata is often eﬃcient, though, as we shall see, the problem is hard in a number of natural scenarios. Thus, learning more complex devices or languages is even harder.  We consider two general learning frameworks: the model of eﬃcient exact learning and the model of identiﬁcation in the limit. For each of these models, we brieﬂy discuss the problem of learning automata and describe an algorithm.  We ﬁrst give a brief review of some basic automata deﬁnitions and algorithms, then discuss the problem of eﬃcient exact learning of automata and that of the identiﬁcation in the limit.  16.2 Finite automata  We will denote by Σ a ﬁnite alphabet. The length of a string x ∈ Σ∗ over that alphabet is denoted by x. The empty string is denoted by  cid:15 , thus  cid:15  = 0. For any string x = x1 ··· xk ∈ Σ∗ of length k ≥ 0, we denote by x[j] = x1 ··· xj its preﬁx of length j ≤ k and deﬁne x[0] as  cid:15 . Finite automata are labeled directed graphs equipped with initial and ﬁnal states.  The following gives a formal deﬁnition of these devices.  Deﬁnition 16.1  Finite automata  A ﬁnite automaton A is a 5-tuple  Σ, Q, I, F, E  where Σ is a ﬁnite alphabet, Q a ﬁnite set of states, I ⊆ Q a set of initial states, F ⊆ Q a set of ﬁnal states, and E ⊆ Q ×  Σ ∪ { cid:15 }  × Q a ﬁnite set of transitions. Figure 16.1a shows a simple example of a ﬁnite automaton. States are represented by circles. A bold circle indicates an initial state, a double circle a ﬁnal state. Each transition is represented by an arrow from its origin state to its destination state with its label in Σ ∪ { cid:15 }.  b  0  ε  b  a  3  a  b  a  1  2  b  0  a  a  1  2  b  a  b  3   16.3 Efﬁcient exact learning  361  A path from an initial state to a ﬁnal state is said to be an accepting path. An automaton is said to be trim if all of its states are accessible from an initial state and admit a path to a ﬁnal state, that is, if all of its states lie on an accepting path. A string x ∈ Σ∗ is accepted by an automaton A iﬀ x labels an accepting path. For convenience, we will say that x ∈ Σ∗ is rejected by A when it is not accepted. The set of all strings accepted by A deﬁnes the language accepted by A denoted by L A . The class of languages accepted by ﬁnite automata coincides with the family of regular languages, that is, languages that can be described by regular expressions.  Any ﬁnite automaton admits an equivalent automaton with no  cid:15 -transition, that is, no transition labeled with the empty string: there exists a general  cid:15 -removal algorithm that takes as input an automaton and returns an equivalent automaton with no  cid:15 -transition.  An automaton with no  cid:15 -transition is said to be deterministic if it admits a unique initial state and if no two transitions sharing the same label leave any given state. A deterministic ﬁnite automaton is often referred to by the acronym DFA, while the acronym NFA is used for arbitrary automata, that is, non-deterministic ﬁnite automata. Any NFA admits an equivalent DFA: there exists a general  exponential- time  determinization algorithm that takes as input an NFA with no  cid:15 -transition and returns an equivalent DFA. Thus, the class of languages accepted by DFAs coincides with that of the languages accepted by NFAs, that is regular languages. For any string x ∈ Σ∗ and DFA A, we denote by A x  the state reached in A when reading x from its unique initial state. A DFA is said to be minimal if it admits no equivalent deterministic automaton with a smaller number of states. There exists a general minimization algorithm taking as input a deterministic automaton and returning a minimal one that runs in O E log Q . When the input DFA is acyclic, that is when it admits no path forming a cycle, it can be minimized in linear time O Q+E . Figure 16.1b shows the minimal DFA equivalent to the NFA of ﬁgure 16.1a.  16.3 Efﬁcient exact learning  In the eﬃcient exact learning framework, the problem consists of identifying a tar- get concept c from a ﬁnite set of examples in time polynomial in the size of the representation of the concept and in an upper bound on the size of the representa- tion of an example. Unlike the PAC-learning framework, in this model, there is no stochastic assumption, instances are not assumed to be drawn according to some unknown distribution. Furthermore, the objective is to identify the target concept   362  Chapter 16 Learning Automata and Languages  exactly, without any approximation. A concept class C is said to be eﬃciently exactly learnable if there is an algorithm for eﬃcient exact learning of any c ∈ C. We will consider two diﬀerent scenarios within the framework of eﬃciently exact learning: a passive and an active learning scenario. The passive learning scenario is similar to the standard supervised learning scenario discussed in previous chapters but without any stochastic assumption: the learning algorithm passively receives data instances as in the PAC model and returns a hypothesis, but here, instances are not assumed to be drawn from any distribution. In the active learning scenario, the learner actively participates in the selection of the training samples by using various types of queries that we will describe. In both cases, we will focus more speciﬁcally on the problem of learning automata.  16.3.1 Passive learning The problem of learning ﬁnite automata in this scenario is known as the minimum consistent DFA learning problem . It can be formulated as follows: the learner receives a ﬁnite sample S =   x1, y1 , . . . ,  xm, ym   with xi ∈ Σ∗ and yi ∈ {−1, +1} for any i ∈ [m]. If yi = +1, then xi is an accepted string, otherwise it is rejected. The problem consists of using this sample to learn the smallest DFA A consistent with S, that is the automaton with the smallest number of states that accepts the strings of S with label +1 and rejects those with label −1. Note that seeking the smallest DFA consistent with S can be viewed as following Occam’s razor principle. The problem just described is distinct from the standard minimization of DFAs. A minimal DFA accepting exactly the strings of S labeled positively may not have the smallest number of states: in general there may be DFAs with fewer states accepting a superset of these strings and rejecting the negatively labeled sample strings. For example, in the simple case S =   a, +1 ,  b,−1  , a minimal deter- ministic automaton accepting the unique positively labeled string a or the unique negatively labeled string b admits two states. However, the deterministic automa- ton accepting the language a∗ accepts a and rejects b and has only one state.  Passive learning of ﬁnite automata turns out to be a computationally hard prob- lem. The following theorems present several negative results known for this prob- lem.  Theorem 16.2 The problem of ﬁnding the smallest deterministic automaton consis- tent with a set of accepted or rejected strings is NP-complete.  Hardness results are known even for a polynomial approximation, as stated by the following theorem. Theorem 16.3 If P  cid:54 = NP, then, no polynomial-time algorithm can be guaranteed to ﬁnd a DFA consistent with a set of accepted or rejected strings of size smaller than   16.3 Efﬁcient exact learning  363  a polynomial function of the smallest consistent DFA, even when the alphabet is reduced to just two elements.  Other strong negative results are known for passive learning of ﬁnite automata under various cryptographic assumptions.  These negative results for passive learning invite us to consider alternative learn- ing scenarios for ﬁnite automata. The next section describes a scenario leading to more positive results where the learner can actively participate in the data selection process using various types of queries.  16.3.2 Learning with queries The model of learning with queries corresponds to that of a  minimal  teacher or oracle and an active learner. In this model, the learner can make the following two types of queries to which an oracle responds:    membership queries: the learner requests the target label f  x  ∈ {−1, +1} of an instance x and receives that label;    equivalence queries: the learner conjectures hypothesis h; it receives the response  yes if h = f , a counter-example otherwise.  We will say that a concept class C is eﬃciently exactly learnable with membership and equivalence queries when it is eﬃciently exactly learnable within this model.  This model is not realistic, since no such oracle is typically available in practice. Nevertheless, it provides a natural framework, which, as we shall see, leads to positive results. Note also that for this model to be signiﬁcant, equivalence must be computationally testable. This would not be the case for some concept classes such as that of context-free grammars, for example, for which the equivalence problem is undecidable. In fact, equivalence must be further eﬃciently testable, otherwise the response to the learner cannot be supplied in a reasonable amount of time.21  Eﬃcient exact learning within this model of learning with queries implies the following variant of PAC-learning: we will say that a concept class C is PAC- learnable with membership queries if it is PAC-learnable by an algorithm that has access to a polynomial number of membership queries.  Theorem 16.4 Let C be a concept class that is eﬃciently exactly learnable with mem- bership and equivalence queries, then C is PAC-learnable using membership queries. Proof: Let A be an algorithm for eﬃciently exactly learning C using membership and equivalence queries. Fix  cid:15 , δ > 0. We replace in the execution of A for learning  21 For a human oracle, answering membership queries may also become very hard in some cases when the queries are near the class boundaries. This may also make the model diﬃcult to adopt in practice.   364  Chapter 16 Learning Automata and Languages  target c ∈ C, each equivalence query by a test of the current hypothesis on a polynomial number of labeled examples. Let D be the distribution according to which points are drawn. To simulate the tth equivalence query, we draw mt = 1  cid:15   log 1 δ + t log 2  points i.i.d. according to D to test the current hypothesis ht. If ht is consistent with all of these points, then the algorithm stops and returns ht. Otherwise, one of the points drawn does not belong to ht, which provides a counter-example. Since A learns c exactly, it makes at most T equivalence queries, where T is polynomial in the size of the representation of the target concept and in an upper bound on the size of the representation of an example. Thus, if no equivalence query is positively responded by the simulation, the algorithm will terminate after T equivalence queries and return the correct concept c. Otherwise, the algorithm stops at the ﬁrst equivalence query positively responded by the simulation. The hypothesis it returns is not an  cid:15 -approximation only if the equivalence query stop- ping the algorithm is incorrectly responded positively. By the union bound, since  for any ﬁxed t ∈ [T ], P[R ht  >  cid:15 ] ≤  1−  cid:15  mt, the probability that for some t ∈ [T ],  R ht  >  cid:15  can be bounded as follows:  P[∃t ∈ [T ] : R ht  >  cid:15 ] ≤  P[R ht  >  cid:15 ]  T cid:88 t=1 T cid:88 t=1  1 −  cid:15  mt ≤  T cid:88 t=1  ≤  e−mt cid:15  ≤  δ 2t ≤  T cid:88 t=1  +∞ cid:88 t=1  δ 2t = δ.  1  an  cid:15 -approximation. Finally, the maximum number of points drawn is cid:80 T  Thus, with probability at least 1 − δ, the hypothesis returned by the algorithm is t=1 mt =  cid:15   T log 1 log 2 , which is polynomial in 1  cid:15 , 1 δ, and T . Since the rest of the computational cost of A is also polynomial by assumption, this proves the  cid:3  PAC-learning of C.  δ + T  T +1   2  16.3.3 Learning automata with queries In this section, we describe an algorithm for eﬃcient exact learning of DFAs with membership and equivalence queries. We will denote by A the target DFA and by   cid:98 A the DFA that is the current hypothesis of the algorithm. For the discussion of  the algorithm, we assume without loss of generality that A is a minimal DFA.  The algorithm uses two sets of strings, U and V . U is a set of access strings: reading an access string u ∈ U from the initial state of A leads to a state A u . The algorithm ensures that the states A u , u ∈ U , are all distinct. To do so, it uses a set V of distinguishing strings. Since A is minimal, for two distinct states q and q cid:48  of A, there must exist at least one string that leads to a ﬁnal state from q and not from q cid:48 , or vice versa. That string helps distinguish q and q cid:48 . The set of strings V   16.3 Efﬁcient exact learning  365   a    b    c   Figure 16.2   a  Classiﬁcation tree T , with U = { cid:15 , b, ba} and V = { cid:15 , a}.  b  Current automaton  cid:98 A constructed  using T .  c  Target automaton A.  help distinguish any pair of access strings in U . They deﬁne in fact a partition of all strings of Σ∗.  The objective of the algorithm is to ﬁnd at each iteration a new access string distinguished from all previous ones, ultimately obtaining a number of access strings equal to the number of states of A. It can then identify each state A u  of A with its access string u. To ﬁnd the destination state of the transition labeled with a ∈ Σ leaving state u, it suﬃces to determine, using the partition induced by V the access string u cid:48  that belongs to the same equivalence class as ua. The ﬁnality of each state can be determined in a similar way.  Both sets U and V are maintained by the algorithm via a binary decision tree T similar to those presented in chapter 9. Figure 16.2a shows an example. T deﬁnes the partition of all strings induced by the distinguishing strings V . The leaves of T are each labeled with a distinct u ∈ U and its internal nodes with a string v ∈ V . The decision tree question deﬁned by v ∈ V , given a string x ∈ Σ∗, is whether xv is accepted by A, which is determined via a membership query. If accepted, x is assigned to right sub-tree, otherwise to the left sub-tree, and the same is applied recursively with the sub-trees until a leaf is reached. We denote by T  x  the label of the leaf reached. For example, for the tree T of ﬁgure 16.2a and target automaton A of ﬁgure 16.2c, T  baa  = b since baa is not accepted by A  root question  and baaa is  question at node a . At its initialization step, the algorithm ensures that the root node is labeled with  cid:15 , which is convenient to check the ﬁnality of the strings.  The tentative hypothesis DFA  cid:98 A can be constructed from T as follows. We denote by ConstructAutomaton   the corresponding function. A distinct state  cid:98 A u  is created for each leaf u ∈ U . The ﬁnality of a state  cid:98 A u  is determined based on the sub-tree of the root node that u belongs to:  cid:98 A u  is made ﬁnal iﬀ u belongs  B Example a leaving y by examining    from a classiﬁcation tree T:  T y to each leaf of the tree with   Classiﬁcation tree  B  a  ba  a  !  !  a leaving y by examining   !  ba  a  ba  page  Courant Institute, NYU  a  !  !  b  a  ε  b  b  b  a  a  b  ba  a  0  b  2  b  3  b  b  a  1  a  a   366  Chapter 16 Learning Automata and Languages  QueryLearnAutomata    1 2 T ← T0 3  t ← MembershipQuery  cid:15    cid:98 A ← A0 4 while  EquivalenceQuery  cid:98 A   cid:54 = true  do  x ← CounterExample   if  T = T0  then  5  6  7  8  9  10  T ← T1  cid:46  nil replaced with x. else j ← argmink A x[k]   cid:54 ≡T  cid:98 A x[k]  Split  cid:98 A x[j − 1]    cid:98 A ← ConstructAutomaton T    11 return  cid:98 A  Figure 16.3 Algorithm for learning automata with membership and equivalence queries. A0 is a single-state automaton with self-loops labeled with all a ∈ Σ. That state is initial. It is ﬁnal iﬀ t = true. T0 is a tree with root node labeled with  cid:15  and two leaves, one labeled with  cid:15 , the other with nil. the right leaf is labeled with  cid:15  labels iﬀ t = true. T1 is the tree obtained from T0 by replacing nil with x.  to the right sub-tree that is iﬀ u =  cid:15 u is accepted by A. The destination of the  Figure 16.3 shows the pseudocode of the algorithm. The initialization steps at lines 1–3 construct a tree T with a single internal node labeled with  cid:15  and one leaf string labeled with  cid:15 , the other left undetermined and labeled with nil. They also  transition labeled with a ∈ Σ leaving state  cid:98 A u  is the state  cid:98 A v  where v = T  ua . Figure 16.2b shows the DFA  cid:98 A constructed from the decision tree of ﬁgure 16.2a. For convenience, for any x ∈ Σ∗, we denote by U   cid:98 A x   the access string identifying state  cid:98 A x . deﬁne a tentative DFA  cid:98 A with a single state with self-loops labeled with all elements At each iteration of the loop of lines 4–11, an equivalence query is used. If  cid:98 A  of the alphabet. That single state is an initial state. It is made a ﬁnal state only if  cid:15  is accepted by the target DFA A, which is determined via the membership query of line 1.  is not equivalent to A, then a counter-example string x is received  line 5 . If T is the tree constructed in the initialization step, then the leaf labeled with nil is replaced with x  lines 6–7 . Otherwise, since x is a counter-example, states A x    16.3 Efﬁcient exact learning  367  Figure 16.4  be both determined using the tree T and membership queries  line 8 .  has the same ﬁnality as the initial state A  cid:15   of A. The equivalence of A x[j]  and  the same label xj to two non-equivalent states. Let v be a distinguishing string for  Illustration of the splitting procedure Split  cid:98 A x[j − 1]  . and  cid:98 A x  have a diﬀerent ﬁnality; thus, the string x deﬁning A x  and the access string U   cid:98 A x   are assigned to diﬀerent equivalence classes by T . Thus, there exists a smallest j such that A x[j]  and  cid:98 A x[j]  are not equivalent, that is, such that the preﬁx x[j] of x and the access string U   cid:98 A x[j]   are assigned to diﬀerent leaves by T . j cannot be 0 since the initialization ensures that  cid:98 A  cid:15   is an initial state and  cid:98 A x[j]  is tested by checking the equality of T  x[j]  and T  U   cid:98 A x[j]   , which can Now, by deﬁnition, A x[j − 1]  and  cid:98 A x[j − 1]  are equivalent, that is T assigns x[j−1] to the leaf labeled with U   cid:98 A x[j−1]  . But, x[j−1] and U   cid:98 A x[j−1]   must be distinguished since A x[j − 1]  and  cid:98 A x[j − 1]  admit transitions labeled with A x[j]  and  cid:98 A x[j] . v can be obtained as the least common ancestor of the leaves labeled with x[j] and U   cid:98 A x[j]  . To distinguish x[j − 1] and U   cid:98 A x[j − 1]  , it string x[j − 1] which, by construction, is distinguished from U   cid:98 A x[j − 1]   and all Thus, the number of access strings  or states of  cid:98 A  increases by one at each are of the form A u  for a distinct u ∈ U . A and  cid:98 A have then the same number of states and in fact A =  cid:98 A. Indeed, let  A u , a, A u cid:48    be a transition in A, then labeled with u cid:48 . The destination of the transition from  cid:98 A u  with label a is found is, u cid:48 . Thus, by construction, the same transition   cid:98 A u , a,  cid:98 A u cid:48    is created in  cid:98 A.  by deﬁnition the equality A ua  = A u cid:48   holds. The tree T deﬁnes a partition of all strings in terms of their distinguishing strings in A. Since in A, ua and u cid:48  lead to the same state, they are assigned to the same leaf by T , that is, the leaf  suﬃces to split the leaf of T labeled with T  x[j − 1]  to create an internal node xjv dominating a leaf labeled with x[j − 1] and another one labeled with T  x[j − 1]   line 9 . Figure 16.4 illustrates this construction. Thus, this provides a new access  by ConstructAutomaton   by determining the leaf in T assigned to ua, that  iteration of the loop. When it reaches the number of states of A, all states of A  other access strings.  v   v   T  x[j − 1]   u   xj v  u   T  x[j − 1]   x[j − 1]   368  Chapter 16 Learning Automata and Languages  A   cid:98 A  T  counter-example x  x = b  x = baa  x = baaa  Figure 16.5 Illustration of the execution of Algorithm QueryLearnAutomata   for the target automaton A.  Each line shows the current decision tree T and the tentative DFA  cid:98 A constructed using T . When  cid:98 A is not equivalent to A, the learner receives a counter-example x indicated in the third column.  Also, a state A u  of A is ﬁnal iﬀ u accepted by A that is iﬀ u is assigned to the right sub-tree of the root node by T , which is the criterion determining the ﬁnality  of  cid:98 A u . Thus, the automata A and  cid:98 A coincide. most x tree operations are performed. Constructing  cid:98 A requires O ΣA  tree  The following is the analysis of the running-time complexity of the algorithm. At each iteration, one new distinguished access string is found associated to a distinct state of A, thus, at most A states are created. For each counter-example x, at operations. The cost of a tree operation is O A  since it consists of at most A  a  0  b  2  a  3  b  b  a  1  a  b  x = baa accepted  ε  ε  NIL  x = baa accepted  a  b  a  !  a  a  a  a  x = baa accepted  !  !  a  !  b  ba  a  !  a  a  a !  !  a  a  ba  a  a  x = baa accepted  a  !  b  a  b  a  a !  !  a  a  ba  b  !  a  b  !  ba  a  !  a  a  ba  a  !  b  b  b  a  a  b  ba  a  !  a  a  ba  a  a  !  a  ba  a  baa  !  ba  a  !  a  !  a  b  !  ba  baa  a  !  a  a  ba  a  !  b  ba  a  baa  b  b  a  b  a  b   16.4  Identiﬁcation in the limit  369  membership queries. Thus, the overall complexity of the algorithm is in O ΣA2 + nA , where n is the maximum length of a counter-example. Note that this analysis assumes that equivalence and membership queries are made in constant time.  Our analysis shows the following result.  Theorem 16.5  Learning DFAs with queries  The class of all DFAs is eﬃciently exactly learnable using membership and equivalence queries.  Figure 16.5 illustrates a full execution of the algorithm in a speciﬁc case. In the next section, we examine a diﬀerent learning scenario for automata.  16.4  Identiﬁcation in the limit  In the identiﬁcation in the limit framework, the problem consists of identifying a target concept c exactly after receiving a ﬁnite set of examples. A class of languages is said to be identiﬁable in the limit if there exists an algorithm that identiﬁes any language L in that class after examining a ﬁnite number of examples and its hypothesis remains unchanged thereafter.  This framework is perhaps less realistic from a computational point of view since it requires no upper bound on the number of instances or the eﬃciency of the algorithm. Nevertheless, it has been argued by some to be similar to the scenario of humans learning languages. In this framework as well, negative results hold for the general problem of learning DFAs.  Theorem 16.6 Deterministic automata are not identiﬁable in the limit from positive examples.  Some sub-classes of ﬁnite automata can however be successfully identiﬁed in the limit. Most algorithms for inference of automata are based on a state-partitioning paradigm. They start with an initial DFA, typically a tree accepting the ﬁnite set of sample strings available and the trivial partition: each block is reduced to one state of the tree. At each iteration, they merge partition blocks while preserving some congruence property. The iteration ends when no other merging is possible and the ﬁnal partition deﬁnes the automaton inferred. Thus, the choice of the congruence fully determines the algorithm and a variety of diﬀerent algorithms can be deﬁned by varying that choice. A state-splitting paradigm can be similarly deﬁned starting from the single-state automaton accepting Σ∗. In this section, we present an algorithm for learning reversible automata, which is a special instance of the general state-partitioning algorithmic paradigm just described.  Let A =  Σ, Q, I, F, E  be a DFA and let π be a partition of Q. The DFA deﬁned by the partition π is called the automaton quotient of A and π. It is denoted by   370  Chapter 16 Learning Automata and Languages  A π and deﬁned as follows: A π =  Σ, π, Iπ, Fπ, Eπ  with  Iπ = {B ∈ π : I ∩ B  cid:54 = ∅} Fπ = {B ∈ π : F ∩ B  cid:54 = ∅} Eπ = { B, a, B cid:48   : ∃ q, a, q cid:48   ∈ E  q ∈ B, q cid:48  ∈ B cid:48 , B ∈ π, B cid:48  ∈ π}.  Let S be a ﬁnite set of strings and let Pref S  denote the set of preﬁxes of all strings of S. A preﬁx-tree automaton accepting exactly the set of strings S is a particular DFA denoted by P T  S  =  Σ, Pref S ,{ cid:15 }, S, ES  where Σ is the set of alphabet symbols used in S and ES deﬁned as follows:  ES = { x, a, xa  : x ∈ Pref S , xa ∈ Pref S }.  Figure 16.7a shows the preﬁx-tree automaton of a particular set of strings S.  16.4.1 Learning reversible automata In this section, we show that the sub-class of reversible automata or reversible languages can be identiﬁed in the limit. In particular, we show that the language can be identiﬁed given a positive presentation. A positive presentation of a language L is an inﬁnite sequence  xn n∈N such that {xn : n ∈ N} = L. Thus, in particular, for any x ∈ L there exists n ∈ N such that exists N ∈ N such that for n ≥ N the hypothesis it returns is L.  x = xn. An algorithm identiﬁes L in the limit from a positive presentation if there  Given a DFA A, we deﬁne its reverse AR as the automaton derived from A by making the initial state ﬁnal, the ﬁnal states initial, and by reversing the direction of every transition. The language accepted by the reverse of A is precisely the language of the reverse  or mirror image  of the strings accepted by A.  Deﬁnition 16.7  Reversible automata  A ﬁnite automaton A is said to be reversible iﬀ both A and AR are deterministic. A language L is said to be reversible if it is the language accepted by some reversible automaton.  Some direct consequences of this deﬁnition are that a reversible automaton A has a unique ﬁnal state and that its reverse AR is also reversible. Note also that a trim reversible automaton A is minimal. Indeed, if states q and q cid:48  in A are equivalent, then, they admit a common string x leading both from q and from q cid:48  to a ﬁnal state. But, by the reverse determinism of A, reading the reverse of x from the ﬁnal state must lead to a unique state, which implies that q = q cid:48 . For any u ∈ Σ∗ and any language L ⊆ Σ∗, let Suﬀ L u  denote the set of all possible suﬃxes in L for u:  Suﬀ L u  = {v ∈ Σ∗ : uv ∈ L}.   16.1    16.4  Identiﬁcation in the limit  371  Suﬀ L u  is also often denoted by u−1L. Observe that if L is a reversible language, then the following implication holds for any two strings u, u cid:48  ∈ Σ∗: Suﬀ L u  ∩ Suﬀ L u cid:48    cid:54 = ∅ =⇒ Suﬀ L u  = Suﬀ L u cid:48  .   16.2   Indeed, let A be a reversible automaton accepting L. Let q be the state of A reached from the initial state when reading u and q cid:48  the one reached reading u cid:48 . If v ∈ Suﬀ L u  ∩ Suﬀ L u cid:48  , then v can be read both from q and q cid:48  to reach the ﬁnal state. Since AR is deterministic, reading back the reverse of v from the ﬁnal state must lead to a unique state, therefore q = q cid:48 , that is Suﬀ L u  = Suﬀ L u cid:48  . Let A =  Σ, Q,{i0},{f0}, E  be a reversible automaton accepting a reversible language L. We deﬁne a set of strings SL as follows:  SL = {d[q]f [q] : q ∈ Q} ∪ {d[q], a, f [q cid:48 ] : q, q cid:48  ∈ Q, a ∈ Σ} ,  where d[q] is a string of minimum length from i0 to q, and f [q] a string of minimum length from q to f0. As shown by the following proposition, SL characterizes the language L in the sense that any reversible language containing SL must contain L.  Proposition 16.8 Let L be a reversible language. Then, L is the smallest reversible language containing SL. Proof: Let L cid:48  be a reversible language containing SL and let x = x1 ··· xn be a string accepted by L, with xk ∈ Σ for k ∈ [n] and n ≥ 1. For convenience, we also deﬁne x0 as  cid:15 . Let  q0, x1, q1 ···  qn−1, xn, qn  be the accepting path in A labeled with x. We show by recurrence that Suﬀ L cid:48  x0 ··· xk  = Suﬀ L cid:48  d[qk]  for all k ∈ {0, . . . , n}. Since d[q0] = d[i0] =  cid:15 , this clearly holds for k = 0. Now assume that Suﬀ L cid:48  x0 ··· xk  = Suﬀ L cid:48  d[qk]  for some k ∈ {0, . . . , n − 1}. This implies immediately that Suﬀ L cid:48  x0 ··· xkxk+1  = Suﬀ L cid:48  d[qk]xk+1 . By deﬁnition, SL contains both d[qk+1]f [qk+1] and d[qk]xk+1f [qk+1]. Since L cid:48  includes SL, the same holds for L cid:48 . Thus, f [qk+1] belongs to Suﬀ L cid:48  d[qk+1  ∩ Suﬀ L cid:48  d[qk]xk+1 . In view of  16.2 , this implies that Suﬀ L cid:48  d[qk]xk+1  = Suﬀ L cid:48  d[qk+1] . Thus, we have Suﬀ L cid:48  x0 ··· xkxk+1  = Suﬀ L cid:48  d[qk+1] . This shows that Suﬀ L cid:48  x0 ··· xk  = Suﬀ L cid:48  d[qk]  holds for all k ∈ {0, . . . , n}, in particular, for k = n. Note that since qn = f0, we have f [qn] =  cid:15 , therefore d[qn] = d[qn]f [qn] is in SL ⊆ L cid:48 , which implies that Suﬀ L cid:48  d[qn]  contains  cid:15  and thus that Suﬀ L cid:48  x0 ··· xn  contains  cid:15 . This is  cid:3  equivalent to x = x0 ··· xn ∈ L cid:48 . Figure 16.6 shows the pseudocode of an algorithm for inferring a reversible au- tomaton from a sample S of m strings x1, . . . , xm. The algorithm starts by creating a preﬁx-tree automaton A for S  line 1  and then iteratively deﬁnes a partition π of the states of A, starting with the trivial partition π0 with one block per state  line 2 . The automaton returned is the quotient of A and the ﬁnal partition π deﬁned.   372  Chapter 16 Learning Automata and Languages  LearnReversibleAutomata S =  x1, . . . , xm   1 A =  Σ, Q,{i0}, F, E  ← P T  S  2 π ← π0  cid:46  trivial partition. 3 list ← { f, f cid:48   : f cid:48  ∈ F}  cid:46  f arbitrarily chosen in F . 4 while list  cid:54 = ∅ do 5  Remove list,  q1, q2   if B q1, π   cid:54 = B q2, π  then  B1 ← B q1, π  B2 ← B q2, π  for all a ∈ Σ do  if  succ B1, a   cid:54 = ∅  ∧  succ B2, a   cid:54 = ∅  then  Add list,  succ B1, a , succ B2, a     if  pred B1, a   cid:54 = ∅ ∧  pred B2, a   cid:54 = ∅   then  Add list,  pred B1, a , pred B2, a     Update succ, pred, B1, B2  π ← Merge π, B1, B2   16 return A π  6  7  8  9  10  11  12  13  14  15  Figure 16.6 Algorithm for learning reversible automata from a set of positive strings S.  The algorithm maintains a list list of pairs of states whose corresponding blocks are to be merged, starting with all pairs of ﬁnal states  f, f cid:48   for an arbitrarily chosen ﬁnal state f ∈ F  line 3 . We denote by B q, π  the block containing q based on the partition π. For each block B and alphabet symbol a ∈ Σ, the algorithm also maintains a successor succ B, a , that is, a state that can be reached by reading a from a state of B; succ B, a  = ∅ if no such state exists. It maintains similarly the predecessor pred B, a , which is a state that admits a transition labeled with a leading to a state in B; pred B, a  = ∅ if no such state exists. Then, while list is not empty, a pair is removed from list and processed as follows. If the pair  q1, q cid:48 1  has not been already merged, the pairs formed by the successors and predecessors of B1 = B q1, π  and B2 = B q2, π  are added to list   16.4  Identiﬁcation in the limit  373   a    b   Figure 16.7 Example of inference of a reversible automaton.    cid:15 , aa, bb, aaaa, abab, abba, baba .  b  Automaton  cid:98 A returned by LearnReversibleAutomata   posite directions. The language accepted by  cid:98 A is that of strings with an even number of as and bs.  for the input S. A double-direction arrow represents two transitions with the same label with op-   a  Preﬁx-tree P T  S  representing S =   lines 10–13 . Before merging blocks B1 and B2 into a new block B cid:48  that deﬁnes a new partition π  line 15 , the successor and predecessor values for the new block B cid:48  are deﬁned as follows  line 14 . For each symbol a ∈ Σ, succ B cid:48 , a  = ∅ if succ B1, a  = succ B2, a  = ∅, otherwise succ B cid:48 , a  is set to one of succ B1, a  if it is non-empty, succ B2, a  otherwise. The predecessor values are deﬁned in a similar way. Figure 16.7 illustrates the application of the algorithm in the case of a sample with m = 7 strings.  Proposition 16.9 Let S be a ﬁnite set of strings and let A = P T  S  be the preﬁx-tree automaton deﬁned from S. Then, the ﬁnal partition deﬁned by LearnReversibleAutomata   used with input S is the ﬁnest partition π for which A π is reversible.  Proof: Let T be the number of iterations of the algorithm for the input sample S. We denote by πt the partition deﬁned by the algorithm after t ≥ 1 iterations of the loop, with πT the ﬁnal partition.  A πT is a reversible automaton since all ﬁnal states are guaranteed to be merged into the same block as a consequence of the initialization step of line 3 and, for any block B, by deﬁnition of the algorithm, states reachable by a ∈ Σ from B are contained in the same block, and similarly for those admitting a transition labeled with a to a state of B.  Let π cid:48  be a partition of the states of A for which A π cid:48  is reversible. We show by recurrence that πT reﬁnes π cid:48 . Clearly, the trivial partition π0 reﬁnes π cid:48 . Assume that πs reﬁnes π cid:48  for all s ≤ t. πt+1 is obtained from π by merging two blocks B q1, πt  and B q2, πt . Since πt reﬁnes π cid:48 , we must have B q1, πt  ⊆ B q1, π cid:48    Example  L = !, aa, bb, aaaa, abab, abba, baba .  0  1  a  b  4  7  9  3  6  8  a  a  b  b  a  b  a  a  a  b  a  b  2  5  14  10  11  12  13  {1, 3, 8, 12}  {6, 10}  {0, 2, 4, 7,  9, 13, 14}  a a  b b  {5, 11}  b b  a a   374  Chapter 16 Learning Automata and Languages  and B q2, πt  ⊆ B q2, π cid:48  . To show that πt+1 reﬁnes π cid:48 , it suﬃces to prove that B q1, π cid:48   = B q2, π cid:48  . A reversible automaton has only one ﬁnal state, therefore, for the partition π cid:48 , all ﬁnal states of A must be placed in the same block. Thus, if the pair  q1, q2  processed at the  t + 1 th iteration is a pair of ﬁnal states placed in list at the initialization step  line 3 , then we must have B q1, π cid:48   = B q2, π cid:48  . Otherwise,  q1, q2  was placed in list as a pair of successor or predecessor states of two states q cid:48 1 and q cid:48 2 merged at a previous iteration s ≤ t. Since πs reﬁnes π cid:48 , q cid:48 1 and q cid:48 2 are in the same block of π cid:48  and since A π cid:48  is reversible, q1 and q2 must also be in the same block as successors or predecessors of the same block for the same label a ∈ Σ,  cid:3  thus B q1, π cid:48   = B q2, π cid:48  .  Theorem 16.10 Let S be a ﬁnite set of strings and let A be the automaton returned by LearnReversibleAutomata   when used with input S. Then, L A  is the smallest reversible language containing S.  Proof: Let L be a reversible language containing S, and let A cid:48  be a reversible automaton with L A cid:48   = L. Since every string of S is accepted by A cid:48 , any u ∈ Pref S  can be read from the initial state of A cid:48  to reach some state q u  of A cid:48 . Consider the automaton A cid:48  cid:48  derived from A cid:48  by keeping only states of the form q u  and transitions between such states. A cid:48  cid:48  has the unique ﬁnal state of A cid:48  since q u  is ﬁnal for u ∈ S, and it has the initial state of A cid:48 , since  cid:15  is a preﬁx of strings of S. Furthermore, A cid:48  cid:48  directly inherits from A cid:48  the property of being deterministic and reverse deterministic. Thus, A cid:48  cid:48  is reversible. The states of A cid:48  cid:48  deﬁne a partition of Pref S : u, v ∈ Pref S  are in the same block iﬀ q u  = q v . Since by deﬁnition of the preﬁx-tree P T  S , its states can be iden- tiﬁed with Pref S , the states of A cid:48  cid:48  also deﬁne a partition π cid:48  of the states of P T  S  and thus A cid:48  cid:48  = P T  S  π cid:48 . By proposition 16.9, the partition π deﬁned by algorithm LearnReversibleAutomata   run with input S is the ﬁnest such that P T  S  π is reversible. Therefore, we must have L P T  S  π  ⊆ L P T  S  π cid:48   = L A cid:48  cid:48  . Since A cid:48  cid:48  is a sub-automaton of A cid:48 , L contains L A cid:48  cid:48   and therefore L P T  S  π  =  cid:3  L A , which concludes the proof.  Theorem 16.11  Identiﬁcation in the limit of reversible languages  Let L be a reversible language, then algorithm LearnReversibleAutomata   identiﬁes L in the limit from a positive presentation.  Proof: Let L be a reversible language. By proposition 16.8, L admits a ﬁnite characteristic sample SL. Let  xn n∈N be a positive presentation of L and let Xn denote the union of the ﬁrst n elements of the sequence. Since SL is ﬁnite, there exists N ≥ 1 such that SL ⊆ XN . By theorem 16.10, for any n ≥ N , LearnReversibleAutomata   run on the ﬁnite sample Xn returns the smallest   16.5 Chapter notes  375  reversible language L cid:48  containing Xn a fortiori SL, which, by deﬁnition of SL,  cid:3  implies that L cid:48  = L.  The main operations needed for the implementation of the algorithm for learning reversible automata are the standard find and union to determine the block a state belongs to and to merge two blocks into a single one. Using a disjoint-set data structure for these operations, the time complexity of the algorithm can be shown to be in O nα n  , where n denotes the sum of the lengths of all strings in the input sample S and α n  the inverse of the Ackermann function, which is essentially constant  α n  ≤ 4 for n ≤ 1080 . 16.5 Chapter notes  For an overview of ﬁnite automata and some related results, see Hopcroft and Ullman [1979] or the more recent Handbook chapter by Perrin [1990], as well as the series of books by M. Lothaire [Lothaire, 1982, 1990, 2005] and the even more recent book by De la Higuera [2010].  Theorem 16.2, stating that the problem of ﬁnding a minimum consistent DFA is NP-hard, is due to Gold [1978]. This result was later extended by Angluin [1978]. Pitt and Warmuth [1993] further strengthened these results by showing that even an approximation within a polynomial function of the size of the smallest automaton is NP-hard  theorem 16.3 . Their hardness results apply also to the case where prediction is made using NFAs. Kearns and Valiant [1994] presented hardness results of a diﬀerent nature relying on cryptographic assumptions. Their results imply that no polynomial-time algorithm can learn consistent NFAs polynomial in the size of the smallest DFA from a ﬁnite sample of accepted and rejected strings if any of the generally accepted cryptographic assumptions holds: if factoring Blum integers is hard; or if the RSA public key cryptosystem is secure; or if deciding quadratic residuosity is hard. Most recently, Chalermsook et al. [2014] improved the non-approximation guarantee of Pitt and Warmuth [1993] to a tight bound.  On the positive side, Trakhtenbrot and Barzdin [1973] showed that the smallest ﬁnite automaton consistent with the input data can be learned exactly from a uniform complete sample, whose size is exponential in the size of the automaton. The worst-case complexity of their algorithm is exponential, but a better average- case complexity can be obtained assuming that the topology and the labeling are selected randomly [Trakhtenbrot and Barzdin, 1973] or even that the topology is selected adversarially [Freund et al., 1993].  Cortes, Kontorovich, and Mohri [2007a] study an approach to the problem of learning automata based on linear separation in some appropriate high-dimensional feature space; see also Kontorovich et al. [2006, 2008]. The mapping of strings to   376  Chapter 16 Learning Automata and Languages  that feature space can be deﬁned implicitly using the rational kernels presented in chapter 6, which are themselves deﬁned via weighted automata and transducers.  The model of learning with queries was introduced by Angluin [1978], who also proved that ﬁnite automata can be learned in time polynomial in the size of the minimal automaton and that of the longest counter-example. Bergadano and Var- ricchio [1995] further extended this result to the problem of learning weighted au- tomata deﬁned over any ﬁeld  see also an optimal algorithm by Bisht et al. [2006] . Using the relationship between the size of a minimal weighted automaton over a ﬁeld and the rank of the corresponding Hankel matrix, the learnability of many other concepts classes such as disjoint DNF can be shown [Beimel et al., 2000]. Our description of an eﬃcient implementation of the algorithm of Angluin [1982] using decision trees is adapted from Kearns and Vazirani [1994].  The model of identiﬁcation in the limit of automata was introduced and analyzed by Gold [1967]. Deterministic ﬁnite automata were shown not to be identiﬁable in the limit from positive examples [Gold, 1967]. But, positive results were given for the identiﬁcation in the limit of a number of sub-classes, such as the family of k- reversible languages Angluin [1982] considered in this chapter. Positive results also hold for learning subsequential transducers Oncina et al. [1993]. Some restricted classes of probabilistic automata such as acyclic probabilistic automata were also shown by Ron et al. [1995] to be eﬃciently learnable.  There is a vast literature dealing with the problem of learning automata.  In particular, positive results have been shown for a variety of sub-families of ﬁnite automata in the scenario of learning with queries and learning scenarios of diﬀerent kinds have been introduced and analyzed for this problem. The results presented in this chapter should therefore be viewed only as an introduction to that material.  16.6 Exercises  16.1 Minimal DFA. Show that a minimal DFA A also has the minimal number of transitions among all other DFAs equivalent to A. Prove that a language L is regular iﬀ Q = {Suﬀ L u  : u ∈ Σ∗} is ﬁnite. Show that the number of states of a minimal DFA A with L A  = L is precisely the cardinality of Q.  16.2 VC-dimension of ﬁnite automata.   a  What is the VC-dimension of the family of all ﬁnite automata? What does that imply for PAC-learning of ﬁnite automata? Does this result change if we restrict ourselves to learning acyclic automata  automata with no cycles ?   16.6 Exercises  377   b  Show that the VC-dimension of the family of DFAs with at most n states is  bounded by O Σn log n .  16.3 PAC learning with membership queries. Give an example of a concept class C that is eﬃciently PAC-learnable with membership queries but that is not eﬃciently exactly learnable.  16.4 Learning monotone DNF formulae with queries. Show that the class of mono- tone DNF formulae over n variables is eﬃciently exactly learnable using mem- bership and equivalence queries.  Hint: a prime implicant t of a formula f is a product of literals such that t implies f but no proper sub-term of t implies f . Use the fact that for monotone DNF, the number of prime implicants is at the most the number of terms of the formula.   16.5 Learning with unreliable query responses. Consider the problem where the learner must ﬁnd an integer x selected by the oracle within [n], where n ≥ 1 is given. To do so, the learner can ask questions of the form  x ≤ m?  or  x > m?  for m ∈ [n]. The oracle responds to these questions but may give an incorrect response to k questions. How many questions should the learner ask to determine x?  Hint: observe that the learner can repeat each question 2k + 1 times and use the majority vote.   16.6 Algorithm for learning reversible languages. What is the DFA A returned by the algorithm for learning reversible languages when applied to the sample S = {ab, aaabb, aabbb, aabbbb}? Suppose we add a new string to the sample, say x = abab. How should A be updated to compute the result of the algorithm for S ∪ {x}? More generally, describe a method for updating the result of the algorithm incrementally.  16.7 k-reversible languages. A ﬁnite automaton A cid:48  is said to be k-deterministic if it is deterministic modulo a lookahead k: if two distinct states p and q are both initial, or are both reached from another state r by reading a ∈ Σ, then no string u of length k can be read in A cid:48  both from p and q. A ﬁnite automaton A is said to be k-reversible if it is deterministic and if AR is k-deterministic. A language L is k-reversible if it is accepted by some k-reversible automaton.   a  Prove that L is k-reversible iﬀ for any strings u, u cid:48 , v ∈ Σ∗ with v = k,  Suﬀ L uv  ∩ Suﬀ L u cid:48 v   cid:54 = ∅ =⇒ Suﬀ L uv  = Suﬀ L u cid:48 v .  b  Show that a k-reversible language admits a characteristic language.   378  Chapter 16 Learning Automata and Languages   c  Show that the following deﬁnes an algorithm for learning k-reversible au- tomata. Proceed as in the algorithm for learning reversible automata but with the following merging rule instead: merge blocks B1 and B2 if they can be reached by the same string u of length k from some other block and if B1 and B2 are both ﬁnal or have a common successor.   17 Reinforcement Learning  This chapter presents an introduction to reinforcement learning, a rich area of machine learning with connections to control theory, optimization, and cognitive sciences. Reinforcement learning is the study of planning and learning in a scenario where a learner actively interacts with the environment to achieve a certain goal. This active interaction justiﬁes the terminology of agent used to refer to the learner. The achievement of the agent’s goal is typically measured by the reward it receives from the environment and which it seeks to maximize.  We ﬁrst introduce the general scenario of reinforcement learning and then intro- duce the model of Markov decision processes  MDPs , which is widely adopted in this area, as well as essential concepts such as that of policy or policy value related to this model. The rest of the chapter presents several algorithms for the planning problem, which corresponds to the case where the environment model is known to the agent, and then a series of learning algorithms for the more general case of an unknown model.  17.1 Learning scenario  The general scenario of reinforcement learning is illustrated by ﬁgure 17.1. Un- like the supervised learning scenario considered in previous chapters, here, the learner does not passively receive a labeled data set. Instead, it collects informa- tion through a course of actions by interacting with the environment. In response to an action, the learner or agent, receives two types of information: its current state in the environment, and a real-valued reward , which is speciﬁc to the task and its corresponding goal.  The objective of the agent is to maximize its reward and thus to determine the best course of actions, or policy, to achieve that objective. However, the information he receives from the environment is only the immediate reward related to the action just taken. No future or long-term reward feedback is provided by the environment. An important aspect of reinforcement learning is to consider delayed rewards or   380  Chapter 17 Reinforcement Learning  Figure 17.1 Representation of the general scenario of reinforcement learning.  penalties. The agent is faced with the dilemma between exploring unknown states and actions to gain more information about the environment and the rewards, and exploiting the information already collected to optimize its reward. This is known as the exploration versus exploitation trade-oﬀ inherent to reinforcement learning. Note that there are several diﬀerences between the learning scenario of reinforce- ment learning and that of supervised learning examined in most of the previous chapters. Unlike supervised learning, in reinforcement learning there is no ﬁxed distribution according to which instances are drawn; it is the choice of a policy that deﬁnes the distribution over observations. In fact, slight changes to the policy may have dramatic eﬀects on the rewards received. Furthermore, in general, the environment may not be ﬁxed and could vary as a result of the actions selected by the agent. This may be a more realistic model for some learning problems than the standard supervised learning. Finally, note that, unlike supervised learning, in reinforcement learning, training and testing phases are intermixed.  Two main settings can be distinguished here: the one where the environment model is known to the agent, in which case its objective of maximizing the reward received is reduced to a planning problem; and the one where the environment model is unknown, in which case the agent faces a learning problem. In the latter setting, the agent must learn from the state and reward information gathered to both gain information about the environment and determine the best action policy. This chapter presents algorithmic solutions for both of these settings.  17.2 Markov decision process model  We ﬁrst introduce the model of Markov decision processes  MDPs , a model of the environment and interactions with the environment widely adopted in reinforcement learning. An MDP is a Markovian process deﬁned as follows.  Deﬁnition 17.1  MDPs  A Markov decision process  MDP  is deﬁned by:    a set of states S, possibly inﬁnite.  Agent  Environment  action  state  reward   17.3 Policy  381  Figure 17.2 Illustration of the states and transitions of an MDP at diﬀerent times.    a start state or initial state s0 ∈ S.   a set of actions A, possibly inﬁnite.   a transition probability P[s cid:48 s, a]: distribution over destination states s cid:48  = δ s, a .   a reward probability P[r cid:48 s, a]: distribution over rewards returned r cid:48  = r s, a .  The model is Markovian because the transition and reward probabilities depend only on the current state s and not the entire history of states and actions taken. This deﬁnition of MDP can be further generalized to the case of non-discrete state and action sets. In a discrete-time model, actions are taken at a set of decision epochs {0, . . . , T}, and this is the model we will adopt in what follows. This model can also be straightforwardly generalized to a continuous-time one where actions are taken at arbitrary points in time.  When T is ﬁnite, the MDP is said to have a ﬁnite horizon. Independently of the ﬁniteness of the time horizon, an MDP is said to be ﬁnite when both S and A are ﬁnite sets. Here, we are considering the general case where the reward r s, a  at state s when taking action a is a random variable. However, in many cases, the reward is assumed to be a deterministic function the state and action pair  s, a . Figure 17.2 illustrates the model corresponding to an MDP. At time t ∈ {0, . . . , T} the state observed by the agent is st and it takes action at ∈ A. The state reached is st+1  with probability P[st+1st, at]  and the reward received rt+1 ∈ R  with probability P[rt+1st, at] .  Many real-world tasks can be represented by MDPs. Figure 17.3 gives the example  of a simple MDP for a robot picking up balls on a tennis court.  17.3 Policy  The main problem for an agent in an MDP environment is to determine the action to take at each state, that is, an action policy.  17.3.1 Deﬁnition Deﬁnition 17.2  Policy  A policy is a mapping π : S → ∆ A , where ∆ A  is the set of probability distributions over A. A policy π is deterministic if for any s, there exists a unique a ∈ A such that π s  a  = 1. In that case, we can identify π with a mapping from S to A and use π s  to denote that action.  at rt+1  at+1 rt+2  st+1  st+2  st   382  Chapter 17 Reinforcement Learning  More precisely, this is the deﬁnition of a stationary policy since the choice of the distribution of actions does not depend on time. More generally, we could deﬁne a non-stationary policy as a sequence of mappings πt : S → ∆ A  indexed by t. In particular, in the ﬁnite horizon case, a non-stationary policy is typically necessary for optimizing rewards.  The agent’s objective is to ﬁnd a policy that maximizes its expected  reward  return. The return it receives following a deterministic policy π along a speciﬁc sequence of states s0, . . . , sT is deﬁned as follows:    for a ﬁnite horizon  T < ∞ :  cid:80 T   for an inﬁnite horizon  T = ∞ :  cid:80 +∞t=0 γtr cid:0 st, π st  cid:1 , where γ ∈ [0, 1  is a constant  factor less than one used to discount future rewards.  t=0 r cid:0 st, π st  cid:1 .  Note that the return is a single scalar summarizing a possibly inﬁnite sequence of immediate rewards. In the discounted case, early rewards are viewed as more valuable than later ones.  17.3.2 Policy value This leads to the following deﬁnition of the value of a policy at each state. Deﬁnition 17.3  Policy value  The value Vπ s  of a policy π at state s ∈ S is deﬁned as the expected reward returned when starting at s and following policy π:   ﬁnite horizon: Vπ s  = E   inﬁnite discounted horizon: Vπ s  = E where the expectations are over the random selection of an action at according to the distribution π st , which is explicitly indicated, and over the random states st  t=0 r cid:0 st, at cid:1  cid:12  cid:12  cid:12  s0 = s cid:105 ; at∼π st  cid:104  cid:80 +∞t=0 γtr cid:0 st, at cid:1  cid:12  cid:12  cid:12  s0 = s cid:105 ,  at∼π st  cid:104  cid:80 T  reached and the reward values r cid:0 st, at cid:1 .22 An inﬁnite undiscounted horizon is also  often considered based on the limit of the average reward, when it exists.  17.3.3 Optimal policies Starting from a state s ∈ S, to maximize its reward, an agent naturally seeks a policy π with the largest value Vπ s . In this section, we will show that, remarkably, for any ﬁnite MDP in the inﬁnite horizon setting, there exists a policy that is optimal for any start state, that is one with the following deﬁnition. Deﬁnition 17.4  Optimal policy  A policy π∗ is optimal if its value is maximal for every state s ∈ S, that is, for any policy π and any state s ∈ S, Vπ∗  s  ≥ Vπ s .  22 More generally, in all that follows, the randomization with respect to the reward function and the next state will not be explicitly indicated to simplify the notation.   17.3 Policy  383  Figure 17.3 Example of a simple MDP for a robot picking up balls on a tennis court. The set of actions is A = {search, carry, pickup} and the set of states reduced to S = {start, other}. Each transition is labeled with the action followed by the probability of the transition probability and the reward received after taking that action. R1, R2, and R3 are real numbers indicating the reward associated to each transition  case of deterministic reward .  Moreover, we will show that for any MDP there exists a deterministic optimal policy. To do so, it is convenient to introduce the notion of state-action value function.  Qπ s, a  = E[r s, a ] + E  Deﬁnition 17.5  State-action value function  The state-action value function Q associ- ated to a policy π is deﬁned for all  s, a  ∈ S × A as the expected return for taking action a ∈ A at state s ∈ S and then following policy π:  at∼π st  cid:34 +∞ cid:88 t=1  γtr cid:0 st, at cid:1  cid:12  cid:12  cid:12  s0 = s, a0 = a cid:35  = E cid:104 r s, a  + γVπ s1  cid:12  cid:12  cid:12  s0 = s, a0 = a cid:105 . a∼π s  cid:2 Qπ s, a  cid:3  = Vπ s   see also proposition 17.9  a∼π cid:48  s  cid:2 Qπ s, a  cid:3  ≥ E  a∼π s  cid:2 Qπ s, a  cid:3  cid:17  ⇒ cid:16 ∀s ∈ S, Vπ cid:48  s  ≥ Vπ s  cid:17 .  Observe that E Theorem 17.6  Policy improvement theorem  For any two policies π and π cid:48  the follow- ing holds:   cid:16 ∀s ∈ S, E  Furthermore, a strict inequality for at least one state s in the left-hand side implies a strict inequality for at least one s in the right-hand side.   17.1   start   search [.1, R1]  search [.9, R1]  carry [.5, R3]  other  carry [.5, -1] pickup [1, R2]   384  Chapter 17 Reinforcement Learning  Proof: Assume that π and π cid:48  verify the left-hand side. For any s ∈ S, we have  =  E  E  a∼π s  cid:2 Qπ s, a  cid:3  Vπ s  = E a∼π cid:48  s  cid:2 Qπ s, a  cid:3  ≤ E a∼π cid:48  s  cid:104 r s, a  + γVπ s1  cid:12  cid:12  cid:12  s0 = s cid:105  = E a1∼π s1  cid:2 Qπ s1, a1  cid:3  cid:12  cid:12  cid:12  s0 = s cid:105  a∼π cid:48  s  cid:104 r s, a  + γ = E a1∼π cid:48  s1  cid:2 Qπ s1, a1  cid:3  cid:12  cid:12  cid:12  s0 = s cid:105  a∼π cid:48  s  cid:104 r s, a  + γ ≤ E a1∼π cid:48  s1  cid:104 r s, a  + γr s1, a1  + γ2Vπ s2  cid:12  cid:12  cid:12  s0 = s cid:105 . E γt E[r st, at ] + γT +1Vπ sT +1  cid:12  cid:12  cid:12  s0 = s cid:21 . at∼π cid:48  st  cid:20  T cid:88 t=0 γt E[r st, at ] cid:12  cid:12  cid:12  s0 = s cid:21  = Vπ cid:48  s . at∼π cid:48  st  cid:20  +∞ cid:88 t=0  Vπ s  ≤  a∼π cid:48  s   E  E  Vπ s  ≤  Since Vπ sT +1  is bounded, taking the limit T → +∞ gives  Proceeding in this way shows that for any T ≥ 1:  Finally, any strict inequality in the left-hand side property results in a strict in-  cid:3  equality in the chain of inequalities above.  Theorem 17.7  Bellman’s optimality condition  A policy π is optimal iﬀ for any pair  s, a  ∈ S × A with π s  a  > 0 the following holds: Qπ s, a cid:48  .   17.2   a ∈ argmax a cid:48 ∈A  Proof: By Theorem 17.6, if the condition  17.2  does not hold for some  s, a  with π s  a  > 0, then the policy π is not optimal. This is because π can then be improved by deﬁning π cid:48  such that π cid:48  s cid:48   = π s  for s cid:48   cid:54 = s and π cid:48  s  concen- a∼π cid:48  s  cid:2 Qπ s cid:48 , a  cid:3  = trated on any element of argmaxa cid:48 ∈A Qπ s, a cid:48  . π cid:48  veriﬁes E a∼π s  cid:2 Qπ s, a  cid:3 . Thus, a∼π s  cid:2 Qπ s cid:48 , a  cid:3  for s cid:48   cid:54 = s and E a∼π cid:48  s  cid:2 Qπ s, a  cid:3  > E E by Theorem 17.6, Vπ cid:48  s  > Vπ s  for at least one s and π is not optimal. Conversely, let π cid:48  be a non-optimal policy. Then there exists a policy π and at least one state s for which Vπ cid:48  s  < Vπ s . By Theorem 17.6, this implies that a∼π s  cid:2 Qπ s, a  cid:3 . Thus, a∼π cid:48  s  cid:2 Qπ s, a  cid:3  < E there exists some state s ∈ S with E  cid:3  π cid:48  cannot satisfy the condition  17.2 .   17.3 Policy  385  Theorem 17.8  Existence of an optimal deterministic policy  Any ﬁnite MDP admits an optimal deterministic policy.  Proof: Let π∗ be a deterministic policy maximizing  cid:80 s∈S Vπ s . π∗ exists since If π∗ were not optimal, by there are only ﬁnitely many deterministic policies. Theorem 17.7, there would exist a state s with π s   cid:54 ∈ argmaxa cid:48 ∈A Qπ s, a cid:48  . By theorem 17.6, π∗ could then be improved by choosing a policy π with π s  ∈ argmaxa cid:48 ∈A Qπ s, a cid:48   and π coinciding with π∗ for all other states. But then π would verify Vπ∗ s  ≤ Vπ s  with a strict inequality at least for one state. This would contradict the fact that π∗ maximizes cid:80 s∈S Vπ s .  cid:3   In view of the existence of a deterministic optimal policy, in what follows, to simplify the discussion, we will consider only deterministic policies. Let π∗ denote a  deterministic  optimal policy, and let Q∗ and V ∗ denote its corresponding state- action value function and value function. By Theorem 17.7, we can write   17.3    17.4   ∀s ∈ S, π∗ s  = argmax a∈A  Q∗ s, a .  Thus, the knowledge of the state-action value function Q∗ is suﬃcient for the agent to determine the optimal policy, without any direct knowledge of the reward or transition probabilities. Replacing Q∗ by its deﬁnition gives the following system of equations for the optimal policy values V ∗ s  = Q∗ s, π∗ s  :  ∀s ∈ S, V ∗ s  = max  a∈A cid:110  E[r s, a ] + γ cid:88 s cid:48 ∈S  P[s cid:48 s, a]V ∗ s cid:48   cid:111 ,  also known as Bellman equations. Note that this system of equations is not linear due to the presence of the max operator.  17.3.4 Policy evaluation The value of a policy at state s can be expressed in terms of its values at other states, forming a system of linear equations. Proposition 17.9  Bellman equations  The values Vπ s  of policy π at states s ∈ S for an inﬁnite horizon MDP obey the following system of linear equations:  ∀s ∈ S, Vπ s  = E a1∼π s   [r s, a1 ] + γ cid:88 s cid:48   P[s cid:48 s, π s ]Vπ s cid:48  .   17.5    386  Chapter 17 Reinforcement Learning  Proof: We can decompose the expression of the policy value as a sum of the ﬁrst term and the rest of the terms, which admit γ as a multiplier:  Vπ s  = E cid:34 +∞ cid:88 t=0  γtr cid:0 st, π st  cid:1  cid:12  cid:12  cid:12  cid:12  s0 = s cid:35  .  = E[r s, π s  ] + γ E cid:34 +∞ cid:88 t=0 = E[r s, π s  ] + γ E cid:34 +∞ cid:88 t=0  = E[r s, π s ] + γ E[Vπ δ s, π s   ].  γtr cid:0 st+1, π st+1  cid:1  cid:12  cid:12  cid:12  cid:12  s0 = s cid:35  γtr cid:0 st+1, π st+1  cid:1  cid:12  cid:12  cid:12  cid:12  s1 = δ s, π s   cid:35   This completes the proof.  This a linear system of equations, also known as Bellman equations, that is distinct from the non-linear system  17.4 . The system can be rewritten as   cid:3   V = R + γPV,   17.6   using the following notation: P denotes the transition probability matrix deﬁned  by Ps,s cid:48  = P[s cid:48 s, π s ] for all s, s cid:48  ∈ S; V is the value column matrix whose sth component is Vs = Vπ s ; and R the reward column matrix whose sth component is Rs = E[r s, π s ]. V is typically the unknown variable in the Bellman equations and is determined by solving for it.  The following theorem shows that, for a ﬁnite MDP, this system of linear equa-  tions admits a unique solution.  Theorem 17.10 For a ﬁnite MDP, Bellman’s equations admit a unique solution given by   17.7   Proof: The Bellman equations  17.6  can be equivalently written as  V0 =  I − γP −1R.   I − γP V = R.  Thus, to prove the theorem it suﬃces to show that  I− γP  is invertible. To do so, note that the inﬁnity of P can be computed using its stochasticity properties:   cid:107 P cid:107 ∞ = max  Pss cid:48  = max  s  cid:88 s cid:48   s  cid:88 s cid:48   P[s cid:48 s, π s ] = 1.  This implies that  cid:107 γP cid:107 ∞ = γ < 1. The eigenvalues of γP are thus all less than  cid:3  one, and  I − γP  is invertible.   17.4 Planning algorithms  387  Thus, for a ﬁnite MDP, when the transition probability matrix P and the reward expectations R are known, the value of policy π at all states can be determined by inverting a matrix.  17.4 Planning algorithms  In this section, we assume that the environment model is known. That is, the  transition probability P[s cid:48 s, a] and the expected reward E[r s, a ] for all s, s cid:48  ∈ S and a ∈ A are assumed to be given. The problem of ﬁnding the optimal policy then does not require learning the parameters of the environment model or estimating other quantities helpful in determining the best course of actions, it is purely a planning problem.  This section discusses three algorithms for this planning problem: the value iter- ation algorithm, the policy iteration algorithm, and a linear programming formu- lation of the problem.  17.4.1 Value iteration The value iteration algorithm seeks to determine the optimal policy values V ∗ s  at each state s ∈ S, and thereby the optimal policy. The algorithm is based on the Bellman equations  17.4 . As already indicated, these equations do not form a system of linear equations and require a diﬀerent technique to determine the solution. The main idea behind the design of the algorithm is to use an iterative method to solve them: the new values of V  s  are determined using the Bellman equations and the current values. This process is repeated until a convergence condition is met.  For a vector V in RS, we denote by V  s  its sth coordinate, for any s ∈ S. Let Φ : RS → RS be the mapping deﬁned based on Bellman’s equations  17.4 :  ∀s ∈ S, [Φ V ] s  = max  a∈A cid:110  E[r s, a ] + γ cid:88 s cid:48 ∈S  P[s cid:48 s, a]V  s cid:48   cid:111 .  The maximizing actions a ∈ A in these equations deﬁne an action to take at each state s ∈ S, that is a policy π. We can thus rewrite these equations in matrix terms as follows:   17.8    17.9   Φ V  = max  π {Rπ + γPπV},  where Pπ is the transition probability matrix deﬁned by  Pπ ss cid:48  = P[s cid:48 s, π s ] for all s, s cid:48  ∈ S, and Rπ the reward vector deﬁned by  Rπ s = E[r s, π s ], for all s ∈ S. The algorithm is directly based on  17.9 . The pseudocode is given above. Start- ing from an arbitrary policy value vector V0 ∈ RS, the algorithm iteratively applies   388  Chapter 17 Reinforcement Learning  ValueIteration V0  1 V ← V0 2 while  cid:107 V − Φ V  cid:107  ≥  1−γ  cid:15  3   cid:46  V0 arbitrary value do  γ  V ← Φ V   4  return Φ V   Figure 17.4 Value iteration algorithm.  Φ to the current V to obtain a new policy value vector until  cid:107 V− Φ V  cid:107  <  1−γ  cid:15  , where  cid:15  > 0 is a desired approximation. The following theorem proves the conver- gence of the algorithm to the optimal policy values.  γ  Theorem 17.11 For any initial value V0, the sequence deﬁned by Vn+1 = Φ Vn  converges to V∗.  Proof: We ﬁrst show that Φ is γ-Lipschitz for the  cid:107  ·  cid:107 ∞.23 For any s ∈ S and V ∈ RS, let a∗ s  be the maximizing action deﬁning Φ V  s  in  17.8 . Then, for any s ∈ S and any U ∈ RS, Φ V  s  − Φ U  s  ≤ Φ V  s  − cid:16  E[r s, a∗ s  ] + γ cid:88 s cid:48 ∈S P[s cid:48   s, a∗ s ]U s cid:48   cid:17  P[s cid:48 s, a∗ s ][V s cid:48   − U s cid:48  ] P[s cid:48 s, a∗ s ] cid:107 V − U cid:107 ∞ = γ cid:107 V − U cid:107 ∞.  = γ cid:88 s cid:48 ∈S ≤ γ cid:88 s cid:48 ∈S  Proceeding similarly with Φ U  s  − Φ V  s , we obtain Φ U  s  − Φ V  s  ≤ γ cid:107 V − U cid:107 ∞. Thus, Φ V  s  − Φ U  s  ≤ γ cid:107 V − U cid:107 ∞ for all s, which implies   cid:107 Φ V  − Φ U  cid:107 ∞ ≤ γ cid:107 V − U cid:107 ∞,  23 A β-Lipschitz function with β < 1 is also called β-contracting. In a complete metric space, that is a metric space where any Cauchy sequence converges to a point of that space, a β-contracting function f admits a ﬁxed point: any sequence  f  xn  n∈N converges to some x with f  x  = x. RN , N ≥ 1, or, more generally, any ﬁnite-dimensional vector space, is a complete metric space.   17.4 Planning algorithms  389  Figure 17.5 Example of MDP with two states. The state set is reduced to S = {1, 2} and the action set to A = {a, b, c, d}. Only transitions with non-zero probabilities are represented. Each transition is labeled with the action taken followed by a pair [p, r] after a slash separator, where p is the probability of the transition and r the expected reward for taking that transition.  that is the γ-Lipschitz property of Φ. Now, by Bellman equations  17.4 , V∗ = Φ V∗ , thus for any n ∈ N,  cid:107 V∗ − Vn+1 cid:107 ∞ =  cid:107 Φ V∗  − Φ Vn  cid:107 ∞ ≤ γ cid:107 V∗ − Vn cid:107 ∞ ≤ γn+1 cid:107 V∗ − V0 cid:107 ∞, which proves the convergence of the sequence to V∗ since γ ∈  0, 1 . The  cid:15 -optimality of the value returned by the algorithm can be shown as follows. By the triangle inequality and the γ-Lipschitz property of Φ, for any n ∈ N,   cid:3    cid:107 V∗ − Vn+1 cid:107 ∞ ≤  cid:107 V∗ − Φ Vn+1  cid:107 ∞ +  cid:107 Φ Vn+1  − Vn+1 cid:107 ∞  =  cid:107 Φ V∗  − Φ Vn+1  cid:107 ∞ +  cid:107 Φ Vn+1  − Φ Vn  cid:107 ∞ ≤ γ cid:107 V∗ − Vn+1 cid:107 ∞ + γ cid:107 Vn+1 − Vn cid:107 ∞.  Thus, if Vn+1 is the policy value returned by the algorithm, we have   cid:107 V∗ − Vn+1 cid:107 ∞ ≤  γ  1 − γ  cid:107 Vn+1 − Vn cid:107 ∞ ≤  cid:15 .   cid:15    number of iterations.  The convergence of the algorithm is in O log 1 observe that  cid:107 Vn+1−Vn cid:107 ∞ =  cid:107 Φ Vn −Φ Vn−1  cid:107 ∞ ≤ γ cid:107 Vn−Vn−1 cid:107 ∞ ≤ γn cid:107 Φ V0 −V0 cid:107 ∞. Thus, if n is the largest integer such that  1−γ  cid:15  γ ≤  cid:107 Vn+1 − Vn cid:107 ∞, it must verify γ ≤ γn cid:107 Φ V0  − V0 cid:107 ∞ and therefore n ≤ O cid:0  log 1  cid:15  cid:1 .24  1−γ  cid:15   Indeed,  24 Here, the O-notation hides the dependency on the discount factor γ. As a function of γ, the running time is not polynomial.  a [3 4, 2]  c [1, 2]  1  2  a [1 4, 2]  b [1, 2]  d [1, 3]   390  Chapter 17 Reinforcement Learning   cid:46  π0 arbitrary policy  PolicyIteration π0  1 π ← π0 2 π cid:48  ← nil 3 while  π  cid:54 = π cid:48   do 4  V ← Vπ π cid:48  ← π π ← argmaxπ{Rπ + γPπV}  5  6  7  return π  Figure 17.6 Policy iteration algorithm.   cid:46  policy evaluation: solve  I − γPπ V = Rπ.   cid:46  greedy policy improvement.  Figure 17.5 shows a simple example of MDP with two states. The iterated values  of these states calculated by the algorithm for that MDP are given by  Vn+1 1  = max cid:110 2 + γ cid:16  3 Vn+1 2  = max cid:110 3 + γVn 1 , 2 + γVn 2  cid:111 .  Vn 1  +  1 4  4  Vn 2  cid:17 , 2 + γVn 2  cid:111   For V0 1  = −1, V0 2  = 1, and γ = 1 2, we obtain V1 1  = V1 2  = 5 2. Thus, both states seem to have the same policy value initially. However, by the ﬁfth iteration, V5 1  = 4.53125, V5 2  = 5.15625 and the algorithm quickly converges to the optimal values V∗ 1  = 14 3 and V∗ 2  = 16 3 showing that state 2 has a higher optimal value.  17.4.2 Policy iteration An alternative algorithm for determining the best policy consists of using pol- icy evaluations, which can be achieved via a matrix inversion, as shown by theo- rem 17.10. The pseudocode of the algorithm known as policy iteration algorithm is given in ﬁgure 17.6. Starting with an arbitrary action policy π0, the algorithm repeatedly computes the value of the current policy π via that matrix inversion and greedily selects the new policy as the one maximizing the right-hand side of the Bellman equations  17.9 .  The following theorem proves the convergence of the policy iteration algorithm.   17.4 Planning algorithms  391   17.10   Theorem 17.12 Let  Vn n∈N be the sequence of policy values computed by the algo- rithm, then, for any n ∈ N, the following inequalities hold:  Vn ≤ Vn+1 ≤ V∗.  Proof: Let πn+1 be the policy improvement at the nth iteration of the algorithm. We ﬁrst show that  I − γPπn+1 −1 preserves ordering, that is, for any column matrices X and Y in RS, if  Y − X  ≥ 0, then  I − γPπn+1 −1 Y − X  ≥ 0. As shown in the proof of theorem 17.10,  cid:107 γP cid:107 ∞ = γ < 1. Since the radius of convergence of the power series  1− x −1 is one, we can use its expansion and write   I − γPπn+1 −1 =   γPπn+1 k.  ∞ cid:88 k=0  Thus, if Z =  Y − X  ≥ 0, then  I − γPπn+1 −1Z = cid:80 ∞k=0 γPπn+1  kZ ≥ 0, since  the entries of matrix Pπn+1 and its powers are all non-negative as well as those of Z.  Now, by deﬁnition of πn+1, we have  Rπn+1 + γPπn+1Vn ≥ Rπn + γPπn Vn = Vn,  which shows that Rπn+1 ≥  I − γPπn+1 Vn. Since  I − γPπn+1  −1 preserves or- dering, this implies that Vn+1 =  I − γPπn+1 −1Rπn+1 ≥ Vn, which concludes the  cid:3  proof of the theorem.  Note that two consecutive policy values can be equal only at the last iteration of the algorithm. The total number of possible policies is AS, thus this constitutes a straightforward upper bound on the maximal number of iterations. Better upper bounds of the form O cid:0 AS  For the simple MDP shown by ﬁgure 17.5, let the initial policy π0 be deﬁned by π0 1  = b, π0 2  = c. Then, the system of linear equations for evaluating this policy is  S  cid:1  are known for this algorithm.   cid:40 Vπ0 1  = 1 + γVπ0  2   Vπ0 2  = 2 + γVπ0  2 ,  1−γ and Vπ0 2  = 2 1−γ .  which gives Vπ0  1  = 1+γ Theorem 17.13 Let  Un n∈N be the sequence of policy values generated by the value iteration algorithm, and  Vn n∈N the one generated by the policy iteration algo- rithm. If U0 = V0, then,  ∀n ∈ N, Un ≤ Vn ≤ V∗.   17.11   Proof: We ﬁrst show that the function Φ previously introduced is monotonic. Let U and V be such that U ≤ V and let π be the policy such that Φ U  = Rπ+γPπU.   392  Then,  Chapter 17 Reinforcement Learning  Φ U  ≤ Rπ + γPπV ≤ max  π cid:48  {Rπ cid:48  + γPπ cid:48 V} = Φ V .  The proof is by induction on n. Assume that Un ≤ Vn, then by the monotonicity of Φ, we have  Un+1 = Φ Un  ≤ Φ Vn  = max  π {Rπ + γPπVn}.  Let πn+1 be the maximizing policy, that is, πn+1 = argmaxπ{Rπ +γPπVn}. Then,  Φ Vn  = Rπn+1 + γPπn+1Vn ≤ Rπn+1 + γPπn+1Vn+1 = Vn+1,  and thus Un+1 ≤ Vn+1. The theorem shows that the policy iteration algorithm converges in a smaller num- ber of iterations than the value iteration algorithm due to the optimal policy. But, each iteration of the policy iteration algorithm requires computing a policy value, that is, solving a system of linear equations, which is more expensive to compute than an iteration of the value iteration algorithm.   cid:3   17.4.3 Linear programming An alternative formulation of the optimization problem deﬁned by the Bellman equations  17.4  or the proof of Theorem 17.8 is via linear programming  LP , that is an optimization problem with a linear objective function and linear constraints. LPs admit  weakly  polynomial-time algorithmic solutions. There exist a variety of diﬀerent methods for solving relatively large LPs in practice, using the simplex method, interior-point methods, or a variety of special-purpose solutions. All of these methods could be applied in this context.  By deﬁnition, the equations  17.4  are each based on a maximization. These maximizations are equivalent to seeking to minimize all elements of {V  s  : s ∈ S} P[s cid:48 s, a]V  s cid:48  ,  s ∈ S . Thus,  under the constraints V  s  ≥ E[r s, a ] + γ cid:80 s cid:48 ∈S  this can be written as the following LP for any set of ﬁxed positive weights α s  > 0,  s ∈ S :   17.12   min  V  cid:88 s∈S  α s V  s   subject to ∀s ∈ S,∀a ∈ A, V  s  ≥ E[r s, a ] + γ cid:88 s cid:48 ∈S  P[s cid:48 s, a]V  s cid:48  ,   17.5 Learning algorithms  393  where α > 0 is the vector with the sth component equal to α s .25 To make each coeﬃcient α s  interpretable as a probability, we can further add the constraints  that  cid:80 s∈S α s  = 1. The number of rows of this LP is SA and its number  of columns S. The complexity of the solution techniques for LPs is typically more favorable in terms of the number of rows than the number of columns. This motivates a solution based on the equivalent dual formulation of this LP which can be written as  E[r s, a ] x s, a    17.13   max  x  cid:88 s∈S,a∈A subject to ∀s ∈ S, cid:88 a∈A  x s cid:48 , a  = α s cid:48   + γ  cid:88 s∈S,a∈A  P[s cid:48 s, a] x s cid:48 , a   ∀s ∈ S,∀a ∈ A, x s, a  ≥ 0,  and for which the number of rows is only S and the number of columns SA. Here x s, a  can be interpreted as the probability of being in state s and taking action a.  17.5 Learning algorithms  This section considers the more general scenario where the environment model of an MDP, that is the transition and reward probabilities, is unknown. This matches many realistic applications of reinforcement learning where, for example, a robot is placed in an environment that it needs to explore in order to reach a speciﬁc goal. How can an agent determine the best policy in this context? Since the environ- ment models are not known, it may seek to learn them by estimating transition or reward probabilities. To do so, as in the standard case of supervised learning, the agent needs some amount of training information. In the context of reinforcement learning with MDPs, the training information is the sequence of immediate rewards the agent receives based on the actions it has taken.  There are two main learning approaches that can be adopted. One known as the model-free approach consists of learning an action policy directly. Another one, a model-based approach, consists of ﬁrst learning the environment model, and then of using that to learn a policy. The Q-learning algorithm we present for this problem is widely adopted in reinforcement learning and belongs to the family of model-free approaches.  25 Let us emphasize that the LP is only in terms of the variables V  s , as indicated by the subscript of the minimization operator, and not in terms of V  s  and α s .   394  Chapter 17 Reinforcement Learning  The estimation and algorithmic methods adopted for learning in reinforcement learning are closely related to the concepts and techniques in stochastic approxi- mation. Thus, we start by introducing several useful results of this ﬁeld that will be needed for the proofs of convergence of the reinforcement learning algorithms presented.  17.5.1 Stochastic approximation Stochastic approximation methods are iterative algorithms for solving optimization problems whose objective function is deﬁned as the expectation of some random variable, or to ﬁnd the ﬁxed point of a function H that is accessible only through noisy observations. These are precisely the type of optimization problems found in reinforcement learning. For example, for the Q-learning algorithm we will describe, the optimal state-action value function Q∗ is the ﬁxed point of some function H that is deﬁned as an expectation and thus not directly accessible.  We start with a basic result whose proof and related algorithm show the ﬂavor of more complex ones found in stochastic approximation. The theorem is a general- ization of a result known as the strong law of large numbers. It shows that under some conditions on the coeﬃcients, an iterative sequence of estimates µm converges almost surely  a.s.  to the mean of a bounded random variable.  Theorem 17.14  Mean estimation  Let X be a random variable taking values in [0, 1] and let x0, . . . , xm be i.i.d. values of X. Deﬁne the sequence  µm m∈N by  µm+1 =  1 − αm µm + αmxm,  with µ0 = x0, αm ∈ [0, 1], cid:80 m≥0 αm = +∞ and cid:80 m≥0 α2  a.s.  µm  −−→ E[X].  m < +∞. Then,   17.14    17.15   Proof: We give the proof of the L2 convergence. The a.s. convergence is shown later for a more general theorem. By the independence assumption, for m ≥ 0, Var[µm+1] =  1 − αm 2 Var[µm] + α2 m.  17.16  Let  cid:15  > 0 and suppose that there exists N ∈ N such that for all m ≥ N , Var[µm] ≥  cid:15 . Then, for m ≥ N ,  m Var[xm] ≤  1 − αm  Var[µm] + α2  Var[µm+1] ≤ Var[µm] − αm Var[µm] + α2 which implies, by reapplying this inequality, that  m ≤ Var[µm] − αm cid:15  + α2 m,  Var[µm+N ] ≤ Var[µN ] −  cid:15   m+N cid:88 n=N  cid:123  cid:122   αn +  α2 n  ,  m+N cid:88 n=N   cid:125   →−∞ when m→∞   cid:124    17.5 Learning algorithms  395  contradicting Var[µm+N ] ≥ 0. Thus, this contradicts the existence of such an integer N . Therefore, for all N ∈ N, there exists m0 ≥ N such that Var[µm0] ≤  cid:15 . Choose N large enough so that for all m ≥ N , the inequality αm ≤  cid:15  holds. This m m∈N and thus  αm m∈N converges to zero in view is possible since the sequence  α2 of cid:80 m≥0 α2 m < +∞. We will show by induction that for any m ≥ m0, Var[µm] ≤  cid:15 , which implies the statement of the theorem. Assume that Var[µm] ≤  cid:15  for some m ≥ m0. Then, using this assumption, inequality 17.16, and the fact that αm ≤  cid:15 , the following inequality holds:  Var[µm+1] ≤  1 − αm  cid:15  +  cid:15 αm =  cid:15 .  Thus, this proves that limm→+∞ Var[µm] = 0, that is the L2 convergence of µm to  cid:3  E[X]. Note that the hypotheses of the theorem related to the sequence  αm m∈N hold in particular when αm = 1 m . The special case of the theorem with this choice of αm coincides with the strong law of large numbers. This result has tight connections with the general problem of stochastic optimization.  Stochastic optimization is the general problem of ﬁnding the solution to the  equation  where x ∈ RN , when  x = H x ,    H x  cannot be computed, for example, because H is not accessible or because  the cost of its computation is prohibitive;   but an i.i.d. sample of m noisy observations H xi  + wi are available, i ∈ [m], where the noise random variable w has expectation zero: E[w] = 0.  This problem arises in a variety of diﬀerent contexts and applications. As we shall see, it is directly related to the learning problem for MDPs.  One general idea for solving this problem is to use an iterative method and deﬁne a sequence  xt t∈N in a way similar to what is suggested by theorem 17.14:  xt+1 =  1 − αt xt + αt[H xt  + wt]  = xt + αt[H xt  + wt − xt],   17.17    17.18   where  αt t∈N follow conditions similar to those assumed in theorem 17.14. More generally, we consider sequences deﬁned via  xt+1 = xt + αtD xt, wt ,   17.19   where D is a function mapping RN ×RN to RN . There are many diﬀerent theorems  guaranteeing the convergence of this sequence under various assumptions. We will present one of the most general forms of such theorems, which relies on the following result.   396  Chapter 17 Reinforcement Learning    Xt converges to a limit  with probability one .  The following is one of the most general forms of such theorems.  Theorem 17.15  Supermartingale convergence  Let  Xt t∈N,  Yt t∈N, and  Zt t∈N be sequences of non-negative random variables such that  cid:80 +∞t=0 Yt < +∞. Let Ft denote all the information for t cid:48  ≤ t: Ft = { Xt cid:48  t cid:48 ≤t,  Yt cid:48  t cid:48 ≤t,  Zt cid:48  t cid:48 ≤t}. Then, if E cid:104 Xt+1 cid:12  cid:12 Ft cid:105  ≤ Xt + Yt − Zt, the following holds:    cid:80 +∞t=0 Zt < +∞. Theorem 17.16 Let D be a function mapping RN × RN to RN ,  wt t∈N a sequence of random variables in RN ,  αt t∈N a sequence of real numbers, and  xt t∈N a sequence deﬁned by xt+1 = xt + αtD xt, wt  with x0 ∈ RN . Let Ft denote the entire history up to t, that is: Ft = { xt cid:48  t cid:48 ≤t,  wt cid:48  t cid:48 ≤t−1,  αt cid:48  t cid:48 ≤t}, and let Ψ 2 for some x∗ ∈ RN . Assume that D and  α t∈N denote the function x → 1 2 cid:107 x− x∗ cid:107 2 verify the following conditions:   ∃K1, K2 ∈ R : E cid:104  cid:107 D xt, wt  cid:107 2 2 cid:12  cid:12 Ft cid:105  ≤ K1 + K2 Ψ xt ;   ∃c ≥ 0 : ∇Ψ xt  cid:62  E cid:104 D xt, wt  cid:12  cid:12 Ft cid:105  ≤ −c Ψ xt ;   αt > 0, cid:80 +∞t=0 αt = +∞, cid:80 +∞t=0 α2  Then, the sequence xt converges almost surely to x∗:  t < +∞.  xt  a.s.  −−→ x∗.   17.20   Proof: Since function Ψ is quadratic, a Taylor expansion gives Ψ xt+1  = Ψ xt  + ∇Ψ xt  cid:62  xt+1 − xt  + Thus,  1 2   xt+1 − xt  cid:62 ∇2Ψ xt  xt+1 − xt .  α2  α2  t K2  t K1  α2 t 2  α2 t 2   K1 + K2Ψ xt    ≤ Ψ xt  − αtcΨ xt  + = Ψ xt  +  E cid:104  cid:107 D xt, wt  cid:107 2 cid:12  cid:12 Ft cid:105   E cid:104 Ψ xt+1  cid:12  cid:12 Ft cid:105  = Ψ xt  + αt∇Ψ xt  cid:62  E cid:104 D xt, wt  cid:12  cid:12 Ft cid:105  + 2  cid:17 Ψ xt .  2 − cid:16 αtc − Since by assumption the series  cid:80 +∞t=0 α2 2  cid:1 Ψ xt  verges to zero. Therefore, for t suﬃciently large, the term  cid:0 αtc − α2 2  cid:1 Ψ xt  < +∞. Since Ψ xt  converges and cid:80 +∞t=0 α2 verges and cid:80 +∞t=0 cid:0 αtc− α2 +∞, we have  cid:80 +∞t=0 2 Ψ xt  < +∞. But, since  cid:80 +∞t=0 αt = +∞, if the limit of Ψ xt  were non-zero, we would have  cid:80 +∞t=0 αtcΨ xt  = +∞. This implies  has the sign of αtcΨ xt  and is non-negative, since αt > 0, Ψ xt  ≥ 0, and c > 0. Thus, by the supermartingale convergence theorem 17.15, Ψ xt  con- t <  t  t and thus  αt t con-  t is convergent,  α2  t K2  t K2  t K2  α2   17.5 Learning algorithms  397  that the limit of Ψ xt  is zero, that is limt→∞  cid:107 xt − x∗ cid:107 2 → 0, which implies  cid:3  xt  a.s.−−→ x∗.  The following is another related result for which we do not present the full proof.  Theorem 17.17 Let H be a function mapping RN to RN ,  wt t∈N a sequence of ran- dom variables in RN ,  αt t∈N a sequence of real numbers, and  xt t∈N a sequence  deﬁned by  entire history up to t, and assume that the following conditions are met:  ∀s ∈ [N ], xt+1 s  = xt s  + αt s  cid:2 H xt  s  − xt s  + wt s  cid:3 , for some x0 ∈ RN . Deﬁne Ft by Ft = { xt cid:48  t cid:48 ≤t,  wt cid:48  t cid:48 ≤t−1 αt cid:48  t cid:48 ≤t}, that is the   ∃K1, K2 ∈ R : E cid:104  cid:107 wt cid:107 2 s  cid:12  cid:12 Ft cid:105  ≤ K1 + K2  cid:107 xt cid:107 2 for some norm  cid:107  ·  cid:107 ;   E cid:2 wt cid:12  cid:12 Ft cid:3  = 0;   ∀s ∈ [N ], cid:80 +∞t=0 αt s  = +∞, cid:80 +∞t=0 α2   H is a  cid:107  ·  cid:107 ∞-contraction with ﬁxed point x∗. Then, the sequence xt converges almost surely to x∗:  t  s  < +∞; and  The next sections present several learning algorithms for MDPs with an unknown  model.  xt  a.s.  −−→ x∗.   17.21   17.5.2 TD 0  algorithm This section presents an algorithm, TD 0  algorithm, for evaluating a policy in the case where the environment model is unknown. The algorithm is based on Bellman’s linear equations giving the value of a policy π  see proposition 17.9 :  Vπ s  = E[r s, π s ] + γ cid:88 s cid:48   = E  s cid:48  cid:2 r s, π s   + γVπ s cid:48  s cid:3 .  P[s cid:48 s, π s ]Vπ s cid:48    However, here the probability distribution according to which this last expectation is deﬁned is not known. Instead, the TD 0  algorithm consists of   sampling a new state s cid:48 ; and   updating the policy values according to the following, which justiﬁes the name of  the algorithm:  V  s  ←  1 − α V  s  + α[r s, π s   + γV  s cid:48  ] = V  s  + α[r s, π s   + γV  s cid:48   − V  s  ].  cid:125   temporal diﬀerence of V values   cid:123  cid:122    cid:124    17.22   Here, the parameter α is a function of the number of visits to the state s.   398  Chapter 17 Reinforcement Learning  TD 0    1 V ← V0  cid:46  initialization. 2  for t ← 0 to T do  3  4  5  6  7  8  9  s ← SelectState   for each step of epoch t do r cid:48  ← Reward s, π s   s cid:48  ← NextState π, s  V  s  ←  1 − α V  s  + α[r cid:48  + γV  s cid:48  ] s ← s cid:48   return V  The pseudocode of the algorithm is given above. The algorithm starts with an arbitrary policy value vector V0. An initial state is returned by SelectState at the beginning of each epoch. Within each epoch, the iteration continues until a ﬁnal state is found. Within each iteration, action π s  is taken from the current state s following policy π. The new state s cid:48  reached and the reward r cid:48  received are observed. The policy value of state s is then updated according to the rule  17.22  and current state set to be s cid:48 .  The convergence of the algorithm can be proven using theorem 17.17. We will give instead the full proof of the convergence of the Q-learning algorithm, for which that of TD 0  can be viewed as a special case.  17.5.3 Q-learning algorithm This section presents an algorithm for estimating the optimal state-action value function Q∗ in the case of an unknown model. Note that the optimal policy or policy value can be straightforwardly derived from Q∗ via: π∗ s  = argmaxa∈A Q∗ s, a  and V ∗ s  = maxa∈A Q∗ s, a . To simplify the presentation, we will assume a deterministic reward function.  The Q-learning algorithm is based on the equations giving the optimal state-  action value function Q∗  17.1 :  Q∗ s, a  = E[r s, a ] + γ cid:88 s cid:48 ∈S  = E s cid:48   [r s, a  + γ max a∈A  P[s cid:48   s, a]V ∗ s cid:48   Q∗ s cid:48 , a ].   17.5 Learning algorithms  399  Q-Learning π  1 Q ← Q0 2  for t ← 0 to T do   cid:46  initialization, e.g., Q0 = 0.  s ← SelectState   for each step of epoch t do  3  4  5  6  7  8  9  10 return Q  policy π derived from Q, e.g.,  cid:15 -greedy.  a ← SelectAction π, s   cid:46  r cid:48  ← Reward s, a  s cid:48  ← NextState s, a  Q s, a  ← Q s, a  + α cid:2 r cid:48  + γ maxa cid:48  Q s cid:48 , a cid:48   − Q s, a  cid:3  s ← s cid:48   As for the policy values in the previous section, the distribution model is not known. Thus, the Q-learning algorithm consists of the following main steps:   sampling a new state s cid:48 ; and   updating the policy values according to the following: Q s, a  ←  1 − α Q s, a  + α[r s, a  + γ max a cid:48 ∈A  Q s cid:48 , a cid:48  ].   17.23   where the parameter α is a function of the number of visits to the state s.  The algorithm can be viewed as a stochastic formulation of the value iteration algorithm presented in the previous section. The pseudocode is given above. Within each epoch, an action is selected from the current state s using a policy π derived from Q. The choice of the policy π is arbitrary so long as it guarantees that every pair  s, a  is visited inﬁnitely many times. The reward received and the state s cid:48  observed are then used to update Q following  17.23 . Theorem 17.18 Consider a ﬁnite MDP. Assume that for all s ∈ S and a ∈ A, t  s, a  < +∞ with αt s, a  ∈ [0, 1]. Then, the Q-learning algorithm converges to the optimal value Q∗  with probability one .   cid:80 +∞t=0 αt s, a  = +∞, and  cid:80 +∞t=0 α2  Note that the conditions on αt s, a  impose that each state-action pair is visited inﬁnitely many times.   400  Chapter 17 Reinforcement Learning  a cid:48   Proof: Let  Qt s, a  t≥0 denote the sequence of state-action value functions at  s, a  ∈ S × A generated by the algorithm. By deﬁnition of the Q-learning updates,  This can be rewritten as the following for all s ∈ S and a ∈ A:  Qt+1 st, at  = Qt st, at  + α cid:2 r st, at  + γ max Qt+1 s, a  = Qt s, a  + αt s, a  cid:20 r s, a  + γ Qt s cid:48 , a cid:48   −  + γαt s, a  cid:20 max  Qt st+1, a cid:48   − Qt st, at  cid:3 . Qt u, a cid:48   cid:105  − Qt s, a  cid:21  Qt u, a cid:48   cid:105  cid:21  ,  u∼P[·s,a] cid:104 max u∼P[·s,a] cid:104 max  if we deﬁne s cid:48  = NextState s, a  and αt s, a  as 0 if  s, a   cid:54 =  st, at  and αt st, at  otherwise. Now, let Qt denote the vector with components Qt s, a , wt the vector whose s cid:48 th entry is   17.24   E  E  a cid:48   a cid:48   a cid:48   and H Qt  the vector with components H Qt  s, a  deﬁned by  wt s  = max  Qt s cid:48 , a cid:48   −  a cid:48   H Qt  s, a  = r s, a  + γ  a cid:48   E  u∼P[·s,a] cid:104 max u∼P[·s,a] cid:104 max  E  a cid:48   Qt u, a cid:48   cid:105  , Qt u, a cid:48   cid:105  .  Then, in view of  17.24 ,  We now show that the hypotheses of theorem 17.17 hold for Qt and wt, which will imply the convergence of Qt to Q∗. The conditions on αt hold by assumption. By  ∀ s, a  ∈ S × A, Qt+1 s, a  = Qt s, a  + αt s, a  cid:2 H Qt  s, a − Qt s, a  + γwt s  cid:3 . deﬁnition of wt, E[wt cid:12  cid:12 Ft] = 0. Also, for any s cid:48  ∈ S, u∼P[·s,a] cid:104 max Qt s cid:48 , a cid:48   = 2 cid:107 Qt cid:107 ∞.  Qt s cid:48 , a cid:48   + cid:12  cid:12  cid:12  cid:12   wt s  ≤ max ≤ 2 max  Qt u, a cid:48   cid:105  cid:12  cid:12  cid:12  cid:12    max  E  a cid:48   a cid:48   a cid:48   s cid:48    17.5 Learning algorithms  401   cid:3   t  s  cid:12  cid:12 Ft cid:3  ≤ 4 cid:107 Qt cid:107 2  Thus, E cid:2 w2 any Q1, Q2 ∈ RS×A, and  s, a  ∈ S × A, we can write H Q2  x, a  − H Q1  x, a  = cid:12  cid:12  cid:12  cid:12 γ  ∞. Finally, H is a γ-contraction for  cid:107  ·  cid:107 ∞ since for  a cid:48   Q2 u, a cid:48   − max Q2 u, a cid:48   − max [Q2 u, a cid:48   − Q1 u, a cid:48  ]  Q1 u, a cid:48   cid:105  cid:12  cid:12  cid:12  cid:12  Q1 u, a cid:48   cid:12  cid:12  cid:12  cid:105   a cid:48   [Q2 u, a cid:48   − Q1 u, a cid:48  ]  a cid:48   E  E  u∼P[·s,a] cid:104 max u∼P[·s,a] cid:104  cid:12  cid:12  cid:12 max  ≤ γ ≤ γ ≤ γ max = γ cid:107 Q2 − Q1 cid:107 ∞.  u∼P[·s,a] max  max  E  a cid:48   a cid:48   a cid:48   u  Since H is a contraction, it admits a ﬁxed point Q∗: H Q∗  = Q∗.  The choice of the policy π according to which an action a is selected  line 5  is not speciﬁed by the algorithm and, as already indicated, the theorem guarantees the convergence of the algorithm for an arbitrary policy so long as it ensures that every pair  s, a  is visited inﬁnitely many times. In practice, several natural choices are considered for π. One possible choice is the policy determined by the state-action value at time t, Qt. Thus, the action selected from state s is argmaxa∈A Qt s, a . But this choice typically does not guarantee that all actions are taken or that all states are visited. Instead, a standard choice in reinforcement learning is the so- called  cid:15 -greedy policy, which consists of selecting with probability  1−  cid:15   the greedy action from state s, that is, argmaxa∈A Qt s, a , and with probability  cid:15  a random action from s, for some  cid:15  ∈  0, 1 . Another possible choice is the so-called Boltzmann exploration, which, given the current state-action value Q, epoch t ∈ {0, . . . , T}, and current state s, consists of selecting action a with the following probability:  pt as, Q  =  Q s,a   e  τt   cid:80 a cid:48 ∈A e  ,  Q s,a cid:48     τt  where τt is the temperature. τt must be deﬁned so that τt → 0 as t → +∞, which ensures that for large values of t, the greedy action based on Q is selected. This is natural, since as t increases, we can expect Q to be close to the optimal function. On the other hand, τt must be chosen so that it does not tend to 0 too fast to ensure that all actions are visited inﬁnitely often. It can be chosen, for instance, as 1  log nt s  , where nt s  is the number of times s has been visited up to epoch t. Reinforcement learning algorithms include two components: a learning policy, which determines the action to take, and an update rule, which deﬁnes the new estimate of the optimal value function. For an oﬀ-policy algorithm, the update rule does not necessarily depend on the learning policy. Q-learning is an oﬀ-policy algorithm since its update rule  line 8 of the pseudocode  is based on the max   402  Chapter 17 Reinforcement Learning  operator and the comparison of all possible actions a cid:48 , that is the greedy action, which may not coincide with the action recommended by the current the policy π. More generally, an oﬀ-policy algorithm evaluates or improves one policy, while acting based on another policy.  In contrast, the algorithm presented in the next section, SARSA, is an on-policy algorithm. An on-policy algorithm evaluates and improves the current policy used for control. It evaluates the return based on the algorithm’s policy.  17.5.4 SARSA SARSA is also an algorithm for estimating the optimal state-action value function in the case of an unknown model. The pseudocode is given in ﬁgure 17.7. The algorithm is in fact very similar to Q-learning, except that its update rule  line 9 of the pseudocode  is based on the action a cid:48  selected by the learning policy. Thus, SARSA is an on-policy algorithm, and its convergence therefore crucially depends on the learning policy. In particular, the convergence of the algorithm requires, in addition to all actions being selected inﬁnitely often, that the learning policy becomes greedy in the limit. The proof of the convergence of the algorithm is nevertheless close to that of Q-learning.  The name of the algorithm derives from the sequence of instructions deﬁning successively s, a, r cid:48 , s cid:48 , and a cid:48 , and the fact that the update to the function Q depends on the quintuple  s, a, r cid:48 , s cid:48 , a .  17.5.5 TD λ  algorithm Both TD 0  and Q-learning algorithms are only based on immediate rewards. The idea of TD λ  consists instead of using multiple steps ahead. Thus, for n > 1 steps, we would have the update  where Rn  t is deﬁned by  V  s  ← V  s  + α  Rn  t − V  s  ,  Rn  t = rt+1 + γrt+2 + . . . + γn−1rt+n + γnV  st+n .  How should n be chosen? Instead of selecting a speciﬁc n, TD λ  is based on a geometric distribution over all rewards Rn instead of Rn  t =  1−λ  cid:80 +∞n=0 λnRn  t where λ ∈ [0, 1]. Thus, the main update becomes  t , that is, it uses Rλ  t  V  s  ← V  s  + α  Rλ  t − V  s  .  The pseudocode of the algorithm is given above. For λ = 0, the algorithm coincides with TD 0 . λ = 1 corresponds to the total future reward.   17.5 Learning algorithms  403  SARSA π  1 Q ← Q0 2  for t ← 0 to T do   cid:46  initialization, e.g., Q0 = 0.  s ← SelectState   a ← SelectAction π Q , s   cid:46  for each step of epoch t do  3  4  5  6  7  8  9  10  11  12 return Q  Figure 17.7 The SARSA algorithm.  policy π derived from Q, e.g.,  cid:15 -greedy.  r cid:48  ← Reward s, a  s cid:48  ← NextState s, a  a cid:48  ← SelectAction π Q , s cid:48    cid:46  Q s, a  ← Q s, a  + αt s, a  cid:2 r cid:48  + γQ s cid:48 , a cid:48   − Q s, a  cid:3  s ← s cid:48  a ← a cid:48   policy π derived from Q, e.g.,  cid:15 -greedy.  In the previous sections, we presented learning algorithms for an agent navigating in an unknown environment. The scenario faced in many practical applications is more challenging; often, the information the agent receives about the environment is uncertain or unreliable. Such problems can be modeled as partially observable Markov decision processes  POMDPs . POMDPs are deﬁned by augmenting the deﬁnition of MDPs with an observation probability distribution depending on the action taken, the state reached, and the observation. The presentation of their model and solution techniques are beyond the scope of this material.  17.5.6 Large state space In some cases in practice, the number of states or actions to consider for the en- vironment may be very large. For example, the number of states in the game of backgammon is estimated to be over 1020. Thus, the algorithms presented in the previous section can become computationally impractical for such applications. More importantly, generalization becomes extremely diﬃcult.  Suppose we wish to estimate the policy value Vπ s  at each state s using expe- rience obtained using policy π. To cope with the case of large state spaces, we   404  Chapter 17 Reinforcement Learning  TD λ    1 V ← V0  cid:46  initialization. 2 e ← 0 3  for t ← 0 to T do  4  5  6  7  8  9  10  11  12  13  s ← SelectState   for each step of epoch t do s cid:48  ← NextState π, s  δ ← r s, π s   + λV  s cid:48   − V  s  e s  ← λe s  + 1 for u ∈ S do  if u  cid:54 = s then  e u  ← γλe u   V  u  ← V  u  + αδe u   s ← s cid:48   14 return V  can map each state of the environment to RN via a mapping Φ : S → RN , with N relatively small  N ≈ 200 has been used for backgammon  and approximate Vπ s  by a function fw s  parameterized by some vector w. For example, fw could be a linear function deﬁned by fw s  = w · Φ s  for all s ∈ S, or some more complex non-linear function of w. The problem then consists of approximating Vπ with fw and can be formulated as a regression problem. Note, however, that the empirical data available is not i.i.d.  Suppose that at each time step t the agent receives the exact policy value Vπ st . Then, if the family of functions fw is diﬀerentiable, a gradient descent method applied to the empirical squared loss can be used to sequentially update the weight vector w via:  1 2  [Vπ st  − fwt st ]2 = wt + α[Vπ st  − fwt st ]∇wtfwt st . wt+1 = wt − α∇wt It is worth mentioning, however, that for large action spaces, there are simple cases where the methods used do not converge and instead cycle.   17.6 Chapter notes  17.6 Chapter notes  405  Reinforcement learning is an important area of machine learning with a large body of literature. This chapter presents only a brief introduction to this area. For a more detailed study, the reader could consult the book of Sutton and Barto [1998], whose mathematical content is short, or those of Puterman [1994] and Bertsekas [1987], which discuss in more depth several aspects, as well as the more recent book of Szepesv´ari [2010]. The Ph.D. theses of Singh [1993] and Littman [1996] are also excellent sources.  Some foundational work on MDPs and the introduction of the temporal diﬀerence  TD  methods are due to Sutton [1984]. Q-learning was introduced and analyzed by Watkins [1989], though it can be viewed as a special instance of TD methods. The ﬁrst proof of the convergence of Q-learning was given by Watkins and Dayan [1992].  Many of the techniques used in reinforcement learning are closely related to those of stochastic approximation which originated with the work of Robbins and Monro [1951], followed by a series of results including Dvoretzky [1956], Schmetterer [1960], Kiefer and Wolfowitz [1952], and Kushner and Clark [1978]. For a recent survey of stochastic approximation, including a discussion of powerful proof techniques based on ODE  ordinary diﬀerential equations , see Kushner [2010] and the refer- ences therein. The connection with stochastic approximation was emphasized by Tsitsiklis [1994] and Jaakkola et al. [1994], who gave a related proof of the con- vergence of Q-learning. For the convergence rate of Q-learning, consult Even-Dar and Mansour [2003]. For recent results on the convergence of the policy iteration algorithm, see Ye [2011], which shows that the algorithm is strongly polynomial for a ﬁxed discount factor.  Reinforcement learning has been successfully applied to a variety of problems including robot control, board games such as backgammon in which Tesauro’s TD- Gammon reached the level of a strong master [Tesauro, 1995]  see also chapter 11 of Sutton and Barto [1998] , chess, elevator scheduling problems [Crites and Barto, 1996], telecommunications, inventory management, dynamic radio channel assign- ment [Singh and Bertsekas, 1997], and a number of other problems  see chapter 1 of Puterman [1994] .    Conclusion  We described a large variety of machine learning algorithms and techniques and discussed their theoretical foundations as well as their use and applications. While this is not a fully comprehensive presentation, it should nevertheless oﬀer the reader some idea of the breadth of the ﬁeld and its multiple connections with a variety of other domains, including statistics, information theory, optimization, game theory, and automata and formal language theory.  The fundamental concepts, algorithms, and proof techniques we presented should supply the reader with the necessary tools for analyzing other learning algorithms, including variants of the algorithms analyzed in this book. They are also likely to be helpful for devising new algorithms or for studying new learning schemes. We strongly encourage the reader to explore both and more generally to seek enhanced solutions for all theoretical, algorithmic, and applied learning problems.  The exercises included at the end of each chapter, as well as the full solutions we provide separately, should help the reader become more familiar with the techniques and concepts described. Some of them could also serve as a starting point for research work and the investigation of new questions.  Many of the algorithms we presented as well as their variants can be directly used in applications to derive eﬀective solutions to real-world learning problems. Our detailed description of the algorithms and discussion should help with their implementation or their adaptation to other learning scenarios.  Machine learning is a relatively recent ﬁeld and yet probably one of the most active ones in computer science. Given the wide accessibility of digitized data and its many applications, we can expect it to continue to grow at a very fast pace over the next few decades. Learning problems of diﬀerent nature, some arising due to the substantial increase of the scale of the data, which already requires processing billions of records in some applications, others related to the introduction of completely new learning frameworks, are likely to pose new research challenges and require novel algorithmic solutions. In all cases, learning theory, algorithms,   408  Conclusion  and applications form an exciting area of computer science and mathematics, which we hope this book could at least partly communicate.   A Linear Algebra Review  In this appendix, we introduce some basic notions of linear algebra relevant to the material presented in this book. This appendix does not represent an exhaustive tutorial, and it is assumed that the reader has some prior knowledge of the subject.  A.1 Vectors and norms  We will denote by H a vector space whose dimension may be inﬁnite.  A.1.1 Norms Deﬁnition A.1 A mapping Φ : H → R+ is said to deﬁne a norm on H if it veriﬁes the following axioms:    deﬁniteness: ∀x ∈ H, Φ x  = 0 ⇔ x = 0;   homogeneity: ∀x ∈ H, ∀α ∈ R, Φ αx  = αΦ x ;   triangle inequality: ∀x, y ∈ H, Φ x + y  ≤ Φ x  + Φ y . A norm is typically denoted by  cid:107  ·  cid:107 . Examples of vector norms are the absolute value on R and the Euclidean  or L2  norm on RN . More generally, for any p ≥ 1 the Lp norm is deﬁned on RN  cid:16  N cid:88  as   A.1  The L1, L2, and L∞ norms are some of the most commonly used norms, where  cid:107 x cid:107 ∞ = maxj∈[N ] xj. Two norms  cid:107  ·  cid:107  and  cid:107  ·  cid:107  cid:48  are said to be equivalent iﬀ there exists α, β > 0 such that for all x ∈ H,  xjp cid:17 1 p  ∀x ∈ RN ,   cid:107 x cid:107 p =  j=1  .  The following general inequalities relating these norms can be proven straightforwardly:  α cid:107 x cid:107  ≤  cid:107 x cid:107  cid:48  ≤ β cid:107 x cid:107 .  √ N cid:107 x cid:107 2  cid:107 x cid:107 2 ≤  cid:107 x cid:107 1 ≤ √  cid:107 x cid:107 ∞ ≤  cid:107 x cid:107 2 ≤ N cid:107 x cid:107 ∞  cid:107 x cid:107 ∞ ≤  cid:107 x cid:107 1 ≤ N cid:107 x cid:107 ∞.   A.5  The second inequality of the ﬁrst line can be shown using the Cauchy-Schwarz inequality presented later while the other inequalities are clear. These inequalities show the equivalence of these three norms. More generally, all norms on a ﬁnite-dimensional space are equivalent. The following additional properties hold for the L∞ norm: for all x ∈ H,  ∀p ≥ 1,  cid:107 x cid:107 ∞ ≤  cid:107 x cid:107 p ≤ N 1 p cid:107 x cid:107 ∞ p→+∞ cid:107 x cid:107 p =  cid:107 x cid:107 ∞.  lim   A.2    A.3    A.4    A.6    A.7    410  Appendix A Linear Algebra Review  The inequalities of the ﬁrst line are straightforward and imply the limit property of the second line.  Deﬁnition A.2  Hilbert space  A Hilbert space is a vector space equipped with an inner product  cid:104 ·, · cid:105  and that is complete  all Cauchy sequences are convergent . The inner product induces a norm deﬁned as follows:   cid:107 x cid:107 H = cid:112  cid:104 x, x cid:105 .  ∀x ∈ H,   A.8     cid:104 y, x cid:105   .  ∀y ∈ RN ,  A.1.2 Dual norms Deﬁnition A.3 Let  cid:107  ·  cid:107  be a norm on RN . Then, the dual norm  cid:107  ·  cid:107 ∗ associated to  cid:107  ·  cid:107  is the norm deﬁned by   cid:107 y cid:107 ∗ = sup  cid:107 x cid:107 =1 For any p, q ≥ 1 that are conjugate that is such that 1 q = 1, the Lp and Lq norms are dual norms of each other. In particular, the dual norm of L2 is the L2 norm, and the dual norm of the L1 norm is the L∞ norm. Proposition A.4  H ¨older’s inequality  Let p, q ≥ 1 be conjugate: 1 x, y ∈ RN , with equality when yi = xip−1 for all i ∈ [N ]. Proof: The statement holds trivially for x = 0 or y = 0; thus, we can assume x  cid:54 = 0 and y  cid:54 = 0. Let a, b > 0. By the concavity of log  see deﬁnition B.7 , we can write    cid:104 x, y cid:105   ≤  cid:107 x cid:107 p cid:107 y cid:107 q,  q = 1. Then, for all  p + 1  p + 1   A.10    A.9   log  ap +  bq  log ap  +  log bq  = log a  + log b  = log ab .  Taking the exponential of the left- and right-hand sides gives   cid:18  1  p   cid:19   1 q  ≥ 1 p  1 q  1 q  ap +  bq ≥ ab,  1 p  which is known as Young’s inequality. Using this inequality with a = xj  cid:107 x cid:107 p and b = yj  cid:107 y cid:107 q for j ∈ [N ] and summing up gives   cid:80 N j=1 xj yj  cid:107 x cid:107 p cid:107 y cid:107 q  ≤ 1 p   cid:107 x cid:107 p  cid:107 x cid:107 p   cid:107 y cid:107 q  cid:107 y cid:107 q  1 q  +  =  +  = 1.  1 p  1 q  j=1 xj yj, the inequality claim follows. The equality case can be veriﬁed  cid:3   Since   cid:104 x, y cid:105   ≤  cid:80 N  straightforwardly.  Taking p = q = 2 immediately yields the following result known as the Cauchy-Schwarz inequality. Corollary A.5  Cauchy-Schwarz inequality  For all x, y ∈ RN ,    cid:104 x, y cid:105   ≤  cid:107 x cid:107 2 cid:107 y cid:107 2,  with equality iﬀ x and y are collinear.  Let H be the hyperplane in RN whose equation is given by  w · x + b = 0,  Then, the following identity holds for all p ≥ 1: dp x, H  =  w · x + b   cid:107 w cid:107 q  ,  dp x, H  = inf x cid:48 ∈H   cid:107 x cid:48  − x cid:107 p.  for some normal vector w ∈ RN and oﬀset b ∈ R. Let dp x, H  denote the distance of x to the hyperplane H, that is,   A.11    A.12    A.13   where q is the conjugate of p: 1 of the results of appendix B to the constrained optimization problem  A.12 .  q = 1.  A.13  can be shown by a straightforward application  p + 1   A.2 Matrices  411  A.1.3 Relationship between norms A general form for the inequalities seen in equations  A.3 ,  A.4  and  A.5 , which holds for all Lp norms, is shown in the following proposition. Proposition A.6 Let 1 ≤ p ≤ q. Then the following inequalities hold for all x ∈ RN :   A.14  Proof: First, assume x  cid:54 = 0, otherwise the inequalities hold trivially. Then the ﬁrst inequality holds using 1 ≤ p ≤ q as follows:   cid:107 x cid:107 q ≤  cid:107 x cid:107 p ≤ N  1  p − 1  q  cid:107 x cid:107 q .   cid:20  cid:107 x cid:107 p   cid:107 x cid:107 q   cid:35  1 p ≤  =  i=1   cid:21 p  N cid:88   cid:32  N cid:88   i=1   cid:20  xi cid:107 x cid:107 q  cid:21 p ≥ N cid:88   cid:124  cid:123  cid:122  cid:125 ≤1  cid:33  p q  cid:32  N cid:88   i=1  = 1.   cid:21 q   cid:20  xi cid:107 x cid:107 q q 1  cid:33 1− p  q−p  q  p   xip   q p   1   i=1  =  cid:107 x cid:107 qN  1  p − 1 q ,   cid:3   Finally, the second inequality follows by using H¨older’s inequality  proposition A.4    cid:34  N cid:88   i=1   cid:107 x cid:107 p =  xip  which completes the proof.  A.2 Matrices  For a matrix M ∈ Rm×n with m rows and n columns, we denote by Mij its ijth entry, for all i ∈ [m] and j ∈ [n]. For any m ≥ 1, we denote by Im the m-dimensional identity matrix, and refer to it as I when the dimension is clear from the context. The transpose of M is denoted by M cid:62  and deﬁned by  M cid:62  ij = Mji for all  i, j . For any two matrices M ∈ Rm×n and N ∈ Rn×p,  MN  cid:62  = N cid:62 M cid:62 . M is said to be symmetric iﬀ Mij = Mji for all  i, j , that is, iﬀ M = M cid:62 . i=1 Mii. For any two matrices M ∈ Rm×n and N ∈ Rn×m, the following identity holds: Tr[MN] = Tr[NM]. More generally, the following cyclic property holds with the appropriate dimensions for the matrices M, N, and P:  The trace of a square matrix M is denoted by Tr[M] and deﬁned as Tr[M] = cid:80 N   A.15  The inverse of a square matrix M, which exists when M has full rank, is denoted by M−1 and  Tr[MNP] = Tr[PMN] = Tr[NPM].  is the unique matrix satisfying MM−1 = M−1M = I.  A.2.1 Matrix norms A matrix norm is a norm deﬁned over Rm×n where m and n are the dimensions of the matrices considered. Many matrix norms, including those discussed below, satisfy the following submulti- plicative property:  A.16  The matrix norm induced by the vector norm  cid:107  ·  cid:107 p or the operator norm induced by that norm is also denoted by  cid:107  ·  cid:107 p and deﬁned by   cid:107 MN cid:107  ≤  cid:107 M cid:107  cid:107 N cid:107 .  The norm induced for p = 2 is known as the spectral norm, which equals the largest singular value of M  see section A.2.2 , or the square-root of the largest eigenvalue of M cid:62 M:   cid:107 M cid:107 p = sup  cid:107 x cid:107 p≤1   cid:107 Mx cid:107 p .   cid:113    cid:107 M cid:107 2 = σ1 M  =  λmax M cid:62 M .   A.17    A.18    412  Appendix A Linear Algebra Review  Not all matrix norms are induced by vector norms. The Frobenius norm denoted by  cid:107  ·  cid:107 F is the most notable of such norms and is deﬁned by:   cid:18  m cid:88   n cid:88   i=1  j=1   cid:19 1 2  M2 ij  .   cid:107 M cid:107 F =  The Frobenius norm can be interpreted as the L2 norm of a vector when treating M as a vector of size mn. It also coincides with the norm induced by the Frobenius product, which is the inner product deﬁned for all M, N ∈ Rm×n by   A.19   This relates the Frobenius norm to the singular values of M:   cid:104 M, N cid:105 F = Tr[M cid:62 N]. r cid:88   F = Tr[M cid:62 M] =   cid:107 M cid:107 2  σi M 2 ,  i=1  where r = rank M . The second equality follows from properties of SPSD matrices  see sec- tion A.2.3 . For any j ∈ [n], let Mj denote the jth column of M, that is M = [M1 · · · Mn]. Then, for any p, r ≥ 1, the Lp,r group norm of M is deﬁned by  One of the most commonly used group norms is the L2,1 norm deﬁned by   cid:107 M cid:107 p,r =   cid:107 Mi cid:107 r  p   cid:19 1 r  .   cid:107 M cid:107 2,1 =   cid:107 Mi cid:107 2 .   cid:18  n cid:88  n cid:88   j=1  i=1  M = UM ΣM V cid:62  M .  A.2.2 Singular value decomposition The compact singular value decomposition  SVD  of M, with r = rank M  ≤ min m, n , can be written as follows: The r × r matrix ΣM = diag σ1, . . . , σr  is diagonal and contains the non-zero singular values of M sorted in decreasing order, that is σ1 ≥ . . . ≥ σr > 0. The matrices UM ∈ Rm×r and VM ∈ Rn×r have orthonormal columns that contain the left and right singular vectors of M corresponding to the sorted singular values. We denote by Uk ∈ Rm×k the top k ≤ r left singular vectors of M.  The orthogonal projection onto the span of Uk can be written as PUk = UkU cid:62   k , where PUk is SPSD and idempotent, i.e., P2 = PUk . Moreover, the orthogonal projection onto the subspace orthogonal to Uk is deﬁned as PUk,⊥. Similar deﬁnitions, i.e., Vk, PVk , PVk,⊥, hold for the right singular vectors. The generalized inverse, or Moore-Penrose pseudo-inverse of a matrix M is denoted by M†  Uk  and deﬁned by  † M = diag σ−1 where Σ the pseudo-inverse coincides with the matrix inverse: M† = M−1.  1 , . . . , σ−1   A.20  r  . For any square m × m matrix M with full rank, i.e., r = m,  † M† = UM Σ M V cid:62  M ,  A.2.3 Symmetric positive semideﬁnite  SPSD  matrices Deﬁnition A.7 A symmetric matrix M ∈ Rm×m is said to be positive semideﬁnite iﬀ  for all x ∈ Rm. M is said to be positive deﬁnite if the inequality is strict.  x cid:62 Mx ≥ 0   A.21   Kernel matrices  see chapter 6  and orthogonal projection matrices are two examples of SPSD matrices. It is straightforward to show that a matrix M is SPSD iﬀ its eigenvalues are all non- negative. Furthermore, the following properties hold for any SPSD matrix M:   A.2 Matrices  413    M admits a decomposition M = X cid:62 X for some matrix X and the Cholesky decomposition provides one such decomposition in which X is an upper triangular matrix.    The left and right singular vectors of M are the same and the SVD of M is also its eigenvalue decomposition.   The SVD of an arbitrary matrix X = UX ΣX V cid:62  X deﬁnes the SVD of two related SPSD matrices: the left singular vectors  UX   are the eigenvectors of XX cid:62 , the right singular vectors  VX   are the eigenvectors of X cid:62 X and the non-zero singular values of X are the square roots of the non-zero eigenvalues of XX cid:62  and X cid:62 X.    The trace of M is the sum of its singular values, i.e., Tr[M] = cid:80 r  i=1 σi M , where rank M  = r.    The top singular vector of M, u1, maximizes the Rayleigh quotient, which is deﬁned as  In other words, u1 = argmaxx r x, M  and r u, M  = σ1 M . Similarly, if M cid:48  = PUi,⊥M, that is, the projection of M onto the subspace orthogonal to Ui, then ui+1 = argmaxx r x, M cid:48  , where ui+1 is the  i + 1 st singular vector of M.  r x, M  =  x cid:62 Mx x cid:62 x  .    B Convex Optimization  In this appendix, we introduce the main deﬁnitions and results of convex optimization needed for the analysis of the learning algorithms presented in this book.  B.1 Differentiation and unconstrained optimization  We start with some basic deﬁnitions for diﬀerentiation needed to present Fermat’s theorem and to describe some properties of convex functions. Deﬁnition B.1  Gradient  Let f : X ⊆ RN → R be a diﬀerentiable function. Then, the gradient of f at x ∈ X is the vector in RN denoted by ∇f  x  and deﬁned by  ∇f  x  =    cid:104  ∂2f   .  ∂f ∂x1   x   ...  ∂f ∂xN   x    cid:105   ∇2f  x  =  ∂xi, xj   x   1≤i,j≤N  .  Deﬁnition B.2  Hessian  Let f : X ⊆ RN → R be a twice diﬀerentiable function. Then, the Hessian of f at x ∈ X is the matrix in RN×N denoted by ∇2f  x  and deﬁned by  Next, we present a classic result for unconstrained optimization. Theorem B.3  Fermat’s theorem  Let f : X ⊆ RN → R be a diﬀerentiable function. If f admits a local extremum at x∗ ∈ X, then ∇f  x∗  = 0, that is, x∗ is a stationary point.  B.2 Convexity  This section introduces the notions of convex sets and convex functions. Convex functions play an important role in the design and analysis of learning algorithms, in part because a local minimum of a convex function is necessarily also a global minimum. Thus, the properties of a hypothesis that is learned by ﬁnding a local minimum of a convex optimization are often well understood, while for some non-convex optimization problems there may be a very large number of local minima for which no clear characterization of the learned hypothesis can be given. Deﬁnition B.4  Convex set  A set X ⊆ RN is said to be convex if for any two points x, y ∈ X the segment [x, y] lies in X, that is  {αx +  1 − α y : 0 ≤ α ≤ 1} ⊆ X.   416  Appendix B Convex Optimization  Figure B.1 Examples of a convex  left  and a concave  right  functions. Note that any line segment drawn between two points on the convex function lies entirely above the graph of the function while any line segment drawn between two points on the concave function lies entirely below the graph of the function.  The following lemma illustrates several operations on convex sets that preserve convexity. These  deﬁned, is convex.  i∈I Ci is also convex.  of these sets  cid:84   Proof: The ﬁrst property holds since for any x, y ∈  cid:84   will be useful for proving several subsequent results of this section. Lemma B.5  Operations that preserve convexity of sets  The following operations on convex sets preserve convexity:   Let {Ci}i∈I be any family of sets where for all i ∈ I the set Ci is convex. Then the intersection   Let C1 and C2 be convex sets, then their sum C1 + C2 = {x1 + x2 : x1 ∈ C1, x2 ∈ C2}, when   Let C1 and C2 be convex sets, then their cross-product  C1 × C2  is also convex.   Any projection of a convex set C is also convex. i∈I Ci and any α ∈ [0, 1], we have αx +  1 − α y ∈ Ci for any i ∈ I by the convexity of Ci. The second property holds since for any  x1 + x2 ,  y1 + y2  ∈  C1 + C2  we have α x1 + x2  +  1 − α  y1 + y2  =  αx1 +  1 − α y1 + αx2 +  1 − α y2  ∈  C1 + C2 , which follows since αx1 +  1 − α y1 ∈ C1 and αx2 +  1 − α y2 ∈ C2. The third property holds since for  x1, x2 ,  y1, y2  ∈  C1 × C2  we have α x1, x2  +  1 − α  y1, y2  =  αx1 +  1 − α y1, αx2 +  1 − α y2  ∈  C1 × C2 , where the membership holds due to the assumption that C1 and C2 are convex. Finally, the fourth property holds by noting that for any decomposition of the convex set C into projections C1 and C2, such that C =  C1 × C2 , it must be the case that C1 is convex. If C2 is empty, then the result is trivially true. Otherwise, ﬁx an element x2 ∈ C2, then for any x, y ∈ C1 and any α ∈ [0, 1] we have α x, x2  +  1 − α  y, x2  ∈ C, which implies αx +  1 − α y ∈ C1. Since  cid:3  C1 was chosen arbitrarily, this fact holds for any projection of C. Note that many set operations may not preserve convexity. Consider the union of disjoint intervals [a, b] ∪ [c, d] where a < b < c < d. Clearly [a, b] and [c, d] are convex, however we have on R: 2 b +  1 − 1 Deﬁnition B.6  Convex hull  The convex hull conv X  of a set of points X ⊆ RN is the minimal convex set containing X and can be equivalently deﬁned as follows:  1  2  c  cid:54 ∈  [a, b] ∪ [c, d] .  cid:110  m cid:88   conv X  =   B.1  Let Epi f denote the epigraph of function f : X → R, that is the set of points lying above its graph: { x, y  : x ∈ X, y ≥ f  x }.  αi = 1  i=1  i=1  .  αixi : m ≥ 1, ∀i ∈ [m], xi ∈ X, αi ≥ 0,  m cid:88    cid:111    B.2 Convexity  417  Figure B.2 Illustration of the ﬁrst-order property satisﬁed by all convex functions.  Deﬁnition B.7  Convex function  Let X be a convex set. A function f : X → R is said to be convex iﬀ Epi f is a convex set, or, equivalently, if for all x, y ∈ X and α ∈ [0, 1],  f  αx +  1 − α y  ≤ αf  x  +  1 − α f  y  .   B.2  f is said to be strictly convex if inequality  B.2  is strict for all x, y ∈ X where x  cid:54 = y and α ∈  0, 1 . f is said to be  strictly  concave when −f is  strictly  convex. Figure B.1 shows simple examples of convex and concave functions. Convex functions can also be characterized in terms of their ﬁrst- or second-order diﬀerential. Theorem B.8 Let f be a diﬀerentiable function, then f is convex if and only if dom f   is convex and the following inequalities hold:  ∀x, y ∈ dom f  , f  y  − f  x  ≥ ∇f  x  ·  y − x  .   B.3   The property  B.3  is illustrated by ﬁgure B.2: for a convex function, the hyperplane tangent at x is always below the graph. Theorem B.9 Let f be a twice diﬀerentiable function, then f is convex iﬀ dom f   is convex and its Hessian is positive semideﬁnite:  ∀x ∈ dom f  , ∇2f  x   cid:23  0 .  Recall that a symmetric matrix is positive semideﬁnite if all of its eigenvalues are non-negative. Further, note that when f is scalar, this theorem states that f is convex if and only if its second derivative is always non-negative, that is, for all x ∈ dom f  , f cid:48  cid:48  x  ≥ 0. Example B.10  Linear functions  Any linear function f is both convex and concave, since equa- tion  B.2  holds with equality for both f and −f by the deﬁnition of linearity. Example B.11  Quadratic function  The function f : x  cid:55 → x2 deﬁned over R is convex since it is twice diﬀerentiable and for all x ∈ R, f cid:48  cid:48  x  = 2 > 0. Example B.12  Norms  Any norm  cid:107 · cid:107  deﬁned over a convex set X is convex since by the triangle inequality and the homogeneity property of the norm, for all α ∈ [0, 1], x, y ∈ X, we can write   cid:107 αx +  1 − α y cid:107  ≤  cid:107 αx cid:107  +  cid:107  1 − α y cid:107  = α cid:107 x cid:107  +  1 − α  cid:107 y cid:107  .   418  Appendix B Convex Optimization  Example B.13  Maximum function  The max function deﬁned for all x ∈ RN , by x  cid:55 → maxj∈[N ] xj is convex. For all α ∈ [0, 1], x, y ∈ RN , by the sub-additivity of max, we can write   αxj +  1 − α yj   ≤ max   αxj   + max  j  j    1 − α yj   = α max   xj   +  1 − α  max   yj   .  j  j  max  j  One useful approach for proving convexity or concavity of functions is to make use of composition rules. For simplicity of presentation, we will assume twice diﬀerentiability, although the results can also be proven without this assumption. Lemma B.14  Composition of convex concave functions  Assume h : R → R and g : RN → R are twice diﬀerentiable functions and for all x ∈ RN , deﬁne f  x  = h g x  . Then the following implications are valid:    h is convex and non-decreasing, and g is convex =⇒ f is convex.   h is convex and non-increasing, and g is concave =⇒ f is convex.   h is concave and non-decreasing, and g is concave =⇒ f is concave.   h is concave and non-increasing, and g is convex =⇒ f is concave. Proof: We restrict ourselves to N = 1, since it suﬃces to prove convexity  concavity  along all arbitrary lines that intersect the domain. Now, consider the second derivative of f :  f cid:48  cid:48  x  = h cid:48  cid:48  g x  g cid:48  x 2 + h cid:48  g x  g cid:48  cid:48  x  .   B.4  Note that if h is convex and non-decreasing, we have h cid:48  cid:48  ≥ 0 and h cid:48  ≥ 0. Furthermore, if g is convex we also have g cid:48  cid:48  ≥ 0, and it follows that f cid:48  cid:48  x  ≥ 0, which proves the ﬁrst statement. The  cid:3  remainder of the statements are proven in a similar manner. Example B.15  Composition of functions  The previous lemma shows the convexity or concav- ity of the following composed functions:   If f : RN → R is convex, then exp f   is convex.   Any squared norm  cid:107  ·  cid:107 2 is convex.    For all x ∈ RN the function x  cid:55 → log  cid:80 N  j=1 xj   is concave.  The following two lemmas give examples of two other operations preserving convexity.  Lemma B.16  Pointwise supremum or maximum of convex functions  Let  fi i∈I be a fam- ily of convex functions deﬁned over a convex set C. Then, their pointwise supremum f deﬁned for all x ∈ C by f  x  = supi∈I fi x   resp. their pointwise maximum if I < +∞  is a convex function. Proof: Observe that Epi f = ∩i∈I Epi fi and is therefore convex as an intersection of convex  cid:3  sets. Example B.17  Pointwise supremum of convex functions  The lemma shows in particular the convexity of the following functions:   A piecewise linear function f deﬁned for all x ∈ RN by f  x  = maxi∈[m] w cid:62   i x + bi is convex as  a pointwise maximum of aﬃne  and thus convex  functions.    The maximum eigenvalue λmax M  is a convex function over the set of symmetric matrices M since the set of symmetric matrices is convex and since λmax M  = sup cid:107 x cid:107 2≤1 x cid:62 Mx is deﬁned as the supremum of the linear  and thus convex  functions M  cid:55 → x cid:62 Mx. n matrix M. Then, by a similar argument, M  cid:55 →  cid:80 k   More generally, let λ1 M , . . . , λk M  denote the top k ≤ n eigenvalues of a symmetric n ×  cid:80 k i=1 λi M  is a convex function since M  cid:55 → cid:80 n i=1 λi M  or M  cid:55 → cid:80 n    Using the previous property, along with the fact that Tr M  is linear in M, also shows that i=1 λi −M   i=k+1 λi M  = Tr M  − cid:80 k  i=n−k+1 λi M  = − cid:80 k  i Mui, where u1, . . . , uk is an orthonormal basis of V.   cid:80 k i=1 u cid:62   i=1 λi M  = supdim V =k  are concave functions.   B.3 Constrained optimization  419  Lemma B.18  Partial inﬁmum  Let f be a convex function deﬁned over a convex set C ⊆ X × Y and let B ⊆ Y be a convex set such that A = {x ∈ X : ∃y ∈ B   x, y  ∈ C} is non-empty. Then, A is a convex set and the function g deﬁned for all x ∈ A by g x  = inf y∈B f  x, y  is convex. Proof: First note that the intersection of the convex sets C and  X × B  is convex. Thus, A is convex since it is the projection of the convex set C ∩  X × B  onto X. Let x1 and x2 be in A. By deﬁnition of g, for any  cid:15  > 0, there exist y1, y2 ∈ B with  x1, y1 ,  x2, y2  ∈ C such that f  x1, y1  ≤ g x1  +  cid:15  and f  x2, y2  ≤ g x2  +  cid:15 . Then, for any α ∈ [0, 1],  g αx1 +  1 − α x2  = inf y∈B  f  αx1 +  1 − α x2, y   ≤ f  αx1 +  1 − α x2, αy1 +  1 − α y2  ≤ αf  x1, y1  +  1 − α f  x2, y2  ≤ αg x1  +  1 − α g x2  +  cid:15 .  Since the inequality holds for all  cid:15  > 0, it implies  g αx1 +  1 − α x2  ≤ αg x1  +  1 − α g x2 ,  which completes the proof. Example B.19 The lemma shows in particular that the distance to a convex set B, d x, B  = inf y∈B  cid:107 x − y cid:107 , is a convex function of x in any normed vector space, since  x, y   cid:55 →  cid:107 x − y cid:107  is jointly convex in x and y for any norm  cid:107  ·  cid:107 .   cid:3   The following is a useful inequality applied in a variety of contexts. It is in fact a quasi-direct  consequence of the deﬁnition of convexity. Theorem B.20  Jensen’s inequality  Let X be a random variable taking values in a non-empty convex set C ⊆ RN with a ﬁnite expectation E[X], and f a measurable convex function deﬁned over C. Then, E[X] is in C, E[f  X ] is ﬁnite, and the following inequality holds:  Proof: We give a sketch of the proof, which essentially follows from the deﬁnition of convexity. Note that for any ﬁnite set of elements x1, . . . , xn in C and any positive reals α1, . . . , αn such that   cid:80 n  i=1 αi = 1, we have  f  E[X]  ≤ E[f  X ].   cid:16  n cid:88   i=1  f   cid:17  ≤ n cid:88   i=1  αixi  αif  xi  .  This follows straightforwardly by induction from the deﬁnition of convexity. Since the αis can be interpreted as probabilities, this immediately proves the inequality for any distribution with a ﬁnite support deﬁned by α =  α1, . . . , αn : f  E  [X]  ≤ E  [f  X ] .  α  α  Extending this to arbitrary distributions can be shown via the continuity of f on any open set, which is guaranteed by the convexity of f , and the weak density of distributions with ﬁnite support  cid:3  in the family of all probability measures.  B.3 Constrained optimization  We now deﬁne a general constrained optimization problem and the speciﬁc properties associated to convex constrained optimization problems.   420  Appendix B Convex Optimization  Deﬁnition B.21  Constrained optimization problem  Let X ⊆ RN and f, gi : X → R, for all i ∈ [m]. Then, a constrained optimization problem has the form:  min x∈X  f  x   subject to: gi x  ≤ 0, ∀i ∈ {1, . . . , m}.  m cid:88   i=1  This general formulation does not make any convexity assumptions and can be augmented with equality constraints. It is referred to as the primal problem in contrast with a related problem introduced later. We will denote by p∗ the optimal value of the objective. For any x ∈ X, we will denote by g x  the vector  g1 x , . . . , gm x   cid:62 . Thus, the constraints can be written as g x  ≤ 0. To any constrained optimization problem, we can associate a Lagrange function that plays an important role in the analysis of the problem and its relationship with another related optimization problem. Deﬁnition B.22  Lagrangian  The Lagrange function or the Lagrangian associated to the general constrained optimization problem deﬁned in  B.21  is the function deﬁned over X × R+ by:  ∀x ∈ X, ∀α ≥ 0, L x, α  = f  x  +  αigi x  ,  where the variables αi are known as the Lagrange or dual variables with α =  α1, . . . , αm  cid:62 . Any equality constraint of the form g x  = 0 for a function g can be equivalently expressed by two inequalities: −g x  ≤ 0 and +g x  ≤ 0. Let α− ≥ 0 be the Lagrange variable associated to the ﬁrst constraint and α+ ≥ 0 the one associated to the second constraint. The sum of the terms corresponding to these constraints in the deﬁnition of the Lagrange function can therefore be written as αg x  with α =  α+ − α− . Thus, in general, for an equality constraint g x  = 0 the Lagrangian is augmented with a term αg x  but with α ∈ R not constrained to be non-negative. Note that in the case of a convex optimization problem, equality constraints g x  are required to be aﬃne since both g x  and −g x  are required to be convex. Deﬁnition B.23  Dual function  The  Lagrange  dual function associated to the constrained op- timization problem is deﬁned by  ∀α ≥ 0, F  α  = inf x∈X  L x, α  = inf x∈X   B.5    cid:0 f  x  +  m cid:88   i=1  αigi x  cid:1  .  Note that F is always concave, since the Lagrangian is linear with respect to α and since the inﬁmum preserves concavity. We further observe that  since for any feasible x, f  x  + cid:80 m  ∀α ≥ 0, F  α  ≤ p∗ ,   B.6  i=1 αigi x  ≤ f  x . The dual function naturally leads to the  following optimization problem. Deﬁnition B.24  Dual problem  The dual  optimization  problem associated to the constrained optimization problem is  max  F  α  subject to: α ≥ 0 .  α  The dual problem is always a convex optimization problem  as a maximization of a concave problem . Let d∗ denote an optimal value. By  B.6 , the following inequality always holds:   weak duality . The diﬀerence  p∗ − d∗  is known as the duality gap. The equality case  strong duality   d∗ = p∗  d∗ ≤ p∗  does not hold in general. However, strong duality does hold when convex problems satisfy a constraint qualiﬁcation. We will denote by int X  the interior of the set X.   B.3 Constrained optimization  421  Deﬁnition B.25  Strong constraint qualiﬁcation  Assume that int X   cid:54 = ∅. Then, the strong constraint qualiﬁcation or Slater’s condition is deﬁned as  ∃ x ∈ int X  : g x  < 0.   B.7  A function h : X → R is said to be aﬃne if it can be deﬁned for all x ∈ X by h x  = w · x + b, for some w ∈ RN and b ∈ R. Deﬁnition B.26  Weak constraint qualiﬁcation  Assume that int X   cid:54 = ∅. Then, the weak con- straint qualiﬁcation or weak Slater’s condition is deﬁned as  ∃ x ∈ int X  : ∀i ∈ [m], cid:0 gi x  < 0 cid:1  ∨ cid:0 gi x  = 0 ∧ gi aﬃne cid:1 .   B.8   We next present suﬃcient and necessary conditions for solutions to constrained optimization  problems, based on the saddle point of the Lagrangian and Slater’s condition. Theorem B.27  Saddle point — sufﬁcient condition  Let P be a constrained optimization prob- lem over X = RN . If  x∗, α∗  is a saddle point of the associated Lagrangian, that is,  ∀x ∈ RN , ∀α ≥ 0, L x∗, α  ≤ L x∗, α∗  ≤ L x, α∗ ,   B.9   then x∗ is a solution of the problem P . Proof: By the ﬁrst inequality, the following holds:  ∀α ≥ 0, L x∗, α  ≤ L x∗, α∗  ⇒ ∀α ≥ 0, α · g x∗  ≤ α∗ · g x∗    B.10  where g x∗  ≤ 0 in  B.10  follows by letting α → +∞ and α∗·g x∗  = 0 follows by letting α → 0. In view of  B.10 , the second inequality in  B.9  gives,  ⇒ g x∗  ≤ 0 ∧ α∗ · g x∗  = 0 ,  Thus, for all x satisfying the constraints, that is g x  ≤ 0, we have  ∀x, L x∗, α∗  ≤ L x, α∗  ⇒ ∀x, f  x∗  ≤ f  x  + α∗ · g x .  f  x∗  ≤ f  x ,  which completes the proof. Theorem B.28  Saddle point — necessary condition  Assume that f and gi, i ∈ [m], are con- vex functions and that Slater’s condition holds. Then, if x is a solution of the constrained opti- mization problem, there exists α ≥ 0 such that  x, α  is a saddle point of the Lagrangian. Theorem B.29  Saddle point — necessary condition  Assume that f and gi, i ∈ [m], are con- vex diﬀerentiable functions and that the weak Slater’s condition holds. If x is a solution of the constrained optimization problem, then there exists α ≥ 0 such that  x, α  is a saddle point of the Lagrangian.   cid:3   We conclude with a theorem providing necessary and suﬃcient optimality conditions when the  problem is convex, the objective function diﬀerentiable, and the constraints qualiﬁed. Theorem B.30  Karush-Kuhn-Tucker’s theorem  Assume that f, gi : X → R, ∀i ∈ [m] are con- vex and diﬀerentiable and that the constraints are qualiﬁed. Then x is a solution of the constrained program if and if only there exists α ≥ 0 such that,  ∇xL x, α  = ∇xf  x  + α · ∇xg x  = 0 ∇αL x, α  = g x  ≤ 0 α · g x  =  m cid:88   αigi x  = 0 .  i=1  The conditions B.11–B.13 are known as the KKT conditions. Note that the last two KKT con- ditions are equivalent to  These equalities are known as complementarity conditions.  g x  ≤ 0 ∧  ∀i ∈ {1, . . . , m}, ¯αigi x  = 0 .   B.11    B.12    B.13    B.14    Appendix B Convex Optimization  422  write  Proof: For the forward direction, since the constraints are qualiﬁed, if x is a solution, then there exists α such that the  x, α  is a saddle point of the Lagrangian and all three conditions are satisﬁed  the ﬁrst condition follows by deﬁnition of a saddle point, and the second two conditions follow from  B.10  . In the opposite direction, if the conditions are met, then for any x such that g x  ≤ 0, we can  f  x  − f  x  ≥ ∇xf  x  ·  x − x   i=1  = − m cid:88  ≥ − m cid:88  = − m cid:88   i=1  i=1  αi∇xgi x  ·  x − x   αi[gi x  − gi x ]  αigi x  ≥ 0,   convexity of f     ﬁrst condition    convexity of gis    third condition   which shows that f  x  is the minimum of f over the set of points satisfying the constraints.  cid:3   B.4 Fenchel duality  In this section, we present an alternative theory of convex optimization or convex analysis where the functions f considered may be non-diﬀerentiable and take inﬁnite values. Throughout, this section, the set X denotes a Hilbert space with the inner product denoted by  cid:104 ·, · cid:105 . However, the results presented can be straightforwardly extended to the case of a Banach space. We consider functions taking values in [−∞, +∞]. The domain of a function f : X → [−∞, +∞] is deﬁned as the set   B.15  We extend the deﬁnition of convexity and say that f : X → [−∞, +∞] is convex if it is convex over dom f  , that is if for all x, x cid:48  ∈ dom f   and all t ∈ [0, 1],  dom f   = {x ∈ X : f  x  < +∞}.   B.16  for all  u, v  ∈ R2 with u ≥ f  x  and v ≥ f  x cid:48  . A convex function is said to be proper if it takes values in  −∞, +∞] and if it is not uniformly equal to +∞. It is said to be closed when its epigraph is closed.  f  tx +  1 − t x cid:48   ≤ tu +  1 − t v,  B.4.1 Subgradients Deﬁnition B.31 Let f : X →  −∞, +∞] be a convex function. Then, a vector g ∈ X is a subgra- dient of f at a point x ∈ dom f   if the following inequality holds for all z ∈ X:  f  z  ≥ f  x  +  cid:104 z − x, g cid:105 .   B.17   The set of all subgradients at x is called the subdiﬀerential of f at x and is denoted by ∂f  x  with ∂f  x  = ∅ for x  cid:54 ∈ dom f  .  Thus, g is a subgradient at x iﬀ the hyperplane with normal vector g passing through the point  x, f  x   is below the graph of f , that is iﬀ it is supporting the graph of f . Figure 14.1 illustrates these deﬁnitions. The following lemma shows that if f is diﬀerentiable at x ∈ dom f  , then its subdiﬀerential is  reduced to its gradient at x. Lemma B.32 If f is diﬀerentiable at x ∈ dom f  , then ∂f  x  = {∇f  x }.   B.4 Fenchel duality  423   cid:3   Proof: Clearly, the gradient ∇f  x  is always a subgradient at x. Now, let g be in ∂f  x . Then, by deﬁnition of a subgradient, for any  cid:15  ∈ R,  f  x +  cid:15  ∇f  x  − g   ≥ f  x  +  cid:15  cid:104 ∇f  x  − g, g cid:105 .  A ﬁrst-order Taylor series expansion gives  f  x +  cid:15  ∇f  x  − g   − f  x  =  cid:15  cid:104 ∇f  x , ∇f  x  − g cid:105  + o  cid:15  cid:107 ∇f  x  − g cid:107  .  In view of that, the ﬁrst inequality can be rewritten as  which implies  cid:107 ∇f  x  − g cid:107  = o 1  and ∇f  x  = g.   cid:15  cid:107 ∇f  x  − g cid:107 2 ≤ o  cid:15  cid:107 ∇f  x  − g cid:107  ,  Proposition B.33 Let f : X →  −∞, +∞] be a proper function. Then, x∗ is a global minimizer of f iﬀ ∂f  x∗  contains 0.  Proof: Since f is proper, if x∗ is a minimizer, f  x∗  cannot be +∞. Thus x∗ must be in dom f    and thus ∂f  x  is not deﬁned to be empty . Now, x∗ is a global minimizer iﬀ for all z ∈ X, f  z  ≥ f  x∗ , that is iﬀ 0 is a subgradient of f at x∗.  cid:3   B.4.2 Core The core of a set C ⊆ X is denoted by core C  and deﬁned as follows:  core C  = {x ∈ C : ∀u ∈ X, ∃ cid:15  > 0  ∀t ∈ [0,  cid:15 ],  x + tu  ∈ C}.   B.18  Thus, for x ∈ core C , for any direction u,  x + tu  is in C for t suﬃciently small. In view of this deﬁnition, core C  clearly contains the interior of C, int C . Proposition B.34 Let h : X → [−∞, +∞] be a convex function. If there exists x0 ∈ core dom h   such that h x0  > −∞, then h x  > −∞ for all x ∈ X. Proof: Let x be in X. Since x0 is in core dom h  , there exists t > 0 such that x cid:48  is in dom h , that is such that h x cid:48  holds: h x cid:48    cid:104  0  < +∞. Since x0 = 1 1+t x cid:48  v ⇐⇒ v ≥ 1 + t  B.19  t 0 ] > −∞, which concludes the proof.  cid:3  [h x0  − 1  0 = x0 + t x0− x   cid:105  1+t x, by convexity, the following  t 1 + t for all v ≥ h x . This implies h x  ≥ 1+t  0 + t h x0  − 1 1 + t  h x0  ≤ 1 1 + t  h x cid:48  0   1+t h x cid:48   0  +  t  The proof of the following result is left as an exercise  Exercise B.3 .  Proposition B.35 Let h : X →  −∞, +∞] be a convex function. Then, h admits a subdiﬀerential at any x ∈ core dom h  .  B.4.3 Conjugate functions Deﬁnition B.36  Conjugate functions  Let X be a Hilbert space. The conjugate function or Fenchel conjugate of a function f : X  cid:55 → [−∞, +∞] is the function f∗ : X  cid:55 → [−∞, +∞] deﬁned by  B.20    cid:8  cid:104 u, x cid:105  − f  x  cid:9 .  f∗ u  = sup x∈X  Note that f∗ is convex as the pointwise supremum of the set of aﬃne and thus convex functions u  cid:55 →  cid:104 x, u cid:105  − f  x . Also, if there exists x such that f  x    −∞. Conjugation is order-reversing: for any f and g, if f ≤ g, then g∗ ≤ f∗. Also, it is straightforward to see that if f is closed proper convex, then f∗∗ = f .  Figure B.3 illustrates the deﬁnition of conjugate functions. As shown by the ﬁgure, conjugate functions correspond to a dual description of the epigraph of a function in terms of supporting hyperplanes and their crossing points.   424  Appendix B Convex Optimization  Figure B.3 Illustration of the conjugate f∗ of a function f . Given y, x∗ is the point at which the distance between the hyperplane of equation z =  cid:104 x, y cid:105  with normal y  slope y in dimension one  and the plot of f  x  is the largest. This largest distance is equal to f∗ y . The parallel hyperplane z =  cid:104 x − x∗, y cid:105  + f  x∗  with normal y and passing through the point  x∗, f  x∗  is shown. This is a supporting hyperplane of the plot of f  x . The point at which it intercepts the y-axis  crossing point  has y-coordinate −f∗ y .  Lemma B.37  Conjugate of extended relative entropy  Let p0 ∈ ∆ be a distribution over X such that p0 x  > 0 for all x ∈ X. Deﬁne f : RX → R by   cid:40   D p cid:107 p0  +∞  if p ∈ ∆ otherwise.  Then, the conjugate function f∗ : RX → R of f is deﬁned by  f  p  =  ∀q ∈ RX, f∗ q  = log   cid:18  cid:88   cid:0  cid:104 p, q cid:105  − D p cid:107 p0  cid:1  = sup  x∈X Proof: By deﬁnition of f , for any q ∈ RX, we can write  p∈∆ Fix q ∈ RX and let ¯q ∈ ∆ be deﬁned for all x ∈ X by  sup p∈RX  p0 x eq x   .   cid:0  cid:104 p, q cid:105  − D p cid:107 p0  cid:1 .   cid:19    cid:80   ¯q x  =  p0 x eq x  x∈X p0 x eq x   =  p0 x eq x  Ep0 [eq]  .  Then, the following holds for all p ∈ ∆: [log eq ] − E   cid:105  p0eq Since D p cid:107 ¯q  ≥ 0 and D p cid:107 ¯q  = 0 for p = ¯q, this shows that supp∈∆ and concludes the proof.   cid:104 p, q cid:105  − D p cid:107 p0  = E  = E  p p0   cid:104    cid:105    cid:104   log  log  p  p  p  p   B.21    B.22   = −D p cid:107 ¯q  + log E p0   cid:0 p·q−D p cid:107 p0  cid:1  = log cid:0  Ep0 [eq] cid:1   [eq].   cid:3   Table B.1 gives a series of other examples of functions and their conjugates. The following is  an immediate consequence of the deﬁnition of the conjugate functions.   B.4 Fenchel duality  425  Table B.1 Examples of functions g and their conjugates g∗.  g x   dom g   f  ax   a  cid:54 = 0   f  x + b   af  x   a > 0   αx + β  xp p −xp p   p > 1    0 < p < 1  √  1 + x2 − log x   ex  log  1 + ex   − log  1 − ex   X  X  X  R  R  R  R  R  R+ R  R+ \ {0}   cid:40   g∗ y   f∗ cid:0  y  cid:1   cid:1  af∗ cid:0  y  a  a  f∗ y  −  cid:104 b, y cid:105    cid:40 −β  if y = α +∞ otherwise yq q  q = 1     1 p + 1   1 p + 1  − cid:112 1 −  y 2  q  q = 1   − −y q   cid:40   − 1 + log −y    y log y  − y , 0 ,  if y > 0 if y = 0   cid:40   y log y  +  1 − y  log 1 − y  , 0 ,  y log y  −  1 + y  log 1 + y  , 0 ,  if 0 < y < 1 if y = 0, 1  if y > 0 if y = 0  dom g∗   X∗ X∗ X∗  R  R  R− [−1, 1] R− \ {0}  R+  [0, 1]  R+  Proposition B.38  Fenchel’s inequality  Let X be a Hilbert space. For any function f : X  cid:55 → [−∞, +∞] and any x ∈ dom f   and u ∈ X, the following inequality holds:  f  x  + f∗ u  ≥  cid:104 u, x cid:105  .   B.23   Equality holds iﬀ u is a subgradient of f at x. We will denote by A∗ the adjoint operator of a bounded  or continuous  linear map A : X → Y. Also, we denote by cont f   the set of points x ∈ X at which f : X → [−∞, +∞] is ﬁnite and continuous. Theorem B.39  Fenchel duality theorem  Let X and Y be two Hilbert spaces, f : X →  −∞, +∞] and g : Y →  −∞, +∞] two convex functions and A : X → Y a bounded linear map. Then, the following two optimization problems   Fenchel problems  {f  x  + g Ax } {−f∗ A∗y  − g∗ −y }  p∗ = inf x∈X d∗ = sup y∈Y  satisfy the weak duality p∗ ≥ d∗. If further f and g satisfy the condition  0 ∈ core cid:0  dom g  − A dom f    cid:1 ,  or the stronger condition then strong duality holds, that is p∗ = d∗ and the supremum in the dual problem is attained if d∗ ∈ R.  A dom f    ∩ cont g   cid:54 = ∅,   426  Appendix B Convex Optimization  Proof: By Fenchel’s inequality  proposition B.38  applied to both f and g, for any x ∈ X and y ∈ Y, the following inequalities hold:  f  x  + f∗ A∗y  ≥  cid:104 A∗y, x cid:105  =  cid:104 y, Ax cid:105  = −  cid:104 −y, Ax cid:105  ≥ −g Ax  − g∗ −y .  Comparing the leftmost and the rightmost terms gives  f  x  + f∗ A∗y  ≥ −g Ax  − g∗ −y  ⇐⇒ f  x  + g Ax  ≥ −f∗ A∗y  − g∗ −y .  Taking the inﬁmum over x ∈ X of the left-hand side and the supremum over y ∈ Y of the right-hand side of the last inequality yields p∗ ≥ d∗.  Consider now the function h : Y → [−∞, +∞] deﬁned for all u ∈ Y by  {f  x  + g Ax + u }.  h u  = inf x∈X   B.24  Since  x, u   cid:55 → f  x  + g Ax + u  is convex, h is convex as the inﬁmum over one argument of that function. u is in dom h  iﬀ there exists x ∈ X such that f  x  + g Ax + u  < +∞, that is iﬀ there exists x ∈ X such that f  x  < +∞ and g Ax + u  < +∞, that is iﬀ there exists x ∈ dom f   such that  Ax + u  ∈ dom g . Thus, we have dom h  = dom g  − A dom f  .  If p∗ = −∞, then strong duality clearly holds. Otherwise, p∗ > −∞. If 0 ∈ core cid:0  dom g  − A dom f    cid:1  = core dom h  , then 0 is in dom h  and p∗ < +∞. Thus, p∗ = h 0  is in R. By  proposition B.34, since h 0  > −∞ and 0 ∈ core dom h  , h takes values in  −∞, +∞]. Thus, by proposition B.35, h admits a subgradient −y at 0. By deﬁnition of y, for all x ∈ X and u ∈ Y,  h 0  ≤ h u  +  cid:104 y, u cid:105   ≤ f  x  + g Ax + u  +  cid:104 y, u cid:105  = {f  x  −  cid:104 A∗y, x cid:105 } + {g Ax + u  +  cid:104 y, u cid:105  +  cid:104 A∗y, x cid:105 } = {f  x  −  cid:104 A∗y, x cid:105 } + {g Ax + u  +  cid:104 y, Ax + u cid:105 }.  Taking the inﬁmum over u and the supremum over x yields  h 0  ≤ −f∗ A∗y  − g∗ −y  ≤ d∗ ≤ p∗ = h 0 ,  which proves d∗ = p∗ and that the supremum deﬁning d∗ is reached at y. Finally, assume that A dom f    ∩ cont g   cid:54 = ∅ and let u ∈ A dom f    ∩ cont g . Then, u = Ax with x ∈ dom f   and u ∈ cont g  ⊆ dom g . Thus, we have 0 = u − Ax ∈ dom g  − A dom f  . Since g is continuous at u and g u  is ﬁnite, for any v ∈ X, there exists  cid:15  > 0 such that g u + tv  is ﬁnite for all t ∈ [0,  cid:15 ], thus wt =  u + tv  ∈ dom g . Therefore, for any t ∈ [0,  cid:15 ], tv = wt − u =  cid:3   wt − Ax ∈ dom g  − A dom f  , which shows that 0 ∈ core cid:0  dom g  − A dom f    cid:1 .   cid:8 f  x  + g x  cid:9 . Figure B.4 illustrates the Fenchel duality  To illustrate the theorem, consider the case where A is the identity operator. The primal optimization problem is then minx theorem in that case. The primal problem consists of ﬁnding the point x∗ at which the distance between the plots of f  x  and −g x  is minimal since f  x  + g x  = f  x  −  −g x  . As shown by the ﬁgure, under the conditions of the theorem, this coincides with seeking y∗ at which the diﬀerence of the conjugate values of f  x  and −g x , that is the diﬀerence −f∗ y  − g∗ −y  is maximal.  B.5 Chapter notes  The results presented in this appendix are based on several key theorems: theorem B.3 due to Fer- mat  1629 ; theorem B.27 due to Lagrange  1797 , theorem B.30 due to Karush [1939] and Kuhn and Tucker [1951], and theorem B.39 due to Werner Fenchel, based on the notion of conjugate functions or Legendre transformations. For a more extensive material on convex optimization, we strongly recommend the books of Boyd and Vandenberghe [2004], Bertsekas, Nedi´c, and Ozdaglar [2003], Rockafellar [1997], Borwein and Lewis [2000] and Borwein and Zhu [2005] which have formed the basis for part of the material presented in this appendix. In particular, our table of conjugate functions is extracted from [Borwein and Lewis, 2000].   B.6 Exercises  427  Figure B.4 Illustration of Fenchel Duality; minx   cid:8 f  x  + g x  cid:9  = maxy   cid:8  − f∗ y  − g∗ −y  cid:9 .  B.6 Exercises  B.1 Give the conjugate of the function f deﬁned by f  x  = x for all x ∈ R.  B.2 Prove the correctness of the conjugate function g∗ for each function g of Table B.1.  B.3 Give the proof of Proposition B.35.  f  x  −g x   −f∗ y   −f∗ y −g∗ −y   g∗ −y   −f∗ y∗ −g∗ −y∗   0, 0   normal y∗  normal y  x∗  x    C Probability Review  In this appendix, we give a brief review of some basic notions of probability and will also deﬁne the notation that is used throughout the textbook.  C.1 Probability  A probability space is a tuple consisting of three components: a sample space, an events set, and a probability distribution:    sample space Ω: Ω is the set of all elementary events or outcomes possible in a trial, for example,  each of the six outcomes in {1, . . . , 6} when casting a die.   events set F : F is a σ-algebra, that is a set of subsets of Ω containing Ω that is closed under  complementation and countable union  therefore also countable intersection . An example of an event may be “the die lands on an odd number”.    probability distribution: P is a mapping from the set of all events F to [0, 1] such that P[Ω] = 1, P[∅] = 1, and, for all mutually exclusive events A1, . . . , An,  P[A1 ∪ . . . ∪ An] =  P[Ai].  n cid:88   i=1  The discrete probability distribution associated with a fair die can be deﬁned by P[Ai] = 1 6 for i ∈ {1 . . . 6}, where Ai is the event that the die lands on value i.  C.2 Random variables Deﬁnition C.1  Random variables  A random variable X is a function X : Ω → R that is mea- surable, that is such that for any interval I, the subset of the sample space {ω ∈ Ω : X ω  ∈ I} is an event. The probability mass function of a discrete random variable X is deﬁned as the function x  cid:55 → P[X = x]. The joint probability mass function of discrete random variables X and Y is deﬁned as the function  x, y   cid:55 → P[X = x ∧ Y = y].  A probability distribution is said to be absolutely continuous when it admits a probability density function, that is a function f associated to a real-valued random variable X that satisﬁes for all a, b ∈ R  P[a ≤ X ≤ b] =  f  x dx .   C.1    cid:90  b  a   430  Appendix C Probability Review  Figure C.1 Approximation of the binomial distribution  in red  by a normal distribution  in blue .  Deﬁnition C.2  Binomial distribution  A random variable X is said to follow a binomial distri- bution B n, p  with n ∈ N and p ∈ [0, 1] if for any k ∈ {0, 1, . . . , n},  Deﬁnition C.3  Normal distribution  A random variable X is said to follow a normal  or Gaus- sian  distribution N  µ, σ2  with µ ∈ R and σ > 0 if its probability density function is given by,   cid:16 n  cid:17   k  P[X = k] =  pk 1 − p n−k .   cid:18  −  x − µ 2   cid:19   .  2σ2  f  x  =  1√  2πσ2  exp  The standard normal distribution N  0, 1  is the normal distribution with zero mean and unit variance.  The normal distribution is often used to approximate a binomial distribution. Figure C.1 illus- trates that approximation. Deﬁnition C.4  Laplace distribution  A random variable X is said to follow a Laplace distri- bution with location parameter µ ∈ R and scale parameter b > 0 if its probability density function is given by,   cid:18  − x − µ   cid:19   .  b  f  x  =  exp  1 2b  Deﬁnition C.5  Gibbs distributions  Given a set X and feature function Φ : X → RN , a random variable X is said to follow a Gibbs distribution with parameter w ∈ RN if for any x ∈ X,  The normalizing quantity in the denominator Z = cid:80   P[X = x] =   cid:80   exp w · Φ x   x∈X exp w · Φ x    .  x∈X exp w· Φ x   is also called the partition  function. Deﬁnition C.6  Poisson distribution  A random variable X is said to follow a Poisson distri- bution with λ > 0 if for any k ∈ N,  P[X = k] =  λke−λ  .  k!  The deﬁnition of the following family of distributions uses the notion of independence of random variables deﬁned in the next section. Deﬁnition C.7  χ2-squared distribution  The χ2-distribution  or chi-squared distribution  with k degrees of freedom is the distribution of the sum of the squares of k independent random variables, each following a standard normal distribution.  0.15  0.1  0.05  0  0  10  20  30   C.3 Conditional probability and independence  431   C.2    C.3   C.3 Conditional probability and independence Deﬁnition C.8  Conditional probability  The conditional probability of event A given event B is deﬁned by  when P[B]  cid:54 = 0.  P[A  B] =  P[A ∩ B]  P[B]  ,  Deﬁnition C.9  Independence  Two events A and B are said to be independent if  Equivalently, A and B are independent iﬀ P[A  B] = P[A] when P[B]  cid:54 = 0.  P[A ∩ B] = P[A] P[B].  A sequence of random variables is said to be independent and identically distributed  i.i.d.  when the random variables are mutually independent and follow the same distribution. The following are basic probability formulae related to the notion of conditional probability. They hold for any events A, B, and A1, . . . , An, with the additional constraint P[B]  cid:54 = 0 needed for the Bayes formula to be well deﬁned:  P[  i=1  n cid:91   P[A ∪ B] = P[A] + P[B] − P[A ∩ B]  P[Ai]  Ai] ≤ n cid:88  Ai] = P[A1] P[A2  A1] · · · P[An  n−1 cid:92   P[B  A] P[A]  P[B]  i=1  P[A  B] =  n cid:92   P[   sum rule    C.4    union bound    C.5    Bayes formula    C.6   i=1  Ai]   C.7  The sum rule follows immediately from the decomposition of A ∪ B as the union of the disjoint sets A and  B − A ∩ B . The union bound is a direct consequence of the sum rule. The Bayes formula follows immediately from the deﬁnition of conditional probability and the observation that: P[AB] P[B] = P[BA] P[A] = P[A∩ B]. Similarly, the chain rule follows the observation that P[A1] P[A2A1] = P[A1 ∩ A2]; using the same argument shows recursively that the product of the Finally, assume that Ω = A1∪ A2∪ . . .∪ An with Ai∩ Aj = ∅ for i  cid:54 = j, i.e., the Ais are mutually  ﬁrst k terms of the right-hand side equals P[ cid:84 k   chain rule .  i=1 Ai].  i=1  disjoint. Then, the following formula is valid for any event B:  P[B] =  P[B  Ai] P[Ai]   C.8  This follows the observation that P[B  Ai] P[Ai] = P[B ∩ Ai] by deﬁnition of the conditional probability and the fact that the events B ∩ Ai are mutually disjoint.   theorem of total probability .  i=1  n cid:88   C.4 Expectation and Markov’s inequality Deﬁnition C.10  Expectation  The expectation or mean of a random variable X is denoted by E[X] and deﬁned by  E[X] =  x P[X = x].   C.9    cid:88   x  When X follows a probability distribution D, we will also write Ex∼D[x] instead of E[X] to explic- itly indicate the distribution. A fundamental property of expectation, which is straightforward to verify using its deﬁnition, is that it is linear, that is, for any two random variables X and Y and any a, b ∈ R, the following holds:  E[aX + bY ] = a E[X] + b E[Y ].   C.10    432  Appendix C Probability Review  Furthermore, when X and Y are independent random variables, then the following identity holds:  E[XY ] = E[X] E[Y ].   C.11   Indeed, by deﬁnition of expectation and of independence, we can write  E[XY ] =  xy P[X = x ∧ Y = y] =  xy P[X = x] P[Y = y]   cid:88   cid:16  cid:88   x,y  x  =  x P[X = x]  y P[Y = y]  ,   cid:17  cid:16  cid:88   y   cid:17    cid:88   x,y  where in the last step we used Fubini’s theorem . The following provides a simple bound for a non-negative random variable in terms of its expectation, known as Markov’s inequality. Theorem C.11  Markov’s inequality  Let X be a non-negative random variable with E[X] < ∞. Then for all t > 0,  P cid:2 X ≥ t E[X] cid:3  ≤ 1  .  t  Proof: The proof steps are as follows:  P[X ≥ t E[X]] =  P[X = x]  x≥t E[X]   cid:88  ≤  cid:88  ≤ cid:88   cid:20  X  x≥t E[X]  x  = E  P[X = x]  x  t E[X]  P[X = x]   cid:21   x  t E[X] 1 t  t E[X]  =   cid:16    by deﬁnition   using  x  t E[X]  ≥ 1   cid:17    extending non-negative sum    linearity of expectation .  This concludes the proof.  C.5 Variance and Chebyshev’s inequality Deﬁnition C.12  Variance — Standard deviation  The variance of a random variable X is de- noted by Var[X] and deﬁned by  The standard deviation of a random variable X is denoted by σX and deﬁned by  For any random variable X and any a ∈ R, the following basic properties hold for the variance, which can be proven straightforwardly:  Var[X] = E[ X − E[X] 2].  σX = cid:112 Var[X].  Var[X] = E[X 2] − E[X]2 Var[aX] = a2 Var[X].   C.12    cid:3    C.13    C.14    C.15    C.16   Furthermore, when X and Y are independent, then   C.17  Indeed, using the linearity of expectation and the identity E[X] E[Y ] − E[XY ] = 0 which holds by the independence of X and Y , we can write  Var[X + Y ] = Var[X] + Var[Y ].  Var[X + Y ] = E[ X + Y  2] − E[X + Y ]2  = E[X 2 + Y 2 + 2XY ] −  E[X]2 + E[Y ]2 + 2 E[XY ]  =  E[X 2] − E[X]2  +  E[Y 2] − E[Y ]2  + 2 E[X] E[Y ] − E[XY ]  = Var[X] + Var[Y ].   C.5 Variance and Chebyshev’s inequality  433   C.18    C.19    cid:3   The following inequality known as Chebyshev’s inequality bounds the deviation of a random  variable from its expectation in terms of its standard deviation. Theorem C.13  Chebyshev’s inequality  Let X be a random variable with Var[X] < +∞. Then, for all t > 0, the following inequality holds:  P cid:2 X − E[X] ≥ tσX   cid:3  ≤ 1  .  P cid:2 X − E[X] ≥ tσX  t2   cid:3  = P[ X − E[X] 2 ≥ t2σ2  Proof: Observe that:  X ]. The result follows by application of Markov’s inequality to  X − E[X] 2. We will use Chebyshev’s inequality to prove the following theorem. Theorem C.14  Weak law of large numbers  Let  Xn n∈N be a sequence of independent ran- dom variables with the same mean µ and variance σ2 < ∞. Let X n = 1 i=1 Xi, then, for any  cid:15  > 0,   cid:80 n   cid:3   n  Proof: Since the variables are independent, we can write  P[X n − µ ≥  cid:15 ] = 0.  lim n→∞  n cid:88   i=1   cid:20  Xi   cid:21   n  Var[X n] =  Var  =  nσ2 n2  =  σ2 n  .  P[X n − µ ≥  cid:15 ] ≤ σ2 n cid:15 2  ,  Thus, by Chebyshev’s inequality  with t =  cid:15   Var[X n] 1 2 , the following holds:  which implies  C.19 .  Example C.15  Applying Chebyshev’s inequality  Suppose we roll a pair of fair dice n times. Can we give a good estimate of the total value of the n rolls? If we compute the mean and variance, we ﬁnd µ = 7n and σ2 = 35 6n  we leave it to the reader to verify these expressions . Thus, applying Chebyshev’s inequality, we see that the ﬁnal sum will lie within 7n ± 10 6 n in at least 99 percent of all experiments. Therefore, the odds are better than 99 to 1 that the sum will be between 6.975M and 7.025M after 1M rolls.   cid:113  35  Deﬁnition C.16  Covariance  The covariance of two random variables X and Y is denoted by Cov X, Y   and deﬁned by  Cov X, Y   = E cid:2  X − E[X]  Y − E[Y ]  cid:3 .   C.20   Two random variables X and Y are said to be uncorrelated when Cov X, Y   = 0. It is straight- forward to see that if two random variables X and Y are independent then they are uncorrelated, but the converse does not hold in general. The covariance deﬁnes a positive semideﬁnite and symmetric bilinear form:    symmetry: Cov X, Y   = Cov Y, X  for any two random variables X and Y ;   bilinearity: Cov X + X cid:48 , Y   = Cov X, Y   + Cov X cid:48 , Y   and Cov aX, Y   = a Cov X, Y   for any random variables X, X cid:48 , and Y and a ∈ R;   positive semideﬁniteness: Cov X, X  = Var[X] ≥ 0 for any random variable X. The following Cauchy-Schwarz inequality holds for random variables X and Y with Var[X] < +∞ and Var[Y ] < +∞:   Cov X, Y   ≤ cid:112 Var[X] Var[Y ].   C.21   The following deﬁnition   434  Appendix C Probability Review  Deﬁnition C.17 The covariance matrix of a vector of random variables X =  X1, . . . , XN   is the matrix in RN×N denoted by C X  and deﬁned by  C X  = E cid:2  X − E[X]  X − E[X]  cid:62  cid:3 .   C.22   Thus, C X  =  Cov Xi, Xj   ij . It is straightforward to show that C X  = E[XX cid:62 ] − E[X] E[X] cid:62 .   C.23   We close this appendix with the following well-known theorem of probability. Theorem C.18  Central limit theorem  Let X1, . . . , Xn be a sequence of i.i.d. random variables n = σ2 n. Then,  X n−µ  σn with mean µ and standard deviation σ. Let X n = 1 converges to the N  0, 1  in distribution, that is for any t ∈ R, n 1√ 2π  P[ X n − µ  σn ≤ t] =  i=1 Xi and σ2   cid:80 n  cid:90  t  e− x2  lim n→∞  2 dx .  −∞  C.6 Moment-generating functions  The expectation E[X p] is called the pth-moment of the random variable X. The moment- generating function of a random variable X is a key function from which its diﬀerent moments can be straightforwardly computed via diﬀerentiation at zero. It can therefore be crucial for specifying the distribution of X or analyzing its properties. Deﬁnition C.19  Moment-generating function  The moment-generating function of a random variable X is the function MX : t  cid:55 → E[etX ] deﬁned over the set of t ∈ R for which the expectation is ﬁnite. If MX is diﬀerentiable at zero, then the pth-moment of X is given by E[X p] = M  p  X  0 . We will present in the next chapter a general bound on the moment-generating function of a zero-mean bounded random variable  Lemma D.1 . Here, we illustrate its computation in two special cases. Example C.20  Standard normal distribution  Let X be a random variable following a normal distribution with mean 0 and variance 1. Then, MX is deﬁned for all t ∈ R by  MX  t  =  e− x2  2 etxdx = e  t2 2  1√ 2π  1√ 2π  e− 1  2  x−t 2  dx = e  t2 2 ,   C.24   by recognizing that the last integrand is the probability density function of a normal distribution with mean t and variance 1.   cid:90  ∞  −∞   cid:90  ∞  −∞  Example C.21  χ2-distribution  Let X be a random variable following a χ2-squared distribution i where the Xis are independent and follow  i=1 X 2  Let t < 1 2. By the i.i.d. assumption about the variables Xi, we can write  with k degrees of freedom. We can write X = cid:80 k  cid:105  k cid:89   a standard normal distribution.  etX2  i  =  By deﬁnition of the standard normal distribution, we have 1√ 2π  E[etX2  e −x2  2 dx =  etx2  1 ] =  E[etX ] = E cid:104  k cid:89   cid:90  +∞  cid:90  +∞  1√ 2π  −∞  i=1  =  1√ 2π  −∞  E cid:2 etX2  i cid:3  = E cid:2 etX2 1 cid:3 k.  cid:90  +∞  i=1  e 1−2t  −x2  2 dx  e −u2 2√ 1 − 2t √  −∞ du =  1 − 2t − 1 2 , 1 − 2t x.  where we used the change of variable u = function of the χ2-distribution is given by  In view of that, the moment-generating  ∀t < 1 2, MX  t  = E[etX ] =  1 − 2t − k 2 .   C.25    C.7 Exercises  C.7 Exercises  435  C.1 Let f :  0, +∞  → R+ be a function admitting an inverse f−1 and let X be a random variable. Show that if for any t > 0, P[X > t] ≤ f  t , then, for any δ > 0, with probability at least 1 − δ, X ≤ f−1 δ .  C.2 Let X be a discrete random variable taking non-negative integer values. Show that E[X] =  P[X ≥ n]  Hint: rewrite P[X = n] as P[X ≥ n] − P[X ≥ n + 1] .   cid:80   n≥1    D Concentration Inequalities  In this appendix, we present several concentration inequalities used in the proofs given in this book. Concentration inequalities give probability bounds for a random variable to be concentrated around its mean, or for it to deviate from its mean or some other value.  D.1 Hoeffding’s inequality  We ﬁrst present Hoeﬀding’s inequality, whose proof makes use of the general Chernoﬀ bounding technique. Given a random variable X and  cid:15  > 0, this technique consists of proceeding as follows to bound P[X ≥  cid:15 ]. For any t > 0, ﬁrst Markov’s inequality is used to bound P[X ≥  cid:15 ]:  P[X ≥  cid:15 ] = P[etX ≥ et cid:15 ] ≤ e−t cid:15  E[etX ] .   D.1  Then, an upper bound g t  is found for E[etX ] and t is selected to minimize e−t cid:15 g t . For Hoeﬀd- ing’s inequality, the following lemma provides an upper bound on E[etX ]. Lemma D.1  Hoeffding’s lemma  Let X be a random variable with E[X] = 0 and a ≤ X ≤ b with b > a. Then, for any t > 0, the following inequality holds:  Proof: By the convexity of x  cid:55 → ex, for all x ∈ [a, b], the following holds:   D.2   Thus, using E[X] = 0,  E[etX ] ≤ E  where,   cid:20  b − X  cid:18  b  b − a  b − a For any t > 0, the ﬁrst and second derivative of φ are given below:  = ta + log  φ t  = log  b − a  +  eta +  E[etX ] ≤ e  t2 b−a 2  8  .  eta +  etb .  x − a b − a   cid:21   etb  =   cid:19   b  b − a  eta +   cid:18  b  etx ≤ b − x b − a X − a b − a  eta +  etb  −a b − a aet b−a   −a b − a  etb = eφ t  ,  −a b − a  et b−a   .   cid:19   = a −  a  b  b−a e−t b−a  − a b−a  ,  φ cid:48  t  = a −  φ cid:48  cid:48  t  =  b  b−a et b−a   b−a − a −abe−t b−a  b−a e−t b−a  − a [ b α 1 − α e−t b−a  b − a 2 [ 1 − α e−t b−a  + α]2  b−a ]2  =  =  α   1 − α e−t b−a   [ 1 − α e−t b−a  + α]  [ 1 − α e−t b−a  + α]   b − a 2 .   438  Appendix D Concentration Inequalities  where α denotes −a u = Thus, by the second order expansion of function φ, there exists θ ∈ [0, t] such that:  b−a . Note that φ 0  = φ cid:48  0  = 0 and that φ cid:48  cid:48  t  = u 1 − u  b − a 2 where . Since u is in [0, 1], u 1− u  is upper bounded by 1 4 and φ cid:48  cid:48  t  ≤  b−a 2  [ 1−α e−t b−a +α]  α  4  .  which completes the proof.  φ t  = φ 0  + tφ cid:48  0  +  φ cid:48  cid:48  θ  ≤ t2  b − a 2  ,  8  t2 2  The lemma can be used to prove the following result known as Hoeﬀding’s inequality. Theorem D.2  Hoeffding’s inequality  Let X1, . . . , Xm be independent random variables with Xi taking values in [ai, bi] for all i ∈ [m]. Then, for any  cid:15  > 0, the following inequalities hold for  Sm = cid:80 m  i=1 Xi:  P[Sm − E[Sm] ≥  cid:15 ] ≤ e−2 cid:15 2  cid:80 m P[Sm − E[Sm] ≤ − cid:15 ] ≤ e−2 cid:15 2  cid:80 m  i=1 bi−ai 2 i=1 bi−ai 2  .  Proof: Using the Chernoﬀ bounding technique and lemma D.1, we can write:  P[Sm − E[Sm] ≥  cid:15 ] ≤ e−t cid:15  E[et Sm−E[Sm] ]   D.3   cid:3    D.4    D.5   where we chose t = 4 cid:15   cid:80 m  i=1   independence of Xis   E[et Xi−E[Xi] ]  = e−t cid:15 Πm ≤ e−t cid:15 Πm = e−t cid:15 et2 cid:80 m ≤ e−2 cid:15 2  cid:80 m i=1 bi − ai 2 to minimize the upper bound. This proves the ﬁrst  cid:3   i=1et2 bi−ai 2 8 i=1 bi−ai 2 8 i=1 bi−ai 2   lemma D.1   ,  statement of the theorem, and the second statement is shown in a similar way. When the variance σ2 s are relatively small, better concentration bounds can be derived  see Bennett’s and Bernstein’s inequalities proven in exercise D.6 .  of each random variable Xi is known and the σ2 Xi  Xi  D.2 Sanov’s theorem  Here, we present a ﬁner upper bound than Hoeﬀding’s inequality expressed in terms of the binary relative entropy. Theorem D.3  Sanov’s theorem  Let X1, . . . , Xm be independent random variables drawn ac- cording to some distribution D with mean p and support included in [0, 1]. Then, for any q ∈ [0, 1],  the following inequality holds for  cid:98 p = 1 p +  1 − q  log 1−q  where D q cid:107 p  = q log q  m   cid:80 m P[ cid:98 p ≥ q] ≤ e−mD q cid:107 p  ,  i=1 Xi:  1−p is the binary relative entropy of p and q.   D.3 Multiplicative Chernoff bounds  439  Proof: For any t > 0, by convexity of the function x  cid:55 → etx, the following inequality holds for all x ∈ [0, 1]: etx = et[ 1−x ·0+x·1] ≤ 1 − x + etx. In view of that, for any t > 0, we can write  P[ cid:98 p ≥ q] = P[etm cid:98 p ≥ etmq] = P[etm cid:98 p ≥ etmq] ≤ e−tmq E[etm cid:98 p] = e−tmq E[et cid:80 m = e−tmq  E[etXi ]  i=1 Xi ]  m cid:89  m cid:89   i=1  E[1 − Xi + etXi]  ≤ e−tmq = [e−tq 1 − p + etp ]m.  i=1   by Markov’s inequality    ∀x ∈ [0, 1], etx ≤ 1 − x + etx   p 1−q  . Plugging in this value of t in the inequality above yields P[ cid:98 p ≥ q] ≤ e−mD q cid:107 p .  cid:3   Now, the function f : t  cid:55 → e−tq 1 − p + etp  =  1 − p e−tq + pet 1−q  reaches its minimum at t = log q 1−p  Note that for any  cid:15  > 0,  cid:15  ≤ 1 − p, with the choice q = p +  cid:15 , the theorem implies   D.6  This is a ﬁner bound than Hoeﬀding’s inequality  Theorem D.2  since, by Pinsker’s inequality  Proposition E.7 , D p +  cid:15  cid:107 p  ≥ 1 2  2 cid:15  2 = 2 cid:15 2. Similarly, we can derive a symmetric bound by applying the theorem to the random variables Yi = 1 − Xi. Then, for any  cid:15  > 0,  cid:15  ≤ p, with the choice q = p −  cid:15 , the theorem implies  P[ cid:98 p ≥ p +  cid:15 ] ≤ e−mD p+ cid:15  cid:107 p  .  P[ cid:98 p ≤ p −  cid:15 ] ≤ e−mD p− cid:15  cid:107 p  .   D.7   D.3 Multiplicative Chernoff bounds  Sanov’s theorem can be used to prove the following multiplicative Chernoﬀ bounds. Theorem D.4  Multiplicative Chernoff bounds  Let X1, . . . , Xm be independent random vari- ables drawn according to some distribution D with mean p and support included in [0, 1]. Then,  for any γ ∈ cid:2 0, 1   cid:80 m  i=1 Xi:  p − 1 cid:3 , the following inequality holds for  cid:98 p = 1 P[ cid:98 p ≥  1 + γ p] ≤ e− mpγ2 P[ cid:98 p ≤  1 − γ p] ≤ e− mpγ2  .  3  2  m  Proof: The proof consists of deriving in each case a ﬁner lower bound for the binary relative entropy than Pinsker’s inequality. Using the inequalities log 1 + x  ≥ x and log 1 + x  < x, 1+ x 2 we can write  −D  1 + γ p cid:107 p  =  1 + γ p log  p  +  1 −  1 + γ p  log   cid:21    cid:20    cid:20   1 − p  1 −  1 + γ p   cid:21   γp  1 +  1 − p − γp   1 + γ p  +  1 − p − γp  log  +  1 − p − γp   − γ2p 2 1 + γ 2  =  =  γp  1 − p − γp −γ2p 2 + γ  1   cid:35   =  1 + γ p log −γ 1 + γ 2  ≤  1 + γ p  1 + γ   cid:34   = γp  ≤ −γ2p  2 + 1  1 − 1 + γ 1 + γ 2 −γ2p  =  .  3   440  Appendix D Concentration Inequalities  Similarly, using the inequalities  1−x  log 1−x  ≥ −x+ x2 we can write  2 valid for x ∈  0, 1  and log 1−x  < −x,  −D  1 − γ p cid:107 p  =  1 − γ p log  +  1 −  1 − γ p  log  p   1 − γ p  1  1 − γ   cid:17   ≤ cid:16   =  1 − γ p log  +  1 − p + γp  log  1 −  γ − γ2 2  p +  1 − p + γp   1 − p + γp  =  .  2  −γp   cid:21    cid:20    cid:20   1 − p  1 −  1 − γ p   cid:21   γp  1 − p + γp −γ2p  This completes the proof.   cid:3   D.4 Binomial distribution tails: Upper bounds  Let X1, . . . , Xm be independent random variables taking values in {0, 1} with P[Xi = 1] = p ∈ i=1 Xi follows the binomial distribution B m, p . We will denote  i=1 Xi. Then, the following equality and inequalities hold:  pk 1 − p m−k   Binomial formula    cid:16 m   cid:17   k  =  k= cid:100  p+ cid:15  m cid:101   X − p >  cid:15   by X the average X = 1 m  [0, 1] for i = 1, . . . , m. Then, cid:80 m  cid:80 m  cid:105  m cid:88   cid:105  ≤ e−2m cid:15 2  cid:105  ≤ e  cid:105  ≤ e  cid:16   cid:15   cid:17   cid:105  ≤ e−mD p+ cid:15  cid:107 p   P cid:104  P cid:104  P cid:104  P cid:104  P cid:104   X − p >  cid:15   X − p >  cid:15   X − p >  cid:15   X − p >  cid:15   − m cid:15 2  −mσ2θ  2σ2+ 2 cid:15  3  σ2   Hoeﬀding’s inequality    Bernstein’s inequality    Bennett’s inequality    Sanov’s inequality ,  where σ2 = p 1 − p  = Var[Xi] and θ x  =  1 + x  log 1 + x  − x. The last three inequalities are shown in exercises D.6 and D.7. Using Bernstein’s inequality, for example, we can see that for  cid:15  relatively small, that is  cid:15   cid:28  2σ2, the upper bound is approximately of the form e admits a Gaussian behavior. For  cid:15   cid:29  2σ2, e− 3m cid:15   2σ2 and thus 2 , the upper bound admits a Poisson behavior. Figure D.1 shows a comparison of these bounds for diﬀerent values of the variance σ2 = p 1−p :  − m cid:15 2  small variance  p = .05 , large variance  p = .5 .  D.5 Binomial distribution tails: Lower bound  Let X be a random variable following the binomial distribution B m, p  and let k be an integer such that p ≤ 1 known as Slud’s inequality holds:  2 and mp ≤ k ≤ m 1 − p . Then, the following inequality  4 and k ≥ mp or p ≤ 1   cid:34    cid:112 mp 1 − p  N ≥ k − mp   cid:35   ,  P[X ≥ k] ≥ P   D.8   where N is in standard normal form.   D.6 Azuma’s inequality  441  Figure D.1 Comparison of tail bounds for a binomial random variable for  cid:15  = .3 and p = .05  small variance  or p = .5  maximal variance  as a function of the sample size m.  D.6 Azuma’s inequality  This section presents a concentration inequality that is more general than Hoeﬀding’s inequality. Its proof makes use of a Hoeﬀding’s inequality for martingale diﬀerences. Deﬁnition D.5  Martingale difference  A sequence of random variables V1, V2, . . . is a martin- gale diﬀerence sequence with respect to X1, X2, . . . if for all i > 0, Vi is a function of X1, . . . , Xi and  E[Vi+1X1, . . . , Xi] = 0 .  The following result is similar to Hoeﬀding’s lemma. Lemma D.6 Let V and Z be random variables satisfying E[V Z] = 0 and, for some function f and constant c ≥ 0, the inequalities:  f  Z  ≤ V ≤ f  Z  + c .  E[etV Z] ≤ et2c2 8 .  Then, for all t > 0, the following upper bound holds:   D.11  Proof: The proof follows using the same steps as in that of lemma D.1 with conditional expecta- tions used instead of expectations: conditioned on Z, V takes values in [a, b] with a = f  Z  and  cid:3  b = f  Z  + c and its expectation vanishes.  The lemma is used to prove the following theorem, which is one of the main results of this section. Theorem D.7  Azuma’s inequality  Let V1, V2, . . . be a martingale diﬀerence sequence with re- spect to the random variables X1, X2, . . ., and assume that for all i > 0 there is a constant ci ≥ 0 and random variable Zi, which is a function of X1, . . . , Xi−1, that satisfy  Zi ≤ Vi ≤ Zi + ci . Then, for all  cid:15  > 0 and m, the following inequalities hold:   cid:20  m cid:88   cid:20  m cid:88   i=1  P  i=1  Vi ≥  cid:15   ≤ exp  P  Vi ≤ − cid:15   ≤ exp   cid:21   cid:21    cid:19   cid:19    cid:18  −2 cid:15 2 cid:80 m  cid:18  −2 cid:15 2 cid:80 m  i=1 c2 i  i=1 c2 i  .   D.9    D.10    D.12    D.13    D.14    442  Appendix D Concentration Inequalities  i=1 Vk. Then, using Chernoﬀ’s bounding technique, for any  Proof: For any k ∈ [m], let Sk = cid:80 k  t > 0, we can write  P cid:2 Sm ≥  cid:15  cid:3  ≤ e−t cid:15  E[etSm ] = e−t cid:15  E cid:2 etSm−1 E[etVmX1, . . . , Xm−1] cid:3  ≤ e−t cid:15  E[etSm−1 ]et2c2 ≤ e−t cid:15 et2 cid:80 m = e−2 cid:15 2  cid:80 m where we chose t = 4 cid:15   cid:80 m  i=1 c2  i=1 c2 i=1 c2  m 8  i  8  D.7 McDiarmid’s inequality  the theorem, and the second statement is shown in a similar way.  i , i to minimize the upper bound. This proves the ﬁrst statement of  cid:3    lemma D.6    iterating previous argument   The following is the main result of this section. Its proof makes use of Azuma’s inequality. Theorem D.8  McDiarmid’s inequality  Let X1, . . . , Xm ∈ Xm be a set of m ≥ 1 independent random variables and assume that there exist c1, . . . , cm > 0 such that f : Xm → R satisﬁes the following conditions:   cid:12  cid:12 f  x1, . . . , xi, . . . , xm  − f  x1, . . . , x cid:48    D.15   i ∈ X. Let f  S  denote f  X1, . . . , Xm , then, for all  for all i ∈ [m] and any points x1, . . . , xm, x cid:48   cid:15  > 0, the following inequalities hold:  P[f  S  − E[f  S ] ≥  cid:15 ] ≤ exp   D.16   i, . . . xm  cid:12  cid:12  ≤ ci ,  cid:18  −2 cid:15 2 cid:80 m  cid:18  −2 cid:15 2 cid:80 m   cid:19   cid:19   i=1 c2 i  .  Note that V =  cid:80 m  P[f  S  − E[f  S ] ≤ − cid:15 ] ≤ exp   D.17  Proof: Deﬁne a sequence of random variables Vk, k ∈ [m], as follows: V = f  S  − E[f  S ], V1 = E[V X1] − E[V ], and for k > 1,  i=1 c2 i  Vk = E[V X1, . . . , Xk] − E[V X1, . . . , Xk−1] .  X1, . . . , Xk. Conditioning on X1, . . . , Xk−1 and taking its expectation is therefore:  k=1 Vk. Furthermore, the random variable E[V X1, . . . , Xk] is a function of  E cid:2  E[V X1, . . . , Xk]X1, . . . , Xk−1   cid:3  = E[V X1, . . . , Xk−1],  which implies E[VkX1, . . . , Xk−1] = 0. Thus, the sequence  Vk k∈[m] is a martingale diﬀerence sequence. Next, observe that, since E[f  S ] is a scalar, Vk can be expressed as follows:  Vk = E[f  S X1, . . . , Xk] − E[f  S X1, . . . , Xk−1] . Thus, we can deﬁne an upper bound Wk and lower bound Uk for Vk by:  Wk = sup x Uk = inf x  E[f  S X1, . . . , Xk−1, x] − E[f  S X1, . . . , Xk−1] E[f  S X1, . . . , Xk−1, x] − E[f  S X1, . . . , Xk−1].  Now, by  D.15 , for any k ∈ [m], the following holds:  E[f  S X1, . . . , Xk−1, x] − E[f  S X1, . . . , Xk−1, x cid:48 ] ≤ ck ,   D.18   k=1 Vk, which yields exactly  D.16  and  D.17 .  In view of these inequalities, we can apply Azuma’s inequality to  cid:3   McDiarmid’s inequality is used in several of the proofs in this book.  It can be understood in terms of stability: if changing any of its argument aﬀects f only in a limited way, then, its deviations from its mean can be exponentially bounded. Note also that Hoeﬀding’s in-  Wk − Uk = sup x,x cid:48   thus, Uk ≤ Vk ≤ Uk + ck.  V = cid:80 m   D.8 Normal distribution tails: Lower bound  443  equality  theorem D.2  is a special instance of McDiarmid’s inequality where f is deﬁned by f :  x1, . . . , xm   cid:55 → 1  m  i=1 xi.   cid:80 m  D.8 Normal distribution tails: Lower bound  If N is a random variable following the standard normal distribution, then for u > 0,   cid:16    cid:113   1 − e−u2 cid:17   .  P[N ≥ u] ≥ 1 2  1 −   D.19   D.9 Khintchine-Kahane inequality  The following inequality is useful in a variety of diﬀerent contexts, including in the proof of a lower bound for the empirical Rademacher complexity of linear hypotheses  chapter 6 . Theorem D.9  Khintchine-Kahane inequality  Let  H,  cid:107  ·  cid:107   be a normed vector space and let x1, . . . , xm be m ≥ 1 elements of H. Let σ =  σ1, . . . , σm  cid:62  with σis independent uniform random variables taking values in {−1, +1}  Rademacher variables . Then, the following inequalities hold:   cid:20  cid:13  cid:13  cid:13  m cid:88   i=1  1 2  E σ   cid:13  cid:13  cid:13 2 cid:21    cid:18   ≤  E σ   cid:20  cid:13  cid:13  cid:13  m cid:88   i=1   cid:13  cid:13  cid:13  cid:21  cid:19 2 ≤ E  σ   cid:20  cid:13  cid:13  cid:13  m cid:88   i=1   cid:13  cid:13  cid:13 2 cid:21   σixi  σixi  σixi  .   D.20   product  cid:81 m  Proof: The second inequality is a direct consequence of the convexity of x  cid:55 → x2 and Jensen’s inequality  theorem B.20 . To prove the left-hand side inequality, ﬁrst note that for any β1, . . . , βm ∈ R, expanding the m , with exponents δ1, . . . , δm in {0, 1}. We will use the notation βδ1 i=1 δm for any δ =  δ1, . . . , δm  ∈ {0, 1}m. In view of that, for any  α1, . . . , αm  ∈ Rm and t > 0, the following equality holds:  m = βδ and δ =  cid:80 m  i=1 1 + βi  leads exactly to the sum of all monomials βδ1  1 · · · βδm  1 · · · βδm  δ∈{0,1}m Diﬀerentiating both sides with respect to t and setting t = 1 yields  δ∈{0,1}m  i=1  αδ tδ =  t2−δαδ.  t2  m cid:89  m cid:89    1 + αi t  = t2  cid:88   cid:89   1 + αi  − m cid:88   αj   cid:88   cid:88   2  i=1  For any σ ∈ {−1, +1}m, let Sσ be deﬁned by Sσ =  cid:107 sσ cid:107  with sσ = cid:80 m {−1, +1}m yields cid:88   i=1 σixi. Then, setting i, multiplying both sides of  D.21  by SσSσ cid:48  , and taking the sum over all σ, σ cid:48  ∈  αi = σiσ cid:48    1 + αi  =  δ∈{0,1}m  m cid:89    2 − δ αδ .   D.21    cid:18   i cid:54 =j  j=1  i  − m cid:88   j=1   1 + σiσ cid:48   2  i=1  σ,σ cid:48 ∈{−1,+1}m   cid:19   cid:88   SσSσ cid:48   are null: since Sσ = S−σ, we have  cid:80   Note that the terms of the right-hand sum with δ ≥ 2 are non-positive. The terms with δ = 1 σ∈{−1,+1}m σδSσ = 0 in that case. Thus, the right-hand . The left-hand  side can be upper bounded by the term with δ = 0, that is, 2  δ∈{0,1}m  δ∈{0,1}m  σ,σ cid:48 ∈{−1,+1}m   2 − δ σδσ cid:48 δSσSσ cid:48   cid:88  σδσ cid:48 δSσSσ cid:48   cid:105 2  cid:104   cid:88   cid:17 2  cid:16  cid:80   σ∈{−1,+1}m  σδSσ  σ∈{−1,+1}m Sσ  .   D.22   j  i cid:54 =j   cid:89   1 + σiσ cid:48  σj σ cid:48   cid:88  i   cid:88   cid:88   σ,σ cid:48 ∈{−1,+1}m   2 − δ   δ∈{0,1}m   2 − δ   =  =  =   444  Appendix D Concentration Inequalities  side of  D.22  can be rewritten as follows:   2m+1 − m2m−1 S2   cid:88  = 2m  cid:88   σ∈{−1,+1}m  σ + 2m−1  cid:88  σ + 2m−1  cid:88   S2  σ∈{−1,+1}m σ cid:48 ∈B σ,1   SσSσ cid:48    cid:18   cid:88   σ∈{−1,+1}m   D.23  where B σ, 1  denotes the set of σ cid:48  that diﬀer from σ in exactly one coordinate j ∈ [m], that is one coordinate j ∈ [m], thus, cid:80  the set of σ cid:48  with Hamming distance one from σ. Note that for any such σ cid:48 , sσ − sσ cid:48  = 2σj xj for σ cid:48 ∈B σ,1  sσ − sσ cid:48  = 2sσ. In light of that and using the triangle  σ∈{−1,+1}m  σ cid:48 ∈B σ,1   Sσ  ,  Sσ cid:48  −  m − 2 Sσ  inequality, we can write   cid:19    m − 2 Sσ =  cid:107 msσ cid:107  −  cid:107 2sσ cid:107  =   cid:13  cid:13  cid:13   cid:88  ≤ cid:13  cid:13  cid:13   cid:88   σ cid:48 ∈B σ,1   σ cid:48 ∈B σ,1   sσ  sσ cid:48    cid:13  cid:13  cid:13   sσ − sσ cid:48    cid:13  cid:13  cid:13  − cid:13  cid:13  cid:13   cid:88   cid:13  cid:13  cid:13  ≤  cid:88   cid:105 2  σ cid:48 ∈B σ,1   Sσ cid:48  .  σ cid:48 ∈B σ,1   bounded by the ﬁrst sum 2m cid:80   Thus, the second sum of  D.23  is non-negative and the left-hand side of  D.22  can be lower σ. Combining this with the upper bound found for σ ≤ 2 S2  2m  cid:88    cid:104   cid:88   σ∈{−1,+1}m S2   D.22  gives  Sσ  .  σ∈{−1,+1}m Dividing both sides by 22m and using P[σ] = 1 2m gives Eσ[S2 proof.  σ∈{−1,+1}m  σ] ≤ 2 Eσ[Sσ] 2 and completes the  cid:3   The constant 1 2 appearing in the ﬁrst inequality of  D.20  is optimal. To see this, consider the case where m = 2 and x1 = x2 = x for some non-zero vector x ∈ H. Then, the left-hand side of the ﬁrst inequality is 1  cid:107 x cid:107 2 Eσ[σ1 + σ2] 2 =  cid:107 x cid:107 2. 2 H, we can write  i=1  cid:107 xi cid:107 2 =  cid:107 x cid:107 2 and the right-hand side  cid:0  Eσ  cid:80 m m cid:88    cid:2  cid:107  σ1 + σ2 x cid:107  cid:3  cid:1 2 = m cid:88   Note that when the norm  cid:107 · cid:107  corresponds to an inner product, as in the case of a Hilbert space   cid:20    cid:21   σiσj  xi · xj    =  [σiσj ] xi · xj   =   cid:107 xi cid:107 2,  σixi  =   cid:13  cid:13  cid:13 2 cid:21   since by the independence of the random variables σi, for i  cid:54 = j, Eσ[σiσj ] = Eσ[σi] Eσ[σj ] = 0. Thus,  D.20  can then be rewritten as follows:   cid:20  cid:13  cid:13  cid:13  m cid:88   i=1  E σ  E σ  i,j=1  i=1  E σ  i,j=1  m cid:88   cid:13  cid:13  cid:13  cid:21  cid:19 2 ≤ m cid:88   i=1  m cid:88   i=1  1 2   cid:107 xi cid:107 2 ≤   cid:18    cid:20  cid:13  cid:13  cid:13  m cid:88   i=1  E σ  σixi   cid:107 xi cid:107 2 .   D.24   D.10 Maximal inequality  The following gives an upper bound on the expectation of the maximum of a ﬁnite set of random variables that is useful in several contexts. Theorem D.10  Maximal inequality  Let X1 . . . Xn be n ≥ 1 real-valued random variables such that for all j ∈ [n] and t > 0, E[etXj ] ≤ e for some r > 0. Then, the following inequality holds:  t2 r2  2  E cid:104    cid:105  ≤ r cid:112 2 log n.  Proof: For any t > 0, by the convexity of exp and Jensen’s inequality, the following holds:  max j∈[n]  Xj  et E[maxj∈[n] Xj ] ≤ E[et maxj∈[n] Xj ] = E cid:104    cid:105  ≤ E cid:104   cid:88   etXj  j∈[n]   cid:105  ≤ ne  t2r2  2  .  etXj  max j∈[n]   D.11 Chapter notes  445  Taking the log of both sides yields  E cid:104    cid:105  ≤ log n  tr2 2  .  r  t  +  √  Xj  2 log n  max j∈[n]  Choosing t =  , which minimizes the right-hand side, gives the upper bound r   D.25  2 log n.  cid:3  Note that, in view of the expression of their moment-generating function  equation  C.24  , for standard Gaussian random variables Xj , the assumptions of the theorem hold as equalities: E[etXj ] = e Corollary D.11  Maximal inequality  Let X1 . . . Xn be n ≥ 1 real-valued random variables such i=1 Yij where, for each ﬁxed j ∈ [n], Yij are independent zero mean random variables taking values in [−ri, +ri], for some ri > 0. Then, the following inequality holds:  that for all j ∈ [n], Xj = cid:80 m  t2 2 .  √  E cid:104   max j∈[n]  Xj   cid:105  ≤ r cid:112 2 log n,   cid:113  cid:80 m  with r =  i=1 r2 i .  Proof: By the independence of the Yij s for ﬁxed j and Hoeﬀding’s lemma  Lemma D.1 , the following inequality holds for all j ∈ [n]:  E[etXj ] = E cid:104  m cid:89    cid:105   m cid:89   E[etYij ] ≤ m cid:89   etYij  =  t2r2 j  e  2 = e  t2r2  2  ,  i=1 r2  i . The result then follows immediately by Theorem D.10.  i=1  i=1  i=1   D.26   cid:3   with r2 = cid:80 m  D.11 Chapter notes  Several of the concentration inequalities presented in this chapter are based on a bounding tech- nique due to Chernoﬀ [1952]. Theorem D.3 is due to Sanov [1957]. For the exponential inequal- ity of exercise D.7, which is an alternative form of Sanov’s inequality, see [Hagerup and R¨ub, 1990] and the references therein. The multiplicative Chernoﬀ bounds presented in this chapter  Theorem D.4  were given by Angluin and Valiant [1979]. Hoeﬀding’s inequality and lemma  Lemma D.1 and Theorem D.2  are due to Hoeﬀding [1963]. The improved version of Azuma’s inequality [Hoeﬀding, 1963, Azuma, 1967] presented in this chapter is due to McDiarmid [1989]. The improvement is a reduction of the exponent by a factor of 4. This also appears in McDi- armid’s inequality, which is derived from the inequality for bounded martingale sequences. The inequalities presented in exercise D.6 are due to Bernstein [1927] and Bennett [1962]; the exercise is from Devroye and Lugosi [1995].  The binomial inequality of section D.5 is due to Slud [1977]. The tail bound of section D.8 is due to Tate [1953]  see also Anthony and Bartlett [1999] . The Khintchine-Kahane inequality was ﬁrst studied in the case of real-valued variables x1, . . . , xm by Khintchine [1923], with better constants and simpler proofs later provided by Szarek [1976], Haagerup [1982], and Tomaszewski [1982]. The inequality was extended to normed vector spaces by Kahane [1964]. The proof presented here is due to Lata cid:32 la and Oleszkiewicz [1994] and provides the best possible constants.  D.12 Exercises  D.1 Twins paradox. Professor Mamoru teaches at a university whose computer science and math  building has F = 30 ﬂoors.   1  Assume that the ﬂoors are independent and that they have the same probability to be selected by someone taking the elevator. How many people should take the elevator in order to make it likely  probability more than half  that two of them go to the same ﬂoor?   446  Appendix D Concentration Inequalities   Hint: use the Taylor series expansion of e−x = 1 − x + . . . and give an approximate general expression of the solution.    2  Professor Mamoru is popular, and his ﬂoor is in fact more likely to be selected than others. Assuming that all other ﬂoors are equiprobable, derive the general expression of the probability that two people go to the same ﬂoor, using the same approximation as before. How many people should take the elevator in order to make it likely that two of them go to the same ﬂoor when the probability of Professor Mamoru’s ﬂoor is .25, .35, or .5? When q = .5, would the answer change if the number of ﬂoors were instead F = 1,000?   3  The probability models assumed in  1  and  2  are both naive. If you had access to the  data collected by the elevator guard, how would you deﬁne a more faithful model?  D.2 Estimating label bias. Let D be a distribution over X and let f : X × {−1, +1} be a labeling function. Suppose we wish to ﬁnd a good approximation of the label bias of the distribution D, that is of p+ deﬁned by:  Let S be a ﬁnite labeled sample of size m drawn i.i.d. according to D. Use S to derive an  estimate  cid:98 p+ of p+. Show that for any δ > 0, with probability at least 1 − δ, p+ − cid:98 p+ ≤  cid:113  log 2 δ    D.27   p+ = P x∼D  [f  x  = +1].  2m .  D.3 Biased coins. Professor Moent has two coins in his pocket, coin xA and coin xB. Both coins are slightly biased, i.e., P[xA = 0] = 1 2 −  cid:15  2 and P[xB = 0] = 1 2 +  cid:15  2, where 0 <  cid:15  < 1 is a small positive number, 0 denotes heads and 1 denotes tails. He likes to play the following game with his students. He picks a coin x ∈ {xA, xB} from his pocket uniformly at random, tosses it m times, reveals the sequence of 0s and 1s he obtained and asks which coin was tossed. Determine how large m needs to be for a student’s coin prediction error to be at most δ > 0.   a  Let S be a sample of size m. Professor Moent’s best student, Oskar, plays according to the decision rule fo : {0, 1}m → {xA, xB} deﬁned by fo S  = xA iﬀ N  S  < m 2, where N  S  is the number of 0’s in sample S. Suppose m is even, then show that  error fo  ≥ 1 2  N  S  ≥ m 2  error fo  >  − m cid:15 2  1 − e  P cid:104  1 − cid:104   cid:104  1 − cid:104   1 4   cid:104   1 4  .   cid:12  cid:12  cid:12 x = xA  cid:105  1− cid:15 2 cid:105  1 2 cid:105   cid:105  1 2 cid:105   .  1− cid:15 2  .   D.28    D.29    D.30    c  Argue that if m is odd, the probability can be lower bounded by using m + 1 in the bound  in  a  and conclude that for both odd and even m, 1 − e  error fo  >  − 2 cid:100 m 2 cid:101  cid:15 2   d  Using this bound, how large must m be if Oskar’s error is at most δ, where 0 < δ < 1 4.  What is the asymptotic behavior of this lower bound as a function of  cid:15 ?   e  Show that no decision rule f : {0, 1}m → {xA, xB} can do better than Oskar’s rule fo.  Conclude that the lower bound of the previous question applies to all rules.  D.4 Concentration bounds. Let X be a non-negative random variable satisfying P[X > t] ≤ 2m  Hint: to do that, use the  for all t > 0 and some c > 0. Show that E[X 2] ≤ log ce   ce−2mt2   b  Assuming m even, show that   D.12 Exercises  447  identity E[X 2] = cid:82  +∞  P[X 2 > t]dt, write  cid:82  +∞  ﬁnd the best u to minimize the upper bound .  0  0  = cid:82  u 0 + cid:82  +∞  u  , bound the ﬁrst term by u and  D.5 Comparison of Hoeﬀding’s and Chebyshev’s inequalities. Let X1, . . . , Xm be a sequence of random variables taking values in [0, 1] with the same mean µ and variance σ2 < ∞ and let X = 1 m  i=1 Xi.   cid:80 m   a  For any  cid:15  > 0, give a bound on P[X − µ >  cid:15 ] using Chebyshev’s inequality, then Hoeﬀd-  ing’s inequality. For what values of σ is Chebyshev’s inequality tighter?   b  Assume that the random variables Xi take values in {0, 1}. Show that σ2 ≤ 1  4 . Use this to simplify Chebyshev’s inequality. Choose  cid:15  = .05 and plot Chebyshev’s inequality thereby modiﬁed and Hoeﬀding’s inequality as a function of m  you can use your preferred program for generating the plots .  D.6 Bennett’s and Bernstein’s inequalities. The objective of this problem is to prove these two  inequalities.  X ≤ c,  where   a  Show that for any t > 0, and any random variable X with E[X] = 0, E[X 2] = σ2, and  E[etX ] ≤ ef  σ2 c2 ,   cid:16  1  f  x  = log  e−ctx +  x  1 + x  1 + x  ect cid:17   .   D.31    cid:20  1  m  m cid:88   i=1  P   cid:20  1  m  P  m cid:88    cid:20  1  m cid:88    cid:21    cid:21    cid:21    b  Show that f cid:48  cid:48  x  ≤ 0 for x ≥ 0.   c  Using Chernoﬀ’s bounding technique, show that  −tm cid:15 + cid:80 m  Xi ≥  cid:15   ≤ e  i=1 f  σ2 Xi   c2 ,  where  σ2  Xi  is the variance of Xi.   d  Show that f  x  ≤ f  0  + xf cid:48  0  =  ect − 1 − ct x.   e  Using the bound derived in  4 , ﬁnd the optimal value of t.   f  Bennett’s inequality. Let X1, . . . , Xm be independent real-valued random variables with  zero mean such that for i = 1, . . . , m, Xi ≤ c. Let σ2 = 1  . Show that  where θ x  =  1 + x  log 1 + x  − x.  i=1  Xi >  cid:15   ≤ exp  − mσ2 c2  θ  σ2   g  Bernstein’s inequality. Show that under the same conditions as Bennett’s inequality  ≤ exp  Hint: show that for all x ≥ 0, θ x  ≥ h x  = 3  Xi >  cid:15   i=1  m  P  x2 x+3 .   2  −  m cid:15 2  2σ2 + 2c cid:15  3   h  Write Hoeﬀding’s inequality assuming the same conditions. For what values of σ is Bern-  stein’s inequality better than Hoeﬀding’s inequality?   cid:18    cid:18    cid:80 m  cid:18   cid:15 c  cid:19  cid:19   m  i=1 σ2 Xi  ,   cid:19   .   D.32    D.33    448  Appendix D Concentration Inequalities  D.7 Exponential inequality. Let X be a random variable following a binomial distribution B m, p .   a  Use Sanov’s inequality to show that the following exponential inequality holds for any   cid:15  > 0:  .   D.34    b  Use that to show that the following holds:   cid:17 1− p+ cid:15   cid:21 m   cid:20  X  m  P   cid:21   − p >  cid:15   ≤   cid:20  X  m  P   cid:20  cid:16  p  cid:21   p+ cid:15   1− p+ cid:15     cid:17 p+ cid:15  cid:16  1−p  cid:17 m p+ cid:15   ≤ cid:16  p  cid:21   −mpθ   cid:15   p+ cid:15   − p >  cid:15   ≤ e  p  ,  − p >  cid:15    cid:20  X  m  P  em cid:15 .   c  Prove that  where θ is deﬁned as in exercise D.6.   D.35    D.36    E Notions of Information Theory  This chapter introduces some basic notions of information theory useful for the presentation of several learning algorithms and their properties. The deﬁnitions and theorems are given in the case of discrete random variables or distributions, but they can be straightforwardly extended to the continuous case.  We start with the notion of entropy, which can be viewed as a measure of the uncertainty of a  random variable.  E.1 Entropy Deﬁnition E.1  Entropy  The entropy of a discrete random variable X with probability mass function p x  = P[X = x] is denoted by H X  and deﬁned by  H X  = − E [log p X  ] = − cid:88   x∈X  p x  log p x  .   E.1   We deﬁne by the same expression the entropy of a distribution p and abusively denote that by H p .  The base of the logarithm is not critical in this deﬁnition since it only aﬀects the value by a multiplicative constant. Thus, unless otherwise speciﬁed, we will consider the natural logarithm If we use base 2, then − log2 p x   is the number of bits needed to represent p x .  base e . Thus, by deﬁnition, the entropy of X can be viewed as the average number of bits  or amount of information  needed for the description of the random variable X. By the same property, the entropy is always non-negative:   E.2  As an example, the entropy of a biased coin Xp taking value 1 with probability p and 0 with probability 1 − p is given by  H X  ≥ 0.  H Xp  = −p log p −  1 − p  log 1 − p .   E.3   The corresponding function of p is often referred to as the binary entropy function. Figure E.1 shows a plot of that function when using base 2 for the logarithm. As can be seen from the ﬁgure, the entropy is a concave function.26 It reaches its maximum at p = 1 2 , which corresponds to the most uncertain case, and its minima at p = 0 or p = 1 which correspond to the fully certain cases. More generally, assume that the input space X has a ﬁnite cardinality N ≥ 1. Then, by Jensen’s  26 We will see later that the entropy function is always concave.   450  Appendix E Notions of Information Theory  Figure E.1 A plot of the binary entropy as a function of the bias p.  inequality, in view of the concavity of logarithm, the following inequality holds:   cid:20    cid:21   H X  = E  log  1  p X   ≤ log E   cid:20  1   cid:21   p X    cid:18  cid:88   x∈X   cid:19   p x  p x   = log  = log N.   E.4   Thus, more generally, the maximum value of the entropy is log N , that is the entropy of the uniform distribution.  The entropy is a lower bound on lossless data compression and is therefore a critical quantity to consider in information theory. It is also closely related to the notions of entropy in thermo- dynamics and quantum physics.  E.2 Relative entropy  Here, we introduce a measure of divergence between two distributions p and q, relative entropy, which is related to the notion of entropy. The following is its deﬁnition in the discrete case. Deﬁnition E.2  Relative entropy  The relative entropy  or Kullback-Leibler divergence  of two distributions p and q is denoted by D p cid:107 q  and deﬁned by  D p cid:107 q  = E  log  p   cid:20    cid:21    cid:88    cid:20  p x    cid:21  0 = +∞ for a > 0.  p x  log  q x   ,  p X  q X   =  x∈X 0 = 0, and a log a  with the conventions 0 log 0 = 0, 0 log 0   E.5   Note that, in view of these conventions, whenever q x  = 0 for some x in the support of p  p x  > 0 , the relative entropy is inﬁnite: D p cid:107 q  = ∞. Thus, the relative entropy does not provide an informative measure of the divergence of p and q in such cases.  As for the entropy, the base of the logarithm is not critical in the deﬁnition of the relative entropy and we will consider the natural logarithm unless otherwise speciﬁed. If we use base 2, the relative entropy can be interpreted in terms of coding length. Ideally, one could design for p an optimal code with average length the entropy H p . The relative entropy is the average number of additional bits needed to encode p when using an optimal code for q instead of one for p since it can be expressed as the diﬀerence D p cid:107 q  = Ep[log proposition, is always non-negative. Proposition E.3  Non-negativity of relative entropy  For any two distributions p and q, the following inequality holds: Furthermore, D p cid:107 q  = 0 iﬀ p = q.  q X  ]− H p , which, as shown by the following  D p cid:107 q  ≥ 0.   E.6   1   E.2 Relative entropy  451  Proof: By the concavity of logarithm and Jensen’s inequality, the following holds:   cid:88   −D p cid:107 q  =  p x  log  x : p x >0   cid:18  q x    cid:19   p x    cid:18   cid:88   cid:18   cid:88   x : p x >0  x : p x >0  ≤ log  = log   cid:19   p x   q x  p x    cid:19   q x   ≤ log 1  = 0.  that  cid:80  that set, we must have  cid:80   Thus, the relative entropy is always non-negative for all distributions p and q. The equality D p cid:107 q  = 0 can hold only if both of the inequalities above are equalities. The last one implies x : p x >0 q x  = 1. Since the log function is strictly concave, the ﬁrst inequality can be p x  is some constant α over {x : p x  > 0}. Since p x  sums to one over an equality only if q x  x : p x >0 q x  = α. Thus, α = 1, which implies q x  = p x  for all x ∈ {x : p x  > 0} and thus for all x. Finally, by deﬁnition, for any distribution p, D p cid:107 p  = 0,  cid:3  which completes the proof. in general, D p cid:107 q   cid:54 = D q cid:107 p  for two The relative entropy is not a distance. It is asymmetric: distributions p and q. Furthermore, in general, the relative entropy does not verify the triangle inequality. Corollary E.4  Log-sum inequality  For any set of non-negative real numbers a1, . . . , an and b1, . . . , bn, the following inequality holds:  n cid:88    cid:16  ai   cid:17  ≥ cid:16  n cid:88    cid:17   ai  log  ai log   cid:19    cid:18  cid:80 n i=1 ai cid:80 n 0 = +∞ for a > 0.  i=1 bi  ,   E.7   i=1  i=1  is a constant  does not depend on i .  bi with the conventions 0 log 0 = 0, 0 log 0 0 = 0, and a log a Furthermore, equality holds in  E.7  iﬀ ai bi  Proof: With the conventions adopted, it is clear that the equality holds if  cid:80 n ai = 0 for all i ∈ [n], or  cid:80 n  cid:80 n i=1 ai  cid:54 = 0 and  cid:80 n can multiply them by positive constants such that  cid:80 n  i=1 ai = 0, that is i=1 bi = 0, that is bi = 0 for all i ∈ [n]. Thus, we can assume that i=1 bi  cid:54 = 0. Since the inequality is invariant by scaling of the ais or bis, we i=1 bi = 1. The inequality then coincides with the non-negativity of the relative entropy of the distributions thereby deﬁned by  cid:3  ais and bis and the result holds by Proposition E.3. Corollary E.5  Joint convexity of relative entropy  The relative entropy function  p, q   cid:55 → D p cid:107 q  is convex.  i=1 ai = cid:80 n  Proof: For any α ∈ [0, 1] and any four probability distributions p1, p2, q1, q2, by the Log-sum inequality  Corollary E.4 , the following holds for any ﬁxed x:   cid:21    cid:20  αp1 x  +  1 − α p2 x   cid:20  αp1 x   cid:21   cid:20   1 − α p1 x   cid:17  ≤ αD p1 cid:107 q1  +  1 − α D p2 cid:107 q2 ,  αq1 x  +  1 − α q2 x  ≤ αp1 x  log  +  1 − α p2 x  log   1 − α q2 x   αq1 x    cid:21   .   E.8    E.9   cid:3    cid:16   Summing up these inequalities over all x yields: αp1 +  1 − α p2 cid:107 αq1 +  1 − α q2  D  which concludes the proof.   αp1 x  +  1 − α p2 x   log  Corollary E.6  Concavity of the entropy  The entropy function p  cid:55 → H p  is concave.  Proof: Observe that for any ﬁxed distribution p0 over X, by deﬁnition of the relative entropy, we can write  p x  log p0 x  .   E.10   D p cid:107 p0  =   cid:88   x∈X  p x  log p x   − cid:88   x∈X   Appendix E Notions of Information Theory  452  Thus, H p  = −D p cid:107 p0  − cid:80   x∈X p x  log p0 x  . By Corollary E.5, the ﬁrst term is a concave function of p. The second term is linear in p and therefore is concave. Thus, H is concave as a  cid:3  sum of two concave functions.  Proposition E.7  Pinsker’s inequality  For any two distributions p and q, the following inequal- ity holds:  D p cid:107 q  ≥ 1 2   cid:107 p − q cid:107 2 1.   E.11   f  q0  = p0 log  Proof: We ﬁrst show that the inequality holds for distributions over a set A = {a0, a1} of cardinality 2. Let p0 = p a0  and q0 = q a0 . Fix p0 ∈ [0, 1] and consider the function f : q0  cid:55 → f  q0  deﬁned by  +  1 − p0  log  Observe that f  p0  = 0 and that for q0 ∈  0, 1 ,  p0 q0 1 − p0  E.13  1 − q0 Since  1 − q0 q0 ≤ 1 − 4] is non-negative. Thus, f cid:48  q0  ≤ 0 for q0 ≤ p0 and f cid:48  q0  ≥ 0 for q0 ≥ p0. Thus, f reaches its minimum at q0 = p0, which implies f  q0  ≥ f  p0  = 0 for all q0. Since f  q0  can be expressed as follows:  + 4 p0 − q0  =  q0 − p0   f cid:48  q0  = − p0 q0 1  − 2 p0 − q0 2.  1 − p0 1 − q0   1 − q0 q0   1−q0 q0   E.12   − 4   cid:21    cid:20   4 , [  +  1  .  f  q0  = D p cid:107 q  − 2 p0 − q0 2   cid:104 p0 − q0 +  1 − p0  −  1 − q0  cid:105 2  = D p cid:107 q  − 1 2 = D p cid:107 q  − 1 2  this proves the inequality for a set A = {a0, a1} of cardinality 2.  Now, consider the distributions p cid:48  and q cid:48  deﬁned over A = {a0, a1} with p cid:48  a0  = cid:80  and q cid:48  a0  = cid:80   p x , q x  where a0 = {x ∈ X : p x  ≥ q x } and a1 = {x ∈ X : p x  < q x }. By  x∈a0   E.16    cid:107 p − q cid:107 2  1 ≥ 0,  x∈a0  the Log-sum inequality  Corollary E.4 ,  Combining this inequality with the observation that  D p cid:107 q  =  p x  log  +   cid:21    cid:20  p x   cid:21   cid:20  p a0   q x   q a0    cid:21    cid:20  p x   cid:21   q x    cid:88   x∈a1  p x  log   cid:20  p a1   q a1   + p a1  log   cid:88   x∈a0  ≥ p a0  log = D p cid:48  cid:107 q cid:48  .   cid:107 p cid:48  − q cid:48  cid:107 1 =  p a0  − q a0   −  p a1  − q a1     cid:88   cid:88   x∈a0  =   p x  − q x   −  cid:88   x∈a1  p x  − q x    p x  − q x    x∈X  = =  cid:107 p − q cid:107 1, 2 cid:107 p − q cid:107 2  shows that D p cid:107 q  ≥ D p cid:48  cid:107 q cid:48   ≥ 1  1 and concludes the proof.  Deﬁnition E.8  Conditional relative entropy  Let p and q be two probability distributions de- ﬁned over X × Y and r a distribution over X. Then, the conditional relative entropy of p and q with respect to the marginal r is deﬁned as the expectation of the relative entropy of p ·X  and q ·X  with respect to r:   cid:104   D cid:0 p ·X  cid:107 q ·X  cid:1  cid:105   E X∼r   cid:88   x∈X   cid:88   y∈Y  =  r x   p yx  log  = D  cid:101 p cid:107  cid:101 q ,  p yx  q yx    E.24    E.14    E.15    E.17    E.18    E.19    E.20    E.21    E.22    E.23   cid:3    E.3 Mutual information  453  Figure E.2 Illustration of the quantity measured by the Bregman divergence deﬁned based on a convex and diﬀerentiable function F . The divergence measures the distance between F  x  and the hyperplane tangent to the curve at point y.  where  cid:101 p x, y  = r x p yx  and  cid:101 q x, y  = r x q yx , with the conventions 0 log 0 = 0, 0 log 0  0 = 0,  and a log a  0 = +∞ for a > 0.  E.3 Mutual information Deﬁnition E.9  Mutual information  Let X and Y be two random variables with joint probability distribution function p ·, ·  and marginal probability distribution functions p x  and p y . Then, the mutual information of X and Y is denoted by I X, Y   and deﬁned as follows:  I X, Y   = D p x, y  cid:107 p x p y   p X, Y   p X p Y    = E  p x,y   log   cid:20    cid:21   =   cid:20  p x, y   cid:88  0 = +∞ for a > 0.  p x, y  log  p x p y   x∈X,y∈Y   cid:21   ,  with the conventions 0 log 0 = 0, 0 log 0  0 = 0, and a log a   E.25    E.26   When the random variables X and Y are independent, their joint distributions is the product of the marginals p x  and p y . Thus, the mutual information is a measure of the closeness of the joint distribution p x, y  to its value when X and Y are independent, where closeness is measured via the relative entropy divergence. As such, it can be viewed as a measure of the amount of information that each random variable can provide about the other. Note that by Proposition E.3, the equality I X, Y   = 0 holds iﬀ p x, y  = p x p y  for all x, y, that is iﬀ X and Y are independent.  E.4 Bregman divergences  Here we introduce the so-called unnormalized relative entropy  cid:101 D deﬁned for all non-negative  functions p, q in RX by   cid:101 D p cid:107 q  =   cid:88   x∈X   cid:20  p x    cid:21   q x   + cid:0 q x  − p x  cid:1 ,  p x  log   E.27    454  Appendix E Notions of Information Theory  Table E.1 Examples of Bregman divergences and corresponding convex functions.  Squared L2-distance Mahalanobis distance Unnormalized relative entropy  BF  x  cid:107  y   cid:107 x − y cid:107 2  cid:101 D x  cid:107  y    x − y  cid:62 Q x − y    cid:80   F  x   cid:107 x cid:107 2 x cid:62 Qx  i∈I x i  log x i   − x i   0 = +∞ for a > 0. The relative entropy with the conventions 0 log 0 = 0, 0 log 0 coincides with the unnormalized relative entropy when restricted to ∆×∆, where ∆ is the family of relative entropy, in particular, it can be shown that  cid:101 D p cid:107 q  ≥ 0. Many of these properties are in distributions deﬁned over X. The relative entropy inherits several properties of the unnormalized  0 = 0, and a log a  fact shared by a broader family of divergences known as Bregman divergences. Deﬁnition E.10  Bregman divergences  Let F be a convex and diﬀerentiable function deﬁned over a convex  open  set C in a Hilbert space H. Then, the Bregman divergence BF associated to F is deﬁned for all x, y ∈ C by  BF  x cid:107 y  = F  x  − F  y  −  cid:104 ∇F  y , x − y cid:105  .   E.28  Thus, BF  x  cid:107  y  measures the diﬀerence of F  x  and its linear approximation. Figure E.2 illus- trates this deﬁnition. Table E.1 provides several examples of Bregman divergences along with their corresponding convex functions F  x . Note that, although the unnormalized relative entropy is a Bregman divergence, the relative entropy is not a Bregman divergence since it is deﬁned over the simplex which is not an open set and has an empty interior.  The following proposition presents several general properties of Bregman divergences.  Proposition E.11 Let F be a convex and diﬀerentiable function deﬁned over a convex set C in a Hilbert space H. Then, the following properties hold: 1. ∀x, y ∈ C, BF  x  cid:107  y  ≥ 0. 2. ∀x, y, z ∈ C,  cid:104 ∇F  x  − ∇F  y , x − z cid:105  = BF  x  cid:107  y  + BF  z  cid:107  x  − BF  z  cid:107  y .  3. BF is convex in its ﬁrst argument. If additionally F is strictly convex, then BF is strictly  4. Linearity:  let G be a convex and diﬀerentiable function over C, then, for any α, β ∈ R,  convex in its ﬁrst argument.  BαF +βG = αBF + βBG.  For the following properties, we will assume additionally that F is strictly convex.  5. Projection: for any y ∈ C and any closed convex set K ⊆ C, the BF -projection of y over K,  PK y  = argminx∈K BF  x  cid:107  y , is unique.  6. Pythagorean theorem: for y ∈ C and any closed convex set K ⊆ C, the following holds for all  x ∈ K:  BF  x  cid:107  y  ≥ BF  x  cid:107  PK y   + BF  PK y   cid:107  y .   E.29   7. Conjugate divergence: assume that F is closed proper strictly convex, and that the norm of its gradient tends to inﬁnity near the boundary of C: limx→∂C  cid:107 ∇F  x  cid:107  = +∞. The pair  C, F   is then said to be a convex function of Legendre type. Then, the conjugate of F , F ∗, is diﬀerentiable and the following holds for all x, y ∈ C:  BF  x  cid:107  y  = BF ∗  ∇F  y   cid:107  ∇F  x  .   E.30    E.4 Bregman divergences  455  Figure E.3 A depiction of the Pythagorean theorem stated in proposition E.11, where the squared length of each line illustrates the magnitude of the Bregman divergence between the points it connects.  Proof: Property  1  holds by convexity of the function F  the graph of F is above its tangent, see equation  B.3  .  Property  2  follows directly from the deﬁnition of the Bregman divergence:  BF  x  cid:107  y  + BF  z  cid:107  x  − BF  z  cid:107  y   = −  cid:104 ∇F  y , x − y cid:105  −  cid:104 ∇F  x , z − x cid:105  +  cid:104 ∇F  y , z − y cid:105  =  cid:104 ∇F  x  − ∇F  y , x − z cid:105  .  Property  3  holds since x  cid:55 → F  x  − F  y  −  cid:104 ∇F  y , x − y cid:105  is convex as a sum of the convex function x  cid:55 → F  x  and the aﬃne and thus convex function x  cid:55 → −F  y − cid:104 ∇F  y , x − y cid:105 . Similarly, BF is strictly convex with respect to its ﬁrst argument if F is strictly convex, as a sum of a strictly convex function and an aﬃne function.  BαF +βG = αF  x  + βG x  − αF  y  − βG y  − cid:10 ∇ cid:0 αF  y  + βG y  cid:1 , x − y cid:11   = α cid:0 F  x  − F  y  −  cid:104 ∇F  y , x − y cid:105  cid:1  + β cid:0 G x  − G y  −  cid:104 ∇G y , x − y cid:105  cid:1   Property  4  follows from a series of equalities:  = αBF + βBG,  where we have used the fact that both the gradient and inner-product are linear functions.  Property  5  holds since, by Property  3 , minx∈K BF  x  cid:107  y  is a convex optimization problem For property  6 , ﬁx y ∈ C and let J be the function deﬁned for all α ∈ [0, 1] by  with a strictly convex objective function.  J α  = BF  αx +  1 − α PK y   cid:107  y .  Since C is convex, for any α ∈ [0, 1], αx +  1− α PK y  is in C. F is diﬀerentiable over C therefore J is also diﬀerentiable as a composition of F with α  cid:55 → αx +  1− α PK y . By deﬁnition of PK y , for any α ∈  0, 1],  J α  − J 0   BF  αx +  1 − α PK y   cid:107  y  − BF  PK y   cid:107  y   =  ≥ 0.  This implies that J cid:48  0  ≥ 0. From the following expression of J α :  α  α  J α  = F  αx +  1 − α PK y   − F  y  −  cid:104 ∇F  y , αx +  1 − α PK y  − y cid:105  ,   E.31    E.32   we can compute its derivative at 0:  J cid:48  0  =  cid:104 x − PK y , ∇F  PK y   cid:105  −  cid:104 ∇F  y , x − PK y  cid:105   = −BF  x  cid:107  PK y   + F  x  − F  PK y   −  cid:104 ∇F  y , x − PK y  cid:105  = −BF  x  cid:107  PK y   + F  x  − F  PK y   −  cid:104 ∇F  y , x − y cid:105  −  cid:104 ∇F  y , y − PK y  cid:105  = −BF  x  cid:107  PK y   + BF  x  cid:107  y  + F  y  − F  PK y   −  cid:104 ∇F  y , y − PK y  cid:105  = −BF  x  cid:107  PK y   + BF  x  cid:107  y  − BF  PK y   cid:107  y  ≥ 0,  which concludes the proof of Property  6 .  y  PK y   x  K   456  Appendix E Notions of Information Theory   cid:8  cid:104 x, y cid:105  − F  x  cid:9 .  For property  7 , note that, by deﬁnition, for any y, F ∗ is deﬁned by  F ∗ y  = sup x∈C   E.33  F ∗ is convex and admits a sub-diﬀerential at any y. By the strict convexity of F , the function x  cid:55 →  cid:104 x, y cid:105  − F  x  is strictly concave and diﬀerentiable over C and the norm of its gradient, y − ∇F  x , tends to inﬁnity near the boundary of C  by the corresponding property assumed for F  . Thus, its supremum is reached at a unique point xy ∈ C where its gradient is zero, that is at xy with ∇F  xy  = y. This implies that for any y, ∂F ∗ y , the subdiﬀerential of F ∗, is reduced to a singleton. Thus, F ∗ is diﬀerentiable and its gradient at y is ∇F ∗ y  = xy = ∇−1F  y . Since F ∗ is convex and diﬀerentiable, its Bregman divergence is well deﬁned. Furthermore, F ∗ y  = and the expression of ∇F ∗ y  and F ∗ y  we can write   cid:10 ∇F −1 y , y cid:11  − F  ∇F −1 y   since xy = ∇−1F  y . For any x, y ∈ C, using the deﬁnition of BF ∗  = F ∗ ∇F  y   − F ∗ ∇F  x   − cid:10 ∇−1F  ∇F  x  , ∇F  y  − ∇F  x  cid:11  BF ∗  ∇F  y   cid:107  ∇F  x   = cid:10 ∇−1F  ∇F  y  , ∇F  y  cid:11  − F  ∇−1F  ∇F  y     − cid:10 ∇−1F  ∇F  x  , ∇F  x  cid:11  + F  ∇−1F  ∇F  x    −  cid:104 x, ∇F  y  − ∇F  x  cid:105   = F ∗ ∇F  y   − F ∗ ∇F  x   −  cid:104 x, ∇F  y  − ∇F  x  cid:105   =  cid:104 y, ∇F  y  cid:105  − F  y  −  cid:104 x, ∇F  x  cid:105  + F  x  −  cid:104 x, ∇F  y  − ∇F  x  cid:105  =  cid:104 y, ∇F  y  cid:105  − F  y  + F  x  −  cid:104 x, ∇F  y  cid:105  = F  x  − F  y  −  cid:104 x − y, ∇F  y  cid:105  = BF  x  cid:107  y ,  which completes the proof.  Notice that while the unnormalized relative entropy  and thus the relative entropy  are convex functions of the pair of their arguments, this in general does not hold for all Bregman divergences, only convexity with respect to the ﬁrst argument is guaranteed.  The notion of Bregman divergence can be extended to the case of non-diﬀerentiable functions   cid:3    see section 14.3 .  E.5 Chapter notes  The notion of entropy presented in this chapter is due to Shannon [1948] who, more generally, within the same article, set the foundation of information theory. More general deﬁnitions of entropy  R´enyi entropy  and relative entropy  R´enyi divergence  were later introduced by R´enyi [1961]. The Kullback-Leibler divergence was introduced in [Kullback and Leibler, 1951].  Pinsker’s inequality is due to Pinsker [1964]. Finer inequalities relating the relative entropy and the L1-norm were later given by Csisz´ar [1967] and Kullback [1967]. See [Reid and Williamson, 2009] for a generalization of such inequalities to the case of f -divergences. The notion of Bregman divergence is due to Bregman [1967].  For a more extensive material on information theory, we strongly recommend the book of Cover  and Thomas [2006].   E.6 Exercises  457  Figure E.4 An illustration of the parallelogram identity.  E.6 Exercises  E.1 Parallelogram identity. Prove the following parallelogram identity for any three distributions  Does the equality hold if we replace the relative entropy by the norm-2 squared? Figure E.4 illustrates a particular example of this identity. Note, in the example we have  p, q, and r on X:  D p  cid:107  r  + D q  cid:107  r  = 2D  2   cid:107 p − r cid:107 2 = cid:13  cid:13  cid:0 p − p + q = cid:13  cid:13 p − p + q  cid:107 q − r cid:107 2 = cid:13  cid:13  cid:0 q − p + q = cid:13  cid:13 q − p + q  2  2  and   cid:17   .  q  cid:107  p + q  2   E.34   2  2  + D  + D   cid:107  r   cid:16    cid:17    cid:16   p  cid:107  p + q   cid:16  p + q  cid:1  + cid:0  p + q  cid:13  cid:13 2 + cid:13  cid:13  p + q  cid:1  + cid:0  p + q  cid:13  cid:13 2 + cid:13  cid:13  p + q   cid:17  − r cid:1  cid:13  cid:13 2 − r cid:13  cid:13 2 − 2 cos π − θ  cid:13  cid:13 p − p + q − r cid:1  cid:13  cid:13 2 − r cid:13  cid:13 2 − 2 cos θ  cid:13  cid:13 q − p + q  2  2  2  2  2  − r cid:13  cid:13   cid:13  cid:13  cid:13  cid:13  p + q − r cid:13  cid:13  .  cid:13  cid:13  cid:13  cid:13  p + q  Summing these two quantities shows the identity holds for the example.  2  2  2  2  p + q  p  q  θ  p + q  2  r    F Notation  Table F.1 Summary of notation.  R R+ Rn Rn×m [a, b]  a, b  {a, b, c} [n] N log loga S S s ∈ S X  Y H  cid:104 ·, · cid:105  v 1 vi  cid:107 v cid:107   cid:107 v cid:107 p u ◦ v  Set of real numbers Set of non-negative real numbers Set of n-dimensional real-valued vectors Set of n × m real-valued matrices Closed interval between a and b Open interval between a and b Set containing elements a, b and c The set {1, 2, . . . , n} Set of natural numbers, i.e., {0, 1, . . .} Logarithm with base e Logarithm with base a An arbitrary set Number of elements in S An element in set S Input space Target space Feature space Inner product in feature space An arbitrary vector Vector of all ones ith component of v L2 norm of v Lp norm of v Hadamard or entry-wise product of vectors u and v   460  Appendix F Notation  Composition of functions f and g Composition of weighted transducers T1 and T2 An arbitrary matrix Spectral norm of M Frobenius norm of M Transpose of M Pseudo-inverse of M Trace of M Identity matrix  f ◦ g T1 ◦ T2 M  cid:107 M cid:107 2  cid:107 M cid:107 F M cid:62  M† Tr[M] I K : X × X → R Kernel function over X K 1A hS R ·   Kernel matrix Indicator function indicating membership in subset A The hypothesis function returned when training with sample S Generalization error or risk Empirical error or risk with respect to sample S Empirical margin error with margin ρ and with respect to sample S Rademacher complexity over all samples of size m Empirical Rademacher complexity with respect to sample S Standard normal distribution Expectation over x drawn from distribution D  Kleene closure over a set of characters Σ   cid:98 RS  ·   cid:98 RS,ρ ·   cid:98 RS  ·   Rm ·   [·]  N  0, 1  E x∼D Σ∗   Bibliography  Shivani Agarwal and Partha Niyogi. Stability and generalization of bipartite ranking algorithms. In Conference On Learning Theory, pages 32–47, 2005.  Shivani Agarwal, Thore Graepel, Ralf Herbrich, Sariel Har-Peled, and Dan Roth. Generalization bounds for the area under the ROC curve. Journal of Machine Learning Research, 6:393–425, 2005.  Nir Ailon and Mehryar Mohri. An eﬃcient reduction of ranking to classiﬁcation. In Conference On Learning Theory, pages 87–98, 2008.  Mark A. Aizerman, E. M. Braverman, and Lev I. Rozono`er. Theoretical foundations of the potential function method in pattern recognition learning. Automation and Remote Control, 25: 821–837, 1964.  Cyril Allauzen and Mehryar Mohri. N-way composition of weighted ﬁnite-state transducers. In- ternational Journal of Foundations of Computer Science, 20 4 :613–627, 2009.  Cyril Allauzen, Corinna Cortes, and Mehryar Mohri. Large-scale training of SVMs with automata kernels. In International Conference on Implementation and Application of Automata, pages 17– 27, 2010.  Erin L. Allwein, Robert E. Schapire, and Yoram Singer. Reducing multiclass to binary: A unifying approach for margin classiﬁers. Journal of Machine Learning Research, 1:113–141, 2000.  Noga Alon and Joel Spencer. The Probabilistic Method. John Wiley, 1992.  Noga Alon, Shai Ben-David, Nicol`o Cesa-Bianchi, and David Haussler. Scale-sensitive dimensions, uniform convergence, and learnability. Journal of ACM, 44:615–631, July 1997.  Yasemin Altun and Alexander J. Smola. Unifying divergence minimization and statistical inference via convex duality. In Conference On Learning Theory, pages 139–153, 2006.  Galen Andrew and Jianfeng Gao. Scalable training of l1-regularized log-linear models. In Pro- ceedings of ICML, pages 33–40, 2007.  Dana Angluin. On the complexity of minimum inference of regular sets. Information and Control, 39 3 :337–350, 1978.  Dana Angluin. Inference of reversible languages. Journal of the ACM, 29 3 :741–765, 1982.  Dana Angluin and Leslie G. Valiant. Fast probabilistic algorithms for hamiltonian circuits and matchings. J. Comput. Syst. Sci., 18 2 :155–193, 1979.  Martin Anthony and Peter L. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge University Press, 1999.  Nachman Aronszajn. Theory of reproducing kernels. Transactions of the American Mathematical Society, 68 3 :337–404, 1950.  Patrick Assouad. Densit´e et dimension. Annales de l’institut Fourier, 33 3 :233–282, 1983.   462  Bibliography  Kazuoki Azuma. Weighted sums of certain dependent random variables. Tohoku Mathematical Journal, 19 3 :357–367, 1967.  Maria-Florina Balcan, Nikhil Bansal, Alina Beygelzimer, Don Coppersmith, John Langford, and Gregory B. Sorkin. Robust reductions from ranking to classiﬁcation. Machine Learning, 72 1-2 : 139–153, 2008.  Peter L. Bartlett and Shahar Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3, 2002.  Peter L. Bartlett, St´ephane Boucheron, and G´abor Lugosi. Model selection and error estimation. Machine Learning, 48:85–113, September 2002a.  Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. Localized Rademacher complexities. In Conference on Computational Learning Theory, volume 2375, pages 79–97. Springer-Verlag, 2002b.  Amos Beimel, Francesco Bergadano, Nader H. Bshouty, Eyal Kushilevitz, and Stefano Varricchio. Learning functions represented as multiplicity automata. Journal of the ACM, 47:2000, 2000.  Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In Neural Information Processing Systems, 2001.  George Bennett. Probability inequalities for the sum of independent random variables. Journal of the American Statistical Association, 57:33–45, 1962.  Christian Berg, Jens P.R. Christensen, and Paul Ressel. Harmonic Analysis on Semigroups: Theory of Positive Deﬁnite and Related Functions, volume 100. Springer, 1984.  Francesco Bergadano and Stefano Varricchio. Learning behaviors of automata from shortest coun- terexamples. In European Conference on Computational Learning Theory, pages 380–391, 1995.  Adam L. Berger, Stephen Della Pietra, and Vincent J. Della Pietra. A maximum entropy approach to natural language processing. Comp. Linguistics, 22 1 , 1996.  Joseph Berkson. Application of the logistic function to bio-assay. Journal of the American Statistical Association, 39:357–365, 1944.  Sergei Natanovich Bernstein. Sur l’extension du th´eor`eme limite du calcul des probabilit´es aux sommes de quantit´es d´ependantes. Mathematische Annalen, 97:1–59, 1927.  Dimitri P. Bertsekas. Dynamic Programming: Deterministic and Stochastic Models. Prentice- Hall, 1987.  Dmitri P. Bertsekas, Angelica Nedi´c, and Asuman E. Ozdaglar. Convex Analysis and Optimiza- tion. Athena Scientiﬁc, 2003.  Laurence Bisht, Nader H. Bshouty, and Hanna Mazzawi. On optimal learning algorithms for multiplicity automata. In Conference On Learning Theory, pages 184–198, 2006.  Avrim Blum and Yishay Mansour. From external to internal regret. In Conference On Learning Theory, pages 621–636, 2005.  Avrim Blum and Yishay Mansour. Learning, regret minimization, and equilibria. In Noam Nisan, Tim Roughgarden, ´Eva Tardos, and Vijay Vazirani, editors, Algorithmic Game Theory, chapter 4, pages 4–30. Cambridge University Press, 2007.  Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. Learnability and the Vapnik-Chervonenkis dimension. Journal of the ACM, 36 4 :929–965, 1989.  Jonathan Borwein and Qiji Zhu. Techniques of Variational Analysis. Springer, New York, 2005.  Jonathan M. Borwein and Adrian S. Lewis. Convex Analysis and Nonlinear Optimization, Theory and Examples. Springer, 2000.  Bernhard E. Boser, Isabelle M. Guyon, and Vladimir N. Vapnik. A training algorithm for optimal margin classiﬁers. In Conference On Learning Theory, pages 144–152, 1992.   Bibliography  463  Olivier Bousquet and Andr´e Elisseeﬀ. Stability and generalization. Journal of Machine Learning Research, 2:499–526, 2002.  Stephen P. Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.  Lev M. Bregman. The relaxation method of ﬁnding the common point of convex sets and its ap- plication to the solution of problems in convex programming. USSR Computational Mathematics and Mathematical Physics, 7:200–217, 1967.  Leo Breiman. Prediction games and arcing algorithms. Neural Computation, 11:1493–1517, Oc- tober 1999.  Leo Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classiﬁcation and Regression Trees. Wadsworth, 1984.  Nicol`o Cesa-Bianchi. Analysis of two gradient-based algorithms for on-line regression. Journal of Computer System Sciences, 59 3 :392–411, 1999.  Nicol`o Cesa-Bianchi and G´abor Lugosi. Potential-based algorithms in online prediction and game theory. In Conference On Learning Theory, pages 48–64, 2001.  Nicol`o Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games. Cambridge University Press, 2006.  Nicol`o Cesa-Bianchi, Yoav Freund, David Haussler, David P. Helmbold, Robert E. Schapire, and Manfred K. Warmuth. How to use expert advice. Journal of the ACM, 44 3 :427–485, 1997.  Nicol`o Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line learning algorithms. In Neural Information Processing Systems, pages 359–366, 2001.  Nicol`o Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line learning algorithms. IEEE Transactions on Information Theory, 50 9 :2050–2057, 2004.  Nicol`o Cesa-Bianchi, Yishay Mansour, and Gilles Stoltz. Improved second-order bounds for pre- diction with expert advice. In Conference On Learning Theory, pages 217–232, 2005.  Parinya Chalermsook, Bundit Laekhanukit, and Danupon Nanongkai. Pre-reduction graph prod- ucts: Hardnesses of properly learning dfas and approximating edp on dags. In Symposium on Foundations of Computer Science, pages 444–453. IEEE, 2014.  Bernard Chazelle. The Discrepancy Method: Randomness and Complexity. Cambridge University Press, New York, NY, USA, 2000.  Stanley F. Chen and Ronald Rosenfeld. A survey of smoothing techniques for ME models. IEEE Transactions on Speech and Audio Processing, 8 1 , 2000.  Herman Chernoﬀ. A measure of asymptotic eﬃciency for tests of a hypothesis based on the sum of observations. The Annals of Mathematical Statistics, 23 4 :493–507, 12 1952.  Michael Collins, Robert E. Schapire, and Yoram Singer. Logistic regression, Adaboost and Breg- man distances. Machine Learning, 48:253–285, September 2002.  Corinna Cortes and Mehryar Mohri. AUC optimization vs. error rate minimization. In Neural Information Processing Systems, 2003.  Corinna Cortes and Mehryar Mohri. Conﬁdence intervals for the area under the ROC curve. In Neural Information Processing Systems, volume 17, Vancouver, Canada, 2005. MIT Press.  Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine Learning, 20 3 :273–297, 1995.  Corinna Cortes, Patrick Haﬀner, and Mehryar Mohri. Rational kernels: Theory and algorithms. Journal of Machine Learning Research, 5:1035–1062, 2004.  Corinna Cortes, Leonid Kontorovich, and Mehryar Mohri. Learning languages with rational ker- nels. In Conference On Learning Theory, volume 4539 of Lecture Notes in Computer Science, pages 349–364. Springer, Heidelberg, Germany, June 2007a.   464  Bibliography  Corinna Cortes, Mehryar Mohri, and Ashish Rastogi. An alternative ranking problem for search engines. In Workshop on Experimental Algorithms, pages 1–22, 2007b.  Corinna Cortes, Mehryar Mohri, and Jason Weston. A general regression framework for learning string-to-string mappings. In Predicted Structured Data. MIT Press, 2007c.  Corinna Cortes, Mehryar Mohri, Dmitry Pechyony, and Ashish Rastogi. Stability of transductive regression algorithms. In International Conference on Machine Learning, Helsinki, Finland, July 2008a.  Corinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh. Learning sequence kernels. In Pro- ceedings of IEEE International Workshop on Machine Learning for Signal Processing, Canc´un, Mexico, October 2008b.  Corinna Cortes, Yishay Mansour, and Mehryar Mohri. Learning bounds for importance weighting. In Neural Information Processing Systems, Vancouver, Canada, 2010a. MIT Press.  Corinna Cortes, Mehryar Mohri, and Ameet Talwalkar. On the impact of kernel approximation on learning accuracy. In Conference on Artiﬁcial Intelligence and Statistics, 2010b.  Corinna Cortes, Spencer Greenberg, and Mehryar Mohri. Relative deviation learning bounds and generalization with unbounded loss functions. ArXiv 1310.5796, October 2013. URL http:   arxiv.org pdf 1310.5796v4.pdf.  Corinna Cortes, Mehryar Mohri, and Umar Syed. Deep boosting. In International Conference on Machine Learning, pages 1179–1187, 2014.  Corinna Cortes, Vitaly Kuznetsov, Mehryar Mohri, and Umar Syed. Structural Maxent models. In International Conference on Machine Learning, pages 391–399, 2015.  David Cossock and Tong Zhang. Statistical analysis of Bayes optimal subset ranking. Transactions on Information Theory, 54 11 :5140–5154, 2008.  IEEE  Thomas M. Cover and Joy M. Thomas. Elements of Information Theory. Wiley-Interscience, 2006.  Trevor F. Cox and Michael A. A. Cox. Multidimensional Scaling. Chapman & Hall CRC, 2nd edition, 2000.  Koby Crammer and Yoram Singer. Improved output coding for classiﬁcation using continuous relaxation. In Neural Information Processing Systems, 2001.  Koby Crammer and Yoram Singer. On the algorithmic implementation of multiclass kernel-based vector machines. Journal of Machine Learning Research, 2, 2002.  Robert Crites and Andrew Barto. Improving elevator performance using reinforcement learning. In Neural Information Processing Systems, pages 1017–1023. MIT Press, 1996.  Imre Csisz´ar. observations. Studia Scientiarum Mathematicarum Hungarica, 2:299–318, 1967.  Information-type measures of diﬀerence of probability distributions and indirect  Felipe Cucker and Steve Smale. On the mathematical foundations of learning. Bulletin of the American Mathematical Society, 39 1 :1–49, 2001.  J. N. Darroch and D. Ratcliﬀ. Generalized iterative scaling for log-linear models. Annals of Mathematical Statistics, pages 1470–1480, 1972.  Sanjoy Dasgupta and Anupam Gupta. An elementary proof of a theorem of Johnson and Linden- strauss. Random Structures and Algorithms, 22 1 :60–65, 2003.  Colin De la Higuera. Grammatical inference: University Press, 2010.  learning automata and grammars. Cambridge  Giulia DeSalvo, Mehryar Mohri, and Umar Syed. Learning with deep cascades. In Conference on Algorithmic Learning Theory, pages 254–269, 2015.   Bibliography  465  Luc Devroye and G´abor Lugosi. Lower bounds in pattern recognition and learning. Pattern Recognition, 28 7 :1011–1018, 1995.  Luc Devroye and T. J. Wagner. Distribution-free inequalities for the deleted and holdout error estimates. IEEE Transactions on Information Theory, 25 2 :202–207, 1979a.  Luc Devroye and T. J. Wagner. Distribution-free performance bounds for potential function rules. IEEE Transactions on Information Theory, 25 5 :601–604, 1979b.  Thomas G. Dietterich. An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization. Machine Learning, 40 2 :139–157, 2000.  Thomas G. Dietterich and Ghulum Bakiri. Solving multiclass learning problems via error- correcting output codes. Journal of Artiﬁcial Intelligence Research, 2:263–286, 1995.  Harris Drucker and Corinna Cortes. Boosting decision trees. In Neural Information Processing Systems, pages 479–485, 1995.  Harris Drucker, Robert E. Schapire, and Patrice Simard. Boosting performance in neural networks. International Journal of Pattern Recognition and Artiﬁcial Intelligence, 7 4 :705–719, 1993.  Miroslav Dud´ık, Steven J. Phillips, and Robert E. Schapire. Maximum entropy density estimation with generalized regularization and an application to species distribution modeling. Journal of Machine Learning Research, 8, 2007.  Richard M. Dudley. The sizes of compact subsets of Hilbert space and continuity of Gaussian processes. Journal of Functional Analysis, 1 3 :290–330, 1967.  Richard M. Dudley. A course on empirical processes. Lecture Notes in Mathematics, 1097:2 – 142, 1984.  Richard M. Dudley. Universal Donsker classes and metric entropy. Annals of Probability, 14 4 : 1306–1326, 1987.  Richard M. Dudley. Uniform Central Limit Theorems. Cambridge University Press, 1999.  Nigel Duﬀy and David P. Helmbold. Potential boosters? Systems, pages 258–264, 1999.  In Neural Information Processing  Aryeh Dvoretzky. On stochastic approximation. In Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability, pages 39–55, 1956.  Cynthia Dwork, Ravi Kumar, Moni Naor, and D. Sivakumar. Rank aggregation methods for the web. In International World Wide Web Conference, pages 613–622, 2001.  Bradley Efron, Trevor Hastie, Iain Johnstone, and Robert Tibshirani. Least angle regression. Annals of Statistics, 32 2 :407–499, 2004.  James P. Egan. Signal Detection Theory and ROC Analysis. Academic Press, 1975.  Andrzej Ehrenfeucht, David Haussler, Michael J. Kearns, and Leslie G. Valiant. A general lower bound on the number of examples needed for learning. In Conference On Learning Theory, pages 139–154, 1988.  Jane Elith, Steven J. Phillips, Trevor Hastie, Miroslav Dud´ık, Yung En Chee, and Colin J. Yates. A statistical explanation of MaxEnt for ecologists. Diversity and Distributions, 1, 2011.  Eyal Even-Dar and Yishay Mansour. Learning rates for q-learning. Machine Learning, 5:1–25, 2003.  Dean P. Foster and Rakesh V. Vohra. Calibrated learning and correlated equilibrium. Games and Economic Behavior, 21:40–55, 1997.  Dean P. Foster and Rakesh V. Vohra. Asymptotic calibration. Biometrika, pages 379–390, 1998.  Dean P. Foster and Rakesh V. Vohra. Regret in the on-line decision problem. Games and Economic Behavior, 29 1-2 :7–35, 1999.   466  Bibliography  Yoav Freund. Boosting a weak learning algorithm by majority. In Information and Computation, pages 202–216. Morgan Kaufmann Publishers Inc., 1990.  Yoav Freund. Boosting a weak learning algorithm by majority. Information and Computation, 121:256–285, September 1995.  Yoav Freund and Robert E. Schapire. Game theory, on-line prediction and boosting. In Conference On Learning Theory, pages 325–332, 1996.  Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer System Sciences, 55 1 :119–139, 1997.  Yoav Freund and Robert E. Schapire. Large margin classiﬁcation using the perceptron algorithm. Machine Learning, 37:277–296, 1999a.  Yoav Freund and Robert E. Schapire. Adaptive game playing using multiplicative weights. Games and Economic Behavior, 29 1-2 :79–103, October 1999b.  Yoav Freund, Michael J. Kearns, Dana Ron, Ronitt Rubinfeld, Robert E. Schapire, and Linda Sellie. Eﬃcient learning of typical ﬁnite automata from random walks. In Proceedings the ACM Symposium on Theory of Computing, pages 315–324, 1993.  Yoav Freund, Raj D. Iyer, Robert E. Schapire, and Yoram Singer. An eﬃcient boosting algorithm for combining preferences. Journal of Machine Learning Research, 4, 2003.  Jerome H. Friedman. Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29:1189–1232, 2000.  Jerome H. Friedman, Trevor Hastie, and Robert Tibshirani. Additive logistic regression: A sta- tistical view of boosting. Annals of Statistics, 38 2 , 2000.  E. Mark Gold. Language identiﬁcation in the limit. Information and Control, 10 5 :447–474, 1967.  E. Mark Gold. Complexity of automaton identiﬁcation from given data. Information and Control, 37 3 :302–320, 1978.  Joshua Goodman. Exponential priors for maximum entropy models. NAACL, pages 305–312, 2004.  In Proceedings of HLT-  David M. Green and John A Swets. Signal Detection Theory and Psychophysics. Wiley, 1966.  Michelangelo Grigni, Vincent Mirelli, and Christos H Papadimitriou. On the diﬃculty of designing good classiﬁers. SIAM Journal on Computing, 30 1 :318–323, 2000.  Adam J. Grove and Dale Schuurmans. Boosting in the limit: Maximizing the margin of learned ensembles. In Proceedings of the Fifteenth National Conference on Artiﬁcial Intelligence, pages 692–699, 1998.  Uﬀe Haagerup. The best constants in the Khintchine inequality. Studia Math, 70 3 :231–283, 1982.  Torben Hagerup and Christine R¨ub. A guided tour of chernoﬀ bounds. Information Processing Letters, 33 6 :305–308, 1990.  Jihun Ham, Daniel D. Lee, Sebastian Mika, and Bernhard Sch¨olkopf. A kernel view of the dimen- sionality reduction of manifolds. In International Conference on Machine Learning, 2004.  James A. Hanley and Barbara J. McNeil. The meaning and use of the area under a receiver operating characteristic  ROC  curve. Radiology, 143:29–36, 1982.  James Hannan. Approximation to Bayes risk in repeated plays. Contributions to the Theory of Games, 3:97–139, 1957.  Sergiu Hart and Andreu M. Mas-Colell. A simple adaptive procedure leading to correlated equi- librium. Econometrica, 68 5 :1127–1150, 2000.   Bibliography  467  David Haussler. Decision theoretic generalizations of the PAC model for neural net and other learning applications. Information and Computation, 100 1 :78–150, 1992.  David Haussler. Sphere packing numbers for subsets of the boolean n-cube with bounded Vapnik- Chervonenkis dimension. Journal of Combinatorial Theory, Series A, 69 2 :217 – 232, 1995.  David Haussler. Convolution Kernels on Discrete Structures. Technical Report UCSC-CRL-99-10, University of California at Santa Cruz, 1999. David Haussler, Nick Littlestone, and Manfred K. Warmuth. Predicting {0,1}-functions on ran- domly drawn points  extended abstract . In Symposium on Foundations of Computer Science, pages 100–109, 1988.  Ralf Herbrich, Thore Graepel, and Klaus Obermayer. Large margin rank boundaries for ordinal regression. In Advances in Large Margin Classiﬁers, pages 115–132. MIT Press, Cambridge, MA, 2000.  Wassily Hoeﬀding. Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association, 58 301 :13–30, 1963.  Arthur E. Hoerl and Robert W. Kennard. Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 12 1 :55–67, 1970.  Klaus-Uwe H¨oﬀgen, Hans-Ulrich Simon, and Kevin S. Van Horn. Robust trainability of single neurons. Journal of Computer and Systems Sciences, 50 1 :114–125, 1995.  John E. Hopcroft and Jeﬀrey D. Ullman. Computation. Addison-Wesley, 1979.  Introduction to Automata Theory, Languages and  Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S. Sathiya Keerthi, and S. Sundararajan. A dual coordinate descent method for large-scale linear SVM. In International Conference on Machine Learning, pages 408–415, 2008.  Tommi Jaakkola, Michael I. Jordan, and Satinder P. Singh. Convergence of stochastic iterative dynamic programming algorithms. Neural Computation, 6:1185–1201, 1994.  Kalervo J¨arvelin and Jaana Kek¨al¨ainen. documents. In ACM Special Interest Group on Information Retrieval, pages 41–48, 2000.  IR evaluation methods for retrieving highly relevant  Information theory and statistical mechanics. Physical Review, 106 4 :620–630,  E. T. Jaynes. 1957.  E. T. Jaynes. Papers on probability, statistics, and statistical physics. Synthese library. D. Reidel Pub. Co., 1983.  Thorsten Joachims. Optimizing search engines using clickthrough data. In Knowledge and Dis- covery and Data Mining, pages 133–142, 2002.  William B. Johnson and Joram Lindenstrauss. Extensions of Lipschitz mappings into a Hilbert space. Contemporary Mathematics, 26:189–206, 1984.  Jean-Pierre Kahane. Sur les sommes vectorielles  cid:80  ±un. Comptes Rendus Hebdomadaires des  S’eances de l’Acad´emie des Sciences, Paris, 259:2577–2580, 1964.  Adam Kalai and Santosh Vempala. Eﬃcient algorithms for online decision problems. In Conference On Learning Theory, pages 26–40, 2003.  William Karush. Minima of Functions of Several Variables with Inequalities as Side Constraints. Master’s thesis, Department of Mathematics, University of Chicago, 1939.  Jun’ichi Kazama and Jun’ichi Tsujii. Evaluation and extension of maximum entropy models with inequality constraints. In Proceedings of EMNLP, pages 137–144, 2003.  Michael J. Kearns and Yishay Mansour. A fast, bottom-up decision tree pruning algorithm with near-optimal generalization. In International Conference on Machine Learning, pages 269–277, 1998.   468  Bibliography  Michael J. Kearns and Yishay Mansour. On the boosting ability of top-down decision tree learning algorithms. Journal of Computer and System Sciences, 58 1 :109–128, 1999.  Michael J. Kearns and Dana Ron. Algorithmic stability and sanity-check bounds for leave-one-out cross-validation. Neural Computation, 11 6 :1427–1453, 1999.  Michael J. Kearns and Robert E. Schapire. Eﬃcient distribution-free learning of probabilistic concepts  extended abstract . In Symposium on Foundations of Computer Science, pages 382– 391, 1990.  Michael J. Kearns and Leslie G. Valiant. Cryptographic limitations on learning boolean formulae and ﬁnite automata. Technical Report 14, Harvard University, 1988.  Michael J. Kearns and Leslie G. Valiant. Cryptographic limitations on learning boolean formulae and ﬁnite automata. Journal of ACM, 41 1 :67–95, 1994.  Michael J. Kearns and Umesh V. Vazirani. An Introduction to Computational Learning Theory. MIT Press, 1994. Aleksandr Khintchine. ¨Uber dyadische br¨uche. Mathematische Zeitschrift, 18 1 :109–116, 1923.  Jack Kiefer and Jacob Wolfowitz. Stochastic estimation of the maximum of a regression function. Annals of Mathematical Statistics, 23 1 :462–466, 1952.  George Kimeldorf and Grace Wahba. Some results on tchebycheﬃan spline functions. Journal of Mathematical Analysis and Applications, 33 1 :82–95, 1971.  Jyrki Kivinen and Manfred K. Warmuth. Boosting as entropy projection. Learning Theory, pages 134–144, 1999.  In Conference On  Vladimir Koltchinskii. Rademacher penalties and structural risk minimization. IEEE Transactions on Information Theory, 47 5 :1902–1914, 2001.  Vladimir Koltchinskii and Dmitry Panchenko. Rademacher processes and bounding the risk of function learning. In High Dimensional Probability II, pages 443–459. Birkh¨auser, 2000.  Vladmir Koltchinskii and Dmitry Panchenko. Empirical margin distributions and bounding the generalization error of combined classiﬁers. Annals of Statistics, 30, 2002.  Leonid Kontorovich, Corinna Cortes, and Mehryar Mohri. Learning linearly separable languages. In Algorithmic Learning Theory, pages 288–303, 2006.  Leonid Kontorovich, Corinna Cortes, and Mehryar Mohri. Kernel methods for learning languages. Theoretical Computer Science, 405:223–236, 2008.  Harold W. Kuhn and Albert W. Tucker. Nonlinear programming. In 2nd Berkeley Symposium, pages 481–492, Berkeley, 1951. University of California Press.  Solomon Kullback. A lower bound for discrimination information in terms of variation. IEEE Transactions on Information Theory, 13 1 :126–127, 1967.  Solomon Kullback and Richard A. Leibler. On information and suﬃciency. Ann. Math. Statist., 22 1 :79–86, 1951.  Harold Kushner. Stochastic approximation: a survey. Wiley Interdisciplinary Reviews Computa- tional Statistics, 2 1 :87–96, 2010.  Harold J. Kushner and D. S. Clark. Stochastic Approximation Methods for Constrained and Unconstrained Systems, volume 26 of Applied Mathematical Sciences. Springer-Verlag, 1978.  Vitaly Kuznetsov, Mehryar Mohri, and Umar Syed. Multi-class deep boosting. In Neural Infor- mation Processing Systems, 2014.  John Laﬀerty. Additive models, boosting, and inference for generalized divergences. In Conference On Learning Theory, pages 125–133, 1999.   Bibliography  469  John D. Laﬀerty, Stephen Della Pietra, and Vincent J. Della Pietra. Statistical learning algorithms based on bregman distances. In Proceedings of the Canadian Workshop on Information Theory, 1997.  John D. Laﬀerty, Andrew McCallum, and Fernando C. N. Pereira. Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data. In International Conference on Machine Learning, pages 282–289, 2001.  Rafa cid:32 l Lata cid:32 la and Krzysztof Oleszkiewicz. On the best constant in the khintchine-kahane inequality. Studia Math, 109 1 :101–104, 1994.  Guy Lebanon and John D. Laﬀerty. Boosting and maximum likelihood for exponential models. In Neural Information Processing Systems, pages 447–454, 2001.  Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: Isoperimetry and Processes. Springer, New York, 1991.  Ehud Lehrer. A wide range no-regret theorem. Games and Economic Behavior, 42 1 :101–115, 2003.  Nick Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. Machine Learning, 2 4 :285–318, 1987.  Nick Littlestone. From on-line to batch learning. 269–284, 1989.  In Conference On Learning Theory, pages  Nick Littlestone and Manfred K. Warmuth. The weighted majority algorithm. In Symposium on Foundations of Computer Science, pages 256–261, 1989.  Nick Littlestone and Manfred K. Warmuth. The weighted majority algorithm. Information and Computation, 108 2 :212–261, 1994.  Michael L. Littman. Algorithms for Sequential Decision Making. PhD thesis, Brown University, 1996.  Philip M. Long and Rocco A. Servedio. Random classiﬁcation noise defeats all convex potential boosters. Machine Learning, 78:287–304, March 2010.  M. Lothaire. Combinatorics on Words. Cambridge University Press, 1982.  M. Lothaire. Mots. Herm`es, 1990.  M. Lothaire. Applied Combinatorics on Words. Cambridge University Press, 2005.  Robert Malouf. A comparison of algorithms for maximum entropy parameter estimation. Proceedings of CoNLL-2002, pages 49–55, 2002.  In  Christopher D. Manning and Dan Klein. Optimization, maxent models, and conditional estimation without magic. In Proceedings of HLT-NAACL, 2003.  Yishay Mansour and David A. McAllester. Boosting with multi-way branching in decision trees. In Neural Information Processing Systems, pages 300–306, 1999.  Yishay Mansour and David A. McAllester. Generalization bounds for decision trees. In Conference On Learning Theory, pages 69–74, 2000.  Llew Mason, Jonathan Baxter, Peter L. Bartlett, and Marcus R. Frean. Boosting algorithms as gradient descent. In Neural Information Processing Systems, pages 512–518, 1999.  Pascal Massart. Some applications of concentration inequalities to statistics. Annales de la Facult´e des Sciences de Toulouse, IX:245–303, 2000.  Peter McCullagh. Regression models for ordinal data. Journal of the Royal Statistical Society B, 42 2 , 1980.  Peter McCullagh and John A. Nelder. Generalized Linear Models. Chapman & Hall, 1983.   470  Bibliography  Colin McDiarmid. On the method of bounded diﬀerences. Surveys in Combinatorics, 141 1 : 148–188, 1989.  Ron Meir and Gunnar R¨atsch. Advanced lectures on machine learning, machine learning summer school, canberra, australia. In Machine Learning Summer School, pages 118–183, 2002.  Ron Meir and Gunnar R¨atsch. An Introduction to Boosting and Leveraging, pages 118–183. Springer, 2003.  James Mercer. Functions of positive and negative type, and their connection with the theory of in- tegral equations. Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character, 209 441-458 :415, 1909.  Sebastian Mika, Bernhard Scholkopf, Alex J. Smola, Klaus-Robert Muller, Matthias Scholz, and Gunnar Ratsch. Kernel PCA and de-noising in feature spaces. In Neural Information Processing Systems, pages 536–542, 1999.  Marvin Minsky and Seymour Papert. Perceptrons: An Introduction to Computational Geometry. MIT Press, 1969.  Mehryar Mohri. Semiring frameworks and algorithms for shortest-distance problems. Journal of Automata, Languages and Combinatorics, 7 3 :321–350, 2002.  Mehryar Mohri. Weighted automata algorithms. In Manfred Droste, Werner Kuich, and Heiko Vogler, editors, Handbook of Weighted Automata, pages 213–254. Springer, 2009.  Mehryar Mohri and Afshin Rostamizadeh. Stability bounds for stationary ϕ-mixing and β-mixing processes. Journal of Machine Learning Research, 11:789–814, 2010.  Mehryar Mohri and Afshin Rostamizadeh. Perceptron mistake bounds. ArXiv 1305.0208, March 2013.  Mehryar Mohri, Fernando Pereira, and Michael D. Riley. Weighted automata in text and speech processing. European Conference on Artiﬁcial Intelligence, Workshop on Extended Finite State Models of Language, 2005.  Jorge Nocedal. Updating quasi-newton matrices with limited storage. Mathematics of Computa- tion, 35 151 :773–782, 1980.  Albert B.J. Novikoﬀ. On convergence proofs on perceptrons. In Proceedings of the Symposium on the Mathematical Theory of Automata, volume 12, pages 615–622, 1962.  Jos´e Oncina, Pedro Garc´ıa, and Enrique Vidal. Learning subsequential transducers for pattern recognition interpretation tasks. IEEE Transactions on Pattern Analysis and Machine Intelli- gence, 15 5 :448–458, 1993.  Karl Pearson. On lines and planes of closest ﬁt to systems of points in space. Philosophical Magazine, 2 6 :559–572, 1901.  Fernando C. N. Pereira and Michael D. Riley. Speech recognition by composition of weighted ﬁnite automata. In Finite-State Language Processing, pages 431–453. MIT Press, 1997.  Dominique Perrin. Finite automata. In J. Van Leuwen, editor, Handbook of Theoretical Computer Science, Volume B: Formal Models and Semantics, pages 1–57. Elsevier, 1990.  Steven J. Phillips, Miroslav Dud´ık, and Robert E. Schapire. A maximum entropy approach to species distribution modeling. In Proceedings of ICML, 2004.  Steven J. Phillips, Robet P. Anderson, and Robert E. Schapire. Maximum entropy modeling of species geographic distributions. Ecological Modelling, 190:231–259, 2006.  Stephen Della Pietra, Vincent J. Della Pietra, and John D. Laﬀerty. Inducing features of random ﬁelds. IEEE Trans. Pattern Anal. Mach. Intell., 19 4 , 1997.  Mark Semenovich Pinsker. Information and Information Stability of Random Variables and Pro- cesses. Holden-Day, 1964.   Bibliography  471  Leonard Pitt and Manfred K. Warmuth. The minimum consistent DFA problem cannot be ap- proximated within any polynomial. Journal of the ACM, 40 1 :95–142, 1993.  John C. Platt. Fast training of support vector machines using sequential minimal optimization. In Advances in Kernel Methods, pages 185–208. MIT Press, 1999.  David Pollard. Convergence of Stochastic Processess. Springer, 1984.  David Pollard. Asymptotics via empirical processes. Statistical Science, 4 4 :341 – 366, 1989.  Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc., 1994.  J. Ross Quinlan. Induction of decision trees. Machine Learning, 1 1 :81–106, 1986.  J. Ross Quinlan. C4.5: Programs for Machine Learning. Morgan Kaufmann, 1993.  Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. Information Processing Systems, pages 1177–1184, 2007.  In Neural  Adwait Ratnaparkhi. Maximum entropy models for natural language processing. In Encyclopedia of Machine Learning, pages 647–651. Springer, 2010.  Gunnar R¨atsch and Manfred K. Warmuth. Maximizing the margin with boosting. In Conference On Learning Theory, pages 334–350, 2002.  Gunnar R¨atsch, Sebastian Mika, and Manfred K. Warmuth. On the convergence of leveraging. In NIPS, pages 487–494, 2001.  Gunnar R¨atsch, Takashi Onoda, and Klaus-Robert M¨uller. Soft margins for AdaBoost. Machine Learning, 42:287–320, March 2001.  Mark D. Reid and Robert C. Williamson. Generalised pinsker inequalities. In 22nd Conference on Learning Theory  COLT 2009 , 2009.  Alfr´ed R´enyi. On measures of entropy and information. In Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics, pages 547–561. University of California Press, 1961.  Ryan Rifkin and Aldebaro Klautau. In defense of one-vs-all classiﬁcation. Journal of Machine Learning Research, 5:101–141, 2004.  Ryan M. Rifkin. Everything Old Is New Again: A Fresh Look at Historical Approaches in Machine Learning. PhD thesis, Massachusetts Institute of Technology, 2002.  H. Robbins and S. Monro. A stochastic approximation method. Annals of Mathematical Statistics, 22 3 :400–407, 1951.  R. Tyrrell Rockafellar. Convex analysis. Princeton University Press, 1997.  W.H. Rogers and T. J. Wagner. A ﬁnite sample distribution-free performance bound for local discrimination rules. Annals of Statistics, 6 3 :506–514, 1978.  Dana Ron, Yoram Singer, and Naftali Tishby. On the learnability and usage of acyclic probabilistic ﬁnite automata. In Journal of Computer and System Sciences, pages 31–40, 1995.  Frank Rosenblatt. The perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review, 65 6 :386, 1958.  Ronald Rosenfeld. A maximum entropy approach to adaptive statistical language modelling. Computer Speech & Language, 10 3 :187–228, 1996.  Sam T. Roweis and Lawrence K. Saul. Nonlinear dimensionality reduction by locally linear em- bedding. Science, 290 5500 :2323, 2000.  Cynthia Rudin, Ingrid Daubechies, and Robert E. Schapire. The dynamics of AdaBoost: Cyclic behavior and convergence of margins. Journal of Machine Learning Research, 5:1557–1595, 2004.   472  Bibliography  Cynthia Rudin, Corinna Cortes, Mehryar Mohri, and Robert E. Schapire. Margin-based ranking meets boosting in the middle. In Conference On Learning Theory, 2005.  Walter Rudin. Fourier analysis on groups. Number 12 in Interscience tracts in pure and applied mathematics. John Wiley & Sons, 1990.  I. N. Sanov. On the probability of large deviations of random variables. Matematicheskii Sbornik, 42 84 :11–44, 1957.  Norbert Sauer. On the density of families of sets. Journal of Combinatorial Theory, Series A, 13  1 :145–147, 1972.  Craig Saunders, Alexander Gammerman, and Volodya Vovk. Ridge regression learning algorithm in dual variables. In International Conference on Machine Learning, volume 521, 1998.  Robert E. Schapire. The strength of weak learnability. Machine Learning, 5:197–227, July 1990.  Robert E. Schapire. The boosting approach to machine learning: An overview. Estimation and Classiﬁcation, pages 149–172. Springer, 2003.  In Nonlinear  Robert E. Schapire and Yoav Freund. Boosting: Foundations and Algorithms. The MIT Press, 2012.  Robert E. Schapire and Yoram Singer. predictions. Machine Learning, 37 3 :297–336, 1999.  Improved boosting algorithms using conﬁdence-rated  Robert E. Schapire and Yoram Singer. Boostexter: A boosting-based system for text categoriza- tion. Machine Learning, 39 2-3 :135–168, 2000.  Robert E. Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee. Boosting the margin: A new explanation for the eﬀectiveness of voting methods. In International Conference on Machine Learning, pages 322–330, 1997.  Leopold Schmetterer. Stochastic approximation. In Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, pages 587–609, 1960.  Isaac J. Schoenberg. Metric spaces and positive deﬁnite functions. Transactions of the American Mathematical Society, 44 3 :522–536, 1938.  Bernhard Sch¨olkopf and Alex Smola. Learning with Kernels. MIT Press, 2002.  Bernhard Sch¨olkopf, Ralf Herbrich, Alex J. Smola, and Robert Williamson. A generalized repre- senter theorem. Technical Report 2000-81, Neuro-COLT, 2000.  Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnability and stability in the general learning setting. In Conference On Learning Theory, 2009.  Claude E. Shannon. A mathematical theory of communication. Bell System Technical Journal, 27:379423, 1948.  John Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press, 2004.  John Shawe-Taylor, Peter L. Bartlett, Robert C. Williamson, and Martin Anthony. Structural risk minimization over data-dependent hierarchies. IEEE Transactions on Information Theory, 44 5 :1926–1940, 1998.  Saharon Shelah. A combinatorial problem; stability and order for models and theories in inﬁnitary languages. Paciﬁc Journal of Mathematics, 41 1 , 1972.  Satinder P. Singh. Learning to Solve Markovian Decision Processes. PhD thesis, University of Massachusetts, 1993.  Satinder P. Singh and Dimitri Bertsekas. Reinforcement learning for dynamic channel allocation in cellular telephone systems. In Neural Information Processing Systems, pages 974–980. MIT Press, 1997.   Bibliography  473  Maurice Sion. On general minimax theorems. Paciﬁc Journal of Mathematics, 8 1 :171–176, 1958.  Eric V. Slud. Distribution inequalities for the binomial law. Annals of Probability, 5 3 :404–412, 1977.  Bharath Sriperumbudur and Zolt´an Szab´o. Optimal rates for random fourier features. In Neural Information Processing Systems, pages 1144–1152, 2015.  Gilles Stoltz and G´abor Lugosi. Internal regret in on-line portfolio selection. In Conference On Learning Theory, pages 403–417, 2003.  Rich Sutton. Temporal Credit Assignment in Reinforcement Learning. PhD thesis, University of Massachusetts, 1984.  Richard S. Sutton and Andrew G. Barto. Reinforcement Learning : An Introduction. MIT Press, 1998.  S.J. Szarek. On the best constants in the Khintchin inequality. Studia Math, 58 2 :197–208, 1976.  Csaba Szepesv´ari. Algorithms for Reinforcement Learning. Synthesis Lectures on Artiﬁcial Intel- ligence and Machine Learning. Morgan & Claypool, 2010.  Eiji Takimoto and Manfred K. Warmuth. Path kernels and multiplicative updates. In Conference On Learning Theory, pages 74–89, 2002.  Benjamin Taskar, Carlos Guestrin, and Daphne Koller. Max-margin Markov networks. In Neural Information Processing Systems, 2003.  Robert F. Tate. On a double inequality of the normal distribution. The Annals of Mathematical Statistics, 1:132–134, 1953.  Joshua Tenenbaum, Vin de Silva, and John C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290 5500 :2319–2323, 2000.  Gerald Tesauro. Temporal diﬀerence learning and TD-gammon. Communications of the ACM, 38:58–68, March 1995.  Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B, 58 1 :267–288, 1996.  B. Tomaszewski. Two remarks on the Khintchine-Kahane inequality. maticum, volume 46, 1982.  In Colloquium Mathe-  Boris Trakhtenbrot and Janis M. Barzdin. Finite Automata: Behavior and Synthesis. North- Holland, 1973.  John N. Tsitsiklis. Asynchronous stochastic approximation and q-learning. In Machine Learning, volume 16, pages 185–202, 1994.  Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun. Large mar- gin methods for structured and interdependent output variables. Journal of Machine Learning Research, 6:1453–1484, 2005.  Leslie G. Valiant. A theory of the learnable. Communications of the ACM, 27 11 :1134–1142, 1984.  Vladimir N. Vapnik. Statistical Learning Theory. Wiley-Interscience, 1998.  Vladimir N. Vapnik. The Nature of Statistical Learning Theory. Springer-Verlag, 2000.  Vladimir N. Vapnik. Estimation of Dependences Based on Empirical Data. Springer-Verlag, 2006.  Vladimir N. Vapnik and Alexey Chervonenkis. A note on one class of perceptrons. Automation and Remote Control, 25, 1964.  Vladimir N. Vapnik and Alexey Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and Its Applications, 16:264, 1971.   474  Bibliography  Vladimir N. Vapnik and Alexey Chervonenkis. Theory of Pattern Recognition. Nauka, 1974.  Santosh S. Vempala. The random projection method. In DIMACS Series in Discrete Mathematics and Theoretical Computer Science, volume 65. American Mathematical Society, 2004.  Pierre Franois Verhulst. Notice sur la loi que la population suit dans son accroissement. Corre- spondance math´ematique et physique, 10:113–121, 1838.  Pierre Franois Verhulst. Recherches math´ematiques sur la loi d’accroissement de la population. Nouveaux M´emoires de l’Acad´emie Royale des Sciences et Belles-Lettres de Bruxelles, 18:1–42, 1845.  Mathukumalli Vidyasagar. A Theory of Learning and Generalization: With Applications to Neural Networks and Control Systems. Springer-Verlag, 1997.  Sethu Vijayakumar and Si Wu. Sequential support vector classiﬁers and regression. International Conference on Soft Computing, 1999.  John von Neumann. Zur Theorie der Gesellschaftsspiele. Mathematische Annalen, 100 1 :295–320, 1928.  Vladimir G. Vovk. Aggregating strategies. In Conference On Learning Theory, pages 371–386, 1990.  Grace Wahba. Spline Models for Observational Data, volume 59 of CBMS-NSF Regional Con- ference Series in Applied Mathematics. Society for Industrial and Applied Mathematics, 1990.  Christopher J. C. H. Watkins. Learning from Delayed Rewards. PhD thesis, Cambridge University, 1989.  Christopher J. C. H. Watkins. Dynamic alignment kernels. Technical Report CSD-TR-98-11, Royal Holloway, University of London, 1999.  Christopher J. C. H. Watkins and Peter Dayan. Q-learning. Machine Learning, 8 3-4 :279–292, 1992.  Andr´e Weil. L’int´egration dans les groupes topologiques et ses applications, volume 1145. Hermann Paris, 1965.  Kilian Q. Weinberger and Lawrence K. Saul. An introduction to nonlinear dimensionality reduc- tion by maximum variance unfolding. In Conference on Artiﬁcial Intelligence, 2006.  Jason Weston and Chris Watkins. Support vector machines for multi-class pattern recognition. European Symposium on Artiﬁcial Neural Networks, 4 6 , 1999.  Bernard Widrow and Marcian E. Hoﬀ. Adaptive switching circuits. Neurocomputing: Foundations of Research, 1988.  Peter M. Williams. Bayesian regularisation and pruning using a Laplace prior. Neural Computa- tion, 7:117–143, 1994.  Huan Xu, Shie Mannor, and Constantine Caramanis. Sparse algorithms are not stable: A no-free- lunch theorem. In Conference on Communication, Control, and Computing, pages 1299–1303, 2008.  Yinyu Ye. The simplex and policy-iteration methods are strongly polynomial for the markov decision problem with a ﬁxed discount rate. Mathematics of Operations Research, 36 4 :593–603, 2011.  Tong Zhang. Statistical behavior and consistency of classiﬁcation methods based on convex risk minimization. Annals of Statistics, 32:56–134, 2003a.  Tong Zhang. Sequential greedy approximation for certain convex optimization problems. IEEE Trans. Inf. Theor., 49 3 :682–691, 2003b.  Martin Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In International Conference on Machine Learning, pages 928–936, 2003.   Index  L1-geometric margin, see margin L1-margin, see margin L1-regularized  AdaBoost, 165 logistic regression, 325  β-contracting, 388 β-stable, 334, 338, 340–342  uniformly, 334   cid:15 -greedy policy, 401  cid:15 -insensitive loss, 282  cid:15 -transition, 361 γ-fat-dimension, 274, see fat-shattering  dimension  γ-shattered, 274, see fat-shattered log-linear model, 321, 326 ρ-margin loss function, see margin σ-admissible, 337, 338, 340–342 σ-algebra, 429 k-CNF formula, 18, 19 k-deterministic, 377 k-reversible, 377 k-term DNF formula, 18 n-fold cross-validation, 71 n-way composition, 128, 136 pth-moment, 434  absolutely continuous, 429 accepted, 28, 361 accepting path, 123, 361 access string, 364–368 accuracy, 8, 11, 17, 23, 46, 148–150, 154,  167, 169, 172, 244, 283  action, 7, 163, 164, 183, 205, 240, 379–383,  387–390, 393, 398, 399, 401–404  greedy, 401, 402 policy, see policy random, 401  active learning, 7, 362 acyclic, 361 AdaBoost, 145  L1-regularized, 165  AdaBoost’s weak learning condition, 162 AdaBoost.MH, 222, 223, 236, 237 AdaBoost.MR, 222, 236, 238 adaptive boosting, 150 adversarial, 177, 178, 180, 204, 260  argument, 180 assumption, 178 choice, 260 scenario, 177  advice, 178 aﬃne, 421 agent, 379 aggregated algorithms, 213, 221 algebraic transductions, 127 algorithm  dependent, 333 deterministic, 183, 258–260, 264 learning, 1, 4–6, 9, 19, 20, 23, 24, 27, 43, 46, 47, 55, 57, 71, 80, 85, 98, 146, 148, 150, 168, 173, 179, 186, 202, 252, 257, 333, 334, 343, 362 oﬀ-policy, 401, 402 on-policy, 402 randomized, 186, 209, 239, 259, 260, 264  pairwise ranking, 245, 255, 256  algorithmic stability, see stability   476  Index  approximately correct, 11 approximation error, 61–64 area under the curve, see AUC AUC, 239, 255, 256, 264, 265 automaton  k-reversible, 377 deterministic, 360, see also DFA, 361, 362 ﬁnite, 125, 129, 130, 360, 361, 370, 375, 377 learning, 359 preﬁx-tree, 370, 371, 373 reverse deterministic, 374 reversible, 370, 371, 373, 374  Azuma’s inequality, 202, 442, 445  average  noise, 23 precision, 263 regret, 186  base  Bayes  classiﬁer set, 146 classiﬁers, 146 rankers, 244  classiﬁer, 22, 47, 61, 74, 75, 78, 140 error, 22, 23, 28, 61, 67, 259 formula, 431 hypothesis, 22 scoring function, 74  Bellman  equations, 385–387, 389, 390, 392  Bennett’s inequality, 447 Bernstein’s inequality, 438, 440, 447 bias, 46, 71, 296, 446, 450 bigram, 128, 129  gappy, 128, 129 kernel, 128, 129  binary  classiﬁcation, 9 classiﬁer, 79 decision tree, 224 entropy, 450 entropy function, 449 space partition  BSP  trees, 225 binomial distribution, 430, 440, 448  bipartite ranking, 251 Boltzmann exploration, 401 boosted, 168 boosting, 145–149, 152, 154, 155, 159, 160, 163, 165, 167–172, 174, 175, 221–224, 236, 237, 239, 244–246, 251, 291, 298, 320, 330  by ﬁltering, 168 by majority, 168 multi-class, 213, see also AdaBoost.MH, see also AdaBoost.MR, 237 round, 147, 148, 171 trees, 291  Bregman divergence, 169, 295, 307, 313,  331, 337, 453–456 generalized, 337, 338  calibration problem, 229, 230 categorical question, 224 Cauchy-Schwarz inequality, 53, 97, 98,  112, 118, 197, 309, 310, 339, 341, 409, 410, 433  chain rule, 431 Chebyshev’s inequality, 433, 447 Chernoﬀ  bound, 28, 45 bounding technique, 437, 438 multiplicative bounds, 439, 445  chi-squared distribution, 430 Cholesky decomposition, 115, 413  partial, 280, 285  classiﬁcation, 3, 259  binary, 4, 33, 34, 61, 74, 79, 102, 159, 173, 213, 228–231, 239, 244, 252, 257, 264, 271, 281, 325, 330, 331 document, 2, 3, 106, 215 image, 3, 140 linear, 79, 105, 177, 190, 198 multi-class, xiii, 168, 213–215, 217, 219–222, 224, 225, 228, 229, 232, 233, 235–237, 264, 315, 321, 331 text, 3  classiﬁer, 38, 67, 146–149, 153, 159, 164,  169, 172, 173, 214, 223, 230–232, 239, 255, 273  base, 146–148, 151, 160, 162, 163, 167, 169, 170, 175, 224   Index  477  binary, 74, 229–232, 259, 265 linear, 80  clique, 234 closed, 422 clustering, 3, 117, 224 co-accessible, 125 code  binary, 232 word, 231, 232  complementarity conditions, 83, 90, 282,  421  complete metric space, 388 composition, 123 concave, 84, 91, 118, 226, 278, 304,  416–418, 420, 449, 451, 452, 456  function, 84, 118, 416, 449, 452 problem, 420  concentration inequality, xiv, 437, 445 concept, 1, 3, 9–19, 24–27, 29, 36, 41,  54–57, 145, 179, 180, 347, 348, 361–364, 369, 377  class, 10–12, 14, 16–19, 24–27, 29, 54, 55, 57, 145, 179, 362, 363, 377 class universal, 17  conditional  maximum entropy models, 315 probability, 431 relative entropy, 316, 319, 331, 452  Conditional Random Fields, see CRFs conﬁdence, 17, 27, 28, 57, 92, 93, 95, 97,  148, 155, 157, 159, 215, 229, 231, 232, 241, 264, 315  margin, 92, see margin, 95, 97, 155, 157, 159  conjugate, 157, 201, 300, 304, 307, 308,  313, 328, 329, 410, 423, 424, 426, 427, 454  function, 300, 304, 307, 308, 328, 329, 423, 424, 427  constraint  equality, 102, 420 qualiﬁcation, 420 qualiﬁcation strong, 421 qualiﬁcation weak, 421  context-free grammars, 363 convex, 415, 417  combination, 157–159, 222 diﬀerentiable functions, 421 function, 73, 151, 152, 165, 235, 237, 255, 275, 307, 337, 415–419, 422, 423, 455 function of Legendre type, 454 functions, 415, 421 hull, 37, 38, 157, 191, 250, 416 loss, 77, 153, 250, 284 optimization, xiii, xiv, 75, 82, 84, 88, 99, 110, 151, 162, 166, 221, 278, 286, 299, 304, 307, 317, 320, 323, 325, 331, 415, 420, 422, 426, 455 set, 299, 300, 317, 318, 416–419, 454 sets, 415 strictly, 82, 299, 307, 317, 417, 454, 455  convexity, 47, 75, 77, 88, 107, 188, 191, 203, 210, 211, 237, 277, 310, 416, 418–420, 422, 423, 437, 439, 443, 444, 451, 455, 456  core, 423 covariance, 348, 349, 353, 356, 433  matrix, 348, 349, 353, 356, 434 covering, 49, 58, 59, 133, 134, 263  number, 49, 58, 134, 263  CRFs, 235, 237 cross-validation, 68–73, 99, 102, 103, 166,  167, 285, 323, 324  n-fold, 70–73, 88, 228 error, 71, 102  consistent, 7–9, 15–17, 19–21, 24, 27, 55,  data  57, 163, 172, 179, 362–364, 375  algorithm, 16, 17 case, 20, 21 DFA, 362, 363, 375 hypothesis, 15–17, 172 pairwise, 257  constrained optimization problem, 420  set, 102, 109, 139, 161, 255, 347, 348, 357, 379 test, 5, 6 training, 5, 6, 21, 24, 25, 71, 72, 87, 88, 93, 101, 102, 170, 176, 228, 229 unseen, 8   478  validation, 4 DCG, 263, 264  normalized, 263  decision  epochs, 381 stump, 154 tree, 154, 155, 168, 169, 213, 221, 224, 225, 227, 228, 236, 291, 298, 365, 366, 368, 376 tree binary, 238, 365  DeepBoost, 169 degrees of freedom, 430 deterministic, 22, 361, 381 determinization, 361 DFA, 361, 362, 364–366, 368–370,  375–377  consistent, 362, 363, 375 equivalent, 361 minimal, 361, 362, 364, 376  dichotomy, 34, 36, 37, 50 diﬀerentiable  function, 170, 337, 415, 417, 453, 454 dimensionality reduction, 3, 6, 117, 347,  348, 351, 354, 356  discounted cumulative gain, see DCG distinguishing strings, 364 distribution, 430 -free model, 11 binomial, 430, 440, 448 chi-squared, 430 Gaussian, 303 Gibbs, 295, 299, 300, 306, 312, 317, 430 Laplace, 430 normal, 330, 355, 358, 430, 434, 443, 460 Poisson, 430 probability, 22, 131, 140, 397, 403, 429, 431, 453 divergence  Bregman, 169, 295, 307, 313, 331, 337, 453–456 Bregman generalized, 337, 338 Kullback-Leibler, 344, 450, 456 R´enyi, 456  DNF formula, 17 doubling trick, 185, 189, 204, 205 dual, 420  Langrange function, 420  Index  norm, 158, 287, 301, 308, 324, 327, 410 optimization, 83–85, 89–91, 99, 103, 116, 142, 194, 222, 237, 278, 279, 284, 292, 295, 302, 315, 318, 319 optimization problem, 420 problem, 84, 91, 278, 279, 282, 284, 299, 300, 302, 307, 313, 317, 318, 320, 331, 420, 425 variables, 83, 86, 90, 292, 293  duality  Fenchel, 300, 307, 328, 426 gap, 420 strong, 84, 420, 425, 426 weak, 425  early stopping, 165, 167, 170 edge, 149, 246 eigenvalue, 411, 413, 418 emphasis function, 262 empirical  error, 10 kernel map, 113, 114 kernel maps, 112 Rademacher complexity, 30 risk, 10 risk minimization, see risk minimization  empty string, 122, 360 ensemble  algorithms, 145 hypotheses, 155 methods, 145, 155, 165, 250, 251  entropy, 168, 169, 199, 201, 226, 227, 295, 296, 298, 299, 302, 306–308, 312, 317, 330, 331, 344, 345, 438, 439, 449–454, 456, 457, 477, 482  binary, 450 binary function, 449 conditional relative, 319, 331 maximum, 295 maximum conditional, 315 R´enyi, 456 regularized, 344 relative, 450 relative conditional, 316, 452 unnormalized relative, 453  envelope, 290   Index  479  environment, 1, 7, 379–381, 387, 393, 397,  403, 404  model, 379, 380, 387, 393, 397 unknown, 403  epigraph, 416 equivalence queries, 363 equivalent, 409 Erd¨os, 43 ERM, see risk minimization error, 10  empirical, 10, 19–21, 57, 59, 65, 67, 88, 145, 148–151, 154, 168, 171, 172, 175, 211, 214, 227, 236, 241, 246–248, 252, 269, 270, 273, 275, 276, 286, 294, 334, 336 estimation, 61–64, 67, 73 excess, 64, 65, 67, 74, 76–78 generalization, 10, 11, 16, 19–22, 24, 26, 43, 59, 65, 66, 69, 70, 79, 86, 87, 95, 97–99, 140, 155, 161, 166, 172, 178, 201, 202, 204, 212, 214, 217, 230, 238, 241–243, 252, 268, 276, 323, 334, 336, 342 leave-one-out, 72, 85, 86, 193, 194, 293, 294, 342 mean squared, 228, 275, 277, 289 reconstruction, 353 test, 6, 155 training, 65, 155, 174 true, 20  error-correcting output codes   ECOC , 231  estimation error, 61 events, 140 set, 429  examples, 4  labeled, 5–7, 59, 71, 170, 223, 364 misclassiﬁed, 171 negative, 23 positive, 16, 17, 227, 369, 376  excess error, 61 expectation, 431  linearity, 86, 133, 336, 432  expected loss, 163 experience, 1 expert  advice, 27, 177–179  algorithm, 205 best, 7, 178, 181–183, 205  exploration versus exploitation, 7 exponential inequality, 448  false  negative, 12 positive, 12, 103, 143, 256 positive rate, 256  fat-shattered, 274 fat-shattering, 290 dimension, 274  feature, 4  extraction, 347 function, 430 mapping, 112–114, 117–119, 137, 139, 219, 220, 243, 244, 275, 276, 281, 283, 284, 297, 315, 347, 350 missing, 228 space, 92, 97–99, 104, 106, 107, 112, 130, 131, 139, 140, 167, 224, 243, 276, 280, 289, 376, 459 vector, 156, 225, 234, 279, 280, 300, 320, 321, 327, 350  Fenchel  conjugate, 423 duality theorem, 300, 307, 328, 426 problems, 425  Fermat’s theorem, 415 ﬁnal  state, 123, 125, 360, 361, 364, 366, 370–372, 374, 398 weight, 122–124  ﬁnite, 381  horizon, 381 query subset, 257  ﬁxed point, 230, 388, 394, 397, 401 Frobenius  norm, 412 product, 412  Fubini’s theorem, 44, 432 function  aﬃne, 151, 275, 455 measurable, 22, 28, 273 symmetric, 107, 262   480  game  zero-sum, 163, 164, 204  gap penalty, 128 Gaussian, 430  distribution, 303 kernel, 110 generalization  bound, 14, 15, 21, 35, 43, 58, 59, 93, 94, 97, 159, 217, 220, 235, 236, 243, 268, 272, 273, 280, 283, 284, 287, 288, 293, 331, 333, 341, 343 error, 10  geometric margin  L1-, 157, 161  317, 430 gradient, 415  Gibbs distribution, 295, 299, 300, 306, 312,  descent, 191, 192, 207, 289, 291, 294, 304, 313, 320, 327, 404  Gram matrix, 84, 108, 139 graph  acyclic, 55 Laplacian, 352, 357 neighborhood, 352, 353 structure, 235  graphical model, 234 group  Lasso, see Lasso norm, 412  Index  Hoeﬀding’s  inequality, 19, 27, 59, 69, 134, 203, 268, 269, 437–439, 441, 443, 445, 447 lemma, 188, 201, 306, 441, 445  horizon  ﬁnite, 382 inﬁnite, 382, 385  Huber loss, 284 hyperparameters, 4, 5 hyperplane  marginal, 81, 83, 87, 90, 91 maximum-margin, 80, 81, 83, 197 optimal, 100  hypothesis  base, 151, 152, 155–157, 173, 174, 211, 255 linear, 64, 97, 98, 155, 265, 277 set, 5, 7, 10 set ﬁnite, 15, 20, 21, 25, 27, 53, 180, 268 single, 19  i.i.d., 10, 11, 15, 20, 32, 193, 194, 252, 296,  334–336, 404, 431, 480  impurity  Gini index, 226 misclassiﬁcation, 226  inconsistent, 9  case, 19, 27, 269  growth function, 29, 34–36, 40–42, 50, 56,  63, 333   i.i.d. , 431  inequality  independent and identically distributed  H¨older’s inequality, 166, 210, 301, 303,  322, 329, 411  Halving algorithm, 179, 181, 183 Hamming distance, 214, 231, 232,  234, 444  Hessian, 82, 84, 210, 313, 415, 417 Hilbert space, 73, 105, 107, 108, 110, 112,  113, 121, 138, 139, 141, 142, 410, 422, 423, 425, 444, 454  pre-, 112 reproducing kernel, 110–112, 117, 336, 350  hinge loss, 88, 89, 99, 174, 207, 341–343  quadratic, 89, 343  Azuma’s, 202, 442, 445 Bennett’s, 447 Bernstein’s, 438, 440, 447 Cauchy-Schwarz, 53, 97, 98, 112, 118, 197, 309, 310, 339, 341, 409, 410, 433 Chebyshev’s, 433, 447 concentration, xiv, 437, 445 exponential, 448 H¨older’s, 166, 210, 301, 303, 322, 329, 411 Hoeﬀding’s, 19, 27, 59, 69, 134, 203, 268, 269, 437–439, 441, 443, 445, 447 Jensen’s, 51–53, 77, 97, 104, 118, 133, 134, 188, 311, 318, 327, 443, 444, 450, 451   Index  481  Khintchine-Kahane, 118, 186, 445 Log-sum, 451, 452 Markov’s, 134, 354, 432, 433, 437 maximal, 324 McDiarmid’s, 29, 31, 32, 139, 310, 335, 442, 443, 445 Pinsker’s, 302, 318, 344, 439, 456 Slud’s, 440 Young’s, 410  information theory, xiv, 23, 407, 449, 450,  456  initial state, 381 input space, 9, 22, 30, 33, 52, 56, 79,  105–109, 112, 121, 130, 139, 173, 197, 213, 240, 267, 275, 285, 351, 449  instances, 9 inverse  generalized, 412  Isomap, 351, 352, 356 iterative scaling  generalized, 313  Jensen’s inequality, 51–53, 77, 97, 104,  118, 133, 134, 188, 311, 318, 327, 443, 444, 450, 451  Johnson-Lindenstrauss lemma, 348, 354,  356  joint probability mass function, 429  kernel, 105, 106  approximate feature maps, 131 bigram, 128, 129 bigram sequence, 129 continuous, 131, 142 convolution, 136 diﬀerence, 138 empirical map, 112, 113 functions, 107, 130–132, 135, 137, 138, 222, 343, 350 gappy bigram, 129 Gaussian, 110, 113, 116 map empirical, 114 matrix, 108, 113–116, 118, 128, 143, 244, 270, 278, 280, 282, 284, 285, 293, 343, 344, 350, 352–354, 357 methods, xiv, 85, 105, 106, 130, 136, 351  negative deﬁnite symmetric, 105, 119, 121, 141 normalized, 112, 113, 116, 137, 285 PCA, 347, 349–354, 356, 357 polynomial, 108, 109, 131, 139 positive deﬁnite, 108 positive deﬁnite symmetric, 110–119, 121, 137–140, 197, 199, 219, 220, 233, 243, 276, 281–284, 289, 293, 336, 338, 350 positive semideﬁnite, 108 rational, 105, 122, 127, 142 ridge regression, 267, 275–277, 292, 294, 333, 343 sequence, 121, 129 shift-invariant, 131 sigmoid, 110  Khintchine-Kahane inequality, 118, 186,  KKT conditions, 83, 89, 221, 278, 282,  445  284, 421  KPCA, 347, 349–354, 356, 357 Kullback-Leibler divergence, 344, 450, 456  labels, 4, 9 Lagrange, 83, 89, 90, 99, 101, 102, 166,  304, 313, 324, 420  dual function, see dual function, 420 multipliers, 101, 102 variables, 83, 89, 90  Lagrangian, 83, 89, 90, 221, 278, 282, 284,  420–422  Laplace distribution, 430 Laplacian eigenmaps, 351, 352, 357 Lasso, 267, 275, 285–288, 290, 291, 293,  294, 343 group, 289 on-line, 294  law of large numbers, 394 learner  strong, 146 weak, 145, 146  learning  active, 7, 362 algorithm, 1, 4–6, 9, 19, 20, 23, 24, 27, 43, 46, 47, 55, 57, 71, 80, 85, 98, 146,   482  Index  148, 150, 168, 173, 179, 186, 202, 252, 257, 333, 334, 343, 362 algorithm PAC, 12, 16, 26–28, 146 algorithm weak, 146, 244 on-line, 7, 177 passive, 7 policy, 401 problem, 380 reinforcement, 7, 379 with queries, 363  leave-one-out  cross-validation, 71 error, 85  lemma  Hoeﬀding’s, 188, 201, 306, 441, 445 Johnson-Lindenstrauss, 348, 354, 356 Massart’s, 35, 51, 287 Sauer’s, 40–43, 49, 50, 55 Talagrand’s, 52, 216, 217, 242  linear  -ly separable labeling, 50 algebra, 409 classiﬁcation problem, 79 classiﬁers, 79  Lipschitz  function, 52, 93  LLE, 353, 354, 356, 357 locally linear embedding, see LLE Log-sum inequality, 451, 452 logistic, 330 form, 326 loss, 153 multinomial regression, 315 regression, 153, 315, 325  logistic regression  L1-regularized, 325  loss   cid:15 -insensitive, 282 convex, 77, 153, 250, 284 function, 268 hinge, 88, 89, 99, 174, 207, 341–343 Huber, 284 logistic, 153 margin, 92 matrix, 163 quadratic  cid:15 -insensitive, 283 quadratic hinge, 88, 89, 343  squared, 268 loss function, 4  manifold learning, 3 margin  L1-, 156 L1-geometric, 156 conﬁdence, 92 geometric, 80 hard, 88 loss function, 92 soft, 88  Markov decision process, see MDP Markov’s inequality, 134, 354, 432, 433,  437  martingale diﬀerences, 441 Massart’s lemma, 35, 51, 287 Maxent  conditional, 316 conditional models, 315 conditional principle, 316 conditional structural models, 330 models, 295, 298, 299, 306, 307, 312, 315–317, 319–321, 325–327, 330–332 principle, 298, 299, 302, 306, 312, 315, 316, 319, 330 structural models, 312 unregularized, 298 unregularized conditional, 316  maximal inequality, 324 maximum a posteriori, 297 maximum entropy models, 295 maximum likelihood  principle, 296  McDiarmid’s inequality, 29, 31, 32, 139,  310, 335, 442, 443, 445  MDP, 380, 381, 383, 385, 389–391, 393  ﬁnite, 382, 385–387, 399 partially observable, 403  mean squared error, 268 measurable, 429 membership queries, 363 Mercer’s condition, 107, 142 minimization, 361 mistake  bound, 179   Index  bound model, 179 model, 178  mixed strategy, 163 model  -based, 393 -free approach, 393 selection, 61, 71 model selection, 61 moment-generating function, 354, 434,  445  mono-label case, 213 Moore-Penrose pseudo-inverse, 412 multi-class classiﬁcation, xiii, 168,  213–215, 217, 219–222, 224, 225, 228, 229, 232, 233, 235–237, 264, 315, 321, 331  multi-label case, 213 mutual information, 453  NFA, 361 node impurity, 226 noise, 23 non-realizable, 29 non-stationary policy, 382 norm, 409  483  OVA, see one-versus-all OVO, see one-versus-one  PAC, 11  agnostic learning, 22, 48, 61 learnable, 11 learnable with membership queries, 363 learning, 12, 14, 16–19, 22, 23, 26–28, 45, 57, 145, 146, 343, 361, 363, 364, 376 learning algorithm, 11, 12, 16, 26–28, 146 learning framework, 9 weakly learnable, 145  packing numbers, 49 pairwise  consistent, 257 independence, 258  parallelogram identity, 457 part-of-speech tagging, 233 partition function, 300, 430 passive, 362  learning, 7  PCA, 347–351, 356–358 penalized risk estimate, 211 Perceptron  dual, 158, 287, 301, 308, 324, 327, 410 Frobenius, 412 matrix, 411 operator induced, 411 spectral, 411  normal, 430  distribution, 330, 355, 358, 430, 434, 443, 460  normalization, 110 normalized discounted cumulative gain,  see DCG normalized  Occam’s razor principle, 23, 43, 79, 269,  362  oﬀ-policy algorithm, 401 on-line learning, 7, 177 on-policy algorithm, 402 one-versus-all, 229–232, 236 one-versus-one, 229–232, 238 orthogonal projection, 412 outlier, 87, 168  algorithm, 100, 190–193, 196–199, 201, 206–208, 265 dual, 197 kernel, 197, 206, 212 update, 207 voted, 193, 197  Pinsker’s inequality, 302, 318, 344, 439,  456  pivot, 261 planning, 379, 387 pointwise  maximum, 418 supremum, 418  Poisson distribution, 430 policy, 379–381, 390, 393  iteration algorithm, 390 value, 379  POMDP, see MDP partially observable positive, 12  deﬁnite, 105, 108, 412 deﬁnite symmetric, 107, 108 semideﬁnite, 108, 412   484  Index  precision, 263 preference  -based setting, 240, 257 function, 240, 257  generalization bound, 158 local, 48  Rademacher variables, 30, 32, 51, 118, 186,  219, 443  preﬁx-tree automaton, 370 primal problem, 420 principal component analysis, see PCA prior  radial basis function, 110 Radon’s theorem, 38 random variable, 429 Randomized-Weighted-Majority algorithm,  knowledge, 5 probability, 27  probabilistic method, 43 probability, 11  density function, 429 distribution, 429 mass function, 429 space, 429  probably approximately correct, see PAC probit model, 330 proper, 422 pseudo-dimension, 267, 272 pure strategies, 163  Q-learning  algorithm, 393, 394, 398, 399  QP, 82, 84, 100, 101, 230, 235, 282, 288  convex, 82, 91, 282–284  quadratic   cid:15 -insensitive loss, 283 hinge loss, 88 programming, 82 SVR, 283  QuickSort algorithm, 261  randomized, 260  R´enyi  divergence, 456 entropy, 456  184, 186, 209  RankBoost, 236–239, 244–255, 264, 265 ranking, 3, 239, 259 RankPerceptron, 265 rational, 126 Rayleigh quotient, 349, 413 RBF, see radial basis function realizable, 29 recall, 263 reconstruction error, 348 regression, 3, 228, 267 kernel, 267, 275, 276 linear, 267, 275 ordinal, 264 support vector, 267, 275, 281  regret, 7, 178, 258  external, 178, 205 internal, 205 swap, 205  regular  expressions, 361 languages, 361  regularization  -based algorithm, 72 parameter, 73 path, 288 term, 73  reinforcement learning, 7, 379 relative entropy, 450 representer theorem, 117 reproducing  Rademacher complexity, 29–36, 43, 48,  50–53, 63, 68, 79, 97, 100, 118, 157–159, 169, 213, 219, 220, 236, 239, 241, 243, 251, 263, 267, 269, 270, 274, 275, 287, 298, 316, 333, 460  bound, 43, 287, 298, 316 empirical, 29–31, 33, 34, 49, 51–53, 93, 97, 117, 118, 143, 157, 216, 250, 271, 277, 283, 287, 323, 324, 327, 443  property, 111  reversible, 370  languages, 370  reward, 379  probability, 381  risk, 10 risk minimization  empirical, 34, 62–65, 67, 68, 70, 73, 236   Index  structural, 64 voted, 78  RKHS, see Hilbert space ROC curve, 239, see also AUC, 255, 256,  264  RWM algorithm, see  Randomized-Weighted-Majority algorithm  sample  complexity, 1, 9 space, 429  Sanov’s theorem, 439 SARSA algorithm, 402, 403 Sauer’s lemma, 40–43, 49, 50, 55 scoring function, 240 semi-supervised learning, 6 shattering, 36, 271 shortest-distance, 123  algorithm, 136  singular value, 412  decomposition, 350, 412, 413  singular vector, 412 slack variables, 87 Slater’s condition, 421  weak, 421  Slud’s inequality, 440 SMO algorithm, 84, 100–102 society, 213 soft-max, 235 sphere trees, 225 SPSD, see symmetric positive semideﬁnite squared loss, 268 SRM, 64–70, 72, 73, 77 stability, 334  algorithmic, 333 coeﬃcient, 334  standard deviation, 432 start state, 381 state, 379, 380 state-action value function, 385, 394, 398,  402  stationary  point, 415 policy, 382  stochastic  485  approximation, 394 assumption, 361, 362 scenario, 21 subgradient descent, 191  structural risk minimization, see risk  minimization  structured  output, 233  stumps, 154 subdiﬀerential, 337, 422 subgradient, 337, 422 submultiplicative property, 411 supervised learning, 6 support vector, 83, 90, 193 support vector machines, see SVM support vector networks, see SVM SVD, see singular value decomposition SVM, 79–84, 86, 90, 91, 100, 101, 103, 105, 116, 130, 143, 156, 162, 170, 194, 196, 207, 222, 243, 281 multi-class, 221, 236  SVMStruct, 235 SVR, 275, 281–285, 289, 291–293, 333,  337, 339–341  dual, 291 on-line, 291 quadratic, 284, 294  symmetric, 411  positive semideﬁnite, 108, 110, 114, 115, 128, 279, 357, 412, 413  Talagrand’s lemma, 52, 216, 217, 242 tensor product, 114 test sample, 4 theorem  Fenchel duality, 300, 307, 328, 426 Fermat’s, 415 Fubini’s, 44, 432 Radon’s, 38 representer, 117 Sanov’s, 439  trace, 411 training sample, 4 transducer  weighted, 122  transductive inference, 6   Index  486  transition, 360  probability, 381  transpose, 411 trigrams, 106 true positive rate, 256  uncentered, 356 uncombined algorithms, 213, 221 uncorrelated, 433 uniform convergence bound, 15 unnormalized relative entropy, 453 unstable, 228 unsupervised learning, 6 update rule, 401  validation sample, 4 validation set, 68 value, 382  iteration algorithm, 387  variance, 432 VC-dimension, 29, 36–40, 42, 43, 45, 46, 48–51, 53–57, 63, 68, 77, 79, 91, 98, 100, 103, 104, 145, 154, 155, 158, 167, 168, 170, 172, 179, 238, 243, 263, 267, 271–273, 290, 333, 376, 377  generalization bound, 57  visualization, 347 voted risk minimization, see risk  minimization  weight, 181  function, 262  Weighted-Majority algorithm, 181, 183,  184, 186, 198, 205  Widrow-Hoﬀ algorithm, 289, 290  on-line, 291  Winnow  algorithm, 198, 199, 206 update, 198  witness, 271 WM algorithm, see Weighted-Majority  algorithm  Young’s inequality, 410  zero-sum game, 163   Adaptive Computation and Machine Learning  Francis Bach, Editor  Bioinformatics: The Machine Learning Approach, Pierre Baldi and Søren Brunak  Reinforcement Learning: An Introduction, Richard S. Sutton and Andrew G. Barto  Graphical Models for Machine Learning and Digital Communication, Brendan J. Frey  Learning in Graphical Models, Michael I. Jordan  Causation, Prediction, and Search, second edition, Peter Spirtes, Clark Glymour, and Richard Scheines  Principles of Data Mining, David Hand, Heikki Mannila, and Padhraic Smyth  Bioinformatics: The Machine Learning Approach, second edition, Pierre Baldi and Soren Brunak  Learning Kernel Classiﬁers: Theory and Algorithms, Ralf Herbrich  Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond, Bernhard Sch¨olkopf and Alexander J. Smola  Introduction to Machine Learning, Ethem Alpaydin  Gaussian Processes for Machine Learning, Carl Edward Rasmussen and Christopher K.I. Williams  Semi-Supervised Learning, Olivier Chapelle, Bernhard Sch¨olkopf, and Alexander Zien, Eds.  The Minimum Description Length Principle, Peter D. Gr¨unwald  Introduction to Statistical Relational Learning, Lise Getoor and Ben Taskar, Eds.  Probabilistic Graphical Models: Principles and Techniques, Daphne Koller and Nir Friedman  Introduction to Machine Learning, second edition, Ethem Alpaydin  Machine Learning in Non-Stationary Environments: Introduction to Covariate Shift Adapta- tion, Masashi Sugiyama and Motoaki Kawanabe  Boosting: Foundations and Algorithms, Robert E. Schapire and Yoav Freund  Machine Learning: A Probabilistic Perspective, Kevin P. Murphy  Foundations of Machine Learning, Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar  Introduction to Machine Learning, third edition, Ethem Alpaydin  Deep Learning, Ian Goodfellow, Yoshua Bengio, and Aaron Courville  Elements of Causal Inference, Jonas Peters, Dominik Janzing, and Bernhard Sch¨olkopf   Machine Learning for Data Streams, with Practical Examples in MOA, Albert Bifet, Ricard Gavald, Geoﬀrey Holmes, Bernhard Pfahringer  Foundations of Machine Learning, second edition, Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar

@highlight

A new edition of a graduate-level machine learning textbook that focuses on the analysis and theory of algorithms. This book is a general introduction to machine learning that can serve as a textbook for graduate students and a reference for researchers. It covers fundamental modern topics in machine learning while providing the theoretical basis and conceptual tools needed for the discussion and justification of algorithms. It also describes several key aspects of the application of these algorithms. The authors aim to present novel theoretical tools and concepts while giving concise proofs even for relatively advanced topics. Foundations of Machine Learning is unique in its focus on the analysis and theory of algorithms. The first four chapters lay the theoretical foundation for what follows; subsequent chapters are mostly self-contained. Topics covered include the Probably Approximately Correct (PAC) learning framework; generalization bounds based on Rademacher complexity and VC-dimension; Support Vector Machines (SVMs); kernel methods; boosting; on-line learning; multi-class classification; ranking; regression; algorithmic stability; dimensionality reduction; learning automata and languages; and reinforcement learning. Each chapter ends with a set of exercises. Appendixes provide additional material including concise probability review. This second edition offers three new chapters, on model selection, maximum entropy models, and conditional entropy models. New material in the appendixes includes a major section on Fenchel duality, expanded coverage of concentration inequalities, and an entirely new entry on information theory. More than half of the exercises are new to this edition